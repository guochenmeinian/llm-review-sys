# Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning

 Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, Xiangyang Ji

Department of Automation

Tsinghua University, Beijing, China

{sjz18, qy22, hc-zhang19}@mails.tsinghua.edu.cn

cclvr@163.com

xyji@tsinghua.edu.cn

Equal contribution.

###### Abstract

Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. To mitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the agent number, which is therefore theoretically superior to the direct treatment referred to above, especially when the agent number is large. We further conduct experiments on four environments including both discrete and continuous action settings on both existing and our man-made datasets, demonstrating that CFCQL outperforms existing methods on most datasets and even with a remarkable margin on some of them.

## 1 Introduction

Online Reinforcement Learning (Online RL) needs frequently deploying untested policies to environment for data collection and policy optimization, making it dangerous and inefficient to apply in the real-world scenarios (e.g. autonomous vehicle teams). While, Offline Reinforcement Learning (Offline RL) aims to learn policies from a fixed dataset rather than from interacting with the environment, and therefore is suitable for the real applications with highly safety requirements or without efficient simulators [25].

Directly applying off-policy RL to the offline setting may fail due to overestimation [13; 24]. Existing works usually tackle this problem by pessimism. They either utilize behavior regularization to constrain the learning policy close to the behavior policy induced by the dataset [49; 20; 11], or conduct conservative(pessimistic) value iteration to mitigate unexpected overestimation [18; 21]. It has been demonstrated both theoretically and empirically that these methods can achieve comparable performance to their online counterparts under some conditions [21].

Though cooperative Multi-agent Reinforcement Learning (MARL) has gained extraordinary success in various multiplayer video games such as Dota [4], StarCraft [40] and soccer [23], applying current MARL in real scenarios is still challenging due to the same safety and efficiency concerns in single-agent setting, then it is worth conducting investigation for offline RL in multi-agent setting. Compared with single-agent setting, offline RL in multi-agent setting has its own difficulty. On the one hand, the action space explodes exponentially as the agent number increases, then an arbitrary joint action is more likely to be an OOD action given the fixed dataset size, making the OOD phenomenon more severe. This will further exacerbate the issues of extrapolation error and overestimation in the policy evaluation process and thus induce an unexpected or even disastrous final policy [52]. On the other hand, as the core of MARL, the agents need to consider not only their own actions but also other agents' actions as well as contributions to the global return in order to achieve an overall high performance, which undoubtedly increases the difficulty for theoretical analysis. Besides, instead of just guaranteeing single policy improvement, the bounded team performance is also a key concern to offline MARL.

There exist few works to combine MARL with offline RL. Pan et al. [34] uses Independent Learning [44] as a simple solution, i.e., each agent regards others as part of the environment and independently performs offline RL learning. Although mitigating the joint action OOD issue through decoupling the agents's learning completely, it essentially still adopts a single-agent paradigm to learn and thus cannot enjoy the recent progress on MARL that a centralized value function can empower better team coordination [37; 27]. To utilize the advantage of centralized training with decentralized execution (CTDE), Yang et al. [52] applies implicit constraint approach on the value decomposition network [43], which alleviates the extrapolation error and gains some performance improvement empirically. Although it proposes some theoretical analysis for the convergence property of value function, whether the team performance can be bounded from below as agents number increases remains still unknown.

In this paper, we introduce a novel offline MARL algorithm called Counterfactual Conservative Q-Learning (CFCQL) and aim to address the overestimation issue rooted from joint action OOD phenomenon. It adopts CTDE paradigm to realize team coordination, and incorporates the state-of-the-art offline RL algorithm CQL to conduct conservative value estimation. CQL is preferred due to its theoretical guarantee and flexibility of implementation. One direct treatment is to regard all the agents as a single one and conduct standard CQL learning on the joint policy space which we call MACQL. However, too much conservatism will be generated in this way since the induced penalty can be exponentially large in the joint policy space. Instead, CFCQL separately regularizes each agent in a counterfactual way to avoid too much conservatism. Specifically, each agent separately contributes CQL regularization for the global Q value and then a weighted average is used as an overall regularization. When calculating agent \(i\)'s regularization term, rather than sampling OOD actions from the joint action space as MACQL does, we only sample OOD actions for agent \(i\) and leave other agents' actions sampled in the dataset. We prove that CFCQL enjoys underestimation property as CQL does and the safe policy improvement guarantee independent of the agents number \(n\), which is advantageous to MACQL especially under the situation with a large \(n\).

We conduct experiments on \(1\) man-made environment Equal Line, and \(3\) commonly used multi-agent environments: StarCraft II [40], Multi-agent Particle Environment [27], and Multi-agent MuJoCo [35], including both discrete and continuous action space setting. With datasets collected by Pan et al. [34] and ourselves, our method outperforms existing methods in most settings and even with a large margin on some of them.

We summarize our contributions as follows: (1) we propose a novel offline MARL method CFCQL based on CTDE paradigm to address the overestimation issue and achieve team coordination at the same time. (2) We theoretically compare CFCQL and MACQL to show that CFCQL is advantagous to MACQL on the performance bounds and safe policy improvement guarantee as agent number is large. (3) In hard multi-agent offline tasks with both discrete and continuous action space, our method shows superior performance to the state-of-the-art.

## 2 Related Works

**Offline RL.** Standard RL algorithms are especially prone to fail due to erroneous value overestimation induced by the distributional shift between the dataset and the learning policy. Theoretically, it is proved that pessimism can alleviate overestimation effectively and achieve good performance even with non-perfect data coverage [5; 15; 26; 22; 39; 51; 55; 6]. In the algorithmic line, there are broadly two categories: uncertainty based ones and behavior regularization based ones. Uncertainty based approaches attempt to estimate the epistemic uncertainty of Q-values or dynamics, and then utilize this uncertainty to pessimistically estimating Q in a model-free manner [2; 50; 3], or conduct learning on the pessimistic dynamic model in a model-based manner [54; 16]. Behavior regularization based algorithms constrain the learned policy to lie close to the behavior policy in either explicit or implicit ways [20; 13; 49; 18; 21; 11], and is advantageous over uncertainty based methods in computation efficiency and memory consumption. Among these class of algorithms, CQL[21] is preferred due to its superior empirical performance and flexibility of implementation.

**MARL.** A popular paradigm for multi-agent reinforcement learning is centralized training with decentralized execution (CTDE) [37; 27]. Centralized training inherits the idea of joint action learning [7], empowering the agents with better team coordination. And decentralized execution enjoys the deploying flexibility of independent learning [44]. In CTDE, some value-based works concentrate on decomposing the single team reward to all agents by value function factorization [43; 37], based on which they further derive and extend the Individual-Global-Max (IGM) principle for policy optimality analysis [42; 47; 38; 46]. Another group of works focus on the actor-critic framework, using a centralized critic to guide each agent's policy update [27; 9]. Some variants of PPO [41], including IPPO [8], MAPPO [53], and HAPPO [19] also show great potential in solving complex tasks. All these works use the online learning paradigm, therefore disturbed by extrapolation error when transferred to the offline setting.

**Offline MARL.** To make MARL applied in the more practical scenario with safety and training efficiency concerns, offline MARL is proposed. Jiang & Lu [14] shows the challenge of applying BCQ [13] to independent learning, and Pan et al. [34] uses zeroth-order optimization for better coordination among agents' policies. Although it empirically shows fast convergence rate, the policies trained by independent learning have no theoretical guarantee for the team performance. With CTDE paradigm, Yang et al. [52] adopts the same treatment as Peng et al. [36] and Nair et al. [30] to avoid sampling new actions from current policy, and Tseng et al. [45] regards the offline MARL as a sequence modeling problem, solving it by supervised learning. As a result, both methods' performance relies heavily on the data quality. In contrast, CFCQL does not require the learning policies stay close to the behavior policy, and therefore performs well on datasets with low quality. Meanwhile, CFCQL is theoretically guaranteed to be safely improved, which ensures its performance on datasets with high quality.

## 3 Preliminary

### MARL Symbols

We use a _decentralised partially observable Markov decision process_ (Dec-POMDP) [32]\(G=\langle S,\mathbf{A},I,P,r,Z,O,n,\gamma\rangle\) to model a fully collaborative multi-agent task with \(n\) agents, where \(s\in S\) is the global state. At time step \(t\), each agent \(i\in I\equiv\{1,...,n\}\) chooses an action \(a^{i}\in A\), forming the joint action \(\mathbf{a}\in\mathbf{A}\equiv A^{n}\). \(T(s^{\prime}|s,\mathbf{a}):S\times\mathbf{A}\times S\rightarrow[0,1]\) is the environment's state transition distribution. All agents share the same reward function \(r(s,\mathbf{a}):S\times\mathbf{A}\rightarrow\mathbb{R}\). \(\gamma\in[0,1)\) is the discount factor. Each agent \(i\) has its local observations \(o^{i}\in\mathbf{O}\) drawn from the observation function \(Z(s,i):S\times I\to O\) and chooses an action by its policy \(\pi^{i}(a^{i}|o^{i}):O\rightarrow\Delta([0,1]^{|A|})\). The agents' joint policy \(\boldsymbol{\pi}:=\prod_{i=1}^{n}\pi^{i}\) induces a joint _action-value function_: \(Q^{\boldsymbol{\pi}}(s,\mathbf{a})=\mathbb{E}[R|s,\mathbf{a}]\), where \(R=\sum_{t=0}^{\infty}\gamma^{t}r_{t}\) is the discounted accumulated team reward. We assume \(\forall r,|r|\leq R_{\max}\). The goal of MARL is to find the optimal joint policy \(\boldsymbol{\pi}^{*}\) such that \(Q^{\boldsymbol{\pi}^{*}}(s,\mathbf{a})\geq Q^{\boldsymbol{\pi}}(s,\mathbf{a})\), for all \(\boldsymbol{\pi}\) and \((s,\mathbf{a})\in S\times\mathbf{A}\). In offline MARL, we need to learn the optimal \(\boldsymbol{\pi}^{*}\) from a fixed dataset sampled by an unknown behaviour policy \(\boldsymbol{\beta}\), and we can not deploy any new policy to the environment to get feedback.

### Value Functions in MARL

In MARL settings with discrete action space, we can maintain for each agent a local-observation-based Q function \(Q^{i}(o^{i},a^{i})\), and define the local policy \(\pi^{i}:=\arg\max_{a}Q^{i}(o^{i},a)\). To train local policies with single team reward, Rashid et al. [37] proposes QMIX, using a global \(Q^{tot}\) to model the cumulative team reward. Define the Bellman operator \(\mathcal{T}\) as \(\mathcal{T}Q(s,\bm{a})=r+\gamma\max_{\bm{a}^{\prime}}Q(s^{\prime},\bm{a}^{\prime})\). QMIX aims to minimize the temporal difference:

\[\hat{Q}_{k+1}^{tot}\leftarrow\arg\min_{Q}\mathbb{E}_{s,\bm{a},s^{\prime} \sim\mathcal{D}}[(Q(s,\bm{a})-\hat{\mathcal{T}}\hat{Q}_{k}^{tot}(s,\bm{a}))^ {2}],\] (1)

where \(\hat{\mathcal{T}}\) is the empirical Bellman operator using samples from the replay buffer \(\mathcal{D}\), and \(\hat{Q}^{tot}\) is the empirical global Q function, computed from \(s\) and \(Q^{i}\)s represented by a neural network satisfying \(\partial Q^{tot}/\partial Q^{i}\geq 0\). With such restriction, QMIX can ensure the Individual-Global-Max principle that \(\arg\max_{\bm{a}}Q^{tot}(s,\bm{a})=(\arg\max_{a^{1}}Q^{1}(o^{1},a^{1}),...,\arg \max_{a^{n}}Q^{n}(o^{n},a^{n}))\). For continuous action space, a neural network is used to represent the local policy \(\pi^{i}\) for each agent. Lowe et al. [27] proposes MADDPG, maintaining a centralized critic \(Q^{tot}\) to directly guide the local policy update by gradient descent. And the update rule of \(Q^{tot}\) is similar to Eq. 1, just replacing \(\mathcal{T}\) with \(\mathcal{T}^{\bm{\pi}}\). \(\mathcal{T}^{\bm{\pi}}Q=r+\gamma P^{\bm{\pi}}Q\), where \(P^{\bm{\pi}}Q(s,\bm{a})=\mathbb{E}_{s^{\prime}\sim T(\cdot|s,\bm{a}),\bm{a}^{ \prime}\sim\bm{\pi}(\cdot|s^{\prime})}[Q(s^{\prime},\bm{a}^{\prime})]\).

### Conservative Q-Learning

Offline RL is prone to the overestimation of value functions. For an intuitive explanation of the underlying cause, please refer to Appendix E.1. Kumar et al. [21] proposes Conservative Q-Learning (CQL), adding a regularizer to the Q function to address the overestimation issue, which maximizes the Q function of the state-action pairs from the dataset distribution \(\beta\), while penalizing the Q function sampled from a new distribution \(\mu\) (e.g., current policy \(\pi\)). The conservative policy evaluation is shown as follows:

\[\hat{Q}_{k+1}\leftarrow\arg\min_{Q}\alpha[\mathbb{E}_{s\sim\mathcal{D},\alpha \neq k}[Q(s,a)]-\mathbb{E}_{s\sim\mathcal{D},\alpha\neq k}[Q(s,a)]]+\frac{1}{2 }\mathbb{E}_{s,a,s^{\prime}\sim\mathcal{D}}[(Q(s,a)-\hat{\mathcal{T}}^{\pi} \hat{Q}_{k}(s,a))^{2}].\] (2)

As shown in Theorem 3.2 in Kumar et al. [21], with a large enough \(\alpha\), we can obtain a Q function, whose expectation over actions is lower than that of the real Q, then the overestimation issue can be mitigated.

## 4 Proposed Method

We first show the overestimation problem of multi-agent value functions in Sec. 4.1, and a direct solution by the naive extension of CQL to multi-agent settings in Sec. 4.2, which we call MACQL. In Sec. 4.3, we propose Counterfactual Conservative Q-Learning (CFCQL) and further demonstrate that it can realize conservatism in a more mild and controllable way which is independent of the number of agents. Then we compare MACQL and CFCQL from the aspect of safe improvement performance. In Sec. 4.4, we present our novel and practical offline RL algorithm.

### Overestimation in Offline MARL

The Q-values in online learning usually face the overestimation problem [1; 33], which becomes more severe in offline settings due to distribution shift and offline policy evaluation [13]. Offline MARL suffers from the same issue even worse since the joint action space explodes exponentially as the agent number increases, then an arbitrary joint action is more likely to be an OOD action given the fixed dataset size. To illustrate this phenomenon, we design a toy Multi-Agent Markov Decision Process (MMDP) as shown in Fig.1(a). There are five agents and three states. All agents are randomly initialized and need learn to take three actions (stay, move up, move down) to move to or stay in the state \(S2\). The dataset consists of \(1000\) samples with reward larger than \(0.8r_{\max}\). As shown in Fig. 1(b), if not specially designed for offline setting,

Figure 1: (a) A decomposable Multi-Agent MDP which urges the agents moves to or stays in \(S2\). The number of agents is set to 5. (b) The learning curve of the estimated joint state-action value function in the given MMDP. The true value is indicated by the dotted line, which represents the maximum discounted return. (c) The performance curve of the corresponding methods.

naive application of QMIX leads to exponentially explosion of value estimates and the sub-optimal policy performance, Fig. 1(c).

### Multi-Agent Conservative Q-Learning

When treating multiple agents as a unified single agent and only regarding the joint policy of the \(\bm{\beta},\bm{\mu},\bm{\pi}\), that is \(\bm{\beta}(\bm{a}|s)=\prod_{i=1}^{n}\beta^{i}(a^{i}|s)\), \(\bm{\mu}(\bm{a}|s)=\prod_{i=1}^{n}\mu^{i}(a^{i}|s)\) and \(\bm{\pi}(\bm{a}|s)=\prod_{i=1}^{n}\pi^{i}(a^{i}|s)\), we can extend the single-agent CQL to CTDE multi-agent setting straightforwardly and derive the conservative policy evaluation style directly as follows:

\[\hat{Q}_{k+1}^{tot}\leftarrow\operatorname*{arg\,min}_{Q}\alpha\bigg{[}\mathbb{ E}_{s\sim\mathcal{D},\bm{a}\sim\bm{\mu}}[Q(s,\bm{a})]-\mathbb{E}_{s\sim\mathcal{D}, \bm{a}\sim\bm{\beta}}[Q(s,\bm{a})]\bigg{]}+\hat{\mathcal{E}}_{\mathcal{D}}( \bm{\pi},Q,k),\] (3)

where we represent the temporal difference (TD) error concisely as \(\hat{\mathcal{E}}_{\mathcal{D}}(\bm{\pi},Q,k):=\frac{1}{2}\mathbb{E}_{s,\bm{a},s^{\prime}\sim\mathcal{D}}[(Q(s,\bm{a})-\hat{\mathcal{T}}^{\bm{\pi}}\hat{Q}_{ k}^{tot}(s,\bm{a}))^{2}]\). In the rest of the paper, we mainly focus on the global Q function of a centralized critic and omit the superscript "tot" for ease of expression.

Similar to CQL, MACQL can learn a lower-bounded Q-value with a proper \(\alpha\), then the overestimation issue in offline MARL can be mitigated. According to Theorem 3.2 in Kumar et al. [21], the degree of pessimism relies heavily on \(D_{CQL}(\bm{\pi},\bm{\beta})(s):=\sum_{\bm{a}}\bm{\pi}(\bm{a}|s)\frac{\bm{\pi} (\bm{a}|s)}{\hat{\mathcal{D}}(\bm{a}|s)}-1]\). However, as we will show in Sec. 4.3, \(D_{CQL}\) expands exponentially as the number of agents \(n\) increases, resulting in an over-pessimistic value function and a mediocre policy improvement guarantee for MACQL.

### Counterfactual Conservative Q-Learning

Intuitively, we aim to learn a value function that prevents the overestimation of the policy value, which is expected not too far away from the true value in the meantime. Similar to MACQL as introduced above, we add a regularization in the policy evaluation process, by penalizing the values for OOD actions and rewarding the values for in-distribution actions. However, rather than regarding all the agents as a single one and conducting standard CQL method in the joint policy space, in CFCQL method, each agent separately contributes regularization for the global Q value and then a weighted average is used as an overall regularization. When calculating agent \(i\)'s regularization term, instead of sampling OOD actions from the joint action space as MACQL does, we only sample OOD actions for agent \(i\) and leave other agents' actions sampled in the dataset. Specifically, using \(\bm{a}^{-i}\) to denote the joint actions other than agent \(i\). Eq. 4 is our proposed policy evaluation iteration:

\[\hat{Q}_{k+1}\leftarrow\operatorname*{arg\,min}_{Q}\alpha\bigg{[}\sum_{i=1}^ {n}\lambda_{i}\mathbb{E}_{s\sim\mathcal{D},\bm{a}^{i}\sim\mu^{i},\bm{a}^{-i} \sim\bm{\beta}^{-i}}[Q(s,\bm{a})]-\mathbb{E}_{s\sim\mathcal{D},\bm{a}\sim\bm{ \beta}}[Q(s,\bm{a})]\bigg{]}+\hat{\mathcal{E}}_{\mathcal{D}}(\bm{\pi},Q,k),\] (4)

where \(\alpha,\lambda_{i}\) are hyper-parameters, and \(\sum_{i=1}^{n}\lambda_{i}=1,\lambda_{i}\geq 0\). We refer to our method as 'counterfactual' because its structure bears resemblance to counterfactual methods in MARL [9]. This involves obtaining each agent's counterfactual baseline by marginalizing out a single agent's action while keeping the other agents' actions fixed. The intuitive rationale behind employing a counterfactual-like approach is that by individually penalizing each agent's out-of-distribution (OOD) actions while holding the others' actions constant from the datasets, we can effectively mitigate the out-of-distribution problem in offline MARL with reduced pessimism, as illustrated in the rest of this section.

The theoretical analysis is arranged as follows: In Theorem 4.1 we show the new policy evaluation leads to a milder conservatism on value function, which still lower bounds the true value. Then we compare the conservatism degree between CFCQL and MACQL in Theorem 4.2. In Theorem 4.3 and Theorem 4.4 we show the effect of milder conservatism brought by counterfactual treatments on the performance guarantee (All proofs are defered to Appendix A).

**Theorem 4.1** (Equation 4 results in a lower bound of value function).: _The value of the policy under the Q function from Equation 4, \(\hat{V}^{\bm{\pi}}(s)=\mathbb{E}_{\bm{\pi}(\bm{a}|s)}[\hat{Q}^{\bm{\pi}}(s,\bm{a })]\), lower-bounds the true value of the policy obtained via exact policy evaluation, \(V^{\bm{\pi}}(s)=\mathbb{E}_{\bm{\pi}(\bm{a}|s)}[Q^{\bm{\pi}}(s,\bm{a})]\), when \(\bm{\mu}=\bm{\pi}\), according to: \(\forall s\in\mathcal{D},\hat{V}^{\bm{\pi}}(s)\leq V^{\bm{\pi}}(s)-\alpha\left[ \left(I-\gamma P^{\bm{\pi}}\right)^{-1}\mathbb{E}_{\bm{\pi}}\left[\sum_{i=1}^{ n}\lambda_{i}\frac{\pi^{i}}{\beta^{i}}-1\right]\right](s)+\left[\left(I-\gamma P^{\bm{\pi}} \right)^{-1}\frac{C_{r,T,\delta}R_{\max}}{(1-\gamma)\sqrt{|\mathcal{D}|}} \right].\) Define \(D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s):=\sum_{\bm{a}}\bm{\pi}(\bm{a}|s)[\sum_{i=1} ^{n}\lambda_{i}\frac{\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}|s)}-1]\). If \(\alpha>\frac{C_{r,T,\delta}R_{\max}}{1-\gamma}\cdot\max_{s\in\mathcal{D}}\frac{ 1}{\sqrt{|\mathcal{D}(s)|}}\)\(D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)^{-1},\forall s\in\mathcal{D},\hat{V}^{\bm{ \pi}}(s)\leq V^{\bm{\pi}}(s)\), with \(probability\geq 1-\delta\). When \(\mathcal{T}^{\bm{\pi}}=\hat{\mathcal{T}}^{\bm{\pi}}\), then any \(\alpha>0\) guarantees \(\hat{V}^{\bm{\pi}}(s)\leq V^{\bm{\pi}}(s),\forall s\in\mathcal{D}\)._

Theorem 4.1 mainly differs from Theorem 3.2 in Kumar et al. [21] on the specific form of the divergence: \(D_{CQL}\) and \(D_{CQL}^{CF}\), both of which determine the degrees of conservatism when \(\bm{\pi}\) and \(\bm{\beta}\) are different. Empirically, we find \(D_{CQL}\) generally becomes too large when the number of agents expands, resulting in a low, even negative value function. To measure the scale difference between \(D_{CQL}(\bm{\pi},\bm{\beta})(s):=\prod_{i}(D_{CQL}(\pi^{i},\beta^{i})(s)+1)-1\) and \(D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s):=\sum_{i}\lambda_{i}D_{CQL}(\pi^{i}, \beta^{i})(s)\), we have Theorem 4.2:

**Theorem 4.2**.: \(\forall s,\bm{\pi},\bm{\beta}\)_, one has \(0\leq D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)\leq D_{CQL}(\bm{\pi},\bm{\beta})(s)\), and the following inequality holds:_

\[\frac{D_{CQL}(\bm{\pi},\bm{\beta})(s)}{D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)} \geq\exp\left(\sum_{i=1,i\neq j}^{n}KL(\pi^{i}(s)||\beta^{i}(s))\right),\] (5)

_where \(j=\arg\max_{k}\mathbb{E}_{\pi^{k}}\frac{\pi^{k}(s)}{\beta^{k}(s)}\) represents the agent whose policy distribution is the most far away from the dataset._

Since \(D_{CQL}^{CF}\) can be written as \(\sum_{i}\lambda_{i}D_{CQL}(\pi^{i},\beta^{i})(s)\), it can be regarded as a weighted average of each individual agents' policy deviations from its individual behavior policy. Therefore, the scale of \(D_{CQL}^{CF}\) is independent of the number of agents. Instead, Theorem 4.2 shows that both \(D_{CQL}\) and the ratio of \(D_{CQL}\) and \(D_{CQL}^{CF}\) explode exponentially as the number of agents \(n\) increases.

Then we discuss the influence of \(D_{CQL}^{CF}\) on the property of safe policy improvement. Define \(\hat{M}\) as the MDP induced by the transitions observed in the dataset. Let the empirical return \(J(\bm{\pi},\hat{M})\) be the discounted return for any policy \(\bm{\pi}\) in \(\hat{M}\) and \(\hat{Q}\) be the fixed point of Equation 4. Then the optimal policy of CFCQL is equivalently obtained by solving:

\[\bm{\pi}_{CF}^{*}(\bm{a}|s)\leftarrow\arg\max_{\bm{\pi}}J(\bm{\pi},\hat{M})- \alpha\frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{M}^{\bm{\pi}}(s)}[D_{CQL}^{CF}( \bm{\pi},\bm{\beta})(s)].\] (6)

See the proof in Appendix A.3. Eq. 6 can be regarded as adding a \(D_{CQL}^{CF}\)-related regularizer to \(J(\bm{\pi},\hat{M})\). Replacing \(D_{CQL}^{CF}\) with \(D_{CQL}\) in the regularizer, we can get a similar form of \(\bm{\pi}_{M}^{*}\) for MACQL. We next discuss the performance bound of CFCQL and MACQL on the true MDP \(\hat{M}\). If we assume the full coverage of \(\beta^{i}\) on \(M\), we have:

**Theorem 4.3**.: _Assume \(\forall s,a,i,\beta^{i}(a|s)\geq\epsilon\), then with probability \(\geq 1-\delta\),_

\[J(\bm{\pi}_{MA}^{*},M)\geq J(\bm{\pi}^{*},M)-\frac{\alpha}{1- \gamma}(\frac{1}{\epsilon^{n}}-1)-\text{sampling error},\] \[J(\bm{\pi}_{CF}^{*},M)\geq J(\bm{\pi}^{*},M)-\frac{\alpha}{1- \gamma}(\frac{1}{\epsilon}-1)-\text{sampling error},\]

_where \(\bm{\pi}^{*}=\arg\max_{\bm{\pi}}J(\bm{\pi},M)\), i.e. the optimal policy, and sampling error is a constant dependent on the MDP itself and \(\mathcal{D}\)._

Theorem 4.3 shows that when sampling error and \(\alpha\) are small enough, the performances gap induced by \(\bm{\pi}_{CF}^{*}\) can be small, but that induced by \(\bm{\pi}_{MA}^{*}\) expands when \(n\) increases, making \(\bm{\pi}_{MA}^{*}\) far away from the optimal. Upon examining the performance gap, we ultimately compare the safe policy improvement guarantees the two methods on \(M\):

**Theorem 4.4**.: _Assume \(\forall s,a,i,\beta^{i}(a|s)\geq\epsilon\). The policy \(\bm{\pi}_{MA}^{*}(\bm{a}|s)\) is a \(\zeta^{MA}\)-safe policy improvement over \(\bm{\beta}\) in the actual MDP \(M\), i.e., \(J(\bm{\pi}_{MA}^{*},M)\geq J(\bm{\beta},M)-\zeta^{MA}\). And the policy \(\bm{\pi}_{CF}^{*}(\bm{a}|s)\) is a \(\zeta^{CF}\)-safe policy improvement over \(\bm{\beta}\) in \(M\), i.e., \(J(\bm{\pi}_{CF}^{*},M)\geq J(\bm{\beta},M)-\zeta^{CF}\). When \(n\geq\log_{\frac{1}{\epsilon}}\left(\frac{1}{\epsilon}+\frac{2}{\alpha}\frac{ \sqrt{|A|}}{\sqrt{|\mathcal{D}(s)|}}(C_{r,\delta}+\frac{\gamma R_{\max}C_{r, \delta}}{1-\gamma})\cdot(\frac{1}{\sqrt{\epsilon}}-1)\right)\), \(\zeta^{CF}\leq\zeta^{MA}\)._

Detailed formation of \(\zeta^{MA}\) and \(\zeta^{CF}\) is provided in Appendix A.5. Theorem 4.4 shows that with a large enough agent count \(n\), CFCQL has better safe policy improvement guarantee than MACQL. The validation experiment of this theoretical result is presented in Section 5.1.

### Practical Algorithm

The fixed point of Eq. 4 provides an underestimated Q function for any policy \(\mu\). But it is computationally inefficient to solve Eq. 4 every time after one step policy update. Similar to CQL [21], we also choose a \(\bm{\mu}(\bm{a}|s)\) that would maximize the current \(\hat{Q}\) with a \(\bm{\mu}\)-regularizer. If the regularizer aims to minimize the KL divergence between \(\mu^{i}\) and a uniform distribution, \(\mu^{i}(a^{i}|s)\propto\exp\left(\mathbb{E}_{\bm{a}^{-i}\sim\bm{\beta}^{-i}}Q (s,a^{i},\bm{a}^{-i})\right)\), which results in the update rule of Eq. 7 (See Appendix B.1 for detailed derivation):

\[\min_{Q}\alpha\mathbb{E}_{s\sim\mathcal{D}}\bigg{[}\sum_{i=1}^{n}\lambda_{i} \mathbb{E}_{\bm{a}^{-i}\sim\bm{\beta}^{-i}}[\log\sum_{a^{i}}\exp(Q(s,\bm{a}))] -\mathbb{E}_{\bm{a}\sim\bm{\beta}}[Q(s,\bm{a})]\bigg{]}+\hat{\mathcal{E}}_{ \mathcal{D}}(\bm{\pi},Q).\] (7)

Finally, we need to specify each agent's weight of minimizing the policy Q function \(\lambda_{i}\). Theoretically, any simplex of \(\bm{\lambda}\) that satisfies \(\sum_{i=1}^{n}\lambda_{i}=1\) can be used to induce an underestimated value function linearly increasing as agents number as we expect. Therefore, a simple way is to set \(\lambda_{i}=\frac{1}{n},\forall i\) where each agent contributes penalty equally. Another way is to prioritize penalizing the agent that exhibits the greatest deviation from the dataset, which is the one-hot style of \(\bm{\lambda}\):

\[\lambda_{i}(s)=\left\{\begin{array}{ll}1.0,&i=\arg\max_{j}\mathbb{E}_{\pi^{ j}}\frac{\pi^{j}(s)}{\beta^{j}(s)}\\ 0.0,&others\end{array}\right.\] (8)

We assert that each agent's conservative contribution deserves to be considered and differed according to their degree of deviation. As a result, both the uniform and the one-hot treatment present some limitations. Consequently, we employ a moderate softmax variant of Eq. 8:

\[\forall i,s,\lambda_{i}(s)=\frac{\exp(\tau\mathbb{E}_{\pi^{i}}\frac{\pi^{i}(s) }{\beta^{i}(s)})}{\sum_{j=1}^{n}\exp(\tau\mathbb{E}_{\pi^{j}}\frac{\pi^{j}(s) }{\beta^{j}(s)})},\] (9)

where \(\tau\) is a predefined temperature coefficient, controlling the influence of \(\mathbb{E}_{\pi^{i}}\frac{\pi^{i}}{\beta^{i}}\) on \(\lambda_{i}\). When \(\tau\to 0\), \(\lambda_{i}\rightarrow\frac{1}{n}\), and when \(\tau\rightarrow\infty\), it turns into Eq. 8. To compute Eq. 9, we need an explicit expression of \(\pi^{i}\) and \(\beta^{i}\). In discrete action space, \(\pi^{i}\) can be estimated by \(\exp\left(\mathbb{E}_{\bm{a}^{-i}\sim\bm{\beta}^{-i}}Q(s,a^{i},\bm{a}^{-i})\right)\), and we use behavior cloning [29] to train a parameterized \(\bm{\beta}(s)\) from the dataset. In continuous action space, \(\pi^{i}\) is parameterized by each agent's local policy. For \(\beta^{i}\), we use the method of explicit estimation of behavior density in Wu et al. [48], which is modified from a VAE [17] estimator. Details for computing \(\bm{\lambda}\) are defered to Appendix B.2.

For policy improvement in continuous action space, we also take derivation of a counterfactual Q function for each agent, rather than updating all agents' policy together like in MADDPG. Specifically, the gradient of each agent \(i\)'s policy \(\pi^{i}\) is calculated by:

\[\nabla_{a^{i}}\mathbb{E}_{s,\bm{a}^{-i}\sim\mathcal{D},a^{i}\sim\pi^{i}(s)}Q _{\theta}(s,\bm{a})\] (10)

The reason is that in CFCQL, we only minimize \(Q(s,\pi^{i},\bm{\beta}^{-i})\), rather than \(Q(s,\bm{\pi})\). Using the untrained \(Q(s,\bm{\pi})\) to directly guide PI like MADDPG may result in a bad policy.

We summarize CFCQL in discrete and continuous action space in Algorithm 1 as CFCQL-D and -C, separately.

```
1:Initialize \(Q_{\theta}\), target network \(Q_{\hat{\theta}}\), target update interval \(t_{tar}\), replay buffer \(\mathcal{D}\), and optionally \(\bm{\pi}_{\psi}\) for CFCQL-C
2:for\(t=1,2,\ldots,t_{max}\)do
3: Sample \(N\) transitions \(\{s,\bm{a},s^{\prime},r\}\) from \(\mathcal{D}\)
4: Compute \(Q_{\theta}(s,\bm{a})\) using the structure of QMIX for CFCQL-D or MADDPG for CFCQL-C.
5: Calculate \(\bm{\lambda}\) according to Eq. 9
6: Update \(Q_{\theta}\) by Eq. 7 with sampled transitions. Using \(\hat{\mathcal{T}}_{\theta}\) for CFCQL-D, and \(\hat{\mathcal{T}}_{\theta}^{\pi_{\psi}t}\) for CFCQL-C
7: (Only for CFCQL-C) For each agent \(i\), take one-step policy improvement for \(\pi^{i}_{\psi}\) according to Eq. 10
8:if\(t\mod t_{tar}=0\)then
9: update target network \(\hat{\theta}\leftarrow\theta\)
10:endif
11:endfor ```

**Algorithm 1** CFCQL-D and CFCQL-C

We consider the effect of the policy improvement in continuous action space. In continuous action space, \(\pi^{i}\) can be estimated by \(\exp\left(\mathbb{E}_{\pi^{i}}\frac{\pi^{i}(s)}{\beta^{i}(s)}\right)\), and we use behavior cloning [29] to train a parameterized \(\bm{\beta}(s)\) from the dataset. In continuous action space, \(\pi^{i}\) is parameterized by each agent's local policy. For \(\beta^{i}\), we use the method of explicit estimation of behavior density in Wu et al. [48], which is modified from a VAE [17] estimator. Details for computing \(\bm{\lambda}\) are defered to Appendix B.2.

Experiments

**Baselines.** We compare our method CFCQL with several offline Multi-Agent baselines, where baselines with prefix \(MA\) adopt CTDE paradigm and the others adopt independent learning paradigm: **BC**: Behavior cloning. **TD3-BC[11]**: One of the state-of-the-art single agent offline algorithm, simply adding the BC term to TD3 [12]. **MACQL**: Naive extension of conservative Q-learning, as proposed in Sec.4.2. **MACQL[52]**: Multi-agent version of implicit constraint Q-learning by decomposed multi-agent joint-policy under implicit constraint. **OMAR[34]**: Using zeroth-order optimization for better coordination among agents' policies, based on independent CQL (**ICQL**). **MADTKD[45]**: Using decision transformer to represent each agent's policy, trained with knowledge distillation. **IQL[18]** and **AWAC[31]**: variants of advantage weighted behaviour cloning, which are SOTA on single agent offline RL. Details for baseline implementations are in Appendix C.3.

Each algorithm is run for five random seeds, and we report the mean performance with standard deviation2. Four environments are adopted to evaluate our method, including both discrete action space and continuous action space. We have relocated additional experimental results to Appendix D to conserve space.

Footnote 2: Our code and datasets are available at: https://github.com/thu-rllab/CFCQL

### Equal Line

To empirically compare CFCQL and MACQL as agents number \(n\) increases, we design a multi-agent task called \(Equal\_Line\), which is a one-dimensional simplified version of \(Equal\_Space\) introduced in Tseng et al. [45]. Details of the environment are in Appendix C.1. The \(n\) agents need to cooperate to disperse and ensure every agent is equally spaced. The datasets consist of \(1000\) trajectories sampled by executing a fully-pretrained policy of QMIX[37], i.e. \(Expert\) dataset. We plot the performance ratios of CFCQL and MACQL to the behavior policy for different agent number \(n\) in Fig.2(b). It can be observed that the performance of MACQL degrades dramatically as the increase of number of agents while the performance of CFCQL remains basically stable. The results strongly verify the conclusion we proposed in Sec.4.3, that CFCQL has better policy improvement guarantee than MACQL with a large enough number of agents \(n\).

### Multi-agent Particle Environment

In this section, we test CFCQL on Multi-agent Particle Environemnt with continuous action space. We use the dataset and the adversary agent provided by Pan et al. [34]. The performance of the trained model is measured by the normalized score \(100\times(S-S_{Random})/(S-S_{Expert})\)[10].

In Table 1, we only show the comparison of our method and the current state-of-the-art method OMAR and IQL to save space. For complete comparison with more baselines, e.g., TD3+BC and

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Env** & **Dataset** & **OMAR** & **MACQL** & **IQL** & **CFCQL** \\ \hline \multirow{4}{*}{**CN**} & Random & 34.4\(\pm\)5.3 & 45.6\(\pm\)8.7 & 5.5\(\pm\)1.1 & **62.2\(\pm\)8.1** \\  & Med-Rep & 37.9\(\pm\)12.3 & 25.5\(\pm\)5.9 & 10.8\(\pm\)4.5 & **52.2\(\pm\)9.6** \\  & Medium & 47.9\(\pm\)18.9 & 14.3\(\pm\)20.2 & 28.2\(\pm\)3.9 & **65.0\(\pm\)10.2** \\  & Expert & **114.9\(\pm\)2.6** & 12.2\(\pm\)31 & 103.7\(\pm\)2.5 & 112\(\pm\)4 \\ \hline \multirow{4}{*}{**PP**} & Random & 11.1\(\pm\)2.8 & 25.2\(\pm\)11.5 & 1.3\(\pm\)1.6 & **78.5\(\pm\)15.6** \\  & Med-Rep & 47.1\(\pm\)15.3 & 11.9\(\pm\)9.2 & 23.2\(\pm\)12 & **71.1\(\pm\)6** \\  & Medium & 66.7\(\pm\)23.2 & 55\(\pm\)43.2 & 53.6\(\pm\)19.9 & **68.5\(\pm\)21.8** \\  & Expert & 116.2\(\pm\)19.8 & 108.4\(\pm\)21.5 & 109.3\(\pm\)10.1 & **118.2\(\pm\)13.1** \\ \hline \multirow{4}{*}{**World**} & Random & 5.9\(\pm\)5.2 & 11.7\(\pm\)11 & 2.9\(\pm\)4.0 & **68\(\pm\)20.8** \\  & Med-Rep & 42.9\(\pm\)19.5 & 13.2\(\pm\)16.2 & 41.5\(\pm\)5.5 & **73.4\(\pm\)23.2** \\ \cline{1-1}  & Medium & 74.6\(\pm\)11.5 & 67.4\(\pm\)48.4 & 70.5\(\pm\)15.3 & **93.8\(\pm\)31.8** \\ \cline{1-1}  & Expert & 110.4\(\pm\)25.7 & 99.7\(\pm\)31 & 107.8\(\pm\)17.7 & **119.7\(\pm\)26.4** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on Multi-agent Particle Environment. CN: Cooperative Navigation. PP:Predator-prey. World: World.

Figure 2: (a) The \(Equal\_Line\) environment where n=3. (b) Performance ratio of CFCQL and MACQL to the behaviour policy with a varing number of agents.

[MISSING_PAGE_FAIL:9]

struggle in 6h_vs_8z, a highly challenging map. MADTKD, employing supervised learning and knowledge distillation, works well but seldom surpasses BC. IQL and AWAC are competitive baselines but they still fall short compared to CFCQL in most datasets. CFCQL significantly outperforms all baselines on most datasets, achieving state-of-the-art results, with its success attributed to moderate and appropriate conservati* [3] An, G., Moon, S., Kim, J.-H., and Song, H. O. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in Neural Information Processing Systems_, 34, 2021.
* [4] Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* [5] Buckman, J., Gelada, C., and Bellemare, M. G. The importance of pessimism in fixed-dataset policy optimization. _arXiv preprint arXiv:2009.06799_, 2020.
* [6] Cheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. Adversarially trained actor critic for offline reinforcement learning. _arXiv preprint arXiv:2202.02446_, 2022.
* [7] Claus, C. and Boutilier, C. The dynamics of reinforcement learning in cooperative multiagent systems. In _AAAI Conference on Artificial Intelligence (AAAI)_, 1998.
* [8] de Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S. Is independent learning all you need in the starcraft multi-agent challenge? _arXiv preprint arXiv:2011.09533_, 2020.
* [9] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2018.
* [10] Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [11] Fujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [12] Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pp. 1587-1596. PMLR, 2018.
* [13] Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, pp. 2052-2062. PMLR, 2019.
* [14] Jiang, J. and Lu, Z. Offline decentralized multi-agent reinforcement learning. _arXiv preprint arXiv:2108.01832_, 2021.
* [15] Jin, Y., Yang, Z., and Wang, Z. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pp. 5084-5096. PMLR, 2021.
* [16] Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. Morel: Model-based offline reinforcement learning. _arXiv preprint arXiv:2005.05951_, 2020.
* [17] Kingma, D. P. and Welling, M. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [18] Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O. Offline reinforcement learning with fisher divergence critic regularization. In _International Conference on Machine Learning_, pp. 5774-5783. PMLR, 2021.
* [19] Kuba, J. G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y. Trust region policy optimisation in multi-agent reinforcement learning. _arXiv preprint arXiv:2109.11251_, 2021.
* [20] Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* [21] Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* [22] Kumar, A., Hong, J., Singh, A., and Levine, S. Should i run offline reinforcement learning or behavioral cloning? In _Deep RL Workshop NeurIPS 2021_, 2021.
* [23] Kurach, K., Raichuk, A., Stanczyk, P., Zajac, M., Bachem, O., Espeholt, L., Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., et al. Google research football: A novel reinforcement learning environment. _arXiv preprint arXiv:1907.11180_, 2019.

* [24] Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. In _Conference on Robot Learning_, pp. 1702-1712. PMLR, 2022.
* [25] Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* [26] Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. Provably good batch reinforcement learning without great exploration. _arXiv preprint arXiv:2007.08202_, 2020.
* [27] Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. In _Advances in neural information processing systems (NeurIPS)_, 2017.
* [28] Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W., Wen, Y., Zhang, H., Wang, J., and Xu, B. Offline pre-trained multi-agent decision transformer: One big sequence model conquers all starcrafti tasks. _arXiv preprint arXiv:2112.02845_, 2021.
* [29] Michie, D., Bain, M., and Hayes-Miches, J. Cognitive models from subcognitive skills. _IEE control engineering series_, 44:71-99, 1990.
* [30] Nair, A., Dalal, M., Gupta, A., and Levine, S. Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* [31] Nair, A., Gupta, A., Dalal, M., and Levine, S. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* [32] Oliehoek, F. A., Amato, C., et al. _A concise introduction to decentralized POMDPs_. Springer, 2016.
* [33] Pan, L., Rashid, T., Peng, B., Huang, L., and Whiteson, S. Regularized softmax deep multi-agent q-learning. _Advances in Neural Information Processing Systems_, 34:1365-1377, 2021.
* [34] Pan, L., Huang, L., Ma, T., and Xu, H. Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification. In _International Conference on Machine Learning_, pp. 17221-17237. PMLR, 2022.
* [35] Peng, B., Rashid, T., Schroeder de Witt, C., Kamienny, P.-A., Torr, P., Bohmer, W., and Whiteson, S. Facmac: Factored multi-agent centralised policy gradients. _Advances in Neural Information Processing Systems_, 34:12208-12221, 2021.
* [36] Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [37] Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. _arXiv preprint arXiv:1803.11485_, 2018.
* [38] Rashid, T., Farquhar, G., Peng, B., and Whiteson, S. Weighted qmix: Expanding monotonic value function factorisation. _arXiv e-prints_, pp. arXiv-2006, 2020.
* [39] Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34, 2021.
* [40] Samvelyan, M., Rashid, T., de Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. _arXiv preprint arXiv:1902.04043_, 2019.
* [41] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [42] Son, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2019.

* [43] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V. F., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In _International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)_, 2018.
* [44] Tan, M. Multi-agent reinforcement learning: Independent vs. cooperative agents. In _International Conference on Machine Learning (ICML)_, 1993.
* [45] Tseng, W.-C., Wang, T.-H., Lin, Y.-C., and Isola, P. Offline multi-agent reinforcement learning with knowledge distillation. In _Advances in Neural Information Processing Systems_, 2022.
* [46] Wan, L., Liu, Z., Chen, X., Wang, H., and Lan, X. Greedy-based value representation for optimal coordination in multi-agent reinforcement learning. _arXiv preprint arXiv:2112.04454_, 2021.
* [47] Wang, J., Ren, Z., Liu, T., Yu, Y., and Zhang, C. Qplex: Duplex dueling multi-agent q-learning. _arXiv preprint arXiv:2008.01062_, 2020.
* [48] Wu, J., Wu, H., Qiu, Z., Wang, J., and Long, M. Supported policy optimization for offline reinforcement learning. In _Advances in Neural Information Processing Systems_, 2022.
* [49] Wu, Y., Tucker, G., and Nachum, O. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* [50] Wu, Y., Zhai, S., Srivastava, N., Susskind, J., Zhang, J., Salakhutdinov, R., and Goh, H. Uncertainty weighted actor-critic for offline reinforcement learning. _arXiv preprint arXiv:2105.08140_, 2021.
* [51] Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34, 2021.
* [52] Yang, Y., Ma, X., Li, C., Zheng, Z., Zhang, Q., Huang, G., Yang, J., and Zhao, Q. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 34:10299-10312, 2021.
* [53] Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of mappo in cooperative, multi-agent games. _arXiv preprint arXiv:2103.01955_, 2021.
* [54] Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. Mopo: Model-based offline policy optimization. _arXiv preprint arXiv:2005.13239_, 2020.
* [55] Zanette, A., Wainwright, M. J., and Brunskill, E. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34, 2021.

Detailed Proof

### Proof of Theorem 4.1

Proof.: Similar to the proof of Theorem 3.2 in Kumar et al. [21], we first prove this theorem in the absence of sampling error, and then incorporate sampling error at the end. By set the derivation of the objective in Eq. 4 to zero, we can compute the Q-function update induced in the exact, tabular setting(\(\mathcal{T}^{\bm{\pi}}=\hat{\mathcal{T}}^{\bm{\pi}}\) and \(\bm{\pi}_{\bm{\beta}}(\mathbf{a}|s)=\hat{\bm{\pi}}_{\bm{\beta}}(\mathbf{a}|s)\)).

\[\forall\;s,\bm{a},k,\;\hat{Q}^{k+1}(s,\bm{a})=\mathcal{T}^{\bm{\pi}}\hat{Q}^{k }(s,\bm{a})-\alpha\left[\Sigma_{i=1}^{n}\lambda_{i}\frac{\mu^{i}}{\pi_{\beta}^ {i}}-1\right]\] (A.1)

Then, the value of the policy, \(\hat{V}^{k+1}\) can be proved to be underestimated, since:

\[\hat{V}^{k+1}(s)=\mathbb{E}_{\bm{a}\sim\bm{\pi}(\bm{a}|s)}\left[\hat{Q}^{\bm{ \pi}}(s,\bm{a})\right]=\mathcal{T}^{\bm{\pi}}\hat{V}^{k}(s)-\alpha\mathbb{E}_ {\bm{a}\sim\bm{\pi}(\bm{a}|s)}\left[\Sigma_{i=1}^{n}\lambda_{i}\frac{\mu^{i}}{ \pi_{\beta}^{i}}-1\right]\] (A.2)

Next, we will show that \(D_{CQL}^{CF}(s)=\Sigma_{a}\bm{\pi}(\bm{a}|s)\left[\Sigma_{i=1}^{n}\lambda_{i} \frac{\mu^{i}(a^{i}|s)}{\pi_{\beta}^{i}(a^{i}|s)}-1\right]\) is always positive, when \(\mu^{i}(a^{i}|s)=\pi^{i}(a^{i}|s)\):

\[D_{CQL}^{CF}(s) =\Sigma_{a}\bm{\pi}(\bm{a}|s)\left[\Sigma_{i=1}^{n}\lambda_{i} \frac{\mu^{i}(a^{i}|s)}{\pi_{\beta}^{i}(a^{i}|s)}-1\right]\] (A.3) \[=\Sigma_{i=1}^{n}\lambda_{i}\left[\Sigma_{a^{i}}\tau^{i}(a^{i}|s) \left[\frac{\mu^{i}(a^{i}|s)}{\pi_{\beta}^{i}(a^{i}|s)}-1\right]\right]\] (A.4) \[=\Sigma_{i=1}^{n}\lambda_{i}\left[\Sigma_{a^{i}}(\pi^{i}(a^{i}|s) -\pi_{\beta}^{i}(a^{i}|s)+\pi_{\beta}^{i}(a^{i}|s))\left[\frac{\mu^{i}(a^{i}|s) }{\pi_{\beta}^{i}(a^{i}|s)}-1\right]\right]\] (A.5) \[=\Sigma_{i=1}^{n}\lambda_{i}\left[\Sigma_{a^{i}}(\pi^{i}(a^{i}|s) -\pi_{\beta}^{i}(a^{i}|s))\left[\frac{\pi^{i}(a^{i}|s)-\pi_{\beta}^{i}(a^{i}|s )}{\pi_{\beta}^{i}(a^{i}|s)}\right]+\Sigma_{a^{i}}\pi_{\beta}^{i}(a^{i}|s) \left[\frac{\mu^{i}(a^{i}|s)}{\pi_{\beta}^{i}(a^{i}|s)}-1\right]\right]\] (A.6) \[=\Sigma_{i=1}^{n}\lambda_{i}\left[\Sigma_{a^{i}}\left[\frac{(\pi^ {i}(a^{i}|s)-\pi_{\beta}^{i}(a^{i}|s))^{2}}{\pi_{\beta}^{i}(a^{i}|s)}\right]+ 0\right]\;since,\forall i,\Sigma_{a^{i}}\pi^{i}(a^{i}|s)=\Sigma_{a^{i}}\pi_{ \beta}^{i}(a^{i}|s)=1\] (A.7) \[\geq 0\] (A.8)

As shown above, the \(D_{CQL}^{CF}(s)\geq 0\), and \(D_{CQL}^{CF}(s)=0\), iff \(\pi^{i}(a^{i}|s)=\pi_{\beta}^{i}(a^{i}|s)\). This implies that each value iterate incurs some underestimation, i.e. \(\hat{V}^{k+1}(s)\leq\mathcal{T}^{\bm{\pi}}\hat{V}^{k}(s)\).

We can compute the fixed point of the recursion in Equation A.2 and get the following estimated policy value:

\[\hat{V}^{\bm{\pi}}(s)=V^{\bm{\pi}}(s)-\alpha\left[(I-\gamma P^{\bm{\pi}})^{-1} \Sigma_{a}\bm{\pi}(\bm{a}|s)\left[\Sigma_{i=1}^{n}\lambda_{i}\frac{\mu^{i}(a^{ i}|s)}{\hat{\pi}_{\beta}^{i}(a^{i}|s)}-1\right]\right](s)\] (A.9)

Because the \((I-\gamma P^{\bm{\pi}})^{-1}\) is non negative and the \(D_{CQL}^{CF}(s)\geq 0\), it's easily to prove that in the absence of sampling error, Theorem 4.1 gives a lower bound.

**Incorporating sampling error**. According to the conclusion in Kumar et al. [21], we can directly write down the result with sampling error as follows:

\[\hat{V}^{\bm{\pi}}(s)\leq V^{\bm{\pi}}(s)-\alpha\left[(I-\gamma P^{\bm{\pi}})^{- 1}\Sigma_{a}\bm{\pi}(\bm{a}|s)\left[\Sigma_{i=1}^{n}\lambda_{i}\frac{\mu^{i}( a^{i}|s)}{\hat{\pi}_{\beta}^{i}(a^{i}|s)}-1\right]\right](s)+\left[(I- \gamma P^{\bm{\pi}})^{-1}\frac{C_{r,T,\sigma}R_{max}}{(1-\gamma)\sqrt{|D|}}\right]\] (A.10)So, the statement of Theorem 4.1 with sampling error is proved. Please refer to the Sec.D.3 in Kumar et al. [21] For detailed proof. Besides, the choice of \(\alpha\) in this case to prevent overestimation is given by:

\[\alpha\geq\max_{s,\bm{a}\in D}\frac{C_{r,T,\sigma}R_{max}}{(1-\gamma)\sqrt{|D|}} \cdot\max_{s\in D}\left[\Sigma_{a}\bm{\pi}(\bm{a}|s)\left[\Sigma_{i=1}^{n} \lambda_{i}\frac{\mu^{i}(a^{i}|s)}{\tilde{\pi}_{\beta}^{i}(a^{i}|s)}-1\right] \right]^{-1}\] (A.11)

### Proof of Theorem 4.2

Proof.: According to the definition, we can get the formulation of \(D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)\) and \(D_{CQL}(\bm{\pi},\bm{\beta})(s)\) as follow:

\[D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s) =\mathbb{E}_{\bm{a}\sim\bm{\pi}(\cdot|s)}\left(\left[\sum_{i=1}^{ n}\lambda_{i}\frac{\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}|s)}\right]-1\right)\] (A.12) \[=\sum_{i=1}^{n}\lambda_{i}\left(\sum_{a^{i}}\frac{\pi^{i}(a^{i}| s)*\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}|s)}\right)-1\geq 0\] (A.13)

\[D_{CQL}(\bm{\pi},\bm{\beta})(s) =\mathbb{E}_{\bm{a}\sim\bm{\pi}(\cdot|s)}\left(\left[\frac{\bm{ \pi}(\bm{a}|s)}{\bm{\beta}(\bm{a}|s)}\right]-1\right)\] (A.14) \[=\prod_{i=1}^{n}\left(\sum_{a^{i}}\frac{\pi^{i}(a^{i}|s)*\pi^{i}( a^{i}|s)}{\beta^{i}(a^{i}|s)}\right)-1\geq 0\] (A.15)

Then, by taking the logarithm of \(D_{CQL}(\bm{\pi},\bm{\beta})(s)\), we get:

\[\ln(D_{CQL}(\bm{\pi},\bm{\beta})(s)+1)=\sum_{i=1}^{n}\ln\left(\mathbb{E}_{a^{ i}\sim\pi^{i}(\cdot|s)}\frac{\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}|s)}\right)\] (A.16)

As \(\Sigma_{i}\lambda_{i}=1\), it's obvious that

\[\ln(D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)+1)\leq\ln\left(\sum_{a ^{j}}\frac{\pi^{j}(a^{j}|s)*\pi^{j}(a^{j}|s)}{\beta^{j}(a^{j}|s)}\right), where\;j=\arg\max_{k}\mathbb{E}_{\pi^{k}}\frac{\pi^{k}}{\beta^{k}}\] (A.17)

By combining equation A.16 and inequation A.17, we get

\[\frac{D_{CQL}(\bm{\pi},\bm{\beta})(s)+1}{D_{CQL}^{CF}(\bm{\pi}, \bm{\beta})(s)+1} \geq\exp\left(\sum_{i=1,i\neq j}^{n}\ln\left(\mathbb{E}_{a^{i}\sim \pi^{i}(\cdot|s)}\frac{\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}|s)}\right)\right)\] (A.18) \[\geq\exp\left(\sum_{i=1,i\neq j}^{n}KL(\pi^{i}(s)||\beta^{i}(s)) \right),where\;j=\arg\max_{k}\mathbb{E}_{\pi^{k}}\frac{\pi^{k}}{\beta^{k}}\] (A.19)

the second inequality is derived from the Jensen's inequality. As the Kullback-Leibler Divergence is non-negative, it's obvious that \(D_{CQL}(\bm{\pi},\bm{\beta})(s)\geq D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)\), then we can simplify the left-hand side of this inequality:

\[\frac{D_{CQL}(\bm{\pi},\bm{\beta})(s)}{D_{CQL}^{CF}(\bm{\pi},\bm{\beta})(s)} \geq\exp\left(\sum_{i=1,i\neq j}^{n}KL(\pi^{i}(s)||\beta^{i}(s))\right),where \;j=\arg\max_{k}\mathbb{E}_{\pi^{k}}\frac{\pi^{k}}{\beta^{k}}\] (A.20)

### Proof of Equation 6

Proof.: Similar to the proof of Lemma D.3.1 in CQL [21], \(Q\) is obtained by solving a recursive Bellman fixed point equation in the empirical MDP \(\hat{M}\), with an altered reward, \(r(s,a)-\alpha\left[\sum_{i}\lambda_{i}\frac{\pi^{i}(a^{i}|s)}{\beta^{i}(a^{i}| s)}-1\right]\), hence the optimal policy \(\bm{\pi}^{*}(\bm{a}|s)\) obtained by optimizing the value under the CFCQL Q-function equivalently is characterized via Eq. 6. 

### Proof of Theorem 4.3

Proof.: Similar to Eq. 6, \(\bm{\pi}^{*}_{MA}\) is equivalently obtained by solving:

\[\bm{\pi}^{*}_{MA}(\bm{a}|s)\leftarrow\arg\max_{\bm{\pi}}J(\bm{\pi},\hat{M})- \alpha\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\bm{\pi}^{*}}_{\hat{M}}(s)}[D_{ CQL}(\bm{\pi},\bm{\beta})(s)].\] (A.21)

Recall that \(\forall s,\bm{\pi},\bm{\beta},D_{CQL}(\bm{\pi},\bm{\beta})(s)\geq 0\). We have

\[J(\bm{\pi}^{*}_{MA},\hat{M})\geq J(\bm{\pi}^{*}_{MA},\hat{M})-\alpha\frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\bm{\pi}^{*}_{MA}}_{\hat{M}}(s)}[D_{CQL}(\bm{\pi}^{*}_{MA},\bm{\beta}) (s)]\] (A.22) \[\geq J(\bm{\pi}^{*},\hat{M})-\alpha\frac{1}{1-\gamma}\mathbb{E}_{s\sim d ^{\bm{\pi}^{*}}_{\hat{M}}(s)}[D_{CQL}(\bm{\pi}^{*},\bm{\beta})(s)].\]

Then we give an upper bound of \(\mathbb{E}_{s\sim d^{\bm{\pi}^{*}}_{\hat{M}}(s)}[D_{CQL}(\bm{\pi}^{*},\bm{\beta })(s)]\). Due to the assumption that \(\beta^{i}\) is greater than \(\epsilon\) anywhere, we have

\[D_{CQL}(\bm{\pi},\bm{\beta})(s)= \sum_{\bm{a}}\bm{\pi}(\bm{a}|s)[\frac{\bm{\pi}(\bm{a}|s)}{\bm{ \beta}(\bm{a}|s)}-1]=\sum_{\bm{a}}\bm{\pi}(\bm{a}|s)[\frac{\bm{\pi}(\bm{a}|s)} {\prod_{i=1}^{n}\beta^{i}(a^{i}|s)}-1]\] (A.23) \[\leq \left(\frac{1}{\epsilon^{n}}\sum_{\bm{a}}\bm{\pi}(\bm{a}|s)[\bm{ \pi}(\bm{a}|s)]\right)-1\leq\frac{1}{\epsilon^{n}}-1.\]

Combining Eq. A.22 and Eq. A.23, we can get

\[J(\bm{\pi}^{*}_{MA},\hat{M})\geq J(\bm{\pi}^{*},\hat{M})-\frac{\alpha}{1- \gamma}(\frac{1}{\epsilon^{n}}-1)\] (A.24)

Recall the sampling error proved in [21] and referred to above in (A.10), we can use it to bound the performance difference for any \(\bm{\pi}\) on true and empirical MDP by

\[|J(\bm{\pi},M)-J(\bm{\pi},\hat{M})|\leq\frac{C_{r,T,\delta}R_{max}}{(1-\gamma )^{2}}\sum_{s}\frac{\rho(s)}{\sqrt{|D(s)|}},\] (A.25)

then let \(sampling\ error:=2\cdot\frac{C_{r,T,\delta}R_{max}}{(1-\gamma)^{2}}\sum_{s} \frac{\rho(s)}{\sqrt{|D(s)|}}\), and incorporate it into (A.24), we get

\[J(\bm{\pi}^{*}_{MA},M)\geq J(\bm{\pi}^{*},M)-\frac{\alpha}{1-\gamma}(\frac{1} {\epsilon^{n}}-1)-\text{sampling error}\] (A.26)

where \(sampling\ error\) is a constant dependent on the MDP itself and D. Note that during the proof we do not take advantage of the nature of \(\bm{\pi}^{*}\). Actually \(\bm{\pi}^{*}\) can be replaced by any policy \(\bm{\pi}\). The reason we use \(\bm{\pi}^{*}\) is that it can give that largest lower bound, resulting in the best policy improvement guarantee. Similarly, \(D^{CF}_{CQL}\) can be bounded by \(\frac{1}{\epsilon}-1\):

\[D^{CF}_{CQL}(\bm{\pi},\bm{\beta})(s)= \sum_{i=1}^{n}\lambda_{i}\sum_{a^{i}}\pi^{i}(a^{i}|s)[\frac{\pi^{i }(a^{i}|s)}{\beta^{i}(a^{i}|s)}-1]\] (A.27) \[\leq \left(\frac{1}{\epsilon}\sum_{i=1}^{n}\lambda_{i}\sum_{a^{i}}\pi ^{i}(a^{i}|s)[\pi^{i}(a^{i}|s)]\right)-1\] \[\leq \frac{1}{\epsilon}\left(\sum_{i=1}^{n}\lambda_{i}\right)-1=\frac{ 1}{\epsilon}-1.\]

### Proof of Theorem 4.4

We first show the theorem of safe policy improvement guarantee for MACQL and CFCQL, separately. Then we compare these two gaps.

MACQL has a safe policy improvement guarantee related to the number of agents \(n\):

**Theorem A.1**.: _Given the discounted marginal state-distribution \(d_{\hat{M}}^{\bm{\pi}}\), we define \(\mathcal{B}(\bm{\pi},D)=\mathbb{E}_{s\sim d_{\hat{M}}^{\bm{\pi}}}[\sqrt{D\left( \bm{\pi},\bm{\beta}\right)(s)+1}]\). The policy \(\bm{\pi}_{MA}^{*}(\bm{a}|s)\) is a \(\zeta^{MA}\)-safe policy improvement over \(\bm{\beta}\) in the actual MDP \(M\), i.e., \(J(\bm{\pi}_{MA}^{*},M)\geq J(\bm{\beta},M)-\zeta^{MA}\), where \(\zeta^{MA}=2\left(\frac{C_{r,\delta}}{1-\gamma}+\frac{\gamma R_{\text{max}}C_{ T,\delta}}{(1-\gamma)^{2}}\right)\cdot\frac{\sqrt{|A|}}{\sqrt{|\mathcal{D}(s)|}} \mathcal{B}(\bm{\pi}_{MA}^{*},D_{CQL})+\frac{\alpha}{1-\gamma}(\frac{1}{ \epsilon^{n}}-1)-(J(\bm{\pi}^{*},\hat{M})-J(\hat{\bm{\beta}},\hat{M}))\)._

Proof.: We can first get a \(J(\bm{\pi}_{MA}^{*},\hat{M})\)-related policy improvement guarantee following the proof of Theorem 3.6 in Kumar et al. [21]:

\[J(\bm{\pi}_{MA}^{*},M)\geq J(\bm{\beta},M)-\left(2\left(\frac{C_{r,\delta}}{1-\gamma}+\frac{ \gamma R_{\text{max}}C_{T,\delta}}{(1-\gamma)^{2}}\right)\cdot\frac{\sqrt{|A|} }{\sqrt{|\mathcal{D}(s)|}}\mathcal{B}(\bm{\pi}_{MA}^{*},D_{CQL})\right.\] (A.28) \[-\left(J(\bm{\pi}_{MA}^{*},\hat{M})-J(\hat{\bm{\beta}},\hat{M}) \right)\right)\]

According to Eq. A.21, \(\bm{\pi}_{MA}^{*}\) is obtained by optimizing \(J(\bm{\pi},\hat{M})\) with a \(D_{CQL}\)-related regularizer. And Theorem 4.3 shows that \(D_{CQL}\) can be extremely large when the team size expands, which may severely change the optimization objective and affects the shape of the optimization plane. Therefore, \(J(\bm{\pi}_{MA}^{*},\hat{M})\) may be extremely low, and keeping \(J(\bm{\pi}_{MA}^{*},\hat{M})\) in Eq. A.28 results in a mediocre policy improvement guarantee. To bound \(J(\bm{\pi}_{MA}^{*},\hat{M})\), we introduce Eq. A.24 into Eq. A.28, we get the following:

\[J(\bm{\pi}_{MA}^{*},M)\geq J(\bm{\beta},M)-\left(2\left(\frac{C_{r,\delta}}{1-\gamma}+\frac{ \gamma R_{\text{max}}C_{T,\delta}}{(1-\gamma)^{2}}\right)\cdot\frac{\sqrt{|A| }}{\sqrt{|\mathcal{D}(s)|}}\mathcal{B}(\bm{\pi}_{MA}^{*},D_{CQL})\right.\] (A.29) \[+\frac{\alpha}{1-\gamma}(\frac{1}{\epsilon^{n}}-1)-(J(\bm{\pi}^{ *},\hat{M})-J(\hat{\bm{\beta}},\hat{M}))\right)\]

This complete the proof. 

We can get a similar \(\zeta^{CF}\) satisfying \(J(\bm{\pi}_{CF}^{*},M)\geq J(\bm{\beta},M)-\zeta^{CF}\) for CFCQL, which is independent of \(n\):

\[\zeta^{CF}=2\left(\frac{C_{r,\delta}}{1-\gamma}+\frac{\gamma R_{\text{max}}C_ {T,\delta}}{(1-\gamma)^{2}}\right)\!\cdot\!\frac{\sqrt{|A|}}{\sqrt{|\mathcal{D }(s)|}}\mathcal{B}(\bm{\pi}_{CF}^{*},D_{CQL}^{CF})+\frac{\alpha}{1-\gamma}( \frac{1}{\epsilon}-1)-(J(\bm{\pi}^{*},\hat{M})-J(\hat{\bm{\beta}},\hat{M}))\] (A.30)

Then we can prove Theorem 4.4.

Proof.: Subtract \(\zeta^{CF}\) from \(\zeta^{MA}\), and we get:

\[\zeta^{MA}-\zeta^{CF}=2\left(\frac{C_{r,\delta}}{1-\gamma}+\frac{\gamma R_{ \text{max}}C_{T,\delta}}{(1-\gamma)^{2}}\right)\frac{\sqrt{|A|}}{\sqrt{| \mathcal{D}(s)|}}\left(\mathcal{B}(\bm{\pi}_{MA}^{*},D_{CQL})-\mathcal{B}(\bm {\pi}_{CF}^{*},D_{CQL}^{CF})\right)+\frac{\alpha}{1-\gamma}(\frac{1}{\epsilon ^{n}}-\frac{1}{\epsilon})\] (A.31)

Let the right side \(\geq 0\), and we can get

\[n\geq\log_{\frac{1}{\epsilon}}\left[\max\left(1,\frac{1}{\epsilon}+\frac{2}{ \alpha}\frac{\sqrt{|A|}}{\sqrt{|\mathcal{D}(s)|}}\left(C_{r,\delta}+\frac{ \gamma R_{\text{max}}C_{T,\delta}}{1-\gamma}\right)\cdot\left[\mathcal{B}\left( \bm{\pi}_{CF}^{*},D_{CQL}^{CF}\right)-\mathcal{B}\left(\bm{\pi}_{MA}^{*},D_{CQL }\right)\right]\right)\right]\] (A.32)

According to Theorem 4.3,

\[\mathcal{B}\left(\bm{\pi}_{CF}^{*},D_{CQL}^{CF}\right)=\mathbb{E}_{s\sim d_{ \hat{M}}^{*}}[\sqrt{D_{CQL}^{CF}(\bm{\pi}_{CF}^{*},\bm{\beta})(s)+1}]\leq \mathbb{E}_{s\sim d_{\hat{M}}^{*}}[\sqrt{\frac{1}{\epsilon}-1+1}]=\frac{1}{ \sqrt{\epsilon}}\] (A.33)In the meantime, we have

\[\mathcal{B}\left(\bm{\pi}_{CF}^{*},D_{CQL}^{CF}\right)=\mathbb{E}_{s\sim d_{M}^{ \bm{\pi}_{MA}^{*}}}[\sqrt{D_{CQL}(\bm{\pi}_{MA}^{*},\bm{\beta})(s)+1}]\geq \mathbb{E}_{s\sim d_{M}^{\bm{\pi}_{MA}^{*}}}[\sqrt{D_{CQL}(\bm{\beta},\bm{\beta}) (s)+1}]=1\] (A.34)

Therefore, we can relax the lower bound of \(n\) to a constant that

\[n\geq\log_{\frac{1}{\epsilon}}\left(\frac{1}{\epsilon}+\frac{2}{\alpha}\frac{ \sqrt{|A|}}{\sqrt{|\mathcal{D}(s)|}}(C_{r,\delta}+\frac{\gamma R_{\max}C_{T, \delta}}{1-\gamma})\cdot(\frac{1}{\sqrt{\epsilon}}-1)\right)\] (A.35)

## Appendix B Implement Details

### Derivation of the Update Rule

To utilize the Eq. 4 for policy optimization, following the analysis in the Section 3.2 in Kumar et al. [21], we formally define optimization problems over each \(\mu^{i}(a^{i}|s)\) by adding a regularizer \(R(\mu^{i})\). As shown below, we mark the modifications from the Eq. 4 in red.

\[\begin{split}\min_{Q}\max_{\bm{\mu}}\alpha\bigg{[}& \sum_{i=1}^{n}\lambda_{i}\mathbb{E}_{s\sim\mathcal{D},a^{i}\sim\mu^{i},\bm{a} ^{-i}\sim\bm{\beta}^{-i}}[Q(s,\bm{a})]-\mathbb{E}_{s\sim\mathcal{D},\bm{a} \sim\bm{\beta}}[Q(s,\bm{a})]\bigg{]}\\ &\qquad+\frac{1}{2}\mathbb{E}_{s,\bm{a},s^{\prime}\sim\mathcal{D} }\bigg{[}(Q(s,\bm{a})-\hat{\mathcal{T}}^{\bm{\pi}}\hat{Q}_{k}(s,\bm{a}))^{2} \bigg{]}+\sum_{i=1}^{n}\lambda_{i}R(\mu^{i}),\end{split}\] (B.36)

By choosing different regularizer, there are a variety of instances within CQL family. As recommended in Kumar et al. [21], we choose \(R(\mu^{i})\) to be the KL-divergence against a Uniform distribution over action space, i.e., \(R(\mu^{i})=-D_{KL}(\mu^{i},Unif(a^{i}))\). Then we can get the following objective for \(\mu^{i}\):

\[\max_{\mu^{i}}\mathbb{E}_{x\sim\mu^{i}(x)}[f(x)]+\mathcal{H}(\mu^{i}),\quad s.t.\ \sum_{x}\mu^{i}(x)=1,\mu^{i}(x)\geq 0,\forall x,\] (B.37)

where \(\forall s,f(x)=Q(s,x,\bm{a}^{-i})\). The optimal solution is:

\[\mu^{i*}(x)=\frac{1}{Z}\exp(f(x)),\] (B.38)

where \(Z\) is the normalization factor, i.e., \(Z=\sum_{x}\exp(f(x))\). Plugging this back into Eq. B.36, we get:

\[\begin{split}\min_{Q}\ \alpha\mathbb{E}_{s\sim\mathcal{D}}\bigg{[}& \sum_{i=1}^{n}\lambda_{i}\mathbb{E}_{\bm{a}^{-i}\sim\bm{\beta}^{-i}}[\log\sum_ {a^{i}}\exp(Q(s,\bm{a}))]-\mathbb{E}_{\bm{a}\sim\bm{\beta}}[Q(s,\bm{a})]\bigg{]} \\ &\qquad+\frac{1}{2}\mathbb{E}_{s,\bm{a},s^{\prime}\sim\mathcal{D} }\bigg{[}(Q(s,\bm{a})-\hat{\mathcal{T}}^{\bm{\pi}_{k}}\hat{Q}_{k}(s,\bm{a}))^ {2}\bigg{]}.\end{split}\] (B.39)

### Details for Computing \(\lambda\)

To compute \(\lambda\), we need an explicit expression of \(\pi^{i}\) and \(\beta^{i}\). In the setting of discrete action space, as we use Q-learning, \(\pi^{i}\) can be expressed by the Boltzman policy, i.e.

\[\pi^{i}(a^{i}_{j})=\frac{\exp\big{(}\mathbb{E}_{\bm{a}^{-i}\sim\bm{\beta}^{-i}} Q(s,a^{i}_{j},\bm{a}^{-i})\big{)}}{\sum_{k}\exp\big{(}\mathbb{E}_{\bm{a}^{-i}\sim \bm{\beta}^{-i}}Q(s,a^{i}_{k},\bm{a}^{-i})\big{)}}\] (B.40)

We use behaviour cloning to pre-train a parameterized \(\bm{\beta}(s)\) with a three-level fully-connected network and MLE(Maximum Likelihood Estimation) loss.

With the explicit expression of \(\pi^{i}\) and \(\beta^{i}\), we can directly compute \(\lambda\) with Eq. 8 and Eq. 9. While, in practice, we find the \(\mathbb{E}_{\pi^{i}}\frac{\pi^{i}(s)}{\beta^{i}(s)}\) may introduce extreme variance as its large scale and fluctuations,which will hurt the performance. Instead, we take the logarithm of it and further reduced it to the Kullback-Leibler Divergence as follow:

\[\forall i,s,\lambda_{i}(s)=\frac{\exp\left(-\tau D_{KL}(\pi^{i}(s)||\beta^{i}(s)) \right)}{\sum_{j=1}^{n}\exp\left(-\tau D_{KL}(\pi^{j}(s)||\beta^{j}(s))\right)},\] (B.41)

For continuous action space, we use the deterministic policy like in MADDPG, whose policy distribution can be regared as a Dirac delta function. Therefore, we approximate \(\mathbb{E}_{\pi^{j}}\frac{\pi^{j}(s)}{\beta^{j}(s)}\) by the following:

\[\mathbb{E}_{\pi^{j}}\frac{\pi^{j}(s)}{\beta^{j}(s)}\approx\frac{1}{\beta^{j}( \pi^{j}(s)|s)}\] (B.42)

Then we need to obtain an explicit expression of \(\beta^{i}\). We first train a VAE [17] from the dataset to obtain the lower bound of \(\beta^{i}\). Let \(p_{\phi}(a,z|s)\) and \(q_{\varphi}(z|a,s)\) be the decoder and the encoder of the trained VAE, respectively. According to Wu et al. [48], \(\beta^{j}(a^{j}|s)\) can be explicitly estimated by (We omit the superscript \(j\) for brevity):

\[\begin{split}\log\beta_{\phi}(a\mid s)&=\log \mathbb{E}_{q_{\varphi}(z|a,s)}\left[\frac{p_{\phi}(a,z\mid s)}{q_{\varphi}(z \mid a,s)}\right]\\ &\approx\mathbb{E}_{z^{(l)}q_{\varphi}(z|a,s)}\left[\log\frac{1} {L}\sum_{l=1}^{L}\frac{p_{\phi}\left(a,z^{(l)}\mid s\right)}{q_{\varphi}\left( z^{(l)}\mid a,s\right)}\right]\\ &\stackrel{{\text{def}}}{{=}}\widetilde{\log\pi_{ \beta}(a\mid s;\varphi,\phi,L)}.\end{split}\] (B.43)

Therefore, we can sample from the VAE \(L\) times to estimate \(\beta^{i}\). The sampling error reduces as \(L\) increases.

## Appendix C Experimental Details

### Tasks

\(Equal\_Line\) is a multi-agent task which we design by simplify the space shape of \(Equal\_Space\) to one-dimension. There are \(n\) agents and they are randomly initialized to the interval \([0,2]\). The state space is a a one-dimensional bounded region in \([0,\max(10,2*n)]\) and the local action space is a discrete, eleven-dimensional space, i.e. \([0,-0.01,-0.05,-0.1,-0.5,-1,0.01,0.05,0.1,0.5,1]\), which represents the moving direction and distance at each step. The reward is shared by the agents and formulated as \(10*(n-1)\frac{min\_dis-last\_step\_min\_dis}{line\_length}\), which will spur the agents to cooperate to spread out and keep the same distance between each other.

For Multi-agent Particle Environment and Multi-agent Mujoco, we adopt the open-source implementations from Lowe et al. [27]3 and Peng et al. [35]4 respectively. And we use the datasets and the adversary agents provided by Pan et al. [34].

Footnote 3: https://github.com/openai/multiagent-particle-envs

Footnote 4: https://github.com/schroederdewitt/multiagent_mujoco

For StarCraft II Micromanagement Benchmark, we use the open-source implementation from Samvelyan et al. [40]5 and choose four maps with different difficulty and number of agents as the experimental scenarios, which is summarized in Table 4. We construct our own datasets with QMIX [37] by collecting training or evaluating data.

Footnote 5: https://github.com/oxwhirl/smac

### StarCraft II datasets collection

The datasets are made based on the training process or trained model of QMIX[37]. Specially, the \(Medium\) or \(Expert\) datasets are sampled by executing a partially-pretrained policy with a medium performance level or a fully-pretrained policy. The \(Medium-Replay\) datasets are exactly the replay buffer during training until the policy reaches the medium performance. The \(Mixed\) datasets are the equal mixture of \(Medium\) and \(Expert\) datasets. All datasets contain five thousand trajectories, except for the \(Medium-Replay\).

### Baselines

**BC**: behavior cloning. In discrete action space, we train a three-level MLP network with MLE loss. In continuous action space, we use the method of explicit estimation of behavior density in Wu et al. [48], which is modified from a VAE [17] estimator. **TD3-BC[11]**: One of the SOTA single agent offline algorithm, simply adding the BC term to TD3 [12]. We use the open-source implementation6 and modify it to a CTDE version with centralised critic. **IQL[18]** and **AWAC[31]**: variants of advantage weighted behaviour cloning. We refer to the open-source implementation7 and implement a CTDE version similar to TD3-BC. **MACQL**:naive extension of conservative Q-learning, as proposed in Sec. 3.3. We implement it based on the open-source implementation8. As the joint action space is enormous, we sample \(N\) actions for the logsumexp operation. **MAICQ[52]**:multi-agent version of implicit constraint Q-learning by propose the decomposed multi-agent joint-policy under implicit constraint. We use the open-source implementation9 in discrete action space and cite the experimental results in continuous action space from Pan et al. [34]. **OMAR[34]**:uses zeroth-order optimization for better coordination among agents' policies, based on independent CQL (**ICQL**). We cite the experimental results in continuous action space from Pan et al. [34] and implement a version in discrete action space based on the open-source implementation10. **MADTKD[45]**:uses decision transformer to represent each agent's policy and trains with knowledge distillation. As lack of open-source implementation, We implement it based on the open-source implementation11 of another Decision Transformer based method **MADT[28]**.

Footnote 6: https://github.com/sfujim/TD3_BC

Footnote 7: https://github.com/tinkoff-ai/CORL

Footnote 8: https://github.com/aviralkumar2907/CQL

Footnote 9: https://github.com/YiqinYang/ICQ

Footnote 10: https://github.com/ling-pan/OMAR

Footnote 11: https://github.com/ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer

### Resources

We use \(2\) servers to run all the experiments. Each one has 8*NVIDIA RTX 3090 GPUs, and 2*AMD 7H12 CPUs. Each setting is repeated for \(5\) seeds. For one seed in SC2, it takes about \(1.5\) hours. For MPE, \(10\) minutes is enough. The experiments on MaMuJoCo cost the most, about \(5\) hours for each seed.

### Code, Hyper-parameters and Reproducibility

Please refer to this repository12 for the code, datasets and the hyper-parameters of our method. For each dataset number \(0,1,2,3,4\), we use the seed \(0,1,2,3,4\), respectively.

Footnote 12: https://github.com/tuh-rllab/CFCQL

## Appendix D More results

### Complete Results on MPE

Table 5 shows the complete results of our methods and more baselines on Multi-agent Particle Environment. Some results are cited from Pan et al. [34].

\begin{table}
\begin{tabular}{l l l l} \hline
**Maps** & **Agents** & **Enemies** & **Difficulty** \\ \hline
2s3z & 2 Stalkers \& 3 Zealots & 2 Stalkers \& 3 Zealots & Easy \\
3s\_vs\_5z & 3 Stalkers & 5 Zealots & Easy \\
5m\_vs\_6m & 5 Marines & 6 Marines & Hard \\
6h\_vs\_8z & 6 Hydralisks & 8 Zealots & Super Hard \\ \hline \end{tabular}
\end{table}
Table 4: The details of tested maps in the StarCraft II micromanagement benchmark

### Temperature Coefficient in Continuous Action Space

We carry out ablations of \(\tau\) on MPE's map World in Fig. 4. We find that although. the best \(\tau\) differs in different datasets, the overall performance is not sensitive to \(\tau\), which verifies the theoretical analysis that any simplex of \(\lambda\) that \(\sum_{i=1}^{h}\lambda_{i}=1\) can induce an underestimated value function.

### Ablation on CQL \(\alpha\)

We carry out ablations of \(\alpha\) on MPE's map World in Fig. 5. We find that \(\alpha\) plays a more important role for team performance on narrow distributions (e.g., \(Expert\) and \(Medium\)) than that on wide distributions (e.g., \(Random\) and \(Medium-Replay\)).

### Component Analysis on Counterfactual style

In the environment MaMuJo, except for the counterfactual Q function, we also analyze whether the counterfactual treatment in CFCQL can be incorporated in other components and help further improvement in Table 6. We find that the counterfactual policy improvement is critical for this

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Env** & **Dataset** & **MAICQ** & **MATD3-BC** & **ICQL** & **OMAR** & **MACQL** & **IQL** & **AWAC** & **CFCQL** \\ \hline \multirow{4}{*}{**CN**} & Random & 6.3\(\pm\)3.5 & 9.8\(\pm\)4.9 & 24.0\(\pm\)9.8 & 34.4\(\pm\)5.3 & 45.6\(\pm\)8.7 & 5.5\(\pm\)1.1 & 0.5\(\pm\)3.7 & **62.2\(\pm\)8.1** \\  & Medium-replay & 13.6\(\pm\)5.7 & 15.4\(\pm\)5.6 & 20.0\(\pm\)8.4 & 37.9\(\pm\)12.3 & 25.5\(\pm\)5.9 & 10.8\(\pm\)4.5 & 2.7\(\pm\)3 & **52.2\(\pm\)9.6** \\  & Medium & 29.3\(\pm\)5.5 & 29.3\(\pm\)4.8 & 34.1\(\pm\)2.7 & 47.9\(\pm\)18.9 & 14.3\(\pm\)20.2 & 28.2\(\pm\)3.9 & 25.7\(\pm\)4.1 & **65.0\(\pm\)10.2** \\  & Expert & 10.0\(\pm\)2.3 & 108.3\(\pm\)3.3 & 98.2\(\pm\)25.2 & **11.94\(\pm\)26.6** & 12.2\(\pm\)3.1 & 10.7\(\pm\)2.5 & 103.3\(\pm\)3.5 & 112\(\pm\)4 \\ \hline \multirow{4}{*}{**PP**} & Random & 2.2\(\pm\)2.6 & 5.7\(\pm\)3.5 & 5.0\(\pm\)8.2 & 11.1\(\pm\)2.8 & 25.2\(\pm\)11.5 & 1.3\(\pm\)1.6 & 0.2\(\pm\)1.0 & **78.5\(\pm\)15.6** \\  & Medium-replay & 34.5\(\pm\)27.8 & 28.7\(\pm\)20.9 & 24.8\(\pm\)17.3 & 47.1\(\pm\)15.3 & 11.9\(\pm\)9.2 & 23.2\(\pm\)12 & 8.3\(\pm\)5.3 & **71.1\(\pm\)6** \\  & Medium & 63.3\(\pm\)20.0 & 65.1\(\pm\)29.5 & 61.7\(\pm\)23.1 & 66.7\(\pm\)23.2 & 55\(\pm\)43.2 & 53.6\(\pm\)19.9 & 50.9\(\pm\)19.0 & **68.5\(\pm\)21.8** \\  & Expert & 113.0\(\pm\)14.4 & 115.2\(\pm\)12.5 & 93.9\(\pm\)14.0 & 116.2\(\pm\)19.8 & 108.4\(\pm\)21.5 & 109.3\(\pm\)10.1 & 106.5\(\pm\)10.1 & **118.2\(\pm\)13.1** \\ \hline \multirow{4}{*}{**World**} & Random & 1.0\(\pm\)3.2 & 2.8\(\pm\)5.5 & 0.6\(\pm\)2.0 & 5.9\(\pm\)52.2 & 11.7\(\pm\)11.1 & 2.9\(\pm\)4.0 & -2.4\(\pm\)2.0 & **68.2\(\pm\)0.8** \\  & Medium-replay & 12.0\(\pm\)9.1 & 17.4\(\pm\)8.1 & 29.6\(\pm\)13.8 & 42.9\(\pm\)19.5 & 13.2\(\pm\)16.2 & 41.5\(\pm\)9.5 & 8.9\(\pm\)5.1 & **73.4\(\pm\)23.2** \\ \cline{1-1}  & Medium & 71.9\(\pm\)20.0 & 73.4\(\pm\)9.3 & 58.6\(\pm\)11.2 & 74.6\(\pm\)11.5 & 67.4\(\pm\)48.4 & 70.5\(\pm\)15.3 & 63.9\(\pm\)14.2 & **93.8\(\pm\)31.8** \\ \cline{1-1}  & Expert & 109.5\(\pm\)22.8 & 110.3\(\pm\)21.3 & 71.9\(\pm\)28.1 & 110.4\(\pm\)25.7 & 99.7\(\pm\)31 & 107.8\(\pm\)17.7 & 107.6\(\pm\)15.6 & **119.7\(\pm\)26.4** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Complete results on Multi-agent Particle Environment.

Figure 4: Ablations of \(\tau\) on World.

Figure 5: Ablations of \(\alpha\) on World.

environment. With CF_P, the method shows great performance gain on narrow data distribution, e.g., the \(Expert\) dataset.

### Analysis on Size of Dataset

An additional study examines dataset size effects. We generate a full dataset with \(50,000\) trajectories for each type in map \(6h\_vs\_8z\), creating smaller datasets by sampling \(5,50,500,5000\) trajectories. Fig. 6 displays CFCQL's testing winning rate for varying dataset sizes. It's notable that to ensure fairness, the maximum number of training steps for all datasets and algorithms on the SMAC environment is fixed at 1e7. In this additional study, however, we trained the CFCQL algorithm until convergence to eliminate the impact of underfitting. The results demonstrate that larger datasets contribute to improved convergence performances, thus confirming the scalability of CFCQL for larger data samples.

## Appendix E Discussion

### Overestimation in Offline RL

We offer an intuitive explanation for the phenomenon of overestimation caused by distribution shift in this section. For a rigorous proof, we refer readers to the related works.

In offline RL, a key challenge arises due to the distribution mismatch between the behavior policy--the policy responsible for generating the data--and the target policy, which is the policy one aims to improve. This mismatch can result in extrapolation errors during value function estimation, often leading to overestimation. Specifically, during the policy evaluation stage, the dataset may not encompass all possible state-action pairs in the Markov Decision Process (MDP), leading to inaccurate \(Q\) function estimates for unseen state-action pairs. These estimates may be either too high or too low compared to the actual \(Q\) values. Subsequently, in the policy improvement stage, the algorithm tends to shift towards actions that appear to offer higher rewards based on these overestimated values. Unlike online RL, which can implement the current training policy in the environment to obtain real feedback and thereby correct the policy's direction, offline RL lacks this corrective mechanism unless careful loss design is employed. When using common bootstrapping methods like temporal difference learning for training the \(Q\) function, these overestimation errors can propagate, affecting estimates

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Dataset & Default & +CF\_T & -CF\_P & MACQL \\ \hline Random & 39.7\(\pm\)4.0 & **48.7\(\pm\)1.8** & 23.9\(\pm\)9.2 & 5.3\(\pm\)0.5 \\ Med-Rep & **59.5\(\pm\)8.2** & 58.9\(\pm\)9.6 & 43.5\(\pm\)5.6 & 36.7\(\pm\)7.1 \\ Medium & **80.5\(\pm\)9.6** & 76.2\(\pm\)12.1 & 43.8\(\pm\)7.8 & 51.5\(\pm\)26.7 \\ Expert & **118.5\(\pm\)4.9** & 118.1\(\pm\)6.9 & 3.7\(\pm\)3.1 & 50.1\(\pm\)20.1 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Component Analysis on MaMuJoCo. CF_T: computing target Q by \(\mathbb{E}_{i\sim\text{Unif}(1,n)}\mathbb{E}_{i^{\prime},\bm{a}^{-1}\sim \mathcal{D},a^{1}\sim\pi^{\prime}}Q_{\hat{\theta}}(s,\bm{a})\). CF_P: the policy improvement (PI) by Eq. 10, otherwise using MADDPG’s PI.

Figure 6: Hyperparameters examination on the size of datasets.

for other state-action pairs. This can result in a chain reaction of overestimations, potentially causing an exponential explosion of the \(Q\) function.