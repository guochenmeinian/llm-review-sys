# The Best of Both Worlds: On the Dilemma of

Out-of-distribution Detection

 Qingyang Zhang\({}^{1}\), Qiuxuan Feng\({}^{1}\), Joey Tianyi Zhou\({}^{2}\), Yatao Bian\({}^{3}\),

Qinghua Hu\({}^{1}\), Changqing Zhang\({}^{1}\)

College of Intelligence and Computing, Tianjin University\({}^{1}\)

A*STAR\({}^{2}\), Tencent AI Lab\({}^{3}\)

Work done during an internship at Tencent AI Lab.Correspondence to Changqing Zhang <zhangchangqing@tju.edu.cn>

###### Abstract

Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identify semantic OOD samples and robustly generalize for covariate-shifted OOD samples. However, we discover that the superior OOD detection performance of state-of-the-art methods is achieved by secretly sacrificing the OOD generalization ability. Specifically, the classification accuracy of these models could deteriorate dramatically when they encounter even minor noise. This phenomenon contradicts the goal of model trustworthiness and severely restricts their applicability in real-world scenarios. What is the hidden reason behind such a limitation? In this work, we theoretically demystify the "_sensitive-robust_" dilemma that lies in many existing OOD detection methods. Consequently, a theory-inspired algorithm is induced to overcome such a dilemma. By decoupling the uncertainty learning objective from a Bayesian perspective, the conflict between OOD detection and OOD generalization is naturally harmonized and a dual-optimal performance could be expected. Empirical studies show that our method achieves superior performance on standard benchmarks. To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without compromising OOD generalization ability. Our code is available at https://github.com/QingyangZhang/DUL.

## 1 Introduction

Endowing machine learning models with out-of-distribution (OOD) detection and OOD generalization ability are both essential for their deployment in the open world [1, 2, 3]. We borrow an example of autonomous driving from [4] to demonstrate the motivation of these two tasks. Given a machine learning model trained on in-distribution (ID) data (top image in Fig. 1 (a)), OOD detection aims to sensitively perceive uncertainty arising upon outliers that do not belong to any known classes of training data [5] (bottom right image in Fig. 1 (a)). While OOD generalization expects machine learning models to be robust in the presence of unexpected noise or corruption, e.g., rainy or snowy weather (bottom left image in Fig. 1 (a)). In this paper, we reveal that many previous methods pursue OOD detection performance at a secret cost of sacrificing OOD generalization ability. To make things worse, we observe that some SOTA OOD detection methods may result in a catastrophic collapse in classification performance (\(\sim\)15% accuracy degradation) when encountering even slight noise. One pioneering work [4] makes a trade-off between OOD detection and OOD generalization, but the relationship between these two tasks is still largely unexplored. The learning objectives of these two tasks are seemingly conflicting at first glance. OOD detection encourages sensitive uncertainty awareness (highly uncertain prediction) on unseen data, while generalization expects the prediction to be confident and robust under unforeseeable distributional shifts. Previous work in OOD detectionresearch area [4] characterizes the relationship between OOD detection and OOD generalization as a trade-off and thus striking for a balanced performance. However, this trade-off significantly limits the employment of current state-of-the-art OOD detection methods. Naturally, one might require the model to be aware of the OOD input for ensuring safety, but certainly does not expect to sacrifice the generalization ability, not to mention that the catastrophically collapsed classification performance under noise or corruption.

In this work, we first uncover the potential reason behind this limitation by characterizing the generalization error lower bound of previous OOD detection methods, which is referred to _sensitive-robust dilemma_. To overcome the dilemma, we devise a novel Decoupled Uncertainty Learning (DUL) framework for dual-optimal performance. The decoupled uncertainties are separately responsible for characterizing semantic OOD (detection) and covariate-shifted OOD (generalization). Thanks to the decoupled uncertainty learning objective, dual-optimal OOD detection and OOD generalization performance could be expected. Our emphasis lies on a particular category of OOD detection methods in the classification task, including max softmax probability (MSP) based model [6], energy-based model (EBM) [8] and Bayesian methods [9]. This selection offers two-fold advantages. First, MSP, EBM and Bayesian detectors encompass major OOD detection advances in classification task [5]. Second, numerous OOD detection works in diverse learning tasks (classification, object detection [12], time-series prediction and image segmentation) are all roughly related to classification [13]. The contributions of this paper are summarized as follows:

* This paper reveals that existing SOTA OOD detection methods may suffer from catastrophic degradation in terms of OOD generalization. That is, their superior OOD detection ability is achieved by (secretly) sacrificing OOD generalization ability. We theoretically demystify the sensitive-robust dilemma in learning objectives as the main reason behind such a limitation.
* In contrast to previous works that characterize OOD detection and generalization as conflictive learning tasks and thus implying an inevitable trade-off, we propose a novel learning framework termed Decoupled Uncertainty Learning (DUL) to successfully break through the limitation beyond a simple trade-off. Our DUL substantially harmonizes the conflict between OOD detection and OOD generalization, which achieves the best OOD detection performance without sacrificing the OOD generalization ability.
* We conduct extensive experiments on standard benchmarks to validate our findings. Our DUL achieves dual-optimal OOD detection and OOD generalization performance. To our best knowledge, DUL is the first method that gains state-of-the-art OOD detection performance without sacrificing OOD generalization ability.

Figure 1: **(a)**: Models trained on in-distribution (ID) data inevitably encounter distributional shifts during their deployment. OOD generalization expects the model to correctly classify covariate-shifted data that undergoes noise or corruption due to environmental issues. OOD detection aims to identify samples that do not belong to any known classes for trustworthiness consideration. **(b)**: Limitations of current advanced OOD detection methods. We consider 8 representative OOD detection methods including the baseline method MSP [6] (without any OOD detection regularization), Entropy [7], EBM [8], Bayesian [9], SOTA OOD detection methods WOODS [10], POEM [11], recent advanced SCONE [4] which aims to seek for a good trade-off and the proposed DUL. All these methods exhibit a degraded generalization ability compared to baseline method MSP and lie in a trade-off area except our DUL. The goal of this paper is to understand and mitigate this phenomenon.

Related works

**OOD detection** aims to indicate whether the input arises from unknown classes that are not present in training data, which is essential for model trustworthiness. In the classification task, the majority of advanced OOD detection methods include MSP detectors which characterize samples with lower max softmax probability as OOD [6; 14; 15; 7; 16]. EBM detectors identify high energy samples as OOD and frequently establish better performance than MSP detectors [8; 11; 17; 4], and various other types OOD detection methods such as distance-based detectors [18], non-parametric KNN-based detectors [19] which also show promises. According to the training paradigm, OOD detection methods can be split into auxiliary OOD-free and auxiliary OOD-required methods. Auxiliary OOD-free methods directly use the model pre-trained on ID data only for OOD detection. Another line of methods assumes that some OOD data is accessible during training and incorporates auxiliary outlier datasets (collected from websites or other datasets) for further enhancing OOD detection performance. By exposing the model to some semantic OOD during training, auxiliary OOD-required methods frequently outperform auxiliary OOD-free methods on commonly-used benchmarks [20; 5].

**OOD generalization** expects the model to be robust under unforeseeable noise or corruption [21; 22; 23; 24; 25]. Basically, OOD generalization expects invariant and confident prediction on OOD data. Examples include classic domain adaption (DA) methods which encourage the model's behavior to be invariant across different distributions [21; 26; 27]. Besides, test-time adaption (TTA) directly encourages confident predictions on OOD data by minimizing predictive entropy [28; 29; 30]. However, as we will show later, confident prediction and invariance are seemingly conflictive to OOD detection purpose and further imply an unavoidable trade-off. The most related work to our paper is SCONE [4], which strikes to keep a balance between OOD detection and generalization performance. We argue that such a trade-off is not necessary and the conflict can be elegantly eliminated.

**Uncertainty estimation in Bayesian framework.** In the Bayesian framework, predictive uncertainty can be regarded as an indicator of whether the input sample is prone to be OOD. Since OOD samples are unseen during training and thus should be of higher uncertainty than ID. The overall predictive uncertainty of a classification model can be decomposed into three factors according to their source, including data (aleatoric) uncertainty (AU), distributional uncertainty (DU), and model (epistemic) uncertainty (EU) [31; 9]. AU measures the natural complexity of the data (e.g., class overlap, label noise) and EU results from the difficulty of estimating the model parameters with finite training data. DU arises due to the mismatch between the distributions of test and training data. A line of classic measurement can be used to capture various types of uncertainty including entropy, mutual information, and differential entropy [9].

## 3 Preliminaries

We consider \(K\)-class classification task with classifier \(f:\mathcal{X}\rightarrow\mathbb{R}^{K}\) parameterized by \(\theta\), where \(\mathcal{X}\) is the input space and \(\mathcal{Y}=\{1,2,...,K\}\) denotes the target space. The model output \(f_{\theta}(x)\) is considered as logits. The \(k\)-th element in logits is denoted as \(f_{k}(x)\) indicates the confidence of predicting \(x\) to class \(k\). The predicted distribution \(F(x)\) is obtained by normalizing \(f(x)\) with the softmax function. We first formalize all possible distributions that the model might encounter.

* In-distribution \(P^{\mathrm{ID}}_{\mathcal{X}\mathcal{Y}}\) which denotes the distribution of labeled training data.
* Covariate-shifted OOD \(P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}\) which is relevant to OOD generalization. \(P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}\) is of the same label space with ID. However, its marginal distribution \(P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}\) encounters shifts due to unexpected noise or corruption.
* Semantic OOD \(P^{\mathrm{SEM}}_{\mathcal{X}\mathcal{Y}^{\prime}}\) is the distribution of data that do not belong to any known class. Its label space has no overlap with the known ID label space, i.e., \(\mathcal{Y}^{\prime}\cap\mathcal{Y}=\emptyset\).

In the following paper, we omit the subscript for simplicity. The goal of OOD detection is to build a detector \(G:\mathcal{X}\rightarrow[\mathrm{IN},\mathrm{OUT}]\) to decide whether an input \(x\) is semantic OOD data or not through a thresholding function \(G\) deduced from classifier \(f\)

\[G_{\gamma}(x)=\begin{cases}\mathrm{IN}&g_{f}(x)\leq\gamma\\ \mathrm{OUT}&g_{f}(x)>\gamma\end{cases}\,,\] (1)where \(\gamma\) is the threshold. \(g_{f}\) is an OOD scoring function deduced from \(f\), which is expected to assign a higher value to OOD than ID. For example, in MSP detectors, \(g_{f}(x)=-\max_{k}F(x)\) where \(F(x)\) is the predicted softmax probability (negative max softmax probability). In EBM detectors, \(g_{f}\) is realized by the energy function \(E(x;f):=-\log\sum_{i=1}^{K}e^{f_{k}(x)}\) and the semantic input of OOD should be of high energy [8]. Since it is difficult to foresee \(P^{\mathrm{SEM}}\) one will encounter, a board line of OOD detection works [7; 8; 9; 11; 17; 12; 32; 33] regularize the model on some auxiliary OOD data \(P^{\mathrm{SEM}}_{\mathrm{train}}\) during training (e.g., data from the web or other datasets), and expect the model can learn useful heuristic to handle unknown test-time OOD \(P^{\mathrm{SEM}}_{\mathrm{test}}\). The learning objective is shown as follows

\[\min_{\theta}\,\mathbb{E}_{(x,y)\sim P^{\mathrm{ID}}}[\mathcal{L}_{\mathrm{CE }}(f(x),y)]+\lambda\mathbb{E}_{\tilde{x}\sim P^{\mathrm{EDM}}_{\mathrm{train} }}[\mathcal{L}_{\mathrm{reg}}f(\tilde{x})],\] (2)

where \(\mathcal{L}_{\mathrm{CE}}\) is the standard cross entropy loss for the original classification task. \(\mathcal{L}_{\mathrm{reg}}\) is the OOD detection regularization term depending on the detector used, which generally encourages a high uncertainty on \(P^{\mathrm{SEM}}_{\mathrm{train}}\). For example, \(\mathcal{L}_{\mathrm{reg}}\) is set to cross entropy between \(F(x)\) and the uniform distribution for MSP detector [7]. In EBM detectors [8], \(\mathcal{L}_{\mathrm{reg}}\) is realized as a margin ranking loss to explicitly encourage a large energy gap between ID and semantic OOD. In this paper, we are interested in this setting for the following reasons:

1. In contrast to labeled data in supervised learning literature, auxiliary OOD data can be unlabeled and easy-to-collected in practice [11].
2. Most SOTA methods involve auxiliary outliers [5; 20] for superior performance.
3. Even under some strict assumptions that \(P^{\mathrm{SEM}}_{\mathrm{train}}\) is unavailable, recent works utilize GAN [15], diffusion model [34] or sampling strategy [12] to generate "virtual" outliers for training.

Thus we believe this setting is promising and the cost of auxiliary outliers is minor given the importance of ensuring model trustworthiness. At test-time, the model is evaluated in terms of

* ID accuracy (ID-Acc \(\uparrow\)) which measures the model's performance on \(P^{\mathrm{ID}}\),
* OOD accuracy (OOD-Acc \(\uparrow\)) measures the OOD generalization ability on \(P^{\mathrm{COV}}\),
* False positive rate at 95% true positive rate (FPR95\(\downarrow\)) := \(\mathbb{E}_{x\sim P^{\mathrm{SEM}}_{\mathrm{test}}}(\mathbb{I}(G_{\gamma}(x)= \mathrm{IN}))\) measures the OOD detection ability, where \(\gamma\) is chosen when true positive rate (TPR) is \(95\%\). \(\mathbb{I}\) is the indicator function. In OOD detection, ID samples are considered as negative.

It is worth noting that in the standard OOD detection setting [11; 4], the test OOD data should not have any overlapped classes or samples with training-time auxiliary OOD data \(P^{\mathrm{SEM}}_{\mathrm{train}}\). Let \(\mathcal{Y}^{\mathrm{SEM}}_{\mathrm{test}}\) and \(\mathcal{Y}^{\mathrm{SEM}}_{\mathrm{train}}\) be the label space of \(P^{\mathrm{SEM}}_{\mathrm{test}}\) and \(P^{\mathrm{SEM}}_{\mathrm{train}}\) respectively, we have \(\mathcal{Y}^{\mathrm{SEM}}_{\mathrm{test}}\cap\mathcal{Y}^{\mathrm{SEM}}_{ \mathrm{train}}=\emptyset\). Otherwise, OOD detection would be a trivial problem.

## 4 Sensitive-robust Dilemma of Out-of-distribution Detection

In this section, we detail the limitation of current OOD detection methods: their OOD detection performance is achieved at the cost of generalization ability. This limitation implies the potential risk of SOTA OOD detection methods and underscores the urgent need for a better solution. Firstly, we re-examine representative OOD detection methods of six different types, including 1) baseline model MSP that is trained without any OOD detection regularization [6], 2) entropy-regularization (Entropy) that encourages high predictive entropy on OOD [7], which is devised for MSP detectors, 3) energy-regularization for EBM detectors that enforces the output with high energy score for OOD input [8], 4) Bayesian uncertainty learning that encourages high overall uncertainty on OOD [9], 5) state-of-the-art OOD detection methods WOODS [10] and POEM [11] 6) the most related SCONE [4] that seeks for a trade-off between OOD detection and generalization performance.

**Limitation of current OOD detection methods.** In Fig. 1 (b), we investigate current OOD detection methods in terms of OOD classification error and FPR95. The expected classifier should yield both low OOD classification error and FPR95. As it is observed, despite the superior OOD detection performance, all above methods significantly underperform the baseline MSP in terms of OOD generalization. By contrast, our method (DUL) successfully overcomes the limitation.

**Theoretical justification.** Toward understanding the limitation, we provide theoretical analysis for two types of most popular OOD detection methods, i.e., MSP and EBM detectors. Our analysis identifies the "_sensitive-robust_" dilemma as the main reason behind such a limitation. The roadmap of our analysis is: (1) inspired by transfer learning theory, we first reveal that OOD detection regularization applied on semantic OOD may also affect the behavior of model on covariate-shifted OOD; (2) then we demonstrate why MSP detectors suffer from poor generalization by characterizing its generalization error bound; (3) we further identify that EBM methods [8] suffer from a similar drawback when incorporating with gradient-based optimization. First of all, we recap the definition of disparity discrepancy in transfer learning theory [35, 27].

**Definition 1** (Disparity with Total Variation Distance).: _Given two hypotheses \(f^{\prime},f\in\mathcal{F}\) and distribution \(P\), we define the Disparity with Total Variation Distance between them as_

\[\mathrm{disp}_{P}(f^{\prime},f)=\mathbb{E}_{P}[TV(F_{f}||F_{f^{\prime}})],\] (3)

_where \(F_{f^{\prime}},F_{f}\) are the class distributions predicted by \(f^{\prime},f\) respectively. \(TV(\cdot||\cdot)\) is the total variation distance, i.e., \(TV(F_{f}||F_{f^{\prime}})=\frac{1}{2}\sum_{k=1}^{K}||F_{f,k}-F_{f^{\prime},k}|\)._

**Definition 2** (Disparity Discrepancy with Total Variation Distance, DD with TVD).: _Given a hypothesis space \(\mathcal{F}\) and two distributions \(P,Q\), the Disparity Discrepancy with Total Variation Distance (DD with TVD) is defined as_

\[d_{\mathcal{F}}(P,Q):=\sup_{f^{\prime},f\in\mathcal{F}}(\mathrm{disp}_{P}(f^ {\prime},f)-\mathrm{disp}_{Q}(f^{\prime},f)).\] (4)

Disparity discrepancy (DD) measures the "distance" between two distributions \(P,Q\) which considers the hypothesis space. DD is one of the most fundamental conceptions in transfer learning theory which constrains the behavior of hypothesis in \(\mathcal{F}\) should not be dissimilar substantially on different distributions \(P\) and \(Q\). 3 If the DD between semantic OOD and covariate-shifted OOD is limited, one can suppose that OOD detection regularization applied to semantic OOD samples will also influence the model's behavior on covariate-shifted OOD. Thus encouraging high uncertainty on semantic OOD may also result in highly uncertain prediction on covariate-shifted OOD, which is potentially harmful to generalization ability. We first formalize this intuition for MSP detectors.

Footnote 3: Encompassed by [36] as a special case, our definition is realized using TVD for theoretical convenience.

**Theorem 1** (Sensitive-robust dilemma).: _Let \(\mathcal{P}^{\mathrm{COV}}\), \(P^{\mathrm{SEM}}_{\mathrm{test}}\) be the covariate-shifted OOD and semantic OOD distribution. \(\mathrm{GError}_{\mathrm{P}^{\mathrm{COV}}}(f)\) denotes standard cross entropy loss taking expectation on \(P^{\mathrm{COV}}\), i.e., generalization error. Then we have_

\[\underbrace{\mathrm{GError}_{\mathrm{P}^{\mathrm{COV}}}(f)}_{\mathrm{OOD~{} generalization~{}error}}\geq\,C-\sqrt{\frac{1}{8\kappa^{2}}}\,\mathbb{E}_{P^{ \mathrm{SEM}}_{\mathrm{test}}}[\underbrace{\mathcal{L}_{\mathrm{reg}}(f)}_{ \mathrm{OOD~{}detection~{}loss}}-\log K]^{\frac{1}{2}}-\frac{1}{2\kappa}d_{ \mathcal{F}}(\mathcal{P}^{\mathrm{COV}},\mathcal{P}^{\mathrm{SEM}}_{\mathrm{ test}}),\] (5)

_where \(\mathcal{L}_{\mathrm{reg}}\) is the OOD detection loss devised for MSP detectors defined in [7], i.e., cross-entropy between predicted distribution \(F(x)\) and uniform distribution. \(d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_{\mathrm{test}})\) is DD with TVD that measures the dissimilarity of covariate-shifted OOD and semantic OOD. \(C\) and \(\kappa\) are both some constants depending on hypothesis space \(\mathcal{F}\), \(P^{\mathrm{COV}}\) and \(P^{\mathrm{SEM}}_{\mathrm{test}}\)._

The proof is deferred in Appendix A. Theorem 1 demonstrates that for MSP detectors, the OOD detection objective conflicts with OOD generalization. The model's generalization error lower bound is negatively correlated with OOD detection loss that the model tries to minimize. Thus given a limited \(d_{\mathcal{F}}\), pursuing low OOD detection loss on \(P^{\mathrm{SEM}}_{\mathrm{test}}\) will also inevitably result in highly uncertain prediction on \(P^{\mathrm{COV}}\). It is worth noting that such an interpretative theorem is applicable for all MSP-based OOD detectors no matter whether the model involves \(P^{\mathrm{SEM}}_{\mathrm{train}}\) during training or not. Since the inherent motivation of OOD detection methods lies in minimizing the OOD detection loss in \(P^{\mathrm{SEM}}_{\mathrm{test}}\), regardless of the training strategies used.

**Why a limited \(d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_{\mathrm{test}})\) is practical?** In Theorem 1, \(d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_{\mathrm{test}})\) measures the dissimilarity between \(P^{\mathrm{COV}}\) and \(P^{\mathrm{SEM}}_{\mathrm{test}}\). It seems that this lower bound will be very small and trivial when \(d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_{\mathrm{test}})\) is large enough. However, since the semantic OOD samples can be any samples that do not belong to ID classes, one can suppose that semantic OOD samples are extremely diverse and some are of high similarity with ID and covariate-shifted OOD [37]. Detecting these "ID-like" OOD samples is inherently the core challenge of OOD detection [11, 17, 38]. Thus, it is reasonable to assume a limited \(d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_{\mathrm{test}})\). We provide more discussions in the Appendix D.2.

[MISSING_PAGE_FAIL:6]

uncertainty. Such a property is well suited to achieve OOD detection and generalization jointly since high DU no longer necessarily indicates high overall uncertainty.

**Decoupled Uncertainty Learning.** While the aforementioned Bayesian framework enjoys theoretical potentiality, its learning object [9] lacks consideration of OOD generalization. Similar to other OOD detection methods, it also directly enforces high overall uncertainty on OOD

\[\min_{\theta}\mathbb{E}_{\mathcal{P}^{\mathrm{ID}}}\mathrm{KL}(p(y|x))||p(\hat{y }|x))+\mathbb{E}_{\mathcal{P}^{\mathrm{SEM}}_{\mathrm{train}}}\mathrm{KL}(p(\mu| \tilde{x}))||\mathrm{Dir}(\mu|\alpha=\mathbf{1})),\] (10)

where \(p(y|x),p(\hat{y}|x)\) are the ground-truth distribution and predicted distribution on ID. The model's prediction on OOD is enforced to be close to a rather flat Dirichlet distribution. It is worth noting that \(\mathrm{Dir}(\mu|\alpha=\mathbf{1})\) means all classes are equiprobable, and the entropy of the final prediction is maximized. As shown in Fig. 1 (b), the vanilla Bayesian method [9] also suffers from degraded OOD generalization performance. To this end, we propose **Decoupled Uncertainty Learning** (DUL), a novel OOD detection regularization method that explicitly encourages high DU on OOD samples without affecting the overall uncertainty. Similarly to previous OOD detection methods [8], our DUL is also devised in a finetune manner for effectiveness. Given a classifier \(f_{\theta_{0}}\) well pre-trained on \(P^{\mathrm{ID}}\), the goal of DUL lies in enhancing its OOD detection performance without sacrificing any generalization ability. Specifically, we finetune the model by encouraging higher DU but non-increased overall uncertainty on \(P^{\mathrm{SEM}}_{\mathrm{train}}\). The learning objective of DUL is

\[\min_{\theta} \underbrace{\mathbb{E}_{(x,y)\sim P^{\mathrm{ID}}}[\mathcal{L}_{ \mathrm{CE}}(f(x),y)]}_{\mathrm{ID\ classification}}+\lambda\underbrace{\mathbb{E}_{ \tilde{x}\sim P^{\mathrm{SEM}}_{\mathrm{train}}}||\mathrm{max}(0,(h_{0}+m_{ \mathrm{OUT}})-h)||_{\tau}}_{\mathrm{high\ distributional\ uncertainty\ (detection)}}\] (11) \[\mathrm{s.t.} \underbrace{H(p(\hat{y}|\tilde{x}))=H(p_{0}(\hat{y}|\tilde{x}))} _{\mathrm{non-increased\ overall\ uncertainty\ (generalization)}}\forall\ \tilde{x}\sim\ P^{\mathrm{SEM}}_{\mathrm{train}},\]

where \(H(\cdot)\) is the entropy. \(p(\hat{y}|\tilde{x})\) and \(p_{0}(\hat{y}|\tilde{x})\) are the predicted distribution on semantic OOD data \(\tilde{x}\) after and before finetuning. The first term is the original ID classification loss. The second term is OOD detection loss, which encourages high DU on outlier \(\tilde{x}\). \(m_{\mathrm{OUT}}\) and \(\tau>0\) are hyperparameters. \(h_{0},h\) are DU on \(\tilde{x}\) before and after finetuning. Here we measure DU with the differential entropy (\(h|p(\mu|\tilde{x},\theta)|=-\int_{S^{K-1}}p(\mu|\tilde{x})\mathrm{ln}(p(\mu| \tilde{x}))d\mu\), \(S\) is a \(K\)-simplex). We refer interested readers to the Appendix D.1 for mathematical details. The third term constraining on \(H(p(\hat{y}|\tilde{x}))\) avoids increment of overall uncertainty during finetuning and thus the generalization ability can be retained. Considering the difficulty of constrained optimization, we convert Eq. 11 into an unconstrained form and get our final minimizing objective

\[\underbrace{\mathbb{E}_{P^{\mathrm{ID}}}[\mathcal{L}_{\mathrm{CE}}(f(x),y)]}_{ \mathrm{ID\ classification}}+\mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{train}}}\{ \lambda\underbrace{||\mathrm{max}(0,(h_{0}+m_{\mathrm{OUT}})-h)||_{\tau}}_{ \mathrm{high\ distributional\ uncertainty}}+\underbrace{\gamma\mathrm{KL}(p(\hat{y}| \tilde{x})||p_{0}(\hat{y}|\tilde{x}))}_{\mathrm{unchanged\ overall\ uncertainty}}\},\] (12)

where \(\gamma\) is hyperparameter. In contrast to previous Bayesian method [9], DUL only encourages high DU rather than overall uncertainty on OOD and explicitly discourages high entropy in the final prediction. The implementation details are in Appendix D.1.

## 6 Experiment

We conduct experiments to validate our analysis and the superiority of DUL. The questions to be verified are Q1 Motivation. To what extent does OOD detection conflict with OOD generalization in previous methods? Q2 Effectiveness. Does DUL achieve better OOD detection and generalization performance compared to its counterparts? Q3 Interpretability. Does the proposed method well decouple uncertainty as expected?

### Experimental Setup

Our settings follow the common practice [8; 11; 20; 5] in OOD detection. Here we present a brief description and more details about datasets, metrics, and implementation are in Appendix B.1 and B.2.

**Datasets. \(\circ\) ID datasets \(P^{\mathrm{ID}}\).** We train the model on different ID datasets including CIFAR-10, CIFAR-100 and ImageNet-200 (a subset of ImageNet-1K [39] with 200 classes). \(\circ\)**Auxiliary OOD datasets \(P^{\mathrm{SEM}}_{\mathrm{train}}\)**. In CIFAR experiments, we use ImageNet-RC as \(P^{\mathrm{SEM}}_{\mathrm{train}}\). ImageNet-RC is a down-sampled variant of the original ImageNet-1K which is widely adopted in previous OOD detection works [8; 11; 17]. We also conduct experiments on the recent TIN-597 [20] as an alternative. When ImageNet-200 is ID, the remaining 800 classes termed ImageNet-800 are considered as \(P_{\rm train}^{\rm SEM}\). o **OOD detection test sets**\(P_{\rm test}^{\rm SEM}\) are a suite of diverse datasets introduced by commonly used benchmark [5]. In CIFAR experiments, we use SVHN [40], Places365 [41], Textures [42], LSUN-R, LSUN-C [43] and iSUN [44] as \(P_{\rm test}^{\rm SEM}\). When \(P^{\rm ID}\) is ImageNet-200, \(P_{\rm test}^{\rm SEM}\) consists of iNaturalist [45], Open-Image [46], NINCO [47] and SSB-Hard [48]. It is worth noting that in standard OOD detection settings, there should be no overlapped classes between \(P^{\rm ID}\), \(P_{\rm train}^{\rm SEM}\) and \(P_{\rm test}^{\rm SEM}\), otherwise OOD detection is a trivial problem. o **OOD generalization test sets**\(P_{\rm COV}\) is the original ID test set corrupted with additive Gaussian noise of \(\mathcal{N}(0,5)\), following [4]. Besides, we also conduct experiments on CIFAR10-C, CIFAR100-C and ImageNet-C which involve 15 diverse types of different noise or corruption (e.g., snow, rain, frost, fog...) in Appendix C.

\begin{table}
\begin{tabular}{c c|c c|c c c} \hline \hline \multirow{2}{*}{\(P^{\rm ID}/P_{\rm train}^{\rm SEM}\)} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Model generalization} & \multicolumn{3}{c}{OOD detection} \\  & & ID-Acc \(\uparrow\) & OOD-Acc \(\uparrow\) & FPR \(\downarrow\) & AUROC \(\uparrow\) & AUPR \(\uparrow\) \\ \hline \multirow{5}{*}{CIFAR-10 / None} & MSP & \(96.11\) & \(87.35\) & \(41.96\) & \(89.28\) & \(68.00\) \\  & EBM (pretrain) & \(96.11\) & \(87.35\) & \(32.45\) & \(89.34\) & \(75.22\) \\  & Maxlogits & \(96.11\) & \(87.35\) & \(32.90\) & \(89.26\) & \(74.47\) \\  & Mahalanobis & \(96.11\) & \(87.35\) & \(32.53\) & \(93.93\) & \(74.96\) \\ \hline \multirow{5}{*}{CIFAR-10 / ImageNet-RC} & Entropy & \(96.04\) & \(72.57\) & \(6.63\) & \(98.72\) & \(94.00\) \\  & EBM (finetune) & \(96.10\) & \(79.03\) & \(3.61\) & \(98.39\) & \(94.88\) \\  & POEM & \(94.32\) & \(78.89\) & \(\mathbf{3.32}\) & \(\mathbf{98.99}\) & \(\mathbf{99.38}\) \\  & DPN & \(95.69\) & \(85.52\) & \(4.28\) & \(98.53\) & \(94.93\) \\  & WOODS & \(96.01\) & \(80.14\) & \(7.12\) & \(98.45\) & \(92.46\) \\  & SCONE & \(95.96\) & \(78.50\) & \(7.02\) & \(98.45\) & \(92.46\) \\  & DUL (ours) & \(96.02^{\pm 0.00}\) & \(\mathbf{88.02}^{\pm 0.02}\) & \(58.98^{\pm 0.12}\) & \(98.47^{\pm 0.02}\) & \(92.41^{\pm 1.29}\) \\  & DUL\({}^{\dagger}\) (ours) & \(96.04^{\pm 0.00}\) & \(\mathbf{87.53}^{\pm 0.09}\) & \(5.99^{\pm 0.06}\) & \(98.28^{\pm 0.01}\) & \(98.40^{\pm 0.13}\) \\ \hline \multirow{5}{*}{CIFAR-10 / TIN-597} & Entropy & \(95.94\) & \(80.51\) & \(11.60\) & \(97.93\) & \(92.16\) \\  & EBM (finetune) & \(95.38\) & \(83.67\) & \(19.36\) & \(87.51\) & \(83.63\) \\  & POEM & \(95.44\) & \(83.17\) & \(24.34\) & \(80.83\) & \(94.25\) \\  & DPN & \(94.39\) & \(79.23\) & \(17.27\) & \(94.92\) & \(87.67\) \\  & WOODS & \(95.57\) & \(84.08\) & \(7.58\) & \(\mathbf{98.29}\) & \(93.08\) \\  & SCONE & \(95.19\) & \(84.68\) & \(8.02\) & \(98.21\) & \(93.08\) \\  & DUL (ours) & \(\mathbf{96.06}^{\pm 0.01}\) & \(87.03^{\pm 0.02}\) & \(\mathbf{6.87}^{\pm 0.67}\) & \(98.21^{\pm 0.01}\) & \(91.29^{\pm 1.29}\) \\  & DUL\({}^{\dagger}\) (ours) & \(95.94^{\pm 0.00}\) & \(\mathbf{88.10}^{\pm 0.07}\) & \(10.34^{\pm 0.01}\) & \(97.67^{\pm 0.01}\) & \(\mathbf{98.59}^{\pm 0.06}\) \\ \hline \hline \multirow{5}{*}{CIFAR-100 / None} & MSP & \(80.99\) & \(55.95\) & \(74.63\) & \(80.19\) & \(42.59\) \\  & EBM (finetune) & \(80.99\) & \(55.95\) & \(67.42\) & \(82.67\) & \(49.35\) \\  & Maxlogits & \(80.99\) & \(55.95\) & \(69.32\) & \(82.30\) & \(47.60\) \\  & Mahalanobis & \(80.99\) & \(55.95\) & \(61.51\) & \(85.97\) & \(56.10\) \\ \hline \multirow{5}{*}{CIFAR-100 / TIN-597} & Entropy & \(80.21\) & \(45.48\) & \(22.29\) & \(95.33\) & \(82.34\) \\  & EBM (finetune) & \(80.53\) & \(48.14\) & \(13.47\) & \(96.78\) & \(87.84\) \\  & POEM & \(78.15\) & \(42.18\) & \(\mathbf{9.89}\) & \(\mathbf{97.79}\) & \(\mathbf{98.40}\) \\  & DPN & \(78.90\) & \(50.14\) & \(18.36\) & \(95.42\) & \(74.45\) \\  & WOODS & \(80.69\) & \(54.38\) & \(38.15\) & \(92.01\) & \(71.79\) \\  & SCONE & \(80.80\) & \(\mathbf{56.73}\) & \(47.60\) & \(80.61\) & \(65.29\) \\  & DUL (ours) & \(\mathbf{81.30}^{\pm 0.04}\) & \(\mathbf{56.27}^{\pm 2.32}\) & \(12.49^{\pm 0.05}\) & \(95.24^{\pm 0.01}\) & \(86.72^{\pm 0.08}\) \\  & DUL (ours) & \(\mathbf{81.23}^{\pm 0.00}\) & \(\mathbf{55.54}^{\pm 0.54}\) & \(11.12^{\pm 0.05}\) & \(95.46^{\pm 0.36}\) & \(96.49^{\pm 0.13}\) \\ \hline \multirow{5}{*}{CIFAR-100 / TIN-597} & Entropy & \(80.15\) & \(46.25\) & \(26.88\) & \(93.50\) & \(79.81\) \\  & EBM (finetune) & \(79.94\) & \(50.00\) & \(26.87\) & \(91.68\) & \(80.08\) \\  & POEM & \(78.68\) & \(52.53\) & \(32.71\) & \(91.30\) & \(94.65\) \\  & DPN & \(78.64\)

**Metrics.** For OOD detection performance evaluation, we report the average FPR95, AUROC and AUPR to be consistent with [11]. OOD generalization ability is compared in terms of classification accuracy (OOD-Acc). Besides, we also report classification accuracy on ID test sets (ID-Acc).

**Compared methods.** We compare DUL with a board line of OOD detection methods, including auxiliary OOD required and auxiliary OOD free methods. \(\circ\)**Auxiliary OOD-free methods** do not require \(P_{\mathrm{train}}^{\mathrm{SEM}}\) during training, including MSP [6], Maxlogits [49], pretrained EBM [8] and Mahalaobis [18]. \(\circ\)**Auxiliary OOD-required methods** explicitly regularize the model on \(P_{\mathrm{train}}^{\mathrm{SEM}}\), including entropy-regularization (Entropy) [7], finetuned EBM [8], DPN of Bayesian framework [9], POEM [11] and WOODS [10]. We also compare our DUL to recent advanced SCONE [4] which aims to keep a balance between OOD detection and generalization.

### Experimental Results

**Dilemma between OOD detection and generalization (Q1).** We validate the dilemma mentioned before in Fig. 1. As shown in Tab. 1, though many advanced methods establish superior OOD detection performance, their OOD generalization degrades a lot. For example, recent SOTA POEM achieves nearly perfect OOD detection performance on CIFAR10 when ImageNet-RC serves as \(P_{\mathrm{train}}^{\mathrm{SEM}}\) with \(3.32\%\) false positive error rate (FPR95). However, its OOD-Acc drops a lot (about \(10\%\)) compared to baseline MSP. This phenomenon is also observed in other advanced methods. To further detail this phenomenon, we reduce the weight of OOD detection regularization terms in Entropy and finetuned EBM and show the performance on both OOD detection and generalization. As shown in Table 3, when the regularization strength increases, OOD detection performance improves (lower FPR.), while the OOD generalization performance degrades (higher error rate).

**OOD detection and generalization ability (Q2).** As shown in Tab. 1, DUL establishes strong overall performance in terms of both OOD detection and generalization. We highlight a few essential observations: 1) **Compared to auxiliary OOD free methods**, DUL establishes substantial improvement due to additional regularization on auxiliary outliers. 2) **Compared to auxiliary OOD required methods**, our method achieves superior OOD detection performance without sacrificing generalization ability. Meanwhile, previous OOD detection methods commonly exhibit severely degraded classification accuracy, with many cases increasing by more than \(10\%\) error rate. 3) **Comparison to the most related work SCONE [4].** Despite recent advanced SCONE simultaneously considering both two targets, we observe that it can be hard to find a good trade-off. In contrast, dual-optimal OOD detection and generalization performance is achieved by our DUL. Noted that DUL is the only method that achieves _state-of-the-art_ detection performance (mostly the best or second best) without degraded generalization ability (no red values in the entire row). The sensitive-robust dilemma is no longer observed in our method. These observations justify our expectation of DUL. 4) **Combining with existing methods.** Besides, to further demonstrate the effectiveness of the proposed DUL, we also add the unchanged overall uncertainty term in Eq.12 to the original Entropy and finetuned EBM. The results in Table 2 show that DUL regularization can also benefit EBM. However, combining Entropy with our regularization can not improve the accuracy substantially. This is not surprising, since the target of Entropy (high entropy prediction) and our DUL (non-increased entropy) directly conflict according to Theorem 1. 5) **Comparison to methods with an extra OOD detect branch.** Different from aforementioned methods, a line of recent OOD detectors [50; 51; 52; 17] employ extra output branches aside from the classification logits (with a shared backbone for feature extraction). For these OOD detectors, our theoretical analysis is not directly applicable and further analysis from a feature learning perspective may be needed in future work. However, the proposed DUL is devised in a finetune manner. Compared to OOD detectors with extra output branches that requires re-training the classifier from scratch, DUL can be applied to any pre-trained model (e.g., from torchvision, huggingface), with modest computation overhead.

**Visualization of estimated uncertainty (Q3).** To evaluate the uncertainty estimation, we visualize the distribution of ID (CIFAR-10) and OOD (SVHN) samples in terms of uncertainty. As we can see in Fig. 2 (b), our DUL establishes a distinguishable (distributional) uncertainty gap between test-time ID and OOD data, which indicates a good sensitiveness for OOD detection. By contrast, the baseline method MSP (Fig.2 (a)) can not effectively discriminate ID and OOD. Besides, we visualize the predictive entropy (overall uncertainty) on covariate-shifted OOD (CIFAR-10 with Gaussian noise) in Fig. 2 (c), our DUL yields much lower entropy compared to other methods. Besides, we visualize the data uncertainty on semantic OOD test data (Textures) when CIFAR-10 is ID in Fig. 6.2. Theinvestigated methods are 1) pretrained model training on ID dataset only, 2) finetuned model with OOD detection regularization (ablating the last term in Eq.12), and 3) finetuned model with the full DUL method described by Eq.12. As shown in Fig. 6.2, to keep the overall uncertainty and enlarge the distributional uncertainty (for OOD detection), the data uncertainty must be reduced. We use Eq.17 from [9] to calculate data uncertainty. The distributional uncertainty is shifted by subtracting that on ID dataset. These results meet our expectation.

## 7 Conclusion

This paper provides both theoretical and empirical analysis towards understanding the dilemma between OOD detection and generalization. We demonstrate that the superior OOD detection performance of current advances are achieved at the cost of generalization ability. The theory-inspired algorithm successfully removes the conflict between previous OOD detection and generalization methods. For SOTA OOD detection performance, our implementation assumes that auxiliary outliers are available during training. This limitation is noteworthy for our DUL as well as the most existing SOTA OOD detection methods. We argue that this added cost is minor and reasonable given the significance of ensuring model trustworthiness in open-environments. Reducing the dependency on auxiliary OOD data can be an interesting research direction for the future exploration.

## Acknowledgement

This work was supported by the National Natural Science Foundation of China (62376193 and 61925602). The authors thank NeurIPS anonymous peer reviewers for their helpful suggestions.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline  & \multicolumn{2}{c|}{Model generalization} & \multicolumn{2}{c}{OOD detection} \\ Method & ID-Acc \(\uparrow\) & OOD-Acc \(\uparrow\) & FPR \(\downarrow\) & AUC \(\uparrow\) \\ \hline Entropy & 96.04 & 72.57 & 6.63 & 98.72 \\ EBM (finetune) & 96.10 & 70.33 & 6.61 & 98.39 \\ POEM & 94.12 & 78.89 & 3.32 & 98.99 \\ \hline EBM w. DUL & 95.19 & 87.45 & 61.71 & 98.28 \\ Entropy w. DUL & 96.10 & 87.41 & 92.56 & 95.92 \\ \hline DUL & 96.02 & 88.01 & 5.89 & 98.47 \\ DUL\({}^{\dagger}\) & 96.04 & 87.53 & 5.99 & 98.28 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Additional results when equip DUL to existing methods i.e., Entropy and finetuned EBM. ID dataset is CIFAR-10. \(P_{\mathrm{train}}^{\mathrm{SEM}}\) is ImageNet-RC. \(P_{\mathrm{test}}^{\mathrm{Cov}}\) is the original CIFAR-10 testset corrupted by Gaussian noise \(\mathcal{N}(0,5)\).

Figure 3: Visualization of uncertainty on semantic OOD test dataset when CIFAR-10 is ID dataset. Without DUL (orange), all three types of uncertainty will increase altogether. In contrast, DUL (green) increases the DU but decreases the AU, which further lead to unchanged overall uncertainty.

\begin{table}
\begin{tabular}{c c c|c c c} \hline \hline  & \multicolumn{2}{c|}{Entropy} & \multicolumn{2}{c}{EBM} \\ \(\lambda\) & OOD-Err \(\downarrow\) & FPR \(\downarrow\) & \(\lambda\) & OOD-Err \(\downarrow\) & FPR \(\downarrow\) \\ \hline
0 & 9.55 & 35.15 & 0 & 9.55 & 20.57 \\ \(5\times 10^{-4}\) & 13.58 & 8.36 & \(1\times 10^{-4}\) & 9.46 & 14.69 \\ \(5\times 10^{-3}\) & 15.48 & 6.37 & \(1\times 10^{-3}\) & 10.32 & 13.54 \\ \(5\times 10^{-2}\) & 17.97 & 5.71 & \(1\times 10^{-2}\) & 16.43 & 8.15 \\ \(5\times 10^{-1}\) & 18.53 & 5.60 & \(1\times 10^{-1}\) & 24.38 & 6.11 \\ \hline \hline \end{tabular}
\end{table}
Table 3: We tune the weight of OOD detection regularization term for EBM as well as Entropy and report the FPR (OOD detection metric) and error rate (Err, OOD generalization metric). The experimental settings are the same with Table 2.

Figure 2: Visualization of different types of uncertainty estimated by DUL.

## References

* [1] Chunjong Park, Anas Awadalla, Tadayoshi Kohno, and Shwetak Patel. Reliable and trustworthy machine learning for health using dataset shift detection. _Advances in Neural Information Processing Systems_, 34:3043-3056, 2021.
* [2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. _arXiv preprint arXiv:1606.06565_, 2016.
* [3] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* [4] Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert D Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In _International Conference on Machine Learning_, 2023.
* [5] Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Yixuan Li, Ziwei Liu, Yiran Chen, and Hai Li. Openood v1.5: Enhanced benchmark for out-of-distribution detection. _arXiv preprint arXiv:2306.09301_, 2023.
* [6] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _International Conference on Learning Representations_, 2017.
* [7] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. _International Conference on Learning Representations_, 2019.
* [8] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. _Advances in neural information processing systems_, 2020.
* [9] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. _Advances in neural information processing systems_, 2018.
* [10] Julian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. Training ood detectors in their natural habitats. In _International Conference on Machine Learning_, 2022.
* [11] Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior sampling. In _International Conference on Machine Learning_, 2022.
* [12] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don't know by virtual outlier synthesis. _International Conference on Learning Representations_, 2022.
* [13] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv preprint arXiv:2110.11334_, 2021.
* [14] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. _International Conference on Learning Representations_, 2018.
* [15] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. _International Conference on Learning Representations_, 2018.
* [16] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. _Advances in neural information processing systems_, 2019.
* [17] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. In _Machine Learning and Knowledge Discovery in Databases_, 2021.
* [18] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 2018.

* Sun et al. [2022] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In _International Conference on Machine Learning_, pages 20827-20840. PMLR, 2022.
* Yang et al. [2022] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-of-distribution detection. _Advances in Neural Information Processing Systems_, 35:32598-32611, 2022.
* Arjovsky et al. [2019] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* Hu and Hong [2013] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization. _Available at Optimization Online_, 2013.
* Michel et al. [2021] Paul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the second player in distributionally robust optimization. _International Conference on Learning Representations_, 2021.
* Sinha et al. [2018] Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional robustness with principled adversarial training. _International Conference on Learning Representations_, 2018.
* Han et al. [2022] Zongbo Han, Zhipeng Liang, Fan Yang, Liu Liu, Lanqing Li, Yatao Bian, Peilin Zhao, Bingzhe Wu, Changqing Zhang, and Jianhua Yao. Umix: Improving importance weighting for subpopulation shift via uncertainty-aware mixup. _Advances in Neural Information Processing Systems_, 2022.
* Zhao et al. [2019] Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In _International conference on machine learning_, pages 7523-7532. PMLR, 2019.
* Zhang et al. [2019] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_, 2019.
* Wang et al. [2021] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. _International Conference on Learning Representations_, 2021.
* Zhang et al. [2022] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. _Advances in neural information processing systems_, 35:38629-38642, 2022.
* Niu et al. [2023] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. _arXiv preprint arXiv:2302.12400_, 2023.
* Kendall and Gal [2017] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? _Advances in neural information processing systems_, 2017.
* Wang et al. [2024] Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, and Bo Han. Learning to augment distributions for out-of-distribution detection. _Advances in Neural Information Processing Systems_, 36, 2024.
* Choi et al. [2023] Hyunjun Choi, Hawook Jeong, and Jin Young Choi. Balanced energy regularization loss for out-of-distribution detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15691-15700, 2023.
* Du et al. [2024] Xuefeng Du, Yiyou Sun, Jerry Zhu, and Yixuan Li. Dream the impossible: Outlier imagination with diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ganin and Lempitsky [2015] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In _International Conference on Machine Learning_, 2015.
* Mansour et al. [2008] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. _Advances in neural information processing systems_, 21, 2008.

* [37] William Yang, Byron Zhang, and Olga Russakovsky. Imagenet-ood: Deciphering modern out-of-distribution detection algorithms. _International Conference on Learning Representations_, 2024.
* [38] Yichen Bai, Zongbo Han, Changqing Zhang, Bing Cao, Xiaoheng Jiang, and Qinghua Hu. Id-like prompt learning for few-shot out-of-distribution detection. _arXiv preprint arXiv:2311.15243_, 2023.
* [39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* [40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_. Granada, Spain, 2011.
* [41] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2017.
* [42] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2014.
* [43] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* [44] Junting Pan and Xavier Giro-i Nieto. End-to-end convolutional network for saliency prediction. _arXiv preprint arXiv:1507.01422_, 2015.
* [45] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.
* [46] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International journal of computer vision_, 128(7):1956-1981, 2020.
* [47] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In _International Conference on Machine Learning_, 2023.
* [48] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. The semantic shift benchmark. In _ICML 2022 Shift Happens Workshop_, 2022.
* [49] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. _International conference on machine learning_, 2022.
* [50] Wenjun Miao, Guansong Pang, Xiao Bai, Tianqi Li, and Jin Zheng. Out-of-distribution detection in long-tailed recognition with calibrated outlier class learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 4216-4224, 2024.
* [51] Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, and Matthias Hein. Breaking down out-of-distribution detection: Many methods based on ood training data estimate a combination of the same core quantities. In _International Conference on Machine Learning_, pages 2041-2074. PMLR, 2022.
* [52] Kai Liu, Zhihang Fu, Chao Chen, Sheng Jin, Ze Chen, Mingyuan Tao, Rongxin Jiang, and Jieping Ye. Category-extensible out-of-distribution detection via hierarchical context descriptions. _Advances in Neural Information Processing Systems_, 36, 2024.

* [53] Clement L Canonne. A short note on an inequality between kl and tv. _arXiv preprint arXiv:2202.07198_, 2022.
* [54] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4921-4930, 2022.
* [55] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? _International Conference on Learning Representations_, 2022.
* [56] Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. _International Conference on Learning Representations_, 2019.
* [57] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _British Machine Vision Conference_, 2016.
* [58] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016.
* [59] Yarin Gal et al. Uncertainty in deep learning. 2016.

## Appendix A Proofs

* Experimental Details
* 1 Datasets details
* 2 Implementation details
* Additional Results
* 3 Uncertainty estimation.
* 4 Time-consuming comparison.
* 5 Full results with standard deviation.
* 6 Results of different types of corruption.
* 7 OOD detection results on individual datasets.
* 8 Empirical evidence

* 1 Math derivation
* 2 Discussion about Disparity Discrepancy
* 3 Social Impact

## Appendix A Proofs

First, we recap the definitions of Disparity with Total Variation Distance and Disparity Discrepancy.

**Definition 3** (Disparity with Total Variation Distance).: _Given two hypotheses \(f^{\prime},f\in\mathcal{F}\) and distribution \(P\), we define the Disparity with Total Variation Distance between them as_

\[\mathrm{disp}_{P}(f^{\prime},f)=\mathbb{E}_{P}[TV(F_{f}||F_{f^{\prime}})],\] (13)

_where \(F_{f},F_{f^{\prime}}\) are the class distributions predicted by \(f^{\prime},f\) respectively. \(TV(\cdot||\cdot)\) is the total variation distance (TVD), i.e., \(TV(F_{f}||F_{f^{\prime}})=\frac{1}{2}\sum_{k=1}^{K}||F_{f,k}-F_{f^{\prime},k} ||_{1}\)._

**Definition 4** (Disparity Discrepancy with Total Variation Distance, DD with TVD).: _Given a hypothesis space \(\mathcal{F}\) and two distributions \(P,Q\), the Disparity Discrepancy with Total Variation Distance (DD with TVD) is defined as_

\[d_{\mathcal{F}}(P,Q):=\sup_{f^{\prime},f\in\mathcal{F}}(\mathrm{disp}_{P}(f^ {\prime},f)-\mathrm{disp}_{Q}(f^{\prime},f)).\] (14)

Since TVD is a distance measurement of two distribution. It yields the triangle equality. That is, for any distribution \(P_{\mathcal{X}}\) support on \(\mathcal{X}\) and hypotheses \(f^{1},f^{2}\) and \(f^{3}\in\mathcal{F}\), we have

\[\begin{split}\mathrm{disp}_{P_{\mathcal{X}}}(f^{1},f^{2})& \leq\mathbb{E}_{x\sim P_{\mathcal{X}}}[TV(F_{f^{1}}(x)||F_{f^{3}}(x))]+ \mathbb{E}_{x\sim P_{\mathcal{X}}}[TV(F_{f^{2}}(x)||F_{f^{3}}(x))],\\ \mathrm{disp}_{P_{\mathcal{X}}}(f^{1},f^{2})&\geq \mathbb{E}_{x\sim P_{\mathcal{X}}}[TV(F_{f^{1}}(x)||F_{f^{3}}(x))]-\mathbb{E}_ {x\sim P_{\mathcal{X}}}[TV(F_{f^{2}}(x)||F_{f^{3}}(x))].\end{split}\] (15)

To prove Theorem 1, we need the following lemmas.

**Lemma 1**.: _For any \(f\in\mathcal{F}\), we have_

\[\mathbb{E}_{P^{\mathrm{COV}}}TV(F_{f}||U)\leq\mathbb{E}_{P^{\mathrm{SEM}}_{ \mathrm{test}}}TV(F_{f}||U)+d_{\mathcal{F}}(P^{\mathrm{COV}},P^{\mathrm{SEM}}_ {\mathrm{test}})+\lambda\] (16)

_where \(\lambda\) is a constant independent of \(f\). \(U\) is the \(K\)-classes uniform distribution. \(P^{\mathrm{COV}}\) is the covariate-shifted OOD distribution. \(P^{\mathrm{SEM}}_{\mathrm{test}}\) is the semantic OOD distribution._Proof.: Let \(f^{*}\) be the hypothesis which jointly minimizes the total variance distance between the predicted distribution \(F_{f}\) with uniform distribution \(U\) taking expectation on \(P^{\mathrm{COV}}\) and \(P^{\mathrm{SEM}}_{\mathrm{test}}\), which is to say

\[f^{*}:=\operatorname*{argmin}_{f\in\mathcal{F}}\{\mathbb{E}_{x\sim P^{\mathrm{ COV}}}[TV(F_{f}(x)||U)]+\mathbb{E}_{x\sim P^{\mathrm{SEM}}_{\mathrm{test}}}[TV(F_{f}(x)||U)]\}.\] (17)

Set \(\lambda=\mathbb{E}_{x\sim P^{\mathrm{COV}}}[TV(F_{f^{*}}(x)||U)]+\mathbb{E}_{x \sim P^{\mathrm{SEM}}_{\mathrm{test}}}[TV(F_{f^{*}}(x)||U)]\), then by the triangle equality we have

\[\begin{split}\mathbb{E}_{P^{\mathrm{COV}}}TV(F_{f}||U)& \leq\operatorname{disp}_{P^{\mathrm{COV}}}(f,f^{*})+\mathbb{E}_{P^{ \mathrm{COV}}}TV(F_{f^{*}}||U)\\ &\leq\mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f}||U)- \mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f}||U)+\operatorname{ disp}_{P^{\mathrm{COV}}}(f,f^{*})+\mathbb{E}_{P^{\mathrm{COV}}}TV(F_{f^{*}}||U)\\ &\leq\mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f}||U)+ \mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f^{*}}||U)\\ &\quad-\operatorname{disp}_{P^{\mathrm{SEM}}_{\mathrm{test}}} (f,f^{*})+\operatorname{disp}_{P^{\mathrm{COV}}}(f,f^{*})+\mathbb{E}_{P^{ \mathrm{COV}}}TV(F_{f^{*}}||U)\\ &\leq\mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f}||U)+ d_{\mathcal{F}}(P^{\mathrm{SEM}}_{\mathrm{test}},P^{\mathrm{COV}})+\lambda.\end{split}\] (18)

Intuitively speaking, Lemma 1 demonstrates that if the classifier \(f\) express high overall uncertainty on \(P^{\mathrm{SEM}}_{\mathrm{test}}\) (i.e., the predicted distribution \(F_{f}\) is close to uniform distribution), it will also tend to high uncertain prediction on \(P^{\mathrm{COV}}\) given a limited \(d_{\mathcal{F}}(P^{\mathrm{SEM}}_{\mathrm{test}},P^{\mathrm{COV}})\).

**Lemma 2**.: _[Inequality between KL and TV] For any K-class distribution \(P\) and \(Q\) on \(\{1,\cdots,K\}\) and \(\kappa>0\), the following inequality holds_

\[\frac{1}{2\kappa}KL(P||Q)+\frac{\kappa}{4}\geq TV(P||Q).\] (19)

Proof.: The proof can be found in [53] page 8. 

**Lemma 3**.: _Denote the OOD detection loss used for MSP detectors as \(\mathcal{L}_{\mathrm{reg}}\), then we have_

\[\mathbb{E}_{P^{\mathrm{SEM}}_{\mathrm{test}}}TV(F_{f}||U)\leq\mathbb{E}_{P^{ \mathrm{SEM}}_{\mathrm{test}}}\sqrt{\frac{1}{2}(\mathcal{L}_{\mathrm{reg}}(f) -\log K)}\] (20)

_where \(TV(\cdot||\cdot)\) is total variance distance (TVD). \(U\) denotes uniform distribution support on \(\mathcal{Y}=\{1,2\cdots K\}\). \(\mathcal{L}_{\mathrm{reg}}\) is defined in [7] is the cross-entropy between predicted distribution \(F_{f}(x)\) and uniform distribution \(U\)._

Lemma 2 means that minimizing the OOD detection loss will constrains the predicted distribution to be close to uniform distribution, which is a intuitive and straightforward result.

Proof.: In \(K\)-classes classification task, for any sample \(\tilde{x}\) drawn from \(P^{\mathrm{SEM}}_{\mathcal{X}}\), we have

\[\mathcal{L}_{\mathrm{reg}}(f(\tilde{x}))=KL(U||F_{f})+H(U).\] (21)

Applying Pinsker's Inequality, the following inequality holds

\[\mathcal{L}_{\mathrm{reg}}(f(\tilde{x}))=KL(F_{f}||U)+H(U)\geq 2TV(F_{f}( \tilde{x})||U)^{2}+H(U).\] (22)

Noted that \(H(U)=\log K\), we can re-write above inequality as

\[TV(F_{f}(\tilde{x})||U)\leq\sqrt{\frac{1}{2}(\mathcal{L}_{\mathrm{reg}}(f( \tilde{x}))-\log K)}.\] (23)

Then, by taking expectation on \(P^{\mathrm{SEM}}_{\mathcal{X}}\) we can get the result. 

Now we are ready to present the proof of Theorem 1.

Proof.: By the definition, the generalization error can be written as

\[\begin{split}\mathrm{GError}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{ Y}}}(f):=&\mathbb{E}_{(x,y)\sim P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}} \mathcal{L}_{\mathrm{CE}}(f(x),y)\\ =&\mathbb{E}_{(x,y)\sim P^{\mathrm{COV}}_{\mathcal{X }\mathcal{Y}}}[KL(P_{true}||F_{f}(x))+H(P_{true})]\end{split}\] (24)

where \(P_{true}\) is the target distribution given input \(x\) (i.e., the true class distribution) and \(\mathcal{L}_{\mathrm{CE}}(\cdot)\) denotes the cross-entropy loss.

Applying Lemma 2, for any \(x\) we have

\[KL(P_{true}||F_{f})\geq\frac{1}{2\kappa}(TV(P_{true}||F_{f}))+\frac{\kappa}{4}.\] (25)

By the sub-additivity of TVD, we have

\[\begin{split} KL(P_{true}||F_{f}(x))\geq&\frac{1}{ 2\kappa}(TV(P_{true}||F_{f}))+\frac{\kappa}{4}\\ \geq&\frac{1}{2\kappa}[TV(P_{true}||U)-TV(F_{f}(x) ||U)]+\frac{\kappa}{4}.\end{split}\] (26)

Taking expectation on \(P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}\), we have

\[\begin{split}\mathrm{GError}_{P^{\mathrm{COV}}_{\mathcal{X} \mathcal{Y}}}(f)=&\mathbb{E}_{(x,y)\sim P^{\mathrm{COV}}_{ \mathcal{X}\mathcal{Y}}}KL(P_{true}||F_{f}(x))+H(P_{true})\\ \geq&\frac{1}{2\kappa}\mathbb{E}_{(x,y)\sim P^{ \mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}[TV(P_{true}||U)-TV(F_{f}(x)||U)]+\frac{ \kappa}{4}+\mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true}) \end{split}\] (27)

Applying Lemma 1 and Lemma 2,

\[\begin{split}\mathrm{GError}_{P^{\mathrm{COV}}_{\mathcal{X}}}(f)=& \mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}KL[P_{true}||F_{f}(x)]+ \mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true})\\ \geq&\frac{1}{2\kappa}\mathbb{E}_{P^{\mathrm{COV}}_ {\mathcal{X}\mathcal{Y}}}[TV(P_{true}||U)-TV(F_{f}(x)||U)]+\frac{\kappa}{4}+ \mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true})\\ \geq&\frac{1}{2\kappa}\mathbb{E}_{P^{\mathrm{COV}}_ {\mathcal{X}\mathcal{Y}}}TV(P_{true}||U)-\frac{1}{2\kappa}\mathbb{E}_{P^{ \mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}TV(F_{f}||U)\\ \geq&\frac{1}{2\kappa}\mathbb{E}_{P^{\mathrm{COV}}_ {\mathcal{X}\mathcal{Y}}}TV(P_{true}||U)-\frac{1}{2\kappa}\mathbb{E}_{P^{ \mathrm{SEM}}_{\mathcal{X}}}TV(F_{f}||U)\\ &-\frac{1}{2\kappa}d_{\mathcal{F}}(P^{\mathrm{COV}}_{\mathcal{X} \mathcal{Y}},P^{\mathrm{SEM}}_{\mathcal{X}})-\frac{\lambda}{2\kappa}+\frac{ \kappa}{4}+\mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true}) \\ \geq&\frac{1}{2\kappa}\mathbb{E}_{P^{\mathrm{COV}}_ {\mathcal{X}\mathcal{Y}}}TV(P_{true}||U)-\frac{1}{2\kappa}\mathbb{E}_{P^{ \mathrm{SEM}}_{\mathcal{X}}}\sqrt{\frac{1}{2}(\mathcal{L}_{\mathrm{reg}}(f)- \log K)}\\ &-\frac{1}{2\kappa}d_{\mathcal{F}}(P^{\mathrm{COV}}_{\mathcal{X} \mathcal{Y}},P^{\mathrm{SEM}}_{\mathcal{X}})-\frac{1}{2\kappa}\lambda+\frac{ 4}{\kappa}+\mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true}).\end{split}\] (28)

Given the fact that \(P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}\), \(P_{true}\) are both fixed, \(H(P_{true})\) and \(TV(P_{true}||U)\) are constants for each \(x\). Finally, we get

\[\mathrm{GError}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}(f)\geq C-\frac{1}{2 \kappa}\mathbb{E}_{P^{\mathrm{SEM}}_{\mathcal{X}}}\sqrt{\frac{1}{2}(\mathcal{ L}_{\mathrm{reg}}(f)-\log K)}-\frac{1}{2\kappa}d_{\mathcal{F}}(P^{\mathrm{COV}}_{ \mathcal{X}},P^{\mathrm{SEM}}_{\mathcal{X}}),\] (29)

where \(C=\frac{1}{2\kappa}(\mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}}}TV(P_{true}||U)- \lambda+8)+\mathbb{E}_{P^{\mathrm{COV}}_{\mathcal{X}\mathcal{Y}}}H(P_{true})\) is a constant. 

## Appendix B Experimental Details

### Datasets details

**ID datasets \(P^{\mathrm{ID}}\).** ID datasets are chosen following common practice in OOD detection. We use CIFAR-10, CIFAR-100 and ImageNet-200 as \(P^{\mathrm{ID}}\). ImageNet-200 is a subset of the original ImageNet-1K introduced by [20, 5].

**Auxiliary OOD datasets \(P^{\mathrm{SEM}}_{\mathrm{train}}\).** For CIFAR experiments, we use ImageNet-RC and TIN-597 as auxiliary datasets. ImageNet-RC is a down-sampled variant of the ImageNet-1K, which consistsof 1000 classes and 1,281,167 images. We also conduct experiments on TIN-597 as an alternative for ImageNet-RC. TIN-597 is introduced by recent work [5]. The resolutions of ID and auxiliary samples are both \(64\times 64\). For ImageNet experiments, we use a subset of ImageNet-1K consisting of 200 classes as ID datasets. The remaining images belong to other 800 classes are utilized as auxiliary datasets. The resolutions of ID and auxiliary images are both \(224\times 224\).

**OOD detection test datasets \(P_{\text{test}}^{\text{SEM}}\).** In CIFAR experiments, following standard practice [8], we use SVHN [40], Textures [42], Places365 [41], iSUN [44], LSUN-C and LSUN-R [43] to evaluate the OOD detection performance. \(\circ\) The SVHN test set comprises 26,032 color images of house numbers. \(\circ\) Textures (Describable Textures Dataset, DTD) consists of 5,640 images depicting natural textures. \(\circ\) Places365 dataset consists scenic images of 365 different categories. Each class consists of 900 images. \(\circ\) The iSUN dataset is a subset of the SUN database with 8,925 images. \(\circ\) The Large-scale Scene Understanding dataset (LSUN) comprises a testing set with 10,000 images of 10 different scenes. LSUN offers two datasets, LSUN-C and LSUN-R. In LSUN-C, the original high-resolution images are randomly cropped into \(32\times 32\). Meanwhile, in LSUN-R, the images are resized to \(32\times 32\). In ImageNet experiments, we follow the settings of [5], where OpenImage-O [54], SSB-hard [55], Textures [42], iNaturalist [45] and NINCO [47] are selected as OOD detection test datasets. \(\circ\) OpenImage-O contains 17632 manually filtered images and is 7.8 \(\times\) larger than the ImageNet dataset. \(\circ\) SSB-hard is selected from ImageNet-21K. It consists of 49K images and 980 categories. \(\circ\) Textures (Describable Textures Dataset, DTD) consists of 5,640 images depicting natural textures. \(\circ\) iNaturalist consists of 859000 images from over 5000 different species of plants and animals. \(\circ\) NINCO consists with a total of 5879 samples of 64 classes which are non-overlapped with ImageNet-1K.

**OOD generalization test datasets \(P^{\text{COV}}\).** Following previous work [4], we corrupt the original test data with Gaussian noise of zero mean and variance of 5 in the main paper. In appendix, we conduct additional experiments involving CIFAR10-C, CIFAR100-C and ImageNet-C [56] with 15 diverse types of noise.

### Implementation details

#### b.2.1 CIFAR experiments.

We use WideResNet-40-10 [57] as the backbone network, which comprises 40 layers. The widen factor is set to 10. We use SGD optimizer to train all methods with dropout strategy. The dropout rate is 0.3. The momentum is set to 0.9 and weight decay is set to 0.0005.

**Pretraining details.** The pretrained model is obtained by training WideResNet-40-10 for 200 epochs with an initial learning rate of 0.1. We decay the learning rate by a factor of 0.2 at the 60-th, 120-th, and 160-th epochs. Batch size is set to 128.

**Finetuning details.**\(\circ\) For Entropy and EBM, we finetune the pretrained model for 20 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. Following the official implementation, the weight of OOD detection regularization term is set to 0.5 and 0.1 for Entropy and EBM (finetune) respectively. The hyperparameters \(m_{\mathrm{ID}}\) and \(m_{\mathrm{OOD}}\) in EBM regularization learning are set to -25 and -7 respectively. The ID batch size is 128 and the OOD batch size is set to 256. \(\circ\) For SCONE, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The batch size is 32, the OOD batch size is 64. The margin of the OOD detection boundary is set to 1. To be aligned with most previous works in OOD detection and generalization, we assume \(P^{\text{COV}}_{\mathcal{X}}\) is unavailable during finetuning. \(\circ\) For WOODS, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The ID batch size is 32, the OOD batch size is 64. Other hyperparameters settings are consistent with SCONE. \(\circ\) For DUL, \(\alpha_{0}\) is set to 12. While finetuning on CIFAR10, the \(m_{\mathrm{ID}}\) and \(m_{\mathrm{OOD}}\) are set to 10 and 30 respectively. The weight \(\lambda,\gamma\) are set to 0.3 and 2. We train for 20 epochs with an initial learning rate of 0.00005, utilizing a cosine annealing strategy to adjust the learning rate. While finetuning on CIFAR100/TIN-597, the \(m_{\mathrm{ID}}\) and \(m_{\mathrm{OOD}}\) are set to 10 and 30 respectively. The weights \(\lambda,\gamma\) are set to 0.05 and 2 respectively. We finetune for 20 epochs with an initial learning rate of 0.00005, utilizing a cosine annealing strategy to adjust the learning rate. While finetuning on CIFAR100/ImageNet-RC, we set \(h_{0}=0\). The \(m_{\mathrm{ID}}\) and \(m_{\mathrm{OOD}}\) are set to -430 and -370 respectively. We train for 30 epochs with an initial learning rate of 0.0001, utilizing a cosine annealing strategy to adjust the learning rate.

The weights \(\lambda,\gamma\) are set to 0.1 and 1 respectively. For CIFAR-100/ImageNet-RC, we set \(\tau=2\) and otherwise \(\tau=1\). \(\circ\) For DUL\({}^{\dagger}\), we use Thompson sampling strategy [11] for OOD informativeness mining. The sampling hyperparameters are consistent with that of POEM.

**Training from scratch details.**\(\circ\) For POEM, we train from scratch for 200 epochs with an initial learning rate of 0.1, and decay the learning rate by a factor of 0.2 at the \(60\)-th, \(120\)-th, and \(160\)-th epochs following [57]. The ID and OOD batch size are set to 128 and 256 respectively. Following the official implementation, the pool of outliers consists of randomly selected 400,000 samples from auxiliary datasets, and only 50,000 samples (same size as the ID training set) are selected for training based on the boundary score. \(\circ\) For DPN, we train for 200 epochs with an initial learning rate of 0.1, and decay the learning rate by a factor of 0.2 at the 60-th, 120-th, and 160-th epochs. The Dirichlet parameters \(\alpha\) are calculated by performing ReLU plus one on the model's outputs, i.e., \(\alpha=\mathrm{ReLU}(f(x))+1\). \(\alpha_{0}\) is set to 15 and 12 respectively when training on CIFAR10 and CIFAR100. The auxiliary datasets are ImageNet and TIN-597. The ID and OOD batch size are set to 128 and 256 respectively. When training on CIFAR100/TIN-597, the OOD regularization weight \(\lambda\) is set to 0.05. In other cases, \(\lambda\) is set to 0.5.

#### b.2.2 ImageNet experiments.

We use ResNet18 [58] as the backbone network. We use SGD optimizer to train all the models. The momentum is set to 0.9.

**Pretraining details.** The pretrained model is obtained by training ResNet18 for 100 epochs with an initial learning rate of 0.1, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Batch size is set to 64.

**Finetuning details.**\(\circ\) For Energy regularized learning, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Following the official implementation, the weights of OOD detection regularization term are set to 0.1. Specifically, the \(m_{\mathrm{ID}}\) and \(m_{\mathrm{OOD}}\) in energy regularization method are set to -25 and -7 respectively. The ID batch size is 64 and the OOD batch size is set to 128. \(\circ\) For Entropy, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.001, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. The ID and OOD batch size are set to 64 and 128 respectively. Following the official implementation, the weights of OOD detection regularization term are set to 0.5.\(\circ\) For SCONE, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0005. The batch size is 32, the OOD batch size is 64. The margin of the OOD detection boundary is set to 1. To be aligned with most previous works in OOD detection and generalization, we assume \(P_{X}^{\mathrm{COV}}\) is unavailable during finetuning. \(\circ\) For WOODS, we finetune the pretrained model for 10 epochs with an initial learning rate of 0.0002, utilizing a cosine annealing strategy to adjust the learning rate. The batch size is 32, the OOD batch size is 64. Other hyperparameters of WOODS are consistent with SCONE. \(\circ\) For our DUL, the Dirichlet parameters \(\alpha\) are calculated by performing ReLU and exp operation on the model's outputs, i.e., \(\alpha=\exp(\mathrm{ReLU}(f(x)))\). For numerical stability on large scale benchmark, we measure the distributional uncertainty by the strength of Dirichlet distribution. \(\lambda,\gamma\) are set to 0.1 and 4 respectively. We set \(\tau=1\) in large-scale ImageNet experiments.

**Training from scratch details.**\(\circ\) For DPN, we train ResNet18 for 100 epochs with an initial learning rate of 0.1, utilizing a cosine annealing strategy to adjust the learning rate. The weight decay is set to 0.0001. Batch size is set to 64. The Dirichlet parameters \(\alpha\) are calculated by performing ReLU plus one on the model's outputs, i.e., \(\alpha=\mathrm{ReLU}(f(x))+1\). The ID classification loss is set to KL-divergence between predicted class distribution under Dirichlet prior and target distribution because of the inconvenience of directly setting \(\alpha_{0}\). The target distribution is obtained by label smoothing strategy with parameter of 0.01 [9]. The weight of regularization term applied on OOD auxiliary samples is 1.

### Additional Results

#### Uncertainty estimation.

We add Gaussian noise with zero mean and varying variance \(\epsilon\) on CIFAR-10 and investigate the estimated distributional uncertainty and overall uncertainty. Distributional uncertainty is measured by differential entropy. It clear that with DUL regularization, the prediction yields a low overall uncertainty and high distributional uncertainty on covariate-shifted data. We conduct experiments on CIFAR-10/ImageNet-RC and CIFAR-10/TIN-597, tabular results are shown in Tab. 4.

#### Time-consuming comparison.

We compare the time-cost of proposed DUL to other training-required OOD detection methods in Tab. 5. We run all the experiments on one single NVIDIA GeForce RTX-3090 GPU. Compared with other OOD detection methods in a finetune manner, DUL does not introduce noticeably extra cost of computation.

#### Full results with standard deviation.

Full results with standard deviation are presented this section. In CIFAR experiments, we report the mean and standard deviation in 5 random runs. In ImageNet experiments, we report the mean and

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \(\mathcal{P}_{\mathcal{N}}^{\text{in}}/\mathcal{P}_{\mathcal{X}}^{\text{max}}\) & Uncertainty type & DUL & \(\epsilon=0.0\) & \(\epsilon=2.0\) & \(\epsilon=4.0\) & \(\epsilon=6.0\) & \(\epsilon=8.0\) & \(\epsilon=10.0\) \\ \hline \multirow{4}{*}{\begin{tabular}{c} CIFAR-10 \\ ImageNet-RC \\ \end{tabular} } & \multirow{2}{*}{Distributional uncertainty} & � & -21.33 & -18.94 & -15.62 & -13.71 & -12.91 & -12.85 \\  & & ✓ & -21.23 & -19.42 & -16.96 & -15.47 & -14.85 & -14.58 \\ \cline{2-8}  & \multirow{2}{*}{Total uncertainty} & ✗ & 0.04 & 0.47 & 1.31 & 1.93 & 2.20 & 2.28 \\  & & ✓ & 0.03 & 0.17 & 0.48 & 0.77 & 0.98 & 1.14 \\ \hline \multirow{4}{*}{
\begin{tabular}{c} CIFAR-10 \\ TIN-597 \\ \end{tabular} } & \multirow{2}{*}{Distributional uncertainty} & ✗ & -20.94 & -20.14 & -18.29 & -16.23 & -14.71 & -13.72 \\  & & ✓ & -21.48 & -20.68 & -19.21 & -17.70 & -16.57 & -15.78 \\ \cline{1-1} \cline{2-8}  & \multirow{2}{*}{Total uncertainty} & ✗ & 0.06 & 0.15 & 0.51 & 1.06 & 1.55 & 1.92 \\ \cline{1-1}  & & ✓ & 0.04 & 0.08 & 0.18 & 0.34 & 0.51 & 0.68 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Mean value of estimated uncertainty on CIFAR-10-C with varying severity of Gaussian noise with zero mean and variance of \(\epsilon\).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Method & CIFAR-10/ImageNet-RC & CIFAR-10/TIN-597 & CIFAR-100/ImageNet-RC & CIFAR-100/TIN-597 \\ \hline EBM (finetune) & 354.87 & 229.12 & 355.70 & 112.47 \\ Entropy & 355.98 & 140.88 & 1108.43 & 120.85 \\ DPN & 842.05 & 75.62 & 841.69 & 80.98 \\ POEM & 615.51 & 483.04 & 825.87 & 500.01 \\ WOODS & 808.77 & 291.37 & 906.67 & 282.50 \\ SCONE & 911.83 & 183.57 & 1169.33 & 160.00 \\ DUL & 329.58 & 101.08 & 925.62 & 100.43 \\ DUL* & 598.68 & 597.82 & 544.26 & 595.25 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average execution times (s) per epoch of training required OOD detection methods. Compare to other OOD detection methods, DUL does not introduce noticeable computational cost.

standard deviation in 3 random runs to be consist with [5]. CIFAR experimental results are shown in Tab. 6. Large-scale ImageNet results are shown in Tab. 7.

### Results of different types of corruption.

We conduct additional experiments on CIFAR10-C, CIFAR100-C and ImageNet-C with 15 different types of corruption. The results validate that the proposed method can improve the overall performance under different types of corruption.

### OOD detection results on individual datasets.

We provide OOD detection results of DUL on each individual OOD detection test dataset in Tab. 8 and Tab. 9, based on the checkpoint with random seed 1.

\begin{table}
\begin{tabular}{c|c|c c|c c c} \hline \hline \multirow{2}{*}{\(\mathcal{P}^{\text{ID}}/\mathcal{P}^{\text{SEIM}}_{\text{train}}\)} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{ID/OOD generalization} & \multicolumn{3}{c}{OOD detection} \\  & & ID-Acc. \(\uparrow\) & OOD-Acc. \(\uparrow\) & FPR \(\downarrow\) & AUROC \(\uparrow\) & AUPR \(\uparrow\) \\ \hline \multirow{4}{*}{CIFAR-10} & MSP & \(96.11^{\pm 0.09}\) & \(87.35^{\pm 0.58}\) & \(41.96^{\pm 3.85}\) & \(89.28^{\pm 1.12}\) & \(68.00^{\pm 2.19}\) \\  & EBM (pretrain) & \(96.11^{\pm 0.09}\) & \(87.35^{\pm 0.58}\) & \(32.45^{\pm 3.45}\) & \(89.34^{\pm 1.21}\) & \(75.22^{\pm 2.67}\) \\ Only & Maxlogits & \(96.11^{\pm 0.09}\) & \(87.35^{\pm 0.58}\) & \(32.90^{\pm 5.51}\) & \(89.26^{\pm 1.21}\) & \(74.47^{\pm 2.55}\) \\  & Mahalanobis & \(96.11^{\pm 0.09}\) & \(87.35^{\pm 0.58}\) & \(32.53^{\pm 6.51}\) & \(93.93^{\pm 2.68}\) & \(74.96^{\pm 7.47}\) \\ \hline \multirow{4}{*}{CIFAR-10} & Entropy & \(96.04^{\pm 0.14}\) & \(72.57^{\pm 3.67}\) & \(6.63^{\pm 0.80}\) & \(98.72^{\pm 0.14}\) & \(94.00^{\pm 1.00}\) \\  & EBM (pretrain) & \(96.10^{\pm 0.23}\) & \(79.03^{\pm 2.53}\) & \(3.61^{\pm 0.71}\) & \(98.39^{\pm 0.39}\) & \(94.88^{\pm 0.91}\) \\  & POEM & \(94.32^{\pm 0.14}\) & \(78.89^{\pm 2.25}\) & \(3.32^{\pm 0.41}\) & \(98.99^{\pm 0.17}\) & \(99.38^{\pm 0.12}\) \\ ImageNet-RC & DPN & \(95.69^{\pm 0.17}\) & \(85.52^{\pm 0.51}\) & \(4.28^{\pm 0.60}\) & \(98.53^{\pm 0.17}\) & \(94.93^{\pm 0.60}\) \\  & WOODS & \(96.04^{\pm 0.16}\) & \(80.14^{\pm 1.60}\) & \(7.12^{\pm 1.54}\) & \(98.04^{\pm 0.21}\) & \(92.92^{\pm 0.56}\) \\  & SCONE & \(95.96^{\pm 0.08}\) & \(78.80^{\pm 1.57}\) & \(7.02^{\pm 1.06}\) & \(98.45^{\pm 0.12}\) & \(92.46^{\pm 0.93}\) \\  & DUL (ours) & \(96.02^{\pm 0.07}\) & \(88.01^{\pm 0.54}\) & \(5.89^{\pm 0.35}\) & \(98.47^{\pm 0.12}\) & \(92.42^{\pm 1.14}\) \\  & DUL\({}^{\dagger}\) (ours) & \(96.04^{\pm 0.03}\) & \(87.53^{\pm 0.70}\) & \(5.990^{\pm 0.25}\) & \(98.28^{\pm 0.11}\) & \(98.40^{\pm 0.36}\) \\ \hline \multirow{4}{*}{CIFAR-10} & Entropy & \(95.94^{\pm 0.00}\) & \(80.51^{\pm 0.58}\) & \(11.60^{\pm 0.54}\) & \(97.93^{\pm 0.15}\) & \(92.16^{\pm 0.50}\) \\  & EBM (pretrain) & \(95.53^{\pm 0.13}\) & \(83.67^{\pm 1.41}\) & \(19.36^{\pm 1.92}\) & \(87.51^{\pm 1.51}\) & \(83.63^{\pm 1.73}\) \\  & POEM & \(95.44^{\pm 0.18}\) & \(83.17^{\pm 1.30}\) & \(24.34^{\pm 2.48}\) & \(86.53^{\pm 1.13}\) & \(94.25^{\pm 0.53}\) \\  & DPN & \(94.39^{\pm 0.38}\) & \(79.23^{\pm 2.50}\) & \(17.27^{\pm 0.17}\) & \(94.92^{\pm 0.65}\) & \(87.67^{\pm 0.88}\) \\ TIN-597 & WOODS & \(95.57^{\pm 0.64}\) & \(83.12^{\pm 1.71}\) & \(7.580^{\pm 0.52}\) & \(98.29^{\pm 0.04}\) & \(93.39^{\pm 0.39}\) \\  & SCONE & \(95.19^{\pm 0.77}\) & \(84.68^{\pm 1.41}\) & \(8.02^{\pm 0.92}\) & \(98.22^{\pm 0.08}\) & \(93.08^{\pm 0.50}\) \\  & DUL (ours) & \(96.06^{\pm 0.08}\) & \(87.93^{\pm 0.02}\) & \(6.87^{\pm 0.82}\) & \(98.21^{\pm 0.12}\) & \(91.29^{\pm 1.18}\) \\  & DUL\({}^{\dagger}\) (ours) & \(95.94^{\pm 0.09}\) & \(\mathbf{88.10}^{\pm 0.27}\) & \(10.34^{\pm 0.34}\) & \(97.67^{\pm 0.09}\) & \(\mathbf{98.59^{\pm 0.24}}\) \\ \hline \multirow{4}{*}{CIFAR-100} & MSP & \(80.99^{\pm 0.16}\) & \(55.95^{\pm 1.38}\) & \(74.63^{\pm 2.43}\) & \(80.19^{\pm 1.65}\) & \(42.59^{\pm 2.79}\) \\  & EBM (pretrain) & \(80.99^{\pm 0.16}\) & \(55.95^{\pm 1.38}\) & \(67.42^{\pm 4.35}\) & \(82.67^{\pm 1.82}\) & \(49.35^{\pm 4.00}\) \\  & Maxlogits & \(80.99^{\pm 0.16}\) & \(55.95^{\pm 1.38}\) & \(69.32^{\pm 3.97}\) & \(82.30^{\pm 1.79}\) & \(47.60^{\pm 3.68}\) \\  & Mahalanobis & \(80.99^{\pm 0.16}\) & \(55.95^{\pm 1.38}\) & \(61.51^{\pm 3.62}\) & \(85.97^{\pm 1.22}\) & \(56.10^{\pm 3.22}\) \\ \hline \multirow{4}{*}{CIFAR-100} & Entropy & \(80.21^{\pm 0.09}\) & \(45.48^{\pm 0.78}\) & \(22.29^{\pm 1.32}\) & \(95.33^{\pm 0.28}\) & \(82.34^{\pm 1.11}\) \\  & EBM (finetune) & \(80.53^{\pm 0.22}\) & \(48.14^{\pm 0.33}\) & \(13.47^{\pm 0.48}\) & \(96.78^{\pm 0.13}\) & \(87.84^{\pm 0.56}\) \\  & POEM & \(78.15^{\pm 0.18}\) &

\begin{table}
\begin{tabular}{c|c|c c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{\(\mathcal{P}^{\mathrm{ID}}/\mathcal{P}^{\mathrm{SEM}}_{\mathrm{train}}\)} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{ID/OOD Generalization} & \multicolumn{3}{c}{OOD Detection} \\  & & \multicolumn{3}{c}{ID-Acc. \(\uparrow\)} & \multicolumn{3}{c}{OD-Acc. \(\uparrow\)} & \multicolumn{3}{c}{FPR \(\downarrow\)} & \multicolumn{3}{c}{AUROC \(\uparrow\)} & \multicolumn{3}{c}{APPR \(\uparrow\)} & \multicolumn{3}{c}{AUROC \(\uparrow\)} & \multicolumn{3}{c}{APPR \(\uparrow\)} \\ \hline \multirow{6}{*}{ImageNet-200} & MSP & \(85.15^{\pm 0.33}\) & \(74.84^{\pm 0.47}\) & \(58.23^{\pm 1.54}\) & \(86.98^{\pm 0.24}\) & \(82.27^{\pm 0.32}\) \\  & EBM (pretrain) & \(85.15^{\pm 0.33}\) & \(74.84^{\pm 0.47}\) & \(51.94^{\pm 0.82}\) & \(88.18^{\pm 0.11}\) & \(84.75^{\pm 0.08}\) \\ \cline{2-13}  & Maxlogits & \(85.15^{\pm 0.33}\) & \(74.84^{\pm 0.47}\) & \(51.62^{\pm 0.20}\) & \(88.30^{\pm 0.09}\) & \(84.71^{\pm 0.07}\) \\ \cline{2-13}  & Entropy & \(84.92^{\pm 0.30}\) & \(74.75^{\pm 0.52}\) & \(53.62^{\pm 0.76}\) & \(89.05^{\pm 0.07}\) & \(85.02^{\pm 0.08}\) \\ \cline{2-13}  & EBM (finetune) & \(84.14^{\pm 0.11}\) & \(73.31^{\pm 0.57}\) & \(50.73^{\pm 0.83}\) & \(85.74^{\pm 0.30}\) & \(82.81^{\pm 0.16}\) \\  & DPN & \(84.87^{\pm 0.30}\) & \(74.40^{\pm 0.90}\) & \(63.84^{\pm 0.70}\) & \(87.18^{\pm 0.18}\) & \(80.61^{\pm 0.35}\) \\  & WOODS & \(84.99^{\pm 0.62}\) & \(74.98^{\pm 0.46}\) & \(51.71^{\pm 2.84}\) & \(88.30^{\pm 0.56}\) & \(84.80^{\pm 0.98}\) \\  & SCONE & \(84.93^{\pm 0.71}\) & \(74.91^{\pm 0.49}\) & \(52.52^{\pm 3.54}\) & \(88.19^{\pm 0.41}\) & \(84.50^{\pm 1.08}\) \\  & DUL (ours) & \(\mathbf{85.65^{\pm 0.07}}\) & \(\mathbf{75.59^{\pm 0.12}}\) & \(\mathbf{49.14^{\pm 0.13}}\) & \(\mathbf{89.27^{\pm 0.03}}\) & \(\mathbf{85.62^{\pm 0.03}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: OOD detection and generalization performance comparison with standard variance. Substantially improvement and degradation (\(\geq 0.5\)) compare to baseline method w.r.t. MSP are highlighted in blue or red respectively. The **best** and **second** best results are in bold or underlined. Similar with CIFAR experiments, DUL establishes strong OOD detection performance (always the best or second best) without degraded generalization i.e., the entire row is either blue or black.

\begin{table}
\begin{tabular}{c|c c c c c c c c|c c c|c c} \hline \hline \multirow{2}{*}{\(\mathcal{P}^{\mathrm{ID}}/\mathcal{P}^{\mathrm{SEM}}_{\mathrm{train}}\)} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{OpenImage-O} & \multicolumn{3}{c}{SSB-hard} & \multicolumn{3}{c}{Textures} & \multicolumn{3}{c}{iNaturalist} & \multicolumn{3}{c}{NINCO} \\ \cline{3-13}  & & \multicolumn{3}{c}{FPR\(\downarrow\)} & AUROC\(\uparrow\) & FPR\(\downarrow\) & AUROC\(\uparrow\) & FPR\(\downarrow\) & AUROC\(\uparrow\) & FPR\(\downarrow\) & AUROC\(\uparrow\) & FPR\(\downarrow\) & AUROC\(\uparrow\) \\ \hline ImageNet-200/800 & DUL & 49.68 & 91.31 & 72.40 & 80.60 & 30.76 & 92.98 & 33.21 & 94.76 & 59.92 & 86.61 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Classification error rate comparison on CIFAR10-C. ID dataset is CIFAR10.

\begin{table}
\begin{tabular}{c|c|c c c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{\(\mathcal{P}^{\mathrm{ID}}/\mathcal{P}^{\mathrm{SEM}}_{\mathrm{train}}\)} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{OpenImage-O} & \multicolumn{3}{c}{SSB-hard} & \multicolumn{3}{c}{Textures} & \multicolumn{3}{c}{iNaturalist} & \multicolumn{3}{c}{NINCO} \\ \cline{3-13}  & & \multicolumn{3}{c}{FPR\(\downarrow\)} & \multicolumn{3}{c}{AUROC\(\uparrow\)} & \multicolumn{3}{c}{FPR\(\downarrow\)} & \multicolumn{3}{c}{AUROC\(\uparrow\)} & \multicolumn{3}{c}{FPR\(\downarrow\)} & \multicolumn{3}{c}{APROC\(\uparrow\)} & \multicolumn{3}{c}{APROC\(\uparrow\)} & \multicolumn{3}{c}{APPR\(\downarrow\)} & \multicolumn{3}{c}{APROC\(\uparrow\)} \\ \hline \multirow{2}{*}{\begin{tabular}{c} MSP \\ \end{tabular} } & \multirow{2}{*}{Note} & \(53.35\) & \(39.98\) & \(4.07\) & \(15.61\) & \(42.35\) & \(19.33\) & \(19.83\) & \(14.39\) & \(18.02\) & \(9.61\) & \(5.09\) & \(18.19\) & \(13.45\) & \(22.71\) & \(18.61\) & \(23.64\) \\  & Maxlogits & \(53.35\) & \(39.98\) & \(44.07\) & \(15.61\) & \(42.35\) & \(19.33\) & \(19.83\) & \(14.39\) & \(18.02\) & \(9.61\) & \(5.09\) & \(18.19\) & \(13.45\) & \(22.27\) & \(18.61\) & \(23.64\) \\  & & \(53.35\) & \(39.98\) & \(44.07\) & \(15.61\) & \(42.35\) & \(19.33\) & \(19.33\) & \(19.83\) & \(14.39\) & \(18.02\) & \(9.61\) & \(5.09\) & \(18.19\) & \(13.45\) & \(22.27\) & \(18.61\) & \(23.64\) \\ \hline \multirow{2}{*}{
\begin{tabular}{c} Entropy \\ \end{tabular} } & \multirow{2}{*}{67.20} & \(53.59\) & \(39.79\) & \(17.09\) & \(81.99\) & \(20.00\) & \(22.17\) & \(22.41\) & \(37.18\) & \(11.43\) & \(5.82\) & \(16.99\) & \(14.28\) & \(28.92\) & \(21.37\) & \(3.42\) \\  & EBM (finetune) & \(62.49\) & \(50.00\) & \(73.01\) & \(21.01\) & \(79.30\) & \(20.06\) & \(28.90\) & \(21.02\) & \(27.00\) & \(11.55\) & \(5.60\) & \(15.08\) & \(15.15\) & \(31.46\) & \(22.69\) & \(31.72\) \\  & DPN &

[MISSING_PAGE_EMPTY:23]

### Empirical evidence

Here we provide empirical supports to our intuition about energy-based OOD detection regularization. We calculate the entropy of predicted distribution before and after finetuning with Energy regularization [8]. The results show can support our claim in Section 4.

## Appendix D Discussions

### Math derivation

**Differential entropy.** The proposed DUL calculates differential entropy as OOD detection measurement. Here we detail how to calculate the differential entropy of a Dirichlet distribution. The following derivation of differential entropy is taken from [9]. The differential entropy of a Dirichlet parameterized by \(\alpha\) is calculated by

\[h[p(\mu|x)] =-\int_{S}p(\mu|x)\ln(p(\mu|x))d\mu\] (30) \[=\sum_{k}^{K}\ln\Gamma(\alpha_{k})-\ln\Gamma(\alpha_{0})-\sum_{k} ^{K}(\alpha_{k}-1)\cdot(\psi(\alpha_{k})-\psi(\alpha_{0}))\]

where \(\alpha_{0}\) is the strength of Dirichlet, i.e., \(\alpha_{0}=\sum_{K}\alpha_{k}\). \(\alpha_{k}\) denotes the \(k\)-th element in \(\alpha\). \(\Gamma\) is the Gamma function and \(\psi\) is the digamma function. Here we provide a PyTorch implementation on how to calculate distribution uncertainty measured by differential entropy.

``` defdiff_entropy(alphas): alpha0=torch.sum(alphas,dim=1) returntorch.sum( torch.lgamma(alphas)-(alphas-1)*(torch.digamma(alphas)-torch.digamma(alpha0).unsqueezee(1)), dim=1)-torch.lgamma(alpha0)logits=model(x) alpha=torch.Relu(logits)+1 diff_entropy=diff_entropy(alpha) ```

We refer interested readers to [9] and Gal's PhD Thesis [59] for more detailed math derivations.

### Discussion about Disparity Discrepancy

In section 4, we claim that a limited disparity discrepancy between test-time semantic OOD and covariate-shifted OOD is practical. Here we provide some empirical evidence and discussion to support such a claim.

**The key challenge of OOD detection lies in identifying ID-like semantic OOD.** As mentioned in recent works [38], effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data is the core challenge of OOD detection. Recent works regularize models on ID-like OOD to enhance the OOD detection performance. Since the ID-like semantic OOD samples are more difficult to be detected and more informative. For example, NTOM [17] and

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \(\mathcal{P}^{\text{ID}}\) & \(\mathcal{P}^{\text{SEM}}_{\text{train}}\) & Before & After \\ \hline CIFAR10 & ImageNet-RC & 0.11 & 1.05 \\ CIFAR10 & TIN-597 & 0.11 & 0.14 \\ CIFAR100 & ImageNet-RC & 0.92 & 3.39 \\ CIFAR100 & TIN-597 & 0.92 & 1.15 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Predictive entropy of predicted distribution on covariate-shifted OOD dataset before and after finetuning with energy-based OOD detection regularization [8].

POEM [11] utilizes greedy and Thompson sampling strategies to find semantic OOD samples which are more closely to ID. [38] proposes to explicitly discover outliers near ID by prompt learning.

**Semantic OOD and covariate OOD can be very similar in practice.** As shown in Fig. 4. There exists many similar samples from semantic OOD and covariate OOD in large-scale commonly used benchmarks. We borrow some examples from recent works [47] to show case.

### Social Impact

AI safety and trustworthiness are closely related to our work. This paper presents work to harmonize the conflicts between out-of-distribution detection methods and model generalization. The proposed method puts effort to enhance machine learning models for their safely deployment on out-of-distribution data, avoiding both undesirable behavior and degraded performance in challenging high-stake tasks. However, due to the bias from data used by current OOD detection benchmark, e.g., large-scale ImageNet, the ones using the proposed method need to carefully consider the selection of auxiliary outliers for safety-critical applications.

Figure 4: Semantic OOD samples can be very similar to ID.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In conclusion and Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provide the full set of assumptions and a complete (and correct) proof in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 6 and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: See Appendix and supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides above details in Section 6 and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have carefully reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In Appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The license and terms of use explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.