# Multilinear Mixture of Experts:

Scalable Expert Specialization through Factorization

 James Oldfield\({}^{1}\)1 Markos Georgopoulos Grigorios G. Chrysos\({}^{2}\) Christos Tzelepis\({}^{3}\)

**Yannis Panagakis\({}^{4,5}\) Mihalis A. Nicolaou\({}^{6}\) Jiankang Deng\({}^{7}\) Ioannis Patras\({}^{1}\)**

\({}^{1}\)Queen Mary University of London \({}^{2}\)University of Wisconsin-Madison \({}^{3}\)City University of London \({}^{4}\)National and Kapodistrian University of Athens \({}^{5}\)Archimedes AI, Athena RC \({}^{6}\)The Cyprus Institute \({}^{7}\)Imperial College London

Footnote 1: Corresponding author: j.a.oldfield@qmul.ac.uk

###### Abstract

The Mixture of Experts (MoE) paradigm provides a powerful way to decompose dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. However, a major challenge lies in the computational cost of scaling the number of experts high enough to achieve fine-grained specialization. In this paper, we propose the **M**ultilinear **M**ixture **of** Experts (\(\bm{\mu}\)**MoE**) layer to address this, focusing on vision models. \(\mu\)MoE layers enable scalable expert specialization by performing an implicit computation on prohibitively large weight tensors _entirely in factorized form_. Consequently, \(\mu\)MoEs (1) avoid the restrictively high inference-time costs of dense MoEs, yet (2) do not inherit the training issues of the popular sparse MoEs' discrete (non-differentiable) expert routing. We present both qualitative and quantitative evidence that scaling \(\mu\)MoE layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level, further enabling manual bias correction in CelebA attribute classification. Finally, we show qualitative results demonstrating the expert specialism achieved when pre-training large GPT2 and MLP-Mixer models with parameter-matched \(\mu\)MoE blocks at every layer, maintaining comparable accuracy. Our code is available at: https://github.com/james-oldfield/muMoE.

## 1 Introduction

The Mixture of Experts (MoE) architecture [1] has reemerged as a powerful class of conditional computation, playing the pivotal role in scaling up recent large language [2, 3, 4, 5], vision [6], and multi-modal models [7]. MoEs apply different subsets of layers (referred to as 'experts') for each input, in contrast to the traditional approach of using the same single layer for all inputs. This provides a form of input-conditional computation [8, 9, 10, 11] that is expressive yet efficient. However, through their substantial performance gains, an important emergent property of MoEs is frequently underutilized: the innate tendency of experts to specialize in distinct subtasks. Indeed, the foundational work of Jacobs et al. [12] on MoEs describes this property, highlighting how implementing a particular function with modular building blocks (experts) often leads to subcomputations that are easier to understand individually than their dense layer counterparts-with larger expert counts allowing for more fine-grained specialization.

Independent of model performance, a successful decomposition of the layer's functionality into human-comprehensible subtasks offers many significant benefits. Firstly, the mechanisms through which a network produces an output are more _interpretable_: the output is a sum of modular components, each contributing individual functionality. Yet, the value of interpretable computationextends beyond just transparency [13] and explainability [14]. An important corollary of successful task decomposition amongst experts is that layers are easier to debug and edit. Biased or unsafe behaviors can be better localized to specific experts' subcomputation, facilitating manual correction or surgery in a way that minimally affects the other functionality of the network. Addressing such behaviors is particularly crucial in the context of foundation models; being often fine-tuned as black boxes pre-trained on unknown, potentially imbalanced data distributions. Furthermore, there is evidence that traditional fairness techniques are less effective in large-scale models [15; 16]. However, to achieve fine-grained expert specialism at the class level (or more granular still), one needs the ability to significantly scale up the number of experts. When using only a small expert count, each expert is forced to process and generalize across _multiple_ distinct semantic concepts, hindering specialization. Conversely, a large expert count means each can specialize to a more specific set of semantically similar inputs. Alas, the dominating'sparse' MoE paradigm of selecting only the top-\(K\) experts [17] is not only parameter-inefficient for large expert counts, but also has several well-known issues due to its discrete expert routing-often leading to training instability and difficulties in scaling the total expert count, amongst other challenges [18; 19].

In this paper, we propose the _Multilinear Mixture of Experts_ (\(\mu\)MoE) layer to address these issues. \(\mu\)MoEs are designed to scale gracefully to dense operations involving _tens of thousands_ of experts at once through implicit computations on a factorized form of the experts' weights. Furthermore, in contrast to the dominant sparse MoEs' [17] non-differentiable nature, \(\mu\)MoEs are differentiable by design, and thus do not inherit the associated training issues. We summarize the benefits of \(\mu\)MoEs' model form over existing MoEs in Table 1. Crucially, we show evidence that scaling up the number of \(\mu\)MoE experts leads to increased expert specialism when fine-tuning foundation models for vision tasks. Our evidence is provided in three forms: (1) firstly, through the usual qualitative evaluation of inspecting inputs by their expert coefficients. Secondly (2), we further explore the _causal_ role of each expert through counterfactual interventions [20]. Lastly, (3) we show how final-layer \(\mu\)MoE expert specialism facilitates the practical task of model editing-how subcomputation in specific combinations of experts biased towards demographic subpopulations can be manually corrected through straightforward guided edits.

Building on these findings, we demonstrate that \(\mu\)MoEs offer a compelling alternative to MLPs for pre-training both vision and language models with up to \(100\)M parameters-enabling large numbers of specialized experts while maintaining comparable performance and parameter counts to the original networks' _single_ dense MLPs.

Our contributions and core claims can be summarized as follows:

* We introduce \(\mu\)MoE layers-a mechanism for computing vast numbers of subcomputations and efficiently fusing them conditionally on the input.
* We show both qualitatively (through visualization) and quantitatively (through counterfactual intervention) that _increasing the number of \(\mu\)MoE experts increases task modularity_-learning to specialize in processing just specific input classes when fine-tuning large foundation models for vision tasks. Further, we show manual editing of \(\mu\)MoE expert combinations can straightforwardly mitigate demographic bias in CelebA attribute classification.
* We pre-train both language (GPT2) and vision (MLP-mixer) \(\mu\)MoE networks, establishing experimentally that models with parameter-matched \(\mu\)MoE blocks are competitive with existing MLP blocks whilst facilitating expert specialism (qualitatively) throughout.

## 2 Related Work

Mixture of ExpertsRecent years have seen a resurgence of interest in the Mixture of Experts (MoE) architecture for input-conditional computation [17; 12; 21; 2]. One primary motivation for MoEs is their increased model capacity through large parameter count [17; 4; 2]. In contrast to a single dense layer, the outputs of multiple experts performing separate computations are combined (sometimes with multiple levels of hierarchy [22; 23]). A simple approach to fusing the outputs is by taking either a convex [23] or linear [24] combination of the output of each expert. The

\begin{table}
\begin{tabular}{l c c c} \hline  & \multicolumn{1}{c}{**Parameter**} & \multicolumn{1}{c}{**FLOPs**} \\  & **Differentiable** & **efficient** & **efficient** \\ \hline Dense MoE [1] & & & & \\ Sparse MoE [17] & & & & \\ \hline \(\mu\)**MoE (ours)** & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Benefits of the proposed \(\mu\)MoEs’ model form over existing MoEs.

seminal work of Shazeer et al. [17] however proposes to take a _sparse_ combination of only the top-\(K\) most relevant experts, greatly reducing the computational costs of evaluating them all. More recent works employ a similar sparse gating function to apply just a subset of experts [2; 25], scaling to billions [3] and trillions of parameters [4]. The discrete expert selection choice of sparse MoEs is not without its problems, however-often leading to several issues including training stability and expert under-utilization [18; 19].

Particularly relevant to this paper are works focusing on designing MoE models to give rise to more interpretable subcomputation [26; 27; 28]-hearkening back to one of the original works of Jacobs et al. [12], where experts learned subtasks of discriminating between different lower/uppercase vowels. Indeed a common observation is that MoE experts appear to specialize in processing inputs with similar high-level features. Researchers have observed MoE experts specializing in processing specific syntax [17] and parts-of-speech [29] for language models, and foreground/background [30] and image categories (e.g. 'wheeled vehicles') [24] in vision. Evidence of shared vision-language specialist is even found in the multi-modal MoEs of Mustafa et al. [7].

Several works instead target how to make conditional computation more efficient: by sharing expert parameters across layers [31], factorizing gating network parameters [32], or dynamic convolution operations [33]. Relatedly, Gao et al. [34] jointly parameterize the experts' weight matrices with a Tensor-Train decomposition [35]. However, such approach still suffers from the Sparse MoE's instability and expert under-utilization issues, and stochastic masking of gradients must be performed to lead to balanced experts. Furthermore, whilst Gao et al. [34] share parameters across expert matrices, efficient implicit computation of thousands of experts simultaneously is not facilitated, in contrast to the \(\mu\)MoE layer.

Factorized layersin the context of deep neural networks provide several important benefits. Replacing traditional operations with low-rank counterparts allows efficient fine-tuning [36] / training [37; 38], and modeling of higher-order interactions [39; 40; 41; 42; 43], and convolutions [44]. In addition to reducing computational costs, tensor factorization has also proven beneficial in the context of multi-task/domain learning [45; 46] through the sharing of parameters/low-rank factors across tasks. Furthermore, parameter efficiency through weight factorization often facilitates the design and efficient implementation of novel architectures such as polynomial networks [47; 48; 49] or tensor contraction layers [50]. The recent DFC layer in Babiloni et al. [51] also performs dynamic computation using the CP decomposition [52] like \(\mu\)MoEs. Nevertheless, the two works have very different goals and model properties due to how the weight matrices are generated. \(\mu\)MoEs take a sparse, convex combination of \(N\) explicit experts' latent factors. This consequently leads to specialized subcomputations in a way that facilitates the interpretability and editability presented in this paper. DFCs can be seen to apply an MLP to input vectors at this step in analogy, which does not provide the necessary model properties of interest here.

## 3 Methodology

We first formulate the proposed \(\mu\)MoE layer in Section 3.1, introducing 2 unique resource-efficient models and forward passes in Section 3.1.1. Finally, we show in Section 3.1.2 how \(\mu\)MoEs recover linear MoEs as a special case.

NotationWe denote scalars \(x\in\mathbb{R}\) with lower-case letters, and vectors \(\mathbf{x}\in\mathbb{R}^{I_{1}}\) and matrices \(\mathbf{X}\in\mathbb{R}^{I_{1}\times I_{2}}\) in lower- and upper-case boldface latin letters respectively. Tensors \(\mathcal{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\ldots\times I_{d}}\) of order \(d\) are denoted with calligraphic letters. We refer to the \((i_{1},i_{2},\ldots,i_{d})\)-th element of this tensor with both \(\mathcal{X}(i_{1},i_{2},\ldots,i_{d})\in\mathbb{R}\) and \(x_{i_{1}i_{2}\ldots i_{d}}\in\mathbb{R}\). Finally, we use a colon to index into all elements along a particular mode: given \(\mathcal{X}\in\mathbb{R}^{I_{1}\times I_{2}\times I_{3}}\) for example, \(\mathbf{X}_{:i_{3}}\in\mathbb{R}^{I_{1}\times I_{2}}\) or equivalently \(\mathcal{X}(:,:,i_{3})\in\mathbb{R}^{I_{1}\times I_{2}}\) is the matrix at index \(i_{3}\) of the final mode of the tensor. We use \(\mathcal{X}\times_{n}\mathbf{u}\) to denote the **mode-\(n\) (vector) product**[53] of a tensor \(\mathcal{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\ldots\times I_{N}}\) and vector \(\mathbf{u}\in\mathbb{R}^{I_{n}}\) whose resulting elements are given by \((\mathcal{X}\times_{n}\mathbf{u})_{i_{1}\ldots i_{n-1}i_{n+1}\ldots i_{N}}= \sum_{i_{n}=1}^{I_{n}}x_{i_{1}i_{2}\ldots i_{N}}u_{i_{n}}\).

### The \(\mu\)MoE layer

\(\mu\)MoEs provide a scalable way to execute and fuse large numbers of operations on an input vector by formalizing conditional computation through resource-efficient multilinear operations. A \(\mu\)MoE layer comprised of \(N\) many experts (and a single level of expert hierarchy) is parameterized by weight tensor \(\mathcal{W}\in\mathbb{R}^{N\times I\times O}\) and expert gating parameter \(\mathbf{G}\in\mathbb{R}^{I\times N}\). Given an input vector \(\mathbf{z}\in\mathbb{R}^{I}\) (denoting the hidden representation of an individual token, for example), its forward pass can be expressed through the series of tensor contractions:

\[\mathbf{a} =\phi(\mathbf{G}^{\top}\mathbf{z})\in\mathbb{R}^{N},\] \[\mathbf{y} =\mathcal{W}\times_{1}\mathbf{a}\times_{2}\mathbf{z}\] \[=\sum_{n=1}^{N}\sum_{i=1}^{I}\mathbf{w}_{ni:}z_{i}a_{n}\in \mathbb{R}^{O},\] (1)

where \(\mathbf{a}\) is the vector of expert coefficients and \(\phi\) is the entmax activation [54, 55]. The \(\mu\)MoE layer can be understood as taking a sparse, convex combination of \(N\) many affine transformations2 of input vector \(\mathbf{z}\), weighted by the coefficients in \(\mathbf{a}\). The first tensor contraction in the forward pass (\(\sum_{i}\mathbf{W}_{::}z_{i}\in\mathbb{R}^{N\times O}\)) matrix-multiplies the input vector with _every_ expert's weight matrix. The following tensor contraction with expert coefficients a takes a linear combination of the results, yielding the output vector. The forward pass can be visualized intuitively as multiplying and summing over the modes in a 3D tensor, which we illustrate in Figure 1. Furthermore, \(\mu\)MoEs readily generalize to hierarchical conditional computations by introducing additional modes to the weight tensor and corresponding vectors of expert coefficients (see Appendix E).

Footnote 2: Incementing the dimension of the second ‘input’ mode of the weight tensor \(\mathcal{W}\in\mathbb{R}^{N\times(I+1)\times O}\) and appending a \(1\) to the input vector \(\mathbf{z}\in\mathbb{R}^{I+1}\) folds a per-expert bias term into the computation.

#### 3.1.1 Computation in factorized form

Our key insight is that the dense \(\mu\)MoE forward pass over all \(N\) experts simultaneously can be **computed entirely in factorized form, needing never materialize prohibitively large weight tensors**. This allows \(\mu\)MoEs' computations to scale gracefully to many thousands of experts simultaneously, without the problematic top-\(K\) gating [17]. To achieve this, we (1) first parameterize the experts' weights \(\mathcal{W}\in\mathbb{R}^{N\times I\times O}\) with a tensor factorization and (2) re-derive fast forward passes of Equation (1) to operate solely in factorized form.

In the context of a \(\mu\)MoE layer, the various choices of tensor factorizations make different trade-offs regarding parameter/FLOP counts and rank constraints. We derive two unique resource-efficient \(\mu\)MoE variants to suit different computational budgets and choices of expert counts. We now present the derivations of the forward passes of the factorized \(\mu\)MoE models (with einsum pseudocode implementations in Appendix B):

Cp\(\mu\)MoEImposing CP structure [52, 56] of rank \(R\) on the weight tensor, we can write \(\mathcal{W}=\sum_{r=1}^{R}\mathbf{u}_{r}^{(1)}\circ\mathbf{u}_{r}^{(2)}\circ \mathbf{u}_{r}^{(3)}\in\mathbb{R}^{N\times I\times O}\) as a sum of \(R\) outer products, with factor matrices \(\mathbf{U}^{(1)}\in\mathbb{R}^{R\times N},\mathbf{U}^{(2)}\in\mathbb{R}^{R \times I},\mathbf{U}^{(3)}\in\mathbb{R}^{R\times O}\). This reduces the parameter count from \(NIO\) (such as with sparse/dense MoEs and regular \(\mu\)MoEs) to just \(R(N+I+O)\). Crucially, we can further rewrite the CP\(\mu\)MoE layer's forward pass entirely in factorized form without ever materializing the full tensor (plugging the CP-composed tensor into Equation (1)) as:

\[\mathbf{y}=\sum_{n=1}^{N}\sum_{i=1}^{I}\bigg{(}\sum_{r=1}^{R}\mathbf{u}_{r}^{( 1)}\circ\mathbf{u}_{r}^{(2)}\circ\mathbf{u}_{r}^{(3)}\bigg{)}_{ni:}z_{i}a_{n} =\sum_{r=1}^{R}\left(\mathbf{U}^{(2)}\mathbf{z}\right)_{r}\left(\mathbf{U}^{(1 )}\mathbf{a}\right)_{r}\mathbf{u}_{r}^{(3)}\in\mathbb{R}^{O},\] (2)

with Equation (2) being analogous to the fast computation in Babiloni et al. [51], only here the operations of combining the weights and producing the outputs can be expressed in a single step. Whilst the original naive CP\(\mu\)MoE forward pass has a FLOP count3 of \(NIO\), the fast computation

Figure 1: The forward pass of an (unfactorized) \(\mu\)MoE layer as a series of tensor contractions: the experts’ weight matrices (yellow 2D slices) are matrix-multiplied with the input vector and summed (weighted by the red expert coefficients).

above has just \(R(N+I+O)\) (the same number of factorized layer parameters). With moderate values of both \(R\) and \(N\), the layer becomes significantly more resource-efficient than vanilla \(\mu\)MoEs.

TR\(\mu\)MoEWe propose a second \(\mu\)MoE variant based on the Tensor Ring [58] (TR) factorization that can offer even better efficiency for large values of \(N\). In TR format, \(\mathcal{W}\in\mathbb{R}^{N\times I\times O}\) has three factor tensors: \(\mathcal{U}^{(1)}\in\mathbb{R}^{R_{1}\times N\times R_{2}}\), \(\mathcal{U}^{(2)}\in\mathbb{R}^{R_{2}\times I\times R_{3}}\), \(\mathcal{U}^{(3)}\in\mathbb{R}^{R_{3}\times O\times R_{1}}\), where \(R_{i}\) are the manually chosen ranks4. The weight tensor's elements in TR format are given by: \(w_{nio}=\text{tr}\big{(}\mathbf{U}_{:n:}^{(1)}\mathbf{U}_{:i:}^{(2)}\mathbf{U} _{:o:}^{(3)}\big{)}\)[58]. TR\(\mu\)MoE's forward passes can be computed efficiently by contracting the first two factor tensors with the input/expert coefficients vectors and then combining the results:

Footnote 4: Setting \(R_{1}=1\) recovers a Tensor Train [35]\(\mu\)MoE.

\[\mathbf{y}=\sum_{n=1}^{N}\sum_{i=1}^{I}\mathbf{w}_{ni:}z_{i}a_{n}=\sum_{r_{1} =1}^{R_{1}}\sum_{r_{3}=1}^{R_{3}}\big{(}\underbrace{\big{(}\mathcal{U}^{(1)} \times_{2}\mathbf{a}\big{)}(\mathcal{U}^{(2)}\times_{2}\mathbf{z})}_{[R_{1} \times R_{3}]}\big{)}_{r_{1}r_{3}}\mathbf{u}_{r_{3}:r_{1}}^{(3)}\in\mathbb{R} ^{O},\] (3)

yielding a modified FLOP count of \((R_{1}NR_{2}+R_{2}IR_{3}+R_{1}R_{2}R_{3}+R_{1}OR_{3})\) with just \((R_{1}NR_{2}+R_{2}IR_{3}+R_{3}OR_{1})\) parameters. With large \(N\) contributing to the computational cost only through \(R_{1}NR_{2}\), the TR\(\mu\)MoE can prove even more resource-efficient than CP\(\mu\)MoEs by choosing small values of \(R_{1},R_{2}\). We refer readers to Appendix D for a further discussion of decomposition choice, derivations of how tensor rank translates to expert matrix rank, and FLOPs comparisons.

#### 3.1.2 \(\mu\)MoEs recover dense MoEs as a special case

Finally, we note how unfactorized \(\mu\)MoE layers with a single level of expert hierarchy recover dense MoE layers [17; 11] as a special case. When computing Equation (1) over the full materialized weight tensor, one can alternatively write the output element-wise as \(y_{o}=\mathbf{a}^{\top}\mathbf{W}_{:o:}\mathbf{z}\). This highlights an interesting technical connection between neural network layers: dense MoE layers in this tensor formulation can be seen to share a similar functional form to bilinear layers, which have also found applications in interpretability [59; 60].

## 4 Experiments

We start in Section 4.1 by presenting both qualitative and quantitative experiments validating that the experts learn to specialize in processing different semantic clusters of the input data. In Section 4.2 we demonstrate one practical benefit of the learned specialism-showing how expert-conditional re-writing can correct for specific demographic bias in CelebA attribute classification. Finally, in Section 4.3 we train both large language and large vision models with \(\mu\)MoE layers throughout-providing qualitative evidence of expert specialism and model performance competitive with networks using MLP blocks. Please see Appendix H for detailed ablation studies, and Appendix I for experiments with hierarchical \(\mu\)MoEs.

Implementation detailsBefore applying the activation function to the expert coefficients we apply batch- and layer-normalization to \(\mu\)MoE layers in vision and language models respectively (see Appendix H.3 for an ablation). Interestingly, we do not find the need for any load-balancing losses. We fix the TR\(\mu\)MoE ranks to be \(R_{1}=R_{2}=4\) throughout (see Appendix D.1.2).

### Expert specialism: visualization & intervention

Our first objective is to show that **scaling \(\mu\)MoE's expert count leads to more specialized experts**. We provide evidence of this effect both qualitatively (through _visualization_) and quantitatively (through _intervention_).

To isolate the impact of \(\mu\)MoE layers and varying expert counts, we first explore the controlled setting of fine-tuning large foundation models CLIP [61]ViT-B-32 and DINO [62] on ImageNet1k (following the fine-tuning protocol in Ilharco et al. [63; 64]). Whilst fine-tuning large foundation models is an important application of \(\mu\)MoE layers in its own right (e.g. as explored later in Section 4.2 for fairer models), the ability to cheaply train many models with different \(\mu\)MoE layer configurations forms an ideal setting in which to study their properties.

#### 4.1.1 Qualitative results

We first show _random_ examples in Figure 2 of images processed (with expert coefficient \(\geq 0.5\)) by the experts by each CP\(\mu\)MoE layer (the class labels and expert coefficients are overlaid in white and green text respectively). Using only a modest number of experts (e.g. 32) appears to lead to some 'polysemanticity' [65] in experts-with some processing unrelated classes of images (e.g. 'gators', 'limos', and a 'quilt' for Expert 1 on the right). On the other hand, using a much larger number of total experts appears to yield more specialization, with many experts contributing their computation to only images of the same single class label or broader semantic category. Please see Figure 16 in the Appendix for many more random images for the first \(10\) experts per model to observe this same trend more generally, and Figure 17 for even finer-grained specialism with \(2048\)-expert \(\mu\)MoE layers.

#### 4.1.2 Quantitative results: expert monosemanticity

The qualitative evidence above hints at the potential of a prominent benefit to scaling up the number of experts with \(\mu\)MoEs. Such subjective interpretations alone about expect specialism are _hypotheses_, rather than conclusions however [66]. Similarities in images processed by the same expert give us an intuitive explanation of its function but do not show the expert's computation contributes _causally_[20, 67, 68] to the subtask of processing specific human-understandable patterns of input features [69, 70]. However, the absence of ground-truth labels for interpretable features of the input one may be interested in (e.g. specific types of textures in images, or words related to 'Harry Potter') makes this difficult to quantify in any objective or systematic manner.

Despite the absence of fine-grained labels, we _can_ quantify and compare the class-level specialism a \(\mu\)MoE expert exhibits on the ImageNet1k dataset as an (imperfect) proxy [71]. Following the causal intervention protocol of Elazar et al. [20], we ask the specific counterfactual question about solely each expert \(n\) in a \(\mu\)MoE layer in turn: _"had expert \(n\)'s weight matrix \(\mathbf{W}_{n}\) not contributed its computation, would the network's test-set accuracy for class \(c\) have dropped?"_ Practically speaking, given a network fine-tuned with an \(\mu\)MoE layer, we achieve this by intervening in the forward pass by zeroing the \(n^{\text{th}}\) expert's weight matrix \(\mathbf{W}_{n}:=\mathbf{0}\), leaving every other aspect of the forward pass completely untouched. Let the elements of \(\mathbf{y},\hat{\mathbf{y}}^{(n)}\in\mathbb{R}^{C}\) denote the test set accuracy for the \(C=1000\) ImageNet1k classes, pre- and post-intervention of expert \(n\) respectively. We collect the normalized difference to per-class accuracy in the vector \(\mathbf{d}^{(n)}\), whose elements are given by \(d_{c}^{(n)}=(y_{c}-\hat{y}_{c}^{(n)})/y_{c}\). At the two extremes, when the full network's accuracy for class \(c\) drops completely from \(y_{c}\) to \(0\) upon manually excluding expert \(n\)'s computation we get \(d_{c}^{(n)}=1\), whilst \(d_{c}^{(n)}=0\) means the absence of the subcomputation did not change class \(c\)'s test set accuracy at all. We thus estimate the 'class-level polysemanticity' of expert \(n\) as the distance between

Figure 3: **Higher expert counts lead to more monosemantic experts**: mean expert class-level polysemanticity of Equation (4) (\(\downarrow\)) as a function of the total number of experts. Results are shown for both CLIP ViT-B-32 and DINO models fine-tuned on ImageNet1k with CP\(\mu\)MoE layers.

Figure 2: Specialization in \(256\) vs \(32\) total expert CP\(\mu\)MoE layers (fine-tuned on CLIP ViT-B-32). Each row displays _randomly_ selected images processed (with coefficient \(\geq 0.5\)) by the first few experts for the two models. The more we scale the expert count, the greater the apparent expert specialist (to single visual themes or image categories).

[MISSING_PAGE_FAIL:7]

bias without requiring images' sensitive attribute value at test time). These are shown in Table 2 for the two different experiments on CelebA, where the proposed intervention outperforms baseline alternative methods in the majority of settings. Please see Appendix J for details about the baseline methods and fairness metrics used, and further discussion of results.

### Large language/vision \(\mu\)MoE networks

Finally, we train from scratch \(12\) layer \(124\)M-parameter GPT-2 [81] LLMs on OpenWebText [82] for the language domain and \(8\) layer S-16 variant7 MLP-Mixers [80] on ImageNet1k [83] for vision. We replace _every_ MLP block's 2 linear layers with 2 \(\mu\)MoE layers. Each token \(t\)'s input vector \(\mathbf{z}_{t}\in\mathbb{R}^{I}\) is therefore transformed with \(\mu\)MoE blocks of the form:

Footnote 7: The S-16 model is the largest configuration that fits into 4x80GB A100 GPUs using the original paper’s batch size of 4096.

\[\mathbf{y}_{t}=\sum_{n_{2}=1}^{N}\sum_{h=1}^{H}\mathbf{w}_{n_{2}h :\text{{\sf GELU}}}^{(2)}\bigg{(}\sum_{n_{1}=1}^{N}\sum_{i=1}^{I}\mathbf{w}_{n _{1i}:}^{(1)}z_{ti}a_{tn_{1}}\bigg{)}_{h}a_{tn_{2}},\quad\mathbf{a}_{t}=\phi( \mathbf{G}^{\top}\mathbf{z}_{t}),\]

where \(\mathbf{a}_{t}\in\mathbb{R}^{N}\) are the expert coefficients for each specific token and block, \(H\) is the dimension of the block's hidden layer, and \(\mathcal{W}^{(1)}\in\mathbb{R}^{N\times I\times H},\mathcal{W}^{(2)}\in \mathbb{R}^{N\times H\times O}\) are the (implicit) \(\mu\)MoE weight

Figure 4: Top-activating patches (top rows) and their full images (second rows) for the first 3 experts across 2 CP\(\mu\)MoE-e64 layers in \(\mu\)MoE MLP-mixer [80] models-\(\mu\)MoE blocks exhibit coarse-grained specialism (e.g. texture) earlier and more fine-grained specialism (e.g. objects) deeper in the network.

tensors for each of the two layers. We manually set the \(\mu\)MoE ranks to parameter-match each original network and set the number of experts (per block) to \(N=64\) for vision models and \(N=256\) for LLMs. Consequently, with this configuration, **each layer's \(\bm{\mu}\)MoE block performs computations with \(N\) experts yet has the same parameter counts and FLOPs as a single, dense MLP block**.

\(\bm{\mu}\)MoE-MixerFor vision, our key findings are that earlier \(\mu\)MoE channel-mixing blocks' experts appear (qualitatively) to exhibit specialisms to colors, shapes, and textures, whilst later layers exhibit more object-specific specialization. We plot the patches from the training set for which each expert most contributes its computation in Figure 4 for both a shallow and deep layer to illustrate this-earlier layers' experts contribute strongly to the processing of similar _patches_ (top rows, e.g. specific edges) whilst later layers' experts process tokens based more on the similarity of their surrounding semantic context (bottom rows, e.g. images of animals). We further show in Figure 12 results for the first 2 experts across all 8 blocks where such scale-specific specialism is apparent across the entire network.

\(\bm{\mu}\)MoE-Gpt2For LLMs, we see promising qualitative evidence of experts specializing throughout a corpus of 1M generated 100-token sequences. At layer 5, for example, the generated tokens that use expert 8 with the highest coefficient are compound adjectives (Figure 5), whilst expert 37 most highly activates for equality and comparison operators in code and scientific text (please see examples of

Figure 5: Top-activating generated tokens for 4 manually selected experts for GPT-2 trained with CP\(\bm{\mu}\)MoE blocks at every layer (each token is highlighted by the coefficient of the expert in question), exhibiting specializations to concepts including compound adjectives and equality operators.

many unfiltered experts in Figures 13 and 14). Whilst monosemanticity is not always attained, \(\mu\)MoE layers nonetheless facilitate a level of specialism not facilitated by dense MLP layers.

One important result here is that \(\mu\)MoE networks in this setup are significantly more parameter-efficient than both dense and sparse MoEs with the same expert count, as shown in Table 4. For example, GPT-\(2\) models with \(256\) sparse/dense MoE experts require a prohibitive \(14.5\)B MLP parameters alone, relative to just \(57\)M MLP parameters with \(\mu\)MoEs of the same expert counts.

\(\mu\)MoE performanceFinally, we substantiate our claim that networks pre-trained and fine-tuned with parameter-matched \(\mu\)MoE layers are competitive with their existing linear layer alternatives across multiple domains/machine learning tasks. We present in Table 3 the performance results for MLP-Mixer S-\(16\)[80], NanoGPT GPT-\(2\)[81], and (fine-tuned) CLIP ViT-B-32 [61] models on the OWT and ImageNet1k datasets. Following Section 4.1.1, we replace all linear layers with \(\mu\)MoE blocks (and a single \(\mu\)MoE final layer for fine-tuning CLIP). We initialize all linear layers following the default PyTorch \(U[-k,k]\) initialization for a fair comparison. Please see Appendix F for experimental details and learning curves, and Appendix I for experiments with varying expert count and hierarchical \(\mu\)MoEs. Crucially, whilst \(\mu\)MoE layers provide additional interpretability benefits through scalable expert specialization, they do not sacrifice accuracy when parameter-matched to MLP blocks, as seen from the comparable performance.

## 5 Conclusion

In this paper, we introduced the Multilinear Mixture of Experts layer (\(\mu\)MoE). We demonstrated that larger expert counts lead to increased specialization, and how \(\mu\)MoE layers make this computationally tractable through factorized forward passes. \(\mu\)MoEs scale to large expert counts much more gracefully than existing MoEs, yet avoid the issues from popular gating mechanisms. As a further practical example of \(\mu\)MoE's task decomposition, we illustrated how manual guided edits can be made to correct bias towards demographic subpopulations in fine-tuned foundation models. Having also shown matching performance in addition to expert specialism in both large vision and language models, we believe \(\mu\)MoE layers constitute an important step towards facilitating increasingly performant models that do not trade off fairness/interpretability for accuracy.

LimitationsFirstly, it is important to state again that our quantitative evaluation only captures expert behavior on the test set, not out-of-distribution data [70, 84]. Furthermore, expert specialism in large models is only demonstrated qualitatively (through the expert coefficients) due to the absence of fine-grained labels. Developing ways of quantifying fine-grained expert specialism is an important direction for future research. Finally, our experimental results demonstrated comparable accuracies of \(\mu\)MoE networks only for models with parameter counts on the order of 100 million. Where resources permit, future work should explore the scalability of expert specialization and performance of \(\mu\)MoEs in even larger-scale LLMs.

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Model} & NanoGPT (gpt2) & MLP-Mixer (S-\(16\)) \\  & \(N=256\) & \(N=64\) \\ \hline Dense/Sparse MoE & \(14.5\)**B** & \(1.13\)**B** \\
**CP\({}_{\mu}\)MoE** & **57.0M** & **17.7M** \\
**TB\({}_{\mu}\)MoE** & **57.4M** & **17.4M** \\ \hline \hline \end{tabular}
\end{table}
Table 4: MLP parameters required for networks with the same expert counts.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **MLP-mixer** S-\(16\) (ImageNET\(1\)k) & **GPT-2 NanoGPT** (OWT) & **CLIP** B-\(32\) (ImageNET\(1\)k) \\  & Val. acc. (\(\uparrow\)) & \#params & Val. loss (\(\downarrow\)) & \#params & Val. acc. (\(\uparrow\)) & \#params \\ \hline MLPs & 70.31 & 18.5M & **2.876** & 124M & 77.99 & 769K \\
**TR\(\mu\)MoEs** & 71.26 & 18.3M & 2.886 & 124M & **78.71** & 771K \\
**CP\(\mu\)MoEs** & **71.29** & 18.6M & 2.893 & 124M & 78.07 & 769K \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of \(\mu\)MoEs and dense MLPs across different models and tasks. We use \(N=64\)\(\mu\)MoE experts for the two vision tasks and \(N=256\) for GPT2. MLP mixers and GPT2s are pre-trained for 300 epochs and 100k iterations respectively, whilst CLIP is fine-tuned for 10 epochs.

## References

* Jacobs et al. [1991] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.
* Jiang et al. [2021] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mixtral of experts, 2024.
* Lepikhin et al. [2021] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In _Int. Conf. Learn. Represent. (ICLR)_, 2021.
* Fedus et al. [2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.
* Gale et al. [2023] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. _Proceedings of Machine Learning and Systems_, 5, 2023.
* Riquelme et al. [2021] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 34:8583-8595, 2021.
* Mustafa et al. [2022] Basil Mustafa, Carlos Riquelme Ruiz, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with LIMoe: the language-image mixture of experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 2022.
* Ha et al. [2017] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In _Int. Conf. Learn. Represent. (ICLR)_, 2017.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 30, 2017.
* Han et al. [2021] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. _IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)_, 44(11):7436-7456, 2021.
* Chen et al. [2020] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: Attention over convolution kernels. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 11030-11039, 2020.
* Jacobs et al. [1991] Robert A Jacobs, Michael I Jordan, and Andrew G Barto. Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. _Cognitive science_, 15(2):219-250, 1991.
* Lipton [2018] Zachary C. Lipton. The mythos of model interpretability. _Communications of the ACM_, 61(10):36-43, September 2018. ISSN 1557-7317.
* Ribeiro et al. [2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.
* Mao et al. [2023] Yuzhen Mao, Zhun Deng, Huaxiu Yao, Ting Ye, Kenji Kawaguchi, and James Zou. Last-layer fairness fine-tuning is simple and effective for neural networks. In _Proceedings of the 2nd Workshop on Spurious Correlations, Invariance and Stability at the International Conference on Machine Learning (ICML 2023)_, 2023.
* Cherepanova et al. [2021] Valeria Cherepanova, Vedant Nanda, Micah Goldblum, John P Dickerson, and Tom Goldstein. Technical challenges for training fair neural networks. _arXiv preprint arXiv:2102.06764_, 2021.
* Shazeer et al. [2017] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _Int. Conf. Learn. Represent. (ICLR)_, 2017.

* [18] Muqeeth Mohammed, Haokun Liu, and Colin Raffel. Models with conditional computation learn suboptimal solutions. In _I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification_, 2022.
* [19] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. In _Int. Conf. Learn. Represent. (ICLR)_, 2024.
* [20] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. _Transactions of the Association for Computational Linguistics_, 9:160-175, 2021.
* [21] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. In _Int. Conf. Mach. Learn. Worksh. (ICMLW)_, 2015.
* [22] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In _Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)_, volume 2, pages 1339-1344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791.
* [23] David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. In _Int. Conf. Mach. Learn. Worksh. (ICMLW)_, volume abs/1312.4314, 2013.
* [24] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 32, 2019.
* [25] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In _Int. Conf. Mach. Learn. (ICML)_, pages 5547-5569. PMLR, 2022.
* [26] Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez, Damien Jose, Ahmed H Awadallah, and Jianfeng Gao. Sparsely activated mixture-of-experts are robust multi-task learners. _arXiv preprint arXiv:2204.07689_, 2022.
* [27] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah Smith, and Luke Zettlemoyer. Demix layers: Disentangling domains for modular language modeling. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.407.
* [28] Aya Abdelsalam Ismail, Sercan O Arik, Jinsung Yoon, Ankur Taly, Soheil Feizi, and Tomas Pfister. Interpretable mixture of experts. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* [29] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In _Int. Conf. Mach. Learn. (ICML)_, 2021.
* [30] Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu Yuan. Residual mixture of experts, 2022.
* [31] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper. In _Conf. on Artifi. Intel. (AAAI)_, volume 36, pages 8779-8787, 2022.
* [32] Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. _arXiv preprint arXiv:1312.4461_, 2013.
* [33] Yunsheng Li, Yinpeng Chen, Xiyang Dai, mengchen liu, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei Chen, and Nuno Vasconcelos. Revisiting dynamic convolution via matrix decomposition. In _Int. Conf. Learn. Represent. (ICLR)_, 2021.
* [34] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. Parameter-efficient mixture-of-experts architecture for pre-trained language models. In _Proceedings of the 29th International Conference on Computational Linguistics_, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.
* [35] I. Oseledets. Tensor-train decomposition. _SIAM J. Sci. Comput._, 33:2295-2317, 2011.
* [36] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.

* Novikov et al. [2015] Alexander Novikov, Dmitriu Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 28, 2015.
* Garipov et al. [2016] Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate tensorization: compressing convolutional and fc layers alike. _arXiv preprint arXiv:1611.03214_, 2016.
* Novikov et al. [2017] Alexander Novikov, Mikhail Trofimov, and Ivan Oseledets. Exponential machines. In _Int. Conf. Learn. Represent. Worksh._, 2017.
* Georgopoulos et al. [2021] Markos Georgopoulos, James Oldfield, Mihalis A Nicolaou, Yannis Panagakis, and Maja Pantic. Mitigating demographic bias in facial datasets with style-based multi-attribute transfer. _Int. J. Comput. Vis. (IJCV)_, 129(7):2288-2307, 2021.
* Babiloni et al. [2020] Francesca Babiloni, Ioannis Marras, Gregory Slabaugh, and Stefanos Zafeiriou. Tesa: Tensor element self-attention via matricization. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 13945-13954, 2020.
* Georgopoulos et al. [2020] Markos Georgopoulos, Grigorios Chrysos, Maja Pantic, and Yannis Panagakis. Multilinear latent conditioning for generating unseen attribute combinations. In _Int. Conf. Mach. Learn. (ICML)_, 2020.
* Cheng et al. [2024] Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, and Volkan Cevher. Multilinear operator networks, 2024.
* Kossaifi et al. [2020] Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M. Hospedales, and Maja Pantic. Factorized higher-order cnns with an application to spatio-temporal emotion estimation. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_. IEEE, June 2020.
* Bulat et al. [2020] Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Incremental multi-domain learning with network latent tensor factorization. In _Conf. on Artifi. Intel. (AAAI)_, volume 34, pages 10470-10477, 2020.
* Yang and Hospedales [2017] Yongxin Yang and Timothy M. Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In _Int. Conf. Learn. Represent. (ICLR)_, 2017.
* Chrysos et al. [2020] Grigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Stefanos Zafeiriou. P-nets: Deep polynomial neural networks. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 7325-7335, 2020.
* Chrysos et al. [2021] Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Panagakis, and Stefanos P Zafeiriou. Deep polynomial neural networks. _IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)_, page 1-1, 2021. ISSN 1939-3539.
* Babiloni et al. [2021] Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Grigorios Chrysos, and Stefanos Zafeiriou. Poly-nl: Linear complexity non-local layers with 3rd order polynomials. In _Int. Conf. Comput. Vis. (ICCV)_, pages 10518-10528, 2021.
* Kossaifi et al. [2017] Jean Kossaifi, Aran Khanna, Zachary Lipton, Tommaso Furlanello, and Anima Anandkumar. Tensor contraction layers for parsimonious deep nets. In _IEEE Conf. Comput. Vis. Pattern Recog. Worksh. (CVPRW)_, pages 26-32, 2017.
* Babiloni et al. [2023] Francesca Babiloni, Thomas Tanay, Jiankang Deng, Matteo Maggioni, and Stefanos Zafeiriou. Factorized dynamic fully-connected layers for neural networks. In _Int. Conf. Comput. Vis. Worksh. (ICCVW)_, pages 1374-1383, October 2023.
* Hitchcock [1927] Frank Lauren Hitchcock. The expression of a tensor or a polyadic as a sum of products. _Journal of Mathematics and Physics_, 6:164-189, 1927.
* Kolda and Bader [2009] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. _SIAM Review_, 51(3):455-500, 2009. doi: 10.1137/07070111X.
* Peters et al. [2019] Ben Peters, Vlad Niculae, and Andre F. T. Martins. Sparse sequence-to-sequence models. In Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1504-1519, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1146.
* Correia et al. [2019] Goncalo M. Correia, Vlad Niculae, and Andre F. T. Martins. Adaptively sparse transformers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2174-2184, HongKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1223.
* Carroll and Chang [1970] J. Douglas Carroll and Jih Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of "eckart-young" decomposition. _Psychometrika_, 35:283-319, 1970.
* [57] fvcore: Flop counter for pytorch models. https://github.com/facebookresearch/fvcore. Accessed: 2024-05-16.
* Zhao et al. [2016] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decomposition. _ArXiv_, abs/1606.05535, 2016.
* Sharkey [2023] Lee Sharkey. A technical note on bilinear layers for interpretability. _arXiv preprint arXiv:2305.03452_, 2023.
* Pearce et al. [2024] Michael T. Pearce, Thomas Dooms, and Alice Rigg. Weight-based decomposition: A case for bilinear MLPs, 2024.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Int. Conf. Mach. Learn. (ICML)_, 2021.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Int. Conf. Comput. Vis. (ICCV)_, 2021.
* Ilharco et al. [2022] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 35:29262-29277, 2022.
* Ilharco et al. [2023] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In _Int. Conf. Learn. Represent. (ICLR)_, 2023.
* Elhage et al. [2022] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. _arXiv preprint arXiv:2209.10652_, 2022.
* Rauker et al. [2023] Tilman Rauker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 464-483. IEEE, 2023.
* Ravfogel et al. [2021] Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In Arianna Bisazza and Omri Abend, editors, _Proceedings of the 25th Conference on Computational Natural Language Learning_, pages 194-209, Online, November 2021. Association for Computational Linguistics.
* Meng et al. [2022] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 35:17359-17372, 2022.
* Rudin [2019] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. _Nature machine intelligence_, 1(5):206-215, 2019.
* Casper [2023] Stephen Casper. Broad critiques of interpretability research. 2023. URL https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/gw69uug255garfjYN4.
* Hod et al. [2021] Shlomi Hod, Daniel Filan, Stephen Casper, Andrew Critch, and Stuart Russell. Quantifying local specialization in deep neural networks. _arXiv preprint arXiv:2110.08058_, 2021.
* Buolamwini and Gebru [2018] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _Conference on fairness, accountability and transparency_, pages 77-91. PMLR, 2018.
* Gebru et al. [2021] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, 2021.
* Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Int. Conf. Comput. Vis. (ICCV)_, December 2015.

* [75] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In _Int. Conf. Learn. Represent. (ICLR)_, 2023.
* [76] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 2016.
* [77] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 9322-9331, 2020.
* [78] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 33:728-740, 2020.
* [79] Mohshan Alvi, Andrew Zisserman, and Christoffer Nellaker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In _Proceedings of the European Conference on Computer Vision (ECCV) Workshops_, 2018.
* [80] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. MLP-mixer: An all-MLP architecture for vision. _Adv. Neural Inform. Process. Syst. (NeurIPS)_, 34:24261-24272, 2021.
* [81] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 2019. URL https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.
* [82] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.
* [83] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 248-255, 2009.
* [84] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viegas, and Martin Wattenberg. An interpretability illusion for bert. _arXiv preprint arXiv:2104.07143_, 2021.
* [85] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In _Int. Conf. Learn. Represent. (ICLR)_, 2022.
* [86] Ledyard R. Tucker. Some mathematical notes on three-mode factor analysis. _Psychometrika_, 31:279-311, 1966.
* [87] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1(3):211-218, 1936.
* [88] Pratyusha Sharma, Jordan T. Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. In _Int. Conf. Learn. Represent. (ICLR)_, 2024.
* [89] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 7959-7971, 2022.
* [90] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In _IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)_, pages 8919-8928, 2020.

## Appendix

### Table of Contents

* 1 Broader impact
* 2 Fast \(\mu\)MoE implementations
	* 2.1 CP\(\mu\)MoE einsum implementation
	* 2.2 TR\(\mu\)MoE einsum implementation
* 3 \(\mu\)MoE forward pass visualization
* 4 Decomposition choice, matrix rank, and computational cost
	* 4.1 Tensor ranks to matrix rank
	* 4.2 Why is low-rankness a reasonable assumption?
	* 4.3 MoE/\(\mu\)MoE parameter count comparisons
* 5 Hierarchical \(\mu\)MoE model derivations
	* 5.1 Hierarchical CP\(\mu\)MoE
	* 5.2 Hierarchical TR\(\mu\)MoE
* 6 Experimental details
	* 6.1 Network configurations and hyperparamters
	* 6.2 Weight initialization
* 7 Expert specialism: additional results
	* 7.1 Large scale models
	* 7.2 LLM steering
	* 7.3 CLIP ViT-B-32
* 8 Ablation studies
	* 8.1 Entmax vs softmax
	* 8.2 Fast forward pass computation speedups
	* 8.3 Batch normalization
	* 8.4 Expert load
* 9 Additional performance results
	* 1.1 CLIP ViT-B-32 ImageNet1k ablations
	* 1.2 Hierarchical \(\mu\)MoEs
	* 1.3 Comparisons to dense/sparse MoEs
	* 10.4 Fairness baselines & metric details
	* 11.1 Fairness: additional results
	* 11.2 Model re-writing
	* 11.3 **L*
* **NeurIPS Paper Checklist**

## Appendix A Broader impact

This paper presents work whose goal is to advance the field of _interpretable_ machine learning. Our goal is not to improve model capabilities but rather an orthogonal one of designing architecturesmore interpretable and controllable. As with many work with an interpretability focus, however, the \(\mu\)MoE layer could nonetheless facilitate the further development of SOTA models through its more expressive computation. We thus encourage the development of further guardrails against potentially harmful dual-uses of such technology. We release our code upon acceptance to facilitate further research along such lines.

## Appendix B Fast \(\mu\)MoE implementations

We here detail how to implement the fast forward passes of the \(\mu\)MoE models in a batch-wise manner, where each mini-batch element is a 2D matrix of shape \(\mathbf{Z}\in\mathbb{R}^{T\times C}\) (with 'token' and 'channel' dimensions) with PyTorch and einops' [85] einsum:

### CP\(\mu\)MoE einsum implementation

The CP\(\mu\)MoE forward pass can be implemented with:

```
#CPmuMoE(r=CPrank,b=batch_dim,t=tokens,
#i=input_dim,o=output_dim,a[e]=expert_coefs,n*=expert_dims) y=einsum(G3,a[0]@G1.T,z@G2.T,'r o,b tr,b tr ->b t o') ```

And a two-level hierarchical CP\(\mu\)MoE with an additional factor matrix as:

```
#CPmuMoE(r=CPrank,b=batch_dim,t=tokens,
#i=input_dim,o=output_dim,a[e]=expert_coefs,n*=expert_dims)
#AM2-levelhierarchicalCPmuMoE,assumingGi'sofappropriatensape y=einsum(G4,a[0]@G1.T,a[1]@G2.T,z@G3.T,
1'r o,b tr,b tr ->b t o') ```

### TR\(\mu\)MoE einsum implementation

TR\(\mu\)MoEs can be implemented with:

```
#TRmuMoE(r*=TRranks,b=batch_dim,t=tokens,
#i=input_dim,o=output_dim,a[e]=expert_coefs,n*=expert_dims)
#batchedmode-2tensor-vectorproducts f1=einsum(a[0],G1,'btn1,r1n1r2->btr1r2') f2=einsum(z,G2,'bti,r2ir3->btr2r3')
#batch-multiplyf1@f2 fout=einsum(f1,f2,'btr1r2,btr2r3->btr1r3')
#contractwithfinalTRcore y=einsum(G3,fout,'r3o r1,btr1r3->bt o') ```

And a two-level hierarchical version with an additional TR-core as:

```
#TRmuMoE(r*=TRranks,b=batch_dim,t=tokens,
#i=input_dim,o=output_dim,a[e]=expert_coefs,n*=expert_dims)
#AM2-levelhierarchicalTRmuMoE,assumingadditionalTRcoresGi f1=einsum(a[0],G1,'btn1,r1n1r2->btr1r2') f2=einsum(a[1],G2,'btn2,r2n2r3->btr2r3') f3=einsum(z,G3,'bti,r3ir4->btr3r4') ```

```
#batch-multiplyf1@f2@f3 fout=einsum(f1,f2,'btr1r2,btr2r3->btr1r3') fout=einsum(fout,f3,'btr1r3,btr3r4->btr1r4')
#contractwithfinalTRcore y=einsum(G4,fout,'r4o r1,btr1r4->bt o') ```

## Appendix C \(\mu\)MoE forward pass visualization

For intuition, we provide a visualization in Figure 6 of the step-by-step series of tensor contractions \(\mathcal{W}\times_{1}\mathbf{a}\times_{2}\mathbf{z}\in\mathbb{R}^{O}\) that the \(\mu\)MoE computes (in non-factorized form).

## Appendix D Decomposition choice, matrix rank, and computational cost

In this section we present a further detailed discussion of decomposition choice, validating our choices and comparing alternative options. The computational costs of each fast \(\mu\)MoE forward pass and tensor-matrix rank relationships implications derived in this section are summarized in Table 5.

### Tensor ranks to matrix rank

One important consideration is how the chosen tensor ranks bound the resulting experts' matrix rank in \(\mu\)MoE layers. Here, we derive the matrix ranks as a function of tensor ranks for each model in turn.

#### d.1.1 CP\(\mu\)MoEs: rank analysis

CP\(\mu\)MoEs are parameterized by factor matrices \(\mathbf{U}^{(1)}\in\mathbb{R}^{R\times N},\mathbf{U}^{(2)}\in\mathbb{R}^{R \times I},\mathbf{U}^{(3)}\in\mathbb{R}^{R\times O}\) for chosen CP-rank \(R\). Following _Section 3_ of Kolda and Bader [53] which provides the matricization/unfolding of CP tensors, we can write expert \(n\)'s weight matrix as

\[\mathbf{W}_{n}=\mathbf{U}^{(2)^{\top}}\left(\mathbf{U}^{(1)^{\top}}_{:n} \odot\mathbf{U}^{(3)^{\top}}\right)^{\top}\in\mathbb{R}^{I\times O},\] (6)

Figure 6: An intuitive visualization of the \(\mu\)MoE (unfactorized) forward pass, as visualized (as a series of tensor contractions) in 5 steps. Each step contributes to producing the output vector \(\mathbf{y}\in\mathbb{R}^{O}\) either by contracting with the expert coefficients \(\mathbf{a}\in\mathbb{R}^{N}\), or with the input vector \(\mathbf{z}\in\mathbb{R}^{I}\), along the appropriate mode of the collective weight tensor \(\mathcal{W}\in\mathbb{R}^{N\times I\times O}\).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **Param-efficient** & **Param-efficient** & & \\  & (**medium** & \(N\)) & **(large** & \(N\)) & **\# Parameters** & **Estimated \# FLOPs** & **Max. expert matrix rank** \\ \hline Dense MoE & & & \(NO\) & \(NO\) & \(NO\) & \(\min\{I,O\}\) \\ Sparse MoE & & & \(NO\) & \(KIO\) & \(\min\{I,O\}\) \\
**CP\(\mu\)MoE** & & & \(R(N+I+O)\) & \(R(N+I+O)\) & \(R(N+I+O)\) & \(\min\{I,O,R\}\) \\
**TP\(\mu\)MoE** & & & \(R_{1}NR_{2}+R_{1}R_{3}+R_{3}OR_{1}\) & \(R_{2}R_{3}+R_{3}NR_{2}+R_{1}R_{3}R_{3}+R_{3}OR_{3}\) & \(\min\{R_{1},\min\{R_{1},\min\{R_{2},\}I,O\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: A computational comparison of decomposition choice for \(\mu\)MoE layers and existing MoEs.

where \(\odot\) is the Khatri-Rao product [53], and \(\mathbf{U}_{:n}^{(1)}\in\mathbb{R}^{R\times 1}\) is the column of the factor matrix associated with expert \(n\) (including a singleton dimension for the Khatri-Rao product to be well-defined). Through the linear algebra rank inequality for matrix products, we have

\[\text{rank}(\mathbf{W}_{n})=\text{rank}\left({\mathbf{U}^{(2)}}^{ \top}\left({\mathbf{U}_{:n}^{(1)}}^{\top}\odot{\mathbf{U}^{(3)}}^{\top} \right)^{\top}\right)\leq\min\bigg{\{}\text{rank}(\underbrace{\mathbf{U}^{(2 )}}_{R\times I}),\text{rank}(\underbrace{{\mathbf{U}^{(1)}_{:n}}^{\top}\odot{ \mathbf{U}^{(3)}}^{\top}}_{O\times R})\bigg{\}}.\] (7)

Therefore a single CP\(\mu\)MoE's \(n\)th expert's matrix rank is bounded by \(\min\{I,O,R\}\).

#### d.1.2 TR\(\mu\)MoEs: rank analysis

We now turn our attention to TR\(\mu\)MoEs, where we will see that the TR ranks \(R_{1},R_{2},R_{3}\) translate very favorably into matrix rank at smaller computational cost than with CP\(\mu\)MoEs. First recall that TR\(\mu\)MoEs are parameterized instead by core tensors \(\mathcal{U}^{(1)}\in\mathbb{R}^{R_{1}\times N\times R_{2}}\), \(\mathcal{U}^{(2)}\in\mathbb{R}^{R_{2}\times I\times R_{3}}\), \(\mathcal{U}^{(3)}\in\mathbb{R}^{R_{3}\times O\times R_{1}}\), with chosen ranks \(R_{1},R_{2},R_{3}\). We can derive an expression to materialize expert \(n\)'s matrix through the sum of matrix products of the TR cores as:

\[\mathbf{W}_{n}=\sum_{r_{3}=1}^{R_{3}}\bigg{(}\underbrace{\mathbf{U}^{(3)}_{: \gamma_{3}:}}_{\bigcirc\times R_{1}}\underbrace{\mathbf{U}^{(1)}_{:n}}_{R_{1} \times R_{2}}\underbrace{\mathbf{U}^{(2)}_{:r_{3}}}_{R_{2}\times I}\bigg{)}^{ \top}\in\mathbb{R}^{I\times O}.\] (8)

The matrix product rank inequality applies to each \(I\times O\) matrix summand, whilst the matrix sum rank inequality applies to the outer matrix sum:

\[\text{rank}(\mathbf{W}_{n}) =\text{rank}\bigg{(}\sum_{r_{3}=1}^{R_{3}}\left(\mathbf{U}^{(3)} _{r_{3}:}\mathbf{U}^{(1)}_{:n:}\mathbf{U}^{(2)}_{:r_{3}}\right)^{\top}\bigg{)}\] (9) \[\leq\sum_{r_{3}=1}^{R_{3}}\text{rank}\big{(}\big{(}\mathbf{U}^{(3 )}_{r_{3}:}\mathbf{U}^{(1)}_{:n:}\mathbf{U}^{(2)}_{:r_{3}}\big{)}^{\top}\big{)}\] (10) \[\leq\sum_{r_{3}=1}^{R_{3}}\min\bigg{\{}\text{rank}\big{(}\mathbf{ U}^{(3)}_{r_{3}:}\big{)},\text{rank}\big{(}\mathbf{U}^{(1)}_{:n:}\big{)},\text{ rank}\big{(}\mathbf{U}^{(2)}_{:r_{3}}\big{)},\bigg{\}}.\] (11)

Consequently, expert \(n\)'s materialized weight matrix in TR\(\mu\)MoEs has a more generous upper bound of \(\min\big{\{}R_{3}\cdot\min\{R_{1},R_{2}\},I,O\big{\}}\)8.

Footnote 8: Regardless of how large \(R_{3}\) is, the rank of the matrix cannot exceed \(\min\{I,O\}\).

Through this analysis, we observe that one can choose large values of \(R_{3}\) yet small \(R_{1},R_{2}\) to yield a high expert matrix rank with few parameters, justifying the choice of \(R_{1}=R_{2}=4\) in the main paper.

#### d.1.3 Tucker\(\mu\)MoEs: rank analysis

One popular alternative decomposition is the Tucker decomposition [86]. Here we derive the resulting matrix rank of this alternative \(\mu\)MoE variant and detail why it's not as desirable as the proposed \(\mu\)MoE variants.

A Tucker\(\mu\)MoE composes an \(\mu\)MoE weight tensor through the series of mode-\(n\) products [53]: \(\mathcal{W}=\mathcal{Z}\times_{1}\mathbf{U}^{(1)}\times_{2}\mathbf{U}^{(2)} \times_{3}\mathbf{U}^{(3)}\), where \(\mathcal{Z}\in\mathbb{R}^{R_{N}\times R_{I}\times R_{O}}\) is the so-called 'core tensor' and \(\mathbf{U}_{1}\in\mathbb{R}^{N\times R_{N}},\mathbf{U}_{2}\in\mathbb{R}^{I \times R_{I}},\mathbf{U}_{3}\in\mathbb{R}^{O\times R_{O}}\) are the 'factor matrices' for the tensor's three modes.

Again following Kolda and Bader [53] a single expert \(n\)'s weight matrix can be rewritten through the matricization involving the Kronecker product \(\otimes\) as:

\[\mathbf{W}_{n}=\mathbf{U}^{(2)}\mathbf{Z}_{(2)}\left(\mathbf{U}^{(1)}_{n} \otimes\mathbf{U}^{(3)}\right)^{\top}\in\mathbb{R}^{I\times O},\] (12)where \(\mathbf{Z}_{(2)}\in\mathbb{R}^{R_{I}\times(R_{O}\cdot R_{N})}\) is the so-called mode-\(2\) (matrix) unfolding of the core tensor [53]. Consequently, the same rank inequality applies:

\[\text{rank}(\mathbf{W}_{n}) =\text{rank}\left(\mathbf{U}^{(2)}\mathbf{Z}_{(2)}\left(\mathbf{U} _{n}^{(1)}\otimes\mathbf{U}^{(3)}\right)^{\top}\right)\] (13) \[\leq\min\bigg{\{}\text{rank}(\underbrace{\mathbf{U}^{(2)}}_{I \times R_{I}}),\text{rank}(\underbrace{\mathbf{Z}_{(2)}}_{R_{I}\times(R_{O} \cdot R_{N})}),\text{rank}(\underbrace{\mathbf{U}_{n}^{(1)}\otimes\mathbf{U}^{ (3)}}_{O\times(R_{O}\cdot R_{N})})\bigg{\}},\] (14)

Where we see the much more restrictive matrix rank upper bound applies: \(\min\left\{\min(I,R_{I}),\min(R_{I},R_{O}\cdot R_{N}),\min(O,R_{O})\right\}\). Thus in practice, _both_\(R_{I},R_{O}\) need to be large to yield a large matrix rank, which is in conflict with the goal of maintaining a moderate number of parameters.

### Why is low-rankness a reasonable assumption?

Given we've seen that parameter-efficient \(\mu\)MoE layers lead to low-rank expert weight matrices, a natural question is whether or not low-rankness in MLP linear layers' weight matrices is a reasonable assumption or constraint.

Our strongest piece of evidence supporting the claim is experimental in nature: we've seen from the results in Section 4.3 that using all parameter-matched \(\mu\)MoE layers for both MLP mixers and GPT-2 models leads to no significant drop in accuracy from their linear layer counterparts (see also Appendix I for many more results).

To investigate this further we perform a rank ablation on our trained MLP-Mixer model with the original linear layers' weights. Concretely, we compute the truncated SVD of each MLP block's 2 linear layer weight matrices. We explore the impact on the model's ImageNet1k validation set accuracy when using only the top-\(k\) singular vectors/values (the best rank-\(k\) approximation [87]). The validation set accuracy using truncated SVD weights in every mixer block is plotted in Figure 7-we see here that discarding as many as _half_ the total number of (bottom) singular vectors/values to approximate the original weights leads to negligible difference to the validation set accuracy. In other words, low-rank approximations of MLP Mixers' weights retain their representational power sufficiently well to produce nearly the same validation set accuracy as the original model. Such findings are consistent with results in recent work in the language domain [88], where low-rank approximations of MLP layers can even sometimes boost original performance. The accuracy retained by MLP Mixers here even after such aggressive rank reduction constitutes further evidence that full-rank weights are not always necessary.

### MoE/\(\mu\)MoE parameter count comparisons

We plot in Figure 8 the parameter counts for \(\mu\)MoE layers as a function of the expert counts (sweeping from \(N=2\) experts through to \(N=16,384\)), relative to dense/sparse MoEs (with rank \(R_{1}=R_{2}=4\) TR\(\mu\)MoEs), for the first layer in a MLP-mixer channel-mixing block [80]. As can be seen, both \(\mu\)MoE variants are vastly more parameter-efficient than dense/sparse MoEs.

Given TR\(\mu\)MoEs offer even better parameter efficiency for larger numbers of experts, we suggest opting for CP\(\mu\)MoEs when using expert counts less than \(\sim 128\), and considering TR\(\mu\)MoEs for higher values.

Latency and memory usagecomparisons between the \(\mu\)MoE, linear layers, and alternative MoEs are shown in Table 6, where the \(\mu\)MoEs perform favorably.

Figure 7: Val. accuracy for an S-16 MLP-mixer when performing truncated SVD on all MLP’s linear layers’ weight; model accuracy is closely retained even with half the singular vectors.

## Appendix E Hierarchical \(\mu\)MoE model derivations

In the main paper, the fast forward passes are derived for a single level of expert hierarchy. One additional attractive property of \(\mu\)MoEs is their straightforward extension to multiple levels of expert hierarchy-one simply increments the number of modes of the weight tensor and includes another tensor contraction with new expert coefficients. Hierarchical \(\mu\)MoEs intuitively implement "and" operators in expert selection at each level, and further provide a mechanism through which to increase the total expert count at a small parameter cost. Here, we derive the fast forward passes for \(\mu\)MoE layers in their most general form with \(E\) levels of expert hierarchy. For intuition, we first further visualize \(\mu\)MoE layers with 2 levels of hierarchy in Figure 9-note how we have an extra mode to the weight tensor, and an extra contraction over the new expert mode to combine its outputs.

Given that hierarchical \(\mu\)MoEs involve very high-order tensors, we adopt the popular mode-\(n\) product [53] to express the forward passes in as readable a way as possible. The **mode-\(n\) (vector) product** of a tensor \(\mathcal{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\ldots\times I_{N}}\) and vector \(\mathbf{u}\in\mathbb{R}^{I_{n}}\) is denoted by \(\mathcal{X}\times_{n}\mathbf{u}\)[53], with its elements given by:

\[\left(\mathcal{X}\times_{n}\mathbf{u}\right)_{i_{1}\ldots i_{n-1}i_{n+1} \ldots i_{N}}=\sum_{i_{n}=1}^{I_{n}}x_{i_{1}i_{2}\ldots i_{N}}u_{i_{n}}.\]

We first introduce the formulation of an \(E\)-level hierarchical \(\mu\)MoE layer from Equation (1) in the main paper: given input \(\mathbf{z}\in\mathbb{R}^{I}\), the most general form of \(\mu\)MoE layer is parameterized by weight tensor \(\mathcal{W}\in\mathbb{R}^{N_{1}\times\ldots\times N_{E}\times I\times O}\) and \(E\) many expert gating parameters \(\{\mathbf{G}_{e}\in\mathbb{R}^{I\times N_{e}}\}_{e=1}^{E}\). The

\begin{table}
\begin{tabular}{l c c} \hline \hline Layer type & Peak memory usage (MB) & Latency per single input (ms) \\ \hline Linear layer & 12.07 & 0.01 \\ Dense MoE (\(N=128\)) & 390.17 & 1.17 \\ Sparse MoE (\(N=128\)) & 765.19 & 0.80 \\ TR\(\mu\)MoE (\(N=128\)) & 15.87 & 0.94 \\ CP\(\mu\)MoE (\(N=128\)) & 14.02 & 1.05 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of different layers’ peak memory usage and latency (per single input). We use 128 experts in each MoE layer, and set the rank of the \(\mu\)MoEs to parameter-match that of the linear layer.

Figure 8: \(\mu\)MoE layer parameter count as a function of expert count.

explicit, unfactorized forward pass is given by:

\[\mathbf{a}_{e} =\phi(\mathbf{G}_{e}^{\top}\mathbf{z})\in\mathbb{R}^{N_{e}},\quad \forall e\in\{1,\ldots,E\},\] \[\mathbf{y} =\mathcal{W}\times_{1}\mathbf{a}_{1}\times_{2}\ldots\times_{E} \mathbf{a}_{E}\times_{E+1}\mathbf{z}\] \[=\sum_{n_{1}=1}^{N_{1}}a_{1n_{1}}\ldots\sum_{n_{E}=1}^{N_{E}}a_{ EN_{E}}\big{(}\underbrace{\mathbf{W}_{n_{1}\ldots n_{E}:}^{\top}}_{O\times I} \mathbf{z}\big{)}\in\mathbb{R}^{O},\] (15)

where Equation (15) is expressed as sums over the \(E\)-many expert modes to make it clear that hierarchical \(\mu\)MoEs take convex combinations of \(\prod_{e=1}^{E}N_{e}\) many experts' outputs (given there are \(N_{e}\) experts at each level of hierarchy). With expert coefficients \(\{\mathbf{a}_{e}\in\mathbb{R}^{N_{e}}\}_{e=1}^{E}\), the factorized forward passes of the most general hierarchical \(\mu\)MoE layers are given for the two variants below.

### Hierarchical CP\(\mu\)MoE

The full CP\(\mu\)MoE model of rank \(R\) has an implicit weight tensor \(\mathcal{W}=\sum_{r=1}^{R}\mathbf{u}_{r}^{(1)}\circ\mathbf{u}_{r}^{(2)}\circ \mathbf{u}_{r}^{(3)}\circ\cdots\circ\mathbf{u}_{r}^{(E+2)}\in\mathbb{R}^{N_{1 }\times\cdots\times N_{E}\times I\times O}\), with factor matrices \(\mathbf{U}^{(1)}\in\mathbb{R}^{R\times N_{1}},\ldots,\mathbf{U}^{(E)}\in \mathbb{R}^{R\times N_{E}},\mathbf{U}^{(E+1)}\in\mathbb{R}^{R\times I}, \mathbf{U}^{(E+2)}\in\mathbb{R}^{R\times O}\). The implicit, factorized forward pass is given by:

\[\mathbf{y} =\left(\sum_{r=1}^{R}\mathbf{u}_{r}^{(1)}\circ\mathbf{u}_{r}^{(2 )}\circ\mathbf{u}_{r}^{(3)}\circ\cdots\circ\mathbf{u}_{r}^{(E+2)}\right) \times_{1}\mathbf{a}_{1}\times_{2}\ldots\times_{E}\mathbf{a}_{E}\times_{E+1} \mathbf{z}\] \[=\sum_{r=1}^{R}\mathbf{u}_{r}^{(E+2)}\big{(}\sum_{n_{1},\ldots,n_ {E},i}u_{rn_{1}}^{(1)}a_{1_{n_{1}}}\cdots u_{rn_{E}}^{(E)}a_{E_{n_{E}}}u_{ri}^{ (E+1)}z_{i}\big{)}\] \[=\sum_{r=1}^{R}\mathbf{u}_{r}^{(E+2)}\big{(}\mathbf{U}^{(1)} \mathbf{a}_{1}\big{)}_{r}\cdots\big{(}\mathbf{U}^{(E)}\mathbf{a}_{E}\big{)}_{ r}\cdot\big{(}\mathbf{U}^{(E+1)}\mathbf{z}\big{)}_{r}\in\mathbb{R}^{O}.\] (16)

### Hierarchical TR\(\mu\)MoE

In TR format, \(\mathcal{W}\in\mathbb{R}^{N_{1}\times\cdots\times N_{E}\times I\times O}\) has \(E+2\) factor tensors: \(\mathcal{U}^{(1)}\in\mathbb{R}^{R_{1}\times N_{1}\times R_{2}},\ldots,\mathcal{ U}^{(E)}\in\mathbb{R}^{R_{E}\times N_{E}\times R_{E+1}},\mathcal{U}^{(E+1)}\in \mathbb{R}^{R_{E+1}\times I\times R_{E+2}},\mathcal{U}^{(E+2)}\in\mathbb{R}^ {R_{E+2}\times O\times R_{1}}\), where \(R_{i}\) are the manually chosen ranks. The weight tensor's elements are given by:

\[w_{n_{1}\ldots n_{E}io}=\text{tr}\big{(}\mathbf{U}^{(1)}_{:n_{1}:}\cdots \mathbf{U}^{(E)}_{:n_{E}}\mathbf{U}^{(E+1)}_{:i:}\mathbf{U}^{(E+2)}_{:o:}\big{)}.\]

Figure 9: Illustration of a **two-hierarchy**\(\mu\)MoE layer’s (unfactorized) forward pass as a series of tensor contractions. The \(N_{1}\cdot N_{2}\) many experts’ weight matrices are visualized as 2D horizontal slices in yellow, which are (1) matrix-multiplied with the input vector, (2) summed over the first expert mode (weighted by the first expert coefficients \(\mathbf{a}_{1}\) in red), and (3) summed over the second expert mode (weighted by the second expert mode’s coefficients \(\mathbf{a}_{2}\) in dark green).

We derive the fast factorized forward pass in terms of a series of mode-\(2\) products:

\[\mathbf{y} =\sum_{i}\sum_{n_{1},\dots n_{E}}\mathcal{W}(n_{1},\cdots,n_{E},i,:) \mathbf{a}_{1}(n_{1})\cdots\mathbf{a}_{E}(n_{E})\mathbf{z}(i)\] (17) \[=\sum_{r_{1},r_{E+2}}\mathbf{u}_{r_{E+2}}^{(E+2)}(\underbrace{( \mathcal{U}^{(1)}\times_{2}\mathbf{a}_{1})\cdots(\mathcal{U}^{(E)}\times_{2} \mathbf{a}_{E})(\mathcal{U}^{(E+1)}\times_{2}\mathbf{z})}_{R_{1}\times R_{E+2}} )_{r_{1}r_{E+2}}\in\mathbb{R}^{O}.\] (18)

## Appendix F Experimental details

### Network configurations and hyperparamters

Here we provide the full experimental details and setups to reproduce the performance results in the paper for each of the networks. We further include the per-epoch accuracy plots for additional transparency into the training processes.

The experimental configurations used to reproduce the performance results in the main paper follow as closely as possible those specified in the main paper of MLP-mixer [80] and open-source code (https://github.com/lucidrains/mlp-mixer-pytorch), the open-source code for NanoGPT (https://github.com/karpathy/nanoGPT) for GPT2 [81], and the robust fine-tuning protocol of [89] for CLIP [61]. These values are summarized in Table 7. We plot the learning curves for the training of both models in Figures 10 and 11.

Rank choicesThroughout all experiments in the main paper, we fix the TR\(\mu\)MoE ranks for the first two modes to be \(R_{1}=R_{2}=4\). This way, we can maximize the effective expert matrix ranks at a low parameter cost, as shown in Appendix D.1.2. The final TR rank \(R_{3}\) is varied to parameter-match the networks in question. For CP\(\mu\)MoEs, we set the single CP rank \(R\) to parameter-match the baselines.

Training timesEach MLP mixer model takes just under 3 days to train on 4xA100 80GB GPUs. The NanoGPT models take 2-3 days to train for \(100k\) iterations, with the same resources.

### Weight initialization

We initialize each element of the factor matrices/tensors for the input and output modes from a \(U[-\sqrt{k},\sqrt{k}]\) distribution (following PyTorch's linear layers' initialization strategy), for \(k=1/\mathrm{in\_features}\), where \(\mathrm{in\_features}\) is the dimension of the input to each factor matrix/tensor during the factorized forward passes.

Factor matrices for the expert modes are initialized to replicate the weight matrices along the expert mode (plus optional noise). For CP\(\mu\)MoEs, this corresponds to sampling the factor matrices' elements from a \(\mathcal{N}(1,\sigma)\) distribution. For TR\(\mu\)MoEs, the weight matrices can instead be replicated along the expert mode by initializing each slice (e.g. \(\mathcal{G}_{1}(:,i,:)\)) as a diagonal matrix with its elements sampled from \(\mathcal{N}(1,\sigma)\). In all our experiments we set \(\sigma:=1\) to introduce noise along the first expert mode, and \(\sigma:=0\) for additional expert modes.

## Appendix G Expert specialism: additional results

### Large scale models

We first show in Figure 12 the top-activating examples for MLP-mixers trained with both CP\(\mu\)MoE and TR\(\mu\)MoE blocks. Examples are shown for the first two experts as they appear numerically for

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline  & Learning rate & Batch size & Weight decay & Warmup & Training duration & Stochastic hard/augment length & Mixup length & Mixed precision & Random precision & fixed & Random precision & fixed & Hardware \\ \hline MLP Mixer & 1e-3 & 4096 & 1e-4 & 10k & 300 epochs & True & 15 & 0 & 0.5 & brife & 0 & 4xA100 80GB \\ NanoGPT & 6e-4 & 24 & 1e-1 & 2k & 100k iter & False & 0 & 0 & 0 & brife & 0 & 4xA100 80GB \\ CLIP & 3e-5 & 4096 & 1e-1 & 500 & 10 epochs & False & 0 & 0 & 0 & brife & 0 & 1xA100 80GB \\ \hline \hline \end{tabular}
\end{table}
Table 7: Experimental configuration and settings for the results reported in the main paper in Section 4.3.

each of the \(8\) layers, where we observe the same phenomenon of earlier blocks specializing to textures, and later blocks to higher-level abstract concepts/objects.

Secondly, in Figure 13 we show the top \(32\) activating tokens for the first \(6\) experts (as they appear numerically) for layer \(5\) in GPT2 models trained with CP\(\mu\)MoEs replacing every MLP block. Whilst there are clear coherent themes amongst the top-activating tokens, we do see some examples of multiple themes being processed with high coefficients by the same experts (e.g. example #20 in expert 2's top-activating examples appears unrelated to the context of the other top-activating tokens) indicating a certain degree of expert polysemanticity (as expected in the large open domain of web text).

Figure 11: Training and validation loss for the GPT-2 models for 100k iterations.

Figure 10: Training loss and validation accuracy for the MLP-mixers models for 300 epochs.

Figure 12: Top-activating patches (and their surrounding image context) for the first experts at two blocks in MLP-mixer models. \(\mu\)MoE blocks (with \(N=64\)) exhibit coarse-grained specialism (e.g., texture) earlier and more fine-grained specialism (e.g., object category) deeper in the network.

[MISSING_PAGE_EMPTY:26]

## Appendix B

Figure 14: Top-activating \(32\) tokens for the unfiltered experts 7-12 (as ordered numerically) at layer 5 in the CP\(\mu\)MoE GPT2 model.

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_EMPTY:29]

Figure 16: **High vs low total expert count**: _Randomly_ selected training set images with expert coefficient \(\geq 0.5\) for the first \(10\) numerical experts (of those processing any images with coefficient \(\geq 0.5\)). Results are with CP-r512 \(\mu\)MoE layers with 256 (left) and 32 (right) total experts respectively. We highlight the apparent specialist of the experts when a higher total number is used. (**Please zoom for detail**)

Figure 17: **Fine-grained expert specialisms**: _Manually_ selected experts (and images ranked by _highest_ expert coefficients) processing what appears to be very fine-grained categories (e.g. animals with footballs, trolleys in water, etc.). Model fine-tuned on ImageNet1k with a high number of \(2048\) experts and a CP-r512 \(\mu\)MoE final CLIP layer. **(Please zoom for detail)**

Figure 18: **Penultimate layer CP\(\mu\)MoE**: Percentage of per-class test set accuracy lost when intervening and ablating particular experts (along the columns). In general, the more total experts (rows), the more class-level monosemantic the experts are as indicated by the mass centred on fewer classes, and with higher magnitude. Shown are the first \(4\) experts in each model (row) to change \(\geq 0.5\) of any class’ accuracy when counterfactually ablated.

Figure 19: **Final layer CP\(\mu\)MoE**: Percentage of per-class test set accuracy lost when intervening and ablating particular experts (along the columns). In general, the more total experts (rows), the more class-level monosemantic the experts are as indicated by the mass centred on fewer classes, and with higher magnitude. Shown are the first \(4\) experts in each model (row) to change \(\geq 0.5\) of any class’ accuracy when counterfactually ablated.

[MISSING_PAGE_FAIL:34]

### Expert load

Here, we plot the expert load in Figure 22 to give a visual indication of how many images are processed by each expert with \(a_{e}\geq 0.5\) for CP\(\mu\)MoE final layers fine-tuned on ImageNet1k with a CLIP backbone. Whilst clearly, not all experts have images with a coefficient of at least \(0.5\), we see a relatively uniform spread over all experts. Furthermore, we note the cost from 'dead' experts is not particularly troublesome in an \(\mu\)MoE given its factorized form-speaking informally, we would rather have too many experts than too few, so long as there exist select individual experts conducting the subcomputations of interest.

## Appendix I Additional performance results

### CLIP ViT-B-32 ImageNet1k ablations

Here, we compare the performance of parameter-matched \(\mu\)MoE final layers (for varying expert counts \(N\)) to linear layers for fine-tuning large vision-language models (CLIP ViT-B-32) on ImageNet1k. Following the robust fine-tuning protocol of [89], we use the largest possible batch size (to fit on one A100 GPU) of \(4096\), and the same learning rate of \(3e-05\).

For \(\mu\)MoE layers, we reduce the layer ranks to parameter match _single_ linear layers for each value of total expert count. We plot in Figure 14(a) the ImageNet1k validation loss after 10 epochs of training, where all expert counts out-perform the linear layers initialized the same default way with elements from \(U[-k,k]\). However, to parameter-match single dense linear layers, we must decrease the \(\mu\)MoE layer rank upon increasing the expert count. This is a concrete example of where the extra parameter efficiency of TR\(\mu\)MoEs can come in useful (as discussed in Appendix D.1.2). Consequently, TR\(\mu\)MoEs' resulting expert matrix ranks are increasingly larger than that of CP\(\mu\)MoEs in the parameter-matched setting. For example, the parameter-matched layers with 512 experts in Figure 14(a) have a max expert matrix rank of 165 for the CP\(\mu\)MoE compared to a much larger 208 for the TR\(\mu\)MoE.

Figure 22: Expert load: Number of training set images with expert coefficient \(a_{n}\geq 0.5\) for CP\(\mu\)MoE models fine-tuned on ImageNet1k. Bars are drawn with 3x width and colored sequentially in a repeating order of distinct colors to help visually distinguish between neighbors.

We attribute TR\(\mu\)MoE's even greater performance gains over CP\(\mu\)MoEs here to the more favorable relationship between tensor rank and expert matrix rank (a larger weight matrix rank meaning the resulting layers' activations live in a larger dimensional subspace) (see Figure 1(b)).

### Hierarchical \(\mu\)MoEs

Hierarchical \(\mu\)MoE MixersWe train from scratch two hierarchical \(\mu\)MoE MLP-mixer S-16 models for \(300\) epochs on ImageNet following the same configuration as in Section 4.3 of the main paper. Concretely, we use a **two-level** hierarchical \(\mu\)MoE with \(N_{1}=64\) experts for the first level and \(N_{2}=2\) experts for the second layer (\(128\) total effective experts). As shown through the results in Table 9, the hierarchical \(\mu\)MoE's also perform well against the MLP alternatives, whilst providing even better parameter-efficiency.

Hierarchical \(\mu\)MoE fine-tuning layersWe also perform additional experiments with hierarchical \(\mu\)MoEs used to fine-tune CLIP ViT-B-32 models on ImageNet1k. Here we use the experimental setup in [63; 64], training each model for a single epoch with the specified learning rate of \(1e-05\). We fine-tune hierarchical \(\mu\)MoE CLIP models with up to \(4\) levels of hierarchy as shown in Table 10, where the best-performing models (averaged over 5 runs) are found with \(2\) levels of hierarchy.

### Comparisons to dense/sparse MoEs

The goal of the \(\mu\)MoE layer is to facilitate more interpretable subcomputations with a similar number of parameters and FLOPs to regular dense layers. Whilst the layer does not aim to improve on the _capabilities_ of existing MoE layers, we nonetheless provide an initial comparison study here in Figure 24 for completeness. As can be seen, in addition to the scalable expert specialization provided,

Figure 23: Comparative analysis of fine-tuning CLIP ViT-B-32 with \(\mu\)MoE layers using different configurations. **All experiments have the same number of parameters**.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model & Val. acc. (\(\uparrow\)) & \# Experts per block & \# Params \\ \hline MLP & 70.31 & n/a & 18.5M \\
**CP\(\mu\)MoE** (hierarchy=1) & 71.29 & \(64\) & 18.6M \\
**TR\(\mu\)MoE** (hierarchy=1) & 71.26 & \(64\) & 18.3M \\
**CP\(\mu\)MoE** (hierarchy=2) & 71.24 & \(64\cdot 2\) & 19.5M \\
**TR\(\mu\)MoE** (hierarchy=2) & **71.56** & \(64\cdot 2\) & 18.7M \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Hierarchical S-\(16\) TR\(\mu\)MoE-mixers and CP\(\mu\)MoE-mixers: ImageNet1k val. accuracy at 300 epochs pre-training; \(N_{1}=64,N_{2}=2\) experts).**

[MISSING_PAGE_FAIL:37]

* **Standard deviation bias** computes the standard deviation of the accuracy for the different subpopulations [77]. Intuitively, a small STD bias indicates similar performance across groups.
* **Max-Min Fairness** quantifies the worst-case performance for the different demographic subpopulations [78], with \(\max\min_{y\in\mathcal{Y},a\in\mathcal{A}}P(\hat{Y}=y|A=a,Y=y)\). We compute this as the minimum of the test-set accuracy for the \(4\) subpopulations in each experiment.

**Baselines**

* **Oversample** we oversample the low-support subpopulation to balance the number of input images that have the sensitive attribute for the value of the target attribute wherein bias occurs. For example, we oversample the 'blond males' to match the number of 'blond females' for the first experiment, and oversample the number of 'old females' to match the number of 'old males' for the second.
* **Blind thresholding** is implemented by unconditionally increasing/decreasing the logits in the target direction for all outputs. Concretely, the results in the main paper are achieved by setting \(\lambda:=2.5\) and \(\bar{\mathbf{a}}\) to a vector of ones in Equation (5) for all experiments. We find this value of \(\lambda\) to give us the best results for the attribute-blind re-writing [76].
* **Adversarial debiasing** we observe in Table 2 the same poor performance for the adversarial debiasing technique as is reported in Wang et al. [90]. We hypothesize that the same issues face the technique in our experimental setup. In particular, even in the absence of discriminative information for the 'gender' label in the final representation, information about correlated attributes (e.g. wearing makeup) are likely still present. This makes it fundamentally challenging to apply fairness-through-unawareness techniques in the CelebA multi-class setting.

## Appendix K Fairness: additional results

### Model re-writing

The full per-subpopulation test set accuracies are shown in Figure 25 for the two experiments in the main paper. The first rows show the accuracies before layer re-write, the second rows after re-write, and the third rows the absolute difference between the two. As can be seen in the 'before-after difference' final rows of Figure 25, the proposed expert-conditional re-write provides much more precision in changing only the computation for the target populations.

Figure 25: CelebA Subpopulation accuracies before (first rows) and after intervention (second rows), followed by their absolute difference (third rows). **Green rectangles** denote the target subpopulation for each experiment (subfigure).

## Appendix L NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Claims regarding both qualitative and quantitative expert specialism for fine-tuning large foundation models are demonstrated in Section 4.1, where the benefits of scaling the expert counts are also substantiated both qualitatively and quantitatively. Claims regarding bias mitigation are substantiated in Section 4.2. Qualitative expert specialism is provided for large models (along with their performance) in Section 4.3.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations clearly state the lack of evaluation for out-of-domain data for vision, and the difficulties in further evaluating expert specialism quantitatively in large models (given the lack of ground-truth).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Technical derivations of models are made throughout (and further basic derivations of expert matrix rank), but no novel theoretical results are presented.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Full experiment settings/config/hyperparameters are provided in Table 7, and the supporting code (https://github.com/james-oldfield/muMoE) provides even more explicit experimental instructions. Learning curves are also plotted in Figures 10 and 11 for additional transparency. Pseudocode implementations are also given in Appendix B.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Model code for \(\mu\)MoEs and the experiments in the paper are found at:https://github.com/james-oldfield/muMoE.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: As found in Table 7, where we state we follow these choices based on the default parameters of the original papers introducing the models, or the default configurations used by the open-source maintainer for GPT2.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [No] Justification: We do include mean (and STD) of the results over multiple fine-tuning models, but we only have single runs over the large models due to resource constraints. For these single runs of large models, we always set all random seeds to \(0\) for reproducibility.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details are provided in Appendix F.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: No ethical concerns to note.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper proposed a layer that provides more transparent, explainable, and editable networks. We discuss positive social impacts throughout the paper, but also acknowledge and discuss the potential negative impacts in Appendix A.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No models posing a high risk of misuse are to be released.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, the open-source codebases on which we base our code are explicitly referenced.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: None introduced.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects involved.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: No human subjects involved.