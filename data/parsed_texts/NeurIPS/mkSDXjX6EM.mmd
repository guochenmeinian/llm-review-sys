# FIND: A Function Description Benchmark for Evaluating Interpretability Methods

 Sarah Schwettmann\({}^{1}\)1 Tamar Rott Shaham\({}^{1}\)1

Joanna Materzynska\({}^{1}\) Neil Chowdhury\({}^{1}\) Shuang Li\({}^{1}\)

Jacob Andreas\({}^{1}\) David Bau\({}^{2}\) Antonio Torralba\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)Northeastern University

Footnote 1: Indicates equal contribution. Address correspondence to: schwett@mit.edu, tamarott@mit.edu

###### Abstract

Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces **find** (**F**unction **IN**terpretation and **D**escription), a benchmark suite for evaluating the building blocks of automated interpretability methods. **FIND** contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (a1a) generates function descriptions. We find that an a1a, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure--acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, **FIND** also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that **FIND** will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.

## 1 Introduction

The central task of interpretability research is to explain the functions that AI systems learn from data. Investigating these functions requires experimentation with trained models, using tools that incorporate varying degrees of human input. Hand-tooled approaches that rely on close manual inspection (Zeiler and Fergus, 2014; Zhou et al., 2014; Mahendran and Vedaldi, 2015; Olah et al., 2017, 2020; Elhage et al., 2021) or search for predefined phenomena (Wang et al., 2022; Nanda et al., 2022) are increasingly complemented by more automatic approaches that enable larger-scale analysis (Bau et al., 2020; Mu and Andreas, 2020; Hernandez et al., 2022; Oikarinen and Weng, 2023,Conmy et al., 2023]. Selection between approaches is highly application-dependent; to date, no single protocol answers all queries users might have about a system [Doshi-Velez and Kim, 2017, Vaughan and Wallach, 2020]. However, considering the growing body of evidence that LMs are capable of complex reasoning and problem-solving across domains [Wei et al., 2022, OpenAI, 2023, Yao et al., 2023, Lightman et al., 2023], we recognize their potential to become backbones of generalized agents for automated interpretability. Indeed, recent open-ended techniques have used pretrained LMs to describe the behavior of black-box text modules [Singh et al., 2023], including individual units inside other LMs [Bills et al., 2023]. As we enter a regime where model explanation is performed by models that are themselves uninterpretable, external evaluations of these techniques will be vital. At present, evaluation of network description procedures is limited and bespoke, in part because measuring performance on real-world problems is difficult when ground-truth descriptions of network structure are unknown [Doshi-Velez and Kim, 2017, Lipton, 2018, Miller, 2019, Hooker et al., 2019].

This paper introduces **Find** (**F**unction **IN**erpretation and **D**escription), a benchmark suite for evaluating the building blocks of automated interpretability methods on functions whose structure is known _a priori_ (see Figure 1). FIND is built from over 2000 procedurally generated function interpretation problems (_e.g._\(f:\) country \(\mapsto\) capital, unless country is in South America, in which case \(f\) returns undefined). In each problem, candidate interpretation procedures (interpreters) are given black-box access to functions, optionally accompanied by metadata (_e.g._ the domain of the function). After evaluating these functions on chosen inputs (_e.g._ Japan, Mexico, Peru), interpreters must eventually return a structured description that can be evaluated against other descriptions or used to simulate function behavior.

We also introduce a new interpretation method that uses Automated Interpretability Agents (aias) to interactively probe functions and explain their behavior. We formulate our approach to the interpretation problem as an implementation of the scientific method, wherein the goal of the aias to describe the process underlying observed input-output relations. In contrast to existing full-text explanation systems that apply automatic captioning methods (e.g. [Hernandez et al., 2022, Bills et al., 2023]) to pre-selected input-output pairs, the aias generates the data itself, by running the function on inputs it selects, observing the outputs, and updating hypotheses until it can describe the function to human end-users. We evaluate both aias and existing, non-interactive automated interpretability methods on **Find**. While exhibiting sophisticated experimentation strategies and out-performing non-interactive methods, the top-performing aias nonetheless fails to adequately describe \(48\)% of functions. **Find** thus shows that, in spite of promising recent results, methods based on current LMs alone are unlikely to robustly automate even high-level interpretability tasks.

**Find** focuses on the black-box function description paradigm because black-box description appears as a subroutine (or is the sole operation) implemented by almost every existing automated interpretation method, spanning label retrieval, program synthesis, and learning-based approaches. The

Figure 1: **The find benchmark.****Find** is constructed procedurally: atomic functions are defined across domains including elementary numeric operations (purple), string operations (green), and synthetic neural modules that compute semantic similarity to reference entities (yellow) and implement real-world factual associations (blue). Complexity is introduced through composition, bias, approximation and noise. We provide an LM-based interpretation baseline that compares text and code interpretations to ground-truth function implementations.

[MISSING_PAGE_FAIL:3]

inputs. One example is Nanda et al. (2022), where an algorithm using trigonometric manipulations was found to perform modular arithmetic in a toy transformer model. We sample 1000 numeric functions from the find API for the benchmark dataset. \(85\%\) are parameterized atomic functions under noiseless and noisy conditions, and \(15\%\) are compositions.

**Atomic functions** are defined as explicit functions found in many mathematical and scientific computing libraries such as Python (Van Rossum, 2020), SciPy (Virtanen et al., 2020), and NumPY (Harris et al., 2020), as well as standard neural activation functions such as ReLU (Fukushima, 1975; Nair and Hinton, 2010). Table 1 shows examples of atomic functions defined in find. For each function in the set of atomic functions \(\mathcal{A}\), we sample native parameters, scaling factor \(a\), and bias \(b\). Parameters and sampling procedure details are provided in the Appendix and the find API.

**Composition** of atomic functions \(f(x)\) and \(g(x)\) applies an operator sampled from \(C=\{\cdot,+\}\), where \(f\circ g=f(x)\cdot g(x)\) or \(f(x)+g(x)\). Composed functions are sampled from a subset \(\mathcal{A}_{\mathcal{C}}\) of atomic functions \(\mathcal{A}\) to limit final complexity. \(f(x),g(x)\in\mathcal{A}_{\mathcal{C}}\) are described in the Appendix.

**Observation noise** added to \(f(x)\) tests how well the interpreter is able to estimate an underlying function in the presence of additive noise, and whether it is able to distinguish between different types of noise. \(15\%\) of functions \(f(x)\in\mathcal{A}\) in the find benchmark are sampled with additive noise \(f(x)+X\), where \(X\) follows either a Normal, Uniform, or Poisson distribution.

**Domain corruption** replaces function values locally with random values. This is done either inside or outside of a sampled interval \(I\) of range \(\{[a,b]\,[a,\infty]\,[-\infty,a]\}\), where \(a\sim\mathcal{U}_{[-100,100]}\). The length of a finite interval is sampled from \(\mathcal{U}_{[5,20]}\). Corruption is defined as noise \(X\in\mathcal{N}(\mu,0.01)\) replacing the function values on \(I\). We choose \(\mu\) as the mean value of \(f(x)\) on its domain. The interpreter is prompted to discover the corrupted interval, if any, and to return \(a\) and \(b\). \(15\%\) of the functions \(f(x)\in\mathcal{A}\) in the benchmark dataset are corrupted on part of their domain.

**Approximation** of atomic functions is implemented using a two-layer neural network (MLP) with a ReLU non-linearity. For \(15\%\) of the functions in the dataset, we train an MLP for 10k epochs on 10k points uniformly sampled on its domain bounded by \((-100,100)\). Trained MLPs are provided in the benchmark dataset and loaded by the corresponding function during interpretation 2.

Footnote 2: We provide an LM baseline where MLPs are treated as black boxes during interpretation, similar to the other functions in the dataset. An alternate paradigm could allow probing of internal activations and parameters.

### Functions on strings

We build on a long history of using toy problems on strings (Hofstadter et al., 1995; Hofstadter, 1995; Mitchell, 2021) and simple visual matrices (Lowett and Forbus, 2017; Wang and Su, 2015; Carpenter et al., 1990; Chollet, 2019) to test the ability of a system to reverse-engineer underlying symbolic operations. As this release of find is designed to evaluate language-based interpretability systems, we focus on functions with text inputs and procedurally generate a set of functions on strings with different levels of complexity. The benchmark dataset contains 1000 string functions sampled from the find API, representing both atomic functions (30%) and compositions (70%).

**Atomic functions** include common string manipulation operations such as concatenate, replace, and reverse, and implementations of copycat problems from Hofstadter et al. (1995), such as shift_last (\(abc\mapsto abd\)). Example atomic string operations are shown in Table 1, and the full set can be found in the find API.

**Composition** of atomic functions \(f(x)\) and \(g(x)\) is defined as \((g\circ f)(x)=g(f(x))\). The find API supports compositions where \((g\circ f)(x)\) is well-defined (i.e. function pairs where \((g\circ f)(x)=x\) are excluded). Example composition functions are shown in Table 1.

### Synthetic neural modules

find includes a set of synthetic neural modules that perform word-level tasks built from Wikidata properties (Vrandecic and Krotzsch, 2014). We construct two types of text modules: functions involving lexical semantics and functions involving factual relations. find implements synthetic neural modules on text inputs using a single pretrained language model (Vicuna-13B) instructed to apply different rules to inputs in response to queries (See Figure 2). Vicuna (Chiang et al., 2023) is a fine-tuned LLaMA model licensed under an Apache License 2.0 and open-sourced for non commercial use. Each function in the dataset is built to interact with Vicuna to return the relevant output to the interpreter, which tests the function on different inputs to recover the underlying rule. We find that Vicuna reliably implements functions expressed in prompts (reliability scores are provided for each function type; see Appendix for full evaluation).

**Type 1: Entities**

**Atomic entity functions**\(f_{e}(x)\) compare input text to reference concepts (_entities_) drawn from metaclasses of Wikidata properties related to a particular concept or subject area (e.g. _related to food and eating_) and return a value indicating how associated an input is with the entity. Table 1 shows example entities, and the complete list of 140 atomic entities included in **find** is provided in the Appendix. The task of an interpreter evaluating an entity function is to recover and describe the underlying concept, or, what all of the inputs that produced high output values had in common. For example, if the entity is _plants and botany_, \(f_{e}(\texttt{garden})\) and \(f_{e}(\texttt{tree})\) should return high values, while \(f_{e}(\texttt{car})\) should return a low value. This task is relevant to lines of work that automatically summarize maximally activating exemplar sets to characterize neuron function (Bau et al., 2020; Mu and Andreas, 2020; Hernandez et al., 2022; Bills et al., 2023; Singh et al., 2023), generalized to a setting where the interpreter must also produce the data.

To implement each function, we instruct Vicuna to synthesize a binary response to interpreter queries, corresponding to whether an input word is associated (return 1) or unassociated (return 0) with the reference entity (see the Appendix for Vicuna prompts). The function output \(s=f_{e}(x)\) is then a continuous scalar value representing Vicuna's internal probability of 1 being the response token, between a choice of 0 and 1. Specifically, \(s=p_{1}/(p_{0}+p_{1})\), where \(p_{i}=e^{\text{legi}_{i}}\) and \(\text{logit}_{i}\) represents Vicuna's output logit for token \(i\). To validate that Vicuna can reliably identify associations between inputs and reference entities, we collect a human-labeled set of concepts \(\hat{x}_{j}\) associated with each entity \(e_{j}\) in the dataset, and compute \(\hat{s}=f_{e_{j}}(\hat{x}_{j})\). The mean value of \(\hat{s}\) across all \(140\) entities in **find**, for \(10\) human-annotated concepts per entity, is \(0.85\). We compute the same score for \(10\) distractor concepts per entity, sampled from the list of human annotations of other entities. The mean score for the distractors is \(0.08\). See the Appendix for full experiment details.

**Composed entity functions** mimic the behavior of neurons inside deep networks that are selective for multiple concepts (Fong and Vedaldi, 2018; Olah et al., 2020; Mu and Andreas, 2020). We construct compositions of entity functions by sampling two entities from the dataset and instructing Vicuna to return 1 if the input value to the function is associated with either entity. 60 composed entity functions are included in the **find** benchmark.

**Type 2: Relations**

More complex functions inside language models learn factual associations (Meng et al., 2022). To mimic the behavior of these modules, we construct a set of relation functions that map inputs to outputs according to a real-world factual association (for instance, river \(\mapsto\) length or gemstone \(\mapsto\) color). Atomic relations are drawn from Wikidata properties included in the Wikidata dump provided by Sorokin and Gurevych (2018). Table 1 shows example relation functions; a full list is provided in the Appendix. Again we use Vicuna to implement a black box function that applies the rule in the relation to interpreter inputs (we verify that Vicuna returns factually correct answers, see Appendix). Figure 2 shows the prompt template for Vicuna and an example relation function.

**Relation functions with bias** include corruption to part of the function domain. **find** tests whether an interpretability model can uncover which parts of the function domain have been corrupted. Corrupted relation functions return undefined for a small region of the function domain; for example, we corrupt the country \(\mapsto\) capital relation on the subdomain Asia by prompting Vicuna to return undefined for inputs in the subdomain.

Figure 2: **Synthetic neural module implementation.** Vicuna acts as a backbone that generates the outputs of neural modules. Each function provides instructions to Vicuna that mimic the behavior of either a single neuron (”return an association score with a predefined concept”) or a more complex module (“map inputs to outputs according to a a predefined mapping”). The interpreter then interacts with Vicuna as a black-box function.

Evaluation protocol

The **find** protocol evaluates interpreters' natural language descriptions of functions in all categories. In categories where domain corruption is introduced (numeric functions and factual relations), interpreters are evaluated on their ability to return a description of the domain of the function, indicating any subdomain where the function behaves differently. In the mathematical reasoning and strings categories, interpreters are evaluated on their ability to output code that approximates the function. Section 4 provides details on the interpretation procedures we evaluate and how they are instructed to engage with functions.

### Evaluation metrics

To evaluate the accuracy of function descriptions produced by an interpreter, we use success indicators for individual function interpretations and calculate average success rates across all functions. For functions where interpreters write code approximating the function (numeric and string functions), we score the accuracy of the interpretation by running the interpreter's code on a representative test set and comparing the result to execution of the ground-truth function. To evaluate language descriptions of functions, we judge how well the interpreter's description agrees with ground-truth function behavior (for string functions and synthetic neural modules). Below we describe success indicators for each function category in more detail.

**Evaluating code-based interpretations.** Running interpretation on the **find** benchmark produces descriptions of numeric and string functions in code as well as natural language. We measure explanation accuracy by comparing performance of the interpreter's code to the ground-truth function implementation on a test set. For numeric functions, we compute a normalized mean-squared error \(\mathbb{E}[(f(x)-g(x))^{2}]/\mathbb{E}[f(x)^{2}]\) for \([-128\leq x\leq 128]\) between the ground truth **find** function \(f\) and the interpreter's implementation \(g\), and regard a successful estimation as one with Normalized MSE (NMSE) \(<0.1\). For string functions, we run an exact-matching binary test (\(f\)(<string>\({}_{i}\))=\(g\)(<string>\({}_{i}\))) on a set of 10 test inputs per function.

**Evaluating language-based interpretations.** We define a "unit testing" protocol where an LM evaluator selects which of three input-output (I-O) pairs corresponds to a language description of a function. To judge the interpreter's accuracy, we provide the estimated function description (_e.g._\(f:\text{country}\mapsto\text{capital}\)) to the evaluator, as well as three example I-O pairs: one execution of the ground-truth function (_e.g._\(\mathtt{Germany}\mapsto\mathtt{Berlin}\)) and two randomly sampled distractors (I-O pairs for other **find** functions of the same type; _e.g._\(\mathtt{Germany}\mapsto\mathtt{Europe}\), \(\mathtt{ruby}\mapsto\mathtt{red}\)). The evaluator selects which I-O pair matches the functionality of the description from the interpreter. If the language description is accurate, the evaluator should select the ground-truth I-O pair as the best match. We run this procedure on language descriptions of string functions and synthetic neural modules for a test set of ten different triplets per function. Test sets are constructed to reveal representative behavior of each function using inputs inside and outside of the function domain. For relations corrupted on part of their domain (_e.g._\(f:\text{country}\mapsto\text{capital}\), unless the country is in South America), the test set includes two ground truth examples from the corrupted subdomain (_e.g._\(\mathtt{Peru}\mapsto\mathtt{undefined}\), \(\mathtt{Argentina}\mapsto\mathtt{undefined}\)). For entity functions that compute similarity to a reference concept (_e.g._\(\mathtt{Greek}\mathtt{Work}\mathtt{Mithology}\)), we provide the evaluator with only input concepts instead of I-O pairs (_e.g._\(\mathtt{Athena}\), \(\mathtt{skiing}\), \(\mathtt{GPU}\)), and ask which concept the function described by the interpreter is selective for. As test cases can be designed to isolate specific function behaviors, we find the unit testing protocol to be more sensitive to small differences in function descriptions (_e.g._ "transportation" vs "road transportation") than other description-matching methods, such as having an LM directly grade the agreement between descriptions (see the Appendix for more details).

We finetune Vicuna-13b (vicuna-evaluator) to perform the unit testing task and select representative samples matching descriptions of functions in the **find** dataset. LM-judges have been shown to be scalable and accurate surrogates for human judgments, which are otherwise expensive to obtain (Zheng et al., 2023). While proprietary models such as GPT-4 demonstrate strong agreement with humans, using such models to compare interpreter performance on a benchmark task incurs prohibitive costs associated with API access and poses reproducibility challenges as model versions are deprecated. We find that \(\mathtt{vicuna-evaluator}\) matches _ground-truth_ function descriptions to representative inputs and outputs more accurately than GPT-4, GPT-3.5, or pretrained Vicuna-13b (see Appendix for evaluation). Furthermore, \(\mathtt{vicuna-evaluator}\) makes judgments that are highly correlated with those of human subjects performing the same task (see Appendix for experiment details). The fine-tuned \(\mathtt{vicuna-evaluator}\) checkpoint and training dataset can be downloaded from the \(\mathtt{FIND}\) repository. Training details are provided in the Appendix.

**Extensions: Targeted evaluation.** For users of the \(\mathtt{FIND}\) benchmark that produce function interpretations in a structured format, this benchmark enables other evaluations targeted at specific end-use cases for interpretability tools. For example, researchers may use \(\mathtt{FIND}\) to explicitly evaluate whether \(\mathtt{interpreters}\) can pick out the portion of the domain that is corrupted, whether they can identify components of composed functions, or whether they identified the noise model.

## 4 Automated Interpretability Methods

\(\mathtt{FIND}\) can be used to evaluate any \(\mathtt{interpreter}\) system that has the ability to execute Python scripts autonomously at the command line. As a first demonstration, we evaluate several approaches inspired by recent work [16, 23, 24] that use pre-trained language models to perform interpretation. We run experiments using the OpenAI Python API. A custom plugin equips the \(\mathtt{interpreter}\) model with the command \(\mathtt{PYTHON}(\mathtt{function.py\ input})\) that it can use to call functions on its own selection of inputs. Scripts for reproducing these baselines and complete interpretation dialogues are available in the \(\mathtt{FIND}\) code repository.

We evaluate three different interpretation methods. (i) _Non-interactive_: interpretability tasks often involve descriptions of precomputed exemplars of function behavior. In this setting, the \(\mathtt{interpreter}\) is equipped with a fixed set of inputs to use for probing the functions, and is prompted to produce a description based only on function outputs for these exemplars, mirroring the non-interactive description paradigm used by \(\mathtt{milan}\)[16] and recent \(\mathtt{milan}\)-type LM analysis approaches [23]. (ii) _Automated Interpretability Agents_: LM-based \(\mathtt{aias}\) are prompted to interact with the functions in \(\mathtt{FIND}\). The \(\mathtt{interpreter}\) serves as an agent and runs \(\mathtt{PYTHON}(\mathtt{function.py\ input})\) on inputs it selects. In the base setting, the agents' search process is not initialized with pre-selected inputs; rather, the \(\mathtt{aia}\) forms hypotheses by itself, generates data to test its current hypothesis, and interactively updates it based on observations of function outputs. Figure 3 shows complete interactions between a GPT-4-based \(\mathtt{aia}\) and example functions in two categories. At the beginning of the interaction, the agent is prompted to run experiments on the functions to produce data that it can use to explain function behavior. Prompts for two categories of functions are reproduced below. (iii) We also evaluate a combination of both approaches (\(\mathtt{aia}\) + \(\mathtt{milan}\)). In this setting, the interpretability agent is prompted to begin interpretation with a provided set of exemplars, and then interactively continues the interpretation procedure.

All interpretation methods use off-the-shelf LMs (GPT-4 [12], GPT-3.5 (\(\mathtt{GPT-3.5-turbo}\)) [12], and Llama-2 (\(\mathtt{Llama-2-13b-chat}\), \(\mathtt{Llama-2-70b-chat}\)) [11]) as \(\mathtt{interpreters}\), and operate without a human in the loop. \(\mathtt{aia}\)-based interactions show structured interpretation procedures, even when the final explanation is not fully accurate (see the predicted numeric function in Figure 3). The \(\mathtt{aia}\) makes and tests informed hypotheses about the functions: for example, testing densely spaced points in a specific interval to reveal a "separating point" in the behavior of the numeric function, and testing different forms of transportation to determine whether a synthetic neuron is selective for land, sea, or air travel (Figure 3). Once the \(\mathtt{aia}\) has gathered sufficient data and produces a final description, the run stops automatically. The Appendix further describes experimental settings.

**Prompt for numeric functions:**

Hi, your job is to interpret a function f(x) that is implemented in the \(\mathtt{function.py}\) script. f(x) takes numerical inputs. All you can do is call f(x) on a value of x by running \(\mathtt{PYTHON}(\mathtt{function.py})\) value). Try to describe the function implemented by f(x) by running experiments on it. You can call the function on multiple inputs at a time by running \(\mathtt{PYTHON}(\mathtt{function.py})\) value|\(\mathtt{value2}\) value|\(\mathtt{3}\)...). You can run the function multiple times on the same input. We encourage testing a large range of values before writing a description. You can test values between \(\mathtt{-128}\) and \(\mathtt{128}\). The output could be noisy and your goal is to find a good simple description of f(x) that explains most of the function behavior. f(x) may combine multiple different operations. There may be an interval inside the domain where the function behaves differently. If that is the case, describe that interval. Your description of the function should have three parts. First,

[DEGSUPITION]: Describe that the function is doing in language.

[DORMAN]: What is the domain of the function? Is there an interval where the function displays unexpected behavior? What are the numeric bounds of that interval? What is going on there?

[CODE]: Write Python code that approximates the behavior of the function.

[MISSING_PAGE_FAIL:8]

## 5 Results

We evaluate the aia method as well as the non-interactive baselines with different off-the-shelf LMs. Results are summarized in Table 2 and example function descriptions are shown in Figure 4. Additional example interpretations of functions in all categories are provided in the Appendix.

**GPT-4 is a stronger interpretability agent than GPT-3.5 and Llama-2.** Success rates for all function categories are reported in Table 2 (aia columns). The GPT-4 interpretability agent achieves universally higher success rates than GPT-3.5 and Llama-2 (which often score at chance value). In fact, we find that Llama-2 often invents the output of the function without executing it (see examples in Appendix). This is also reflected in the mean length of interpretation dialogues, which are \(1.4,1.4,2.1\), and \(4.1\) for Llama-2-13b-chat, Llama-2-70b-chat, GPT-3.5 and GPT-4 respectively (we count the number of interpreter-function interactions). We additionally observe that interpretations of string functions receive significantly higher scores using the unit testing protocol compared to string-matching outputs of estimated code. As unit testing selects for representative examples of function behavior and not exact string-matches, the procedure is more forgiving of less specific descriptions, or descriptions with minor inaccuracies (see Appendix for additional discussion of unit testing limitations).

**Interactive vs. non-interactive.** In addition to aias, we evaluate two other interpretation methods: milan[1] where the interpreter produces a description based only on function outputs for a given set of exemplars, and a combination of aia + milan, where the same set of exemplars is used to initialize the interpretation session, and the agent is subsequently permitted to perform additional experimentation. For both settings, we sample the exemplars (two related to the function, eight distractors) from a human-constructed list of inputs associated with each function (see Appendix for details). In the non-interactive milan setting, we observe a small improvement in performance over uninitialized aias; GPT-4 interpretation performance improves from \(0.56\) to \(0.89\) on entity functions, and decreases slightly on relations, from \(.78\) to \(.74\). Initializing aias with milan exemplars and allowing additional experimentation dramatically boosts the performance of GPT-4 and GPT-3.5 agents (and also slightly improves Llama-2 performance), as shown in Table 2 and Figure 5 (see _init._). Notably, when initialized with exemplars, GPT-3.5 exhibits performance interpreting entity functions comparable to GPT-4. These results suggest that off-the

\begin{table}
\begin{tabular}{l c c c c c c c c c} \multicolumn{1}{c}{Code (exact match)} & \multicolumn{6}{c}{Language (unit test)} \\ \cline{2-10} \multicolumn{1}{c}{} & \multicolumn{1}{c}{Numeric} & \multicolumn{1}{c}{Strings} & \multicolumn{1}{c}{Strings} & \multicolumn{1}{c}{Entities} & \multicolumn{3}{c}{Relations} \\ \cline{2-10} \multicolumn{1}{c}{} & aIA & aIA & aIA & aIA & milan & aIA +milan & aIA & milan & aIA +milan \\ \hline Llama-2-13b-chat & 0 & 0 & 0.33 & 0.34 & 0.54 & 0.58 & 0.46 & 0.45 & 0.42 \\ Llama-2-70b-chat & 0.01 & 0.01 & 0.33 & 0.34 & 0.61 & 0.62 & 0.47 & 0.44 & 0.46 \\ GPT-3.5 & 0.12 & 0.13 & 0.66 & 0.39 & 0.81 & 0.88 & 0.37 & 0.64 & 0.68 \\ GPT-4 & **0.33** & **0.23** & **0.82** & **0.56** & **0.89** & **0.89** & **0.78** & **0.74** & **0.92** \\ \hline \end{tabular}
\end{table}
Table 2: **Interpretation success rates.** For each function type we report the successful estimation rate (**higher better**) based on different indicators, and with different experimental settings (_e.g._ initialization with exemplars).

Figure 4: **AIA (GPT-4) interpretations.** Examples from all **FIND** categories, with evaluation scores (NMSE and unit test) marked in red if below the success threshold (NMSE\(>0.1\), unit test\(<0.33\)) and green otherwise.

shelf LM agents are limited by breadth of search. Indeed, LMs tend to start sampling with simple words (_e.g._ apple, dog, car) which do not reveal function behavior for highly specific reference entities (_e.g._ _The New York Times, arachnids and arachnology_). We view exemplar computation as one of many "tools" that an interpretability agent could use, and hope that this benchmark will drive exploration of additional tools (_e.g._ example synthesis) as part of automated interpretation methods. Procedures that combine initialization and interactive experimentation could improve the efficiency of existing labeling approaches that use large fixed datasets (Bau et al., 2017) to precompute maximally activating inputs, and potentially also surface novel behaviors not captured in predefined sets of exemplars.

Which functions can LMs recover?Figure 4 shows examples of aia interpretations that successfully explain the behavior of some functions (_e.g._ cases (b),(d)), but fail to fully characterize more complex functions (_e.g._ cases (a),(c)), also reflected in some per-subcategory success scores (Figure 5). This is a limitation of using off-the-shelf LMs as agents: they may miss small corruptions to part of the domain (Figure 3(a)), which in real-world interpretability settings could stem from bias in the training set. LMs could be outfitted specifically for interpretability with additional tools (_e.g._ for sampling) and further improved by fine-tuning.

## 6 Related work

Explanation evaluation methods have previously benchmarked salience methods according to their ability to recover ground-truth segmentations (Zhang et al., 2018; Selvaraju et al., 2017; Fong and Vedaldi, 2017; Yang and Kim, 2019), or identify inputs that causally affect outputs (Zeiler and Fergus, 2014; Petsiuk et al., 2018; Wagner et al., 2019; DeYoung et al., 2020). Explanation benchmarks have also evaluated correlation with system performance (Adebayo et al., 2018; Casper et al., 2023) or human understanding of decisions (Kim et al., 2022). Our benchmark differs because we evaluate global explanations of black box functions instead of evaluating local explanations of decisions.

Model interpretability metrics have quantified the degree of model interpretability, _i.e._ by measuring how closely deep network features match a human-labeled concept (Bau et al., 2017; Kim et al., 2018; Goh et al., 2021; Wu et al., 2021; Burgess et al., 2018; Mu and Andreas, 2020; Geva et al., 2021). While these methods can measure disentanglement in model representations, they are only as good as their ability to identify interpretable features. We tackle this problem by providing a benchmark for interpretation methods themselves, rather than the models that they explain.

Full-text explanation systems provide natural-language explanations for black box systems or their features (Hendricks et al., 2016; Camburu et al., 2018; Ehsan et al., 2018; Kumar and Talukdar, 2020; Hernandez et al., 2022). Recent efforts exploit the capabilities of LMs to explain data directly, but these works utilize only tiny evaluation benchmarks, including 19 synthetic neuron puzzles in Bills et al. (2023) and 54 ground-truth module topics in Singh et al. (2023). Motivated by the promise of this LM-driven approach and the need to quantify performance, our work provides a comprehensive benchmark of full-text black-box explanation systems.

## 7 Conclusion

We introduce **Find**, a new benchmark for evaluating automated interpretability methods. Baseline results show early evidence that agents built from advanced LMs can construct hypotheses and experiments to validate them, supporting the suitability of LMs as general-purpose interpretability backbones. However, we find many functions that LM agents cannot sufficiently explain, suggesting augmentation with additional tools will be necessary for robust automation of interpretability tasks.

Figure 5: **AIA interpretation scores by subcategory (with GPT-4).** Complex functions are usually more difficult to interpret than atomic functions. String functions are evaluated using the string-matching indicator.

## Acknowledgments and Disclosure of Funding

We are grateful for the support of the MIT-IBM Watson AI Lab, the Open Philanthropy foundation, an Amazon Research Award, Hyundai NGV, ARL grant W911NF-18-2-0218, the National Science Foundation under Grant Nos. 2212310 and 2238240, the Zuckerman STEM Leadership Program, and the Viterbi Fellowship. The funders had no role in dataset design, experimental design or analysis, decision to publish, or preparation of the manuscript. The authors have no competing interests to report.

## References

* Adebayo et al. (2018) J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim. Sanity checks for saliency maps. _Advances in neural information processing systems_, 31, 2018.
* Bau et al. (2017) D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying interpretability of deep visual representations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6541-6549, 2017.
* Bau et al. (2020) D. Bau, J.-Y. Zhu, H. Strobelt, A. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual units in a deep neural network. _Proceedings of the National Academy of Sciences_, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907375117. URL https://www.pnas.org/content/early/2020/08/31/1907375117.
* Bills et al. (2023) S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html, 2023.
* Brown et al. (2020) T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Burgess et al. (2018) C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in \(\beta\)-vae. _arXiv preprint arXiv:1804.03599_, 2018.
* Camburu et al. (2018) O.-M. Camburu, T. Rocktaschel, T. Lukasiewicz, and P. Blunsom. e-snli: Natural language inference with natural language explanations. _Advances in Neural Information Processing Systems_, 31, 2018.
* Carpenter et al. (1990) P. A. Carpenter, M. A. Just, and P. Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. _Psychological review_, 97(3):404, 1990.
* Casper et al. (2023) S. Casper, Y. Li, J. Li, T. Bu, K. Zhang, and D. Hadfield-Menell. Benchmarking interpretability tools for deep neural networks. _arXiv preprint arXiv:2302.10894_, 2023.
* Chiang et al. (2023) W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
* Chollet (2019) F. Chollet. On the measure of intelligence. _arXiv preprint arXiv:1911.01547_, 2019.
* Conmy et al. (2023) A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023.
* DeYoung et al. (2020) J. DeYoung, S. Jain, N. F. Rajani, E. Lehman, C. Xiong, R. Socher, and B. C. Wallace. Eraser: A benchmark to evaluate rationalized NLP models. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4443-4458, 2020.
* Doshi-Velez and Kim (2017) F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. _arXiv preprint arXiv:1702.08608_, 2017.
* Ehsan et al. (2018) U. Ehsan, B. Harrison, L. Chan, and M. O. Riedl. Rationalization: A neural machine translation approach to generating natural language explanations. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 81-87, 2018.
* Ehsan et al. (2019)N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. https://transformer-circuits.pub/2021/framework/index.html.
* Fong and Vedaldi (2018) R. Fong and A. Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8730-8738, 2018.
* Fong and Vedaldi (2017) R. C. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In _Proceedings of the IEEE international conference on computer vision_, pages 3429-3437, 2017.
* Fukushima (1975) K. Fukushima. Cognitron: A self-organizing multilayered neural network. _Biological cybernetics_, 20(3-4):121-136, 1975.
* Geva et al. (2021) M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 5484-5495, 2021.
* Goh et al. (2021) G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah. Multimodal neurons in artificial neural networks. _Distill_, 6(3):e30, 2021.
* Grunwald and Van Ommen (2017) P. Grunwald and T. Van Ommen. Inconsistency of bayesian inference for misspecified linear models, and a proposal for repairing it. 2017.
* Gurnee et al. (2023) W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. _arXiv preprint arXiv:2305.01610_, 2023.
* Harris et al. (2020) C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del Rio, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.
* Hendricks et al. (2016) L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell. Generating visual explanations. In _ECCV 2016_, pages 3-19. Springer, 2016.
* Hernandez et al. (2022) E. Hernandez, S. Schwettmann, D. Bau, T. Bagashvili, A. Torralba, and J. Andreas. Natural language descriptions of deep visual features. In _International Conference on Learning Representations_, 2022. URL https://arxiv.org/abs/2201.11114.
* Hofstadter (1995) D. R. Hofstadter. _Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought_. Basic books, 1995.
* Hofstadter et al. (1995) D. R. Hofstadter, M. Mitchell, et al. The copycat project: A model of mental fluidity and analogy-making. _Advances in connectionist and neural computation theory_, 2:205-267, 1995.
* Hooker et al. (2019) S. Hooker, D. Erhan, P.-J. Kindermans, and B. Kim. A benchmark for interpretability methods in deep neural networks. _Advances in neural information processing systems_, 32, 2019.
* Kim et al. (2018) B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In _International conference on machine learning_, pages 2668-2677. PMLR, 2018.
* Kim et al. (2022) S. S. Kim, N. Meister, V. V. Ramaswamy, R. Fong, and O. Russakovsky. Hive: evaluating the human interpretability of visual explanations. In _ECCV 2022_, pages 280-298. Springer, 2022.
* Kumar and Talukdar (2020) S. Kumar and P. Talukdar. NILE: Natural language inference with faithful natural language explanations. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8730-8742, 2020.
* Lightman et al. (2023) H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let's verify step by step, 2023.

Z. C. Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. _Queue_, 16(3):31-57, 2018.
* Lovett and Forbus [2017] A. Lovett and K. Forbus. Modeling visual problem solving as analogical reasoning. _Psychological review_, 124(1):60, 2017.
* Mahendran and Vedaldi [2015] A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5188-5196, 2015.
* Meng et al. [2022] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. _Advances in Neural Information Processing Systems_, 36, 2022.
* Miller [2019] T. Miller. Explanation in artificial intelligence: Insights from the social sciences. _Artificial intelligence_, 267:1-38, 2019.
* Mitchell [2021] M. Mitchell. Abstraction and analogy-making in artificial intelligence. _Annals of the New York Academy of Sciences_, 1505(1):79-101, 2021.
* Mu and Andreas [2020] J. Mu and J. Andreas. Compositional explanations of neurons. _Advances in Neural Information Processing Systems_, 33:17153-17163, 2020.
* Nair and Hinton [2010] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 807-814, 2010.
* Nanda et al. [2022] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In _The Eleventh International Conference on Learning Representations_, 2022.
* Oikarinen and Weng [2023] T. Oikarinen and T.-W. Weng. Clip-dissect: Automatic description of neuron representations in deep vision networks. _International Conference on Learning Representations_, 2023.
* Olah et al. [2017] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. _Distill_, 2(11):e7, 2017.
* Olah et al. [2020] C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits. _Distill_, 5(3):e00024-001, 2020.
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* Petsiuk et al. [2018] V. Petsiuk, A. Das, and K. Saenko. Rise: Randomized input sampling for explanation of black-box models. In _Proceedings of the British Machine Vision Conference_, 2018.
* Selvaraju et al. [2017] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* Singh et al. [2023] C. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu, and J. Gao. Explaining black box text modules in natural language with language models, 2023.
* Sorokin and Gurevych [2018] D. Sorokin and I. Gurevych. Modeling semantics with gated graph neural networks for knowledge base question answering. In _Proceedings of the 27th International Conference on Computational Linguistics_, pages 3306-3317, 2018.
* Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* Van Rossum [2020] G. Van Rossum. _The Python Library Reference, release 3.8.2_. Python Software Foundation, 2020.
* Van Rossum [2020]J. W. Vaughan and H. Wallach. A human-centered agenda for intelligible machine learning. _Machines We Trust: Getting Along with Artificial Intelligence_, 2020.
* Virtanen et al. [2020] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. _Nature methods_, 17(3):261-272, 2020.
* Vrandecic and Krotzsch [2014] D. Vrandecic and M. Krotzsch. Wikidata: a free collaborative knowledgebase. _Communications of the ACM_, 57(10):78-85, 2014.
* Wagner et al. [2019] J. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, and S. Behnke. Interpretable and fine-grained visual explanations for convolutional neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9097-9107, 2019.
* Wang and Su [2015] K. Wang and Z. Su. Automatic generation of raven's progressive matrices. In _Twenty-fourth international joint conference on artificial intelligence_, 2015.
* Wang et al. [2022] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. In _The Eleventh International Conference on Learning Representations_, 2022.
* Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* Wu et al. [2021] Z. Wu, D. Lischinski, and E. Shechtman. Stylespace analysis: Disentangled controls for stylegan image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12863-12872, 2021.
* Yang and Kim [2019] M. Yang and B. Kim. Benchmarking Attribution Methods with Relative Feature Importance. _CoRR_, abs/1907.09701, 2019.
* Yao et al. [2023] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.
* Zeiler and Fergus [2014] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In _ECCV 2014_, pages 818-833. Springer, 2014.
* Zhang et al. [2018] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. Top-down neural attention by excitation backprop. _International Journal of Computer Vision_, 126(10):1084-1102, 2018.
* Zheng et al. [2023] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging LLM-as-a-judge with MT-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.
* Zhou et al. [2014] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. _arXiv preprint arXiv:1412.6856_, 2014.

## Appendix A Limitations

We view find as a test of necessary, but not sufficient, capabilities for automated interpretation. The ultimate test of these interpretation methods' effectiveness must be their ability to generate actionable insights about real models, which find does not evaluate. But clean, simple benchmarks with ground-truth answers have been a major driver of more general capabilities in LMs, and we hope that find can play a similar role in interpretability research. Additionally, the current release of find only includes black-box interpretation problems. This style of problem is relevant to many existing automated procedures such as NetDissect (Bau et al., 2017) and milan (Hernandez et al., 2022), which treat single neurons as black boxes; however, most interpretability work occurs in a white-box setting. We intend for find to be extended in the future to incorporate white-box function interpretation problems, including descriptions of individual components of neural circuits (the IOI circuit from Wang et al. (2022) is just one example), some of which could be represented as the types of composition problems that are already included in find, but where interpreters access and label individual sub-computations inside composed functions.

## Appendix B Ethics statement

find includes entities drawn from Wikidata that represent real-world concepts like _video games_, _paleontology_, and _airports_, as well as more potentially sensitive topics like _The Holocaust_, _World War II_, and _disasters_ (see Appendix D). We include these topics in find because they are relevant to behaviors inside neural networks trained on real-world data, that we want automated interpretation procedures to be able to describe. We additionally note that interpreters which test the semantic similarity between input concepts and reference entities may surface surprising or controversial similarity scores that stem from learned biases inside the LM backbone of the synthetic neural modules (_e.g._ men having a higher similarity score with _mathematics_ than women). Advanced interpretation procedures would be able to name and describe these biases, but the find evaluation protocol currently does not test ability to discover biases other than those included explicitly in find (via corruptions to function subdomains).

## Appendix C Additional interpretation examples

Below we provide additional examples of aia interpretations performed with a GPT-4 agent for each of the find categories. For numeric functions, we report the NMSE score (lower is better) marked in green if the score is below the success threshold, or red otherwise. For strings and synthetic neural modules, we report the unit test score (higher is better) marked in green if the score is above chance, or red otherwise. Complete interpretation dialogues are available in the find code repository.

Figure 1: **Numeric function interpretation examples.** Plots compare the code interpretation to the ground-truth function implementation. Points on the **find** function sampled by the GPT-4 aia are indicated. NMSE scores are marked in green if the score is below the success threshold (\(0.1\)), or red otherwise.

[MISSING_PAGE_EMPTY:17]

FIND documentation

The **Find** API, benchmark dataset, and associated metadata are open-sourced under an MIT license and available at: https://github.com/multimodal-interpretability/FIND.

The **Find** repository contains the utilities necessary for reproducing benchmark results for the LM baselines reported in the paper, and running and evaluating interpretation of the **Find** functions with other interpreters defined by the user.

The **Find** dataset itself is hosted on Zenodo with DOI: 10.5281/zenodo.8034162. The GitHub README provides instructions for downloading the dataset file into the appropriate directory for use with the **Find** utilities.

**Responsibility statement.** The authors affirm that they have full authority to license the dataset under the terms of the MIT License and they accept all responsibilities for the dataset.

**Dataset.** Each function is expressed as an independent.py file that can be called by the interpreter with the **Find** utilities. The benchmark dataset includes:

1. 1000 numeric functions sampled from the **Find** API (see section E)
2. 1000 string functions sampled from the **Find** API (see section F)
3. 275 synthetic neural modules with an LM backbone (see section G )

An interpreter can be tested on a single function or the entire **Find** dataset using collect_interpretations.py. Most of the functions are self-contained, see one example here (find_dataset/numeric/f00470/function_code.py):

import numpy as np imports

def function(x):  return (-4.5xx**4 - 0.7xx**3 - 2.6x**2 + 0.4x**1 + 3.9) *  (-25.3 * np.where(x > 0, x, x * 0.3) + 5.2)

if _name_ == '_main_':  outputs = ''_'  for arg in sys.argv[1:]:  x = float(arg)  try:  out = function(x)  except:  out = ''None'  outputs += f'((arg), (out))'  print(f'function input - output pairs: (outputs)')

For numeric functions that load trained MLPs, the dataset includes a.pt file with model weights in the same folder as function_code.py. Other functions (synthetic neural modules) that are implemented with an LM backbone rely on calls to a self-hosted Vicuna inference server. For example, this function (find_dataset/neurons_entities/f00013/function_code.py) simulates a neuron selective for works of fiction by measuring Vicuna output logits:

[MISSING_PAGE_FAIL:19]

\(15\%\) of the numeric functions in the **FIND** dataset are compositions of two atomic functions \(f(x)\) and \(g(x)\). To limit the complexity of the compositions, we choose \(f(x)\) and \(g(x)\) from a subset \(\mathcal{A}_{\mathcal{C}}\) of atomic functions, where \(\mathcal{A}_{\mathcal{C}}=\texttt{linear}\), polynomial, step, RELU, constant, ceiling, floor, rectangle, square wave, and combine them using only multiplication and addition.

**Find** is extensible: the API can be used as a general numeric function generator, allowing the user to make other design choices targeting specific interpretability applications.

## Appendix F String function API details

The API for generating string functions can be found at:

https://github.com/multimodal-interpretability/FIND/src/make_functions/make_strings.

All functions and sampling parameters used to create the **Find** dataset are articulated in **string_functions.py**. Like with numeric functions, we define a subset (described in the API) of string functions with lower complexity to use in compositions.

## Appendix G Synthetic neural modules

To validate the reliability of Vicuna as a backbone for the synthetic neural modules, we score its execution of the **Find** functions on human labels associated with each entity in **Find**, and ground-truth factual relation pairs (extracted from Wikidata).

### Entities

For each entity, we construct a list of 10 associated concepts (_e.g._ for the _climate_ entity, associated words include humidity, temperature, atmosphere). In Table A1 we list all entities included in **Find**, and report the mean function output across 10 human labels associated with each entity (function output is computed from Vicuna output logits) and the mean function output for 10 labels randomly sampled from the rest of the dataset. A score closer to 1 indicates stronger association between input concept and reference entity. For all entities in **Find**, Vicuna scores reliably distinguish between associated and unassociated concepts. We additionally show selectivity of each entity function for concepts associated with that entity relative to all other entities in Figure A4.

\begin{table}
\begin{tabular}{l c c} \hline
**Entity** & **Same Entity Score** & **Random Entity Score** \\ \hline rail transport & 0.672 & 0.001 \\ cycling & 0.846 & 0.011 \\ the New York Times & 0.703 & 0.163 \\ aircraft & 0.737 & 0.003 \\ bodies of water & 0.992 & 0.094 \\ mineralogy & 0.984 & 0.054 \\ occupations & 0.976 & 0.178 \\ professional wrestling & 0.872 & 0.004 \\ quantity indicating a percentage & 0.996 & 0.148 \\ plays & 0.546 & 0.149 \\ horses & 0.964 & 0.005 \\ photography & 0.977 & 0.026 \\ music & 0.965 & 0.038 \\ fashion & 0.898 & 0.154 \\ chess & 0.700 & 0.004 \\ racket sports & 0.828 & 0.129 \\ art & 0.991 & 0.457 \\ disasters & 0.687 & 0.022 \\ spacecraft & 0.915 & 0.018 \\ video games & 0.991 & 0.035 \\ the relationship of an element to its class & 0.893 & 0.708 \\ basketball & 0.767 & 0.009 \\ winter sports & 0.984 & 0.002 \\ American football & 0.849 & 0.036 \\ golf & 0.925 & 0.026 \\ baseball & 0.893 & 0.006 \\ ice hockey & 0.663 & 0.003 \\ women and feminism & 0.819 & 0.120 \\ lighthouses & 0.493 & 0.010 \\ tennis & 0.804 & 0.008 \\ water sports & 0.958 & 0.014 \\ comics & 0.637 & 0.012 \\ algorithms & 0.801 & 0.021 \\ tourism & 0.690 & 0.186 \\ bridges & 0.685 & 0.012 \\ books & 0.774 & 0.111 \\ linguistics & 0.995 & 0.464 \\ utilization and ownership & 0.402 & 0.206 \\ online communities & 0.889 & 0.084 \\ television & 0.851 & 0.023 \\ astronomy & 0.995 & 0.054 \\ disability & 0.637 & 0.023 \\ proteins & 0.399 & 0.072 \\ human anatomy & 0.966 & 0.053 \\ gymnastics & 0.751 & 0.026 \\ architecture & 0.800 & 0.055 \\ sculpture & 0.795 & 0.030 \\ archaeology & 0.836 & 0.083 \\ theater & 0.899 & 0.104 \\ dams & 0.776 & 0.012 \\ birds and ornithology & 0.993 & 0.070 \\ computing & 0.988 & 0.302 \\ time and duration & 0.699 & 0.052 \\ age & 0.832 & 0.040 \\ weapons and military equipment & 0.820 & 0.003 \\ ratios and proportions & 0.446 & 0.155 \\ typefaces and typography & 0.911 & 0.094 \\ burials graves and memorials & 0.878 & 0.010 \\ processes and manufacturing & 0.645 & 0.058 \\ paleontology & 0.934 & 0.054 \\ banking & 0.922 & 0.034 \\ lakes & 0.552 & 0.060 \\ \hline \end{tabular}
\end{table}
Table 1: continued from previous page

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Entity** & **Same Entity Score** & **Random Entity Score** \\ \hline natural science & 0.970 & 0.359 \\ geography & 0.981 & 0.039 \\ insects and entomology & 0.897 & 0.033 \\ philosophy & 0.991 & 0.168 \\ poetry & 0.991 & 0.071 \\ encyclopedias & 0.642 & 0.350 \\ television shows & 0.914 & 0.095 \\ awards prizes and honours & 0.997 & 0.159 \\ the Middle Ages & 0.871 & 0.053 \\ plants and botany & 0.994 & 0.189 \\ marine biology & 0.996 & 0.059 \\ color & 0.780 & 0.080 \\ airports & 0.712 & 0.003 \\ science & 0.946 & 0.284 \\ anime and manga & 0.969 & 0.094 \\ Christiantiy & 0.988 & 0.015 \\ Buddhism & 0.993 & 0.002 \\ Greek mythology & 0.949 & 0.013 \\ Judiasm and the Jewish people & 0.965 & 0.012 \\ music genres & 0.987 & 0.144 \\ fictional characters & 0.892 & 0.144 \\ rap and hip hop & 0.693 & 0.055 \\ Islam & 0.949 & 0.016 \\ The Walt Disney Company & 0.937 & 0.005 \\ agriculture & 0.982 & 0.104 \\ hiking & 0.384 & 0.018 \\ New York City & 0.969 & 0.113 \\ gardens & 0.896 & 0.093 \\ personality traits & 0.879 & 0.172 \\ camping & 0.728 & 0.065 \\ gender & 0.555 & 0.049 \\ cemeteries and graves & 0.984 & 0.027 \\ London & 0.954 & 0.105 \\ Los Angeles & 0.949 & 0.042 \\ Chicago & 0.959 & 0.047 \\ Paris & 0.789 & 0.017 \\ Berlin & 0.923 & 0.052 \\ sailing & 0.897 & 0.001 \\ swimming & 0.901 & 0.015 \\ the Holoceneust & 0.788 & 0.007 \\ arachnids and arachnology & 0.798 & 0.128 \\ musical instruments & 0.997 & 0.042 \\ meteorology & 0.926 & 0.011 \\ disease & 0.787 & 0.010 \\ Ireland & 0.784 & 0.020 \\ libraries & 0.612 & 0.122 \\ museums & 0.848 & 0.058 \\ journalism & 0.883 & 0.020 \\ buildings & 0.795 & 0.153 \\ cryptocurrencies & 0.951 & 0.001 \\ events and news & 0.725 & 0.229 \\ Louisiana & 0.745 & 0.012 \\ revolutions & 0.623 & 0.076 \\ mines and mining & 0.918 & 0.039 \\ Switzerland & 0.947 & 0.050 \\ World War II & 0.986 & 0.064 \\ \hline
**Average** & **0.846** & **0.079** \\ \hline \hline \end{tabular}
\end{table}
Table 1: continued from previous pageFigure A4: Entity functions implemented using Vicuna respond more strongly to concepts related to their reference entity (diagonal) than to concepts related to other entities (off-diagonal). The value in row \(i\) and column \(j\) corresponds to the average output of the entity \(e_{i}\) function for 10 inputs associated with entity \(e_{j}\). Each row is normalized using softmax. The prompt template used to instruct Vicuna to implement each function is shown in Section G.

## Appendix D2 Relations

**find** includes 75 synthetic neural modules that map inputs to outputs using real-world factual relations drawn from Wikidata properties. The set of relation functions is built from \(25\) atomic relations (_e.g._ country \(\mapsto\) capital) and two domain corruptions per relation, where the function returns undefined for a small region of the domain (_e.g._ country \(\mapsto\) capital EXCEPT if country in South America, then return undefined). Table A2 lists atomic relation functions included in **find**, and the two domain corruptions applied to each function. We evaluate Vicuna accuracy applying relation functions to input concepts by scoring the accuracy of the mapping applied to \(10\) inputs per function, where inputs are drawn from concepts in Wikidata with the property applied by each relation function. Mean Vicuna accuracy across relations in **find** is \(91.2\%\).

## Appendix E Interpretation

Utilities for reproducing interpretation experiments and adding other, user-defined interpreters are provided in the GitHub repository. We report interpretation baselines using GPT-4, GPT-3.5 ( GPT-3.5-turbo), and Llama-2 (Llama-2-13b-chat, Llama-2-70b-chat). Interpretation dialogues never exceeded context length of 8K tokens for GPT-4, 4K tokens for GPT 3.5, and 2048 for Llama-2. Full interpretation dialogues from our baselines can be downloaded from https://data.csail.mit.edu/FIND/FIND-interpretations.zip.

## Appendix E1 Llama-2 hallucination

We note that Llama-2 often fails to engage with the functions and instead hallucinates outputs before function output values are returned to the interpretation dialogue. Example hallucinations are shown below for Llama-2-13b-chat interpreting a synthetic neural module function. We tested different system prompts to encourage Llama-2 to perform the task. The top-performing Llama-2 prompt is reproduced in Section G.

## Appendix E2 Sampling initialization experiments (aia +milan)

Initialization experiments were performed for synthetic neural modules where search strategy (initially guessing very simple words) handicapped interpreter performance. As described in Section 5 of the main paper, in this setting the interpreter was provided with a list of 10 inputs per function to use for initialization (two associated with the function, eight distractors). These exemplars were sampled from the lists of human-labeled concepts associated with each function as described in Appendix G. The set of suggested initial inputs we used for each function is included in the **find** dataset as a JSON file in each synthetic neural module subfolder.

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_EMPTY:26]

similar behavior for relations with domain corruptions (e.g. descriptions of "maps countries to capitals" and "maps countries to capitals except in South America" are judged as highly similar).

## Appendix G Prompts

Interpreter prompts for all function categories, system prompts, and Vicuna prompts are provided in the GitHub Repository. Those not shown in the main paper are reproduced below.

**Entity function Vicuna prompt**

I am going to give you an input. Return only 1 if that input is related to {entity}. Return only 0 otherwise. Do not return any additional information other than the 0 or 1 answer.

**Input**: {input}.

**Entity function Vicuna prompt: composition**

I am going to give you an input. Return only 1 if that input is related to {entity}. Return only 1 if that input is related to {entity}. Return only 0 otherwise. Do not return any additional information other than the 0 or 1 answer.

**Input**: {input}.

**Entity function Vicuna system prompt**

You are an AI trained to identify words related to input concepts.

**Relation function Vicuna prompt**

I am going to give you an input. If the input is {concept1}, return only {concept2}. If the input is not {concept1}, return 'undefined'. Input: {input}.

**Relation function Vicuna prompt**

I am going to give you an input. If the input is {concept1}, return only {concept2}. If the input is not {concept1}, return 'undefined'. If {exception}, return 'undefined'. Input: {input}.

**Relation function Vicuna system prompt**

You are a helpful assistant. You apply real-world knowledge to map input words to output words according to a rule provided by the user. Try to be as precise as possible.

[MISSING_PAGE_FAIL:28]