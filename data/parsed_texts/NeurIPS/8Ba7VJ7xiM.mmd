# Analyzing Generalization of Neural Networks

through Loss Path Kernels

 Yilan Chen

UCSD CSE

yilan@ucsd.edu

&Wei Huang

RIEKN AIP

wei.huang.vr@riken.jp

&Hao Wang

MIT-IBM Watson AI Lab

hao@ibm.com

Charlotte Loh

MIT EECS

cloh@mit.edu

&Akash Srivastava

MIT-IBM Watson AI Lab

akash.srivastava@ibm.com

&Lam M. Nguyen

IBM Research

LamNguyen.MLTD@ibm.com

&Tsui-Wei Weng

UCSD HDSI

lweng@ucsd.edu

Correspondence to: Yilan Chen and Tsui-Wei Weng.

###### Abstract

Deep neural networks have been increasingly used in real-world applications, making it critical to ensure their ability to adapt to new, unseen data. In this paper, we study the generalization capability of neural networks trained with (stochastic) gradient flow. We establish a new connection between the loss dynamics of gradient flow and general kernel machines by proposing a new kernel, called loss path kernel. This kernel measures the similarity between two data points by evaluating the agreement between loss gradients along the path determined by the gradient flow. Based on this connection, we derive a new generalization upper bound that applies to general neural network architectures. This new bound is tight and strongly correlated with the true generalization error. We apply our results to guide the design of neural architecture search (NAS) and demonstrate favorable performance compared with state-of-the-art NAS algorithms through numerical experiments.

## 1 Introduction

Deep learning models have been increasingly used in applications with significant societal impact. Therefore, it is crucial to ensure that these models perform well not only on the training data but also on the new and unseen data. Classical learning theory attributes the generalization ability of machine learning (ML) models to the small complexity of the hypothesis class [56]. However, modern ML models, such as deep neural networks (NNs), can have billions of parameters yet still exhibit strong generalization abilities [26, 9]. This is because various elements of the learning algorithms, including optimization methods, training data, and neural architectures, can all influence the inductive bias, which in turn shapes the generalization abilities of neural networks [29, 40]. While the overall hypothesis class may be large, the "effective domain" of this class, which ultimately determines the model's generalization abilities, is often much smaller [54, 24, 50, 15]. Hence, it is vital to develop algorithmic generalization bounds to capture this effective domain of the hypothesis class.

There has been significant work investigating the generalization of neural networks in their infinite-width regime through kernel methods [28, 3, 32, 2, 11]. They showed that an infinite-width NNtrained by gradient flow and squared loss is equivalent to a kernel regression with neural tangent kernel (NTK) [28; 3]. Moreover, Arora et al. [2], Cao & Gu [11] further characterized the generalization behaviors of such ultra-wide NNs by deriving data-dependent and NTK-based generalization bounds. However, they only considered ultra-wide fully connected NNs with a square (or logistic) loss function. In practice, NNs are usually not ultra-wide and have more complex architectures such as CNNs. Hence, it is crucial to establish generalization bounds that hold in a more general setting.

In this paper, we analyze the generalization capability of NNs trained using (stochastic) gradient flow across a wide range of NN architectures. Our key technical contribution is to establish a new connection between the loss dynamics of (stochastic) gradient flow and general kernel machines with a special kernel that we named the _loss path kernel_. This new kernel calculates the similarity between two data points by integrating the inner product of the loss gradient evaluated at these points, along the path determined by the gradient flow. Based on this connection, we develop a novel generalization bound by analyzing the complexity of kernel machines induced by various training sets. Our generalization bound is tight and can be applied to a broad class of NN architectures, not restricted to ultra-wide NNs (see Table 1 for a comparison with existing results derived from NTK theory). Numerical experiments demonstrate that our bound maintains a strong correlation with the true generalization error of NNs trained with gradient descent (GD) (see Figure 1 and 2 in Sec. 6 & 7). Given this observation, we use our generalization bound to guide the design of neural architecture search (NAS) and demonstrate through numerical experiments that our approach can achieve a favorable performance compared with state-of-the-art training-free and minimum-training NAS algorithms [37; 13; 39]. In summary, our contributions are:

* In Sec. 4.1 and 5, we show for the first time that the loss of NNs trained by (stochastic) gradient flow is equivalent to a _general_ kernel machine. This result enables us to investigate the generalization capability of NNs from the perspective of kernel theory.
* In Sec. 4.2 and 5, we derive tight generalization bounds for NNs based on the aforementioned equivalence. Our result is very general as it holds for any continuously differentiable NN architectures including finite-width and infinite-width NNs. Experiments demonstrate that our bounds are tight (4690\(\times\) tighter than existing norm-based bounds and 55\(\times\) tighter than existing NTK-based bounds as shown in Appendix A.2) and highly correlated with the true generalization error.
* In Sec. 6 and 7, we apply our theory to study special cases including infinite-width NNs, stable algorithms, and norm-constrained NNs. We apply our bound to guide the design of NAS. Numerical experiments demonstrate that our approach achieves a favorable performance compared with state-of-the-art NAS algorithms on NAS-Bench-201 benchmark [19].

## 2 Related Work

**Generalization theory in deep learning.** Generalization is a crucial aspect of deep learning theory and various techniques have been proposed to study it. For example, Bartlett et al. [7] derived tight bounds for the VC dimension of NNs with ReLU activation functions. There is also a line of work that measures the capacity of NNs based on different norms, margins [5; 41; 6; 44], and sharpness-based measures [42; 43; 1] to explain the generalization behaviors of NNs. Additionally, there are theories studying the generalization of NNs from PAC-Bayes [35; 22] and information-theoretical approach

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Arora et al. [2] & Cao \& Gu [11] & **Ours** \\ \hline Bound & \(\sqrt{\frac{2\mathbf{Y}^{\top}(\mathbf{H}^{\infty})^{-1}\mathbf{Y}}{n}}\) & \(\tilde{O}(L\cdot\sqrt{\frac{\mathbf{Y}^{\top}(\mathbf{G})^{-1}\mathbf{Y}}{n}})\) & Theorem 3, Theorem 5 \\ Model & Ultra-wide two-layer FCNN & Ultra-wide FCNN & **General continuously differentiable NN** \\ Data & i.i.d. data with \(\|\bm{x}\|=1\) & i.i.d. data with \(\|\bm{x}\|=1\) & i.i.d. data with \(\|\bm{x}\|=1\) & i.i.d. data \\ Loss & Square loss & Logistic loss & **Continuously differentiable \& bounded loss** \\ During training & No & No & **Yes** \\ Multi-outputs & No & No & **Yes** \\ Training algorithm & GD & SGD & (Stochastic) gradient flow \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing NTK-based generalization bounds. \(\mathbf{\Theta}=\Theta(\mathbf{X},\mathbf{X})\) is the NTK on training samples and \(\mathbf{H}^{\infty}\) is the NTK of the first layer. \(L\) represents the number of NN layers. We highlight some unique properties of our results in blue color. “During training” means our bound can be calculated at any time during training while existing NTK-based bounds only hold for the NNs at convergence. “Multi-outputs” means our bound holds for NNs with multiple outputs.

[48; 58]. For example, Dziugaite & Roy [22] numerically evaluated and optimized the PAC-Bayes bound of stochastic NN and obtained a non-vacuous generalization bound. In contrast, we study the generalization of NNs by building a new connection between NNs and kernel machines. We refer readers to Valle-Perez & Louis [55] for a more comprehensive review of the generalization theory of NNs.

**Neural tangent kernel (NTK).** NTK was first introduced in Jacot et al. [28], where the authors demonstrated that a fully-trained, infinite-width NN follows kernel gradient descent in the function space with respect to the NTK. Under gradient flow and squared loss, the fully-trained infinite-width NN is equivalent to kernel regression with the NTK [28; 3]. Chen et al. [14] further established the equivalence between infinite-width NNs and regularized kernel machines. Arora et al. [2] studied the generalization capacity of ultra-wide, two-layer NNs trained by GD and square loss, while Cao & Gu [11] examined the generalization of deep, ultra-wide NNs trained by stochastic gradient descent (SGD) and logistic loss. Both studies derived generalization bounds of converged NNs based on NTK. Besides, Huang et al. [27] studied the convergence and generalization of PAC-Bayesian learning for deep, ultra-wide NNs. Later, Domingos [17] showed that every model trained by gradient flow is a "kernel machine" with the weights and bias as functions of input data, which however can be much more complex than a typical kernel machine and our general kernel machine in Definition 2. Chen et al. [14] showed that every NN trained by gradient flow is a general kernel machine but their kernel is valid only in very limited cases - when the loss gradient of output is a constant. Otherwise, the kernel is not symmetric and not valid. In this paper, we consider an equivalence between the loss of NNs and general kernel machines, which resolves the previous asymmetric problem of the kernel function and also makes the generalization analysis of multi-outputs easier.

**Neural Architecture Search (NAS).** NAS aims to automate the discovery of top-performance neural networks to reduce human efforts. However, most existing NAS algorithms require heavy training of a supernet or intensive architecture evaluations, suffering from heavy resource consumption [47; 34; 18; 33]. Thus, it is crucial to develop training-free or minimum-training NAS algorithms to reduce the computational cost and select the best architecture at the same time [37; 13; 39]. Since our generalization bound has a strong correlation with the true generalization error, we apply it to design a new minimum-training NAS algorithm. We demonstrate in Table 2 that with a simple random search algorithm, our approach can achieve a favorable performance compared with state-of-the-art training-free and minimum-training NAS algorithms.

## 3 Kernel Machine and Loss Path Kernel

In this section, we define notation, provide a brief overview of kernel methods, and introduce the main concept of interest--the loss path kernel.

### Preliminaries

Consider a supervised learning problem where the task is to predict an output variable in \(\mathcal{Y}\subseteq\mathbb{R}^{k}\) using a vector of input variables in \(\mathcal{X}\subseteq\mathbb{R}^{d}\). Let \(\mathcal{Z}\triangleq\mathcal{X}\times\mathcal{Y}\). We denote the training set by \(\mathcal{S}\triangleq\left\{\bm{z}_{i}\right\}_{i=1}^{n}\) with \(\bm{z}_{i}\triangleq(\bm{x}_{i},\bm{y}_{i})\in\mathcal{Z}\). We assume each point is drawn i.i.d. from an underlying distribution \(\mu\). Let \(\mathbf{X}=[\bm{x}_{1},\cdots,\bm{x}_{n}]^{T}\in\mathbb{R}^{n\times d}\), \(\mathbf{Y}=[\bm{y}_{1},\cdots,\bm{y}_{n}]^{T}\in\mathbb{R}^{n\times k}\), and \(\mathbf{Z}=[\mathbf{X},\mathbf{Y}]\in\mathbb{R}^{n\times(d+k)}\).

We express a neural network in a general form \(f(\bm{w},\bm{x}):\mathbb{R}^{p}\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{k}\) where \(\bm{w}\in\mathbb{R}^{p}\) represents its parameters and \(\bm{x}\in\mathbb{R}^{d}\) is an input variable. The goal of a learning algorithm is to find a set of parameters that minimizes a population risk \(L_{\mu}(\bm{w})=\mathbb{E}_{\bm{z}\sim\mu}\left[\ell(\bm{w},\bm{z})\right]\) where \(\ell(\bm{w},\bm{z})\triangleq\ell(f(\bm{w},\bm{x}),\bm{y})\) is a loss function. Throughout this paper, we assume that \(\ell(\bm{w},\bm{z})\in[0,1]\) and is continuously differentiable. In practice, the underlying distribution \(\mu\) is unknown so the learning algorithm minimizes an empirical risk on the training set \(\mathcal{S}\) instead: \(L_{\mathcal{S}}(\bm{w})=\frac{1}{n}\sum_{i=1}^{n}\ell(\bm{w},\bm{z}_{i})\). The _generalization gap_ is defined as \(L_{\mu}(\bm{w})-L_{\mathcal{S}}(\bm{w})\). The loss gradient with respect to the parameters \(\bm{w}\) is \(\nabla_{\bm{w}}\ell(\bm{w},\bm{z})=\nabla_{\bm{w}}f(\bm{w},\bm{x})^{\top}\nabla _{f}\ell(f(\bm{w},\bm{x}),\bm{y})\in\mathbb{R}^{p\times 1}\). Our analysis only requires that \(\ell(\bm{w},\bm{z})\) is continuously differentiable w.r.t. \(\bm{w}\) and \(\bm{z}\) so \(f\) can be either a fully connected neural network or a convolutional network or a residual network.

### Kernel Method

Kernel methods [16; 51; 53] search for linear relations in high-dimensional feature space by using kernel functions. Rather than computing the coordinates in the feature space explicitly, they only need to calculate the inner product between data pairs, making it computationally easier.

A kernel is a function \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), such that for all \(\bm{x},\bm{x}^{\prime}\in\mathcal{X}\), \(K(\bm{x},\bm{x}^{\prime})=\langle\Phi(\bm{x}),\Phi(\bm{x}^{\prime})\rangle\), where \(\Phi:\mathcal{X}\to\mathcal{F}\) is a mapping from \(\mathcal{X}\) to an (inner product) feature space \(\mathcal{F}\).

**Proposition 1** (Shawe-Taylor et al. [53]).: _A function \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), which is either continuous or has a finite domain, is a kernel function if and only if it is a symmetric function and, for any finite subset of \(\mathcal{X}\), \(\bm{x}_{1},\cdots,\bm{x}_{n}\in\mathcal{X}\), the matrix \(K(\mathbf{X},\mathbf{X})\) is positive semi-definite, where \(K(\mathbf{X},\mathbf{X})\) is a \(n\times n\) matrix whose \((i,j)\)-th entry is \(K(\bm{x}_{i},\bm{x}_{j})\)._

**Definition 1** (Kernel machine).: Let \(\mathcal{H}\) be the reproducing kernel Hilbert space (RKHS) corresponding to a kernel \(K(\bm{x},\bm{x}^{\prime})=\langle\Phi(\bm{x}),\Phi(\bm{x}^{\prime})\rangle\). A kernel machine \(g:\mathcal{X}\to\mathbb{R}\) is a linear function in \(\mathcal{H}\) such that its weight vector \(\bm{\beta}\) can be expressed as a linear combination of the training points, i.e. \(g(\bm{x})=\langle\bm{\beta},\Phi(\bm{x})\rangle+b=\sum_{i=1}^{n}a_{i}K(\bm{x}_ {i},\bm{x})+b\), where \(\bm{\beta}=\sum_{i=1}^{n}a_{i}\Phi(\bm{x}_{i})\) and \(b\) is a constant. The RKHS norm of \(g\) is \(\left\|g\right\|_{\mathcal{H}}=\left\|\sum_{i=1}^{n}a_{i}\Phi(\bm{x}_{i}) \right\|=\sqrt{\sum_{i,j}a_{i}a_{j}K(\bm{x}_{i},\bm{x}_{j})}\).

Next, we introduce general kernel machine, which generalizes the concept of kernel machine.

**Definition 2** (General kernel machine).: A general kernel machine \(g:\mathcal{X}\to\mathbb{R}\) with a kernel \(K(\bm{x},\bm{x}^{\prime})\) is \(g(\bm{x})=\sum_{i=1}^{n}a_{i}K(\bm{x}_{i},\bm{x})+h(\bm{x})\), where \(h:\mathcal{X}\to\mathbb{R}\) is a function of \(\bm{x}\). When \(h(\bm{x})\) is a constant, \(g(\bm{x})\) reduces to a kernel machine in Definition 1.

### Neural Tangent Kernel and Loss Path Kernel

Neural tangent kernel (NTK) has been introduced by Jacot et al. [28] to establish an equivalence between infinite-width NNs and kernel regression. After then, there is a growing line of work applying NTK theory to study properties of over-parameterized NNs, such as optimization convergence [21; 20] and generalization capability [2; 11]. The neural tangent kernel [28] associated with a NN \(f(\bm{w},\bm{x})\) at \(\bm{w}\) is defined as \(\hat{\Theta}(\bm{w};\bm{x},\bm{x}^{\prime})=\nabla_{\bm{w}}f(\bm{w},\bm{x}) \nabla_{\bm{w}}f(\bm{w},\bm{x}^{\prime})^{\top}\in\mathbb{R}^{k\times k}\). Under certain conditions, such as infinite width limit and NTK parameterization, the NTK converges to a deterministic limit kernel \(\Theta(\bm{x},\bm{x}^{\prime})\cdot\mathbf{I}_{k}\) that remains constant during training: \(\hat{\Theta}(\bm{w};\bm{x},\bm{x}^{\prime})\to\Theta(\bm{x},\bm{x}^{\prime}) \cdot\mathbf{I}_{k}\), where \(\Theta(\bm{x},\bm{x}^{\prime}):\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) is a scalar kernel and \(\mathbf{I}_{k}\) is a \(k\times k\) identity matrix. Next, we introduce the main concepts of interest in this paper: the _loss tangent kernel_ and the _loss path kernel_. They are central to characterizing the generalization behaviors of NNs trained by (stochastic) gradient flow.

**Definition 3** (Loss Tangent Kernel (LTK) \(\mathds{K}\)).: The loss tangent kernel associated with the loss function \(\ell(\bm{w},\bm{z})\) is defined as \(\bar{\mathds{K}}(\bm{w};\bm{z},\bm{z}^{\prime})=\langle\nabla_{\bm{w}}\ell(\bm {w},\bm{z}),\nabla_{\bm{w}}\ell(\bm{w},\bm{z}^{\prime})\rangle\in\mathbb{R}\).

The LTK \(\bar{\mathds{K}}\) has a natural connection with the NTK \(\hat{\Theta}\) by applying the chain rule:

\[\bar{\mathds{K}}(\bm{w};\bm{z},\bm{z}^{\prime})=\nabla_{f}\ell(\bm{w},\bm{z})^ {\top}\hat{\Theta}(\bm{w};\bm{x},\bm{x}^{\prime})\nabla_{f}\ell(\bm{w},\bm{z} ^{\prime}).\]

Next, we introduce the loss path kernel, which integrates the LTK along a given path of the parameters. Later, we will characterize this path via the gradient flow dynamics.

**Definition 4** (Loss Path Kernel (LPK) \(\mathds{K}_{T}\)).: Suppose the weights follow a continuous path \(\bm{w}(t):[0,T]\to\mathbb{R}^{p}\) in their domain with a starting point \(\bm{w}(0)=\bm{w}_{0}\), where \(T\) is a predetermined constant. This path is determined by the training set \(\mathcal{S}\) and the training time \(T\). We define the loss path kernel associated with the loss function \(\ell(\bm{w},\bm{z})\) along the path as \(\mathds{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\triangleq\int_{0}^{T}\bar{ \mathds{K}}(\bm{w}(t);\bm{z},\bm{z}^{\prime})\mathrm{d}t\).

In Appendix B, we show LTK is Riemann integrable so the integral in the above definition is well-defined. Intuitively, the LTK \(\bar{\mathds{K}}(\bm{w};\bm{z},\bm{z}^{\prime})\) measures the similarity between data points \(\bm{z}\) and \(\bm{z}^{\prime}\) by comparing their loss gradients when evaluated using a fixed neural network parameter \(\bm{w}\). The LPK \(\mathds{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\) measures the overall similarity during the entire training time.

### Rademacher Complexity

Rademacher complexity [52] measures the complexity of a hypothesis class. It takes into account the data distribution and is a central concept in statistical learning theory. Next, we recall its definition and a generalization upper bound via Rademacher complexity.

**Definition 5** (Empirical Rademacher complexity \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G})\)).: Let \(\mathcal{F}=\left\{f:\mathcal{X}\rightarrow\mathbb{R}^{k}\right\}\) be a hypothesis class. We denote \(\mathcal{G}\) as the set of loss functions associated with each function in \(\mathcal{F}\), defined as \(\mathcal{G}=\{g:(\bm{x},\bm{y})\rightarrow\ell(f(\bm{x}),\bm{y}),f\in\mathcal{ F}\}\). The empirical Rademacher complexity of \(\mathcal{G}\) with respect to a sample set \(\mathcal{S}\) is defined as: \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G})=\frac{1}{n}\operatorname{\mathbb{ E}}_{\bm{\sigma}}\left[\sup_{g\in\mathcal{G}}\sum_{i=1}^{n}\sigma_{i}g(\bm{z}_{i})\right]\), where \(\bm{\sigma}=(\sigma_{1},\ldots,\sigma_{n})\) and \(\sigma_{i}\) are independent uniform random variables taking values in \(\{+1,-1\}\).

**Theorem 1** (Theorem 3.3 in Mohri et al. [38]).: _Let \(\mathcal{G}\) be a family of functions mapping from \(\mathcal{Z}\) to \([0,1]\). Then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\) over the draw of an i.i.d. sample set \(\mathcal{S}=\{\bm{z}_{1},\ldots,\bm{z}_{n}\}\), the following holds for all \(g\in\mathcal{G}\): \(\operatorname{\mathbb{E}}_{\bm{z}}\left[g(\bm{z})\right]-\frac{1}{n}\sum_{i= 1}^{n}g(\bm{z}_{i})\leq 2\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G})+3\sqrt{ \frac{\log(2/\delta)}{2n}}\)._

## 4 Gradient Flow

In this section, we establish a new connection between the loss dynamics of gradient flow and a general kernel machine equipped with the LPK. Using this result, we introduce a new generalization bound by analyzing the complexity of the collection of kernel machines induced by all possible training sets. Our analysis applies to a wide range of neural network architectures, as long as they are continuously differentiable. Our numerical experiments validate the tightness of our bound and its strong correlation with the true generalization error.

### Loss Dynamics of Gradient Flow and Its Equivalence with General Kernel Machine

Consider the gradient flow dynamics (gradient descent with infinitesimal step size):

\[\frac{\text{d}\bm{w}(t)}{\text{d}t}=-\nabla_{\bm{w}}L_{S}(\bm{w}(t))=-\frac{1 }{n}\sum_{i=1}^{n}\nabla_{\bm{w}}\ell(\bm{w}(t),\bm{z}_{i}).\] (1)

The above ODE is well-defined for a wide variety of conditions, e.g. local Lipschitz-continuity of the gradient or semi-convexity of the loss function [49; 23]. Next, we establish its connection with the general kernel machine (KM) in the following theorem.

**Theorem 2** (Equivalence with general KM.).: _Suppose \(\bm{w}(T)=\bm{w}_{T}\) is a solution of (1) at time \(T\) with initialization \(\bm{w}(0)=\bm{w}_{0}\). Then for any \(\bm{z}\in\mathcal{Z}\),_

\[\ell(\bm{w}_{T},\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}_{T}(\bm{z},\bm{z} _{i};\mathcal{S})+\ell(\bm{w}_{0},\bm{z}),\]

_where \(\mathsf{K}_{T}\) is defined in Definition 4._

The above theorem demonstrates that the loss of the NN at a certain fixed time is a general kernel machine. Herein, \(\mathsf{K}_{T}\) is the LPK and we prove in Appendix C.1 that it is a valid kernel. Unlike previous NTK works that establish the equivalence between _infinite-width_ NNs and kernel machines, our equivalence is much more general and holds for any NN that is continuously differentiable. Based on this equivalence, we characterize the generalization of NNs from the perspective of kernels. Note that \(\mathsf{K}_{T}\) is a function of \(\mathcal{S}\) and this property enables us to establish a data-dependent generalization bound shortly.

### Generalization Bounds

We introduce the main result in this section: a generalization bound for NNs whose weights follow gradient flow in (1) at time \(T\). We derive this bound by analyzing the Rademacher complexity of the function class of kernel machines induced by different training sets with constrained RKHS norms. Recall that each training set yields a distinct LPK. We define the collection of all such LPKs by

\[\mathcal{K}_{T}\triangleq\{\mathsf{K}_{T}(\cdot,\cdot;\mathcal{S}^{\prime}): \mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n}),\frac{1}{n^{2}}\sum_{i, j}\mathsf{K}_{T}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{\prime};\mathcal{S}^{\prime}) \leq B^{2}\},\] (2)

where \(B>0\) is some constant, \(\mathcal{S}^{\prime}=\{\bm{z}_{1}^{\prime},\ldots,\bm{z}_{n}^{\prime}\}\), \(\mu^{\otimes n}\) is the joint distribution of \(n\) i.i.d. samples drawn from \(\mu\), and \(\mathsf{supp}(\mu^{\otimes n})\) is the support set of \(\mu^{\otimes n}\). Recall that \(\mathcal{S}=\{\bm{z}_{1},\ldots,\bm{z}_{n}\}\) is the trainingset. Note the set in (2) includes the case of \(\mathcal{S}^{\prime}=\mathcal{S}\) if \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i},\bm{z}_{j};\mathcal{S})\leq B^ {2}\). Then we introduce a class of general kernel machines, corresponding to all different kernels in \(\mathcal{K}_{T}\).

\[\mathcal{G}_{T}\triangleq\left\{g(\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K} (\bm{z},\bm{z}_{i}^{\prime};\mathcal{S}^{\prime})+\ell(\bm{w}_{0},\bm{z}): \mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}\right\}.\]

Note that \(g(\bm{z})\in\mathcal{G}_{T}\) corresponds to \(\ell(\bm{w}_{T},\bm{z})\) trained from one possible dataset \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\). Next, we compute the Rademacher complexity of \(\mathcal{G}_{T}\) and use it to obtain a generalization bound.

**Theorem 3**.: \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\min\{U_{1},U_{2}\}\)_. Here_

\[U_{1} =\frac{B}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z}; \mathcal{S}^{\prime}))+\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j})},\] \[U_{2} =\inf_{\epsilon>0}\left(\frac{\epsilon}{n}+\sqrt{\frac{2\ln \mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left\|\right\|_{1})}{n}} \right),\]

_where \(\mathcal{G}_{T}^{\mathcal{S}}=\{g(\mathbf{Z})=(g(\bm{z}_{1}),\ldots,g(\bm{z}_{ n})):g\in\mathcal{G}_{T}\}\), \(\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left\|\right\|_{1})\) is the covering number of \(\mathcal{G}_{T}^{\mathcal{S}}\) with the \(\ell_{1}\)-norm and_

\[\Delta(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right].\]

The term \(U_{1}\) is composed by two components \(\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\text{ Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime}))\) and \(\Delta(\bm{z}_{i},\bm{z}_{j})\). The first component, according to the definition of LPK, quantifies the maximum magnitude of the loss gradient in \(\mathcal{K}_{T}\) evaluated with the set \(\mathcal{S}\) throughout the training trajectory. The second component assesses the range of variation of LPK within the set \(\mathcal{K}_{T}\). The term \(U_{2}\) is obtained from analyzing the covering number of \(\mathcal{G}_{T}\). It shows that if the variation of the loss dynamics of gradient flow with different training data is small, then the complexity of \(\mathcal{G}_{T}\) will also be small. The norm constraint \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{\prime };\mathcal{S}^{\prime})\leq B^{2}\) balances a tradeoff between the tightness of the bound and the expressiveness of the set \(\mathcal{G}_{T}\) (the number of datasets covered). Combining these two bounds with Theorem 1, we obtain the following generalization bound.

**Corollary 1** (Generalization bound for NN).: _Fix \(B>0\). Let \(\hat{\mathcal{R}}_{\mathcal{S}}^{gf}(\mathcal{G}_{T})=\min\left(U_{1},U_{2}\right)\) where \(U_{1}\) and \(U_{2}\) are defined in Theorem 3. For any \(\delta\in(0,1)\), with probability at least \(1-\delta\) over the draw of an i.i.d. sample set \(\mathcal{S}=\{\bm{z}_{i}\}_{i=1}^{n}\), if \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i},\bm{z}_{j};\mathcal{S})\leq B ^{2}\), the following holds for \(\ell(\bm{w}_{T},\bm{z})\) that trained from \(\mathcal{S}\),_

\[L_{\mu}(A_{T}(\mathcal{S}))-L_{S}(A_{T}(\mathcal{S}))\leq 2\hat{\mathcal{R}}_{ \mathcal{S}}^{gf}(\mathcal{G}_{T})+3\sqrt{\frac{\log(2/\delta)}{2n}},\]

_where \(\bm{w}_{T}=A_{T}(\mathcal{S})\) is the output from the gradient flow (1) at time \(T\) by using \(\mathcal{S}\) as input._

Our result owns many compelling properties.

* First, our bound holds in a general setting as it does not hedge on a special NN architecture. In contrast, existing works [2, 11] only consider fully connected NNs and require NN to be ultra-wide.
* Our bound depends on the data distribution through the quantities in \(U_{1}\) and \(U_{2}\). This property not only significantly tightens our bound but can also help explain some empirical observations of NNs. For example, different from classical generalization theory, e.g. VC dimension, our complexity bounds depend on the labels directly, which helps explain the random label phenomenon [59] as shown in Figure 3 in Sec. 7.
* Our experiments in Sec. 7 (Figure 2) demonstrate the tightness of the generalization bound. Intuitively, our bound is tight because (1) instead of considering the entire hypothesis class, we focus on the subset of interest characterized by running gradient flow from a starting point \(\bm{w}_{0}\); (2) we get the bound from an equivalence between NNs and general kernel machines, whose generalization bounds are tighter. Finally, we compare our generalization bound with two existing NTK-based bounds in Table 1.

Stochastic Gradient Flow

In the previous section, we derived a generalization bound for NNs trained from full-batch gradient flow. Here we extend our analysis to stochastic gradient flow and derive a corresponding generalization bound. To start with, we recall the dynamics of stochastic gradient flow (SGD with infinitesimal step size). Let \(\mathcal{S}_{t}\subseteq\{1,\dots,n\}\) be the indices of batch data used in time interval \([t,t+1]\) and \(|\mathcal{S}_{t}|=m\) be the batch size. We establish a new connection between the loss dynamics of stochastic gradient flow and a general kernel machine. Then we investigate the complexity of the collection of such kernel machines that can be induced by various training sets.

**Theorem 4**.: _Suppose \(\bm{w}(T)=\bm{w}_{T}\) is a solution of stochastic gradient flow at time \(T\in\mathbb{N}\) with initialization \(\bm{w}(0)=\bm{w}_{0}\). Then for any \(\bm{z}\in\mathcal{Z}\),_

\[\ell(\bm{w}_{T},\bm{z})=\sum_{t=0}^{T-1}\sum_{i\in\mathcal{S}_{t}}-\frac{1}{m} \mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S})+\ell(\bm{w}_{0},\bm{z}),\]

_where \(\mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S})=\int_{t}^{t+1}\bar{\mathsf{ K}}(\bm{w}(t);\bm{z},\bm{z}_{i})dt\) with \(\bar{\mathsf{K}}\) defined in Definition 3._

The above theorem shows that the loss of the NN in stochastic gradient flow dynamics can be characterized by a sum of general kernel machines. In particular, when we use the full batch at each time interval (i.e., \(m=n\)), the above result recovers Theorem 2. To study its generalization behavior, we introduce the class of kernel machines induced by different training sets \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\) with constrained RKHS norms. Specifically, given \(B_{t}>0\) for \(t=0,\cdots,T-1\), we define

\[\mathcal{K}_{T}=\{(\mathsf{K}_{0,1}(\cdot,\cdot;\mathcal{S}^{\prime}),\cdots, \mathsf{K}_{T-1,T}(\cdot,\cdot;\mathcal{S}^{\prime})):\mathcal{S}^{\prime}\in \mathsf{supp}(\mu^{\otimes n}),\frac{1}{m^{2}}\sum_{i,j\in\mathcal{S}_{t}} \mathsf{K}_{t,t+1}(\bm{z}^{\prime}_{i},\bm{z}^{\prime}_{j};\mathcal{S}^{ \prime})\leq B_{t}^{2}\}.\]

Note this set includes the kernel induced by the training set \(\mathcal{S}\) if it satisfies the constraints. Then \(\ell(\bm{w}_{T},\bm{z})\) trained from all feasible \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\) form a function class

\[\mathcal{G}_{T}\triangleq\Big{\{}\sum_{t=0}^{T-1}\sum_{i\in\mathcal{S}_{t}}- \frac{1}{m}\mathsf{K}_{t,t+1}(\bm{z},\bm{z}^{\prime}_{i};\mathcal{S}^{\prime} )+\ell(\bm{w}_{0},\bm{z}):\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}\Big{\}}.\] (3)

Next, we upper bound the Rademacher complexity of the function class \(\mathcal{G}_{T}\). This bound can naturally translate into a generalization bound by equipping with Theorem 1.

**Theorem 5**.: _The Rademacher complexity of \(\mathcal{G}_{T}\) defined in (3) has an upper bound:_

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\sum_{t=0}^{T-1}\frac{B_ {t}}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K} _{T}}\text{Tr}(\mathsf{K}_{t,t+1}(\bm{Z},\bm{Z});\mathcal{S}^{\prime})+\sum_{i \neq j}\Delta_{t}(\bm{z}_{i},\bm{z}_{j})}.\]

_where \(\Delta_{t}(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i}, \bm{z}_{j};\mathcal{S}^{\prime})-\ \inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime} )\right]\)._

We assumed that mini-batches indices \(\mathcal{S}_{t}\) are chosen before training. However, our analysis can be extended to accommodate random mini-batch selections of any sampling strategy by enumerating all potential \(\mathcal{S}_{t}\) in \(\mathcal{K}_{T}\).

## 6 Case Study & Use Case

In the previous sections, we derived generalization bounds for NNs trained with (stochastic) gradient flow. These bounds may initially appear complex due to their dependence on the training process. Here we show that these bounds can be significantly simplified by applying them to infinite-width NNs (and stable algorithms in Appendix E.2, norm-constraint NNs in Appendix E.3). Moreover, we demonstrate that our generalization bounds maintain a high correlation with the true generalization error. As a result, we use them to guide the design of NAS, and our experimental results demonstrate that this approach has a favorable performance compared with state-of-the-art algorithms.

### Infinite-width NN

In this subsection, we consider a special case of infinite-width NNs trained by gradient flow and derive pre-computed generalization bounds. We focus on gradient flow to simplify the presentations but our results can be directly extended to stochastic gradient flow. For an infinite-width NN, under certain conditions, the neural tangent kernel keeps unchanged during training: \(\hat{\Theta}(\bm{w}_{t};\bm{x},\bm{x}^{\prime})\rightarrow\Theta(\bm{x},\bm{x}^{ \prime})\cdot\mathbf{I}_{k}\). Consider a \(\rho\)-Lipschitz loss function, i.e. \(\|\nabla_{f}\ell(\bm{w},\bm{z})\|\leq\rho\). The Rademacher complexity in Theorem 3 can be bounded by \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq U_{\infty}\), where

\[U_{\infty}=\frac{\rho B\sqrt{T}}{n}\sqrt{\sum_{i,j}|\Theta(\bm{x}_{i},\bm{x}_{ j})|}.\] (4)

In this infinite-width regime, our bound has no dependence on the initialization \(\bm{w}_{0}\) since the NTK converges to a deterministic limit and has no dependence on the parameters. That means the bound holds for all possible \(\bm{w}_{0}\) of the width NNs trained by gradient flow from initialization. Compared with the bound \(\hat{O}(L\cdot\sqrt{\frac{\mathbf{Y}^{\top}(\bm{\Theta})^{-1}\mathbf{Y}}{n}})\) in [11], \(U_{\infty}\) has several advantages: (1) it has no dependence on the number of layers \(L\); (2) it holds for NNs with multiple outputs.

### Correlation Analysis and NAS

As a practical application, we apply our generalization bounds to guide the design of NAS. We first introduce a quantity \(U_{\text{sgd}}\) simplified from the bound in Theorem 5, defined as

\[U_{\text{sgd}}=\sum_{t=0}^{T-1}\frac{1}{n}\sqrt{\frac{1}{m^{2}}\sum_{i,j\in \mathcal{S}_{t}}\mathcal{K}_{t,t+1}(\bm{z}_{i},\bm{z}_{j};\mathcal{S})}\sqrt{ \text{Tr}(\mathcal{K}_{t,t+1}(\mathbf{Z},\mathbf{Z});\mathcal{S})}.\]

\(U_{\text{sgd}}\) can be computed along with training a NN via SGD on a training set \(\mathcal{S}\). Combining it with the training loss, we define the following quantity as an estimate of the population loss:

\[\text{Gene}(\bm{w},\mathcal{S})=L_{\mathcal{S}}(\bm{w})+2U_{\text{sgd}}.\] (5)

We analyze the correlation between \(\text{Gene}(\bm{w},\mathcal{S})\) and the true generalization error by randomly sampling 100 NN architectures from NAS-Bench-201 [19]. For each, we compute both \(\text{Gene}(\bm{w},\mathcal{S})\) and the true generalization error. Since solving the gradient flow ODE is computationally infeasible for the large NNs in NAS-Bench-201, we apply a trapezoidal rule to approximate the integration in LPK \(K_{t,t+1}\). This approximation enables us to compute \(U_{\text{sgd}}\) efficiently. Figure 1 demonstrates the correlation between \(\text{Gene}(\bm{w},\mathcal{S})\) and the test error. The left figures plot the test error at epoch 1 or 2 against \(\text{Gene}(\bm{w},\mathcal{S})\) of the respective epochs, showing a strong positive correlation between them. The right figures plot the test error at convergence against \(\text{Gene}(\bm{w},\mathcal{S})\) at epoch 1 or 2, which also demonstrate a positive correlation. The outlier is caused by some architecture with large loss gradients. This experiment shows that \(\text{Gene}(\bm{w},\mathcal{S})\) at the initial training stage can predict the performance of NNs at convergence. Based on this observation, we use \(\text{Gene}(\bm{w},\mathcal{S})\) as a metric in NAS for selecting architectures at the initial training stage (see Table 2). This approach significantly reduces computational costs compared with training-based NAS algorithms [47; 34; 18; 33].

Figure 1: Correlation between \(\text{Gene}(\bm{w},\mathcal{S})\) and the test error on CIFAR-100 at epoch 1 and epoch 2. Kendall’s tau shows they have a strong positive correlation.

## 7 Numerical Experiments

We conduct comprehensive numerical experiments to demonstrate our generalization bounds. We observe that our complexity bounds are tight with respect to the generalization gap and can capture how noisy label influences the generalization behaviors of NNs. Moreover, we apply \(\text{Gene}(\bm{w},\mathcal{S})\) in (5) to NAS and demonstrate favorable performance compared with state-of-the-art algorithms.

**(I) Generalization bound in Corollary 1.** In Figure 2 (more detailed in Figure A.4), we use a logistic loss to train a two-layer NN with 100 hidden nodes for binary classification on MNIST 1 and 7 [31] by full-batch gradient flow and compute its generalization bound. Due to the computational cost of solving the gradient flow ODE and computing the kernel, we only train and calculate the bound on \(n=1000\) training samples. The bound would be tighter with more training samples. The NN is initialized using the NTK parameterization [28]. We use the Softplus activation function, defined as \(\text{Softplus}(x)=\frac{1}{\beta}\ln(1+e^{\beta x})\). This function is continuously differentiable and serves as a smooth approximation to the ReLU activation function. In our experiments, we set \(\beta=10\). To train the NN via gradient flow, we solve the gradient flow ODE given by (1) to decide the NN parameters. For the equivalent general KM, we compute the LTK using the NN parameters and integrate it to get the LPK \(\mathbb{K}_{T}\). These ODEs are computed with torchdiffeq [12]. To estimate the generalization bound, we train the NN on (20, 50, 100) independently sampled training sets \(\mathcal{S}^{\prime}\) to estimate the \(\mathcal{K}_{T}\) and \(\mathcal{G}_{T}\), and the supremum in the bound \(U_{1}\) is estimated by taking the maximum over the finite set \(\mathcal{K}_{T}\). For \(U_{2}\), we compute an upper bound of it by setting \(\epsilon\) as the largest \(\ell_{1}\) distance between any two \(g(\mathbf{Z})\in\mathcal{G}_{T}^{\mathcal{S}}\) and \(\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left\|\kern-1.0pt\right\| \kern-1.0pt\kern-1.0pt\right_{1})=1\) because in this case any \(g(\mathbf{Z})\in\mathcal{G}_{T}^{\mathcal{S}}\) will satisfy as an \(\epsilon\)-cover. We run each experiment five times and plot the mean and standard deviation. The numerical experiments demonstrate that our complexity bound is tight and can capture the generalization gap well. As the number of \(\mathcal{S}^{\prime}\) increases, our bound converges to the true supremum value. To estimate the true supremum value, we apply the extreme value theory in Appendix A.3 and show the gap between the finite maximum and supremum is small, validating using a finite maximum as an estimate for our bound.

We train the NN using GD with a finite learning rate \(\eta=10\) to compare with the NN trained by gradient flow. The training time \(t=\eta\times\) training steps. In Figure 2 (a) and Figure A.4 (a)(b), we observe that the loss of the NN trained by GD and that trained by gradient flow are consistently close throughout the entire training process. Consequently, while we established upper bounds for NNs trained by gradient flow, these results can serve as an (approximate) upper bound for NNs trained by GD with a finite learning rate.

Notably, for the NN in this experiment, the VC dimension [7] is \(55957.3\), the norm-based bound in [6] is \(140.7\) at \(T=1000\), and the NTK-based bound for an ultra-wide NN in [11] is 1.44, which are all vacuous (larger than 1), while our bound is tight (0.03 at \(T=1000\)). See a detailed comparison in Appendix A.2. We also conduct an experiment of three-layer NN (3072-100-10) trained on binary CIFAR-10 (cat and dog) [30] in Figure A.5, where there is a larger generalization gap.

Figure 2: **Experiment (I). (a) shows the dynamics of logistic loss for 5 randomly selected training samples for NN trained by gradient flow (NN GF), NN trained by GD (NN GD), and the equivalent general kernel machine (KM) in Theorem 2. The dynamics of NN GF and KM overlap and thus verify the equivalence in Theorem 2. The dynamics of NN GF and NN GD are consistently close throughout the training process. (b) shows NN GF’s training loss, test loss, test error, and upper bound for \(L_{\mu}(\bm{w}_{T})\) in Corollary 1. (c) shows that the complexity bound \(\hat{\mathcal{R}}_{g}^{gf}(\mathcal{G}_{T})\) in Corollary 1 captures the generalization gap \(L_{\mu}(\bm{w}_{T})-L_{\mathcal{S}}(\bm{w}_{T})\) well. It first increases and then converges after sufficient training time.**

**(II) Generalization bound with label noise.** The settings are similar to Experiment (I) except we corrupt the labels with different portions of noise and calculate the bound after training NN until \(T=20000\). We estimate the bound with 20 training sets \(\mathcal{S}^{\prime}\). The results in Figure 3 show that our bound has a strong correlation with the generalization gap and increases with the portion of label noise. Unlike classical generalization theory, e.g. VC dimension, our generalization bound can help explain the random label phenomenon [59].

**(III) Neural architecture search (NAS).** We apply \(\text{Gene}(\bm{w},\mathcal{S})\) in Eq. (5) to guide the design of NAS. The results are shown in Table 2. We use a simple random search (RS) with \(\text{Gene}(\bm{w},\mathcal{S})\), where 100 architectures are sampled from the search space for evaluation, and the architecture with smallest \(\text{Gene}(\bm{w},\mathcal{S})\) is selected. \(U_{\text{sgd}}\) is estimated with a batch of data of size 600. Build upon Sec. 6.2, we apply \(\text{Gene}(\bm{w},\mathcal{S})_{1}\) and \(\text{Gene}(\bm{w},\mathcal{S})_{2}\) (\(\text{Gene}(\bm{w},\mathcal{S})\) after training 1 and 2 epochs) to select NN architectures at the initial training stage in order to reduce computational costs. We compare our method with state-of-the-art training-free/minimum-training NAS algorithms [13; 39]. We run the experiments four times with different random seeds and report the mean and standard deviation. We reproduce the results in Chen et al. [13] using their released code and directly adopt the results reported in Mok et al. [39] as they did not release the code. The results show our approach of RS + \(\text{Gene}(\bm{w},\mathcal{S})\) can achieve favorable performance compared with state-of-the-art training-free/minimum-training NAS algorithms.

## 8 Conclusion and Future Work

In this paper, we establish a new connection between the loss dynamics of (stochastic) gradient flow and a general kernel machine. Building upon this result, we introduce generalization bounds for NNs trained from (stochastic) gradient flow. Our bounds hold for any continuously differentiable NN architectures (both finite-width and ultra-wide) and are generally tighter than existing bounds. Moreover, for infinite-width NNs, we obtain a pre-computed generalization bound for the whole training process. Finally, we apply our results to NAS and demonstrate favorable performance compared with state-of-the-art NAS algorithms.

There are several directions for future research. First, evaluating our generation bounds relies on the loss gradient, which may contain private and sensitive information. One potential fix would be accessing such information in a differentially private manner and it would be interesting to investigate how this "noisy" observation of gradient information influences our generalization bounds. Second, it is worth exploring how other optimization algorithms and different model architectures influence the generalization bounds. Finally, our bounds provide worst-case guarantees to the generalization of NNs and it would be interesting to extend our results to obtain expected bounds for further sharpening the results.

\begin{table}
\begin{tabular}{l|l l|l l} \hline \hline \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\  & Accuracy & Best & Accuracy & Best \\ \hline
**Baselines** & & & & \\ TENAS [13] & 93.08\(\pm\)0.15 & 93.25 & 70.37\(\pm\)2.40 & **73.16** \\ RS + LGA\({}_{3}\)[39] & 93.64 & & 69.77 & \\ \hline
**Ours** & & & & \\ RS + \(\text{Gene}(\bm{w},\mathcal{S})_{1}\) & 93.68\(\pm\)0.12 & 93.84 & 72.02\(\pm\)1.43 & 73.15 \\ RS + \(\text{Gene}(\bm{w},\mathcal{S})_{2}\) & **93.79\(\pm\)**0.18 & **94.02** & **72.76\(\pm\)**0.33 & 73.15 \\ \hline Optimal & 94.37 & & 73.51 & \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Experiment (III).** Comparison with state-of-the-art training-free/minimum-training NAS methods on NAS-Bench-201. Test accuracy with mean and deviation are reported. "Best" is the best accuracy over the four runs. “Optimal” indicates the best test accuracy achievable in NAS-Bench-201 search space. RS: randomly sample 100 architectures and select the one with the best metric value.

Figure 3: **Experiment (II).** Generalization bound with label noise at \(T=20000\).

Acknowledgement

We thank the anonymous reviewers for valuable suggestions to improve the paper. We also thank the San Diego Supercomputer Center and the MIT-IBM Watson AI Lab for computing resources. Y. Chen and T.-W. Weng are supported by National Science Foundation under Grant No. 2107189 and 2313105.

## References

* Arora et al. [2018] Arora, S., Ge, R., Neyshabur, B., and Zhang, Y. Stronger generalization bounds for deep nets via a compression approach. _arXiv preprint arXiv:1802.05296_, 2018.
* Arora et al. [2019] Arora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pp. 322-332. PMLR, 2019.
* Arora et al. [2019] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019.
* Awasthi et al. [2020] Awasthi, P., Frank, N., and Mohri, M. On the rademacher complexity of linear hypothesis sets. _arXiv preprint arXiv:2007.11045_, 2020.
* Bartlett and Mendelson [2002] Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3(Nov):463-482, 2002.
* Bartlett et al. [2017] Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* Bartlett et al. [2019] Bartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. _The Journal of Machine Learning Research_, 20(1):2285-2301, 2019.
* Bassily et al. [2020] Bassily, R., Feldman, V., Guzman, C., and Talwar, K. Stability of stochastic gradient descent on nonsmooth convex losses. _Advances in Neural Information Processing Systems_, 33:4381-4391, 2020.
* Belkin et al. [2019] Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* Boucheron et al. [2013] Boucheron, S., Lugosi, G., and Massart, P. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013.
* Cao and Gu [2019] Cao, Y. and Gu, Q. Generalization bounds of stochastic gradient descent for wide and deep neural networks. _Advances in neural information processing systems_, 32, 2019.
* Chen [2018] Chen, R. T. Q. torchdiffeq, 2018. URL https://github.com/rtqichen/torchdiffeq.
* Chen et al. [2021] Chen, W., Gong, X., and Wang, Z. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. _arXiv preprint arXiv:2102.11535_, 2021.
* Chen et al. [2021] Chen, Y., Huang, W., Nguyen, L., and Weng, T.-W. On the equivalence between neural network and support vector machine. _Advances in Neural Information Processing Systems_, 34, 2021.
* Chizat and Bach [2020] Chizat, L. and Bach, F. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_, pp. 1305-1338. PMLR, 2020.
* Cristianini et al. [2000] Cristianini, N., Shawe-Taylor, J., et al. _An introduction to support vector machines and other kernel-based learning methods_. Cambridge university press, 2000.
* Domingos [2020] Domingos, P. Every model learned by gradient descent is approximately a kernel machine. _arXiv preprint arXiv:2012.00152_, 2020.

* [18] Dong, X. and Yang, Y. Searching for a robust neural architecture in four gpu hours. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1761-1770, 2019.
* [19] Dong, X. and Yang, Y. Nas-bench-201: Extending the scope of reproducible neural architecture search. _arXiv preprint arXiv:2001.00326_, 2020.
* [20] Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pp. 1675-1685. PMLR, 2019.
* [21] Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* [22] Dziugaite, G. K. and Roy, D. M. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. _arXiv preprint arXiv:1703.11008_, 2017.
* [23] Grant, C. P. Theory of ordinary differential equations. _Brigham Young University_, 2014.
* [24] Gunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. Implicit bias of gradient descent on linear convolutional networks. _Advances in Neural Information Processing Systems_, 31, 2018.
* [25] Hardt, M., Recht, B., and Singer, Y. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pp. 1225-1234. PMLR, 2016.
* [26] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* [27] Huang, W., Liu, C., Chen, Y., Da Xu, R. Y., Zhang, M., and Weng, T.-W. Analyzing deep pac-bayesian learning with neural tangent kernel: Convergence, analytic generalization bound, and efficient hyperparameter selection. _Transactions on Machine Learning Research_, 2023.
* [28] Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [29] Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima. _arXiv preprint arXiv:1609.04836_, 2016.
* [30] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.
* [31] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [32] Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems_, 32, 2019.
* [33] Li, L. and Talwalkar, A. Random search and reproducibility for neural architecture search. In _Uncertainty in artificial intelligence_, pp. 367-377. PMLR, 2020.
* [34] Liu, H., Simonyan, K., and Yang, Y. Darts: Differentiable architecture search. _arXiv preprint arXiv:1806.09055_, 2018.
* [35] Maurer, A. A note on the pac bayesian theorem. _arXiv preprint cs/0411099_, 2004.
* [36] McDiarmid, C. et al. On the method of bounded differences. _Surveys in combinatorics_, 141(1):148-188, 1989.
* [37] Mellor, J., Turner, J., Storkey, A., and Crowley, E. J. Neural architecture search without training. In _International Conference on Machine Learning_, pp. 7588-7598. PMLR, 2021.

* Mohri et al. [2018] Mohri, M., Rostamizadeh, A., and Talwalkar, A. _Foundations of machine learning_. MIT press, 2018.
* Mok et al. [2022] Mok, J., Na, B., Kim, J.-H., Han, D., and Yoon, S. Demystifying the neural tangent kernel from a practical perspective: Can it be trusted for neural architecture search without training? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 11861-11870, 2022.
* Neyshabur et al. [2014] Neyshabur, B., Tomioka, R., and Srebro, N. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Neyshabur et al. [2015] Neyshabur, B., Tomioka, R., and Srebro, N. Norm-based capacity control in neural networks. In _Conference on Learning Theory_, pp. 1376-1401. PMLR, 2015.
* Neyshabur et al. [2017] Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. Exploring generalization in deep learning. _Advances in neural information processing systems_, 30, 2017.
* Neyshabur et al. [2017] Neyshabur, B., Bhojanapalli, S., and Srebro, N. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. _arXiv preprint arXiv:1707.09564_, 2017.
* Neyshabur et al. [2019] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. The role of over-parametrization in generalization of neural networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=BygfghAcYX.
* Novak et al. [2020] Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast and easy infinite neural networks in python. In _International Conference on Learning Representations_, 2020. URL https://github.com/google/neural-tangents.
* Paszke et al. [2019] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Pham et al. [2018] Pham, H., Guan, M., Zoph, B., Le, Q., and Dean, J. Efficient neural architecture search via parameters sharing. In _International conference on machine learning_, pp. 4095-4104. PMLR, 2018.
* Russo and Zou [2016] Russo, D. and Zou, J. Controlling bias in adaptive data analysis using information theory. In _Artificial Intelligence and Statistics_, pp. 1232-1240. PMLR, 2016.
* Santambrogio [2017] Santambrogio, F. {Euclidean, metric, and Wasserstein} gradient flows: an overview. _Bulletin of Mathematical Sciences_, 7:87-154, 2017.
* Savarese et al. [2019] Savarese, P., Evron, I., Soudry, D., and Srebro, N. How do infinite width bounded norm networks look in function space? In _Conference on Learning Theory_, pp. 2667-2690. PMLR, 2019.
* Scholkopf et al. [2002] Scholkopf, B., Smola, A. J., Bach, F., et al. _Learning with kernels: support vector machines, regularization, optimization, and beyond_. MIT press, 2002.
* Shalev-Shwartz and Ben-David [2014] Shalev-Shwartz, S. and Ben-David, S. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* Shawe-Taylor et al. [2004] Shawe-Taylor, J., Cristianini, N., et al. _Kernel methods for pattern analysis_. Cambridge university press, 2004.
* Soudry et al. [2018] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Valle-Perez and Louis [2020] Valle-Perez, G. and Louis, A. A. Generalization bounds for deep learning. _arXiv preprint arXiv:2012.04115_, 2020.
* Vapnik and Chervonenkis [1971] Vapnik, V. and Chervonenkis, A. Y. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and its Applications_, 16(2):264, 1971.

* [57] Weng, T.-W., Zhang, H., Chen, P.-Y., Yi, J., Su, D., Gao, Y., Hsieh, C.-J., and Daniel, L. Evaluating the robustness of neural networks: An extreme value theory approach. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=BkUH1MZOb.
* [58] Xu, A. and Raginsky, M. Information-theoretic analysis of generalization capability of learning algorithms. _Advances in Neural Information Processing Systems_, 30, 2017.
* [59] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.

## Appendix A Additional Experiments

Experiments are implemented with PyTorch [46] on 24G A5000 and 32G V100 GPUs.

### Computation Cost of Experiments

In Experiment (I), estimating the bound with 20 \(S^{\prime}\) (solving 20 gradient flow ODE) costs 500s and training NN costs 0.29s. The GPU memory required by estimating the bound is 2406MB and training NN requires 1044MB.

For Experiment (III), we report the averaged computational cost (GPU hours) of our approach for one architecture and the computational cost of training one NN architecture to convergence in

Figure A.4: **Experiment (I). (a) shows the dynamics of logistic loss for 5 randomly selected training samples for NN trained by gradient flow (NN GF), NN trained by gradient descent (NN GD), and the equivalent general kernel machine (KM) in Theorem 2. The dynamics of NN GF and KM overlap and verify the equivalence in Theorem 2. The dynamics of NN GF and NN GD are consistently close throughout the training process. (b) shows the differences between NN GF and KM are \(0\), which verifies our equivalence in Theorem 2. The differences between NN GD and KM are also small. (c) shows NN GF’s training loss, test loss, test error, and population loss bounds we estimated. Bound with \(U_{1}/U_{2}\) is the bound for \(L_{\mu}(w)\) by applying \(U_{1}/U_{2}\) in Corollary 1. (d) shows \(U_{1}\) and \(U_{2}\) in Theorem 3 first increase then converge after sufficient training time. The numerical experiments demonstrate that our complexity bound is tight and can capture the generalization gap well. As the number of \(\mathcal{S}^{\prime}\) increases, our bound converges to the true supremum value.**

\begin{table}
\begin{tabular}{l|l|l} \hline \hline GPU hours & CIFAR-10 & CIFAR-100 \\ \hline RS + Gene(\(\bm{w},\mathcal{S}\)) (Ours) & 0.036 & 0.037 \\ Training one NN architecture to convergence & 1.83 & 2.56 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Averaged computational cost (GPU hours) for one architecture in Experiment (III).

Table 3. Note our approach calculates \(\text{Gene}(\bm{w},\mathcal{S})\) only after training for 1 or 2 epochs, which saves computational cost a lot.

### Comparison with Existing Generalization Bounds

To make a stronger comparison with existing work, we have conducted two additional experiments below to show that our bound is much tighter than prior work on both finite-width NNs and infinite-width NNs.

**(1)** For the finite-width NN, we compare with previous uniform convergence bounds (VC dimension [7] and norm-based bounds [6]) as the NTK-based bounds [2, 11] are limited to infinite or ultra-wide NNs. Note our bounds are also uniform convergence bounds. We follow the same setting in our Experiment I, and calculate the bound for two-layer NN with 100 hidden nodes (finite-width) at training time T=1000 and sample size n=1000. Here \(L\) is the number of layers, \(p\) is the number of parameters, \(W^{i}\) is the weight matrix for the \(i\)-th layer, and \(m\) is the largest width of NN including the input dimension. Constants in big \(O\) are ignored in the calculation. The results in Table 4 show that our bound is much tighter than previous VC dimensions and norm-based bounds.

Finally, we remark that existing works have observed that VC dimension and norm-based bounds are mostly vacuous (see e.g., Figure 5 of [44] and Figure 4 in [1]) while our bound is non-vacuous as shown above and in Figure 2 and Figure A.5.

**(2)** For the infinite-width NN, we compare with the NTK-based bound in Cao & Gu [11] in a similar setting as Experiment I - two-layer NN and binary MNIST (1 vs. 7) with n=1000. Note that their bound only holds for an ultra-wide NN. We compute the infinite-width NTK using Neural Tangents [45] and calculate their bound accordingly. For our bound, we train a two-layer NN with 1000 hidden nodes (to approximate an ultra-wide NN) and compute our bound at \(T=1000\) (almost convergence). Constants in big \(O\) are ignored in the calculation. The results in Table 5 show that our bound is much tighter than previous NTK-based bounds.

Figure A.5: **Experiment (I) on CIFAR-10**. Generalization bound for three-layer NN (3072-100-100-1) trained on binary CIFAR-10 (cat and dog). The experiment demonstrates that our complexity bound is tight and can capture the generalization gap well.

[MISSING_PAGE_FAIL:17]

To compute \(U_{2}\), we compute an upper bound of it by setting \(\epsilon\) as the largest \(\ell_{1}\) distance between any two \(g(\mathbf{Z})\in\mathcal{G}_{T}^{\mathcal{S}}\) and \(\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left|\left|\left|\right| \right|_{1})=1\) because in this case any \(g(\mathbf{Z})\in\mathcal{G}_{T}^{\mathcal{S}}\) will satisfy as an \(\epsilon\) cover.

\[U_{2}=\inf_{\epsilon>0}\left(\frac{\epsilon}{n}+\sqrt{\frac{2\ln\mathcal{N}( \mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left|\left|\right|\right|_{1})}{n}} \right)\leq\sup_{g_{1},g_{2}\in\mathcal{G}_{T}}\frac{1}{n}\left\|g_{1}( \mathbf{Z})-g_{2}(\mathbf{Z})\right\|_{1}\]

Denote the right hand side as \(U_{2}^{*}\). Since each \(\mathsf{K}(z_{i},z_{j};\mathcal{S}^{\prime})\leq L_{\ell}T^{2}\), \(U_{2}^{*}\leq 2L_{\ell}T^{2}\).

Note \(\mathbf{Z}\) is fixed. Then for all \(g_{1},g_{2}\in\mathcal{G}_{T}\), \(\left\|g_{1}(\mathbf{Z})-g_{2}(\mathbf{Z})\right\|_{1}\) are i.i.d. random variables, because \(g\in\mathcal{G}_{T}\) are trained from i.i.d. \(\mathcal{S}^{\prime}\). Consider a finite set \(\mathcal{G}_{N_{g}}\subset\mathcal{G}_{T}\) with size \(N_{g}=\left|\mathcal{G}_{N_{g}}\right|\). As \(N_{g}\rightarrow\infty\),

\[\hat{U}_{2}\triangleq\max_{g_{1},g_{2}\in\mathcal{G}_{N_{g}}}\left\|g_{1}( \mathbf{Z})-g_{2}(\mathbf{Z})\right\|_{1}\overset{P}{\rightarrow}U_{2}^{*}, \quad N_{g}\rightarrow\infty.\]

Then we can apply extreme value theory to estimate \(U_{2}^{*}\). We generate \(N_{b}\) batch of \(\mathcal{G}_{N_{g}}\), compute their \(\hat{U}_{2}\) and store them in a set. Then with these \(\hat{U}_{2}\)'s, we perform a maximum likelihood estimation of reverse Weibull distribution parameters, and the location estimate \(a_{w}\) is used as an estimate of the \(U_{2}^{*}\). To validate that reverse Weibull distribution is a good fit for the empirical distribution of the \(\hat{U}_{2}\)'s, we conduct Kolmogorov-Smirnov goodness-of-fit test (a.k.a. K-S test) to calculate the K-S test statistics D and corresponding p-values. The null hypothesis is that \(\hat{U}_{2}\)'s follow a reverse Weibull distribution.

We follow the same setting as Experiment I and want to estimate \(U_{2}^{*}\) at \(T=1000\). Figure A.6 shows a result of estimating \(U_{2}^{*}\) with \(N_{b}=50\) and \(N_{g}=1000\). The estimated \(U_{2}^{*}=0.06\) is quite close to the finite maximum, validating using a finite maximum as an estimate for our bound.

For \(U_{1}\),

\[U_{1}=\frac{B}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{ \prime}))+\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j})}\]

where

\[\Delta(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right].\]

\(\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime}))\) and \(\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\) for \(i,j\in[n]\) are random variables that only depends on \(\mathcal{S}^{\prime}\). For different \(\mathcal{S}^{\prime},\mathcal{S}^{\prime\prime}\in\mathsf{supp}(\mu^{\otimes n})\), \(\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime}))\) and \(\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime\prime}))\) are i.i.d. random variables. Similarly for \(\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime}),i,j\in[n]\). We assume the finite maximum of each random variable follows a reverse Weibull distribution and estimate their supremum then sum them together to get

Figure A.6: Estimating \(U_{2}^{*}\) with \(N_{b}=50\) finite maximum over sets of size \(N_{g}=1000\). The red line is the fitted probability distribution function (PDF) of the reverse Weibull distribution. The small D-statistics of K-S test (ks) and large p-values (pval) show the hypothesized reverse Weibull distribution fits the empirical distribution of \(\hat{U}_{2}\) well. The estimated \(U_{2}^{*}=0.06\) is quite close to the finite maximum, validating using a finite maximum as an estimate for our bound.

\(U_{1}\). Figure A.7 shows estimating \(\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\text{Tr}( \mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime}))\) and \(\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{ K}(\bm{z}_{1},\bm{z}_{2};\mathcal{S}^{\prime})\). The estimated extreme values are close to the finite maximum, validating using a finite maximum as an estimate for our bound. Due to the computational cost of estimating \(n^{2}\) extreme values, we leave estimating \(U_{1}\) as future work.

## Appendix B Integrability of Loss Tangent Kernel

By the assumption that the loss is continuously differentiable, the loss gradient \(\nabla_{w}\ell(\bm{w}(t),\bm{z})\) is continuous w.r.t. \(\bm{w}(t)\). Together with \(\bm{w}(t)\) is continuous w.r.t. \(t\), \(\nabla_{w}\ell(\bm{w}(t),\bm{z})\) is continuous w.r.t. \(t\). After the inner product, the LTK \(\bar{\mathsf{K}}(\bm{w}(t);\bm{z},\bm{z}^{\prime})\) is still continuous w.r.t. \(t\). By the continuity of LTK on the compact set \([0,T]\), LTK is bounded and Riemann integrable on \([0,T]\). Therefore, the integral in LPK is well-defined.

For the full-batch gradient flow (1) we considered in this paper, \(\bm{w}(t)\) is differentiable therefore continuous w.r.t. \(t\) by the gradient flow equation. Hence, the integral in LPK is well-defined.

For stochastic gradient flow, the continuity of the path \(\bm{w}(t)\) can be argued as follows. For each time interval \([t,t+1]\), we assume that the same batch of data is used. Within this time interval, \(\bm{w}(t)\) is continuous w.r.t. \(t\). For the next time interval \([t+1,t+2]\), even if a different data batch is used, the gradient flow ODE initializes the \(\bm{w}(t+1)\) with the solution from the end point of the previous interval, \([t,t+1]\). This ensures the continuity of \(\bm{w}(t)\) across distinct time intervals. In short, the continuity of \(\bm{w}(t)\) within each time interval, combined with the initialization of the ODE, will ensure the continuity of \(\bm{w}(t)\) in the entire time interval \([0,T]\).

## Appendix C Complete Proofs for Gradient Flow

### Proof of Theorem 2

**Theorem 2**.: _Suppose \(\bm{w}(T)=\bm{w}_{T}\) is a solution of (1) at time \(T\) with initialization \(\bm{w}(0)=\bm{w}_{0}\). Then for any \(\bm{z}\in\mathcal{Z}\),_

\[\ell(\bm{w}_{T},\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}_{T}(\bm{z},\bm{z} _{i};\mathcal{S})+\ell(\bm{w}_{0},\bm{z}),\]

_where \(\mathsf{K}_{T}\) is defined in Definition 4._

Proof.: Consider the gradient flow:

\[\frac{d\bm{w}_{t}}{dt}=-\nabla_{\bm{w}}L_{S}(\bm{w}_{t})=-\frac{1}{n}\sum_{i =1}^{n}\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}_{i}).\] (6)

For any differentiable loss function \(\ell(\bm{w},\bm{z})\), by chain rule,

\[\frac{d\ell(\bm{w}_{t},\bm{z})}{dt}=\left\langle\nabla_{\bm{w}}\ell(\bm{w}_{t },\bm{z}),\frac{d\bm{w}_{t}}{dt}\right\rangle.\] (7)

Plug in the gradient flow expression of \(\frac{d\bm{w}_{t}}{dt}\) in (6) into (7). We have

\[\frac{d\ell(\bm{w}_{t},\bm{z})}{dt} =\left\langle\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}),-\nabla_{\bm{ w}}L_{S}(\bm{w}_{t})\right\rangle\] \[=\left\langle\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}),-\frac{1}{n} \sum_{i=1}^{n}\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}_{i})\right\rangle\] (by definition of \[L_{S}(\bm{w}_{t})\] ) \[=-\frac{1}{n}\sum_{i=1}^{n}\left\langle\nabla_{\bm{w}}\ell(\bm{w }_{t},\bm{z}),\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}_{i})\right\rangle\] (by the linearity of inner product) \[=-\frac{1}{n}\sum_{i=1}^{n}\bar{\mathsf{K}}(\bm{w}_{t};\bm{z}, \bm{z}_{i}).\] (by Definition 3 of the LTK)Integrate both sides from \(0\) to \(T\) over the path \(\bm{w}(t)\) taken by the parameters during the gradient flow,

\[\ell(\bm{w}_{T},\bm{z})-\ell(\bm{w}_{0},\bm{z}) =\int_{0}^{T}-\frac{1}{n}\sum_{i=1}^{n}\bar{\mathsf{K}}(\bm{w}_{t}; \bm{z},\bm{z}_{i})dt\] \[=-\frac{1}{n}\sum_{i=1}^{n}\int_{0}^{T}\bar{\mathsf{K}}(\bm{w}_{t };\bm{z},\bm{z}_{i})dt\] (By linearity of integration) \[=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}_{T}(\bm{z},\bm{z}_{i}; \mathcal{S}).\] (By Definition 4 of LPK)

Thus we have

\[\ell(\bm{w}_{T},\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}_{T}(\bm{z},\bm{z} _{i};\mathcal{S})+\ell(\bm{w}_{0},\bm{z}).\]

Below, we prove that \(\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S}):\mathcal{Z}\times\mathcal{ Z}\to\mathbb{R}\) is a valid kernel. We can show \(\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\) is a valid kernel by proving \(\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\) is continuous and the kernel matrix \(\mathsf{K}_{T}(\mathbf{Z},\mathbf{Z};\mathcal{S})\in\mathbb{R}^{n\times n}\) is positive semi-definite (PSD) for any finite subset of \(\mathcal{Z}\), \(\bm{z}_{1},\cdots,\bm{z}_{n}\in\mathcal{Z}\)[53]. By Definition 4 of the loss path kernel,

\[\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S}) =\int_{0}^{T}\bar{\mathsf{K}}(\bm{w}_{t};\bm{z},\bm{z}^{\prime})dt\] \[=\int_{0}^{T}\left\langle\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}), \nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}^{\prime})\right\rangle\ dt.\]

Since \(\ell(\bm{w}_{t},\bm{z})\) is continuously differentiable for \(\bm{w}_{t}\) and \(\bm{z}\), \(\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z})\) is continuous for \(\bm{z}\). After the inner product and integration, it is still a continuous function. Thus \(\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\) is a continuous function for \(\bm{z}\).

Denote \(\Phi_{t}(\bm{z})=\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z})\), then by Definition 4 of the loss path kernel,

\[\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S}) =\int_{0}^{T}\bar{\mathsf{K}}(\bm{w}_{t};\bm{z},\bm{z}^{\prime})dt\] \[=\int_{0}^{T}\left\langle\nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}), \nabla_{\bm{w}}\ell(\bm{w}_{t},\bm{z}^{\prime})\right\rangle\ dt\] \[=\int_{0}^{T}\left\langle\Phi_{t}(\bm{z}),\Phi_{t}(\bm{z}^{\prime })\right\rangle\ dt.\]

For \(\forall\bm{u}\in\mathbb{R}^{n}\) and \(\forall\ \{\bm{z}_{i}\}_{i=1}^{n}\subseteq\mathcal{Z}\),

\[\bm{u}^{T}\mathsf{K}_{T}(\mathbf{Z},\mathbf{Z};\mathcal{S})\bm{u} =\sum_{i=1}^{n}\sum_{j=1}^{n}u_{i}u_{j}\mathsf{K}_{T}(\bm{z}_{i}, \bm{z}_{j};\mathcal{S})\] \[=\sum_{i=1}^{n}\sum_{j=1}^{n}u_{i}u_{j}\int_{0}^{T}\left\langle \Phi_{t}(\bm{z}_{i}),\Phi_{t}(\bm{z}_{j})\right\rangle\ dt\] \[=\int_{0}^{T}\sum_{i=1}^{n}\sum_{j=1}^{n}u_{i}u_{j}\left\langle \Phi_{t}(\bm{z}_{i}),\Phi_{t}(\bm{z}_{j})\right\rangle\ dt\] \[=\int_{0}^{T}\left\langle\sum_{i=1}^{n}u_{i}\Phi_{t}(\bm{z}_{i}), \sum_{j=1}^{n}u_{j}\Phi_{t}(\bm{z}_{j})\right\rangle\ dt\] \[=\int_{0}^{T}\left\|\sum_{i}^{n}u_{i}\Phi_{t}(\bm{z}_{i})\right\|^ {2}\ dt\] \[\geq 0.\]

Thus the matrix \(\mathsf{K}_{T}(\mathbf{Z},\mathbf{Z};\mathcal{S})\) is PSD and \(\mathsf{K}_{T}(\bm{z},\bm{z}^{\prime};\mathcal{S})\) is therefore a valid kernel.

### Proof of Theorem 3

**Theorem 3**.: \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\min\{U_{1},U_{2}\}\)_. Here_

\[U_{1} =\frac{B}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime })\in\mathcal{K}_{T}}\mathsf{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^ {\prime}))+\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j})},\] \[U_{2} =\inf_{\epsilon>0}\left(\frac{\epsilon}{n}+\sqrt{\frac{2\ln \mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left|\right|\right|_{1})}{n }}\right),\]

_where \(\mathcal{G}_{T}^{\mathcal{S}}=\{g(\mathbf{Z})=(g(\bm{z}_{1}),\ldots,g(\bm{z}_ {n})):g\in\mathcal{G}_{T}\}\), \(\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left|\right|_{1})\) is the covering number of \(\mathcal{G}_{T}^{\mathcal{S}}\) with the \(\ell_{1}\)-norm and_

\[\Delta(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right].\]

#### c.2.1 Proof of \(U_{1}\)

Proof.: Recall the definition of \(\mathcal{G}_{T}\),

\[\mathcal{G}_{T}=\{g(\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}(\bm{z},\bm{z} _{i}^{\prime};\mathcal{S}^{\prime})+\ell(\bm{w}_{0},\bm{z}):\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}\}.\]

where

\[\mathcal{K}_{T}=\{\mathsf{K}_{T}(\cdot,\cdot;\mathcal{S}^{\prime}):\mathcal{S} ^{\prime}\in\mathsf{supp}(\mu^{\otimes n}),\frac{1}{n^{2}}\sum_{i,j}\mathsf{K }_{T}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{\prime};\mathcal{S}^{\prime})\leq B^{2}\}.\]

Suppose \(\mathsf{K}(\bm{z},\bm{z}^{\prime};\mathcal{S}^{\prime})=\langle\Phi_{\mathcal{ S}^{\prime}}(\bm{z}),\Phi_{\mathcal{S}^{\prime}}(\bm{z}^{\prime})\rangle\). Define

\[\mathcal{G}_{T}^{U}=\{g(\bm{z})=\langle\bm{\beta},\Phi_{\mathcal{S}^{\prime}} (\bm{z})\rangle+\ell(\bm{w}_{0},\bm{z}):\|\bm{\beta}\|\leq B,\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}\}.\]

We first show \(\mathcal{G}_{T}\subseteq\mathcal{G}_{T}^{U}\). For \(\forall g(\bm{z})\in\mathcal{G}_{T}\),

\[g(\bm{z}) =\sum_{i=1}^{n}-\frac{1}{n}\mathsf{K}(\bm{z},\bm{z}_{i}^{\prime}; \mathcal{S}^{\prime})+\ell(\bm{w}_{0},\bm{z})\] \[=\sum_{i=1}^{n}-\frac{1}{n}\left\langle\Phi_{\mathcal{S}^{\prime} }(\bm{z}),\Phi_{\mathcal{S}^{\prime}}(\bm{z}_{i}^{\prime})\right\rangle+\ell( \bm{w}_{0},\bm{z})\ \ \ \text{(by definition $\mathsf{K}(\bm{z},\bm{z}^{\prime};\mathcal{S}^{\prime})= \langle\Phi_{\mathcal{S}^{\prime}}(\bm{z}),\Phi_{\mathcal{S}^{\prime}}(\bm{z}^ {\prime})\rangle$)}\] \[=\left\langle\Phi_{\mathcal{S}^{\prime}}(\bm{z}),\sum_{i=1}^{n}- \frac{1}{n}\Phi_{\mathcal{S}^{\prime}}(\bm{z}_{i}^{\prime})\right\rangle+\ell (\bm{w}_{0},\bm{z})\] \[=\langle\Phi_{\mathcal{S}^{\prime}}(\bm{z}),\bm{\beta}_{\mathcal{S }^{\prime}}\rangle+\ell(\bm{w}_{0},\bm{z})\] \[=\langle\bm{\beta}_{\mathcal{S}^{\prime}},\Phi_{\mathcal{S}^{ \prime}}(\bm{z})\rangle+\ell(\bm{w}_{0},\bm{z})\] \[=\langle\bm{\beta}_{\mathcal{S}^{\prime}},\Phi_{\mathcal{S}^{ \prime}}(\bm{z})\rangle+\ell(\bm{w}_{0},\bm{z})\] \[=\langle\bm{\beta}_{\mathcal{S}^{\prime}},\Phi_{\mathcal{S}^{ \prime}}(\bm{z})\rangle+\ell(\bm{w}_{0},\bm{z})\]

We know by definition of \(\mathcal{G}_{T}\), \(\left\|\bm{\beta}_{\mathcal{S}^{\prime}}\right\|^{2}=\frac{1}{n^{2}}\sum_{i,j} \mathsf{K}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{\prime};\mathcal{S}^{\prime})\leq B ^{2}\). Thus \(g(\bm{z})\in\mathcal{G}_{T}^{U}\). Since \(\forall g(\bm{z})\in\mathcal{G}_{T}\), \(g(\bm{z})\in\mathcal{G}_{T}^{U}\), \(\mathcal{G}_{T}\subseteq\mathcal{G}_{T}^{U}\). But \(\mathcal{G}_{T}^{U}\) is strictly larger than \(\mathcal{G}_{T}\) because \(\bm{\beta}_{\mathcal{S}^{\prime}}\) is a fixed vector for a fixed \(\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\) while \(\bm{\beta}\) in \(\mathcal{G}_{T}^{U}\) is a vector of any direction.

Then by the property of Rademacher complexity,

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}) \leq\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{U})\] \[=\frac{1}{n}\,\mathbb{E}\left[\sup_{g\in\mathcal{G}_{T}^{U}}\sum_{ i=1}^{n}\sigma_{i}g(\bm{z}_{i})\right]\] \[=\frac{1}{n}\,\mathbb{E}\left[\sup_{\bm{\sigma}}\sum_{[\mathsf{K} (\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sum_{i=1}^{n}\sigma_{i} \left(\left\langle\bm{\beta},\Phi_{\mathcal{S}^{\prime}}(\bm{z}_{i})\right\rangle +\ell(\bm{w}_{0},\bm{z}_{i})\right)\right]\] \[=\frac{1}{n}\,\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sum_{i=1}^{n}\sigma_{i}\left\langle \bm{\beta},\Phi_{\mathcal{S}^{\prime}}(\bm{z}_{i})\right\rangle\right]+\frac{1 }{n}\,\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\sum_{i=1}^{n}\sigma_{i}\ell(\bm{w}_{0},\bm{z}_{i})\right]\] \[=\frac{1}{n}\,\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\left\langle\bm{\beta},\sum_{i=1}^{n} \sigma_{i}\Phi_{\mathcal{S}^{\prime}}(\bm{z}_{i})\right\rangle\right]\] \[=\frac{B}{n}\,\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\left\|\sum_{i=1}^{n}\sigma_{i}\Phi_{ \mathcal{S}^{\prime}}(\bm{z}_{i})\right\|\right]\] \[=\frac{B}{n}\,\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\left(\sum_{i=1}^{n}\sum_{j=1}^{n} \sigma_{i}\sigma_{j}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime}) \right)^{\frac{1}{2}}\right]\] \[=\frac{B}{n}\,\mathbb{E}\left[\left(\sup_{\mathsf{K}(\cdot,\cdot ;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i }\sigma_{j}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right)^{ \frac{1}{2}}\right]\] \[\leq\frac{B}{n}\left(\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot ;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sum_{i=1}^{n}\mathsf{K}(\bm{z}_{i}, \bm{z}_{i})+\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_ {T}}\sum_{i\neq j}\sigma_{i}\sigma_{j}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{ S}^{\prime})\right]\right)^{\frac{1}{2}}\] \[=\frac{B}{n}\left(\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{ S}^{\prime}))+\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\sum_{i\neq j}\sigma_{i}\sigma_{j}\mathsf{K}(\bm{z} _{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\right)^{\frac{1}{2}}.\]

The second term in the square root is \[\mathbb{E}\left[\sup_{\bm{\sigma}}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sum_{i\neq j}\sigma_{i}\sigma_{j} \mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\] \[\leq\mathbb{E}\left[\sum_{i\neq j}\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sigma_{i}\sigma_{j}\mathsf{K}(\bm{z} _{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\] \[=\sum_{i\neq j}\mathbb{E}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\sigma_{i}\sigma_{j}\mathsf{K}(\bm{z} _{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\] \[=\sum_{i\neq j}\left(\mathbb{P}\left(\sigma_{i}\sigma_{j}=+1 \right)\left[\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K} _{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]+\mathbb{P} \left(\sigma_{i}\sigma_{j}=-1\right)\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}-\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})\right]\right)\] \[=\sum_{i\neq j}\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\] \[=\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j})\]

where we define \(\Delta(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j}; \mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot,;\mathcal{S}^{\prime})\in \mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\). Thus in total,

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\hat{\mathcal{R}}_{ \mathcal{S}}(\mathcal{G}_{T}^{U})\leq\frac{B}{n}\left(\sup_{\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{S}^{\prime}))+\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j}) \right)^{\frac{1}{2}}\]

#### c.2.2 Proof of \(U_{2}\)

Proof.: To simplify the notation, denote \(\mathsf{K}_{\mathcal{S}^{\prime}}(\cdot,\cdot)=\mathsf{K}_{T}(\cdot,\cdot; \mathcal{S}^{\prime})\). Denote \(g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z})=\sum_{i=1}^{n}-\frac{1}{n}\mathsf{ K}_{\mathcal{S}^{\prime}}(\bm{z},\bm{z}_{i}^{\prime})+\ell(\bm{w}_{0},\bm{z})\) for a fixed \(\mathsf{K}_{\mathcal{S}^{\prime}}\), which is a singleton hypothesis class. Then \(\mathcal{G}_{T}\) is a union set of such function classes with different kernels,

\[\mathcal{G}_{T}=\left\{g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z})=\sum_{i=1 }^{n}-\frac{1}{n}\mathsf{K}_{\mathcal{S}^{\prime}}(\bm{z},\bm{z}_{i}^{\prime}) +\ell(\bm{w}_{0},\bm{z}):\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T} \right\}=\bigcup_{\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}}\left\{g_ {\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z})\right\}\]

Then we can rewrite \(\mathcal{G}_{T}^{\mathcal{S}}=\left\{g_{\mathsf{K}_{\mathcal{S}^{\prime}}}( \mathbf{Z})=(g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{1}),\ldots,g_{ \mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{n})):\mathsf{K}_{\mathcal{S}^{ \prime}}\in\mathcal{K}_{T}\right\}\). Suppose \(\mathcal{G}_{T}^{\mathcal{G}}\subseteq\mathcal{G}_{T}^{\mathcal{S}}\) is a minimal \(\epsilon\)-cover of \(\mathcal{G}_{T}^{\mathcal{S}}\) with \(\left\|\left\|{}_{1}\right\|\right\|_{1}\). Denote \(\widetilde{g}_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\mathbf{Z})=(\widetilde{g}_{ \mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{1}),\ldots,\widetilde{g}_{\mathsf{K} _{\mathcal{S}^{\prime}}}(\bm{z}_{n}))\in\mathcal{G}_{T}^{\mathcal{S}}\) as the closest element to \(g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\mathbf{Z})\in\mathcal{G}_{T}^{\mathcal{S}}\) and \(\widetilde{g}_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z})=\sum_{i=1}^{n}-\frac{1} {n}\widetilde{K}_{\mathcal{S}^{\prime}}(\bm{z},\bm{z}_{i}^{\prime})+\ell(\bm{ w}_{0},\bm{z})=g_{\widetilde{K}_{\mathcal{S}^{\prime}}}(\bm{z})\) with \(\widetilde{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}\). Denote the set of all \(\widetilde{K}_{\mathcal{S}^{\prime}}\) as \(\mathcal{K}_{T}^{\mathcal{S}}\). Since one \(\widetilde{g}_{\mathsf{K}_{\mathcal{S}^{\prime}}}\) corresponds to one \(\widetilde{K}_{\mathcal{S}^{\prime}}\), \(\left|\mathcal{K}_{T}^{\mathcal{S}}\right|=\left|\mathcal{G}_{T}^{\mathcal{S}} \right|=\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left\|\left\|{}_{1}\right\| \right)\).

Based on the above, we can write the Rademacher complexity of \(\mathcal{G}_{T}\) as

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})=\frac{1}{n}\,\mathbb{E}\left[ \sup_{\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}}\sum_{i=1}^{n}\sigma_{i}g_ {\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})\right].\]

For any \(\lambda>0\), by Jensen's inequality,

\[e^{\lambda\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})} \leq\mathbb{E}\left[e^{\lambda\left[\frac{1}{n}\sup_{\mathsf{K}_{ \mathcal{S}^{\prime}}\in\mathcal{K}_{T}}\sum_{i=1}^{n}\sigma_{i}g_{\mathsf{K}_{ \mathcal{S}^{\prime}}}(\bm{z}_{i})\right]}\right]\] \[=\mathbb{E}\left[\sup_{\mathsf{K}_{\mathcal{S}^{\prime}}\in \mathcal{K}_{T}}e^{\lambda\left[\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}g_{ \mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})\right]}\right].\] (8)Utilizing the definition of \(\epsilon\)-covering, the quantity on the exponent in Eq. (8) is

\[\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}g_{\mathsf{K}_{g^{\prime}}}( \boldsymbol{z}_{i})\] \[=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\left(g_{\mathsf{K}_{g^{\prime }}}(\boldsymbol{z}_{i})-\widetilde{g}_{\mathsf{K}_{g^{\prime}}}(\boldsymbol{z} _{i})+\widetilde{g}_{\mathsf{K}_{g^{\prime}}}(\boldsymbol{z}_{i})\right)\] \[=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\widetilde{g}_{\mathsf{K}_{g^ {\prime}}}(\boldsymbol{z}_{i})+\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\left(g_{ \mathsf{K}_{g^{\prime}}}(\boldsymbol{z}_{i})-\widetilde{g}_{\mathsf{K}_{g^{ \prime}}}(\boldsymbol{z}_{i})\right)\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\widetilde{g}_{\mathsf{K} _{g^{\prime}}}(\boldsymbol{z}_{i})+\frac{1}{n}\sum_{i=1}^{n}\left|g_{\mathsf{ K}_{g^{\prime}}}(\boldsymbol{z}_{i})-\widetilde{g}_{\mathsf{K}_{g^{\prime}}}( \boldsymbol{z}_{i})\right|\] \[=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\widetilde{g}_{\mathsf{K}_{g ^{\prime}}}(\boldsymbol{z}_{i})+\frac{1}{n}\left\|g_{\mathsf{K}_{g^{\prime}}}( \boldsymbol{Z})-\widetilde{g}_{\mathsf{K}_{g^{\prime}}}(\boldsymbol{Z}) \right\|_{1}\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\widetilde{g}_{\mathsf{K}_ {g^{\prime}}}(\boldsymbol{z}_{i})+\frac{\epsilon}{n}.\] (by the definition of the \[\epsilon\]-covering)

Substitute this inequality into Eq. (8). We have

\[e^{\lambda\widetilde{\mathcal{R}}_{S}(\mathcal{G}_{T})} \leq\mathbb{E}\left[\sup_{\mathsf{K}_{g^{\prime}}\in\mathcal{K}_ {T}}e^{\lambda\left[\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}g_{\widetilde{W}_{ \mathsf{K}_{g^{\prime}}}(\boldsymbol{z}_{i})+\frac{\epsilon}{n}}\right]}\right]\] \[=e^{\frac{\lambda\epsilon}{n}}\mathbb{E}\left[\sup_{\mathsf{K}_{g ^{\prime}}\in\mathcal{K}_{T}}e^{\lambda\left[\frac{1}{n}\sum_{i=1}^{n}\sigma_{ i}g_{\widetilde{W}_{\mathsf{K}_{g^{\prime}}}(\boldsymbol{z}_{i})}\right]}\right] (\widetilde{g}_{\mathsf{K}_{g^{\prime}}}(\boldsymbol{z}_{i})=g_{ \widetilde{K}_{g^{\prime}}}(\boldsymbol{z}_{i}))\] \[\stackrel{{(i)}}{{=}}e^{\frac{\lambda\epsilon}{n}} \mathbb{E}\left[\max_{\widetilde{K}_{g^{\prime}}\in\mathcal{K}_{T}}e^{\lambda \left[\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}g_{\widetilde{W}_{\mathsf{K}_{g^{ \prime}}}(\boldsymbol{z}_{i})}\right]}\right]\] \[\leq e^{\frac{\lambda\epsilon}{n}}\sum_{\widetilde{K}_{g^{\prime }}\in\mathcal{K}_{T}^{\epsilon}}\mathbb{E}\left[e^{\lambda\left[\frac{1}{n} \sum_{i=1}^{n}\sigma_{i}g_{\widetilde{W}_{\mathsf{K}_{g^{\prime}}}( \boldsymbol{z}_{i})}\right]}\right]\] (9)

where \((i)\) is because we only use the \(g_{\widetilde{K}_{g^{\prime}}}\) instead of \(g_{\mathsf{K}_{g^{\prime}}}\), which corresponds to \(\widetilde{K}_{\mathsf{S}^{\prime}}\in\mathcal{K}_{T}^{\epsilon}\) instead of \(\mathsf{K}_{\mathsf{S}^{\prime}}\in\mathcal{K}_{T}\), so it is equivalent to take the maximum over \(\widetilde{K}_{\mathsf{S}^{\prime}}\in\mathcal{K}_{T}^{\epsilon}\). Denote \(\xi(\sigma_{1},\dots,\sigma_{n})=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}g_{ \widetilde{K}_{g^{\prime}}}(\boldsymbol{z}_{i})\). Note \(g_{\widetilde{K}_{g^{\prime}}}(\boldsymbol{z}_{i})\in[0,1]\) for \(i\in[n]\). Then for all \(i\in[n]\),

\[\sup_{\boldsymbol{\sigma}}\left|\xi(\sigma_{1},\cdots,\sigma_{i},\cdots,\sigma_{n})-\xi(\sigma_{1},\cdots,-\sigma_{i},\cdots,\sigma_{n})\right|\] \[\leq\sup_{\boldsymbol{\sigma}}\left|\frac{2}{n}\sigma_{i}g_{ \widetilde{K}_{g^{\prime}}}(\boldsymbol{z}_{i})\right|\] \[\leq\frac{2}{n}.\]

Thus \(\xi(\sigma_{1},\cdots,\sigma_{n})\) satisfies the bounded difference property. Let \(c_{i}=\frac{2}{n}\). By [36, 10], \(\xi\) is a sub-Gaussian variable and satisfies

\[\mathbb{E}\left[e^{\lambda\xi}\right] \leq e^{\frac{\lambda^{2}}{8}\sum_{i=1}^{n}c_{i}^{2}}e^{\lambda \,\mathbb{E}[\xi]}\] \[=e^{\frac{\lambda^{2}}{2n}}e^{\lambda\,\mathbb{E}[\xi]}.\]

Since \(g_{\widetilde{K}_{g^{\prime}}}\) is a singleton function, \(\mathbb{E}\left[\xi\right]=\frac{1}{n}\,\mathbb{E}_{\boldsymbol{\sigma}} \left[\sum_{i=1}^{n}\sigma_{i}g_{\widetilde{K}_{g^{\prime}}}(\boldsymbol{z}_{ i})\right]=0\). Thus

\[\mathbb{E}\left[e^{\lambda\xi}\right]\leq e^{\frac{\lambda^{2}}{2n}}.\]

Take this into Eq. (9), in total, we have

\[e^{\lambda\widetilde{\mathcal{R}}_{S}(\mathcal{G}_{T})}\leq e^{\frac{\lambda \epsilon}{n}}\sum_{\widetilde{K}_{g^{\prime}}\in\mathcal{K}_{T}^{\epsilon}}e^{ \frac{\lambda^{2}}{2n}}=e^{\frac{\lambda\epsilon}{n}+\frac{\lambda^{2}}{2n}} \left|\mathcal{K}_{T}^{\epsilon}\right|.\]Take the logarithm on both sides of the equation,

\[\lambda\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\frac{\lambda\epsilon}{n }+\frac{\lambda^{2}}{2n}+\ln\left|\mathcal{K}_{T}^{\epsilon}\right|.\]

Divide \(\lambda\) on both sides,

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\frac{\epsilon}{n}+\frac{ \lambda}{2n}+\frac{1}{\lambda}\ln\left|\mathcal{K}_{T}^{\epsilon}\right|\]

By taking \(\lambda=\sqrt{2n\ln\left|\mathcal{K}_{T}^{\epsilon}\right|}\), we get

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\frac{\epsilon}{n}+\sqrt{ \frac{2\ln\left|\mathcal{K}_{T}^{\epsilon}\right|}{n}}.\]

Take the infimum over \(\epsilon>0\) and note \(\left|\mathcal{K}_{T}^{\epsilon}\right|=\left|\mathcal{G}_{T}^{\epsilon} \right|=\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}},\epsilon,\left|\kern-1.0pt \left|\kern-1.0pt\left|\kern-1.0pt\left|\right|_{1}\right)\right|\). We get

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\inf_{\epsilon>0}\left( \frac{\epsilon}{n}+\sqrt{\frac{2\ln\mathcal{N}(\mathcal{G}_{T}^{\mathcal{S}}, \epsilon,\left|\kern-1.0pt\left|\kern-1.0pt\left|\right|_{1}\right)}{n}} \right).\]

### A lower bound of \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{U})\)

Here we give a lower bound of \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{U})\). Similar lower bounds for a linear model were proved in [4, 6] without the supremum. Our lower bound match the trace term in the upper bound \(U_{1}\), which shows the upper bound \(U_{1}\) is relatively tight.

**Theorem 7**.: \[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{U})\geq\frac{B}{\sqrt{2n}} \sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}}\sqrt{\text{Tr}(\mathsf{K}_{ S^{\prime}}(\mathbf{Z},\mathbf{Z}))}.\]

Proof.: Recall

\[\mathcal{G}_{T}^{U}=\{g(\bm{z})=\langle\bm{\beta},\Phi_{\mathcal{S}^{\prime}} (\bm{z})\rangle+\ell(\bm{w}_{0},\bm{z}):\|\bm{\beta}\|\leq B,\mathsf{K}_{S^{ \prime}}\in\mathcal{K}_{T}\}.\]The Rademacher complexity of \(\mathcal{G}_{T}^{U}\) is

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{U}) =\frac{1}{n}\operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}}\left[ \sup_{g\in\mathcal{G}_{T}^{U}}\sum_{i=1}^{n}\sigma_{i}g(\boldsymbol{z}_{i})\right]\] \[=\frac{1}{n}\operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}} \left[\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}}\sup_{\|\boldsymbol{ \beta}\|\leq B}\sum_{i=1}^{n}\sigma_{i}\left(\left\langle\boldsymbol{\beta}, \Phi_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\right\rangle+\ell(\boldsymbol{ w}_{0},\boldsymbol{z}_{i})\right)\right]\] \[=\frac{1}{n}\operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}} \left[\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}}\sup_{\|\boldsymbol{ \beta}\|\leq B}\left\langle\boldsymbol{\beta},\sum_{i=1}^{n}\sigma_{i}\Phi_{ \mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\right\rangle\right]+\operatorname*{ \mathbb{E}}_{\boldsymbol{\sigma}}\left[\sum_{i=1}^{n}\sigma_{i}\ell( \boldsymbol{w}_{0},\boldsymbol{z}_{i})\right]\] \[=\frac{1}{n}\operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}} \left[\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}}\sup_{\|\boldsymbol{ \beta}\|\leq B}\left\langle\boldsymbol{\beta},\sum_{i=1}^{n}\sigma_{i}\Phi_{ \mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\right\rangle\right]\] \[=\frac{B}{n}\operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}} \left[\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}}\left\|\sum_{i=1}^{n} \sigma_{i}\Phi_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\right\|\right]\] (dual norm property) \[\geq\frac{B}{n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}} \operatorname*{\mathbb{E}}_{\boldsymbol{\sigma}}\left[\left\|\sum_{i=1}^{n} \sigma_{i}\Phi_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\right\|\right]\] \[\geq\frac{B}{n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}} \left\|\operatorname*{\mathbb{E}}\left[\sum_{i=1}^{n}\sigma_{i}\Phi_{\mathcal{ S}^{\prime}}(\boldsymbol{z}_{i})\right]\right\|\] (norm sub-additivity) \[=\frac{B}{n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}} \left(\sum_{j\in\mathbb{N}_{+}}\left(\operatorname*{\mathbb{E}}_{\boldsymbol{ \sigma}}\left[\left|\sum_{i=1}^{n}\sigma_{i}[\Phi_{\mathcal{S}^{\prime}}( \boldsymbol{z}_{i})]_{j}\right|\right]\right)^{2}\right)^{\frac{1}{2}}\] (by the definition of 2-norm) \[\geq\frac{B}{n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}} \left(\sum_{j\in\mathbb{N}_{+}}\left(\frac{1}{\sqrt{2}}\left|\sum_{i=1}^{n}[ \Phi_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i})]_{j}^{2}\right|^{\frac{1}{2}} \right)^{2}\right)^{\frac{1}{2}}\] (Khintchine-Kahane inequality) \[=\frac{B}{\sqrt{2}n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T}} \left(\sum_{j\in\mathbb{N}_{+}}\left|\sum_{i=1}^{n}[\Phi_{\mathcal{S}^{\prime}} (\boldsymbol{z}_{i})]_{j}^{2}\right|\right)^{\frac{1}{2}}\] \[=\frac{B}{\sqrt{2}n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T} }\left(\sum_{i=1}^{n}\sum_{j\in\mathbb{N}_{+}}[\Phi_{\mathcal{S}^{\prime}}( \boldsymbol{z}_{i})]_{j}^{2}\right)^{\frac{1}{2}}\] (rearrange the summations) \[=\frac{B}{\sqrt{2}n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T} }\left(\sum_{i=1}^{n}\|\Phi_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i})\|^{2} \right)^{\frac{1}{2}}\] \[=\frac{B}{\sqrt{2}n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T} }\left(\sum_{i=1}^{n}\mathsf{K}_{\mathcal{S}^{\prime}}(\boldsymbol{z}_{i}, \boldsymbol{z}_{i})\right)^{\frac{1}{2}}\] \[=\frac{B}{\sqrt{2}n}\sup_{\mathsf{K}_{S^{\prime}}\in\mathcal{K}_{T }}\sqrt{\text{Tr}(\mathsf{K}_{\mathcal{S}^{\prime}}(\boldsymbol{Z},\boldsymbol{Z }))}.\]

### Proof of Corollary 1

**Corollary 1**.: _Fix \(B>0\). Let \(\hat{\mathcal{R}}_{\mathcal{S}}^{gf}(\mathcal{G}_{T})=\min\left(U_{1},U_{2}\right)\) where \(U_{1}\) and \(U_{2}\) are defined in Theorem 3. For any \(\delta\in(0,1)\), with probability at least \(1-\delta\) over the draw of an i.i.d. sample set \(\mathcal{S}=\left\{\boldsymbol{z}_{i}\right\}_{i=1}^{n}\), if \(\frac{1}{n^{\delta}}\sum_{i,j}\mathsf{K}_{T}(\boldsymbol{z}_{i},\boldsymbol{z}_ {j};\mathcal{S})\leq B^{2}\), the following holds for \(\ell(\boldsymbol{w}_{T},\boldsymbol{z})\) that trained from \(\mathcal{S}\),_

\[L_{\mu}(A_{T}(\mathcal{S}))-L_{S}(A_{T}(\mathcal{S}))\leq 2\hat{\mathcal{R}}_{ \mathcal{S}}^{gf}(\mathcal{G}_{T})+3\sqrt{\frac{\log(2/\delta)}{2n}},\]_where \(\bm{w}_{T}=A_{T}(\mathcal{S})\) is the output from the gradient flow (1) at time \(T\) by using \(\mathcal{S}\) as input._

Proof.: Apply Theorem 1 to \(\mathcal{G}_{T}\) and \(\mathcal{S}\), for all \(g\in\mathcal{G}_{T}\)

\[\mathop{\mathbb{E}}_{z}\left[g(\bm{z})\right]-\frac{1}{n}\sum_{i=1}^{n}g(\bm{z} _{i})\leq 2\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})+3\sqrt{\frac{\log (2/\delta)}{2n}}.\]

Since \(g(\bm{z})\in\mathcal{G}_{T}\) corresponds to \(\ell(A_{T}(\mathcal{S}^{\prime}),\bm{z})\) for all \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\) that \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{ \prime};\mathcal{S}^{\prime})\leq B^{2}\), it holds for all these feasible \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\) that

\[\mathop{\mathbb{E}}_{z}\left[\ell(A_{T}(\mathcal{S}^{\prime}), \bm{z})\right]-\frac{1}{n}\sum_{i=1}^{n}\ell(A_{T}(\mathcal{S}^{\prime}),\bm{ z}_{i})\] \[=L_{\mu}(A_{T}(\mathcal{S}^{\prime}))-L_{S}(A_{T}(\mathcal{S}^{ \prime}))\] (by definition of

\[L_{S}(\bm{w})\]

 and

\[L_{\mu}(\bm{w})\]

) \[\leq 2\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})+3\sqrt{\frac{ \log(2/\delta)}{2n}}.\]

From Theorem 3, we have \(\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\min\left(U_{1},U_{2}\right)\). Define \(\hat{\mathcal{R}}_{\mathcal{S}}^{gf}(\mathcal{G}_{T})=\min\left(U_{1},U_{2}\right)\) and take into above inequality,

\[L_{\mu}(A_{T}(\mathcal{S}^{\prime}))-L_{S}(A_{T}(\mathcal{S}^{ \prime})) \leq 2\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})+3\sqrt{ \frac{\log(2/\delta)}{2n}}\] \[\leq 2\hat{\mathcal{R}}_{\mathcal{S}}^{gf}(\mathcal{G}_{T})+3 \sqrt{\frac{\log(2/\delta)}{2n}}.\]

Since above holds for all \(\mathcal{S}^{\prime}\in\mathsf{supp}(\mu^{\otimes n})\) that \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{ \prime};\mathcal{S}^{\prime})\leq B^{2}\), and \(\mathcal{S}\) is also in \(\mathsf{supp}(\mu^{\otimes n})\), then if \(\frac{1}{n^{2}}\sum_{i,j}\mathsf{K}_{T}(\bm{z}_{i},\bm{z}_{j};\mathcal{S})\leq B ^{2}\), we have

\[L_{\mu}(A_{T}(\mathcal{S}))-L_{S}(A_{T}(\mathcal{S}))\leq 2\hat{\mathcal{R}}_{ \mathcal{S}}^{gf}(\mathcal{G}_{T})+3\sqrt{\frac{\log(2/\delta)}{2n}}.\]

## Appendix D Complete Proofs for Stochastic Gradient Flow

### Proof of Theorem 4

**Theorem 4**.: _Suppose \(\bm{w}(T)=\bm{w}_{T}\) is a solution of stochastic gradient flow at time \(T\in\mathbb{N}\) with initialization \(\bm{w}(0)=\bm{w}_{0}\). Then for any \(\bm{z}\in\mathcal{Z}\),_

\[\ell(\bm{w}_{T},\bm{z})=\sum_{t=0}^{T-1}\sum_{i\in\mathcal{S}_{t}}-\frac{1}{m} \mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S})+\ell(\bm{w}_{0},\bm{z}),\]

_where \(\mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S})=\int_{t}^{t+1}\bar{\mathsf{ K}}(\bm{w}(t);\bm{z},\bm{z}_{i})dt\) with \(\bar{\mathsf{K}}\) defined in Definition 3._

Proof.: For each time interval \([t,t+1]\) and data batch \(\mathcal{S}_{t}\), Stochastic gradient flow can be treated as full-batch gradient flow. Applying Theorem 2 for each \([t,t+1]\) and data batch \(\mathcal{S}_{t}\), we have

\[\ell(\bm{w}_{t+1},\bm{z})=\sum_{i\in\mathcal{S}_{t}}-\frac{1}{m}\mathsf{K}_{t, t+1}(\bm{z},\bm{z}_{i};\mathcal{S})+\ell(\bm{w}_{t},\bm{z}),\] (10)

where

\[\mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S})=\int_{t}^{t+1}\bar{\mathsf{K}} (\bm{w}_{t};\bm{z},\bm{z}_{i})dt.\]For time \(T\in\mathbb{N}\),

\[\ell(\bm{w}_{T},\bm{z})-\ell(\bm{w}_{0},\bm{z}) =\sum_{t=0}^{T-1}\ell(\bm{w}_{t+1},\bm{z})-\ell(\bm{w}_{t},\bm{z})\] \[=\sum_{t=0}^{T-1}\sum_{i\in\mathcal{S}_{t}}-\frac{1}{m}\mathsf{K}_ {t,t+1}(\bm{z},\bm{z}_{i};\mathcal{S}).\] (takes in Eq. ( 10 ))

### Proof of Theorem 5

**Theorem 5**.: _The Rademacher complexity of \(\mathcal{G}_{T}\) defined in (3) has an upper bound:_

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T})\leq\sum_{t=0}^{T-1}\frac{B_{t }}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T }}\text{\rm Tr}(\mathsf{K}_{t,t+1}(\bm{Z},\bm{Z});\mathcal{S}^{\prime})+\sum_{ i\neq j}\Delta_{t}(\bm{z}_{i},\bm{z}_{j})}.\]

_where \(\Delta_{t}(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i}, \bm{z}_{j};\mathcal{S}^{\prime})-\right.\left.\inf_{\mathsf{K}(\cdot,\cdot; \mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i},\bm{z}_ {j};\mathcal{S}^{\prime})\right]\)._

Proof.: Recall,

\[\mathcal{G}_{T}=\{g(\bm{z})=\sum_{t=0}^{T-1}\sum_{i\in\mathcal{S}_{t}}-\frac{ 1}{m}\mathsf{K}_{t,t+1}(\bm{z},\bm{z}_{i}^{\prime};\mathcal{S}^{\prime})+\ell (\bm{w}_{0},\bm{z}):\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{ K}_{T}\}.\]

where

\[\mathcal{K}_{T}=\{(\mathsf{K}_{0,1}(\cdot,\cdot;\mathcal{S}^{\prime}),\cdots, \mathsf{K}_{T-1,T}(\cdot,\cdot;\mathcal{S}^{\prime})):\mathcal{S}^{\prime}\in \mathsf{supp}(\mu^{\otimes n}),\frac{1}{m^{2}}\sum_{i,j\in\mathcal{S}_{t}} \mathsf{K}_{t,t+1}(\bm{z}_{i}^{\prime},\bm{z}_{j}^{\prime};\mathcal{S}^{ \prime})\leq B_{t}^{2}\}.\]

For \(t=0,1,\cdots,T-1\), let

\[\mathcal{G}_{t}=\{g(\bm{z})=\sum_{i\in\mathcal{S}_{t}}-\frac{1}{m}\mathsf{K}_ {t,t+1}(\bm{z},\bm{z}_{i}^{\prime};\mathcal{S}^{\prime}):\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}\},\]

Then we have

\[\mathcal{G}_{T}\subseteq\mathcal{G}_{0}\oplus\mathcal{G}_{1}\oplus\cdots \oplus\mathcal{G}_{T-1}\oplus\left\{\ell(\bm{w}_{0},\bm{z})\right\}.\]

Since the set on the RHS involves combinations of kernels induced from distinct training set \(\mathcal{S}^{\prime}\), it is a strictly larger set than the LHS. Apply Theorem 3 bound \(U_{1}\) for each \(\mathcal{G}_{t}\) on \(\mathcal{S}\),

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{t})\leq\frac{B_{t}}{n}\left(\sup _{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\text{\rm Tr }(\mathsf{K}_{t,t+1}(\bm{Z},\bm{Z});\mathcal{S}^{\prime})+\sum_{i\neq j}\Delta_ {t}(\bm{z}_{i},\bm{z}_{j})\right)^{\frac{1}{2}},\] (11)

where \(\Delta_{t}(\bm{z}_{i},\bm{z}_{j})=\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i}, \bm{z}_{j};\mathcal{S}^{\prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\mathsf{K}_{t,t+1}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^ {\prime})\right]\).

By the monotonicity and linear combination of Rademacher complexity [38] and take in (11),

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}) \leq\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{0}\oplus\mathcal{ G}_{1}\oplus\cdots\oplus\mathcal{G}_{T-1}\oplus\left\{\ell(\bm{w}_{0},\bm{z})\right\})\] \[=\sum_{t=0}^{T-1}\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{t})+ \hat{\mathcal{R}}_{\mathcal{S}}(\{\ell(\bm{w}_{0},\bm{z})\})\] \[\leq\sum_{t=0}^{T-1}\frac{B_{t}}{n}\left(\sup_{\mathsf{K}(\cdot, \cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}}\text{\rm Tr}(\mathsf{K}_{t,t+1} (\bm{Z},\bm{Z});\mathcal{S}^{\prime})+\sum_{i\neq j}\Delta_{t}(\bm{z}_{i},\bm{z} _{j})\right)^{\frac{1}{2}}.\]Complete Proofs for Case Study

### Generalization bounds for Infinite-width NNs

#### e.1.1 Proof of Eq. (4)

Proof.: We bound \(U_{1}\) in Theorem 3 for an infinite-width NN. For an infinite-width NN, the NTK keeps unchanged during training:

\[\hat{\Theta}(\bm{w}_{t};\bm{x},\bm{x}^{\prime})\to\Theta(\bm{x},\bm{x}^{\prime} )\cdot\mathbf{I}_{k}.\]

Then for our loss path kernel, for any \(\bm{z},\bm{z}^{\prime}\in\mathcal{Z}\) and any \(\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T}\),

\[\mathsf{K}(\bm{z},\bm{z}^{\prime};\mathcal{S}^{\prime}) =\int_{0}^{T}\bar{\mathsf{K}}(\bm{w}_{t};\bm{z},\bm{z}^{\prime})dt\] (12) \[=\int_{0}^{T}\nabla_{f}\ell(\bm{w}_{t},\bm{z})^{\top}\hat{\Theta} (\bm{w}_{t};\bm{x},\bm{x}^{\prime})\nabla_{f}\ell(\bm{w}_{t},\bm{z}^{\prime})\,dt\] \[=\int_{0}^{T}\nabla_{f}\ell(\bm{w}_{t},\bm{z})^{\top}\Theta(\bm{ x},\bm{x}^{\prime})\cdot\mathbf{I}_{k}\nabla_{f}\ell(\bm{w}_{t},\bm{z}^{\prime})\,dt\] \[=\Theta(\bm{x},\bm{x}^{\prime})\cdot\int_{0}^{T}\nabla_{f}\ell( \bm{w}_{t},\bm{z})^{\top}\nabla_{f}\ell(\bm{w}_{t},\bm{z}^{\prime})\,dt.\]

Consider a \(\rho\)-Lipschitz loss function, i.e. \(\|\nabla_{f}\ell(\bm{w}_{t},\bm{z})\|\leq\rho\), e.g. \(\rho=1\) for hinge loss and logistic loss, \(\rho=\sqrt{2}\) for cross-entropy loss with one-hot labels. Then we have

\[-\rho^{2}\leq\nabla_{f}\ell(\bm{w}_{t},\bm{z})^{\top}\nabla_{f}\ell(\bm{w}_{t},\bm{z}^{\prime})\leq\rho^{2}.\]

Thus

\[-\rho^{2}T\leq\int_{0}^{T}\nabla_{f}\ell(\bm{w}_{t},\bm{z})^{\top}\nabla_{f} \ell(\bm{w}_{t},\bm{z}^{\prime})\,dt\leq\rho^{2}T.\]

Then by this inequality and Eq. (12),

\[-\rho^{2}T\left|\Theta(\bm{x},\bm{x}^{\prime})\right|\leq\mathsf{K}(\bm{z}, \bm{z}^{\prime};\mathcal{S}^{\prime})\leq\rho^{2}T\left|\Theta(\bm{x},\bm{x}^{ \prime})\right|.\]

Since \(\Theta(\bm{x}_{i},\bm{x}_{i})\geq 0\) for \(i\in[n]\), \(\mathsf{K}(\bm{z}_{i},\bm{z}_{i};\mathcal{S}^{\prime})\leq\rho^{2}T\cdot \Theta(\bm{x}_{i},\bm{x}_{i})\) and \(\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z});\mathcal{S}^{\prime})\leq\text{Tr }(\rho^{2}T\cdot\Theta(\mathbf{X},\mathbf{X}))\).

\[\Delta(\bm{z}_{i},\bm{z}_{j}) =\frac{1}{2}\left[\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{ \prime})-\inf_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{\prime})\in\mathcal{K}_{T} }\mathsf{K}(\bm{z}_{i},\bm{z}_{j};\mathcal{S}^{\prime})\right]\] \[\leq\frac{1}{2}\left[\rho^{2}T\left|\Theta(\bm{x}_{i},\bm{x}_{j })\right|-(-\rho^{2}T\left|\Theta(\bm{x},\bm{x}_{i})\right|)\right]\] \[=\rho^{2}T\left|\Theta(\bm{x}_{i},\bm{x}_{j})\right|\]

Take these terms into \(U_{1}\),

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}) \leq\frac{B}{n}\sqrt{\sup_{\mathsf{K}(\cdot,\cdot;\mathcal{S}^{ \prime})\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K}(\mathbf{Z},\mathbf{Z};\mathcal{ S}^{\prime}))+\sum_{i\neq j}\Delta(\bm{z}_{i},\bm{z}_{j})}\] (13) \[\leq\frac{B}{n}\sqrt{\text{Tr}(\rho^{2}T\cdot\Theta(\mathbf{X}, \mathbf{X}))+\sum_{i\neq j}\rho^{2}T\left|\Theta(\bm{x}_{i},\bm{x}_{j})\right|}\] \[=\frac{\rho B\sqrt{T}}{n}\sqrt{\sum_{i,j}|\Theta(\bm{x}_{i},\bm{x} _{j})|}.\]

### Generalization bound for stable algorithms

Let \(\mathcal{S}^{\prime}\) and \(S^{\prime}_{1}\) be two datasets that only differ in one data point. We make the following stability assumption for gradient decent.

**Assumption 1** (uniform stability of GD.).: Assume \(\left\|A_{t}(\mathcal{S}^{\prime})-A_{t}(S^{\prime}_{1})\right\|=\frac{ct}{n}\) for some constant \(c>0\). Assume \(\ell(\bm{w},\bm{z})\) is \(L_{\ell}\)-Lipschitz and \(\beta_{\ell}\)-smooth for any \(\bm{z}\in\mathcal{Z}\). Then \(\left\|\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z})-\nabla_{\bm{ w}}\ell(A_{t}(S^{\prime}_{1}),\bm{z})\right\|\leq\beta_{\ell}\left\|A_{t}( \mathcal{S}^{\prime})-A_{t}(S^{\prime}_{1})\right\|=c\beta_{\ell}\frac{t}{n}\).

This kind of stability results of GD and SGD are proved in [8, 25]. Under these stability and smoothness assumptions, we can bound the deviation of \(g(\bm{z}_{i})\) from its expectation with high probability and further bound the complexity based on Theorem 3.

**Theorem 8**.: _For any \(\delta\in(0,1)\), let \(\mathcal{G}_{T}^{\delta}\subset\mathcal{G}_{T}\) be a \(1-\delta\) subset of \(\mathcal{G}_{T}\), i.e. \(\left|\mathcal{G}_{T}^{\delta}\right|=\left(1-\delta\right)\left|\mathcal{G}_ {T}\right|\). Under Assumption 1, we have at least one of such \(\mathcal{G}_{T}^{\delta}\),_

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{\delta})\leq\left(2L_{\ell}^ {2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\sqrt{\frac{2\log(\frac{2n}{\delta})}{n}}.\]

This bound will naturally translate into a generalization bound by equipping with Theorem 1. This bound has a convergence rate of \(\tilde{O}(1/\sqrt{n})\). It shows that the complexity of NN trained by GD has a polynomial dependence on \(L_{\ell}\), \(\beta_{\ell}\), and training time \(T\).

Proof.: Let \(\mathcal{G}_{T}^{\delta,\mathcal{S}}=\left\{g(\bm{Z})=(g(\bm{z}_{1}),\ldots,g( \bm{z}_{n})):g\in\mathcal{G}_{T}^{\delta}\right\}\). Then similar bound as \(U_{2}\) in Theorem 3 holds for \(\mathcal{G}_{T}^{\delta}\),

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{\delta})\leq\inf_{\epsilon>0 }\left(\frac{\epsilon}{n}+\sqrt{\frac{2\ln\mathcal{N}(\mathcal{G}_{T}^{\delta,\mathcal{S}},\epsilon,\left\|\right\|_{1})}{n}}\right)\] (14)

We consider the covering of \(\mathcal{G}_{T}^{\delta}\) and upper bound the right hand side. Without loss of generality, suppose \(\mathcal{S}^{\prime}\) and \(S^{\prime}_{1}\) differ in the first sample. That is \(\mathcal{S}^{\prime}=\{\bm{z}^{\prime}_{1},\ldots,\bm{z}^{\prime}_{n}\}\) and \(S^{\prime}_{1}=\{\hat{\bm{z}}^{\prime}_{1},\ldots,\bm{z}^{\prime}_{n}\}\). For any fixed \(i\in[n]\) and \(g_{\mathcal{K}_{S^{\prime}_{1}}},g_{\mathcal{K}_{S^{\prime}}}\in\mathcal{G}_{T}\),

\[g_{\mathcal{K}_{S^{\prime}_{1}}}(\bm{z}_{i})-g_{\mathcal{K}_{S^{ \prime}}}(\bm{z}_{i})\] \[=\frac{1}{n}\mathcal{K}_{\mathcal{S}^{\prime}}(\bm{z}_{i},\bm{z}^ {\prime}_{1})-\frac{1}{n}\mathcal{K}_{S^{\prime}_{1}}(\bm{z}_{i},\hat{\bm{z}} ^{\prime}_{1})+\sum_{j=2}^{n}\frac{1}{n}\mathcal{K}_{\mathcal{S}^{\prime}}(\bm {z}_{i},\bm{z}^{\prime}_{j})-\sum_{j=2}^{n}\frac{1}{n}\mathcal{K}_{S^{\prime} _{1}}(\bm{z}_{i},\bm{z}^{\prime}_{j})\]

The first two terms are

\[\frac{1}{n}\mathcal{K}_{\mathcal{S}^{\prime}}(\bm{z}_{i},\bm{z}^ {\prime}_{1})-\frac{1}{n}\mathcal{K}_{S^{\prime}_{1}}(\bm{z}_{i},\hat{\bm{z}} ^{\prime}_{1})\] \[=\frac{1}{n}\int_{0}^{T}\left\langle\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{ \prime}),\bm{z}^{\prime}_{1})\right\rangle dt-\frac{1}{n}\int_{0}^{T}\left\langle \nabla_{\bm{w}}\ell(A_{t}(S^{\prime}_{1}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t }(S^{\prime}_{1}),\hat{\bm{z}}^{\prime}_{1})\right\rangle dt\] \[=\frac{1}{n}\int_{0}^{T}\left\langle\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{ \prime}),\bm{z}^{\prime}_{1})\right\rangle-\left\langle\nabla_{\bm{w}}\ell(A_{t }(S^{\prime}_{1}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}(S^{\prime}_{1}),\hat{ \bm{z}}^{\prime}_{1})\right\rangle dt\] \[\leq\frac{1}{n}\int_{0}^{T}\left\|\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{i})\right\|\left\|\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}^{\prime}_{1})\right\|+\left\|\nabla_{\bm{w}}\ell(A _{t}(S^{\prime}_{1}),\bm{z}_{i})\right\|\left\|\nabla_{\bm{w}}\ell(A_{t}(S^{ \prime}_{1}),\hat{\bm{z}}^{\prime}_{1})\right\|dt\] \[\leq\frac{1}{n}\int_{0}^{T}L_{\ell}^{2}+L_{\ell}^{2}dt\] \[=\frac{1}{n}2L_{\ell}^{2}T\]The last two terms are

\[\sum_{j=2}^{n}\frac{1}{n}\mathsf{K}_{\mathcal{S}^{\prime}}(\bm{z}_{i},\bm{z}_{j}^{\prime})-\sum_{j=2}^{n}\frac{1}{n}\mathsf{K}_{\mathcal{S}^{\prime}_ {1}}(\bm{z}_{i},\bm{z}_{j}^{\prime})\] \[=\frac{1}{n}\sum_{j=2}^{n}\mathsf{K}_{\mathcal{S}^{\prime}}(\bm{z} _{i},\bm{z}_{j}^{\prime})-\mathsf{K}_{\mathcal{S}^{\prime}_{1}}(\bm{z}_{i},\bm {z}_{j}^{\prime})\] \[=\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}\left\langle\nabla_{\bm{w} }\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{j}^{\prime})\right\rangle dt-\int_{0}^{T}\left \langle\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{i}),\nabla_ {\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^{\prime})\right\rangle dt\] \[=\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}\left\langle\nabla_{\bm{w} }\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{j}^{\prime})\right\rangle-\left\langle\nabla_{ \bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A _{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^{\prime})\right\rangle dt\] \[=\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}\left\langle\nabla_{\bm{w} }\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{j}^{\prime})\right\rangle-\left\langle\nabla_{ \bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t} (\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^{\prime})\right\rangle\] \[\qquad+\left\langle\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime }),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^ {\prime})\right\rangle-\left\langle\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{ \prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{ z}_{j}^{\prime})\right\rangle dt\] \[=\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}\left\langle\nabla_{\bm{w} }\ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i}),\nabla_{\bm{w}}\ell(A_{t}( \mathcal{S}^{\prime}),\bm{z}_{j}^{\prime})-\nabla_{\bm{w}}\ell(A_{t}(\mathcal{ S}^{\prime}_{1}),\bm{z}_{j}^{\prime})\right\rangle\] \[\qquad+\left\langle\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime }),\bm{z}_{i})-\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{i}), \nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^{\prime}) \right\rangle dt\] \[\leq\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}\left\|\nabla_{\bm{w}} \ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{i})\right\|\left\|\nabla_{\bm{w}} \ell(A_{t}(\mathcal{S}^{\prime}),\bm{z}_{j}^{\prime})-\nabla_{\bm{w}}\ell(A_{ t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j}^{\prime})\right\|\] \[\qquad+\left\|\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}), \bm{z}_{i})-\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{i}) \right\|\left\|\nabla_{\bm{w}}\ell(A_{t}(\mathcal{S}^{\prime}_{1}),\bm{z}_{j} ^{\prime})\right\|dt\] \[\leq\frac{1}{n}\sum_{j=2}^{n}\int_{0}^{T}L_{\ell}\beta_{\ell} \frac{ct}{n}+\beta_{\ell}\frac{ct}{n}L_{\ell}dt\] (by Assumption 1) \[=\left(1-\frac{1}{n}\right)cL_{\ell}\beta_{\ell}\frac{T^{2}}{n}\]

In total,

\[g_{\mathsf{K}_{\mathcal{S}^{\prime}_{1}}}(\bm{z}_{i})-g_{\mathsf{K }_{\mathcal{S}^{\prime}}}(\bm{z}_{i})\] \[\leq\frac{1}{n}2L_{\ell}^{2}T+\left(1-\frac{1}{n}\right)cL_{\ell} \beta_{\ell}\frac{T^{2}}{n}\] \[=\frac{1}{n}\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\] \[\leq\frac{1}{n}\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\]

Similarly \(g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})-g_{\mathsf{K}_{\mathcal{S}^{ \prime}_{1}}}(\bm{z}_{i})\leq\frac{1}{n}\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{ \ell}T^{2}\right)\). Thus \(\left|g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})-g_{\mathsf{K}_{\mathcal{S}^ {\prime}_{1}}}(\bm{z}_{i})\right|\leq\frac{1}{n}\left(2L_{\ell}^{2}T+cL_{\ell} \beta_{\ell}T^{2}\right)\). Then by McDiarmid's inequality, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),

\[\left|g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})-\mathop{ \mathbb{E}}_{\mathcal{S}^{\prime}}\left[g_{\mathsf{K}_{\mathcal{S}^{\prime}}}( \bm{z}_{i})\right]\right|\leq\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2} \right)\sqrt{\frac{\log(\frac{2}{\delta})}{2n}}\]

Then, by a union bound, with probability at least \(1-\delta\), for all \(i\in[n]\),

\[\left|g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})-\mathop{ \mathbb{E}}_{\mathcal{S}^{\prime}}\left[g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_ {i})\right]\right|\leq\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right) \sqrt{\frac{\log(\frac{2n}{\delta})}{2n}}\]

This means that with probability at least \(1-\delta\), for all \(i\in[n]\),

\[g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})\in\left[\mathop{ \mathbb{E}}_{\mathcal{S}^{\prime}}\left[g_{\mathsf{K}_{\mathcal{S}^{\prime}}}( \bm{z}_{i})\right]-\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right) \sqrt{\frac{\log(\frac{2n}{\delta})}{2n}},\mathop{\mathbb{E}}_{\mathcal{S}^{ \prime}}\left[g_{\mathsf{K}_{\mathcal{S}^{\prime}}}(\bm{z}_{i})\right]+\left(2L_{ \ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\sqrt{\frac{\log(\frac{2n}{\delta})}{2 n}}\right]\]Use the \(g_{\mathsf{K}_{S^{\prime}}}\) in this range to construct the \(\mathcal{G}_{T}^{\delta}\). Then for any \(g_{1}(\mathbf{Z}),g_{2}(\mathbf{Z})\in\mathcal{G}_{T}^{\delta,\mathcal{S}}\),

\[g_{1}(\mathbf{Z})-g_{2}(\mathbf{Z}) =\sum_{i=1}^{n}|g_{1}(\bm{z}_{i})-g_{2}(\bm{z}_{i})|\] \[\leq\sum_{i=1}^{n}2\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2 }\right)\sqrt{\frac{\log(\frac{2n}{\delta})}{2n}}\] \[=\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\sqrt{2n \log(\frac{2n}{\delta})}.\]

Take \(\epsilon\) as this value, then \(\mathcal{N}(\mathcal{G}_{T}^{\delta,\mathcal{S}},\epsilon,\left|\kern-1.0pt \left|\kern-1.0pt\left|\kern-1.0pt\left|\right|_{1}\right)=1\right.\). Take this into Eq. (14),

\[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}_{T}^{\delta}) \leq\inf_{\epsilon>0}\left(\frac{\epsilon}{n}+\sqrt{\frac{2\ln \mathcal{N}(\mathcal{G}_{T}^{\delta,\mathcal{S}},\epsilon,\left|\kern-1.0pt \left|\kern-1.0pt\left|\right|_{1}\right)}{n}}\right)\] \[\leq\left(2L_{\ell}^{2}T+cL_{\ell}\beta_{\ell}T^{2}\right)\sqrt{ \frac{2\log(\frac{2n}{\delta})}{n}}.\]

### Norm-constrained neural network

For simplicity, we consider the one-dimensional output in this subsection, i.e. \(k=1\), and assume the loss function is \(\rho\)-lipschitz for the model output, \(\|\ell(\hat{\bm{y}},\bm{y})-\ell(\hat{\bm{y}}^{\prime},\bm{y})\|\leq\rho(\hat {\bm{y}}-\hat{\bm{y}}^{\prime})\) for every \(\hat{\bm{y}},\hat{\bm{y}}^{\prime}\in\mathbb{R}\). Consider one-layer NNs:

\[\mathcal{F}=\left\{f(\bm{w},\bm{x})=\bm{w}^{T}\bm{x}:\bm{w}\in\mathbb{R}^{d}, \bm{x}\in\mathbb{R}^{d}\right\}.\]

**Proposition 2**.: _For the function class of one-layer NN defined above,_

\[\sup_{\mathsf{K}_{S^{\prime}}\in\mathsf{K}_{T}}\text{Tr}(\mathsf{K}_{S^{ \prime}}(\mathbf{Z},\mathbf{Z}))\leq\rho^{2}T\sum_{i=1}^{n}\left\|\bm{x}_{i} \right\|^{2}.\]

For this one-layer NN, we do not need a norm constraint. For \(L\)-layer NNs with norm constraints:

\[\mathcal{F}=\left\{f(\bm{w},\bm{x})=W^{L}\sigma(W^{L-1}\cdots\sigma(W^{1}\bm{ x})):\left\|W_{t}^{h}\right\|\leq B_{i},t\in[0,T]\right\}\]

where \(W^{h}\in\mathbb{R}^{d_{h}\times d_{h-1}}\) for \(h\in[L]\) with \(d_{L}=1,d_{0}=d\). \(\sigma\) is the element-wise activation function and is 1-lipschitz with \(\sigma(0)=0\). With these norm constraints of the parameters during the training, we can further bound the trace term in \(U_{1}\) as follows.

**Theorem 9**.: _For the function class of \(L\)-layer NN defined above,_

\[\sup_{\mathsf{K}_{S^{\prime}}\in\mathsf{K}_{T}}\text{Tr}(\mathsf{K}_{S^{ \prime}}(\mathbf{Z},\mathbf{Z}))\leq\rho^{2}T\sum_{i=1}^{n}\left\|\bm{x}_{i} \right\|^{2}\prod_{j=1}^{L}B_{j}^{2}\sum_{h=1}^{L}\frac{1}{B_{h}^{2}}.\]

This bound shows that this quantity has a linear relation with \(T\). With a finer constraint of \(\left\|W_{t}^{i}\right\|\) during the training, for example, \(\left\|W_{t}^{i}\right\|\leq B_{i,t^{\prime}},t\in[t^{\prime},t^{\prime}+1]\), we can get tighter bound. Although similar to previous norm-based bounds that have a polynomial dependence with the norms of the parameters, our bound has a clear dependence on training time \(T\), which is not achievable from previous approaches. But note these bounds can be very loose since they are worse-case bounds.

#### e.3.1 Proof of Proposition 2

Proof.: \[\mathsf{K}_{\mathcal{S}^{\prime}}(\bm{z},\bm{z}^{\prime}) =\int_{0}^{T}\left\langle\ell^{\prime}(f_{t}(\bm{x}),\bm{y})\nabla_{ \bm{w}}f_{t}(\bm{x}),\ell^{\prime}(f_{t}(\bm{x}^{\prime}),\bm{y}^{\prime}) \nabla_{\bm{w}}f_{t}(\bm{x}^{\prime})\right\rangle dt\] \[=\int_{0}^{T}\left\langle\ell^{\prime}(f_{t}(\bm{x}),\bm{y})\bm{ x},\ell^{\prime}(f_{t}(\bm{x}^{\prime}),\bm{y}^{\prime})\bm{x}^{\prime} \right\rangle dt\] \[=\int_{0}^{T}\ell^{\prime}(f_{t}(\bm{x}),\bm{y})\ell^{\prime}(f_ {t}(\bm{x}^{\prime}),\bm{y}^{\prime})\left\langle\bm{x},\bm{x}^{\prime}\right\rangle dt\] \[\leq\rho^{2}\left|\left\langle\bm{x},\bm{x}^{\prime}\right\rangle \right|T.\]

Thus

\[\text{Tr}(\mathsf{K}_{\mathcal{S}^{\prime}}(\mathbf{Z},\mathbf{Z}))=\sum_{i=1 }^{n}\mathsf{K}_{T}(\bm{z}_{i},\bm{z}_{i})\leq\sum_{i=1}^{n}\rho^{2}\left\|\bm{ x}_{i}\right\|^{2}T=\rho^{2}T\sum_{i=1}^{n}\left\|\bm{x}_{i}\right\|^{2}.\]

Since this holds for any \(\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}\),

\[\sup_{\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K }_{\mathcal{S}^{\prime}}(\mathbf{Z},\mathbf{Z}))\leq\rho^{2}T\sum_{i=1}^{n} \left\|\bm{x}_{i}\right\|^{2}.\]

#### e.3.2 Proof of Theorem 9

Proof.: Denote

\[f^{h}(\bm{x})=W^{h}g^{h-1}(\bm{x})\in\mathbb{R}^{d_{h}},\quad g^{h-1}(\bm{x}) =\sigma(f^{h-1}(\bm{x}))\in\mathbb{R}^{d_{h-1}},\quad h\in[L]\]

\[\frac{\partial f(\bm{w},\bm{x})}{\partial W^{h}}=b^{h}(\bm{x})\left(g^{h-1}( \bm{x})\right)^{T}\in\mathbb{R}^{d_{h}\times d_{h-1}},\quad h\in[L]\]

where

\[b^{h}(\bm{x})=\left\{\begin{array}{ll}1\in\mathbb{R},&h=L,\\ D^{h}(\bm{x})\left(W^{h+1}\right)^{T}b^{h+1}(\bm{x})\in\mathbb{R}^{d_{h}}&h\in [L-1],\end{array}\right.\]

\[D^{h}(\bm{x})=diag(\dot{\sigma}(f^{h}(\bm{x})))\in\mathbb{R}^{d_{h}\times d_{h }},\qquad h\in[L-1].\]

Then for any \(h\in[L]\), we can compute

\[\left\langle\frac{\partial f(\bm{w},\bm{x})}{\partial W^{h}}, \frac{\partial f(\bm{w},\bm{x}^{\prime})}{\partial W^{h}}\right\rangle\] \[=\left\langle b^{h}(\bm{x})\left(g^{h-1}(\bm{x})\right)^{T},b^{h} (\bm{x}^{\prime})\left(g^{h-1}(\bm{x}^{\prime})\right)^{T}\right\rangle\] (inner product of matrices) \[=\text{Tr}\left(g^{h-1}(\bm{x})b^{h}(\bm{x})^{T}b^{h}(\bm{x}^{ \prime})\left(g^{h-1}(\bm{x}^{\prime})\right)^{T}\right)\] \[=\text{Tr}\left(\left(g^{h-1}(\bm{x}^{\prime})\right)^{T}g^{h-1}( \bm{x})b^{h}(\bm{x})^{T}b^{h}(\bm{x}^{\prime})\right)\] \[=\left\langle g^{h-1}(\bm{x}),g^{h-1}(\bm{x}^{\prime})\right\rangle \cdot\left\langle b^{h}(\bm{x}),b^{h}(\bm{x}^{\prime})\right\rangle,\]

where we have for the first term,

\[\left\langle g^{h-1}(\bm{x}),g^{h-1}(\bm{x}^{\prime})\right\rangle \leq\left\|g^{h-1}(\bm{x})\right\|\left\|g^{h-1}(\bm{x}^{\prime})\right\|\] \[\leq\prod_{j=1}^{h-1}\left\|W^{j}\right\|^{2}\left\|\bm{x}\right\| \left\|\bm{x}^{\prime}\right\|.\]The second term can be bounded as

\[\left\langle b^{h}(\bm{x}),b^{h}(\bm{x}^{\prime})\right\rangle \leq\left\|b^{h}(\bm{x})\right\|\left\|b^{h}(\bm{x}^{\prime})\right\|\] \[=\left\|D^{h}(\bm{x})\left(W^{h+1}\right)^{T}b^{h+1}(\bm{x}) \right\|\left\|D^{h}(\bm{x}^{\prime})\left(W^{h+1}\right)^{T}b^{h+1}(\bm{x}^{ \prime})\right\|\] \[\leq\left\|D^{h}(\bm{x})\right\|\left\|W^{h+1}\right\|\left\|b^{h +1}(\bm{x})\right\|\left\|D^{h}(\bm{x}^{\prime})\right\|\left\|W^{h+1}\right\| \left\|b^{h+1}(\bm{x}^{\prime})\right\|\] \[\leq\left\|W^{h+1}\right\|^{2}\left\|b^{h+1}(\bm{x})\right\| \left\|b^{h+1}(\bm{x}^{\prime})\right\|\] \[\leq\prod_{j=h+1}^{L}\left\|W^{j}\right\|^{2}.\]

Thus in total,

\[\left\langle\frac{\partial f(\bm{w},\bm{x})}{\partial W^{h}}, \frac{\partial f(\bm{w},\bm{x}^{\prime})}{\partial W^{h}}\right\rangle =\left\langle g^{h-1}(\bm{x}),g^{h-1}(\bm{x}^{\prime})\right\rangle \cdot\left\langle b^{h}(\bm{x}),b^{h}(\bm{x}^{\prime})\right\rangle\] \[\leq\prod_{j=1}^{h-1}\left\|W^{j}\right\|^{2}\left\|\bm{x}\right\| \left\|\bm{x}^{\prime}\right\|\cdot\prod_{j=h+1}^{L}\left\|W^{j}\right\|^{2}\] \[=\left\|\bm{x}\right\|\left\|\bm{x}^{\prime}\right\|\frac{\prod_{ j=1}^{L}\left\|W^{j}\right\|^{2}}{\left\|W^{h}\right\|^{2}}\]

Since the tangent kernel \(\left\langle\nabla_{\bm{w}}f(\bm{w},\bm{x}),\nabla_{\bm{w}}f(\bm{w},\bm{x}^{ \prime})\right\rangle=\sum_{h=1}^{L}\left\langle\frac{\partial f(\bm{w},\bm{x} )}{\partial W^{h}},\frac{\partial f(\bm{w},\bm{x}^{\prime})}{\partial W^{h}}\right\rangle\), we obtain an upper bound for the tangent kernel,

\[\left\langle\nabla_{\bm{w}}f(\bm{w},\bm{x}),\nabla_{\bm{w}}f(\bm{w},\bm{x}^{ \prime})\right\rangle\leq\left\|\bm{x}\right\|\left\|\bm{x}^{\prime}\right\| \prod_{j=1}^{L}\left\|W^{j}\right\|^{2}\sum_{h=1}^{L}\frac{1}{\left\|W^{h} \right\|^{2}}.\]

Thus

\[\mathsf{K}_{\mathcal{S}^{\prime}}(\bm{z},\bm{z}^{\prime}) =\int_{0}^{T}l^{\prime}(f(\bm{w}_{t},\bm{x}),\bm{y})l^{\prime}(f( \bm{w}_{t},\bm{x}^{\prime}),\bm{y}^{\prime})\left\langle\nabla_{\bm{w}}f(\bm{ w}_{t},\bm{x}),\nabla_{\bm{w}}f(\bm{w}_{t},\bm{x}^{\prime})\right\rangle dt\] \[\leq\int_{0}^{T}\rho^{2}\left\|\bm{x}\right\|\left\|\bm{x}^{ \prime}\right\|\prod_{j=1}^{L}\left\|W^{j}\right\|^{2}\sum_{h=1}^{L}\frac{1}{ \left\|W^{h}\right\|^{2}}dt\] \[\leq\int_{0}^{T}\rho^{2}\left\|\bm{x}\right\|\left\|\bm{x}^{ \prime}\right\|\prod_{j=1}^{L}B_{j}^{2}\sum_{h=1}^{L}\frac{1}{B_{h}^{2}}dt\] \[=\rho^{2}T\left\|\bm{x}\right\|\left\|\bm{x}^{\prime}\right\|\prod _{j=1}^{L}B_{j}^{2}\sum_{h=1}^{L}\frac{1}{B_{h}^{2}}.\]

Thus,

\[\text{Tr}(\mathsf{K}_{\mathcal{S}^{\prime}}(\mathbf{Z},\mathbf{Z}))\leq\rho^{2 }T\sum_{i=1}^{n}\left\|\bm{x}_{i}\right\|^{2}\prod_{j=1}^{L}B_{j}^{2}\sum_{h= 1}^{L}\frac{1}{B_{h}^{2}}.\]

Since this holds for any \(\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}\),

\[\sup_{\mathsf{K}_{\mathcal{S}^{\prime}}\in\mathcal{K}_{T}}\text{Tr}(\mathsf{K}_ {\mathcal{S}^{\prime}}(\mathbf{Z},\mathbf{Z}))\leq\rho^{2}T\sum_{i=1}^{n} \left\|\bm{x}_{i}\right\|^{2}\prod_{j=1}^{L}B_{j}^{2}\sum_{h=1}^{L}\frac{1}{B_ {h}^{2}}.\]