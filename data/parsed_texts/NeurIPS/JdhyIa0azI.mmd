# Neural Functional Transformers

 Allan Zhou\({}^{1}\) Kaien Yang\({}^{1}\) Yiding Jiang\({}^{2}\) Kaylee Burns\({}^{1}\)

Winnie Xu\({}^{1}\) Samuel Sokota\({}^{2}\) J. Zico Kolter\({}^{2}\) Chelsea Finn\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)Carnegie Mellon University

ayz@cs.stanford.edu

###### Abstract

The recent success of neural networks as implicit representation of data has driven growing interest in _neural functionals_: models that can process other neural networks as input by operating directly over their weight spaces. Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called _neural functional Transformers_ (NFTs). NFTs respect weight-space permutation symmetries while incorporating the advantages of attention, which have exhibited remarkable success across multiple domains. In experiments processing the weights of feedforward MLPs and CNNs, we find that NFTs match or exceed the performance of prior weight-space methods. We also leverage NFTs to develop Inr2Array, a novel method for computing permutation invariant latent representations from the weights of implicit neural representations (INRs). Our proposed method improves INR classification accuracy by up to \(+17\%\) over existing methods. We provide an implementation of our layers at https://github.com/AllanYangZhou/nfn.

## 1 Introduction

Deep neural networks have emerged as flexible modeling tools applicable to a wide variety of different fields, ranging from natural language processing to vision to the natural sciences. The sub-field of implicit neural representations (INRs) has achieved significant success in using neural networks to represent data such as 3D surfaces or scenes [32, 30, 37]. This has fueled interest in techniques that directly operate in weight space to modify or extract information from a neural network through its weights or gradients. However, developing models that can process weight-space objects is challenging due to their high dimensional nature. As a consequence, some existing methods for processing datasets of weights assume a restricted training process that reduces the effective weight space [9, 3, 6].

In contrast, we follow recent work in building permutation equivariant weight-space models called _neural functionals_, that can process neural network weights1 without such restrictions [31, 46]. In particular, this work concerns neural functionals that are equivariant to permutations of the weights that correspond to re-arranging neurons. These weight-space permutations, known as _neuron permutation symmetries_, exactly preserve the network's behavior (see examples in Figure 1). Since equivariance to neuron permutations is typically a useful inductive bias for weight-space tasks, neural functionals achieve superior generalization compared to non-equivariant architectures. Despite this, their performance on weight-space tasks remains significantly worse than convolutional networks' performance on analogous image-space tasks [31, 46], suggesting that neural functional architectures can be significantly improved.

Footnote 1: For clarity, we use “weights” (and “weight space”) to describe the weights (and space they belong to) for the network being processed by a neural functional. We use “parameters” for the weights of the neural functional.

While existing neural functionals rely on _linear_ layers (analogous to convolution), some of the most successful architectures in other domains, such as Transformers [41; 8], rely on _nonlinear_ attention mechanisms. Motivated by this fact, our work develops novel equivariant weight-space layers based on attention. While naive self-attention between input weights is permutation equivariant, it cannot distinguish true neuron permutation symmetries (which preserve network behavior) from "false" permutation symmetries2 (which do not preserve network behavior). In contrast, we prove that our weight-space self-attention layer is _minimally equivariant_: it is equivariant to _only_ neuron permutations, which are the actual weight-space symmetries.

Footnote 2: Moreover, naive self-attention between input weights is computationally intractable.

When composed into deep architectures, our weight-space attention layers give rise to neural functional transformers (NFTs). As an immediate application, we use NFTs to develop Inr2Array, a method for mapping INR weights into compact latent representations that are trained through a reconstruction-based objective. Inr2Array produces permutation-invariant latents that can be useful for downstream tasks such as INR classification. We also construct NFTs for a variety of other tasks such as weight-space editing or generalization prediction.

NFTs are competitive with or outperform existing methods on each task without using more parameters. Notably, NFTs and Inr2Array improve downstream INR classification accuracy over the best existing methods on multiple datasets, by up to \(+17\%\). Overall, our experiments show that attention-based neural functionals can be more expressive and powerful than existing architectures that rely on linear weight-space layers, leading to improved performance across weight-space tasks.

## 2 Preliminaries

Consider a feedforward network having \(n_{i}\) neurons at each layer \(i=0,\cdots,L\), including the input and output layers. The network has weights3\(W=\big{\{}\,W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}}\,|\,i\in \llbracket 1..L\rrbracket\,\big{\}}\) belonging to weight space, \(\mathcal{W}\). More generally, we are interested in multi-channel weight space features that can arise from stacking multiple weight space objects such as weights and gradients. For \(c\)-channel weight-space features \(W\in\mathcal{W}^{c}\), we have \(W=\big{\{}\,W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times c}\,|\,i\in \llbracket 1..L\rrbracket\,\big{\}}\) and each \(W^{(i)}_{jk}:=W^{(i)}_{j,k,:}\in\mathbb{R}^{c}\) is a vector rather than a scalar.

Footnote 3: For simplicity, we omit the biases from our discussion here, see the Appendix for full detail.

Since the neurons in each hidden layer \(i\in\{1,\cdots,L-1\}\) have no inherent ordering, the feedforward network is invariant to the symmetric group \(S_{n_{i}}\) of permutations of the neurons in layer \(i\). We follow Zhou et al. [46] in studying the **neuron permutation** (NP) group \(\mathcal{S}_{\text{NP}}\coloneqq S_{0}\times\cdots\times S_{n_{L}}\), which further assumes permutation symmetry of the input and output layers. Although this simplifying assumption is usually too strong, it can be effectively corrected in practice using position encodings at the input and output layers [46].

Consider a neuron permutation \(\sigma=(\sigma_{0},\cdots,\sigma_{L})\in\mathcal{S}_{\text{NP}}\). Each \(\sigma_{i}\) permutes the neurons of layer \(i\), which correspond to the output space (rows) of \(W^{(i-1)}\) and the input space (columns) of \(W^{(i)}\). Hence the action on weight space features \(W\in\mathcal{W}^{c}\) is denoted \(\sigma W\), where

\[[\sigma W]^{(i)}_{jk}=W^{(i)}_{\sigma_{i}^{-1}(j),\sigma_{i-1}^{-1}(k)},\quad \sigma=(\sigma_{0},\cdots,\sigma_{L})\in\mathcal{S}_{\text{NP}}.\] (1)

As illustrated in Figure 1, a neuron permutation must always permute the columns of \(W^{(i)}\) and the rows of \(W^{(i-1)}\) simultaneously in order to preserve network behavior.

This work is focused on principled architecture design for neural functionals, i.e., architectures that process weight space features [46]. Recent work has shown the benefit of incorporating neuron permutation symmetries into neural functional architectures by enforcing equivariance (or invariance) to neuron permutation symmetries [31; 46]. Consider weight-space function classes parameterized by \(\theta\in\Theta\). We say that a function class \(f:\mathcal{W}^{c}\times\Theta\to\mathcal{W}^{c}\) is \(\mathcal{S}_{\text{NP}}\)**-equivariant** if permuting the input by any \(\sigma\in\mathcal{S}_{\text{NP}}\) always has the same effect as permuting the output by \(\sigma\):

\[\sigma f(W;\theta)=f(\sigma W;\theta),\quad\forall\sigma\in\mathcal{S}_{\text{ NP}},W\in\mathcal{W}^{c},\theta\in\Theta.\] (2)

Similarly, a function class \(f:\mathcal{W}^{c}\times\Theta\to\mathbb{R}\) is \(\mathcal{S}_{\text{NP}}\)**-invariant** if \(f(\sigma W;\theta)=f(W;\theta)\) for all \(\sigma,W,\) and \(\theta\).

Our weight-space layers build off of the dot-product attention mechanism, which takes a query \(q\) and a set of \(N\) key-value pairs \(\set{(k_{p},v_{p})}_{p=1}^{N}\) and computes, in the single-headed case:

\[\textsc{Attn}\left(q,\set{(k_{p},v_{p})}_{p=1}^{N}\right)=\sum_{p}v_{p}\left( \frac{\exp(q\cdot k_{p})}{\sum_{p^{\prime}}\exp(q\cdot k_{p^{\prime}})}\right).\] (3)

If all \(q,k_{p},v_{p}\) are vectors in \(\mathbb{R}^{d}\), then the output is also a vector in \(\mathbb{R}^{d}\). The definition extends to the situation where \(q,k,v_{p}\) are multi-dimensional arrays of equal shape, by taking the dot product between these arrays in the natural way. We can also use multi-headed attention ([41]) without significant modification.

## 3 Neural Functional Transformers

We now introduce two attention-based layers for processing weight-space features: weight-space self-attention (which is _\(\mathcal{S}_{\text{NP}}\)-equivariant_) and weight-space cross-attention (which is _\(\mathcal{S}_{\text{NP}}\)-invariant_). We then describe how to compose these layers into deep architectures called neural functional transformers (NFTs).

### Equivariant weight-space self-attention

A key concept motivating the design of our weight-space self-attention layer is the distinction between actual NP symmetries \(\mathcal{S}_{\text{NP}}\) that preserve network behavior, and other permutations of the weights that can generally change network behavior. For example, consider a \(\tau\in S_{\text{dim}(\mathcal{W})}\) that permutes the columns of \(W^{(i)}\) differently from the rows of \(W^{(i-1)}\) or that moves weights between different weight matrices (see Figure 1). Such permutations of the weight space are called "false symmetries" [31] since they generally modify network behavior. We are interested in equivariance to the actual weight-space symmetries (neuron permutations) but _not_ to any false symmetries, a property we call _minimal_ equivariance:

**Definition 1**.: _A function class \(f:\mathcal{W}^{c}\times\Theta\rightarrow\mathcal{W}^{c}\) is minimally \(\mathcal{S}_{\text{NP}}\)-equivariant if it is \(\mathcal{S}_{\text{NP}}\)-equivariant (Eq. 2), but not equivariant to any false symmetries. More precisely, for any \(\tau\in S_{\text{dim}(\mathcal{W})}\) such that \(\tau\notin\mathcal{S}_{\text{NP}}\), there exist a \(W\in\mathcal{W}^{c}\) and \(\theta\in\Theta\) such that \(f(\tau W;\theta)\neq\tau f(W;\theta)\)._

Consider naive self-attention between the \(\dim(\mathcal{W})\) weights \(\set{W^{(i)}_{jk}}\), which would be equivariant to _any_ weight-space permutation. This would be \(\mathcal{S}_{\text{NP}}\)-equivariant, but would also be equivariant to any false permutation symmetries, meaning that it is not minimally equivariant. In other words, naive self-attention is overly constrained by too many symmetries, including those that are not actually relevant to weight-space tasks.

Our weight-space self-attention layer \(\textsc{SA}(\cdot;\theta_{Q},\theta_{K},\theta_{V}):\mathcal{W}^{c} \rightarrow\mathcal{W}^{c}\) is more sensitive to the distinction between neuron permutations and false symmetries. It operates on weight-space features with \(c\) channels and is parameterized by query/key/value projection matrices \(\theta_{Q},\theta_{K},\theta_{V}\in\mathbb{R}^{c\times c}\).

Figure 1: **Left:** An illustration of how neuron permutations \(\sigma=(\sigma_{0},\cdots,\sigma_{L})\) act on weight matrices \(W=(W^{(0)},\cdots,W^{(L)})\). Each \(\sigma_{i}\) simultaneously permutes the rows and columns of adjacent weight matrices \(W^{(i)},W^{(i+1)}\). **Right:** Two examples of false symmetries, i.e., permutations that don’t preserve network behavior: (1) when \(\tau_{i}\) permutes the rows and columns of adjacent matrices permute differently and (2) when \(\tilde{\tau}\) permutes weights across layers.

Each entry of the output feature is computed:

\[\text{SA}\left(W;\theta_{Q},\theta_{K},\theta_{V}\right)_{jk}^{(i)}= \quad\text{{Attn}}\left(Q_{j,:}^{(i)},\Big{\{}\left(K,V\right)_{:,q }^{(i-1)}\Big{\}}_{q=1}^{n_{i-2}}\bigcup\Big{\{}\left(K,V\right)_{p,:}^{(i)} \Big{\}}_{p=1}^{n_{i}}\right)_{k}\] (4) \[+\text{{Attn}}\left(Q_{:,k}^{(i)},\Big{\{}\left(K,V\right)_{:,q}^{( i)}\Big{\}}_{q=1}^{n_{i-1}}\bigcup\Big{\{}\left(K,V\right)_{p,:}^{(i+1)}\Big{\}}_{p=1}^{n_ {i+1}}\right)_{j}\] (5) \[+\text{{Attn}}\left(Q_{jk}^{(i)},\Big{\{}\left(K,V\right)_{pq}^{( s)}\Big{|}\;\forall s,p,q\;\Big{\}}\right),\;\text{where}\] (6) \[Q_{j,k}^{(i)}:=\theta_{Q}W_{jk}^{(i)},\quad K_{j,k}^{(i)}:= \theta_{K}W_{jk}^{(i)},\quad V_{j,k}^{(i)}:=\theta_{V}W_{jk}^{(i)},\] (7)

and the notation \((K,V)_{jk}^{(i)}\) is shorthand for the tuple \(\left(K_{jk}^{(i)},V_{jk}^{(i)}\right)\). Appendix A.2 defines the more general version of this layer, which handles inputs with both weights and biases.

The final term (Eq. 6) is simply naive self-attention between the inputs \(\{\,W_{jk}^{(i)}\in\mathbb{R}^{c}\,\}\), but the first two terms give the layer additional structure relevant to neuron permutations in particular, as illustrated by Figure 2. To understand the symmetry properties of the first two terms, consider the top input to Attn in Figure 2, which is constructed from the rows of \(W^{(\ell)}\) and the columns of \(W^{(ell+1)}\). If the rows of \(W^{(\ell)}\) and the columns of \(W^{(\ell+1)}\) are permuted simultaneously, the pairwise dot products between any two vectors in the input is preserved and the attention matrix is unchanged. Then the output of Attn is simply permuted, giving equivariance. On the other hand, independently permuting the rows and columns of \(W^{(\ell)}\) and \(W^{(\ell+1)}\) will in general change the attention matrix, breaking equivariance. Since such independent permutations are false symmetries, this behavior is exactly what we need to achieve minimal equivariance.

Since neuron permutations never permute weights _between_ different layers \(W^{(i)}\) and \(W^{(i^{\prime})}\), we may additionally use learned layer position encodings \(\{\,\phi^{(i)}\in\mathbb{R}^{c}\;|\;i\in\llbracket 1..L\rrbracket\,\}\) to prevent equivariance to such permutations:

\[\text{LayerEnc}\left(W;\{\,\phi^{(i)}\,\}_{i=1}^{L}\right)_{jk}^{(i)}=W_{jk} ^{(i)}+\phi^{(i)}.\] (8)

We can now compose self-attention with position encoding to obtain SA \(\circ\)LayerEnc.

**Theorem 3.1** (Minimal equivariance).: _The combined layer SA \(\circ\)LayerEnc is **minimally**\(\mathcal{S}_{NP}\)-equivariant._

Figure 2: An illustration of Eqs 4-5 of our weight-space self-attention layer, in the single channel case. The inputs to Attn (Eq 3) are matrices with rows and columns corresponding to the sequence and feature dimensions, respectively. The final term (Eq 6, not shown here) simply computes self attention between all weights.

Proof (Sketch).: To show equivariance: LayerEnc is clearly equivariant to neuron permutations. We can check \(\text{SA}(\sigma W)^{(i)}_{jk}=\text{SA}(W)^{(i)}_{\sigma_{i}^{-1}(j),\sigma_{i -1}^{-1}(k)}\) by expanding the left hand side term-by-term using the definition (Eq. 4). To show non-equivariance to false symmetries: we can broadly classify false symmetries into three types and check non-equivariance to each of them. Appendix A.1 provides the full proof. 

Practical considerations.The final term of SA (Eq. 6) amounts to self-attention between each weight-space feature \(W^{(i)}_{jk}\in\mathbb{R}^{c}\) and requires \(O\left((\dim\mathcal{W})^{2}c\right)\) operations, which is usually intractable.

In practice, we replace this term by a self-attention between the row-column sums \(W^{(i)}_{*,*}=\sum_{j,k}W^{(i)}_{jk}\) of each weight matrix. This preserves interaction between weight-space features in different weight matrices, while reducing the computational complexity to \(O\left(L^{2}c\right)\). One can verify that this simplified version also maintains minimal \(\mathcal{S}_{\text{NP}}\)-equivariance. Meanwhile, consider the self-attention operation required to compute the first two terms of SA (depicted in Figure 2). If \(n_{i}=n\) for all \(i\), then the first two terms require \(O(Ln^{3}c)\) operations, which is feasible for moderate \(n\) but can become expensive when processing very wide networks. As a result, weight-space attention layers are usually more computationally expensive than linear weight-space layers.

### Invariant weight-space cross-attention

Stacking weight-space self-attention layers produces equivariant weight-space features, but some situations require producing an \(\mathcal{S}_{\text{NP}}\)_-invariant_ output. Existing approaches achieve invariance by relying on a summation or maximization over the rows and columns of each input \(W^{(i)}\)[31, 46].

It is natural to extend these existing invariant layers by using attention over the input's entries, which has similar permutation invariant properties as summation and maximization. Our weight-space cross-attention layer \(\text{CA}:\mathcal{W}^{c}\rightarrow\mathbb{R}^{d}\) is simply a cross attention between a learned query \(e\in\mathbb{R}^{d}\) and the set keys and values produced from weight-space features \(\left\{\,W^{(i)}_{jk}\in\mathbb{R}^{c}\,\right\}\):

\[\text{CA}\left(W;e,\theta_{K},\theta_{V}\right)=\texttt{Attn}\left(e,\left\{ \,\left(\theta_{K}W^{(s)}_{pq},\theta_{V}W^{(s)}_{pq}\right)\,\Big{|}\,\, \forall s,p,q\,\right\}\right),\] (9)

with \(\theta_{K},\theta_{V}\in\mathbb{R}^{d\times c}\) being learned projection matrices. By repeating this operation with \(M\) different learned embeddings \(e_{1},\cdots,e_{M}\), we can easily extend this into an invariant map \(\mathcal{W}^{c}\rightarrow\mathbb{R}^{M\times d}\). We depict the operation of the multi-embedding case in Figure 3.

### Convolutional weight spaces

Although our description thus far focuses on fully-connected weight-space features \(W\), we can also extend our layers to convolutional weight spaces. Suppose \(W\) is the \(c\)-channel weight-space feature corresponding to a 1D convolutional network with filter widths \(k_{i}\) at each layer \(i\). It contains matrices

Figure 3: An illustration of our weight-space cross-attention layer, which pools weight-space features into a set of \(\mathcal{S}_{\text{NP}}\)-invariant vectors. The layer uses a set of learned queries \(Q\in\mathbb{R}^{M\times c}\) to attend over keys and values produced from the weight space features.

\(W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times k_{i}\times c}\). As in Zhou et al. [46], the filter dimension \(k_{i}\) can be folded into the channel dimension, creating a feature in \(\mathbb{R}^{n_{i}\times n_{i-1}\times(k_{i}c)}\) with \(k_{i}c\) channels. The problem is that \(k_{i}\) may not be consistent across all \(i\), while dot-product attention operations require that the channel dimensions for each \(W^{(i)}\) match. We solve this problem by choosing a shared channel size \(\tilde{c}\) and using learned linear projections \(\textsc{Proj}_{i}:\mathbb{R}^{n_{i}\times n_{i-1}\times(k_{i}c)}\rightarrow \mathbb{R}^{n_{i}\times n_{i-1}\times\tilde{c}}\) to guarantee matching channel dimensions. The output of \(\textsc{SA}(\cdot)\) can then be restored to the original channel dimension by another set of learned linear projections \(\textsc{UnProj}_{i}:\mathbb{R}^{n_{i}\times n_{i-1}\times\tilde{c}}\rightarrow \mathbb{R}^{n_{i}\times n_{i-1}\times k_{i}\times c}\).

### Building Neural Functional Transformers

Following Transformer architecture design [41], we form stackable "blocks" that combine weight-space self attention with LayerNorm, pointwise MLPs, and residual connections. Each block maps \(\mathcal{W}^{c}\rightarrow\mathcal{W}^{c}\) and preserves \(\mathcal{S}_{\text{NP}}\)-equivariance:

\[\textsc{Block}(W) =Z+\textsc{MLP}(\textsc{LN}(Z))\] (10) \[Z =W+\textsc{SA}(\textsc{LN}(W)),\] (11)

where both the \(\textsc{MLP}:\mathbb{R}^{c}\rightarrow\mathbb{R}^{c}\) and LayerNorm \(\textsc{LN}:\mathbb{R}^{c}\rightarrow\mathbb{R}^{c}\) operate "pointwise," or independently on each input \(W^{(i)}_{jk}\). Since pointwise operations are permutation equivariant, the overall block is an \(\mathcal{S}_{\text{NP}}\)-equivariant map on \(\mathcal{W}^{c}\). To build neural functional Transformers (NFTs), we stack multiple blocks into a deep equivariant architecture. For tasks which require \(\mathcal{S}_{\text{NP}}\)-invariance, like classification or learning an invariant latent space of weights (Section 4), we can apply weight-space cross-attention on top of the features produced by the final \(\textsc{Block}(\cdot)\). The invariant activations produced by \(\textsc{CA}(\cdot)\) can then be fed into an MLP that produces the final output.

Additionally, the NFT's input is often one or a few channels, while our desired hidden channel dimension may be significantly larger (e.g., \(c=256\)). We can apply any function \(g:\mathbb{R}^{c_{1}}\rightarrow\mathbb{R}^{c_{2}}\) to each \(W^{(i)}_{jk}\in\mathbb{R}^{c_{1}}\) to project from the input channel dimension to the hidden channel dimension while preserving equivariance. In our experiments, we use random Fourier features [35] to achieve this projection.

## 4 Inr2Array: Learning an invariant latent space for weights

An application of interest for weight-space architectures is to learn a compact latent representation of weights, which can be useful for downstream tasks such as classifying the signal represented by an implicit neural representation (INR). The recently introduced Inr2vec[6] can learn a representation of INR weights by using a reconstruction objective, but requires every INR share the same initialization. In this section, we leverage neural functionals to extend the inr2vec framework to the independent initialization setting, which is significantly more challenging but removes any restrictions on the INR training process.

Inr2vec uses an encoder-decoder setup to map INR weights into useful latent representations. We make two key changes to the original Inr2Vec formulation:

1. We implement the encoder with an \(\mathcal{S}_{\text{NP}}\)-invariant neural functional transformer (NFT). This guarantees that the latent representation is invariant to neuron permutations of the weights.
2. We allow the latent representation to be a spatially meaningful _array_ of vectors. In particular, each vector in the latent array is responsible for encoding a single spatial patch of the INR.

The first modification is a useful inductive bias because the signal represented by an INR should be invariant to neuron permutations. The second modification is inspired by Spatial Functa [3], which found that giving INR latent spaces spatial structure was helpful in their meta-learning setting. We call our representation learning approach Inr2Array due to its modified latent space structure.

The following explanation focuses on 2D INRs that represent images on the coordinate grid \([-1,1]^{2}\), though the general case follows naturally. We first split the coordinate grid into \(M\) spatial patches \(P_{1}\cup\cdots\cup\hat{P}_{M}=[-1,1]^{2}\). Given a \(\textsc{SIREN}(\cdot;W)\), \(\textsc{Inr2Array}\) uses an \(\mathcal{S}_{\text{NP}}\)-invariant NFT encoder \(\textsc{Enc}_{\theta}(\cdot)\) to map weights \(W\) to a latent array of \(M\) vectors, \(z\in\mathbb{R}^{M\times d}\). The decoder \(\textsc{Dec}_{\theta}:\mathbb{R}^{d}\rightarrow\mathcal{W}\) produces a set of weights \(\left\{\begin{array}{l}\hat{W}_{i}\;\Big{|}\;i=1,\cdots,M\;\right\}\), one for each vector \(z_{i}:=z_{i,:}\in\mathbb{R}^{d}\). Each \(\hat{W}_{i}(z_{i})\) is responsible for parameterizing the SIREN only for the spatial patch \(P_{i}\subset[-1,1]^{2}\). The objective is to reconstruct the original INR's content using the decoded weights:

\[\mathcal{L}(\theta,W)=\sum_{i=1}^{M}\sum_{x\in P_{i}}\left(\text{SIREN }\left(x;\hat{W}_{i}\right)-\text{SIREN }\left(x;W\right)\right)^{2},\] (12) \[\hat{W}_{i}=\text{Dec}_{\theta}(z_{i}),\quad z=\text{Enc}_{\theta }(W).\] (13)

Our \(\text{Inr2Array}\) encoder architecture uses multiple weight-space self-attention layers followed by a weight-space cross-attention layer CA, which is especially well-suited for producing \(M\times d\) permutation invariant arrays. For the decoder, we use the hypernetwork design of Sitzmann et al. [37], where the hypernetworks are conditioned on the spatial latent \(z_{i}\in\mathbb{R}^{d}\). In principle, one can also design an invariant encoder using (non-attentive) NFN layers, and we will compare both options in our experiments.

## 5 Experiments

Our experiments broadly involve two types of weight-space datasets: (1) datasets of neural networks (e.g., CNN image classifiers) trained with varying hyperparameters, where the goal is to model interesting properties of the trained networks (e.g., generalization) from their weights; and (2) datasets containing the weights of INRs each representing a single data signal, such as an image. Tasks including editing INR weights to modify the represented signal, or predicting target information related to the signal (e.g., image class). For our INR experiments, we construct datasets for MNIST [25], FashionMNIST [42], and CIFAR-10 [23] following the procedure of Zhou et al. [46] exactly.

### INR classification with \(\text{Inr2Array}\)

Classifying the signal represented by an INR directly from its weights is a challenging problem, with current approaches requiring that all INRs share initialization [10; 3; 6]. In the "vanilla" setting where INRs can be initialized and trained independently, state-of-the-art methods struggle to classify signals from even simple datasets [31; 46]. We train \(\text{Inr2Array}\) to learn a latent representation of INR weights that can be used for more effective classification in this vanilla setting.

We implement both NFT (weight-space self-attention and cross-attention) and \(\text{NFN}_{\text{NP}}\)[46] variants of the \(\text{Inr2Array}\) encoder, and distinguish the two variants by superscripts: \(\text{Inr2Array}^{\text{NFT}}\) and \(\text{Inr2Array}^{\text{NFN}}\). For the decoder, we use the hypernetwork design of Sitzmann et al. [37], where the hypernetworks are conditioned on the spatial latent \(z_{i}\in\mathbb{R}^{d}\). Appendix B.3 describes the implementation and training of each variant in full detail.

We train separate encoders for each INR dataset shown in Table 6 of the appendix, which reports the reconstruction mean square error (Eq 12) on test INR inputs. The NFT encoder consistently achieves lower reconstruction error than the NFN encoder, and \(\text{Inr2Array}^{\text{NFT}}\) also produces qualitatively better reconstructions than \(\text{Inr2Array}^{\text{NFN}}\) (Figure 4), confirming that NFTs enable higher quality encoding than previous neural functionals.

Once trained, the \(\text{Inr2Array}\) encoder maps INR weights to compact latent arrays that represent the INR's contents. Downstream tasks such as classifying the INR's signal (e.g., image) can now be performed directly on these \(\mathcal{S}_{\text{NP}}\)-invariant latent arrays \(z\in\mathbb{R}^{M\times d}\). Concretely, in \(K\)-way

Figure 4: \(\text{Inr2Array}\) reconstructions for random samples from each INR dataset. The NFT encoder produces higher qualitatively better reconstructed SIRENs on each dataset, especially CIFAR-10. Blocking artifacts are due to the spatial latent structure, and suggest further room for improvement.

classification the trained encoder Enc : \(\mathcal{W}\rightarrow\mathbb{R}^{M\times d}\) can be composed with any classification head \(f:\mathbb{R}^{M\times d}\rightarrow\mathbb{R}^{K}\) to form an \(\mathcal{S}_{\text{NP}}\)-invariant classifier \(f\circ\text{Enc}:\mathcal{W}\rightarrow\mathbb{R}^{K}\). Since the encoder is already trained by the reconstruction objective, we only train \(f\) during classification and keep the encoder fixed. We choose to implement \(f\) using a Transformer classification head [41], which views the latent array input as a length-\(M\) sequence of \(d\)-dimensional vectors.

Existing INR classifiers, which we refer to as DWS [31] and NFN [46], are permutation-invariant architectures that directly map input weights to predicted labels. We also create a modified NFN classifier (Spatial NFN) which produces an \(M\times d\) array of activations before a convolutional classifier head, to test the impact of spatial latents independent of the Inn2Array training process. Appendix B.4 describes the setup for each method in further detail. Finally, we measure the performance of NFT classifiers without Inr2Array.

Table 1 shows that Inn2Array significantly improves classification test accuracies across the board. In addition, the best performance is achieved by implementing the encoder using our attention-based layers (NFT) compared to linear weight-space layers (NFN). Notably, Inn2Array\({}^{\text{NFT}}\) achieves a test accuracy of \(64.4\%\) on CIFAR-10 (the hardest dataset), an improvement of \(+17\%\) over the previous best result of \(46.6\%\) by NFNs. They also achieve an MNIST test accuracy of \(98.5\%\), up from \(92.9\%\) by NFNs and \(85.7\%\) by DWS. The vanilla NFT performance is comparable to DWS and NFNs (better than the former and worse than the latter), suggesting the importance of Inr2Array in addition to the architecture. Finally, the Spatial NFN fails to improve performance over standard NFN classifiers, implying that the particular objective of Inr2Array is crucial to make spatial latent spaces useful. The results show that learning a latent array representation of INR weights using Inr2Array and NFTs can greatly improve performance on downstream INR tasks like classification.

#### 5.1.1 Term-by-term ablation

Here we investigate the importance of each term in our weight space self-attention layer (Eqs. 4-6). The first two terms of SA, illustrated by Figure 2, amount to self-attention between the rows and columns of _adjacent_ weights, while the third term is a _global_ self-attention between features from all layers. Note that in practice we use a tractable approximation of the third term described in Sec 3.1. For this ablation experiment we either keep only the first two terms (**AdjacentSA**) or only the third term (**GlobalSA**). **FullSA** denotes the original layer.

Table 2 shows the results of this ablation experiment on Inr2Array\({}^{\text{NFT}}\) performance in the MNIST classification task. We see that our full layer (FullSA) performs best, ablating the final term (AdjacentSA) only slightly degrades performance, and ablating the first two terms (GlobalSA) drastically harms performance. This is interesting since the first two terms are necessary to achieve _minimal equivariance_, while the third term alone is not minimally

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & MNIST & FashionMNIST & CIFAR-10 \\ \hline Inr2Vec [6] & \(19.1\pm 0.18\) & \(23.3\pm 0.36\) & \(16.7\pm 0.24\) \\ DWS [31] & \(74.4\pm 0.14\) & \(64.8\pm 0.69\) & \(41.5\pm 0.43\) \\ NFN [46] & \(92.9\pm 0.38\) & \(75.6\pm 1.07\) & \(46.6\pm 0.13\) \\ Spatial NFN & \(92.9\pm 0.46\) & \(70.8\pm 0.53\) & \(45.6\pm 0.11\) \\ NFT & \(89.9\pm 1.03\) & \(72.7\pm 0.05\) & \(44.8\pm 0.32\) \\ \hline Inr2Array\({}^{\text{NFN}}\) (Ours) & \(94.6\pm 0.00\) & \(76.7\pm 0.00\) & \(45.4\pm 0.00\) \\ Inr2Array\({}^{\text{NFT}}\) (Ours) & \(\mathbf{98.5\pm 0.00}\) & \(\mathbf{79.3\pm 0.00}\) & \(\mathbf{63.4\pm 0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test accuracy (%) for weight-space INR classification in the MNIST, FashionMNIST, and CIFAR-10 datasets. Inr2Array significantly improves over current state-of-the-art results in this setting. For the NFN baseline [46] we report the higher performance out of their NP and HNP variants. Note that the MNIST and FashionMNIST results reported for DWS in Navon et al. [31] are on their own independently constructed INR datasets, while we use the datasets from Zhou et al. [46] for all methods for consistent comparison. Uncertainties indicate standard error over three runs.

\begin{table}
\begin{tabular}{|l|c|} \hline Model & MNIST \\ \hline FullSA & 98.5 \\ AdjacentSA & 97.6 \\ GlobalSA & 37.4 \\ \hline \end{tabular}
\end{table}
Table 2: Test classification accuracy of Inr2Array\({}^{\text{NFT}}\) when ablating the terms of SA (Eqs.4-6).

equivariant (but helps propagate information between any two weight matrices). The results emphasize the importance of designing our layer to achieve minimal equivariance rather than naively applying self-attention to the weights.

### Editing INRs

We also evaluate NFTs on editing INR weights to alter their signal, e.g., to modify the represented image. The goal of this task is to edit the weights of a trained SIREN to alter its encoded image, expressed as a difference \(W^{\prime}\gets W+\Delta(W)\). Neural functionals are trained to learn \(\Delta(\cdot)\) that achieves some desired visual transformation, such as dilating the image. Permutation equivariance is a useful inductive bias here since if \(\Delta(W)\) is the desired edit for \(W\), then for any neuron permutation \(\sigma\) the desired edit to \(\sigma W\) is \(\sigma\Delta(W)\). In addition to the MNIST dilation and CIFAR contrast tasks from Zhou et al. [46], we also introduce several new editing tasks: MNIST erosion, MNIST gradient, and FashionMNIST gradient. Gradient tasks roughly amount to edge detection; Figure 5 in the appendix shows sample inputs and targets for each editing task.

We compare NFT editing performance against the two NFN variants [46], with each method using the a similar number of parameters (\(\sim 7\)M). For full training details, see Appendix B.2). Table 3 shows the test mean square error (MSE) between the edited INR and the ground truth visual transformation. NFTs consistently outperform NFNs on most INR editing tasks. In addition, Table 5 in the appendix shows that NFTs generally obtain both lower training and test error, indicating that the attention-based architecture enables greater expressivity. Figure 5 shows qualitative samples produced by each editing method on each task.

### Predicting CNN classifier generalization

Whereas the previous experiments have focused on the weight spaces of INRs (which are implemented by small MLPs), we would also like to evaluate how NFTs process other weight spaces such as those belonging to convolutional neural network classifiers. Large-scale empirical studies have produced datasets of trained classifiers under different hyperparameter settings [40; 11], enabling a data-driven approach to modeling generalization from their weights, which could lead to new insights. Prior methods for predicting classifier generalization typically rely on extracting hand-designed features from the weights before using them to predict the test accuracy [18; 43; 40; 19; 28].

We now study how NFTs can model generalization from raw weights using the Small CNN Zoo [40], which contains thousands of CNNs trained on image classification datasets. In addition to comparing against the two neural functional variants \(\text{NFN}_{\text{NP}}\) and \(\text{NFN}_{\text{HNP}}\)[46], we also show the performance of

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\text{NFN}_{\text{HNP}}\)[46] & \(\text{NFN}_{\text{NP}}\)[46] & StatNN [40] & NFT (Ours) \\ \hline CIFAR-10-GS & \(\mathbf{0.934\pm 0.001}\) & \(0.922\pm 0.001\) & \(0.915\pm 0.002\) & \(0.926\pm 0.001\) \\ SVHN-GS & \(\mathbf{0.931\pm 0.005}\) & \(0.856\pm 0.001\) & \(0.843\pm 0.000\) & \(0.858\pm 0.000\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: (Small CNN Zoo [40] benchmark.) Rank correlation \(\tau\) for predicting the generalization of CNN image classifiers with unseen hyperparameters trained CIFAR-10-GS and SVHN-GS (GS=grayscale). The NFT outperforms \(\text{NFN}_{\text{NP}}\) and hand-picked features (StatNN), while \(\text{NFN}_{\text{HNP}}\) performs best overall. Uncertainties indicate standard error over two runs.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\text{NFN}_{\text{HNP}}\)[46] & \(\text{NFN}_{\text{NP}}\)[46] & NFT (Ours) \\ \hline MNIST (erode) & \(0.0228\pm 0.0003\) & \(0.0223\pm 0.0000\) & \(\mathbf{0.0194\pm 0.0002}\) \\ MNIST (dilate) & \(0.0706\pm 0.0005\) & \(0.0693\pm 0.0009\) & \(\mathbf{0.0510\pm 0.0004}\) \\ MNIST (gradient) & \(0.0607\pm 0.0013\) & \(0.0566\pm 0.0000\) & \(\mathbf{0.0484\pm 0.0007}\) \\ \hline FashionMNIST (gradient) & \(0.0878\pm 0.0002\) & \(0.0870\pm 0.0001\) & \(\mathbf{0.0800\pm 0.0002}\) \\ \hline CIFAR (contrast) & \(0.0204\pm 0.0000\) & \(0.0203\pm 0.0000\) & \(0.0200\pm 0.0002\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test mean squared error to the target visual transformation for five INR editing tasks. NFTs generally achieve lower test error than NFN variants. Uncertainties indicate standard error over three seeds.

a hand-designed features approach called StatNN [40]. Each generalization predictor is trained on thousands of CNN weights produced under varying initialization and optimization hyperparameters, and is then tested on held out weights produced using unseen hyperparameters. Appendix B.1 describes the experimental setup in full detail.

Table 4 shows the test performance of each method using the Kendall rank correlation coefficient \(\tau\)[20]. Across both datasets, neural functionals operating on raw weights outperform the hand-designed features baseline StatNN. NFTs slightly outperform NFNNP on each dataset, while NFNHNP achieves the best performance overall. Note that both NFNNP and NFTs use layers that assume input and output neurons are permutable (NP). Although we use input/output position encoding to remove the stronger NP assumptions, the results suggest that HNP designs may be naturally better suited to this task.

## 6 Related Work

It is well known that the weight spaces of neural networks contain numerous symmetries, i.e., transformations that preserve the network's behavior [17]. Permutation symmetries in particular have been studied in the context of neural network loss landscapes [14; 4; 12] and weight-space merging [38; 1]. However, most prior methods for processing weight space objects do not explicitly account for these symmetries [2; 26; 15; 24; 45; 7; 21], although some have tried to encourage permutation equivariance through data augmentation [33; 29]. Another workaround approach is to explicitly restrict the weight space being considered by, for example, fixing the initialization of all networks being processed [6] or meta-learning modulations of a set of base parameters [10; 3]. Instead, we study the problem of encoding permutation symmetries into the neural functional itself, without any restrictions on the weight space being processed.

There are a variety of methods that incorporate symmetries into deep neural network architectures [36; 22; 13]. Examples include (group) convolutions for images [25; 5] and permutation equivariant architectures for general set-structured inputs [34; 44; 16; 39; 27]. Most directly related to our work are that of Navon et al. [31] and Zhou et al. [46], who introduce linear layers equivariant to the neuron permutation symmetries of feedforward networks. We extend their approach by introducing nonlinear equivariant layers based on the attention mechanism, and use them to construct NFTs.

## 7 Conclusion

This work introduces neural functional transformers, a novel class of weight-space models designed with neuron permutation symmetries in mind. Our approach extends recent work on permutation-equivariant weight-space models using nonlinear self-attention and cross-attention layers, which can enable greater expressivity compared to existing linear layers.

We empirically evaluate the effectiveness of NFTs on weight-space tasks involving datasets of trained CNN classifiers and implicit neural representations (INRs). Operating on weights alone, NFTs can predict the test accuracy of CNN classifiers, modify the content of INRs, and classify INR signals. We also use NFTs to develop Inr2Array, a method for mapping INR weights to compact and \(\mathcal{S}_{\text{NP}}\)-invariant latent representations, which significantly improve performance in downstream tasks such as INR classification.

Some limitations of NFTs include increased computational costs from self-attention relative to linear weight-space layers, and the difficulty of training large NFT architectures stably. Future work may address these limitations, and explore additional applications of NFTs such as for learned optimization or weight-space generative modeling. Overall, our work highlights the potential of attention-based weight-space layers offers a promising direction for the development of more expressive and powerful neural functional architectures.

## Acknowledgments and Disclosure of Funding

We thank Adriano Cardace and Yoonho Lee for interesting discussions and suggestions related to the development of this paper. AZ and KB are supported by the NSF Graduate Research Fellowship Program. This project was supported by Juniper Networks and ONR grant N00014-22-1-2621.

## References

* [1] S. K. Ainsworth, J. Hayase, and S. Srinivasa. Git re-basin: Merging models modulo permutation symmetries. _arXiv preprint arXiv:2209.04836_, 2022.
* [2] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learning to learn by gradient descent by gradient descent. _Advances in neural information processing systems_, 29, 2016.
* [3] M. Bauer, E. Dupont, A. Brock, D. Rosenbaum, J. Schwarz, and H. Kim. Spatial functa: Scaling functa to imagenet classification and generation. _arXiv preprint arXiv:2302.03130_, 2023.
* [4] J. Brea, B. Simsek, B. Illing, and W. Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. _arXiv preprint arXiv:1907.02911_, 2019.
* [5] T. Cohen and M. Welling. Group equivariant convolutional networks. In _International conference on machine learning_, pages 2990-2999. PMLR, 2016.
* [6] L. De Luigi, A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, and L. Di Stefano. Deep learning on implicit neural representations of shapes. _arXiv preprint arXiv:2302.05438_, 2023.
* [7] L. Deutsch, E. Nijkamp, and Y. Yang. A generative model for sampling high-performance and diverse weights for neural networks. _arXiv preprint arXiv:1905.02898_, 2019.
* [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [9] E. Dupont, Y. W. Teh, and A. Doucet. Generative models as distributions of functions. _arXiv preprint arXiv:2102.04776_, 2021.
* [10] E. Dupont, H. Kim, S. Eslami, D. Rezende, and D. Rosenbaum. From data to functa: Your data point is a function and you should treat it like one. _arXiv preprint arXiv:2201.12204_, 2022.
* [11] G. Eilertsen, D. Jonsson, T. Ropinski, J. Unger, and A. Ynnerman. Classifying the classifier: dissecting the weight space of neural networks. _arXiv preprint arXiv:2002.05688_, 2020.
* [12] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _arXiv preprint arXiv:2110.06296_, 2021.
* [13] M. Finzi, M. Welling, and A. G. Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In _International Conference on Machine Learning_, pages 3318-3328. PMLR, 2021.
* [14] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of DNNs. _Advances in neural information processing systems_, 31, 2018.
* [15] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. _arXiv preprint arXiv:1609.09106_, 2016.
* [16] J. Hartford, D. Graham, K. Leyton-Brown, and S. Ravanbakhsh. Deep models of interactions across sets. In _International Conference on Machine Learning_, pages 1909-1918. PMLR, 2018.
* [17] R. Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In _Advanced Neural Computers_, pages 129-135. Elsevier, 1990.
* [18] Y. Jiang, D. Krishnan, H. Mobahi, and S. Bengio. Predicting the generalization gap in deep networks with margin distributions. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJlQfnCqKX.
* [19] Y. Jiang, P. Natekar, M. Sharma, S. K. Aithal, D. Kashyap, N. Subramanyam, C. Lassance, D. M. Roy, G. K. Dziugaite, S. Gunasekar, et al. Methods and analysis of the first competition in predicting generalization of deep learning. In _NeurIPS 2020 Competition and Demonstration Track_, pages 170-190. PMLR, 2021.

* [20] M. G. Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938.
* [21] B. Knyazev, M. Drozdzal, G. W. Taylor, and A. Romero Soriano. Parameter prediction for unseen deep architectures. _Advances in Neural Information Processing Systems_, 34:29433-29448, 2021.
* [22] R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In _International Conference on Machine Learning_, pages 2747-2755. PMLR, 2018.
* [23] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [24] D. Krueger, C.-W. Huang, R. Islam, R. Turner, A. Lacoste, and A. Courville. Bayesian hypernetworks. _arXiv preprint arXiv:1710.04759_, 2017.
* [25] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. _The handbook of brain theory and neural networks_, 3361(10):1995, 1995.
* [26] K. Li and J. Malik. Learning to optimize. _arXiv preprint arXiv:1606.01885_, 2016.
* [27] H. Maron, O. Litany, G. Chechik, and E. Fetaya. On learning sets of symmetric elements. In _International conference on machine learning_, pages 6734-6744. PMLR, 2020.
* [28] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. _The Journal of Machine Learning Research_, 22(1):7479-7551, 2021.
* [29] L. Metz, J. Harrison, C. D. Freeman, A. Merchant, L. Beyer, J. Bradbury, N. Agrawal, B. Poole, I. Mordatch, A. Roberts, et al. Velo: Training versatile learned optimizers by scaling up. _arXiv preprint arXiv:2211.09760_, 2022.
* [30] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: representing scenes as neural radiance fields for view synthesis (2020). _arXiv preprint arXiv:2003.08934_, 2020.
* [31] A. Navon, A. Shamsian, I. Achituve, E. Fetaya, G. Chechik, and H. Maron. Equivariant architectures for learning in deep weight spaces. _arXiv preprint arXiv:2301.12780_, 2023.
* [32] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* [33] W. Peebles, I. Radosavovic, T. Brooks, A. A. Efros, and J. Malik. Learning to learn with generative models of neural network checkpoints. _arXiv preprint arXiv:2209.12892_, 2022.
* [34] C. Qi, H. Su, K. Mo, and L. Guibas. Pointnet: deep learning on point sets for 3d classification and segmentation. cvpr (2017). _arXiv preprint arXiv:1612.00593_, 2016.
* [35] A. Rahimi and B. Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* [36] S. Ravanbakhsh, J. Schneider, and B. Poczos. Equivariance through parameter-sharing. In _International conference on machine learning_, pages 2892-2901. PMLR, 2017.
* [37] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. _Advances in Neural Information Processing Systems_, 33:7462-7473, 2020.
* [38] N. Tatro, P.-Y. Chen, P. Das, I. Melnyk, P. Sattigeri, and R. Lai. Optimizing mode connectivity via neuron alignment. _Advances in Neural Information Processing Systems_, 33:15300-15311, 2020.
* [39] E. H. Thiede, T. S. Hy, and R. Kondor. The general theory of permutation equivarant neural networks and higher order graph variational encoders. _arXiv preprint arXiv:2004.03990_, 2020.

* [40] T. Unterthiner, D. Keysers, S. Gelly, O. Bousquet, and I. Tolstikhin. Predicting neural network accuracy from weights. _arXiv preprint arXiv:2002.11448_, 2020.
* [41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [42] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
* [43] S. Yak, J. Gonzalvo, and H. Mazzawi. Towards task and architecture-independent generalization gap predictors. _arXiv preprint arXiv:1906.01550_, 2019.
* [44] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola. Deep sets. doi: 10.48550. _arXiv preprint ARXIV.1703.06114_, 2017.
* [45] C. Zhang, M. Ren, and R. Urtasun. Graph hypernetworks for neural architecture search. _arXiv preprint arXiv:1810.05749_, 2018.
* [46] A. Zhou, K. Yang, K. Burns, Y. Jiang, S. Sokota, J. Z. Kolter, and C. Finn. Permutation equivariant neural functionals. _arXiv preprint arXiv:2302.14040_, 2023.

Weight-space self-attention

### Proof of minimal equivariance

We repeat the claim about minimal equivariance for weight-space self-attention and layer position encodings \(\textsc{SA}\circ\textsc{LayerEnc}:\mathcal{W}^{c}\rightarrow\mathcal{W}^{c}\):

**Theorem 3.1** (Minimal equivariance).: _The combined layer \(\textsc{SA}\circ\textsc{LayerEnc}\) is **minimally \(\mathcal{S}_{\text{NP}}\)-equivariant._

Proof.: We first show \(\mathcal{S}_{\text{NP}}\)-equivariance, before checking that it minimally equivariant.

**Equivariance**. It is straightforward to check that \(\textsc{LayerEnc}\) is equivariant to any permutation that preserves the layer index (which neuron permutations do). At a high level, we check that \(\textsc{SA}\) is \(\mathcal{S}_{\text{NP}}\)-equivariant by verifying that \(\textsc{SA}(\sigma W)^{(i)}_{jk}=\textsc{SA}(W)^{(i)}_{\sigma^{-1}_{i}(j), \sigma^{-1}_{i-1}(k)}\). We proceed term by term.

First, it will be useful to show that \(\textsc{Attn}\) (Eq. 3) is equivariant to permutations \(\pi\in S_{d}\) of each key, query, and value input. In particular, let \([\pi q][i]=q[\pi^{-1}(i)]\) permute the entries of the input vectors:

\[\textsc{Attn}\left(\pi q,\{\pi k_{p},\pi v_{p}\}_{p=1}^{N}\right)[i] =\left[\sum_{p}\left(\frac{\exp(q\cdot k_{p})}{\sum_{p^{\prime}} \exp(q\cdot k_{p^{\prime}})}\right)(\pi v_{p})\right][i]\] (14) \[=\sum_{p}\left(\frac{\exp(q\cdot k_{p})}{\sum_{p^{\prime}}\exp(q \cdot k_{p^{\prime}})}\right)v_{p}[\pi^{-1}(i)]\] (15) \[=\textsc{Attn}(q,\{k_{p},v_{p}\}_{p=1}^{N})[\pi^{-1}(i)]\] (16)

Note that in the first equality we use \(\pi q\cdot\pi k=q\cdot k\).

Since the key, query, and value projections are only linear projections along the channel dimension, \(K(\sigma W)=\sigma K(W)\), where \(K\) denotes all keys produced by linearly projection of \(W\) by \(\theta_{K}\) (and similarly for \(Q,V\)). Then consider, for example, how the first term (Eq 4) changes under \(W\mapsto\sigma W\):

\[\textsc{Attn}\left(\sigma Q^{(i)}_{j,:},\left\{(\sigma K,\sigma V )^{(i-1)}_{:,q}\right\}_{q}\bigcup\left\{(\sigma K,\sigma V)^{(i)}_{p:}\right\} _{p}\right)[k]=\] (17) \[\textsc{Attn}\left(\sigma_{i-1}Q_{\sigma^{-1}_{i}(j),:},\left\{( \sigma_{i-1}K^{(i-1)}_{:,q},\sigma_{i-1}V^{(i-1)}_{:,q})\right\}_{q}\bigcup \left\{(\sigma_{i-1}K^{(i)}_{p:},\sigma_{i-1}V^{(i)}_{p:})\right\}_{p}\right)[k]\] (18)

Note the distinction between neuron permutations of the weight space \([\sigma Q]\) and permutations of individual vectors like \(\sigma_{i-1}Q_{p:}\), and that permutations along the KV set indices (e.g., \(q\) in the first set) are ignored since set order is irrelevant. Using the equivariance of \(\textsc{Attn}\) (Eqs. 14-16):

\[\textsc{Attn}\left(Q^{(i)}_{\sigma^{-1}_{i}(j),:},\left\{(K,V)^{(i-1)}_{:,q} \right\}_{q}\bigcup\left\{(K,V)^{(i)}_{p:}\right\}_{p}\right)[\sigma^{-1}_{i- 1}(k)]\] (19)

Notice that compared to Eq. 4 the output indices have changed \(j\mapsto\sigma^{-1}_{i}(j)\) and \(k\mapsto\sigma^{-1}_{i-1}(k)\), showing that the first term is equivariant. We can repeat this argument in a similar fashion for the second term (Eq. 5), while the third term (Eq. 6) applies \(\textsc{Attn}\) by treating each weight as an individual token, so its equivariance follows immediately from the permutation equivariance of \(\textsc{Attn}\). This verifies that all terms of \(\textsc{SA}\) are \(\mathcal{S}_{\text{NP}}\)-equivariant.

**Minimal equivariance**. Here our goal is to show _non_-equivariance to permutations \(\tau\in S_{\text{dim}(\mathcal{W})}\) but \(\tau\notin\mathcal{S}_{\text{NP}}\) (false symmetries). That is, we should find some input \(W\) such that \(\textsc{SA}(\textsc{LayerEnc}(\tau W))\neq\tau\textsc{SA}(\textsc{Layer Enc}(W))\), for some parameters \(\{\,\theta_{Q},\theta_{K},\theta_{V}\,\}\) and \(\{\,\phi^{(i)}\,\}\).

It will be useful to define the actions of permutations in \(S_{\text{dim}(\mathcal{W})}\) on the indices of the weights, rather than the weights themselves. The index-space for a weight space \(\mathcal{W}\) is defined as a set of 3-tuples representing layer number, row, and column, respectively:

\[\mathbb{I}=\{\,(i,j,k)\mid i=\llbracket 1..L\rrbracket,j=\llbracket 1..n_{i} \rrbracket;k=\llbracket 1..n_{i-1}\rrbracket\,\}\,.\] (20)For \(\alpha\in\mathbb{I}\), we use the subscripts \(\ell,r,c\) to denote the layer, row, and column indices respectively. That is, \(\alpha=(\alpha_{\ell},\alpha_{r},\alpha_{c})=(i,j,k)\).

Notice that the set of arbitrary permutations \(S_{\dim(\mathcal{W})}\) is simply the set of all bijections \(\tau:\mathbb{I}\to\mathbb{I}\) (any index can be arbitrarily permuted to any other index), while \(\mathcal{S}_{\text{NP}}\) is the subgroup of bijections \(\sigma\) that can be written as:

\[\sigma(i,j,k)=(i,\sigma_{i}(j),\sigma_{i-1}(k)),\quad\forall(i,j,k)\in\mathbb{ I}.\] (21)

False symmetries can broadly be categorized into three groups. In fact, we can verify that any permutation \(\tau\) that fails to satisfy one of these three cases must in \(\mathcal{S}_{\text{NP}}\):

1. **False symmetry that permutes weights across layers.** There exists an \((i,j,k)\in\mathbb{I}\) such that \(\tau(i,j,k)_{\ell}\neq i\).
2. **False symmetry where row and column permutations are not independent.** Suppose a false symmetry \(\tau\) does not fall under case (1), i.e., it preserves the layer index. Then it may still fail to have independent permutations of row and columns. In this case, either there is an \((i,j,k)\) and \(q\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\), but \(\tau(i,j,q)_{r}\neq j^{\prime}\). Or there is an \((i,j,k)\) and \(p\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\) but \(\tau(i,p,k)_{c}\neq k^{\prime}\).
3. **Adjacent permutations decoupled.** If a false symmetry \(\tau\) is not one of the two cases above, there must be an \((i,j,k)\in\mathbb{I}\) and \(q\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\) but \(\tau(i-1,k,q)_{r}\neq k^{\prime}\). That is, an instance where one row pair of layer \(i-1\) do not permute the same way as the columns of layer \(i\).

We can now check non-equivariance for \(\tau\) in each case. To simplify the notation here we will assume \(c=1\) although the general case is similar:

1. If \(\tau\) does not preserve layer index, LayerEnc will be non-equivariant since each layer has a different encoding.
2. Suppose there is an \((i,j,k)\) and \(q\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\), but \(\tau(i,j,q)_{r}\neq j^{\prime}\), and let \(\theta_{Q}=\theta_{K}=\theta_{V}=I\). Now consider the input \(W\in\mathcal{W}\) such that all weights are \(0\) except for \(W^{(i)}_{jk}=W^{(i)}_{jq}=1\). Then we can check that \([\tau\text{SA}(W)]_{i,j^{\prime},k^{\prime}}\neq\text{SA}(\tau W)_{i,j^{ \prime},k^{\prime}}\), so the layer is not equivariant to \(\tau\). We can construct a similar argument for the other case, where there is an \((i,j,k)\) and \(p\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\) but \(\tau(i,p,k)_{c}\neq k^{\prime}\).
3. In this case there must be an \((i,j,k)\in\mathbb{I}\) and \(q\) such that \(\tau(i,j,k)=(i,j^{\prime},k^{\prime})\) but \(\tau(i-1,k,q)_{r}\neq k^{\prime}\). Again let \(\theta_{Q}=\theta_{K}=\theta_{V}=I\) and consider \(W\in\mathcal{W}\) such that all weights are \(0\) except for \(W^{(i)}_{jk}=W^{(i-1)}_{kq}=1\). Then, similar to the above case, we can verify that \([\tau\text{SA}(W)]_{i,j^{\prime},k^{\prime}}\neq\text{SA}(\tau W)_{i,j^{\prime },k^{\prime}}\), so the layer is not equivariant to such \(\tau\).

Taken together, the three cases above show that although SA \(\circ\) LayerEnc is \(\mathcal{S}_{\text{NP}}\)-equivariant, it is not equivariant to any other permutations \(\tau\notin\mathcal{S}_{\text{NP}}\). Hence the layer is _minimally_ equivariant. 

### Full description including biases

Let \(\mathcal{B}\) be the space of biases, and \(\mathcal{U}=\mathcal{W}\times\mathcal{B}\) the full space of both weights and biases. Then the full weight-space self-attention layer \(\text{SA}:\mathcal{U}^{c}\to\mathcal{U}^{c}\) is defined, for input \(U=(W,b)\):

\[\text{SA}(U;\theta_{Q},\theta_{K},\theta_{V})=\left(Y(U),z(U)\right),\] (22)where \(Y(U)\in\mathcal{W}^{c}\) and \(z(U)\in\mathcal{B}^{c}\), with:

\[Y(W,b)_{jk}^{(i)} =\quad\texttt{Attn}\left(Q_{j,:}^{(i)},KV_{1}\right)_{k}+\texttt{ Attn}\left(Q_{:,k}^{(i)},KV_{2}\right)_{j}+\texttt{Attn}\left(Q_{jk}^{(i)},KV_{3} \right),\] (23) \[z(W,b)_{j}^{(i)} =\quad\texttt{Attn}\left(q^{(i)},KV_{2}\right)_{j}+\texttt{Attn} \left(q_{j}^{(i)},KV_{3}\right),\text{ where}\] (24) \[KV_{1} =\] (25) \[KV_{2} =\] (26) \[KV_{3} = \left\{\left.\left(K,V\right)_{pq}^{(i)}\,\right|\,\forall s,p,q \,\right\}\bigcup\left\{\left.\left(k,v\right)_{p}^{(s)}\,\right|\,\forall s,p \,\right\},\text{ and}\] (27) \[Q_{j,:}^{(i)} :=\theta_{Q}W_{jk}^{(i)},\quad K_{j,k}^{(i)}:=\theta_{K}W_{jk}^{ (i)},\quad V_{j,k}^{(i)}:=\theta_{V}W_{jk}^{(i)},\] (28) \[q_{j}^{(i)} :=\theta_{Q}b_{j}^{(i)},\quad k_{j}^{(i)}:=\theta_{K}b_{j}^{(i)}, \quad v_{j}^{(i)}:=\theta_{V}b_{j}^{(i)}.\] (29)

We use \(KV_{1},\,KV_{2},KV_{3}\) to denote the three sets of key-value pairs involved in the layer computation and distinguish the linear projections of weights and biases using uppercase (\(Q,K,V\)) and lowercase (\(q,k,v\)), respectively.

## Appendix B Experiment details

Recall from Section 3.4 that NFTs consist of "blocks" that include weight-space self attention, layer normalization, and feedforward MLPs. Additionally, the first layer is typically a projection from the input channel dimensionality to the hidden channel dimensionality, for which we use random Fourier features.

Here we summarize the hyperparameters (and other training information) for the NFTs in each experiment in this paper. NFTs are described by the following hyperparameters:

* # blocks: Number of equivariant weight-space blocks \(\texttt{Block}(\cdot)\)
* # channels: channel dimension \(c\) used by the blocks.
* MLP hidden dim: the hidden layer size of the feedforward MLPs within each block
* Fourier scale: standard deviation of random vectors used to compute random Fourier features
* Fourier size: number of random fourier features
* # attn heads: Number of attention heads used by weight-space self-attention.
* dropout p: Dropout probability for attention matrix and in feedforward networks.
* invariant layer (for \(\mathcal{S}_{\text{NP}}\)-invariant NFTs): Whether the invariant pooling is done with summation (HNP/NP [46]) or cross-attention (ours).
* CA \(M\): If using cross attention, the number of learned query embeddings \(e_{1},\cdots,e_{M}\).
* CA dim: Dimension of vectors going into cross attention.

### Generalization prediction

Each Small CNN Zoo dataset that we consider (CIFAR-10-GS and SVHN-GS) contains the weights of \(30000\) CNNs trained with varying hyperparameters. The weights are split into \(15000\) for training and validation and the rest for testing. We use \(20\%\) of the non-test data for validation, and train on the remaining \(12000\). We use the validation data to sweep over Fourier scales of \(3\) or \(10\) and try both HNP (summation) or CA (cross attention) layers to achieve invariance.

### Style editing

We implement four visual transformations: erode reduces the size of objects in an image by removing pixels on the boundary; dilate adds pixels to object boundaries and can be thought of as the opposite of erode; morphological gradient performs edge detection by computing the difference between dilation and erosion; and contrast magnifies color differences in an image, making objects more distinguishable. Figure 5 shows visual samples of the input and target for each task.

### Inr2Array

We split the SIRENs of each dataset into training, validation, and testing sets, and train the encoder and decoder using only the training set. We use the validation error to sweep over the following hyperparameters (for both the NFT and NFN variants): # blocks (\(4\) vs \(6\)), # channels (\(256\) vs \(512\)), MLP hidden dim (\(512\) vs \(1024\)), Fourier scale (\(3\) vs \(10\)), # attn heads (\(4\) vs \(8\)), and dropout (\(0.1\) vs \(0\)). After sweeping these hyperparameters, we use the best hyperparameter configuration to train the encoder and decoder and do early stopping with the validation error. The encoder portion of the best checkpoint can then be used to produce latent array representations of INR inputs.

Figure 5: Samples of target image transformations, as well as original and edited INRs for each editing task in Table 3. Despite the fact that the NFT quantitatively outperforms the \(\text{NFN}_{\text{HNP}}\) and \(\text{NFN}_{\text{NP}}\), compelling qualitative differences in the edited images can be difficult to identify due to small image size.

### INR Classification

After training Inr2Array on a given dataset, we select the checkpoint with the smallest reconstruction error on the validation set, and use the encoder to produce a \(16\times 256\) latent array representation for any input SIREN. We view the latent as a length-\(16\) sequence of \(256\)-dim vectors, and project each vector into \(\mathbb{R}^{512}\) before feeding the array into Transformer encoder with \(12\) blocks. This produces an output \(z^{o}\in\mathbb{R}^{16\times 512}\); we take the first output \(z_{0}^{o}\in\mathbb{R}^{512}\) and feed it into a 2-layer MLP with \(512\) hidden units that produces the classification logits. We train with cross-entropy loss for \(100\) epochs, using the AdamW optimizer and mixup augmentation. Using the validation accuracy, we sweep over weight decay (\(0.1\) vs \(0.01\)) and mixup alpha (\(0\) vs \(0.2\) vs \(1\)) and perform early stopping.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Inr2ArrayNFN & Inr2ArrayNF \\ \hline MNIST & \(0.046\pm 0.001\) & \(\mathbf{0.027\pm 0.003}\) \\ FashionMNIST & \(0.085\pm 0.006\) & \(\mathbf{0.070\pm 0.006}\) \\ CIFAR-10 & \(0.117\pm 0.008\) & \(\mathbf{0.036\pm 0.006}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Mean squared error (MSE) of Inr2Array reconstructions on test INR weights. Using an NFT encoder in Inr2Array achieves better reconstruction error, produces higher quality samples, and leads to better downstream performance.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{FashionMNIST} & \multicolumn{2}{c}{CIFAR-10} \\ \hline  & Train & Test & Train & Test & Train & Test \\ DWS [31] & – & \(85.7\pm 0.57\) & – & \(65.5\pm 0.48\) & – & – \\ NFN [46] & \(96.2\pm 0.24\) & \(92.9\pm 0.38\) & \(84.1\pm 1.02\) & \(75.6\pm 1.07\) & \(61.0\pm 6.60\) & \(46.6\pm 0.13\) \\ Spatial NFN (Ours) & \(95.3\pm 0.59\) & \(92.9\pm 0.46\) & \(76.4\pm 0.87\) & \(70.8\pm 0.53\) & \(52.3\pm 0.30\) & \(45.6\pm 0.11\) \\ Inr2ArrayNFN (Ours) & \(100\pm 0.00\) & \(94.6\pm 0.00\) & \(92.8\pm 0.00\) & \(76.7\pm 0.00\) & \(98.5\pm 0.00\) & \(45.4\pm 0.00\) \\ Inr2ArrayNFN (Ours) & \(100\pm 0.00\) & \(\mathbf{98.5\pm 0.00}\) & \(97.6\pm 0.00\) & \(\mathbf{79.3\pm 0.00}\) & \(100\pm 0.00\) & \(\mathbf{63.4\pm 0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Classification train and test accuracies (%) on datasets of image INRs (MNIST, FashionMNIST, and CIFAR-10). Inr2Array outperforms prior state-of-the-art results in this setting (DWS and NFN). For the NFN baseline [46] we report the higher performance out of their NP and HNP variants. Uncertainties indicate standard error over three runs.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{NFN\({}_{\text{INP}}\)[46]} & \multicolumn{2}{c}{NFN\({}_{\text{NP}}\)[46]} & \multicolumn{2}{c}{NFT (Ours)} \\ \hline \multirow{2}{*}{MNIST (erode)} & Train & \(0.0217\pm 0.0004\) & \(0.0212\pm 0.0002\) & \(0.0195\pm 0.0004\) \\  & Test & \(0.0225\pm 0.0001\) & \(0.0223\pm 0.0000\) & \(\mathbf{0.0194\pm 0.0002}\) \\ \hline \multirow{2}{*}{MNIST (dilate)} & Train & \(0.0575\pm 0.0010\) & \(0.0671\pm 0.0017\) & \(0.0473\pm 0.0006\) \\  & Test & \(0.0656\pm 0.0000\) & \(0.0693\pm 0.0009\) & \(\mathbf{0.0510\pm 0.0004}\) \\ \hline \multirow{2}{*}{MNIST (gradient)} & Train & \(0.0536\pm 0.0020\) & \(0.0547\pm 0.0009\) & \(0.0474\pm 0.0005\) \\  & Test & \(0.0552\pm 0.0003\) & \(0.0566\pm 0.0000\) & \(\mathbf{0.0484\pm 0.0007}\) \\ \hline \multirow{2}{*}{FashionMNIST (gradient)} & Train & \(0.0851\pm 0.0012\) & \(0.0853\pm 0.0008\) & \(0.0795\pm 0.0009\) \\  & Test & \(0.0859\pm 0.0001\) & \(0.0870\pm 0.0001\) & \(\mathbf{0.0800\pm 0.0002}\) \\ \hline \multirow{2}{*}{CIFAR (contrast)} & Train & \(0.0191\pm 0.0002\) & \(0.0185\pm 0.0002\) & \(0.0192\pm 0.0005\) \\  & Test & \(0.0204\pm 0.0000\) & \(0.0203\pm 0.0000\) & \(0.0200\pm 0.0002\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Mean squared error to ground truth visual transformation for five INR editing tasks. NFTs generally achieve lower both lower training and test error than NFN variants without using more parameters, suggesting greater expressivity from attention. Uncertainties indicate standard error over three seeds.