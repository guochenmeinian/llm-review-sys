# Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models

 Anant Raj

Coordinated Science Laboraotry

University of Illinois Urbana-Champaign.

Inria, Ecole Normale Superieure

PSL Research University, Paris, France.

anant.raj@inria.fr

&Umut Simsekli

Inria, CNRS, Ecole Normale Superieure

PSL Research University, Paris, France.

umut.simsekli@inria.fr

&Alessandro Rudi

Inria, Ecole Normale Superieure

PSL Research University, Paris, France.

alessandro.rudi@inria.fr

###### Abstract

This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities [10] (the positive semi-definite - PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision \(\varepsilon\) with a cost that is \(m^{2}d\log(1/\varepsilon)\) where \(m\) is the dimension of the model, \(d\) the dimension of the space. The proposed approach consists of: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error \(\varepsilon\), and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. \(\beta\)-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance \(\varepsilon\) from the solution of the equation, with a model of dimension \(m=\varepsilon^{-(d+1)/(\beta-2s)}(\log(1/\varepsilon))^{d+1}\) where \(1/2\leq s\leq 1\) is the fractional power to the Laplacian, and total computational complexity of \(O(m^{3.5}\log(1/\varepsilon))\) and then (b) for Fokker-Planck equation, it is able to produce i.i.d. samples with error \(\varepsilon\) in Wasserstein-1 distance, with a cost that is \(O(d\varepsilon^{-2(d+1)/\beta-2}\log(1/\varepsilon)^{2d+3})\) per sample. This means that, if the probability associated with the SDE is somewhat regular, i.e. \(\beta\geq 4d+2\), then the algorithm requires \(O(\varepsilon^{-0.88}\log(1/\varepsilon)^{4.5d})\) in the preparatory phase, and \(O(\varepsilon^{-1/2}\log(1/\varepsilon)^{2d+2})\) for each sample. Our results suggest that as the true solution gets smoother, we can circumvent the curse of dimensionality without requiring any sort of convexity.

## 1 Introduction

High dimensional stochastic differential equations (SDE) and their associated partial differential equations (PDEs) arise often in various scientific and engineering applications such as control systems, aviation, fluid dynamics, etc [12, 13, 14]. Sampling from an SDE is an active area of research. Generally, a direct approach like Langevin sampling [1, 10, 11] is commonly employed to sample from SDEs. However, an alternative approach involves approximating the solution of the associated PDE in a form that enables easy sampling. In this paper, we focus on the latter approach.

The exact solutions of most of the high-dimensional PDEs are not computable in closed form. Hence, for this reason, finding approximate numerical solutions to high-dimensional SDEs and related PDEs has been an important area of research in numerical methods [1, 12, 13]. With the advent of new machine learning methods, the question naturally arises as to whether we can leverage the expressive power of these models to effectively capture the solution of high-dimensional partial differential equations (PDEs). Recent research efforts have explored this direction by employing various machine learning models for different classes of PDEs [1, 10, 11, 12]. For instance, works such as [13, 14, 15] have harnessed the capabilities of reproducing kernel Hilbert spaces (RKHS) to model PDE solutions. With the recent developments in neural network models and architecture, [10, 11, 16] consider neural networks to model and approximate the solution of high dimensional PDEs. Despite impressive algorithmic innovation, the theoretical understanding of these approaches is still limited, leaving a significant class of PDEs beyond the reach of existing machine learning-based modeling frameworks or with inadequate theoretical insights into the obtained solutions.

In this paper, our goal is to sample from the stochastic differential equations (SDE) that are driven by a Brownian motion or an \(\alpha\)-stable process. The time evolution of probability density functions (PDFs) of stochastic differential equations (SDE) that are driven by a Brownian motion or an \(\alpha\)-stable process is given by Fokker-Planck Equation (FPE) and the fractional Fokker-Planck Equation (fractional FPE) respectively (see e.g, [17, 18, 19]. Therefore, our focus in this study lies in approximating the solution of the Fokker-Planck Equation (FPE), along with its fractional counterpart, as discussed in [10], and subsequently sampling from the approximated solution. The modeling of solutions for FPEs (FPEs) and fractional FPEs poses significant challenges, encompassing the following key aspects: (i) Solutions of FPEs and fractional FPEs are probability densities that are always non-negative and vanish at infinity, (ii) Modeling a probability density is hard because of the normalization property, and (iii) Fractional FPEs involve a fractional Laplacian operator that is non-local in nature. In order for an approximate solution of FPEs or fractional FPEs to be useful for sampling from the corresponding stochastic differential equations (SDEs), it is necessary that sampling from the approximate solution of the corresponding partial differential equation (PDE) be straightforward. Unfortunately, a comprehensive algorithmic approach that addresses the above-mentioned issues and includes a formal approximation analysis is currently lacking in the literature.

This paper adopts a positive semi-definite (PSD) model-based approach to effectively tackle the problem at hand. The recently proposed PSD model [14, 15, 16] offers promising solutions to the challenges encountered in probability density modeling, making it an excellent choice for representing the solution of FPEs and their fractional counterparts. In a recent work by [14], it was demonstrated that employing a Gaussian kernel in the PSD model allows for the exact determination of the normalization constant and enables the characterization of a broad range of probability densities using such PSD models. Moreover, an algorithm for sampling from the PSD model is also proposed recently with statistical guarantees [14].

Driven by these insights, we explore the utilization of a PSD model for approximating the solution of FPEs and fractional FPEs. Consequently, we employ the sampling algorithm presented in[14] to generate samples from the approximate solution. Our primary objective in this study is to obtain rigorous guarantees on the approximation error of PSD models when applied to the approximation of solutions of FPEs and fractional FPEs. Under regularity conditions on the solution of these PDEs, we make the following contributions.

Contributions:We make the following contributions in this paper:

* We show that the PSD-based representation of the solution of FPE has good approximation properties under the regularity assumptions on the true solution of the PDE. In particular, we show that we can achieve an approximation error of \(O(\varepsilon)\) while using \(m=O(\varepsilon^{-(d+1)/(\beta-2)}(\log\frac{1}{\varepsilon})^{(d+1)/2})\) number of samples for PSD representation where \(d\) is the dimension of space variables and the true solution is \(\beta\) times differentiable (**Theorem 1 and 2**). In addition, we are able to provide Wasserstein guarantees for the sampling error of a PSD model-driven solution when sampling from an SDE driven by a brownian motion (**Corollary 1**). An important contribution of our approach is that it relies on much weaker assumptions on the solution of associated PDE (\(\beta\) times differentiable) compared to existing methods that employ dissipativity, log-Sobolev, and Poincare like conditions to provide sampling error guarantees [17, 18, 19, 19, 18, 19, 19].

* We demonstrate that utilizing a positive semi-definite (PSD) based representation is a natural and effective method to approximate the fractional Laplacian operator when applied to a probability density. In particular, we provide results that illustrate the effectiveness of PSD-based representations for Gaussian and shift-invariant kernels, which can be represented using Bochner's theorem (**Theorem 3**).
* As a final contribution, we also establish an approximation guarantee for the solution of the fractional FPE under certain regularity assumptions on the true solution. This extends our earlier result for the FPE and demonstrates the effectiveness of the PSD-based approach in approximating the solution of both FPE and fractional FPE We achieve an approximation error of \(O(\varepsilon)\) while using \(m=O(\varepsilon^{-(d+1)/(\beta-2s)}(\log\frac{1}{\varepsilon})^{(d+1)/2})\) number of samples for PSD representation where \(d\) is the dimension of space variables and the true solution is \(\beta\) times differentiable (**Theorem 4 and 5**). This result directly implies a Wasserstein guarantee on the sampling error when sampling from an SDE driven by an \(\alpha\)-stable levy process using the PSD model. To the best of our knowledge, our algorithm stands as the first of its kind, providing a provable guarantee for sampling from an SDE driven by an \(\alpha\)-stable levy process as well as for approximating the solution of fractional FPEs under regularity assumption on the solution.

Our results clearly suggest that as the true solution gets smoother, we require less number of samples to approximate the true solution using PSD-based representation, hence circumventing the curse of dimensionality. All the proofs are provided in the appendix.

## 2 Related Work

[14] provides a good introduction on PDEs. [12, 13] have discussed numerical methods to solve a PDE at length.

**Machine learning-based solution to PDEs:** Modeling the solution of a PDE using machine learning approaches and in particular kernel methods is a well-established technique leading to optimal sample complexity and very efficient algorithms (see for example [1, 15, 16, 17, 18, 19]). With the recent advancement of neural networks in machine learning, new approaches have emerged. A deep learning-based approach that can handle general high-dimensional parabolic PDEs was introduced in [10]. Two popular approaches that were proposed in this context are (i) _Physics inspired neural networks (PINN)_[15] and (ii) _Deep Ritz method_[12]. However, no statistical optimality was provided in the original papers. Recently, [13] studied the statistical limits of deep learning and Fourier basis techniques for solving elliptic PDEs. The sample complexity to obtain a solution via a PINN and Deep Ritz model are shown to be \(O\left(n^{-\frac{2\beta-2}{d+2\beta-4}}\log n\right)\) and \(O\left(n^{-\frac{2\beta-2}{d+2\beta-2}}\log n\right)\) respectively where \(\beta\) is the order of smoothness of the solution. Subsequently, in a series of papers [14, 15, 16], the theoretical properties of solving a different class of PDEs are discussed under restrictive assumptions on the function class (barron space). For an overview, we refer to [1]. Solutions of PDEs that involve a fractional operator have also been modeled by function in RKHS [1, 1, 2]. However, these works do not provide finite sample approximating error guarantees for the solution obtained. A review of kernel-based numerical methods for PDEs can be found in [14, 15].

**The (fractional) Fokker-Planck equation:** Various efforts have been made to approximate the solution of FPE and its fractional counterpart by numerical methods [14, 15, 16, 17]. [13, 18, 19] utilized a radial basis kernel-based approach to numerically compute the solution of an FPE. RKHS-based methods have also been used to obtain an approximate solution [1, 2]. However, the above mentioned works are numerical in nature and do not discuss the approximation limits of the solution. On the more applied side, various works have been proposed by harnessing the expressive power of deep networks to model the solution of FPEs and fractional FPEs [14, 15, 16, 17]. However, a systematic theoretical study is missing from the literature.

**Langevin Monte Carlo:** An important line of research considers the probabilistic counterpart of FPE that is directly simulating the SDEs associated with the FPEs by time-discretization. Under convexity assumptions [1, 1, 18] proved guarantees for obtaining a _single_ (approximate) sample from the SDE for a given time. These frameworks were later extended in [13, 14, 15], so that the convexity assumptions were weakened to more general notions such as uniform dissipativity. [13] introduced a more general SDE framework, which covers a larger class of PDEs. In the case of sampling SDEs with domain constraints [1, 1, 18, 19]proposed Langevin-based methods under convexity assumption, which were later weakened in [11, 12]. [13, 14] considered SDEs which corresponded to the fractional FPE in dimension one and this approach was later extended in [15, 16]. An overview of these methods is provided in [14]. Contrary to this line of research, which typically requires convexity or dissipativity, our framework solely requires smoothness conditions.

PSD Models:PSD models are an effective way to model non-negative functions and enjoy the nice properties of linear models and more generally RKHS. It was introduced in [13] and their effectiveness for modeling probability distributions was shown in [15]. It has been also effectively used in optimal transport estimation [16, 17], in finding global minima of a non-convex function [13], in optimal control [1]. In [13], an algorithm to draw samples from a PSD model was proposed.

## 3 Notations and Background

Notation:Space input domain is denoted as \(\mathcal{X}\) and time input domain is denoted as \(\mathcal{T}\). Combined space \(\mathcal{X}\times\mathcal{T}\) is denoted as \(\widehat{\mathcal{X}}\). \(L^{\infty}(\widehat{\mathcal{X}})\), \(L^{1}(\widehat{\mathcal{X}})\) and \(L^{2}(\widehat{\mathcal{X}})\) denote respectively the space of essentially bounded, absolutely integrable and square-integrable functions with respect to Lebesgue measure over \(\widehat{\mathcal{X}}\). \(W_{2}^{\beta}(\widehat{\mathcal{X}})\) denotes the Sobolev space of functions whose weak derivatives up to order \(\beta\) are square-integrable on \(\widehat{\mathcal{X}}\). We denote by \(\mathbb{R}^{d}_{++}\) the space vectors in \(\mathbb{R}^{d}\) with positive entries, \(\mathbb{R}^{n\times d}\) the space of \(n\times d\) matrices, \(\mathbb{S}^{n}_{+}=\mathbb{S}_{+}(\mathbb{R}^{n})\) the space of positive semi-definite \(n\times n\) matrices. Given a vector \(\eta\in\mathbb{R}^{d}\), we denote \(\mathrm{diag}(\eta)\in\mathbb{R}^{d\times d}\) the diagonal matrix associated to \(\eta\).

### (Fractional) Fokker-Planck Equation

The FPE describes the time evaluation of particle density if the particles are moving in a vector field and are also influenced by random noise like Brownian motion. Let us consider the following SDE driven by the standard Brownian motion (Wiener process) \(W_{t}\):

\[dX_{t}=\mu(X_{t},t)dt+\sigma dW_{t}\] (1)

where \(X_{t}\in\mathbb{R}^{d}\) is a random variable, \(\mu(X_{t},t)\in\mathbb{R}^{d}\) is drift, \(\sigma\in\mathbb{R}^{d\times k}\) and \(W_{t}\) is a \(k\)-dimensional Wiener process. \(\mu_{i}(X_{t},t)\) denotes the \(i\)th element of vector \(\mu(X_{t},t)\). The diffusion tensor1 is defined by \(D=\frac{1}{2}\sigma\sigma^{\top}\).

Footnote 1: To simplify our analysis, we will consider diffusion tensor \(D\) independent from \(X_{t}\) and \(t\) in this work.

The probability density function \(p(x,t)\) of the random variable \(X_{t}\) solving the SDE (1) is given by the FPE, given as follows [13, 14]:

\[\frac{\partial p(x,t)}{\partial t}=-\sum_{i=1}^{d}\frac{\partial}{\partial x _{i}}(\mu_{i}(x,t)p(x,t))+\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2} }{\partial x_{i}\partial x_{j}}p(x,t),\text{ such that }p(0,x)=p_{0}(x).\] (2)

It is known that when \(\mu(X_{t},t)\) is the gradient of a potential or objective function \(f:\mathcal{X}\rightarrow\mathbb{R}\), i.e., \(\mu=\nabla f\), where \(f\) satisfies certain smoothness and growth conditions, the stationary distribution of the stochastic process in equation (1) exists and is described by the so-called Gibbs distribution [14].

Despite of having nice theoretical properties, Brownian-driven SDEs are not suitable in certain models in engineering, physics and biology [12, 13]. A popular non-Gaussian SDE used in such applications is given as follows:

\[dX_{t}=\mu(X_{t},t)dt+dL_{t}^{\alpha},\] (3)

where \(L_{t}^{\alpha}\) denotes the (rotationally invariant) \(\alpha\)-stable Levy process in \(\mathbb{R}^{d}\) and is defined as follows for \(\alpha\in(0,2]\)[14]:

1. \(L_{0}^{\alpha}=0\) almost surely;
2. For any \(t_{0}<t_{1}<\cdots<t_{N}\), the increments \(L_{t_{n}}^{\alpha}-L_{t_{n-1}}^{\alpha}\) are independent;
3. The difference \(L_{t}^{\alpha}-L_{s}^{\alpha}\) and \(L_{t-s}^{\alpha}\) have the same distribution, with the characteristic function \(\mathbb{E}\left[e^{j(u,L_{t-s}^{\alpha})}\right]=\exp(-(t-s)^{\alpha}\|u\|_{ 2}^{2})\) for \(t>s\) and \(j=\sqrt{-1}\);
4. \(L_{t}^{\alpha}\) is continuous in probability.

In the SDE described in equation (3), we choose \(\sigma=1\) for the simplicity of the expressions. When \(\alpha=2\), the process recovers the Brownian motion, i.e., \(L_{t}^{\alpha}=\sqrt{2}W_{t}\). The fundamental difference between \(L^{\alpha}_{t}\) and \(W_{t}\) when \(\alpha<2\) is that the increments of this process are heavy-tailed, with the following property: \(\mathbb{E}[L^{\alpha}_{t}]^{p}|=+\infty\) for all \(p\geq\alpha\) and \(t\geq 0\). This heavy-tailed structure makes the solution paths of (3) discontinuous: \((X_{t})_{t\geq 0}\) can exhibit at most countable number of discontinuities. SDEs based on \(\alpha\)-stable processes have also received some attention in machine learning as well [20, 19, 21, 22]. The resulting governing equation for stable-driven SDEs is similar to the traditional Fokker-Planck equation except that the order \(\alpha\) of the highest derivative is fractional when \(\alpha<2\). The following generalization of the FPE is known in the literature as fractional FPE because of the presence of fractional Laplacian operator [19]:

\[\frac{\partial p(x,t)}{\partial t}=-\sum_{i=1}^{d}\frac{\partial}{\partial x_ {i}}(\mu_{i}(x,t)p(x,t))-(-\Delta)^{\alpha/2}p(x,t),\text{ such that }p(0,x)=p_{0}(x),\] (4)

where \((-\Delta)^{s}\) is the non-local Laplacian operator which is also known as the fractional Laplacian operator and is induced by the stable process. For \(0<s<1\), the fractional Laplacian of order \(s\), \((-\Delta)^{s}\) can be defined on functions \(f:\mathbb{R}^{d}\to\mathbb{R}\) as a singular integral as follows: \((-\Delta)^{s}f(x)=c_{d,s}\int_{\mathbb{R}^{d}}\frac{f(x)-f(y)}{\|x-\|_{2}^{d+ 2s}}\ dy,\) where \(c_{d,s}=\frac{4^{s}\Gamma(d/2+s)}{\pi d^{2}|\Gamma(-s)|}.\) A more useful representation of the fractional Laplacian operator is given as a Fourier multiplier, \(\mathcal{F}[(-\Delta)^{s}f](u)=\|u\|^{2s}\mathcal{F}[f](u),\) where \(\mathcal{F}[f](u)=\int_{\mathbb{R}^{d}}f(x)e^{j\pi^{\top}u}\ dx\). We will utilize the Fourier multiplier-based representation of the fractional Laplacian operator to prove our approximation result for the fractional FPE.

### PSD Models for Probability Representation

Consider a feature map representation \(\phi:\mathcal{X}\to\mathcal{H}\) from input space \(\mathcal{X}\) to a Hilbert space \(\mathcal{H}\), and a linear operator \(M\in\mathbb{S}_{+}(\mathcal{H})\), then PSD models are represented as in [13],

\[f(x;M,\phi)=\phi(x)^{\top}M\phi(x).\] (5)

It is clear that \(f(x;M,\phi)\) is a non-negative function and hence PSD models offer a general way to parameterize non-negative functions. We consider \(\mathcal{H}\) to be the RKHS corresponding to the kernel \(k\) such that \(\phi(x)=k(x,\cdot)\). In particular, we will consider the case where: \(\phi=\phi_{\eta}:\mathbb{R}^{d}\to\mathcal{H}_{\eta}\) is the feature map associated with the Gaussian kernel \(k_{\eta}(x,y)=\phi_{\eta}(x)^{\top}\phi_{\eta}(y)=e^{-\eta\|x-y\|^{2}}\) with \(\eta>0\). Let us assume that \(M\) lies in the span of feature maps corresponding to \(n\) data points, \(X=\{x_{1},\cdots,x_{n}\}\) then \(M=\sum_{i,j}A_{ij}\phi(x_{i})\phi(x_{j})^{\top}\) for some \(A\in\mathbb{S}_{+}^{n}\). Hence, a Gaussian PSD model can be defined as, \(f(x;A,X,\eta)=\sum_{ij}A_{ij}k_{\eta}(x_{i},x)k_{\eta}(x_{j},x)\). Given two base point matrices \(X\in\mathbb{R}^{n\times d}\) and \(X^{\prime}\in\mathbb{R}^{m\times d}\), then \(K_{X,X^{\prime},\eta}\) denotes the kernel matrix with entries \((K_{X,X^{\prime},\eta})_{ij}=k_{\eta}(x_{i},x_{j}^{\prime})\) where \(x_{i}\) and \(x_{j}^{\prime}\) are the \(i\)-th and \(j\)-th rows of \(X\) and \(X^{\prime}\) respectively. Gaussian PSD model has a lot of interesting properties and the details can be found in [10]. But, we would like to mention below an important aspect of PSD models which enable them to be used as an effective representation for probability density:

**Sum Rule (Marginalization and Integration)**: The integral of a PSD model can be computed as,

\[\int f(x;A,X,\eta)\ dx=c_{2\eta}\text{Tr}(AK_{X,X,\frac{\eta}{2}}),\ \text{where}\ c_{\eta}=\int k(0,x)\ dx.\]

Similarly, only one variable of a PSD model can also be integrated to result in the sum rule.

**Proposition 1** (Marginalization in one variable [10]).: _Let \(X\in\mathbb{R}^{n\times d}\), \(Y\in\mathbb{R}^{\times\times^{\prime}}\), \(A\in\mathbb{S}_{+}(\mathbb{R}^{n})\) and \(\eta,\eta^{\prime}\in\mathbb{R}+\), then the following integral is a PSD model,_

\[\int f(x,y;A,[X,Y],(\eta,\eta^{\prime}))\ dx=f(y;B,Y,\eta^{\prime})\ \text{with}\ B=c_{2\eta}A\circ K_{X,X,\frac{\eta}{2}}.\]

In a recent work, [13] gave an efficient algorithm to sample from a PSD model, with the following result

**Proposition 2** (Efficient sampling from PSD models [13]).: _Let \(\varepsilon>0\). Given a PSD model \(f(x;A,\eta)\) with \(A\in\mathbb{R}^{m\times m}\) for \(x\in\mathbb{R}^{d}\). There exists an algorithm (presented in [13]) that samples i.i.d. points from a probability \(\tilde{f}_{\varepsilon}\) such that the 1-Wasserstein distance between \(f(\cdot;A,\eta)\) and \(\tilde{f}_{\varepsilon}\) satisfies \(\mathbb{W}_{1}(f,\tilde{f}_{\varepsilon})\leq\varepsilon\). Moreover, the cost of the algorithm is \(O(m^{2}d\log(d/\varepsilon))\)._

## 4 Approximation of the (Fractional) Fokker-Planck Equation

In this section, we investigate the approximating properties of PSD models for solving FPEs and fractional FPEs. Our focus is on the approximation guarantee achieved by fitting a single PSD model in the joint space-time domain. Let's denote the solution of the PDE as \(p^{\star}(x,t)\), where we consider the FPE in section 4.1 and the fractional FPE in section 4.2. We introduce mild assumptions on the domain and the solution probability density. Throughout the paper, we assume the spatial domain \(\mathfrak{X}\) to be \((-R,R)^{d}\) and the time domain \(\mathfrak{T}\) to be \((0,R)\). We have the following assumption from [10] on the solution \(p^{\star}\) of the PDE,

**Assumption 1**.: _Let \(\beta>2\), \(q\in\mathbb{N}\). There exists \(f_{1},f_{2},\cdots,f_{q}\in W^{\beta}_{2}(\widetilde{\mathfrak{X}})\cap L^{ \infty}(\widetilde{\mathfrak{X}})\), such that the density \(p^{\star}:\widetilde{\mathfrak{X}}\to\mathbb{R}\) satisfies, \(p^{\star}(x,t)=\sum_{i=j}^{q}f_{j}^{2}(x,t)\)._

The assumption above is quite general and satisfied by a wide family of probabilities, as discussed in the Proposition 5 of [10]. We reiterate the result here below.

**Proposition 3** (Proposition 5, [10]).: _The assumption above is satisfied by_

* _any_ \(p^{\star}(x,t)\) _that is_ \(\beta\)_-times differentiable and strictly positive on_ \([-R,R]^{d}\times[0,R]\)_,_
* _any exponential model_ \(p(x,t)=e^{-v(x,t)}\) _such that_ \(v\in W^{\beta}_{2}(\widetilde{\mathfrak{X}})\cap L^{\infty}(\widetilde{ \mathfrak{X}})\)_,_
* _any mixture model of Gaussians or, more generally, of exponential models from (ii),_
* _any_ \(p\) _that is_ \(\beta+2\)_-times differentiable on_ \([-R,R]^{d}\times[0,R]\) _with a finite set of zeros in_ \(\widetilde{\mathfrak{X}}\) _and with positive definite Hessian in each zero._

_Moreover when \(p^{\star}\) is \(\beta\)-times differentiable over \([-R,R]^{d}\times[0,R]\), then it belongs to \(W^{\beta}_{2}(\widetilde{\mathfrak{X}})\cap L^{\infty}(\widetilde{\mathfrak{X }})\)._

We will need to impose some extra mild assumptions on \(f_{j}\) for \(j\in\{1,\cdots,q\}\) to approximate the solution of a fractional Fokker-Planck equation because of involvement of the fractional laplacian operator (non-local operator). We will cite these extra assumptions when we describe the approximation results in section 4.2. Apart from that, we also have a boundedness assumption for the coefficients of the PDE in the domain we consider.

**Assumption 2**.: _We have for all \((x,t)\in\widetilde{\mathfrak{X}}\), \(\mu_{i}(x,t)\leq R_{\mu}\), \(\frac{\partial\mu_{i}(x,t)}{\partial x_{i}}\leq R_{\mu_{p}}\), and \(D_{ij}\leq R_{d}\) for all \((i,j)\in\{1,\cdots,d\}\)._

It is also obvious to see that the PSD-based representation of probability density vanishes at infinity.

### Approximating Solution of Fokker-Planck Equation

Let us consider the Fokker-Planck equation given in equation (2) and \(p^{\star}(x,t)\) is the solution of Fokker-Planck equation in equation (2) which satisfies the assumption 1. Hence, \(p^{\star}(x,t)\) satisfies the following,

\[\frac{\partial p^{\star}(x,t)}{\partial t}=-\sum_{i=1}^{d}\frac{ \partial}{\partial x_{i}}(\mu_{i}(x,t)p^{\star}(x,t))+\sum_{i=1}^{d}\sum_{j=1} ^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star}(x,t).\]

For now, we are ignoring the initial value as we only are interested in approximation error and we will have a uniform approximation of the solution density \(p^{\star}\) and its derivatives up to the order 2 in the domain.

Given \(m\) base points pair \((\tilde{x}_{i},\tilde{t}_{i})\in\widetilde{\mathfrak{X}}\) for \(i\in\{1,2,\cdots,m\}\) of space and time, let us consider the following approximation model for the solution of FPE,

\[\tilde{p}(x,t)=\sum_{i,j}A_{ij}k_{X,T}((x,t),(\tilde{x}_{i},\tilde{t}_{i})) \cdot k_{X,T}((x,t),(\tilde{x}_{j},\tilde{t}_{j}))\] (6)

where \(k_{X,T}\) is the joint kernel across space and time and \(A\) is a positive semi-definite matrix of dimension \(\mathbb{R}^{m\times m}\). \(k_{X,T}\) can be defined as the product of two Gaussian kernels \(k_{X}\) and \(k_{T}\) which are defined on space and time domain respectively i.e \(k_{X,T}((x,t),(x^{\prime},t^{\prime}))=k_{X}(x,x^{\prime})\cdot k_{T}(t,t^{ \prime})\). We represent \(\phi_{X}(x)=k_{X}(x,\cdot)\in\mathcal{H}_{X}\), \(\phi_{T}(t)=k_{T}(t,\cdot)\in\mathcal{H}_{T}\) and \(\phi_{X,T}(x,t)=\phi_{X}(x)\otimes\phi_{T}(t)\). Here, we assume that \(k_{X}\) and \(k_{T}\) are both Gaussian kernels and hence the RKHS \(\mathcal{H}_{X}\) and \(\mathcal{H}_{T}\) correspond to the RKHS of the Gaussian kernel. For simplicity, we keep the kernel bandwidth parameter \(\eta\) the same for \(k_{X}\) and \(k_{T}\). It is clear that the joint kernel \(k_{X,T}\) is also Gaussian with the kernel bandwidth parameter equal to \(\eta\). Given \(M=\sum_{i,j}A_{ij}\phi_{X,T}(x_{i},t_{i})\phi_{X,T}(x_{j},t_{j})^{\top},p(x,t)\) can also be represented with function \(f((x,t),A,X,\eta)\) as,

\[\tilde{p}(x,t)=\underbrace{\phi_{X,T}(x,t)^{\top}M\phi_{X,T}(x,t)}_{:=f((x,t) ;M,\phi_{X,T})}=\underbrace{\psi(x,t)^{\top}A\psi(x,t)}_{:=f((x,t),A,\bar{X}, \eta)},\] (7)where \(\psi(x,t)\in\mathbb{R}^{n}\), \([\psi(x,t)]_{i}=k_{X}(x,\tilde{x}_{i})\cdot k_{T}(t,\tilde{t}_{i})\) and \(\tilde{X}\in\mathbb{R}^{m\times d+1}\) is the concatenated data matrix of space and time for \(m\) pair of samples \((x_{i},t_{i})\) for \(i\in\{1,\cdots,m\}\).

We can now easily optimize an associate loss function with the FPE to obtain the unknown PSD matrix \(A\). Let us assume that there exists a positive semi-definite matrix \(A\) for which the PSD model \(\tilde{p}(x,t)\) approximates the true solution \(p^{\star}(x,t)\) well. In that case, our goal is to obtain an upper bound on the following error objective:

\[\left\|\frac{\partial(p^{\star}-\tilde{p})(x,t)}{\partial t}+\sum_{i=1}^{d} \frac{\partial}{\partial x_{i}}(\mu_{i}(x,t)(p^{\star}-\tilde{p})(x,t))-\sum_ {i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}} (p^{\star}-\tilde{p})(x,t)\right\|_{L^{2}(\tilde{\chi})}.\] (8)

Our approximation guarantee works in two steps. In the first step, we find an infinite dimensional positive operator \(M_{\varepsilon}:\mathcal{H}_{X}\otimes\mathcal{H}_{T}\to\mathcal{H}_{X} \otimes\mathcal{H}_{T}\) such that if we denote \(\hat{p}(x,t)=\phi_{X,T}(x,t)^{\top}M_{\varepsilon}\phi_{X,T}(x,t)\) then for some \(M_{\varepsilon}\), \(\hat{p}\) is \(\varepsilon\)-approximation of the true density \(p^{\star}\). Clearly, for a set of functions \(\tilde{f}_{1},\cdots,\tilde{f}_{q}\in\mathcal{H}_{X}\otimes\mathcal{H}_{T}\), \(M_{\varepsilon}=\sum_{j=1}^{q}\tilde{f}_{j}\tilde{f}_{j}^{\top}\) is a positive operator from \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\to\mathcal{H}_{X}\otimes\mathcal{H}_{T}\). Now, we carefully construct \(\tilde{f}_{j}\) for \(j\in\{1,\cdots,q\}\) in RKHS \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\) such that approximation guarantee in theorem 1 is satisfied. Details of the construction is given in Appendix. We represent \((x,t)\) as a \(d+1\) dimensional vector \(\tilde{x}\) and a mollifier function \(g\) (details are in the appendix) such that \(g_{v}(x)=v^{-(d+1)}g(\tilde{x}/v)\). Then, we construct our \(\tilde{f}_{j}=f_{j}\star g_{v}\) for \(j\in\{1,2,\cdots,q\}\). We prove the result in Theorem 1 with this construction. To prove our approximation error guarantee for the solution of FPE using PSD model, we assume the derivatives of \(f_{1},\cdots,f_{q}\) up to order 2 are bounded which is already covered in Assumption 1.

**Theorem 1**.: _Let \(\beta>2,q\in\mathbb{N}\). Let \(f_{1},\ldots,f_{q}\in W_{2}^{\beta}(\mathbb{R}^{d})\cap L^{\infty}(\mathbb{R} ^{d})\) and the function \(p^{\star}=\sum_{i=1}^{q}f_{i}^{2}\). Let \(\varepsilon\in(0,1]\) and let \(\eta\in\mathbb{R}^{d}_{++}\). Let \(\phi_{X,T}\) be the feature map of the Gaussian kernel with bandwidth \(\eta\) and let \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\) be the associated RKHS. Then there exists \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_ {T})\) with \(\operatorname{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\hat{p}(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds under assumption 2 on the coefficients of the FPE,_

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d} \frac{\partial}{\partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))-\sum_{i=1}^{d}\sum_{j =1}^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\hat{p}(x,t) \right\|_{L^{2}(\tilde{\chi})}=O(\varepsilon),\] (9) \[\text{and }\operatorname{tr}(\mathsf{M}_{\varepsilon})\leq\hat{C}| \eta|^{1/2}\left(1+\varepsilon^{\frac{2\beta}{\beta-2}}\exp\left(\frac{\tilde{C }}{\eta_{0}}e^{-\frac{2}{\beta-2}}\right)\right),\]

_where \(|\eta|=\det(\operatorname{diag}(\eta))\), and \(\tilde{C}^{\prime}\)and\(\hat{C}^{\prime}\) depend only on \(\beta,d,\|f_{i}\|_{W_{2}^{\beta}(\mathbb{R}^{d})},\|f_{i}\|_{L^{\infty}(\mathbb{R }^{d})},\left\|\frac{\partial f_{i}}{\partial x_{j}}\right\|_{L^{2}(\mathbb{R }^{d})}\), and \(\left\|\frac{\partial^{2}f_{i}}{\partial x_{j}x_{k}}\right\|_{L^{2}(\mathbb{R }^{d})}\)._

Even though, from theorem 1, we know that there exists a positive operator \(\mathsf{M}_{\varepsilon}:\mathcal{H}_{X}\otimes\mathcal{H}_{T}\to\mathcal{H}_{X} \otimes\mathcal{H}_{T}\) such that \(\hat{p}(x,t)=\phi_{X,T}(x,t)^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}(x,t)\) approximate the solution \(p^{\star}(x,t)\) of FPE from Theorem 1, it is not clear how to compute \(\mathsf{M}_{\varepsilon}\) as we do not have access of \(f_{j}\) for \(j\in\{1,2,\cdots,q\}\) beforehand. Hence, to get the final approximation error bound we need to further approximate \(\hat{p}(x,t)\) with a PSD model of finite dimension \(\tilde{p}(x,t)\) which can be computed if we have the access to a finite number of base points \((x_{i},t_{i})\) for \(i\in\{1,\cdots,m\}\) in the domain \(\widetilde{\mathcal{X}}\). Let us now construct a matrix \(A_{m}\) as follows. Consider \(A_{m}=K_{(X,T)(X,T)}^{-1}\tilde{Z}M\tilde{Z}^{\star}K_{(X,T)(X,T)}^{-1}\), where \(K_{(X,T)(X,T)}\) is a kernel matrix in \(\mathbb{R}^{m\times m}\) such that \([K_{(X,T)(X,T)}]_{ij}=\phi_{X,T}(x_{i},t_{i})^{\top}\phi_{X,T}(x_{j},t_{j})\). Let us define the operator \(\tilde{Z}:\mathcal{H}_{X}\otimes\mathcal{H}_{T}\to\mathbb{R}^{m}\) such that \(\tilde{Z}u=(\psi(x_{1},t_{1})^{\top}u,\cdots,\psi(x_{m},t_{m})^{\top}u)\), and \(\tilde{Z}^{\star}\alpha=\sum_{i=1}^{m}\psi(x_{i},t_{i})\alpha_{i}\). We further define a projection operator \(\tilde{P}=\tilde{Z}^{\star}K_{(X,T)(X,T)}^{-1}\tilde{Z}\). Hence, we now define the approximation density \(\tilde{p}(x,t)\) as

\[\tilde{p}(x,t)=\underbrace{\psi(x,t)^{\top}A_{m}\psi(x,t)}_{:=f((x,t),A_{m}, \tilde{X},\eta)}=\underbrace{\phi_{X,T}(x,t)^{\top}\tilde{P}M\tilde{P}\phi_{X,T }(x,t)}_{:=f((x,t),\tilde{P}M\tilde{P},\phi_{X,T})}.\]

In the main result of this section below, we show that \(\tilde{p}(x,t)\) is a good approximation for \(\hat{p}(x,t)\) and hence for the true solution \(p^{\star}(x,t)\).

**Theorem 2**.: _Let \(p^{\star}\) satisfy Assumptions 1 and 2. Let \(\varepsilon>0\) and \(\beta>2\). There exists a Gaussian PSD model of dimension \(m\in\mathbb{N}\), i.e., \(\tilde{p}(x,t)=f((x,t),A_{m},X,\eta)\), with \(A_{m}\in\mathbb{S}_{+}^{m}\) and \(X\in\mathbb{R}^{m\times d+1}\)_and \(\eta_{m}\in\mathbb{R}^{d}_{++}\), such that with \(m=O(\varepsilon^{-(d+1)/(\beta-2)}(\log\frac{1}{\varepsilon})^{(d+1)/2})\), we have,_

\[\left\|\frac{\partial\tilde{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\tilde{p}(x,t))-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\tilde{p}(x,t)\right\|_{L^{2} (\tilde{\chi})}=O(\varepsilon).\] (10)

Theorem 2 indicates that there is a positive semi-definite matrix \(A_{m}\in\mathbb{R}^{m\times m}\) such that \(\tilde{p}(x,t)=\psi(x,t)^{\top}A_{m}\psi(x,t)\) is a good approximation of \(p^{\star}(x,t)\). Next, we would see the usefulness of the above result in Theorem 2 in sampling from the solution of an FPE given the drift term \(\mu\) satisfies the regularity condition as in [1, Theorem 1.1] and \(\mu\in W^{\beta}_{2}(\widetilde{\mathfrak{X}})\). In this case, the drift function \(\mu\) can be approximated by a function \(\hat{\mu}\) in a Gaussian RKHS, and the approximation guarantees are due to interpolation result in Lemma 4. The following result is the direct consequence of results in the Theorem 2 and Proposition 2.

Efficient sampling methodLet \(\hat{\mu}\in\mathscr{H}_{\eta}\) be the approximation of \(\mu\) such that \(\|\mu-\hat{\mu}\|_{L^{2}(\widetilde{\mathfrak{X}})}=O(\varepsilon)\), by Gaussian kernel approximation. Let us apply the construction of Thm.2 to \(\hat{\mu}\).

**Step 1:** Find the best model \(\hat{A}_{m}\) by minimizing the l.h.s. of equation (12) over the set of \(m\)-dimensional Gaussian PSD models. Note that in that case, since everything is a linear combination of Gaussians (or products of Gaussians), we can compute the \(L^{2}\) distance in closed form and minimize it over the positive semidefinite cone of \(m\)-dimensional matrices, obtaining \(\hat{A}_{\gamma}\) exactly.

**Step 2:** Denote the resulting PSD model by \(\hat{p}_{\gamma}(x,t)=f(x,t;\hat{A}_{\gamma},\eta)\). At any time \(t\) of interest we sample from the probability \(\hat{p}_{\gamma}(\cdot,t)\) via the sampling algorithm in [13].

**Corollary 1** (Error of sampling).: _Assume that \(\mu\in W^{\beta}_{2}(\widetilde{\mathfrak{X}})\) satisfies the regularity condition as in [1, Theorem 1.1]. For any time \(t\), the algorithm described above provides i.i.d. samples from a probability \(\tilde{p}_{t}\) that satisfies in expectation \(\mathbb{E}_{t}\ \mathbb{W}_{1}(p^{\star}_{t},\tilde{p}_{t})^{2}=O( \varepsilon^{2}),\) where \(p^{\star}_{t}:=p^{\star}(\cdot,t)\) is the probability at time \(t\) of a stochastic process with drift \(\mu\) and diffusion \(D\). Moreover, the cost of the algorithm is \(O(m^{2}d\log(d/\varepsilon))\) where \(m=O(\varepsilon^{-d/\beta-2}(\log(1/\varepsilon))^{d})\)._

This result is proven in the appendix after theorem 2. It turns out that the parameter of the PSD model can be learned by a simple semi-definite program [13] which has a computational complexity of \(O(m^{3.5}\log(1/\varepsilon))\) where \(m\) is the dimension of the model.

**Remark**.: _It is evident from the aforementioned result that our method excels in sampling from the Stochastic Differential Equation (SDE) while ensuring the particle density adheres to Assumption 1 within a bounded domain. In contrast, traditional sampling methods such as Langevin struggle to sample from this extensive class due to the absence of dissipativity, log-sobolev, and Poincare-like conditions._

### Approximating Solution of Fractional Fokker-Planck Equation

In this section, our aim is to approximate the solution \(p^{\star}(x,t)\) of a fractional Fokker-Planck equation, similar to the previous section. The equation replaces the Laplacian operator with a fractional Laplacian operator while keeping other terms unchanged. The fractional Laplacian operator, being non-local, is generally challenging to compute analytically. Therefore, approximating the fractional operator on a function has become a separate research direction. Below, we demonstrate that utilizing a PSD model-based representation is a natural choice for approximating the fractional operator acting on probability density. We present results for two cases: (i) a Gaussian kernel with the bandwidth parameter \(\eta\), and (ii) a shift-invariant kernel represented using Bochner's theorem [14].

**Theorem 3**.: _Consider probability density represented by a PSD model as \(p(x)=f(x;A,X,\eta)=\sum_{i,j=1}^{m}A_{ij}k(x_{i},x)k(x_{j},x)\) for some PSD matrix \(A\), where \(X\in\mathbb{R}^{m\times d}\) is the data matrix whose \(i\)-th row represents sample \(x_{i}\in\mathbb{R}^{d}\) for \(i\in\{1,\cdots,m\}\), then_

_1. if the kernel \(k\) is Gaussian with bandwidth parameter \(\eta\), we have_

\[(-\Delta)^{s}p(x)=C\sum_{i,j=1}^{n}A_{ij}e^{-\eta(\|x_{i}\|^{2}+\|x_{j}\|^{2}) }\mathbb{E}_{\xi\sim N(\mu,\Sigma)}\|\xi\|^{2s}e^{-j\xi^{\top}x}]\]

_where \(\mu=2j\eta(x_{i}+x_{j})\), \(\Sigma=4\eta I\) and \(C\) can be computed in the closed form._

_2. if the kernel \(k\) have the bochner's theorem representation, i.e. \(k(x-y)=\int_{\mathbb{R}^{d}}q(\omega)e^{j\omega^{\top}(x-y)}\ d\omega\) where \(q\) is a probability density, then_

\[(-\Delta)^{s}p(x)=\sum_{i,j=1}^{n}A_{ij}\ \mathbb{E}[\|\omega_{\ell}+\omega_{k}\|^{ 2s}e^{j\omega_{\ell}^{\top}(x_{i}+x)}e^{j\omega_{k}^{\top}(x_{j}+x)}],\]

_where expectations are over \(\omega_{\ell},\omega_{k}\sim q(\cdot)\)._We utilize the Fourier multiplier-based definition of the fractional Laplacian operator in Theorem 3. In both cases discussed in Theorem 3, empirical estimation of \((-\Delta)^{s}p(x)\), obtained from finite samples (empirical average), is sufficient for practical purposes. Shifting our focus from the representation of a non-local operator acting on a probability density, we now turn to the approximation result for the solution of a fractional Fokker-Planck equation in this section. We assume that the optimal solution \(p^{\star}(x,t)\) can be expressed as a sum of squares of \(q\) functions, as stated in assumption 1. To establish the approximation results for the fractional Fokker-Planck equation, we need to impose an additional mild assumption on \(f_{j}\) for \(j\in 1,\cdots,q\), in addition to assumption 1, which we state next.

**Assumption 3**.: _Let \(\mathcal{F}[f](\cdot)\) denotes the Fourier transform of a function \(f\), then \(\mathcal{F}[f_{i}](\cdot)\in L^{1}(\tilde{\mathcal{X}})\cap L^{\infty}(\tilde {\mathcal{X}})\), and \(\mathcal{F}[(-\Delta)^{s}f_{i}](\cdot)\in L^{1}(\tilde{\mathcal{X}})\) for \(i\in\{1,\cdots q\}\)._

Similar to the previous section, it requires two steps to obtain the approximation error bound. By similar construction as in section 4.1, we define an infinite dimensional positive operator \(M_{\varepsilon}:\mathcal{H}_{X}\otimes\mathcal{H}_{T}\to\mathcal{H}_{X} \otimes\mathcal{H}_{T}\) and based on this positive operator, we have a probability density \(\hat{p}\) as \(\hat{p}(x,t)=\phi_{X,T}(x,t)^{\top}M_{\varepsilon}\phi_{X,T}(x,t)\) which is \(O(\varepsilon)\) approximation of \(p^{\star}(x,t)\). The result is stated below in Theorem 4 and proven in Appendix C.3.

**Theorem 4**.: _Let \(\beta>2,q\in\mathbb{N}\). Let \(f_{1},\ldots,f_{q}\) satisfy assumptions 1 and 3 and the function \(p^{\star}=\sum_{i=1}^{q}f_{i}^{2}\). Let \(\varepsilon\in(0,1]\) and let \(\eta\in\mathbb{R}^{d}_{++}\). Let \(\phi_{X,T}\) be the feature map of the Gaussian kernel with bandwidth \(\eta\) and let \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\) be the associated RKHS. Then there exists \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_ {T})\) with \(\operatorname{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\hat{p}(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds under assumption 2 on the coefficients of the fractional FPE,_

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d} \frac{\partial}{\partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))+(-\Delta)^{s}\hat{p} (x,t)\right\|_{L^{2}(\tilde{\mathcal{X}})}=O(\varepsilon),\] (11) \[\text{and}\ \ \operatorname{tr}(\mathsf{M}_{\varepsilon})\leq \hat{C}|\eta|^{1/2}\left(1+\varepsilon^{\frac{2\beta}{\beta-2s}}\exp\left( \frac{\tilde{C}^{\prime}}{\eta_{0}}\varepsilon^{-\frac{2}{\beta-2s}}\right) \right),\]

_where \(|\eta|=\det(\operatorname{diag}(\eta))\), and \(\tilde{C}\) and \(\hat{C}\) depend only on \(\beta,d\) and properties of \(f_{1},\cdots,f_{q}\)._

Major difficulty in proving the result in Theorem 4 arises from the challenge of effectively managing the fractional Laplacian term. To tackle this issue, we utilize the Fourier-based representation to gain control over the error associated with the fractional Laplacian. The details can be found in Lemma7, presented in Appendix C.1. As a next step, we would follow a similar procedure as in section 4.1 to obtain the final approximation bound. In the second part of the proof, we utilize Gagliardo-Nirenberg inequality [13] for fractional Laplacian to obtain the final approximation guarantee. In the next result, we show that for a particular choice of a PSD matrix \(A_{m}\), \(\tilde{p}(x,t)=\psi(x,t)^{\top}A_{m}\psi(x,t)=\phi_{X,T}(x,t)^{\top}\hat{P}M \tilde{P}\phi_{X,T}(x,t)\) is a good approximation for the true solution \(p^{\star}(x,t)\) of the fractional Fokker-Planck equation.

**Theorem 5**.: _Let \(p^{\star}\) satisfy Assumptions 1, 2, and 3. Let \(\varepsilon>0\) and \(\beta>2\). There exists a Gaussian PSD model of dimension \(m\in\mathbb{N}\), i.e., \(\tilde{p}(x,t)=f((x,t),A_{m},X,\phi_{X,T})\), with \(A_{m}\in\mathbb{S}_{+}^{m}\) and \(X\in\mathbb{R}^{m\times d+1}\) and \(\eta_{m}\in\mathbb{R}^{d}_{++}\), such that with \(m=O(\varepsilon^{-(d+1)/(\beta-2s)}(\log\frac{1}{\varepsilon})^{(d+1)/2})\), we have_

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d} \frac{\partial}{\partial x_{i}}(\mu_{i}(x,t)\tilde{p}(x,t))+(-\Delta)^{s}\tilde {p}(x,t)\right\|_{L^{2}(\tilde{\mathcal{X}})}\leq O(\varepsilon).\] (12)

Similar results as in Corollary 1 can be obtained for the case fractional Fokker-Planck equations as well. We can directly utilize results from [14] to bound the Wasserstein metric between the solution of two fractional FPE whose drift terms are close in \(L^{\infty}\) metric. However, to find a PSD matrix \(A_{m}\) from finite samples does require two separate sampling procedures which would contribute to the estimation error, (i) sampling to estimate the mean in the estimation of the fractional Laplacian and (ii) sampling of a finite number of data points from the data generating distribution.

**Remark**.: _As far as we know, Theorem 5 represents the first proof of approximation error bounds for the solution of the Fractional Fokker-Planck Equation when the approximated solution is a density function. This significant result allows for the utilization of the algorithm presented in [13] to sample from the approximate solution._Conclusion

In this paper, we study the approximation properties of PSD models in modeling the solutions of FPEs and fractional FPEs. For the FPE, we show that a PSD model of size \(m=\tilde{O}(\varepsilon^{-(d+1)/(\beta-2)})\) can approximate the true solution density up to order \(\varepsilon\) in \(L^{2}\) metric where \(\beta\) is the order of smoothness of the true solution. Furthermore, we extend our result to the fractional FPEs to show that the required model size to achieve \(\varepsilon\)-approximation error to the solution density is \(m=\tilde{O}(\varepsilon^{-(d+1)/(\beta-2s)})\) where \(s\) is the fractional order of the fractional Laplacian. In the process, we also show that PSD model-based representations for probability densities allow an easy way to approximate the fractional Laplacian operator (non-local operator) acting on probability density. As a future research direction, we would like to investigate and obtain a finite sample bound on the estimation error under regularity conditions on the drift term \(\mu\).

## Acknowledgement

Anant Raj is supported by the a Marie Sklodowska-Curie Fellowship (project NN-OVEROPT 101030817). Umut Simsekli's research is supported by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute) and the European Research Council Starting Grant DYNASTY - 101039676. Alessandro Rudi acknowleges support of the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute)and support of the European Research Council (grant REAL 947908).

## References

* [AA20] Omar Abu Arapub. Numerical simulation of time-fractional partial differential equations arising in fluid flows via reproducing kernel method. _International Journal of Numerical Methods for Heat & Fluid Flow_, 30(11):4711-4733, 2020.
* [AAS20] Omar Abu Arapub and Mohammed Al-Smadi. An adaptive numerical approach for the solutions of fractional advection-diffusion and dispersion equations in singular case under riesz's derivative operator. _Physica A: Statistical Mechanics and its Applications_, 540:123257, 2020.
* [Ame14] William F Ames. _Numerical methods for partial differential equations_. Academic press, 2014.
* [Arq18] Omar Abu Arapub. Numerical solutions for the robin time-fractional partial differential equations of heat and fluid flows based on the reproducing kernel algorithm. _International Journal of Numerical Methods for Heat & Fluid Flow_, 28(4):828-856, 2018.
* [ASA19] Mohammed Al-Smadi and Omar Abu Arapub. Computational algorithm for solving fredholm time-fractional partial integrodifferential equations of dirichlet functions type with error estimates. _Applied Mathematics and Computation_, 342:280-294, 2019.
* [Bar01] E Barkai. Fractional fokker-planck equation, solution, and application. _Physical Review E_, 63(4):046118, 2001.
* [BCE\({}^{+}\)22] Krishna Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang. Towards a theory of non-log-concave sampling: first-order stationarity guarantees for langevin monte carlo. In _Conference on Learning Theory_, pages 2896-2923. PMLR, 2022.
* [BCRB22] Eloise Berthier, Justin Carpentier, Alessandro Rudi, and Francis Bach. Infinite-dimensional sums-of-squares for optimal control. In _2022 IEEE 61st Conference on Decision and Control (CDC)_, pages 577-582. IEEE, 2022.
* [BDMP17] Nicolas Brosse, Alain Durmus, Eric Moulines, and Marcelo Pereyra. Sampling from a log-concave distribution with compact support with proximal langevin monte carlo. In _Conference on learning theory_, pages 319-342. PMLR, 2017.
* [BEL15] Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Finite-time analysis of projected langevin monte carlo. _Advances in Neural Information Processing Systems_, 28, 2015.
* [BH17] Jake Bouvrie and Boumediene Hamzi. Kernel methods for the approximation of some key quantities of nonlinear systems. _Journal of Computational Dynamics_, 4(1&2):1-19, 2017.

* [BHJK20] Christian Beck, Martin Hutzenthaler, Arnulf Jentzen, and Benno Kuckuck. An overview on deep learning-based approximation methods for partial differential equations. _arXiv preprint arXiv:2012.12348_, 2020.
* [BKO\({}^{+}\)96] Ted Belytschko, Yury Krongauz, Daniel Organ, Mark Fleming, and Petr Krysl. Meshless methods: an overview and recent developments. _Computer methods in applied mechanics and engineering_, 139(1-4):3-47, 1996.
* [BQL\({}^{+}\)22] Kaijun Bao, Xu Qian, Ziyuan Liu, Haifeng Wang, and Songhe Song. Function-valued rkhs-based operator learning for differential equations. _arXiv preprint arXiv:2202.09488_, 2022.
* [BRS16] Vladimir I Bogachev, Michael Rockner, and Stanislav V Shaposhnikov. Distances between transition probabilities of diffusions and applications to nonlinear fokker-planck-kolmogorov equations. _Journal of Functional Analysis_, 271(5):1262-1300, 2016.
* [CDJB20] Niladri Chatterji, Jelena Diakonikolas, Michael I Jordan, and Peter Bartlett. Langevin monte carlo without smoothness. In _International Conference on Artificial Intelligence and Statistics_, pages 1716-1726. PMLR, 2020.
* [CEL\({}^{+}\)21] Sinho Chewi, Murat A Erdogdu, Mufan Bill Li, Ruoqi Shen, and Matthew Zhang. Analysis of langevin monte carlo from poincar\(\backslash\)'e to log-sobolev. _arXiv preprint arXiv:2112.12662_, 2021.
* [CHOS21] Yifan Chen, Bamdad Hosseini, Houman Owhadi, and Andrew M Stuart. Solving and learning nonlinear pdes with gaussian processes. _Journal of Computational Physics_, 447:110668, 2021.
* [CLL21] Ziang Chen, Jianfeng Lu, and Yulong Lu. On the representation of solutions to elliptic pdes in barron spaces. _Advances in neural information processing systems_, 34:6454-6465, 2021.
* [CMW\({}^{+}\)21] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (pinns) for fluid mechanics: A review. _Acta Mechanica Sinica_, 37(12):1727-1738, 2021.
* [Dal17] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, pages 651-676, 2017.
* [DK19] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. _Stochastic Processes and their Applications_, 129(12):5278-5311, 2019.
* [DM14] Mehdi Dehghan and Vahid Mohammadi. The numerical solution of fokker-planck equation with radial basis functions (rbfs) based on the meshless technique of kansas approach and galerkin method. _Engineering Analysis with Boundary Elements_, 47:38-63, 2014.
* [DM17] Alain Durmus and Eric Moulines. Nonasymptotic convergence analysis for the unadjusted langevin algorithm. 2017.
* [DM19] Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted langevin algorithm. 2019.
* [DMM19] Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of langevin monte carlo via convex optimization. _The Journal of Machine Learning Research_, 20(1):2666-2711, 2019.
* [DRM22] Tim De Ryck and Siddhartha Mishra. Generic bounds on the approximation error for physics-informed (and) operator learning. _arXiv preprint arXiv:2205.11393_, 2022.
* [Dua15] Jinqiao Duan. _An introduction to stochastic dynamics_, volume 51. Cambridge University Press, 2015.
* [EH21] Murat A Erdogdu and Rasa Hosseinzadeh. On the convergence of langevin monte carlo: The interplay between tail growth and smoothness. In _Conference on Learning Theory_, pages 1776-1822. PMLR, 2021.
* [Eva22] Lawrence C Evans. _Partial differential equations_, volume 19. American Mathematical Society, 2022.
* [Fas07] Gregory E Fasshauer. _Meshfree approximation methods with MATLAB_, volume 6. World Scientific, 2007.
* [FF15] Bengt Fornberg and Natasha Flyer. Solving pdes with radial basis functions. _Acta Numerica_, 24:215-258, 2015.

* [FGN13] Xiaobing Feng, Roland Glowinski, and Michael Neilan. Recent developments in numerical methods for fully nonlinear second order partial differential equations. _siam REVIEW_, 55(2):205-267, 2013.
* [GW79] Ian Gladwell and R Wait. A survey of numerical methods for partial differential equations. _Oxford: Clarendon Press_, 1979.
* [HF08] Isom Herron and Michael Foster. _Partial differential equations in fluid dynamics_. Cambridge Univ. Press, 2008.
* [HJE18] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. _Proceedings of the National Academy of Sciences_, 115(34):8505-8510, 2018.
* [HKRC18] Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored langevin dynamics. _Advances in Neural Information Processing Systems_, 31, 2018.
* [HMW21] Lu-Jing Huang, Mateusz B Majka, and Jian Wang. Approximation of heavy-tailed distributions via stable-driven sdes. 2021.
* [Ise09] Arieh Iserles. _A first course in the numerical analysis of differential equations_. Number 44. Cambridge university press, 2009.
* [KKL\({}^{+}\)21] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* [KLY21] Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artificial neural networks. _European Journal of Applied Mathematics_, 32(3):421-435, 2021.
* [KPRS22] Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider. A theoretical analysis of deep neural networks and parametric pdes. _Constructive Approximation_, 55(1):73-125, 2022.
* [KRP12] S Kazem, JA Rad, and K Parand. Radial basis functions methods for solving fokker-planck equation. _Engineering Analysis with Boundary Elements_, 36(2):181-189, 2012.
* [Lam21] Andrew Lamperski. Projected stochastic gradient langevin algorithms for constrained sampling and non-convex learning. In _Conference on Learning Theory_, pages 2891-2937. PMLR, 2021.
* [LAT04] Fawang Liu, Vo Anh, and Ian Turner. Numerical solution of the space fractional fokker-planck equation. _Journal of Computational and Applied Mathematics_, 166(1):209-219, 2004.
* [LCL\({}^{+}\)21] Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, and Jose Blanchet. Machine learning for elliptic pdes: fast rate generalization bound, neural scaling law and minimax optimality. _arXiv preprint arXiv:2110.06897_, 2021.
* [LL22] Jianfeng Lu and Yulong Lu. A priori generalization error analysis of two-layer neural networks for solving high dimensional schrodinger eigenvalue problems. _Communications of the American Mathematical Society_, 2(01):1-21, 2022.
* [LLMD18] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In _International conference on machine learning_, pages 3208-3216. PMLR, 2018.
* [LLW21] Jianfeng Lu, Yulong Lu, and Min Wang. A priori generalization analysis of the deep ritz method for solving high dimensional elliptic equations. _arXiv preprint arXiv:2101.01708_, 2021.
* [LT00] Irena Lasiecka and Roberto Triggiani. _Control theory for partial differential equations: Volume 1, Abstract parabolic systems: Continuous and approximation theories_, volume 1. Cambridge University Press, 2000.
* [MCF15] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. _Advances in neural information processing systems_, 28, 2015.
* [MFBR20] Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Non-parametric models for non-negative functions. _Advances in neural information processing systems_, 33:12816-12826, 2020.
* [MFBR22] Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Sampling from arbitrary functions via psd models. In _International Conference on Artificial Intelligence and Statistics_, pages 2823-2861. PMLR, 2022.
* [MP18] Carlo Morosi and Livio Pizzocchero. On the constants for some fractional gagliardo-nirenberg and sobolev inequalities. _Expositiones Mathematicae_, 36(1):32-77, 2018.

* [MVB\({}^{+}\)21] Boris Muzellec, Adrien Vacher, Francis Bach, Francois-Xavier Vialard, and Alessandro Rudi. Near-optimal estimation of smooth transport maps with kernel sums-of-squares. _arXiv preprint arXiv:2112.01907_, 2021.
* [NF21] Christopher Nemeth and Paul Fearnhead. Stochastic gradient markov chain monte carlo. _Journal of the American Statistical Association_, 116(533):433-450, 2021.
* [Nir11] Louis Nirenberg. On elliptic partial differential equations. In _Il principio di minimo e sue applicazioni alle equazioni funzionali_, pages 1-48. Springer, 2011.
* [NSGR19] Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gael Richard. First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise. _Advances in neural information processing systems_, 32, 2019.
* [NSR19] Than Huy Nguyen, Umut Simsekli, and Gael Richard. Non-asymptotic analysis of fractional langevin monte carlo for non-convex optimization. In _International Conference on Machine Learning_, pages 4810-4819. PMLR, 2019.
* [Pav14] Grigorios A Pavliotis. _Stochastic processes and applications: diffusion processes, the Fokker-Planck and Langevin equations_, volume 60. Springer, 2014.
* [PMB13] Lukas Pichler, Arif Masud, and Lawrence A Bergman. Numerical solution of the fokker-planck equation by finite difference and finite element methods--a comparative study. _Computational Methods in Stochastic Dynamics: Volume 2_, pages 69-85, 2013.
* [PR20] Vivek G Patel and Nikunj V Rachchh. Meshless method-review on recent developments. _Materials today: proceedings_, 26:1598-1603, 2020.
* [PRDVR20] Nicolo Pagliana, Alessandro Rudi, Ernesto De Vito, and Lorenzo Rosasco. Interpolation and learning with scale dependent kernels. _arXiv preprint arXiv:2006.09984_, 2020.
* [RC21] Alessandro Rudi and Carlo Ciliberto. Psd representations for effective probability models. _Advances in Neural Information Processing Systems_, 34, 2021.
* [RMFB20] Alessandro Rudi, Ulysse Marteau-Ferey, and Francis Bach. Finding global minima via kernel approximations. _arXiv preprint arXiv:2012.11978_, 2020.
* [RPK19] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* [RR96] Hannes Risken and Hannes Risken. _Fokker-planck equation_. Springer, 1996.
* [RRT17] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In _Conference on Learning Theory_, pages 1674-1703. PMLR, 2017.
* [Rud17] Walter Rudin. _Fourier analysis on groups_. Courier Dover Publications, 2017.
* [RZ10] Christian Rieger and Barbara Zwicknagl. Sampling inequalities for infinitely smooth functions, with applications to interpolation and machine learning. _Advances in Computational Mathematics_, 32(1):103-129, 2010.
* [RZGS23] Anant Raj, Lingjiong Zhu, Mert Gurbuzbalaban, and Umut Simsekli. Algorithmic stability of heavy-tailed sgd with general loss functions. _arXiv preprint arXiv:2301.11885_, 2023.
* [SAA20] Oleg Szehr, Dario Azzimonti, and Laura Azzimonti. An exact kernel framework for spatio-temporal dynamics. _arXiv preprint arXiv:2011.06848_, 2020.
* [Sch07] R Schaback. Kernel-based meshless methods. _Lecture Notes for Taught Course in Approximation Theory. Georg-August-Universitat Gottingen_, 2007.
* [Sim17] Umut Simsekli. Fractional langevin monte carlo: Exploring levy driven stochastic differential equations for markov chain monte carlo. In _International Conference on Machine Learning_, pages 3200-3209. PMLR, 2017.
* [SR20] Adil Salim and Peter Richtarik. Primal dual interpretation of the proximal stochastic gradient langevin algorithm. _Advances in Neural Information Processing Systems_, 33:3786-3796, 2020.

* [SS\({}^{+}\)16] Saburou Saitoh, Yoshihiro Sawano, et al. _Theory of reproducing kernels and applications_. Springer, 2016.
* [SS22] B Sepehrian and Z Shamohammadi. Solution of the liouville-caputo time-and reisz space-fractional fokker-planck equation via radial basis functions. _Asian-European Journal of Mathematics_, 15(11):2250195, 2022.
* [SSDE20] Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension, heavy tails, and generalization in neural networks. _Advances in Neural Information Processing Systems_, 33:5138-5151, 2020.
* [SSG19] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. In _International Conference on Machine Learning_, pages 5827-5837. PMLR, 2019.
* [Ste21] George Stepaniants. Learning partial differential equations in reproducing kernel hilbert spaces. _arXiv preprint arXiv:2108.11580_, 2021.
* [STL96] Gennady Samorodnitsky, Murad S Taqqu, and RW Linde. Stable non-gaussian random processes: stochastic models with infinite variance. _Bulletin of the London Mathematical Society_, 28(134):554-555, 1996.
* [SW06] Robert Schaback and Holger Wendland. Kernel techniques: from machine learning to meshless methods. _Acta numerica_, 15:543-639, 2006.
* [Tad12] Eitan Tadmor. A review of numerical methods for nonlinear partial differential equations. _Bulletin of the American Mathematical Society_, 49(4):507-554, 2012.
* [Tro10] Fredi Troltzsch. _Optimal control of partial differential equations: theory, methods, and applications_, volume 112. American Mathematical Soc., 2010.
* [TSIG22] Armin Tabandeh, Neetesh Sharma, Leandro Iannacone, and Paolo Gardoni. Numerical solution of the fokker-planck equation using physics-based mixture models. _Computer Methods in Applied Mechanics and Engineering_, 399:115424, 2022.
* [UHK18] Sabir Umarov, Marjorie Hahn, and Kei Kobayashi. _Beyond the Triangle: Brownian Motion, Ito Calculus, and Fokker-Planck Equation-Fractional Generalizations_. World Scientific, 2018.
* [VMR\({}^{+}\)21] Adrien Vacher, Boris Muzzellec, Alessandro Rudi, Francis Bach, and Francois-Xavier Vialard. A dimension-free computational upper-bound for smooth optimal transport estimation. In _Conference on Learning Theory_, pages 4143-4173. PMLR, 2021.
* [Wen04] Holger Wendland. _Scattered data approximation_, volume 17. Cambridge university press, 2004.
* [WWLZ22] Jia-Li Wei, Guo-Cheng Wu, Bao-Qing Liu, and Zhengang Zhao. New semi-analytical solutions of the time-fractional fokker-planck equation by the neural network method. _Optik_, 259:168896, 2022.
* [XCZG18] Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. _Advances in Neural Information Processing Systems_, 31, 2018.
* [XZL\({}^{+}\)20] Yong Xu, Hao Zhang, Yongge Li, Kuang Zhou, Qi Liu, and Jurgen Kurths. Solving fokker-planck equation using deep learning. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 30(1):013133, 2020.
* [Y\({}^{+}\)18] Bing Yu et al. The deep ritz method: a deep learning-based numerical algorithm for solving variational problems. _Communications in Mathematics and Statistics_, 6(1):1-12, 2018.
* [Yan13] Limei Yan. Numerical solutions of fractional fokker-planck equations using iterative laplace transform method. In _Abstract and applied analysis_, volume 2013. Hindawi, 2013.
* [ZDL22] Jiayu Zhai, Matthew Dobson, and Yao Li. A deep learning method for solving fokker-planck equations. In _Mathematical and Scientific Machine Learning_, pages 568-597. PMLR, 2022.
* [ZFM\({}^{+}\)20] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically understanding why sgd generalizes better than adam in deep learning. _Advances in Neural Information Processing Systems_, 33:21285-21296, 2020.

* [Zho08] Ding-Xuan Zhou. Derivative reproducing properties for kernel methods in learning theory. _Journal of computational and Applied Mathematics_, 220(1-2):456-463, 2008.
* [ZL22] Yuping Zheng and Andrew Lamperski. Constrained langevin algorithms with l-mixing external random variables. _arXiv preprint arXiv:2205.14192_, 2022.
* [ZZ22] Xiaolong Zhang and Xicheng Zhang. Ergodicity of supercritical sdes driven by alpha-stable processes and heavy-tailed sampling. _arXiv preprint arXiv:2201.10158_, 2022.

Appendix

## Appendix A Existence of an appropriate PSD matrix \(M_{\varepsilon}\) (Fokker-Planck Equation)

### Useful Results

Consider the mollifier function \(g\) defined as in [10].

\[g(x)=\frac{2^{-d/2}}{V_{d}}\|x\|^{-d}J_{d/2}(2\pi\|x\|)J_{d/2}(4\pi\|x\|),\] (13)

where \(J_{d/2}\) is the Bessel function of the first kind of order \(d/2\) and \(V_{d}=\int_{\|x\|\leq 1}dx=\frac{\pi^{d/2}}{\Gamma(d/2+1)}\). \(g_{v}(x)=v^{-d}g(x/v)\).

**Lemma 1** ([10], Lemma D.3).: _The function \(g\) defined above satisfies \(g\in L^{1}(\mathbb{R}^{d})\cap L^{2}(\mathbb{R}^{d})\) and \(\int g(x)dx=1\). Moreover, for any \(\omega\in\mathbb{R}^{d}\), we have_

\[\mathbf{1}_{\{\|\omega\|<1\}}(\omega)\leq\mathcal{F}[g](\omega)\leq\mathbf{1 }_{\{\|\omega\|\leq 3\}}(\omega).\]

The following proposition provides a useful characterization of the space \(W^{\beta}_{2}(\mathbb{R}^{d})\)

**Proposition 4** (Characterization of the Sobolev space \(W^{k}_{2}(\mathbb{R}^{d})\), [21]).: _Let \(k\in\mathbb{N}\). The norm of the Sobolev space \(\|\cdot\|_{W^{k}_{2}(\mathbb{R}^{d})}\) is equivalent to the following norm_

\[\|f\|^{\prime 2}_{W^{k}_{2}(\mathbb{R}^{d})}=\int_{\mathbb{R}^{d}}\ |\mathcal{F}[f]( \omega)|^{2}\ (1+\|\omega\|^{2})^{k}\ d\omega,\quad\forall f\in L^{2}(\mathbb{R}^{d})\] (14)

_and satisfies_

\[\tfrac{1}{(2\pi)^{2k}}\|f\|^{2}_{W^{k}_{2}(\mathbb{R}^{d})}\leq\|f\|^{\prime} _{W^{k}_{2}(\mathbb{R}^{d})}\ \leq\ 2^{2k}\|f\|^{2}_{W^{k}_{2}(\mathbb{R}^{d})},\quad\forall f\in L^{2}(\mathbb{R }^{d}).\] (15)

_Moreover, when \(k>d/2\), then \(W^{k}_{2}(\mathbb{R}^{d})\) is a reproducing kernel Hilbert space._

**Lemma 2**.: _Consider the definition of \(g_{v}(x)=v^{-d}g(x/v)\) where \(g\) is defined in equation (13). Let \(\beta>2,q\in\mathbb{N}\). Let \(f_{1},\dots,f_{q}\in W^{\beta}_{2}(\mathbb{R}^{d})\cap L^{\infty}(\mathbb{R}^ {d})\) and the function \(p^{\star}=\sum_{i=1}^{q}f_{i}^{2}\). Let us assume that the Assumption 2 is also satisfied. Let \(\varepsilon\in(0,1]\) and let \(\eta\in\mathbb{R}^{d}_{++}\). Let \(\phi_{X}\gamma_{E}\) be the feature map of the Gaussian kernel with bandwidth \(\eta\) and let \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\) be the associated RKHS. Then there exists \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_{T})\) with \(\operatorname{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\hat{p}(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds for \(v>0\) and \(k\in\{1,\cdots d\}\),_

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}-\frac{\partial p ^{\star}(x,t)}{\partial t}\right\|_{L^{1}(\mathbb{R}^{d})}\leq 8\pi(2v)^{(\beta-1)} \sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_{i}\|_{L_{2}( \mathbb{R}^{d})}\] \[\qquad\qquad\qquad+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{ \beta}_{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{i}}{\partial t}\right\|_{L _{2}(\mathbb{R}^{d})}\|g\|_{L_{1}(\mathbb{R}^{d})},\] (16) \[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d})}\leq 8\pi(2v)^{(\beta-1)} \sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_{i}\|_{L^{\infty}( \mathbb{R}^{d})}\] \[\qquad\qquad\qquad+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{ \beta}_{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{i}}{\partial t}\right\|_{L ^{\infty}(\mathbb{R}^{d})}\|g\|_{L_{1}(\mathbb{R}^{d})}\] (17) \[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{k}}\right\|_{L^{1}(\mathbb{R}^{d})}\leq 8\pi(2v)^{( \beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_{i}\|_{L_{2}( \mathbb{R}^{d})}\] \[\qquad\qquad\qquad+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{ \beta}_{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{i}}{\partial x_{k}}\right\| _{L_{2}(\mathbb{R}^{d})}\|g\|_{L_{1}(\mathbb{R}^{d})}\] (18) \[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{k}}\right\|_{L^{2}(\mathbb{R}^{d})}\leq 8\pi(2v)^{( \beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_{i}\|_{L^{ \infty}(\mathbb{R}^{d})}\] (19)\[\left\|\frac{\partial}{\partial x_{i}}f-\frac{\partial}{\partial x_{i}}(f \star g_{v})\right\|_{L_{2}(\mathbb{R}^{d})}^{2}\leq\frac{16\pi^{2}(2v)^{2( \beta-1)}}{(1+v^{2})^{\beta-1}}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{d})}^{2}\] (22)

We now bound \(\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial\hat{p}(x,t)}{ \partial t}\right\|_{L^{\prime}(\mathbb{R}^{d})}\) and \(\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{i}}-\frac{\partial\hat{p}(x, t)}{\partial x_{i}}\right\|_{L^{\prime}(\mathbb{R}^{d})}\).

As in [10], let us denote, \(f_{i}\star g_{v}\) as \(f_{i,v}\). We consider

\[M_{\varepsilon}=\sum_{i=1}^{q}f_{i,v}f_{i,v}^{\top}.\]

Hence,

\[\hat{p}(x,t)=f(x,t)=\phi_{X,T}(x,t)^{\top}M_{\varepsilon}\phi_{X,T}(x,t)=\sum _{i=1}^{q}f_{i,v}^{2}(x,t).\]

This gives,

\[\frac{\partial}{\partial x_{i}}p^{\star}-\frac{\partial}{\partial x _{i}}\hat{p} =2\sum_{i=1}^{q}\left[f_{i}\frac{\partial f_{i}}{\partial x_{i}} -f_{i,v}\frac{\partial f_{i,v}}{\partial x_{i}}\right]=2\sum_{i=1}^{q}\left[f _{i}\frac{\partial f_{i}}{\partial x_{i}}-f_{i}\frac{\partial f_{i,v}}{ \partial x_{i}}+f_{i}\frac{\partial f_{i,v}}{\partial x_{i}}-f_{i,v}\frac{ \partial f_{i,v}}{\partial x_{i}}\right]\] \[=2\sum_{i=1}^{q}\left[f_{i}\left(\frac{\partial f_{i}}{\partial x _{i}}-\frac{\partial f_{i,v}}{\partial x_{i}}\right)+(f_{i}-f_{i,v})\frac{ \partial f_{i,v}}{\partial x_{i}}\right]\]Hence, we have

\[\left\|\frac{\partial}{\partial x_{i}}p^{\star}-\frac{\partial}{ \partial x_{i}}\hat{p}\right\|_{L_{1}(\mathbb{R}^{d})}\leq 2\sum_{i=1}^{q}\left\| \frac{\partial f_{i}}{\partial x_{i}}-\frac{\partial f_{i,v}}{\partial x_{i}} \right\|_{L_{2}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L_{2}(\mathbb{R}^{d})}+ 2\sum_{i=1}^{q}\left\|f_{i}-f_{i,v}\right\|_{L_{2}(\mathbb{R}^{d})}\left\| \frac{\partial f_{i,v}}{\partial x_{i}}\right\|_{L_{2}(\mathbb{R}^{d})}.\] (24)

Similarly,

\[\left\|\frac{\partial}{\partial t}p^{\star}-\frac{\partial}{ \partial t}\hat{p}\right\|_{L_{1}(\mathbb{R}^{d})}\leq 2\sum_{i=1}^{q}\left\| \frac{\partial f_{i}}{\partial t}-\frac{\partial f_{i,v}}{\partial t}\right\| _{L_{2}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L_{2}(\mathbb{R}^{d})}+2\sum_ {i=1}^{q}\left\|f_{i}-f_{i,v}\right\|_{L_{2}(\mathbb{R}^{d})}\left\|\frac{ \partial f_{i,v}}{\partial t}\right\|_{L_{2}(\mathbb{R}^{d})}.\] (25)

Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{1}(\mathbb{R}^{d})}\leq 2\sum_{i=1}^{q} \left\|\frac{\partial f_{i}}{\partial t}-\frac{\partial f_{i,v}}{\partial t} \right\|_{L^{2}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L^{2}(\mathbb{R}^{d})}+ 2\sum_{i=1}^{q}\left\|f_{i}-f_{i,v}\right\|_{L^{2}(\mathbb{R}^{d})}\left\| \frac{\partial f_{i,v}}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d})}\] \[\leq 8\pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2} ^{\beta}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L_{2}(\mathbb{R}^{d})}+2(2v)^{ \beta}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2}^{\beta}(\mathbb{R}^{d})} \left\|\frac{\partial f_{i,v}}{\partial t}\right\|_{L_{2}(\mathbb{R}^{d})}\] \[\leq 8\pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2} ^{\beta}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L_{2}(\mathbb{R}^{d})}+2(2v)^{ \beta}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2}^{\beta}(\mathbb{R}^{d})} \left\|\frac{\partial f_{i}}{\partial t}\right\|_{L_{2}(\mathbb{R}^{d})}\] \[=8\pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2}^{ \beta}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L_{2}(\mathbb{R}^{d})}+2(2v)^{ \beta}\sum_{i=1}^{q}\left\|f_{i}\right\|_{W_{2}^{\beta}(\mathbb{R}^{d})} \left\|\frac{\partial f_{i}}{\partial t}\right\|_{L_{2}(\mathbb{R}^{d})}\left\| g\right\|_{L_{1}(\mathbb{R}^{d})}.\]

If we choose

\[v=\min\left(\left(\frac{\varepsilon}{2^{\beta-1}16\pi C_{1}} \right)^{\frac{1}{\beta-1}},\left(\frac{\varepsilon}{2^{\beta+1}C_{2}}\right)^ {\frac{1}{\beta}}\right)\] (26)

where \(C_{1}=\sum_{i=1}^{q}\|f_{i}\|_{W_{2}^{\beta}(\mathbb{R}^{d})}\|f_{i}\|_{L_{2}( \mathbb{R}^{d})}\) and \(C_{2}=\sum_{i=1}^{q}\|f_{i}\|_{W_{2}^{\beta}(\mathbb{R}^{d})}\left\|\frac{ \partial f_{i}}{\partial t}\right\|_{L_{2}(\mathbb{R}^{d})}\left\|g\right\|_{L _{1}(\mathbb{R}^{d})}\). Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{1}(\mathbb{R}^{d})}\leq\varepsilon.\]

Similarly,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d})}=2\sum_{i=1}^{q} \left\|\frac{\partial f_{i}}{\partial t}-\frac{\partial f_{i,v}}{\partial t} \right\|_{L^{2}(\mathbb{R}^{d})}\left\|f_{i}\right\|_{L^{\infty}(\mathbb{R}^{d} )}+2\sum_{i=1}^{q}\left\|f_{i}-f_{i,v}\right\|_{L^{2}(\mathbb{R}^{d})}\left\| \frac{\partial f_{i,v}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d})}.\]

Now,

\[\left\|\frac{\partial f_{i,v}}{\partial t}\right\|_{L^{\infty}( \mathbb{R}^{d})}=\left\|\frac{\partial f_{i}}{\partial t}\star g_{v}\right\|_{L^ {\infty}(\mathbb{R}^{d})}\leq\left\|\frac{\partial f_{i}}{\partial t}\right\|_{L ^{\infty}(\mathbb{R}^{d})}\left\|g_{v}\right\|_{L^{1}(\mathbb{R}^{d})}=\left\| \frac{\partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d})}\left\|g \right\|_{L^{1}(\mathbb{R}^{d})}.\]

Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d})}\leq 8\pi(2v)^{(\beta-1)} \sum_{i=1}^{q}\|f_{i}\|_{W_{2}^{\beta}(\mathbb{R}^{d})}\|f_{i}\|_{L^{ \infty}(\mathbb{R}^{d})}\] \[+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W_{2}^{\beta}(\mathbb{R}^{d })}\left\|\frac{\partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d})}\|g \|_{L_{1}(\mathbb{R}^{d})}.\]

If we choose

\[v=\min\left(\left(\frac{\varepsilon}{2^{\beta-1}16\pi C_{1}} \right)^{\frac{1}{\beta-1}},\left(\frac{\varepsilon}{2^{\beta+1}C_{2}}\right)^{ \frac{1}{\beta}}\right)\]where \(C_{1}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_{i}\|_{L^{\infty} (\mathbb{R}^{d})}\) and \(C_{2}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\left\|\frac{ \partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d})}\|g\|_{L_{1}( \mathbb{R}^{d})}\). Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial\hat{p}(x,t)}{ \partial t}\right\|_{L^{2}(\mathbb{R}^{d})}\leq\varepsilon.\]

Similarly,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{k}}\right\|_{L^{1}(\mathbb{R}^{d})}\leq 8 \pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_ {i}\|_{L_{2}(\mathbb{R}^{d})}\] (27) \[+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{ d})}\left\|\frac{\partial f_{i}}{\partial x_{k}}\right\|_{L_{2}(\mathbb{R}^{d})}\|g \|_{L_{1}(\mathbb{R}^{d})}\]

and

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{k}}\right\|_{L^{2}(\mathbb{R}^{d})}\leq 8 \pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\|f_ {i}\|_{L^{\infty}(\mathbb{R}^{d})}\] \[+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{ d})}\left\|\frac{\partial f_{i}}{\partial x_{k}}\right\|_{L^{\infty}(\mathbb{R}^{ d})}\|g\|_{L_{1}(\mathbb{R}^{d})}.\]

**Lemma 3**.: _Consider the definition of \(g_{v}(x)=v^{-d}g(x/v)\) where \(g\) is defined in equation (13). Let \(\beta>2,q\in\mathbb{N}\). Let \(f_{1},\dots,f_{q}\in W^{\beta}_{2}(\mathbb{R}^{d})\cap L^{\infty}(\mathbb{R}^{d})\) and the function \(p^{\star}=\sum_{i=1}^{q}f_{i}^{2}\). Let us assume that the Assumption 2 is also satisfied. Let \(\varepsilon\in(0,1]\) and let \(\eta\in\mathbb{R}^{d}_{++}\). Let \(\phi_{X,T}\) be the feature map of the Gaussian kernel with bandwidth \(\eta\) and let \(\mathcal{H}_{X}\otimes\mathcal{H}_{T}\) be the associated RKHS. Then there exists \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_{T})\) with \(\operatorname{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\hat{p}(x,t)=f(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds for all \(i,j\in\{1,\cdots d\}\),_

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star }(x,t)-\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\hat{p}(x,t)\right\|_{ L^{1}(\mathbb{R}^{d})}\leq\frac{16\pi(2v)^{(\beta-1)}}{(1+v^{2})^{\frac{\beta-1}{2}}}(1 +\|g\|_{L^{1}(\mathbb{R}^{d})})\sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{ d})}\left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{2}(\mathbb{R}^{d})}\] \[+\frac{32\pi^{2}(2v)^{(\beta-2)}}{(1+v^{2})^{\frac{\beta-2}{2}}} \sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\left\|f_{m}\right\|_{L^{2}( \mathbb{R}^{d})}+2(2v)^{\beta}\sum_{m=1}^{q}\|f_{m}\|_{W^{\beta}_{2}(\mathbb{R }^{d})}\left\|\frac{\partial^{2}f_{m}}{\partial x_{i}\partial x_{j}}\right\|_{L^ {2}(\mathbb{R}^{d})}\|g\|_{L^{1}(\mathbb{R}^{d})},\]

_and_

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star }(x,t)-\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\hat{p}(x,t)\right\|_{ L^{2}(\mathbb{R}^{d})}\leq\frac{16\pi(2v)^{(\beta-1)}}{(1+v^{2})^{\frac{\beta-1}{2}}}(1 +\|g\|_{L^{1}(\mathbb{R}^{d})})\sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{ d})}\left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{\infty}( \mathbb{R}^{d})}\] \[+\frac{32\pi^{2}(2v)^{(\beta-2)}}{(1+v^{2})^{\frac{\beta-2}{2}}} \sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\left\|f_{m}\right\|_{L^{ \infty}(\mathbb{R}^{d})}+2(2v)^{\beta}\sum_{m=1}^{q}\|f_{m}\|_{W^{\beta}_{2}( \mathbb{R}^{d})}\left\|\frac{\partial^{2}f_{m}}{\partial x_{i}\partial x_{j}} \right\|_{L^{\infty}(\mathbb{R}^{d})}\|g\|_{L^{1}(\mathbb{R}^{d})}.\]

Proof.: We have \(g\) defined as in [1].

\[g_{v}(x)=\frac{2^{-d/2}}{V_{d}}\|x\|^{-d}J_{d/2}(2\pi\|x\|)J_{d/2}(4\pi\|x\|),\] (28)

where \(J_{d/2}\) is the Bessel function of the first kind of order \(d/2\) and \(V_{d}=\int_{\|x\|\leq 1}dx=\frac{\pi^{d/2}}{\Gamma(d/2+1)}\). \(g_{v}(x)=v^{-d}g(x/v)\). We use result of lemma 1 to prove the statement. We have, for a function \(f\in W^{\beta}_{2}(\mathbb{R}^{d})\)

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f-\frac{ \partial^{2}}{\partial x_{i}x_{j}}(f\star g_{v})\right\|_{L_{2}(\mathbb{R}^{d})}^{2} =\left\|\mathcal{F}\left[\frac{\partial^{2}}{\partial x_{i}\partial x _{j}}f\right]-\mathcal{F}\left[\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}(f \star g_{v})\right]\right\|_{L_{2}(\mathbb{R}^{d})}^{2}\] \[=\left\|4\pi^{2}\omega_{i}\omega_{j}\mathcal{F}[f]-4\pi^{2}\omega_ {i}\omega_{j}\mathcal{F}[f\star g_{v}]\right\|_{L_{2}(\mathbb{R}^{d})}\] \[=\left\|4\pi^{2}\omega_{i}\omega_{j}\mathcal{F}[f](1-\mathcal{F}[g _{v}])\right\|_{L_{2}(\mathbb{R}^{d})}^{2}\]\[=16\pi^{4}\int_{\mathbb{R}^{d}}\omega_{i}^{2}\omega_{j}^{2}|\mathcal{ F}[f](\omega)|^{2}|1-\mathcal{F}[g](\epsilon\omega)|^{2}\;d\omega\] \[\leq 16\pi^{4}\pi^{2}\int_{t\|\omega\|\geq 1}\omega_{i}^{2}\omega_{j} ^{2}|\mathcal{F}[f](\omega)|^{2}\;d\omega\] \[=16\pi^{4}\int_{v\|\omega\|\geq 1}\omega_{i}^{2}\omega_{j}^{2}(1+\| \omega\|^{2})^{-\beta}(1+\|\omega\|^{2})^{\beta}|\mathcal{F}[f](\omega)|^{2}\;d\omega\] \[\leq 16\pi^{4}\sup_{v\|\omega\|\geq 1}\frac{\omega_{i}^{2}\omega_{j} ^{2}}{(1+\|\omega\|^{2})^{\beta}}\int(1+\|\omega\|^{2})^{\beta}|\mathcal{F}[f ](\omega)|^{2}\;d\omega\] \[\leq 16\pi^{4}\sup_{v\|\omega\|\geq 1}\frac{\omega_{i}^{2} \omega_{j}^{2}}{(1+\|\omega\|^{2})^{\beta}}2^{2\beta}\|f\|_{W_{2}^{\beta}( \mathbb{R}^{d})}^{2}\] \[\leq 16\pi^{4}\sup_{v\|\omega\|\geq 1}\frac{1}{(1+\|\omega\|^{2}) ^{\beta-2}}2^{2\beta}\|f\|_{W_{2}^{\beta}(\mathbb{R}^{d})}^{2}\] \[\leq\frac{256\pi^{4}(2v)^{2(\beta-2)}}{(1+v^{2})^{\beta-2}}\|f\|_ {W_{2}^{\beta}(\mathbb{R}^{d})}^{2}.\]

As in [10], let us denote, \(f_{i}\star g_{v}\) as \(f_{i,v}\). We consider

\[M_{\varepsilon}=\sum_{i=1}^{q}f_{i,v}f_{i,v}^{\top}.\]

Hence,

\[\hat{p}(x,t)=f(x,t)=\phi_{X,T}(x,t)^{\top}M_{\varepsilon}\phi_{X,T}(x,t)=\sum_ {i=1}^{q}f_{i,v}^{2}(x,t).\]

Now we have,

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star }(x,t)-\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f(x,t)\right\|_{L^{1} (\mathbb{R}^{d})}=\left\|2\sum_{m=1}^{q}\left[\frac{\partial f_{m}}{\partial x _{i}}\frac{\partial f_{m}}{\partial x_{j}}-\frac{\partial f_{m,v}}{\partial x _{i}}\frac{\partial f_{m,v}}{\partial x_{j}}\right.\right.\] \[\left.\left.+f_{m}\frac{\partial^{2}f_{m}}{\partial x_{i} \partial x_{j}}-f_{m,v}\frac{\partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j }}\right]\right\|_{L^{1}(\mathbb{R}^{d})}\] \[\leq 2\sum_{m=1}^{q}\left[\left\|\frac{\partial f_{m}}{\partial x _{i}}-\frac{\partial f_{m,v}}{\partial x_{i}}\right\|_{L^{2}(\mathbb{R}^{d})} \left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{2}(\mathbb{R}^{d})}+ \left\|\frac{\partial f_{m}}{\partial x_{j}}-\frac{\partial f_{m,v}}{\partial x _{j}}\right\|_{L^{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{m,v}}{\partial x _{j}}\right\|_{L^{2}(\mathbb{R}^{d})}\right]\] \[+2\sum_{m=1}^{q}\left[\left\|\frac{\partial^{2}f_{m}}{\partial x _{i}\partial x_{j}}-\frac{\partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j}} \right\|_{L^{2}(\mathbb{R}^{d})}\left\|f_{m}\right\|_{L^{2}(\mathbb{R}^{d})}+ \left\|f_{m}-f_{m,v}\right\|_{L^{2}(\mathbb{R}^{d})}\left\|\frac{\partial^{2} f_{m,v}}{\partial x_{i}\partial x_{j}}\right\|_{L^{2}(\mathbb{R}^{d})}\right]\]

Similarly,

\[\left\|\sum_{i=1}^{d}\sum_{j=1}^{d}\left[\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}p^{\star}(x,t))-\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}f(x,t))\right]\right\|_{L_{1}(\mathbb{R}^{ d})}\] \[\leq\sup_{\begin{subarray}{c}i,j\\ :=R_{d}\end{subarray}}D_{ij}\sum_{i=1}^{d}\sum_{j=1}^{d}\left\|\frac{\partial^{ 2}}{\partial x_{i}\partial x_{j}}p^{\star}(x,t)-\frac{\partial^{2}}{\partial x _{i}\partial x_{j}}f(x,t)\right\|_{L_{1}(\mathbb{R}^{d})}.\] (29)

Now,

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star}(x,t)-\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f(x,t)\right\|_{L^{1}( \mathbb{R}^{d})}\] \[\leq 2\sum_{m=1}^{q}\left[\left\|\frac{\partial f_{m}}{\partial x _{i}}-\frac{\partial f_{m,v}}{\partial x_{i}}\right\|_{L^{2}(\mathbb{R}^{d})} \left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{2}(\mathbb{R}^{d})}+ \left\|\frac{\partial f_{m}}{\partial x_{j}}-\frac{\partial f_{m,v}}{\partial x_{j }}\right\|_{L^{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{m,v}}{\partial x_{j}} \right\|_{L^{2}(\mathbb{R}^{d})}\right]\]

[MISSING_PAGE_EMPTY:21]

\[+f_{m}\frac{\partial^{2}f_{m}}{\partial x_{i}\partial x_{j}}-f_{m,v} \frac{\partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j}}\bigg{\|}_{L^{2}(\mathbb{ R}^{d})}\] \[\leq 2\sum_{m=1}^{q}\left[\left\|\frac{\partial f_{m}}{\partial x_{i }}-\frac{\partial f_{m,v}}{\partial x_{i}}\right\|_{L^{2}(\mathbb{R}^{d})} \left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{\infty}(\mathbb{R}^{d} )}+\left\|\frac{\partial f_{m}}{\partial x_{j}}-\frac{\partial f_{m,v}}{ \partial x_{j}}\right\|_{L^{2}(\mathbb{R}^{d})}\left\|\frac{\partial f_{m,v}}{ \partial x_{j}}\right\|_{L^{\infty}(\mathbb{R}^{d})}\right]\] \[+2\sum_{m=1}^{q}\left[\left\|\frac{\partial^{2}f_{m}}{\partial x _{i}\partial x_{j}}-\frac{\partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j}} \right\|_{L^{2}(\mathbb{R}^{d})}\left\|f_{m}\right\|_{L^{\infty}(\mathbb{R}^{ d})}+\left\|f_{m}-f_{m,v}\right\|_{L^{2}(\mathbb{R}^{d})}\left\|\frac{ \partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j}}\right\|_{L^{\infty}( \mathbb{R}^{d})}\right]\]

Let us now compute the following,

\[\left\|\frac{\partial f_{m,v}}{\partial x_{j}}\right\|_{L^{\infty}(\mathbb{R}^ {d})}\leq\left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{\infty}( \mathbb{R}^{d})}\|g\|_{L^{1}(\mathbb{R}^{d})}\]

and

\[\left\|\frac{\partial^{2}f_{m,v}}{\partial x_{i}\partial x_{j}} \right\|_{L^{\infty}(\mathbb{R}^{d})}\leq\left\|\frac{\partial^{2}f_{m}}{ \partial x_{i}\partial x_{j}}\right\|_{L^{\infty}(\mathbb{R}^{d})}\|g\|_{L^{1} (\mathbb{R}^{d})}\]

Similarly,

\[\left\|\sum_{i=1}^{d}\sum_{j=1}^{d}\left[\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}p^{\star}(x,t))-\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}f(x,t))\right]\right\|_{L^{2}(\mathbb{R}^ {d})}\] \[\leq\sup_{\begin{subarray}{c}i,j\\ :=R_{d}\end{subarray}}D_{ij}\sum_{i=1}^{d}\sum_{j=1}^{d}\left\|\frac{\partial ^{2}}{\partial x_{i}\partial x_{j}}p^{\star}(x,t)-\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}f(x,t)\right\|_{L^{2}(\mathbb{R}^{d})}.\] (30)

Combining everything together, we have

\[\left\|\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star} (x,t)-\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f(x,t)\right\|_{L^{2}( \mathbb{R}^{d})}\leq\frac{16\pi(2v)^{(\beta-1)}}{(1+v^{2})^{\frac{\beta-1}{ 2}}}(1+\|g\|_{L^{1}(\mathbb{R}^{d})})\sum_{m=1}^{q}\|f\|_{W_{2}^{\beta}(\mathbb{ R}^{d})}\left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{\infty}( \mathbb{R}^{d})}\] \[+\frac{32\pi^{2}(2v)^{(\beta-2)}}{(1+v^{2})^{\frac{\beta-2}{2}}} \sum_{m=1}^{q}\|f\|_{W_{2}^{\beta}(\mathbb{R}^{d})}\left\|f_{m}\right\|_{L^{ \infty}(\mathbb{R}^{d})}+2(2v)^{\beta}\sum_{m=1}^{q}\|f_{m}\|_{W_{2}^{\beta}( \mathbb{R}^{d})}\left\|\frac{\partial^{2}f_{m}}{\partial x_{i}\partial x_{j}} \right\|_{L^{\infty}(\mathbb{R}^{d})}\|g\|_{L^{1}(\mathbb{R}^{d})}.\]

Hence, we get the final result by replacing \(f(x,t)\) with \(\hat{p}(x,t)\). 

### Approximation Result by a PSD Model (Proof of Theorem 1)

Proof.: Now, we just need to collect results from Lemmas 2 and 3 to get the final bound. Let us assume that there exist an optimal density function \(p^{\star}(x,t)\) which is the solution of fokker-planck equation. Hence, We would like to show that there exists a psd operator \(M_{\varepsilon}\) such that if \(\hat{p}(x,t)=f(x,t)=\phi_{X,T}(x,t)^{\top}M_{\varepsilon}\phi_{X,T}(x,t)\) such that we can obtain bound for following three quantities for \(r\in\{1,2\}\):

1. \(\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial\hat{p}(x,t)}{ \partial t}\right\|_{L^{r}(\mathbb{R}^{d+1})}=O(\varepsilon)\).
2. \[\left\|\sum_{i=1}^{d}\frac{\partial}{\partial x_{i}}(\mu_{i}(x,t) p^{\star}(x,t)-\mu_{i}(x,t)\hat{p}(x,t))\right\|_{L^{r}(\mathbb{R}^{d+1})}= \left\|\sum_{i=1}^{d}\left[\mu_{i}(x,t)\Big{[}\frac{\partial p^{\star}(x,t)}{ \partial x_{i}}-\frac{\partial\hat{p}(x,t)}{\partial x_{i}}\Big{]}\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+ \left.\frac{\partial\mu_{i}(x,t)}{\partial x_{i}}(p^{\star}(x,t)-\hat{p}(x,t)) \right]\right\|_{L^{1}(\mathbb{R}^{d+1})}=O(\varepsilon),\]
3. \(\left\|\sum_{i=1}^{d}\sum_{j=1}^{d}\left[\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}p^{\star}(x,t))-\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(D_{ij}\hat{p}(x,t))\right]\right\|_{L^{1}( \mathbb{R}^{d+1})}=O(\varepsilon)\).

If the above three condition holds, then, we can show that for \(r\in\{1,2\}\),

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))-\sum_{i=1}^{d}\sum_{j=1}^{d}\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}(D_{ij}\hat{p}(x,t))\right\|_{L^{r} (\mathbb{R}^{d+1})}=O(\varepsilon).\]

We have from Lemma 2 and equation (26), if we choose

\[v=\min\left(\left(\frac{\varepsilon}{2^{\beta-1}16\pi C_{1}}\right)^{\frac{ 1}{\beta-1}},\left(\frac{\varepsilon}{2^{\beta+1}C_{2}}\right)^{\frac{1}{ \beta}}\right)\]

where \(C_{1}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{ \infty}(\mathbb{R}^{d+1})}\) and \(C_{2}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\left\|\frac{ \partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L_{1 }(\mathbb{R}^{d+1})}\). Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial f(x,t)}{ \partial t}\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq\varepsilon.\]

From [14, Theorem D.4 (Eq. D.25) ], we have,

\[\left\|p^{\star}(x,t)-\hat{p}(x,t)\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq(2v)^ {\beta}(1+\|g\|_{L^{1}(\mathbb{R}^{d+1})})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{ 2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{\infty}(\mathbb{R}^{d+1})}.\]

This proves the first part. Going further,

\[\left\|\sum_{i=1}^{d}\frac{\partial}{\partial x_{i}}(\mu_{i}(x,t) p^{\star}(x,t)-\mu_{i}(x,t)\hat{p}(x,t))\right\|_{L^{2}(\mathbb{R}^{d+1})}= \left\|\sum_{i=1}^{d}\left[\mu_{i}(x,t)\Big{[}\frac{\partial p^{\star}(x,t)}{ \partial x_{i}}-\frac{\partial\hat{p}(x,t)}{\partial x_{i}}\Big{]}\right.\right.\] \[\left.\left.+\left.\frac{\partial\mu_{i}(x,t)}{\partial x_{i}}(p^ {\star}(x,t)-\hat{p}(x,t))\right]\right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[\leq\sum_{i=1}^{d}\left\|\mu_{i}(x,t)\Big{[}\frac{\partial p^{ \star}(x,t)}{\partial x_{i}}-\frac{\partial\hat{p}(x,t)}{\partial x_{i}}\Big{]} \right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[\leq\sup_{\underbrace{x,t\in(X,\gamma)}_{x,t}\sup_{i}\mu_{i}(x,t) }\sum_{i=1}^{d}\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{i}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{i}}\right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[\leq R_{\mu}\sum_{i=1}^{d}\left\|\frac{\partial p^{\star}(x,t)}{ \partial x_{i}}-\frac{\partial\hat{p}(x,t)}{\partial x_{i}}\right\|_{L^{2}( \mathbb{R}^{d+1})}+dR_{\mu_{p}}\left\|p^{\star}(x,t)-\hat{p}(x,t)\right\|_{L^{2 }(\mathbb{R}^{d+1})}\] (31)

Now,

\[\left\|\sum_{i=1}^{d}\frac{\partial}{\partial x_{i}}(\mu_{i}(x,t) p^{\star}(x,t)-\mu_{i}(x,t)\hat{p}(x,t))\right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[\leq R_{\mu}\sum_{i=1}^{d}\left\|\frac{\partial p^{\star}(x,t)}{ \partial x_{i}}-\frac{\partial\hat{p}(x,t)}{\partial x_{i}}\right\|_{L^{2}( \mathbb{R}^{d+1})}+dR_{\mu_{p}}\left\|p^{\star}(x,t)-\hat{p}(x,t)\right\|_{L^{ 2}(\mathbb{R}^{d+1})}\]

From [14, Theorem D.4 (Eq. D.25) ], we have,

\[\left\|p^{\star}(x,t)-\hat{p}(x,t)\right\|_{L^{1}(\mathbb{R}^{d+1})}\leq(2v)^ {\beta}(1+\|g\|_{L^{1}(\mathbb{R}^{d+1})})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{ 2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{\infty}(\mathbb{R}^{d+1})}.\]

And previously, we showed in lemma 2

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{\partial\hat{p}(x, t)}{\partial x_{k}}\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq 8\pi(2v)^{(\beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{ 2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{\infty}(\mathbb{R}^{d+1})}\] (32)\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))-\sum_{i=1}^{d}\sum_{j=1}^{d}\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}(D_{ij}\hat{p}(x,t))\right\|_{L^{2} (\mathbb{R}^{d+1})}\leq\varepsilon(1+d(R_{\mu}+R_{\mu_{p}})+d^{2}R_{d}).\]

We can bound the trace of the matrix as follows. We have,

\[M_{\varepsilon}=\sum_{i=1}^{q}f_{i,v}f_{i,v}^{\top}.\]

From [11, Equation D.21], we have

\[Tr(M_{\varepsilon})=\sum_{i=1}^{q}\|f_{i,v}\|_{\mathcal{H}}=c_{n}2^{2\beta}(1 +(\frac{v}{3})^{2\beta}e^{\frac{8\theta}{v_{0}\varepsilon^{2}}})\sum_{i=1}^{q }\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\]

Since, we choose,

\[v=\left(\frac{\varepsilon}{96\pi^{2}2^{\beta-2}C}\right)^{\frac{1}{\beta-2}},\]where \(C=C_{1}+C_{2}+C_{3}\), \(C_{1}=(1+\|g\|_{L^{1}(\mathbb{R}^{d+1})})\sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}( \mathbb{R}^{d+1})}\left\|\frac{\partial f_{m}}{\partial x_{j}}\right\|_{L^{ \infty}(\mathbb{R}^{d+1})}\), \(C_{2}=\sum_{m=1}^{q}\|f\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\left\|f_{m}\right\| _{L^{\infty}(\mathbb{R}^{d+1})}\) and \(C_{3}=\sum_{m=1}^{q}\|f_{m}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\left\|\frac{ \partial^{2}f_{m}}{\partial x_{i}\partial x_{j}}\right\|_{L^{\infty}(\mathbb{ R}^{d+1})}\|g\|_{L^{1}(\mathbb{R}^{d+1})}\).

\[Tr(M_{\varepsilon}) =c_{\eta}2^{2\beta}(1+(\frac{v}{3})^{2\beta}e^{\frac{8\eta}{ \eta_{0}v^{2}}})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\] \[=c_{\eta}2^{2\beta}\left(1+3^{-2\beta}\left(\frac{\varepsilon}{9 6\pi^{2}2^{\beta-2}C}\right)^{\frac{2\beta}{\beta-2}}e^{\frac{8\eta_{0}v^{2}} {\eta_{0}v^{2}}}\right)\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})} ^{2}\] \[=c_{\eta}2^{2\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}( \mathbb{R}^{d+1})}^{2}\left(1+3^{-2\beta}\left(\frac{\varepsilon}{\tilde{C}} \right)^{\frac{2\beta}{\beta-2}}e^{\frac{8\eta}{\eta_{0}\left(\frac{\tilde{C} }{\tilde{C}}\right)^{\frac{2}{\beta-2}}}}\right)\] \[=\hat{C}|\eta|^{1/2}\left(1+e^{\frac{2\beta}{\beta-2}}e^{\frac{ \tilde{C}^{\prime}}{\eta_{0}v^{2}}\frac{\tilde{C}^{\prime}}{\eta_{0}v^{2}-2}} \right)\leq\hat{C}|\eta|^{1/2}\left(1+e^{\frac{2\beta}{\beta-2}}\exp\left( \frac{\tilde{C}^{\prime}}{\eta_{0}}\varepsilon^{-\frac{2}{\beta-2}}\right)\right)\]

where \(\tilde{C}=96\pi^{2}2^{\beta-2}C\), \(\tilde{C}^{\prime}=89\tilde{C}^{\frac{2}{\beta-2}}\), and \(\hat{C}=\pi^{-d/2}2^{2\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^ {d+1})}^{2}\max\left(1,\frac{3^{-2\beta}}{\tilde{C}^{\frac{2\beta}{\beta-2}}}\right)\). 

### Compression Result

Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be an open set with Lipschitz boundary, contained in the hypercube \([-R,R]^{d}\) with \(R>0\). Given \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\in\mathcal{X}\) be \(m\) points in \([-R,R]^{d}\). Define the base point matrix \(\tilde{X}\in\mathbb{R}^{m\times d}\) to be the matrix whose \(j\)-th row is the point \(\tilde{x}_{j}\). The following result holds. We introduce the so called _fill distance_[20]

\[h=\max_{x\in[-R,R]^{d}}\min_{z\in\tilde{X}}\|x-z\|.\] (34)

Now, we state some useful results below.

### Useful Results

**Lemma 4** ([10], Lemma C.1 ).: _Let \(T=(-R,R)^{d}\) and \(\eta\in\mathbb{R}^{d}_{++}\). Let \(u\in\mathcal{H}_{\eta}\) satisfying \(u(\tilde{x}_{1})=\cdots=u(\tilde{x}_{m})=0\). There exists three constants \(c,C,C^{\prime}\) depending only on \(d\) (and in particular; independent from \(R,\eta,u,\tilde{x}_{1},\ldots,\tilde{x}_{m}\)), such that, when \(h\leq\sigma/C^{\prime}\), then,_

\[\|u\|_{L^{\infty}(T)} \leq Cq_{\eta}\,e^{-\frac{c\sigma}{h}\log\frac{c\sigma}{h}}\,\|u\|_{ \mathcal{H}_{\eta}},\]

_with \(q_{\eta}=\det(\frac{1}{\eta_{+}}\,\mathrm{diag}(\eta))^{-1/4}\) and \(\sigma=\min(R,\frac{1}{\sqrt{\eta_{+}}})\) and \(\eta_{+}=\max_{i=1,\ldots,d}\eta_{i}\)._

**Lemma 5** (Norm of function's derivative with scattered zeros).: _Let \(T=(-R,R)^{d}\) and \(\eta\in\mathbb{R}^{d}_{++}\). Let \(u\in\mathcal{H}_{\eta}\) satisfying \(u(\tilde{x}_{1})=\cdots=u(\tilde{x}_{m})=0\). There exists three constants \(c,C,C^{\prime}\) depending only on \(d\) (and in particular, independent from \(R,\eta,u,\tilde{x}_{1},\ldots,\tilde{x}_{m}\)), such that, when \(h\leq\sigma/C^{\prime}\), then,_

\[\|D^{j}u\|_{L^{\infty}(T)} \leq Ge^{-c\sigma/h}\log(c\sigma/h)\|u\|_{\mathcal{H}_{\eta}},\]

_with \(G\) be some positive constant and \(\sigma=\min(R,\frac{1}{\sqrt{\eta_{+}}})\) and \(\eta_{+}=\max_{i=1,\ldots,d}\eta_{i}\)._

Proof.: Here, we utilize Gagliardo-Nirenberg inequality [20]. We have, for \(0<k<\infty\), \(0\leq j<\infty\) and \(\vartheta(j,m)=\frac{j}{k}+\frac{d}{2k}<1\), then

\[\|D^{j}u\|_{L^{\infty}(T)}\leq G\|u\|_{L^{2}(T)}^{1-\vartheta(j,k)}\|D^{k}u\|_{L^{ 2}(T)}^{\vartheta(j,k)},\] (35)

where \(G=\frac{1}{2^{d/2}\pi^{d/4-1/2}\sqrt{\Gamma(d/2)(1-\vartheta)^{1-\vartheta}\, \vartheta^{\alpha}k\sin\pi\vartheta}}\).

Considering the case for \(j=1\) and \(j=2\), we have,

\[\|D^{1}u\|_{L^{\infty}(T)}\leq G_{1}\|u\|_{L^{2}(T)}^{1-\frac{d+2}{2k}}\|D^{k}u\|_{ L^{2}(T)}^{\frac{d+2}{2k}},\]\[\|D^{2}u\|_{L^{\infty}(T)}\leq G_{2}\|u\|_{L^{2}(T)}^{1-\frac{d+2}{2k}}\|D^{k}u\|_{ L^{2}(T)}^{\frac{d+4}{2k}}\]

where \(G_{1}=\frac{1}{2^{d/2}\pi^{d/4-1/2}\sqrt{\Gamma(d/2)}(1-\frac{d+4\lambda^{2}}{2k })^{1-\frac{d+2}{2k}}(\frac{d+2}{2k})^{\frac{d+2}{2k}}k\sin\pi(\frac{d+4\lambda }{2k})}\) and

\(G_{2}=\frac{1}{2^{d/2}\pi^{d/4-1/2}\sqrt{\Gamma(d/2)}(1-\frac{d+4\lambda^{2}}{2 k})^{1-\frac{d+4}{2k}}(\frac{d+4\lambda}{2k})^{\frac{d+4}{2k}}k\sin\pi(\frac{d+4 \lambda}{2k})}\). We here have assumed that \(k>\frac{d}{2}+2\). From the defeinition, we have

\[\|D^{1}u\|_{L^{\infty}(T)} \leq G_{1}\|u\|_{L^{2}(T)}^{1-\frac{d+2}{2k}}\|D^{k}u\|_{L^{2}(T)} ^{\frac{d+2}{2k}},\] \[\|D^{2}u\|_{L^{\infty}(T)} \leq G_{2}\|u\|_{L^{2}(T)}^{1-\frac{d+4}{2k}}\|D^{k}u\|_{L^{2}(T)} ^{\frac{d+4}{2k}}.\]

From the definition,

\[\|D^{1}u\|_{L^{\infty}(T)} \leq G_{1}\|u\|_{L^{2}(T)}^{1-\frac{d+2}{2k}}\|u\|_{W^{2}_{2}(T)} ^{\frac{d+2}{2k}},\] \[\|D^{2}u\|_{L^{\infty}(T)} \leq G_{2}\|u\|_{L^{2}(T)}^{1-\frac{d+4}{2k}}\|u\|_{W^{2}_{2}(T)} ^{\frac{d+4}{2k}}.\]

Now by Theorem 4.3 of [10], we have for some constants \(B_{d}\) and \(B^{\prime}_{d}\) depending only on d,

\[\|u\|_{L^{2}(T)}\leq\frac{B_{d}^{k}k^{k}}{k!}h^{k}\|u\|_{W^{k}_{2}(T)},\]

when \(kh\leq\frac{R}{B^{\prime}_{d}}\). Hence, we have,

\[\|D^{1}u\|_{L^{\infty}(T)} \leq G_{1}\frac{B_{d}^{k}k^{k}}{k!}h^{k}\|u\|_{W^{k}_{2}(T)},\] \[\|D^{2}u\|_{L^{\infty}(T)} \leq G_{2}\frac{B_{d}^{k}k^{k}}{k!}h^{k}\|u\|_{W^{k}_{2}(T)}.\]

From equation C.9 in [10, Proof of Lemma C.1], we have

\[\|u\|_{W^{k}_{2}(T)}\leq\|u\|_{\mathcal{X}_{\eta}}c_{\eta}^{1/2}(4\eta_{+})^{ k/2}\sqrt{k!}.\]

Hence,

\[\|D^{1}u\|_{L^{\infty}(T)} \leq G_{1}\frac{B_{d}^{k}k^{k}}{k!}h^{k}\|u\|_{\mathcal{X}_{\eta} }c_{\eta}^{1/2}(4\eta_{+})^{k/2}\sqrt{k!}=G_{1}\frac{(2\sqrt{\eta_{+}}B_{d}hk)^ {k}}{\sqrt{k!}}\|u\|_{\mathcal{X}_{\eta}},\] \[\|D^{2}u\|_{L^{\infty}(T)} \leq G_{2}\frac{B_{d}^{k}k^{k}}{k!}h^{k}\|u\|_{\mathcal{X}_{\eta} }c_{\eta}^{1/2}(4\eta_{+})^{k/2}\sqrt{k!}=G_{2}\frac{(2\sqrt{\eta_{+}}B_{d}hk) ^{k}}{\sqrt{k!}}\|u\|_{\mathcal{X}_{\eta}}.\]

Now, we make similar choice of the parameter \(h\) and \(k\) as in [10]. If we choose, \(C_{3}=\frac{1}{\max(2B_{d},B^{\prime}_{d})}\), \(h\leq\frac{C_{3}}{d+4}\min(R,1/\sqrt{\eta_{+}})\), \(k=\lfloor s\rfloor\) and \(s=\frac{C_{3}}{h}\min(R,1/\sqrt{\eta_{+}})\), then we have \(hk\leq\frac{R}{B^{\prime}_{d}}\) and \(2\sqrt{\eta_{+}}B_{d}hk\leq 1\). Moreover, \(\frac{1}{\sqrt{k!}}\leq e^{-\frac{k}{2}\log\frac{k}{2}}\). If we also assume that \(G=\max(G_{1},G_{2})\), then for \(j=\{1,2\}\)

\[\|D^{j}u\|_{L^{\infty}(T)}\leq G\frac{1}{\sqrt{k!}}\|u\|_{\mathcal{X}_{\eta}} \leq Ge^{-s/4}\log(s/4)\|u\|_{\mathcal{X}_{\eta}}.\]

The final result is obtained by writing \(s/4=c\sigma/h\) with \(\sigma=\min(R,1/\sqrt{\eta_{+}})\) and \(c=C_{3}/4\). 

**Lemma 6** (Lemma 3, page 28 [11]).: _Let \(\mathcal{X}\subset\mathbb{R}^{d}\) with non-zero volume. Let \(\mathcal{H}\) be a reproducing kernel Hilbert space on \(\mathcal{X}\), associated to a continuous uniformly bounded feature map \(\phi:\mathcal{X}\rightarrow\mathcal{H}\). Let \(A:\mathcal{H}\rightarrow\mathcal{H}\) be a bounded linear operator. Then,_

\[\sup_{x\in\mathcal{X}}\|A\phi(x)\|_{\mathcal{H}}\leq\sup_{\|f\|_{\mathcal{H}} \leq 1}\|A^{*}f\|_{C(\mathcal{X})}.\]

_In particular, if \(\mathcal{X}\subset\mathbb{R}^{d}\) is a non-empty open set, then \(\sup_{x\in\mathcal{X}}\|A\phi(x)\|_{\mathcal{H}}\leq\sup_{\|f\|_{\mathcal{H}} \leq 1}\|A^{*}f\|_{L^{\infty}(\mathcal{X})}\)._

**Theorem 6** ( Theorem C.3 [16]).: _Let \(R>0,\eta\in\mathbb{R}^{d}_{++},m\in\mathbb{N}\). Let \(\mathcal{X}\subseteq T=(-R,R)^{d}\) be a non-empty open set and let \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\) be a set of distinct points. Let \(h>0\) be the fill distance associated to the points w.r.t \(T\) (defined in eq. (34)). Let \(\tilde{P}:\mathcal{H}_{\eta}\to\mathcal{H}_{\eta}\) be the associated projection operator (see definition in eq. (38)). There exists three constants \(c,C,C^{\prime}\), such that, when \(h\leq\sigma/C^{\prime}\),_

\[\sup_{x\in\mathcal{X}}\|(I-\tilde{P})\phi_{\eta}(x)\|_{\mathcal{H}_{\eta}}\leq Cq _{\eta}\;e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}}.\]

_Here \(q_{\eta}=\det(\frac{1}{\eta_{+}}\operatorname{diag}(\eta))^{-1/4}\) and \(\sigma=\min(R,\frac{1}{\sqrt{\eta_{+}}})\), \(\eta_{+}=\max_{i}\eta_{i}\). The constants \(c,C^{\prime},C^{\prime\prime}\) depend only on \(d\) and, in particular, are independent from \(R,\eta,\tilde{x}_{1},\ldots,\tilde{x}_{m}\)._

Similar to the results in theorem 6, we will obtain results for derivatives.

**Theorem 7**.: _Let \(R>0,\eta\in\mathbb{R}^{d}_{++},m\in\mathbb{N}\). Let \(\mathcal{X}\subseteq T=(-R,R)^{d}\) be a non-empty open set and let \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\) be a set of distinct points. Let \(h>0\) be the fill distance associated to the points w.r.t \(T\) (defined in eq. (34)). Let \(\tilde{P}:\mathcal{H}_{\eta}\to\mathcal{H}_{\eta}\) be the associated projection operator (see definition in eq. (38)). There exists three constants \(c,C,C^{\prime}\), such that, when \(h\leq\sigma/C^{\prime}\),_

\[\sup_{x\in\mathcal{X}}\|(I-\tilde{P})D^{j}\phi_{\eta}(x)\|_{\mathcal{H}_{\eta} }\leq C_{1}q_{\eta}\;e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}},\]

_for some positive constant \(C_{1}\). Here \(q_{\eta}=\det(\frac{1}{\eta_{+}}\operatorname{diag}(\eta))^{-1/4}\) and \(\sigma=\min(R,\frac{1}{\sqrt{\eta_{+}}})\), \(\eta_{+}=\max_{i}\eta_{i}\). The constants \(c,C^{\prime},C^{\prime\prime}\) depend only on \(d\) and, in particular, are independent from \(R,\eta,\tilde{x}_{1},\ldots,\tilde{x}_{m}\)._

Proof.: If the function \(f\) lies in the RKHS, then using the property of the projection operator \(\tilde{P}\), we have

\[D^{j}(\tilde{P}f)(x_{i})=D^{j}\langle\tilde{P}f,\phi(x_{i})\rangle=D^{j} \langle f,\tilde{P}\phi(x_{i})\rangle=D^{j}\langle f,\phi(x_{i})\rangle= \langle f,D^{j}\phi(x_{i})=(D^{j}f)(x_{i}).\]

Hence, \(D^{j}f-\tilde{P}D^{j}f(\tilde{x}_{i})=0\) for all \(i\in\{1,\cdots,m\}\). By lemma 4, we know that there exist three constants \(c,C,C^{\prime}\) depending only on \(d\) such that when \(h\leq\sigma/C^{\prime}\) we have that the following holds \(\|u\|_{L^{\infty}(T)}\leq Cq_{\eta}\;e^{-\frac{\sigma\pi}{h}\log\frac{\sigma \pi}{h}}\), for any \(u\in\mathcal{H}_{\eta}\) such that \(u(\tilde{x}_{1})=\cdots=u(\tilde{x}_{m})=0\). Since, for any \(u\in\mathcal{H}_{\eta}\), we have that \(D^{j}f-\tilde{P}D^{j}f\) belongs to \(\mathcal{H}_{\eta}\) and satisfies such property, we can apply lemma 4 with \(u=(I-\tilde{P})D^{j}f\), obtaining, under the same assumption on \(h\),

\[\|(I-\tilde{P})D^{j}f\|_{L^{\infty}(T)}\leq Cq_{\eta}\;e^{-\frac{\sigma\pi}{h} \log\frac{\sigma\pi}{h}}\|D^{j}f\|_{\mathcal{H}_{\eta}},\quad\forall f\in \mathcal{H}_{\eta},\]

where we used the fact that \(\|(I-\tilde{P})D^{j}f\|_{\mathcal{H}_{\eta}}\leq\|I-\tilde{P}\|\|D^{j}f\|_{ \mathcal{H}_{\eta}}\) and \(\|I-\tilde{P}\|\leq 1\), since \(P\) is a projection operator and so also \(I-P\) satisfies this property. Derivative reproducing properties for kernel can be looked at in [11]. The final result is obtained by applying lemma 6 with \(A=I-\tilde{P}\), from which we have

\[\sup_{x\in\mathcal{X}}\|(I-\tilde{P})D^{j}\phi(x)\|_{\mathcal{H}} \leq\sup_{\|D^{j}f\|\leq 1}\|(I-\tilde{P})D^{j}f\|_{L^{\infty}(T)} \leq\sup_{\|D^{j}f\|\leq 1}Cq_{\eta}\;e^{-\frac{\sigma\pi}{h}\log\frac{ \sigma\pi}{h}}\|D^{j}f\|_{\mathcal{H}_{\eta}}\] \[=C_{1}q_{\eta}\;e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}},\]

for some positive \(C_{1}\). The same result can also be obtained by the use of Lemma 5 with a condition that \(f\) is \(d/2+1\) differentiable function which is a slightly weaker condition on the order of smoothness.

### Compression of a PSD model for Fokker-Planck

We have \((\tilde{x}_{1},\tilde{t}_{1}),\tilde{x}_{2},\cdots(\tilde{x}_{m},\tilde{t}_{m}) \in\mathcal{X}\times\mathcal{T}\) which is an open bounded subset with Lipschitz boundary of the cube \([-R,R]^{d}\times[0,T]\). \(\tilde{X}\) be the base point matrix whose j-rows are the points \(\tilde{x}_{j}\). Let us now consider the model \(p=f(\cdot,M,\psi)\) and \(\tilde{p}=f(\cdot;A_{m},\tilde{X},\psi)\) where \(A_{m}=K_{(X,T)(X,T)}^{-1}\tilde{Z}M\tilde{Z}^{\star}K_{(X,T)(X,T)}^{-1}\).

\[\tilde{Z}u=(\psi(\tilde{x}_{1},\tilde{t}_{1})^{\top}u,\cdots,\psi(\tilde{x}_{m}, \tilde{t}_{m})^{\top}u).\] (36)\[\tilde{Z}^{\star}\alpha=\sum_{i=1}^{m}\psi(\tilde{x}_{i},\tilde{t}_{i})\alpha_{i}.\] (37)

Hence, for any \(A\in\mathbb{R}^{m\times m}\)

\[\tilde{Z}^{\star}A\tilde{Z}=\sum_{i,j=1}^{m}A_{i,j}\psi(\tilde{x}_{i},\tilde{t }_{i})\psi(\tilde{x}_{i},\tilde{t}_{i})^{\top}.\]

Let us also define associated projection operator,

\[\tilde{P}=\tilde{Z}^{\star}K^{-1}_{(X,T)(X,T)}\tilde{Z}.\] (38)

Some properties associated with it,

\[\tilde{P}^{2}=\tilde{P},\ \tilde{P}\tilde{Z}^{\star}=\tilde{Z}^{\star},\ \tilde{Z}\tilde{P}=\tilde{Z}.\]

**Theorem 8**.: _Let \(\eta\in\mathbb{R}^{d}_{++}\) and let \(\mathsf{M}\in\mathbb{S}_{+}(\mathcal{H}_{\eta})\). Let \(\tilde{\mathfrak{X}}\) be an open bounded subset with the Lipschitz boundary of the cube \([-R,R]^{d}\times[0,R]\), \(R>0\). Let \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\in\tilde{\mathfrak{X}}\) and \(\tilde{X}\) be the base point matrix whose \(j\)-rows are the points \(\tilde{x}_{j}\) with \(j=1,\ldots,m\). Consider the model \(p=f(\cdot\ ;\mathsf{M},\psi)\) and the the compressed model \(\tilde{p}=f(\cdot\ ;\ A_{m},\tilde{X},\eta)\) with_

\[A_{m} =\ K^{-1}_{\tilde{X},\tilde{X},\tilde{\mathfrak{X}}}\tilde{ \mathsf{Z}}\mathsf{M}\tilde{Z}^{\star}\ K^{-1}_{\tilde{X},\tilde{X},\eta},\]

_where \(\tilde{Z}:\mathcal{H}_{\eta}\to\mathbb{R}^{m}\) is defined in eq. (36) in terms of \(\tilde{X}_{m}\). Let \(h\) be the fill distance (defined in eq. (34)) associated to the points \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\). The there exist three constants \(c,C,C^{\prime}\) depending only on \(d\) such that, when \(h\leq\sigma/C^{\prime}\), with \(\sigma=\min(R,1/\sqrt{\eta_{+}}),\eta_{+}=\max_{i=1,\ldots,d}\eta_{i},q_{\eta}= \det(\frac{1}{\eta_{+}}\operatorname{diag}(\eta))^{-1/4}\), then we have_

\[\left|\frac{\partial(p(x,t)-\tilde{p}(x,t))}{\partial t}+\sum_{ i=i}^{d}\frac{\partial(\mu_{i}(x,t)(p(x,t)-\tilde{p}(x,t)))}{\partial x_{i}}- \sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x _{j}}(p(x,t)-\tilde{p}(x,t))\right|\] \[\leq(2+dR_{\mu}+2dR_{\mu}+4d^{2}R_{\mu})\|M\|C^{2}q_{\eta}^{2}e^{ \frac{-2\sigma e}{h}\log\frac{c\sigma}{h}}+(2+dR_{\mu}+2d^{2}R_{\mu})\sqrt{\| M\|p(x,t)}\ Cq_{\eta}e^{-\frac{c\sigma}{h}\log\frac{c\sigma}{h}}\] \[\qquad\qquad+2Cq_{\eta}e^{-\frac{c\sigma}{h}\log\frac{c\sigma}{h }}\sqrt{\|M\|\tilde{f}_{t}(x;M,\psi)}+(2R_{\mu}+4dR_{d})Cq_{\eta}e^{-\frac{c \sigma}{h}\log\frac{c\sigma}{h}}\sum_{i=1}^{d}\sqrt{\|M\|\tilde{f}_{x_{i}}(x;M,\psi)}\] \[\qquad\qquad+2R_{d}Cq_{\eta}e^{-\frac{c\sigma}{h}\log\frac{c \sigma}{h}}\sum_{i=1}^{d}\sum_{j=1}^{d}\sqrt{\|M\|\tilde{f}_{x_{j}x_{i}}(x;M, \psi)},\]

_where \(\tilde{f}_{x_{i}}(x;M,\psi)=\frac{\partial\psi(x,t)}{\partial x_{i}}^{\top}M \frac{\partial(x,t)}{\partial x_{i}}\), \(\tilde{u}_{x_{i}}(x)=\left\|(I-\tilde{P})\frac{\partial\psi(x,t)}{\partial x_{i }}\right\|_{\mathbb{X}_{q_{\eta}}}\), \(\tilde{f}_{x_{j}x_{i}}(x;M,\psi)=\frac{\partial^{2}\psi(x,t)}{\partial x_{j}x _{i}}^{\top}M\frac{\partial^{2}\psi(x,t)}{\partial x_{j}x_{i}}\) and \(\tilde{u}_{x_{j}x_{i}}(x)=\left\|(I-\tilde{P})\frac{\partial^{2}\psi(x,t)}{ \partial x_{j}x_{i}}\right\|_{\mathbb{X}_{q_{\eta}}}\)._

Proof.: We have,

\[p(x,t)-\tilde{p}(x,t)=\psi(x,t)^{\top}(\tilde{P}M\tilde{P}-M) \psi(x,t).\] (39)

Let now consider every term separately.

\[\frac{\partial p(x,t)}{\partial t}-\frac{\partial\tilde{p}(x,t)}{ \partial t} =2\frac{\partial\psi(x,t)}{\partial t}^{\top}M\psi(x,t)-2\frac{ \partial\psi(x,t)}{\partial t}^{\top}\tilde{P}M\tilde{P}\psi(x,t)\] \[=2\frac{\partial\psi(x,t)}{\partial t}^{\top}(M-\tilde{P}M\tilde{P })\psi(x,t).\] (40)

Similarly,

\[\frac{\partial p(x,t)}{\partial x_{i}}-\frac{\partial\tilde{p}(x,t)}{ \partial x_{i}} =2\frac{\partial\psi(x,t)}{\partial x_{i}}^{\top}M\psi(x,t)-2\frac{ \partial\psi(x,t)}{\partial x_{i}}^{\top}\tilde{P}M\tilde{P}\psi(x,t)\]\[=2\frac{\partial\psi(x,t)}{\partial x_{i}}^{\top}(M-\tilde{P}M\tilde{P})\psi(x,t).\] (41)

Finally,

\[\frac{\partial^{2}p(x,t)}{\partial x_{i}x_{j}}-\frac{\partial^{2} \tilde{p}(x,t)}{\partial x_{i}x_{j}}=2\psi_{i}^{\prime}(x,t)^{\top}M\psi_{j}^{ \prime}(x,t)+2\psi_{ji}^{\prime\prime}(x,t)^{\top}M\psi(x,t)\] \[\qquad-2\psi_{i}^{\prime\prime}(x,t)^{\top}\tilde{P}M\tilde{P} \psi_{j}^{\prime}(x,t)-2\psi_{ji}^{\prime\prime}(x,t)^{\top}\tilde{P}M\tilde{P }\psi(x,t)\] \[=2\psi_{ji}^{\prime\prime}(x,t)^{\top}(M-\tilde{P}M\tilde{P})\psi (x,t)+2\psi_{i}^{\prime}(x,t)^{\top}(M-\tilde{P}M\tilde{P})\psi_{j}^{\prime}(x,t).\] (42)

We have,

\[\tilde{P}M\tilde{P}-M=(I-\tilde{P})M(I-\tilde{P})-M(I-\tilde{P})-(I-\tilde{P} )M.\]

Since \(|a^{\top}ABAa|\leq\|Ala\|_{\mathcal{H}}^{2}\|B\|\) and \(|a^{\top}ABAa|\leq\|Ala\|_{\mathcal{H}}\|B^{1/2}\|\|B^{1/2}a\|_{\mathcal{H}}\), for any \(a\) in a Hilbert space \(\mathcal{H}\) and for \(A,B\) bounded symmetric linear operators with \(B\in\mathbb{S}_{+}(\mathcal{H})\), by bounding the terms of the equation above, we have for any \(x\in\mathbb{R}^{d}\),

\[|\psi(x,t)^{\top}(\tilde{P}M\tilde{P}-M)\psi(x,t)| \leq 2\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{q}}\|M\|^{1/2}\|M^{ 1/2}\psi(x,t)\|_{\mathcal{H}_{q}}\] \[\qquad+\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{q}}^{2}\|M\|\] \[=2c_{M}^{1/2}f(x;M,\psi)^{1/2}\tilde{u}_{1}(x)+c_{M}\tilde{u}_{1 }(x)^{2},\] (43)

where \(c_{M}=\|M\|\) and we denoted by \(\tilde{u}_{1}(x)\) the quantity \(\tilde{u}_{1}(x)=\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{q}}\) and we noted that \(\|M^{1/2}\psi(x,t)\|_{\mathcal{H}_{q}}^{2}=\psi(x,t)^{\top}M\psi(x,t)=f(x\,;M,\psi(x,t)).\) Now,

\[\left|\frac{\partial\psi(x,t)}{\partial x_{i}}^{\top}(M-\tilde{P}M \tilde{P})\psi(x,t)\right|\leq\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{q}}\| M\|\left(I-\tilde{P})\frac{\partial\psi(x,t)}{\partial x_{i}}\right\|_{ \mathcal{H}_{q}}\] \[+\left\|(I-\tilde{P})\frac{\partial\psi(x,t)}{\partial x_{i}} \right\|_{\mathcal{H}_{q}}\|M\|^{1/2}\|M^{1/2}\psi(x,t)\|_{\mathcal{H}_{q}}+ \left\|(I-\tilde{P})\psi(x,t)\right\|_{\mathcal{H}_{q}}\|M\|^{1/2}\left\|M^{1 /2}\frac{\partial\psi(x,t)}{\partial x_{i}}\right\|_{\mathcal{H}_{q}}\] \[=c_{M}\tilde{u}_{1}(x)\tilde{u}_{x_{i}}(x)+c_{M}^{1/2}f(x;M,\psi) ^{1/2}\tilde{u}_{x_{i}}(x)+c_{M}^{1/2}\tilde{f}_{x_{i}}(x;M,\psi)^{1/2}\tilde {u}_{1}(x),\] (44)

where \(\tilde{f}_{x_{i}}(x;M,\psi)=\frac{\partial\psi(x,t)}{\partial x_{i}}^{\top}M \frac{\partial\psi(x,t)}{\partial x_{i}}\) and \(\tilde{u}_{x_{i}}(x)=\left\|(I-\tilde{P})\frac{\partial\psi(x,t)}{\partial x_{i }}\right\|_{\mathcal{H}_{q}}\).

Following similar steps, we get

\[\left|\frac{\partial\psi(x,t)}{\partial t}^{\top}(M-\tilde{P}M \tilde{P})\psi(x,t)\right|\leq c_{M}\tilde{u}_{1}(x)\tilde{u}_{t}(x)+c_{M}^{1/ 2}f(x;M,\psi)^{1/2}\tilde{u}_{t}(x)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+c_{ M}^{1/2}\tilde{f}_{t}(x;M,\psi)^{1/2}\tilde{u}_{1}(x),\] (45)

where \(\tilde{f}_{t}(x;M,\psi)=\frac{\partial\psi(x,t)}{\partial t}^{\top}M\frac{ \partial\psi(x,t)}{\partial t}\) and \(\tilde{u}_{t}(x)=\left\|(I-\tilde{P})\frac{\partial\psi(x,t)}{\partial t} \right\|_{\mathcal{H}_{q}}\).

\[\left|\psi_{ji}^{\prime\prime}(x,t)^{\top}(M-\tilde{P}M\tilde{P} )\psi(x,t)\right|\leq c_{M}\tilde{u}_{1}(x)\tilde{u}_{x_{j}x_{i}}(x)+c_{M}^{1/ 2}f(x;M,\psi)^{1/2}\tilde{u}_{x_{j}x_{i}}(x)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\[\leq\left|\frac{\partial p(x,t)}{\partial t}-\frac{\partial\tilde{p}(x,t)}{ \partial t}\right|+\sum_{i=1}^{d}\left|\mu_{i}(x,t)\frac{\partial p(x,t)}{ \partial x_{i}}-\mu_{i}(x,t)\frac{\partial\tilde{p}(x,t)}{\partial x_{i}} \right|+\sum_{i=1}^{d}\left|\frac{\partial\mu_{i}(x,t)}{\partial x_{i}}p(x,t)- \frac{\partial\mu_{i}(x,t)}{\partial x_{i}}\tilde{p}(x,t)\right|\] \[\qquad\qquad+\left|\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}p(x,t)-D_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}\tilde{p}(x,t)\right|\] \[\leq\left|\frac{\partial p(x,t)}{\partial t}-\frac{\partial\tilde {p}(x,t)}{\partial t}\right|+R_{\mu}\sum_{i=1}^{d}\left|\frac{\partial p(x,t)} {\partial x_{i}}-\frac{\partial\tilde{p}(x,t)}{\partial x_{i}}\right|+dR_{\mu _{p}}(p(x,t)-\tilde{p}(x,t))\] \[\qquad\qquad\qquad+R_{d}\sum_{i=1}^{d}\sum_{j=1}^{d}\left|\frac{ \partial^{2}}{\partial x_{i}\partial x_{j}}p(x,t)-\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}\tilde{p}(x,t)\right|\] \[=2\left|\frac{\partial\psi(x,t)}{\partial t}^{\top}(M-\tilde{P}M \tilde{P})\psi(x,t)\right|+2R_{\mu}\sum_{i=1}^{d}\left|\frac{\partial\psi(x,t )}{\partial x_{i}}^{\top}(M-\tilde{P}M\tilde{P})\psi(x,t)\right|+dR_{\mu_{p}}| \psi(x,t)^{\top}(M-\tilde{P}M\tilde{P})\psi(x,t)|\] \[\qquad\qquad+2R_{d}|\psi_{ji}^{\prime\prime}(x,t)^{\top}(M-\tilde {P}M\tilde{P})\psi(x,t)+\psi_{i}^{\prime}(x,t)^{\top}(M-\tilde{P}M\tilde{P}) \psi_{j}^{\prime}(x,t)|\] \[\leq 2\left|\frac{\partial\psi(x,t)}{\partial t}^{\top}(M-\tilde{P}M \tilde{P})\psi(x,t)\right|+2R_{\mu}\sum_{i=1}^{d}\left|\frac{\partial\psi(x,t )}{\partial x_{i}}^{\top}(M-\tilde{P}M\tilde{P})\psi(x,t)\right|+dR_{\mu_{p}}| \psi(x,t)^{\top}(M-\tilde{P}M\tilde{P})\psi(x,t)|\] \[\qquad\qquad+2R_{d}|\psi_{ji}^{\prime\prime}(x,t)^{\top}(M- \tilde{P}M\tilde{P})\psi(x,t)|+2R_{d}|\psi_{i}^{\prime}(x,t)^{\top}(M-\tilde{ P}M\tilde{P})\psi_{j}^{\prime}(x,t)|.\]

Now, combining everything together, we get

\[\left|\frac{\partial p(x,t)}{\partial t}-\frac{\partial\tilde{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial(\mu_{i}(x,t)p(x,t))}{\partial x _{i}}-\frac{\partial(\mu_{i}(x,t)\tilde{p}(x,t))}{\partial x_{i}}-\sum_{i=1}^{ d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p(x,t)-D_{ij} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\tilde{p}(x,t)\right|\] \[\leq 2c_{M}\tilde{u}_{1}(x)\tilde{u}_{1}(x)+2c_{M}^{1/2}f(x;M, \psi)^{1/2}\tilde{u}_{t}(x)+2c_{M}^{1/2}\tilde{f}_{t}(x;M,\psi)^{1/2}\tilde{u} _{1}(x)+dR_{\mu_{p}}(2c_{M}^{1/2}f(x;M,\psi)^{1/2}\tilde{u}_{1}(x)+c_{M} \tilde{u}_{1}(x)^{2})\] \[\qquad\qquad+2R_{\mu}\sum_{i=1}^{d}[c_{M}\tilde{u}_{1}(x)\tilde{u }_{x_{i}}(x)+c_{M}^{1/2}f(x;M,\psi)^{1/2}\tilde{u}_{x_{i}}(x)+c_{M}^{1/2} \tilde{f}_{x_{i}}(x;M,\psi)^{1/2}\tilde{u}_{1}(x)]\] \[\qquad\qquad+2R_{d}\sum_{i=1}^{d}\sum_{j=1}^{d}[c_{M}\tilde{u}_{ 1}(x)\tilde{u}_{x_{j}x_{i}}(x)+c_{M}^{1/2}f(x;M,\psi)^{1/2}\tilde{u}_{x_{j}x_{ i}}(x)+c_{M}^{1/2}\tilde{f}_{x_{j}x_{i}}(x;M,\psi)^{1/2}\tilde{u}_{1}(x)]\] \[\qquad\qquad+2R_{d}\sum_{i=1}^{d}\sum_{j=1}^{d}[c_{M}\tilde{u}_{ \mu_{j}}(x)\tilde{u}_{x_{i}}(x)+c_{M}^{1/2}f_{x_{i}}(x;M,\psi)^{1/2}\tilde{u} _{x_{j}}(x)+c_{M}^{1/2}\tilde{f}_{x_{j}}(x;M,\psi)^{1/2}\tilde{u}_{x_{i}}(x)].\]

Now from theorems 6 and 7, we have that when the fill distance \(h<\frac{\sigma}{C^{\prime}}\) with \(\sigma=\min(R,\frac{1}{\tau})\), then

\[\|\tilde{u}_{1}\|_{L^{\infty}(\mathcal{X})}\leq Cq_{\eta}e^{-\frac{\sigma\pi}{h} \log\frac{\sigma\pi}{h}},\ \|\tilde{u}_{t}\|_{L^{\infty}(\mathcal{X})}\leq Cq_{\eta}e^{-\frac{\sigma\pi}{h} \log\frac{\sigma\pi}{h}},\|\tilde{u}_{x_{i}}\|_{L^{\infty}(\mathcal{X})}\leq Cq _{\eta}e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}}\] (48)

\[\text{ and }\|\tilde{u}_{x_{i}x_{j}}\|_{L^{\infty}(\mathcal{X})}\leq Cq_{\eta}e^{- \frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}}\]

for all \(i\in\{1,\cdots,d\}\), \(j\in\{1,\cdots,d\}\) and some constant \(C,c,C^{\prime}\) depending only on \(d\). Hence, we have

\[\left|\frac{\partial(p(x,t)-\tilde{p}(x,t))}{\partial t}+\sum_{i=i }^{d}\frac{\partial(\mu_{i}(x,t)(p(x,t)-\tilde{p}(x,t)))}{\partial x_{i}}- \sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x _{j}}(p(x,t)-\tilde{p}(x,t))\right|\] \[\leq(2+dR_{\mu}+2dR_{\mu}+4d^{2}R_{d})\|M\|C^{2}q_{\eta}^{2}e^{- \frac{2c\sigma}{h}\log\frac{\sigma\pi}{h}}+(2+dR_{\mu_{p}}+2dR_{\mu}+2d^{2}R_{d}) \sqrt{\|M\|p(x,t)}\ Cq_{\eta}e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h}}\] \[\qquad\qquad+2Cq_{\eta}e^{-\frac{\sigma\pi}{h}\log\frac{\sigma\pi}{h} }\sqrt{\|M\|\tilde{f}_{t}(x;M,\psi)}+(2R_{\mu}+4dR_{d})Cq_{\eta}e^{-\frac{ \sigma\pi}{h}\log\frac{\sigma\pi}{h}}\sum_{i=1}^{d}\sqrt{\|M\|\tilde{f}_{x_{i}} (x;M,\psi)}\] \[\qquad\qquad+2R_{d}Cq_{\eta}e^{-\frac{\sigma\pi}{h}\log\frac{ \sigma\pi}{h}}\sum_{i=1}^{d}\sum_{j=1}^{d}\sqrt{\|M\|\tilde{f}_{x_{j}x_{i}} (x;M,\psi)}.\]

### Final Approximation Bound (Proof of Theorem 2)

Proof.: From the previous result in theorem 1, we know that there exists an \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_{T})\) with \(\mathrm{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\tilde{p}(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds under assumption 2 on the coefficients of the fractional FPE,

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{ \partial}{\partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))-\sum_{i=1}^{d}\sum_{j=1}^{d }D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\hat{p}(x,t)\right\|_{L ^{2}(\tilde{\chi})}=O(\varepsilon),\] (49)

for an appropriate choice of parameters. We also know that

\[\frac{\partial p^{\star}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{ \partial}{\partial x_{i}}(\mu_{i}(x,t)p^{\star}(x,t))-\sum_{i=1}^{d}\sum_{j=1} ^{d}D_{ij}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}p^{\star}(x,t)=0.\] (50)

We know that, for given \(A_{m},\tilde{p}(x,t)=\psi(x,t)^{\top}\tilde{P}\mathsf{M}_{\varepsilon}\tilde {P}\psi(x,t)^{\top}\) where \(\tilde{P}\) is the projection operator defined earlier. From the results in theorem 8, we have

\[\left|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=i}^{d}\frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{ \partial x_{i}}-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(\hat{p}(x,t)-\tilde{p}(x,t))\right|\] \[\leq(2+dR_{\mu}+2dR_{\mu}+4d^{2}R_{d})\|\mathsf{M}_{\varepsilon} \|C^{2}q_{\eta}^{2}e^{-\frac{2\varepsilon\varepsilon\eta}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}+(2+dR_{\mu_{\mu}}+2d^{2}R_{d})\sqrt{\|\mathsf{M}_{ \varepsilon}\|p(x,t)}\,Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}\] \[\qquad\qquad+2Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log \frac{\varepsilon\sigma}{\hbar}}\sqrt{\|\mathsf{M}_{\varepsilon}\|\tilde{f}_{ \varepsilon}(x;M,\psi)}+(2R_{\mu}+4dR_{d})Cq_{\eta}e^{-\frac{\varepsilon\sigma} {\hbar}\log\frac{\varepsilon\sigma}{\hbar}}\sum_{i=1}^{d}\sqrt{\|\mathsf{M}_ {\varepsilon}\|\tilde{f}_{\varepsilon_{i}}(x;M,\psi)}\] \[\qquad\qquad+2R_{d}Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar} \log\frac{\varepsilon\sigma}{\hbar}}\sum_{i=1}^{d}\sum_{j=1}^{d}\sqrt{\| \mathsf{M}_{\varepsilon}\|\tilde{f}_{\varepsilon_{j}x_{i}}(x;M_{\varepsilon}, \psi)}.\]

It is clear that some big constant \(Q\), \(\tilde{f}_{x_{j}x_{i}}(x;\mathsf{M}_{\varepsilon},\eta)\leq Q\|\mathsf{M}_{ \varepsilon}\|\), for all \(i,j\in\{1,\cdots,q\}\). Similarly, \(\tilde{f}_{x_{i}}(x;\mathsf{M}_{\varepsilon},\eta)\leq Q\|\mathsf{M}_{ \varepsilon}\|\) for all \(i\in\{1,\cdots,q\}\) and \(\tilde{f}_{\varepsilon}(x;\mathsf{M}_{\varepsilon},\eta)\leq Q\|\mathsf{M}_{ \varepsilon}\|\). Hence, we have,

\[\left|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=i}^{d}\frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{ \partial x_{i}}-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(\hat{p}(x,t)-\tilde{p}(x,t))\right|\] \[\leq(2+dR_{\mu}+2dR_{\mu}+4d^{2}R_{d})\|\mathsf{M}_{\varepsilon} \|C^{2}q_{\eta}^{2}e^{-\frac{2\varepsilon\sigma}{\hbar}\log\frac{\varepsilon \sigma}{\hbar}}+(2+dR_{\mu_{\mu}}+2dR_{\mu}+2d^{2}R_{d})\sqrt{\|\mathsf{M}_{ \varepsilon}\|p(x,t)}\,Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}\] \[\qquad+2QCq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}\|\mathsf{M}_{\varepsilon}\|+(2QR_{\mu}+4Qd^{2}R_{d}) Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}\| \mathsf{M}_{\varepsilon}\|+2QR_{d}d^{2}Cq_{\eta}e^{-\frac{\varepsilon\sigma}{ \hbar}\log\frac{\varepsilon\sigma}{\hbar}}\|\mathsf{M}_{\varepsilon}\|\] \[\leq(2+dR_{\mu}+2dR_{\mu}+4d^{2}R_{d})\|\mathsf{M}_{\varepsilon} \|C^{2}q_{\eta}^{2}e^{-\frac{2\varepsilon\sigma}{\hbar}\log\frac{\varepsilon \sigma}{\hbar}}+(2+dR_{\mu_{\mu}}+2dR_{\mu}+2d^{2}R_{d})\sqrt{\|\mathsf{M}_{ \varepsilon}\|p(x,t)}\,Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}\] \[\qquad+Q(2C+2R_{\mu}d+6Qd^{2}R_{d})Cq_{\eta}e^{-\frac{\varepsilon \sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}\|\mathsf{M}_{\varepsilon}\|.\]

Now, let us compute \(\|\mathsf{M}_{\varepsilon}\|\). We have

\[\|\mathsf{M}_{\varepsilon}\|\leq Tr(M_{\varepsilon})=\hat{C}\tau^{(d )/2}\left(1+\varepsilon^{\frac{2\beta}{\beta-2}}e^{\frac{\hat{C}^{\prime}}{ \tau}\varepsilon-\frac{\eta^{2}}{2-2}}\right).\]

Let us choose, \(\tau=\frac{\hat{C}^{\prime}\varepsilon-\frac{\eta^{2}-2}{2}}{\frac{2\beta}{ \beta-2}\log(\frac{1+R}{\varepsilon})}\). Hence,

\[\|M_{\varepsilon}\|\leq C_{3}(1+R)^{\frac{2\beta}{\beta-2}}\varepsilon^{-\frac{d +1}{\beta-2}}.\]

Now,

\[\left\|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=i}^{d}\frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{ \partial x_{i}}-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(\hat{p}(x,t)-\tilde{p}(x,t))\right\|_{L^{2}(\tilde{ \chi})}\] \[\leq(2+3dR_{\mu}+4d^{2}R_{d})\|\mathsf{M}_{\varepsilon}\|C^{2}q_ {\eta}^{2}e^{-\frac{2\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}R ^{(d+1)/2}+Q(2C+2R_{\mu}d+6Qd^{2}R_{d})Cq_{\eta}e^{-\frac{\varepsilon\sigma}{ \hbar}\log\frac{\varepsilon\sigma}{\hbar}}\|\mathsf{M}_{\varepsilon}\|R^{(d+1)/2}\] \[\qquad+(2+dR_{\mu_{\mu}}+2dR_{\mu}+2d^{2}R_{d})\sqrt{\|\mathsf{M }_{\varepsilon}\|p(x,t)\,Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}}\] \[=\underbrace{(2+3dR_{\mu}+4d^{2}R_{d})}_{:=P_{1}}\|\mathsf{M}_{ \varepsilon}\|C^{2}q_{\eta}^{2}e^{-\frac{2\varepsilon\sigma}{\hbar}\log\frac{ \varepsilon\sigma}{\hbar}}R^{(d+1)/2}+\underbrace{Q(2C+2R_{\mu}d+6Qd^{2}R_{d})}_{:=P _{2}}Cq_{\eta}e^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{ \hbar}}\|\mathsf{M}_{\varepsilon}\|R^{(d+1)/2}\] \[\qquad+\underbrace{(2+3dR_{\mu}+2d^{2}R_{d})}_{:=P_{3}}\|\sqrt{ \|\mathsf{M}_{\varepsilon}\|p(x,t)\|_{L^{2}(\tilde{\chi})}}\,Cq_{\eta}e^{- \frac{\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}\] \[=P_{1}\|\mathsf{M}_{\varepsilon}\|C^{2}q_{\eta}^{2}e^{-\frac{ 2\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}R^{(d+1)/2}+P_{2}Cq_{ \eta}e^{-\frac{\varepsilon\sigma}{\hbar}\logThen, note that \(\|p^{1/2}\|_{L^{2}(\mathcal{X})}=\|p\|_{L^{1}(\mathcal{X})}^{1/2}\). Hence, using the argument in [10], we have

\[\|p^{1/2}\|_{L^{2}(\hat{\mathcal{X}})}\leq 2R.\]

Term \(R\) comes from the fact that for each time instance \(p(x,t)\) is a density and \(t\in(0<R)\). Hence,

\[\left\|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=1}^{d}\frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{ \partial x_{i}}-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{\partial x _{i}\partial x_{j}}(\hat{p}(x,t)-\tilde{p}(x,t))\right\|_{L^{2}(\hat{\mathcal{ X}})}\] \[\leq P_{1}\|\mathbb{M}_{\varepsilon}\|C^{2}q_{v}^{2}e^{\frac{-2 \varepsilon t}{\hbar}\log\frac{\varepsilon t}{\hbar}}R^{(d+1)/2}+P_{2}Cq_{v}e ^{-\frac{\varepsilon t}{\hbar}\log\frac{\varepsilon t}{\hbar}}\|\mathbb{M}_{ \varepsilon}\|R^{(d+1)/2}\] \[\qquad\qquad+2P_{3}R\|\mathbb{M}_{\varepsilon}\|^{1/2}Cq_{v}e^{ -\frac{\varepsilon t}{\hbar}\log\frac{\varepsilon t}{\hbar}}\] \[\leq P_{1}C_{3}(1+R)^{\frac{2\beta}{\beta-2}}\varepsilon^{-\frac {d+1}{\beta-2}}C^{2}q_{v}^{2}e^{\frac{-2\varepsilon t}{\hbar}\log\frac{ \varepsilon t}{\hbar}}R^{(d+1)/2}+P_{2}Cq_{v}e^{-\frac{\varepsilon t}{\hbar} \log\frac{\varepsilon t}{\hbar}}C_{3}(1+R)^{\frac{2\beta}{\beta-2}} \varepsilon^{-\frac{d+1}{\beta-2}}R^{(d+1)/2}\] \[\qquad\qquad+2P_{3}RC_{3}^{1/2}(1+R)^{\frac{\beta}{\beta-2}} \varepsilon^{-\frac{d+1}{2(\beta-2)}}Cq_{v}e^{-\frac{\varepsilon t}{\hbar} \log\frac{\varepsilon t}{\hbar}}.\]

By choosing \(h=c\sigma/s\) with \(s=\max(C^{\prime},(1+\frac{d+1}{2(\beta-2)})\log\frac{1}{\varepsilon}+(1+ \frac{d}{2})\log(1+R)\,+\log(\hat{C})+e)\), for some big enough \(\hat{C}\). Since \(s\geq e\), then \(\log s\geq 1\), so

\[Ce^{-\frac{\varepsilon\sigma}{\hbar}\log\frac{\varepsilon\sigma}{\hbar}}=Ce^ {-s\log s}\leq Ce^{-s}\leq C^{\prime}(1+R)^{-d/2}\varepsilon^{1+\frac{d+1}{2( \beta-2)}}.\]

Hence, we have,

\[\left\|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=i}^{d}\frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{ \partial x_{i}}-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij}\frac{\partial^{2}}{ \partial x_{i}\partial x_{j}}(\hat{p}(x,t)-\tilde{p}(x,t))\right\|_{L^{2}(\hat {\mathcal{X}})}\] \[=O(\varepsilon).\]

Combining evrything together and by the use of triangles inequality, we have

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\tilde{p}(x,t))-\sum_{i=1}^{d}\sum_{j=1}^{d}D_{ij} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\tilde{p}(x,t)\right\|_{L^{2} (\hat{\mathcal{X}})}=O(\varepsilon).\]

To conclude we recall the fact that \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\) is a \(h\)-covering of \(T\), guarantees that the number of centers \(m\) in the covering satisfies

\[m\leq(1+\frac{2R\sqrt{d+1}}{h})^{d+1}.\]

Then, since \(h\geq c\sigma/(C_{4}\log\frac{C_{5}\log(1+R)}{\varepsilon})\) with \(C_{4}=1+d/\min(2(\beta-2),2)\) and \(C_{5}=(\hat{C})^{1/C_{4}}\), and since \(\sigma=\min(R,1/\sqrt{\tau})\), then \(R/\sigma=\max(1,R\sqrt{\tau})\leq 1+\sqrt{\hat{C}^{\prime}}\varepsilon^{-1/( \beta-2)}(\log\frac{1+R}{\varepsilon})^{-1/2}\), so after final calculation, we have

\[m^{\frac{1}{d+1}}\leq 1+2R\sqrt{d+1}/h\leq C_{8}+C_{9}\log\frac{1+R}{ \varepsilon}+C_{10}\varepsilon^{-\frac{1}{\beta-2}}\left(\log\frac{1+R}{ \varepsilon}\right)^{1/2},\]

for some constants \(C_{8}\), \(C_{9}\) and \(C_{10}\) independent of \(\varepsilon\). 

### Proof of Corollary 1

Proof of Corollary 1.: Let \(p^{\star}\) be the probability corresponding to the solution of the Fokker-Planck equation with drift \(\mu\) and \(\hat{p}\) be the conditional probability corresponding to the solution of the Fokker-Planck equation with drift \(\hat{\mu}\). And denote by \(\hat{p}_{\gamma}(x,t)\) the \(m\) dimensional PSD model obtained by applying the construction of Thm. 2 to the Fokker-Planck equation defined in terms of \(\hat{\mu}\). Hence, from the direct application of the result in [11, Theorem 1.1] since \(\|\mu-\hat{\mu}\|_{L^{2}}=O(\varepsilon)\) we have,

\[\|p^{\star}-\hat{p}\|_{L^{2}(T\times X)}=O(\varepsilon).\] (52)

From, theorem 2, we have,

\[\|\hat{p}-\hat{p}_{\gamma}\|_{L^{2}(T\times X)}=O(\varepsilon).\]

Finally due to proposition 2, for any PSD model \(f\) there exist an algorithm that samples i.i.d. from the probability \(p_{f}\) satisfying

\[\mathbb{W}_{1}(f,p_{f})=O(\varepsilon).\]Now note that for any \(t\), the function \(\hat{p}_{\gamma}(t,\cdot)\) is still a PSD model (see [10]). Then, for any \(t\in(0,T)\), we have that there exists probabilities \(\hat{p}_{t,\alpha}\) such that

\[\mathbb{W}_{1}(\hat{p}_{\gamma}(t,\cdot),\hat{p}_{t,\alpha}(\cdot))=O(\varepsilon).\]

Then, since \(\mathcal{W}_{1}\) is a distance and is bounded by the \(L^{2}\) distance since we are on a bounded domain, there exists a \(C\) depending on the diameter and the volume of the domain, such that for any \(t\)

\[\mathbb{W}_{1}(p^{\star}(t,\cdot),\hat{p}_{t,\alpha}(\cdot))^{2}\leq C(\|p^{ \star}(t,\cdot)-\hat{p}^{\star}(t,\cdot)\|_{L^{2}(X)}^{2}+\|\hat{p}(t,\cdot)- \hat{p}_{\gamma}(t,\cdot)\|_{L^{2}(X)}^{2}+\mathbb{W}_{1}(\hat{p}_{\gamma},p_{ \alpha})^{2})\]

Then, by taking the expectation over \(t\in(0,T)\) we have

\[\mathbb{E}_{t}\,\mathbb{W}_{1}(p^{\star}(t,\cdot),\hat{p}_{t,\alpha}(\cdot))^ {2}\leq C(\|p^{\star}-\hat{p}^{\star}\|_{L^{2}(T\times X)}^{2}+\|\hat{p}(t, \cdot)-\hat{p}_{\gamma}(t,\cdot)\|_{L^{2}(T\times X)}^{2}+\mathbb{W}_{1}(\hat {p}_{\gamma},p_{\alpha})^{2})=O(\varepsilon^{2})\]

## Appendix B Estimation of Fractional Laplacian Operator on a Probability Density

Before we go into the details of the estimation, let us first recall bochner's theorem.

**Theorem 9** ([14]).: _A continuous kernel \(k(x,y)=k(x-y)\) on \(\mathbb{R}^{d}\) is positive definite if and only if \(k(\delta)\) is the Fourier transform of a non-negative measure._

Proof of Theorem 3.: In our psd model approximation, we have

\[p(x)=f(x;A,X,\eta)=\sum_{i,j=1}^{m}A_{ij}k(x_{i},x)k(x_{j},x).\]

We will first prove the part (i) of the result. Now, we know that, fractional laplacian operator is defined as,

\[\mathcal{F}[(-\Delta)^{s}f](\xi) =\|\xi\|^{2s}\mathcal{F}[f](\xi)\] \[=\|\xi\|^{2s}\sum_{i,j=1}^{m}A_{ij}\mathcal{F}[k(x_{i},\cdot)k(x_ {j},\cdot)](\xi)\] \[=\|\xi\|^{2s}\sum_{i,j}^{m}A_{ij}\mathcal{F}[k(x_{i},\cdot)](\xi )*\mathcal{F}[k(x_{j},\cdot)](\xi).\]

Now that, \(k\) is a gaussian kernel with bandwidth \(\eta\). Hence,

\[k(y,x) =e^{-\eta\|y-x\|_{2}^{2}}\] \[\mathcal{F}[k(y,\cdot)](\xi) =ce^{j\xi^{\top}y-\frac{1}{4\eta}\|\xi\|_{2}^{2}}\] \[=ce^{-\frac{1}{4\eta}\left(\|\xi\|_{2}^{2}-4j\eta\xi^{\top}y-4 \eta^{2}\|y\|^{2}+4\eta^{2}\|y\|^{2}\right)}\] \[=ce^{-\eta\|y\|^{2}}\cdot e^{-\frac{1}{4\eta}\|\xi-2j\eta\|^{2}},\]

for some \(c>0\) which can be computed in closed form. Now, we know that convolution of two multivariate gaussian functions are still gaussian. Hence,

\[\mathcal{F}[k(x_{i},\cdot)](\xi)*\mathcal{F}[k(x_{j},\cdot)](\xi)=c^{\prime}e ^{-\eta(\|x_{i}\|^{2}+\|x_{j}\|^{2})}e^{-\frac{1}{4\eta}\|\xi-2j\eta(x_{i}+x_ {j})\|^{2}},\]

for some constant \(c^{\prime}>0\) which again can be computed in the closed form. Hence, we have

\[\mathcal{F}[(-\Delta)^{s}f](\xi) =\|\xi\|^{2s}\sum_{i,j=1}^{m}A_{ij}\mathcal{F}[k(x_{i},\cdot)]( \xi)*\mathcal{F}[k(x_{j},\cdot)](\xi)\] \[=c^{\prime}\sum_{i,j=1}^{m}A_{ij}\|\xi\|^{2s}e^{-\eta(\|x_{i}\|^ {2}+\|x_{j}\|^{2})}e^{-\frac{1}{8\eta}\|\xi-2j\eta(x_{i}+x_{j})\|^{2}}.\] (53)Finally we have,

\[(-\Delta)^{s}f(x) =C^{\prime}\sum_{i,j=1}^{m}A_{ij}\int\|\xi\|^{2s}e^{-\eta(\|x_{i}\|^ {2}+\|x_{j}\|^{2})}e^{-\frac{1}{\hbar\eta}\|\xi-2j\eta(x_{i}+x_{j})\|^{2}}e^{-j \xi^{\top}x}\;d\xi\] \[=C^{\prime}\sum_{i,j=1}^{m}A_{ij}e^{-\eta(\|x_{i}\|^{2}+\|x_{j}\|^ {2})}\int\|\xi\|^{2s}e^{-j\xi^{\top}x}\cdot e^{-\frac{1}{\hbar\eta}\|\xi-2j \eta(x_{i}+x_{j})\|^{2}}\;d\xi.\]

By inspecting the above equation carefully, we can see that after appropriate scaling,

\[(-\Delta)^{s}f(x)=C\sum_{i,j=1}^{m}A_{ij}e^{-\eta(\|x_{i}\|^{2}+\|x_{j}\|^{2})} \mathbb{E}_{\xi\sim\mathcal{N}(\mu,\Sigma)}[\|\xi\|^{2s}e^{-j\xi^{\top}x}]\] (54)

where \(\mu=2j\eta(x_{i}+x_{j})\) and \(\Sigma=4\eta I\). One can sample from \(\mathcal{N}(\mu,\Sigma)\) to estimate the fractional laplacian operator for PSD model.

Now, we will prove part (ii) of the theorem statement. From the bochner's theorem, we have,

\[k(x-y)=\int_{\mathbb{R}^{d}}q(\omega)e^{j\omega^{\top}(x-y)}\;d\omega,\]

where \(q\) is the probability density. In our psd model approximation, we have

\[p(x)=f(x;A,X,\eta) =\sum_{i,j=1}^{m}A_{ij}k(x_{i},x)k(x_{j},x)\] \[=\sum_{i,j=1}^{m}A_{ij}\left(\int_{\mathbb{R}^{d}}e^{j\omega_{ \ell}^{\top}(x_{i}-x)}q(\omega_{\ell})\;d\omega_{\ell}\right)\left(\int_{ \mathbb{R}^{d}}e^{j\omega_{k}^{\top}(x_{j}-x)}q(\omega_{k})\;d\omega_{k}\right)\] \[=\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}e ^{j\omega_{\ell}^{\top}(x_{i}-x)}e^{j\omega_{k}^{\top}(x_{j}-x)}q(\omega_{ \ell})q(\omega_{k})\;d\omega_{\ell}d\omega_{k}.\]

Now, we know that, fractional laplacian operator is defined as,

\[\mathcal{F}[(-\Delta)^{s}f](\xi)=\|\xi\|^{2s}\mathcal{F}[f](\xi)\] \[=\|\xi\|^{2s}\left(\int\left(\sum_{i,j=1}^{m}A_{ij}\int_{ \mathbb{R}^{d}}\int_{\mathbb{R}^{d}}e^{j\omega_{\ell}^{\top}(x_{i}-x)}e^{j \omega_{k}^{\top}(x_{j}-x)}q(\omega_{\ell})q(\omega_{k})\;d\omega_{\ell}d \omega_{k}\right)e^{-j\xi^{\top}x}\;dx\right)\] \[=\|\xi\|^{2s}\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{ \mathbb{R}^{d}}\left(\int_{\mathbb{R}^{d}}e^{j\omega_{\ell}^{\top}(x_{i}-x)}e^ {j\omega_{k}^{\top}(x_{j}-x)}e^{-j\xi^{\top}x}\;dx\right)q(\omega_{\ell})q( \omega_{k})\;d\omega_{\ell}d\omega_{k}\] \[=\|\xi\|^{2s}\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{ \mathbb{R}^{d}}\left(\int e^{j(\omega_{\ell}^{\top}x_{i}+\omega_{k}x_{j})}.e^ {-jx^{\top}(\omega_{\ell}+\omega_{k}+\xi)}\;dx\right)q(\omega_{\ell})q(\omega_ {k})\;d\omega_{\ell}d\omega_{k}\] \[=\|\xi\|^{2s}\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{ \mathbb{R}^{d}}e^{j(\omega_{\ell}^{\top}x_{i}+\omega_{k}x_{j})}\left(\int_{ \mathbb{R}^{d}}e^{-jx^{\top}(\omega_{\ell}+\omega_{k}+\xi)}\;dx\right)q(\omega _{\ell})q(\omega_{k})\;d\omega_{\ell}d\omega_{k}\] \[=\frac{1}{p^{2}}\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{ \mathbb{R}^{d}}e^{j(\omega_{\ell}^{\top}x_{i}+\omega_{k}x_{j})}\|\xi\|^{2s} \delta(\omega_{\ell}+\omega_{k}+\xi)\;q(\omega_{\ell})q(\omega_{k})\;d\omega _{\ell}d\omega_{k}.\]

Hence,

\[(-\Delta)^{s}f(x) =\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}} e^{j(\omega_{\ell}^{\top}x_{i}+\omega_{k}x_{j})}\left(\int_{\mathbb{R}^{d}}\|\xi\|^{2s} \delta(\omega_{\ell}+\tilde{\omega}_{k}+\xi)e^{-jx^{\top}\xi}\;d\xi\right)\;q (\omega_{\ell})q(\omega_{k})\;d\omega_{\ell}d\omega_{k}\] \[=\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}e ^{j(\omega_{\ell}^{\top}x_{i}+\omega_{k}x_{j})}\|\omega_{\ell}+\tilde{\omega}_{ k}\|^{2s}e^{jx^{\top}(\omega_{\ell}+\tilde{\omega}_{k})}\;q(\omega_{\ell})q(\omega_{k}) \;d\omega_{\ell}d\omega_{k}\] \[=\sum_{i,j=1}^{m}A_{ij}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}e ^{j\omega_{\ell}^{\top}(x_{i}+x)}e^{j\omega_{k}^{\top}(x_{j}+x)}\|\omega_{\ell} +\tilde{\omega}_{k}\|^{2s}\;q(\omega_{\ell})q(\omega_{k})\;d\omega_{\ell}d\omega _{k}\]\[=\left\|\left\|\omega\right\|^{2s}\mathcal{F}[p^{*}-\hat{p}](\omega)\right\|_{L^{ 2}(\mathbb{R}^{d+1})}=\left\|\left\|\omega\right\|^{2s}\mathcal{F}\left[\sum_{i= 1}^{q}f_{i}^{2}-f_{i,v}^{2}\right](\omega)\right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[=\left\|\left\|\omega\right\|^{2s}\mathcal{F}\left[\sum_{i=1}^{q} (f_{i}-f_{i,v})(f_{i}+f_{i,v})\right](\omega)\right\|_{L^{2}(\mathbb{R}^{d+1}) }=\left\|\left\|\omega\right\|^{2s}\sum_{i=1}^{q}\mathcal{F}\left[(f_{i}-f_{i, v})(f_{i}+f_{i,v})\right](\omega)\right\|_{L^{2}(\mathbb{R}^{d+1})}\] \[\leq\sum_{i=1}^{q}\left\|\left\|\omega\right\|^{2s}\mathcal{F} \left[(f_{i}-f_{i,v})(f_{i}+f_{i,v})\right](\omega)\right\|_{L^{2}(\mathbb{R}^ {d+1})}=\sum_{i=1}^{q}\left\|\left\|\omega\right\|^{2s}\mathcal{F}\left[(f_{i} -f_{i,v})\left(\omega\right)\right\|_{L^{2}(\mathbb{R}^{d+1})}.\]

Let us consider the function,

\[\left\|\omega\right\|^{2s}\underbrace{\mathcal{F}\left[(f_{i}-f_{i,v})\right] (\omega)}_{:=m(\omega)}\ast\underbrace{\mathcal{F}\left[f_{i}+f_{i,v}\right] (\omega)}_{:=n(\omega)}=\int\|\omega\|^{2s}m(\omega-y)n(y)\;d\omega\]\[=\int\|\omega-y+y\|^{2s}m(\omega-y)n(y)\;d\omega\leq c_{1}\int\| \omega-y\|^{2s}m(\omega-y)n(y)\;dy+c_{2}\int\|y\|^{2s}m(\omega-y)n(y)\;dy\] \[=c_{1}[\|\omega\|^{2s}m(\omega)]*n(\omega)+c_{2}m(\omega)*[\| \omega\|^{2s}n(\omega)].\]

Hence,

\[\|(-\Delta)^{s}(p^{*}-\hat{p})\|_{L^{2}(\mathbb{R}^{d+1})}\leq\underbrace{c_{ 1}\|[\|\omega\|^{2s}m(\omega)]*n(\omega)\|_{L^{2}(\mathbb{R}^{d})}}_{:=\mathrm{ T}_{1}}+\underbrace{c_{2}\|m(\omega)*[\|\omega\|^{2s}n(\omega)]\|_{L^{2}( \mathbb{R}^{d+1})}}_{:=\mathrm{T}_{2}}\]

Now, we can apply Young's convolution inequality on \(\mathrm{T}_{1}\) and \(\mathrm{T}_{2}\). Hence,

\[\mathrm{T}_{1} \leq c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})} \|n(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\] \[=c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d})}\| \mathcal{F}[f_{i}+f_{i,v}]\|_{L^{1}(\mathbb{R}^{d+1})}\] \[\leq c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})} \left[\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d+1})}+\|\mathcal{F}[f_{i,v}] \|_{L^{1}(\mathbb{R}^{d+1})}\right]\] \[=c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})} \left[\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d})}+\|\mathcal{F}[f_{i}]. \mathcal{F}[g_{v}]\|_{L^{1}(\mathbb{R}^{d+1})}\right]\] \[\leq c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})} \left[\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d+1})}+\|\mathcal{F}[f_{i}]\| _{L^{1}(\mathbb{R}^{d+1})}\|\mathcal{F}[g_{v}]\|_{L^{\infty}(\mathbb{R}^{d+1} )}\right].\]

In the last equation, we applied holder's inequality. From uniform continuity and the Riemann-Lebesgue lemma, we have

\[\|\mathcal{F}[g_{v}]\|_{L^{\infty}(\mathbb{R}^{d+1})}\leq\|g_{v}\|_{L^{1}( \mathbb{R}^{d+1})}=\int g_{v}(vx)\;dx=\int t^{-d}|g(x/t)|dx=\int|g(x)|\;dx=\| g\|_{L^{1}(\mathbb{R}^{d+1})}.\]

Hence,

\[\mathrm{T}_{1}\leq c_{1}\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1}) }\left[\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d+1})}+\|\mathcal{F}[f_{i}] \|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L^{1}(\mathbb{R}^{d+1})}\right]\]

Similarly,

\[\mathrm{T}_{2} \leq c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d})}\|\|\omega\|^{2s} n(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\] \[\leq c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}\left[\|\|\omega \|^{2s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}+\|\|\omega\|^{2s }\mathcal{F}(f_{i,v})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\right]\] \[=c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d})}\left[\|\|\omega\|^{2s }\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}+\|\|\omega\|^{2s} \mathcal{F}(f_{i})(\omega)\mathcal{F}(g_{v})(\omega)\|_{L^{1}(\mathbb{R}^{d+1} )}\right].\]

In the above equation, we can apply holder's inequality to get,

\[\mathrm{T}_{2} \leq c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}\left[\|\|\omega \|^{2s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}+\|\|\omega\|^{2 s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\|\mathcal{F}(g_{v})( \omega)\|_{L^{\infty}(\mathbb{R}^{d+1})}\right]\] \[\leq c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}\left[\|\|\omega \|^{2s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}+\|\|\omega\|^{2 s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\|\mathcal{F}(g_{v})( \omega)\|_{L^{\infty}(\mathbb{R}^{d+1})}\right].\]

Hence,

\[\mathrm{T}_{2}\leq c_{2}\|m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})} \left[\|\|\omega\|^{2s}\mathcal{F}(f_{i})(\omega)\|_{L^{1}(\mathbb{R}^{d+1})}+ \|\|\omega\|^{2s}\mathcal{F}[f_{i}](\omega)\|_{L^{1}(\mathbb{R}^{d+1})}\|g\|_{L ^{1}(\mathbb{R}^{d+1})}\right]\]

The term \(\|m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}\) and \(\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}\) can be bounded using similar technique as before. We will use the result from Lemma 1.

\[\|\|\omega\|^{2s}m(\omega)\|_{L^{2}(\mathbb{R}^{d+1})}^{2} \leq\int\|\omega\|^{4s}|\mathcal{F}[f_{i}](\omega)|^{2}\|1-\mathcal{ F}[g](v\omega)|^{2}\;d\omega\] \[\leq\int_{v\|\omega\|\geq 1}\|\omega\|^{4s}|\mathcal{F}[f_{i}]( \omega)|^{2}|\;d\omega\] \[=\int_{v\|\omega\|\geq 1}\|\omega\|^{4s}(1+\|\omega\|^{2})^{-\beta}(1+ \|\omega\|^{2})^{\beta}|\mathcal{F}[f_{i}](\omega)|^{2}|\;d\omega\] \[\leq 2^{2\beta}\sup_{v\|\omega\|\geq 1}\frac{\|\omega\|^{4s}}{(1+\| \omega\|^{2})^{\beta}}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\]\[=2^{2s}\frac{(2v)^{2(\beta-2s)}}{(1+v^{2})^{\beta-2s}}\|f_{i}\|_{W^{ \beta}_{2}(\mathbb{R}^{d+1})}^{2}\leq 2^{2s}(2v)^{2(\beta-2s)}\|f_{i}\|_{W^{ \beta}_{2}(\mathbb{R}^{d+1})}^{2}.\]

Combining everything together, we have

\[\|(-\Delta)^{s}(p^{\star}-\hat{p})\|_{L^{2}(\mathbb{R}^{d+1})}\leq c _{1}(2v)^{(\beta-2s)}\sum_{i=1}^{q}C_{T_{1}}^{(i)}\|f_{i}\|_{W^{\beta}_{2}( \mathbb{R}^{d+1})}^{2}+c_{2}(2v)^{\beta}\sum_{i=1}^{q}C_{T_{2}}^{(i)}\|f_{i}\| _{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2},\]

where \(C_{T_{1}}^{(i)}=\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d+1})}+\|\mathcal{F }[f_{i}]\|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L^{1}(\mathbb{R}^{d+1})}\) and \(C_{T_{2}}^{(i)}=\big{|}\|\omega\|^{2s}\mathcal{F}[f_{i}](\omega)\|_{L^{1}( \mathbb{R}^{d+1})}+\big{|}\|\omega\|^{2s}\mathcal{F}[f_{i}](\omega)\|_{L^{1}( \mathbb{R}^{d+1})}\|g\big{\|}_{L^{1}(\mathbb{R}^{d+1})}\). 

### Proof of Theorem 4

From the previous lemma, we have,

\[\|(-\Delta)^{s}(p^{\star}-\hat{p})\|_{L^{2}(\mathbb{R}^{d+1})}\leq c _{1}(2v)^{(\beta-2s)}\sum_{i=1}^{q}C_{T_{1}}^{(i)}\|f_{i}\|_{W^{\beta}_{2}( \mathbb{R}^{d+1})}^{2}+c_{2}(2v)^{\beta}\sum_{i=1}^{q}C_{T_{2}}^{(i)}\|f_{i}\| _{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2},\]

for some positive \(c_{1}\) and \(c_{2}\) where \(C_{T_{1}}^{(i)}=\|\mathcal{F}[f_{i}]\|_{L^{1}(\mathbb{R}^{d+1})}+\|\mathcal{F }[f_{i}]\|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L^{1}(\mathbb{R}^{d+1})}\) and \(C_{T_{2}}^{(i)}=\big{|}\big{|}\|\omega\|^{2s}\mathcal{F}[f_{i}](\omega)\|_{L^{1 }(\mathbb{R}^{d+1})}\|g\|_{L^{1}(\mathbb{R}^{d+1})}\). Now, if we choose, for some constant \(c\)

\[v=\left(\frac{\varepsilon}{c2^{\beta-2s}\sum_{i=1}^{q}(C_{T_{1}}^{(i)}+C_{T_{2 }}^{(i)})\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}}\right)^{\frac{1}{ \beta-2s}},\]

then

\[\|(-\Delta)^{s}(p^{\star}-\hat{p})\|_{L^{2}(\mathbb{R}^{d+1})}\leq\varepsilon.\]

We can bound the other two terms as using lemma 2. We have,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq 8\pi(2v)^{( \beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L ^{\infty}(\mathbb{R}^{d+1})}\] \[+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{ d+1})}\left\|\frac{\partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d+1})} \|g\|_{L_{1}(\mathbb{R}^{d+1})}.\]

If we choose

\[v=\min\left(\left(\frac{\varepsilon}{2^{\beta-1}16\pi C_{1}}\right)^{\frac{1}{ \beta-1}},\left(\frac{\varepsilon}{2^{\beta+1}C_{2}}\right)^{\frac{1}{\beta}}\right)\]

where \(C_{1}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{ \infty}(\mathbb{R}^{d+1})}\) and \(C_{2}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d})}\left\|\frac{ \partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L_{1}( \mathbb{R}^{d+1})}\). Hence,

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial t}-\frac{\partial \hat{p}(x,t)}{\partial t}\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq\varepsilon.\]

Similarly, from lemma 2

\[\left\|\frac{\partial p^{\star}(x,t)}{\partial x_{k}}-\frac{ \partial\hat{p}(x,t)}{\partial x_{k}}\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq 8\pi(2v)^{( \beta-1)}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L ^{\infty}(\mathbb{R}^{d+1})}\] \[+2(2v)^{\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{ d+1})}\left\|\frac{\partial f_{i}}{\partial x_{k}}\right\|_{L^{\infty}(\mathbb{R}^{d+1})} \|g\|_{L_{1}(\mathbb{R}^{d+1})}.\]From [14, Theorem D.4 (Eq. D.27) ], we have,

\[\|p^{\star}(x,t)-\hat{p}(x,t)\|_{L^{2}(\mathbb{R}^{d+1})}\leq(2v)^{\beta}(1+\|g \|_{L^{1}(\mathbb{R}^{d+1})})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^ {d+1})}\|f_{i}\|_{L^{\infty}(\mathbb{R}^{d+1})}.\]

Hence, if we choose,

\[v=\min\left(\left(\frac{\varepsilon}{2^{\beta-1}16\pi C_{1}}\right)^{\frac{1}{ \beta-1}},\left(\frac{\varepsilon}{2^{\beta+1}C_{2}}\right)^{\frac{1}{\beta}},\left(\frac{\varepsilon}{2^{\beta+1}C_{3}}\right)^{\frac{1}{\beta}}\right)\]

where \(C_{1}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\|f_{i}\|_{L^{ \infty}(\mathbb{R}^{d+1})}\), \(C_{2}=\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\left\|\frac{ \partial f_{i}}{\partial t}\right\|_{L^{\infty}(\mathbb{R}^{d+1})}\|g\|_{L_{1 }(\mathbb{R}^{d+1})}\qquad\text{and}\qquad C_{3}=\qquad\qquad(1+\|g\|_{L^{1}( \mathbb{R}^{d+1})})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}\| f_{i}\|_{L^{\infty}(\mathbb{R}^{d+1})}\), then, we have

\[\left\|\sum_{i=1}^{d}\frac{\partial}{\partial x_{i}}(\mu_{i}(x,t)p^{\star}(x,t )-\mu_{i}(x,t)\hat{p}(x,t))\right\|_{L^{2}(\mathbb{R}^{d+1})}\leq d(R_{\mu}+R_ {\mu_{p}})\varepsilon.\]

Hence, by combining everything together, we have

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))-(-\Delta)^{s}(\hat{p}(x,t))\right\| _{L^{2}(\mathbb{R}^{d+1})}\leq\varepsilon(2+d(R_{\mu}+R_{\mu_{p}})).\]

We can bound the trace of the matrix as follows. We have,

\[M_{\varepsilon}=\sum_{i=1}^{q}f_{i,v}f_{i,v}^{\top}.\]

From [14, Equation D.21], we have

\[Tr(M_{\varepsilon})=\sum_{i=1}^{q}\|f_{i,v}\|_{\mathcal{H}}=c_{\eta}2^{2\beta }(1+(\frac{v}{3})^{2\beta}e^{\frac{-8\eta}{\eta_{0}v^{2}}})\sum_{i=1}^{q}\|f_ {i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\]

Since, we choose,

\[v=\left(\frac{\varepsilon}{96\pi^{2}2^{\beta-2}C}\right)^{\frac{1}{\beta-2z}},\]

where \(C\) depends on \(f_{i}\)s, \(\mathcal{F}[f_{i}]\) and it's derivative.

\[Tr(M_{\varepsilon}) =c_{\eta}2^{2\beta}(1+(\frac{v}{3})^{2\beta}e^{\frac{-8\eta}{ \eta_{0}v^{2}}})\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\] \[=c_{\eta}2^{2\beta}\left(1+3^{-2\beta}\left(\frac{\varepsilon}{9 6\pi^{2}2^{\beta-2}C}\right)^{\frac{2-\beta}{-2z}}e^{\frac{8\eta}{\eta_{0}v^{ 2}}}\right)\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{R}^{d+1})}^{2}\] \[=c_{\eta}2^{2\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{ R}^{d+1})}^{2}\left(1+3^{-2\beta}\left(\frac{\varepsilon}{\tilde{C}} \right)^{\frac{2\beta}{\beta-2z}}e^{\frac{8\eta}{\eta_{0}\left(\frac{\varepsilon }{\tilde{C}}\right)^{\beta-2z}}}\right)\] \[=\tilde{C}|\eta|^{1/2}\left(1+\varepsilon^{\frac{2\beta}{\beta-2 z}}e^{\frac{\tilde{C}^{\prime}}{\eta_{0}\cdot\tilde{\beta}-2z}}\right)\leq\tilde{C}|\eta|^{1/2} \left(1+\varepsilon^{\frac{2\beta}{\beta-2z}}\exp\left(\frac{\tilde{C}^{\prime} }{\eta_{0}}\varepsilon^{-\frac{2}{\beta-2z}}\right)\right)\]

where \(\tilde{C}=96\pi^{2}2^{\beta-2}C\), \(\tilde{C}^{\prime}=89\tilde{C}^{\frac{2}{\beta-2}}\), and \(\tilde{C}=\pi^{-d/2}2^{2\beta}\sum_{i=1}^{q}\|f_{i}\|_{W^{\beta}_{2}(\mathbb{ R}^{d+1})}^{2}\max\left(1,\frac{3^{-2\beta}}{\tilde{C}^{\frac{2\beta}{\beta-2z}}}\right)\).

### Approximation Properties of PSD Model for Fractional Laplacian (Proof of Theorem 4)

Proof.: From the previous result in theorem 4, we know that there exists an \(\mathsf{M}_{\varepsilon}\in\mathbb{S}_{+}(\mathcal{H}_{X}\otimes\mathcal{H}_{T})\) with \(\operatorname{rank}(\mathsf{M}_{\varepsilon})\leq q\), such that for the representation \(\hat{p}(x,t)=\phi_{X,T}^{\top}\mathsf{M}_{\varepsilon}\phi_{X,T}\), following holds under assumption 2 on the coefficients of the fractional FPE,

\[\left\|\frac{\partial\hat{p}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{ \partial x_{i}}(\mu_{i}(x,t)\hat{p}(x,t))+(-\Delta)^{s}\hat{p}(x,t)\right\|_{L^ {2}(\tilde{x})}=O(\varepsilon),\] (56)

for an appropriate choice of parameters. We also know that

\[\frac{\partial p^{*}(x,t)}{\partial t}+\sum_{i=1}^{d}\frac{\partial}{\partial x _{i}}(\mu_{i}(x,t)p^{*}(x,t))+(-\Delta)^{s}p^{*}(x,t)=0.\] (57)

We know that, for given \(A_{m}\), \(\tilde{p}(x,t)=\psi(x,t)^{\top}\tilde{P}\mathsf{M}_{\varepsilon}\tilde{P} \psi(x,t)^{\top}\) where \(\tilde{P}\) is the projection operator defined earlier. We will utilize the Gagliardo-Nirenberg inequality to control the error in compression of fractional laplacian compressed model with respect to the original one. For any fixed \(t\),

\[(-\Delta)^{s}[\hat{p}(x,t)-\tilde{p}(x,t)]=(-\Delta)^{s}[\psi(x,t)^{\top}( \tilde{P}\mathsf{M}_{\varepsilon}\tilde{P}-\mathsf{M}_{\varepsilon})\psi(x,t)]\]

By applying Gagliardo-Nirenberg inequality for fractional laplacian [10], we have

\[\|(-\Delta)^{s}\beta_{t}(x)\|_{L^{2}(\mathbb{R}^{d})}\leq\underbrace{\|\beta_ {t}(x)\|_{L^{2}(\mathbb{R}^{d+1})}^{1-\vartheta}}_{:=T_{1}}\cdot\underbrace{ \|\Delta\beta_{t}(x)\|_{L^{2}(\mathbb{R}^{d+1})}^{\vartheta}}_{:=T_{2}},\]

where \(\vartheta=s\leq 1\) and \(\Delta\) is normal laplacian operator. Let us consider the term \(T_{1}\) and \(T_{2}\) separately. In our case, \(\beta_{t}(x)=\psi(x,t)^{\top}(\tilde{P}\mathsf{M}_{\varepsilon}\tilde{P}- \mathsf{M}_{\varepsilon})\psi(x,t)\). In the proof of theorem 8, we have proved the bound on \(\Delta\beta_{t}(x)\) and \(\beta_{t}(x)\). From equation (43), we have,

\[|\psi(x,t)^{\top}(\tilde{P}\mathsf{M}_{\varepsilon}\tilde{P}- \mathsf{M}_{\varepsilon})\psi(x,t)| \leq 2\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{\eta}}\|\mathsf{M}_ {\varepsilon}\|^{1/2}\|\mathsf{M}_{\varepsilon}^{1/2}\psi(x,t)\|_{\mathcal{H} _{\eta}}\] \[\qquad\qquad+\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{\eta}}^{2} \|\mathsf{M}_{\varepsilon}\|\] \[=2c_{M}^{1/2}f(x;\mathsf{M}_{\varepsilon},\psi)^{1/2}\tilde{u}_{ 1}(x)+c_{M}\tilde{u}_{1}(x)^{2},\]

where \(c_{M}=\|\mathsf{M}_{\varepsilon}\|\) and we denoted by \(\tilde{u}_{1}(x)\) the quantity \(\tilde{u}_{1}(x)=\|(I-\tilde{P})\psi(x,t)\|_{\mathcal{H}_{\eta}}\) and we noted that \(\|\mathsf{M}_{\varepsilon}^{1/2}\psi(x,t)\|_{\mathcal{H}_{\eta}}^{2}=\psi(x,t )^{\top}\mathsf{M}_{\varepsilon}\psi(x,t)=f(x\,;\,\mathsf{M}_{\varepsilon}, \psi(x,t))\). Similarly, we have shown that

\[\frac{\partial^{2}p(x,t)}{\partial x_{i}x_{j}}-\frac{\partial^{2}\tilde{p}(x, t)}{\partial x_{i}x_{j}}=2\psi_{ji}^{\prime\prime}(x,t)^{\top}(\mathsf{M}_{ \varepsilon}-\tilde{P}\mathsf{M}_{\varepsilon}\tilde{P})\psi(x,t)+2\psi_{i}^{ \prime}(x,t)^{\top}(\mathsf{M}_{\varepsilon}-\tilde{P}\mathsf{M}_{ \varepsilon}\tilde{P})\psi_{j}^{\prime}(x,t).\] (58)

From equations (46) and (47)

\[\left|\psi_{ji}^{\prime\prime}(x,t)^{\top}(\mathsf{M}_{ \varepsilon}-\tilde{P}\mathsf{M}_{\varepsilon}\tilde{P})\psi(x,t)\right| \leq c_{M}\tilde{u}_{1}(x)\tilde{u}_{x_{j}x_{i}}(x)+c_{M}^{1/2}f(x;\mathsf{M} _{\varepsilon},\psi)^{1/2}\tilde{u}_{x_{j}x_{i}}(x)\] \[+c_{M}^{1/2}\tilde{f}_{x_{j}x_{i}}(x;\mathsf{M}_{ \varepsilon},\psi)^{1/2}\tilde{u}_{1}(x),\] (59)

where \(\tilde{f}_{x_{j}x_{i}}(x;\mathsf{M}_{\varepsilon},\psi)=\frac{\partial^{2} \psi(x,t)}{\partial x_{j}x_{i}}^{\top}\mathsf{M}_{\varepsilon}\frac{\partial^ {2}\psi(x,t)}{\partial x_{j}x_{i}}\) and \(\tilde{u}_{x_{j}x_{i}}(x)=\left\|(I-\tilde{P})\frac{\partial^{2}\psi(x,t)}{ \partial x_{j}x_{i}}\right\|_{\mathcal{H}_{\eta}}\).

And,

\[\psi_{i}^{\prime}(x,t)^{\top}(\mathsf{M}_{\varepsilon}-\tilde{P} \mathsf{M}_{\varepsilon}\tilde{P})\psi_{j}^{\prime}(x,t)\leq c_{M}\tilde{u}_{x_{ j}}(x)\tilde{u}_{x_{i}}(x)+c_{M}^{1/2}f_{x_{i}}(x;\mathsf{M}_{\varepsilon},\psi)^{1/2} \tilde{u}_{x_{j}}(x)\] \[+c_{M}^{1/2}\tilde{f}_{x_{j}}(x;\mathsf{M}_{\varepsilon},\psi)^{1 /2}\tilde{u}_{x_{i}}(x).\] (60)

Follwoing similar steps of calculations as that is Theorem 8 and Theroem 2, we get

\[\left\|\frac{\partial(\hat{p}(x,t)-\tilde{p}(x,t))}{\partial t}+\sum_{i=i}^{d} \frac{\partial(\mu_{i}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t)))}{\partial x_{i}}+(- \Delta)^{s}(p^{*}(x,t)(\hat{p}(x,t)-\tilde{p}(x,t))\right\|_{L^{2}(\tilde{x})}\]\[=P_{1}\|\mathsf{M}_{\varepsilon}\|C^{2}q_{\tau}^{2}e^{-\frac{2\xi \varepsilon}{\hbar}\log\frac{\varepsilon\pi}{\hbar}}R^{(d+1)/2}+P_{2}Cq_{\tau}e^ {-\frac{\sigma\pi}{\hbar}\log\frac{\sigma\pi}{\hbar}}\|\mathsf{M}_{\varepsilon }\|R^{(d+1)/2}\] \[\qquad\qquad+P_{3}\|\mathsf{M}_{\varepsilon}\|^{1/2}Cq_{\tau}e^{ -\frac{\sigma\pi}{\hbar}\log\frac{\sigma\pi}{\hbar}}\|\sqrt{p(x,t)}\|_{L^{2}( \tilde{\chi})},\] (61)

for some constant \(P_{1},P_{2}\) and \(P_{3}\). Now, let us compute \(\|\mathsf{M}_{\varepsilon}\|\). We have

\[\|\mathsf{M}_{\varepsilon}\|\leq Tr(\mathsf{M}_{\varepsilon})=\hat{C}\tau^{(d+ 1)/2}\left(1+\varepsilon^{\frac{2\beta}{\beta-2\varepsilon}}e^{\frac{\hat{C} \prime}{\beta}\varepsilon^{-\frac{2}{\beta-2\varepsilon}}}\right).\]

Let us choose, \(\tau=\frac{\hat{C}^{\prime}{\varepsilon}-\frac{\tau^{2}-2\tilde{\varepsilon} }{\beta-2\varepsilon}}{\frac{2\beta}{\beta-2\varepsilon}\log(\frac{1+R}{ \varepsilon})}\). Hence,

\[\|\mathsf{M}_{\varepsilon}\|\leq C_{3}(1+R)^{\frac{2\beta}{\beta-2\varepsilon }}\varepsilon^{-\frac{d+1}{\beta-2\varepsilon}},\]

For some constant \(C_{3}\). Hence, Then, note that \(\|p^{1/2}\|_{L^{2}(\mathfrak{X})}=\|p\|_{L^{1}(\mathfrak{X})}^{1/2}\). Hence, using the argument in [10], we have

\[\|p^{1/2}\|_{L^{2}(\tilde{\chi})}\leq 2R.\]

Term \(R\) comes from the fact that for each time instance \(p(x,t)\) is a density and \(t\in(0<R)\). Now,

\[=P_{1}C_{3}(1+R)^{\frac{2\beta}{\beta-2\varepsilon}}e^{-\frac{d+ 1}{\beta-2\varepsilon}}C^{2}q_{\tau}^{2}e^{\frac{-2\varepsilon\varepsilon}{ \hbar}\log\frac{\sigma\pi}{\hbar}}R^{(d+1)/2}+P_{2}Cq_{\tau}e^{-\frac{\sigma \pi}{\hbar}\log\frac{\sigma\pi}{\hbar}}C_{3}(1+R)^{\frac{2\beta}{\beta-2 \varepsilon}}\varepsilon^{-\frac{d+1}{\beta-2\varepsilon}}R^{(d+1)/2}\] \[\qquad\qquad+P_{3}C_{3}^{1/2}R(1+R)^{\frac{\beta}{\beta-2 \varepsilon}}e^{-\frac{d+1}{\beta(2-2\varepsilon)}}Cq_{\tau}e^{-\frac{\sigma \pi}{\hbar}\log\frac{\sigma\pi}{\hbar}}\|\sqrt{p(x,t)}\|_{L^{2}(\tilde{\chi})}.\]

By choosing \(h=c\sigma/s\) with \(s=\max(C^{\prime},(1+\frac{d+1}{2(\beta-2s)})\log\frac{1}{\varepsilon}+(1+ \frac{d}{2})\log(1+R)\,+\log(\hat{C})+e)\), for some big enough \(\hat{C}\). Since \(s\geq e\), then \(\log s\geq 1\), so

\[Ce^{-\frac{\varepsilon\varepsilon}{\hbar}\log\frac{\varepsilon\pi}{\hbar}}= Ce^{-s\log s}\leq Ce^{-s}\leq C^{\prime}(1+R)^{-d/2}\varepsilon^{1+\frac{d+1}{2( \beta-2\varepsilon)}}.\]

Hence, combining everything, we have from triangles inequality,

\[\left\|\frac{\partial(p^{*}(x,t)-\tilde{p}(x,t))}{\partial t}+ \sum_{i=i}^{d}\frac{\partial(\mu_{i}(x,t)(p^{*}(x,t)-\tilde{p}(x,t)))}{\partial x _{i}}+(-\Delta)^{s}(p^{*}(x,t)-\tilde{p}(x,t))\right\|_{L^{2}(\tilde{\chi})}\] \[=O(\varepsilon)\]

To conclude we recall the fact that \(\tilde{x}_{1},\ldots,\tilde{x}_{m}\) is a \(h\)-covering of \(T\), guarantees that the number of centers \(m\) in the covering satisfies

\[m\leq(1+\tfrac{2R\sqrt{d+1}}{\hbar})^{d+1}.\]

Then, since \(h\geq c\sigma/(C_{4}\log\frac{C_{5}\log(1+R)}{\varepsilon})\) with \(C_{4}=1+d/\min(2(\beta-2s),2)\) and \(C_{5}=(\hat{C})^{1/C_{4}}\), and since \(\sigma=\min(R,1/\sqrt{\tau})\), then \(R/\sigma=\max(1,R\sqrt{\tau})\leq 1+\sqrt{\hat{C}^{\prime}}\varepsilon^{-1/( \beta-2)}(\log\frac{1+R}{\varepsilon})^{-1/2}\), so after final calculation, we have

\[m^{\frac{1}{d+1}}\leq 1+2R\sqrt{d+1}/h\leq C_{8}+C_{9}\log\frac{1+R}{ \varepsilon}+C_{10}\varepsilon^{-\frac{1}{\beta-2\varepsilon}}\left(\log\frac {1+R}{\varepsilon}\right)^{1/2},\]

for some constants \(C_{8}\), \(C_{9}\) and \(C_{10}\) independent of \(\varepsilon\).