Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning

 Stephen McAleer

Carnegie Mellon University

&Gabriele Farina

MIT

&Gaoyue Zhou

Carnegie Mellon University

Mingzhi Wang

Peking University

&Yaodong Yang

Peking University

&Tuomas Sandholm

Carnegie Mellon University

###### Abstract

Recent algorithms have achieved superhuman performance at a number of two-player zero-sum games such as poker and go. However, many real-world situations are multi-player games. Zero-sum two-_team_ games, such as bridge and football, involve two teams where each member of the team shares the same reward with every other member of that team, and each team has the negative of the reward of the other team. A popular solution concept in this setting, called TMECor, assumes that teams can jointly correlate their strategies before play, but are not able to communicate during play. This setting is harder than two-player zero-sum games because each player on a team has different information and must use their public actions to signal to other members of the team. Prior works either have game-theoretic guarantees but only work in very small games, or are able to scale to large games but do not have game-theoretic guarantees. In this paper we introduce two algorithms: Team-PSRO, an extension of PSRO from two-player games to team games, and Team-PSRO Mix-and-Match which improves upon Team PSRO by better using population policies. In Team-PSRO, in every iteration both teams learn a joint best response to the opponent's meta-strategy via reinforcement learning. As the reinforcement learning joint best response approaches the optimal best response, Team-PSRO is guaranteed to converge to a TMECor. In experiments on Kuhn poker and Liar's Dice, we show that a tabular version of Team-PSRO converges to TMECor, and a version of Team PSRO using deep cooperative reinforcement learning beats self-play reinforcement learning in the large game of Google Research Football.

## 1 Introduction

Two-player zero-sum games have served as testbeds for artificial intelligence research. Algorithms that have achieved superhuman performance in games such as go (Silver et al., 2017) and poker (Bowling et al., 2015; Brown and Sandholm, 2017) are widely seen as milestones. These algorithms use game-theoretic techniques to find an approximate Nash equilibrium, in other words a strategy that cannot be exploited by humans. Despite these successes in two-player games, real-world scenarios often involve more than two players.

In this paper we study a class of games with multiple players where each player shares reward with the other players in one of two teams, which we call two-team games. A canonical example of a two-team game is bridge, a game in which current algorithms fail to compete with expert humans. There are multiple solution concepts for team games, but the one we consider is known as TMECor. In this setting, we allow each team to jointly sample a value from a distribution to correlate their strategies before play. This setting has many nice properties and is the natural solution concept for many team games. Two joint team strategies are in a TMECor if the value that a joint best response for an opponent team gets against each team is the same as the value when both teams are playing their current strategy.

There are two strands of research surrounding two-team games. The first has developed tabular methods that have game-theoretic guarantees. These algorithms work well on very small games but do not scale to large team games. The second has developed deep reinforcement learning algorithms that can scale to large two-team games, but end up being exploitable because they are not based on game-theoretic algorithms.

In this paper, we introduce the first scalable game-theoretic techniques for two-team games. We show that a straightforward extension of PSRO to team games, called Team PSRO, and is guaranteed to converge to an approximate TMECorr. We also introduce a novel algorithm called Team PSRO Mix-and-Match (Team PSRO-MM) that mixes and matches best responses to create a larger population and show that it outperforms Team PSRO. In our experiments, we show that Team PSRO-MM outperforms Team PSRO and that both methods outperform self-play on the large benchmark game of Google Research Football.

Team PSRO is based on _policy space response oracles (PSRO)_(Lanctot et al., 2017). PSRO is already one of the most promising methods for finding approximate Nash equilibrium in large two-player zero-sum games because it is simple to use with existing RL methods, it naturally provides a measure of approximate exploitability, and doesn't require full game-tree traversals. Methods based on PSRO such as AlphaStar (Vinyals et al., 2019) and Pipeline PSRO (McAleer et al., 2020) have achieved state-of-the-art performance on Starcraft and Barrage Stratego, respectively. Our method, called Team PSRO is the first PSRO-based algorithm for team games and we show that it converges to TMECor when the RL best response is strong enough.

Despite being the first scalable method for computing TMECor, Team PSRO does not efficiently use the population of best responses because each policy must be deployed with the corresponding policy in its joint best response. Building on Team PSRO we fix this issue by allowing policies to be mixed and matched with other policies in the population that are not the corresponding policy in its joint best response. We show that the resulting algorithm, called _Team PSRO Mix-and-Match (Team PSRO-MM)_ is able to empirically improve upon Team PSRO. We hypothesize that this is because computing best responses is more costly that evaluating the expected value of policies when the number of policies is low enough.

Our contributions include the following:

1. We show that a straightforward extension of PSRO to team games converges to TMECor.
2. We introduce a novel method based on Team PSRO, called Team PSRO Mix-and-Match, that emperically converges faster by better using the best responses in the population.

## 2 Background

Extensive-form games (EFGs) model games that are played on a game tree, and can capture both sequential and simultaneous moves, as well as private information. In this section, for simplicity we focus on four-player zero-sum games where two players--\(\mathsf{T}\mathsf{1}\) and \(\mathsf{T}\mathsf{2}\)--play as a team against two opponent players, denoted by \(\mathsf{O}\mathsf{1}\) and \(\mathsf{O}\mathsf{2}\). However, these methods generalize to games with more than two players, as shown in our experiments on Google Research Football.

Each node \(v\) in the game tree belongs to exactly one player \(i\in\{\mathsf{T}\mathsf{1},\mathsf{T}\mathsf{2},\mathsf{O}\mathsf{1},\mathsf{ O}\mathsf{2}\}\cup\{\mathsf{c}\}\) whose turn is to move. Player \(\mathsf{c}\) is a special player, called the _chance player_. It models exogenous stochasticity in the environment, such as drawing a card from a deck or tossing a coin. The edges leaving \(v\) represent the actions available at that node. Any node without outgoing edges is called a _leaf_ and represents an end state of the game. We denote the set of such nodes by \(Z\). Each \(z\in Z\) is associated with a tuple of payoffs specifying the payoff \(u_{i}(z)\) of each player \(i\in\{\mathsf{T}\mathsf{1},\mathsf{T}\mathsf{2},\mathsf{O}\mathsf{1},\mathsf{ O}\mathsf{2}\}\) at \(z\). The product of the probabilities of all actions of \(\mathsf{c}\) on the path from the root of the game to leaf \(z\) is denoted by \(p_{\mathrm{c}}(z)\).

Private information is represented via _information set_ (infoset). In particular, the set of nodes belonging to \(i\in\{\mathsf{T}\mathsf{1},\mathsf{T}\mathsf{2},\mathsf{O}\mathsf{1},\mathsf{O} \mathsf{2}\}\) is partitioned into a collection \(\mathcal{I}_{i}\) of non-empty sets: each \(I\in\mathcal{I}_{i}\) groups together nodes that Player \(i\) cannot distinguish among, given what they have observed. Necessarily, for any \(I\in\mathcal{I}_{i}\) and \(v,w\in I\), nodes \(v\) and \(w\) must have the same set of available actions. Consequently, we denote the set of actions available at all nodes of \(I\) by \(A_{I}\). As it is customary in the related literature, we assume _perfect recall_, that is, no player forgets what he/she knew earlier in the game. Finally, given players \(i\) and \(j\), two infostes \(I_{i}\in\mathcal{I}_{i}\), \(I_{j}\in\mathcal{I}_{j}\) are _connected_, denoted by \(I_{i}\coloneqq I_{j}\), if there exist \(v\in I_{i}\) and \(w\in I_{j}\) such that the path from the root to \(v\) passes through \(w\) or vice versa.

Sequences.The set of _sequences_ of Player \(i\), denoted by \(\Sigma_{i}\), is defined as \(\Sigma_{i}\coloneqq\{(I,a):I\in\mathcal{I}_{i},a\in A_{I}\}\cup\{\varnothing\}\), where the special element \(\varnothing\) is called the _empty sequence_ of Player \(i\). The _parent sequence_ of a node \(v\) of Player \(i\), denoted \(\sigma(v)\), is the last sequence (information set-action pair) for Player \(i\) encountered on the path from the root of the game to that node. Since the game has perfect recall, for each \(I\in\mathcal{I}_{i}\), nodes belonging to \(I\) share the same _parent sequence_. So, given \(I\in\mathcal{I}_{i}\), we denote by \(\sigma(I)\in\Sigma_{i}\) the unique parent sequence of nodes in \(I\). Additionally, we let \(\sigma(I)=\varnothing\) if Player \(i\) never acts before infoset \(I\).

**Reduced-normal-form plans.** A _reduced-normal-form_ plan \(\pi_{i}\) for Player \(i\) defines a choice of action for every information set \(I\in\mathcal{I}_{i}\) that is still reachable as a result of the other choices in \(\pi\) itself. The set of reduced-normal-form plans of Player \(i\) is denoted \(\Pi_{i}\). We denote by \(\Pi_{i}(I)\) the subset of reduced-normal-form plans that prescribe all actions for Player \(i\) on the path from the root to information set \(I\in\mathcal{I}_{i}\). Similarly, given \(\sigma=(I,a)\in\Sigma_{i}\), let \(\Pi_{i}(\sigma)\subseteq\Pi_{i}(I)\) be the set of reduced-normal-form plans belonging to \(\Pi_{i}(I)\) where Player \(i\) plays action \(a\) at \(I\), and let \(\Pi_{i}(\varnothing)\coloneqq\Pi_{i}\). Finally, given a leaf \(z\in Z\), we denote with \(\Pi_{i}(z)\subseteq\Pi_{i}\) the set of reduced-normal-form plans where Player \(i\) plays so as to reach \(z\).

**Sequence-form strategies.** A _sequence-form strategy_ is a compact strategy representation for perfect-recall players in EFGs (Romanovskii, 1962; Koller et al., 1996). Given a player \(i\in\{\mathsf{T}\mathsf{1},\mathsf{T}\mathsf{2},\mathsf{O}\mathsf{1},\mathsf{ O}\mathsf{2}\}\) and a normal-form strategy \(\mu\in\Delta(\Pi_{i})\),2 the sequence-form strategy induced by \(\mu\) is the real vector \(\bm{y}\), indexed over \(\sigma\in\Sigma_{i}\), defined as \(y[\sigma]\coloneqq\sum_{\pi\in\Pi_{i}(\sigma)}\mu(\pi)\). The set of sequence-form strategies that can be induced as \(\mu\) varies over \(\Delta(\Pi_{i})\) is denoted by \(\mathcal{Y}_{i}\) and is known to be a convex polytope (called the _sequence-form polytope_) defined by a number of constraints equal to \(|\mathcal{I}_{i}|\)(Koller et al., 1996).

Footnote 2: \(\Delta(X)\) denotes the probability simplex over the finite set \(X\).

**TMECor as a Bilinear Saddle-Point Problem.** A TMECor strategy is a probability distribution \(\mu_{\mathsf{T}}\) over the set of randomized strategy profiles \(\mathcal{Y}_{\mathsf{T}\mathsf{1}}\times\mathcal{Y}_{\mathsf{T}\mathsf{2}}\) that guarantees maximum expected utility for the team against the best-responding opponent team \(\{\mathsf{O}\mathsf{1},\mathsf{O}\mathsf{2}\}\). Since each player has perfect recall, any randomized strategy for a player is equivalent to a distribution over reduced-normal-form pure strategies (Kuhn, 1953). Hence, any distribution over profiles of randomized strategies of the team members can be expressed in an equivalent way as a distribution over _deterministic_ strategy profiles \(\Pi_{\mathsf{T}\mathsf{1}}\times\Pi_{\mathsf{T}\mathsf{2}}\). Denote by \(\mathcal{P}(\Pi_{\mathsf{T}})\) all possible combinations of pure strategies of a population \(\Pi_{\mathsf{T}}\): \(\{(\pi_{\mathsf{T}\mathsf{1}},\pi_{\mathsf{T}\mathsf{2}})|\pi_{\mathsf{T}\mathsf{ 1}},\pi_{\mathsf{T}\mathsf{2}}\in\Pi_{\mathsf{T}}\}\). The benefit of this transformation is that \(\Pi_{\mathsf{T}\mathsf{1}}\times\Pi_{\mathsf{T}\mathsf{2}}\) is a finite set, unlike \(\mathcal{Y}_{\mathsf{T}\mathsf{1}}\times\mathcal{Y}_{\mathsf{T}\mathsf{2}}\). For this reason, TMECor is usually defined in the literature as a distribution over \(\Pi_{\mathsf{T}\mathsf{1}}\times\Pi_{\mathsf{T}\mathsf{2}}\) without loss of generality. We will follow the same approach in our characterization.

For each leaf \(z\), let \(\hat{u}_{\mathsf{T}}(z)\coloneqq(u_{\mathsf{T}\mathsf{1}}(z)+u_{\mathsf{T} \mathsf{2}}(z))p_{\mathsf{C}}(z)\). The expected utility of the team can be written as the following function of the distributions of play \(\mu_{\mathsf{T}}\in\Delta(\Pi_{\mathsf{T}\mathsf{1}}\times\Pi_{\mathsf{T} \mathsf{2}}),\mu_{\mathsf{O}}\in\Delta(\Pi_{\mathsf{O}\mathsf{1}}\times\Pi_{ \mathsf{O}\mathsf{2}})\):

\[u_{\mathsf{T}}(\mu_{\mathsf{T}},\mu_{\mathsf{O}})\!:=\!\sum_{z\in Z}\hat{u}_{ \mathsf{T}}(z)\!\!\left(\!\sum_{\begin{subarray}{c}\pi_{\mathsf{T}\mathsf{1}} \in\Pi_{\mathsf{T}\mathsf{1}}(z)\\ \pi_{\mathsf{T}\mathsf{2}}\in\Pi_{\mathsf{T}\mathsf{2}}(z)\end{subarray}}\!\!coordinated strategy \(\mu_{\mathsf{T}}\in\Delta(\Pi_{\mathsf{T}\uparrow}\times\Pi_{\mathsf{T}2})\) and the opponent team plays according to the coordinated strategy \(\mu_{\mathsf{O}}\in\Delta(\Pi_{\mathsf{O}\uparrow}\times\Pi_{\mathsf{O}2})\). In the zero-sum setting, this amounts to finding a solution of the optimization problem

\[\max_{\mu_{\mathsf{T}}\in\Delta(\Pi_{\mathsf{T}\uparrow}\times\Pi_{\mathsf{T} \parallel})}\min_{\mu_{\mathsf{O}}\in\Delta(\Pi_{\mathsf{O}\uparrow}\times \Pi_{\mathsf{O}2})}u_{\mathsf{T}}(\mu_{\mathsf{T}},\mu_{\mathsf{O}}).\] (2)

Note that if a pair of joint strategies \((\mu_{\mathsf{T}},\mu_{\mathsf{O}})\) are in a TMECor, then

\[u_{\mathsf{T}}(\mu_{\mathsf{T}},\mu_{\mathsf{O}})=u_{\mathsf{T}}(\mathbbm{B} \mathbbm{R}_{\mathsf{T}}(\mu_{\mathsf{O}}),\mu_{\mathsf{O}})=u_{\mathsf{T}}( \mu_{\mathsf{T}},\mathbbm{B}\mathbbm{R}_{\mathsf{O}}(\mu_{\mathsf{T}}))\] (3)

### Approximate TMECor

Define an \(\epsilon\)-best response (\(\epsilon\)-BR) \(\mathbbm{B}\mathbb{R}_{\mathsf{T}}^{\epsilon}(\mu_{\mathsf{O}})\) as any coordinated strategy that achieves expected utility against the opponent coordinated strategy within \(\epsilon\) of optimal: \(u_{\mathsf{T}}(\mathbbm{B}\mathbb{R}_{\mathsf{T}}(\mu_{\mathsf{O}}),\mu_{ \mathsf{O}})-\epsilon\leq u_{\mathsf{T}}(\mathbbm{B}\mathbb{R}_{\mathsf{T}}^{ \epsilon}(\mu_{\mathsf{O}}),\mu_{\mathsf{O}})\). The _exploitability_\(e(\mu_{\mathsf{T}},\mu_{\mathsf{O}})\) of a pair of correlated strategies \((\mu_{\mathsf{T}},\mu_{\mathsf{O}})\) is defined as \(e(\mu_{\mathsf{T}},\mu_{\mathsf{O}})=u_{\mathsf{T}}(\mathbbm{B}\mathbb{R}_{ \mathsf{T}}(\mu_{\mathsf{O}}),\mu_{\mathsf{O}})+u_{\mathsf{O}}(\mu_{\mathsf{ T}},\mathbbm{B}\mathbb{R}_{\mathsf{O}}(\mu_{\mathsf{T}}))\). A pair of joint strategies \((\mu_{\mathsf{T}},\mu_{\mathsf{O}})\) is in an \(\epsilon\)_-approximate TMECor_ if \(e(\mu_{\mathsf{T}},\mu_{\mathsf{O}})\leq\epsilon\)

## 3 Related Work

### Double Oracle (DO) and Policy Space Response Oracles (PSRO)

Double Oracle (McMahan et al., 2003) is an algorithm for finding a NE in two-player zero-sum normal-form games. The algorithm works by maintaining a population of strategies for each player. Each iteration a NE is computed for the game restricted to strategies in each player's population. Then, a best response to this NE for each player is computed and added to the population. Although in the worst case DO must expand all pure strategies, in many games DO empirically terminates early and outperforms existing methods. Policy-Space Response Oracles (PSRO) Lanctot et al. (2017) scales DO to large games by using reinforcement learning to approximate a best response. The restricted-game NE is computed on the empirical game matrix generated by having each policy in the population play each opponent policy and tracking average utility in a payoff matrix (Wellman, 2006).

There are a number of methods related to PSRO. NXDO (McAleer et al., 2021; Tang et al., 2023) iteratively adds reinforcement learning policies to a population but solve an extensive-form restricted game and has shown to be more efficient than PSRO in certain games. AlphaStar (Vinyals et al., 2019) trains a population of policies through a procedure that is somewhat similar to PSRO. AlphaStar also uses some elements of self-play when constructing its population, and outputs a meta-Nash equilibrium of the population at test time. P2SRO (McAleer et al., 2020) parallelizes PSRO with convergence guarantees. Other work has generalized PSRO to more players (Muller et al., 2020; Marris et al., 2021), incorporated diversity objectives to cover more of the strategy space (Liu et al., 2021; Perez-Nieves et al., 2021; McAleer et al., 2022; Slumbers et al., 2023; Yao et al., 2023), and meta-learned the meta-distribution (Feng et al., 2021). PSRO has also been applied to core problems in game theory (Zhang et al., 2023) and reinforcement learning (Liang et al., 2023).

### Team Games

In this paper we study the TMECor Celli & Gatti (2018); Farina et al. (2018); Zhang & Sandholm (2021); Zhang et al. (2022) setting where players on the same team are allowed to coordinate before playing. A related solution concept is an equilibrium where team members are not able to coordinate, which is known as a _team-maxmin equilibrium_ (TME) strategy (von Stengel & Koller, 1997; Basilico et al., 2017; Zhang & An, 2020, 2020, 2021; Kalogiannis et al., 2021; Anagnostides et al., 2023). A solution of this type yields the maximum expected utility for the team players against a best-responding opponent. The TMECor solution concept has several advantages over TME. First, the team is guaranteed at least as much expected utility under TMECor than under TME (Celli & Gatti, 2018). Second, the TMECor objective is convex while the TME objective is not convex.

### Cooperative Reinforcement Learning

In this section we focus on the regime of _centralized training and decentralized execution (CTDE)_ in cooperative reinforcement learning. In the CTDE regime, we assume to have access to all agent'sobservations during training time, which is a reasonable assumption for our setting of computational game solving. CTDE methods such as MADDPG Lowe et al. (2017) and COMA Foerster et al. (2018) make use of this centralized training learning a centralized critic that can condition on private information. Although this critic is taken away at test time and only the actor is used in decentralized execution, by conditioning on all relevant information, the centralized critic can reduce much more variance than an individual agent's critic.

Multi-Agent PPO (MAPPO) (Yu et al., 2021; Kuba et al., 2022) trains one PPO agent for the whole team with a centralized critic, among other tricks. Although off-policy CTDE methods such as MADDPG and QMix(Rashid et al., 2018; Schafer et al., 2023; Mguni et al., 2023) tend to outperform on-policy methods such as COMA (Papoudakis et al., 2020; Wang et al., 2021), in practice MAPPO can be a surprisingly effective and stable baseline, and we use it as our joint best response in Team PSRO.

### Deep RL for Team Games

A number of previous works have studied deep reinforcement learning methods in team games. Jaderberg et al. (2019) and Liu et al. (2021, 2019) train deep RL agents via self play and population based training to achieve high performance on a capture the flag video game and a simulated soccer environment, respectively. Berner et al. (2019) use self-play reinforcement learning to achieve expert level performance on Dota. None of these methods are grounded in game theoretic techniques, however, and may not be expected to converge to a TMECor. Concurrent to our work, Xu et al. (2023) propose an algorithm that is similar to Team PSRO, but they do not make a connection to TMECor and do not propose the idea of mixing and matching.

## 4 Team PSRO

As described in the previous section, existing methods for learning approximate TMECorrs are tabular, and as a result will not scale to large games. In this paper we propose Team-PSRO. Team-PSRO makes the simple observation that approximate joint best responses can be learned via cooperative reinforcement learning. The resulting algorithm is very similar to PSRO in that every iteration, a best response is computed against the opponent's restricted distribution and the resulting policy is added to a population. However, instead of computing single-agent best responses for a single player, the best responses for Team-PSRO are joint best responses for a single team. These joint best responses are a set of policies for each member of the team and are learned through cooperative reinforcement learning.

### Tabular Team Double Oracle

In this section we introduce _team double oracle_, a double oracle method for computing TMECor in zero-sum games with two teams. Team DO is conceptually the same as existing single oracle algorithms (Celli and Gatti, 2018) for team games but extends these algorithms to two team games. Team DO is very similar to the original double oracle algorithm for two-player zero-sum normal form games. Team DO maintains a population of pure joint strategies for each team \(\Pi_{\mathbb{T}},\Pi_{\mathbb{O}}\). Each pure strategy in a team's population is a pure joint strategy for each player on that team \(\mu_{\mathbb{T}}=(\pi_{\mathbb{T}_{1}},\pi_{\mathbb{T}_{2}})\). Similar to DO, we construct a restricted game in the space of pure strategies of the populations for each team. Our restricted game \(G(\Pi_{\mathbb{T}},\Pi_{\mathbb{O}})\) is a two-player zero-sum normal-form game that takes as actions the pure strategies \(\mu_{\mathbb{T}}\in\Pi_{\mathbb{T}}\) for the team and \(\mu_{\mathbb{O}}\in\Pi_{\mathbb{O}}\) for the opponent team. Every iteration, the NE \((\mu_{\mathbb{T}}^{\tau},\mu_{\mathbb{O}}^{c})\) of the restricted game \(G(\Pi_{\mathbb{T}}^{t},\Pi_{\mathbb{O}}^{t})\) is computed. Next, a best response to this NE in the original game \(\mathbb{BR}_{\mathbb{O}}(\mu_{\mathbb{T}}^{\tau})\) is computed and added to the population. Team DO terminates when the best response for both players does not improve over the Nash value of the restricted game. Like DO, since there are a finite number of pure strategies, team DO must terminate (potentially in a number of iterations exponential in the size of the game). When it does terminate, the NE of the restricted game is a TMECor of the original game because no best response can improve on the Nash value. Team DO is described in Algorithm 1.

Compared to other methods for finding TMECor, team DO has a number of downsides. First, there are no known convergence rates for double oracle algorithms in general and team DO in particular. In fact, there exist games where Team DO must expand all joint pure strategies of the game before 

[MISSING_PAGE_FAIL:6]

### Tabular Team Double Oracle Mix-and-Match

In this section we introduce _team double oracle mix-and-match (Team DO-MM)_, which builds on Team DO by adding additional joint policies to the population for free. Similarly to Team DO, Team DO-MM maintains a population of pure joint strategies for each team \(\Pi_{\tau},\Pi_{\text{O}}\). Each pure strategy in a team's population is a pure joint strategy for each player on that team \(\mu_{\text{T}}=(\pi_{\text{T}\uparrow},\pi_{\text{T}\downarrow})\). In Team DO-MM, however, we construct a restricted game in an expanded space of pure strategies that considers all possible mixtures of policies from each joint best response. Contrasted with Team DO, Team DO-MM considers pure strategies that mix and match policies from different best responses of the players within each team. Our restricted game \(G(\mathcal{P}(\Pi_{\text{T}}),\mathcal{P}(\Pi_{\text{O}}))\) is a two-player zero-sum normal-form game that takes as actions the pure strategies \(\mu_{\text{T}}\in\mathcal{P}(\Pi_{\text{T}})\) for the team and \(\mu_{\text{O}}\in\mathcal{P}(\Pi_{\text{O}})\) for the opponent team. Similar to Team DO, every iteration, the NE \((\mu_{\text{T}}^{\tau},\mu_{\text{O}}^{\tau})\) of the restricted game \(G(\mathcal{P}(\Pi_{\text{T}}^{\text{t}}),\mathcal{P}(\Pi_{\text{O}}^{\tau}))\) is computed. Next, a best response to this NE in the original game \(\mathbb{BR}_{\text{O}}(\mu_{\text{T}}^{\tau})\) is computed and added to the population. Team DO-MM also terminates when the best response for both players does not improve over the Nash value of the restricted game. Because upon termination no best response can improve on the Nash value, Team DO-MM inherits the convergence guarantee of Team DO to TMECor. Team DO-MM is shown as a one-line change in Algorithm 1.

**Proposition 1**.: _Tabular Team DO and Team DO-MM with exact best responses converge to a TMECor._

Proof.: Proof is contained in the Appendix. 

### Team PSRO

We now describe our main contribution, _team PSRO_ and _team PSRO Mix-and-Match_. All existing tabular algorithms cannot scale to a game the size of Google Research Football. And existing algorithms such as self play do not minimize exploitability, even though they might do well sometimes in practice. Team PSRO scales up Team DO to large games by swapping out an exact joint best response \(\mathbb{BR}_{\text{T}}(\mu_{\text{O}}^{\tau})\) for an approximate joint best response \(\beta_{\text{T}}\) that is trained through reinforcement learning. Instead of learning a policy for each player individually, as in PSRO, Team PSRO trains a joint best response every iteration. This joint best response is a pair of policies that are trained using cooperative RL. In particular, we use MAPPO to train the joint best response. As in PSRO, once the joint best response for a team is trained, it is added to to that team's population \(\Pi_{\text{T}}^{t}\) and a two-player zero-sum normal form restricted game \(G(\Pi_{\text{T}}^{t},\Pi_{\text{O}}^{t})\) is created. The actions of this normal form game correspond to pure joint strategies in the population and the payoffs correspond to the estimated expected value when one team chooses one joint policy and the other team chooses another joint policy \(u_{\text{T}}(\beta_{\text{T}}(\mu_{\text{O}}),\mu_{\text{O}}^{r})\). Payoff values in the restricted game are estimated by sampling rollouts from both teams' joint policies. Team PSRO is described in Algorithm 2.

Note that when the joint best responses achieve expected value within \(\epsilon\) of the optimal best response, then upon convergence Team PSRO will output an approximate TMECor. Even when the cooperative reinforcement learning is not within \(\epsilon\) of the optimal best response, we find that in practice the reward that joint best responses achieve against the restricted NE tends to go down over time. We call this evaluation metric of the sum of the reward that both joint best responses achieve against the restricted NE _approximate exploitability_ because the this is a lower bound on the exact exploitability. The better the cooperative RL algorithm is in team PSRO, the more accurately the approximate exploitability will match true exploitability. As mentioned previously, a key benefit of Team PSRO is that it outputs approximate exploitability as a byproduct of the algorithm. If the RL algorithm is very strong and Team PSRO converges, then one can be reasonably sure that the strategy produced is strong.

The MAPPO algorithm we use contains a centralized critic value function that has access to the history of the game. Since we train the joint best response centrally but execute decentrally, MAPPO allows the critic to see the partner's cards at training time to reduce variance in the update. Another MAPPO trick we use is to share the parameters of the neural network between partners in a joint best response. This has been found to help stabilize training. Although we use MAPPO as our joint RL best response, in principle any cooperative RL algorithm could be used to learn an approximate joint best response.

### Team PSRO Mix-and-Match

Similarly to how Team DO-MM builds on Team DO by adding mixed-and-matched pure strategies, Team PSRO Mix-and-Match (Team PSRO-MM) builds on Team PSRO by adding mixed-and-matched deep RL policies. As in Team PSRO, once the joint best response for a team is trained, it is added to to that team's population \(\Pi_{\uparrow}^{t}\). Similarly to Team DO-MM, an expanded two-player zero-sum normal form restricted game \(G(\mathcal{P}(\Pi_{\uparrow}^{t}),\mathcal{P}(\Pi_{\circ}^{t}))\) is created. The actions of this normal form game correspond to mixed-and-matched joint strategies in the population and the payoffs correspond to the estimated expected value when one team chooses one joint policy and the other team chooses another joint policy \(u_{\uparrow}(\beta_{\uparrow}(\mu_{\O}),\mu_{\O}^{r})\). Payoff values in the restricted game are estimated by sampling rollouts from both teams' joint policies. Team PSRO-MM is described in Algorithm 2.

``` Result: Approximate TMECorr Input: Initial population \(\Pi_{\uparrow}^{0},\Pi_{\O}^{0}\) repeat{for\(t=0,1,\dots\)} ifTeam-PSRO-MMthen \((\mu_{\uparrow}^{r},\mu_{\O}^{r})\leftarrow\) NE in restricted game \(G(\mathcal{P}(\Pi_{\uparrow}^{t}),\mathcal{P}(\Pi_{\O}^{t}))\) else \((\mu_{\uparrow}^{r},\mu_{\O}^{r})\leftarrow\) NE in restricted game \(G(\Pi_{\uparrow}^{t},\Pi_{\O}^{t})\) for\(m\) iterations do  Update joint best response \(\beta_{\uparrow}\) toward \(\mathbb{BR}_{\uparrow}(\mu_{\O}^{r})\) via cooperative RL  Update joint best response \(\beta_{\O}\) toward \(\mathbb{BR}_{\O}(\mu_{\uparrow}^{r})\) via cooperative RL \(\Pi_{\uparrow}^{t+1}\leftarrow\Pi_{\uparrow}^{t}\cup\{\beta_{\uparrow}\}\) \(\Pi_{\O}^{t+1}\leftarrow\Pi_{\O}^{t}\cup\{\beta_{\O}\}\) until\(u_{\uparrow}(\beta_{\uparrow}(\mu_{\O}),\mu_{\O}^{r})+u_{\O}(\mu_{\uparrow}^{r}, \beta_{\O}(\mu_{\uparrow}))\leq u_{\uparrow}(\mu_{\uparrow}^{r},\mu_{\O}^{r})+ u_{\O}(\mu_{\uparrow}^{r},\mu_{\O}^{r})+\epsilon\) Return:\((\mu_{\uparrow}^{r},\mu_{\O}^{r})\) ```

**Algorithm 2** Team PSRO (Mix-and-Match)

## 5 Experiments

### Team DO Experiments

We run Team DO and Team DO-MM on two small games, Kuhn poker and Liar's dice, and present the results in Figure 1. Although we do not know of any existing regret bounds for double oracle techniques, these results demonstrate that in practice double oracle methods work well and can find approximate TMECor in a reasonable number of iterations. Notably, Team DO-MM is able to achieve faster convergence compared to Team DO in the number of iterations. However, we found that since calculating best responses in this game does not take much time, computing the extra payoff values caused Team DO-MM to converge much slower in wall clock time compared to Team DO.

Figure 1: Experimental results in tabular gamesAdditionally, both Team-DO methods outperform Fictitious Team Play (Farina et al., 2018), which adds team best responses to the opponent average strategy every iteration. In large games, as shown below, finding a best response requires much more time than evaluating the expected value of two team policies, so Team DO-MM converges faster in wall clock time as well. We include a detailed analysis of wall clock time on these tabular experiments in the appendix.

### Team PSRO Experiments

We demonstrate that Team PSRO can scale to large games by training it on the benchmark game of Google Research Football (GRF) (Kurach et al., 2020). Because there do not exist any other game-theoretic techniques for approaching TMECor in large games, we only compare to self play. Although we do not know of any theoretical guarantees that self play will converge to TMECor, methods based on self play (Berner et al., 2019; Liu et al., 2019; Jaderberg et al., 2019) have achieved impressive performance on large team games. As a result, we believe that self-play reinforcement learning is a strong benchmark to beat. In these experiments we use the four-player version of GRF and modify it so that each player only observes the information of his own team, the ball, and three closest opponents' information (which means that there are always two opponents that are not observable for a player). We also include results on the perfect-information version of the game in the Appendix and find that the same pattern holds where Team PSRO-MM is better than Team PSRO which is better than self-play. We also include further training details in the Appendix.

We use the same reinforcement learning algorithm for both Team PSRO and self play, namely MAPPO with a centralized critic and shared parameters for a team. We allow the centralized critic to see all players' information. Self play reinforcement learning continually trains one joint policy for one team against the other joint policy for the other team and does not make use of a population like Team PSRO. In Figure 2, we see that Team PSRO and Team PSRO-MM are both able to outperform self-play across all three measures of Elo, average goal difference against the built-in AI, and relative population performance (Balduzzi et al., 2019). Furthermore, Team PSRO-MM outperforms Team PSRO with a small additional cost of extra rollouts to evaluate the matrix payoffs. Since this is such a large game, the time needed to compute the BRs is much more than the time needed to evaluate two team policies, so the extra time is negligible.

## 6 Discussion

### Limitations

As with any double oracle algorithm, the main limitation of our algorithm is that it suffers from a lack of practical guarantees. It is possible that all pure strategies of the game must be expanded before termination, and indeed we find that in Kuhn poker, even using an exact best response plateaus indefinitely. With that being said, sometimes exploitability can be too strict of a measure if the exact best response is hard to find. In the case of team games, where finding an exact best response is NP-Hard, perhaps being approximately unexploitable against a reinforcement learning best response is a reasonable goal, and one that we achieve with Team-PSRO.

Figure 2: Team PSRO Results on Google Research Football. Both Team PSRO and Team PSRO-MM outperform self play, with Team PSRO-MM performing the best.

### Future Work

There are many future directions to build on this work. First, we are interested in continuing to scale up to large games, with the goal of becoming superhuman at bridge. A second direction of future work involves studying the problem of cooperative reinforcement learning directly, where improvements should directly transfer to our setting. A third research direction is transferring over new PSRO algorithms such as Anytime PSRO (McAleer et al., 2022b), and NXDO (McAleer et al., 2021). Related efforts in solving team games using regret minimization and subgame solving are possibly complimentary to our approach.

## 7 Acknowledgements

This material is based on work supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award A240108S001.

## References

* Anagnostides et al. (2023) Anagnostides, I., Kalogiannis, F., Panageas, I., Vlatakis-Gkaragkounis, E.-V., and McAleer, S. Algorithms and complexity for computing nash equilibria in adversarial team games. _Economics and Computation (EC)_, 2023.
* Balduzzi et al. (2019) Balduzzi, D., Garnelo, M., Bachrach, Y., Czarnecki, W., Perolat, J., Jaderberg, M., and Graepel, T. Open-ended learning in symmetric zero-sum games. _International Conference on Machine Learning (ICML)_, 2019.
* Basilico et al. (2017) Basilico, N., Celli, A., Nittis, G. D., and Gatti, N. Team-maxmin equilibrium: efficiency bounds and algorithms. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_, pp. 356-362, 2017.
* Berner et al. (2019a) Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019a.
* Berner et al. (2019b) Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019b.
* Bowling et al. (2015) Bowling, M., Burch, N., Johanson, M., and Tammelin, O. Heads-up limit hold'em poker is solved. _Science_, 347(6218), January 2015.
* Brown & Sandholm (2017) Brown, N. and Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, pp. eaao1733, Dec. 2017.
* Celli & Gatti (2018) Celli, A. and Gatti, N. Computational results for extensive-form adversarial team games. In _AAAI Conference on Artificial Intelligence (AAAI)_, pp. 965-972, 2018.
* Farina & Sandholm (2020) Farina, G. and Sandholm, T. Polynomial-time computation of optimal correlated equilibria in two-player extensive-form games with public chance moves and beyond. In _ArXiv preprint_, 2020.
* Farina et al. (2018a) Farina, G., Celli, A., Gatti, N., and Sandholm, T. Ex ante coordination and collusion in zero-sum multi-player extensive-form games. In _Advances in Neural Information Processing Systems_, pp. 9638-9648, 2018a.
* Farina et al. (2018b) Farina, G., Celli, A., Gatti, N., and Sandholm, T. Ex ante coordination and collusion in zero-sum multi-player extensive-form games. _Advances in Neural Information Processing Systems_, 31, 2018b.
* Farina et al. (2021) Farina, G., Celli, A., Gatti, N., and Sandholm, T. Connecting optimal ex-ante collusion in teams to extensive-form correlation: Faster algorithms and positive complexity results. In _International Conference on Machine Learning_, 2021.
* Farina et al. (2018)Feng, X., Slumbers, O., Wan, Z., Liu, B., McAleer, S., Wen, Y., Wang, J., and Yang, Y. Neural auto-curricula in two-player zero-sum games. _Advances in Neural Information Processing Systems_, 34, 2021.
* Foerster et al. (2018) Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.
* Jaderberg et al. (2019) Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., Beattie, C., Rabinowitz, N. C., Morcos, A. S., Ruderman, A., et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. _Science_, 364(6443):859-865, 2019.
* Kalogiannis et al. (2021) Kalogiannis, F., Vlatakis-Gkaragkounis, E.-V., and Panageas, I. Teamwork makes von neumann work: Min-max optimization in two-team zero-sum games. _arXiv preprint arXiv:2111.04178_, 2021.
* Koller et al. (1996) Koller, D., Megiddo, N., and von Stengel, B. Efficient computation of equilibria for extensive two-person games. _Games and Economic Behavior_, 14(2), 1996.
* Kuba et al. (2022) Kuba, J. G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y. Trust region policy optimisation in multi-agent reinforcement learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=EcGGFkNTxdJ.
* Kuhn (1953) Kuhn, H. W. Extensive games and the problem of information. In Kuhn, H. W. and Tucker, A. W. (eds.), _Contributions to the Theory of Games_, volume 2 of _Annals of Mathematics Studies, 28_, pp. 193-216. Princeton University Press, Princeton, NJ, 1953.
* Kurach et al. (2020) Kurach, K., Raichuk, A., Stanczyk, P., Zajac, M., Bachem, O., Espeholt, L., Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., et al. Google research football: A novel reinforcement learning environment. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pp. 4501-4510, 2020.
* Lanctot et al. (2017) Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Perolat, J., Silver, D., and Graepel, T. A unified game-theoretic approach to multiagent reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Liang et al. (2023) Liang, Y., Sun, Y., Zheng, R., Liu, X., Sandholm, T., Huang, F., and McAleer, S. Game-theoretic robust reinforcement learning handles temporally-coupled perturbations. _arXiv preprint arXiv:2307.12062_, 2023.
* Liu et al. (2019) Liu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T. Emergent coordination through competition. _International Conference on Learning Representations_, 2019.
* Liu et al. (2021a) Liu, S., Lever, G., Wang, Z., Merel, J., Eslami, S., Hennes, D., Czarnecki, W. M., Tassa, Y., Omidshafiei, S., Abdolmaleki, A., et al. From motor control to team play in simulated humanoid football. _arXiv preprint arXiv:2105.12196_, 2021a.
* Liu et al. (2021b) Liu, X., Jia, H., Wen, Y., Hu, Y., Chen, Y., Fan, C., Hu, Z., and Yang, Y. Towards unifying behavioral and response diversity for open-ended learning in zero-sum games. _Advances in Neural Information Processing Systems_, 34:941-952, 2021b.
* Lowe et al. (2017) Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, pp. 6379-6390, 2017.
* Marris et al. (2021) Marris, L., Muller, P., Lanctot, M., Tuyls, K., and Graepel, T. Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers. In _International Conference on Machine Learning_, pp. 7480-7491. PMLR, 2021.
* McAleer et al. (2020) McAleer, S., Lanier, J., Fox, R., and Baldi, P. Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games. In _Advances in Neural Information Processing Systems_, 2020.
* McAleer et al. (2018)McAleer, S., Lanier, J., Baldi, P., and Fox, R. XDO: A double oracle algorithm for extensive-form games. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* McAleer et al. (2022a) McAleer, S., Lanier, J., Wang, K., Baldi, P., Fox, R., and Sandholm, T. Self-play psro: Toward optimal populations in two-player zero-sum games. _arXiv preprint arXiv:2207.06541_, 2022a.
* McAleer et al. (2022b) McAleer, S., Wang, K., Lanier, J. B., Lanctot, M., Baldi, P., Sandholm, T., and Fox, R. Anytime PSRO for two-player zero-sum games. _CoRR_, abs/2201.07700, 2022b. URL https://arxiv.org/abs/2201.07700.
* McMahan et al. (2003) McMahan, H. B., Gordon, G. J., and Blum, A. Planning in the presence of cost functions controlled by an adversary. _Proceedings of the 20th International Conference on Machine Learning (ICML)_, 2003.
* Mguni et al. (2023) Mguni, D., Jafferjee, T., Chen, H., Wang, J., Fei, L., Feng, X., McAleer, S., Tong, F., Wang, J., and Yang, Y. Mansa: Learning fast and slow in multi-agent systems. _International Conference on Machine Learning (ICML)_, 2023.
* Muller et al. (2020) Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., Hennes, D., Marris, L., Lanctot, M., Hughes, E., et al. A generalized training approach for multiagent learning. _International Conference on Learning Representations (ICLR)_, 2020.
* Papoudakis et al. (2020) Papoudakis, G., Christiano, F., Schafer, L., and Albrecht, S. V. Comparative evaluation of multi-agent deep reinforcement learning algorithms. _arXiv preprint arXiv:2006.07869_, 2020.
* Perez-Nieves et al. (2021) Perez-Nieves, N., Yang, Y., Slumbers, O., Mguni, D. H., Wen, Y., and Wang, J. Modelling behavioural diversity for learning in open-ended games. In _International Conference on Machine Learning_, pp. 8514-8524. PMLR, 2021.
* Rashid et al. (2018) Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In Dy, J. and Krause, A. (eds.), _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pp. 4295-4304. PMLR, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/rashid18a.html.
* Romanovskii (1962) Romanovskii, I. Reduction of a game with complete memory to a matrix game. _Soviet Mathematics_, 3, 1962.
* Schafer et al. (2023) Schafer, L., Slumbers, O., McAleer, S., Du, Y., Albrecht, S. V., and Mguni, D. Ensemble value functions for efficient exploration in multi-agent reinforcement learning. _arXiv preprint arXiv:2302.03439_, 2023.
* Silver et al. (2017) Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., Driessche, G. v. d., Graepel, T., and Hassabis, D. Mastering the game of go without human knowledge. _Nature_, 550(7676):354-359, 10 2017. ISSN 0028-0836. doi: 10.1038/nature24270. URL http:https://doi.org/10.1038/nature24270.
* Slumbers et al. (2023) Slumbers, O., Mguni, D. H., McAleer, S., Wang, J., and Yang, Y. Learning risk-averse equilibria in multi-agent systems. _International Conference on Machine Learning_, 2023.
* Tang et al. (2023) Tang, X., Dinh, L. C., McAleer, S. M., and Yang, Y. Regret-minimizing double oracle for extensive-form games. _International Conference on Machine Learning_, 2023.
* Vinyals et al. (2019a) Vinyals, O., Babuschkin, I., Czarnecki, M. W., Mathieu, M., Dudzik, A., Chung, J., Choi, H. D., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, P. J., Jaderberg, M., Vezhnevets, S. A., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, L. T., Gulcehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wunsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., and Silver, D. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, pp. 1-5, 2019a.

* Vinyals et al. (2019) Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019b.
* von Stengel & Koller (1997) von Stengel, B. and Koller, D. Team-maxmin equilibria. _Games and Economic Behavior_, 21(1-2):309-321, 1997.
* Wang et al. (2021) Wang, Y., Han, B., Wang, T., Dong, H., and Zhang, C. {DOP}: Off-policy multi-agent decomposed policy gradients. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=6FqKiVAdI3Y.
* Wellman (2006) Wellman, M. P. Methods for empirical game-theoretic analysis. _AAAI conference on artificial intelligence_, 2006.
* Xu et al. (2023) Xu, Z., Liang, Y., Yu, C., Wang, Y., and Wu, Y. Fictitious cross-play: Learning global nash equilibrium in mixed cooperative-competitive games. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_, pp. 1053-1061, 2023.
* Yao et al. (2023) Yao, J., Liu, W., Fu, H., Yang, Y., McAleer, S., Fu, Q., and Yang, W. Policy space diversity for non-transitive games. _Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* Yu et al. (2021) Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of ppo in cooperative, multi-agent games. _arXiv preprint arXiv:2103.01955_, 2021.
* Zhang & Sandholm (2021) Zhang, B. H. and Sandholm, T. Team correlated equilibria in zero-sum extensive-form games via tree decompositions. _arXiv preprint arXiv:2109.05284_, 2021.
* Zhang et al. (2022) Zhang, B. H., Farina, G., and Sandholm, T. Team belief dag form: A concise representation for team-correlated game-theoretic decision making. _arXiv preprint arXiv:2202.00789_, 2022.
* Zhang et al. (2023) Zhang, B. H., Farina, G., Anagnostides, I., Cacciamani, F., McAleer, S. M., Haupt, A. A., Celli, A., Gatti, N., Conitzer, V., and Sandholm, T. Computing optimal equilibria and mechanisms via learning in zero-sum extensive-form games. _Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* Zhang & An (2020) Zhang, Y. and An, B. Computing team-maxmin equilibria in zero-sum multiplayer extensive-form games. In _AAAI Conference on Artificial Intelligence (AAAI)_, pp. 2318-2325, 2020a.
* Zhang & An (2020) Zhang, Y. and An, B. Converging to team-maxmin equilibria in zero-sum multiplayer games. In _International Conference on Machine Learning (ICML)_, 2020b.
* Zhang et al. (2020) Zhang, Y., An, B., and Cerny, J. Computing ex ante coordinated team-maxmin equilibria in zero-sum multiplayer extensive-form games. In _ArXiv preprint:2009.12629_, 2020.

## Appendix A Proof of Proposition 1

Proof.: Note that by Equation 3 and the stopping condition in Algorithm 1, when Team DO(-MM) terminates, the meta-NE \((\mu_{\Gamma}^{r},\mu_{\bigcirc}^{r})\) is a TMECor. What is left to show is that Team DO(-MM) eventually terminates. Team DO(-MM) eventually terminates because in every iteration it either adds a new strategy for at least one player or it terminates. To see this, consider the case where no new strategies are added for either player in an iteration. Then the best response value for each player is no greater than the value of their meta-NE against the opponent, so Equation 3 is satisfied and Team DO(-MM) terminates in a TMECor. 

## Appendix B Tabular Best-Response Oracle

In this section we give the formulation of the team-best-response oracle we implemented in the tabular case.

As discussed in the body (Section 4.1), the algorithm is based on integer programming and implements the algorithm introduced by Celli & Gatti (2018). The integer programming formulation uses the 

[MISSING_PAGE_FAIL:14]

enhance the training process. The input layer of the network is structured as \([133\times 128]\) to ensure effective feature representation and processing. Furthermore, the player's action space is defined using a default set of 19 discrete actions, encompassing fundamental moves such as movement, passing, and shooting. Consequently, the output layer of the network is configured as \([128\times 19]\), allowing for the mapping of features to appropriate action selections.

**Reward Shaping.** Due to the sparsity of rewards in the GRF environment, it is crucial to design appropriate reward mechanisms. In addition to the rewards provided by the environment, we have designed specific reward components, including win reward, goal reward, yellow card reward, checkpoint reward and lost ball penalty, to expedite our training process and enhance its effectiveness. We adopt a consistent reward configuration across all our experiments.

**Training.** For each algorithm, our models were trained for 1500M steps on a cluster with 256 CPUs and 8 RTX3090 GPUs, using three different random seeds. The batch size for each GPU was set to 512 and The discount factor \(\gamma\) was set to 0.9999. We employed the Adam optimizer with a learning rate of 0.00005.Specifically, for team PSRO-MM, to mitigate the increased training cost associated with incorporating excessive mix strategies, we implemented a strategy of introducing two mix strategies to the population in each round (We set two here, however, you can add more if you wish). Each mix strategy was constructed by randomly selecting pure strategies from past checkpoints and assigning them to individual players within the team. This approach allows for a controlled inclusion of mix strategies while managing the overall training overhead, enabling an efficient training process while benefiting from the incorporation of diverse strategies within the population. More hyper-parameters are summarized as bellow:

**Evaluation Metrics.** Due to the complexity of the GRF, calculating the exploitability of a specific policy or Predictive Error (PE) for a certain population becomes challenging. Both metrics involve max or min operators, and approximating the best response can be significantly inaccurate in such a complex game. We use the relative population performance (RPP)Vinyals et al. (2019) as the metric. RPP represents the game interaction results of two populations A and B under their meta-nash equilibrium. Besides, considering our objective of identifying robust policies with strong performance in real-world games, we assess the effectiveness by comparing the average goal differences, score and Elo between the policies obtained from different methods and the built-in bots with varying difficulty levels within the General Reinforcement Learning Framework (GRF). This evaluation allows us to gauge the capabilities of the policies in challenging game scenarios and their ability to outperform predefined bot opponents.

**Elo Calculation** Elo refers to the Elo rating system used to measure the relative skill levels of the algorithms. For the computation of Elo ratings, they was computed by making agents play against each other. Here is a detailed explanation of how they were computed:

* Assign an initial Elo rating (1000 in our setting) to each player.
* Determine the expected score of each player in a game. This is calculated as follows:
* For player A: expected score = \(\frac{1}{1+10^{-\frac{(B-\lambda)}{400}}}\)

\begin{table}
\begin{tabular}{c c} \hline \hline Hyper-Parameters & Value \\ \hline game length & 3000 \\ batch size & 512 \\ max clipped value loss & 0.2 \\ gradient clip norm & 10 \\ value loss & huber loss \\ discount factor \(\gamma\) & 0.9999 \\ learning rate & 5e-4 \\ ppo update number & 5 \\ gain & 0.01 \\ GAE \(\lambda\) & 0.95 \\ entropy coefficient & 0.001 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hyper-parameters* For player B: expected score = \(\frac{1}{1+10^{\frac{(A-B)}{400}}}\) where A and B are the current Elo ratings of the two players.
* Play the game and determine the actual score of each player.
* Update the Elo ratings of the two players based on the outcome of the game and the expected scores:
* For player A: new rating = old rating + \(K\times\) (actual score - expected score)
* For player B: new rating = old rating + \(K\times\) (expected score - actual score) where \(K\) is a constant that determines the "weight" of the update (set as 10 in this case).
* Repeat the process for each game, using the updated Elo ratings from the previous game as the starting point for the next game.

**Additional Experiments.** Since each player can observe the entire game information in GRF, we conducted experiments involving feature engineering to create an imperfect information scenario in the main body. In this scenario, each player is only able to observe information related to teammates, the ball, and the three nearest opposing players, which means there is always a lack of information regarding two opposing players. However, we also conducted experiments with perfect information, as depicted in the figures. Remarkably, we observed that the same pattern held true in both imperfect and perfect information scenarios, with Team PSRO-MM outperforming Team PSRO, which in turn outperformed self-play.

Additionally, we aimed to compare the performance of both Team PSRO and Team PSRO-MM as the training time steps increased, using the policy trained by self-play as the benchmark. Since draws can occur in football, win rate may not accurately reflect the policies' performance. Therefore, we used average scores as the evaluation metric, where the score for a single game is 1 for a win, 0.5 for a draw, and 0 for a loss. To better reflect the relative strength of each policy against the other, we multiplied the average scores by 100.

The results of the experiments are presented in Figure 4, with the dashed line representing the performance of the policy trained through self-play. We can see that both Team PSRO and Team PSRO-MM ultimately outperform self-play. Additionally, Team PSRO and Team PSRO-MM exhibit comparable performance in the initial stages of training. However, as the training timesteps progress, Team PSRO-MM exhibits superior performance, surpassing Team PSRO. This signifies that the integration of mixed strategies enhance the training effectiveness and robustness of the models. With an extended duration of training, Team PSRO-MM demonstrates a heightened ability to adapt to complex game environments, yielding higher-quality strategies.

Finally, we include additional comparisons with fictitious play (FP) and prioritized fictitous self play (PFSP). These methods learn a team best response to either the opponent average strategy (in FP) or a mixture of the average and the most recent (PFSP). We show in figure 5 that Team PSRO and Team PSRO-MM outperform these additional baselines as well.

Figure 3: Team PSRO Results on Google Research Football perfect information scenario. Both Team PSRO and Team PSRO-MM outperform self play, with Team PSRO-MM performing the best.

Figure 4: Team PSRO results on both perfect and imperfect information scenarios of Google Research Football. Both Team PSRO and Team PSRO-MM outperform self-play, with Team PSRO-MM performing the best.

Figure 5: We include additional baselines of fictitious play and prioritized fictitious self play and show that Team PSRO and Team PSRO-MM are able to outperform all existing baselines.