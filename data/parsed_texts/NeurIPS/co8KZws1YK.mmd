# Light Unbalanced Optimal Transport

Milena Gazdieva

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

milena.gazdieva@skoltech.ru &Arip Asadulaev

ITMO University

Artificial Intelligence Research Institute

Moscow, Russia

asadulaev@airi.net &Evgeny Burnaev

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

e.burnaev@skoltech.ru &Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

a.korotin@skoltech.ru

###### Abstract

While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers that deal with the _unbalanced_ EOT (UEOT) problem \(-\) the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver. Our advancement consists of developing a novel view on the optimization of the UEOT problem yielding tractable and a non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to a fast, simple, and effective solver which allows solving the continuous UEOT problem in minutes on CPU. We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds. We give illustrative examples of the solver's performance. The code is publicly available at

https://github.com/milenagazdieva/LightUnbalancedOptimalTransport

## 1 Introduction

The computational _optimal transport_ (OT) has proven to be a powerful tool for solving various popular tasks, e.g., image-to-image translation [68, 17, 50, 28], image generation [67, 13, 7] and biological data transfer [5, 42, 66]. Historically, the majority of early works in the field were built upon solving the OT problem between discrete probability measures [10, 53]. Only recently the advances in the field of generative models have led to explosive interest from the ML community in developing the **continuous** OT solvers, see [38] for a survey. The setup of this problem assumes that the learner needs to estimate the OT plan between continuous measures given only empirical samples of data from them. Due to convexity-related issues of OT problem [40], many works consider the EOT problem, i.e., use _entropy_ regularizers which guarantee, e.g., the uniqueness of learned plans.

Meanwhile, researches attract attention to other shortcomings of the classic OT problem. It enforces hard constraints on the marginal measures and, thus, does not allow for mass variations. As a result, OT shows high sensitivity to an imbalance of classes and outliers in the source and targetmeasures [4] which are almost inevitable for large-scale datasets. To overcome these issues, it is common to consider extensions of the OT problem, e.g., unbalanced OT/EOT (UOT/UEOT) [8; 43]. The unbalanced OT/EOT formulations allow for variation of total mass by relaxing the marginal constraints through the use of divergences.

The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example, [45] uses a regression to interpolate the discrete solutions, and [16; 36] build a flow matching upon them. Almost all of the other solvers [9; 70] employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver that can play the role of a simple baseline for unbalanced EOT.

**Contributions.** We develop a novel _lightweight_ solver to estimate continuous **unbalanced** EOT couplings between probability measures (SS4). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the UEOT plans. We provide the generalization bounds for our solver (SS4.4) and experimentally test in several tasks (SS5.1, SS5.2).

**Notations.** We work in the Euclidian space \((\mathbb{R}^{d},\|\cdot\|)\). We use \(\mathcal{P}_{2,ac}(\mathbb{R}^{d})\) to denote the set of absolutely continuous (a.c.) Borel probability measures on \(\mathbb{R}^{d}\) with finite second moment and differential entropy. The set of nonnegative measures on \(\mathbb{R}^{d}\) with finite second moment is denoted as \(\mathcal{M}_{2,+}(\mathbb{R}^{d})\). We use \(\mathcal{C}_{2}(\mathbb{R}^{d})\) to denote the space of all _continuous_ functions \(\zeta:\mathbb{R}^{d}\to\mathbb{R}\) for which \(\exists a=a(\zeta),b=b(\zeta)\) such that \(\forall x\in\mathbb{R}^{d}:|\zeta(x)|\leq a+b\|x\|^{2}\). Its subspace of functions which are additionally _bounded from above_ is denoted as \(\mathcal{C}_{2,b}(\mathbb{R}^{d})\). For a.c. measure \(p\in\mathcal{P}_{2,ac}(\mathbb{R}^{d})\) (or \(\mathcal{M}_{2,+}(\mathbb{R}^{d})\)), we use \(p(x)\) to denote its density at a point \(x\in\mathbb{R}^{d}\). For a given a.c. measure \(\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\), we denote its total mass by \(\|\gamma\|_{1}\operatorname*{\,=\,}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d} }\gamma(x,y)dxdy\). We use \(\gamma_{x}(x)\), \(\gamma_{y}(y)\) to denote the marginals of \(\gamma(x,y)\). They satisfy the equality \(\|\gamma_{x}\|_{1}\!=\!\|\gamma_{y}\|_{1}\!=\!\|\gamma\|_{1}\). We write \(\gamma(\cdot|x)\) to denote the conditional _probability_ measure. Each such measure has a unit total mass. We use \(\overline{f}\) to denote the Fenchel conjugate of a function \(f\): \(\overline{f}(t)\operatorname*{\,=\,}\sup_{u\in\mathbb{R}}\{ut-f(u)\}\). We use \(\mathbb{I}_{A}\) to denote the convex indicator of a set \(A\), i.e., \(\mathbb{I}_{A}(x)=0\) if \(x\in A\); \(\mathbb{I}_{A}(x)=+\infty\) if \(x\notin A\).

## 2 Background

Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to [10; 24; 53], unbalanced EOT - to [8; 43].

\(f\)**-divergences for positive measures**. For _positive_ measures \(\mu_{1},\mu_{2}\in\mathcal{M}_{2,+}(\mathbb{R}^{d^{\prime}})\) and a lower semi-continuous function \(f:\mathbb{R}\to\mathbb{R}\cup\{\infty\}\), the \(f\)_-divergence_ between \(\mu_{1},\mu_{2}\) is defined by

\[D_{f}\left(\mu_{1}\|\mu_{2}\right)\operatorname*{\,=\,}\int_{\mathbb{R}^{d^{ \prime}}}f\!\left(\frac{\mu_{1}(x)}{\mu_{2}(x)}\right)\!\mu_{2}(x)dx\text{ if }\mu_{1}\ll\mu_{2}\text{ and }+\infty\text{ otherwise}.\]

We consider \(f(t)\) which are convex, non-negative and attain zero uniquely when \(t=1\). In this case, \(D_{f}\) is a valid measure of dissimilarity between two positive measures (see Appendix C for details). This means that \(D_{f}(\mu_{1}\|\mu_{2})\!\geq\!0\) and \(D_{f}(\mu_{1}\|\mu_{2})\!=\!0\) if and only if \(\mu_{1}=\mu_{2}\). In our paper, we also assume that all \(f\) that we consider satisfy the property that \(\overline{f}\) is a _non-decreasing_ function.

Kullback-Leibler divergence \(\operatorname{D_{KL}}\)[8; 62], is a particular case of such \(f\)-divergence for positive measures. It has a generator function \(f_{\text{KL}}(t)\operatorname*{\,=\,}\!t\log t\!-\!t\!+\!1\). Its convex conjugate \(\overline{f_{\text{KL}}}(t)\!=\!\exp(t)-1\). Another example is the \(\chi^{2}\)-divergence \(\operatorname{D_{\chi^{2}}}\) which is generated by \(f_{\chi^{2}}(t)\operatorname*{\,=\,}\!\!\operatorname*{\,=\,}(t-1)^{2}\) when \(t\geq 0\) and \(\infty\) otherwise. The convex conjugate of this function is \(\overline{f_{\chi^{2}}}(t)=-1\) if \(t\!<\!-2\) and \(\frac{1}{4}t^{2}\!+\!t\) when \(t\!\geq\!2\).

_Remark_.: By the definition, convex conjugates of \(f_{\text{KL}}\) and \(f_{\chi^{2}}\) divergences are proper, non-negative and non-decreasing. These properties are used in some of our theoretical results.

**Entropy for positive measures.** For \(\mu\in\mathcal{M}_{2,+}(\mathbb{R}^{d^{\prime}})\), its entropy [8] is given by

\[H(\mu)\operatorname*{\,=\,}\!-\int_{\mathbb{R}^{d^{\prime}}}\mu(x)\!\log\mu(x) dx+\|\mu\|_{1}\text{, if }\mu\text{ is a.c. and }-\infty\text{ otherwise}.\] (1)

When \(\mu\in\mathcal{P}_{2,ac}(\mathbb{R}^{d^{\prime}})\), i.e., \(\|\mu\|_{1}\!=\!1\), equation (1) is the usual differential entropy minus 1.

**Classic EOT formulation (with the quadratic cost).** Consider two probability measures \(p\in\mathcal{P}_{2,ac}(\mathbb{R}^{d})\), \(q\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^{d})\). For \(\varepsilon>0\), the EOT problem between \(p\) and \(q\) is

\[\min_{\pi\in\Pi(p,q)}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\frac{\|x-y\|^{2 }}{2}\pi(x,y)dxdy-\varepsilon H(\pi),\] (2)

where \(\Pi(p,q)\) is the set of probability measures \(\pi\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^{d}\!\times\!\mathbb{R}^{d})\) with marginals \(p\), \(q\) (transport plans). Plan \(\pi^{*}\) attaining the minimum exists, it is unique and called the _EOT plan_.

Classic EOT imposes hard constraints on the marginals which leads to several issues, e.g., sensitivity to outliers [4], inability to handle potential measure shifts such as class imbalances in the measures \(p,q\). The UEOT problem [70, 9] overcomes these issues by relaxing the marginal constraints [62].

**Unbalanced EOT formulation (with the quadratic cost).** Let \(D_{f_{1}}\) and \(D_{f_{2}}\) be two \(f\)-divergences over \(\mathbb{R}^{d}\). For two probability measures \(p\in\mathcal{P}_{2,ac}(\mathbb{R}^{d})\), \(q\in\mathcal{P}_{2,ac}(\mathbb{R}^{d})\) and \(\varepsilon>0\), the unbalanced EOT problem between \(p\) and \(q\) consists of finding a minimizer of

\[\inf_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})}\int_{ \mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\frac{\|x-y\|^{2}}{2}\gamma(x,y)dxdy- \varepsilon H(\gamma)+D_{f_{1}}\left(\gamma_{x}\|p\right)+D_{f_{2}}\left( \gamma_{y}\|q\right),\] (3)

see Fig. 1. Here the minimum is attained for a unique \(\gamma^{*}\) which is called the _unbalanced optimal entropic_ (UEOT) plan. Typical choices of \(f_{i}\) (\(i\in[1,2]\)) are \(f_{i}(t)=\tau_{i}f_{\text{KL}}(t)\) or \(f_{i}(t)=\tau_{i}f_{\chi^{2}}(t)\) (\(\tau_{i}>0\)) yielding the scaled \(\text{D}_{\text{KL}}\) and \(\text{D}_{\chi^{2}}\), respectively. In this case, the bigger \(\tau_{1}\) (\(\tau_{2}\)) is, the more \(\gamma_{x}\) (\(\gamma_{y}\)) is penalized for not matching the corresponding marginal distribution \(p\) (\(q\)).

_Remark 1_.: While the set \(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\) contains not only a.c. measures, infimum in problem (3) is attained for a.c. measure \(\gamma^{*}\) since \(-\varepsilon H(\gamma^{*})\) term turns to \(+\infty\) otherwise. Thus, almost everywhere in the paper we are actually interested in the subset of a.c. measures in \(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\).

_Remark 2_.: The balanced EOT problem (2) is a special case of (3). Indeed, let \(f_{1}\) and \(f_{2}\) be the convex indicators of \(\{1\}\), i.e., \(f_{1}=f_{2}=\mathbb{I}_{x=1}\). Then the \(f\)-divergences \(D_{f_{1}}\left(\gamma_{x}\|p\right)\) and \(D_{f_{2}}\left(\gamma_{y}\|q\right)\) become infinity if \(p\neq\gamma_{x}\) or \(q\neq\gamma_{y}\), and become zeros otherwise.

**Dual form of unbalanced EOT problem (3)** is formulated as follows

\[\sup_{(\phi,\psi)}\!\!\left\{\!-\!\varepsilon\!\!\int_{\mathbb{R}^{d}}\!\!\! \int_{\mathbb{R}^{d}}\!\!\!\exp\{\frac{1}{\varepsilon}(\phi(x)\!+\!\psi(y)\!- \!\frac{\|x-y\|^{2}}{2})\}dxdy-\!\!\int_{\mathbb{R}^{d}}\!\!\!\overline{f}_{1 }(-\phi(x))p(x)dx-\!\!\int_{\mathbb{R}^{d}}\!\!\overline{f}_{2}(-\psi(y))q(y) dy\right\}\!.\] (4)

It is known that there exist two measurable functions \(\phi^{*}\), \(\psi^{*}\) delivering maximum to (4). They have the following connection with the solution of the primal unbalanced problem (3):

\[\gamma^{*}(x,y)\!=\!\exp\{\frac{\phi^{*}(x)}{\varepsilon}\}\!\exp\{-\frac{\|x- y\|^{2}}{2\varepsilon}\}\!\exp\{\frac{\psi^{*}(y)}{\varepsilon}\}.\] (5)

_Remark_.: For some of our results, we will need to restrict potentials \((\phi,\psi)\) in problem (4) to the space \(\mathcal{C}_{2,b}(\mathbb{R}^{d})\times\mathcal{C}_{2,b}(\mathbb{R}^{d})\). Since established variants of dual forms [8] typically correspond to other functional spaces, we derive and theoretically justify a variant of the _dual problem_ (4) in Appendix A.3. Note that it may hold that optimal potentials \(\psi^{*},\phi^{*}\notin\mathcal{C}_{2,b}(\mathbb{R}^{D})\), i.e., the supremum is not achieved within our considered spaces. This is not principle for our subsequent derivations.

**Computational UEOT setup.** Analytical solution for the _unbalanced_ EOT problem is, in general, not known.1 Moreover, in real-world setups where unbalanced EOT is applicable, the measures \(p\), \(q\) are typically not available explicitly but only through their empirical samples (datasets).

Footnote 1: Analytical solutions are known only for some specific cases. For example, [33] consider Gaussian measures and UEOT problem with \(\text{D}_{\text{KL}}\) divergence instead of the differential entropy. This setup differs from ours.

We assume that data measures \(p,q\in\mathcal{P}_{2,ac}(\mathbb{R}^{d})\) are unknown and accessible only by a limited number of i.i.d. empirical samples \(\{x_{0},...,x_{N}\}\!\sim\!p\), \(\{y_{0},...,y_{M}\}\!\sim\!q\). We aim to approximate the optimal UEOT plan \(\gamma^{*}\) solving (3) between the entire measures \(p,q\). The recovered plans should allow the out-of-sample estimation, i.e., generation of samples from \(\gamma^{*}(\cdot|x^{\text{new}})\) where \(x^{\text{new}}\) is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from \(\gamma^{*}_{x}\).

Figure 1: Unbalanced EOT problem.

The described setup is typically called the _continuous EOT_ and should not be twisted up with the _discrete EOT_[53; 10]. There the aim is to recover the (unbalanced) EOT plan between the empirical measures \(\hat{p}=\!\frac{1}{N}\sum_{i=1}^{N}\delta_{x_{i}}\), \(\hat{q}\!=\!\frac{1}{M}\sum_{j=1}^{M}\delta_{y_{j}}\) and out-of-sample estimations are typically not needed.

## 3 Related Work

Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function [27; 25; 2]. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans). We refer to [39] for a detailed review.

At the same time, there exist a large amount of works within the discrete OT/EOT setup [10; 15; 69; 51], see [53] for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide a generalization to the new unseen (test) data. Only recently ML community started developing out-of-sample estimation procedures based on discrete/batched OT. For example, [19; 56; 32; 14; 48; 57] mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear.

**Balanced OT/EOT solvers.** There exists a vast amount of neural solvers for continuous OT problem. Most of them learn the OT maps (or plans) via solving saddle point optimization problems [3; 18; 37; 22; 60; 50]. Though the recent works [28; 61; 11; 41; 30] tackle the EOT problem (2), they consider its balanced version. Hence they are not relevant to us. Among these works, only [41; 30] evade non-trivial training/inference procedures and are ideologically the closest to ours. The difference between them consists of the particular loss function used. In fact, **our paper** proposes the solver which subsumes these solvers and generalizes them for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see SS4.

**Unbalanced OT/EOT solvers.** A vast majority of early works in this field tackle the discrete UOT/UEGT setup [6; 20; 54] but the principles behind their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers that tackle the continuous unbalanced OT/EOT setup employ discrete solutions in the construction of their solvers. For example, [45] regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method [47]. In [16] and [36], the authors implement Flow Matching [44, FM] and conditional FM on top of the discrete UEOT plans, respectively. The algorithm of the latter consists of regressing neural networks on top of scaling factors and simultaneously learning a conditional vector field to transport the mass between re-balanced measures. Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified.

The recent papers [70; 9] are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using _complex_ GAN-style procedures. Thus, these GANs often lean on heavy neural parametrization, may incur instabilities during training, and require careful hyperparameter selection [46].

For completeness, we also mention other papers which are only slightly relevant to us. For example, [23] considers incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works [12; 4] incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise.

In contrast to the listed works, our paper proposes a _theoretically justified and lightweight_ solver to the UEOT problem, see Table 1 for the detailed comparison of solvers.

\begin{table}
\begin{tabular}{c|c|c|c} \hline
**Solver** & **Problem** & **Phileries** & **What recovers** \\ \hline
[70] & \multirow{2}{*}{UOT} & Solve \(\epsilon\)-transform based semi-dual & Scaling factor \(\gamma^{*}\)/\(\hat{p}\)/\(\hat{q}\)/\(\hat{r}\) and & Complex max-min objective; \\  & & max-min reformulation of UOT using neural nets & stochastic OT map \(T^{*}\) (\(z,z\)) & 3 neural networks \\ \hline
[45] & \multirow{2}{*}{
\begin{tabular}{c} Custom \\ UOT \\ \end{tabular} } & Regression on top of discrete EIT & Scaling factors and & Heuristically uses \\  & & between re-balanced measures & OT maps between re-scaled measures & minibatch OT pre-expantances \\ \hline
[9] & \multirow{2}{*}{\begin{tabular}{c} UOT \\ \end{tabular} } & Salves semi-dual max-min & \multirow{2}{*}{\begin{tabular}{c} Stochastic UOT map \(T^{*}\) (\(x,z\)) \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} Complex max-min objective; \\ 2 neural networks \\ \end{tabular} } \\  & & Flow Matching up of discrete UOT & & Particularly uses \\  & using neural nets & \((v_{r},\theta)\!\in\![0,1]\) to knapper the mass & minibatch OT pre-expantances \\ \hline
[36] & \multirow{2}{*}{\begin{tabular}{c} UEOT \\ \end{tabular} } & Conditional Flow Matching on top of discrete EIT & Scaling factors and parameter conditional & Heuristically uses \\  & & between re-balanced measures using neural nets & vector feed \((v_{r},\theta)\!\in\![0,1]\) to knapper & Heuristically uses \\  & & the mass between re-scaled measures & minibatch OT pre-expantances \\ \hline U-LightOT & \multirow{2}{*}{
\begin{tabular}{c} SUFOOT} & Solve non-minstraent reformulation & Density of UOT \(\hat{p}\hat{q}^{*}\) (together with light \\  & of dual UEOT using Gaussian Mixtures & procedure to sample \(z\sim\gamma_{z}^{*}\)) and \(\gamma\sim\gamma_{y}^{*}\!\left(\lfloor z\right)\) & Heuristically uses \\ \hline \end{tabular} } \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of the principles of existing UOT/UEGT solvers and **our** proposed light solver.

[MISSING_PAGE_FAIL:5]

### Parametrization and the Optimization Procedure

**Parametrization.** Recall that \(u_{\omega}\) parametrizes the density of the marginal \(\gamma_{x}^{*}\) which is unnormalized. Setting \(x=0\) in equation (7), we get \(\gamma^{*}(y|x=0)\propto v^{*}(y)\) which means that \(v^{*}\) also corresponds to an unnormalized density of a measure. These motivate us to use the unnormalized Gaussian mixture parametrization for the potential \(v_{\theta}(y)\) and measure \(u_{\omega}(x)\):

\[v_{\theta}(y)\!=\!\sum_{k=1}^{K}\!\!\alpha_{k}\mathcal{N}(y|r_{k},\!\varepsilon S _{k});\ \ u_{\omega}(x)\!\!=\!\!\!\sum_{l=1}^{L}\!\beta_{l}\mathcal{N}(x|\mu_{l},\! \varepsilon\Sigma_{l}).\] (11)

Here \(\theta\stackrel{{\mathrm{def}}}{{=}}\{\alpha_{k},r_{k},S_{k}\}_ {k=1}^{K}\), \(w\stackrel{{\mathrm{def}}}{{=}}\{\beta_{l},\mu_{l},\Sigma_{l}\}_ {l=1}^{L}\) with \(\alpha_{k},\beta_{l}\geq 0\), \(r_{k},\mu_{l}\in\mathbb{R}^{d}\) and \(0\prec S_{k},\Sigma_{l}\in\mathbb{R}^{d\times d}\). The covariance matrices are scaled by \(\varepsilon\) just for convenience.

For this type of parametrization, it holds that \(\|u_{\omega}\|_{1}=\sum_{l=1}^{L}\beta_{l}\). Moreover, there exist closed-from expressions for the normalization constant \(c_{\theta}(x)\) and conditional plan \(\gamma_{\theta}(y|x)\), see [41, Proposition 3.2]. Specifically, define \(r_{k}(x)\stackrel{{\mathrm{def}}}{{=}}r_{k}+S_{k}x\) and \(\widetilde{\alpha}_{k}(x)\stackrel{{\mathrm{def}}}{{=}}\alpha_{ k}\exp\{\frac{x^{T}S_{k}x+2r_{k}^{T}x}{2\varepsilon}\}\). It holds that

\[c_{\theta}(x)=\sum_{k=1}^{K}\widetilde{\alpha}_{k}(x);\ \ \gamma_{\theta}(y|x)=\frac{1}{c_{ \theta}(x)}\sum_{k=1}^{K}\widetilde{\alpha}_{k}(x)\mathcal{N}(y|r_{k}(x), \varepsilon S_{k}).\] (12)

Using this result and (11), we get the expression for \(\gamma_{\theta,w}\):

\[\gamma_{\theta,w}(x,y)=u_{\omega}(x)\cdot\gamma_{\theta}(y|x)=\underbrace{ \sum_{l=1}^{L}\beta_{l}\mathcal{N}(x|\mu_{l},\!\varepsilon\Sigma_{l})}_{u_{ \omega}(x)}\cdot\underbrace{\frac{\sum_{k=1}^{K}\widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_{k}(x),\varepsilon S_{k})}{\sum_{k=1}^{K}\widetilde{\alpha}_{ k}(x)}}_{\gamma_{\theta}(y|x)}.\] (13)

**Training.** We recall that the measures \(p,q\) are accessible only by a number of empirical samples (see the learning setup in SS2). Thus, given samples \(\{x_{1},...,x_{N}\}\) and \(\{y_{1},...,y_{M}\}\), we optimize the empirical analog of (10):

\[\widehat{\mathcal{L}}(\theta,w)\!\stackrel{{\mathrm{def}}}{{=} }\!\frac{1}{N}\sum_{i=1}^{N}\overline{f}_{1}(-\varepsilon\log\frac{u_{\omega} (x_{i})}{c_{\theta}(x_{i})}-\frac{\|x_{i}\|^{2}}{2})\!+\!\frac{1}{M}\sum_{j=1}^ {M}\overline{f}_{2}(-\varepsilon\log v_{\theta}(y_{j})\!-\!\frac{\|y_{j}\|^{2 }}{2})\!\!+\!\varepsilon\|u_{\omega}\|_{1}\] (14)

using minibatch gradient descent procedure w.r.t. parameters \((\theta,w)\). In the parametrization of \(v_{\theta}\) and \(u_{\omega}\) (11), we utilize the diagonal matrices \(S_{k}\), \(\Sigma_{l}\). This allows decreasing the number of learnable parameters in \(\theta\) and speeding up the computation of inverse matrices \(S_{k}^{-1},\,\Sigma_{l}^{-1}\).

**Inference.** Sampling from the conditional and marginal measures \(\gamma_{\theta}(y|x)\!\approx\!\gamma^{*}(y|x)\), \(u_{\omega}\!\approx\!\gamma_{x}^{*}\) is easy and lightweight since they are explicitly parametrized as Gaussian mixtures, see (12), (11).

### Connection to Related Prior Works

The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks first appeared in the EOT/SB benchmark [29]. There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In [41], the authors utilized this type of parametrization to obtain a light solver **(LightSB)** for the **balanced** EOT.

Our solver for **unbalanced** EOT (10) subsumes their solver for balanced EOT as well as one problem subsumes the other for the special case of divergences, see the remark in SS2. Let \(f_{1}\), \(f_{2}\) be convex indicators of \(\{1\}\). Then \(\overline{f_{1}}(t)=t\) and \(\overline{f_{2}}(t)=t\) and objective (10) becomes

\[\mathcal{L}(\theta,w)\!=\!\int_{\mathbb{R}^{d}}\!(-\varepsilon\log \frac{u_{\omega}(x)}{c_{\theta}(x)}\!-\!\frac{\|x\|^{2}}{2})p(x)dx+\!\int_{ \mathbb{R}^{d}}\!(-\varepsilon\log v_{\theta}(y)\!-\!\frac{\|y\|^{2}}{2})q(y) dy\!+\!\varepsilon\|u_{\omega}\|_{1}=\] \[-\varepsilon\Big{(}\!\int_{\mathbb{R}^{d}}\!\log\frac{u_{\omega}(x )}{c_{\theta}(x)}p(x)dx\!+\!\!\!\int_{\mathbb{R}^{d}}\!\log v_{\theta}(y)q(y) dy\!-\!\|u_{\omega}\|_{1}\Big{)}-\underbrace{\int_{\mathbb{R}^{d}}\!\frac{\|x\|^{2}}{2}p(x) dx\!-\!\!\int_{\mathbb{R}^{d}}\!\frac{\|y\|^{2}}{2}q(y)dy}_{\mathbb{R}\!\text{Const}(p,q)}\!=\] \[\varepsilon\Big{(}\!\int_{\mathbb{R}^{d}}\log c_{\theta}(x)p(x)dx \!\Big{)}\!-\!\!\int_{\mathbb{R}^{d}}\log v_{\theta}(y)q(y)dy\big{)}\,-\] (15) \[\varepsilon\int_{\mathbb{R}^{d}}\log u_{\omega}(x)p(x)dx+\varepsilon \|u_{\omega}\|_{1}-\text{Const}(p,q).\] (16)Here line (15) depends exclusively on \(\theta\) and exactly coincides with the LightSB's objective, see [41, Proposition 8]. At the same time, line (16) depends only on \(\omega\), and its minimum is attained when \(u_{\omega}=p\). Thus, this part is not actually needed in the balanced case, see [41, Appendix C].

### Generalization Bounds and Universal Approximation Property

Theoretically, to recover the UEOT plan \(\gamma^{*}\), one needs to solve the problem \(\mathcal{L}(\theta,\omega)\to\min_{\theta,\omega}\) as stated in our Theorem 4.1. In practice, the measures \(p\) and \(q\) are accessible via empirical samples \(X\stackrel{{\text{def}}}{{=}}\{x_{1},...,x_{N}\}\) and \(Y\stackrel{{\text{def}}}{{=}}\{y_{1},...,y_{M}\}\), thus, one needs to optimize the empirical counterpart \(\widehat{\mathcal{L}}(\theta,\omega)\) of \(\mathcal{L}(\theta,\omega)\), see (14). Besides, the available potentials \(u_{\omega}\), \(v_{\theta}\) over which one optimizes the objective come from the restricted class of functions. Specifically, we consider unnormalized Gaussian mixtures \(u_{\omega}\), \(v_{\theta}\) with \(K\) and \(L\) components respectively. Thus, one may naturally wonder: **how close is the recovered plan \(\gamma_{\widehat{\theta},\widehat{\omega}}\) to the UEOT plan \(\gamma^{*}\)** given that \((\widehat{\theta},\widehat{\omega})=\arg\min_{\theta,\omega}\widehat{\mathcal{ L}}(\theta,\omega)\)?

To address this question, we study the _generalization error_\(\mathbb{E}\text{D}_{\text{KL}}\left(\gamma^{*}\|\gamma_{\widehat{\theta},\widehat{\omega}}\right)\), i.e., the expectation of \(\text{D}_{\text{KL}}\) between \(\gamma^{*}\) and \(\gamma_{\widehat{\theta},\widehat{\omega}}\) taken w.r.t. the random realization of the train datasets \(X\sim p\), \(Y\sim q\).

Let \((\overline{\theta},\overline{\omega})=\arg\min_{(\theta,\omega)\in\Theta\times \Omega}\mathcal{L}(\theta,\omega)\) denote the best approximators of \(\mathcal{L}(\theta,\omega)\) in the admissible class. From Theorem 4.1 it follows that that \(\mathbb{E}\text{D}_{\text{KL}}\left(\gamma^{*}\|\gamma_{\widehat{\theta}, \widehat{\omega}}\right)\) can be upper bounded using \(\mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega})-\mathcal{L}^{*})\). The latter can be decomposed into the estimation and approximation errors:

\[\mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega})-\mathcal{L}^{*})\!= \!\mathbb{E}[\mathcal{L}(\widehat{\theta},\widehat{\omega})\!-\!\mathcal{L}( \overline{\theta},\overline{\omega})]\!+\!\mathbb{E}[\mathcal{L}(\overline{ \theta},\overline{\omega})\!-\!\mathcal{L}^{*}]\!=\!\underbrace{\mathbb{E}[ \mathcal{L}(\widehat{\theta},\widehat{\omega})\!-\!\mathcal{L}(\overline{ \theta},\overline{\omega})]}_{\text{Estimation error}}\!+\![\mathcal{L}( \overline{\theta},\overline{\omega})\!-\!\mathcal{L}^{*}]\.\] (17)

We establish the quantitative bound for the estimation error in the proposition below.

**Proposition 4.2** (Bound for the estimation error).: _Let \(p,q\) be compactly supported and assume that \(\overline{f}_{1},\ \overline{f}_{2}\) are Lipshitz. Assume that the considered parametric classes \(\Theta\), \(\Omega\left(\ni(\theta,\omega)\right)\) consist of unnormalized Gaussian mixtures with \(K\) and \(L\) components respectively with bounded means \(\|r_{k}\|,\|\mu_{l}\|\leq R\) (for some \(R>0\)), covariances \(sI\preceq S_{k},\Sigma_{l}\preceq SI\) (for some \(0<s\leq S\)) and weights \(a\leq\alpha_{k},\beta_{l}\leq A\) (for some \(0<a\leq A\)). Then the following holds:_

\[\mathbb{E}\big{[}\mathcal{L}(\widehat{\theta},\widehat{\omega})-\mathcal{L}( \overline{\theta},\overline{\omega})\big{]}\leq O(\frac{1}{\sqrt{N}})+O( \frac{1}{\sqrt{M}}),\]

_where \(O(\cdot)\) hides the constants depending only on \(K,L,R,s,S,a,A,p,q,\varepsilon\) but not on sizes \(M,N\)._

This proposition allows us to conclude that the estimation error converges to zero when \(N\) and \(M\) tend to infinity at the usual parametric rate. It remains to clarify the question: _can we make the approximation error arbitrarily small_? We answer this question positively in our Theorem below.

**Theorem 4.3** (Gaussian mixture parameterization for the variables provides the universal approximation of UEOT plans).: _Let \(p\) and \(q\) be compactly supported and assume that \(\overline{f}_{1},\ \overline{f}_{2}\) are Lipshitz. Then for all \(\delta>0\) there exist Gaussian mixtures \(v_{\theta}\), \(u_{\omega}\) (11) with **scalar** covariances \(S_{k}=\lambda_{k}I_{d}\succ 0\), \(\Sigma_{l}=\zeta_{l}I_{d}\succ 0\) of their components that satisfy \(\text{D}_{\text{KL}}\left(\gamma^{*}\|\gamma_{\theta,\omega}\right)\leq \varepsilon^{-1}(\mathcal{L}(\theta,\omega)-\mathcal{L}^{*})<\delta\)._

**In summary**, results of this section show that one can make the generalization error _arbitrarily small_ given a sufficiently large amount of samples and components in the Gaussian parametrization. It means that theoretically our solver can recover the UEOT plan arbitrarily well.

## 5 Experiments

In this section, we test our U-LightOT solver on several setups from the related works. The code is written using PyTorch framework and is publicly available at

https://github.com/milenagazdieva/LightUnbalancedOptimalTransport.

The experiments are issued in the convenient form of *.ipynb notebooks. Each experiment requires several minutes of training on CPU with 4 cores. Technical _training details_ are given in Appendix B.

### Example with the Mixture of Gaussians

We provide an illustrative _'Gaussians Mixture'_ example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the experimental setup proposed in [16, Figure 2] and define the probability measures \(p\), \(q\) as follows (Fig. 1(a)):

\[p(x) = \frac{1}{4}\mathcal{N}(x|(-3,3),0.1\cdot I_{2})+\frac{3}{4}\mathcal{ N}(x|(1,3),0.1\cdot I_{2}),\] \[q(y) = \frac{3}{4}\mathcal{N}(y|(-3,0),0.1\cdot I_{2})+\frac{1}{4} \mathcal{N}(y|(1,0),0.1\cdot I_{2}).\]

We test our U-LightOT solver with _scaled_\(\mathrm{D_{KL}}\) divergences, i.e., \(f_{1}(t),f_{2}(t)\) are defined by \(\tau\cdot f_{\text{KL}}(t)=\tau(t\log(t)-t+1)\) where \(\tau>0\). We provide the learned plans for \(\tau\in[1,\ 10^{1},\ 10^{2}]\). The results in Fig. 2 show that parameter \(\tau\) can be used to control the level of unbalancedness of the learned plans. For \(\tau=1\), our U-LightOT solver truly learns the UEOT plans, see Fig. 1(d). When \(\tau\) increases, the solver fails to transport the mass from the input Gaussians to the closest target ones. Actually, for \(\tau=10^{2}\), our solutions are similar to the solutions of (41, LightSB) solver which approximates balanced EOT plans between the measures. Hereafter, we treat \(\tau\) as the _unbalancedness_ parameter. In Appendix C, we test the performance of our solver with \(\underline{\text{D}}_{\chi^{2}}\) divergence.

_Remark_.: Here we conduct all experiments with the entropy regularization parameter \(\varepsilon=0.05\). The parameter \(\varepsilon\) is responsible for the stochasticity of the learned transport \(\gamma_{\theta}(\cdot|x)\). Since we are mostly interested in the correct transport of the mass (controlled by \(f_{1},f_{2}\)) rather than the stochasticity, we do not pay much attention to \(\varepsilon\) throughout the paper.

### Unpaired Image-to-Image Translation

Another popular testbed which is usually considered in OT/EOT papers is the unpaired image-to-image translation [71] task. Since our solver uses the parametrization based on Gaussian mixture, it may be hard to apply U-LightOT for learning translation directly in the image space. Fortunately, nowadays it is common to use autoencoders [59] for more efficient translation. We follow the setup of (41, Section 5.4) and use pre-trained ALAE autoencoder [55] for \(1024\times 1024\) FFHQ dataset [34] of human faces. We consider different subsets of FFHQ dataset (_Adult_, _Young_, _Man_, _Woman_) and all variants of translations between them: _Adult\(\leftrightarrow\)Young_ and _Woman\(\leftrightarrow\)Man_.

The main challenge of the described translations is the **imbalance** of classes in the images from source and target subsets, see Table 2. Let us consider in more detail _Adult\(\rightarrow\)Young_ translation. In the FFHQ dataset, the amount of _adult men_ significantly outnumbers the _adult women_, while the amount of _young men_ is smaller than that of _young women_. Thus, balanced OT/EOT solvers are expected to translate some of _adult man_ representatives to _young woman_ ones. At the same time, solvers based on unbalanced transport are supposed to alleviate this issue.

**Baselines.** We perform a comparison with the recent procedure (16, UOT-FM) which considers roughly the same setup and demonstrates good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching [44]. For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OT-FM) [44] to demonstrate the issues of the balanced solvers. We also consider neural network based solvers relying on the adversarial learning such as UOT-SD [9] and UOT-GAN[70].

\begin{table}
\begin{tabular}{c|c c} \hline
**Class** & _Man_ & _Woman_ \\ \hline _Young_ & \(15K\) & \(23K\) \\ \hline _Adult_ & \(7K\) & \(3.5K\) \\ \hline \end{tabular}
\end{table}
Table 2: Number of _train_ FFHQ images for each subset.

**Metrics.** We aim to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., keeping the gender in _Adult\(\rightarrow\)Young_ translation. Thus, we train a \(99\%\) MLP classifier for gender using the latent codes of images. We use it to compute the accuracy of preserving the gender during the translation. We denote this number by _accuracy (keep)_.

Meanwhile, it is also important to ensure that the generated images belong to the distribution of the target images rather than the source ones, e.g., belong to the distribution of _young_ people in our example. To monitor this property, we use another 99% MLP classifier to identify whether each of the generated images belong to the target domain or to the source one. Then we calculate the fraction of the generated images belonging to the target domain which we denote as _accuracy (target)_.

**Results.** Unbalanced OT/EOT approaches are equipped with some kind of unbalancedness parameter (like \(\tau\) in our solver) which influences the methods' results: with the increase of unbalancedness, accuracy of keeping the class increases but the accuracy of mapping to the target decreases. The latter is because of the relaxation of the marginal constraints in UEOT. For a fair comparison, we aim to compute the trade-offs between the keep/target accuracies for different values of the unbalancedness. We do this for our solver and UOT-FM. We found that GAN-based unbalanced approaches show unstable behaviour, so we report UOT-SD and UOT-GAN's results only for one parameter value.

For convenience, we visualize the accuracies' pairs for our solver and its competitors in Fig. 3. The evaluation shows that our U-LightOT method can effectively solve translation tasks in high dimensions (\(d=512\)) and outperforms its alternatives in dealing with class imbalance issues. Namely, we see that our solver allows for achieving the best accuracy of keeping the attributes of the input images than other methods while it provides good accuracy of mapping to the target class. While balanced methods and some of the unbalanced ones provide really high accuracy of mapping to the target, their corresponding accuracies of keeping the attributes are worse than ours meaning that we are better in dealing with class imbalance issues. As expected, for large parameter \(\tau\), the results of our U-LightOT solver coincide with those for LightSB which is a balanced solver. Meanwhile, our solver has the lowest wall-clock running time among the existing unbalanced solvers, see Table 3 for comparison. We demonstrate qualitative results of our solver and baselines in Fig. 4. The choice of unbalancedness parameter for visualization of our method and UOT-FM is detailed in Appendix B.

We present the results of the _quantitative comparison_ in the form of tables in Appendix D.1. In Appendix C, we perform the _ablation study_ of our solver focusing on the selection of parameters \(\tau\), \(\varepsilon\) and number of Gaussian mixtures' components.

## 6 Discussion

**Potential impact.** Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures \(\gamma_{\theta}(y|x)\approx\gamma^{*}(y|x)\) of the UEOT plan.

Figure 3: Visualization of pairs of accuracies (_keep-target_) for our U-LightOT solver and other OT/EOT methods in the image translation experiment. The values of unbalancedness parameters for our U-LightOT solver (\(\tau\)) and [16, UOT-FM] (\(\lambda=reg\_m\)) are specified directly on the plots.

\begin{table}
\begin{tabular}{c|c c c} \hline
**Experiment** & [16, UOT-FM] & [9, UOT-SD] & [70, UOT-GAN] & U-LightOT (**ours**) \\ \hline _Time (sec)_ & 03:21 & 18:11 & 16:30 & **02:38** \\ \hline \end{tabular}
\end{table}
Table 3: Comparison of wall-clock running times of unbalanced OT/EOT solvers in _Young\(\rightarrow\)Adult_ translation. The best results are in **bold**, second best are underlined.

Moreover, it allows for sampling both from the conditional measure \(\gamma_{\theta}(y|x)\) and marginal measure \(u_{\omega}(x)\approx\gamma_{x}^{*}(x)\). Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of use. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lightweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres.

The limitations and broader impact of our solver are discussed in Appendix E.

ACKNOWLEDGEMENTS. The work of Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). We thank Kirill Sokolov, Mikhail Persiianov and Petr Mokrov for providing valuable feedback and suggestions for improving the proofs and clarity of our paper.

Figure 4: Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solvers applied in the latent space of ALAE [55] for FFHQ images [34] (1024\(\times\)1024).

## References

* Agrawal and Horel [2021] R. Agrawal and T. Horel. Optimal bounds between f-divergences and integral probability metrics. _Journal of Machine Learning Research_, 22(128):1-59, 2021.
* Arjovsky et al. [2017] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* Asadulaev et al. [2024] A. Asadulaev, A. Korotin, V. Egiazarian, and E. Burnaev. Neural optimal transport with general cost functionals. In _The Twelfth International Conference on Learning Representations_, 2024.
* Balaji et al. [2020] Y. Balaji, R. Chellappa, and S. Feizi. Robust optimal transport with applications in generative modeling and domain adaptation. _Advances in Neural Information Processing Systems_, 33:12934-12944, 2020.
* Bunne et al. [2023] C. Bunne, S. G. Stark, G. Gut, J. S. Del Castillo, M. Levesque, K.-V. Lehmann, L. Pelkmans, A. Krause, and G. Ratsch. Learning single-cell perturbation responses using neural optimal transport. _Nature Methods_, 20(11):1759-1768, 2023.
* Chapel et al. [2021] L. Chapel, R. Flamary, H. Wu, C. Fevotte, and G. Gasso. Unbalanced optimal transport through non-negative penalized linear regression. _Advances in Neural Information Processing Systems_, 34:23270-23282, 2021.
* Chen et al. [2021] T. Chen, G.-H. Liu, and E. Theodorou. Likelihood training of schrodinger bridge using forward-backward sdes theory. In _International Conference on Learning Representations_, 2021.
* Chizat [2017] L. Chizat. _Unbalanced optimal transport: Models, numerical methods, applications_. PhD thesis, Universite Paris sciences et lettres, 2017.
* Choi et al. [2024] J. Choi, J. Choi, and M. Kang. Generative modeling through the semi-dual formulation of unbalanced optimal transport. In _Advances in Neural Information Processing Systems_, volume 36, 2024.
* Cuturi [2013] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In _Advances in neural information processing systems_, pages 2292-2300, 2013.
* Daniels et al. [2021] M. Daniels, T. Maunu, and P. Hand. Score-based generative neural networks for large-scale optimal transport. _Advances in neural information processing systems_, 34:12955-12965, 2021.
* Dao et al. [2023] Q. Dao, B. Ta, T. Pham, and A. Tran. Robust diffusion gan using semi-unbalanced optimal transport. _arXiv preprint arXiv:2311.17101_, 2023.
* De Bortoli et al. [2021] V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* Deb et al. [2021] N. Deb, P. Ghosal, and B. Sen. Rates of estimation of optimal transport maps using plug-in estimators via barycentric projections. _Advances in Neural Information Processing Systems_, 34:29736-29753, 2021.
* Dvurechenskii et al. [2018] P. Dvurechenskii, D. Dvinskikh, A. Gasnikov, C. Uribe, and A. Nedich. Decentralize and randomize: Faster algorithm for Wasserstein barycenters. In _Advances in Neural Information Processing Systems_, pages 10760-10770, 2018.
* Eyring et al. [2024] L. Eyring, D. Klein, T. Uscidda, G. Palla, N. Kilbertus, Z. Akata, and F. Theis. Unbalancedness in neural monge maps improves unpaired domain translation. In _The Twelfth International Conference on Learning Representations_, 2024.
* Fan et al. [2020] J. Fan, A. Taghvaei, and Y. Chen. Scalable computations of Wasserstein barycenter via input convex neural networks. _arXiv preprint arXiv:2007.04462_, 2020.
* Fan et al. [2023] J. Fan, S. Liu, S. Ma, H.-M. Zhou, and Y. Chen. Neural monge map estimation and its applications. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=2mZSIQscj3. Featured Certification.

* Fatras et al. [2020] K. Fatras, Y. Zine, R. Flamary, R. Gribonval, and N. Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. In _AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics_, volume 108, pages 1-20, 2020.
* Fatras et al. [2021] K. Fatras, T. Sejourne, R. Flamary, and N. Courty. Unbalanced minibatch optimal transport; applications to domain adaptation. In _International Conference on Machine Learning_, pages 3186-3197. PMLR, 2021.
* Flamary et al. [2021] R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenfos, K. Fatras, N. Fournier, et al. Pot: Python optimal transport. _The Journal of Machine Learning Research_, 22(1):3571-3578, 2021.
* Gazdieva et al. [2022] M. Gazdieva, L. Rout, A. Korotin, A. Kravchenko, A. Filippov, and E. Burnaev. An optimal transport perspective on unpaired image super-resolution. _arXiv preprint arXiv:2202.01116_, 2022.
* Gazdieva et al. [2023] M. Gazdieva, A. Korotin, D. Selikhanovych, and E. Burnaev. Extremal domain translation with neural optimal transport. In _Advances in Neural Information Processing Systems_, volume 36, 2023.
* Genevay [2019] A. Genevay. _Entropy-Regularized Optimal Transport for Machine Learning_. Theses, PSL University, Mar. 2019. URL https://theses.hal.science/tel-02319318.
* Genevay et al. [2018] A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with sinkhorn divergences. In _International Conference on Artificial Intelligence and Statistics_, pages 1608-1617. PMLR, 2018.
* Gozlan et al. [2017] N. Gozlan, C. Roberto, P.-M. Samson, and P. Tetali. Kantorovich duality for general transport costs and applications. _Journal of Functional Analysis_, 273(11):3327-3405, 2017.
* Gulrajani et al. [2017] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. In _Advances in Neural Information Processing Systems_, pages 5767-5777, 2017.
* Gushchin et al. [2023] N. Gushchin, A. Kolesov, A. Korotin, D. Vetrov, and E. Burnaev. Entropic neural optimal transport via diffusion processes. In _Advances in Neural Information Processing Systems_, 2023.
* Gushchin et al. [2023] N. Gushchin, A. Kolesov, P. Mokrov, P. Karpikova, A. Spiridonov, E. Burnaev, and A. Korotin. Building the bridge of schr\(\backslash\)" odinger: A continuous entropic optimal transport benchmark. _arXiv preprint arXiv:2306.10161_, 2023.
* Gushchin et al. [2024] N. Gushchin, S. Kholkin, E. Burnaev, and A. Korotin. Light and optimal schr\(\backslash\)"odinger bridge matching. In _arXiv preprint arXiv:2402.03207_, 2024.
* Hardy et al. [1952] G. H. Hardy, J. E. Littlewood, and G. Polya. _Inequalities_. Cambridge university press, 1952.
* Hutter and Rigollet [2021] J.-C. Hutter and P. Rigollet. Minimax estimation of smooth optimal transport maps. 2021.
* Janati et al. [2020] H. Janati, B. Muzellec, G. Peyre, and M. Cuturi. Entropic optimal transport between unbalanced gaussian measures has a closed form. _Advances in neural information processing systems_, 33:10468-10479, 2020.
* Karras et al. [2019] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* Kingma and Ba [2014] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Klein et al. [2023] D. Klein, T. Uscidda, F. Theis, and M. Cuturi. Generative entropic neural optimal transport to map within and across spaces. _arXiv preprint arXiv:2310.09254_, 2023.
* Korotin et al. [2021] A. Korotin, L. Li, A. Genevay, J. M. Solomon, A. Filippov, and E. Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. _Advances in Neural Information Processing Systems_, 34:14593-14605, 2021.

* [38] A. Korotin, L. Li, J. Solomon, and E. Burnaev. Continuous wasserstein-2 barycenter estimation without minimax optimization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=3tFAs5E-Pe.
* [39] A. Korotin, A. Kolesov, and E. Burnaev. Kantorovich strikes back! wasserstein gans are not optimal transport? In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [40] A. Korotin, D. Selikhanovych, and E. Burnaev. Kernel neural optimal transport. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=Zuc_MHTUma4.
* [41] A. Korotin, N. Gushchin, and E. Burnaev. Light schr\(\backslash\)" odinger bridge. In _The Twelfth International Conference on Learning Representations_, 2024.
* [42] T. Koshizuka and I. Sato. Neural lagrangian schr\(\backslash\)"{o}dinger bridge: Diffusion modeling for population dynamics. In _The Eleventh International Conference on Learning Representations_, 2022.
* [43] M. Liero, A. Mielke, and G. Savare. Optimal entropy-transport problems and a new hellinger-kantorovich distance between positive measures. _Inventiones mathematicae_, 211(3):969-1117, 2018.
* [44] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* [45] F. Lubeck, C. Bunne, G. Gut, J. S. del Castillo, L. Pelkmans, and D. Alvarez-Melis. Neural unbalanced optimal transport via cycle-consistent semi-couplings. _arXiv preprint arXiv:2209.15621_, 2022.
* [46] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs created equal? a large-scale study. In _Advances in neural information processing systems_, pages 700-709, 2018.
* [47] A. Makkuva, A. Taghvaei, S. Oh, and J. Lee. Optimal transport mapping via input convex neural networks. In _International Conference on Machine Learning_, pages 6672-6681. PMLR, 2020.
* [48] T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman. Plugin estimation of smooth optimal transport maps. _arXiv preprint arXiv:2107.12364_, 2021.
* [49] M. Mohri, A. Rostamizadeh, and A. Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [50] P. Mokrov, A. Korotin, and E. Burnaev. Energy-guided entropic neural optimal transport. In _The Twelfth International Conference on Learning Representations_, 2024.
* [51] Q. M. Nguyen, H. H. Nguyen, Y. Zhou, and L. M. Nguyen. On unbalanced optimal transport: Gradient methods, sparsity and approximation error. _arXiv preprint arXiv:2202.03618_, 2022.
* [52] T. T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximation by finite mixtures of continuous density functions that vanish at infinity. _Cogent Mathematics & Statistics_, 7(1):1750861, 2020.
* [53] G. Peyre, M. Cuturi, et al. Computational optimal transport. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [54] K. Pham, K. Le, N. Ho, T. Pham, and H. Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In _International Conference on Machine Learning_, pages 7673-7682. PMLR, 2020.
* [55] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto. Adversarial latent autoencoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14104-14113, 2020.

* [56] A.-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. _arXiv preprint arXiv:2109.12004_, 2021.
* [57] P. Rigollet and A. J. Stromme. On the sample complexity of entropic optimal transport. _arXiv preprint arXiv:2206.13472_, 2022.
* [58] R. Rockafellar. Duality and stability in extremum problems involving convex functions. _Pacific Journal of Mathematics_, 21(1):167-187, 1967.
* [59] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [60] L. Rout, A. Korotin, and E. Burnaev. Generative modeling with optimal transport maps. In _International Conference on Learning Representations_, 2022.
* [61] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, and M. Blondel. Large scale optimal transport and mapping estimation. In _International Conference on Learning Representations_, 2018.
* [62] T. Sejourne, G. Peyre, and F.-X. Vialard. Unbalanced optimal transport, from theory to numerics. _arXiv preprint arXiv:2211.08775_, 2022.
* [63] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [64] A. Taghvaei and A. Jalali. 2-Wasserstein approximation via restricted convex potentials with application to improved training for GANs. _arXiv preprint arXiv:1902.07197_, 2019.
* [65] A. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In _International conference on machine learning_, pages 9526-9536. PMLR, 2020.
* [66] F. Vargas, P. Thodoroff, A. Lamacraft, and N. Lawrence. Solving schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* [67] G. Wang, Y. Jiao, Q. Xu, Y. Wang, and C. Yang. Deep generative learning via schrodinger bridge. In _International Conference on Machine Learning_, pages 10794-10804. PMLR, 2021.
* [68] Y. Xie, M. Chen, H. Jiang, T. Zhao, and H. Zha. On scalable and efficient computation of large scale optimal transport. In _International Conference on Machine Learning_, pages 6882-6892. PMLR, 2019.
* [69] Y. Xie, Y. Luo, and X. Huo. An accelerated stochastic algorithm for solving the optimal transport problem. _arXiv preprint arXiv:2203.00813_, 2022.
* [70] K. D. Yang and C. Uhler. Scalable unbalanced optimal transport using generative adversarial networks. In _International Conference on Learning Representations_, 2018.
* [71] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. _Advances in neural information processing systems_, 30, 2017.

[MISSING_PAGE_FAIL:15]

Since (19)=(20), inequalities (22), (23), (24) actually turn to equalities.

_Step 2._ Since we consider convex, lower semi-continuous functions \(f\), it holds that \(\overline{\overline{f}}=f\). Then, substituting \(\overline{f}\) in formula (21), we derive

\[-f(-t)=-\overline{\overline{f}}(-t)=\inf_{u\in\mathbb{R}}\{ut+\overline{f}(u) \}\Longrightarrow-f(-t)\leq ut+\overline{f}(u)\ \ \forall u\in\mathbb{R}.\] (25)

Thus, for arbitrary \(x\in\mathbb{R}^{d}\), taking \(f=f_{1}\), \(t=-\frac{\gamma_{x}^{*}(x)}{p(x)}\), we get that \(-f_{1}\Big{(}\frac{\gamma_{x}^{*}(x)}{p(x)}\Big{)}\leq u\frac{\gamma_{x}^{*}( x)}{p(x)}+\overline{f_{1}}(-u)\ \forall u\in\mathbb{R}.\) At the same time, from _step 1_, we know that \(-f_{1}\Big{(}\frac{\gamma_{x}^{*}(x)}{p(x)}\Big{)}=\frac{\gamma_{x}^{*}(x)}{p( x)}\phi^{*}(x)+\overline{f_{1}}(-\phi^{*}(x))\), see (22). Then

\[\frac{\gamma_{x}^{*}(x)}{p(x)}\phi^{*}(x)+\overline{f_{1}}(-\phi^ {*}(x))\leq u\frac{\gamma_{x}^{*}(x)}{p(x)}+\overline{f_{1}}(-u)\ \forall u\in\mathbb{R}\Longrightarrow\] \[\frac{\gamma_{x}^{*}(x)}{p(x)}(\phi^{*}(x)-u)\leq(\overline{f_{1} }(-u)-\overline{f_{1}}(-\phi^{*}(x))\ \forall u\in\mathbb{R}.\] (26)

Similarly, for arbitrary \(y\in\mathbb{R}^{d}\)

\[\frac{\gamma_{y}^{*}(y)}{q(y)}(\psi^{*}(y)-u)\leq(\overline{f_{2}}(-u)- \overline{f_{2}}(-\psi^{*}(y))\ \forall u\in\mathbb{R}.\] (27)

_Step 3._ Now we are ready to prove the main result. We note that:

\[\text{D}_{\text{KL}}\ (\gamma^{*}\|\gamma_{\theta,\omega})\!=\int_{ \mathbb{R}^{d}\times\mathbb{R}^{d}}\gamma^{*}(x,y)\log\gamma^{*}(x,y)dxdy-\] \[\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\!\!\gamma^{*}(x,y)\log \gamma_{\theta,\omega}(x,y)dxdy+\|\gamma_{\theta,\omega}\|_{1}-\|\gamma^{*}\| _{1}.\] (28)

While the ground-truth UEOT plan \(\gamma^{*}(x,y)\) and the optimal dual variables \(\phi^{*}(x)\), \(\psi^{*}(y)\) are connected via equation (5), our parametrized plan \(\gamma_{\theta,\omega}(x,y)\) can be expressed using \(\phi_{\theta,\omega}(x)\), \(\psi_{\theta}(y)\) as \(\gamma_{\theta,\omega}(x,y)=\exp\{\varepsilon^{-1}(\phi_{\theta,\omega}(x)+ \psi_{\theta}(y)-\|x-y\|^{2}/2)\}\), see (9). Then

\[\varepsilon^{-1}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\!\!\gamma^{*}(x,y) \big{(}\phi_{\theta,\omega}(x)\!+\!\psi_{\theta}(y)\!-\!\|x-y\|^{2}/2\big{)}dxdy \!+\|\gamma_{\theta,\omega}\|_{1}-\|\gamma^{*}\|_{1}=\]

\[\varepsilon^{-1}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\gamma^{*}(x,y) \big{(}\phi^{*}(x)+\psi^{*}(y)-\phi_{\theta,\omega}(x)-\psi_{\theta}(y)\big{)} dxdy+\|\gamma_{\theta,\omega}\|_{1}-\|\gamma^{*}\|_{1}=\]

\[\varepsilon^{-1}\int_{\mathbb{R}^{d}}\frac{\gamma_{x}^{*}(x)}{p(x)}\big{(} \phi^{*}(x)-\phi_{\theta,\omega}(x)\big{)}p(x)dx+\]

\[\varepsilon^{-1}\int_{\mathbb{R}^{d}}\frac{\gamma_{y}^{*}(y)}{q(y)}\big{(} \psi^{*}(y)-\psi_{\theta}(y)\big{)}q(y)dy+\|\gamma_{\theta,\omega}\|_{1}-\| \gamma^{*}\|_{1}\stackrel{{\eqref{eq:1}}}{{\leq}}\]

\[\varepsilon^{-1}\!\!\int_{\mathbb{R}^{d}}(\overline{f}_{1}(-\phi_{\theta, \omega}(x))\!-\!\overline{f}_{1}(-\phi^{*}(x)))p(x)dx+\]

\[\varepsilon^{-1}\!\int_{\mathbb{R}^{d}}(\overline{f}_{2}(-\psi_{\theta}(y))\!- \!\overline{f}_{2}(-\psi^{*}(y)))q(y)dy+\!\|\gamma_{\theta,\omega}\|_{1}\!-\! \|\gamma^{*}\|_{1}\!=\]

\[\varepsilon^{-1}\cdot\Big{(}\!\int_{\mathbb{R}^{d}}\!\!\overline{f}_{1}(-\phi_{ \theta,\omega}(x))p(x)dx\!+\!\!\!\int_{\mathbb{R}^{d}}\!\!\overline{f}_{2}(- \psi_{\theta}(y))q(y)dy\!+\!\varepsilon\|\gamma_{\theta,\omega}\|_{1}\!-\]

\[\underbrace{(\int_{\mathbb{R}^{d}}\!\!\overline{f}_{1}(-\phi^{*}(x))p(x)dx\!+ \!\!\int_{\mathbb{R}^{d}}\!\!\overline{f}_{2}(-\psi^{*}(y))q(y)dy\!+\! \varepsilon\|\gamma^{*}\|_{1}}_{\mathcal{L}^{*}\stackrel{{ \text{\tiny{\rm def}}}}{{=}}}=\]

\[\varepsilon^{-1}(\mathcal{L}(\theta,\omega)-\mathcal{L}^{*}).\]

### Proof of Proposition 4.2

We begin with proving the auxiliary theoretical results (Propositions A.1, A.2) which are needed to prove the main proposition of this section.

**Proposition A.1** (Rademacher bound on the estimation error).: _It holds that_

\[\mathbb{E}\big{[}\mathcal{L}(\widehat{\theta})-\mathcal{L}(\overline{\theta}) \big{]}\leq 4\mathcal{R}_{N}(\mathcal{F}_{1},p)+4\mathcal{R}_{M}(\mathcal{F}_{2},q),\]

_where \(\mathcal{F}_{1}=\{\overline{f}_{1}(-\phi_{\theta,\omega})|(\theta,\omega)\in \Theta\times\Omega\}\), \(\mathcal{F}_{2}=\{\overline{f}_{2}(-\psi_{\theta})|\theta\in\Theta\}\) for \(\phi_{\theta,\omega}(x)=\varepsilon\log\frac{u_{\omega}(x)}{c_{\theta}(x)}+ \frac{\|x\|^{2}}{2}\), \(\psi_{\theta}(y)=\varepsilon\log v_{\theta}(y)+\frac{\|y\|^{2}}{2}\), and \(\mathcal{R}_{N}(\mathcal{F}_{1},p)\), \(\mathcal{R}_{M}(\mathcal{F}_{2},q)\) denote the Rademacher complexity [63, SS26] of the functional classes \(\mathcal{U},\ \mathcal{V}\) w.r.t. to the sample sizes \(N\), \(M\) of distributions \(p\), \(q\)._

Proof of Proposition a.1.: The derivation of this fact is absolutely analogous to [41, Proposition B.1], [50, Theorem 4] or [64, Theorem 3.4]. 

**Proposition A.2** (Bound on the Rademacher complexity of the considered classes).: _Let \(0<a\leq A\), let \(0<u\leq U\), let \(0<w\leq W\) and \(V>0\). Consider the class of functions_

\[\mathcal{F}_{1}=\big{\{}x\mapsto\overline{f}_{1}(-\varepsilon\log u_{\omega}(x )+\varepsilon\log c_{\theta}(x)-\frac{\|x\|^{2}}{2})\big{\}},\]

\[\mathcal{F}_{2}=\big{\{}y\mapsto\overline{f}_{2}(-\varepsilon\log v_{\theta}( y)-\frac{\|y\|^{2}}{2})\big{\}}\text{ where }\]

\[u_{\omega}(x),\ v_{\theta}(y),\ c_{\theta}(x)\text{ belong to the class }\]

\[\mathcal{V}=\big{\{}x\mapsto\sum_{k=1}^{K}\alpha_{k}\exp\big{(}x^{T}U_{k}x+v_{ k}^{T}x+w_{k}\big{)}\text{ with }\] (29)

\[uI\preceq U_{k}=U_{k}^{T}\preceq UI;\|v_{k}\|\leq V;w\leq w_{k}\leq W;a\leq \alpha_{k}\leq A\big{\}}.\]

_Following [41, Proposition B.2], we call the functions of the class \(\mathcal{V}\) as constrained log-sum-exp quadratic functions. We assume that \(\overline{f}_{1},\ \overline{f}_{2}\) are Lipshitz functions and measures \(p\), \(q\) are compactly supported with the supports lying in a zero-centered ball of a radius \(R>0\). Then_

\[\mathcal{R}_{N}(\mathcal{F}_{1},p)\leq\frac{C_{0}}{\sqrt{N}},\ \mathcal{R}_{M}( \mathcal{F}_{2},q)\leq\frac{C_{1}}{\sqrt{M}}\]

_where the constants \(C_{0}\), \(C_{1}\)**do not depend** on sizes \(N\), \(M\) of the empirical samples from \(p,\ q\)._

Proof of Proposition a.2.: Thanks to [41, Proposition B.2], the Rademacher complexities of constrained log-sum-exp quadratic functions \(x\mapsto\log u_{\omega}(x)\), \(x\mapsto\log c_{\theta}(x)\) and \(y\mapsto\log v_{\theta}(y)\) are known to be bounded by \(O(\frac{1}{\sqrt{N}})\) or \(O(\frac{1}{\sqrt{M}})\) respectively. According to the definition of Rademacher complexity, for single quadratic functions \(x\mapsto\frac{x^{T}x}{2}\) (\(y\mapsto\frac{y^{T}y}{2}\)) it is just equal to zero. Then, using the well-known scaling and additivity properties of the Rademacher complexity [63], we get that \(x\mapsto-\varepsilon\log u_{\omega}(x)+\varepsilon\log c_{\theta}(x)-\frac{ \|x\|^{2}}{2}\) and \(y\mapsto-\varepsilon\log v_{\theta}(y)-\frac{\|y\|^{2}}{2}\) are bounded by \(O(\frac{1}{\sqrt{N}})\) and \(O(\frac{1}{\sqrt{M}})\) respectively. The remaining step is to recall that \(\overline{f}_{1}(x)\) and \(\overline{f}_{2}(y)\) are Lipschitz. Therefore, according to Talagrand's contraction principle [49], the Rademacher complexities of \(\mathcal{F}_{1}\) and \(\mathcal{F}_{2}\) are also bounded by \(O(\frac{1}{\sqrt{N}})\) and \(O(\frac{1}{\sqrt{M}})\), respectively. 

Proof of Proposition a.2.: The proof directly follows from Propositions A.1 and A.2. 

### Proof of Theorem 4.3

To begin with, we provide a quick reminder about the Fenchel-Rockafellar theorem which is needed to derive the dual form of problem (3).

**Theorem A.3** (Fenchel-Rockafellar [58]).: _Let \((E,E^{\prime})\) and \((F,F^{\prime})\) be two couples of topologically paired spaces. Let \(A:E\mapsto F\) be a continuous linear operator and \(\overline{A}:F^{\prime}\mapsto E^{\prime}\) be its adjoint. Let\(f\) and \(g\) be lower semi-continuous and proper convex functions defined on \(E\) and \(F\) respectively. If there exists \(x\in\text{dom}f\) s.t. \(g\) is continuous at \(Ax\), then_

\[\sup_{x\in E}-f(-x)-g(Ax)=\min_{\overline{y}\in F^{\prime}}\overline{f}( \overline{A}\;\overline{y})+\overline{g}(\overline{y})\]

_and the min is attained. Moreover, if there exists a maximizer \(x\in E\) then there exists \(\overline{y}\in F^{\prime}\) satisfying \(Ax\in\partial\overline{g}(\overline{y})\) and \(\overline{A}\overline{y}\in\partial f(-x)\)._

We note that in the below theorem \(\mathcal{C}_{2}(\mathbb{R}^{d})\)**does not** denote the space of twice differentiable continuous functions. The exact definition of this space and \(\mathcal{C}_{2,b}(\mathbb{R}^{d})\) is given in **notations** part.

**Theorem A.4** (Dual form of problem (3)).: _The primal UEOT problem (3) has the dual counterpart (4) where the potentials \((\phi,\psi)\) belong to the space \(\mathcal{C}_{2,b}(\mathbb{R}^{d})\times\mathcal{C}_{2,b}(\mathbb{R}^{d})\). The minimum of (3) is attained for a unique \(\gamma^{*}\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\)._

Proof.: We recall that in the primal form, the minimization is performed over functions \(\gamma\) belonging to \(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\). In this proof, we suppose that this space is endowed with a coarsest topology \(\sigma(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}))\) which makes continuous the linear functionals \(\gamma\mapsto\int\zeta d\gamma,\;\forall\zeta\in\mathcal{C}_{2}(\mathbb{R}^{ d}\times\mathbb{R}^{d})\). Then the topological space \(\big{(}\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}),\sigma( \mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}))\big{)}\) has a topological dual \(\big{(}\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}),\sigma( \mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}))^{\prime}\) which, actually, is (linear) isomorphic to the space \(\mathcal{C}_{2}(\mathbb{R}^{d}\times\mathbb{R}^{d})\), see [26, Lemma 9.9]. This fact opens an opportunity to apply the well-celebrated Fenchel-Rockafellar theorem. For this purpose, we will consider the following spaces: \(E\stackrel{{\text{def}}}{{=}}\mathcal{C}_{2}(\mathbb{R}^{d}) \times\mathcal{C}_{2}(\mathbb{R}^{d})\), \(F\stackrel{{\text{def}}}{{=}}\mathcal{C}_{2}(\mathbb{R}^{d}\times \mathbb{R}^{d})\) and their duals \(E^{\prime}\stackrel{{\text{def}}}{{=}}\mathcal{M}_{2,+}(\mathbb{R }^{d})\times\mathcal{M}_{2,+}(\mathbb{R}^{d})\) and \(F^{\prime}\stackrel{{\text{def}}}{{=}}\mathcal{M}_{2,+}(\mathbb{R }^{d}\times\mathbb{R}^{d})\).

_Step 1._ Recall that the convex conjugate of any function \(g:\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\to\mathbb{R}\cup\{+\infty\}\) is defined for each \(\zeta\in(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}))^{\prime}\cong \mathcal{C}_{2}(\mathbb{R}^{d}\times\mathbb{R}^{d})\) as \(\overline{g}(\zeta)=\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times \mathbb{R}^{d})}\{\gamma,\zeta\big{\rangle}-g(\gamma)\}\). For the convenience of further derivations, we introduce additional functionals corresponding to the summands in the primal UEOT problem (3):

\[P(\gamma)\stackrel{{\text{def}}}{{=}}\int_{\mathbb{R }^{d}}\int_{\mathbb{R}^{d}}\frac{\|x-y\|^{2}}{2}\gamma(x,y)dxdy-\varepsilon H (\gamma);\] \[F_{1}(\gamma_{x})\stackrel{{\text{def}}}{{=}}D_{f_{ 1}}\left(\gamma_{x}\|p\right);\;\;\;F_{2}(\gamma_{y})\stackrel{{ \text{def}}}{{=}}D_{f_{2}}\left(\gamma_{y}\|q\right).\] (30)

For our purposes, we need to calculate the convex conjugates of these functionals. Fortunately, convex conjugates of \(f\)-divergences \(F_{1}(\gamma_{x})\) and \(F_{2}(\gamma_{y})\) are well-known, see [1, Proposition 23], and equal to

\[\overline{F_{1}}(\phi)\stackrel{{\text{def}}}{{=}}\int_{\mathbb{R }^{d}}\overline{f_{1}}(\phi(x))p(x)dx,\;\;\;\overline{F_{2}}(\psi)\stackrel{{ \text{def}}}{{=}}\int_{\mathbb{R}^{d}}\overline{f_{2}}(\psi(y))q(y)dy.\]

To proceed, we calculate the convex conjugate of \(P(\gamma)\):

\[\overline{P}(\zeta)=\overline{\int_{\mathbb{R}^{d}}\int_{\mathbb{ R}^{d}}\frac{\|x-y\|^{2}}{2}\gamma(x,y)dxdy-\varepsilon H(\gamma)} =\] \[=\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^ {d})}\Big{\{}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\zeta(x,y)\gamma(x,y)dxdy- \big{(}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\frac{\|x-y\|^{2}}{2}\gamma(x,y)dxdy-\] \[\varepsilon H(\gamma)\big{)}\Big{\}} =\] \[\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d}) }\Big{\{}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\big{(}\zeta(x,y)-\frac{\|x-y \|^{2}}{2}\big{)}\gamma(x,y)dxdy+\] \[\varepsilon\underbrace{\Big{(}\int_{\mathbb{R}^{d}}\int_{ \mathbb{R}^{d}}-\gamma(x,y)\log\gamma(x,y)dxdy+\|\gamma\|_{!}\Big{)}}_{=H(\gamma)}\Big{\}} =\] \[\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{ d})}\varepsilon\cdot\Big{\{}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\gamma(x,y) \Big{(}\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon}-\log\gamma(x,y) \Big{)}dxdy+\|\gamma\|_{1}\Big{\}}=\]\[\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})}(- \varepsilon)\cdot\Big{\{}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\gamma(x,y) \big{(}\log\gamma(x,y)-\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon} \big{)}dxdy-\|\gamma\|_{1}\Big{\}}=\] \[\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{ d})}(-\varepsilon)\cdot\Big{\{}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\gamma(x,y) \big{(}\log\gamma(x,y)-\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon} \big{)}dxdy-\|\gamma\|_{1}+\] \[\underbrace{\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\exp\{ \frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon}\}dxdy-\int_{\mathbb{R}^{d }}\int_{\mathbb{R}^{d}}\exp\{\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{ \varepsilon}\}dxdy}_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R }^{d})}(-\varepsilon)\cdot\Bigg{\{}\mathsf{D}_{\text{KL}}\left(\gamma\|\exp\{ \frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon}\}\right)\ -\] \[\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\exp\{\frac{\zeta(x,y)- \frac{\|x-y\|^{2}}{2}}{\varepsilon}\}dxdy\Bigg{\}}.\] (32)

Here in the transition from (31) to (32), we keep in mind our prior calculations of \(\mathsf{D}_{\text{KL}}\) in (28). Recall that \(\mathsf{D}_{\text{KL}}\) is non-negative and attains zero at the unique point

\[\gamma(x,y)=\exp\{\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{ \varepsilon}\}.\] (33)

Thus, we get

\[\overline{P}(\zeta)=\varepsilon\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\exp \{\frac{\zeta(x,y)-\frac{\|x-y\|^{2}}{2}}{\varepsilon}\}dxdy.\] (34)

_Step 2_. Now we are ready to apply the Fenchel-Rockafellar theorem in our case. To begin with, we show that this theorem is applicable to problem (3), i.e., that the functions under consideration satisfy the necessary conditions. Indeed, it is known that the convex conjugate of any functional (e.g., \(\overline{F_{1}}(\cdot)\), \(\overline{F_{2}}(\cdot)\), \(\overline{P}(\cdot)\)) is **lower semi-continuous** and **convex**. Besides, the listed functionals are **proper convex2**. Indeed, the properness of \(\overline{F_{1}}(\cdot)\) and \(\overline{F_{2}}(\cdot)\) follows from the fact that \(f\)-divergences are known to be lower-semicontinuous and proper themselves, while properness of \(\overline{P}(\cdot)\) is evident from (32).

Footnote 2: _Proper convex_ function is a real-valued convex function which has a non-empty domain, never attains the value \((-\infty)\) and is not identically equal to \((+\infty)\). This property ensures that the minimization problem for this function has non-trivial solutions.

Now we consider the linear operator \(A:\mathcal{C}_{2}(\mathbb{R}^{d})\times\mathcal{C}_{2}(\mathbb{R}^{d})\mapsto \mathcal{C}_{2}(\mathbb{R}^{d}\times\mathbb{R}^{d})\) which is defined as \(A(\phi,\psi):(x,y)\mapsto\phi(x)+\psi(y)\). It is continuous, and its adjoint is defined on \(\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})\) as \(\overline{A}(\gamma)=(\gamma_{x},\gamma_{y})\). Indeed, \(\langle\overline{A}(\gamma),(u,v)\rangle=\langle\gamma,A(u,v)\rangle=\int_{ \mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\gamma(x,y)(u(x)+v(y))dxdy=\int_{\mathbb{R }^{d}}\gamma_{x}(x)u(x)dx+\int_{\mathbb{R}^{d}}\gamma_{y}(y)v(y)dy\). Thus, the strong duality and the existence of minimizer for (3) follows from the Fenchel-Rockafellar theorem which states that problems

\[\sup_{(\phi,\psi)\in\mathcal{C}_{2}(\mathbb{R}^{d})\times\mathcal{C}_{2}( \mathbb{R}^{d})}\{-\overline{P}(A(\phi,\psi))-\overline{F_{1}}(-\phi)- \overline{F_{2}}(-\psi)\}\] (35)

and

\[\min_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{d}\times\mathbb{R}^{d})}\{P( \gamma)+F_{1}(\gamma_{x})+F_{2}(\gamma_{y})\}\] (36)

are equal. The uniqueness of the minimizer for (3) comes from the strict convexity of \(P(\cdot)\) (which holds thanks to the entropy term). Note that the conjugate of the sum of \(F_{1}\) and \(F_{2}\) is equal to the sum of their conjugates since they are defined for separate non-intersecting groups of parameters.

Next we prove that the supremum can be restricted to \(\mathcal{C}_{2,b}(\mathbb{R}^{d}\times\mathbb{R}^{d})\). Here we use "\(\wedge\)" to denote the operation of taking minimum between the function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and real value \(k\): \((f\wedge k)(x)\stackrel{{\text{def}}}{{=}}\min(f(x),k)\). Then analogously to [26, Theorem 9.6], we get:\[\sup_{(\phi,\psi)\in\mathcal{C}_{2}(\mathbb{R}^{d})\times\mathcal{C}_{2}( \mathbb{R}^{d})}\lim_{k_{1},k_{2}\to\infty}\{-\overline{P}(A(\phi\wedge k_{1}, \psi\wedge k_{2}))-\overline{F_{1}}(-(\phi\wedge k_{1}))-\overline{F_{2}}(-( \psi\wedge k_{2}))\}\leq\] \[\sup_{(\phi,\psi)\in\mathcal{C}_{2,b}(\mathbb{R}^{d})\times \mathcal{C}_{2,b}(\mathbb{R}^{d})}\{-\overline{P}(A(\phi,\psi))-\overline{F_{1 }}(-\phi)-\overline{F_{2}}(-\psi)\}.\] (37)

Since the another inequality is obvious, the two quantities are equal which completes the proof.

Proof of Theorem 4.3.: Our aim is to prove that for all \(\delta>0\) there exist unnormalized Gaussian mixtures \(u_{\omega}\) and \(v_{\theta}\) s.t. \(\mathcal{L}(\theta,\omega)-\mathcal{L}^{*}<\delta\varepsilon\). We define

\[\mathcal{J}(\phi,\psi)\stackrel{{\text{def}}}{{=}}\]

Then from (4) and Theorem (A.4), it follows that \(\mathcal{L}^{*}=\inf_{(\phi,\psi)\in\mathcal{C}_{2,b}(\mathbb{R}^{d})\times \mathcal{C}_{2,b}(\mathbb{R}^{d})}\mathcal{J}(\phi,\psi)\). Finally, using the definition of the infimum, we get that for all \(\delta^{\prime}>0\) there exist some functions \((\widehat{\phi},\ \widehat{\psi})\in\mathcal{C}_{2,b}(\mathbb{R}^{d})\times \mathcal{C}_{2,b}(\mathbb{R}^{d})\) such that \(\mathcal{J}(\widehat{\phi},\widehat{\psi})\!\!<\!\!\mathcal{L}^{*}\!\!+\!\! \delta^{\prime}\). For further derivations, we set \(\delta^{\prime}\stackrel{{\text{def}}}{{=}}\frac{\delta \varepsilon}{2}\) and pick the corresponding \((\widehat{\phi},\ \widehat{\psi})\).

_Step 1._ We start with the derivation of some inequalities useful for future steps. Since \((\phi,\psi)\in\mathcal{C}_{2,b}(\mathbb{R}^{d})\times\mathcal{C}_{2,b}( \mathbb{R}^{d})\), they have upper bounds \(\widehat{a}\) and \(\widehat{b}\) such that for all \(x,y\in\mathbb{R}^{d}\): \(\widehat{\phi}(x)\leq\widehat{a}\) and \(\widehat{\psi}(y)\leq\widehat{b}\) respectively. We recall that by the assumption of the theorem, measures \(p\) and \(q\) are compactly supported. Thus, there exist balls centered at \(x=0\) and \(y=0\) and having some radius \(R>0\) which contain the supports of \(p\) and \(q\) respectively. Then we define

\[\widetilde{\phi}(x)\stackrel{{\text{def}}}{{=}} \widehat{\phi}(x)-\max\{0,\max\{\|x\|^{2}-R^{2},\|x\|^{4}-R^{4}\}\}\leq \widehat{\phi}(x)\leq\widehat{a};\] \[\widetilde{\psi}(y)\stackrel{{\text{def}}}{{=}} \widehat{\psi}(y)-\max\{0,\|y\|^{2}-R^{2}\}\leq\widehat{\psi}(y)\leq\widehat{b}.\]

We get that

\[\widetilde{\phi}(x)\leq\widehat{\phi}(x),\widetilde{\psi}(y)\leq \widehat{\psi}(y)\Longrightarrow\] \[\varepsilon\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\exp\{\frac {1}{\varepsilon}(\widetilde{\phi}(x)+\widetilde{\psi}(y)\!-\!\frac{\|x-y\|^{2} }{2})\}dxdy\leq\] \[\varepsilon\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\exp\{\frac {1}{\varepsilon}(\widehat{\phi}(x)+\widehat{\psi}(y)\!-\!\frac{\|x-y\|^{2}}{2 })\}dxdy\] (38)

Importantly, for all \(x\) and \(y\) within the supports of \(p\) and \(q\) it holds that \(\widetilde{\phi}(x)=\widehat{\phi}(x)\) and \(\widetilde{\psi}(y)=\widehat{\psi}(y)\), respectively. Then

\[\int_{\mathbb{R}^{d}}\overline{f}_{1}(-\widehat{\phi}(x))p(x)dx= \int_{\mathbb{R}^{d}}\overline{f}_{1}(-\widetilde{\phi}(x))p(x)dx,\] \[\int_{\mathbb{R}^{d}}\overline{f}_{2}(-\widehat{\psi}(y))q(y)dy =\int_{\mathbb{R}^{d}}\overline{f}_{2}(-\widetilde{\psi}(y))q(y)dy.\] (39)

Combining (38) and (39), we get that \(\mathcal{J}(\widehat{\phi},\widehat{\psi})\leq\mathcal{J}(\widehat{\phi}, \widehat{\psi})<\mathcal{L}^{*}+\delta\).

Before moving on, we note that functions \(\exp\{\widetilde{\phi}(x)/\varepsilon\}\) and \(\exp\{\widetilde{\psi}(y)/\varepsilon\}\) are continuous and non-negative. Therefore, since measures \(p\) and \(q\) are compactly supported, there exist some constants \(e_{\min},\ h_{\min}>0\) such that \(\exp\{\widetilde{\phi}(x)/\varepsilon\}>e_{\min}\) and \(\exp\{\widetilde{\psi}(y)/\varepsilon\}>h_{\min}\) for all \(x\) and \(y\) from the supports of measures \(p\) and \(q\) respectively. We keep these constants for future steps.

_Step 2._ This step of our proof is similar to [41, Theorem 3.4]. We get that

\[\exp\big{(}\widetilde{\phi}(x)/\varepsilon\big{)}\leq\exp\bigg{(}\frac{ \widehat{a}-\max\{0,\|x\|^{2}-R^{2}\}}{\varepsilon}\bigg{)}\leq\exp\big{(} \frac{\widehat{a}+R^{2}}{\varepsilon}\big{)}\cdot\exp(-\|x\|^{2}/\varepsilon \big{)},\] (40)

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_EMPTY:23]

\[\mathcal{L}^{*}+\delta^{\prime}+\delta^{\prime\prime}\Bigg{[}L_{1} \cdot\frac{\varepsilon}{e_{\min}}+L_{2}\cdot\frac{\varepsilon}{h_{\min}}+ \varepsilon(2\pi\varepsilon)^{\frac{d}{2}}\Big{(}1+(\pi\varepsilon)^{\frac{d}{2 }}\exp\big{\{}\frac{\widehat{a}+R^{2}}{\varepsilon}\big{\}}\Big{)}\Bigg{]}\leq\] \[\mathcal{L}^{*}+\frac{\delta\varepsilon}{2}+\frac{\delta \varepsilon}{2}\Longrightarrow\] \[\text{D}_{\text{KL}}\left(\gamma_{\theta,\omega}\|\gamma^{*} \right)\leq\varepsilon^{-1}(\mathcal{L}(\theta,\omega)-\mathcal{L}^{*})<\delta\]

which completes the proof. 

_Remark_.: In fact, the assumption of the Lishitzness of \(\overline{f_{1}}(x)\), \(\overline{f_{2}}(x)\) can be omitted. Indeed, under the _"everything is compact"_ assumptions of Proposition 4.2 and Theorem 4.3, inputs to \(\overline{f_{1}}(\cdot)\), \(\overline{f_{2}}(\cdot)\) also always belong to certain compact sets. The convex functions are known to be Lipshitz on compact subsets of \(\mathbb{R}\), see [31, Chapter 3, SS18], and we actually do not need the Lipschitzness on the entire \(\mathbb{R}\).

## Appendix B Experiments Details

### General Details

To minimize our objective (14), we parametrize \(\alpha_{k},r_{k},S_{k}\) of \(v_{\theta}\) and \(\beta_{l},\mu_{l},\Sigma_{l}\) of \(u_{\omega}\) in (11). Here we follow the standard practices in deep learning and parametrize logarithms \(\log\alpha_{k},\ \log\beta_{l}\) instead of directly parameterizing \(\alpha_{k},\beta_{l}\). In turn, variables \(r_{k},\ \mu_{l}\) are parametrized directly as multi-dimensional vectors. We consider diagonal matrices \(S_{k},\ \Sigma_{l}\) and parametrize them via their diagonal values \(\log(S_{k})_{i,\,i}\) and \(\log(\Sigma_{l})_{i,\,i}\) respectively. We initialize the parameters following the scheme in [41]. In all our experiments, we use the Adam optimizer.

### Details of the Experiment with Gaussian Mixtures

We use \(K=L=5\), \(\varepsilon=0.05\), \(lr=3e-4\) and batchsize \(128\). We do \(2\cdot 10^{4}\) gradient steps. For the LightSB algorithm, we use the parameters presented by the authors in the official repository.

### Details of the Image Translation Experiment

We use the code and decoder model from

https://github.com/podgorskiy/ALAE

We download the data and neural network extracted attributes for the FFHQ dataset from

https://github.com/ngushchin/LightSB/

In the _Adult_ class we include the images with the attribute \(\textit{Age}\geq 44\); in the _Young_ class - with the \(\textit{Age}\in[16,44]\). We excluded the images with faces of children to increase the accuracy of classification per _gender_ attribute. For the experiments with our solver, we use weighted DKL divergence with parameters \(\tau\) specified in Appendix C, and set \(K=L=10\), \(\varepsilon=0.05\), \(lr=1\), and batch size to \(128\). We do \(5\cdot 10^{3}\) gradient steps using Adam optimizer [35] and MultiStepLR scheduler with parameter \(\gamma=0.1\) and milestones\(=[500,1000]\). For testing [41, LightSB] solver, we use the official code (see the link above) and instructions provided by the authors.

**Baselines.** For the OT-FM and UOT-FM methods, we parameterize the vector field \((v_{t,\theta})_{t\in[0,1]}\) for mass transport using a 2-layer feed-forward network with 512 hidden neurons and ReLU activation. An additional sinusoidal embedding[65] was applied for the parameter \(t\). The learning rate for the Adam optimizer was set to 1e-4. To obtain an optimal transport plan \(\pi^{*}(x,y)\) discrete OT solvers from the POT [21] package were used. These methods are built on the solutions (plans \(\pi^{*}(x,y)\)) of discrete OT problems, to obtain them we use the POT [21] package. Especially for the UOT-FM, we use the ot.unbalanced.sinkhorn with the regularization equal to \(0.05\). We set the number of training and inference time steps equal to \(100\). To obtain results of UOT-FM for Fig. 3, we run this method for 3K epochs with parameter \(reg\_m\in[5e-4,5e-3,5e-2,0.5,1,10,10^{2},10^{3},10^{4},10^{5},10^{6}]\) and reported the mean values of final metrics for 3 independent launches with different seeds. In Tables 20, 21, 22, for each translation we report the results for one chosen parameter specified in Appendix D.1. We use the corresponding checkpoints of UOT-FM to visualize its performance in Fig. 4. For [9, UOT-SD], [70, UOT-GAN] we use the official code provided by the authors, see the links:

https://github.com/Jae-Moo/Unbalanced-Optimal-Transport-Generative-Model

https://github.com/uhlerlab/unbalanced_ot

While both UOT-SB, UOT-GAN methods were not previously applied to the FFHQ dataset, we set up a grid search for the parameters and followed the instructions provided by the authors for parameter settings. Both for UOT-SB, UOT-GAN, we used a 3-layer neural network with 512 hidden neurons, and ReLU activation was used for the generator networks and the potential and discriminator, respectively. Adam optimizer [35] with \(lr=10^{-5}\) and \(lr=10^{-4}\) was used to train the networks in UOT-SB and UOT-GAN, respectively. We train the methods for 10K iterations and set a batch size to 128. For UOT-SD, we used \(\text{D}_{\text{KL}}\) divergence with their unbalancedness parameter \(\tau\!=\!0.002\). For other parameters, we used the default values provided by the authors for CIFAR-10 generation tasks. For all baseline models which use entropy regularization, we set \(\varepsilon=0.05\).

## Appendix C Additional Discussion & Experiments with \(f\)-divergences

**Details about \(f\)-divergences between positive measures.** In the classic form, \(f\)-divergences are defined as measures of dissimilarity between two _probability_ measures. This definition should be revised when dealing with measures of arbitrary masses. In the paragraph below we show that if the function \(f\) is convex, non-negative, and attains zero uniquely at point \(\{1\}\) then \(D_{f}\left(\mu_{1}\|\mu_{2}\right)\) is a valid measure of dissimilarity between two positive measures.

Let \(\mu_{1},\mu_{2}\in\mathcal{M}_{2,+}(\mathbb{R}^{d^{\prime}})\) be two positive measures. The \(f\)-divergence satisfies \(D_{f}\left(\mu_{1}\|\mu_{2}\right)\geq 0\) which is obvious from the non-negativity of \(f\). From the definition of \(D_{f}\left(\mu_{1}\|\mu_{2}\right)\) and the fact that function \(f\) attains zero uniquely at a point \(\{1\}\), we obtain that \(D_{f}\left(\mu_{1}\|\mu_{2}\right)=0\) if and only if \(\mu_{1}(x)=\mu_{2}(x)\) holds \(\mu_{2}\)-everywhere. Actually, \(\mu_{1}(x)=\mu_{2}(x)\) should hold for all \(x\) as \(\mu_{1}\) must be absolutely continuous w.r.t. \(\mu_{2}\) (otherwise \(D_{f}\left(\mu_{1}\|\mu_{2}\right)\) is assumed to be equal \(+\infty\)).

**The usage of \(D_{\chi^{2}}\) divergence.** We tested the performance of our solver with scaled \(\text{D}_{\text{KL}}\) divergences in the main text, see SS5.1, SS5.2. For completeness, here we evaluate our solver with \(\text{D}_{\chi^{2}}\) divergence in _Gaussian Mixture_ experiment. We use the same experimental setup as in SS5.1 and present the qualitative results in Fig. 5. Interestingly, the solver's results differ from those which we obtain for \(\text{D}_{\text{KL}}\) divergence. For \(\text{D}_{\chi^{2}}\) divergence, supports of learned plans' marginals constitute only parts of source and target measures' supports when \(\tau=1\). The issue disappears with a slight increase of \(\tau\), i.e., for \(\tau=2\). At the same time, a further increase of \(\tau\) is useless, since the learned plans fail to deal with class imbalance issue. Thus, parameter \(\tau\) should be adjusted heuristically. In the case of \(\text{D}_{\text{KL}}\) divergence, supports coincide for all \(\tau\), see Fig. 2. This motivates us to use \(\text{D}_{\text{KL}}\) divergences in our main experiments.

**Parameter \(\tau\) in Gaussian Mixture experiment.** An ablation study on unbalancedness parameter \(\tau\) in _Gaussian Mixture_ experiment is conducted in SS5.1 and above in this section. For completeness, we also perform the quantitative assessment of our solver with \(\text{D}_{\text{KL}}\) divergence for different unbalancedness parameters \(\tau\). We compute the normalized OT cost \((\mathbb{E}_{x\sim p}\mathbb{E}_{y\sim(y|x)}\frac{(x-y)^{2}}{2})\) between the source and generated distributions, and the Wasserstein distance between the generated and target

Figure 5: Conditional plans \(\gamma_{\theta,\omega}(y|x)\) learned by our solver with scaled \(\text{D}_{\chi^{2}}\) divergences in _Gaussians Mixture_ experiment (\(\tau\in[1,2,5,10]\)).

distributions (computed by a discrete OT solver). For completeness, we additionally calculate the metrics for the _balanced_[41, LightSB] approach. The results are presented in Table 4.

_Results._ Recall that the unbalanced nature of our solver leads to two important properties. Firstly, our solver better preserves the properties of the input objects than the balanced approaches \(-\) indeed, it allows for preserving object attributes (classes) even in the case of class imbalance. Secondly, due to the relaxed boundary condition for the target distribution, the distribution generated by our solver is naturally less similar to the target distribution than for balanced methods.

The above intuitive reasoning is confirmed by the metrics we obtained. Indeed, as the \(\tau\) parameter increases, when our method becomes more and more similar to balanced approaches, the normalized OT cost between the source and generated distributions increases, and the Wasserstein distance between learned and target distributions decreases. LightSB [1] baseline, which is a purely balanced approach, shows the best quality in terms of Wasserstein distance and the worst in terms of OT cost.

**Parameters \(\tau\), \(\varepsilon\) in image experiments.** The effect of entropy regularization parameter \(\varepsilon\) is well studied, see, e.g., [28, 41]. Namely, increasing the parameter \(\varepsilon\) stimulates the conditional distributions \(\gamma_{\theta}(y|x)\) to become more dispersed. Still, below we provide an additional quantitative analysis of its influence on the learned translation. Besides, we address the question _how does the parameter \(\tau\) influence the performance of our solver in image translation experiments?_ To address this question, we learn the translations _Young\(\rightarrow\) Adult_, _Man\(\rightarrow\) Woman_ on FFHQ dataset varying the parameters \(\tau\), \(\varepsilon\), see SS5.2 for the experimental setup details. We test our solver with scaled D\({}_{\text{KL}}\) divergence training it for 5K iterations. Other hyperparameters are in Appendix B. In Tables 5, 8, 11, 14, we report the accuracy of keeping the attributes of the source images (e.g., gender in _Young\(\rightarrow\)Adult_ translation). In Tables 6, 9, 12, 15, we report the accuracy of mapping to the correct target class (e.g., _adult_ people in _Young\(\rightarrow\)Adult_ translation). In Tables 7, 10, 13, 16, we report FD metrics which is defined as _Frechet distance_ between means and covariances of the learned and the target measures. For convenience, we additionally illustrate the results of ablation studies on Fig. 6.

_Results_ show that increase of \(\varepsilon\) negatively influences both accuracy of keeping the attributes of the source images and FD of generated latent codes which is caused by an increased dispersi

\begin{table}
\begin{tabular}{l|l l l l l l} \hline \hline \(\varepsilon\)\({}^{\tau}\) & 10 & 20 & 50 & \(10^{2}\) & **250** & \(10^{3}\) & \(10^{6}\) \\ \hline \(0.01\) & \(38.94\pm 0.91\) & \(51.92\pm 0.80\) & \(67.68\pm 1.06\) & \(75.49\pm 0.30\) & \(81.34\pm 1.06\) & \(83.67\pm 0.74\) & \(85.27\pm 1.35\) \\ \(0.05\) & \(40.09\pm 0.16\) & \(53.19\pm 0.49\) & \(69.01\pm 0.74\) & \(77.41\pm 0.67\) & \(81.78\pm 0.33\) & \(84.80\pm 0.90\) & \(85.63\pm 0.76\) \\ \(0.1\) & \(44.18\pm 1.50\) & \(56.74\pm 0.58\) & \(71.77\pm 0.57\) & \(78.34\pm 0.91\) & \(83.70\pm 0.54\) & \(87.07\pm 0.37\) & \(88.21\pm 0.23\) \\ \(0.5\) & \(50.51\pm 0.34\) & \(65.61\pm 2.04\) & \(81.50\pm 1.17\) & \(87.48\pm 0.06\) & \(92.21\pm 0.29\) & \(92.93\pm 0.14\) & \(93.80\pm 0.55\) \\ \(1.0\) & \(-\) & \(70.81\pm 2.69\) & \(83.82\pm 2.45\) & \(89.56\pm 0.50\) & \(93.78\pm 0.35\) & \(95.04\pm 0.20\) & \(95.50\pm 0.41\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test accuracy (\(\uparrow\)) of mapping to the target in _Young \(\rightarrow\) Adult_ translation.

\begin{table}
\begin{tabular}{l|l l l l l l} \hline \hline
**Method** & U-LightMOT (\(\tau=1\)) & U-LightMOT (\(\tau=10\)) & U-LightMOT (\(\tau=50\)) & U-LightMOT (\(\tau=100\)) & LightSB \\ \hline OT cost (\(\downarrow\)) & \(2.023\) & \(2.913\) & \(3.874\) & \(3.931\) & \(3.952\) \\ \hline \(\mathbb{W}_{2}\)-distance (\(\downarrow\)) & \(2.044\) & \(1.107\) & \(0.138\) & \(0.091\) & \(0.088\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Normalized OT cost between the input and learned distributions, and Wasserstein distance between the learned and target distributions in _Gaussian mixture_ experiment.

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline \hline \(\varepsilon\)\({}^{\tau}\) & 10 & 20 & 50 & \(10^{2}\) & **250** & \(10^{3}\) & \(10^{6}\) \\ \hline \(0.01\) & \(38.94\pm 0.91\) & \(51.92\pm 0.80\) & \(67.68\pm 1.06\) & \(75.49\pm 0.30\) & \(81.34\pm 1.06\) & \(83.67\pm 0.74\) & \(85.27\pm 1.35\) \\ \(0.05\) & \(40.09\pm 0.16\) & \(53.19\pm 0.49\) & \(69.01\pm 0.74\) & \(77.41\pm 0.67\) & \(81.78\pm 0.33\) & \(84.80\pm 0.90\) & \(85.63\pm 0.76\) \\ \(0.1\) & \(44.18\pm 1.50\) & \(56.74\pm 0.58\) & \(71.77\pm 0.57\) & \(78.34\pm 0.91\) & \(83.70\pm 0.54\) & \(87.07\pm 0.37\) & \(88.21\pm 0.23\) \\ \(0.5\) & \(50.51\pm 0.34\) & \(65.61\pm 2.04\) & \(81.50\pm 1.17\) & \(87.48\pm 0.06\) & \(92.21\pm 0.29\) & \(92.93\pm 0.14\) & \(93.80\pm 0.55\) \\ \(1.0\) & \(-\) & \(70.81\pm 2.69\) & \(83.82\pm 2.45\) & \(89.56\pm 0.50\) & \(93.78\pm 0.35\) & \(95.04\pm 0.20\) & \(95.50\pm 0.41\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Test FD (\(\downarrow\)) of generated latent codes in _Young \(\rightarrow\) Adult_ translation.

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline \hline \(\varepsilon\)\({}^{\tau}\) & 10 & 20 & 50 & \(10^{2}\) & **250** & \(10^{3}\) & \(10^{6}\) \\ \hline \(0.01\) & \(38.55\pm 1.24\) & \(26.77\pm 1.59\) & \(17.39\pm 0.19\) & \(16.42\pm 1.96\) & \(12.28\pm 1.16\) & \(11.46\pm 0.70\) & \(10.85\pm 0.12\) \\ \(0.05\) & \(42.05\pm 3.04\) & \(31.47\pm 0.33\) & \(25.90\pm 1.49\) & \(18.31\pm 0.16\) & \(17.15\pm 0.48\) & \(16.27\pm 0.62\) & \(19.89\pm 5.51\) \\ \(0.1\) & \(50.05\pm 2.48\) & \(40.67\pm 0.73\) & \(31.15\pm 0.32\) & \(27.49\pm 0.56\) & \(25.86\pm 0.88\) & \(24.57\pm 0.06\) & \(26.30\pm 0.15\) \\ \(0.5\) & \(114.49\pm 1.06\) & \(104.42\pm 0.33\) & \(98.21\pm 4.21\) & \(92.48\pm 0.98\) & \(89.42\pm 0.10\) & \(88.74\pm 0.12\) & \(89.55\pm 1.18\) \\ \(1.0\) & \(-\) & \(16.39\pm 1.50\) & \(149.84\pm 0.86\) & \(142.39\pm 0.18\) & \(137.40\pm 0.21\) & \(135.85\pm 0.1Interestingly, accuracy of mapping to the correct target class does not have an evident dynamics w.r.t. \(\varepsilon\). At the same time, when \(\tau\)_increases_, the learned plans provide _worse accuracy_ for keeping the input latents' class but _better FD_ of generated latent codes and accuracy of mapping to the target

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\(\tau\) & 10 & 20 & 50 & \(\mathbf{10^{2}}\) & \(10^{3}\) & \(10^{6}\) \\ \hline
0.01 & \(54.84\pm 4.12\) & \(37.87\pm 0.55\) & \(27.27\pm 3.17\) & \(22.69\pm 3.48\) & \(27.73\pm 4.28\) & \(17.32\pm 1.29\) \\
0.05 & \(51.61\pm 2.58\) & \(51.25\pm 11.78\) & \(34.28\pm 1.04\) & \(27.29\pm 2.60\) & \(30.63\pm 1.67\) & \(25.77\pm 2.22\) \\
0.1 & \(59.45\pm 3.60\) & \(51.17\pm 3.28\) & \(43.05\pm 3.68\) & \(38.02\pm 1.64\) & \(34.42\pm 0.91\) & \(37.54\pm 1.79\) \\
0.5 & \(132.45\pm 6.07\) & \(119.82\pm 1.07\) & \(107.16\pm 1.48\) & \(107.23\pm 6.76\) & \(103.24\pm 3.22\) & \(100.71\pm 1.66\) \\
1.0 & \(-\) & \(182.26\pm 2.20\) & \(164.51\pm 1.50\) & \(156.41\pm 2.04\) & \(146.73\pm 0.28\) & \(146.05\pm 0.10\) \\ \hline \end{tabular}
\end{table}
Table 13: Test FD (\(\downarrow\)) of generated latent codes in _Man \(\rightarrow\) Woman_ translation.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\(\tau\) & 10 & 20 & 50 & \(\mathbf{10^{2}}\) & \(10^{3}\) & \(10^{6}\) \\ \hline
0.01 & \(67.45\pm 0.51\) & \(76.38\pm 0.36\) & \(84.86\pm 0.23\) & \(87.87\pm 0.37\) & \(91.56\pm 0.40\) & \(92.29\pm 0.37\) \\
0.05 & \(67.21\pm 0.10\) & \(76.66\pm 1.22\) & \(84.43\pm 0.40\) & \(87.79\pm 0.38\) & \(91.98\pm 0.50\) & \(92.29\pm 0.71\) \\
0.1 & \(65.29\pm 1.54\) & \(76.58\pm 1.10\) & \(84.29\pm 0.73\) & \(88.65\pm 0.64\) & \(92.33\pm 0.49\) & \(92.87\pm 0.11\) \\
0.5 & \(69.13\pm 3.43\) & \(76.74\pm 1.39\) & \(82.33\pm 6.53\) & \(88.66\pm 2.89\) & \(94.06\pm 0.62\) & \(94.91\pm 0.45\) \\
1.0 & \(-\) & \(80.13\pm 1.89\) & \(85.87\pm 0.53\) & \(91.00\pm 0.62\) & \(95.66\pm 0.41\) & \(96.37\pm 0.52\) \\ \hline \end{tabular}
\end{table}
Table 9: Test accuracy (\(\uparrow\)) of mapping to the target in _Adult \(\rightarrow\) Young_ translation.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\(\tau\) & 10 & 20 & 50 & \(\mathbf{10^{2}}\) & \(10^{3}\) & \(10^{6}\) \\ \hline
0.01 & \(64.38\pm 0.29\) & \(74.57\pm 0.78\) & \(85.78\pm 0.87\) & \(90.32\pm 0.70\) & \(93.79\pm 0.26\) & \(94.19\pm 0.40\) \\
0.05 & \(60.97\pm 1.14\) & \(74.99\pm 0.32\) & \(85.01\pm 0.64\) & \(90.22\pm 0.50\) & \(93.87\pm 0.25\) & \(94.44\pm 0.07\) \\
0.1 & \(61.36\pm 0.60\) & \(73.52\pm 0.24\) & \(86.38\pm 0.40\) & \(90.38\pm 0.39\) & \(94.19\pm 0.28\) & \(94.73\pm 0.36\) \\
0.5 & \(65.74\pm 0.87\) & \(73.61\pm 0.58\) & \(86.45\pm 0.48\) & \(89.80\pm 0.66\) & \(95.14\pm 0.58\) & \(95.59\pm 0.61\) \\
1.0 & \(-\) & \(78.18\pm 0.18\) & \(86.85\pm 0.74\) & \(91.50\pm 0.74\) & \(95.63\pm 0.14\) & \(95.93\pm 0.17\) \\ \hline \end{tabular}
\end{table}
Table 12: Test accuracy (\(\uparrow\)) of mapping to the target in _Man \(\rightarrow\) Woman_ translation.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\(\tau\) & 10 & 20 & 50 & \(\mathbf{10^{2}}\) & \(10^{3}\) & \(10^{6}\) \\ \hline
0.01 & \(67.45\pm 0.51\) & \(76.38\pm 0.36\) & \(84.86\pm 0.23\) & \(87.87\pm 0.37\) & \(91.56\pm 0.40\) & \(92.29\pm 0.37\) \\
0.05 & \(67.21\pm 0.10\) & \(76.66\pm 1.22\) & \(84.43\pm 0.40\) & \(87.79\pm 0.38\) & \(91.98\pm 0.50\) & \(92.29\pm 0.71\) \\
0.1 & \(65.29\pm 1.54\) & \(76.58\pm 1.10\) & \(84.29\pm 0.73\) & \(88.65\pm 0.64\) & \(92.33\pm 0.49\) & \(92.87\pm 0.11\) \\
0.5 & \(69.13\pm 3.43\) & \(76.74\pm 1.39\) & \(82.33\pm 6.53\) & \(88.66\pm 2.89\) & \(94.06\pm 0.62\) & \(94.91\pm 0.45\) \\
1.0 & \(-\) & \(80.13\pm 1.89\) & \(85.87\pm 0.53\) & \(91.00\pm 0.62\) & \(95.66\pm 0.41\) & \(96.37\pm 0.52\) \\ \hline \end{tabular}
\end{table}
Table 10: Test FD (\(\downarrow\)) of generated latent codes in _Adult \(\rightarrow\) Young_ translation.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\(\tau\) & 10 & 20 & 50 & \(\mathbf{10^{2}}\) & \(10^{3}\) & \(10^{6}\) \\ \hline
0.01 & \(67.45\pm 0.51\) & \(76.38\pm 0.36\) & \(84.86\pm 0.23\) & \(87.87\pm 0.37\) & \(91.56\pm 0.40\) & \(92.29\pm 0.37\) \\
0.05 & \(67.21\pm 0.10\) & \(76.66\pm 1.22\) & \(84.43\pm 0.40\) & \(87.79\pm 0.38\) & \(91.98\pm 0.50\) & \(92.29\pm 0.71\) \\
0.1 & \(65.29\pm 1.54\) & \(76.58\pm 1.10\) & \(84.29\pm 0.73\) & \(88.65\pm 0.64\) & \(92.33\pm 0.49\) & \(92.87\pm 0.11\) \\
0.5 & \(69.13\pm 3.43\) &class. It is an expected behavior since for bigger \(\tau\), the constraints on the marginals of the learned plans become more strict. That is, we enforce the marginals of the learned plans to be closer to source and target measures which allows learning more accurate mappings to target measure but does not allow keeping the source classes in the case of imbalance issues. Interestingly, in _Adult\(\to\)Young_ translation, FD of learned latents and accuracy of mapping to the target do not change much for \(\tau\geq 10^{2}\) while accuracy of keeping the attributes exhibits a significant drop between \(\tau=10^{2}\) and \(\tau=10^{3}\). Thus, we can treat \(\tau=10^{2}\) as optimal since it provides the best trade-off between the quality of learned translations and their ability to keep the features of input latents. In the case of _Young\(\to\)Adult_ translation, the values of accuracy and FD exhibit significant differences for considered \(\tau\). Thus, we may consider a more detailed scale and choose \(\tau=2.5\cdot 10^{2}\) as the optimal one.

It is important to note that our method offers a flexible way to select a domain translation configuration that allows for better preserving the properties of the original objects or generating a distribution closer to the target one. The final optimal configuration selection remains at the discretion of the user. The highlighted values in the Tables are used for comparison with other approaches in SSD.1.

_Remark_. FD should be treated as a _relative_ measure of similarity between learned and target measures. The results obtained by balanced solver [41, LightSB] (equivalent to ours for big \(\tau\)) are considered as a gold standard.

**Number of Gaussian components in potentials.** For completeness, we perform an ablation study of our U-LightOT solver with different number on Gaussian components (\(K\), \(L\)) in potentials \(v_{\theta}\) and \(u_{\omega}\), respectively. We run the solver in _Young\(\to\)Adult_ translation with 5K steps, \(\varepsilon=0.05\) and set \(\tau=250\). The quantitative results (accuracy of keeping the class, accuracy of mapping to the target, FD of generated latents vs target latents) are presented in the Tables below.

The results show that in the considered task, our solver provides good performance even for small number of Gaussian components. This can be explained by the smoothness of the latent representations of data in ALAE autoencoder.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\({}^{\tau}\) & 10 & 20 & 50 & \(10^{2}\) & \(\mathbf{10^{3}}\) & \(10^{6}\) \\ \hline
0.01 & 47.00\(\pm\)1.74 & 33.65\(\pm\)0.67 & 23.85\(\pm\)1.15 & 20.83\(\pm\)1.27 & 16.48\(\pm\)0.09 & 18.78\(\pm\)3.44 \\
0.05 & 52.30\(\pm\)1.29 & 39.79\(\pm\)1.25 & 29.66\(\pm\)1.81 & 27.23\(\pm\)3.45 & 24.68\(\pm\)2.80 & 23.43\(\pm\)1.91 \\
0.1 & 58.40\(\pm\)0.62 & 48.66\(\pm\)0.73 & 37.55\(\pm\)0.35 & 37.74\(\pm\)2.39 & 31.63\(\pm\)0.42 & 32.83\(\pm\)2.02 \\
0.5 & 131.17\(\pm\)0.78 & 120.63\(\pm\)0.76 & 108.44\(\pm\)0.60 & 104.85\(\pm\)1.17 & 101.29\(\pm\)0.11 & 102.26\(\pm\)1.58 \\
1.0 & \(-\) & 186.46\(\pm\)0.92 & 169.64\(\pm\)0.63 & 160.52\(\pm\)0.42 & 152.78\(\pm\)0.13 & 152.37\(\pm\)0.09 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Test FD (\(\downarrow\)) of generated latent codes in _Woman \(\to\)Man_ translation.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(\varepsilon\)\({}^{\tau}\) & 10 & 20 & 50 & \(10^{2}\) & \(\mathbf{10^{3}}\) & \(10^{6}\) \\ \hline
0.01 & 51.54\(\pm\)0.48 & 64.67\(\pm\)1.09 & 77.59\(\pm\)0.39 & 82.52\(\pm\)0.49 & 88.61\(\pm\)0.48 & 88.99\(\pm\)0.18 \\
0.05 & 49.26\(\pm\)0.72 & 64.09\(\pm\)1.08 & 76.88\(\pm\)0.17 & 83.47\(\pm\)0.52 & 88.59\(\pm\)0.55 & 89.14\(\pm\)0.55 \\
0.1 & 49.74\(\pm\)0.78 & 63.99\(\pm\)0.82 & 76.96\(\pm\)0.54 & 82.45\(\pm\)0.22 & 89.29\(\pm\)0.55 & 89.14\(\pm\)0.38 \\
0.5 & 48.64\(\pm\)2.31 & 60.88\(\pm\)0.55 & 77.58\(\pm\)0.28 & 82.10\(\pm\)1.17 & 89.82\(\pm\)0.99 & 90.63\(\pm\)1.06 \\
1.0 & \(-\) & 62.62\(\pm\)0.24 & 74.95\(\pm\)1.17 & 82.42\(\pm\)0.73 & 90.20\(\pm\)0.34 & 90.75\(\pm\)0.37 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Test accuracy (\(\uparrow\)) of mapping to the target in _Woman \(\to\)Man_ translation.

Figure 6: Visualization of ablation studies on parameters \(\tau\), \(\varepsilon\) in image translation experiment.

[MISSING_PAGE_FAIL:29]

### Outlier robustness property of U-LightOT solver

To show the outlier robustness property of our solver, we conduct the experiment on Gaussian Mixtures with added outliers and visualize the results in Fig. 7. The setup of the experiment, in general, follows the _Gaussian mixtures_ experiment setup described in section 5.2 of our paper. The difference consists in outliers (small gaussians) added to the input and output measures.

_The results_ show that our U-LightOT solver successfully eliminates the outliers and manages to simultaneously handle the class imbalance issue. At the same time, the balanced LightSB [41] solver fails to deal with either of these problems.

## Appendix E Limitations and Broader Impact

**Limitations.** One limitation of our solver is the usage of the Gaussian Mixture parametrization which might restrict the scalability of our solver. This points to the necessity for developing ways to optimize objective (4) with more general parametrization, e.g., neural networks.

**Broader impact.** Our work aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline
**Experiment** & OT-FM & [41, LightSB] _or_ & [16, UOT-FM] & UOT-SD & UOT-GAN & U-LightOT \\  & [16] & [30, LightSB] & [9] & [70] & **(ours)** \\ \hline _Young\(\rightarrow\)Adult_ & \(11.93\) & \(15.50\) & \(11.57\,(reg_{-}m=0.005)\) & \(13.28\) & \(11.23\) & \(17.15\,(\tau=250)\) \\ _Adult\(\rightarrow\)Young_ & \(14.10\) & \(21.41\) & \(17.00\,(reg_{-}m=0.005)\) & \(18.44\) & \(14.94\) & \(30.79\,(\tau=10^{2})\) \\ \hline _Man\(\rightarrow\)Human_ & \(16.20\) & \(20.91\) & \(10.31\,(reg_{-}m=0.005)\) & \(16.13\) & \(22.41\) & \(27.29\,(\tau=10^{2})\) \\ _Woman\(\rightarrow\)Max_ & \(11.42\) & \(30.87\) & \(6.99\,(reg_{-}m=0.05)\) & \(13.23\) & \(10.55\) & \(24.68\,(\tau=10^{3})\) \\ \hline \hline \end{tabular}
\end{table}
Table 22: Comparison of FD between the generated and learned latents.

Figure 7: Conditional plans \(\gamma_{\theta,\omega}(y|x)\) learned by our solver (\(\tau=1\)) and LightSB in _Gaussian Mixtures with outliers_ experiment.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: For each contribution listed in abstract and introduction we provide the links to the sections about them.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our solver in SS6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide the proofs and the assumptions in Appendix A.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a full list of the experimental details in Appendix B. We use publicly available datasets. The code for the experiments is provided in supplementary material.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code for our solver is included in supplementary material and will be made public after acceptance of our paper. We use publicly available datasets.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the experimental details in Appendix B.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The focus of the papers is mostly theoretical. The experiments are provided just for illustration of the derived theoretical results.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computational resources are discussed in SS5.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conforms with NeurIPS Code of Ethics. We discuss the societal impact of our paper in SS6.

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the societal impact of our paper in SS6.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The research does not release data or models that have a high risk for misuse and does not need safeguards.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited the creators all of the used assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is included in supplementary material. The license for the code will be provided after the paper acceptance.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not include crowdsourcing experiments or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not include crowdsourcing experiments or research with human subjects.