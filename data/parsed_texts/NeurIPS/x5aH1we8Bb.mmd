# Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (_i.e._, 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation.

## 1 Introduction

The perception system of self-driving cars heavily rely on DNNs to process input data and comprehend the environment. Although DNNs have exhibited great improvements in performance, they have been found vulnerable to adversarial examples [2, 15, 41, 24]. These adversarial examples crafted by adding imperceptible perturbations to input data, can lead DNNs to make wrong predictions. Motivated by the safety-critical nature of self-driving cars, we aim to explore the possibility of generating physically realizable adversarial examples to disrupt 3D detectors in driving scenarios, and further improve the robustness of 3D detectors through adversarial training.

The 2D pixel perturbations (digital attacks) [15, 41] have been proven effective in attacking DNNs in various computer vision tasks [53, 56, 13]. However, these 2D pixel attacks are restricted to digital space and are difficult to realize in our 3D world. To address this challenge, several works have proposed physical attacks. For example, Athalye _et al._[2] propose the framework of Expectation Over Transformation (EOT) to improve the attack robustness over 3D transformation. Other researchers generate adversarial examples beyond image space through differentiable rendering, as seen in [54, 59]. These methods show great promise for advancing the field of 3D adversarial attacks and defense but are still limited in synthetic environments.

Given the safety-critical demand for self-driving cars, several works have proposed physically realizable attacks and defense methods in driving scenarios. For example, Cao _et al._[5, 6] propose to learn 3D-aware adversarial attacks capable of generating adversarial mesh to attack 3D detectors.

However, their works only consider learning a 3D adversarial example for a few specific frames. Thus, the learned example is not universal and may not transfer to other scenes. To mitigate this problem, Tu _et al._[43, 44] propose to learn a transferable adversary that is placed on top of a vehicle. Such an adversary can be used in any scene to hide the attacked object from 3D detectors. However, reproducing their attack in our physical world can be challenging since their adversary must have direct contact with the attacked object. We list detailed comparisons of prior works in Tab. 1.

To address the above challenges and generate 3D adversarial examples in driving scenarios, we build Adv3D upon recent advances in NeRF [35] that provide both differentiable rendering and realistic synthesis. In order to generate physically realizable attacks, we model Adv3D in a patch-attack [40] manner and use an optimization-based approach that starts with a realistic NeRF object [26] to learn its 3D adversarial texture. We optimize the adversarial texture to minimize the predicted confidence of all objects in the scenes, while keeping shape unchanged. During the evaluation, we render the input agnostic NeRF in randomly sampled poses, then we paste the rendered patch onto the unseen validation set to evaluate the attack performance. Owing to the transferability to poses and scenes, our adversarial examples can be executed without prior knowledge of the scene and do not need direct contact with the attacked objects, thus making for more feasible attacks compared with [43, 44, 57, 62]. Finally, we provide thorough evaluations of Adv3D on camera-based 3D object detection with the nuScenes [4] dataset. Our contributions are summarized as follows:

* We introduce Adv3D, the first exploration of formulating adversarial examples as NeRF to attack 3D detectors in autonomous driving. Adv3D provides 3D-aware and photorealistic synthesis that was previously unavailable.
* By incorporating the proposed primitive-aware sampling and semantic-guided regularization, Adv3D generates adversarial examples with enhanced physical realism and realizability.
* We conduct extensive real-world experiments and demonstrate the transferability of our adversarial examples across unseen environments and detectors.

## 2 Related Work

### Adversarial Attack

DNNs are known to be vulnerable to adversarial attacks, where a small perturbation in the input data can cause drastic changes in the output predictions. Szegedy _et al._[41] first discovered that adversarial examples, generated by adding visually imperceptible perturbations to the original images, make DNNs predict a wrong category with high confidence. These vulnerabilities were also discovered in object detection and semantic segmentation [30, 56]. Moreover, DPatch [30] proposes transferable patch-based attacks by compositing a small patch to the input image. However, perturbing image pixels alone does not guarantee that adversarial examples can be created in the physical world. To address this issue, several works have performed physical attacks [3, 8, 18, 23, 46, 52, 58, 61] and exposed real-world threats. For example, Athalye _et al._[2] generated robust 3D adversarial objects by introducing the Expectation Over Transformation (EOT) method. Cheng _et al._[11] developed an adversarial patch with physical-oriented transformations to attack a depth estimation network. In our work, we mainly aim to generate 3D adversarial examples for 3D object detection in driving scenarios.

### Robustness in Autonomous Driving

With the safety-critical nature, it is necessary to pay special attention to robustness in autonomous driving systems [47]. LiDAR-Adv [6] proposes to learn input-specific adversarial point clouds to fool LiDAR detectors. Tu _et al._[44] produces generalizable point clouds that can be placed on a

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Methods** & **Transferability** & **Adv. Type** & **Additional Requirements** \\ \hline Cao _et al._[5, 6] & Poses & 3D Mesh & Model, Annotation \\ Tu _et al._[43, 44] & Poses, Scenes & 3D Mesh & Model, Annotation \\ Xie _et al._[57] & Scenes, Categories & 2D Patch & Model, Annotation \\ \hline Adv3D & Poses, Scenes, Categories & 3D NeRF & Model \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with prior works of adversarial attack in autonomous driving.

vehicle roof to hide it. Furthermore, several work [1, 5, 43] try to attack a multi-sensor fusion system by optimizing 3D mesh through differentiable rendering. We compare our method with prior works in Tab. 1. Our method demonstrates stronger transferability and fewer requirements than prior works.

### Image Synthesis using NeRF

NeRF [35] enables photorealistic synthesis in a 3D-aware manner. Recent advances [45, 60] in NeRF allow for control over materials, illumination, and 6D pose of objects. Additionally, NeRF's rendering comes directly from real-world reconstruction, providing more physically accurate and photorealistic synthesis than previous mesh-based methods that relied on human handicrafts. Moreover, volumetric rendering [19] enables NeRF to perform accurate and efficient gradient computation compared with dedicated renderers in mesh-based differentiable rendering [9, 29, 21].

Recently, there has been tremendous progress in driving scene simulation using NeRF. Block-NeRF [42] achieves city-scale reconstruction by modeling the blocks of cities with several isolated NeRFs to increase capacity. FEGR [51] learns to intrinsically decompose the driving scene for applications such as relighting. Lift3D [26] use NeRF to generate new objects and augment them to driving datasets, demonstrating the capability of NeRF to improve downstream task performance. The driving scene simulation provides a perfect test bed to evaluate the effectiveness of self-driving cars. Our method is related to Lift3D, but aims to understand and improve the robustness of 3D detectors.

## 3 Preliminary

### Camera-based 3D Object Detection in Autonomous Driving

Camera-based 3D object detection is the fundamental task in autonomous driving. Without loss of generality, we focus on evaluating the robustness of camera-based 3D detectors.

The 3D detectors process image data and aim to predict 3D bounding boxes of all surrounding objects. The parameterization of a 3D bounding box can be written as \(\mathbf{b}=\{\mathbf{R},\mathbf{t},\mathbf{s},c\}\), where \(\mathbf{R}\in SO(3)\) is the rotation of the box, \(\mathbf{t}=(x,y,z)\) indicate translation of the box center, \(\mathbf{s}=(l,w,h)\) represent the size (length, width, and height) of the box, and \(c\) is the confidence of the predicted box.

The network structure of camera-based 3D object detectors can be roughly categorized into FoV-based (front of view) and BEV-based (bird's eye view). FoV-based methods [48, 49, 50] can be easily built by adding 3D attribute branches to 2D detectors. BEV-based methods [38, 39] typically convert 2D image feature to BEV feature using camera parameters, then directly detect objects on BEV planes. We refer readers to recent surveys [25, 31] for more detail.

### Differentiable Rendering using NeRF

Our method leverages the differentiable rendering scheme proposed by NeRF [35]. NeRF parameterizes the volumetric density and color as a function of input coordinates. NeRF uses multi-layer perceptron (MLP) or hybrid neural representations [14, 7, 36] to represent this function. For each pixel on an image, a ray \(\mathbf{r}(t)=\mathbf{r}_{o}+\mathbf{r}_{d}\cdot t\) is cast from the camera's origin \(\mathbf{r}_{o}\) and passes through the direction of the pixel \(\mathbf{r}_{d}\) at distance \(t\). In a ray, we uniformly sample \(K\) points from the near plane \(t_{near}\) to the far plane \(t_{far}\), the \(k^{th}\) distance is thus calculated as \(t_{k}=t_{near}+(t_{far}-t_{near})\cdot k/K\). For any queried point \(\mathbf{r}(t_{k})\) on the ray, the network takes its position \(\mathbf{r}\left(t_{k}\right)\) and predicts the per-point color \(\mathbf{c}_{k}\) and density \(\tau_{k}\) with:

\[\left(\mathbf{c}_{k},\tau_{k}\right)=\mathrm{Network}\left(\mathbf{r}\left(t _{k}\right)\right).\] (1)

Note that we omit the direction term as suggested by [16]. The final predicted color of each pixel \(\mathbf{C}(\mathbf{r})\) is computed by approximating the volume rendering integral using numerical quadrature [34]:

\[\begin{split}\mathbf{C}(\mathbf{r})=\sum_{k=0}^{K-1}T_{k}\left(1 -\exp\left(-\tau_{k}\left(t_{k+1}-t_{k}\right)\right)\right)\mathbf{c}_{k},\\ \text{with}\quad T_{k}=\exp\left(-\sum_{k^{\prime}<k}\tau_{k^{ \prime}}\left(t_{k^{\prime}+1}-t_{k^{\prime}}\right)\right).\end{split}\] (2)

We build our NeRF upon Lift3D [26]. Lift3D is a 3D generation framework that generates photorealistic objects by fitting multi-view images synthesized by 2D generative modes [20] using NeRF. The network of Lift3D is a conditional NeRF with additional latent code input, which controls the shape and texture of the rendered object. The conditional NeRF in Lift3D is a tri-plane parameterized [7] generator. With its realistic generation and 3D controllability, Lift3D has demonstrated that the training data generated by NeRF can help to improve downstream task performance. To further explore and exploit the satisfactory property of NeRF, we present a valuable and important application in this work: we leverage the NeRF-generated data to investigate and improve the robustness of the perception system in self-driving cars.

## 4 Method

We illustrate the pipeline of Adv3D in Fig. 1. We aim to learn a transferable adversarial example in 3D detection that, when rendered in any pose (_i.e._, location and rotation), can effectively hide surrounding objects from 3D detectors in any scenes by lowering their confidence. In Sec. 4.1, to improve the physical realizability of adversarial examples, we propose (1) Primitive-aware sampling to enable 3D patch attacks. (2) Disentangle NeRF that provides feasible geometry, and (3) Semantic-guided regularization that enables camouflage adversarial texture. To enhance the transferability across poses and scenes, we formulate the learning paradigm of Adv3D within the EOT framework [2] in Sec. 4.3.

### 3D Adversarial Example Generation

We use a gradient-based method to train our adversarial examples. The training pipeline involves 4 steps: **(i)** randomly sampling the pose of an adversarial example, **(ii)** rendering the example in the sampled pose, **(iii)** pasting the rendered patch into the original image of the training set, and finally, **(iv)** computing the loss and optimizing the latent codes. During inference, we discard the **(iv)** step.

#### 4.1.1 Pose Sampling

To achieve adversarial attack in arbitrary object poses, we apply Expectation of Transformation (EOT) [2] by randomly sampling object poses. The poses of adversarial examples are parameterized as 3D boxes \(\mathbf{b}\) that are restricted to a predefined ground plane in front of the camera. We model the ground plane as a uniform distribution \(\mathcal{B}\) in a specific range that is detailed in the supplement. During training, we independently sample the rendering poses of adversarial examples, and approximate the expectation by taking the average loss over the whole batch.

#### 4.1.2 Primitive-aware Sampling

We model the primitive of adversarial examples as NeRF tightly bound by 3D boxes, in order to enable non-contact and physically realizable attacks. During volume rendering, we compute the intersection of rays \(\mathbf{r}(t)\) with the sampled pose \(\mathbf{b}=\{\mathbf{R},\mathbf{t},\mathbf{s}\}\in\mathcal{B}\), finding the first hit point and the last hit point of box \((t_{near},t_{far})\) by the AABB-ray intersection algorithm [33]. We then sample our points inside

Figure 1: **Adv3D aims to generate 3D adversarial examples that consistently perform attacks under different poses during rendering. We initialize adversarial examples from Lift3D [26]. During training, we optimize the texture latent codes of NeRF to minimize the detection confidence of all surrounding objects. During inference, we evaluate the performance reduction of pasting the adversarial patch rendered using randomly sampled poses on the validation set.**

the range \((t_{near},t_{far})\) to reduce large unnecessary samples and avoid contact with the environment.

\[(t_{near},t_{far})=Intersect(\mathbf{r},\mathbf{b}),\] (3)

\[\mathbf{r}^{\prime}(t_{k}) =\mathbf{\tilde{r}}(t_{near})+(\mathbf{\tilde{r}}(t_{far})- \mathbf{\tilde{r}}(t_{near}))\cdot k/K,\] (4) \[\mathbf{\tilde{r}}(t)=Transform(\mathbf{r}(t),\mathbf{b}),\] (5)

where \(\mathbf{\tilde{r}}(t)\) is the sampled points with additional global to local transformation. Specifically, we use a 3D affine transformation to map original sampled points \(\mathbf{r}(t)=\mathbf{r}_{o}+\mathbf{r}_{d}\cdot t\) into a canonical space \(\mathbf{\tilde{r}}=\{x,y,z\}\in[-1,1]\). This ensures that all the sampled points regardless of their distance from the origin, are transformed to the range \([-1,1]\), thus providing a compact input representation for NeRF network. The transformation is given by:

\[Transform(\mathbf{r},\mathbf{b})=\mathbf{s^{-1}}\cdot(\mathbf{R^{-1}}\cdot \mathbf{r}-\mathbf{t}),\] (6)

where \(\mathbf{b}=\{\mathbf{R},\mathbf{t},\mathbf{s}\}\), \(\mathbf{R}\in SO(3)\) is rotation matrix of the box, \(\mathbf{t},\mathbf{s}\in\mathbb{R}^{3}\) indicate translation and scale vector that move and scale the unit cube to desired location and size. The parameters of \(\mathbf{b}\) are sampled from a pre-defined distribution \(\mathcal{B}\) detailed in the supplement.

Then, the points lied in \([-1,1]\) are projected to exactly cover the tri-plane features \(\mathbf{z}\) for interpolation. Finally, a small MLP takes the interpolated features as input and predicts RGB and density:

\[(\mathbf{c}_{k},\tau_{k})=\mathrm{MLP}(Interpolate(\mathbf{z},\mathbf{r}^{ \prime}\left(t_{k}\right))).\] (7)

The primitive-aware sampling enables patch attacks [40] in a 3D-aware manner by lifting the 2D patch to a 3D box, enhancing the physical realizability by ensuring that the adversarial example only has a small modification to the original 3D environment.

#### 4.1.3 Disentangled NeRF Parameterization

The original parameterization of NeRF combines the shape and texture into a single MLP, resulting in an entangled shape and texture generation. Since shape variation is challenging to reproduce in the real world, we disentangle shape and texture generation and only set the texture as adversarial examples.

We obtain texture latents \(\mathbf{z_{tex.}}\) and shape latents \(\mathbf{z_{shape}}\) from the Lift3D. During volume rendering, we disentangle shape and texture generation by separately predicting RGB and density:

\[\mathbf{c}_{k}=\mathrm{Network}(\mathbf{z_{tex.}},\mathbf{r}^{\prime}\left(t_ {k}\right)),\quad\tau_{k}=\mathrm{Network}(\mathbf{z_{shape}},\mathbf{r}^{ \prime}\left(t_{k}\right)),\] (8)

where \(\mathbf{z_{shape}}\) is fixed and \(\mathbf{z_{texture}}\) is being optimized. Our disentangled parametrization can also be seen as a geometry regularization in [43, 44] but keeps geometry unchanged as a usual vehicle, leading to a more realizable adversarial example.

#### 4.1.4 Semantic-guided Regularization

Setting the full part of the vehicle as adversarial textures is straightforward, but not always feasible in the real world. To improve the physical realizability, we propose to optimize individual semantic parts, such as doors and windows of a vehicle. Specifically, as shown in Fig. 2 (d, e)), we only set a specific part of the vehicle as adversarial texture while maintaining others unchanged. This semantic-guided regularization leads to a camouflage adversarial texture that is less likely spotted in the real world.

To achieve this, we add a semantic branch to Lift3d [26] to predict semantic part labels of the sampled points. We re-train Lift3d by fitting multi-view images and semantic labels generated by editGAN [28]. Using semantic-guided regularization, we maintain the original texture and adversarial part texture at the same time but only optimize the adversarial part texture while leaving the original texture unchanged. This approach allows us to preserve a large majority of parts as usual, but to alter only the specific parts that are adversarial (see Fig. 2 (b, c)). Potential attackers can easily print the adversarial sticker and stick it on the semantic part of vehicles to hide surrounding objects.

In our implementation, we query the NeRF network twice, one for the adversarial texture and the other for the original texture. Then, we replace the part of original texture with the adversarial texture indexed by semantic labels in the point space.

### Gradient Propagation

After rendering the adversarial examples, we paste the adversarial patch into the original image through image composition. The attacked image can be expressed as \(I_{1}\times M+I_{2}\times(1-M)\) where \(I_{1}\) and \(I_{2}\) are the patch and original image, \(M\) is foreground mask predicted by NeRF. Next, the attacked images are fed to pretrained and fixed 3D detectors to compute the objective and back-propagate the gradients. Since both the rendering and detection pipelines are differentiable, Adv3D allows gradients from the objective to flow into the texture latent codes during optimization.

### Learning Paradigm

We formulate our learning paradigm as EOT [2] that finds adversarial texture latent codes by minimizing the expectation of a binary cross-entropy loss over sampled poses and original images:

\[\mathbf{z_{tex.}}=\arg\min_{\mathbf{z_{tex.}}}\mathbb{E}_{\mathbf{b}\sim \mathcal{B}}\mathbb{E}_{\mathbf{x}\sim\mathcal{X}}[-\log(1-P(I(\mathbf{x}, \mathbf{b},\mathbf{z_{tex.}}))],\] (9)

where \(\mathbf{b}\) is the rendering pose sampled from the predefined distribution of ground plane \(\mathcal{B}\), \(\mathbf{x}\) is the original image sampled from the training set \(\mathcal{X}\), \(I(\mathbf{x},\mathbf{b},\mathbf{z_{tex.}})\) is the attacked image that composited by the original image \(\mathbf{x}\) and the adversarial patch rendered using pose \(\mathbf{b}\) and texture latent code \(\mathbf{z_{tex.}}\), and \(P(I(\cdot))\) represents the confidence of all proposals predicted by detectors. We approximate the expectation by averaging the objective of the independently sampled batch. The objective is a binary cross-entropy loss that minimizes the confidence of all predicted bounding boxes, including adversarial objects and normal objects.

Built within the framework of EOT, Adv3D helps to improve the transferability and robustness of adversarial examples over the sampling parameters (poses and scenes here). This means that the attack can be performed without prior knowledge of the scene and are able to disrupt models across different poses and times in a non-contact manner.

### Adversarial Defense by Data Augmentation

Toward defenses against our adversarial attack, we also study adversarial training to improve the robustness of 3D detectors. Adversarial training is typically performed by adding image perturbations using a few PGD steps [55, 32] during the training of networks. However, our adversarial example is too expensive to generate for the bi-level loop of the min-max optimization objective. Thus, instead of generating adversarial examples from scratch at every iteration, we directly leverage the transferable adversarial examples to augment the training set. We use the trained adversarial example to locally store a large number of rendered images to avoid repeated computation. During adversarial training, we randomly paste the rendered adversarial patch into the training images with a probability of \(30\%\), while remaining others unchanged. We provide experimental results in Sec. 5.4.

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline
**Models** & **Backbone** & **Type** & **Clean NDS** & **Adv NDS** & **Clean mAP** & **Adv mAP** \\ \hline FCOS3D [48] & ResNet101 & FoV & 0.3770 & 0.2674 & 0.2980 & 0.1272 \\ PGD-Det [49] & ResNet101 & FoV & 0.3934 & 0.2694 & 0.3174 & 0.1321 \\ DETR3D [50] & ResNet101 & FoV & 0.4220 & 0.2755 & 0.3470 & 0.1336 \\ BEVDet [17] & ResNet50 & BEV & 0.3822 & 0.2247 & 0.3076 & 0.1325 \\ BEVFormer-Tiny [27] & ResNet50 & BEV & 0.3540 & 0.2264 & 0.2524 & 0.1217 \\ BEVFormer-Base [27] & ResNet101 & BEV & 0.5176 & 0.3800 & 0.4167 & 0.2376 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of different detectors under our attack. Clean NDS and mAP denote evaluation using original validation data. Adv NDS and mAP denote evaluation using attacked data.

Figure 2: Rendered results of our adversarial examples. **(a)** Image and semantic label of an instance predicted by NeRF. **(b)** Top: our example without semantic-guided regularization. Bottom: our example with semantic-guided regularization. **(c)** Multi-view consistent synthesis of our examples. **(d,e)** The texture transfer results of side and back part adversary to other vehicles.

## 5 Experiments

In this section, we first describe the training detail of our adversarial attacks. Then we present the experiments of semantic-guided regularization in Sec. 5.1, the analysis of 3D attack in Sec. 5.2, the transferability across detectors in Sec. 5.3, and our adversarial defense method in Sec. 5.4.

DatasetWe conduct our experiments on the widely used nuScenes dataset [4]. This dataset is collected using 6 surrounded-view cameras that cover the full 360\({}^{\circ}\) field of view around the ego-vehicle. The dataset contains 700 scenes for training and 150 scenes for validation. In our work, we train our adversarial examples on the training set and evaluate performance drop on the validation set.

TrainingWe implement our methods using PyTorch [37] and MMDetection3D [12]. All detectors are resumed from checkpoints available on their open-source repositories to match the original performance exactly. We only select one instance from Lift3D [26] as the initialization of examples. We conduct our experiments using 8 NVIDIA A100 80G GPUs. We use the Adam optimizer [22] with a learning rate of 1e-3 for texture latents. In practice, we optimize texture latents on the training set for five epochs with the same batch size as used during training detectors. We do not use any regularization except for semantic-guided regularization. In all experiments without specified, we render two adversarial examples per image. We ablate the number of rendered adversaries in the supplement.

Target Detectors and MetricsWe evaluate the robustness of six representative detectors. Three are FoV-based, and three are BEV-based. The FoV-based methods are FCOS3D [48], PGD-Det [49] and DET3D [50]. The BEV-based methods are BEVDet [17], BEVFormer-Tiny [27] and BEVFormer-Base. Following prior work [57], we evaluate the performance drop on the validation set after the attack. Specifically, we use the Mean Average Precision (mAP) and nuScenes Detection Score (NDS) [4] to evaluate the performance of 3D detectors.

Quantitative ResultsWe provide the experimental results of adversarial attacks in Tab. 2. The attacks are conducted in a full-part manner without semantic-guided regularization to investigate the upper limit of attack performance. We found that, in spite of FoV-based or BEV-based, they display similar robustness. Meanwhile, we see a huge improvement of robustness by utilizing a stronger backbone (ResNet101 versus ResNet50) when comparing BEVFormer-Base with BEVFormer-Tiny. We hope these results will inspire researchers to develop 3D detectors with enhanced robustness.

Visualization ResultsWe visualize our attack results with semantic-guided regularization in Fig. 3 (a,b), and without regularization in Fig. 3 (c). The disappearance of detected objects is caused by their lower confidence scores. For example, the confidence predicted by detectors in Fig. 3 (a) have declined

Figure 3: Visualization of BEVDet [17] prediction on nuScenes validation set under our attacks. The visualization threshold is set at \(0.6\). The adversarial NeRF can hide surrounding objects by minimizing their predicted confidence in a non-contact manner (making the yellow boxes disappear).

from \(0.6\) to \(0.4\), and are therefore filtered out by the visualization threshold of \(0.6\). In Fig. 3 (a), we find that our adversarial NeRF is realistic enough to be detected by a 3D detector if it doesn't display much of the adversarial texture. However, once the vehicle shows a larger area of the adversarial texture as seen in Fig. 3 (b), it will hide all objects including itself due to our untargeted objective.

### Semantic Parts Analysis

In Tab. 3, we provide experiments on the impact of different semantic parts on attack performance. Specifically, we focused on three salient parts of the vehicle: the front, side, and rear. Our results show that compared with adversarial attacks using full parts, the semantic-guided regularization leads to a slightly lower performance drop, but remains a realistic appearance and less likely spotted adversarial texture as illustrated in Fig. 2 (b).

Since we do not have access to annotation during training, we additionally conduct "No Part" experiment that no part of the texture is adversarial, to evaluate the impact of the collision and occlusion. We acknowledge that part of performance degradation can be attributed to the occlusion to original objects and the false positive prediction of adversarial objects (see Fig. 3 (a)), since we do not update the ground truth of adversarial objects to the validation set.

### Effectiveness of 3D-aware attack

To validate the effectiveness of our 3D attacks, we ablate the impact of different poses on the attack performance. In Fig. 4 (a), we divide the BEV plane into \(10\times 10\) bins ranging from \(x\in[-5m,5m]\) and \(z\in[10m,15m]\). We then evaluate the relative mAP drop (percentage) of BEVDet [17] by sampling one adversarial example inside the bin per image, while keeping rotation randomly sampled. Similarly, we conduct experiments of \(30\) uniform rotation bins ranging from \([0,2\pi]\) in Fig. 4 (b). The experimental results demonstrate that all aspects of location and rotation achieve a valid attack (performance drop \(>30\%\)), thereby proving the transferability of poses in our 3D-aware attack.

A finding that contrasts with prior work [44] is the impact of near and far locations in \(z\) axis. Our adversarial example is more effective in the near region compared with the far region, while Tu _et al._[44] display a roughly uniform distribution in all regions. We hypothesize that the attack performance is proportional to the area of the rendered patch, which is highly related to the location

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Data** & **Adv train** & **NDS** & **mAP** \\ \hline Clean val & ✗ & 0.304 & 0.248 \\ Clean val & ✓ & **0.311** & **0.255** \\ \hline Adv val † & ✗ & 0.224 & 0.132 \\ Adv val † & ✓ & **0.264** & **0.181** \\ \hline Adv val § & ✓ & 0.228 & 0.130 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of adversarial training.

Figure 4: To examine the 3D-aware property of our adversarial examples, we ablate the relative performance drop by sampling adversarial examples within different bins of location and rotation.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Semantic Part** & **NDS** & **mAP** \\ \hline Clean & 0.382 & 0.307 \\ No Part & 0.302 & 0.234 \\ Full Parts & 0.224 & 0.132 \\ Part of Front & 0.267 & 0.148 \\ Part of Side & 0.265 & 0.149 \\ Part of Rear & 0.268 & 0.151 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Attack results of different semantic parts.

of objects. Similar findings are also displayed in rotation. The vehicle that poses vertically to the ego vehicle results in a larger rendered area, thus better attack performance.

### Transferability Across Different Detectors

In Tab. 5, we evaluate the transferability of adversarial examples across different detectors. To this end, we train a single adversarial example of each detector separately, then use the example to evaluate the performance drop of other detectors. We show that there is a high degree of transferability between different models. Among them, we observe that DETR3D [50] appears to be more resilient to adversarial attacks than other detectors. We hypothesize this can be attributed to the sparsity of the query-based method. During the projection of 3D query to the 2D image plane, only a single point of the feature is indexed by interpolation, thus fewer areas of adversarial features will be sampled. This finding may have insightful implications for the development of more robust 3D detectors in the future.

### Adversarial Defense by Data Augmentation

We present the results of adversarial training in Tab. 4. The symbol \(\dagger\) indicates attacks using the same adversarial example used in adversarial training, while \(\lx@sectionsign\) indicates a different example. We observe that incorporating adversarial training improves not only the robustness against the seen adversarial examples, but also the clean performance. However, we also note that our adversarial training is not capable of transferring to unseen adversarial examples trained in the same way, mainly due to the fixed adversarial example during adversarial training. Furthermore, we hope that future work can conduct in-depth investigations and consider handling the bi-level loop of adversarial training in order to better defend against adversarial attacks.

## 6 Limitation and Future Work

Learning to Sample and AttackAs we do not have access to the dataset annotations, we can not model the explicit relationship between adversarial and normal objects to avoid collision, and the collision itself can cause a performance drop ("No Parts" in Tab. 3). Future work can apply geometry-aware composition [10] to mitigate this problem. Additionally, future research can explore learning to predict optimal poses of adversarial objects to maximize the effectiveness of attacks.

Potential Harmful ConsequencesThe trained adversarial examples have the potential to induce serious traffic accidents in driving scenarios. However, our work is not intended to cause disruptions in autonomous driving systems. Instead, our goal is to use the examples to gain a deeper understanding of the systems and improve their robustness. We hope our work will draw more attention of the community to further verify and enhance the robustness of autonomous driving systems.

## 7 Conclusion

In this paper, we propose **Adv3D**, the first attempt to model adversarial examples as NeRF. Adv3D enhances the physical realizability of attacks through our proposed primitive-aware sampling and semantic-guided regularization. Compared with prior works of adversarial examples in autonomous driving, our examples are more threatening in practice as we carry non-contact attacks, have feasible 3D shapes as usual vehicles, and display camouflage adversarial texture. Extensive experimental results also demonstrate that Adv3d transfers well to different poses, scenes, and detectors. We hope our work provides valuable insights for creating more realistic evaluations to investigate and improve the robustness of autonomous driving systems.

\begin{table}
\begin{tabular}{l c|c c c c} \hline \hline
**TargetSource** & Clean & FCOS3D & PGD-Det & DETR3D & BEVDet & BEVFormer \\ \hline FCOS3D [48] & 0.298 & **0.124** & 0.141 & 0.144 & 0.176 & 0.158 \\ PGD-Det [49] & 0.317 & 0.172 & **0.131** & 0.150 & 0.186 & 0.172 \\ DETR3D [50] & 0.347 & 0.188 & 0.170 & **0.133** & 0.212 & 0.198 \\ BEVDet [17] & 0.307 & 0.148 & 0.145 & 0.140 & **0.132** & 0.140 \\ BEVFormer [27] & 0.252 & 0.175 & 0.155 & 0.136 & 0.177 & **0.124** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Transferability of our attack to unseen detectors. We evaluate the robustness of **target** detectors using an adversarial example trained on **source** detectors. Reported in mAP.

## References

* [1] Mazen Abdelfattah, Kaiwen Yuan, Z Jane Wang, and Rabab Ward. Adversarial attacks on camera-lidar models for 3d car detection. In _IROS_, 2021.
* [2] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. 2018.
* [3] Tom B Brown, Dandelion Mane, Aurko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch. _arXiv preprint arXiv:1712.09665_, 2017.
* [4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _CVPR_, 2020.
* [5] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang Yang, Qi Alfred Chen, Mingyan Liu, and Bo Li. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. In _IEEE Symposium on Security and Privacy (SP)_, 2021.
* [6] Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adversarial objects against lidar-based autonomous driving systems. _arXiv preprint arXiv:1907.05418_, 2019.
* [7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In _CVPR_, 2022.
* [8] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. Shapeshifrer: Robust physical adversarial attack on faster r-cnn object detector. In _ECML PKDD_, 2019.
* [9] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. _NeurIPS_, 32, 2019.
* [10] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In _CVPR_, 2021.
* [11] Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, and Xiangyu Zhang. Physical attack on monocular depth estimation with optimal adversarial patches. In _ECCV_, 2022.
* [12] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. https://github.com/open-mmlab/mmdetection3d, 2020.
* [13] Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu. Benchmarking adversarial robustness on image classification. In _CVPR_, 2020.
* [14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _CVPR_, 2022.
* [15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _ICLR_, 2015.
* [16] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d aware generator for high-resolution image synthesis. In _ICLR_, 2022.
* [17] Junjie Huang, Guan Huang, Zheng Zhu, Ye Yun, and Dalong Du. Bevdet: High-performance multi-camera 3d object detection in bird-eye-view. _arXiv preprint arXiv:2112.11790_, 2021.
* [18] Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L. Yuille, Changqing Zou, and Ning Liu. Universal physical camouflage attacks on object detectors. In _CVPR_, 2020.
* [19] James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. _ACM SIGGRAPH computer graphics_, 18(3), 1984.
* [20] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In _CVPR_, 2020.
* [21] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In _CVPR_, 2018.
* [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [23] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world adversarial attack on arcface face id system. In _ICPR_. IEEE, 2021.
* [24] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _ICLR Workshop_, 2017.

* [25] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanning Deng, Hao Tian, Xizhou Zhu, Li Chen, Yulu Gao, Xiangwei Geng, Jia Zeng, Yang Li, Jiazhi Yang, Xiaosong Jia, Bohan Yu, Yu Qiao, Dahua Lin, Si Liu, Junchi Yan, Jianping Shi, and Ping Luo. Delving into the devids of bird's-eye-view perception: A review, evaluation and recipe. _arXiv preprint arXiv:2209.05324_, 2022.
* [26] Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, and Ying-Cong Chen. Lift3d: Synthesize 3d training data by lifting 2d gan to 3d generative radiance field. In _CVPR_, 2023.
* [27] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. _ECCV_, 2022.
* [28] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. In _NeurIPS_, 2021.
* [29] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In _ICCV_, 2019.
* [30] Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, and Yiran Chen. Dpatch: An adversarial patch attack on object detectors. _arXiv preprint arXiv:1806.02299_, 2018.
* [31] Yuexin Ma, Tai Wang, Xuyang Bai, Huitong Yang, Yuenan Hou, Yaming Wang, Y. Qiao, Ruigang Yang, Dinesh Manocha, and Xinge Zhu. Vision-centric bev perception: A survey. 2022.
* [32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [33] Alexander Majercik, Cyril Crassin, Peter Shirley, and Morgan McGuire. A ray-box intersection algorithm and efficient dynamic voxel rendering. _Journal of Computer Graphics Techniques Vol_, 7(3), 2018.
* [34] Nelson Max. Optical models for direct volume rendering. _IEEE TVCG_, 1995.
* [35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.
* [36] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM ToG_, 2022.
* [37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _NeurIPS_, 32, 2019.
* [38] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In _ECCV_, 2020.
* [39] Cody Reading, Ali Harakeh, Julia Chae, and Steven L. Waslander. Categorical depth distributionnetwork for monocular 3d object detection. _CVPR_, 2021.
* [40] Abhijith Sharma, Yijun Bian, Phil Munz, and Apurva Narayan. Adversarial patch attacks and defences in vision-based tasks: A survey. _arXiv preprint arXiv:2206.08304_, 2022.
* [41] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _ICLR_, 2014.
* [42] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-neff: Scalable large scene neural view synthesis. In _CVPR_, 2022.
* [43] James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar, Ersin Yumer, and Raquel Urtasun. Exploring adversarial robustness of multi-sensor perception systems in self driving. _arXiv preprint arXiv:2101.06784_, 2021.
* [44] James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, and Raquel Urtasun. Physically realizable adversarial examples for lidar object detection. In _CVPR_, 2020.
* [45] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In _CVPR_, 2022.
* [46] Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and Xianglong Liu. Dual attention suppression attack: Generate adversarial camouflage in physical world. In _CVPR_, 2021.
* [47] Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat, Sergio Casas, Mengye Ren, and Raquel Urtasun. Advsim: Generating safety-critical scenarios for self-driving vehicles. _CVPR_, 2021.
* [48] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully convolutional one-stage monocular 3d object detection. In _ICCV Workshop_, 2021.
* [49] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. Probabilistic and Geometric Depth: Detecting objects in perspective. In _CoRL_, 2021.

* [50] Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao,, and Justin M. Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In _CoRL_, 2021.
* [51] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse rendering of urban scenes. In _CVPR_, June 2023.
* [52] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang. Physical adversarial attack on vehicle detector in the carla simulator. _arXiv preprint arXiv:2007.16118_, 2020.
* [53] Chong Xiang, Charles R Qi, and Bo Li. Generating 3d adversarial point clouds. In _CVPR_, 2019.
* [54] Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes for visual recognition. In _CVPR_, 2019.
* [55] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples improve image recognition. In _CVPR_, 2020.
* [56] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In _CVPR_, 2017.
* [57] Shaoyuan Xie, Zichao Li, Zeyu Wang, and Cihang Xie. On the adversarial robustness of camera-based 3d object detection. _arXiv preprint arXiv:2301.10766_, 2023.
* [58] Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, and Xue Lin. Adversarial t-shirt! evading person detectors in a physical world. In _ECCV_, 2020.
* [59] Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi-Keung Tang, and Alan L Yuille. Adversarial attacks beyond the image space. In _CVPR_, 2019.
* [60] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. _ToG_, 2021.
* [61] Yang Zhang, Hassan Foroosh, Philip David, and Boqing Gong. Camou: Learning physical vehicle camouflages to adversarially attack detectors in the wild. In _International Conference on Learning Representations_, 2019.
* [62] Zijian Zhu, Yichi Zhang, Hai Chen, Yinpeng Dong, Shu Zhao, Wenbo Ding, Jiachen Zhong, and Shibao Zheng. Understanding the robustness of 3d object detection with bird's-eye-view representations in autonomous driving. _arXiv preprint arXiv:2303.17297_, 2023.