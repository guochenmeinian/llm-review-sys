# Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality

Antoine Scheid\({}^{1}\)

Aymeric Capitaine\({}^{1}\)

\({}^{1}\) Centre de Mathematiques Appliquees - CNRS - Ecole polytechnique - Palaiseau, 91120, France

\({}^{2}\) INRIA Saclay, Universite Paris Saclay, LMO - Orsay, 91400, France

\({}^{3}\) University of California, Berkeley

\({}^{4}\) Inria, Ecole Normale Superieure, PSL Research University - Paris, 75, France

###### Abstract

In economic theory, the concept of externality refers to any indirect effect resulting from an interaction between players that affects the social welfare. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To address this issue, we consider a two-player bandit setting where the actions of one of the players affect the other player and we extend the Coase theorem (Coase, 2013). This result shows that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enable transfers and bargaining between the players. Our work removes the classical assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights, the social welfare breaks down. We then design a policy for the players which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty.

## 1 Introduction

The concept of _externality_ is used in economics to capture phenomena that impact the global welfare stemming from economic interactions without any compensation (Buchanan and Stubblebine, 2006; Shah et al., 2018). externality is generally considered as market failure since they result in a loss of collective welfare. Given its practical importance (Dahlman, 1979; Greenfield et al., 2009), mechanisms that characterize and mitigate externalities are central to modern economic thinking.

A first approach to tackle the adverse effects of externalities in modern economics was based on quotas and taxation (see, e.g., Pigou, 2017, and the references therein). However, Coase's theorem (Coase, 2013) shows that in the presence of well-defined property rights and low transaction costs, parties affected by externalities can privately negotiate efficient solutions, and recover a welfare efficient allocation through transfers and bargaining.

Throughout the paper, we use the following simple example to illustrate our results.

**Example 1**.: _Consider two firms 1 (upstream) and 2 (downstream) respectively producing quantities \(q_{1}\geqslant 0\) and \(q_{2}\geqslant 0\) of a good sold at a fixed price \(p>0\). They incur strictly increasing and convex costs, captured by the differentiable cost functions \(c_{1}:q_{1}\mapsto c_{1}(q_{1})\) and \(c_{2}:q_{2}\mapsto c_{2}(q_{2})\), satisfying\(c_{1}(0)=0\) and \(c_{2}(0)=0\). We assume that firm \(1\) exerts on firm 2 a constant externality \(\alpha>0\) per unit produced. In other words, their profit functions (or utilities) are given by_

\[\pi_{1}(q_{1})=pq_{1}-c_{1}(q_{1})\quad\text{and}\quad\pi_{2}(q_{1},q_{2})=pq_{2 }-c_{2}(q_{2})-\alpha q_{1}\.\]

One simple concrete illustration consists in an upstream firm emitting pollutants that reduce the downstream firm's production. If the upstream firm owns the property rights, it may receive a payment from the downstream one to reduce its output and thereby pollution. On the other hand, if the downstream firm owns the property right, it may require compensation from the upstream firm to allow its operation.

The Coase theorem demonstrates that in both cases with appropriate property rights, the resulting levels of production would be welfare efficient. The theorem is typically explained in textbooks under the assumption that the players have perfect knowledge of their own utility or profit function, as well as that of others. However, this assumption is unlikely to hold in real-world scenarios, where players have to learn about their own preferences and those of their competitors.

This example is, of course, a simplification of real world scenarios. For example, Abildrup et al. (2012) consider a more complex setting to model the interaction between farmers and waterworks in Denmark where the farmers have the property rights whereas the waterworks can pay to reduce pollution. In particular, they demonstrate the failure of the theorem, attributing it to the breakdown of the main assumptions: no transaction costs, maximizing behaviors and perfect information, with an important focus on strategic behaviors and the asymmetry of information. We restore the latter in our work and provide foundations for the theorem to hold in more realistic scenarios.

_A key question is whether the Coase theorem holds when players learn their preferences over time._

We investigate this question within the framework of a multi-armed bandit learning. We build upon recent works that extend the classical bandit setting to economics in which there are two players interacting via principal-agent protocols (Dogan et al., 2023, 2023, Scheid et al., 2024). This allows us to capture, for example, a version of the two-firm problem where firms are uncertain regarding both their profit functions and the degree of externality on other firms. We represent production decisions in this problem as arms which can be played by the firms at any round over time, with the goal of finding decisions that maximize their rewards. More precisely, we assume that the reward of the upstream firm only depends on its own action, while the reward of the downstream firm depends on both its action and the upstream firm's action. This dependency on both actions allows us to capture externalities.

The property rights in Example 1, or more generally over a bandit instance, amount to giving a firm the possibility of engaging in monetary transfers that influence the arms that are played over time. The owner of the bandit instance will face a problem of bandit learning with transfers with an upstream player who is also learning his preferences.

To account for the efficiency of a policy in this setup, we extend the classical static notion of welfare efficiency to the online setting. We say that a policy is _Welfare efficient_ if the social welfare regret is sub-linear. Proving the Coase theorem within our setup therefore boils down to show that if the bandit owner (who is without loss of generality the upstream player in this study1) runs a no-regret bandit algorithm to learn and exploit his preferences, the downstream player can then choose an optimal transfer scheme leading to a sub-linear total regret.

Footnote 1: The fact that efficiency is restored whoever is given the property rights is known as the _invariance_ property of the Coase theorem.

Our contributions are as follows:

* We show that when an upstream agent exerts externality on a downstream agent, in the absence of property right, the social welfare breaks down. Put differently, no joint policy of the agents can be welfare efficient.
* We then introduce property rights and show how it affects the game. In this case, bargaining and transfers are available to the players. We propose a policy for the downstream player that leads to welfare efficiency when the upstream player follows any black-box no-regret policy under mild assumption. This solution addresses the breakdown issue at equilibrium. Put together, we show an online version of the _Coase theorem_.

Setup and Inefficiency of Externality

### Bandit game

We consider a sequential bandit game in which two players (downstream and upstream) simultaneously play actions in a bandit instance for a horizon \(T\in\mathbb{N}^{\star}\). The action set for both players is \(\mathcal{A}=\{1,\ldots,K\},K\in\mathbb{N}^{\star}\).

The reward distributions of the agents differ. Given a family of distributions \(\{\gamma_{a}\,:\,a\in\mathcal{A}\}\) indexed by \(\mathcal{A}\), the upstream player's rewards are provided by an i.i.d. family of random variables

\[\{(Z_{a}(t))_{t\in[T]}\,:\,a\in\mathcal{A}\}\enspace,\quad\text{ where }Z_{a}(t)\sim\gamma_{a}\quad\text{for any }t\in[T]\text{ and }a\in \mathcal{A}\;.\]

To model the externality exerted by the upstream on the downstream player, we assume that the latter has a reward that depends both on her action and on that of the upstream player. Formally, this is modeled through a family of distributions \(\{\nu_{a,b}\,:\,a,b\in\mathcal{A}\}\) double-indexed by \(\mathcal{A}\) and an i.i.d. family of random variables

\[\{(X_{a,b}(t))_{t\in[T]}\,:\,a,b\in\mathcal{A}\}\enspace,\quad\text{ where }X_{a,b}(t)\sim\nu_{a,b}\]

is the reward received by the downstream player at time \(t\) if she pulls the arm \(b\) and the upstream player pulls the arm \(a\).

**Players.** We assume that players are risk-neutral expected-utility maximizers, and we define their expected utilities for any \((a,b)\in\mathcal{A}\times\mathcal{A}\) as

\[v^{\text{up}}(a)=\int z\,\gamma_{a}(\mathrm{d}z)\in\mathbb{R}\quad\text{and} \quad v^{\text{down}}(a,b)=\int x\,\nu_{a,b}(\mathrm{d}x)\in\mathbb{R}\;.\]

The distributions \((\gamma_{a})_{a\in\mathcal{A}}\) and \((\nu_{a,b})_{(a,b)\in\mathcal{A}^{2}}\) are unknown to both the downstream and the upstream players and they aim to learn the distributions with best mean rewards by sequentially observing samples from \(\{(Z_{a}(t))_{t\in[T]}\,:\,a\in\mathcal{A}\}\) and \(\{(X_{a,b}(t))_{t\in[T]}\,:\,a,b\in\mathcal{A}\}\).

Moreover, we suppose that players are rational in hindsight; that is, they minimize their regret. Formally, the upstream player aims to minimize his regret defined as

\[\mathfrak{R}^{\text{up}}_{\text{n}}(T,\Pi^{\text{up}}_{\text{n}})=T\mu^{ \star,\text{up}}-\mathbb{E}\!\left[\sum_{t=1}^{T}v^{\text{up}}(A_{t})\right] \,,\text{ where }\mu^{\star,\text{up}}=\max_{a\in\mathcal{A}}v^{\text{up}}(a)\;,\] (1)

while the downstream player seeks to minimize her external regret defined as

\[\mathfrak{R}^{\text{down}}_{\text{n}}(T,\Pi^{\text{up}}_{\text{n}},\Pi^{ \text{down}}_{\text{n}})=\mathbb{E}\!\left[\sum_{t=1}^{T}\max_{b\in\mathcal{A }}v^{\text{down}}(A_{t},b)-v^{\text{down}}(A_{t},B_{t})\right]\,,\] (2)

where the players' actions \((A_{t})_{t\in[T]}\), \((B_{t})_{t\in[T]}\) as well as their policies \(\Pi^{\text{up}}_{\text{n}}\), \(\Pi^{\text{down}}_{\text{n}}\) are defined below. Note that the utility of the downstream player also depends on the actions taken by the upstream player, which represents the externality exerted by the upstream player on the downstream player, hence the strategic dimension of our setting. We first consider a game where no property right is defined, so each player is free to pick his preferred arm irrespectively of the other player's choice. This will result in a breakdown of the total utility.

**Policies without property rights.** Consider first the upstream player. Based on a policy \(\Pi^{\text{up}}_{\text{n}}\) (for example a no-regret bandit algorithm such as the **Upper Confidence Bounds** algorithm (UCB) [10] or the \(\varepsilon\)-greedy algorithm [11, 12]), we define his history \((\mathcal{H}^{\text{up,n}}_{t})_{t\in[T]}\) by induction. We set \(\mathcal{H}^{\text{up,n}}_{0}=\varnothing\) and supposing that \(\mathcal{H}^{\text{up,n}}_{t}\) is defined for \(t\in[T]\), then

\[\mathcal{H}^{\text{up,n}}_{t+1}=\mathcal{H}^{\text{up,n}}_{t}\cup\left\{A_{t +1},V_{t+1},Z_{A_{t+1}}(t+1)\right\}\,,\]

where \((V_{s})_{s\in\mathbb{N}^{\star}}\) is a family of independent uniform random variables in \([0,1]\), allowing for randomization in the policy, and \(A_{t+1}\) is provided by \(\Pi^{\text{up}}_{\text{n}}\), following \(\Pi^{\text{up}}_{\text{n}}\colon(V_{t+1},\mathcal{H}^{\text{up,n}}_{t})\mapsto A _{t+1}\).

Second, consider the downstream player and an algorithm \(\Pi^{\text{down}}_{\text{n}}\) (specifically a no-regret bandit algorithm). We define her history \((\mathcal{H}^{\text{down,n}}_{t})_{t\in[T]}\) by induction. We set \(\mathcal{H}^{\text{down,n}}_{0}=\varnothing\) and supposing that \(\mathcal{H}^{\text{down,n}}_{t}\) is defined for \(t\in[T]\), then

\[\mathcal{H}^{\text{down,n}}_{t+1}=\mathcal{H}^{\text{down,n}}_{t}\cup\left\{A_ {t+1},B_{t+1},U_{t+1}X_{A_{t+1},B_{t+1}}(t+1)\right\}\,,\]where \((U_{s})_{s\in\mathbb{N}^{*}}\) is a family of independent uniform random variables in \([0,1]\) allowing for randomization in the policy and \(B_{t+1}\) is provided by \(\Pi_{\mathrm{n}}^{\mathrm{down}}\), following \(\Pi_{\mathrm{n}}^{\mathrm{down}}\colon(U_{t+1},\mathcal{H}_{t}^{\mathrm{down}, \mathrm{n}})\mapsto B_{t+1}\).

**Welfare efficiency.** We now introduce the notion of _Welfare efficiency_ for our setup. The global utility, or _social welfare_, of the players at round \(t\) is defined as \(v^{\mathrm{up}}(A_{t})+v^{\mathrm{down}}(A_{t},B_{t})\). We define the _socially optimal action_\((a^{\mathrm{sw}},b^{\mathrm{sw}})\in\mathcal{A}\times\mathcal{A}\) of the game as

\[(a^{\mathrm{sw}},b^{\mathrm{sw}})\in\mathrm{argmax}_{a,b\in\mathcal{A}}\;\;v^ {\mathrm{up}}(a)+v^{\mathrm{down}}(a,b)\;,\] (3)

as well as the _global regret_ (or _social welfare regret_) associated with policies \(\Pi_{\mathrm{n}}^{\mathrm{up}}\) and \(\Pi_{\mathrm{n}}^{\mathrm{down}}\) as

\[\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{ \mathrm{n}}^{\mathrm{down}})=T\left(v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{ \mathrm{down}}(a^{\mathrm{sw}},b^{\mathrm{sw}})\right)-\sum_{t=1}^{T}\mathbb{ E}\big{[}v^{\mathrm{up}}(A_{t})+v^{\mathrm{down}}(A_{t},B_{t})\big{]}\;.\] (4)

Then, the joint policies \(\Pi_{\mathrm{n}}^{\mathrm{up}}\) and \(\Pi_{\mathrm{n}}^{\mathrm{down}}\) for the players are said to be _Welfare efficient_ if

\[\lim_{T\to+\infty}\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^{\mathrm{down}})/T=0\;.\]

Intuitively, this condition implies that the frequency of the socially optimal action \((a^{\mathrm{sw}},b^{\mathrm{sw}})\) tends to 1 as \(T\) goes to infinity. In this sense, it mimics the usual, static Welfare efficiency criterion. As we will see, \((\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^{\mathrm{down}})\) is typically not _Welfare efficient_ when there is a disalignment in the game between the players' individual interests based on their rationality and the social welfare.

### Inefficiency without property rights

We first present a result that captures the adverse consequence of externality on social welfare. The upstream player does not take into account the indirect cost incurred by the downstream player when he chooses his action. This drives the social welfare away from its optimal level. We illustrate this fact within our simple bilateral externality example.

**Example 1** (continuing from p. 1).: _We show that the competitive outcome, where each firm maximizes its profit independently, is not welfare efficient in the presence of externality. Define the social welfare as the function_

\[W:(q_{1},q_{2})\mapsto\pi_{1}(q_{1})+\pi_{2}(q_{1},q_{2})=p(q_{1}+q_{2})-(c_{ 1}(q_{1})+c_{2}(q_{2}))-\alpha q_{1}\;.\] (5)

_By definition, the welfare efficient outcome \((q_{1}^{*},q_{2}^{*})\in\mathbb{R}_{+}^{2}\) satisfies \(W(q_{1}^{*},q_{2}^{*})\geqslant W(q_{1},q_{2})\) for any \((q_{1},q_{2})\in\mathbb{R}_{+}^{2}\). Since \(W\) is differentiable and strictly concave, \((q_{1}^{*},q_{2}^{*})\) is uniquely defined by the condition \(\nabla W(q_{1}^{*},q_{2}^{*})=0\), that is_

\[c_{1}^{\prime}(q_{1})-\alpha=p\quad\text{and}\quad c_{2}^{\prime}(q_{2})=p\;.\] (6)

_Note that at the welfare efficient optimum, firm 1 does not equalize marginal cost with marginal profit, but produces less to account for the negative effect of externality on firm 2. We now characterize the competitive outcome \((q_{1}^{\prime},q_{2}^{\prime})\in\mathbb{R}_{+}^{2}\). Since \(\pi_{1}\) and \(\pi_{2}\) are differentiable and strictly concave, \((q_{1}^{\prime},q_{2}^{\prime})\) satisfies_

\[c_{1}^{\prime}(q_{1}^{\prime})=p\quad\text{and}\quad c_{2}^{\prime}(q_{2}^{ \prime})=p\;.\] (7)

_For the competitive outcome to be welfare efficient, we require, by Equation (6) and Equation (7),_

\[c_{1}^{\prime}(q_{1}^{\prime})-\alpha=c_{1}^{\prime}(q_{1}^{\prime}),\quad \text{that is}\quad\alpha=0\;.\]

_This proves that whenever there are externalities, no competitive outcome is efficient._

We now show that in our model, when there is no property right and under mild assumptions, no achievable policy is welfare efficient whenever there is a misalignment between the players' interests and the social welfare. The upstream player's policy \(\Pi_{\mathrm{n}}^{\mathrm{up}}\) is said to be _no-regret_ if \(\lim_{T\to+\infty}\mathfrak{R}_{\mathrm{n}}^{\mathrm{up}}(T,\Pi_{\mathrm{n}}^ {\mathrm{up}})/T=0\), where \(\mathfrak{R}_{\mathrm{n}}^{\mathrm{up}}\) is defined in (1).

**Theorem 2**.: _Suppose that \(\mathrm{argmax}_{a\in\mathcal{A}}\;v^{\mathrm{up}}(a)\) is the singleton \(\{a_{\star}^{u}\}\) and that_

\[v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{\mathrm{sw}},b^{ \mathrm{sw}})-v^{\mathrm{up}}(a_{\star}^{u})+v^{\mathrm{down}}(a_{\star}^{u},b )>0\;,\] (8)

_for any \(b\in\mathcal{A}\). In the absence of property rights and when the upstream player runs any no-regret policy \(\Pi_{\mathrm{n}}^{\mathrm{up}}\), we have \(\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^ {\mathrm{down}})=\Omega(T)\). Therefore, \(\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^ {\mathrm{down}})=\Omega(T)\) and \((\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^{\mathrm{down}})\) is not welfare efficient._Condition (8) in Theorem 2 represents the unalignment between the upstream player's preference and the optimal choice from a social welfare point of view. Note that the upstream and downstream players can both have an \(o(T)\) external regret, while the social welfare regret still grows linearly with \(T\) because of the unfavorable interactions between their policies.

## 3 Online Property Game with Bargaining Players

### Online Property Game

We now consider the same repeated game in the form of a _property game_ where one of the players possesses the bandit instance (_upstream player_). As in the original setup of Coase (2013), the other player (_downstream player_) will provide the bandit owner with transfers to incentivize him to choose some specific action and influence the outcome of the game in her favor.

We show in Appendix B that our method applies similarly when property rights are given to the upstream player rather than the downstream player. Hence, there is no loss of generality in considering the aforementioned framework. In this sense, we recover the _invariance property_ of the Coasean bargaining (Mas-Colell et al., 1995).

**Example 1** (continuing from p. 1).: _We now illustrate how Coasean bargaining re-instaures efficiency. Suppose without loss of generality that property rights are such that firm 2 can pay \(\tau\in\mathbb{R}_{+}\) to firm 1 for it to operate at a level \(\tilde{q}_{1}\). Profits become_

\[\bar{\pi}_{2}:(q_{1},q_{2},\tau,\tilde{q}_{1})\mapsto\pi_{2}(q_{1},q_{2})- \mathbbm{1}_{\{q_{1}=\tilde{q}_{1}\}}\tau\quad\text{and}\quad\bar{\pi}_{1}:(q _{1},\tau,\tilde{q}_{1})\mapsto\pi(q_{1})+\mathbbm{1}_{\{q_{1}=\tilde{q}_{1}\}}\tau.\]

_Consider the competitive outcome \((q_{1},q_{2},\tau,\tilde{q}_{1})\in\mathbb{R}_{+}^{4}\) which satisfies_

\[q_{1}=q_{1}(\tau,\tilde{q}_{1})\in\arg\max_{q_{1}^{\prime}\geq 0} \bar{\pi}_{1}(q_{1}^{\prime},\tau,\tilde{q}_{1})\quad\text{and}\] \[\bar{\pi}_{2}(q_{1}(\tau,\tilde{q}_{1}),q_{2},\tau,\tilde{q}_{1}) =\max_{q_{2}^{\prime},\tau^{\prime},\tilde{q}_{1}^{\prime}}\bar{\pi}_{2}(q_{ 1}(\tau^{\prime},\tilde{q}_{1}^{\prime}),q_{2}^{\prime},\tau^{\prime},\tilde{ q}_{1}^{\prime}).\]

_The condition on \(q_{1}\) accounts for the rationality of the firm \(1\) and the fact that its choice depends on the payment \((\tau,\tilde{q}_{1})\). Obviously, the optimal solution is reached for \(\tilde{q}_{1}=q_{1}\) and \(\tau=\max_{q^{\prime}}\pi_{1}(q^{\prime})-\pi_{1}(\tilde{q}_{1})\). Plugging this back in the expression of \(\bar{\pi}_{2}\) then yields_

\[(q_{1},q_{2})=\operatorname{argmax}_{q_{1}^{\prime},q_{2}^{\prime}}\pi_{1}(q _{1}^{\prime})+\pi_{2}(q_{2}^{\prime})=\operatorname{argmax}_{q_{1}^{\prime},q_{2}^{\prime}}W(q_{1}^{\prime},q_{2}^{\prime})\,\]

_so the competitive outcome \((q_{1},q_{2})\) is welfare efficient._

The transfers at each step can be interpreted as a contract between two players (see, e.g., Bolton and Dewatripont, 2004; Salanie, 2005, for general contract theory) and providing the right amount of incentives relates to adjusting a contract in an online setting (see Dutting et al., 2019; Guruganesh et al., 2021; Zhu et al., 2022; Fallah and Jordan, 2023; Guruganesh et al., 2024; Ananthakrishnan et al., 2024, for learning-based perspectives about contracts).

Similarly to Example 1, we modify the players' policies to now account for the transfer \(\tau(t)\) that the downstream player offers at round \(t\) to the upstream player if he picks action \(\tilde{a}_{t}\). The downstream player's policy at round \(t\) does not only output an arm \(B_{t}\) but now a triple \((\tilde{a}_{t},\tau(t),B_{t})\), where \(B_{t}\) is the arm that she should play and \(\tilde{a}_{t}\) is the arm on which a transfer \(\tau(t)\) is offered to the upstream player. On the upstream player's side, the policy still outputs an arm \(A_{t}\) to play but also takes as an input the incentive \((\tilde{a}_{t},\tau(t))\). In addition, the instantaneous utility of the upstream player becomes \(Z_{A_{t}}(t)+\mathbbm{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\), whereas the downstream player receives \(X_{A_{t},B_{t}}(t)-\mathbbm{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\).

**Policies with property rights.** Based on policies \(\Pi_{\mathrm{p}}^{\mathrm{up}}\) for the upstream player and \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) for the downstream player, we define their histories \((\mathcal{H}_{t}^{\mathrm{up,p}})_{t\in[T]}\) and \((\mathcal{H}_{t}^{\mathrm{down,p}})_{t\in[T]}\) by induction. We set \(\mathcal{H}_{0}^{\mathrm{up,p}}=\varnothing\), \(\mathcal{H}_{0}^{\mathrm{down,p}}=\varnothing\) and supposing that \(\mathcal{H}_{t}^{\mathrm{up,p}}\), \(\mathcal{H}_{t}^{\mathrm{down,p}}\) are defined for \(t\in[T]\), then

\[\mathcal{H}_{t+1}^{\mathrm{up,p}}=\mathcal{H}_{t}^{\mathrm{up,p}}\cup\left\{ \tilde{a}_{t+1},\tau(t+1),A_{t+1},V_{t+1},Z_{A_{t+1}}(t+1)\right\}\]

and

\[\mathcal{H}_{t+1}^{\mathrm{down,p}}=\mathcal{H}_{t}^{\mathrm{down,p}}\cup \left\{\tilde{a}_{t+1},\tau(t+1),A_{t+1},B_{t+1},U_{t+1},X_{A_{t+1},B_{t+1}}(t+1 )\right\}\,,\]where \((V_{s})_{s\in\mathbb{N}^{*}}\), \((U_{s})_{s\in\mathbb{N}^{*}}\) are two families of independent uniform random variables in \([0,1]\) allowing for randomization in the policies, and the remaining quantities are given by \(\Pi_{\mathrm{p}}^{\mathrm{down}}\colon(U_{t+1},\mathcal{H}_{t}^{\mathrm{down},\mathrm{p}})\mapsto(\tilde{a}_{t+1},\tau(t+1),B_{t+1})\) and \(\Pi_{\mathrm{p}}^{\mathrm{up}}\colon(\tilde{a}_{t+1},\tau(t+1),V_{t+1}, \mathcal{H}_{t}^{\mathrm{up},\mathrm{p}})\mapsto A_{t+1}\).

**Players' goal.** Given a transfer \(\tau\) from the downstream to the upstream player on arm \(\tilde{a}\), actions \(a\) and \(b\) respectively are chosen by the upstream and the downstream player, the upstream player's expected utility reads \(v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{a}}(a)\tau\) while the downstream player's expected utility is \(v^{\mathrm{down}}(a,b)-\mathbb{1}_{\tilde{a}}(a)\tau\). This defines the upstream player's expected regret for a horizon \(T\) as

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{up}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi_ {\mathrm{p}}^{\mathrm{down}})=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\max_{a\in \mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{a}_{t}}(a)\tau(t)\}-(v^ {\mathrm{up}}(A_{t})+\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t))\Bigg{]}\;.\] (9)

Based on the upstream player's utility, the downstream player aims on a single round at proposing an optimal transfer \(\tau^{\mathrm{opt}}\) on an arm \(a^{\mathrm{opt}}\in\mathcal{A}\) as well as picking an arm \(b^{\mathrm{opt}}\in\mathcal{A}\) which solves

\[\text{maximize }(a,b,\tau)\mapsto v^{\mathrm{down}}(a,b)-\tau\] (10) \[\text{such that }\tau\in\mathbb{R}_{+},b\in\mathcal{A},a\in \operatorname{argmax}_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime}) +\mathbb{1}_{a}(a^{\prime})\tau\}\;.\]

Her regret for any horizon \(T\) is defined as

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi _{\mathrm{p}}^{\mathrm{down}})=T\mu^{*,\mathrm{down}}-\mathbb{E}\Bigg{[}\sum_ {t=1}^{T}v^{\mathrm{down}}(A_{t},B_{t})-\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau (t)\Bigg{]}\;,\] (11)

where we define \(\mu^{*,\mathrm{down}}=v^{\mathrm{down}}(a^{\mathrm{opt}},b^{\mathrm{opt}})- \tau^{\mathrm{opt}}\) as the optimal utility she can aim for. We can see that the downstream player's influence is exerted through her action choice \(B_{t}\) as well as through transfers which enable her to influence the upstream player's actions. Hence, the notion of external regret is obsolete here. The game has now the form of a repeated _Stackelberg game_[20].

**Lemma 1**.: _Recall that \(\mu^{*,\mathrm{down}}\) is the downstream player's optimal reward as defined as a solution of (10). We have \(\mu^{*,\mathrm{down}}=\max_{a,b\in\mathcal{A}}\{v^{\mathrm{down}}(a,b)+v^{ \mathrm{up}}(a)\}-\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})\}\), as well as \((a^{\mathrm{opt}},b^{\mathrm{opt}})=(a^{\mathrm{sw}},b^{\mathrm{sw}})\) and \(\mu^{*,\mathrm{up}}+\mu^{*,\mathrm{down}}=v^{\mathrm{up}}(a^{\mathrm{sw}})+v^ {\mathrm{down}}(a^{\mathrm{sw}},b^{\mathrm{sw}})=\max_{a,b\in\mathcal{A}}\{v^ {\mathrm{up}}(a)+v^{\mathrm{down}}(a,b)\}\), where \(\mu^{*,\mathrm{up}}\) is defined in Equation (1). Moreover, for any integer \(T\in\mathbb{N}^{*}\), and policies \(\Pi_{\mathrm{p}}^{\mathrm{up}}\), \(\Pi_{\mathrm{p}}^{\mathrm{down}}\), we have that_

\[\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi_{\mathrm{p}}^ {\mathrm{down}})\leq\mathfrak{R}_{\mathrm{p}}^{\mathrm{up}}(T,\Pi_{\mathrm{p }}^{\mathrm{up}},\Pi_{\mathrm{p}}^{\mathrm{down}})+\mathfrak{R}_{\mathrm{p}}^ {\mathrm{down}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi_{\mathrm{p}}^{\mathrm{ down}})\;.\]

This lemma has an interesting economic interpretation: if both players individually seek for their own interest within this online property game, they will together converge towards the optimal global utility. Individual rationality moves the outcome of the game towards the optimal social welfare. The transfers allow the players to align their goals and share the global reward, in line with the Coase theorem. Consequently, if both players run no-regret policies \(\Pi_{\mathrm{p}}^{\mathrm{up}}\) and \(\Pi_{\mathrm{p}}^{\mathrm{down}}\), the social welfare regret will also be in \(o(T)\). The rest of the paper shows that such no-regret policies exist. To this end, we introduce the following assumptions.

Without loss of generality, we assume that the upstream player's utility is rescaled and shifted, which corresponds to the following assumption on the reward distribution \((\gamma_{a})_{a\in\mathcal{A}}\) in \(\mathbb{R}\).

**H1**.: _For any \(a\in\mathcal{A}\), we have \(v^{\mathrm{up}}(a)\in[0,1]\)._

We now make a high probability bound assumption on the upstream player's regret.2.

Footnote 2: A similar assumption is made in the work of Donahue et al. [2024] but with a stronger instantaneous regret bound which does not encompass the UCB’s regret bound.

**H2**.: _There exist \(\mathrm{C},\zeta>0,\kappa\in[0,1)\) such that for any \(s,t\in[T]\) with \(s+t\leq T\), any \(\{\tau_{a}\}_{a\in[K]}\in\mathbb{R}_{+}^{K}\) and any policy \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) that offers almost surely a transfer \((\tilde{a}_{l},\tau(l))=(\tilde{a}_{l},\tau_{\tilde{a}_{l}})\) for any \(l\in\{s+1,\ldots,s+t\}\), the batched regret of the upstream player following \(\Pi_{\mathrm{p}}^{\mathrm{up}}\) satisfies, with probability at least \(1-t^{-\zeta}\),_

\[\sum_{l=s+1}^{s+t}\max_{a\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{ a}_{l}}(a)\tau_{\tilde{a}_{l}}\}-(v^{\mathrm{up}}(A_{l})+\mathbb{1}_{\tilde{a}_{l}}(A_{l}) \tau_{\tilde{a}_{l}})\leq\mathrm{C}t^{\kappa}\;.\]The constraint on the downstream player's algorithm \(\Pi^{\rm down}_{\rm p}\) enforces constant incentives associated with any arm \(a\in\mathcal{A}\) within the batch, while the incentivized actions \((\tilde{a}_{l})_{l\in\{s+1,\ldots,s+t\}}\) may change. Proposition 2 in Appendix C shows that an adaptation of UCB taking account the incentives satisfies \(\mathbf{H2}\) with \(\mathrm{C}=8\sqrt{K\log(KT^{3})}\), \(\kappa=1/2\) and \(\zeta=2\). Note that usual bandit algorithms such as AAE, ETC or EXP-IX also satisfy the assumption (see, e.g., Donahue et al., 2024, Lattimore and Szepesvari, 2020).

### Downstream player's procedure

We fix the policy \(\Pi^{\rm up}_{\rm p}\) which can be any algorithm satisfying \(\mathbf{H2}\) for the upstream player and introduce the algorithm BELGIC (Bandits and Externalities for a Learning Game with Incentivized Coase) which provides a policy achieving sub-linear regret for the downstream player. It can be seen as an online bargaining strategy to mitigate externalities. Simply put, BELGIC unfolds in two steps. First note that for any action \(a\in\mathcal{A}\), the optimal (lowest) transfer to offer to the upstream player to make him choose \(a\) is

\[\tau^{\star}_{a}=\max_{a^{\prime}\in\mathcal{A}}v^{\rm up}(a^{\prime})-v^{\rm up }(a)\,\] (12)

as detailed in Appendix A. Therefore, a batched binary search procedure (Algorithm 2) first allows the downstream player to estimate the optimal transfers \(\tau^{\star}_{1},\ldots,\tau^{\star}_{K}\) with a good precision level of \(1/T^{\beta}\), where \(\beta>0\). More precisely, the downstream player offers a constant incentive \((\tilde{a},\tau_{\tilde{a}})\) for a batch of time steps of length \(\tilde{T}=\lceil T^{\alpha}\rceil\). The observation of \(T^{\neq}_{\tilde{a}}\), the number of steps from the batch for which the upstream player does not pick \(\tilde{a}\) allows her to estimate whether \(\tau_{\tilde{a}}\) is above or below \(\tau^{\star}_{\tilde{a}}\) and adjust it, following Lemma 2 in Appendix A under the condition that \(\alpha,\beta\) satisfy

\[\beta/\alpha<(1-\kappa)\.\] (13)

The procedure needs to be run for \(K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil\) rounds since we have to make \(\lceil\log_{2}T^{\beta}\rceil\) batches of binary search of length \(\lceil T^{\alpha}\rceil\) on each of the \(K\) arms (see Scheid et al., 2024). This corresponds to the first phase of BELGIC as described in Algorithm 2. At the end of this stage, the estimated transfers \((\hat{\tau}_{a})_{a\in\mathcal{A}}\) satisfy the bound in Proposition 1. These are then used to feed the subroutine Bandit-Alg.

**Proposition 1**.: _Under **H1** and **H2**, after the first phase of BELGIC which consists in \(K\lceil T^{\alpha}\rceil\lceil\log T^{\beta}\rceil\) steps of binary search grouped in \(\lceil\log_{2}T^{\beta}\rceil\) batches per arm \(a\in\mathcal{A}\), we have that_

\[\mathbb{P}\Big{(}\text{ for any }a\in\mathcal{A},\hat{\tau}_{a}-4/T^{\beta}- \mathrm{CT}^{(\kappa-1)/2}\leqslant\tau^{\star}_{a}\leqslant\hat{\tau}_{a} \Big{)}\geqslant 1-K\lceil\log_{2}T^{\beta}\rceil/T^{\alpha\zeta}\.\]

The additional term \(1/T^{\beta}+\mathrm{CT}^{\kappa-1}\) in \(\hat{\tau}_{a}\) ensures that if \(\mathbf{H2}\) holds, the upstream player necessarily plays the incentivized action \(\tilde{a}_{t}\) at round \(t\) with high probability. Lemmas 2 and 6 from Appendix C show how the binary search batches in BELGIC allow us to estimate \(\tau^{\star}_{a}\) depending on \(\tilde{T}-T^{\neq}_{a}\), the number of times that arm \(a\) has been pulled by the upstream player during the batch.

Then, any bandit subroutine Bandit-Alg, such as UCB or \(\varepsilon\)-greedy, for instance, can be run in a black-box fashion on the shifted bandit instance, where the rewards are shifted by the upper estimated transfers \((\hat{\tau}_{a})_{a\in\mathcal{A}}\). The downstream player computes a shifted history \(\tilde{\mathcal{H}}^{\rm down,p}_{t}\) such that for any \(t\leqslant K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil\), \(\tilde{\mathcal{H}}^{\rm down,p}_{t}=\varnothing\) and for any \(t>K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil\)

\[\tilde{\mathcal{H}}^{\rm down,p}_{t}=\begin{cases}&\{\tilde{a}_{t},B_{t},\tau( t),U_{t},X_{\tilde{a}_{t},B_{t}}(t)-\hat{\tau}_{\tilde{a}_{t}}\}\cup\tilde{ \mathcal{H}}^{\rm down,p}_{t-1}\text{ if }\tilde{a}_{t}=A_{t}\\ &\tilde{\mathcal{H}}^{\rm down,p}_{t-1}\quad\text{ otherwise },\end{cases}\] (14)

which serves to feed Bandit-Alg, following

\[\texttt{Bandit-Alg}\colon(U_{t},\tilde{\mathcal{H}}^{\rm down,p}_{t-1})\mapsto( \tilde{a}_{t},B_{t})\in\mathcal{A}\times\mathcal{A}\.\] (15)

For any family of constant incentives \(\{\tau_{a}\}_{a\in\mathcal{A}}\in\mathbb{R}^{K}_{+}\), we define \(\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\tau_{a}\}_{a\in\mathcal{A}})\) as the regret for the downstream player's subroutine Bandit-Alg on the bandit instance with shifted means over \(T\) rounds, following

\[\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\tau_{a}\}_{a\in\mathcal{A}})=T\max_ {a,b\in\mathcal{A}^{2}}\mathbb{E}\big{[}v^{\rm down}_{a,b}(1)-\tau_{a}\big{]}- \mathbb{E}\Bigg{[}\sum_{t=1}^{T}v^{\rm down}(\tilde{a}_{t},B_{t})-\tau_{ \tilde{a}_{t}}\Bigg{]}\.\]

Note that here, Bandit-Alg aims to maximize the shifted reward \((v^{\rm down}(a,b)-\tau_{a})_{(a,b)\in\mathcal{A}^{2}}\).

```
1:Input: Set of actions \(\mathcal{A}=[K]\), time horizon \(T\), subroutine \(\Pi_{\mathrm{p}}^{\mathrm{up}}\), upstream player's regret constants \(\mathrm{C},\kappa\), parameters \(\alpha\) and \(\beta\).
2: Compute \(\tilde{\mathcal{H}}_{s}^{\mathrm{down},p}=\varnothing\) for any \(s\leqslant K\lceil\log_{2}T^{\beta}\rceil\lceil T^{\alpha}\rceil\).
3:for\(a\in\mathcal{A}\)do
4:# See Algorithm 2
5:\(\underline{\tau}_{a},\overline{\tau}_{a}=\mathtt{Binary\ Search}(a,\lceil\log_{2}T^{\beta} \rceil,\lceil T^{\alpha}\rceil,0,1)\)
6:endfor
7: For any action \(a\in\mathcal{A}\), \(\hat{\tau}_{a}=\overline{\tau}_{a}+1/T^{\beta}+\mathrm{C}T^{(\kappa-1)/2}\).
8:for\(t=K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil+1,\ldots,T\)do
9: Get recommended actions by \(\mathtt{Bandit-Alg}\) on the \(\mathcal{A}\times\mathcal{A}\) bandit instance, \((\tilde{a}_{t},B_{t})=\mathtt{Bandit-Alg}(U_{t},\tilde{\mathcal{H}}_{t-1}^{ \mathrm{down,p}})\).
10: Offer a transfer \(\hat{\tau}_{\tilde{a}_{t}}\) on action \(\tilde{a}_{t}\), nothing for any other action \(a^{\prime}\in\mathcal{A}\) and play action \(B_{t}\).
11: Observe \(A_{t}=\Pi_{\mathrm{p}}^{\mathrm{up}}(\hat{\tau}_{t+1},\tau(t+1),V_{t},\tilde{ \mathcal{H}}_{t-1}^{\mathrm{up,p}}),X_{\tilde{a}_{t},B_{t}}(t)\)
12:if\(A_{t}=\tilde{a}_{t}\)then update history \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}\).
13:endif
14: Update upstream player's history \(\mathcal{H}_{t}^{\mathrm{up,p}}\).
15:endfor ```

**Algorithm 2**Binary Search Subroutine

```
1:Input: action \(a,N_{T},\tilde{T},\underline{\tau}_{a},\overline{\tau}_{a}\).
2:for\(d=0,\ldots,N_{T}-1\)do
3: Compute \(\tau_{a}^{\mathrm{mid}}=(\overline{\tau}_{a}(d)+\underline{\tau}_{a}(d))/2,T_{a }^{\neq}=0\).
4:for\(t=d\tilde{T}+1,\ldots,d\tilde{T}+\tilde{T}\)do
5: Propose transfer \(\tau_{a}^{\mathrm{mid}}(d)\) on arm \(a\) and nothing for any other action \(a^{\prime}\in\mathcal{A}\).
6:\(A_{t}=\Pi_{\mathrm{p}}^{\mathrm{up}}(t,\tau^{\mathrm{mid}}(a),a,V_{t}, \mathcal{H}_{t-1}^{\mathrm{up,p}})\)
7:if\(A_{t}\neq a\)then:\(T^{\neq}+1\)
8:endif
9: Update upstream player's history \(\mathcal{H}_{t}^{\mathrm{up,p}}\).
10:endfor
11:if\(\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}<T_{a}^{\neq}<\tilde{T}-\mathrm{C} \tilde{T}^{\kappa+\beta/\alpha}\)then Return \(\underline{\tau}_{a}(d),\overline{\tau}_{a}(d)\).
12:elseif\(T_{a}^{\neq}\leqslant\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\)then\(\overline{\tau}_{a}(d)=\tau_{a}^{\mathrm{mid}}(d)+1/T^{\beta}\) and update history \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}\).
13:else\(\underline{\tau}_{a}(d)=\tau_{a}^{\mathrm{mid}}(d)-1/T^{\beta}\) and update history \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}\).
14:endif
15:endfor ```

**Algorithm 3**Binary Search Subroutine

**Theorem 3**.: _Assume that **H**1 and **H**2 hold. Then \(\mathtt{BELGIC}\), run with \(\alpha,\beta\) satisfying (13) and any bandit subroutine \(\mathtt{Bandit-Alg}\), has an overall regret \(\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}\) such that_

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\Pi_{\mathrm{p}}^{ \mathrm{up}},\mathtt{BELGIC}) \leqslant 2(3+2\mathrm{C}+\bar{v}-\underline{v})\log_{2}(T)(2T^{1- \alpha\zeta}+T^{(\kappa+1)/2}+\lceil T^{\alpha}\rceil)+4T^{1-\beta}\] \[\quad+\mathfrak{R}_{\mathtt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a}\}_ {a\in\mathcal{A}})\]

_where, for ease of notation_

\[\bar{v}=\max_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{\mathrm{down}}(a,b)\} \quad\text{ and }\quad\underline{v}=\min_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{ \mathrm{down}}(a,b)\}\;.\]

**Knowledge of \(\mathrm{C}\) and \(\kappa\).** An upper bound on \(\mathrm{C}\) and \(\kappa\) is sufficient to compute the hyperparameters in \(\mathtt{BELGIC}\). Theorem 3 shows that the bigger \(\mathrm{C}\) and \(\kappa\) are, the worse is the downstream player's regret, hence the interest of knowing them more precisely.

**Corollary 1**.: _Assume that the upstream player's distribution \((\gamma_{a})_{a\in\mathcal{A}}\) is such that **H**1 holds. In addition, suppose that the distributions \((\gamma_{a})_{a\in\mathcal{A}}\) and \((\nu_{a,b})_{a,b\in\mathcal{A}\times\mathcal{A}}\) are \(1\)-sub-Gaussian and that the upstream player plays \(\Pi_{\mathrm{p}}^{\mathrm{up}}=Algorithm\ 3\) (a slight modification of \(\mathtt{UCB}\) to take into account the incentives). Then the downstream player's regret when she runs \(\mathtt{BELGIC}\) with parameters \(\alpha=3/4\) and \(\beta=1/4\) (which satisfy (13)) and subroutine \(\mathtt{Bandit-Alg}=\mathtt{UCB}\) satisfies the following upper bound3_

Footnote 3: Note that it is \(K\) and not \(\sqrt{K}\) here, since the action space is of cardinality \(K^{2}\) for the downstream player.

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\mathit{UCB},\mathtt{ BELGIC}) \leqslant(10+4K+32\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v}) \log_{2}(T)(3+2T^{3/4})\] \[\quad+3K^{2}(\bar{v}-\underline{v})\.\]

The upper bound on the social welfare regret in Lemma 1 together with Corollary 1 shows that when the upstream player runs \(\Pi_{\mathrm{p}}^{\mathrm{up}}=\mathtt{UCB}\) and the downstream player runs \(\mathtt{BELGIC}\), the social welfare regret then satisfies \(\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\mathtt{BELGIC})= \mathcal{O}(K\log(T)^{T(\kappa+1)/2})\).

In other words, if the downstream player runs \(\mathtt{BELGIC}\) which produces a policy \(\Pi_{\mathrm{down}}^{\mathrm{n}}\), for any upstream policy \(\Pi_{\mathrm{p}}^{\mathrm{up}}\), \((\Pi_{\mathrm{down}}^{\mathrm{n}},\Pi_{\mathrm{p}}^{\mathrm{up}})\) is welfare efficient.

**Influence of the upstream performance.** It is interesting to note that in the downstream player's regret bound, the upstream player's regret bound in \(\mathcal{O}(T^{\kappa})\) plays a significant role: the downstream player never learns faster than the upstream player. The latter's performance determines the social welfare convergence rate towards the social optimum. We can observe that the players' bounded rationality (Selten, 1990; Jones, 1999) and personal interest make the game converge towards the optimal social welfare equilibrium--even though they are both learning here.

**Experiments.** We conclude this section with experiments showing the empirical convergence of our algorithm to a social optimum. In the simulation, we consider two firms, with firm 1 being upstream and firm 2 being downstream. Their profit functions are respectively given by

\[\pi_{1}\mathrel{\mathop{:}}\mapsto\max\biggl{\{}q_{1}-2\Bigl{(}\frac{q_{1}}{1 0}\Bigr{)}^{2}+2\frac{q_{1}}{10},0\biggr{\}}\,\]

and

\[\pi_{2}(q_{1},q_{2})\mapsto\max\Biggl{\{}-16\biggl{(}\frac{q_{1}}{10}-\frac{6 }{10}\biggr{)}^{2}+8\biggl{(}q_{1}-\frac{6}{10}\biggr{)}^{2}+\frac{1}{50}q_{2},0\Biggr{\}}\.\]

Thus, firm 1's and firm 2's profit functions depends quadratically on \(q_{1}\) with an firm \(1\) optimum at \(q_{1}^{\prime}=5\) and a social optimum at \(q_{1}^{*}=8\). Note that in the expression of \(\pi_{2}\), \(q_{2}\) has very little influence as compared to \(q_{1}\) - which allows to plot profits for only one value of \(q_{2}\).

We discretize the setup, consider a bandit instance (horizon \(T=5.10^{6}\), \(10\) arms, average over \(10\) rounds) and we assume that \(\mathtt{UCB}\) is used as a subroutine. In the first setting, there are no property rights and each firm runs \(\mathtt{UCB}\) on their side. Second, property rights are defined and firm 2 runs \(\mathtt{BELGIC}\) as its policy. The plots in Figure 1 display the empirical frequencies and show empirically the effectiveness of \(\mathtt{BELGIC}\) to mitigate externalities.

## 4 Related work

Our work addresses the impact of externalities and is therefore related to taxation theory (see, e.g., Mirflees et al., 2011; Salanie, 2011, and the references therein), a prominent solution for this issue, as

Figure 1: Empirical frequencies of the upstream player’s actions when property rights are not defined (left) and when they are defined (right).

exemplified by the _Pigouvian tax_(see Pigou, 2017). Taxation is a fundamental aspect of all developed economies, with \(30\%\) to \(50\%\) of national income derived from taxes. The topic has been fruitful for various scenarios, including the carbon tax (Carattini et al., 2018; Metcalf and Weisbach, 2009), alcohol markets (Griffith et al., 2019), or business taxation (Boadway and Bruce, 1984). Taxation can also be studied through an operations research lens, where it is used to enhance system efficiency or manage specific games (Rougharden, 2010; Caragiannis et al., 2010; Bilo and Vinci, 2019). Recent work by Cui et al. (2024) explores online mechanisms to maximize efficiency in congestion games.

Mechanism designs (Myerson, 1989; Nisan and Ronen, 1999; Laffont and Martimort, 2009) allow to design games that have specific desired outcomes. Deploying these mechanisms in their classical economical form assume that players' utility functions are known a priori, which is often unrealistic. There is a major need to blend mechanism design with machine learning.

However, our approach differs, since, drawing inspiration from Coase's theory, we implement an online version of his theorem (Coase, 2013; Cooter, 1982), incorporating uncertainty to tackle the breakdown of social welfare in an online setting. We use the bandit setup (see Lattimore and Szepesvari, 2020; Slivkins et al., 2019) as a general and convenient way to model the game introduced by Coase. However, our work differs from considering a single agent playing a bandit game. Instead, we focus on the more general problem of multi-players bandits, a field receiving a growing attention from the community (see e.g., Boursier and Perchet, 2019, 2022; Sankararaman et al., 2019).

Our approach is inspired by the principal-agent model introduced by Dogan et al. (2023), which was further extended by Dogan et al. (2023), Scheid et al. (2024). However, unlike the models proposed in the work of Dogan et al. (2023, 2023), we do not specify a particular bandit algorithm for the upstream player and instead, we allow him to use any no-regret algorithm satisfying \(\mathbf{H2}\) in a black-box fashion. Conversely, the model of Scheid et al. (2024) assumes that the upstream player is always fully informed and best-responding, whereas we assume that he is also learning. Chen et al. (2023) leverages a similar model to study information acquisition by a principal through an agent's actions. However, in their model, the agent is also almighty and knows exactly the costs associated with each action. Designing incentives in an unknown environment is related to auction theory incorporating uncertainty, as it is explored in the work of Feng et al. (see 2018), Li et al. (see 2023). Similar issues have been explored in the _Reinforcement Learning_ framework within a leader-follower game (see Chen et al., 2023, with quantal responses by the follower) or in a principal-agent game with incentive design as done by Ben-Porat et al. (2023). Donahue et al. (2024) also study a two-players repeated Stackelberg game on a bandit instance but instead of allowing for transfers, their main focus concerns the achievability of a _Stackelberg equilibrium_ through iterations of bandit policies: the same kind of goal also appears in Collina et al. (2023). Such principal-agent setups are of some interest to model various real-world situations such as the design of fundings for hospitals (Wang et al., 2024) or have been studied with multiple agents through the lens of auction design in dynamic setups (Bergemann and Said, 2010; Chen et al., 2023), or to account for fairness (Fallah et al., 2024).

In our game, the downstream player needs to learn the optimal transfers/incentives to offer to the upstream player. This is related to the _Incentivized Exploration_ literature (Mansour et al., 2016; Simchowitz and Slivkins, 2023; Esmaeili et al., 2023), which is often cast in terms of a benevolent planner who aims to optimize the global welfare of agents via plausible recommendations. A related model is _Bayesian Persuasion_(Kamenica and Gentzkow, 2011), where a sender influences a receiver's action through sending a signal. This model has begun to be studied in learning settings (see, e.g., Castiglioni et al., 2020; Bernasconi et al., 2022; Wu et al., 2022, 2022).

## 5 Conclusion

This paper studies a model of externalities in a two-players sequential game where both players learn their optimal actions. We first show that when the players act independently, then a misalignment between the players' interests and the social welfare leads to a breakdown of the global utility. We then introduce interactions through transfers, which restores a social welfare optimum, representing the online version of the _Coase theorem_. To that purpose, we propose a policy for the downstream player which allows her to estimate the optimal transfers as well as choosing the best actions. The mathematical difficulty comes from the learning aspect on both sides. Since our work is coined in a learning framework for mechanism design, several directions for research are open, as for instance extensions to the multi-agent setting, which raises many questions.

## Acknowledgements

Funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

## References

* A. Anandakrishnan, S. Bates, M. Jordan, and N. Haghtalab (2024)Delegating data collection in decentralized machine learning. In International Conference on Artificial Intelligence and Statistics, pp. 478-486. Cited by: SS1.
* P. Auer (2002)Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research3 (Nov), pp. 397-422. Cited by: SS1.
* O. Ben-Porat, Y. Mansour, M. Moshkovitz, and B. Taitler (2023)Principal-agent reward shaping in mdps. arXiv preprint arXiv:2401.00298. Cited by: SS1.
* D. Bergemann and M. Said (2010)Dynamic auctions: a survey. Wiley Encyclopedia of Operations Research and Management Science. Cited by: SS1.
* M. Bernasconi, M. Castiglioni, A. Marchesi, N. Gatti, and F. Trovo (2022)Sequential information design: learning to persuade in the dark. Advances in Neural Information Processing Systems35, pp. 15917-15928. Cited by: SS1.
* V. Bilo and C. Vinci (2019)Dynamic taxes for polynomial congestion games. ACM Transactions on Economics and Computation (TEAC)7 (3), pp. 1-36. Cited by: SS1.
* R. Boadway and N. Bruce (1984)A general proposition on the design of a neutral business tax. Journal of Public Economics24 (2), pp. 231-239. Cited by: SS1.
* P. Bolton and M. Dewatripont (2004)Contract theory. MIT press. Cited by: SS1.
* E. Boursier and V. Perchet (2022)SIC-mmab: synchronisation involves communication in multiplayer multi-armed bandits. Advances in Neural Information Processing Systems32. Cited by: SS1.
* E. Boursier and V. Perchet (2022)A survey on multi-player bandits. arXiv preprint arXiv:2211.16275. Cited by: SS1.
* S. Bubeck, N. Cesa-Bianchi, et al. (2012)Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends(r) in Machine Learning5 (1), pp. 1-122. Cited by: SS1.
* J. M. Buchanan and W. C. Stubblebine (2006)Externality. In Inframarginal Contributions to Development Economics, pp. 55-73. Cited by: SS1.
* I. Caragiannis, C. Kaklamanis, and P. Kanellopoulos (2010)Taxes for linear atomic congestion games. ACM Transactions on Algorithms (TALG)7 (1), pp. 1-31. Cited by: SS1.
* S. Carattini, M. Carvalho, and S. Fankhauser (2018)Overcoming public resistance to carbon taxes. Wiley Interdisciplinary Reviews: Climate Change9 (5), pp. e531. Cited by: SS1.
* M. Castiglioni, A. Celli, A. Marchesi, and N. Gatti (2020)Online bayesian persuasion. Advances in Neural Information Processing Systems33, pp. 16188-16198. Cited by: SS1.
* S. Chen, M. Wang, and Z. Yang (2023)Actions speak what you want: provably sample-efficient reinforcement learning of the quantal stackelberg equilibrium from strategic feedbacks. arXiv preprint arXiv:2307.14085. Cited by: SS1.

Siyu Chen, Jibang Wu, Yifan Wu, and Zhuoran Yang. Learning to incentivize information acquisition: Proper scoring rules meet principal-agent model. In _International Conference on Machine Learning_, pages 5194-5218. PMLR, 2023b.
* Chen et al. (2023c) Yurong Chen, Qian Wang, Zhijian Duan, Haoran Sun, Zhaohua Chen, Xiang Yan, and Xiaotie Deng. Coordinated dynamic bidding in repeated second-price auctions with budgets. In _International Conference on Machine Learning_, pages 5052-5086. PMLR, 2023c.
* 877, 2013. URL https://EconPapers.repec.org/RePEc:ucp:jlawec:doi:10.1086/674872.
* Collina et al. (2023) Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies for finitely repeated games. In _Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems_, pages 643-651, 2023.
* Cooter (1982) Robert Cooter. The cost of coase. _The Journal of Legal Studies_, 11(1):1-33, 1982.
* Cui et al. (2024) Qiwen Cui, Maryam Fazel, and Simon S Du. Learning optimal tax design in nonatomic congestion games. _arXiv preprint arXiv:2402.07437_, 2024.
* Dahlman (1979) Carl J Dahlman. The problem of externality. _The journal of law and economics_, 22(1):141-162, 1979.
* Dogan et al. (2023a) Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Estimating and incentivizing imperfect-knowledge agents with hidden rewards. _arXiv preprint arXiv:2308.06717_, 2023a.
* Dogan et al. (2023b) Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Repeated principal-agent games with unobserved agent rewards and perfect-knowledge agents. _arXiv preprint arXiv:2304.07407_, 2023b.
* Donahue et al. (2024) Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and Aleksandrs Slivkins. Impact of decentralized learning on player utilities in stackelberg games. _arXiv preprint arXiv:2403.00188_, 2024.
* Dutting et al. (2019) Paul Dutting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 369-387, 2019.
* Esmaeili et al. (2023) Seyed A Esmaeili, Suho Shin, and Aleksandrs Slivkins. Robust and performance incentivizing algorithms for multi-armed bandits with strategic agents. _arXiv preprint arXiv:2312.07929_, 2023.
* Fallah and Jordan (2023) Alireza Fallah and Michael I Jordan. Contract design with safety inspections. _arXiv preprint arXiv:2311.02537_, 2023.
* Fallah et al. (2024) Alireza Fallah, Michael I Jordan, and Annie Ulichney. Fair allocation in dynamic mechanism design. _arXiv preprint arXiv:2406.00147_, 2024.
* Feng et al. (2018) Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. Learning to bid without knowing your value. In _Proceedings of the 2018 ACM Conference on Economics and Computation_, pages 505-522, 2018.
* Greenfield et al. (2009) Thomas K Greenfield, Yu Ye, William Kerr, Jason Bond, Jurgen Rehm, and Norman Giesbrecht. Externalities from alcohol consumption in the 2005 us national alcohol survey: implications for policy. _International journal of environmental research and public health_, 6(12):3205-3224, 2009.
* Griffith et al. (2019) Rachel Griffith, Martin O'Connell, and Kate Smith. Tax design in the alcohol market. _Journal of public economics_, 172:20-35, 2019.
* Guruganesh et al. (2021) Guru Guruganesh, Jon Schneider, and Joshua R Wang. Contracts under moral hazard and adverse selection. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 563-582, 2021.
* Guruganesh et al. (2024) Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning agent. _arXiv preprint arXiv:2401.16198_, 2024.
* Jones (1999) Bryan D Jones. Bounded rationality. _Annual review of political science_, 2(1):297-321, 1999.
* Jones et al. (2019)* Kamenica and Gentzkow (2011) Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. _American Economic Review_, 101(6):2590-2615, 2011.
* Laffont and Martimort (2009) Jean-Jacques Laffont and David Martimort. The theory of incentives: the principal-agent model. In _The theory of incentives_. Princeton university press, 2009.
* Langford and Zhang (2007) John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. _Advances in neural information processing systems_, 20, 2007.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Li et al. (2023) Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng, and Xiaotie Deng. Learning-based ad auction design with externalities: the framework and a matching-based approach. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1291-1302, 2023.
* Mansour et al. (2016) Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian exploration: Incentivizing exploration in bayesian games. _arXiv preprint arXiv:1602.07570_, 2016.
* Mas-Colell et al. (1995) Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. _Microeconomic Theory_. Oxford University Press, New York, 1995.
* Metcalf and Weisbach (2009) Gillbert E Metcalf and David Weisbach. The design of a carbon tax. _Harv. Envtl. L. Rev._, 33:499, 2009.
* Mirrlees et al. (2011) James Mirrlees et al. _Tax by design: The Mirrlees review_. OUP Oxford, 2011.
* Myerson (1989) Roger B Myerson. _Mechanism design_. Springer, 1989.
* Nisan and Ronen (1999) Noam Nisan and Amir Ronen. Algorithmic mechanism design. In _Proceedings of the thirty-first annual ACM symposium on Theory of computing_, pages 129-140, 1999.
* Perchet et al. (2016) Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. _The Annals of Statistics_, 44:660-681, 2016.
* Pigou (2017) Arthur Pigou. _The economics of welfare_. Routledge, 2017.
* Robbins (1952) Herbert Robbins. Some aspects of the sequential design of experiments. _Bulletin of the American Mathematical Society_, page 527-535, 1952.
* Roughgarden (2010) Tim Roughgarden. Algorithmic game theory. _Communications of the ACM_, 53(7):78-86, 2010.
* Salanie (2005) Bernard Salanie. _The economics of contracts: a primer_. MIT press, 2005.
* Salanie (2011) Bernard Salanie. _The economics of taxation_. MIT press, 2011.
* Sankararaman et al. (2019) Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi armed bandits. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 3(3):1-35, 2019.
* Scheid et al. (2024) Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Eric Moulines, Michael I Jordan, and Alain Durmus. Incentivized learning in principal-agent bandit games. _ICML_, 2024.
* Selten (1990) Reinhard Selten. Bounded rationality. _Journal of Institutional and Theoretical Economics (JITE)/Zeitschrift fur die gesamte Staatswissenschaft_, 146(4):649-658, 1990.
* Shah et al. (2018) Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. _Advances in Neural Information Processing Systems_, 31, 2018.
* Simchowitz and Slivkins (2023) Max Simchowitz and Aleksandrs Slivkins. Exploration and incentives in reinforcement learning. _Operations Research_, 2023.
* Slivkins et al. (2019) Aleksandrs Slivkins et al. Introduction to multi-armed bandits. _Foundations and Trends(r) in Machine Learning_, 12(1-2):1-286, 2019.
* Slivkins et al. (2019)Heinrich Von Stackelberg. _Market structure and equilibrium_. Springer Science & Business Media, 2010.
* Wang et al. (2024) Serena Wang, Stephen Bates, P Aronow, and Michael Jordan. On counterfactual metrics for social welfare: incentives, ranking, and information asymmetry. In _International Conference on Artificial Intelligence and Statistics_, pages 1522-1530. PMLR, 2024.
* Wu et al. (2022a) Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Markov persuasion processes and reinforcement learning. In _ACM Conference on Economics and Computation_, 2022a.
* Wu et al. (2022b) Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng Xu. Sequential information design: Markov persuasion process and its efficient reinforcement learning. _arXiv preprint arXiv:2202.10678_, 2022b.
* Zhu et al. (2022) Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I Jordan. The sample complexity of online contract design. _arXiv preprint arXiv:2211.05732_, 2022.

Algorithmic Subroutine for the Binary Search

**Optimal Transfer.** For any given round \(t\geqslant 1\), action \(a\in\mathcal{A}\) and \(\varepsilon>0\), the downstream player can incentivize any best-responding upstream player to choose \(a\) by offering a transfer, \(\tau_{a}^{\star,\varepsilon}\in\mathbb{R}_{+}\), defined as:

\[\tau_{a}^{\star,\varepsilon}=\max_{a^{\prime}\in\mathcal{A}}v^{\mathrm{up}}(a^{ \prime})-v^{\mathrm{up}}(a)+\varepsilon\;.\]

With this transfer, it holds that for any \(a^{\prime}\in\mathcal{A},a^{\prime}\neq a\), we have \(v^{\mathrm{up}}(a^{\prime})<v^{\mathrm{up}}(a)+\tau_{a}^{\star,\varepsilon}\), ensuring the upstream player's action \(A_{t}=a\), since action \(a\) yields a superior reward. Consequently,

\[\tau_{a}^{\star}=\lim_{\varepsilon\to 0}\tau_{a}^{\star,\varepsilon}=\max_{a^{ \prime}\in\mathcal{A}}v^{\mathrm{up}}(a^{\prime})-v^{\mathrm{up}}(a)\]

represents the infimal transfer necessary to make arm \(a\) the best upstream player's choice.

**First step of \(\mathtt{BELGIC}\): estimation of the optimal transfers.** Suppose that we consider an arm \(a\in\mathcal{A}\) and that the downstream player offers an incentive \(\tau_{a}\) to the upstream player if he picks this arm. We consider this procedure with a constant incentive \(\tau_{a}\) for a batch of time steps of length \(\tilde{T}=\lceil T^{\alpha}\rceil\) due to the fact that the upstream player is learning (see Perchet et al., 2016, for batched bandits in the usual multi-armed setting). Lemma 2 shows that for \(\mathtt{BELGIC}\) to accurately estimate \(\tau_{a}^{\star}\) with high probability, it must hold

\[\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}<\tilde{T}/2\;,\;\;\text{which is equivalent to}\;\;\mathrm{C}T^{\kappa\alpha+\beta-\alpha}<1/2\;,\] (16)

This is why we impose the condition (13) on \(\alpha\) and \(\beta\), namely \(\beta/\alpha<1-\kappa\), which ensures that \(\kappa(\alpha-1)+\beta<0\) and therefore \(\lim_{T\to+\infty}T^{\kappa(\alpha-1)+\beta}=0\). More precisely, we define for \(a\in[K]\)

\[\Lambda_{a}=(a-1)\lceil\log_{2}T^{\beta}\rceil\tilde{T}\;,\]

which is the step after which starts the binary search procedure on arm \(a\). For any \(d\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\) on arm \(a\), we define

\[k_{a,d}=\Lambda_{a}+(d-1)\tilde{T}\] (17)

as the step after which starts the \(d\)-th batch iteration on arm \(a\), and we consider

\[T_{a,d}^{\neq}=\text{Card}\;\{t\in\{k_{d,a}+1,\ldots,k_{d,a}+\tilde{T}\}\; \text{such that}\;A_{t}\neq a\}\;,\] (18)

where \((A_{t})_{t\in\{1,\ldots,K\lceil\log_{2}T^{\beta}\rceil\tilde{T}\}}\) is given by Algorithm 2. Lemma 2 shows that for any \(a\in\mathcal{A},d\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\), with high probability

\[\text{if}\;T_{d,a}^{\neq}<\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha} \;\text{and}\;T_{d,a}^{\neq}>\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha},\; \text{then}\;|\tau_{a}^{\star}-\hat{\tau}_{d,a}|\leqslant 1/T^{\beta}\;,\] (19)

where \(\hat{\tau}_{a,d}\) is the current estimate of \(\tau_{a}^{\star}\) offered for iteration \(k_{a,d}\). In case (18) does not hold, it means that the upstream player has misplaced and has chosen most of the steps a suboptimal action, leading to an instantaneous regret for him larger than the bound given in **H2**. This is why (19) holds with high probability. To sum up, the first phase of \(\mathtt{BELGIC}\) consists in \(\lceil\log_{2}T^{\beta}\rceil\) batches of binary search on each arm \(a\in\mathcal{A}\) to obtain a precision level \(1/T^{\beta}\) on the optimal transfer \(\tau_{a}^{\star}\).

During this phase in Algorithm 2, we define \(\overline{\tau}_{a}(d)\in\mathbb{R}_{+}\) as the upper estimate and \(\underline{\tau}_{a}(d)\in\mathbb{R}_{+}\) as the lower estimate of \(\tau_{a}^{\star}\) after \(d\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\) rounds of binary search on arm \(a\). For any \(t\in[T]\) and \(a\in\mathcal{A}\), we define \(\tau_{a}^{\mathrm{mid}}(t)=(\overline{\tau}_{a}(t)+\underline{\tau}_{a}(t))/2\). \(\overline{\tau}_{a}(d),\underline{\tau}_{a}(d),\tau_{a}^{\mathrm{mid}}(d)\) are updated at the end of the \(d\)-th binary search batch of length \(\tilde{T}\) on arm \(a\). We define \(N_{T}=\lceil\log_{2}T^{\beta}\rceil\) as the number of binary search steps per arm.

After this first binary search phase, the downstream player computes estimates of the optimal transfers \(\tau_{a}^{\star}\)

\[(\hat{\tau}_{a})_{a\in\mathcal{A}}=(\overline{\tau}_{a}(\lceil\log_{2}T^{\beta }\rceil)+1/T^{\beta}+\mathrm{C}T^{(\kappa-1)/2})_{a\in\mathcal{A}}\;,\]

and offers these transfers \((\tau_{a}^{\star})_{a\in\mathcal{A}}\) to make the upstream player play any action \(\tilde{a}\in\mathcal{A}\) she wants.

**Second Step.** After the first phase during which the optimal incentives are estimated by the downstream player through \((\hat{\tau}_{a})_{a\in\mathcal{A}}\), she runs in the second phase the subroutine \(\mathtt{Bandit-Alg}\) on the \(\mathcal{A}\times\mathcal{A}\) bandit instance driven by her action \(B_{t}\) and the upstream player's one \(A_{t}\). More precisely, any bandit subroutine \(\mathtt{Bandit-Alg}\), such as \(\mathtt{UCB}\) or \(\varepsilon\mathtt{-greedy}\), for instance, can be run in a black-box fashion on the shifted bandit instance, where rewards are shifted by the upper estimated transfers \((\hat{\tau}_{a})_{a\in\mathcal{A}}\). The downstream player computes a shifted history \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}=\varnothing\) for any \(t\leqslant K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil\) and for any \(t>K\lceil T^{\alpha}\rceil\lceil\log_{2}T^{\beta}\rceil\)

\[\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}=\left\{\begin{array}{rl}&\{\tilde{ a}_{t},B_{t},\tau(t),U_{t},X_{\tilde{a}_{t},B_{t}}(t)-\hat{\tau}_{\tilde{a}_{t}}\} \cup\tilde{\mathcal{H}}_{t-1}^{\mathrm{down,p}}\text{ if }\hat{a}_{t}=A_{t}\\ &\tilde{\mathcal{H}}_{t-1}^{\mathrm{down,p}}\quad\text{ otherwise }.\end{array}\right.\]

which serves to feed \(\mathtt{Bandit-Alg}\), following \(\mathtt{Bandit-Alg}\colon(U_{t},\tilde{\mathcal{H}}_{t-1}^{\mathrm{down,p}}) \mapsto(\tilde{a}_{t},B_{t})\in\mathcal{A}\times\mathcal{A}\). Note that here, \(\mathtt{Bandit-Alg}\) aims to maximize the shifted reward \((\nu^{\mathrm{down}}(a,b)-\hat{\tau}_{a})_{(a,b)\in\mathcal{A}^{2}}\).

Based on this decision by \(\mathtt{Bandit-Alg}\), \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) offers the incentive \(\hat{\tau}_{\tilde{a}_{t}}\) associated with action \(\tilde{a}_{t}\) and plays action \(B_{t}\). Lemma 5 ensures that \(\tilde{a}_{t}\) is the upstream player's best choice. Therefore, \(\mathbf{H}\mathbf{2}\) ensures that the upstream player will not deviate from the downstream player's recommendation with high probability.

## Appendix B Invariance when the property rights are given to the downstream player

Our focus in the paper was the case where the upstream player possesses the bandit instance and receives monetary payments. We argue here that the symmetric situation, i.e. when the property rights are given to the downstream player, can be analysed in the exact same way.

Assume that the the downstream player owns the bandit instance. This implies that (i) they can prescribe what arm the upstream player has to play at each round, and (ii) the upstream player may perform a monetary transfer to influence the arm they are allowed to pull.

Consider the same bandit setup as before. Formally, the downstream player's action is \((A_{t},B_{t})\in\mathcal{A}\times\mathcal{A}\) where \(A_{t}\) is the arm that the upstream player is prescribed to pull (he cannot deviate since the downstream player has the property rights), while \(B_{t}\) is the arm played by the downstream player. On the other hand, the upstream player's policy outputs at each round the action \((\tilde{a}_{t},\tau(t))\in\mathcal{A}\times\mathbb{R}_{+}\), where \(\tilde{a}_{t}\) is the arm they choose to incentivize and \(\tau(t)\) is the amount of transfer. It means that the downstream player receives a transfer \(\tau(t)\) if she prescribes action \(A_{t}=\tilde{a}_{t}\). As a consequence, the instantaneous utility of the upstream player is \(Z_{A_{t}}-\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\), while the downstream player receives \(X_{A_{t},B_{t}}+\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\). In that case, the upstream player may perform a binary search on each arm \(\bar{a}\in\mathcal{A}\) to identify the optimal incentive \(\tau_{\tilde{a}}^{*}\), by considering Card \(\{t\in[T]\) such that \(\tilde{a}_{t}=\bar{a}\}\) during batches designed for the binary search and then play on the shifted bandit instance as we explained. This situation and the upstream player's strategy are now equivalent to the one presented before.

## Appendix C Proofs and Technical Results

Recall that we defined the shifted history \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}\) that will serve to feed \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) at time \(t\) as \(\tilde{\mathcal{H}}_{t}^{\mathrm{down,p}}=(\tilde{a}_{s},\tau(s),A_{s},V_{s},Z _{A_{s}}(s))_{s\leqslant t}\).

**Theorem 4**.: _Suppose that \(\operatorname*{argmax}_{a\in\mathcal{A}}v^{\mathrm{up}}(a)\) is the singleton \(\{a_{\star}^{u}\}\) and that_

\[v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{\mathrm{sw}},b^{\mathrm{ sw}})-v^{\mathrm{up}}(a_{\star}^{u})+v^{\mathrm{down}}(a_{\star}^{u},b)>0\,\]

_for any \(b\in\mathcal{A}\). In the absence of property rights and when the upstream player runs any no-regret policy \(\Pi_{\mathrm{n}}^{\mathrm{up}}\), we have \(\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n} }^{\mathrm{down}})\geqslant T\Delta^{\mathrm{sw}}-\mathfrak{R}_{\mathrm{n}}^{ \mathrm{up}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}})\Delta^{\mathrm{sw}}/\Delta^{ \mathrm{up}}\), where \(\Delta^{\mathrm{up}}=\min_{a^{\prime}\in\mathcal{A}\setminus\{a_{\star}^{u}\}} v^{\mathrm{up}}(a_{\star}^{u})-v^{\mathrm{up}}(a^{\prime})\) and \(\Delta^{\mathrm{sw}}=v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{ \mathrm{sw}},b^{\mathrm{sw}})-\max_{b\in\mathcal{A}}(v^{\mathrm{up}}(a^{ \mathrm{u}})+v^{\mathrm{down}}(a_{\star}^{u},b))\). Therefore, \(\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n} }^{\mathrm{down}})=\Omega(T)\) and \((\Pi_{\mathrm{n}}^{\mathrm{up}},\Pi_{\mathrm{n}}^{\mathrm{down}})\) is not welfare efficient._

Proof of Theorem 4.: Since \(\operatorname*{argmax}_{a\in\mathcal{A}}v^{\mathrm{up}}(a)\) is the singleton \(\{a_{\star}^{\mathrm{up}}\}\), we define \(\Delta^{\mathrm{up}}=\min_{a^{\prime}\in\mathcal{A}\setminus\{a_{\star}^{u}\}} v^{\mathrm{up}}(a_{\star}^{u})-v^{\mathrm{up}}(a^{\prime})\) as the upstream player reward gap and \(\Delta^{\mathrm{sw}}=v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{ \mathrm{sw}},b^{\mathrm{sw}})-\max_{b\in\mathcal{A}}(v^{\mathrm{up}}(a_{\star}^{ u})+v^{\mathrm{down}}(a_{\star}^{u},b))\) as the _social welfare_ reward gap if the upstream player plays his most preferred action.

Denote \(N_{\star}^{\mathrm{up}}(T)\) the number of pulls of the upstream player up to time \(T\) on the arm \(a_{\star}^{u}\). By definition of \(\Delta^{\mathrm{up}}\), we have that for any step \(t\in[T]\) such that \(A_{t}\neq a_{\star}^{\mathrm{up}}\),\(v^{\mathrm{up}}(a^{\mathrm{up}}_{*})-v^{\mathrm{up}}(A_{t})\geqslant\min_{a^{ \prime}\in\mathcal{A}\setminus\{a^{*}_{*}\}}v^{\mathrm{up}}(a^{\mathrm{u}}_{*}) -v^{\mathrm{up}}(a^{\prime})=\Delta^{\mathrm{up}}\). There are \(T-N^{\mathrm{up}}_{\star}(T)\) such steps, which leads to

\[\mathfrak{R}^{\mathrm{up}}_{\mathrm{n}}(T,\Pi^{\mathrm{up}}_{n}, \Pi^{\mathrm{down}}_{\mathrm{p}}) \geqslant\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\max_{a\in\mathcal{A}} \{v^{\mathrm{up}}(a)\}-v^{\mathrm{up}}(A_{t})\Bigg{]}\] \[\geqslant(T-\mathbb{E}[N^{\mathrm{u}}_{\star}(T)])\Delta^{ \mathrm{up}}\,\]

and we obtain

\[\mathbb{E}[N^{\mathrm{u}}_{\star}(T)]\geqslant T-\mathfrak{R}^{\mathrm{up}}_{ \mathrm{n}}(T,\Pi^{\mathrm{up}}_{\mathrm{n}})/\Delta^{\mathrm{up}}\.\]

Moreover, for any \(t\in[T]\) such that \(A_{t}=a^{\mathrm{up}}_{*}\) and any \(B_{t}\in\mathcal{A}\), \(v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{\mathrm{sw}},b^{ \mathrm{sw}})-(v^{\mathrm{up}}(A_{t})+v^{\mathrm{down}}(A_{t},B_{t}))=v^{ \mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{\mathrm{sw}},b^{\mathrm{sw }})-(v^{\mathrm{up}}(a^{\mathrm{up}}_{*})+v^{\mathrm{down}}(a^{\mathrm{up}},B _{t}))\geqslant\Delta^{\mathrm{sw}}\) by definition, which leads to

\[\mathfrak{R}^{\mathrm{sw}}(T,\Pi^{\mathrm{up}}_{\mathrm{n}},\Pi^{\mathrm{ down}}_{\mathrm{n}})\geqslant\mathbb{E}[N^{\mathrm{u}}_{\star}(T)]\Delta^{\mathrm{sw}}\,\]

and we obtain

\[\mathfrak{R}^{\mathrm{sw}}(T,\Pi^{\mathrm{up}}_{\mathrm{n}},\Pi^{\mathrm{ down}}_{\mathrm{n}})\geqslant T\Delta^{\mathrm{sw}}-\mathfrak{R}^{\mathrm{up}}_{ \mathrm{n}}(T,\Pi^{\mathrm{up}}_{\mathrm{n}})\Delta^{\mathrm{sw}}/\Delta^{ \mathrm{up}}\.\]

Since \(\lim_{T\to+\infty}\mathfrak{R}^{\mathrm{up}}_{\mathrm{n}}(T,\Pi^{\mathrm{up}}_{ \mathrm{n}})/T=0\), we have \(\lim_{T\to+\infty}\mathfrak{R}^{\mathrm{sw}}(T,\Pi^{\mathrm{up}}_{\mathrm{n}}, \Pi^{\mathrm{down}}_{\mathrm{n}})/T=\Delta^{\mathrm{sw}}>0\), hence the result. 

The proof of Theorem 2 is an immediate consequence of Theorem 4.

**Lemma 1**.: _Recall that \(\mu^{\star,\mathrm{down}}\) is the downstream player's optimal reward as defined as a solution of (10). We have \(\mu^{\star,\mathrm{down}}=\max_{a,b\in\mathcal{A}}\{v^{\mathrm{down}}(a,b)+v^ {\mathrm{up}}(a)\}-\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})\}\), as well as \((a^{\mathrm{opt}},b^{\mathrm{opt}})=(a^{\mathrm{sw}},b^{\mathrm{sw}})\) and \(\mu^{\star,\mathrm{up}}+\mu^{\star,\mathrm{down}}=v^{\mathrm{up}}(a^{\mathrm{ sw}})+v^{\mathrm{down}}(a^{\mathrm{sw}},b^{\mathrm{sw}})=\max_{a,b\in \mathcal{A}}\{v^{\mathrm{up}}(a)+v^{\mathrm{down}}(a,b)\}\), where \(\mu^{\star,\mathrm{up}}\) is defined in Equation (1). Moreover, for any integer \(T\in\mathbb{N}^{\star}\), and policies \(\Pi^{\mathrm{up}}_{\mathrm{p}}\), \(\Pi^{\mathrm{down}}_{\mathrm{p}}\), we have that_

\[\mathfrak{R}^{\mathrm{sw}}(T,\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down }}_{\mathrm{p}})\leqslant\mathfrak{R}^{\mathrm{up}}_{\mathrm{p}}(T,\Pi^{ \mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down}}_{\mathrm{p}})+\mathfrak{R}^{ \mathrm{down}}_{\mathrm{p}}(T,\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{ down}}_{\mathrm{p}})\.\]

Proof of Lemma 1.: Recall that \(\mu^{\star,\mathrm{down}}\) is defined as \(\mu^{\star,\mathrm{down}}=\sup_{a,b\in\mathcal{A}^{2},\tau\in\mathbb{R}_{+}} \{v^{\mathrm{down}}(a,b)-\tau\}\,\) such that \(a\in\operatorname{argmax}_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{ \prime})+\mathbb{1}_{a}(a^{\prime})\tau\}\) and \(a^{\mathrm{up}}_{*}=\operatorname{argmax}_{a^{\prime}\in\mathcal{A}}\,v^{ \mathrm{up}}(a)\). Note that we can write

\[\mu^{\star,\mathrm{down}}=\max\{\sup_{a,b\in\mathcal{A}^{2},\tau\in\mathbb{R}_{+ }}\mathbb{1}_{\tilde{\mathcal{A}}}(a,\tau)(v^{\mathrm{down}}(a,b)-\tau),\max_ {b\in\mathcal{A}}v^{\mathrm{down}}(a^{\mathrm{up}}_{*},b)\}\,\]

where \(\tilde{\mathcal{A}}=\{(a,\tau)\colon v^{\mathrm{up}}(a)+\tau\geqslant\max_{a^{ \prime}}v^{\mathrm{up}}(a^{\prime})+\mathbb{1}_{a}(a^{\prime})\tau\}\) which is the set of pairs \((a,\tau)\in\mathcal{A}\times\mathbb{R}_{+}\) such that the constraint binds. However, we also have by definition that \(v^{\mathrm{up}}(a^{\mathrm{up}}_{*})+0\geqslant\max_{a^{\prime}\in\mathcal{A}}v ^{\mathrm{up}}(a^{\prime})+\mathbb{1}_{a^{\mathrm{up}}_{*}}(a^{\prime})\cdot 0\) and hence, \((a^{\mathrm{up}}_{*},0)\in\tilde{\mathcal{A}}\). Therefore, since \(v^{\mathrm{down}}(a^{\mathrm{up}}_{*},b)\geqslant 0\) for any \(b\in\mathcal{A}\), we can write

\[\mu^{\star,\mathrm{down}}=\sup_{(a,\tau)\in\tilde{\mathcal{A}},b\in\mathcal{A}} \{v^{\mathrm{down}}(a,b)-\tau\}\.\]

First note that if \((a,\tau)\in\tilde{\mathcal{A}}\), then for any \(a^{\prime}\in\mathcal{A},\tau\in\mathbb{R}_{+}\), \(v^{\mathrm{up}}(a)+\tau\geqslant v^{\mathrm{up}}(a^{\prime})+\mathbb{1}_{a}(a^{ \prime})\tau\), which gives

\[v^{\mathrm{up}}(a)-v^{\mathrm{up}}(a^{\prime})\geqslant(\mathbb{1}_{a}(a^{ \prime})-1)\tau\.\]

However, either \(a=a^{\prime}\) and hence \(v^{\mathrm{up}}(a)-v^{\mathrm{up}}(a^{\prime})=0\), either \(a\neq a^{\prime}\) and hence \(\mathbb{1}_{a}(a^{hence the first part of the result. Since \(\mu^{\star,\mathrm{up}}\) is defined as \(\mu^{\star,\mathrm{up}}=\max_{a\in\mathcal{A}}v^{\mathrm{up}}(a)\), we have that

\[\mu^{\star,\mathrm{down}}+\mu^{\star,\mathrm{up}} =\max_{a,b\in\mathcal{A}}\{v^{\mathrm{down}}(a,b)-\max_{a^{\prime }\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})\}+v^{\mathrm{up}}(a)\}+\max_{a \in\mathcal{A}}v^{\mathrm{up}}(a)\] \[=\max_{a,b\in\mathcal{A}}\{v^{\mathrm{down}}(a,b)+v^{\mathrm{up} }(a)\}\] \[=v^{\mathrm{up}}(a^{\mathrm{sw}})+v^{\mathrm{down}}(a^{\mathrm{ sw}},b^{\mathrm{sw}})\.\]

Now summing \(\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}\) and \(\mathfrak{R}_{\mathrm{p}}^{\mathrm{up}}\) as defined in (9) and (11), we obtain

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{up}}(T,\Pi_{\mathrm{p}}^{ \mathrm{up}},\Pi_{\mathrm{p}}^{\mathrm{down}})+\mathfrak{R}_{\mathrm{p}}^{ \mathrm{down}}(T,\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi_{\mathrm{p}}^{\mathrm{down }})\] \[\quad=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\max_{a\in\mathcal{A}}\{v^{ \mathrm{up}}(a)+\mathbb{1}_{\tilde{a}_{t}}(a)\tau(t)\}-(v^{\mathrm{up}}(A_{t} )+\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t))\Bigg{]}\] \[\quad+T\max_{a,b\in\mathcal{A}^{2}}\{v^{\mathrm{down}}(a,b)-\max _{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})\}+v^{\mathrm{up}}(a )\}-\mathbb{E}\Bigg{[}\sum_{t=1}^{T}v^{\mathrm{down}}(A_{t},B_{t})-\mathbb{1 }_{\tilde{a}_{t}}(A_{t})\tau(t)\Bigg{]}\] \[\quad\geqslant T\max_{a\in\mathcal{A}}\{v^{\mathrm{up}}(a)\}- \mathbb{E}\Bigg{[}\sum_{t=1}^{T}v^{\mathrm{up}}(A_{t})+\mathbb{1}_{\tilde{a}_ {t}}(A_{t})\tau(t)\Bigg{]}-\mathbb{E}\Bigg{[}\sum_{t=1}^{T}v^{\mathrm{down}}( A_{t},B_{t})-\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\Bigg{]}\] \[\quad+T\max_{a,b\in\mathcal{A}^{2}}\{v^{\mathrm{down}}(a,b)+v^{ \mathrm{up}}(a)\}-T\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{ \prime})\}\] \[\quad=T\max_{a,b\in\mathcal{A}^{2}}\{v^{\mathrm{up}}(a)+v^{ \mathrm{down}}(a,b)\}-\mathbb{E}\Bigg{[}\sum_{t=1}^{T}v^{\mathrm{up}}(A_{t})+v ^{\mathrm{down}}(A_{t},B_{t})\Bigg{]}\] \[\quad=\mathfrak{R}^{\mathrm{sw}}(T,\Pi_{\mathrm{p}}^{\mathrm{up }},\Pi_{\mathrm{p}}^{\mathrm{down}})\,\]

hence the result. 

For a downstream player's policy \(\Pi_{\mathrm{p}}^{\mathrm{down}}\), we define \(\tilde{\mathfrak{R}}_{\mathrm{p}}^{\mathrm{up}}\) as the upstream player's regret without expectation, following

\[\tilde{\mathfrak{R}}_{\mathrm{p}}^{\mathrm{up}}(\{s+1,\ldots,s+t\},\Pi_{ \mathrm{p}}^{\mathrm{up}},\Pi_{\mathrm{p}}^{\mathrm{down}})=\sum_{l=s+1}^{s+t }\max_{a\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{a}_{l}}(a)\tau (l)\}-(v^{\mathrm{up}}(A_{l})+\mathbb{1}_{\tilde{a}_{l}}(A_{l})\tau(l))\,\]

where \((\tilde{a}_{l},\tau(l))_{l\in[T]}\) are the incentives output by \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) and \((A_{l})_{l\in[T]}\) are the output of \(\Pi_{\mathrm{p}}^{\mathrm{up}}\). Recall the assumption that we use on the upstream player's regret for a policy \(\Pi_{\mathrm{p}}^{\mathrm{up}}\). We show here that it is satisfied by typical no-regret bandit algorithms.

**H2**.: _There exist \(\mathrm{C},\zeta>0,\kappa\in[0,1)\) such that for any \(s,t\in[T]\) with \(s+t\leqslant T\), any \(\{\tau_{a}\}_{a\in[K]}\in\mathbb{R}_{+}^{K}\) and any policy \(\Pi_{\mathrm{p}}^{\mathrm{down}}\) that offers almost surely a transfer \((\tilde{a}_{l},\tau(l))=(\tilde{a}_{l},\tau_{\tilde{a}_{l}})\) for any \(l\in\{s+1,\ldots,s+t\}\), the batched regret of the upstream player following \(\Pi_{\mathrm{p}}^{\mathrm{up}}\) satisfies, with probability at least \(1-t^{-\zeta}\),_

\[\sum_{l=s+1}^{s+t}\max_{a\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{ \tilde{a}_{l}}(a)\tau_{\tilde{a}_{l}}\}-(v^{\mathrm{up}}(A_{l})+\mathbb{1}_{ \tilde{a}_{l}}(A_{l})\tau_{\tilde{a}_{l}})\leqslant\mathrm{C}t^{\kappa}\.\]

We present the upstream player's UCB subroutine.

**Proposition 2**.: _Let \(s,t\in[T]\) such that \(s+t\leqslant T\). Suppose that there exists a family \((\tau_{a})_{a\in\mathcal{A}}\) of constant incentives associated with each arm \(a\in\mathcal{A}\) such that we have in Algorithm 3: \((\tilde{a}_{l},\tau(l))_{s+l\in\{s+1,\ldots,s+t\}}=(\tilde{a}_{l},\tau_{\tilde{ a}_{l}})_{l\in\{s+1,\ldots,s+t\}}\) as an output of \(\Pi_{\mathrm{p}}^{\mathrm{down}}\). Suppose that the distributions \(\gamma_{a}\) are \(1\)-sub-Gaussian. Then with probability at least \(1-T^{-2}\), the regret of the version of UCB given in Algorithm 3 run by the upstream player satisfies_

\[\tilde{\mathfrak{R}}_{\mathrm{p}}^{\mathrm{up}}(\{s+1,\ldots,s+t\},\textit{UCB}, \Pi_{\mathrm{p}}^{\mathrm{down}})\leqslant 8\sqrt{\log(KT^{3})}\sqrt{tK}\.\]

Note that the major difference between this assumption and the regret bounds that we generally consider in multi-armed bandit problems is that we consider the regret without expectation here.

Proof of Proposition 2.: The proof is adapted from the proof of Bubeck et al. (2012, Theorem 2.1). Let \(s,t\in[T]\) such that \(s+t\leqslant T\). For any integer \(l\in\{s+1,\ldots,s+t\}\), we write \(n_{l}(a)=\text{Card }\{l^{\prime}\in[l]\text{ such that }A_{l^{\prime}}=a\}\) for the number of pulls of arm \(a\) and \(\hat{\mu}_{a}(l)\) for the empirical mean utility of the arm \(a\in\mathcal{A}\) estimated on the batch \(\{1,\ldots,l\}\): \(\hat{\mu}_{a}(l)=n_{l}(a)^{-1}\sum_{k=1}^{l}\mathbb{1}_{a_{l}}(A_{k})X_{a}(k)\). Since the rewards on the incentivized bandit instance are \(1\)-sub-Gaussian, a Hoeffding bound gives that for any \(\delta\in(0,1)\), \(a\in\mathcal{A}\), \(l\in\{s+1,\ldots,s+t\}\), and any family of arms \((a_{l^{\prime}})_{l^{\prime}\in\{1,\ldots,l\}}\) such that Card \(\{j\colon a_{j}=a\}=k\), we have that

\[\mathbb{P}\Big{(}|\hat{\mu}_{a}(l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/ \delta)/k}\;\Big{|}\;(A_{l^{\prime}})_{l^{\prime}\in\{1,\ldots,s+l\}}=(a_{l^{ \prime}})_{l^{\prime}\in\{1,\ldots,s+l\}}\Big{)}\leqslant\delta\;.\]

Therefore, for any \(\delta\in(0,1)\), \(a\in\mathcal{A}\), \(l\in\{s+1,\ldots,s+t\}\), we have the following bound

\[\mathbb{P}\Big{(} |\hat{\mu}_{a}(l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/ \delta)/n_{l}(a)}\Big{)}\] \[=\mathbb{P}\!\left(\bigcup_{k=1}^{l}\bigcup_{\begin{subarray}{c}(a _{l^{\prime}})_{l^{\prime}\text{ s.t.}}\\ \text{Card}\{l^{\prime}\in\{1,\ldots,l\}\colon a_{l^{\prime}}=a\}=k\end{subarray}} \{|\hat{\mu}_{a}(l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/\delta)/k}\}\right)\] \[\leqslant\sum_{k=1}^{l}\mathbb{P}\!\left(\bigcup_{\begin{subarray}{ c}(a_{l^{\prime}})_{l^{\prime}\text{ s.t.}}\\ \text{Card}\{l^{\prime}\in\{1,\ldots,l\}\colon a_{l^{\prime}}=a\}=k\end{subarray}} \{|\hat{\mu}_{a}(s+l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/\delta)/k}\} \right)\] \[\leqslant\sum_{k=1}^{l}\sum_{\begin{subarray}{c}(a_{l^{\prime}}) _{l^{\prime}\text{ s.t.}}\\ \text{Card}\{l^{\prime}\in\{1,\ldots,l\}\colon a_{l^{\prime}}=a\}=k\end{subarray}} \mathbb{P}\Big{(}|\hat{\mu}_{a}(s+l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/ \delta)/k}\;\Big{|}\;A_{l}=a_{l}\Big{)}\mathbb{P}(A_{l}=a_{l})\] \[\leqslant T\delta\;,\]

and with an union bound, we obtain that

\[\mathbb{P}\Big{(}\exists\;l\in[t],a\in\mathcal{A}\text{ such that }|\hat{\mu}_{a}(l)-v^{\text{up}}(a)|\geqslant 2\sqrt{\log(2/\delta)/n_{l}(a)} \Big{)}\leqslant T^{2}K\delta\;,\]

Considering the probability of the opposite event, we have that

\[\mathbb{P}\Big{(}\text{for any }l\in[t],a\in\mathcal{A},|\hat{\mu}_{a}(l)-v^{ \text{up}}(a)|\leqslant 2\sqrt{\log(2T^{2}K/\delta)/n_{l}(a)}\Big{)}\geqslant 1- \delta\;,\] (20)

where we rescaled \(\delta\) as \(\delta/T^{2}K\).

For the remaining of the proof, we take \(\delta=T^{-2}\) and define \(a_{l}^{*}=\operatorname*{argmax}_{a\in\mathcal{A}}\{v^{\text{up}}(a)+ \mathbb{1}_{\hat{a}_{l}}(a)\tau\}\). We now assume that the event \(\{\text{for any }l\in\{s+1,\ldots,s+t\},a\in\mathcal{A},|\hat{\mu}_{a}(l)-v^{ \text{up}}(a)|\leqslant 2\sqrt{\log(2T^{2}K/\delta)/n_{l}(a)}\}\) holds. If at some step \(l\in\{s+1,\ldots,s+t\}\), action \(A_{l}\) is chosen in Algorithm 3, it means that

\[\hat{\mu}_{A_{l}}(l)+2\sqrt{\log(tK/\delta)/n_{l}(A_{l})}+\mathbb{1}_{\hat{a} _{l}}(A_{l})\tau_{\widehat{a}_{l}}\geqslant\hat{\mu}_{a_{l}^{*}}(l)+2\sqrt{ \log(tK/\delta)/n_{l}(a_{l}^{*})}+\mathbb{1}_{\hat{a}_{l}}(a_{l}^{*})\tau_{ \widehat{a}_{l}}\;,\]with regards to the choice of actions in UCB based on the upper confidence bound. We now decompose the whole regret on the batch \(\{s+1,\ldots,s+t\}\) defined as \(\tilde{\mathfrak{H}}^{\rm up}_{\rm p}(\{s+1,\ldots,s+t\},\texttt{UCB},\Pi^{\rm down }_{\rm p})\). (20) ensures that with probability at least \(1-1/T^{2}\)

\[\tilde{\mathfrak{H}}^{\rm up}_{\rm p}(\{s+1,\ldots,s+t\},\texttt{UCB},\Pi^{ \rm down}_{\rm p})=\sum_{l=s+1}^{s+t}v^{\rm up}(a_{l}^{\star})+\mathbb{1}_{ \tilde{a}_{l}}(a_{t}^{\star})\tau_{\tilde{a}_{l}}-(v^{\rm up}(A_{l})+\mathbb{1} _{\tilde{a}_{l}}(A_{l})\tau_{\tilde{a}_{l}})\]

\[\leqslant\sum_{l=s+1}^{s+t}\hat{\mu}_{a_{l}}(l)+2\sqrt{\log(Kt/ \delta)/n_{l}(a_{l}^{\star})}-v^{\rm up}(A_{l})+\mathbb{1}_{\tilde{a}_{l}}(a_{ t}^{\star})\tau_{\tilde{a}_{l}}-\mathbb{1}_{\tilde{a}_{l}}(A_{l})\tau_{ \tilde{a}_{l}}\] \[\leqslant\sum_{l=s+1}^{s+t}\hat{\mu}_{A_{l}}(l)+2\sqrt{\log(Kt/ \delta)/n_{l}(A_{l})}-v^{\rm up}(A_{l})\] \[\leqslant\sum_{l=s+1}^{s+t}\hat{\mu}_{A_{l}}(l)+2\sqrt{\log(Kt/ \delta)/n_{l}(A_{l})}-(\hat{\mu}_{A_{l}}(l)-2\sqrt{\log(Kt/\delta)/n_{l}(A_{l} )})\] \[\leqslant 4\sqrt{\log(Kt/\delta)}\sum_{l=s+1}^{s+t}\sqrt{1/n_{l}(A_{ l})}\;,\]

and we have

\[\sum_{l=s+1}^{s+t}\sqrt{1/n_{l}(A_{l})} \leqslant\sum_{i=1}^{K}\sum_{l=s+1}^{s+t}\sqrt{\mathbb{1}_{A_{l} }(i)/n_{l}(i)}\] \[\leqslant\sum_{i=1}^{K}\sum_{j=n_{s+t}(i)}^{n_{s+t}(i)}1/\sqrt{j} \leqslant 2\sum_{i=1}^{K}\sqrt{n_{s+t}(i)-n_{s}(i)}\;,\] (21)

where the last step holds because for any integers \(s,t\in\mathbb{N}^{\star}\), we have that

\[\sum_{l=s+1}^{s+t}\frac{1}{\sqrt{l}}=\sum_{l=s+1}^{s+t}\frac{1}{\sqrt{l}}\int _{x=l-1}^{l}\mathrm{d}x\leqslant\sum_{l=s+1}^{s+t}\int_{x=l-1}^{l}\frac{ \mathrm{d}x}{\sqrt{x}}=\int_{x=s}^{s+t}\frac{\mathrm{d}x}{\sqrt{x}}=2(\sqrt{s +t}-\sqrt{s})\;.\]

Using Cauchy-Schwarz inequality we obtain from (21)

\[1/K\sum_{l=s+1}^{s+t}\sqrt{1/n_{l}(A_{l})}\leqslant 2\sqrt{1/K\sum_{i=1}^{K}n _{s+t}(i)-n_{s}(i)}=2\sqrt{t/K}\;,\]

which gives \(\sum_{l=s+1}^{s+t}\sqrt{1/n_{l}(A_{l})}\leqslant 2\sqrt{tK}\).

Finally plugging all the terms together, since \(\delta=1/T^{2}\), we obtain that with probability at least \(1-1/T^{2}\)

\[\tilde{\mathfrak{H}}^{\rm up}_{\rm p}(\{s+1,\ldots,s+t\},\texttt{UCB}) \leqslant 8\sqrt{\log(KT^{3})}\sqrt{tK}\;.\]

**Lemma 2**.: _Assume **H**2 holds and consider some arm \(a\in\mathcal{A}\) such that we run the \(d\)-th batch of binary search on \(a\) with \(d\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\): for any \(t\in\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\), we have \((\tilde{a}_{t},\tau(t))=(a,\tau_{a})\) with \(\tau_{a}=\tau_{a}^{\rm mid}(d)\) and \(\tilde{T}=\lceil T^{\alpha}\rceil\). Recall that we defined in (18): \(T_{a,d}^{\neq}=\text{Card }\{t\in\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\text{ such that }A_{t}\neq a\}\). Let \(\beta\in(0,1)\) be such that \(\beta<\alpha(1-\kappa)\). Given that the event \(\{\tilde{\mathfrak{H}}^{\rm up}_{\rm p}(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi^{\rm up}_{\rm p},\Pi^{\rm down}_{\rm p})\leqslant\mathrm{C}\tilde{T}^{ \kappa}\}\) holds, we have that_

* _If_ \(T_{a,d}^{\neq}<\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\)_, then_ \(\tau_{a}^{\star}<\tau_{a}+1/T^{\beta}\)_._
* _If_ \(T_{a,d}^{\neq}>\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\)_, then_ \(\tau_{a}^{\star}>\tau_{a}-1/T^{\beta}\)_._

_Consequently, with probability at least \(1-2T^{-\alpha\zeta}\), if \(\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}<T_{a,d}^{\neq}<\tilde{T}-\mathrm{C} \tilde{T}^{\kappa+\beta/\alpha}\), then \(|\tau_{a}^{\star}-\tau_{a}|\leqslant 1/T^{\beta}\)._Proof of Lemma 2.: The whole proof is done conditionally on the event

\[\{\tilde{\mathfrak{P}}^{\mathrm{up}}_{\mathrm{p}}(\{k_{a,d}+1,\ldots,k_{a,d}+ \tilde{T}\},\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down}}_{\mathrm{p}}) \leqslant\mathrm{C}\tilde{T}^{\kappa}\}\.\]

Note that it holds with probability at least \(1-\tilde{T}^{-\zeta}\geqslant 1-T^{-\alpha\zeta}\) since we suppose that \(\Pi^{\mathrm{up}}_{\mathrm{p}}\) satisfies \(\mathbf{H}2\).

Suppose that we have \(\tau_{a}\geqslant\tau_{a}^{\star}+1/T^{\beta}\). By definition of the optimal incentives, we obtain, using by assumption \(\tau_{a}\geqslant\tau_{a}^{\star}+1/T^{\beta}\)

\[\mathbb{1}_{a}(a)\tau_{a}+v^{\mathrm{up}}(a) \geqslant\tau_{a}^{\star}+v^{\mathrm{up}}(a)+1/T^{\beta}\] \[=\max_{a^{\prime}\in\mathcal{A}}v^{\mathrm{up}}(a^{\prime})-v^{ \mathrm{up}}(a)+v^{\mathrm{up}}(a)+1/T^{\beta}\] \[=\max_{a^{\prime}}v^{\mathrm{up}}(a^{\prime})+1/T^{\beta}\,\]

which ensures that \(a\) is the optimal arm for the upstream player during the batch \(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\) that we consider. In that case, since \(a\) is the best arm, by definition of the upstream player's utility, the reward gap \(\mathbb{E}[v^{\mathrm{up}}(a)+\tau_{a}-(v^{\mathrm{up}}(A_{t})+\mathbb{1}_{a }(A_{t})\tau_{a})]\) for the upstream player at any step \(t\in\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\) is at least \(v^{\mathrm{up}}(a)+\tau_{a}-\max_{a^{\prime}\in\mathcal{A}}v^{\mathrm{up}}(a^ {\prime})\), and we obtain

\[C\tilde{T}^{\kappa}\geqslant\tilde{\mathfrak{P}}^{\mathrm{up}}_{\mathrm{p}}( \{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{ \mathrm{down}}_{\mathrm{p}})\geqslant T_{a,d}^{\neq}(\tau_{a}+v^{\mathrm{up}}( a)-\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})\})\,\]

by \(\mathbf{H}2\), \(T_{a,d}^{\neq}\) being the number of steps for which a suboptimal arm has been chosen. Therefore, by definition of the optimal incentives, we obtain that

\[C\tilde{T}^{\kappa}\geqslant T_{a,d}^{\neq}(\tau_{a}-\tau_{a}^{\star}) \geqslant T_{a,d}^{\neq}/T^{\beta}\geqslant T_{a,d}^{\neq}\tilde{T}^{-\beta/ \alpha}\,\]

which gives: \(T_{a,d}^{\neq}\leqslant\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\). Therefore, if we take the contrapositive, we obtain that during the sequence \(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\), if \(T_{a,d}^{\neq}>\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\), then with probability at least \(1-T^{-\alpha\zeta}\), \(\tau_{a}<\tau_{a}^{\star}+1/T^{\beta}\), or equivalently \(\tau_{a}^{\star}>\tau_{a}-1/T^{\beta}\).

Now suppose that \(\tau_{a}\leqslant\tau_{a}^{\star}-1/T^{\beta}\). By definition of the optimal incentives, we obtain

\[\mathbb{1}_{a}(a)\tau_{a}+v^{\mathrm{up}}(a) \leqslant\tau_{a}^{\star}+v^{\mathrm{up}}(a)\] \[\leqslant\max_{a^{\prime}\in\mathcal{A}}v^{\mathrm{up}}(a^{\prime })-v^{\mathrm{up}}(a)+v^{\mathrm{up}}(a)-1/T^{\beta}\] \[=\max_{a^{\prime}}v^{\mathrm{up}}(a^{\prime})-1/T^{\beta}\,\]

which ensures that \(a\) is a suboptimal arm for the upstream player during this batch of time steps. Therefore, arm \(a\) which has a reward gap bigger than \(1/T^{\beta}\) since \(\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{a}(a^{\prime}) \}-(v^{\mathrm{up}}(a)+\tau_{a})\geqslant 1/T^{\beta}\) and arm \(a\) has been picked \(\tilde{T}-T_{a,d}^{\neq}\) times. Consequently, we have that

\[\mathrm{C}\tilde{T}^{\kappa} \geqslant\tilde{\mathfrak{P}}^{\mathrm{up}}_{\mathrm{p}}(\{k_{a,d} +1,\ldots,k_{a,d}+\tilde{T}\},\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down} }_{\mathrm{p}})\] \[\geqslant(\tilde{T}-T_{a,d}^{\neq})(\max_{a^{\prime}\in\mathcal{ A}}v^{\mathrm{up}}(a^{\prime})-(\tau_{a}+v^{\mathrm{up}}(a)))\] \[\geqslant(\tilde{T}-T_{a,d}^{\neq})\tilde{T}^{-\beta/\alpha}\,\]

which gives \(T_{a,d}^{\neq}\geqslant\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\). Therefore, if we take the contrapositive, we obtain that if \(T_{a,d}^{\neq}<\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\), then with probability at least \(1-T^{-\alpha\zeta}\), \(\tau_{a}>\tau_{a}^{\star}-1/T^{\beta}\), or equivalently \(\tau_{a}^{\star}<\tau_{a}+1/T^{\beta}\).

For the second part of the proof, suppose that: \(\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}<T_{a,d}^{\neq}<\tilde{T}^{\kappa+ \beta/\alpha}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\).

From the above result, we have that \(\tau_{a}^{\star}<\tau_{a}+1/T^{\beta}\) and \(\tau_{a}^{\star}>\tau_{a}-1/T^{\beta}\) with probability at least \(1-2T^{-\alpha\zeta}\). Plugging these inequalities in the absolute value \(|\tau_{a}^{\star}-\tau_{a}|\) concludes the proof. 

**Lemma 3**.: _Assume that we run Algorithm 2 and consider some binary search batch iteration \(d\in\lceil\log T^{\beta}\rceil\) run on arm \(a\in\mathcal{A}\). Then \(0\leqslant\underline{\tau}_{a}(a)\leqslant\tau_{a}^{\mathrm{mid}}(d)\leqslant \overline{\tau}_{a}(d)\leqslant 1\)._Proof of Lemma 3.: The proof proceeds by induction. Considering some action \(a\in\mathcal{A}\), we have for the initialisation before any binary search is run: \(\underline{\tau}_{a}(0)=0,\overline{\tau}_{a}(0)=1\) and therefore \(\tau_{a}^{\rm mid}(0)\in[\underline{\tau}_{a}(0),\overline{\tau}_{a}(0)]\). We now consider that a number \(d\) of binary search batches has been run on \(a\). Suppose that we run an additional binary search batch on action \(a\). We have

\[\tau_{a}^{\rm mid}(d+1)=\frac{\overline{\tau}_{a}(d)+\underline{\tau}_{a}(d)}{2} \text{ which gives }\tau_{a}^{\rm mid}(d+1)\in[\underline{\tau}_{a}(d),\overline{\tau}_{a}(d)]\.\] (22)

After this iteration of binary search, we either have \(\overline{\tau}_{a}(d+1)=\tau_{a}^{\rm mid}(d+1)+1/T^{\beta}\) and \(\underline{\tau}_{a}(d+1)=\underline{\tau}_{a}(d)\) or \(\overline{\tau}_{a}(d+1)=\tau_{a}^{\rm mid}(d)\) and \(\underline{\tau}_{a}(d+1)=\tau_{a}^{\rm mid}(d+1)-1/T^{\beta}\). Therefore, we still have \(0\leqslant\underline{\tau}_{a}(d+1)\leqslant\tau_{a}^{\rm mid}(d+1) \leqslant\overline{\tau}_{a}(d+1)\leqslant 1\), hence the result for any \(d\in\lceil\log_{2}T^{\beta}\rceil\) by induction. 

**Lemma 4**.: _Consider some arm \(a\in\mathcal{A}\) and suppose that we have run \(D\in\mathbb{N}^{\star}\) batches of binary search of length \(\tilde{T}\) on \(a\). Then, we have that_

\[\mathbb{P}\!\left(\bigcap_{d\in[D]}\{\tilde{\mathfrak{P}}_{\rm p}^{\rm up}(\{ k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{\rm p}^{\rm down })\leqslant\mathrm{C}\tilde{T}^{\star}\}\right)\geqslant 1-D/T^{\alpha\zeta}\.\]

Proof of Lemma 4.: First observe that for any batch \(d\in\{1,\ldots,D\}\) of binary search run on arm \(a\) during steps \(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\}\), we have by **H2** that

\[\mathbb{P}\!\left(\tilde{\mathfrak{P}}_{\rm p}^{\rm up}(\{k_{a,d}+1,\ldots,k _{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{\rm p}^{\rm down})\geqslant \mathrm{C}\tilde{T}^{\kappa}\right)\leqslant 1/\tilde{T}^{\zeta}\,\]

and applying a union bound over the \(D\) batches, we have that

\[\mathbb{P}\!\left(\bigcup_{d\in[D]}\{\tilde{\mathfrak{P}}_{\rm p}^ {\rm up}(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{ \rm p}^{\rm down})\geqslant\mathrm{C}\tilde{T}^{\kappa}\}\right)\] \[\leqslant\sum_{j=1}^{D}\mathbb{P}\!\left(\{\tilde{\mathfrak{P}}_{ \rm p}^{\rm up}(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up}, \Pi_{\rm p}^{\rm down})\geqslant\mathrm{C}\tilde{T}^{\kappa}\}\right)\] \[\leqslant D/\tilde{T}^{\zeta}\,\]

which gives that

\[\mathbb{P}\!\left(\bigcap_{d\in[D]}\{\tilde{\mathfrak{P}}_{\rm p}^{\rm up}(\{ k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{\rm p}^{\rm down })\leqslant\mathrm{C}\tilde{T}^{\kappa}\}\right)\geqslant 1-D/T^{\alpha\zeta}\,\]

hence the result. 

**Lemma 5**.: _Suppose that the upstream player runs a subroutine \(\Pi_{\rm p}^{\rm up}\) satisfying **H** 2. Consider some action \(a\in\mathcal{A}\) and the \(D\)-th binary search batch of length \(\tilde{T}\) run on arm \(a\) with \(D\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\). Then_

\[\bigcap_{d\in[D]}\{\tilde{\mathfrak{P}}_{\rm p}^{\rm up}(\{k_{a,d}+1,\ldots,k _{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{\rm p}^{\rm down})\leqslant \mathrm{C}\tilde{T}^{\kappa}\}\subseteq\{\tau_{a}^{\star}\in[\underline{\tau}_{a }(D),\overline{\tau}_{a}(D)]\}\,\]

_and the probability of these events is at least \(1-\lceil\log_{2}T^{\beta}\rceil/T^{\alpha\zeta}\). We also have that_

\[|\overline{\tau}_{a}(D)-\underline{\tau}_{a}(D)|\leqslant 1/2^{D}+2/T^{\beta} \text{ holds almost surely}\.\]

Proof of Lemma 5.: Suppose that the conditions of the lemma hold and consider some arm \(a\).

We show by induction on the number of binary search batches \(D\) that have been run on \(a\) that \(\bigcap_{d\in[D]}\{\tilde{\mathfrak{P}}_{\rm p}^{\rm up}(\{k_{a,d}+1,\ldots,k _{a,d}+\tilde{T}\},\Pi_{\rm p}^{\rm up},\Pi_{\rm p}^{\rm down})\leqslant \mathrm{C}\tilde{T}^{\kappa}\}\subseteq\{\tau_{a}^{\star}\in[\underline{\tau}_{a }(D),\overline{\tau}_{a}(D)]\}\). If it is true, Lemma 4 completes this first part of the proof.

The initialisation holds since \(\underline{\tau}_{a}(0)=0\), \(\overline{\tau}_{a}(0)=1\) and \(\tau_{a}^{\star}=\max_{a^{\prime}\in\mathcal{A}}v^{\rm up}(a^{\prime})-v^{ \rm up}(a)\in[0,1]\) with probability 1 - since \(\max_{a^{\prime}\in\mathcal{A}}v^{\rm up}(a^{\prime})\in[0,1]\) and \(v^{\rm up}(a)\in[0,1]\).

We suppose that the property is true for some integer \(D<\lceil\log_{2}T^{\beta}\rceil\) and that we have run one more binary search on arm \(a\). We have

\[\tau_{a}^{\mathrm{mid}}(D+1)=\frac{\overline{\tau}_{a}(D)+\underline{\tau}_{a}(D )}{2}\,\]

\(\tau_{a}^{\mathrm{mid}}(D+1)\) being the incentive offered to the upstream player if he chooses action \(a\) during the \(D\)-th batch \(\{k_{a,D}+1,\ldots,k_{a,D}+\tilde{T}\}\). After this batch, if \(T_{a,D}^{\neq}<\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\), \(\mathsf{BELGIC}\) updates \(\overline{\tau}_{a}(D+1)=\tau_{a}^{\mathrm{mid}}(D+1)+1/T^{\beta}\), \(\underline{\tau}_{a}(D+1)=\underline{\tau}_{a}(D)\) and Lemma 2 ensures that \(\underline{\tau}_{a}(D+1)<\tau_{a}^{*}<\overline{\tau}_{a}(D+1)\) given \(\{\tilde{\mathfrak{R}}^{\mathrm{up}}_{\mathrm{p}}(\{k_{a,D+1}+1,\ldots,k_{a, D+1}+\tilde{T}\},\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down}}_{ \mathrm{p}})\leqslant\mathrm{C}\tilde{T}^{\kappa}\}\). Thus the induction holds.

Otherwise, if \(T_{a,D}^{\neq}>\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\), \(\mathsf{BELGIC}\) updates \(\underline{\tau}_{a}(D+1)=\tau_{a}^{\mathrm{mid}}(D+1)-1/T^{\beta}\), \(\overline{\tau}_{a}(D+1)=\overline{\tau}_{a}(D)\) and Lemma 2 ensures that \(\underline{\tau}_{a}(D+1)<\tau_{a}^{\star}<\overline{\tau}_{a}(D+1)\) given \(\{\tilde{\mathfrak{R}}^{\mathrm{up}}_{\mathrm{p}}(\{k_{a,D+1}+1,\ldots,k_{a, D+1}+\tilde{T}\},\Pi^{\mathrm{up}}_{\mathrm{p}},\Pi^{\mathrm{down}}_{ \mathrm{p}})\leqslant\mathrm{C}\tilde{T}^{\kappa}\}\). The induction still holds.

Consequently, we have that for any number \(D\) of binary search batches run on arm \(a\), \(\tau_{a}^{\star}\in[\underline{\tau}_{a}(D),\overline{\tau}_{a}(D)]\) with probability \(1-D/T^{\alpha\zeta}\)

For the second part of the proof, we define \(u(D)=\overline{\tau}_{a}(D)-\underline{\tau}_{a}(D)\geqslant 0\) as the length of the interval containing \(\tau_{a}^{\star}\) with probability at least \(1-D/T^{\alpha\zeta}\). We have \(u(0)=1\). Suppose that after \(D\) iterations of binary search batches, the next batch of binary search \(\{k_{a,D+1}+1,\ldots,k_{a,D+1}+\tilde{T}\}\) outputs \(T_{a,D+1}^{\neq}<\tilde{T}-\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\). Then, the update of Algorithm 2 gives

\[u_{D+1} =\overline{\tau}_{a}(D+1)-\underline{\tau}_{a}(D+1)\] \[=\tau_{a}^{\mathrm{mid}}(D+1)+1/T^{\beta}-\underline{\tau}_{a}(D)\] \[=\frac{\overline{\tau}_{a}(D)+\underline{\tau}_{a}(D)}{2}- \underline{\tau}_{a}(D)+1/T^{\beta}\] \[=\frac{\overline{\tau}_{a}(D)-\underline{\tau}_{a}(D)}{2}+1/T^{\beta}\] \[=u_{D}/2+1/T^{\beta}\.\]

On the other hand, if \(T_{a,D+1}^{\neq}>\mathrm{C}\tilde{T}^{\kappa+\beta/\alpha}\), the update gives

\[u_{D+1} =\overline{\tau}_{a}(D+1)-\underline{\tau}_{a}(D+1)\] \[=\overline{\tau}_{a}(D_{t}^{a})-(\tau_{a}^{\mathrm{mid}}(D+1)-1/ T^{\beta})\] \[=\overline{\tau}_{a}(D)-\frac{\overline{\tau}_{a}(D)+\underline{ \tau}_{a}(D)}{2}+1/T^{\beta}\] \[=\frac{\overline{\tau}_{a}(D)-\underline{\tau}_{a}(D)}{2}+1/T^{\beta}\] \[=u_{D}/2+1/T^{\beta}\.\]

We can see that \((u_{D})_{D\geqslant 0}\) is an arithmetico-geometric sequence defined by \(u_{D+1}=u_{D}/2+1/T^{\beta}\) with an initial term \(u_{0}=1\). Writing \(r=1/T^{\beta}/(1-1/2)=2/T^{\beta}\), we obtain that

\[|\overline{\tau}_{a}(D)-\underline{\tau}_{a}(D)|=u_{D}=1/2^{D}(1-r)+r=1/2^{D}( 1-2/T^{\beta})+2/T^{\beta}\leqslant 1/2^{D}+2/T^{\beta}\,\]

for any \(D\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\), hence the result. 

**Lemma 6**.: _Suppose that the upstream player runs a policy \(\Pi^{\mathrm{up}}_{\mathrm{p}}\) satisfying **H2**. Considering some action \(a\in\mathcal{A}\), we have that after the binary search batch \(D=\lceil\log_{2}T^{\beta}\rceil\): \(\mathbb{P}(\underline{\tau}_{a}(D)\leqslant\tau_{a}^{\star}\leqslant\overline {\tau}_{a}(D)\leqslant\underline{\tau}_{a}+3/T^{\beta})\geqslant 1-D/T^{\alpha\zeta}\)._

Proof of Lemma 6.: We suppose that the event \(\bigcap_{d\in[\lceil\log_{2}T^{\beta}\rceil]}\{\tilde{\mathfrak{R}}^{\mathrm{up }}_{\mathrm{p}}(\{k_{a,d}+1,\ldots,k_{a,d}+\tilde{T}\},\Pi^{\mathrm{up}}_{ \mathrm{p}},\Pi^{\mathrm{down}}_{\mathrm{p}})\leqslant\mathrm{C}\tilde{T}^{ \kappa}\}\) holds. Lemma 4 ensures that this event holds with probability at least \(1-\lceil\log_{2}T^{\beta}\rceil/T^{\alpha\zeta}\).

After \(D=\lceil\beta\log_{2}T\rceil\) batches of binary search on arm \(a\), we have by Lemma 5 that

\[|\tau_{a}(D)-\underline{\tau}_{a}(D)|\leqslant 1/2^{D}+2/T^{\beta}\leqslant 1/2^{ \beta\log_{2}T}+2/T^{\beta}=3/T^{\beta}\.\]

Lemma 5 guarantees that \(\tau_{a}^{\star}\in[\underline{\tau}_{a}(D);\overline{\tau}_{a}(D)]\) with probability at least \(1-D/T^{\alpha\zeta}\), and we obtain \(\underline{\tau}_{a}(D)\leqslant\tau_{a}^{\star}\leqslant\overline{\tau}_{a} (D)\leqslant\underline{\tau}_{a}(D)+3/T^{\beta}\) with the same probability. 

**Proposition 1**.: _Under **H1** and **H2**, after the first phase of \(\mathtt{BELGIC}\) which consists in \(K\lceil T^{\alpha}\rceil\lceil\log T^{\beta}\rceil\) steps of binary search grouped in \(\lceil\log_{2}T^{\beta}\rceil\) batches per arm \(a\in\mathcal{A}\), we have that_

\[\mathbb{P}\Big{(}\text{ for any }a\in\mathcal{A},\hat{\tau}_{a}-4/T^{\beta}- \mathrm{C}T^{(\kappa-1)/2}\leqslant\tau_{a}^{\star}\leqslant\hat{\tau}_{a} \Big{)}\geqslant 1-K\lceil\log_{2}T^{\beta}\rceil/T^{\alpha\zeta}\.\]

Proof of Proposition 1.: We consider a number of binary search batches \(D=\lceil\log T^{\beta}\rceil\) and we define the event \(\mathcal{G}\) as \(\mathcal{G}=\big{\{}\text{for any }a\in\mathcal{A},\underline{\tau}_{a}(D) \leqslant\tau_{a}^{\star}\leqslant\overline{\tau}_{a}(D)\leqslant\underline{ \tau}_{a}(D)+3/T^{\beta}\big{\}}\). We have that

\[\mathbb{P}(\mathcal{G}) =\mathbb{P}\Bigg{(}\bigcap_{a\in\mathcal{A}}\big{\{}\,\underline{ \tau}_{a}(D)\leqslant\tau_{a}^{\star}\leqslant\overline{\tau}_{a}(D) \leqslant\underline{\tau}_{a}+3/T^{\beta}\big{\}}\Bigg{)}\] \[=1-\mathbb{P}\Bigg{(}\bigcup_{a\in\mathcal{A}}\big{\{}\,\underline {\tau}_{a}(D)\leqslant\tau_{a}^{\star}\leqslant\overline{\tau}_{a}(t) \leqslant\underline{\tau}_{a}(D)+3/T^{\beta}\big{\}}^{\mathrm{c}}\Bigg{)}\] \[\geqslant 1-\sum_{a\in\mathcal{A}}\mathbb{P}\Big{(}\{\underline{ \tau}_{a}(D)\leqslant\tau_{a}^{\star}\leqslant\overline{\tau}_{a}(D) \leqslant\underline{\tau}_{a}(D)+3/T^{\beta}\big{\}}^{\mathrm{c}}\Big{)}\,\]

where the last inequality holds with an union bound. Lemma 6 with \(D=\lceil\log_{2}T^{\beta}\rceil\) ensures that we have

\[\mathbb{P}\Big{(}\big{\{}\underline{\tau}_{a}(D)\leqslant\tau_{a}^{\star} \leqslant\overline{\tau}_{a}(D)\leqslant\underline{\tau}_{a}(D)+3/T^{\beta} \big{\}}^{\mathrm{c}}\Big{)}\leqslant D/T^{\alpha\zeta}\,\]

and since \(\text{Card}\{\mathcal{A}\}=K\), we obtain

\[\mathbb{P}(\mathcal{G})\geqslant 1-K\lceil\log_{2}T^{\beta}\rceil/T^{\alpha \zeta}\.\]

Since the estimated incentives are defined as \(\hat{\tau}_{a}=\overline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)+1/T^{\beta }+\mathrm{C}T^{(\kappa-1)/2}\), we can conclude

\[\mathbb{P}(\text{ for any }a\in\mathcal{A},\hat{\tau}_{a}-4/T^{\beta}- \mathrm{C}T^{(\kappa-1)/2}\leqslant\tau_{a}^{\star}\leqslant\hat{\tau}_{a} )\geqslant 1-K\lceil\log_{2}T^{\beta}\rceil/T^{\alpha\zeta}\,\]

since whenever \(\underline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)\leqslant\tau_{a}^{\star} \leqslant\overline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)\leqslant \underline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)+1/T^{\beta}\), we also have by definition: \(\hat{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)-4/T^{\beta}-\mathrm{C}T^{( \kappa-1)/2}\leqslant\underline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil) \leqslant\tau_{a}^{\star}\leqslant\hat{\tau}_{a}(\lceil\log_{2}T^{\beta} \rceil)\). 

**Theorem 3**.: _Assume that **H1** and **H2** hold. Then \(\mathtt{BELGIC}\), run with \(\alpha,\beta\) satisfying (13) and any bandit subroutine \(\mathtt{Bandit\_Alg}\), has an overall regret \(\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}\) such that_

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\Pi_{\mathrm{p}}^{ \mathrm{up}},\mathtt{BELGIC}) \leqslant 2(3+2\mathrm{C}+\bar{v}-\underline{v})\log_{2}(T)(2T^{1- \alpha\zeta}+T^{(\kappa+1)/2}+\lceil T^{\alpha}\rceil)+4T^{1-\beta}\] \[\quad+\mathfrak{R}_{\mathtt{Bandit\_Alg}}(T,\nu,\{\hat{\tau}_{a} \}_{a\in\mathcal{A}})\]

_where, for ease of notation_

\[\bar{v}=\max_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{\mathrm{down}}(a,b)\} \quad\text{ and }\quad\underline{v}=\min_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{ \mathrm{down}}(a,b)\}\.\]

Proof of Theorem 3.: Suppose that the conditions of Theorem 3 are satisfied. By definition, \(\Lambda_{K+1}+1\in[T]\) is the step at which starts the run of the subroutine \(\mathtt{Bandit\_Alg}\), since \(\Lambda_{K+1}=K\lceil T^{\alpha}\rceil\lceil\beta\log T\rceil\). All the binary searche batches have length \(\tilde{T}=\lceil T^{\alpha}\rceil\). For any \(a\in\mathcal{A},d\in\{1,\ldots,\lceil\log_{2}T^{\beta}\rceil\}\), we define the event

\[\mathsf{B}_{a,d}=\Big{\{}\tilde{\mathfrak{R}}_{\mathrm{p}}^{\mathrm{up}}(\{k_{a,d }+1,\ldots,k_{a,d}+\tilde{T}\},\Pi_{\mathrm{p}}^{\mathrm{up}},\Pi_{\mathrm{p}}^{ \mathrm{down}})\leqslant\mathrm{C}\tilde{T}^{\kappa}\Big{\}}\,\]

as well as

\[\mathcal{E}=\bigcap_{\begin{subarray}{c}a\in[K]\\ d\in[\lceil\beta\log T\rceil]\end{subarray}}\mathsf{B}_{a,d}\bigcap\Big{\{} \tilde{\mathfrak{R}}_{\mathrm{p}}^{\mathrm{up}}(\{\Lambda_{K+1}+1,\ldots,T\}, \Pi_{\mathrm{p}}^{\mathrm{up}})\leqslant\mathrm{C}(T-\Lambda_{K+1})^{\kappa} \Big{\}}\,\]

[MISSING_PAGE_EMPTY:25]

After the binary search, at each step \(t\in\{\Lambda_{K+1}+1,\ldots,T\}\), Bandit-Alg recommends \((\tilde{a}_{t},B_{t})\in\mathcal{A}\times\mathcal{A}\) following (15) and BELGIC offers an incentive \((\tilde{a}_{t},\hat{\tau}_{\hat{a}_{t}})\) with \(\hat{\tau}_{a}=\overline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)+1/T^{\beta} +\mathrm{CT}^{(\kappa-1)/2}\).

Lemma 5 ensures that \(\mathcal{E}\subseteq\{\tau_{a}^{\star}\in[\mathbb{I}_{\mathcal{L}}(\lceil \log_{2}T^{\beta}\rceil),\overline{\tau}_{a}(\lceil\log_{2}T^{\beta}\rceil)]\) for any \(a\in\mathcal{A}\}\bigcap\{\tilde{\mathfrak{P}}_{\mathrm{p}}^{\mathrm{up}}(\{ \Lambda_{K+1}+1,\ldots,T\},\Pi_{\mathrm{p}}^{\mathrm{up}})\leqslant\mathrm{C}( T-\Lambda_{K+1})^{\kappa}\}\). Therefore, if \(\mathcal{E}\) holds, for any \(a\in\mathcal{A}\), we have that

\[v^{\mathrm{up}}(a)+\hat{\tau}_{a} =v^{\mathrm{up}}(a)+\overline{\tau}_{a}+1/T^{\beta}+\mathrm{CT}^{ (\kappa-1)/2}\] \[>v^{\mathrm{up}}(a)+\tau_{a}^{\star}+\mathrm{CT}^{(\kappa-1)/2}\] \[=v^{\mathrm{up}}(a)+\max_{a^{\prime\prime}\in\mathcal{A}}v^{ \mathrm{up}}(a^{\prime\prime})-v^{\mathrm{up}}(a)+\mathrm{CT}^{(\kappa-1)/2}\;,\]

and therefore, for any \(a^{\prime}\in\mathcal{A}\) such that \(a^{\prime}\neq a\), we have that

\[v^{\mathrm{up}}(a)+\hat{\tau}_{a}>v^{\mathrm{up}}(a^{\prime})+\mathrm{CT}^{ (\kappa-1)/2}\;.\] (27)

This shows that at any steps \(t\in\{\Lambda_{K+1}+1,\ldots,T\}\) after the binary search, we have on the event \(\mathcal{E}\) that

\[\tilde{a}_{t}=\mathrm{argmax}_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{ \prime})+\mathbb{1}_{\tilde{a}_{t}}(a^{\prime})\hat{\tau}_{\tilde{a}_{t}}\}\;,\] (28)

and the reward gap at step \(t\) for any \(a\neq\tilde{a}_{t}\) is defined as

\[\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})+\mathbb{1}_{ \tilde{a}_{t}}(a^{\prime})\hat{\tau}_{\tilde{a}_{t}}\}-(v^{\mathrm{up}}(a)+ \mathbb{1}_{\tilde{a}_{t}}(a)\hat{\tau}_{\tilde{a}_{t}})=v^{\mathrm{up}}( \tilde{a}_{t})+\hat{\tau}_{a}-v^{\mathrm{up}}(a)\;.\] (29)

Following (27), the reward gap from (29) satisfies

\[\max_{a^{\prime}\in\mathcal{A}}\{v^{\mathrm{up}}(a^{\prime})+\mathbb{1}_{ \tilde{a}_{t}}(a^{\prime})\hat{\tau}_{\tilde{a}_{t}}\}-(v^{\mathrm{up}}(a)+ \mathbb{1}_{\tilde{a}_{t}}(a)\hat{\tau}_{\tilde{a}_{t}})\geqslant\mathrm{CT}^ {(\kappa-1)/2}\;.\]

We now define two sets

\[\mathrm{I}_{T} =\{t\in\{K\lceil T^{\alpha}\rceil\lceil\log_{2}T\rceil+1,\ldots,T\} \text{ such that }\tilde{a}_{t}=A_{t}\}\;,\] \[\mathrm{J}_{T} =\{t\in\{K\lceil T^{\alpha}\rceil\lceil\log_{2}T\rceil+1,\ldots, T\}\text{ such that }\tilde{a}_{t}\neq A_{t}\}\;,\]

which satisfy \(\mathrm{I}_{T}\cup\mathrm{J}_{T}=\{K\lceil T^{\alpha}\rceil\lceil\log_{2}T \rceil+1,\ldots,T\}\) almost surely. As shown in (28), \(\mathrm{I}_{T}\) corresponds to all the steps during which the upstream player picked the best arm and for any \(t\in\mathrm{I}_{T}\)

\[v^{\mathrm{up}}(A_{t})+\mathbb{1}_{\tilde{a}_{t}}(A_{t})\tau(t)\geqslant\max_ {a\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{a}_{t}}(a)\tau(t)\}\;,\]

while by (29), for any \(t\in\mathrm{J}_{T}\), we have that

\[\max_{a\in\mathcal{A}}\{v^{\mathrm{up}}(a)+\mathbb{1}_{\tilde{a}_{t}}(a)\hat{ \tau}_{\tilde{a}_{t}}\}-(v^{\mathrm{up}}(A_{t})+\mathbb{1}_{\tilde{a}_{t}}(A_ {t})\hat{\tau}_{\tilde{a}_{t}})\geqslant\mathrm{CT}^{(\kappa-1)/2}\;.\] (30)

**H2** ensures that if \(\mathcal{E}\) holds, then \(\mathfrak{N}_{\mathrm{p}}^{\mathrm{up}}(\{\Lambda_{K+1}+1,\ldots,T\},\Pi_{ \mathrm{p}}^{\mathrm{up}},\texttt{BELGIC})\leqslant\mathrm{C}\,T^{\kappa}\), and this condition together with (30) gives that

\[\mathrm{Card}\{\mathrm{J}_{T}\}\,\mathrm{C}\,T^{(\kappa-1)/2}\leqslant \mathfrak{N}_{\mathrm{p}}^{\mathrm{up}}(\{\Lambda_{K+1}+1,\ldots,T\},\Pi_{ \mathrm{p}}^{\mathrm{up}},\texttt{BELGIC})\leqslant\mathrm{CT}^{\kappa}\;,\]and consequently \(\text{Card}\{\text{J}_{T}\}\leqslant T^{(\kappa+1)/2}\). We now bound \((\mathbf{B})\) as follows

\[\mathbb{E}[\mathbb{1}(\mathcal{E})(\mathbf{B})] =\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t=\Lambda_{K+1}+1} ^{T}\mu^{*,\text{down}}-\big{(}v^{\text{down}}(A_{t},B_{t})-\hat{\tau}_{\hat{a }_{t}}\big{)}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t\in\text{J}_{T} }\max_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{\text{down}}(a,b)-\tau_{a}^{ \star}\}-\big{(}v^{\text{down}}(\tilde{a}_{t},B_{t})-\hat{\tau}_{\hat{a}_{t}} \big{)}\Bigg{]}\] \[\quad+\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t\in\text{J }_{T}}\underbrace{\mu^{*,\text{down}}-\big{(}v^{\text{down}}(A_{t},B_{t})- \hat{\tau}_{\hat{a}_{t}}\big{)}}_{\leqslant 3+\text{C}+\bar{v}-\underline{v}} \Bigg{]}\] \[\leqslant\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t\in \text{I}_{T}}\max_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{\text{down}}(a,b)- \hat{\tau}_{a}-(v^{\text{down}}(\tilde{a}_{t},B_{t})-\hat{\tau}_{\hat{a}_{t}}) \}+\max_{a^{\prime}\in\mathcal{A}}\{\hat{\tau}_{a^{\prime}}-\tau_{a^{\prime}} ^{\star}\}\Bigg{]}\] \[\quad+(3+\text{C}+\bar{v}-\underline{v})\mathbb{E}[\mathbb{1}( \mathcal{E})\text{Card}\{\text{J}_{T}\}]\] \[=\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t\in\text{I}_{T} }\max_{a,b\in\mathcal{A}\times\mathcal{A}}\{v^{\text{down}}(a,b)-\hat{\tau}_{a }\}-(v^{\text{down}}(\tilde{a}_{t},B_{t})-\hat{\tau}_{\hat{a}_{t}})\Bigg{]}\] \[\quad+\mathbb{E}\Bigg{[}\mathbb{1}(\mathcal{E})\sum_{t\in\text{I }_{T}}\max_{a^{\prime}\in\mathcal{A}}\{\hat{\tau}_{a^{\prime}}-\tau_{a^{\prime }}^{\star}\}\Bigg{]}+(3+\text{C}+\bar{v}-\underline{v})\,T^{(\kappa+1)/2}\] \[\leqslant\mathfrak{R}_{\texttt{Bandit-Alg}}(\text{Card}\{\text{I} _{T}\},\nu,\{\hat{\tau}_{a}\}_{a\in\mathcal{A}})+\mathbb{E}\bigg{[}\mathbb{1}( \mathcal{E})\text{Card}\{\text{I}_{T}\}\max_{a^{\prime}\in\mathcal{A}}\{\hat{ \tau}_{a^{\prime}}-\tau_{a^{\prime}}^{\star}\}\bigg{]}\] \[\quad+(3+\text{C}+\bar{v}-\underline{v})\,T^{(\kappa+1)/2}\;,\]

where the first step holds by Lemma 1. Using Lemma 5, as well as the definition of \(\mathcal{E}\), we obtain

\[\mathbb{E}\bigg{[}\mathbb{1}(\mathcal{E})\text{Card}\{\text{I}_{T} \}\max_{a^{\prime}\in\mathcal{A}}\{\hat{\tau}_{a^{\prime}}-\tau_{a^{\prime}}^{ \star}\}\bigg{]} \leqslant\mathbb{E}\Big{[}\mathbb{1}(\mathcal{E})\text{Card}\,\{ \mathcal{E}\}(4/T^{\beta}+\text{CT}^{(\kappa-1)/2})\Big{]}\] \[\leqslant\mathbb{E}[\mathbb{1}(\mathcal{E})T](4/T^{\beta}+\text{ CT}^{(\kappa-1)/2})\] \[\leqslant 4T^{1-\beta}+\text{CT}^{(\kappa+1)/2}\;,\]

which finally gives

\[\mathbb{E}[\mathbb{1}(\mathcal{E})(\mathbf{B})]\leqslant\mathfrak{R}_{ \texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a}\}_{a\in\mathcal{A}})+4T^{1-\beta}+( 3+2\text{C}+\bar{v}-\underline{v})\,T^{(1+\kappa)/2}\;,\] (31)

and plugging together (26) and (31) in the decomposition (25) gives the following bound

\[\mathbb{E}\Big{[}\mathbb{1}(\mathcal{E})\tilde{\mathfrak{R}}_{ \text{p}}^{\text{down}}(T,\Pi_{\text{p}}^{\text{up}},\texttt{BELGIC})\Big{)} \Big{]} \leqslant\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a} \}_{a\in\mathcal{A}})+4T^{1-\beta}+(3+2\text{C}+\bar{v}-\underline{v})\,T^{(1+ \kappa)/2}\] \[\quad+(3+\text{C}+\bar{v}-\underline{v})K\lceil T^{\alpha}\rceil(1+ \log_{2}T)\;,\]

and summing the bounds on the events \(\mathcal{E}\) and \(\mathcal{E}^{c}\) finally gives

\[\mathfrak{R}_{\text{p}}^{\text{down}}(T,\Pi_{\text{p}}^{\text{up}}, \texttt{BELGIC}) \leqslant\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a} \}_{a\in\mathcal{A}})+4T^{1-\beta}+(3+2\text{C}+\bar{v}-\underline{v})\,T^{(1+ \kappa)/2}\] \[\quad+(2+\text{C}+\bar{v}-\underline{v})K\lceil T^{\alpha}\rceil(1+ \log_{2}T)\] \[\quad+(3+\text{C}+\bar{v}-\underline{v})(K(1+\log_{2}T)T^{1-\alpha \zeta}+T^{1-\zeta})\] \[\leqslant\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a} \}_{a\in\mathcal{A}})+4T^{1-\beta}+(3+2\text{C}+\bar{v}-\underline{v})\,T^{(1+ \kappa)/2}\] \[\quad+(3+\bar{C}+\bar{v}-\underline{v})((1+\log_{2}T)(\lceil T^{ \alpha}\rceil+T^{1-\alpha\zeta})+T^{1-\zeta})\] \[\leqslant(3+2\text{C}+\bar{v}-\underline{v})(T^{1-\zeta}+T^{( \kappa+1)/2}+(1+\log_{2}T)(\lceil T^{\alpha}\rceil+T^{1-\alpha\zeta}))\] \[\quad+\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a} \}_{a\in\mathcal{A}})+4T^{1-\beta}\] \[\leqslant 2(3+2\text{C}+\bar{v}-\underline{v})\log_{2}(T)(2T^{1-\alpha \zeta}+T^{(\kappa+1)/2}+\lceil T^{\alpha}\rceil)+4T^{1-\beta}\] \[\quad+\mathfrak{R}_{\texttt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a}\}_{a \in\mathcal{A}})\;.\]

**Corollary 1**.: _Assume that the upstream player's distribution \((\gamma_{a})_{a\in\mathcal{A}}\) is such that **H** 1 holds. In addition, suppose that the distributions \((\gamma_{a})_{a\in\mathcal{A}}\) and \((\nu_{a,b})_{a,b\in\mathcal{A}\times\mathcal{A}}\) are \(1\)-sub-Gaussian and that the upstream player plays \(\Pi_{\mathrm{p}}^{\mathrm{up}}=Algorithm\ 3\) (a slight modification of UCB to take into account the incentives). Then the downstream player's regret when she runs \(\mathtt{BELGIC}\) with parameters \(\alpha=3/4\) and \(\beta=1/4\) (which satisfy (13)) and subroutine \(\mathtt{Bandit-Alg}=\mathtt{UCB}\) satisfies the following upper bound4_

Footnote 4: Note that it is \(K\) and not \(\sqrt{K}\) here, since the action space is of cardinality \(K^{2}\) for the downstream player.

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\mathtt{UCB}, \mathtt{BELGIC}) \leqslant(10+4K+32\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v}) \log_{2}(T)(3+2T^{3/4})\] \[\quad+3K^{2}(\bar{v}-\underline{v})\.\]

Proof of Corollary 1.: First note that \(\Pi_{\mathrm{p}}^{\mathrm{up}}=Algorithm\ 3\) satisfies \(\mathbf{H}2\) with constants \(\kappa=1/2\), \(\zeta=2\), \(\mathrm{C}=8\sqrt{K\log(KT^{3})}\), following Proposition 2. Note that \(\beta/\alpha=1/3<1/2=1-\kappa\), therefore Equation (13) is satisfied. Plugging these terms in the bound from Theorem 3 with \(\alpha=3/4,\beta=1/4\) gives

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\mathtt{UCB}, \mathtt{BELGIC}) \leqslant 2(3+16\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v})\log_{2}(T )(2T^{1-3/2}+T^{3/4}+\lceil T^{3/4}\rceil)\] \[\quad+4T^{1/4}+8\sqrt{K^{2}\log_{2}(T)}\ T^{1/2}+3K^{2}(\bar{v}- \underline{v})\]

where we use the bound for \(\mathfrak{R}_{\mathtt{Bandit-Alg}}(T,\nu,\{\hat{\tau}_{a}\}_{a\in\mathcal{A}})\) with \(\mathtt{Bandit-Alg}=\mathtt{UCB}\) run on any bandit instance with \(K^{2}\) arms, \(1\)-subgaussian rewards and reward gaps of at most \(\max_{a,b\in\mathcal{A}\times\mathcal{A}}v^{\mathrm{down}}(a,b)-\min_{a,b\in \mathcal{A}\times\mathcal{A}}v^{\mathrm{down}}(a,b)=\bar{v}-\underline{v}\), following (Lattimore and Szepesvari, 2020, Theorem 7.2). Therefore, we have that

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\mathtt{UCB}, \mathtt{BELGIC}) \leqslant(6+32\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v}) \log_{2}(T)(2+1+2T^{3/4})\] \[\quad+4T^{1/4}+3K^{2}(\bar{v}-\underline{v})+8K\sqrt{\log_{2}T}T^ {1/2}\] \[\leqslant(10+32\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v}) \log_{2}(T)(3+2T^{3/4})\] \[\quad+3K^{2}(\bar{v}-\underline{v})+8K\sqrt{\log_{2}(T)}\,T^{1/2}\,\]

which finally gives

\[\mathfrak{R}_{\mathrm{p}}^{\mathrm{down}}(T,\mathtt{UCB}, \mathtt{BELGIC}) \leqslant(10+4K+32\sqrt{K\log_{2}(KT^{3})}+\bar{v}-\underline{v}) \log_{2}(T)(3+2T^{3/4})\] \[\quad+3K^{2}(\bar{v}-\underline{v})\,\]

hence the result.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract mostly presents the issue that we tackle in the paper, namely presenting an online version of the _Coase theorem_. This contribution is the most important part of our paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Our work presents a model to explore an online version of a theory from economics. Thereby, it implicitly has limitations due to the fact that a choice was made in the model. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Our work presents a lot of theorems supported by assumptions (see **H**2, Theorems 2 and 3...). All of Appendix C is here to provide a theoretical support to these results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA]Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g., negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [NA] Justification: Since our work is mostly a theoretic contribution, we do not present experiments. Therefore, our paper is not concerned by this issue (same answer as for item 5). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: As explained in the Related Works, an issue with externalities is the harm caused to all the parties involved in the considered setting. Here, we try to provide a setup so the players can negotiate and achieve a social welfare optimal equilibrium. Therefore, we are confident in the fact that we follow the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We consider an issue arising in several real-world situations and provide an algorithmic solution to it. We realize that this theoretical contribution still needs some work to be implemented but we hope that it will contribute to positive social impacts in the future. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Our work is a theoretical contribution. Thereby, it does not propose a release of data or any kind of trained model. We mix learning and theories from economics, which does not involve yet risks for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use any assets requiring such conditions here. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not present such kind of new assets. Guidelines:* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not incorporate any experiment involving human subjects. Therefore, we are not concerned by this item. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not incorporate any experiment involving human subjects, hence our answer. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.