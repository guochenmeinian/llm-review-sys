# Global Identifiability of \(\ell_{1}\)-based Dictionary Learning via Matrix Volume Optimization

 Jingzhou Hu &Kejun Huang

Department of Computer and Information Science and Engineering

University of Florida

Gainesville, FL 32611

{jingzhouhu,kejun.huang}@ufl.edu

###### Abstract

We propose a novel formulation for dictionary learning that minimizes the determinant of the dictionary matrix, also known as its volume, subject to the constraint that each row of the sparse coefficient matrix has unit \(\ell_{1}\) norm. The main motivation for the proposed formulation is that it provides global identifiability guarantee of the groundtruth dictionary and sparse coefficient matrices, up to the inherent and inconsequential permutation and scaling ambiguity, if a set of vectors obtained from the coefficient matrix lies inside the \(\ell_{\infty}\) norm ball but contains the \(\ell_{2}\) norm ball in their convex hull. Unlike existing work on identifiability of dictionary learning, our result is global, meaning that a globally optimal solution to our proposed formulation has to be a permuted and rescaled version of the groundtruth factors. Another major improvement in our result is that there is no additional assumption on the dictionary matrix other than it is nonsingular, unlike most other works that require the atoms of the dictionary to be mutually incoherent. We also provide a probabilistic analysis and show that if the sparse coefficient matrix is generated from the widely adopted Bernoulli-Gaussian model, then it is globally identifiable if the sample size is bigger than a constant times \(k\log k\), where \(k\) is the number of atoms in the dictionary, with overwhelming probability. The bound is essentially the same as those local identifiability results, but we show that it is also global. Finally, we propose algorithms to solve the new proposed formulation, specifically one based on the linearized-ADMM with efficient per-iteration updates. The proposed algorithms exhibit surprisingly effective performance in correctly and efficiently recovering the dictionary, as demonstrated in the numerical experiments.

## 1 Introduction

Dictionary learning is a famous unsupervised learning problem [16] that also goes by the name sparse coding [10] or sparse component analysis [1]. It stems from the intuition that samples from a data set can all be approximately represented as a sparse combination of atoms from a complete or overcomplete dictionary, or \(\bm{x}_{l}=\bm{A}\bm{s}_{i}\) for \(i=1,\ldots,n\), where \(\bm{x}_{l}\in\mathbb{R}^{m}\) are the given data points, columns of \(\bm{A}\in\mathbb{R}^{m\times k}\) represents atoms of a dictionary, and \(\bm{s}_{i}\in\mathbb{R}^{k}\) are the sparse representations of \(\bm{x}_{i}\), indicating that \(\bm{x}_{l}\) is linearly dependent on only a few atoms from the dictionary \(\bm{A}\) with coefficients given in the nonzeros of \(\bm{s}_{l}\). Staking \(\bm{x}_{i}\) as columns of \(\bm{X}\in\mathbb{R}^{m\times n}\) and \(\bm{s}_{i}\) as columns of \(\bm{S}\in\mathbb{R}^{k\times n}\), it can be written as a matrix factorization model \(\bm{X}=\bm{A}\bm{S}\). Although in some cases there exist known dictionaries that admit good-quality sparse representations, such as the DCT or wavelet basis for image compression, sometimes it is better to learn a sparse-inducing dictionary from data in an unsupervised fashion, hence the task of dictionary learning that tries to estimate \(\bm{A}\) and \(\bm{S}\) jointly. In most cases, peopleassume \(m\leq k\): the dictionary is complete if \(m=k\), and overcomplete if \(m<k\). In this paper we focus on the complete dictionary case when \(\bm{A}\) is square and nonsingular, which is common in a lot of prior works that this work is built upon (Gribonval and Schnass, 2010; Sun et al., 2016, 2016; Wu and Yu, 2017; Wang et al., 2020)

Dictionary learning has found numerous applications in signal denoising (Elad and Aharon, 2006), audio coding (Plumbley et al., 2009), and medical imaging (Tosic et al., 2010), to name just a few. On the theory side, most of the existing works have focused on algorithm design. Famous algorithms include \(k\)-SVD (Aharon et al., 2006a) and online dictionary learning (Mairal et al., 2009), among numerous other algorithms based on generic nonconvex algorithm design with guarantee of convergence to a stationary point. More recently, there have appeared a line of research that attempts to show global optimality for dictionary learning under more restrictive assumptions, such as (Spielman et al., 2012; Agarwal et al., 2016; Arora et al., 2014, 2015; Sun et al., 2016, 2016, 2019; Bai et al., 2019; Zhai et al., 2020, 2020, 2020; Tolooshams and Ba, 2022).

### Model identifiability

The dictionary learning problem can be considered an instance of the broader matrix factorization model \(\bm{X}=\bm{A}\bm{S}\) subject to the opaque requirement that \(\bm{S}\) is sparse. It is well-known that matrix factorization without any additional assumptions on the latent factors is not unique, since we can always "insert" an invertible matrix \(\bm{Q}\) and \(\bm{Q}^{-1}\) as \(\bm{X}=\widehat{\bm{A}\bm{S}}\) where \(\widehat{\bm{A}}=\bm{A}\bm{Q}\) and \(\widehat{\bm{S}}=\bm{Q}^{-1}\bm{S}\), and one cannot distinguish whether \(\bm{S}\) or \(\widehat{\bm{S}}\) are the groundtruth sources. Such rotation ambiguity cannot be resolved by the well-known principal component analysis (PCA) (Jolliffe, 2002).

On the other hand, most matrix factorization models allow scaling and permutation ambiguity, i.e., a permutation matrix \(\bm{\Pi}\) and a diagonal matrix \(\bm{D}\) such that the recovered mixing matrix is \(\bm{AD\Pi}\) and the recovered sources are \(\bm{\Pi}^{T}\bm{D}^{-1}\bm{S}\). In addition to inherent constraints that fit the application, it is sometimes helpful to have an optimization criterion in search for the "correct" factorization (up to scaling and permutation). In light of this, we formally define the identifiability of any matrix factorization model as follows:

**Definition 1.1** (Identifiability).: Consider the generative model \(\bm{X}=\bm{A}^{\natural}\bm{S}^{\natural}\), where \(\bm{A}^{\natural}\) and \(\bm{S}^{\natural}\) are the groundtruth latent factors. Let \((\bm{A}^{\star},\bm{S}^{\star})\) be optimal for an identification criterion \(q\)

\[(\bm{A}^{\star},\bm{S}^{\star})=\operatorname*{arg\,min}_{\bm{X}=\bm{A}\bm{S} \atop\bm{A}\in\mathcal{A},\bm{S}\in\mathcal{S}}q(\bm{A},\bm{S}),\]

where \(\mathcal{A}\) and \(\mathcal{S}\) are some constraint set that \(\bm{A}\) and \(\bm{S}\) should belong to, respectively. If \(\bm{A}^{\natural}\) and/or \(\bm{S}^{\natural}\) satisfy some condition such that for any \((\bm{A}^{\star},\bm{S}^{\star})\), there exist a permutation matrix \(\bm{\Pi}\) and a diagonal matrix \(\bm{D}\) such that \(\bm{A}^{\natural}=\bm{A}^{\star}\bm{D}\bm{I}\) and \(\bm{S}^{\natural}=\bm{\Pi}^{T}\bm{D}^{-1}\bm{S}^{\star}\), then we say that the matrix factorization model is essentially identifiable, up to permutation and scaling, under that condition.

For dictionary learning in specific, where the essential constraint on the latent factors is sparsity on \(\bm{S}\), there have been some existing works on its identifiability issue. It has been shown that simply by putting a hard sparsity constraint on \(\bm{S}\) is enough to guarantee a unique factorization, provided that the sample size \(n\) is large enough (Aharon et al., 2006b; Hillar and Sommer, 2015; Cohen and Gillis, 2019). The main drawback is that the required sample size \(n\) is usually too large to be practical--results from (Aharon et al., 2006b) and (Hillar and Sommer, 2015) both require \(n\) to be \(O((k+1){k\choose s})\), where \(s\) is the number of nonzeros that each column of \(\bm{S}\) is allowed to have; it has been relaxed to \(O(k^{3}/(k-s)^{2})\) by Cohen and Gillis (2019), but still very large. The existence of \(s\) in the bound also shows that an outlier sample that does not admit a sparse representation from the dictionary is absolutely not allowed in the data set, which is also restrictive. Inspired by the success of compressive sensing and sparse recovery, it is perhaps more common to use the \(\ell_{1}\) norm to approximately sparsify \(\bm{S}\). Identifiability results based on the \(\ell_{1}\) norm formulation has been predominantly local, meaning the model is identifiable within a neighborhood of the groundtruth factors \((\bm{A}^{\natural},\bm{S}^{\natural})\), but on the bright side the sample size requirement is typically down to \(O(k\log k)\) and allows the existence of dense outliers (Gribonval and Schnass, 2010; Wu and Yu, 2017; Wang et al., 2020).

### This paper

In this paper, we intend to bridge the gap by providing _global_ identifiability guarantee of complete dictionary recovery using a \(\ell_{1}\) norm based formulation, given as follows:

\[\operatorname*{minimize}_{\bm{A},\bm{S}}\ \ |\det\bm{A}|\qquad\text{subject to}\ \ \bm{X}=\bm{A}\bm{S},\|\bm{S}_{j,\cdot}\|_{1}\leq 1,j=1, \ldots,k,\] (1)

To the best of our knowledge, this is a novel formulation that has never appeared in the literature; however, as we will show in the next section, the intuition stems from the more commonly used \(\ell_{1}\)-based formulations. We stress that the formulation is \(\ell_{1}\)-based, as the \(\ell_{1}\) norm is the most widely used term to promote sparsity of the solution. Although in the context of dictionary learning, using the \(\ell_{1}\) norm as the convex surrogate for sparsity is not enough to make the overall problem tractable, there are still benefits of using it instead of other ones such as hard sparsity constraints, for example in terms of sample complexity as introduced in the previous subsection.

After some gentle derivation to motivate the proposed new formulation (1) for dictionary learning, we provide the following contributions:

1. We give a deterministic characterization of the identifiability condition on the sparse coefficient matrix \(\bm{S}\) under which the dictionary (as well as the sparse coefficients) can be uniquely determined, up to the inherent and inconsequential permutation and scaling ambiguity, via solving (1). Unlike existing work that shows local identifiability, our result is global, meaning that the groundtruth factors have to be a global minimizer to (1). Furthermore, perhaps surprisingly, our result only require the complete dictionary to be nonsingular--there are no additional assumptions that require the atoms to be mutually incoherent whatsoever.
2. The result shows that \(\ell_{1}\)-based dictionary learning is globally identifiable if a certain set of points, which are by definition inside the hypercube \([-1,1]^{k}\), contains the unit ball in their convex hull. This geometric assumption is called "sufficiently scattered", with similar ideas appearing in pertinent unsupervised learning models. We provide intuition why this assumption generally leads to a sparse \(\bm{S}\). We further show that if the sparse coefficient follows the widely used Bernoulli-Gaussian generative model, then it is identifiable with very high probability, provided the sample size \(n\) is bigger than a constant times \(k\log(k)\). This is the same sample complexity bound obtained in some of the local identifiability analysis, but we show that it is also global, using the proposed new formulation (1).
3. We propose algorithms to solve (1), specifically one based on linearized-ADMM in the main paper; in the supplementary we also present one based on the Frank-Wolfe framework and the other based on block coordinate descent. Even though the proposed formulation (1) is nonconvex (just like any other dictionary learning formulation), numerical experiments on synthetic data show that they achieve global optimality in almost all cases, provided that the groundtruth latent factors satisfy the identifiability assumptions given before.

## 2 Problem Formulation

We start by proposing a novel formulation for dictionary learning. The purpose is to show that the seemingly different formulation does not come out of thin air, but is highly related, if not equivalent, to existing ones that have been studied extensively. Consider the following formulation adopted in [10, 11, 12, 13]:

\[\operatorname*{minimize}_{\bm{A},\bm{S}}\ \ \ \ \|\bm{S}\|_{1}\qquad\text{ subject to}\ \ \bm{X}=\bm{A}\bm{S},\|\bm{A}_{\cdot,j}\|\leq 1,j=1, \ldots,k.\] (2)

We use the noiseless model \(\bm{X}=\bm{A}\bm{S}\) since the main focus of this work is identifiability, and a lot of algorithms are designed by replacing this term with a loss function that measures the residual \(\bm{X}-\bm{A}\bm{S}\), e.g. [10]. The purpose of the \(\ell_{1}\) norm of \(\bm{S}\) is to encourage it to be sparse, but since it is also dependent on the scaling of \(\bm{S}\), the constraint \(\|\bm{A}_{\cdot,j}\|\leq 1\) is imposed onto the columns of \(\bm{A}\). Such constraints are without loss of generality due to the scaling ambiguity \(\bm{A}\bm{S}=\bm{A}\bm{D}\bm{D}^{-1}\bm{S}\), so for any admissible factorization, we can rescale the columns of \(\bm{A}\) to unit norm, while absorbing the scaling into the rows of \(\bm{S}\), without affecting the matrix product.

Due to the Lagrangian multiplier theorem, there exist positive scalars \(\beta_{1},\ldots,\beta_{k}\) and \(\gamma_{1},\ldots,\gamma_{k}\) such that any solution of the following problem is also optimal for (2)\[\underset{\bm{A},\bm{S}}{\text{minimize}}\ \ \sum_{j=1}^{k}\gamma_{j}\|\bm{A}_{:j}\|\qquad \text{subject to}\ \ \bm{X}=\bm{A}\bm{S},\|\bm{S}_{j}.\|_{1}\leq\beta_{j},j=1,\ldots,k,\] (3)

since they would lead to exactly the same Lagrangian function with appropriate choices of \(\gamma_{j}\) and \(\beta_{j}\). Again, due to the inherent scaling ambiguity, we can assume without loss of generality that \(\beta_{1}=\cdots=\beta_{k}=1\), leaving the unknown parameters to just \(\gamma_{j}\)'s.

If we replace the summation in the objective of (3) with product, or equivalently replacing the arithmetic mean with the geometric mean, the term \(\prod_{j}\gamma_{j}\|\bm{A}_{:j}\|\) would be intimately related to \(|\det\bm{A}|\) if \(\bm{A}\) is square. Indeed, there must exist some unknown matrix \(\bm{G}\) such that \(|\det\bm{G}\bm{A}|=\prod_{j}\gamma_{j}\|\bm{A}_{:j}\|\). Furthermore, for the purpose of optimization it is completely unnecessary to know \(\bm{G}\) since \(|\det\bm{G}\bm{A}|=|\det\bm{G}||\det\bm{A}|\), which is just a positive scaling of \(|\det\bm{A}|\) that does not affect the optimization solution. As a result, our proposed novel formulation of dictionary learning takes the form of (1).

Although our paper focuses on the case of complete dictionaries, i.e., when \(\bm{A}\) is square and nonsingular, they can be effortlessly extended to the overdetermined case when \(\bm{A}\) is tall, and the only modification is to change the objective to \(\det\bm{A}^{\top}\bm{A}\). This is sometimes called the volume of the matrix [1] with applications in change-of-variable integration [1] and probability [1]. It has been used as the identification criterion for several variants of nonnegative matrix factorization with an interesting geometric interpretation of finding the minimum volume enclosing simplex of a set of points [11, 12, 13, 14, 15, 16]. The most related work is the polytopic matrix factorization framework proposed by Tatii and Erdogan [2021], which includes a case that constraints the _columns_ of \(\bm{S}\) to have bounded \(\bm{\ell}_{1}\) norm. Our proposed formulation constrains the rows of \(\bm{S}\), which seems to be a minor difference but is in fact a great leap forward since row scaling of \(\bm{S}\) is inherent for any matrix factorization, while column scaling is not. It also turns out that the identifiability analysis would be drastically different, albeit the formulations look similar.

## 3 Global identifiability

In this section we study the conditions under which the proposed dictionary learning formulation (1) is identifiable, i.e., any solution to (1) equals to the groundtruth dictionary \(\bm{A}^{\natural}\) and sparse coefficients \(\bm{S}^{\natural}\) up to permutation and scaling, as per Definition 1.1. The groundtruth \(\bm{S}^{\natural}\) can have arbitrary scaling, and it is without loss of generality to assume that its rows have unit \(\ell_{1}\) norms; we denote such rescaled \(\bm{S}^{\natural}\) to be \(\widetilde{\bm{S}}^{\natural}\), i.e., \(\|\widetilde{\bm{S}}^{\natural}_{j:}\|_{1}\leq 1,j=1,\ldots,k\); we absorb the counterscaling into the columns of \(\bm{A}^{\natural}\) to generate \(\widetilde{\bm{A}}^{\natural}\), so that \(\bm{X}=\bm{A}^{\natural}\bm{S}^{\natural}=\widetilde{\bm{A}}^{\natural} \widetilde{\bm{S}}^{\natural}\).

Since we consider only complete dictionaries, for any feasible \((\bm{A},\bm{S})\) for (1), there must exist a nonsingular matrix \(\bm{W}\) such that \(\bm{S}=\bm{W}\widetilde{\bm{S}}^{\natural}\) and \(\bm{A}=\widetilde{\bm{A}}^{\natural}\bm{W}^{-1}\), therefore Problem (1) is, conceptually, equivalent to

\[\underset{\bm{W}}{\text{maximize}}\ \ |\det\bm{W}|\qquad\text{subject to}\ \ \|\bm{w}^{\top} \widetilde{\bm{S}}^{\natural}\|_{1}\leq 1,j=1,\ldots,k,\] (4)

where \(\bm{w}^{\top}_{j}\) denotes the \(j\)th row of \(\bm{W}\). Since \(\bm{W}=\bm{A}^{-1}\widetilde{\bm{A}}^{\natural}\) and \(\widetilde{\bm{A}}^{\natural}\) is feasible, the optimal value of (4) is at least 1. On the other hand, if the model is identifiable, then the optimal value of (4) should be exactly equal to 1, and the optimal solution to (4) must be a permutation matrix times a diagonal \(\pm 1\) matrix. The rest of this section is dedicated to characterizing when such condition holds for the sparse coefficients \(\bm{S}\). Notice that all we need for the dictionary \(\bm{A}\) is that its columns are independent, so that the transform from (1) to (4) is possible--there is no additional assumptions such as "incoherence" for the rest of the arguments to hold, which is a great improvement to the existing works such as [17, 14, 15, 16, 17, 18].

### Deterministic characterization

The intuition is simple: the Hadamard inequality states that \(|\det\bm{W}|\leq\prod_{j}\|\bm{w}_{j}\|\), so a sufficient condition for identifiability is that the set of solutions to the following (nonconvex) quadratic programming

\[\underset{\bm{w}}{\text{maximize}}\ \ \|\bm{w}\|^{2}\quad\text{subject to}\ \ \|\bm{w}^{\top}\widetilde{\bm{S}}^{\natural}\|_{1}\leq 1\] (5)is \(\{\pm\bm{e}_{1},\ldots,\pm\bm{e}_{k}\}\); in other words, a maximizer must be an indicator vector or its negative.

To proceed, we introduce the following notion:

**Assumption 3.1** (Sufficiently scattered in the hypercube).: Let \(\mathcal{B}\) denote the Euclidean ball \(\mathcal{B}=\{\bm{x}\in\mathbb{R}^{k}\mid\|\bm{x}\|\leq 1\}\) and \(\mathcal{C}\) denote the hypercube \(\mathcal{C}=\{\bm{x}\in\mathbb{R}^{k}\mid\|\bm{x}\|_{\infty}\leq 1\}\). A set \(\mathcal{S}\) is sufficiently scattered in the hypercube if:

1. \(\mathcal{B}\subseteq\mathcal{S}\subseteq\mathcal{C}\);
2. \(\partial\mathcal{B}\cap\partial\mathcal{S}=\{\pm\bm{e}_{1},\ldots,\pm\bm{e}_{ k}\}\), where \(\partial\) denotes the boundary of the set.

A geometric illustration of a set that is sufficiently scattered in the hypercube is shown in Figure 1, which in this case is a polytope with vertices depicted as dots. As we can see, \(\mathcal{B}\) is a subset of the hypercube \([-1,1]^{k}\), but touches the boundary of \([-1,1]^{k}\) at points \(\pm\bm{e}_{1},\ldots,\pm\bm{e}_{k}\). If a set \(\mathcal{S}\) is sufficiently scattered in \([-1,1]^{k}\), \(\mathcal{S}\) contains \(\mathcal{B}\) as a subset and, as a second requirement, \(\mathcal{B}\) touches the boundary of \(\mathcal{S}\) only at those points too.

The term "sufficiently scattered" was first coined by Huang et al. (2016) to characterize the identifiability condition for nonnegative matrix factorization that has already appeared in (Huang et al., 2013). The difference is that in (Huang et al., 2013, 2016; Fu et al., 2018), the condition is defined over the conic hull of a set of points in the nonnegative orthant containing a specific hyperbolic cone. It has also been defined over the convex hull of a set of points in the probability simplex (Fu et al., 2015; Lin et al., 2015). The most related case is the sufficiently scattered condition defined for the hypercube (\(\ell_{\infty}\)-norm ball) (Tatli and Erdogan, 2021), as well as the standard orthoplex (\(\ell_{1}\)-norm ball).

To see how the sufficiently scattered condition could be useful for dictionary learning, we first rewrite the single constraint in (5) as, equivalently, a set of \(2^{n}\) linear inequalities \(\bm{w}^{\top}\widehat{\mathbf{S}}^{\natural}\bm{v}\leq 1,\forall\bm{v} \in\{\pm 1\}^{n}\). Since the rows of \(\widehat{\mathbf{S}}^{\natural}\) have been rescaled to have unit \(\ell_{1}\) norm, we must have that \(\|\widehat{\mathbf{S}}^{\natural}\bm{v}\|_{\infty}\leq 1\), meaning that all vectors \(\widehat{\mathbf{S}}^{\natural}\bm{v}\) should lie in the hypercube \([-1,1]^{k}\), and we further assume the convex hull of all such \(\widehat{\mathbf{S}}^{\natural}\bm{v}\) is sufficiently scattered. Define the \(n\times 2^{n}\) matrix \(\bm{V}\) that contains all elements of \(\{\pm 1\}^{n}\) as its columns, we have the following deterministic characterization of the identifiability of (1):

**Theorem 3.2**.: _Consider the generative model \(\bm{X}=\bm{A}^{\natural}\mathbf{S}^{\natural}\), where \(\bm{A}^{\natural}\in\mathbb{R}^{k\times k}\) is the groundtruth dictionary and \(\bm{S}\) is the groundtruth sparse coefficients. If \(\operatorname{rank}(\bm{A}^{\natural})=k\) and \(\operatorname{conv}(\mathbf{S}^{\natural}\bm{V})\) is sufficiently scattered in the hypercube as in Assumption 3.1, then for any solution of (1), denoted as \((\bm{A}^{\star},\bm{S}^{\star})\), there exist a permutation matrix \(\bm{\Pi}\) and a diagonal matrix \(\bm{D}\) such that \(\bm{A}^{\natural}=\bm{A}^{\star}\bm{D}\bm{\Pi}\) and \(\mathbf{S}^{\natural}=\bm{\Pi}^{T}\bm{D}^{-1}\bm{S}^{\star}\). In other words, dictionary learning is identifiable if the groundtruth \(\bm{A}^{\natural}\) has full column rank and \(\mathbf{S}^{\natural}\bm{V}\) is sufficiently scattered._

The full proof is relegated to Appendix A. Here we provide a concise sketch of the proof: if \(\widehat{\mathbf{S}}^{\natural}\bm{V}\) is sufficiently scattered, a solution to (5) must be \(\pm\bm{e}_{j}\); as a result, \(|\det\bm{W}|\leq\prod_{j}\|\bm{w}_{j}\|\leq 1\), and equality holds only if \(\bm{W}\) is a permutation matrix with some sign flips of its rows.

How is this assumption relevant to the sparsity of \(\mathbf{S}\)? Notice that if \(\bm{S}\bm{V}\) is sufficiently scattered, then for _any_ vector \(\bm{u}\) such that \(\|\bm{u}\|\leq 1\), there must exist a convex coefficient \(\bm{\theta}\), i.e., \(\bm{\theta}\succeq 0\) and \(\bm{\Pi}^{T}\bm{\theta}=1\), such that \(\bm{u}=\bm{S}\bm{V}\bm{\theta}\). Due to the definition of \(\bm{V}\), we know that the vector \(\bm{y}=\bm{V}\bm{\theta}\) must satisfy \(\|\bm{y}\|_{\infty}\leq 1\). Let us instantiate it at \(\bm{u}=\bm{e}_{j}\). This means the \(j\)th row of \(\bm{S}\) inner product with \(\bm{y}\) equals to \(1\). But the \(j\)th row of \(\bm{S}\) has unit \(\ell_{1}\) norm and \(\|\bm{y}\|_{\infty}\leq 1\), so their inner product is \(1\) only if \(\bm{y}^{\dagger}=\operatorname{sign}(\mathbf{S}_{j,\cdot})\), which is uniquely defined if \(\mathbf{S}_{j,\cdot}\) is completely dense. Now \(\bm{e}_{j}=\bm{S}\bm{y}\) also implies the inner products between \(\bm{y}\) and the rest of the rows of \(\mathbf{S}\) all equal to zero, which is almost impossible if \(\bm{y}^{\top}=\operatorname{sign}(\mathbf{S}_{j,\cdot})\) is the only candidate. On the other hand, if some of the entries in \(\mathbf{S}_{j,\cdot}\) are zeros, then the corresponding entries in \(\bm{y}\) are free to choose any value in \([-1,1]\); to make the rest of the \(k-1\) inner products equal to zero, we generically need at least \(k-1\) free variables in \(\bm{y}\), meaning \(\mathbf{S}_{j,\cdot}\) contains at least \(k-1\) zeros--it is still not guaranteed that \(\|\bm{y}\|_{\infty}\leq 1\), but it intuitively makes sense that the more zeros each row of \(\bm{S}\) contains, the more likely that \(\bm{e}_{j}=\bm{S}\bm{y}\) is feasible, which is necessary for \(\bm{S}\bm{V}\) to be sufficiently scattered.

Figure 1: An example of sufficiently scattered in 3D.

### Probabilistic characterization

The analysis given in SS3.1 gives an exact characterization of when dictionary learning is identifiable using the proposed formulation (1) with a matrix volume identification criterion. However, given some sparse coefficient matrix \(\bm{S}\), checking whether it satisfies the identifiability condition in Theorem 3.2 amounts to solving a nonconvex quadratic programming (5), which is NP-hard in general. In this section, we assume that the sparse coefficient matrix \(\bm{S}\) is generated from a probabilistic generative model, specifically the Bernoulli-Gaussian model that has been widely considered in the literature, such as (Gribonval and Schnass, 2010; Sun et al., 2016), and show that it would satisfy guarantee identifiability with high probability, provided that the number of data points \(n\) is \(O(k\log k)\).

**Assumption 3.3** (Bernoulli-Gaussian model).: The matrix \(\bm{S}\in\mathbb{R}^{k\times n}\) is generated from a Bernoulli-Gaussian model with parameter \(p\in(0,1)\), denoted as \(\bm{S}\sim\mathcal{BG}(p)\), if its elements are i.i.d. with \(S_{ij}=b_{ij}\bm{g}_{ij}\), where \(b_{ij}\in\{0,1\}\) are i.i.d. Bernoulli random variables with \(\Pr[b_{ij}=1]=p\) (thus \(\Pr[b_{ij}=0]=1-p\)) and \(\bm{g}_{ij}\sim\mathcal{N}(0,1)\) are i.i.d. standard normal random variables.

Note that due to the scaling ambiguity of matrix factorization the assumption that the normal random variable has unit variance is without loss of generality--its rows will be rescaled to have unit \(\bm{\ell}_{1}\) norms nonetheless. What we are essentially assuming is that 1) a significant portion of \(\bm{S}\) are zero; 2) nonzeros of \(\bm{S}\) have zero mean and approximately the same variance.

**Theorem 3.4**.: _Suppose \(\bm{S}\in\mathbb{R}^{k\times n}\) is generated from the Bernoulli-Gaussian model \(\mathcal{BG}(p)\), and \(\widetilde{\bm{S}}\) is obtained by scaling its rows to have unit \(\bm{\ell}_{1}\) norm. Then_

\[\Pr\left[\sup_{\|\bm{w}^{\sharp}\bm{\hat{s}}\|_{1}\leq 1}\|\bm{w}\|>1 \right]\leq 4\exp\left(k\log(k)-np(1-p)\right).\] (6)

**Corollary 3.5**.: _Consider the generative model \(\bm{X}=\bm{A}^{\natural}\bm{S}^{\natural}\), where \(\bm{A}^{\natural}\in\mathbb{R}^{k\times k}\) is the groundtruth dictionary and \(\bm{S}\) is the groundtruth sparse coefficients. If \(\operatorname{rank}(\bm{A}^{\natural})=k\) and the matrix \(\bm{S}^{\natural}\in\mathbb{R}^{k\times n}\) is generated from the Bernoulli-Gaussian model \(\mathcal{BG}(p)\), then \((\bm{A}^{\natural},\bm{S}^{\natural})\) are globally identifiable via optimizing (1) with probability at least \(1-4\exp\left(k\log(k)-np(1-p)\right).\)_

The proof of Theorem 3.4 is relegated to Appendix B. It is appealing to see that the presented result looks much cleaner than prior work such as (Gribonval and Schnass, 2010; Wu and Yu, 2017; Wang et al., 2020), on top of the fact that such identifiability result is _global_, and does not require _any_ incoherence assumption on the dictionary, as long as it is nonsingular. An interesting observation is that most of the foundational results have appeared in the seminal work (Gribonval and Schnass, 2010); the difference is that they are used in the improved formulation (1), and in return leads to more significant result in Corollary 3.5. From the above result, we observe the following:

1. The right-hand-side of (6) would be a valid probability (\(<1\)) if \(n\) is bigger than a constant times \(k\log(k)/p(1-p)\), and very quickly approaches zero as it grows bigger. This agrees with the prior work on local identifiability that the sample complexity is of the same order (Gribonval and Schnass, 2010; Geng and Wright, 2014; Wu and Yu, 2017; Wang et al., 2020).
2. The Bernoulli parameter \(p\) cannot be exactly equal to \(1\) or \(0\), which affirms the obvious fact that \(\bm{S}\) can neither be completely dense nor entirely zero. The more interesting implication is that DL is identifiable (with high probability) as long as there is _some_ nonzero probability of having zeros in \(\bm{S}\), no matter how small that probability is, provided the sample size is large enough.

## 4 Proposed algorithm

In this section, we propose two algorithms for the volume-optimization based formulation (1). We will reformulate (1) as a determinant maximization problem subject to linear constraints, and then apply the Frank-Wolfe algorithm or block coordinate descent, which are both guaranteed to converge to a stationary point.

Since we assume \(\bm{A}\) is nonsingular, we can define \(\bm{P}=\bm{A}^{-1}\) and apply a change of variable to problem (1): the objective would become minimizing \(1/|\det\bm{P}|\), which we apply the log function and make it \(-\log\det|\bm{P}|\), and in the constraints we can now eliminate the \(\bm{S}\) variables by simply requiring \(\|\bm{P}\bm{X}_{j,:}\|_{1}\leq 1,j=1,\ldots,k\). This leads to the following reformulation

\[\operatorname*{minimize}_{\bm{P}}\ \ -\log|\det\bm{P}|\qquad\text{ subject to}\ \ \ \|[\bm{P}\bm{X}]_{j,:}\|_{1}\leq 1,j=1,\ldots,k.\] (7)Problem (7) now has a convex, or more specifically linear, constraint set, although the objective is still not convex. In Appendix 4, we propose two algorithms to directly tackle this formulation, one based on the Frank-Wolfe framework, and the other is an instance of block coordinate descent. Both algorithms guarantee convergence to a stationary point, which is the best one can get for a generic nonconvex problem. However, both algorithms require solving a linear programming problem in each iteration, making the overall algorithm double-looped, and the time efficiency is greatly affected by the specific subroutine to solve linear programming. In the rest of this section, we propose an algorithm based on the linearized alternating direction method of multipliers (L-ADMM) with well-defined and low-complexity iterations.

### Linearized alternating direction method of multipliers (L-ADMM)

We first modify the formulation (7) by introducing an auxiliary variable \(\bm{S}\):

\[\underset{\bm{P},\bm{S}}{\text{minimize}}\quad-\log|\det\bm{P}| \qquad\text{subject to}\quad\bm{P}\bm{X}=\bm{S},\;\;\|\bm{S}_{j,:}\|_{1}\leq 1,j=1, \ldots,k.\] (8)

Formulation (8) consists of two sets of variables (\(\bm{P}\) and \(\bm{S}\)) over two separable functions (one of them being an indicator function of \(\bm{S}\) that the \(\bm{\ell}_{1}\) norms of its rows are no bigger than 1) and linear equality constraints. It is easy to derive the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) for (8):

\[\bm{P}_{(t+1)} \leftarrow\arg\min_{\bm{P}}\;\;-\log|\det\bm{P}|+(\rho/2)\|\bm{P} \bm{X}-\bm{S}_{(t)}+\bm{U}_{(t)}\|^{2},\] (9a) \[\bm{S}_{(t+1)} \leftarrow\arg\min_{\bm{S}}\;\;\mathbb{1}_{\|:\|1\leq 1}(\bm{S})+( \rho/2)\|\bm{P}_{(t+1)}\bm{X}-\bm{S}+\bm{U}_{(t)}\|^{2},\] (9b) \[\bm{U}_{(t+1)} \leftarrow\bm{U}_{(t)}+\bm{P}_{(t+1)}\bm{X}-\bm{S}_{(t+1)}.\] (9c)

The second step (9b) is well-defined, as it projects each row of the matrix \(\bm{P}_{(t+1)}\bm{X}+\bm{U}_{(t)}\) to the \(\bm{\ell}_{1}\) norm ball, which can be efficiently computed with linear complexity (Duchi et al., 2008). In our implementation we use the bisection method described in (Parikh and Boyd, 2014). The first step (9a), however, is not clear how to compute. One popular method to mitigate this issue, which is prevalent in ADMM, is to take a linear approximation of the loss function of \(\bm{P}\) at the previous iterate \(\bm{P}_{(t)}\)(Lu et al., 2021). The gradient of \(\log|\det\bm{P}|\) is \(\bm{P}^{-\gamma}\), therefore the linear approximation of \(-\log|\det\bm{P}|\) at \(\bm{P}_{(t)}\) is \(-\log|\det\bm{P}_{(t)}|-\operatorname{Tr}\bm{P}_{(t)}^{-1}(\bm{P}-\bm{P}_{(t)})\). The overall update of \(\bm{P}_{(t+1)}\) becomes minimizing a convex quadratic function, which can be done in closed-form. The derived linearized ADMM (L-ADMM) iterates are

\[\bm{P}_{(t+1)} \leftarrow((\bm{S}_{(t)}-\bm{U}_{(t)})\bm{X}^{\gamma}+(1/\rho)\bm {P}_{(t)}^{-\gamma})(\bm{X}\bm{X}^{\gamma})^{-1},\] (10a) \[\bm{S}_{(t+1)} \leftarrow\operatorname{Proj}_{\|:\|1\leq 1}(\bm{P}_{(t+1)}\bm{X}+ \bm{U}_{(t)}),\] (10b) \[\bm{U}_{(t+1)} \leftarrow\bm{U}_{(t)}+\bm{P}_{(t+1)}\bm{X}-\bm{S}_{(t+1)}.\] (10c)

**Algorithm 1** Solving (7) with L-ADMM

```
1: take the QR factorization of \(\bm{X}^{\top}=\bm{Q}\bm{R}\)
2: set \(\rho=nk\) and initialize \(\bm{P}_{(0)}\)
3:for\(t=0,1,2,\ldots\) until convergence do
4:\(\bm{P}_{(t+1)}\leftarrow(\bm{S}_{(t)}-\bm{U}_{(t)})\bm{Q}+(1/\rho)\bm{P}_{(t)}^ {-\gamma}\)
5:\(\bm{S}_{(t+1)}\leftarrow\operatorname{Proj}_{\|:\|1\leq 1}(\bm{P}_{(t+1)}\bm{Q}^{ \top}+\bm{U}_{(t)})\)
6:\(\bm{U}_{(t+1)}\leftarrow\bm{U}_{(t)}+\bm{P}_{(t+1)}\bm{Q}^{\top}-\bm{S}_{(t+1)}\)
7:endfor
8:return\(\bm{P}_{(t)}\bm{R}^{-\gamma}\) ```

**Algorithm 2** Solving (7) with L-ADMM

Finally, we notice that formulation (8) and the derived L-ADMM algorithm are invariant under linear transformation of columns of \(\bm{X}\), meaning if we replace \(\bm{X}\) with \(\bm{\widetilde{X}}=\bm{G}\bm{X}\) in (8), where \(\bm{G}\) is a \(k\times k\) invertible matrix, then every iterate \(\bm{P}_{(t)}\) is uniquely mapped to \(\bm{P}_{(t)}\bm{G}^{-1}\) (while \(\bm{S}_{(t)}\) and \(\bm{U}_{(t)}\) are exactly the same), and the objective value has a constant difference \(-\log|\det\bm{P}_{(t)}\bm{G}^{-1}|=-\log|\det\bm{P}_{(t)}|+\log|\det\bm{G}|\). This observation allows us to preprocess the data matrix \(\bm{X}\) by orthogonalizing its rows, so that the update for \(\bm{P}\) in (10a) can be further simplified. The overall algorithm is summarized in Algorithm 1. We empirically found that setting \(\rho=nk\) works very well in practice.

Experiments

We now provide some numerical experiments to showcase the effectiveness of the proposed volume optimization formulation using the two algorithms described in SS4. All the experiments are conducted in MATLAB.

### Optimization performance

We synthetically generate random problems, and show the surprisingly effective performance of the two algorithms proposed in SS4. For \(k=20\) and \(n=1000\), we randomly generate the groundtruth sparse coefficient matrix \(\bm{S}^{\natural}\) according to the Bernoulli-Gaussian model with \(p=0.5\), and the groundtruth dictionary matrix \(\bm{A}^{\natural}\) completely random, and generate the data matrix \(\bm{X}=\bm{A}^{\natural}\bm{S}^{\natural}\). Due to Theorem 3.4, we know it is identifiable with very high probability, despite \(n\) being not that big compared to the number of atoms \(k\). Matrix \(\bm{X}\) is used as input to the L-ADMM algorithm as described in Alg. 1. Although Problem (7) is nonconvex, as long as it is identifiable, we know the global optimum is attained at \(\bm{P}^{\star}=\bm{D}(\bm{A}^{\natural})^{-1}\), where \(\bm{D}\) is a diagonal matrix with the \(i\)th diagonal equal to the inverse of the \(\ell_{1}\) norm of the \(i\)th row of \(\bm{S}^{\natural}\). As a result, \(-\log|\det\bm{D}(\bm{A}^{\natural})^{-1}|\) is the optimal value for Problem (7) as long as the model is identifiable, and we shall see whether the proposed algorithm is able to attain that optimal value. On the other hand, since L-ADMM directly tackles formulation 8, it is not guaranteed that \(\bm{P}_{(t)}\) is feasible in every iteration, which makes little sense to check the difference \(-\log|\det\bm{P}_{(t)}|+\log|\det\bm{P}^{\star}|\). We instead check the optimality gap of the Lagrangian function values, using the optimal dual variable \(\bm{A}\), since we have

\[-\log|\det\bm{P}|+\operatorname{Tr}(\bm{P}\bm{X}-\bm{S})\bm{A}^{\star}\geq- \log|\det\bm{P}^{\star}|,\]

for any \(\bm{P}\) and a feasible \(\bm{S}\). Obviously, the gap equals to zero when \(\bm{P}=\bm{P}^{\star}\) and \(\bm{S}=\bm{S}^{\star}\), in which case \(\bm{P}^{\star}\bm{X}-\bm{S}^{\star}=0\). Furthermore, it is easy to show that an optimal \(\bm{\varLambda}\) is \((\widetilde{\bm{S}}^{\natural})^{\dagger}\). In this simulation with known groundtruth factors, we will use this to measure the optimality gap.

The convergence behavior of 100 random trials of the L-ADMM Algorithm 1 are shown in Figure 2. The somewhat surprising observation is that it achieves essentially zero optimality gap in all instances, which is somewhat surprising as we are, after all, trying to solve NP-hard problems. Among the 100 random trials, L-ADMM usually requires approximately 500-1000 iterations to converge; together with the low-complexity iterations described in Algorithm 1, it usually takes just a few seconds to execute a trial. For a dictionary learning problem of this size, our proposed algorithm takes a lot less time than most existing state-of-the-arts. Some more detailed comparisons can be found in the supplementary material.

### Dictionary recovery

Next we fix the sample size \(n=1000\) and vary the dictionary size as well as the sparsity parameter \(\bm{p}\) in the Bernoulli-Gaussian model and check how often do we get exact recovery of the groundtruth dictionary \(\bm{A}^{\natural}\) up to column permutation and scaling. In the previous experiment we simply check the optimality gap between the algorithm output and \(-\log|\det\bm{D}(\bm{A}^{\natural})^{-1}|\), which is also a pretty good indicator of exact recovery since the only ambiguity is an orthogonal rotation, which is very unlikely to exist. Nevertheless, to ensure definitively that the dictionary is recovered as \(\bm{P}^{-1}\), we will normalize the column and use the Hungarian algorithm (Kuhn, 1955) to find the best column matching, and then calculate the estimation error. We declare success if the estimation error is smaller than 1e-5. The results are shown in Figure 3, which agrees with the bound in Theorem 3.4.

Figure 2: 100 random trials of the proposed L-ADMM for dictionary learning as in Algorithm 1. Since we know the groundtruth dictionary is the (essentially) unique minimizer, we can use it to calculate the optimality gap. We see that the optimality gap goes to zero in all random instances.

We would like to emphasize that compared to similar figures that have appeared in the literature, the groundtruth dictionaries that we try to identify are far from being orthogonal, nor do we care to check whether they satisfy the incoherence assumption; moreover, the recovery is of the _entire_ dictionary, not one of the atoms. More comparisons with some existing algorithms can be found in the supplementary.

### Real data experiment

We conclude this section with a classical application of dictionary learning on image compression. For a given image, it is first divided into \(8\times 8\) non-overlapping patches, reshaped into a vector in \(\mathbb{R}^{k}\) with \(k=64\), and stacked as columns of the data matrix \(\bm{X}\). After feeding it into the L-ADMM Algorithm 1, the output is inverted to obtain an estimate of the dictionary \(\bm{A}\). Each column of \(\bm{A}\) is then reshaped back into a \(8\times 8\) patch to be shown as an atom for the dictionary. For a monochrome image with \(512\times 512\) pixels, it would be reshaped into a data matrix \(\bm{X}\) of size \(64\times 4096\). Thanks to the high-efficiency of the proposed L-ADMM algorithm, learning a dictionary on a data set of this size takes only a few minutes from a random initialization. The resulting dictionary learned from a natural image is shown in Figure 4. The result agrees with most other dictionary learning algorithms, with some atoms representing edges and bright spots, while some others resemble a few DCT bases.

## 6 Conclusion

In this paper we aimed at answering the question of when dictionary learning is identifiable, with a specific requirement that the formulation uses the \(\ell_{1}\) norm to promote sparsity. With a novel formulation that minimizes the volume of the dictionary subject to \(\ell_{1}\) constraints, we close the case with an affirmative yes, complete dictionary learning is indeed _globally identifiable_ under mild conditions. Compared to similar works, our result is global, requires no conditions such as incoherence on the complete dictionary, and at the same time achieves the same sample complexity bound when the sparse coefficients satisfy the Bernoulli-Gaussian model. Finally, we propose algorithms to solve the volume optimization problem, specifically one based on linearized ADMM in the main paper, and demonstrate their exceptional performances on both synthetic and real data.

## Acknowledgments and Disclosure of Funding

This work is supported in part by NSF ECCS-2237640.

## References

* Agarwal et al. (2016) Alekh Agarwal, Animashree Anandkumar, Prateek Jain, and Praneeth Netrapalli. Learning sparsely used overcomplete dictionaries via alternating minimization. _SIAM Journal on Optimization_, 26(4):2775-2799, 2016.
* Aharon et al. (2006a) Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. _IEEE Transactions on signal processing_, 54(11):4311-4322, 2006a.
* Aharon et al. (2006b) Michal Aharon, Michael Elad, and Alfred M Bruckstein. On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them. _Linear algebra and its applications_, 416(1):48-67, 2006b.
* Arora et al. (2014) Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In _Conference on Learning Theory_, pages 779-806. PMLR, 2014.
* Arora et al. (2015) Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In _Conference on Learning Theory_, pages 113-149. PMLR, 2015.
* Bai et al. (2019) Yu Bai, Qijia Jiang, and Ju Sun. Subgradient descent learns orthogonal dictionaries. In _International Conference on Learning Representations_, 2019.
* Ben-Israel (1992) Adi Ben-Israel. A volume associated with mxn matrices. _Linear algebra and its applications_, 167:87-111, 1992.
* Ben-Israel (1999) Adi Ben-Israel. The change-of-variables formula using matrix volume. _SIAM Journal on Matrix Analysis and Applications_, 21(1):300-312, 1999.
* Ben-Israel (2000) Adi Ben-Israel. An application of the matrix volume in probability. _Linear Algebra and its Applications_, 321(1-3):9-25, 2000.
* Bennett (1962) George Bennett. Probability inequalities for the sum of independent random variables. _Journal of the American Statistical Association_, 57(297):33-45, 1962.
* Bertsekas (1999) Dimitri P. Bertsekas. _Nonlinear Programming_. Athena Scientific, 2 edition, 1999.
* Boyd et al. (2011) Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. _Foundations and Trends(r) in Machine learning_, 3(1):1-122, 2011.
* Cohen and Gillis (2019) Jeremy E Cohen and Nicolas Gillis. Identifiability of complete dictionary learning. _SIAM Journal on Mathematics of Data Science_, 1(3):518-536, 2019.
* Duchi et al. (2008) John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the \(\ell_{1}\)-ball for learning in high dimensions. In _Proceedings of the 25th International Conference on Machine Learning_, pages 272-279, 2008.
* Elad and Aharon (2006) Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. _IEEE Transactions on Image processing_, 15(12):3736-3745, 2006.
* Fu et al. (2015) Xiao Fu, Wing-Kin Ma, Kejun Huang, and Nicholas D Sidiropoulos. Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain. _IEEE Transactions on Signal Processing_, 63(9):2306-2320, 2015.
* Fu et al. (2018) Xiao Fu, Kejun Huang, and Nicholas D Sidiropoulos. On identifiability of nonnegative matrix factorization. _IEEE Signal Processing Letters_, 25(3):328-332, 2018.
* Geng and Wright (2014) Quan Geng and John Wright. On the local correctness of l1-minimization for dictionary learning. In _2014 IEEE International Symposium on Information Theory_, pages 3180-3184. IEEE, 2014.
* Georgiev et al. (2005) Pando Georgiev, Fabian Theis, and Andrzej Cichocki. Sparse component analysis and blind source separation of underdetermined mixtures. _IEEE transactions on neural networks_, 16(4):992-996, 2005.
* Geng et al. (2015)* Gribonval and Schnass (2010) Remi Gribonval and Karin Schnass. Dictionary identification--sparse matrix-factorization via \(\ell_{1}\)-minimization. _IEEE Transactions on Information Theory_, 56(7):3523-3539, 2010.
* Gribonval et al. (2015) Remi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, and Matthias Seibert. Sample complexity of dictionary learning and other matrix factorizations. _IEEE Transactions on Information Theory_, 61(6):3469-3486, 2015.
* Hillar and Sommer (2015) Christopher J Hillar and Friedrich T Sommer. When can dictionary learning uniquely recover sparse data from subsamples? _IEEE Transactions on Information Theory_, 61(11):6290-6297, 2015.
* Huang and Fu (2019) Kejun Huang and Xiao Fu. Detecting overlapping and correlated communities without pure nodes: Identifiability and algorithm. In _International Conference on Machine Learning_, pages 2859-2868, 2019.
* Huang et al. (2013) Kejun Huang, Nicholas D Sidiropoulos, and Ananthram Swami. Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition. _IEEE Transactions on Signal Processing_, 62(1):211-224, 2013.
* Huang et al. (2016) Kejun Huang, Xiao Fu, and Nikolaos D Sidiropoulos. Anchor-free correlated topic modeling: Identifiability and algorithm. _Advances in Neural Information Processing Systems_, 29, 2016.
* Huang et al. (2018) Kejun Huang, Xiao Fu, and Nicholas D. Sidiropoulos. Learning hidden Markov models from pairwise co-occurrences with application to topic modeling. In _International Conference on Machine Learning (ICML)_, pages 2073-2082, 2018.
* Jolliffe (2002) Ian T Jolliffe. _Principal Component Analysis_. Springer, 2 edition, 2002.
* Kuhn (1955) Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* Lin et al. (2015) Chia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, and ArulMurugan Ambikapathi. Identifiability of the simplex volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case. _IEEE Transactions on Geoscience and Remote Sensing_, 53(10):5530-5546, 2015.
* Lu et al. (2021) Songtao Lu, Jason D Lee, Meisam Razaviyayn, and Mingyi Hong. Linearized admm converges to second-order stationary points for non-convex problems. _IEEE Transactions on Signal Processing_, 69:4859-4874, 2021.
* Mairal et al. (2009) Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In _Proceedings of the 26th annual international conference on machine learning_, pages 689-696, 2009.
* Olshausen and Field (1997) Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? _Vision Research_, 37(23):3311-3325, 1997.
* Parikh and Boyd (2014) Neal Parikh and Stephen Boyd. Proximal algorithms. _Foundations and Trends(r) in Optimization_, 1(3):127-239, 2014.
* Pisier (1999) Gilles Pisier. _The volume of convex bodies and Banach space geometry_, volume 94. Cambridge University Press, 1999.
* Plumbley et al. (2009) Mark D Plumbley, Thomas Blumensath, Laurent Daudet, Remi Gribonval, and Mike E Davies. Sparse representations in audio and music: from coding to source separation. _Proceedings of the IEEE_, 98(6):995-1005, 2009.
* Rambhatla et al. (2019) Sirisha Rambhatla, Xingguo Li, and Jarvis Haupt. Noodl: Provable online dictionary learning and sparse coding. In _International Conference on Learning Representations_, 2019.
* Rockafellar (1970) R Tyrrell Rockafellar. _Convex Analysis_, volume 18. Princeton University Press, 1970.
* Shen et al. (2020) Yifei Shen, Ye Xue, Jun Zhang, Khaled Letaief, and Vincent Lau. Complete dictionary learning via lp-norm maximization. In _Conference on Uncertainty in Artificial Intelligence_, pages 280-289. PMLR, 2020.
* Shen et al. (2019)Daniel A. Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Shie Mannor, Nathan Srebro, and Robert C. Williamson, editors, _Proceedings of the 25th Annual Conference on Learning Theory_, volume 23 of _Proceedings of Machine Learning Research_, pages 37.1-37.18, Edinburgh, Scotland, 25-27 Jun 2012. PMLR.
* Sun et al. (2016a) Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. _IEEE Transactions on Information Theory_, 63(2):853-884, 2016a.
* Sun et al. (2016b) Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere ii: Recovery by riemannian trust-region method. _IEEE Transactions on Information Theory_, 63(2):885-914, 2016b.
* Tatli and Erdogan (2021) Gokcan Tatli and Alper T Erdogan. Polytopic matrix factorization: Determinant maximization based criterion and identifiability. _IEEE Transactions on Signal Processing_, 69:5431-5447, 2021.
* Tolooshams and Ba (2022) Bahareh Tolooshams and Demba E Ba. Stable and interpretable unrolled dictionary learning. _Transactions on Machine Learning Research_, 2022.
* Tosic and Frossard (2011) Ivana Tosic and Pascal Frossard. Dictionary learning. _IEEE Signal Processing Magazine_, 28(2):27-38, 2011.
* Tosic et al. (2010) Ivana Tosic, Ivana Jovanovic, Pascal Frossard, Martin Vetterli, and Neb Duric. Ultrasound tomography with learned dictionaries. In _2010 IEEE International Conference on Acoustics, Speech and Signal Processing_, pages 5502-5505. IEEE, 2010.
* Wang et al. (2020) Yu Wang, Siqi Wu, and Bin Yu. Unique sharp local minimum in l1-minimization complete dictionary learning. _Journal of Machine Learning Research_, 21(63):1-52, 2020.
* Wu and Yu (2017) Siqi Wu and Bin Yu. Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. _The Journal of Machine Learning Research_, 18(1):6121-6176, 2017.
* Zhai et al. (2020a) Yuexiang Zhai, Hermish Mehta, Zhengyuan Zhou, and Yi Ma. Understanding l4-based dictionary learning: Interpretation, stability, and robustness. In _International Conference on Learning Representations_, 2020a.
* Zhai et al. (2020b) Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma. Complete dictionary learning via l4-norm maximization over the orthogonal group. _Journal of Machine Learning Research_, 21(165):1-68, 2020b.

Proof of Theorem 3.2

The proof uses the notion of polar sets from convex analysis [Rockafellar, 1970]:

**Definition A.1** (Polar set).: Given a set \(\mathcal{C}\) that contains the origin, its polar set, denoted as \(\mathcal{C}^{\circ}\), is defined as

\[\mathcal{C}^{\circ}=\{\boldsymbol{x}\mid\boldsymbol{x}^{\top}\boldsymbol{y} \leq 1,\forall\boldsymbol{y}\in\mathcal{C}\}.\]

As an example, the polar of the unit ball \(\mathcal{B}=\{\boldsymbol{x}\|\|\boldsymbol{x}\|\leq 1\}\) is itself, i.e., \(\boldsymbol{B}^{\circ}=\boldsymbol{B}\), using the Cauchy-Schwartz inequality. Due to the definition of dual norm, we also have immediately that the polar set of any norm ball is the norm ball defined by its dual norm, e.g., the \(\ell_{\infty}\) norm ball \([-1,1]^{n}\) is the polar of the \(\boldsymbol{\ell}_{1}\) norm ball \(\{\boldsymbol{x}\|\boldsymbol{x}\|_{1}\leq 1\}\). Given a polyhedron in the form of \(\operatorname{conv}(\boldsymbol{H})\), i.e., the convex hull of a set of vectors given as the columns of \(\boldsymbol{H}\), then its polar is a polyhedron in the form of linear inequalities:

\[\operatorname{conv}(\boldsymbol{H})^{\circ}=\{\boldsymbol{x}\mid\boldsymbol {H}^{\top}\boldsymbol{x}\leq 1\}.\]

The important property of the polar sets is that if two sets satisfy \(\mathcal{A}\subseteq\mathcal{B}\), then \(\mathcal{B}^{\circ}\subseteq\mathcal{A}^{\circ}\), which will be crucial in the sequel. The proof is not hard and can be found in standard textbooks such as [Rockafellar, 1970].

Before proving Theorem 3.2, we show the following lemma as intermediate step:

**Lemma A.2**.: _If \(\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V})\) is sufficiently scattered in the hypercube as described in Assumption 3.1, then the solution set of problem (5) is \(\{\pm\boldsymbol{e}_{1},\ldots,\pm\boldsymbol{e}_{k}\}\)._

Proof.: As explained in the main paper, the single constraint \(\|\boldsymbol{w}^{\top}\widehat{\boldsymbol{S}}^{\natural}\|_{1}\leq 1\) is equivalent to a set of \(2^{n}\) linear inequalities

\[\boldsymbol{w}^{\top}\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V}\leq \boldsymbol{I}^{\top},\]

where \(\boldsymbol{V}\in\{\pm 1\}^{n\times 2^{n}}\) contains all \(2^{n}\) vectors \(\{\pm 1\}^{n}\) as its columns. This means \(\boldsymbol{w}\in\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural} \boldsymbol{V})^{\circ}\), i.e., \(\boldsymbol{w}\) belongs to the polar set of \(\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V})\). The sufficiently scattered condition first assumes that \(\mathcal{B}\subseteq\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural} \boldsymbol{V})\), so \(\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V})^{\circ}\subseteq \mathcal{B}\) since \(\mathcal{B}^{\circ}=\mathcal{B}\). We immediately have that any feasible \(\boldsymbol{w}\) for (5) must satisfy \(\|\boldsymbol{w}\|\leq 1\). Obviously, letting \(\boldsymbol{w}=\pm\boldsymbol{e}_{j}\) is feasible and attains the maximum.

The second assumption is that the intersection of \(\partial\mathcal{B}\) and \(\partial\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V})\), the boundary of the two sets, is the finite set \(\{\pm\boldsymbol{e}_{1},\ldots,\pm\boldsymbol{e}_{k}\}\). This, together with the first assumption, implies that for any direction \(\boldsymbol{y}\) not along the coordinates, there exists a point along that direction with norm strictly bigger than \(1\) that still lie in \(\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V})\), i.e., for any \(\boldsymbol{y}\neq\pm\boldsymbol{e}_{j}\) and \(\|\boldsymbol{y}\|=1+\epsilon\) with sufficiently small \(\epsilon>0\), there exists \(\boldsymbol{\theta}\in\Delta^{n}\) such that \(\boldsymbol{y}\in\operatorname{conv}(\widehat{\boldsymbol{S}}^{\natural} \boldsymbol{V})\). Then \(\boldsymbol{w}\) must also satisfy \(\boldsymbol{w}^{\top}\boldsymbol{y}\leq 1\). Now consider \(\boldsymbol{w}=\boldsymbol{y}/\|\boldsymbol{y}\|\), we would have that \(\|\boldsymbol{w}\|=1\) but it's infeasible since \(\boldsymbol{w}^{\top}\boldsymbol{y}=\|\boldsymbol{y}\|>1\). This excludes any vector with norm \(1\) that is not in \(\{\pm\boldsymbol{e}_{1},\ldots,\pm\boldsymbol{e}_{k}\}\) to be optimal for (5). 

Proof of Theorem 3.2.: Consider the rescaled groundtruth factors \((\widehat{\boldsymbol{A}}^{\natural},\widehat{\boldsymbol{S}}^{\natural})\) and an optimal solution to (1) \((\boldsymbol{A}^{\star},\boldsymbol{S}^{\star})\). Obviously, both of them are feasible:

\[\boldsymbol{X}=\widehat{\boldsymbol{A}}^{\natural}\widehat{\boldsymbol{S}}^{ \natural}=\boldsymbol{A}^{\star}\boldsymbol{S}^{\star},\qquad\|\widehat{ \boldsymbol{S}}^{\natural}_{j,\cdot}\|_{1}\leq 1,\|\boldsymbol{S}^{\star}_{j,\cdot}\|_{1} \leq 1,\quad j=1,\ldots,k.\]

Since \(\boldsymbol{A}^{\star}\) is invertible, we have that \(\boldsymbol{S}^{\star}=(\boldsymbol{A}^{\star})^{-1}\boldsymbol{X}=( \boldsymbol{A}^{\star})^{-1}\widehat{\boldsymbol{A}}^{\natural}\widehat{ \boldsymbol{S}}^{\natural}\). Now define \(\boldsymbol{W}=(\boldsymbol{A}^{\star})^{-1}\widehat{\boldsymbol{A}}^{\natural}\). Since \(\widetilde{\boldsymbol{A}}^{\star}\) is optimal for (1), we have that

\[|\det\boldsymbol{A}^{\star}|\leq|\det\widetilde{\boldsymbol{A}}^{\natural}| \quad\Rightarrow\quad|\det(\boldsymbol{A}^{\star})^{-1}\widetilde{ \boldsymbol{A}}^{\natural}|\geq 1\quad\Rightarrow\quad|\det\boldsymbol{W}|\geq 1.\] (11)

On the other hand, \(\boldsymbol{S}^{\star}=\boldsymbol{W}\widehat{\boldsymbol{S}}^{\natural}\) being feasible means

\[\|\boldsymbol{w}_{j}^{\top}\widehat{\boldsymbol{S}}^{\natural}\|_{1}\leq 1, \quad j=1,\ldots,k,\]

i.e., each row of \(\boldsymbol{W}\) satisfies the constraint set of (5). Since \(\widehat{\boldsymbol{S}}^{\natural}\boldsymbol{V}\) satisfies the sufficiently scattered condition, Lemma A.2 implies that

\[\|\boldsymbol{w}_{j}\|\leq 1,\quad j=1,\ldots,k.\]As a result, using the Hadamard inequality we have

\[|\det\bm{W}|\leq\prod_{j=1}^{k}\|\bm{w}_{j}\|\leq 1,\] (12)

and the supremum is attained when each row of \(\bm{W}\) is \(\pm\bm{e}_{j}\). Combining (11) and (12) shows that \(|\det\bm{W}|=1\) and each row of \(\bm{W}\) is \(\pm\bm{e}_{j}\). Furthermore, \(|\det\bm{W}|\) is nonzero only if none of the rows is collinear to each other, therefore \(\bm{W}\) must be a permutation matrix times a diagonal matrix with \(\pm 1\) on its diagonals. This guarantees identifiability of \(\widetilde{\bm{S}}^{\natural}\) (and \(\widetilde{\bm{A}}^{\natural}\)) up to permutation and sign ambiguity, which is equivalent to identifiability of \(\bm{A}^{\natural}\) and \(\bm{S}^{\natural}\) up to permutation and scaling. 

## Appendix B Proof of Theorem 3.4

We assume that \(\bm{S}\sim\mathcal{BG}(\bm{p})\), and the first thing we do is rescaling its rows to have unit \(\ell_{1}\) norms and use it for the problem 5. To simplify the analysis, we can instead directly maximize \(\|\bm{w}\|^{2}\) subject to \(\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1}\leq 1\), and compare it with the largest \(\ell_{1}\) norm of the rows of \(\bm{S}\). The complement of the intended probability can be bounded as

\[\Pr\left[\sup_{\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1}\leq 1}\|\bm{w} \|\leq 1\right]\geq\Pr\left[\sup_{\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1} \leq 1}\|\bm{w}\|\leq\alpha\cap\max_{j}\|\bm{S}_{j}.\|_{1}\geq\alpha\right],\]

with arbitrary choice of \(\alpha\). Conversely,

\[\Pr\left[\sup_{\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1} \leq 1}\|\bm{w}\|>1\right] \leq\Pr\left[\sup_{\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1} \leq 1}\|\bm{w}\|>\alpha\cup\max_{j}\|\bm{S}_{j}.\|_{1}<\alpha\right]\] \[\leq\Pr\left[\sup_{\|\bm{w}^{\mskip-1.5mu \mathsf{T}}\bm{S}\|_{1} \leq 1}\|\bm{w}\|>\alpha\right]+\Pr\left[\max_{j}\|\bm{S}_{j}.\|_{1}<\alpha\right]\] (13)

where the second inequality is obtained from the union bound. The rest of this section is dedicated to bounding the above two terms. Both of these results rely on the following version of the Bernstein inequality [Bennett, 1962]:

**Theorem B.1** (Bernstein's inequality).: _Let \(Z_{1},\ldots,Z_{n}\) be independent random variables with \(\operatorname{E}[Z_{i}^{2}]\leq v^{2}\) and there exists some constant \(c\) such that for all integer \(d>2\)_

\[\operatorname{E}[|Z_{i}|^{d}]\leq\frac{1}{2}d!v^{2}c^{d-2}.\] (14)

_Then_

\[\Pr\left[\left|\sum_{i=1}^{n}(Z_{i}-\operatorname{E}[Z_{i}])\right|>\epsilon \right]\leq 2\exp\left(-\frac{\epsilon^{2}}{2(nv^{2}+c\epsilon)}\right)\]

**Lemma B.2** (Bounding the second term in (13)).: _Suppose \(\bm{S}\in\mathbb{R}^{k\times n}\) is generated from the Bernoulli-Gaussian model \(\mathcal{BG}(\bm{p})\). Then_

\[\Pr\left[\max_{j}\|\bm{S}_{j}.\|_{1}<np(\sqrt{2/\pi}-\epsilon)\right]\leq 2k \exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right)\]

Proof.: Let \(\bm{s}=(s_{1},\ldots,s_{n})\) be generated from \(\mathcal{BG}(\bm{p})\), i.e., each \(s_{i}=b_{i}g_{i}\) with \(b_{i}\) Bernoulli with probability \(p\) and \(g_{i}\sim\mathcal{N}(0,1)\), we will use Bernstein's inequality with \(Z_{i}=b_{i}|g_{i}|\). The moments of \(b_{i}\) all equal to \(p\). The random variable \(|g_{i}|\) follows a Chi-distribution of degree 1 so its moments are

\[\operatorname{E}[|g_{i}|^{d}]=2^{d/2}\frac{\Gamma\left(\frac{d+1}{2}\right)}{ \Gamma\left(\frac{1}{2}\right)}.\] (15)

For \(d=1\) and 2 we have \(\operatorname{E}[Z_{i}]=p\sqrt{2/\pi}\) and \(\operatorname{E}[Z_{i}^{2}]=p\). Using the recurrence relation for the Gamma function \(\Gamma(t+1)=t\Gamma(t)\) and \(\sqrt{2}/\Gamma(1/2)=\sqrt{2/\pi}<1\) we can bound the rest of the moments with \(d>2\) as

\[\operatorname{E}[|Z_{i}|^{d}]\leq p\frac{d!}{2^{d/2}}.\] (16)Therefore the moments satisfy (14) with \(c=1/\sqrt{2}\), results in

\[\Pr\left[\|\bm{s}\|_{1}<np(\sqrt{2/\pi}-\epsilon)\right] \leq\Pr\left[\left\|\sum_{i=1}^{n}(Z_{i}-\mathrm{E}[Z_{i}])\right\|> np\epsilon\right]\] \[\leq 2\exp\left(-\frac{n^{2}p^{2}\epsilon^{2}}{2(np+np\epsilon/ \sqrt{2})}\right)\] \[=2\exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right).\]

Finally, using the union bound

\[\Pr\left[\max_{j}\|S_{j:}\|_{1}<np(\sqrt{2/\pi}-\epsilon)\right] \leq k\Pr\left[\|S_{j:}\|_{1}<np(\sqrt{2/\pi}-\epsilon)\right]\] \[\leq 2k\exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon} \right).\]

We now proceed to bound the first term in (13). First we note the following equivalence:

\[\Pr\left[\sup_{\|\bm{w}^{*}\|_{1}\leq 1}\|\bm{w}\|>\alpha\right]=\Pr\left[\inf_ {\|\bm{w}\|=1}\|\bm{S}^{*}\bm{w}\|_{1}<1/\alpha\right]\] (17)

We are also going to use the following notion of \(\delta\)-cover from convex geometry (Pisier, 1999) that holds for all \(\ell_{p}\) norm balls, but we are only to instantiate the Eucleadian ball:

**Lemma B.3** (\(\delta\)-cover).: _A finite \(\delta\)-cover of the unit sphere in \(\mathbb{R}^{k}\) is a finite set \(\mathcal{C}_{\delta}\) of points with unit \(\ell_{2}\) norm such that any point on the unit sphere is within \(\epsilon\) away from an element in \(\mathcal{C}_{\delta}\), i.e._

\[\min_{\bm{w}_{i}\in\mathcal{C}_{\delta}}\|\bm{w}-\bm{w}_{i}\|<\delta,\ \ \forall\|\bm{w}\|=1.\]

_For \(\delta\in(0,1)\) there always exists an \(\delta\)-cover \(\mathcal{C}_{\epsilon}\) with cardinality \(|\mathcal{C}_{\delta}|<(3/\delta)^{k}\)._

**Lemma B.4**.: _Let \(\mathcal{C}_{\delta}=\{\bm{w}_{i}\}\) be an \(\delta\)-cover for the sphere in \(\mathbb{R}^{k}\). Assume that we have both the lowerbound_

\[\|\bm{S}^{*}\bm{w}_{i}\|_{1}\geq\beta,\forall\bm{w}_{i}\in\mathcal{C}_{\delta}\]

_and the upperbound_

\[\|\bm{S}^{\mathsf{T}}\|_{1}=\sup_{\|\bm{w}\|_{1}\leq 1}\|\bm{S}^{\mathsf{T}} \bm{w}\|_{1}\leq\gamma.\]

_Then_

\[\inf_{\|\bm{w}\|\leq 1}\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}\geq\beta-\gamma \delta\sqrt{k}\]

Proof.: By definition of the \(\delta\)-cover, for all \(\bm{w}\) with unit norm we can find \(\bm{w}_{i}\in\mathcal{C}_{\delta}\) with \(\|\bm{w}-\bm{w}_{i}\|<\delta\). Therefore

\[\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}\geq\|\bm{S}^{\mathsf{T}}\bm{w} _{i}\|_{1}-\|\bm{S}^{\mathsf{T}}(\bm{w}-\bm{w}_{i})\|_{1} \geq\beta-\|\bm{S}^{\mathsf{T}}\|_{1}\|\bm{w}-\bm{w}_{i}\|_{1}\] \[\geq\beta-\|\bm{S}^{\mathsf{T}}\|_{1}\|\bm{w}-\bm{w}_{i}\|\sqrt{k }\geq\beta-\gamma\delta\sqrt{k}.\]

**Lemma B.5** (Bounding the first term in (13)).: _Suppose \(\bm{S}\in\mathbb{R}^{k\times n}\) is generated from the Bernoulli-Gaussian model \(\mathcal{BG}(p)\). Then_

\[\Pr\left[\inf_{\|\bm{w}\|=1}\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}<np(\sqrt{2/\pi}- \epsilon)-\delta\sqrt{k}np(\sqrt{2/\pi}+\epsilon)\right]\leq\left(\left(\frac {3}{\delta}\right)^{k}+1\right)2\exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2} \epsilon}\right),\]

_where \(\delta\in(0,1)\) represents any choice of \(\delta\)-cover for the unit sphere._Proof.: Following Lemma B.4, we have

\[\Pr\left[\inf_{\|\bm{w}\|\leq 1}\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}<\beta-\gamma \delta\sqrt{k}\right]\leq\sum_{\bm{w}_{i}\in\mathcal{C}_{\delta}}\Pr\left[\|\bm{S }^{\mathsf{T}}\bm{w}_{i}\|_{1}<\beta\right]+\Pr\left[\|\bm{S}^{\mathsf{T}}\|_{1 }>\gamma\right],\] (18)

where \(\mathcal{C}_{\delta}\) is a \(\delta\)-cover of the unit sphere with cardinality \(|\mathcal{C}_{\delta}|<(3/\delta)^{k}\) according to Lemma B.3.

The bound to the first term in (18) is almost identical to Lemma B.2. Dropping the subscript of \(\bm{w}_{i}\), we write

\[\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}=\sum_{i=1}^{n}\left|\sum_{j=1}^{k}b_{ij}g_{ij }m_{j}\right|:=\sum_{i=1}^{n}|Z_{i}|.\]

Without the absolute value, \(Z_{i}\) is normally distributed with zero mean and variance \(\sigma^{2}=\sum_{j}m_{j}^{2}b_{ij}\), so

\[\operatorname{E}[|Z_{i}|^{d}]=\operatorname{E}[|\sigma G|^{d}]=\operatorname{ E}[|\sigma|^{d}]\operatorname{E}[|G|^{d}],\]

where \(G\) is a standard normal random variable. Since \(\sigma^{2}=\sum_{j}m_{j}^{2}b_{ij}\leq\sum_{j}m_{j}^{2}=1\), we have for \(d\geq 2\)

\[\operatorname{E}[|\sigma|^{d}]\leq\operatorname{E}[|\sigma|^{2}]=\operatorname {E}\left[\sum_{j}m_{j}^{2}b_{ij}\right]=p,\]

where for \(d=1\) we have

\[\operatorname{E}[|\sigma|]\geq\operatorname{E}[|\sigma|^{2}]=\operatorname{E} \left[\sum_{j}m_{j}^{2}b_{ij}\right]=p.\]

Furthermore, \(|G|\) is chi-distributed with degree \(1\), whose moments are given in (15), so the moments of \(|Z_{i}|\) again follows (16). Again, the moments satisfy (14) with \(c=1/\sqrt{2}\), results in

\[\Pr\left[\|\bm{S}^{\mathsf{T}}\bm{w}\|_{1}<np(\sqrt{2/\pi}-\epsilon)\right] \leq\Pr\left[\left|\sum_{i=1}^{n}(Z_{i}-\operatorname{E}[Z_{i}] )\right|>np\epsilon\right]\] \[\leq 2\exp\left(-\frac{n^{2}p^{2}\epsilon^{2}}{2(np+np\epsilon/ \sqrt{2})}\right)\] \[=2\exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right).\] (19)

To bound \(\|\bm{S}^{\mathsf{T}}\|_{1}\), we recall that this is the \(\bm{\ell}_{1}\) induced norm for matrix \(\bm{S}^{\mathsf{T}}\), which is shown to be the maximum of the \(\bm{\ell}_{1}\) norms of the columns of \(\bm{S}^{\mathsf{T}}\). This means we can use similar arguments used in Lemma B.2 (but applied to the other direction) to have

\[\Pr\left[\|\bm{S}^{\mathsf{T}}\|_{1}>np(\sqrt{2/\pi}+\epsilon)\right] =\Pr\left[\max_{j}\|\bm{S}_{j,\cdot}\|_{1}>np(\sqrt{2/\pi}+ \epsilon)\right]\] \[\leq\Pr\left[\|\bm{S}_{j,\cdot}\|_{1}>np(\sqrt{2/\pi}+\epsilon)\right]\] \[\leq\Pr\left[\left|\sum_{i=1}^{n}(Z_{i}-\operatorname{E}[Z_{i}] )\right|>np\epsilon\right]\leq 2\exp\left(-\frac{np\epsilon^{2}}{2+\sqrt{2} \epsilon}\right),\] (20)

where we pick an arbitrary \(j\in[k]\) in the second line since this event implies that the maximum \(\bm{\ell}_{1}\) norm of the rows are lowerbounded, and in the third line each \(Z_{i}=b_{i}|g_{i}|\) satisfies (16). The proof is complete by combining (18), (19), and (20) with \(\beta=np(\sqrt{2/\pi}-\epsilon)\) and \(\gamma=np(\sqrt{2/\pi}+\epsilon)\). 

Proof of Theorem 3.4.: We first instantiate Lemma B.5 with

\[\delta=\frac{n^{2}p^{2}(\sqrt{2/\pi}-\epsilon)^{2}-1}{n^{2}p^{2}(\sqrt{2/\pi} -\epsilon)(\sqrt{2/\pi}+\epsilon)\sqrt{k}},\]which obviously satisfies \(\delta<1\) and would also satisfies \(\delta>0\) if

\[\epsilon<\sqrt{\frac{2}{\pi}}-\frac{1}{np}.\]

Then we have

\[\Pr\left[\inf_{\|\bm{w}\|=1}\|\bm{S}^{\mathrm{T}}\bm{w}\|_{1}<1/np(\sqrt{2/\pi}- \epsilon)\right]\leq\left(\left(\frac{3}{\delta}\right)^{k}+1\right)2\exp\left( -\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right),\]

Combining (13), (17), and Lemma B.2 with \(\alpha=np(\sqrt{2/\pi}-\epsilon)\), we obtain

\[\Pr\left[\sup_{\|\bm{w}^{\mathrm{T}}\|_{1}\leq 1}\|\bm{w}\|>1\right] \leq\Pr\left[\inf_{\|\bm{w}\|=1}\|\bm{S}^{\mathrm{T}}\bm{w}\|_{1 }<1/\alpha\right]+\Pr\left[\max_{j}\|\bm{S}_{j,:}\|_{1}<\alpha\right]\] \[\leq 2\left(k+\left(\frac{3}{\delta}\right)^{k}+1\right)\exp \left(-\frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right).\] (21)

Further, with

\[\epsilon<\frac{(\sqrt{3}k^{1/4}-1)\sqrt{2/\pi}}{\sqrt{3}k^{1/4}+1},\]

where the right hand side is obviously positive, we have

\[\delta=\frac{n^{2}p^{2}(\sqrt{2/\pi}-\epsilon)^{2}-1}{n^{2}p^{2}(\sqrt{2/\pi}- \epsilon)(\sqrt{2/\pi}+\epsilon)\sqrt{k}}>\frac{n^{2}p^{2}(\sqrt{2/\pi}- \epsilon)^{2}}{n^{2}p^{2}(\sqrt{2/\pi}+\epsilon)^{2}\sqrt{k}}>\frac{3}{k}.\]

We can further relax (21) to

\[\Pr\left[\sup_{\|\bm{w}^{\mathrm{T}}\|_{1}\leq 1}\|\bm{w}\|>1\right] \leq 2\left(k+\left(k\right)^{k}+1\right)\exp\left(-\frac{np \epsilon^{2}}{2+\sqrt{2}\epsilon}\right)\leq 4\left(k\right)^{k}\exp\left(- \frac{np\epsilon^{2}}{2+\sqrt{2}\epsilon}\right)\] \[\leq 4\exp\left(k\log(k)-np(1-p)\right),\]

where in the last inequality we simply picked a small-enough \(\epsilon\) so that

\[\frac{\epsilon^{2}}{2+\sqrt{2}\epsilon}<1-p.\]

This completes the proof. 

## Appendix C Additional Algorithms

In this section we introduce two algorithms that directly tackles the formulation (7), repeated here:

\[\underset{\bm{p}}{\text{minimize}}\quad-\log|\det\bm{P}|\qquad\text{subject to}\quad\|[\bm{P}\bm{X}]_{j,:}\|_{1}\leq 1,j=1,\ldots,k.\]

### Frank-Wolfe algorithm

Our first attempt is to apply the Frank-Wolfe algorithm to approximately solve (7). The Frank-Wolfe algorithm, also known as the conditional gradient method for constrained optimization [1], iteratively minimizes a linear objective, defined by the gradient at the current iterate, under the same constraint set to determine the search direction and obtain the next iterate via some line search approach along the search direction. For the log-determinant objective in (7), we have that the gradient is \(-(\bm{P}^{-1})^{\mathrm{T}}\). As a result, the Frank-Wolfe algorithm for (7) is given in Algorithm 2.

Regarding the line search step, we propose to use the backtracking line search (Armijo rule) [1] to guarantee sufficient decrease of the objective function. Since the constraint set of (7) is convex, as long as \(\bm{P}_{(t)}\) is feasible, then \(\bm{P}_{(t+1)}\) is also feasible since it is a convex combination of \(\bm{P}_{(t)}\) and \(\bm{P}_{d}\), which are by definition feasible. Therefore, the only nontrivial part is to find a feasible initialization \(\bm{P}_{(0)}\). This can be done by optimizing an arbitrary linear objective subject to the same constraint as (7) (which means one should not apply line search at this step). If \(\bm{A}\) is square, there is a simple initialization that works really well in our experience, by setting \(\bm{P}_{(0)}\) as a diagonal matrix with

\[\bm{P}_{(0)}(c,c)=1/\|\bm{X}_{j,:}\|_{1},\]

i.e., rescaling each row of \(\bm{X}\) to have unit \(\ell_{1}\) norm.

In terms of complexity, each iteration is dominated by the linear programming with \(k^{2}\) variables. Without exploiting any structure, the per-iteration complexity could be as high as \(O(k^{6})\). However, the linear programming to be solved in Algorithm 2 is blessed with structures to be exploited to greatly reduce the complexity. Denote \(\bm{p}_{i}\) as the \(i\)th row of \(\bm{P}\), then the linear programming in each iteration of Algorithm 2 is in fact \(k\) independent problems, each involving only one row of \(\bm{P}\); let \(\bm{f}_{i}\) denote the \(i\)th column of \(\bm{P}_{(t)}^{-1}\), then we should solve the following problem with \(i=1,\ldots,k\)

\[\operatorname*{minimize}_{\bm{p}_{i}}\ -\bm{f}_{i}^{\top}\bm{p}_{i}\quad \text{ subject to }\ \|\bm{p}_{i}^{\top}\bm{X}\|_{1}\leq 1.\] (22)

Each of these problems involves \(k\) variables, which can be solved with \(O(k^{3})\) flops. This important observation brings the per-iteration complexity of Algorithm 2 down to \(O(k^{4})\).

Note that the Frank-Wolfe algorithm is also able to handle the overdetermined case when the dictionary matrix \(\bm{A}\) is tall, although this case is rarely considered in the context of dictionary learning: as long as \(\bm{A}\) has full column rank, we can make the change-of-variable \(\bm{P}=\bm{A}^{\dagger}\), and the objective of (7) can be replaced by \((1/2)\log(\bm{P}\bm{P}^{\top})\), with its gradient equals to \(-(\bm{P}^{\dagger})^{\top}\). Everything else follows easily.

We implement the algorithm in MATLAB and use the built-in linprog function in MATLAB to solve each of the linear programming sub-problems (22). Dropping the subscripts in (22), the \(\ell_{1}\) norm constraint can be transformed as linear ones using the standard trick:

\[\operatorname*{minimize}_{\bm{p},\bm{t}}\ -\bm{f}^{\top}\bm{p}\qquad\text{ subject to }\ -\bm{t}\leq\bm{p}_{i}^{\top}\bm{X}\leq\bm{t},\bm{T}^{\top}\bm{t}\leq 1.\]

Using the same setting as in SS5.1 but with smaller problem dimension \(n=200\) and \(k=10\), the convergence of 50 random trials are shown in Figure 5.

### Cyclic row update

Inspired by the row-separable structure of the constraint set in (7), we also attempt to solve (7) via a block coordinate descent-type algorithm that cyclically updates each row. However, we note that this algorithm only works when \(\bm{P}\) is square, unlike the Frank-Wolfe algorithm, which can be extended to the rare overdetermined case when \(\bm{A}\) is tall. We will drop the log term in the objective function of (7) when designing the cyclic row update algorithm in this subsection.

Using the Laplace's formula, we know that \(\det\bm{P}\) is a linear function with respect to the \(i\)th row of \(\bm{P}\) using the co-factor expansion

\[\det\bm{P}=\sum_{j=1}^{k}(-1)^{i+j}P_{ij}\det\bm{P}_{ij},\]where the matrix \(\bm{P}_{ij}\) is obtained by deleting the \(i\)th row and \(j\)th column of \(\bm{P}\). Then to apply the block coordinate descent algorithm (Bertsekas, 1999), one would minimize \(-|\bm{f}_{i}^{\top}\bm{p}_{i}|\) subject to \(\|\bm{p}_{i}^{\top}\bm{X}\|_{1}\leq 1\), where \(\bm{f}_{i}\in\mathbb{R}^{k}\) is a vector defined according to the co-factor expansion formula.

It looks as if we need to solve two linear programs for each row update, one to maximize \(\bm{f}_{i}^{\top}\bm{p}_{i}\) and one to minimize \(\bm{f}_{i}^{\top}\bm{p}_{i}\), but notice that the constraint \(\|\bm{p}_{i}^{\top}\bm{X}\|_{1}\leq 1\) is invariant under a sign flip, therefore the only difference between the solutions of the two linear programs would be a sign flip, which is both acceptable due to the inherent sign ambiguity in matrix factorization and dictionary learning in specific. Furthermore, recall that the Cramer's rule shows that

\[\left[\bm{P}^{-1}\right]_{ji}=(-1)^{i+j}\det\bm{P}_{ij}\Big{/}\det\bm{P},\]

meaning we can simply define \(\bm{f}_{i}\) as the \(i\)th column of \(\bm{P}^{-1}\), and it would not affect the row updates. This makes the definition of \(\bm{f}_{i}\) exactly the same way as in (22).

``` initialize \(\bm{P}_{(0)}\) repeat for\(i=1,\dots,k\)do \(\bm{f}=\bm{P}^{-1}\bm{e}_{i}\) \(\bm{p}=\arg\min_{\bm{p}}-\bm{f}^{\top}\bm{p}\) subject to \(\|\bm{p}^{\top}\bm{X}\|_{1}\leq 1\)  replace \(i\)th row of \(\bm{P}\) with \(\bm{p}\) endfor untilconvergence ```

**Algorithm 3** Solving (7) with cyclic row update

The two proposed algorithms 2 and 3 shows striking similarities, especially considering they both fundamentally solve linear programs in the form of (22) in each iteration. For nonconvex optimization, both algorithms guarantees that every limit point is a stationary point with some additional assumptions, which is reassuring to know. The differences are as follows:

1. Block coordinate descent is guaranteed to monotonically improve the objective function by design, so there is no need for line search as in Frank-Wolfe. This could save some computation as calculating \(\det\bm{P}\) may not be cheap when \(k\) is large.
2. On the other hand, each iteration of Frank-Wolfe only need to calculate \(\bm{P}^{-1}\) once for the update of all its \(k\) rows, whereas cyclic row update requires to recalculate \(\bm{P}^{-1}\) for each of its row updates. Overall, the per-iteration complexity is almost identical.

An interesting observation is that solving one subproblem (22) resembles the problem formulations in works such as (Spielman et al., 2012; Geng and Wright, 2014; Sun et al., 2016; Zhai et al., 2020; Shen et al., 2020) that attempt to find one atom of the dictionary at a time in a deflation manner. The difference is that they are still trying to solve nonconvex problems, and they typically require multiple random initializations due to the risk of finding the same atoms multiple times. From a different perspective, we see that the benefit of using the proposed volume optimization framework provides a

Figure 5: 50 random trials of the Frank-Wolfe Algorithm 2.

smartly chosen search direction given by rows of \(\bm{P}^{-1}\), which is by definition the current iterate of the dictionary.

This algorithm is again implemented in MATLAB using the built-in linprog function. Using the same setting as before with the Frank-Wolfe algorithm, the convergence of 50 random trials are shown in Figure 6. Comparing the two algorithms specifically, we see that Frank-Wolfe succeeds in all 50 trials, while CRU has one failure, although CRU takes much fewer number of iterations to converge.

## Appendix D Additional Experiments

Here we provide some additional experiments to showcase the performance of the proposed volume-optimization formulation as well as the three proposed algorithms. We start by demonstrating the time-efficiency of the three proposed algorithms to motivate the use of L-ADMM as the go-to method. Then we compare with some state-of-the-art dictionary learning algorithms with identifiability guarantees to showcase the improved performance of dictionary recovery by incorporating the volume criterion.

### Comparing the three proposed algorithms

In Figures 2, 5, and 6, we showed the per-iteration convergence of L-ADMM, Frank-Wolfe, and cyclic row update (i.e., block coordinate descent [BCD]) as described in Algorithm 1, 2, and 3. While all three algorithms manage to obtain essentially zero optimality gap, L-ADMM takes significantly more number of iterations; on the other hand, it is obvious that the cost of L-ADMM to perform one iteration is much cheaper than those of Frank-Wolfe or BCD. In this subsection we show that, thanks to the simple and efficient per-iteration update of L-ADMM, it is significantly faster than the other two proposed algorithms. We generate synthetic data sets like before, with \(n=1000\) and the sparsity level \(p=0.5\), and set \(k=20\) in Figure 6(a) and \(k=50\) in Figure 6(b). Notice that both the vertical and horizontal axes are in log scale, indicating that L-ADMM is several magnitude faster than the other alternatives. The difference would only become bigger as we increase \(n\) and \(k\).

### Dictionary recovery

We compare the dictionary recovery performance of the proposed volume-optimization formulation (using the L-ADMM algorithm in Algorithm 1) with some state-of-the-arts. The most related work are [10, 20, 21], which focus on the local identifiability of formulation (2) without proposing new algorithms. For algorithms with claimed identifiability guarantees, the closest we find are the alternating direction method (ADM) by Sun et al. [2016a,b], \(\ell_{4}\)-norm maximization (\(\ell_{4}\)-max) by Zhai et al. [2020b], and \(\ell_{3}\)-norm maximization (\(\ell_{3}\)-max) by Shen et al. [2020]. We note that ADM is not the main algorithm that is analyzed in Sun et al. [2016a,b], but it works better and has the benefit of recovering the entire dictionary simultaneously; the analyzed manifold optimization algorithm in [Sun et al., 2016a,b] recovers the

Figure 6: 50 random trials of the cyclic row update Algorithm 3.

dictionary one atom at a time, which requires multiple initializations to guarantee complete recovery, and such procedures are not provided by the authors on their Github page.

Note that all three baseline algorithms, ADM, \(\ell_{4}\)-max, and \(\ell_{3}\)-max, require that the groundtruth dictionary to be _orthogonal_. They claim that this is, to some extend, without loss of generality by pre-whitening the data \((\bm{X}\bm{X}^{\top})^{-1/2}\bm{X}\). We discover that this claim is only true if the sparse coefficients matrix \(\bm{S}\) follows strictly from the Bernoulli-Gaussian generative model, and that the sample size tends to infinity, in which case \((1/n)\bm{S}\bm{S}^{\top}\to\bm{p}\bm{I}\); if this assumption does not hold, such a pre-whitening step is not able to make an arbitrary dictionary orthogonal. As a result, when we use the same experimental setting as in SS5.2, none of these algorithms are able to exactly recover the true dictionary in any circumstance.

If we restrict ourselves to orthogonal dictionary recovery, then we obtain some sensible results from the three baseline algorithms, although they still perform significantly worse than our proposed work. What is more, even for exactly orthogonal dictionaries they are not able to recover them with very high accuracy. Here we test them in two cases: In the first case shown in Tables 1 and 2, we fix \(n=1000\) and \(k=20\), and vary the sparsity level \(p\) from \(0.1\) to \(0.9\); in the other case shown in Tables 3 and 4, we fix \(n=1000\) and \(p=0.5\), and vary the dictionary size \(k\) from \(5\) to \(50\). The recovered dictionary is again permuted using the Hungarian algorithm, and then we declare a successful recovery if the error is less than a threshold. In each case we present two tables, one with the threshold to be \(10^{-5}\) (same as the threshold used in SS5.2), and the other with threshold \(10^{-2}\). We see that the three baseline algorithms can provide reasonable result with the more loose threshold, especially \(\ell_{3}\)-max. Regardless, the proposed volume-optimization formulation with the L-ADMM, performs significantly better than all of them.

Figure 7: Convergence (with respect to time in seconds) of the three proposed algorithms. In both cases \(n=1000\) and \(p=0.5\), and we vary the value of \(k\). It is clear from both figures that L-ADMM is significantly more efficient than Frank-Wolfe or BCD.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline  & \(k=5\) & \(k=10\) & \(k=15\) & \(k=20\) & \(k=25\) & \(k=30\) & \(k=35\) & \(k=40\) & \(k=45\) & \(k=50\) \\ \hline This work & 1 & 1 & 1 & 1 & 0.9 & 1 & 1 & 1 & 1 & 0.8 \\ \hline ADM [21, 22] & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \(\ell_{4}\)-max [23] & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \end{tabular}
\end{table}
Table 4: Probability of success when fixing \(n=1000\) and \(p=0.5\), threshold \(10^{-2}\)

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline  & \(p=0.1\) & \(p=0.2\) & \(p=0.3\) & \(p=0.4\) & \(p=0.5\) & \(p=0.6\) & \(p=0.7\) & \(p=0.8\) & \(p=0.9\) \\ \hline This work & 1 & 1 & 1 & 1 & 0.9 & 1 & 0.9 & 0 & 0 \\ \hline ADM [21, 22] & 1 & 1 & 0.9 & 0.1 & 0 & 0 & 0 & 0 & 0 \\ \hline \(\ell_{4}\)-max [23] & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \end{tabular}
\end{table}
Table 1: Probability of success when fixing \(n=1000\) and \(k=20\), threshold \(10^{-5}\)

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline  & \(k=5\) & \(k=10\) & \(k=15\) & \(k=20\) & \(k=25\) & \(k=30\) & \(k=35\) & \(k=40\) & \(k=45\) & \(k=50\) \\ \hline This work & 1 & 1 & 1 & 1 & 0.9 & 1 & 1 & 1 & 1 & 0.8 \\ \hline ADM [21, 22] & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \(\ell_{4}\)-max [23] & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline \end{tabular}
\end{table}
Table 3: Probability of success when fixing \(n=1000\) and \(p=0.5\), threshold \(10^{-5}\)

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline  & \(p=0.1\) & \(p=0.2\) & \(p=0.3\) & \(p=0.4\) & \(p=0.5\) & \(p=0.6\) & \(p=0.7\) & \(p=0.8\) & \(p=0.9\) \\ \hline This work & 1 & 1 & 1 & 1 & 0.9 & 1 & 0.9 & 0 & 0 \\ \hline ADM [21, 22] & 1 & 1 & 0.9