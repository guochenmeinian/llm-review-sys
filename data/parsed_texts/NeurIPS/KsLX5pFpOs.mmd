# Proportional Fairness in Clustering:

A Social Choice Perspective

 Leon Kellerhals

Technische Universitat Clausthal

leon.kellerhals@tu-clausthal.de &Jannik Peters

National University of Singapore

peters@nus.edu.sg

###### Abstract

We study the proportional clustering problem of Chen et al. (ICML'19) and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters (EC'23) simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al., but also to individual fairness (Jung et al., FORC'20) and the "core" (Li et al., ICML'21). In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters imply approximations to these stronger guarantees.

## 1 Fair clustering

Fair decision-making is a crucial research area in artificial intelligence and machine learning. To ensure fairness, a plethora of different fairness notions, algorithms and settings have been introduced, studied, and implemented. One area in which fairness has been applied extensively is _(centroid) clustering_: We are given a set of \(n\) data points which we want to partition into \(k\) clusters by choosing \(k\) "centers" and assigning each point to a center by which it is _represented well_. Fairness now comes into play when, e.g., the data points correspond to human individuals.

Fairness notions in clustering usually depend on one decision: whether one takes demographic information (such as gender, income, etc.) into account or whether one is agnostic to it. A large part of work on fair clustering has focused on incorporating such demographic information, starting with the seminal work of Chierichetti et al. (2017) who aimed to proportionally balance the number of people of a certain type in each cluster center. However, not all work on fair clustering relies on demographic information. Independently, and in different contexts, Jung, Kannan, and Lutz (2020) and Chen, Fain, Lyu, and Munagala (2019) instead tried to derive fairness notions from the instance itself. For Jung et al. this lead to their notion of _individual fairness_: Given a population of size \(n\), with \(k\) cluster centers to be opened, every agent should be entitled to a cluster center not further away than their \(\frac{n}{k}\)-th neighbor. While this is not always achievable, Jung et al. gave a simple algorithm achieving a \(2\)-approximation to this notion. Chen et al. were motivated not by being fair towards individual members of the population (or agents), but towards groups of agents, defining their notion of _proportional fairness_: no group of size at least \(\frac{n}{k}\) should be able to suggest a cluster center they all would be better off with. This notion is also not always achievable, and Chen et al. gave a simple \(\left(1+\sqrt{2}\right)\)-approximation for it.

So far, the individual and proportional fairness notions (and some other related fairness notions) have existed in parallel, with similarities between the two being acknowledged but not formalized.1 In their survey, Dickerson et al. (2023b) highlight this as a general issue in fair clustering: "each notion that was introduced [...] does not refer to or consider the interaction with the previously introduced fairness notions in clustering". Moreover, they call for "other fairness notions in clustering that are also compatible with one another" and "general notions which possibly encompass existing ones".

Footnote 1: For instance, in a recent tutorial on fair clustering (Awasthi et al., 2022), the two notions were treated as separate unconnected paradigms.

We follow this call and prove proportional and individual fairness, as well as a fairness notion by Li et al. (2021) which we will call the _transferable core_, to be tightly related to another. In an effort to encompass these three notions, we make use of proportionality axioms from _multiwinner voting_, an area in computational social choice (Lackner and Skowron, 2022). Here, given the votes of \(n\) agents, the goal is to elect a size-\(k\) committee which fulfills some proportionality guarantee. We lift one of the simplest proportionality guarantees (JR) to work with metric distances and prove that any clustering fulfilling our guarantee also fulfills the best approximations for the three notions, all _simultaneously_. Moreover, such a clustering can be computed in polynomial time. Taking the multiwinner voting approach further, we also look at the lifted version of a stronger proportionality guarantee (PJR). This changes how points (agents) interact with cluster centers as they become represented not by one, but possibly multiple centers. While this is not standard for "vanilla" clustering, it is very fitting for more democratic settings, where the chosen "centers" end up possessing voting power to represent the agents. The resulting proportionality guarantee indeed highly relates to work by Ebadian and Micha (2024) who, motivated by _sortition_ (the randomized selection of citizens' panels (Flanigan et al., 2021)), introduced a generalization of the proportional fairness notion. Indeed, the multiwinner voting perspective allows us to prove better approximation guarantees for their fairness notion.

Our contributions.As our first main result, we provide a simple bridge between proportional fairness and individual fairness (see Section 2). Any approximation of the former is also an approximation of the latter. In particular, for any \(\alpha,\beta\geq 1\) we show that (i) any \(\alpha\)-approximation to proportional fairness is also an \((1+\alpha)\)-approximation to individual fairness and (ii) any \(\beta\)-approximation to individual fairness is also a \(2\beta\)-approximation to proportional fairness. These approximations are tight. We also prove a similar connection between proportional fairness and the transferable core. Our connections imply for instance that bi-criteria approximations that optimize \(k\)-means and, say, individual fairness (Vakilian and Yalciner, 2022; Bateni et al., 2024) also maintain approximations guarantees to the other fairness notions. Further, if one wants to show incompatibility of a different clustering notion with approximate proportional or individual fairness, it is sufficient to show this for one of the two notions, instead of creating instances for both (as done by Dickerson et al. (2023a)).

Secondly, in Section 3, we draw a connection to the area of multiwinner voting and reinterpret proportionality notions introduced by Brill and Peters (2023) to work with distance metrics; we call the resulting guarantees mJR and mPJR. Both of these are efficiently computable when the space of possible centers is finite. Remarkably, with simple proofs, we are able to show that any clustering satisfying mJR achieves the _best known_ approximations to individual and proportional fairness notions and the transferable core. For the transferable core, we even improve upon the bound derived by Li et al. (2021). Finally, motivated by settings such as partition and multiwinner voting in which agents do not only care about their closest cluster center but are represented by multiple centers, we show that a strong core stability guarantee (introduced by Ebadian and Micha (2024)) can be achieved by any clustering satisfying mPJR. We also deal with the case in which the center candidate space is unbounded (e.g., in Euclidean clustering settings), in which the above-mentioned algorithms can become intractable. Here, we show that satisfying the proportionality guarantees only for the set of agents is sufficient to obtain constant-factor approximations to proportional fairness and the core stability guarantee by Ebadian and Micha (2024).

Lastly, in Section 4, we focus on sortition: Here, the set of agents and cluster candidates is equal and each agent must be chosen with equal probability. Employing techniques from the above results, we are able to give a simpler proof achieving a better approximation guarantee for the core notion by Ebadian and Micha (2024).

Figure 1 (left) gives an overview over our results and our achieved approximation guarantees. Proofs of some results are deferred to a full version of this manuscript (Kellerhals and Peters, 2023).

Related work._Individual fairness_ was introduced by Jung et al. (2020). Since then, follow-up work mainly focused on bi-criteria approximation guarantees (Mahabadi and Vakilian, 2020; Negahbani and Chakrabarty, 2021; Vakilian and Yalciner, 2022; Chhaya et al., 2022; Bateni et al., 2024). Additionally, Han et al. (2023) studied individual fairness for clustering with outliers and Sternbach and Cohen (2023) incorporated demographic information into individual fairness. The individual fairness notion was also carried over to the setting of approval-based multiwinner voting (Brill et al., 2024). We mention that the name "individual fairness" is also used for other (unrelated) fairness notions (e.g. Kar et al., 2023; Chakrabarti et al., 2022).

_Proportional fairness_ was first studied by Chen et al. (2019). Micha and Shah (2020) showed that the greedy capture algorithm by Chen et al. achieves better approximation guarantees in certain metric spaces (including the Euclidean space with the \(2\)-norm) and studied its complexity. Li et al. (2021) introduced notions inspired by Chen et al., which are related to the transferable core concept from algorithmic game theory. Aziz et al. (2024) introduced proportionality axioms and rules directly inspired from social choice theory to proportional clustering. Among other things, they showed that every outcome satisfying DPRF (see Section 3.1) achieves an \(\left(1+\sqrt{2}\right)\)-approximation to proportional fairness. Further connections to social choice or relations between the above fairness notions of Jung et al. (2020) or Li et al. (2021) remain unexplored, though.

Ebadian and Micha (2024) study proportionality in the setting of _sortition_ (see e.g., Flanigan et al. (2021)), proposing a generalization of proportional fairness and a refined variant of greedy capture. This variant and its proportionality were used by Caragiannis et al. (2024) to construct panels whose decisions align with that of the underlying population. The most recent work directly related to ours was created independently and in parallel to ours by Kalayci et al. (2024). They study proportional fairness and the transferable core in an incomplete information setting and show that just knowing the order of the distances to between agents and center candidates suffices to achieve a \(5.71\)-approximation to proportional fairness.

Caragiannis et al. (2024) study proportional fairness in a _non-centroid_ based fair clustering setting, in which points are not assigned to cluster centers. Instead they are grouped into clusters and derive utility based on the other agents in their cluster. For this setting, Caragiannis et al. (2024) also build on the proportional fairness framework studied in this work and also take inspiration from concepts from multiwinner voting: in their case, they adopt the _FJR_ axiom of Peters et al. (2021). The setting of non-centroid clustering is closely related to the study of _hedonic games_, with the difference being that in hedonic games, the number of clusters is not pre-determined. See for instance, Fanelli et al. (2021), Demeulemeester and Peters (2023), and Fioravanti et al. (2023) for recent works on approximate core stability in hedonic games. Ahmadi et al. (2022), Aamand et al. (2023), and Mosenzon and Vakilian (2024) further studied a notion of individual stability for clustering, in which individual agents should not be able to deviate from their clusters. This, however, is unrelated to group stability as studied in this work.

Figure 1: _Left:_ An overview over connections between and bounds on fairness notions, i.e., \(\alpha\)-proportional fairness (\(\alpha\)-PF), \(\beta\)-individual fairness (\(\beta\)-IF), the \((\gamma,\alpha)\)-transferable core (\((\gamma,\alpha)\)-TC), and the \(\alpha\)-\(q\)-core. See Sections 2 and 3 for the corresponding definitions and results. If \(\textsc{A}\to\Pi\), then algorithm \(\textsc{A}\) produces outcomes satisfying \(\Pi\). If \(\Pi\to\Gamma\), then any outcome satisfying \(\Pi\) also satisfies \(\Gamma\). If \(\Gamma\) takes a parameter \(\alpha\), then the label specifies the parameter that can be satisfied (for the transferable core, the result holds for all \(\gamma>1\)). _Right:_ The metric space for the examples used throughout the paper. Edges without labels have length \(1\), the distance between any two points is given by the length of the shortest path between them.

_Multiwinner voting_ is the branch of computational social choice theory dealing with selecting multiple instead of just one candidate as a winner. A main branch herein focuses on _proportionality_. While much of the literature on proportionality, starting with Aziz et al. (2017), focuses on approval preferences (see Lackner and Skowron (2022) for a recent book on this topic), proportionality notions also exist for ordinal preferences (Dummett, 1984). These notions were recently strengthened by Aziz and Lee (2020, 2021) and Brill and Peters (2023), with the latter forming the basis for the proportionality axioms we discuss in this paper. We are further closely related to the works of Caragiannis et al. (2022) and Ebadian et al. (2022) who studied the representation of a given committee by investigating the distances of agents to their \(q\)-closest committee member.

Model and notation.Let \((\mathcal{X},d)\) be a (pseudo)-metric space with a distance function \(d\colon\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) satisfying \(d(i,i)=0\), \(d(i,j)=d(j,i)\) and \(d(i,j)+d(j,k)\geq d(i,k)\). Let \(i\in\mathcal{X}\) be a point. For \(r\in\mathbb{R}\), define \(B(i,r)=\{j\in\mathcal{X}\colon d(i,j)\leq r\}\) to be the ball of radius \(r\) around \(i\). For \(W\subseteq\mathcal{X}\), let \(d(i,W)=\min_{c\in W}d(i,c)\). For \(q\leq|W|\), \(d^{q}(i,W)\) is distance to the \(q\)-th closest point in \(W\) to \(i\). Note that \(d^{1}(i,W)=d(i,W)\) and that \(d^{q}(i,W)\leq d(i,j)+d^{q}(j,W)\) for \(i,j\in N\).

Throughout the paper, we are given a set of _agents_\(N=[n]\) and a (possibly infinite) set of _candidates (facilities)_\(C\), both of which lie in a metric space \((\mathcal{X},d)\), and a number \(k\in\mathbb{N}^{+}\). A _clustering_ or _outcome_ is a subset \(W\subseteq C\) of at most \(k\) candidates. The elements \(c\in W\) are called _centers_. Our examples use the _(weighted) graph metric_ in which the points are the vertices of a graph with edge lengths, and the distance between two points is the length of a shortest path between them.

## 2 Relations between proportional fairness notions

In this section, we prove the relations between proportional fairness (Chen et al., 2019), individual fairness (Jung et al., 2020), and the transferable core (Li et al., 2021). We first define the notions.

The idea of _proportional fairness_ is the following: If there is a candidate \(c\) such that at least \(\frac{n}{k}\) agents are closer to \(c\) by a factor \(\alpha\) than to their closest cluster center in the outcome \(W\), then we say that the agents will _deviate_ to \(c\). If there is no such candidate, the outcome satisfies \(\alpha\)-proportional fairness.

**Definition 1**.: For \(\alpha\geq 1\) an outcome \(W\) satisfies \(\alpha\)-_proportional_ fairness, if there is no group \(N^{\prime}\subseteq N\) of agents with \(|N^{\prime}|\geq\frac{n}{k}\) and \(c\notin W\) such that \(\alpha\cdot d(i,c)<d(i,W)\) for all \(i\in N^{\prime}\).

While \((2-\varepsilon)\)-proportional fair outcomes need not exist (for any \(\varepsilon>0\)), \(\left(1+\sqrt{2}\right)\)-proportional fair outcomes can be computed for any metric space (Chen et al., 2019; Micha and Shah, 2020).

To define _individual fairness_, denote by \(r_{N,k}(i)\) be the radius of the smallest ball around an agent \(i\in N\) that encloses at least \(\frac{n}{k}\) agents, i.e., \(r_{N,k}(i)=\min\{r\in\mathbb{R}\colon|B(i,r)\cap N|\geq\frac{n}{k}\}\). We drop the subscripts \(N\) and \(k\) if clear from context. For this definition to properly work, we additionally need the assumption that \(N\subseteq C\), i.e., any agent can be chosen as center. Otherwise, a secluded group of agents without any possible cluster centers around them would never be able to get a center close to them in the outcome. Indeed, this is a plausible restriction in metric clustering, as oftentimes the centers may be picked from the (infinite) set of points in the metric space.

**Definition 2**.: For an instance with \(N\subseteq C\), for \(\beta\geq 1\) an outcome \(W\) satisfies \(\beta\)-_individual fairness_ if \(d(i,W)\leq\beta r_{N,k}(i)\) for all \(i\in N\).

It is known that an outcome satisfying \(2\)-individual fairness always exists, while there are instances with no \((2-\varepsilon)\)-individually fair outcome (Jung et al., 2020).

The _transferable core_2 notion is based on the concept of transferable utilities from game theory. Comparing to proportional fairness, the notion considers the average utility for each group.

Footnote 2: We remark that Li et al. (2021) call this notion just “core”, we rename it to avoid confusion with the core notion of Ebadian and Micha (2024).

**Definition 3**.: For \(\gamma,\alpha\geq 1\), an outcome \(W\) is in the \((\gamma,\alpha)\)_-transferable core_ if there is no group of agents \(N^{\prime}\subseteq N\) and candidate \(c\notin W\) with \(|N^{\prime}|\geq\gamma\frac{n}{k}\) and \(\alpha\sum_{i\in N^{\prime}}d(i,c)<\sum_{i\in N^{\prime}}d(i,W)\).

It is known that the for any \(\gamma>1\) there are outcomes in the \((\gamma,\max(4,\frac{3\gamma-1}{\gamma-1}))\)-transferable core while there need not be outcomes in the \((\gamma,\min(1,\frac{1}{\gamma-1}))\)-transferable core (Li et al., 2021).

_Example 1_.: Consider the instance depicted in Figure 1 (right) with \(k=5\) and the associated graph distance metric. Assume that cluster centers can only be placed on the depicted agents. We have \(\frac{n}{k}=2\); thus any two agents are able to deviate to another center. The outcome \(W=\{1,2,3,6,9\}\) satisfies \(1\)-proportional fairness: The agents \(1,\ldots,4\) have distance \(0\) to a center, while every remaining agent has distance at most \(1\) to a center.

To see the difference between proportional fairness, individual fairness, and the transferable core, consider the same instance with \(k=4\), so \(\frac{n}{k}=2.5\). Here, the outcome \(W=\{1,2,6,7\}\) satisfies \(1\)-proportional fairness, however it does not satisfy \(1\)-individual fairness. Agent \(8\) could look at their \(2\) closest neighbors, \(5\) and \(9\), both at a distance of \(1\). However, the distance of \(8\) to the outcome is \(2\). Observe that \(W\) also is not in the \((1,1)\)-transferable core. Here, for the group \(N^{\prime}=\{8,9,10\}\) and candidate \(c=9\), we have \(\sum_{i\in N^{\prime}}d(i,c)=2<\sum_{i\in N^{\prime}}d(i,W)=4\). \(\diamond\)

### Proportional and individual fairness

We first show that proportional and individual fairness are the same up to a factor of at most \(2\).

**Theorem 1**.: _Let \(\alpha,\beta\geq 1\). If \(N\subseteq C\), then an outcome that satisfies \(\alpha\)-proportional fairness also satisfies \((1+\alpha)\)-individual fairness, and an outcome that satisfies \(\beta\)-individual fairness also satisfies \(2\beta\)-proportional fairness. If \(N=C\), then an outcome that satisfies \(\beta\)-individual fairness also satisfies \((1+\beta)\)-proportional fairness._

Proof.: Let \(W\subseteq C\) be an outcome satisfying \(\alpha\)-proportional fairness, \(j\in N\) be any agent, and \(N_{j}=\{i\in N\colon d(i,j)\leq r(j)\}\). As \(N\subseteq C\), there is an \(i\in N_{j}\) with \(d(i,W)\leq\alpha d(i,j)\); otherwise the coalition \(N_{j}\) deviates to candidate \(j\). Thus, by the triangle inequality, \(d(j,W)\leq d(i,j)+d(i,W)\leq(1+\alpha)d(i,j)\leq(1+\alpha)r(j)\), and hence \(W\) satisfies \((1+\alpha)\)-individual fairness.

Now suppose the outcome \(W\) satisfies \(\beta\)-individual fairness. Let \(N^{\prime}\subseteq N\) with \(|N^{\prime}|\geq\frac{n}{k}\) and \(c\notin W\) be an unchosen candidate. Take \(i^{*}\in N^{\prime}\) to be the agent in \(N^{\prime}\) furthest away from \(c\). If \(N\subseteq C\), then the radius \(r(i^{*})\) containing \(\lceil\frac{n}{k}\rceil\) agents is at most as large as the most distant agent in \(N^{\prime}\), i.e., there is an \(i^{\prime}\in N^{\prime}\) with \(r(i^{*})\leq d(i^{*},i^{\prime})\leq d(i^{*},c)+d(c,i^{\prime})\). Then \(d(i^{*},W)\leq\beta r(i^{*})\leq\beta(d(i^{*},c)+d(c,i^{\prime}))\leq 2\beta d(i^{* },c)\). If \(N=C\), then, since \(|N^{\prime}|\geq\frac{n}{k}\), we have \(r(c)\leq d(c,i^{*})\); thus \(d(c,W)\leq\beta d(c,i^{*})\). Therefore, \(d(i^{*},W)\leq d(i^{*},c)+d(c,W)\leq(1+\beta)d(i^{*},c)\), and thus \(W\) also satisfies \((1+\beta)\)-proportional fairness. 

Indeed, we also show that all three provided bounds are tight.

**Theorem 2**.: _For every \(\alpha,\beta\geq 1\) and \(\varepsilon>0\), there are instances with \(N=C\) for which there exists (1) an outcome which satisfies \(\alpha\)-proportional fairness, but not \((1+\alpha-\varepsilon)\)-individual fairness, and (2) an outcome which satisfies \(\beta\)-individual fairness, but not \((1+\beta-\varepsilon)\)-proportional fairness. Moreover, there are instances with \(N\subseteq C\) for which there exists (3) an outcome which satisfies \(\beta\)-individual fairness, but not \((2\beta-\varepsilon)\)-proportional fairness._

### Proportional fairness and the transferable core

It is easy to see that the \((1,\alpha)\)-transferable core implies \(\alpha\)-proportional fairness. For \(\gamma>1\) however, the \((\gamma,\alpha)\)-transferable core does not imply any meaningful proportional fairness approximation (consider \(\frac{n}{k}\) agents on one point and \((\gamma-1)\frac{n}{k}\) agents "far" away). Hence, we focus on the other direction and show that a proportional fairness approximation implies one to the transferable core.

**Theorem 3**.: _An outcome satisfying \(\alpha\)-proportional fairness is in the \(\left(\gamma,\frac{\gamma(\alpha+1)}{\gamma-1}\right)\)-transferable core for any \(\alpha\geq 1\) and \(\gamma>1\)._

Proof.: Let \(W\subseteq C\) satisfy \(\alpha\)-proportional fairness. Let \(N^{\prime}\subseteq N\) be a group of agents of size \(n^{\prime}\geq\gamma\frac{n}{k}\), \(c\notin W\), and shorten \(\eta=\lceil\frac{n}{k}\rceil\). Further, assume the agents \(N^{\prime}=\{i_{1},\ldots,i_{n^{\prime}}\}\) are ordered by their increasing distance to \(c\), i.e., \(d(i_{j},c)\leq d(i_{j+1},c)\) for every \(j\in[n^{\prime}-1]\). Let \(J_{0}=\{i_{1},\ldots,i_{\eta}\}\) and \(j_{0}\in J_{0}\) such that \(d(j_{0},W)\leq\alpha d(j_{0},c)\); such an agent must exist due to \(\alpha\)-proportional fairness. Next, for \(\lambda=1,\ldots,n^{\prime}-\eta\), we inductively define \(J_{\lambda}=\{i_{1},\ldots,i_{\eta+\lambda}\}\setminus\{j_{0},\ldots,j_{\lambda -1}\}\), and choose \(j_{\lambda}\in J_{\lambda}\) such that \(d(j_{\lambda},W)\leq\alpha d(j_{\lambda},c)\leq\alpha d(i_{\eta+\lambda},c)\) (note that \(|J_{\lambda}|=\eta\)). Thus,

\[\sum_{\lambda=0}^{n^{\prime}-\eta}d(j_{\lambda},W)\leq\alpha\sum_{z=\eta}^{n^{ \prime}}d(i_{z},c)\leq\alpha\sum_{i\in N^{\prime}}d(i,c).\] (1)

[MISSING_PAGE_EMPTY:6]

We want to point out that mJR is _significantly_ weaker than mPJR. Indeed, to satisfy mPJR, an outcome \(W\) may need to contain several candidates \(c\) such that \(d(i,c)>d(i,W)\) for all agents \(i\in N\), i.e., \(c\) is no-ones "first choice" among \(W\); mJR does not have this property. This makes mJR the more sensible of the two axioms for "vanilla" clustering, in which one only cares about the closest center to each agent. mPJR however, is a natural axiomatic choice for settings such as sortition or even social choice in general: Here, agents may benefit from having more than a single representative. We provide some intuition for mJR and mPJR and this property in the example below.

_Example 2_.: To see the differences between the proportionality axioms, consider the instance depicted in Figure 1 (right). First consider instance (a) on the left with \(N=C=\{1,\ldots,10\}\), \(k=4\), and the outcome \(W=\{1,2,3,6\}\). Here, \(\frac{n}{k}=2.5\). First, we note that this outcome does not satisfy \(1\)-proportional fairness: The agents \(8,9,10\) are closer to \(9\) than they are to the closest winner in \(W\). It does however satisfy mJR: Among every group of at least three agents that have a common candidate within distance \(y\), there is one agent that has a cluster center \(w\in W\) within distance \(y\). For example, \(8,9,10\) have candidate \(9\) at distance \(1\), and the distance of \(9\) to the closest center is also \(1\). This outcome does not satisfy mPJR though, since the group \(5,\ldots,10\) would deserve at least two candidates within distance \(1\) in \(W\). An outcome satisfying mPJR is \(W=\{1,2,3,9\}\). For \(y=0\), only the group \(\{1,\ldots,4\}\) shares a candidate, but also have a center at distance \(0\).

If \(k=5\), then, to satisfy mPJR, an outcome must contain at least two of the four candidates. But there are outcomes satisfying mJR that contain only one of \(1,\ldots,4\). This property of mPJR makes it suited for settings in which agents may want to be represented by multiple candidates, e.g., in political settings, in which the candidates end up possessing voting power to represent the agents. \(\diamond\)

Independently of Brill and Peters (2023); Aziz et al. (2024) introduced two notions they call _Proportionally Representative Fairness_. The first notion is called "discrete" (DPRF), and the second is called "unconstrained" (UPRF). Indeed, DPRF is equivalent to mPJR. UPRF was introduced to tackle the case when the candidate space is unbounded. We discuss how it relates to mPJR and the other fairness notions in the full version (Kellerhals and Peters, 2023).

Aziz et al. (2024) show that an outcome satisfying DPRF (mPJR) also fulfills \(\left(1+\sqrt{2}\right)\)-proportional fairness. We show hereafter that this already holds for the (much weaker) mJR axiom.

### Fairness bounds for mJR outcomes

We now prove the approximation guarantees implied by mJR. We remark that the bound for the transferable core below improves upon the analysis of Li et al. (2021). The proof is deferred to the full version (Kellerhals and Peters, 2023).

**Theorem 5**.: _Let \(W\) be an outcome satisfying mJR. Then it also satisfies \(\left(1+\sqrt{2}\right)\)-proportional fairness, \(2\)-individual fairness, and is in the \(\left(\gamma,\frac{2\gamma}{\gamma-1}\right)\)-transferable core for any \(\gamma>1\)._

If the candidate space is finite, then an outcome satisfying mJR can be computed in polynomial time by the greedy capture algorithm (Chen et al., 2019; Micha and Shah, 2020; Li et al., 2021). We briefly recall its procedure:

greedy capture starts off with an empty clustering \(W\). It maintains a radius \(\delta\) (initially \(\delta=0\)) and smoothly increases \(\delta\). If there is a candidate \(c\) such that at least \(\frac{n}{k}\) agents have distance at most \(\delta\) to \(c\), it adds \(c\) to \(W\) and deletes the \(\frac{n}{k}\) agents. If an agent has distance at most \(\delta\) to a candidate in \(W\), then it is deleted as well. This is continued until all agents are deleted.

_Example 3_.: Consider the instance in Figure 2. Here, with \(k=4\), greedy capture, would first open one of \(1,\ldots,4\) with \(\delta=0\) and remove all agents from \(1,\ldots,4\). Then for \(\delta=1\) it could either open \(6\) or \(9\), removing all adjacent agents to it. Then there are two agents remaining, which would

Figure 2: Metric space for some of the examples. Edges without labels have length \(1\).

be assigned to either \(6\) or \(9\) for \(\delta=2\). Thus, in this instance, greedy capture only opens two clusters. \(\diamond\)

**Proposition 6**.: _Any outcome returned by greedy capture satisfies mJR._

### Fairness bounds for mPJR outcomes

Recall that mPJR is equivalent to the DPRF notion by Aziz et al. (2024). To satisfy their notion, they designed a generalization of the expanding approvals rule from multiwinner voting Aziz and Lee (2020, 2021) (in which the agents' preferences over the candidates are ordinal) to the setting of proportional clustering. They refer to this generalization as spatial expanding approvals. As Aziz et al. show, spatial expanding approvals can be computed in polynomial time for finite candidate spaces.

In general, spatial expanding approvals behaves similarly to greedy capture. It also starts off with an empty clustering \(W\) and a radius \(\delta=0\) as well and additionally gives each agent a budget \(b_{i}=\frac{k}{n}\). It then smoothly increases the radius \(\delta\). When there is a candidate \(c\notin W\) for which the agents at a distance of at most \(\delta\) have a budget of at least \(1\), it decreases the budget of these agents collectively by exactly \(1\) and adds \(c\) to \(W\).

_Example 4_.: Consider the instance in Figure 2. Here, with \(k=4\), spatial expanding approvals would give each agent a budget of \(\frac{4}{10}=\frac{2}{5}\). For \(\delta=0\), it will open a cluster from \(1,\ldots,4\) and decrease their budgets by exactly \(1\), for instance it could set the budget of \(1\) and \(2\) to \(0\) and of \(3\) to \(\frac{1}{5}\). Then for \(\delta=1\) it could again open \(6\) and \(9\), for instance by removing the budget of \(6\) and \(9\) to \(\frac{1}{5}\) and of \(5,7,8,10\) to zero. The remaining budget is exactly \(1\), which would be spent for \(\delta=10\) on \(5\). Thus, one possible final outcome is \(\{1,5,6,9\}\). \(\diamond\)

Remarkably, as shown by Aziz et al. (2024), it does not matter in which way the budget is subtracted and which candidate meeting the budget is selected; the outcomes of the algorithm will satisfy mPJR in any case.

**Proposition 7**.: _Any outcome returned by spatial expanding approvals satisfies mPJR._

We now turn to fairness measures implied by mPJR. As any outcome satisfying mPJR also fulfills mJR, the results in Theorem 5 also hold for mPJR. Indeed, mPJR is stricter in the sense that larger groups must also be represented justly by a proportional number of candidates: an \(\alpha\) percentage of the population should roughly be close to an \(\alpha\) percentage of the centers.

This property makes mPJR fit well into metric social choice settings such as sortition. For this, Ebadian and Micha (2024) introduced a fairness notion that measures proportionality in this setting by considering for each agent not only the closest center, but the first \(q\) closest centers. In that, their notion called \(\alpha\)-\(q\)-core naturally generalizes \(\alpha\)-proportional fairness; the two are equal when \(q=1\).

**Definition 5**.: For \(\alpha\geq 1\) an outcome \(W\) is in the \(\alpha\)-\(q\)-_core_, if there is no \(\ell\in\mathbb{N}\) and no \(N^{\prime}\subseteq N\) with \(|N^{\prime}|\geq\ell^{n}_{k}\) and set \(C^{\prime}\subseteq C\) with \(q\leq|C^{\prime}|\leq\ell\) such that \(\alpha\cdot d^{q}(i,C^{\prime})<d^{q}(i,W)\) for all \(i\in N^{\prime}\).

_Example 5_.: Consider the instance in Figure 2 with \(k=5\) and the outcome \(W=\{1,2,3,6,9\}\). As mentioned above, \(W\) satisfies \(1\)-proportional fairness. For the \(3\)-core however, consider the set \(N^{\prime}=\{5,\ldots,10\}\) deviating to \(C^{\prime}=\{6,9,10\}\). The distance of any member of \(N^{\prime}\) to \(6\), \(9\), or \(10\) is at most \(3\), while the distance to the third most distant center in the outcome is at least \(10\). Thus, when considering the distances to the third most distant candidate in \(C^{\prime}\), every agent in \(N^{\prime}\) would improve by at least a factor of \(\frac{10}{3}\). Thus, \(W\) is only in the \(\frac{10}{3}\)-\(3\)-core. \(\diamond\)

We mention in passing that we introduce similar gerneralizations for individual fairness and the transferable core, in which each agent is represented by \(q\) candidates instead of one. The definitions and obtained results can be found in the full version of this paper Kellerhals and Peters (2023).

Ebadian and Micha (2024) show that, if \(N=C\) (every agent is a candidate and vice versa), for a given \(q\), one can compute a \(\frac{5+\sqrt{41}}{2}q\)-core outcome.3 We show that mPJR (or DPRF) provides a better guarantee for the \(q\)-core, for all values of \(q\) simultaneously.

Footnote 3: In addition, their randomized algorithm selects each agent with the same probability, a desirable property in the context of sortition, i.e., the randomized selection of citizen assemblies Flanigan et al. (2021).

**Theorem 8**.: _If an outcome satisfies mPJR, then, for every \(q\leq k\), it is in the \(5\)-\(q\)-core._

[MISSING_PAGE_FAIL:9]

Stronger fairness bounds for sortition

Ebadian and Micha (2024) introduced fair greedy capture, a randomized generalization of greedy capture for the setting of sortition. It works in the setting in which \(N=C\) and is parameterized by some parameter \(q\leq k\). Like greedy capture it smoothly increases a radius around each agent/candidate. Once this radius contains at least \(q\frac{n}{k}\) agents, it selects \(q\) of them uniformly at random and deletes in total \(\lceil q\frac{n}{k}\rceil\) of these agents. Together with an adequate final sampling step, one can show that this selects each agent with a probability of exactly \(\frac{k}{n}\).

Ebadian and Micha show that any clustering returned by the algorithm is in the \(\frac{3+\sqrt{17}}{2}\)-1-core when parameterized by \(q=1\) and in the \(\frac{5+\sqrt{41}}{2}\approx 5.7\)-\(q\)-core when parameterized by \(q>1\).4 We improve upon their analysis (with a simpler proof) and show that fair greedy capture satisfies a better bound for every parameter \(q\leq k\).

Footnote 4: Recall that the algorithm receives \(q\) as a parameter in the input; thus, as opposed to our Theorem 8, the bounds only hold for one \(q\) at a time. In a recently updated version of their preprint, Ebadian and Micha (2024) modified their initial version of FGC to additionally satisfy the \(6\)-\(q\) core for all \(q\) simultaneously.

**Theorem 12**.: _Let \(N=C\) and \(q\leq k\). Then any outcome \(W\) returned by fair greedy capture parameterized by \(q\) is in the \(\frac{3+\sqrt{17}}{2}\)-\(q\)-core._

## 5 Conclusion and future work

In this paper, we studied proportional clustering from a social choice perspective and showed that our new _metric JR axioms_ enable near-optimal approximations of fairness notions for clustering. An interesting open question, both relevant to social choice and clustering is related to a different relaxation of proportional fairness (or core fairness) introduced by Jiang et al. (2020). Instead of bounding the factor by which the agents can improve, they bound the size of the deviating coalition (similar to the transferable core). In that sense, no group of size \(\gamma\frac{n}{k}\) should exist, who could all deviate to a candidate they like more. In their work, they show that there are instances for which no solution with \(\gamma<2\) can exist while for any \(\varepsilon>0\) a solution with \(\gamma=16+\varepsilon\) exists. Since these results only care about the relative ordering of the candidates, they also translate to clustering. Closing this bound, or improving it for certain metric spaces, seems like an interesting problem. It would be also intriguing to study the probabilistic analog of the core (Cheng et al., 2020; Jiang et al., 2020), especially if the results generalize to the \(q\)-core and if certain metric spaces admit simple algorithms to compute it.

Further, spatial expanding approvals (Section 3.3) is more of a family of algorithms, parameterized by how candidates are selected and how budgets are deducted. Is there any way to axiomatically (or quantitatively) distinguish its different parameterizations? In the context of approval-based multi-winner voting, the _Method of Equal Shares_(Peters and Skowron, 2020) can be seen as an instantiation of spatial expanding approvals which provides stronger proportionality guarantees than other algorithms in the family. Is something similar possible for our setting, e.g., can one go from proportionality axioms inspired by PJR to axioms inspired by the stronger EJR axiom (Aziz et al., 2017)? As shown by Brill and Peters (2023, Example 7) the straightforward extension of studying mEJR (or rank-EJR in their notation) is not possible, as outcomes satisfying mEJR may easily fail to exist. However, the metric variant of the PJR+ axiom of Brill and Peters (2023) may be of greater interest in the clustering setting. It is easy to see that spatial expanding approvals satisfies mPJR+. Is it also possible to derive better proportionality or core approximations from mPJR+?

Naturally, our work still leaves several open questions when it comes to the approximation factors of our notions. What are the best attainable factors for proportional fairness and the \(q\)-core? Further, the questions of Jung et al. (2020) whether the bound of \(2\) on individual fairness can be improved for Euclidean spaces and of Micha and Shah (2020) whether for (unweighted) graph metrics with \(N=C\) a \(1\)-proportional fair clustering always exist, are still open.

## References

* A. Aamand, J. Chen, A. Liu, S. Silwal, P. Sukprasert, and A. Vakilian (2024)Constant approximation for individual preference stable clustering. In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS), pp. 43646-43661. Cited by: SS1.

S. Ahmadi, P. Awasthi, S. Khuller, M. Kleindessner, J. Morgenstern, P. Sukprasert, and A. Vakilian (2022)Individual preference stability for clustering. In Proceedings of the 39th International Conference on Machine Learning (ICML), pp. 162:197-246. Cited by: SS1.
* P. Awasthi, B. Brubach, D. Chakrabarty, J. P. Dickerson, S. A. Esmaeili, M. Kleindessner, M. Knittel, J. Morgenstern, S. Samadi, A. Srinivasan, and L. Tsepenekas (2022)Fairness in clustering. Note: https://www.fairclustering.com/ Cited by: SS1.
* H. Aziz and B. E. Lee (2020)The expanding approvals rule: improving proportional representation and monotonicity. Social Choice and Welfare54, pp. 1-45. Cited by: SS1.
* H. Aziz and B. E. Lee (2021)Proportionally representative participatory budgeting with ordinal preferences. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI), pp. 5110-5118. Cited by: SS1.
* H. Aziz, B. E. Lee, S. Morota Chu, and J. Vollen (2024)Proportionally representative clustering. In Proceedings of the 20th International Workshop on Internet and Network Economics (WINE), pp.. Cited by: SS1.
* M. Bateni, V. Cohen-Addad, A. Epasto, and S. Lattanzi (2024)A scalable algorithm for individually fair k-means clustering. In Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 3151-3159. Cited by: SS1.
* M. Brill and J. Peters (2023)Robust and verifiable proportionality axioms for multiwinner voting. In Proceedings of the 24th ACM Conference on Economics and Computation (ACM-EC), pp.. Cited by: SS1.
* M. Brill, J. Israel, E. Micha, and J. Peters (2024)Individual representation in approval-based committee voting. Social Choice and Welfare. Cited by: SS1.
* I. Caragiannis, N. Shah, and A. A. Voudouris (2022)The metric distortion of multiwinner voting. Artificial Intelligence313, pp. 103802. Cited by: SS1.
* I. Caragiannis, E. Micha, and J. Peters (2024)Can a few decide for many? The metric distortion of sortition. In Proceedings of the 41st International Conference on Machine Learning (ICML), pp. 235:5660-5679. Cited by: SS1.
* I. Caragiannis, E. Micha, and N. Shah (2024)Proportional fairness in non-centroid clustering. In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS), pp.. Cited by: SS1.
* D. Chakrabarti, J. P. Dickerson, S. A. Esmaeili, A. Srinivasan, and L. Tsepenekas (2022)A new notion of individually fair clustering: \(\alpha\)-equitable k-center. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 6387--6408. Cited by: SS1.
* X. Chen, B. Fain, L. Lyu, and K. Munagala (2019)Proportionally fair clustering. In Proceedings of the 36th International Conference on Machine Learning (ICML), pp. 1032-1041. Cited by: SS1.
* Y. Cheng, Z. Jiang, K. Munagala, and K. Wang (2020)Group Fairness in Committee Selection. ACM Transactions on Economics and Computation8 (4), pp. 23:1-23:18. Cited by: SS1.
* R. Chhaya, A. Dasgupta, J. Choudhari, and S. Shit (2022)On coresets for fair regression and individually fair clustering. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 9603-9625. Cited by: SS1.
* F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii (2017)Fair clustering through fairlets. In Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), pp. 5029-5037. Cited by: SS1.
* T. Demeulemeester and J. Peters (2023)Relaxed core stability for hedonic games with size-dependent utilities. In Proceedings of the 48th International Symposium on Mathematical Foundations of Computer Science (MFCS 2023), pp. 41:1-41:14. Cited by: SS1.
*J. P. Dickerson, S. A. Esmaeili, J. Morgenstern, and C. J. Zhang. Doubly constrained fair clustering. In _Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS)_, pages 13267-13293, 2023a.
* Dickerson et al. [2023b] J. P. Dickerson, S. A. Esmaeili, J. Morgenstern, and C. J. Zhang. Fair clustering: Critique, caveats, and future directions. Neurips 2023 Workshop on Algorithmic Fairness through the Lens of Time, 2023b. Full version arXiv:2406.15960 [cs.LG].
* Dummett [1984] M. Dummett. _Voting Procedures_. Oxford University Press, 1984.
* Ebadian and Micha [2024] S. Ebadian and E. Micha. Boosting sortition via proportional representation. Technical report, arXiv:2406.00913 [cs.GT], 2024.
* Ebadian et al. [2022] S. Ebadian, G. Kehne, E. Micha, A. D. Procaccia, and N. Shah. Is sortition both representative and fair? In _Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS)_, pages 3431-3443, 2022.
* Fanelli et al. [2021] A. Fanelli, G. Monaco, and L. Moscardelli. Relaxed core stability in fractional hedonic games. In _Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI)_, pages 182--188, 2021.
* Fioravanti et al. [2023] S. Fioravanti, M. Flammini, B. Kodric, and G. Varricchio. \(\varepsilon\)-fractional core stability in hedonic games. In _Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS)_, pages 28723-28734, 2023.
* Flanigan et al. [2021] B. Flanigan, P. Golz, A. Gupta, B. Hennig, and A. D. Procaccia. Fair algorithms for selecting citizens' assemblies. _Nature_, 596:548-552, 2021.
* Han et al. [2023] L. Han, D. Xu, Y. Xu, and P. Yang. Approximation algorithms for the individually fair k-center with outliers. _Journal of Global Optimization_, 87(2):603-618, 2023.
* Jiang et al. [2020] Z. Jiang, K. Munagala, and K. Wang. Approximately stable committee selection. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC)_, pages 463-472. ACM, 2020.
* Jung et al. [2020] C. Jung, S. Kannan, and N. Lutz. Service in your neighborhood: Fairness in center location. In _Proceedings of the 1st Symposium on Foundations of Responsible Computing (FORC)_, pages 5:1--5:15, 2020.
* Kalayci et al. [2024] Y. H. Kalayci, D. Kempe, and V. Kher. Proportional representation in metric spaces and low-distortion committee selection. In _Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI)_, pages 9815-9823, 2024.
* Kar et al. [2023] D. Kar, M. Kosan, D. Mandal, S. Medya, A. Silva, P. Dey, and S. Sanyal. Feature-based individual fairness in k-clustering. In _Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, pages 2772-2774, 2023.
* Kellerhals and Peters [2023] L. Kellerhals and J. Peters. Proportional fairness in clustering: A social choice perspective. Technical report, arXiv:2310.18162 [cs.LG], 2023.
* Lackner and Skowron [2022] M. Lackner and P. Skowron. _Multi-Winner Voting with Approval Preferences_. Springer, 2022.
* Li et al. [2021] B. Li, L. Li, A. Sun, C. Wang, and Y. Wang. Approximate group fairness for clustering. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, pages 6381-6391, 2021.
* Mahabadi and Vakilian [2020] S. Mahabadi and A. Vakilian. Individual fairness for k-clustering. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 6586-6596, 2020.
* Micha and Shah [2020] E. Micha and N. Shah. Proportionally fair clustering revisited. In _Proceedings of the 47th International Colloquium on Automata, Languages, and Programming (ICALP)_, pages 85:1-85:16, 2020.
* Mosenzon and Vakilian [2024] R. Mosenzon and A. Vakilian. Scalable algorithms for individual preference stable clustering. In _Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1108-1116. PMLR, 2024.
* Mosenzon et al. [2020]M. Negahbani and D. Chakrabarty. Better algorithms for individually fair \(k\)-clustering. In _Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS)_, pages 13340-13351, 2021.
* Peters and Skowron [2020] D. Peters and P. Skowron. Proportionality and the limits of welfarism. In _Proceedings of the 21st ACM Conference on Economics and Computation (ACM-EC)_, pages 793-794. ACM, 2020.
* Peters et al. [2021] D. Peters, G. Pierczynski, and P. Skowron. Proportional participatory budgeting with additive utilities. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 12726-12737, 2021.
* Sternbach and Cohen [2023] H. Sternbach and S. Cohen. Fair facility location for socially equitable representation. In _Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, pages 2775-2777, 2023.
* Vakilian and Yalcuner [2022] A. Vakilian and M. Yalcuner. Improved approximation algorithms for individually fair clustering. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 8758-8779, 2022.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims including claimed approximation guarantees and connections are shown via proofs Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss several lower bounds and incompatibilities throughout the paper. We do not really make any assumptions (except being given a metric space), run no experiments, and we discuss computational issues in Section 3.4 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide proofs (or references) for all theoretical results given. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our work does not contain any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We provide no experimental results and use no data or code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: We provide no experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We provide no experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: We provide no experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not work with human subjects, nor create datasets. We do not see any overlap with the described potential ethics violations. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a purely theoretical owrk. We do not think that any of the examples listed below fit to be discussed here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not see any potential high risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use any assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not use any assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No experiments are performed in this paper. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No experiments are performed in this paper. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.