# Neural Krylov Iteration for Accelerating

Linear System Solving

 Jian Luo &Jie Wang\({}^{1}\)&Hong Wang\({}^{1}\)&Huanshuo Dong\({}^{1}\)

&Zijie Geng\({}^{1}\)&Hanzhu Chen\({}^{1}\)&Yufei Kuang\({}^{1}\)

\({}^{1}\)MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,

University of Science and Technology of China

{jianluo,wanghong1700}@mail.ustc.edu.cn

jiewangx@ustc.edu.cn

Corresponding author. Email: jiewangx@ustc.edu.cn

###### Abstract

Solving large-scale sparse linear systems is essential in fields like mathematics, science, and engineering. Traditional numerical solvers, mainly based on the Krylov subspace iteration algorithm, suffer from the low-efficiency problem, which primarily arises from the less-than-ideal iteration. To tackle this problem, we propose a novel method, namely **Neural** Krylov **Iteration** (**NeurKItt**), for accelerating linear system solving. Specifically, NeurKItt employs a neural operator to predict the invariant subspace of the linear system and then leverages the predicted subspace to accelerate linear system solving. To enhance the subspace prediction accuracy, we utilize QR decomposition for the neural operator outputs and introduce a novel projection loss function for training. NeurKItt benefits the solving by using the predicted subspace to guide the iteration process, significantly reducing the number of iterations. We provide extensive experiments and comprehensive theoretical analyses to demonstrate the feasibility and efficiency of NeurKItt. In our main experiments, NeurKItt accelerates the solving of linear systems across various settings and datasets, achieving up to a 5.5\(\times\) speedup in computation time and a 16.1\(\times\) speedup in the number of iterations.

## 1 Introduction

Solving linear systems is the cornerstone of scientific computing, with applications in various fields including mathematics, science, and engineering[28]. Traditional solvers rely on the Krylov subspace iteration algorithm to tackle large-scale sparse linear systems[59]. It starts with a random initial vector, which corresponds to a one-dimensional linear subspace, and progressively expands this subspace to approximate the solution by iteratively minimizing the residual error.

However, the convergence speed and stability of the Krylov subspace iteration algorithms are significantly influenced by the dimensions and characteristics of the employed subspaces in the iterations[3]. Computational inefficiency and instability often arise in scenarios involving high-dimensional problems or large matrices, especially when the matrices exhibit poor conditioning[9]. This inefficiency primarily arises from the less-than-ideal iteration, which leads to a higher number of iterations and consequently longer solving time. To address these challenges, our key insight is that prior knowledge about the linear system's invariant subspace benefits the Krylov subspace iteration by guiding the iteration process, which reduces the required number of iterations[13].

Motivated by this insight, we introduce **Neural** Krylov **Iteration** (**NeurKItt**), a novel method that leverages neural networks to accelerate linear system solving. NeurKItt comprises two modules:the subspace prediction module and the acceleration module. The subspace prediction module uses a neural network to predict the invariant subspace of linear systems. Inspired by the concept that mapping a linear system to its invariant subspace can be viewed as an operator between two Hilbert spaces, we adopt the neural operator for subspace prediction. We also integrate thin QR decomposition and a projection loss function to optimize training, improving subspace prediction performance. For the acceleration module, it leverages the property that Krylov subspace iteration approximates an invariant subspace of the linear system. When partial information about this subspace is provided, our acceleration module utilizes it to reduce the iterations needed for precise solutions, effectively accelerating the process and addressing the challenges.

We provide comprehensive analyses to demonstrate NeurKItt's efficiency. Furthermore, extensive experiments conducted across various solver settings and different PDE problems validate the effectiveness of our approach. The results show that NeurKItt significantly accelerates linear systems solving, achieving up to a 5.5\(\times\) speedup. Both our theoretical analyses and experimental results collectively demonstrate the efficiency of NeurKItt.

We summarize our contributions as follows:

* To the best of our knowledge, our work is the first to apply the data-driven approach to optimize Krylov iteration algorithms for solving generic non-symmetric linear systems.
* We introduce a novel strategy that predicts the invariant subspace of the linear system to accelerate Krylov iteration. To facilitate the subspace prediction, we design a projection loss for efficient training, in conjunction with QR decomposition for stable outputs.
* Extensive experiments and theoretical analysis demonstrate that NeurKItt reduces the computational cost and the number of iterations for solving linear systems involving non-symmetric matrices.

## 2 Related Work

### Traditional Numerical Algorithms

In the field of computational mathematics, various algorithms have been devised to address the challenge of solving systems of linear equations. Among these, methods based on the Krylov subspace have garnered attention for their efficacy in handling large matrices[33; 43]. These methods notably mitigate computational complexity by seeking approximate solutions within a constrained, smaller subspace. Within this context, the Generalized Minimal Residual (GMRES) method [45; 14] plays a pivotal role, particularly in addressing non-symmetric matrices.

To enhance the efficiency of these Krylov subspace methods, a range of preconditioning methods are employed[9]. These methods aim to improve matrix conditioning, reduce iteration counts, and boost stability. Preconditioning varieties include Jacobi[46], Additive Schwarz Method (ASM)[64], and

Figure 1: The variation in tolerance for NeurKItt compared to GMRES, where each line represents an experiment under a solving method and a specific preconditioning. Notably, the NeurKItt substantially enhances the efficiency of solving the linear systems, with a reduction in the number of iterations by up to a factor of 16 and achieving a speed-up of up to 5.5 times.

Successive Over-Relaxation (SOR)[42]. They simplify the matrix structure for faster resolution, a significant advantage in large-scale problems.

Our methodology, NeurKltt, utilizes invariant subspaces to hasten convergence in Krylov subspaces, thereby expediting linear system resolutions. It is also engineered to integrate with existing preconditioning techniques, boosting its efficiency and applicability in complex computational scenarios.

### Learning-based Acceleration Algorithms

Although learning-based linear system solvers require training, the training time is negligible compared to the time saved over millions of calls. Thus, there has been a surge in learning-based acceleration efforts in recent years. Research in using neural networks for accelerating linear system solving [30; 23; 60] use neural networks to optimize the Conjugate Gradient algorithm, thereby accelerating the solution of symmetric positive definite linear systems. [37] accelerated the solution of the Poisson equation. [20] focused on accelerating algorithm iterations by learning better algorithm parameters. Recent studies have utilized neural networks for matrix preconditioning. [17; 38; 53] have focused on improving algebraic multigrid preconditioning algorithms. [15; 50] have applied CNNs and machine learning, respectively, to optimize block Jacobi and ILU precondition. Furthermore, specialized preconditioning research is being conducted in areas like physics[2; 7], engineering[47], and climate science[1], demonstrating the breadth of these applications.

However, these studies often face limitations due to specific matrix properties or mainly focus on reducing low-precision solving costs, with fewer advancements in accelerating high-precision solving. In contrast, our approach, NeurKltt, addresses all these limitations. It employs subspace prediction to refine iterative methods universally across Krylov subspace algorithms. This approach not only reduces the initial iterations but also significantly improves the speed and stability of subsequent iterations.

### Neural Operators

Numerical methods for solving PDEs commonly involves solving systems of linear equations, which require extensive computational resources. Recently, Neural operators (NOs), such as the Fourier Neural Operator (FNO)[31; 55; 12] and Deep Operator Network (DeepONet)[35; 36], have shown effectiveness in solving PDEs. Despite their effectiveness, NOs often grapple with challenges like diminished accuracy and stringent boundary condition prerequisites, which limit their applicability as standalone replacements for conventional algorithms in various scientific computing contexts[63; 19]. Given that mapping a matrix to its subspace is fundamentally an operator mapping task, NOs hold significant potential as tools for predicting matrix subspaces.

## 3 Preliminaries

### Krylov Subspace Iteration

In the realm of large-scale sparse linear systems, Krylov subspace methods are frequently employed as a standard solution strategy [46; 16]. At the heart of this methodology is the principle of utilizing the matrix involved in the linear system to iteratively construct a subspace. This subspace is crafted to approximate the actual solution space, with the iterative process continuing until the generated Krylov subspace sufficiently encompasses the real solution. Such an approach enables the iterative solutions within this confined subspace to progressively approximate the accurate solution vector \(\bm{x}\), thereby efficiently converging towards it.

Suppose we now solve the system

\[\bm{Ax}=\bm{b},\] (1)

where \(\bm{A}\in\mathbb{C}^{n\times n}\). The \(m\)-th Krylov subspace associated with the matrix \(\bm{A}\) and the starting vector \(\bm{r}\in\mathbb{C}^{n}\) as follows:

\[\mathcal{K}_{m}(\bm{A},\bm{r})=\text{span}\{\bm{r},\bm{A}\bm{r},\bm{A}^{2}\bm {r},\ldots,\bm{A}^{m-1}\bm{r}\}.\] (2)

Generally, \(\bm{r}=\bm{A}\bm{x}_{0}-\bm{b}\) represents the initial residual, where \(\bm{x}_{0}\) is typically generated randomly or selected as an initial guess for the solution. The iterative process of the Krylov subspace leads to the Arnoldi relation:

\[\bm{A}\bm{V}_{m}=\bm{V}_{m+1}\underline{\bm{H}}_{m}=\bm{V}_{m}\bm{H}_{m}+\bm{ v}_{m+1}h_{m+1,m}\bm{e}_{m}^{T},\] (3)where \(\bm{V}_{m}=(\bm{v}_{1},\dots,\bm{v}_{m})\in\mathbb{C}^{n\times m}\) comprises unit vectors \(\bm{v}_{i}\in\mathbb{C}^{n}\) for \(i=1,\dots,m\), mutually orthogonal, and \(\bm{V}_{m+1}=(\bm{V}_{m},\bm{v}_{m+1})\) extends this set. \(\underline{\bm{H}}_{m}\in\mathbb{C}^{(m+1)\times m}\) is an upper Hessenberg matrix. \(\bm{H}_{m}\in\mathbb{C}^{m\times m}\) is the first \(m\) rows of \(\underline{\bm{H}}_{m}\), distinct due to an extra element \(h_{m+1,m}\) at the \(m+1\)-th row, \(m\)-th column. \(\bm{e}_{m}\) is the \(m\)-th unit column vector, with its \(m\)-th element as \(1\).

Krylov algorithms reduce the computational effort needed for large linear systems by creating a Krylov subspace. However, starting this process with a random vector can be time-consuming. It's suggested that understanding the matrix's invariant subspace before beginning the iterative process, a warm-starting approach, could notably decrease the required iterations and improve convergence speed and stability.

### Neural Operators

NeurKltt utilizes the framework of neural operator architectures, notably the FNO [26; 31; 27]. We begin by defining a spatial dimension \(d\in\mathbb{N}\) and a corresponding domain \(D\subset\mathbb{R}^{d}\). Our focus is on approximating operators \(\mathcal{G}:\mathcal{A}(D;\mathbb{R}^{d_{a}})\to\mathcal{U}(D;\mathbb{R}^{d_ {u}})\). Here, \(a\in\mathcal{A}(D;\mathbb{R}^{d_{a}})\) and \(u\in\mathcal{U}(D;\mathbb{R}^{d_{u}})\) denote functions that map the domain \(D\) to \(\mathbb{R}^{d_{a}}\) and \(\mathbb{R}^{d_{u}}\), respectively, where \(d_{a},d_{u}\in\mathbb{N}\). Both spaces \(\mathcal{A}(D;\mathbb{R}^{d_{a}})\) and \(\mathcal{U}(D;\mathbb{R}^{d_{u}})\) are identified as Banach spaces.

In line with the definition provided by [32], a neural operator \(\mathcal{N}:\mathcal{A}(D;\mathbb{R}^{d_{a}})\to\mathcal{U}(D;\mathbb{R}^{d_ {u}})\) is conceptualized as a mapping expressed by

\[\mathcal{N}(a)=\mathcal{Q}\circ\mathcal{L}_{L}\circ\dots\circ\mathcal{L}_{1} \circ\mathcal{R}(a),\] (4)

for a designated depth \(L\in\mathbb{N}\). In this formulation, \(\mathcal{R}:\mathcal{A}(D;\mathbb{R}^{d_{a}})\to\mathcal{U}(D;\mathbb{R}^{d_ {v}})\), with \(d_{v}\geq d_{u}\), functions as a lifting operator, while \(\mathcal{Q}:\mathcal{U}(D;\mathbb{R}^{d_{v}})\to\mathcal{U}(D;\mathbb{R}^{d_ {u}})\) serves as a local projection operator.

## 4 Method

NeurKltt++ utilizes a neural operator to predict the invariant subspace \(\hat{\mathcal{K}}\) of the linear system, and then uses it to accelerate the Krylov subspace iteration. Generally, NeurKltt is broadly divided into two components.

Footnote ‡: Our code is available at https://github.com/smart-JLuo/NeurKltt

**Subspace Prediction Module**: Inspired by operator learning, we employ the neural operator capable of predicting subspaces at a low cost, with the help of QR decomposition. The aim is to precisely predict the invariant subspace \(\hat{\mathcal{K}}\) of the linear system.

**Acceleration Module**: Originating from the mathematical theories of Krylov iteration, we have developed a Krylov algorithm tailored to our problem. Utilizing the invariant subspace, this method expedites Krylov Subspace convergence and enhances stability.

### Subspace Prediction Module

The mapping from the linear system to its corresponding invariant subspace can be considered an operator, i.e., a mapping between two Hilbert spaces. Solving such problems involves finding the corresponding solution operator. In our subspace prediction module, we choose the Fourier Neural Operator (FNO)[31] for subspace prediction. We give a brief introduction to FNO in Appendix C. Generally, for a linear system \(Ax=b\) derived from the parametric PDE problem, to predict its invariant subspace \(\hat{\mathcal{K}}\), the input to FNO is the input function \(a\in\mathbb{R}^{d_{a}}\) from the given PDE, where \(d_{a}\in\mathbb{N}\). We provide a detailed discussion in Appendix B about how to build a linear system problem from a PDE problem, and what is the input function.

Our task is to learn the mapping between two Hilbert spaces \(\mathcal{G}:\mathbb{R}^{d_{a}}\to\mathbb{C}^{d\times n}\) using FNO. For FNO, the lifting transformation \(\mathcal{R}\) first lifts the input \(a\) to a higher dimensional representation \(v_{0}\in\mathbb{C}^{d_{a}\times c}\), where \(c\) is the number of channels. Then we feed the \(v_{0}\) to Fourier layers. After \(T\) Fourier layers, we have \(v_{T}\in\mathbb{C}^{d_{a}\times c}\) from the last Fourier layer, which keeps the same shape as \(v_{0}\). The FNO's output \(X=Q(v_{T})\) is the projection of \(v_{T}\) by the transformation \(Q:\mathbb{C}^{d_{a}\times c}\to\mathbb{C}^{d_{A}\times n}\). NeurKltt then uses QR decomposition to orthogonalize the matrix \(X\), obtaining the predictedsubspace \(\hat{\mathcal{K}}\). We provide more details about how to predict the subspace given the 2D Darcy flow problem in Appendix C.

We define the \(one-sided\)\(distance\)[51] from the subspace \(\mathcal{Q}\) to the subspace \(\mathcal{C}\) as

\[\delta(\mathcal{Q},\mathcal{C})=\|(\bm{I}-\bm{\Pi}_{\mathcal{C}})\bm{\Pi}_{ \mathcal{Q}}\|_{2},\] (5)

where \(\bm{\Pi}\) represents the projection operator for the associated space. Its mathematical interpretation corresponds to the largest principal angle between the two subspaces \(\mathcal{Q}\) and \(\mathcal{C}\)[3], and defining \(\bm{P}_{\mathcal{Q}}\) as the spectral projector onto \(\mathcal{Q}\). The objective of the subspace prediction task is to approximate the invariant subspace of the matrix \(\bm{A}\) with our predicted subspace \(\hat{\mathcal{K}}:\)

\[\operatorname*{arg\,min}_{\theta}\delta(\hat{\mathcal{K}},\mathcal{S}),\] (6)

where \(\theta\) represents the parameters of NO, and \(\delta(\hat{\mathcal{K}},\mathcal{S})\) is the distance between the two subspaces. Based on subsequent Theoretical Analysis 5.2, we choose \(\mathcal{S}\) as the invariant subspace associated with the smallest \(n\) eigenvalues, assuming \(\mathcal{S}=\operatorname{span}\bm{S}=\operatorname{span}\{\bm{s}_{1},\bm{s}_ {2},\dots,\bm{s}_{n}\}\), where \(s_{i}\) is the eigenvector corresponding to the \(i\)-th smallest eigenvalue of the given matrix \(A,i=1,2,\dots,n\). To reduce computational complexity, considering the norm equivalence theorem in Banach spaces, we optimize using the following loss function:

\[\operatorname*{arg\,min}_{\theta}\sum_{i=1}^{n}d(\hat{\mathcal{K}},s_{i}).\] (7)

This norm transforms the computation of subspace distance into the computation of the distance from a vector to a subspace. According to the properties of the Hilbert space, there exists a unique orthogonal decomposition:

\[s_{i}=x+y\quad(x\in\hat{\mathcal{K}},y\in\hat{\mathcal{K}}^{\perp}).\] (8)

We represent the projection operator for \(\hat{\mathcal{K}}\) by \(\bm{Q}\), with \(\bm{P}_{\mathcal{Q}}=\bm{Q}\bm{Q}^{*}\), leading to the relationship:

\[d(\hat{\mathcal{K}},s_{i})=\|s_{i}-\bm{Q}\bm{Q}^{*}s_{i}\|.\] (9)

Consequently, our projection loss function \(l(\hat{\mathcal{K}},\mathcal{S})\) is defined as follows:

\[l(\hat{\mathcal{K}},\mathcal{S})=\sum_{i=1}^{n}\|s_{i}-\bm{Q}\bm{Q}^{*}s_{i}\|.\] (10)

We train our subspace module by optimizing the above formula. Experiments have shown that as the projection loss decreases, the principal angle of our predicted subspace also decreases. The principal angle is a mathematical indicator derived in Theoretical Analysis 5.1 that influences the acceleration.

Figure 2: Algorithm Flow Diagram: **(a)** Finding solution \(\bm{x}\) for given matrices \(\bm{A}\) and \(\bm{b}\). **(b)** Traditional Algorithm: Krylov iterations from a random initial vector. **(c1)****NeurKItt** Subspace Prediction Module: Utilizing a neural operator for estimating the invariant subspace of matrix \(\bm{A}\). **(c2)****NeurKItt** Acceleration Module: Using the predicted invariant subspace of the matrix to guide the iteration, thereby accelerating the Krylov iteration process.

### Acceleration Module

For Krylov algorithms, the iteration count is a critical factor influencing computational load. Drawing inspiration from the Krylov recycling algorithms[40; 13; 6; 58; 41; 24], our approach NeurKItt utilizes the predicted invariant subspace \(\hat{\mathcal{K}}\) as the deflation space within the Krylov iteration framework. We provide the pseudocode of the acceleration module in Appendix A.2. The key procedure of our implementation is outlined as follows:

Considering the linear system (1), we obtain a \(k\)-dimensional invariant subspace \(\hat{\mathcal{K}}\) from the subspace prediction module. Then, NeurKItt computes matrices \(\bm{U}_{k},\bm{C}_{k}\in\mathds{C}^{n\times k}\) from \(\hat{\mathcal{K}}\) and \(\bm{A}\) such that \(\bm{A}\bm{U}_{k}=\bm{C}_{k}\) and \(\bm{C}_{k}^{H}\bm{C}_{k}=\bm{I}_{k}\), the specific algorithm can be found in Appendix A.1. We can get the Arnoldi relation of our NeurKItt:

\[(\bm{I}-\bm{C}_{k}\bm{C}_{k}^{H})\bm{A}\bm{V}_{m-k}=\bm{V}_{m-k+1}\underline{ \bm{H}}_{m-k}.\] (11)

This suggests that when the \(k\)-dimensional invariant subspace is given, there is no need to start from scratch for constructing a new Krylov subspace \(\mathcal{K}(\bm{A},\bm{r})\). Building upon this foundation, \(\mathcal{K}(\bm{A},\bm{r})\) can converge more rapidly to the subspace where the solution \(\bm{x}\) lies. This can significantly reduce the dimensionality of the final Krylov subspace, leading to a marked decrease in the number of iterations and resulting in accelerated performance. Compared to NeurKItt, GMRES can be intuitively conceptualized as the special case of our NeurKItt, where \(k\) is initialized at zero[8; 39; 40].

Existing Krylov recycling algorithms share the same Arnoldi relation with NeurKItt. However, they can only accelerate the linear systems solving when there are strongly correlated linear systems provided, which is not common in scientific computing. For example, when solving PDEs with finite element software, matrices vary in size and element positions due to discretization methods and different grids, which makes the recycling invariant subspace from previous matrices impossible. Additionally, storing subspaces from previous solving requires significant storage when the previous linear system is large. Thus, these methods are not widely used in general scenarios. In contrast, NeurKItt, which leverages neural operators to predict subspaces, is not affected by the lack of strongly correlated linear systems.

## 5 Theoretical Analysis

### Convergence Analysis

Let \(\mathcal{Q}\) be an \(l\)-dimensional invariant subspace of matrix \(\bm{A}\), and let \(\mathcal{C}=\operatorname{range}(\bm{C}_{k})\) be a \(k\)-dimensional space (\(k\geq l\)) selected to approximate \(\mathcal{Q}\). We refer to Theorem 3.1 in [40] for an in-depth convergence analysis under NeurKItt.

**Theorem 5.1**.: _[_40_]_ _Considering a space \(\mathcal{C}\), define \(\mathcal{V}=\operatorname{range}(\bm{V}_{m-k+1}\underline{\bm{H}}_{m-k})\) be the \((m-k)\)-dimensional Krylov subspace generated by NeurKItt as in (11). Let \(\bm{r}_{0}\in\mathds{C}^{n}\), and \(\bm{r}_{1}=(\bm{I}-\bm{\Pi}_{\mathcal{C}})\bm{r}_{0}\). Then, for each \(\mathcal{Q}\) such that \(\delta(\mathcal{Q},\mathcal{C})<1\),_

\[\begin{split}\min_{\bm{d}_{1}\in\mathcal{V}\oplus\mathcal{C}}\| \bm{r}_{0}-\bm{d}_{1}\|_{2}\leq&\min_{\bm{d}_{2}\in(\bm{I}-\bm{P} _{\mathcal{Q}})\mathcal{V}}\|(\bm{I}-\bm{P}_{\mathcal{Q}})\bm{r}_{1}-\bm{d}_{2 }\|_{2}\\ +\frac{\gamma}{1-\delta}\|\bm{P}_{\mathcal{Q}}\|_{2}\cdot\|(\bm{ I}-\bm{\Pi}_{\mathcal{V}})\bm{r}_{1}\|_{2},\end{split}\] (12)

_where \(\gamma=\|(\bm{I}-\bm{\Pi}_{\mathcal{C}})\bm{P}_{\mathcal{Q}}\|_{2}\) and \(\delta=\delta(\mathcal{Q},\mathcal{C})\)._

The left-hand side is the residual norm subsequent to \(m-k\) iterations of NeurKItt, employing the predictive subspace \(\mathcal{C}\). In contrast, on the right-hand side, the initial term epitomizes the convergence of a deflated problem, given that all components within the subspace \(\mathcal{Q}\) have been eradicated[39; 49]. The subsequent term on the right embodies a constant multiplied by the residual following \(m-k\) iterations of NeurKItt, when solving for \(\bm{r}_{1}\). Supposed that the predictive space \(\mathcal{C}\) encompass an invariant subspace \(\mathcal{Q}\), then we have \(\delta=\gamma=0\) for the given \(\mathcal{Q}\), which ensures that the convergence rate of NeurKItt matches or surpasses that of the deflated problem. In most cases, \(\|\bm{P}_{\mathcal{Q}}\|_{2}\) is numerically stable and not large, therefore a reduced value of \(\delta\) directly correlates with faster convergence in NeurKItt.

Now we compare two approaches to accelerating the linear systems solving[33]:1. Providing an initial prediction for the solution, allows Krylov algorithms to start from this prediction. The solution involved might come from solutions to similar systems or predictions such as via neural networks[62].
2. Predicting the matrix invariant subspace, using this approximate subspace to speed up Krylov iterations.

For the first approach, particularly with large matrices derived from PDEs, the norm \(\|\bm{A}\|_{2}\) tends to be large. Our theoretical analysis in Appendix D suggests that FNO's direct solution prediction will not significantly accelerate the linear system solving, possibly only reducing a few Krylov subspace iterations. NeurKItt belongs to the second approach. This crucially speeds up the convergence of Krylov subspace iterations, markedly reducing total iterations and improving stability.

### Subspace Property Analysis

The crucial questions about predicted subspace arise when accelerating Krylov subspace iterations: (1) What kind of subspace should be selected for acceleration? (2) Given the computational cost associated with employing neural networks, is it necessary to expend substantial computational resources to predict a highly accurate subspace?

**Regarding the first question**: As indicated by our convergence analysis 5.1, effectively accelerating the solution of linear systems only requires a predicted invariant subspace of matrix \(\bm{A}\). A matrix has many invariant subspaces, but which of these are easier to learn by the neural network[40]?

To investigate this problem, we simplify the specific scenario of the linear systems problem. The following definitions and assumptions for Theorem 5.2 are from the reference [24]. Specifically, We deal with a Hermitian positive definite matrix \(\bm{A}\) and a corresponding Hermitian perturbation \(\bm{E}\), allowing \(\bm{A}\) to have the eigendecomposition[24]:

\[\bm{A}=[\bm{Q}_{1}\bm{Q}_{2}\bm{Q}_{3}]\text{diag}(\bm{\Lambda}_{1},\bm{ \Lambda}_{2},\bm{\Lambda}_{3})[\bm{Q}_{1}\bm{Q}_{2}\bm{Q}_{3}]^{H},\] (13)

where \(\bm{Q}=[\bm{Q}_{1}\bm{Q}_{2}\bm{Q}_{3}]\) is an orthogonal matrix, \(\bm{\Lambda}_{1}=\text{diag}(\lambda_{1}^{(1)},\ldots,\lambda_{j_{1}}^{(1)})\), \(\bm{\Lambda}_{2}\) and \(\bm{\Lambda}_{3}\) are defined in a similar fashion. Let the eigenvalues satisfy the following ordering*:

Footnote *: For non-Hermitian matrices, the eigenvalues will be sorted by comparing their modulus, such that \(|\lambda_{1}|\leq|\lambda_{2}|\leq|\lambda_{3}|\leq|\lambda_{4}|\leq\cdots\leq |\lambda_{n}|\), where \(\lambda_{i}\in\mathbb{C}\), \(i=1,2,\ldots,n\), is the eigenvalue of a given non-Hermitian matrix \(A\).

\[\lambda_{1}^{(1)}\leq\ldots\leq\lambda_{j_{1}}^{(1)}<\lambda_{1}^{(2)}\leq \ldots\leq\lambda_{j_{2}}^{(2)}<\lambda_{1}^{(3)}\leq\ldots\leq\lambda_{j_{3}} ^{(3)}.\]

We consider the change in the invariant subspace range(\(\bm{Q}_{1}\)) under a symmetric perturbation \(\bm{E}\) of \(\bm{A}\). Let \(\theta_{1}(\cdot,\cdot)\) denote the largest canonical angle between two spaces. We do not require that \(||\bm{E}||_{F}\) be small, but we assume that the projection of \(\bm{E}\) onto the subspace range(\([\bm{Q}_{1}\bm{Q}_{2}]\)) is small. We assume that \(||[\bm{Q}_{1}\bm{Q}_{2}]^{H}\bm{E}||_{F}\leq\epsilon\) and that \(\epsilon\) is small relative to \(\lambda_{1}^{(2)}-\lambda_{j_{1}}^{(1)}\). We also assume that \(\eta=||\bm{Q}_{3}^{H}\bm{E}||_{F}\) is small relative to \(\lambda_{1}^{(3)}-\lambda_{j_{1}}^{(1)}\). Note that we do not need to assume that \(\lambda_{j_{2}}^{(2)}-\lambda_{j_{1}}^{(1)}\) is large. Also, let

\[\mu\equiv\min(\lambda_{1}^{(2)}-\epsilon,\lambda_{1}^{(3)}-\eta)-2\epsilon-( \lambda_{j1}^{(1)}+\epsilon)>2\epsilon,\]

\[\tilde{\mu}\equiv\mu\left(1-\frac{2\epsilon^{2}}{\mu^{2}}\right)+\lambda_{j_{ 1}}^{(1)}+\epsilon.\]

**Theorem 5.2**.: _[_24_]_ _Let \(\bm{A}\) be Hermitian positive definite and have the eigendecomposition given in (13), and let \(\bm{E}\), \(\epsilon\), \(\eta\), \(\mu\), and \(\tilde{\mu}\) be defined as above. Then there exists a matrix \(\tilde{\bm{Q}}_{1}\) conforming to \(\bm{Q}_{1}\) such that range(\(\tilde{\bm{Q}}_{1}\)) is a simple invariant subspace of \(\bm{A}+\bm{E}\), and_

\[\tan\theta_{1}(\text{range}(\bm{Q}_{1}),\text{range}(\tilde{\bm{Q}}_{1})) \leq\frac{\epsilon}{\tilde{\mu}}.\]

The specific proof can be found in [24]. A similar bound applies to the perturbation of the eigenvalues associated with \(\bm{Q}_{1}\). In the context of Theorem 5.1 and our NeurKItt, \(\bm{Q}_{1}\) corresponds to \(\bm{Q}\), whereas \(\bm{Q}_{2}\) and \(\bm{Q}_{3}\) can be chosen to fit the theorem.

This theorem shows that if changes in a matrix occur in the subspace corresponding to larger eigenvalues, the subspace associated with the smallest eigenvalues is minimally affected, as long as the changes are smaller than the gap between the smallest and larger eigenvalues. To facilitate learning of matrix invariant subspaces, we use the subspace composed of eigenvectors corresponding to the smallest eigenvalues as the prediction target for our neural operator.

**Regarding the second question**: The analysis of the first question, along with the error convergence rate proved in Theorem 5.1, suggests that investing in substantial computational resources for predicting a high-precision subspace is unnecessary. As long as the predicted subspace shows a distinct correlation with the matrix invariant subspace, particularly when \(\delta\) is small, significant acceleration can be achieved without resorting to high precision[13]. This principle underpins our development of a low-precision, cost-effective subspace prediction framework. Although the subspace prediction may not be precise, the final algorithm has achieved remarkable acceleration results.

## 6 Experiment

### Experiment Settings

To comprehensively evaluate the performance of NeurKltt, we conducted extensive experiments. Our analysis centers on two primary performance metrics viewed through three perspectives. These tests are conducted on 3 datasets. Specifically, the three Perspectives are: (i) Matrix preconditioning techniques, spanning 7 standard methods. (ii) Accuracy criteria for linear system solutions, emphasizing 8 distinct tolerances. (iii) Different matrix sizes, considering 3 variations. For more details, please refer to the Appendix F.1.

**Baselines.** NeurKltt focuses on solving linear systems that involve large sparse non-symmetric matrices. The GMRES algorithm is widely used for non-symmetric large sparse linear system solving, which serves as the predominant solution. And we set it as the benchmark for our study. We use GMRES from PETSc (version 3.19.4).

**Datasets.** To investigate the algorithm's adaptability to various types of matrices, we examined three different linear equation challenges, each rooted in a PDE: 1. Helmholtz Equation [62]. 2. Darcy Flow Problem [31; 44; 27; 36]; 3. Non-uniform Heat Conduction Equation [48; 25; 4; 18]. For an in-depth exposition of the dataset and its generation, kindly refer to the Appendix F.4. For the runtime environment, refer to Appendix F.2.

**Metrics.** We adopt time speedup for "GMRES solving time / NeurKltt solving time" and iteration speedup for "GMRES iteration count / NeurKltt iteration count". Speedup over 1 denotes better NeurKltt performance. We also provide the average time spent and average iteration count in Appendix H. We use principal angle in Equation 5 to show how close the predicted subspace and the target subspace are.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Dataset** & **Tol** & None & Jacobi & Blacobi & SOR & ASM & ICC & ILU \\ \hline \multirow{4}{*}{Darcy} & 1e-2 & 5.55 / 16.13 & 3.66 / 11.00 & 1.87 / 6.82 & 2.20 / 7.78 & 1.91 / 6.64 & 2.64 / 6.35 & 2.60 / 6.35 \\  & 1e-4 & 4.66 / 12.35 & 2.89 / 7.66 & 1.97 / 5.52 & 2.24 / 6.35 & 2.05 / 5.58 & 2.40 / 5.47 & 2.36 / 5.47 \\  & 1e-7 & 3.95 / 9.93 & 2.38 / 5.88 & 1.75 / 4.46 & 1.98 / 4.97 & 1.91 / 4.60 & 2.09 / 4.61 & 2.06 / 6.42 \\  & 1e-10 & 3.67 / 8.98 & 2.16 / 5.31 & 1.65 / 4.08 & 1.81 / 4.31 & 1.83 / 4.13 & 1.86 / 4.16 & 1.87 / 4.16 \\  & 1e-12 & 3.61 / 8.81 & 2.22 / 5.45 & 1.67 / 3.97 & 1.72 / 4.05 & 1.74 / 3.94 & 1.79 / 3.96 & 1.78 / 3.96 \\ \hline \multirow{4}{*}{Heat} & 1e-2 & 3.15 / 9.48 & 2.24 / 6.29 & 1.52 / 5.33 & 1.54 / 5.96 & 1.48 / 5.00 & 1.90 / 4.83 & 1.97 / 4.83 \\  & 1e-4 & 3.23 / 8.50 & 2.03 / 5.27 & 1.47 / 4.79 & 1.71 / 5.12 & 1.49 / 4.39 & 1.84 / 4.30 & 1.89 / 4.30 \\  & 1e-7 & 2.63 / 7.63 & 1.55 / 4.35 & 1.51 / 4.30 & 1.51 / 4.28 & 1.45 / 3.81 & 1.69 / 3.73 & 1.70 / 3.73 \\  & 1e-10 & 2.93 / 7.34 & 1.50 / 4.27 & 1.50 / 4.08 & 1.42 / 3.90 & 1.38 / 3.59 & 1.65 / 3.54 & 1.64 / 3.54 \\  & 1e-12 & 2.90 / 7.26 & 1.54 / 4.35 & 1.45 / 3.96 & 1.41 / 3.76 & 1.42 / 3.51 & 1.61 / 3.46 & 1.60 / 3.46 \\ \hline \multirow{4}{*}{Helmholtz} & 1e-2 & 2.39 / 7.68 & 2.37 / 7.68 & 1.33 / 5.49 & 1.58 / 5.66 & 1.32 / 4.83 & 1.54 / 4.31 & 1.51 / 4.31 \\  & 1e-4 & 2.69 / 7.99 & 2.72 / 7.99 & 1.36 / 4.29 & 1.45 / 4.36 & 1.40 / 4.00 & 1.60 / 3.72 & 1.59 / 3.72 \\ \cline{1-1}  & 1e-7 & 2.55 / 7.29 & 2.65 / 7.29 & 1.52 / 4.08 & 1.60 / 4.54 & 1.43 / 3.91 & 1.68 / 3.76 & 1.68 / 3.76 \\ \cline{1-1}  & 1e-10 & 2.52 / 6.74 & 2.52 / 6.74 & 1.56 / 4.01 & 1.77 / 4.59 & 1.56 / 3.95 & 1.70 / 3.75 & 1.69 / 3.75 \\ \cline{1-1}  & 1e-12 & 2.53 / 6.56 & 2.48 / 6.56 & 1.52 / 3.91 & 1.72 / 4.56 & 1.64 / 3.97 & 1.68 / 3.71 & 1.67 / 3.71 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our NeurKltt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as “time speedup / iteration speedup”.

### Main Experiment

We compare NeurKItt with GMRES under various preconditioners and tolerance, and Table 1 presents main experimental results. More detailed experimental results about time and number of iterations can be found in Appendix H.

The results show that across all tolerances and preconditioning techniques, NeurKIt has been effective in accelerating the linear system solving and reducing the number of iterations required by the Krylov subspace method, achieving up to a 5.5x speedup in computation time and a 16.1x speedup in the number of iterations. These results demonstrate that the invariant subspace predicted by NeurKIt greatly enhances the convergence speed of the Krylov algorithms, thereby reducing the iteration count and accelerating the solving.

With the solution accuracy increasing, the acceleration does not significantly decrease. Though the acceleration effect of NeurKIt varies under different preconditioning, the minimum time speedup is no less than 1.67 in the Darcy flow problem, which suggests NeurKIt can be effectively combined with various preconditioning methods.

### Generalization of Matrix Size

We conducted experiments on the Darcy Flow problem with varying matrix sizes under different preconditioning methods and tolerances. We provide the results in Figure 3. The experimental results show that our method consistently improves its acceleration effect as the matrix size increases, across different preconditioning methods and tolerances, which indicates that NeurKIt especially benefits solving large sparse linear systems.

### Ablation Study

We conducted ablation experiments to further validate the performance of FNO and demonstrate the effectiveness of the designed projection loss. The results of the ablation experiments are shown in Table 2. The results demonstrate that FNO significantly outperforms MLP when using the same projection loss. This indicates that FNO is indeed capable of learning a better mapping from the parameter matrix to the subspace, resulting in predicted subspaces that are closer to the target subspaces.

In addition, we also attempted to utilize the Mean Squared Error (MSE) Loss for subspace prediction. The experimental results show that applying the MSE Loss on subspace learning is merely ineffective. Since we report the principal angle in radians, it implies that the predicted subspace \(\hat{\mathcal{K}}\) is merely orthogonal to the target subspace \(\mathcal{S}\). Such subspace \(\hat{\mathcal{K}}\) cannot be effectively utilized to accelerate linear system solving. Besides, it can be inferred that QR decomposition we have

\begin{table}
\begin{tabular}{l c} \hline \hline
**Model** & **Principal Angle\(\downarrow\)** \\ \hline NeurKItt & 0.07 \\ w/o FNO & 0.54 \\ w/o QR decomposition & 1.57 \\ w/o Projection Loss & 1.57 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of different settings on principal Angle (rad) for the Darcy flow problem, with a matrix size of 32,400. We replace the FNO with MLP for “w/o FNO” and replace the projection loss with MSE loss for ”w/o Projection Loss”.

Figure 3: Experiments on the Darcy Flow problem with varying matrix sizes. The results indicate that as the matrix size increases, both time speedup and iteration speedup increase.

employed plays an important role in subspace prediction. These results verifies that the projection loss and QR decomposition enable the model to better predict subspace.

## 7 Limitation and Conclusions

**Limitation** Our method only considers solving non-symmetric matrices, but for matrices with specific structures, we need to tail our method to achieve faster solving. Furthermore, we have not considered the potential impact of different preconditioning techniques on the subspace and have not optimized for different preconditioning techniques, though the influence caused by preconditioning has little impact based on the observation of our experiments.

**Conclusions** We propose NeurKItt, a novel method for accelerating linear systems solving through subspace prediction. To the best of our knowledge, this is the first attempt to apply a data-driven approach to optimize the Krylov subspace algorithm for non-symmetric linear systems. Specifically, we employ FNO to predict the invariant subspace and design a projection loss function for this task. By utilizing the predicted invariant subspace, we achieve accelerated Krylov subspace iterations. Theoretical analysis and extensive experiments demonstrate that NeurKIt effectively reduces computational costs and iteration counts.

## Acknowledgements

We would like to thank all the anonymous reviewers for their insightful comments. This work was supported in part by National Key R&D Program of China under contract 2022ZD0119801, National Nature Science Foundations of China grants U23A20388 and 62021001.

## References

* [1] Jan Ackmann, Peter D Duben, Tim N Palmer, and Piotr K Smolarkiewicz. Machine-learned preconditioners for linear solvers in geophysical fluid flows. _arXiv preprint arXiv:2010.02866_, 2020.
* [2] Yael Azulay and Eran Treister. Multigrid-augmented deep learning preconditioners for the helmholtz equation. _SIAM Journal on Scientific Computing_, (0):S127-S151, 2022.
* [3] Christopher Beattie, Mark Embree, and John Rossi. Convergence of restarted krylov subspaces to invariant subspaces. _SIAM Journal on Matrix Analysis and Applications_, 25(4):1074-1109, 2004.
* [4] James V Beck, Ben Blackwell, and Charles R St Clair. _Inverse heat conduction: Ill-posed problems_. James Beck, 1985.
* [5] Michele Benzi, Carl D Meyer, and Miroslav Tma. A sparse approximate inverse preconditioner for the conjugate gradient method. _SIAM Journal on Scientific Computing_, 17(5):1135-1149, 1996.
* [6] Philipp Birken, Jurjen Duintjer Tebbens, Andreas Meister, and Miroslav Tma. Preconditioner updates applied to cfd model problems. _Applied Numerical Mathematics_, 58(11):1628-1641, 2008.
* [7] Salvatore Cali, Daniel C Hackett, Yin Lin, Phiala E Shanahan, and Brian Xiao. Neural-network preconditioners for solving the dirac equation in lattice gauge theory. _Physical Review D_, 107(3):034508, 2023.
* [8] Luiz Mariano Carvalho, Serge Gratton, Rafael Lago, and Xavier Vasseur. A flexible generalized conjugate residual method with inner orthogonalization and deflated restarting. _SIAM Journal on Matrix Analysis and Applications_, 32(4):1212-1235, 2011.
* [9] Ke Chen. _Matrix preconditioning techniques and applications_, volume 19. Cambridge University Press, 2005.
* [10] Lu Cheng and Kuan Xu. Solving time-dependent pdes with the ultraspherical spectral method, 2023.

* [11] Eric De Sturler. Truncation strategies for optimal krylov subspace methods. _SIAM Journal on Numerical Analysis_, 36(3):864-889, 1999.
* [12] Wenhan Gao, Ruichen Xu, Hong Wang, and Yi Liu. Coordinate transform fourier neural operators for symmetries in physical modelings. _Transactions on Machine Learning Research_, 2024.
* [13] Andre Gaul. Recycling krylov subspace methods for sequences of linear systems. 2014.
* [14] Gene H Golub and Charles F Van Loan. _Matrix computations_. JHU press, 2013.
* [15] Markus Gotz and Hartwig Anzt. Machine learning-aided numerical linear algebra: Convolutional neural networks for the efficient preconditioner generation. In _2018 IEEE/ACM 9th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (scalA)_, pages 49-56. IEEE, 2018.
* [16] Anne Greenbaum. _Iterative methods for solving linear systems_. SIAM, 1997.
* [17] Daniel Greenfeld, Meirav Galun, Ronen Basri, Irad Yavneh, and Ron Kimmel. Learning to optimize multigrid pde solvers. In _International Conference on Machine Learning_, pages 2415-2423. PMLR, 2019.
* [18] Stefan Guttel and Francoise Tisseur. The nonlinear eigenvalue problem. _Acta Numerica_, 26:1-94, 2017.
* [19] Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, and Jun Zhu. Physics-informed machine learning: A survey on problems, methods and applications. _arXiv preprint arXiv:2211.08064_, 2022.
* [20] Jun-Ting Hsieh, Shengjia Zhao, Stephan Eismann, Lucia Mirabella, and Stefano Ermon. Learning neural pde solvers with convergence guarantees. _arXiv preprint arXiv:1906.01200_, 2019.
* [21] Thomas JR Hughes. _The finite element method: linear static and dynamic finite element analysis_. Courier Corporation, 2012.
* [22] Claes Johnson. _Numerical solution of partial differential equations by the finite element method_. Courier Corporation, 2012.
* [23] Ayano Kaneda, Osman Akar, Jingyu Chen, Victoria Alicia Trevino Kala, David Hyde, and Joseph Teran. A deep conjugate direction method for iteratively solving linear systems. In _International Conference on Machine Learning_, pages 15720-15736. PMLR, 2023.
* [24] Misha E Kilmer and Eric De Sturler. Recycling subspace information for diffuse optical tomography. _SIAM Journal on Scientific Computing_, 27(6):2140-2166, 2006.
* [25] Seid Koric and Diab W Abueidda. Data-driven and physics-informed deep learning operators for solution of heat conduction equation with parametric heat source. _International Journal of Heat and Mass Transfer_, 203:123809, 2023.
* [26] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators. _The Journal of Machine Learning Research_, 22(1):13237-13312, 2021.
* [27] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021.
* [28] Steven J Leon, Lisette G De Pillis, and Lisette G De Pillis. _Linear algebra with applications_. Pearson Prentice Hall Upper Saddle River, NJ, 2006.
* [29] Randall J LeVeque. _Finite volume methods for hyperbolic problems_, volume 31. Cambridge university press, 2002.
* [30] Yichen Li, Peter Yichen Chen, Wojciech Matusik, et al. Learning preconditioner for conjugate gradient pde solvers. _arXiv preprint arXiv:2305.16432_, 2023.

* [31] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* [32] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020.
* [33] Jorg Liesen and Zdenek Strakos. _Krylov subspace methods: principles and analysis_. Numerical Mathematics and Scie, 2013.
* [34] Chih-Jen Lin and Jorge J More. Incomplete cholesky factorizations with limited memory. _SIAM Journal on Scientific Computing_, 21(1):24-45, 1999.
* [35] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* [36] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, 2022.
* [37] Kevin Luna, Katherine Klymko, and Johannes P Blaschke. Accelerating gmres with deep learning in real-time. _arXiv preprint arXiv:2103.10975_, 2021.
* [38] Ilay Luz, Meirav Galun, Haggai Maron, Ronen Basri, and Irad Yavneh. Learning algebraic multigrid using graph neural networks. In _International Conference on Machine Learning_, pages 6489-6499. PMLR, 2020.
* [39] Ronald B Morgan. Gmres with deflated restarting. _SIAM Journal on Scientific Computing_, 24(1):20-37, 2002.
* [40] Michael L Parks, Eric De Sturler, Greg Mackey, Duane D Johnson, and Spandan Maiti. Recycling krylov subspaces for sequences of linear systems. _SIAM Journal on Scientific Computing_, 28(5):1651-1674, 2006.
* [41] Michael Lawrence Parks. The iterative solution of a sequence of linear systems arising from nonlinear finite element analysis, 2005.
* [42] William H Press. _Numerical recipes 3rd edition: The art of scientific computing_. Cambridge university press, 2007.
* [43] Ouyuan Qin and Kuan Xu. Solving nonlinear odes with the ultraspherical spectral method. _arXiv preprint arXiv:2306.17688_, 2023.
* [44] Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators. _arXiv preprint arXiv:2204.11127_, 2022.
* [45] Youcef Saad and Martin H Schultz. Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear systems. _SIAM Journal on scientific and statistical computing_, 7(3):856-869, 1986.
* [46] Yousef Saad. _Iterative methods for sparse linear systems_. SIAM, 2003.
* [47] Johannes Sappl, Laurent Seiler, Matthias Harders, and Wolfgang Rauch. Deep learning of preconditioners for conjugate gradient solvers in urban water related problems. _arXiv preprint arXiv:1906.06925_, 2019.
* [48] Rishi Sharma, Amir Barati Farimani, Joe Gomes, Peter Eastman, and Vijay Pande. Weakly-supervised deep learning of heat transport via physics informed loss. _arXiv preprint arXiv:1807.11374_, 2018.
* [49] Valeria Simoncini and Daniel B Szyld. On the occurrence of superlinear convergence of exact and inexact krylov subspace methods. _SIAM review_, 47(2):247-272, 2005.

* [50] Rita Stanaityte. _ILU and Machine Learning Based Preconditioning for the Discretized Incompressible Navier-Stokes Equations_. PhD thesis, University of Houston, 2020.
* [51] G. W. Stewart and Ji guang Sun. _Matrix Perturbation Theory_. Academic Press, 1990.
* [52] John C Strikwerda. _Finite difference schemes and partial differential equations_. SIAM, 2004.
* [53] Ali Taghibakhshi, Scott MacLachlan, Luke Olson, and Matthew West. Optimization-based algebraic multigrid coarsening using reinforcement learning. _Advances in neural information processing systems_, 34:12129-12140, 2021.
* [54] Andrea Toselli and Olof Widlund. _Domain decomposition methods-algorithms and theory_, volume 34. Springer Science & Business Media, 2004.
* [55] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators. In _The Eleventh International Conference on Learning Representations_.
* [56] Lloyd N Trefethen. _Spectral methods in MATLAB_. SIAM, 2000.
* [57] Lloyd N Trefethen. Spectra and pseudospectra: the behavior of nonnormal matrices and operators. 2020.
* [58] Hong Wang, Zhongkai Hao, Jie Wang, Zijie Geng, Zhen Wang, Bin Li, and Feng Wu. Accelerating data generation for neural operators via krylov subspace recycling. _arXiv preprint arXiv:2401.09516_, 2024.
* [59] David S Watkins. _The matrix eigenvalue problem: GR and Krylov subspace methods_. SIAM, 2007.
* [60] Cheng Yang, Xubo Yang, and Xiangyun Xiao. Data-driven projection method in fluid simulation. _Computer Animation and Virtual Worlds_, 27(3-4):415-424, 2016.
* [61] David Young. Iterative methods for solving partial difference equations of elliptic type. _Transactions of the American Mathematical Society_, 76(1):92-111, 1954.
* [62] Enrui Zhang, Adar Kahana, Eli Turkel, Rishikesh Ranade, Jay Pathak, and George Em Karniadakis. A hybrid iterative numerical transferable solver (hints) for pdes based on deep operator network and relaxation methods. _arXiv preprint arXiv:2208.13273_, 2022.
* [63] Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stark, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tung Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alan Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Lio, Rose Yu, Stephan Gunnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. Artificial intelligence for science in quantum, atomistic, and continuum systems. _arXiv preprint arXiv:2307.08423_, 2023.
* [64] Xuejun Zhang. Multilevel schwarz methods. _Numerische Mathematik_, 63(1):521-539, 1992.

Algorithmic Details

### computes matrices \(\bm{U}_{k}\) and \(\bm{C}_{k}\)

NeurKItt can be modified to solve (1) by carrying over subspace \(\hat{\mathcal{K}}=\mathrm{span}\{\bm{y}_{1},\bm{y}_{2},\ldots,\bm{y}_{k}\}\) with respect to \(\bm{A}\) as follows [11, 40]:

\[[\bm{Q},\bm{R}]=\text{thin QR decomposition of }\bm{A}\bm{Y}_{k},\] \[\bm{C}_{k}=\bm{Q};\quad\bm{U}_{k}=\bm{Y}_{k}\bm{R}^{-1},\]

where the matrix \(\bm{Y}_{k}=[\bm{y}_{1},\bm{y}_{2},\ldots,\bm{y}_{k}]\). The matrices \(\bm{U}_{k}\) and \(\bm{C}_{k}\) to satisfy

\[\bm{A}\bm{U}_{k}=\bm{C}_{k};\quad\bm{C}_{k}^{H}\bm{C}_{k}=\bm{I}_{k}.\]

### NeurKItt Pseudocode

```
1 Select \(m\), the maximum size of the Krylov subspace. Define \(k\) as the dimension of the predicted invariant subspace \(\hat{\mathcal{K}}\), where \(\hat{\mathcal{K}}=\mathrm{span}\{\bm{y}_{1},\bm{y}_{2},\ldots,\bm{y}_{k}\}\).
2 Given matrix \(A\), use subspace prediction module to predict the matrix \(\bm{Y}_{k}=[\bm{y}_{1},\bm{y}_{2},\ldots,\bm{y}_{k}]\) and set \(tol\) as the convergence tolerance. Choose an initial guess \(\bm{x}_{0}\). Calculate \(\bm{r}_{0}=\bm{b}-\bm{A}\bm{x}_{0}\), and initialize \(i=1\).
3 Let \([\bm{Q},\bm{R}]\) be the reduced QR-factorization of \(\bm{A}\bm{Y}_{k}\).
4\(\bm{C}_{k}=\bm{Q}\)
5\(\bm{U}_{k}=\bm{Y}_{k}\bm{R}^{-1}\)
6\(\bm{x}_{1}=\bm{x}_{0}+\bm{U}_{k}\bm{C}_{k}{}^{H}\bm{r}_{0}\)
7\(\bm{r}_{1}=\bm{r}_{0}-\bm{C}_{k}\bm{C}_{k}{}^{H}\bm{r}_{0}\)
8while\(\|\bm{r}_{i}\|_{2}>tol\)do
9\(i=i+1\)
10 Perform \(m-k\) Arnoldi steps with the linear operator \((\bm{I}-\bm{C}_{k}\bm{C}_{k}^{H})\bm{A}\), letting \(\bm{v}_{1}=\bm{r}_{i-1}/\|\bm{r}_{i-1}\|_{2}\) and generating \(\bm{V}_{m-k+1}\), \(\underline{\bm{H}}_{m-k}\) and \(\bm{B}_{m-k}\).
11 Let \(\bm{D}_{k}\) be a diagonal scaling matrix such that \(\widehat{\bm{U}}_{k}=\bm{U}_{k}\bm{D}_{k}\), where the columns of \(\widetilde{\bm{U}}_{k}\) have unit norm.
12\(\widehat{\bm{V}}_{m}=[\widehat{\bm{U}}_{k}\;\;\bm{V}_{m-k}]\)
13\(\widehat{\bm{W}}_{m+1}=[\bm{C}_{k}\;\;\bm{V}_{m-k+1}]\)
14\(\underline{\bm{G}}_{m}=\left[\begin{array}{cc}\bm{D}_{k}&\bm{B}_{m-k}\\ 0&\widehat{\bm{H}}_{m-k}\end{array}\right]\)
15 Solve min \(\|\widehat{\bm{W}}_{m+1}^{H}\bm{r}_{i-1}-\underline{\bm{G}}_{m}\bm{y}\|_{2}\) for \(\bm{y}\).
16\(\bm{x}_{i}=\bm{x}_{i-1}+\widehat{\bm{V}}_{m}\bm{y}\)
17\(\bm{r}_{i}=\bm{r}_{i-1}-\widehat{\bm{W}}_{m+1}\underline{\bm{G}}_{m}\bm{y}\)
18 Compute the \(k\) eigenvectors \(\widetilde{\bm{z}}_{i}\) of \(\underline{\bm{G}}_{m}^{H}\underline{\bm{G}}_{m}\widetilde{\bm{z}}_{i}= \widetilde{\theta}_{i}\underline{\bm{G}}_{m}^{H}\widehat{\bm{W}}_{m+1}^{H} \widehat{\bm{V}}_{m}\widetilde{\bm{z}}_{i}\) associated with smallest magnitude eigenvalues \(\widetilde{\theta}_{i}\) and store in \(\bm{P}_{k}\).
19\(\widetilde{\bm{Y}}_{k}=\widehat{\bm{V}}_{m}\bm{P}_{k}\)
20 Let \([\bm{Q},\bm{R}]\) be the reduced QR-factorization of \(\underline{\bm{G}}_{m}\bm{P}_{k}\).
21\(\bm{C}_{k}=\widehat{\bm{W}}_{m+1}\bm{Q}\)
22\(\bm{U}_{k}=\widetilde{\bm{Y}}_{k}\bm{R}^{-1}\)
23\(\bm{x}_{i}\) as the numerical solution, \(\bm{r}_{i}\) as the residual, \(i\) as the iteration count. ```

**Algorithm 1**NeurKItt Krylov subspace iteration
From Partial Differential Equation to Linear System

We model a steady-state PDE problem as a corresponding system of linear equations using discrete numerical methods such as FDM, FEM, and FVM [52, 21, 22, 29, 10]. Numerical methods discretize a partial differential equation problem by mapping it from an infinite-dimensional function space to a finite-dimensional space, resulting in a system of linear equations.

We take the process of discretizing the two-dimensional inhomogeneous Helmholtz equation using the Finite Difference Method (FDM) as an example to illustrate how to transform a PDE into a system of linear equations:

\[\nabla^{2}u(x,y)+k^{2}(x,y)u(x,y)=f(x,y).\] (14)

This equation could be approximated as:

\[\frac{u(x+\Delta t,y)+u(x-\Delta t,y)+u(x,y+\Delta t)+u(x,y-\Delta t)-4u(x,y)}{ \Delta t^{2}}+k^{2}(x,y)u(x,y)=f(x,y),\] (15)

We use a \(2\times 2\) internal grid (i.e., \(N_{x}=N_{y}=2\) and \(\Delta x=\Delta y\)), the unknowns \(u_{i,j}\) can be arranged in row-major order as follows: \(u_{1,1},u_{1,2},u_{2,1},u_{2,2}\). For central differencing on a \(2\times 2\) grid, the vector \(\bm{b}\) will contain the values of \(f_{i,j}=f(x_{i},y_{j})\) and the linear equation system \(\bm{A}\bm{x}=\bm{b}\) can be expressed as:

\[\begin{bmatrix}-4+k_{1,1}^{2}&1&1&0\\ 1&-4+k_{1,2}^{2}&0&1\\ 1&0&-4+k_{2,1}^{2}&1\\ 0&1&1&-4+k_{2,2}^{2}\end{bmatrix}\begin{bmatrix}u_{1,1}\\ u_{1,2}\\ u_{2,1}\\ u_{2,2}\end{bmatrix}=\begin{bmatrix}f_{1,1}\\ f_{1,2}\\ f_{2,1}\\ f_{2,2}\end{bmatrix}.\]

In this example, we call the discretization of function \(k^{2}(x,y)\) the input function, corresponding to a \(2\times 2\) grid. The function inputs vary from PDE problems.

## Appendix C A Brief Introduction to Fourier Neural Operator

The Fourier Neural Operator (FNO) aims to map a continuous input function \(\mathcal{A}\) to its corresponding continuous output function \(\mathcal{U}\) within the Fourier domain. For end-to-end training on FNO, the function pairs \((\mathcal{A},\mathcal{U})\) are discretized into instance pairs \((a,u)\). The purpose is to learn a mapping \(\mathcal{N}\) between \((a,u)\), which can be expressed as:

\[u=\mathcal{N}(a)\] (16)

The mapping \(\mathcal{N}\) consists of several sequential steps: first, the input channel is lifted using \(\mathcal{R}\); next, the mapping is performed through \(L\) Fourier layers \(\{\mathcal{L}_{1},\mathcal{L}_{2},\dots,\mathcal{L}_{L}\}\); finally, the output is projected back to the original channel using \(\mathcal{Q}\).

\[\mathcal{N}(a)=\mathcal{Q}\circ\mathcal{L}_{L}\circ\dots\circ\mathcal{L}_{1} \circ\mathcal{R}(a),\] (17)

\(\mathcal{Q}\) and \(\mathcal{R}\) are pixel-wise transformations that can be implemented using models like MLP.

A typical Fourier layer consists of a pixel-wise linear transformation, characterized by weight \(W\) and bias \(b\), along with an integral kernel operator \(\mathcal{K}\):

\[\mathcal{L}(x)=\sigma(Wx+b+\mathcal{K}(x)),\] (18)

where \(\sigma\) is the nonlinear activation function. The integral kernel operator \(\mathcal{K}\) composes three steps: Fast Fourier Transformation (FFT), spectral linear transformation, and inverse FFT.

In the subspace prediction task, we use projection layer \(\mathcal{Q}\) mapping the last fno layer output to the target space. Each channel in the FNO output represents one of the basic vector of the predicted invariant subspace. We then apply QR decomposition to the output. Let the input function be \(a\in\mathbb{R}^{d_{a}\times d_{a}}\) from the Darcy flow problem, where \(d_{a}\in\mathbb{N}\) is the resolution of the input function, which yields a linear system \(Ax=b\) for numerical solving, where matrix \(A\in\mathbb{R}^{d_{A}\times d_{A}}\). We aim to predict a subspace \(\hat{\mathcal{K}}\), as a matrix \(X\in\mathbb{C}^{d_{A}\times n}\), for matrix \(A\) in the linear system. First, the FNO'slifting layer \(\mathcal{R}\) transforms \(a\) to a higher-dimensional representation \(v_{0}\in\mathbb{C}^{d_{a}\times d_{a}\times c}\), while the 3rd dimension is channel dimension. After \(T\) Fourier layers forward, we have the \(v_{T}\in\mathbb{C}^{d_{a}\times d_{a}\times c}\). Because Fourier layers keep the shape unchanged, we apply a transformation \(Q\) to map the \(v_{T}\) to the desired space, \(\hat{\mathcal{K}}=Q(v_{T})\), with \(Q:\mathbb{C}^{d_{a}\times d_{a}\times c}\rightarrow\mathbb{C}^{d_{A}\times n}\).

In practice, transformation \(Q\) is a stack of transformation layers. It first flattens the first and second dimension of \(v_{T}\), obtaining \(q_{0}\in\mathbb{C}^{d_{a}^{\ast}\times c}\). Then a fully-connected neural network (FNN) applies the mapping \(\mathbb{C}^{d_{a}^{2}\times c}\rightarrow\mathbb{C}^{d_{a}^{2}\times n}\) to \(q_{0}\), obtaining \(q_{1}\in\mathbb{C}^{d_{a}^{2}\times n}\). And another FNN applies the mapping \(\mathbb{C}^{d_{a}^{2}\times n}\rightarrow\mathbb{C}^{d_{A}\times n}\) to \(q_{1}\), obtaining the output \(X\in\mathbb{C}^{d_{A}\times n}\). Finally, we apply QR decomposition to \(X\) for orthogonalizing, obtaining \(\hat{\mathcal{K}}=\mathrm{span}\{X\}\).

Appendix D Analyzing the Efficiency of Using FNO Predictions as Initial Solutions in Linear System Solvers

### Theoretical Analysis

The central premise of this discussion hinges on the potential application of Neural Operators (NOs), such as the Fourier Neural Operator (FNO) [31], in addressing Partial Differential Equation (PDE) problems. Specifically, it explores the feasibility of utilizing these neural operators to predict solutions to PDE problems. Such predictions could serve as preliminary solution vectors for Krylov subspace methods in the resolution of corresponding linear systems. This approach holds promise for expediting the solution process.

However, the practicality of this concept, particularly in contexts where FNO is commonly discussed, warrants careful consideration. This is especially true in scenarios involving large matrix linear systems that emerge from Finite Difference Methods (FDM) [52], spectral methods [56], and similar approaches. A significant obstacle to the application of this method is the disparity between the type of relative error typically discussed in the context of neural operators and the relative error conventionally employed in computational mathematics. This divergence is further complicated by the impact of the norm of the PDE discretization matrix, which exhibits a tendency to increase with the refinement of the grid.

In the context of linear systems derived from PDEs, we consider the following standard formulation:

\[\bm{A}\bm{x}=\bm{b}.\] (19)

Within the framework of NOs, the prevalent metric for error assessment is the Mean Squared Error (MSE) of the solution function or solution vector \(\bm{x}\), designated as \(rtol_{NO}\):

\[rtol_{NO}=\frac{\|\hat{\bm{x}}_{NO}-\bm{x}\|}{\|\bm{x}\|},\] (20)

where \(\hat{\bm{x}}_{NO}\) represents the solution as predicted by the NO, while \(\bm{x}\) denotes the precise solution.

In the domain of computational mathematics, the relative residual \(rtol\) is commonly utilized as the error metric for resolving linear systems:

\[rtol=\frac{\|\bm{A}\hat{\bm{x}}-\bm{b}\|}{\|\bm{x}\|},\] (21)

where \(\hat{\bm{x}}\) represents the solution as predicted by the Krylov subspace method.

Utilizing orthogonal polynomial grid theory, specifically Theorem 30.1 in Trefethen's work [57], page 290, offers crucial insights for this analysis.

**Theorem D.1**.: _[_57_, P. 290]_ _For any \(N\), \(\|\bm{D}_{N}\|_{2}>N^{2}/3\)._

In this context, \(N\) denotes the count of discretization points in a one-dimensional Ordinary Differential Equation (ODE), and \(\bm{D}_{N}\) represents the discretization matrix constructed using Chebyshev orthogonal polynomial zeros as grid points, with the matrix dimension being \(N\). This assertion holds similarly for other orthogonal polynomial grids, and a uniformly divided grid can be analogized to a grid defined by the zeros of Fourier polynomials.

The theorem suggests that with an increase in the number of grid points, resulting in a larger matrix size, the norm of the matrix \(\|\bm{A}\|_{2}\) associated with the discretized PDE also escalates. Then We consider the following equation:

\[\frac{rtol_{NO}}{rtol}=\frac{\|\hat{\bm{x}}_{NO}-\bm{x}\|}{\|\bm{A}\hat{\bm{x}}- \bm{b}\|}=\frac{\|\hat{\bm{x}}_{NO}-\bm{x}\|}{\|\bm{A}(\hat{\bm{x}}-\bm{x})\|}.\] (22)

Consequently, when \(\|\bm{A}\|_{2}\) is substantially large, there emerges a notable discrepancy between \(rtol_{NO}\) and \(rtol\). Generally, the predictive accuracy \(rtol_{NO}\) of the FNO lies in the range of \(10^{-2}\) to \(10^{-4}\). However, if the norm \(\|\bm{A}\|_{2}\) approaches the magnitude of \(10^{5}\), the relative tolerance \(rtol\) as per computational mathematics algorithms may only range between \(10^{0}\) and \(10^{2}\). In such instances, leveraging the FNO's predicted solution as the initial vector for solving the linear system may not substantially expedite the solution process.

### Experimental Analysis

To empirically corroborate the theoretical framework presented, we delve into the Heat dataset F.4.3 utilized in this study, characterized by a matrix size of 52900. The experimental results underscore several noteworthy observations.

Employing the original FNO for training, we observed \(rtol_{NO}\sim 6*10^{-3}\), which is in line with the benchmarks reported in the foundational FNO paper. However, because \(\|\bm{A}\|_{2}\sim 1.2*10^{6}\), it results in \(rtol\sim 60\). This empirical finding is consistent with our theoretical predictions. In such a context, utilizing the FNO-derived solution as the initial vector for the linear system does not markedly expedite the resolution process.

This analysis implicitly suggests that utilizing predicted solutions as initial vectors in iterative algorithms mandates a high level of precision in the neural network model. Nonetheless, it is observed that in standard application scenarios, the impact on accelerating the process is marginal.

In contrast to this method, our algorithm NeurKItt, by predicting the invariant subspace, not only reduces the initial number of iterations but also, more importantly, substantially improves the convergence rate in subsequent iterations, As demonstrated in the convergence analysis 5.1. This marks a pivotal deviation from former techniques that focus on expediting the process by merely predicting initial solutions. Our theoretical analysis 5.2 shows that the predicted invariant subspace does not require high precision to accelerate the solution of linear systems and significantly reduce the number of iterations.

## Appendix E Thin QR Decomposition

In our research, we utilize the Thin QR decomposition algorithm, a prevalent technique in numerical linear algebra [14]. The essence of Thin QR decomposition lies in its capacity to factorize a matrix into two distinct components: an orthogonal matrix \(\bm{Q}\) and an upper triangular matrix \(\bm{R}\). This factorization is instrumental in diminishing both the computational complexity and the storage demands.

For a matrix \(\bm{A}\) belonging to \(\mathbb{C}^{n\times m}\) with the condition \(n\geq m\), the Thin QR decomposition is mathematically articulated as:

\[\bm{A}=\bm{Q}\bm{R},\] (23)

where \(\bm{Q}\) is an \(n\times m\) orthogonal matrix that complies with \(\bm{Q}^{*}\bm{Q}=\bm{I}\), and \(\bm{R}\) is an \(m\times m\) upper triangular matrix. In our study, the integration of QR decomposition within the neural network architecture is strategically employed to effectively resolve the orthogonal basis of a subspace.

## Appendix F Details of Experimental Data and Setup

### Specific parameters of the main experiment

**Model Training and Predicting**: We employ the 5 FNO layers with modes from {20, 32, 40} and the width from {32, 50, 64}. The learning rate is fix at \(1\times 10^{-3}\) while the batch size is selected from {16, 32}. The number of subspace dimensions is fixed at 10 in all experiments. For darcy flow and heat datasets, we generate 8000 samples for training and 1600 samples for testing. For Helmholtz, we generate 1000 samples for training and 200 samples for testing.

**Baseline**: To achieve maximum efficiency, we initiated our process by creating linear equation systems of PDEs using either Python. These were subsequently processed within the C programming environment. For the GMRES solver, we employed the most recent version of PETSc, specifically 3.19.4.

**Three Perspectives**:

1. **Precondition**: The act of preconditioning is vital when tackling large matrix equations, as it significantly speeds up the resolution process and augments algorithmic stability. Given the variety of scenarios that may arise, we explored over 10 prevalent preconditioning methods, adapting them to suit different contexts.

2. **Tolerance**: The precision threshold of a solution directly influences both the number of iterations required and the overall computational time. Different algorithms manifest varying rates of convergence, and specific NOs hold unique tolerance standards. Our comprehensive evaluation encompassed a range of tolerances, with a focus on pinpointing 5-8 levels of optimal error precision for our analyses.

3. **Matrix Dimensionality**: The efficiency of an algorithm can vary with changes in matrix size. Furthermore, different NOs often require matrices of various sizes. Therefore, our investigation included the study of 5 distinct matrix sizes.

**Two Performance Metrics**: 1. **Computational Time**: This metric provides a clear and direct measure of an algorithm's performance. 2. **Iteration Count**: The count of iterations required reflects the algorithm's numerical sensitivity and stability, offering valuable insights into its performance characteristics.

### Environment

To ensure consistency in our evaluations, all comparative experiments were conducted under uniform computing environments. Specifically, the environments used are detailed as follows:

CPU: Intel(r) Xeon(r) Gold 6246R CPU @ 3.40GHz

GPU: NVIDIA GeForce RTX 3090

### Precondition

In this study, we implemented a series of preconditioning methods for our experiments, which are outlined below.

1. None: In scenarios where 'none' is selected for preconditioning, the linear system is addressed directly in its original form. This means that no preliminary transformations or adjustments are applied to the system before the iterative solving process.

2. Diagonal Preconditioning (Jacobi) [46]: This technique focuses solely on the diagonal components of the coefficient matrix. The preconditioner in this approach is essentially the inverse of these diagonal elements, offering a straightforward preconditioning solution.

3. Block Jacobi (BJacobi) [5]: A more advanced form of Jacobi preconditioning, Block Jacobi divides the coefficient matrix into smaller, manageable blocks, each representing a specific subdomain or a separate problem. These blocks are then preconditioned individually using their diagonal elements.

4. Successive Over-relaxation (SOR) [61]: SOR, a modification of the Gauss-Seidel method, incorporates a relaxation factor to enhance the process of convergence. This method reformulates the original problem by applying a weight, often improving the convergence rate for certain iterative algorithms.

5. Additive Schwarz Method (ASM) [54]: ASM operates on the principle of domain decomposition, partitioning the main problem into several smaller subdomains. Each subdomain's problem is solved independently, and the local solutions are then aggregated to form the overall solution.

6. Incomplete Cholesky (ICC) [34]: ICC is a preconditioning method based on the Cholesky decomposition, but drops certain off-diagonal elements during the decomposition, making it "incomplete". It's utilized for symmetric positive definite problems.

7. Incomplete LU (ILU) [46]: ILU is based on LU decomposition, but like ICC, drops certain off-diagonal elements during the decomposition. ILU can be applied to nonsymmetric problems.

### Datasets

#### f.4.1 Helmholtz Equation

We consider a two-dimensional Helmholtz equation, which can be described by the following equation [62]:

\[\Delta u(x,y)+k^{2}(x,y)u(x,y)=f(x,y),\] (24)

where \(u(x,y)\) is the wave function, and \(k(x,y)\) denotes the spatially varying wavenumber, with \(f\) representing the source term, corresponding to origins of electromagnetic or acoustic waves. This equation is pivotal in several physical realms, notably in Acoustics, Electromagnetism, and Quantum Mechanics. Within these fields, the Helmholtz equation models phenomena such as wave propagation and vibration patterns.

In the context of the Helmholtz equation, the wavenumber \(k(x,y)\) is intimately connected to the frequency of the wave and the characteristics of the medium through which the wave propagates. In our experimental setup, the value of \(k(x,y)\) is determined using the GRF method [35] while the \(f(x,y)\) is the constant.

#### f.4.2 Darcy Flow problem

We consider two-dimensional Darcy flows, which can be described by the following equation [31, 44, 27, 36]:

\[-\nabla\cdot(K(x,y)\nabla h(x,y))=f(x,y),\] (25)

where \(K\) is the permeability field, \(h\) is the pressure, and \(f\) is a source term which can be either a constant or a space-dependent function.

In our experiment, \(K(x,y)\) is derived using the GRF method [35]. The term \(f(x,y)\) is consistently set as a constant \(f\).

#### f.4.3 Non-uniform Heat Conduction Equation

We consider a two-dimensional heat conduction equation in a non-uniform medium, which is formulated as follows [4, 18]:

\[k(\frac{\partial^{2}T}{\partial x^{2}}+\frac{\partial^{2}T}{\partial y^{2}}) +\frac{\partial k}{\partial x}\frac{\partial T}{\partial x}+\frac{\partial k }{\partial y}\frac{\partial T}{\partial y}+qT=f,\] (26)

where \(T(x,y)\) is the temperature, \(k(x,y)\) represents the spatially varying thermal conductivity, and \(q(x,y)T(x,y)\) models internal or external heat sources/sinks. The term \(f(x,y)\) acts as an external heat influence.

This equation is crucial for studying heat distribution in materials with varying properties, such as in the thermal management of electronic devices. Solving this equation helps predict temperature variations, essential for designing effective cooling or heating systems.

In our experiment, \(k(x,y)\) is derived using the GRF [35]. And \(q(x,y)\) is samples from \(U(0,1)\). The term \(f(x,y)\) is consistently set as a constant \(f\).

## Appendix G Analysis of Hyperparameters

### Hyperparameters of Fourier Neural Operator

Fourier Neural Operator has three key hyperparameters: model layers, mode and width for fourier layer. We conduct experiments to investigate the impacts of these hyperparameters. Results in Table 8 suggest that a proper combination of these hyper-parameters will improve the performance.

### Impact of Invariant Subspace Dimension

We conduct extensive experiments investigate the acceleration impact of subspace dimension. Results in Figure 4 shows that with the subspace dimension increasing, iteration speedup increase as well.

However, the time speedup does not consistently increase with the subspace dimension. As shown in the left figure of Figure 4, the time speedup reaches its maximum when the subspace dimension is 8. When the subspace dimension increase from 8, the time speedup decreases. This may be influenced by the subspace size since the subspaces are dense matrices. Recalling the Arnoldi relation in Section 4.2, larger subspaces mean larger dense matrix multiplications. When the subspace dimension is sufficiently large, the additional time cost of dense matrix operations will offset the time savings from reduced iterations, thereby reducing the overall time speedup. This indicates that a larger subspace dimension is not necessarily better.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Layer** & 2 & 5 & 8 & 10 & **Mode** & 8 & 16 & 32 & 64 \\ \hline Principal Angle & 0.18 & 0.12 & 0.11 & 0.13 & Principal Angle & 0.47 & 0.22 & 0.13 & 0.10 \\ Time Speedup & 1.47 & 1.68 & 1.69 & 1.54 & Time Speedup & 1.19 & 1.46 & 1.69 & 1.71 \\ Iteration Speedup & 8.15 & 9.86 & 9.45 & 8.04 & Iteration Speedup & 5.91 & 7.96 & 9.86 & 9.66 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c} \hline \hline
**Width** & 8 & 16 & 32 & 64 \\ \hline Principal Angle & 0.25 & 0.14 & 0.12 & 0.09 \\ Time Speedup & 0.98 & 1.64 & 1.69 & 1.83 \\ Iteration Speedup & 4.93 & 9.23 & 9.86 & 10.29 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance of NeurKItt with changes in layers, modes, and widths. We use Darcy Flow problem with matrix size of 32400 in these experiments, where the tolerance is fixed to 1e-5 and the preconditioning is None. Subspace dimension is fixed to 20.

Figure 4: Experiments on the Darcy Flow problem with varying subspace dimensions. The matrix size is fixed to 32400.

Detailed experimental results

### Time Efficiency Analysis for Subspace Prediction Module

Table 9 shows the time cost of the subspace prediction module for a single pass. The first row lists datasets with matrix side lengths. It's obvious that across various datasets, the computational cost of the subspace prediction module is extremely low, typically taking only a few milliseconds.

For comparison, in Darcy Flow problem, solving a linear system using GMRES without preconditioning at a tolerance of 1e-12 requires 34.8 seconds. While the neural network's inference cost is much more lower, which is negligible.

### Training Time Analysis

We train each dataset for 120 epochs, and list the training costs in Table 10. The total training time for the Helmholtz dataset is shorter compared to the other two datasets. This is because the problem is simple, allowing us to train on a dataset with fewer samples, and the matrix size involved in the problem is smaller than Darcy Flow and Heat problem.

As shown in the Figure 5, the training typically converges within 100 min. For the Helmholtz problem, it converges in 10 min; for the Darcy Flow dataset and Heat dataset, it converges less than 100 min. However, in scientific computing software, where solving linear systems is frequently required and often time-consuming, the time saved by our approach will add-up to pay off the training costs.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & \begin{tabular}{c} Helmholtz \\ 62500 \\ \end{tabular} & \begin{tabular}{c} Heat \\ 90000 \\ \end{tabular} & 
\begin{tabular}{c} Darcy \\ 160000 \\ \end{tabular} \\ \hline Training Time (h) & 0.51 & 6.48 & 10.47 \\ \hline \hline \end{tabular}
\end{table}
Table 10: The 120 epochs training cost for subspace prediction module.

Figure 5: Training time costs for main experiments.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

### Helmholtz Equation

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Preconditioning} & Metrics & Methods & 1-E-02 & 1-E-03 & 1-E-04 & 1-E-06 & 1-E-07 & 1-E-08 & 1-E-10 & 1-E-12 \\ \hline \multirow{4}{*}{None} & \multirow{4}{*}{time(s)} & GMRES & 0.68 & 1.01 & 1.32 & 1.94 & 2.23 & 2.55 & 3.18 & 3.79 \\  & & & \(\pm\)0.01 & \(\pm\)0.01 & \(\pm\)0.01 & \(\pm\)0.02 & \(\pm\)0.02 & 40.02 & 40.02 & 40.03 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 0.28 & 0.39 & 0.49 & 0.71 & 0.87 & 0.98 & 1.26 & 1.50 \\  & & & 0.01 & \(\pm\)0.02 & 40.02 & 40.03 & 40.04 & 40.04 & 40.04 & 40.04 \\ \cline{3-11}  & & \multirow{2}{*}{iter} & GMRES & 1378.28 & 2101.40 & 2825.34 & 4274.92 & 5000.12 & 5725.39 & 7176.56 & 8710.98 \\  & & & \(\pm\)0.53 & \(\pm\)16.11 & 421.76 & \(\pm\)32.99 & 438.60 & 444.20 & 455.42 & 468.68 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 179.49 & 262.28 & 353.46 & 565.67 & 686.25 & 811.86 & 1064.24 & 1327.98 \\  & & & \(\pm\)5.28 & \(\pm\)12.08 & 417.57 & \(\pm\)5.206 & 429.78 & 432.68 & 434.25 & 435.47 \\ \hline \multirow{4}{*}{Jacobi} & \multirow{4}{*}{time(s)} & \multirow{4}{*}{time(s)} & 0.70 & 1.01 & 1.32 & 1.96 & 2.26 & 2.56 & 3.20 & 3.86 \\  & & & \(\pm\)0.01 & \(\pm\)0.01 & \(\pm\)0.01 & 40.02 & 40.02 & 40.02 & 40.02 & 40.03 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 0.30 & 0.40 & 0.49 & 0.72 & 0.85 & 1.00 & 1.27 & 1.56 \\  & & & \(\pm\)0.01 & \(\pm\)0.02 & 40.02 & 40.03 & 40.04 & 40.04 & 40.04 & 40.04 \\ \cline{3-11}  & & \multirow{2}{*}{iter} & GMRES & 1378.28 & 2101.40 & 2825.34 & 4274.92 & 5000.12 & 5725.39 & 7176.56 & 8711.24 \\  & & & \(\pm\)0.53 & \(\pm\)16.11 & 421.76 & 432.93 & 438.60 & 444.20 & 455.41 & 468.41 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 179.49 & 262.28 & 353.45 & 565.66 & 686.26 & 811.87 & 1064.27 & 1327.96 \\  & & & \(\pm\)5.28 & \(\pm\)12.08 & 417.60 & \(\pm\)25.08 & \(\pm\)29.78 & 432.68 & 434.27 & 435.54 \\ \hline \multirow{4}{*}{BJacobi} & \multirow{4}{*}{time(s)} & GMRES & 0.21 & 0.27 & 0.34 & 0.46 & 0.50 & 0.56 & 0.67 & 0.76 \\  & & & \(\pm\)0.00 & 40.00 & 40.00 & 40.00 & 40.00 & 40.00 & 40.00 & 40.00 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 0.16 & 0.18 & 0.25 & 0.30 & 0.33 & 0.37 & 0.43 & 0.50 \\  & & & \(\pm\)0.00 & 40.00 & 40.01 & 40.01 & 40.01 & 40.01 & 40.01 & 40.01 \\ \cline{3-11}  & & \multirow{2}{*}{iter} & GMRES & 226.80 & 345.11 & 464.77 & 761.76 & 742.92 & 897.95 & 1109.02 & 1301.74 \\  & & & \(\pm\)1.10 & \(\pm\)2.12 & 41.64 & 44.50 & 43.04 & 42.73 & 43.12 & 46.50 \\ \cline{3-11}  & & \multirow{2}{*}{NewKlt} & 41.30 & 71.43 & 108.44 & 168.11 & 194.55 & 221.47 & 276.24 & 332.82 \\  & & & \(\pm\)0.54 & \(\pm\)1.18 & \(\pm\)44.10 & 45.96 & 46.64 & 46.80 & 48.79 & 49.57 \\ \hline \multirow{4}{*}{SOR} & \multirow{4}{*}{time(s)} & GMRES & 0.24 & 0.31 & 0.38 & 0.56 & 0.63 & 0.70 & 0.85 & 0.98 \\  & & & \(\pm\)0.00 & 40.00 & 40.00 & 40.00 & 40.01 & 40.01 & 40.01 \\ \cline{3-11}  & & & \(\pm\)0.15 & 0.22 & 0.27 & 0.34 & 0.39 & 0.42 & 0.48 & 0.57 \\  & & & \(\pm\)0.00 & 40.01 & 40.01 & 40.01 & 40.01 & 40.01 & 40.01 \\ \cline{3-11}  & & \multirow{2}{*}{iter} & GMRES & 278.40 & 426.14 & 574.14 & 873.73 & 1042.37 & 1173.98 & 1472.20 & 1776.41 \\  & & & \(\pm\)2.18 & \(\pm\)3.14 & \(\pm\)3.85 & \(\pm\)45.38 & \(\pm\)87.41 & \(\pm\)13.56 & 46.89 & 421.33 \\ \cline{3-11}  & & & \(\pm\)9.15 & \(\pm\)9.24 & \(\pm\Do we need to learn the neural network for matrices of a fixed size?

An important question is whether a model trained on a fixed-size dataset can be used to predict other matrices under the same PDE problem, which is common in scientific computing. In this section, we attempt to address this question using Darcy Flow dataset. First, we downsample the input function of the larger Darcy Flow dataset to generate new Darcy Flow datasets according to the discretization method mentioned in B. Specifically, we downsample the original dataset with a matrix size of 158,404 to obtain Darcy Flow datasets with matrix sizes of 39,204 and 9,604.

Next, we use the subspace prediction module trained on the Darcy Flow dataset with a matrix size of 9,604 to predict the subspaces of larger datasets. The input to the module is the input function from the larger matrix dataset, which is downsampled to the same size as the input function of the 9,604 matrix dataset, and the output is the predicted subspace of the original matrix dataset size, recovered using cubic spline interpolation.

In our experiments, the principal angles obtained through this method are generally small. For example, the model trained on the 9,604 matrix dataset predicted a principal angle of 0.07 for the test set of the 39,204 matrix dataset and 0.08 for the test set of the 158,404 matrix dataset. The experimental results in Table 14 are based on the acceleration results using the predicted invariant subspaces.

These results indicate that our method can scale across different matrix sizes for the given parametric PDE, which demonstrates that NeurKIt is practical. Additionally, using this approach for subspace prediction significantly reduces the required training time.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline Dataset & Tol & None & Jacobi & BJacobi & SOR & ASM & ICC & ILU \\ \hline \multirow{4}{*}{Darcy 39204} & 1e-2 & 3.05 / 11.00 & 2.15 / 8.75 & 1.27 / 4.85 & 1.41 / 5.20 & 1.37 / 4.08 & 2.15 / 3.44 & 2.14 / 3.44 \\  & 1e-4 & 2.79 / 1.99 & 1.99 / 6.16 & 1.24 / 4.47 & 1.43 / 4.45 & 1.19 / 5.51 & 1.29 / 3.09 & 1.28 / 3.09 \\  & 1e-7 & 2.65 / 7.21 & 1.72 / 4.91 & 1.45 / 3.92 & 1.33 / 3.86 & 2.17 / 3.51 & 1.27 / 2.87 & 2.16 / 2.87 \\  & 1e-10 & 2.39 / 6.78 & 1.60 / 4.27 & 1.27 / 3.65 & 1.37 / 3.62 & 1.25 / 3.12 & 1.26 / 2.80 & 1.24 / 2.80 \\  & 1e-12 & 2.36 / 6.60 & 1.54 / 4.10 & 1.31 / 3.57 & 1.36 / 3.52 & 1.27 / 3.06 & 1.27 / 2.80 & 1.26 / 2.80 \\ \hline \multirow{4}{*}{Darcy 158404} & 1e-2 & 4.83 / 13.80 & 2.81 / 8.21 & 1.92 / 6.59 & 2.25 / 7.02 & 1.82 / 6.16 & 2.24 / 5.95 & 2.43 / 5.95 \\  & 1e-4 & 4.35 / 11.25 & 2.42 / 6.49 & 1.80 / 5.06 & 2.09 / 5.66 & 1.80 / 4.96 & 2.14 / 5.01 & 2.09 / 5.01 \\ \cline{1-1}  & 3.50 / 8.92 & 2.06 / 5.35 & 1.62 / 4.18 & 1.87 / 4.67 & 1.66 / 4.10 & 1.80 / 4.17 & 1.78 / 4.17 \\ \cline{1-1}  & 1e-10 & 3.15 / 8.17 & 1.92 / 4.93 & 1.57 / 3.89 & 1.69 / 4.17 & 1.61 / 3.75 & 1.66 / 3.82 & 1.64 / 3.82 \\ \cline{1-1}  & 1e-12 & 3.07 / 7.99 & 2.01 / 5.13 & 1.55 / 3.81 & 1.64 / 3.97 & 1.56 / 3.63 & 1.62 / 3.69 & 1.60 / 3.69 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Experiments on using subspace prediction module trained on 9604 Darcy Flow dataset to accelerate the datasets with different matrix sizes. Results indicate that NeurKIt is able to accelerate linear systems with different sizes of the matrix when the parametric PDE is given.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims in the abstract and introduction accurately reflect the contribution and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We refer to Section 7 for the discussion of our limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We have cite the specific paper with the assumptions and proofs for the Theorems used in our paper. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the pseudocode for our algorithm in Appendix A, and provide the details about experimental settings and hyperparameter of our model in Appendix F. The dataset generation method is provided in Appendix F.4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We refer to Section 4 for code repo. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details about our experiments and settings in Appendix F Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the stand deviation of average time cost and average iteration count in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the details about experiments compute recources in Appendix F.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Contents in our paper focus on accelerating the linear system solving, which could have potential help to the industry or other field involving linear system solving. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our data and model focus on linear system solving, especially PDE problems, which have no high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have checked this and confirmed the paper poses no such risks. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the instruction in supplemental material about our code and data. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.