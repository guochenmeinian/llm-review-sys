# Non-Myopic Batch Bayesian Experimental Design

for Quantifying Statistical Expectation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

For almost a century, the fundamental method to estimate statistical expectation has been Monte Carlo with the core idea of learning a system by many random samples [1]. Although the convergence of Monte Carlo is guaranteed by the _law of large numbers_, its convergence rate--inversely proportional to the square root of the number of samples--is notoriously slow. That becomes a problem in scenarios where systems, such as global weather or autonomous cars, can only be evaluated by expensive numerical or physical experiments, requiring an efficient method with minimum evaluations.

To increase the convergence rate, a sequential Bayesian experimental design framework targeting statistical expectation was developed in [2; 3]. Specifically, they use the Gaussian process regression (GPR) as the surrogate and greedily select the next-best sample one by one which maximizes the information-theoretic acquisition, i.e., the information gain of adding the next sample. Although [2] shows that the proposed method works in several synthetic and practical cases, its sequential nature does bring two drawbacks. Firstly, the samples need to be evaluated one by one, making the duration of the whole process remarkably long. In contrast, the standard Monte Calor determines all samples in the beginning which can maximally utilize parallel computational resources in evaluating samples. Secondly, the determination of samples only focuses on the benefits of the immediate next sample without considering the long-term benefits, for example, the convergence after a certain number of samples.

In this work, we develop a non-myopic batch Bayesian experimental design for statistical expectation. The next batch of samples is selected, which maximizes the long-term information gain (as the acquisition) when they are added together. In addition, we formulate an analytic approximation of the acquisition to facilitate its optimization. The superior performance of the proposed algorithm, in terms of wall time saving and a faster or matched convergence rate than sequential sampling, is demonstrated in a case with arbitrary complex functions generated by RBF kernel and another case using a stochastic oscillator.

## 2 Method

### Problem setup

We consider an input-to-response (ItR) system described by a response function \(f(\mathbf{x}):\mathbb{R}^{d}\rightarrow\mathbb{R}\) with \(\mathbf{x}\) a \(d\)-dimensional random input. The input probability \(p_{\mathbf{x}}(\mathbf{x})\) is assumed to be known and our objective is the statistical expectation defined as:

\[q=\int f(\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}.\] (1)To compute \(q\), we take a Bayesian perspective by placing \(f\) a Gaussian process prior \(f\sim\mathcal{GP}(0,k(\mathbf{x},\mathbf{x}^{\prime}))\) where \(k\) is covariance function with hyperparameters \(\bm{\theta}\). Given a dataset \(\mathcal{D}_{n}=\{\mathbf{X}_{n},\mathbf{Y}_{n}\}\) consisting of \(n\) inputs \(\mathbf{X}_{n}=\{\mathbf{x}^{i}\in\mathbb{R}^{d}\}_{i=1}^{n}\) and the corresponding outputs \(\mathbf{Y}_{n}=\{f(\mathbf{x}^{i})\in\mathbb{R}\}_{i=1}^{n}\), the underling relation \(f\) is predicted as a posterior Gaussian process \(f(\mathbf{x})|\mathcal{D}_{n}\sim\mathcal{GP}(m_{n}(\mathbf{x}),k_{n}(\mathbf{ x},\mathbf{x}^{\prime}))\) with formulae of posterior mean \(m_{n}\) and covariance \(k_{n}\) detailed in Appendix A. The statistical expectation \(q|\mathcal{D}_{n}\) then becomes a random variable with randomness coming from the epistemic uncertainties of \(f(\mathbf{x})|\mathcal{D}_{n}\). Our goal is to choose the most informative batch of samples by optimizing the acquisition function that facilitates convergence of \(q\). In the following, we will discuss the form of the acquisition function.

### Acquisition function

For selecting the next samples, a popular way is to maximize the information gain (measured by K-L divergence) between the current estimation \(q|\mathcal{D}_{n}\) and hypothetical next estimation \(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}\) after adding \(s\) number of samples \(\tilde{\mathbf{X}}_{s}\) with responses \(\tilde{\mathbf{Y}}_{s}\) (see [2, 3] for a sequential version):

\[\mathbf{X}_{s}^{*} =\operatorname*{argmax}_{\tilde{\mathbf{X}}_{s}}\ \mathbb{E}\Big{[}\int\mathrm{KL}\big{(}p(q|\mathcal{D}_{n},\tilde{ \mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s})\,\|\,p(q|\mathcal{D}_{n})\big{)}\Big{]},\] \[\equiv\operatorname*{argmax}_{\tilde{\mathbf{X}}_{s}}\ \int\mathrm{KL}\big{(}p(q|\mathcal{D}_{n},\tilde{ \mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s})\,\|\,p(q|\mathcal{D}_{n})\big{)}\ p(\tilde{ \mathbf{Y}}_{s}|\tilde{\mathbf{X}}_{s},\mathcal{D}_{n})\mathrm{d}\tilde{ \mathbf{Y}}_{s},\] (2)

where \(\tilde{\mathbf{Y}}_{s}\) is chosen based on the current surrogate \(f(\mathbf{x})|\mathcal{D}_{n}\) following a distribution of \(\mathcal{N}(\tilde{\mathbf{Y}}_{s};m_{n}(\tilde{\mathbf{X}}_{s}),k_{n}(\tilde {\mathbf{X}}_{s},\tilde{\mathbf{X}}_{s}))\). Another more intuitive way is to minimize the variance, as the _predicted_ mean squared estimation error (MSE), of \(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}\):

\[\mathbf{X}_{s}^{*} =\operatorname*{argmin}_{\tilde{\mathbf{X}}_{s}}\ \mathbb{E}\Big{[}\int \mathrm{var}(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}) \Big{]}\] \[\equiv\operatorname*{argmin}_{\tilde{\mathbf{X}}_{s}}\ \int \mathrm{var}(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s})\ p(\tilde{ \mathbf{Y}}_{s}|\tilde{\mathbf{X}}_{s},\mathcal{D}_{n})\mathrm{d}\tilde{ \mathbf{Y}}_{s}\] \[\equiv\operatorname*{argmin}_{\tilde{\mathbf{X}}_{s}}\ \mathrm{var}(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{ \mathbf{Y}}_{s})\] (3)

where \(\mathrm{var}(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s})\) is a constant for \(\tilde{\mathbf{Y}}_{s}\).

Indeed, these two ways are equivalent for estimating the statistical expectation (see detailed derivations in Appendix B), and the optimization finally becomes:

\[\mathbf{X}_{s}^{*}=\operatorname*{argmax}_{\tilde{\mathbf{X}}_{s}}\ \int\mathbf{k}_{n}(\mathbf{x},\tilde{\mathbf{X}}_{s})p_{\mathbf{x}}( \mathbf{x})\mathrm{d}\mathbf{x}\ \mathbf{K}_{n}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{X}}_{s})^{-1}\int \mathbf{k}_{n}(\tilde{\mathbf{X}}_{s},\mathbf{x})p_{\mathbf{x}}(\mathbf{x}) \mathrm{d}\mathbf{x}.\] (4)

While (4) seems straightforward, a numerical integration for the right-hand side can become prohibitively expensive. To make the optimization (likely a high dimensional problem) feasible, we develop an analytical approximation for the acquisition in Appendix C. With the analytical solution, (4) is solved by a multi-start Quasi-Newton method with gradient computed through automatic differentiation in PyTorch1.

Footnote 1: https://github.com/pytorch/pytorch

### Overall algorithm

We finally show the overall algorithm in Algorithm 1. In each iteration, the number of samples to be selected is specified by \(s(t)\) with \(t\) the index of iterations. Setting \(s(t)=1\) reduces to the sequential algorithm in [2] and [3]. In this algorithm, one might wonder why we don't schedule all samples initially, considering that the sample responses do not directly appear in (4). Regarding this, we note that the sample responses do influence (4) implicitly via hyperparameters \(\bm{\theta}\) of the Gaussian process. (Should we know the hyperparameters in the beginning, we can determine all samples in one batch where the MSE in (3) is reduced much faster compared with sequential sampling, as shown in Appendix D.) In other words, a sequential algorithm can update the surrogate after each sample, making the selection of the next sample based on a more accurate model (although in a myopic way). The sampling efficiency of the batch algorithm needs to be evaluated in light of the benefits of long-term perspective and the disadvantages of less frequent model updates, which will be demonstrated in the next section.

## 3 Results

In this section, we test the performance of the proposed batch design algorithm in two cases: (1) a larger number of complex functions from realizations of Gaussian processes in SS3.1, and (2) a stochastic oscillator in SS3.2. In each case, we compare the results of batch design (_batch-design_) with sequential design (_seq-design_), direct random sampling (_random_), and random sampling with Gaussian process surrogate (_random-gpr_). For _random_, the expectation is directly computed as the mean of samples, while for _random-gpr_ the expectation is computed with a surrogate learned from random samples. The comparison between _random-gpr_ and _random_ highlights the impact of imposing a prior for \(f\), while the advantage of choosing optimal samples over random samples is evidenced in the contrast between _seq-design_ and _random-gpr_. Finally, the difference between _batch-design_ and _seq-design_ measures the effectiveness of picking a group of samples simultaneously instead of a single sample during each iteration.

### RBF functions

We firstly test the proposed algorithm in 100 two-dimensional functions constructed from RBF kernel. The hyperparameters for generating these functions are \(\boldsymbol{\theta}=\{4,\mathrm{I}_{2}\}\) (see Appendix A for format of \(\boldsymbol{\theta}\)) with an example shown in figure 1(a).

The results for a standard Gaussian input \(p_{\mathbf{x}}(\mathbf{x})\) with the assumption of known hyperparameters are demonstrated in figure 1(b). Considering there are 100 different functions, we average the error across all functions where, in each function, the error is computed in a root mean squared form of 50 runs considering the randomness in drawing samples. For _seq-design_ and _batch-design_, the sampling position is fixed, so we will directly take the fixed error. For _batch-design_, we sample only one batch in the beginning as we assume the hyperparameters are known. From figure 1(b), we can see that methods are ranked in an increasing performance from _random_ to _random-gpr_ to _seq-design_ and finally _batch-design_. That means the prior information is useful and a careful design would also improve the performance. Regarding the design method, the batch design is better than the sequential design as it optimizes all samples as a whole.

Figure 1: (a) an example of two-dimensional RBF functions. Results of RBF functions with (b) known hyperparameters and (c) learned hyperparameters: _random_ (), _random-gpr_ (), _seq-design_ (), and _batch-design_ () (\(s=4\)).

We further consider situations where the hyperparameters are unknown with results shown in figure 1(c). For both _batch-design_ and _seq-design_, we use 4 initial samples, and the error of each function is also computed in a root mean squared form across different initializations. The _batch-design_ with \(s(t)=4\) performs almost the same with _seq-design_, meaning the pro of a non-myopic design is actually offset by the con of fewer hyperparameters updates. But we note that the wall computational time of _batch-design_ is only a quarter of _seq-design_.

### Stochastic oscillator

We next consider a stochastic oscillator also used in [4; 5; 6]. In particular, the oscillator equation is formulated as

\[\ddot{u}(t)+\delta\dot{u}(t)+F(u)=\xi(t),\] (5)

where \(u(t)\) is the state variable, \(F\) a nonlinear restoring force. The stochastic process \(\xi(t)\), with a correlation function \(\sigma_{\xi}^{2}e^{-\tau^{2}/(2l_{\xi}^{2})}\), is approximated by a two-term Karhunen-Loeve expansion

\[\xi(t)=\sum_{i=1}^{2}x_{i}\lambda_{i}\phi_{i}(t),\] (6)

with \(\lambda_{i}\) and \(\phi_{i}(t)\) respectively the eigenvalue and eigenfunction of the correlation function, \(\mathbf{x}\equiv(x_{1},x_{2})\) is a standard normal variable as the input to the system, satisfying \(p_{\mathbf{x}}(\mathbf{x})=\mathcal{N}(\mathbf{0},\mathrm{I}_{2})\) with \(\mathrm{I}_{2}\) being a \(2\times 2\) identity matrix. The \(F\) term and values of the parameters are kept the same as those in the existing works. The response of the system is considered as the mean value of \(u(t;\mathbf{x})\) in the interval \([0,25]\):

\[f(\mathbf{x})=\frac{1}{25}\int_{0}^{25}u(t;\mathbf{x})\mathrm{d}t,\] (7)

with contour shown in figure 2(a).

We plot the results for different methods in figure 2(b). All results are root mean squared errors with randomness in _random_ and _random-gpr_ coming from random sampling and randomness in _seq-design_ and _batch-design_ coming from initializations. For _batch-design_, we test both \(s(t)=2\) and \(s(t)=4\). It demonstrates that _seq-design_ performs best among all while _batch-design_ with \(s(t)=2\) is almost on par with _seq-design_ albeit slightly less efficient.

## 4 Discussion

In this work, we develop a non-myopic batch Bayesian experimental design algorithm for statistical expectation, where the next batch of samples is selected to maximize the information gained (or equivalently to minimize the estimation uncertainty). We apply the results in two test cases, showing that if the hyperparameters (prior) are known, the batch design algorithm converges much faster than the sequential design. For more typical situations requiring learned hyperparameters, the batch design algorithm performs slightly worse, if not equally well, compared to the sequential design. However, it offers substantial savings in wall time. Further tests on additional cases with varying dimensions, complexities, and \(s(\cdot)\) are ongoing and will be presented in a full paper.

Figure 2: (a) response function of the stochastic oscillator. (b) results of _random_ (), _random-gpr_ (), _seq-design_ (), _batch-design_ with \(s(t)=4\) (), _batch-design_ with \(s(t)=2\) ().

## References

* [1] Art B. Owen. _Monte Carlo theory, methods and examples_. https://artowen.su.domains/mc/, 2013.
* [2] Piyush Pandita, Ilias Bilionis, and Jitesh Panchal. Bayesian optimal design of experiments for inferring the statistical expectation of expensive black-box functions. _Journal of Mechanical Design_, 141(10), 2019.
* [3] Xianliang Gong and Yulin Pan. Discussion:"bayesian optimal design of experiments for inferring the statistical expectation of expensive black-box functions"(pandita, p., bilionis, i., and panchal, j., 2019, asme j. mech. des., 141 (10), p. 101404). _Journal of Mechanical Design_, 144(5), 2022.
* [4] Mustafa A Mohamad and Themistoklis P Sapsis. Sequential sampling strategy for extreme event statistics in nonlinear dynamical systems. _Proceedings of the National Academy of Sciences_, 115(44):11138-11143, 2018.
* [5] Antoine Blanchard and Themistoklis Sapsis. Output-weighted optimal sampling for bayesian experimental design and uncertainty quantification. _SIAM/ASA Journal on Uncertainty Quantification_, 9(2):564-592, 2021.
* [6] Xianliang Gong and Yulin Pan. A generalized likelihood-weighted optimal sampling algorithm for rare-event probability quantification. _arXiv preprint arXiv:2310.14457_, 2023.
* [7] Carl Edward Rasmussen. Gaussian processes in machine learning. In _Summer School on Machine Learning_, pages 63-71. Springer, 2003.
* [8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.

## Appendix A Gaussian process regression

In this section, we briefly introduce the Gaussian process regression (GPR) [7], which is a probabilistic machine learning approach. Consider the task of inferring the underline relatioin \(f\) from dataset \(\mathcal{D}_{n}=\{\mathbf{X}_{n},\mathbf{Y}_{n}\}\) consisting of \(n\) inputs \(\mathbf{X}_{n}=\{\mathbf{x}^{i}\in\mathbb{R}^{d}\}_{i=1}^{n}\) and the corresponding outputs \(\mathbf{Y}_{n}=\{f(\mathbf{x}^{i})\in\mathbb{R}\}_{i=1}^{n}\). In GPR, a prior, representing our beliefs over all possible functions we expect to observe, is placed on \(f\) as a Gaussian process \(f(\mathbf{x})\sim\mathcal{GP}(0,k(\mathbf{x},\mathbf{x}^{\prime}))\) with zero mean and covariance function \(k\) (usually defined by a radial-basis-function (RBF) kernel):

\[k(\mathbf{x},\mathbf{x}^{\prime})=\tau^{2}{\rm exp}(-\frac{1}{2}((\mathbf{x}- \mathbf{x}^{\prime})^{T}\Lambda^{-1}(\mathbf{x}-\mathbf{x}^{\prime}))),\] (8)

where \(\tau\) (characteristic amplitude) and diagonal matrix \(\Lambda\) (characteristic length scales) are hyperparameters \(\bm{\theta}=\{\tau,\Lambda\}\) determined by maximizing the likelihood \(p(\mathbf{Y}_{n})\).

Following the Bayes' theorem, the posterior prediction for \(f\) given the dataset \(\mathcal{D}\) can be derived to be another Gaussian:

\[f(\mathbf{x})|\mathcal{D}\sim\mathcal{GP}(m_{n}(\mathbf{x}),k_{n}(\mathbf{x},\mathbf{x}^{\prime})),\] (9)

with mean and covariance respectively:

\[m_{n}(\mathbf{x}) =\mathbf{k}(\mathbf{x},\mathbf{X}_{n})\mathbf{K}(\mathbf{X}_{n},\mathbf{X}_{n})^{-1}\mathbf{Y}_{n},\] (10) \[k_{n}(\mathbf{x},\mathbf{x}^{\prime}) =k(\mathbf{x},\mathbf{x}^{\prime})-\mathbf{k}(\mathbf{x},\mathbf{ X}_{n})\mathbf{K}(\mathbf{X}_{n},\mathbf{X}_{n})^{-1}\mathbf{k}(\mathbf{X}_{n}, \mathbf{x}^{\prime}),\] (11)

where matrix element \(\mathbf{K}(\mathbf{X}_{n},\mathbf{X}_{n})_{ij}=k(\mathbf{x}^{i},\mathbf{x}^{j})\).

## Appendix B Equivalence of (2) and (3)

In this chapter, we will show formulae of (2) and (3) and their equivalence (see a similar conclusion for sequential design in [3]).

We first notice that \(q|\mathcal{D}_{n}\) follows a Gaussian distribution with mean \(\mu_{1}\) and variance \(\sigma_{1}^{2}\):

\[p(q|\mathcal{D}_{n})=\mathcal{N}(q;\mu_{1},\sigma_{1}^{2}),\] (12) \[\mu_{1} =\mathbb{E}\big{[}\int f(\mathbf{x})p_{\mathbf{x}}(\mathbf{x}) \mathrm{d}\mathbf{x}|\mathcal{D}_{n}\big{]}\] \[=\int m_{n}(\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d} \mathbf{x},\] (13) \[\sigma_{1}^{2} =\mathbb{E}\Big{[}\big{(}\int f(\mathbf{x})p_{\mathbf{x}}( \mathbf{x})\mathrm{d}\mathbf{x}\big{)}^{2}|\mathcal{D}_{n}\Big{]}-\mathbb{E} \Big{[}\big{(}\int f(\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x} \big{)}|\mathcal{D}_{n}\Big{]}^{2}\] \[=\iint k_{n}(\mathbf{x},\mathbf{x}^{\prime})p_{\mathbf{x}}( \mathbf{x})p_{\mathbf{x}}(\mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime} \mathrm{d}\mathbf{x}.\] (14)

After adding \(s\) hypothetical samples \(\{\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}\}\), \(f\) follows an updated distribution \(f(\mathbf{x})|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s} \sim\mathcal{GP}(m_{n+s}(\mathbf{x}),k_{n+s}(\mathbf{x},\mathbf{x}^{\prime}))\) with

\[m_{n+s}(\mathbf{x}) =m_{n}(\mathbf{x})+\mathbf{k}_{n}(\mathbf{x},\tilde{\mathbf{X}}_{ s})\mathbf{K}_{n}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{X}}_{s})^{-1}(\tilde{ \mathbf{Y}}_{s}-m_{n}(\tilde{\mathbf{X}}_{s})),\] (15) \[k_{n+s}(\mathbf{x},\mathbf{x}^{\prime}) =k_{n}(\mathbf{x},\mathbf{x}^{\prime})-\mathbf{k}_{n}(\mathbf{x}, \tilde{\mathbf{X}}_{s})\mathbf{K}_{n}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{X} }_{s})^{-1}\mathbf{k}_{n}(\tilde{\mathbf{X}}_{s},\mathbf{x}^{\prime}).\] (16)

The quantity \(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}\) can then be represented by another Gaussian with mean \(\mu_{2}\) and variance \(\sigma_{2}^{2}\):

\[p(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{ s})=\mathcal{N}(q;\mu_{2}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}), \sigma_{2}^{2}(\tilde{\mathbf{X}}_{s})),\] (17) \[\mu_{2}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s}) =\int m_{n+s}(\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d} \mathbf{x}\] \[=\mu_{1}+\int\mathbf{k}_{n}(\mathbf{x},\tilde{\mathbf{X}}_{s})p_ {\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}\;\mathbf{K}_{n}(\tilde{\mathbf{X} }_{s},\tilde{\mathbf{X}}_{s})^{-1}(\tilde{\mathbf{Y}}_{s}-m_{n}(\tilde{ \mathbf{X}}_{s})),\] (18) \[\sigma_{2}^{2}(\tilde{\mathbf{X}}_{s}) =\iint k_{n+s}(\mathbf{x},\mathbf{x}^{\prime})p_{\mathbf{x}}( \mathbf{x})p(\mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime}\mathrm{d} \mathbf{x}\] \[=\sigma_{1}^{2}-\int\mathbf{k}_{n}(\mathbf{x},\tilde{\mathbf{X}}_{ s})p_{\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}\;\mathbf{K}_{n}(\tilde{\mathbf{X} }_{s},\tilde{\mathbf{X}}_{s})^{-1}\int\mathbf{k}_{n}(\tilde{\mathbf{X}}_{s}, \mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}.\] (19)

With (12) and (17), one can simplify the objective function in (2):

\[\int\mathrm{KL}\big{(}p(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s}, \tilde{\mathbf{Y}}_{s})\|\,p(q|\mathcal{D}_{n})\big{)}\,p(\tilde{\mathbf{Y}}_{ s}|\tilde{\mathbf{X}}_{s},\mathcal{D}_{n})\mathrm{d}\tilde{\mathbf{Y}}_{s}\] \[= \iint p(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y} }_{s})\log\frac{p(q|\mathcal{D}_{n},\tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{ s})}{p(q|\mathcal{D}_{n})}\mathrm{d}q\,p(\tilde{\mathbf{Y}}_{s}|\tilde{ \mathbf{X}}_{s},\mathcal{D}_{n}))\mathrm{d}\tilde{\mathbf{Y}}_{s}\] \[= \int\big{(}\log(\frac{\sigma_{1}}{\sigma_{2}(\tilde{\mathbf{X}}_{ s})})+\frac{\sigma_{2}^{2}(\tilde{\mathbf{X}}_{s})}{2\sigma_{1}^{2}}+\frac{(\mu_{2}( \tilde{\mathbf{X}}_{s},\tilde{\mathbf{Y}}_{s})-\mu_{1})^{2}}{2\sigma_{1}^{2} }-\frac{1}{2}\big{)}p(\tilde{\mathbf{Y}}_{s}|\tilde{\mathbf{X}}_{s},\mathcal{ D}_{n})\mathrm{d}\tilde{\mathbf{Y}}_{s}\] \[= \log(\frac{\sigma_{1}}{\sigma_{2}(\tilde{\mathbf{X}}_{s})})+\frac{ 1}{2\sigma_{1}^{2}}\big{(}\sigma_{2}^{2}(\tilde{\mathbf{X}}_{s})-\sigma_{1}^{2}\] \[\qquad\qquad+\int\mathbf{k}_{n}(\mathbf{x},\tilde{\mathbf{X}}_{s})p_ {\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}\;\mathbf{K}_{n}(\tilde{\mathbf{X} }_{s},\tilde{\mathbf{X}}_{s})^{-1}\int\mathbf{k}_{n}(\tilde{\mathbf{X}}_{s}, \mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d}\mathbf{x}\big{)}\] \[= \log(\frac{\sigma_{1}}{\sigma_{2}(\tilde{\mathbf{X}}_{s})}).\] (20)

Since \(\sigma_{1}\) does not depend on \(\tilde{\mathbf{X}}_{s}\), (2) can be reformulated as

\[\mathbf{X}_{s}^{*} =\mathrm{argmin}_{\tilde{\mathbf{X}}_{s}}\;\sigma_{2}^{2}(\tilde{ \mathbf{X}}_{s})\] (21) \[=\mathrm{argmax}_{\tilde{\mathbf{X}}_{s}}\;\int\mathbf{k}_{n}( \mathbf{x},\tilde{\mathbf{X}}_{s})p_{\mathbf{x}}(\mathbf{x})\mathrm{d} \mathbf{x}\;\mathbf{K}_{n}(\tilde{\mathbf{X}}_{s},\tilde{\mathbf{X}}_{s})^{-1} \int\mathbf{k}_{n}(\tilde{\mathbf{X}}_{s},\mathbf{x})p_{\mathbf{x}}(\mathbf{x} )\mathrm{d}\mathbf{x},\] (22)

where (21) is exactly (3). The final optimization problem (22) ((4) in SS2.2) is obtained by substituting (19) into (21).

## Appendix C Analytical approximation of (4)

In computing the right-hand side of (4), the heaviest computation involved is the integral \(\int k_{n}(\tilde{\mathbf{x}},\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d} \mathbf{x}\). Expanding \(k_{n}\) with (11), we have:

\[\int k_{n}(\tilde{\mathbf{x}},\mathbf{x})p_{\mathbf{x}}(\mathbf{x})\mathrm{d} \mathbf{x}=\mathcal{K}(\tilde{\mathbf{x}})-\mathbf{k}(\tilde{\mathbf{x}}, \mathbf{X}_{n})\mathbf{K}(\mathbf{X}_{n},\mathbf{X}_{n})^{-1}\mathcal{K}( \mathbf{X}_{n}),\] (23)

with

\[\mathcal{K}(\mathbf{x})=\int k(\mathbf{x},\mathbf{x}^{\prime})p_{\mathbf{x}}( \mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime},\] (24)

If the input \(\mathbf{x}\) is Gaussian with mean \(\mathbf{w}\) and covariance \(\Sigma\), (24) has analytical expression for RBF kernel with characteristic length \(\Lambda\):

\[\int k(\mathbf{x},\mathbf{x}^{\prime})\mathcal{N}(\mathbf{x}; \mathbf{w},\Sigma)\mathrm{d}\mathbf{x}^{\prime}=|\Sigma\Lambda^{-1}+\mathrm{I }|^{-\frac{1}{2}}k(\mathbf{x},\mathbf{w};\Sigma+\Lambda).\] (25)

To make \(\mathcal{K}\) analytically tractable for arbitrary \(p_{\mathbf{x}}(\mathbf{x})\), we approximate \(p_{\mathbf{x}}(\mathbf{x})\) with the Gaussian mixture model [8] with \(n_{GMM}\) Gaussian functions:

\[p_{\mathbf{x}}(\mathbf{x})\approx\sum_{i=1}^{n_{GMM}}\alpha_{i} \mathcal{N}(\mathbf{x};\mathbf{w}_{i},\Sigma_{i}).\] (26)

(24) can then be formulated as:

\[\mathcal{K}(\mathbf{x}) \approx\sum_{i=1}^{n_{GMM}}\alpha_{i}\int k(\mathbf{x},\mathbf{x} ^{\prime})\mathcal{N}(\mathbf{x}^{\prime};\mathbf{w}_{i},\Sigma_{i})\mathrm{d }\mathbf{x}^{\prime}\] \[=\sum_{i=1}^{n_{GMM}}\alpha_{i}|\Sigma_{i}\Lambda^{-1}+\mathrm{I }|^{-\frac{1}{2}}k(\mathbf{x},\mathbf{w}_{i};\Sigma_{i}+\Lambda).\] (27)

## Appendix D Potential improvement on sampling efficiency

In this section, we show the accelerated reduction of MSE in (3) for batch sampling over sequential sampling. This serves as theoretical evidence of the potential for enhanced sampling efficiency.

In figure 3, we plot the standard deviation of \(q|\mathcal{D}_{n}\) (square root of MSE in (3)) with batch design (one batch) and sequential design for standard Gaussian input and known hyperparameters \(\boldsymbol{\theta}=\{4,\mathrm{I}_{2}\}\) with \(\mathrm{I}_{2}\) being a \(2\times 2\) identity matrix. It shows the batch design performs much better than the sequential design which is anticipated as we get the 'free lunch'--the benefits of a long-term perspective without any side effects from fewer model updates. The sampling positions of sequential design and batch design are plotted in figure 4. The batch samples show beautiful symmetric structures fitting the symmetric input and hyperparameters. In contrast, sequential samples show a strong greedy pattern. For example, when we have three samples, the sequential samples clearly favor one direction while batch samples form an equilateral triangle. (**see next page for figure 4**)Figure 4: Sampling position of sequential design (\(\bullet\)) and batch design (\(\bullet\)) for Gaussian input \(\mathbf{x}\sim\mathcal{N}(\mathbf{0},\mathrm{I}_{2})\) and known hyperparameters \(\boldsymbol{\theta}=\{4,\mathrm{I}_{2}\}\).