# Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity

 Kaiqu Liang  Zixu Zhang  Jaime Fernandez Fisac

Princeton University

{kl2471,zixuz,jfisac}@princeton.edu

###### Abstract

Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning processes. We propose introspective planning, a systematic approach that align LLM's uncertainty with the inherent ambiguity of the task. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a newly introduced safe mobile manipulation benchmark, demonstrate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Furthermore, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests. The webpage and code are accessible at https://introplan.github.io.

## 1 Introduction

Large Language Models (LLMs), when pre-trained on internet-scale text corpora, demonstrate emergent capabilities that extend far beyond mere text comprehension and generation as their scale increases [5, 40]. Through prompting [9] and in-context learning [26], these models have shown remarkable adaptability ranging from answering complex questions and solving mathematical problems to generating computer code and engaging in sophisticated reasoning processes during inference [36]. Robots interacting with humans can leverage the capabilities of LLMs to interpret task instructions in natural language, employ common sense reasoning to understand their environment, and devise high-level action plans grounded in the capabilities and affordances of the robot [1, 21].

The reliability of LLM outputs has direct implications downstream robotics tasks. Language models are prone to hallucinations [13], which cause models to generate plans that are at odds with common-sense knowledge, not executable by the robot, or incompatible with the environment constraints [48]. For example, if a human user asks a robot to bake some bread in a kitchen containing an oven and varied cookware, the robot's LLM may generate a decision to use a plastic ray without considering the risk of it melting. Furthermore, possible ambiguities in the user's request can also introduce uncertainty into the LLM's reasoning and planning [37]. In our example, while multiple containers are suitable for the stated task, biases inherited from training data may tilt action generation towards certain options. Therefore, the robot needs to calibrate its uncertainty quantification and seek further communication with users when ambiguities are identified.

Human beings assess their internal values and knowledge of their own abilities to guide domain-level reasoning processes: this is referred to as introspective reasoning [19]. In this paper, we observe that LLMs can leverage an analogous reasoning scheme to better assess underlying uncertainty when generating plans. We propose a novel method for constructing a knowledge base that utilizes LLMs to generate human-aligned introspective reasoning examples with minimal human input. During inference, this human-aligned knowledge guides LLMs to produce more reliable and interpretable plans. Unlike traditional Retrieval-Augmented Generation (RAG) approaches [10; 20; 29; 43; 51], which utilize open-source, off-the-shelf knowledge bases to enhance text generation, our approach retrieves few-shot introspective reasoning examples from the knowledge base. This enables LLMs to explicitly reason about uncertainties and formulate plans in a structured format. Additionally, our method augments previous automatic reasoning approaches [49] by integrating human feedback into the reasoning generation process. We have observed that introspective planning, when integrated with conformal prediction [2; 3], refines the LLM's uncertainty and achieves a tighter guarantee.

**Statement of contributions.** To the best of our knowledge, this is the first work to integrate retrieval-augmented planning with conformal prediction, refining language agents' uncertainty and reducing user queries while maintaining statistical guarantees. Key contributions are summarized as follows:

* We propose a novel _introspective planning_ scheme that prompts language-enabled agents to proactively assess their own confidence regarding task compliance and safety for multiple candidate plans, with a guaranteed probability that the agent will either execute the actions desired by the user or ask an appropriate follow-up question to disambiguate the user's intent.
* We introduce a new, weakly supervised offline knowledge base construction method that guides the LLM to generate human-aligned introspective reasoning examples as post-hoc rationalizations of human-selected safe-and-compliant plans.
* We create a new Safe Mobile Manipulation benchmark, which augments previous mobile manipulation datasets with safety-critical scenarios and introduces new metrics to evaluate a planner's specification compliance, safety, and degree of conservativeness.

## 2 Introspective Planning

The fundamental aim of introspective planning is to guide LLMs to reason about their own uncertainty regarding task compliance and safety for multiple candidate plans. While this guidance can take various forms, our implementation here is based on distilling example reasoning processes from a knowledge base to inform the LLM via in-context learning.

**Problem Formulation.** Similar to [31], we cast LLM-based planning as multiple-choice question answering (MCQA). Given a task statement \(d_{i}\) and the observation \(o_{i}\), the LLM planner first generates a set of candidate plans \(\mathcal{C}_{i}\), each assigned a unique letter label from \(\mathcal{Y}\coloneqq\{\cdot A^{\ast},\,\cdot B^{\ast},\,\cdot C^{\ast},\ldots\}\). The planner then predicts \(\hat{y}_{i}\in\mathcal{Y}\), aiming to match the unknown true user intent \(z_{i}\). For example, consider

Figure 1: Illustration of the introspective planning pipeline. **Knowledge base construction:** The LLM generates knowledge entries based on human-provided instructions and valid options. **Deployment:** Upon receiving an instruction, the LLM formulates possible next steps, consults the knowledge base to retrieve the most relevant examples, and uses them as prompts for prediction.

the stated task \(d_{i}\) "_Bring me that soda_" with the observation \(o_{i}\) that a banana, a pack of chips, and a can of Coke are placed on the counter. The LLM planner will first generate three options \(\mathcal{C}_{i}\) of bringing each item to the user, and predict the label \(\hat{y}_{i}\) corresponding to the Coke.

**Knowledge Base Construction.** Consider a training set \(\mathcal{U}=\{(x_{i},\mathcal{G}_{i})\}_{i=1}^{N}\) comprising \(N\) instances. For each instance, \(x_{i}:=(d_{i},o_{i})\) encompasses a pair of the task \(d_{i}\) and the observation \(o_{i}\), and a set of all valid options \(\mathcal{G}_{i}\) satisfying the task specification and the observation. To construct the knowledge base, we query LLM and generate a set of candidate plans \(\mathcal{C}_{i}\) with alphabetic label from \(\mathcal{Y}\), conditioned on the task \(d_{i}\), the observation \(o_{i}\), along with hand-crafted few-shot examples. This is followed by prompting the LLM to produce _rationale_\(k_{i}\) given the ground truth valid options \(\mathcal{G}_{i}\). Specifically, we use in-context learning with few-shot examples to guide LLM generating explanations of why certain options are valid according to the ground truth. Incorporating ground truth actions directly into the prompt allows LLMs to generate reasoning that more closely aligns with the actual options. To facilitate retrieval during the inference phase, we compute the textual embedding of each instruction \(d_{i}\) as the key to the knowledge and store them in the knowledge base dictionary \(\mathcal{K}\). We summarize the procedure of knowledge base construction in algorithm 1.

**Planning with Knowledge Retrieval.** At inference time, the planner selects the most pertinent reasoning examples from the knowledge base \(\mathcal{K}\) to aid the LLM's reasoning. Given a test instance \(x_{\text{test}}=(d_{\text{test}},o_{\text{test}})\), we compute the cosine similarity between the text embedding of \(d_{\text{test}}\) and all keys of \(\mathcal{K}\). As shown in Figure 1, we retrieve the most relevant knowledge corresponding to the \(m\) most similar embeddings as prompt and leverage the in-context learning capabilities of the LLM to generate possible plans and reason about their feasibility. To select the desired robot plan \(\hat{y}_{\text{test}}\) with generated reasoning, we can use two distinctive prediction methods: (1) **Direct Prediction**: We ask the LLM to output the best option \(\hat{y}_{\text{test}}\) along with all possible plans and explanations. (2) **Conformal Prediction**: Instead of directly predicting \(\hat{y}_{\text{test}}\), we construct a set of valid candidate plans \(\hat{\mathcal{G}}_{\text{test}}\subseteq\mathcal{C}_{\text{test}}\) by querying LLM's confidence \(\hat{f}(y_{i}|x_{\text{test}},\mathcal{C}_{\text{test}},k_{\text{test}})\) for each label \(y_{i}\in\mathcal{Y}\) given the prompt constructed by knowledge retrieval process and generated reasoning. The robot will request human for help if multiple valid options are included in the \(\hat{\mathcal{G}}_{\text{test}}\). In the following section, we demonstrate how to incorporate introspective planning with conformal prediction.

```
1:\(\mathcal{U}=\{(x_{1},\mathcal{G}_{1}),\dots,(x_{N},\mathcal{G}_{N})\}\)
2:\(\mathcal{K}\leftarrow\{\}\)\(\triangleright\) Knowledge Base Initialization
3:for each train example \(x_{i}=(d_{i},o_{i})\)do
4:\(\mathcal{C}_{i}\leftarrow\text{GenerateChoices}(x_{i})\)
5:\(k_{i}\leftarrow\text{GenerateKnowledge}(x_{i},\mathcal{C}_{i},\mathcal{G}_{i})\)
6:\(e_{i}\leftarrow\text{Embed}(d_{i})\)
7:\(\mathcal{K}[e_{i}]\leftarrow\{x_{i},\mathcal{C}_{i},k_{i},\mathcal{G}_{i}\}\)
8:endfor ```

**Algorithm 1** Knowledge Base Construction

```
1:\(x_{\text{test}}=(d_{\text{test}},o_{\text{test}})\), \(\mathcal{K}\), \(m\), \(\hat{q}\)
2:\(\mathcal{K}_{\text{similar}}\leftarrow\text{FindSimilar}(e_{\text{test}}, \mathcal{K}.\text{keys}(,m)\)
3:\(\mathcal{C}_{\text{test}},k_{\text{test}}\leftarrow\text{Plan}(x_{\text{test}}, \mathcal{K}_{\text{similar}})\)
4:\(\hat{\mathcal{G}}_{\text{test}}\leftarrow\text{ConformalPred}(x_{\text{test}}, \mathcal{C}_{\text{test}},k_{\text{test}},\hat{q})\)
5:if\(\hat{\mathcal{G}}_{\text{test}}|==1\)then\(y_{\text{test}}\leftarrow\hat{\mathcal{G}}_{\text{test}}\)
6:else Request further instructions
7:endif ```

**Algorithm 2** Introspective Conformal Prediction

## 3 Introspective Conformal Prediction

Successful human-centered robot autonomy hinges on accurate comprehension of users' goals--in situations where a user-specified task admits multiple valid interpretations, it is crucial for the robot to detect task ambiguity and solicit further instructions. Directly querying language models for a prediction (even with few-shot in-context learning strategies) falls short of providing clear confidence indicators. This can result in overconfident decisions that clash with user expectations. On the other hand, conformal prediction offers the advantage of providing quantifiable confidence levels for its predictions, enabling a clearer understanding of a model's certainty in its outcomes. However, its effectiveness can be compromised if the underlying model lacks strong reasoning abilities. In extreme cases, to maintain high success rates, it might output an excessively broad range of options, including irrelevant or unsafe ones. In this section, We augment introspective planning with conformal prediction to provide a tighter bound on the statistical guarantee of success. The synergy of these approaches is illustrated in Figure 2.

Conformal Calibration.Consider a calibration dataset \(\mathcal{Z}=\{(x_{i},\mathcal{C}_{i},k_{i},z_{i})\}_{i=1}^{N}\), comprising tuples that include tasks \(x_{i}\), plans \(\mathcal{C}_{i}\), rationale \(k_{i}\), and user intents \(z_{i}\). These tuples are drawn independently from an unknown distribution. The goal of conformal prediction is to generate a prediction set \(\hat{\mathcal{G}}_{\text{test}}\subseteq\mathcal{C}_{\text{test}}\) for new samples, ensuring that the actual user intent \(z_{\text{test}}\) is likely to be included. Specifically, conformal prediction aims to achieve:

\[\mathbb{P}(z_{\text{test}}\in\hat{\mathcal{G}}_{\text{test}})\geq 1-\epsilon,\] (1)

where \(1-\epsilon\) represents the desired level of confidence. During the calibration process, we compute nonconformity scores \(S=\{s_{i}:s_{i}=1-\hat{f}(z_{i}|x_{i},\mathcal{C}_{i},k_{i})\}_{i=1}^{N}\) using the confidence score \(\hat{f}\) from the LLM for all samples of \(\mathcal{Z}\). The critical threshold, \(\hat{q}\), represents the empirical quantile calculated at the \(\frac{\hat{\lceil}(N+1)(1-\epsilon)\rceil}{N}\) position within these scores, which follows:

\[\hat{q}=\text{Quantile}(s_{1},...,s_{N};\frac{\lceil(N+1)(1-\epsilon)\rceil} {N})\] (2)

Conformal Prediction.Utilizing the calibrated threshold \(\hat{q}\), we construct the prediction set for a test instance \(x_{\text{test}}\) by including all options \(y\) for which the confidence level meets or exceeds \(1-\hat{q}\) as:

\[\hat{\mathcal{G}}_{\text{test}}=\{y\in\mathcal{C}_{\text{test}}|\hat{f}(y|x_{ \text{test}},\mathcal{C}_{\text{test}},k_{\text{test}})\geq 1-\hat{q}\}.\] (3)

This approach ensures the coverage criterion specified in Equation (1), providing a statistically justified guarantee for the comprehensiveness of the prediction set. The proof is shown in Appendix E.

As the marginal guarantee in Equation (1) depends on both the calibration set and the test set, every time we have a new test instance \(z_{test}\), we would ideally sample a new calibration set to maintain the same level of statistical assurance, which could be too resource-intensive. However, we can choose \(N\) large enough to control the fluctuations in coverage by analyzing its distribution. The distribution of coverage has an analytic form as follows [38]:

\[\mathbb{P}(z_{\text{test}}\in\hat{\mathcal{G}}_{\text{test}}|\{z_{1},\dots,z_ {N}\})\geq\text{Beta}_{N+1-l,l}^{-1}(\delta),\] (4)

where \(l=\lfloor(N+1)\hat{\epsilon}\rfloor\), \(\text{Beta}_{N+1-l,l}^{-1}(\delta)\) denotes the inverse CDF (quantile) level of \(\delta\) in a Beta distribution with parameters \(N+1-l\) and \(l\), and \(\hat{\epsilon}\) is the threshold used for calibration. Additionally, prior research by Sadin (2019) [32] demonstrates that conformal prediction minimizes the prediction set size, suggesting that robots employing this method require the least human intervention while attaining desired success rates.

Prior work KnowNo [31] utilizes a similar conformal prediction approach for planning. In this work, we significantly enhance this framework by incorporating introspective planning rationale \(k_{i}\) to improve the likelihood function's effectiveness. This adjustment optimizes the distribution of nonconformity scores, leading to a tighter concentration around smaller values. Such refinement leads to tighter bounds, improving the framework's reliability and reducing its conservativeness, as will be demonstrated in subsequent sections. We summarize the procedure of introspective conformal prediction in Algorithm 2.

Figure 2: Demonstration of using conformal prediction with Introspective Planning. After generating multiple options, we query the LLM for the explanation by introspective planning and then ask the model to predict the most correct option. Based on the likelihood scores of true intents from a calibration dataset, conformal prediction finds the quantile value \(\hat{q}\) (0.85), and includes any options scoring above \(\geq 1-\hat{q}=0.15\) in the prediction set for each test scenario. This method guarantees the correct answer is included among the options, at a confidence level specified by the user.

## 4 Evaluation

### Evaluation Method

**Asking for help is not enough.** Previous work uses success rate and help rate as metrics, but these do not fully capture a planner's performance. For example, if the instruction is to bring the soda and the robot's prediction set includes Coke, Sprite, and apple, the robot will ask for help but ask the wrong question due to an irrelevant option. Neither the success rate nor the help rate captures this issue because'success' is only defined as the prediction set containing the user intent. Additionally, help rate can sometimes be misleading. A low help rate does not necessarily indicate good performance, as an effective predictor should ideally seek help whenever instructions are ambiguous.

To address these, we categorized errors into three types: **(1)** The robot is uncertain, but the task is unambiguous. **(2)** The robot is certain but wrong. **(3)** The robot is uncertain, and the task is ambiguous, but it asks the wrong question. Based on this analysis, we proposed new metrics to capture these errors. Exact set rate and non-compliant contamination rate effectively measure the error type (3). Overask rate captures the error type (1) while the overstep rate captures the error type (2). Additionally, we propose Unsafe Contamination Rate (UCR) and Unsafe Rate to measure the robot's performance in prioritizing safety, which previous metrics do not account for.

**Metrics.** Beyond the success rate and help rate, we introduce additional metrics to more comprehensively evaluate the performance of our planner.

* Success Rate (SR): How often the language model's predictions match the user's intent, calculated as the percentage of cases where the predicted actions include the correct intent.
* Help Rate (HR): Fraction of cases where the prediction set encompasses more than one option, such that robots will require further human instructions, \(\text{HR}=N_{\text{ask}}/N\).
* Exact Set Rate (ESR): Frequency of the LLM's predictions perfectly aligning with all valid actions inferred from instructions. It evaluates the model's ability to generate precise responses.
* Non-compliant Contamination Rate (NCR): Proportion of prediction sets containing options that deviate from the given instructions, measuring the LLM's ability to follow instructions accurately and ask the right questions to clarify uncertainty.
* Unsafe Contamination Rate (UCR): Frequency at which the prediction sets include potentially unsafe options, assessing the model's ability to prioritize safety in responses.
* Overask Rate: Fraction of instances when the planner is uncertain while the task is unambiguous. Count (robot is uncertain while the task is unambiguous)/Count (task is unambiguous)

Figure 3: **Qualitative results on Safe Mobile Manipulation. We compared our approach with KnowNo [31], both using conformal prediction with an 85% target success rate. Our method generates explanations via introspective planning before applying conformal prediction, whereas KnowNo directly predicts valid options using conformal prediction. We observed that KnowNo _over-step_ in the left case and _over-ask_ in the right case while IntroPlan generates more precise prediction sets.*** Overstep Rate: Fraction of the planner generating over-confident or incorrect options when the planner is certain. Count (robot is certain but wrong)/Count (robot is certain)
* Unsafe Rate: Frequency at which planner is certain to execute unsafe action.

**Baselines.** We benchmarked our proposed introspective planning against various prompt-based methods to gauge its effectiveness in LLM reasoning. The _Prompt Set_[31] instruct the LLM to directly output the prediction through few-shot in-context learning. _Prompt Set+CoT_[41] applies a Chain of Thought (CoT) process to simultaneously produce explanations and predictions. _Retrieval-Q-CoT_[49] utilizes CoT to generate reasoning in a training dataset and retrieves the most relevant prompt during inference. _Auto-CoT_[49] automates prompts selecting process by using clustering to ensure a broad representation of diverse scenarios.

To further illustrate the effectiveness of introspective planning in enhancing conformal prediction, we compared our method with _KnownNo_[31], which integrates conformation prediction with _Prompt Set_. Additionally, we employed conformal calibration for _Retrieval-Q-CoT_[49] serving as an extra baseline due to its use of retrieval augmentation for reasoning. All calibration processes used the same dataset with 400 instances. We set \(\delta=0.01\) to be consistent with KnowNo, ensuring that the empirical coverage exceeds the conditional coverage with probability \(1-\delta=0.99\).

### Datasets

**Mobile Manipulation:** The original calibration or training dataset comprises 400 examples, while the test set includes 200 examples. We also evaluated the robustness to two kinds of distribution shifts: covariate shift and concept shift, using three additional datasets: one with 200 unambiguous instructions, another with 200 ambiguous instructions, and a third with 100 novel scenes and instructions. The original dataset follows the same distribution of different types of examples as in KnowNo [31], encompassing a range of types such as single-label, multi-label, spatially-ambiguous, unsafe, and Winograd tasks. The experiment results on this dataset are in Appendix A.

**Safe Mobile Manipulation:** We augment the original mobile manipulation dataset to emphasize safety, with 400 examples for calibration and 200 for testing. Additionally, we assembled a dataset of 200 safety-critical scenarios, categorized into three types: (1) ambiguous instructions that become clear when safety is considered, e.g., choosing between a metal and plastic bowl for microwave use, (2) ambiguous instructions considering safety, such as selecting the correct bowl for microwave heating among stainless steel, plastic, or ceramic options, and (3) unambiguous but unsafe instructions, such as 'place a metal bowl in the microwave'.

**Tabletop Rearrangement:** The task involves moving colored blocks and bowls on a table according to specific instructions. These instructions are intentionally designed to include ambiguities in attributes (such as alternative names for objects and colors), numbers (using vague terms for quantities), and spatial relationships (using general terms for directions and orientations). For this dataset, 400 examples were used for calibration and an additional 200 examples for testing. The experiment results on this dataset are in Appendix A andeach dataset are detailed in Appendix B.

## 5 Results

**Implementation details.** We implemented all tasks using OpenAI's GPT-3.5 (text-davinci-003) and GPT-4 Turbo (gpt-4-1106-preview). We only presented GPT-4 Turbo results on Safe Mobile Manipulation in the main paper. All of the other results are in Appendix A. We employed Sentence-BERT [30] to encode instructions, retrieving the top \(m=3\) text embeddings based on cosine similarity. We used the default temperature of 0 to sample the LLM's response. The knowledge base and the calibration set contain 400 tasks each. Appendix C contains an in-depth experimental exploration of performance variation with knowledge base size, with sizes of 10, 50, 100, and 200, suggesting that modest knowledge bases with around 100 examples still enable satisfactory results.

**Trade-off between direct and conformal prediction:** Our experiments indicate that introspective planning with direct prediction significantly outperforms all other baselines in terms of performance metrics. However, this approach does not guarantee success. On the other hand, introspective planning with conformal prediction guarantees success and surpasses other methods employing conformal prediction. Nevertheless, a noticeable performance gap exists between direct prediction and conformal prediction. This highlights an intriguing trade-off: while conformal prediction provides success guarantees, it tends to be more conservative.

### Direct Prediction

In Tab. 1, our analysis highlights that introspective planning with direct prediction outperforms all baseline methods in both decision and prediction metrics. KnowNo is too conservative with low and Non-compliant contamination rate (NCR) and exact set rate (ESR). As a result, despite guaranteeing a high success rate, it suffers from a very high Over-Ask Rate (OAR), which is not

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{Prediction Metrics} & \multicolumn{4}{c}{Decision Metrics} \\ \cline{2-9} Method & ESR \(\uparrow\) & NCR \(\downarrow\) & UCR \(\downarrow\) & SR \(\uparrow\) & HR & OAR \(\downarrow\) & OSR \(\downarrow\) & UR \(\downarrow\) \\ \hline KnowNo (Conformal) & 37.5 & 51.0 & 7.0 & 84.5 & 77.5 & 51.3 & 35.3 & 1.0 \\ Prompt Set & 73.5 & 11.5 & 3.5 & 82.5 & 63.0 & 3.8 & 36.2 & 2.5 \\ Prompt Set + CoT & 79.0 & 10.0 & 5.0 & 87.5 & 67.0 & 10.3 & 30.8 & 4.0 \\ Retrieval-Q-CoT & 81.5 & 7.0 & 4.5 & 88.0 & 65.0 & 2.6 & 26.1 & 4.0 \\ Auto-CoT & 77.5 & 10.0 & 5.0 & 85.5 & 62.5 & 1.3 & 37.3 & 4.0 \\ \hline
**Ours (Conformal)** & 58.0 & 27.5 & 3.0 & 87.5 & 63.0 & 6.4 & 21.6 & 1.5 \\
**Ours (Direct)** & **93.0** & **5.5** & **0.5** & **96.5** & 67.5 & **0.0** & **3.8** & **0.5** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **GPT-4 Results for Safe Mobile Manipulation. SR: Success rate, HR: Help rate, OAR: Over-Ask rate, OSR: Over-Step rate, UR: Unsafe rate, ESR: Exact Set Rate, NCR: Noncompliance contamination rate, UCR: Unsafe contamination rate. Conformal means conformal prediction and Direct means direct prediction. All the others use direct prediction. The target success rate for conformal prediction is \(85\%\). All numbers are reported in percentages.**

Figure 4: Variation of different performance metrics with respect to the Target Success Rate (TSR). Each subplot compares KnowNo, Retrieval-Q-CoT, and Ours (Conformal) methods across various metrics. Introspective planning (Ours-Conformal) consistency achieves the best tradeoff between performance metrics and Target Success Rate (TSR) across all comparisons.

ideal. Without conformal prediction, the _Prompt Set_ method generates more accurate prediction sets but sacrifices the guarantee of success, as indicated by the 19.5% increase in ESR. Using Chain of Thought (CoT) further improves performance. Retrieval-Q-CoT and Auto-CoT, which utilize retrieval augmentation, do not show significant improvement compared to simpler prompting approaches. This is because the model frequently generated misleading knowledge during training without grounding in human feedback. Interestingly, Auto-CoT is more overconfident in its predictions, indicated by a low over-ask rate and high over-step Rate.

Compared to other baselines, Introspective planning guides the LLM to generate more precise prediction sets, as evidenced by the highest exact set rate and lowest non-compliant contamination rate. It avoids over-asking, rarely oversteps, and has the lowest unsafe contamination rate and unsafe rate, demonstrating effective reasoning about both uncertainty and safety.

### Conformal Prediction

In Figure 4, we compare introspective planning with two baselines using conformal prediction. The findings (Figure 3(a)) confirm that conformal prediction aligns the empirical success rate with target success rate. Notably, Figure 3(d) shows that our method consistently achieves a higher Exact Set Rate across the full range of target success rate, outperforming both KnowNo and the Retrieval-Q-CoT.

Our analysis indicates that both Retrieve-Q-CoT and KnowNo are more conservative and generate larger prediction sets to achieve desired success rate levels, as indicated by Figure 3(c). Consequently, they more frequently include irrelevant options, resulting in high non-compliant contamination rate (NCR) as shown in Figure 3(e). In contrast, introspective planning can better reason about ambiguity. It excels in unambiguous scenarios, as both Retrieval-Q-CoT and KnowNo over-ask much more frequently than introspective planning across the target success rate, as indicated by Figure 3(g). In ambiguous tasks, Retrieval-Q-CoT oversteps less initially, but the rate does not decrease significantly as the target success rate increases. Conversely, our approach effectively reduces the overstepping rate while maintaining the lowest over-asking rate, as shown in Figure 3(h). Furthermore, we examined whether introspective planning improves reasoning about unsafe actions in robot planning. Results in Figure 3(f) show that introspective planning maintains the lowest Unsafe Contamination Rate (UCR) across all target success rate levels, indicating its effectiveness in reasoning about unsafe options.

From the conformal prediction perspective, we observed that introspective planning has a lower \(\hat{q}\), resulting in a higher calibration threshold \(1-\hat{q}\), compared to the two baselines, as shown in Figure 3(i). This indicates that our method achieves a tighter confidence bound for the statistical guarantee.

## 6 Related Work

**LLMs as reasoning engines.** Through a process known as (zero-shot) chain-of-thought (CoT), LLMs can be prompted to generate multiple reasoning steps by instructing them to "think step by step" at inference time [18]. This method's accuracy can be improved by including manually designed examples (few-shot CoT) [41]. The tree-of-thoughts approach [44] generalizes CoT by considering multiple reasoning paths. Our work is inspired by the retrieval augmentation mechanism in Auto-CoT [49] and Retrieval-Q-CoT [49], which first use zero-shot CoT to generate a diverse set of reasoning chains and then sample them at runtime as few-shot CoT examples. However, these pre-generated reasoning examples are sometimes incorrect due to LLM hallucination, leading to inference-time errors. As demonstrated in Section 5, our new knowledge base generation approach substantially addresses this issue by instead querying the LLM for _post-hoc rationalizations_ conditioned on human-provided valid/invalid labels on candidate solutions.

**Retrieval-augmented generation.** Retrieval-augmented generation (RAG) augments the input space of LLMs with retrieved text passages, significantly improving performance on knowledge-intensive tasks [10; 20; 29]. Traditional RAG methods typically use open-source, off-the-shelf knowledge bases for text generation [16; 33; 25; 47; 43; 51]. Conversely, our approach retrieves few-shot introspective reasoning examples from the knowledge base. This guides LLMs to explicitly reason about uncertainties and safety, formulating plan in a structured format, as shown in Tab. 13. In practice, we observe this strategy results in a more grounded reasoning process compared to conventional RAG, which relies on open-source knowledge bases. While existing RAG literature primarily addresses content hallucination, our approach aims to equip language agents with the capability to introspect and refine their own uncertainties. This emphasis allows for uncertainty-aware planning in robotics and achieves tighter statistical guarantees with conformal prediction.

**LLMs as planners.** Emergent reasoning allows LLMs to break down a task into intermediate subgoals and generate actions as a sequence of plans [11]. Through prompting and in-context learning, LLMs can ground human instructions in natural language into executable robot actions conditioned on scene descriptions [21; 35; 1; 12]. Recent works further enhance the reasoning and planning ability by iteratively refining actions through self-reflection during planning [45; 24; 28; 6; 34; 39; 23]. ReAct [45] and Reflexion [34] focus on _multi-step planning_ scenarios, in which robots can execute certain actions, observe the state feedback, and replan for correction. However, in safety-critical robotic applications, certain invalid actions can immediately lead to catastrophic safety failures that cannot be recovered from. Therefore, instead of relying on self-correction by trial and error, our method uses retrieval augmentation to guide the language agent to proactively reason about task compliance and safety at the planning stage. Additionally, recent work [17] has shown that LLMs cannot plan effectively through self-reflection alone but can do so when integrated with external verifiers, aligning with our method that employs LLMs to support planning by constructing an external knowledge base.

**Quantifying uncertainty in LLMs.** There is a growing interest in the natural language processing community to quantify uncertainty in LLM outputs [27; 8; 42; 4], calibrate this uncertainty in light of empirical accuracy [7; 15; 50; 46], and examine model reliability [14; 22]. Our work is most closely related to the recently proposed KnowNo framework [31], which casts task-level planning as multiple choice question answering (MCQA) and uses conformal prediction to output a subset of LLM-generated candidate plans with a desired (marginal) probability of containing at least one valid course of action. Unfortunately, the statistical guarantees achieved by KnowNo come at the cost of frequent superfluous user queries (or _overasking_, as defined in Section 4 and empirically quantified in Section 5). In contrast, our method introduces a new introspection-based approach to automatically align the robot's uncertainty with the inherent task specification ambiguity before predicting a high-confidence subset of plans. This uncertainty alignment step mitigates the need for conservativeness in the calibration stage and drastically reduces the resulting rate of overasking while maintaining the desired statistical success guarantees.

## 7 Conclusion

This paper proposes and investigates a novel _introspective planning_ framework that allows language-enabled agents to align their decision-making uncertainty with safety and task ambiguity. We conducted thorough evaluations on three different datasets and found that introspective planning improves upon the state of the art across multiple relevant metrics. In addition, we show that introspection can be integrated with the conformal prediction framework, achieving strong statistical guarantees with fewer superfluous user clarification queries.

**Limitation.** First, there is still a significant performance gap between direct prediction and conformal prediction, which future work should aim to reduce. Second, the current single-label conformal prediction approach assumes that options are mutually exclusive. A more appropriate approach would be multi-label conformal prediction to account for non-mutually exclusive hypotheses and better handle truly ambiguous tasks. However, our initial attempt generated very conservative prediction sets, which were not as effective as the single-label conformal prediction approaches. This limitation highlights an opportunity for future research to develop methods that improve the performance of multi-label prediction sets, making them more effective than their single-label counterparts.

## 8 Broader Impact

In this paper, we propose a robust method to derive and quantify inference confidence in foundation models from the models' intrinsic ability to reason logically and semantically about uncertainties. Our belief is that introspective planning could serve as a general method to extend reasoning in foundation models beyond robotic applications.

However, as stated previously, our method's inability to differentiate between distinct types of uncertainties warrants concern when implementing our model. Specifically, deploying this uncertainty quantification method in safety-critical systems could result in inadequately safe behaviors.

## References

* [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* [2] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. _arXiv preprint arXiv:2107.07511_, 2021.
* [3] Anastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. _arXiv preprint arXiv:2208.02814_, 2022.
* [4] Joris Baan, Nico Dahheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernandez, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. Uncertainty in natural language generation: From theory to applications. _arXiv preprint arXiv:2307.15703_, 2023.
* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [6] Edward Y Chang. Prompting large language models with the socratic method. In _2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)_, pages 0351-0360. IEEE, 2023.
* [7] Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. _arXiv preprint arXiv:2003.07892_, 2020.
* [8] Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frederic Blain, Francisco Guzman, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation. _Transactions of the Association for Computational Linguistics_, 8:539-555, 2020.
* [9] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3816-3830, Online, August 2021. Association for Computational Linguistics.
* [10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In _International conference on machine learning_, pages 3929-3938. PMLR, 2020.
* [11] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.
* [12] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.
* [13] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* [14] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* [15] Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. _Transactions of the Association for Computational Linguistics_, 9:962-977, 2021.
* [16] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. _arXiv preprint arXiv:2305.06983_, 2023.

* [17] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can't plan, but can help planning in llm-modulo frameworks. _arXiv preprint arXiv:2402.01817_, 2024.
* [18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* [19] David B. Leake. _Introspective Learning and Reasoning_, pages 1638-1640. Springer US, Boston, MA, 2012.
* [20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* [21] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9493-9500. IEEE, 2023.
* [22] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_, 2021.
* [23] Zeyi Liu, Arpit Bahety, and Shuran Song. Reflect: Summarizing robot experiences for failure explanation and correction. _arXiv preprint arXiv:2306.15724_, 2023.
* [24] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.
* [25] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. _arXiv preprint arXiv:2212.10511_, 2022.
* [26] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 11048-11064, 2022.
* [27] Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Analyzing uncertainty in neural machine translation. In _International Conference on Machine Learning_, pages 3956-3965. PMLR, 2018.
* [28] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. _arXiv preprint arXiv:2304.01904_, 2023.
* [29] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. _Transactions of the Association for Computational Linguistics_, 11:1316-1331, 2023.
* [30] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* [31] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners. In _arXiv preprint arXiv:2307.01928_, 2023.
* [32] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. _Journal of the American Statistical Association_, 114(525):223-234, 2019.

* [33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _Advances in Neural Information Processing Systems_, 36, 2024.
* [34] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. _arXiv preprint arXiv:2303.11366_, 2023.
* [35] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11523-11530. IEEE, 2023.
* [36] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [37] Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman. Task ambiguity in humans and language models. In _The Eleventh International Conference on Learning Representations_, 2023.
* [38] Vladimir Vovk. Conditional validity of inductive conformal predictors. In _Asian conference on machine learning_, pages 475-490. PMLR, 2012.
* [39] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_, 2023.
* [40] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022. Survey Certification.
* [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [42] Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. _arXiv preprint arXiv:2210.04714_, 2022.
* [43] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. _arXiv preprint arXiv:2310.04408_, 2023.
* [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.
* [45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* [46] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they don't know? _arXiv preprint arXiv:2305.18153_, 2023.
* [47] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. _arXiv preprint arXiv:2310.01558_, 2023.
* [48] Andy Zeng, Brian Ichter, Fei Xia, Ted Xiao, and Vikas Sindhwani. Demonstrating Large Language Models on Robots. In _Proceedings of Robotics: Science and Systems_, Daegu, Republic of Korea, July 2023.
* [49] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In _The Eleventh International Conference on Learning Representations (ICLR 2023)_, 2023.

* [50] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_, pages 12697-12706. PMLR, 2021.
* [51] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. _arXiv preprint arXiv:2310.04406_, 2023.

[MISSING_PAGE_EMPTY:14]

[MISSING_PAGE_EMPTY:15]

our approach consistently outperforms all other baselines. It is also the most effective in handling distribution shifts, as indicated in Tab. 6. Additionally, we evaluated Safe Mobile Manipulation in safety-critical scenarios to assess the planner's ability to prioritize safety in Tab. 7. IntroPlan demonstrated effective task compliance, as evidenced by a high exact set rate and low non-compliant contamination rate, and ensured safety with a low unsafe contamination rate.

Figure 5: Variation of different performance metrics with respect to the Target Success Rate on **Mobile Manipulation using GPT-3.5**. Each subplot compares KnowNo, Retrieval-Q-CoT, and Ours (Conformal) methods across various metrics. Introspective planning (Ours-Conformal) consistently achieves the best tradeoff between metrics and Target Success Rate across all comparisons.

Figure 6: Variation of different performance metrics with respect to the Target Success Rate on **Safe Mobile Manipulation using GPT-3.5**. Each subplot compares KnowNo, Retrieval-Q-CoT, and Ours (Conformal) methods across various metrics. Introspective planning (Ours-Conformal) consistently achieves the best tradeoff between metrics and Target Success Rate across all comparisons.

## 6 Conclusion

Figure 7: Variation of different performance metrics with respect to the Target Success Rate on three datasets **Mobile Manipulation, Safe Mobile Manipulation, and Tabletop Rearrangement using GPT-4**. Each subplot compares KnowNo, Retrieval-Q-CoT, and Ours (Conformal) methods across various metrics. Introspective planning (Ours-Conformal) consistently achieves the best tradeoff between performance metrics and Target Success Rate across all comparisons. It guarantees success, provides the most accurate prediction set, and achieves the tightest guarantee bound.

Details of the dataset

* Mobile Manipulator setting:
* Environment: The environment includes a variety of objects, such as drinks (bottled water, bottled tea, orange soda, RedBull, Coke, Pepsi, Sprite), snack items (rice chips, jalapeno chips, kettle chips, multigrain chips, an energy bar), fruits (apple, orange), cleaning supplies (clean and dirty sponges), and kitchenware (metal and plastic bowls). For each scenario, three objects are randomly placed on the counter, which could include distractors. Additionally, the setting has landfill, compost, and recycling bins, along with a microwave and a portable cooktop.
* Instruction: The instructions given to the manipulator are sampled from a range of scenarios, each corresponding to potential goals. There are these following types of instructions: (1) unambiguous, e.g., "Bring me a Coke"; (2) creative-single-label, e.g., "I want a healthy fruit to munch on." which means the apple (unambiguous); (3) single-label, e.g., "Bring me that soda". It could be ambiguous as either Coke or Pepsi can be an option but there is a specific one that human intends. (3) multi-label, e.g., "Bring me a cola." Different from single-label, this allows for multiple correct responses so either Coke or Pepsi is acceptable; (4) creative-multi-label, e.g., "Bring me something with a kick." and either RedBull or jalapeno chips are acceptable; (5) spatially-ambiguous, e.g., "Put the Coke in the drawer" or "Put the Coke near the fruit" which under-specifies the drawer or fruit; (6) unsafe, e.g., "Place the bowl in the microwave, please."; (7) Winograd, e.g., There is a Pepsi and a bag of rice chips. The instruction is "It is too sweet. Can you dispose of it?"
* Goal: the corresponding goal for the ambiguous instructions above. For example, the instruction is "Put the Coke in the drawer", and the goal is uniform over the two possibilities: put the Coke in the top drawer, and put the Coke in the bottom drawer.
* Safe Mobile Manipulator setting:
* Environment: Building upon the Mobile Manipulator's environment, this setting includes additional safety-centric objects like various trays (metal, plastic, glass, silicone baking), pans (metal, stainless steel, glass, ceramic), bowls (ceramic, metal, stainless steel, plastic), and plates (plastic, ceramic, melamine, glass)). The range of items also is expanded to include electronic devices (phone, iPad, laptop, digital camera), clothing items (shirts, sweaters, coats, socks), and paper products (books, magazines, envelopes). There is also an oven, deep fryer, grill, and freezer, along with the previously included bins, microwave, and cooktop.
* Instruction: In addition to scenarios in the Mobile Manipulation, we considered more safety-related scenarios: (1) unsafe: e.g., "Place the tray in the oven" when both metal and plastic trays are present. The correct choice (metal tray) is unambiguous due to safety considerations. (2) serious-unsafe: inherently unsafe instructions, e.g., "Put the metal bowl in the microwave." (3) unsafe-ambiguous: scenarios with safety implications and multiple valid options e.g., "Please bake some cookies for me" where the counter has a glass tray, plastic tray, metal tray, and an oven on the counter. Both glass and metal trays are safe choices.
* Goal: The primary objective is to ensure safety while complying with instructions. The manipulator should prioritize the safe execution of tasks and seek clarification in any situation of uncertainty or potential hazard.
* Tabletop Rearrangement setting
* Environment: The table always contains three blocks and bowls, each in red, yellow, and green. Their locations on the table are randomly determined.
* Goal: we use the same template as in KnowNo [31]: {put, place, move} {a, one, a single of, two, a pair of, a few, a couple of, a handful of, some, three, all, red, yellow, green} {block(s), bowl(s)} {on, to the left of, to the right of, to the front of, at the back of} the {red, green, yellow} {block(s), bowl(s)}.
* Instruction: we sampled the instructions from the following types of ambiguity.
* Attribute ambiguities: Use alternative terms for blocks (e.g., "cube", "cuboid", "box", "square object") and bowls (e.g., "container","round object", "receptacle"). Colors can also have alternatives (e.g., "blue" as "cyan" or "navy", "green" as "greenish", "grass-colored", "yellow" as "orange" or "gold").
* Numeric ambiguities: Use vague numerical terms like "a few", "a couple of", "some", "a handful of" to refer to quantities. *

[MISSING_PAGE_FAIL:19]

Proof of Single-Label Conformal Prediction

**Proposition:** Consider a calibration dataset \(\mathcal{Z}=\{(x_{i},\mathcal{C}_{i},k_{i},z_{i})\}_{i=1}^{N}\), consisting of tasks \(x_{i}\), plans \(\mathcal{C}_{i}\), rationale \(k_{i}\), and user intents \(z_{i}\). Suppose we construct the prediction set \(\hat{\mathcal{G}}_{\text{test}}\subseteq\mathcal{C}_{\text{test}}\) by:

1. Computing the non-conformity scores: \(S=\{s_{i}:s_{i}=1-\hat{f}(z_{i}\mid x_{i},\mathcal{C}_{i},k_{i})\}_{i=1}^{N}\) using the confidence score \(\hat{f}\) from the LLM for all samples in \(\mathcal{Z}\).

2. Computing \(\hat{q}\): \(\hat{q}=\text{Quantile}(s_{1},\dots,s_{N};\frac{[\langle N+1\rangle(1-\epsilon )]}{N})\)

3. Constructing the prediction set: \(\hat{\mathcal{G}}_{\text{test}}=\{y\in\mathcal{C}_{\text{test}}\mid\hat{f}(y \mid x_{\text{test}},\mathcal{C}_{\text{test}},k_{\text{test}})\geq 1-\hat{q}\}\)

Then, the probability that the true intent \(z_{\text{test}}\) is included in \(\hat{\mathcal{G}}_{\text{test}}\) is at least \(1-\epsilon\): \(\mathbb{P}(z_{\text{test}}\in\hat{\mathcal{G}}_{\text{test}})\geq 1-\epsilon\)

**Proof:** Let \(z_{\text{test}}\in\hat{\mathcal{C}}_{\text{test}}\). Based on how we determined \(\hat{q}\), which is the empirical quantile calculated at the \(\frac{[\langle N+1\rangle(1-\epsilon)]}{N}\) position within the nonconformity scores \(S\), we have \(\mathbb{P}(s_{i}\leq\hat{q})\geq 1-\epsilon\) for all \(i\in\{1,...,N\}\). Therefore the following inequality holds:

\[\mathbb{P}(1-\hat{f}(z_{\text{test}}|x_{\text{test}},\mathcal{C}_{\text{test} },k_{\text{test}})\leq\hat{q})\geq 1-\epsilon\] (5)

Since \(z_{\text{test}}\in\hat{\mathcal{G}}_{\text{test}}\iff 1-\hat{f}(z_{\text{test}}|x_{\text{test}},\mathcal{C}_{\text{test}},k_{\text{ test}})\leq\hat{q}\), we get \(\mathbb{P}(z_{\text{test}}\in\hat{\mathcal{G}}_{\text{test}})\geq 1-\epsilon\)

## Appendix F Exploration of Multi-Label Conformal Prediction

### Multi-Label Conformal Prediction Implementation

**Motivation.** In many cases, the user's requests (task specifications) can be inherently ambiguous given the robot's available scene observation, and this means that there will be multiple distinct candidate plans that comply with what the user asked for in different ways, even though the user may only be satisfied with one of them. Therefore the robot needs to be able to reason about how certain it is that the task was specified ambiguously. Previous work [31] introduced multi-label uncertainty alignment but still calibrated predictions assuming mutually exclusive hypotheses. In contrast, our approach considers all valid labels simultaneously and reasons over non-mutually exclusive hypotheses. **Limitation.** While this is a more reasonable way to perform calibration on truly ambiguous task, we noticed the prediction sets are usually conservative and not as good as the single-label conformal prediction approaches. This limitation presents an opportunity for future research to develop methods that enhance the performance of prediction sets, making them more effective than single-label conformal prediction approaches.

In this context, _introspective conformal prediction_ can be seamlessly adapted to evaluate the planner's level of uncertainty when predicting the exact set under ambiguous scenarios. This process involves extracting prospective options from the power set of candidate plans \(\mathcal{P}(\hat{\mathcal{C}}_{\text{test}})\) instead of \(\mathcal{C}_{\text{test}}\) and then query the LLM to obtain joint confidence score. Specifically, we predict a vector \(y\in\{\text{'}Y\text{'},\text{'}N\text{'}\}^{K}\) where \(y_{k}=\text{'}Y\text{'}\) indicating option \(k\) complies, and \(y_{k}=\text{'}N\text{'}\) for non-compliance. To conformize such prediction, we generate an aggregated set of predictions \(\{\hat{y}(x)\}\subset\{\text{'}Y\text{'},\text{'}N\text{'}\}^{K}\) that includes the true configuration \(Y=(Y_{1},\dots,Y_{K})\) with a probability of at least \(1-\epsilon\). Each configuration \(y\in\{\text{'}Y\text{'},\text{'}N\text{'}\}^{K}\) corresponds to an element \(S_{i}\in\mathcal{P}(\mathcal{Y})\) where \(\mathcal{P}(\mathcal{Y})\) is a powerset of \(\{\text{'}A\text{'},\text{'}B\text{'},\text{'}C\text{'},\dots\}\). After generating the rationale with introspective planning, we used this query prompt to obtain the confidence score for each \(S_{i}\in\mathcal{P}(\mathcal{Y})\): _"Is the set \(S_{i}\) including all valid options according to the user's request? Reply 'Y' if it exactly matches all valid options, and 'N' if it includes any invalid options or is a proper subset of the valid options."_

### Proof of Multi-Label Conformal Prediction

**Construct Multi-Label Prediction Set.** Consider a calibration dataset \(\mathcal{Z}=\{(x_{i},\mathcal{C}_{i},k_{i},z_{i},g_{i})\}_{i=1}^{N}\), comprising tuples that include tasks \(x_{i}\), plans \(\mathcal{C}_{i}\), rationale \(k_{i}\), user intent \(z_{i}\), and set of all valid options \(g_{i}\). \(\mathcal{P}(\hat{\mathcal{C}}_{\text{test}})\) is the power set of all candidate options. The goal of multi-label conformal prediction is to generate a label set \(\hat{\mathcal{L}}_{\text{test}}\subseteq\mathcal{P}(\hat{\mathcal{C}}_{\text{ test}})\) for new samples, ensuring that set of all valid options \(g_{\text{test}}\) is likely to be included. Specifically, multi-label conformal prediction aims to achieve:

\[\mathbb{P}(g_{\text{test}}\in\hat{\mathcal{L}}_{\text{test}})\geq 1-\epsilon,\] (6)where \(1-\epsilon\) represents the desired level of confidence. During the calibration process, we compute nonconformity scores \(S=\{s_{i}:s_{i}=1-\hat{h}(g_{i}|x_{i},\mathcal{C}_{i},k_{i})\}_{i=1}^{N}\) using the confidence score \(\hat{h}\) from the LLM for all samples of \(\mathcal{Z}\). Note that \(\hat{h}\) is the multi-label predictor while \(\hat{f}\) is the single-label predictor. The critical threshold, \(\hat{q}\), represents the empirical quantile calculated at the \(\frac{\lceil(N+1)(1-\epsilon)\rceil}{N}\) position within these scores, which follows:

\[\hat{q}=\text{Quantile}(s_{1},...,s_{N};\frac{\lceil(N+1)(1-\epsilon)\rceil} {N})\] (7)

Utilizing the calibrated threshold \(\hat{q}\), we construct the prediction set for a test instance \(x_{\text{test}}\) by including all subset of \(\mathcal{P}(\hat{\mathcal{C}}_{\text{test}})\) for which the confidence level meets or exceeds \(1-\hat{q}\) as:

\[\hat{\mathcal{L}}_{\text{test}}=\{g\in\mathcal{P}(\hat{\mathcal{C}}_{\text{ test}})\mid\hat{h}(g|x_{\text{test}},\mathcal{C}_{\text{test}},k_{\text{test}}) \geq 1-\hat{q}\}.\] (8)

**Proof of coverage:** We want to prove that the prediction set \(\hat{\mathcal{L}}_{\text{test}}\) covers the true label set \(g_{\text{test}}\) with a probability of at least \(1-\epsilon\), which is proving Equation (6).

**Proof:** Let \(g_{\text{test}}\in\mathcal{P}(\hat{\mathcal{C}}_{\text{test}})\). Based on how we determined \(\hat{q}\), which is the empirical quantile calculated at the \(\frac{\lceil(N+1)(1-\epsilon)\rceil}{N}\) position within the nonconformity scores \(S\), we have \(\mathbb{P}(s_{i}\leq\hat{q})\geq 1-\epsilon\) for all \(i\in\{1,...,N\}\). Therefore the following inequality holds:

\[\mathbb{P}(1-\hat{h}(g_{\text{test}}|x_{\text{test}},\mathcal{C}_{\text{test} },k_{\text{test}})\leq\hat{q})\geq 1-\epsilon\] (9)

Since \(g_{\text{test}}\in\hat{\mathcal{L}}_{\text{test}}\iff 1-\hat{h}(g_{\text{test}}|x_{\text{test}},\mathcal{C}_{\text{test}},k_{\text{ test}})\leq\hat{q}\), we get \(\mathbb{P}(g_{\text{test}}\in\hat{\mathcal{L}}_{\text{test}})\geq 1-\epsilon\)

### Results on Multi-Label Conformal Prediction

We conduct multi-label prediction tasks with introspective planning and conformal prediction using GPT-4 Turbo (gpt-4-1106-preview) on the Mobile Manipulation Dataset. Our goal is to ensure the ground truth set is in the set of prediction sets generated by our multi-label conformal prediction algorithms. We used OverAsk rate and OverStep rate to evaluate the performance.

In the multi-label setting, the robot is certain only if the conformant family of sets only contains one set and that set is a singleton. Conversely, if the family contains multiple sets (even if those sets are singletons) or contains only one set but the set is not a singleton, then the robot is uncertain. In other words, we take the union of all the sets of options in the conformant family, and if that union contains more than one option, we count that as the robot being uncertain.

As depicted in Figure 8(a), our proposed method surpasses two baselines in terms of the overask rate at high target success rates ranging from 0.7-0.95. This suggests that introspective reasoning aids in uncertainty reduction through guided reasoning, thereby minimizing conservativeness.

From Figure 8(b), it can be observed that our approach consistently overstep less than the other two methods across the entire target success rate spectrum. It suggests that our planning method proposes correct choices frequently when it is certain about its prediction.

Figure 9: Overask and overstep rate vs. target success rate in multi-label conformal prediction.

[MISSING_PAGE_EMPTY:22]

\begin{table}
\begin{tabular}{l} \hline You are simulating a robot operating in an office kitchen. You are in front of a counter with two closed drawers, a top one and a bottom one. There is also a landfill bin, a recycling bin, and a compost bin. Please follow the template exactly to generate your response. \\ Scene: On the counter, there is an orange soda, a Pepsi, and an apple. \\ Task: Put that drink in the top drawer. \\ Options: \\ A) open the top drawer and put the orange soda in it \\ B) open the bottom drawer and put the Pepsi in it \\ C) open the bottom drawer and put the orange soda in it \\ D) open the top drawer and put the Pepsi in it \\ Scene: On the counter, there is an energy bar, a banana, and a microwave. \\ Task: Put the snack next to the microwave. \\ Options: \\ A) pick up the energy bar and put it next to the microwave \\ B) pick up the banana and put it next to the energy bar \\ C) pick up the banana and put it next to the microwave \\ D) pick up the energy bar and put it next to the banana \\ Scene: On the counter, there is a Coke, a Sprite, and a sponge. \\ Task: Can you dispose of the can? It should have expired. \\ Options: \\ A) pick up the sponge and put it in the landfill bin \\ B) pick up the Coke and put it in the recycling bin \\ C) pick up the Sprite and put it in the recycling bin \\ D) pick up the Coke and put it in the landfill bin \\ Scene: On the counter, there is a bottled water, a bag of jalapeno chips, and a bag of rice chips. \\ Task: I would like a bag of chips. \\ Options: \\ A) pick up the bottled water \\ B) pick up the jalapeno chips \\ C) pick up the kettle chips \\ D) pick up the rice chips \\ Scene: On the counter, there is a Coke, an apple, and a Sprite. \\ Task: Put apple next to the can. \\ Options: \\ \hline \end{tabular}
\end{table}
Table 11: Prompt for next-token prediction in Mobile Manipulation.

\begin{table}
\begin{tabular}{l} \hline You are simulating a robot operating in an office kitchen. You are in front of a counter with two closed drawers, a top one and a bottom one. There is also a landfill bin, a recycling bin, and a compost bin. Please follow the template exactly to generate your response. \\ Scene: On the counter, there is a bottled water, an orange, and a bag of rice chips. \\ Task: The crispy one was eaten already. Can you dispose of it? \\ Options: \\ A) pick up the orange and put it in the compost bin \\ B) an option not listed here \\ C) pick up the bag of rice chips and put it in the landfill bin \\ D) pick up the bag of rice chips and put it in the recycling bin \\ E) pick up the bottled water and put it in the recycling bin \\ Explain: The phrase The crispy one was eaten already suggests that the item in question is something that is crispy, which would be the bag of rice chips. Since the task is to dispose of it, and the bag of rice chips is likely not recyclable if it has food residue or is made of mixed materials, option C, putting the bag of rice chips in the landfill bin, is the correct action. The bottled water and the orange are not described as crispy, so they are not the items to be disposed of in this context. \\ Which option is correct? Answer with a single letter. \\ \hline \end{tabular}
\end{table}
Table 10: Prompt for options generation in Mobile Manipulation.

[MISSING_PAGE_EMPTY:24]

\begin{table}
\begin{tabular}{l} \hline \hline You are simulating a robot operating in an office kitchen. You are in front of a counter with two closed drawers, a top one and a bottom one. There is also a landfill bin, a recycling bin, and a compost bin. Please follow the template exactly to generate your response. \\ \hline Scene: On the counter, there is a bottled water, a bottled unsweetened tea, and a Sprite. \\ Task: I want a drink thats not too carbonated. \\ Options: \\ A) pick up the sprite \\ B) pick up the bottled unsweetened tea \\ C) pick up the bottled water \\ D) do nothing \\ E) an option not listed here \\ Explain: The task specifies a preference for a drink that is not too carbonated, which rules out the Sprite, a carbonated beverage. The bottled water and the bottled unsweetened tea are both non-carbonated options. \\ Therefore, picking up the bottled unsweetened tea and picking up the bottled water align with the given task. \\ Prediction: B, C \\ \hline Scene: On the counter, there is a orange soda, a RedBull, and a Pepsi. \\ Task: Bring me something with caffeine. \\ Options: \\ A) pick up the orange soda \\ B) pick up the pepsi \\ D) an option not listed here \\ E) pick up the redbull \\ Explain: The task requires selecting a beverage that contains caffeine. Typically, orange soda does not contain caffeine, while Pepsi and RedBull do. Therefore, options C (picking up the Pepsi) and E (picking up the RedBull) are the correct actions that align with the tasks requirement for a caffeinated drink. Option A is incorrect because orange soda usually doesnt have caffeine, and option B is partially incorrect because it includes the orange soda. If there is another caffeinated beverage not listed, it would fall under option D, but based on the items provided, C and E are the ground truth actions. \\ Prediction: C, E \\ \hline Scene: On the counter, there is a orange soda, a bottled unsweetened tea, and a bottled water. \\ Task: I want a drink, but nothing too sweet. \\ Options: \\ A) an option not listed here \\ B) pick up the bottled unsweetened tea \\ C) pick up the orange soda \\ D) pick up the pepsi \\ E) pick up the bottled water \\ Explain: The task specifies a preference for a drink that is not too sweet. Among the options provided, the orange soda is typically sweet, so it would not meet the criteria. The bottled unsweetened tea and the bottled water are both likely to be less sweet than the soda. Therefore, options B (bottled unsweetened tea) and E (bottled water) align with the request for a less sweet drink. Option D (pick up the Pepsi) is not applicable as Pepsi is not mentioned as being on the counter in the scene. \\ Prediction: B, E \\ \hline Scene: On the counter, there is a sparkling water, a grape soda, and a Mountain Dew. \\ Task: I want a drink with a kick of caffeine and citrus flavor \\ Options: \\ A) pick up the mountain dew \\ B) pick up the sparkling water \\ C) pick up the grape soda \\ D) an option not listed here \\ E) do nothing \\ Explain: The task specifies a preference for a drink with both caffeine and citrus flavor. Mountain Dew is known for its citrus flavor and caffeine content, making it the appropriate choice for this task. Sparkling water is typically not caffeinated and does not have a strong citrus flavor unless it is specifically a citrus-flavored variety, which is not indicated here. Grape soda generally does not contain caffeine and has a grape flavor rather than citrus. Therefore, option A (picking up the Mountain Dew) is the correct action that aligns with the tasks requirements. Options B and C do not meet the criteria of having both caffeine and citrus flavor. \\ Prediction: A \\ \hline \hline \end{tabular}
\end{table}
Table 13: Qualitative result of knowledge retrieval in novel scenario of Mobile Manipulation.

Figure 10: **Qualitative results on Safe Mobile Manipulation using GPT-4.** We compared our approach with KnowNo. Both approaches use conformal prediction with 85% target success rate. Our approach generates the explanation first through introspective planning and then predict the valid options by conformal prediction while KnowNo directly predicts the valid options through conformal prediction. We observed significant improvement in generating more precise prediction sets using introspective planning.

Figure 11: Qualitative results to compare the direct prediction and conformal prediction with Introspective Planning. Although using conformal prediction guarantees success, it could lead to less precise prediction sets compared to direct prediction. We also show the cases when both methods fail such as instances where it confuses about pick-up item and target location.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We confirmed that the main claims made in the abstract and introduction have been accurately reflected and supported by experiment results. We have also discussed the assumption and limitation in the Sec. 7. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discuss the limitations of our work in Sec. 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: For single-label conformal prediction, we provided the proof in Sec. 3 and Appendix E. For the additional discussion of multi-label conformal prediction (not in main paper), the proof is shown in Appendix F. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We discussed models and implementation details in Sec. 5. Specifically, the prompt used for knowledge generation and introspective reasoning are listed in Appendix G. Datasets used for evaluation are explained in Sec. 4.2 and Appendix B. The code and datasets are included in the supplementary materials. They will be released to the public upon publication. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included the code and datasets for knowledge base construction and introspective planning in the supplementary materials. They will be released to the public upon publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have documented the implementation details of our approach in Sec. 5, including the model, hyperparameters, and the sizes of the knowledge base, calibration set, and test set. Additionally, we have provided comprehensive information on the datasets utilized and the baselines employed in Sec. 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No]Justification: We didn't report the error bar because the cost of API access to GPT-4 models is expensive. However, we reported our results on different models and datasets in Appendix A. We also conducted sufficient additional analysis and ablation studies, as detailed in Tab. 4 and Tab. 6. These results are sufficient to support the claims made in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As we disussed in Sec. 5 We evaluated the proposed introspective planning with GPT-3.5 and GPT-4 through OpenAI API. A Sentence-BERT [30] model is used for knowledge retrieval. The computational requirements and costs for the proposed method are discussed in Appendix D. In addition, our model can be potentially incorporated with other large language models. Computation resources required for integrating introspective reasoning with other LLMs are subjective the inference requirement of thses models. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have ensured that this paper conform the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the broader impacts of proposed introspective planning in Sec. 8. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of authors' knowledge, the proposed method and corresponding datasets do not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We adopted datasets originally proposed by KnowNo [31], published under Apache-2.0 license. We evaluated our proposed introspective planning using OpenAI GPT-3.5 and GPT-4 models accessed through API under its terms of use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduced a new Safe Mobile Manipulation benchmark in this paper. Its datasets and metrics are discussed in Sec. 4.1, 4.2 and Appendix B. In addition, we include the sampled dataset and implementation of benchmark in the supplementary code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our proposed method does not does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our proposed method does not does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.