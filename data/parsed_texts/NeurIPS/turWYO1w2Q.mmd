# Information Directed Tree Search Reasoning and Planning with Language Agents

 Yash Chandak, Hyunji Alex Nam, Allen Nie, Jonathan Lee and Emma Brunskill

Department of Computer Science

Stanford, CA

<ychandak@stanford.edu>

###### Abstract

Solving challenging tasks often require agentic formulation of language models that can do multi-step reasoning and progressively solve the task by collecting various feedback. For computational efficiency, it may be advantageous to quantify the information associated with different feedback and guide the search such that the solution can be obtained quickly. To explore this possibility, we take a Bayesian approach and propose an _information directed tree search_ (IDTS) algorithm that makes use of in-context learning to approximate the information associated with different feedback. We explore the effectivity of IDTS on challenging tasks involving programming, formal math, and natural language. Interestingly, while we find advantages over simple uniform search methods, the proposed approach is about comparable to MCTS even though it explores different paths. We discuss some possibilities for our findings and highlight open questions for future work.

## 1 Introduction

Large language models (LLMs) have become integral for building autonomous agents that aim to find solutions to challenging tasks. Many such tasks often require multi-step reasoning, are not well specified, or require hierarchical decomposition (Hao et al., 2023; Zelikman et al., 2022). In such cases, even though LLMs cannot directly provide the complete answer, they can provide partially correct responses, or answer sub-parts of the prompt. Such responses can be then paired with other source(s) of feedback that can subsequently guide the reasoning of the language models. For instance, if the generated response is a code, then the feedback can be provided by program compilers and unit testing (Zhang et al., 2023; Zhou et al., 2023). If the generated response is formal proofs for mathematical statements, then auto-verifiers can be used (First et al., 2023; Zheng et al., 2023). Further, language models can also be used to self-critique their responses (Zhou et al., 2023).

However, such sources of feedback vary in terms of their quality. For instance, compilers can only flag incorrect code - they cannot tell what to change in that incorrect code. In contrast, while language model critics can suggest what to change, they might often hallucinate and provide inaccurate feedback. Similarly, humans may want to work with a language model to achieve a goal but may not know themselves what the right steps to reach that goal are. This raises the main question of interest: How do we leverage partially correct response generating language models, with partially correct source(s) of feedback, to find solution to important problems?

We cast this as a planning problem with partially correct feedback, and design a new tree search procedure for inference time planning. To be computationally efficient when resources are limited, unlike MCTS (Coquelin and Munos, 2007; Kocsis and Szepesvari, 2006) that does a naive countbased exploration, we take a Bayesian approach and prioritize search towards feedback that provides higher _information gain_ towards the solution.

Problem Statement:Let \(o\in\mathcal{O}\) and \(a\in\mathcal{A}\) correspond to an observation and an action. let each observation consist of some context \(x\in\mathcal{X}\) and a reward \(r\in\mathbb{R}\), such that \(o=(x,r)\). Let \(O_{i}=(X_{i},R_{i})\) and \(A_{i}\) be the random variables corresponding to observation and action at interaction step \(i\). Specifically, the context \(X_{0}\) in the initial observation \(O_{0}\) contains the question, and the action \(A_{i}\) is an agent's attempt at the solution and \(X_{i+1}\) contains rich feedback on that action. The goal of the agent is to determine an answer \(Y\in\mathcal{Y}\) to that question within a fixed number of interactions. For example, for coding tasks, \(X_{0}\) is a coding question, \(A_{i}\) is the language model's code, \(Y\) is a correct code solution, and \(O_{i+1}\) contains the error messages from compiler and results from unit tests for \(A_{i}\).

We define a state \(s\in\mathcal{S}\) to be the history of the interaction so far, i.e., \(S_{i}\coloneqq(O_{0},A_{0},O_{1},A_{1},\ldots,O_{i})\). Let \(\pi:\mathcal{S}\to\mathcal{T}(\mathcal{A})\) be a policy and let \(\tau^{\pi}(s)\) correspond to the trajectory unrolled using policy \(\pi\) starting from a state \(s\). In this work, we build upon pretrained LLMs and thus implicitly leverage side information (e.g., data on the internet) to find the solution \(Y\). Let \(\mathcal{D}\) denote such data. Let the random variable \(\mathcal{T}_{t}\) be the tree containing everything observed till the end of the \(t^{\text{th}}\) iteration. With a slight abuse of notation, we will use a subscript of \(t\) to denote implicit conditioning on everything observed so far, i.e., \(\mathcal{T}_{t}\) and \(D\). For example, \(\mathbb{P}_{t}(Y=y|x)\coloneqq\mathbb{P}(Y=y|x,\mathcal{T}_{t},D)\).

Information Directed Sampling:Given random variables \(X\) and \(Y\), the information gain for \(Y\) on observing \(X\) is \(\mathcal{I}_{t}(Y;X)\coloneqq\mathcal{H}_{t}(Y)-\mathcal{H}_{t}(Y|X)\), where \(\mathcal{H}_{t}(Y)\coloneqq-\sum_{y\in\mathcal{Y}}\mathbb{P}_{t}(y)\log \mathbb{P}_{t}(y)\) is the entropy and \(\mathcal{H}_{t}(Y|X)\coloneqq-\sum_{x\in\mathcal{X}}\mathbb{P}_{t}(x)\sum_{y \in\mathcal{Y}}\mathbb{P}_{t}(y|x)\log\mathbb{P}_{t}(y|x)\) is the conditional entropy.

In the bandit setting, let \(Z(A)\coloneqq(O,A)\) be the rich observation (potentially including reward), and action \(A=a\) on executing \(a\). For the optimal action \(a^{*}\), let \(\mathcal{I}_{t}(a^{*};Z(a))\) corresponds to the information gain towards the agent's belief over the optimal action \(a^{*}\) given \(Z(a)\). Similar to Bayesian experimental design (Rainforth et al., 2024), information directed sampling(Russo and Van Roy, 2014) or its \(\lambda\)-regularized version (Hao and Lattimore, 2022) selects an action such that it maximizes the performance as well the information gained by the agent,

\[a\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\frac{\mathcal{I}_{t}(a^{*};Z(a ))}{\mathbb{E}_{t}[r(a^{*})-r(a)]^{2}},\qquad\qquad a\in\operatorname*{arg\,max }_{a\in\mathcal{A}}\ r(a)+\lambda\,\mathcal{I}_{t}(a^{*};Z(a)).\] (1)

Monte-Carlo Tree Search:A popular planning procedure is MCTS that tracks \(N_{t}(s,a)\) and \(N_{t}(s)\), i.e., the number of times a particular \((s,a)\) pair and \((s)\) has been chosen during tree traversal till iterate \(t\). Let \(\mathcal{T}_{t}\) be the tree expanded at the start of a given iterate \(t\). At every iterate \(t\), one of the nodes of \(\mathcal{T}_{t}\) is chosen to be expanded by selecting action \(a\in\mathcal{A}\) for a given state \(s\in\mathcal{S}\). Most MCTS algorithms are based on the seminal upper confidence tree (UCT) algorithm (Kocsis and Szepesvari, 2006; Coquelin and Munos, 2007) that chooses

\[a\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\left\{\hat{q}_{t}(s,a)+c\sqrt{ \frac{\log\,N_{t}(s)}{N_{t}(s,a)}}\right\},\] (2)

where \(\hat{q}\) is the estimate of the cumulative return (Sutton and Barto, 2018), and the term in blue guides the exploration. While (2) provides one approach to balance exploration and exploitation based on counts, not only such discrete notion of counts fail to account for similarities between different states, it also fails to incorporate richer notion of explorations, e.g., even if an agent has taken an action

\begin{table}
\begin{tabular}{l|c c c c}  & **LLMs** & **Planning** & **Robust** & **Rich Exploration** \\ \hline One-shot generation & ✓ & ✗ & ✗ & ✗ \\ Iterative Refinement/CoT & ✓ & ✓ & ✗ & ✗ \\ Search (MCTS, best-first, etc.) & ✓ & ✓ & ✓ & ✗ \\ IDTS (Proposed) & ✓ & ✓ & ✓ & ✓ \\ \end{tabular}
\end{table}
Table 1: Methods that have single chain of thought are often not robust when the feedback is only partially correct. In contrast, search based methods maintain several solutions and corresponding feedback, and are thus less likely to be derailed in their reasoning if the feedback is not perfect. However, conventional search based methods still explore using count-based statistics, which fail to account for any notion of information associated with each feedback. The proposed IDTS method aims at alleviating all these issues. We discuss related work in more detail in Appendix A.

\(a\in\mathcal{A}\) often, it might still want to explore that action more if the _information_ being obtained from doing so is large.

The key insight of this work is to leverage ideas from (1) and use information gain to improve the exploration technique for monte-carlo tree search (2). Particularly, for our problem setup, we let the action \(a\) be a complete sentence/solution generated by a LLM, and \(s\) be the history of past interactions. Then, if the task is related to coding, we want to prioritize a solution \(a\), which together with its error messages and unit-tests \(Z(a)\) provides higher information gain towards the solution \(a^{*}\). (While in principle, maximization over the action set \(\mathcal{A}\) in intractable, similar to prior work (Zhou et al., 2023; Jang et al., 2020) we let \(\mathcal{A}\) be k sampled completions for any given state \(s\).) In the remaining sections, we discuss how to estimate information gain associated with taking any action \(a\) in state \(s\).

## 2 Information Directed Tree Search

Central to our idea is a procedure to estimate information gain (IG), such that the proposed idea can scale and be used with LLMs. Here we focus on explaining the core idea of computing IG using in-context learning for a single interaction step. Due to space constraints, in Appendix B, we discuss how to use chain-rule of IG to recursively decompose IG over a _sequence of actions_ under a policy.

Recall that a state \(s\) is the history of interactions, and let \(s^{\prime}=(s,a,o)\) be the subsequent state on enacting \(a\). The information gain of taking an action \(a\) and observing the new state \(s^{\prime}\) is the reduction in the uncertainty of the agent's belief over the solution \(Y\), i.e., \(\mathcal{I}_{t}(Y;s^{\prime})=\mathcal{H}_{t}(Y)-\mathcal{H}_{t}(Y|s^{\prime})\).

Computing \(\mathcal{I}_{t}(Y;s^{\prime})\) requires estimating entropies \(\mathcal{H}_{t}(Y)\) and \(\mathcal{H}_{t}(Y|s^{\prime})\), which in turn depend on the probability distributions \(\mathbb{P}_{t}(y)\) and \(\mathbb{P}_{t}(y|s^{\prime})\) for \(\forall y\in\mathcal{Y}\), respectively. In conventional RL methods, this requires updating the posterior given the new state \(s^{\prime}\). Specifically, recall \(\mathbb{P}_{t}(y|s^{\prime})=p(Y=y|s^{\prime},\mathcal{T}_{t},D)\), where \(\mathcal{T}_{t}\) is the tree explored till the iterate \(t\), and let \(\mathcal{T}_{t+1}=(\mathcal{T}_{t}\cup s^{\prime})\) be the tree for the next iterate after observing \(s^{\prime}\). Let \(\phi\in\Phi\) be the parameters of the environment model for the problem the agent is facing. Here, the posterior updates entail computing

\[p(y|s^{\prime},\mathcal{T}_{t},D)=p(y|\mathcal{T}_{t+1},D)=\int p(y|x_{0},\phi )p(\phi|\mathcal{T}_{t+1},D)\mathrm{d}\phi.\] (3)

Unfortunately, such posterior updates can be challenging when \(\phi\) is high-dimensional. The challenge is particularly exacerbated because posterior needs to be _repeatedly_ updated for \(\mathcal{T}_{t+k}\), where \(k\geq 1\), as new data is acquired, thereby making prior methods intractable beyond simple/linear settings.

In-context learning for posterior updates:To mitigate this challenge, we build upon the recent insights (Xie et al., 2021; Lee et al., 2023) that draw connections between in-context learning and Bayesian inference. In our setting, in-context learning provides a remarkably simple posterior update for \(p(y|\mathcal{T}_{t+1},D)\). Specifically, consider the model in Figure 1. Let \(\theta\) be the parameters of the general world model, and \(\phi\) be the model parameters for the specific problem agent is dealing with, and \(D\) is the internet data. We will make the following assumption,

**Assumption 1**.: \(\forall t,\mathcal{I}(\theta;\mathcal{T}_{t}|D)=0\)_._

Assumption 1 states that given the internet-scale data \(D\), a few (since \(t\) is usually small) problem specific interactions contained in \(\mathcal{T}_{t}\) do not provide any more information about the general world model \(\theta\). This implies \(p(\theta|\mathcal{T}_{t},D)=p(\theta|D)\). Under this viewpoint we can express \(p(y|\mathcal{T}_{t+1},D)\) as the following,

\[p(y|\mathcal{T}_{t+1},D) =\iint p(y,\phi,\theta|\mathcal{T}_{t+1},D)\mathrm{d}\theta \mathrm{d}\phi=\iint\biggl{(}\int p(y,\phi|\mathcal{T}_{t+1},\theta,D)\mathrm{ d}\phi\biggr{)}\,p(\theta|\mathcal{T}_{t+1},D)\mathrm{d}\theta\] (4) \[=\int p(y|\mathcal{T}_{t+1},\theta,D)p(\theta|\mathcal{T}_{t+1},D )\mathrm{d}\theta\stackrel{{(a)}}{{=}}\int p(y|\mathcal{T}_{t+1}, \theta)p(\theta|D)\mathrm{d}\theta,\] (5)

where (a) follows because of the model in Figure 1, and Assumption 1. Similarly, \(\mathbb{P}_{t}(y)=p(y|\mathcal{T}_{t},D)\) needed to compute \(\mathcal{H}_{t}(Y)\) can be factorized as \(p(y|\mathcal{T}_{t},D)=\int p(y|\mathcal{T}_{t},\theta)p(\theta|D)\mathrm{d}\theta\). In Appendix B we discuss how once the posterior \(\mathbb{P}_{t}(y|s^{\prime})\) and \(\mathbb{P}_{t}(y)\) are available, we can readily estimate the entropies \(\mathcal{H}_{t}(Y|s^{\prime})\) and \(\mathcal{H}_{t}(Y)\), and thus also the information gain \(\mathcal{I}_{t}(Y;s^{\prime})\).

Figure 1: Graphical model for the data generating process.

**Advantages:** Unlike \(p(\phi|\mathcal{T}_{t+1},D)\) in (3) that does an explicit update to obtain posterior over \(\phi\) for the underlying environment, in (5) the posterior is over the parameters of the world-model \(p(\theta|D)\) and thus do _not_ require an _explicit_ update when new data is acquired. Instead, the new data is used _in-context_\(p(y|\mathcal{T}_{t+1},\theta)\) to obtain the desired \(p(y|\mathcal{T}_{t+1},D)\). Not only this avoid any updates to the parameters \(\theta\), but also as new data becomes available then \(p(y|\mathcal{T}_{t+k},D)\) can also be computed readily for \(k\geq 1\). _This would not have been possible without the in-context learning ability of LLMs._

Further, (5) reduces the problem to standard uncertainty estimation for deep learning (Gawlikowski et al., 2021; Abdar et al., 2021). Perhaps the most popular technique is to use an ensemble of N models for a Monte-Carlo estimate of the integral in (5). For extremely large models, ensembles can be created using multiple-low rank adapter instead (Malinin and Gales, 2020; Kuhn et al., 2023). For simplicity, we use \(N=1\) in our experiments.

## 3 Empirical Analysis

Complete algorithm for the proposed IDTS method and more details about the experimental setup is provided in Appendix C. We run all the experiments using OpenAI GPT models (Achiam et al., 2023), and the results are presented in Figure 2. We compare the performances of the above methods on the following domains:

**Code (Python):** This is based on the HumanEval benchmark (Chen et al., 2021). We focus on the hard problems by filtering out the ones that can be solved by GPT-3.5-turbo in one-shot. Here, an action corresponds to the entire solution. The feedback consists of results from the synthetically generated (and thus potentially incorrect) unit-tests, error messages from the compiler, and self-critique of the solution given the unit-test results and error messages.

**Math (Lean):** We use the MiniF2F benchmark (Zheng et al., 2021) for this task. Each action corresponds to an entire formal proof, and the feedback consists of the error messages from the Lean (Moura and Ullrich, 2021) compiler along with the self-critique of the generated solution.

**LLF-Bench: (Cheng et al., 2023)** This task requires recommending movies to a user whose interests are hidden from the agent. After every suggestion, the domain provides natural language hint regarding how close the recommended movies are to the user's interests. In the feedback, we also incorporate self-critique of the solution based on the response received.

While IDTS shows some promise on coding and LLF-bench, the gains over MCTS are not significant at the moment to justify additional complexity associated with IDTS. Math domain stands out, as neither the error messages from Lean compiler had corrective feedback, nor the self-critique provided any meaningful feedback. As such, just ignoring any feedback and sampling diverse solutions emerged to be the best strategy in this domain.

Figure 2: We compare the following baselines across domains: **MCTS:** standard MCTS with UCT style bonus, **Iterative Refinement:** This is sequential interaction (equivalent to MCTS with branching factor=1), **Sample and select:** This samples k solutions at the root node, and effectively disregards any feedback (equivalent to MCTS with tree depth=1), **IDTS:** This is the proposed algorithm, that builds on MCTS but uses information gain to drive exploration as opposed to the UCT style bonus. The maximum number of nodes expanded is 10. For MCTS and IDTS, the branching factor is 4.

Discussion and Future Work

While in principle, having access to the true IG should increase the efficiency of the search significantly, it is not feasible at the LLM scale to obtain the true IG. IDTS avoids (repeated) posterior updates using in-context learning, but still requires useful uncertainty measure for LLMs (Malinin and Gales, 2020; Quach et al., 2023). An important direction for future work is to have a deeper study on the estimation error in the IG computation, as finding other better methods of IG estimation is required before using IG is likely to be helpful with LLMs.

Note, however, that search using IG only requires deciding which node to explore. As such, it might only be needed that the _relative_ (not absolute) values of the IG across nodes are accurate. While this could potentially mitigate the IG estimation challenge, assessing the quality of ranking also remains challenging as the true rankings are unknown. Further, expanding \(\gg>10\) nodes per tree would provide invaluable insights on the utility of different exploration strategies.

## References

* Abdar et al. (2021) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. _Information fusion_, 76:243-297, 2021. [Page(s): 4]
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Alteschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023. [Page(s): 4]
* Arumugam and Van Roy (2021) Dilip Arumugam and Benjamin Van Roy. Deciding what to learn: A rate-distortion approach. In _International Conference on Machine Learning_, pages 373-382. PMLR, 2021a. [Page(s): 9]
* Arumugam and Van Roy (2021b) Dilip Arumugam and Benjamin Van Roy. The value of information when deciding what to learn. _Advances in Neural Information Processing Systems_, 34:9816-9827, 2021b. [Page(s): 9]
* Aygun et al. (2022) Eser Aygun, Ankit Anand, Laurent Orseau, Xavier Glorot, Stephen M Mcaleer, Vlad Firoiu, Lei M Zhang, Doina Precup, and Shibl Mourad. Proving theorems using incremental learning and hindsight experience replay. In _International Conference on Machine Learning_, pages 1198-1210. PMLR, 2022. [Page(s): 9]
* Biyik et al. (2023) Erdem Biyik, Fan Yao, Yinlam Chow, Alex Haig, Chih-wei Hsu, Mohammad Ghavamzadeh, and Craig Boutilier. Preference elicitation with soft attributes in interactive recommendation. _arXiv preprint arXiv:2311.02085_, 2023. [Page(s): 9]
* Boutilier (2002) Craig Boutilier. A pomdp formulation of preference elicitation problems. In _AAAI/IAAI_, pages 239-246. Edmonton, AB, 2002. [Page(s): 9]
* Boutilier (2013) Craig Boutilier. Computational decision support: Regret-based models for optimization and preference elicitation, 2013. [Page(s): 9]
* Chajewska et al. (2000) Urszula Chajewska, Daphne Koller, and Ronald Parr. Making rational decisions using adaptive utility elicitation. In _Aaai/Iaai_, pages 363-369, 2000. [Page(s): 9]
* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Helay Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Aleleau Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [Page(s): 4]Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language feedback. _arXiv preprint arXiv:2312.06853_, 2023.
* [Paeg(s): 4]
* Coquelin and Munos [2007] Pierre-Arnaud Coquelin and Remi Munos. Bandit algorithms for tree search. _arXiv preprint cs/0703062_, 2007.
* Das et al. [2024] Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Provably sample efficient rlhf via active preference optimization. _arXiv preprint arXiv:2402.10500_, 2024.
* First et al. [2023] Emily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: whole-proof generation and repair with large language models. _arXiv preprint arXiv:2303.04910_, 2023.
* Freedman et al. [2023] Rachel Freedman, Justin Svegliato, Kyle Wray, and Stuart Russell. Active teacher selection for reinforcement learning from human feedback. _arXiv preprint arXiv:2310.15288_, 2023.
* Gawlikowski et al. [2021] Jakob Gawlikowski, Cedrique Rovile Nijeutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. _arXiv preprint arXiv:2107.03342_, 2021.
* Hao and Lattimore [2022] Botao Hao and Tor Lattimore. Regret bounds for information-directed reinforcement learning. _Advances in Neural Information Processing Systems_, 35:28575-28587, 2022.
* Hao et al. [2022] Botao Hao, Tor Lattimore, and Chao Qin. Contextual information-directed sampling. In _International Conference on Machine Learning_, pages 8446-8464. PMLR, 2022.
* Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. _arXiv preprint arXiv:2305.14992_, 2023.
* Jang et al. [2020] Youngsoo Jang, Seokin Seo, Jongmin Lee, and Kee-Eung Kim. Monte-carlo planning and learning with language action value estimates. In _International Conference on Learning Representations_, 2020.
* Ji et al. [2024] Kaixuan Ji, Jiafan He, and Quanquan Gu. Reinforcement learning from human feedback with active queries. _arXiv preprint arXiv:2402.09401_, 2024.
* Kirschner and Krause [2018] Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_, pages 358-384. PMLR, 2018.
* Kocsis and Szepesvari [2006] Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In _European conference on machine learning_, pages 282-293. Springer, 2006.
* Kuhn et al. [2023] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. _arXiv preprint arXiv:2302.09664_, 2023.
* Lakshminarayanan et al. [2017] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* Lample et al. [2022] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. _Advances in Neural Information Processing Systems_, 35:26337-26349, 2022.
* Lee et al. [2023] Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. _arXiv preprint arXiv:2306.14892_, 2023.
* Lieck et al. [2017] Robert Lieck, Vien Ngo, and Marc Toussaint. Exploiting variance information in monte-carlo tree search. _HSDIP 2017_, page 26, 2017.
* Limperg and From [2023] Jannis Limperg and Asta Halkjaer From. Aesop: White-box best-first proof search for lean. In _Proceedings of the 12th ACM SIGPLAN International Conference on Certified Programs and Proofs_, pages 253-266, 2023.
* Liu et al. [2021]Xinyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen, et al. Reinforcement learning, bit by bit. _Foundations and Trends(r) in Machine Learning_, 16(6):733-865, 2023.
* Malinin and Gales [2020] Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. _arXiv preprint arXiv:2002.07650_, 2020.
* Margatina et al. [2023] Katerina Margatina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. Active learning principles for in-context learning with large language models. _arXiv preprint arXiv:2305.14264_, 2023.
* Mehta et al. [2021] Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, and Willie Neiswanger. An experimental design perspective on model-based reinforcement learning. _arXiv preprint arXiv:2112.05244_, 2021.
* Mehta et al. [2022] Viraj Mehta, Ian Char, Joseph Abbate, Rory Conlin, Mark Boyer, Stefano Ermon, Jeff Schneider, and Willie Neiswanger. Exploration via planning for information about the optimal trajectory. _Advances in Neural Information Processing Systems_, 35:28761-28775, 2022.
* Mehta et al. [2023] Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. 2023.
* de Moura and Ullrich [2021] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In _Automated Deduction-CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 12-15, 2021, Proceedings 28_, pages 625-635. Springer, 2021.
* Nikolov et al. [2018] Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-directed exploration for deep reinforcement learning. _arXiv preprint arXiv:1812.07544_, 2018.
* OpenAI [2023] OpenAI. New models and developer products announced at devday, 2023. https://openai.com/blog/new-models-and-developer-products-announced-at-devday.
* Osband et al. [2021] Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, and Benjamin Van Roy. Epistemic neural networks. _arXiv preprint arXiv:2107.08924_, 2021.
* Parascandolo et al. [2020] Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B Hamrick, Nicolas Heess, Alexander Neitz, and Theophane Weber. Divide-and-conquer monte carlo tree search for goal-directed planning. _arXiv preprint arXiv:2004.11410_, 2020.
* Polu et al. [2022] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. _arXiv preprint arXiv:2202.01344_, 2022.
* Quach et al. [2023] Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S Jaakkola, and Regina Barzilay. Conformal language modeling. _arXiv preprint arXiv:2306.10193_, 2023.
* Rainforth et al. [2024] Tom Rainforth, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. Modern bayesian experimental design. _Statistical Science_, 39(1):100-114, 2024.
* Russo and Van Roy [2014] Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. _Advances in Neural Information Processing Systems_, 27, 2014.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Wang et al. [2023a] Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, et al. Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 12632-12646, 2023a.
* Wang et al. [2023b]Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. Hypothesis search: Inductive reasoning with language models. _arXiv preprint arXiv:2309.05660_, 2023b. [Page(s): 9]
* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022. [Page(s): 9]
* Xie et al. [2021] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021. [Page(s): 3]
* Xin et al. [2023] Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. Lego-prover: Neural theorem proving with growing libraries. _arXiv preprint arXiv:2310.00656_, 2023. [Page(s): 9]
* Yang et al. [2023] Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. Leandoo: Theorem proving with retrieval-augmented language models. _arXiv preprint arXiv:2306.15626_, 2023. [Page(s): 9]
* Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023. [Page(s): 9]
* Zanette and Sarkar [2017] Andrea Zanette and Rahul Sarkar. Information directed reinforcement learning. _Tech. Rep., Technical report_, 2017. [Page(s): 9]
* Zelikman et al. [2022] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A unified natural language framework for algorithmic reasoning. _arXiv preprint arXiv:2212.10561_, 2022. [Page(s): 1, 9]
* Zelikman et al. [2023] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively self-improving code generation. _arXiv preprint arXiv:2310.02304_, 2023. [Page(s): 9]
* Zhang et al. [2023a] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. Planning with large language models for code generation. _arXiv preprint arXiv:2303.05510_, 2023a. [Page(s): 1, 9]
* Zhang et al. [2023b] Zheyu Zhang, Zhuorui Ye, Yikang Shen, and Chuang Gan. Autonomous tree-search ability of large language models. _arXiv preprint arXiv:2310.10686_, 2023b. [Page(s): 9]
* Zheng et al. [2023b] Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated theorem proving. _arXiv preprint arXiv:2309.15806_, 2023b. [Page(s): 1, 9]
* Zheng et al. [2021] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. _arXiv preprint arXiv:2109.00110_, 2021. [Page(s): 4]
* Zhou et al. [2023] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. _arXiv preprint arXiv:2310.04406_, 2023. [Page(s): 1, 9]
* Zhu et al. [2022] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problem via cooperative reasoning induced language models. _arXiv preprint arXiv:2210.16257_, 2022. [Page(s): 9]
* Zhuang et al. [2023] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search. _arXiv preprint arXiv:2310.13227_, 2023. [Page(s): 9]Information Directed Tree Search

(Appendix)

## Appendix A Related Work

### LLM + MCTS based Reasoning and Planning

Chain of thought reasoning (Wei et al., 2022) and self improvement (Zelikman et al., 2023) have shown a lot of success in the past. Building on the success of these tree of thought (Yao et al., 2023) maintains a diverse set of solutions. Alternatively, planning via tree search based algorithms have also been studied (Hao et al., 2023; Zhou et al., 2023). Extensions of \(A^{*}\) search (Zhuang et al., 2023), or even in-context tree traversal (Zhang et al., 2023b), and hierarchical search (Parascandolo et al., 2020; Zelikman et al., 2022; Wang et al., 2023b) have also been explored.

Several methods have considered solving formal math problems using different variants of MCTS and best-first search (Yang et al., 2023; Limperg and From, 2023; Wang et al., 2023a; Lample et al., 2022; Zhu et al., 2022), doing proof repair using feedback (akin to iterative-refinement) (Zheng et al., 2023; First et al., 2023), by hierarchical decomposition (Xin et al., 2023), or incremental learning (Aygun et al., 2022; Polu et al., 2022). MCTS for language based RL games (Jang et al., 2020) and code-generation (Zhang et al., 2023a) has also been considered.

The approaches developed in these are largely complementary to ours, but none of them focus on the topic of how to explore more efficiently in the presence of partially correct feedback.

### Information Directed Sampling

Building upon the initial work on IDS (Russo and Van Roy, 2014), the core idea has been further expounded by several (Lu et al., 2023; Hao et al., 2022; Hao and Lattimore, 2022) with a focus on theoretical aspects of the regret associated with various instances of the IDS idea. On the practical side, Zanette and Sarkar (2017) considered the tabular setting and aimed at a tractable approximation of the information ratio using a variance based bound for IG. From one perspective, if we could have replaced the information gain term in the proposed IDTS method with a variance based bound, then our method would come closer to using Bernstein based bonuses for tree search (Lieck et al., 2017).

Kirschner and Krause (2018) discuss how in Gaussian setting, IG based on Bayesian posterior distribution can be expressed in frequentist terms. Nikolov et al. (2018) built upon this to consider the generic/deepRL setting and estimate both the aleatoric (using distributional RL) and epistemic uncertainty (using a q-ensemble). IDS has also been used for doing open/closed loop planning by changing the'reward' function to consider the information gained about the optimal trajectory by running the desired sequence of actions Mehta et al. (2021, 2022). While related, none of them tackle the LLM setting.

Our work is also related to works that discuss what information to gather (Arumugam and Van Roy, 2021), and how the acquisition of it can be made faster (Arumugam and Van Roy, 2021). Similarly, it also related to the work on reformulating POMDPs for optimizing long term value of information when eliciting preferences (Boutilier, 2002) and query optimization (Chajewska et al., 2000; Boutilier, 2013; Biyik et al., 2023). Several recent works also consider the task of strategically collecting labels for which pair of outputs to query in order to minimize the number of samples need for RLHF training (Mehta et al., 2023; Freedman et al., 2023; Ji et al., 2024; Das et al., 2024) or for in-context learning (Margatina et al., 2023). Our work complements this direction and considers strategic interaction at the inference time to find a solution to given question, and involves no RLHF.

## Appendix B Information Gain Estimation

### Preliminaries

Let \(X\) and \(Y\) be two random variables. Recall, from the chain rule of entropy:

\[\mathcal{H}(X,Y)=\mathcal{H}(X)+\mathcal{H}(Y|X)\] (Chain rule of entropy) (6)Similarly, information gain between random variables \(\{X_{1},X_{2}\}\) and \(Y\) can be written as,

\[\mathcal{I}(Y;X_{1},X_{2}) =\mathcal{H}(Y)-\mathcal{H}(Y|X_{1},X_{2})\] (7) \[=\mathcal{H}(Y)-\mathcal{H}(Y|X_{1})+\mathcal{H}(Y|X_{1})- \mathcal{H}(Y|X_{1},X_{2})\] (8) \[=\mathcal{I}(Y;X_{1})+\mathcal{I}(Y;X_{2}|X_{1}).\] (Chain rule of information gain) (9)

### Intra-interaction decomposition:

Recall that \(p(y|\mathcal{T}_{t+1},D)\) from (5) was required to compute the information gain \(\mathcal{I}_{t}(Y;s^{\prime})\),

\[p(y|\mathcal{T}_{t+1},D)=\int p(y|\mathcal{T}_{t+1},\theta)p(\theta|D)\mathrm{ d}\theta,\] (10)

Here, the random variable \(Y\) that characterizes the agent's belief over the solution is a sequence of token \((Y_{0},Y_{1},\ldots)\) that is generated auto-regressively. This structure can be leveraged to specify an estimator that decomposes the information gain at per token level,

\[\hat{\mathcal{I}}_{t}(Y;s^{\prime})\coloneqq-\sum_{i=1}^{K_{2}}\log\mathbb{P} _{t}(Y_{i}|Y_{i-1})+\sum_{i=1}^{K_{1}}\log\mathbb{P}_{t}(Y_{i}|Y_{:i-1},s^{ \prime}),\] (11)

where \(\mathbb{P}_{t}(Y_{i}|Y_{Y_{i-1}})\) is a random variable because conditioning is on a random variable as well. Further, notice that the two sequence of \(Y_{i}\)'s are different, as one is conditioned on \(s^{\prime}\) and the other is not. \(K_{1}\) and \(K_{2}\) are the lengths of the generated sentences.

Estimator \(\hat{\mathcal{I}}_{t}(Y;s^{\prime})\) decomposes information gain such that its computation requires logprobs of the generated tokens, which is accessibly even from proprietary large-language models like GPT [15]. However, when using open-source models, notice that the agent has additional access to the logprobs of the tokens _not_ sampled. These can be leveraged to create the following improved estimator that can have lower variance than \(\hat{\mathcal{I}}_{t}(Y;s^{\prime})\), but still provides an unbiased estimator of \(\mathcal{I}_{t}(Y;s^{\prime})\).

\[\widetilde{\mathcal{I}}_{t}(Y;s^{\prime})\coloneqq\sum_{i=1}^{K_{2}}\mathcal{ H}_{t}(Y_{i}|Y_{:i-1})-\sum_{i=1}^{K_{1}}\mathcal{H}_{t}(Y_{i}|Y_{:i-1},s^{ \prime}).\] (12)

It can be observed using chain-rule of entropy and tower law of expectations that,

\[\mathbb{E}\Big{[}\widetilde{\mathcal{I}}_{t}(Y;s^{\prime})\Big{]}=\mathbb{E} \Big{[}\hat{\mathcal{I}}_{t}(Y;s^{\prime})\Big{]}=\mathcal{I}_{t}(Y;s^{ \prime}).\] (13)

### Inter-interaction decomposition:

let \(\pi\) be a tree-traversal policy that operates on the tree \(\mathcal{T}_{t}\) (This policy has a composite nature, where it either selects one of the already expanded branches, or draws a new sample to create a new branch in the tree). For any given node \(s\), let \(\tau^{\pi}(s)\) be a random trajectory that could be unrolled using \(\pi\) when starting at node \(s\). Let \(\mathcal{L}_{t}(s)\) be the set of leaves under the _subtree_ of \(s\) in \(\mathcal{T}_{t}\). Let \(\mathcal{C}_{t}(s)\) be the set of immediate children of \(s\), and \(\mathcal{A}_{t}(s)\) be all the ancestors of \(s\) in \(\mathcal{T}_{t}\).

In the following Theorem 1, we formalize a procedure to recursively estimate the information gain on expanding the sub-tree of a given node \(s\). This has an intuitive form that asserts that the information gain from expanding the subtree of a node \(s\) equals the expected information gain from expanding the children of node \(s\).

**Theorem 1**.: _The information gain from \(\tau_{t}(s)\) can be expressed as the following recursive form,_

\[\mathcal{I}_{t}(Y;\tau^{\pi}(s))=\sum_{s^{\prime}\in\mathcal{C}_{t}(s)}\mathbb{ P}_{t}(s^{\prime}|s;\pi)\,\mathcal{I}_{t}(Y;\tau^{\pi}(s^{\prime}))=\sum_{x\in \mathcal{L}_{t}(s)}\mathbb{P}_{t}(x|s;\pi)\,\mathcal{I}_{t}(Y;\tau^{\pi}(x)).\] (14)

Proof.: Consider the following decomposition \(\tau(s)=\{s\cup\tau_{2}\}\), where \(\tau_{2}\) is the part of the trajectory \(\tau(s)\) without node \(s\). Note that the first node in \(\tau_{2}\) will be a \(s^{\prime}\in\mathcal{C}_{t}(s)\).

\[\mathcal{I}_{t}(Y;\tau(s))=\mathcal{H}_{t}(Y)-\mathcal{H}_{t}(Y|\tau(s))\] (15)Considering the term in red in 15,

\[\mathcal{H}_{t}(Y|\tau(s)) =\mathcal{H}_{t}(Y|s,\tau_{2})\] (16) \[=\sum_{x,\tau}\mathbb{P}_{t}(s=x)\,\mathbb{P}_{t}(\tau_{2}=\tau|s=x )\,\mathcal{H}_{t}(Y|x,\tau)\] (17) \[\overset{(a)}{=}\sum_{\tau}\mathbb{P}_{t}(\tau_{2}=\tau|s)\, \mathcal{H}_{t}(Y|s,\tau)\] (18) \[=\sum_{s^{\prime}\in\mathcal{C}(s)}\mathbb{P}_{t}(s^{\prime}|s) \sum_{\tau}\mathbb{P}_{t}(\tau_{2}=\tau|s,s^{\prime})\,\mathcal{H}_{t}(Y|s,\tau)\] (19) \[\overset{(b)}{=}\sum_{s^{\prime}\in\mathcal{C}(s)}\mathbb{P}_{t }(s^{\prime}|s)\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau)\,\mathcal{H}_{ t}(Y|s,\tau)\] (20) \[=\sum_{s^{\prime}\in\mathcal{C}(s)}\mathbb{P}_{t}(s^{\prime}|s) \mathcal{H}_{t}(Y|s,\tau(s^{\prime}))\] (21)

where \((a)\) follows as \(s\) is the first node in the trajectory \(\tau(s)\) deterministically. \((b)\) follows as conditioned on \(s^{\prime}\), \(\tau_{2}\) does not depend on \(s\). Now, focusing on the blue term in 21,

\[\mathcal{H}_{t}(Y|s,\tau(s^{\prime})) =-\sum_{x,\tau}\mathbb{P}_{t}(s=x)\,\mathbb{P}_{t}(\tau(s^{\prime })=\tau|s=x)\sum_{y}\mathbb{P}_{t}(y|x,\tau)\log\mathbb{P}_{t}(y|x,\tau)\] (22) \[\overset{(a)}{=}-\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau )\sum_{y}\mathbb{P}_{t}(y|s,\tau)\log\mathbb{P}_{t}(y|s,\tau)\] (23) \[=-\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau)\sum_{y}\mathbb{ P}(y|s,\tau,\mathcal{T}_{t})\log\mathbb{P}(y|s,\tau,\mathcal{T}_{t})\] (24) \[\overset{(b)}{=}-\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau )\sum_{y}\mathbb{P}(y|\tau,\mathcal{T}_{t})\log\mathbb{P}(y|\tau,\mathcal{T}_{ t})\] (25) \[=-\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau)\sum_{y}\mathbb{ P}_{t}(y|\tau)\log\mathbb{P}_{t}(y|\tau)\] (26) \[=-\sum_{\tau}\mathbb{P}_{t}(\tau(s^{\prime})=\tau)\,\mathcal{H}_ {t}(Y|\tau)\] (27) \[=\mathcal{H}_{t}(Y|\tau(s^{\prime}))\] (28)

where \((a)\) follows because \(s\) and \(s^{\prime}\) are fixed variables in the LHS, and the trajectory \(\tau(s^{\prime})\) does not depend on ancestor node \(s\) of \(s^{\prime}\). \((b)\) follows because by construct ancestor \(s\) is a part of \(\mathcal{T}_{t}\), i.e., \(s\in\mathcal{T}_{t}\) already. Therefore, combining 15, 21, and 28,

\[\mathcal{I}_{t}(Y;\tau(s)) =\mathcal{H}_{t}(Y)-\sum_{s^{\prime}\in\mathcal{C}_{t}(s)}\mathbb{ P}_{t}(s^{\prime}|s)\,\mathcal{H}_{t}(Y|\tau(s^{\prime}))\] \[=\sum_{s^{\prime}\in\mathcal{C}_{t}(s)}\mathbb{P}_{t}(s^{\prime}| s)[\mathcal{H}_{t}(Y)-\mathcal{H}_{t}(Y|\tau(s^{\prime}))]\] \[=\sum_{s^{\prime}\in\mathcal{C}_{t}(s)}\mathbb{P}_{t}(s^{\prime}| s)\,\mathcal{I}_{t}(Y;\tau_{t}(s^{\prime})).\]

Now, unrolling \(\mathcal{I}_{t}(Y;\tau_{t}(s^{\prime}))\) using the above recursion and observing that for a leaf node \(x\in\mathcal{L}_{t}(s)\), \(\tau(x)=x\), we obtain the aforementioned result.

Contrast this recursive propagation of information gain, with recursive propagation of the (terminal) reward. Information gain from intermediate states in a trajectory is zero, similar to intermediate rewards for many games. This permits minimal modification of existing MCTS methods to incorporate rich feedback and perform an information-directed exploration.

### Information Gain Bellman Recursion

Let \(\mathscr{I}_{t}^{\pi}(s)\) be the information gained towards the optimal solution \(y\) on unrolling a trajectory \(\tau^{\pi}(s)\), given the agent is at state \(s\) during iteration \(t\). Also, recall that the sub-script of \(t\) denotes implicit conditioning on all the data observed so far (e.g., in the code-gen setting, it denotes conditioning on all the internet data that has been used for model training),

\[\mathscr{I}_{t}^{\pi}(s)\coloneqq\mathcal{I}_{t}(Y;\tau^{\pi}(s)|s).\] (29)

One should consider \(\mathscr{I}_{t}^{\pi}(h)\) analogous to the state-value function for a policy \(\pi\), but instead of being for the long-term return to go, it is for the long-term information gain. Similar to the state-value function, it is also possible to define a Bellman recursion for the information gain (recall that by construct, a state is defined using the history of the trajectory).

**Theorem 2**.: _Information gain recursion:_

\[\mathscr{I}_{t}^{\pi}(s)=\mathcal{I}_{t}(Y;S^{\prime}|s)+\mathbb{E}_{\pi}[ \mathscr{I}_{t}^{\pi}(S^{\prime})|s],\] (30)

_where \(S^{\prime}\) is observed following the policy \(\pi\) from state \(s\)._

**Corollary 1**.: \(n\)_-step Information gain backup_

\[\mathscr{I}_{t}^{\pi}(s)=\mathbb{E}_{\pi}\biggl{[}\sum_{i=0}^{n-1}\mathcal{I} _{t}(Y;S_{i+1}|S_{i})+\mathscr{I}_{t}^{\pi}(S_{n})|s\biggr{]},\] (31)

_where \(S_{0}=s\)._

Proof.: Follows by unrolling the recursion in (30). 

Proof.: Proof of Theorem \(2\). Without loss of generality, consider \(s=(O_{0})\), i.e., just the starting state, and consider the length of a trajectory to be \(L\),

\[\mathscr{I}_{t}^{\pi}(s) =\mathcal{I}_{t}(Y;\tau^{\pi}(s)|s)\] (32) \[=\mathcal{I}_{t}(Y;(A_{0},O_{1},A_{1},\dots,O_{L})|s)\] (33) \[=\mathcal{I}_{t}(Y;S^{\prime}|s)+\mathcal{I}_{t}(Y;(A_{1},\dots, O_{L})|s,S^{\prime}) \because\text{Chain rule of info gain}\] (34) \[=\mathcal{I}_{t}(Y;S^{\prime}|s)+\mathcal{I}_{t}(Y;(A_{1},\dots, O_{L})|S^{\prime}) \because\text{$S^{\prime}=s\cup(A_{0},O_{1})$}\] (35) \[=\mathcal{I}_{t}(Y;S^{\prime}|s)+\sum\mathbb{P}_{t}(s^{\prime}|s; \pi)\mathcal{I}_{t}(Y;\tau^{\pi}(s^{\prime})|s^{\prime})\] (36) \[=\mathcal{I}_{t}(Y;S^{\prime}|s)+\sum\mathbb{P}_{t}(s^{\prime}|s; \pi)\mathscr{I}_{t}^{\pi}(s^{\prime}) \because\text{By definition of }\mathscr{I}_{t}^{\pi}(s^{\prime})\] (37) \[=\mathcal{I}_{t}(Y;S^{\prime}|s)+\mathbb{E}_{\pi}[\mathscr{I}_{t }^{\pi}(S^{\prime})|s].\] (38)

**Remark 1**.: _Note that as a'state' is the history of everything that has occured so far, this makes the decision process resemble an (acyclic) tree, where each'state' can only be reached through a unique path, and can't be revisited again. This is good, as it is aligned with the problems we aim to resolve. But this may be not so good, because it probably makes learning \(\mathscr{I}(\cdot)\) much harder._

**Remark 2**.: _During test-time inference, we need a value function that estimates \(\mathscr{I}_{t}^{\pi}(h)\) so as to perform \(n\)-step information backup. But how should we learn \(\mathscr{I}_{t}^{\pi}(h)\)? Here it is important to distinguish the implicit conditioning on (the entire past training data) through the subscript \(t\), and the explicit conditioning (i.e., in-context) on the observations made in the current interaction. Therefore, this will require an 'intermediate' phase where we will need pairs of \((s,\hat{\mathcal{I}}_{t}^{\pi}(Y;\tau^{\pi}(s)|s))\) from a held-out dataset to estimate \(\mathscr{I}_{t}^{\pi}(s)\), which can then be used during the test time to do (\(n\)-step) backup. The important thing to note here is that the data from the 'intermediate' phase should not be used, ideally, to improve the language model again, as that would change the meaning of the implicit-conditioning on \(t\). For example, if \(\mathcal{I}_{t}^{\pi}(Y;\tau^{\pi}(s)|s)\) had a non-zero value and then we use \(s\) to update the language model again, and if we assume everything to be perfect, then \(\mathcal{I}_{t+1}^{\pi}(Y;\tau^{\pi}(s)|s)\) should be 0 as there is no new information to be gained from \(s\)._

Due to the above challenges in estimating long-term information gain \(\mathscr{I}_{t}^{\pi}(s)\), we resort to myopic version, where only one-step information gain backup is used, and \(\mathscr{I}_{t}^{\pi}(s)\) is set to 0. This is akin to how even UCT does not take into account uncertainty of \(Q\), it still does one-step/bandit uncertainty.

Empirical Details

In Algorithm 1 we present the pseudo-code for IDTS. The steps highlighted in blue are the key differences from the typical tree-search/UCT.

For different domains, we defined \(\texttt{Value}(s,\tau^{\pi}(s))\) differently. For coding domain, this corresponded to the the fraction of unit test passed by the latest solution available in \(\tau^{\pi}(s)\). For LLF-bench, this comprised of the scalar feedback form the environment for how relevant is the latest recommendation to the movie that the user had in mind. For math domain, we set \(\texttt{Value}(s,\tau^{\pi}(s))=0\), and treat the task as a pure exploration problem.

For all the domains, we set \(n=1\) for rollout in the Evaluate function. Therefore, \(\tau^{\pi}(s)=s^{\prime}\). With this setting \(\hat{\mathcal{I}}(Y;\tau^{\pi}(s)|\mathcal{T},D)\), where in the algorithm \(\mathcal{T}\) represents the current tree, can be equivalently expressed as \(\hat{\mathcal{I}}_{t}(Y;s^{\prime})\), where \(t\) is the current iterate.

Results for the code, and math domain used gpt-3.5-turbo-0125, and for the LLF-bench gpt-4-turbo-2024-04-09 was used.

```
1FunctionIDTSearch(\(s_{0}\)):
2Create tree \(\mathcal{T}\) with root state \(s_{0}\)whilewithin compute budgetdo
3\(s\leftarrow\texttt{TreePolicy}(s_{0})\)\(v,i\leftarrow\texttt{Evaluate}(s)\)\(\texttt{Backup}(s,v,i)\)
4return\(\underset{s\in\mathcal{T}}{\arg\max}\,V(s)\)
5FunctionTreePolicy(\(s\)):
6while\(s\) is non-terminaldo
7if\(s\) is not \(K\)-expandedthen
8returnExpand(\(s\))
9else
10\(s\leftarrow\texttt{BestChild}(s)\)
11return\(s\)
12
13FunctionBestChild(\(s\)):
14return\(\underset{s^{\prime}\in\mathcal{C}_{t}(s)}{\arg\max}\,V(s^{\prime})+ \lambda I(s^{\prime})\) ```

**Algorithm 1**The IDTS algorithm

**Practical Approximations:** Recall from 5 that we need to estimate \(p(y|s^{\prime},\mathcal{T}_{t},D)\). This corresponds to epistemic uncertainty for the LLM. Several methods exists for estimating this [Lakshminarayanan et al., 2017, Osband et al., 2021], we make use of the ensemble approach where

\[p(y|s^{\prime},\mathcal{T}_{t},D)=\int p(y|s^{\prime},\mathcal{T}_{t},\theta)p (\theta|D)\mathrm{d}\theta\approx\frac{1}{M}\sum_{i=1}^{M}p(y|s^{\prime}, \mathcal{T}_{t},\theta_{i}).\] (39)

For extremely large models, ensembles can be created using multiple-low rank adapter instead [Malinin and Gales, 2020, Kuhn et al., 2023]. For our experiments, we simply use \(M=1\), such that \(p(y|s^{\prime},\mathcal{T}_{t},D)\approx p(y|s^{\prime},\mathcal{T}_{t},\theta)\). Similarly for \(p(y|\mathcal{T}_{t},D)\approx p(y|\mathcal{T}_{t},\theta)\). Computing an estimate information gain \(\mathcal{I}_{t}(Y;s^{\prime})\) now simply corresponds to estimating the difference in entropy of \(p(y|s,\mathcal{T}_{t},\theta)\) and \(p(y|\mathcal{T}_{t},\theta)\).

Further, conditioning on the information of the entire tree \(\mathcal{T}_{t}\) might require very large context window for the LLMs. To avoid such long context, instead of conditioning on the content of all the nodes in \(\mathcal{T}_{t}\), we condition only on the ancestor nodes of \(s^{\prime}\). Finally, we also note that that the estimator in (11) can result in negative values because of the sampling error. To avoid this, we clip the minimum value of the information gain to be 0.