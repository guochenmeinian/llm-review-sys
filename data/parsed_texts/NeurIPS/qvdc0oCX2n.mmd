# CLIPLoss and Norm-Based Data Selection Methods

for Multimodal Contrastive Learning

 Yiping Wang

University of Washington

&Yifang Chen

University of Washington

&Wendan Yan

University of Washington

&Alex Fang

University of Washington

&Wenjing Zhou

University of Michigan

&Kevin Jamieson

University of Washington

&Simon Shaolei Du

University of Washington

Equal contribution. Correspondence to ypwang61@cs.washington.edu. Codes are available at https://github.com/ypwang61/negCLIPLoss_NormSim.

###### Abstract

Data selection has emerged as a core issue for large-scale visual-language model pretraining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric). While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce **negCLIPLoss**, a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, **NormSim**, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp [1]. Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3% improvement on ImageNet-1k and a 2.8% improvement on 38 downstream evaluation tasks. Moreover, both **negCLIPLoss** and **NormSim** are compatible with existing techniques. By combining our methods with the current best methods DFN [2] and HYPE [3], we can boost average performance on downstream tasks by 0.9%, achieving a new state-of-the-art on the DataComp-medium benchmark2.

Footnote 2: DataComp benchmark: https://www.datacomp.ai/dcclip/leaderboard.html.

## 1 Introduction

Curating large-scale visual-language datasets from web-sourced data has become common for pretraining multi-modal models. However, the quality of these web-curated data pairs remains a critical bottleneck. Research has shown that the choice of dataset significantly impacts model performance, irrespective of the models and training techniques employed [4, 5, 6, 7, 8, 9, 10, 11], and this motivatesthe development of various data selection strategies. This paper focuses on optimizing subset selection from a fixed data pool to train a CLIP model [4] that achieves superior performance on zero-shot downstream tasks.

Classical methods _rely solely on OpenAI's (OAI) pretrained CLIP model_ (i.e., a teacher model) and focus on better utilizing the embeddings. The most commonly used one is calculating CLIPScore, which measures the cosine similarity between the visual and language embeddings of the CLIP model for the same sample, to eliminate low-quality data with mismatches between text and image. Other works also leverage heuristic distribution alignment techniques to select samples relevant to downstream tasks, such as image-based filtering [1]. These approaches are generally viewed as providing only limited enhancements. However, we argue that the potential of those embeddings has been heavily under-explored. This work seeks a universal method to better employ any given embeddings, not only from OAI CLIP, but also from other CLIP-style models.

On the other hand, recent leading data filtering methods, instead of focusing on improving embedding utilization stagey itself, mainly follow the other two directions, both employing external resources. They either (1) use _external non-CLIP models_ that aid in data selection, (2) or use _external high-quality multi-modal data_ to train a _better CLIP-style embedding model_ than the original OAI CLIP to filter out low-quality data. Specifically, in the first line of works, HYPE [3] leverages embeddings from hyperbolic models instead of the classical Euclidean-based CLIP to measure how each data point has semantically overlaps with other data points and filters out data with low specificity. T-MARS [12] removes images where the text is the only feature correlated with the caption using FAST [13], an off-the-shelf OCR text detection model. Devil [14] applies fasttext [15] to remove non-English texts and use BLIP-2 [16] model for digit recognition to keep useful images with digits. The second direction, represented by Data Filtering Network (DFN) [2], involves training a new CLIP-style teacher model that uses high-quality datasets like HQITP-350M. Although the embeddings extracted from this model perform worse than the OAI CLIP in downstream tasks, it is particularly good at filtering out low-quality data. Notably, some of these methods can be combined and indeed, merging the selected data from DFN and HYPE achieves current state-of-art as shown in HYPE [3].

Previous works mainly focus on improving the CLIP embedding quality or utilizing an external model to do filtering but employ the CLIP embedding in a suboptimal way by only using classical methods like CLIPScore. In contrast, in this work, we focus on improving the filtering methods themselves for any given CLIP embedding. We show that there are universal and more effective strategies for utilizing any CLIP teacher model, regardless of its architecture (e.g., B/32 or L/14) or the dataset it was trained on (e.g., OpenAI-WIT-400M or DFN's high-quality dataset). These strategies should always be orthogonal to the use of any newly trained CLIP-style models like DFN and might also be compatible with methods using external models like FAST and BLIP-2.

**Our Contributions.** We propose an alternative to CLIPScores that we call **negCLIPLoss** that more accurately characterizes data quality. We also introduce a new distribution metric we call the p-Norm Similarity Score (**NormSim**) when knowledge about downstream tasks is available. Two major observations directly inform our proposals:

* [leftmargin=*,noitemsep,topsep=0pt]
* Firstly, we observe that classical methods measure the quality of a multi-modal sample by computing the cosine similarity between its visual and language embeddings, believing that lower similarity indicates that the text does not match its image part well. However, we find that some less informative samples may have a systematic bias, which leads to higher CLIPScores. For example, the language part containing the word "image" can result in higher similarity with any visual part, even when the text does not accurately describe its image content. Our proposed method **negCLIPLoss**, inspired by the standard CLIPLoss, normalizes the original CLIPScore by the similarity between a sample and its contrastive pairs. For example, the high score caused by the word "image" is typically consistent across its contrastive pairs, so our adjustment reduces this bias. As we have highlighted, such replacement can be universally applied across different embedding models. See Fig. 2 for illustrations.
* Secondly, if one has access to examples drawn from the same distribution as the target task, it is natural to assume that this extra knowledge could be leveraged to inform the data filtering process. We propose the **NormSim** metric to measure the vision similarity between a training sample \(x\) and the target task dataset \(X^{v}_{\text{target}}\in\mathbb{R}^{n\times D}\) defined as \(\|f_{v}(X^{v}_{\text{target}})f_{v}(x^{v})\|_{p}\), where \(f_{v}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}\) is the vision encoder of teacher model so that \(f_{v}(X^{v}_{\text{target}})\in\mathbb{R}^{n\times d}\), \(f_{v}(x^{v})\in\mathbb{R}^{d}\), and \(f_{v}(X^{v}_{\text{target}})f_{v}(x^{v})\in\mathbb{R}^{n}\), and \(\|\cdot\|_{p}\) is the \(p\) norm; effective choices are \(p=2\) or \(\infty\). Notably, unlike previous ImageNet-based filtering [1], which tries to keep the training set as diverse as downstream tasks by clustering the training set and finding the nearest neighbor group for _every target sample_, our method does not explicitly consider the diversity but select examples as long as it is close to _any target sample_ (i.e. select high NormSim score). Notably, **negCLIPLoss** and **NormSim** enjoy complementary effect in data selection. See Fig. 3.

To illustrate the effectiveness of our methods, we use a widely used benchmark DataComp [1] as our primary method of evaluating the datasets created by our data filtering methods. We show that, by simply replacing the CLIPScores with **negCLIPLoss** and utilizing **NormSim** we are able to exceed the best OAI-CLIP(L/14)-based baseline by 5.3% on ImageNet-1k and 2.8% on average across 38 downstream tasks, which is similar or even better than the performance achieved by many external-resources-based methods. Notably, even if the target downstream tasks are not available, using NormSim on a proxy downstream task constructed from the training set, called **NormSim\({}_{2}\)-D**, combined with negCLIPLoss, can also gain a 1.9% improvement on 38 downstream evaluation.

Moreover, the improvements achieved by our methods are not limited to OAI CLIP-based methods but can also be obtained by combining our methods with advanced models that require external resources. _By merging the subset selected by **negCLIPLoss** and **NormSim** with the subset selected by current state-of-the-art method "HYPE \(\cup\) DFN", we can further improve it by 0.9% on both ImageNet-1k and on average 38 downstream tasks. Besides, we can also achieve a 0.8% improvement on average 38 tasks over "HYPE \(\cup\) DFN" using only the data selected by DFN and our strategies._ More importantly, we demonstrate that negCLIPLoss, as a replacement for CLIPScore, can be applied to any other embedding models like OAI-L/14, OAI-B/32, and DFN-B/32, universally boosting performance from 0.4% to 3.0% on an average of 38 tasks. This result is not only technically insightful for understanding the information available in embeddings but also practically significant. Compared to existing methods, our approach saves a significant amount of computational time on both reprocessing and new embedding retraining as shown in Table 5.

## 2 Problem Setup

**Data Filtering on Multimodal Dataset.** We are given a training dataset \(D_{\text{train}}=\{x^{v},x^{l}\}\), where \((x^{v},x^{l})\in\mathbb{R}^{D}\) is the image-text (vision-language) training pair. For convenience, we will let superscript \(vl\) denote either modality so that, for example, \(x^{vl}\in x^{v},x^{l}\). Our goal is to identify a subset \(S\subset D_{\text{train}}\) that maximizes the zero-shot accuracy of the CLIP model on some downstream tasks when \(S\) is used to train the CLIP model.

**CLIP score and embedding.** Recent efforts, such as LAION [5] and DataComp [1], use OpenAI's CLIP ViT-L/14 model [4] as a teacher model to obtain quality score. Here we denote this vanilla CLIP model as \(\bar{f}_{vl}\). For any pair \(x^{vl}\), the model outputs a normalized unit-vector \(\bar{f}_{vl}(x^{vl})\). If \(X^{vl}:=\{x^{vl}_{1},\dots,x^{vl}_{m}\}\) denotes a dataset containing \(m\) samples, then we define \(\bar{f}_{vl}(X^{vl})=[\bar{f}_{vl}(x^{vl}_{1}),\dots,\bar{f}_{vl}(x^{vl}_{m}) ]^{\top}\in\mathbb{R}^{m\times d}\) as the embedding matrix. The popular filtering metric "CLIPScore" is defined as \(\langle\bar{f}_{v}(x^{v}),\bar{f}_{l}(x^{l})\rangle\in[-1,1]\).

**Dataset and model.** Here we follow the pipeline of Datacomp [1] to standardize the training and evaluation process. This is a testbed for dataset experiments aiming to open-source and further improve the vanilla CLIP model and is widely adopted in previous data selection papers [17; 18; 12; 2; 19; 7]. We will give more details in Sec. 4.

## 3 Data Filtering Strategy

### negCLIPLoss: A Better Metric than CLIPScore

In this section, we introduce a better and statistically interpretable quality metric called negCLIPLoss, which directly replaces the common metric CLIPScore. Fig. 1 illustrates how negCLIPLoss works. This new metric only requires negligible extra computational costs and no additional external data collection costs. As the name suggested, this metric is inspired by the standard CLIP loss used in the actual training process of the teacher CLIP model, which is defined as

\[\ell_{B^{*}}(x^{vl}_{i})=-\frac{1}{2}\left[\log\frac{\text{exp}(\bar{f}_{v}(x ^{v}_{i})^{\top}\bar{f}_{l}(x^{l}_{i})/\tau)}{\sum_{j\in B^{*}}\text{exp}(\bar {f}_{v}(x^{v}_{i})^{\top}\bar{f}_{l}(x^{l}_{j})/\tau)}+\log\frac{\text{exp}( \bar{f}_{v}(x^{v}_{i})^{\top}\bar{f}_{l}(x^{l}_{i}))/\tau}{\sum_{j\in B^{*}} \text{exp}(\bar{f}_{v}(x^{v}_{j})^{\top}\bar{f}_{l}(x^{l}_{i})/\tau)}\right]\] (1)

Here \(B^{*}\) is the random batch where \(i\)-th sample belongs during a particular training step, and \(\tau\) is the learnable temperate parameter. Notably, the teacher loss differs from CLIPScore primarily by a normalization term \(\mathcal{R}^{*}\) as follows:

\[-\tau\cdot\ell_{B^{*}}(x_{i}^{vl})=\underbrace{\bar{f}_{v}(x_{i}^{v})^{\top}\bar{ f}_{l}(x_{i}^{l})}_{\text{CLIPScore}}-\underbrace{\frac{\tau}{2}\left[\log\sum_{j\in B^{*}} \exp(\frac{\bar{f}_{v}(x_{i}^{v})^{\top}\bar{f}_{l}(x_{j}^{l})}{\tau})+\log\sum _{j\in B^{*}}\exp(\frac{\bar{f}_{v}(x_{j}^{v})^{\top}\bar{f}_{l}(x_{i}^{l})}{ \tau})\right]}_{\text{normalization term }\mathcal{R}^{*}}\]

In practice, since the training dataset of teacher CLIP models, like OAI-WIT400M [4], and the actual batch divisions \(B^{*}\) is inaccessible, we randomly select \(K\) batches from the student model's training data and use the averaged results from \(\{B_{k}\}_{i=1}^{K}\) to estimate the normalization term \(\mathcal{R}^{*}\) on \(B^{*}\):

\[\text{negCLIPLoss}(x_{i}^{vl}):=-\frac{\tau}{K}\sum_{k=1}^{K}\ell_{B_{k}}(x_{i }^{vl})\;\approx\;\text{CLIPScore}(x_{i}^{vl})-\mathcal{R}^{*}\] (2)

Here \(\{B_{k}\}_{i=1}^{K}\) are some batches randomly selected from the student model's training data and \(x_{i}\in B_{k},\forall k\). We choose \(K=10\) in our experiments, but any sample size larger than 5 is sufficiently stable for estimating the original CLIPLoss (Details in Appendix D.1). Besides, in Sec. 4.3.3 we also show that the computational cost introduced by \(\mathcal{R}\) remains negligible compared to other baselines. The temperature \(\tau\) and batch size \(|B^{*}|\) can be directly obtained from the parameters of the pretrained teacher model. More details of negCLIPLoss are in Appendix, including the concentration analysis of \(\mathcal{R}\) (Appendix A.1), pseudocode (Algorithm 1), and the ablation study of \(\tau\) and \(|B|\) (Appendix C.2).

**Motivation behind negCLIPLoss.** Other existing works also use loss-guided data selection, such as LESS [20] in NLP, CoDis [21] in CV, and RHO [22] in general data scheduling scenarios. However, it is still unclear whether selecting based on teacher loss is suitable for multi-modal contrastive learning. Here we give an affirmative answer as shown in Fig. 2, where we can see negCLIPLoss performs better than or on par with CLIPScore consistently.

To illustrate how teacher loss helps our selection, we demonstrate that the normalization term provided by negCLIPLoss is crucial for correcting the overestimation or underestimation inherent in CLIPScore. A high normalization term implies that either the image embedding, text embedding, or both can easily match multiple contrastive pairs beyond their

Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term \(\mathcal{R}\). negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, “Top X%” denotes that the score represents the top X% _high_ values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, “\(\mathcal{R}:\) Top \(100\%\)” means this data has almost the smallest \(\mathcal{R}\) among the whole dataset, which represents that it contains highly specific elements in both images and texts.

Figure 2: Comparison of negCLIPLoss and CLIPScore across different downsampling ratios on DataComp-medium.

corresponding counterparts. For example, in the bottom right of Fig. 1, the text containing "Image" or "Photo" can be easily matched with any visual content. Similarly, the image of "verloopring" only contains very simple features and can be matched with many words like "white", "empty" or "circle", etc. Consequently, despite a high absolute CLIPScore, the relative negCLIPLoss within its batch can be lower. In contrast, the bottom left features highly specific elements in both text and images, such as "Islands Harbor," "American football", and "sheep at green". These elements are specific and less likely to match with contrastive pairs, resulting in a higher relative negCLIPLoss.

### NormSim: A New Training-Target Similarity Metric

Our proposed negCLIPLoss is a universal approach to improve filtering performance by estimating quality better, and it does not rely on any downstream task. Now, if we can access some knowledge of the downstream tasks, we could further improve the performance by using a vision-only _\(p\)-norm similarity to target data_ metric to measure the relationship between each training sample and the downstream target data. We will discuss the reason to use vision-only embedding later in this section.

Specifically, we assume access to the target set of downstream tasks and denote them as \(X_{\text{target}}=\{x_{\text{target},(1)},\dots,x_{\text{target},(m)}\}\), where each \(x_{\text{target},(i)}\in\mathbb{R}^{d}\) is _i.i.d._-sampled from the target downstream distribution \(\mathcal{P}_{\text{target}}\)3, but without overlapping with the test set. Then, for each training sample \(x^{vl}\) and the corresponding target set \(X_{\text{target}}\), the NormSim is defined as:

Footnote 3: Although out-of-distribution tasks like “WILDS” have distribution shift between training data and test data, they still provides useful information of the test data.

\[\text{NormSim}_{p}(X_{\text{target}},x):=\|\bar{f}_{v}(X_{\text{target}}^{v}) \bar{f}_{v}(x^{v})\|_{p}=\left(\sum_{x_{t}\in X_{\text{target}}}|\langle\bar {f}_{v}(x_{t}^{v}),\bar{f}_{v}(x^{v})\rangle|^{p}\right)^{1/p}\] (3)

We select the subset \(S\) by choosing the samples with top-\(N\) highest NormSim scores. The choice of the norm type \(p\) can be based on the data distribution and training process. In this paper, we consider two instantiations of \(p\):

When \(p=2\), our data selection method can be regarded as the following equation. It's equivalent to selecting a subset that aligns with the principal components of the target set variance (Appendix C.6.1).

Figure 3: Illustration of NormSim on DataComp. \(X_{\text{target}}\) is the target prior data. “Top X%” denotes that the score represents the top X% high values within the entire dataset. (a) Visualization of data with different NormSim and negCLIPLoss. Here we use \(\text{NormSim}_{2}\)(ImageNet-1k) as an example. Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low \(\text{NormSim}_{2}\) (Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO. For example, they contain many images dominated by OCR content and make little contribution to improving downstream performance. (b) Illustration of a rough comparison of sampling data for different filtering methods. Using “negCLIPLoss \(\cap\) NormSim” filtering can balance the quality and relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E for more visualization.)

\[S=\arg\max_{|S|=N}\sum_{i\in S}\text{NormSim}_{2}(x_{t},x_{i}),\quad\text{ NormSim}_{2}(x_{t},x_{i})=\left(\sum_{x_{t}\in X_{\text{target}}}\left|\bar{f}_{v}(x _{t}^{v})^{\top}\bar{f}_{v}(x^{v})\right|^{2}\right)^{1/2}\] (4)

When \(p=\infty\), the distance metric can be regarded as an even more optimistic measure, such that a training sample will be selected if it has high similarity to _any target sample_. Note that this is different from nearest-neighbor-based method used in image-based filtering [1], where they are trying to find the nearest training sample of _every target sample_. In this case, it can be regarded as:

\[S=\arg\max_{|S|=N}\sum_{i\in S}\text{NormSim}_{\infty}(x_{t},x_{i}),\qquad \text{NormSim}_{\infty}(x_{t},x_{i})=\max_{x_{i}\in X_{\text{target}}}\bar{f}_ {v}(x_{t}^{v})^{\top}\bar{f}_{v}(x_{i}^{v})\] (5)

In Appendix D.3, we also show that our NormSim\({}_{\infty}\) can outperform the nearest neighbor selection on the downstream target tasks. Here, we show an example selected via the NormSim\({}_{2}\)(ImageNet-1k) in Fig. 3, showing that this vision-target-aware method is complementary to the quality-based one.

**Choice of Target Data.** In the experiment parts, we try two kinds of target data: training data from ImageNet-1k (1.3M) or training data from all 24 accessible downstream tasks (2.1M)4. We denote them as \(\text{NormSim}_{p}\)(IN-1k) and \(\text{NormSim}_{p}\)(Target), respectively.

Footnote 4: Here we only use the target data for data selection, instead of training on them. The target dataset is significantly smaller than pretraining set like DataComp-medium (128M) or external datasets like HQITP-350M utilized by DFN [2].

**Necessity of using vision-only information** We use only the visual information \(x^{v}\) instead of multimodal information \(x^{v^{l}}\) for measuring similarity. This is because common crawled text often has brief captions, making the OAI CLIP language embedding weaker than its visual embedding model [1, 23, 24, 25]. Consequently, the language part cannot characterize the pre-training and downstream task distribution as well as the visual part. This phenomenon is also observed in Gadre et al. [1], where image-based filtering (select data whose image embeddings are similar to that from ImageNet-1k) outperforms text-based filtering (select data whose captions contain words from ImageNet-21k). More ablation studies are provided in Appendix D.4.

**Generality of NormSim in choosing teacher model.** Notably, since we just use image embeddings in the NormSim metric, we believe it unnecessary to use CLIP model to obtain NormSim. NormSim can be a general metric for selecting target-related image/image-text data if any good image representations are given, like the representations obtained from pretrained ResNet-50.

**Theoretical justification.** Unlike many existing methods that force diversity by selecting training samples around each \(\bm{x}_{\text{target}}\), our strategy maximizes similarity without directly considering data diversity. For the \(p=2\) case, we demonstrate that maximizing \(\text{NormSim}_{2}\) is optimal under a linear model \(\bar{f}_{v}\), as shown in Appendix A.2. Our theorem also provides error guarantees for noisy embeddings and explains when vision-only embeddings outperform combined vision and language embeddings. Recent work by Joshi et al. [26] provides a similar analysis but focuses on high-quality data and cross-variance between images and texts. This approach is less effective than image-only methods for filtering noisy datasets, as discussed above.

**Using proxy when downstream \(\bm{X}_{\text{target}}\) is inaccessible.** Surprisingly, we show that the 2-norm can also be used when only the pre-training set is available. In this case, we construct a proxy "target" set from the pre-training set itself. Specifically, let \(S_{i}\) be the selected subset at step \(i\), then we treat the current \(S_{i}\) as the proxy "target" set. To construct the next smaller set, we select the next data batch \(S_{i+1}\) satisfying \(\arg\max_{S_{i+1}\in S_{i}}\sum_{x\in S}\text{NormSim}_{2}(S_{i},x)\), until reaching an N size subset. We call this approach \(\text{NormSim}_{2}\)-D (Dynamic) and will specify the algorithm details in Appendix C.3.

## 4 Experimental Results

In this section, we evaluate the performance of negCLIPLoss and NormSim, aiming to address the following questions: **Q1:** Given a fixed CLIP teacher model, can our methods more effectively utilize CLIP embeddings for data filtering? **Q2:** Are our methods applicable to diverse CLIP teacher models with varying architectures or different pretrained datasets? **Q3:** How does our method compare to other leading approaches that utilize external models or multimodal datasets? Additionally, could our method be compatible with these methods and enhance their effectiveness?

### Setup

We adhere to the standardized training and evaluation protocols of the DataComp benchmark [1]. **Training configuration.** We employ the medium-scale training configuration of DataComp (DataComp-medium). It provides a substantial dataset comprising 128 million low-quality, web-curated image-text pairs to be filtered. Once the data subset is obtained by some data filtering strategy, it will be used to train a fixed CLIP-B/32 model in a fixed training budget that allows the model to pass 128 million data points an epoch. Therefore, smaller subsets will be repeated more frequently, ensuring a fair comparison. We note that the size of the DataComp dataset becomes smaller over time since some URLs of images become invalid5, and we only successfully downloaded about 110M data. Therefore, the results of baselines on the leaderboard do not apply to our datasets, and we reproduce all the top baselines on the leaderboard with their public UIDs of the selected data.

Footnote 5: See https://github.com/mlfoundations/datacomp/issues/3. Similar issues are proposed by \(\mathbb{D}^{2}\) pruning [18].

**Evaluation.** We measured the model performance on 38 downstream datasets including image classification and retrieval tasks followed by DataComp. The image classification tasks contain ImageNet-1k [27], ImageNet distribution shifts [28, 29, 30, 31], 11 datasets from the Visual Task Adaptation Benchmark (VTAB) [32] and 3 datasets from WILDS [33, 34]. Retrieval datasets contain Flickr30k [35], MSCOCO [36] and WinoGAViL [37].

**Teacher model architecture.** Our experiments utilize two architectures for OpenAI's CLIP teacher models: ViT-L/14 and ViT-B/32. Additionally, we use the public version of DFN (DFN-P) proposed by Fang et al. [2] as a teacher model, and its architecture is also ViT-B/32.

### Baselines

We restate the three current research directions mentioned before based on how much external resources are employed: (D1) using OAI CLIP alone while optimizing embedding employment strategies, (D2) training and using a more advanced CLIP embedding model based on external data, and (D3) utilizing non-CLIP external models to aid data selection. It is important to note that D2 and D3 may also incorporate strategies from D1. For example, CLIPScore (D1) has been used in almost all the top methods. Therefore, we categorize baselines by the largest possible category they encompass. According to the above categorization, we summarize the baselines we used in our experiments as follows. Please refer to Fig. 4 and Appendix C.4 for more details.

**D1: OAI CLIP embedding only.** The learner can only access the pretraining dataset (like DataComp-medium), the original OAI CLIP teacher model that is used to extract embeddings, and some target data of the downstream tasks which is much smaller than the pretraining dataset (like ImageNet-1k). In this category, we don't use any existing external non-CLIP models or any newly trained CLIP model based on external multi-modal dataset. In detail, This category includes (1) **CLIPScore**[38], which only uses CLIPScore for filtering as we mentioned before. (2) **Image-based filtering**[1], which uses ImageNet-1K training data as the downstream target data for data filtering. It applies k-means clustering to the _image_ embeddings of training data and selects clusters closest to the ImageNet-1K embeddings. Gadre et al. [1] also try to combine image-based filtering and CLIPScore together. (3) \(\mathbb{D}^{2}\) **Pruning**[18], which represents the dataset as an undirected graph and selects the data by combining difficulty and diversity. They use the CLIP score to initialize their graph.

**D2, D3: Accessible external model and multi-modal data.** All the current top baselines enable the learner to utilize external resources, either to train a better CLIP teacher model or to help filtering using existing models' properties. In detail, (1) **DFN**[2] trains another CLIP data filtering network via external high-quality data. Their currently public model (**DFN-P**) is trained on CC12M [39] + CC3M [40] + SS15M [41], while the best DFN is trained on nonpublic HQITP-350M [2], which is even larger than DataComp-medium. (2) **HYPE**[3] leverages hyperbolic embeddings (different from CLIP embedding) and the concept of entailment cones to filter out samples with meaningless or underspecified semantics, enhancing the specificity of each sample. (3) **HYPE**\(\cup\)**DFN** proposed by [3] samples subset separately for each method and then merge them. This is the state-of-the-art method on the DataComp benchmark for medium size. (4) Other methods including **T-MARS**[12], **Devils**[14], **MLM**[42], which leverage external models such as text detection model FAST [13], BLIP-2 [16] and LLAVA-1.5 [43, 44] to heuristically select data. See details in Appendix C.4.

**Cross-setting comparison.** We make these separations for fair comparison. Intuitively, performance should be ranked as **D2, D3 > D1**. However, our results show that cross-setting comparisons are possible and our D1 methods can perform similar or even better than most of D3 methods.

[MISSING_PAGE_FAIL:8]

#### 4.3.2 Try Other Teacher Models (Q2)

To evaluate whether our method applies to other CLIP teacher models, we replaced OAI CLIP-L/14 with OAI CLIP-B/32 and DFN-P as embedding models. We compare the best baseline "CLIPScore" with our "negCLIPLoss" and best strategy "negCLIPLoss \(\cap\) NormSim\({}_{\infty}\)(Target)" as shown in Table 1 and Appendix D.2. Note that the original DFN paper selects a subset comprising 19.2M data points, which accounts for approximately \(17.5\%\) of our dataset and \(15\%\) of their dataset, we incorporate these sampling ratios into our comparison.

**negCLIPLoss can be applied to different CLIP embedding models.** Our proposed negCLIPLoss, as a replacement of CLIPScore, not only leads to better performance compared to all the other baselines using OAI CLIP-L/14 as shown in Table 2, but also achieves universal improvement on the other two CLIP embedding models, OAI CLIP-B/32 and DFN-P as shown in Table 1. Our methods can consistently outperform all downstream tasks for different filtering ratios and models, like a 0.5%-5.4% increase on ImageNet-1k.

**Embedding required by NormSim should have good downstream performance.** When combining negCLIPLoss with NormSim\({}_{\infty}\), OAI CLIP-B/32 and DFN-P exhibit completely different behaviors. The former obtains results even better than those in Table 2, which uses OAI CLIP-L/14 as the teacher model, while DFN-P achieves results even worse than using negCLIPLoss alone6. The reason is that, unlike OAI CLIP-B/32, DFN-P is specially designed for data filtering _at the expense of downstream task performance_, as claimed by its authors. For example, the ImageNet-1k accuracy for DFN-P, OAI CLIP-B/32, and OAI CLIP-L/14 are 45%, 63%, and 75%, respectively. This indicates that the embeddings obtained from DFN on target data might be highly unreliable, leading to inaccurate similarity calculations between training and target data. To support this, if we use DFN-P to evaluate negCLIPLoss but utilize OAI CLIP-B/32 for calculating NormSim, as shown in "negCLIPLoss (17.5%) \(\cap\) NormSim\({}_{\infty}^{\text{B/32}}\)(Target)", we can further improve the results compared to using negCLIPLoss alone. Its average performance on 38 tasks is even higher than utilizing the best DFN (trained on HQITP-350M) with CLIPScore, as shown in Table 3.

Footnote 6: see “negCLIPLoss (30%) \(\cap\) NormSim\({}_{\infty}\)(Target)” versus “negCLIPLoss (20%)/(30%)” and “negCLIPLoss (17.5%) \(\cap\) NormSim\({}_{\infty}\)(Target)” versus “negCLIPLoss (17.5%)/(15%)”

#### 4.3.3 Comparison with D2 & D3 Categories (Q3)

In this part, we compare all the D2 & D3 baselines mentioned in Sec. 4.2 together with our best strategy in Table 3. Here we reproduce all the baselines if their official UIDs are available. For "A \(\cup\) B" mentioned in Table 3, we follow the way of "HYPE \(\cup\) DFN" in Kim et al. [3] to merge the data, which generates the sampling subset separately for each method and then merge them. This will result in oversampling the shared data, which is intuitively more important.7 We also show the best result

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline \multirow{2}{*}{**Type**} & \multirow{2}{*}{**Filtering Strategy**} & **Dataset Size** & **IN-1k** & **IN Dist. Shift** & **VTAB** & **Retrieval** & **Avg.** \\  & & **Size** & (1) & (5) & (11) & (3) & (38) \\ \hline D3 & T-MARS [12] & 22M & 30.8 & 26.3 & 34.8 & 25.4 & 34.1 \\ D3 & Devil [14] & 20M & 31.0 & 26.7 & 35.9 & 24.7 & 34.5 \\ D3 & MLM [42] & 38M & 30.3 & 25.6 & 36.0 & **29.0** & 34.5 \\ D3 & HYPE [3] & 10M & 30.3 & 25.8 & 34.3 & 22.2 & 31.9 \\ D2 & DFN [2] & 16M & 36.0 & 30.1 & 36.2 & 27.0 & 35.4 \\ D3 & DFN \(\cup\) HYPE [3] & 20M & 36.4 & 30.8 & 38.5 & 28.0 & 36.8 \\ \hline D1 & **Ours (20\%)** & 22M & 32.4 & 27.4 & 35.9 & 26.3 & 35.2 \\ D3 & DFN \(\cup\)**Ours (20\%)* & 23M & 36.4 & 30.9 & **38.6** & 28.1 & 37.6 \\ D3 & DFN \(\cup\) HYPE \(\cup\)**Ours (10\%)* & 22M & **37.3** & **31.4** & 38.5 & 27.6 & **37.7** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of all D1&D2&D3 top methods on DataComp-medium. The results of MLM [42] are from their paper, while all other baselines are reproduced on our downloaded dataset using their official UIDs. “Ours (20%)” refers to use “negCLIPLoss (30%) \(\cap\) NormSim\({}_{\infty}\)(Target)” to get 20% of original data, while “Ours (10%)” denotes applying “negCLIPLoss (20%) \(\cap\) NormSim\({}_{\infty}\)(Target)” to get 10%. And we use “*” to indicate the case where we choose the intersection of the data selected by using OAI CLIP-B/32 and OAI CLIP-L/14 separately, which results in about 15M data for “Ours (20%)*” and 7.4M data for “Ours (10%)*”.

we obtain by combining our method with DFN [2] and HYPE [3] on the full DataComp-medium dataset in Table 4, where the baselines are from DataComp benchmark.

**Our methods can outperform most of the D3 methods.** In Table 3, we show that without using any external models or data, our best combination, i.e., using OAI CLIP-B/32 for "negCLIPLoss (30%) \(\cap\) NormSim\({}_{\infty}\)(Target)" (**Ours (20%)**), still outperforms all methods except DFN and "DFN \(\cup\) HYPE". This answers the first part of Q3 and further indicates that some external models may be redundant since CLIP embeddings already contain necessary information.

**We can further improve the SOTA method.** In Table 3, we show that our model can further boost the performance of the current SOTA method "HYPE \(\cup\) DFN" by 0.9% on both ImageNet-1k and on average 38 downstream tasks, and close results can be achieved even without combining HYPE which utilizes the external embedding model MERU [45]. And we update the SOTA performance of the DataComp-medium (full dataset) benchmark as shown in Table 4. Here we use the data selected by both OAI CLIP-B/32 and L/14, which we found is more robust than using one of them alone. Our better results answer the second part of Q3, that is, our methods can be compatible with other D2&D3 methods.

## 5 Conclusion and Limitation

In this paper, we introduce two metrics, negCLIPLoss and NormSim, to enhance data selection in multimodal contrastive learning without relying on external resources. negCLIPLoss provides a more accurate quality metric compared to the commonly used CLIPScore, while NormSim measures the similarity between pretraining data and target data for known downstream tasks. Experiments show that our methods achieve results that are competitive with or even better to approaches using external models or datasets. Additionally, negCLIPLoss and NormSim are compatible with existing top techniques, allowing us to achieve a new state-of-the-art by combining them.

A notable limitation of our study is the exclusion of larger pretraining datasets, such as the large and xlarge scales of DataComp. However, DataComp-medium is the most commonly used benchmark for data selection in CLIP pretraining, and our method has demonstrated both effectiveness (Table 2-3) and efficiency (Table 5) on it. Future directions include exploring better ways to merge data selected by different methods and incorporating our methods into data scheduling scenarios.

## 6 Acknowledgement

We thank Tong Chen, Pang Wei Koh, Xiaochuang Han, Rui Xin, Luyao Ma, Lei Chen, and other members in the UW ML Group for many insightful discussions and helpful feedback. The research of Kevin Jamieson and Yifang Chen are partially supported by the NSF through the University of Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF 2007036. SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF CCF 2019844, and NSF IIS 2229881.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Strategy** & **IN-1k** & **Avg.** \\ \hline No filtering & 17.6 & 25.8 \\ CLIPScore [38] & 27.3 & 32.8 \\ T-MARS [12] & 33.0 & 36.1 \\ Devils [14] & 32.0 & 37.1 \\ DFN [2] & 37.1 & 37.3 \\ DFN \(\cup\) HYPE [3] & **38.2** & 37.9 \\ \hline DFN \(\cup\)**Ours (20\%)** & 37.5 & 38.6 \\ DFN \(\cup\) HYPE \(\cup\)**Ours (10\%)** & **38.2** & **38.8** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of the top methods on the full DataComp-medium dataset (128M data).

## References

* [1] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* [2] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. _arXiv preprint arXiv:2309.17425_, 2023.
* [3] Wonjae Kim, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, and Sangdoo Yun. Hype: Hyperbolic entailment filtering for underspecified images and texts. _arXiv preprint arXiv:2404.17507_, 2024.
* [4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [5] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2818-2829, 2023.
* [7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [8] Huy V Vo, Vasil Khalidov, Timothee Darcet, Theo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Automatic data curation for self-supervised learning: A clustering-based approach. _arXiv preprint arXiv:2405.15613_, 2024.
* [9] Tzu-Heng Huang, Changho Shin, Sui Jiet Tay, Dyah Adila, and Frederic Sala. Multimodal data curation via object detection and filter ensembles. _arXiv preprint arXiv:2401.12225_, 2024.
* [10] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. _Advances in Neural Information Processing Systems_, 36:34201-34227, 2023.
* [11] Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and Ari S Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters. _arXiv preprint arXiv:2401.04578_, 2024.
* [12] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-mars: Improving visual representations by circumventing text feature learning. _arXiv preprint arXiv:2307.03132_, 2023.
* [13] Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, and Tong Lu. Fast: Faster arbitrarily-shaped text detector with minimalist kernel representation, 2021.
* [14] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the details: A deep dive into the rabbit hole of data filtering. _arXiv preprint arXiv:2309.15954_, 2023.
* [15] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. _arXiv preprint arXiv:1607.01759_, 2016.
* [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.

* [17] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. _arXiv preprint arXiv:2307.10350_, 2023.
* [18] Adyasha Maharana, Prateek Yadav, and Mohit Bansal. D2 pruning: Message passing for balancing diversity and difficulty in data pruning. _arXiv preprint arXiv:2310.07931_, 2023.
* [19] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and Ari Morcos. Sieve: Multimodal dataset pruning using image captioning models. _arXiv preprint arXiv:2310.02110_, 2023.
* [20] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. _arXiv preprint arXiv:2402.04333_, 2024.
* [21] Xiaobo Xia, Bo Han, Yibing Zhan, Jun Yu, Mingming Gong, Chen Gong, and Tongliang Liu. Combating noisy labels with sample selection by mining high-discrepancy examples. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1833-1843, October 2023.
* [22] Soren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedik Holten, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 15630-15649. PMLR, 17-23 Jul 2022.
* [23] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? _arXiv preprint arXiv:2107.06383_, 2021.
* [24] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. In _International Conference on Machine Learning_, pages 25994-26009. PMLR, 2022.
* [25] Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. When are lemons purple? the concept association bias of clip. _arXiv preprint arXiv:2212.12043_, 2022.
* [26] Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman. Data-efficient contrastive language-image pretraining: Prioritizing data quality over quantity. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 1000-1008. PMLR, 02-04 May 2024.
* [27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [28] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. _Advances in Neural Information Processing Systems_, 32, 2019.
* [29] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 5389-5400. PMLR, 09-15 Jun 2019.
* [30] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, 2021.
* [31] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8340-8349, 2021.

* [32] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. _arXiv preprint arXiv:1910.04867_, 2019.
* [33] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [34] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds benchmark for unsupervised adaptation. _arXiv preprint arXiv:2112.05090_, 2021.
* [35] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.
* [36] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* [37] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. _Advances in Neural Information Processing Systems_, 35:26549-26564, 2022.
* [38] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.
* [39] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3558-3568, 2021.
* [40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, Melbourne, Australia, July 2018. Association for Computational Linguistics.
* [41] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. _Advances in Neural Information Processing Systems_, 35:21455-21469, 2022.
* [42] Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, and Heng Wang. Finetuned multimodal language models are high-quality image-text data filters. _arXiv preprint arXiv:2403.02677_, 2024.
* [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [44] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. _See https://vicuna. lmsys. org (accessed 14 April 2023)_, 2(3):6, 2023.
* [45] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. Hyperbolic image-text representations. In _International Conference on Machine Learning_, pages 7694-7731. PMLR, 2023.
* [46] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. In _International Conference on Artificial Intelligence and Statistics_, pages 4348-4380. PMLR, 2023.

* [47] Gene H Golub and Charles F Van Loan. _Matrix computations_. JHU press, 2013.
* [48] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [49] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with gpus. _IEEE Transactions on Big Data_, 7(3):535-547, 2019.
* [50] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [51] Sachin Goyal, Pratyush Maini, Zachary C Lipton, Aditi Raghunathan, and J Zico Kolter. Scaling laws for data filtering-data curation cannot be compute agnostic. _arXiv preprint arXiv:2404.07177_, 2024.

Theoretical Interpretation

### Concentration of Normalization Term in negCLIPLoss

In this section, we construct a theorem using the concentration inequality to show that when the batch size is sufficiently large, the normalization term \(R^{B_{k}}\) obtained from actual batch \(B_{k}\) can approximate \(R^{B^{*}}\) calculated using ground truth batch \(B^{*}\) quite well. The details are as follows:

We assume that the pretraining dataset \(\mathcal{D}\) is independent and identically distributed (_i.i.d._) sampled from some distribution \(\mathcal{P}\). Besides, to use pretraining data batch to approximate the ground truth batch, one necessary condition is that their distribution is similar. Here for simplicity, we assume that they are also _i.i.d._.

**Assumption A.1**.: _We assume that the ground-truth batch of data \(B^{*}\) used by the teacher model is i.i.d. to the pretraining dataset \(\mathcal{D}\) which is required to be filtered._

For simplicity, we denote \(s_{ij}=\bar{f}_{v}(x_{i}^{v})^{\top}\bar{f}_{l}(x_{j}^{l}),i,j\in B\) to be the cross-image-text similarities in the batch \(B\). Then the normalization term can be written as

\[\mathcal{R}_{i}^{B}=\frac{\tau}{2}\left[\log(\sum_{j\in B}\exp(s_{ij}/\tau))+ \log(\sum_{j\in B}\exp(s_{ji}/\tau))\right]\]

Here \(s_{ij}\in[-1,1]\). We will show that \(\mathcal{R}_{i}^{B}=(1+o(1))\cdot\mathcal{R}_{i}^{B^{*}}\) for all \(i\) when \(|B|\) is sufficiently large, which means that we can use the random batch to approximate the ground-truth batch.

**Theorem A.1**.: _If Assumption A.1 holds and the batch size satisfies \(|B|=|B^{*}|\), then we have \(\mathcal{R}_{i}^{B}=\Theta(\log(|B|))\) while \(|\mathcal{R}_{i}^{B}-\mathcal{R}_{i}^{B^{*}}|=O(\frac{1}{\sqrt{|B|}})\) for any \(i\in B\cap B^{*}\)._

Proof.: Since \(s_{ij}\in[-1,1]\), It's obvious that \(\mathcal{R}_{i}^{B}=\Theta(\log(|B|))\). Let \(\alpha_{ij}:=\exp(s_{ij}/\tau)-\mathbb{E}_{j}[\exp(s_{ij}/\tau)]\), then \(\alpha_{ij}\) is zero-mean. Note that since the data is _i.i.d._, so does \(\alpha_{ij}\), and we denote \(\gamma:=\mathbb{E}_{j}[\alpha_{ij}^{2}]\). Note that \(|\alpha_{ij}|\leq e^{1/\tau}=:M\), from Bernstein inequality we have

\[\mathbb{P}(|\sum_{j\in B}\alpha_{ij}|\geq t)\leq 2\exp(-\frac{\frac{1}{2}t^{2}}{|B |\gamma+\frac{1}{3}Mt})\]

A similar conclusion holds for \(B^{*}\). These result that with probability at least \(1-\eta\), we have

\[|\sum_{j\in B}\alpha_{ij}|\leq\max\{2\sqrt{|B|\gamma\ln(\frac{2}{\eta})}, \frac{4}{3}M\ln(\frac{2}{\eta})\}=:t(|B|,\gamma,\eta,M)\]

Thus we have \(|\sum_{j\in B}\exp(\frac{s_{ij}}{\tau})-\sum_{j\in B^{*}}\exp(\frac{s_{ij}}{ \tau})|\leq 2t(|B|,\gamma,\eta)\). Furthermore, for any \(x_{1},x_{2}>1\), it's easy to prove that \(|\log(x_{1})-\log(x_{2})|\leq\frac{|x_{1}-x_{2}|}{\min(x_{1},x_{2})}\). Therefore, we have \(|\log(\sum_{j\in B}\exp(\frac{s_{ij}}{\tau}))-\log(\sum_{j\in B^{*}}\exp(\frac {s_{ij}}{\tau}))|\lesssim O(\frac{1}{\sqrt{|B|}})\). Similar claims hold for \(|\mathcal{R}_{i}^{B}-\mathcal{R}_{i}^{B^{*}}|\).

### Optimality of NormSim\({}_{2}\) Under Linear Assumption

In this section, we give a theoretical justification on the NormSim metric when \(p=2\) under the linear model assumptions when low quality image and mismatched text has already been removed. In other words, we mainly focus on the following strategy.

\[S=\arg\max_{|S|=N}\sum_{i\in S}\bar{f}_{v}(x_{i}^{v})^{\top}\underbrace{\left( \frac{1}{|\mathcal{X}_{\text{target}}|}\sum_{x_{1}\in\mathcal{X}_{\text{target} }}\bar{f}_{v}(x_{1}^{v})\bar{f}_{v}(x_{1}^{v})^{\top}\right)}_{\mathbb{E}_{ \text{target,proxy}}}\bar{f}_{v}(x_{1}^{v})\] (6)

#### a.2.1 Theoretical Setup

Training data.For any \(\bm{x}^{v},\bm{x}^{l}\in\mathbb{R}^{d}\) observable image and text training pairs, we define \(\bm{z}^{v},\bm{z}^{l}\) to be the corresponding latent vectors which contain all semantically pertinent information about our tasks of interest. Similar to previous theoretical work [46], we assume each i.i.d pair \(\bm{z}^{vl}\) follows zero-mean sub-gaussian distribution whose cross-covariance satisfies

\[\text{Cov}(\bm{z}^{v},\bm{z}^{l}) =\Sigma_{\text{train}}=\text{diag}(\sigma_{1},\sigma_{2},\ldots), \|\bm{z}^{vl}\| =1\]

and each \(\bm{x}^{vl}\) is generated based on a linear model such that

\[\bm{x}^{vl}=G^{*}_{vl}\bm{z}^{vl}+\bm{\xi}^{vl}.\]

Here \(G^{*}_{vl}\in O_{d\times r}\) is the othonormal ground truth representation mapping from the latent vector space to the input space, and \(\xi^{vl}\sim\mathcal{N}(0,I_{d})\) are _i.i.d._ random noise.

Also we denote the cross covariance of any finite dataset \(S^{\prime}\) (e.g. the given train set \(D_{\text{train}}\)) as \(\Sigma_{S^{\prime}}\).

Test data.For any zero-shot downstream task, we assume it shares almost same data generation process as the training set, except its the cross-covariance \(\Sigma_{\text{target}}\) does not necessarily equal \(\Sigma_{\text{train}}\), which necessitate the choice of \(\bar{\Sigma}_{\text{target\_proxy}}\).

CLIP embedding model as teacher.Under the linear model assumption, we have a teacher model \(\bar{f}_{vl}=\bar{G}_{vl}\), whose generated clip embedding can partially recover the ground truth hidden vector \(\bm{z}^{vl}\) with error.

Formally, we say teacher has \(\epsilon^{n}_{v}\) error if for all possible \(n\) budget subsets \(S\subset D_{\text{train}}\),

\[\frac{1}{|S|}\left\|\sum_{\bm{x}^{vl}\in S}\bar{G}_{v}^{\top}\bm{x}^{v}(\bm{x }^{v})^{\top}\bar{G}_{v}-\sum_{\bm{x}^{vl}\in S}\bm{z}^{v}(\bm{z}^{v})^{\top} \right\|_{*}\leq\epsilon^{n}_{v}\]

where the same notation applies for the language modal. By the orthonormal assumption on the ground truth matrix \(G^{*}_{vl}\), we see that \(\bar{G}_{v}^{\top}\) is aiming to inverting the map. In addition, we say the teacher has \(\epsilon^{n}_{v*l}\) cross modal error

\[\frac{1}{|S|}\|\sum_{\bm{x}^{vl}\in S}\bar{G}_{v}^{\top}\bm{x}^{v}(\bm{x}^{l}) ^{\top}\bar{G}_{l}-\sum_{\bm{x}^{vl}\in S}\bm{z}^{v}(\bm{z}^{l})^{\top}\|_{*} \leq\epsilon^{n}_{v*l}\]

When all \(\epsilon^{n}_{v},\epsilon^{n}_{l},\epsilon^{n}_{v*l}\to 0\) as \(n\to\infty\), then we say the teacher is strong for both modalities. But it might also be possible that only one modal, for example, visual is strong. That is \(\epsilon^{n}_{v}\to 0,\epsilon^{n}_{l},\epsilon^{n}_{v*l}\gg\epsilon^{n}_{v}\).

Model and training.According to Lemma 4.1 in [46], using the CLIP loss to optimize the linear model has approximately the same training dynamics as using the regularized linear loss. Therefore, here we assume that we are learning \(G_{v},G_{l}\) by maximizing the clip score gap between the contrastive pairs, plus a regularizer,

\[\min_{G_{v},G_{l}}\mathcal{L}^{\rho}_{S}(G_{v},G_{l}):=\min_{G_{v},G_{l}}\frac {\sum_{i\in S}\sum_{j\in S}(s_{ij}-s_{ii})}{|S|(|S|-1)}+\frac{\rho}{2}\frac{|S| }{|S|-1}\|G_{v}G_{l}^{\top}\|_{F}^{2}\]

where \(s_{ij}:=\langle G_{v}^{\top}\bm{x}^{v}_{i},G_{l}^{\top}\bm{x}^{l}_{j}\rangle\) and \(\rho>0\) is some regularizer-related _constant_. Note that this objective maximizes self-similarity and minimizes similarity between disparate pairs. Note that this "loss" can be negative, avoiding the trivial null solution of all zeros. We denote this training process from any given \(S\) as \(G_{vl}=\mathcal{A}^{\rho}(S)\).

Goal and metric.Under the same principle as our training loss function, we measure the performance of any learnt \(G_{v},G_{l}\) on some downstream task with distribution \(\mathcal{D}_{\text{target}}\) as test loss \(\mathcal{L}_{\text{target}}(G_{v},G_{l}):=\)

\[\mathbb{E}_{\bm{x}^{vl}\sim\mathcal{D}_{\text{target}}}(\langle G_{v}^{\top} \bm{x}^{v},G_{l}^{\top}\bm{x}^{l}_{2}\rangle-\langle G_{v}^{\top}\bm{x}^{v},G_{l }^{\top}\bm{x}^{l}\rangle)\]This is inspired by the following classification accuracy. Assume that the test data including \(C\) class, and the class distribution is \(\mathcal{C}\). For every class \(c\), the training data \(\bm{x}=(\bm{x}^{v},\bm{x}^{l})\) satisfies distribution \(\mathcal{P}_{c}\). We further assume the corresponding classification templates are \(\{\bm{x}_{c}\}_{c=1}^{C}\). Thus we define classification accuracy as

\[\text{AC}(G_{v},G_{l})=\mathbb{E}_{c,c^{\prime}\sim\mathcal{C}\times\mathcal{C} }\left[\mathbb{E}_{\bm{x}_{i}\sim\mathcal{P}_{c}}\bm{1}[s_{ic}>s_{ic^{\prime}}]\right]\]

Therefore our goal is to minimize its gap between the best hind-side subset, for any \(\rho\), without budget constraints,

\[\Delta^{\rho}(S)=\mathcal{L}_{\text{target}}(\hat{G}_{vl})-\min_{S^{\prime} \in\mathcal{D}_{\text{train}}}\mathcal{L}_{\text{target}}(\mathcal{A}^{\rho} (S^{\prime})),\hat{G}_{vl}=\mathcal{A}^{\rho}(S)\]

#### a.2.2 Generalization Guarantees

We now provide theoretical guarantees and postpone our proof into Appendix A.2.3. **Firstly, we are going to prove the intuition behind NormSim\({}_{2}\)score.**

**Lemma A.1** (Intuition behind NormSim\({}_{2}\)).: _With high probability at least \(1-\frac{1}{|S|d}\), suppose the hind-side best subset has at least \(\underline{n}\) number of samples, then we have_

\[\Delta^{\rho}(S)=\underbrace{\frac{1}{\rho}\max_{S^{\prime}\in D_{\text{train }}}\left(\operatorname{Tr}\left(\Sigma_{\text{target}}(\Sigma_{S^{\prime}}- \Sigma_{S})\right)\right)}_{\text{NormSim${}_{2}$related term}}+\underbrace{ \mathcal{O}\left(\sqrt{\frac{d\log(d|S|)}{\underline{n}}}+\sqrt{\frac{d\log (d|S|)}{|S|}}\right)}_{\text{noise}}\]

Proof sketch.: 

Under the assumption that both \(\bm{z}^{vl}\), \(\xi_{vl}\) is zero-mean, maximizing the clip score gap is equivalent to maximizing the clip score of the same sample.

\[\mathcal{L}_{\text{target}}(\hat{G}_{v},\hat{G}_{l}):=-\mathbb{E}_{\bm{x}^{vl }\sim\mathcal{D}_{\text{train}}}\langle\hat{G}_{v}^{\top}\bm{x}^{v},\hat{G}_ {l}^{\top}\bm{x}^{l}\rangle\]

By minimizing the regularized training loss \(\mathcal{L}_{S}^{\rho}(G_{v},G_{l})\) using Eckart-Young-Mirsky Theorem, we get a closed form solution of \(\hat{G}\) as

\[\hat{G}_{v}\hat{G}_{l}^{\top}\approx\frac{1}{\rho}G_{v}^{*}\Sigma_{S}\cdot(G_ {l}^{*})^{\top}+\text{noise depend on }S\]

Combining the result in

and

\[\mathcal{L}_{\text{target}}(\hat{G}_{vl})\approx-\frac{1}{\rho} \operatorname{Tr}\left(\Sigma_{\text{target}}\Sigma_{S}\right)-\text{noise depend on }S\]

The same analysis can be applied on \(\min_{S^{\prime}\in D_{\text{train}}}\mathcal{L}_{\text{target}}(\mathcal{A}(S ^{\prime}))\) as well. Rearranging these two equations gives us the final result. 

This lemma shows the the \(\Delta(S)\) is depend on the NormSim\({}_{2}\)-related term and the noise term which comes from \(\xi\). When \(\underline{n}\) and \(|S|\) is large enough, then the NormSim\({}_{2}\)-related term will become dominant. This aligns with our practice experience that the final performance is less sensitive to the small variation in the number of select data as long as that is sufficient. Moreover, in some special cases where test distribution has identity cross-variance, then sampling by choosing CLIP score might be enough.

**Now we are ready to give a proof on the choice of \(\tilde{\Sigma}_{\text{target}}\) and visual-only information.** Specifically, the strategy error mainly comes from (1). The unknown test distribution shift from training. (2). The unobservable ground truth \(\Sigma_{S}\). To tackle error (1), we assume some prior knowledge on test by using the proxy test variance \(\tilde{\Sigma}_{\text{target}}\). To tackle the error (2), there are two possible solutions as shown below. Based on the theoretical interpretation, we should choose different strategy based on the property of the teacher embedding model.

\[S_{\text{vision+language}}=\arg\max_{S}\operatorname{Tr}\left( \tilde{\Sigma}_{\text{target}}(\sum_{\bm{x}^{vl}\in S}\bar{G}_{v}^{\top}\bm{x} ^{v}(\bm{x}^{l})^{\top}\bar{G}_{l})\right)\] \[S_{\text{vision only}}=\arg\max_{S}\operatorname{Tr}\left(\tilde{ \Sigma}_{\text{target}}(\sum_{\bm{x}^{vl}\in S}\bar{G}_{v}^{\top}\bm{x}^{v}( \bm{x}^{v})^{\top}\bar{G}_{v})\right)\]

**Theorem A.2** (Main).: _Under the assumption of Lemma A.1,_

\[\Delta^{\rho}(S) \leq\text{noise}+\frac{1}{\rho}\|\bar{\Sigma}_{\text{target}}- \Sigma_{\text{target}}\|\|\Sigma_{S}-\Sigma_{\text{best}}\|_{*}\] \[+\frac{1}{\rho}\begin{cases}\epsilon_{vsl}^{S}&\text{(vision+ language)}\\ \epsilon_{v}^{S}+\sqrt{1-\frac{1}{|S|}\sum_{i\in[S]}\langle\bm{z}^{v},\bm{z}^{l} \rangle}\end{cases}\quad\text{(vision only)}\]

Firstly, it is evident that the greater the difference between \(\bar{\Sigma}_{\text{target}}\) and \(\Sigma_{\text{target}}\), the less improvement we can expect. Moreover, in scenarios where \(\epsilon_{l}\) is large (indicating lower accuracy in the language part) while \(\epsilon_{v}\) is small (indicating higher accuracy in the vision part), it may be advisable to opt for vision-only embeddings. However, the learner should also consider the term \(\sqrt{1-\frac{1}{|S|}\sum_{i\in S}\langle\bm{z}^{v},\bm{z}^{l}\rangle}\), which represents the alignment between the ground truth visual and language latent vectors, essentially reflecting the intrinsic quality of the data. If this term is already significant, relying solely on vision information as a proxy for language information could lead to suboptimal results.

#### a.2.3 Detailed proofs

**Lemma A.2**.: _Let_

\[\hat{G}_{v},\hat{G}_{l}=\arg\min_{G_{v},G_{l}\in\mathbb{R}^{d\times r}}\mathcal{ L}(G_{v},G_{l})\] (7)

_Then we have_

\[\hat{G}_{v}\hat{G}_{l}^{\top}=\frac{1}{\rho}G_{v}^{*}\Sigma_{S}(G_{l}^{*})^{ \top}+P_{1}+P_{2}+P_{3}+P_{4}\] (8)

_where noise terms \(P_{i}\) are defined in (12), (13), (14) and (15)._

Proof.: Note that \(s_{ij}=(\bm{x}_{j}^{l})^{\top}G_{l}G_{v}^{\top}\bm{x}_{i}^{v}=\text{Tr}(G_{v}^ {\top}\bm{x}_{i}^{v}(\bm{x}_{j}^{l})^{\top}G_{l})\), like the proof of Corollary B.1. in [46], we have

\[\mathcal{L}(G_{v},G_{l}) = \frac{\sum_{i\in S}\sum_{j\in S}(s_{ij}-s_{ii})}{|S|(|S|-1)}+ \frac{\rho}{2}\frac{|S|}{|S|-1}\|G_{v}G_{l}^{\top}\|_{F}^{2}\] \[= \frac{\sum_{i\in S}\sum_{j\in S}s_{ij}-|S|\sum_{i\in S}s_{ii}}{|S| (|S|-1)}+\frac{\rho}{2}\frac{|S|}{|S|-1}\|G_{v}G_{l}^{\top}\|_{F}^{2}\] \[= -\,\text{Tr}\left(G_{v}^{\top}\left[\frac{1}{|S|-1}\sum_{i\in S} \bm{x}_{i}^{v}(\bm{x}_{i}^{l})^{\top}-\frac{|S|}{|S|-1}\bar{\bm{x}}^{v}(\bar{ \bm{x}}^{l})^{\top}\right]G_{l}\right)+\frac{\rho}{2}\frac{|S|}{|S|-1}\|G_{v}G _{l}^{\top}\|_{F}^{2}\] \[=: -\,\text{Tr}(G_{v}^{\top}\Gamma G_{l})+\frac{\rho}{2}\frac{|S|}{| S|-1}\|G_{v}G_{l}^{\top}\|_{F}^{2}\]

where \(\bar{\bm{x}}^{vl}:=(\sum_{i\in S}\bm{x}_{i}^{vl})/|S|\). Then by the Eckart-Young-Mirsky Theorem (For example, Theorem 2.4.8 in Golub et al. [47]), we know that

\[\arg\min_{G_{v}\in\mathbb{R}^{d\times r},G_{l}\in\mathbb{R}^{d \times r}}\mathcal{L}(G_{v},G_{l})\] \[= \arg\max_{G_{v}\in\mathbb{R}^{d\times r},G_{l}\in\mathbb{R}^{d \times r}}\text{Tr}(G_{v}^{\top}\Gamma G_{l})-\frac{\rho}{2}\frac{|S|}{|S|-1} \|G_{v}G_{l}^{\top}\|_{F}^{2}\] \[= \{(G_{v},G_{l})\in\mathbb{R}^{d\times r}\times\mathbb{R}^{d \times r}:G_{v}G_{l}^{\top}=\frac{1}{\rho}\frac{|S|-1}{|S|}\text{SVD}_{r}( \Gamma)\}\qquad\text{(Eckart-Young-Mirsky Theorem)}\]

where the notation \(\text{SVD}_{r}(\Gamma)\) means choosing the first \(r\) components of the matrix \(\Gamma\). Further note that

\[\Gamma = \frac{1}{|S|-1}\sum_{i\in S}\bm{x}_{i}^{v}(\bm{x}_{i}^{l})^{\top}- \frac{|S|}{|S|-1}\bar{\bm{x}}^{v}(\bar{\bm{x}}^{l})^{\top}\] (9) \[=: P_{0}+P_{1}+P_{2}+P_{3}+P_{4}\] (10)Here note that \(\Sigma_{S}=\frac{1}{|S|}\sum_{i\in S}\bm{z}_{i}^{v}(\bm{z}_{i}^{l})^{\top}\), we have \(P_{i}\) as follows:

\[P_{0} := \frac{|S|}{|S|-1}G_{v}^{*}\cdot\Sigma_{S}\cdot(G_{l}^{*})^{\top}\] (11) \[P_{1} := \frac{1}{|S|-1}G_{v}^{*}\sum_{i\in S}\bm{z}_{i}^{v}(\bm{\xi}_{i}^ {l})^{\top}\] (12) \[P_{2} := \frac{1}{|S|-1}\sum_{i\in S}\bm{\xi}_{i}^{v}(\bm{z}_{i}^{l})^{\top }(G_{l}^{*})^{\top}\] (13) \[P_{3} := \frac{1}{|S|-1}\sum_{i\in S}\bm{\xi}_{i}^{(1)}(\bm{\xi}_{i}^{(2)} )^{\top}\] (14) \[P_{4} := -\frac{|S|}{|S|-1}\bm{\bar{x}}^{v}(\bm{\bar{x}}^{l})^{\top}\] (15)

It's clear that the rank of the matrix \(P_{0}\) is no more than \(r\), so \(\mathrm{SVD}_{r}(P_{0})=P_{0}\). And for \(i\in\{1,2,3,4\}\), \(P_{i}\) are noise terms with \(\mathbb{E}[P_{i}]=O\). 

**Lemma A.3**.: _For any fixed \(S\), w.h.p \(1-\delta\) the noise term can be upper bounded by \(\sqrt{\frac{d\log(1/\delta)}{|S|}}\)_

Proof.: To upper bound the P1 and P2, we have

\[\|\sum_{i}\bm{z}_{i}^{vl}(\xi_{i}^{vl})^{\top}\|_{*}^{2}=\mathrm{ Tr}\left(\sum_{i,j}\xi_{i}^{vl}(\bm{z}_{i}^{vl})^{\top}\bm{z}_{j}^{vl}\xi_{j}^{vl }\right)=\sum_{i,j}(\bm{z}_{i}^{vl})^{\top}\bm{z}_{j}^{vl}(\xi_{j}^{vl})^{ \top}\xi_{i}^{vl}\] \[\mathbb{E}\|\sum_{i}\bm{z}_{i}^{vl}(\xi_{i}^{vl})^{\top}\|_{*}^{2} =\mathbb{E}\left[\sum_{i}(\bm{z}_{i}^{vl})^{\top}\bm{z}_{i}^{vl}(\xi_{i}^{vl}) ^{\top}\xi_{i}^{vl}\right]=|S|d\]

Regarding each \((\bm{z}_{i}^{vl})^{\top}\bm{z}_{j}^{vl}(\xi_{j}^{vl})^{\top}\xi_{i}^{vl}\) as weakly dependent variable, then by using Bernstein inequality, we have, with high probability \(1-\delta\),

\[\|\sum_{i}\bm{z}_{i}^{vl}(\xi_{i}^{vl})^{\top}\|_{*}^{2}\leq|S|d+\sqrt{d|S|^{2 }\sigma_{\xi}^{2}\log(1/\delta)}\leq|S|d\sqrt{\log(1/\delta)}\]

So \(\frac{1}{|S|}\|\sum_{i}\bm{z}_{i}^{vl}(\xi_{i}^{vl})^{\top}\|_{*}\leq\sqrt{ \frac{d\log(1/\delta)}{|S|}}\). Note that \(\|\bm{\bar{x}}^{vl}\|\lesssim\sqrt{\frac{\log(|S|d)}{|S|}}\) (like Proposition 2.5 in Wainwright et al. [48]), it is easy to see that P3 ad P4 are the low order terms if \(\delta\lesssim\frac{1}{|S|d}\). 

**Lemma A.4** (Intuition behind VAS).: _With high probability \(1-\delta\), suppose the hind-side best subset has at least \(\underline{n}\) number of samples, then we have_

\[\Delta(S)=\frac{1}{\rho}\max_{S^{\prime}\in D_{\text{max}}}\left(\mathrm{Tr} \left(\Sigma_{\text{target}}(\Sigma_{S^{\prime}}-\Sigma_{S})\right)\right)+ \sqrt{\frac{d\log(1/\delta)}{\underline{n}}}+\sqrt{\frac{d\log(1/\delta)}{|S|}}\]

Proof.: For any learnt \(G_{v},G_{l}\) based on dataset \(S\), we have

\[\mathcal{L}_{\text{test}}(G_{v},G_{l}) =\mathrm{Tr}(G_{v}^{\top}\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{ \text{target}}}[\bm{x}^{v}(\bm{x})^{\top}]G_{l})\] \[=\mathrm{Tr}(\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{\text{target} }}[\bm{x}^{v}(\bm{x})^{\top}]G_{l}G_{v}^{\top})\] \[=\frac{1}{\rho}\mathrm{Tr}\left(\mathbb{E}_{\bm{x}_{vl}\sim \mathcal{D}_{\text{target}}}[\bm{x}^{v}(\bm{x})^{\top}]G_{l}^{\top}\Sigma_{S}( G_{v}^{*})^{\top}\right)-\mathrm{Tr}\left(\mathbb{E}_{\bm{x}_{vl}\sim \mathcal{D}_{\text{target}}}[\bm{x}^{v}(\bm{x})^{\top}]\text{noise}_{S}\right)\] \[=\frac{1}{\rho}\mathrm{Tr}\left((G_{v}^{*})^{\top}\mathbb{E}_{\bm{x }_{vl}\sim\mathcal{D}_{\text{target}}}[\bm{x}^{v}(\bm{x})^{\top}]G_{l}^{*} \Sigma_{S}\right)-\mathrm{Tr}\left(\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{ \text{target}}}[\bm{x}^{v}(\bm{x})^{\top}]\text{noise}_{S}\right)\] \[=-\frac{1}{\rho}\mathrm{Tr}\left(\Sigma_{\text{target}}\Sigma_{S} \right)-\mathrm{Tr}\left(\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{\text{target}}} [\bm{x}^{v}(\bm{x})^{\top}]\text{noise}_{S}\right)\]Here the first equation comes from Theorem A.4 and the third equation comes from Lemma A.2.

Consequently, we have

\[-\min_{S^{\prime}\in D_{\text{train}}}\mathcal{L}_{\text{test}}( \mathcal{A}(S^{\prime})) =\max_{S^{\prime}\in D_{\text{train}}}\left(\frac{1}{\rho}\operatorname {Tr}\left(\Sigma_{\text{target}}\Sigma_{S^{\prime}}\right)+\operatorname{Tr} \left(\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{\text{target}}}[\bm{x}^{v}(\bm {x}^{l})^{\top}]\text{noise}_{S^{\prime}}\right)\right)\] \[\leq\frac{1}{\rho}\max_{S^{\prime}\in D_{\text{train}}}\left( \operatorname{Tr}\left(\Sigma_{\text{target}}\Sigma_{S^{\prime}}\right)\right)+ \|\mathbb{E}_{\bm{x}_{vl}\sim\mathcal{D}_{\text{target}}}[\bm{x}^{v}(\bm{x}^{ l})^{\top}]\|\|\text{noise}_{S^{\prime}}\|_{*}\] \[\leq\frac{1}{\rho}\max_{S^{\prime}\in D_{\text{train}}}\left( \operatorname{Tr}\left(\Sigma_{\text{target}}\Sigma_{S^{\prime}}\right)\right)+ \mathcal{O}\left(\sqrt{\frac{d\log(1/\delta)}{\underline{n}}}\right)\]

Therefore, we have the final result as

\[\Delta(S) =\mathcal{L}_{\text{test}}(\hat{G}_{vl})-\min_{S^{\prime}\in D_{ \text{train}}}\mathcal{L}_{\text{test}}(\mathcal{A}(S^{\prime}))\] \[=\frac{1}{\rho}\max_{S^{\prime}\in D_{\text{train}}}\left( \operatorname{Tr}\left(\Sigma_{\text{target}}(\Sigma_{S^{\prime}}-\Sigma_{S}) \right)\right)+\mathcal{O}\left(\sqrt{\frac{d\log(1/\delta)}{\underline{n}}}+ \sqrt{\frac{d\log(1/\delta)}{|S|}}\right)\]

**Theorem A.3** (Main).: _Under the assumption of Lemma A.1, we have_

\[\Delta(S) \leq\text{noise}+\|\bar{\Sigma}_{\text{target}}-\Sigma_{\text{ target}}\|\|\Sigma_{S}-\Sigma_{\text{best}}\|_{*}\] \[+\begin{cases}\epsilon_{v*l}^{S}\quad\text{(vision+language)}\\ \left(\epsilon_{v}^{S}+\sqrt{1-\frac{1}{|S|}\sum_{i\in[S]}\langle\bm{z}^{v}, \bm{z}^{l}\rangle\rangle}\right)\quad\text{(vision only)}\end{cases}\]

Proof.: Based on Lemma A.1, we will focus on the error cause from selecting subset \(S\), that is, \(\operatorname{Tr}\Sigma_{\text{target}}\Sigma_{S}\). Since the exact \(\Sigma_{\text{target}}\) is unknown, we assume the access to some proxy \(\bar{\Sigma}_{\text{target}}\) instead.

Recall that, for any \(S\), we have ground-truth \(\Sigma_{S}=\mathbb{E}_{\bm{x}_{vl}\in S}\bm{z}^{v}(\bm{z}^{l})^{\top}\). Unfortunately, this is not directly observable by the learner. Instead, the learner is able to observe some proxy \(\bar{\Sigma}_{S}\) based on the teacher model \(\bar{G}_{vl}\) and therefore solving

\[\operatorname*{arg\,max}_{S}\operatorname{Tr}\left(\bar{\Sigma}_{\text{target }}\bar{\Sigma}_{S}\right)\]

and therefore, denote \(\Sigma_{\text{best}}=\operatorname*{arg\,max}_{S^{\prime}\in D_{\text{train} }}\operatorname{Tr}\left(\Sigma_{\text{target}}\Sigma_{S^{\prime}}\right)\)

\[\operatorname{Tr}\left(\Sigma_{\text{target}}(\Sigma_{\text{ best}}-\Sigma_{S})\right) =\operatorname{Tr}\left(\bar{\Sigma}_{\text{target}}(\Sigma_{ \text{best}}-\bar{\Sigma}_{S})\right)+\operatorname{Tr}\left(\bar{\Sigma}_{ \text{target}}(\bar{\Sigma}_{S}-\Sigma_{S})\right)+\operatorname{Tr}\left(( \Sigma_{\text{target}}-\bar{\Sigma}_{\text{target}})(\Sigma_{\text{best}}- \Sigma_{S})\right)\] \[\leq\operatorname{Tr}\left(\bar{\Sigma}_{\text{target}}(\bar{ \Sigma}_{S}-\Sigma_{S})\right)+\operatorname{Tr}\left((\Sigma_{\text{target}}- \bar{\Sigma}_{\text{target}})(\Sigma_{\text{best}}-\Sigma_{S})\right)\] \[\leq\|\Sigma_{\text{target}}\|\|\bar{\Sigma}_{S}-\Sigma_{S}\|_{*}+ \|\bar{\Sigma}_{\text{target}}-\Sigma_{\text{target}}\|\|\Sigma_{S}-\Sigma_{ \text{best}}\|_{*}\]

where the first inequality is by the definition of \(\bar{\Sigma}_{S}\) and the second inequality comes from holder's inequality. Now the key is to upper bound \(\|\bar{\Sigma}_{S}-\Sigma_{S}\|_{*}\) based on our chosen strategy.

In option 1, we use the clip embedding from both visual and language modal. That is, choose \(\bar{\Sigma}_{S}=\sum_{\bm{x}_{vl}\in S}(\bar{G}_{v})^{\top}\bm{x}^{v}(\bm{x}^{ l})^{\top}\bar{G}_{l}\). Then we have

\[\|\bar{\Sigma}_{S}-\Sigma_{S}\|_{*}\leq\frac{1}{|S|}\|\sum_{\bm{x}_{vl}\in S}( \bar{G}_{v})^{\top}\bm{x}^{v}(\bm{x}^{l})^{\top}\bar{G}_{l}-\sum_{\bm{x}_{vl} \in S}\bm{z}^{v}(\bm{z}^{l})^{\top}\|_{*}\leq\epsilon_{v*l}^{S}\]

In option 2, we use the clip embedding from language model only. That is choose \(\bar{\Sigma}_{S}=\sum_{\bm{x}_{vl}\in S}\bar{G}_{v}^{\top}\bm{x}^{v}(\bm{x}^{ v})^{\top}\bar{G}_{v}\). Then, by definition of \(\epsilon_{S}\), we have

\[\|\bar{\Sigma}_{S}-\Sigma_{S}\|_{*} \leq\frac{1}{|S|}\|\sum_{\bm{x}_{vl}\in S}\bar{G}_{v}^{\top}\bm{x}^ {v}(\bm{x}^{v})^{\top}\bar{G}_{v}-\sum_{\bm{x}_{vl}\in S}\bm{z}^{v}(\bm{z}^{v})^ {\top}\|_{*}+\frac{1}{|S|}\|\sum_{\bm{x}_{vl}\in S}\bm{z}^{v}(\bm{z}^{v})^{\top}- \Sigma_{S}\|_{*}\] \[\leq\epsilon_{v}^{S}+\frac{1}{|S|}\|\sum_{\bm{x}_{vl}\in S}\bm{z}^ {v}(\bm{z}^{v})^{\top}-\Sigma_{S}\|_{*}\]Now to further bound the second term, we have

\[\frac{1}{|S|}\|\sum_{\bm{x}_{vl}\in S}\bm{z}^{v}(\bm{z}^{v})^{\top}- \Sigma_{S}\|_{*} \leq\frac{1}{|S|}\|Z_{v}^{\top}\|_{*}\|Z_{v}-Z_{l}\|_{*}\] \[=\frac{1}{|S|}\sqrt{\operatorname{Tr}Z_{v}Z_{v}^{\top}}\sqrt{ \operatorname{Tr}(Z_{v}-Z_{l})^{\top}(Z_{v}-Z_{l})}\] \[=\frac{1}{|S|}\sqrt{\operatorname{Tr}(I_{n\times n})}\sqrt{2 \operatorname{Tr}\left(I_{n\times n}-Z_{v}Z_{l}^{\top}\right)}\] \[=\frac{1}{|S|}\sqrt{2|S|(|S|-\sum_{i\in[S]}\langle\bm{z}^{v},\bm{ z}^{l}\rangle)}\] \[=\sqrt{1-\frac{1}{|S|}\sum_{i\in[S]}\langle\bm{z}^{v},\bm{z}^{l} \rangle)}\]

Therefore, we finish the proof. 

**Theorem A.4** (A simplified version of test loss).: _Under the assumption that both \(\bm{z}_{vl},\xi_{vl}\) is zero-mean, maximizing the clip score gap is equivalent to maximize the clip score of the same sample._

\[\mathcal{L}_{\text{target}}(G_{v},G_{l}):=-\mathbb{E}_{\bm{x}_{vl}\sim \mathcal{D}_{\text{target}}}\langle G_{v}^{\top}\bm{x}_{v},G_{l}^{\top}\bm{x} _{l}\rangle\]

Proof.: For any \(\bm{x}_{vl}\), we have

\[\mathbb{E}_{\bm{x}_{vl}^{\prime}\sim\mathcal{D}_{\text{target}}} (\langle G_{v}^{\top}\bm{x}_{v},G_{l}^{\top}\bm{x}_{l}^{\prime}\rangle- \langle G_{v}^{\top}\bm{x}_{v},G_{l}^{\top}\bm{x}_{l}\rangle)\] \[=\langle G_{v}^{\top}\bm{x}_{v},G_{l}^{\top}\mathbb{E}_{\bm{x}_{ v}^{\prime}\sim\mathcal{D}_{\text{target}}}(\bm{x}_{l}^{\prime}-\bm{x}_{l})\rangle\] \[=-\langle G_{v}^{\top}\bm{x}_{v},G_{l}^{\top}\bm{x}_{l}\rangle\]Appendix B Illustration of Different Directions for Data Selection in Multimodal Contrastive Learning

We summarize our main idea of categorizing the current top data selection methods in Figure 4.

## Appendix C Details of Experiments

### Computation Cost

Our algorithm can significantly reduce the computational cost compared to many existing works as shown in Table 5. For example, when the CLIP embeddings are obtained (cost about 50 hours for CLIP-B/32), both T-MARS [12] and MLM [42] still require more than 900 hours data pre-processing time to extract the required information from 110M size dataset of DataComp-medium, while we only need about 5 hours. On the other hand, DFN, although has a similar forward speed (i.e. preprocessing time), requires retraining a new CLIP teacher model on the HQITP-350M, which is larger than DataComp-medium.

We give some details in estimating the preprocessing time of other methods:

* For **T-MARS** and \(\mathbb{D}^{2}\) pruning, we run their official code on DataComp-small (11M) data, and simply scale the preprocessing time by 10 for DataComp-medium, given that the preprocessing time for T-MARS is proportional to the size of the pretraining dataset, while \(\mathbb{D}^{2}\) pruning is no faster than linear.

Figure 4: Illustration of different directions for data selection methods for multimodal contrastive learning. Here we use four colors to denote the four main resources we can obtain: CLIP teacher model, downstream target data (which is much smaller than the external multimodal dataset or pretraining dataset), the external image-text dataset, and the external non-CLIP model. **Direction 1** denotes the methods that only use the original OAI CLIP teacher model and the downstream target data. **Direction 2** represents the methods that use external datasets to train a new CLIP teacher model for improving filtering, like DFN [2]. **Direction 3** denotes the methods that use external non-CLIP model to select the data that may be heuristically helpful for downstream tasks, like image without too much text or be more special. In general, _D1 method using only CLIP embedding, like negCLIPLoss, is orthogonal to D2. And both D1 and D2 can be combined with D3 to explore better filtering results._ In the experiments part of the main paper (Sec. 4), we further show that our proposed D1 methods: NormSim and negCLIPLoss, can outperform all the D3 baselines except the best method “HYPE \(\cup\) DFN”. And we can achieve the new state-of-the-art by combining our methods with that method.

* For **MLM**, we get the estimated time from their paper. They mention that they need 6.1 minutes to process 10k samples on A100, which results in 1120 A100 hours for our dataset (110M). We need to mention that their estimation time of calculating CLIP embedding is inaccurate and we can do it much faster than their claim using the DataComp pipeline.
* For **Devil**, it needs to run the k-means clustering algorithm from the faiss library on the embedding space, which is estimated to cost 120 L40 hours on DataComp-medium. Using BLIP-2 [16] to scan the whole dataset will need about 470 A100 hours from the experimental details in [17]. From https://lambdalabs.com/gpu-benchmarks, we roughly assume that 120 L40 hours are at least comparable to 40 A100 hours for K-means clustering.
* For **HYPE**, they claim that MERU is as efficient as CLIP, but they still need at least 120 L40 hours for processing 110M data for their final score, since it uses the image embedding clusters on DataComp-medium obtained from running k-means clustering algorithm.

### Details of negCLIPLoss

We give the pseudocode of calculating negCLIPLoss in Algorithm 1, which is specially designed for pytorch-style parallel matrix calculation. It can be fully accelerated and the computation cost introduced by the normalization term is negligible compared with the training time or preprocessing time of other top baselines as detailed in Table C.1.

In negCLIPLoss, we need to get the batch size \(|B|\) and the value of the learnable temperature parameter \(\tau\) at the final step of the teacher model pretraining stage. For OAI CLIP-L/14 and OAI CLIP-B/32, these values are \(\tau=0.01\) and \(|B|=32768\).

We also have an ablation study about the temperature parameter and batch size chosen for CLIP teacher models as shown in Table 6. We will see that in general, a larger batch size will result in better performance, and \(\tau=0.01,b=32768\) is the best choice for both OAI CLIP-B/32 and DFN-P. The reason for such a batch size is that a larger batch can contain more contrastive data pairs, which is also supported by the concentration result of the normalization term proved in Appendix A.1, and thus it can check the image-text matching between more different data. Therefore, we always consider the largest batch size 32768 which can fit into a single 24G GPU in the CLIP forward pass, which is also the OAI CLIP training batch size.

### Details of NormSim\({}_{2}\)-D

In this section, we illustrate the details of our NormSim\({}_{2}\)-D algorithm. The top-\(N\) selection method is aiming to achieve the object:

\[S=\arg\max_{|S|=N}\sum_{i\in S}\bar{f}_{v}(x_{i}^{v})^{\top}\left(\frac{1}{|X_ {\text{target}}|}\sum_{x_{i}\in X_{\text{target}}}\bar{f}_{v}(x_{i}^{v})^{ \top}\right)\bar{f}_{v}(x_{i}^{v})\] (16)

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline \multirow{2}{*}{**Type**} & **Filtering** & **Ext. Model** & **Size of** & **Preprocess** & **Training** & **Avg.** \\  & **Strategy** & **Used** & **Ext. Dataset** & **Time** & **Time** & **Avg.** \\ \hline D1 & \(\mathbb{D}^{2}\) Pruning [18] & **NA** & **NA** & \textgreater{}70 L40 h & 65 L40 h & 29.5 \\ D3 & T-MARS [12] & FAST [13] & **NA** & 950 L40 h & 65 L40 h & 34.1 \\ D3 & MLM [42] & LLAvA-1.5 [43, 44] & 50K & 1120 A100 h & 65 L40 h & 34.5 \\ D3 & Devil [14] & fasttext [15], BLIP-2 [16] & **NA** & 510 A10 h & 65 L40 h & 34.5 \\ D3 & HYPE [3] & MERU [45] & 27M & \textgreater{}120 L40 h & 65 L40 h & 31.9 \\ \hline D1 & **Ours (20\%)** & **NA** & **NA** & **5 L40 h** & 65 L40 h & **35.2** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of preprocessing time and external resources needed between our method and other D3 category methods. We skip DFN since it’s orthogonal to our negCLIPLoss method and we can directly improve it as mentioned in Table 1. Here since all the baselines below except MLM use a pretrained CLIP model, we only count the time that doesn’t contain that for inferring CLIP image/text embeddings (about 50 L40 hours for OAI CLIP-B/32), which is also adopted in DataComp benchmark [1]. The external dataset corresponds to the external multimodal dataset used for training or finetuning the external model. Notably, the preprocessing time for the following methods are all approximately linearly proportional to the amount of unfiltered pretrained dataset.

when the actual \(X_{\text{target}}\) is unknown. In practice, removing one data at a time is too slow. Therefore, we remove a batch of data for every step. In detail, if the number of steps is \(\tau\), and let \(\bar{\Sigma}_{\text{test},i}=\frac{1}{|S_{i}|}\sum_{j\in S_{i}}\bar{f}_{v}( \bm{x}_{j}^{v})\bar{f}_{v}(\bm{x}_{j}^{v})^{\top}\) where \(S_{i}\) is the selected subset at step \(i\), then we will remove the data satisfies the following equation step-by-step until reaching the final subset size:

\[S_{i}\setminus S_{i+1}=\arg\min_{x_{t}\in S_{i}}\left[\bar{f}_{v}(x_{l}^{v})^{ T}\cdot\left(\frac{1}{|S_{i}|}\sum_{x_{t}\in S_{i}}\bar{f}_{v}(x_{t}^{v})\bar{f}_{v}(x_ {t}^{v})^{\top}\right)\cdot\bar{f}_{v}(x_{l}^{v})\right],\quad i\in\{0,\dots, \tau-1\}\]

Then we can detail the algorithm process of NormSim\({}_{2}\)-D in Algorithm 2. In general, the smaller the step size, the better the results. But in experiments, we find that it's already enough to get good results when \(\tau=500\).

### Details of Related Works

We add some details about the baselines used in our paper as follows.

* **Text-based filtering.**[1] proposes a text-based filtering that tries to select the data that contains caption overlapping with the class name from ImageNet-21K or ImageNet-1K.
* **Image-based filtering.**[1] also proposes a heuristic way to sample the visual content overlaps with ImageNet-1K classes. They first apply filtering by language (only choose English caption by fasttext [15]) and caption length (over two words and 5 characters). Then they cluster the image embeddings from training data to 100K groups using Faiss [49], and keep the groups whose cluster center is the nearest neighbor to at least one image embedding of ImageNet-1K image.

\begin{table}
\begin{tabular}{l c c c c c c} \hline
**OAI CLIP-B/32** & **Size** & **IN-1k** & **IN Dist. Shift** & **VTAB** & **Retr.** & **Avg.** \\ \hline
**CLIPScore (30\%)**[38] & 33M & 27.6 & 24.2 & 33.6 & 25.1 & 33.2 \\ \hline
**negCLIPLoss (30\%)** & & & & & & \\ \hline \(b=16384,\tau=0.01\) & 33M & **28.8** & 25.0 & 32.5 & 26.2 & 33.0 \\ \(b=16384,\tau=0.02\) & 33M & 28.6 & 24.8 & 33.3 & 25.3 & 33.1 \\ \(b=16384,\tau=0.07\) & 33M & 28.0 & 24.2 & 33.5 & 25.1 & 32.6 \\ \(b=32768,\tau=0.001\) & 33M & 16.0 & 13.9 & 25.1 & 19.4 & 24.4 \\ \(b=32768,\tau=0.005\) & 33M & 28.5 & 25.0 & 33.6 & **27.0** & 33.0 \\ \(b=32768,\tau=0.01\) & 33M & **28.8** & **25.1** & **33.7** & 26.6 & **33.6** \\ \(b=32768,\tau=0.02\) & 33M & 28.5 & 24.8 & 33.6 & 26.2 & 32.9 \\ \(b=32768,\tau=0.07\) & 33M & 28.2 & 24.5 & 32.8 & 25.2 & 32.7 \\ \hline
**negCLIPLoss (30\%) \(\cap\) NormSim\({}_{\infty}\)(Target)** & & & & & & \\ \hline \(b=16384,\tau=0.01\) & 22M & **32.4** & **27.4** & 34.5 & 26.1 & 34.7 \\ \(b=16384,\tau=0.02\) & 22M & 31.8 & 26.7 & 35.0 & 24.9 & 34.2 \\ \(b=16384,\tau=0.07\) & 22M & 31.0 & 26.3 & 35.0 & 25.5 & 33.9 \\ \(b=32768,\tau=0.005\) & 22M & 32.2 & 27.2 & 35.3 & **26.5** & 34.8 \\ \(b=32768,\tau=0.01\) & 22M & **32.4** & **27.4** & **35.9** & 26.3 & **35.2** \\ \hline
**DFN-P** & **Size** & **IN-1k** & **IN Dist. Shift** & **VTAB** & **Retr.** & **Avg.** \\ \hline
**negCLIPLoss** & & & & & & \\ \hline
15\%, \(b=16384,\tau=0.07\) & 16M & 31.0 & 27.0 & 35.2 & 26.8 & 34.2 \\
15\%, \(b=32768,\tau=0.01\) & 16M & **31.3** & 27.3 & **35.8** & 26.4 & 34.6 \\
17.5\%, \(b=16384,\tau=0.07\) & 19M & **31.3** & 27.2 & 33.5 & **27.6** & 33.5 \\
17.5\%, \(b=32768,\tau=0.01\) & 19M & 31.2 & **27.5** & 35.7 & 27.0 & **34.7** \\ \hline
**negCLIPLoss (17.5\%) \(\cap\) NormSim\({}_{\infty}^{8/32}\)(Target)** & & & & & & \\ \hline \(b=16384,\tau=0.07\) & 16M & 31.1 & **27.4** & 34.8 & **26.1** & 34.2 \\ \(b=32768,\tau=0.01\) & 16M & **31.6** & 27.3 & **37.2** & 25.5 & **35.7** \\ \hline \end{tabular}
\end{table}
Table 6: Ablation study about the temperature parameters \(\tau\) and batch size \(b\) for CLIP teacher model. The values obtained from the last training step of the teacher models are \(\tau=0.01,b=32768\) for OAI CLIP-B/32, OAI CLIP-L/14, and \(b=16384,\tau=0.07\) for DFN-P. In the main paper, we use \(b=32768,\tau=0.01\) for all three kinds of teacher models.

* \(\mathbb{D}^{2}\) **Pruning.**[18] tries to represent the dataset as an undirected graph for coreset selection. They assign the difficulty for each example and use message passing to update the difficulty score incorporating the difficulty of its neighboring examples, and finally try to keep both diverse and difficult subsets. For our experiments, we adhere to the default hyperparameters of \(\mathbb{D}^{2}\) on DataComp as specified in their official codebase.
* **T-MARS**[12] uses a text detection model like FAST [13] to filter out the data that only contain the texts of caption in the image and don't have other useful image features.
* **Devils**[14] combines many ways for data filtering. At the very first it filter data based on heuristic rules like text length, frequency of texts, and image size, and it also use CLIPScore for cross-modality matchment. Then it adopts target distribution alignment methods similar to image-based filtering, but instead of using ImageNet-1k only, it uses 22 downstream tasks as the target set. Further, it adopts external models fasttext [15] to remove non-English captions and image-captioning model BLIP-2 [50] to select images with MNIST-style digits.
* **MLM**[42] prompts GPT-4V to construct instruction data including the image-text data, and use it to fine-tune a smaller vision-language model like LLaVA-1.5 [43, 44] into a filtering network. Nevertheless, the number of parameters of LLaVA-1.5 is still much larger than CLIP, and thus LLaVA-1.5 has a much longer preprocessing time as mentioned in Table C.1.

### How to Choose Hyperparameters

The main hyper-parameters of our negCLIPLoss and NormSim are the target numbers for filtering (refer to Appendix C.2 for the setting of temperature and batch size), which is also the main concerns for all the top baselines like DFN, MLM, and T-MARS. In the case of DataComp settings, noting that all the top baselines in DataComp-medium benchmark keep the downsampling ratios ranging from 15% 30% to achieve the best results, we can set the sampling ratio as some previous baselines. Our method with OAI CLIP teacher model first selects the data with the top 30% negCLIPLoss, and then selects the top 66.7% NormSim scores to keep 20% of the original pool. We don't tune the target size carefully here for fair comparison.

In more general cases, we can recommend some **training-dataset-independent** thresholds for NormSim, since the scores only depends on the norm \(p\) and target data rather than other data in the pool. We recommend to set the threshold as 0.7 for NormSim\({}_{\infty}\)(Target) and 0.15 for NormSim\({}_{2}\)(IN-1k) in general. On the other hand for negCLIPLoss, note that like NormSim, CLIPScore is also training-dataset-independent, we recommend to first find the percentile of the data with CLIPScore=0.21, and then downsample the dataset using CLIPLoss until that particular percentile.

Overall, finding optimal filtering ratio for data selection algorithm is always difficult and out of the scope of this paper. From the paper about the scaling law for data filtering [51], downsampling size even depends on the computation budget. When you have more budget, you should sample more data for learning. And thus another possible solution is to use their fitting formula to get some recommended downsampling ratios.

At last, we also note that _in data selection problem, visualization is a simple but effective way for tuning parameters or finding downsampling ratios_. People can first randomly select a small subset (like 1000 data) on some pretraining data subset, and then calculate the target scores (CLIPScore, negCLIPLoss, NormSim or any other metrics) on them, and finally visualize the data corresponding to scores at different percentiles, like top 10%, 30%, 50% and 70% of the negCLIPLoss. In this way, we can determine the threshold of filtering directly by observating the data. We also give some visualization examples of our methods in Appendix E, We believe this is an effective way to give some guidance on how to roughly select the initial downsampling ratios.

### Discussion of NormSim

#### c.6.1 How NormSim\({}_{2}\) Connects to Selecting the Data in Principal Components.

For convenience, we let \(f(x_{t})\) denote the image embedding of the target data \(x_{t}\in X_{T}\), and \(f(x_{s})\) denotes the image embeddings of training data \(x_{s}\in X_{S}\). Then the definition of NormSim on a data \(x_{s}\) is

\[\text{NormSim}_{p}(X_{T},x_{s})=\left(\sum_{x_{t}\in X_{T}}[f(x_{t})^{\top}f(x _{s})]^{p}\right)^{1/p}\] (17)

Then when \(p=2\), we have

\[\text{NormSim}_{2}(X_{T},x_{s})= \left(\sum_{x_{t}\in X_{T}}[f(x_{s})^{\top}f(x_{t})]\cdot[f(x_{t} )^{\top}f(x_{s})]\right)^{1/2}\] (18) \[= \left(f(x_{s})^{\top}\cdot\sum_{x_{t}\in X_{T}}[f(x_{t})f(x_{t}) ^{\top}]\cdot f(x_{s})\right)^{1/2}\] (19) \[\propto \left[f(x_{s})^{\top}\left(\frac{1}{|X_{T}|}\sum_{x_{t}\in X_{T} }f(x_{t})f(x_{t})^{\top}\right)f(x_{s})\right]^{1/2}\] (20)Note that \(\Lambda=\frac{1}{|X|}\sum_{x_{i}\in X_{T}}f(x_{t})f(x_{t})^{\top}\) is the variance matrix of the target image embeddings. Then using \(\text{NormSim}_{2}\) for filtering, we have

\[S =\arg\max_{|S|=N}\sum_{x_{s}\in X_{S}}\text{NormSim}_{2}(X_{T},x_{s})\] (21) \[\text{NormSim}_{2}(X_{T},x_{s}) =f(x_{s})^{\top}\cdot\Lambda\cdot f(x_{s})\] (22) \[=f(x_{s})^{\top}U\cdot S\cdot U^{\top}f(x_{s})\] (23) \[=\sum_{j=1}^{r}s_{j}\cdot[f(x_{s})^{\top}u_{j}]^{2}\] (24)

Here \(\Lambda=USU^{\top}\) is the eigen decomposition of \(\Lambda\), where \(S=\text{diag}(s_{1},\ldots,s_{r})\) with \(s_{1}>\ldots>s_{r}\) are the matrix of eigenvalues, and \(U=[u_{1},\ldots,u_{r}]\in\mathbb{R}^{d\times r}\) are the corresponding eigenvectors (i.e., the principal component directions). Note that the column vectors of \(U\) and \(f(x_{s})\) are all unit vectors, (24) shows that \(\text{NormSim}_{2}\) select the data that match with the principal components, i.e., eigen directions \(u_{j}\) with large eigen values \(s_{j}\).

#### c.6.2 Why NormSim works well without expliclty considering data diversity.

We answer this question by the following reasons:

* Many top baselines, such as DFN and T-MARS, also don't explicitly consider diversity, yet they still provide good performance. Devil even shows that valuable data is worth sampling multiple times, which they call "quality duplication". Therefore, one important reason why NormSim works well without explicitly considering diversity may be that when the computing budget is limited, as in the DataComp benchmark, the model first needs to learn the most useful and representative data, which should be similar to some target data.
* Moreover, we chose validation data from 24 downstream tasks ranging from ImageNet to EuroSet, which may have covered a sufficiently diverse range of target examples for NormSim to calculate similarity. The diversity of the target data will consequently result in the diversity of the selected subset. And this also implies the importance of selecting a good target dataset.
* An additional reason may be that our proposed negCLIPLoss already implicitly selects more diverse data, as shown in Figure 1 of the main paper. If some training data are diverse, they will match less with other data and thus have a lower normalization term. This results in a larger negCLIPLoss and a higher probability of being sampled.

## Appendix D Additional Results

### Stability Analysis of Batch Sampling Numbers in negCLIPLoss

We show that negCLIPLoss is not sensitive to the number of random select batches \(K\) in Figure 5.

### Universality of negCLIPLoss over Different Teacher Models

We show the complete results of applying our methods to different teacher models like OAI CLIP-B/32 and DFN-P in Table 7. Detail descriptions are in Sec. 4.

### NormSim\({}_{\infty}\) is Better than Nearest Neighbor Selection

We also try to use near-neighbor selection for aligning downstream distribution. Here, we calculate the ranks of pretraining data for each target (the higher the rank, the higher the similarity), and then for each pre-train data, we keep its highest rank. Finally, we select the data with the highest ranks as the nearest neighbor selected subset.

In Table 8, we show that given the training data of 22 downstream tasks, our \(\text{NormSim}_{\infty}\) can outperform near neighbor selection under the same downsampling ratio. The reason may be that the distribution between the target and pretraining set is not well aligned, so if you force the algorithm to find the nearest train data for each target, that train data may be sometimes random and not helpful. On the other hand, \(\text{NormSim}_{\infty}\) will not select this kind of data. It will select the data whose best similarity score exceeds some general threshold, rather than just consider ranks.

### Vision-Only NormSim is Better than Using Both Vision and Language

In DataComp [1], they show that image-based filtering is better than text-based filtering. In our paper, we also do an ablation study to support this. Due to the restriction of computation resources, we run our \(\text{NormSim}_{2}\)(IN-1k) and \(\text{NormSim}_{2}\)-D on DataComp-small as an example. Since ImageNet-1k

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**OAI CLIP-B/32** & \begin{tabular}{c} **Dataset** \\ **Size** \\ \end{tabular} & \begin{tabular}{c} **IN-1k** \\ (1 sub-task) \\ \end{tabular} & \begin{tabular}{c} **IN Dist. Shift** \\ (5) \\ \end{tabular} & \begin{tabular}{c} **VTBAB** \\ (11) \\ \end{tabular} & \begin{tabular}{c} **Retrieval** \\ (3) \\ \end{tabular} & 
\begin{tabular}{c} **Avg.** \\ (38) \\ \end{tabular} \\ \hline CLIPScore (20\%) & 22M & 27.0 & 23.8 & 33.0 & 22.9 & 32.2 \\ CLIPScore (30\%) & 33M & 27.6 & 24.2 & 33.6 & 25.1 & 33.2 \\ \hline negCLIPLoss (20\%) & 22M & 28.9 & 24.8 & 34.3 & 24.3 & 33.0 \\ negCLIPLoss (30\%) & 33M & 28.8 & 25.1 & 33.7 & 26.6 & 33.6 \\ \hline negCLIPLoss (30\%) \(\cap\)\(\text{NormSim}_{\infty}\)(Target) & 22M & **32.4** & **27.4** & **35.9** & **26.3** & **35.2** \\ \hline \hline
**DFN-P** & & & & & & \\ \hline CLIPScore (15\%) & 16M & 25.9 & 23.3 & 32.9 & 21.9 & 31.6 \\ CLIPScore (17.5\%) & 19M & 30.2 & 26.8 & 34.1 & 26.5 & 33.8 \\ CLIPScore (20\%) & 22M & 29.7 & 26.8 & 33.0 & 27.0 & 33.1 \\ CLIPScore (30\%) & 33M & 28.4 & 24.7 & 33.2 & 26.8 & 32.7 \\ \hline negCLIPLoss (15\%) & 16M & 31.3 & 27.3 & 35.8 & 26.4 & 34.6 \\ negCLIPLoss (17.5\%) & 19M & 31.2 & **27.5** & 35.7 & 27.0 & **34.7** \\ negCLIPLoss (20\%) & 22M & 30.7 & 27.4 & 33.6 & **27.5** & 33.8 \\ negCLIPLoss (30\%) & 33M & 28.9 & 25.5 & 33.4 & 27.3 & 33.2 \\ \hline negCLIPLoss (30\%) \(\cap\)\(\text{NormSim}_{\infty}\)(Target) & 22M & 29.4 & 23.6 & 33.5 & 24.2 & 32.5 \\ negCLIPLoss (17.5\%) \(\cap\)\(\text{NormSim}_{\infty}\)(Target) & 16M & 31.5 & 26.4 & 34.6 & 25.4 & 34.4 \\ negCLIPLoss (17.5\%) \(\cap\)\(\text{NormSim}_{\infty}^{\text{80}32}\)(Target) & 16M & **31.6** & 27.3 & **37.2** & 25.5 & **35.7** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results on DataComp-medium from the top methods that use only OpenAI’s CLIP-B/32 model or public version of DFN (DFN-P).

Figure 5: Results of negCLIPLoss with a different number of batch samples (denoted as \(K\)) on DataComp-medium. Solid lines denote negCLIPLoss, while dashed lines denote CLIPScore. Here, we use OAI CLIP-L/14 as the pretrained model. We can see that once \(K\geq 5\), negCLIPLoss consistently outperforms CLIPScore across all subtask metrics. In the main paper, we set \(K=10\).

only has labels rather than long texts for describing images, we need to generate the caption before calculating NormSim\({}_{2}\)(IN-1k). We select 80 templates as the original CLIP paper [4], generate prompts for each class, and take the mean of their embeddings as the representative text embedding for images within that class.

The results are in Table 9. We can see that for both metrics, we have **"image only" > "image \(\times\) text" > "text only"**. We believe the reason for NormSim\({}_{2}\)(IN-1k) is that the images themselves can convey significantly more features than the text prompts generated by labels. For NormSim\({}_{2}\)-D, it should be related to the large amounts of low-quality captions in the web-curated dataset. And "image \(\times\) text" will also be influenced by the informativeness and the quality of captions. In short, for NormSim, using vision-only embeddings is a best choice.

## Appendix E Additional Visualization

We further visualize8 more data with different negCLIPLoss in Figure 6, 7 and 8. And similar for NormSim\({}_{\infty}\)(Target) in Figure 9, 10 and 11.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Filtering Strategy** & **IN-1k** & **VTAB** & **Avg.** \\ \hline negCLIPLoss (30\%) & 27.9 & 33.2 & 32.9 \\ \hline Nearest Neibor Selection & 31.5 & 34.9 & 34.0 \\ NormSim\({}_{\infty}\)(Target) & **31.7** & **36.0** & **35.0** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison between NormSim\({}_{\infty}\) and nearest neighbor selection. We use OAI CLIP-L/14 as the teacher model and assume both methods have been intersected with negCLIPLoss (30%). The size of the selected subset is 22M.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Filtering Strategy**\(\cap\) CLIP score (45\%) & **IN-1k** & **IN Dist. Shift** & **VTAB** & **Retrieval** & **Average** \\ \hline Random Sampling & 4.2 & 4.9 & 17.2 & 11.6 & 15.6 \\ \hline
**NormSim** (IN-1k, image) & **5.2** & **5.5** & 19.0 & **12.2** & **17.4** \\
**NormSim** (IN-1k, text) & 3.9 & 4.2 & 16.3 & 11.3 & 14.9 \\
**NormSim** (IN-1k, image \(\times\) text) & 4.3 & 4.9 & 17.5 & 11.8 & 15.9 \\ \hline
**NormSim-D** (image) & 4.7 & 5.4 & **19.7** & 11.7 & 17.3 \\
**NormSim-D** (text) & 3.5 & 4.1 & 16.7 & 11.1 & 15.4 \\
**NormSim-D** (image \(\times\) text) & 3.6 & 4.2 & 18.4 & 11.1 & 15.8 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation Study on the NormSim and its variants on DataComp-small (11M). All experiments first select 45% data based on the CLIP score, then use corresponding approaches to obtain 3.3M data.“image” or “text” means using the variance of image or text embeddings to represent \(\bar{\Sigma}_{\text{target}}\), and “image \(\times\) text” means representing \(\bar{\Sigma}_{\text{target}}\) with the cross-covariance of image and text embeddings.

Figure 6: Visualization of a small subset whose negCLIPLoss rank top 100% high in DataComp-medium.

Figure 7: Visualization of a small subset whose negCLIPLoss rank top 50% high in DataComp-medium.

Figure 8: Visualization of a small subset whose negCLIPLoss rank top 10% high in DataComp-medium.

Figure 9: Visualization of the images from a small subset whose NormSim\({}_{\infty}\)(Target) rank top 100% high in DataComp-medium.

Figure 10: Visualization of the images from a small subset whose NormSim\({}_{\infty}\)(Target) rank top 50% high in DataComp-medium.

Figure 11: Visualization of the images from a small subset whose NormSim\({}_{\infty}\)(Target) rank top 10% high in DataComp-medium.

## Appendix F NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes we clearly define 1. the benchmark we are using; 2.the methods with its key insights 3.the empirical improvement. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss this briefly in the last section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The full version of theory of NormSim results are in Appendix. A and we provide all the assumptions and proofs. We briefly mentioned this in Sec. 3.2. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The main results are in the Sec. 4. We also provide experiment details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code will be provided according to Neurips code submission guidance. After got accepted, we will open source that. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The main results are in the Sec.4. We also provide experiment details in Appendix C Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Almost all existing works, like DFN, HYPE, and MLM, only run the training once on DataComp-medium. Training on a 128M size dataset is very costly and relatively stable, so it is commonly believed that there is no need to rerun experiments with different training seeds. In the experiments, we fix all the training seeds to be 0 for fair comparison. For our algorithm, most of them are deterministic. The only one involving randomness is negCLIPLoss, which requires resampling K=10 times. For it we provide a sensitivity analysis in Fig. 5. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We discuss the computing cost estimation and comparison in Appendix C.1. We didn't explicitly calculate the memories since it is quite standard under the DataComp benchmark. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: This research focuses on the methodology part of data selection. All experiments are performed under the existing standard dataset. So as long as those datasets itself maybe harmless, our research will not make any negative impact. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper will only provide UID's of selected data from existing datasets (DataComp-medium [1]). This paper will not release any model or new dataset. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the DataComp [1] which introduces the URL for the dataset and the code/models used to implement the benchmark. Guidelines: * The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. All metrics are fixed evaluations. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.