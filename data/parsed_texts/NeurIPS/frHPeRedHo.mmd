# Agnostically Learning Single-Index Models using Omnipredictors

 Aravind Gollakota

Aprikshit Gopalan

Apple

Adam R. Klivans

UTAustin

Konstantinos Stavropoulos

UT Austin

###### Abstract

We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary monotone and Lipschitz activations. All prior work either held only in the realizable setting or required the activation to be known. Moreover, we only require the marginal to have bounded second moments, whereas all prior work required stronger distributional assumptions (such as anticoncentration or boundedness). Our algorithm is based on recent work by Gopalan et al. (2023) on omniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and relies on the relationship between Bregman divergences (or matching losses) and \(\ell_{p}\) distances. We also provide new guarantees for standard algorithms like GLMtron and logistic regression in the agnostic setting.

## 1 Introduction

Generalized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frameworks in statistics and supervised learning McCullagh (1984); Agresti (2015), capturing and generalizing basic models such as linear and logistic regression. In the GLM framework, labeled examples \((\mathbf{x},y)\) are assumed to satisfy \(u(\mathbb{E}[y|\mathbf{x}])=\mathbf{w}\cdot\mathbf{x}\) (or \(\mathbb{E}[y|\mathbf{x}]=u^{-1}(\mathbf{w}\cdot\mathbf{x})\)), where \(u\) is a known monotone function (called the link function) and \(\mathbf{w}\) is an unknown vector. Single-Index Models (SIMs) are a generalization in which the monotone link function \(u\) is also unknown.

In the realizable setting where the labels are indeed generated according to a GLM with a known Lipschitz link function, the GLMtron algorithm of Kakade et al. (2011) is a simple and efficient learning algorithm. When the ground truth is only assumed to be a SIM (i.e. the link function is unknown), it can be learned efficiently by the Isotron algorithm (Kalai and Sastry, 2009; Kakade et al., 2011).

In this work, we consider the significantly more challenging _agnostic_ setting, where the labels are arbitrary and not necessarily realizable by any SIM. Importantly, we do not fix a link function in advance; our goal is to output a predictor that has squared error comparable to that of the optimal SIM with an arbitrary monotone and Lipschitz link function. We can equivalently view this as a natural squared-error regression problem in which the final optimality guarantee must hold with respect to all SIMs with bounded weights and monotone, Lipschitz link functions.1

Footnote 1: In this context, in recent times it has also been common to refer to the class of GLMs (resp. SIMs) as the class of _single neurons_ with known (resp. unknown) activation functions.

Formally, consider a distribution \(D\) over \(\mathbb{R}^{d}\times[0,1]\) and denote the squared error of a function \(h:\mathbb{R}^{d}\to\mathbb{R}\) by \(\mathrm{err}_{2}(h)=\mathbb{E}_{(\mathbf{x},y)\sim D}[(y-h(\mathbf{x}))^{2}]\). Let \(\mathrm{opt}(\mathrm{SIM})\) denote optimal value of \(\mathrm{err}_{2}(h)\) over all SIMs \(h\) with bounded weights and arbitrary \(1\)-Lipschitz monotone activations (we call the inverse \(u^{-1}\) of a link function \(u\) the activation function). Given access to samples from \(D\), the goal of an agnostic learning algorithm is to compute a predictor \(p:\mathbb{R}^{d}\to[0,1]\) with error \(\mathrm{err}_{2}(p)\) that, with high probability over the samples, is comparable to the error of the optimal SIM:

\[\mathrm{err}_{2}(p)\leq\mathrm{opt}(\mathrm{SIM})+\varepsilon.\]Our main result is the first efficient learning algorithm with a guarantee of this form.

**Theorem 1.1** (Informal, see Theorem 3.1).: _Let \(\mathrm{SIM}_{B}\) denote the class of SIMs of the form \(\mathbf{x}\mapsto u^{-1}(\mathbf{w}\cdot\mathbf{x})\) for some \(1\)-Lipschitz function \(u^{-1}\) and \(\|\mathbf{w}\|_{2}\leq B\). Let \(D\) be any distribution over \(\mathbb{R}^{d}\times[0,1]\) whose marginal on \(\mathbb{R}^{d}\) has bounded second moments. There is an efficient algorithm (Algorithm 1) that agnostically learns \(\mathrm{SIM}_{B}\) over \(D\) up to error_

\[\mathrm{err}_{2}(p)\leq O\big{(}B\sqrt{\mathrm{opt}(\mathrm{SIM}_{B})}\big{)} +\varepsilon.\]

This result provides a guarantee comparable to that of the Isotron algorithm (Kalai and Sastry, 2009; Kakade et al., 2011) (which also tackles the SIM setting, where the link function is unknown) but for the challenging agnostic setting rather than the realizable setting (where \(\mathrm{opt}(\mathrm{SIM}_{B},D)=0\)). Moreover, Isotron's guarantees require the distribution to be supported on the unit ball, whereas we only require a mild second moment condition.

In our view, this result helps establish Algorithm 1 (and indeed any algorithm to compute omnipredictors, as we introduce and discuss shortly) as an efficient and powerful _off-the-shelf_ supervised learning algorithm, akin to random forests or gradient-boosted trees.

A natural question is whether our guarantees are near-optimal, e.g., whether we can obtain a guarantee of the form \(\mathrm{err}_{2}(p)\leq\mathrm{opt}(\mathrm{SIM})+\varepsilon\). However, there is strong evidence that such results cannot be obtained using efficient algorithms (Goel et al., 2019; Diakonikolas et al., 2020; Ouel et al., 2020; Ouel et al., 2021). We partially justify the form of our guarantee by showing in Section 5 (adapting a result due to Diakonikolas et al. (2022)) that one cannot avoid a dependence on the norm bound \(B\). Further closing the gap between upper and lower bounds is an important direction for future work.

Overview of techniques.We now describe the main ingredients and techniques that go into proving Theorem 1.1. Our starting point is the connection between GLMs and so-called matching losses (Auer et al., 1995). This connection arises from the fact that fitting a GLM with a known link function is equivalent to minimizing (over the class of all linear functions) a certain convex loss known as the matching loss corresponding to that link function (see Definition 1.8). (We refer the reader to (Gopalan et al., 2023, Sec 5.1) for a more detailed discussion.)

Importantly for us, minimizing the matching loss corresponding to a link function \(u\) yields a meaningful guarantee even in the agnostic setting, where the Bayes optimal predictor (i.e. \(\mathbf{x}\mapsto\mathbb{E}[y|\mathbf{x}]\)) is not necessarily a GLM at all. Specifically, it turns out to be equivalent (via Fenchel-Legendre duality) to finding the closest predictor to the Bayes optimal predictor in the metric of the _Bregman divergence_ associated with the link function \(u\) (see e.g. (Gopalan et al., 2023, Lemma 5.4)).

This is a powerful observation, but it still assumes that we have a fixed and known link function \(u\) in mind. In the agnostic setting, it is arguably unrealistic to assume that we know the best link function for the distribution at hand. Remarkably, recent work by Gopalan et al. (2022, 2023) has shown that there exist efficient learning algorithms that simultaneously minimize all matching losses corresponding to arbitrary monotone and Lipschitz link functions. Their solution concept is called an _omnipredictor_, i.e., a single predictor that is able to compete with the best-fitting classifier in a class \(\mathcal{C}\) as measured by a large class of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They obtain such predictors through calibrated multiaccuracy (Gopalan et al., 2023) or multicalibration (Gopalan et al., 2022).

From the point of view of ordinary supervised learning or regression, however, an optimality guarantee in terms of such matching losses or Bregman divergences is hard to interpret. A much more standard metric is simply that of squared error. The key final step for our results is to find a way of translating (a) optimality (ranging over all linear functions) in terms of all matching losses simultaneously into (b) optimality (ranging over all SIMs) in terms of squared error. We do so by proving simple analytic _distortion inequalities_ relating matching losses to \(\ell_{p}\) losses, which we believe may be of independent interest.

On a technical level, to prove these distortion inequalities we first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz link functions (see Lemma 2.2). We then obtain our results for general Lipschitz link functions by carefully approximating them using bi-Lipschitz link functions (see Lemma 3.3).

Further applications.As further applications of our approach, if we let \(\mathrm{opt}(\mathrm{GLM}_{u^{-1},B})\) denote the optimal value of \(\mathrm{err}_{2}(h)\) over all GLMs of the form \(\mathbf{x}\mapsto u^{-1}(\mathbf{w}\cdot\mathbf{x})\), where \(\|\mathbf{w}\|_{2}\leq B\), we obtain the following result about bi-Lipschitz link functions (including, for example, the Leaky ReLU).

**Theorem 1.2** (Informal, see Theorem 2.1).: _Let \(u:\mathbb{R}\to\mathbb{R}\) be a bi-Lipschitz invertible link function. Then, any predictor \(p:\mathbb{R}^{d}\to\mathbb{R}\) that is an \(\varepsilon\)-approximate minimizer of the population matching loss that corresponds to \(u\), with respect to a distribution \(D\) over \(\mathbb{R}^{d}\times[0,1]\) satisfies_

\[\mathrm{err}_{2}(p)\leq O\big{(}\mathrm{opt}(\mathrm{GLM}_{u^{-1},B})\big{)}+ O(\varepsilon)\]

This guarantee holds under milder distributional assumptions than are required by comparable prior work on agnostically learning GLMs or single neurons (Frei et al., 2020; Diakonikolas et al., 2022). Moreover, when we focus on distortion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of \(\widetilde{O}(\mathrm{opt}(\mathrm{GLM}_{u^{-1},B}))\) for logistic regression, when \(u\) is the logit link function (i.e., when \(\mathrm{GLM}_{u^{-1},B}\) is the class of sigmoid neurons).

**Theorem 1.3** (Informal, see Theorem 4.1).: _Let \(u(t)=\ln(\frac{t}{1-t})\). Then, any predictor \(p:\mathbb{R}^{d}\to\mathbb{R}\) that is an approximate \(\varepsilon\)-minimizer of the population logistic loss, with respect to a distribution \(D\) over \(\mathbb{R}^{d}\times[0,1]\) whose marginal on \(\mathbb{R}^{d}\) has subgaussian tails in every direction satisfies_

\[\mathrm{err}_{2}(p)\leq\widetilde{O}\big{(}\mathrm{opt}(\mathrm{GLM}_{u^{-1},B })\big{)}+O(\varepsilon)\]

While our error guarantee for this problem is weaker than that of Diakonikolas et al. (2022), we do not make the anti-concentration assumptions their results require.

### Background and Relation to Prior Work

We note that matching losses have been studied in various previous works either implicitly (Kakade et al., 2011) or explicitly (Auer et al., 1995; Diakonikolas et al., 2020, Gopalan et al., 2023) and capture various fundamental algorithms like logistic and linear regression (McCullagh, 1984; Agresti, 2015). However, to the best of our knowledge, our generic and direct approach for transforming matching loss guarantees to squared error bounds, has not been explored previously. Furthermore, our results do not depend on the specific implementation of an algorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide new agnostic error guarantees for various existing algorithms of the literature. For example, our results imply new guarantees for the GLMtron algorithm of Kakade et al. (2011) in the agnostic setting, since GLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss corresponding to a specified link function.

Matching losses over linear functions are also linked to the Chow parameters (O'Donnell and Servedio, 2008) through their gradient with respect to \(\mathbf{w}\), as observed by Diakonikolas et al. (2020). In fact, the norm of the matching loss gradient is also linked to multiaccuracy, a notion that originates to fairness literature (Hebert-Johnson et al., 2018; Kim et al., 2019). A stationary point \(\mathbf{w}\) of a matching loss that corresponds to a GLM with link \(u\) is associated with a multiaccurate predictor \(p(\mathbf{x})=u^{-1}(\mathbf{w}\cdot\mathbf{x})\), i.e., a predictor such that for all \(i\in[d]\), \(\mathbb{E}[\mathbf{x}_{i}(y-p(\mathbf{x}))]=0\). The work of (Gopalan et al., 2022, 2023) on omnipredictors presents a single predictor that is better than any linear model \(\mathbf{w}\cdot\mathbf{x}\) for every matching loss. The results of Gopalan et al. (2022) show that a multicalibrated predictor (with respect to the features \(\mathbf{x}_{i}\)) is an omnipredictor for all convex losses, whereas Gopalan et al. (2023) shows that the simpler condition of calibrated multicarucary suffices for matching losses that arise from GLMs. In view of the relationship between multicarucary and the gradient of the matching loss, our results show that, while multiaccuracy implies bounds on agnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically learning all SIMs.

The work of Shalev-Shwartz et al. (2011) showed strong agnostic learning guarantees in terms of the absolute error (rather than the squared error) of the form \(\mathrm{opt}+\varepsilon\) for a range of GLMs, but their work incurs an exponential dependence on the weight norm \(B\). In comparison, for the absolute loss, we get a bound of the form \(B\,\mathrm{opt}\log(1/\mathrm{opt})\) for logistic regression (see Theorem 4.3). In more recent years, the problem of agnostically learning GLMs has frequently also been phrased as the problem of agnostically learning single neurons (with a known activation). For the ReLU activation, work by Goel et al. (2017) showed an algorithm achieving error \(\mathrm{opt}+\varepsilon\) in time \(\mathrm{poly}(d)\exp(1/\varepsilon)\) over marginals on the unit sphere, and Diakonikolas et al. (2020) showed an algorithm achieving error \(O(\operatorname{opt})+\varepsilon\) in fully polynomial time over isotropic log-concave marginals. The work of Frei et al. (2020); Diakonikolas et al. (2022) both show guarantees for learning general neurons (with known activations) using the natural approach of running SGD directly on the squared loss (or a regularized variant thereof). Frei et al. (2020) achieves error \(O(\operatorname{opt})\) for any given strictly increasing activation and \(O(\sqrt{\operatorname{opt}})\) for the ReLU activation, but they assume that the marginal distribution is bounded. In contrast, we only assume that the marginal distribution has bounded second moments and we do not consider that the activation is known. Diakonikolas et al. (2022) proved an \(O(\operatorname{opt})\) guarantee for a wide range of activations (including the ReLU), in the setting where the activation is known and over a large class of structured marginals, which need, however, to satisfy strong concentration and anti-concentration properties.

In terms of lower bounds and hardness results for this problem, the work of (Goel et al., 2019; Diakonikolas et al., 2020; Goel et al., 2020; Diakonikolas et al., 2021, 2022) has established superpolynomial hardness even for the setting of agnostically learning single ReLUs over Gaussian marginals.

Limitations and directions for future work.While we justify a certain dependence on the norm bound \(B\) in our main result on agnostically learning SIMs, we do not completely justify the exact form of Theorem 1.1. An important direction for future work is to tightly characterize the optimal bounds achievable for this problem, as well as to show matching algorithms.

### Preliminaries

For the following, \((\mathbf{x},y)\) is used to denote a labelled sample from a distribution \(D\) over \(\mathbb{R}^{d}\times\mathcal{Y}\), where \(\mathcal{Y}\) denotes the interval \([0,1]\) unless it is specified to be the set \(\{0,1\}\). We note that, although we provide results for the setting where the labels lie within \([0,1]\), we may get similar results for any bounded label space. We use \(\mathbb{P}_{D}\) (resp. \(\mathbb{E}_{D}\)) to denote the probability (resp. expectation) over \(D\) and, similarly, \(\mathbb{P}_{S}\) (resp. \(\mathbb{E}_{S}\)) to denote the corresponding empirical quantity over a set \(S\) of labelled examples. Throughout the paper, we will use the term differentiable function to mean a function that is differentiable except on finitely many points. Our main results will assume the following about the marginal distribution on \(\mathbb{R}^{d}\).

**Definition 1.4** (Bounded moments).: For \(\lambda\geq 1\) and \(k\in\mathbb{N}\), we say that a distribution \(D_{\mathbf{x}}\) over \(\mathbb{R}^{d}\) has \(\lambda\)-bounded \(2k\)-th moments if for any \(\mathbf{v}\in\mathbb{S}^{d-1}\) we have \(\mathbb{E}_{\mathbf{x}\sim D_{\mathbf{x}}}[(\mathbf{v}\cdot\mathbf{x})^{2k}]\leq\lambda\).

For a concept class \(\mathcal{C}:\mathbb{R}^{d}\to\mathbb{R}\), we define \(\operatorname{opt}(\mathcal{C},D)\) to be the minimum squared error achievable by a concept \(c:\mathbb{R}^{d}\to\mathbb{R}\) in \(\mathcal{C}\) with respect to the distribution \(D\). We now define our main learning task.

**Definition 1.5** (Agnostic learning).: Let \(\mathcal{C}:\mathbb{R}^{d}\to\mathbb{R}\) be a concept class, let \(\Psi:[0,1]\to[0,1]\) be an increasing function, let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\) and \(\varepsilon>0\). We say that an algorithm \(\mathcal{A}\) agnostically learns the class \(\mathcal{C}\) up to error \(\Psi(\operatorname{opt}(\mathcal{C},D))+\varepsilon\) if algorithm \(\mathcal{A}\), given a number of i.i.d. samples from \(D\), outputs, with probability at least \(2/3\) over the samples and the randomness of \(\mathcal{A}\), a hypothesis \(h:\mathbb{R}^{d}\to[0,1]\) with \(\mathbf{E}_{D}[(y-h(\mathbf{x}))^{2}]\leq\Psi(\operatorname{opt}(\mathcal{C },D))+\varepsilon\).

We will also provide results that are specific to the sigmoid activation and work under the assumption that the marginal distribution is sufficiently concentrated.

**Definition 1.6** (Concentrated marginals).: For \(\lambda>0\) and \(\gamma\), we say that a distribution \(D_{\mathbf{x}}\) over \(\mathbb{R}^{d}\) is \((\lambda,\gamma)\)-concentrated if for any \(\mathbf{v}\in\mathbb{S}^{d-1}\) and \(r\geq 0\) we have \(\mathbb{P}_{\mathbf{x}\sim D_{\mathbf{x}}}[|\mathbf{v}\cdot\mathbf{x}|\geq r] \leq\lambda\cdot\exp(-r^{\gamma})\).

**Definition 1.7** (Fenchel-Legendre pairs).: We call a pair of functions \((f,g)\) a Fenchel-Legendre pair if the following conditions hold.

1. \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is continuous, non-decreasing, differentiable and \(1\)-Lipschitz with range \(\text{ran}(g^{\prime})\supseteq(0,1)\) and \(g(t)=\int_{0}^{t}g^{\prime}(\tau)\ d\tau\), for any \(t\in\mathbb{R}\).
2. \(f:\text{ran}(g^{\prime})\to\mathbb{R}\) is the convex conjugate (Fenchel-Legendre transform) of \(g\), i.e., we have \(f(r)=\sup_{t\in\mathbb{R}}r\cdot t-g(t)\) for any \(r\in\text{ran}(g^{\prime})\).

For such pairs of functions (and their derivatives \(f^{\prime},g^{\prime}\)), the following are true for \(r\in\text{ran}(g^{\prime})\) and \(t\in\text{ran}(f^{\prime})\) (note that \(\text{ran}(f^{\prime})\) is not necessarily \(\mathbb{R}\) when \(g^{\prime}\) is not invertible).

\[g^{\prime}(f^{\prime}(r))=r\;\;\text{and}\;\;f(r)=rf^{\prime}(r)-g (f^{\prime}(r)),\;\text{for}\;r\in\text{ran}(g^{\prime})\] (1.1) \[f^{\prime}(g^{\prime}(t))=t\;\;\text{and}\;\;g(t)=tg^{\prime}(t)- f(g^{\prime}(t)),\;\text{for}\;t\in\text{ran}(f^{\prime})\] (1.2)Note that \(g^{\prime}\) will be used as an activation function for single neurons and \(f^{\prime}\) corresponds to the unknown link function of a SIM (or the known link function of a GLM). We say that \(g^{\prime}\) is bi-Lipschitz if for any \(t_{1}<t_{2}\in\mathbb{R}\) we have that \((g^{\prime}(t_{2})-g^{\prime}(t_{1}))/(t_{2}-t_{1})\in[\alpha,\beta]\). If \(g^{\prime}\) is \([\alpha,\beta]\) bi-Lipschitz, then \(f^{\prime}\) is \([\frac{1}{\beta},\frac{1}{\alpha}]\) bi-Lipschitz. However, the converse implication is not necessarily true when \(g^{\prime}\) is not strictly increasing.

**Definition 1.8** (Matching Losses).: For a non-decreasing and Lipschitz activation \(g^{\prime}:\mathbb{R}\to\mathbb{R}\), the matching loss \(\ell_{g}:\mathcal{Y}\times\mathbb{R}\to\mathbb{R}\) is defined pointwise as follows:

\[\ell_{g}(y,t)=\int_{0}^{t}g^{\prime}(\tau)-y\ d\tau,\]

where \(g(t)=\int_{0}^{t}g^{\prime}\). The function \(\ell_{g}\) is convex and smooth with respect to its second argument. The corresponding population matching loss is

\[\mathcal{L}_{g}(c\ ;D)=\operatorname*{\mathbb{E}}_{(\mathbf{x},y)\sim D}\Bigl{[} \ell_{g}(y,c(\mathbf{x}))\Bigr{]}\] (1.3)

In Equation (1.3), \(c:\mathbb{R}^{d}\to\mathbb{R}\) is some concept and \(D\) is some distribution over \(\mathbb{R}^{d}\times[0,1]\). In the specific case where \(c\) is a linear function, i.e., \(c(\mathbf{x})=\mathbf{w}\cdot\mathbf{x}\), for some \(\mathbf{w}\in\mathbb{R}^{d}\), then we may alternatively denote \(\mathcal{L}_{g}(c\ ;D)\) with \(\mathcal{L}_{g}(\mathbf{w}\ ;D)\).

We also define the Bregman divergence associated with \(f\) to be \(\mathbb{D}_{f}(q,r)=f(q)-f(r)-(q-r)f^{\prime}(r)\), for any \(q,r\in\text{ran}(g^{\prime})\). Note that \(\mathbb{D}_{f}(q,r)\geq 0\) with equality iff \(q=r\).

**Definition 1.9** (SIMs and GLMs as Concept Classes).: For \(B>0\), we use \(\operatorname{SIM}_{B}\) to refer to the class of all SIMs of the form \(\mathbf{x}\mapsto g^{\prime}(\mathbf{w}\cdot\mathbf{x})\) where \(\|\mathbf{w}\|_{2}\leq B\) and \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is an arbitrary \(1\)-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We define \(\operatorname{GLM}_{g^{\prime},B}\) similarly except for the case where \(g^{\prime}\) is fixed and known.

We also define the notion of calibrated multiaccuracy that we need to obtain omnipredictors in our context.

**Definition 1.10** (Calibrated Multiaccuracy).: A predictor \(p:\mathbb{R}^{d}\to[0,1]\) is called \(\varepsilon\)-multiaccurate if for all \(i\in[d]\), \(|\operatorname*{\mathbb{E}}[\mathbf{x}_{i}(y-p(\mathbf{x}))]|\leq\varepsilon\). It is called \(\varepsilon\)-calibrated if \(|\operatorname*{\mathbb{E}}_{p(\mathbf{x})}\operatorname*{\mathbb{E}}_{y|p( \mathbf{x})}[y-p(\mathbf{x})]|\leq\varepsilon\).

## 2 Distortion Bounds for the Matching Loss

In this section, we propose a simple approach for bounding the squared error of a predictor that minimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to squared loss bounds in a generic way, through appropriate pointwise distortion bounds between the two losses. In particular, for a given matching loss \(\mathcal{L}_{g}\), we transform guarantees on \(\mathcal{L}_{g}\) that are competitive with the optimum linear minimizer of \(\mathcal{L}_{g}\) to guarantees on the squared error that are competitive with the optimum GLM whose activation (\(g^{\prime}\)) depends on the matching loss at hand.

We now provide the main result we establish in this section.

**Theorem 2.1** (Squared Error Minimization through Matching Loss Minimization).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\), let \(0<\alpha\leq\beta\) and let \((f,g)\) be a Fenchel-Legendre pair such that \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is \([\alpha,\beta]\) bi-Lipschitz. Suppose that for a predictor \(p:\mathbb{R}^{d}\to\text{ran}(g^{\prime})\) we have_

\[\mathcal{L}_{g}(f^{\prime}\circ p\,;D)\leq\min_{\|\mathbf{w}\|_{2}\leq B} \mathcal{L}_{g}(\mathbf{w}\,;D)+\varepsilon\] (2.1)

_Then we also have: \(\operatorname{err}_{2}(p)\leq\frac{\beta}{\alpha}\cdot\operatorname{opt}( \operatorname{GLM}_{g^{\prime},B})+2\beta\varepsilon\)._

The proof of Theorem 2.1 is based on the following pointwise distortion bound between matching losses corresponding to bi-Lipschitz link functions and the squared distance.

**Lemma 2.2** (Pointwise Distortion Bound for bi-Lipschitz link functions).: _Let \(0<\alpha\leq\beta\) and let \((f,g)\) be a Fenchel-Legendre pair such that \(f^{\prime}:\text{ran}(g^{\prime})\to\mathbb{R}\) is \([\frac{1}{\beta},\frac{1}{\alpha}]\) bi-Lipschitz. Then for any \(y,p\in\text{ran}(g^{\prime})\) we have_

\[\ell_{g}(y,f^{\prime}(p))-\ell_{g}(y,f^{\prime}(y))=\mathbb{D}_{f}(y,p)\in \left[\frac{1}{2\beta}(y-p)^{2},\frac{1}{2\alpha}(y-p)^{2}\right]\]In the case that \(f^{\prime}\) is differentiable on \((0,1)\), the proof of Lemma 2.2 follows from an application of Taylor's approximation theorem of degree \(2\) on the function \(f\), since the Bregman divergence \(\mathbb{D}_{f}(y,p)\) is exactly equal to the error of the second degree Taylor's approximation of \(f(y)\) around \(p\) and \(f^{\prime\prime}(\xi)\in[\frac{1}{\beta},\frac{1}{\alpha}]\) for any \(\xi\in\text{ran}(g^{\prime})\). The relationship between \(\ell_{g}\) and \(\mathbb{D}_{f}\) follows from property (1.2). Note that when \(g^{\prime}\) is \([\alpha,\beta]\) bi-Lipschitz, then \(f^{\prime}\) is \([\frac{1}{\beta},\frac{1}{\alpha}]\) bi-Lipschitz.

Theorem 2.1 follows by applying Lemma 2.2 appropriately to bound the error of a predictor \(p\) by its matching loss \(\mathcal{L}_{g}(f^{\prime}\circ p)\) and bound the matching loss of the linear function corresponding to \(\mathbf{w}^{*}\) by the squared error of \(g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})\), where \(g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})\) is the element of \(\mathrm{GLM}_{g^{\prime},B}\) with minimum squared error.

Although Theorem 2.1 only applies to bi-Lipschitz activations \(g^{\prime}\), it has the advantage that the assumption it makes on \(p\) corresponds to a convex optimization problem and, when the marginal distribution has certain concentration properties (for generalization), can be solved efficiently through gradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we can obtain \(O(\mathrm{opt})\) efficiently under mild distributional assumptions in the agnostic setting.

## 3 Agnostically Learning Single-Index Models

In this section, we provide our main result on agnostically learning SIMs. We combine the distortion bounds we established in Section 2 with results from Gopalan et al. (2023) on Omniprediction, which can be used to learn a predictor \(p\) that satisfies the guarantee of Theorem 2.1 simultaneously for all bi-Lipschitz activations. By doing so, we obtain a result for all Lipschitz and non-decreasing activations simultaneously.

**Theorem 3.1** (Agnostically Learning SIMs).: _Let \(\lambda,B\geq 1\), \(\varepsilon>0\) and let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\) with second moments bounded by \(\lambda\). Then, Algorithm 1 agnostically learns the class \(\mathrm{SIM}_{B}\) over \(D\) up to squared error \(O(B\sqrt{\lambda}\sqrt{\mathrm{opt}(\mathrm{SIM}_{B},D)})+\varepsilon\) using time and sample complexity \(\mathrm{poly}(d,B,\lambda,\frac{1}{\varepsilon})\). Moreover, the same is true for any algorithm with an omniprediction guarantee like the one of Theorem 3.2._

In order to apply Theorem 2.1, we use the following theorem which is a combination of results in Gopalan et al. (2023), where they show that the matching losses corresponding to a wide class of functions can all be minimized simultaneously by an efficiently computable predictor.

**Theorem 3.2** (Omnipredictors for Matching Losses, combination of results in Gopalan et al. (2023)).: _Let \(\lambda,L,R,B\geq 1\), \(\varepsilon>0\) and let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\) whose marginal on \(\mathbb{R}^{d}\) has \(\lambda\)-bounded second moments. Then, Algorithm 1, given sample access to \(D\), with probability at least \(2/3\) returns a predictor \(p:\mathbb{R}^{d}\to(0,1)\) with the following guarantee. For any Fenchel-Legendre pair \((f,g)\) such that \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is \(L\)-Lipschitz, and \(f^{\prime}\) takes values within the interval \([-R,R]\), \(p\) satisfies_

\[\mathcal{L}_{g}(f^{\prime}\circ p\;;D)\leq\min_{\|\mathbf{w}\|_{2}\leq B} \mathcal{L}_{g}(\mathbf{w}\;;;D)+\varepsilon.\]

_The algorithm requires time and sample complexity \(\mathrm{poly}(\lambda,B,L,R,\frac{1}{\varepsilon})\)._

Algorithm 1 is a version of the algorithm of Gopalan et al. (2023) for calibrated multiaccuracy, specific to our setting. For a more quantitative version of Theorem 3.2, see Theorem C.3 in the appendix.

We aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the class of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based on Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of another predictor and the squared expected distance between the two predictors.

**Lemma 3.3**.: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\). Let \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) be some fixed activation, and \(f^{\prime}\) its dual. Consider the class \(\mathrm{GLM}_{g^{\prime},B}\), and let \(\mathbf{w}^{*}\) be the weights achieving \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B},D)\). Let \(\phi^{\prime}:\mathbb{R}\to\mathbb{R}\) be an \([\alpha,\beta]\) bi-Lipschitz function (differentiable except possibly at finitely many points) that we wish to approximate \(g^{\prime}\) by. Any predictor \(p:\mathbb{R}^{d}\to\mathbb{R}\) that satisfies_

\[\mathcal{L}_{\phi}(f^{\prime}\circ p\;;D)\leq\min_{\|\mathbf{w}\|_{2}\leq B} \mathcal{L}_{\phi}(\mathbf{w}\;;D)+\varepsilon\]

_also satisfies the following \(\ell_{2}\) error guarantee:_

\[\mathrm{err}_{2}(p)\leq\frac{2\beta}{\alpha}\,\mathrm{opt}(\mathrm{GLM}_{g^{ \prime},B})+\frac{2\beta}{\alpha}\;\mathbb{E}\left[\left(g^{\prime}(\mathbf{w} ^{*}\cdot\mathbf{x})-\phi^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})\right)^{2} \right]+2\beta\varepsilon.\]By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we are now ready to prove our main theorem.

Proof of Theorem 3.1.: We will combine Theorem 3.2, which states that there is an efficient algorithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing and Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss corresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we may assume that \(\varepsilon<1/2\), since otherwise the problem is trivial (output the zero function and pick \(C=2\)).

As a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded (according to Definition C.1, for \(\gamma=0\)). In particular, let \(\phi^{\prime}:\mathbb{R}\to\mathbb{R}\) be an \([\alpha,\beta]\) bi-Lipschitz activation for some \(\beta\geq\alpha>0\) such that \(\phi^{\prime}(s)\in[-1,2]\) for some \(s\in\mathbb{R}\) and let \(\psi^{\prime}\) be the inverse of \(\phi^{\prime}\) (\(\phi^{\prime}\) is invertible since it is strictly increasing). We will show that \(\psi^{\prime}(r)\in[-R,R]\) for any \(r\in[0,1]\) and \(R=O(|s|+1/\alpha)\).

We pick \(r_{0}=0\), \(r_{1}=1\) and get that \(|\psi^{\prime}(\phi^{\prime}(s))-\psi^{\prime}(r_{0})|\leq\frac{1}{\alpha}|\phi^{ \prime}(s)-r_{0}|\leq\frac{2}{\alpha}\). Hence \(\psi^{\prime}(r_{0})\geq\psi^{\prime}(\phi^{\prime}(s))-\frac{2}{\alpha}=s- \frac{1}{\alpha}\). Similarly, we get \(\psi^{\prime}(r_{1})\leq s+\frac{2}{\alpha}\). Therefore, \(\psi^{\prime}(r)\in[\psi^{\prime}(0),\psi^{\prime}(1)]\subseteq[-|s|-\frac{2}{ \alpha},|s|+\frac{2}{\alpha}]\), for any \(r\in[0,1]\), due to monotonicity of \(\psi^{\prime}\).

For a given non-decreasing and \(1\)-Lipschitz \(g^{\prime}\), we will now show that there is a bounded bi-Lipschitz activation \(\phi^{\prime}\) such that if the assumption of Lemma 3.3 is satisfied for \(\phi^{\prime}\) by a predictor \(p\), then the error of \(p\) is bounded by

\[\mathrm{err}_{2}(p)\leq O\big{(}B\sqrt{\lambda}(\mathrm{opt}(\mathrm{GLM}_{g^{ \prime},B}))^{1/2}\big{)}+O(\lambda B^{2}\varepsilon)\]

Suppose, first, that \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\leq\varepsilon^{2}\). Then, we pick \(\phi^{\prime}(t)=g^{\prime}(t)+\varepsilon t\). Note that \(\phi^{\prime}\) is \([\varepsilon,1+\varepsilon]\) bi-Lipschitz. Moreover, since \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\leq\varepsilon^{2}\), we must have some \(s\in\mathbb{R}\) with \(|s|\leq 2\lambda B^{2}\) such that \(g^{\prime}(s)\in[-1,2]\). Otherwise, we would have \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})=\mathbf{E}[(g^{\prime}(\mathbf{w}^{ *}\cdot\mathbf{x})-y)^{2}]\geq\mathbf{E}[(g^{\prime}(\mathbf{w}^{*}\cdot \mathbf{x})-y)^{2}\mid|\mathbf{w}^{*}\cdot\mathbf{x}\leq 2\lambda B^{2}|]\cdot \mathbb{P}[|\mathbf{w}^{*}\cdot\mathbf{x}|\leq 2\lambda B^{2}]\geq 1- \mathbb{P}[|\mathbf{w}^{*}\cdot\mathbf{x}|>2\lambda B^{2}|\geq\frac{1}{4}> \varepsilon^{2}\), due to Chebyshev's inequality, the fact that \(\mathbf{w}^{*}\in\mathcal{W}\) and the bounded moments assumption. Therefore, \(\psi^{\prime}\) is \((R=2\lambda B^{2}+\frac{2}{\varepsilon},\gamma=0)\)-bounded and we have

\[\mathbb{E}\left[\left(g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})-\phi^{\prime} (\mathbf{w}^{*}\cdot\mathbf{x})\right)^{2}\right]\leq\varepsilon^{2}\ \mathbb{E}((\mathbf{w}^{*}\cdot\mathbf{x})^{2}]\leq\varepsilon^{2}\lambda B^{2}\]

As a consequence, under the assumption of Lemma 3.3 for \(\phi^{\prime}\), the error of the corresponding predictor \(p\) is \(\mathrm{err}_{2}(p)\leq 2(1+\varepsilon)\varepsilon+2(1+\varepsilon)\varepsilon \lambda B^{2}+2(1+\varepsilon)\varepsilon=O(\lambda B^{2}\varepsilon)\).

In the case that \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})>\varepsilon^{2}\), we pick \(\phi^{\prime}(t)=g^{\prime}(t)+t\ \frac{\sqrt{\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})}}{B \sqrt{\lambda}}\). We may also assume that \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\leq 1/2\), since otherwise any predictor with range \([0,1]\) will have error at most \(2\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\). Then, \(\psi\) is \((O(\lambda B^{2}+\frac{B\sqrt{\lambda}}{\varepsilon}),0)\)-bounded, \(\phi^{\prime}\) is \([\frac{1}{B}\sqrt{\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})}/\sqrt{\lambda},1+ \frac{1}{B}]\) bi-Lipschitz which gives

\[\mathbb{E}\left[\left(g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})-\phi^{\prime} (\mathbf{w}^{*}\cdot\mathbf{x})\right)^{2}\right]\leq\frac{\mathrm{opt}( \mathrm{GLM}_{g^{\prime},B})}{B^{2}\lambda}\ \mathbb{E}[(\mathbf{w}^{*}\cdot\mathbf{x})^{2}]\leq \mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\]

As a consequence, under the assumption of Lemma 3.3 for \(\phi^{\prime}\), the error of the corresponding predictor \(p\) is \(\mathrm{err}_{2}(p)\leq 4(1+\frac{1}{B})B\sqrt{\lambda}\sqrt{\mathrm{opt}( \mathrm{GLM}_{g^{\prime},B})}+2(1+\frac{1}{B})\varepsilon\). Using a similar approach as for the case \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\leq\varepsilon\), we can show that \(\psi^{\prime}\) is polynomially bounded (as per Definition C.1), since \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\leq\frac{1}{2}\).

To conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial) choice of parameters (\(R=O(\lambda B^{2}+\frac{B\sqrt{\lambda}}{\varepsilon})\), \(L=2\)), to show that there is an efficient algorithm that outputs a predictor \(p:\mathbb{R}^{d}\to(0,1)\) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations (\(\phi^{\prime}\)) with sufficiently bounded inverses (\(\psi^{\prime}\)). 

## 4 Stronger Guarantees for Logistic Regression

In this section, we follow the same recipe we used in Section 2 to get distortion bounds similar to Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assumption that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular, Theorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only provides a guarantee of \(O(\sqrt{\mathrm{opt}})\) for squared error. We use appropriate pointwise distortion bounds for the matching loss corresponding to the sigmoid activation and provide guarantees of \(\widetilde{O}(\mathrm{opt})\) for logistic regression with respect to both squared and absolute error, under appropriate assumptions about the concentration of the marginal distribution. The proofs of this section are provided in Appendix D.

For the logistic model, the link function \(f^{\prime}\) is defined as \(f^{\prime}(r)=\ln(\frac{r}{1-r})\), for \(r\in(0,1)\) and the corresponding activation \(g^{\prime}\) is the sigmoid \(g^{\prime}(t)=\frac{1}{1+e^{-t}}\) for \(t\in\mathbb{R}\). The corresponding matching loss is the logistic loss.

Squared error.We first provide a result for squared loss minimization. In comparison to Theorem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation is not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.

**Theorem 4.1** (Squared Loss Minimization through Logistic Loss Minimization).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\) whose marginal on \(\mathbb{R}^{d}\) is \((1,2)\)-concentrated. Let \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) be the sigmoid activation, i.e., \(g^{\prime}(t)=(1+e^{-t})^{-1}\) for \(t\in\mathbb{R}\). Assume that for some \(B>0\), \(\varepsilon>0\) and a predictor \(p:\mathbb{R}^{d}\to(0,1)\) we have_

\[\mathcal{L}_{g}(f^{\prime}\circ p\,;D)\leq\min_{\mathbf{w}:\|\mathbf{w}\|_{2} \leq B}\mathcal{L}_{g}(\mathbf{w}\,;D)+\varepsilon\] (4.1)

_If we let \(\operatorname{opt}_{g}=\min_{\|\mathbf{w}\|_{2}\leq B}\operatorname{err}_{2}( g^{\prime}_{\mathbf{w}})\), then for the predictor \(p\) and some universal constant \(C>0\) we also have_

\[\operatorname{err}_{2}(p)\leq C\operatorname{opt}_{g}\exp\left(B^{2}+\sqrt{B^ {2}\,\log\frac{1}{\operatorname{opt}_{g}}}\right)+2\varepsilon.\]

In particular, the squared error of \(p\) is upper bounded by \(\widetilde{O}(\operatorname{opt}_{g})\), since the function \(t\mapsto\exp(\log^{1/2}t)\) is asymptotically smaller than any polynomial function \(t\mapsto t^{\gamma}\) with \(\gamma>0\).

Once more, the proof of our result is based on an appropriate pointwise distortion bound which we provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is the Kullback-Leibler divergence and by combining Pinsker's inequality (lower bound) with Lemma 4.1 of Gotze et al. (2019) (upper bound).

**Lemma 4.2** (Pointwise Distortion Bound for Sigmoid).: _Let \(g^{\prime}\) be the sigmoid activation. Then, for any \(y,p\in(0,1)\) we have that \(\mathbb{D}_{f}(y,p)=\mathbb{D}_{\text{KL}}(y\|p)=y\ln(y/p)+(1-y)\ln(\frac{1-y} {1-p})\). Moreover_

\[\ell_{g}(y,f^{\prime}(p))-\ell_{g}(y,f^{\prime}(y))=\mathbb{D}_{\text{KL}}(y \|p)\in\left[\frac{1}{2}(y-p)^{2},\ \frac{2}{\min\{p,1-p\}}\cdot(y-p)^{2}\right]\]

We translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this case, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the upper bound on the surrogate loss provided by Lemma 4.2, we apply it to \(p\gets g^{\prime}(\mathbf{w}\cdot\mathbf{x})\), where \(g^{\prime}\) is the sigmoid function, and observe that the quantity \(\frac{1}{p(1-p)}\) is exponential in \(|\mathbf{w}\cdot\mathbf{x}|\). Hence, when the marginal is \((\lambda,2)\)-concentrated (subgaussian concentration), then \(\frac{1}{p(1-p)}\) is effectively bounded.

Absolute error.All of the results we have provided so far have focused on squared error minimization. We now show that our approach yields results even for the absolute error, which can also be viewed as learning in the p-concept model (Kearns and Schapire, 1994). In particular, for a distribution \(D\) over \(\mathbb{R}^{d}\times[0,1]\), we define the absolute error of a predictor \(p:\mathbb{R}^{d}\to[0,1]\) as follows.

\[\operatorname{err}_{1}(p)=\operatorname*{\mathbb{E}}_{(\mathbf{x},y)\sim D}[| y-p(\mathbf{x})|]\]

In the specific case when the labels are binary, i.e., \(y\in\{0,1\}\), we have

\[\operatorname{err}_{1}(p)=\operatorname*{\mathbb{E}}_{(\mathbf{x},y)\sim D}[| y-p(\mathbf{x})|]=\operatorname*{\mathbb{P}}_{(\mathbf{x},y,y_{p})\sim D_{p}}[y \neq y_{p}]\] (see Proposition A.2 )

where the distribution \(D_{p}\) is over \(\mathbb{R}^{d}\times\{0,1\}\times\{0,1\}\) and is formed by drawing samples \((\mathbf{x},y)\) from \(D\) and, given \(\mathbf{x}\), forming \(y_{p}\) by drawing a conditionally independent Bernoulli random variable with parameter \(p(\mathbf{x})\). We provide the following result.

**Theorem 4.3** (Absolute Loss Minimization through Logistic Loss Minimization).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times\{0,1\}\) whose marginal on \(\mathbb{R}^{d}\) is \((1,1)\)-concentrated. Let \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) be the sigmoid activation, i.e., \(g^{\prime}(t)=(1+e^{-t})^{-1}\) for \(t\in\mathbb{R}\). Assume that for some \(B>0\), \(\varepsilon>0\) and a predictor \(p:\mathbb{R}^{d}\to(0,1)\) we have_

\[\mathcal{L}_{g}(f^{\prime}\circ p\,;D)\leq\min_{\mathbf{w}:\|\mathbf{w}\|_{2} \leq B}\mathcal{L}_{g}(\mathbf{w}\,;D)+\varepsilon\] (4.2)

_If we let \(\operatorname{opt}_{g}=\min_{\|\mathbf{w}\|_{2}\leq B}\operatorname{err}_{1}( g^{\prime}_{\mathbf{w}})\), then for the predictor \(p\) and some universal constant \(C>0\) we also have_

\[\operatorname{err}_{1}(p)\leq C\,B\operatorname{opt}_{g}\,\log\frac{1}{ \operatorname{opt}_{g}}+\varepsilon\]

The corresponding distortion bound in this case is between the absolute and logistic losses and works when the labels are binary.

**Lemma 4.4** (Pointwise Distortion between Absolute and Logistic Loss).: _Let \(g^{\prime}\) be the sigmoid activation. Then, there is a constant \(c\in\mathbb{R}\) such that for any \(y\in\{0,1\}\) and \(p\in(0,1)\), we have_

\[\ell_{g}(y,f^{\prime}(p))-c\in\left[|y-p|\,\ 2\cdot\ln\left(\frac{1}{p(1-p)} \right)\cdot|y-p|\right]\]

The bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the p-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guarantees (Diakonikolas et al., 2022b) for agnostically learning sigmoid neurons, such algorithms typically make strong distributional assumptions including concentration, anti-concentration and anti-anti-concentration or boundedness.

Moreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by Diakonikolas et al. (2020c, Theorem 4.1), which concerns the problem of agnostically learning halfspaces by minimizing convex surrogates. In particular, they show that even under log-concave marginals, no convex surrogate loss can achieve a guarantee better than \(O(\mathrm{opt}\log(1/\mathrm{opt}))\), where \(\mathrm{opt}\) is measured with respect to the \(\ell_{1}\) error (which is equal to the probability of error). The result is not directly comparable to our upper bound, since we examine the sigmoid activation. Their setting can be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the sigmoid tends to the step function), but the main complication is that our upper bound is of the form \(O(B\mathrm{opt}\log(1/\mathrm{opt}))\), which scales with \(B\). However, their lower bound concerns marginal distributions that are not only concentrated, but are also anti-concentrated and anti-anticoncentrated, while our results only make concentration assumptions.

## 5 Necessity of Norm Dependence

In this final section, we use a lower bound due to Diakonikolas et al. (2022a) on agnostic learning of GLMs using SQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize to the case of the standard sigmoid or logistic function. A modification of their proof ensures that the bound holds under isotropic marginals.2

Footnote 2: Specifically, our features correspond to all multilinear monomials (or parities) of degree at most \(k\) over \(\{\pm 1\}^{n}\), whereas they use all monomials (not necessarily multilinear) of degree at most \(k\). These yield equivalent representations since the hard distributions are obtained from the uniform distribution on \(\{\pm 1\}^{n}\).

**Theorem 5.1** (SQ Lower Bound for Agnostically Learning GLMs, variant of (Diakonikolas et al., 2022a, Thm C.3)).: _Let \(g^{\prime}:\mathbb{R}^{d}\to\mathbb{R}\) be the standard logistic function. Any SQ algorithm either requires \(d^{\omega(1)}\) queries or \(d^{-\omega(1)}\) tolerance to distinguish between the following two labeled distributions:_

* _(Labels have signal.)_ \(D_{\mathrm{signal}}\) _on_ \(\mathbb{R}^{d}\times\mathbb{R}\) _is such that_ \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B},D_{\mathrm{signal}})\leq\exp(- \Omega(\log^{1/4}d))=o(1)\) _for some_ \(B=\mathrm{poly}(d)\)_._
* _(Labels are random.)_ \(D_{\mathrm{random}}\) _on_ \(\mathbb{R}^{d}\times\mathbb{R}\) _is such that the labels_ \(y\) _are drawn i.i.d. from_ \(\{a,b\}\) _for certain universal constants_ \(a,b\in[0,1]\)_. In particular,_ \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B},D_{\mathrm{random}})=\Omega(1)\) _for any_ \(B\)_._

_Both \(D_{\mathrm{signal}}\) and \(D_{\mathrm{random}}\) have the same marginal on \(\mathbb{R}^{d}\), with \(1\)-bounded second moments._

Let us consider applying our main theorem (Theorem 3.1) to this setting, with \(D\) being either \(D_{\mathrm{signal}}\) or \(D_{\mathrm{random}}\), and with the same \(B=\mathrm{poly}(d)\) as is required to achieve small error in the "labels have signal" case. We would obtain a predictor with \(\ell_{2}\) error at most \(B\sqrt{\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})}\) (or indeed with \(\mathrm{SIM}_{B}\) in place of \(\mathrm{GLM}_{g^{\prime},B}\)). Since this is \(\omega(1)\), this guarantee is insufficient to distinguish the two cases above, which is as it should be since our main algorithm indeed fits into the SQ framework.

Theorem 5.1 does, however, justify a dependence on the norm \(B\) in our main result. In particular, it is clear that a guarantee of the form \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})^{c}\) for any universal constant \(c>0\) (independent of \(B\)) would be too strong, as it would let us distinguish the two cases above. In fact, this lower bound rules out a large space of potential error guarantees stated as functions of \(B\) and \(\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})\). For instance, for sufficiently large \(d\), it rules out any error guarantee of the form \(\exp(O(\log^{1/5}B))\cdot\mathrm{opt}(\mathrm{GLM}_{g^{\prime},B})^{c^{\prime}}\) for any universal constant \(c^{\prime}>0\).

## Acknowledgments and Disclosure of Funding

We wish to thank the anonymous reviewers of NeurIPS 2023 for their constructive feedback. Aravind Gollakota was at UT Austin while this work was done, supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of Machine Learning (IFML). Adam R. Klivans was supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of Machine Learning (IFML). Konstantinos Stavropoulos was supported by NSF award AF-1909204, the NSF AI Institute for Foundations of Machine Learning (IFML), and by scholarships from Bodossaki Foundation and Leventis Foundation.

## References

* Leibniz-Zentrum fur Informatik, 2023. (document), 1.1.3, 3.2, 3.2, 1. C.1, C.3, C.1, C.1
* McCullagh [1984] Peter McCullagh. Generalized linear models. _European Journal of Operational Research_, 16(3):285-292, 1984. ISSN 0377-2217. doi: https://doi.org/10.1016/0377-2217(84)90282-0. URL https://www.sciencedirect.com/science/article/pii/0377221784902820.
* Agresti [2015] Alan Agresti. _Foundations of linear and generalized linear models_. John Wiley & Sons, 2015.
* Kakade et al. [2011] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. _Advances in Neural Information Processing Systems_, 24, 2011.
* The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009_, 2009.
* Goel et al. [2019] Surbhi Goel, Sushrut Karmalkar, and Adam Klivans. Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals. _Advances in neural information processing systems_, 32, 2019.
* Diakonikolas et al. [2020a] Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. _Advances in Neural Information Processing Systems_, 33:13586-13596, 2020a.
* Goel et al. [2020b] Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds via functional gradients. _Advances in Neural Information Processing Systems_, 33:2147-2158, 2020b.
* Diakonikolas et al. [2021] Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Mikhail Belkin and Samory Kpotute, editors, _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 1552-1584. PMLR, 15-19 Aug 2021.
* Diakonikolas et al. [2022a] Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, and Lisheng Ren. Hardness of learning a single neuron with adversarial label noise. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 8199-8213. PMLR, 28-30 Mar 2022a.
* Auer et al. [1995] Peter Auer, Mark Herbster, and Manfred K. K Warmuth. Exponentially many local minima for single neurons. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural Information Processing Systems_, volume 8. MIT Press, 1995. URL https://proceedings.neurips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf.
* Gopalan et al. [2022] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipreditors. In _13th Innovations in Theoretical Computer Science Conference (ITCS 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* Gopalan et al. [2022b]Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient descent. _Advances in Neural Information Processing Systems_, 33:5417-5428, 2020.
* Diakonikolas et al. (2022b) Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning a single neuron with adversarial label noise via gradient descent. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4313-4361. PMLR, 02-05 Jul 2022b. URL https://proceedings.mlr.press/v178/diakonikolas22c.html.
* Diakonikolas et al. (2020b) Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi. Approximation schemes for relu regression. In _Conference on Learning Theory_, pages 1452-1485. PMLR, 2020b.
* O'Donnell and Servedio (2008) Ryan O'Donnell and Rocco A Servedio. The chow parameters problem. In _Proceedings of the fortieth annual ACM symposium on Theory of computing_, pages 517-526, 2008.
* Hebert-Johnson et al. (2018) Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (Computationally-identifiable) masses. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1939-1948. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/hebert-johnson18a.html.
* Kim et al. (2019) Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, pages 247-254, 2019.
* Shalev-Shwartz et al. (2011) Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. _SIAM Journal on Computing_, 40(6):1623-1646, 2011.
* Goel et al. (2017) Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. In _Conference on Learning Theory_, pages 1004-1042. PMLR, 2017.
* Gotze et al. (2019) Friedrich Gotze, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions of weakly dependent random variables. _Electronic Journal of Probability_, 24:1-19, 2019.
* Kearns and Schapire (1994) Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. _Journal of Computer and System Sciences_, 48(3):464-497, 1994.
* Diakonikolas et al. (2020c) Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 18540-18549. Curran Associates, Inc., 2020c. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/d785bf9067f8af9e078b93cf26de2b54-Paper.pdf.

Technical Lemmas

In this section, we provide some technical Lemmas that we use in our proofs.

**Proposition A.1** (Weak Learner for Linear Functions).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[-1,1]\) whose marginal on \(\mathbb{R}^{d}\) has \(\lambda\)-bounded second moments and \(B>0\). For any \(\varepsilon>0\) and \(\delta\in(0,1)\), there is a universal constant \(C>0\) and an algorithm that given a set \(S\) of i.i.d. samples from \(D\) of size at least \(C\cdot\frac{d^{2}\lambda B^{2}}{\varepsilon^{2}}\log\frac{1}{\delta}\), runs in time \(O(d\left|S\right|)\) and satisfies the following specifications with probability at least \(1-\delta\)_

1. _If_ \(\mathbb{E}_{(\mathbf{x},z)\sim D}[z(\mathbf{w}\cdot\mathbf{x})]\geq\varepsilon\) _for some_ \(\mathbf{w}\in\mathbb{R}^{d}\) _with_ \(\|\mathbf{w}\|_{2}\leq B\)_, then the algorithm accepts. Otherwise, it may or may not reject and return a special symbol._
2. _If the algorithm accepts then it returns_ \(\mathbf{w}\in\mathbb{R}^{d}\) _with_ \(\|\mathbf{w}\|_{2}\leq B\) _such that we have_ \(\mathbb{E}_{(\mathbf{x},z)\sim D}[z(\mathbf{w}\cdot\mathbf{x})]\geq\varepsilon/4\)_._

Proof.: We will prove the proposition for \(\delta=1/6\). We may boost the probability of success with repetition.

The algorithm computes the vector \(\mathbf{v}=\mathbb{E}_{S}[z\,\mathbf{x}]\). If \(\|\mathbf{v}\|_{2}\leq\frac{3\varepsilon}{4B}\), then the algorithm rejects and outputs a special symbol. Otherwise, it outputs the vector \(\frac{B}{\|\mathbf{v}\|_{2}}\mathbf{v}\).

Suppose, first, that \(\mathbb{E}_{(\mathbf{x},z)\sim D}[z(\mathbf{w}\cdot\mathbf{x})]\geq\varepsilon\) for some \(\mathbf{w}\) with \(\|\mathbf{w}\|_{2}\leq B\). Then, due to Chebyshev's inequality we have for any \(i\in[d]\)

\[\mathbb{P}\left[\left|\operatorname*{\mathbb{E}}_{S}[z\,\mathbf{x}]-\operatorname *{\mathbb{E}}_{D}[z\,\mathbf{x}_{i}]\right|>\frac{\varepsilon}{8\,B\sqrt{d}} \right]\leq\frac{64\,d\,B^{2}}{\left|S\right|\varepsilon^{2}}\,\mathbb{E}[ \mathbf{x}_{i}^{2}]\leq\frac{64\,d\,B^{2}\,\lambda}{\left|S\right|\varepsilon^{ 2}}\leq\frac{1}{6}\quad\text{(for large enough $|S|$)}\]

Hence, with probability at least \(5/6\), we have \(\|\operatorname*{\mathbb{E}}_{S}[z\mathbf{x}]-\operatorname*{\mathbb{E}}_{D} [z\mathbf{x}]\|_{2}\leq\frac{\varepsilon}{8B}\), due to a union bound. Therefore, \(\|\mathbf{v}\|_{2}\geq\|\operatorname*{\mathbb{E}}_{D}[z\mathbf{x}]\|_{2}- \frac{\varepsilon}{8B}\geq\frac{\operatorname*{\mathbb{E}}_{D}[z(\mathbf{w} \cdot\mathbf{x})]}{B}-\frac{\varepsilon}{8B}\geq\frac{7B}{8}\) and the algorithm accepts.

Suppose, now, that the algorithm accepts. Then, we have \(\|\mathbf{v}\|_{2}>\frac{3\varepsilon}{4B}\) and (with probability at least \(5/6\)) we have

\[\operatorname*{\mathbb{E}}_{D}\left[\frac{B}{\|\mathbf{v}\|_{2}}\,z(\mathbf{v }\cdot\mathbf{x})\right]=\frac{B}{\|\mathbf{v}\|_{2}}\mathbf{v}\cdot \operatorname*{\mathbb{E}}[z\mathbf{x}]\geq\varepsilon/4\]

since \(\|\operatorname*{\mathbb{E}}_{S}[z\mathbf{x}]-\operatorname*{\mathbb{E}}_{D} [z\mathbf{x}]\|_{2}\leq\frac{\varepsilon}{8B}\). This concludes the proof. 

**Proposition A.2**.: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times\{0,1\}\) and \(p:\mathbb{R}^{d}\to[0,1]\). Consider the distribution \(D_{p}\) over \(\mathbb{R}^{d}\times\{0,1\}\times\{0,1\}\), which is formed by drawing samples \((\mathbf{x},y)\) from \(D\) and, given \(\mathbf{x}\), forming \(y_{p}\) by drawing a conditionally independent Bernoulli random variable with parameter \(p(\mathbf{x})\). Then we have_

\[\operatorname*{err}_{1}(p)=\operatorname*{\mathbb{E}}_{(\mathbf{x},y)\sim D}[ |y-p(\mathbf{x})|]=\operatorname*{\mathbb{P}}_{(\mathbf{x},y,y_{p})\sim D_{p}}[ y\neq y_{p}]\]

Proof.: Since over \(D_{p}\), \(y\) and \(y_{p}\) are conditionally independendent, we have

\[\operatorname*{err}_{1}(p)=\operatorname*{\mathbb{E}}[|y-p(\mathbf{x})|] =\operatorname*{\mathbb{E}}[(1-p(\mathbf{x}))\mathbbm{1}\{y=1\} +p(\mathbf{x})\mathbbm{1}\{y=0\}]\] \[=\operatorname*{\mathbb{E}}_{x}\left[\mathbb{P}[y_{p}=0|\mathbf{x }]\,\mathbb{P}[y=1|\mathbf{x}]+\mathbb{P}[y_{p}=1|\mathbf{x}]\,\mathbb{P}[y= 0|\mathbf{x}]\right]\] \[=\operatorname*{\mathbb{E}}_{x}[\mathbb{P}[y\neq y_{p}|\mathbf{x }]]=\mathbb{P}[y\neq y_{p}]\]

## Appendix B Proofs from Section 2

### Proof of Theorem 2.1

To prove Theorem 2.1, we first prove the following more general theorem. Theorem 2.1 may then be easily recovered from this by setting \(\mathcal{H}=\operatorname*{GLM}_{g^{\prime},B}\) and observing that \(f^{\prime}(g^{\prime}(\mathbf{w}\cdot\mathbf{x}))=\mathbf{w}\cdot\mathbf{x}\), since \(g^{\prime}\) is invertible.

**Theorem B.1** (Squared Error Minimization through Distorted Matching Loss Minimization).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\), let \(0<\alpha\leq\beta\) and let \((f,g)\) be a pair of Fenchel-Legendre dual functions such that \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is continuous, non-decreasing and \(f^{\prime}:\operatorname{ran}(g^{\prime})\to\mathbb{R}\) is \([\frac{1}{\beta},\frac{1}{\alpha}]\) bi-Lipschitz. Let \(\varepsilon>0\) and \(\mathcal{H}\subseteq\{\mathbb{R}^{d}\to\operatorname{ran}(g^{\prime})\}\). Assume that for a predictor \(p:\mathbb{R}^{d}\to\operatorname{ran}(g^{\prime})\) we have_

\[\mathcal{L}_{g}(f^{\prime}\circ p\,;D)\leq\min_{h\in\mathcal{H}}\mathcal{L}_{g }(f^{\prime}\circ h\,;D)+\varepsilon\] (B.1)

_Then, for the predictor \(p\), we also have: \(\operatorname{err}_{2}(p)\leq\frac{\beta}{\alpha}\cdot\min_{h\in\mathcal{H}} \operatorname{err}_{2}(h)+2\beta\varepsilon\)._

Proof.: We apply Lemma 2.2 with \(y\gets y\) and \(p\gets p(\mathbf{x})\) and take expectations over \(D\) on both sides. We have that

\[\operatorname{err}_{2}(p)\leq 2\beta\cdot\mathbb{E}\,\ell_{g}(y,f^{\prime}(p( \mathbf{x})))-2\beta\cdot\mathbb{E}\,\ell_{g}(y,f^{\prime}(y))\]

Therefore, we can bound the squared error of \(p\) as follows.

\[\operatorname{err}_{2}(p) \leq 2\beta\cdot\mathcal{L}_{g}(f^{\prime}\circ p\,;D)-2\beta \cdot Q^{*}\] \[\leq 2\beta\cdot\mathcal{L}_{g}(f^{\prime}\circ h\,;D)-2\beta \cdot Q^{*}\] (by assumption, for any \[h\in\mathcal{H}\] )

where \(Q^{*}=\mathbb{E}\,\ell_{g}(y,f^{\prime}(y))\).

We now apply Lemma 2.2 again with \(y\gets y\) and \(p\gets h(\mathbf{x})\) and we similarly have

\[\mathbb{E}\,\ell_{g}(y,f^{\prime}\circ h(\mathbf{x}))-Q^{*}\leq\frac{1}{2 \alpha}\operatorname{err}_{2}(h)\]

Therefore, for any \(h\in\mathcal{H}\), we have, in total: \(\operatorname{err}_{2}(p)\leq\frac{\beta}{\alpha}\operatorname{err}_{2}(h)+2 \beta\varepsilon\). 

We first prove Lemma 2.2, which we restate here for convenience.

**Lemma B.2**.: _Assume \(f^{\prime}\) is \([1/\beta,1/\alpha]\) bi-Lipschitz and differentiable on all except from a finite number of points on any bounded interval. Then for any \(y,p\in\operatorname{ran}(g^{\prime})\) we have_

\[\ell_{g}(y,f^{\prime}(p))-\ell_{g}(y,f^{\prime}(y))=D_{f}(y,p)\in\left[\frac{ 1}{2\beta}(y-p)^{2},\frac{1}{2\alpha}(y-p)^{2}\right]\]

Proof.: We first show that \(\ell_{g}(y,f^{\prime}(p))-\ell_{g}(y,f^{\prime}(y))=D_{f}(y,p)\). In particular, we have

\[g(f^{\prime}(p))=f^{\prime}(p)g^{\prime}(f^{\prime}(p))-f(g^{\prime}(f^{ \prime}(p)))=pf^{\prime}(p)-f(p)\,,\]

since \(f^{\prime}(p)\in\operatorname{ran}(f^{\prime})\) and we know that \(g(t)=tg^{\prime}(t)-f(g^{\prime}(t))\) for any \(t\in\operatorname{ran}(f^{\prime})\) as well as \(g^{\prime}(f^{\prime}(p))=p\) for any \(p\in\operatorname{ran}(g^{\prime})\). Therefore, we have

\[\ell_{g}(y,f^{\prime}(p))-\ell_{g}(y,f^{\prime}(y)) =g(f^{\prime}(p))-yf^{\prime}(p)-g(f^{\prime}(y))+yf^{\prime}(y)\] \[=pf^{\prime}(p)-f(p)-yf^{\prime}(p)-yf^{\prime}(y)+f(y)+yf^{ \prime}(y)=\] \[=f(y)-f(p)-(y-p)f^{\prime}(p)=D_{f}(y,p)\,.\]

Let \(\psi:\operatorname{ran}(g^{\prime})\to\mathbb{R}\) be such that \(\psi^{\prime}(p)=f^{\prime}(p)\) and \(\psi^{\prime}\) is differentiable on the open interval between \(y\) and \(p\), with \(\psi^{\prime\prime}(\xi)\in[1/\beta,1/\alpha]\) for any \(\xi\) between \(y\) and \(p\). Let \(\gamma_{y}:=f(y)-\psi(y)\), \(\gamma_{p}:=f(p)-\psi(p)\) and \(\gamma_{\psi}:=2\max\{|\gamma_{y}|,|\gamma_{p}|\}\). Then we have that

\[D_{f}(y,p)=\psi(y)-\psi(p)-(y-p)\psi^{\prime}(p)+(\gamma_{y}-\gamma_{p})=\frac {1}{2}\psi^{\prime\prime}(\xi)(y-p)^{2}+(\gamma_{y}-\gamma_{p})\]

\[D_{f}(y,p)\in\left[\frac{1}{2\beta}(y-p)^{2}-2\gamma_{\psi},\frac{1}{2\alpha}(y -p)^{2}+2\gamma_{\psi}\right]\,,\]

for any \(\psi\) as defined above (say \(\psi\in\Psi\)). In particular, we have

\[D_{f}(y,p)\leq\frac{1}{2\alpha}(y-p)^{2}+2\inf_{\psi\in\Psi}\gamma_{\psi}\text{ and}\]

\[D_{f}(y,p)\geq\frac{1}{2\beta}(y-p)^{2}-2\inf_{\psi\in\Psi}\gamma_{\psi}\]

Since, we have only a finite number of points where the derivative is not well defined, a simple smoothening technique may give us \(\Psi\) such that \(\inf_{\psi\in\Psi}\gamma_{\psi}=0\).

Proofs from Section 3

### Proof of Theorem 3.2

We first define a boundedness property which we use in order to apply the results from Gopalan et al. (2023). The property states that the activation function (the partial inverse of the link function) must either have a range that covers all possible labels, or has a range whose closure covers all possible labels and the rate with which the labels are covered as we tend to the limits of the domain is at least polynomial. For example, the sigmoid activation tends to \(1\) (resp. \(0\)) exponentially fast as its argument increases (resp. decreases).

**Definition C.1** (Bounded Functions).: Let \(u:(0,1)\to\mathbb{R}\) be a non-decreasing function defined on the interval \((0,1)\). For \(R,\gamma\geq 0\), we say that \(u\) is \((R,\gamma)\)-bounded on \([0,1]\) if for any \(\varepsilon>0\), there are \(r_{0}\leq r_{1}\in[0,1]\) such that if we let \(u(r_{i})=\lim_{r\to r_{i}}u(r)\), \(i=0,1\) then

\[\max\{-u(r_{0}),u(r_{1})\} \leq R\left(\frac{1}{\varepsilon}\right)^{\gamma}\] \[(1-r_{1})(u(r)-u(r_{1})) \leq\varepsilon\text{ for }r\geq r_{1}\text{ and }\] \[r_{0}(u(r_{0})-u(r)) \leq\varepsilon\text{ for }r\leq r_{0}\]

**Proposition C.2**.: _Let \(u:(0,1)\to(-R,R)\) be non-decreasing and continuous, then \(u\) is \((R,0)\)-bounded._

We restate a more quantitative version of Theorem 3.2 here for convenience.

**Theorem C.3** (Omnipredictors for Matching Losses, combination of results in Gopalan et al. (2023)).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\times[0,1]\) whose marginal on \(\mathbb{R}^{d}\) has \(\lambda\)-bounded second moments. There is an algorithm that, given sample access to \(D\), with high probability returns a predictor \(p:\mathbb{R}\to(0,1)\) with the following guarantee. For any pair of Fenchel-Legendre dual functions \((f,g)\) such that \(g^{\prime}:\mathbb{R}\to\mathbb{R}\) is continuous, non-decreasing and \(L\)-Lipschitz, and \(f^{\prime}\) is \((R,\gamma)\)-bounded (see Definition C.1), \(p\) satisfies_

\[\mathcal{L}_{g}(f^{\prime}\circ p\,;D)\leq\min_{\|\mathbf{w}\|_{2}\leq B} \mathcal{L}_{g}(\mathbf{w}\,;D)+\varepsilon.\]

_The algorithm requires time_

\[O\left(d^{3}B^{4}L^{4}\lambda^{2}R^{3}\Big{(}\frac{3}{\varepsilon}\Big{)}^{3 +3\gamma}+dB^{2}L^{2}\lambda R^{4}\Big{(}\frac{3}{\varepsilon}\Big{)}^{4+4 \gamma}+B^{10}L^{10}\lambda^{5}R^{12}\Big{(}\frac{3}{\varepsilon}\Big{)}^{12+ 12\gamma}\log\Big{(}\frac{BL\lambda R}{\varepsilon^{1+\gamma}}\Big{)}\right)\]

_and sample complexity_

\[O\left(d^{2}B^{4}L^{4}\lambda^{2}R^{3}\Big{(}\frac{3}{\varepsilon}\Big{)}^{3 +3\gamma}+B^{8}L^{8}\lambda^{4}R^{10}\Big{(}\frac{3}{\varepsilon}\Big{)}^{10 +10\gamma}\log\Big{(}\frac{BL\lambda R}{\varepsilon^{1+\gamma}}\Big{)}\right)\]

Proof of Theorem 3.2.: The idea of the proof (given by Gopalan et al. (2023)) is that in each repetition of both the inner and the outer loop of Algorithm 1, there is a potential function which reduces by some amount that is bounded away above zero. The potential function is in fact the function \(\mathbf{E}_{D}\big{[}(p^{*}(\mathbf{x})-p(\mathbf{x}))^{2}\big{]}\), where \(p^{*}(\mathbf{x})=\mathbf{E}_{D}[y^{\prime}|\mathbf{x}]\) (for us, \(y^{\prime}\) is formed by drawing a Bernoulli random variable with probability of success \(y\), given an example \((\mathbf{x},y)\in\mathbb{R}^{d}\times[0,1]\) drawn from \(D\)). Since \(\mathbf{E}_{D}[(p^{*}(\mathbf{x})-p(\mathbf{x}))^{2}]\in[0,1]\), the number of iterations of each of the loops has to be bounded. Moreover, after the completion of each of the inner loops, the current value of \(p\) corresponds to an approximately multiaccurate predictor and note that the algorithm terminates if and only if a discretized version \(p^{\delta}\) of (the multiaccurate) \(p\) is approximately calibrated. The output is then \(p^{\delta}\) which is close to \(p\) (and hence also approximately multiaccurate), but also approximately calibrated. We will now present a small number of technical modifications we need to make in the proof of Gopalan et al. (2023) in order to specialize it to our setting. The main difference here is that we do not consider the distribution of \(\mathbf{x}\) to have bounded norm with probability \(1\), but we only assume it to have bounded second moments. For the following, we assume that \(g\) is \(1\)-Lipschitz, since we can set \(B\gets B\cdot L\) and push the Lipschitz constant in the domain of \(\mathbf{w}\).

Suppose first that the marginal of \(D\) on \(\mathbb{R}^{d}\) is supported on the unit ball \(\mathbb{B}_{d}\) and that the labels are binary. Then, the result would follow from Theorems 7.7 and A.4 of Gopalan et al. (2023). In particular,Theorem 7.7 states that given access to a weak learner with the specifications of Proposition A.1, there is an efficient algorithm that computes an \(\varepsilon_{1}\)-calibrated and \((\mathcal{C},\varepsilon_{1})\)-multiaccurate predictor \(p\), where the notions of calibration and multicancuracy originate to the literature of fairness and are defined, e.g., in Definitions 3.1 and 3.2 of Gopalan et al. (2023) and \(\mathcal{C}=\{\mathbf{x}\to\mathbf{w}\cdot\mathbf{x}\mid\|\mathbf{w}\|_{2} \leq B\}\cup\{\mathbf{x}\to 1\}\) (the class \(\mathcal{C}\) is bounded as long as \(\|\mathbf{x}\|_{2}\leq 1\) almost surely). Theorem A.4 states that for \(\varepsilon_{2}>0\), any \(\varepsilon_{1}\)-calibrated and \((\mathcal{C},\varepsilon_{1})\)-multiaccurate predictor \(p\) minimizes simultaneously the matching loss corresponding to any pair \((f,g)\in\mathcal{F}\) (where \(f\) is \((R,\gamma)\)-bounded) up to error

\[R(1/\varepsilon_{2})^{\gamma}\varepsilon_{1}+\varepsilon_{1}+\varepsilon_{2}\]

The expression above is formed by proving that any pair \((f,g)\in\mathcal{F}\) has the property that \(f^{\prime}\) is \((\varepsilon_{2},R(1/\varepsilon_{2})^{\gamma})\)-approximately optimal (as per the Definition A.1 of Gopalan et al. (2023)), for any \(\varepsilon_{2}>0\). In particular, we would like to show that for any \(\varepsilon_{2}>0\) there exists \(\widehat{f}^{\prime}\) such that the following is true for any \(r\in[0,1]\)

\[\ell_{g}(r,\widehat{f}^{\prime}(r)) \leq\ell_{g}(r,f^{\prime}(r))+\varepsilon_{2}\] \[|\widehat{f}^{\prime}(r)| \leq R\cdot(1/\varepsilon_{2})^{\gamma}\]

We may pick \(\widehat{f}^{\prime}\) as follows (for \(r_{0}\leq r_{1}\) as given by Definition C.1 for \(\varepsilon\leftarrow\varepsilon_{2}\), since \(f^{\prime}\) is \((R,\gamma)\)-bounded).

\[\widehat{f}^{\prime}(r)=\begin{cases}f^{\prime}(r),\text{ if }r\in[r_{0},r_{1}]\\ f^{\prime}(r_{0}),\text{ if }r<r_{0}\\ f^{\prime}(r_{1}),\text{ if }r>r_{1}\end{cases}\]

The desired result follows from using the expression for \(\ell_{g}\), the convexity of \(g\) (since \(g^{\prime}\) is non decreasing) and the guarantees of Definition C.1. In particular, we have that \(|\widehat{f}^{\prime}|\leq\max\{|f^{\prime}(r_{0})|,|f^{\prime}(r_{1})|\}\) since \(f^{\prime}\) is increasing. Moreover, for \(r\in[r_{0},r_{1}]\) we have \(\ell_{g}(r,\widehat{f}^{\prime}(r))=\ell_{g}(r,f^{\prime}(r))\) and for \(r<r_{0}\) we have

\[\ell_{g}(r,\widehat{f}^{\prime}(r)) =\ell_{g}(r,f^{\prime}(r_{0}))=g(f^{\prime}(r_{0}))-rf^{\prime}(r _{0})\] \[\leq r_{0}(f^{\prime}(r_{0})-f^{\prime}(r))+g(f^{\prime}(r))-rf^{ \prime}(r_{0})\] (since \[g\] is convex) \[\leq r_{0}(f^{\prime}(r_{0})-f^{\prime}(r))+g(f^{\prime}(r))- rf^{\prime}(r)\] (since \[f^{\prime}\] is increasing and \[r<r_{0}\] ) \[\leq\varepsilon_{2}+g(f^{\prime}(r))-rf^{\prime}(r)\] (since \[f^{\prime}\] is bounded according to Definition C.1 ) \[=\ell_{g}(r,f^{\prime}(r))+\varepsilon_{2}\]

Similarly, for \(r>r_{1}\), we have

\[\ell_{g}(r,\widehat{f}^{\prime}(r)) =\ell_{g}(r,f^{\prime}(r_{1}))=g(f^{\prime}(r_{1}))-rf^{\prime}(r _{1})\] \[\leq r_{1}(f^{\prime}(r_{1})-f^{\prime}(r))+g(f^{\prime}(r))-rf^{ \prime}(r_{1})\] (since \[g\] is convex) \[\leq(1-r_{1})(f^{\prime}(r_{1})-f^{\prime}(r))+g(f^{\prime}(r))+( 1-r)f^{\prime}(r_{1})-f^{\prime}(r)\] \[\leq(1-r_{1})(f^{\prime}(r_{1})-f^{\prime}(r))+g(f^{\prime}(r))- rf^{\prime}(r)\] (since \[f^{\prime}\] is increasing, \[1>r>r_{1}\] ) \[\leq\varepsilon_{2}+g(f^{\prime}(r))-rf^{\prime}(r)\] (since \[f^{\prime}\] is bounded according to Definition C.1 ) \[=\ell_{g}(r,f^{\prime}(r))+\varepsilon_{2}\]

In order to acquire \(R(1/\varepsilon_{2})^{\gamma}\varepsilon_{1}+\varepsilon_{1}+\varepsilon_{2}\leq\varepsilon\), we set \(\varepsilon_{2}=\varepsilon/3\) and \(\varepsilon_{1}=(\varepsilon/3)^{1+\gamma}/R\).

However, we only assume that the marginal distribution has \(\lambda\)-bounded second moments and we, therefore, need to make certain modifications to the proof of their Theorem 7.7. In particular, the boundedness assumption is used in the proofs of Lemma 7.2, Lemma 7.6 and Theorem 7.7 in Gopalan et al. (2023).

During the execution of Algorithm 1, two types of updates are made. The first type of update is done within the inner loop and corresponds to beginning from a predictor \(p_{\mathrm{old}}\) and acquiring \(p_{\mathrm{new}}\) which is the function \(\mathbf{x}\mapsto(p_{\mathrm{old}}(\mathbf{x})+\sigma(\mathbf{w}\cdot\mathbf{ x}))_{[0,1]}\), where \(\mathbf{w}\) is the output of the weak learner of our Proposition A.1, run with \(\varepsilon\leftarrow\varepsilon_{3}\) (\(\varepsilon_{3}\) will be specified later). To lower bound the decrease in the potential function during this type of update, we use a version of Lemma 7.6 in Gopalan et al. (2023). In this case, one needs to pick a step size \(\sigma\) that is polynomially smaller than the guarantee that the weak learner provides. In particular, if the weak learner of our Proposition A.1 is run with \(\varepsilon\leftarrow\varepsilon_{3}\), then one acquires (following the proof of Lemma 7.6 in Gopalan et al. (2023))

\[\mathbf{E}[(p^{*}(\mathbf{x})-p_{\mathrm{old}}(\mathbf{x}))^{2}]-\mathbf{E}[(p^ {*}(\mathbf{x})-p_{\mathrm{new}}(\mathbf{x}))^{2}]\geq\frac{\sigma\cdot \varepsilon_{3}}{2}-B^{2}\lambda\sigma^{2}\,.\]

If \(\sigma\) is picked to be \(\sigma=\frac{\varepsilon_{3}}{4B^{2}\lambda}\), then the quantity of interest \(\mathbf{E}[(p^{*}(\mathbf{x})-p_{\mathrm{old}}(\mathbf{x}))^{2}]-\mathbf{E}[( p^{*}(\mathbf{x})-p_{\mathrm{new}}(\mathbf{x}))^{2}]\) is lower bounded by \(\frac{\varepsilon_{3}^{2}}{16B^{2}\lambda}\). Hence the number of iterations of the inner loop is upper bounded by \(O(\frac{B^{2}\lambda}{\varepsilon_{3}^{2}})\). Note that in their original algorithm, \(\sigma\) was picked equal to \(\varepsilon_{3}\) and this is why \(\varepsilon_{3}\) (or another corresponding parameter) does not appear in their proofs. The updated choice of \(\sigma\) generates a polynomial overhead in time and sample complexity. We pick \(\varepsilon_{3}=\frac{1}{2}(\varepsilon_{1}-B\sqrt{\lambda\delta})\) so that each time we exit the inner loop, we have that, with high probability, the current value of \(p\) corresponds to an \((\varepsilon_{1}-B\sqrt{\lambda\delta})\)-multiaccurate predictor.

The second type of update (the outer loop update) is a calibration step where \(p_{\mathrm{new}}\) is set to be the following function with the notation of Algorithm 1

\[\mathbf{x}\mapsto\sum_{j=0}^{1/\delta}\bar{y}_{j}\mathbbm{1}\{p_{\mathrm{old} }(\mathbf{x})\in I_{j}\}\]

In this case, Corollary 7.5 of Gopalan et al. (2023) can be used as is to acquire that \(\mathbf{E}[(p^{*}(\mathbf{x})-p_{\mathrm{old}}(\mathbf{x}))^{2}]-\mathbf{E}[( p^{*}(\mathbf{x})-p_{\mathrm{new}}(\mathbf{x}))^{2}]\geq\varepsilon_{1}^{2}/8\) if we set \(\delta\leq\varepsilon_{1}^{2}/C\) for some large enough universal constant \(C>0\). Hence, the number of iterations of the outer loop is upper bounded by \(O(1/\varepsilon_{1}^{2})\).

It remains to show that once the algorithm terminates, the output is approximately calibrated and multiaccurate. Regarding multiaccuracy, whenever we exit the inner loop, the current value of \(p\) corresponds (with high probability) to an \((\varepsilon_{1}-B\sqrt{\lambda\delta})\)-multiaccurate predictor. The predictor \(p^{\delta}\) is close to \(p\) in absolute distance and therefore can be shown to be approximately multiaccurate using some version of Lemma 7.2 in Gopalan et al. (2023). In particular, following the proof of Lemma 7.2, the multiaccuracy guarantee for \(p^{\delta}\) changes to \((\mathcal{C},\alpha+B\sqrt{\lambda\delta})\)-multiaccuracy, where \(\alpha\) is the multiaccuracy guarantee for \(p\), by using a Cauchy-Schwarz inequality and bounding \(\mathbb{E}[(\mathbf{w}\cdot\mathbf{x})^{2}]\) by \(B^{2}\cdot\lambda\). Since \(\alpha=\varepsilon_{1}-B\sqrt{\lambda\delta}\), the multiaccuracy guarantee is \(\varepsilon_{1}\). The calibration guarantee follows by the fact that the termination criterion corresponds to empirically checking whether \(p^{\delta}\) is calibrated and by using Lemma 7.4 in Gopalan et al. (2023), the empirical estimate should be close to the true calibration error. Overall, once we terminate, the output is \(\varepsilon_{1}\) calibrated and multiaccurate.

Since the time and sample complexity of each of the calls of the weak learner depends on \(\varepsilon_{3}\), we pick \(\delta\leftarrow\frac{\varepsilon_{1}^{2}}{C(1+B^{2}\lambda)}\) so that \(\varepsilon_{3}=\Theta(\varepsilon_{1})\) and \(\delta\leq\frac{\varepsilon_{1}^{2}}{C}\) as required.

Sample Complexity.For each of the inner loop iterations, we require a fresh sample of size \(O(\frac{d^{2}B^{2}\lambda}{\varepsilon_{1}^{2}})\), as specified by Proposition A.1 (by setting the probability of success to a sufficiently small constant). For the outer loop, we require \(O(\frac{1+B^{8}\lambda^{4}}{\varepsilon_{1}^{2}}\log(\frac{1+B\lambda}{ \varepsilon_{1}}))\) samples per iteration, so that the calibration error estimate is accurate enough, according to Lemma 7.4 in Gopalan et al. (2023).

Time Complexity.Each of the inner loop iterations requires \(O(\frac{d^{3}B^{2}\lambda}{\varepsilon_{1}^{2}})\) time and each of the outer loop iterations requires an additional \(O(d\frac{1+B^{2}\lambda}{\varepsilon_{1}^{2}}+\frac{1+B^{10}\lambda^{5}}{ \varepsilon_{1}^{10}}\log(\frac{1+B\lambda}{\varepsilon_{1}}))\) time.

The final technical complication we need to address is that their algorithm works only given binary labels. We can, however, form binary labels as follows. Let \((\mathbf{x},y)\) be drawn from \(D\). We have that \(y\in[0,1]\). Given \(y\), we draw an independent Bernoulli random variable \(y^{\prime}\) with probability of success \(y\), forming the distribution \(D^{\prime}\) over \(\mathbb{R}^{d}\times\{0,1\}\). We run the algorithm of Gopalan et al. (2023) on \(D^{\prime}\) and receive a predictor \(p\) such that

\[\mathcal{L}_{g}(f^{\prime}\circ p\;;D^{\prime})\leq\min_{\mathbf{w}\in\mathcal{W }}\mathcal{L}_{g}(\mathbf{w}\;;D^{\prime})+\varepsilon\,,\ \text{ for any }(f,g)\in\mathcal{F}\]We have that

\[\mathcal{L}_{g}(c\,;D^{\prime}) =\operatorname*{\mathbb{E}}_{\mathbf{x},y^{\prime}}[g(c(\mathbf{x}) )-y^{\prime}c(\mathbf{x})]\] \[=\operatorname*{\mathbb{E}}_{\mathbf{x}}\left[g(c(\mathbf{x}))- \operatorname*{\mathbb{E}}_{y}\left[\operatorname*{\mathbb{E}}_{y^{\prime}} \left[y^{\prime}\right]y\right]\biggm{|}\mathbf{x}\biggm{|}c(\mathbf{x})\right]\] \[=\operatorname*{\mathbb{E}}_{\mathbf{x}}\left[g(c(\mathbf{x}))- \operatorname*{\mathbb{E}}_{y}\left[y\right]\mathbf{x}\right]c(\mathbf{x})\right]\] \[=\operatorname*{\mathbb{E}}_{\mathbf{x},y}[g(c(\mathbf{x}))-yc( \mathbf{x})]=\mathcal{L}_{g}(c\,;D)\]

This concludes the proof of Theorem 3.2. 

### Proof of Lemma 3.3

We first apply Theorem 2.1 with \(g^{\prime}\leftarrow\phi^{\prime}\) to get that for \(\phi^{\prime}_{\mathbf{w}}(\mathbf{x})=\phi^{\prime}(\mathbf{w}\cdot\mathbf{ x})\), we have

\[\operatorname*{err}_{2}(p) \leq\frac{\beta}{\alpha}\operatorname*{err}_{2}(\phi^{\prime}_{ \mathbf{w}^{*}})+2\beta\varepsilon\] (since inequality holds for \[\mathbf{w}\in\mathcal{W}\] )

Moreover, we have

\[\operatorname*{err}_{2}(\phi^{\prime}_{\mathbf{w}^{*}}) =\operatorname*{\mathbb{E}}\left[\left(y-\phi^{\prime}(\mathbf{w}^ {*}\cdot\mathbf{x})\right)^{2}\right]\] \[=\operatorname*{\mathbb{E}}\left[\left(y-g^{\prime}(\mathbf{w}^{ *}\cdot\mathbf{x})+g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})-\phi^{\prime}( \mathbf{w}^{*}\cdot\mathbf{x})\right)^{2}\right]\] \[\leq 2\operatorname*{\mathbb{E}}\left[\left(y-g^{\prime}( \mathbf{w}^{*}\cdot\mathbf{x})\right)^{2}\right]+2\operatorname*{\mathbb{E}} \left[\left(g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})-\phi^{\prime}(\mathbf{w}^ {*}\cdot\mathbf{x})\right)^{2}\right]\] \[=2\operatorname*{opt}_{g}+2\operatorname*{\mathbb{E}}\left[\left(g ^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})-\phi^{\prime}(\mathbf{w}^{*}\cdot \mathbf{x})\right)^{2}\right]\]

This concludes the proof of lemma 3.3.

## Appendix D Proofs from Section 4

### Proof of Theorem 4.1

In the case we consider here, \(g^{\prime}\) is the sigmoid activation, i.e., \(g^{\prime}(t)=(1+e^{-t})^{-1}\) for any \(t\in\mathbb{R}\). In particular, the pointwise surrogate loss we consider satisfies

\[\ell_{g}(y,f^{\prime}(p))=y\ln\frac{1}{p}+(1-y)\ln\frac{1}{1-p}-\ln 2\,,\]

for any \(y\in[0,1]\) and \(p\in(0,1)\). We may extend Lemma 4.2 to also capture \(y\in\{0,1\}\), by defining \(\ell_{g}(0,f^{\prime}(0))=\ell_{g}(1,f^{\prime}(1))=-\ln 2\) (the inequality would hold under this definition). Hence, following a similar procedure as the one used for proving Theorem B.1, we obtain the following by applying Lemma 4.2

\[\operatorname*{err}_{2}(p) \leq 2\left(\mathcal{L}_{g}(f^{\prime}\circ p)-\operatorname*{ \mathbb{E}}[\ell_{g}(y,f^{\prime}(y))]\right)\] (D.1) \[\mathcal{L}_{g}(\mathbf{w}^{*})-\operatorname*{\mathbb{E}}[\ell_ {g}(y,f^{\prime}(y))] \leq\operatorname*{\mathbb{E}}\left[\frac{2}{g^{\prime}(\mathbf{w} ^{*}\cdot\mathbf{x})\vee(1-g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x}))} \cdot(y-g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x}))^{2}\right]\] (D.2) \[\mathcal{L}_{g}(f^{\prime}\circ p) \leq\mathcal{L}_{g}(\mathbf{w}^{*})+\varepsilon\] (D.3)

Therefore, in order to prove Theorem 4.1, it is sufficient to provide a strong enough upper bound for the quantity of the right hand side of Equation (D.2) in terms of \(\operatorname*{opt}_{g}\). We observe that

\[\frac{2}{g^{\prime}(\mathbf{w}^{*}\cdot\mathbf{x})\vee(1-g^{\prime}(\mathbf{w }^{*}\cdot\mathbf{x}))}\leq 4\exp(|\mathbf{w}^{*}\cdot\mathbf{x}|)\,,\text{ for any }\mathbf{x}\in \mathbb{R}^{d}\]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]