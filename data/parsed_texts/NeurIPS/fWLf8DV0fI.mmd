# Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules

 Zhiyuan Liu\({}^{\dagger}\) Yaorui Shi\({}^{\ddagger}\) An Zhang\({}^{\dagger}\) Enzhi Zhang\({}^{\lx@sectionsign}\)

Kenji Kawaguchi\({}^{\dagger}\) Xiang Wang\({}^{\ddagger}\) Tat-Seng Chua\({}^{\dagger}\)

\({}^{\dagger}\)National University of Singapore, \({}^{\ddagger}\)University of Science and Technology of China

\({}^{\lx@sectionsign}\)Hokkaido University

{acharkq,shiyaorui,xiangwang1223}@gmail.com,anzhang@u.nus.edu

enzhi.zhang.n6@elms.hokudai.ac.jp, {kenji,chuats}@comp.nus.edu.sg

###### Abstract

Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (_i.e.,_ subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in MGM. Our results show that a subgraph-level tokenizer and a sufficiently expressive decoder with remask decoding have a large impact on the encoder's representation learning. Finally, we propose a novel MGM method **SimSGT**, featuring a **S**imple **G**NN-based **T**okenizer (**SGT**) and an effective decoding strategy. We empirically validate that our method outperforms the existing molecule self-supervised learning methods. Our codes and checkpoints are available at https://github.com/syr-cn/SimSGT.

## 1 Introduction

Molecular representation learning (MRL) [1; 2; 3] is a critical research area with numerous vital downstream applications, such as molecular property prediction [4], drug discovery [5; 6], and retrosynthesis [7; 8]. Given that molecules can be represented as graphs, graph self-supervised learning (SSL) is a natural fit for this problem. Among the various graph SSL techniques, Masked Graph Modeling (MGM) has recently garnered significant interest [9; 10; 11].

In this paper, we study MRL through MGM, aiming to pretrain a molecule encoder for subsequent fine-tuning in downstream applications. After looking at the masked modeling methods in graph [12; 9; 10], language [13; 14], and computer vision [15; 16], we summarize that MGM relies on three key components - graph tokenizer, graph masking, and graph autoencoder, as Figure 1 shows:

* **Graph tokenizer.** Given a graph \(g\), the graph tokenizer employs a graph fragmentation function [1; 17; 18; 2] to break \(g\) into smaller subgraphs, such as nodes and motifs. Then, thesefragments are mapped into fixed-length tokens to serve as the targets being reconstructed later. Clearly, the granularity of graph tokens determines the abstraction level of representations in masked modeling [19, 16]. This is especially relevant for molecules, whose properties are largely determined by patterns at the granularity of subgraphs [20]. For example, the molecule shown in Figure 2 contains a benzene ring subgraph. Benzene ring confers the molecule aromaticity, making it more stable than saturated compounds that only have single bonds [21]. Therefore, applying graph tokenizers that generate subgraph-level tokens might improve the downstream performances.
* **Graph masking.** Before feeding into the autoencoder, \(g\) is corrupted by adding random noise, typically through randomly masking nodes or dropping edges [12, 11]. Graph masking is crucial to prevent the autoencoder from merely copying the input, and guide the autoencoder to learn the relationships between co-occurring graph patterns.
* **Graph autoencoder.** Graph autoencoder consists of a graph encoder and a graph decoder [12, 9]. The graph encoder generates the corrupted graph's hidden representations, based on which the graph decoder attempts to recover the corrupted information. The encoder and the decoder are jointly optimized by minimizing the distance between the decoder's outputs and the reconstruction targets, _i.e.,_ the graph tokens induced by the graph tokenizer. Given the complex subgraph-level tokens as targets, an effective reconstruction might demand a sufficiently expressive graph decoder.

Although all three components mentioned above are crucial, previous MGM studies primarily focus on graph masking [12, 11, 22, 23] and graph encoder [2, 24, 25, 26], with less emphasis on the tokenizers and decoders. For example, while there exist extensive motif-based fragmentation functions for MRL [1, 17, 18, 2], they have been overlooked as tokenizers for MGM. Moreover, many previous works [12, 10, 11, 22, 23, 2, 25] employ a linear or MLP decoder for graph reconstruction, leaving more expressive decoders largely unexplored.

In this work, we first summarize the various fragmentation functions as graph tokenizers, at the granularity of nodes, edges, motifs, and Graph Neural Networks (GNNs). Given this summary, we systematically evaluate their empirical performances for MGM. Our analysis shows that reconstructing subgraph-level tokens in MGM can improve over the node tokens. Moreover, we find that a sufficiently expressive decoder combined with remask decoding [9] could improve the encoder's representation quality. Notably, remask "decouples" the encoder and decoder, redirecting the encoder's focus away from molecule reconstruction and more towards MRL, leading to better downstream performances. In summary, we reveal that incorporating a subgraph-level tokenizer and a sufficiently expressive decoder with remask decoding gives rise to improved MGM performance.

Based on the findings above, we propose a novel pretraining framework - Masked Graph Modeling with a Simple GNN-based Tokenizer (**SimSGT**). SimSGT employs a **Simple GNN-**T**okenizer (**SGT**) that removes the nonlinear update function in each GNN layer. Surprisingly, we show that a single-layer SGT demonstrates competitive or better performances compared to other pretrained GNN-based and chemistry-inspired tokenizers. SimSGT adopts the GraphTrans [27] architecture for its encoder and a smaller GraphTrans for its decoder, in order to provide sufficient capacity for both the tasks of MRL and molecule reconstruction. Furthermore, we propose remask-v2 to decouple the encoder and decoder of the GraphTrans architecture. Finally, SimSGT is validated on downstream molecular property prediction and drug-target affinity tasks [28, 29], surpassing the leading graph SSL methods (_e.g.,_ GraphMAE [9] and Mole-BERT [10]).

## 2 Preliminary

In this section, we begin with the introduction of MGM. Then, we provide a categorization of existing graph tokenizers. Finally, we discuss the architecture of graph autoencoders for MGM.

**Notations.** Let \(\mathcal{G}\) denote the space of graphs. A molecule can be represented as a graph \(g=(\mathcal{V},\mathcal{E})\in\mathcal{G}\), where \(\mathcal{V}\) is the set of nodes and \(\mathcal{E}\) is the set of edges. Each node \(i\in\mathcal{V}\) is associated with a node feature \(\bm{x}_{i}\in\mathbb{R}^{d_{0}}\) and each edge \((i,j)\in\mathcal{E}\) is associated with an edge feature \(\bm{e}_{ij}\in\mathbb{R}^{d_{1}}\). The graph \(g\)'s structure can also be represented by its adjacency matrix \(\mathbf{A}\in\{0,1\}^{|\mathcal{V}|\times|\mathcal{V}|}\), such that \(\mathbf{A}_{ij}=1\) if \((i,j)\in\mathcal{E}\) and \(\mathbf{A}_{ij}=0\) otherwise.

### Preliminary: Masked Graph Modeling

Here we illustrate MGM's three key steps: graph tokenizer, graph masking, and graph autoencoder.

**Graph tokenizer.** Given a graph \(g\), we leverage a graph tokenizer \(tok(g)=\{\mathbf{y}_{t}=m(t)\in\mathbb{R}^{d}|t\in f(g)\}\) to generate its graph tokens as the reconstruction targets. The tokenizer \(tok(\cdot)\) is composed of a fragmentation function \(f\) that breaks \(g\) into a set of subgraphs \(f(g)=\{t=(\mathcal{V}_{t},\mathcal{E}_{t})|t\subseteq g\}\), and a mapping function \(m(t)\in\mathbb{R}^{d}\) that transforms the subgraphs into fixed-length vectors. In this work, we allow \(f(g)\) to include overlapped subgraphs to enlarge the scope of graph tokenizers.

**Graph masking.** Further, we add noises to \(g\) by random node masking. Here we do not use edge dropping because Hou _et al._[9] empirically show that edge dropping easily leads to performance drop in downstream tasks. Specifically, node masking samples a random subset of nodes \(\mathcal{V}_{m}\subseteq\mathcal{V}\) and replaces their features with a special token \(\mathbf{m}_{0}\in\mathbb{R}^{d_{0}}\). We denote the masked node feature by \(\tilde{\mathbf{x}}_{i}\):

\[\tilde{\mathbf{x}}_{i}=\begin{cases}\mathbf{m}_{0},&\forall i\in\mathcal{V}_{ m}\\ \mathbf{x}_{i},&\text{otherwise}\end{cases}.\] (1)

**Graph autoencoder.** The corrupted graph \(\tilde{g}\) is then fed into a graph autoencoder for graph reconstruction. We defer the details of the graph autoencoder's architecture to Section 2.3. Let \(\{\mathbf{z}_{i}|i\in\mathcal{V}\}\) be the node-wise outputs of the graph autoencoder. We obtain subgraph \(t\)'s prediction \(\hat{\mathbf{y}}_{t}=\text{MEAN}(\{\mathbf{z}_{i}|i\in\mathcal{V}_{t}\})\) by mean pooling the representations of its nodes, if not especially noted. The graph autoencoder is trained by minimizing the distance between the predictions \(\{\hat{\mathbf{y}}_{t}|t\in f(g)\}\) and the targets \(tok(g)=\{\mathbf{y}_{t}|t\in f(g)\}\). The reconstruction loss is accumulated on tokens that include corrupted information \(\{t|t\in f(g),\mathcal{V}_{t}\cap\mathcal{V}_{m}\neq\emptyset\}\):

\[L_{0}=\frac{1}{|f(g)|}\sum_{t\in f(g),\mathcal{V}_{t}\cap\mathcal{V}_{m}\neq \emptyset}\ell(\hat{\mathbf{y}}_{t},\mathbf{y}_{t}),\] (2)

where \(\ell(\cdot,\cdot)\) is the loss function, dependent on the type of \(\mathbf{y}_{t}\). We use mean square error [15] for \(\ell(\cdot,\cdot)\) when \(\mathbf{y}_{t}\) is a continuous vector, and use cross-entropy [16] for \(\ell(\cdot,\cdot)\) when \(\mathbf{y}_{t}\) is a discrete value.

### Revisiting Molecule Tokenizers

Scrutinizing the current MRL methods, we summarize the molecule tokenizers into four distinct categories, as Table 1 shows. A detailed description of the first three categories is systematically provided here, while the Simple GNN-based Tokenizer is introduced in Section 3.

**Node, edge tokenizer [12; 11].** A graph's nodes and edges can be used as graph tokens directly:

\[tok_{\text{node}}(g)=\{\mathbf{y}_{i}=\bm{x}_{i}|i\in\mathcal{V}\},\quad tok_ {\text{edge}}(g)=\{\mathbf{y}_{ij}=\bm{e}_{ij}|(i,j)\in\mathcal{E}\}.\] (3)

Figure 2(a) illustrates the use of atomic numbers of nodes and bond types of edges as graph tokens in a molecule. These have been widely applied in previous research [12; 11; 25; 24], largely due to their

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Tokenizers** & **Subgraph types** & **Tokens** & **Potential limitations** \\ \hline Node, edge & Nodes and edges & Features of nodes and edges & Low-level feature \\ Motif & FGs, cycles, _etc._ & Motif types & Rely on expert knowledge \\ Pretrained GNN & Rooted subtrees & Frozen GNN representations & Extra pretraining for tokenizer \\ Simple GNN & Rooted subtrees & Frozen GNN representations & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of graph tokenizers.

simplicity. However, atomic numbers and bond types are low-level features. Reconstructing them may be suboptimal for downstream tasks that require a high-level understanding of graph semantics.

**Motif-based tokenizer.** Motifs are statistically significant subgraph patterns in graph structures. For molecules, functional groups (FGs) are motifs that are manually curated by experts based on the FGs' biochemical characteristics [30; 31]. For example, molecules that contain benzene rings exhibit the property of aromaticity. Considering that the manually curated FGs are limited and cannot fully cover all molecules, previous works [32; 1; 18] employ chemistry-inspired fragmentation functions for motif discovery. Here we summarize the commonly used fragmentation functions:

* **FGs**[18; 2]. An FG is a molecule's subgraph that exhibits consistent chemical behaviors across various compounds. In chemistry toolkits [30; 31], the substructural patterns of FGs are described by the SMARTS language [33]. Let \(\mathcal{S}_{0}=\left\{s_{i}\right\}_{i=1}^{|\mathcal{S}_{0}|}\) be a set of SMARTS patterns for FGs, and let \(p_{s}(g)\) be the function returning the set of \(s\) FGs in \(g\). FG-based fragmentation works as: \[f_{\text{FG}}(g,\mathcal{S}_{0})=\cup_{s\in\mathcal{S}_{0}}p_{s}(g),\] (4)
* **Cycles**[32; 1; 18]. Cycles in molecules are often extracted as motifs due to their potential chemical significance. Figure 2(b) depicts the process of fragmenting a five-node cycle as a motif. If two cycles overlapped more than two atoms, they can be merged because they constitute a bridged compound [34]. Let \(C_{n}\) represent a cycle of \(n\) nodes. They can be written as: \[f_{\text{cycle}}(g) =\{t|t=C_{|t|},t\subseteq g\},\] (5) \[f_{\text{cycle-merge}}(g) =\{t_{i}\cup t_{j}|t_{i},t_{j}\in f_{\text{cycle}}(g),i\neq j,|t_ {i}\cap t_{j}|>2\},\] (6)
* **BRICS**[35; 1]. BRICS fragments a molecule at the potential cleavage sites, where chemical bonds can be broken under certain environmental conditions or catalysts. The key step of BRICS is to identify a molecule \(g\)'s potential cleavage sites, denoted by \(\psi(g)\). This is achieved by finding bonds with both sides matching one of the pre-defined "FG-like" substructural patterns \(\mathcal{S}_{1}\) in BRICS: \[\psi(g)=\cup\{\mathcal{E}\setminus(\mathcal{E}_{t}\cup\mathcal{E}_{g-t})|t,g- t\in f_{\text{FG}}(g,\mathcal{S}_{1})\},\] (7) where \(g-t=g[\mathcal{V}\setminus\mathcal{V}_{t}]\) denotes deleting \(g\)'s nodes in \(t\) and the corresponding incident edges [36]; \(\mathcal{E}\setminus(\mathcal{E}_{t}\cup\mathcal{E}_{g-t})\) contains the bond that connects \(t\) and \(g-t\). Next, \(g\) is fragmented into the maximum subgraphs that contain no bonds in the cleavage sites \(\psi(g)\): \[f_{\text{BRICS}}(g)=\{t|\psi(g)\cap\mathcal{E}_{t}=\emptyset,f_{\text{BRICS}}(t )\cap 2^{t}=\{t\},t\subseteq g\}.\] (8)

Figure 3: Examples for the first three types of graph tokenizers and their induced subgraphs. (b) A motif-based tokenizer that applies the fragmentation functions of cycles and the remaining nodes. (c) A two-layer GIN-based tokenizer that extracts 2-hop rooted subtrees for every node in the graph.

Note that, the original BRICS includes more rules, such as a constraint on \(t\) and \(g-t\)'s combinations. We present only the key step here for simplicity.
* **Remaining nodes and edges**[32, 18]: Given another fragmentation function \(f_{0}\), the nodes and edges that are not included in any of \(f_{0}\)'s outputs are treated as individual subgraphs. This improves \(f_{0}\)'s coverage on the original graph. Figure 2(b) shows an example of fragmenting remaining nodes after \(f_{\text{cycle}}\): nodes that are not in any cycles are treated as individual subgraphs.

To obtain more fine-grained subgraphs, previous works [1, 32, 18] usually combine several fragmentation functions together by unions (_e.g., \(f_{1}(g)\cup f_{2}(g)\)_) or compositions (_e.g., \(\{f_{2}(t)|t\in f_{1}(g)\}\)_). Let \(f_{\text{motif}}\) be the final fragmentation function after combination. We break every molecule in the dataset by \(f_{\text{motif}}\) and gather a motif vocabulary \(\mathcal{M}\), which filters out infrequent motifs by a threshold. Then, given a new molecule \(g^{\prime}\), we can generate its tokens by one-hot encoding its motifs \(f_{\text{motif}}(g^{\prime})\):

\[tok_{\text{motif}}(g^{\prime})=\{\mathbf{y}_{t}=\text{one-hot}(t,\mathcal{M}) |t\in f_{\text{motif}}(g^{\prime})\}.\] (9)

**Pretrained GNN-based tokenizer [10].** Pretrained GNNs can serve as graph tokenizers. Take a \(k\)-layer Graph Isormophism Network (GIN) [37] as an example. Its node embedding summarizes the structural information of the node's \(k\)-hop rooted subtree, making it a subgraph-level graph token (Figure 2(c)). A GIN performs the fragmentation and the mapping simultaneously. It can be written as:

\[tok_{\text{GIN}}(g)=\{\mathbf{y}_{i}=\text{SG}(\bm{h}_{i}^{(k)})|i \in\mathcal{V}\},\] (10) \[\bm{h}_{i}^{(k)}=\text{COMBINE}^{(k)}(\bm{h}_{i}^{(k-1)},\text{AGGREGATE }^{(k)}(\{\bm{h}_{j}^{k-1},j\in\mathcal{N}(i)\})),\] (11)

where \(\text{AGGREGATE}(\cdot)\) collects information from node \(i\)'s neighbors, based on which \(\text{COMBINE}(\cdot)\) updates \(i\)'s representation. \(\text{SG}(\cdot)\) denotes stop-gradient, which stops the gradient flow to the tokenizer during MGM pretraining. In addition to GINs, other GNNs can also serve as tokenizers. To obtain meaningful graph tokens, we pretrain a GNN before employing it as a tokenizer [10]. Once pretrained, this GNN is frozen and employed for the subsequent MGM pretraining. In Section 4, we evaluate the prevalent graph pretraining strategies for tokenizers. Given that GNN-based tokenizers provide node-wise tokens, we directly minimize the distance between the graph tokens and the autoencoder's outputs \(\{\mathbf{z}_{i}\}_{i=1}^{|\mathcal{V}|}\) of the masked nodes \(\mathcal{V}_{m}\), _i.e.,_\(L_{0}=\frac{1}{|\mathcal{V}_{m}|}\sum_{i\in\mathcal{V}_{m}}\ell(\hat{\mathbf{y} }_{i}=\mathbf{z}_{i},\mathbf{y}_{i})\).

### Revisiting Graph Autoencoders

**Background.** Graph autoencoder consists of a graph encoder and a graph decoder. We pretrain them with the objective of graph reconstruction. Once well pre-trained, the encoder is saved for downstream tasks. MGM works [12, 2] usually adopt expressive graph encoders like GINEs [12] and Graph Transformers [2]. However, the exploration on expressive decoders has been limited. Many previous works [12, 10, 11, 22, 2, 25] apply a linear or an MLP decoder, similar to BERT's design [13].

However, recent studies [15, 38] have revealed the disparity between representation learning and reconstruction tasks. They show that a sufficiently expressive decoder could improve the encoder's representation quality. Delving into these studies, we identify two key elements to improve the representation quality: a sufficiently expressive decoder and remask decoding [9].

**Sufficiently expressive decoder.** Following [15, 38], we devise smaller versions of the encoder's architecture as decoders. We adopt the GINE [12] and **G**raph**Trans**[27] (denoted as **GTS**) as encoders. GTS stacks transformer layers on top of the GINE layers to improve the ability of modeling global interactions. Table 2 summarizes their different versions that we compare in this work.

**Remask decoding [9].** Remask controls the focus of the encoder and the decoder. Let \(\{\bm{h}_{i}|i\in\mathcal{V}\}\) be the encoder's node-wise hidden representations for the masked graph. Remask decoding masks the hidden representations of the masked nodes \(\mathcal{V}_{m}\) again by a special token \(\mathbf{m}_{1}\in\mathbb{R}^{d}\) before feeding them into the decoder. Formally, the remasked hidden representation \(\bm{\tilde{h}}_{i}\) is as follows:

\[\bm{\tilde{h}}_{i}=\begin{cases}\mathbf{m}_{1},&\forall i\in\mathcal{V}_{m}\\ \bm{h}_{i},&\text{otherwise}\end{cases}.\] (12)

\begin{table}
\begin{tabular}{l|c} \hline \hline Model & GINE, dim 300 Transformer, dim 128 \\ \hline Linear & - & - \\ GINE & 5 layer & - \\ GINE-Small & 3 layer & - \\ GTS & 5 layer & 4 layer \\ GTS-Small & 3 layer & 1 layer \\ GTS-Tiny & 1 layer & 1 layer \\ \hline \hline \end{tabular}
\end{table}
Table 2: The compared GNN architectures for encoders and decoders.

Remarks the encoder's ability on predicting the corrupted information by removing the encoder's representation on the masked part. The encoder is enforced to generate effective representations for the unmasked part, to provide signals for the decoder for graph reconstruction.

## 3 Methodology

In this section, we present our method - Masked Graph Modeling with a Simple GNN-based Tokenizer (**SimSGT**) (Figure 4). Specifically, it applies the GTS [27] architecture for both its encoder and decoder. SimSGT features a **S**imple **G**NN-based **T**okenizer (**SGT**), and employs a new remask strategy to decouple the encoder and decoder of the GTS architecture.

**Simple GNN-based Tokenizer.** SGT simplifies existing aggregation-based GNNs [37] by removing the nonlinear update functions in GNN layers. It is inspired by studies showing that carefully designed graph operators can generate effective node representations [39; 40]. Formally, a \(k\)-layer SGT is:

\[tok_{\text{SGT}}(g) =\{\mathbf{y}_{i}=\text{SG}([\mathbf{H}_{i}^{(1)},...,\mathbf{H} _{i}^{(k)}])|i\in\mathcal{V}\},\] (13) \[\mathbf{H}_{i}^{(0)} =\text{Embedding}(\mathbf{x}_{i})\in\mathbb{R}^{d}, \forall i\in\mathcal{V},\] (14) \[\hat{\mathbf{H}}^{(l)} =\omega(\mathbf{A})\cdot\mathbf{H}^{(l-1)}\in\mathbb{R}^{| \mathcal{V}|\times d}, 1\leq l\leq k,\] (15) \[\mathbf{H}^{(l)} =\text{BatchNorm}(\hat{\mathbf{H}}^{(l)}),\] (16)

where \(\text{Embedding}(\cdot)\) is a linear layer that uses the weights of the encoder's node embedding function; \(\mathbf{H}_{i}^{(l)}\) is the \(i\)-th row of \(\mathbf{H}^{(l)}\); \(\text{BatchNorm}(\cdot)\) is a standard Batch Normalization layer without the trainable scaling and shifting parameters [41]; and \(\omega(\mathbf{A})\) is the graph operator that represents the original GNN's aggregation function. For example, GIN has \(\omega(\mathbf{A})=\mathbf{A}+(1+\epsilon)\mathbf{I}\) and GCN has \(\omega(\mathbf{A})=\tilde{\mathbf{D}}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D }}^{-1/2}\), where \(\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) and \(\tilde{\mathbf{D}}\) is the degree matrix of \(\tilde{\mathbf{A}}\).

Note that, SGT does not have trainable weights, allowing its deployment without pretraining. Its tokenization ability relies on the graph operator \(\omega(\mathbf{A})\) that summarizes each node's neighbor information. Additionally, we concatenate the outputs of every SGT layer to include multi-scale information. We show in experiments that SGT transforms the original GNN into an effective tokenizer.

**Graph autoencoder.** SimSGT employs the GTS architecture as the encoder, and a smaller version of GTS (_i.e.,_ GTS-Small in Table 2) as the decoder. This architecture follows the asymmetric encoder-decoder design in previous works [15; 38]. Further, we propose a new remask strategy, named **remask-v2**, to decouple SimSGT's encoder layers and decoder layers,

**Remask-v2.** Remask-v2 constrains the encoder's ability on predicting the corrupted information by dropping the masked nodes' \(\mathcal{V}_{m}\) representations before the Transformer layers (Figure 4). After the Transformer layers, we pad special mask tokens \(\mathbf{m}_{1}\in\mathbb{R}^{d}\) to make up the previously dropped nodes' hidden representations. Compared to the original remask, remask-v2 additionally prevents the Transformer layers from processing the masked nodes. It thereby avoids the gap of processing masked nodes in pretraining but not in fine-tuning [15].

Figure 4: Overview of the SimSGT’s framework.

## 4 Rethinking Masked Graph Modeling for Molecules

**Experimental setting.** In this section, we perform experiments to assess the roles of tokenizer and decoder in MGM for molecules. Our experiments follow the transfer learning setting in [12; 9]. We pretrain MGM models on 2 million molecules from ZINC15 [42], and evalute the pretrained models on eight classification datasets in MoleculeNet [28]: BBBP, Tox21, ToxCast, Sider, ClinTox, MUV, HIV, and Bace. These downstream datasets are divided into train/valid/test sets by scaffold split to provide an out-of-distribution evaluation setting. We report the mean performances and standard deviations on the downstream datasets across ten random seeds. Throughout the experiments, we use random node masking of ratio \(0.35\). More detailed experimental settings can be found in Appendix D.

### Rethinking Decoder

We investigate the impact of an expressive decoder on MGM's performance. A single-layer GIN-based SGT tokenizer is utilized in these experiments. Table 2(a) summarizes the results.

**Finding 1. A sufficiently expressive decoder with remask decoding is crucial for MGM.** Table 2(a) shows that using a sufficiently expressive decoder with remask decoding can significantly improve downstream performance. This can be attributed to the disparity between molecule reconstruction and MRL tasks: the last several layers in the autoencoder will be specialized on molecule reconstruction, while losing some representation ability [15; 38]. When using a linear decoder, the last several layers in the encoder will be reconstruction-specialized, which can yield suboptimal results in fine-tuning.

A sufficiently expressive decoder is essential to account for the reconstruction specialization. As shown by Figure 4(a), employing an expressive decoder results in significantly lower reconstruction loss than a linear decoder. However, increasing the decoder's expressiveness without remask decoding cannot improve the downstream performance (Table 2(a)). This leads to our exploration on remask.

**Finding 2. Remask constrains the encoder's efforts on graph reconstruction for effective MRL.** Figure 4(b) shows that the linear probing accuracy of the masked atom types on an encoder pretrained with remask is significantly lower than the accuracy on an encoder pretrained without remask. This shows that remask makes the encoder spend less effort on predicting the corrupted information. Moreover, remask only slightly sacrifices the autoencoder's reconstruction ability, when paired with the GTS-Small decoder (Figure 4(a)). Combining the

\begin{table}
\begin{tabular}{l c c} \hline \hline Encoder & Decoder & Avg. \\ \hline GTS & GTS-Tiny & 74.7 \\ GTS & GTS-Small & **75.8** \\ GTS & GTS & 74.9 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Testing decoder’s size. Remask-v2 is used.

observation that remask improves downstream performances (Table 2(a)), it indicates that remask constrains the encoder's efforts on graph reconstruction, allowing it to focus on MRL.

In addition, remask-v2 outperforms remask-v1 with GTS architecture. This improvement can be attributed to remask-v2's ability to prevent the Transformer layers from processing the masked nodes, avoiding the gap of using masked nodes in pretraining but not in fine-tuning. Finally, we test decoder's sizes in Table 4. When the encoder is GTS, GTS-Small decoder provides the best performance.

### Rethinking Tokenizer

We investigate the impact of tokenizers on MGM's performance. In the following experiments, the graph autoencoder employs the GTS encoder and the GTS-Small decoder with remask-v2, given their superior performances in the previous section. The results are summarized in Table 2(b).

**Compared tokenizers.** We use the node tokenizer as the baseline. For motif-based tokenizers, we employ the leading fragmentation methods: MGSSL [1] and RelMole [18]. For GNN-based tokenizers, we compare the prevalent pretraining strategies - GraphCL [4], GraphMAE [9], and VQ-VAE [10; 43] - to pretrain tokenizers on the 2 million molecules from ZINC15.

**Finding 3. Reconstructing subgraph-level tokens can give rise to MRL.** Table 2(b) shows that, given the appropriate setup, incorporating motif-based tokenizers or GNN-based tokenizers in MGM can provide better downstream performances than the node tokenizer. This observation underscores the importance of applying a subgraph-level tokenizer in MGM.

**Finding 4. Single-layer SGT outperforms or matches other tokenizers.** Table 2(b) shows that a single-layer SGT, applied to GIN, delivers comparable performance to a four-layer GIN pretrained by VQ-VAE, and surpasses other tokenizers. Further, Figure 6 shows that SGT can transform GCN and GraphSAGE into competitive tokenizers. We attribute SGTs' good performances to the effectiveness of their graph operators in extracting structural information. It has been shown that linear graph operators can effectively summarize structural patterns for node classification [39; 40].

**Finding 5. GNN-based tokenizers have achieved higher performances than motif-based tokenizers.** We hypothesize that this is due to GNN's ability of summarizing structural patterns. When using GNN's representations as reconstruction targets, the distance between targets reflects the similarity between their underlying subgraphs - a nuanced relationship that the one-hot encoding of motifs fails to capture. We leave the potential incorporation of GNNs into motif-based tokenizers for future works.

Finally, we show that although GNN-based tokenizers are agnostic to chemistry knowledge, incorporating them in MGM can improve the recognition of FGs. In Figure 7, we use a linear classifier to probe the encoder's mean pooling output to predict the FGs within the molecule. We use RDkit [30] to extract 85 types of FG. Details are in Appendix D. It can be seen that, incorporating a single-layer SGT in MGM improves the encoder's ability to identify FGs in comparison to the node tokenizer.

## 5 Comparison to the State-of-the-Art Methods

In this section, we compare SimSGT to the leading molecule SSL methods for molecular property prediction and a broader range of downstream tasks. For fair comparison, we report the performances of SimSGT and its variant that uses the GINE architecture. This variant employs a GINE as encoder and a GINE-Small as decoder (Table 2), and implements remask-v1 decoding.

**Molecular property prediction.** The molecular property prediction experiment follows the same transfer learning setting as Section 3. Table 5 presents the results. It can be observed that SimSGT outperforms all the baselines in average performances with both the GTS and GINE architectures. Notably, SimSGT with GTS establishes a new state-of-the-art of \(75.8\%\) ROC-AUC. It exceeds the second method by an absolute \(1.8\%\) points in average performance and achieves the best performance in five out of the eight molecular property prediction datasets. SimSGT with GINE outperforms the baselines by \(0.4\%\) points in average performance. These improvements demonstrate the effectiveness of our proposed tokenizer and decoder to improve MGM performance for molecules.

Figure 7: Linear probing encoder’s output for FGs.

[MISSING_PAGE_FAIL:9]

and standard deviations across 10 random seeds. The performances are reported in Table 7. We can observe that SimSGT consistently outperforms representative baselines of GraphCL, GraphMAE, and Mole-BERT.

**Computational cost.** We compare the wall-clock pretraining time for SimSGT and key baselines in Table 8. We can observe that: 1) SimSGT's pretraining time is on par with GraphMAE [9]. This efficiency is largely attributed to the minimal computational overhead of our SGT tokenizer; 2) In comparison to Mole-BERT [10], the prior benchmark in molecule SSL, SimSGT is approximately three times faster. The computational demands of Mole-BERT can be attributed to its combined approach of MGM training and contrastive learning.

## 6 Conclusion and Future Works

In this work, we extensively investigate the roles of tokenizer and decoder in MGM for molecules. We compile and evaluate a comprehensive range of molecule fragmentation functions as molecule tokenizers. The results reveal that a subgraph-level tokenizer gives rise to MRL performance. Further, we show by empirical analysis that a sufficiently expressive decoder with remask decoding improves the molecule encoder's representation quality. In light of these findings, we introduce SimSGT, a novel MGM approach with subgraph-level tokens. SimSGT features a Simple GNN-based Tokenizer capable of transforming GNNs into effective graph tokenizers. It further adopts the GTS architecture for its encoder and decoder, and incorporates a new remask strategy. SimSGT exhibits substantial improvements over existing molecule SSL methods. For future works, the potential application of molecule tokenizers to joint molecule-text modeling [3], remains an interesting direction.

## Acknowledgement

This research is supported by the National Natural Science Foundation of China (9227010114) and the University Synergy Innovation Program of Anhui Province (GXXT-2022-040). This material is based upon work supported by the Google Cloud Research Credit program with the award (6NW8-CF7K-3AG4-1WH1). This research is supported by NExT Research Center.

## References

* [1] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. In _NeurIPS_, pages 15870-15882, 2021.
* [2] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In _NeurIPS_, 2020.
* [3] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In _EMNLP_, 2023.
* [4] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In _NeurIPS_, 2020.
* [5] Laurianne David, Amol Thakkar, Rocio Mercado, and Ola Engkvist. Molecular representations in ai-driven drug discovery: a review and practical guide. _Journal of Cheminformatics_, 12(1):1-22, 2020.
* [6] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _ICML_, pages 1263-1272. PMLR, 2017.
* [7] Chaochao Yan, Qianggang Ding, Peilin Zhao, Shuangjia Zheng, Jinyu Yang, Yang Yu, and Junzhou Huang. Retroxpert: Decompose retrosynthesis prediction like A chemist. In _NeurIPS_, 2020.
* [8] Umit V Ucak, Islambek Ashymamatov, Junsu Ko, and Juyong Lee. Retrosynthetic reaction pathway prediction through neural machine translation of atomic environments. _Nature communications_, 13(1):1186, 2022.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Model & GraphMAE & Mole-BERT & S2GAE & GraphMAE2 & SimSGT \\ \hline Pretrain Time & 527 min & 2199 min & 1763 min & 1195 min & 645 min \\ \hline \hline \end{tabular}
\end{table}
Table 8: Time spent for pretraining 100 epochs on ZINC15 when using the GTS encoder.

* [9] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In _KDD_, pages 594-604. ACM, 2022.
* [10] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-BERT: Rethinking pre-training graph neural networks for molecules. In _The Eleventh International Conference on Learning Representations_, 2023.
* [11] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help graph convolutional networks? In _ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 10871-10880. PMLR, 2020.
* [12] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In _ICLR_, 2020.
* [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, pages 4171-4186. Association for Computational Linguistics, 2019.
* [14] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020.
* [15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In _CVPR_, pages 15979-15988. IEEE, 2022.
* [16] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In _ICLR_. OpenReview.net, 2022.
* [17] Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In _KDD_, pages 3585-3594. ACM, 2021.
* [18] Zewei Ji, Runhan Shi, Jiarui Lu, Fang Li, and Yang Yang. Relmole: Molecular representation learning based on two-level graph similarities. _Journal Chemical Information Modeling_, 62(22):5361-5372, 2022.
* [19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, pages 8821-8831, 2021.
* [20] Mukund Deshpande, Michihiro Kuramochi, Nikil Wale, and George Karypis. Frequent substructure-based approaches for classifying chemical compounds. _IEEE Transactions on Knowledge and Data Engineering_, 17(8):1036-1050, 2005.
* [21] Jonathan Clayden, Nick Greeves, and Stuart Warren. _Organic chemistry_. Oxford university press, 2012.
* [22] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, and Weiqiang Wang. Maskgae: Masked graph modeling meets graph autoencoders. _CoRR_, abs/2205.10053, 2022.
* [23] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. MGAE: marginalized graph autoencoder for graph clustering. In _CIKM_, pages 889-898. ACM, 2017.
* [24] Sixiao Zhang, Hongxu Chen, Haoran Yang, Xiangguo Sun, Philip S. Yu, and Guandong Xu. Graph masked autoencoders with transformers. _CoRR_, abs/2202.08391, 2022.
* [25] Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. Adaptive graph encoder for attributed graph embedding. In _KDD_, pages 976-985. ACM, 2020.
* [26] Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and Jie Tang. Mask and reason: Pre-training knowledge graph transformers for complex logical queries. In _KDD_, pages 1120-1130. ACM, 2022.
* [27] Zhanghao Wu, Paras Jain, Matthew A. Wright, Azalia Mirhoseini, Joseph E. Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. In _NeurIPS_, pages 13266-13279, 2021.
* [28] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.

* [29] Tapio Pahikkala, Antti Airola, Sami Pietila, Sushil Shakyawar, Agnieszka Szwajda, Jing Tang, and Tero Aittokallio. Toward more realistic drug-target interaction predictions. _Briefings Bioinform._, 16(2):325-337, 2015.
* [30] Greg Landrum. Rdkit documentation. _Release_, 1(1-79):4, 2013.
* [31] Elena S Salmina, Norbert Haider, and Igor V Tetko. Extended functional groups (efg): an efficient set for chemical characterization and structure-activity relationship studies of chemical compounds. _Molecules_, 21(1):1, 2015.
* [32] Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. In _ICML_, volume 80 of _Proceedings of Machine Learning Research_, pages 2328-2337. PMLR, 2018.
* [33] Daylight Chemical Information Systems, Inc. _Daylight Theory Manual_. http://www.daylight.com/dayhtml/doc/theory/theory.smarts.html.
* [34] Jonathan Clayden, Nick Greeves, and Stuart Warren. _Organic chemistry_. Oxford university press, 2012.
* [35] Jorg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, and Matthias Rarey. On the art of compiling and using'drug-like'chemical fragment spaces. _ChemMedChem: Chemistry Enabling Drug Discovery_, 3(10):1503-1507, 2008.
* [36] Reinhard Diestel. _Graph Theory, 4th Edition_, volume 173 of _Graduate texts in mathematics_. Springer, 2012.
* [37] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [38] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _NeurIPS_, 2022.
* [39] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. Simplifying graph convolutional networks. In _ICML_, volume 97 of _Proceedings of Machine Learning Research_, pages 6861-6871. PMLR, 2019.
* [40] Lei Chen, Zhengdao Chen, and Joan Bruna. On graph neural networks versus graph-augmented mlps. In _ICLR_. OpenReview.net, 2021.
* [41] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _ICML_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 448-456. JMLR.org, 2015.
* ligand discovery for everyone. _J. Chem. Inf. Model._, 55(11):2324-2337, 2015.
* [43] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In _NeurIPS_, pages 14837-14847, 2019.
* [44] Thin Nguyen, Hang Le, Thomas P. Quinn, Tri Nguyen, Thuc Duy Le, and Svetha Venkatesh. Graphtta: predicting drug-target binding affinity with graph neural networks. _Bioinform._, 37(8):1140-1147, 2021.
* [45] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3d geometry. In _ICLR_. OpenReview.net, 2022.
* [46] Simon Axelrod and Rafael Gomez-Bombarelli. GEOM: energy-annotated molecular conformations for property prediction and molecular generation. _CoRR_, abs/2006.05531, 2020.
* [47] Quantum machine. http://quantum-machine.org/datasets/. Accessed 2023-03.
* [48] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. _Nat. Mach. Intell._, 4(2):127-134, 2022.
* [49] Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael M. Bronstein, and Federico Monti. SIGN: scalable inception graph neural networks. _CoRR_, abs/2004.11198, 2020.

* [50] Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In _ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 4839-4848. PMLR, 2020.
* [51] Shichang Zhang, Ziniu Hu, Arjun Subramonian, and Yizhou Sun. Motif-driven contrastive learning of graph representations. _arXiv preprint arXiv:2012.12533_, 2020.
* [52] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.
* [53] Steven Bird. Nltk: the natural language toolkit. In _Proceedings of the COLING/ACL 2006 Interactive Presentation sessions_, pages 69-72, 2006.
* [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NIPS_, pages 5998-6008, 2017.
* [55] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. In _NeurIPS_, 2022.
* [56] Alan Baade, Puyuan Peng, and David Harwath. MAE-AST: masked autoencoding audio spectrogram transformer. In _INTERSPEECH_, pages 2438-2442. ISCA, 2022.
* [57] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_. OpenReview.net, 2021.
* [58] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In _ACL (System Demonstrations)_, pages 55-60, 2014.
* [59] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In _ACL (1)_. The Association for Computer Linguistics, 2016.
* A new approach to self-supervised learning. In _NeurIPS_, 2020.
* [61] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _CVPR_, 2021.
* [62] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. In _ICLR_, 2022.
* [63] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In _ICLR_. OpenReview.net, 2022.
* [64] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In _ICLR_. OpenReview.net, 2022.
* [65] Ziqiao Zhang, Jihong Guan, and Shuigeng Zhou. Fragat: a fragment-oriented multi-scale graph attention model for molecular property prediction. _Bioinform._, 37(18):2981-2987, 2021.
* [66] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. In _NeurIPS_, 2022.
* [67] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. In _NeurIPS_, 2022.
* [68] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. _arXiv preprint arXiv:2302.07090_, 2023.

* [69] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In _ICLR_, 2019.
* [70] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. In _ICLR_, 2020.
* [71] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _ICML_, Proceedings of Machine Learning Research, pages 12121-12132, 2021.
* [72] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. In _NeurIPS_, pages 15920-15933, 2021.
* [73] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation learning with local and global structure. In _ICML_, volume 139, pages 11548-11558, 2021.
* [74] Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In _ICML_, pages 13052-13065, 2022.
* [75] Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. S2GAE: self-supervised graph autoencoders are generalizable learners with graph masking. In _WSDM_, pages 787-795. ACM, 2023.
* [76] Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang. Graphmae2: A decoding-enhanced masked self-supervised graph learner. In _WWW_, volume abs/2304.04779, 2023.
* [77] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _arXiv preprint arXiv:2005.00687_, 2020.
* [78] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _AAAI_, pages 3538-3545. AAAI Press, 2018.
* [79] Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. In _ICLR_. OpenReview.net, 2021.
* [80] Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown, Moshe Tennenholtz, and Yoav Shoham. Pmi-masking: Principled masking of correlated spans. In _ICLR_. OpenReview.net, 2021.

Limitations

Our results and analysis on the graph tokenizer and graph decoder are confined to the task of MGM pretraining. Different tokenizers and decoders might offer advantages for other generative modeling methods, such as autoregressive modeling [32].

SGTs [39, 40] are limited in expressive power for graph structures compared to standard GNNs, like GNNs [37]. Theoretically, the separation of expressiveness power between SGTs and standard GNNs grows exponentially in the GNN's depth [40]. However, SGTs exhibit comparable, if not better, performances to pretrained GNN-based tokenizers, as demonstrated in Table 2(b). We attribute this intriguing observation to two key factors. Firstly, SGTs (_i.e._, simple GNNs) are still powerful and can "distinguish almost all non-isomorphic graphs" [40]. They have shown decent results in practice [39, 49]. Secondly, we conjecture that a better pretraining method for GNN-based tokenizers could exist, but current pretraining techniques do not fully harness the potential of GNNs in their roles as effective tokenizers. Indeed, the significant difference in performance between GraphCL and VQ-VAE (Table 2(b)) emphasizes the impact of pretraining methods on the tokenizer's performance. We leave the investigation of how to effectively pretrain GNN-based tokenizers as future works.

## Appendix B Related Works

We have included the literature review of MGM in the main body of the paper. Here we elaborate on the literature review in the following areas.

**Molecule SSL with motifs.** Motifs are statistically significant subgraph patterns [32, 50], and have been applied in existing molecule SSL methods. Autoregressive pretraining methods [32, 1, 50] generate motifs instead of nodes at each generation step, in order to improve the validity of the generated molecules. Motifs are also used in contrastive learning [17, 51, 18]. Sun _et al._[17] substitute motifs within a molecule with their chemically similar counterparts to create high-quality augmentations. [51, 18] construct molecules' views at the motif-level to supplement the original views at the atom-level. In predictive pretraining, Rong _et al._[2] pretrain a graph encoder to predict the FGs inside the molecule. These previous works have developed extensive molecule fragmentation methods for motif discovery. However, these fragmentation methods have been overlooked as tokenizers in MGM pretraining. Our work addresses this gap by summarizing the common fragmentation rules and examining the performances of the selected fragmentation methods in MGM pretraining.

**Data tokenization.** Tokenization is a data pre-processing technique that divides the original data into smaller elements and converts them to tokens. It is widely used in NLP to split sentences into word-level units [13, 52, 53]. Due to the surging interests in Transformers [54], tokenization is also applied on images [15, 16] and audios [55, 56]. Tokenization fragments these data into sequences of patches to fit the shapes of transformer's input and output. A tokenizer can be designed by heuristics [57], incorporating domain knowledge [58], and pretraining on the target dataset [59, 19, 43]. In this work, we study graph tokenizers, which are less explored in previous works.

**Relations to contrastive learning.** When using a GNN-based tokenizer, MGM involves minimizing the distances between the outputs from two network branches (_i.e.,_ the tokenizer branch and the autoencoder branch). At first glance, this design might seem similar to the contrastive learning methods of BYOL [60], SimSiam [61], and BGRL [62], which also minimize the output differences between two network branches. However, a closer inspection reveals several critical distinctions between MGM and these methods. Firstly, MGM feeds uncorrupted data to the tokenizer branch and feeds corrupted data to the autoencoder branch, encouraging the autoencoder to reconstruct the missing information. In contrast, BYOL, SimSiam, and BGRL use corrupted data in both of their branches, constituting different training objectives. Secondly, while BYOL, SimSiam, and BGRL employ nearly identical architectures for their two branches, MGM can adopt distinctly different architectures for its autoencoder and tokenizer. In our best-performing experiment, the autoencoder has more than ten layers of GNNs and Transformers, while the tokenizer is a shallow single-layer network (Table 3). Finally, MGM employs remask decoding to constrain the encoder's ability on reconstruction, which is not used in contrastive learning methods [60, 61, 62].

**Subgraph-enhanced Graph Neural Network.** Subgraph-enhanced GNN [63, 64, 65] refers to an emerging class of GNNs that fragments a graph into subgraphs before encoding, in order to improve the GNNs' expressiveness [66, 67, 68]. The common graph fragmentation method is node-wise, such that each fragmented subgraph is associated with a unique node in the original graph. For example, ESAN [63] obtains subgraphs by sampling ego-networks or deleting one node from the original graph. Given the subgraphs, subgraph-enchanced GNNs generate node embeddings in every subgraph by applying a series of equivariant message-passing layers [63, 64]. Finally, these embeddings are pooled to output the graph embedding. Our work is related to subgraph-enhanced GNNs that we also study graph fragmentation. The major distinction is that we focus on using the tokens derived from these fragmented graphs as the reconstruction targets in MGM for molecules.

## Appendix C Pseudo Code

We present the pseudocode of SimSGT. This code uses a single-layer SGT of GIN as an example.

```
##phi:graphencoder
##rho:graphdecoder defSOT(g,embed):
##SOT:single-layerGIN tokenizer x,edge_index=g
##messagepassing x=propagate(embed(x),edge_index)+(1+eps)+embed(x)
##batchnormalizationlayer x=batchnorm(x) returnx

forgimloader:
#randommasking g_hat_m_pos=random_masking(g)#m_pos:maskpositions
##tokenization.embedisalinearlayer y=SOT(g,phi.embed).detach()#detach:stop-gradient
##autoencoderforward y_hat=rho(remask(phi(g_hat),m_pos))
##minimizes loss loss=distance_loss(y_hat[m_pos],y[m_pos]) loss,backward() ```

**Algorithm 1** Pytorch style pseudocode of SimSGT

## Appendix D Experimental Setup

**Computational resource.** We perform experiments on an NVIDIA DGX A100 server. Each individual experiment can be run on a single GPU without exceeding 30 GBs of GPU memory.

### Compared Methods

**Motif-based tokenizers.** We now elaborate on the details of the two compared motif-based tokenizers:

* **MGSSL**[1] employs the BRICS [35] method for molecule fragmentation (Section 2.2). To obtain more fine-grained fragments, MGSSL employs two additional rules to break the BRICS's output fragments: 1) separate the single atoms attached to cycles; 2) if a connected subgraph comprising three or more atoms is not part of a cycle, break it down as a new fragment.
* **RelMole**[18] combines the fragmentation functions of Cycles and FGs for molecule fragmentation (Section 2.2). Further, it extracts the carbon-carbon single bonds that are not covered in the previous step as new fragments.

We use the motif vocabulary provided by their paper for molecule fragmentation. Given a molecule, we convert its fragmented motifs into one-hot encodings, which serve as the reconstruction targets.

**Pretrained GNN-based tokenizers.** We use the atomic numbers as the node features and exclude edge features in pretrained GNN-based tokenizers. We show in Appendix E that incorporating edge features in GNN tokenizers can decrease performance. Due to the removal of edge features, the tokenizer uses the architecture of GIN [37] instead of GINE [12]. We have reported performances of GNN-based tokenizers that are pretrained by GraphCL [4], GraphMAE [9], and VQ-VAE [10; 43]. The implementation of VQ-VAE follows [10] and groups the latent codes by atomic numbers. We strictly follow the procedure in the mentioned papers to pretrain GNNs, which are later used as tokenizers.

**Simple GNN-based tokenizers (SGTs).** An SGT uses the node feature of atomic number. It uses the graph encoder's linear embedding function of atomic numbers. We present the graph operators for our tested SGTs below:

\[\text{GIN:} \omega(\mathbf{A}) =\mathbf{A}+(1+\epsilon)\mathbf{I},\] (17) \[\text{GCN:} \omega(\mathbf{A}) =\tilde{\mathbf{D}}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{ -1/2},\] (18) \[\text{GraphSAGE:} \omega(\mathbf{A}) =\tilde{\mathbf{D}}^{-1}\tilde{\mathbf{A}},\] (19)

where \(\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) and \(\tilde{\mathbf{D}}\) is the degree matrix of \(\tilde{\mathbf{A}}\); \(\epsilon\) is set to \(0.5\) empirically.

**Baselines.** We now describe the details of our reported baseline methods:

* **Infomax**[69] learns node representations by maximizing the mutual information between the local summaries of node patches and the patches' graph-level global summaries.
* **ContextPred**[12] uses the embeddings of subgraphs to predict their context graph structures.
* **InfoGraph**[70] conducts graph representation learning by maximizing the mutual information between graph-level representations and local substructures of various scales.
* **GraphCL**[4] performs graph-level contrastive learning with combinations of four graph augmentations, namely node dropping, edge perturbation, subgraph cropping, and feature masking.
* **JOAO**[71] proposes a framework to automatically search proper data augmentations for GCL.
* **AD-GCL**[72] applies adversarial learning for adaptive graph augmentation to remove the redundant information in graph samples.
* **GraphLOG**[73] leverages clustering to construct hierarchical prototypes of graph samples. They further contrast each local instance with its corresponding higher prototype for contrastive learning.
* **RGCL**[74] trains a rationale generator to identify the causal subgraph in graph augmentation. Each graph's causal subgraph and its complement are leveraged in contrastive learning.
* **BGRL**[62] trains an online encoder by learning to predict the output of a target encoder. The target encoder shares the same architecture as the online encoder and is updated through exponentially moving average. The inputs of the online encoder and the target encoder are two different graph augmentations.

\begin{table}

\end{table}
Table 9: Experimental settings for pretraining on 2 million molecules from ZINC15 and fine-tuning on eight datasets in MoleculeNet: BBBP, Tox21, ToxCast, Sider, ClinTox, MUV, HIV, and Bace.

* **GraphMAE**[9] shows that a linear classifier is insufficient for decoding node types. It applies a GNN for decoding and proposes remask to decouple the functions of the encoder and decoder in the autoencoder.
* **GraphMVP**[45] uses a contrastive loss and a generative loss to connect the 2-dimensional view and 3-dimensional view of the same molecule, in order to inject the 3-dimensional knowledge into the 2-dimensional graph encoder.
* **S2GAE**[75] randomly masks a portion of edges of graphs and pretrain the graph encoder to predict the missing edges.
* **GraphMAE2**[76] applies multi-view random re-mask decoding as a regularization for MGM pretraining.
* **Mole-BERT**[10] combines a contrastive learning objective and a masked atom modeling objective for MRL. Specifically, they observe that mask atom prediction is an overly easy pretraining task. Therefore, they employ a GNN tokenizer pretrained by VQ-VAE [43] to generate more complex reconstruction targets for masked atom modeling.

### Experimentes in Section 4 and Table 5

Here we elaborate the experimental setting for pretraining on 2 million molecules from ZINC15 [42] and fine-tuning on the eight classification datasets in MoleculeNet [28]: BBBP, Tox21, ToxCast, Sider, ClinTox, MUV, HIV, and Bace. This setting covers the experiments in Section 4 and Table 5.

**Molecule representations.** For SimSGT and other compared methods, we follow previous works [12; 4] and use a minimal set of molecule features as the graph representations (Table 8(a)). These features unambiguously describe the two-dimensional structure of molecules.

**Hyper-parameters.** Table 8(b) summarizes the hyper-parameters. We use different hyper-parameters given different graph encoders. The architectures of the two graph encoders are borrowed from previous works: GINE [12] and GTS [27]. We use large batch sizes of 1024 and 2048 to speed up pretraining. We do not use dropout during pretraining. During fine-tuning, we 50% dropout in GINE layers and 30% dropout in transformer layers. The learning rate for the MUV dataset is 10 times smaller than other datasets. Following [4; 73], we report the last epoch's test performance. We

\begin{table}

\end{table}
Table 10: Experimental setting for pretraining on the 50 thousand molecules from the GEOM dataset and fine-tuning on the four molecular property prediction (regression) datasets and DTA datasets.

report the mean performances and the standard deviations across 10 random seeds. Baselines are reproduced using the same setting.

**Linear probing experiments.** Here we elaborate on the settings of our linear probing experiments (Figure 4(b) and Figure 7). Specifically, we randomly split the 2 million molecules from ZINC15 into train set (90%) and test set (10%). We train the MGM models on the train set and save the encoder's checkpoint every epoch. The linear classifiers are trained for 1000 epochs on the encoder's frozen hidden representations. We train linear classifiers using 25600 molecule samples from the training set and evaluate them on the whole test set.

* **Probing masked atom types (Figure 4(b)).** We let linear classifiers predict the masked atom types using the masked atoms' hidden representations. During linear probing, we disable remask-v2 to obtain the masked atoms' hidden representations. Molecules are randomly masked by 0.35 during probing. We use accuracy (%) as the evaluation metric.
* **Probing FGs (Figure 7).** Following [2], we extract 85 types of FGs for each molecule using RDkit [30]. FGs are represented by 85-dimensional binary vectors, whose each dimension indicates the presence of a certain FG. Afterward, we train multi-label linear classifiers on the frozen encoder's mean pooling outputs for FG prediction. Molecules are not masked during probing. We use ROC-AUC (%) as the evaluation metric.

### Experiments in Table 6

We present the experimental setting for pretraining on the 50 thousand molecules from GEOM [46] and fine-tuning on the four molecule property prediction (regression) datasets and two DTA datasets. Our experimental setting follows that in [45]. This setting covers the experiments in Table 6.

**Molecule representations.** In the graph autoencoder, we use 9-dimensional node features and 3-dimensional edge features of molecules provided by the OGB [77] package, following GraphMVP [45]. The features are summarized in Table 9(a). Note that, our tokenizer uses only the atomic numbers as node features and does not use edge features.

**Hyper-parameters.** The hyperparameters are summarized in Table 9(b). We tune the hyperparameters in the fine-tuning stage using the validation performance. Following [45], we report the test performance at the epoch selected by the validation performance. We do not use dropout during pretraining. During fine-tuning, we 50% dropout in GINE layers and 30% dropout in transformer layers.

For a fair comparison, we reproduce Mole-BERT [10]'s performance by pretraining on the 50 thousand molecule samples from the GEOM dataset [46]. The original Mole-BERT is trained on a larger dataset of 2 million molecules from ZINC15 [42].

### Experiments in Table 7

The hyperparameters for fine-tuning on the QM datasets are reported in Table 11.

## Appendix E More Experimental Results

In this section, we provide more experimental results. If not especially noted, these experiments employ an autoencoder of GTS encoder and GTS-Small decoder with remask-v2, and a tokenizer of a single-layer SGT of GIN. Other settings follow that in Appendix D.2.

**Influence of edge features for pretrained GNN-based tokenizer.** We ablate the impact of the "bond type" and "bond direction" edge features in pretrained GNN-based tokenizers. We use GINE and

\begin{table}
\begin{tabular}{l c c} \hline \hline QM Dataset & batch size & lr \\ \hline QM7 & 32 & 4e-4 \\ QM8 & 256 & 1e-3 \\ QM9 & 256 & 1e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Hyperparameters for fine-tuning on the QM datasets.

GIN for the tokenizer with and without edge features. Table 12 shows that including edge features in GNN-based tokenizers negatively influences the transfer learning performance. Therefore, we exclude edge features from pretrained GNN-based tokenizers in our experiments.

**Influence of Batch Normalization layers in SGTs.** The Batch Normalization [41] (BN) layers in SGTs are crucial to avoid loss vanishment. Figure 7(a) presents a comparison between SGT "with _vs_ without BN". Without the BN layer, the MSE loss drops to lower than \(0.01\) within a few steps of pretraining. Such small loss values lead to significant model underfitting.

As shown by Figure 7(b), the token values of SGT without BN follow a sharp distribution: the values are primarily distributed around zero, and their standard deviation (std) is smaller than \(0.35\). This minor std issue might be caused by the smoothing effect of GNNs [78]. An expressive neural network (_i.e.,_ a graph autoencoder) can quickly fit this sharp target distribution and minimize the loss to a negligible value, causing the problem of loss vanishment. However, if a BN layer is used, it forces each dimension of the tokenizer output to have an std of \(1.00\), so as to "spread out" the distribution of the SGT tokens. These new SGT tokens of a larger std are harder to fit. They keep the MSE loss at a reasonable range of \(0.10\sim 0.15\) (Figure 7(a)).

**Mask ratio.** We apply random node masking throughout the experiments [12]. Figure 9 presents SimSGT's sensitivity with respect to the mask ratios. SimSGT is not sensitive to mask ratios such that a wide range of ratios (0.25 \(\sim\) 0.45) can generate competitive performances. The ratio of 0.35 achieves the best performance. This ratio is much lower than that for images, where a ratio of 0.75 can generate promising performances [15].

**Balancing the distribution of reconstruction targets.** As shown in Figure 11, the popularly used ZINC15 dataset includes 12 types of atoms, and 95% of the atoms are distributed on the top three atom types. This skewed distribution renders the node-level token reconstruction an easy pretraining task [10]. Figure 10 shows that the accuracy of predicting node-level tokens converges quickly. Such an easy pretraining task can lead to suboptimal performance as suggested by existing SSL literature [79, 80]. In Figure 11, we show that the induced subgraphs of a single-layer SGT (_i.e.,_ one-hop rooted subtrees) follow a more balanced distribution than the distribution of nodes. SGT tokens also have a larger vocabulary size: ZINC15 includes \(555\) types of one-hop rooted subtrees. Consequently, the accuracy of predicting tokens of a single-layer SGT takes more epochs to converge (Figure 10).

**Pooling Method for Subgraph Representations.** In previous experiments, we use mean pooling to obtain the subgraph representations for motif-based tokenizers, following the method of obtaining graph representations in [4, 12]. Here we add results for MGSSL tokenizer using sum and max

\begin{table}
\begin{tabular}{l c|c c c c c} \hline \hline \multirow{2}{*}{Tokenizer} & \multirow{2}{*}{Edge feature} & \multicolumn{4}{c}{Tokenizer GNN’s depth} \\  & & 1 & 2 & 3 & 4 & 5 \\ \hline \multirow{2}{*}{Pretrain, GraphCL} & ✗ & **75.1** & 74.5 & **74.2** & **74.0** & **74.6** \\  & ✓ & 74.2 & **74.7** & 73.7 & 73.8 & 73.2 \\ \hline \multirow{2}{*}{Pretrain, GraphMAE} & ✗ & **75.1** & **74.9** & **74.9** & **75.4** & **75.2** \\  & ✓ & 74.6 & 74.6 & 74.3 & 74.6 & 75.0 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Average transfer learning ROC-AUC (%) scores on the eight classification datasets in MoleculeNet. Including edge features in tokenizers decreases the performance.

Figure 8: SimSGT pretraining on the 2 millions molecules from ZINC15.

[MISSING_PAGE_EMPTY:21]

\begin{table}
\begin{tabular}{l c c c c c c c c c|c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Datasets}} & \multicolumn{1}{c}{BBBP} & \multicolumn{1}{c}{Tox21} & \multicolumn{1}{c}{ToxCast} & SIDER & \multicolumn{1}{c}{ClinTox} & \multicolumn{1}{c}{MUV} & \multicolumn{1}{c}{HIV} & \multicolumn{1}{c|}{ BACE} & \multicolumn{1}{c}{Avg.} \\ \hline Motif, MGSSL, Mean & 72.54\({}_{0.9}\) & 77.54\({}_{0.4}\) & 65.24\({}_{0.6}\) & 60.74\({}_{0.9}\) & 85.03\({}_{0.3}\) & 79.94\({}_{1.5}\) & 78.04\({}_{1.5}\) & 83.04\({}_{0.1}\) & 75.2 \\ Motif, MGSSL, Max & 71.54\({}_{0.9}\) & 75.82\({}_{1.2}\) & 66.24\({}_{0.7}\) & 60.74\({}_{1.3}\) & 82.62\({}_{0.2}\) & 78.94\({}_{1.8}\) & 76.54\({}_{1.4}\) & 83.84\({}_{1.6}\) & 74.5 \\ Motif, MGSSL, Sum & 71.74\({}_{1.4}\) & 75.94\({}_{0.6}\) & 66.14\({}_{0.7}\) & 60.41\({}_{1.4}\) & 83.44\({}_{1.5}\) & 79.54\({}_{1.0}\) & 76.84\({}_{1.2}\) & 84.21\({}_{1.1}\) & 74.8 \\ \hline \hline \end{tabular}
\end{table}
Table 13: ROC-AUC (%) scores on eight MoleculeNet datasets. Compared methods use GTS encoder and GTS-Small decoder with remask-v2 decoding.

\begin{table}
\begin{tabular}{l c c c c c c c c c|c} \hline \hline Encoder & Decoder & Remask & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & Avg. \\ \hline GINE & Linear & - & 72.54\({}_{0.8}\) & 76.04\({}_{0.4}\) & 63.74\({}_{0.5}\) & 60.14\({}_{0.7}\) & 81.22\({}_{2.6}\) & 74.21\({}_{1.6}\) & 78.04\({}_{0.3}\) & 79.61\({}_{1.4}\) & 73.2 \\ GINE & GINE-Small & - & 70.94\({}_{0.6}\) & 75.14\({}_{0.5}\) & 63.54\({}_{0.4}\) & 61.04\({}_{0.4}\) & 79.14\({}_{2.6}\) & 76.04\({}_{0.5}\) & 76.34\({}_{0.5}\) & 82.54\({}_{0.9}\) & 73.0 \\ GINE & GINE-Small & v1 & 70.24\({}_{1.0}\) & 76.44\({}_{0.6}\) & 64.24\({}_{0.4}\) & 61.94\({}_{0.8}\) & 80.94\({}_{1.8}\) & 77.84\({}_{1.1}\) & 78.14\({}_{1.1}\) & 83.61\({}_{1.1}\) & **74.1** \\ \hline GTS & Linear & - & 72.94\({}_{0.7}\) & 76.64\({}_{0.8}\) & 63.84\({}_{0.8}\) & 58.34\({}_{1.3}\) & 81.94\({}_{0.5}\) & 78.54\({}_{1.5}\) & 78.04\({}_{0.2}\) & 83.14\({}_{0.9}\) & 74.1 \\ GTS & GTS-Small & – & 72.04\({}_{0.6}\) & 74.74\({}_{0.4}\) & 63.74\({}_{0.4}\) & 58.94\({}_{0.6}\) & 86.04\({}_{0.2}\) & 78.94\({}_{1.7}\) & 77.34\({}_{0.7}\) & 81.04\({}_{0.7}\) & 74.1 \\ GTS & GTS-Small & v1 & 71.34\({}_{0.7}\) & 77.01\({}_{0.6}\) & 66.24\({}_{0.6}\) & 60.61\({}_{1.4}\) & 84.53\({}_{3.4}\) & 81.51\({}_{1.5}\) & 77.04\({}_{0.6}\) & 83.54\({}_{1.2}\) & 75.2 \\ GTS & GTS-Small & v2 & 72.24\({}_{0.9}\) & 76.88\({}_{0.9}\) & 65.92\({}_{0.8}\) & 61.74\({}_{0.8}\) & 85.74\({}_{1.8}\) & 81.44\({}_{1.4}\) & 78.04\({}_{1.9}\) & 84.34\({}_{0.6}\) & **75.8** \\ GTS & GTS-Tiny & v2 & 71.94\({}_{1.2}\) & 77.24\({}_{1.1}\) & 65.64\({}_{0.5}\) & 61.74\({}_{1.4}\) & 82.94\({}_{2.4}\) & 79.64\({}_{1.4}\) & 76.84\({}_{1.3}\) & 82.14\({}_{1.5}\) & 74.7 \\ GTS & GTS & v2 & 70.74\({}_{1.2}\) & 76.44\({}_{0.9}\) & 66.14\({}_{0.4}\) & 60.34\({}_{0.9}\) & 84.74\({}_{1.6}\) & 79.64\({}_{0.7}\) & 76.84\({}_{1.9}\) & 84.54\({}_{0.8}\) & 74.9 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Transfer learning ROC-AUC (%) scores on the eight classification datasets in MoleculeNet.

\begin{table}
\begin{tabular}{l c c c c c c c c|c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Tokenizer}} & \multicolumn{1}{c}{BBBP} & \multicolumn{1}{c}{Tox21} & \multicolumn{1}{c}{ToxCast} & SIDER & \multicolumn{1}{c}{ClinTox} & \multicolumn{1}{c}{MUV} & \multicolumn{1}{c|}{HIV} & \multicolumn{1}{c|}{ BACE} & \multicolumn{1}{c}{Avg.} \\ \hline Node & 70.34\({}_{0.9}\) & 76.44\({}_{1.0}\) & 65.74\({}_{0.7}\) & 61.74\({}_{0.9}\) & 81.94\({}_{0.5}\) & 79.84\({}_{0.7}\) & 77.41\({}_{1.8}\) & 84.64\({}_{1.1}\) & 74.7 \\ Motif, MGSSL & 72.54\({}_{0.9}\) & 77.54\({}_{0.4}\) & 65.24\({}_{0.6}\) & 60.74\({}_{0.9}\) & 85.04\({}_{0.3}\) & 79.94\({}_{1.5}\) & 78.04\({}_{1.5}\) & 83.04\({}_{1.0}\) & 75.2 \\ Motif, RefNlOle & 71.44\({}_{1.3}\) & 77.14\({}_{0.4}\