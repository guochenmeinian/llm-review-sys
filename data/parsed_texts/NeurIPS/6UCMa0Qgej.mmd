# Adversarial Model for Offline Reinforcement Learning

 Mohak Bhardwaj

University of Washington

mohakb@cs.washington.edu

&Tengyang Xie

Microsoft Research & UW-Madison

tx@cs.wisc.edu

&Byron Boots

University of Washington

bboots@cs.washington.edu

&Nan Jiang

UIUC

nanjiang@illinois.edu

&Ching-An Cheng

Microsoft Research, Redmond

chinganc@microsoft.com

Equal contributionOpen source code is available at: [https://sites.google.com/view/armorofflinerl/](https://sites.google.com/view/armorofflinerl/).

###### Abstract

We propose a novel model-based offline Reinforcement Learning (RL) framework, called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can robustly learn policies to improve upon an arbitrary reference policy regardless of data coverage. ARMOR is designed to optimize policies for the worst-case performance relative to the reference policy through adversarially training a Markov decision process model. In theory, we prove that ARMOR, with a well-tuned hyperparameter, can compete with the best policy within data coverage when the reference policy is supported by the data. At the same time, ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with _any_ admissible hyperparameter, would never degrade the performance of the reference policy, even when the reference policy is not covered by the dataset. To validate these properties in practice, we design a scalable implementation of ARMOR, which by adversarial training, can optimize policies without using model ensembles in contrast to typical model-based methods. We show that ARMOR achieves competent performance with both state-of-the-art offline model-free and model-based RL algorithms and can robustly improve the reference policy over various hyperparameter choices.2

Footnote 2: [https://github.com/shengxie/ARMOR](https://github.com/shengxie/ARMOR)

## 1 Introduction

Offline reinforcement learning (RL) is a technique for learning decision-making policies from logged data (Lange et al., 2012; Levine et al., 2020; Jin et al., 2021; Xie et al., 2021). In comparison with alternate learning techniques, such as off-policy RL and imitation learning (IL), offline RL reduces the data assumption needed to learn good policies and does not require collecting new data. Theoretically, offline RL can learn the best policy that the given data can explain: as long as the offline data includes the scenarios encountered by a near-optimal policy, an offline RL algorithm can learn such a near-optimal policy, even when the data is collected by highly sub-optimal policies and/or is not diverse. Such robustness to data coverage makes offline RL a promising technique for solving real-world problems, as collecting diverse or expert-quality data in practice is often expensive or simply infeasible.

The fundamental principle behind offline RL is the concept of pessimism, which considers worst-case outcomes for scenarios without data. In algorithms, this is realized by (explicitly or implicitly) constructing performance lower bounds in policy learning which penalizes uncertain actions.

Various designs have been proposed to construct such lower bounds, including behavior regularization (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021), point-wise pessimism based on negative bonuses or truncation (Kidambi et al., 2020; Jin et al., 2021), value penalty (Kumar et al., 2020; Yu et al., 2020), or two-player games (Xie et al., 2021; Uehara and Sun, 2021; Cheng et al., 2022). Conceptually, the tighter the lower bound is, the better the learned policy would perform; see a detailed discussion of related work in Appendix C.

Despite these advances, offline RL still has not been widely adopted to build learning-based decision systems beyond academic research. One important factor we posit is the issue of performance degradation: Usually, the systems we apply RL to have currently running policies, such as an engineered autonomous driving rule or a heuristic-based system for diagnosis, and the goal of applying a learning algorithm is often to further improve upon these baseline _reference policies_. As a result, it is imperative that the policy learned by the algorithm does not degrade the base performance. This criterion is especially critical for applications where poor decision outcomes cannot be tolerated.

However, running an offline RL algorithm based on pessimism, in general, is not free from performance degradation. While there have been algorithms with policy improvement guarantees (Laroche et al., 2019; Fujimoto et al., 2019; Kumar et al., 2020; Fujimoto and Gu, 2021; Cheng et al., 2022), such guarantees apply only to the behavior policy that collects the data, which might not necessarily be the reference policy. In fact, quite often these two policies are different. For example, in robotic manipulation, it is common to have a dataset of activities different from the target task. In such a scenario, comparing against the behavior policy is meaningless, as these policies do not have meaningful performance in the target task.

In this work, we propose a novel model-based offline RL framework, called \(\underline{\text{Ad}}\)dvestigal \(\underline{\text{Model}}\) for \(\underline{\text{Offline}}\)Rinforcement Learning (ARMOR), which can robustly learn policies that improve upon an arbitrary reference policy by adversarially training a Markov decision process (MDP) model, regardless of the data quality. ARMOR is designed based on the concept of relative pessimism (Cheng et al., 2022), which aims to optimize for the worst-case relative performance over uncertainty. In theory, we prove that, owing to relative pessimism, the ARMOR policy never degrades the performance of the reference policy for a range of hyperparameters which is given beforehand, a property known as Robust Policy Improvement (RPI) (Cheng et al., 2022). In addition, when the right hyperparameter is chosen, and the reference policy is covered by the data, we prove that the ARMOR policy can also compete with any policy covered by the data in an absolute sense. To our knowledge, RPI property of offline RL has so far been limited to comparing against the data collection policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021; Cheng et al., 2022). In ARMOR, by adversarially training an MDP model, we extend the technique of relative pessimism to achieve RPI with _arbitrary_ reference policies, regardless of whether they collected the data or not (Fig. 1).

In addition to theory, we design a scalable deep-learning implementation of ARMOR to validate these claims that jointly trains an MDP model and the state-action value function to minimize the estimated performance difference between the policy and the reference using model-based rollouts. Our implementation achieves state-of-the-art (SoTA) performance on D4RL benchmarks (Fu et al., 2020), while using only a _single_ model (in contrast to ensembles used in existing model-based offline RL works). This makes ARMOR a better framework for using high-capacity world models (e.g.(Hafner et al., 2023)) for which building an ensemble is too expensive. We also empirically validate the RPI property of our implementation.

Figure 1: Robust Policy Improvement: ARMOR can improve performance over the reference policy (REF) over a broad range of pessimism hyperparameter (purple) regardless of data coverage. ORL denotes best offline RL policy without using the reference policy, and reference is obtained by behavior cloning on expert dataset.

## 2 Preliminaries

Markov Decision ProcessWe consider learning in the setup of an infinite-horizon discounted Markov Decision Process (MDP). An MDP \(M\) is defined by the tuple \(\langle\mathcal{S},\mathcal{A},P_{M},R_{M},\gamma\rangle\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(P_{M}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta\left(\mathcal{S}\right)\) is the transition dynamics, \(R_{M}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is a scalar reward function and \(\gamma\in[0,1]\) is the discount factor. A policy \(\pi\) is a mapping from \(\mathcal{S}\) to a distribution on \(\mathcal{A}\). For \(\pi\), we let \(d_{M}^{\pi}(s,a)\) denote the discounted state-action distribution obtained by running \(\pi\) on \(M\) from an initial state distribution \(d_{0}\), i.e \(d_{M}^{\pi}(s,a)=(1-\gamma)\operatorname{\mathbb{E}}_{\pi,M}\left[\sum_{t=0}^{ \infty}\gamma^{t}1\left(s_{t}=s,a_{t}=a\right)\right]\). Let \(J_{M}(\pi)=\operatorname{\mathbb{E}}_{\pi,M}\left[\sum_{t=m}^{\infty}\gamma^{t }r_{t}\right]\) be the expected discounted return of policy \(\pi\) on \(M\) starting from \(d_{0}\), where \(r_{t}=R_{M}(s_{t},a_{t})\). We define the value function as \(V_{M}^{\pi}(s)=\operatorname{\mathbb{E}}_{\pi,M}\left[\sum_{t=0}^{\infty}\gamma ^{t}r_{t}|s_{0}=s\right]\), and the state-action value function (i.e., Q-function) as \(Q_{M}^{\pi}(s,a)=\operatorname{\mathbb{E}}_{\pi,M}\left[\sum_{t=0}^{\infty} \gamma^{t}r_{t}|s_{0}=s,s_{0}=a\right]\). By this definition, we note \(J_{M}(\pi)=\operatorname{\mathbb{E}}_{d_{0}}[V_{M}^{\pi}(s)]=\operatorname{ \mathbb{E}}_{d_{0},\pi}[Q_{M}^{\pi}(s,a)]\). We use \([0,V_{\max}]\) to denote the range of value functions, where \(V_{\max}\geq 1\). We denote the ground truth MDP as \(M^{\star}\), and \(J=J_{M^{\star}}\)

Offline RLThe aim of offline RL is to find the policy that maximizes \(J(\pi)\), while using a fixed dataset \(\mathcal{D}\) collected by a behavior policy \(\mu\). We assume the dataset \(\mathcal{D}\) consists of \(\{(s_{n},a_{n},r_{n},s_{n+1})\}_{n=1}^{N}\), where \((s_{n},a_{n})\) is sampled from \(d_{M^{\star}}^{\mu}\) and \(r_{n},s_{n+1}\) follow \(M^{\star}\); for simplicity, we also write \(\mu(s,a)=d_{M^{\star}}^{\mu}(s,a)\).

We assume that the learner has access to a Markovian policy class \(\Pi\) and an MDP model class \(\mathcal{M}\).

**Assumption 1** (Realizability).: _We assume the ground truth model \(M^{\star}\) is in the model class \(\mathcal{M}\)._

In addition, we assume that we are provided a reference policy \(\pi_{\text{ref}}\). In practice, such a reference policy represents a baseline whose performance we want to improve with offline RL and data.

**Assumption 2** (Reference policy).: _We assume access to a reference policy \(\pi_{\text{ref}}\), which can be queried at any state. We assume \(\pi_{\text{ref}}\) is realizable, i.e., \(\pi_{\text{ref}}\in\Pi\)._

If \(\pi_{\text{ref}}\) is not provided, we can still run ARMOR as a typical offline RL algorithm, by first performing behavior cloning on the data and setting the cloned policy as \(\pi_{\text{ref}}\). In this case, ARMOR has RPI with respect to the behavior policy.

Robust Policy ImprovementRPI is a notion introduced in Cheng et al. (2022), which means that the offline algorithm can learn to improve over the behavior policy, using hyperparameters within a known set. Algorithms with RPI are more robust to hyperparameter choices, and they are often derived from the principle of relative pessimism (Cheng et al., 2022). In this work, we extend the RPI concept to compare with an arbitrary reference (or baseline) policy, which can be different from the behavior policy and can take actions outside data support.

## 3 Adversarial Model for Offline Reinforcement Learning (ARMOR)

ARMOR is a model-based offline RL algorithm designed with relative pessimism. The goal of ARMOR is to find a policy \(\widehat{\pi}\) that maximizes the performance difference \(J(\widehat{\pi})-J(\pi_{\text{ref}})\) to a given reference policy \(\pi_{\text{ref}}\), while accounting for the uncertainty due to limited data coverage. ARMOR achieves this by solving a two-player game between a learner policy and an adversary MDP model:

\[\widehat{\pi}=\operatorname*{argmax}_{\pi\in\Pi}\min_{M\in\mathcal{M}_{\alpha }}J_{M}(\pi)-J_{M}(\pi_{\text{ref}}) \tag{1}\]

based on a version space of MDP models

\[\mathcal{M}_{\alpha}=\{M\in\mathcal{M}:\mathcal{E}_{\mathcal{D}}(M)-\min_{M^{ \prime}\in\mathcal{M}}\mathcal{E}_{\mathcal{D}}(M^{\prime})\leq\alpha\}, \tag{2}\]

where we define the model fitting loss as

\[\mathcal{E}_{\mathcal{D}}(M)\coloneqq-\sum_{\mathcal{D}}\log P_{M}(s^{\prime }\mid s,a)+\nicefrac{{(R_{M}(s,a)-r)^{2}}}{{V_{\max}^{2}}} \tag{3}\]

and \(\alpha\geq 0\) is a bound on statistical errors such that \(M^{\star}\in\mathcal{M}_{\alpha}\). In this two-player game, ARMOR is optimizing a lower bound of the relative performance \(J(\pi)-J(\pi_{\text{ref}})\). This is due to the construction that \(M^{\star}\in\mathcal{M}_{\alpha}\), which ensures \(\min_{M\in\mathcal{M}_{\alpha}}J_{M}(\pi)-J_{M}(\pi_{\text{ref}})\leq J_{M^{ \star}}(\pi)-J_{M^{\star}}(\pi_{\text{ref}})\).

One interesting property that follows from optimizing the relative performance lower bound is that \(\widehat{\pi}\) is guaranteed to always be no worse than \(\pi_{\text{ref}}\), for a wide range of \(\alpha\) and regardless of the relationship between \(\pi_{\text{ref}}\) and the data \(\mathcal{D}\).

**Proposition 1**.: _For any \(\alpha\) large enough such that \(M^{\star}\in\mathcal{M}_{\alpha}\), it holds that \(J(\widehat{\pi})\geq J(\pi_{\text{ref}})\)._

This fact can be easily reasoned: Since \(\pi_{\text{ref}}\in\Pi\), we have \(\max_{\pi\in\Pi}\min_{M\in\mathcal{M}_{\alpha}}J_{M}(\pi)-J_{M}(\pi_{\text{ ref}})\geq\min_{M\in\mathcal{M}_{\alpha}}J_{M}(\pi_{\text{ref}})-J_{M}(\pi_{ \text{ref}})=0\). In other words, ARMOR achieves the RPI property with respect to any reference policy \(\pi_{\text{ref}}\) and offline dataset \(\mathcal{D}\).

This RPI property of ARMOR is stronger than the RPI property in the literature. In comparison, previous algorithms with RPI (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021; Cheng et al., 2022) are only guaranteed to be no worse than the behavior policy that collected the data. In Section 3.2, we will also show that when \(\alpha\) is set appropriately, ARMOR can provably compete with the best data covered policy as well, as prior offline RL works (e.g., Xie et al., 2021; Uehara and Sun, 2021; Cheng et al., 2022).

### An Illustrative Toy Example

Why does ARMOR have the RPI property, even when the reference policy \(\pi_{\text{ref}}\) is not covered by the data \(\mathcal{D}\)? While we will give a formal analysis soon in Section 3.2, here we provide some intuitions as to why this is possible. First, notice that ARMOR has access to the reference policy \(\pi_{\text{ref}}\). Therefore, a trivial way to achieve RPI with respect to \(\pi_{\text{ref}}\) is to just output \(\pi_{\text{ref}}\). However, this naive algorithm while never degrading \(\pi_{\text{ref}}\) cannot learn to improve from \(\pi_{\text{ref}}\). ARMOR achieves these two features simultaneously by _1)_ learning an MDP Model, and _2)_ adversarially training this MDP model to minimize the relative performance difference to \(\pi_{\text{ref}}\) during policy optimization.

We illustrate this by a one-dimensional discrete MDP example with five possible states as shown in Figure 2. The dynamic is deterministic, and the agent always starts in the center cell. The agent receives a lower reward of 0.1 in the left-most state and a high reward of 1.0 upon visiting the rightmost state. Say, the agent only has access to a dataset from a sub-optimal policy that always takes the left action to receive the 0.1 reward. Further, let's say we have access to a reference policy that demonstrates optimal behavior on the true MDP by always visiting the right-most state. However, it is unknown a priori that the reference policy is optimal. In such a case, typical offline RL methods can only recover the sub-optimal policy from the dataset as it is the best-covered policy in the data.

ARMOR can learn to recover the expert reference policy in this example by performing rollouts with the adversarially trained MDP model. From the realizability assumption (Assumption 1), we know that the version space of models contains the true model (i.e., \(M^{\star}\in\mathcal{M}_{\alpha}\)). The adversary can then choose a model from this version space where the reference policy \(\pi_{\text{ref}}\) maximally outperforms the learner. In this toy example, the model selected by the adversary would be the one allowing the expert policy to reach the right-most state. Now, optimizing relative performance difference with respect to this model will ensure that the learner can recover the expert behavior, since the only way for the learner to stay competitive with the reference policy is to mimic the reference policy in the region outside data support. In other words, the reason why ARMOR has RPI to \(\pi_{\text{ref}}\) is that

Figure 2: A toy MDP illustrating the RPI property of ARMOR. (Top) The true MDP has deterministic dynamics where taking the left (\(a_{l}\)) or right (\(a_{r}\)) actions takes the agent to corresponding states; start state is in yellow. The suboptimal behavior policy visits only the left part of the state space, and the reference policy demonstrates optimal behavior by always choosing \(a_{r}\). (Bottom) A subset of possible data-consistent MDP models in the version space. The adversary always chooses the MDP that makes the reference maximally outperform the learner. In response, the learner will learn to mimic the reference outside data support to be competitive.

its adversarial model training procedure can augment the original offline data with new states and actions that would cover those generated by running the reference policy.3

Footnote 3: Note that ARMOR does not depend on knowledge of the true reward function and similar arguments hold in the case of learned rewards as we illustrate in Appendix E.

### Theoretical Analysis

Now we make the above discussions formal and give theoretical guarantees on ARMOR's absolute performance and RPI property. To this end, we introduce a single-policy concentrability coefficient, which measures the distribution shift between a policy \(\pi\) and the data distribution \(\mu\).

**Definition 1** (Generalized Single-policy Concentrability).: _We define the generalized single-policy concentrability for policy \(\pi\), model class \(\mathcal{M}\) and offline data distribution \(\mu\) as \(\mathfrak{C}_{\mathcal{M}}(\pi)\coloneqq\sup_{M\in\mathcal{M}}\frac{\mathbb{E }_{\pi}[\mathcal{E}^{\star}(M)]}{\mathbb{E}_{\mu}[\mathcal{E}^{\star}(M)]},\) where \(\mathcal{E}^{\star}(M)=D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{\star} }(\cdot\mid s,a)\right)^{2}+{(R_{M}(s,a)-R^{\star}(s,a))^{2}\mathord{\left/ \vphantom{{R_{M}(s,a)-R^{\star}(s,a)}}\right.\kern-1.2pt}{V_{\mathrm{max}}^{2}}}\)._

Note that \(\mathfrak{C}_{\mathcal{M}}(\pi)\) is always upper bounded by the standard single-policy concentrability coefficient \(\|d^{\pi}/\mu\|_{\infty}\)(e.g., Jin et al., 2021; Rashidinejad et al., 2021; Xie et al., 2021b), but it can be smaller in general with model class \(\mathcal{M}\). It can also be viewed as a model-based analog of the one in Xie et al. (2021a). A detailed discussion around \(\mathfrak{C}_{\mathcal{M}}(\pi)\) can be found in Uehara and Sun (2021).

First, we present the absolute performance guarantee of ARMOR, which holds for a well-tuned \(\alpha\).

**Theorem 2** (Absolute performance).: _Under Assumption 1, there is an absolute constant \(c\) such that for any \(\delta\in(0,1]\), if we set \(\alpha=c\cdot(\log(\left\lvert\mathcal{M}\right\rvert/\delta))\) in Eq. (2), then for any reference policy \(\pi_{\mathsf{ref}}\) and comparator policy \(\pi^{\dagger}\in\Pi\), with probability \(1-\delta\), the policy \(\widehat{\pi}\) learned by ARMOR in Eq. (1) satisfies that \(J(\pi^{\dagger})-J(\widehat{\pi})\) is upper bounded by_

\[\mathcal{O}\left(\left(\sqrt{\mathfrak{C}_{\mathcal{M}}(\pi^{\dagger})}+\sqrt{ \mathfrak{C}_{\mathcal{M}}(\pi_{\mathsf{ref}})}\right)\frac{V_{\mathrm{max}}} {1-\gamma}\sqrt{\frac{\log(\left\lvert\mathcal{M}\right\rvert/\delta)}{n}} \right).\]

Roughly speaking, Theorem 2 shows that \(\widehat{\pi}\) learned by ARMOR can compete with any policy \(\pi^{\dagger}\) with a large enough dataset, as long as the offline data \(\mu\) has good coverage on \(\pi^{\dagger}\) (good coverage over \(\pi_{\mathsf{ref}}\) can be automatically satisfied if we simply choose \(\pi_{\mathsf{ref}}=\mu\), which yields \(\mathfrak{C}_{\mathcal{M}}(\pi_{\mathsf{ref}})=1\)). Compared to the closest model-based offline RL work (Uehara and Sun, 2021), if we set \(\pi_{\mathsf{ref}}=\mu\) (data collection policy), Theorem 2 leads to almost the same guarantee as Uehara and Sun (2021, Theorem 1) up to constant factors.

In addition to absolute performance, below we show that, under Assumptions 1 and 2, ARMOR has the RPI property to \(\pi_{\mathsf{ref}}\): it always improves over \(J(\pi_{\mathsf{ref}})\) for _a wide range of parameter \(\alpha\)_. Compared with the model-free ATAC algorithm in Cheng et al. (2022, Proposition 6), the threshold for \(\alpha\) in Theorem 3 does not depend on sample size \(N\) due to the model-based nature of ARMOR.

**Theorem 3** (Robust strong policy improvement).: _Under Assumptions 1 and 2, there exists an absolute constant \(c\) such that for any \(\delta\in(0,1]\), if: i) \(\alpha\geq c\cdot(\log(\left\lvert\mathcal{M}\right\rvert/\delta))\) in Eq. (2); ii) \(\pi_{\mathsf{ref}}\in\Pi\), then with probability \(1-\delta\), the policy \(\widehat{\pi}\) learned by ARMOR in Eq. (1) satisfies \(J(\widehat{\pi})\geq J(\pi_{\mathsf{ref}})\)._

The detailed proofs of Theorems 2 and 3, as well as the discussion on how to relax Assumptions 1 and 2 to the misspecified model and policy classes are deferred to Appendix A.

## 4 Practical Implementation

In this section, we present a scalable implementation of ARMOR (Algorithm 1) that approximately solves the two-player game in Eq. (1). We first describe the overall design principle and then the algorithmic details.

### A Model-based Actor Critic Approach

For computational efficiency, we take a model-based actor critic approach and solve a regularized version of Eq. (1). We construct this regularized version by relaxing the constraint in the inner minimization of Eq.1 to a regularization term and introducing an additional critic function. To clearly elaborate this, we first present the regularized objective in its complete form, and subsequently derive it from Eq.1.

Let \(\mathcal{F}:\{f:\mathcal{S}\times\mathcal{A}\rightarrow[0,V_{\max}]\}\) be a class of critic functions. The regularized objective is given as

\[\tilde{\pi}\in\operatorname*{argmax}_{\pi\in\Pi}\ \mathcal{L}_{d^{\pi^{\text{ part}}}_{M}}(\pi,f) \tag{6}\]

where \(\mathcal{E}_{\mathcal{D}}(M)=\sum_{\mathcal{D}}-\log P_{M}(s^{\prime}\ \big{|}\ s,a)+(R_{M}(s,a)-r^{2})/V_{\max}^{2}\) is the model-fitting error, \(\mathcal{L}_{d^{\pi^{\text{ part}}}_{M}}(\pi,f):=\mathbb{E}_{d^{\pi^{\text{ part}}}_{M}}[f(s,\pi)-f(s,\pi_{\text{ref}})]\) is equal to the performance difference \((1-\gamma)(J_{M}(\pi)-J_{M}(\pi_{\text{ref}}))\), \(\mathcal{E}_{\rho_{\pi_{\text{ref}}},\tau}(\pi,f,M)\) denotes the squared Bellman error on the distribution \(\rho_{\pi_{\text{ref}},\pi}\) that denotes the distribution generated by first running \(\pi_{\text{ref}}\) and then rolling out \(\pi\) in \(M\) (with a switching time sampled from a geometric distribution of \(\gamma\)), and \(\beta,\lambda\) act as the Lagrange multipliers.

This regularized formulation in Eq.6 can be derived as follows. Assuming \(Q_{M}^{\pi}\in\mathcal{F}\), and using the facts that \(J_{M}(\pi)=\mathbb{E}_{d_{0}}[Q_{M}^{\pi}(s,\pi)]\) and the Bellman equation \(Q_{M}^{\pi}(s,a)=r_{M}(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P_{M}(s,a)}[Q_{M} ^{\pi}(s^{\prime},\pi)]\), we can rewrite Eq.1 as

\[\max_{\pi\in\Pi}\min_{M\in\mathcal{M},f\in\mathcal{F}}\quad\mathbb{E}_{d^{\pi ^{\text{ part}}}_{M}}[f(s,\pi)-f(s,\pi_{\text{ref}})]. \tag{7}\]

\[\text{s.t.}\quad\mathcal{E}_{\mathcal{D}}(M)\leq\alpha+\min_{M^{\prime}\in \mathcal{M}}\mathcal{E}_{\mathcal{D}}(M^{\prime})\]

\[\forall s,a\in\text{supp}(\rho_{\pi_{\text{ref}},\pi}),\quad f(s,a)=r_{M}(s,a )+\gamma\mathbb{E}_{s^{\prime}\sim P_{M}(s,a)}[f(s^{\prime},\pi)]\]

We then convert the constraints in Eq.7 into regularization terms in the inner minimization by introducing Lagrange multipliers (\(\beta\), \(\lambda\)), following (Xie et al., 2021a; Cheng et al., 2022), and drop the constants not affected by \(M,f,\pi\), which results in Eq.6.

### Algorithm Details

Algorithm 1 is an iterative solver for approximating the solution to Eq. (6). Here we further approximate \(d_{M}^{\pi_{\text{ref}}}\) and \(\rho_{\pi_{\text{ref}},\pi}\) in Eq. (6) using samples from the state-action buffer \(\mathcal{D}_{\text{model}}\). We want ensure that \(\mathcal{D}_{\text{model}}\) has a larger coverage than both \(d_{M}^{\pi_{\text{ref}}}\) and \(\rho_{\pi_{\text{ref}},\pi}\). We do so heuristically, by constructing the model replay buffer \(\mathcal{D}_{\text{model}}\) through repeatedly rolling out \(\pi\) and \(\pi_{\text{ref}}\) with the adversarially trained MDP model \(M\), such that \(\mathcal{D}_{\text{model}}\) contains a diverse training set of state-action tuples.

Specifically, the algorithm takes as input an offline dataset \(\mathcal{D}_{\text{real}}\), a policy \(\pi\), an MDP model \(M\) and two critic networks \(f_{1},f_{2}\). At every iteration, the algorithm proceeds in two stages. First, the adversary is optimized to find a data-consistent model that minimizes the performance difference with the reference policy. We sample mini-batches of only states and actions \(\mathcal{D}_{\text{real}}^{\text{mini}}\) and \(\mathcal{D}_{\text{model}}^{\text{mini}}\) from the real and model-generated datasets respectively (creftype 4). The MDP model \(M\) is queried on these mini-batches to generate next-state and reward predictions. The adversary then updates the model and Q-functions (creftype 5) using the gradient of the loss described in Eq. (4), where

\[\mathcal{L}_{\mathcal{D}_{M}}(f,\pi,\pi_{\text{ref}})\coloneqq \mathbb{E}_{\mathcal{D}_{M}}[f(s,\pi(s))-f(s,\pi_{\text{ref}}(s)]\] \[\mathcal{E}_{\mathcal{D}_{M}}^{w}(f,M,\pi)\coloneqq(1-w)\mathcal{ E}_{\mathcal{D}}^{td}(f,f,M,\pi)+w\mathcal{E}_{\mathcal{D}}^{td}(f,\bar{f},M,\pi)\] \[\mathcal{E}_{\mathcal{D}_{\text{real}}^{\text{mini}}}(M)\coloneqq \mathbb{E}_{\mathcal{D}_{\text{real}}^{\text{mini}}}[-\log P_{M}(s^{\prime} \mid s,a)+(\nicefrac{{R_{M}(s,a)-r)^{2}}}{{V_{\text{max}}^{2}}}]\]

\(\mathcal{L}_{\mathcal{D}_{M}}\) is the pessimistic loss term that forces the \(f\) to predict a lower value for the learner than the reference on the sampled states. \(\mathcal{E}_{\mathcal{D}_{M}}^{w}\) is the Bellman surrogate to encourage the Q-functions to be consistent with the model-generated data \(\mathcal{D}_{M}\). We use the double Q residual algorithm loss similar to Cheng et al. (2022), which is defined as a convex combination of the temporal difference losses with respect to the critic and the delayed target networks, \(\mathcal{E}_{\mathcal{D}}^{td}(f,f^{\prime},M,\pi)\coloneqq\mathbb{E}_{ \mathcal{D}}\left[\left(f(s,a)-r-\gamma f^{\prime}(s^{\prime},\pi)\right)^{2} \right]\). \(\mathcal{E}_{\mathcal{D}}(M)\) is the model-fitting loss that ensures the model is data-consistent. \(\beta\) and \(\lambda\) control the effect of the pessimistic loss, by constraining Q-functions and models the adversary can choose. Once the adversary is updated, we update the policy (creftype 6) to maximize the pessimistic loss as defined in Eq. (5). Similar to Cheng et al. (2022), we choose one Q-function and a slower learning rate for the policy updates (\(\eta_{\text{fast}}\gg\eta_{\text{slow}}\)).

We remark that \(\mathcal{E}_{\mathcal{D}_{M}}^{w}\) not only affects \(f_{1},f_{2}\), but also \(M\), i.e., it forces the model to generate transitions where the Q-function is Bellman consistent. This allows the pessimistic loss to indirectly affect the model learning, thus making the model adversarial. Consider the special case where \(\lambda=0\) in the loss of creftype 4. The model here is no longer forced to be data consistent, and the adversary can now freely update the model via \(\mathcal{E}_{\mathcal{D}_{M}}^{w}\) such that the Q-function is always Bellman consistent. As a consequence, the algorithm becomes equivalent to IL on the model-generated states. We empirically study this behavior in our experiments (creftype 5).

Lines 7 and 8 describe our model-based rollout procedure. We incrementally rollout both \(\pi\) and \(\pi_{\text{ref}}\) from states in \(\mathcal{D}_{\text{real}}^{\text{mini}}\) for a horizon \(H\), and add the generated transitions to \(\mathcal{D}_{\text{model}}\). The aim of this strategy is to generate a distribution with large coverage for training the adversary and policy, and we discuss this in detail in the next section.

Finally, it is important to note the fact that neither the pessimistic nor the Bellman surrogate losses uses the real transitions; hence our algorithm is completely model-based from a statistical point of view, that the value function \(f\) is solely an intermediate variable that helps in-model optimization and not directly fit from data.

## 5 Experiments

We test the efficacy of ARMOR on two major fronts: (1) performance comparison to existing offline RL algorithms, and (2) robust policy improvement over a reference policy that is not covered by the dataset, a novel setting that is not applicable to existing works4. We use the D4RL (Fu et al., 2020) continuous control benchmarks datasets for all our experiments and the code will be made public.

Footnote 4: In Appendix F we empirically show how imitation learning can be obtained as a special case of ARMOR

Experimental Setup:We parameterize \(\pi,f_{1},f_{2}\) and \(M\) using feedforward neural networks, and set \(\eta_{\text{fast}}=5e-4\), \(\eta_{\text{slow}}=5e-7\), \(w=0.5\) similar to Cheng et al. (2022). In all our experiments, we vary only the \(\beta\) and \(\lambda\) parameters which control the amount of pessimism; others are fixed. Importantly, we set the rollout horizon to be the max episode horizon defined in the environment.

The dynamics model is pre-trained for 100k steps using model-fitting loss on the offline dataset. ARMOR is then trained for 1M steps on each dataset. Refer to Appendix F for more details.

### Comparison with Offline RL Baselines

By setting the reference policy to the behavior-cloned policy on the offline dataset, we can use ARMOR as a standard offline RL algorithm. Table 1 shows a comparison of the performance of ARMOR against SoTA model-free and model-based offline RL baselines. In the former category, we consider ATAC (Cheng et al., 2022), CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2021), and for the latter we consider MoREL (Kidambi et al., 2020), MOPO (Yu et al., 2020), and RAMBO (Rigter et al., 2022). We also compare against COMBO (Yu et al., 2021) which is a hybrid model-free and model-based algorithm. In these experiments, we initially warm start the optimization for 100k steps, by training the policy and Q-function using behavior cloning and temporal difference learning respectively on the offline dataset to ensure the learner policy is initialized to be the same as the reference. Overall, we observe that ARMOR consistently outperforms or is competitive with the best baseline algorithm on most datasets. Specifically, compared to other purely model-based baselines (MoREL, MOPO and RAMBO), there is a marked increase in performance in the _walker2d-med, hopper-med-exp_ and _walker2d-med-exp_ datasets. We would like to highlight two crucial elements about ARMOR, in contrast to other model-based baselines - (1) ARMOR achieves SoTA performance using only a _single_ neural network to model the MDP, as opposed to complex network ensembles employed in previous model-based offline RL methods (Kidambi et al., 2020; Yu et al., 2021, 2020; Rigter et al., 2022), and (2) to the best of our knowledge, ARMOR is the only purely model-based offline RL algorithm that has shown performance comparable with model-free algorithms on the high-dimensional Adroit environments. The lower performance compared to RAMBO on _halfcheetah-med_ and _halfcheetah-med-replay_ may be attributed to that the much larger computational budget used by RAMBO is required for convergence on these datasets.

### Robust Policy Improvement

Next, we test whether the practical version of ARMOR demonstrates RPI of the theoretical version. We consider a set of 14 datasets comprised of the _medium_ and _medium-replay_ versions of D4RL locomotion tasks, as well as the _human_ and _cloned_ versions of the Adroit tasks, with the reference policy set to be the stochastic behavior cloned policy on the expert dataset. We chose these combi

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline Dataset & ARMOR & MoREL & MOPO & RAMBO & COMBO & ATAC & CQL & IQL & BC \\ \hline hopper-med & **101.4** & **95.4** & 28.0 & **92.8** & **97.2** & 85.6 & 86.6 & 66.3 & 29.0 \\ walker2d-med & **90.7** & 77.8 & 17.8 & **86.9** & **81.9** & **89.6** & 74.5 & 78.3 & 6.6 \\ halfcheetah-med & 54.2 & 42.1 & 42.3 & **77.6** & 54.2 & 53.3 & 44.4 & 47.4 & 36.1 \\ hopper-med-replay & **97.1** & **93.6** & 67.5 & **96.6** & 89.5 & **102.5** & 48.6 & **94.7** & 11.8 \\ walker2d-med-replay & **85.6** & 49.8 & 39.0 & **85.0** & 56.0 & **92.5** & 32.6 & 73.9 & 11.3 \\ halfcheetah-med-replay & 50.5 & 40.2 & 53.1 & **68.9** & 55.1 & 48.0 & 46.2 & 44.2 & 38.4 \\ hopper-med-exp & **103.4** & **108.7** & 23.7 & 83.3 & **111.1** & **111.9** & **111.0** & 91.5 & **111.9** \\ walker2d-med-exp & **112.2** & 95.6 & 44.6 & 68.3 & **103.3** & **114.2** & 98.7 & **109.6** & 6.4 \\ halfcheetah-med-exp & **93.5** & 53.3 & 63.3 & **93.7** & **90.0** & **94.8** & 62.4 & **86.7** & 35.8 \\ \hline pen-human & **72.8** & - & - & - & - & 53.1 & 37.5 & **71.5** & 34.4 \\ hammer-human & 1.9 & - & - & - & - & 1.5 & **4.4** & 1.4 & 1.5 \\ door-human & 6.3 & - & - & - & - & 2.5 & **9.9** & 4.3 & 0.5 \\ relocate-human & **0.4** & - & - & - & - & 0.1 & 0.2 & 0.1 & 0.0 \\ pen-cloned & **51.4** & - & - & - & - & 43.7 & 39.2 & 37.3 & **56.9** \\ hammer-cloned & 0.7 & - & - & - & - & 1.1 & **2.1** & 0.8 \\ door-cloned & -0.1 & - & - & - & - & **3.7** & 0.4 & 1.6 & -0.1 \\ relocate-cloned & -0.0 & - & - & - & - & **0.2** & -0.1 & -0.2 & -0.1 \\ pen-exp & 112.2 & - & - & - & - & **136.2** & 107.0 & - & 85.1 \\ hammer-exp & **118.8** & - & - & - & - & **126.9** & 86.7 & - & **125.6** \\ door-exp & **98.7** & - & - & - & - & **99.3** & **101.5** & - & 34.9 \\ relocate-exp & **96.0** & - & - & - & - & **99.4** & **95.0** & - & **101.3** \\ \hline \end{tabular}
\end{table}
Table 1: Performance comparison of ARMOR against baselines on the D4RL datasets. The values for ARMOR denote last iteration performance averaged over 4 random seeds, and baseline values were taken from their respective papers. The values denote normalized returns based on random and expert policy returns similar to Fu et al. (2020). Boldface denotes performance within \(10\%\) of the best performing algorithm. We report results with standard deviations in Appendix F.

nations of dataset quality and reference, to ensure that the reference policy takes out-of-distribution actions with respect to the data. Unlike Sec. 5.1 here the reference policy is a black-box given as a part of the problem definition. This opens the question of how the learner should be initialized, since we can not trivially initialize the learner to be the reference as in the previous experiments.6 In a similar spirit to Sec. 5.1, one might consider initializing the learner close to the reference by behavior cloning the reference policy on the provided dataset during warmstart, i.e, by replacing the dataset actions with reference actions. However, when the reference chooses out of support actions, this procedure will not provide a good global approximation of the reference policy, which can make the optimization problem harder. Instead, we propose to learn a residual policy where the learned policy outputs an additive correction to the reference (Silver et al., 2018). This is an appropriate choice since ARMOR does not make any restrictive assumptions about the structure of the policy class. Figure 3 shows the normalized return achieved by ARMOR for different \(\beta\), with fixed values for remaining hyperparameters. We observe that ARMOR is able to achieve performance comparable or better than the reference policy for a range of \(\beta\) values uniformly across all datasets, thus verifying the RPI property in practice. Specifically, there is significant improvement via RPI in the _hammer_, _door_ and _relocate_ domains, where running ARMOR as a pure offline RL algorithm(Section 5.1) does not show any progress 7. Overall, we note the following metrics:

Footnote 6: In Appendix F.5 we provide further experiments for different choices of reference policies.

Footnote 7: We provide comparisons when using a behavior cloning initialization for the learner in Appendix F.

* In 14/14 datasets, ARMOR shows RPI (i.e., ARMOR policy is no worse than the reference when measured by overlap of confidence intervals). Further, considering the difference between ORL and REF as a rough indication of whether the reference is within data support, we note that in 12/14 cases REF is strictly better than ORL, and in all those cases ARMOR demonstrates RPI.
* In 5/14 datasets, the ARMOR policy is strictly better than the reference. (Criterion: the lower confidence of ARMOR performance is better than upper confidence of REF). It is important to note that this metric is highly dependent on the quality of the reference policy. Since the reference is near-expert, it can be hard for some environments to improve significantly over it.

## 6 Discussion

The RPI of ARMOR is highly valuable as it allows easy tuning of the pessimism hyperparameter without performance degradation. We believe that leveraging this property can pave the way for real-world deployment of offline RL. Thus, we next present a discussion of RPI.8

Footnote 8: Due to space limit, we defer the complete discussion to Appendix D and only provide salient points here.

_When does RPI actually improve over the reference policy?_

Given ARMOR's ability to improve over an arbitrary policy, the following question naturally arises: Can ARMOR nontrivially improve the output policy of other offline algorithms, including itself? If this were true, can we repeatedly run ARMOR to improve over itself and obtain the _best_ policy any algorithm can learn offline? Unfortunately, the answer is negative. Not only can ARMOR not

Figure 3: Verification of RPI over the reference policy for different \(\beta\) (purple). ORL denotes the performance of offline RL with ARMOR ( Table 1), and REF is the performance of reference policy.5

improve over itself, but it also cannot improve over a variety of algorithms (e.g., absolute pessimism or minimax regret). In fact, the optimal policy of an _arbitrary_ model in the version space \(\mathcal{M}_{\alpha}\) is provably unimprovable (Corollary10; AppendixD). With a deep dive into when RPI gives nontrivial improvement (AppendixD), we found some interesting observations, which we highlight here.

Return maximization and regret minimization are _different_ in offline RLThese objectives generally produce different policies, even though they are equivalent in online RL. Their equivalence in online RL relies on the fact that online exploration can eventually resolve any uncertainty. In offline RL with an arbitrary data distribution, there will generally be model uncertainty that cannot be resolved, and the worst-case reasoning over such model uncertainty (i.e., \(\mathcal{M}_{\alpha}\)) leads to definitions that are no longer equivalent. Moreover, it is impossible to compare return maximization and regret minimization and make a claim about which is better. _They are not simply an algorithm design choice, but are definitions of the learning goals and guarantees themselves_--and are thus incomparable: if we care about obtaining a guarantee for the worst-case return, the return maximization is optimal by definition; if we are more interested in a guarantee for the worst-case regret, then regret minimization is optimal. We also note that analyzing algorithms under a metric that is different from the one they are designed for can lead to unusual conclusions, e.g., Xiao et al. (2021) show that optimistic/neutral/pessimistic algorithms are equally minimax-optimal in terms of their regret guarantees in offline multi-armed bandits. However, the algorithms they consider are optimistic/pessimistic with respect to the _return_ (as commonly considered in the offline RL literature) not the _regret_ which is the performance metric they are interested in analyzing.

\(\pi_{\mathsf{ref}}\) **is more than a hyperparameter--it defines the performance metric and learning goal**Corollary10 in AppendixD shows that ARMOR has many different fixed points: when \(\pi_{\mathsf{ref}}\) is chosen from these fixed points, the solution to Eq.1 is also \(\pi_{\mathsf{ref}}\). Furthermore, some of them may seem quite unreasonable for offline learning (e.g., the greedy policy to an arbitrary model in \(\mathcal{M}_{\alpha}\) or even the optimistic policy). This is not a defect of the algorithm. Rather, because of the unresolvable uncertainty in the offline setting, there are many different performance metrics/learning goals that are generally incompatible/incomparable, and the agent designer must make a conscious choice among them and convey the intention to the algorithm. In ARMOR, such a choice is explicitly conveyed by \(\pi_{\mathsf{ref}}\), which makes ARMOR subsume return maximization and regret minimization as special cases.

## 7 Conclusion

We have presented a model-based offline RL framework, ARMOR, that can improve over arbitrary reference policies regardless of data coverage, by using the concept of relative pessimism. ARMOR provides strong theoretical guarantees with general function approximators, and exhibits robust policy improvement over the reference policy for a wide range of hyper-parameters. We have also presented a scalable deep learning instantiation of the theoretical algorithm. Empirically, we demonstrate that ARMOR indeed enjoys the RPI property, and has competitive performance with several SoTA model-free and model-based offline RL algorithms, while employing a simpler model architecture (a single MDP model) than other model-based baselines that rely on ensembles. This also opens the opportunity to leverage high-capacity world models (Hafner et al., 2023) with offline RL in the future. However, there are also some **limitations**. While RPI holds for the pessimism parameter, the others still need to be tuned. In practice, the non-convexity of the optimization can also make solving the two-player game challenging. For instance, if the adversary is not strong enough (i.e., far from solving the inner minimization), RPI would break. Further, runtime of ARMOR is slightly slower than model-free algorithms owing to extra computations for model rollouts.

## Acknowledgments and Disclosure of Funding

Nan Jiang acknowledges funding support from NSF IIS-2112471 and NSF CAREER IIS-214178.

## References

* Agarwal et al. (2020) Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. _Advances in Neural Information Processing Systems_, 33:20095-20107, 2020.
* Goyal et al. (2017)Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. _Machine Learning_, 71(1):89-129, 2008.
* Chen and Jiang (2019) Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051, 2019.
* Chen et al. (2022) Xiong-Hui Chen, Yang Yu, Zheng-Mao Zhu, Zhihua Yu, Zhenjun Chen, Chenghe Wang, Yinan Wu, Hongqiu Wu, Rong-Jun Qin, Ruijin Ding, et al. Adversarial counterfactual environment model learning. _arXiv preprint arXiv:2206.04890_, 2022.
* Cheng et al. (2022) Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. _International Conference on Machine Learning_, 2022.
* Farahmand et al. (2010) Amir Massoud Farahmand, Remi Munos, and Csaba Szepesvari. Error propagation for approximate policy and value iteration. In _Advances in Neural Information Processing Systems_, 2010.
* Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* Fujimoto and Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in Neural Information Processing Systems_, 34:20132-20145, 2021.
* Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International Conference on Machine Learning_, pages 1587-1596. PMLR, 2018.
* Fujimoto et al. (2019) Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International Conference on Machine Learning_, pages 2052-2062, 2019.
* Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* Jin et al. (2021) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* Kakade (2001) Sham M Kakade. A natural policy gradient. _Advances in Neural Information Processing Systems_, 14, 2001.
* Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. In _Advances in Neural Information Processing Systems_, 2020.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations_, 2015.
* Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* Kumar et al. (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32:11784-11794, 2019.
* Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Lange et al. (2012) Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. _Reinforcement learning: State-of-the-art_, pages 45-73, 2012.
* Lange et al. (2013)Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with baseline bootstrapping. In _International Conference on Machine Learning_, pages 3652-3661. PMLR, 2019.
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Liu et al. (2022) Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, volume 178, pages 5175-5220. PMLR, 2022.
* Liu et al. (2020a) Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with stationary distribution correction. In _Uncertainty in Artificial Intelligence_, pages 1180-1190. PMLR, 2020a.
* Liu et al. (2020b) Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy reinforcement learning without great exploration. _Advances in Neural Information Processing Systems_, 33:1264-1274, 2020b.
* Munos (2003) Remi Munos. Error bounds for approximate policy iteration. In _Proceedings of the Twentieth International Conference on International Conference on Machine Learning_, pages 560-567, 2003.
* Munos and Szepesvari (2008) Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. _Journal of Machine Learning Research_, 9(5), 2008.
* Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716, 2021.
* Rigter et al. (2022) Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:16082-16097, 2022.
* Shi et al. (2022) Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. In _International Conference on Machine Learning_, pages 19967-20025. PMLR, 2022.
* Siegel et al. (2020) Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. _arXiv preprint arXiv:2002.08396_, 2020.
* Silver et al. (2018) Tom Silver, Kelsey Allen, Josh Tenenbaum, and Leslie Kaelbling. Residual policy learning. _arXiv preprint arXiv:1812.06298_, 2018.
* Uehara and Sun (2021) Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In _International Conference on Learning Representations_, 2021.
* van de Geer (2000) Sara A van de Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* Wu et al. (2019) Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* Xiao et al. (2021) Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. On the optimality of batch policy optimization algorithms. In _International Conference on Machine Learning_, pages 11362-11371. PMLR, 2021.
* Xie and Jiang (2020) Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. In _Conference on Uncertainty in Artificial Intelligence_, pages 550-559. PMLR, 2020.
* Xie and Jiang (2021) Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In _International Conference on Machine Learning_, pages 11404-11413. PMLR, 2021.
* Xie et al. (2021)Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 34:6683-6694, 2021a.
* Xie et al. (2021b) Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _Advances in Neural Information Processing Systems_, 34:27395-27407, 2021b.
* Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. _Advances in Neural Information Processing Systems_, 33:14129-14142, 2020.
* Yu et al. (2021) Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in Neural Information Processing Systems_, 34:28954-28967, 2021.
* Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Zhang (2006) Tong Zhang. From \(\varepsilon\)-entropy to kl-entropy: Analysis of minimum information complexity density estimation. _The Annals of Statistics_, 34(5):2180-2210, 2006.

Proofs for Section 3

### Technical Tools

**Lemma 4** (Simulation lemma).: _Consider any two MDP model \(M\) and \(M^{\prime}\), and any \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\), we have_

\[|J_{M}(\pi)-J_{M^{\prime}}(\pi)|\leq\frac{V_{\max}}{1-\gamma}\mathbb{E}_{d^{ \pi}}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{\prime}}(\cdot\mid s,a)\right)\right]+\frac{1}{1-\gamma}\mathbb{E}_{d^{\pi}}\left[|R_{M}(s,a)-R_{M^ {\prime}}(s,a)|\right].\]

Lemma 4 is the standard simulation lemma in model-based reinforcement learning literature, and its proof can be found in, e.g., Uehara and Sun (2021, Lemma 7).

### MLE Guarantees

We use \(\ell_{\mathcal{D}}(M)\) to denote the likelihood of model \(M=(P,R)\) with offline data \(\mathcal{D}\), where

\[\ell_{\mathcal{D}}(M)=\prod_{(s,a,r,s^{\prime})\in\mathcal{D}}P_{M}(s^{\prime }\mid s,a). \tag{8}\]

For the analysis around maximum likelihood estimation, we largely follow the proving idea of Agarwal et al. (2020); Liu et al. (2022), which is inspired by Zhang (2006).

The next lemma shows that the ground truth model \(M^{\star}\) has a comparable log-likelihood compared with MLE solution.

**Lemma 5**.: _Let \(M^{\star}\) be the ground truth model. Then, with probability at least \(1-\delta\), we have_

\[\max_{M\in\mathcal{M}}\log\ell_{\mathcal{D}}(M)-\log\ell_{\mathcal{D}}(M^{ \star})\leq\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}).\]

Proof of Lemma 5.: The proof of this lemma is obtained by a standard argument of MLE (see, e.g., van de Geer, 2000). For any \(M\in\mathcal{M}\),

\[\mathbb{E}\left[\exp\left(\log\ell_{\mathcal{D}}(M)-\log\ell_{ \mathcal{D}}(M^{\star})\right)\right] =\mathbb{E}\left[\frac{\ell_{\mathcal{D}}(M)}{\ell_{\mathcal{D}}( M^{\star})}\right]\] \[=\mathbb{E}\left[\frac{\prod_{(s,a,r,s^{\prime})\in\mathcal{D}}P_ {M}(s^{\prime}\mid s,a)}{\prod_{(s,a,r,s^{\prime})\in\mathcal{D}}P_{M^{\star} }(s^{\prime}\mid s,a)}\right]\] \[=\mathbb{E}\left[\prod_{(s,a,r,s^{\prime})\in\mathcal{D}}\frac{P _{M}(s^{\prime}\mid s,a)}{P_{M^{\star}}(s^{\prime}\mid s,a)}\right]\] \[=\mathbb{E}\left[\prod_{(s,a)\in\mathcal{D}}\mathbb{E}\left[ \frac{P_{M}(s^{\prime}\mid s,a)}{P_{M^{\star}}(s^{\prime}\mid s,a)}\right]\,s,a\right]\Bigg{]}\] \[=\mathbb{E}\left[\prod_{(s,a)\in\mathcal{D}}\sum_{s^{\prime}}P_ {M}(s^{\prime}\mid s,a)\right]\] \[=1. \tag{9}\]

Then by Markov's inequality, we obtain

\[\mathbb{P}\left[\left(\log\ell_{\mathcal{D}}(M)-\log\ell_{\mathcal{ D}}(M^{\star})\right)>\log(\nicefrac{{1}}{{\delta}})\right]\] \[\leq\underbrace{\mathbb{E}\left[\exp\left(\log\ell_{\mathcal{D}} (M)-\log\ell_{\mathcal{D}}(M^{\star})\right)\right]}_{=1\text{ by Eq.~{}\eqref{eq:prob}}}\cdot\exp\left[-\log(\nicefrac{{1}}{{\delta}})\right]=\delta.\]

Therefore, taking a union bound over \(\mathcal{M}\), we obtain

\[\mathbb{P}\left[\left(\log\ell_{\mathcal{D}}(M)-\log\ell_{\mathcal{D}}(M^{ \star})\right)>\log(\nicefrac{{|\mathcal{M}|}}{{\delta}})\right]\leq\delta.\]

This completes the proof.

The following lemma shows that, the on-support error of any model \(M\in\mathcal{M}\) can be captured via its log-likelihood (by comparing with the MLE solution).

**Lemma 6**.: _For any model \(M\), we have with probability at least \(1-\delta\),_

\[\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{\star}}( \cdot\mid s,a))^{2}\right]\leq\mathcal{O}\left(\frac{\log\ell_{\mathcal{D}}(M^ {\star})-\log\ell_{\mathcal{D}}(M)+\log(\nicefrac{{|\mathcal{M}|}}{{\delta}})}{ n}\right),\]

_where \(\ell_{\mathcal{D}}(\cdot)\) is defined in Eq. (8)._

Proof of Lemma 6.: By Agarwal et al. (2020, Lemma 25), we have

\[\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{\star}}( \cdot\mid s,a))^{2}\right]\leq\,-\,2\log\mathbb{E}_{\mu\times P_{M^{\star}}} \left[\exp\left(-\frac{1}{2}\log\left(\frac{P_{M^{\star}}(s^{\prime}\mid s,a)} {P_{M}(s^{\prime}\mid s,a)}\right)\right)\right], \tag{10}\]

where \(\mu\times P_{M^{\star}}\) denote the ground truth offline joint distribution of \((s,a,s^{\prime})\).

Let \(\widetilde{\mathcal{D}}=\{(\widetilde{s}_{i},\widetilde{a}_{i},\widetilde{r}_ {i},\widetilde{s}^{\prime}_{i})\}_{i=1}^{n}\sim\mu\) be another offline dataset that is independent to \(\mathcal{D}\). Then,

\[-n\cdot\log\mathbb{E}_{\mu\times P_{M^{\star}}}\left[\exp\left(- \frac{1}{2}\log\left(\frac{P_{M^{\star}}(s^{\prime}\mid s,a)}{P_{M}(s^{\prime} \mid s,a)}\right)\right)\right]\right.\] \[=\,-\sum_{i=1}^{n}\log\mathbb{E}_{(\widetilde{s}_{i},\widetilde{a }_{i},\widetilde{s}^{\prime}_{i})\sim\mu}\left[\exp\left(-\frac{1}{2}\log\left( \frac{P_{M^{\star}}(\widetilde{s}^{\prime}_{i}\mid\widetilde{s}_{i},\widetilde {a}_{i})}{P_{M}(\widetilde{s}^{\prime}_{i}\mid\widetilde{s}_{i},\widetilde{a}_ {i})}\right)\right)\right]\] \[=\,-\log\mathbb{E}_{\widetilde{\mathcal{D}}\sim\mu}\left[\exp \left(\sum_{i=1}^{n}-\frac{1}{2}\log\left(\frac{P_{M^{\star}}(\widetilde{s}^ {\prime}_{i}\mid\widetilde{s}_{i},\widetilde{a}_{i})}{P_{M}(\widetilde{s}^{ \prime}_{i}\mid\widetilde{s}_{i},\widetilde{a}_{i})}\right)\right)\,\left| \,\mathcal{D}\right]\] \[=\,-\log\mathbb{E}_{\widetilde{\mathcal{D}}\sim\mu}\left[\exp \left(\sum_{(s,a,s^{\prime})\in\widetilde{\mathcal{D}}}-\frac{1}{2}\log\left( \frac{P_{M^{\star}}(s^{\prime}\mid s,a)}{P_{M}(s^{\prime}\mid s,a)}\right) \right)\,\left|\,\mathcal{D}\right]. \tag{11}\]

We use \(\ell_{M}(s,a,s^{\prime})\) as the shorthand of \(-\frac{1}{2}\log\left(\frac{P_{M^{\star}}(s^{\prime}\mid s,a)}{P_{M}(s^{\prime }\mid s,a)}\right)\), for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\). By Agarwal et al. (2020, Lemma 24) (see also Liu et al., 2022, Lemma 15), we know

\[\mathbb{E}_{\mathcal{D}\sim\mu}\left[\exp\left(\sum_{(s,a,s^{\prime})\in \widetilde{\mathcal{D}}}\ell_{M}(s,a,s^{\prime})-\log\mathbb{E}_{\widetilde{ \mathcal{D}}\sim\mu}\left[\exp\left(\sum_{(s,a,s^{\prime})\in\widetilde{ \mathcal{D}}}\ell_{M}(s,a,s^{\prime})\right)\,\left|\,\mathcal{D}\right]-\log \left|\mathcal{M}\right|\right)\right]\leq 1.\]

Thus, we can use Chernoff method as well as a union bound on the equation above to obtain the following exponential tail bound: with probability at least \(1-\delta\), we have for all \((P,R)=M\in\mathcal{M}\),

\[-\log\mathbb{E}_{\widetilde{\mathcal{D}}\sim\mu}\left[\exp\left(\sum_{(s,a,s^ {\prime})\in\widetilde{\mathcal{D}}}\ell_{M}(s,a,s^{\prime})\right)\,\left|\, \mathcal{D}\right]\leq-\sum_{(s,a,s^{\prime})\in\mathcal{D}}\ell_{M}(s,a,s^{ \prime})+2\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}). \tag{12}\]

Plugging back the definition of \(\ell_{M}\) and combining Eqs. (10) to (12), we obtain

\[n\cdot\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P(\cdot\mid s,a),P_{M^{\star}} (\cdot\mid s,a)\right)^{2}\right]\leq\frac{1}{2}\sum_{(s,a,s^{\prime})\in \mathcal{D}}\log\left(\frac{P_{M^{\star}}(s^{\prime}\mid s,a)}{P(s^{\prime} \mid s,a)}\right)+2\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}).\]

Therefore, we obtain

\[n\cdot\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P(\cdot\mid s,a),P_{M^{\star}}(\cdot\mid s,a)\right)^{2}\right]\] \[\lesssim\sum_{(s,a,s^{\prime})\in\mathcal{D}}\log\left(\frac{P_{M ^{\star}}(s^{\prime}\mid s,a)}{P(s^{\prime}\mid s,a)}\right)+\log(\nicefrac{{| \mathcal{M}|}}{{\delta}})\] \[=\,\log\ell_{\mathcal{D}}(M^{\star})-\log\ell_{\mathcal{D}}(M)+ \log(\nicefrac{{|\mathcal{M}|}}{{\delta}}).\hskip 56.905512pt(\ell_{\mathcal{D}}( \cdot)\text{ is defined in Eq.~{}\eqref{eq:d

### Guarantees about Model Fitting Loss

**Lemma 7**.: _Let \(M^{\star}\) be the ground truth model. Then, with probability at least \(1-\delta\), we have_

\[\mathcal{E}_{\mathcal{D}}(M^{\star})-\min_{M\in\mathcal{M}}\mathcal{E}_{\mathcal{ D}}(M)\leq\mathcal{O}\left(\log(\nicefrac{{|\mathcal{M}|}}{{\delta}})\right),\]

_where \(\mathcal{E}_{\mathcal{D}}\) is defined in Eq. (3)._

Proof of Lemma 7.: By definition, we know

\[\mathcal{E}_{\mathcal{D}}(M)=-\log\ell_{\mathcal{D}}(M)+\nicefrac{{(R_{M}(s,a) -r)^{2}}}{{V_{\max}^{2}}}\]

By Lemma 5, we know

\[\max_{M\in\mathcal{M}}\log\ell_{\mathcal{D}}(M)-\log\ell_{\mathcal{D}}(M^{ \star})\leq\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}). \tag{13}\]

In addition, by Xie et al. (2021a, Theorem A.1) (with setting \(\gamma=0\)), we know w.p. \(1-\delta\),

\[\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\left(R^{\star}(s,a)-r\right)^{2}-\min_ {M\in\mathcal{M}}\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\left(R_{M}(s,a)-r \right)^{2}\lesssim\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}). \tag{14}\]

Combining Eqs. (13) and (14) and using the fact of \(V_{\max}\geq 1\), we have w.p. \(1-\delta\),

\[\mathcal{E}_{\mathcal{D}}(M^{\star})-\min_{M\in\mathcal{M}}\mathcal{ E}_{\mathcal{D}}(M)\] \[\leq\max_{M\in\mathcal{M}}\log\ell_{\mathcal{D}}(M)-\min_{M\in \mathcal{M}}\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\nicefrac{{(R_{M}(s,a)-r)^ {2}}}{{V_{\max}^{2}}}+\mathcal{E}_{\mathcal{D}}(M^{\star})\] \[\lesssim\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}).\]

This completes the proof. 

**Lemma 8**.: _For any \(M\in\mathcal{M}\), we have with probability at least \(1-\delta\),_

\[\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P _{M^{\star}}(\cdot\mid s,a)\right)^{2}+\nicefrac{{(R_{M}(s,a)-R^{\star}(s,a))^ {2}}}{{V_{\max}^{2}}}\right]\] \[\qquad\qquad\leq\mathcal{O}\left(\frac{\mathcal{E}_{\mathcal{D}} (M)-\mathcal{E}_{\mathcal{D}}(M^{\star})+\log(\nicefrac{{|\mathcal{M}|}}{{ \delta}})}{n}\right),\]

_where \(\mathcal{E}_{\mathcal{D}}\) is defined in Eq. (3)._

Proof of Lemma 8.: By Lemma 6, we have w.p. \(1-\delta\),

\[n\cdot\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{ \star}}(\cdot\mid s,a)\right)^{2}\right]\lesssim\log\ell_{\mathcal{D}}(M^{ \star})-\log\ell_{\mathcal{D}}(M)+\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}). \tag{15}\]

Also, we have

\[n\cdot\mathbb{E}_{\mu}\left[\left(R_{M}(s,a)-R^{\star}(s,a) \right)^{2}\right] \tag{16}\] \[=n\cdot\mathbb{E}_{\mu}\left[\left(R_{M}(s,a)-r\right)^{2}\right] -n\cdot\mathbb{E}_{\mu}\left[(R^{\star}(s,a)-r)^{2}\right]\] (see, e.g., Xie et al., 2021a, Eq. (A.10) with \(\gamma=0\)) \[\lesssim\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\left(R_{M}(s,a)-r \right)^{2}-\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\left(R^{\star}(s,a)-r \right)^{2}+\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}),\]

where the last inequality is a direct implication of Xie et al. (2021a, Lemma A.4). Combining Eqs. (15) and (16) and using the fact of \(V_{\max}\geq 1\), we obtain

\[n\cdot\mathbb{E}_{\mu}\left[D_{\mathrm{TV}}\left(P_{M}(\cdot\mid s,a),P_{M^{\star}}(\cdot\mid s,a)\right)^{2}+\nicefrac{{(R_{M}(s,a)-R^{\star}( s,a))^{2}}}{{V_{\max}^{2}}}\right]\] \[\lesssim\log\ell_{\mathcal{D}}(M^{\star})-\sum_{(s,a,r,s^{\prime })\in\mathcal{D}}\nicefrac{{(R^{\star}(s,a)-r)^{2}}}{{V_{\max}^{2}}}-\log\ell _{\mathcal{D}}(M)+\sum_{(s,a,r,s^{\prime})\in\mathcal{D}}\nicefrac{{(R_{M}(s,a )-r)^{2}}}{{V_{\max}^{2}}}+\log(\nicefrac{{|\mathcal{M}|}}{{\delta}})\] \[=\mathcal{E}_{\mathcal{D}}(M)-\mathcal{E}_{\mathcal{D}}(M^{ \star})+\log(\nicefrac{{|\mathcal{M}|}}{{\delta}}).\]

This completes the proof.

[MISSING_PAGE_FAIL:17]

Due to the unboundedness of the likelihood, we conjecture that naively defining misspecification error using total variation without any accommodation on the MLE loss may be insufficient for the steps above. To resolve that, we may adopt an alternate misspecification definition, e.g., \(\left[\log P_{M^{*}}(s^{\prime}\mid s,a)-\log P_{M^{*}}(s^{\prime}\mid s,a) \right]\leq\varepsilon,\ \forall(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times \mathcal{S}\), or add extra smoothing to the MLE loss with regularization.

Proof of Theorem 3.: \[J(\pi_{\mathsf{ref}})-J(\widehat{\pi}) =J(\pi_{\mathsf{ref}})-J(\pi_{\mathsf{ref}})-[J(\widehat{\pi})-J( \pi_{\mathsf{ref}})]\] \[\leq\ -\min_{M\in\mathcal{M}_{\alpha}}\left[J_{M}(\widehat{\pi})-J _{M}(\pi_{\mathsf{ref}})\right]\] (by Lemma 5, we have \[M^{\star}\in\mathcal{M}_{\alpha}\)) \[=\ -\max_{\pi\in\Pi}\min_{M\in\mathcal{M}_{\alpha}}\left[J_{M}( \pi)-J_{M}(\pi_{\mathsf{ref}})\right]\] (by the optimality of \[\widehat{\pi}\] from Eq. (1)) \[\leq\ -\min_{M\in\mathcal{M}_{\alpha}}\left[J_{M}(\pi_{\mathsf{ref}}) -J_{M}(\pi_{\mathsf{ref}})\right]\] ( \[\pi_{\mathsf{ref}}\in\Pi\] ) \[=0.\]

A misspecified version of Theorem 3 can be derived similarly to what we discussed about that of Theorem 2. If the policy class is also misspecified, where there exists only \(\widetilde{\pi}_{\mathsf{ref}}\in\Pi\) that is close to \(\pi_{\mathsf{ref}}\) up to some misspecification error, the second last step of the proof of Theorem 3 becomes \(-\min_{M\in\mathcal{M}_{\alpha}}\left[J_{M}(\widetilde{\pi}_{\mathsf{ref}})-J _{M}(\pi_{\mathsf{ref}})\right]\leq\mathsf{misspecification}\) error by simply applying the performance difference lemma on the difference between \(\widetilde{\pi}_{\mathsf{ref}}\) and \(\pi_{\mathsf{ref}}\).

## Appendix B Proofs for Section 6

Proof of Lemma 9.: We prove the result by contradiction. First notice \(\min_{M\in\mathcal{M}}J_{M}(\pi^{\prime})-J_{M}(\pi^{\prime})=0\). Suppose there is \(\overline{\pi}\in\Pi\) such that \(\min_{M\in\mathcal{M}_{\alpha}}J_{M}(\bar{\pi})-J_{M}(\pi^{\prime})>0\), which implies that \(J_{M}(\bar{\pi})>J_{M}(\pi^{\prime})\), \(\forall M\in\mathcal{M}_{\alpha}\). Since \(\mathcal{M}\subseteq\mathcal{M}_{\alpha}\), we have

\[\min_{M\in\mathcal{M}}J_{M}(\bar{\pi})+\psi(M)>\min_{M\in\mathcal{M}}J_{M}(\pi ^{\prime})+\psi(M)=\max_{\pi\in\Pi}\min_{M\in\mathcal{M}}J_{M}(\pi)+\psi(M)\]

which is a contradiction of the maximin optimality. Thus \(\max_{\pi\in\Pi}\min_{M\in\mathcal{M}_{\alpha}}J_{M}(\bar{\pi})-J_{M}(\pi^{ \prime})=0\), which means \(\pi^{\prime}\) is a solution.

For the converse statement, suppose \(\pi\) is a fixed point. We can just let \(\psi(M)=-J_{M}(\pi)\). Then this pair of \(\pi\) and \(\psi\) by definition of the fixed point satisfies Eq. (19). 

## Appendix C Related Work

There has been an extensive line of works on reinforcement with offline/batch data, especially for the case with the data distribution is rich enough to capture the state-action distribution for any given policy (Munos, 2003; Antos et al., 2008; Munos and Szepesvari, 2008; Farahmand et al., 2010; Lange et al., 2012; Chen and Jiang, 2019; Liu et al., 2020; Xie and Jiang, 2020, 2021). However, this assumption is not practical since the data distribution is typically restricted by factors such as the quality of available policies, safety concerns, and existing system constraints, leading to narrower coverage. As a result, recent offline RL works in both theoretical and empirical literature have focused on systematically addressing datasets with inadequate coverage.

Modern offline reinforcement learning approaches can be broadly categorized into two groups for the purpose of learning with partial coverage. The first type of approaches rely on behavior regularization, where the learned policy is encouraged to be close to the behavior policy in states where there is insufficient data (e.g., Fujimoto et al., 2018; Laroche et al., 2019; Kumar et al., 2019; Siegel et al., 2020). These algorithms ensure that the learned policy performs at least as well as the behavior policy while striving to improve it when possible, providing a form of safe policy improvement guarantees. These and other studies (Wu et al., 2019; Fujimoto and Gu, 2021; Kostrikov et al., 2021) have provided compelling empirical evidence for the benefits of these approaches.

The second category of approaches that has gained prevalence relies on the concept of _pessimism under uncertainty_ to construct lower-bounds on policy performance without explicitly constraining the policy. Recently, there have been several model-free and model-based algorithms based on this concept that have shown great empirical performance on high dimensional continuous control tasks. Model-free approaches operate by constructing lower bounds on policy performance and then optimizing the policy with respect to this lower bound (Kumar et al., 2020; Kostrikov et al., 2021). The model-based counterparts first learn a world model and the optimize a policy using model-based rollouts via off-the-shelf algorithms such as Natural Policy Gradient (Kakade, 2001) or Soft-Actor Critic (Haarnoja et al., 2018). Pessimism is introduced by either terminating model rollouts using uncertainty estimation from an ensemble of neural network models (Kidambi et al., 2020) or modifying the reward function to penalize visiting uncertain regions (Yu et al., 2020). Yu et al. (2021) propose a hybrid model-based and model-free approach that integrates model-based rollouts into a model-free algorithm to construct tighter lower bounds on policy performance. On the more theoretical side, the offline RL approaches built upon the pessimistic concept (e.g., Liu et al., 2020; Jin et al., 2021; Rashidinejad et al., 2021; Xie et al., 2021; Zanette et al., 2021; Uehara and Sun, 2021; Shi et al., 2022) also illustrate desired theoretical efficacy under various of setups.

Another class of approaches employs an adversarial training framework, where offline RL is posed a two player game between an adversary that chooses the worst-case hypothesis (e.g., a value function or an MDP model) from a hypothesis class, and a policy player that tried to maximize the adversarially chosen hypothesis. Xie et al. (2021) propose the concept of Bellman-consistent pessimism to constrain the class of value functions to be Bellman consistent on the data. Cheng et al. (2022) extend this framework by introducing a relative pessimism objective which allows for robust policy improvement over the data collection policy \(\mu\) for a wide range of hyper-parameters. Our approach can be interpreted as a model-based extension of Cheng et al. (2022). These approaches provide strong theoretical guarantees even with general function approximators while making minimal assumptions about the function class (realizability and Bellman completeness). Chen et al. (2022) provide an adversarial model learning method that uses an adversarial policy to generate a data-distribution where the model performs poorly and iteratively updating the model on the generated distribution. There also exist model-based approaches based on the same principle (Uehara and Sun, 2021; Rigter et al., 2022) for optimizing the absolute performance. Of these, Rigter et al. (2022) is the closest to our approach, as they also aim to find an adversarial MDP model that minimizes policy performance. They use a policy gradient approach to train the model, and demonstrate great empirical performance. However, their approach is based on absolute pessimism and does not enjoy the same RPI property as ARMOR.

## Appendix D A Deeper Discussion of Robust Policy Improvement

### How to formally define RPI?

Improving over some reference policy has been long studied in the literature. To highlight the advantage of ARMOR, we formally give the definition of different policy improvement properties.

**Definition 2** (Robust policy improvement).: _Suppose \(\widehat{\pi}\) is the learned policy from an algorithm. We say the algorithm has the policy improvement (PI) guarantee if \(J(\pi_{\mathsf{ref}})-J(\widehat{\pi})\leq\nicefrac{{o(N)}}{{N}}\) is guaranteed for some reference policy \(\pi_{\mathsf{ref}}\) with offline data \(\mathcal{D}\sim\mu\), where \(N=|\mathcal{D}|\). We use the following two criteria w.r.t. \(\pi_{\mathsf{ref}}\) and \(\mu\) to define different kinds PI:_

1. _[label=()]_
2. _The PI is strong, if_ \(\pi_{\mathsf{ref}}\) _can be selected arbitrarily from policy class_ \(\Pi\) _regardless of the choice data-collection policy_ \(\mu\)_; otherwise, PI is weak (i.e.,_ \(\pi_{\mathsf{ref}}\equiv\mu\) _is required)._
3. _The PI is robust if it can be achieved by a range of hyperparameters with a known subset._

Weak policy improvement is also known as _safe policy improvement_ in the literature (Fujimoto et al., 2019; Laroche et al., 2019). It requires the reference policy to be also the behavior policy that collects the offline data. In comparison, strong policy improvement imposes a stricter requirement, which requires policy improvement _regardless_ of how the data were collected. This condition is motivated by the common situation where the reference policy is not the data collection policy. Finally, since we are learning policies offline, without online interactions, it is not straightforward to tune the hyperparameter directly. Therefore, it is desirable that we can design algorithms with these properties in a robust manner in terms of hyperparameter selection. Formally, Definition 2 requires the policy improvement to be achievable by a set of hyperparameters that is known before learning.

Theorem 3 indicates the robust strong policy improvement of ARMOR. On the other hand, algorithms with robust weak policy improvement are available in the literature (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021; Cheng et al., 2022); this is usually achieved by designing the algorithm to behave like IL for a known set of hyperparameter (e.g., behavior regularization algorithms have a weight that can turn off the RL behavior and regress to IL). However, deriving guarantees of achieving the best data-covered policy of the IL-like algorithm is challenging due to its imitating nature. To our best knowledge, ATAC (Cheng et al., 2022) is the only algorithm that achieves both robust (weak) policy improvement as well as guarantees absolute performance.

### When RPI actually improves?

Given ARMOR's ability to improve over an arbitrary policy, the following questions naturally arise: _Can ARMOR nontrivially improve the output policy of other algorithms (e.g., such as those based on absolute pessimism (Xie et al., 2021)), including itself?_ Note that outputting \(\pi_{\mathsf{ref}}\) itself always satisfies RPI, but such result is trivial. By "nontrivially" we mean a non-zero worst-case improvement. If the statement were true, we would be able to repeatedly run ARMOR to improve over itself and then obtain the _best_ policy any algorithm can learn offline.

Unfortunately, the answer is negative. Not only ARMOR cannot improve over itself, but it also cannot improve over a variety of algorithms. In fact, the optimal policy of an _arbitrary_ model in the version space is unimprovable (see Corollary 10)! Our discussion reveals some interesting observations (e.g., how equivalent performance metrics for online RL can behave very differently in the offline setting) and their implications (e.g., how we should choose \(\pi_{\mathsf{ref}}\) for ARMOR). Despite their simplicity, we feel that many in the offline RL community are not actively aware of these facts (and the unawareness has led to some confusion), which we hope to clarify below.

SetupWe consider an abstract setup where the learner is given a version space \(\mathcal{M}_{\alpha}\) that contains the true model and needs to choose a policy \(\pi\in\Pi\) based on \(\mathcal{M}_{\alpha}\). We use the same notation \(\mathcal{M}_{\alpha}\) as before, but emphasize that it does not have to be constructed as in Eqs.2 and 3. In fact, for the purpose of this discussion, the data distribution, sample size, data randomness, and estimation procedure for constructing \(\mathcal{M}_{\alpha}\) are **all irrelevant**, as our focus here is how decisions should be made with a given \(\mathcal{M}_{\alpha}\). This makes our setup very generic and the conclusions widely applicable.

To facilitate discussion, we define the _fixed point_ of ARMOR's relative pessimism step:

**Definition 3**.: _Consider Eq.1 as an operator that maps an arbitrary policy \(\pi_{\mathsf{ref}}\) to \(\widehat{\pi}\). A fixed point of this relative pessimism operator is, therefore, any policy \(\pi\in\Pi\) such that \(\pi\in\operatorname*{argmax}_{\pi^{\prime}\in\Pi}\min_{M\in\mathcal{M}_{\alpha }}J_{M}(\pi^{\prime})-J_{M}(\pi)\)._

Given the definition, relative pessimism cannot improve over a policy if it is already a fixed point. Below we show a sufficient and necessary condition for being a fixed point, and show a number of concrete examples (some of which may be surprising) that are fixed points and thus unimprovable.

**Lemma 9** (Fixed-point Lemma).: _For any \(\mathcal{M}\subseteq\mathcal{M}_{\alpha}\) and any \(\psi:\mathcal{M}\to\mathbb{R}\), consider the policy_

\[\pi\in\operatorname*{argmax}_{\pi^{\prime}\in\Pi}\min_{M\in\mathcal{M}}J_{M}( \pi^{\prime})+\psi(M) \tag{19}\]

_Then \(\pi\) is a fixed point in Definition3. Conversely, for any fixed point \(\pi\) in Definition3, there is a \(\psi:\mathcal{M}\to\mathbb{R}\) such that \(\pi\) is a solution to Eq.19._

**Corollary 10**.: _The following are fixed points of relative pessimism (Definition3):_

1. _Absolute-pessimism policy, i.e.,_ \(\psi(M)=0\)_._
2. _Relative-pessimism policy for any reference policy, i.e.,_ \(\psi(M)=-J_{M}(\pi_{\mathsf{ref}})\)_._
3. _Regret-minimization policy, i.e.,_ \(\psi(M)=-J_{M}(\pi_{M}^{\star})\)_, where_ \(\pi_{M}^{\star}\in\operatorname*{argmax}_{\pi\in\Pi}J_{M}(\pi)\)_._
4. _Optimal policy of an_ arbitrary _model_ \(M\in\mathcal{M}_{\alpha}\)_,_ \(\pi_{M}^{\star}\)_, i.e.,_ \(\mathcal{M}=\{M\}\)_. This would include the optimistic policy, that is,_ \(\operatorname*{argmax}_{\pi\in\Pi,M\in\mathcal{M}_{\alpha}}J_{M}(\pi)\)__

Return maximization and regret minimization are _different_ in offline RLWe first note that these four examples generally produce different policies, even though some of them optimize for objectives that are traditionally viewed as equivalent in online RL (the "worst-case over \(\mathcal{M}_{\alpha}\)" part of the definition does not matter in online RL), e.g., absolute pessimism optimizes for \(J_{M}(\pi)\), which is the same as minimizing the regret \(J_{M}(\pi_{M}^{*})-J_{M}(\pi)\) for a fixed \(M\). However, their equivalence in online RL relies on the fact that online exploration can eventually resolve any model uncertainty when needed, so we only need to consider the performance metrics w.r.t. the true model \(M=M^{*}\). In offline RL with an arbitrary data distribution (since we do not make any coverage assumptions), there will generally be model uncertainty that cannot be resolved, and worst-case reasoning over such model uncertainty (i.e., \(\mathcal{M}_{\alpha}\)) separates apart the definitions that are once equivalent.

Moreover, it is impossible to compare return maximization and regret minimization and make a claim about which one is better. They are not simply an algorithm design choice, but are definitions of the learning goals and the guarantees themselves--thus incomparable: if we care about obtaining a guarantee for the worst-case _return_, the return maximization is optimal by definition; if we are more interested in obtaining a guarantee for the worst-case _regret_, then again, regret minimization is trivially optimal. We also note that analyzing algorithms under a metric that is different from the one they are designed for can lead to unusual conclusions. For example, Xiao et al. (2021) show that optimistic/neutral/pessimistic algorithms9 are equally minimax-optimal in terms of their regret guarantees in offline multi-armed bandits. However, the algorithms they consider are optimistic/pessimistic w.r.t. the return--as commonly considered in the offline RL literature--not w.r.t. the regret which is the performance metric they are interested in analyzing.

Footnote 9: Incidentally, optimistic/neutral policies correspond to #4 in Corollary 10.

\(\pi_{\text{ref}}\) **is more than a hyperparameter--it defines the performance metric and learning goal**Corollary 10 shows that ARMOR (with relative pessimism) has many different fixed points, some of which may seem quite unreasonable for offline learning, such as greedy w.r.t. an arbitrary model or even optimism (#4). From the above discussion, we can see that this is not a defect of the algorithm. Rather, in the offline setting with unresolvable model uncertainty, there are many different performance metrics/learning goals that are generally incompatible/incomparable with each other, and the agent designer must make a choice among them and convey the choice to the algorithm. In ARMOR, such a choice is explicitly conveyed by the choice of \(\pi_{\text{ref}}\), which subsumes return maximization and regret minimization as special cases (#2 and #3 in Corollary 10)

## Appendix E A More Comprehensive Toy Example for RPI

We illustrate with a simple toy example why ARMOR intuitively demonstrates the RPI property even when \(\pi_{\text{ref}}\) is not covered by the data \(\mathcal{D}\). ARMOR achieves this by _1)_ learning an MDP Model, and _2)_ adversarially training this MDP model to minimize the relative performance difference to \(\pi_{\text{ref}}\) during policy optimization. Consider a one-dimensional discrete MDP with five possible states as shown in Figure 4. The dynamics is deterministic, and the agent always starts in the center cell. The agent receives a lower reward of 0.1 in the left-most state and a high reward of 1.0 upon visiting the right-most state. Say, the agent only has access to a dataset from a sub-optimal policy that always takes the left action to receive the 0.1 reward. Further, let's say we have access to a reference policy that demonstrates optimal behavior on the true MDP by always choosing the right action to visit the right-most state. However, it is unknown a priori that the reference policy is optimal. In such a case, typical offline RL methods can only recover the sub-optimal policy from the dataset as it is the best-covered policy in the data. Now, for the sake of clarity, consider the current learner policy is same as the behavior policy, i.e it always takes the left action.

ARMOR can learn to recover the expert reference policy in this example by performing rollouts with the adversarially trained MDP model. From the realizability assumption we know that the version space of models contains the true model (i.e., \(M^{*}\in\mathcal{M}_{\alpha}\)). The adversary can then choose a model from this version space where the reference policy \(\pi_{\text{ref}}\) maximally outperforms the learner. Note, that ARMOR does not require the true reward function to be known. In this toy example, the model selected by the adversary would be the one that not only allows the expert policy to reach the right-most state, but also predicts the highest reward for doing so. Now, optimizing to maximize relative performance difference with respect to this model will ensure that the learner can recover the expert behavior, since the only way for the learner to stay competitive with the reference policy is to mimic the reference policy in the region outside data support. In other words, the reason why ARMOR has RPI to \(\pi_{\text{ref}}\) is that its adversarial model training procedure can augment the original offline data with new states and actions that would cover those generated by running the reference policy in the true environment, even though ARMOR does not have knowledge of \(M^{*}\).

## Appendix F Further Experimental Details

### Experimental Setup and Hyper-parameters

We represent our policy \(\pi\), Q-functions \(f_{1},f_{2}\) and MDP model \(M\) as standard fully connected neural networks. The policy is parameterized as a Gaussian with a state-dependent covariance, and we use a tanh transform to limit the actions to the action space bound similar to Haarnoja et al. (2018). The MDP model learns to predict the next state distribution, rewards and terminal states, where the reward and next-state distributions part are parameterized as Gaussians with state-dependent covariances. The model fitting loss consists of negative log-likelihood for the next-state and reward and binary cross entropy for the terminal flags. In all our experiments we use the same model architecture and a fixed value of \(\lambda\). We use Adam optimizer (Kingma and Ba, 2015) with fixed learning rates \(\eta_{fast}\) and \(\eta_{slow}\) similar to Cheng et al. (2022). Also similar to prior work (Kidambi et al., 2020), we let the MDP model network predict delta differences to the current state. The rollout horizon is always set to the maximum episode steps per environment. A complete list of hyper-parameters can be found in Table 3.

**Compute:** Each run of ARMOR has access to 4CPUs with 28GB RAM and a single Nvidia T4 GPU with 16GB memory. With these resources each run tasks around 6-7 hours to complete. Including all runs for 4 seeds, and ablations this amounts to approximately 2500 hours of GPU compute.

Figure 4: A toy MDP illustrating the RPI property of ARMOR. (Top) The true MDP has deterministic dynamics where taking the left (\(a_{l}\)) or right (\(a_{r}\)) actions takes the agent to corresponding states; start state is in yellow. The suboptimal behavior policy only visits only the left part of the state space, and the reference policy demonstrates optimal behavior by always choosing \(a_{r}\). (Bottom) A subset of possible data-consistent MDP models (dynamics + rewards) in the version space. The adversary always chooses the MDP that makes the reference maximally outperform the learner. In response, the learner will learn to mimic the reference outside data support to be competitive.

### Detailed Performance Comparison and RPI Ablations

In Table 4 we show the performance of ARMOR compared to model-free and model-based offline RL baselines with associate standard deviations over 8 seeds. For ablation, here we also include ARMOR\({}^{\dagger}\), which is running ARMOR in Algorithm 1 but without the model optimizing for the Bellman error (that is, the model is not adversarial). Although ARMOR\({}^{\dagger}\) does not have any theoretical guarantees (and indeed in the worst case its performance can be arbitrarily bad), we found that ARMOR\({}^{\dagger}\) in these experiments is performing surprisingly well. Compared with ARMOR, ARMOR\({}^{\dagger}\) has less stable performance when the dataset is diverse (e.g. _-med-replay_ datasets) and larger learning variance. Nonetheless, ARMOR\({}^{\dagger}\) using a single model is already pretty competitive with other algorithms. We conjecture that this is due to that Algorithm 1 also benefits from pessimism due to adversarially trained critics. Since the model buffer would not cover all states and actions (they are continuous in these problems), the adversarially trained critic still controls the pessimism for actions not in the model buffer, as a state guard. As a result, the algorithm can tolerate the model quality more.

### Effect of Residual Policy

In Figure 5, we show the effect on RPI of different schemes for initializing the learner for several D4RL datasets. Specifically, we compare using a residual policy( Section 5) versus behavior cloning the reference policy on the provided offline dataset for learner initialization. Note that this offline dataset is the suboptimal one used in offline RL and is different from the expert-level dataset used to train and produce the reference policy. We observe that using a residual policy (purple) consistently shows RPI across all datasets. However, with behavior cloning initialization (pink), there is a large variation in performance across datasets. While RPI is achieved with behavior cloning initialization on _hopper_, _walker2d_ and _hammer_ datasets, performance can be arbitrarily bad compared to the reference on other problems. As an ablation, we also study the effect of using a residual policy in

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline Dataset & ARMOR & ARMOR\({}^{\dagger}\) & ARMOR\({}^{\dagger}\) & ARMOR\({}^{\dagger}\) & MagellL & MOPO & RAMHO & COMBO & AITAC & COL & RQI. & IBC \\ \hline \hline hopper-med & **101.4 + 0.3** & **106.4 + 1.7** & 53.3 + 2.8 & **95.4** & 28.0 + 12.4 & **92.8 + 6.8** & **97.2 + 12.5** & 85.6 & 66.3 & 29.0 \\ walker2d-med & **90.7 + 4.4** & **91.0 + 10.4** & 79.0 + 2.2 & 77.8 & 17.8 + 19.3 & **86.9 + 27the offline RL case where no explicit reference is provided, and the behavior cloning policy is used as the reference similar to Section 5.1. We include the results in Table 4 as ARMOR\({}^{re}\), where we observe that using a residual policy overall leads to worse performance across all datasets. This lends evidence to the fact that using a residual policy is a _compromise_ in instances where initializing the learner exactly to the reference policy is not possible.

### Connection to Imitation Learning

As mentioned in Section 4.2, IL is a special case of ARMOR with \(\lambda=0\). In this setting, the Q-function can fully affect the adversarial MDP model, so the best strategy of the policy is to mimic the reference. We test this on the _expert_ versions of the D4RL locomotion tasks in Table 5, and observe that ARMOR can indeed perform IL to match expert performance.

### Ablation Study: RPI for Different Reference Policies

Here we provide ablation study results for robust policy improvement under different reference policies for a wide range of \(\beta\) values (pessimism hyper-parameter). For all the considered reference policies we present average normalized scores for ARMOR and reference (REF) over multiple random seeds, and observe that ARMOR can consistently outperform the reference for a large range of \(\beta\) values.

**Random Dataset Reference** We use a reference policy obtained by running behavior cloning on the random versions of different datasets. This is equivalent to using a randomly initialized neural network as the reference.

Figure 5: Comparison of different policy initializations for RPI with varying pessimism hyper-parameter \(\beta\). ORL denotes the performance of offline RL with ARMOR ( Table 1), and REF is the performance of reference policy. Purple represents residual policy initialization and pink is initialization using behavior cloning of the reference on the suboptimal offline RL dataset.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Dataset & ARMOR-IL & BC \\ \hline hopper-exp & 111.6 & 111.7 \\ walker2d-exp & 108.1 & 108.5 \\ halfcheetah-exp & 93.9 & 94.7 \\ \hline \end{tabular}
\end{table}
Table 5: ARMOR-IL on expert datasets. By setting \(\lambda=0\), \(\beta>0\) we recover IL.

[MISSING_PAGE_FAIL:25]