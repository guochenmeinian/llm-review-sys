# Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking

 Xi Chen\({}^{1}\), Chuan Qin\({}^{2}\), Chuyu Fang\({}^{3}\), Chao Wang\({}^{1}\),

**Chen Zhu\({}^{1}\)**, **Fuzhen Zhuang\({}^{4,5}\)**, **Hengshu Zhu\({}^{2}\)\({}^{\dagger}\)**, **Hui Xiong\({}^{6,7}\)**

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Computer Network Information Center, Chinese Academy of Sciences

\({}^{3}\)Baidu Inc. \({}^{4}\)Institute of Artificial Intelligence, Beihang University

\({}^{5}\)SKLSDE, School of Computer Science, Beihang University

\({}^{6}\)AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)

\({}^{7}\)Department of Computer Science and Engineering,

The HongKong University of Science and Technology, Hong Kong SAR

chenxi0401@mail.ustc.edu.cn,

{chuanqin0426, fangchuyu2022, chadwang2012, zc3930155}@gmail.com,

zhuangfuzhen@buaa.edu.cn, zhuhengshu@gmail.com, xionghui@ust.hk

Equal contributionsCorresponding Authors

###### Abstract

In a rapidly evolving job market, skill demand forecasting is crucial as it enables policymakers and businesses to anticipate and adapt to changes, ensuring that workforce skills align with market needs, thereby enhancing productivity and competitiveness. Additionally, by identifying emerging skill requirements, it directs individuals towards relevant training and education opportunities, promoting continuous self-learning and development. However, the absence of comprehensive datasets presents a significant challenge, impeding research and the advancement of this field. To bridge this gap, we present Job-SDF, a dataset designed to train and benchmark job-skill demand forecasting models. Based on millions of public job advertisements collected from online recruitment platforms, this dataset encompasses monthly recruitment demand. Our dataset uniquely enables evaluating skill demand forecasting models at various granularities, including occupation, company, and regional levels. We benchmark a range of models on this dataset, evaluating their performance in standard scenarios, in predictions focused on lower value ranges, and in the presence of structural breaks, providing new insights for further research. Our code and dataset are publicly accessible via the https://github.com/Job-SDF/benchmark.

## 1 Introduction

Job skills encompass a range of abilities and competencies essential for performing tasks effectively in the workplace. These skills are broadly categorized into hard skills, such as technical and analytical abilities, and soft skills, including communication, teamwork, and adaptability [1]. Accurate forecasting of skill demand helps businesses and policymakers anticipate and address skill shortages and mismatches, and promotes skill development in high-demand areas, thereby supporting economic growth and stability [2; 3]. By identifying emerging skill requirements, individuals are directed towards relevant training and education opportunities, fostering continuous self-learning and development to stay competitive in the labor market [4; 5; 6; 7; 8; 9; 10]. By tracking skill demand trends,employers gain deeper insight into recruits' priorities, enhancing person-job fit. [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21]. Moreover, forecasting informs educational and training programs, ensuring that curricula align with the labor market's evolving needs [22; 23; 24].

Traditionally, skill demand analysis has relied on labor-intensive, survey-based methods limited to specific companies or occupations [25; 26; 27]. However, over the past decade, the rapid evolution of the internet has spurred the emergence of online recruitment platforms. These platforms have become the primary channels for job advertisements for numerous enterprises and organizations, accumulating vast amounts of job advertisement data. By leveraging this data, researchers have formulated skill demand forecasting as a time series task, utilizing various machine learning models such as autoregressive integrated moving average (ARIMA) [28], recurrent neural networks (RNNs) [29], and dynamic graph autoencoders (DyGAEs) [30], to predict future skill needs.

A major challenge impeding progress in this field is the lack of comprehensive and publicly accessible datasets. Existing studies do not provide open-source datasets, making it difficult for researchers to replicate experimental results and identify bottlenecks in current research. Furthermore, these datasets primarily focus on predicting skill demand variations across different occupations, with a notable lack of modeling and prediction at other granularities, such as companies or regions. This limitation hinders comprehensive comparisons between different models and impedes the exploration of potential downstream applications, such as human capital strategy development and regional policy formulation. Additionally, the significant variations in skill demand present further challenges. Existing studies, which rely on metrics such as Mean Squared Error (MSE), struggle to evaluate the performance of skill demand forecasting models for low-frequency skill terms. For instance, some emerging skills, such as large language models (LLMs), may initially show low demand but are crucial for the job market due to their potential to reshape existing occupations.

To this end, in this paper, we propose Job-SDF, a multi-granularity dataset designed for job skill demand forecasting research. Specifically, we collected millions of public job advertisements from online recruitment platforms. By extracting skill terms from job advertisement texts, we quantified the monthly skill demand at various granularities, including occupations, companies, and regions, to construct our dataset. This dataset encompasses 2,324 types of skills, 52 occupations, 521 companies, and 7 regions. We then use the Job-SDF dataset to benchmark a wide range of models for job skill demand forecasting tasks at various granularities. These models include statistical time series models (e.g., ARIMA [31]), deep learning-based methods such as RNN-based models [32; 30], Transformer-based models [33; 34; 35; 36; 37], MLP-based models [38; 39], as well as several state-of-the-art time-series forecasters [40; 41]. Performance is evaluated using regression metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Additionally, we use the Symmetric Mean Absolute Percentage Error (SMAPE) [42] and Relative Root Mean Squared Error (RRMSE) [43] metrics to account for the significantly varying nature of skill demand values, which is particularly useful for evaluating model performance in predicting lower value ranges. Moreover, we further investigate the impact of structural breaks in job skill demand time series data on model performance. The Job-SDF dataset, along with data loaders, example codes for different models, and evaluation setup, are publicly available in our GitHub repository: https://github.com/Job-SDF/benchmark.

## 2 Related Work

Skill demand forecasting can analyze how skills evolve over time, aiding experts in evaluating technological advancements [44; 45; 46], assessing wage inequality [47; 48; 49], and generating employment opportunities [50]. Furthermore, the skills required in the 21st-century workplace will differ significantly from those in previous eras [51]. Predicting skill demands benefits personal career transitions and corporate management strategies.

Recently, with the rapid accumulation of data and continuous advancements in technology, skill demand forecasting has demonstrated significant vitality. _Das et al._ proposed a method for dynamic task allocation to investigate the evolution of job task requirements over a decade of AI innovation across different salary levels [28]. Given the effectiveness of RNN in multi-step prediction, some researchers have integrated skill demand forecasting with RNN algorithms, achieving promising results [29; 32]. In addition, considering the supply-demand dynamics of the labor market concurrently, CHGH designed a joint prediction model based on the encoder-decoder architecture to achieve trend prediction for both skill supply and demand sides [30]. Moreover, to capture the dynamic information of occupations, a pretraining-enhanced dynamic graph autoencoder has been developed to efficiently forecast skill demand at the occupational granularity [52].

However, the predominance of closed-source datasets has significantly elevated the barrier of researchers and constrained the pace of methodological advancements. While open-source skill-related datasets such as O*NET [53] and ESCO [54] provide skill taxonomies, they do not quantify skill demand. Furthermore, the current research data focuses either on macro-market skill demand predictions or analyses at a specific granularity, neglecting multi-level labor market analysis. This limitation generally hampers the transferability of the modeling approaches.

## 3 Job-SDF Dataset

The Job-SDF dataset is built from job advertisements collected on online recruitment platforms, encompassing dynamic job skill demand time series data at various granularities, recorded monthly. The dataset is CC BY-NC-SA 4.0 licensed, accessible via the URL https://github.com/Job-SDF/benchmark. We summarize the dataset construction process, task description, and dataset analysis below, with supplementary details provided in the Appendix F.

### Data Collection and Processing

**Job Advertisement Collection.** We collected public job advertisements for 52 occupations from 521 companies on online recruitment platforms. We obtained unique records after removing identical job advertisements posted simultaneously by different companies on various platforms. Each record contains five types of information: (1) _Job Requirement_, which is a text segment that outlines the specific skills required of candidates applying for the job; (2) _Company_, which identifies the company that posted the job advertisement; (3) _Occupation_, which specifies the job advertisement's category. Our dataset encompasses 52 detailed occupations (L2-level), such as front-end development engineer and financial investment analyst. Additionally, these 52 occupations are grouped into 14 broader categories (L1-level); (4) _Region_, which indicates the primary geographic divisions in China where the job postings are located. These regions are classified based on their geographical orientation; (5) _Posting Time_, which records the date when the job was posted, including the year, month, and day.

**Job Skill Extraction.** After acquiring the job advertisement data, we utilized a Named Entity Recognition (NER) model, as referenced in [55, 56, 57, 58], to explicitly extract skill requirements from the _Job Requirement_ of each advertisement. Specifically, we first annotated a dataset for training the NER model by identifying skill terms within the job requirement texts. To achieve this, we devised a set of regular expressions tailored to the characteristics of skill descriptions and used these to match skill words in job advertisements. Subsequently, we merged all matched skill words to formulate a raw skill dictionary, including their corresponding frequencies across job advertisements. We then filtered out low-frequency words and manually annotated the raw skill dictionary to create a refined skill dictionary. Along this line, we excluded unreasonable skill words matched by the regular expressions that did not appear in the refined skill dictionary, establishing an initial correspondence between the _Job Requirement_ and the skill requirements.

Based on this annotated data, we trained an NER model to extract required skills from the _Job Requirement_ section for all job advertisements. Experts then aggregated the skills extracted by the NER model based on their meaning and content, grouping together those with similar meanings or repeated expressions. This process resulted in a skill dictionary \(\mathcal{S}\) of 2,324 standardized skill words, mapping original skill word descriptions to standardized skill words. The skill dictionary was then used to filter and map the skill words extracted by the NER model, ultimately obtaining standardized skill requirements for each job requirement. These standardized requirements were added to the job advertisement data as a new field, _Skill Requirements_.

**Job Skill Demand Estimation.** Generally, the demand for different skills in the job market can be estimated by the volume of job advertisements listing these specific skills as requirements within a given time period [30]. Formally, given job advertisement data \(\mathcal{P}=\{\mathcal{P}_{1},...,\mathcal{P}_{t},...,\mathcal{P}_{T}\}\), where each \(\mathcal{P}_{t}\) represents the job advertisements posted at timestamp \(t\), we use \(D_{s,t}=\sum_{p\in\mathcal{P}_{t}}\mathbf{1}(s\in p)\) to estimate the demand for skill \(s\in\mathcal{S}\) at time \(t\). \(s\in p\) indicates that job advertisement \(p\) requires skill \(s\).

Along this line, we can calculate skill demand at various granularities, such as occupation and company levels. We define the sets of L1-level occupations, L2-level occupations, companies, and regions as \(\mathcal{A}^{o_{1}}\), \(\mathcal{A}^{o_{2}}\), \(\mathcal{A}^{c}\), and \(\mathcal{A}^{r}\), respectively. The demands for skill \(s\) at time \(t\) under granularity \(i\in\{o_{1},o_{2},c,r\}\) is then defined as follows:

\[D^{i}_{s,t}=[D^{i}_{s,t,a^{i}}]_{a^{i}\in\mathcal{A}^{i}},\;\;D^{i}_{s,t,a^{i}}= \sum_{p\in\mathcal{P}_{t}}\mathbf{1}(s\in p)\cdot\mathbf{1}(a^{i}\in p),\] (1)

where \(a^{i}\in p\) represents a job advertisement \(p\) containing the attribute \(a^{i}\) under granularity \(i\). Similarly, we can further define skill demands \(D^{i,j,...,k}_{s,t}\) across multiple granularities \(\{i,j,...,k\}\) by calculating:

\[D^{i,j,...,k}_{s,t,\overline{a}}=\sum_{p\in\mathcal{P}_{t}}\mathbf{1}(s\in p )\cdot\mathbf{1}(a^{i}\in p\wedge a^{j}\in p\wedge...\wedge a^{k}\in p),\] (2)

where \(\overline{a}=\{a^{i},a^{j},...,a^{k}\}\), \(a^{i}\in\mathcal{A}^{i},a^{j}\in\mathcal{A}^{j},...,a^{k}\in\mathcal{A}^{k}\), and \(D^{i,j,...,k}_{s,t}\in\mathbb{R}^{|\mathcal{A}^{i}||\mathcal{A}^{j}|...| \mathcal{A}^{k}|}\).

### Job Skill Demand Forecasting Tasks

We study model performance through job skill demand forecasting tasks at different granularities, including single and multiple levels. The primary goal of these tasks is to predict future job skill demands based on historical time series data of various skills. Formally, we have:

**Definition 1** (Job Skill Demand Forecasting): _Given a granularity or a set of granularities \(g\) and the observed job skill demand series from the previous \(K\) timestamps, i.e., \(\{D^{g}_{:,t-K+1},...,D^{g}_{:,t}\}\), the goal of job skill demand forecasting is to learn a forecasting model \(\mathcal{M}\) to predict the demand values for the next \(H\) timestamps, denoted by \(\{\hat{D}^{g}_{:,t+1},\ldots,\hat{D}^{g}_{:,t+H}\}\)._

Our dataset includes skill demand time series data for L1-level occupations, L2-level occupations, companies, regions, and their combinations. We follow a standard protocol [59] that categorizes all time-series data into training, validation, and test sets in chronological order with a ratio of 9:1:2. In the main text, we demonstrate results with \(K\) set to 6 months and consider \(H\) as 3 months to evaluate the performance of different forecasting models. More settings and results can be found in the Appendix D and our project repository. Based on the Job-SDF dataset, other researchers can easily adjust the parameters to suit their research objectives.

### Dataset Analysis

**Varying Nature of Skill Demand.** The values of skill demand exhibit significant differences and generally follow a long-tail distribution. This indicates that, at a specific granularity, only a few skills have high demand, while a wide range of skills are required by a limited number of jobs. For instance,

Figure 1: Data analysis on Job-SDF. (a) illustrates the long-tail phenomenon of skill demands under the product manager and doctor occupations. (b) illustrates the results under the Chow test for the absence (left) and presence (right) of structural breaks.

[MISSING_PAGE_FAIL:5]

69]. Notably, _LSTM_ have demonstrated their effectiveness in predicting changes in skill shares over time [29]. However, conventional RNNs often encounter performance degradation when handling excessively long look-back windows and forecast horizons. To address this challenge, _SegRNN_[32] introduces segment-wise iterations, which reduce the recurrence count within RNNs, thereby significantly enhancing performance in time series forecasting tasks.

**Transformer-based Model.** Recently, Transformer-based models [70] have gained widespread recognition in long-term time series forecasting due to their global modeling capabilities. Leveraging the attention mechanism, _Reformer_[37] introduces locally sensitive hashing to approximate attention by grouping similar queries. _Informer_[33] incorporates low-rank matrices in self-attention mechanisms to accelerate computation. _Autoformer_[34] employs block decomposition and autocorrelation mechanisms to more effectively capture the intrinsic features of time series data. _FedFormer_[36] utilizes DFT-based frequency-enhanced attention, obtaining attentive weights through the spectrums of queries and keys and calculating the weighted sum in the frequency domain. To address the challenges of non-stationary time series, the _Non-stationary Transformer (NSTransformer)_[35] introduces a sequence stabilization module and proposes a de-stationary attention mechanism. Additionally, _PatchTST_[71] is a channel-independent patch time series transformer model that features patching and channel-independence as its key design elements.

**MLP-based Model.** Multiple Layer Projection (MLP) has been introduced in time series forecasting, demonstrating superior performance compared to transformer-based models in both accuracy and efficiency [38]. Specifically, _DLinear_[38] uses series decomposition as a pre-processing step before linear regression. _FreTS_[72] explores a novel approach by applying MLPs in the frequency domain for time series forecasting. _TSMixer_[39] employs MLPMixer blocks, segments input time series into fixed windows, and applies gated MLP transformations and permutations to enhance accuracy.

**Graph-based Models.** Graph Neural Networks (GNNs) can learn non-Euclidean relationships, making them effective for identifying associations in structured data and generating joint representations from different perspectives [73, 74, 75, 76]. CHGH [30] uses an adaptive graph enhanced by skill co-occurrence relationships to link skill supply and demand sequences. This fusion of representations across views improves the performance of joint skill supply and demand prediction tasks. Pre-DyGAE [52] targets skill demand prediction from an occupational perspective. It builds an occupation-skill bipartite graph based on the skill demands of occupations and captures the dynamic changes in these relationships. This method allows for predicting both potential occupational skills and skill demands, leveraging a dynamic graph perspective.

**Fourier-based Models.** By utilizing Fourier projection, _FiLM_[40] not only captures long-term time dependencies but also effectively reduces noise in forecasting. To address the challenge of non-stationary time-series forecasting, _Koopa_[41] disentangles time-variant and time-invariant components from complex non-stationary series using a Fourier Filter and designs the Koopman Predictor to forecast dynamics.

### Evaluation Metrics

To evaluate the performance of various benchmark models in job skill demand forecasting tasks, we selected two commonly used regression metrics: MAE and RMSE. MAE is calculated over \(H\) observations using the formula: \(\frac{1}{H}\sum_{i=1}^{H}\left|y_{i}-\hat{y}_{i}\right|\), where \(y_{i}\) represents the ground truth value and \(\hat{y}_{i}\) is the predicted value. RMSE is calculated as: \(\sqrt{\frac{1}{H}\sum_{i=1}^{H}\left(y_{i}-\hat{y}_{i}\right)^{2}}\). Both MAE and RMSE are scale-dependent metrics, which makes them unsuitable for comparison across different granularities. Additionally, these metrics are less sensitive to prediction errors at lower skill demand values. Therefore, we additionally applied SMAPE [42] and RRMSE [77] to assess the performance of various forecasting models. SMAPE considers both the magnitude and direction of errors, making it suitable for comparing forecasts across different scales. RRMSE measures the square root of the average of the squared percentage errors.

\[SMAPE=\frac{2}{H}*\sum_{i=1}^{H}\frac{\left|y_{i}-\hat{y}_{i}\right|}{\left|y_ {i}\right|+\left|\hat{y}_{i}\right|},\ \ RRMSE=\sqrt{\frac{\frac{1}{H}\sum_{i=1}^{H}\left(y_{i}-\hat{y}_{i} \right)^{2}}{\sum_{i=1}^{H}\left(\hat{y}_{i}\right)^{2}}}.\] (3)

### Benchmark Results

**Overall Performance.** In Table 1, we present the performance of various models evaluated using two metrics: MAE and RMSE. The following conclusions can be drawn: (1) The traditional statistical method, Prophet, demonstrates relatively poor predictive performance. This may be due to seasonal and holiday factors not being the primary influencers in skill demand prediction. (2) Most Transformer-based models, including Transformer, Autoformer, Informer, and Reformer, exhibit subpar overall predictive performance. This is likely because these models are designed to address long-range temporal dependencies, which are not well-suited for the current shorter time series context. (3) In contrast, PatchTST, unlike these Transformer-based models that perform point-wise modeling of time series, segments the time series into patches and inputs them into the Transformer. This allows the model to focus on more local information. A similar idea is also employed in the SegRNN. This strategy significantly enhances the performance of these models in predicting job skill demand. (4) The performance of different linear models on our dataset varies significantly. For instance, DLinear outperforms most Transformer-based models, while TSMixer performs poorly. This discrepancy may be due to the tendency of more complex MLP-based models to overfit our dataset. (5) CHGH and Pre-DyGAE exhibit poor performance in the separate skill demand forecasting scenario, likely due to a mismatch between their model design and the context of our dataset. Specifically, CHGH relies on sequential data from the supply side of skills, which is lacking in our dataset. Conversely, Pre-DyGAE focuses more on predicting whether a skill will be required by an occupation in the future. (6) Finally, FiLM achieved the best performance in most cases, demonstrating the robustness of the denoising-based model.

**Low-Demand Skill Prediction Performance.** Considering the varying nature of skill demand values, we further employed SMAPE and RRMSE metrics to focus on the predictive performance of different models for low-demand skills. As shown in Table 2, the experimental results indicate the following: (1) PatchTST achieved the best SMAPE performance in most cases, validating its ability to more accurately predict the trends of low-demand skills. (2) Based on scale-independent metrics, we can compare the performance of models at different granularities. It can be observed that RRMSE exhibits a significant trend of variation across different granularities; specifically, as the granularity becomes finer, the RRMSE performance deteriorates. This indicates that predicting skill demand at finer granularities is more challenging. Additionally, FiLM shows the least variation across multiple granularities, further validating its ability to provide stable and reliable predictions under varying granularities and demand value ranges. (3) Although Koopa performs averagely on MAE and RMSE metrics, it excels in predicting low-demand skills, particularly in terms of SMAPE. Similarly, NStransformer also performs well in scenarios focusing on low-demand skill predictions. This success can be attributed to both methods being designed to handle non-stationary time series. They effectively filter noise from historical sequences and restore intrinsic non-stationary information

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c|}{L1-Occupation} & \multicolumn{2}{c|}{L2-Occupation} & \multicolumn{2}{c|}{Region\&L1-O} & \multicolumn{2}{c|}{Region\&L2-O} & \multicolumn{2}{c}{Company} \\  & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE \\ \hline ARIMA & 20.27 & 256.89 & 6.46 & 115.79 & 3.98 & 58.65 & 1.31 & 27.42 & 1.31 & 38.88 \\ Prophet & 29.15 & 356.67 & 8.95 & 161.01 & 5.08 & 72.21 & 1.62 & 33.02 & 1.55 & 41.19 \\ \hline LSTM & 19.05 & 194.67 & 7.09 & 116.36 & 3.92 & 51.59 & 1.29 & 23.31 & 1.35 & 26.47 \\ SegRNN & 12.28 & 108.28 & 5.01 & 68.83 & 3.14 & 34.26 & 1.05 & 15.96 & 1.01 & 16.03 \\ \hline CHGH & 22.09 & 261.49 & 7.09 & 116.58 & 3.91 & 51.46 & 1.28 & 23.24 & 1.34 & 26.52 \\ Pre-DyGAE & 22.98 & 187.90 & 7.04 & 82.97 & 4.24 & 38.62 & 1.37 & 17.39 & 1.24 & 18.24 \\ \hline Transformer & 22.06 & 215.09 & 7.58 & 118.21 & 4.01 & 52.04 & 1.35 & 23.44 & 1.26 & 24.99 \\ Autoformer & 23.06 & 186.76 & 8.22 & 100.02 & 6.45 & 57.77 & 2.41 & 24.10 & 3.31 & 38.55 \\ Informer & 22.21 & 205.24 & 7.43 & 117.38 & 3.88 & 50.13 & 1.30 & 23.07 & 1.26 & 24.92 \\ Reformer & 22.11 & 204.35 & 7.46 & 116.60 & 3.91 & 50.95 & 1.25 & 22.81 & 1.54 & 27.37 \\ FEDformer & 22.87 & 181.93 & 7.46 & 88.97 & 4.63 & 43.21 & 1.98 & 21.73 & 2.43 & 26.92 \\ NStransformer & 17.36 & 149.46 & 5.75 & 86.24 & 3.45 & 37.09 & 1.15 & 17.45 & 2.13 & 34.83 \\ PatchTST & 14.91 & 141.06 & 5.15 & 78.86 & 3.10 & 35.38 & 1.04 & 16.57 & 1.01 & 19.09 \\ \hline DLinear & 16.61 & 154.88 & 5.44 & 81.61 & 3.24 & 36.67 & 1.07 & 16.79 & 1.05 & 18.85 \\ TSMixer & 21.34 & 192.85 & 8.14 & 106.65 & 5.81 & 62.14 & 5.95 & 68.26 & 13.96 & 144.96 \\ FreTS & 16.47 & 167.61 & 6.52 & 106.39 & 3.65 & 47.81 & 1.22 & 21.92 & 1.26 & 25.39 \\ \hline FiLM & 12.95 & 117.17 & 5.08 & 65.65 & 3.24 & 29.90 & 1.14 & 14.01 & 1.17 & 15.87 \\ Koopa & 19.91 & 179.30 & 6.05 & 91.87 & 3.53 & 40.73 & 1.15 & 18.71 & 1.08 & 20.18 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison on MAE and RMSE.

into time-dependent relationships, making them more adept at handling the fluctuating nature of low-demand skill time series data.

**Performance on Skill Demand Series with Structural Breaks.** As described in Section 3.3, in the dynamically changing job market, skill demand time series data exhibit structural breaks. To assess the impact of this phenomenon on different models in the skill demand forecasting task, we used the Chow test to detect structural breaks in the skill demand time series. The corresponding predictive performance of different models is presented in Tables 3 and 4. We observe the following phenomena: (1) Compared to the predictive performance on the full dataset, the performance on time series data with structural breaks is significantly worse. This finding underscores the complexity and unpredictability of skill trends that experience structural breaks. (2) FiLM has achieved results close to the overall skill demand prediction in terms of SMAPE and RRMSE metrics. This validates that FiLM can effectively mitigate the disruptive impact of structural breaks on skill demand forecasting. (3) Furthermore, while the overall predictive performance of skill demand forecasting at both the Region&L2-O and Company granularity levels is similar, significant differences emerge when forecasting skills experiencing structural breaks. This suggests that skills undergoing structural breaks display more predictable patterns at the Region&L2-O granularity level compared to the Company level, making them relatively easier to forecast.

\begin{table}
\begin{tabular}{l|l l|l l|l l|l l|l l} \hline \hline Model & \multicolumn{2}{l|}{L1-Occupation (\%)} & \multicolumn{2}{l|}{L2-Occupation (\%)} & \multicolumn{2}{l|}{Region&\&\&\&\multicolumn{1}{l|}{1-O (\%)} & \multicolumn{2}{l|}{Region&\&\&\multicolumn{1}{l|}{2-O (\%)}} & \multicolumn{2}{l}{Company (\%)} \\  & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE \\ \hline ARIMA & 35.72 & 47.89 & 25.00 & 58.87 & 23.86 & 58.07 & 13.58 & 73.57 & 20.17 & 147.94 \\ Prophet & 41.22 & 67.78 & 28.35 & 88.47 & 26.75 & 71.60 & 15.07 & 93.04 & 22.31 & 167.77 \\ \hline LSTM & 41.38 & 57.90 & 32.85 & 83.70 & 31.58 & 68.40 & 22.93 & 87.36 & 30.26 & 174.40 \\ SegRNN & 39.81 & 37.58 & 33.35 & 50.53 & 35.30 & 48.53 & 23.84 & 61.90 & 33.07 & 86.27 \\ \hline CHGH & 40.27 & 66.05 & 29.60 & 84.10 & 28.11 & 68.42 & 17.42 & 87.45 & 26.72 & 176.70 \\ PreDyGAE & 49.87 & 83.67 & 60.54 & 83.60 & 59.32 & 66.56 & 72.67 & 98.09 & 26.21 & 145.73 \\ \hline Transformer & 55.59 & 64.25 & 44.23 & 84.27 & 31.15 & 76.16 & 33.04 & 86.87 & 27.61 & 164.36 \\ Autofomer & 70.28 & 53.75 & 74.37 & 63.40 & 90.14 & 65.57 & 91.51 & 74.46 & 107.05 & 99.60 \\ Informer & 56.85 & 58.18 & 44.04 & 88.72 & 34.75 & 69.59 & 29.29 & 90.15 & 32.41 & 164.37 \\ Reformer & 56.58 & 61.35 & 40.58 & 83.70 & 32.21 & 72.87 & 20.86 & 90.85 & 45.25 & 169.87 \\ FEDformer & 69.30 & 54.03 & 69.29 & 60.00 & 73.17 & 52.69 & 81.73 & 70.06 & 94.19 & 97.97 \\ NStransformer & 38.11 & 47.19 & 26.30 & 60.73 & 24.98 & 48.89 & 14.55 & 63.29 & 24.20 & 100.78 \\ PatchTST & 34.70 & 51.17 & 24.52 & 58.80 & 25.15 & 44.96 & 13.50 & 67.48 & 19.89 & 115.34 \\ \hline DLinear & 41.84 & 52.89 & 34.35 & 60.22 & 33.47 & 51.05 & 25.77 & 64.65 & 30.71 & 108.66 \\ TSMixer & 56.59 & 61.17 & 72.29 & 99.35 & 82.48 & 87.29 & 120.85 & 96.49 & 155.20 & 102.14 \\ FreTS & 39.76 & 54.42 & 30.18 & 80.44 & 28.58 & 66.11 & 17.62 & 85.04 & 27.24 & 174.56 \\ \hline FiLM & 39.51 & 37.55 & 29.65 & 43.86 & 28.79 & 37.66 & 17.24 & 47.75 & 25.72 & 76.92 \\ Koopa & 37.84 & 58.30 & 25.72 & 65.34 & 24.41 & 57.81 & 13.98 & 74.00 & 20.43 & 123.96 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison on SMAPE and RRMSE.

\begin{table}
\begin{tabular}{l|l l|l l|l l|l l|l l} \hline \hline Model & \multicolumn{2}{l|}{L1-Occupation} & \multicolumn{2}{l|}{L2-Occupation} & \multicolumn{2}{l|}{Region&\&\&\multicolumn{1}{l|}{1-O} & \multicolumn{2}{l|}{Region}&\&\multicolumn{1}{l|}{2-O} & \multicolumn{2}{l}{Company} \\  & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE \\ \hline LSTM & 87.30 & 554.46 & 57.95 & 400.22 & 18.99 & 149.53 & 7.91 & 52.38 & 24.40 & 159.02 \\ SegRNN & 61.92 & 390.54 & 43.97 & 276.57 & 15.85 & 114.04 & 6.56 & 37.84 & 17.98 & 112.13 \\ \hline CHGH & 94.30 & 629.32 & 58.06 & 401.45 & 19.00 & 149.75 & 7.90 & 52.50 & 24.37 & 159.44 \\ PreGyGAE & 78.35 & 493.83 & 48.69 & 336.15 & 17.49 & 136.66 & 7.31 & 38.88 & 19.76 & 164.43 \\ \hline Transformer & 98.66 & 580.58 & 61.73 & 404.17 & 19.37 & 151.12 & 8.45 & 55.46 & 22.41 & 152.27 \\ Autoformer & 107.22 & 533.06 & 67.66 & 350.97 &

## 5 Conclusion

In this work, we introduced Job-SDF, a dataset designed for training and benchmarking job-skill demand forecasting models. Compiled from millions of public job advertisements collected from online recruitment platforms, this dataset includes monthly recruitment demand for 2,324 types of skills across 52 occupations, 521 companies, and 7 regions. Using this dataset, we validated a wide range of time-series forecasting approaches, including statistical models, RNN-based models, Transformer-based models, MLP-based models, Graph-based models, and Fourier-based models. Furthermore, we conducted extensive experiments to compare the performance of various methods in predicting skill demand at different granularities. We hope that Job-SDF will facilitate further research in this field.

## Acknowledgements

This work was supported in part by the National Key R&D Program of China (Grant No.2023YFF0725001), in part by the National Natural Science Foundation of China (Grant No.92370204), in part by the Guangdong Basic and Applied Basic Research Foundation (Grant No.2023B1515120057), in part by Guangzhou-HKUST (GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality, in part by Nansha Postdoctoral Research Project, and in part by the National Natural Science Foundation of China (Grant No.62176014), the Fundamental Research Funds for the Central Universities.

## References

* [1] David Autor et al. The polarization of job opportunities in the us labor market: Implications for employment and earnings. _Center for American Progress and The Hamilton Project_, 6:11-19, 2010.
* [2] James J Heckman, Jora Stixrud, and Sergio Urzua. The effects of cognitive and noncognitive abilities on labor market outcomes and social behavior. _Journal of Labor economics_, 24(3):411-482, 2006.
* [3] Chuan Qin, Hengshu Zhu, Dazhong Shen, Ying Sun, Kaichun Yao, Peng Wang, and Hui Xiong. Automatic skill-oriented question generation and recommendation for intelligent job interviews. _ACM Transactions on Information Systems_, 42(1):1-32, 2023.
* [4] Marios Kokkodis and Panagiotis G Ipeirotis. Demand-aware career path recommendations: A reinforcement learning approach. _Management science_, 67(7):4362-4383, 2021.
* [5] Rui Zha, Chuan Qin, Le Zhang, Dazhong Shen, Tong Xu, Hengshu Zhu, and Enhong Chen. Career mobility analysis with uncertainty-aware graph autoencoders: A job title transition perspective. _IEEE Transactions on Computational Social Systems_, 11(1):1205-1215, 2023.

\begin{table}
\begin{tabular}{l|l l|l l|l l|l l|l l} \hline \hline Model & \multicolumn{2}{l|}{L1-Occupation (\%)} & \multicolumn{2}{l|}{L2-Occupation (\%)} & \multicolumn{2}{l|}{Region\&L1-O (\%)} & \multicolumn{2}{l|}{Region\&L2-O (\%)} & \multicolumn{2}{l}{Company (\%)} \\  & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE & SMAPE & RRMSE \\ \hline LSTM & 43.78 & 58.05 & 48.93 & 84.46 & 46.64 & 78.31 & 42.03 & 58.48 & 68.38 & 187.30 \\ SegRNN & 39.22 & 37.80 & 43.09 & 51.14 & 45.17 & 54.31 & 39.41 & 41.40 & 57.45 & 89.65 \\ \hline CHGH & 44.91 & 66.31 & 48.90 & 84.87 & 45.43 & 78.32 & 39.79 & 58.89 & 68.36 & 189.91 \\ PreDyGAE & 52.35 & 47.15 & 56.56 & 59.31 & 52.06 & 61.22 & 44.13 & 42.31 & 70.26 & 106.88 \\ \hline Transformer & 50.01 & 64.47 & 53.10 & 84.95 & 46.50 & 86.56 & 47.67 & 61.23 & 64.92 & 177.43 \\ Autoformer & 63.46 & 54.08 & 68.62 & 64.14 & 87.93 & 68.97 & 88.95 & 63.85 & 115.00 & 100.60 \\ Informer & 51.11 & 58.40 & 51.89 & 89.70 & 47.81 & 80.86 & 44.90 & 57.55 & 65.11 & 177.16 \\ Reformer & 50.79 & 61.59 & 51.51 & 84.53 & 46.86 & 84.15 & 40.81 & 58.59 & 72.36 & 181.36 \\ FEDformer & 62.83 & 54.37 & 64.37 & 60.84 & 72.24 & 58.55 & 80.03 & 54.29 & 103.27 & 100.65 \\ NSTransformer & 45.36 & 47.46 & 47.63 & 61.85 & 43.04 & 57.60 & 36.72 & 39.72 & 170.57 & 113.87 \\ PatchTST & 40.89 & 51.48 & 43.26 & 59.69 & 41.51 & 51.85 & 34.74 & 43.12 & 55.26 & 122.56 \\ \hline DLinear & 43.14 & 53.20 & 45.25 & 61.13 & 45.26 & 58.80 & 41.15 & 41.71 & 57.65 & 115.24 \\ TSM Mixer & 54.31 & 61.31 & 76.08 & 99.84 & 85.12 & 95.81 & 117.39 & 93.66 & 160.55 & 102.23 \\ FreTS & 42.44 & 54.59 & 48.24 & 81.17 & 45.39 & 75.43 & 39.85 & 57.83 & 68.39 & 187.94 \\ \hline FiLM & 38.96 & 37.82 & 44.23 & 44.52 & 44.95 & 43.06 & 40.05 & 30.80 & 56.37 & 80.77 \\ Koopa & 46.45 & 58.59 & 47.13 & 66.28 & 42.60 & 66.20 & 36.24 & 47.48 & 58.98 & 131.77 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance comparison on data with structural breaks on RRMSE and SMAPE.

* [6] Rui Zha, Ying Sun, Chuan Qin, Le Zhang, Tong Xu, Hengshu Zhu, and Enhong Chen. Towards unified representation learning for career mobility analysis with trajectory hypergraph. _ACM Transactions on Information Systems_, 42(4):1-28, 2024.
* [7] Xiaoshan Yu, Chuan Qin, Qi Zhang, Chen Zhu, Haiping Ma, Xingyi Zhang, and Hengshu Zhu. Disco: A hierarchical disentangled cognitive diagnosis framework for interpretable job recommendation. _arXiv preprint arXiv:2410.07671_, 2024.
* [8] Ying Sun, Hengshu Zhu, Lu Wang, Le Zhang, and Hui Xiong. Large-scale online job search behaviors reveal labor market shifts amid covid-19. _Nature Cities_, 1(2):150-163, 2024.
* [9] Le Dai, Yu Yin, Chuan Qin, Tong Xu, Xiangnan He, Enhong Chen, and Hui Xiong. Enterprise cooperation and competition analysis with a sign-oriented preference network. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 774-782, 2020.
* [10] Hengshu Zhu, Hui Xiong, Fangshuang Tang, Qi Liu, Yong Ge, Enhong Chen, and Yanjie Fu. Days on market: Measuring liquidity in real estate markets. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 393-402, 2016.
* [11] Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, and Hui Xiong. Enhancing person-job fit for talent recruitment: An ability-aware neural network approach. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 25-34, 2018.
* [12] Chuan Qin, Hengshu Zhu, Chen Zhu, Tong Xu, Fuzhen Zhuang, Chao Ma, Jingshuai Zhang, and Hui Xiong. Duerquiz: A personalized question recommender system for intelligent job interview. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2165-2173, 2019.
* [13] Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Chao Ma, Enhong Chen, and Hui Xiong. An enhanced neural network approach to person-job fit in talent recruitment. _ACM Transactions on Information Systems (TOIS)_, 38(2):1-33, 2020.
* [14] Kaichun Yao, Jingshuai Zhang, Chuan Qin, Peng Wang, Hengshu Zhu, and Hui Xiong. Knowledge enhanced person-job fit for talent recruitment. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pages 3467-3480. IEEE, 2022.
* [15] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. Exploring large language model for graph data understanding in online job recommendations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 9178-9186, 2024.
* [16] Shuqing Bian, Xu Chen, Wayne Xin Zhao, Kun Zhou, Yupeng Hou, Yang Song, Tao Zhang, and Ji-Rong Wen. Learning to match jobs with resumes from sparse interaction data using multi-view co-teaching network. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 65-74, 2020.
* [17] Yong Luo, Huaizheng Zhang, Yonggang Wen, and Xinwen Zhang. Resumegan: an optimized deep representation learning framework for talent-job fit via adversarial learning. In _Proceedings of the 28th ACM international conference on information and knowledge management_, pages 1101-1110, 2019.
* [18] Yoosof Mashayekhi, Nan Li, Bo Kang, Jefrey Lijffijt, and Tijl De Bie. A challenge-based survey of e-recruitment recommendation systems. _ACM Computing Surveys_, 56(10):1-33, 2024.
* [19] Yong Luo, Huaizheng Zhang, Yongjie Wang, Yonggang Wen, and Xinwen Zhang. Resumenet: A learning-based framework for automatic resume quality assessment. In _2018 IEEE International Conference on Data Mining (ICDM)_, pages 307-316. IEEE, 2018.
* [20] Dazhong Shen, Chuan Qin, Hengshu Zhu, Tong Xu, Enhong Chen, and Hui Xiong. Joint representation learning with relation-enhanced topic models for intelligent job interview assessment. _ACM Transactions on Information Systems (TOIS)_, 40(1):1-36, 2021.

* [21] Feihu Jiang, Chuan Qin, Kaichun Yao, Chuyu Fang, Fuzhen Zhuang, Hengshu Zhu, and Hui Xiong. Enhancing question answering for enterprise knowledge bases using large language models. In _International Conference on Database Systems for Advanced Applications_, pages 273-290. Springer, 2024.
* [22] David J Deming. The growing importance of social skills in the labor market. _The Quarterly Journal of Economics_, 132(4):1593-1640, 2017.
* [23] Liyi Chen, Chuan Qin, Ying Sun, Xin Song, Tong Xu, Hengshu Zhu, and Hui Xiong. Collaboration-aware hybrid learning for knowledge development prediction. In _Proceedings of the ACM on Web Conference 2024_, pages 3976-3985, 2024.
* [24] Yunfei Zhang, Chuan Qin, Dazhong Shen, Haiping Ma, Le Zhang, Xingyi Zhang, and Hengshu Zhu. Relicd: a reliable cognitive diagnosis framework with confidence awareness. In _2023 IEEE International Conference on Data Mining (ICDM)_, pages 858-867. IEEE, 2023.
* [25] William Rasdorf, Joseph E Hummer, and Stephanie C Vereen. Data collection opportunities and challenges for skilled construction labor demand forecast modeling. _Public Works Management & Policy_, 21(1):28-52, 2016.
* [26] Joshua Healy, Kostas Mavromaras, and Peter J Sloane. Adjusting to skill shortages in australian smes. _Applied Economics_, 47(24):2470-2487, 2015.
* [27] Lutz Bellmann and Olaf Hubler. The skill shortage in german establishments before, during and after the great recession. _Jahrbucher fur Nationalokonomie und Statistik_, 234(6):800-828, 2014.
* [28] Subhro Das, Sebastian Steffen, Wyatt Clarke, Prabhat Reddy, Erik Brynjolfsson, and Martin Fleming. Learning occupational task-shares dynamics for the future of work. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, pages 36-42, 2020.
* [29] Maysa Malfiza Garcia de Macedo, Wyatt Clarke, Eli Lucherini, Tyler Baldwin, Dilermando Queiroz Neto, Rogerio Abreu de Paula, and Subhro Das. Practical skills demand forecasting via representation learning of temporal dynamics. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 285-294, 2022.
* [30] Wenshuo Chao, Zhaopeng Qiu, Likang Wu, Zhuoning Guo, Zhi Zheng, Hengshu Zhu, and Hao Liu. A cross-view hierarchical graph learning hypernetwork for skill demand-supply joint prediction. In _AAAI_, 2024.
* [31] Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. Stock price prediction using the arima model. In _2014 UKSim-AMSS 16th international conference on computer modelling and simulation_, pages 106-112. IEEE, 2014.
* [32] Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, and Haotong Zhang. Segrnn: Segment recurrent neural network for long-term time series forecasting. _arXiv preprint arXiv:2308.11200_, 2023.
* [33] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, pages 11106-11115, 2021.
* [34] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in neural information processing systems_, 34:22419-22430, 2021.
* [35] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. _Advances in Neural Information Processing Systems_, 35:9881-9893, 2022.
* [36] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International conference on machine learning_, pages 27268-27286. PMLR, 2022.

* [37] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [38] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, pages 11121-11128, 2023.
* [39] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and Jun Zhou. Timemixer: Decomposable multiscale mixing for time series forecasting. _arXiv preprint arXiv:2405.14616_, 2024.
* [40] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. _Advances in Neural Information Processing Systems_, 35:12677-12690, 2022.
* [41] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Saba Sareminia. A support vector based hybrid forecasting model for chaotic time series: Spare part consumption prediction. _Neural Processing Letters_, 55(3):2825-2841, 2023.
* [43] Chao Chen, Jamie Twycross, and Jonathan M Garibaldi. A new accuracy measure based on bounded relative error for time series forecasting. _PloS one_, 12(3):e0174202, 2017.
* [44] Alan Manning. We can work it out: the impact of technological change on the demand for low-skill workers. _Scottish Journal of Political Economy_, 51(5):581-608, 2004.
* [45] John M Abowd, John C Haltiwanger, Julia Lane, Kevin L McKinney, and Kristin Sandusky. Technology and the demand for skill: an analysis of within and between firm differences, 2007.
* [46] Chuan Qin, Le Zhang, Yihang Cheng, Rui Zha, Dazhong Shen, Qi Zhang, Xi Chen, Ying Sun, Chen Zhu, Hengshu Zhu, et al. A comprehensive survey of artificial intelligence techniques for talent analytics. _arXiv preprint arXiv:2307.03195_, 2023.
* [47] Chinhui Jahn. Wage inequality and demand for skill: evidence from five decades. _ILR Review_, 52(3):424-443, 1999.
* [48] Jong-Wha Lee and Dainh Wie. Technological change, skill demand, and wage inequality: Evidence from indonesia. _World Development_, 67:238-250, 2015.
* [49] Ying Sun, Fuzhen Zhuang, Hengshu Zhu, Qi Zhang, Qing He, and Hui Xiong. Market-oriented job skill valuation with cooperative composition neural network. _Nature communications_, 12(1):1992, 2021.
* [50] George S Benson and Edward E Lawler III. Raising skill demand: Generating good jobs. _Transforming the US Workforce Development System. Urbana: Labor and Employment Relations Association_, 2011.
* [51] Margaret Hilton. _Research on future skill demands: A workshop summary_. National Academies Press, 2008.
* [52] Xi Chen, Chuan Qin, Zhigaoyuan Wang, Yihang Cheng, Chao Wang, Hengshu Zhu, and Hui Xiong. Pre-dygae: Pre-training enhanced dynamic graph autoencoder for occupational skill demand forecasting. In _Proceedings of the 33th International Joint Conference on Artificial Intelligence_, 2024.
* [53] Manuel Cifuentes, Jon Boyer, David A Lombardi, and Laura Punnett. Use of o* net as a job exposure matrix: a literature review. _American journal of industrial medicine_, 53(9):898-914, 2010.
* [54] Filippo Chiarello, Gualtiero Fantoni, Terence Hogarth, Vito Giordano, Liga Baltina, and Irene Spada. Towards esco 4.0-is the european classification of skills in line with industry 4.0? a text mining approach. _Technological Forecasting and Social Change_, 173:121177, 2021.

* [55] Chuan Qin, Kaichun Yao, Hengshu Zhu, Tong Xu, Dazhong Shen, Enhong Chen, and Hui Xiong. Towards automatic job description generation with capability-aware neural networks. _IEEE Transactions on Knowledge and Data Engineering_, 35(5):5341-5355, 2022.
* [56] Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai Zhang, Hengshu Zhu, Fuzhen Zhuang, and Hui Xiong. Recruitpro: A pretrained language model with skill-aware prompt learning for intelligent recruitment. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3991-4002, 2023.
* [57] Kaichun Yao, Chuan Qin, Hengshu Zhu, Chao Ma, Jingshuai Zhang, Yi Du, and Hui Xiong. An interactive neural network approach to keyphrase extraction in talent recruitment. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2383-2393, 2021.
* [58] Feihu Jiang, Chuan Qin, Jingshuai Zhang, Kaichun Yao, Xi Chen, Dazhong Shen, Chen Zhu, Hengshu Zhu, and Hui Xiong. Towards efficient resume understanding: A multi-granularity multi-modal pre-training approach. _arXiv preprint arXiv:2404.13067_, 2024.
* [59] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. _Advances in Neural Information Processing Systems_, 35:38775-38787, 2022.
* [60] Gregory C Chow. Tests of equality between sets of coefficients in two linear regressions. _Econometrica: Journal of the Econometric Society_, pages 591-605, 1960.
* [61] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-series forecasting. _Advances in neural information processing systems_, 33:17766-17778, 2020.
* [62] Ting Guo, Feng Hou, Yan Pang, Xiaoyun Jia, Zhongwei Wang, and Ruili Wang. Learning and integration of adaptive hybrid graph structures for multivariate time series forecasting. _Information Sciences_, 648:119560, 2023.
* [63] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. _Time series analysis: forecasting and control_. John Wiley & Sons, 2015.
* [64] Sean J Taylor and Benjamin Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* [65] Chen Zhu, Hengshu Zhu, Hui Xiong, Pengliang Ding, and Fang Xie. Recruitment market trend analysis with sequential latent variable models. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 383-392, 2016.
* [66] Qi Zhang, Tong Xu, Hengshu Zhu, Lifu Zhang, Hui Xiong, Enhong Chen, and Qi Liu. Aftershock detection with multi-scale description based neural network. In _2019 IEEE International Conference on Data Mining (ICDM)_, pages 886-895. IEEE, 2019.
* [67] Qi Zhang, Hengshu Zhu, Ying Sun, Hao Liu, Fuzhen Zhuang, and Hui Xiong. Talent demand forecasting with attentive neural sequential model. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 3906-3916, 2021.
* [68] Qi Zhang, Hengshu Zhu, Qi Liu, Enhong Chen, and Hui Xiong. Exploiting real-time search engine queries for earthquake detection: A summary of results. _ACM Transactions on Information Systems (TOIS)_, 39(3):1-32, 2021.
* [69] Dazhong Shen, Qi Zhang, Tong Xu, Hengshu Zhu, Wenjia Zhao, Zikai Yin, Peilun Zhou, Lihua Fang, Enhong Chen, and Hui Xiong. A machine learning-enhanced robust p-phase picker for real-time seismic monitoring. _arXiv preprint arXiv:1911.09275_, 2019.
* [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

* Nie et al. [2022] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. _arXiv preprint arXiv:2211.14730_, 2022.
* Yi et al. [2024] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
* Chen et al. [2022] Liyi Chen, Zhi Li, Tong Xu, Han Wu, Zhefeng Wang, Nicholas Jing Yuan, and Enhong Chen. Multi-modal siamese network for entity alignment. In _Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining_, pages 118-126, 2022.
* Chen et al. [2023] Liyi Chen, Zhi Li, Weidong He, Gong Cheng, Tong Xu, Nicholas Jing Yuan, and Enhong Chen. Entity summarization via exploiting description complementarity and salience. _IEEE Transactions on Neural Networks and Learning Systems_, 34(11):8297-8309, 2023.
* Shen et al. [2021] Dazhong Shen, Chuan Qin, Chao Wang, Hengshu Zhu, Enhong Chen, and Hui Xiong. Regularizing variational autoencoder with diversity and uncertainty awareness. _arXiv preprint arXiv:2110.12381_, 2021.
* Chen et al. [2021] Miao Chen, Chao Wang, Chuan Qin, Tong Xu, Jianhui Ma, Enhong Chen, and Hui Xiong. A trend-aware investment target recommendation system with heterogeneous graph. In _2021 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2021.
* Rawal and Ahmad [2024] Keerti Rawal and Aijaz Ahmad. Mining latent patterns with multi-scale decomposition for electricity demand and price forecasting using modified deep graph convolutional neural networks. _Sustainable Energy, Grids and Networks_, page 101436, 2024.
* Rozemberczki et al. [2021] Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzman Lopez, Nicolas Collignon, and Rik Sarkar. PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models. In _Proceedings of the 30th ACM International Conference on Information and Knowledge Management_, page 4564-4573, 2021.
* Wu et al. [2023] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _International Conference on Learning Representations_, 2023.
* Pareja et al. [2020] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In _Proceedings of the AAAI conference on artificial intelligence_, pages 5363-5370, 2020.
* Seo et al. [2018] Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. In _Neural Information Processing: 25th International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13-16, 2018, Proceedings, Part I 25_, pages 362-373. Springer, 2018.
* Zhao et al. [2020] Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li. T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction. _IEEE Transactions on Intelligent Transportation Systems_, pages 3848-3858, 2020.
* Chen et al. [2022] Jinyin Chen, Xueke Wang, and Xuanheng Xu. Gc-lstm: Graph convolution embedded lstm for dynamic network link prediction. _Applied Intelligence_, pages 1-16, 2022.
* Taheri et al. [2019] Aynaz Taheri, Kevin Gimpel, and Tanya Berger-Wolf. Learning to represent the evolution of dynamic graphs with recurrent models. In _Companion Proceedings of The 2019 World Wide Web Conference_, WWW '19, New York, NY, USA, 2019. Association for Computing Machinery.

Ethics Statement

In accordance with the Code of Ethics as outlined by NeurIPS 1, we present the following discussion on the ethical considerations and potential societal impact of our research.

Footnote 1: https://neurips.cc/public/EthicsGuidelines

Privacy.Our data is collected from online platforms, encompassing job advertisements, which is public by various companies. This collection method ensures that all information utilized is already available to the public, having been disclosed by companies for the purpose of recruitment. Consequently, our analysis inherently respects privacy norms, as it involves no proprietary or confidential data, relying solely on publicly shared statistics and general information.

Consent.We have extensively collected job advertisement data from online platforms where the information is openly and transparently available. Our analysis is based on the statistical data derived from these job advertisements, rather than the original textual information, significantly minimizing the risk of privacy breaches. Therefore, the use of this data is permissible.

Deprecated Datasets.Not applicable.

Copyright and Fair Use.The library used in our research is publicly available and distributed under the MIT License. We list the used assets along with their licenses as follows:

* PyTorch Geometric Temporal [78] consists of various dynamic and temporal geometric deep learning, embedding, and spatio-temporal regression methods from a variety of published research papers. This library is open-source and available at https://github.com/benedekrozemberczki/pytorch_geometric_temporal, which is publicly available and distributed under the MIT License.
* Time Series Library [79] is an open-source library for deep learning researchers, especially for deep time series analysis. This library is available at https://github.com/thuml/Time-Series-Library, which is publicly available and distributed under the MIT License.

Representative Evaluation Practice.Job-SDF has extensively covered skill demand data at multiple granular levels, including regions, occupations, and companies. The detailed nature of the data proves invaluable in advancing tasks related to skill demand prediction. It significantly assists job seekers in identifying employment opportunities, employers in recruiting suitable talent, and educational institutions in tailoring their curricula to meet market demands. Moreover, this comprehensive dataset not only facilitates the analysis of skill trends but also serves as a platform for verifying time-series algorithms within a specialized application domain.

Safety.Our research does not employ technologies that directly inflict harm on individuals.

Security.Since the data sources are all publicly available, there is no risk of security incidents.

Discrimination.Our data is strictly limited to market analysis and does not judge the superiority or inferiority of skills themselves, thus it does not involve any issues of discrimination.

Deception & Harassment.We firmly believe that our data cannot be used for deceptive practices or harassment activities.

Human Rights.Not applicable.

Bias and Fairness.Since our data is collected from the public job postings of a subset of companies, there may be statistical biases in skill demands across different industries or regions. While it may not represent the entirety, it effectively captures the main trends in skill demands.

Code and Data Availability

The dataset is CC BY-NC-SA 4.0 licensed, accessible via the URL https://github.com/Job-SDF/benchmark. We also provide a website for the project: https://job-sdf.github.io/with information on how to use the data and code.

## Appendix C Computational Resource

Due to inherent design and size constraints of the models combined with varying data sizes at different granularities, the deployment environments for each model are distinct. The CHGH model, which requires over 80GB of memory, is exclusively deployed on CPU platforms to accommodate its substantial resource demands. In contrast, the PreDyGAE model operates solely on GPU infrastructure, leveraging the computational efficiencies of the NVIDIA A800 GPUs. For other models, deployment strategies are tailored according to the granularity of the data. Experiments at the labor market, regions, L1 occupations, L2 occupations, and Region & L1 occupations granularities are conducted on GPUs, capitalizing on the enhanced processing capabilities of these units for handling moderate data volumes. However, at the granularities of Region & L2 and company, where data volumes are significantly larger, deployment shifts to CPUs. Overall, the training time of different models are shown in Table 5.

## Appendix D Additional Experimental Results

Due to page limitations in the main text, we have included additional experimental content in the appendix. First, we present the results of repeated trials of the benchmark models discussed in the main text in the first subsection. Subsequently, we focus on the performance of existing benchmark models in predicting demand for low-frequency skills. Further, we have constructed a co-occurrence relationship between skills as prior knowledge based on the training set and employed various Graph Neural Network (GNN)-based multivariate time series forecasting methods in the task of job skill demand forecasting, demonstrating promising results. Finally, considering that the skill demand proportion may be more meaningful than the skill demand volume in certain contexts, we have constructed a dataset for skill demand proportion and showcased the performance of benchmark models on this task.

### Repeated Experiments on Job Skill Demand Forecasting

To demonstrate the robustness and reliability of our experimental results, we first repeated the experiments multiple times as described in the main text. Additionally, we extended our analysis to include experiments across the entire labor market and at various regional granularities.

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline \hline Model & Market & Region & L1-O & L2-O & R\&L1-O & R\&L2-O & Company \\ \hline LSTM & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 37.7 & 39.0 \\ SegRNN & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 342.8 & 458.2 \\ CHGH & 17.7 & 132.8 & 170.2 & 258.3 & 1300.3 & 490.6 & 6604.2 \\ PreDyGAE & 1-10 & 16.5 & 30.0 & 48.1 & 48.1 & 88.2 & 126.2 \\ Transformer & 0-0.5 & 0-0.5 & 0-0.5 & 0.5-1 & 0.5-1 & 128.2 & 166.5 \\ Autoformer & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0.5-1 & 304.3 & 325.0 \\ Informer & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 133.8 & 171.7 \\ Reformer & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 36.0 & 52.5 \\ FEDformer & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 193.8 & 198.5 \\ NStransformer & 0-0.5 & 0.5-1 & 0.5-1 & 0-0.5 & 0.5-1 & 128.5 & 195.7 \\ PatchTST & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 1202.8 & 2558.0 \\ DLinear & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 20.0 & 39.1 \\ TSMixer & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 24.0 & 97.0 \\ FreTS & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 85.0 & 200.0 \\ FiLM & 0-0.5 & 0.5-1 & 0.5-1 & 1-10 & 1-10 & 598.0 & 1464.7 \\ Koopa & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 0-0.5 & 38.3 & 68.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Training time (minute) of different models for job skill demand forecasting.

Implementment Details.We utilized the Time-series-Library 2 to implement some of the models. The hyperparameters were uniformly set as follows: a learning rate of 0.0001, 20 epochs of training, a hidden layer dimension of 2048, the GELU activation function, and MSE loss as the loss function. Early stopping was employed to prevent overfitting by terminating training early when necessary. Data from the first 24 months were used for pre-training, and the model was fine-tuned using the next 6 months to capture trend changes. Finally, the model was used to infer skill demands for the last 6 months. All other hyperparameters were kept consistent with those in the original paper. To ensure the reliability of our findings, we repeated these experiments four times, using random seeds set to 0, 1, 2, and 3, respectively.

Footnote 2: https://github.com/thuml/Time-Series-Library

Overall PerformanceTable 6 displays the mean and standard deviation results of repeated experiments on the benchmark models for the job skill demand forecasting task as presented in the main text. Initially, we supplement the experimental results at the overall labor market level and regional granularity, where the RMSE averages over 1000. In cases of coarser granularity, due to the larger base of demand values, the prediction deviations are significant.

Performance on Skill Demand Series with Structural BreaksTable 7 presents the results of repeated experiments on forecasting skill demand sequences that have undergone structural breaks. Initially, the overall errors are quite pronounced, underscoring the challenge of accurately predicting these skills. Moreover, FiLM performs well on most metrics, which further verifies its robustness.

### Job Skill Demand Forecasting for Low-Frequency Skills

In multigranular skill demand sequences, a significant number of skills remain inactive or in low frequency over extended periods. These skills might continue to have low demand in the future (indicating low importance), or they might suddenly gain interest from certain professions or companies, leading to rapid growth. In this study, we define low-frequency skills as those that appear fewer than twice in the time slices of the training set. Predicting the demand for these skills is challenging because their data points are predominantly zero during training, resulting in a lack of effective observational data. Therefore, we specifically present the demand prediction results of the existing benchmark models for these low-frequency skills.

Results.We continued to test the demand prediction effect on low-frequency skills using the benchmark models described in the main text, and the results are shown in Table 8. From this, we can draw the following conclusions: Firstly, there is a significant increase in the error on the RRMSE metric, indicating that low-demand skills are difficult to predict accurately. Secondly, Koopa has the best predictive performance in this scenario. We also found that the performance of SegRNN significantly decreases, suggesting that SegRNN's segment learning approach is not suitable for predicting low-frequency skill demands due to a lack of effective observational data, rendering the learning segments meaningless.

### Skill Co-occurrence Graph Enhanced Job Skill Demand Forecasting

In the task of job skill demand forecasting, fully leveraging the inter-relationships among different skills is beneficial for downstream tasks. Therefore, we construct a prior graph with co-occurrence frequency from the training data to include as a dataset component. Given a set of granularities \(i,j,\dots,k\), we constructed the skill co-occurrence graph as \(\mathcal{G}^{i,j,\dots,k}=(\mathcal{V}^{i,j,\dots,k},\mathcal{E}^{i,j,\dots,k})\), where \(\mathcal{V}^{i,j,\dots,k}\) is the extended skill set under the multiple granularities. The edge weight \(e_{v,v^{\prime}}\in\mathcal{E}^{i,j,\dots,k}\) between nodes \(v\) and \(v^{\prime}\) is determined by the co-occurrence frequency of the node pair \(v,v^{\prime}\) in the job advertisement data for training \(\mathcal{P}^{train}\). Specifically, given \(v=(a^{i},a^{j},\dots,a^{k},s)\) and \(v^{\prime}=(a^{i\prime},a^{j\prime},...,a^{k\prime},s^{\prime})\), \(e_{v,v^{\prime}}\) is calculated as:

\[e_{v,v^{\prime}}=\sum_{p\in\mathcal{P}^{train}}\prod_{x\in\{a^{i},a^{j},\dots,a^{k},a^{i\prime},a^{i\prime},a^{i\prime},\dots,a^{k\prime},s,s^{\prime}\}} \mathbf{1}_{p}(x\in p).\] (4)

This information will serve as prior knowledge, reflecting global inter-skill dependency patterns.

Benchmark ModelsTo fully utilize the prior information from the co-occurency graph, we introduce several GNN-based methods for multivariate time series prediction. These methods leverage GNNs to extract the influences between different variables, effectively capturing the relationships among various time series. The specific models are as follows:

* **EvolveGCN**[80]: EvolveGCN introduces a recurrent mechanism to update the network parameters, as GCN parameters, for capturing the dynamism of the graphs. Two methods are introduced: EvolveGCNH, which learns the weight matrix of the graph at each time

\begin{table}
\begin{tabular}{c|l l l l l l l} \hline \hline  & Model & Market & Region & L1-O & L2-O & R& R&L1-O & R&L2-O & Company \\ \hline \multirow{11}{*}{**EvolveGCN**} & LSTM & \(314.54\pm_{0.57}\) & \(499.29\pm_{0.20}\) & \(24.43\pm_{4.91}\) & \(8.04\pm_{0.87}\) & \(4.44\pm_{0.47}\) & \(1.45\pm_{0.15}\) & \(1.49\pm_{0.13}\) \\  & SegRNN & \(190.05\pm_{0.37}\) & \(35.92\pm_{0.47}\) & \(16.37\pm_{0.73}\) & \(5.81\pm_{0.73}\) & \(3.68\pm_{0.5}\) & \(1.32\pm_{0.25}\) & \(1.23\pm_{0.2}\) \\  & CHGH & \(315.47\pm_{0.04}\) & \(50.03\pm_{0.02}\) & \(25.62\pm_{2.33}\) & \(8.04\pm_{0.87}\) & \(4.43\pm_{0.48}\) & \(1.51\pm_{0.7}\) & \(1.54\pm_{0.73}\) \\  & PreDyAE & \(189.95\pm_{0.01}\) & \(20.27\pm_{1.15}\) & \(6.81\pm_{0.21}\) & \(4.08\pm_{0.15}\) & \(1.85\pm_{0.43}\) & \(1.85\pm_{0.56}\) \\  & Transformer & \(304.72\pm_{0.79}\) & \(54.98\pm_{0.13}\) & \(27.43\pm_{0.49}\) & \(8.93\pm_{1.24}\) & \(4.92\pm_{0.83}\) & \(1.75\pm_{0.36}\) & \(1.69\pm_{0.39}\) \\  & Autoformer & \(465.86\pm_{2.78}\) & \(60.9\pm_{0.61}\) & \(31.97\pm_{0.13}\) & \(9.97\pm_{1.6}\) & \(6.68\pm_{0.21}\) & \(2.6\pm_{0.17}\) & \(2.85\pm_{0.42}\) \\  & Informer & \(340.76\pm_{1.6}\) & \(550.83\pm_{0.35}\) & \(27.54\pm_{1.87}\) & \(8.81\pm_{1.6}\) & \(4.87\pm_{0.91}\) & \(1.73\pm_{0.39}\) & \(1.69\pm_{0.39}\) \\  & Reformer & \(449.52\pm_{0.62}\) & \(54.83\pm_{0.22}\) & \(27.35\pm_{0.47}\) & \(8.88\pm_{1.3}\) & \(4.96\pm_{0.7}\) & \(1.71\pm_{0.42}\) & \(1.86\pm_{0.34}\) \\  & FEDformer & \(469.46\pm_{1.52}\) & \(60.37\pm_{0.19}\) & \(31.87\pm_{0.22}\) & \(9.54\pm_{1.9}\) & \(5.83\pm_{1.1}\) & \(2.41\pm_{0.39}\) & \(2.52\pm_{0.08}\) \\  & NsTransformer & \(208.23\pm_{1.33}\) & \(37.39\pm_{0.46}\) & \(19.13\pm_{1.61}\) & \(6.29\pm_{0.5}\) & \(3.79\pm_{0.31}\) & \(1.26\pm_{0.1}\) & \(1.64\pm_{0.44}\) \\  & PatchST & \(204.91\pm_{12.41}\) & \(36.33\pm_{2.33}\) & \(18.69\pm_{0.45}\) & \(6.12\pm_{0.89}

step as a hidden state, and EvolveGCNO, which directly employs the weight evolution as a hidden state output, decoupled from node embedding.
* **GConvGRU**[81]: This model integrates convolutional neural networks (CNNs) on graphs to identify spatial structures and recurrent neural networks (RNNs) to detect dynamic patterns. Two architectures, GConvGRU and GConvLSTM, are explored for the Graph Convolutional Recurrent Network (GCRN).
* **TGCN**[82]: The temporal graph convolutional network (T-GCN) model, which is in combination with the graph convolutional network (GCN) and gated recurrent unit (GRU). Specifically, the GCN is used to learn complex topological structures to capture spatial

\begin{table}
\begin{tabular}{|c|l l l l l l l|} \hline  & Model & Market & Region & L-O & L-O & R\&L-O & R\&L-O & R\&L-O & Company \\ \hline \multirow{11}{*}{**GCN**} & LSTM & \(243.99_{+0.56}\) & \(109.63_{+0.04}\) & \(101.68_{+1.13}\) & \(63.39_{+4.79}\) & \(21.02_{+1.85}\) & \(8.55_{+0.39}\) & \(26.23_{+1.57}\) \\  & SegRNN & \(256.67_{+0.54}\) & \(76.09_{+1.01}\) & \(68.08_{+5.53}\) & \(63.49_{+0.88}\) & \(16.12_{+0.74}\) & \(7.07_{+0.47}\) & \(18.91_{+0.85}\) \\  & CHGH & \(425.17_{+0.04}\) & \(109.82_{+0.06}\) & \(104.14_{+1.92}\) & \(63.54_{+0.50}\) & \(18.11_{+1.93}\) & \(15.16_{+0.43}\) & \(19.75_{+11.33}\) \\  & PreDyGAE & \(269.27_{+0.01}\) & \(85.05_{+0.96}\) & \(84.11_{+1.87}\) & \(56.28_{+0.72}\) & \(16.57_{+0.84}\) & \(9.51_{+0.39}\) & \(22.92_{+0.13}\) \\  & Transformer & \(460.53_{+2.11}\) & \(120.62_{+0.44}\) & \(113.11_{+1.19}\) & \(69.45_{+0.70}\) & \(22.42_{+1.95}\) & \(9.42_{+1.88}\) & \(26.93_{+1.42}\) \\  & Autoformer & \(632.29_{+2.76}\) & \(131.92_{+1.22}\) & \(131.01_{+1.22}\) & \(75.47_{+0.77}\) & \(25.82_{+0.93}\) & \(11.76_{+0.39}\) & \(36.65_{+0.8}\) \\  & Informer & \(462.88_{+1.99}\) & \(120.89_{+0.98}\) & \(113.36_{+1.32}\) & \(68.13_{+1.46}\) & \(22.22_{+2.91}\) & \(9.14_{+1.36}\) & \(26.82_{+1.06}\) \\  & Reformer & \(656.72_{+2.72}\) & \(120.58_{+0.84}\) & \(112.51_{+1.11}\) & \(69.02_{+1.79}\) & \(22.42_{+1.88}\) & \(5.091_{+1.44}\) & \(28.16_{+2.29}\) \\  & FEHDFore & \(652.02_{+6.45}\) & \(131.05_{+0.27}\) & \(130.18_{+2.25}\) & \(72.12_{+1.19}\) & \(23.06_{+2.34}\) & \(11.01_{+0.29}\) & \(32.89_{+1.1}\) \\  & NStransformer & \(258.16_{+1.4}\) & \(81.54_{+1.21}\) & \(82.02_{+2.09}\) & \(49.66_{+0.33}\) & \(16.88_{+0.26}\) & \(7.07_{+0.2}\) & \(28.31_{+0.09}\) \\  & PatchTIST & \(282.34_{+1.71}\) & \(79.14_{+3.75}\) & \(80.46_{+2.76}\) & \(48.45_{+3.13}\) & \(16.29_{+1.29}\) & \(7.01_{+0.41}\) & \(19.65_{+1.48}\) \\  & DLinear & \(277.33_{+2.32}\) & \(77.47_{+0.99}\) & \(77.64_{+3.23}\) & \(46.52_{+0.66}\) & \(16.03_{+0.08}\) & \(6.64_{+0.22}\) & \(18.73_{+0.51}\) \\  & TSMixer & \(698.72_{+0.76}\) & \(146.22_{+2.26}\) & \(130.91_{+2.4}\) & \(88.39_{+1.48}\) & \(28.49_{+1.48}\) & \(16.46_{+0.55}\) & \(79.42_{+0.65}\) \\  & FreTS & \(419.07_{+6.85}\) & \(108.63_{+1.75}\) & \(98.46_{+1.46}\) & \(62.17_{+1.54}\) & \(20.54_{+1.82}\) & \(8.47_{+0.54}\) & \(25.89_{+1.53}\) \\  & FiLM & \(277.66_{+3.44}\) & \(78.58_{+2.62}\) & \(69.66_{+1.22}\) & \(45.72_{+1.52}\) & \(15.26_{+0.87}\) & \(6.79_{+0.39}\) & \(19.86_{+0.88}\) \\  & Koopa & \(282.85_{+0.69}\) & \(78.75_{+1.42}\) & \(83.57_{+1.43}\) & \(48.75_{+1.55}\) & \(16.92_{+0.47}\) & \(6.936_{+1.33}\) & \(19.16_{+1.01}\) \\ \hline \multirow{11}{*}{**GCN**} & LSTM & \(2148.33_{+0.79}\) & \(534.61_{+0.67}\) & \(704.41_{+116.30}\) & \(466.21_{+0.24}\) & \(178.29_{+0.26}\) & \(58.54_{+3.62}\) & \(19.272_{+0.76}\) \\  & SegRNN & \(1310.19_{+0.81}\) & \(304.22_{+2.76}\) & \(387.11_{+1.34}\) & \(264.53_{+1.99}\) & \(109.27_{+1.44}\) & \(36.38_{+1.34}\) & \(102.84_{+0.96}\) \\  & CIGH & \(2153.77_{+0.87}\) & \(537.90_{+1.78}\) & \(37.54_{+1.75}\) & \(168.27_{+0.79}\) & \(17.17_{+1.48}\) & \(40.18_{+0.76}\) & \(20.37_{+0.88}\) & \(20.38_{+0.73}\) \\  & PreDyGAE & \(1510.41_{+0.61}\) & \(403.02_{+0.68}\) & \(529.39_{+1.83}\) & \(388.54_{+0.46}\) & \(159.45_{+1.71}\) & \(36.48_{+1.19}\) & \(16.02_{+0.48}\) \\  & Transformer & \(2154.53_{+0.361}\) & \(56.07_{+2.72}\) & \(176.71_{+2.55}\) & \(47.406_{+0.88}\) & \(179.25_{+0.64}\) & \(6.515_{+1.55}\) & \(19.91_{+1.88}\) \\  & Autoorter & \(2566.07_{+1.129}\) & \(489.09_{+0.95}\) & \(62.06_{+0.16}\) & \(201.60_{+0.33}\) & \(338.57_{+1.43}\) & \(155.94_{+1.41}\) & \(58.15_{+0.41}\) &

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

declines across the overall labor market. However, as the granularity of the forecast becomes finer, the model performance improves, and at a finer granularity, the EvolveGCN method outperforms the state-of-the-art (SOTA) methods mentioned in the main text considerably. We analyze that the finer the granularity, the more accurately the co-occurrence graph reflects the associations between skills, while coarser granularity might introduce excessive noise leading to decreased model performance. The fine-grained co-occurrence graph accurately reflects the interrelationships between skills at different granularities, which aids in enhancing the model's prediction accuracy. Secondly, we find significant differences in the RRMSE metric among these methods, with EvolveGCN showing superior performance because it can learn the evolution of GCN parameter weights over time, thus capturing the evolving dependencies among edges. Therefore, based on the provided co-occurrence graph, it can effectively learn the evolution of skill relationships, which is beneficial for dynamic prediction of skill demand.

For skill demand forecasting in scenarios involving structural breaks, as shown in Table 10, the improvements in the methods based on the co-occurrence graph are greater than those in the overall skill demand forecasting task. This suggests that skills experiencing structural breaks have strong interconnections, and the co-occurrence graph helps the model to identify the patterns of skill demand sequences that are likely to undergo structural breaks, thus further enhancing the prediction effectiveness for this category of skills.

In the task of predicting low-frequency skills, as shown in Table 11, methods like GConvLSTM significantly outperform EvolveGCN. This is due to the sparse observable data for these skills, which leads to sparse connectivity edges on the co-occurrence graph.

### Job Skill Demand Proportion Forecasting

In the main text, we discussed the issue of skill demand prediction. However, consider a scenario where the number of skill postings for a particular occupation is very low, leading to a low demand for that occupational skills. Nevertheless, these skills might constitute a significant portion of the profession's core competencies. Therefore, using skill demand alone may not adequately measure the importance of these skills within the occupation. To address this, we introduce an extended dataset that includes the skill demand propotion. We define the skill demand proportion as:

\[R^{i}_{s,t}=[R^{i}_{s,t,a^{i}}]_{a^{i}\in\mathcal{A}^{i}},\ \ R^{i}_{s,t,a^{i}}= \frac{\sum_{p\in\mathcal{P}_{t}}\mathbf{1}(s\in p)\cdot\mathbf{1}(a^{i}\in p)}{ \sum_{p\in\mathcal{P}_{t}}\mathbf{1}(a^{i}\in p)},\] (5)

\begin{table}
\begin{tabular}{c|c|c c c c c c c} \hline \hline  & Model & Market & Region & L1-O & L2-O & R&L1-O & R&L2-O & Company \\ \hline \multirow{8}{*}{**Cone**} & EvolveGCN & \(35.53\)\(\pm\)\(27.13\) & \(2.28\)\(\pm\)\(17.14\) & \(1.47\)\(\pm\)\(11.31\) & \(0.55\)\(\pm\)\(0.42\) & \(0.31\)\(\pm\)\(0.23\) & \(0.14\)\(\pm\)\(0.11\) & \(0.19\)\(\pm\)\(0.15\) \\  & EvolveGCN & \(27.87\)\(\pm\)\(27.32\) & \(3.56\)\(\pm\)\(25.76\) & \(1.69\)\(\pm\)\(19.70\) & \(0.57\)\(\pm\)\(0.43\) & \(0.31\)\(\pm\)\(0.24\) & \(0.27\)\(\pm\)\(0.21\) & \(0.17\)\(\pm\)\(0.15\) \\  & GConvGRU & \(63.05\)\(\pm\)\(63.16\) & \(3.06\)\(\pm\)\(23.44\) & \(0.25\)\(\pm\)\(19.10\) & \(0.18\)\(\pm\)\(0.14\) & \(0.11\)\(\pm\)\(0.09\) & \(0.05\)\(\pm\)\(0.04\) & \(0.11\)\(\pm\)\(0.09\) \\  & TGCN & \(7.37\)\(\pm\)\(5.53\) & \(2.16\)\(\pm\)\(15.65\) & \(0.13\)\(\pm\)\(0.11\) & \(0.61\)\(\pm\)\(0.46\) & \(0.26\)\(\pm\)\(0.21\) & \(0.15\)\(\pm\)\(0.12\) & \(0.15\)\(\pm\)\(0.11\) \\  & GCLSTM & \(0.62\)\(\pm\)\(0.47\) & \(0.41\)\(\pm\)\(0.32\) & \(1.37\)\(\pm\)\(0.14\) & \(0.15\)\(\pm\)\(0.11\) & \(0.12\)\(\pm\)\(0.09\) & \(0.09\)\(\pm\)\(0.07\) & \(0.12\)\(\pm\)\(0.09\) \\  & GConvLSTM & \(0.79\)\(\pm\)\(0.6\) & \(0.43\)\(\pm\)\(0.33\) & \(0.2\)\(\pm\)\(0.15\) & \(0.13\)\(\pm\)\(0.1\) & \(0.10\)\(\pm\)\(0.08\) & \(0.06\)\(\pm\)\(0.05\) & \(0.13\)\(\pm\)\(0.1\) \\  & DyGEncoder & \(10.14\)\(\pm\)\(0.79\) & \(0.47\)\(\pm\)\(0.36\) & \(0.24\)\(\pm\)\(0.18\) & \(0.13\)\(\pm\)\(0.1\) & \(0.15\)\(\pm\)\(0.11\) & \(0.09\)\(\pm\)\(0.07\) & \(0.14\)\(\pm\)\(0.11\) \\ \hline \multirow{8}{*}{**Cone**} & EvolveGCN & \(144.53\)\(\pm\)\(11.39\) & \(11.08\)\(\pm\)\(0.86\) & \(11.65\)\(\pm\)\(0.59\) & \(5.52\)\(\pm\)\(22.2\) & \(3.48\)\(\pm\)\(0.26\) & \(1.7\)\(\pm\)\(1.3\) & \(1.81\)\(\pm\)\(1.38\) \\  & EvolveGCN & \(83.54\)\(\pm\)\(0.71\) & \(1.79\)\(\pm\)\(19.21\) & \(12.82\)\(\pm\)\(0.79\) & \(5.74\)\(\pm\)\(3.48\) & \(3.8\)\(\pm\)\(0.2\) & \(1.95\)\(\pm\)\(1.49\) & \(1.65\)\(\pm\)\(2.16\) \\  & GConvGRU & \(63.06\)\(\pm\)\(0.16\) & \(3.65\)\(\pm\)\(2.79\) & \(1.19\)\(\pm\)\(0.91\) & \(0.87\)\(\pm\)\(0.66\) & \(1.59\)\(\pm\)\(12.2\) & \(0.81\)\(\pm\)\(0.62\) & \(1.72\)\(\pm\)\(1.31\) \\  & TGCN & \(21.13\)\(\pm\)\(1.64\) & \(8.8\)\(\pm\)\(0.69\) & \(3.12\)\(\pm\)\(0.86\) & \(3.12\)\(\pm\)\(23.2\) & \(2.31\)\(\pm\)\(17.1\) & \(1.10\)\(\pm\)\(0.85\) & \(1.78\)\(\pm\)\(1.36\) \\  & GCLSTM & \(1.63\)\(\pm\)\(1.24\) & \(2.48\)\(\pm\)\(1.89\) & \(2.8\)\(\pm\)\(2.14\) & \(0.86\)\(\pm\)\(0.66\) & \(1.61\)\(\pm\)\(12.2\) & \(0.83\)\(\pm\)\(0.63\) & \(1.73\)\(\pm\)\(1.32\) \\  & GConLSTM & \(1.23\)\(\pm\)\(0.94\) & \(2.1\)\(\pm\)\(1.6\) & \(1.14\)\(\pm\)\(0.87\) & \(0.85\)\(\pm\)\(0.65\) & \(1.6\)\(\pm\)\(12.2\) & \(0.81\)\(\pm\)\(0.62\) & \(1.74\)\(\pm\)\(1.33\) \\  & DyGEncoder & \(1.49\)\(\pm\)\(1.14\) & \(2.12\)\(\pm\)\(1.62\) & \(1.15\)\(\pm\)\(0.88\) & \(0.86\)\(\pm\)\(0.66\) & \(1.59\)\(\pm\)\(12.2\) & \(0.81\)\(\pm\)\(0.62\) & \(1.73\)\(\pm\)\(1.32\) \\ \hline \multirow{8}{*}{**Cone**} & EvolveGCN & \(47.39\)\(\pm\)\(36.19\) & \(42.25\)\(\pm\)\(33.27\) & \(34.5\)\(\pm\)\(20.35\) & \(25.49\)\(\pm\)\(19.47\) & \(20.27\)\(\pm\)\(15.48\) & \(12.66\)\(\pm\)\(0.97\) & \(21.3\)\(\pm\)\(16.27\) \\  & EvolveGCN & \(115.04\)\(\pm\)\(57.86\) & \(89.67\)\(\pm\)\(65.

where \(a^{i}\in p\) represents a job advertisement \(p\) containing the attribute \(a^{i}\) under granularity \(i\). Similarly, we can further define skill demand proportions \(R^{i,j,...,k}_{s,t}\) across multiple granularities \(\{i,j,...,k\}\) by

\begin{table}
\begin{tabular}{l|l l l l l l l l} \hline \hline  & Model & Market & Region & L1-O & L2-O & R\&L1-O & R\&L2-O & Company \\ \hline \multirow{9}{*}{\begin{tabular}{l} \(\mathrm{STM}\) \\ \(\mathrm{SEgRNN}\) \\ \end{tabular} } & LSTM & \(0.14\pm_{0.0}\) & \(0.49\pm_{0.01}\) & \(1.21\pm_{0.03}\) & \(2.26\pm_{0.05}\) & \(2.43\pm_{0.03}\) & \(3.51\pm_{0.06}\) & \(2.69\pm_{0.03}\) \\  & \(\mathrm{SEgRNN}\) & \(0.15\pm_{0.02}\) & \(1.25\pm_{0.02}\) & \(2.63\pm_{0.11}\) & \(4.2\pm_{0.04}\) & \(6.6\pm_{1.12}\) & \(10.16\pm_{2.44}\) & \(5.52\pm_{0.53}\) \\  & \(\mathrm{CHGH}\) & \(0.14\pm_{0.01}\) & \(0.19\pm_{0.0}\) & \(0.35\pm_{0.0}\) & \(0.64\pm_{0.01}\) & \(0.7\pm_{0.0}\) & \(0.91\pm_{0.07}\) & \(1.03\pm_{0.01}\) \\  & \(\mathrm{PreDyGAE}\) & \(0.08\pm_{0.01}\) & \(0.09\pm_{0.0}\) & \(0.12\pm_{0.02}\) & \(0.23\pm_{0.02}\) & \(0.18\pm_{0.0}\) & \(0.27\pm_{0.15}\) & \(0.29\pm_{0.16}\) \\  & \(\mathrm{Transformer}\) & \(0.62\pm_{0.02}\) & \(3.89\pm_{0.02}\) & \(10.87\pm_{0.05}\) & \(20.58\pm_{0.01}\) & \(22.02\pm_{0.01}\) & \(31.26\pm_{0.02}\) & \(23.47\pm_{0.01}\) \\  & \(\mathrm{Autoformer}\) & \(1.29\pm_{0.14}\) & \(8.84\pm_{1.05}\) & \(24.5\pm_{0.22}\) & \(43.99\pm_{0.94}\) & \(47.53\pm_{0.67}\) & \(66.32\pm_{0.12}\) & \(0.502\pm_{0.66}\) \\  & \(\mathrm{Informer}\) & \(0.55\pm_{0.02}\) & \(3.83\pm_{0.03}\) & \(10.86\pm_{0.03}\) & \(20.6\pm_{0.01}\) & \(20.24\pm_{0.01}\) & \(31.23\pm_{0.01}\) & \(23.48\pm_{0.0}\) \\  & \(\mathrm{RefFormer}\) & \(0.56\pm_{0.01}\) & \(3.85\pm_{0.01}\) & \(10.9\pm_{0.03}\) & \(20.65\pm_{0.02}\) & \(22.01\pm_{0.01}\) & \(31.25\pm_{0.01}\) & \(23.48\pm_{0.0}\) \\  & \(\mathrm{FEDformer}\) & \(1.42\pm_{0.04}\) & \(8.58\pm_{1.03}\) & \(25.11\pm_{0.55}\) & \(41.15\pm_{0.45}\) & \(47.07\pm_{0.06}\) & \(6.59\pm_{0.23}\) & \(50.0\pm_{0.07}\) \\  & \(\mathrm{NStransformer}\) & \(0.06\pm_{0.0}\) & \(0.09\pm_{0.0}\) & \(0.11\pm_{0.0}\) & \(0.19\pm_{0.0}\) & \(0.24\pm_{0.0}\) & \(0.34\pm_{0.0}\) & \(0.34\pm_{0.0}\) \\  & \(\mathrm{PatchTST}\) & \(0.06\pm_{0.0}\) & \(0.09\pm_{0.01}\) & \(0.11\pm_{0.02}\) & \(0.18\pm_{0.04}\) & \(0.23\pm_{0.05}\) & \(0.33\pm_{0.07}\) & \(0.33\pm_{0.06}\) \\  & \(\mathrm{DLinear}\) & \(0.27\pm_{1.15}\) & \(1.65\pm_{1.07}\) & \(4.63\pm_{1.09}\) & \(8.76\pm_{0.86}\) & \(9.4\pm_{0.26}\) & \(13.33\pm_{8.89}\) & \(10.05\pm_{0.65}\) \\  & \(\mathrm{TSMixer}\) & \(0.85\pm_{0.33}\) & \(3.15\pm_{0.69}\) & \(7.57\pm_{1.43}\) & \(18.88\pm_{0.49}\) & \(15.12\pm_{1.35}\) & \(18.76\pm_{2.85}\) & \(1.77\pm_{1.58}\) \\  & \(\mathrm{FreTS}\) & \(0.12\pm_{0.0}\) & \(0.31\pm_{0.04}\) & \(0.69\pm_{0.11}\) & \(1.26\pm_{0.21}\) & \(1.37\pm_{0.22}\) & \(1.94\pm_{0.32}\) & \(1.54\pm_{0.24}\) \\  & \(\mathrm{FiLM}\) & \(0.06\pm_{0.01}\) & \(0.09\pm_{0.01}\) & \(0.12\pm_{0.01}\) & \(0.19\pm_{0.02}\) & \(0.25\pm_{0.03}\) & \(0.35\pm_{0.05}\) & \(0.34\pm_{0.05}\) \\  & \(\mathrm{Koopa}\) & \(0.06\pm_{0.0}\) & \(0.08\pm_{0.0}\) & \(0.09\pm_{0.0}\) & \(0.15\pm_{0.01}\) & \(0.2\pm_{0.01}\) & \(0.27\pm_{0.02}\) & \(0.28\pm_{0.01}\) \\ \hline \multirow{9}{*}{
\begin{tabular}{l} \(\mathrm{STM}\) \\ \(\mathrm{SEgRNN}\) \\ \end{tabular} } & LSTM & \(0.61\pm_{0.01}\) & \(1.74\pm_{0.04}\) & \(2.89\pm_{0.08}\) & \(4.06\pm_{0.09}\) & \(4.16\pm_{0.05}\) & \(5.21\pm_{0.1}\) & \(4.43\pm_{0.05}\) \\  & \(\mathrm{SegRNN}\) & \(1.32\pm_{0.17}\) & \(4.98\pm_{0.11}\) & \(6.49\pm_{0.22}\) & \(7.7\pm_{0.19}\) & \(11.23\pm_{1.78}\) & \(14.58\pm_{3.23}\) & \(9.18\pm_{0.83}\) \\  & \(\mathrm{CHGH}\) & \(0.36\pm_{0.01}\) & \(0.49\pm_{0.0}\) & \(0.7\pm_{0.0}\) & \(1.12\pm_{0.02}\) & \(1.14\pm_{0.0}\) & \(2.11\pm_{0.32}\) & \(2.31\pm_{0.07}\) \\  & \(\mathrm{PreDyGAE}\) & \(0.23\pm_{0.0}\) & \(0.29\pm_{0.0}\) & \(0.36\pm_{0.0}\) & \(0.85\pm_{0.0}\) & \(0.91\pm_{0.0}\) & \(1.15\pm_{0.15}\) & \(1.29\pm_{0.72}\) \\  & \(\mathrm{Transformer}\) & \(6.22\pm_{0.12}\) & \(1.66\pm_{0.05}\) & \(2.79\pm_{0.14}\) & \(38.45\pm_{0.03}\) & \(0.39\pm_{0.77}\) & \(37.40\pm_{0.02}\) & \(40.7\pm_{0.01}\) \\  & \(\mathrm{Autoformer}\) & \(1.11\pm_{2.51}\) & \(38.94\pm_{0.6calculating:

\[R^{i,j,...,k}_{s,t,\overline{a}}=\frac{\sum_{p\in\mathcal{P}_{t}}\mathbf{1}(s\in p )\cdot\mathbf{1}(a^{i}\in p\wedge a^{j}\in p\wedge...\wedge a^{k}\in p)}{\sum_{ p\in\mathcal{P}_{t}}\mathbf{1}(a^{i}\in p\wedge a^{j}\in p\wedge...\wedge a^{k}\in p)},\] (6)

where \(\overline{a}=\{a^{i},a^{j},...,a^{k}\}\), \(a^{i}\in\mathcal{A}^{i},a^{j}\in\mathcal{A}^{j},...,a^{k}\in\mathcal{A}^{k}\), and \(R^{i,j,...,k}_{s,t}\in\mathbb{R}^{|\mathcal{A}^{i}||\mathcal{A}^{j}|...| \mathcal{A}^{k}|}\).

ResultsWe continue to utilize the benchmark models described in the main text for this task, and the results, as shown in Table 12, lead to the following conclusions: Firstly, the best-performing model on the task of forecasting the proportion of skill demand is Koopa. This model, integrating time series decomposition and Fourier transformations, effectively captures the distribution changes in demand proportions. Secondly, there is a significant variation in performance across models in this task. For example, models like DLinear perform poorly on this task, though they are reasonably effective in skill demand forecasting. We analyze that predicting percentages is distinct from forecasting skill demand, as percentage predictions are also influenced by the demand for other skills at the same granularity. Therefore, simple linear models are not advantageous for capturing the complex interrelations and influences among multiple pieces of information.

## Appendix E Data Structure and Components

Our dataset comprises five components for each granularity level: job skill demand sequences, job skill demand proportion sequences, ID mapping index, the indexes of skills with structural breaks, and skill co-occurrence graph. Each component is structured as follows: (1) _Job Skill Demand Sequences:_ These are presented in tabular files, where each row represents a specific skill, and each column corresponds to a different time slice (month). Each cell within the table contains a numerical value that reflects the demand for the respective skill during that month. (ii) _Job Skill Demand Proportion Sequences:_ This component is also formatted in tabular files similar to the skill demand sequences. However, each cell in these tables displays a value between 0 and 1, representing the proportion of demand defined in Eq 6. This provides a normalized view of skill demand across different granularities. (iii) _ID mapping index:_ In the dataset, various elements such as regions, occupations, companies, and skills are represented using unique identifiers (IDs) for the convenience of experimentation and analysis. An index table is provided that maps each ID to the actual names of regions, occupations, and skills, facilitating clear and effective data interpretation. The names of companies, however, are withheld due to potential privacy concerns. This selective anonymization ensures that while company-related data remains confidential, researchers can still access necessary details about other variables. During the review period, we open part of id-to-entity mappings to illustrate its use and structure. (iv) _Indexes of skills with structural breaks:_ In the provided dataset, data concerning skills that have experienced structural breaks are organized in JSON format. Each granularity level is represented by a separate JSON file, which contains a list of indexes. These indexes correspond to the skills that have undergone structural breaks and can be directly mapped to the skill indexes in the skill demand sequences. The purpose of supplying this data is to facilitate research on the demand trends of skills that have exhibited structural breaks, enabling a detailed analysis of their demand dynamics over time. (v) _Skill Co-occurrence Graph:_ This data is provided as a set of triples (skill ID_1, skill ID_2, frequency of co-occurrence), forming a collection that outlines the co-occurrence relationships between skills. Each triple indicates how frequently two skills are mentioned or required together within the job advertisements in the training data, serving as a prior knowledge graph to enhance predictive modeling by capturing relationships between skills.

## Appendix F Datasheet for Datasets

### Motivation

* **For what purpose was the dataset created?** The dataset in this paper was created specifically for the task of skill demand forecasting. The creation of this dataset aims to address two primary objectives: first, to fill the existing gap in publicly available datasets for skill demand forecasting; second, to offer a dataset that encompasses multiple levels of granularity. This diversity in granularity enables the possibility of predicting skill trends at various levels of detail. By providing such a comprehensive dataset, researchers can gain a more thorough understanding of and ability to predict changes in skill demand, thereby facilitating job seekers in finding suitable positions and aids recruiters in hiring the right talent. Furthermore, this dataset can support the development of education, training, and employment policies.
* **Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?** Our dataset is jointly developed by a collaborative effort from the following affiliations: University of Science and Technology of China, Computer Network Information Center, Chinese Academy of Sciences, Institute of Artificial Intelligence, Beihang University, and Artificial Intelligence Thrust, The Hong Kong University of Science and Technology (Guangzhou).
* **Who funded the creation of the dataset?** This work was supported by University of Science and Technology of China, Computer Network Information Center, Chinese Academy of Sciences, and The Hong Kong University of Science and Technology (Guangzhou).

### Composition

* **What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?** See the Section E.
* **How many instances are there in total (of each type, if appropriate)?** We collected millions of public job advertisements, covering 521 companies, 52 occupations, and 7 regions. We further extracted 2335 skills and tracked the monthly demand for these skills, thus ultimately forming sequences of demand for 2335 skills over 36 months, broken down by company, occupation, and region.
* **Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?** Yes, the dataset encompasses an exhaustive collection of job recruitment demands posted by various companies on online platforms. In the subsequent data processing stages, we employed rule-based matching and Named Entity Recognition (NER) models to extract as many relevant skill demands as possible from each job advertisement. Additionally, we filtered out certain implausible skill terms based on skill frequency analysis and manual review. Therefore, the dataset includes all possible instances except for those terms deemed unreasonable and filtered out during the review process.
* **What data does each instance consist of?** See the Subsection 3.1 and Section E.
* **Is there a label or target associated with each instance?** Yes, given that our dataset is a time series collection, it is structured for unsupervised learning. This means there are no explicit labels or target variables associated with each instance. Instead, the data is used to observe and analyze patterns over time, particularly in skill demand and relationships between skills, without predefined outcomes or classifications.
* **Is any information missing from individual instances?** N/A
* **Are relationships between individual instances made explicit (e.g., user movie ratings, social network links)?** We supplemented our analysis with a co-occurrence graph to explicitly represent the relationships between skills based on their co-occurrence in various contexts.
* **Are there recommended data splits (e.g., training, development/validation, testing)?** Yes, we recommended the data split in Subsection 3.2.
* **Are there any errors, sources of noise, or redundancies in the dataset?** Yes. See the Subsection 3.1
* **Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?** The dataset is self-contained for the tasks described in the paper, which is collected from online websites. a) We can not ensure the linked resources exist for a long time and consistent. b) There is no official archive related to the linked resource. c) There are no restrictions.
* **Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?** No.
* **Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?** No.

### Collection Process

* How was the data associated with each instance acquired? See the Section 3.1.
* What mechanisms or procedures were used to collect the data? See Section 3.1.
* If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? No.
* Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? We have collaborated with industry experts who have relevant experience to assist in data annotation for skill dictionary construction. Their participation is voluntary and unpaid.
* Over what timeframe was the data collected? The raw data, consisting of job advertisements, was collected over a period in the past three years.
* Were any ethical review processes conducted (e.g., by an institutional review board)? No.

### Preprocessing/cleaning/labeling

* Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes. See the Subsection 3.1.
* Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? No.
* Is the software that was used to preprocess/clean/label the data available? No.

### Uses

* Has the dataset been used for any tasks already? No.
* Is there a repository that links to any or all papers or systems that use the dataset? No.
* What (other) tasks could the dataset be used for? N/A
* Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? N/A.
* Are there tasks for which the dataset should not be used? N/A.

### Distribution

* Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? No. Our dataset will be managed and maintained by our research group.
* How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The evaluation dataset is released to the public and hosted on GitHub.
* When will the dataset be distributed? It has been released now.
* Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? Our dataset will be distributed under the CC BY-SA 4.0 license.
* Have any third parties imposed IP-based or other restrictions on the data associated with the instances? No.
* Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? No.

### Maintenance

* Who will be supporting/hosting/maintaining the dataset? The authors.
* How can the owner/curator/manager of the dataset be contacted (e.g., email address)? By the email address.

* **Is there an erratum?** N/A.
* **Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?** Yes, the dataset is expected to be updated with additional job advertisements collected over extended periods, potentially on a biannual basis. These updates will focus on increasing our data volume over time. All updates will be made available and communicated to dataset consumers via GitHub.
* **If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?** N/A.
* **Will older versions of the dataset continue to be supported/hosted/maintained?** We plan to maintain the newest version only.
* **If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?** Yes. Contact the authors of the paper.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section X.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Please refer to the Abstract and Introduction sections. 2. Did you describe the limitations of your work? Please refer to dataset limitation in Section 3.3. 3. Did you discuss any potential negative societal impacts of your work? Please refer to the Appendix A. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? This work adheres to the NeurIPS Code of Ethics in each respect.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? All the code, data, and instructions needed to reproduce the main experimental results can be found at https://github.com/Job-SDF/benchmark. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We have provided the detailed implementation descriptions in the Appendix. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Due to the page limit, we report error bars in the Appendix. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? The experimental environment is reported in the Appendix.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] We have cited the original paper that produced the code package. 2. Did you mention the license of the assets? [Yes] We have stated the detailed version and license of each asset. See the Appendix for the details. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] We have released a new dataset and the codes for the benchmark models. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Please refer to the Appendix. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? The proposed dataset does not contain personally identifiable information or offensive content. See the Appendix for the details.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]