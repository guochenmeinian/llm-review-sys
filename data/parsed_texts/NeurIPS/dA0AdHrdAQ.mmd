# ELCC: the Emergent Language Corpus Collection

 Brendon Boldt

Language Technologies Insitute

Carnegie Mellon University

Pittsburgh, PA 15213

bboldt@cs.cmu.edu &David Mortensen

Language Technologies Insitute

Carnegie Mellon University

Pittsburgh, PA 15213

dmortens@cs.cmu.edu

###### Abstract

We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora collected from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex tasks like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, limits the variety of languages that are studied, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable new directions of research which focus their purview on the properties of emergent languages themselves rather than on experimental apparatus.

## 1 Introduction

Emergent communication (also called _emergent language_) studies machine learning simulations that attempt to model the development of communication systems from scratch. These simulations have recently been argued to have many potential applications, both in artificial intelligence and in the scientific study of human communicative behavior [14]. However, this potential has been stymied, due in part to the challenges--up to this point--in comparing emergent communication systems (ECSs)1 in a way that allows their properties and utility to be understood.

Footnote 1: Emergent communications systems are more commonly referred to as simply “environments”; we choose to use the term “system” in order to emphasize that what goes into producing an emergent language is more than just an environment including also the architecture of the agents, optimization procedure, datasets, and more.

There have been no standard datasets, for example, that represent output from various systems across the literature. The only recourse, for researchers desiring to compare ECSs or the languages that they generate, has been to reimplement or collect code for earlier ECSs. Getting this code to run is a significant challenge, requiring the research to chase outdated versions of Python, install CUDA drivers (again), and work with sparsely documented code. The results of such efforts, too, would lack comparability and reproducibility. To our knowledge, efforts at large-scale comparison between different outputs of different ECSs are not reported in the literature.

This paper introduces the Emergent Language Corpus Collection (ELCC), a resource that addresses this glaring lacuna in the field of emergent communication research; namely, it is a set of corporaof "languages" generated by most of the prominent ECS types described in the current literature (accompanied by extensive metadata regarding the typology of the ECS, how the data were produced, and the statistical properties of the resulting corpus). With these data, it is possible for researchers, even those with limited technological expertise, to compare emergent languages, whether in their structural properties [22] or their utility in pretraining models for downstream NLP tasks [23]. ELCC is published on Hugging Face Datasets at https://huggingface.co/datasets/bboldt/elcc with data and code licensed under the CC BY 4.0 and MIT licenses, respectively.

We discuss related work in Section 2. Section 3 lays out the design of ELCC while Section 4 describes the content of the collection. Section 5 presents some brief analyses, discussion, and future work related to ELCC. Finally, we conclude in Section 6.

ContributionsThe primary contribution of this paper is a first-of-its kind data resource which will enable broader engagement and new research direction within the field of emergent communication. Additionally, code published for reproducing the data resource also improve the reproducibility of existing ECS implementations in the literature, supporting further research beyond just the data resource itself.

## 2 Related Work

Emergent communicationThere is no direct precedent to this paper in the emergent communication literature that we are aware of. Perkins (2021) introduces the TexRel dataset, but this is a dataset observations for training ECSs, not data generated by them. Some papers do provide the emergent language corpora generated from their experiments (e.g., Yao et al. (2022)), although these papers are few in number and only include the particular ECS used in the paper. At a high level, the EGG framework [17] strives to make emergent languages easily accessible, though instead of providing corpora directly, it provides a framework for implementing ECSs. Thus, while EGG is more useful for someone building new systems entirely, it is not as geared towards research projects aiming directly at analyzing emergent languages themselves.

Data resourcesAt a high level, ELCC is a collection of different subdatasets which all represent various manifestations of common phenomenon (emergent communication, in this case). On a basic level, ELCC could is somewhat analogous to any multi-lingual dataset (where "human language" is the phenomenon). Taking the notion of "phenomenon" more narrowly (i.e., of more direct scientific interest), it could be compared to Blum et al. (2023), which presents a collection of grammar snapshot pairs for \(52\) different languages as instances of diachronic language change. Zheng et al. (2024) present a dataset of conversations from Chatbot Arena containing where "text generated by different LLMs" is the phenomenon of interest. Insofar as ELCC documents the basic typology of different ECSs, it is similar to the World Atlas of Language Structures (WALS) Dryer and Haspelmath (2013).

## 3 Design

### Format

ELCC is a collection of ECSs each of which has one or more associated _variants_ which correspond to runs with different hyperparameter settings (e.g., different random seed, message length, dataset). Each variant has metadata along with the corpus generated from its settings. Each ECS has its own metadata as well and code to generate the corpus and metadata of each variant. The structure of ELCC is illustrated in Figure 1.

ECS metadataEnvironment metadata provides a basic snapshot of a given system and where it falls in the taxonomy of ECSs. As the collection grows, this makes it easier to ascertain the contents of the collection and easily find the most relevant corpora for a given purpose. This metadata will also serve as the foundation for future analyses of the corpora looking how the characteristics of an ECS relate to the properties of its output.

* The URL of where the code for producing the data is from.
* The URL of the original repo if **source** is a fork.
* The URL of the paper documenting the ECS (if any).
* The high level category of the game implemented in the ECS; currently one of _signalling_, _conversation_, or _navigation_.
* A finer-grained categorization of the game, if applicable.
* The type of observation that the agents make; currently either _vector_ or _image_ (i.e., an image embedding).
* Whether or not the observation is continuous as opposed to discrete (e.g., image embeddings versus concatenated one-hot vectors).
* Whether the data being communicated about is from a natural source (e.g., pictures), synthetic, or does not apply (e.g., in a social deduction game).
* A dictionary where each entry corresponds to one of the variants of the particular ECS. Each entry in the dictionary contains any relevant hyperparameters that distinguish it from the other variants.
* Whether or not the ECS implements seeding the random elements of the system.
* Whether or not the ECS has multiple steps per episode.
* Whether or not agents both send and receive messages.
* Whether or not multiple utterances are included per line in the dataset.
* Whether or not the ECS has a population of \(>\)2 agents.

These metadata are stored as YAML files in each ECS directory. A Python script is provided to validate these entries against a schema. See Appendix A for an example of such a metadata file.

CorpusEach _corpus_ comprises a list of _lines_ each of which is, itself, an array of _tokens_ represented as integers. Each line corresponds to a single episode or round in the particular ECS. In the case of multi-step or multi-agent systems, this might comprise multiple individual utterances which are then concatenated together to form the line (no separation tokens are added). Each corpus is generated from a single run of the ECS; that is, they are never aggregated from distinct runs of the ECS.

Concretely, a _corpus_ is formatted as a JSON lines (JSONL) file where each _line_ is a JSON array of integer _tokens_. This is visualized in Figure 2. There are a few advantages of JSONL: (1) it is JSON-based meaning it is standardized and has wide support across programming languages, (2) it is easy to interpret and work with for those without a computer science background, and (3) it is

Figure 1: The file structure of ELCC.

line-based meaning it is easy to process with command line tools.2 Corpora are also available as single JSON objects (i.e., and array of arrays), accessible via the Croissant ecosystem.

Footnote 2: https://github.com/journals/cai/cai/cai/cai.

Corpus analysisFor each corpus in ELCC we run a suite of analyses to produce a quantitative snapshot. This suite metrics is intended not only to paint a robust a picture of the corpus but also to serve as jumping-off point for future analyses on the corpora. Specifically, we apply the following to each corpus: token count, unique tokens, line count, unique lines, tokens per line, tokens per line stand deviation, \(1\)-gram entropy, normalized \(1\)-gram entropy, entropy per line, \(2\)-gram entropy, \(2\)-gram conditional entropy, EoS token present, and EoS padding. _Normalized \(1\)-gram entropy_ is computed as \(1\)_-gram entropy_ divided by the maximum entropy given the number of unique tokens in that corpus.

We consider an EoS (end-of-sentence) token to be present when: (1) every line ends with token consistent across the entire corpora, and (2) the first occurrence of this token in a line is only ever followed by more the same token. For example, 0 could be an EoS token in the corpus [[1,2,0],[1,0,0]] but not [[1,2,0],[0,1,0]]. EoS padding is defined as a corpus having an EoS token, all lines being the same length, and the EoS token occurs more than once in a line at least once in the corpus.

Additionally, each corpus also has a small amount of metadata copied directly from the output of the ECS; for example, this might include the success rate in a signalling game environment. We do not standardize this because it can vary widely from ECS to ECS, though it can still be useful for comparison to other results among variants within an ECS.

ReproducibilityELCC is designed with reproducibility in mind. With each ECS, code is included to reproduce the corpora and analysis metadata. Not only does this make ELCC reproducible, but it sometimes helps the reproducibility of the underlying implementation insofar as it fixes bugs, specifies Python environments, and provides examples of how to run an experiment with a certain set of hyperparameters. Nevertheless, in this code, we have tried to keep as close to the original implementations as possible. When the underlying implementation supports it, we set the random seed (or keep the default) for the sake of consistency, although many systems do not offer ways to easily set this.

\begin{table}
\begin{tabular}{l l l l r} \hline \hline Source & Type & Data source & Multi-agent & Multi-step & \(n\) corp. \\ \hline Kharitonov et al. (2021) & signalling & synthetic & No & No & \(15\) \\ Yao et al. (2022a) & signalling & natural & No & No & \(2\) \\ Mu and Goodman (2021b) & signalling & both & No & No & \(6\) \\ Chaabouni et al. (2022) & signalling & natural & Yes & No & \(5\) \\ Unger and Bruni (2020) & navigation & synthetic & No & Yes & \(18\) \\ Boldt and Mortensen (2022) & navigation & synthetic & No & Yes & \(20\) \\ Brandizzi et al. (2022) & conversation & — & Yes & Yes & \(7\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Taxonomic summary the contents of ELCC.

Figure 2: Example of a three-line corpus in the JSON lines format.

## 4 Content

ELCC contains \(73\) corpora across \(8\) ECSs taken from the literature for which free and open source implementations were available. With our selection we sought to capture variation across a three distinct dimensions:

1. Variation across ECSs generally, including elements like game types, message structure, data sources, implementation details.
2. Variation among different hyperparameter settings within an ECS, including message length, vocabulary size, dataset, and game difficulty.
3. Variation within a particular hyperparameter setting that comes from inherent stochasticity in the system; this is useful for gauging the stability or convergence of an ECS.

Table 1 shows an overview of the taxonomy of ELCC based on the ECS-level metadata. In addition to this, Table 2 provides a quantitative summary of the corpus-level metrics described in Section 3.1 (full results in Appendix C). The following sections describe the individual ECSs in more detail.

### Scope

The scope of the contents of ELCC is largely the same as discussed in reviews such as Lazaridou and Baroni (2020) and Boldt and Mortensen (2024, Section 1.2). This comprises agent-based models for simulating the formation of "natural" language from scratch using deep neural networks. Importantly, _from scratch_ means that the models are not pretrained or tuned on human language. Typically, such simulations make use of reinforcement learning to train the neural networks, though this is not a requirement in principle.

One criterion that we do use to filter ECSs for inclusion is its suitability for generating corpora as described above. This requires that the communication channel is discrete, analogous to the distinct words/morphemes which for the units of human language. This excludes a small number of emergent communication papers have approached emergent communication through constrained continuous channels like sketching (Mihai and Hare, 2021) or acoustic-like signals (Eloff et al., 2023). Other systems use discrete communication but are in essence one token per episode (e.g., Tucker et al. (2021)) which would not form a suitable corpus for addressing most research questions.

### Signalling games

The _signalling game_ (or _reference game_) (Lewis, 1969) represents a plurality, if not majority, of the systems present in the literature. A brief, non-exhaustive review of the literature yielded \(43\) papers

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & min & \(25\%\) & \(50\%\) & \(75\%\) & max \\ \hline Token Count & \(48616\) & \(67248\) & \(110000\) & \(1061520\) & \(42977805\) \\ Line Count & \(999\) & \(5765\) & \(10000\) & \(10000\) & \(2865187\) \\ Tokens per Line & \(5.87\) & \(7.00\) & \(11.00\) & \(33.53\) & \(7212.72\) \\ Tokens per Line SD & \(0.00\) & \(0.00\) & \(2.31\) & \(13.81\) & \(445.84\) \\ Unique Tokens & \(2\) & \(7\) & \(10\) & \(20\) & \(902\) \\ Unique Lines & \(18\) & \(1253\) & \(2440\) & \(4911\) & \(309405\) \\
1-gram Entropy & \(0.36\) & \(2.12\) & \(2.80\) & \(3.37\) & \(6.60\) \\
1-gram Normalized Entropy & \(0.16\) & \(0.71\) & \(0.82\) & \(0.90\) & \(1.00\) \\
2-gram Entropy & \(0.42\) & \(3.16\) & \(4.11\) & \(5.88\) & \(12.88\) \\
2-gram Conditional Entropy & \(0.06\) & \(0.85\) & \(1.41\) & \(2.54\) & \(6.29\) \\ Entropy per Line & \(4.38\) & \(21.23\) & \(30.80\) & \(71.85\) & \(30233.52\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Five-number summar of the analyses across corpora of ELCC. Entropy in bits.

which use minor variations of the signalling game, a large number considering the modest body of emergent communication literature (see Appendix B). The basic format of the signalling game is a single round of _sender_ agent making an observation, passing a message to the _receiver_ agent, and the receiver performing an action based on the information from the message. The popularity of this game is, in large part, because of its simplicity in both concept and implementation. Experimental variables can be manipulated easily while introducing minimal confounding factors. Furthermore, the implementations can entirely avoid the difficulties of reinforcement learning by treating the sender and receiver agents as a single neural network, resulting in autoencoder with a discrete bottleneck which can be trained with backpropagation and supervised learning.

The two major subtypes of the signalling game are the _discrimination game_ and the _reconstruction game_. In the discrimination game, the receiver must answer a multiple-choice question, that is, select the correct observation from among incorrect "distractors". In the reconstruction game, the receiver must recreate the input directly, similar to the decoder of an autoencoder.

VanillaFor the most basic form of the signalling game, which we term "vanilla", we use the implementation provided in the Emergent of LanGuage in Games (EGG) framework [14, 15]. It is vanilla insofar as it comprises the signalling game with the simplest possible observations (synthetic, concatenated one-hot vectors), a standard agent architecture (i.e., RNNs) and no additional dynamics or variations on the game. Both the discrimination game and the reconstruction game are included. This system provides a good point of comparison for other ECSs which introduce variations on the signalling game. The simplicity of the system additionally makes it easier to vary hyperparameters: for example, the size of the dataset can be scaled arbitrarily and there is no reliance on pretrained embedding models.

Natural images"Linking emergent and natural languages via corpus transfer" [23, 16] presents a variant of the signalling game which uses embeddings of natural images as the observations. In particular, the system uses embedded images from the MS-COCO and Conceptual Captions datasets consisting of pictures of everyday scenes. Compared to the uniformly sampled one-hot vectors in the vanilla setting, natural image embeddings are real-valued with a generally smooth probability distribution rather than being binary or categorical. Furthermore natural data distributions are not uniform and instead have concentrations of probability mass on particular elements; this non-uniform distribution responsible for various features of human language (e.g., human languages' bias towards describing warm colors [17, 18]).

Differing observations"Emergent communication of generalizations" [24, 16] presents a variant of the discrimination signalling game which they term the _concept game_. The concept game changes the way that the sender's observation corresponds with the receiver's observations. In the vanilla discrimination game, the observation the sender sees is exactly the same as the correct observation that the sender sees. In the concept game, the sender instead observes a set of inputs which share a particular concept (e.g., red triangle and red circle are both red), and the correct observation (among distractors) shown to the receiver contains the same concept (i.e., red) while not being identical to those observed by the sender. The rationale for this system is that the differing observations will encourage the sender to communicate about abstract concepts rather than low-level details about the observation. This ECS also presents the vanilla discrimination game as well as the _sertef game_, which is similar to the reference game except that the whole object is consistent (e.g., different sizes and locations of a red triangle).

Multi-agent population"Emergent communication at scale" [15, 16] presents a signalling game system with populations of agents instead of the standard fixed pair of sender and receiver. For each round of the game, then, a random sender is paired with a random receiver. This adds a degree of realism to the system, as natural human languages are ways developed within a population and not just between two speakers (cf. idioglossia). More specifically, language developing among a population of agents prevents some degree "overfitting" between sender and receiver; in this context, having a population of agents functions as an ensembling approach to regularization.

### Other games

Considering that the signalling game is close to the simplest possible game for an ECS, moving beyond the signalling game generally entails an increase in complexity. There is no limit to the theoretical diversity of games, although some of the most common games that we see in the literature are conversation-based games (e.g., negotiation, social deduction) and navigation games. These games often introduce new aspects to agent interactions like: multi-step episodes, multi-agent interactions, non-linguistic actions, and embodiment.

These kinds of systems, as a whole, are somewhat less popular in the literature. On a practical level, more complex systems are more complex to implement and even harder to get to converge reliably--many higher-level behaviors, such as planning or inferring other agent's knowledge, are difficult problems for reinforcement learning in general, let alone with discrete multi-agent emergent communication. On a methodological level, more complexity in the ECS makes it harder to formally analyze the system as well as eliminate confounding factors in empirical investigation. With so many moving parts, it can be difficult to prove that some observed effect is not just a result of some seemingly innocent hyperparameter choice (e.g., learning rate, samples in the rollout buffer) (Boldt and Mortensen, 2022). Nevertheless, we have reason to believe that these complexities are critical to understanding and learning human language as a whole (Bisk et al., 2020), meaning that the difficulties of more complex systems are worth overcoming as they are part of the process of creating more human-like, and therefore more useful, emergent languages.

Grid-world navigation"Generalizing Emergent Communication" (Unger and Bruni, 2020, BSD-3-clause license) introduces an ECS which takes some of the basic structure of the signalling game and applies it to a navigation-based system derived from the synthetic Minigrid/BabyAI environment (Chevalier-Boisvert et al., 2018, 2023). A sender with a bird's-eye view of the environment sends messages to a receiver with a limited view who has to navigate to a goal location. Beyond navigation, some environments present a locked door for which the receiver must first pick up a key in order to open.

What distinguishes this system most from the signalling game is that it is multi-step and embodied such that the utterances within an episodes are dependent on each other. Among other things, this changes the distribution properties of the utterances. For example, if the receiver is in Room A at timestep \(T\), it is more likely to be in Room A at timestep \(T+1\); thus if utterances are describing what room the receiver is in, this means that an utterance at \(T+1\) has less uncertainty given the content of an utterance at \(T\). Practically speaking, the multiple utterances in a given episode are concatenated together to form a single line in the corpus in order to maintain the dependence of later utterances on previous ones.

Continuous navigation"Mathematically Modeling the Lexicon Entropy of Emergent Language" (Boldt and Mortensen, 2022, GPL-3.0 license) introduces a simple navigation-based ECS which is situated in a continuous environment. A "blind" receiver is randomly initialized in an obstacle-free environment and must navigate toward a goal zone guided by messages from the center which observes the position of the receiver relative to the goal. The sender sends a single discrete token at each timestep, and a line in the dataset consists of the utterances from each timestep concatenated together. This system shares the time-dependence between utterances of the grid-world navigation system although with no additional complexity of navigating around obstacle, opening doors, etc. On the other hand, the continuous nature of this environment provides built-in stochasticity since there are (theoretically) infinitely many distinct arrangements of the environment that are possible, allowing for more natural variability in the resulting language.

Social deduction"RLupus: Cooperation through the emergent communication in The Werewolf social deduction game" [14, GPL-3.0 license] introduces an ECS based on the social deduction game _Werewolf_ (a.k.a., _Mafia_) where, through successive rounds of voting and discussion, the "werewolves" try to eliminate the "villagers" before the villagers figure out who the werewolves are. In a given round, the discussion takes the form of all agents broadcasting a message to all other agents after which a vote is taken on whom to eliminate. As there are multiple rounds in a given game, this system introduces multi-step as well as multi-speaker dynamics into the language. Furthermore, the messages also influence distinct actions in the system (i.e., voting). These additional features in the system add the potential for communication strategies that are shaped by a variety of heterogeneous factors rather than simply the distribution of observations (as in the signalling game).

## 5 Discussion

Work enabled by ELCCIn the typical emergent communication paper, only a small amount of time and page count is allocated to analysis with the lion's share being taken up by designing the ECS, implementing it, and running experiments (see Figure 3). Even if one reuses an existing implementation, a significant portion of work still goes towards designing and running the experiments, and the analysis is still limited to that single system. ELCC, on the other hand, enables research which can not only be dedicated wholly to analysis but analysis across a variety of system. Furthermore, removing the necessity of implementing and/or running experiments allows researchers without machine learning backgrounds to contribute to emergent communication research from more linguistic angles that otherwise would not be possible.

In particular, ELCC enable work that focuses on the lexical properties of emergent communication, looking at the statical properties and patterns of the surface forms of a given language (e.g., Zipf's law). Ueda et al. [2023] is a prime example of this; this paper investigates whether or not emergent languages obey Harris' Atticulation Schema relating conditional entropy to the presence of word boundaries. The paper finds mixed evidence for HAS in emergent languages but only evaluated a handful of settings in a single ECS; ELCC could be used in such a case to radically extend the scope emergent languages evaluated. Additionally, ELCC can similarly extend the range of emergent languages evaluated in the context of machine learning, such as Yao et al. [2022], Boldt and Mortensen [2024] which look at emergent language's suitability for deep transfer learning to downstream NLP tasks.

ECS implementations and reproducibilityIn the process of compiling ELCC, we observed a handful of trends in the implementations of emergent communication systems. A significant proportion of papers do not publish the implementations of experiments, severely limiting the ease reproducing the results or including such work in a project such as ELCC, considering that a large amount of the work in creating an ECS is not in the design but in the details of implementation. Even when a free and open source implementation is available, many projects suffer from underspecified Python dependencies (i.e., no indication of versions) which can be difficult to reproduce if the project

Figure 3: Typical allocation of resources to an emergent communication paper (time-wise and page count-wise): analysis is a relatively small component. ELCC enables research which can focus primarily on analysis.

is older than a few years. Furthermore, some projects also fail to specify the particular hyperparameter settings or commands to run the experiments presented in the paper; while these can often be recovered with some investigation, this and the above issue prove to be obstacles which could easily be avoided. For an exemplar of a well-documented, easy-to-run implementation of an ECS and its experiments, see Mu and Goodman (2021b) at https://github.com/jayelm/emergent-generalization/ which not only provides dependencies with version and documentation how to download the data but also a complete shell script which executes the commands to reproduce the experiments.

Future of ELCCWhile ELCC is a complete resource as presented in this paper, ELCC is intended to be an ongoing project which incorporates further ECSs, analyses, and taxonomic features as the body of emergent communication literature and free and open source implementations continues to grow. This approach involves the community not only publishing well-documented implementation of their ECSs but also directly contributing to ELCC in the spirit of scientific collaboration and free and open source software. ELCC, then, is intended to become a hub for a variety of stakeholders in the emergent communication research community, namely a place for: ECS developers to contribute and publicize their work, EC researchers to stay up-to-date on new ECSs, and EC-adjacent researchers to find emergent languages which they can analyze or otherwise use for their own research.

LimitationsWhile ELCC provides a representative sample of the ECSs present in the literature, it is not comprehensive collection of all of the open source implementations let alone all ECSs in the literature. This limitation is especially salient in the case foundational works in EC which have no open source implementations (e.g., Mordatch and Abbeel (2018)). Additionally, ELCC only provides unannotated corpora without any reference to the semantics of the communication, which limits the range of analyses that can be performed (e.g., measures of compositionality are precluded because since it fundamentally a relationship between surface forms and their semantics). In terms of compute resources, we estimate that on the order of 150 GPU-hours (NVIDIA A6000 or equivalents) on an institutional cluster were used in the development of ELCC.

## 6 Conclusion

In this paper, we have introduced ELCC, a collection of emergent language corpora annotated with taxonomic metadata and suite of descriptive metrics derived from free and open source implementations of emergent communication systems introduced in the literature. ELCC also provides code for running these implementations, in turn, making those implementations more reproducible. This collection is the first of its kind in providing easy access to a variety of emergent language corpora. Thus, it enables new kinds of research on emergent communication as it serves as a foundation for research focused on the analysis of emergent languages themselves.

## References

* Bisk et al. [2020] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. Experience grounds language. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 8718-8735, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.703. URL https://www.aclweb.org/anthology/2020.emnlp-main.703.
* Blum et al. [2023] Frederic Blum, Carlos Barrientos, Adriano Ingunza, Damian E Blasi, and Roberto Zariquiery. Grammars across time analyzed (GATA): a dataset of 52 languages. _Scientific Data_, 10(1):835, 2023.
* Boldt and Mortensen [2024a] Brendon Boldt and David Mortensen. Xferbench: Benchmark for emergent languages using deep transfer learning. https://github.com/brendon-boldt/xferbench/, 2024a.
* Boldt and Mortensen [2022] Brendon Boldt and David R. Mortensen. Mathematically modeling the lexicon entropy of emergent language. _arXiv_, 2211.15783, 2022. URL https://arxiv.org/abs/2211.15783.
* Blasi et al. [2018]Brendon Boldt and David R Mortensen. A review of the applications of deep learning-based emergent communication. _Transactions on Machine Learning Research_, 2024b. ISSN 2835-8856. URL https://openreview.net/forum?id=jesKcQxQ7j.
* Bouchacourt and Baroni [2018] Diane Bouchacourt and Marco Baroni. How agents see things: On visual representations in an emergent language game. _arXiv_, arXiv:1808.10696, 2018.
* Brandizzi et al. [2022] Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. RLupus: Cooperation through the emergent communication in The Werewolf social deduction game. _Intelligenza Artificiale_, 15(2):55-70, 2022. URL https://content.iospress.com/articles/intelligenza-artificiale/ia210081.
* Bullard et al. [2021] Kalesha Bullard, Douwe Kiela, Franziska Meier, Joelle Pineau, and Jakob Foerster. Quasi-equivalence discovery for zero-shot emergent communication. _arXiv_, arXiv:2103.08067, 2021.
* Carmeli et al. [2022] Boaz Carmeli, Ron Meir, and Yonatan Belinkov. Emergent quantized communication. _arXiv_, arXiv:2211.02412, 2022.
* Chaabouni et al. [2019] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient encoding in emergent communication. _arXiv_, arXiv:1905.12561, 2019.
* Chaabouni et al. [2020] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compositionality and generalization in emergent languages. _arXiv_, arXiv:2004.09124, 2020.
* Chaabouni et al. [2022] Rahma Chaabouni, Florian Strub, Florent Altche, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=AUGBDIV9rL.
* Chevalier-Boisvert et al. [2018] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. _arXiv preprint arXiv:1810.08272_, 2018.
* Chevalier-Boisvert et al. [2023] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. _CoRR_, abs/2306.13831, 2023.
* Chowdhury et al. [2020a] Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, Jianwei Qiu, and Peter Tu. Symbolic semantic segmentation and interpretation of covid-19 lung infections in chest ct volumes based on emergent languages. _arXiv_, arXiv:2008.09866, 2020a.
* Chowdhury et al. [2020b] Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, and Peter Tu. Emergent symbolic language based deep medical image classification. _arXiv_, arXiv:2008.09860, 2020b.
* Dagan et al. [2020] Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Co-evolution of language and agents in referential games. _arXiv_, arXiv:2001.03361, 2020.
* Denamganai and Walker [2020] Kevin Denamganai and James Alfred Walker. On (emergent) systematic generalisation and compositionality in visual referential games with straight-through gumbel-softmax estimator. _arXiv_, arXiv:2012.10776, 2020.
* Dessi et al. [2019] Roberto Dessi, Diane Bouchacourt, Davide Crepaldi, and Marco Baroni. Focus on what's informative and ignore what's not: Communication strategies in a referential game. _arXiv_, arXiv:1911.01892, 2019.
* Dessi et al. [2021] Roberto Dessi, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from scratch (with a generic visual processor emerging on the side). _arXiv_, arXiv:2106.04258, 2021.
* Dessi et al. [2020b]C. M. Downey, Xuhui Zhou, Leo Z. Liu, and Shane Steinert-Threlkeld. Learning to translate by learning to communicate. _arXiv_, arXiv:2207.07025, 2022.
* Dryer and Haspelmath [2013] Matthew S. Dryer and Martin Haspelmath, editors. _WALS Online_. Max Planck Institute for Evolutionary Anthropology, Leipzig, 2013. URL https://wals.info/.
* Eloff et al. [2023] Kevin Eloff, Okko Rasanen, Herman A. Engelbrecht, Arnu Pretorius, and Herman Kamper. Towards learning to speak and hear through multi-agent communication over a continuous acoustic channel. _arXiv_, 2111.02827, 2023.
* Gibson et al. [2017] Edward Gibson, Richard Futrell, Julian Jara-Ettinger, Kyle Mahowald, Leon Bergen, Sivalogeswaran Ratnasingam, Mitchell Gibson, Steven T. Piantadosi, and Bevil R. Conway. Color naming across languages reflects color use. _Proceedings of the National Academy of Sciences_, 114(40):10785-10790, 2017. doi: 10.1073/pnas.1619666114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1619666114.
* Guo et al. [2019] Shangmin Guo, Yi Ren, Serhii Havrylov, Stella Frank, Ivan Titov, and Kenny Smith. The emergence of compositional languages for numeric concepts through iterated learning in neural agents. _arXiv_, arXiv:1910.05291, 2019.
* Guo et al. [2020] Shangmin Guo, Yi Ren, Agnieszka Slowik, and Kory Mathewson. Inductive bias and language expressivity in emergent communication. _arXiv_, arXiv:2012.02875, 2020.
* Havrylov and Titov [2017] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. _arXiv_, arXiv:1705.11192, 2017.
* Keresztury and Bruni [2020] Bence Keresztury and Elia Bruni. Compositional properties of emergent languages in deep learning. _arXiv_, arXiv:2001.08618, 2020.
* Kharitonov and Baroni [2020] Eugene Kharitonov and Marco Baroni. Emergent language generalization and acquisition speed are not tied to compositionality. _arXiv_, arXiv:2004.03420, 2020.
* Kharitonov et al. [2019] Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Entropy minimization in emergent languages. _arXiv_, arXiv:1905.13687, 2019.
* Kharitonov et al. [2021] Eugene Kharitonov, Roberto Dessi, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. EGG: a toolkit for research on Emergence of lanGuage in Games. https://github.com/facebookresearch/EGG, 2021.
* Khomtchouk and Sudhakaran [2018] Bohdan Khomtchouk and Shyam Sudhakaran. Modeling natural language emergence with integral transform theory and reinforcement learning. _arXiv_, arXiv:1812.01431, 2018.
* Lan et al. [2020] Nur Geffen Lan, Emmanuel Chemla, and Shane Steinert-Threlkeld. On the spontaneous emergence of discrete and compositional signals. _arXiv_, arXiv:2005.00110, 2020.
* Lazaridou and Baroni [2020] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning era. _CoRR_, abs/2006.02419, 2020. URL https://arxiv.org/abs/2006.02419.
* Lazaridou et al. [2016] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. _arXiv_, arXiv:1612.07182, 2016.
* Lazaridou et al. [2018] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. _arXiv_, arXiv:1804.03984, 2018.
* Lewis [1969] David Kellogg Lewis. _Convention: A Philosophical Study_. Wiley-Blackwell, Cambridge, MA, USA, 1969.
* Li and Bowling [2019] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from emergent communication. _arXiv_, arXiv:1906.02403, 2019.

Yaoyiran Li, Edoardo M. Ponti, Ivan Vulic, and Anna Korhonen. Emergent communication pretraining for few-shot machine translation. _arXiv_, arXiv:2011.00890, 2020. doi: 10.18653/v1/2020. cooling-main.416.
* Lowe et al. [2020] Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interaction between supervision and self-play in emergent communication. _arXiv_, arXiv:2002.01093, 2020.
* Luna et al. [2020] Diana Rodriguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, and Elia Bruni. Internal and external pressures on language emergence: least effort, object constancy and frequency. _arXiv_, arXiv:2004.03868, 2020.
* Mahaut et al. [2023] Mateo Mahaut, Francesca Franzon, Roberto Dessi, and Marco Baroni. Referential communication in heterogeneous communities of pre-trained visual deep networks. _arXiv_, arXiv:2302.08913, 2023.
* Mihai and Hare [2019] Daniela Mihai and Jonathon Hare. Avoiding hashing and encouraging visual semantics in referential emergent language games. _arXiv_, arXiv:1911.05546, 2019.
* Mihai and Hare [2021a] Daniela Mihai and Jonathon Hare. The emergence of visual semantics through communication games. _arXiv_, arXiv:2101.10253, 2021a.
* Mihai and Hare [2021b] Daniela Mihai and Jonathon Hare. Learning to draw: Emergent communication through sketching. _arXiv_, 2106.02067, 2021b.
* Mordatch and Abbeel [2018] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018. ISBN 978-1-57735-800-8.
* Mu and Goodman [2021a] Jesse Mu and Noah Goodman. Emergent communication of generalizations. _arXiv_, arXiv:2106.02668, 2021a.
* Mu and Goodman [2021b] Jesse Mu and Noah Goodman. Emergent communication of generalizations. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 17994-18007. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf.
* Ohmer et al. [2021] Xenia Ohmer, Michael Marino, Michael Franke, and Peter Konig. Mutual influence between language and perception in multi-agent communication games. _arXiv_, arXiv:2112.14518, 2021. doi: 10.1371/journal.pcbi.1010658.
* Ohmer et al. [2022] Xenia Ohmer, Marko Duda, and Elia Bruni. Emergence of hierarchical reference systems in multi-agent communication. _arXiv_, arXiv:2203.13176, 2022.
* Perkins [2021a] Hugh Perkins. Neural networks can understand compositional functions that humans do not, in the context of emergent communication. _arXiv_, arXiv:2103.04180, 2021a.
* Perkins [2021b] Hugh Perkins. Texrel: a green family of datasets for emergent communications on relations. _arXiv preprint arXiv:2105.12804_, 2021b.
* Portelance et al. [2021] Eva Portelance, Michael C. Frank, Dan Jurafsky, Alessandro Sordoni, and Romain Laroche. The emergence of the shape bias results from communicative efficiency. _arXiv_, arXiv:2109.06232, 2021.
* Ren et al. [2020] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby. Compositional languages emerge in a neural iterated learning model. _arXiv_, arXiv:2002.01365, 2020.
* Rita et al. [2020] Mathieu Rita, Rahma Chaabouni, and Emmanuel Dupoux. "lazimpa": Lazy and impatient neural agents learn to communicate efficiently. _arXiv_, arXiv:2010.01878, 2020.
* Rita et al. [2021]* Rita et al. [2022a] Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel Dupoux. On the role of population heterogeneity in emergent communication. _arXiv_, arXiv:2204.12982, 2022a.
* Rita et al. [2022b] Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, and Florian Strub. Emergent communication: Generalization and overfitting in lewis games. _arXiv_, arXiv:2209.15342, 2022b.
* Steinert-Threlkeld [2019] Shane Steinert-Threlkeld. Paying attention to function words. _arXiv_, arXiv:1909.11060, 2019.
* Slowik et al. [2020] Agnieszka Slowik, Abhinav Gupta, William L. Hamilton, Mateja Jamnik, Sean B. Holden, and Christopher Pal. Structural inductive biases in emergent communication. _arXiv_, arXiv:2002.01335, 2020.
* Tucker et al. [2021a] Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, and Julie Shah. Emergent discrete communication in semantic spaces. _arXiv_, arXiv:2108.01828, 2021a.
* Tucker et al. [2021b] Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, and Julie A Shah. Emergent discrete communication in semantic spaces. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 10574-10586. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/5812f92450ccaf17275500841c70924a-Paper.pdf.
* Ueda et al. [2023] Ryo Ueda, Taiga Ishii, and Yusuke Miyao. On the word boundaries of emergent languages based on harris's articulation scheme. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=b4t9_XASt6G.
* Unger and Bruni [2020] Thomas A. Unger and Elia Bruni. Generalizing emergent communication. _arXiv: Artificial Intelligence_, 2020. URL https://arxiv.org/abs/2001.01772.
* van der Wal et al. [2020] Oskar van der Wal, Silvan de Boer, Elia Bruni, and Dieuwke Hupkes. The grammar of emergent languages. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 3339-3359, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.270. URL https://aclanthology.org/2020.emnlp-main.270.
* Yao et al. [2022a] Shunyu Yao, Mo Yu, Yang Zhang, Karthik Narasimhan, Joshua Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. In _International Conference on Learning Representations (ICLR)_, 2022a.
* Yao et al. [2022b] Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B. Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. _arXiv_, arXiv:2203.13344, 2022b.
* Zaslavsky et al. [2019] Noga Zaslavsky, Charles Kemp, Naftali Tishby, and Terry Regier. Color naming reflects both perceptual structure and communicative need. _Topics in Cognitive Science_, 11(1):207-219, 2019. doi: https://doi.org/10.1111/tops.12395. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12395.
* Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kucinski et al. [2021] Lukasz Kucinski, Tomasz Korbak, Pawel Kolodziej, and Piotr Milos. Catalytic role of noise and necessity of inductive biases in the emergence of compositional communication. _arXiv_, arXiv:2111.06464, 2021.

Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? See Section 5. 3. Did you discuss any potential negative societal impacts of your work? This work deals solely with emergent communication (i.e., synthetic data) which has been gathered from code bases with open source licenses. Research on emergent communication is still in "basic" stage with few, if any, immediate impacts outside the research community. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Section 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 4. 2. Did you mention the license of the assets? See Section 4. 3. Did you include any new assets either in the supplemental material or as a URL? See Section 1. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? The data produced is synthetic and has little semantic content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?

## Appendix A ECS-Level Metadata Example

See Figure 4.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{l c c} \hline \hline name & 2-gram Entropy & 2-gram Conditional Entropy \\ \hline babyvi=sor(GoGoObj) & 1.544519 & 0.306888 \\ babyvi=sor(GoGoObj.Lceded & 1.147285 & 0.160295 \\ babyvi=sor(GoGoObj.Lced\_ambiguous & 1.798413 & 0.254393 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-freq\_1 & 2.071162 & 0.607508 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-freq\_2 & 1.538991 & 0.153070 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-freq\_32 & 0.420175 & 0.062050 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-freq\_4 & 2.406197 & 0.409242 \\ babyvi=sor(GoGoGoObj.Lced\_ambiguous-freq\_4 & 3.097571 & 0.452418 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-msg\_32 & 1.463220 & 0.112660 \\ babyvi=sor(GoGoObj.Lced\_ambiguous-msg\_4 & 1.173705 & 0.795367 \\ babyvi=sor(GoGoObj.Locked & 2.497606 & 0.504450 \\ babyvi=sor(GoGoObj.Locked-freq\_1 & 1.092966 & 0.196541 \\ babyvi=sor(GoGoObj.Locked-freq\_2 & 2.877898 & 0.768115 \\ babyvi=sor(GoGoObj.Locked-freq\_32 & 1.731359 & 0.087790 \\ babyvi=sor(GoGoObj.Locked-freq\_4 & 2.979210 & 0.814026 \\ babyvi=sor(GoGoObj.Locked-freq\_4 & 3.043978 & 0.435771 \\ babyvi=sor(GoGoObj.Locked-freq\_32 & 3.157215 & 0.216230 \\ babyvi=sor(GoGoObj.Locked-freq\_4 & 2.030255 & 0.854029 \\ corpus=transfer-yao-e4-ikcce & 2.095689 & 0.661383 \\ corpus=transfer-yao-e4-ikcce & 12.884451 & 6.285130 \\ ce=cat-sci(vali)magnet-10x1 & 6.811992 & 2.831113 \\ ce=cat-sci(vali)magnet-1x1 & 6.328754 & 2.420041 \\ ce=cat-sci(vali)magnet-1x1 & 6.682813 & 2.761016 \\ ce=at-sci(vali)magnet-1x10 & 6.375876 & 2.400379 \\ ce=sci(mismatch+att.+att.+3.4.val\_3-dist.0-seed & 4.434835 & 1.438094 \\ egg-discrimination+att.+att.+3.4.val\_3-dist.1-seed & 3.550278 & 1.055580 \\ egg-discrimination+att.+att.+3.4.val\_3-dist.2-seed & 3.544613 & 0.979835 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.0-seed & 3.97021 & 1.335551 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.1-seed & 4.308021 & 1.420628 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.2-seed & 3.73839 & 1.164541 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.2-seed & 4.371053 & 1.509123 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.2-seed & 3.578326 & 1.115826 \\ egg-discrimination+att.+6.+3.4.val\_3-dist.2-seed & 4.407096 & 1.320061 \\ egg-discrimination+att.+3.4.val\_3-dist.0-seed & 3.504384 & 1.077631 \\ egg-discrimination+att.+3.4.val\_3-dist.1-seed & 3.712531 & 1.156216 \\ egg-discrimination+att.+.8.val\_3-dist.2-seed & 4.006086 & 1.203946 \\ egg-discrimination+att.+.4.val\_10-cos.10-ln.2 & 3.12115 & 0.915787 \\ egg-reconstruction+att.+6.+3.1-0.0.0-ln.1 & 3.370294 & 1.170751 \\ egg-reconstruction+att.+3.+val\_10-vecx10-ln.1 & 3.515011 & 1.219244 \\ genecalizations=mg-goodman(web-ecaport) & 5.686797 & 1.539352 \\ generalizations-mg-goodman(web-ecaport) & 5.641346 & 2.533465 \\ generalizations-mg-goodman(web-et,reference & 5.509094 & 2.296366 \\ generalizations-mg-goodman(web-ecaport) & 6.00857 & 2.841134 \\ generalizations-mg-goodman(web-ecaport) & 5.908455 & 2.684016 \\ para(-c)enter(vali)magnet-set_reference & 6.409305 & 3.043749 \\ para(-c)enter(vali)magnet-1x1 & 4.42024 & 1.434806 \\ nav-a(-c)enter(lexi)magnet-1x1 & 5.389004 & 1.621472 \\ nav-a(-c)enter(lexi)magnet-1x1 & 4.465472 & 1.466419 \\ nav-a(-c)enter(lexi)magnet-1x1 & 4.717891 & 1.472561 \\ nav-a(-c)enter(lexi)magnet-1x1 & 4.106729 & 1.302528 \\ nav-a(-c)enter(lexi)magnet-1x1 & 5.098629 & 1.563950 \\ nav-a(-c)enter(lexi)magnet-1x1 & 4.335838 & 1.307361 \\ nav-a(-c)enter(lexi)magnet-1x1 & 5.643441 & 1.708649 \\ nav-a(-c)enter(lexi)magnet-1x1 & 4.123176 & 1.364599 \\ nav-a(-c)enter(lexi)magnet-1x1 & 5.001934 & 1.544348 \\ nav-a(-c)enter(lexi)magnet-1x1 & 3.40517 & 1.410484 \\ nav-a(-c)enter(lexi)magnet-1x1 & 3.466046 & 1.487293 \\ nav-a(-c)enter(lexi)magnet-1x1 & 3.396763 & 1.410126 \\ nav-a(-c)enter(p)magnet-1x1 & 3.377160 & 1.394467 \\ nav-a(-c)enter(p)magnet-1x1 & 3.777791 & 1.466642 \\ nav-a(-c)enter(p)magnet-1x1 & 4.20502 & 1.447624 \\ nav-a(-c)enter(p)magnet-1x1 & 8.121348 & 3.216181 \\ nav-a(-c)enter(p)magnet-1x1 & 7.739814 & 2.7737120 \\ nav-a(-c)enter(p)magnet-1x1 & 8.433494 & 3.092856 \\ nav-a(-c)enter(p)magnet-1x1 & 8.600965 & 3.394618 \\ rhpsps21-player.run-0 & 6.956412 & 2.893892 \\ rhps21-player.run-1 & 7.400071 & 3.062111 \\ rhpsps21-player.run-2 & 7.0398822 & 3.0427370 \\ rhps29-player.run-1 & 5.83233 & 2.0803656 \\ rhpsps29-player.run-1 & 5.925070 & 2.805487 \\ rhps39-player.run-2 & 5.979073 & 2.888910 \\ rhps39-player.run-3 & 5.865222 & 2.753987 \\ \hline \hline \end{tabular}
\end{table}
Table 6: