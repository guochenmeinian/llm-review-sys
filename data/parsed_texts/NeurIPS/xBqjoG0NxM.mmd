# SODA: Robust Training of Test-Time Data Adaptors

 Zige Wang\({}^{1,2}\) Yonggang Zhang\({}^{2}\) Zhen Fang\({}^{3}\) Long Lan\({}^{4}\)

Wenjing Yang\({}^{4}\) Bo Han\({}^{2}\)

\({}^{1}\)School of Computer Science, Peking University \({}^{2}\)Hong Kong Baptist University

\({}^{3}\)University of Technology Sydney \({}^{4}\)National University of Defense Technology

###### Abstract

Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction. For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption. Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters.

## 1 Introduction

Deep neural networks have emerged as a dominant tool in solving artificial intelligence problems due to their exceptional performance across various tasks [19, 20, 42], thus being deployed to various environments. However, in practice, these models typically suffer performance degradation due to distribution discrepancies between training and test data [1, 13, 47, 35]. To mitigate this issue, Test-Time Adaptation (TTA) is proposed as a promising solution, where unlabeled test data are leveraged to modify the parameters of deployed models to alleviate performance degradation [7, 21, 39].

In practice, the parameters of deployed models may be unmodifiable and inaccessible in many applications due to intellectual property protection, misuse prevention, or privacy concerns in healthcare and finance [22, 46]. The difficulty in modifying model parameters has hindered previous efforts aimed at adapting deployed models to the test distribution [15, 23, 29]. It is shown that training a data adaptor to modify test data offers an alternative solution to mitigate the performance degradation caused by distribution discrepancy [18, 37]. However, the difficulty in accessing model parameters makes gradient computation a challenging task, hindering these data adaptation methods.

One straightforward approach involves utilizing zeroth-order optimization (ZOO) [25] to train the data adaptor to adapt test data to fit deployed models. In particular, ZOO can be employed to estimate gradients for the optimization of data adaptors without modifying and accessing the parameters of deployed models. Therefore, test-time data adaptation with ZOO makes it possible to improve the performance of deployed models in many practical scenarios. However, introducing data adaptorstrained with ZOO brings limited improvement. To endow data adaptors with reliability in improving model performance, we revisit ZOO in the context of test-time data adaptation. Training of the data adaptor depends heavily on the predicted labels. Thus, unreliable predicted labels will lead to unreliable gradient estimations in ZOO, which makes data features corrupted rather than adapted to deployed models. This is consistent with our observations as depicted in Figure 1.

Built upon the observation, we propose pseudo-label-robust data adaptation (SODA) to enhance deployed models with inaccessible parameters. SODA robustifies the training of data adaptors by splitting the test dataset into two subsets. One subset contains data with high-confidence predictions and is regarded as a reliable dataset. The other contains the remaining data points, considered to be an unreliable subset. For data in the reliable dataset, SODA trains the data adaptor using ZOO in a supervised manner. In the meantime, SODA encourages the data adaptor to preserve input information in an unsupervised manner for data corruption mitigation over unreliable data. To verify the efficacy of SODA, we evaluate it on three widely used benchmark datasets under various settings. Our experimental results demonstrate that SODA can effectively mitigate the performance degradation of deployed models in the presence of distribution shifts.

To sum up, our main contributions are presented as follows:

* To enhance deployed models with unmodifiable and inaccessible parameters, we propose to shift from model adaptation to data adaptation, where zeroth-order optimization is employed to estimate gradients for the training of the data adaptor. Unfortunately, test-time data adaptation causes corruption of data features, leading to limited performance improvements.
* To exploit the potential of test-time data adaptation, we find that the limited improvement issue stems from the unreliable nature of the predicted labels used in ZOO. Thus, we propose SODA to robustify the training of test-time data adaptation, where the data adaptor is trained to preserve input information for data with unreliable predictions.
* We verify the effectiveness of SODA under various settings. Our experimental results demonstrate that SODA can directly enhance the deployed model under distribution shifts without accessing and modifying the model parameters.

## 2 Related Work

**Test-time adaptation.** Test-time adaptation is a machine learning technique that addresses the distribution shift problem using only unlabeled test data before making predictions. To mitigate the distribution discrepancy, most previous works adapt the pre-trained model to test data by modifying

Figure 1: Demonstration of corrupted data and adapted data. The left part shows examples of the original test data from CIFAR-10-C, the corrupted data generated by the data adaptor trained with unreliable pseudo-labels, and the adapted data generated by the data adaptor in SODA. The right part shows the corresponding accuracy of test data, corrupted data, and adapted data on CIFAR-10/100-C.

the full or part of the model parameters. Some advanced works [7; 21] adapt the feature extractor to obtain more efficient feature representations of test data, while others [16; 17; 45] modify or replace the last linear classification layer to improve prediction over the extracted features. Batch normalization calibration [23; 34; 43; 49] is also exploited to adjust the statistics and affine parameters in batch normalization layers. Unlike previous works, our work focuses on situations where model parameters are unmodifiable and adapts test data to the pre-trained model as an alternative solution.

**White-box input adaptation.** Recently, several works have put effort into input-level optimization in test-time adaptation by changing the input data or features. Auxiliary auto-encoders [12; 18], amplitude features and Fourier-style calibration [50], label-preserving features along with a generative model [31], and generative diffusion model [10] are utilized to achieve input-level adaptation. These works require either modification of the model training process or gradients from the deployed model at test time. In contrast, our work focuses on test-time data adaptation without accessing the training process and gradients of the deployed model, making it more practical and broadly applicable.

**Domain adaptation of black-box predictors.** Domain Adaptation of Black-Box Predictors (DABP) is a subcategory of unsupervised domain adaptation [9; 30] that solves a more restricted and practical application setting. Under this setting, the pre-trained model is treated as a black box, and only the model's output is available during adaptation. Few works [22; 32; 46] have proposed solutions to this challenge using knowledge distillation to transfer knowledge from the black-box model to target models. In this work, we address the black-box setting in the context of test-time adaptation and propose to perform data adaptation without knowledge transfer. Besides, while previous DABP works require training on the entire unlabeled test dataset, our proposed method can also deal with online black-box settings where the test data arrive sequentially.

**Zeroth-order optimization.** Zeroth-order optimization (ZOO) [25] is a family of optimization methods that do not require gradient information to search for the optimal solution. Instead, it explores the searching space using various techniques [6; 24] to estimate the optimization direction. Although it can be less efficient than first-order optimization methods, ZOO methods can be helpful in scenarios where gradient information is unavailable or expensive to compute. For instance, recent studies have applied ZOO to perform adversarial attacks to black-box neural networks [48], hyperparameter optimization in federated learning without gradient descent [51], and transfer learning on black-box models [40]. In this work, we propose to leverage ZOO for test-time data adaptation on deployed models with inaccessible gradient information.

## 3 Methodology

We mainly focus on the \(C\)-way image classification task with a distribution shift between the training and test data, following previous works [26; 39; 43]. Given a deployed model \(\mathbf{M}\) with inaccessible parameters, our purpose is to improve its prediction accuracy on unlabeled test data \(\mathbf{X}=\{\mathbf{x}_{1},...,\mathbf{x}_{n}\}\). Since the parameters and inner structures of \(\mathbf{M}\) are unknown, only the output prediction probabilities are available from \(\mathbf{M}\) during the entire adaptation process. Namely, the proposed method pseudo-label-robust data adaptation (SODA) aims to adapt \(\mathbf{X}\) to \(\mathbf{M}\) without requiring access to the parameters of \(\mathbf{M}\). The overall framework of SODA is shown in Figure 2.

### Preliminary

Before elaborating on our proposed framework, we introduce zeroth-order optimization (ZOO). ZOO is a gradient-free alternative of first-order optimization (FOO), e.g., SGD, SCD, and Adam. Most ZOO methods follow the structure of FOO and consist of three fundamental steps [25]: gradient estimation, descent direction computation, and point updating. They utilize function-value-based gradient estimations to approximate the full or stochastic gradients computed in FOO.

One commonly used ZOO gradient estimation strategy is multi-point estimation [6; 24]. Given a continuously differentiable objective function \(f(\bm{\theta})\) on a \(d\)-dimensional variable \(\bm{\theta}\in\mathbb{R}^{d}\), multi-point estimation computes directional derivative approximation as follows:

\[\widehat{\nabla}_{\bm{\theta}}f(\bm{\theta}):=\frac{1}{\mu q}\sum_{i=1}^{q} \big{[}(f(\bm{\theta}+\mu\mathbf{u}_{i})-f(\bm{\theta}))\mathbf{u}_{i}\big{]},\] (1)where \(\mathbf{u}_{1},...,\mathbf{u}_{q}\) are \(q\) random direction vectors typically drawn from the standard multivariate normal distribution \(\mathcal{N}(\mathbf{0},\mathbf{I})\), and \(\mu\) is the smoothing parameter. On a mini-batch of data points \(\mathbf{x}_{1},...,\mathbf{x}_{l}\), the estimated gradient \(\hat{\bm{\delta}}\) is the average of the multi-point estimations to all data points \(\mathbf{x}_{1},...,\mathbf{x}_{l}\):

\[\hat{\bm{\delta}}=\frac{1}{l}\sum_{i=1}^{l}\widehat{\nabla}_{\bm{\theta}}f( \bm{\theta};\mathbf{x}_{i}).\] (2)

Various strategies are adopted to compute the descent direction. For zeroth-order stochastic gradient descent (ZO-SGD) utilized in our work, the descent direction is set as the current gradient estimation \(\hat{\bm{\delta}}\). The point updating rule is the same as in traditional SGD: with learning rate \(\eta\),

\[\bm{\theta}=\bm{\theta}-\eta\hat{\bm{\delta}}.\] (3)

### Zeroth-Order Optimization in Test-Time Data Adaptation

Let \(\mathbf{G}\) with parameters \(\bm{\theta}\) be the data adaptor for test-time data adaptation. For each test data point \(\mathbf{x}_{i}\) (\(i=1,...,n\)), \(\mathbf{G}\) transforms it to form the adapted data \(\mathbf{x}_{i}^{\bm{\theta}}\) for inference as follows:

\[\mathbf{x}_{i}^{\bm{\theta}}=\mathbf{G}(\mathbf{x}_{i};\bm{\theta}).\] (4)

Ideally, the data adaptor \(\mathbf{G}\) should be trained by minimizing the KL divergence between the predicted probabilities of the adapted data and the true labels of test data. Typically, the training process requires back-propagating the gradients from the deployed model to the data adaptor. A challenge arises as gradient computation is infeasible for the deployed model \(\mathbf{M}\) with inaccessible parameters. In this regard, ZOO provides an effective approach to estimating gradients, as discussed in the previous section. Considering the parameters \(\bm{\theta}\) of the data adaptor \(\mathbf{G}\) as the variables to be optimized, utilizing ZOO in test-time data adaptation is to replace the objective function \(f(\bm{\theta})\) with the training objective function used to train the data adaptor \(\mathbf{G}\). Assuming that the true one-hot label \(\mathbf{y}_{i}\) of a test data point \(\mathbf{x}_{i}\) is given, and replacing the function \(f(\bm{\theta})\) in Eq. (1) with KL divergence loss \(\mathcal{L}(\cdot,\cdot):=\mathrm{KL}(\cdot\|\cdot)\), the directional derivative approximation w.r.t. \(\mathcal{L}(\cdot,\cdot)\) and \((\mathbf{x}_{i},\mathbf{y}_{i})\) is

\[\widehat{\nabla}_{\bm{\theta}}\mathcal{L}_{i}=\frac{1}{\mu q}\sum_{j=1}^{q} \big{[}\big{(}\mathcal{L}(\mathbf{y}_{i},\mathbf{M}\circ\mathbf{G}(\mathbf{x }_{i};\bm{\theta}+\mu\mathbf{u}_{j}))-\mathcal{L}(\mathbf{y}_{i},\mathbf{M} \circ\mathbf{G}(\mathbf{x}_{i};\bm{\theta}))\big{)}\mathbf{u}_{j}\big{]}.\] (5)

Figure 2: The overall framework of SODA. (a) Before adaptation, SODA first performs reliable pseudo-label selection according to prediction confidence. (b) During adaptation, the data adaptor with parameter \(\bm{\theta}\) is trained over the test data with reliable pseudo-labels using cross-entropy loss and those with unreliable pseudo-labels using mutual information maximization. The gradient is estimated using (c) zeroth-order optimization.

In test-time adaptation, the true labels of test data are obviously unknown. A common strategy is to use the predicted pseudo-label \(\hat{\mathbf{y}}_{i}\) as the substitute of the true label \(\mathbf{y}_{i}\).

However, the pseudo-labels are unreliable due to the inaccurate model prediction under distribution shifts, causing the corrupted data features depicted in Figure 1. Let \(\bm{\sigma}_{i}\) denote the disturbance of pseudo-label \(\hat{\mathbf{y}}_{i}\), i.e., \(\hat{\mathbf{y}}_{i}=\bm{\sigma}_{i}+\mathbf{y}_{i}\), and \(\hat{\mathbf{p}}_{i}^{\bm{\theta}}=\mathbf{M}\circ\mathbf{G}(\mathbf{x}_{i}; \bm{\theta})\) denote the predicted probability of the adapted data point \(\mathbf{x}_{i}^{\bm{\theta}}=\mathbf{G}(\mathbf{x}_{i};\bm{\theta})\), the KL divergence loss at test point \(\mathbf{x}_{i}\) becomes:

\[\mathcal{L}_{i}=-H(\mathbf{y}_{i}+\bm{\sigma}_{i})+\mathcal{L}_{\rm ce}( \mathbf{y}_{i},\hat{\mathbf{p}}_{i}^{\bm{\theta}})-\bm{\sigma}_{i}\log\hat{ \mathbf{p}}_{i}^{\bm{\theta}},\] (6)

where \(\mathcal{L}_{\rm ce}(\cdot,\cdot)\) is the cross-entropy loss. Then, replacing \(\mathbf{y}_{i}\) with \(\hat{\mathbf{y}}_{i}\) in Eq. (5), the directional derivative approximation becomes

\[\widehat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}=\widehat{\nabla}_{\bm {\theta}}\mathcal{L}_{\rm ce}+\frac{\bm{\sigma}_{i}}{\mu q}\sum_{j=1}^{q}\log \frac{\hat{\mathbf{p}}_{i}^{\bm{\theta}}}{\hat{\mathbf{p}}_{i}^{\bm{\theta}+ \mu\mathbf{y}_{j}}}\mathbf{u}_{j},\] (7)

where \(\widehat{\nabla}_{\bm{\theta}}\mathcal{L}_{\rm ce}=\frac{1}{\mu q}\sum_{j=1}^ {q}\big{[}\big{(}\mathcal{L}_{\rm ce}(\mathbf{y}_{i},\hat{\mathbf{p}}_{i}^{ \bm{\theta}+\mu\mathbf{y}_{j}})-\mathcal{L}_{\rm ce}(\mathbf{y}_{i},\hat{ \mathbf{p}}_{i}^{\bm{\theta}})\big{)}\mathbf{u}_{j}\big{]}\) is the ideal directional derivative approximation. The derivations of Eq. (6) and Eq. (7) are deferred to Appendix A. In Eq. (7), the last term is the disturbing term directly introduced by \(\bm{\sigma}_{i}\), causing the difference between \(\widehat{\nabla}_{\bm{\theta}}\mathcal{L}_{\rm ce}\) and the unreliable directional derivative approximation \(\widehat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}\). \(\widehat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}\) further leads to unreliable estimated gradients in Eq. (2), which hinders the optimization of \(\bm{\theta}\) and the training of \(\mathbf{G}\).

### Pseudo-Label-Robust Training

A direct strategy to alleviate the impact of the disturbing term in Eq. (7) is to select pseudo-labels with small \(\bm{\sigma}_{i}\). The selected pseudo-labels form reliable pseudo-label set \(\hat{\mathbf{Y}}_{r}\) to train the data adaptor in a supervised manner. In particular, two basic criteria for reliable pseudo-label selection are adopted in SODA: 1) the prediction confidence should be higher than a threshold \(\tau\), indicating the selected pseudo-labels have small disturbances; 2) the number of selected reliable pseudo-labels for each class should be less than \((1-\rho)n/C\) to maintain the balance among classes, where \(\rho\) is the noise ratio, and \(C\) is the number of classes. The test data points corresponding to the selected reliable pseudo-labels are considered as reliable data set \(\mathbf{X}_{r}\), which is trained over with cross-entropy loss \(\mathcal{L}_{\rm ce}\) and \(\hat{\mathbf{Y}}_{r}\).

To mitigate the data corruption over the remaining test data points \(\mathbf{X}_{u}\) with unreliable pseudo-labels, SODA trains over them in an unsupervised manner. Following previous works [21; 22], mutual information maximization [4; 36; 41] is a widely-used unsupervised loss that can encourage both global diversity and local certainty of model predictions by maximizing the mutual information between the input data sample and the predicted probabilities. Thus, it is adopted to preserve input information in \(\mathbf{X}_{u}\) as shown in Eq. (8), where \(\mathbf{x}_{i}^{\bm{\theta}}=\mathbf{G}(\mathbf{x}_{i};\bm{\theta})\) and \(\hat{\mathbf{p}}_{i}=\mathbf{M}\circ\mathbf{G}(\mathbf{x}_{i};\bm{\theta})\).

\[\mathcal{L}_{\rm im}(\mathbf{X}_{u}^{\bm{\theta}})=\mathbb{E}_{\mathbf{x}_{i} ^{\bm{\theta}}\in\mathbf{X}_{u}^{\bm{\theta}}}[\sum_{k=1}^{C}\hat{\mathbf{p}} _{ik}\log\hat{\mathbf{p}}_{ik}]-\sum_{k=1}^{C}\mathbb{E}_{\mathbf{x}_{i}^{\bm {\theta}}\in\mathbf{X}_{u}^{\bm{\theta}}}\hat{\mathbf{p}}_{ik}\log\mathbb{E}_ {\mathbf{x}_{i}^{\bm{\theta}}\in\mathbf{X}_{u}^{\bm{\theta}}}\hat{\mathbf{p}}_{ ik}.\] (8)

### Theoretical Analysis

To theoretically show the effectiveness of the proposed pseudo-label-robust training strategy, we analyze the expected gradient estimation error [5; 24] in the training of test-time data adaptor with zeroth-order optimization. For simplicity, we consider the special case where the estimated gradient equals directional derivative approximation with mini-batch size equal to 1. The expected gradient estimation error \(\mathcal{R}_{\mathbf{X}}\) between the estimated gradient \(\hat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}\) and the true gradient \(\nabla_{\bm{\theta}}\mathcal{L}_{i}\) w.r.t. the whole test dataset \(\mathbf{X}\) is:

\[\mathcal{R}_{\mathbf{X}}=\mathbb{E}_{\mathbf{X}}\big{[}\mathbb{E}[\parallel\hat {\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}-\nabla_{\bm{\theta}}\mathcal{L} _{i}\parallel_{2}]\big{]}.\] (9)

Denoting \(h(\mathbf{x}_{i})=-\bm{\sigma}_{i}\log\hat{\mathbf{p}}_{i}^{\bm{\theta}}\) in Eq. (6), the gradient of the KL divergence loss is \(\nabla_{\bm{\theta}}\mathcal{L}_{i}=\nabla_{\bm{\theta}}\mathcal{L}_{\rm ce}+ \nabla_{\bm{\theta}}h\). Accordingly, the estimated gradient of the KL divergence loss is \(\widehat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{i}=\widehat{\nabla}_{\bm{ \theta}}\mathcal{L}_{\rm ce}+\widehat{\nabla}_{\bm{\theta}}h\). Then, before applying pseudo-label-robust data adaptation, the upper bound of expected gradient estimation error is:

\[\mathcal{R}_{\mathbf{X}}\leq\mathbb{E}_{\mathbf{X}}\big{[}\mathbb{E}[\parallel \widehat{\nabla}_{\bm{\theta}}\tilde{\mathcal{L}}_{\rm ce}-\nabla_{\bm{ \theta}}\mathcal{L}_{\rm ce}\parallel_{2}]+\mathbb{E}[\parallel\widehat{ \nabla}_{\bm{\theta}}h-\nabla_{\bm{\theta}}h\parallel_{2}]\big{]}.\] (10)

[MISSING_PAGE_FAIL:6]

**Baselines. Deployed** is the deployed model without adaptation. We compare our proposed SODA framework with two DABP baselines utilizing knowledge distillation. **DINE**[22] distills the knowledge from the deployed model to a target model by minimizing the KL divergence between smoothed pseudo-labels and target model predictions. **BETA**[46] divides the target domain into easy- and hard-to-adapt subdomains, then mutually distills twin networks with weak-strong augmentation on two subdomains. We further implement four vanilla baselines of test-time data adaptation using ZOO. **DA-Direct** directly generates adapted data instead of perturbations using the same network structure and initial pseudo-labels. **DA-PGD** adopts PGD [27] to directly generate perturbations using estimated gradients computed by ZOO and the training objective of SODA. **DA-ZOO-Input** uses the same data adaptor and training objective as SODA, except that the variables optimized in ZOO are the input data of the deployed model, i.e., the output adapted data of the data adaptor. The gradients of data adaptor parameters are computed based on the estimated gradients w.r.t. the adapted data. **DA-PL** trains the same data adaptor as SODA using initial pseudo-labels. Assuming gradient information is accessible, we also implement **SODA-R** using first-order gradients computed from the deployed model. To compare SODA with model adaptation, we implement **MA-SO** that modifies model parameters (except the last linear layer) using the same training objective as SODA.

**Deployed model settings.** For all experiments regarding CIFAR-10/100-C tasks, we adopt the CIFAR-10/100 pretrained ResNet-50 [11] model used in [26] and [38] as the deployed model. For experiments regarding the ImageNet-C task, we adopt the ImageNet pre-trained ResNet-50 model provided by TorchVision [28] as the deployed model. Except for SODA-R and MA-SO, the deployed model is frozen with only output probabilities accessible. For SODA-R, the deployed model is also frozen, but gradients can be computed and back-propagated through the model. For MA-SO, the deployed model is set in training mode, and all parameters can be modified.

**Implementation details.** For SODA, the data adaptor uses a small network with two convolutional layers and an instance normalization layer in between to generate perturbations added to the original test data. For SODA-R in the CIFAR-10/100-C tasks, two ResNet blocks as in [33] are inserted between the convolutional layers to form a larger data adaptor. For SODA-R in the ImageNet-C task, one ResNet block and a couple of downsampling/upsampling layers are inserted instead. Detailed data adaptor structure is described in Appendix B.3. For all methods except DINE, BETA, DA-PGD, and SODA-R, the data adaptor/model is optimized using SGD with learning rate = 1e-3, momentum = 0.9, and weight decay = 1e-5. For DINE and BETA, we follow the same settings as in [22] and [46], and the target models are both ResNet-50 initialized with ImageNet-pretrained weights downloaded from TorchVision [28]. For DA-PGD, the step size is also set to be 1e-3. SODA-R is optimized using Adam with learning rate = 1e-3 and weight decay = 1e-5. Batch size = 256 is fixed for all methods. The number of training epochs = 150 for all baselines except DINE and BETA, we train them for 120 epochs and fine-tune them for 30 epochs. For all methods using ZOO, the query number \(q\) = 5 for CIFAR-10-C and ImageNet-C and \(q\) = 10 for CIFAR-100-C, smoothing parameter

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline Categories & Methods & FO Grad. & Model Mod. & C10-C & C100-C & IN-C \\ \hline - & Deployed & - & - & 72.39 & 41.41 & 31.36 \\ \hline \multirow{3}{*}{Distill.} & DINE & ✓ & ✗ & 73.86 & 40.52 & - \\  & BETA & ✓ & ✗ & 75.71 & 39.62 & - \\ \hline \multirow{4}{*}{DA} & DA-PGD & ✗ & ✗ & 24.63 & 4.15 & 14.39 \\  & DA-ZOO-Input & ✗ & ✗ & 68.70 & 31.53 & 17.57 \\ \cline{1-1}  & DA-Direct & ✗ & ✗ & 70.48 & 37.67 & 29.37 \\ \cline{1-1}  & DA-PL & ✗ & ✗ & 72.93 & 41.44 & 31.91 \\ \cline{1-1}  & SODA (Ours) & ✗ & ✗ & **82.55** & **52.41** & **42.14** \\ \cline{1-1}  & SODA-R (Ours) & ✓ & ✗ & **88.39** & 60.31 & 48.70 \\ \hline MA & MA-SO & ✓ & ✓ & 86.54 & **62.02** & **56.90** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average accuracies (%) on CIFAR-10-C (**C10-C**), CIFAR-100-C(**C100-C**) and ImageNet-C (**IN-C**). **FO Grad.** means the requirement of first-order gradient from the target model or deployed model. **Model Mod.** means the requirement of modifying the parameters of deployed models. **Distill.** indicates methods using knowledge distillation to learn target models. **DA** indicates methods using test-time data adaptation. **MA** indicates methods using model adaptation.

\(\mu\) = 1e-3. All experiments are repeated with three random seeds. The code implementation can be found at https://github.com/tmlr-group/SODA.

### Experimental Results

**Effectiveness of SODA.** Table 1 shows the accuracies on CIFAR-10-C, CIFAR-100-C and ImageNet-C averaged over 19 corruptions. In the mostly restricted setting where first-order gradient computation and model modification are both not allowed, SODA improves the deployed model prediction accuracy by a large margin, 10% on CIFAR-10-C, 11% on CIFAR-100-C and 11% on ImageNet-C. SODA also outperforms the DABP baselines on CIFAR10-C and CIFAR100-C tasks. Note that DINE and BETA are excluded in experiments on ImageNet-C because they are required to initialize their target models by ImageNet pre-trained weights, which violates the TTA setting. Especially on CIFAR-100-C and ImageNet-C task with much lower initial prediction accuracy, all baselines except MA-SO fail to improve, while SODA and SODA-R make significant improvement.

**SODA v.s. model adaptation.** Further relaxing the restriction on the deployed model, when parameters of the deployed model are frozen, but gradient computation is feasible, SODA-R achieves comparable accuracy with completely unrestricted MA-SO on CIFAR-100-C and even better accuracy on CIFAR-10-C. It shows that data adaptation can be as effective as model adaptation for deployed models with inaccessible parameters. On ImageNet-C, SODA-R performs worse than MA-SO, suggesting that improvement is still needed on large-scale datasets.

**Effect of pseudo-label-robust training strategy in SODA.** The failures of DA-Direct show that directly generating data using unreliable pseudo-labels fails to adapt data to the deployed model due to the corrupted data features. By adding perturbations to the original test data, DA-PL improves from DA-Direct to a small degree but still fails to enhance the deployed model. With the same data adaptor as DA-PL, SODA can improve the model prediction to a large degree, indicating the effectiveness of the proposed pseudo-label-robust training strategy.

**Effect of data adaptor parameter ZOO in SODA.** Both using a perturbation generation strategy, DA-PGD fails by directly generating perturbations using estimated gradients without learning a data adaptor, and DA-ZOO-Input achieves worse results by directly optimizing the model input using ZOO. With the same training objective, the significant improvements made by SODA indicate the effectiveness of the data adaptor parameter ZOO adopted in SODA.

### Discussion

**Effect of query number in ZOO.** To analyze the relation between query number \(q\) and adaptation accuracy in SODA, we conduct experiments with \(q\in\{2,5,10,20,50\}\). The results in Table 2 show that SODA is not sensitive to query number used in ZOO. Especially on CIFAR-10-C, only a slight performance drop is observed when \(q\) = 2. On CIFAR-100-C, a larger query number contributes to accuracy improvement more observably, but when \(q>10\), the contribution becomes less efficient, as the computation cost is also increased. Balancing the trade-off between accuracy and computation costs, we finally choose \(q\) = 5 for CIFAR-10-C and 10 for CIFAR-100-C.

**Zeroth-order optimization v.s. first-order optimization.** To better illustrate the effect of ZOO, we implement a comparing baseline as SODA-FO that shares the same setting with SODA but uses the first-order gradients back-propagated from the deployed model. Note that SODA-FO and SODA-R differ in data adaptor network structure, optimizer, and other strategies as discussed in Appendix C.1. As shown in Table 2, although SODA does not achieve the same accuracies as SODA-FO due to the unavoidable gradient estimation error, it still indicates competitive performance when gradient computation is infeasible. In Figure 3, SODA using ZOO has the slowest convergence speeds

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Query Numbers & 2 & 5 & 10 & 20 & 50 & SODA-FO \\ \hline CIFAR-10-C & 82.43 & 82.55 & 82.57 & 82.59 & 82.53 & 85.97 \\ CIFAR-100-C & 51.03 & 52.19 & 52.41 & 52.97 & 52.97 & 54.32 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of SODA using ZOO with different query numbers and SODA using first-order gradients. The network structure, training objective, and training strategy are the same. Averaged accuracies (%) over 19 corruptions are reported.

and lowest accuracies on both datasets. SODA-FO has similar convergence speeds as SODA-R but converges to lower accuracies, indicating that the strategies used in SODA-R with first-order optimization, i.e., deeper network and Adam optimizer, can boost the training of the data adaptor.

**Effect of data adaptor network complexity.** We also explore the effect of data adaptor network complexity on SODA and SODA-R. To increase network complexity, we change the number of ResNet blocks in the data adaptor. Results of data adaptor with {0, 1, 2, 3} ResNet blocks are reported in Table 3. For SODA, accuracy decreases as network complexity increases, indicating that complex networks hinder data adaptation using ZOO. However, the accuracy of SODA-R increases along with network complexity. This contrast illustrates that a more complex data adaptor can achieve higher accuracy but is restricted and even encumbered by zeroth-order gradient estimation.

**SODA with fewer test data points.** We further explore the effectiveness of SODA with fewer test data points. We randomly choose \(\{50,100,500,1000,5000,10000\}\) test data points evenly distributed across 10 classes in CIFAR-10-C for each corruption as smaller test datasets. The averaged accuracies are reported in Figure 4. The performance of SODA is better when training data adaptor over more test data points. Nevertheless, training over 5,000 data points achieves comparable accuracy with training over 10,000 data points, showing that SODA does not require an extremely large number of data points to achieve good performance. Besides, SODA can still improve with less than 500 test data points, providing promising insights for test-time data adaption with smaller test datasets.

**Hyperparameter sensitivity.** We conduct sensitivity analysis regarding the noise ratio \(\rho\), the confidence threshold \(\tau\) and the balancing parameter \(\alpha\). The detailed results illustrated in Appendix C.3 show that SODA is robust to different combinations of hyperparameters, but adapting an adapted threshold instead of a fixed threshold might further improve the performance of SODA.

### SODA for Online Test-Time Adaptation

More practically, test data is not entirely available but arrives sequentially, i.e., online test-time adaptation. Hence, we further implement SODA-O as a variant of SODA under online settings. Given mini-batches of test data arrived sequentially \(\{\mathbf{X}_{1},...,\mathbf{X}_{T}\}\), SODA-O adapts one mini-batch at a time before processing the next mini-batch with knowledge accumulated from the previous mini-batches.

The main difference between SODA-O and SODA lies in the reliable pseudo-label selection. Without access to the test data before adaptation, SODA-O maintains an ordered queue \(\mathbf{Q}\) with maximum size \(S\) to store the selected reliable pseudo-labels \(\mathbf{Y}_{r}\) and their corresponding data points \(\mathbf{X}_{r}\). Specifically,for a mini-batch \(\mathbf{X}_{t}\) arrives at time \(t\), reliable pseudo-labels \(\mathbf{Y}_{r_{t}}\) with prediction confidences higher than \(\tau\) are selected and pushed into \(\mathbf{Q}\) along with their corresponding test data points \(\mathbf{X}_{r_{t}}\). To maintain the class balance in \(\mathbf{Q}\), the pseudo-labels with the smallest confidence for class \(k\) will be popped out once the number of pseudo-labels for \(k\) in \(\mathbf{Q}\) is larger than \(S/C\). Then, the remaining data points \(\mathbf{X}_{u_{t}}\) in \(\mathbf{X}_{t}\) are considered as \(\mathbf{X}^{\prime}_{u_{t}}\), all pseudo-labels and data points stored in \(\mathbf{Q}\) are considered as \(\mathbf{Y}^{\prime}_{r_{t}}\) and \(\mathbf{X}^{\prime}_{r_{t}}\). \(\mathbf{X}^{\prime}_{u_{t}}\) and \(\mathbf{X}^{\prime}_{r_{t}}\) form a small test dataset to train the data adaptor as in SODA, i.e. Step 2 in Algorithm 1. After adaptation, the inference of \(\mathbf{X}_{t}\) is given by the current data adaptor and the deployed model. Note that the optimization in SODA-O is not repeated after reaching the entire test dataset but only repeats for the current test data batch and the cached queue. During the adaptation of the current test data batch, the previous data batches are no longer available except for those saved in the queue. The data adaptor, hyperparameter setting, and training strategy of SODA-O are the same as SODA. The queue size \(S\) = 1,000 for both CIFAR-10-C and CIFAR-100-C.

**Effect of epochs per mini-batch.** We fix batch size = 256 and conduct experiments with different numbers of epochs per mini-batch, i.e., {5, 10, 30, 50, 100, 150} epochs/batch. As shown in Table 5, SODA-O is effective under online setting. As epochs/batch increase, the accuracy of SODA-O also increases and approaches the accuracy of SODA. But more training epochs means more processing time for each mini-batch, leading to a time-accuracy trade-off. With 10 epochs/batch, SODA-O can still improve the deployed model by 5% and 4% on CIFAR-10-C and CIFAR-100-C.

**Effect of batch size.** Fixing epochs/batch = 10, we also conduct experiments with different batch sizes, {32, 64, 128, 256}. The averaged accuracies are shown in Table 4. The results show that the performance of SODA-O is stable across different batch sizes. The best performance is achieved with batch size = 128. Only a slight performance drop is observed when batch size is reduced to 32, indicating that SODA-O can handle relatively small batch size.

## 5 Limitations

While providing valuable insights into robust test-time data adaptation for deployed models with inaccessible parameters, it is also essential to consider the limitations that may have affected the efficiency and effectiveness of our proposed framework. The main limitation of SODA is the time consumption brought by i) multiple queries required for gradient estimation in ZOO and ii) multiple passes required for training the random initialized data adaptor. In the future, one solution to alleviate this issue could be leveraging ZOO methods with higher query efficiency and less estimation error. Moreover, it is unclear whether SODA designed for OOD generalization can benefit OOD detection [8; 44]. Thus, we leave the exploration for OOD detection as our future work.

## 6 Conclusion

This paper focuses on two major challenges in adapting deployed machine learning models with inaccessible parameters at test-time: unmodifiable model parameters and infeasible gradient computation. Without modifying the model parameters, a data adaptor is adopted to adapt test data to the deployed model. Zeroth-order optimization is further leveraged to train the data adaptor with estimated gradients. Revisiting ZOO in test-time data adaptation, we discover that the unreliable gradient estimation in ZOO is due to the unreliable pseudo-labels assigned to test data. The proposed pseudo-label-robust data adaptation (SODA) addresses this issue with reliable pseudo-label selection and input information maximization. Our experiments on three widely used out-of-distribution benchmarks demonstrate the effectiveness of SODA in both offline and online settings.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Methods & Deployed & \multicolumn{6}{c}{SODA-O} & SODA \\ \hline Epochs/Batch & - & 5 & 10 & 30 & 50 & 100 & 150 & 150* \\ \hline CIFAR-10-C & 72.39 & 75.22 & 77.03 & 79.63 & 80.38 & 81.33 & 81.71 & 82.55 \\ CIFAR-100-C & 41.41 & 43.59 & 45.81 & 48.56 & 49.26 & 50.04 & 50.12 & 52.41 \\ \hline \multicolumn{8}{l}{*SODA is trained over the entire test dataset for 150 epochs} \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average accuracies (%) on CIFAR-10-C and CIFAR-100-C under online setting with different number of epochs per batch (batch size = 256).

## Acknowledgments and Disclosure of Funding

This work was partially supported by the National Natural Science Foundation of China (No. 62372459, No. 62376282).

## References

* [1] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [2] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In _Advances in Neural Information Processing Systems_, 2019.
* [3] Malik Boudiaf, Jerome Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben Ayed. A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses. In _European Conference on Computer Vision_, 2020.
* [4] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual information and'phantom targets. In _Advances in Neural Information Processing Systems_, 1991.
* [5] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In _ACM Workshop on Artificial Intelligence and Security_, 2017.
* [6] John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. _IEEE Transactions on Information Theory_, 61(5):2788-2806, 2015.
* [7] Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Scholkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. In _International Conference on Learning Representations_, 2022.
* [8] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection learnable? In _Advances in Neural Information Processing Systems_, 2022.
* [9] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In _International Conference on Machine Learning_, 2015.
* [10] Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, and Dequan Wang. Back to the source: Diffusion-driven test-time adaptation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2016.
* [12] Yufan He, Aaron Carass, Lianrui Zuo, Blake E Dewey, and Jerry L Prince. Autoencoder based self-supervised test-time adaptation for medical image analysis. _Medical image analysis_, 72:102136, 2021.
* [13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _International Conference on Computer Vision_, 2021.
* [14] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019.
* [15] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. In _Advances in Neural Information Processing Systems_, 2021.
* [16] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In _Advances in Neural Information Processing Systems_, 2021.
* [17] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. In _International Conference on Learning Representations_, 2023.

* Karani et al. [2021] Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. _Medical Image Analysis_, 68:101907, 2021.
* Krizhevsky et al. [2017] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* LeCun et al. [2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _Nature_, 521(7553):436-444, 2015.
* Liang et al. [2020] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning_, 2020.
* Liang et al. [2022] Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* Lim et al. [2023] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test-time adaptation. In _International Conference on Learning Representations_, 2023.
* Liu et al. [2018] Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method of multipliers: Convergence analysis and applications. In _International Conference on Artificial Intelligence and Statistics_, 2018.
* Liu et al. [2020] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. _IEEE Signal Processing Magazine_, 37(5):43-54, 2020.
* Liu et al. [2021] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In _Advances in Neural Information Processing Systems_, 2021.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [28] TorchVision maintainers and contributors. Torchvision: Pytorch's computer vision library. https://github.com/pytorch/vision, 2016.
* Niu et al. [2023] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In _International Conference on Learning Representations_, 2023.
* Pan and Yang [2010] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _IEEE Transactions on Knowledge and Data Engineering_, 22(10):1345-1359, 2010.
* Pandey et al. [2021] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference-time label-preserving target projections. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
* Peng et al. [2022] Qucheng Peng, Zhengming Ding, Lingjuan Lyu, Lichao Sun, and Chen Chen. Toward better target representation for source-free and black-box domain adaptation. _arXiv preprint arXiv:2208.10531_, 2022.
* Poursaeed et al. [2018] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturbations. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2018.
* Schneider et al. [2020] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In _Advances in Neural Information Processing Systems_, 2020.
* Shen et al. [2021] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* Shi and Sha [2012] Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In _International Conference on Machine Learning_, 2012.

* [37] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In _Advances in Neural Information Processing Systems_, 2022.
* [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. In _Advances in Neural Information Processing Systems_, 2022.
* [39] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _International Conference on Machine Learning_, 2020.
* [40] Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho. Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources. In _International Conference on Machine Learning_, 2020.
* [41] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In _International Conference on Learning Representations_, 2020.
* [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, 2017.
* [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In _International Conference on Learning Representations_, 2020.
* [44] Qizhou Wang, Feng Liu, Yonggang Zhang, Jing Zhang, Chen Gong, Tongliang Liu, and Bo Han. Watermarking for out-of-distribution detection. In _Advances in Neural Information Processing Systems_, 2022.
* [45] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In _International Conference on Learning Representations_, 2021.
* [46] Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. In _International Conference on Learning Representations_, 2023.
* [47] Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Scholkopf, and Kun Zhang. Adversarial robustness through the lens of causality. In _International Conference on Learning Representations_, 2022.
* [48] Yonggang Zhang, Ya Li, Tongliang Liu, and Xinmei Tian. Dual-path distillation: A unified framework to improve black-box attacks. In _International Conference on Machine Learning_, 2020.
* [49] Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. In _International Conference on Learning Representations_, 2023.
* [50] Xingchen Zhao, Chang Liu, Anthony Sicilia, Seong Jae Hwang, and Yun Fu. Test-time fourier style calibration for domain generalization. In _International Joint Conference on Artificial Intelligence_, 2022.
* [51] Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. _arXiv preprint arXiv:2112.08524_, 2021.

## Appendix A Derivation of Directional Derivative Approximation in SODA

In Section 3.2, given a deployed model \(\mathbf{M}\), the ideal objective function of training the data adaptor \(\mathbf{G}\) with parameters \(\bm{\theta}\) in SODA is the KL divergence between the predicted probability \(\hat{\mathbf{p}}_{i}^{\bm{\theta}}=\mathbf{M}\circ\mathbf{G}(\mathbf{x}_{i};\bm {\theta})\) of the adapted data point \(\mathbf{x}_{i}^{\bm{\theta}}=\mathbf{G}(\mathbf{x}_{i};\bm{\theta})\) and the true label \(\mathbf{y}_{i}\) of the original data point \(\mathbf{x}_{i}\). Because \(\mathbf{y}_{i}\) is not available at test time, pseudo-label \(\hat{\mathbf{y}}_{i}\) predicted by \(\mathbf{M}\) is adopted as a substitute of \(\mathbf{y}_{i}\). Due to the inaccurate model prediction under distribution shifts, there is a disturbance \(\bm{\sigma}_{i}\) in \(\hat{\mathbf{y}}_{i}\) compared to \(\mathbf{y}_{i}\), i.e. \(\hat{\mathbf{y}}_{i}=\bm{\sigma}_{i}+\mathbf{y}_{i}\). Hence, the KL divergence loss \(\mathcal{L}(\cdot,\cdot):=\mathrm{KL}(\cdot\|\cdot)\) at test data point \(\mathbf{x}_{i}\) is

\[\begin{split}\mathcal{L}_{i}&=KL(\hat{\mathbf{y}}_{i }\|\hat{\mathbf{p}}_{i}^{\bm{\theta}})=\hat{\mathbf{y}}_{i}\log\frac{\hat{ \mathbf{y}}_{i}}{\hat{\mathbf{p}}_{i}^{\bm{\theta}}}\\ &=(\mathbf{y}_{i}+\bm{\sigma}_{i})\log\frac{\mathbf{y}_{i}+\bm{ \sigma}_{i}}{\hat{\mathbf{p}}_{i}^{\bm{\theta}}}\\ &=(\mathbf{y}_{i}+\bm{\sigma}_{i})\log(\mathbf{y}_{i}+\bm{ \sigma}_{i})-\mathbf{y}_{i}\log\hat{\mathbf{p}}_{i}^{\bm{\theta}}-\bm{\sigma} _{i}\log\hat{\mathbf{p}}_{i}^{\bm{\theta}}\\ &=-H(\mathbf{y}_{i}+\bm{\sigma}_{i})+\mathcal{L}_{\mathrm{ce}}( \mathbf{y}_{i},\hat{\mathbf{p}}_{i}^{\bm{\theta}})-\bm{\sigma}_{i}\log\hat{ \mathbf{p}}_{i}^{\bm{\theta}}\end{split}\] (14)

where \(\mathcal{L}_{\mathrm{ce}}(\mathbf{y}_{i},\hat{\mathbf{p}}_{i}^{\bm{\theta}})\) is the cross entropy loss between \(\mathbf{y}_{i}\) and \(\hat{\mathbf{p}}_{i}^{\bm{\theta}}\). Because the gradient information is inaccessible from the deployed model, zeroth-order optimization (ZOO) is utilized to estimate gradients for the training of the data adaptor in SODA. To do this, the objective function \(f(\bm{\theta})\) in Eq. (1) is replaced with the training objective function \(\mathcal{L}_{i}\) in test-time data adaptation. Denote \(\mathcal{L}_{i}^{\bm{\theta}}\) as the KL divergence loss computed by data adaptor with parameters \(\bm{\theta}\), the directional derivative approximation of ZOO is

\[\begin{split}\widehat{\nabla}_{\bm{\theta}}\hat{\mathcal{L}}_{i }&=\frac{1}{\mu q}\sum_{j=1}^{q}\big{[}\big{(}KL(\hat{\mathbf{y}}_ {i}\|\hat{\mathbf{p}}_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}})-KL(\hat{\mathbf{y} }_{i}\|\hat{\mathbf{p}}_{i}^{\bm{\theta}})\big{)}\mathbf{u}_{j}\big{]}\\ &=\frac{1}{\mu q}\sum_{j=1}^{q}\big{[}\big{(}(\mathcal{L}_{ \mathrm{ce}}(\mathbf{y},\hat{\mathbf{p}}_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}} )-\bm{\sigma}_{i}\log\hat{\mathbf{p}}_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}})-( \mathcal{L}_{\mathrm{ce}}(\mathbf{y},\hat{\mathbf{p}}_{i}^{\bm{\theta}})-\bm{ \sigma}_{i}\log\hat{\mathbf{p}}_{i}^{\bm{\theta}})\big{)}\mathbf{u}_{j}\big{]} \\ &=\frac{1}{\mu q}\sum_{j=1}^{q}\big{[}\big{(}\mathcal{L}_{\mathrm{ce }}(\mathbf{y},\hat{\mathbf{p}}_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}})-\mathcal{L }_{\mathrm{ce}}(\mathbf{y},\hat{\mathbf{p}}_{i}^{\bm{\theta}})\big{)}\mathbf{u }_{j}\big{]}+\frac{1}{\mu q}\sum_{j=1}^{q}\big{[}\big{(}\bm{\sigma}_{i}\log \hat{\mathbf{p}}_{i}^{\bm{\theta}}-\bm{\sigma}_{i}\log\hat{\mathbf{p}}_{i}^{ \bm{\theta}+\mu\mathbf{u}_{j}}\big{)}\mathbf{u}_{j}\big{]}\\ &=\widehat{\nabla}_{\bm{\theta}}\mathcal{L}_{\mathrm{ce}}+\frac{ \bm{\sigma}_{i}}{\mu q}\sum_{j=1}^{q}\log\frac{\hat{\mathbf{p}}_{i}^{\bm{ \theta}}}{\hat{\mathbf{p}}_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}}}\mathbf{u}_{j}, \end{split}\] (15)

where \(\widehat{\nabla}_{\bm{\theta}}\mathcal{L}_{\mathrm{ce}}=\frac{1}{\mu q}\sum_{j =1}^{q}\big{[}\big{(}\mathcal{L}_{\mathrm{ce}}(\mathbf{y}_{i},\hat{\mathbf{p} }_{i}^{\bm{\theta}+\mu\mathbf{u}_{j}})-\mathcal{L}_{\mathrm{ce}}(\mathbf{y}_ {i},\hat{\mathbf{p}}_{i}^{\bm{\theta}})\big{)}\mathbf{u}_{j}\big{]}\) is the ideal directional derivative approximation.

## Appendix B Implementation Details

### Implementation details of DINE and BETA

The implementations of DINE and BETA on CIFAR-10-C and CIFAR-100-C are kept the same, following their original work [22] and [46]. For DINE, the momentum hyperparameter \(\gamma\) = 0.6 and the Mixup balancing hyperparameter \(\beta\) = 1. For BETA, \(\tau\) = 0.8 for domain division, \(\alpha\) = 1.0 for Mixup, \(\lambda_{mse}\) = 0, sharpening factor \(T\) = 0.5, and adversarial regularizer \(\gamma\) = 0.1. The training strategy of DINE and BETA are both SGD with learning rate = 0.001 for target network backbones and 0.01 for MLP classifiers. momentum = 0.9 and weight decay = 1e-3 are also adopted.

### Software and hardware

In our paper, all models are implemented using PyTorch 1.13.1. The ImageNet pre-trained weights used in DINE and BETA are downloaded from TorchVision 0.14.1. The experiments are conducted using NVIDIA A100-PCIE-40GB GPU with CUDA 11.7.

### Network structure of data adaptor

Figure 5 shows the network structure of the data adaptor used in our experiments. The basic structure of the data adaptor consists of two convolutional layers and an instance normalization layer in between. Multiple ResNet blocks and downsampling/upsampling layers can be inserted into the convolutional layers to form a deeper network as in [33]. For all methods except DA-Direct, the adapted data is generated by treating the network output as perturbation and adding it to the original data.

## Appendix C Additional Analysis

### Discussion about SODA and SODA-R

Compared with SODA, SODA-R uses computed first-order gradients and adopts several techniques to improve the performance, i.e., deeper data adaptor with 2 ResNet blocks, Adam optimizer, perturbation regularization, and dropout. The effect of network complexity has already been discussed in Section 4.3. In this subsection, we first introduce the perturbation regularization used in SODA-R, then evaluate the effect of perturbation regularization, different optimizers, and dropout on SODA and SODA-R.

#### c.1.1 Perturbation regularization in SODA-R

In SODA and SODA-R, the adapted data is computed by perturbing the original data with a generated perturbation. To further restrict the impact of generated perturbations on data \(\mathbf{X}_{r}=\{\mathbf{x}_{r_{1}},\mathbf{x}_{r_{2}},...,\mathbf{x}_{r_{l_{ r}}}\}\) and \(\mathbf{X}_{u}=\{\mathbf{x}_{u_{1}},\mathbf{x}_{u_{2}},...,\mathbf{x}_{u_{l_{ 1}}}\}\), perturbation regularization with \(l_{1}\) norm is used: let \(\mathbf{x}_{r_{i}}^{\boldsymbol{\theta}}\) and \(\mathbf{x}_{u_{i}}^{\boldsymbol{\theta}}\) be the corresponding adapted data of \(\mathbf{x}_{r_{i}}\) and \(\mathbf{x}_{r_{u}}\),

\[\mathcal{R}(\mathbf{X})=\mathbb{E}_{\mathbf{x}_{i}\in\mathbf{X}_{r}}\big{\|} \mathbf{x}_{r_{i}}^{\boldsymbol{\theta}}-\mathbf{x}_{r_{i}}\big{\|}_{1}+ \mathbb{E}_{\mathbf{x}_{i}\in\mathbf{X}_{u}}\big{\|}\mathbf{x}_{u_{i}}^{ \boldsymbol{\theta}}-\mathbf{x}_{u_{i}}\big{\|}_{1}.\] (16)

First-order gradients of the perturbation regularization are directly computed and back-propagated through the data adaptor. Hence, the training objective of SODA-R becomes:

\[\mathcal{L}_{\mathrm{all}}(\mathbf{X},\hat{\mathbf{Y}}_{r})=\mathcal{L}_{ \mathrm{im}}(\mathbf{X}_{u})+\alpha\mathcal{L}_{\mathrm{ce}}(\mathbf{X}_{r}, \hat{\mathbf{Y}}_{r})+\beta\mathcal{R}(\mathbf{X}),\] (17)

where \(\beta\) is the weight of perturbation regularization and is set to be 0.005 for CIFAR-10-C and 0.01 for CIFAR-100-C.

#### c.1.2 Evaluation of perturbation regularization in SODA and SODA-R

We evaluate the effect of perturbation regularization in SODA and SODA-R on CIFAR-10-C and CIFAR-100-C. Except for the perturbation regularization term in the training objective, all other settings are kept the same as in the main experiments. The results are shown in Table 6 and Table 7.

Figure 5: Network structure of data adaptor. The green block is the convolutional layer, the blue block is the instance normalization layer, the white block is the dropout layer, and the orange block is the downsampling/upsampling layer. The dashed blocks can be added or removed to form different network structures. The red dashed line means the network output is added to the original data to generate the adapted data.

It shows that perturbation regularization can improve the performance of SODA-R using first-order optimization, especially on CIFAR-100-C. However, it largely hinders the performance of SODA using zeroth-order optimization. The computed first-order gradients of perturbation regularization are more accurate than the estimated zeroth-order gradients of the main training objective. Thus, the data adaptor tends to optimize the perturbation regularization term first, resulting in perturbations with too small norms. The perturbations with too small norms do not have enough ability to modify the test data, which might be the reason for the worse performance achieved by SODA with perturbation regularization. One possible solution to this problem could be treating perturbation regularization as an optimization constraint and using constrained ZOO methods to train the data adaptor.

#### c.1.3 Evaluation of optimizers in SODA and SODA-R

We evaluate the effect of optimizers used in SODA and SODA-R on CIFAR-10-C and CIFAR-100-C. Except for the optimizer used to train the data adaptor, all other settings are kept the same as in the main experiments. The results are shown in Table 8 and Table 9. On CIFAR-10-C, SODA trained by SGD and Adam achieve almost the same accuracy, while SODA-R trained by Adam achieves 3.3% higher accuracy than SGD. On CIFAR-100-C, SODA-R trained by Adam still outperforms SODA-R trained by SGD, but SODA trained by Adam achieves even worse accuracy than SODA trained by SGD. It shows that Adam optimizer can improve the training of data adaptors using first-order gradients but fails when using the estimated zeroth-order gradients.

#### c.1.4 Evaluation of dropout in SODA and SODA-R

We also evaluate the effect of dropout on SODA and SODA-R. As depicted in Figure 5, a dropout layer can be inserted into the ResNet block. We conduct experiments using data adaptors with and without dropout layers for SODA and SODA-R. To keep the same network structure as SODA-R, the data adaptor used in SODA also has 2 ResNet blocks. The dropout ratio is set to be 0.5. All other settings are kept the same as in the main experiments. Table 10 and Table 11 show the results on CIFAR-10-C and CIFAR-100-C, respectively. For SODA-R, adding dropout layers can improve the accuracy by 0.7% on CIFAR-10-C and 2% on CIFAR-100-C. However, for SODA, adding dropout layers extremely hinders the performance, especially on CIFAR-100-C. This contrast indicates that dropout harms data adaptor optimized using estimated zeroth-order gradients while positively affecting data adaptor optimized using computed first-order gradients. The reason might be that the extra randomness introduced by dropout increases the difficulty of gradient estimation in zeroth-order optimization. Note that the accuracy of SODA using a data adaptor with 2 ResNet blocks on CIFAR-100-C is worse than that using a data adaptor with 0 ResNet blocks, which is consistent with the results on CIFAR-10-C as shown in Table 3.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & SODA & SODA-R \\ \hline w/ regularization & 73.40 & 88.39 \\ w/o regularization & 82.55 & 87.96 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of SODA and SODA-R with and without perturbation regularization on CIFAR-10-C. \(\beta=0.005\) in experiments w/ regularization.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & SODA & SODA-R \\ \hline SGD & 82.55 & 84.95 \\ Adam & 82.75 & 88.39 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparing of SODA and SODA-R using SGD and Adam optimizer on CIFAR-10-C.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & SODA & SODA-R \\ \hline SGD & 52.41 & 58.32 \\ Adam & 49.75 & 60.31 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison of SODA and SODA-R using SGD and Adam optimizer on CIFAR-100-C.

To sum up, compared with SODA using zeroth-order optimization, SODA-R uses first-order optimization and adopts deeper network structure, perturbation regularization, Adam optimizer, and dropout to improve the performance. However, these techniques cannot improve or even hinder the performance of SODA. This comparison shows that the common boosting strategies used in first-order optimization cannot be directly applied to zeroth-order optimization, leading to the limited performance of methods using zeroth-order optimization.

### Convergence of SODA

In Figure 3, the convergence speeds of SODA on CIFAR-10-C and CIFAR-100-C are slower than SODA-FO and SODA-R and do not achieve complete convergence after training with 150 epochs. We further train SODA on Gaussian noise in CIFAR-10-C and CIFAR-100-C for 300 epochs to show the complete convergence of SODA as depicted in Figure 6. With more training epochs, SODA can achieve higher accuracies on both datasets. However, training with more epochs means more adaptation processing time or more computing resources with parallel computation. For time and resource efficiency, we only report the accuracies achieved at 150 epochs in our main experiments, which already improves the deployed model by a large margin. If computing time and resources are not restricted, SODA can further improve the deployed model for higher accuracy.

### Hyperparameter analysis of reliable pseudo-label selection

We evaluate the hyperparameters in reliable pseudo-label selection, namely the confidence threshold \(\tau\), the noise ratio \(\rho\), and the balancing parameter \(\alpha\). \(\tau\) and \(\rho\) controls the number of selected reliable pseudo-labels. With lower \(\tau\) and lower \(\rho\), the number of selected reliable pseudo-labels increases. We evaluate \(\tau\) in \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\), and \(\rho\) in \(\{0.1,0.3,0.5,0.7,0.9\}\). Note that when \(\tau=0\), the number of selected pseudo-labels is not equal to \((1-\rho)n\), where \(n\) is the total number of test data points, because the pseudo-labels are not evenly distributed across classes as depicted in Figure 7(a). The inaccurate model prediction tends to bias towards a few classes, leading to more pseudo-labels belonging to those classes. \(\alpha\) controls the balance between the supervised training

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & SODA & SODA-R \\ \hline w/ dropout & 32.19 & 88.39 \\ w/o dropout & 80.56 & 87.54 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparing of SODA and SODA-R with and without dropout layers on CIFAR-10-C.

Figure 6: Accuracy convergence on Gaussian noise in (a) CIFAR-10-C and (b) CIFAR-100-C for 300 epochs.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & SODA & SODA-R \\ \hline w/ dropout & 5.47 & 60.31 \\ w/o dropout & 43.96 & 58.27 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison of SODA and SODA-R with and without dropout layers on CIFAR-100-C.

objective \(\mathcal{L}_{\mathrm{ce}}\) and the unsupervised training objective \(\mathcal{L}_{\mathrm{im}}\). We evaluate \(\alpha\) in \(\{0.01,0.001,0.0001\}\). The results of SODA with different sets of hyperparameters on CIFAR-10-C Gaussian noise level 5 corruption are shown in Figure 7. The performance of SODA is stable across different hyperparameter settings. A common trend among different \(\alpha\) is that accuracy tends to increase when \(\tau\) and \(\rho\) decrease, i.e., the top-right corner of each figure. This trend shows that the performance of the data adaptor can be improved using more selected pseudo-labels, which further indicates the reliability of the selected pseudo-labels. There is a mild tendency of performance drop in the overall performance of SODA when unsupervised learning of test points with unreliable pseudo-labels is overwhelmed by supervised learning of reliable pseudo-labels with larger \(\alpha\), indicating that learning on test points with unreliable pseudo-labels also has a contribution to the performance of SODA. To balance the supervised and unsupervised learning terms, we finally choose \(\alpha\) = 0.0001 in our main experiments. Although better performance can be achieved by carefully fine-tuning \(\tau\) and \(\rho\) with a validation set to show the general performance of SODA and select the most reliable pseudo-labels for different corruptions, we set \(\tau\) = 0.9 and \(\rho\) = 0.9 for CIFAR-10/100-C tasks and \(\tau\) = 0.1 and \(\rho\) = 0.9 for ImageNet-C task in our main experiments without elaborated hyperparameter fine-tuning.

### Evaluation of queue size in SODA-O

We evaluate the effect of queue size in SODA-O. A larger queue size means more past reliable pseudo-labels and their corresponding test data points are stored and used in the adaptation process of the current mini-batch. Fixing batch size = 128, we conduct experiments on queue size \(\{500,1000,2000,3000\}\), and the results are shown in Table 12. The performance of SODA-O is stable across different queue sizes, especially when the queue size is smaller. When queue size increases, the ratio of reliable pseudo-labels used to train the data adaptor for the current mini-batch also increases. It makes the training of the data adaptor more biased towards the supervised training with the reliable pseudo-labels. Thus, the mild performance drop observed along with the larger queue size might indicate that the reliable pseudo-labels still have disturbance, and the unsupervised training of data points with unreliable pseudo-labels is useful to alleviate the negative effect caused by the remaining disturbance.

### Results on challenging Office-Home tasks

We conduct experiments on three challenging Office-Home tasks to show the effectiveness of SODA and SODA-R on different kinds of distribution shifts. For SODA, the network structure of the

Figure 7: Evaluation of reliable pseudo-label selecting hyperparameters on CIFAR-10-C Gaussian noise corruption level 5. Numbers are prediction accuracies (%) after adaptation.

data adaptor is the same as that in CIFAR-10/100-C tasks. For SODA-R, a deeper data adaptor is adopted with 2 downsampling/upsampling layers and 2 ResNet blocks. Both SODA and SODA-R are trained using SGD with learning rate = 1e-3, momentum = 0.9, and weight decay = 1e-5 for 60 epochs. A warmup process is used to first train the data adaptor with pseudo-label supervision for 10 epochs. Following DINE [22] and BETA [46], MixMatch [2] and EMA pseudo-label updating are also adopted. The weights of cross-entropy loss and mutual information loss are set to be 0.1 and 0.01, respectively. The confidence threshold \(\tau\) is set to be 0.5. The noise ratio \(\rho\) is set to be 0. The query number is set to 5 for SODA. The results are listed in Table 13. The results show that SODA and SODA-R can improve the deployed model in challenging Office-Home tasks with initial prediction accuracies lower than 50%

## Appendix D Qualitative Evaluation of SODA

### T-SNE Visualization of SODA features

To qualitatively evaluate the performance of SODA, we use T-SNE to visualize the feature embeddings of SODA, i.e., the input features of the last classification layer in the deployed model before and after adaptation in Figure 8. According to the visualization results, the feature embeddings are much more separated apart between classes after adaptation, showing the effectiveness of SODA.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & FO Grad. & Model Mod. & Art-\textgreater{}Clipart & Product-\textgreater{}Clipart & Realworld-\textgreater{}Clipart \\ \hline Deployed & - & - & 44.67 & 40.53 & 47.12 \\ SODA & ✗ & ✗ & 46.53 & 41.47 & 48.71 \\ SODA-R & ✓ & ✗ & 50.45 & 45.20 & 53.22 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Accuracies (%) on three challenging Office-Home domain adaption tasks. **FO Grad.** means the requirement of first-order gradient from the target model or deployed model. **Model Mod.** means the requirement of modifying the parameters of deployed models.

Figure 8: T-SNE visualization of SODA feature embeddings on CIFAR-10-C pixelate corruption level 5.

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_EMPTY:21]

Figure 9: Examples of test data and adapted data using SODA for 19 corruptions in CIFAR-10-C. The bottom-right data is the original data in the CIFAR-10 test dataset without corruption.