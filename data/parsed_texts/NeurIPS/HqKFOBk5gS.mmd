Shallow Diffuse: Robust and Invisible Water-Marking through Low-Dimensional Subspaces in Diffusion Models

Wenda Li\({}^{1*}\) Huijie Zhang\({}^{1*}\) Qing Qu\({}^{1}\)

\({}^{1}\)Department of Electrical Engineering & Computer Science, University of Michigan

{wdli,huijiezh,qingqu}@umich.edu

###### Abstract

The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce _Shallow Diffuse_, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, _Shallow Diffuse_ decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our _Shallow Diffuse_ outperforms existing watermarking methods in terms of robustness and consistency.

## 1 Introduction

Diffusion models (Ho et al., 2020; Song et al., 2021) have recently become a new dominant family of generative models, powering various commercial applications such as Stable Diffusion (Rombach et al., 2022; Esser et al., 2024), DALL-E (Ramesh et al., 2022; Betker et al., 2023), Imagen (Sahara et al., 2022) Stable Audio (Evans et al., 2024) and Sora (Brooks et al., 2024). These models have significantly advanced the capabilities of text-to-image, text-to-audio, text-to-video, and multi-modal generative tasks. However, the widespread usage of AI-generated content from commercial diffusion models on the Internet has raised several serious concerns: (a) AI-generated misinformation presents serious risks to societal stability by spreading unauthorized or harmful narratives on a large scale (Zellers et al., 2019; Goldstein et al., 2023; Brundage et al., 2018); (b) the memorization of training data by those models (Gu et al., 2023; Sompeplalli et al., 2023; Wen et al., 2023; Zhang et al., 2024) challenges the originality of the generated content and raises potential copyright infringement issues; (c) Iterative training on AI-generated content, known as model collapse (Fu et al., 2024; Alemohammad et al., 2024; Dohmatob et al., 2024; Shumailov et al., 2024; Gibney, 2024) can degrade the quality and diversity of outputs over time, resulting in repetitive, biased, or low-quality generations that may reinforce misinformation and distortions in the wild Internet.

To deal with these challenges, watermarking is a crucial technique for identifying AI-generated content and mitigating its misuse. Typically, it can be applied in two main scenarios: (a) _the server scenario_: where given an initial random seed, the watermark is embedded to the image during the generation process; and (b) _the user scenario_: where given a generated image, the watermark is injected in a post-process manner; (as shown in the left two blocks in Figure 3). Traditional watermarking methods (Cox et al., 2007; Solachidis and Pitas, 2001; Chang et al., 2005; Liu et al., 2019) are mainly designed for the user scenario, embedding detectable watermarks directly into images with minimal modification. However, these methods are vulnerable to attacks. For example, the watermarks can become undetectable with simple corruptions such as blurring on watermarked images. More recent methods considered the server scenario (Zhang et al., 2024; Fernandez et al., 2023; Wen et al., 2023; Yang et al., 2024; Ci et al., 2024), where they improve robustness by integratingwatermarking into the sampling process of diffusion models. For example, the work (Ci et al., 2024; Wen et al., 2023a) embeds the watermark into the initial random seed in the Fourier domain and then samples an image from the watermarked seed. As illustrated in Figure 1, these approaches often lead to inconsistent watermarked images because they significantly alter the noise distribution away from Gaussian. Moreover, they require access to the initial random seed, limiting their use in the user scenario. To the best of our knowledge, there is currently no robust and consistent watermarking method suitable for both the server and user scenarios (more detailed discussion about related works could be found in Appendix A).

To address these limitations, we proposed _Shallow Diffuse_, a robust and consistent watermarking approach that can be employed for both the server and user scenarios. Unlike prior works (Ci et al., 2024; Wen et al., 2023a) that embed watermarks into the initial random seed and entangle the watermarking process with sampling, Shallow Diffuse decouples these two steps by leveraging the low-dimensional subspace in the generation process of diffusion models (Wang et al., 2024; Chen et al., 2024). The key insight is that, due to the low dimensionality of the subspace, a significant portion of the watermark will lie in the null space of this subspace, effectively separating the watermarking from the sampling process (see Figure 3 for an illustration). Our theoretical and empirical analyses demonstrate that this decoupling strategy significantly improves the consistency of the watermark. With better consistency as well as independence from the initial random seed, Shallow Diffuse is flexible for both server and user scenarios.

**Our contributions.** The proposed Shallow Diffuse offers several key advantages over existing watermarking techniques (Cox et al., 2007; Solachidis and Pitas, 2001; Chang et al., 2005; Liu et al., 2019; Zhang et al., 2024; Fernandez et al., 2023; Wen et al., 2023a; Yang et al., 2024; Ci et al., 2024) that we highlight below:

* **Flexibility.** Watermarking via Shallow Diffuse works seamlessly under both server-side and user-side scenarios. In contrast, most of the previous methods only focus on one scenario without a straightforward extension to the other; see Table 1 and Table 2 for demonstrations.
* **Consistency and Robustness.** By decoupling the watermarking from the sampling process, Shallow Diffuse achieves higher robustness and better consistency. Extensive experiments (Table 1 and Table 2 ) support our claims, with extra ablation studies in Figure 3(a) and Figure 3(b).
* **Provable Guarantees.** Unlike previous methods, the consistency and detectability of our approach are theoretically justified. Assuming a proper low-dimensional image data distribution (see Assumption 1), we rigorously establish bounds for consistency (Theorem 1) and detectability (Theorem 2).

Figure 1: **Sampling variance of Tree-Ring Watermarks, RingID and Shallow Diffuse. On the left are the original images, and on the right are the corresponding watermarked images generated using three different techniques: Tree-Ring (Wen et al., 2023a), RingID (Ci et al., 2024), and Shallow Diffuse. For each technique, we generated watermarks using two distinct random seeds, resulting in the respective watermarked images.**

## 2 Preliminaries

We start by reviewing the basics of diffusion models (Ho et al., 2020; Song et al., 2021b; Karras et al., 2022), followed by several key empirical properties that will be used in our approach: the low-rankness and local linearity of the diffusion model (Wang et al., 2024; Chen et al., 2024).

### Preliminaries on diffusion models

Basics of diffusion models.In general, diffusion models consist of two processes:

* _The forward diffusion process._ The forward process progressively perturbs the original data \(\bm{x}_{0}\) to a noisy sample \(\bm{x}_{t}\) for some integer \(t\in[0,T]\) with \(T\in\mathbb{Z}\). As in Ho et al. (2020), this can be characterized by a conditional Gaussian distribution \(p_{t}(\bm{x}_{t}|\bm{x}_{0})=\mathcal{N}(\bm{x}_{t};\sqrt{\alpha_{t}}\bm{x}_{ 0},(1-\alpha_{t})\mathbf{I}_{d})\). Particularly, parameters \(\{\alpha_{t}\}_{t=0}^{T}\) sastify: (_i_) \(\alpha_{0}=1\), and thus \(p_{0}=p_{\mathrm{data}}\), and (_ii_) \(\alpha_{T}=0\), and thus \(p_{T}=\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\).
* _The reverse sampling process._ To generate a new sample, previous works Ho et al. (2020); Song et al. (2021a); Lu et al. (2022); Karras et al. (2022) have proposed various methods to approximate the reverse process of diffusion models. Typically, these methods involve estimating the noise \(\bm{\epsilon}_{t}\) and removing the estimated noise from \(\bm{x}_{t}\) recursively to obtain an estimate of \(\bm{x}_{0}\). Specifically, One sampling step of Denoising Implicit Models (DDIM) Song et al. (2021a) from \(\bm{x}_{t}\) to \(\bm{x}_{t-1}\) can be described as: \[\bm{x}_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{\bm{x}_{t}-\sqrt{1- \alpha_{t}}\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)}{\sqrt{\alpha_{t}}}\right) }_{:=\bm{f}_{\bm{\theta},t}(\bm{x}_{t})}+\sqrt{1-\alpha_{t-1}}\bm{\epsilon}_{ \bm{\theta}}(\bm{x}_{t},t),\] (1) where \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)\) is parameterized by a neural network and trained to predict the noise \(\bm{\epsilon}_{t}\) at time \(t\). From previous works Zhang et al. (2024b); Luo (2022), the first term in Equation (1), defined as \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\), is the _posterior mean predictor_ (PMP) that predict the posterior mean \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\). DDIM could also be applied to a clean sample \(\bm{x}_{0}\) and generate the corresponding noisy \(\bm{x}_{t}\) at time \(t\), named DDIM Inversion. One sampling step of DDIM inversion is similar to Equation (1), by mapping from \(\bm{x}_{t-1}\) to \(\bm{x}_{t}\). For any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote multi-time steps DDIM operator and its inversion as \(\bm{x}_{t_{1}}=\texttt{DDIM}(\bm{x}_{t_{2}},t_{1})\) and \(\bm{x}_{t_{2}}=\texttt{DDIM}\texttt{Inv}(\bm{x}_{t_{1}},t_{2})\).

Text-to-image (T2I) diffusion models & classifier-free guidance (CFG).The diffusion model can be generalized from unconditional to T2I (Rombach et al., 2022; Esser et al., 2024), where the latter enables controllable image generation \(\bm{x}_{0}\) guided by a text prompt \(\bm{c}\). In more detail, when training T2I diffusion models, we optimize a conditional denoising function \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t,\bm{c})\). For sampling, we employ a technique called _classifier-free guidance_ (CFG) (Ho and Salimans, 2022), which substitutes the unconditional denoiser \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t,\bm{c})\) in Equation (1) with the conditional counterpart \(\bm{\tilde{\bm{\theta}}}(\bm{x}_{t},t,\bm{c})\) that can be described as \(\bm{\tilde{\bm{\theta}}}(\bm{x}_{t},t,\bm{c})=(1-\eta)\bm{\epsilon}_{\bm{ \theta}}(\bm{x}_{t},t,\bm{\varnothing})+\eta\bm{\epsilon}_{\bm{\theta}}(\bm{x} _{t},t,\bm{c})\). Here, \(\bm{\mathscr{G}}\) denotes the empty prompt and \(\eta>0\) denotes the strength for the classifier-free guidance. For simplification, for any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote multi-time steps CFG operator as \(\bm{x}_{t_{1}}=\texttt{CFG}(\bm{x}_{t_{2}},t_{1},\bm{c})\). DDIM and DDIM inversion could also be generalized to T2I version, denotes as \(\bm{x}_{t_{1}}=\texttt{DDIM}(\bm{x}_{t_{2}},t_{1},\bm{c})\) and \(\bm{x}_{t_{2}}=\texttt{DDIM}\texttt{Inv}(\bm{x}_{t_{1}},t_{2},\bm{c})\).

### Local Linearity and Intrinsic Low-Dimensionality in PMP

In this work, we will leverage two key properties of the PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) introduced in Equation (1) for watermarking diffusion models. Parts of these properties have been previously identified in recent papers (Wang et al., 2024; Manor and Michaeli, 2024b;a), and they have been extensively studied in (Chen et al., 2024). At one given timestep \(t\in[0,T]\), let us consider the first-order Taylor expansion of the PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+\lambda\Delta\bm{x})\) at the point \(\bm{x}_{t}\):

\[\boxed{\bm{l}_{\bm{\theta}}(\bm{x}_{t};\lambda\Delta\bm{x})\ :=\ \bm{f}_{\bm{\theta},t}(\bm{x}_{t})+\lambda\bm{J}_{\bm{\theta},t}(\bm{x}_{t}) \cdot\Delta\bm{x},}\] (2)

where \(\Delta\bm{x}\in\mathbb{S}^{d-1}\) is a perturbation direction with unit length, \(\lambda\in\mathbb{R}\) is the perturbation strength, and \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})=\nabla_{\bm{x}_{t}}\bm{f}_{\bm{\theta},t}( \bm{x}_{t})\) is the Jacobian of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\). As shown in (Chen et al., 2024), it hasbeen found that within a certain range of noise levels, the learned PMP \(\bm{f}_{\bm{\theta},t}\) exhibits local linearity, and its Jacobian \(\bm{J}_{\bm{\theta},t}\in\mathbb{R}^{d\times d}\) is low rank:

* **Low-rankness of the Jacobian \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\).** As shown in Figure 2(a) of (Chen et al., 2024), the _rank ratio_ for \(t\in[0,T]\)_consistently_ displays a U-shaped pattern across various network architectures and datasets: (_i_) it is close to \(1\) near either the pure noise \(t=T\) or the clean image \(t=0\), (_ii_) \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) is low-rank (i.e., the numerical rank ratio less than \(10^{-2}\)) for all diffusion models within the range \(t\in[0.2T,0.7T]\), (_iii_) it achieves the lowest value around mid-to-late timestep, slightly differs on different architectures and datasets.
* **Local linearity of the PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\).** As shown in Figure 2(b) of (Chen et al., 2024), the mapping \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) exhibits strong linearity across a large portion of the timesteps, which is consistently true among different architectures trained on different datasets. In particular, the work (Chen et al., 2024) evaluated the linearity of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) at \(t=0.7T\) where the rank ratio is close to the lowest value, showing that \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+\lambda\Delta\bm{x})\approx\bm{l}_{\bm{\theta }}(\bm{x}_{t};\lambda\Delta\bm{x})\) even when \(\lambda=40\),

```
1:Inject watermark:
2:Input: original image \(\bm{x}_{0}\) for the user scenario (initial random seed \(\bm{x}_{T}\) for the server scenario), watermark \(\lambda\Delta\bm{x}\), embedding timestep \(t\),
3:Output: watermarked image \(\bm{x}_{0}^{\star\mathcal{W}}\),
4:if user scenario then
5:\(\bm{x}_{t}=\texttt{DDIM-Inv}\left(\bm{x}_{0},t\right)\)
6:else server scenario
7:\(\bm{x}_{t}=\texttt{DDIM}\left(\bm{x}_{T},t\right)\)
8:endif
9:\(\bm{x}_{t}^{\mathcal{W}}\leftarrow\bm{x}_{t}+\lambda\Delta\bm{x}\), \(\bm{x}_{0}^{\mathcal{W}}\leftarrow\texttt{DDIM}\left(\bm{x}_{t}^{\mathcal{W} },0\right)\)\(\triangleright\) Embed watermark
10:\(\bm{x}_{0}^{\star\mathcal{W}}\leftarrow\texttt{DDIM}\left(\bm{x}_{t},0\right)\), \(\bm{x}_{0}^{\star\mathcal{W}}\leftarrow\texttt{ChannelAverage}\left(\bm{x}_{0}^ {\mathcal{W}},\bm{x}_{0}^{\star}\right)\)\(\triangleright\) Channel Average
11:Return:\(\bm{x}_{0}^{\star\mathcal{W}}\)
12:
13:Detect watermark:
14:Input: Attacked image \(\bm{\bar{x}}_{0}^{\mathcal{W}}\), watermark \(\lambda\Delta\bm{x}\), embedding timestep \(t\),
15:Output: Distance score \(\eta\),
16:\(\bm{\bar{x}}_{t}^{\mathcal{W}}\leftarrow\texttt{DDIM-Inv}\left(\bm{\bar{x}}_{0} ^{\mathcal{W}},t\right)\)
17:\(\eta=\texttt{Detector}\left(\bm{\bar{x}}_{t}^{\mathcal{W}},\lambda\Delta\bm{x}\right)\)
18:Return:\(\eta\) ```

**Algorithm 1** Unconditional Shallow Diffuse

## 3 Watermarking by Shallow-Diffuse

In this section, we introduce Shallow Diffuse for watermarking diffusion models. Building on the benign properties of PMP discussed in Section 2.2, we explain how to inject and detect invisible watermarks in _unconditional_ diffusion models in Section 3.1 and Section 3.2, respectively. Algorithm 1 outlines the overall watermarking method for unconditional diffusion models. In Section 3.3, we extend this approach to _text-to-image_ diffusion models, illustrated in Figure 3.

### Injecting invisible watermarks

Consider an unconditional diffusion model \(\bm{\epsilon}_{\bm{\theta}}(\bm{x}_{t},t)\) as we introduced in Section 2.1. Instead of injecting the watermark \(\Delta\bm{x}\) in the initial noise, we inject it in a particular timestep \(t\in[0,T]\) with

\[\bm{x}_{t}^{\mathcal{W}}=\bm{x}_{t}+\lambda\Delta\bm{x},\] (3)

where \(\lambda\in\mathbb{R}\) is the watermarking strength, \(\bm{x}_{t}=\texttt{DDIM-Inv}\left(\bm{x}_{0},t\right)\) under the user scenario and \(\bm{x}_{t}=\texttt{DDIM}(\bm{x}_{T},t)\) under the server scenario. Based upon Section 2.2, we choose the timestep \(t\) so that the Jacobian of the PMP \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})=\nabla_{\bm{x}_{t}}\bm{f}_{\bm{\theta},t}( \bm{x}_{t})\) is _low-rank_. Moreover, based upon the linearity of PMP discussed in Section 2.2, we approximately have

\[\bm{f}_{\bm{\theta},t}(\bm{x}_{t}^{\mathcal{W}})\ =\ \bm{f}_{\bm{\theta},t}(\bm{x}_{t})+ \lambda\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\cdot\Delta\bm{x}\ \approx\ \bm{f}_{\bm{\theta},t}(\bm{x}_{t})\ =\ \hat{\bm{x}}_{0,t},\] (4)where we select the watermark \(\Delta\bm{x}\) to span the entire space \(\mathbb{R}^{d}\)_uniformly_; a more detailed discussion on the pattern design of \(\Delta\bm{x}\) is provided in Section 3.2. The key intuition for Equation (4) to hold is that, when \(r_{t}=\operatorname{rank}(\bm{J}_{\bm{\theta},t}(\bm{x}_{t}))\ll d\) is low, a significant proportion of \(\lambda\Delta\bm{x}\) lies in the _null space_ of \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\) so that \(\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\Delta\bm{x}\thickapprox\bm{0}\).

Therefore, the selection of \(t\) is based on ensuring that \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) is locally linear and that the dimensionality of its Jacobian \(r_{t}\ll d\). In practice, we choose \(t=0.3T\) based on results from the ablation study in Section 4.3. As a results, the injection in Equation (4) maintains better consistency without changing the predicted \(\bm{x}_{0}\). In the meanwhile, it is very robust because any attack on \(\bm{x}_{0}\) would remain disentangled from the watermark, so that \(\lambda\Delta\bm{x}\) remains detectable.

Although in practice we employ the DDIM method instead of PMP for sampling high-quality images, the above intuition still carries over to DDIM. From Equation (1), one step sampling of DDIM in terms of \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) becomes:

\[\bm{x}_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\bm{f}_{\bm{\theta},t}(\bm{x}_{t} )}_{\text{predicted }\bm{x}_{0}}+\frac{\sqrt{1-\alpha_{t-1}}}{\sqrt{1-\alpha_{t}}} \underbrace{(\bm{x}_{t}-\sqrt{\alpha_{t}}\bm{f}_{\bm{\theta},t}(\bm{x}_{t}))}_ {\text{``the direction pointing to }\bm{x}_{t}}.\] (5)

As explained in Song et al. (2021), the first term predicts \(\bm{x}_{0}\) while the second term points towards \(\bm{x}_{t}\). When we inject the watermark \(\Delta\bm{x}\) into \(\bm{x}_{t}\) as given in Equation (3), we know that

\[\bm{x}_{t-1}^{\mathcal{W}} =\sqrt{\alpha_{t-1}}\bm{f}_{\bm{\theta},t}(\bm{x}_{t}^{\mathcal{W }})+\frac{\sqrt{1-\alpha_{t-1}}}{\sqrt{1-\alpha_{t}}}\left(\bm{x}_{t}^{ \mathcal{W}}-\sqrt{\alpha_{t}}\bm{f}_{\bm{\theta},t}(\bm{x}_{t}^{\mathcal{W}} )\right)\] \[\approx\sqrt{\alpha_{t-1}}\bm{f}_{\bm{\theta},t}(\bm{x}_{t})+ \frac{\sqrt{1-\alpha_{t-1}}}{\sqrt{1-\alpha_{t}}}\left(\bm{x}_{t}+\lambda \Delta\bm{x}-\sqrt{\alpha_{t}}\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\right),\] (6)

where the second approximation follows from Equation (4). This implies that the watermark \(\lambda\Delta\bm{x}\) is embedded into the DDIM sampling process entirely through the second term of Equation (6) and it decouples from the first which predicts \(\bm{x}_{0}\). Therefore, similar to our analysis for PMP, the first term in equation 6 maintains the consistency of data generation, while the difference in second term highlighted by blue would be useful for detecting the watermark which we will discuss next. In Appendix D, we provide more rigorous proofs validating the consistency and detectability of our approach.

### Watermark Design and Detection

Second, building on the watermark injection method described in Section 3.1, we discuss the design of the watermark pattern and the techniques for effective detection.

Watermark pattern design.Building on the method proposed by Wen et al. (2023), we inject the watermark in the frequency domain to enhance robustness against adversarial attacks. Specifically, we adapt this approach by defining a watermark \(\lambda\Delta\bm{x}\) for the input \(\bm{x}_{t}\) at timestep \(t\) as follows:

\[\lambda\Delta\bm{x}\ :=\ \textsc{DFT-Inv}\left(\textsc{DFT}\left(\bm{x}_{t} \right)\odot(1-\bm{M})+\bm{W}\odot\bm{M}\right)-\bm{x}_{t},\] (7)

where the Hadamard product \(\odot\) denotes the element-wise multiplication. Additionally, we have the following for Equation (7):

* **Transformation into the frequency domain.** Let \(\textsc{DFT}(\cdot)\) and \(\textsc{DFT-Inv}(\cdot)\) represent the forward and inverse Discrete Fourier Transform (DFT) operators, respectively. As shown in Equation (7), we first apply \(\textsc{DFT}(\cdot)\) to transform \(\bm{x}_{t}\) into the frequency domain, where we then introduce the watermark via a mask. Finally, the modified input is transformed back into the pixel domain using \(\textsc{DFT-Inv}(\cdot)\).
* **The mask and key of watermarks.**\(\bm{M}\) is the mask used to apply the watermark in the frequency domain as shown in the top-left of Figure 2, and \(\bm{W}\) denotes the key of the watermark. Typically, the mask \(\mathbf{M}\) is circular, with the white area representing \(1\) and the black area representing \(0\) in Figure 2, where we use it to modify specific frequency bands of the image. In the following, we discuss the design of \(\bm{M}\) and \(\bm{W}\) in detail.

Previous methods (Wen et al., 2023; Ci et al., 2024) design the mask \(\bm{M}\) to modify the low-frequency components of the initial noise input. While this approach works, as most of the energy in natural images is concentrated in the low-frequency range, it tends to distort the image when such watermarks are injected (see Figure 1 for an illustration). In contrast, as shown in Figure 2, we design the mask \(\bm{M}\) to target the high-frequency components of the image. Since high-frequency components capture fine details where the energy is less concentrated on these bands, modifying them results in less distortion of the original image. This is especially true in our case because we are modifying \(\bm{x}_{t}\), which is closer to \(\bm{x}_{0}\), compared to the initial noise used in (Wen et al., 2023; Ci et al., 2024).To modify the high-frequency components, we apply the DFT without shifting and centering the zero frequency, as illustrated in the bottom-left of Figure 2.

In terms of designing the key \(\bm{W}\), we follow Wen et al. (2023). The key \(\bm{W}\) is composed of multi-rings and each ring has the same value that is drawn from Gaussian distribution; see the top-right of Figure 2 for an illustration. Further ablation studies on the choice of \(\bm{M}\), \(\bm{W}\), and the effects of selecting low-frequency or high-frequency regions for watermarking can be found in Table 3.

**Watermark detection.** During watermark detection, suppose we are given a watermarked image \(\tilde{\bm{x}}_{0}^{\mathcal{W}}\) with certain corruptions, we apply the DDIM Inversion to recover the watermarked image at timestep \(t\), denoted as \(\tilde{\bm{x}}_{t}^{\mathcal{W}}=\texttt{DDIM-Inv}\left(\tilde{\bm{x}}_{0}^{ \mathcal{W}},t\right)\). To detect the watermark, following Wen et al. (2023); Zhang et al. (2024), the \(\texttt{Detector}(\cdot)\) in Algorithm 1 calculates the following p-value:

\[\eta=\frac{\texttt{sum}(\bm{M})\cdot\|\bm{M}\odot\bm{W}-\bm{M}\odot\texttt{ DFT}\left(\tilde{\bm{x}}_{t}^{\mathcal{W}}\right)\|_{F}^{2}}{\|\bm{M}\odot \texttt{DFT}\left(\tilde{\bm{x}}_{t}^{\mathcal{W}}\right)\|_{F}^{2}},\] (8)

where \(\texttt{sum}(\cdot)\) is the summation of all elements of the matrix. Ideally, if \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\) is a watermarked image, \(\bm{M}\odot\bm{W}=\bm{M}\odot\texttt{DFT}\left(\tilde{\bm{x}}_{t}^{\mathcal{W}}\right)\) and \(\eta=0\). When \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\) is a non-watermarked image, \(\bm{M}\odot\texttt{DFT}\left(\tilde{\bm{x}}_{t}^{\mathcal{W}}\right)\) and \(\eta>0\). By choosing a threshold \(\eta_{0}\), non-watermarked images will have \(\eta>\eta_{0}\) and watermarked images will have \(\eta<\eta_{0}\). Theoretically, the derivation of the p-value \(\eta\) could be found in Zhang et al. (2024).

### Extension to Text-to-Image (T2I) Diffusion Models

Up to this point, our discussion has focused exclusively on unconditional diffusion models. Next, we demonstrate how our approach can be readily extended to text-to-image (T2I) diffusion models, which are predominantly used in practice.

Figure 3 provides an overview of our method for T2I diffusion models, which can be flexibly applied to both server and user scenarios. Specifically,

Figure 3: **Overview of Shallow Diffuse for T2I diffusion models.**

* **Watermark injection.** Shallow Diffuse embeds watermarks into the noise corrupted image \(\bm{x}_{t}\) at a specific timestep \(t=0.3T\). In the **server scenario**, given \(\bm{x}_{T}\sim\mathcal{N}(\bm{0},\bm{I}_{d})\) and prompt \(\bm{c}\), we calculate \(\bm{x}_{t}=\texttt{CFG}\left(\bm{x}_{T},t,\bm{c}\right)\). In the **user scenario**, given the generated image \(\bm{x}_{0}\), we compute \(\bm{x}_{t}=\texttt{DDIM-Inv}\left(\bm{x}_{0},t,\bm{\varnothing}\right)\), using an empty prompt \(\bm{\varnothing}\). Next, similar to Section 3.1, we apply DDIM to obtain the watermarked image \(\bm{x}_{0}^{\mathcal{W}}=\texttt{DDIM}\left(\bm{x}_{t}^{\mathcal{W}},0,\bm{ \varnothing}\right)\) and channel averaging \(\bm{x}_{0}^{*\mathcal{W}}\leftarrow\texttt{ChannelAverage}\left(\bm{x}_{0}^{ \mathcal{W}},\texttt{DDIM}\left(\bm{x}_{t},0\right)\right)\). The detailed discussion about channel averaging is in Appendix B.
* **Watermark detection.** During watermark detection, suppose we are given a watermarked image \(\bar{\bm{x}}_{0}^{\mathcal{W}}\) with certain corruptions, we apply the DDIM Inversion to recover the watermarked image at timestep \(t\), denoted as \(\bar{\bm{x}}_{t}^{\mathcal{W}}=\texttt{DDIM-Inv}\left(\bar{\bm{x}}_{0}^{ \mathcal{W}},t,\bm{\varnothing}\right)\). We detect the watermark \(\Delta\bm{x}\) in \(\bar{\bm{x}}_{t}^{\mathcal{W}}\) by calculating \(\eta\) in Equation (8), with detail explained in Section 3.2.

## 4 Experiments

In this section, we present a comprehensive set of experiments to demonstrate the robustness and consistency of _Shallow-Diffuse_ across various datasets. Detailed experiment settings could be found in Appendix C.1. We begin by highlighting its performance in terms of robustness and consistency in both the server scenario (Section 4.1) and the user scenario (Section 4.2). Additionally, we compare Shallow Diffuse with other related works in the trade-off between robustness and consistency, as detailed in Appendix C.3. Moreover, we investigate the effect of timestep \(t\) on both robustness and consistency, with results presented in Section 4.3. Lastly, we provide an ablation study on watermark pattern design, and channel averaging in Appendix C.

### Consistency and robustness under the server scenario

Table 1 compares the performance of Shallow Diffuse with other methods in the user scenario. For reference, we also apply stable diffusion to generate images from the same random seeds, without adding watermarks (referred to as "Stable Diffusion w/o WM" in Table 1). In terms of generation quality, Shallow Diffuse achieves the best FID score among the diffusion-based methods. Additionally, the FID and CLIP scores of Shallow Diffuse are very close to those of Stable Diffusion w/o WM. This similarity arises because the watermarked distribution produced by Shallow Diffuse remains highly consistent with the original generation distribution. Regarding robustness, Shallow Diffuse outperforms all other methods. Although both Gaussian Shading and RingID exhibit comparable generation quality and robustness in the server scenario, they are less suitable for the user scenario. Specifically, Gaussian Shading embeds the watermark into \(\bm{x}_{T}\), which is not accessible to the user, while RingID suffers from poor consistency, as demonstrated in Figure 1 and Table 2.

### Consistency and robustness under the user scenario

Table 2 presents a comparison of Shallow Diffuse's performance against other methods in the user scenario. In terms of consistency, Shallow Diffuse outperforms all other diffusion-based approaches. To measure the upper bound of diffusion-based methods, we apply stable diffusion with \(\bar{\bm{x}}_{0}=\texttt{DDIM}(\texttt{DDIM-Inv}(\bm{x}_{0},t,\varnothing),0,\varnothing)\), and measure the data consistency between \(\bar{x}_{0}\) and \(\bm{x}_{0}\) (denotes in Stable Diffusion w/o WM in Table 2). The upper bound is constrained by errors introduced through DDIM inversion, and Shallow Diffuse comes the closest to reaching this limit. For

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Method & CLIP-Score \(\uparrow\) & FID \(\downarrow\) & Clean & JPEG & G-Blur & G-Noise & Color Inter & Average \\ \hline \hline DarkNet & **0.3298** & 23.73 & 0.970.35 & 0.640.03 & 0.780.00 & 0.440.02 & 0.330.09 & 0.630.03 \\ DarkNet & 0.3291 & 26.00 & **1.001.00** & 0.800.00 & 0.990.80 & 0.970.84 & 0.500.09 & 0.320.45 \\ RandomAN & 0.3252 & **24.60** & 1.000.99 & **0.980.76** & **0.970.72** & **1.000.99** & **0.960.77** & **0.980.81** \\ \hline \multicolumn{8}{l}{**Diffusion Method**} \\ \hline Stable Diffusion w/o WM & 0.3256 & 25.56 & & & & & & \\ Stable Signature & 0.3622 & 30.86 & **1.001.00** & 0.990.76 & 0.570.00 & 0.710.14 & 0.960.87 & 0.810.46 \\ Tree-Bing Watermarks & 0.310 & 52.82 & **1.001.00** & 0.990.73 & 0.800.93 & 0.800.94 & 0.960.67 & 0.970.80 \\ RingID & 0.3285 & 27.13 & **1.001.00** & **1.001.00** & **1.001.00** & 1.000.99 & 0.990.98 & 1.000.99 \\ Gaussian Shading & **0.3631** & 26.17 & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** & **1.001.00** \\ \hline \multicolumn{8}{l}{**Shallow Diffuse (ours)**} \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison under the server scenario.**non-diffusion-based methods, which are not affected by DDIM inversion errors, better image consistency is achievable. As for the robustness, Shallow Diffuse outperforms all other methods in all three datasets. While RivaGAN achieves the best image consistency and comparable watermarking robustness to Shallow Diffuse in the user scenario, Shallow Diffuse is much more efficient. Unlike RivaGAN, which requires training for each individual image, Shallow Diffuse only involves the computational overhead of DDIM and DDIM inversion.

### Relation between injecting timestep, consistency and robustness

Figure 4 shows the relationship between the watermark injection timestep \(t\) and both consistency and robustness 1. Shallow Diffuse achieves optimal consistency at \(t=0.2T\) and optimal robustness at \(t=0.3T\). In practice, we select \(t=0.3T\). This result aligns with the intuitive idea proposed in Section 3.1 and the theoretical analysis in Appendix D: low-dimensionality enhances both data generation consistency and watermark detection robustness. However, according to Chen et al. (2024), the optimal timestep \(r_{t}\) for minimizing \(r_{t}\) satisfies \(t^{*}\in[0.5T,0.7T]\). We believe the best consistency and robustness are not achieved at \(t^{*}\) due to the error introduced by DDIM-Inv. As \(t\) increases, this error grows, leading to a decline in both consistency and robustness. Therefore, the best tradeoff is reached at \(t\in[0.2T,0.3T]\), where \(\bm{J_{\theta,t}}(\bm{x}_{t})\) remains low-rank but \(t\) is still below \(t^{*}\).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{PSNR \(\uparrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multirow{2}{*}{LPPS \(\downarrow\)} & \multicolumn{6}{c}{Wierarchizing Robustness (AUC \(\uparrow\)/IPRP \(\uparrow\)/PRPR)} \\ \cline{4-9}  & & & & & & & & & \\ \hline Clean & JPEG & G.Blur & G.Noise & Color filter & Average \\
**COCO** & & & & & & & & \\ \hline DuCluster & 37.88 & 0.97 & **0.62** & 0.900.83 & 0.480.02 & 0.500.00 & 0.300.00 & 0.350.16 & 0.460.06 \\ DuCluster & 38.06 & **0.96** & **0.62** & **1.000.00** & 0.700.26 & 0.900.83 & 0.910.35 & 0.541.4 & 0.790.43 \\ Re-GAN & **0.85** & **0.98** & **0.04** & **1.000.10** & **1.000.100** & **0.990.86** & **1.000.39** & **9.700.83** & **9.990.22** \\ Style Diffusion \& w/ WM & 32.28 & 0.78 & 0.06 & & & & & \\ Tree-Image Watermarks & 28.22 & 0.51 & 0.41 & **1.000.100** & 0.990.87 & 0.900.86 & 1.000.03 & 0.850.49 & 0.970.81 \\ RivaGAN & **28.22** & 0.38 & 0.61 & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.00** & 9.990.96 \\
**Shadow Diffuse (ours)** & **32.11** & **0.77** & **0.66** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.100** \\
**DiffusionDB** & & & & & & & & & \\ \hline DuCluster & 37.77 & 0.96 & **0.2** & 0.900.76 & 0.710.23 & 0.900.70 & 0.350.01 & 0.520.12 & 0.640.27 \\ DuCluster & 37.84 & 0.97 & **0.82** & **1.000.100** & 0.710.23 & 0.500.00 & 0.320.59 & 0.500.00 & 0.720.23 \\ Re-GAN & **0.85** & **0.98** & 0.94 & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** & **1.000.00** \\ \hline Single Diffusion with WM & 33.42 & 0.85 & 0.10 & & & & & & \\ Tree-Image Watermarks & 25.03 & 0.62 & 0.29 & **1.000.100** & 0.990.68 & 0.940.762 & **1.000.00** & 0.850.13 & 0.940.61 \\ RivaID & 27.9 & 0.21 & 0.77 & **1.000.100** & **1.000.100** & **1.000.100** & **1.000.00** & **1.000.00** & 9.990.86 \\
**Shadow Diffuse (ours)** & **33.07** & **0.84** & **0.84** & **1.000.100** & 1.000.000 & 1.000.99 & 1.000.100** & **1.000.100** & **1.000.00** \\ \hline
**Wuixart** & & & & & & & & & \\ \hline DuCluster & 38.84 & 0.97 & 0.02 & 0.900.73 & 0.440.000 & 0.510.00 & 0.260.00 & 0.496.12 & 0.240.03 \\ DuCluster & 39.44 & 0.98 & 0.02 & **1.000.00** & 0.691.3 & 0.970.00 & 0.370.00 & 0.500.13 & 0.700.04 \\ Re-GAN & **0.44** & **0.98** & **0.05** & **1.000.000** & 0.970.001 & 1.000.52 & **1.000.00** & 0.900.00 & 0.70.05 \\ Simple Diffusion with WM & 31.6 & 0.7 & 0.96 & **1.000.000** & 1.000.97 & 1.000.38 & **1.000.000** & 0.710.000 & 0.920.78 \\ Tree-Image Watermarks & 25.20 & 0.39 & 0.34 & **1.000.000** & 1.000.97 & 1.000.38 & 1.000.000 & 0.920.78 \\
**Shadow Diffuse (ours)** & **34.05** & **0.10** & **1.000.000** & 1.000.99 & 1.000.59 & 1.000.100 & **1.000.00** & **1.000.99** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Comparison under the user scenario.**

Figure 4: **Ablation study of the watermark at different timestep \(t\).**

Conclusion

We proposed Shallow Diffuse, a novel and flexible watermarking technique that operates seamlessly in both server-side and user-side scenarios. By decoupling the watermark from the sampling process, Shallow Diffuse achieves enhanced robustness and greater consistency. Our theoretical analysis demonstrates both the consistency and detectability of the watermarks. Extensive experiments further validate the superiority of Shallow Diffuse over existing approaches.

## References

* Ahmadi et al. (2020) Mahdi Ahmadi, Alireza Norouzi, Nader Karimi, Shadrokh Samavi, and Ali Emami. Redmark: Framework for residual diffusion watermarking based on deep networks. _Expert Systems with Applications_, 146:113157, 2020.
* Al-Haj (2007) Ali Al-Haj. Combined dwt-dct digital image watermarking. _Journal of computer science_, 3(9):740-746, 2007.
* Alemohammad et al. (2024) Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go MAD. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=ShjMHmPso.
* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.
* Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators.
* Brundage et al. (2018) Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artificial intelligence: Forecasting, prevention, and mitigation. _arXiv preprint arXiv:1802.07228_, 2018.
* Chang et al. (2005) Chin-Chen Chang, Piyu Tsai, and Chia-Chen Lin. Svd-based digital image watermarking scheme. _Pattern Recognition Letters_, 26(10):1577-1586, 2005.
* Chen et al. (2024) Siyi Chen, Zhang Huijie, Minzhe Guo, Yifu Lu, Peng Wang, and Qing Qu. Exploring low-dimensional subspaces in diffusion models for controllable image editing. In _Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS2024)_, 2024.
* Ci et al. (2024) Hai Ci, Pei Yang, Yuren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. _arXiv preprint arXiv:2404.14055_, 2024.
* Cox et al. (2007) Ingemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. _Digital watermarking and steganography_. Morgan Kaufmann, 2007.
* Dohmatob et al. (2024) Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. A tale of tails: Model collapse as a change of scaling laws. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=KVVvku47shW.
* Esser et al. (2024) Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Enetzari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In _Forty-first International Conference on Machine Learning_, 2024.
* Evans et al. (2024) Zach Evans, Julian D Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Long-form music generation with latent diffusion. _arXiv preprint arXiv:2404.10301_, 2024.
* Fernandez et al. (2023) Pierre Fernandez, Guillaume Couairon, Herve Jegou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 22466-22477, 2023.
* Fu et al. (2024) Shi Fu, Sen Zhang, Yingjie Wang, Xinmei Tian, and Dacheng Tao. Towards theoretical understandings of self-consuming generative models. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=aw6L8sB2Ts.
* Gibney (2024) Elizabeth Gibney. Ai models fed ai-generated data quickly spew nonsense. _Nature_, 632(8023):18-19, 2024.
* Ghaha et al. (2020)Josh A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. Generative language models and automated influence operations: Emerging threats and potential mitigations. _arXiv preprint arXiv:2301.04246_, 2023.
* Gu et al. (2023) Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On memorization in diffusion models. _arXiv preprint arXiv:2310.02664_, 2023.
* Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* Ho and Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Jahne (2005) Bernd Jahne. _Digital image processing_. Springer Science & Business Media, 2005.
* Kamkari et al. (2024) Hamidreza Kamkari, Brendan Leigh Ross, Rasa Hosseinzadeh, Jesse C Cresswell, and Gabriel Loaiza-Ganem. A geometric view of data complexity: Efficient local intrinsic dimension estimation with diffusion models. In _Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS2024)_, 2024.
* Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* Lee et al. (2020) Jae-Eun Lee, Young-Ho Seo, and Dong-Wook Kim. Convolutional neural network-based digital image watermarking adaptive to the resolution of image and watermark. _Applied Sciences_, 10(19):6854, 2020.
* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.
* Liu et al. (2019) Junxiu Liu, Jiadong Huang, Yuling Luo, Lucchen Cao, Su Yang, Duqu Wei, and Ronglong Zhou. An optimized image watermarking method based on hd and svd in dwt domain. _IEEE Access_, 7:80849-80860, 2019.
* Loaiza-Ganem et al. (2024) Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony L. Caterini, and Jesse C. Cresswell. Deep generative models through the lens of the manifold hypothesis: A survey and new connections. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a90WpmSi0I. Survey Certification, Expert Certification.
* Lu et al. (2022) Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast node solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* Luo (2022) Calvin Luo. Understanding diffusion models: A unified perspective. _arXiv preprint arXiv:2208.11970_, 2022.
* Manor and Michaeli (2024) Hila Manor and Tomer Michaeli. On the posterior distribution in denoising: Application to uncertainty quantification. In _The Twelfth International Conference on Learning Representations_, 2024a. URL https://openreview.net/forum?id=adSGeugiuj.
* Manor and Michaeli (2020) Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pp. 34603-34629. PMLR, 21-27 Jul 2024b. URL https://proceedings.mlr.press/v235/manor24a.html.

* Navas et al. (2008) KA Navas, Mathews Cheriyan Ajay, M Lekshmi, Tampy S Archana, and M Sasikumar. Dwt-dct-svd based watermarking. In _2008 3rd international conference on communication systems software and middleware and workshops (COMSWARE'08)_, pp. 271-274. IEEE, 2008.
* Popescu et al. (2006) Sandu Popescu, Anthony J Short, and Andreas Winter. Entanglement and the foundations of statistical mechanics. _Nature Physics_, 2(11):754-758, 2006.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.
* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.
* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* Shumailov et al. (2024) Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. _Nature_, 631(8022):755-759, 2024.
* Solachidis and Pitas (2001) Vassilios Solachidis and Loannis Pitas. Circularly symmetric watermark embedding in 2-d dft domain. _IEEE transactions on image processing_, 10(11):1741-1753, 2001.
* Sompalli et al. (2023a) Gowthami Sompalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6048-6058, 2023a.
* Sompalli et al. (2023b) Gowthami Sompalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023b. URL https://openreview.net/forum?id=HtMKGbUMt.
* Song et al. (2021a) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021a. URL https://openreview.net/forum?id=St1girACHLP.
* Song et al. (2021b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021b. URL https://openreview.net/forum?id=PxTIG12RRHS.
* Stanczuk et al. (2024) Jan Pawel Stanczuk, Georgios Batzolis, Teo Deveney, and Carola-Bibiane Schonlieb. Diffusion models encode the intrinsic dimension of data manifolds. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=a0XiA6v256.
* Tan et al. (2019) Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi Tanaka. Improved artgan for conditional synthesis of natural image and artwork. _IEEE Transactions on Image Processing_, 28(1):394-409, 2019. doi: 10.1109/TIP.2018.2866698. URL https://doi.org/10.1109/TIP.2018.2866698.
* Wang et al. (2024) Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, and Qing Qu. Diffusion models learn low-dimensional distributions via subspace clustering. _arXiv preprint arXiv:2409.02426_, 2024.
* Wang et al. (2020)* Wang et al. (2004) Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* Wang et al. (2022) Zijie J. Wang, Evan Montoya, David Muncchika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Large-scale prompt gallery dataset for text-to-image generative models. _arXiv:2210.14896 [cs]_, 2022. URL https://arxiv.org/abs/2210.14896.
* Wen et al. (2023a) Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-rings watermarks: Invisible fingerprints for diffusion images. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023a. URL https://openreview.net/forum?id=257JrmubN1.
* Wen et al. (2023b) Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and mitigating memorization in diffusion models. In _The Twelfth International Conference on Learning Representations_, 2023b.
* Yang et al. (2024) Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, and Nenghai Yu. Gaussian shading: Provable performance-lossless image watermarking for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12162-12171, 2024.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. _Advances in neural information processing systems_, 32, 2019.
* Zhang et al. (2024a) Benjamin J Zhang, Siting Liu, Wuchen Li, Markos A Katsoulakis, and Stanley J Osher. Wasserstein proximal operators describe score-based generative models and resolve memorization. _arXiv preprint arXiv:2402.06162_, 2024a.
* Zhang et al. (2024b) Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In _Forty-first International Conference on Machine Learning_, 2024b. URL https://openreview.net/forum?id=Hsli0Qzkc0.
* Zhang et al. (2019) Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robust invisible video watermarking with attention. _arXiv preprint arXiv:1909.01285_, 2019.
* Zhang et al. (2024c) Lijun Zhang, Xiao Liu, Antoni Viros Martin, Cindy Xiong Bearfield, Yuriy Brun, and Hui Guan. Robust image watermarking using stable diffusion, 2024c. URL https://arxiv.org/abs/2401.04247.
* Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 586-595, 2018.
* Zhu et al. (2018) Jiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. Hidden: Hiding data with deep networks. In _European Conference on Computer Vision_, 2018.

## Appendix A Related Work

### Image watermarking

Image watermarking has long been a crucial method for protecting intellectual property in computer vision (Cox et al., 2007; Solachidis & Pitas, 2001; Chang et al., 2005; Liu et al., 2019). Traditional techniques primarily focus on user-side watermarking, where watermarks are embedded into images post-generation. These methods (Al-Haj, 2007; Navas et al., 2008) typically operate in the frequency domain to ensure the watermarks are imperceptible. However, such watermarks remain vulnerable to adversarial attacks and can become undetectable after applying simple image manipulations like blurring.

Early deep learning-based approaches to watermarking (Zhang et al., 2024c; Fernandez et al., 2023; Ahmadi et al., 2020; Lee et al., 2020; Zhu et al., 2018) leveraged neural networks to embed watermarks. While these methods improved robustness and imperceptibility, they often suffer from high computational costs during fine-tuning and lack flexibility. Each new watermark requires additional fine-tuning or retraining, limiting their practicality.

More recently, diffusion model-based watermarking techniques have gained attraction due to their ability to seamlessly integrate watermarks during the generative process without incurring extra computational costs. Techniques such as Wen et al. (2023); Yang et al. (2024); Ci et al. (2024) embed watermarks directly into the initial noise and retrieve the watermark by reversing the diffusion process. These methods enhance robustness and invisibility but are typically restricted to server-side watermarking, requiring access to the initial random seed. Moreover, the watermarks introduced by Wen et al. (2023); Ci et al. (2024) significantly alter the data distribution, leading to variance towards watermarks in generated outputs (as shown in Figure 1).

In contrast to Wen et al. (2023); Ci et al. (2024), our proposed shallow diffuse disentangles the watermark embedding from the generation process by leveraging the high-dimensional null space. This approach, both empirically and theoretically validated, significantly improves watermark consistency and robustness. To the best of our knowledge, this is the first method that supports watermark embedding for both server-side and user-side applications while maintaining high robustness and consistency.

### Low-dimensional subspace in diffusion model

In recent years, there has been growing interest in understanding deep generative models through the lens of the manifold hypothesis (Loaiza-Ganem et al., 2024). This hypothesis suggests that high-dimensional real-world data actually lies in latent manifolds with a low intrinsic dimension. Focusing on diffusion models, Stanczuk et al. (2024) empirically and theoretically shows that the approximated score function (the gradient of the log density of a noise-corrupted data distribution) in diffusion models is orthogonal to a low-dimensional subspace. Building on this, Wang et al. (2024); Chen et al. (2024) find that the estimated posterior mean from diffusion models lies within this low-dimensional space. Additionally, Chen et al. (2024) discovers strong local linearity within the space, suggesting that it can be locally approximated by a linear subspace. This observation motivates our Assumption 1, where we assume the estimated posterior mean lies in a low-dimensional subspace.

Building upon these findings, Stanczuk et al. (2024); Kamkari et al. (2024) introduce a local intrinsic dimension estimator, while Loaiza-Ganem et al. (2024) proposes a method for detecting out-of-domain data. Wang et al. (2024) offers theoretical insights into how diffusion model training transitions from memorization to generalization, and Chen et al. (2024); Manor and Michaeli (2024) explores the semantic basis of the subspace to achieve disentangled image editing. Unlike these previous works, our approach leverages the low-dimensional subspace for watermarking, where both empirical and theoretical evidence demonstrates that this subspace enhances robustness and consistency.

## Appendix B Channel averaging

### Technique details

Natural images have multiple channels denoted by \(C\). Instead of applying watermark \(\lambda\Delta\) to all channels of \(\bm{x}_{t}\), we can apply the watermark to a specific channel \(c\) to make it even more invisible and robust. For this consideration, let us reshape the image \(\bm{x}_{t}\) and the watermark \(\Delta\bm{x}\) into the form \(\bm{x}_{t}\in\mathbb{R}^{H\times W\times C},\lambda\Delta\bm{x}\in\mathbb{R}^{ H\times W\times C}\), where \(H\), \(W\), and \(C\) represent the height, width, and channel dimensions for the image, respectively. These dimensions satisfy \(HWC=d\).

Denote \([\bm{x}_{t}]_{i}\in\mathbb{R}^{H\times W}\) as the \(i\)th channel of \(\bm{x}_{t}\), with \(i\in[C]\). Thus \([\bm{x}_{t}^{W}]_{c}=[\bm{x}_{t}]_{c}+[\lambda\Delta\bm{x}]_{c}\) and \([\bm{x}_{t}^{W}]_{i}=[\bm{x}_{t}]_{i}\) for \(i\neq c\). For the watermark in Equation (3), the channel averaging is defined

Figure 5: Illustration of channel average

as:

\[[\bm{x}_{0}^{\mathcal{W}}]_{i} =\texttt{ChannelAverage}\left(\bm{x}_{0}^{\mathcal{W}},\bm{x}_{0}^{*} \right),\] (9) \[=\left\{\begin{array}{c}[\bm{x}_{0}^{\mathcal{W}}]_{i},i=c\\ (1-\gamma)[\bm{x}_{0}^{\mathcal{W}}]_{i}+\gamma[\bm{x}_{0}^{*}]_{i},i\neq c \end{array},\right.\] (10)

where we applied \(\gamma=1\). In our experiments, we found that we can increase both imperceptibility and robustness by further employing this simple approach. See our ablation study in Appendix C.5 for a more detailed analysis.

## Appendix C Additional Experiments

### Details about experiment settings

BaselineFor the server scenario, we select the following methods as baselines: DWtDct Cox et al. (2007), DwtDctSvd Cox et al. (2007), RivaGAN Zhang et al. (2019), Stable Signature Fernandez et al. (2023), Tree-Ring Watermarks Wen et al. (2023), RingId Ci et al. (2024), and Gaussian Shading Yang et al. (2024). In the user scenario, we adopt the same baseline methods, except for Stable Signature and Gaussian Shading, as these methods are not suitable for this setting.

DatasetsWe use Stable Diffusion 2.1 (Rombach et al., 2022) as the underlying model for our experiments, applying Shallow diffusion within its latent space. For the server scenario (Section 4.1), all diffusion-based methods are based on the same Stable Diffusion, with the original images \(\bm{x}_{0}\) generated from identical initial seeds \(\bm{x}_{T}\). Non-diffusion methods are applied to these same original images \(\bm{x}_{0}\) in a post-watermarking process. A total of 5000 original images are generated for evaluation in this scenario. For the user scenario (Section 4.2), we utilize the MS-COCO Lin et al. (2014), WikiArt Tan et al. (2019), and DiffusionDB datasets Wang et al. (2022). The first two are real-world datasets, while DiffusionDB is a collection of diffusion model-generated images. From each dataset, we select 500 images for evaluation. For the remaining experiments in Appendix C.3, Section 4.3, Appendix C, we use the server scenario and sample 100 images for evaluation.

MetricTo evaluate image consistency under the user scenario, we use peak signal-to-noise ratio (PSNR) Jahne (2005), structural similarity index measure (SSIM) Wang et al. (2004), and Learned Perceptual Image Patch Similarity (LPIPS) Zhang et al. (2018), comparing watermarked images to their original counterparts. In the server scenario, we assess the generation quality of the watermarked images using Contrastive Language-Image Pretraining Score (CLIP-Score) Radford et al. (2021) and Frechet Inception Distance (FID) Heusel et al. (2017). To evaluate robustness, we vary the threshold \(\eta_{0}\) and plot the true positive rate (TPR) against the false positive rate (FPR) for the receiver operating characteristic (ROC) curve. We use the area under the curve (AUC) and TPR when FPR = 0.01 (TPR @1% FPR) as robustness metrics. Robustness is evaluated both under clean conditions (no attacks) and with various attacks, including JPEG compression, Gaussian blurring (G.Blur), Gaussian noise, and color jitter. Details of these attacks are provided in Appendix C.2.

### Details about attacks

In this work, we intensively tested our method on four different watermarking attacks, both in the server scenario and in the user scenario. These watermarking attacks represent the most common image distortion methods in real life, including

* JPEG compression with a compression rate of 25%
* Gaussian blurring (G.Blur) with an \(8\times 8\) filter size
* Gaussian noise (G.Noise) with \(\sigma=0.1\)
* Color jitter with brightness factor uniformly ranges between 0 and 6

### Trade-off between consistency and robustness

Figure 6 illustrates the trade-off between consistency and robustness for Shallow Diffuse and other baselines. As the radius of \(\bm{M}\) increases, the watermark intensity \(\lambda\) also increases, reducing image

[MISSING_PAGE_FAIL:16]

**Theorem 1** (Consistency of the watermarks).: _Suppose Assumption 1 holds and \(\Delta\bm{x}\sim\mathrm{U}(\mathbb{S}^{d-1})\). Let us define \(\hat{\bm{x}}_{0,t}^{\mathcal{W}}\coloneqq\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+ \lambda\Delta\bm{x})\), \(\hat{\bm{x}}_{0,t}\coloneqq\bm{f}_{\bm{\theta},t}(\bm{x}_{t}+\lambda\Delta\bm{ x})\). The \(\ell_{2}\)-norm distance between \(\hat{\bm{x}}_{0,t}^{\mathcal{W}}\) and \(\hat{\bm{x}}_{0,t}\) can be bounded by:_

\[||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x}}_{0,t}||_{2}\leq\lambda Lh(r_{t }),\] (11)

_with probability at least \(1-r_{t}^{-1}\). Here, \(h(r_{t})=\sqrt{\frac{r_{t}}{d}+\sqrt{\frac{18\pi^{3}}{d-2}\text{log}\left(2r_{ t}\right)}}\)._

Our Theorem 1 guarantees that adding the watermark \(\lambda\Delta\bm{x}\) would only change the estimation by an amount of \(\lambda Lh(r_{t})\) with a constant probability. In particular, when \(r_{t}\) is small, it implies that the change in the prediction would be small. Given the relationship between PMP and DDIM in equation 1, the consistency also applies to the practical use. On the other hand, in the following we show that the injected watermark can be detected based upon the second term in Equation (6).

**Theorem 2** (Detectability of the watermarks).: _Suppose Assumption 1 holds and \(\Delta\bm{x}\sim\mathrm{U}(\mathbb{S}^{d-1})\). With \(\bm{x}_{t}^{\mathcal{W}}\) given in Equation (3), define \(\bm{x}_{t}^{\mathcal{W}}-1=\texttt{DDIM}\left(\bm{x}_{t}^{\mathcal{W}},t-1\right)\) and \(\bar{\bm{x}}_{t}^{\mathcal{W}}=\texttt{DDIM-Inv}\left(\bm{x}_{t-1}^{\mathcal{W }},t\right)\). The \(\ell_{2}\)-norm distance between \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\) and \(\bm{x}_{t}^{\mathcal{W}}\) can be bounded by:_

\[||\tilde{\bm{x}}_{t}^{\mathcal{W}}-\bm{x}_{t}^{\mathcal{W}}||_{2}\leq\lambda L \left(-g\left(\alpha_{t},\alpha_{t-1}\right)+g\left(\alpha_{t-1},\alpha_{t} \right)\left(1-Lg\left(\alpha_{t},\alpha_{t-1}\right)\right)\right)h(\max\{r_{ t-1},r_{t}\})\] (12)

_with probability at least \(1-r_{t}^{-1}-r_{t-1}^{-1}\). Here, \(g(x,y)\coloneqq\frac{\sqrt{1-y}\sqrt{x}-\sqrt{1-x}\sqrt{y}}{\sqrt{1-x}}\), \(\forall x,y\in(0,1)\)._

Here \(-g\left(\alpha_{t},\alpha_{t-1}\right)+g\left(\alpha_{t-1},\alpha_{t}\right) \left(1-Lg\left(\alpha_{t},\alpha_{t-1}\right)\right)\) is a small number under the \(\alpha_{t}\) designed for variance preserving (VP) noise scheduler Ho et al. (2020) and \(h(\max\{r_{t-1},r_{t}\})\) is small when \(r_{t}\) is small. This indicates that the difference between \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\) and \(\bm{x}_{t}^{\mathcal{W}}\) is small when \(r_{t}\) is small and \(\bm{x}_{t}^{\mathcal{W}}\) could be recovered by \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\) from one-step DDIM. Therefore, Theorem 2 implies that the injected watermark can be detected with constant probability.

## Appendix E Proofs in Section D

### Proofs of Theorem 1

Proof of Theorem 1.: According to Assumption 1, we have \(||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x}}_{0,t}||_{2}^{2}=\lambda||\bm{ J}_{\bm{\theta},t}(\bm{x}_{t})\cdot\Delta\bm{x}||_{2}^{2}\). From Levy's Lemma proposed in Popescu et al. (2006), given function \(||\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\cdot\Delta\bm{x}||_{2}^{2}:\mathbb{S}^{d -1}\rightarrow\mathbb{R}\) we have:

\[\mathbb{P}\left(||\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\cdot\Delta\bm{x}||_{2}^{2 }-\mathbb{E}\left[||\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\cdot\Delta\bm{x}||_{2}^ {2}\right]|\geq\epsilon\right)\leq 2\exp\left(\frac{-C(d-2)\epsilon^{2}}{L^{2}} \right),\]

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \multirow{2}{*}{Channel average intensity \(\gamma\)} & \multirow{2}{*}{PSNR \(\uparrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multicolumn{6}{c}{Watermarking Robustness (TPR@1\%FPR\(\uparrow\))} \\ \cline{4-9}  & & & Clean & JPEG & G.Blur & G.Noise & Color Jitter \\
0 & 37.1103 & 0.941 & 0.0154 & 1.0000 & 1.0000 & 0.9971 & 1.0000 & 0.9584 \\
1.0 & 36.6352 & 0.931 & 0.0151 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\ \hline \end{tabular}
\end{table}
Table 4: **Ablation study of channel average.**

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \multirow{2}{*}{Watermark embedding channel} & \multirow{2}{*}{PSNR \(\uparrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multirow{2}{*}{LPIPS \(\downarrow\)} & \multicolumn{6}{c}{Watermarking Robustness (TPR@1\%FPR\(\uparrow\))} \\ \cline{4-9}  & & & Clean & JPEG & G.Blur & G.Noise & Color Jitter \\
0th & 36.46 & 0.93 & 0.02 & 1.00 & 1.00 & 1.00 & 1.00 & 0.99 \\
1th & 36.57 & 0.93 & 0.02 & 1.00 & 1.00 & 1.00 & 1.00 & 0.99 \\
2th & 36.13 & 0.92 & 0.02 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
3th & 36.64 & 0.93 & 0.02 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
1th + 2th + 3th & 33.19 & 0.83 & 0.05 & 1.00 & 1.00 & 1.00 & 1.00 & 0.95 \\ \hline \end{tabular}
\end{table}
Table 5: **Ablation study of watermarking embedded channel.**given \(L\) to be the Lipschitz constant of \(||\bm{J_{\theta,t}}(\bm{x}_{t})||_{2}^{2}\) and \(C\) is a positive constant (which can be taken to be \(C=(18\pi^{3})^{-1}\)). From Lemma 2 and Lemma 3, we have:

\[\mathbb{P}\left(\left||\bm{J_{\theta,t}}(\bm{x}_{t})\cdot\Delta\bm{x}\right||_{ 2}^{2}-\frac{||\bm{J_{\theta,t}}(\bm{x}_{t})||_{F}^{2}}{d}\right|\geq\epsilon \right)\leq 2\exp\left(\frac{-(18\pi^{3})^{-1}(d-2)\epsilon^{2}}{||\bm{J_{ \theta,t}}(\bm{x}_{t})||_{2}^{4}}\right).\]

Define \(\frac{1}{r_{t}}\) as the desired probability level, set

\[\frac{1}{r_{t}}=2\exp\left(\frac{-(18\pi^{3})^{-1}(d-2)\epsilon^{2}}{||\bm{J_{ \theta,t}}(\bm{x}_{t})||_{2}^{4}}\right),\]

Solving for \(\epsilon\):

\[\epsilon=||\bm{J_{\theta,t}}(\bm{x}_{t})||_{2}^{2}\sqrt{\frac{18\pi^{3}}{d-2} \log{(2r_{t})}}.\]

Therefore, with probability \(1-\frac{1}{r_{t}}\), we have:

\[||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x}}_{0,t}||_{2}^{2} =\lambda^{2}||\bm{J_{\theta,t}}(\bm{x}_{t})\cdot\Delta\bm{x}||_{2 }^{2},\] \[\leq\frac{\lambda^{2}||\bm{J_{\theta,t}}(\bm{x}_{t})||_{F}^{2}}{d }+\lambda^{2}||\bm{J_{\theta,t}}(\bm{x}_{t})||_{2}^{2}\sqrt{\frac{18\pi^{3}}{d -2}\log{(2r_{t})}},\] \[\leq\lambda^{2}||\bm{J_{\theta,t}}(\bm{x}_{t})||_{2}^{2}\left( \frac{r_{t}}{d}+\sqrt{\frac{18\pi^{3}}{d-2}\log{(2r_{t})}}\right),\] \[=\lambda^{2}L^{2}\left(\frac{r_{t}}{d}+\sqrt{\frac{18\pi^{3}}{d-2 }\log{(2r_{t})}}\right),\]

where the last inequality is obtained from \(||\bm{J_{\theta,t}}(\bm{x}_{t})||_{F}^{2}\leq r_{t}||\bm{J_{\theta,t}}(\bm{x}_ {t})||_{2}^{2}\). Therefore, with probability \(1-\frac{1}{r_{t}}\),

\[||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x}}_{0,t}||_{2}\leq\lambda L\sqrt{ \frac{r_{t}}{d}+\sqrt{\frac{18\pi^{3}}{d-2}\log{(2r_{t})}}}=\lambda Lh(r_{t}).\]

Proof of Theorem 2.: According to Equation (1), one step of DDIM sampling at timestep \(t\) could be represented by PMP \(\bm{f_{\theta,t}}(\bm{x}_{t})\) as:

\[\bm{x}_{t-1} =\sqrt{\alpha_{t-1}}\bm{f_{\theta,t}}(\bm{x}_{t})+\sqrt{1-\alpha_ {t-1}}\left(\frac{\bm{x}_{t}-\sqrt{\alpha_{t}}\bm{f_{\theta,t}}(\bm{x}_{t})}{ \sqrt{1-\alpha_{t}}}\right),\] (13) \[=\sqrt{\frac{1-\alpha_{t-1}}{1-\alpha_{t}}}\bm{x}_{t}+\frac{\sqrt {1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1-\alpha_{t-1}}\sqrt{\alpha_{t}}}{ \sqrt{1-\alpha_{t}}}\bm{f_{\theta,t}}(\bm{x}_{t}),\] (14)

If we inject a watermark \(\lambda\Delta\bm{x}\) to \(\bm{x}_{t}\), so \(x_{t}^{\mathcal{W}}=\bm{x}_{t}+\lambda\Delta\bm{x}\). To solve \(x_{t-1}^{\mathcal{W}}\), we could plugging Equation (2) to Equation (14), we could obtain:

\[\bm{x}_{t-1}^{\mathcal{W}} =\sqrt{\frac{1-\alpha_{t-1}}{1-\alpha_{t}}}\bm{x}_{t}^{\mathcal{W} }+\frac{\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1-\alpha_{t-1}}\sqrt{ \alpha_{t}}}{\sqrt{1-\alpha_{t}}}\bm{f_{\theta,t}}(\bm{x}_{t}^{\mathcal{W}}),\] (15) \[=\bm{x}_{t-1}+\sqrt{\frac{1-\alpha_{t-1}}{1-\alpha_{t}}}\lambda \Delta\bm{x}+\frac{\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1-\alpha_{t-1 }}\sqrt{\alpha_{t}}}{\sqrt{1-\alpha_{t}}}\bm{J_{\theta,t}}(\bm{x}_{t})\Delta \bm{x}\] (16) \[=\bm{x}_{t-1}+\lambda\underbrace{\left(\sqrt{\frac{1-\alpha_{t-1} }{1-\alpha_{t}}}\bm{I}+\frac{\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1- \alpha_{t-1}}\sqrt{\alpha_{t}}}{\sqrt{1-\alpha_{t}}}\bm{J_{\theta,t}}(\bm{x}_ {t})\right)}_{:=\bm{W}_{t}}\Delta\bm{x},\] (17)One step DDIM Inverse sampling at timestep \(t-1\) could be represented by PMP \(\bm{f}_{\bm{\theta},t}(\bm{x}_{t})\) as:

\[\bm{x}_{t}=\sqrt{\frac{1-\alpha_{t}}{1-\alpha_{t-1}}}\bm{x}_{t-1}+\frac{\sqrt{1- \alpha_{t-1}}\sqrt{\alpha_{t}}-\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}}{\sqrt{1- \alpha_{t-1}}}\bm{f}_{\bm{\theta},t-1}(\bm{x}_{t-1}),\] (18)

To detect the watermark, we apply one step DDIM Inverse on \(\bm{x}_{t-1}^{\mathcal{W}}\) at timestep \(t-1\) to obtain \(\tilde{\bm{x}}_{t}^{\mathcal{W}}\):

\[\tilde{\bm{x}}_{t}^{\mathcal{W}} =\sqrt{\frac{1-\alpha_{t}}{1-\alpha_{t-1}}}\bm{x}_{t-1}^{\mathcal{ W}}+\frac{\sqrt{1-\alpha_{t-1}}\sqrt{\alpha_{t}}-\sqrt{1-\alpha_{t}}\sqrt{ \alpha_{t-1}}}{\sqrt{1-\alpha_{t-1}}}\bm{f}_{\bm{\theta},t-1}(\bm{x}_{t-1}^{ \mathcal{W}}),\] \[=\bm{x}_{t}+\lambda\underbrace{\left(\sqrt{\frac{1-\alpha_{t}}{1 -\alpha_{t-1}}}\bm{I}+\frac{\sqrt{1-\alpha_{t-1}}\sqrt{\alpha_{t}}-\sqrt{1- \alpha_{t}}\sqrt{\alpha_{t-1}}}{\sqrt{1-\alpha_{t-1}}}\bm{J}_{\bm{\theta},t-1}( \bm{x}_{t-1})\right)}_{=\bm{W}_{t-1}}\bm{W}_{t}\Delta\bm{x},\] \[=\bm{x}_{t}+\lambda\bm{W}_{t-1}\bm{W}_{t}\Delta\bm{x}=\bm{x}_{t}^ {\mathcal{W}}+\lambda\left(\bm{W}_{t-1}\bm{W}_{t}-\bm{I}\right)\Delta\bm{x}.\]

Therefore:

\[||\tilde{\bm{x}}_{t}^{\mathcal{W}}-\bm{x}_{t}^{\mathcal{W}}||_{2} =\lambda||\left(\bm{W}_{t-1}\bm{W}_{t}-\bm{I}\right)\Delta\bm{x} ||_{2},\] \[=\lambda||\frac{\sqrt{1-\alpha_{t-1}}\sqrt{\alpha_{t}}-\sqrt{1- \alpha_{t}}\sqrt{\alpha_{t-1}}}{\sqrt{1-\alpha_{t}}}\bm{J}_{\bm{\theta},t-1}( \bm{x}_{t-1})\Delta\bm{x},\] \[\qquad+\frac{\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1- \alpha_{t-1}}\sqrt{\alpha_{t}}}{\sqrt{1-\alpha_{t-1}}}\bm{J}_{\bm{\theta},t}( \bm{x}_{t})\Delta\bm{x},\] \[\qquad-\frac{\left(\sqrt{1-\alpha_{t}}\sqrt{\alpha_{t-1}}-\sqrt{1- \alpha_{t-1}}\sqrt{\alpha_{t}}\right)^{2}}{\sqrt{1-\alpha_{t-1}}\sqrt{1- \alpha_{t}}}\bm{J}_{\bm{\theta},t}(\bm{x}_{t-1})\Delta\bm{x}||_{2},\] \[\leq-\lambda g\left(\alpha_{t},\alpha_{t-1}\right)||\bm{J}_{\bm{ \theta},t-1}(\bm{x}_{t-1})\Delta\bm{x}||_{2}+\lambda g\left(\alpha_{t-1}, \alpha_{t}\right)||\bm{J}_{\bm{\theta},t}(\bm{x}_{t})\Delta\bm{x}||_{2}\] \[\qquad-\lambda g\left(\alpha_{t-1},\alpha_{t}\right)g\left(\alpha _{t},\alpha_{t-1}\right)||\bm{J}_{\bm{\theta},t-1}(\bm{x}_{t-1})\bm{J}_{\bm{ \theta},t}(\bm{x}_{t})\Delta\bm{x}||_{2},\] \[\leq-\lambda g\left(\alpha_{t},\alpha_{t-1}\right)||\bm{J}_{\bm{ \theta},t-1}(\bm{x}_{t-1})\Delta\bm{x}||_{2}\] \[\qquad+\lambda g\left(\alpha_{t-1},\alpha_{t}\right)\left(1-g \left(\alpha_{t},\alpha_{t-1}\right)L\right)||\bm{J}_{\bm{\theta},t}(\bm{x}_{t })\Delta\bm{x}||_{2},\] \[=-g\left(\alpha_{t},\alpha_{t-1}\right)||\hat{\bm{x}}_{0,t-1}^{ \mathcal{W}}-\hat{\bm{x}}_{0,t-1}||_{2}\] \[\qquad+g\left(\alpha_{t-1},\alpha_{t}\right)\left(1-g\left(\alpha_ {t},\alpha_{t-1}\right)L\right)||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x }}_{0,t}||_{2},\]

The first inequality holds because \(g\left(\alpha_{t-1},\alpha_{t}\right)<0\) and \(g\left(\alpha_{t},\alpha_{t-1}\right)>0\). The second inequality holds because \(||\bm{J}_{\bm{\theta},t-1}(\bm{x}_{t-1})\bm{J}_{\bm{\theta},t}(\bm{x}_{t}) \Delta\bm{x}||_{2}\leq||\bm{J}_{\bm{\theta},t-1}(\bm{x}_{t-1})||_{2}||\bm{J}_{ \bm{\theta},t}(\bm{x}_{t})\Delta\bm{x}||_{2}\leq L||\bm{J}_{\bm{\theta},t}(\bm {x}_{t})\Delta\bm{x}||_{2}\). From Theorem 1, with probability \(1-\frac{1}{r_{t-1}}\),

\[||\hat{\bm{x}}_{0,t-1}^{\mathcal{W}}-\hat{\bm{x}}_{0,t-1}||_{2}\leq\lambda Lh(r _{t-1}),\]

with probability \(1-\frac{1}{r_{t}}\),

\[||\hat{\bm{x}}_{0,t}^{\mathcal{W}}-\hat{\bm{x}}_{0,t}||_{2}\leq\lambda Lh(r_{t}),\]

Thus, from the union of bound, with a probability at least \(1-\frac{1}{r_{t}}-\frac{1}{r_{t-1}}\),

\[||\tilde{\bm{x}}_{t}^{\mathcal{W}}-\bm{x}_{t}^{\mathcal{W}}||_{2} \leq-\lambda Lg\left(\alpha_{t},\alpha_{t-1}\right)h(r_{t-1})+ \lambda Lg\left(\alpha_{t-1},\alpha_{t}\right)\left(1-g\left(\alpha_{t},\alpha_{t -1}\right)L\right)h(r_{t})\] \[\leq\lambda L\left(-g\left(\alpha_{t},\alpha_{t-1}\right)+g\left( \alpha_{t-1},\alpha_{t}\right)\left(1-Lg\left(\alpha_{t},\alpha_{t-1}\right) \right)\right)h(\max\{r_{t-1},r_{t}\})\]Auxiliary Results

Lemma 1: _Given a unit vector \(\bm{v}_{i}\) with and \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I}_{d})\), we have_

\[\mathbb{E}_{\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I}_{d})}[\left(\bm{v}_{i}^{ T}\bm{\epsilon}\right)^{2}/||\bm{\epsilon}||_{2}^{2}]=\frac{1}{d}.\]

Proof of Lemma 1: Because \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I}_{d})\),

\[\bm{v}_{i}^{T}\bm{\epsilon}\sim\mathcal{N}(\bm{v}_{i}^{T}\bm{0},\bm{v}_{i}^{T} \bm{I}_{d}\bm{v}_{i})=\mathcal{N}(\bm{v}_{i}^{T}\bm{0},\bm{v}_{i}^{T}\bm{I}_{ d}\bm{v}_{i})=\mathcal{N}(0,1),\] (19)

Assume a set of \(d\) unit vectors \(\{v_{1},v_{2},\ldots,v_{i},\ldots,v_{d}\}\) are orthogonormal and are basis of \(\mathbb{R}^{d}\), similarly, we could show that \(\forall j\in[d],X_{j}\coloneqq v_{j}^{T}\bm{\epsilon}\sim\mathcal{N}(0,1)\). Therefore, we could rewrite \(\left(\bm{v}_{i}^{T}\bm{\epsilon}\right)^{2}/||\bm{\epsilon}||_{2}^{2}\) as:

\[\left(\bm{v}_{i}^{T}\bm{\epsilon}\right)^{2}/||\bm{\epsilon}||_{2 }^{2} =\frac{\left(\bm{v}_{i}^{T}\bm{\epsilon}\right)^{2}}{||\sum_{k=1}^ {d}v_{k}v_{k}^{T}\bm{\epsilon}||_{2}^{2}},\] (20) \[=\frac{\left(\bm{v}_{i}^{T}\bm{\epsilon}\right)^{2}}{\sum_{k=1}^ {d}\left(v_{k}^{T}\bm{\epsilon}\right)^{2}},\] (21) \[=\frac{X_{i}^{2}}{\sum_{k=1}^{d}X_{k}^{2}}.\] (22)

Let \(Y_{i}\coloneqq\frac{X_{i}^{2}}{\sum_{j=1}^{d}X_{j}^{2}}\). Because \(\forall j\in[d],X_{j}\coloneqq v_{j}^{T}\bm{\epsilon}\sim\mathcal{N}(0,1), \forall j\in[d],Y_{j}\) has the same distribution. Additionally, \(\sum_{j=1}^{d}Y_{j}=1\). So:

\[\mathbb{E}_{\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I}_{d})}[\frac{\left(\bm{ v}_{i}^{T}\bm{\epsilon}\right)^{2}}{||\bm{\epsilon}||_{2}^{2}}]=\mathbb{E}[Y_{i}]= \frac{1}{d}\mathbb{E}[\sum_{j=1}^{d}Y_{j}]=\frac{1}{d}.\]

Lemma 2: _Given a matrix \(\bm{J}\in\mathbb{R}^{d\times d}\) with \(\mathit{rank}\left(\bm{J}\right)=r\). Given \(\bm{x}\) which is uniformly sampled on the unit hypersphere \(\mathbb{S}^{d-1}\), we have:_

\[\mathbb{E}_{\bm{x}}\left[||\bm{J}\bm{x}||_{2}^{2}\right]=\frac{||\bm{J}||_{F} ^{2}}{d}.\]

Proof of Lemma 2.: Let's define the singular value decomposition of \(\bm{J}=\bm{U}\bm{\Sigma}\bm{V}^{T}\) with \(\Sigma=\mathrm{diag}\left(\sigma_{1},\ldots,\sigma_{r},0\ldots,0\right)\). Therefore, \(\mathbb{E}_{\bm{x}}\left[||\bm{J}\bm{x}||_{2}^{2}\right]=\mathbb{E}_{\bm{x}} \left[||\bm{U}\bm{\Sigma}\bm{V}^{T}\bm{x}||_{2}^{2}\right]=\mathbb{E}_{\bm{z} }\left[||\bm{\Sigma}\bm{z}||_{2}^{2}\right]\) where \(\bm{z}\coloneqq\bm{V}^{T}\bm{x}\) is is uniformly sampled on the unit hypersphere \(\mathbb{S}^{d-1}\). Thus, we have:

\[\mathbb{E}_{\bm{z}}\left[||\bm{\Sigma}\bm{z}||_{2}^{2}\right] =\mathbb{E}_{\bm{z}}\left[||\sum_{i=1}^{r}\sigma_{i}\bm{e}_{i}^{T} \bm{z}||_{2}^{2}\right],\] \[=\mathbb{E}_{\bm{z}}\left[\sum_{i=1}^{r}\sigma_{i}^{2}||\bm{e}_{i} ^{T}\bm{z}||_{2}^{2}\right],\] \[=\sum_{i=1}^{r}\sigma_{i}^{2}\mathbb{E}_{\bm{z}}\left[||\bm{e}_{i} ^{T}\bm{z}||_{2}^{2}\right]=\frac{||\bm{J}||_{F}^{2}}{d},\]

where \(\bm{e}_{i}\) is the standard basis with \(i\)-th element equals to 0. The second equality is because of independence between \(\bm{e}_{i}^{T}\bm{z}\) and \(\bm{e}_{j}^{T}\bm{z}\). The fourth equality is from Lemma 1. 

Lemma 3: _Given function \(f\left(\bm{x}\right)=||\bm{J}\bm{x}||_{2}^{2}\), the lipschitz constant \(L_{f}\) of function \(f\left(\bm{x}\right)\) is:_

\[L_{f}=2||\bm{J}||_{2}^{2}.\]Proof of Lemma 3.: The jacobian of \(f(\bm{x})\) is:

\[\nabla_{\bm{x}}f(\bm{x})=2\bm{J}^{T}\bm{J}\bm{x},\]

Therefore, the lipschitz constant \(L\) follows:

\[L_{f}=\sup_{\bm{x}\in\mathbb{S}^{d-1}}||\nabla_{\bm{x}}f(\bm{x})||_{2}=2\sup_{ \bm{x}\in\mathbb{S}^{d-1}}||\bm{J}^{T}\bm{J}\bm{x}||_{2}=||\bm{J}^{T}\bm{J}||_ {2}=||\bm{J}||_{2}^{2}\]