Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback

 Canzhe Zhao

Shanghai Jiao Tong University

canzhezhao@sjtu.edu.cn

&Ruofeng Yang

Shanghai Jiao Tong University

wanshuiyin@sjtu.edu.cn

&Baoxiang Wang

The Chinese University of Hong Kong, Shenzhen

bxiangwang@cuhk.edu.cn

&Xuezhou Zhang

Boston University

xuezhouz@bu.edu

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

Corresponding author.

###### Abstract

In this work, we study the low-rank MDPs with adversarially changed losses in the full-information feedback setting. In particular, the unknown transition probability kernel admits a low-rank matrix decomposition (Uehara et al., 2022), and the loss functions may change adversarially but are revealed to the learner at the end of each episode. We propose a policy optimization-based algorithm POLO, and we prove that it attains the \(\widetilde{O}(K^{\nicefrac{{5}}{{6}}}A^{\nicefrac{{{5}}}{{2}}}d\ln(1+M)/(1- \gamma)^{2})\) regret guarantee, where \(d\) is rank of the transition kernel (and hence the dimension of the unknown representations), \(A\) is the cardinality of the action space, \(M\) is the cardinality of the model class that contains all the plausible representations, and \(\gamma\) is the discounted factor. Notably, our algorithm is oracle-efficient and has a regret guarantee with no dependence on the size of potentially arbitrarily large state space. Furthermore, we also prove an \(\Omega(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK})\) regret lower bound for this problem, showing that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting. To the best of our knowledge, we present the first algorithm that interleaves representation learning, exploration, and exploitation to achieve the sublinear regret guarantee for RL with nonlinear function approximation and adversarial losses.

## 1 Introduction

In reinforcement learning (RL), the goal is to learn a (near) optimal policy through the interactions between the learner and the environment, which is typically modeled as the Markov decision processes (MDPs) (Feinberg, 1996). When the state and action spaces are finite, many works have established the minimax (near) optimal regret guarantees for MDPs with finite horizon (Azar et al., 2017) and MDPs with infinite horizon (Tossou et al., 2019, He et al., 2021). In real applications of RL, however, the state and action spaces may be arbitrarily large and even infinite, which may lead to the curse of dimensionality. To tackle this issue, a common approach is _function approximation_, which approximates the value functions of given policies with the leverage of feature mappings. Assuming that the feature mapping which embeds the state-action pairs to a low dimensional embedding space is known, RL with linear function approximation has been well studied recently. In particular, linear mixture MDPs (Ayoub et al., 2020) and linear MDPs (Jin et al., 2020b) are the two models of RL with linear function approximation that have been extensively studied. Notably, their (near) optimal regret guarantees are established by Zhou et al. (2021) and He et al. (2023) respectively. Nevertheless, in scenarios with complex and large-scale data, attaining the true underlying feature mappings might be unrealistic, and thus representation learning is needed. Many empirical works have shown that representation learning can accelerate the sample and computation efficiency of RL (Silver et al., 2018; Laskin et al., 2020; Yang and Nachum, 2021; Stooke et al., 2021; Schwarzer et al., 2021; Xie et al., 2022). However, representation learning is provably more difficult in RL and other sequential decision-making problems than in non-sequential and non-interactive problems (_e.g._, supervised learning) (Du et al., 2020; Wang et al., 2021; Weisz et al., 2021; Uehara et al., 2022). To pursue sample-efficient RL in the presence of representation learning, recent works have made initial attempts to study the theoretical guarantees of representation learning in RL under the fixed or stochastic loss functions (Uehara et al., 2022; Zhang et al., 2022).

In practice, however, it might be stringent to assume that the loss functions are stochastic. To tackle this issue, Even-Dar et al. (2009) and Yu et al. (2009) propose the first algorithms with provably theoretical guarantees that can handle adversarial MDPs, where the loss functions may change adversarially in each episode. Subsequently, most of the works in this line of research focus on learning tabular MDPs with adversarial loss functions (Neu et al., 2010, 2012; Arora et al., 2012; Zimin and Neu, 2013; Dekel and Hazan, 2013; Dick et al., 2014; Rosenberg and Mansour, 2019, 2019; Jin and Luo, 2020; Jin et al., 2020; Shani et al., 2020; Chen et al., 2021; Ghasemi et al., 2021; Rosenberg and Mansour, 2021; Jin et al., 2021; Dai et al., 2022; Chen et al., 2022). To learn adversarial MDPs with large state and action spaces, some recent works study RL with adversarial loss functions and linear function approximation (Cai et al., 2020; Neu and Olkhovskaya, 2021; Luo et al., 2021, 2021; He et al., 2022; Zhao et al., 2023; Kong et al., 2023). However, all these existing works assume that the state-action feature representations are known. As aforementioned, in complex and high-dimensional environments, the application of these algorithms may be still hindered due to the potential difficulty of knowing the true feature mappings a priori. Therefore, the following question naturally remains open:

_Can we devise an algorithm to simultaneously tackle the representation learning and adversarially changed loss functions in reinforcement learning?_

In this work, we give an affirmative answer to the above question in the setting of adversarial low-rank MDPs with full-information feedback. Specifically, in this problem, the unknown transition probability kernel admits a low-rank matrix decomposition but the true representations regarding the transitions are not known a priori. Meanwhile, the loss functions are arbitrarily chosen by an adversary across episodes and are revealed to the learner after each episode.

To solve this problem, we propose a policy optimization-based algorithm, which we call **P**olicy **O**ptimization for **LO**w-rank MDPs (POLO). Specifically, our POLO algorithm obtains an \(\widetilde{O}(K^{\nicefrac{{5}}{{6}}}A^{\nicefrac{{1}}{{2}}}d\ln(1+M)/(1- \gamma)^{2})\) regret guarantee for adversarial low-rank MDPs in the full-information feedback setting and is oracle-efficient. Algorithmically, POLO follows similar ideas of optimistic policy optimization methods in that it first constructs optimistic value function estimates and then runs online mirror descent (OMD) over the optimistic value estimates to deal with the adversarially changed loss functions (Shani et al., 2020, Cai et al., 2020; He et al., 2022; Chen et al., 2022). However, in the presence of representation learning, the exploration and exploitation needed to learn the adversarial MDPs are more difficult than them in the tabular case (Shani et al., 2020, Chen et al., 2022) and in the linear case (Cai et al., 2020; He et al., 2022), where the exploration and exploitation can be essentially well balanced by directly modifying the algorithms studying stochastic MDPs in the tabular and linear cases. Concretely, to learn stochastic low-rank MDPs, previous works perform maximum likelihood estimation (MLE) over the experienced transitions (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). Though the balance of representation learning, exploration, and exploitation can be simultaneously handled by the algorithms in these works, these algorithms intrinsically have no regret guarantees but only sample complexity guarantees even in the stochastic case, since these algorithms need to take actions uniformly at certain stepsin each episode (_cf._, Lemma 9 of Uehara et al. (2022)).2 Hence, a straightforward adaption of their methods from the stochastic setting to the adversarial setting will fail to learn adversarial low-rank MDPs. To cope with this issue, we carefully devise an algorithm with a _doubled exploration and exploitation_ scheme, which interleaves (a) the exploration over transitions required in representation learning; and (b) the exploration and exploitation suggested by the policy optimization. To this end, our algorithm adopts a mixed roll-out policy, which consists of a uniformly explorative policy and a policy optimized by OMD. Through carefully tuning the hyper-parameter of the mixing coefficient used in our mixed policy, we can avoid pulling actions uniformly at random to conduct exploration in each episode and only conduct uniform exploration at a certain fraction of all the episodes (see Section 3.1 for details). Besides, unlike tabular and linear (mixture) MDPs, it is in general hard to achieve the point-wise optimism for each state-action pair. Therefore, depart from previous methods (Shani et al., 2020; Cai et al., 2020; He et al., 2022) conducting policy optimization in the _true_ model (_i.e._, the transition kernel characterized by the true representation), our algorithm conducts policy optimization in the fixed _learned_ model with the epoch-based model update, which enables a new analysis scheme that only requires a _near optimism_ at the initial state \(s_{0}\) (see Section 3.2 for

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Algorithm & Model & Feedback & Regret & Unknown \\  & & & & Features \\ \hline OPPO & Linear Mixture & Full- & \(\widetilde{O}\left(d\sqrt{K}/(1-\gamma)^{2}\right)\) & ✗ \\
[Cai et al., 2020] & MDPs & information & & \\ \hline POWERS & Linear Mixture & Full- & \(\widetilde{O}\left(d\sqrt{K}/(1-\gamma)^{3/2}\right)\) & ✗ \\
[He et al., 2022] & MDPs & information & & \\ \hline LSUOB-REPS & Linear Mixture & Bandit & \(\widetilde{O}\left(dS^{2}\sqrt{K}+\sqrt{\frac{SAK}{(1-\gamma)}}\right)\) & ✗ \\ Zhao et al. (2023) & MDPs & Feedback & & \\ \hline Luo et al. (2021) & Linear MDPs & Bandit & \(\widetilde{O}\left(d^{2}K^{14/15}/(1-\gamma)^{4}\right)\) & ✗ \\  & & Feedback & & \\ \hline Dai et al. (2023) & Linear MDPs & Bandit & \(\widetilde{O}\left(\frac{A^{1/9}a^{2/3}}{(1-\gamma)^{2/9}}\right)\) & ✗ \\  & & Feedback & & \\ \hline PO-LSBE & Linear MDPs & Bandit & \(\widetilde{O}\left(\frac{dK^{6/7}}{(1-\gamma)^{2}}+\frac{d^{3/2}K^{5/7}}{(1- \gamma)^{4}}\right)\) & ✗ \\ Sherman et al. (2023) & & Feedback & & \\ \hline GLAP & Linear MDPs & Bandit & \(\widetilde{O}\left(d^{7/5}H^{12/5}K^{4/5}\right)\) & ✗ \\ Kong et al. (2023) & & Feedback & & \\ \hline OPPO+ & Linear MDPs & Full- & \(\widetilde{O}\left(\frac{d^{3/4}K^{3/4}+d^{5/2}\sqrt{K}}{(1-\gamma)^{2}}\right)\) & ✗ \\ Zhong and Zhang (2023) & & information & & \\ \hline POLO & Low-rank & Full- & \(\widetilde{O}\left(\frac{K^{5/6}A^{1/2}d\ln(1+M)}{(1-\gamma)^{2}}\right)\) & ✗ \\
**(Ours)** & MDPs & Information & \(\Omega\left(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK}\right)\) & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparisons of regret bounds with most related works studying adversarial RL with function approximation under unknown transitions. \(K\) is the number of episodes, \(d\) is the ambient dimension of the feature mapping, \(\gamma\) is the discounted factor for infinite-horizon MDPs, and \(S\), \(A\), and \(M\) are the cardinality of the state space, action space, and model class, respectively. Note that the dependence on \(\gamma\) is not strictly comparable since some works originally studying finite-horizon MDPs and these results are translated into results for infinite-horizon MDPs by substituting horizon length \(H\) with \(\Theta(1/(1-\gamma))\). The column of “unknown features” indicates whether the algorithm can work in the case when no true feature mappings are known a priori.

details). Also, we prove a regret lower bound of order \(\Omega(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK})\) for low-rank MDPs with fixed loss functions, which thus also serves as a regret lower bound for our problem and indicates that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting. To the best of our knowledge, this work makes the first step to establish an algorithm with a sublinear regret guarantee for adversarial low-rank MDPs, which permits RL with both nonlinear function approximation and adversarial loss functions. The concrete comparisons between the results of this work and those of previous works are summarized in Table 1.

### Additional Related Works

RL with Function ApproximationSignificant advances have emerged in RL with function approximation to cope with the curse of dimensionality in arbitrarily large state space or action space. In general, these results fall into two categories. The first category studies RL with linear function approximation, including linear MDPs (Yang and Wang, 2019; Jin et al., 2020; Du et al., 2020; Zanette et al., 2020; Wang et al., 2020, 2021; He et al., 2021; Hu et al., 2022; He et al., 2022) and linear mixture MDPs (Ayoub et al., 2020; Zhang et al., 2021; Zhou et al., 2021; He et al., 2021; Zhou and Gu, 2022; Wu et al., 2022; Min et al., 2022; Zhao et al., 2023). Remarkably, He et al. (2023) and Zhou et al. (2021) obtain the nearly minimax optimal regret \(\widetilde{O}(d\sqrt{H^{3}K})\) in linear MDPs and linear mixture MDPs respectively when the loss functions are fixed or stochastic. The other category studies RL with general function approximation. Amongst these works, Jiang et al. (2017), Dann et al. (2018), Sun et al. (2019), Du et al. (2019), and Jin et al. (2021) study the MDPs satisfying the low Bellman-rank assumption, which assumes the Bellman error matrix has a low-rank factorization. Also, Du et al. (2021) consider a similar but slightly more general assumption termed as bounded bilinear rank. Besides, Russo and Roy (2013), Wang et al. (2020), Jin et al. (2021), and Ishfaq et al. (2021) study low Eluder dimension assumption, which is originally proposed to characterize the complexity of function classes for bandit problems.

Representation learning in RL arises when the feature mapping that embeds the state-action pairs in RL with linear function approximation is no longer known a priori. Such a problem is typically studied in the setting of low-rank MDPs, which does not assume the feature mapping of state-action pairs is known. Consequently, the setting of low-rank MDPs strictly generalizes the setting of linear MDPs, but at the cost of being more difficult to learn due to potential nonlinear function approximation induced by representation learning. In this line of research, algorithms with provably sample complexity guarantees have been developed in both model-based methods (Agarwal et al., 2020; Ren et al., 2022; Uehara et al., 2022) and model-free methods (Modi et al., 2021; Zhang et al., 2022), respectively. The model-based algorithms of Agarwal et al. (2020), Ren et al. (2022) and Uehara et al. (2022) learn the representation from a given model class of transition probability kernels. In contrast, the model-free methods do not require model learning but may bear some limitations. In particular, Modi et al. (2021) assume the MDPs satisfying the minimal reachability assumption, and the sample complexity of the algorithm of Zhang et al. (2022) only holds for a special class of low-rank MDPs called block MDPs. Besides, representation learning in Markov games has also been investigated recently (Ni et al., 2022).

RL with Adversarial LossRecent years have witnessed significant advances in learning RL with adversarial losses in the tabular case (Neu et al., 2010; 2012; 2012; Arora et al., 2012; Zimin and Neu, 2013; Dekel and Hazan, 2013; Dick et al., 2014; Rosenberg and Mansour, 2019; 2019; Jin and Luo, 2020; Jin et al., 2020; Shani et al., 2020; Chen et al., 2021; Ghasemi et al., 2021; Rosenberg and Mansour, 2021; Jin et al., 2021; Dai et al., 2022; Chen et al., 2022). When it comes to the setting of linear function approximation, various policy optimization-based methods have been established to solve adversarial linear mixture MDPs (Cai et al., 2020; He et al., 2022) and adversarial linear MDPs (Luo et al., 2021; 2012; 2013; Dai et al., 2023; Sherman et al., 2023; Zhong and Zhang, 2023). The other line of works studies RL with linear function approximation and adversarial losses using occupancy measure-based methods (Neu and Olkhovskaya, 2021; Zhao et al., 2023; Kong et al., 2023). To the best of our knowledge, however, there are no works in existing literature studying RL with both nonlinear function approximation and adversarial loss functions.

## 2 Preliminaries

We consider episodic infinite horizon low-rank MDPs with adversarial loss functions, the preliminaries of which are introduced as follows.

Episodic Infinite-horizon Adversarial MDPsAn episodic infinite horizon adversarial MDP is denoted by a tuple \((\mathcal{S},\mathcal{A},P^{\star},\{\ell_{k}\}_{k=1}^{K},\gamma,d_{0})\),3 where \(\mathcal{S}\) is the state space (with potentially infinitely many states), \(\mathcal{A}\) is the finite action space with cardinality \(|\mathcal{A}|=A\), \(P^{\star}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) is the transition probability kernel such that \(P^{\star}(s^{\prime}\mid s,a)\) is the probability of transferring to state \(s^{\prime}\) from state \(s\) after executing action \(a\), \(\gamma\in[0,1)\) is the discount factor, \(d_{0}\in\Delta(\mathcal{S})\) is the initial distribution over the state space, and \(\ell_{k}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the loss function of episode \(k\) chosen by the adversary. For the ease of exposition, we assume \(d_{0}\) is known.

Footnote 3: Though we focus on episodic infinite-horizon MDPs in this work, note that it is not technically difficult to extend the analyses in this work to the case of episodic finite-horizon MDPs.

In this work, we consider a special class of MDPs called _low-rank MDPs_(Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). Specifically, instead of assuming a known true feature mapping, low-rank MDPs only assume that the transition probability kernel \(P^{\star}\) admits a low-rank decomposition, with the formal definition given as follows.

**Definition 2.1** (Low-rank MDPs).: _An MDP is a low-rank MDP if there exist two feature embedding functions \(\phi^{\star}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) and \(\mu^{\star}:\mathcal{S}\rightarrow\mathbb{R}^{d}\) such that for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\), \(P^{\star}\left(s^{\prime}\mid s,a\right)=\mu^{\star}\left(s^{\prime}\right)^{ \top}\phi^{\star}(s,a)\), where \(\left\|\phi^{\star}(s,a)\right\|_{2}\leq 1\) and for any function \(g:\mathcal{S}\rightarrow[0,1],\left\|\int\mu^{\star}(s)g(s)\mathrm{d}(s) \right\|_{2}\leq\sqrt{d}\)._

Note that the regularity assumption imposed over \(\phi^{\star}\) and \(\mu^{\star}\) is only for the purpose of normalization.

Function ApproximationWhen the state space is arbitrarily large, function approximation is usually considered to permit sample-efficient learning for MDPs. Since the true feature mapping of state-action pairs is not known a priori in the low-rank MDPs, to make this problem tractable, we assume the access to a _realizable_ model class as previous works (Agarwal et al., 2020; Uehara et al., 2022), detailed in the following.

**Assumption 2.1**.: _There exists a known model class \(\mathcal{M}=\{(\mu,\phi):\mu\in\Psi,\phi\in\Phi\}\) satisfying (a) \(\left\|\phi(s,a)\right\|_{2}\leq 1\) and \(\int\mu^{\top}\left(s^{\prime}\right)\phi(s,a)\mathrm{d}\left(s^{\prime} \right)=1\) for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\), \(\mu\in\Psi\), \(\phi\in\Phi\); and (b) \(\left\|\int\mu(s)g(s)\mathrm{d}(s)\right\|_{2}\leq\sqrt{d}\) for any function \(g:\mathcal{S}\rightarrow[0,1]\). Moreover, it holds that \(\mu^{\star}\in\Psi\) and \(\phi^{\star}\in\Phi\)._

Throughout this paper, for the sake of brevity, we assume that the cardinality of \(\Psi\) and \(\Phi\) are finite, meaning that \(\mathcal{M}\) also has bounded cardinality \(M=|\mathcal{M}|\). However, note that extending the analyses to the function classes with infinite cardinality but bounded statistical complexity (_e.g._, VC dimension) is not technically difficult.

Interaction ProtocolWe now introduce the interaction protocol between the learner and the environment. To begin with, denote by \(d_{P}^{\pi}(s,a)=(1-\gamma)\sum_{\tau=0}^{\infty}\gamma^{\tau}d_{P,\tau}^{\pi} (s,a)\) the state-action occupancy distribution, where \(d_{P,\tau}^{\pi}(s,a)\) is the probability of visiting \((s,a)\) at step \(\tau\) under policy \(\pi\) and transition \(P\). With a slight abuse of notation, let \(d_{P}^{\pi}(s)=\sum_{a\in\mathcal{A}}d_{P}^{\pi}(s,a)\) be the state occupancy distribution, denoting the probability of visiting state \(s\) under \(\pi\) and \(P\).

Ahead of time, an MDP is decided by the environment, and only the state space \(\mathcal{S}\) and the action space \(\mathcal{A}\) are revealed to the learner. Meanwhile, the adversary secretly chooses \(K\) loss functions \(\{\ell_{k}\}_{k=1}^{K}\), each of which will be used in one episode. The interaction will proceed in \(K\) episodes. At the beginning of episode \(k\), the learner chooses a stochastic policy \(\pi_{k}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\), where \(\pi_{k}(a\mid s)\) is probability of taking \(a\) at state \(s\). Starting from an initial state \(s_{0}\sim d_{0}\), the learner repeatedly executes policy \(\pi_{k}\) until reaching the termination. After episode \(k\) is terminated, the learner observes a trajectory \(\{(s_{k,\tau},a_{k,\tau})\}_{\tau}\) as well as the loss function \(\ell_{k}\).

For each episode \(k\) and each state-action pair \((s,a)\), the state-action value \(Q_{k}^{\pi}(s,a)\) is defined as \(Q_{k}^{\pi}(s,a)=\mathbb{E}\left[\sum_{\tau=0}^{\infty}\gamma^{\tau}\ell_{k}(s_{ k,\tau},a_{k,\tau})\middle|\pi,P^{\star},(s_{k,0},a_{k,0})=(s,a)\right]\). Also define \(V_{k}^{\pi}(s)=\mathbb{E}_{a\sim\pi(\cdot\mid s)}[Q_{k}^{\pi}(s,a)]\) and \(V_{k}^{\pi}=\mathbb{E}_{s_{0}\sim d_{0}}[V_{k}^{\pi}(s_{0})]\). The learning objective is to minimize the _pseudo regret_ with respect to \(\pi^{\star}\), defined as

\[\mathcal{R}_{K}=\mathbb{E}\left[\sum_{k=1}^{K}\left(V_{k}^{\pi_{k}}-V_{k}^{\pi^{ \star}}\right)\right],\]where the expectation is taken over the potential randomness of the algorithm, \(\pi^{\star}\in\arg\min_{\pi\in\Pi}\sum_{k=1}^{K}V_{k}^{\pi}\) is the fixed optimal policy in hindsight and \(\Pi\) is the set of stochastic policies.

## 3 Algorithm

This section presents our POLO algorithm with the pseudocode illustrated in Algorithm 1. At a high level, POLO leverages a mixed roll-out policy to conduct doubled exploration and exploitation, _i.e._, (a) the exploration over transitions required by representation learning; and (b) the exploration and exploitation over adversarially changed loss functions required by the policy optimization (Section 3.1). To deal with the issue that only the _near optimism_ at the initial state \(s_{0}\) is available in low-rank MDPs, POLO conducts policy optimization in fixed _learned_ models with the epoch-based model update, which enables a new analysis scheme (Section 3.2).

### Doubled Exploration and Exploitation

Let \(\tilde{\pi}_{k}\) be the policy of episode \(k\) computed by policy optimization (_cf._, Eq. (3)). At the beginning of episode \(k\), our algorithm first collects a state \(s_{k}\sim d_{P^{\star}}^{\tilde{\pi}_{k}}\). (Line 6) by invoking a geometric sampling _roll-in_ procedure (Kakade and Langford, 2002; Agarwal et al., 2021; Uehara et al., 2022). Starting from an initial state \(s_{0}\sim d_{0}\), at each step \(\tau\), this roll-in procedure will terminate and return state \(s_{\tau}\) with probability \(1-\gamma\), and otherwise will take action \(a_{\tau}\sim\tilde{\pi}_{k}(\cdot\mid s_{\tau})\) and transfer to the next state \(s_{\tau}\sim P^{\star}(\cdot\mid s_{\tau},a_{\tau})\).

Then our algorithm will further interact with the environment in successive two steps after collecting \(s_{k}\sim d_{P^{\star}}^{\tilde{\pi}_{k}}\) (Line 7 - Line 11) like previous works studying low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). One of the main differences between previous algorithms and ours lies in how to deal with exploration and exploitation when interacting with the environment. In the case of stochastic low-rank MDPs, the key in the analysis is to ensure the (near) optimism of the optimal policy \(\pi^{\star}\) in the learned model, which essentially requires to bound the performance gap \(\widehat{V}_{k}^{\pi^{\star}}-V_{k}^{\pi^{\star}}\) by relating it with the model error regarding \(\pi^{\star}\), _i.e._, \(\mathbb{E}_{(s,a)\sim d_{P^{\star}}^{\pi^{\star}}}[\|\hat{P}_{k}(\cdot\mid s,a )-P^{\star}(\cdot\mid s,a)\|_{1}]\), where \(\hat{P}_{k}\) is the MLE solution defined in Eq. (1). However, this model error is not directly controllable, as the algorithm does not know the optimal policy \(\pi^{\star}\) and can not collect data following \(\pi^{\star}\) to bound the model error. Fortunately, by empirical process theory (Geer, 2000; Zhang, 2006), the model error \(\mathbb{E}_{(s,a)\sim\rho_{k}}[\|\hat{P}_{k}(\cdot\mid s,a)-P^{\star}(\cdot \mid s,a)\|_{1}]\) regarding the executed policies \(\{\tilde{\pi}_{i}\}_{i=1}^{k}\) is bounded, where \(\rho_{k}(s,a)=\frac{1}{k}\sum_{i=1}^{k}d_{P^{\star}}^{\tilde{\pi}_{i}}(s,a)\). Therefore, previous works (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022) ensure the optimism of the optimal policy \(\pi^{\star}\) by applying importance weighting to change the measure from \(d_{P^{\star}}^{\pi^{\star}}\) to \(\rho_{k}\) when bounding the model error. The complication is that to control the ratio \(\pi^{\star}(a\mid s)/\tilde{\pi}_{i}(a\mid s)\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(i\in[K]\) in importance weighting, the algorithms in previous works take actions from the uniform distribution \(U(\mathcal{A})\) over action space \(\mathcal{A}\), which intuitively can be seen as conducting exploration over transitions required by representation learning. Consequently, though these algorithms enjoy excellent sample complexities, they intrinsically do not have regret guarantees due to the uniform exploration over action space, even in the stochastic setting. Moreover, to learn adversarial low-rank MDPs, it is required to take actions adaptively according to the observed loss functions in previous episodes instead of uniformly taking actions. To address this "conflict" so as to learn adversarial low-rank MDPs, we propose to use a mixed roll-out policy to interleave (a) the exploration over transitions required by representation learning; and (b) the exploration and exploitation over the adversarial loss functions by policy optimization, which we call _doubled exploration and exploitation_ and is pivotal to achieving our regret bound as we will shortly see. Formally, our algorithm will conduct the exploration over the transitions with probability \(\xi\) and execute policy \(\tilde{\pi}_{k}\) optimized by OMD with probability \(1-\xi\) respectively, as shown in Line 7 - Line 11.

After interacting with the environment, the newly collected data in these two steps will be used to update the datasets (Line 14), and the empirical transition \(\widehat{P}_{k}\) will be updated by performing MLE over the updated datasets by solving (Line 16)

\[\left(\widehat{\mu}_{k},\widehat{\phi}_{k}\right)=\operatorname*{arg\,max}_{( \mu,\phi)\in\mathcal{M}}\mathbb{E}_{\mathcal{D}_{k}\cup\mathcal{D}_{k}^{ \prime}}\left[\ln\mu^{\top}\left(s^{\prime}\right)\phi(s,a)\right]\,,\] (1)

where we denote \(\mathbb{E}_{\mathcal{D}}\left[f\left(s,a,s^{\prime}\right)\right]=\frac{1}{| \mathcal{D}|}\sum_{(s,a,s^{\prime})\in\mathcal{D}}f\left(s,a,s^{\prime}\right)\).

```
1:Input: Mixing coefficient \(\xi\), epoch length \(L\), regularization coefficients \(\{\lambda_{k}\}_{k=1}^{K}\), bonus coefficients \(\{\alpha_{k}\}_{k=1}^{K}\), model class \(\mathcal{M}\), number of episodes \(K\), learning rate \(\eta\).
2:Initialization: Set \(\mathcal{D}_{0}=\emptyset\), \(\mathcal{D}_{0}^{\prime}=\emptyset\).
3:for\(i=1,2,\ldots,\lceil K/L\rceil\)do
4: Set \(k_{i}=(i-1)L+1\) and \(\tilde{\pi}_{k_{i}}(\cdot\mid s)\) to be uniform for any \(s\in\mathcal{S}\).
5:for\(k=k_{i},k_{i}+1,\ldots,k_{i}+L-1\)do
6: Sample \(s_{k}\) from \(\hat{d}_{\mathcal{D}_{s}}^{\tilde{\pi}_{k}}\).
7: Sample \(c_{k}\sim\mathrm{Ber}(1-\xi)\).
8:if\(c_{k}=1\)then
9: Sample \(a_{k}\sim\tilde{\pi}_{k}(\cdot\mid s_{k}),s^{\prime}_{k}\sim P^{\star}(\cdot \mid s_{k},a_{k}),a^{\prime}_{k}\sim\tilde{\pi}_{k}(\cdot\mid s^{\prime}_{k}),s^{\prime\prime}_{k}\sim P^{\star}(\cdot\mid s^{\prime}_{k},a^{\prime}_{k})\).
10:else
11: Sample \(a_{k}\sim U(\mathcal{A}),s^{\prime}_{k}\sim P^{\star}(\cdot\mid s_{k},a_{k}),a ^{\prime}_{k}\sim U(\mathcal{A}),s^{\prime\prime}_{k}\sim P^{\star}(\cdot\mid s ^{\prime}_{k},a^{\prime}_{k})\).
12:endif
13: Observe the loss function \(\ell_{k}\).
14: Update datasets \(\mathcal{D}_{k}=\mathcal{D}_{k-1}\cup\{(s_{k},a_{k},s^{\prime}_{k})\}\), \(\mathcal{D}^{\prime}_{k}=\mathcal{D}^{\prime}_{k-1}\cup\{(s^{\prime}_{k},a^{ \prime}_{k},s^{\prime\prime}_{k})\}\).
15:if\(k=k_{i}\)then
16: Set the empirical transition \(\widehat{P}_{k}(s^{\prime}\mid s,a)=\widehat{\mu}_{k}(s^{\prime})^{\top} \widehat{\phi}_{k}(s,a)\), \(\forall(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\), via solving Eq. (1).
17: Update the empirical covariance matrix \(\widehat{\Sigma}_{k}=\sum_{(s,a)\in\mathcal{D}_{k}}\widehat{\phi}_{k}(s,a) \widehat{\phi}_{k}(s,a)^{\top}+\lambda_{k}I\).
18: Set the bonus function \(\widehat{b}_{k}(s,a):=\min(\alpha_{k}\|\widehat{\phi}_{k}(s,a)\|_{\widehat{ \Sigma}_{k}^{-1}},2)/(1-\gamma)\), \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\).
19:else
20: Set the empirical transition \(\widehat{P}_{k}=\widehat{P}_{k_{i}}\) and bonus function \(\widehat{b}_{k}=\widehat{b}_{k_{i}}\).
21:endif
22: Compute \(\widehat{Q}_{k}^{\tilde{\pi}_{k}}(\cdot,\cdot)=\mathrm{Policy\text{-}Evaluation}( \widehat{P}_{k},\ell_{k}-\widehat{b}_{k},\tilde{\pi}_{k})\).
23: Update policy \(\tilde{\pi}_{k+1}(\cdot\mid\cdot)\propto\tilde{\pi}_{k}(\cdot\mid\cdot) \exp(-\eta\widehat{Q}_{k}^{\tilde{\pi}_{k}}(\cdot,\cdot))\).
24:endfor
25:endfor ```

**Algorithm 1** Policy Optimization for Low-rank MDPs (POLO)

### Policy Optimization in Fixed Learned Models

It remains to compute the policy \(\tilde{\pi}_{k+1}\) to be used in the next episode. To this end, we resort to the canonical OMD framework to perform policy optimization like previous methods (Shani et al., 2020; Cai et al., 2020; He et al., 2022). However, previous OMD-based policy optimization methods for tabular and linear (mixture) MDPs (Shani et al., 2020; Cai et al., 2020; He et al., 2022) critically depend on the point-wise optimism for each state-action pair, _i.e._, \(\widehat{Q}_{k}^{\tilde{\pi}_{k}}(s,a)\leq\ell_{k}(s,a)+\gamma[P^{\star} \widehat{V}_{k}^{\tilde{\pi}_{k}}](s,a)\), to enable the decomposition (_cf._, Lemma 1 by Shani et al. (2020))

\[\widehat{V}_{k}^{\tilde{\pi}_{k}}(s_{0})-V_{k}^{\pi^{\star}}(s_{0}) =\mathbb{E}\left[\sum_{\tau=0}^{\infty}\gamma^{\tau}\left\langle \tilde{\pi}_{k}(\cdot\mid s_{\tau})-\pi^{\star}(\cdot\mid s_{\tau}),\widehat {Q}_{k}^{\tilde{\pi}_{k}}(s_{\tau},\cdot)\right\rangle\bigg{|}\,\pi^{\star},P ^{\star},s_{0}\right]\] \[+\mathbb{E}\left[\sum_{\tau=0}^{\infty}\gamma^{\tau}\left(\widehat {Q}_{k}^{\tilde{\pi}_{k}}(s_{\tau},a_{\tau})-\ell_{k}(s_{\tau},a_{\tau})- \gamma\left[P^{\star}\widehat{V}_{k}^{\tilde{\pi}_{k}}\right](s_{\tau},a_{\tau} )\right)\bigg{|}\,\pi^{\star},P^{\star},s_{0}\right]\,,\]

where \(\widehat{V}_{k}^{\tilde{\pi}_{k}}\) and \(\widehat{Q}_{k}^{\tilde{\pi}_{k}}\) are the state value and state-action value functions of \(\tilde{\pi}_{k}\) on \((\widehat{P}_{k},\ell_{k}-\widehat{b}_{k})\) with \(\widehat{b}_{k}\) as some bonus function and the expectation is taken over the randomness of sampling \(a_{\tau}\sim\pi^{\star}\left(\cdot\mid s_{\tau}\right)\) and \(s_{\tau+1}\sim P^{\star}\left(\cdot\mid s_{\tau},a_{\tau}\right)\). The summation of the first term in the above display is contributed by competing with the optimal policy \(\pi^{\star}\) in the _true_ model \(P^{\star}\) and can be bounded by common OMD analysis, which thus can be regarded as conducting policy optimization in the _true_ model. The point-wise optimism guarantees that the second term is less than or equal to \(0\).

Nevertheless, in low-rank MDPs, due to the unknown representation, it is generally hard to obtain the above point-wise optimism, which leaves the second optimism term unbounded. To cope with this issue, we instead consider the following decomposition:

\[\widehat{V}_{k}^{\tilde{\pi}_{k}}(s_{0})-V_{k}^{\pi^{\star}}(s_{0})\]\[= \widehat{V}_{k}^{\pi_{k}}(s_{0})-\widehat{V}_{k}^{\pi^{*}}(s_{0})+ \widehat{V}_{k}^{\pi^{*}}(s_{0})-V_{k}^{\pi^{*}}(s_{0})\] \[= \mathbb{E}\left[\sum_{\tau=0}^{\infty}\gamma^{\tau}\left\langle \tilde{\pi}_{k}(\cdot\mid s_{k,\tau})-\pi^{*}(\cdot\mid s_{k,\tau}),\widehat{Q} _{k}^{\pi_{k}}(s_{k,\tau},\cdot)\right\rangle\left|\pi^{*},\widehat{P}_{k},s_{ 0}\right]+\widehat{V}_{k}^{\pi^{*}}(s_{0})-V_{k}^{\pi^{*}}(s_{0})\,,\] (2)

where the first term is contributed by competing against the optimal policy \(\pi^{\star}\) in the _learned_ model \(\widehat{P}_{k}\) and can be seen as conducting policy optimization in _learned_ models. This decomposition will be amenable as long as we can achieve a _near optimism_ at the initial state \(s_{0}\), _i.e._, \(\widehat{V}_{k}^{\pi^{*}}(s_{0})-V_{k}^{\pi^{*}}(s_{0})\lesssim 0\), which turns out to be feasible for low-rank MDPs (Uehara et al., 2022). However, there remains one more caveat. The first term in Eq. (2) is now no longer directly bounded by OMD analysis, due to the local update nature of OMD-based policy optimization at each state and the state occupancy distribution \(d_{\widehat{P}_{k}}^{\pi^{*}}\) now varies across different episodes. To address this issue, Algorithm 1 adopts an epoch-based transition update, in which one epoch has \(L\) episodes and the model is only updated at the first episode in one epoch (Line 15 - Line 20).4 Concretely, Algorithm 1 sets \(\widehat{P}_{k}=\widehat{P}_{k_{i}}\) and \(\widehat{b}_{k}=\widehat{b}_{k_{i}}\), where \(k_{i}\) is the first episode of the epoch to which the episode \(k\) belongs. In this manner, the learned model is _fixed_ in one epoch, and thus the regret of dealing with the adversarial loss functions by competing against the optimal policy \(\pi^{\star}\) can be bounded in one epoch. Subsequently, at the end of episode \(k\), our algorithm first computes the optimistic value estimate \(\widehat{Q}_{k}^{\pi_{k}}\) for current policy \(\tilde{\pi}_{k}\) under \(\widehat{P}_{k}\) together with the bonus-enhanced loss functions \(\ell_{k}-\widehat{b}_{k}\) by policy evaluation (Line 22). Note that this boils down to planning in the setting of linear MDPs for given features in the learned model and this can be done computationally efficiently (Jin et al., 2020b). Then the policy is updated by solving

Footnote 4: Throughout this paper, we suppose for simplicity that the number of episodes \(K\) is divisible by the epoch length \(L\) considered.

\[\tilde{\pi}_{k+1}(\cdot\mid s)\in\operatorname*{arg\,min}_{\pi(\cdot\mid s) \in\Delta(\mathcal{A})}\eta\left\langle\pi(\cdot\mid s),\widehat{Q}_{k}^{\pi _{k}}(s,\cdot)\right\rangle+D_{F}(\pi(\cdot\mid s),\tilde{\pi}_{k}(\cdot\mid s ))\,,\] (3)

where \(\eta>0\) is the learning rate to be tuned later and \(D_{F}(x,y)=F(x)-F(y)-\left\langle x-y,\nabla F(y)\right\rangle\) is the Bregman divergence induced by the regularizer \(F\). With \(F(\pi(\cdot\mid s))=\sum_{a\in\mathcal{A}}\pi(a\mid s)\ln\pi(a\mid s)\) as the negative entropy, the closed-form solution to the above display is shown in Line 23, which can be regarded as a kind of soft policy improvement.

## 4 Analysis

### Regret Upper Bound

The regret upper bound of our POLO algorithm for learning adversarial low-rank MDPs is guaranteed by the following theorem.

**Theorem 4.1**.: _Suppose \(K>d^{6}A^{3}/(1-\gamma)^{6}\). For any adversarial low-rank MDP satisfying Definition 2.1, by setting the epoch length \(L=K^{\nicefrac{{1}}{{2}}}A^{\nicefrac{{-1}}{{2}}}d^{-1}\xi(1-\gamma)\), learning rate \(\eta=(1-\gamma)\sqrt{\ln A/(2L)}\), bonus coefficient \(\alpha_{k}=O(\sqrt{\gamma(A/\xi+d^{2})\ln(Mk/\delta)})\), regularization coefficient \(\lambda_{k}=O(d\ln(Mk/\delta))\), mixing coefficient \(\xi=K^{\nicefrac{{-1}}{{6}}}A^{\nicefrac{{1}}{{2}}}d/(1-\gamma)\), and \(\delta=1/K\), then the regret of Algorithm 1 is upper bounded by_

\[\mathcal{R}_{K}=O\left(\frac{K^{\frac{5}{6}}A^{\frac{1}{2}}d\ln\left(1+AMK^{2} \right)}{\left(1-\gamma\right)^{2}}\right)\,.\]

**Remark 4.1**.: _Ignoring the dependence on all logarithmic factors but \(M\), the regret upper bound can be simplified as \(\widetilde{O}(K^{\nicefrac{{5}}{{6}}}A^{\nicefrac{{1}}{{2}}}d\ln(1+M)/(1- \gamma)^{2})\). Comparing with the regret lower bound \(\Omega(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK})\) in Theorem 4.2, the regret upper bound in Theorem 4.1 matches in \(A\) up to a logarithmic factor but looses in factors of \(K\) and \(d\). Also, note that when \(K\) is large enough such that \(\xi\) and \(L\) can be chosen as \(\xi=K^{\nicefrac{{-1}}{{6}}}A^{\nicefrac{{1}}{{3}}}d^{\nicefrac{{6}}{{3}}}/(1- \gamma)\) and \(L=K^{\nicefrac{{1}}{{2}}}A^{\nicefrac{{-1}}{{2}}}d^{-2}\xi(1-\gamma)=K^{ \nicefrac{{1}}{{3}}}A^{\nicefrac{{-2}}{{3}}}d^{\nicefrac{{-4}}{{3}}}\geq 1\), meaning that \(K\geq d^{4}A^{2}\), the regret upper bound can be further optimized to \(\widetilde{O}(K^{\nicefrac{{5}}{{6}}}A^{\nicefrac{{1}}{{3}}}d^{\nicefrac{{2}} {{3}}}\ln(1+M)/(1-\gamma)^{2})\). Note that this does not conflict with the regret lower bound in Section 4.3 since the magnitude of this upper bound is still larger than that of the regret lower bound as long as \(K\geq A^{\nicefrac{{1}}{{2}}}d^{\nicefrac{{-1}}{{2}}}\gamma^{6}(1-\gamma)^{3}\)._

### Proof of Regret Upper Bound

We now present the proof of Theorem 4.1. To begin with, recall that in each episode \(k\), after state \(s_{k}\) is sampled from \(d_{P^{*}}^{\tilde{\pi}_{k}}\), the actual roll-out policy will be \(\pi_{k}(\cdot\mid s)=\xi\cdot U(\mathcal{A})+(1-\xi)\cdot\tilde{\pi}_{k}(\cdot \mid s)\). Therefore, it holds that

\[\mathcal{R}_{K} =\mathbb{E}\left[\sum_{k=1}^{K}\left(V_{k}^{\pi_{k}}-V_{k}^{\pi^{ *}}\right)\right]\] \[=\mathbb{E}\left[\sum_{k=1}^{K}\mathbb{I}\{c_{k}=1\}\left(V_{k}^ {\pi_{k}}-V_{k}^{\pi^{*}}\right)+\mathbb{I}\{c_{k}=0\}\left(V_{k}^{\pi_{k}}-V_ {k}^{\pi^{*}}\right)\right]\] \[\leq\mathbb{E}\left[\sum_{k=1}^{K}\left(V_{k}^{\tilde{\pi}_{k}}-V_ {k}^{\pi^{*}}\right)\right]+\frac{\xi K}{(1-\gamma)}\,,\] (4)

where the inequality is due to that \(\sum_{\tau=0}^{\infty}\gamma^{\tau}\ell_{k}\left(s_{\tau},a_{\tau}\right)\in[0,1/(1-\gamma)]\) holds for any episode \(k\) and any trajectory \(\{(s_{\tau},a_{\tau})\}_{\tau=0}^{\infty}\). We now turn to bound the first term in Eq. (4) by decomposing it into the following three terms

\[\mathbb{E}\left[\sum_{k=1}^{K}\left(V_{k}^{\tilde{\pi}_{k}}-V_{k}^{\pi^{*}} \right)\right]=\mathbb{E}\left[\underbrace{\sum_{k=1}^{K}\left(V_{k}^{\tilde{ \pi}_{k}}-\widehat{V}_{k}^{\tilde{\pi}_{k}}\right)}_{\text{Estimation Bias Term}}+\underbrace{\sum_{k=1}^{K}\left(\widehat{V}_{k}^{\tilde{\pi}_{k}}- \widehat{V}_{k}^{\pi^{*}}\right)}_{\text{OMD Regret Term}}+\underbrace{\sum_{k=1}^{K} \left(\widehat{V}_{k}^{\pi^{*}}-V_{k}^{\pi^{*}}\right)}_{\text{Optimal Term}}\right]\,.\] (5)

Bounding OMD Regret TermThe OMD Regret Term is contributed by competing against \(\pi^{*}\) using \(\tilde{\pi}_{k}\) with \(\widehat{Q}_{k}^{\tilde{\pi}_{k}}\) as loss function in the learned model \(\widehat{P}_{k_{i}}\). This term is thus bounded by standard OMD analysis, detailed in the following lemma.

**Lemma 4.1**.: _By setting \(\eta=(1-\gamma)\sqrt{\ln A/(2L)}\), the OMD Regret Term is bounded as \(\mathbb{E}\left[\sum_{k=1}^{K}\left(\widehat{V}_{k}^{\tilde{\pi}_{k}}-\widehat {V}_{k}^{\pi^{*}}\right)\right]\leq\frac{K\sqrt{2\ln A}}{\sqrt{L(1-\gamma)^{2}}}\)._

Bounding Optimism TermThe Optimism Term is controlled by choosing appropriate bonus coefficient \(\alpha_{k}\). Note that different from tabular and linear cases, the bonus functions and coefficients here are not devised to control the optimism for each state-action pair. Instead, they are devised to provide a (near) optimism only at the initial state \(s_{0}\).

**Lemma 4.2**.: _By setting \(\alpha_{k}=O(\sqrt{\gamma(A/\xi+d^{2})\ln(Mk/\delta)})\) and \(\lambda_{k}=O(d\ln(Mk/\delta))\), with probability \(1-\delta\), the Optimism Term is bounded as \(\sum_{k=1}^{K}\left(\widehat{V}_{k}^{\pi^{*}}-V_{k}^{\pi^{*}}\right)\leq(L+ \sqrt{K})\sqrt{\frac{A\ln(MN/\delta)}{\xi(1-\gamma)^{3}}}\)._

Bounding Estimation Bias TermIt remains to bound the Estimation Bias Term, which comes from the difference between the values of running the same policy \(\tilde{\pi}_{k}\) in the true model (_i.e._, \(P^{*}\) and \(\ell_{k}\)) and the learned empirical model (_i.e._, \(\widehat{P}_{k}\) and \(\ell_{k}-\widehat{b}_{k}\)), respectively. This term can be translated into the error between the true model and the learned model using the common simulation lemma, which is thus bounded by the summation of bonus functions. Note that since the empirical features used to construct our bonus functions vary in each episode, we first relate the bonus functions with the fixed true feature \(\phi^{*}\) using the one-step trick [Uehara et al., 2022, Zhang et al., 2022], and finally bound this term with the leverage of the canonical elliptical potential lemma. The result is shown in the following lemma.

**Lemma 4.3**.: _By setting \(\alpha_{k}=O(\sqrt{\gamma(A/\xi+d^{2})\ln(Mk/\delta)})\) and \(\lambda_{k}=O(d\ln(Mk/\delta))\), with probability \(1-\delta\), the Estimation Bias Term is bounded as \(\sum_{k=1}^{K}\left(V_{k}^{\tilde{\pi}_{k}}-\widehat{V}_{k}^{\tilde{\pi}_{k}} \right)\leq O\left(\frac{d^{2}\Delta\sqrt{K\xi}}{\xi(1-\gamma)^{3}}\sqrt{\ln(1 +K)\ln(MK/\delta)}\right)\)._

We refer the readers to Appendix A for the proof of the above lemmas. The proof of Theorem 4.1 is now concluded by first combining Eq. (4), Eq. (5), Lemma 4.1, 4.2, and 4.3 and then choosing \(L=K^{\nicefrac{{1}}{{2}}}A^{-\nicefrac{{1}}{{2}}}d^{-1}\xi(1-\gamma)\), \(\xi=K^{-\nicefrac{{1}}{{6}}}A^{\nicefrac{{1}}{{2}}}d/(1-\gamma)\), and \(\delta=1/K\).

Intuitively, the epoch length \(L\) illustrates a trade-off between dealing with the adversarial losses and the representation learning over the unknown transitions. When \(L\) is large, there will be fewer restarts in the running of OMD and thus the learner will suffer less regret contributed by dealing with the adversarial losses as shown by Lemma 4.1. In contrast, a smaller \(L\) enables more frequent model updates, which leads to more accurate model estimation and less regret contributed by the representation learning as shown by Lemma 4.2 and 4.3.

### Regret Lower Bound

This section presents the regret lower bound for learning adversarial low-rank MDPs with fixed loss functions in Theorem 4.2, which thus also serves as a regret lower bound for learning adversarial low-rank MDPs with full-information feedback.

**Theorem 4.2**.: _Suppose \(d\geq 8\), \(S\geq d+1\), \(A\geq d-3\), and \(K\geq 2(d-4)A\). Then for any algorithm \(\mathrm{Alg}\), there exists an episodic infinite-horizon low-rank MDP \(\mathcal{M}_{\mathrm{Alg}}\) with fixed loss function such that the regret for this MDP is lower bounded by \(\Omega(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK})\)._

Proof Sketch.: At a high level, we construct \(dA\) hard-to-learn low-rank MDP instances, which are difficult to distinguish in KL divergence but have very different optimal policies. In particular, all the constructed low-rank MDP instances have three levels of states, in which the only state in the first level is a fixed initial state and the states in the third level are absorbing states. Moreover, only one unique absorbing state in the third level has the lowest loss, which is termed as the "good state". In the constructed low-rank MDP instance \(\mathcal{M}_{(i^{\star},a^{\star})}\), the learner can only take specific action to transfer to state \(s_{2,i^{\star}}\) in the second level and then take the other specific action to transfer to the unique good state. Due to the unknown representations of state-action pairs, the learner needs to distinguish all these \(dA\) low-rank MDP instances, which is essentially equivalent to dealing with a bandit problem with \(dA\) "arms". The detailed proof of Theorem 4.2 is postponed to Appendix B. 

**Remark 4.2**.: _Theorem 4.2, to the best of our knowledge, provides the first regret lower bound for learning low-rank MDPs with fixed loss functions. We note that this regret lower bound can hold when \(d\ll S\) and \(d\ll A\), which thus means that this lower bound is non-trivial. Besides, the regret upper bound in our Theorem 4.1 matches the regret lower bound in \(A\) up to a logarithmic factor but looses a factor of \(\widetilde{O}(K^{\nicefrac{{1}}{{3}}}d^{\nicefrac{{1}}{{2}}}/((1-\gamma) \gamma^{2}))\). Importantly, compared with the regret upper bound \(\widetilde{O}(d\sqrt{K/(1-\gamma)^{3}})\) of linear MDPs [10] (the finite horizon \(H\) is substituted by the effective horizon \(\Theta(1/(1-\gamma))\) in our infinite-horizon setting for a fair comparison), the dependence on \(A\) in the regret lower bound of low-rank MDP shows a clear separation between low-rank MDPs and linear MDPs, which demonstrates that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting._

## 5 Conclusions

In this work, we study learning adversarial low-rank MDPs with unknown transition and full-information feedback. We prove that our proposed algorithm POLO achieves the \(\widetilde{O}(K^{\nicefrac{{5}}{{6}}}A^{\nicefrac{{1}}{{2}}}d\ln(1+M)/(1- \gamma)^{2})\) regret, which is the first sublinear regret guarantee for this challenging problem. The design of our proposed algorithm features (a) a doubled exploration and exploitation scheme to simultaneously learn the transitions and adversarial loss functions; and (b) policy optimization in the fixed learned models with epoch-based model update to enable a new analysis scheme that only requires the near optimism at the initial state instead of the point-wise optimism. Also, we prove an \(\Omega(\frac{\gamma^{2}}{1-\gamma}\sqrt{dAK})\) regret lower bound for this problem, serving as the first regret lower bound for learning low-rank MDPs in the regret minimization setting. Besides, there also remain several interesting future directions to be explored. One natural question is whether it is possible to further optimize the dependence of our regret guarantee on the number of episodes \(K\). The other question is how to also learn adversarial low-rank MDPs with only the bandit feedback available. This is also challenging since the current occupancy measure-based methods and policy optimization-based methods tackling adversarial MDPs with bandit feedback both depend on the point-wise optimism provided by the true feature mapping, which seems not feasible in low-rank MDPs. We hope our results may shed light on better understandings of RL with both nonlinear function approximation and adversarial losses and we leave the above extensions as our future works.

## Limitations

We note that in general our algorithm is oracle-efficient (given access to the MLE computation oracle in Eq. (1)) but may not be computationally efficient as previous works studying low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022; Ni et al., 2022). However, we also remark that in practice, these algorithms including ours are computationally feasible since the computation of MLE is only a standard supervised learning problem and can be implemented using gradient descent methods. The other limitation is that throughout this paper, we assume model class \(\mathcal{M}\) with bounded cardinality \(M\) and the regret upper bound of our algorithm has a logarithmic dependence on \(M\). We remark that similar assumptions and dependence have also appeared in previous theoretical works studying RL with general function approximation (Jiang et al., 2017; Sun et al., 2019). Also, extending the analyses to an infinite hypothesis class is possible if the hypothesis class has bounded statistical complexity (Agarwal et al., 2020).

## Acknowledgement

The corresponding author Shuai Li is supported by National Key Research and Development Program of China (2022ZD0114804) and National Natural Science Foundation of China (62376154, 62006151, 62076161). Baoxiang Wang is partially supported by National Natural Science Foundation of China (62106213, 72150002) and Shenzhen Science and Technology Program (RCBS20210609104356063, JCYJ20210324120011032).

## References

* Abbeel and Ng (2005) Pieter Abbeel and Andrew Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In _Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005_, volume 119 of _ACM International Conference Proceeding Series_, pages 1-8. ACM, 2005.
* Agarwal et al. (2020) Alekh Agarwal, Sham M. Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: structural complexity and representation learning of low rank mdps. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Agarwal et al. (2021) Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _J. Mach. Learn. Res._, 22:98:1-98:76, 2021.
* Arora et al. (2012) Raman Arora, Ofer Dekel, and Ambuj Tewari. Deterministic mdps with adversarial rewards and bandit feedback. In Nando de Freitas and Kevin P. Murphy, editors, _Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA, USA, August 14-18, 2012_, pages 93-101. AUAI Press, 2012.
* Ayoub et al. (2020) Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 463-474. PMLR, 2020.
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 263-272. PMLR, 2017.
* Cai et al. (2020) Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 1283-1294. PMLR, 2020.

Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Minimax regret for stochastic shortest path with adversarial costs and known transition. In _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 1180-1215. PMLR, 2021.
* Chen et al. (2022) Liyu Chen, Haipeng Luo, and Aviv Rosenberg. Policy optimization for stochastic shortest path. In Po-Ling Loh and Maxim Raginsky, editors, _Conference on Learning Theory, 2-5 July 2022, London, UK_, volume 178 of _Proceedings of Machine Learning Research_, pages 982-1046. PMLR, 2022.
* Cheng et al. (2023) Yuan Cheng, Ruiquan Huang, Yingbin Liang, and Jing Yang. Improved sample complexity for reward-free reinforcement learning under low-rank mdps. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.
* Dai et al. (2022) Yan Dai, Haipeng Luo, and Liyu Chen. Follow-the-perturbed-leader for adversarial markov decision processes with bandit feedback. _CoRR_, abs/2205.13451, 2022. doi: 10.48550/arXiv.2205.13451.
* Dai et al. (2023) Yan Dai, Haipeng Luo, Chen-Yu Wei, and Julian Zimmert. Refined regret for adversarial mdps with linear function approximation. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 6726-6759. PMLR, 2023.
* Dann et al. (2018) Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. On oracle-efficient PAC RL with rich observations. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 1429-1439, 2018.
* Dekel and Hazan (2013) Ofer Dekel and Elad Hazan. Better rates for any adversarial deterministic MDP. In _Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013_, volume 28 of _JMLR Workshop and Conference Proceedings_, pages 675-683. JMLR.org, 2013.
* Dick et al. (2014) Travis Dick, Andras Gyorgy, and Csaba Szepesvari. Online learning in markov decision processes with changing cost sequences. In _Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014_, volume 32 of _JMLR Workshop and Conference Proceedings_, pages 512-520. JMLR.org, 2014.
* Domingues et al. (2021) Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In _Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide_, volume 132 of _Proceedings of Machine Learning Research_, pages 578-598. PMLR, 2021.
* Du et al. (2019) Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with rich observations via latent state decoding. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 1665-1674. PMLR, 2019.
* Du et al. (2020) Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* Du et al. (2021) Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in RL. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 2826-2836. PMLR, 2021.
* Even-Dar et al. (2009) Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. Online markov decision processes. _Math. Oper. Res._, 34(3):726-736, 2009.
* Feinberg (1996) A. Feinberg. Markov decision processes: Discrete stochastic dynamic programming (martin l. puterman). _SIAM Rev._, 38(4):689, 1996.
* Fu et al. (2019)Aurelien Garivier, Pierre Menard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in bandit problems. _Math. Oper. Res._, 44(2):377-399, 2019.
* Geer (2000) Sara A Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* Ghasemi et al. (2021) Mahsa Ghasemi, Abolfazl Hashemi, Haris Vikalo, and Ufuk Topcu. No-regret learning with high-probability in adversarial markov decision processes. In _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July 2021_, volume 161 of _Proceedings of Machine Learning Research_, pages 992-1001. AUAI Press, 2021.
* He et al. (2021) Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 4171-4180. PMLR, 2021a.
* He et al. (2021) Jiafan He, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for discounted mdps. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 22288-22300, 2021b.
* He et al. (2022) Jiafan He, Dongruo Zhou, and Quanquan Gu. Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 4259-4280. PMLR, 2022.
* He et al. (2023) Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 12790-12822. PMLR, 2023.
* Hu et al. (2022) Pijhe Hu, Yu Chen, and Longbo Huang. Nearly minimax optimal reinforcement learning with linear function approximation. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 8971-9019. PMLR, 2022.
* Ishfaq et al. (2021) Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 4607-4616. PMLR, 2021.
* Jiang et al. (2017) Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1704-1713. PMLR, 2017.
* Jin et al. (2020a) Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial markov decision processes with bandit feedback and unknown transition. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 4860-4869. PMLR, 2020a.
* Jin et al. (2020b) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria]_, volume 125 of _Proceedings of Machine Learning Research_, pages 2137-2143. PMLR, 2020b.
* Jin et al. (2021a) Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 13406-13418, 2021a.
* Jin et al. (2021b)Tiancheng Jin and Haipeng Luo. Simultaneously learning stochastic and adversarial episodic mdps with known transition. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Jin et al. [2021b] Tiancheng Jin, Longbo Huang, and Haipeng Luo. The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 20491-20502, 2021b.
* Kakade and Langford [2002] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Claude Sammut and Achim G. Hoffmann, editors, _Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002_, pages 267-274. Morgan Kaufmann, 2002.
* Kong et al. [2023] Fang Kong, Xiangcheng Zhang, Baoxiang Wang, and Shuai Li. Improved regret bounds for linear adversarial mdps via linear optimization. _CoRR_, abs/2302.06834, 2023.
* Laskin et al. [2020] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: contrastive unsupervised representations for reinforcement learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 5639-5650. PMLR, 2020.
* Lattimore and Szepesvari [2020] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Luo et al. [2021a] Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. _CoRR_, abs/2107.08346, 2021a.
* Luo et al. [2021b] Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 22931-22942, 2021b.
* Min et al. [2022] Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Learning stochastic shortest path with linear function approximation. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 15584-15629. PMLR, 2022.
* Modi et al. [2021] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free representation learning and exploration in low-rank mdps. _CoRR_, abs/2102.07035, 2021.
* Neu and Olkhovskaya [2021] Gergely Neu and Julia Olkhovskaya. Online learning in mdps with linear function approximation and bandit feedback. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 10407-10417, 2021.
* The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010_, pages 231-243. Omnipress, 2010a.
* Neu et al. [2010b] Gergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos. Online markov decision processes under bandit feedback. In _Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada_, pages 1804-1812. Curran Associates, Inc., 2010b.
* Neu et al. [2012] Gergely Neu, Andras Gyorgy, and Csaba Szepesvari. The adversarial stochastic shortest path problem with unknown transition probabilities. In _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma, Canary Islands, Spain, April 21-23, 2012_, volume 22 of _JMLR Proceedings_, pages 805-813. JMLR.org, 2012.
* Ni et al. [2022] Chengzhuo Ni, Yuda Song, Xuezhou Zhang, Chi Jin, and Mengdi Wang. Representation learning for general-sum low-rank markov games. _CoRR_, abs/2210.16976, 2022.
* Ni et al. [2021]Tongzheng Ren, Tianjun Zhang, Csaba Szepesvari, and Bo Dai. A free lunch from the noise: Provable and practical exploration for representation learning. In _Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands_, volume 180 of _Proceedings of Machine Learning Research_, pages 1686-1696. PMLR, 2022.
* Rosenberg and Mansour (2019) Aviv Rosenberg and Yishay Mansour. Online stochastic shortest path with bandit feedback and unknown transition function. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 2209-2218, 2019a.
* Rosenberg and Mansour (2019) Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 5478-5486. PMLR, 2019b.
* Rosenberg and Mansour (2021) Aviv Rosenberg and Yishay Mansour. Stochastic shortest path with adversarially changing costs. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 2936-2942. ijcai.org, 2021.
* Russo and Van Roy (2013) Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, pages 2256-2264, 2013.
* Schwarzer et al. (2021) Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R. Devon Hjelm, Philip Bachman, and Aaron C. Courville. Pretraining representations for data-efficient reinforcement learning. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12686-12699, 2021.
* Shani et al. (2020) Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with bandit feedback. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 8604-8613. PMLR, 2020a.
* Shani et al. (2020) Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with bandit feedback. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 8604-8613. PMLR, 2020b.
* Sherman et al. (2023) Uri Sherman, Tomer Koren, and Yishay Mansour. Improved regret for efficient online reinforcement learning with linear function approximation. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 31117-31150. PMLR, 2023.
* Silver et al. (2018) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* Stooke et al. (2021) Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9870-9879. PMLR, 2021.
* Sun et al. (2019) Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, volume 99 of _Proceedings of Machine Learning Research_, pages 2898-2933. PMLR, 2019.

Aristide C. Y. Tossou, Debabrota Basu, and Christos Dimitrakakis. Near-optimal optimistic reinforcement learning using empirical bernstein inequalities. _CoRR_, abs/1905.12425, 2019.
* Uehara et al. [2022] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline RL in low-rank mdps. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* Wang et al. [2020] Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Wang et al. [2021a] Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of offline RL with linear function approximation? In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021a.
* Wang et al. [2021b] Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021b.
* Weisz et al. [2021] Gellert Weisz, Philip Amortila, and Csaba Szepesvari. Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. In _Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide_, volume 132 of _Proceedings of Machine Learning Research_, pages 1237-1264. PMLR, 2021.
* Wu et al. [2022] Yue Wu, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal regret for learning infinite-horizon average-reward mdps with linear function approximation. In _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 3883-3913. PMLR, 2022.
* Xie et al. [2022] Zhihui Xie, Zichuan Lin, Junyou Li, Shuai Li, and Deheng Ye. Pretraining in deep reinforcement learning: A survey. _CoRR_, abs/2211.03959, 2022.
* Yang and Wang [2019] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 6995-7004. PMLR, 2019.
* Yang and Nachum [2021] Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential decision making. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 11784-11794. PMLR, 2021.
* Yu et al. [2009] Jia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward processes. _Math. Oper. Res._, 34(3):737-757, 2009.
* Zanette et al. [2020] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]_, volume 108 of _Proceedings of Machine Learning Research_, pages 1954-1964. PMLR, 2020.
* Zanette et al. [2021] Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization and exploration with linear function approximation. In _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 4473-4525. PMLR, 2021.
* Zhang [2006] Tong Zhang. From \(\varepsilon\)-entropy to kl-entropy: Analysis of minimum information complexity density estimation. _The Annals of Statistics_, 34(5):2180-2210, 2006.
* Zhang et al. [2019]Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 26517-26547. PMLR, 2022.
* Zhang et al. (2021) Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S. Du. Improved variance-aware confidence sets for linear bandits and linear mixture MDP. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 4342-4355, 2021.
* Zhao et al. (2023) Canzhe Zhao, Ruofeng Yang, Baoxiang Wang, and Shuai Li. Learning adversarial linear mixture markov decision processes with bandit feedback and unknown transition. In _The Eleventh International Conference on Learning Representations_, 2023.
* Zhong and Zhang (2023) Han Zhong and Tong Zhang. A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. _arXiv preprint arXiv:2305.08841_, 2023.
* Zhou and Gu (2022) Dongruo Zhou and Quanquan Gu. Computationally efficient horizon-free reinforcement learning for linear mixture mdps. _CoRR_, abs/2205.11507, 2022.
* Zhou et al. (2021) Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 4532-4576. PMLR, 2021.
* Zimin and Neu (2013) Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by relative entropy policy search. In Christopher J. C. Burges, Leon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, pages 1583-1591, 2013.