# Selective Explanations

 Lucas Monteiro Paes

Harvard University

lucaspaes@g.harvard.edu &Dennis Wei

IBM Research

dwei@us.ibm.com &Flavio P. Calmon

Harvard University

flavio@seas.harvard.edu

###### Abstract

Feature attribution methods explain black-box machine learning (ML) models by assigning importance scores to input features. These methods can be computationally expensive for large ML models. To address this challenge, there have been increasing efforts to develop _amortized explainers_, where a ML model is trained to efficiently approximate computationally expensive feature attribution scores. Despite their efficiency, amortized explainers can produce misleading explanations. In this paper, we propose _selective explanations_ to (i) detect when amortized explainers generate inaccurate explanations and (ii) improve the approximation of the explanation using a technique we call _explorations with initial guess_. Selective explanations allow practitioners to specify the fraction of samples that receive explanations with initial guess, offering a principled way to bridge the gap between amortized explainers (one inference) and more computationally costly approximations (multiple inferences). Our experiments on various models and datasets demonstrate that feature attributions via selective explanations strike a favorable balance between explanation quality and computational efficiency.

## 1 Introduction

Large black-box models are increasingly used to support decisions in applications ranging from online content moderation [26], hiring [12], and medical diagnostics [35]. In such high-stakes settings, the need to explain "why" a model produces a given output has led to a growing number of perturbation-based _feature attribution_ methods [22, 29, 27, 23, 2, 40]. These methods use input perturbations to assign numerical values to each input feature (e.g., words in a text) a model uses, indicating their influence on the model prediction. They are widely adopted in part because they work in the black-box setting with access only to model output (i.e., without gradients). However, existing feature attribution methods can be prohibitively expensive for the large models used in the current machine learning landscape (e.g., language models with billions of parameters) since they require a significant number of inferences for each individual explanation.

Recent literature has introduced two main _approximation_ strategies to speed up existing feature attribution methods for large models: (i) employing Monte Carlo methods to approximate explanations with fewer computations [22, 29, 5, 24], and (ii) adopting an _amortized_ approach, training a separate model to "mimic" the outputs of a reference explanation method [16, 6, 38, 32, 3, 33]. Monte Carlo approximations can yield accurate approximations for attributions but may converge slowly, limiting their practicality for and online applications. In contrast, amortized explainers require only one inference per explanation, making them efficient for large black-box models and online explanations. However, as shown in Figure 1, amortized explainers can produce highly diverging explanations from their reference due to lack of precision in approximations. Aiming to benefit from Monte Carlo and amortized explainers, we propose _selective explanations_ to answer the questions:

* When are amortized explanations inaccurate?
* How can we improve inaccurate amortized explanations using additional computations?To answer (Q1) and (Q2), we propose _selective explanations_, a method that bridges Monte Carlo and amortized explanations. The selective explainer first trains a model that "learns to select" which data points will receive inaccurate amortized explanations, and then performs additional computations to further approximate target explanations. The key idea behind the selective explanation method is to use Monte Carlo explanations only for points that would receive inaccurate amortized explanations; see Figure 2 for the workflow of selective explanations. The code for generating selective explanations can be found at https://github.com/LucasMonteiroPaes/selective-explanations.

The ideas of predicting selectively and providing recourse with a more accurate but expensive method (usually human feedback) have been explored in classification and regression [28; 10; 7; 9; 11]. To our knowledge, however, these ideas have not been applied to feature attribution methods. We make **two contributions** in this regard that are relevant for selective prediction more generally. (1) Selective prediction uses _quality metrics_ to identify input points for which the predictor (the amortized explainer in our case) would produce inaccurate outputs and recourse is needed. The high-dimensional nature of explanations requires us to develop new quality metrics (Section 3) suitable for this setting. (2) Instead of providing recourse with a Monte Carlo explanation alone, we use an optimized method called _explanations with initial guess_ (Section 4) that combines amortized and Monte Carlo explanations in a optimized manner, improving the approximation to the target explanation beyond that of either method individually.

Our **overall contribution** (3) is to combine (1) and (2) in the form of _selective explanations_, providing explanations with initial guess to improve inaccurate amortized explanations. We validate our selective explanations approach on two language models as well as tabular datasets demonstrating its ability to (i) detect inaccurate explanations from the amortized explainer, (ii) enhancing amortized explanations even when Monte Carlo explanations are inaccurate, and (iii) improving the worst explanations from the amortized model.

Fig. 1: Amortized explainer (a) compared with a target explainer (SHAP [22]) (b) and our selective explanation method (c). All methods flag input parts that contribute to the YelpLLM predicting the given example is a Negative Review. We observe that both target and selective explanations attribute ”not amazing” for the negative review (blue), while the amortized explainer misses this term.

Fig. 2: Workflow of selective explanations.

## 2 Problem Setup & Background

We aim to explain the predictions of a fixed probabilistic black-box model \(h\) that predicts \(h(\bm{x})=(h_{1}(\bm{x}),...,h_{|\mathcal{Y}|}(\bm{x}))\) and outputs \(\operatorname*{argmax}_{j\in\mathcal{Y}}h_{j}(\bm{x})\in\mathcal{Y}\) using a vector of features \(\bm{x}=(x_{1},...,x_{d})\in\mathbb{R}^{d}\). The user specifies an output of interest \(\bm{y}\in\mathcal{Y}\) (usually \(\bm{y}=\operatorname*{argmax}_{j\in\mathcal{Y}}h_{j}(\bm{x})\)) and our goal is to efficiently explain _Why would \(h\) output \(\bm{y}\) for a given \(\bm{x}\)?_ We consider a dataset \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{N}\) comprised of \(N>0\) samples divided into three parts: \(\mathcal{D}_{\mathtt{train}}\) for training \(h\) and the explainers, \(\mathcal{D}_{\mathtt{cal}}\) for calibration and validation, and \(\mathcal{D}_{\mathtt{test}}\) for testing. Thus, \(\mathcal{D}=\mathcal{D}_{\mathtt{train}}\cup\mathcal{D}_{\mathtt{cal}}\cup \mathcal{D}_{\mathtt{test}}\). Moreover, for a subset \(S=\{i_{1},...,i_{|S|}\}\subset[d]\) we write \(\bm{x}_{S}\triangleq(x_{i_{1}},...,x_{i_{|S|}})\).

**Feature Attribution Methods,** also called _explainers_, are functions \(\mathbb{R}^{d}\times\mathcal{Y}\rightarrow\mathbb{R}^{d}\) that assess the importance of each feature for the model's (\(h\)) prediction to be \(\bm{y}\) for a given input vector \(\bm{x}\). We consider three types of explainers:

1. **Target explainers** that use a large number of computations to provide explanations (e.g., SHAP with \(2^{d}\) inferences from model \(h\)) [22; 29], denoted by \(\mathsf{Target}(\bm{x},\bm{y})\);
2. **Monte Carlo explainers** that approximate fixed target explainers using \(n\) inferences from model \(h\) per explanation [22; 24], denoted by \(\mathsf{MC}^{n}(\bm{x},\bm{y})\);
3. **Amortized explainers** are trained to approximate the target explanations using only one inference [6; 38], denoted by \(\mathsf{Amor}(\bm{x},\bm{y})\).

**Remark 1**.: Monte Carlo and amortized explainers aim to approximate the target explanation and are benchmarked on this approximation. We evaluate the performance of Monte Carlo and amortized explainers by computing their distance and correlation to \(\mathsf{Target}(\bm{x},\bm{y})\). The usefulness of target explanations (e.g.: SHAP and Lime) has been validated by user studies and automated metrics in [22; 29; 13; 37; 30; 31]. Therefore, we call **higher-quality** the explanations that closely approximate the computationally expensive target and **lower-quality** the one that diverge from the target.

In practice, we measure the quality of a given explanation that aims to approximate the target explanation using a loss (or distortion) function \(\ell:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\), e.g., mean square error (MSE) and Spearman's correlation. The goal of selective and Monte Carlo explanations is to approximate the target explanations while decreasing the number of computations, i.e., to minimize \(\ell(\mathsf{SE}(\bm{x},\bm{y}),\mathsf{Target}(\bm{x},\bm{y}))\) with few model inferences.

We define _selective explainers_ to provide better approximations to target explanations bridging the gap between Monte Carlo and amortized explainers.

**Definition 1** (**Selective Explainer)**.: For a given model \(h\), an amortized explainer \(\mathsf{Amor}\), a Monte Carlo explainer \(\mathsf{MC}^{n}\), a _combination function_\(\lambda_{h}:\mathbb{R}^{d}\rightarrow\mathbb{R}\), and a _selection function_\(\tau_{\alpha}:\mathbb{R}^{d}\rightarrow\{0,1\}\) (parametrized by \(\alpha\)), we define the _selective explainer_\(\mathsf{SE}(\bm{x},\bm{y})\) as

\[\mathsf{SE}(\bm{x},\bm{y})\triangleq\begin{cases}\mathsf{Amor}(\bm{x},\bm{y})&,\text{ if }\tau_{\alpha}(\bm{x})=1,\\ \lambda_{h}(\bm{x})\mathsf{Amor}(\bm{x},\bm{y})+(1-\lambda_{h}(\bm{x})) \mathsf{MC}^{n}(\bm{x},\bm{y})&,\text{ if }\tau_{\alpha}(\bm{x})=0.\end{cases}\] (1)

When \(\tau_{\alpha}=0\), selective explanations output _explanations with initial guess_ (Definition 2). Explanations with initial guess linearly combine amortized and Monte Carlo explanations to leverage information from both and provide higher-quality explanations than either explainer alone. Selective explanations heavily depend on three objects that we define in this work and that are covered in the rest of the paper: (i) an uncertainty metric (Section 3), (ii) a selection function (Section 3), and (iii) a combination function (Section 4).

* **Uncertainty metrics** (\(s_{h}\)) output the likelihood of the amortized explainer producing a low-quality explanation for an input. Lower \(s_{h}(\bm{x})\) indicates a higher-quality explanation for \(\bm{x}\). We propose two uncertainty metrics: Deep and Learned Uncertainty (Section 3).
* **Selection function** (\(\tau_{\alpha}\)) is a binary rule that outputs 1 for higher-quality amortized explanations and 0 for lower-quality ones based on the uncertainty metric. We define \(\tau_{\alpha}\) to ensure a fraction \(\alpha\) of inputs receive amortized explanations. Smaller \(\alpha\) implies higher-quality selective explanations but also more computations (Section 3).
* **Combination function** (\(\lambda_{h}\)) optimally linearly combines amortized and Monte Carlo explanations to minimize MSE from target explanations (Theorem 1). We propose explanations with initial guess and fit \(\lambda_{h}\) to optimize explanation quality (Section 4).

Algorithm 1 describes the procedure to compute the uncertainty metric, selection function, and combination function using the results we describe in Section 3 and 4. Although selective explanations can be applied to any feature attribution method, we focus on Shapley values since they are widely used and most amortized explainers are tailored for them [16; 38; 6]. We discuss how selective explanations can be applied to LIME and provide more details on feature attribution methods in Appendix B. Next, we describe specific feature attribution methods that we use as building blocks for selective explainers of the form (1).

**Shapley Values (SHAP)**[22] is a **target** explainer that attributes a value \(\phi_{i}\) for each feature \(x_{i}\) in \(\bm{x}=(x_{1},...,x_{d})\) which is the marginal contribution of feature \(x_{i}\) if the model was to predict \(\bm{y}\)

\[\phi_{i}(\bm{x},\bm{y})=\frac{1}{d}\sum_{S\subset[d]/\{i\}}\left(\begin{matrix}d -1\\ |S|\end{matrix}\right)^{-1}\left(h_{\bm{y}}(\bm{x}_{S\cup\{i\}})-h_{\bm{y}}( \bm{x}_{S})\right).\] (2)

SHAP has several desirable properties and is widely used. However, as (2) indicates, computing Shapley values and the attribution vector \(\mathsf{Target}(\bm{x},\bm{y})=(\phi_{1}(\bm{x},\bm{y}),...,\phi_{d}(\bm{x}, \bm{y}))\) requires \(2^{d}\) inferences from \(h\), making SHAP impractical for large models where inference is costly. This has motivated several approximation methods for SHAP, discussed next.

**Shapley Value Sampling (SVS)**[24] is a **Monte Carlo** explainer that approximates SHAP by restricting the sum in (2) to \(m\) uniformly sampled permutations of features performing \(n=md+1\) inferences. We denote SVS that samples \(m\) feature permutations as SVS-\(m\).

**Kernel Shap (KS)**[22] is a **Monte Carlo** explainer that approximates Shapley values using the fact that SHAP can be computed by solving a weighted linear regression problem using \(n\) input perturbations resulting in \(n\) inferences. We refer to Kernel Shap using \(n\) inferences as KS-\(n\).

**Stochastic Amortization**[6] is an **amortized** explainer that uses noisy Monte Carlo explanations to learn target explanations. Covert et al. [6] trained an amortized explainer in a model class \(\mathcal{F}\) (multilayer perceptrons) \(\mathsf{Amor}\in\mathcal{F}\) to take \((\bm{x},\bm{y})\) and predicts an explanation \(\mathsf{Amor}(\bm{x},\bm{y})\approx\mathsf{Target}(\bm{x},\bm{y})\) by minimizing the \(L_{2}\) norm from Monte Carlo explanations \(\mathsf{MC}^{n}(\bm{x},\bm{y})\). Specifically, the amortized explainer is given by

\[\mathsf{Amor}\in\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{(\bm{x},\bm{y} )\in\mathcal{D}_{\text{train}}}\|f(\bm{x},\bm{y})-\mathsf{MC}^{n}(\bm{x},\bm {y})\|_{2}^{2}.\] (3)

**Amortized Shap for LLMs**[38] is a **amortized** explainer similar to stochastic amortization but tailored for LLMs. Yang et al. [38] train a linear regression on the LLM embeddings \([e_{1}(\bm{x}),...,e_{|\bm{x}|}(\bm{x})]\) to minimize the \(L_{2}\) norm from Monte Carlo explanations \(\mathsf{MC}^{n}(\bm{x},\bm{y})\) and define the amortized explainer as \(\mathsf{Amor}(\bm{x},\bm{y})=(W_{\bm{y}}e_{1}(\bm{x})+b_{\bm{y}},...,W_{\bm{y} }e_{|\bm{x}|}(\bm{x})+b_{\bm{y}}),\) where \(W_{\bm{y}}\) is a matrix and \(b_{\bm{y}}\in\mathbb{R}\).

We use stochastic amortization to produce amortized explainers for tabular datasets and amortized Shap for LLMs to produce explainers for LLM predictions. Both explainers are trained using SVS-12 as \(\mathsf{MC}^{n}\). High-quality and Monte Carlo explanations are computed using the Captum library [18].

## 3 Selecting Explanations

This section defines key concepts for selective explainers: (i) uncertainty metrics \(s_{h}\) for amortized explanations and (ii) selection functions (\(\tau_{\alpha}\)) to predict when amortized explanations closely approximate target explanations based on the uncertainty metrics.

Uncertainty Metrics for High-Dimensional Regression:An uncertainty metric is a function tailored for the model \(h\) that takes \(\bm{x}\) and outputs a real number \(s_{h}(\bm{x})\) that encodes information about the uncertainty of the model \(h\) in the prediction for \(\bm{x}\). Generally, if \(s_{h}(\bm{x})<s_{h}(\bm{x}^{\prime})\) then the model is more confident about the prediction \(h(\bm{x})\) than \(h(\bm{x}^{\prime})\)[10, 28]. Existing uncertainty metrics cater to (i) classification [28, 10, 7, 9, 11] and (ii) one-dimensional regression [39, 34, 11, 17], but none specifically address high-dimensional regression - which is our case of interest (\(d\)-dimensional explanations). Next, we propose two uncertainty metrics tailored to high-dimensional outputs: (i) Deep uncertainty and (ii) Learned uncertainty.

**Deep Uncertainty** is inspired by deep ensembles [19], a method that uses an ensemble of models to provide confidence intervals for the predictions of one model. We run the training pipeline for the amortized explainer described in (3) \(k\) times, each with a different random seed, resulting in \(k\) different amortized explainers \(\mathsf{Amor}^{1},...,\mathsf{Amor}^{k}\). We define the deep uncertainty as

\[s_{h}^{\mathsf{Deep}}(\bm{x})\triangleq\frac{1}{dk}\sum_{i=1}^{d}\mathsf{Var} \left(\mathsf{Amor}^{1}(\bm{x})_{i},...,\mathsf{Amor}^{k}(\bm{x})_{i}\right).\] (4)

Here, \(\mathsf{Var}\left(a_{1},...,a_{k}\right)\) is the variance of the sample \(\{a_{1},...,a_{k}\}\) and \(\mathsf{Amor}^{j}(\bm{x})_{i}\) indicates the \(i\)-th entry of the feature attribution vector \(\mathsf{Amor}^{j}(\bm{x})\). Hence, deep uncertainty is the average (across entries) of the variance (across all trained amortized explainers) for the predicted attributions.

If the deep uncertainty for a point \(\bm{x}\) is zero, then the amortized explainers produce the same feature attribution. On the other hand, if the deep uncertainty is high, then the feature attributions vary widely across the amortized explainers. Intuitively, the points with a higher deep uncertainty are more affected by a random seed change, implying more uncertainty in the explanation.

While the Deep Uncertainty approach offers a principled method for estimating the uncertainty of the amortized explainer by leveraging an ensemble of \(k\) models, it is computationally expensive due to the need for training, serving, and running multiple models. This overhead can be prohibitive in practice, especially for large-scale applications. To mitigate this issue, we propose _Learned Uncertainty_, which, although less grounded, requires training and serving only a single model.

**Learned Uncertainty** uses data to predict the amortized explainer uncertainty at an input point \(\bm{x}\). We choose \(\ell\) (the loss function) between two explanations to be MSE. The learned uncertainty metric is a function in the class \(\mathcal{F}\) (multilayer perceptron in our experiments) such that

\[s_{h}^{\mathsf{Learn}}\in\operatorname*{argmin}_{s\in\mathcal{F}}\sum_{(\bm{ x},\bm{y})\in\mathcal{D}_{\mathsf{train}}}\left|s(\bm{x})-\ell\left(\mathsf{Amor}( \bm{x};\bm{y}),\mathsf{MC}^{n}(\bm{x};\bm{y})\right)\right|^{2}.\] (5)

Ideally, instead of using the Monte Carlo explanation \(\mathsf{MC}^{n}\) as the reference in (5), we would like to use target explanations, i.e., \(\ell\left(\mathsf{Amor}(\bm{x};\bm{y}),\mathsf{Target}(\bm{x};\bm{y})\right)\). However, these computationally expensive explanations are usually not available. Thus, we resort to using Monte Carlo explanations.

For large language models, the textual input \(\bm{x}\) is encoded in a sequence of token embedding \([e_{1}(\bm{x}),...,e_{|\bm{x}|}(\bm{x})]\) such that \(e_{i}(\bm{x})\in\mathbb{R}^{d}\) for \(i\in[|\bm{x}|]\). In this case, we use the mean (i.e., "mean-pooling") of the token embeddings to train the learned uncertainty metric instead of \(\bm{x}\).

We analyze the performance of the proposed uncertainty metrics in Section 5.1, showing that it can be used to detect inaccurate explanations from the amortized explainer. Our results indicate that the proposed uncertainty metrics are (i) strongly correlated with how accurate amortized explanations are and (ii) closely approximate the best possible uncertainty measure - the Oracle with knowledge of the approximation quality (Figure 3). Next, we define the selection function that allows practitioners to set a coverage (percentage of points) \(\alpha\) that will receive amortized explanations.

Selection functions:a selection function is the binary qualifier (\(\tau_{\alpha}\)) that thresholds the uncertainty metric by \(t_{\alpha}\in\mathbb{R}\) given by

\[\tau_{\alpha}(\bm{x})\triangleq\begin{cases}1&\text{if }s_{h}(\bm{x})\leq t_{ \alpha}\ \ \text{(high-quality approximations)}\\ 0&\text{if }s_{h}(\bm{x})>t_{\alpha}\ \ \text{(low-quality approximations)}\end{cases}.\] (6)

Intuitively, \(t_{\alpha}\) is the maximum uncertainty level tolerated by the user. In practice, if the output of the selection function is \(1\) (high-quality approximations), we use the explanations from the amortized model because it is probably close to the target; if the output of the selection function is \(0\) (low-quality approximations), we use explanations with initial guess (see Definition 2 bellow) to improve the explanation provided to the user. The threshold \(t_{\alpha}\) is chosen to be the \(\alpha\)-quantile of the uncertainty metric to ensure that at least a fraction \(\alpha\) of points receive a computationally cheap explanation - \(\alpha\) is the _coverage_. Specifically, given \(\alpha\), we calibrate \(t_{\alpha}\) in the calibration dataset \(\mathcal{D}_{\texttt{cal}}\) and compute it as

\[t_{\alpha}\triangleq\min_{t\in\mathbb{R}}t,\text{ such that }\Pr_{\texttt{cal}}[s_{h}(\bm{x}) \leq t]\geq\alpha,\] (7)

where \(\Pr_{\texttt{cal}}\) is the empirical distribution of the calibration dataset. For discussions on selecting coverage with guarantees on the number of inferences for selective explanations, see Appendix C.

**Remark 2**.: A property of selective predictions [10], which is transferred to selective explanations, is that it is possible to control the explainer's performance via the threshold \(t_{\alpha}\) with guaranteed performance without providing predictions for all points. This result is displayed in Figure 3.

## 4 Explanations with Initial Guess

We have introduced methods to detect points likely to receive amortized explanations that poorly approximate the target. This raises the question: _How can we improve the explanations for these points?_ One approach is to simply use Monte Carlo (MC) explanations instead of amortized ones. However, this ignores potentially valuable information already computed by the amortized explainer. In this section, we propose a more effective solution called _explanations with initial guess_, which combines amortized and Monte Carlo explanations to improve quality.

**Explanation with Initial Guess** uses an optimized linear combination of the amortized explanation with a more computationally expensive method - the Monte Carlo explainer - to improve the quality of the explanation. We formally define _explanations with initial guess_ next.

**Definition 2** (Explanation with Initial Guess).: Given a Monte Carlo explainer \(\mathsf{MC}^{n}(\bm{x},\bm{y})\), and a combination function \(\lambda_{h}:\mathbb{R}^{d}\to\mathbb{R}\) that reflects the quality of the amortized explanation \(\mathsf{Amor}\), we define the explanation with initial guess as

\[\mathsf{IG}(\bm{x},\bm{y})\triangleq\lambda_{h}(\bm{x})\mathsf{ Amor}(\bm{x},\bm{y})+(1-\lambda_{h}(\bm{x}))\mathsf{MC}^{n}(\bm{x},\bm{y}).\] (8)

Recall that when \(\tau_{\alpha}(\bm{x})=0\), selective explanations use the explanation with initial guess (1) to improve low-quality amortized explanations, i.e., \(\mathsf{SE}(\bm{x},\bm{y})=\mathsf{IG}(\bm{x},\bm{y})\).

Defining explanations with initial guess as the linear combination between the amortized and the Monte Carlo explanations is inspired by the literature on shrinkage estimators [21; 20] that use an initial guess (\(\mathsf{Amor}(\bm{x},\bm{y})\) in our case) to improve the estimation MSE in comparison to only using the empirical average (a role played by \(\mathsf{MC}^{h}(\bm{x},\bm{y})\) in our case). Next, we tune \(\lambda_{h}\) to minimize the MSE from target explanations.

**Optimizing the Explanation Quality:** Our goal is for explanations with initial guess to approximate the target explanations, i.e., \(||\mathsf{IG}(\bm{x},\bm{y})-\mathsf{Target}(\bm{x},\bm{y})||\). To achieve this goal, we optimize the function \(\lambda_{h}\) as follows.

First, since \(\mathsf{Target}\) is unavailable, we use another Monte Carlo explanation \(\mathsf{MC}^{n^{\prime}}\) to approximate \(\mathsf{Target}\). \(\mathsf{MC}^{n^{\prime}}\) is different from \(\mathsf{MC}^{n}\) and potentially more computationally expensive but not necessarily. Importantly, \(\mathsf{MC}^{n^{\prime}}\) is only needed beforehand when computing \(\lambda_{h}\), not at prediction time. In our experiments, we use SVS-12 for \(\mathsf{MC}^{n^{\prime}}\).

Second, we quantize the range of the uncertainty metric \(s_{h}\) into bins to aggregate points with similar uncertainty and define the bins \(Q_{i}\) by a partition \(0=\alpha_{1}<\alpha_{2}<...<\alpha_{m}=1\) of \([0,1]\):

\[Q_{i}\triangleq[t_{\alpha_{i}},t_{\alpha_{i+1}}),\ \ \forall i\in[m-1]\] (9)where \(t_{\alpha_{i}}\) is defined as in (7). We then define the combination function to be

\[\lambda_{h}(\bm{x})=\lambda_{i}\text{ if }s_{h}(\bm{x})\in Q_{i},\] (10)

\(\lambda_{h}\) is chosen to optimize the explanation-quality for points with similar uncertainty, \(\lambda_{i}\) is given by:

\[\lambda_{i}\triangleq\operatorname*{argmin}_{\lambda\in\mathbb{R}}\sum_{ \begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{cal}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\Big{|}\Big{|}\big{|}\mathsf{IG}(\bm{x}, \bm{y})-\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})\Big{|}\Big{|}_{2}^{2}.\] (11)

The constant \(\lambda_{i}\) is only computed once per bin and stored. At explanation time, when we provide explanations with initial guess (i.e., when \(\tau_{\alpha}(\bm{x})=0\)) (8), we lookup the bin for the point being explained and use the associated \(\lambda_{i}\).

Theorem 1 provides a closed-form solution for \(\lambda_{i}\).

**Theorem 1** (Optimal \(\lambda_{h}\)).: _Let \(0=\alpha_{1}<\alpha_{2}<...<\alpha_{m}=1\) and define \(Q_{i}\) as in (9). Then the solution to the optimization problem in (11) is given by_

\[\lambda_{i}=\frac{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{cal}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}(\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ MC}^{n^{\prime}}(\bm{x},\bm{y}),\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{Amor}(\bm{x},\bm{y}))}{ \sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{cal}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\left|\left|\mathsf{Amor}(\bm{x},\bm{y})- \mathsf{MC}^{n}(\bm{x},\bm{y})\right|\right|_{2}^{2}}.\] (12)

The range of uncertainty functions is **quantized** for two main reasons. First, the uncertainty metric \(s_{h}\) encodes the amortized explainer's uncertainty for each point \(\bm{x}\). This uncertainty quantification should be reflected in the choice of \(\lambda_{h}\). Quantizing the range of \(s_{h}\) allows us to group points with similar uncertainty levels and optimize \(\lambda_{h}\) for each group separately. Second, quantizing the range of \(s_{h}\) enables us to have multiple point per bin \(Q_{i}\) allowing us to compute \(\lambda_{i}\) to minimize the MSE in each bin.

We use the **Monte Carlo** explainer \(\mathsf{MC}^{n^{\prime}}\) because: (i) as mentioned above, we assume we don't have access to target explanations due to computational cost and (ii) even when using this Monte Carlo explainer, we show that in all bins, \(\lambda_{i}\) approximates well the optimal combination function computed assuming access to target explanations from Target defined as

\[\lambda_{i}^{\text{opt}}=\operatorname*{argmin}_{\lambda\in[0,1]}\sum_{ \begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{cal}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\left|\left|\mathsf{IG}(\bm{x},\bm{y})- \mathsf{Target}(\bm{x},\bm{y})\right|\right|_{2}^{2}.\]

Specifically, Theorem 2 shows that \(\lambda_{i}\approx\lambda_{i}^{\text{opt}}\) with high probability. Appendix E shows the formal version of the Theorem along with the proofs for all results in this section.

**Theorem 2** (**Informal \(\lambda_{i}\approx\lambda_{i}^{\text{opt}}\)).: _If (i) \(\mathsf{MC}^{n}\) is sufficiently different from the amortized explainer Amor and (ii) \(\mathsf{MC}^{n^{\prime}}\) approximates the target explanations Target then \(\lambda_{i}\) and \(\lambda_{i}^{\text{opt}}\) are close with high-probability for all bins \(Q_{i}\), i.e.,_

\[|\lambda_{i}-\lambda_{i}^{\text{opt}}|\leq\epsilon\text{ with probability at least }1-e^{-C|Q_{i}|}.\]

_for a \(C>0\) and \(|Q_{i}|\) is the number of points in the validation dataset \(\mathcal{D}_{\text{cal}}\) that are in bin \(Q_{i}\)._

## 5 Experimental Results

This section analyzes the performance of selective explanations and its different components (i) uncertainty measures and (ii) explanations with initial guess. All results are showed in terms of MSE from target explanations, check Appendix D for the same results using Spearman's Rank Correlation.

Experimental Setup:We generate selective explanations and evaluate their MSE and Spearman's correlation compared to the target explanation computed using a large number of inferences1. Although our results hold for any feature and data attribution method, in this section, we focus on Shapley values due to its frequent use and prevalence in the literature on amortized explainers [16; 6; 38]. Seaborn [36] is used to compute \(95\%\) confidence intervals using the bootstrap method.

[MISSING_PAGE_FAIL:8]

Explanations with Initial Guess vs. Monte CarloIn Figure 4 we compare selective explanations improving quality of non-covered points using (i) explanations with initial guess and (ii) Monte Carlo explanations, when amortized explanations are inaccurate (\(\lambda_{h}=0\)). **Case 1:** When the MSE from the Monte Carlo is smaller than from the amortized explainer ((a) and (c)), explanations with initial guess results in a smaller MSE compared to only using Monte Carlo. **Case 2:** When the MSE in Monte Carlo is larger than the amortized explanation MSE ((b) and (d)), only using Monte Carlo increase the MSE while explanations with initial guess reduces the MSE. Together, Cases 1 and 2 suggest that even when lower quality, explanations contain valuable information that can be leveraged by explanations with initial guess to improve explanation quality.

### Efficacy of Selective Explanations

Worst Case Performance Improvement:Figure 5 shows the MSE of selective explanations for the points receiving the highest MSEs. The figure suggests that selective explanations significantly decrease the worst-case MSE of amortized explanations. With just 20% coverage the MSE decreases consistently across datasets. Remarkably, when providing explanations with initial guess for \(20\%\) of the samples in the Yelp dataset (Figure 5 (c)), selective explanations result in MSE for the worst \(5\%\) of points that is about \(30\%\) smaller than the original amortized explanations - this is even more pronounced in the UCI datasets.

Improved Inferences vs. Quality Trade-off:Figure 6 presents the trade-off between number of inferences per explanation and MSE from target explanations using selective and Monte Carlo explanations. The MSE decreases with the number of inferences and selective explanations Pareto dominates Monte Carlo explanations. We also show an "Oracle" that knows a priori how to optimally

Figure 4: Fraction (\(1-\alpha\)) of points which explanations receive additional computations (x-axis) vs. MSE of selective explanations w.r.t. target explanations (y-axis) with coverage \(\alpha\). Naive uses \(\lambda_{h}=0\) while Initial guess uses \(\lambda_{h}\) in (12). MSE is computed across all points in the test dataset. Yelp Review and Toxigen use SVS (12) as Monte Carlo explanations while UCI-Adult and UCI-News use KS (32).

Figure 5: MSE for the 5% of explanations with the highest MSE in the test dataset (y-axis) for selective explanations with varying fraction of points with extra computations (x-axis). Selective explanations are shown in (i) black solid bar using the Learned uncertainty and (ii) striped black bar using Deep uncertainty. Dashed red line shows the MSE of amortized explanations in worst 5% explanations.

route samples in terms of MSE and inferences. We simulate this oracle by pre-computing SVS explanations with parameters 12, 25, and 50, and selecting the one with the smallest MSE from the target SHAP explanation while mantaining the average number inferences shown in x-axis.

Improved Local Fidelity:Figure 7 shows that selective explanations increase the local fidelity of the amortized explainer and that local fidelity increases with the fraction of points that receive additional computations (recourse). Both Yelp and Toxigen models receive explanations with initial guess using SVS-12.

## 6 Final Remarks

Conclusion:We propose _selective explanations_ that first identify which inputs would receive a low-quality but computationally cheap explanation (amortized) and then perform additional model inferences to improve the quality of these explanations. We propose _explanations with initial guess_ to improve the quality of explanations by combining amortized explanations with more expensive explanations Monte Carlo using an optimized combination function, improving the explanation performance. Selective explanations provide a new framework for approximating expensive feature attribution methods. Our experiments indicate that selective explanations (i) efficiently identify points that the amortized explainer would produce low-quality explanations, (ii) improves the quality of the worst-quality amortized explanations, (iii) improves the trade-off between computational cost and explanation quality, and (iv) improves the local fidelity of amortized explanations.

**Limitations:** Selective explanations can be applied to any feature attribution method for which amortized and Monte Carlo explainers were developed. However, our empirical results focus on Shapley values. We leave the application of selective explanations to other attribution methods for future work. Additionally, we do not explore image classifiers, which may also interest the interpretability community. Also, we do not explore selective explanations for Generative Language models due to the lack of amortized explainers for such application.

Figure 6: Number of inferences (x-axis) vs. MSE (y-axis). Black curve shows the performance of selective explanations using Learned uncertainty. Purple curve connects Shapley Value Sampling (SVS) with parameters 12, 25, and 50 sequentially until all samples receive SVS-50 explanations and amortized explanations. The red curve is a the Oracle that optimally trades off MSE and inferences.

Figure 7: Model accuracy (y-axis) when removing the tokens with the highest attribution scores according to the amortized explainer (black), selective explanations with varying coverage and target explanations (red).

Acknowledgments

The authors thank Amit Dhurandhar for early discussions on the trustworthiness of amortized explainers. This material is based upon work supported by the National Science Foundation under awards CAREER-1845852, CIF-1900750, CIF-2231707, and CIF-2312667, FAI-2040880, and also an Apple Scholar Fellowship. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.

## References

* Becker and Kohavi [1996] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.
* Chen and Jordan [2019] Jianbo Chen and Michael I. Jordan. Ls-tree: Model interpretation when the data are linguistic. _ArXiv_, abs/1902.04187, 2019. URL https://api.semanticscholar.org/CorpusID:60441455.
* Chuang et al. [2023] Yu-Neng Chuang, Guanchu Wang, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting Cai, and Xia Hu. Cortx: Contrastive framework for real-time explanation. _ArXiv_, abs/2303.02794, 2023. URL https://api.semanticscholar.org/CorpusID:257365448.
* Covert and Lee [2020] Ian Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation via linear regression. In _International Conference on Artificial Intelligence and Statistics_, 2020. URL https://api.semanticscholar.org/CorpusID:227253750.
* Covert and Lee [2021] Ian Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation using linear regression. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 3457-3465. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/covert21a.html.
* Covert et al. [2024] Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, and Tatsunori Hashimoto. Stochastic amortization: A unified approach to accelerate feature and data attribution, 2024.
* El-Yaniv et al. [2010] Ran El-Yaniv et al. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(5), 2010.
* Fernandes et al. [2015] Kelvin Fernandes, Pedro Vinagre, Paulo Cortez, and Pedro Sernadela. Online News Popularity. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C5NS3V.
* Gangrade et al. [2021] Aditya Gangrade, Anil Kag, and Venkatesh Saligrama. Selective classification via one-sided prediction. In _International Conference on Artificial Intelligence and Statistics_, pages 2179-2187. PMLR, 2021.
* Geifman and El-Yaniv [2017] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf.
* Geifman and El-Yaniv [2019] Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject option. In _International conference on machine learning_, pages 2151-2159. PMLR, 2019.
* explainable job recommendations using llms, 2023.
* Ghosh and Khandoker [2024] Samit Kumar Ghosh and Ahsan H Khandoker. Investigation on explainable machine learning models to predict chronic kidney diseases. _Scientific Reports_, 14(3687), 2024. doi: 10.1038/s41598-024-54375-4. URL https://doi.org/10.1038/s41598-024-54375-4.

* [14] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3309-3326, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.234. URL https://aclanthology.org/2022.acl-long.234.
* [15] Simon Haykin. _Neural networks: a comprehensive foundation_. Prentice Hall PTR, 1994.
* [16] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. FastSHAP: Real-time shapley value estimation. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Zq2G_VTV53T.
* [17] Wenming Jiang, Ying Zhao, and Zehan Wang. Risk-controlled selective prediction for regression deep neural network models. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2020.
* [18] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum: A unified and generic model interpretability library for pytorch, 2020.
* [19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [20] HH Lemmer. Note on shrinkage estimators for the binomial distribution. _Communications in statistics-theory and methods_, 10(10):1017-1027, 1981.
* [21] HH Lemmer. From ordinary to bayesian shrinkage estimators. _South African Statistical Journal_, 15(1):57-72, 1981.
* [22] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30_, pages 4765-4774. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf.
* [23] Vivek Miglani, Aobo Yang, Aram H. Markosyan, Diego Garcia-Olano, and Narine Kokhlikyan. Using Captum to explain generative language models, 2023.
* [24] Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley value estimation. _Journal of Machine Learning Research_, 23(43):1-46, 2022. URL http://jmlr.org/papers/v23/21-0439.html.
* [25] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 119-126, 2020.
* [26] OpenAI. Using gpt-4 for content moderation. https://openai.com/index/using-gpt-4-for-content-moderation. Accessed: 2024-05-01.
* [27] Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, and Soumya Ghosh. Multi-level explanations for generative language models. _arXiv:2403.14459_, 2024. URL https://arxiv.org/abs/2403.14459.
* [28] Stephan Rabanser, Anvith Thudi, Kimia Hamidieh, Adam Dziedzic, and Nicolas Papernot. Selective classification via neural network training dynamics. _ArXiv_, abs/2205.13532, 2022. URL https://api.semanticscholar.org/CorpusID:249097456.

* Ribeiro et al. [2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?": Explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016_, pages 1135-1144, 2016.
* Rong et al. [2023] Yao Rong, Tobias Leemann, Thai-Trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel, Gjergji Kasneci, and Enkelejda Kasneci. Towards human-centered explainable ai: A survey of user studies for model explanations. _IEEE Trans. Pattern Anal. Mach. Intell._, 46(4):2104-2122, November 2023. ISSN 0162-8828. doi: 10.1109/TPAMI.2023.3331846. URL https://doi.org/10.1109/TPAMI.2023.3331846.
* Salih et al. [2023] Ahmed M. A. Salih, Zahra Raisi-Estabragh, Ilaria Boscolo Galazzo, Petia Radeva, Steffen E. Petersen, Karim Lekadir, and Gloria Menegaz. A perspective on explainable artificial intelligence methods: Shap and lime. _Advanced Intelligent Systems_, 2023. URL https://api.semanticscholar.org/CorpusID:258461143.
* Schwab and Karlen [2019] Patrick Schwab and Walter Karlen. Cxplain: Causal explanations for model interpretation under uncertainty. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Paper.pdf.
* Schwarzenberg et al. [2021] Robert Schwarzenberg, Nils Feldhus, and Sebastian Moller. Efficient explanations from empirical explainers. In Jasmijn Bastings, Yonatan Belinkov, Emmanuel Dupoux, Mario Giulianelli, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, _Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pages 240-249, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.blackboxnlp-1.17. URL https://aclanthology.org/2021.blackboxnlp-1.17.
* Shah et al. [2022] Abhin Shah, Yuheng Bu, Joshua K Lee, Subhro Das, Rameswar Panda, Prasanna Sattigeri, and Gregory W Wornell. Selective regression under fairness criteria. In _International Conference on Machine Learning_, pages 19598-19615. PMLR, 2022.
* Wang et al. [2023] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large language models, 2023.
* Waskom [2021] Michael L. Waskom. seaborn: statistical data visualization. _Journal of Open Source Software_, 6(60):3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.
* Weerts et al. [2019] Hilde J. P. Weerts, Werner van Ipenburg, and Mykola Pechenizkiy. A human-grounded evaluation of shap for alert processing. _ArXiv_, abs/1907.03324, 2019. URL https://api.semanticscholar.org/CorpusID:195833476.
* Yang et al. [2023] Chenghao Yang, Fan Yin, He He, Kai-Wei Chang, Xiaofei Ma, and Bing Xiang. Efficient shapley values estimation by amortization for text classification. In _Annual Meeting of the Association for Computational Linguistics_, 2023. URL https://api.semanticscholar.org/CorpusID:258987882.
* Zaoui et al. [2020] Ahmed Zaoui, Christophe Denis, and Mohamed Hebiri. Regression with reject option and application to knn. _Advances in Neural Information Processing Systems_, 33:20073-20082, 2020.
* ECCV 2014_, pages 818-833, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10590-1.
* Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.

Overview

In this supplementary material we provide the following information:

* Appendix B discuss other high-quality and Monte Carlo explainers.
* Appendix C discuss a guide to select the coverage \(\alpha\) when the agent providing selective explanations has a budget for the average number of inferences to provide an explanation.
* Appendix D shows more experimental results on selective explanations.
* Appendix E shows the proofs for the theoretical results in Section 4.

## Appendix B Additional Explanation Methods

In this section, we describe high-quality, Monte Carlo, and amortized explainers with further details.

### High-Quality Explainers

**Shapley Values (SHAP)**[22] is a **high-quality** explainer that attributes a value \(\phi_{i}\) for each feature \(x_{i}\) in \(\bm{x}=(x_{1},...,x_{d})\) which is the marginal contribution of feature \(x_{i}\) if the model was to predict \(\bm{y}\) (2).

\[\phi_{i}(\bm{x},\bm{y})=\frac{1}{d}\sum_{S\subset[d]/\{i\}}\left(\begin{matrix} d-1\\ |S|\end{matrix}\right)^{-1}\left(h_{\bm{y}}(\bm{x}_{S\cup\{i\}})-h_{\bm{y}}(\bm{x }_{S})\right).\] (13)

SHAP has several desirable properties and is widely used. However, as (2) indicates, computing Shapley values and the attribution vector \(\mathsf{Target}(\bm{x},\bm{y})=(\phi_{1}(\bm{x},\bm{y}),...,\phi_{d}(\bm{x}, \bm{y}))\) requires \(2^{d}\) inferences from \(h\), making SHAP impractical for large models where inference is costly. This has motivated several approximation methods for SHAP, discussed next4.

Footnote 4: We also discuss Lime and its amortized version in Appendix B

Local Interpretable Explanations (Lime).Lime is another feature attribution method [29] widely used to provide feature attributions. It relies on selecting combinations of features, removing these features from the input to generate perturbations, and using these perturbations to approximate the black box model \(h\) locally by a linear model. The coefficients of the linear model are considered to be the attribution of each feature. Formally, given a weighting kernel \(\pi(S)\) and a penalty function \(\Omega\), the attribution produced by lime are given by

\[(\phi,a)=\operatorname*{argmin}_{\phi\in\mathbb{R}^{d},a\in\mathbb{R}}\sum_{S \subset[d]}\pi(S)\left(h(\bm{x}_{S})-a_{0}-\sum_{i\in S}\phi_{i}\right),\] (14)

where \(\mathsf{Target}(\bm{x},\bm{y})=\phi\). As in SHAP, to compute the feature attributions using lime, we need to perform a large number of model inferences, which is prohibitive for large models.

### Monte Carlo Lime

**Shapley Value Sampling (SVS)**[24] is a **Monte Carlo** explainer that approximates SHAP by restricting the sum in (2) to specific permutations of feature. SVS computes the attribution scores by uniformly sampling \(m\) features permutations \(S_{1},...,S_{m}\) restricting the sum in (2) and performing \(n=md+1\) inferences. We denote SVS that samples \(m\) feature permutations by SVS-\(m\).

**Kernel Shap (KS)**[22] is a **Monte Carlo** explainer that approximate the Shapley values using the fact that SHAP can be computed by solving the optimization problem

\[(\phi,a)=\operatorname*{argmin}_{\phi\in\mathbb{R}^{d},a\in\mathbb{R}}\sum_{i =1}^{n}\pi(S_{i})\left(h(\bm{x}_{S_{i}})-a_{0}-\sum_{j\in S_{i}}\phi_{j} \right),\] (15)

using \(\pi(S)=\binom{d}{|S|}|S|(d-|S|)\) and where \(\mathsf{MC}^{n}(\bm{x},\bm{y})=\phi\). Kernel Shap samples \(n>0\) feature combinations \(S_{1},...,S_{n}\) and define the feature attributions to be given by the coefficients \(\phi\). We refer to Kernel Shap using \(n\) inferences as KS-\(n\). We use the KS-\(n\) from the Captum library [18] for our experiments.

Sample Constrained Lime.To approximate the attributions from Lime, we consider the sample-contained version of (15). Instead of sampling all feature combinations in \([d]\), we only uniformly sample a fixed number \(n\) of feature combinations \(S_{1},...,S_{n}\). For our experiments, shown in the appendix, we use the Sample Constrained Lime from the Captum library [18].

### Amortized Explainers

**Stochastic Amortization**[6] is a **Amortized** explainer that uses noisy Monte Carlo explanations to learn high-quality explanations. Covert et al. [6] trained an amortized explainer \(\mathsf{Amor}\in\mathcal{F}\) in a hypothesis class \(\mathcal{F}\) (we use multilayer perceptrons) that takes an input and predicts an explanation. Specifically, taking the amortized explainer to be the solution of the training problem given in (3).

\[\mathsf{Amor}\in\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{(\bm{x},\bm{y} )\in\mathcal{D}_{\text{train}}}\|f(\bm{x},\bm{y})-\mathsf{MC}^{n}(\bm{x},\bm {y})\|_{2}^{2}.\] (16)

We are interested in explaining the predictions of large models for text classification. However, the approach in (3) is only suitable for numerical inputs. Hence, we follow the approach from Yang et al. [38] to explain the predictions of large language models, explained next.

**Amortized Shap for LLMs**[38] is a **Amortized** explainer similar to the one in (3) but tailored for LLMs. First, the authors note that they can use the LLM to write all input texts \(\bm{x}\) as a sequence of token embedding \([e_{1}(\bm{x}),...,e_{|\bm{x}|}(\bm{x})]\) where \(e_{i}(\bm{x})\in\mathbb{R}^{d}\) denotes the LLM embedding for the \(i\)-th token contained in the input text \(\bm{x}\) and \(|\bm{x}|\) is the number of tokens in the input text. Second, they restrict \(\mathcal{F}\) in (3) to be the set of all linear regressions that take the token embeddings and output the token attribution score. Then, they solve the optimization problem in

\[W\in\operatorname*{argmin}_{W\in\mathbb{R}^{d},b\in\mathbb{R}}\sum_{(\bm{x}, \bm{y})\in\mathcal{D}_{\text{train}}}\sum_{j=1}^{|\bm{x}|}\|W^{T}e_{j}(\bm{x} )+b-\mathsf{MC}^{n}(\bm{x},\bm{y})_{j}\|_{2}^{2},\] (17)

and define the amortized explainer as \(\mathsf{Amor}(\bm{x})=(W^{T}e_{1}(\bm{x})+b,...,W^{T}e_{|\bm{x}|}(\bm{x})+b)\).

We use stochastic amortization to produce amortized explainers for tabular datasets and Amortized Shap for LLMs to produce explainers for LLM predictions. Both explainers are trained using SVS-12 as \(\mathsf{MC}^{n}\).

## Appendix C Selecting Coverage for a Given Inference Budget

Determining Coverage from Inference Budget:Providing explanations with initial guess increases the number of model inferences from 1 when using solely the amortized explainer to \(n+1\). However, a practitioner may have a budget of inferences, i.e., a maximum average number of inferences they are willing to perform to provide an explanation. We formalize the notion of inference budget in Definition 3.

**Definition 3** (Inference Budget).: Denote by \(\operatorname{N}(\mathsf{SE}(\bm{x},\bm{y}))\) the number of model inferences to produce the explanation \(\mathsf{SE}(\bm{x},\bm{y})\). The inference budget \(\operatorname{N}_{\texttt{budget}}\in\mathbb{N}\) is the maximum average number of inferences a practitioner is willing to perform per explanation, i.e., it is such that

\[\operatorname{N}_{\texttt{budget}}\geq\mathbb{E}\left[\operatorname{N}( \mathsf{SE}(\bm{x},\bm{y}))\right].\] (18)

Once an inference budget \(\operatorname{N}_{\texttt{budget}}\) is defined, the coverage \(\alpha\) should be set to follow it. In Proposition 1, we show the minimum coverage for the selective explanations to follow the inference budget.

**Proposition 1** (Coverage for Inference Budget).: _Let \(\operatorname{\textit{N}}_{\texttt{budget}}\geq 1\) be the inference budget, and assume that the Monte Carlo method \(\mathsf{MC}^{n}(\bm{x},\bm{y})\) uses \(n\) model inferences. Then, the coverage level \(\alpha\) should be chosen such that_

\[\frac{n+1-\operatorname{\textit{N}}_{\texttt{budget}}}{n}=\min_{\alpha\in[0, 1]}\alpha,\text{ such that }\mathbb{E}\left[\operatorname{\textit{N}}(\mathsf{SE}(\bm{x},\bm{y}))\right] \leq\operatorname{\textit{N}}_{\texttt{budget}}.\] (19)

_Recall that SVS-\(m\) performs \(n=1+dm\) inferences (\(\bm{x}\in\mathbb{R}^{d}\)), and KS-\(m\) performs \(n=m\) inferences._More Experimental Results

In this section, we (i) give further implementation details and (ii) discuss further empirical results.

### More Details on Experimental Setup

High-Quality Explanations:We define the high-quality explanations for the tabular datasets to be given by Kernel Shap with as many inferences as needed for convergence, using the Shapley Regression library [4]. For the textual dataset, following [38], we define the high-quality explanations to be given by Kernel Shap using \(8912\) model inferences per explanation.

Amortized Explainers:For the tabular datasets, we use the amortized explainer from [6] that we describe in Section 2. Specifically, we use a multilayer perceptron model architecture to learn the shapley values for the tabular datasets. For the textual datasets, we use the linear regression on token-level textual embeddings to learn the shapley values, as described in Section 2. Both amortized models learn from the training dataset of explanations generated using Shapley Value Sampling from the Captum library [18] with parameter \(12\), i.e., SVS-12.

Uncertainty Metrics:We test the two proposed uncertainty metrics in Section 3, namely, deep uncertainty and uncertainty learn. For **deep uncertainty**, we run the training pipeline for the amortized explainers 20 times for each dataset we perform experiments on, resulting in 20 different amortized explainer that we use to compute (4). For **uncertainty learn**, we use the multilayer perceptron as the hypothesis class with only one hidden layer. The hidden layer was composed of \(\kappa=3d\) neurons where \(d\) is the dimension of the input vector \(\bm{x}\in\mathbb{R}^{d}\). The uncertainty learn metric was trained on \(\mathcal{D}_{\texttt{train}}\), the same training dataset as the amortized explainers.

Dataset sizes:We use 4000 samples from each dataset due to computational limitations on the computation of high-quality explanations used to evaluate selective explanations. All explanations were computed using the Captum library [18]. The dataset \(\mathcal{D}\) with \(N=4000\) samples was partitioned in three parts, \(\mathcal{D}_{\texttt{train}}\) with \(50\%\) of points, \(\mathcal{D}_{\texttt{cal}}\) with \(25\%\) of points, and \(\mathcal{D}_{\texttt{test}}\) with the other \(25\%\) of points.

Computational Resources:All experiments were run in a A100 40 GB GPU. For each dataset, we compute different Monte Carlo explanations. For the UCI-News dataset, the high quality explanations took 4:30 hours to be generate until convergence while for UCI-Adult it took 3:46 hours. For the tabular datasets, all other Monte Carlo explainers were generated in less than 1 hour. For the language models, the high-quality explanations with 8192 model inferences, took 18:51 hours for the Toxigen dataset and 20:00 hours for the Yelp Review datasets. The other used Monte Carlo explanations took proportional (to the number of inferences) time to be generated.

### Uncertainty Measures Impact on Spearman's Correlation

Figure 8 shows in the x-axis the coverage (\(\alpha\)) and in the y-axis the average Spearman's correlation of the selected amortized explanations from high-quality explanations using deep uncertainty (with 20 models) and the uncertainty learn to select low-quality explanations. The Oracle5 is computed by sorting examples by the smallest to higher MSE and computing the average Spearman's correlation in the bottom x-axis points accordingly to the MSE and is the best that can be done in terms of MSE.

Footnote 5: The oracle is computationally expensive because it requires access to high-quality explanations.

Figure 8 shows that the Oracle and proposed uncertainty metrics don't always select the points with the smallest Spearman's correlation first. This implies that MSE and Spearman's correlation don't always align, i.e., there are points with high MSE and high Spearman's correlation at the same time. However, we note that the uncertainty learns selector can be applied to **any** metric \(\ell\) as we define in (5) including Spearman's correlation and any combination of Spearman's correlation and MSE aiming to approximate both metrics. Moreover, when the smallest MSE aligns with the highest Spearman's correlation, i.e., the oracle is decreasing in Spearman's correlation when the coverage increases (Figure 8 (a) and (c)), the proposed uncertainty metrics also accurately detect the low-quality explanations in term of Spearman's correlation.

### The Effect of Explanations with Initial Guess

In Figure 9 we compare explanations with initial guess (Definition 2) to only using the Monte Carlo to provide recourse to the low-quality explanaitons, i.e., \(\lambda_{h}=0\) we call it Naive. In all tested cases, Spearman's correlation of the Monte Carlo method is comparable to or larger than the amortized explainer. Although selective explanations optimized for MSE by using explanations with initial guess (Definition 2), we observe that the Spearman's correlation of selective explanations is close to or larger than the naive method, once again, demonstrating the efficacy of selective explanations.

### Spearman's correlation Explanation of initial guess in the worst explanations

Figure 10 shows the Spearman's rank correlation of selective explanations for the points receiving explanations with the smallest correlation. The figure shows that selective explanations significantly decrease the worst-case Spearman's rank correlation of amortized explanations. With just 20% coverage the Spearman's rank correlation increases consistently across datasets. Remarkably, when providing explanations with initial guess for \(50\%\) of the samples in the Yelp dataset (Figure 10 (c)), selective explanations result in explanations with positive correlation with target explanations in the worst \(5\%\). At the same worst 5% of points, amortized explanations are negatively correlated with target explanations.

### Performance for Different Monte-Carlo Explainers

Figure 11 shows how the MSE and Spearman's correlation behave accordingly with the quality of the Monte Carlo explainer. We compare Kernel Shap and Shapley Value Sampling in all experiments. We observe that when the quality of the Monte Carlo explainer increases, the quality of the Selective explanation also increases, i.e., the MSE decreases and the Spearman's correlation increases. Moreover,

Figure 8: Coverage vs. Spearman’s correlation from the high-quality explanation. Coverage is the percentage of the points that the selection function predicts that will receive a higher-quality explanation, i.e., \(\tau_{t}(\bm{x})=1\). When coverage is \(100\%\) Spearman’s correlation is the average performance for the amortized explainer.

Figure 9: Fraction of the population that receive explanations with initial guess (x-axis) vs. their Spearman’s correlation from the high-quality explanations (y-axis). Naive uses \(\lambda_{h}=0\) while initial guess uses explanations with initial guess, i.e., when \(\lambda_{h}\) is given in (12).

we also observe diminishing returns, i.e., after a certain point, increasing the quality of the Monte Carlo explanations doesn't lead to a tailored increase in performance. For example, observe the SVS method in the tabular datasets Figure 11 (a) and (b). We also observe that providing explanations with initial guess has a high impact on both Spearman's correlation and MSE when only providing recourse too small fraction of the population. For example, when providing explanations with initial guess for \(20\%\) of the population using SVS-12 in the Yelp Review dataset, Figure 11 (c), increases the Spearman's correlation in more than \(50\%\) (from 0.2 to more than 0.3).

### Time Sharing Using Selective Explanations

Figure 6 presents the trade-off between number of inferences per explanation and MSE from target explanations using selective and Monte Carlo explanations. In addition to what is shown in Figure 6, we also show selective explanations using the Deep uncertainty metric as a selector.

Figure 11: MSE (top) and Spearman’s correlation (bottom) for selective explanations using different Monte Carlo explainers.

Figure 10: Spearman’s correlation for the 5% of explanations with the smallest correlation in the test dataset (y-axis) for selective explanations with varying fraction of points with extra computations (x-axis). Selective explanations are shown in (i) black solid bar using the Learned uncertainty and (ii) striped black bar using Deep uncertainty. Dashed red line shows the MSE of amortized explanations in worst 5% explanations.

## Appendix E Proofs of Theoretical Results

**Theorem 1** (Optimal \(\lambda_{h}\)).: _Let \(0=\alpha_{1}<\alpha_{2}<...<\alpha_{m}=1\) and define \(Q_{i}\) as in (9). Then, \(\lambda_{i}\) that solves the optimization problem in (11) is given by_

\[\begin{split}\sum_{(\bm{x},\bm{y})\in\mathcal{D}_{\text{val}}}( \text{{MC}}^{n}(\bm{x},\bm{y})-\text{{MC}}^{n^{\prime}}(\bm{x},\bm{y}),\text{{ MC}}^{n}(\bm{x},\bm{y})-\text{{Amor}}(\bm{x},\bm{y}))\\ \lambda_{i}=\frac{\sum_{(\bm{x},\bm{y})\in\mathcal{D}_{\text{val} }}\left\|\text{{Amor}}(\bm{x},\bm{y})-\text{{MC}}^{n}(\bm{x},\bm{y})\right\|_ {2}^{2}}{\sum_{s_{h}(\bm{x})\in Q_{i}}\left\|\text{{Amor}}(\bm{x},\bm{y})- \text{{MC}}^{n}(\bm{x},\bm{y})\right\|_{2}^{2}}.\end{split}\] (20)

Proof.: First, recall that

\[\lambda_{i} \triangleq\underset{\lambda\in\mathbb{R}}{\text{argmin}}\sum_{ \begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\left|\left|\text{{SE}}(\bm{x},\bm{y})- \text{{MC}}^{n^{\prime}}(\bm{x},\bm{y})\right|\right|_{2}^{2}\] (21) \[=\underset{\lambda\in\mathbb{R}}{\text{argmin}}\sum_{\begin{subarray} {c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\left|\left|\lambda\text{{Amor}}(\bm{x}, \bm{y})+(1-\lambda)\text{{MC}}^{n}(\bm{x},\bm{y})-\text{{MC}}^{n^{\prime}}( \bm{x},\bm{y})\right|\right|_{2}^{2}.\] (22)

Figure 12: Number of inferences (x-axis) vs. MSE (y-axis). Black curve shows the performance of selective explanations using Learned uncertainty. Purple curve connects Shapley Value Sampling (SVS) with parameters 12, 25, and 50 sequentially until all samples receive SVS-50 explanations and amortized explanations. The red curve is a the Oracle that optimally trades off MSE and inferencesNote that the function in (22) is convex in \(\lambda\); therefore, if the derivative of it with respect to \(\lambda\) is zero, then the lambda that achieves the zero gradient is the minima. So, let's derivate (22) to find \(\lambda_{i}\).

\[0 =\frac{d}{d\lambda}\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\left|\left|\lambda\mathsf{Amor}(\bm{x}, \bm{y})+(1-\lambda)\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{MC}^{n^{\prime}}(\bm {x},\bm{y})\right|\right|_{2}^{2}\] (23) \[=2\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{ val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\lambda||\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{Amor}(\bm{x},\bm{y})||^{2}\] (24) \[-2\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{ val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\langle\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y}),\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{Amor}(\bm{x},\bm{y})\rangle\] (25)

From (25) we conclude the proof by showing that

\[\lambda_{i}=\lambda=\frac{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}\langle\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y}),\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{Amor}(\bm{x},\bm{y})\rangle}{\sum_{\begin{subarray}{c}(\bm{x},\bm{y} )\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}}.\] (26)

**Theorem 2** (\(\lambda_{i}\approx\lambda_{i}^{\text{opt}}\)).: _Let the Monte Carlo explanation used to provide recourse \(\mathsf{MC}^{n}\) to be different enough from the amortized explainer, i.e., \(\mathbb{E}\left[||\mathsf{MC}^{n}(X,Y)-\mathsf{Amor}(X,Y)||^{2}\right]=\mu>0\). Also, assume that \(\mathsf{MC}^{n^{\prime}}\) is a good Monte Carlo approximation for the high-quality explainer Target, i.e., \(\mathbb{E}\left[||\mathsf{MC}^{n^{\prime}}(X,Y)-\mathsf{Target}(X,Y)||^{2} \right]=\mu^{*}\) for \(\epsilon>\frac{\sqrt{5\mu^{*}}}{\mu}\). Recall that \(\bm{x}\in\mathbb{R}^{d}\). If the explanations are bounded, i.e., \(||\mathsf{MC}^{n}(\bm{x},\bm{y})||,||\mathsf{Amor}(\bm{x},\bm{y})||,||\mathsf{ Target}(\bm{x},\bm{y})|<Cd\) for some \(C>0\) then_

\[\Pr[\lambda_{i}-\lambda_{i}^{\text{opt}}|>\epsilon]\leq\frac{-\mu^{2}[Q_{i}] }{4C^{d}}+\frac{-\mu^{4}\epsilon^{4}[Q_{i}]}{400C^{d}},\] (27)

_where \(|Q_{i}|\) is the number of points \(\bm{x}\) in the validation dataset \(\mathcal{D}_{\text{val}}\) that are in the bin \(Q_{i}\)._

Proof.: Denote \(|Q_{i}|=|\{(\bm{x},\bm{y})\in\mathcal{D}_{\text{val}},\text{ s.t. }s_{h}(\bm{x})\in Q_{i}\}|\).

We start by showing that if \(\mathbb{E}\left[||\mathsf{MC}^{n}(X,Y)-\mathsf{Amor}(X,Y)||^{2}\right]=\mu\) then

\[\Pr\left[\frac{1}{|Q_{i}|}\sum_{\begin{subarray}{c}(\bm{x},\bm{y })\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}\leq\frac{\mu}{2}\right]\] (28) \[= \Pr\left[\mu-\frac{1}{|Q_{i}|}\sum_{\begin{subarray}{c}(\bm{x}, \bm{y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}\geq\frac{\mu}{2}\right]\] (29) \[\leq e^{\frac{-\mu^{2}[Q_{i}]}{4C^{d}}}.\] (30)

Where the inequality in (30) follows from Hoeffding's inequality and the fact that:

\[||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{Amor}(\bm{x},\bm{y})||^{2}\leq|| \mathsf{MC}^{n}(\bm{x},\bm{y})||+||\mathsf{Amor}(\bm{x},\bm{y})||\leq 2Cd.\] (31)

Second, we recall that \(\mathbb{E}\left[||\mathsf{MC}^{n^{\prime}}(X,Y)-\mathsf{Target}(X,Y)||^{2} \right]=\mu^{*}\leq\frac{\mu^{2}\epsilon^{2}}{5}\). Then, we have that

\[\Pr\left[\frac{1}{|Q_{i}|}\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{Target}(\bm{x},\bm{y})-\mathsf{Amor }(\bm{x},\bm{y})||^{2}\geq\epsilon^{2}\frac{\mu^{2}}{4}\right]\] (32)\[= \Pr\left[\frac{1}{|Q_{i}|}\sum_{\begin{subarray}{c}(\bm{x},\bm{y}) \in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{Target}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}-\mu^{*}\geq\epsilon^{2}\frac{\mu^{2}}{4}-\mu^{*}\right]\] (33) \[\leq \Pr\left[\frac{1}{|Q_{i}|}\sum_{\begin{subarray}{c}(\bm{x},\bm{y} )\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{Target}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}-\mu^{*}\geq\epsilon^{2}\frac{\mu^{2}}{20}\right]\] (34) \[\leq e^{-\frac{\mu^{*}\epsilon^{4}\left[Q_{i}\right]}{4000Cd}}.\] (35)

Where the inequality in (35) follows from Hoeffding's inequality and the fact that:

\[||\mathsf{Target}(\bm{x},\bm{y})-\mathsf{Amor}(\bm{x},\bm{y})||^{2}\leq|| \mathsf{Target}(\bm{x},\bm{y})||+||\mathsf{Amor}(\bm{x},\bm{y})||\leq 2Cd.\] (36)

Third, notice by directly applying Theorem 1 and replacing the Monte Carlo explanation by the high-quality explanation, we have that

\[\lambda_{i}^{\mathsf{opt}}=\frac{\sum_{(\bm{x},\bm{y})\in\mathcal{D}_{\text{ val}}}(\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{Target}(\bm{x},\bm{y}),\mathsf{MC}^{n}( \bm{x},\bm{y})-\mathsf{Amor}(\bm{x},\bm{y}))}{\sum_{\begin{subarray}{c}(\bm {x},\bm{y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}}.\] (37)

Hence, we can write \(\lambda_{i}^{\mathsf{opt}}-\lambda_{i}\) as

\[|\lambda_{i}^{\mathsf{opt}}-\lambda_{i}|\] (38) \[= \left|\frac{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D }_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}|\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})- \mathsf{Target}(\bm{x},\bm{y}),\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{Amor}(\bm {x},\bm{y}))}{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{ val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}}\right|\] (39) \[\leq \frac{\left(\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D }_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})- \mathsf{Target}(\bm{x},\bm{y})||_{2}^{2}||\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{Amor}(\bm{x},\bm{y})||_{2}^{2}\right)^{1/2}}{\sum_{\begin{subarray}{c }(\bm{x},\bm{y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}},\] (40)

where the last inequality (40) comes from the Cauchy-Schwarz inequality. Denote the denominator in (40) by \(\Delta\), i.e.,

\[\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in\mathcal{D}_{\text{ val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}=\Delta.\]

Lastly, notice that \(\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})\) is sampled independently of \(\mathsf{MC}^{n}(\bm{x},\bm{y})\) and that \(\mathsf{Target}(\bm{x},\bm{y})\) is deterministic. Therefore:

\[\Pr[|\lambda_{i}^{\mathsf{opt}}-\lambda_{i}|\geq\epsilon]\] (41) \[\leq \Pr\left[\frac{\left(\frac{\sum_{\begin{subarray}{c}(\bm{x},\bm{ y})\in\mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})- \mathsf{Target}(\bm{x},\bm{y})||_{2}^{2}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||_{2}^{2}}{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n}(\bm{x},\bm{y})-\mathsf{ Amor}(\bm{x},\bm{y})||^{2}}\geq\epsilon\right]\] (42) \[\leq \Pr\left[\frac{\sum_{\begin{subarray}{c}(\bm{x},\bm{y})\in \mathcal{D}_{\text{val}}\\ s_{h}(\bm{x})\in Q_{i}\end{subarray}}||\mathsf{MC}^{n^{\prime}}(\bm{x},\bm{y})- \mathsf{Target}(\bm{x},\bm{y})||_{2}^{2}||\mathsf{MC}^{n}(\bm{x},\bm{y})- \mathsf{Amor}(\bm{x},\bm{y})||_{2}^{2}}{\Delta^{2}}\geq\epsilon^{2}\right]\] (43)\[\leq \Pr\left[\frac{\sum_{(\bm{x},\bm{y})\in\mathcal{D}_{\mathrm{val}}}|| \mathsf{M}\mathsf{C}^{n^{\prime}}(\bm{x},\bm{y})-\mathsf{Target}(\bm{x},\bm{y}) ||_{2}^{2}||\mathsf{M}\mathsf{C}^{n}(\bm{x},\bm{y})-\mathsf{A}\mathsf{A}\mathsf{ A}\mathsf{A}\mathsf{A}\mathsf{A}\mathsf{A}\mathsf{A}}{}.\] (44)

Where the inequality in (42) is a direct application of 40, the inequality in (44) comes from simply conditioning, the inequality in (45) comes from the fact that probabilities are bounded by one getting rid of the first term in (45) (first out of lines) and the fourth term in (45) (forth out of lines) and the fact that \(\mathsf{M}\mathsf{C}^{n^{\prime}}(\bm{x},\bm{y})\) is sampled independently of \(\mathsf{M}\mathsf{C}^{n}(\bm{x},\bm{y})\) and that \(\mathsf{Target}(\bm{x},\bm{y})\) is deterministic. Finally, the last inequality in (46) comes from applying (30) and (35).

Hence, from (46), we conclude that

\[\Pr[|\lambda_{i}^{\mathsf{opt}}-\lambda_{i}|\geq\epsilon]\leq e^{\frac{-\mu^{2}[Q_{i}]}{4Cd}}+e^{\frac{-\mu^{4}\epsilon^{4}[Q_{i}]}{400Cd}}.\] (47)

**Proposition 2** (Coverage for Inference Budget).: _Let \(\textit{N}_{\texttt{budget}}\geq 1\) be the set inference budget, and assume that the Monte Carlo method \(\mathsf{M}\mathsf{C}^{n}(\bm{x},\bm{y})\) uses \(n\) model inferences. Then, the coverage level \(\alpha\) should be chosen such that_

\[\operatorname*{argmin}_{\alpha\in[0,1]}\left\{\mathbb{E}\left[\textit{N}( \mathsf{S}\mathsf{E}(\bm{x},\bm{y}))\right]\leq\textit{N}_{\texttt{budget}} \right\}=\frac{n+1-\textit{N}_{\texttt{budget}}}{n}.\] (48)

_Recall that Shapley Value Sampling with parameter \(m\) performs \(1+dm\) inferences (\(\bm{x}\in\mathbb{R}^{d}\)), and Kernel Shap with parameter \(m\) performs \(m\) inferences._

Proof.: Let \(\alpha\in[0,1]\), then an \(\alpha\) portion of examples receive explanations from the amortized explainer, i.e., they receive one inference, and \(1-\alpha\) portion of examples receive explanations with initial guess, i.e., \(n\) model inferences. Therefore, the expected number of model inferences per instance is given by (49).

\[\mathbb{E}\left[\textit{N}(\mathsf{S}\mathsf{E}(\bm{x},\bm{y}))\right]= \alpha+(1-\alpha)(n+1)\] (49)

In order for the inference budget to be followed, it is necessary that

\[\mathbb{E}\left[\textit{N}(\mathsf{S}\mathsf{E}(\bm{x},\bm{y}))\right]= \alpha+(1-\alpha)(n+1)\leq\textit{N}_{\texttt{budget}}.\] (50)

From (50), we conclude that:

\[\alpha\geq\frac{n+1-\textit{N}_{\texttt{budget}}}{n},\] (51)

Hence,

\[\operatorname*{argmin}\alpha\in[0,1]\left\{\mathbb{E}\left[\textit{N}(\mathsf{ S}\mathsf{E}(\bm{x},\bm{y}))\right]\leq\textit{N}_{\texttt{budget}}\right\}= \frac{n+1-\textit{N}_{\texttt{budget}}}{n}.\] (52)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims are supported both by experiments (Section 5) and theoretical results (Section 4). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation in the end of Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: While we don't show all assumptions in Theorem 2 in the main paper for simplicity, we do provide the necessary assumptions in Appendix E. For all the other results, we provide the the necessary requirements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all necessary information in Section 5 and also in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We share the code to generate selective explanations with the Conda environment in an yml file to run it in the supplementary material. We also share a Github repository where the code is shared. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We do share this informations. In addition, these and others implementation details can be verified in our publicly available code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars using Seaborn [36] with 95% confidence using the bootstrap method. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report it in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, it does. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We propose a method to improve explanation quality of feature attribution methods which social impact is not clear beyond the impacts of the attribution methods themselves. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We don't release novel data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets and models used are referenced. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, they are. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.