# LICO: Explainable Models with

Language-Image COnsistency

 Yiming Lei\({}^{1}\), Zilong Li\({}^{1}\), Yangyang Li\({}^{2}\), Junping Zhang\({}^{1}\), Hongming Shan\({}^{3,4}\)

\({}^{1}\) Shanghai Key Laboratory of Intelligent Information Processing,

School of Computer Science, Fudan University

\({}^{2}\) Academy of Mathematics and Systems Science, Chinese Academy of Sciences

\({}^{3}\) Institute of Science and Technology for Brain-Inspired Intelligence and

MOE Frontiers Center for Brain Science, Fudan University

\({}^{4}\) Shanghai Center for Brain Science and Brain-inspired Technology

{ymlei, jpzhang, hmshan}@fudan.edu.cn,

zilongli23@m.fudan.edu.cn, yyli@amss.ac.cn

Corresponding author.

###### Abstract

Interpreting the decisions of deep learning models has been actively studied since the explosion of deep neural networks. One of the most convincing interpretation approaches is salience-based visual interpretation, such as Grad-CAM, where the generation of attention maps depends merely on categorical labels. Although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper develops a Language-Image COnsistency model for explainable image classification, termed LICO, by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing models without introducing any computational overhead during inference. Source code is made available at https://github.com/ymLeiFDU/LICO.

## 1 Introduction

Although deep neural networks (DNNs) have shown excellent performance in many fields, the lack of interpretability is still a barrier to landing in some high-stakes scenarios such as medical diagnosis, autonomous driving, _etc_. Therefore, the literature proposes various interpretation methods for DNNs and reveals the decision clues of DNNs to some extent.

Popular interpretation methods can be roughly categorized into two types: 1) gradient back-propagation-based, and 2) class activation mapping (CAM)-based. Both of them mainly take image classification as the pretext task and then generate explainable noisy gradients and saliency maps, respectively. As illustrated in Fig. 1(a), CAM-based methods often explore a better weighting scheme for integrating feature maps of a given input image. The gradient-based methods, in Fig. 1(b), alsostart from the output logits while back-propagating gradients to the input space. However, they are all post-hoc approaches that investigate a DNN pre-trained on a specific dataset [1; 2; 3; 4; 5; 6; 7], and yield biased interpretations due to the limited semantic information that a set of one-hot labels can offer. In addition, one-hot labels often trigger overfitting of the pre-trained model so that the effectiveness of existing interpretation methods could be compromised. On the other hand, the latent feature space of such pre-trained models is unable to sense real semantic space reflecting crucial parts in images. Therefore, it may be futile to explore complex post-hoc techniques for interpreting a pre-trained model.

Inspired by advanced large vision-language models (VLMs) such as CLIP [8], which developed generalized image and text encoders through training on huge amounts of image-text pairs, in this paper, we assume that the large VLMs can encode real-world semantic knowledge through contrastive vision-language alignments, whereas the traditional models pre-trained on relatively small datasets, such as ImageNet-1k, are inferior in capturing the true semantic information.

Motivation.The DNN-based image classification framework generally consists of a convolutional neural network (CNN) for feature extraction and a linear layer acting as a classifier. Since feature representations are used as the input to the classifier, if the DNN is truncated from feature representations, it becomes a linear classifier that aims to classify feature representations linearly into discrete one-hot label space. More specifically, the feature representation lies in the learned manifolds of high-dimensional semantic space [9]. Unfortunately, training using cross-entropy loss with one-hot labels cannot guarantee that the manifolds of image features can reflect the distribution of real-world semantic knowledge, which hinders performance improvement of existing interpretation methods.

In this paper, we leverage language information from large VLMs to enhance current interpretation methods for achieving more explainable saliency maps while enabling promising classification performance improvements; see Fig. 1(c). First, we propose Language-Image-COnsistent (LICO) learning to facilitate the alignment between the manifolds of visual features and class-aware language information. To address the discrete nature of categorical classes, we construct a learnable prompt for each class and map all prompts into a continuous space using the CLIP text encoder, which is feasible to align manifolds of both image and text. Second, we impose each prompt token to correlate with certain feature maps. Considering that the feature maps are redundant to the final classification decision, we propose to encourage the context tokens to guide certain feature maps through distribution alignment using optimal transport (OT) theory.

Contributions.We summarize the main contributions of this paper as follows. **(i)** We propose a novel framework to enhance current interpretation methods by introducing language guidance from large VLMs. **(ii)** We model the discrete categorical class to a continuous space by constructing class-aware learnable prompts, hence, enabling consistent manifold matching between image features and text features. **(iii)** To ensure consistent local feature alignment, we utilize OT theory to reduce the distance between distributions of image and text. **(iv)** Extensive experimental results on eight classification datasets demonstrate the superiority of the proposed LICO against current interpretation methods in terms of quantitative and qualitative results. For practical applications, LICO does not introduce any computational overhead during inference while maintaining improved performance.

Figure 1: Motivation of LICO. (a) Existing post-hoc CAM-based methods focus on generating better weighting schemes to obtain weighted-sum attention maps. (b) Gradient-based methods tend to back-propagate gradients from logits to input space. (c) Proposed LICO incorporates learnable prompts to enable image features to approximate semantic information in latent space.

## 2 Related Work

Interpretation methods. Class activation mapping (CAM) is a simple approach to generating class-aware saliency maps for CNNs [1]. Inspired by CAM's effectiveness, its variants including Grad-CAM [2], Grad-CAM++ [3], Score-CAM [4], RISE [10], and Group-CAM [5], were proposed to enhance the precision and interpretability of CNNs. Gradient-based methods such as Guided Back-propagation [11], SmoothGrad [7], and Integrated Gradient (IG) [6] compute the gradients from output logits back to input space. However, they are prone to yielding attribution noise that is inconsistent with human intuition. Some advanced techniques have been developed to reduce attribution noise by utilizing improved path integration theory, such as adaptive path method [12] and important direction method [13]. Distinct from existing interpretation methods that focused on the post-processing of feature map weighting scheme and accurate gradients calculation based on a pre-trained model, our LICO aims to incorporate class-aware learnable prompts that reflect real-world semantic knowledge to guide latent image features. It is worth noting that LICO is compatible with current methods and can consistently improve their performance.

Prompt learning. Prompt learning stems from natural language processing (NLP), enabling large language models to directly model the prediction probability of raw text [14; 15; 16]. Following the proliferation of large VLMs, prompt learning began to surface in the realm of computer vision, significantly enhancing multi-modal tasks and text-guided image generation. To overcome the limitations of learning with fixed prompts used in CLIP, learnable prompts have been explored to provide more generalized performance on downstream tasks. CoOp firstly proposed a framework that constructs learnable prompts to align image and text in latent space [17]. Furthermore, to enable CoOp with a stronger capability to perceive the knowledge of new classes, the authors proposed CoCoOp in which the prompts are learned conditioned on image features [18]. PLOT correlated one given image with multiple prompts of its class by optimal transport, leading different prompts to reflect features with respect to different regions within this image [19]. In our LICO, we propose that learnable prompt tokens in one sentence should be registered to certain parts of an image, as shown in Fig. 1(c).

Optimal transport.Optimal transport (OT) is a typical metric for measuring discrepancy between two distributions, requiring solving a linear programming problem [20; 21]. Thanks to some fast alternatives, OT has been widely utilized in deep learning, including graph cross-domain alignment [22; 23], domain adaptation [24; 25; 26], optimal graph matching for vessel image registration [27], and multi-modal distribution alignment [28; 29]. Although KL-divergence can effectively measure the cost of which the predicted distribution approximates ground-truth distribution, given the normalized image and text distributions, it is intractable to guarantee they share a common metric space. Therefore, we propose to utilize OT to tackle fine-grained cross-modal alignment.

Figure 2: Framework of the proposed LICO. (a) Conventional classification pipeline of DNNs. (b) Language feature extraction with pre-trained text encoder. (c) Manifold matching among samples and optimal transport alignment between feature maps and prompt tokens within each sample.

## 3 Methodology

### Overview of LICO

Fig. 2 presents the overview of LICO framework. Given a mini-batch of \(B\) training samples \(\{(\bm{x}_{i},y_{i},t_{i})\}_{i=1}^{B}\), the input image \(\bm{x}_{i}\) is of size \(H\times W\times C\), where \(C\) is the number of channels, \(H\) and \(W\) are height and width, respectively. \(y_{i}\in\mathcal{C}\) denotes the class label, and \(t_{i}\) represents the text of corresponding label \(y_{i}\). We denote the pre-trained CLIP text encoder as \(g_{\bm{\phi}}\) while we fixed its parameters \(\bm{\phi}\) during training. The image encoder is denoted as \(f_{\bm{\theta}}\) that is to be optimized. The image encoder takes \(\bm{x}_{i}\) as input and outputs feature maps \(\bm{F}_{i}\) with \(N\) channels. Note that \(\bm{F}_{i}\) is a tensor, which is then flattened to one-dimension for each channel; _i.e._, \(\bm{F}_{i}\in\mathbb{R}^{N\times d^{\prime}}\). For the language modeling branch, the text encoder \(g_{\bm{\phi}}\) takes as input the constructed learnable prompt: \(\bm{t}_{i}=[X_{1},X_{2},\ldots,X_{M-1},t_{i}]\), where \([\ldots]\) denotes the concatenation, \(t_{i}\) is the text corresponding to label of the \(i\)-th sample, \(X_{m}\) are learnable context tokens, and \(M-1\) is the number of context tokens. Through the pre-trained text encoder, the prompt vector \(\bm{t}_{i}\) is mapped into a continuous \(d\)-dimension space; _i.e._, \(d=512\) in CLIP.

In order to measure the distances between image and language features in latent space, we append a mapping net \(h_{\bm{\psi}}\), a multilayer perceptron (MLP) with only one hidden layer, to map the \(d\)-dimension language features \(\bm{t}_{i}\in\mathbb{R}^{M\times d}\) to the space of \(d^{\prime}\)-dimension, _i.e._, \(\bm{G}_{i}\in\mathbb{R}^{M\times d^{\prime}}\), which is the same as that of image features \(\bm{F}_{i}\). The structure of \(h_{\bm{\psi}}\) varies along different image encoders. We use \(h_{\bm{\psi}}[a,b]\) to describe its structure, _i.e._, numbers of hidden units and output units are \(a\) and \(b\), respectively. Note that in this paper, we do not intend to modify the structure of the existing image and text encoders so that we can obtain the feature maps and corresponding attention maps that are comparable to existing methods. Consequently, we utilize \(\bm{F}_{i}\) and \(\bm{G}_{i}\) to preserve the language-image-consistent structure in latent space using manifold matching (Section 3.2) and fine-grained feature-language alignment by optimal transport (Section 3.3).

### Language-Image Manifold Matching

To preserve the global manifold structure in latent space, we first measure the relationships, _i.e._ the distances or similarities, among training images and corresponding class-aware prompts.

Although the class categories are discretely distributed that it is infeasible to capture their manifold, we can map them into a continuous space by constructing a prompt vector for each class, then mapping it to a latent space using a pre-trained text encoder. Therefore, we can enforce the image manifold [30] to approximate the language manifold.

In practice, we align the adjacent matrices of language and image features. Specifically, the language adjacent matrix is denoted as \(\bm{A}_{B\times B}^{G}\), and that of image is \(\bm{A}_{B\times B}^{F}\):

\[\bm{A}_{i,j}^{F}=\frac{\text{exp}(-D(\bm{F}_{i},\bm{F}_{j})/\tau)}{\sum_{s=1}^ {B}\text{exp}(-D(\bm{F}_{i},\bm{F}_{s})/\tau)},\hskip 28.452756pt\bm{A}_{i,j}^{G}= \frac{\text{exp}(-D(\bm{G}_{i},\bm{G}_{j})/\tau)}{\sum_{s=1}^{B}\text{exp}(-D( \bm{G}_{i},\bm{G}_{s})/\tau)},\] (1)

where \(D(\cdot,\cdot)\) calculates the distance between two images or two prompts, such as Euclidean distance, \(\tau\) is the temperature to be learned during training. Then, each image is formulated as a distribution \(\bm{A}_{i,:}^{F}\) where each dimension denotes the distance from the \(i\)-th sample to others within a mini-batch. Similar to the class-wise prompts, \(\bm{A}_{i,:}^{G}\) implies the relationships between the class prompt of the \(i\)-th sample and those of others.

Recall our assumption that the large amounts of image-text pairs used in large VLMs, such as CLIP, can lead to a generalized text encoder that well establishes a real-world semantic space. Therefore, for certain downstream tasks and datasets, we aim to introduce the knowledge of this semantic space to the latent space of the target image domain. Then, we propose a manifold matching loss \(\mathcal{L}_{\text{MM}}\), which enables the manifold of image features to approach that of prompt features using KL-divergence:

\[\mathcal{L}_{\text{MM}}=\frac{1}{B}\sum\nolimits_{i=1}^{B}\mathrm{KL}[\bm{A}_{ i,:}^{G}\|\bm{A}_{i,:}^{F}].\] (2)

We note that we do not consider the inverse version \(\mathrm{KL}[\bm{A}_{i,:}^{F}\|\bm{A}_{i,:}^{G}]\) since LICO focuses on enabling image manifold to approach the manifold of language prompts.

### Feature Distribution Alignment by Optimal Transport

While we strive for a coarse alignment of the global manifold structure, it is crucial to establish a correlation between prompt tokens and specific feature maps for each sample, which aids in mitigating the influence of redundant features on the generation of attention maps. Most importantly, the critical feature maps that are most related to the target class should possess the highest similarity with respect to the class token.

Unfortunately, it is challenging to determine or assign which of the \(N\) feature maps are correlated with certain prompt tokens. In this paper, we propose to align \(\bm{G}_{i}=[\bm{g}_{1},\bm{g}_{2},\dots,\bm{g}_{M-1},\bm{g}_{t_{i}}]^{\top} \in\mathbb{R}^{M\times d^{\prime}}\) and \(\bm{F}_{i}=[\bm{f}_{1},\bm{f}_{2},\dots,\bm{f}_{N}]^{\top}\in\mathbb{R}^{N \times d^{\prime}}\) for achieving consistency between feature maps and specific prompt tokens. In other words, different words in a sentence should correspond to parts of an image, and in contrast, partial regions in an image reflect the semantic information delivered by certain tokens. Therefore, it is intractable to measure this distance using KL-divergence since it is not a strict metric, _i.e._, it does not satisfy the property of triangle inequality. In LIGO, we use optimal transport, which is widely used in measuring distances of two distributions, to align distributions of normalized visual features and prompt features.

For the given image feature maps \(\bm{F}_{i}\in\mathbb{R}^{N\times d^{\prime}}\) and a prompt tokens \(\bm{G}_{i}\in\mathbb{R}^{M\times d^{\prime}}\), we construct two discrete distributions:

\[\bm{\mu}=\sum\nolimits_{n=1}^{N}u_{n}\delta_{\bm{f}_{n}},\qquad\bm{v}=\sum \nolimits_{m=1}^{M}v_{m}\delta_{\bm{g}_{m}},\] (3)

where \(\delta_{\bm{f}_{n}}\) is a Dirac function centered at \(\bm{f}_{n}\), so as to \(\delta_{\bm{g}_{m}}\), and the weights \(\bm{u}=\{u_{n}\}_{n=1}^{N}\in\Delta_{N}\) and \(\bm{v}=\{v_{m}\}_{m=1}^{M}\in\Delta_{M}\), \(\Delta\) denotes the \(N\)- and \(M\)-dimensional probability simplex, _i.e._, \(\sum\nolimits_{n=1}^{N}u_{n}=1\), \(\sum\nolimits_{m=1}^{M}v_{m}=1\). Finally, the discrete OT distance for one sample is defined as follows:

\[D_{\text{OT}}(\bm{\mu},\bm{v})= \inf_{\bm{\pi}\in\Pi(\bm{\mu},\bm{v})}\mathbb{E}_{(\bm{\mu},\bm{v })\sim\bm{\pi}}[\bm{C}(\bm{f},\bm{g})]=\min_{\bm{T}\in\Pi(\bm{\mu},\bm{v})} \sum\limits_{n=1}^{N}\sum\limits_{m=1}^{M}\bm{T}_{n,m}\cdot c(\bm{f}_{n},\bm{g} _{m}),\] (4) s.t. \[\bm{T}\bm{1}_{m}=\bm{\mu},\quad\bm{T}\bm{1}_{n}=\bm{v},\]

where \(\bm{C}\in\mathbb{R}^{N\times M}\) represents the cost matrix in which each element \(c(\bm{f}_{n},\bm{g}_{m})\) denotes transportation cost between \(\bm{f}_{n}\) and \(\bm{g}_{m}\). \(\bm{T}\in\mathbb{R}^{N\times M}\) is the transport plan that is to be optimized and \(\Pi(\bm{\mu},\bm{v})\) denotes the transportation polytope that contains all joint probabilities of \(\bm{\mu}\) and \(\bm{v}\). In practice, solving the optimization problem in Eq. (4) often equips with a high computation cost. Thus we use the Sinkhorn algorithm, which is more computationally amenable, to solve an entropy-constrained problem [31]:

\[D_{\text{OT}}(\bm{\mu},\bm{v})=\min_{\bm{T}\in\Pi(\bm{\mu},\bm{v})}\sum \limits_{n=1}^{N}\sum\limits_{m=1}^{M}\bm{T}_{n,m}\cdot c(\bm{f}_{n},\bm{g}_{m })-\lambda\mathbb{H}(\bm{T}),\quad\text{{s.t.}}\quad\bm{T}\bm{1}_{m}=\bm{\mu}, \quad\bm{T}\bm{1}_{n}=\bm{v},\] (5)

where \(\lambda\) is Lagrange multiplier and \(\mathbb{H}(\bm{T})=\sum_{n,m}\bm{T}_{n,m}\text{log}\bm{T}_{n,m}\). Then after a few iterations, we obtain the optimal solutions:

\[\bm{T}^{*}=\text{diag}(\bm{\mu}^{t})\text{exp}(-\bm{C}/\lambda)\text{diag}(\bm {v}^{t}),\] (6)

where \(t\) is the iteration step. \(\bm{\mu}^{t}\) and \(\bm{v}^{t}\) are updated according to following rules:

\[\bm{\mu}^{t}=\bm{\mu}/(\text{exp}(-\bm{C}/\lambda)\bm{v}^{t-1}),\quad\bm{v}^{ t}=\bm{v}/(\text{exp}(-\bm{C}/\lambda)^{\top}\bm{\mu}^{t}).\] (7)

Dynamic context (DC).To endow each image with diverse prompt tokens, we shuffle the learnable context tokens in each training iteration referring to the training procedure in Algorithm 1.

### Final Objective Function

The final training loss function is as follows:

\[\mathcal{L}=\mathcal{L}_{\text{CE}}+\alpha\mathcal{L}_{\text{MM}}+\beta\mathcal{ L}_{\text{OT}},\] (8)

where \(\mathcal{L}_{\text{CE}}\) is the cross-entropy loss, \(\mathcal{L}_{\text{OT}}=\frac{1}{B}\sum\nolimits_{i=1}^{B}D_{\text{OT}}\), \(\alpha\) and \(\beta\) are hyperparameters for adjusting different terms. During the inference phase, we apply the trained image encoder and classifier to conduct conventional classification that yields the predicted probability of a given input image. Note that the text encoder and the MLP mapping do not affect the inference procedure. The detailed algorithm can be found in Algorithm 1.

```
0: Training set \(\mathcal{S}\), total epochs \(U\), image encoder \(f_{\bm{\theta}}\), text encoder \(g_{\bm{\phi}}\), MLP \(h_{\bm{\psi}}\), learnable prompts \(\bm{t}_{i}=[X_{1},X_{2},\dots,\dots,X_{M-1},t_{i}]\).
0: Image encoder with optimal parameter \(\bm{\theta}^{*}\).
1:for\(u=1\) to \(U\)do
2: Sample a mini-batch of (\(\{\bm{x}_{i},\bm{t}_{i},y_{i}\}_{i=1}^{B}\)) from \(\mathcal{S}\).
3: Randomly shuffling \(X_{m}\) and \(t_{i}\). \(\triangleright\) Dynamic context
4:\(\bm{F}_{i}\) = \(f_{\bm{\theta}}(\bm{x}_{i})\), \(\bm{G}_{i}\) = \(h_{\bm{\phi}}(g_{\bm{\phi}}(\bm{t}_{i}))\). \(\triangleright\) Image and text features
5: Calculate \(\bm{A}_{B\times B}^{G}\), \(\bm{A}_{B\times B}^{F}\)\(\triangleright\) Adjacient matrices
6: Calculate \(\mathcal{L}_{\text{MM}}\) according to Eq. (2). \(\triangleright\) Coarse alignment by manifold
7: Optimal transport plan \(\bm{T}^{*}\) by Eq. (6), then calculate \(\mathcal{L}_{\text{OT}}\). \(\triangleright\) Fine-grained alignment by OT
8: Classifier \(\longleftarrow\)\(\bm{F}_{i}\)
9: Total loss: \(\mathcal{L}=\mathcal{L}_{\text{CE}}+\alpha\mathcal{L}_{\text{MM}}+\beta\mathcal{ L}_{\text{OT}}\), update \(\bm{\theta}\), \(\bm{\psi}\), and \(X_{m}\) by gradients: \(\frac{\partial\mathcal{L}}{\partial\bm{\theta}},\frac{\partial\mathcal{L}}{ \partial\bm{\psi}},\frac{\partial\mathcal{L}}{\partial\bm{X}}\).
10:endfor ```

**Algorithm 1** Training Algorithm of LICO.

## 4 Experiments

### Datasets

This paper focuses on image classification task and evaluates the proposed LICO on well-known datasets, including ImageNet-1k [32], CIFAR-10/100 [33], and SVHN [34]. We conduct the classification experiments under the setting of limited training data in which the splits of labeled data follow the previous works for fair comparison [35; 36]. Furthermore, we conduct fine-grained classification on typical benchmarks, including CUB-200 [37], FGVC-Aircraft [38], Stanford Cars-196 [39], VGG Flowers [40]. The evaluation is carried out on both the full dataset and few-shot settings, following the procedure of CGC [41].

### Implementation Details

We employ the ViT-B/32 trained by CLIP [8] as the text encoder and its parameters are fixed during training. The output dimension of this text encoder is \(512\) for each token. The image encoders in our experiments vary along different datasets and training settings. Specifically, for the ImageNet experiments, we utilize ResNet-50 as the image classifier [42]. In doing so, the convolutional layers preceding the final linear layer constitute the image encoder of LICO. For the CIFAR-10/100 and SVHN, we follow the experimental settings in [35; 36], the classification network is Wide ResNet (WRN) [43]. We further conduct the same experiments using another network, the PreAct-ResNet-18 (PARN-18) [44]. For fine-grained classifications on CUB-200, Standford-Cars, Aircraft, and VGG Flowers datasets, we applied the same settings used in CGC for fair comparison [41], where the image encoder is also the ResNet-50 that has been pre-trained on ImageNet-1k with CE loss. Note that for all the experiments of LICO, we only use the trained image encoder and the classifier during the inference phase. The text encoder, learnable prompt contexts, and MLP mapping net are dropped, thus, will not compromise the computational efficiency. Hyperparameters \(\alpha\) and \(\beta\) are set as \(10\) and \(1\), respectively.

All the experiments are implemented by PyTorch [45]. The learning rates for ImageNet, CIFAR-10/100, and SVHN are of \(0.03\) with a consine rate decay schedule, _i.e._, \(\eta=\eta_{0}\cos(\frac{7\pi k}{16K})\), where \(\eta_{0}\) denotes the initial learning rate and \(k\) is the index of training step [46]. We use a standard stochastic gradient descent (SGD) optimizer with a momentum of \(0.9\)[47; 48], and the weight decay is \(0.0001\). The training batch sizes are \(128\) and \(64\) for ImageNet and other datasets, respectively. Specifically, the mapping net for ResNet-50 is \(h_{\bm{\psi}}[512,49]\), \(h_{\bm{\psi}}[512,64]\) for WRN, and \(h_{\bm{\psi}}[512,49]\) for PARN-18. The total training epoch is \(90\) for ImageNet and \(200\) for others. The experiments were trained on four NVIDIA A100 GPUs for ImageNet-1k and one GPU for other datasets.

### Comparison of Interpretation Capability

In this experiment, we compare interpretation results of popular methods, including Grad-CAM [2], Grad-CAM++ [3], RISE [10], Score-CAM [4], Group-CAM [5], and CGC [41]. Note that LICO is compatible with these post-hoc interpretation methods so that in our experiments, we compare the saliency maps obtained by these interpretation methods using baseline ImageNet pre-trained model and LICO trained counterparts of the same architectures.

Qualitative results.Qualitatively, we compare the saliency maps obtained by different interpretation methods in Fig. 3 (Left). We can see that for single object images, LICO can effectively help baseline methods cover more comprehensive and discriminative regions, _e.g._, the head and fur of a bird. However, the baseline methods are inferior in capturing all the objects in the multi-object image. Please refer to Appendix A for more results of multi-class and multi-object cases.

Quantitative results.To achieve quantitative evaluation, we follow [10, 4, 5] to conduct Insertion and Deletion tests. Insertion gradually introduces class-related regions (3.6% pixels) of an original image to a blurred image according to the values of the saliency map. This process is repeated until the blurred image is fully recovered. In contrast, Deletion aims to replace related pixels (3.6%) in a blurred image with those of the corresponding original image. In Table 1, we provide the AUC of the classification score after Softmax. For most of the interpretation methods, LICO consistently improves the Insertion and Deletion values. Although the insertion value of Group-CAM and deletion value of Grad-CAM is better than LICO, the LICO still achieves the best overall values. We also report the quantitative results of corresponding cases with different interpretation methods in Fig. 3 (Right). Please refer to Appendix B for the experiment of Pointing Game [5].

Sanity checks.Sanity check for saliency maps was first proposed in [49], which is a qualitative test aiming at evaluating whether the saliency maps are sensitive to model parameters. We conducted two types of test: cascading randomization from top to bottom layers and independent randomizing of

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Method & Grad-CAM++ & Grad-CAM & RISE & Score-CAM & Group-CAM & CGC \\ \hline Insertion\(\uparrow\) & 50.0/**51.2** & 53.5/**57.1** & 54.0/**54.9** & 55.1/**55.6** & **56.8**/55.2 & 52.2/**55.4** \\ Deletion\(\downarrow\) & 14.8/**11.7** & **13.3**/15.1 & 11.7/**10.8** & 11.5/**11.2** & 12.3/**10.5** & -/15.8 \\ Overall\(\uparrow\) & 35.2/**39.5** & 40.2/**42.0** & 43.6/**44.1** & 42.3/**44.4** & 44.5/**44.7** & -/39.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative comparisons of different interpretation methods on ImageNet in terms of Insertion and Deletion. We report the results: “Baseline/+LICO”. \(Overall=Insertion-Deletion\).

Figure 3: Saliency maps (Left) and Insertion/Deletion curves (Right) on ImageNet-1k validation set.

[MISSING_PAGE_FAIL:8]

Fine-Grained classification.Furthermore, we evaluate LICO on a fine-grained classification problem that requires the model to capture fine-grained features. In Table 4, except for the Aircraft, LICO enhances classification performance under all settings compared with CGC. This observation highlights that contrastive learning among attention maps used in CGC primarily emphasizes similarities between target images and other random/augmented images, while disregarding the measurement of semantic distinctions. For the Aircraft dataset, it is difficult for LICO to achieve significant improvements due to the categorical labels are types of aircraft, _e.g._, the numerical symbols such as 727-200 which are challenging for CLIP text encoder to achieve meaningful embedding, and CLIP has demonstrated relatively lower accuracy on some out of distribution data like aircraft and satellite[8]. To address the challenges, we incorporate prior text knowledge by constructing initial prompts as "a type of aircraft", which allows LICO to improve performance in the few-shot settings while achieving comparable results to CGC under the full setting. This strategy has also been verified in CoCoOp [18].

### Ablation Study

Ablation on manifold matching and OT alignment.In Table 5, we investigate the effectiveness of \(\mathcal{L}_{\text{MM}}\) and \(\mathcal{L}_{\text{OT}}\). We can see that only using \(\mathcal{L}_{\text{OT}}\) obtains more performance drop than that of \(\mathcal{L}_{\text{MM}}\). This indicates that the global consistency of manifolds guarantees the basic performance, and OT only focuses on local feature alignments so that it cannot be sensitive to relationships between intra- and inter-class samples.

Ablation on number of context tokens.In Table 6, we evaluate the performances influenced by different numbers of learnable tokens. We find that 12 is the best choice in our experiments, and the settings of 16 and 20 are relatively better than those of 4 and 8.

Ablation on distance function \(D\) in Eq. (1).In Table 7, we compare the similarity functions used in manifold matching, which measures distances among samples within a mini-batch. We can see that the Euclidean distance is more suitable to our LICO.

Please refer to Appendices E, F, and G for more ablation studies on DC, effects of different type of text encoders, and frozen parameters of prompts, respectively.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline no. & Top-1 & Top-5 & Insertion & Deletion \\ \hline
0 & 75.64 & 91.92 & 54.1 & 17.8 \\
4 & 76.03 & 92.74 & 55.2 & 17.5 \\
8 & 76.09 & 92.89 & 56.3 & 16.0 \\
12 & **76.27** & **92.99** & **57.1** & **15.1** \\
16 & 76.21 & 92.87 & 57.0 & 15.8 \\
20 & 76.14 & 92.93 & 56.9 & 15.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation on no. of context tokens.

\begin{table}
\begin{tabular}{l|c c c|c c c c} \hline \hline Method & 1-shot & 5-shot & 10-shot & Full & 1-shot & 5-shot & 10-shot & Full \\ \hline \multirow{3}{*}{Baseline +CGC [41]} & \multicolumn{5}{c}{CUB-200} & \multicolumn{5}{c}{Standford-Cars} \\ \cline{2-8}  & 13.7\({}_{\pm 0.3}\) & 51.7\({}_{\pm 0.3}\) & 66.4\({}_{\pm 0.2}\) & 80.1\({}_{\pm 0.9}\) & 6.1\({}_{\pm 0.2}\) & 34.3\({}_{\pm 0.4}\) & 61.1\({}_{\pm 0.4}\) & 89.7\({}_{\pm 0.1}\) \\  & 15.8\({}_{\pm 0.3}\) & 55.2\({}_{\pm 0.3}\) & **68.4\({}_{\pm 0.3}\)** & 81.5\({}_{\pm 0.1}\) & 6.5\({}_{\pm 0.2}\) & 36.5\({}_{\pm 0.4}\) & 63.0\({}_{\pm 0.4}\) & 90.3\({}_{\pm 0.1}\) \\  & **16.9\({}_{\pm 0.2}\)** & **55.8\({}_{\pm 0.4}\)** & **68.4\({}_{\pm 0.2}\)** & **82.7\({}_{\pm 0.3}\)** & **7.1\({}_{\pm 0.3}\)** & **37.2\({}_{\pm 0.5}\)** & **64.4\({}_{\pm 0.4}\)** & **91.5\({}_{\pm 0.2}\)** \\ \hline \hline \multicolumn{8}{c}{Aircraft} & \multicolumn{5}{c}{VGG Flowers} \\ \cline{2-8}  & 7.7\({}_{\pm 0.3}\) & 25.7\({}_{\pm 0.4}\) & 41.4\({}_{\pm 0.3}\) & 83.7\({}_{\pm 0.2}\) & 52.1\({}_{\pm 0.5}\) & 85.6\({}_{\pm 0.4}\) & 93.2\({}_{\pm 0.2}\) & 96.1\({}_{\pm 0.2}\) \\  & 8.0\({}_{\pm 0.3}\) & 26.9\({}_{\pm 0.4}\) & 42.9\({}_{\pm 0.3}\) & **85.7\({}_{\pm 0.2}\)** & 53.3\({}_{\pm 0.5}\) & 85.8\({}_{\pm 0.4}\) & **93.4\({}_{\pm 0.2}\)** & 96.2\({}_{\pm 0.2}\) \\  & **8.4\({}_{\pm 0.4}\)** & **27.5\({}_{\pm 0.2}\)** & **43.1\({}_{\pm 0.4}\)** & 85.6\({}_{\pm 0.2}\)** & **55.6\({}_{\pm 0.4}\)** & **86.2\({}_{\pm 0.3}\)** & **93.4\({}_{\pm 0.4}\)** & **96.8\({}_{\pm 0.3}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Fine-grained classification accuracy under full and few-shot settings.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \(\mathcal{L}_{\text{CE}}\) & \(\mathcal{L}_{\text{MM}}\) & \(\mathcal{L}_{\text{OT}}\) & Top-1 & Top-5 & Insert. & Delet. \\ \hline \multirow{3}{*}{**Ablation on manifold matching and OT alignment.** In Table 5, we investigate the effectiveness of \(\mathcal{L}_{\text{MM}}\) and \(\mathcal{L}_{\text{OT}}\). We can see that only using \(\mathcal{L}_{\text{OT}}\) obtains more performance drop than that of \(\mathcal{L}_{\text{MM}}\). This indicates that the global consistency of manifolds guarantees the basic performance, and OT only focuses on local feature alignments so that it cannot be sensitive to relationships between intra- and inter-class samples.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Euclidean & Cosine \\ \hline CIFAR-10 & **95.78\({}_{\pm 0.08}\)** & 95.46\({}_{\pm 0.12}\) \\ CIFAR-100 & **82.22\({}_{\pm 0.02}\)** & 81.85\({}_{\pm 0.05}\) \\ SVHN & **98.25\({}_{\pm 0.06}\)** & 98.21\({}_{\pm 0.06}\) \\ CUB-200 & **82.70\({}_{\pm 0.30}\)** & 82.10\({}_{\pm 0.30}\) \\ Flowers & **96.80\({}_{\pm 0.30}\)** & 96.20\({}_{\pm 0.40}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation on distance function.

Conclusion

In this paper, we proposed LICO to enhance existing visual interpretation methods by incorporating language information, which is compatible with these methods. The LICO aligns image and prompt embeddings globally using manifold matching, simultaneously aligns feature maps with corresponding learnable context tokens by applying optimal transport alignment. Extensive experiments on evaluating interpretation capability and classification performance exhibit both quantitative and qualitative enhancement introduced by LICO. A key limitation of LICO is that it depends on a trainable MLP to project language embeddings into a metric space of same dimension with that of image features, where the output dimension of such MLPs varies according to different image encoders.

Broader ImpactsLICO explores effective visual interpretations of DNNs by introducing language knowledge, which is orthogonal to existing post-hoc interpretation methods. An important merit of LICO is to enhance interpretability while achieving competitive or even better classification performance, applicable to various kinds of tasks and models effectively.

## Acknowledgements

This work was supported in part by Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01) and ZJLab, Natural Science Foundation of Shanghai (No. 21ZR1403600), National Natural Science Foundation of China (Nos. 62101136, 62306075), China Postdoctoral Science Foundation (No. 2022TQ0069), Young Elite Scientists Sponsorship Program by CAST (No. 2022QNRC001), Shanghai Municipal of Science and Technology Project (No. 20JC1419500), and Shanghai Center for Brain Science and Brain-inspired Technology.

## References

* [1]B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba (2016) Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921-2929. Cited by: SS1.
* [2]R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra (2017) Grad-CAM: visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618-626. Cited by: SS1.
* [3]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-CAM++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter Conference on Applications of Computer Vision, pp. 839-847. Cited by: SS1.
* [4]H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel, and X. Hu (2020) Score-CAM: score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and pattern Recognition Workshops, pp. 24-25. Cited by: SS1.
* [5]Q. Zhang, L. Rao, and Y. Yang (2021) Group-CAM: group score-weighted visual explanations for deep convolutional networks. arXiv preprint arXiv:2103.13859. Cited by: SS1.
* [6]M. Sundararajan, A. Taly, and Q. Yan (2017) Axiomatic attribution for deep networks. In International Conference on Machine Learning, pp. 3319-3328. Cited by: SS1.
* [7]D. Smilkov, N. Thorat, B. Kim, F. Viegas, and M. Wattenberg (2017) Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. Cited by: SS1.
* [8]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [9]Y. Wang and X. Wang (2022) "why not other classes?": towards class-contrastive backpropagation explanations. Advances in Neural Information Processing Systems35, pp. 9085-9097. Cited by: SS1.
* [10]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [11]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [12]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [13]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [14]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [15]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [16]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [17]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [18]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [19]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [20]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [21]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [22]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [23]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [24]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [25]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [26]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [27]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [28]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [29]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [30]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [31]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [32]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. Cited by: SS1.
* [33]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from * [10] Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: Randomized input sampling for explanation of black-box models. In _British Machine Vision Conference_, page 151, 2018.
* [11] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. _International Conference on Learning Representations_, 2015.
* [12] Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, and Tolga Bolukbasi. Guided integrated gradients: An adaptive path method for removing noise. In _Proceedings of the IEEE/CVF Conference on Computer Vision and pattern Recognition_, pages 5050-5058, 2021.
* [13] Ruo Yang, Binghui Wang, and Mustafa Bilgic. IDGI: A framework to eliminate explanation noise from integrated gradients. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [14] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* [15] Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? _arXiv preprint arXiv:1909.01066_, 2019.
* [16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [17] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.
* [18] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16816-16825, 2022.
* [19] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. PLOT: Prompt learning with optimal transport for vision-language models. In _The Eleventh International Conference on Learning Representations_, 2022.
* [20] Gaspard Monge. Memoire sur la theorie des deblais et des remblais. _Mem. Math. Phys. Acad. Royale Sci._, pages 666-704, 1781.
* [21] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [22] Hermina Petric Maretic, Mireille El Ghec, Giovanni Chierchia, and Pascal Frossard. GOT: an optimal transport framework for graph comparison. _Advances in Neural Information Processing Systems_, 32, 2019.
* [23] Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. Graph optimal transport for cross-domain alignment. In _International Conference on Machine Learning_, pages 1542-1553. PMLR, 2020.
* [24] Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang, and Brahim Chaib-draa. Domain generalization via optimal transport with metric similarity learning. _arXiv preprint arXiv:2007.10573_, 2020.
* [25] Shengsheng Wang, Bilin Wang, Zhe Zhang, Ali Asghar Heidari, and Huiling Chen. Class-aware sample reweighting optimal transport for multi-source domain adaptation. _Neurocomputing_, 523:213-223, 2023.
* [26] Rosanna Turrisi, Remi Flamary, Alain Rakotomamonjy, and Massimiliano Pontil. Multi-source domain adaptation via weighted joint distributions optimal transport. In _Uncertainty in Artificial Intelligence_, pages 1970-1980, 2022.

* [27] Danilo Motta, Wallace Casaca, and Afonso Paiva. Vessel optimal transport for automated alignment of retinal fundus images. _IEEE Transactions on Image Processing_, 28(12):6154-6168, 2019.
* [28] John Lee, Max Dabagia, Eva Dyer, and Christopher Rozell. Hierarchical optimal transport for multimodal distribution alignment. _Advances in Neural Information Processing Systems_, 32, 2019.
* [29] Shraman Pramanick, Aniket Roy, and Vishal M Patel. Multimodal learning using optimal transport for sarcasm and humor detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3930-3940, 2022.
* [30] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. _Advances in Neural Information Processing Systems_, 26, 2013.
* [31] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, 26, 2013.
* [32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision_, 115:211-252, 2015.
* [33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [34] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* [35] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling. _Advances in Neural Information Processing Systems_, 34:18408-18419, 2021.
* [36] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in Neural Information Processing Systems_, 33:596-608, 2020.
* [37] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. (CNS-TR-2011-001), 2010.
* [38] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In _Proceedings of the IEEE International Conference on Computer Vision Workshops_, pages 554-561, 2013.
* [40] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729, 2008.
* [41] Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, and Hamed Pirsiavash. Consistent explanations by contrastive learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10213-10222, 2022.
* [42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [43] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _British Machine Vision Conference_, 2016.
* [44] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _European Conference on Computer Vision_, 2016.

* [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. _Advances in Neural Information Processing Systems_, 32, 2019.
* [46] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [47] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In _International Conference on Machine Learning_, pages 1139-1147, 2013.
* [48] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. _Ussr Computational Mathematics and Mathematical Physics_, 4(5):1-17, 1964.
* [49] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. _Advances in Neural Information Processing Systems_, 31, 2018.
* [50] Vipin Pillai and Hamed Pirsiavash. Explainable models with consistent interpretations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 2431-2439, 2021.

## Appendix A More Results of Saliency Maps

LICO localizes more discriminative local features.Fig. 5 provides more saliency maps, showing that LICO is able to localize fine local features of target objects, _e.g._, the foot and head in Fig. 5(c).

More comprehensive locations yield lower metrics.Here, we provide some failure cases obtained by our LICO, where the saliency maps of LICO exhibit more comprehensive locations on target objects while resulting in lower AUC values of Insertion and higher AUC values of Deletion compared with baseline methods.

For the cases with multiple target objects in Fig. 6, LICO localizes different objects separately and comprehensively, which demonstrates the consistency between language knowledge and visual features established by our LICO. However, LICO obtains inferior Insertion/Deletion evaluations, which is paradoxical to human cognition. We conjecture that the main reason is attributed to the fixed text encoder, which is not specifically optimized for target tasks, _i.e._, the images with a single object occupy a higher percentage of the dataset such as ImageNet-1k. Therefore, the located multiple objects may be harmful to saliency maps-guide pixel-level insertion and deletion strategies.

In Fig. 7, we provide some examples that contain multi-class objects in one image, and for the target classes, LICO obtains more accurate localizations. In Fig. 8, we verified the effectiveness of LICO on capturing more objects of sinfle class within one image, and we can see that LICO captures more comprehensive objects than those without LICO.

## Appendix B Pointing Game

To verify the localization ability of LICO, we conducted the pointing game on MS COCO 2017 validation set for localization evaluation. Following settings in Score-CAM and Group-CAM, we quantified localization by calculating \(\frac{\mathrm{Hits}}{\mathrm{Hits}+\mathrm{Mises}}\), assessing if salient pixels fall within the annotated bounding boxes. Table 8 shows that LICO consistently improves all the baseline interpretation methods, indicating the effectiveness of regularization by the proposed manifold OT losses.

Figure 5: More samples on ImageNet-1k. For each image, we provide saliency maps of different methods and Insertion/Deletion curves. LICO helps localize fine local features.

## Appendix C Evaluation on ViT

We further trained a ViT-Base-16 network on ImageNet-1k dataset and presented the corresponding accuracy, insertion, and deletion in Table 9. We calculated the \(\mathcal{L}_{\text{MM}}\) and \(\mathcal{L}_{\text{OT}}\) between language tokens and representations of patch tokens and class tokens. For attention maps, we applied Grad-CAM in LIGO-trained ViT-Base-16 by calculating gradients from outputs to the last attention layer of the class token. Table 9 further confirms that the transformer model with LIGO not only performs better in classification but also gains better interpretability than the one without LIGO, which is in line with the finding for the CNN-based backbone.

## Appendix D t-SNE Visualization of Latent Features

We further explore how LIGO can regularize the latent space. In Fig. 9, we provide the t-SNE visualization of learned visual features on CIFAR-10 and SVHN. In Fig. 9(a), our LICO enables different classes with similar distribution shapes. Interestingly, if we roughly categorize the ten

Figure 6: Saliency maps of failure cases with multiple target objects on ImageNet-1k. For each image, we provide saliency maps of different methods and Insertion/Deletion curves.

Figure 7: Saliency maps of multi-class cases.

[MISSING_PAGE_EMPTY:16]

## Appendix E Ablation Studies

Ablation on dynamic context (DC).In the training algorithm of LICO, we designed a dynamic context (DC) for learnable context. We provide some results on ablation of DC in Tables 10 and 11. We can see that, for most of datasets we evaluated, DC consistently improves the classification performance. For few-shot settings in Table 11, DC obtains slight performance improvements, which is attributed to few samples of each class. That is to say, there are insufficient visual feature variety to endow context vectors with various information.

Variation of KL-divergence values between prompts and visual features \(f(\bm{\theta})\).Here, we provide curves of KL-divergence with and without LICO. The KL values obtained by baseline w/o LICO are calculated between CE-trained visual features and fixed CLIP pre-trained vectors, and those obtained by w/ LICO are calculated between visual features and prompts learned by LICO. In Fig. 10, we can see that LICO enables stable descent of KL values, and this indicates that the LICO learned visual features indeed approach the learned language prompts. However, the values obtained by baseline without LICO fluctuate to some extent and cannot decrease further, which is attributed to the domain gap between CLIP pre-trained data and target CIFAR-10. In other words, without manifold matching, it is difficult to enable downstream visual features to be aligned with CLIP pre-trained language knowledge.

## Appendix F Different Text Encoders

Both Word2Vec (W2V) and BERT can replace the CLIP text encoder in our LICO framework. However, they may be inferior in acting as language guidance in LICO: (1) CLIP text encoder was trained with huge amounts of image-text pairs, and the advanced architecture ViT-B/32 has stronger representation ability than W2V; (2) Even though BERT excels at representing language data, latent representation of BERT may not align with target image representations well; (3) Recent studies on prompt learning like CoCoOp has demonstrated the effectiveness of CLIP pre-trained text encoders for downstream tasks.

In Table 12, we conducted the experiments using W2V as a text encoder, mapping texts into 512-D. W2V: using fixed word embeddings and no context tokens for OT loss. W2V-P: replacing class tokens in the prompts, the context tokens are randomly initialized as Gaussian. We can see that W2V

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline  & \multicolumn{4}{c}{CUB-200} & \multicolumn{4}{c}{Standford-Cars} \\ \hline DC & 1-shot & 5-shot & 10-shot & Full & 1-shot & 5-shot & 10-shot & Full \\ \hline \(\bm{\mathcal{X}}\) & **16.9\({}_{\pm 0.2}\)** & **55.8\({}_{\pm 0.4}\)** & 68.2\({}_{\pm 0.2}\) & 82.5\({}_{\pm 0.3}\) & 7.0\({}_{\pm 0.3}\) & 37.1\({}_{\pm 0.5}\) & 64.1\({}_{\pm 0.4}\) & 90.3\({}_{\pm 0.2}\) \\ \(\bm{\checkmark}\) & **16.9\({}_{\pm 0.2}\)** & **55.8\({}_{\pm 0.4}\)** & **68.4\({}_{\pm 0.2}\)** & **82.7\({}_{\pm 0.3}\)** & **7.1\({}_{\pm 0.3}\)** & **37.2\({}_{\pm 0.5}\)** & **64.4\({}_{\pm 0.4}\)** & **91.5\({}_{\pm 0.2}\)** \\ \hline \hline  & \multicolumn{4}{c}{Aircraft} & \multicolumn{4}{c}{VGG Flowers} \\ \hline DC & 1-shot & 5-shot & 10-shot & Full & 1-shot & 5-shot & 10-shot & Full \\ \hline \(\bm{\mathcal{X}}\) & 8.0\({}_{\pm 0.3}\) & 27.4\({}_{\pm 0.3}\) & 42.8\({}_{\pm 0.2}\) & **85.6\({}_{\pm 0.3}\)** & 55.3\({}_{\pm 0.4}\) & 85.7\({}_{\pm 0.4}\) & 92.7\({}_{\pm 0.2}\) & 96.2\({}_{\pm 0.4}\) \\ \(\bm{\checkmark}\) & **8.4\({}_{\pm 0.4}\)** & **27.5\({}_{\pm 0.2}\)** & **43.1\({}_{\pm 0.4}\)** & **85.6\({}_{\pm 0.2}\)** & **55.6\({}_{\pm 0.4}\)** & **86.2\({}_{\pm 0.3}\)** & **93.4\({}_{\pm 0.4}\)** & **96.8\({}_{\pm 0.3}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Ablation on DC on four fine-grained datasets.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Model & Accuracy & Insertion & Deletion \\ \hline ViT-Base-16 w/ LICO & 77.9 & 55.2 & 14.4 \\ ViT-Base-16 w/ LICO & **78.2** & **56.0** & **13.8** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Evaluation on ImageNet-1k using ViT-Base-16.

performed comparably to the None encoder baseline. W2V-P also fails, where the fixed random context tokens mislead the image features. In contrast, the CLIP outperforms both of them. The fixed W2V embedding vectors struggled to correlate well with target image features. Moreover, BERT surpassed W2V and W2V-P due to the generalizability of a stronger pre-trained model. However, BERT still cannot achieve better performances than CLIP. Furthermore, BERT-ALIGN performs competitively and even better than CLIP, which is attributed to its larger training set with more noisy image-text pairs. Consequently, from the results in Table 12 and this paper, we conclude that LICO works better with those vision-language pre-trained text encoders. Pure text encoders like W2V, even the pre-trained BERT with frozen parameters, are inferior in image-text alignment in LICO because the pre-trained parameters are not sensitive to visual features. This deficiency may be addressed by utilizing some transfer learning and domain adaptation tricks.

## Appendix G Frozen Parameters of Prompts

To evaluate the model performance with frozen parameters, we then conducted experiments on CIFAR-10 and ImageNet under two settings of frozen parameters: (1) frozen random initialization (Random) and (2) fixed form of 'a photo of a [CLS]' (Fixed). The insertion and deletion values are obtained by Grad-CAM + LICO.

In Table 13, we can see that the frozen random parameters cannot enable the models to achieve higher performances of accuracy, insertion, and deletion; the reason is that the well-trained CLIP text encoder is capable of sensing the human-understandable phrases and sentences while the random prompts lead to the difficulty in image-text alignment and yield inaccurate semantic representations. However, the form of 'a photo of a [CLS]' performs better than frozen random parameters because this meaningful prompt is more consistent with the input of the original CLIP so that the generated representations can be easily aligned with image representations.

\begin{table}
\begin{tabular}{l c c c|l c c} \hline \hline ImageNet & Top-1\(\uparrow\) & Insertion\(\uparrow\) & Deletion\(\downarrow\) & CIFAR-10 & Full, Acc. & 4000, Acc. \\ \hline Random & 75.88 & 53.3 & 17.8 & Random & 94.9 & 79.5 \\ Fixed & 76.20 & 55.2 & 17.4 & Fixed & 95.4 & 80.7 \\ Learnable (**ours**) & **76.27** & **57.1** & **15.1** & Learnable (**ours**) & **95.8** & **81.5** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Evaluations on frozen parameters of prompts. “Acc.” is short for “Accuracy”.

Figure 10: Variation of KL-divergence value w/ and w/o LICO on CIFAR-10 test set.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Encoder & CLIP & W2V & W2V-P & BERT & BERT-ALIGN & None \\ \hline Full & **95.8** & 95.6 & 94.9 & 95.7 & **95.8** & 95.6 \\
4000 & 81.5 & 81.0 & 80.2 & 81.3 & **81.7** & 80.9 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Comparison of different text encoders on CIFAR-10.