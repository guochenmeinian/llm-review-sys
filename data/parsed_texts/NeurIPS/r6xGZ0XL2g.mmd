# Meta-Learning Adversarial Bandit Algorithms

 Mikhail Khodak

Carnegie Mellon University

khodak@cmu.edu

&Ilya Osadchiy

Technion - Israel Institute of Technology

osadchiy.ilya@gmail.com

&Keegan Harris

CMU

&Maria-Florina Balcan

CMU

&Kfir Y. Levy

Technion

&Ron Meir

Technion

&Zhiwei Steven Wu

CMU

###### Abstract

We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD.

## 1 Introduction

Learning-to-learn [51] is an important area of research that studies how to improve the performance of a learning algorithm by _meta-learning_ its parameters--e.g. initializations, step-sizes, and/or representations--across many similar tasks. The goal is to encode information from previous tasks in order to achieve better performance on future ones. Meta-learning has seen a great deal of experimental work [24, 49], practical impact [21, 29], and theoretical effort [11, 18, 22, 45, 20]. One important setting is online-within-online meta-learning [19, 31], where the learner performs a sequence of tasks, each of which has a sequence of rounds. Past work has studied the _full-information_ setting, where the loss for every arm is revealed after each round. This assumption is not realistic in many applications, e.g. recommender systems and experimental design, where often partial or _bandit_ feedback--only the loss of the action taken--is revealed. Such feedback can be _stochastic_, e.g. the losses are i.i.d. from some distribution, or _adversarial_, i.e. chosen by an adversary. We establish the first formal guarantees for online-within-online meta-learning with adversarial bandit feedback.

As with past full-information meta-learning results, our goal when faced with a sequence of bandit tasks will be to achieve low regret _on average_ across them. Specifically, our task-averaged regret should (a) be no worse than that of algorithms for the single-task setting, e.g. if the tasks are not very similar, and should (b) be much better on tasks that are closely related, e.g. if the same small set of arms do well on all of them. We show that a natural way to achieve both is to initialize and tune online mirror descent (OMD), an algorithm associted with a strictly convex regularizer whose hyperparameters have a significant impact on performance. Our approach works because it can learn the best hyperparameters in hindsight across tasks, which will recover OMD's worst-case optimal performance if the tasks are dissimilar but will take advantage of more optimistic settings if they are related. As generalized distances, the regularizers also induce interpretable measures of similarity between tasks.

### Main contributions

We design a meta-algorithm (Algorithm 1) for learning variants of OMD--specifically those with entropic or self-concordant regularizers--that are used for adversarial bandits. This meta-algorithm combines three _full-information_ algorithms--follow-the-leader (FTL), exponentially weighted online optimization (EWOO), and multiplicative weights (MW)--to set the initialization, step-size, and regularizer-specific parameters, respectively. It works by optimizing a sequence of functions that each _upper-bound_ the regret of OMD on a single task (Theorem 2.1), resulting in (a) interesting notions of task-similarity because these functions depend on generalized notions of distances (Bregman divergences) and (b) adaptivity, i.e not needing to know how similar the tasks are beforehand.

Our first application is to OMD with the Tsallis regularizer [3], a relative of Exp3 [6] that is optimal for adversarial MAB. We bound the task-averaged regret by the Tsallis entropy of the _estimated_ optima-in-hindsight (Corollary 3.1), which we further extend to that of the _true_ optima by assuming a gap between the best and second-best arms (Corollary 3.2). Both results are the first known consequences of the online learnability of Bregman divergences that are _non-convex_ in their second arguments [31], while the latter is obtained by showing that the loss estimators of a modified algorithm identify the optimal arm w.h.p. As an example, our average \(m\)-round regret across \(T\) tasks under the gap assumption is

\[o_{T}(\text{poly}(m))+2\min_{\beta\in(0,1]}\sqrt{H_{\beta}d^{\beta}m/\beta}+o (\sqrt{m})\] (1)

where \(d\) is the number of actions and \(H_{\beta}\) is the Tsallis entropy [52, 3] of the distribution of the optimal actions (\(\beta=1\) recovers the Shannon entropy).1 This entropy is low if all tasks are usually solved by the same few arms, making it a natural task-similarity notion. For example, if only \(s\ll d\) of the arms are ever optimal then \(H_{\beta}=\mathcal{O}(s)\), so using \(\beta=1/\log d\) in (1) yields an asymptotic task-averaged regret of \(\mathcal{O}(\sqrt{sm\log d})\), dropping fast terms. For \(s=\mathcal{O}_{d}(1)\) this beats the minimax optimal rate of \(\Theta(\sqrt{dm})\)[5]. On the other hand, since \(H_{1/2}=\mathcal{O}(\sqrt{d})\), the same bound recovers this rate in the worst-case of dissimilar tasks.

Footnote 1: We use \(\mathcal{O}_{n}(\cdot)\) (and \(o_{n}(\cdot)\)) to denote terms with constant (and sub-constant) dependence on \(n\).

Lastly, we adapt our meta-algorithm to the adversarial BLO problem by setting the regularizer to be a self-concordant barrier function, as in Abernethy et al. [2]. Our bounds yield notions of task-similarity that depend on the constraints of the action space, e.g. over the sphere the measure is the closeness of the average of the estimated optima to the sphere's surface (Corollary 4.1). We also instantiate BLO on the bandit shortest-path problem (Corollary 4.2) [50, 30].

### Related work

While we are the first to consider meta-learning under adversarial bandit feedback, many have studied meta-learning in various _stochastic_ bandit settings [9, 34, 47, 48, 35, 13, 15, 41, 10]. The latter three study stochastic bandits under various task-generation assumptions, e.g. Azizi et al. [10] is in a batch-within-online setting where the optimal arms are adversarial. In contrast, we make no distributional assumptions either within or without. Apart from this difference, the results of Azizi et al. [10] are the ones our MAB results are most easily compared to, which we do in detail in Section 3. Notably, they assume that only \(s\ll d\) of the \(d\) arms are ever optimal across \(T\) tasks and show (roughly speaking) \(\tilde{\mathcal{O}}(\sqrt{sm})\) asymptotic regret; we instead focus on an entropic notion of task-similarity that achieves the same asymptotic regret when specialized to their \(s\ll d\). However, avoiding their explicit assumption has certain advantages, e.g. robustness in the presence of \(o(T)\) outlier tasks (c.f. Section 3.3).

A setting that bears some similarity to online-within-online bandits is that of switching bandits [6], and more generally online learning with dynamic comparators [4, 27, 38, 7, 55]. In such problems, instead of using a static best arm as the comparator we use a piecewise constant sequence of arms, with a limited number of arm switches. The key difference between such work and ours is our assumption that task-boundaries are known; this makes the other setting more general. However, while e.g. Exp3.S [6] can indeed be applied to online meta-learning, guarantees derived from switching costs _cannot_ improve upon just running Tsallis-INF on each task [39, Table 1]. Furthermore, these approaches usually quantify difficulty by the number of switches, whereas we focus on task-similarity. While there exists stochastic-setting work that measures difficulty using a notion of average change in distribution across rounds [53], it does not lead to improved performance if this average change is \(\Omega(T)\), as is the case in e.g. the \(s\)-sparse setting discussed above.

There has been a variety of work on full-information online-within-online meta-learning [32; 12], including tuning OMD [31; 19]. Doing so for bandit algorithms has many additional challenges, including (1) their inherent and high-variance stochasticity, (2) the use of non-Lipschitz and even unbounded regularizers, and (3) the lack of access to task-optima in order to adapt to deterministic, algorithm-independent task-similarity measures. Theoretically our analysis draws on the average regret-upper-bound analysis (ARUBA) framework [31], which observes that OMD can be tuned by targeting its upper bounds, which are affine functions of Bregman divergences, and provide online learning tools for doing so. Our core structural result shows that the distance generating functions \(\psi_{\theta}\) of these Bregman divergences can be tuned without interfering with meta-learning the initialization and step-size; tuning \(\theta\) is critical for adapting to settings such as that of a small set of optimal arms in MAB. Doing so depends on several refinements of the original approach, including bounding the task-averaged-regret via the spectral norm of \(\nabla^{2}\psi_{\theta}\) and expressing the loss of the meta-comparator using only \(\psi_{\theta}\), rather than via its Bregman divergence as in prior work. Finally, applying our structural result requires setting-specific analysis, e.g. to show regularity w.r.t. \(\theta\) or to obtain MAB guarantees in terms of the entropy of the true optimal arms. The latter is especially difficult, as Khodak et al. [31] define task-similarity via full information upper bounds, and involves applying tools from the best-arm-identification literature [1] to show that a constrained variant of Exp3 finds the optimal arm w.h.p.

## 2 Learning the regularizers of bandit algorithms

We consider the problem of meta-learning over bandit tasks \(t=1,\ldots,T\) over some fixed set \(\mathcal{K}\subset\mathbb{R}^{d}\), a (possibly improper) subset of which is the action space \(\mathcal{A}\). On each round \(i=1,\ldots,m\) of task \(t\) we play action \(\mathbf{x}_{t,i}\in\mathcal{A}\) and receive feedback \(\ell_{t,i}(\mathbf{x}_{t,i})\) for some function \(\ell_{t,i}:A\mapsto[-1,1]\). Note that all functions we consider will be linear and so we will also write \(\ell_{t,i}(\mathbf{x})=\langle\ell_{t,i},\mathbf{x}\rangle\). Additionally, we assume the adversary is _oblivious within-task_, i.e. it chooses losses \(\ell_{t,1},\ldots,\ell_{t,m}\) at time \(t\). We will also denote \(\mathbf{x}(a)\) to be the \(a\)-th element of the vector \(\mathbf{x}\in\mathbb{R}^{d},\mathcal{K}^{\circ}\) to be the interior of \(\mathcal{K}\), \(\partial\mathcal{K}\) its boundary, and \(\triangle\) to be the simplex on \(d\) elements. Finally, note that all proofs can be found in the Appendix.

In online learning, the goal on a single task \(t\) is to play actions \(\mathbf{x}_{t,1},\ldots\mathbf{x}_{t,m}\) that minimize the regret \(\sum_{i=1}^{m}\ell_{t,i}(\mathbf{x}_{t,i})-\ell_{t,i}(\hat{\mathbf{x}}_{t})\), where \(\hat{\mathbf{x}}_{t}\in\arg\min_{\mathbf{x}\in\mathcal{K}}\sum_{i=1}^{m}\ell_ {t,i}(\mathbf{x})\). Lifting this to the meta-learning setting, our goal as in past work [31; 19] will be to minimize the **task-averaged regret**: \(\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(\mathbf{x}_{t,i})-\ell_{t,i} (\hat{\mathbf{x}}_{t})\). In particular, we want to use multi-task data to improve average performance as the number of tasks \(T\to\infty\). For example, we wish to attain a task-averaged regret bound of the form \(o_{T}(\text{poly}(m))+\tilde{\mathcal{O}}(V\sqrt{m})+o(\sqrt{m})\), where \(V\in\mathbb{R}_{\geq 0}\) is a measure of task-similarity that is small if the tasks are similar but still yields the worst-case single-task performance--\(\mathcal{O}(\sqrt{dm})\) for MAB and \(\mathcal{O}(d\sqrt{m})\) for BLO--if they are not.

### Online mirror descent as a base-learner

In meta-learning we are commonly interested in learning a within-task algorithm or **base-learner**, a parameterized method that we run on each task \(t\). A popular approach is to learn the initialization and other parameters of a gradient-based method such as gradient descent [24; 44; 36]. If the task optima are close, the best initialization should perform well after only a few steps on a new task. We take a similar approach applied to online mirror descent, a generalization of gradient descent to non-Euclidean geometries [14]. Given a strictly convex **regularizer**\(\psi:\mathcal{K}^{\circ}\mapsto\mathbb{R}\), step-size \(\eta>0\), and initialization \(\mathbf{x}_{t,1}\in\mathcal{K}^{\circ}\), OMD has the iteration

\[\mathbf{x}_{t,i+1}=\operatorname*{arg\,min}_{\mathbf{x}\in\mathcal{K}^{\circ}}B (\mathbf{x}\|\mathbf{x}_{t,1})+\eta\sum_{j\leq i}\langle\nabla\ell_{t,j}( \mathbf{x}_{t,j}),\mathbf{x}\rangle\] (2)

where \(B(\mathbf{x}\|\mathbf{y})=\psi(\mathbf{x})-\psi(\mathbf{y})-\langle\nabla\psi( \mathbf{y}),\mathbf{x}-\mathbf{y}\rangle\) is the **Bregman divergence** of \(\psi\). OMD recovers online gradient descent when \(\psi(\mathbf{x})=\frac{1}{2}\|\mathbf{x}\|_{2}^{2}\), in which case \(B(\mathbf{x}\|\mathbf{y})=\frac{1}{2}\|\mathbf{x}-\mathbf{y}\|_{2}^{2}\); another example is exponentiated gradient, for which \(\psi(\mathbf{p})=\langle\mathbf{p},\log\mathbf{p}\rangle\) is the negative Shannon entropy on probability vectors \(\mathbf{p}\in\triangle\) and \(B\) is the KL-divergence [46]. An important property of \(B\) is that the sum over functions \(B(\mathbf{x}_{t}||\cdot)\) is minimized at the mean \(\bar{\mathbf{x}}\) of the points \(\mathbf{x}_{1},\ldots,\mathbf{x}_{T}\).

OMD on **loss estimators**\(\hat{\ell}_{t,i}\) constructed via partial feedback forms an important class of bandit methods [6; 2; 3]. Their regularizers \(\psi\) are often non-Lipschitz, e.g. the negative entropy, or even unbounded, e.g. the log-barrier. Thus full-information results for tuning OMD, e.g. by Khodak et al. [31] and Denevi et al. [19], do not suffice. We do adapt the former's approach of online learning a sequence \(U_{t}(\mathbf{x},\eta,\theta)\) of affine functions of Bregman divergences from initializations \(\mathbf{x}\) to known points in \(\mathcal{K}\). We are interested in them because the regret of OMD w.r.t. a comparator \(\mathbf{y}\) is bounded by \(B(\mathbf{y}||\mathbf{x})/\eta+\mathcal{O}(\eta m)\)[46; 25]. In our case the operator is based on the estimated optimum \(\hat{\mathbf{x}}_{t}\in\operatorname*{arg\,min}_{\mathbf{x}\in\mathcal{K}} \langle\hat{\ell}_{t},\mathbf{x}\rangle\), where \(\hat{\ell}_{t}=\sum_{i=1}^{m}\hat{\ell}_{t,i}\), resulting from running OMD on task \(t\) using initialization \(\mathbf{x}\in\mathcal{K}\) and hyperparameters \(\eta\) and \(\theta\), which we denote \(\texttt{OMD}_{\eta,\theta}(\mathbf{x})\). Unlike full-information meta-learning, we use a parameter \(\varepsilon>0\) to constrain this optimum to lie in a subset \(\mathcal{K}_{\varepsilon}\subset\mathcal{K}^{\circ}\). Formally, we fix a point \(\mathbf{x}_{1}\in\mathcal{K}^{\circ}\) to be the "center"--e.g. \(\mathbf{x}_{1}=\mathbf{1}_{d}/d\) when \(\mathcal{K}\) is the \(d\)-simplex \(\triangle\)--and define the projection \(\mathbf{c}_{\varepsilon}(\mathbf{x})=\mathbf{x}_{1}+\frac{\mathbf{x}- \mathbf{x}_{1}}{1+\varepsilon}\) mapping from \(\mathcal{K}\) to \(\mathcal{K}_{\varepsilon}\). For example, \(\mathbf{c}_{\frac{\varepsilon}{1+\varepsilon}}(\mathbf{x})=(1-\varepsilon) \mathbf{x}+\varepsilon\mathbf{1}_{d}/d\) on the simplex. This projection allows us to handle regularizers \(\psi\) that diverge near the boundary, but also introduces \(\varepsilon\)-dependent error terms. In the BLO case it also forces us to tune \(\varepsilon\) itself, as initializing too close to the boundary leads to unbounded regret while initializing too far away does not take advantage of task-similarity. Thus the general upper bounds of interest are the following functions of the initialization \(\mathbf{x}\), the step-size \(\eta>0\), and a third parameter \(\theta\) that is either \(\beta\) or \(\varepsilon\), depending on the setting (MAB or BLO):

\[U_{t}(\mathbf{x},\eta,\theta)=\frac{B_{\theta}(\mathbf{c}_{\theta}(\hat{ \mathbf{x}}_{t})\|\mathbf{x})}{\eta}+\eta g(\theta)m+f(\theta)m\] (3)

Here \(B_{\theta}\) is the Bregman divergence of \(\psi_{\theta}\) while \(g(\theta)\geq 1\) and \(f(\theta)\geq 0\) are tunable constants. We overload \(\theta\) to be either \(\beta\) or \(\varepsilon\) for notational simplicity, as we will not tune them simultaneously; if \(\theta=\beta\) (for MAB) then \(c_{\theta}(\mathbf{x})=\mathbf{x}_{1}+\frac{\mathbf{x}-\mathbf{x}_{1}}{1+\varepsilon}\) for fixed \(\varepsilon\), while if \(\theta=\varepsilon\) (for BLO) then \(B_{\theta}\) is the Bregman divergence of a fixed \(\psi\). The reason to optimize this sequence of upper bounds \(U_{t}\) is because they directly bound the task-averaged regret while being no worse than the worst-case single-task regret. Furthermore, an average over Bregman divergences is minimized at the average \(\hat{\mathbf{\mathbf{\hat{x}}}}=\frac{1}{T}\sum_{t=1}^{T}\hat{\mathbf{x}}_{t}\), where it attains the value \(\hat{V}_{\theta}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\theta}(\mathbf{c}_{\theta }(\hat{\mathbf{x}}_{t}))-\psi_{\theta}(\mathbf{c}_{\theta}(\hat{\mathbf{ \mathbf{\hat{x}}}}))\) (c.f. Claim A.1). We will show that this quantity leads to intuitive and interpretable notions of task-similarity in all the applications we study.

### A meta-algorithm for tuning bandit algorithms

To learn these functions \(U_{t}(\mathbf{x},\eta,\theta)\)--and thus to meta-learn \(\texttt{OMD}_{\eta,\theta}(\mathbf{x})\)--our meta-algorithm sets \(\mathbf{x}\) to be the projection \(\mathbf{c}_{\theta}\) of the mean of the estimated optima--i.e. follow-the-leader (FTL) over the Bregman divergences in (3)--while simultaneously setting \(\eta\) via EWOO and \(\theta\) via discrete multiplicative weights (MW). We choose FTL, EWOO, and MW because each is well-suited to the way \(U_{t}\) depends on \(\mathbf{x}\), \(\eta\), and \(\theta\), respectively. First, the only effect of \(\mathbf{x}\) on \(U_{t}\) is via the Bregmandivergence \(B_{\theta}(\mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})||\mathbf{x})\), over which FTL attains logarithmic regret [31]. For \(\eta\), \(U_{t}\) is exp-concave on \(\eta>0\) so long as the first term is nonzero, but it is also non-Lipschitz; the EWOO algorithm is one of the few methods with logarithmic regret on exp-concave losses without a dependence on the Lipschitz constant [26], and we ensure the first term is nonzero by _regularizing_ the upper bounds as follows for some \(\rho>0\) and \(D_{\theta}^{2}=\max_{\mathbf{x},\mathbf{y}\in\mathcal{K}_{\theta}}B_{\theta}( \mathbf{x}||\mathbf{y})\):

\[U_{t}^{(\rho)}(\mathbf{x},\eta,\theta)=\frac{B_{\theta}(\mathbf{c}_{\theta}( \hat{\mathbf{x}}_{t})||\mathbf{x})+\rho^{2}D_{\theta}^{2}}{\eta}+\eta g(\theta )m+f(\theta)m\] (4)

Note that this function is fully defined after obtaining \(\hat{\mathbf{x}}_{t}\) by running OMD on task \(t\), which allows us to use full-information MW to tune \(\theta\) across the grid \(\Theta_{k}\). Showing low regret w.r.t. any \(\theta\in\Theta\supset\Theta_{k}\) then just requires sufficiently large \(k\) and Lipschitzness of \(U_{t}\) w.r.t. \(\theta\). Combining all three algorithms together thus yields the guarantee in Theorem 2.1, which is our main structural result. It implies a generic approach for obtaining meta-learning algorithms by (1) bounding the task-averaged regret by an average of functions of the form \(U_{t}\), (2) applying the theorem to obtain a new bound \(o_{T}(1)+\min_{\theta,\eta}\frac{\hat{V}_{\theta}^{2}}{\eta}+\eta g(\theta)m+ f(\theta)m\), and (3) bounding the estimated task-similarity \(\hat{V}_{\theta}^{2}\) by an interpretable quantity. Crucially, since we can choose any \(\eta>0\), the asymptotic regret is always as good as the worst-case guarantee for running the base-learner separately on each task.

**Theorem 2.1** (c.f. Thm. A.1).: _Suppose \(\mathbf{x}_{1}=\arg\min_{\mathbf{x}\in\mathcal{K}}\psi_{\theta}(\mathbf{x})\ \forall\ \theta\) and let \(D\), \(M\), \(F\), and \(S\) be maxima over \(\theta\) of \(D_{\theta}\), \(D_{\theta}\sqrt{g(\theta)m}\), \(f(\theta)\), and \(\|\nabla^{2}\psi_{\theta}\|_{2}\), respectively. For each \(\rho\in(0,1)\) we can set \(\underline{\eta}\), \(\overline{\eta}\), \(\alpha\), and \(\lambda\) s.t. the expected average of the losses \(U_{t}(\mathbf{c}_{\theta_{t}}(\mathbf{x}_{t}),\eta_{t}(\theta_{t}),\theta_{t})\) of Algorithm 1 is at most_

\[\min_{\theta\in\Theta,\eta>0}\frac{\mathbb{E}\hat{V}_{\theta}^{2}}{\eta}+\eta g (\theta)m+f(\theta)m+\tilde{\mathcal{O}}\left(\frac{\frac{M}{\rho}+Fm}{\sqrt{ T}}+\frac{L_{\eta}}{k}+\frac{M}{\rho^{2}T}+\min\left\{\frac{\rho^{2}D^{2}}{ \eta},\rho M\right\}+\frac{S}{\eta T}\right)\] (5)

_Here \(\hat{V}_{\theta}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\theta}(\mathbf{c}_{ \theta}(\hat{\mathbf{x}}_{t}))-\psi_{\theta}(\mathbf{c}_{\theta}(\hat{ \mathbf{x}}))\) and \(L_{\eta}\) bounds the Lipschitz constant w.r.t. \(\theta\) at \(\hat{V}_{\theta}^{2}/\eta+\eta g(\theta)m+f(\theta)m\). The same bound plus \((M/\rho+Fm)\sqrt{\frac{1}{T}\log\frac{1}{\delta}}\) holds w.p. \(\geq 1-\delta\)._

We keep details of the dependence on \(S\) and other constants as they are important in applying this result, but in most cases setting \(\rho=\frac{1}{\sqrt{T}}\) yields \(\tilde{\mathcal{O}}(T^{\frac{3}{4}})\) regret. While a slow rate, the losses \(U_{t}\) are non-Lipschitz and non-convex in-general, and learning them allows us to tune \(\theta\) over user-specified intervals and \(\eta\) over all positive numbers, which will be crucial later. At the same time, this tuning is what leads to the slow rate, as without tuning (\(k=1\), \(L_{\eta}=0\)) the same \(\rho\) yields \(\tilde{\mathcal{O}}(\sqrt{T})\) regret. Lastly, while we focus on learning guarantees, we note that Algorithm 1 is reasonably efficient, requiring a \(2k\) single-dimensional integrals per task; this is discussed in more detail in Section A.3.

## 3 Multi-armed bandits

We now turn to our first application: the multi-armed bandit problem, where at each round \(i\) of task \(t\) we take action \(a_{t,i}\in[d]\) and observe loss \(\ell_{t,i}(a_{t,i})\in[0,1]\). As we are sampling actions from distributions \(\mathbf{x}\in\mathcal{K}=\bigtriangleup\) on the \(k\)-simplex, the inner product \(\langle\ell_{t,i},\mathbf{x}_{t,i}\rangle\) is the expected loss and the optimal arm \(\hat{a}_{t}\) on task \(t\) can be encoded as a vector \(\hat{\mathbf{x}}_{t}\) s.t. \(\hat{\mathbf{x}}_{t}(a)=1_{a=\hat{a}_{t}}\).

We use as a base-learner a generalization of Exp3 that uses the negative Tsallis entropy \(\psi_{\beta}(\mathbf{p})=\frac{1-\sum_{t=1}^{d}\mathbf{p}^{\beta}(a)}{1-\beta}\) for some \(\beta\in(0,1]\) as the regularizer; this improves regret from Exp3's \(\mathcal{O}(\sqrt{dm\log d})\) to the optimal \(\mathcal{O}(\sqrt{dm})\)[3]. Note that \(-\psi_{\beta}\) is the Shannon entropy in the limit \(\beta\to 1\) and its Bregman divergence \(B_{\beta}(\mathbf{x}||\cdot)\) is non-convex in the second argument. As the Tsallis entropy is non-Lipschitz at the simplex boundary, which is where the estimated and true optima \(\hat{\mathbf{x}}_{t}\) and \(\hat{\mathbf{x}}_{t}\) lie, we will project them using \(\mathbf{c}_{\frac{\varepsilon}{1-\varepsilon}}(\mathbf{x})=(1-\varepsilon) \mathbf{x}+\varepsilon\mathbf{1}_{d}/d\) to the set \(\mathcal{K}_{\frac{\varepsilon}{1-\varepsilon}}=\{\mathbf{x}\in\bigtriangleup: \min_{a}\mathbf{x}(a)\geq\varepsilon/d\}\). We denote the resulting vectors using the superscript \((\varepsilon)\), e.g. \(\hat{\mathbf{x}}_{t}^{(\varepsilon)}=\mathbf{c}_{\frac{\varepsilon}{1- \varepsilon}}(\hat{\mathbf{x}}_{t})\), and also use \(\bigtriangleup^{(\varepsilon)}=\mathcal{K}_{\frac{\varepsilon}{1-\varepsilon}}\) to denote the constrained simplex. For MAB we also study two base-learners: (1) **implicit exploration** and (2) **guaranteed exploration**. The former uses low-variance loss _under_-estimators \(\hat{\ell}_{t,i}(a)=\frac{\ell_{t,i}(a)1_{\ell_{t,i}=a}}{\mathbf{x}_{t,i}(a)+ \gamma}\) for \(\gamma>0\), where \(\mathbf{x}_{t,i}(a)\) is the probability of sampling \(a\) on task \(t\) round \(i\), to enable high probability bounds [43]. On the other hand, **guaranteed exploration** uses unbiased loss estimators (i.e. \(\gamma=0\)) but constrains the action space to \(\bigtriangleup^{(\varepsilon)}\), which we will use to adapt to a task-similarity determined by the _true_ optima-in-hindsight.

### Adapting to low estimated entropy with high probability using implicit exploration

In our first setting, the base-learner runs \(\texttt{OMD}_{\eta_{t},\beta_{t}}(\mathbf{x}_{t,1})\) on \(\gamma\)-regularized estimators with Tsallis regularizer \(\psi_{\beta_{t}}\), step-size \(\eta_{t}\), and initialization \(\mathbf{x}_{t,1}\in\triangle^{(\varepsilon)}\). Standard OMD analysis combined with implicit exploration analysis [43] shows (44) that the task-averaged regret is bounded w.h.p. by

\[(\varepsilon+\gamma d)m+\tilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\gamma T} \right)+\frac{1}{T}\sum\limits_{t=1}^{T}\frac{B_{\beta_{t}}(\mathbf{\hat{x}}_ {t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+\frac{\eta_{t}d^{\beta_{t}}m }{\beta_{t}}\] (6)

The summands have the desired form of \(U_{t}(\mathbf{x}_{t,1},\eta_{t},\beta_{t})\), so by Theorem 2.1 we can bound their average by

\[\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta>0}\frac{\hat{V}_{ \beta}^{2}}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\tilde{\mathcal{O}}\left( \frac{L_{\eta}}{k}+\frac{\left(\frac{d}{\varepsilon}\right)^{2-\underline{ \beta}}}{\eta T}+\left(\rho+\frac{1}{\rho\sqrt{T}}+\frac{1}{\rho^{2}T}\right)d \sqrt{m}\right)\] (7)

where \(\hat{V}_{\beta}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t }^{(\varepsilon)})-\psi_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)})\) is the average difference in Tsallis entropies between the (\(\varepsilon\)-constrained) estimated optima \(\mathbf{\hat{x}}_{t}\) and their empirical distribution \(\mathbf{\hat{x}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{\hat{x}}_{t}\), while \(L_{\eta}\) is the Lipschitz constant of \(\frac{\hat{V}_{\beta}^{2}}{\eta}+\frac{\eta d^{\beta}m}{\beta}\) w.r.t. \(\beta\in[\underline{\beta},\overline{\beta}]\). The specific instantiation of Algorithm 1 that (7) holds for is to do the following at each time \(t\):

\[\begin{split} 1.&\text{sample }\beta_{t}\text{ via the MW distribution }\propto\exp(\mathbf{w}_{t})\text{ over the discretization }\Theta_{k}\text{ of }[\underline{\beta},\overline{\beta}]\subset[0,1]\\ 2.&\text{run }\texttt{OMD}_{\eta_{t},\beta_{t}} \text{ using the initialization }\mathbf{x}_{t,1}=\frac{1}{t-1}\sum_{s<t}\mathbf{\hat{x}}_{t}^{( \varepsilon)}=\frac{\varepsilon}{d}\mathbf{1}_{d}+\frac{1-\varepsilon}{t-1} \sum_{s<t}\mathbf{\hat{x}}_{t}\text{ (FTL)}\\ 3.&\text{update EWOO at each }\beta\in\Theta_{k}\text{ with loss }\frac{B_{\beta}(\mathbf{\hat{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})+\rho^{2}D _{\beta}^{2}}{\eta}+\frac{\eta d^{\beta}m}{\beta},\text{ where }D_{\beta}^{2}=\frac{d^{1-\beta}-1}{1-\beta}\\ 4.&\text{update }\mathbf{p}_{t+1}\text{ using multiplicative weights with expert losses }\frac{B_{\beta}(\mathbf{\hat{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta}+ \frac{\eta d^{\beta}m}{\beta}\end{split}\] (8)

The final guarantee for this procedure, given in full in Theorem B.1, follows by two properties of the Tsallis entropy \(-\psi_{\beta}\): (1) its Lipschitzness w.r.t. \(\beta\in[0,1]\) (c.f. Lem B.1) and (2) the fact that \(\hat{V}_{\beta}^{2}\) is bounded by the entropy \(\hat{H}_{\beta}=-\psi_{\beta}(\mathbf{\hat{x}})\) of the empirical distribution of estimated optima (c.f. Lem B.2), which yields our first notion of task-similarity: _multi-armed bandit tasks are similar if the empirical distribution of their (estimated) optimal arms has low entropy_.

We exemplify the implications of Theorem B.1 in Corollary 3.1, where we consider three regimes of the lower bound \(\underline{\beta}\) on the entropy parameter: \(\underline{\beta}=1\), i.e. always using Exp3; \(\underline{\beta}=1/2\), which corresponds to the optimal worst-case setting [3]; and \(\underline{\beta}=1/\log d\), below which the OMD regret-upper-bound always worsens (and so it does not make sense to try \(\beta<1/\log d\)).

**Corollary 3.1** (c.f. Cors. B.1, B.2, and B.3).: _Suppose \(\overline{\beta}=1\) and we set the initialization, step-size, and entropy parameter of Tsallis OMD with implicit exploration via Algorithm 1 as in Theorem B.1._

1. _If_ \(\underline{\beta}=1\) _and_ \(T\geq\frac{d^{2}}{m}\) _we can ensure_ \(\frac{1}{T}\sum\limits_{t=1}^{T}\sum\limits_{i=1}^{m}\ell_{t,i}(\mathbf{x}_{t,i })-\ell_{t,i}(\mathbf{\hat{x}}_{t})\leq 2\sqrt{\hat{H}_{1}dm}+\tilde{\mathcal{O}} \left(\frac{d^{2}_{3}m^{2}}{\sqrt[3]{T}}\right)\) _w.h.p._
2. _If_ \(\underline{\beta}=\frac{1}{2}\) _and_ \(T\geq\frac{d^{5/2}}{m}\) _we can set_ \(k=\lceil\sqrt[4]{d}\sqrt[4]{T}\rceil\) _and ensure w.h.p. that task-averaged regret is_ \[\min_{\beta\in[\frac{1}{2},1]}2\sqrt{\hat{H}_{\beta}d^{\beta}m/\beta}+\tilde{ \mathcal{O}}\left(\frac{d^{5/7}m^{5/7}}{T^{2/7}}+\frac{d\sqrt{m}}{\sqrt[4]{T}}\right)\] (9)
3. _If_ \(\underline{\beta}=\frac{1}{\log d}\) _and_ \(T\geq\frac{d^{3}}{m}\) _we can set_ \(k=\lceil\sqrt[4]{d}\sqrt[4]{T}\rceil\) _and ensure w.h.p. that task-averaged regret is_ \[\min_{\beta\in(0,1]}2\sqrt{\hat{H}_{\beta}d^{\beta}m/\beta}+\tilde{\mathcal{O}} \left(\frac{d^{3/4}m^{3/4}+d\sqrt{m}}{\sqrt[4]{T}}\right)\] (10)

In all three settings, as \(T\rightarrow\infty\) the regret scales directly with the entropy of the estimated optima-in-hindsight, which is small if most tasks are estimated to be solved by one of a few arms and large if all arms are used roughly equally. Corollary 3.1 demonstrates the importance of tuning \(\beta\): even if tasks are dissimilar, we asymptotically recover the worst-case optimal guarantee \(\mathcal{O}(\sqrt{dm})\) in cases two and three because the entropy is at most \(\frac{d^{1-\beta}}{1-\beta}\). On the other hand, if a constant \(s\ll d\) actions are always minimizers, i.e. the empirical distribution \(\mathbf{\hat{x}}\) is \(s\)-sparse, then the last bound (10)implies that Algorithm 1 can achieve task-averaged regret \(o_{T}(md)+\mathcal{O}(\sqrt{sm\log d})\). At the same time, this tuning is costly, with the last two results having an extra \(\tilde{\mathcal{O}}\left(\frac{d/\sqrt{m}}{\tilde{\mathcal{Y}}T}\right)\) term because of it. Furthermore, the bound of \(\beta=\frac{1}{2}\) has a slightly better dependence on \(d\), \(m\), and \(T\) compared to that of \(\underline{\beta}=\frac{1}{\log d}\) due to the \(\left(\frac{d}{\varepsilon}\right)^{\underline{2}-\underline{\beta}}\) term in the bound (7) returned for MAB by our structural result.

We can compare the \(s\)-sparse result to Azizi et al. [10], who achieve task-averaged regret \(\tilde{\mathcal{O}}(m/\sqrt[3]{T}+\sqrt{sm\log T})\) for _stochastic_ MAB. Despite our adversarial setting and no stipulations on how tasks are related, our bounds are asymptotically comparable if the estimated and true optima are roughly equivalent (ignoring their \(\mathcal{O}(\sqrt{\log T})\)-factor), as we also have \(\tilde{\mathcal{O}}(\sqrt{sm})\) average regret as \(T\to\infty\). Their rate in the number of tasks is better, but at a cost of runtime exponential in \(s\). Apart from generality, we believe a great strength of our results is their adaptiveness; unlike Azizi et al. [10], we do not need to know how many optimal arms there are to adapt to there being few of them.

### Adapting to the entropy of the true optima-in-hindsight using guaranteed exploration

While the entropy of estimated optima-in-hindsight may be useful in some cases where we wish to actually _compute_ the task-similarity, it is otherwise generally more desirable to adapt to an intrinsic and algorithm-independent measure, e.g. the entropy of the _true_ optima-in-hindsight. However, doing so is difficult without further assumptions, as the optima are both hard to identify and the measure itself may not be fully defined in case of ties. Thus in this section we focus on the setting where we have a nonzero performance gap \(\Delta>0\) between the best and second-best arms:

**Assumption 3.1**.: _For some \(\Delta>0\) and all tasks \(t\in[T]\), \(\frac{1}{m}\sum_{i=1}^{m}\ell_{t,i}(a)-\ell_{t,i}(\hat{a}_{t})\geq\Delta\; \forall\;a\neq\hat{a}_{t}\)._

This assumption is common in the best-arm identification literature [28; 1], which we adapt to show that the estimated optimal arms match the true optima, and thus so do their entropies. To do so, we switch to _unbiased_ loss estimators, i.e. \(\gamma=0\), and control their variance by lower-bounding the probability of selecting an arm to be at least \(\frac{\varepsilon}{d}\); this can alternatively be expressed as running OMD using the regularizer \(\psi_{\beta}+I_{\triangle^{(\varepsilon)}}\), where for any \(\mathcal{C}\subset\mathbb{R}^{d}\) the function \(I_{\mathcal{C}}(\mathbf{x})=0\) if \(\mathbf{x}\in\mathcal{C}\) and \(\infty\) otherwise. Guaranteed exploration allows us extend the analysis of Abbasi-Yadkori et al. [1] to show that the estimated arm is optimal w.h.p.:

**Lemma 3.1** (c.f. Lem C.1).: _Suppose for \(\varepsilon>0\) and any \(\beta\in(0,1]\) we run OMD on task \(t\in[T]\) with regularizer \(\psi_{\beta}+I_{\triangle^{(\varepsilon)}}\). If \(m=\tilde{\Omega}(\frac{d}{\varepsilon\Delta^{2}})\) then \(\mathbf{\hat{x}}_{t}=\mathbf{\hat{x}}_{t}\) w.p. \(\geq 1-d\exp(-\Omega(\varepsilon\Delta^{2}m/d))\)._

However, the constraint that the probabilities are at least \(\frac{\varepsilon}{d}\) does lead to \(\varepsilon m\) additional error on each task, with the upper bound on the task-averaged expected regret becoming

\[\mathbb{E}\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(a_{t,i})-\ell_{t,i }(\hat{a}_{t})\leq\varepsilon m+\frac{1}{T}\sum_{t=1}^{T}\frac{\mathbb{E}B_{ \beta_{t}}(\mathbf{\hat{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}} +\frac{\eta_{t}d^{\beta_{t}}m}{\beta_{t}}\] (11)

Moreover, we will no longer set \(\varepsilon=o_{T}(1)\), as this would require \(m\) to be _increasing_ in \(T\) for the best-arm identification result of Lemma C.1 to hold. Thus, unlike in the previous section, our results will contain "fast" terms--terms in the task-averaged regret that are \(o(\sqrt{m})\) but not decreasing in \(T\) nor affected by the task-similarity. They will still improve upon the \(\Omega(\sqrt{dm})\) MAB lower bound if tasks are similar, but the task-averaged regret will not converge to zero as \(T\to\infty\) if the tasks are identical.

Nevertheless, the tuning-dependent component of the upper bounds in (11) has the appropriate form for our structural result--in fact we can use the same meta-algorithm (8) as for implicit exploration--and so we can again apply Theorem 2.1 to get a bound on the task-averaged regret in terms of the average difference \(\hat{V}_{\beta}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t} ^{(\varepsilon)})-\psi_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)})\) of the entropies of the \(\varepsilon\)-constrained estimated task-optima \(\mathbf{\hat{x}}_{t}^{(\varepsilon)}\) and their mean \(\mathbf{\hat{x}}^{(\varepsilon)}\). The easiest way to apply Lemma C.1 to bound \(\hat{V}_{\beta}^{2}\) in terms of \(H_{\beta}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t})-\psi_ {\beta}(\mathbf{\hat{x}})\) is via union bound on all \(T\) tasks to show that \(\mathbf{\hat{x}}_{t}=\mathbf{\hat{x}}_{t}\;\forall\;t\;\text{w.p.}\geq 1-dT\exp(- \Omega(\varepsilon\Delta^{2}m/d))\); however, setting a constant failure probability leads to \(m\) growing, albeit only logarithmically, in \(T\). Instead, by analyzing the worst-case best-arm identification probabilities, we show in Lemma C.2 that the expectation of \(\hat{V}_{\beta}^{2}\) is bounded by \(H_{\beta}+3\beta\frac{(d/\varepsilon)^{1-\beta}-1}{1-\beta}\exp\left(-\frac{3 \varepsilon\Delta^{2}m}{28d}\right)\) without resorting to \(m=\omega_{T}(1)\). Assuming \(m\geq\frac{75d}{\varepsilon\Delta^{2}}\log\frac{d}{\varepsilon\Delta^{2}}\) is enough (69) to bound the second term by \(\frac{56}{d\eta n}\). Then the final result (c.f. Thm. C.1) bounds the expected task-averaged regret as follows (ignoring terms that become \(o_{T}(1)\) after setting \(\rho\) and \(k\)):\[\varepsilon m+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta>0}\frac{h_{ \beta}(\Delta)}{\eta}+\frac{\eta d^{\beta}m}{\beta}\quad\text{for}\quad h_{\beta} (\Delta)=\begin{cases}H_{\beta}+\frac{56}{md}&\text{if}\quad m\geq\frac{75d}{ \varepsilon\Delta^{2}}\log\frac{d}{\varepsilon\Delta^{2}}\\ \frac{d^{1-\beta}-1}{1-\beta}&\text{otherwise}\end{cases}\] (12)

If the gap \(\Delta\) is known and sufficiently large, then we can set \(\varepsilon=\Theta(\frac{d}{\Delta^{2}m})\) to obtain an asymptotic task-averaged regret that scales only with the entropy \(H_{\beta}\) and a fast term that is logarithmic in \(m\):

**Corollary 3.2** (c.f. Cor. C.3).: _Suppose we set the initialization, step-size, and entropy parameter of Tsallis OMD with guaranteed exploration via Algorithm 1 as specified in Theorem C.1. If \([\underline{\beta},\overline{\beta}]=[\frac{1}{\log d},1]\) and \(m\geq\frac{75d}{\Delta^{2}}\log\frac{d}{\Delta^{2}}\), then setting \(\varepsilon=\tilde{\Theta}\left(\frac{d}{\Delta^{2}m}\right)\), \(\rho=\frac{1}{\sqrt[3]{d}\sqrt[5]{mT}}\), and \(k=\lceil\sqrt[3]{d}^{2}mT\rceil\) ensures that the expected task-averaged regret is at most_

\[\min_{\beta\in(0,1]}2\sqrt{H_{\beta}d^{\beta}m/\beta}+\tilde{\mathcal{O}} \left(\frac{d}{\Delta^{2}}+\frac{d^{\frac{4}{3}}m^{\frac{3}{2}}}{\sqrt[3]{T}} +\frac{d^{\frac{5}{3}}m^{\frac{5}{6}}}{T^{\frac{3}{2}}}+\frac{d\Delta^{4}m^{3} }{T}\right)\] (13)

Knowing the gap \(\Delta\) is a strong assumption, as ideally we could set \(\varepsilon\) without it. Note that if \(\varepsilon=\Omega(\frac{1}{m^{p}})\) for some \(p\in(0,1)\) then the condition \(m\geq\frac{75d}{\varepsilon\Delta^{2}}\log\frac{d}{\varepsilon\Delta^{2}}\) only fails if \(m\leq\text{poly}(\frac{1}{\Delta})\), i.e. for gap decreasing in \(m\). We can use this together with the fact that minimizing over \(\eta\) and \(\beta\) in our bound allows us to replace them with any value, even a gap-dependent one, to derive a gap-_independent_ setting of \(\varepsilon\) that ensures a task-similarity-adaptive bound when \(\Delta\) is not too small and falls back to the worst-case optimal guarantee otherwise. Specifically, for indicator \(\iota_{\Delta}=1_{m\geq\frac{75d}{\varepsilon\Delta^{2}}\log\frac{d}{ \varepsilon\Delta^{2}}}\), setting \(\eta=\Theta\left(\sqrt{\frac{h_{\beta}(\Delta)}{d^{2}m/\beta}}\right)\) in (12) and using \(\beta=\frac{1}{2}\) if the condition \(\iota_{\Delta}\) fails yields asymptotic regret at most

\[\varepsilon m+\min_{\beta\in(0,1]}\mathcal{O}\!\!\left(\!\iota_{\Delta}\sqrt {\frac{H_{d}d^{\beta}m}{\beta}}+(1-\iota_{\Delta})\!\sqrt{dm}\!\!\right)\! \leq\varepsilon m+\tilde{\mathcal{O}}\!\left(\!\min\left\{\min_{\beta\in(0,1]} \sqrt{\frac{H_{d}d^{\beta}m}{\beta}}+\frac{d}{\Delta\sqrt[5]{\varepsilon}}, \sqrt[4]{dm}\!\right)\!\!\right)\] (14)

Thus setting \(\varepsilon=\Theta(\sqrt{d}/m^{\frac{2}{3}})\) yields the desired dependence on the entropy \(H_{\beta}\) and a fast term in \(m\):

**Corollary 3.3** (c.f. Cor. C.4).: _In the setting of Corollary 3.2 but with \(m=\Omega(d^{\frac{3}{4}})\) and unknown \(\Delta\), using \(\varepsilon=\Theta(\sqrt{d}/m^{\frac{4}{3}})\) ensures expected task-averaged regret at most_

\[\min\left\{\min_{\beta\in(0,1]}2\sqrt{H_{\beta}d^{\beta}m/\beta}+\tilde{ \mathcal{O}}\left(\frac{d^{\frac{3}{4}}m^{\frac{3}{2}}}{\Delta}\right),8\sqrt {dm}\right\}+\tilde{\mathcal{O}}\left(\frac{d^{\frac{4}{3}}m^{\frac{3}{2}}}{ \sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{\frac{5}{6}}}{T^{\frac{3}{3}}}+\frac{d^{2 }m^{\frac{7}{3}}}{T}\right)\] (15)

While not logarithmic, the gap-dependent term is still \(o(\sqrt{m})\), and moreover the asymptotic regret is no worse than the worst-case optimal \(\mathcal{O}(\sqrt{dm})\). Note that the latter is only needed if \(\Delta=o(1/\sqrt[6]{m})\).

The main improvement in this section is in using the entropy of the true optima, which can be much smaller than that of the estimated optima if there are a few good arms but large noise. Our use of the gap assumption for this seems difficult to avoid for this notion of task-similarity. We can also compare to Corollary 3.1 (10), which did not require \(\Delta>0\) and had no fast terms but had a worse rate in \(T\); in contrast, the \(\mathcal{O}(\frac{1}{\sqrt[3]{T}})\) rates above match that of the closest stochastic bandit result [10]. As before, for \(s\ll d\) "good" arms we obtain \(\mathcal{O}(\sqrt{sm\log d})\) asymptotic regret, assuming the gap is not too small. Finally, we can also compare to the classic shifting regret bound for Exp3.S [6], which translated to task-averaged regret is \(\mathcal{O}(\sqrt{dm\log(dmT)})\). This is worse than even running OMD separately on each task, albeit under weaker assumptions (not knowing task boundaries). It also cannot take advantage of repeated optimal arms, e.g. the case of \(s\ll d\) good arms.

### Adapting to entropic task similarity implies robustness to outliers

While we considered mainly the \(s\)-sparse setting as a way of exemplifying our results and comparing to other work such as Azizi et al. [10], the fact that our approach can adapt to the Tsallis entropy \(\min_{\beta}H_{\beta}\) of the optimal arms implies meaningful guarantees for any low-entropy distribution over the optimal arms, not just sparsely-supported ones. One way to illustrate the importance of this is through an analysis of robustness to outlier tasks. Specifically, suppose that the \(s\)-sparsity assumption--that optima \(\hat{a}_{t}\) lie in a subset of \([T]\) of size \(s\ll d\)--only holds for all but \(\mathcal{O}(T^{p})\) of the tasks \(t\in[T]\), where \(p\in[0,1)\). Then the best we can do using an asymptotic bound of \(\tilde{\mathcal{O}}(\sqrt{sm})\)--e.g. that of Azizi et al.

[10] in the stochastic case or from naively applying \(\min_{\beta\in(0,1]}H_{\beta}d^{3}m/\beta\leq esm\log d\) to any of our previous results--is to substitute \(s+T^{p}\) instead of \(s\), which will only improve over the single-task bound if \(d=\omega(T^{p})\), i.e. in the regime where the number of arms increases with the number of tasks.

However, our notion of task-similarity allows us to do much better, as we can show (c.f. Prop. D.1) that in the same setting \(H_{\beta}=\mathcal{O}(s+\frac{d^{1-\beta}}{T^{\beta(1-p)}})\) for any \(\beta\in[\frac{1}{\log d},\frac{1}{2}]\). Substituting this result into e.g. Corollary 3.3 yields the same asymptotic result of \(\mathcal{O}(\sqrt{sm\log d})\), although the rate in \(T\) is a very slow \(\mathcal{O}(\sqrt{dm}/T^{\frac{1-p}{2\log d}})\). This demonstrates how our entropic notion of task-similarity simultaneously yields strong results in the \(s\)-sparse setting and is meaningful in more general settings.

## 4 Bandit linear optimization

Our last application is bandit linear optimization, in which at task \(t\) round \(i\) we play \(\mathbf{x}_{t,i}\in\mathcal{K}\) in some convex \(\mathcal{K}\subset\mathbb{R}^{d}\) and observe loss \(\langle\ell_{t,i},\mathbf{x}_{t,i}\rangle\in[-1,1]\). We will again use a variant of mirror descent, using a **self-concordant barrier** for \(\psi\) and the specialized loss estimators of Abernethy et al. [2, Alg. 1]. More information on such regularizers can be found in the literature on interior point methods [42]. We pick this class of algorithms because of their optimal dependence on the number of rounds and their applicability to any convex domain \(\mathcal{K}\) via specific barriers \(\psi\), which will yield interesting notions of task-similarity. Our ability to handle non-smooth regularizers via the structural result (Thm. 2.1) is even more important here, as barriers are infinite at the boundaries. Indeed, we will _not_ learn a \(\beta\) parameterizing the regularizer and instead focus on tuning a boundary offset \(\varepsilon>0\). Here we make use of notation from Section 2, where \(\mathbf{c}_{\varepsilon}\) maps points in \(\mathcal{K}\) to a subset \(\mathcal{K}_{\varepsilon}\) defined by the Minkowski function (c.f. Def. E.1) centered at \(\mathbf{x}_{1}=\arg\min_{\mathbf{x}\in\mathcal{K}}\psi(\mathbf{x})\).

From Abernethy et al. [2] we have an upper bound on the expected task-averaged regret of their algorithm run from initializations \(\mathbf{x}_{t,1}\in\mathcal{K}^{\circ}\) with step-sizes \(\eta_{t}>0\) and offsets \(\varepsilon_{t}>0\):

\[\mathbb{E}\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{m}\langle\ell_{t,i},\mathbf{x} _{t,i}-\hat{\mathbf{x}}_{t}\rangle\leq\frac{1}{T}\sum_{t=1}^{T}\frac{\mathbb{ E}B(\mathbf{c}_{\varepsilon_{t}}(\hat{\mathbf{x}}_{t})\|\mathbf{x}_{t,1})}{ \eta_{t}}+(32\eta_{t}d^{2}+\varepsilon_{t})m\] (16)

We can show (88) that \(D_{\varepsilon}^{2}=\max_{\mathbf{x},\mathbf{y}\in\mathcal{K}_{\varepsilon}}B (\mathbf{x}||\mathbf{y})\leq\frac{9\nu\frac{3}{2}K\sqrt{S_{1}}}{\varepsilon}\), where \(\nu\) is the self-concordance constant of \(\psi\) and \(S_{1}=||\nabla^{2}\psi(\mathbf{x}_{1})||_{2}\) is the spectral norm of its Hessian at the center \(\mathbf{x}_{1}\) of \(\mathcal{K}\). Restricting to tuning \(\varepsilon\in[\frac{1}{m},1]\)--which is enough to obtain constant task-averaged regret above if the estimated optima \(\hat{\mathbf{x}}_{t}\) are identical--we can now apply Algorithm 1 via the following instantiation:

* sample \(\varepsilon_{t}\) via the MW distribution \(\propto\exp(\mathbf{w}_{t})\) over the discretization \(\Theta_{k}\) of \([\frac{1}{m},1]\)
* run \(\texttt{O}\texttt{O}\texttt{M}_{\mathcal{H}_{t},\varepsilon_{t}}\) using the initialization \(\mathbf{x}_{t,1}=\frac{1}{t-1}\sum_{s<t}\mathbf{c}_{\varepsilon_{t}}(\hat{ \mathbf{x}}_{t})=\mathbf{x}_{1}+\frac{\sum_{s<t}\hat{\mathbf{x}}_{t}-\mathbf{ x}_{1}}{(1+\varepsilon_{t})(t-1)}\) (FTL)
* update EWOO at each \(\varepsilon\in\Theta_{k}\) with loss \(\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})||\mathbf{x}_{t,1})+ \rho^{2}D_{\varepsilon}^{2}}{\eta}+32\eta d^{2}\) for \(D_{\varepsilon}^{2}=\frac{9\nu\frac{3}{2}K\sqrt{S_{1}}}{\varepsilon}\)
* update \(\mathbf{p}_{t+1}\) using multiplicative weights with expert losses \(\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})||\mathbf{x}_{t,1})}{ \eta}+\varepsilon m\)

Note the similarity to the MAB case (8), with the difference being the upper bound passed to EWOO and MW. Our structural result bounds the expected task-averaged regret as follows (c.f. Thm. E.1):

\[\mathbb{E}\min_{\varepsilon\in[\frac{1}{m},1],\eta>0}\frac{\hat{V}_{ \varepsilon}^{2}}{\eta}+(32\eta d^{2}+\varepsilon)m+\tilde{\mathcal{O}}\left( \frac{\frac{m^{2}}{T}+\frac{1}{k}}{\eta}+\frac{m}{k}+m\min\left\{\frac{\rho^ {2}}{\eta},d\rho\right\}+\frac{dm}{\rho}\sqrt{\frac{\log k}{T}}+\frac{dm}{ \rho^{2}T}\right)\] (18)

For \(\rho=\sigma_{T}(1)\) and \(k=\omega_{T}(1)\) this becomes \(o_{T}(\text{poly}(m))+\mathbb{E}\min_{\varepsilon\in[\frac{1}{m},1],\eta>0} \frac{\hat{V}_{\varepsilon}^{2}}{\eta}+32\eta d^{2}m+\varepsilon m\), where \(\hat{V}_{\varepsilon}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi(\mathbf{c}_{ \varepsilon}(\hat{\mathbf{x}}_{t})-\psi(\mathbf{c}_{\varepsilon}(\hat{ \mathbf{x}}_{t})\). Then by tuning \(\eta\) we get an asymptotic (\(T\to\infty\)) regret of \(4d\hat{V}_{\varepsilon}\sqrt{2m}+\varepsilon m\) for any \(\varepsilon\in[\frac{1}{m},1]\). Our analysis removes the explicit dependence on \(\sqrt{\nu}\) that appears in the single-task regret [2]; as an example, \(\nu\) equals the number of inequalities defining a polytope \(\mathcal{K}\), as in the bandit shortest-path application below.

The remaining challenge is to interpret \(\hat{V}_{\varepsilon}^{2}\), which as we did for MAB we do via specific examples, in this case concrete action domains \(\mathcal{K}\). Our first example is for BLO over the unit sphere \(\mathcal{K}=\{\mathbf{x}\in\mathbb{R}^{d}:\|\mathbf{x}\|_{2}\leq 1\}\) using the appropriate log-barrier regularizer \(\psi(\mathbf{x})=-\log(1-\|\mathbf{x}\|_{2}^{2})\):

**Corollary 4.1** (c.f. Cor. E.1).: _For BLO on the sphere, Algorithm 1 has expected task-averaged regret_

\[\tilde{\mathcal{O}}\left(\frac{dm^{\frac{3}{2}}}{T^{\frac{3}{4}}}+\frac{dm}{ \sqrt[4]{T}}\right)+\min_{\varepsilon\in[\frac{1}{m},1]}4d\sqrt{2m\log\left(1+ \frac{1-\mathbb{E}\|\hat{\mathbf{x}}\|_{2}^{2}}{2\varepsilon+\varepsilon^{2}} \right)}+\varepsilon m\] (19)

The bound above is decreasing in \(\mathbb{E}\|\hat{\mathbf{x}}\|_{2}^{2}\), the expected squared norm of the average of the estimated optima \(\hat{\mathbf{x}}_{t}\). We thus say that _bandit linear optimization tasks over the sphere are similar if the norm of the empirical mean of their (estimated) optima is large_. This makes intuitive sense: if the tasks' optima are uniformly distributed, we should expect \(\mathbb{E}\|\hat{\mathbf{x}}\|_{2}^{2}\) to be small, even decreasing in \(d\). On the other hand, in the degenerate case where the estimated optima \(\hat{\mathbf{x}}_{t}\) are the same across all tasks \(t\in[T]\), we have \(\mathbb{E}\|\hat{\mathbf{x}}\|_{2}^{2}=1\), so the asymptotic task-averaged regret is 1 because we can use \(\varepsilon=\frac{1}{m}\). Perhaps slightly more realistically, if it is \(\frac{1}{m^{p}}\)-away from 1 for some power \(p\geq\frac{1}{2}\) then setting \(\varepsilon=\frac{1}{\sqrt{m}}\) can remove the logarithmic dependence on \(m\). These two regimes illustrate the importance of tuning \(\varepsilon\).

As a last application, we apply our meta-BLO result to the shortest-path problem in online optimization [50; 30]. In its bandit variant [8; 17], at each step \(i=1,\ldots,m\) the player must choose a path \(p_{i}\) from a fixed source \(u\in V\) to a fixed sink \(v\in V\) in a directed graph \(G(V,E)\). At the same time the adversary chooses edge-weights \(\ell_{i}\in\mathbb{R}^{|E|}\) and the player suffers the sum \(\sum_{\varepsilon\in p_{i}}\ell_{i}(e)\) of the weights in their chosen path \(p_{t}\). This can be relaxed as BLO over vectors \(\mathbf{x}\) in a set \(\mathcal{K}\subset[0,1]^{|E|}\) defined by a set \(\mathcal{C}\) of \(\mathcal{O}(|E|)\) linear constraints \((\mathbf{a},b)\)\(\langle\mathbf{a},\mathbf{x}\rangle\leq b\) enforcing flows from \(u\) to \(v\); \(u\) to \(v\) paths can be sampled from any \(\mathbf{x}\in\mathcal{K}\) in an unbiased manner [2, Proposition 1]. On a single-instance, applying the BLO method of Abernethy et al. [2] ensures \(\mathcal{O}(|E|^{\frac{3}{2}}\sqrt{m})\) regret on this problem.

In the multi-instance setting, comprising a sequence \(t=1,\ldots,T\) of shortest path instances with \(m\) adversarial edge-weight vectors \(\ell_{t,i}\) each, we can attempt to achieve better performance by tuning the same method across instances. Notably, we can view this as the problem of learning predictions [33] in the algorithms with predictions paradigm from beyond-worst-case analysis [40], with the OMD initialization on each instance being effectively a prediction of its optimal path. Our meta-learner then has the following average performance across bandit shortest-path instances:

**Corollary 4.2** (c.f. Cor. E.2).: _For multi-task bandit online shortest path, Algorithm 1 with regularizer \(\psi(\mathbf{x})=-\sum_{\mathbf{a},b\in\mathcal{C}}\log(b-\langle\mathbf{a}, \mathbf{x}\rangle)\) attains the following expected average regret across instances_

\[\tilde{\mathcal{O}}\left(\frac{|E|^{4}m^{\frac{3}{2}}}{T^{\frac{3}{4}}}+\frac{| E|^{\frac{5}{2}}m^{\frac{5}{6}}}{\sqrt[4]{T}}\right)+\min_{\varepsilon\in[ \frac{1}{m},1]}4|E|\mathbb{E}\sqrt{2m\sum_{\mathbf{a},b\in\mathcal{C}}\log \left(\frac{\frac{1}{T}\sum_{t=1}^{T}b-\langle\mathbf{a},\mathbf{c}_{ \varepsilon}(\hat{\mathbf{x}}_{t})\rangle}{\sqrt[7]{\prod_{t=1}^{T}b-\langle \mathbf{a},\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})\rangle}}\right)}+ \varepsilon m\] (20)

Here the asymptotic regret scales with the sum across all constraints \(\mathbf{a},b\in\mathcal{C}\) of the log of the ratio between the arithmetic and geometric means across tasks of the distances \(b-\langle\mathbf{a},\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})\rangle\) from the estimated optimum flow \(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})\) to the constraint boundary. As it is difficult to separate the effect of the offset \(\varepsilon\), we do not state an explicit task-similarity measure like in our previous settings. Nevertheless, since the arithmetic and geometric means are equal exactly when all entries are equal--and otherwise the former is larger--the bound does show that regret is small when the estimated optimal flows \(\hat{\mathbf{x}}_{t}\) for each task are at similar distances from the constraints, i.e. the boundaries of the polytope. Indeed, just as on the sphere, if the estimated optima are all the same then setting \(\varepsilon=\frac{1}{m}\) again yields constant averaged regret.

## 5 Conclusion and limitations

We develop and apply a meta-algorithm for learning to initialize and tune bandit algorithms, obtaining task-averaged regret guarantees for both multi-armed and linear bandits that depend on natural, setting-specific notions of task similarity. For MAB, we meta-learn the initialization, step-size, and entropy parameter of Tsallis-entropic OMD and show good performance if the entropy of the optimal arms is small. For BLO, we use OMD with self-concordant regularizers and meta-learn the initialization, step-size, and boundary-offset, yielding interesting domain-specific task-similarity measures. Some natural directions for future work involve overcoming some limitations of our results: can we adapt to a notion of task-similarity that depends on the true optima without assuming a gap for MAB, or at all for BLO? Alternatively, can we design meta-learning algorithms that adapt to both stochastic and adversarial bandits, i.e. a "best-of-both-worlds" guarantee? Beyond this, one could explore other partial information settings, such as contextual bandits or bandit convex optimization.

## Acknowledgments

We thank our anonymous reviewers for helpful suggestions, especially concerning the analysis of robustness to outliers. This material is based on work supported in part by the National Science Foundation under grants CCF-1910321, FAI-1939606, IIS-1901403, SCC-1952085, and SES-1919453; the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003; a Simons Investigator Award; an AWS Machine Learning Research Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship; a Google Faculty Research Award; a J.P. Morgan Faculty Award; a Facebook Research Award; a Mozilla Research Grant; a Facebook PhD Fellowship; and an NDSEG Fellowship. KYL is supported by the Israel Science Foundation (grant No. 447/20) and the Technion Center for Machine Learning and Intelligent Systems (MLIS). The work of RM was partially supported by the Israel Science Foundation grant number 1693/22.

## References

* Abbasi-Yadkori et al. [2018] Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of both worlds: Stochastic & adversarial best-arm identification. In _Proceedings of the 31st Conference On Learning Theory_, 2018.
* Abernethy et al. [2008] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In _Proceedings of the International Conference on Computational Learning Theory_, 2008.
* Abernethy et al. [2015] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In _Advances in Neural Information Processing Systems_, 2015.
* Anava and Karnin [2016] Oren Anava and Zohar S. Karnin. Multi-armed bandits: Competing with optimal sequences. In _Advances in Neural Information Processing Systems_, 2016.
* Audibert et al. [2011] Jean-Yves Audibert, Sebastien Bubeck, and Gabor Lugosi. Minimax policies for combinatorial prediction games. In _Proceedings of the International Conference on Computational Learning Theory_, 2011.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM Journal of Computing_, 32:48-77, 2002.
* Auer et al. [2019] Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknown number of distribution changes. In _Proceedings of the 32nd Annual Conference on Learning Theory_, 2019.
* Awerbuch and Kleinberg [2004] Baruch Awerbuch and Robert D. Kleinberg. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In _Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing_, 2004.
* Azar et al. [2013] Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill, et al. Sequential transfer in multi-armed bandit with finite set of models. In _Advances in Neural Information Processing Systems_, 2013.
* Azizi et al. [2022] MohammadJavad Azizi, Thang Duong, Yasin Abbasi-Yadkori, Andras Gyorgy, Claire Vernade, and Mohammad Ghavamzadeh. Non-stationary bandits and meta-learning with a small set of optimal arms. arXiv, 2022.
* Balcan et al. [2015] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. Efficient representations for lifelong learning and autoencoding. In _Proceedings of the 28th Annual Conference on Learning Theory_, 2015.
* Balcan et al. [2021] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Learning-to-learn non-convex piecewise-Lipschitz functions. In _Advances in Neural Information Processing Systems_, 2021.
* Basu et al. [2021] Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the prior in bandits. In _Advances in Neural Information Processing Systems_, 2021.

* Beck and Teboulle [2003] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31:167-175, 2003.
* Cella et al. [2020] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* Cesa-Bianchi and Lugosi [2006] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* Dani et al. [2008] Varsha Dani, Thomas Hayes, and Sham Kakade. The price of bandit information for online optimization. In _Advances in Neural Information Processing Systems_, 2008.
* Denevi et al. [2018] Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learning around a common mean. In _Advances in Neural Information Processing Systems_, 2018.
* Denevi et al. [2019] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Online-within-online meta-learning. In _Advances in Neural Information Processing Systems_, 2019.
* Du et al. [2021] Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In _Proceedings of the 9th International Conference on Learning Representations_, 2021.
* Duan et al. [2017] Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In _Advances in Neural Information Processing Systems_, 2017.
* Fallah et al. [2020] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based model-agnostic meta-learning algorithms. In _Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics_, 2020.
* Fan et al. [2012] Xiequan Fan, Ion Grama, and Quansheng Liu. Hoeffding's inequality for supermartingales. _Stochastic Processes and their Applications_, 122(10):3545-3559, 2012.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _Proceedings of the 34th International Conference on Machine Learning_, 2017.
* Hazan [2015] Elad Hazan. Introduction to online convex optimization. In _Foundations and Trends in Optimization_, volume 2, pages 157-325. now Publishers Inc., 2015.
* Hazan et al. [2007] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69:169-192, 2007.
* Jadbabaie et al. [2015] Ali Jadbabaie, Alexander Rakhlin, and Shahin Shahrampour. Online optimization: Competing with dynamic comparators. In _Proceedings of the 18th International Conference on Artificial Intelligence and Statistics_, 2015.
* Jamieson and Talwalkar [2015] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In _Proceedings of the 18th International Conference on Artificial Intelligence and Statistics_, 2015.
* Jiang et al. [2019] Yihan Jiang, Jakub Konecny, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv, 2019.
* Kalai and Vempala [2005] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. _Journal of Computer and System Sciences_, 71:291-307, 2005.
* Khodak et al. [2019] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning methods. In _Advances in Neural Information Processing Systems_, 2019.
* Khodak et al. [2021] Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing. In _Advances in Neural Information Processing Systems_, 2021.

* Kuceton et al. [2020] Michael Kuceton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, and Craig Boutilier. Meta-learning bandit policies by gradient ascent. arXiv, 2020.
* Kveton et al. [2021] Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Li et al. [2017] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: Learning to learning quickly for few-shot learning. arXiv, 2017.
* Luo [2017] Haipeng Luo. CSCI 699 Lecture 13. 2017. URL https://haipeng-luo.net/courses/CSCI699/lecture13.pdf.
* Luo et al. [2018] Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in non-stationary worlds. In _Proceedings of the 31st Annual Conference on Learning Theory_, 2018.
* Marinov and Zimmert [2021] Teodor Marinov and Julian Zimmert. The Pareto frontier of model selection for general contextual bandits. In _Advances in Neural Information Processing Systems_, 2021.
* Mitzenmacher and Vassilvitskii [2021] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden, editor, _Beyond the Worst-Case Analysis of Algorithms_. Cambridge University Press, Cambridge, UK, 2021.
* Moradipari et al. [2022] Ahmadreza Moradipari, Mohammad Ghavamzadeh, Taha Rajabzadeh, Christos Thrampoulidis, and Mahnoosh Alizadeh. Multi-environment meta-learning in stochastic linear bandits. In _Proceedings of the 2022 IEEE International Symposium on Information Theory_, 2022.
* Nesterov and Nemirovskii [1994] Yurii Nesterov and Arkadii Nemirovskii. _Interior-Point Polynomial Algorithms in Convex Programming_. SIAM Studies in Applied and Numerical Mathematics, 1994.
* Neu [2015] Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In _Advances in Neural Information Processing Systems_, 2015.
* Nichol et al. [2018] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv, 2018.
* Saunshi et al. [2020] Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A sample complexity separation between non-convex and convex meta-learning. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* Shalev-Shwartz [2011] Shai Shalev-Shwartz. Online learning and online convex optimization. _Foundations and Trends in Machine Learning_, 4(2):107-194, 2011.
* Sharaf and Daume III [2021] Amr Sharaf and Hal Daume III. Meta-learning effective exploration strategies for contextual bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2021.
* Simchowitz et al. [2021] Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel J. Hsu, Thodoris Lykouris, Miro Dudik, and Robert E. Schapire. Bayesian decision-making under misspecified priors with applications to meta-learning. In _Advances in Neural Information Processing Systems_, 2021.
* Snell et al. [2017] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In _Advances in Neural Information Processing Systems_, 2017.
* Takimoto and Warmuth [2003] Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. _Journal of Machine Learning Research_, 4:773-818, 2003.
* Thrun and Pratt [1998] Sebastian Thrun and Lorien Pratt. _Learning to Learn_. Springer Science & Business Media, 1998.

* [52] Constantino Tsallis. Possible generalization of Boltzmann-Gibbs statistics. _Journal of Statistical Physics_, 52:479-487, 1988.
* [53] Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. In _Proceedings of the 34th Annual Conference on Learning Theory_, 2021.
* [54] Takuya Yamano. Some properties of q-logarithm and q-exponential functions in Tsallis statistics. _Physica A: Statistical Mechanics and its Applications_, 305:486-496, 2002.
* [55] Peng Zhao, Guanghui Wang, Lijun Zhang, and Zhi-Hua Zhou. Bandit convex optimization in non-stationary environments. _Journal of Machine Learning Research_, 22(125):1-45, 2021.

Structural results

### Properties of the Bregman divergence

**Lemma A.1**.: _Let \(\psi:\mathcal{C}\mapsto\mathbb{R}\) be a strictly convex function with \(\max_{\mathbf{x}\in\mathcal{C}}\|\nabla^{2}\psi(\mathbf{x})\|_{2}\leq S\) over a convex set \(\mathcal{C}\subset\mathbb{R}^{d}\) over size \(\max_{\mathbf{x}\in\mathcal{C}}\|\mathbf{x}\|_{2}\leq K\), and let \(B(\cdot\|\cdot)\) be the Bregman divergence generated by \(\psi\). Then for any points \(\mathbf{x}_{1},\dots,\mathbf{x}_{T}\in\mathcal{C}\) the actions \(\mathbf{y}_{1}=\arg\min_{\mathbf{x}\in\mathcal{C}}\psi(\mathbf{x})\) and \(\mathbf{y}_{t}=\frac{1}{t-1}\sum_{s<t}\mathbf{x}_{s}\) have regret_

\[\sum_{t=1}^{T}B(\mathbf{x}_{t}||\mathbf{y}_{t})-B(\mathbf{x}_{t}||\mathbf{y}_{ T+1})\leq\sum_{t=1}^{T}\frac{8SK^{2}}{2t-1}\leq 8SK^{2}(1+\log T)\] (21)

Proof.: Note that

\[\nabla_{\mathbf{y}}B(\mathbf{x}||\mathbf{y})=-\nabla\psi(\mathbf{y})-\nabla_{ \mathbf{y}}\langle\nabla\psi(\mathbf{y}),\mathbf{x}\rangle+\nabla_{\mathbf{y}} \langle\nabla\psi(\mathbf{y}),\mathbf{y}\rangle=\operatorname{diag}(\nabla^{2} \psi(\mathbf{y}))(\mathbf{y}-\mathbf{x})\] (22)

so \(B(\mathbf{x}_{t}||\mathbf{y})\) is \(2SK\)-Lipschitz w.r.t. the Euclidean norm. Applying Khodak et al. [31, Prop. B.1] yields the result (note that its assumption of strong convexity of the regularizer can be replaced with strict convexity without changing the proof or result). 

**Claim A.1**.: _Let \(\psi:\mathcal{K}\mapsto\mathbb{R}\) be a strictly-convex function with Bregman divergence \(B(\cdot||\cdot)\) over a convex set \(\mathcal{K}\subset\mathbb{R}^{d}\) containing points \(\mathbf{x}_{1},\dots,\mathbf{x}_{T}\). Then their mean \(\bar{\mathbf{x}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t}\) satisfies_

\[\sum_{t=1}^{T}B(\mathbf{x}_{t}||\bar{\mathbf{x}})=\sum_{t=1}^{T}\psi(\mathbf{ x}_{t})-\psi(\bar{\mathbf{x}})\] (23)

Proof.: \[\begin{split}\sum_{t=1}^{T}B(\mathbf{x}_{t}||\bar{\mathbf{x}})& =\sum_{t=1}^{T}\psi(\mathbf{x}_{t})-\psi(\bar{\mathbf{x}})-\langle \nabla\psi(\bar{\mathbf{x}}),\mathbf{x}_{t}-\bar{\mathbf{x}}\rangle\\ &=\sum_{t=1}^{T}\psi(\mathbf{x}_{t})-\psi(\bar{\mathbf{x}})- \langle\nabla\psi(\bar{\mathbf{x}}),\sum_{t=1}^{T}\mathbf{x}_{t}-\bar{ \mathbf{x}}\rangle=\sum_{t=1}^{T}\psi(\mathbf{x}_{t})-\psi(\bar{\mathbf{x}}) \end{split}\] (24)

### Tuning the step-size

**Lemma A.2**.: _Let \(\ell_{1},\dots,\ell_{T}:\mathbb{R}_{>0}\mapsto\mathbb{R}_{>0}\) be a sequence of functions of form \(\ell_{t}(x)=\frac{B_{t}^{2}}{x}+G^{2}x\) for adversarially chosen \(B_{t}\in[0,D]\) and some \(G>0\). Then for any \(\rho\geq 0\), the actions of EWOO [26, Fig. 4] with parameter \(\frac{2\rho^{2}}{DG}\) run on the modified losses \(\frac{B_{t}^{2}+\rho^{2}D^{2}}{x}+G^{2}x\) over the domain \(\left[\frac{\rho D}{G},\frac{D}{G}\sqrt{1+\rho^{2}}\right]\) achieves regret w.r.t. any \(x>0\) of_

\[\sum_{t=1}^{T}\ell_{t}(x)-\ell_{t}(x)\leq\min\left\{\frac{\rho^{2}D^{2}}{x}, \rho DG\right\}T+\frac{DG(1+\log(T+1))}{2\rho^{2}}\] (25)

Proof.: By Khodak et al. [31, Prop. C.1] the modified functions are \(\frac{2\rho^{2}}{DG}\)-exp-concave. Then Khodak et al. [31, Cor. C.2] with \(B_{t}\) set to \(\frac{B_{t}}{G}\), \(D\) to \(\frac{D}{G}\), \(\alpha_{t}=G^{2}\), and \(\varepsilon=\frac{\rho D}{G}\) yields the result.

**Lemma A.3**.: _For \(\hat{\mathbf{x}}_{1},\ldots,\hat{\mathbf{x}}_{T}\in\partial\mathcal{K}\) consider a sequence of functions of form_

\[U_{t}(\mathbf{x},\eta)=\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})|| \mathbf{x})}{\eta}+\eta G^{2}m\] (26)

_where \(B\) is the Bregman divergence of a strictly convex d.g.f. \(\psi:\mathcal{K}^{\circ}\mapsto\mathbb{R}\) and where \(\mathbf{x}_{1}=\arg\min_{\mathbf{x}\in\mathcal{K}}\psi(\mathbf{x})\) defines the projection \(\mathbf{c}_{\varepsilon}(\mathbf{x})=\mathbf{x}_{1}+\frac{\mathbf{x}_{ \cdot}\mathbf{x}_{\cdot}}{1+\epsilon}\) for some \(\varepsilon>0\). Suppose we play \(\mathbf{x}_{t+1}\leftarrow\mathbf{c}_{\varepsilon}\left(\frac{1}{t}\sum_{s=1}^ {t}\hat{\mathbf{x}}_{s}\right)\) and set \(\eta_{t}\) using the actions of EWOO [26, Fig. 4] with parameter \(\frac{2\rho^{2}}{DG}\) for some \(\rho,D_{\varepsilon}>0\) s.t. \(B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})||\mathbf{x})\leq D_{ \varepsilon}^{2}\)\(\forall\)\(\mathbf{x}\in\mathcal{K}_{\varepsilon}\) on the functions \(\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})||\mathbf{x}_{t})+\rho^{ 2}D_{\varepsilon}^{2}}{\eta}+\eta G^{2}m\) over the domain \(\left[\frac{\rho D_{\varepsilon}}{G\sqrt{m}},\frac{D_{\varepsilon}}{G}\sqrt{ \frac{1+\rho^{2}}{m}}\right]\), with \(\eta_{1}\) being at the midpoint of the domain. Then \(U_{t}(\mathbf{x}_{t},\eta_{t})\leq D_{\varepsilon}G\sqrt{m}\left(\frac{1}{ \rho}+\sqrt{1+\rho^{2}}\right)\)\(\forall\)\(t\in[T]\) and_

\[\sum_{t=1}^{T}U_{t}(\mathbf{x}_{t},\eta_{t})\leq\min_{\eta>0, \mathbf{x}\in\mathcal{K}}\sum_{t=1}^{T}\frac{B(\mathbf{c}_{\varepsilon}(\hat{ \mathbf{x}}_{t})||\mathbf{x})}{\eta}+\eta G^{2}m\\ +\min\left\{\frac{\rho^{2}D_{\varepsilon}^{2}}{\eta},\rho D_{ \varepsilon}G\right\}T+\frac{D_{\varepsilon}G(1+\log(T+1))}{2\rho^{2}}+\frac{8 S_{\varepsilon}K^{2}(1+\log T)}{\eta}\] (27)

_for \(K=\max_{\mathbf{x}\in\mathcal{K}}\|\mathbf{x}\|_{2}\) and \(S_{\varepsilon}=\max_{\mathbf{x}\in\mathcal{K}_{\varepsilon}}\|\nabla^{2}\psi (\mathbf{x})\|_{2}\)._

Proof.: The first claim follows by directly substituting the worst-case values of \(\eta\) into \(U_{t}(\mathbf{x},\eta)\). For the second, apply Lemma A.2 followed by Lemma A.1:

\[\sum_{t=1}^{T} U_{t}(\mathbf{x}_{t},\eta_{t})\] (28) \[=\sum_{t=1}^{T}\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_ {t})||\mathbf{x}_{t})}{\eta_{t}}+\eta_{t}G^{2}m\] \[\leq\min_{\eta>0}\min\left\{\frac{\rho^{2}D_{\varepsilon}^{2}}{ \eta},\rho D_{\varepsilon}G\right\}T+\frac{D_{\varepsilon}G(1+\log(T+1))}{2 \rho^{2}}+\sum_{t=1}^{T}\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t} )||\mathbf{x})}{\eta}+\eta G^{2}m\] \[\leq\min_{\eta>0}\min\left\{\frac{\rho^{2}D_{\varepsilon}^{2}}{ \eta},\rho D_{\varepsilon}G\right\}T+\frac{D_{\varepsilon}G(1+\log(T+1))}{2 \rho^{2}}+\frac{8S_{\varepsilon}K^{2}(1+\log T)}{\eta}\] \[\qquad+\min_{\mathbf{x}\in\mathcal{K}_{\varepsilon}}\sum_{t=1}^{ T}\frac{B(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})||\mathbf{x})}{\eta}+\eta G^{2}m\]

Conclude by noting that the sum of Bregman divergence to \(\mathbf{c}_{\varepsilon}(\hat{\mathbf{x}}_{t})\) is minimized on their convex hull, a subset of \(\mathcal{K}_{\varepsilon}\). 

### Computational and space complexity

Algorithm 1 implicitly maintains a separate copy of FTL for each hyperparameter in the continuous space of EWOO and the grid \(\Theta_{k}\) over the domain of \(\theta\), but explicitly just needs to average the estimated task-optima \(\hat{\mathbf{x}}_{t}\); this is due to the mean-as-minimizer property of Bregman divergences and the linearity of \(\mathbf{c}_{\varepsilon}\). Thus the memory it uses is \(\mathcal{O}(d+k)\), where \(k\) is size of the discretization of \(\Theta\) and should be viewed as sublinear in \(T\), e.g. for MAB with implicit exploration and BLO \(k=\mathcal{O}(\sqrt[4]{d}\sqrt{T})\). Computationally, at each timestep \(t\) and for each grid point we must compute two single-dimensional integrals; the integrands are sums of upper bounds that just need to be incremented once per round, leading to a total per-iteration complexity of \(\mathcal{O}(k)\) (ignoring the running of OMD). Although outside the scope of this work, it may be possible to avoid integration by tuning \(\eta\) with MW as well, rather than EWOO, but likely at the cost of worse regret because it would not take advantage of the exp-concavity of \(U_{t}^{(\rho)}\)

### Main structural result

**Theorem A.1**.: _Consider a family of strictly convex functions \(\psi_{\theta}:\mathcal{K}^{\circ}\mapsto\mathbb{R}\) parameterized by \(\theta\) lying in an interval \(\Theta\subset\mathbb{R}\) of radius \(R_{\theta}\) that are all minimized at the same \(\mathbf{x}_{1}\in\mathcal{K}^{\circ}\), and for \(\hat{\mathbf{x}}_{1},\dots,\hat{\mathbf{x}}_{T}\in\partial\mathcal{K}\) consider a sequence of functions of form \(U_{t}(\mathbf{x},\eta,\theta)\) (3), as well as the associated regularized upper bounds \(U_{t}^{(\rho)}\) (4). Define the maximum divergence \(D=\max_{\theta\in\Theta}D_{\theta}\), radius \(K=\max_{\mathbf{x}\in\mathcal{K}}\|\mathbf{x}\|_{2}\), and \(L_{\eta}\) the Lipschitz constant w.r.t. \(\theta\in\Theta\) of \(\frac{\hat{V}_{\theta}^{2}}{\eta}+\eta g(\theta)m+f(\theta)m\). Then Algorithm 1 with \(\Theta_{k}\subset\Theta\) the uniform discretization of \(\Theta\) s.t. \(\max_{\theta\in\Theta}\min_{\theta^{\prime}\in\Theta_{k}}|\theta-\theta^{ \prime}|\leq\frac{R_{\Theta}}{k}\), \(\rho\in(0,1)\), \(\underline{\eta}(\theta)=\frac{\rho D_{\theta}}{g(\theta)m}\), \(\overline{\eta}(\theta)=D_{\theta}\sqrt{\frac{1+\rho^{2}}{g(\theta)m}}\), \(\alpha(\theta)=\frac{2\rho^{2}}{D_{\theta}\sqrt{g(\theta)m}}\), and \(\lambda=\left(M\left(\frac{1}{\rho}+\sqrt{1+\rho^{2}}\right)+Fm\right)^{-1} \sqrt{\frac{\log k}{2T}}\) leads to a sequence \((\mathbf{x}_{t},\eta_{t}(\theta_{t}),\theta_{t})\) s.t. \(\mathbb{E}\sum_{t=1}^{T}U_{t}(\mathbf{x}_{t},\eta_{t}(\theta_{t}),\theta_{t})\) is bounded by_

\[\begin{split}&\mathbb{E}\min_{\theta\in\Theta,\eta>0}\frac{8SK^{2}(1 +\log T)}{\eta}+\left(\frac{\hat{V}_{\theta}^{2}}{\eta}+\eta g(\theta)m+f( \theta)m+\frac{L_{\eta}R_{\Theta}}{k}+\min\left\{\frac{\rho^{2}D^{2}}{\eta}, \rho M\right\}\right)T\\ &\qquad+\left(\frac{4M}{\rho}+Fm\right)\sqrt{T\log k}+\frac{M(1 +\log(T+1))}{2\rho^{2}}\end{split}\] (29)

_and \(\sum_{t=1}^{T}U_{t}(\mathbf{x}_{t},\eta_{t}(\theta_{t}),\theta_{t})\) is bounded w.p. \(\geq 1-\delta 1_{k>1}\) by_

\[\begin{split}&\min_{\theta\in\Theta,\eta>0}\frac{8SK^{2}(1+ \log T)}{\eta}+\left(\frac{\hat{V}_{\theta}^{2}}{\eta}+\eta g(\theta)m+f( \theta)m+\frac{L_{\eta}R_{\Theta}}{k}+\min\left\{\frac{\rho^{2}D^{2}}{\eta}, \rho M\right\}\right)T\\ &\qquad+\left(\frac{4M}{\rho}+Fm\right)\left(\sqrt{T\log k}+1_{k >1}\sqrt{\frac{T}{2}\log\frac{1}{\delta}}\right)+\frac{M(1+\log(T+1))}{2\rho ^{2}}\end{split}\] (30)

Proof.: In the following proof, we first consider online learning \(U_{t}(\cdot,\cdot,\theta)\) for fixed \(\theta\in\Theta_{k}\). To tune \(\eta\), we online learn the one-dimensional losses \(B_{\theta}(\mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})||\mathbf{c}_{\theta}( \mathbf{x}_{t}))/\eta+\eta g(\theta)\), where \(\mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})\) is the (\(\eta_{t}(\theta)\)-independent) action of FTL at time \(t\). As discussed, the corresponding regularized losses \(U_{t}^{(\rho)}\) are exp-concave, and so running EWOO yields \(\tilde{\mathcal{O}}\left(M/\rho^{2}+\min\left\{\rho^{2}D^{2}/\eta,\rho M \right\}T\right)\) regret w.r.t. the original sequence [31, Cor. C.2]. At the same time, we show that FTL has logarithmic regret on the sequence \(B_{\theta}(\mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})||\cdot)\) that scales with the spectral norm \(S\) of \(\nabla^{2}\psi_{\theta}\) (c.f. Lem. A.1), and that the average loss of the optimal comparator is \(\hat{V}_{\theta}^{2}\) (c.f. Claim A.1). Thus, since we only care about a fixed comparator \(\eta\), dividing by \(\eta T\) yields the first and last terms (5). We run a copy of these algorithms for each \(\theta\in\Theta_{k}\); since their losses are bounded by \(\tilde{\mathcal{O}}(M/\rho+Fm)\), textbook results for MW yield \(\mathcal{O}(\sqrt{T\log k})\) regret w.r.t. \(\theta\in\Theta_{k}\), which we then extend to \(\Theta\supset\Theta_{k}\) using \(L_{\eta}\)-Lipschitzness.

Formally, we have that

\[\begin{split}&\mathbb{E}\sum_{t=1}^{T}U_{t}(\mathbf{x}_{t},\eta_{t}( \theta_{t}),\theta_{t})\\ &=\mathbb{E}\sum_{t=1}^{T}\frac{B_{\theta_{t}}(\mathbf{c}_{ \theta_{t}}(\hat{\mathbf{x}}_{t})||\mathbf{x}_{t})}{\eta_{t}(\theta_{t})}+ \eta_{t}(\theta_{t})g(\theta)m+f(\theta)m\\ &\leq\left(M\left(\frac{1}{\rho}+\sqrt{2}\right)+Fm\right)\sqrt{2T \log k}+\mathbb{E}\min_{\theta\in\Theta_{k}}\sum_{t=1}^{T}\frac{B_{\theta}( \mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})||\mathbf{x}_{t})}{\eta_{t}(\theta)}+ \eta_{t}(\theta)g(\theta)m+f(\theta)m\\ &\leq\left(\frac{4M}{\rho}+Fm\right)\sqrt{T\log k}+\mathbb{E}\min_ {\theta\in\Theta_{k},\eta>0,\mathbf{x}\in\mathcal{K}}\sum_{t=1}^{T}\frac{B_{ \theta}(\mathbf{c}_{\theta}(\hat{\mathbf{x}}_{t})||\mathbf{x})}{\eta}+\eta g (\theta)m+f(\theta)m\\ &\qquad+\min\left\{\frac{\rho^{2}D_{\theta}^{2}}{\eta},\rho D_{\theta}\sqrt{g(\theta)m}\right\}T+\frac{D_{\theta}\sqrt{g( \theta)m}(1+\log(T+1))}{2\rho^{2}}+\frac{8SK^{2}(1+\log T)}{\eta}\end{split}\] (31)where the first inequality is the regret of multiplicative weights with step-size \(\lambda\)[46, Cor. 2.14] and the second is by applying Lemma A.3 for each \(\theta\). We then simplify and apply the definition of \(\hat{V}_{\theta}^{2}\) via Claim A.1 and conclude by applying Lipschitzness w.r.t. \(\theta\):

\[\mathbb{E}\sum_{t=1}^{T}U_{t}(\mathbf{x}_{t},\eta_{t}(\theta_{t}), \theta_{t})\] (32) \[\leq\left(\frac{4M}{\rho}+Fm\right)\sqrt{T\log k}+\mathbb{E}\min_ {\theta\in\Theta_{k},\eta>0}\frac{\hat{V}_{\theta}^{2}T}{\eta}+\eta g(\theta) mT+f(\theta)mT\] \[\qquad+\min\left\{\frac{\rho^{2}D^{2}}{\eta},\rho M\right\}T+ \frac{M(1+\log(T+1))}{2\rho^{2}}+\frac{8SK^{2}(1+\log T)}{\eta}\] \[\leq\mathbb{E}\min_{\theta\in\Theta,\eta>0}\frac{8SK^{2}(1+\log T )}{\eta}+\left(\frac{\hat{V}_{\theta}^{2}}{\eta}+\eta g(\theta)m+f(\theta)m+ \frac{L_{\eta}R_{\Theta}}{k}+\min\left\{\frac{\rho^{2}D^{2}}{\eta},\rho M \right\}\right)T\] \[\qquad+\left(\frac{4M}{\rho}+Fm\right)\sqrt{T\log k}+\frac{M(1+ \log(T+1))}{2\rho^{2}}\]

The w.h.p. guarantee follows by Cesa-Bianchi and Lugosi [16, Lem. 4.1]. 

## Appendix B Implicit exploration

### Properties of the Tsallis entropy

**Lemma B.1**.: _For any \(\varepsilon\in(0,1]\) and \(\mathbf{x}\in\triangle\) s.t. \(\mathbf{x}(a)\geq\frac{\varepsilon}{d}\;\forall\;a\in[d]\) the \(\beta\)-Tsallis entropy \(H_{\beta}(\mathbf{x})=-\frac{1-\sum_{a=1}^{d}\mathbf{x}^{\beta}(a)}{1-\beta}\) is \(d\log\frac{d}{\varepsilon}\)-Lipschitz w.r.t. \(\beta\in[0,1]\)._

Proof.: Let \(\log_{\beta}x=\frac{x^{1-\beta}-1}{1-\beta}\) be the \(\beta\)-logarithm function and note that by Yamano [54, Equation 6] we have \(\log_{\beta}x-\log x=(1-\beta)(\partial_{b}\log_{\beta}x+\log_{\beta}x\log x )\geq 0\;\forall\;\beta\in[0,1]\). Then we have for \(\beta\in[0,1)\) that

\[|\partial_{\beta}H_{\beta}(\mathbf{x})| =\left|\frac{-H_{\beta}(\mathbf{x})-\sum_{a=1}^{d}\mathbf{x}^{ \beta}(a)\log\mathbf{x}(a)}{1-\beta}\right|\] (33) \[=\frac{1}{1-\beta}\left|\sum_{a=1}^{d}\mathbf{x}^{\beta}(a)(\log _{\beta}\mathbf{x}(a)-\log\mathbf{x}(a))\right|\] \[=\frac{1}{1-\beta}\sum_{a=1}^{d}\mathbf{x}^{\beta}(a)(\log_{\beta }\mathbf{x}(a)-\log\mathbf{x}(a))\] \[\leq\frac{1}{1-\beta}\left(\sum_{a=1}^{d}\mathbf{x}(a)\right)^{ \beta}\left(\sum_{a=1}^{d}(\log_{\beta}\mathbf{x}(a)-\log\mathbf{x}(a))^{ \frac{1}{1-\beta}}\right)^{1-\beta}\] \[\leq\frac{1}{1-\beta}\sum_{a=1}^{d}\log_{\beta}\mathbf{x}(a)-\log \mathbf{x}(a)\leq\frac{d}{1-\beta}(\log_{\beta}\frac{d}{\varepsilon}-\log \frac{d}{\varepsilon})\leq-d\log\frac{d}{\varepsilon}\]

where the fourth inequality follows by Holder's inequality, the fifth by subadditivity of \(x^{a}\) for \(a\in(0,1]\), the sixth by the fact that \(\partial_{x}(\log_{\beta}x-\log x)=x^{-\beta}-1/x\leq 0\;\forall\;\beta,x \in[0,1)\), and the last line by substituting \(\beta=0\) since \(\partial_{\beta}\left(\frac{\log_{\beta}x-\log x}{1-\beta}\right)=\frac{2(x-x^ {\beta})-(1-\beta)(x^{\beta}+x)\log x}{x^{\beta}(1-\beta)^{3}}\leq 0\;\forall\;\beta \in[0,1),x\in(0,1/d]\). For \(\beta=1\), applying L'Hopital's rule yields

\[\lim_{\beta\to 1}\partial_{\beta}H_{\beta}(\mathbf{x})=-\frac{1}{2}\lim_{\beta \to 1}\sum_{a=1}^{d}\mathbf{x}^{\beta}(a)\log^{2}\mathbf{x}(a)(1-(1-\beta)\log \mathbf{x}(a))=-\frac{1}{2}\sum_{a=1}^{d}\mathbf{x}(a)\log^{2}\mathbf{x}(a)\] (34)

which is bounded on \([-2d/e^{2},0]\).

**Lemma B.2**.: _Consider \(\mathbf{x}_{1},\ldots,\mathbf{x}_{T}\in\triangle\) s.t. \(\mathbf{x}_{t}(a_{t})=1\) for some \(a_{t}\in[d]\), and let \(\bar{\mathbf{x}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t}\) be their average. For any \(\varepsilon\in(0,1]\) and \(\beta\in(0,1]\) we have that for every \(t\in[T]\)_

\[H_{\beta}(\bar{\mathbf{x}}^{(\varepsilon)})-H_{\beta}(\mathbf{x}_{t}^{( \varepsilon)})\leq H_{\beta}(\bar{\mathbf{x}})\] (35)

_where recall that \(\mathbf{x}^{(\varepsilon)}=\mathbf{c}_{\frac{\varepsilon}{1-\varepsilon}}( \mathbf{x})=\mathbf{1}_{d}/d+(1-\varepsilon)(\mathbf{x}-\mathbf{1}_{d}/d)=(1- \varepsilon)\mathbf{x}+\frac{\varepsilon}{d}\mathbf{1}_{d}\)._

Proof.: Assume w.l.o.g. that \(\bar{\mathbf{x}}(1)\leq\bar{\mathbf{x}}(2)\leq\ldots\leq\bar{\mathbf{x}}(d)\) and \(a_{t}=1\), so that \(\mathbf{x}_{t}^{(\varepsilon)}=\mathbf{e}_{1}^{(\varepsilon)}\). We take the derivative

\[\begin{split}\partial_{\varepsilon}H_{\beta}&\left( (1-\varepsilon)\bar{\mathbf{x}}+\frac{\varepsilon}{d}\mathbf{1}_{d}\right)- \partial_{\varepsilon}H_{\beta}\left(\mathbf{e}_{1}^{(\varepsilon)}\right)\\ &=\frac{d}{1-\beta}\sum_{a=1}^{d-1}\left(\frac{1}{((1- \varepsilon)\bar{\mathbf{x}}(a)+\varepsilon/d)^{1-\beta}}-\frac{1}{( \varepsilon/d)^{1-\beta}}\right)\\ &+\frac{d}{1-\beta}\sum_{a=1}^{d-1}\left(\frac{1}{((1- \varepsilon)+\varepsilon/d)^{1-\beta}}-\frac{1}{((1-\varepsilon)\bar{\mathbf{ x}}(d)+\varepsilon/d)^{1-\beta}}\right)\\ &+\frac{d^{2}}{1-\beta}\sum_{a=1}^{d-1}\bar{\mathbf{x}}(a)\left( \frac{1}{((1-\varepsilon)\bar{\mathbf{x}}(d)+\varepsilon/d)^{1-\beta}}-\frac{ 1}{((1-\varepsilon)\bar{\mathbf{x}}(a)+\varepsilon/d)^{1-\beta}}\right)\end{split}\] (36)

By the assumption that \(\bar{\mathbf{x}}(a)\) is non-decreasing in \(a\), each of the summands above become non-positive. So for \(\varepsilon\in(0,1]\) the derivative is non-positive, and for \(\varepsilon\to 0^{+}\) it goes to \(-\infty\). Thus the l.h.s. of the bound is monotonically non-increasing in \(\varepsilon\) for all \(\varepsilon\in[0,1]\). The result then follows from the fact that for \(\varepsilon=0\) we have \(H_{\beta}\left((1-\varepsilon)\bar{\mathbf{x}}+\frac{\varepsilon}{d}\mathbf{1 }_{d}\right)-H_{\beta}\left(\mathbf{e}_{1}^{(\varepsilon)}\right)=H_{\beta}( \bar{\mathbf{x}})\). 

### Implicit exploration bounds

**Lemma B.3**.: _Suppose we play \(\texttt{OMD}_{\beta,\eta}\) with regularizer \(\psi_{\beta}\) the negative Tsallis entropy and initialization \(\mathbf{x}_{1}\in\triangle\) on the sequence of linear loss functions \(\ell_{1},\ldots,\ell_{T}\in[0,1]^{d}\). Then for any \(\mathbf{x}\in\triangle\) we have_

\[\sum_{t=1}^{T}\langle\ell_{t},\mathbf{x}_{t}-\mathbf{x}\rangle\leq\frac{B_{ \beta}(\mathbf{x}||\mathbf{x}_{1})}{\eta}+\frac{\eta}{\beta}\sum_{a=1}^{d} \mathbf{x}_{t}^{2-\beta}(a)\ell_{t}^{2}(a)\] (37)

Proof.: Note that the following proof follows parts of the course notes by Luo [37], which we reproduce for completeness. The OMD update at each step \(t\) involves the following two steps: set \(\mathbf{y}_{t+1}\in\triangle\) s.t. \(\nabla\psi_{\beta}(\mathbf{y}_{t+1})=\nabla\psi_{\beta}(\mathbf{x}_{t})-\eta \ell_{t}\) and then set \(\mathbf{x}_{t+1}=\arg\min_{\mathbf{x}\in\triangle}B_{\beta}(\mathbf{x}, \mathbf{y}_{t+1})\)[25, Algorithm 14]. Note that by Hazan [25, Equation 5.3] and nonnegativity of the Bregman divergence we have

\[\sum_{t=1}^{T}\langle\ell_{t},\mathbf{x}_{t}-\mathbf{x}\rangle\leq\frac{B_{ \beta}(\mathbf{x}||\mathbf{x}_{1})}{\eta}+\frac{1}{\eta}\sum_{t=1}^{T}B_{\beta} (\mathbf{x}_{t}||\mathbf{y}_{t+1})\] (38)

To bound the second term, note that when \(\psi_{\beta}\) is the negative Tsallis entropy we have

\[\begin{split} B_{\beta}&(\mathbf{x}_{t}||\mathbf{y }_{t+1})\\ &=\frac{1}{1-\beta}\sum_{a=1}^{d}\left(\mathbf{y}_{t+1}^{\beta}(a)- \mathbf{x}_{t}^{\beta}(a)+\frac{\beta}{\mathbf{y}_{t+1}^{1-\beta}(a)}(\mathbf{ x}_{t}(a)-\mathbf{y}_{t+1}(a)\right)\\ &=\frac{1}{1-\beta}\sum_{a=1}^{d}\left((1-\beta)\mathbf{y}_{t+1}^{ \beta}(a)-\mathbf{x}_{t}^{\beta}(a)+\beta\left(\frac{1}{\mathbf{x}_{t}^{1- \beta}(a)}+\frac{1-\beta}{\beta}\eta\ell_{t}(a)\right)\mathbf{x}_{t}(a)\right) \\ &=\sum_{a=1}^{d}\left(\mathbf{y}_{t+1}^{\beta}(a)-\mathbf{x}_{t}^{ \beta}(a)+\eta\mathbf{x}_{t}(a)\ell_{t}(a)\right)\end{split}\] (39)Plugging the following result, which follows from \((1+x)^{\alpha}\leq 1+\alpha x+\alpha(\alpha-1)x^{2}\,\forall\,x\geq 0,\alpha<0\), into the above yields the desired bound.

\[\begin{split}\mathbf{y}_{t+1}^{\beta}(a)=\mathbf{x}_{t}^{\beta}(a )\left(\frac{\mathbf{y}_{t+1}^{\beta-1}(a)}{\mathbf{x}_{t}^{\beta-1}(a)} \right)^{\frac{\beta}{\beta-1}}&=\mathbf{x}_{t}^{\beta}(a)\left(1 +\frac{1-\beta}{\beta}\eta\mathbf{x}_{t}^{1-\beta}(a)\ell_{t}(a)\right)^{ \frac{\beta}{\beta-1}}\\ &\leq\mathbf{x}_{t}^{\beta}(a)\left(1-\eta\mathbf{x}_{t}^{1- \beta}(a)\ell_{t}(a)+\frac{\eta^{2}}{\beta}\mathbf{x}_{t}^{2-2\beta}(a)\ell_ {t}(a)^{2}\right)\\ &=\mathbf{x}_{t}^{\beta}(a)-\eta\mathbf{x}_{t}(a)\ell_{t}(a)+ \frac{\eta^{2}}{\beta}\mathbf{x}_{t}^{2-\beta}(a)\ell_{t}(a)^{2}\end{split}\] (40)

**Theorem B.1**.: _In Algorithm 1, let \(\textsc{OMD}_{\eta,\beta}\) be online mirror descent with the Tsallis entropy regularizer \(\psi_{\beta}\) over \(\gamma\)-offset loss estimators, \(\Theta_{k}\) is a subset of \([\underline{\beta},\overline{\beta}]\subset[\frac{1}{\log d},1]\), and_

\[U_{t}(\mathbf{x},\eta,\beta)=\frac{B_{\beta}(\hat{\mathbf{x}}_{t}^{(e)}|| \mathbf{x})}{\eta}+\frac{\eta d^{\beta}m}{\beta}\] (41)

_where \(\hat{\mathbf{x}}_{t}^{(e)}=(1-\varepsilon)\hat{\mathbf{x}}_{t}+\varepsilon \mathbf{1}_{d}/d\). Note that \(U_{t}^{(\rho)}(\mathbf{x},\eta,\beta)=U_{t}(\mathbf{x},\eta,\beta)+\frac{\rho ^{2}(d^{1-\beta}-1)}{\eta(1-\beta)}\). Then there exists settings of \(\underline{\eta},\overline{\eta},\alpha,\lambda\) s.t. for all \(\varepsilon,\rho,\gamma\in(0,1)\) we have w.p. \(\geq 1-\delta\) that_

\[\begin{split}&\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(a_{t,i})- \ell_{t,i}(\hat{a}_{t})\\ &\leq(\varepsilon+\gamma d)mT+\frac{2+\sqrt{\frac{d\log d}{ \varepsilon m}}}{\gamma}\log\frac{5}{\delta}+\frac{8d\sqrt{m}}{\rho}\left(1_{ k>1}\sqrt{T\log\frac{5k}{\delta}}+\frac{1+\log(T+1)}{16\rho}\right)\\ &\quad+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta>0 }\frac{8\left(\frac{d}{\varepsilon}\right)^{2-\beta}(1+\log T)}{\eta}+\left( \frac{\hat{H}_{\beta}}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{L_{\eta}( \overline{\beta}-\underline{\beta})}{2k}+d\min\left\{\frac{\rho^{2}}{2\eta}, \rho\sqrt{m}\right\}\right)T\end{split}\] (42)

_for \(L_{\eta}=\left(\frac{\log d}{\eta}+\eta m\log^{2}d\right)d\)._

Proof.: In this setting we have \(g(\beta)=d^{\beta}/\beta\), \(f(\beta)=0\), \(D_{\beta}^{2}=\frac{d^{1-\beta}-1}{1-\beta}\), \(D\leq\sqrt{d/2}\), \(M=d\sqrt{m}\), \(F=0\), \(S=(d/\varepsilon)^{2-\beta}\), and \(K=1\). We have that

\[\begin{split}&\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(a_{t,i})- \ell_{t,i}(\hat{a}_{t})\\ &\quad=\sum_{t=1}^{T}\sum_{i=1}^{m}\langle\hat{\ell}_{t,i}, \mathbf{x}_{t,i}\rangle-\ell_{t,i}(\hat{a}_{t})+\gamma\sum_{a=1}^{d}\hat{\ell }_{t,i}(a)\\ &\quad\leq\sum_{t=1}^{T}\frac{B_{\beta_{t}}(\hat{\mathbf{x}}_{t}^ {(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+\sum_{i=1}^{m}\langle\hat{\ell}_{t,i},\hat{\mathbf{x}}_{t}^{(\varepsilon)}\rangle-\ell_{t,i}(\hat{a}_{t})+ \frac{\eta_{t}}{\beta_{t}}\sum_{a=1}^{d}\mathbf{x}_{t,i}^{2-\beta_{t}}(a)\hat {\ell}_{t,i}^{2}(a)+\gamma\sum_{a=1}^{d}\hat{\ell}_{t,i}(a)\\ &\quad\leq\varepsilon mT+\sum_{t=1}^{T}\frac{B_{\beta_{t}}(\hat{ \mathbf{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+\sum_{i=1}^{m} \langle\hat{\ell}_{t,i},\hat{\mathbf{x}}_{t}^{(\varepsilon)}\rangle-\langle \ell_{t,i},\hat{\mathbf{x}}_{t}^{(\varepsilon)}\rangle\\ &\quad\quad\quad+\sum_{t=1}^{T}\frac{\eta_{t}}{\beta_{t}}\sum_{i=1 }^{m}\sum_{a=1}^{d}\mathbf{x}_{t,i}^{1-\beta_{t}}(a)\hat{\ell}_{t,i}(a)+ \gamma\sum_{a=1}^{d}\hat{\ell}_{t,i}(a)\end{split}\] (43)

where the equality follows similarly to Luo [37] since \(\langle\hat{\ell}_{t,i},\mathbf{x}_{t,i}\rangle=\ell_{t,i}(a_{t,i})-\gamma\sum_ {a=1}^{d}\hat{\ell}_{t,i}(a)\), the first inequality follows by Lemma B.3 and the second by Holder's inequality and the definitions of \(\hat{\ell}_{t,i}\) and \(\hat{\mathbf{x}}_{t,i}^{(\varepsilon)}\). We next apply the optimality of \(\hat{a}_{t}\) for \(\sum_{i=1}^{m}\hat{\ell}_{t,i}\) to get

\[\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(a_{t,i})-\ell_{t,i}(\hat{a}_ {t})\] \[\leq\varepsilon mT+\sum_{t=1}^{T}\frac{B_{\beta_{t}}(\hat{ \mathbf{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+(1-\varepsilon) \sum_{i=1}^{m}\hat{\ell}_{t,i}(\hat{a}_{t})-\ell_{t,i}(\hat{a}_{t})+\frac{ \varepsilon}{d}\sum_{a=1}^{d}\hat{\ell}_{t,i}(a)-\ell_{t,i}(a)\] \[\qquad\quad+\sum_{t=1}^{T}\frac{\eta_{t}}{\beta_{t}}\sum_{i=1}^{ m}\sum_{a=1}^{d}\mathbf{x}_{t,i}^{1-\beta_{t}}(a)\hat{\ell}_{t,i}(a)+\gamma \sum_{a=1}^{d}\hat{\ell}_{t,i}(a)\] \[\leq\varepsilon mT+\frac{1+\frac{\varepsilon}{d}+\frac{\overline {\beta}}{\beta}+\gamma}{2\gamma}\log\frac{5}{\delta}+\sum_{t=1}^{T}\frac{B_{ \beta_{t}}(\hat{\mathbf{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}\] \[\qquad\quad+\sum_{t=1}^{T}\frac{\eta_{t}}{\beta_{t}}\sum_{i=1}^{ m}\sum_{a=1}^{d}\mathbf{x}_{t,i}^{1-\beta_{t}}(a)\ell_{t,i}(a)+\gamma\sum_{a=1}^{d} \ell_{t,i}(a)\] \[\leq\varepsilon mT+\frac{2+\sqrt{\frac{d\log d}{em}}}{\gamma} \log\frac{5}{\delta}+\gamma dmT+\sum_{t=1}^{T}\frac{B_{\beta_{t}}(\hat{ \mathbf{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+\frac{\eta_{t}d ^{\beta_{t}}m}{\beta_{t}}\] (44)

where the the second inequality follows by Neu [43, Lemma 1] applied to each of the last four terms and the fifth by the definition of \(\ell_{t,i}\) and using \(\max_{\beta\in[\frac{1}{\log d},1]}\overline{\eta}(\beta)\leq\sqrt{\frac{d}{ em\log d}}\). Substituting into Theorem A.1 and simplifying yields the result except with \(\hat{V}_{\beta}^{2}=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\hat{\mathbf{x}}_{t}^{( \varepsilon)})-\psi_{\beta}(\hat{\mathbf{x}}^{(\varepsilon)})\) in place of \(\hat{H}_{\beta}\), but the former is bounded by the latter by Lemma B.2. 

**Corollary B.1**.: _Let \(\underline{\beta}=\overline{\beta}=1\). Then w.h.p. we can ensure task-averaged regret at most_

\[2\sqrt{\hat{H}_{1}dm}+\tilde{\mathcal{O}}\left(\frac{d\sqrt{m}+d^{\frac{3}{2}} m^{\frac{3}{3}}}{\sqrt[3]{T}}\right)\] (45)

_so long as \(mT\geq d^{2}\) or alternatively ensure_

\[\min\left\{2\sqrt{\hat{H}_{1}dm}+\tilde{\mathcal{O}}\left(\frac{d^{\frac{3}{4 }}m^{\frac{3}{4}}+d\sqrt{m}}{\sqrt[4]{T}}\right),2\sqrt{dm\log d}+\tilde{ \mathcal{O}}\left(\frac{d^{\frac{3}{2}}\sqrt{m}}{\sqrt[4]{T}}\right)\right\}\] (46)

_so long as \(mT\geq d\)._

Proof.: Applying Theorem B.1, simplifying, and dividing by \(T\) yields task-averaged regret at most

\[(\varepsilon+\gamma d)m+\frac{2+\sqrt{\frac{d\log d}{em}}}{\gamma T }\log\frac{5}{\delta}+\left(\frac{1+\log(T+1)}{2\rho^{2}T}+\min\left\{\frac{ \rho^{2}}{\eta\sqrt{m}},\rho\right\}\right)d\sqrt{m}\] (47) \[\qquad+\min_{\eta>0}\frac{8d(1+\log T)}{\varepsilon\eta T}+\left( \frac{\hat{H}_{1}}{\eta}+\eta dm\right)\]

Set \(\gamma=\frac{1}{\sqrt{dmT}}\). Then set \(\varepsilon=\sqrt[3]{\frac{d^{2}}{mT}}\) and \(\rho=\frac{1}{\sqrt[3]{T}}\), and use \(\eta=\sqrt{\frac{\hat{H}_{1}}{dm}}+\frac{1}{\sqrt[3]{dmT}}\) to get the first result. Otherwise, set \(\varepsilon=\sqrt{\frac{d}{mT}}\) and \(\rho=\frac{1}{\sqrt[3]{T}}\), and use the better of \(\eta=\sqrt{\frac{\hat{H}_{1}}{dm}}+\frac{1}{\sqrt[3]{dmT}}\) and \(\eta=\sqrt{\frac{\log d}{dm}}\) to get the second. 

**Corollary B.2**.: _Let \(\underline{\beta}=\frac{1}{2}\) and \(\overline{\beta}=1\) and assume \(mT\geq d^{\frac{5}{2}}\). Then w.h.p. we can ensure task-averaged regret at most_

\[\min_{\beta\in[\frac{1}{2},1]}2\sqrt{\hat{H}_{\beta}d^{\beta}m/\beta}+\tilde{ \mathcal{O}}\left(\frac{d^{\frac{5}{2}}m^{\frac{5}{2}}}{T^{\frac{5}{2}}}+ \frac{d\sqrt{m}}{\sqrt[4]{T}}\right)\] (48)

_using \(k=\left\lceil\sqrt[4]{d}\sqrt[4]{T}\right\rceil\)._Proof.: Applying Theorem B.1, simplifying, and dividing by \(T\) yields task-averaged regret at most

\[(\varepsilon+\gamma d)m+\frac{2+\sqrt{\frac{d\log d}{cm}}}{\gamma T} \log\frac{5}{\delta}+\frac{8d\sqrt{m}}{\rho}\left(\sqrt{\frac{\log\frac{5k}{ \delta}}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\] (49) \[+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta>0}\frac{ 8d^{\frac{3}{2}}(1+\log T)}{\varepsilon^{\frac{3}{2}}\eta T}+\left(\frac{\hat{ H}_{\beta}}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{d}{4k}\left(\frac{\log \frac{d}{\varepsilon}}{\eta}+\eta m\log^{2}d\right)+\rho d\sqrt{m}\right)\]

Set \(\gamma=\frac{1}{\sqrt{dmT}}\), \(\varepsilon=\frac{d^{\frac{3}{2}}}{(mT)^{\frac{3}{2}}}\), \(\rho=\frac{1}{\sqrt[3]{T}}\), and use \(\eta=\sqrt{\frac{\beta\hat{H}_{\beta}}{md^{\beta}}}+\frac{1}{(dmT)^{\frac{3 }{2}}}\) to get the result. 

**Corollary B.3**.: _Let \(\underline{\beta}=\frac{1}{\log d}\) and \(\overline{\beta}=1\) and assume \(mT\geq d^{3}\). Then w.h.p. we can ensure task-averaged regret at most_

\[\min_{\beta\in(0,1]}2\sqrt{\hat{H}_{\beta}d^{\beta}m/\beta}+\tilde{\mathcal{O }}\left(\frac{d^{\frac{3}{4}}m^{\frac{3}{4}}+d\sqrt{m}}{\sqrt[4]{T}}\right)\] (50)

_using \(k=\left\lceil\sqrt[4]{d}\sqrt{T}\right\rceil\)._

Proof.: Applying Theorem B.1, dividing by \(T\), and simplifying yields

\[(\varepsilon+\gamma d)m+\frac{2+\sqrt{\frac{d\log d}{cm}}}{\gamma T }\log\frac{5}{\delta}+\frac{8d\sqrt{m}}{\rho}\left(\sqrt{\frac{\log\frac{5k}{ \delta}}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\] (51) \[+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta>0}\frac{ 8d^{2}(1+\log T)}{\varepsilon^{2}\eta T}+\left(\frac{\hat{H}_{\beta}}{\eta}+ \frac{\eta d^{\beta}m}{\beta}+\frac{d}{2k}\left(\frac{\log\frac{d}{ \varepsilon}}{\eta}+\eta\log^{2}d\right)+\rho d\sqrt{m}\right)\]

Note that \(\hat{H}_{\beta}\) and \(\frac{d^{3}}{\beta}\) are both decreasing on \(\beta<\frac{1}{\log d}\), so \(\beta\) in the chosen interval is optimal over all \(\beta\in(0,1]\). Set \(\gamma=\frac{1}{\sqrt{dmT}}\), \(\varepsilon=\frac{d^{\frac{3}{4}}}{\sqrt[4]{mT}}\), \(\rho=\frac{1}{\sqrt[4]{T}}\), and use \(\eta=\sqrt{\frac{\beta\hat{H}_{\beta}}{md^{\beta}}}+\frac{1}{\sqrt[4]{dmT}}\) to get the result. 

## Appendix C Guaranteed exploration

### Best-arm identification

**Lemma C.1**.: _Suppose for \(\varepsilon>0\) we run OMD on task \(t\in[T]\) with initialization \(\mathbf{x}_{t,1}\in\triangle^{(\varepsilon)}\), regularizer \(\psi_{\beta_{t}}+I_{\triangle^{(\varepsilon)}}\) for some \(\beta_{t}\in(0,1]\), and unbiased loss estimators (\(\gamma=0\)). If Assumption 3.1 holds and \(m>\frac{28d\log d}{3\varepsilon\Delta^{2}}\) then \(\mathbf{\hat{x}}_{t}=\mathbf{\hat{x}}_{t}\) w.p. \(\geq 1-d\kappa\), where \(\kappa=\exp\left(-\frac{3\varepsilon\Delta^{2}m}{28d}\right)\)._

Proof.: We extend the proof by Abbasi-Yadkori et al. [1, Appendices B and F] to arbitrary lower bounds \(\varepsilon/d\) on the probability. First, since \(0\leq\hat{\ell}_{t,i}(a)\leq\frac{d}{\varepsilon}\ell_{t,i}(a)\) we have that

\[-\frac{d}{\varepsilon}\leq-1\leq-\ell_{t,i}(a)\leq\hat{\ell}_{t,i}(a)-\ell_{t,i }(a)\leq\left(\frac{d}{\varepsilon}-1\right)\ell_{t,i}(a)\leq\frac{d}{\varepsilon}\] (52)

and so \(|\hat{\ell}_{t,i}(a)-\ell_{t,i}(a)|\leq\frac{d}{\varepsilon}\). Therefore since the variance of the estimated losses is a scaled Bernoulli we have that

\[\mathrm{Var}(\hat{\ell}_{t,i}(a)-\ell_{t,i}(a))=\mathrm{Var}(\hat{\ell}_{t,i}(a ))=\mathbf{x}_{t,i}(a)(1-\mathbf{x}_{t,i}(a))\left(\frac{\ell_{t,i}(a)}{ \mathbf{x}_{t,i}(a)}\right)^{2}\leq\frac{\ell_{t,i}^{2}(a)}{\mathbf{x}_{t,i}(a )}\leq\frac{d}{\varepsilon}\] (53)We can thus apply a martingale concentration inequality of Fan et al. [23, Corollary 2.1] to the martingale difference sequence (MDS) \(\frac{\varepsilon}{d}(\hat{\ell}_{t,i}(a)-\ell_{t,i}(a))\in[-\frac{\varepsilon}{ d},1]\) to obtain

\[\Pr\left(\sum_{i=1}^{m}\hat{\ell}_{t,i}(a)-\ell_{t,i}(a)\geq\frac{ m\Delta_{a}}{2}\right) =\Pr\left(\frac{\varepsilon}{d}\sum_{i=1}^{m}\hat{\ell}_{t,i}(a) -\ell_{t,i}(a)\geq\frac{\varepsilon m\Delta_{a}}{2d}\right)\] (54) \[\leq\Pr\left(\max_{j\in[m]}\frac{\varepsilon}{d}\sum_{i=j}^{m} \hat{\ell}_{t,i}(a)-\ell_{t,i}(a)\geq\frac{\varepsilon m\Delta_{a}}{2d}\right)\] \[\leq\exp\left(-\frac{2\left(\frac{\varepsilon m\Delta_{a}}{2d} \right)^{2}}{4(\varepsilon m/d+\frac{\varepsilon m\Delta_{a}}{6})}\right)\] \[=\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}}{4d(6+\Delta_{a})}\right)\] \[\leq\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}}{28d}\right)\]

where \(\Delta_{a}=\frac{1}{m}\left|\sum_{i=1}^{m}\ell_{t,i}(a)-\min_{a^{\prime}\neq a }\sum_{i=1}^{m}\ell_{t,i}(a^{\prime})\right|\) is the per-arm loss gap in the last step we apply \(\Delta_{a}\leq 1\). For the symmetric MDS \(-\frac{\varepsilon}{d}\leq\ell_{t,i}(a)-\hat{\ell}_{t,i}(a)\leq 1\) we have

\[\Pr\left(\sum_{i=1}^{m}\hat{\ell}_{t,i}(a)-\ell_{t,i}(a)\leq- \frac{m\Delta_{a}}{2}\right) =\Pr\left(\sum_{i=1}^{m}\ell_{t,i}(a)-\hat{\ell}_{t,i}(a)\geq\frac {m\Delta_{a}}{2}\right)\] (55) \[\leq\exp\left(-\frac{2\left(\frac{m\Delta_{a}}{2}\right)^{2}}{4 \left(\frac{dm}{\varepsilon}+\frac{m\Delta_{a}}{6}\right)}\right)\] \[\leq\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}/d}{4(6+ \varepsilon\Delta_{a}/d)}\right)\] \[\leq\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}}{28d}\right)\]

We can then conclude that

\[\Pr \left(\hat{\mathbf{x}}_{t}\neq\hat{\mathbf{x}}_{t}\right)\] (56) \[\leq\Pr\left(\exists\ a\neq\hat{a}_{t}t:\sum_{i=1}^{m}\hat{\ell} _{t,i}(a)\leq\sum_{i=1}^{m}\ell_{t,i}(\hat{a}_{t})\right)\] \[\leq\Pr\left(\sum_{i=1}^{m}\hat{\ell}_{t,i}(\hat{a}_{t})\geq\sum_ {i=1}^{m}\ell_{t,i}(\hat{a}_{t})+\frac{m\Delta_{\hat{a}_{t}}}{2}\ \vee\ \exists\ a\neq\hat{a}_{t}:\sum_{i=1}^{m}\hat{\ell}_{t,i}(a)\leq\sum_{i=1}^{m} \ell_{t,i}(a)-\frac{m\Delta_{a}}{2}\right)\] \[\leq\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}}{28d}\right)+ \sum_{a\neq\hat{a}_{t}}\exp\left(-\frac{3\varepsilon m\Delta_{a}^{2}}{28d}\right)\] \[\leq d\exp\left(-\frac{3\varepsilon m\Delta^{2}}{28d}\right)\]

where the second-to-last line follows by substituting the bounds (54) and (55) into the left and right terms, respectively. 

**Lemma C.2**.: _Suppose on each task \(t\in[T]\) we run OMD as in Lemma C.1. Then for any \(\beta\in(0,1]\) we have \(\frac{1}{T}\mathbb{E}\sum_{t=1}^{T}\psi_{\beta}(\hat{\mathbf{x}}_{t}^{( \varepsilon)})-\psi_{\beta}(\hat{\mathbf{x}}^{(\varepsilon)})\leq-\psi_{\beta} (\hat{\mathbf{x}})+\frac{3d\kappa\beta}{1-\beta}\left(\left(\frac{d}{ \varepsilon}\right)^{1-\beta}-1\right)\)._Proof.: We consider the expected divergence of the best initialization under the worst-case distribution of best arm estimation, which satisfies Lemma C.1 and (56). We have by Claim A.1 and the mean-as-minimizer property of Bregman divergences that

\[\frac{1}{T}\mathbb{E}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t }^{(\varepsilon)})-\psi_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)}) =\mathbb{E}\min_{\mathbf{x}\in\triangle^{(\varepsilon)}}\frac{1 }{T}\sum_{t=1}^{T}B_{\beta}\left(\mathbf{\hat{x}}_{t}^{(\varepsilon)}|| \mathbf{x}\right)\] \[\leq\min_{\mathbf{x}\in\triangle^{(\varepsilon)}}\mathbb{E}\frac {1}{T}\sum_{t=1}^{T}B_{\beta}\left(\mathbf{\hat{x}}_{t}^{(\varepsilon)}|| \mathbf{x}\right)\] \[=\min_{\mathbf{x}\in\triangle^{(\varepsilon)}}\frac{1}{T}\sum_{t =1}^{T}\sum_{a=1}^{d}\mathbb{P}(a=\hat{a}_{t})B_{\beta}\left(\mathbf{e}_{a}^{( \varepsilon)}||\mathbf{x}\right)\] \[\leq\max_{\begin{subarray}{c}\mathbf{p}_{t}\in\triangle,\forall t \in[T]\\ \mathbf{p}_{t}(a)\leq 2\kappa,\forall t\in[T],a\neq\hat{a}_{t}\\ 1-d\kappa\leq\mathbf{p}_{t}(a),\forall t\in[T],a=\hat{a}_{t}\end{subarray}} \frac{1}{T}\sum_{t=1}^{T}\sum_{a=1}^{d}\mathbf{p}_{t}(a)B_{\beta}\left( \mathbf{e}_{a}^{(\varepsilon)}||\mathbf{x}\right)\] (57)

To simplify the last expression, we define \(\mathbf{\bar{p}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{p}_{t}\) and again apply the (weighted) mean-as-minimizer property, followed by Claim A.1:

\[\min_{\mathbf{x}\in\triangle^{(\varepsilon)}}\frac{1}{T}\sum_{t =1}^{T}\sum_{a=1}^{d}\mathbf{p}_{t}(a)B_{\beta}\left(\mathbf{e}_{a}^{( \varepsilon)}||\mathbf{x}\right) =\min_{\mathbf{x}\in\triangle^{(\varepsilon)}}\sum_{a=1}^{d} \mathbf{\bar{p}}(a)B_{\beta}\left(\mathbf{e}_{a}^{(\varepsilon)}||\mathbf{x}\right)\] \[=\psi_{\beta}(\mathbf{e}_{1}^{(\varepsilon)})-\psi_{\beta}( \mathbf{\bar{p}}^{(\varepsilon)})\] (58)

By substituting into the previous inequality, we can bound the expected divergence for the worst-case \(\mathbf{p}_{t}\) as follows:

\[\frac{1}{T}\mathbb{E}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_ {t}^{(\varepsilon)})-\psi_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)}) \leq\psi_{\beta}\left(\mathbf{e}_{1}^{(\varepsilon)}\right)+ \max_{\begin{subarray}{c}\mathbf{p}_{t}\in\triangle,\forall t\in[T]\\ \mathbf{p}_{t}(a)\leq 2\kappa,\forall t\in[T],a\neq\hat{a}_{t}\\ 1-d\kappa\leq\mathbf{p}_{t}(a),\forall t\in[T],a=\hat{a}_{t}\end{subarray}}- \psi_{\beta}(\mathbf{\bar{p}}^{(\varepsilon)})\] (59) \[\leq\psi_{\beta}\left(\mathbf{e}_{1}^{(\varepsilon)}\right)+ \max_{\begin{subarray}{c}\sum_{t=1}^{T}\sum_{a=1}^{d}\mathbf{p}_{t}(a)=T\\ \sum_{t=1}^{T}\mathbf{p}_{t}(a)\leq 2\kappa(1-\mathbf{\hat{x}}(a))T+\mathbf{ \hat{x}}(a)T,\forall a\end{subarray}}-\psi_{\beta}(\mathbf{\bar{p}}^{( \varepsilon)})\] \[=\psi_{\beta}\left(\mathbf{e}_{1}^{(\varepsilon)}\right)-\min_{ \begin{subarray}{c}\mathbf{\bar{p}}\in\triangle\\ \mathbf{\bar{p}}(a)\geq 2(1-d\kappa)\mathbf{\hat{x}}(a),\forall a\end{subarray}}\psi_{ \beta}(\mathbf{\bar{p}}^{(\varepsilon)})\]

We use the shorthand \(h(\mathbf{x})=\psi_{\beta}\left((1-\varepsilon)\mathbf{x}+\frac{\varepsilon} {4}\mathbf{1}_{d}\right)\). We have

\[-\partial_{\mathbf{x}(a)}\left(\psi_{\beta}(\mathbf{x})\right) =\partial_{\mathbf{x}(a)}\left(\frac{1}{(1-\beta)}\left(\sum_{b=1}^ {d}\mathbf{x}(b)^{\beta}-1\right)\right)\] (60) \[=\partial_{\mathbf{x}(a)}\left(\frac{1}{(1-\beta)}\left(\sum_{b=1} ^{d}\mathbf{x}(b)^{\beta}+\beta d^{1-\beta}(1-\sum_{b=1}^{d}\mathbf{x}(b))-1 \right)\right)\] \[=\frac{\beta}{1-\beta}\cdot\left(\mathbf{x}(a)^{\beta-1}-d^{1- \beta}\right)\]

and therefore

\[\|\nabla h(\mathbf{x})\|_{\infty} =\max_{a=1,\ldots,d}\left|\partial_{\mathbf{x}(a)}\psi_{\beta} \left((1-\varepsilon)\mathbf{x}+\frac{\varepsilon}{d}\mathbf{1}_{d}\right)\right|\] (61) \[\leq\frac{\beta}{1-\beta}\max_{a=1,\ldots,d}\left|((1-\varepsilon )\mathbf{x}(a)+\varepsilon/d)^{\beta-1}-d^{1-\beta}\right|\] \[\leq\frac{\beta}{1-\beta}\left(\left(\frac{d}{\varepsilon} \right)^{1-\beta}-1\right)=\beta\log_{\beta}\left(\frac{d}{\varepsilon}\right)\]Finally, by convexity of \(h\) we have

\[\begin{split}\min_{\begin{subarray}{c}\mathbf{\tilde{p}}\in\triangle \\ \mathbf{\tilde{p}}(a)\geq(1-d\kappa)\tilde{\mathbf{x}}(a),\forall a\\ \mathbf{\tilde{p}}(a)\leq 2\kappa+(1-2\kappa)\tilde{\mathbf{x}}(a),\forall a \end{subarray}}h(\mathbf{\tilde{p}})&\geq h(\mathbf{\hat{x}})- \|\nabla h(\mathbf{\hat{x}})\|_{\infty}\max_{\begin{subarray}{c}\mathbf{ \tilde{p}}\in\triangle\\ \mathbf{\tilde{p}}(a)\geq(1-d\kappa)\tilde{\mathbf{x}}(a),\forall a\\ \mathbf{\tilde{p}}(a)\leq 2\kappa+(1-2\kappa)\tilde{\mathbf{x}}(a),\forall a \end{subarray}}\|\mathbf{\tilde{p}}-\mathbf{\hat{x}}\|_{1}\\ &\geq h(\mathbf{\hat{x}})-3d\kappa\|\nabla h(\mathbf{\hat{x}})\|_{ \infty}\\ &\geq h(\mathbf{\hat{x}})-3d\kappa\beta\log_{\beta}\left(\frac{d}{ \varepsilon}\right)\end{split}\] (62)

so we can substitute into (59) to get

\[\frac{1}{T}\mathbb{E}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t}^{( \varepsilon)})-\psi_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)})\leq-\psi_{ \beta}(\mathbf{\hat{x}}^{(\varepsilon)})+\frac{3d\kappa\beta}{1-\beta}\left( \left(\frac{d}{\varepsilon}\right)^{1-\beta}-1\right)\] (63)

Applying Lemma B.2 completes the proof. 

### Guaranteed exploration bounds

**Lemma C.3**.: _Suppose we play \(\texttt{OMD}_{\beta,\eta}\) with initialization \(\mathbf{x}_{1}\in\triangle^{(\varepsilon)}\), regularizer \(\psi_{\beta}+I_{\triangle^{(\varepsilon)}}\) for some \(\beta\in(0,1]\), and unbiased loss estimators (\(\gamma=0\)) on the sequence of loss functions \(\ell_{1},\dots,\ell_{T}\in[0,1]^{d}\). Then for any \(\hat{a}\in[d]\) we have expected regret_

\[\mathbb{E}\sum_{t=1}^{T}\ell_{t}(a_{t})-\ell_{t}(\hat{a})\leq\frac{\mathbb{E}B _{\beta}(\mathbf{\hat{x}}^{(\varepsilon)}\|\mathbf{x}_{1})}{\eta}+\frac{\eta d ^{\beta}m}{\beta}+\varepsilon m\] (64)

_for \(\mathbf{\hat{x}}\) the estimated optimum of the loss estimators \(\hat{\ell}_{1},\dots,\hat{\ell}_{T}\)._

Proof.: \[\begin{split}\mathbb{E}\sum_{t=1}^{T}\ell_{t}(a_{t})-\ell_{t}( \hat{a})&=\mathbb{E}\sum_{t=1}^{T}\ell_{t}(a_{t})-\langle\ell_{ t},\mathbf{\hat{x}}\rangle\\ &\leq\mathbb{E}\sum_{t=1}^{T}\ell_{t}(a_{t})-\langle\ell_{t}, \mathbf{\hat{x}}^{(\varepsilon)}\rangle+\varepsilon m\\ &=\mathbb{E}\sum_{t=1}^{m}\hat{\ell}_{t}(a_{t})-\langle\hat{\ell }_{t},\mathbf{\hat{x}}^{(\varepsilon)}\rangle+\varepsilon m\\ &\leq\mathbb{E}\sum_{t=1}^{m}\hat{\ell}_{t}(a_{t})-\langle\hat{ \ell}_{t},\mathbf{\hat{x}}^{(\varepsilon)}\rangle+\varepsilon m\\ &\leq\mathbb{E}\left(\frac{B_{\beta}(\mathbf{\hat{x}}^{( \varepsilon)}||\mathbf{x}_{1})}{\eta}+\frac{\eta}{\beta}\sum_{t=1}^{T}\sum_{a=1 }^{d}\hat{\ell}_{t}^{2}(a)\mathbf{x}_{t}^{2-\beta}(a)\right)+\varepsilon m\\ &\leq\frac{\mathbb{E}B_{\beta}(\mathbf{\hat{x}}^{(\varepsilon)} ||\mathbf{x}_{1})}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\varepsilon m\end{split}\] (65)

where the second inequality follows by optimality of \(\mathbf{\hat{x}}\) for the estimated losses \(\hat{\ell}_{t}\), the third by Lemma B.3 constrained to \(\triangle^{(\varepsilon)}\), and the fourth similarly to Theorem B.1 (note both are also effectively shown in Luo [37]). 

**Theorem C.1**.: _In Algorithm 1, let \(\texttt{OMD}_{\eta,\beta}\) be online mirror descent with the regularizer \(\psi_{\beta}+I_{\triangle^{(\varepsilon)}}\) over unbiased (\(\gamma=0\)) loss estimators, \(\Theta_{k}\) is a subset of \([\underline{\beta},\overline{\beta}]\subset[\frac{1}{\log d},1]\), and_

\[U_{t}(\mathbf{x},\eta,\beta)=\frac{B_{\beta}(\mathbf{\hat{x}}_{t}^{( \varepsilon)}||\mathbf{x})}{\eta}+\frac{\eta d^{\beta}m}{\beta}\] (66)_where \(\hat{\mathbf{x}}_{t}^{(\varepsilon)}=(1-\varepsilon)\hat{\mathbf{x}}_{t}+ \varepsilon\mathbf{1}_{d}/d\). Note that \(U_{t}^{(\rho)}(\mathbf{x},\eta,\beta)=U_{t}(\mathbf{x},\eta,\beta)+\frac{\rho^ {2}(d^{1-\beta}-1)}{\eta(1-\beta)}\). Then under Assumption 3.1 there exists settings of \(\underline{\eta},\overline{\eta},\alpha,\lambda\) s.t. for all \(\varepsilon,\rho\in(0,1)\) we have that_

\[\begin{split}&\mathbb{E}\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{m} \ell_{t,i}(a_{t,i})-\ell_{t,i}(\hat{a}_{t})\\ &\leq\varepsilon m+\frac{8d\sqrt{m}}{\rho}\left(1_{k>1}\sqrt{ \frac{\log k}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\\ &\quad+\min_{\beta\in[\underline{\rho},\overline{\beta}],\eta>0} \frac{8\left(\frac{d}{\varepsilon}\right)^{2-\underline{\beta}}(1+\log T)}{ \eta T}+\frac{h_{\beta}(\Delta)}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{L _{\eta}(\overline{\beta}-\underline{\beta})}{2k}+d\min\left\{\frac{\rho^{2} }{2\eta},\rho\sqrt{m}\right\}\end{split}\] (67)

_for \(L_{\eta}=\left(\frac{\log\frac{d}{\varepsilon}}{\eta}+\eta m\log^{2}d\right)d\) and \(h_{\beta}(\Delta)=(H_{\beta}+\frac{56}{dm})\iota_{\Delta}+\frac{d^{1-\beta}-1} {1-\beta}(1-\iota_{\Delta})\) for \(\iota_{\Delta}=1_{m\geq\frac{75d}{\varepsilon\Delta}\log\frac{d}{\varepsilon \Delta^{2}}}\)._

Proof.: By Lemma C.3 we have

\[\mathbb{E}\sum_{t=1}^{T}\sum_{i=1}^{m}\ell_{t,i}(a_{t,i})-\ell_{t,i}(\hat{a}_{ t})\leq\varepsilon mT+\mathbb{E}\sum_{t=1}^{T}\frac{B_{\beta_{t}}(\hat{ \mathbf{x}}_{t}^{(\varepsilon)}||\mathbf{x}_{t,1})}{\eta_{t}}+\frac{\eta_{t}d ^{\beta_{t}}m}{\beta_{t}}\] (68)

Since we have the same environment-dependent quantities as in Theorem B.1, we can substitute the above bound into Theorem A.1 and then apply the Lemma C.2 bound

\[\begin{split}\mathbb{E}\hat{V}_{\beta}^{2}\leq H_{\beta}+\frac{3 d\kappa\beta}{1-\beta}\left(\left(\frac{d}{\varepsilon}\right)^{1-\beta}-1\right)& \leq H_{\beta}+\frac{3d^{2}}{\varepsilon}\exp\left(-\frac{3 \varepsilon\Delta^{2}m}{28d}\right)\\ &=H_{\beta}+\frac{3\varepsilon\Delta^{2}}{d^{2}}\exp\left(4\log \frac{d}{\varepsilon\Delta^{2}}-\frac{3\varepsilon\Delta^{2}m}{28d}\right)\\ &\leq H_{\beta}+\frac{3\varepsilon\Delta^{2}/d^{2}}{\frac{3 \varepsilon\Delta^{2}m}{28d}-4\log\frac{d}{\varepsilon\Delta^{2}}}\\ &\leq H_{\beta}+\frac{56}{dm}\end{split}\] (69)

where the last line follows by assuming \(m\geq\frac{75d}{\varepsilon\Delta^{2}}\log\frac{d}{\varepsilon\Delta^{2}}\). If this condition does not hold, then we apply the default bound of \(\mathbb{E}\hat{V}_{\beta}^{2}\leq=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\hat{ \mathbf{x}}_{t})-\psi_{\beta}(\hat{\mathbf{x}})\leq\frac{d^{1-\beta}-1}{1-\beta}\). 

**Corollary C.1**.: _Let \(\underline{\beta}=\overline{\beta}=1\). Then for known \(\Delta\) and assuming \(m\geq\frac{75d}{\Delta^{2}}\log\frac{d}{\Delta^{2}}\) we can ensure expected task-averaged regret at most_

\[2\sqrt{H_{1}dm+56}+\frac{75d}{\Delta^{2}}W\left(\frac{m}{75}\right)+\tilde{ \mathcal{O}}\left(\frac{d^{\frac{3}{2}}m^{\frac{3}{4}}}{\sqrt{T}}+\frac{d \Delta^{2}m^{2}}{T}\right)\] (70)

_where \(W\) is the Lambert \(W\)-function, while for unknown \(\Delta\) we can ensure expected task-averaged regret at most_

\[2\sqrt{H_{1}dm+56}+\frac{3}{\Delta}\sqrt[3]{50dm\log d\log\frac{d^{2}m^{2}}{150 \Delta^{6}\log d}}+\tilde{\mathcal{O}}\left(\frac{d^{\frac{3}{2}}m^{\frac{3}{4 }}}{\sqrt{T}}+\frac{d^{\frac{3}{3}}m^{\frac{5}{3}}}{T}\right)\] (71)

_so long as \(m^{2}\geq 150d\log d\)._

Proof.: Applying Theorem C.1 and simplifying yields

\[\varepsilon m+\frac{8d\sqrt{m}(1+\log(T+1))}{16\rho^{2}T}+\min_{\eta>0}\frac{8d (1+\log T)}{\varepsilon\eta T}+\frac{h_{1}(\Delta)}{\eta}+\eta dm+\frac{d\rho^ {2}}{2\eta}\] (72)

Then substitute \(\eta=\sqrt{\frac{h_{1}(\Delta)}{dm}}\) and set \(\rho=\sqrt[4]{\frac{1}{dT\sqrt{m}}}\) and \(\varepsilon=\frac{75d}{\Delta^{2}m}W(\frac{m}{75})\) (for known \(\Delta\)) or \(\varepsilon=\sqrt[3]{\frac{150d\log\frac{d}{\varepsilon}}{m^{2}}}\) (otherwise).

**Corollary C.2**.: _Let \(\underline{\beta}=\frac{1}{2}\) and \(\overline{\beta}=1\). Then for known \(\Delta\) and assuming \(m\geq\frac{75d}{\Delta^{2}}\log\frac{d}{\Delta^{2}}\) we can ensure task-averaged regret at most_

\[\min_{\beta\in[\frac{1}{2},1]}2\sqrt{(H_{\beta}m+56/d)d^{\beta}/\beta}+\frac{75 d}{\Delta^{2}}W\left(\frac{m}{75}\right)+\tilde{\mathcal{O}}\left(\frac{d^{ \frac{4}{3}}m^{\frac{2}{3}}}{\sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{\frac{5}{6}} }{T^{\frac{2}{3}}}+\frac{d\Delta^{3}m^{\frac{5}{6}}}{T}\right)\] (73)

_using \(k=\lceil\sqrt[3]{d^{2}mT}\rceil\), while for unknown \(\Delta\) we can ensure expected task-averaged regret at most_

\[\min_{\beta\in[\frac{1}{2},1]}2\sqrt{(H_{\beta}m+56/d)d^{\beta}/\beta}+\frac{ 3}{\Delta}\sqrt[3]{50d^{2}m\log\frac{dm^{2}}{150\Delta^{6}}}+\tilde{\mathcal{ O}}\left(\frac{d^{\frac{4}{3}}m^{\frac{2}{3}}}{\sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{ \frac{5}{6}}}{T^{\frac{2}{3}}}+\frac{d^{\frac{3}{2}}m^{2}}{T}\right)\] (74)

_so long as \(m\geq 5d\sqrt{6}\)._

Proof.: Applying Theorem C.1 and simplifying yields

\[\begin{split}&\varepsilon m+\frac{8d\sqrt{m}}{\rho}\left(\sqrt{ \frac{\log k}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\\ &\quad+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta> 0}\frac{8d^{\frac{3}{6}}(1+\log T)}{\varepsilon^{\frac{3}{2}}\eta T}+\frac{h _{\beta}(\Delta)}{\eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{d}{4k}\left(\frac {\log\frac{d}{\varepsilon}}{\eta}+\eta m\log^{2}d\right)+\frac{d\rho^{2}}{2\eta }\end{split}\] (75)

Then substitute \(\eta=\sqrt{\frac{h_{\beta}(\Delta)}{d^{\beta}m/\beta}}\) and set \(\rho=\sqrt[3]{\frac{1}{d\sqrt{mT}}}\) and \(\varepsilon=\frac{75d}{\Delta^{2}m}W(\frac{m}{75})\) (for known \(\Delta\)) or \(\varepsilon=\sqrt[3]{\frac{150d^{2}}{m^{2}}}\) (otherwise). 

**Corollary C.3**.: _Let \(\underline{\beta}=\frac{1}{\log d}\) and \(\overline{\beta}=1\). Then for known \(\Delta\) and assuming \(m\geq\frac{75d}{\Delta^{2}}\log\frac{d}{\Delta^{2}}\) we can ensure task-averaged regret at most_

\[\min_{\beta\in(0,1]}2\sqrt{(H_{\beta}m+56/d)d^{\beta}/\beta}+\frac{75d}{ \Delta^{2}}W\left(\frac{m}{75}\right)+\tilde{\mathcal{O}}\left(\frac{d^{\frac{ 4}{3}}m^{\frac{2}{3}}}{\sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{\frac{5}{6}}}{T^{ \frac{2}{3}}}+\frac{d\Delta^{4}m^{3}}{T}\right)\] (76)

_using \(k=\lceil\sqrt[3]{d^{2}mT}\rceil\), while for unknown \(\Delta\) we can ensure expected task-averaged regret at most_

\[\min_{\beta\in(0,1]}2\sqrt{(H_{\beta}m+56/d)d^{\beta}/\beta}+\frac{3}{\Delta} \sqrt[3]{50d^{2}m\log\frac{dm^{2}}{150\Delta^{6}}}+\tilde{\mathcal{O}}\left( \frac{d^{\frac{4}{3}}m^{\frac{2}{3}}}{\sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{ \frac{5}{6}}}{T^{\frac{2}{3}}}+\frac{d^{\frac{5}{3}}m^{\frac{7}{3}}}{T}\right)\] (77)

_so long as \(m\geq 5d\sqrt{6}\)._

Proof.: Applying Theorem C.1 and simplifying yields

\[\begin{split}&\varepsilon m+\frac{8d\sqrt{m}}{\rho}\left(\sqrt{ \frac{\log k}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\\ &\quad+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta> 0}\frac{8d^{2}(1+\log T)}{\varepsilon^{2}\eta T}+\frac{h_{\beta}(\Delta)}{ \eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{d}{2k}\left(\frac{\log\frac{d}{ \varepsilon}}{\eta}+\eta m\log^{2}d\right)+\frac{d\rho^{2}}{2\eta}\end{split}\] (78)

Then substitute \(\eta=\sqrt{\frac{h_{\beta}(\Delta)}{d^{\beta}m/\beta}}\) and set \(\rho=\sqrt[3]{\frac{1}{d\sqrt{mT}}}\) and \(\varepsilon=\frac{75d}{\Delta^{2}m}W(\frac{m}{75})\) (for known \(\Delta\)) or \(\varepsilon=\sqrt[3]{\frac{150d^{2}}{m^{2}}}\) (otherwise). 

**Corollary C.4**.: _Let \(\underline{\beta}=\frac{1}{\log d}\) and \(\overline{\beta}=1\). Then for unknown \(\Delta\) and assuming \(m\geq\max\{d^{\frac{3}{4}},56\}\) we can ensure task-averaged regret at most_

\[\min_{\beta\in(0,1]}\min\left\{8\sqrt[3]{d^{2}m+56}\frac{d^{\beta}}{\beta}+ \frac{21d^{\frac{4}{3}}\sqrt[3]{m}}{\Delta}\sqrt[3]{3\log\frac{dm}{\Delta^{2}} }\right\}+\tilde{\mathcal{O}}\left(\frac{d^{\frac{4}{3}}m^{\frac{2}{3}}}{ \sqrt[3]{T}}+\frac{d^{\frac{5}{3}}m^{\frac{5}{6}}}{T^{\frac{2}{3}}}+\frac{d^{2 }m^{\frac{5}{6}}}{T}\right)\] (79)

_using \(k=\lceil\sqrt[3]{d^{2}mT}\rceil\)._Proof.: Applying Theorem C.1 and simplifying yields

\[\begin{split}&\varepsilon m+\frac{8d\sqrt{m}}{\rho}\left(\sqrt{ \frac{\log k}{T}}+\frac{1+\log(T+1)}{16\rho T}\right)\\ &\quad+\min_{\beta\in[\underline{\beta},\overline{\beta}],\eta> 0}\frac{8d^{2}(1+\log T)}{\varepsilon^{2}\eta T}+\frac{h_{\beta}(\Delta)}{ \eta}+\frac{\eta d^{\beta}m}{\beta}+\frac{d}{2k}\left(\frac{\log\frac{d}{ \varepsilon}}{\eta}+\eta m\log^{2}d\right)+\frac{d\rho^{2}}{2\eta}\end{split}\] (80)

Then substitute \(\eta=\sqrt{\frac{h_{\beta}(\Delta)}{d^{\beta}m/\beta}}\) and set \(\rho=\sqrt[3]{\frac{1}{d\sqrt{mT}}}\) and \(\varepsilon=\frac{\sqrt[3]{\frac{1}{d\sqrt{m^{2}}}}}{\sqrt[3]{m^{2}}}\). 

## Appendix D Robustness to outliers

**Proposition D.1**.: _Suppose there exists a constant \(p\in[0,1]\) and a subset \(S\subset[T]\) of size \(s\) such that \(\hat{a}_{t}\in S\) for all but \(\mathcal{O}(T^{p})\) MAB tasks \(t\in[T]\). Then if \(\beta\in[\frac{1}{\log d},\frac{1}{2}]\) we have \(H_{\beta}=\mathcal{O}(s+\frac{d^{1-\beta}}{T^{\beta(1-p)}})\)._

Proof.: Define the vector \(\mathbf{e}_{S}\in[0,1]^{d}\) s.t. \(\mathbf{e}_{S[a]}=1_{a\in S}\). Then by Claim A.1 and the mean-as-minimizer property of Bregman divergences we have

\[\begin{split} H_{\beta}&=-\psi_{\beta}(\mathbf{ \hat{x}})\\ &=\frac{1}{T}\sum_{t=1}^{T}\psi_{\beta}(\mathbf{\hat{x}}_{t})- \psi_{\beta}(\mathbf{\hat{x}})\\ &=\frac{1}{T}\sum_{t=1}^{T}B_{\beta}(\mathbf{\hat{x}}_{t}|| \mathbf{\hat{x}})\\ &=\min_{\mathbf{x}\in\triangle_{d}}\frac{1}{T}\sum_{t=1}^{T}B_{ \beta}(\mathbf{\hat{x}}_{t}||\mathbf{\hat{x}})\\ &\leq\min_{\delta\in(0,1)}\frac{1}{T}\sum_{t=1}^{T}B_{\beta}\left( \mathbf{\hat{x}}_{t}\middle|\left|\frac{1-\delta}{s}\mathbf{e}_{S}+\frac{\delta }{d}\mathbf{1}_{d}\right.\right)\\ &=\min_{\delta\in(0,1)}\frac{1}{T}\sum_{t=1}^{T}\frac{1}{1-\beta }\sum_{a=1}^{d}\left(\frac{1-\delta}{s}1_{a\in S}+\frac{\delta}{d}\right)^{ \beta}-\mathbf{\hat{x}}_{t[a]}^{\beta}+\frac{\beta(\mathbf{\hat{x}}_{t[a]}- \frac{1-\delta}{s}1_{a\in S}-\frac{\delta}{d})}{(\frac{1-\delta}{s}1_{s\in S} +\frac{\delta}{d})^{\beta}}\\ &=\min_{\delta\in(0,1)}\frac{1}{T}\sum_{t=1}^{T}\sum_{a=1}^{d} \left(\frac{1-\delta}{s}1_{a\in S}+\frac{\delta}{d}\right)^{\beta}-\frac{ \mathbf{\hat{x}}_{t[a]}^{\beta}}{1-\beta}+\frac{\beta\mathbf{\hat{x}}_{t[a]}^ {\beta}}{(1-\beta)(\frac{1-\delta}{s}1_{a\in S}+\frac{\delta}{d})^{1-\beta}}\\ &\leq\min_{\delta\in(0,1)}s^{1-\beta}+\delta^{\beta}d^{1-\beta}+ \frac{\beta}{(1-\beta)T}\sum_{t=1}^{T}\sum_{a=1}^{d}\frac{1_{a=\hat{a}_{t}}}{ (1-\beta)(\frac{1-\delta}{s}1_{a\in S}+\frac{\delta}{d})^{1-\beta}}\\ &\leq\min_{\delta\in(0,1)}\frac{s^{1-\beta}}{1-\beta}+\delta^{ \beta}d^{1-\beta}+\mathcal{O}\left(\frac{\beta(\frac{d}{\delta})^{1-\beta}}{(1 -\beta)T^{1-p}}\right)\\ &=\mathcal{O}\left(s+\frac{d^{1-\beta}}{T^{\beta(1-p)}}\right) \end{split}\] (81)

where the last line follows by considering \(\delta=1/T^{1-p}\).

Online learning with self-concordant barrier regularizers

### General results

**Lemma E.1**.: _Let \(\mathcal{K}\subset\mathbb{R}^{d}\) be a convex set and \(\psi:\mathcal{K}^{\circ}\mapsto\mathbb{R}^{d}\) be a self-concordant barrier. Suppose \(\ell_{1},\ldots,\ell_{T}\) are a sequence of loss functions satisfying \(|\langle\ell_{t},\mathbf{x}\rangle|\leq 1\ \forall\ \mathbf{x}\in\mathcal{K}\). Then if we run OMD with step-size \(\eta>0\) as in Abernethy et al. [2, Alg. 1] on the sequence of estimators \(\hat{\ell}_{t}\) our estimated regret w.r.t. any \(\mathbf{x}\in\mathcal{K}_{\varepsilon}\) for \(\varepsilon>0\) will satisfy_

\[\sum_{t=1}^{T}\langle\hat{\ell}_{t},\mathbf{x}_{t}-\mathbf{x}\rangle\leq \frac{B(\mathbf{x}||\mathbf{x}_{1})}{\eta}+32d^{2}\eta T\] (82)

Proof.: The result follows from Abernethy et al. [2] by stopping the derivation on the second inequality below Equation 10. 

**Definition E.1**.: _For any convex set \(\mathcal{K}\) and any point \(\mathbf{y}\in\mathcal{K}\), \(\pi_{\mathbf{y}}(\mathbf{x})=\inf\limits_{t\geq 0,\mathbf{y}^{+}\frac{ \mathbf{x}-\mathbf{x}}{t}\in\mathcal{K}}t\) is the_

**Minkowski function with pole \(\mathbf{y}\)**.

**Lemma E.2**.: _For any \(\mathbf{x}\in\mathcal{K}\subset\mathbb{R}^{d}\) and \(\psi:\mathcal{K}^{\circ}\mapsto\mathbb{R}\) a \(\nu\)-self-concordant regularizer with minimum \(\mathbf{x}_{1}\in\mathcal{K}^{\circ}\), the quantity \(\psi(\mathbf{c}_{\varepsilon}(\mathbf{x}))\) is \(\nu\sqrt{2}\)-Lipschitz w.r.t. \(\varepsilon\in[0,1]\)._

Proof.: Consider any \(\varepsilon,\varepsilon^{\prime}\in[0,1]\) s.t. \(\varepsilon^{\prime}-\varepsilon\in(0,\frac{1}{2}]\) Note that for \(t=\frac{\varepsilon^{\prime}-\varepsilon}{1+\varepsilon}\) we have

\[\mathbf{c}_{\varepsilon^{\prime}}(\mathbf{x})+\frac{\mathbf{c}_{\varepsilon ^{\prime}}(\mathbf{x})-\mathbf{c}_{\varepsilon}(\mathbf{x})}{t}=\mathbf{x}_{ 1}+\frac{\mathbf{x}-\mathbf{x}_{1}}{1+\varepsilon^{\prime}}+\frac{\mathbf{x} _{1}+\mathbf{x}-\mathbf{x}_{1}-\mathbf{x}_{1}-\mathbf{x}_{1}}{t}=\mathbf{x} \in\mathcal{K}\] (83)

so \(\pi_{\mathbf{c}_{\varepsilon^{\prime}}(\mathbf{x})}(\mathbf{c}_{\varepsilon }(\mathbf{x}))\leq\frac{\varepsilon^{\prime}-\varepsilon}{1+\varepsilon} \leq\varepsilon^{\prime}-\varepsilon\). Therefore by Nesterov and Nemirovskii [42, Prop. 2.3.2] we have

\[\psi(\mathbf{c}_{\varepsilon}(\mathbf{x}))-\psi(\mathbf{c}_{\varepsilon^{ \prime}}(\mathbf{x}))\leq\nu\log\left(\frac{1}{1-\pi_{\mathbf{c}_{\varepsilon ^{\prime}}(\mathbf{x})}(\mathbf{c}_{\varepsilon}(\mathbf{x}))}\right)\leq\nu \log\left(\frac{1}{1+\varepsilon-\varepsilon^{\prime}}\right)\leq\nu( \varepsilon^{\prime}-\varepsilon)\sqrt{2}\] (84)

where for the last inequality we used \(-\log(1-x)\leq x\sqrt{2}\) for \(x\in[0,\frac{1}{2}]\). The case of \(\varepsilon^{\prime}-\varepsilon\in(0,1]\) follows by considering \(\varepsilon^{\prime\prime}=\frac{\varepsilon^{\prime}+\varepsilon}{2}\) and applying the above twice.

**Theorem E.1**.: _In Algorithm 1, let \(\textsc{OMD}_{\eta,\varepsilon}\) be online mirror descent over loss estimators specified in Abernethy et al. [2] with a \(\nu\)-self-concordant barrier regularizer \(\psi:\mathcal{K}^{\circ}\mapsto\mathbb{R}\) that satisfies \(\nu\geq 1\) and \(\|\nabla^{2}\psi(\mathbf{x}_{1})\|_{2}=S_{1}\geq 1\). Let \(\Theta_{k}\) be a subset of \([\frac{1}{m},1]\) and_

\[U_{t}(\mathbf{x},\eta,\varepsilon)=\frac{B(\mathbf{c}_{\varepsilon}(\mathbf{ \hat{x}})||\mathbf{x})}{\eta}+32\eta d^{2}+\varepsilon m\] (85)

_Note that \(U_{t}^{(\rho)}(\mathbf{x},\eta,\varepsilon)=U_{t}(\mathbf{x},\eta, \varepsilon)+\frac{\eta\omega^{\frac{3}{2}}\rho^{2}Km\sqrt{S_{1}}}{\eta}\). Then there exists settings of \(\underline{\eta},\overline{\eta},\alpha,\lambda\) s.t. for all \(\varepsilon,\rho\in(0,1)\) we have expected task averaged regret at most_

\[\mathbb{E} \min_{\varepsilon\in[\frac{1}{m},1],\eta>0}\frac{512\nu^{2}K^{2}S _{1}m^{2}(1+\log T)}{\eta}+\left(\frac{\hat{V}_{\varepsilon}^{2}}{\eta}+32 \eta d^{2}m+\varepsilon m+\frac{\nu\sqrt{2}/\eta+m}{k}\right)T\] (86) \[+3\nu^{\frac{3}{4}}m\min\left\{\frac{3\rho^{2}\nu^{\frac{3}{4}} K\sqrt{S_{1}}}{\eta},4d\rho\sqrt{2K\sqrt{S_{1}}}\right\}T\] \[+\frac{7dm}{\rho}\sqrt{2K\sqrt{\nu^{3}S_{1}}}\left(7\sqrt{T\log k }+\frac{1+\log(T+1)}{\rho}\right)\]

Proof.: Let \(\underline{\varepsilon}=\frac{1}{m}\). For any \(\varepsilon\in[\underline{\varepsilon},1]\) and \(\mathbf{x}\in\mathcal{K}\) we have \(\pi_{\mathbf{x}_{1}}(\mathbf{c}_{\varepsilon}(\mathbf{x}))\leq\frac{1}{1+ \varepsilon}\), so by Nesterov and Nemirovskii [42, Prop. 2.3.2] we have

\[\|\nabla^{2}\psi(\mathbf{c}_{\varepsilon}(\mathbf{x}))\|_{2}\leq\left(\frac{ 1+3\nu}{1-\pi_{\mathbf{x}_{1}}(\mathbf{c}_{\varepsilon}(\mathbf{x}))}\right)^ {2}\|\nabla^{2}\psi(\mathbf{x}_{1})\|_{2}\leq\frac{64\nu^{2}S_{1}}{\varepsilon ^{2}}\] (87)

Thus \(S=\max_{\mathbf{x},\mathbf{y}\in\mathcal{K},\varepsilon\in[\underline{ \varepsilon},1]}\|\nabla^{2}\psi(\mathbf{c}_{\varepsilon}(\mathbf{x}))\|_{2}= \frac{64\nu^{2}S_{1}}{\varepsilon^{2}}\) and also

\[D_{\varepsilon}^{2} =\max_{\mathbf{x},\mathbf{y}\in\mathcal{K}}B(\mathbf{c}_{ \varepsilon}(\mathbf{x})||\mathbf{c}_{\varepsilon}(\mathbf{y}))\] (88) \[=\max_{\mathbf{x},\mathbf{y}\in\mathcal{K}}\psi(\mathbf{c}_{ \varepsilon}(\mathbf{x}))-\psi(\mathbf{c}_{\varepsilon}(\mathbf{y}))-\langle \nabla\psi(\mathbf{c}_{\varepsilon}(\mathbf{y})),\mathbf{x}-\mathbf{y}\rangle\] \[\leq\max_{\mathbf{x},\mathbf{y}\in\mathcal{K}}\nu\log\left(\frac{ 1}{1-\pi_{\mathbf{x}_{1}}(\mathbf{c}_{\varepsilon}(\mathbf{x}))}\right)+ \sqrt{\nu\|\nabla^{2}\psi(\mathbf{c}_{\varepsilon}(\mathbf{y}))\|_{2}}\| \mathbf{x}-\mathbf{y}\|_{2}\] \[\leq\nu\log\frac{2}{\varepsilon}+\frac{8\nu^{3}K\sqrt{S_{1}}}{\varepsilon}\] \[\leq\frac{9\nu^{\frac{3}{2}}K\sqrt{S_{1}}}{\varepsilon}\]

where the first inequality follows by Nesterov and Nemirovskii [42, Prop. 2.3.2] and the definition of a self-concordant barrier [2, Def. 5]. In addition, we have \(g(\varepsilon)=32d^{2}\), \(f(\varepsilon)=\varepsilon\), \(M=12d\sqrt{2Km/\varepsilon}\sqrt[4]{\nu^{3}S_{1}}\), and \(F=1\). We have

\[\mathbb{E}\sum_{t=1}^{T}\sum_{i=1}^{m}\langle\ell_{t,i},\mathbf{ x}_{t,i}-\mathbf{\hat{x}}_{t}\rangle \leq\mathbb{E}\sum_{t=1}^{T}\varepsilon_{t}m+\sum_{i=1}^{m}\langle \ell_{t,i},\mathbf{x}_{t,i}-\mathbf{c}_{\varepsilon_{t}}(\mathbf{\hat{x}}_{t})\rangle\] (89) \[\leq\mathbb{E}\sum_{t=1}^{T}\varepsilon_{t}m+\sum_{i=1}^{m} \langle\hat{\ell}_{t,i},\mathbf{x}_{t,i}-\mathbf{c}_{\varepsilon_{t}}(\mathbf{ \hat{x}}_{t})\rangle\] \[\leq\sum_{t=1}^{T}\frac{\mathbb{E}B(\mathbf{c}_{\varepsilon_{t}}( \mathbf{\hat{x}}_{t}||\mathbf{x}_{t,1})}{\eta_{t}}+(32\eta_{t}d^{2}+\varepsilon _{t})m\]

where the first inequality follows by Abernethy et al. [2, Lem. 8], the second by Abernethy et al. [2, Lem. 3], the third by optimality of \(\mathbf{\hat{x}}_{t}\), and the fourth by Lemma E.1. Substituting into Theorem A.1 and simplifying yields the result.

[MISSING_PAGE_FAIL:31]