# Mixed Dynamics In Linear Networks:

Unifying the Lazy and Active Regimes

 Zhenfeng Tu

Courant Institute

New York University

New York, NY 10012

zt2255@nyu.edu

&Santiago Aranguri

Courant Institute

New York University

New York, NY 10012

aranguri@nyu.edu

&Arthur Jacot

Courant Institute

New York University

New York, NY 10012

arthur.jacot@nyu.edu

###### Abstract

The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unifying formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the 'best of both worlds': it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task.

## 1 Introduction

Whether in linear networks or nonlinear ones, there has been a lot of interest in the distinction between the lazy regime [27] and the active regime [16; 43; 15; 52; 13] as the number of neurons grows towards infinity. In the lazy regime the training dynamics become linear, so that they can be easily described in terms of the Neural Tangent Kernel (NTK) [27; 9; 51; 36], while the active regime exhibits complex nonlinear dynamics. While our understanding of the active regime remains much more limited, it appears to be characterized by the emergence of feature learning[22; 52], and of a form of sparsity [3; 11; 2; 1] (the type of sparsity observed depends on the network type [11; 19; 25; 24], but we will focus on fully-connected linear networks which exhibit a rank sparsity in the learned linear map [7; 35; 28; 47]) which are both absent in the lazy regime.

Note that even though it is common to talk of the 'the' active regime, we do not know yet whether there is only one or multiple active regimes. Indeed the term active regime is usually used to describe any regime that differs from the lazy regime and exhibit some form of feature learning. Though we do not have an complete understanding of where the lazy regimes ends and the active regime(s) start, we know that the lazy regime requires extreme overparametrization (a large number of neurons in comparison to the number of datapoints) [5; 20], a 'large' initialization of the weights [15], a small learning rate, and early stopping when using a cross-entropy loss or weight decay. Indeed, active regimes have been observed by breaking either of these requirements: taking limits with mild or no overparametrization [10], taking smaller or even vanishingly small initializations [35; 28], using largelearning rates [33] or SGD [42, 47], or studying the late training dynamics with the cross-entropy loss [30, 17] or weight decay [34, 39, 29, 26]. Though each of these can lead to active regimes with significantly different dynamics, they often lead to similar types of feature learning and sparsity.

In this paper, we study this transition in the context of linear networks and focus mainly on the effects of the width \(w\) and the variance of the weights at initialization \(\sigma^{2}\), and give a precise and almost complete phase diagram, showing the transitions between lazy and active regimes. In this setting, we will show that there typically is only 'one' active regime, which is the same (up to approximation) as the already well-studied balanced regime [44, 7, 8].

But our result also paint a more subtle picture than the lazy/active dichotomy. We propose a more granular approach, where at a certain time some part of the network can be in the lazy regime, while others are in the active or balanced regime. More precisely the network is lazy along the singular values of the matrix represented by the network that are smaller than \(\sigma^{2}w\), and in the active regime along the singular values larger than \(\sigma^{2}w\).

### Contributions

We consider the training dynamics of shallow linear networks \(A_{\theta}=W_{2}W_{1}\) and show that for large enough width \(w\) (the inner dimension), and a iid \(\mathcal{N}(0,\sigma^{2})\) initialization of all weights, the dynamics of \(A_{\theta(t)}\) as a result of training the parameters \(\theta=(W_{1},W_{2})\) with GD/GF on the loss \(\mathcal{L}(\theta)=C(A_{\theta})\) for a general matrix cost \(C\) with learning rate \(\eta\) is approximately given by the self-consistent dynamics

\[\partial_{t}A_{\theta(t)}\approx-\eta\sqrt{A_{\theta}A_{\theta}^{T}+\sigma^{ 4}w^{2}I}\nabla C(A_{\theta})-\eta\nabla C(A_{\theta})\sqrt{A_{\theta}^{T}A_{ \theta}+\sigma^{4}w^{2}I}.\] (1)

These dynamics contain as special cases both the lazy dynamics

\[\partial_{t}A_{\theta(t)}\approx-2\eta\sigma^{2}w\nabla C(A_{\theta})\]

when \(\sigma^{2}w\gg\lambda_{max}(A_{\theta})\) and the balanced dynamics

\[\partial_{t}A_{\theta(t)}=-\eta\sqrt{A_{\theta}A_{\theta}^{T}}\nabla C(A_{ \theta})-\eta\nabla C(A_{\theta})\sqrt{A_{\theta}^{T}A_{\theta}}\]

when \(\sigma^{2}w\ll\lambda_{min}(A_{\theta})\). But it also reveals the whole spectrum of mixed dynamics in between, where some singular values of \(A_{\theta}\) are below the \(\sigma^{2}w\) threshold and some are above it.

This suggests that the lazy/active transition is best understood at a more granular level, where at each time \(t\) every singular value of \(A_{\theta}\) can either be lazy or active/balanced. The mixed regime is the best of both worlds: on one hand, since \(\sqrt{A_{\theta}A_{\theta}^{T}+\sigma^{4}w^{2}I}\) is always positive definite, the network can never get stuck at a saddle/local minimum as can happen in the balanced regime, on the other hand there is a momentum effect where the dynamics along large singular values is much faster than along the small ones, leading to an incremental learning behavior and a low-rank bias, which is absent in lazy learning. By choosing the threshold \(\sigma^{2}w\) adequately, one can best take advantage of these two phenomenon.

Finally, we focus on the task of recovering a low-rank \(d\times d\) matrix \(A^{*}\) from noisy observations \(A^{*}+E\), training on the MSE error \(\frac{1}{d^{2}}\left\|A_{\theta}-(A^{*}+E)\right\|_{F}^{2}\) in the limit as the dimension \(d\), width \(w\) and variance \(\sigma^{2}\) scale together with scaling laws \(w=d^{\gamma_{w}}\) and \(\sigma^{2}=d^{\gamma_{\sigma^{2}}}\). We describe the training dynamics for almost all reasonable scalings \(\gamma_{w},\gamma_{\sigma^{2}}\) leading to a phase diagram with two main regimes:

* **Lazy (\(1<\gamma_{\sigma^{2}}+\gamma_{w}\))** where all singular values remain below the threshold \(\sigma^{2}w\) throughout training, and where the network fails to recover \(A^{*}\) due to the absence of low-rank bias.
* **Active (\(1>\gamma_{\sigma^{2}}+\gamma_{w}\))** where \(K=\mathrm{Rank}A^{*}\) singular values pass the threshold and fit \(A^{*}\) before the other singular values have time to fit the noise \(E\), leading to the recovery of \(A^{*}\).

There are two other degenerate regimes that we avoid: the underparametrized regime when \(w<d\) (or \(\gamma_{w}\ll 1\)) where the rank is constrained by the network architecture rather than the training dynamics, and the noisy regime \(2\gamma_{\sigma^{2}}+\gamma_{w}+1>0\) where the variance of the entries of \(A_{\theta(0)}\) at initialization is infinite.

### Previous Works

Linear networks have been used as a testing ground, a stepping stone on the way to understand nonlinear networks. Linear networks and their training dynamics are in many ways much simpler than nonlinear ones, but in spite of a long research history, our understanding remains limited.

The setting that is best understood is that of diagonal linear networks where the dynamics decouple along the diagonal entries leading to an incremental learning behavior and a sparsity bias [44; 3; 45; 23; 41], some of this analysis has been extended to include effects of initialization scale [48] and SGD [42]. While the same decoupling happens in general linear with diagonal initializations and diagonal task, it remains an extremely strong assumption.

Some work has been done to prove similar incremental learning dynamics outside the diagonal case [35; 28; 31] where the incremental aspect can be understood as the parameters going from saddle to saddle. For shallow linear networks, the training dynamics with MSE can be explicitly solved [21] but remain very complex so that one needs to assume some form of alignment to guarantee convergence [14]. For deeper networks there exists explicit formulas in the mean-field limit where the number of neurons grows to infinity [18], these results can of course be applied to the special case of shallow nets, our paper goes further by giving self-consistent dynamics for the full matrix, revealing the lazy/active transition, and also extends the analysis to finite widths.

A very powerful tool in the analysis of a linear network is its training invariants, and the balancedness condition which greatly simplifies the dynamics [6; 7]. Balanced networks exhibit a momentum effect, where the training dynamics along a singular value \(s_{i}\) have'speed' proportional to \(s_{i}\) itself (or \(s_{i}\) to some power), while this momentum effect seems to be key to understand the low-rank bias of linear networks [8], it also means that one needs to guarantee that the dynamics never approach zero, which is one the main hurdle towards proving convergence in balanced networks. To solve this issue, recent work has focused on initialization that slightly imbalanced [49; 37; 46; 38; 50]. This suggests that it is key to find the right balance between balancedness and imbalancedness to obtain both fast convergence and low-rank bias.

In a concurrent work [32] a similar transition between lazy and active regimes is observed, and the same mixed dynamics are derived for a specific initialization. In contrast, we prove that these dynamics are approximately true with high probability for random Gaussian initializations, which is the standard initialization scheme for neural networks.

Figure 1: For both plots, we train either using gradient descent or the self-consistent dynamics from equation (1), with the scaling \(\gamma_{\sigma^{2}}=-1.85\), \(\gamma_{w}=2.25\) which lies in the active regime. (Left panel): We plot train and test error for both dynamics. We observe that the train/test error for gradient descent is very close to the train/test error for the self-consistent dynamics. (Right panel): We plot with a solid line the singular values of \(A_{\theta(t)}\) when running the self-consistent dynamics, and use a dashed line for the singular values from running gradient descent. In this experiment, \(\text{Rank}A^{\star}=5\). We use different colors for the \(5\) largest singular values and the same color for the remaining singular values. We can see how the \(5\) largest singular values ‘speed up’ as they cross the \(\sigma^{2}w\) threshold, allowing them to converge earlier than the rest. The minimal test error is achieved in the short period where the large singular values have converged but not the rest.

### Setup

We will study shallow linear networks (or matrix factorization) where a \(d_{out}\times d_{in}\) matrix \(A_{\theta}\) is represented as the product of two matrices \(A_{\theta}=W_{2}W_{1}\), where the weight matrices \(W_{1}\) and \(W_{2}\) are respectively \(w\times d_{in}\) and \(d_{out}\times w\) dimensional, for some width \(w\). The parameters \(\theta\) of the network are the concatenation of the entries of both submatrices \(\theta=(W_{1},W_{2})\).

The parameters \(\theta\) are learned in the following manner: they are initialized as i.i.d. Gaussian \(\mathcal{N}(0,\sigma^{2})\), and then optimized with gradient descent to minimize a loss \(\mathcal{L}(\theta)=C(A_{\theta})\). Though most of our analysis works for general convex costs \(C:\mathbb{R}^{d_{out}\times d_{in}}\rightarrow\mathbb{R}\) on matrices, we will in the second part focus on the task of recovering a low-rank matrix \(A^{*}\) from noisy observations \(A^{*}+E\), by training a linear network \(A_{\theta}\) on the MSE loss

\[\mathcal{L}(\theta)=\frac{1}{d^{2}}\left\|A_{\theta}-(A^{*}+E)\right\|_{F}^{2 }.\]

The width \(w\) allows us to control the over parametrization, indeed the set of matrices that can be represented by a network of width \(w\) is the set \(\mathcal{M}_{\leq w}\) of matrices of rank \(w\) or less. The overparametrized regime is when \(w\geq\text{min}\{d_{in},d_{out}\}\) because all matrices can be represented in this case.

### Lazy Dynamics

The evolution of the weight matrices during gradient descent with learning rate \(\eta\) is given by

\[W_{1}(t+1) =W_{1}(t)-\eta W_{2}^{T}(t)\nabla C(A_{\theta(t)})\] \[W_{2}(t+1) =W_{2}(t)-\eta\nabla C(A_{\theta(t)})W_{1}^{T}(t)\]

where we view the gradient \(\nabla C(A_{\theta(t)})\) of the cost \(C\) as a \(d_{out}\times d_{in}\) matrix, which for the MSE cost equals \(\nabla C(A_{\theta(t)})=2d^{-2}(A_{\theta(t)}-(A^{*}+E))\).

But we care more about the evolution of the complete matrix \(A_{\theta(t)}=W_{2}(t)W_{1}(t)\) induced by the evolution of \(W_{1}(t),W_{2}(t)\), which can be approximated by

\[A_{\theta(t+1)}=A_{\theta(t)}-\eta W_{2}(t)W_{2}^{T}(t)\nabla C(A_{\theta(t)} )-\eta\nabla C(A_{\theta(t)})W_{1}^{T}(t)W_{1}(t)+O(\eta^{2}).\] (2)

Thus we see that if we can describe the matrices \(C_{1}=W_{1}^{T}W_{1}\) and \(C_{2}=W_{2}W_{2}^{T}\) throughout training, then we can describe the evolution of \(A_{\theta(t)}\).

When \(w\) is very large, we end up in the lazy regime where the parameters move enough up to a time \(t\) to change \(A_{\theta(t)}\), but not enough to change \(C_{1},{C_{2}}\)1, allowing us to make the approximation \(C_{i}(t)\approx C_{i}(0)\). Furthermore at initialization these matrices concentrate as \(w\rightarrow\infty\) around their expectations \(\mathbb{E}\left[C_{1}\right]=\sigma^{2}wI_{d_{in}}\), \(\mathbb{E}\left[C_{2}\right]=\sigma^{2}wI_{d_{out}}\). The GD dynamics can then be approximated by the much simpler dynamics:

Footnote 1: To be more precise the direction in parameter space that change \(C_{1},C_{2}\) are approximately orthogonal to those that change \(A_{\theta}\), and GD/GF only moves along the later direction.

\[A_{\theta(t+1)}=A_{\theta(t)}-2\eta\sigma^{2}w\nabla C(A_{\theta(t)}),\]

which are equivalent to doing GD on the cost \(C\) directly with a learning rate of \(2\eta\sigma^{2}w\).

One can then easily prove exponential convergence for any convex cost \(C\) following the convergence analysis of traditional linear models. But we can see the absence of feature learning from the fact that the covariance \(C_{1}\) of the 'feature map' \(W_{1}\) is (approximately) constant. More problematic in the context of low-rank matrix recovery is the absence of low-rank bias, indeed one can easily solve the dynamics to obtain

\[A_{\theta(t)}=(A^{*}+E)+(1-4d^{-2}\eta\sigma^{2}w)^{t}(A_{\theta(0)}-(A^{*}+E)),\]

and since \(\mathbb{E}A_{\theta(0)}=0\) we obtain

\[\mathbb{E}\left[A_{\theta(t)}\right]=\left(1-(1-4d^{-2}\eta\sigma^{2}w)^{t} \right)(A^{*}+E).\]The expected test error \(\mathbb{E}\left\|A_{\theta(t)}-A^{*}\right\|^{2}\) is therefore lower bounded by

\[\left\|\mathbb{E}A_{\theta(t)}-A^{*}\right\|^{2}=\left\|(1-4d^{-2}\eta\sigma^{2 }w)^{t}A^{*}+(1-(1-4d^{-2}\eta\sigma^{2}w)^{t})E\right\|^{2}\]

which never approaches zero.

In linear networks, there is no advantage to being in the lazy regime, as we simply recover a simple linear model at an additional cost of more parameters and thus more compute. But we will see that a short period of lazy regime at the beginning of training plays a crucial role in making sure that the subsequent active regime starts from an 'aligned' state.

### Balanced Dynamics

There has been much more focus on so-called balanced linear networks, which are networks that satisfy the balanced condition \(W_{1}W_{1}^{T}=W_{2}^{T}W_{2}\). If the network is balanced at initialization, it remains so throughout training, because, the difference \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\) is an invariant of GF (and an approximate invariant of GD with small enough learning rate).

First observe that the balanced condition implies the following shared eigendecomposition \(W_{1}W_{1}^{T}=W_{2}^{T}W_{2}=USU^{T}\). This implies the following shared SVD decompositions \(W_{1}=U\sqrt{S}U_{in}^{T}\), \(W_{2}=U_{out}\sqrt{S}U^{T}\) and \(A_{\theta}=U_{out}SU_{in}^{T}\). Furthermore, we have \(C_{1}=U_{in}SU_{in}^{T}=\sqrt{A_{\theta}^{T}A_{\theta}}\) and \(C_{2}=U_{out}SU_{out}^{T}=\sqrt{A_{\theta}A_{\theta}^{T}}\), which leads to self-consistent dynamics for \(A_{\theta(t)}\):

\[A_{\theta(t+1)}=A_{\theta(t)}-\eta\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}}\nabla C (A_{\theta(t)})-\eta\nabla C(A_{\theta(t)})\sqrt{A_{\theta(t)}^{T}A_{\theta(t) }}+O(\eta^{2}).\]

Now these dynamics are quite complex in general, and it remains difficult to prove convergence. Indeed one can easily find initializations \(A_{\theta(0)}\) that will not converge, for example if \(A_{\theta(0)}=0\) then GD will remain stuck there. A lot of work has been dedicated to finding conditions that guarantee the convergence of the above dynamics [6, 14], but these assumptions are often quite strong.

The simplest initialization that guarantees convergence (and the one that will be most relevant to our analysis) is the positively aligned initialization. If at initialization \(A_{\theta(0)}\) and \(A^{*}+E\) are 'aligned', i.e. shares the same singular vectors \(A_{\theta(0)}=U_{out}SU_{in}^{T}\) and \(A^{*}+E=U_{out}S^{*}U_{in}^{T}\), then they will remain aligned throughout training \(A_{\theta(t)}=U_{out}S(t)U_{in}^{T}\) and the dynamics decouple along each singular value

\[s_{i}(t+1)=s_{i}(t)+2\eta\left|s_{i}(t)\right|(s_{i}^{*}-s_{i}(t))+O(\eta^{2}).\]

Since we always have \(s_{i}^{*}\geq 0\), then for small enough learning rates \(\eta\), we see that if \(s_{i}(0)\in(0,s_{i}^{*}]\) it will grow monotonically and converge to \(s_{i}^{*}\); if \(s_{i}(0)>s_{i}^{*}\) it will decrease monotonically to \(s_{i}^{*}\), and if \(s_{i}(0)\leq 0\) it will increase and converge to \(0\). Thus one can guarantee convergence if we further assume positive alignment \(s_{i}(0)>0\).

The advantage is that there is a momentum effect in the form of the prefactor \(\left|s_{i}(t)\right|\), which implies that the dynamics along large singular values are faster than along small ones. As a result, if all singular values are initialized with the same small value, then they will at first grow very slowly until they reach a critical size where the momentum effect will make them converge very fast. The singular values aligned with the top singular values of \(A^{*}+E\) will reach this threshold much faster, and they will therefore converge to approximately their final value \(s_{i}=s_{i}^{*}\) at a time when the other singular values are still basically zero. If we stop training at this time then the linear network will have essentially learned only the top \(K\) singular values of \(A^{*}+E\), which is a good approximation for \(A^{*}\), leading to a small test error (see [23] for details).

But this analysis relies on the very strong assumption of positive alignment at initialization. If we do not assume a positive alignment and assume that the \(s_{i}\) are random (i.i.d. w.r.t. a symmetric distribution), then each \(s_{i}\) has probability \(\nicefrac{{1}}{{2}}\) of starting with a negative alignment and getting stuck at zero, which means that with high probability training will fail to recover \(A^{*}\) and will recover only a random subset of the singular values of \(A^{*}\). The presence of these attractive saddles shows the complexity of the balanced dynamics.

A limitation of this approach is that it requires a quadratic cost and a very specific initialization, and in the case of positive alignment, an initialization that requires knowledge of the (SVD of the)true function \(A^{*}\). Nevertheless, the positively aligned and balanced dynamics seem to capture some qualitative phenomenon that has been observed empirically outside of this restricted setting. This is the phenomenon of incremental learning, where if the singular values are initialized as very small, they first grow very slowly, but the multiplicative momentum will lead to come up one by one in a very abrupt manner, and this leads to a low rank bias where the network first only fits the largest singular value, then two largest, and so on. More generally, this can be interpreted as the network performing a greedy low-rank algorithm [35].

Our analysis will confirm the fact that positive alignment happens naturally as a result of a short period of lazy training, allowing us to prove similar decoupling and incremental learning for a general random initialization.

_Remark_.: We can define the time dependent map \(\Theta(G;t)=C_{2}(t)G+GC_{1}(t)\), so that the GD dynamics can be rewritten as \(A_{\theta(t+1)}=A_{\theta(t)}-\eta\Theta(\nabla C(A_{\theta(t)}),t)+O(\eta^{2})\). The map \(\Theta\) is none other than the NTK for shallow linear networks, but it has also been called the preconditioning matrix in previous work [7]. The lazy regime is then characterized by the NTK \(\Theta\) being approximately equal to the time-independent NTK \(\Theta^{\text{lazy}}(G)=2\sigma^{2}wG\), whereas the balanced regime is characterized by the time-dependent \(\Theta^{\text{bal}}(G;t)=\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}}G+G\sqrt{A_{ \theta(t)}^{T}A_{\theta(t)}}\), with the distinction that the time dependence is only through \(A_{\theta(t)}\).

## 2 Mixed Lazy/Balanced Dynamics

Both lazy and balanced dynamics have the surprising but very useful property that the evolution of the network matrix \(A_{\theta}\) is approximately self-consistent: the evolution of \(A_{\theta}\) can be expressed in terms of itself. The lazy approximation becomes correct for a sufficiently large initialization, while the balanced one is correct for a balanced initialization. However, for most initializations, neither of these approximations are correct.

We fill this gap by providing a self-consistent evolution of \(A_{\theta}\) that applies for any initialization scale:

\[\partial_{t}A_{\theta(t+1)}\approx-\eta\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}+ \sigma^{4}w^{2}I\nabla C(A_{t})}-\eta\nabla C(A_{t})\sqrt{A_{\theta(t)}^{T}A_ {\theta(t)}+\sigma^{4}w^{2}I}.\]

This approximation is formalized in the following theorem, denoting \(\hat{C}_{1}(t)=\sqrt{A_{\theta(t)}^{T}A_{\theta(t)}+\sigma^{4}w^{2}I}\) and \(\hat{C}_{2}(t)=\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}+\sigma^{4}w^{2}I}\)

**Theorem 1**.: _For a linear net \(A_{\theta}=W_{2}W_{1}\) with width \(w\), initialized with i.i.d. \(\mathcal{N}(0,\sigma^{2})\) weights and trained with Gradient Flow, we have with high probability that for all time \(t\),_

\[\left\|C_{1}(t)-\hat{C}_{1}(t)\right\|_{op},\left\|C_{2}(t)-\hat{C}_{2}(t) \right\|_{op}\leq\text{min}\left\{O(\sigma^{2}w),O\left(\sqrt{\frac{d}{w}} \left\|C_{1}(t)\right\|_{op}\right)\right\}.\]

Proof.: (sketch) The quantity \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\) is invariant under GF (and approximately so under GD) and it is approximately equal to \(\sigma^{2}w(P_{1}-P_{2})\) for two orthogonal projections \(P_{1},P_{2}\) (at initialization and for all subsequent times because of the invariance). We therefore have

\[W_{1}^{T}(W_{1}W_{1}^{T}-W_{2}^{T}W_{2})^{2}W_{1}\approx\sigma^{4}w^{2}W_{1}^ {T}(P_{1}+P_{2})W_{1}\approx\sigma^{4}w^{2}C_{1}.\]

Thus the pairs \(C_{1},C_{2}\) approximately satisfy the following equations:

\[0 \approx C_{1}^{3}-A_{\theta}^{T}A_{\theta}C_{1}-C_{1}A_{\theta}^ {T}A_{\theta}-\sigma^{4}w^{2}C_{1}+A_{\theta}^{T}C_{2}A_{\theta}\] \[0 \approx C_{2}^{3}-A_{\theta}A_{\theta}^{T}C_{2}-C_{2}A_{\theta}A_ {\theta}^{T}-\sigma^{4}w^{2}C_{2}+A_{\theta}C_{1}A_{\theta}^{T}.\]

The pair \(\hat{C}_{1},\hat{C}_{2}\) is a solution of the above, and one can show that \(C_{1},C_{2}\) must approach them and not any of the other solutions. 

The takeaway from theorem 1 is the following.

1. In the lazy regime where \(\|C_{1}(t)\|_{op}+\|C_{2}(t)\|_{op}\leq O(\sigma^{2}w)\), then \(\|C_{1}(t)-\hat{C}_{1}(t)\|_{op}\leq\sqrt{d/w}\|C_{1}(t)\|_{op}<<\|C_{1}(t)\|_{op}\).

2. In the active regime where \(\|C_{1}(t)\|_{op}/\sigma^{2}w>d^{\varepsilon}>>1\), then \(\|C_{1}(t)-\hat{C}_{1}(t)\|_{op}\leq O(\sigma^{2}w)<<d^{-\varepsilon}\|C_{1}(t) \|_{op}\).

It is true that the error does not vanish. However, for our purpose it suffices to show that \(\|\hat{C}_{1}-C_{1}\|_{op}\) is infinitely smaller than \(C_{1}\) for all times, regardless of the magnitude of \(\|C_{1}(t)\|_{op}\).

We see how both the lazy and balanced dynamics appear as special cases depending on how large the variance at initialization \(\sigma^{2}\) is in comparison to the singular values of the matrix \(\hat{A}_{\theta(t)}\):

* **Lazy:** When \(\sigma^{2}w\gg s_{max}(A_{\theta(t)})\), then \(\hat{C}_{1}\approx\sigma^{2}wI_{d_{out}}\) and \(\hat{C}_{2}\approx\sigma^{2}wI_{d_{in}}\), recovering the lazy dynamics.
* **Balanced:** When \(\sigma^{2}w\ll s_{min}(A_{\theta(t)})\), then \(\hat{C}_{1}\approx\sqrt{A_{\theta(t)}^{T}A_{\theta(t)}}\) and \(\hat{C}_{2}\approx\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}}\), recovering the balanced dynamics.

But clearly there can be times when neither conditions are satisfied, when some singular values of \(A_{\theta(t)}\) are larger than the threshold \(\sigma^{2}w\) while others are smaller, in such cases we are in a mixed regime, where the network is lazy along the small singular values of \(A_{\theta(t)}\) (\(s_{i}\ll\sigma^{2}w\)) and active/balanced along the large ones (\(s_{i}\gg\sigma^{2}w\)).

At initialization, the singular values are of size \(\sigma^{2}\sqrt{wd}\). This implies that with overparametrization (\(w\gg d\)), all singular values start in the lazy regime and follow the simple lazy dynamics, which may (or may not) lead to some singular growing and crossing the \(\sigma^{2}w\) threshold, at which point they will switch to balanced dynamics (after a short transition period when the singular value is around the threshold \(s_{i}\approx\sigma^{2}w\)). Once a singular value is far past the threshold \(s_{i}\gg\sigma^{2}w\), training along this singular value will be much faster than along the lazy singular values (this speed up can be seen in Figure 1). This allows the newly active singular values to converge while the lazy singular values remain almost constant. Once the active singular values have converged, the slow training of the remaining lazy singular values continues until some of these singular values reaches the threshold, or until GD converges.

This type of behavior is illustrated by the following formula, which describes the derivative in time of the \(i\)-th singular value \(s_{i,t}\) of \(A_{t}\), with singular vectors \(u_{i,t},v_{i,t}\):

\[s_{i,t+1}-s_{i,t}\approx\eta_{t}u_{i,t}^{T}\partial_{t}A_{\theta(t)}v_{i,t} \approx-2\eta_{t}\sqrt{s_{i,t}^{2}+\sigma^{4}w^{2}}u_{i,t}^{T}\nabla C(A_{ \theta(t)})v_{i,t},\]

where the prefactor \(2\eta_{t}\sqrt{s_{i,t}^{2}+\sigma^{4}w^{2}}\) describes the effective learning rate along the \(i\)-th singular value, which depends on the \(i\)-th singular value \(s_{i,t}\) itself.

This suggests that it is more natural to distinguish between the lazy and active regime at a much more granular level: at every time \(t\) a singular value can be either active or lazy (or very close to the transition but this typically only happens for a very short time). In contrast, the traditional definition of the lazy regime was defined for a whole network and over the whole training time. To avoid confusion, we call this the pure lazy regime, where all singular values remain lazy throughout training. This begs the question of whether a pure balanced regime also exists, but all singular values will always be lazy for at least a short time period (assuming \(w>d\)), and as we will see this short lazy period plays a crucial role in aligning the network so that the subsequent balanced regime can learn successfully. A pure balanced regime can only be obtained in the underparametrized regime, or by taking a balanced initialization instead of the traditional i.i.d. random initialization.

While this challenges the traditional lazy/active dichotomy, it also reinforces it, as it shows that there is no fundamentally different third regime, only lazy, active, and some mix of the two. Theorem 1 thus allows us to revisit previous descriptions of lazy and balanced dynamics and 'glue them together' to extend them to the general case. This simple strategy will allow to almost fully 'fill in the phase diagram', i.e. describe the dynamics, convergence and generalization properties of DLNs for almost all reasonable initialization scales \(\sigma^{2}\) and widths \(w\).

_Remark_.: The transition of a singular value \(s_{i}\) from lazy to active can be understood as a form of alignment happening in the hidden layer: the two vectors \(W_{1}v_{i}\) and \(W_{2}^{T}u_{i}\) for \(u_{i},v_{i}\) the left and right singular vectors of \(s_{i}\) are orthogonal in the lazy regime and become perpendicular in the balanced regime. Indeed the normalized scalar product of these two vectors satisfies

\[\frac{u_{i}^{T}W_{2}W_{1}v_{i}}{\left\|W_{2}^{T}u_{i}\right\|\left\|W_{1}v_{i} \right\|}=\frac{s_{i}}{\sqrt{u_{i}^{T}C_{2}u_{i}}\sqrt{u_{i}^{T}C_{1}u_{i}}} \approx\frac{s_{i}}{\sqrt{s_{i}^{2}+\sigma^{4}w^{2}}}\]

which is close to zero for lazy singular values \(s_{i}\ll\sigma^{2}w\) and close to one for active ones \(s_{i}\gg\sigma^{2}w\).

### Phase Diagram for MSE

To illustrate the power of Theorem 1 we provide a phase diagram of the behavior of large shallow networks on a MSE task, for almost all (reasonable) choices of width \(w\) and variance \(\sigma^{2}\) scalings.

We want to recover a rank \(K\) and \(d\times d\)-dimensional matrix \(A^{*}\) with \(s_{i}(A^{*})=da_{i}\) for some \(a_{1}\geq a_{2}\geq\cdots\geq a_{K}\) independent of the dimension \(d\). We however only observe a noisy version \(A^{*}+E\) for some \(E\) such that \(\left\|E\right\|_{op}\leq c_{0}d^{\delta}\). One could imagine \(E\) to have iid random Gaussian entries \(\mathcal{N}(0,1)\) in which case \(\left\|E\right\|_{op}\leq c_{0}\sqrt{d}\) with high probability.

Figure 2: As a function of \(\gamma_{\sigma^{2}},\gamma_{w},\) we run GD and plot different quantities. Our theoretical results only apply to the top left region for \(\gamma_{w}>1\) and below the red line, although these plots suggest that some results may extend to smaller \(\gamma_{w}\)s. (Top left panel): We plot the smallest test error \(\frac{1}{d^{2}}\|A_{\theta(t)}-A^{*}\|_{F}^{2}\) in the whole run. The active region (below the black line) has a small error while the lazy region does not. (Top right panel): We plot the stable rank of \(A_{\theta(t)}\) (defined as \(\|A_{\theta(t)}\|_{F}^{2}/\|A_{\theta(t)}\|_{\text{op}}^{2}\)) at the time of minimal test error. In this experiment, we took \(\text{Rank}A^{*}=5\). We see that the active region has approximately the correct rank while the lazy region overestimates it. (Bottom left panel): We plot the number of iterations until minimal test error, illustrating the trade-off between test error and training time. (Bottom right panel): We compute \(\ln\left(\frac{1}{d^{2}}\|A_{\theta(t)}-\hat{A}_{\theta(t)}\|_{F}^{2}\right)\) where \(A_{\theta(t)}\) comes from GD and \(\hat{A}_{\theta(t)}\) from the self-consistent dynamics. We observe that this distance is not only small for the region where our theoretical results apply but also almost everywhere outside this region.

As the dimension \(d\) grows, the size of the network needs to scale too, as well as the initialization variance, but it is unclear what is the optimal way to choose \(w\) and \(\sigma^{2}\). We will therefore consider general scalings \(w=d^{\gamma_{w}}\) and \(\sigma^{2}=d^{\gamma_{\sigma^{2}}}\). We will now describe the \((\gamma_{w},\gamma_{\sigma^{2}})\)-phase diagram which features 4 regimes: underparametrized, infinite-noise, lazy and mixed/active.

We can identify a region of'reasonable' pairs \((\gamma_{\sigma^{2}},\gamma_{w})\) by ruling out degenerate behavior. First, the width \(w\) needs to be larger than the dimension \(d\), since a network of width \(w\) can only represent matrices of rank \(w\) or less, this means that we need \(\gamma_{w}\geq 1\). Another constraint comes from the variance of \(A_{\theta}\) at initialization: the entries \(A_{\theta(0),ij}\) at initialization have variance \(\sigma^{4}w\). We want this variance to go to zero as \(d\) grows which implies that we need \(2\gamma_{\sigma^{2}}+\gamma_{w}<0\).

Now within this reasonable region we observe two regimes, the pure lazy regime for \(1<\gamma_{\sigma^{2}}+\gamma_{w}\) where the network simply fits \(A^{*}+E\) thus failing to learn \(A^{*}\) and the mixed regime for \(1>\gamma_{\sigma^{2}}+\gamma_{w}\) where the dynamics are lazy for a short amount of time until \(K\) singular values grow large enough to switch to the balanced dynamics and fit the true matrix \(A^{*}\).

**Theorem 2**.: _For pairs \(\gamma_{w},\gamma_{\sigma^{2}}\) such that \(\gamma_{w}>1\) and \(2\gamma_{\sigma^{2}}+\gamma_{w}<0\), we have two regimes:_

* _Lazy (\(1<\gamma_{\sigma^{2}}+\gamma_{w}\)): with a learning rate_ \(\eta\ll\frac{d^{2}}{\sigma^{2}w}\) _we have that for all time_ \(t\)_,_ \(\frac{1}{d^{2}}\left\|A_{\theta(t)}-A^{*}\right\|_{F}^{2}\geq c\)_._
* _Active (_\(1>\gamma_{\sigma^{2}}+\gamma_{w}\)): with a learning rate_ \(\eta\ll\frac{d^{2}}{s_{1}(A^{*})}\sim d\)_, and at time_ \[t=\frac{1}{\eta}\left(\frac{\Delta}{a_{K}}+\frac{2\text{max}(1,2\Delta)}{c(a_ {1},\ldots,a_{K})}+\frac{\text{max}(1,2\Delta)}{2a_{K}}\right)d\log d+\eta^{ -1}O(d\log\log d),\] _for_ \(\Delta=1-\gamma_{\sigma^{2}}-\gamma_{w}>0\)_, we have that_ \[\frac{1}{d^{2}}\left\|A_{\theta(t)}-A^{*}\right\|_{F}^{2}\leq O( \sigma^{4}w+\frac{\sigma^{4}w^{2}\log^{2}d}{d^{2}}+d^{-\frac{1}{2}}+\frac{ \sigma^{2}w}{d}+\eta^{2}\frac{\log^{2}d}{d^{2}}),\] _for_ \(c(a_{1},\ldots,a_{K})=\frac{\text{min}_{k,j:a_{k}\neq\sigma_{j}}|a_{k}-a_{j}| a_{K}^{2}}{\text{max}_{k,j:a_{k}\neq a_{j}}|a_{k}^{2}-a_{j}^{2}|}\)_._

Note that all the terms inside the final \(O(\ldots)\) term vanish: \(\sigma^{4}w\to 0\) because \(\gamma_{\sigma^{2}}+\gamma_{w}<0\), \(\frac{\sigma^{4}w^{2}\log^{2}d}{d^{2}}+\frac{\sigma^{2}w}{d}\to 0\) since \(1>\gamma_{\sigma^{2}}+\gamma_{w}\), and \(\eta^{2}\frac{\log^{2}d}{d^{2}}\to 0\) since we assumed \(\eta\ll d\).

This shows that the lazy regime only appears for very large widths \(\gamma_{w}>2\) (or at least the lazy regime with finite variance at initialization). Indeed the choice \(\gamma_{w}=2,\gamma_{\sigma^{2}}=-1\) is at the boundary of the lazy regime with the smallest \(\gamma_{w}\). This could explain why it is rare to observe the lazy regime in practice.

Our theoretical results applies to the overparametrized regime \(w\gg d\), but actually we only want to fit \(A^{*}\) which has a much smaller rank \(r\), and so we might only need \(w\gg r\). Figure 2, top left panel, confirms this, since we see a good generalization even for small widths \(w<d\), and in particular when \(w\approx\mathrm{Rank}A^{*}\). But to leverage this underparametrized regime, one would need to know the rank of the true matrix \(A^{*}\) in advance, which is typically not the case in practice. Nevertheless, the interesting behavior we observe in the (mildly) underparametrized regime warrants further analysis, and the fact that our self-consistent dynamics remain a good approximation in this regime (Figure 2, bottom right panel), suggests that the analysis we present here could be extended to this regime too.

Finally, we observe a trade-off between generalization error and training time: on one hand the test error has terms that scale negatively with \(1-\gamma_{\sigma^{2}}-\gamma_{w}\), which is the distance to the lazy/active transition, on the other hand, the time it takes to reach the minimal loss point scales positively with the same term. This can be seen from Figure 2, bottom left panel, which plots the number of steps required to reach minimal test error, which increases as one goes further into the active regime.

_Remark_.: In general when trying to fit a matrix \(B\) (instead of the special case \(B=A^{*}+E\)), the transition between lazy and mixed regime is when \(\sigma^{2}w\approx\left\|B\right\|_{op}\). Thus the exact location of the transition is task-dependent, so that the same variance \(\sigma^{2}\) and width \(w\) can lead to NTK or mixed regimes depending on the task. For example, let us assume that \(A^{*}\) is full-rank instead of finite rank, then we expect \(\left\|A^{*}\right\|_{op}\sim\sqrt{d}\) instead of \(\left\|A^{*}\right\|_{op}\sim d\), thus the transition would be at \(\frac{1}{2}=\gamma_{\sigma^{2}}+\gamma_{w}\) instead of \(1=\gamma_{\sigma^{2}}+\gamma_{w}\). This suggests that linear networks are able to adapt themselves to the task:leveraging active dynamics when the true data is low-rank to get better generalization, or remaining in the lazy dynamics in the absence of low-rank structure, to take advantage of the faster convergence. Note also that in the absence of sparsity, the lazy regime can be attained with a smaller width (\(\gamma_{w}>1\) instead of \(\gamma_{w}>2\)), since the choice \(\gamma_{w}=1,\gamma_{\sigma^{2}}=-\frac{1}{2}\) is already on the boundary of the lazy regime.

## 3 Conclusion

We prove a surprisingly simple self-consistent dynamic for the evolution of the matrix represented by a shallow linear network under gradient descent. This description not only unifies the already known lazy and balanced dynamics, but reveals the existence of a spectrum of mixed dynamics where some of the singular values are lazy while others are balanced.

Thanks to this description we are able to give an almost complete phase diagram of training dynamics as a function of the scaling of the width and variance at initialization w.r.t. the dimension.

A natural question that comes out of these results is whether nonlinear network also feature similar mixed regimes, and whether they could be the key to understand the convergence of general DNNs.

## References

* [1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [2] Emmanuel Abbe, Enric Boix-Adsera, Matthew Stewart Brennan, Guy Bresler, and Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [3] Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks, 2017.
* [4] Bloemedal Alex, Laszlo Erdos, Antti Knowles, Horng-Tzer Yau, and Jun Yin. Isotropic local laws for sample covariance and generalized wigner matrices. 2014.
* [5] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. pages 242-252, 2019.
* [6] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In _International Conference on Learning Representations_, 2019.
* [7] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 244-253. PMLR, 10-15 Jul 2018.
* [8] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [9] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019.
* [10] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [11] Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.

* [12] Rajendra Bhatia. _Matrix analysis_, volume 169. Springer Science & Business Media, 2013.
* [13] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. _Advances in Neural Information Processing Systems_, 35:32240-32256, 2022.
* [14] Lukas Braun, Clementine Domine, James Fitzgerald, and Andrew Saxe. Exact learning dynamics of deep linear networks with prior knowledge. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 6615-6629. Curran Associates, Inc., 2022.
* [15] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming. _arXiv preprint arXiv:1812.07956_, 2018.
* [16] Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport. In _Advances in Neural Information Processing Systems 31_, pages 3040-3050. Curran Associates, Inc., 2018.
* [17] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 1305-1338. PMLR, 09-12 Jul 2020.
* [18] Lenaic Chizat, Maria Colombo, Xavier Fernandez-Real, and Alessio Figalli. Infinite-width limit of deep linear neural networks. _Communications on Pure and Applied Mathematics_, 2022.
* [19] Zhen Dai, Mina Karzand, and Nathan Srebro. Representation costs of linear neural networks: Analysis and design. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [20] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2019.
* [21] Kenji Fukumizu. Effect of batch learning in multilayer neural networks. In _International Conference on Neural Information Processing_, 1998.
* [22] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(11):113301, 2020.
* [23] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [24] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 23607-23629. Curran Associates, Inc., 2023.
* [25] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. _ICLR_, 2023.
* [26] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In _The Eleventh International Conference on Learning Representations_, 2023.
* [27] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. In _Advances in Neural Information Processing Systems 31_, pages 8580-8589. Curran Associates, Inc., 2018.
* [28] Arthur Jacot, Francois Ged, Berlin Simsek, Clement Hongler, and Franck Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.

* [29] Arthur Jacot, Eugene Golikov, Clement Hongler, and Franck Gabriel. Feature learning in \(l_{2}\)-regularized dnns: Attraction/repulsion and sparsity. In _Advances in Neural Information Processing Systems_, volume 36, 2022.
* [30] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. _CoRR_, abs/1810.02032, 2018.
* [31] Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon Shaolei Du, and Jason D. Lee. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 15200-15238. PMLR, 23-29 Jul 2023.
* [32] Daniel Kunin, Allan Raventos, Clementine Domine, Feng Chen, David Klindt, Andrew Saxe, and Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning, 2024.
* [33] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* [34] Aitor Lewkowycz and Guy Gur-Ari. On the training dynamics of deep networks with \(l\_2\) regularization. _Advances in Neural Information Processing Systems_, 33:4790-4799, 2020.
* [35] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In _International Conference on Learning Representations_, 2020.
* [36] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. _arXiv preprint arXiv:2003.00307_, 2020.
* [37] Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In _International Conference on Machine Learning_, pages 7760-7768. PMLR, 2021.
* [38] Hancheng Min, Rene Vidal, and Enrique Mallada. On the convergence of gradient flow on multi-layer linear models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 24850-24887. PMLR, 23-29 Jul 2023.
* [39] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width relu nets: The multivariate case. In _International Conference on Learning Representations_, 2020.
* [40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. 2019.
* [41] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 7475-7505. Curran Associates, Inc., 2023.
* [42] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 29218-29230. Curran Associates, Inc., 2021.

* [43] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks. In _Advances in Neural Information Processing Systems 31_, pages 7146-7155. Curran Associates, Inc., 2018.
* [44] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014.
* [45] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019.
* [46] Salma Tarmoun, Guilherme Franca, Benjamin D Haeffele, and Rene Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10153-10161. PMLR, 18-24 Jul 2021.
* [47] Zihan Wang and Arthur Jacot. Implicit bias of SGD in \(l_{2}\)-regularized linear DNNs: One-way jumps from high to low rank. In _The Twelfth International Conference on Learning Representations_, 2024.
* [48] Blake Woodworth, Suriya Gunasekar, Pedro Savarese, Edward Moroshko, Itay Golan, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models, 2020.
* [49] Nuoya Xiong, Lijun Ding, and Simon Shaolei Du. How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization. In _OPT 2023: Optimization for Machine Learning_, 2023.
* [50] Ziqing Xu, Hancheng Min, Salma Tarmoun, Enrique Mallada, and Rene Vidal. Linear convergence of gradient descent for finite width over-parametrized linear networks with general initialization. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 2262-2284. PMLR, 25-27 Apr 2023.
* [51] Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. _arXiv preprint arXiv:2006.14548_, 2020.
* [52] Greg Yang and Edward J. Hu. Feature learning in infinite-width neural networks, 2020.
* [53] Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis-kahan theorem for statisticians. _Biometrika_, 102(2):315-323, 2015.
* [54] Difan Zou, Philip M Long, and Quanquan Gu. On the global convergence of training deep linear resnets. _arXiv preprint arXiv:2003.01094_, 2020.

The appendix is structured as follows.

* In section A, we introduce the notation, and establish several results about how perturbing a matrix would impact its singular vectors.
* In section C, we study the gradient flow dynamics of \(A_{t}\) in the active regime and prove that \(A_{t}\) will be approximately aligned with \(A^{*}\) throughout the Saddle-to-Saddle regime.
* In section B, we prove theorem 1 for general cost.
* In section D, we study the gradient flow dynamics for \(A_{\theta(t)}\) in the lazy regime.
* In section E, we show that \(A_{\theta(t)}\) is also approximately aligned with \(A^{*}\) throughout the Saddle-to-Saddle regime, using results in section C and section B. In subsection E.3, we summarize the approximate dynamics of \(A_{\theta(t)}\) throughout training. In section E.4, we bound the final test error.
* In section F, we bound the error from gradient descent and prove theorem 2.
* In section G, we describe the experimental setup.

## Appendix A Preliminaries

### Convention and Notation

**Constants**. \(d\) is the dimension of the input and the output layer, \(K\) is the rank of matrix \(A^{*}\), and \(w\) is the dimension of hidden layer. \(c\) and \(C\) will usually denote constants that are independent of \(d\), and depending on the context, the value of \(c\) and \(C\) might be different. If \(x\) is a scalar that depends on \(d\) and \(y\) is a scalar, then \(x=O(y)\) means there exists a constant \(c\) independent of \(d\), such that for \(d\) sufficiently large we have \(x\leq cy\). If \(A\) is a matrix, then \(A=O(y)\) means there exists a constant \(c\) independent of \(d\) such that for \(d\) sufficiently large, \(\|A\|_{op}\leq cy\). \(O(y)\) can be either a matrix or a scalar, and its meaning will be always clear from context.

**Matrix**. We use \(\|\cdot\|_{op},\|\cdot\|_{F},\|\cdot\|\) to denote the operator norm of a matrix, the Frobenius norm of a matrix, and the \(L^{2}\) norm for vectors. For every matrix \(A\), we define \(\text{max}A\) as the \(L^{\infty}\) norm, \(\text{max}_{i,j}|A_{ij}|\). We use \(I\) to denote identity matrix, the dimension of which is determined by context. We shall assume that the signal singular values of \(A^{*}\) are \(s_{1}^{*},\ldots,s_{K}^{*}\). \(\forall i=1,2,\ldots,K\), and \(s_{i}^{*}=a_{i}d\) where \(a_{1}\geq a_{2}\geq\ldots\geq a_{K}\) are constants independent of \(d\). By selecting proper basis in the input and output space, we assume that \(A^{*}=S^{*}\), where \(S^{*}\) is the diagonal matrix consisting of singular values of \(A^{*}\), ordered from largest to smallest. The \(p,q\)-th element of a matrix \(A\) is denoted \(A_{pq}\). We reserve the notation \(A(i,j)\) for the \(i,j\)-th block matrix of \(A\), which we shall define below.

**Submatrix**. Assume that \(n_{0}=0\), and \(a_{1}=\ldots=a_{n_{1}}>a_{n_{1}+1}=\ldots a_{n_{2}}>\ldots=a_{n_{m}}=a_{K}\), and let \(n_{m+1}=d\). For a matrix \(U\), we define the \(k,j\)-th sub-block \(U(k,j)\) of a matrix \(U\) as \(U_{n_{k-1}+1:n_{k+1},n_{j-1}+1:n_{j}}\), with both sides included. Notice that \(U^{T}(k,j)=U(j,k)^{T}\). In this notation, we can write the singular value decomposition of a matrix \(A\) as

\[A(i,j)=\sum_{k:\text{signal}}U(i,k)S(k,k)V(j,k)+U(i,m+1)S(m+1,m+1)V^{T}(m+1,j).\]

We call an index \(k\) (of sub-block) "signal", if \(k\leq m\). Index \(m+1\) is called "noise". Let \(S(k,k)^{*}\) be block matrix \(A^{*}(n_{k}:n_{k+1},n_{k}:n_{k+1})\). Then \(A^{*}=\operatorname{diag}(S(1,1)^{*},\ldots,S(m,m)^{*})\) and \(S(k,k)^{*}=s_{n_{k}}I\) is the \(k\)-th sub-block of \(A^{*}\). Each matrix has only finitely many sub-blocks.

**Indexing Conventions**. Entries of matrices will usually be indexed by \(p,q,r\) and sub-blocks of matrices will usually be indexed by \(i,j,k,\ell\). Usually \(k\) ranges from 1 to \(m\), and \(j\) usually ranges from 1 to \(m+1\).

**Element-wise Product**.We use \(\odot\) to represent element-wise product of two matrix of the same shape.

**Important Assumptions**. Throughout the paper, we shall always assume that 1. \(\gamma_{w}>1\) (i.e., \(w>>d\)).
2. \(2\gamma_{\sigma^{2}}+\gamma_{w}<0\). (i.e., \(\sigma^{4}w<<1\)).

### Matrix Inequalities

In the proof of main theorems, we will work extensively with inequalities of matrix norms and inequalities that involves element-wise product. The element-wise product appears naturally in the derivative of singular vectors of a matrix.

**Lemma A.1**.: _Assume that \(A\), \(B\) and \(R\) are square matrices. Let \(R_{max}=\text{max}_{i,j}|R_{ij}|\). Then_

\[tr[A(R\odot B)]=tr[BR^{T}\odot A],\]

_and_

\[|tr[A(R\odot B)]|\leq R_{\text{max}}\sqrt{tr(A^{T}A)}\sqrt{tr(B^{T}B)}\]

_In particular, if \(\forall p,q,R_{pq}\geq R_{\text{min}}>0\), then_

\[tr[A^{T}(R\odot A)]\geq R_{\text{min}}tr(A^{T}A)\]

Proof.: All are simple computations.

\[tr[AR\odot B]= \sum_{p,q}A_{pq}R_{qp}B_{qp}\] \[tr[BR^{T}\odot A]= \sum_{p,q}B_{pq}R_{pq}A_{qp}\]

The two equations above prove the first claim.

\[|tr[A(R\odot B)]|\leq \sqrt{tr(A^{T}A)}\sqrt{tr((R\odot B)^{T}R\odot B)}\] \[\leq R_{\text{max}}\sqrt{tr(A^{T}A)}\sqrt{tr(B^{T}B)}\]

This completes the second claim.

\[tr[A^{T}(R\odot A)] =\sum_{i,j}A_{ji}R_{ji}A_{ji}\] \[\geq R_{\text{min}}tr[A^{T}A]\]

This completes the third claim. 

**Lemma A.2**.: \(\sigma_{min}(A)\|B\|_{F}\leq\|AB\|_{F}\leq\sigma_{max}(A)\|B\|_{F}\)_._

Proof.: This is lemma B.3 of [54]. 

### Perturbation of Singular Values and Singular Vectors

We will often use the following variant of the Davis-Kahan \(\sin\theta\) theorem.

**Theorem A.3** (DK-\(\sin\theta\) Theorem).: _. Let \(\Sigma,\hat{\Sigma}\in\mathbb{R}^{p\times p}\) be symmetric, with eigenvalues \(\lambda_{1}\geq\ldots\geq\lambda_{p}\) and \(\hat{\lambda}_{1}\geq\ldots\geq\hat{\lambda}_{p}\). Fix \(1\leq r\leq s\leq p\), let \(d=r-s+1\) and let \(V=(v_{r},\ldots,v_{s})\) and \(\hat{V}=(\hat{v}_{r},\ldots,\hat{v}_{s})\) have orthonormal columns satisfying \(\Sigma v_{j}=\lambda_{j}v_{j}\) and \(\Sigma\hat{v}_{j}=\hat{\lambda}_{j}v_{j}\). Let \(\sigma_{1},\ldots,\sigma_{d}\) be the singular values of \(\hat{V}^{T}V\). Let \(\Theta(V,\hat{V})\) be the diagonal matrix with \(\cos\Theta(V,\hat{V})_{jj}=\sigma_{j}\) and \(\sin\Theta(V,\hat{V})\) be defined entry-wise. Then_

\[\|\sin\Theta(V,\hat{V})\|_{F}\leq\frac{2\text{min}(\sqrt{d}||\hat{\Sigma}- \Sigma\|_{op},\|\hat{\Sigma}-\Sigma\|_{F})}{\text{min}(\lambda_{r-1}-\lambda_{ r},\lambda_{s}-\lambda_{s+1})}.\]

Proof.: This is theorem 2 in [53].

The implication of the theorem is that if two matrices are sufficiently close, then their singular vectors are also close to each other. In the case where \(r=s\) and \(\lambda_{r}\) is of multiplicity 1, the theorem reduces to saying that the sine value of the angle between \(v_{r}\) and \(\hat{v}_{r}\) is very small.

The term \(\|\sin\Theta(V,\hat{V})\|_{F}\) is complicated to take derivative. In this paper we will use the following characterization of alignment, which is easier to take derivatives.

**Lemma A.4**.: _Let \(\hat{\Sigma}\) be a \(d\times d\) diagonal matrix, and let \(s_{1},\ldots,s_{K},\ldots,s_{d}\) be its diagonal entries. Assume that \(s_{1}=\ldots=s_{n_{1}}>s_{n_{1}+1}=\ldots=s_{n_{2}}\geq\ldots s_{n_{m}}=s_{K}> s_{K+1}=\ldots=s_{d}\). Then \(\hat{\Sigma}\) has \(m+1\) blocks in total. Assume that \(\|X-\hat{\Sigma}\|_{op}=d^{\alpha}\). Let \(X=USV^{T}\), and define_

\[x=4K-\sum_{k:\text{signal}}\operatorname{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k )+2U(k,k)V(k,k)^{T}).\]

_Then_

\[x\leq K\frac{\|\hat{\Sigma}-X\|_{op}}{s_{K}}+\frac{ms_{1}}{s_{K}}\left(\frac{ 2\text{min}(\sqrt{K}(\|X\|_{op}+\|\hat{\Sigma}\|_{op}),\|X\|_{F}+\|\hat{\Sigma }\|_{F})}{\text{min}_{k}(s_{n_{k}}^{2}-s_{n_{k+1}}^{2})}\right)^{2}\|X-\hat{ \Sigma}\|_{op}^{2}\]

**Remark**.: _To better understand \(x\), consider the special case where each signal singular value is of multiplicity 1. Since matrix \(X\) is "close" to diagonal matrix \(\hat{\Sigma}\), matrix \(U\) and matrix \(V\) should also be close to identity along signal directions: \(|U_{kk}|\approx 1\), \(|V_{kk}|\approx 1\), \(U_{kk}V_{kk}\approx 1\) for \(1\leq k\leq K\). Quantity \(x\) captures how much \(|U_{kk}|,|V_{kk}|,U_{kk}V_{kk}\) deviate from 1._

Proof.: Let \(X=USV^{T}\). Then \(X^{T}X=VS^{2}V^{T}\) is a symmetric matrix. Let \(V(:,k)=(v_{n_{k-1}+1},\ldots,v_{n_{k}})\) be the singular vectors of \(X\) corresponding to \(s_{n_{k-1}+1},\ldots,s_{n_{k}}\). There is some freedom to choose \(\hat{V}\), but for simplicity we pick \(\hat{V}(:,k)=(e_{n_{k-1}+1},\ldots,e_{n_{k}})\) where \(e_{i}\) is the \(i\)-th coordinate vector. As a result, \(\hat{V}(:,k)^{T}V(:,k)=V(k,k)\). Apply DK-\(\sin\theta\) theorem to \(V\) and \(\hat{V}\), we see that

\[\|\sin\Theta(V(:,k),\hat{V}(:,k))\|_{F}\leq\frac{2\text{min}(\sqrt{n_{k}-n_{k -1}}\|X^{T}X-\hat{\Sigma}^{2}\|_{op},\|X^{T}X-\hat{\Sigma}^{2}\|_{F})}{\text{ min}(s_{n_{k-1}}^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k}+1}^{2})}\]

Let \(\sigma_{p}\) be the singular values of \(V(k,k)\), for \(1\leq p\leq n_{k}-n_{k-1}\). Here we are using the notation for sub-block of a big matrix. Then \(\sin\Theta(V(:,k),\hat{V}(:,k))\) is the diagonal matrix, whose diagonal entries are given by \(\sqrt{1-\sigma_{p}^{2}}\). So \(\|\sin\Theta(V,\hat{V})\|_{F}^{2}=\sum_{p}(1-\sigma_{p}^{2})=tr[I-V(k,k)^{T}V( k,k)]\). Now observe that

\[\|X^{T}X-\hat{\Sigma}^{2}\|_{op}\leq (\|X\|_{op}+\|\hat{\Sigma}\|_{op})\|X-\hat{\Sigma}\|_{op}\] \[\|X^{T}X-\hat{\Sigma}^{2}\|_{F}\leq (\|X\|_{F}+\|\hat{\Sigma}\|_{F})\|X-\hat{\Sigma}\|_{op}\]

We conclude that

\[tr[I-V(k,k)^{T}V(k,k)]\leq\left(\frac{2\text{min}(\sqrt{n_{k}-n_{k-1}}(\|X\|_{ op}+\|\hat{\Sigma}\|_{op}),\|X\|_{F}+\|\hat{\Sigma}\|_{F})}{\text{min}(s_{n_{k-1} }^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k}+1}^{2})}\|X-\hat{\Sigma}\|_{op} \right)^{2}.\]

Similar conclusion is true for \(U(k,k)\). Next we bound \(tr(I-U(k,k)V(k,k)^{T})\).

\[\|\hat{\Sigma}(k,k)-X(k,k)\|_{F}^{2}=\sum_{q_{1},q_{2}=n_{k-1}+1}^{n_{k}}(\hat {\Sigma}_{q_{1}q_{2}}-X_{q_{1}q_{2}})^{2}\leq(n_{k}-n_{k-1})\|\hat{\Sigma}-X\|_ {op}^{2}.\]\[\|\hat{\Sigma}(k,k)-X(k,k)\|_{F}^{2}= \|\hat{\Sigma}(k,k)-\sum_{j}U(k,j)S(j,j)V^{T}(j,k)\|_{F}^{2}\] \[= \|\hat{\Sigma}(k,k)-\sum_{j\neq k}U(k,j)S(j,j)V(k,j)^{T}-U(k,k)S(k, k)V(k,k)^{T}\|_{F}^{2}\] \[\geq \|\hat{\Sigma}(k,k)-U(k,k)\hat{\Sigma}(k,k)V(k,k)^{T}\|_{F}^{2}\] \[-\|\sum_{j\neq k}U(k,j)S(j,j)V(k,j)^{T}\|_{F}^{2}\] \[-\|U(k,k)(S(k,k)-\hat{\Sigma}(k,k))V(k,k)^{T}\|_{F}^{2}\] \[\geq s_{n_{k}}^{2}\|I-U(k,k)V(k,k)^{T}\|_{F}^{2}\] \[-\sum_{j\neq k}\|U(k,j)\|_{F}^{2}\|S(j,j)V(k,j)\|_{op}^{2}\] \[-\|X-\hat{\Sigma}\|_{op}^{2}\] \[\geq s_{n_{k}}^{2}\|I-U(k,k)V(k,k)^{T}\|_{F}^{2}\] \[-s_{1}^{2}tr(I-U(k,k)^{T}U(k,k))tr(I-V(k,k)^{T}V(k,k))\] \[-\|X-\hat{\Sigma}\|_{op}^{2}\]

For every \(n_{k}-n_{k-1}\times n_{k}-n_{k-1}\) matrix \(M\), we have \(tr(A)\leq\sum_{p}|\lambda_{p}|\leq\sqrt{n_{k}-n_{k-1}}\left(\sum|\lambda_{p}| ^{2}\right)^{\frac{1}{2}}=\sqrt{n_{k}-n_{k-1}}\|M\|_{F}\). Using this inequality, we conclude that

\[tr(I-U(k,k)V(k,k)^{T})\leq (n_{k}-n_{k-1})\frac{\|\hat{\Sigma}-X\|_{op}}{s_{n_{k}}}\] \[+\frac{s_{1}}{s_{n_{k}}}\left(\frac{2\text{min}(\sqrt{n_{k}-n_{k- 1}}(\|X\|_{op}+\|\hat{\Sigma}\|_{op}),\|X\|_{F}+\|\hat{\Sigma}\|_{F})}{\text{ min}(s_{n_{k-1}}^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k}+1}^{2})}\|X- \hat{\Sigma}\|_{op}\right)^{2}\]

Summing on \(k\), we conclude that

\[x\leq K\frac{\|\hat{\Sigma}-X\|_{op}}{s_{K}}+\frac{ms_{1}}{s_{K}}\left(\frac{ 2\text{min}(\sqrt{K}(\|X\|_{op}+\|\hat{\Sigma}\|_{op}),\|X\|_{F}+\|\hat{\Sigma }\|_{F})}{\text{min}_{k}(s_{n_{k}}^{2}-s_{n_{k+1}}^{2})}\right)^{2}\|X-\hat{ \Sigma}\|_{op}^{2}\]

In the case where \(d\rightarrow\infty\), \(m\) is a constant, \(\alpha<1\), \(s_{1}=O(s_{K})\), \(s_{K}>cs_{K+1}\), \(K\) is a constant, \(\|X-\hat{\Sigma}\|_{op}=o(\|\hat{\Sigma}\|_{op})\), and \(\text{min}_{k}(s_{n_{k}}^{2}-s_{n_{k+1}}^{2})\geq cs_{K}^{2}\) for some constant \(c\), we have \(x\leq O(\frac{\|X-\hat{\Sigma}\|_{op}}{\|\hat{\Sigma}\|_{op}})\).

**Lemma A.5**.: _Let \(X=USV^{T}\), and define_

\[x=4K-\sum_{k:\text{signal}}\operatorname{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k, k)+2U(k,k)V(k,k)^{T}).\]

_Let \(\Sigma\) be a \(d\times d\) diagonal matrix whose diagonal entries are given by \(b_{1},\ldots,b_{K},0,\ldots,0\),where \(b_{1}=\ldots b_{n_{1}}>b_{n_{1}+1}=\ldots=b_{n_{2}}\geq\ldots=b_{n_{m}}=b_{K}\). Then_

\[\|\Sigma-U^{T}\Sigma V\|_{op}\leq\|\Sigma\|_{op}((m+1)^{3}\sqrt{x}+2(m+1)^{3 }x)\]Proof.: First observe that \(\|\Sigma-U^{T}\Sigma V\|_{op}\leq\sum_{i,j=1}^{m+1}\|\Sigma(i,j)-U^{T}\Sigma V(i,j) \|_{op}\). If \(i\neq j\), then \(\Sigma(i,j)=0\),

\[\|U^{T}\Sigma V(i,j)\|_{op}= \|\sum_{\ell=1}^{m+1}U(\ell,i)^{T}\Sigma(\ell,\ell)V(\ell,j)\|_{op}\] \[\leq b_{1}\sum_{\ell=1}^{m+1}\|U(\ell,i)\|_{op}\|V(\ell,j)\|_{op}\] \[\leq b_{1}(m+1)\sqrt{x}.\]

If \(i=j\), then \(\Sigma(i,i)=b_{n_{i}}I\),

\[\|U^{T}\Sigma V(i,i)-b_{n_{i}}I\|_{op}\leq \|b_{n_{i}}U(i,i)^{T}V(i,i)-b_{n_{i}}I\|_{op}+b_{1}\sum_{\ell\neq i }\|U(\ell,i)\|_{op}\|V(\ell,i)\|_{op}\] \[\leq b_{1}(m+1)x+b_{1}(m+1)x.\]

Therefore

\[\|\Sigma-U^{T}\Sigma V\|_{op}\leq\|\Sigma\|_{op}((m+1)^{3}\sqrt{x}+2(m+1)^{3}x).\]

This bound is not optimal in \(m\), but throughout the paper, \(m\) is of constant order and it is fine to miss a constant factor when estimating error.

## Appendix B Proof of Theorem 1

### Weak bound

We prove the following weak bound.

**Proposition B.1**.: _For every \(\varepsilon>0\) and every \(t<T\), we have with high probability,_

\[\|W_{1}^{T}W_{1}-\sqrt{A^{T}A+\sigma^{4}w^{2}I}\|_{\mathrm{op}}\leq(1+ \varepsilon)\sigma^{2}w\] (3)

_Analogous results holds for \(W_{2}\)._

Our main tool is the following lemma.

**Lemma B.2**.: _For every cost \(C\) and every time \(t\), we have_

\[W_{2}^{T}W_{2}(t)-W_{1}W_{1}^{T}(t)=W_{2}^{T}(0)W_{2}(0)-W_{1}(0)W_{1}^{T}(0)\] (4)

Proof.: Let \(L=\|A^{*}-A\|^{2}\) be the loss function. Then

\[\partial_{t}W_{1} =2W_{2}^{T}\nabla C\] \[\partial_{t}W_{2} =2\nabla CW_{1}^{T}\]

We see that

\[\partial_{t}(W_{1}W_{1}^{T})=2W_{2}^{T}\nabla CW_{1}^{T}+W_{1}\nabla C^{T}W_{ 2}=\partial_{t}(W_{2}^{T}W_{2})\]

Next, we show that at initialization, \(W_{1}W_{1}^{T}\) and \(W_{2}^{T}W_{2}\) are approximately orthogonal projections, up to a factor. Stating precisely, we have the following lemma.

**Lemma B.3**.: _There exists two projections \(P_{1}\) and \(P_{2}:\mathbb{R}^{w}\rightarrow\mathbb{R}^{w}\) such that the following are true._

1. _The image of_ \(P_{1}\) _and_ \(P_{2}\) _are orthogonal to each other;_
2. _With high probability,_ \[\|W_{1}W_{1}^{T}(0)-\sigma^{2}wP_{1}\|_{\mathrm{op}} =O(\sigma^{2}\sqrt{wd}\log d)\] (5) \[\|W_{2}^{T}W_{2}(0)-\sigma^{2}wP_{2}\|_{\mathrm{op}} =O(\sigma^{2}\sqrt{wd}\log d)\] (6)Proof of Lemma B.3.: : At initialization, the rank of \(w\)is \(w\) with probability 1. Therefore \(W_{1}W_{1}^{T}\) has \(w-d\) eigenvalues that are 0, and the \(w\) non-zero eigenvalues equals the eigenvalues of \(d\times d\) matrix \(W_{1}^{T}W_{1}\).

\(W_{1}^{T}W_{1}(0)\) is a (scaled) Wishart ensemble, whose limiting distribution is given by the Marchenko-Pastur law. The Marchenko-Pastur law, as stated in [4], proves the following. Let \(X\) be an \(M\times N\) matrix with complex-valued independent entries \(X_{i\mu}\) such that

1. \(\mathbb{E}[X_{i\mu}]=0\);
2. \(\mathbb{E}[\left|X_{i\mu}\right|^{2}]=\frac{1}{\sqrt{MN}}\);
3. for every \(p\in\mathbb{N}\), there exists a constant \(C_{p}\) such that \[\mathbb{E}\left[\left|(NM)^{\frac{1}{d}}X_{i\mu}\right|^{p}\right]\leq C_{p}.\]

Here, \(M\) satisfies

\[0<C^{-1}\leq\frac{\log M}{\log N}\leq C<\infty\]

for some constant \(C\) independent of \(M\) and \(N\). Let \(\phi=\frac{M}{N}\), which may or may not depend on \(N\). Then the eigenvalues of \(N\times N\) matrix \(X^{*}X\) has the same asymptotics as

\[\rho_{\phi}(dx):=\frac{\sqrt{\phi}}{2\pi}\sqrt{\frac{[(x-\gamma_{-})(\gamma_{ +}-x)]_{+}}{x^{2}}}dx+(1-\phi)_{+}\delta(dx)\]

where

\[\gamma_{\pm}:=\sqrt{\phi}+\frac{1}{\sqrt{\phi}}\pm 2\] (7)

In our situation, \(W_{1}\) is a \(w\times d\) matrix with independent and identically distributed Gaussian entries whose variance is \(\sigma^{2}\). Let \(M=w\), \(N=d\) and therefore \(W_{1}^{T}W_{1}(0)\) has the same distribution as \(\sigma^{2}\sqrt{wd}X^{*}X\). Notice that for this choice of \(M\) and \(N\), the asymptotic distribution eigenvalues of \(X^{*}X\) is \(\rho_{\frac{w}{d}}(dx)\). Notice that \(\rho_{\frac{w}{d}}\) is supported on interval \(\left[\sqrt{\frac{w}{d}}+\sqrt{\frac{d}{w}}-2,\sqrt{\frac{w}{d}}+\sqrt{\frac{d }{w}}+2\right]\), from which we conclude that in the limit, all eigenvalues of \(W_{1}^{T}W_{1}(0)\) is approximately \(\sqrt{\frac{d}{w}}\).

By Theorem 2.10 of [4], we have eigenvalue rigidity results for \(X^{*}X\). Let \(\lambda_{k}^{\prime}\) be the \(k\)-th largest eigenvalue for \(X^{*}X\). \(\forall k\in\{1,2,\ldots,w\}\), we have

\[\left|\lambda_{k}^{\prime}-\gamma_{k}\right|<d^{-\frac{2}{3}+\varepsilon}\] (8)

with high probability. Here \(\gamma_{\alpha}\) is defined through

\[\int_{\gamma_{k}}^{\infty}\rho_{\phi}(dx)=\frac{k}{d}\] (9)

Let \(\lambda_{k}\) be the \(k\)-th largest eigenvalue of \(W_{1}^{T}W_{1}(0)\). By the relationship between \(W_{1}^{T}W_{1}(0)\) and \(X^{*}X\), we know \(\lambda_{k}\) has the same law as \(\sigma^{2}\sqrt{wd}\lambda_{k}^{\prime}\). Then for every \(k\),

\[\left|\lambda_{k}-\sigma^{2}w\right| \leq\sigma^{2}\sqrt{wd}\left(\left|\lambda_{k}^{\prime}-\gamma_{k }\right|+\left|\gamma_{k}-\sqrt{\frac{w}{d}}\right|\right)\] (10) \[\leq O(\sigma^{2}\sqrt{wd})\] (11)

with high probability. We conclude that the first \(w\) eigenvalues of \(W_{1}^{T}W_{1}(0)\) is at most \(O(\sigma^{2}\sqrt{wd})\)-away from 1 and all other eigenvalues are 0. Therefore there exists a projection \(P_{1}\) such that

\[\left\|W_{1}W_{1}^{T}(0)-\sigma^{2}wP_{1}\right\|_{\mathrm{op}}=O(\sigma^{2} \sqrt{wd})\] (12)

Similarly, there exists a projection \(\tilde{P}_{2}\) such that

\[\left\|W_{2}^{T}W_{2}(0)-\sigma^{2}w\tilde{P}_{2}\right\|_{\mathrm{op}}=O( \sigma^{2}\sqrt{wd})\] (13)Notice that \(P_{2}\) is not exactly orthogonal to \(P_{1}\), and it remains to find a projection \(P_{2}\) that is orthogonal to \(P_{1}\) and is close to \(W_{2}^{T}W_{2}(0)\). Assume that the column vectors of \(W_{1}(0)\) are \(u_{1},\ldots,u_{d}\in\mathbb{R}^{w}\) and column vectors of \(W_{2}^{T}(0)\) are \(v_{1},\ldots,v_{d}\in\mathbb{R}^{w}\). For \(k=1,2,\ldots,d\), we define vector \(v_{k}^{\prime}\) as

\[v_{k}^{\prime}=v_{k}-P_{1}v_{k}.\] (14)

We claim that \(P_{1}v_{k}\) is very small. By law of large numbers, \(\|v_{k}\|\leq\sigma\sqrt{w}\log d\) with high probability.

\[\|P_{1}v_{k}\|\leq \|\frac{1}{\sigma^{2}w}W_{1}W_{1}^{T}(0)v_{k}\|+O(\sqrt{\frac{d}{ w}}\|v_{1}\|)\] (15) \[= \frac{1}{\sigma^{2}w}\|\langle u_{1},v_{k}\rangle u_{1}+\ldots+ \langle u_{w},v_{k}\rangle u_{w}\|+O(\sigma\sqrt{d}\log d)\] (16)

Notice that \(\langle u_{j},v_{k}\rangle u_{j},\forall j\) is a family of independent and identically distributed random vectors. For each of these random vectors, all entries have zero mean. The variance of any one of the entries is given by

\[\mathbb{E}\left[\langle u_{i},v_{k}\rangle^{2}\langle u_{j},e_{\ell}\rangle^{ 2}\right]=O(\sigma^{6}w)\] (17)

We conclude that for each \(\ell\) we have, by CLT,

\[\frac{1}{\sigma^{3}w^{\frac{1}{2}}}\frac{\langle u_{1},v_{k}\rangle\langle u_ {1},e_{\ell}\rangle+\ldots+\langle u_{w},v_{k}\rangle\langle u_{w},e_{\ell} \rangle}{\sqrt{d}}\xrightarrow{(d)}N(0,1)\] (18)

In particular,

\[\mathbb{P}\{\max_{\ell}\sum_{i=1}^{w}|\langle u_{i},v_{k}\rangle \langle u_{k},e_{\ell}\rangle|>100\sigma^{3}\sqrt{d}w^{\frac{1}{2}}\log d\}\leq w\mathbb{P}\{\sum_{i=1}^{w}|\langle u_{i},v_{k} \rangle\langle u_{k},e_{1}\rangle|>100\sigma^{3}\sqrt{d}w^{\frac{1}{2}}\log d\}\] \[\leq 2w\mathbb{P}\{N(0,1)>100\log d\}\] \[\leq O(d^{-50})\]

Therefore with high probability, \(\max_{\ell}\sum_{i=1}^{w}|\langle u_{i},v_{k}\rangle\langle u_{k},e_{\ell} \rangle|\leq 100\sigma^{3}\sqrt{d}w^{\frac{1}{2}}\log d\). This implies that with high probability,

\[\|\langle u_{1},v_{k}\rangle u_{1}+\ldots+\langle u_{w},v_{k} \rangle u_{w}\|\leq\sqrt{w}\sigma^{3}\sqrt{d}w^{\frac{1}{2}}\log d\] (19) \[\|P_{1}v_{k}\|\leq\sigma\sqrt{d}\log d\] (20)

Now let \(W_{2}^{\prime T}\) be the matrix with column vector \(v_{1}^{\prime},\ldots,v_{w}^{\prime}\) and let \(P_{2}\) be the projection to the column space of \(W_{2}^{\prime T}\). By construction \(\|W_{2}-W_{2}^{\prime}\|_{\mathrm{op}}\leq O(\sigma\sqrt{d}\log d)\), so \(\|W_{2}^{T}W_{2}-W_{2}^{\prime T}W_{2}^{\prime}\|_{\mathrm{op}}\leq O(\sigma^{ 2}\sqrt{wd}\log d)\). Since the nonzero eigenvalues of \(W_{2}^{T}W_{2}\) is at most \(O(\sigma^{2}\sqrt{wd})\) from 1, we know that the nonzero singular values of \(W_{2}^{\prime T}W_{2}^{\prime}\) is also at most \(O(\sigma^{2}\sqrt{wd})\) from 1. We conclude that \(\|P_{2}-W_{2}^{T}W_{2}\|_{\mathrm{op}}\leq O(\sigma^{2}\sqrt{wd}\log d)\). 

Proof of Proposition B.1.: : We have

\[W_{1}^{T}W_{1}W_{1}^{T}W_{1}(t) =W_{1}^{T}W_{2}^{T}W_{2}W_{1}+W_{1}^{T}(W_{1}W_{1}^{T}(t)-W_{2}^ {T}W_{2}(t))W_{1}\] \[=A^{T}A+W_{1}^{T}(W_{1}W_{1}^{T}(0)-W_{2}^{T}W_{2}(0))W_{1}\]

From lemma B.3 we see that as positive semi-definite matrix, for every constant \(\varepsilon>0\),

\[0\leq W_{1}W_{1}^{T}(0)\leq(1+\varepsilon)\sigma^{2}wI\] (21)

with high probability. Therefore

\[-(1+\varepsilon)\sigma^{2}wW_{1}^{T}W_{1}\leq W_{1}^{T}(P_{1}(0)-P_{2}(0))W_{1} \leq(1+\varepsilon)\sigma^{2}wW_{1}^{T}W_{1}.\] (22)

By moving terms around and corollary, we have

\[(W_{1}^{T}W_{1})^{2}-(1+\varepsilon)\sigma^{2}wW_{1}^{T}W_{1}+(1+ \varepsilon)^{2}\frac{\sigma^{4}w^{2}}{4}I\] (23) \[\leq A^{T}A+\frac{(1+\varepsilon)^{2}}{4}\sigma^{4}w^{2}I\] (24) \[\leq (W_{1}^{T}W_{1})^{2}+(1+\varepsilon)W_{1}^{T}W_{1}+(1+ \varepsilon)^{2}\frac{\sigma^{4}w^{2}}{4}I\] (25)Theorem V.1.9 of [12] states that the square-root function is operator monotone, which implies that if \(A\geq B\) then \(\sqrt{A}\geq\sqrt{B}\). Taking square-root, we have

\[W_{1}^{T}W_{1}-(1+\varepsilon)\frac{\sigma^{2}w}{2}Id\leq\sqrt{A^{T}A+(1+ \varepsilon)^{2}\frac{\sigma^{4}w^{2}}{4}}\leq W_{1}^{T}W_{1}+(1+\varepsilon) \frac{\sigma^{2}w}{2}\] (26)

### Strong Bound

The weak bound does not provide useful information if \(\|W_{1}^{T}W_{1}\|_{op}<<\sigma^{2}w\). For this reason we prove strong bound, which provide useful information if \(\|W_{1}^{T}W_{1}\|_{op}<<\sigma^{2}w^{1+\square}\) for some constant \(\square>0\). Recall that the evolution of weight matrix in gradient descent is given by the following.

\[\frac{d}{dt}W_{1}(t)=\eta W_{2}^{T}\nabla C\] (27)

\[\frac{d}{dt}W_{2}(t)=\eta\nabla CW_{1}^{T}\] (28)

The goal of this section is to prove that

\[W_{1}^{T}W_{1}\approx\sqrt{A^{T}A+\sigma^{4}w^{2}I}\] (29)

For simplicity of notations, we shall assume that \(C_{1}=W_{1}^{T}W_{1}\), \(C_{2}=W_{2}W_{2}^{T}\), \(\hat{C}_{1}=\sqrt{A^{T}A+\sigma^{4}w^{2}I}\) and \(C_{2}=\sqrt{AA^{T}+\sigma^{4}w^{2}I}\). It is easy of see that \(\hat{C}_{1}\) and \(\hat{C}_{2}\) are invertible. Our main result for this section is the following proposition.

**Proposition B.4**.: _For every cost \(C\) we have_

\[\|W_{1}^{T}W_{1}-\sqrt{A^{T}A+\sigma^{4}w^{2}I}\|_{\mathrm{op}}\leq\text{min} \{O(\sigma^{2}w),O\left(\sqrt{\frac{d}{w}}\|W_{1}^{T}W_{1}\|_{op}\right)\}\] (30)

Proof of Lemma b.4.: : We start from the equations:

\[W_{1}^{T}\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\sigma^{4}w^{ 2}I\right]W_{1}=C_{1}^{3}-A^{T}AC_{1}-C_{1}A^{T}A-\sigma^{4}w^{2}C_{1}+A^{T}C_ {2}A\] \[W_{2}\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\sigma^{4}w^{2}I \right]W_{2}^{T}=C_{2}^{3}-AA^{T}C_{2}-C_{2}AA^{T}-\sigma^{4}w^{2}C_{2}+AC_{1 }A^{T}.\]

Our goal is to show that \(C_{1},C_{2}\) are close to the solution \(\hat{C}_{1}=\sqrt{A^{T}A+\sigma^{4}w^{2}I},\hat{C}_{2}=\sqrt{AA^{T}+\sigma^{4 }w^{2}I}\) with

\[0=\hat{C}_{1}^{3}-A^{T}A\hat{C}_{1}-\hat{C}_{1}A^{T}A-\sigma^{4}w ^{2}\hat{C}_{1}+A^{T}\hat{C}_{2}A\] \[0=\hat{C}_{2}^{3}-AA^{T}\hat{C}_{2}-\hat{C}_{2}AA^{T}-\sigma^{4}w ^{2}\hat{C}_{2}+A\hat{C}_{1}A^{T}.\]

Apriori, the cubic equation for \(C_{1}\) and \(C_{2}\) might have multiple solutions. We give an intuitive argument explaining why \(\hat{C}_{1}\) and \(\hat{C}_{2}\) are the correct solutions. By selecting a proper basis, we assume \(A=diag(a_{1},\ldots,a_{d})\) is diagonal. Assume that \((W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}=\sigma^{2}wI\). Also assume that \(C_{1}\) and \(C_{2}\) both commute with \(A\). In this case the equations for \(C_{1}\) and \(C_{2}\) reduces to cubic equation for scalars. Let \(\lambda_{1},\ldots,\lambda_{d}\) be the eigenvalues of \(C_{1}\). Solving the equations for scalars, we have \(\lambda_{i}=0\) or \(\pm\sqrt{a_{i}^{2}+\sigma^{4}w^{2}}\). By lemma B.2 and lemma B.3, we have

\[W_{1}W_{1}^{T}(t)-W_{2}^{T}W_{2}(t)=W_{1}W_{1}^{T}(0)-W_{2}^{T}W_{2}(0)=\sigma ^{2}wP_{1}-\sigma^{2}wP_{2}+o(\sigma^{2}w).\]

Since \(W_{1}W_{1}^{T}\) is positive semi-definite, all its eigenvalues are non-negative, and thus

\[W_{1}W_{1}^{T}(t)\geq(\sigma^{2}w+o(\sigma^{2}w))P_{1}.\]

Since the top \(w\) eigenvalues of \(W_{1}^{T}W_{1}\) is the same as the top \(w\) eigenvalues of \(W_{1}W_{1}^{T}\), we conclude that \(\lambda_{i}\geq\sigma^{2}w(1+o(1))\). This forces \(\lambda_{i}=\sqrt{a_{i}^{2}+\sigma^{4}w^{2}}\).

Since \(C_{1}\), \(C_{2}\) are not assumed to be aligned with \(A\), we cannot reduce the matrix cubic equations into scalar cubic equations. The high level idea for proving \(dC_{i}:=C_{i}-\hat{C}_{i}\) is small is the inverse function theorem.

1. Step 1: show that LHS of the equations are small.
2. Step 2: reduce the RHS of the equations to a linear function of \(dC_{1}\) and \(dC_{2}\). The system of equations is thus reduced to \[\begin{pmatrix}\text{small}\\ \text{small}\end{pmatrix}=\begin{pmatrix}*&*\\ *&*\end{pmatrix}\begin{pmatrix}v_{i}^{T}dC_{1}\\ u_{i}^{T}dC_{2}\end{pmatrix}.\] The \(*\) matrix is now the "Jacobian" matrix, and \(u_{i},v_{i}\) are left and right singular vectors of \(A\).
3. Step 3: prove that the "Jacobian" matrix \(\begin{pmatrix}*&*\\ *&*\end{pmatrix}\) is strictly positive definite, thus proving that \(v_{i}^{T}dC_{1}\) and \(u_{i}^{T}dC_{2}\) have small magnitude for all \(i\).

For gradient flow, \(W_{2}^{T}W_{2}-W_{1}W_{1}^{T}\) is preserved and there exists projections \(P_{1}\) and \(P_{2}\) such that \(W_{2}^{T}W_{2}-W_{1}W_{1}^{T}=\sigma^{2}w(P_{2}-P_{1})+O(\sigma^{2}\sqrt{wd})\). Therefore for every unit vector \(v\) we have

\[\|v^{T}W_{1}^{T}\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\sigma^{4}w^{2}I \right]W_{1}\|\leq\|C_{1}\|_{op}\sigma^{4}d^{\frac{1}{2}}w^{\frac{3}{2}}.\]

Substracting the second pair of equations from the first pair and denoting \(dC_{i}=C_{i}-\hat{C}\), we obtain:

\[\|C_{1}\|_{op}O(\sigma^{4}w^{2}\sqrt{\frac{d}{w}}) =C_{1}^{3}-\hat{C}_{1}^{3}-A^{T}AdC_{1}-dC_{1}A^{T}A-\sigma^{4}w^{ 2}dC_{1}+A^{T}dC_{2}A\] (31) \[\|C_{2}\|_{op}O(\sigma^{4}w^{2}\sqrt{\frac{d}{w}}) =C_{2}^{3}-\hat{C}_{2}^{3}-AA^{T}dC_{2}-dC_{2}AA^{T}-\sigma^{4}w^{ 2}dC_{2}+AdC_{1}A^{T}.\] (32)

Now since

\[C_{1}^{3}-\hat{C}_{1}^{3}=\hat{C}_{1}^{2}dC_{1}+\hat{C}_{1}dC_{1}C_{1}+dC_{1} C_{1}^{2},\]

we substitute the above relation to equation 31 and obtain

\[\|C_{1}\|_{op}O(\sigma^{4}w^{2}\sqrt{\frac{d}{w}}) =\left(\hat{C}_{1}^{2}-A^{T}A\right)dC_{1}+\hat{C}_{1}dC_{1}C_{1} +dC_{1}\left(C_{1}^{2}-A^{T}A\right)-dC_{1}+A^{T}dC_{2}A\] (33) \[=\hat{C}_{1}dC_{1}C_{1}+dC_{1}\left(C_{1}^{2}-A^{T}A\right)+A^{T }dC_{2}A,\] (34)

and similarly for equation 32. For any singular value \(s_{i}\) of \(A\), with left and right singular vectors \(u_{i},v_{i}\), we multiply equation 34 to the left by \(v_{i}^{T}\), and divide both sides by \(\sigma^{2}w\), to obtain an equation for \(v_{i}^{T}dC_{1}\). Similarly, we obtain an equation for \(u_{i}^{T}dC_{2}\): for \(v_{i}^{T}dC_{1}\) and \(u_{i}^{T}dC_{2}\):

\[\|C_{1}\|_{op}O(\sigma^{2}\sqrt{wd}) =v_{i}^{T}dC_{1}\left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w} \right)^{2}+1}C_{1}+\frac{1}{\sigma^{2}w}(C_{1}^{2}-A^{T}A)\right)+\left( \frac{s_{i}}{\sigma^{2}w}\right)u_{i}^{T}dC_{2}A\] \[\|C_{2}\|_{op}O(\sigma^{2}\sqrt{wd}) =u_{i}^{T}dC_{2,i}\left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w} \right)^{2}+1}C_{2}+\frac{1}{\sigma^{2}w}(C_{2}^{2}-AA^{T})\right)+\left( \frac{s_{i}}{\sigma^{2}w}\right)v_{i}^{T}dC_{1}A^{T}.\]

Notice that \(C_{1}^{2}-A^{T}A=W_{1}^{T}(W_{1}W_{1}^{T}-W_{2}^{T}W_{2})W_{1}=\sigma^{2}wW_{1 }^{T}(P_{1}-P_{2})W_{1}+\|C_{1}\|O(\sigma^{2}\sqrt{wd})\). In the two equations above, by replacing \(C_{1}^{2}-A^{T}A\) with \(\sigma^{2}wW_{1}^{T}(P_{1}-P_{2})W_{1}\), we are making an error of at most \(\|C_{1}\|_{op}\|dC_{1}\|_{op}O(\sqrt{\frac{d}{w}})\). From weak bound we know that \(\|dC_{1}\|_{op}\leq O(\sigma^{2}w)\). Therefore the error we made by making the approximation on the right hand side can be absorbed into left hand side.

To show that \(\|v_{i}^{T}dC_{1}\|\) and \(\|u_{i}^{T}dC_{2}\|\) are small, it suffices to show that the \((d_{in}+d_{out})\times(d_{in}+d_{out})\) block matrix

\[\left(\begin{array}{cc}\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}C_{ 1}+W_{1}^{T}(P_{1}-P_{2})W_{1}&\left(\frac{s_{i}}{\sigma^{2}w}\right)A\\ \left(\frac{s_{i}}{\sigma^{2}w}\right)A^{T}&\sqrt{\left(\frac{s_{i}}{\sigma^{2} w}\right)^{2}+1}C_{2}+W_{2}(P_{2}-P_{1})W_{2}^{T}\end{array}\right)\]is strictly positive definite. This matrix can be further simplified to

\[\left(\begin{array}{cc}W_{1}^{T}&0\\ 0&W_{2}\end{array}\right)\left(\begin{array}{cc}\sqrt{\left(\frac{s_{i}}{ \sigma^{2}w}\right)^{2}+1}I+P_{1}-P_{2}&\left(\frac{s_{i}}{\sigma^{2}w}\right)I \\ \left(\frac{s_{i}}{\sigma^{2}w}\right)I&\sqrt{\left(\frac{s_{i}}{\sigma^{2}w} \right)^{2}+1}I+P_{2}-P_{1}\end{array}\right)\left(\begin{array}{cc}W_{1}&0 \\ 0&W_{2}^{T}\end{array}\right)\]

The inner matrix can then be rewritten as \(RR^{T}\) where \(R\) is defined as

\[R=\left(\begin{array}{cc}\sqrt{\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^ {2}+1}+1}P_{1}+\sqrt{\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}-1}P_{ 2}\\ \sqrt{\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}-1}P_{1}+\sqrt{ \sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}+1}P_{2}\end{array}\right).\]

Let \(Q=\left(\begin{array}{cc}W_{1}^{T}&0\\ 0&W_{2}\end{array}\right)\).The "Jacobian" matrix is then \(QR(QR)^{T}\). As described in the strategy, we need to show that the singular values are strictly positive. The smallest nonzero singular value of \(QR(QR)^{T}\) is the same as the smallest nonzero singular value of \((QR)^{T}QR\). We expand \((QR)^{T}QR\) as follows.

\[(QR)^{T}QR\] \[= \left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}+1\right) P_{1}W_{1}W_{1}^{T}P_{1}+\left(\frac{s_{i}}{\sigma^{2}w}\right)P_{1}W_{1}W_{1}^{T}P _{2}\] \[+\left(\frac{s_{i}}{\sigma^{2}w}\right)P_{2}W_{1}W_{1}^{T}P_{1}+ \left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}-1\right)P_{2}W_{1}W_ {1}^{T}P_{2}\] \[+\left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}+1 \right)P_{2}W_{2}^{T}W_{2}P_{2}+\left(\frac{s_{i}}{\sigma^{2}w}\right)P_{1}W_ {2}^{T}W_{2}P_{2}\] \[+\left(\frac{s_{i}}{\sigma^{2}w}\right)P_{2}W_{2}^{T}W_{2}P_{1}+ \left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}-1\right)P_{1}W_{2} ^{T}W_{2}P_{1}\] \[= \left(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}+1\right) (P_{1}+P_{2})\sigma^{2}w\] \[+2(\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}-\left( \frac{s_{i}}{\sigma^{2}w}\right))(P_{1}W_{2}^{T}W_{2}P_{1}+P_{2}W_{1}W_{1}^{T }P_{2})\] \[+\left(\frac{s_{i}}{\sigma^{2}w}\right)(W_{1}W_{1}^{T}+W_{2}^{T}W_ {2})+O(\sigma^{2}\sqrt{wd}).\]

where we used the fact that

\[W_{1}W_{1}^{T}=P_{1}+P_{1}W_{2}^{T}W_{2}P_{1}+P_{1}W_{1}^{T}P_{2}+P_{2}W_{1}W_ {1}^{T}P_{1}+P_{2}W_{1}W_{1}^{T}P_{2}.\]

The \((d_{in}+d_{out})\)-th eigenvalue of the above is lower bounded by \(\sigma^{2}w\sqrt{\left(\frac{s_{i}}{\sigma^{2}w}\right)^{2}+1}+\sigma^{2}w- \sigma^{2}w\left(\frac{s_{i}}{\sigma^{2}w}\right)\geq\sigma^{2}w\). We conclude that

\[\|dC_{1}\|_{op}+\|dC_{2}\|_{op}\leq O(\sqrt{\frac{d}{w}})\|C_{1}\|_{op},\]As suggested by an anonymous referee, it is possible to obtain the same approximated dynamics of \(A_{\theta(t)}\) by imposing a non-homogeneous balance condition (in a different setup). Assume that \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}=2\sigma^{2}wI\). Then

\[C_{1}^{2}+2\sigma^{2}wC_{1}-A^{T}A=0;\]

\[C_{2}^{2}-2\sigma^{2}wC_{2}-AA^{T}=0.\]

Therefore \(C_{1}=-\sigma^{2}w+\sqrt{A^{T}A+\sigma^{4}w^{2}}\) and \(C_{2}=\sigma^{2}w+\sqrt{AA^{T}+\sigma^{4}w^{2}}\). Substituting into Gradient Flow equation, we have

\[\frac{dA}{dt}=-\eta(\sqrt{AA^{T}+\sigma^{4}w^{2}}\nabla C+\nabla C\sqrt{A^{T}A +\sigma^{4}w^{2}}).\]

The advantage of the setup is that it significantly simplifies the proof. The The limitation of the setup is that \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\neq\sigma^{4}w^{2}I\) if the initial variance of entries of \(W_{1}\) and \(W_{2}\) are comparable. In this case, \(W_{1}W_{1}^{T}-W_{2}^{T}W_{2}\) will have \(w\) positive singular values and \(w\) negative singular values, and the absolute value of positive and negative singular values are comparable. In the setup of our problem, the variance of entries of \(W_{1}\) and \(W_{2}\) equal.

## Appendix C Gradient Flow Dynamics of \(A_{t}\) in Active Regime

### Saddle to Saddle Regime

Let \(A_{t}\) have the following dynamics:

\[d^{2}\frac{d}{dt}A_{t}=(A^{*}-A_{t})\sqrt{A_{t}^{T}A_{t}+\sigma^{4}w^{2}I}+ \sqrt{A_{t}A_{t}^{T}+\sigma^{4}w^{2}I}(A^{*}-A_{t}).\]

The goal of this section is to prove that the singular vectors of \(A_{t}\) is well-aligned with the singular vectors of \(A^{*}\), throughout the Saddle-to-Saddle regime. In the rest of the section, we will assume the dependence of \(A_{t}\) on \(t\) and use \(A\) to represent \(A_{t}\). If at initialization, \(A_{t}\) commutes with \(A^{*}\), then throughout the training, \(A_{t}\) will always commute with \(A^{*}\). In this section, we use a delicate stability argument to show that if \(A_{t}\) almost commute with \(A^{*}\) at the beginning of the Saddle to Saddle regime, then it will continue to be almost commutative with \(A^{*}\) throughout the training process.

**Definition C.1**.: _Define \(P_{1}\) be the family of \(d\times d\) matrices \(A\) that satisfies the following conditions._

* \(s_{K}\geq C\sigma^{2}w\)_,_ \(s_{K+1}\leq C^{\prime}\sigma^{2}w\) _and_ \(\frac{s_{K+1}}{s_{K}}\leq c<\frac{1}{2}\) _for some_ \(d\)_-independent constants_ \(c\)_,_ \(C\) _and_ \(C^{\prime}\)_._
* _If_ \(a_{k}>a_{k+1}\)_, then_ \(s_{k}-s_{k+1}\geq cs_{k}\) _for some_ \(d\)_-independent constant_ \(c\)_._

_Define \(P_{1}^{\prime}\) to be the family of \(w\times w\) matrix \(A\) such that \(s_{k}-s_{k+1}\geq\frac{c}{2}s_{k}\) if \(a_{k+1}<a_{k}\), \(\frac{s_{K+1}}{s_{K}}\leq\frac{3}{4}\) and \(s_{K}\geq c\sigma^{2}w\). Let \(\gamma>0\) be constant. Define \(P_{2}(C,\gamma)\) be the family of \(w\times w\) matrices \(A\) satisfying the following conditions._

* _(alignment of signals). Let_ \(A=USV^{T}\)_. Define_ \[x=4K-\sum_{k:\text{signal}}\operatorname{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k )+2U(k,k)V(k,k)^{T}).\]

\(P_{2}(C,\gamma)\) _is the family of matrix_ \(A\) _such that_ \(x\leq Cd^{-\gamma}\)_._

**Theorem C.2**.: _Assume that_

\[d^{2}\frac{dA}{dt}=(A^{*}-A)\sqrt{A^{T}A+\sigma^{4}w^{2}I}+\sqrt{AA^{T}+\sigma ^{4}w^{2}I}(A^{*}-A)\]

\[A(0)\in P_{1}\cap P_{2}(C_{4},\gamma)\]

_Let \(T=O(d\log d)\). Then \(\forall t\in[0,T]\), we have \(A_{t}\in P_{1}\cap P_{2}(C,\text{min}(1,\gamma))\)._Proof.: A simple result of the induction lemma C.3, lemma C.4, lemma C.6 and lemma C.7. 

We will use the following induction lemma to show that the singular vectors of \(A_{t}\) are roughly aligned with \(A^{*}\).

**Lemma C.3**.: _Assume that \(P_{1}\) and \(P_{2}\) be families of increasing sets, and let \(P_{1}^{\prime}\supset P_{1}\). Assume that we have a family of matrices \(A_{t}\), \(0\leq t\leq T\) for some fixed number \(T\). \(T\) does not depend on the family of matrices. Assume that \(A_{0}\in P_{1}\cap P_{2}\). Let \(A_{[t_{1},t_{2}]}=\{A_{t}:t_{1}\leq t\leq t_{2}\}\). Assume the following are true._

1. _If_ \(A_{[0,t]}\in P_{1}\) _then there exists a constant_ \(\varepsilon>0\) _independent of_ \(A_{t}\) _such that_ \(A_{[0,t+\varepsilon]}\subset P_{1}^{\prime}\)_._
2. _Let_ \(t_{1}<t_{2}\)_. If_ \(A_{[0,t_{2}]}\subset P_{1}^{\prime}\) _and_ \(A_{[0,t_{1}]}\in P_{2}\)_, then_ \(A_{[0,t_{2}]}\subset P_{2}\)_._
3. _Let_ \(t_{1}<t_{2}\)_. If_ \(A_{[0,t_{2}]}\subset P_{2}\) _and_ \(A_{[0,t_{1}]}\in P_{1}\) _then_ \(A_{[0,t_{2}]}\subset P_{1}\)_._

_Then \(A_{t}\in P_{1}\) and \(A_{t}\in P_{2}\), \(\forall 0\leq t\leq T\)._

Proof.: Since \(A_{0}\) satisfies \(P_{1}\), use condition 1 we have \(A_{[0,\varepsilon]}\subset P_{1}^{\prime}\). Using condition 2, we know that \(A_{[0,\varepsilon]}\subset P_{2}\). By condition 3 we know that \(A_{[0,\varepsilon]}\subset P_{1}\) and by condition 2, \(A_{[0,\varepsilon]}\subset P_{2}\). Iterate the argument. 

**Lemma C.4**.: _If \(A_{t}\in P_{1}\), then there exists \(\tau=\sigma^{2}\sqrt{dw}d^{-1}\) such that \(A_{[t,t+\tau]}\subset P_{1}^{\prime}\)._

Proof.: Let \(0\leq s\leq\tau\). Using the approximation in time \([t,s+t]\) we have

\[d^{2}\frac{dS}{dt}= I\odot\left(U^{T}A^{*}V\sqrt{S^{2}+\sigma^{4}w^{2}}+\sqrt{S^{2}+ \sigma^{4}w^{2}}U^{T}A^{*}V-2S\sqrt{S^{2}+\sigma^{4}w^{2}}\right)\]

For \(j=1,2,\ldots,K+1\),

\[d^{2}\bigg{|}\frac{ds_{j}}{dt}\bigg{|}\leq Cd(s_{j}+\sigma^{2}w)\]

for some constant \(C\) independent of \(w\) and \(A_{t}\). The conclusion follows from Gronwall.

**Lemma C.5**.: _Assume that_

\[d^{2}\frac{dA}{dt}= (A^{*}-A+E)\sqrt{A^{T}A+\sigma^{2}wI}+\sqrt{AA^{T}+\sigma^{4}w^{2 }}I(A^{*}-A+E).\]

_Then for signal \(k\),_

\[d^{2}\frac{d}{dt}tr[U^{T}(k,k)U(k,k)]=tr[U^{T}(k,k)\sum_{j\neq k}U(k,j)D(j,k)]\]

_Here for \(j\neq k\),_

\[D(j,k)=R(k,j)\odot C(j,k)-R(j,k)\odot C(k,j)^{T}\]

_with \(C=U^{T}(A^{*}+E)V\), \(R_{pq}=\frac{s_{p}\tilde{s}_{q}+s_{p}\tilde{s}_{q}}{s_{p}^{2}-s_{q}^{2}}\) for \(1\leq p,q\leq w\), \(p\neq q\), and \(R(k,j)=R_{n_{k};n_{k+1},n_{j};n_{j+1}}\) being the \(k,j\)-th block of \(R\)._Proof.: Use SVD derivative.

\[\frac{dU}{dt}= U\left(F\odot\left[U^{T}\frac{dA}{dt}VS+SV^{T}\frac{dA^{T}}{dt}U \right]\right)\] (35) \[= U\left(F\odot\left[(U^{T}(A^{*}+E)V-S)\sqrt{S^{2}+\sigma^{4}w^{2}}S +\sqrt{S^{2}+\sigma^{4}w^{2}}(U^{T}(A+E)^{*}V-S)S\right]\right)\] (36) \[+U\left(F\odot\left[\sqrt{S^{2}+\sigma^{4}w^{2}}S(V^{T}(A^{*}+E)^ {T}U-S)+S(V^{T}(A^{*}+E)^{T}U-S)\sqrt{S^{2}+\sigma^{4}w^{2}}\right]\right)\] (37) \[= U\left(F\odot\left[U^{T}(A^{*}+E)V\sqrt{S^{2}+\sigma^{4}w^{2}}S +\sqrt{S^{2}+\sigma^{4}w^{2}}U^{T}(A^{*}+E)VS\right]\right)\] (38) \[+U\left(F\odot\left[\sqrt{S^{2}+\sigma^{4}w^{2}}SV^{T}(A^{*}+E)^ {T}U+SV^{T}(A^{*}+E)^{T}U\sqrt{S^{2}+\sigma^{4}w^{2}}\right]\right)\] (39)

Let \(D=F\odot\left[U^{T}\frac{dA}{dt}VS+SV^{T}\frac{dA^{T}}{dt}U\right]\). Since \(F\) is anti-symmetric and the term in square bracket is symmetric, we know \(D\) is anti-symmetric. Then \(\frac{dU}{dt}=UD\). Let \(\tilde{S}=\sqrt{S^{2}+\sigma^{4}w^{2}}\), \(C=U^{T}(A^{*}+E)V\). As a result, \(\forall 1\leq p,q\leq w\),

\[\frac{dU_{pq}}{dt}=\sum_{r:r\neq q}U_{pr}\frac{1}{s_{q}^{2}-s_{r}^{2}}\left[(s _{q}\tilde{s}_{q}+\tilde{s_{r}}s_{q})C_{rq}+(\tilde{s_{r}}s_{r}+s_{r}\tilde{s}_ {k})C_{rq}\right]\]

Let \(R_{pq}=\frac{s_{q}s_{p}+s_{p}s_{q}}{s_{p}^{2}-s_{q}^{2}}\) if \(p\neq q\) and \(R_{pp}=0\). Then

\[\frac{dU}{dt}(k,k)=\sum_{j:j\neq k}U(k,j)\left(R(k,j)\odot C(j,k)-R(j,k)\odot C (k,j)^{T}\right)\]

\[D(j,k)=R(k,j)\odot C(j,k)-R(j,k)\odot C(k,j)^{T}.\]

As a result, for \(n_{k}\leq p,q<n_{k+1}\),

\[\frac{1}{2}\frac{d}{dt}tr[U^{T}(k,k)U(k,k)] =\sum_{p,q\in[n_{k},n_{k+1})}U_{pq}\sum_{r}U_{pr}D_{rq}\] \[=\sum_{p,q,r\in[n_{k},n_{k+1})}U_{pq}U_{pr}D_{rq}+\sum_{p,q\in[n_{ k},n_{k+1})}U_{pq}\sum_{r\not\in[n_{k},n_{k+1})}U_{pr}D_{rq}\] \[= \sum_{p,q\in[n_{k},n_{k+1})}U_{pq}\sum_{r\not\in[n_{k},n_{k+1})}U_ {pr}D_{rq}\] \[= tr[U^{T}(k,k)\sum_{j\neq k}U(k,j)D(j,k)]\]

**Lemma C.6**.: _If \(A_{[t_{1},t_{2}]}\subset P_{1}\) and \(A_{t_{1}}\in P_{2}(C_{4},\gamma)\), then_

\[d^{2}\frac{dx}{dt}\leq-cdx+O(\sqrt{dx}),\]

_where \(\forall c=\inf_{t\in[t_{1},t_{2}]}\text{min}(\frac{(s_{k}^{*}-s_{j}^{*})(s_{k }+s_{j})(\tilde{s}_{k}+\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}},\frac{(s_{k}-s_{j} )(s_{k}^{*}+s_{j}^{*})(\tilde{s}_{k}+\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}})\). In particular, \(A_{[t_{1},t_{2}]}\subset P_{2}(C,\text{min}(\gamma,1))\) for some constant \(C\). If \(\gamma<1\) then we can take \(C=C_{4}\)._

Proof.: Use previous lemma. Let

\[x=4K-\sum_{k:\text{signal}}\text{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k)+2U(k,k )V(k,k)^{T}).\]\(x\) measures the alignment of singular vectors of \(A\) with singular vectors of \(A^{*}\). From \(UU^{T}=I\) we have

\[\sum_{\ell}U(k,\ell)U(k,\ell)^{T}=I\]

and for every \(j\neq k\),

\[U(k,j)U(k,j)^{T}\leq I-U(k,k)U(k,k)^{T}\leq O(x)\]

In particular, \(\|U(k,j)\|_{op}\leq O(\sqrt{x})\). We first estimate \(C\). For all \(j=1,2,\ldots,w,a_{j}\neq a_{k}\),

\[(U^{T}A^{*}V)(j,k)= U^{T}(j,j)S^{*}(j,j)V(j,k)+U^{T}(j,k)S^{*}(k,k)V(k,k)+\sum_{ \ell\neq j,k}U^{T}(j,\ell)S^{*}_{\ell}V(\ell,k)\] \[= U^{T}(j,j)S^{*}(j,j)V(j,k)+U(k,j)^{T}S^{*}(k,k)V(k,k)\] \[+O(dx)\]

\[(U^{T}EV)(j,k)= \sum_{\ell_{2}\neq k}U^{T}(j,j)E(j,\ell_{2})V(\ell_{2},k)+\sum_{ \ell_{1}\neq j}U^{T}(j,\ell_{1})E(\ell_{1},k)V(k,k)\] \[+U^{T}(j,j)E(j,k)V(k,k)+\sum_{\ell_{1}\neq j,\ell_{2}\neq k}U^{T}( j,\ell_{1})E(\ell_{1},\ell_{2})V(\ell_{2},k)\] \[= O(\sqrt{d}\sqrt{x})+O(\sqrt{d})+O(\sqrt{d}x)\] \[= O(\sqrt{d})\]

\[D(j,k)= R(k,j)\odot C(j,k)-R(j,k)\odot C(k,j)^{T}\] \[= R(k,j)\odot(U(j,j)^{T}S^{*}(j,j)V(j,k))+R(k,j)\odot U(k,j)^{T}S^ {*}(k,k)V(k,k)\] \[-R(j,k)\odot(V(k,j)^{T}S^{*}(k,k)U(k,k)+V(j,j)^{T}S^{*}(j,j)U(j,k))\] \[+O(dx+\sqrt{d})\]

By \(UU^{T}=I\) we know that \(\sum_{\ell}U(j,\ell)U^{T}(\ell,k)=0\). Therefore

\[U(j,j)U(k,j)^{T}+U(j,k)U(k,k)^{T}+O(x)=0\]

\[U(j,k)=-O(x)-U(j,j)U(k,j)^{T}U(k,k).\]

Similarly we have for \(V\),

\[V(k,j)=-O(x)-V(k,k)V(j,k)^{T}V(j,j).\]

We can rewrite \(D(j,k)\) as

\[D(j,k)= R(k,j)\odot(U(j,j)^{T}S^{*}(j,j)V(j,k)+U(k,j)^{T}S^{*}(k,k)V(k,k))\] \[+ R(j,k)\odot(V(j,j)^{T}V(j,k)V(k,k)^{T}S^{*}(k,k)U(k,k)+V(j,j)^{T }S^{*}(j,j)U(j,j)U(k,j)^{T}U(k,k))\] \[+O(dx+\sqrt{d})\]\[d^{2}\frac{d}{dt}\text{tr}\left[U(k,k)^{T}U(k,k)\right]\] \[= 2\text{tr}\left[U(k,k)^{T}\sum_{j}U(k,j)D(j,k)\right]\] \[= 2\sum_{j}\text{tr}\left[U(k,k)^{T}U(k,j)\left(R(k,j)\odot(U(j,j) ^{T}S^{*}(j,j)V(j,k)+U(k,j)^{T}S^{*}(k,k)V(k,k))\right)\right]\] \[+2\sum_{j}\text{tr}\left[U(k,k)^{T}U(k,j)\left(R(j,k)\odot(V(j,j) ^{T}V(j,k)V(k,k)^{T}S^{*}(k,k)U(k,k))\right)\right]\] \[+2\sum_{j}\text{tr}\left[U(k,k)^{T}U(k,j)\left(R(j,k)\odot(V(j,j) ^{T}S^{*}(j,j)U(j,j)U(k,j)^{T}U(k,k))\right)\right]\] \[+O(dx^{\frac{3}{2}}+\sqrt{d}\sqrt{x})\] \[\geq -2\sum_{j}|S^{*}(j,j)R(k,j)+S^{*}(k,k)R(j,k)|\sqrt{tr(U(k,j)^{T}U (k,j)}\sqrt{tr(V(j,k)^{T}V(j,k)}\] \[+2\sum_{j}S^{*}(k,k)\text{tr}[U(k,k)^{T}U(k,j)R(k,j)\odot(U(k,j) ^{T}U(k,k))]\] \[+2\sum_{j}S^{*}(j,j)\text{tr}[U(k,k)^{T}U(k,j)R(j,k)\odot(U(k,j) ^{T}U(k,k))]\] \[+O(dx^{\frac{3}{2}})+O(\sqrt{dx})\]

We know that

\[(S^{*}(k,k)R(k,j)+S^{*}(j,j)R(j,k))-|S^{*}(j,j)R(k,j)+S^{*}(k,k)R( j,k)|\] \[= \text{min}(\frac{(s_{k}^{*}-s_{j}^{*})(s_{k}+s_{j})(\tilde{s}_{ k}+\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}},\frac{(s_{k}-s_{j})(s_{k}^{*}+s_{j}^{ *})(\tilde{s}_{k}+\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}})\] \[\geq cd\]

for some positive constant \(c\) independent of \(w\). We conclude that

\[d^{2}\frac{d}{dt}tr[U(k,k)^{T}U(k,k)]\geq cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx}).\]

The same trick applies to \(V\). We similarly have

\[d^{2}\frac{d}{dt}tr[V(k,k)^{T}V(k,k)]\geq cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx}).\]

It remains to show that \(\frac{d}{dt}tr[V(k,k)^{T}U(k,k)]\) bounded below.

\[d^{2}\frac{d}{dt}\text{tr}[U(k,k)^{T}V(k,k)]=tr[U(k,k)^{T}\frac{dV}{dt}(k,k)] +tr[V(k,k)^{T}\frac{dU}{dt}(k,k)]\]

\[tr[V(k,k)^{T}\frac{d}{dt}U(k,k)]= tr[U(k,k)^{T}\frac{d}{dt}U(k,k)]+tr[(V(k,k)^{T}-U(k,k)^{T}) \frac{d}{dt}U(k,k)]\] \[\geq tr[U(k,k)^{T}\frac{d}{dt}U(k,k)]-\|V(k,k)-U(k,k)\|_{F}\|\frac{d}{dt }U(k,k)\|_{F}\] \[\geq O(dx^{\frac{3}{2}})+cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx})\] \[\geq cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx})\]

Similarly,

\[tr[U(k,k)^{T}\frac{d}{dt}V(k,k)]\geq cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx}).\]This proves that

\[d^{2}\frac{dx}{dt}\leq-cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx}).\] (40)

Observe that if \(x\geq d^{-1+\varepsilon}\) for some \(\varepsilon>0\), then \(\frac{dx}{dt}\leq-\frac{c}{2}dx\) and therefore \(x\) decrease exponentially. On the other hand, if \(x\leq d^{-1-\varepsilon}\) for some \(\varepsilon>0\), then \(\frac{dx}{dt}\geq O(\sqrt{dx})\) and therefore \(x\) might increase. We therefore conclude that \(A_{[t_{1},t_{2}]}\subset P_{2}(C,\text{min}(\gamma,1))\). 

**Lemma C.7**.: _Assume that \(A_{[0,t_{2}]}\subset P_{2}(C_{4},\gamma)\) and \(A_{[0,t_{1}]}\subset P_{1}\). Then \(A_{[0,t_{2}]}\subset P_{1}\)._

Proof.: By assumption we know that \(t_{2}=O(d\log d)\).

\[d^{2}\frac{dS}{dt}= I\odot\left(U^{T}(A^{*}+E)V\sqrt{S^{2}+\sigma^{4}w^{2}}+\sqrt{S ^{2}+\sigma^{4}w^{2}}U^{T}(A^{*}+E)V-2S\sqrt{S^{2}+\sigma^{4}w^{2}}\right)\]

\[d^{2}\frac{ds_{K+1}}{dt}=2\left(O(\sqrt{d})+O(d^{1-\gamma})-s_{K+1}\right) \sqrt{s_{K+1}^{2}+\sigma^{4}w^{2}}\]

\[|s_{K+1}(t_{2})-s_{K+1}(t_{1})|\leq O(\sigma^{2}w(d^{-\frac{1}{2}}+d^{-\gamma} )\log d)<<\sigma^{2}w\]

It remains to verify the gap between different families. Let \(T\) be the first time when \(\left\|A\right\|_{op}=\frac{1}{2}s_{K}^{*}\) and assume that \(t_{2}\leq T\). Assume that \(a_{k}>a_{j}\), then there exists some constant \(\varepsilon\) such that

\[d^{2}\frac{ds_{k}}{dt}\geq(s_{k}^{*}(1-\varepsilon)-s_{k})s_{k}\]

\[d^{2}\frac{ds_{j}}{dt}\leq(s_{j}^{*}(1+\varepsilon)-s_{j})(s_{j}+\sigma^{2}w)\]

An application of Gronwall inequality on \(s_{k}\) and \(s_{j}\) implies that there exists some constant \(\delta>0\) that depends only on \(a_{j},a_{k}\) such that \(\frac{s_{j}(T)}{s_{k}(T)}\leq O(d^{-\delta})\). Next we deal with the case when \(t_{2}>T\). One important observation is that the \(\inf\{t>T:s_{1}(t)\geq\frac{a_{k}d+a_{j}d}{2}\}-T=O(d)\), as a simple result of Gronwall. Moreover, \(s_{2}(T+O(d))\leq\frac{1}{4}s_{K}^{*}\). This implies that \(s_{1}-s_{k}>cs_{1}\) for some constant \(c\). Repeat this argument for all remaining signal singular values completes the proof. 

### First NTK Regime

At initialization, the singular values of \(A\) are of order \(\sigma^{2}\sqrt{wd}\) by using Mechenko-Pastur law on \(A^{T}A\), which is infinitely smaller than \(\sigma^{2}w\). As a result, there is a very short period when the dynamics is very close to NTK. This section is again a delicate stability argument to show that at the end of the first NTK regime, \(A_{t}\) is almost commutative with \(A^{*}\).

**Lemma C.8**.: _Let \(t_{1}\) be the first when the first singular value of \(A\) hits \(\sigma^{2}wd^{-\frac{\delta}{2}}\) for \(\delta=\frac{\gamma_{w}-1}{2}\). Then \(A_{t_{1}}\in P_{2}(C,\text{min}(\frac{1}{2},\frac{\delta}{2}))\) for some constant \(C\)._

Proof.: We approximate the dynamics with NTK dynamics. Assume that

\[d^{2}\frac{dB}{dt}=2(A^{*}+E-B)\sigma^{2}w\]

with \(B(0)=A(0)\). Then

\[d^{2}\frac{d(A-B)}{dt}= 2(B-A)\sigma^{2}w(A^{*}+E-A)\] \[+(\sqrt{A^{T}A+\sigma^{4}w^{2}}-\sigma^{2}w)+(\sqrt{AA^{T}+\sigma ^{4}w^{2}}-\sigma^{2}w)(A^{*}+E-A).\]

A simple application of Gronwall inequality implies that

\[\left\|A-B\right\|_{op}(t)\leq \frac{1}{d^{2}}e^{-2\sigma^{2}wt}\int_{0}^{t}e^{2\sigma^{2}w\tau} \frac{2s_{1}(\tau)^{2}}{\sigma^{2}w}\|A^{*}+E-A\|_{op}(\tau)d\tau\] \[\leq 3\frac{\|A_{t}\|_{op}^{2}}{\sigma^{2}w}s_{1}^{*}\frac{t}{d^{2}}\]We first check \(P_{2}\). Clearly, \(t_{1}\) is of order \(d^{1-\frac{\delta}{2}}\). Then we have

\[\|A_{t_{1}}-\frac{t_{1}}{d^{2}}\sigma^{2}wA^{*}\|_{op}\leq 2d^{-\delta}\|A_{t_{1}}\|_{op}+\|\frac{t_{1}}{d^{2}}\sigma^{2}wE \|_{op}+O(\sigma^{2}\sqrt{wd})\] \[= (O(d^{-\delta})+O(d^{-\frac{1}{2}})+O(d^{-\frac{\delta}{2}}))\|A_ {t_{1}}\|_{op}\]

By lemma C.4 we have \(x\leq Cd^{-\text{min}(\frac{1}{2},\frac{\delta}{2})}\) for some constant \(C\). 

**Lemma C.9**.: _Let \(t_{2}=cd\), where \(c\) is a constant to be chosen. Then_

\[A_{t_{2}}\in P_{1}\cap P_{2}(C,\text{min}(\frac{\gamma_{w}-1}{4},\frac{1}{2})).\]

_Moreover, there exists constants \(b_{1}=\ldots b_{n_{1}}>b_{n_{1}+1}=\ldots=b_{n_{2}}\geq\ldots=b_{n_{m}}=b_{K}\), such that for a \(d\times d\) diagonal matrix \(\Sigma\) whose diagonal entries are given by \(b_{1},\ldots,b_{K},0,\ldots,0\), we have \(\|A_{t_{2}}-\Sigma\|_{op}\leq O(\sigma^{2}w)d^{-\text{min}(\frac{\gamma_{w}- 1}{8},\frac{1}{4})}\)._

Proof.: At time \(t_{2}\), \(\|B(t_{2})\|_{op}=2a_{1}c\sigma^{2}w(1+o(1))\). From previous lemma we know that

\[\|A-B\|_{op}(t)\leq 3\frac{\|A_{t}\|_{op}^{2}}{\sigma^{2}w}s_{1}^{*}\frac{t}{d^{ 2}}.\]

Let \(t=t_{2}\) we see that \(|\|A(t_{2})\|_{op}-\|B(t_{2})\|_{op}|\leq\|A-B\|_{op}(t_{2})\leq 2\frac{\|A_{t_{2}} \|_{op}^{2}}{\sigma^{2}w}a_{1}c\). Therefore as long as \(c\) is sufficiently small, we can guarantee that \(\|A-B\|_{op}(t)\leq\frac{\text{min}_{1\leq k\leq K}(a_{k-1}-a_{k})}{100a_{1}} \|B\|_{op}(t)\). This guarantees that the singular values of \(A\) grows linearly in \([t_{1},t_{2}]\), that \(s_{k}-s_{k+1}\geq cs_{k}\) if \(a_{k}>a_{k+1}\) for some constant \(c\), that \(s_{K+1}<\frac{1}{2}s_{K}\) and that \(A_{t_{2}}\in P_{1}\). It remains to check that \(A_{t_{2}}\) satisfies \(P_{2}(C,\frac{\gamma_{w}-1}{2})\). We are ready to use similar techniques as in lemma C.6. All computation are similar, and the only difference is that \(|R(k,j)|=O(\frac{\sigma^{2}w}{s_{K}})\). We have

\[d^{2}\frac{d}{dt}\text{tr}[U(k,k)^{T}U(k,k)]\geq c\frac{\sigma^{2}w}{s_{K}}dx+ O(dx^{\frac{3}{2}}\frac{\sigma^{2}w}{s_{K}})+O(\sqrt{dx}\frac{\sigma^{2}w}{s_{K}})\]

\[d^{2}\frac{dx}{dt}\leq 2C\frac{\sigma^{2}w}{a_{K}(\sigma^{2}wwt+\sigma^{2}wd^{ -\frac{\delta}{2}})}(-cdx+dx^{\frac{3}{2}}+\sqrt{dx})\]

Therefore \(A_{[t_{1},t_{2}]}\subset P_{2}(C,\text{min}(\frac{\gamma_{w}-1}{4},\frac{1}{2}))\). Let \(\gamma=\text{min}(\frac{\gamma_{w}-1}{4},\frac{1}{2})\). From lemma C.8 we see that if \(n_{k_{1}}+1\leq p_{1},p_{2}\leq n_{k}\), then \(|s_{p_{1}}-s_{p_{2}}|\leq O(d^{-\gamma})s_{K}\). Moreover, the dynamics of \(s_{p_{l}},i=1,2\) are both given by

\[d^{2}\frac{ds_{p_{i}}}{dt}=2(O(\sqrt{d})+O(d^{1-\gamma})+a_{k}d-s_{p_{i}}) \sqrt{s_{p_{i}}^{2}+\sigma^{4}w^{2}}.\]

\[\frac{d|s_{p_{1}}-s_{p_{2}}|}{dt}\leq O(d^{-1})|s_{p_{1}}-s_{p_{2}}|\]

\[|s_{p_{1}}-s_{p_{2}}|(t_{2})\leq O(1)|s_{p_{1}}-s_{p_{2}}|(t_{1})=\sigma^{2}wO (d^{-\gamma-\frac{\gamma_{w}-1}{4}}).\]

Let \(b_{n_{i}}=s_{i}\). By lemma A.5, there exists a \(d\times d\) diagonal matrix \(\Sigma\), whose diagonal entries are given by \(b_{1}=\ldots=b_{n_{1}}>\ldots=b_{n_{2}}\geq\ldots=b_{K}\), such that \(\|A_{t_{2}}-\Sigma\|_{op}\leq O(\sigma^{2}w)d^{-\frac{\gamma}{2}}\). 

## Appendix D The Gradient Flow Dynamics of \(A_{\theta(t)}\) in Lazy Regime

In this section, we use a stability argument to show that if \(\sigma^{2}w\) is infinitely larger than \(d\), then the algorithm cannot converge.

**Proposition D.1**.: _With high probability, for all time \(t\), \(\|A_{\theta(t)}-A^{*}\|_{F}^{2}\geq\frac{1}{3}\text{min}(\|A^{*}\|_{F}^{2},\|E \|_{F}^{2})\)._

Proof.: In gradient flow dynamics, \(\|A_{\theta(t)}-A^{*}-E\|_{F}\) is decreasing with time and therefore for all time \(t\), \(\|A_{\theta(t)}\|_{F}^{2}\leq 9(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\). In particular, \(\|A_{\theta(t)}\|_{op}\leq 9(\|A^{*}\|_{F}+\|E\|_{F})\). Byassumption, \(\|\hat{C}_{1}-C_{1}\|_{op}\leq O(\sqrt{\frac{d}{w}})\|C_{1}\|_{op}=O(d\sqrt{\frac{ d}{w}})\), and the dynamics of \(A_{\theta(t)}\) can be written as

\[d^{2}\frac{dA_{\theta(t)}}{dt}=2(A^{*}+E-A)\sigma^{2}w+(A^{*}+E-A)O(d\sqrt{\frac {d}{w}})+O(d\sqrt{\frac{d}{w}})(A^{*}+E-A)\]

Assume that \(B(0)=A_{\theta(0)}\) and

\[d^{2}\frac{dB}{dt}=2(A^{*}+E-B)\sigma^{2}w.\]

Then

\[\frac{1}{2}d^{2}\frac{d}{dt}\|A_{\theta(t)}-B\|_{F}^{2}= -2\sigma^{2}w\|A_{\theta(t)}-B\|_{F}^{2}+tr((A-B)^{T}(A^{*}+E-A)O (d\sqrt{\frac{w}{d}}))\] \[+tr((A-B)^{T}O(d\sqrt{\frac{w}{d}})(A^{*}+E-A))\] \[\leq -2\sigma^{2}w\|A_{\theta(t)}-B\|_{F}^{2}+\|A_{\theta(t)}-B\|_{F}O (d^{2}\sqrt{\frac{d}{w}})\]

This implies that for all time,

\[\|A_{\theta(t)}-B\|_{F}\leq dO(\frac{d}{\sigma^{2}w}\sqrt{\frac{d}{w}}).\]

The dynamics of \(B\) is linear, and we have

\[B_{t}=(A^{*}+E)(1-e^{-2\sigma^{2}wt/d^{2}})+B_{0}e^{-2\sigma^{2}wt/d^{2}}.\]

\[\|B_{t}-A^{*}\|_{F}^{2}=\|e^{-2\sigma^{2}t/d^{2}}A^{*}+(1-e^{-2\sigma^{2}wt/d^{ 2}})E+B_{0}e^{-2\sigma^{2}wt/d^{2}}\|_{F}^{2}\geq 0.99\|e^{-2\sigma^{2}t/d^{2}}A^{*}+( 1-e^{-2\sigma^{2}wt/d^{2}})E\|_{F}^{2}\]

Let \(P\) be the projection to the image of \(A^{*}\). Then with high probability,

\[\|B_{t}-A^{*}\|_{F}^{2}= \|e^{-2\sigma^{2}wt/d^{2}}A^{*}+(1-e^{-2\sigma^{2}wt/d^{2}})E+B_{ 0}e^{-2\sigma^{2}wt/d^{2}}\|_{F}^{2}\] \[\geq 0.99\|e^{-2\sigma^{2}t/d^{2}}A^{*}+(1-e^{-2\sigma^{2}wt/d^{2}})E \|_{F}^{2}\] \[= 0.99\|e^{-2\sigma^{2}wt/d^{2}}A^{*}+(1-e^{-2\sigma^{2}wt/d^{2}} )PE\|_{F}^{2}+0.99\|(1-e^{-2\sigma^{2}wt/d^{2}})(I-P)E\|_{F}^{2}\] \[\geq \frac{1}{2}\text{min}(\|A^{*}\|_{F}^{2},\|E\|_{F}^{2})\]

Since \(\|A_{\theta(t)}-B\|_{F}^{2}=o(d^{2})\), we have

\[\|A_{\theta(t)}-A^{*}\|_{F}^{2}\geq\frac{1}{3}\text{min}(\|A^{*}\|_{F}^{2},\| E\|_{F}^{2})\]

## Appendix E The Gradient Flow Dynamics of \(A_{\theta(t)}\) in Active Regime

In section C, we proved stability for gradient flow dynamics for \(A_{t}\). In this section, we prove similar stability statements for \(A_{\theta(t)}\), using the approximation results from section B. We also summarize the behavior of \(A_{\theta(t)}\) in section E.3.

### Saddle-to-Saddle Regime

The goal of this section is to show that at the end of the first NTK regime, the singular vectors of \(A_{\theta(t)}\) are roughly aligned with \(A^{*}\). Moreover, the alignment remains to be good throughout the mixed regime. The following generalization of lemma C.6 and lemma C.7 will be useful.

**Lemma E.1**.: _Assume that \(A^{\prime}_{[t_{1},t_{2}]}\subset P_{1}\) and \(A_{t_{1}}\in P_{2}(C_{4},\gamma)\). Let \(s^{\prime}_{1},\ldots,s^{\prime}_{K}\) be the first \(K\) singular values of \(A^{\prime}\). Assume that the dynamics of \(A^{\prime}\) is the following._

\[d^{2}\frac{dA^{\prime}}{dt}=(A^{*}+E-A^{\prime})\sqrt{A^{\prime} A^{\prime}+\sigma^{4}w^{2}}I +\sqrt{A^{\prime}A^{\prime}T+\sigma^{4}w^{2}}(A^{*}+E-A^{\prime})\] \[+O(d^{-\lambda}s^{\prime}_{K})(A^{*}+E-A^{\prime})+(A^{*}+E-A^{ \prime})O(d^{-\lambda}s^{\prime}_{K})\]

_Here, \(O(d^{-\lambda}s^{\prime}_{K})\) is a matrix whose operator norm is bounded by \(O(d^{-\lambda}s^{\prime}_{K})\). Then \(A_{[t_{1},t_{2}]}\subset P_{2}(C_{4},\text{min}(\gamma,1,2\lambda))\)._

Proof.: Let \(A^{\prime}=USV^{T}\). Computing the SVD derivative for \(A^{\prime}\), we have

\[d^{2}\frac{dU}{dt}= U\left(F\odot\left[U^{T}A^{*}V\sqrt{S^{2}+\sigma^{4}w^{2}}S+ \sqrt{S^{2}+\sigma^{4}w^{2}}U^{T}A^{*}VS\right]\right)\] \[+U\left(F\odot\left[\sqrt{S^{2}+\sigma^{4}w^{2}}SV^{T}A^{*}U+V^{T }A^{*}US\sqrt{S^{2}+\sigma^{4}w^{2}}\right]\right)\] \[+U\left(F\odot\left[O(s_{K}d^{-\lambda})(U^{T}A^{*}VS+SV^{T}A^{*} U-2S^{2})\right]\right)\]

Assume that \(a_{k}\neq a_{\ell}\). Notice that

\[F_{k\ell}(O(s_{k}d^{-\lambda})U^{T}A^{*}VS)_{k\ell}=O(d^{1-\lambda})\]

Using the same technique as C.6 we have

\[\frac{1}{2}d^{2}\frac{d}{dt}tr[U(k,k)^{T}U(k,k)]\] \[\geq cdx+O(dx^{\frac{3}{2}})+O(\sqrt{dx})\] \[+tr[U(k,k)^{T}\sum_{j\neq k}U(k,j)(F\odot\left[O(s_{K}d^{-\lambda} )(U^{T}A^{*}VS+SV^{T}A^{*}U-2S^{2})\right])]\] \[\geq cdx+O(dx^{\frac{3}{2}})+O(d^{1-\lambda}\sqrt{x})+O(\sqrt{dx})\]

Similar conclusion also holds for \(tr[U(k,k)^{T}V(k,k)]\) and \(tr[V(k,k)^{T}V(k,k)]\). We conclude that

\[d^{2}\frac{dx}{dt}\leq-cdx+O(dx^{\frac{3}{2}})+O(d^{1-\lambda}\sqrt{x})+O( \sqrt{dx}),\]

and \(A^{\prime}_{[t_{1},t_{2}]}\subset P_{2}(C_{4},\text{min}(\gamma,1,2\lambda))\). 

**Lemma E.2**.: _Assume that \(A^{\prime}_{[t_{1},t_{2}]}\subset P_{2}(C_{4},\gamma)\) and \(A_{t_{1}}\in P_{1}\). Let \(s^{\prime}_{1},\ldots,s^{\prime}_{K}\) be the first \(K\) singular values of \(A^{\prime}\). Assume that the dynamics of \(A^{\prime}\) is the following._

\[d^{2}\frac{dA^{\prime}}{dt}=(A^{*}-A^{\prime})\sqrt{A^{\prime}A^{\prime}+ \sigma^{4}w^{2}}I+\sqrt{A^{\prime}A^{\prime}T+\sigma^{4}w^{2}}(A^{*}-A^{\prime })+O(d^{-\lambda}s^{\prime}_{K})(A^{*}-A)+(A^{*}-A)O(d^{-\lambda}s^{\prime}_{K})\]

_Here, \(O(d^{-\lambda}s^{\prime}_{K})\) is a matrix whose operator norm is bounded by \(O(d^{-\lambda}s^{\prime}_{K})\). Then \(A_{[t_{1},t_{2}]}\subset P_{1}\)._

Proof.: Proceeding as in lemma C.7, we have

\[d^{2}\frac{ds_{p}}{dt}=2(O(\sqrt{d})+O(d^{1-\gamma})+a_{p}d-s_{p})(\sqrt{s_{p }^{2}+\sigma^{4}w^{2}}+O(d^{-\lambda})s_{K})\]

if \(p\) is a signal, and

\[d^{2}\frac{ds_{K+1}}{dt}=2(O(\sqrt{d})+O(d^{1-\gamma})-s_{K+1})(\sqrt{s_{K+1} ^{2}+\sigma^{4}w^{2}}+O(d^{-\gamma})s_{K}).\]

Now the conclusion follows from Gronwall. 

**Theorem E.3**.: _Assume that \(A_{\theta(t)}\subset P_{1}\cap P_{2}(C,\gamma)\). Then \(A_{[\theta(t,T])}\subset P_{1}\cap P_{2}(C,\gamma^{\prime})\) for some constant \(\gamma^{\prime}\)._Proof.: We use induction lemma C.3 to prove the theorem. For the first requirement, notice that for every \(i=1,2,\ldots,w\),

\[d^{2}\bigg{|}\frac{ds_{i}}{dt}\bigg{|}\leq 4\|A^{*}\|_{op}^{2}.\]

If \(A_{\theta(t)}\in P_{1}\), then \(A_{\theta([t,t+\tau])}\subset P_{1}^{\prime}\) for \(\tau=c\frac{\sigma^{2}\sqrt{wd}}{d^{2}\|A^{*}\|_{op}^{2}}\). Here \(c\) is some small constant that depends only on \(P_{1}\). This proves the first requirement. For the second requirement, we observe that \(A_{\theta(t)}\) satisfies the dynamics described in lemma E.1 at each stage, as long as \(\|\hat{C}_{1}-C_{1}\|_{op}\leq O(s_{K}d^{-\lambda}\) for some \(\lambda>0\). It suffices to show that there exists some positive \(\lambda\) independent of \(w\) such that \(\|\hat{C}_{1}-C_{1}\|_{op}\leq O(d^{-\lambda}s_{K})\) in \([0,T]\). Let \(T_{1}\) be the first time when \(\|A\|_{op}=\sigma^{2}w\). In \([0,T_{1}]\), all singular values are of the same order. Therefore

\[\|\hat{C}_{1}-C_{1}\|_{op}\leq O(\|C_{1}\|\sqrt{\frac{d}{w}})=O(s_{K}d^{-\frac {\gamma w-1}{2}}).\]

Now let \(T_{2}\) be the first time when \(s_{1}=\sigma^{2}wd^{\frac{\gamma w-1}{4}}\). In \([T_{1},T_{2}]\), we have

\[\|\hat{C}_{1}-C_{1}\|_{op}\leq O(\|C_{1}\|\sqrt{\frac{d}{w}})\leq O(s_{K}d^{- \frac{\gamma w-1}{4}})\]

Therefore we can pick \(\lambda=\frac{\gamma w}{4}\). During \([T_{1},T_{2}]\), we have \(s_{K}(0)\geq c\sigma^{2}w\) for some constant \(c\), and

\[d^{2}\frac{ds_{K}}{dt}\geq s_{K}(\frac{1}{2}a_{K}d-s_{K})\]

Moreover, since \(d^{2}\frac{ds_{1}}{dt}\leq(2a_{1}-s_{1})(s_{1}+\sigma^{2}w)\), we have \(T_{2}-T_{1}\geq c\frac{\log w}{w}\) for some \(c\). Therefore we have \(s_{K}(T_{2})\geq\sigma^{2}wd^{-\nu}\) for some constant \(\nu>0\). Now in \([T_{2},T]\), we have weak bound \(\|\hat{C}_{1}-\hat{C}\|_{op}\leq O(\sigma^{2}w)\). We can therefore pick \(\lambda=\nu\) for \([T_{2},T]\). The third requirement is verified in lemma E.2. 

### First NTK Regime

As in \(A_{t}\), we need to show that at the end of the NTK regime, \(A_{\theta(t)}\) must be in \(P_{1}\) and \(P_{2}\). Based on what we already have for \(A_{t}\), this conclusion follows from Gronwall.

**Lemma E.4**.: _Assume that \(A_{0}=A_{\theta(0)}\). Let \(T_{1}^{\prime}\) be the first time when \(\|A_{t}\|_{op}+\|A_{\theta(t)}\|_{op}\) reaches \(\sigma^{2}w\). Then_

\[\|A_{T_{1}^{\prime}}-A_{\theta(T_{1}^{\prime})}\|_{op}\leq O(\sigma^{2}\sqrt{wd }).\]

_In particular, \(A_{\theta(T_{1}^{\prime})}\in P_{1}\cap P_{2}(C,\text{min}(\frac{\gamma_{w}-1} {8},\frac{1}{4}))\)._

Proof.: From dynamics of \(A_{t}\) we see that \(T_{1}^{\prime}\leq\frac{c}{w}\) for some constant \(c\). The dynamics of \(A_{t}\) is the following.

\[d^{2}\frac{dA_{t}}{dt}= (A^{*}-A_{t})\sqrt{A_{t}^{T}A_{t}+\sigma^{2}wI}+\sqrt{A_{t}A_{t} ^{T}+\sigma^{4}w^{2}I}(A^{*}-A_{t}).\]

The dynamics of \(A_{\theta(t)}\) can be written as follows.

\[d^{2}\frac{dA_{\theta(t)}}{dt}= (A^{*}-A_{\theta(t)})\sqrt{A_{\theta(t)}^{T}A_{\theta(t)}+\sigma^ {4}w^{2}I}+\sqrt{A_{\theta(t)}A_{\theta(t)}^{T}+\sigma^{4}w^{2}I}(A^{*}-A_{ \theta(t)})\] \[+O(\sigma^{2}\sqrt{wd})(A^{*}-A_{\theta(t)})+(A^{*}-A_{\theta(t) })O(\sigma^{2}\sqrt{wd})\]

Observe that

\[\|A_{\theta(t)}^{T}A_{\theta(t)}-A_{t}^{T}A_{t}\|_{op}\leq \|A_{\theta(t)}^{T}(A_{\theta(t)}-A_{t})\|_{op}+\|(A_{\theta(t)}-A_ {t})^{T}A_{t}\|_{op}\] \[\leq \sigma^{2}w\|A_{\theta(t)}-A_{t}\|_{op}.\]

This implies that we have the following inequality on positive definite matrices.

\[A_{t}^{T}A_{t}-\sigma^{2}w\|A_{\theta(t)}-A_{t}\|_{op}I\leq A_{\theta(t)}^{T}A_{\theta(t)}\leq A_{t}^{T}A_{t}+\sigma^{2}w\|A_{\theta(t)}-A_{t}\|_{op}I\]Assume that \(A_{t}=USV^{T}\), then

\[\|\sqrt{A_{\theta(t)}^{T}A_{\theta(t)}+\sigma^{4}w^{2}I}-\sqrt{A_{t}^ {T}A+\sigma^{4}w^{2}I}\|_{op}\] \[\leq \|\sqrt{\sigma^{4}w^{2}I+A_{t}^{T}A_{t}+\sigma^{2}w\|A_{\theta(t)} -A_{t}\|_{op}I}-\sqrt{\sigma^{4}w^{2}I+A_{t}^{T}A_{t}-\sigma^{2}w\|A_{\theta(t) }-A_{t}\|_{op}I}\|_{op}\] \[= \|\sqrt{\sigma^{4}w^{2}I+\sigma^{2}w\|A_{\theta(t)}-A_{t}\|_{op}I+ S^{2}}-\sqrt{\sigma^{4}w^{2}I-\sigma^{2}w\|A_{\theta(t)}-A_{t}\|_{op}I+S^{2}}\|_{op}\] \[= \text{max}_{i}\left(\sqrt{\sigma^{4}w^{2}+s_{i}^{2}+\sigma^{2}w \|A_{\theta(t)}-A_{t}\|_{op}}-\sqrt{\sigma^{4}w^{2}+s_{i}^{2}-\sigma^{2}w\|A_{ \theta(t)}-A_{t}\|_{op}}\right)\] \[\leq 3\|A_{\theta(t)}-A_{t}\|_{op}\]

We can control the difference between \(A_{t}\) and \(A_{\theta(t)}\).

\[d^{2}\frac{d}{dt}\|A_{t}-A_{\theta(t)}\|_{op}\leq O(d)\|A_{t}-A_{\theta(t)}\|+O(d)O(\sigma^{2}\sqrt{wd})\]

Gronwall inequality implies that

\[\|A_{T_{1}^{\prime}}-A_{\theta(T_{1}^{\prime})}\|_{op}\leq O(\sigma^{2}\sqrt{wd})\]

We check that \(A_{\theta(T_{1}^{\prime})}\) satisfies the \(P_{1}\). Since \(\|A_{T_{1}^{\prime}}-A_{\theta(T_{1}^{\prime})}\|_{op}\leq O(\sigma^{2}\sqrt{wd})\), \(\|A_{\theta(T_{1}^{\prime})}\|_{op}\geq\frac{1}{3}\sigma^{2}w\). Let \(\sigma_{j}(A_{T_{1}^{\prime}})\) and \(\sigma_{j}(A_{\theta(T_{1}^{\prime})})\) be the \(j\)-th singular value of \(A_{T_{1}^{\prime}}\) and \(A_{\theta(T_{1}^{\prime})}\). If \(a_{j}\neq a_{k}\), \(j,k\leq K\), then \(\left|\sigma_{j}(A_{T_{1}^{\prime}})-\sigma_{k}(A_{T_{1}^{\prime}})\right| \geq c\sigma^{2}w\), and therefore

\[\left|\sigma_{j}(A_{\theta(T_{1}^{\prime})})-\sigma_{k}(A_{\theta(T_{1}^{ \prime})})\right|\geq c\sigma^{2}w+O(\sigma^{2}\sqrt{wd}).\]

The noise is clearly of order \(O(\sigma^{2}\sqrt{wd})\). This completes the verification of \(P_{1}\). By lemma C.9, \(\|A_{\theta(T_{1}^{\prime})}-\Sigma\|_{op}\leq\sigma^{2}wO(\sigma^{2}w)d^{- \text{min}(\frac{\gamma_{w}-1}{3},\frac{1}{4})}\). By Lemma C.4 we are done.

### Summary of Approximate Dynamics of \(A_{\theta(t)}\) at Each Stage

In previous sections we have proved that \(A_{\theta(t)}\) satisfies \(P_{1}\) and \(P_{2}(C,\gamma)\) for some \(\gamma>0\). The \(P_{1}\) and \(P_{2}(C,\gamma)\) property actually implies that the dynamics of \(A_{\theta(t)}\) is such that each group of singular values evolve independently. We state the approximate dynamics for each stage of the dynamics, and show that the alignment will be improved, if the alignment is not already good enough.

* **Initialization**. At initialization, \(A_{\theta(0)}\) is a random matrix. The mean of each entry is \(0\) and the variance of each entry is \(\sigma^{2}\sqrt{w}\). The gap between singular values is infinitely smaller than the magnitude of singular values, and the singular vectors are not aligned with \(A^{*}\).
* **Initialization to \(\|A_{\theta(t)}\|_{op}=\sigma^{2}w\)**. Let \(T_{1}\) be the first time when \(\|A_{\theta(t)}\|_{op}\) reaches \(\sigma^{2}w\). \([0,T_{1}]\) corresponds the very short NTK regime for the signals, and the dynamics of \(A_{\theta(t)}\) is approximately linear. The evolution of signal singular values are roughly linear (i.e., bounded above and below by linear functions), and at \(T_{1}\), we have \(A_{\theta(T_{1})}\in P_{2}(C,\text{min}(\frac{\gamma_{w}-1}{8},\frac{1}{4}))\). Since the singular values grows roughly linearly, \(T_{1}=O(d)\).
* \(\|A_{\theta(t)}\|_{op}=\sigma^{2}w\)**to \(\|A_{\theta(t)}\|_{op}=\sigma^{2}w\left(\frac{w}{d}\right)^{\frac{1}{4}}\)**. Let \(T_{2}\) be the first time when \(\|A_{\theta(t)}\|_{op}=\sigma^{2}w\left(\frac{w}{d}\right)^{\frac{1}{4}}\). \([T_{1},T_{2}]\) is the early stage of saddle-to-saddle dynamics. The dynamics of \(s_{i}\) is given by \[d^{2}\frac{ds_{i}}{dt}=2(a_{i}d(1+o(1))-s_{i})\sqrt{s_{i}^{2}+\sigma^{4}w^{2}}(1 +o(1)).\] By theorem 1, we have \(\|\hat{C}_{1}-C_{1}\|_{op}\leq\sigma^{2}w\left(\frac{w}{d}\right)^{-\frac{1}{4}}\). Let \(d^{-\lambda}\sigma^{w}=\sigma^{2}w\left(\frac{w}{d}\right)^{-\frac{1}{4}}\), we have \(\lambda=\frac{\gamma_{w}-1}{4}\). By lemma 4,1, the dynamics of \(x\) satisfies \[d^{2}\frac{dx}{dt}\leq-cdx+O(dx^{\frac{3}{4}})+O(\sqrt{x}d^{1-\frac{\gamma_{w}- 1}{4}})+O(\sqrt{dx})\] with \(x(T_{1})=\frac{\gamma_{w}-1}{2}\). We conclude that \[x(T_{2})\leq O(d^{-\frac{\gamma_{w}-1}{2}}).\]* \(\|A_{\theta(t)}\|_{op}=\sigma^{2}w\left(\frac{w}{d}\right)^{\frac{1}{4}}\) to \(s_{K}(A_{\theta(t)})=\frac{1}{2}a_{K}d\). Let \(T_{3}\) be the first time \(s_{K}(A_{\theta(t)})=\frac{1}{2}a_{K}d\). At time \(T_{2}\), we have \(s_{K}\geq\sigma^{2}wd^{\delta}\) for some \(\delta>0\) that depends only on \(a_{1},\ldots,a_{K}\). Then we have \[\|\hat{C}_{1}-C_{1}\|_{op}\leq O(s_{K}d^{-\delta}),\] \[d^{2}\frac{dx}{dt}\leq-cdx+O(dx^{\frac{3}{2}})+O(\sqrt{x}d^{1-\delta})+O( \sqrt{dx}).\] We conclude that \(x(T_{3})=O(d^{-2\delta})\). The dynamics of \(s_{i}\) is given by \[d^{2}\frac{ds_{i}}{dt}=2(a_{i}d(1+O(d^{-2\delta}))-s_{i})\sqrt{s_{i}^{2}+\sigma ^{4}w^{2}}(1+O(d^{-2\delta})).\] \[s_{i}(a_{i}d-s_{i})\leq d^{2}\frac{ds_{i}}{dt}\leq 2(a_{i}d+Cd^{1-2 \delta}-s_{i})(s_{i}+\sigma^{2}w).\] The bounds on \(\frac{ds_{i}}{dt}\) implies that \[T_{3}-T_{1}=\frac{d\log\frac{a_{K}d}{\sigma^{2}w}}{a_{K}}+O(d)=\frac{1-\gamma _{\sigma^{2}}-\gamma_{w}}{a_{K}}d\log d+O(d).\]
* **Final Stage**. Let \(t\geq T_{3}\). Since \(s_{K}=O(d)\), we have \[\|\hat{C}_{1}-C_{1}\|_{op}\leq O(s_{K}\frac{\sigma^{2}w}{d})=O(s_{K}d^{\gamma _{\sigma^{2}}+\gamma_{w}-1});\] \[d^{2}\frac{dx}{dt}\leq-cdx+O(dx^{\frac{3}{2}})+O(\sqrt{x}d^{\gamma_{\sigma^{2 }}+\gamma_{w}})+O(\sqrt{dx}).\] Recall that the constant \(c\) in term \(-cdx\) must satisfy \[c\leq\frac{1}{d}\text{min}_{k,j:a_{k}\neq a_{j}}((s_{k}^{*}-s_{j}^{*})(s_{k}+s _{j})(\tilde{s}_{k}+\tilde{s}_{j}))\over s_{k}^{2}-s_{j}^{2}},\frac{(s_{k}-s_{ j})(s_{k}^{*}+s_{j}^{*})(\tilde{s}_{k}+\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}})\] As a result, we can take \[c=c(a_{1},\ldots,a_{K})=\frac{\text{min}_{k,j:a_{k}\neq a_{j}}|a_{k}-a_{j}|a_{ K}^{2}}{\text{max}_{k,j:a_{k}\neq a_{j}}|a_{k}^{2}-a_{j}^{2}|}\] Let \(c^{\prime}\) be a large constant such that if \(x>c^{\prime}(d^{-1}+d^{(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\), then \(\frac{dx}{dt}\leq-\frac{c(a_{1},\ldots,a_{K})d}{2}x\). Let \(T_{4}\) be the first time when \(x\leq c^{\prime}(d^{-1}+d^{(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\) after \(T_{3}\). Then \(T_{4}-T_{3}\leq-\frac{2d}{c(a_{1},\ldots,a_{K})}\log\bigl{(}d^{-1}+d^{2(\gamma _{\sigma^{2}}+\gamma_{w}-1)}\bigr{)}+O(d)\). Moreover, for every \(t>T_{4}\), \(x\) cannot be larger than \(c^{\prime}(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\) because \(\frac{dx}{dt}<0\) if \(x=c^{\prime}(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\). After \(T_{4}\), the dynamics of each singular value is given by \[d^{2}\frac{ds_{i}}{dt}=2((1+O(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})) a_{i}d-s_{i})s_{i}(1+O(d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)}))\] Let \(T_{5}\) be the first time after \(T_{4}\) when \(|s_{i}-a_{i}d|\leq O(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\log d\). Then \[T_{5}-T_{4}\leq-\frac{\text{max}(-1,2(\gamma_{\sigma^{2}}+\gamma_{w}-1))}{2a_{ K}}d\log d+O(d\log\log d).\] We conclude that at time \[T^{*}= \left(\frac{1-\gamma_{\sigma^{2}}-\gamma_{w}}{a_{K}}+\frac{2\text{ max}(1,2(-\gamma_{\sigma^{2}}-\gamma_{w}+1))}{c(a_{1},\ldots,a_{K})}+\frac{\text{ max}(1,2(-\gamma_{\sigma^{2}}-\gamma_{w}+1))}{2a_{K}}\right)d\log d\] \[+O(d\log\log d)\] we have \[x(T^{*})\leq O(d^{\text{max}(-1,2(\gamma_{\sigma^{2}}+\gamma_{w}-1))}),\] \[A_{\theta(T^{*})}\in P_{2}(C,\text{max}\{1,2(1-\gamma_{\sigma^{2}}-\gamma_{w})\},\] and \[|s_{i}(T^{*})-a_{i}d|\leq O(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\log d\]

### Analysis of Testing Error

In section E.3 we proved that \(A_{\theta(t)}\) is almost aligned with \(A^{*}\) throughout the training. With the alignment in hand, we are ready to give a time to stop training and the testing error at the end of training.

**Theorem E.5**.: _Assume that \(A_{\theta(t)}\) follows the gradient flow dynamics. At time_

\[T^{*}=\left(\frac{1-\gamma_{\sigma^{2}}-\gamma_{w}}{a_{K}}+\frac{2\text{max}(1,2(-\gamma_{\sigma^{2}}-\gamma_{w}+1))}{c(a_{1},\dots,a_{K})}+\frac{\text{max}( 1,2(-\gamma_{\sigma^{2}}-\gamma_{w}+1))}{2a_{K}}\right)d\log d+O(d\log\log d),\]

_the testing error_

\[\|A_{\theta(T^{*})}-A^{*}\|_{F}^{2}\leq O(\sigma^{4}wd^{2})+O(\sigma^{4}w^{2} \log^{2}d)+O(d^{\frac{3}{2}})+O(\sigma^{2}wd)\]

Proof.: Let \(P_{K}\) be the projection to the largest \(K\) singular values. Then

\[\|A_{\theta(t)}-A^{*}\|_{F}^{2}\leq\|P_{K}A_{\theta(t)}-A^{*}\|_{F}^{2}+\|(I- P_{K})A_{\theta(t)}\|_{F}^{2}.\]

Let \(s_{1},\dots,s_{d}\) be the singular values of \(A_{\theta(t)}\). Then \(\|(I-P_{K})A_{\theta(t)}\|_{F}^{2}=\sum_{p\geq K+1}s_{p}^{2}\). The derivative of \(s_{1},\dots,s_{d}\) reads

\[d^{2}\frac{dS}{dt}= I\odot\left(U^{T}(A^{*}+E)V\sqrt{S^{2}+\sigma^{4}w^{2}}+\sqrt{S^{ 2}+\sigma^{4}w^{2}}U^{T}(A^{*}+E)V-2S\sqrt{S^{2}+\sigma^{4}w^{2}}\right)\] \[+I\odot\left(U^{T}(A^{*}+E)VO(\sigma^{2}w_{1})+O(\sigma^{2}w_{1}) U^{T}(A^{*}+E)V-2SO(\sigma^{2}w_{1})\right)\]

If \(p\geq K+1\), then \(s_{p}\leq C^{\prime}\sigma^{2}w\) for some constant \(C^{\prime}\), and

\[d^{2}\bigg{|}\frac{ds_{p}}{dt}\bigg{|}\leq\big{|}(U^{T}A^{*}V)_{pp}+(U^{T}EV)_ {pp}\big{|}O(\sigma^{2}w).\]

\[d^{2}\frac{d}{dt}\sum_{p\geq K+1}s_{p}^{2}\leq \sum_{p}s_{p}\left(\sum_{q\leq K}U_{qp}V_{qp}A^{*}(qq)+(U^{T}A^{* }V)_{pp}\right)O(\sigma^{2}w)\] \[\leq \sum_{p}s_{p}\left(\sum_{q\leq K}|U_{qp}||V_{qp}|O(d)+O(\sqrt{d} )\right)O(\sigma^{2}w)\] \[\leq \sum_{q\leq K}\left(\sum_{p}s_{p}^{2}\right)^{\frac{1}{2}}\left( \sum_{p}|U_{qp}|^{2}\right)^{\frac{1}{2}}\text{max}_{p}|V_{qp}|O(\sigma^{2}wd )+\left(\sum_{p}s_{p}^{2}\right)^{\frac{1}{2}}O(\sigma^{2}wd)\] \[\leq \left(\sum_{p}s_{p}^{2}\right)^{\frac{1}{2}}\left(O(d^{1-\gamma} \sigma^{2}w)+O(\sigma^{2}wd)\right)\]

We conclude that

\[d^{2}\frac{d}{dt}\|(I-P_{K})A_{\theta(t)}\|_{F}\leq O(\sigma^{2}wd),\]

which implies that

\[\|(I-P_{K})A_{\theta(t)}\|_{F}\leq\sqrt{d}O(\sigma^{2}\sqrt{wd})+O(\sigma^{2} wd\frac{\log d}{d})=O(\sigma^{2}\sqrt{w}d+\sigma^{2}w\log d)\]

\[\|(I-P_{K})A_{\theta(t)}\|_{F}^{2}\leq O(\sigma^{4}wd^{2}+\sigma^{4}w^{2} \log^{2}d)\]

Next we estimate \(\|P_{K}A_{\theta(t)}-A^{*}\|_{F}^{2}\). Notice that \(\|P_{K}A_{\theta(t)}-A^{*}\|_{F}^{2}=\sum_{i,j}\|P_{K}A_{\theta(t)}(i,j)-A^{*}( i,j)\|_{F}^{2}\). If \(i\neq j\), then \(A^{*}(i,j)=0\), and

\[\|P_{K}A_{\theta(t)}(i,j)\|_{F}^{2}\leq \sum_{k:\text{signal},k\neq i}\|U(i,k)\|_{F}^{2}\|S(k,k)V(j,k)^{T }\|_{F}^{2}\] \[+\|U(i,j)S(j,j)\|_{F}^{2}+\|V(j,k)^{T}\|_{F}^{2}\] \[\leq O(xd^{2})\]If \(i=j=m+1\), we also have \(A^{*}(m+1,m+1)=0\), and

\[\|P_{K}A_{\theta(t)}(m+1,m+1)\|_{F}^{2}\leq\sum_{k:\text{signal}}O(d^{2})\|U(m+1,k)\|_{F}^{2}+\|V(m+1,k)\|_{F}^{2}\leq O(d^{2}x)\]

Now assume that \(i=j\) are both signals. Then

\[\|P_{K}A_{\theta(t)}(i,i)-A^{*}(i,i)\|_{F}^{2}\leq \|U(i,i)(S(i,i)-S^{*}(i,i))V^{T}(i,i)\|_{F}^{2}+O(d^{2})\|U(i,i)V(i,i)^{T}-I\|_{F}^{2}\] \[\leq O(d^{-1}+d^{2(\gamma_{\sigma^{2}}+\gamma_{w}-1)})\log d+O(d^{2} \sqrt{x})\]

Since there are only finitely many blocks in total, we conclude that at time \(T^{*}\),

\[\|A_{\theta(T^{*})}-A^{*}\|_{F}^{2}\leq O(\sigma^{4}wd^{2}+\sigma^{4}w^{2}\log^{2}d+d^{2}O(d^{ \text{max}(-\frac{1}{2},(\gamma_{\sigma^{2}}+\gamma_{w}-1))}))+O(d^{-1}+d^{2( \gamma_{\sigma^{2}}+\gamma_{w}-1)})\log d\]

\[\|A_{\theta(T^{*})}-A^{*}\|_{F}^{2}\leq O(\sigma^{4}wd^{2})+O(\sigma^{4}w^{2} \log^{2}d)+O(d^{\frac{3}{2}})+O(\sigma^{2}wd)\] (41)

## Appendix F Gradient Descent Dynamics and Proof of Theorem 2

In this section we prove that gradient descent dynamics of \(A_{\theta(t)}\) is well-approximated by gradient flow dynamics of \(A_{\theta(t)}\).

### Gradient Descent vs Gradient Flow

To study the dynamics of \(A_{t}\) under gradient flow, we show that if the learning rate is small enough, then the gradient flow dynamics will be close to the gradient descent dynamics.

**Lemma F.1**.: _Assume that \(A\) is a matrix (not necessarily square matrix). \(F\) is a function: \(\mathbb{R}^{\dim A}\rightarrow\mathbb{R}^{\dim A}\). The norm \(\|\cdot\|\) satisfies \(\|AB\|\leq\|A\|\|B\|\) for all \(A\) and \(B\). In particular, operator norm and Frobenius norm satisfies this property. Assume that \(\sup_{A}\|F(A)\|\leq C_{0}\) and \(\|\nabla F\|\leq C_{1}\) for some constant. Consider gradient flow dynamics and gradient descent dynamics._

\[\text{Gradient Flow: }\frac{dA_{f}}{dt}=F(A_{f})\]

\[\text{Gradient Descent: }\frac{A_{d}((k+1)\eta)-A_{d}(k\eta)}{\eta}=F(A_{d}(k \eta))\]

_Assume that \(A_{f}(0)=A_{d}(0)\). Then_

\[\|A_{f}-A_{d}\|(k\eta)\leq((1+\eta C_{1})^{k-1}-1)\frac{1}{2}\eta C_{0}\]

Proof.: Notice that we have

\[A_{f}((k+1)\eta)-A_{f}(k\eta)=\int_{0}^{\eta}F(A_{f}(k\eta+t))dt\]

\[(A_{f}-A_{d})((k+1)\eta)-((A_{f}-A_{d})(k\eta))=\int_{0}^{\eta}F(A_{f}(k\eta+t ))-F(A_{d}(k\eta))dt\]

\[\int_{0}^{\eta}F(A_{f}(k\eta+t))-F(A_{d}(k\eta))dt= \int_{0}^{\eta}F(A_{f}(k\eta+t))-F(A_{f}(k\eta))dt\] \[+\eta(F(A_{f}(k\eta))-F(A_{d}(k\eta)))\]

Let \(G(t)=\int_{0}^{t}F(A_{f}(k\eta+s))ds\). Then

\[\int_{0}^{\eta}F(A_{f}(k\eta+t))-F(A_{f}(k\eta))dt= G(\eta)-G(0)-\eta G^{\prime}(0)-\frac{1}{2}\eta^{2}G^{\prime\prime}(\xi)\] \[= \frac{1}{2}\eta^{2}\frac{d}{dt}|_{t=\xi}F(A_{f}(k\eta+t))\] \[= \frac{1}{2}\eta^{2}\frac{\partial F}{\partial A}(A_{f}(k\eta+\xi) )\frac{d}{dt}|_{t=\xi}A_{f}(k\eta+t)\] \[= \frac{1}{2}\eta^{2}\frac{\partial F}{\partial A}(A_{f}(k\eta+\xi) )F(A_{f}(k\eta+\xi))\]\[\|\int_{0}^{\eta}F(A_{f}(k\eta+t))-F(A_{f}(k\eta))dt\|\leq\frac{1}{2}\eta^{2}C_{0}C_ {1}\]

\(\eta\|(F(A_{f}(k\eta))-F(A_{d}(k\eta)))\|\leq\eta\|\nabla F\|\|A_{f}(k\eta)-A_{d}( k\eta)\|\leq\eta C_{1}\|A_{f}(k\eta)-A_{d}(k\eta)\|\)

We conclude that

\[\|A_{f}((k+1)\eta)-A_{d}((k+1)\eta)\|\leq (1+\eta C_{1})\|A_{f}(k\eta)-A_{d}(k\eta)\|+\frac{1}{2}\eta^{2}C_{ 0}C_{1}\]

\[\|A_{f}-A_{d}\|((k+1)\eta)+\frac{1}{2}\eta C_{0}\leq (1+\eta C_{1})(\|A_{f}-A_{d}\|(k\eta)+\frac{1}{2}\eta C_{0})\] \[\leq (1+\eta C_{1})^{k}\frac{1}{2}\eta C_{0}\] \[\|A_{f}-A_{d}\|(k\eta)\leq ((1+\eta C_{1})^{k-1}-1)\frac{1}{2}\eta C_{0}\]

To apply the lemma above we need to prove that \(\|A_{\theta(t)}\|_{F}^{2}\leq O(d^{2})\) throughout the training.

**Lemma F.2**.: _For both lazy and active regime, we always have \(\|A_{\theta(t)}\|_{F}^{2}\leq 10(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\) throughout the training._

Proof.: The gradient descent dynamics of \(A_{\theta(t)}\) is given by

\[A_{\theta(t+1)}-A_{\theta(t)}=\eta d^{-2}(A^{*}+E-A_{\theta(t)})C_{1}+C_{2}(A^ {*}+E-A_{\theta(t)}).\]

\[tr((A^{*}+E-A_{\theta(t+1)})^{T}(A^{*}+E-A_{\theta(t+1)})-(A^{*}+E-A_{\theta(t )})^{T}(A^{*}+E-A_{\theta(t)}))\]

\[= -2tr((A^{*}+E-A_{\theta(t)}))^{T}(A_{\theta(t+1)}-A_{\theta(t)}))+\|A_{ \theta(t+1)}-A_{\theta(t)}\|_{F}^{2}\] \[\leq -2\eta d^{-2}tr((A^{*}+E-A_{\theta(t)})^{T}(C_{1}+C_{2})(A^{*}+E- A_{\theta(t)}))+\eta^{2}O(d^{-2}\|C_{1}\|_{op}^{2})\]

From theorem 2, we know that \(C_{1}+C_{2}\geq\frac{1}{3}\sigma^{2}wI\). Therefore \(tr((A^{*}+E-A_{\theta(t)})^{T}(C_{1}+C_{2})(A^{*}+E-A_{\theta(t)}))\geq\|A^{* }+E-A_{\theta(t)}\|_{F}^{2}\frac{1}{3}\sigma^{2}w\geq cd^{2}\sigma^{2}w\) for some constant \(c\). Therefore for the lazy regime, we always have \(\|A^{*}+E-A_{\theta(t)}\|_{F}^{2}(t+1)\leq\|A^{*}+E-A_{\theta(t)}\|_{F}^{2}(t)\) if \(5(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\leq\|A_{\theta(t)}\|_{F}^{2}\leq 7(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\), which implies that \(\|A_{\theta(t)}\|_{F}^{2}\leq 10(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\) for all time. For the active regime, we have

\[\|A^{*}+E-A_{\theta(t)}\|_{F}^{2}(t+1)-\|A^{*}+E-A_{\theta(t)}\|_{F}^{2}(t) \leq\eta^{2}O(1).\]

Since the training has at most \(O(\frac{T^{*}}{\eta})\) steps, we see that \(\forall t\), \(\|A^{*}+E-A_{\theta(t)}\|_{F}^{2}\leq 2(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})+O(\eta T^{*}) \leq 10(\|A^{*}\|_{F}^{2}+\|E\|_{F}^{2})\). 

Proof of main theorem.: We first consider the active regime. It suffices to consider the error from considering gradient descent, rather than gradient flow. To apply lemma F.1, it is more convenient to consider the dynamics for \(W_{1}\) and \(W_{2}\). By lemma G.2, \(\|W_{1}^{T}W_{1}\|_{F}^{2}+\|W_{2}W_{2}^{T}\|_{F}^{2}\leq O(d^{2})\) and \(\|W_{1}\|_{op}+\|W_{2}\|_{op}\leq O(\sqrt{d})\). The gradient flow dynamics of \([W_{1},W_{2}^{T}]\) is given by

\[\frac{d}{dt}[W_{1},W_{2}^{T}]=d^{-2}[W_{2}^{T}(A^{*}-W_{2}W_{1}),W_{1}(A^{*}-W_ {2}W_{1})^{T}].\]

Let \(F([W_{1},W_{2}^{T}])=d^{-2}[W_{2}^{T}(A^{*}-W_{2}W_{1}),W_{1}(A^{*}-W_{2}W_{ 1})^{T}]\). Then \(\sup_{t}\|F(W_{1},W_{2}^{T})\|_{F}\leq O(d^{-\frac{1}{2}})\). Computing the differential of \(F\), we obtain that

\[dF([W_{1},W_{2}^{T}])=d^{-2}[ -dW_{2}^{T}(A^{*}-W_{2}W_{1})+W_{2}^{T}(-dW_{2}W_{1}-W_{2}dW_{1}),\] \[-dW_{1}(A^{*}-W_{2}W_{1})^{T}+W_{1}(-dW_{1}^{T}W_{2}-W_{1}^{T}dW_{ 2})]\]

and therefore \(\|\nabla F\|_{F}\leq O(d^{-1})\). In the active regime, the total number of training steps is \(\eta^{-1}O(d\log d)\) By lemma F.1,

\[\|W_{1}^{flow}-W_{1}^{descent}\|_{F}(T^{*})\leq O(\eta d^{-\frac{1}{2}})((1+\eta O (d^{-1}))^{\frac{O(d\log d)}{\eta}}-1)=O(\eta d^{-\frac{1}{2}}\log d)\]and the same holds true for \(W_{2}\). We conclude that

\[\|W_{2}^{flow}W_{1}^{flow}-W_{2}^{descent}W_{1}^{descent}\|_{F}\] \[\leq (\|W_{2}^{descent}\|_{op}+\|W_{1}^{descent}\|_{op})O(\eta d^{-\frac {1}{2}}\log d)\] \[= O(\eta\log d)\] \[\|A_{\theta}^{flow}-A_{\theta}^{descent}\|_{F}^{2}\leq O(\eta^{2} \log^{2}d).\]

In the lazy regime, from strong bound we have

\[W_{1}^{T}W_{1}=(1+O(\sqrt{\frac{d}{w}}))\sqrt{\sigma^{4}w^{2}I+A^{T}A}.\]

Therefore \(\|W_{1}\|_{op}\leq O(\sigma\sqrt{w})\), \(\sup_{t}\|F([W_{1},W_{2}^{T}])\|_{F}\leq d^{-2}(\|W_{1}\|_{op}+\|W_{2}\|_{op})O( d)=O(d^{-1}\sigma\sqrt{w})\). Similarly, \(\|\nabla F\|_{F}\leq d^{-2}O(\|A^{*}-W_{2}W_{1}\|_{F}+\|W_{2}\|_{F}\|W_{1}\|_{ F})=d^{-2}O(\sigma^{2}w)\). By lemma F.1, at time \(\frac{100d^{2}\log d}{\sigma^{2}w}\),

\[\|W_{1}^{flow}-W_{1}^{descent}\|_{F}(\frac{100d^{2}\log d}{\sigma^{2}w})\leq O (\eta d^{-1}\sigma\sqrt{w})((1+\eta O(d^{-2}\sigma^{2}w))^{\frac{100d^{2}\log d }{\sigma^{2}w}}-1)=O(\eta d^{-1}\sigma\sqrt{w}\log d).\]

We conclude that

\[\|W_{2}^{flow}W_{1}^{flow}-W_{2}^{descent}W_{1}^{descent}\|_{F}\] \[\leq (\|W_{2}^{descent}\|_{op}+\|W_{1}^{descent}\|_{op})O(\eta d^{-1} \sigma\sqrt{w}\log d)\] \[= O(\eta d^{-1}\sigma^{2}w\log d).\]

\[\|W_{2}^{flow}W_{1}^{flow}-W_{2}^{descent}W_{1}^{descent}\|_{F}\leq O(\eta^{2}d^{-2} \sigma^{2}w\log^{2}d).\]

Recall that in lemma D.1 we proved that

\[\|A_{\theta(t)}^{flow}-B_{t}\|_{F}^{2}=o(d^{2}),\]

and at time \(100\frac{d^{2}\log d}{\sigma^{2}w}\), \(\|B_{t}-A^{*}-E\|_{F}^{2}\leq d^{-50}\), which implies that \(\|A_{\theta}^{descent}-A^{*}-E\|_{F}^{2}\leq o(d^{2})\). Before this time, we have \(\|B_{t}-A^{*}\|_{F}^{2}\geq\frac{1}{3}\text{min}(\|A^{*}_{F}\|_{F}^{2},\|E\|_ {F}^{2})\). After this time, we have

\[tr((A^{*}+E-A_{\theta(t+1)}^{descent})^{T}(A^{*}+E-A_{\theta(t+1 )}^{descent})-(A^{*}+E-A_{\theta(t)}^{descent})^{T}(A^{*}+E-A_{\theta(t)}^{descent }))\] \[= -2tr((A^{*}+E-A_{\theta(t)}^{descent}))^{T}(A_{\theta(t+1)}^{descent }-A_{\theta(t)}^{descent}))+\|A_{\theta(t+1)}^{descent}-A_{\theta(t)}^{descent}\| _{F}^{2}\] \[\leq -2\eta d^{-2}tr((A^{*}+E-A_{\theta(t)}^{descent})^{T}(C_{1}+C_{2 })(A^{*}+E-A_{\theta(t)}^{descent}))\] \[+\eta^{2}tr((A^{*}+E-A_{\theta(t)}^{descent})^{T}(10\sigma^{4}w^{2} I)(A^{*}+E-A_{\theta(t)}^{descent}))\] \[\leq -(2\eta d^{-2}\sigma^{2}-10\eta^{2}\sigma^{4}w^{2})\|A^{*}+E-A_{ \theta(t)}^{descent}\|_{F}^{2}\]

Therefore \(\|A^{*}+E-A_{\theta(t)}^{descent}\|_{F}^{2}\) is decreasing and therefore \(\|A_{\theta}^{descent}-A^{*}\|_{F}^{2}\geq\frac{1}{3}\text{min}(\|A^{*}\|_{F}^ {2},\|E\|_{F}^{2})\) for all time.

## Appendix G Experimental setup

We now describe the experimental setup for the experiments shown in Figures 1 and 2. For all the experiments, we used the losses

\[\mathcal{L}_{\text{train}}(\theta)=\frac{1}{d^{2}}\left\|A_{\theta}-(A^{\star} +E)\right\|_{F}^{2};\quad\mathcal{L}_{\text{test}}(\theta)=\frac{1}{d^{2}} \left\|A_{\theta}-A^{\star}\right\|_{F}^{2}\]

where \(E\) has i.i.d. \(\mathcal{N}(0,1)\) entries, \(A^{\star}=K^{-1/2}\sum_{i=1}^{K}u_{i}v_{i}^{T}\) with \(u_{i},v_{i}\sim\mathcal{N}(0,\text{Id}_{d})\) Gaussian vectors in \(\mathbb{R}^{d}\). This means that \(\text{Rank}A^{\star}=K\). The factor \(K^{-1/2}\) ensures that \(\|A^{\star}\|_{F}=\Theta(d)\).

We then either run the self-consistent dynamics (equation (1)) or gradient descent (equation (2)). Following Theorem 2, we take a learning rate \(\eta=\frac{d^{2}}{cw\sigma^{2}}\) for \(\gamma_{\sigma^{2}}+\gamma_{2}>1,\) and \(\eta=\frac{d^{2}}{c\|A^{*}\|_{\varpi}}\) otherwise, where \(c\) is usually \(50\) but can be taken to be 2 or \(5\) for faster convergence at the cost of more unstable training.

For the experiments in Figure 1, we took \(d=500\) and \(K=5.\) For the experiments in Figure 2, we took \(d=200\) and \(K=5.\) For making the contour plot, we took a grid with \(35\) points for \(\gamma_{\sigma^{2}}\in[-3.0,0.0]\) and \(35\) points for \(\gamma_{w}\in[0,2.8].\) For each of the \(35^{2}\) pair of values for \((\gamma_{\sigma^{2}},\gamma_{w}),\) we ran gradient descent (and for the lower right plot the self-consistent dynamics too) until the train error converged. For all the runs, we took the same realizations of \(A^{\star}\) and \(E.\)

All the experiments were implemented in PyTorch [40]. Experiments took 12 hours of compute, using two GeForce RTX 2080 Ti (11GB memory) and two TITAN V (12GB memory).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contribution section accurately describes our contributions, and all theorems/propositions are proven in the main or the appendix. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations of our results and approach after we state them. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All assumptions are stated in the Theorem statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental setup is described in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We use synthetic data, with a description of how to build this synthetic data. The experiments are only there for visualization purposes, we see no particular need to publish it. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Most details are given in the experimental setup section in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [No] Justification: The numerical experiments are mostly there as a visualization of the theoretical results, our main goal is therefore clarity, which would be hurt by putting error bars everywhere. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In the experimental setup section of the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the Code of Ethics and see no issue. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper is theoretical in nature, so it has no direct societal impact that can be meaningfully discussed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not relevant to our paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We only use our own synthetic data.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release any new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not relevant to this paper. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: Not relevant to this paper.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.