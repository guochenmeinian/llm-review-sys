# Private Federated Frequency Estimation:

Adapting to the Hardness of the Instance

 Jingfeng Wu

Johns Hopkins University

uuujf@jhu.edu

&Wennan Zhu

Google Research

wennanzhu@google.com

&Peter Kairouz

Google Research

kairouz@google.com

&Vladimir Braverman

Rice University

vb21@rice.edu

Work done during an internship at Google Research

###### Abstract

In _federated frequency estimation_ (FFE), multiple clients work together to estimate the frequencies of their collective data by communicating with a server that respects the privacy constraints of Secure Summation (SecSum), a cryptographic multi-party computation protocol that ensures that the server can only access the sum of client-held vectors. For single-round FFE, it is known that _count sketching_ is nearly information-theoretically optimal for achieving the fundamental accuracy-communication trade-offs [3]. However, we show that under the more practical multi-round FEE setting, simple adaptations of count sketching are strictly sub-optimal, and we propose a novel hybrid sketching algorithm that is _provably_ more accurate. We also address the following fundamental question: _how should a practitioner set the sketch size in a way that adapts to the hardness of the underlying problem_? We propose a two-phase approach that allows for the use of a smaller sketch size for simpler problems (e.g. near-sparse or light-tailed distributions). We conclude our work by showing how differential privacy can be added to our algorithm and verifying its superior performance through extensive experiments conducted on large-scale datasets.

## 1 Introduction

In many distributed learning applications, a server seeks to compute population information about data that is distributed across multiple clients (users). For example, consider a distributed frequency estimation problem where there are \(n\) clients, each holding a local data from a domain of size \(d\), and a server that aims to estimate the frequency of the items from the \(n\) clients with the minimum communication cost. This task can be done efficiently by letting each client _binary encode_ their data and send the encoding to the server, at a local communication bandwidth cost of \(\log(d)\) bits. With the binary encoding, the server can faithfully decode _each_ local data and compute the global frequency vector (i.e., the normalized histogram vector).

However, the local data could be sensitive or private, and the clients may wish to keep it hidden from the server. The above binary encoding communication method, unfortunately, allows the server to observe each individual local data, and therefore may not satisfy the users' privacy concerns. _Federated Analytics_ (FA) [15, 14] addresses this issue by developing new methods that enable the server to learn population information about the clients while preventing the server from prying on any individual local data. In particular, a cryptographicmulti-party computation protocol, _Secure Summation_ (SecSum) [16], has become a widely adopted solution to provide data minimization guarantees for FA [16]. Specifically, SecSum sets up a communication protocol between clients and the server, which injects carefully designed additive noise to each data that cancels out when _all of the local data are summed together_, but blurs out (information theoretically) each individual local data. Under SecSum, the server is able to faithfully obtain the correct summation of the data from all clients but is unable to read a single local data. The _federated frequency estimation_ (FFE) problem refers to the distributed frequency estimation problem under the constraint of SecSum. Clearly, the binary encoding method is not compatible with SecSum, because when the binary encoding is passed to the server through SecSum, the server only gets the summation of the binary encodings of the users' data, which does not provide sufficient information for computing the global frequency vector.

A naive approach to FFE can be accomplished by employing _one-hot encoding_: each client encodes its local data into a \(d\)-dimensional one-hot vector that represents the local frequency vector and sends it to the server through SecSum. Then the server observes the summation of the local frequency vectors using SecSum and scales it by the number of clients to obtain the true frequency vector. However, this one-hot encoding approach costs \(\Theta(d\log(n))\) bits of communication per client. This is because SecSum adds noise from a finite group of size \(\Theta(\log n)\) to each component of the \(d\)-dimensional local frequency vector [16] to avoid overflows. With a linear dependence on domain size \(d\), the one-hot encoding approach is inefficient for large domain problems, which is a common setting in practice. In what follows, we focus on the regime where \(d>n\).

Recently, linear compression methods were applied to mitigate the high communication cost issue for FFE with large domains [17, 20]. The idea is to first _linearly compress_ the local frequency vector into a lower dimensional vector before sending it to the server through SecSum; as linear compression operators commute with the summation operator, the server equivalently observes a linearly compressed global frequency vector though SecSum (after rescaling by the number of clients). The server then applies standard decoding methods to approximately recover the global frequency vector from the linearly compressed one. In particular, Chen et al. [20] show that CountSketch [15] (among other sparse recovery methods) can be used as a linear compressor for the above purpose, which leads to a communication bandwidth cost of \(\mathcal{O}(n\log(d)\log(n))\) bits. Therefore when \(d>n\), CountSketch achieves a saving in local communication bandwidth compared to the one-hot encoding method that requires \(\Theta(d\log(n))\) bits. Moreover, Chen et al. [20] show that for FFE with a single communication round, an \(\Omega(n\log(d))\) local communication cost is information-theoretically _unavoidable_ for worst-case data distributions, i.e., we cannot do better without making additional assumptions on the global frequency vector.

Contributions.In this work, we make three notable extensions to CountSketch for FFE problems.

1. We show that the way Chen et al. [20] set up the sketch size (linear in the number of clients \(n\)) is often _pessimistic_ (see Corollary 2.4). In fact, in the streaming literature, the estimation error induced by CountSketch is known to adapt to the tail norm of the global frequency vector [15], which is often sub-linear in \(n\). Motivated by this, we provide an easy-to-use, two-phase approach that allows practitioners to determine the necessary sketch size by automatically adapting to the hardness of the FFE problem instance.
2. We consider FFE with multiple communication rounds, which better models practical deployments of FA where aggregating over (hundreds of) millions of clients in a single round is not possible due to device availability and limited server bandwidth. We propose a new multi-round sketch algorithm called HybridSketch that _provably_ performs better than simple adaptations of CountSketch in the multi-round setting, leading to further improvements in the communication cost. Surprisingly, we show that HybridSketch adapts to the tail norm of a _heterogeneity vector_ (see Theorem 3.2). Moreover, the tail of the heterogeneity vector is always no heavier, and could be much lighter, than that of the global frequency vector, explaining the advantage of HybridSketch. For instance, on the C4 dataset [16] with a domain size of \(d=150,868\) and \(150,000\) users, we show that our method can reduce the sketch size by \(83\%\) relative to simple sketching methods when the number of sketch rows is not very large.
3. We extend the Gaussian mechanism for CountSketch proposed by Pagh and Thorup [20], Zhao et al. [20] to the multi-round FFE setting to show how our sketching methods can be made differentially private [17]. We also characterize the trade-offs between accuracy and privacy for our proposed method.

We conclude by verifying the performance of our methods through experiments conducted on several large-scale datasets. All proofs and additional experimental results are differed to the appendices.

## 2 Adapting CountSketch to the Hardness of the Instance

In this part, we focus on single-round FFE and show how CountSketch can achieve better results when the underlying problem is simpler. Motivated by this, we also provide a two-phase method for auto-tuning the hyperparameters of CountSketch, allowing it to automatically adapt to the hardness of the instance.

**Single-Round FFE.** Consider \(n\) clients, each holding an item from a discrete domain of size \(d\). The items are denoted by \(x_{t}\in[d]\) for \(t=1,\ldots,n\). Then the frequency of item \(j\) is denoted by

\[f_{j}:=\tfrac{1}{n}\sum_{t=1}^{n}\mathds{1}\left[x_{t}=j\right].\]

We use \(\mathbf{x}_{t}\) to denote the one-hot representation of \(x_{t}\), i.e., \(\mathbf{x}_{t}=\mathbf{e}_{x_{t}}\) where \((\mathbf{e}_{t})_{t=1}^{d}\) refers to the canonical basis. Then the frequency vector can be denoted by

\[\mathbf{f}:=(f_{1},\ldots,f_{d})^{\top}=\tfrac{1}{n}\sum_{t=1}^{n}\mathbf{x}_{ t}\in[0,1]^{d}.\]

In single-round FFE, the \(n\) clients communicate with a server once under the constraint of SecSum, and aim to estimate the frequency vector \(\mathbf{f}\). Note that SecSum ensures that the server can only observe the sum of the local data.

```
0:\(n\) clients with local data \(x_{t}\in[d]\) for \(t=1,\ldots,n\). Sketch length \(L\) and width \(W\).
1: The server prepares independent hash functions and broadcasts them to each client: \[h_{\ell}:[d]\to[W],\;\sigma_{\ell}:[d]\to\{\pm 1\},\;\text{for}\;\ell\in[L].\]
2:for Client \(t=1,\ldots,n\) in parallel do
3: Client \(t\) encodes the local data \(x_{t}\in[d]\) to \(\texttt{enc}(x_{t})\in\mathbb{R}^{L\times W}\) where \[\big{(}\texttt{enc}(x_{t})\big{)}_{\ell,k}=\mathds{1}\left[h_{\ell}(x_{t})=k \right]\cdot\sigma_{\ell}(x_{t})\;\text{for}\;\ell\in[L],\;k\in[W].\]
4: Client \(t\) sends \(\texttt{enc}(x_{t})\in\mathbb{R}^{L\times W}\) to SecSum.
5:endfor
6:\(\texttt{SecSum}\) receives \(\big{(}\texttt{enc}(x_{t})\big{)}_{t=1}^{n}\) and only reveals the summation \(\sum_{t=1}^{n}\texttt{enc}(x_{t})\) to the server.
7:for Item \(j=1,\ldots,d\) in parallel do
8: Server produces \(L\) estimators for \(f_{j}\): \[\texttt{dec}(j;\ell):=\sigma_{\ell}(j)\cdot\big{(}\tfrac{1}{n}\sum_{t=1}^{n} \texttt{enc}(x_{t})\big{)}_{\ell,h_{\ell}(j)}\;\text{for}\;\ell\in[L].\]
9: Server computes the median of the \(L\) estimators: \[\texttt{dec}(j):=\texttt{median}\{\texttt{dec}(j;\ell):\;\ell\in[L]\}.\]
10:endfor
11:return\((\texttt{dec}(j))_{j=1}^{d}\) as estimate to \((f_{j})_{j=1}^{d}\). ```

**Algorithm 1** Count Sketch for Federated Frequency Estimation

**Count Sketch.** CountSketch is a classic streaming algorithm that dates back to [1]. In the literature of streaming algorithms, CountSketch has been extensively studied and is known to be able to adapt to the hardness of the problem instance. Specifically, CountSketch of a fixed size induces an estimation error adapting to the tail norm of the global frequency vector [13].

A recent work by Chen et al. [2022] apply CountSketch to single-round FFE. See Algorithm 1 for details. They show that CountSketch approximately solves single-round FFE with a communication cost of \(\mathcal{O}(n\log(d)\log(n))\) bits per client. Moreover, they show \(\Omega(n\log(d))\) bits of communication per client is unavoidable for worst-case data distributions (unless additional assumptions are made),confirming its near optimality. However, the results by Chen et al. (2022) are _pessimistic_ as they ignore the ability of CountSketch to adapt to the hardness of the problem instance. In what follows, we show how the performance of CountSketch can be improved when the underlying problem becomes simpler.

We first present a problem-dependent accuracy guarantee for CountSketch of a fixed size, \(L\times W\), that gives the sharpest bound to our knowledge. The bound is due to Minton and Price (2014) and is restated for our purpose.

**Proposition 2.1** (Restated Theorem 4.1 in Minton and Price (2014)).: _Let \((\hat{f}_{j})_{j=1}^{d}\) be estimates produced by CountSketch (see Algorithm 1). Then for each \(p\in(0,1)\), \(W\geq 2\) and \(L\geq\log(1/p)\), it holds that: for each \(j\in[d]\), with probability at least \(1-p\),_

\[|\hat{f}_{j}-f_{j}|<C\cdot\sqrt{\frac{\log(1/p)}{L}\cdot\frac{1}{W}\cdot\sum_{ i>W}(f_{i}^{*})^{2}},\]

_where \((f_{i}^{*})_{i\geq 1}\) refers to \((f_{i})_{i\geq 1}\) sorted in non-increasing order, and \(C>0\) is an absolute constant._

For the concreteness of discussion, we will focus on \(\ell_{\infty}\) as a measure of estimation error in the remainder of the paper. Our discussions can be easily extended to \(\ell_{2}\) or other types of error measures. Proposition 2.1 directly implies the following \(\ell_{\infty}\)-error bounds for CountSketch (by an application of union bound).

**Corollary 2.2** (\(\ell_{\infty}\)-error bounds for CountSketch).: _Consider Algorithm 1. Then for each \(p\in(0,1)\), \(L=\log(d/p)\) and \(W\geq 2\), it holds that: with probability at least \(1-p\),_

\[\|\mathtt{dec}(\cdot)-\mathbf{f}\|_{\infty}<C\cdot\sqrt{\frac{1}{W}\cdot\sum_ {i>W}(f_{i}^{*})^{2}},\] (1)

_where \(C>0\) is an absolute constant. In particular, (1) implies that_

\[\|\mathtt{dec}(\cdot)-\mathbf{f}\|_{\infty}<C/W.\]

According to Corollary 2.2, the estimation error is smaller when the underlying frequency vector \((f_{i}^{*})_{i\geq 1}\) has a lighter tail. In other words, CountSketch requires a smaller communication bandwidth when the global frequency vector has a lighter tail. Our next Corollary 2.3 precisely characterizes this adaptive property in terms of the required communication bandwidth. To show this, we will need the following definition on the _probable approximate correctness_ of an estimate.

**Definition 1** (\((\tau,p)\)-correctness).: An estimate \(\hat{\mathbf{f}}:=(\hat{f}_{i})_{i=1}^{d}\) of the global frequency vector \(\mathbf{f}:=(f_{i})_{i=1}^{d}\) is \((\tau,p)\)_-correct_ if

\[\mathbb{P}\Big{\{}\|\hat{\mathbf{f}}-\mathbf{f}\|_{\infty}:=\max_{i}|\hat{f}_ {i}-f_{i}|>\tau\Big{\}}<p.\]

**Corollary 2.3** (Oracle sketch size).: _Fix parameters \(\tau,p\in(0,1)\). Then for CountSketch (see Algorithm 1) to produce an \((\tau,p)\)-correct estimate, it suffices to set the sketch size to \(L=\log(d/p)\) and_

\[W=C\cdot\min\bigg{\{}\Big{(}\#\{f_{i}:f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\cdot \sum_{f_{i}<\tau}f_{i}^{2}\Big{)},\ n\bigg{\}},\] (2)

_where \(C>0\) is an absolute constant. In particular, the width \(W\) in (2) satisfies_

\[W\leq W_{\text{worst}}:=C\cdot\min\big{\{}2/\tau,\ n\big{\}}.\] (3)

Corollary 2.3 suggests that the sketch size can be set smaller if the underlying frequency vector has a lighter tail. When translated to the communication bits per client (that is \(\mathcal{O}(L\cdot W\cdot\log(n))\), where \(\log(n)\) accounts for the cost of SecSum), Corollary 2.3 implies that CountSketch requires

\[\mathcal{O}\Big{(}\min\big{\{}\#\{f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\sum_{f_{i }<\tau}f_{i}^{2},\ n\big{\}}\log(d)\log(n)\Big{)}\leq\mathcal{O}(\min\{1/\tau, n\}\log(d)\log(n))\] (4)

bits of communication per client to be \((\tau,p)\)-correct. In the worst case where \((f_{i})_{i=1}^{d}\) is \(\Theta(n)\)-sparse and \(\tau=\mathcal{O}(1/n)\), (4) nearly matches the \(\Omega(n\log(d))\) information-theoretic worst-case communication cost shown in Chen et al. (2022), ignoring the \(\log(n)\) factor from SecSum. However, in practice, \((f_{i})_{i=1}^{d}\) has a fast-decaying tail, and (4) suggests that CountSketch can use less communication to solve the problem. We provide the following examples for a better illustration of the sharp contrast between the worst and typical cases.

**Corollary 2.4** (Examples).: _Fix parameters \(\tau,p\in(0,1)\). Consider Algorithm 1 with sketch length \(L=\log(d/p)\). Then in each case for Algorithm 1 to produce an \((\tau,p)\)-correct estimate for \(\tau>1/n\):_

1. _When_ \(f_{i}\propto 2^{-i}\)_, it suffices to set_ \(W=\Theta(\log(1/\tau))\)_._
2. _When_ \(f_{i}\propto i^{-a}\) _for_ \(a>1\)_, it suffices to set_ \(W=\Theta(\tau^{-1/a})\)_._
3. _When_ \(f_{i}\propto i^{-1}\log^{-b}(i)\) _for_ \(b>1\)_, it suffices to set_ \(W=\Theta(\tau^{-1}\log^{-b}(1/\tau))\)_._
4. _When_ \(f_{i}=10/n\) _for_ \(i=1,\ldots,n/10\)_, it suffices to set_ \(W=\Theta(1/\tau)\)_._

**A Two-Phase Method for Hyperparameter Setup.** Corollary 2.3 allows to use CountSketch with a smaller width for an easier single-round FFE problem, saving communication bandwidth. However, the sketch size formula given by (2) in Corollary 2.3 relies on crucial information of the frequency \((f_{i})_{i\geq 1}\), i.e., \(\#\{f_{i}:f_{i}\geq\tau\}\) and \(\sum_{f_{i}<\tau}f_{i}^{2}\), which are unknown to who sets the sketch size. Thus, it is unclear if and how these gains can be realized in practical deployments.

We resolve this quandary by observing that in practice, the frequency vector often follows Zipf's law (Cevher, 2009; Powers, 1998). This motives us to conservatively model the global frequency vector by a polynomial with parameters. By doing so, we can first run a small CountSketch to collect data from a (randomly sampled) fraction of the clients for estimating the parameters. Then based on the estimated parameter, we can set up an appropriate sketch size for a CountSketch to solve the FFE problem. This two-phase method is formally stated as follows.

We approximate the (sorted) global frequency vector \((f_{i}^{*})_{i=1}^{d}\) by a polynomial (Cevher, 2009) with two parameters \(\alpha>0\) and \(\beta>0\), such that

\[f_{i}^{*}\approx\operatorname{poly}\left(i;\alpha,\beta\right),\ \ \operatorname{poly}\left(i;\alpha,\beta\right):=\begin{cases} \beta\cdot i^{-\alpha},&i\leq i^{*};\\ 0,&i>i^{*},\end{cases}\]

where \(i^{*}:=\max\{i:\sum_{j=1}^{i}\beta\cdot j^{-\alpha}\leq 1\}\) is set such that \(\operatorname{poly}\left(i;\alpha,\beta\right)\) is a valid frequency vector. Here's an executive summary of the proposed approach for setting the sketch size.

1. Randomly select a subset of clients (e.g., \(5,000\) out of \(10^{6}\).)
2. Fix a small sketch (e.g., \(16\times 100\)) and run Algorithm 1 with the subset of clients to obtain an estimate (\(\tilde{f_{i}}\)).
3. Use the top-\(k\) values (e.g., top \(20\)) from \(\tilde{f_{i}}\) to fit a polynomial with parameter \(\alpha\) and \(\beta\) (under squared error).
4. Solve Equation (4) under the approximation that \(f_{i}^{*}\approx\beta\cdot i^{\alpha}\) and output \(W\) according to the result.

**Experiments.** We conduct three sets of experiments to verify our methods. In the first set of experiments, we simulate a single-round FFE problem with the Gowalla dataset (Cho et al., 2011). The dataset contains \(6,442,892\) lists of location information. We first construct a domain of size \(d=175,000\), which corresponds to a grid over the US map. Then we sample \(n=d/10=17,500\) lists of the location information (that all belong to the domain created) to represent the data of \(n\) clients, uniformly at random. This way, we set up a single-round FFE problem with \(n=17,500\) clients in a domain of size \(d=175,000\). In the experiments, we fix the confidence parameter to be \(p=0.1\) and the sketch length to be \(L=\ln(2d/p)\approx 16\). The targeted \(\ell_{\infty}\)-error \(\tau\) is chosen evenly from \((10^{-3},\ 10^{-1})\). We only test \(\tau>20/n\) because it is less important to estimate frequencies over items with small counts (say, \(20\)). For CountSketch, we compute sketch width with three strategies, using (2) (called "instance optimal"), using (3) (called "minimax optimal"), and using the two-phase method. We emphasize that the "instance optimal" method is not a practical algorithm as it requires access to unknown information about the frequency; we use it only for demonstrating the correctness of our theory. We set all constant factors to be \(2\). The results are presented in Figures 1(a) and (b). We observe that the "minimax optimal" way of hyperparameter choice is in fact suboptimal in practice, and is improved by the "instance optimal" and the two-phase strategies.

In the second set of experiments, we run simulations on the "Colossal Clean Crawled Corpus" (C4) dataset (Bowman et al., 2020), which consists of clean English text scraped from the web. We treat each domain in the dataset as a user and calculate the number of examples each user has. The domain size \(d=150,868\), which is the maximum example count per user. We randomly sample \(n=150,000\) users from the dataset. We fix the sketch length to be \(L=5\). Other parameters are the same as the Gowalla dataset. The results are presented in Figures 1(d) and (e), and are consistent with what we have observed in the Gowalla simulations.

In the third set of experiments, we run simulations on a Twitter dataset Sentiment-140 (Go et al., 2009). The dataset contains \(d=739,972\) unique words from \(N=659,497\) users. We randomly sample one word from each user to construct our experiment dataset. The number of rounds \(M=10\), and in each round, \(n=N/10=65,949\) clients participate. The algorithm setup is the same as in the Gowalla experiments. Results are provided in Figures 1(g) and (h), and are consistent with our prior understandings.

## 3 Sketch Methods for Multi-Round Federated Frequency Estimation

In practice, having all clients participate in a single communication round is usually infeasible due to the large number of devices, their unpredictable availability, and limited server bandwidth (Bonawitz et al., 2019). This motivates us to consider a multi-round FFE setting.

**Multi-Round FFE.** Consider a FFE problem with \(M\) rounds of communication. In each round, \(n\) clients participate, each holding an item from a universe of size \(d\). The items are denoted by \(x_{t}^{(m)}\in[d]\), where \(t\in[n]\) denotes the client index and \(m\in[M]\) denotes the round index. For simplicity, we assume in each round a new set of clients participate. So in total there are \(N=Mn\) clients. Then the frequency of item \(j\) is now denoted by

\[f_{j}:=\frac{1}{Mn}\sum_{m=1}^{M}\sum_{t=1}^{n}\mathds{1}\left[x_{t}^{(m)}=j \right].\]

For the \(m\)-th round, the local frequency is denoted by \(f_{j}^{(m)}:=\frac{1}{n}\sum_{t=1}^{n}\mathds{1}\left[x_{t}^{(m)}=j\right].\) Clearly, we have \(f_{j}=\frac{1}{M}\sum_{m=1}^{M}f_{j}^{(m)}.\) Similarly, we use \(\mathbf{x}_{t}^{(m)}\) to denote the one-hot representation of \(x_{t}^{(m)}\), i.e.,

Figure 1: Single-round and multi-round FFE simulations. Subfigures (a) and (b) compare different hyperparameter strategies for CountSketch in a single-round FFE problem on the Gowalla dataset (Cho et al., 2011). Subfigure (c) compares three sketch methods in a multi-round FFE problem on the Gowalla dataset. Subfigures (d), (e), and (f) are counterparts of subfigures (a), (b), and (c), respectively, but on the C4 (Bowman et al., 2020) dataset. Similarly, subfigures (g), (h), and (i) are counterparts of subfigures (a), (b), and (c), respectively, but on the Sentiment-140 (Go et al., 2009) dataset.

\(\mathbf{x}_{t}^{(m)}=\mathbf{e}_{x_{t}^{(m)}}\) where \((\mathbf{e}_{t})_{t=1}^{d}\) refers to the canonical basis. Then the frequency vector can be denoted by \(\mathbf{f}:=(f_{1},\dots,f_{d})^{\top}\). The aim is to estimate the frequency vector \(\mathbf{f}\) in a manner that is compatible with SecSum.

**Baseline Method 1: Shared Sketch.** A multi-round FFE problem can be reduced to a single-round FFE problem with a large communication. Specifically, one can apply the CountSketch with the same randomness for every round; after collecting all the sketches from the \(M\) round, one simply averages them. Due to the linearity of the sketching compress method, this is equivalent to a single round setting with \(N=Mn\) clients. We refer to this method as _count sketch with shared hash design_ (SharedSketch).

Thanks to the reduction idea, we can obtain the error and sketch size bounds for SharedSketch via applying Corollaries 2.2 and 2.3 to SharedSketch by replacing \(n\) by \(N=Mn\),

**Baseline Method 2: Fresh Sketch.** A multi-round FFE problem can also be broken down to \(M\) independent single-round FFE problems. Specifically, one can apply _independent_ CountSketch in each round, and decode \(M\) local estimators for the \(M\) local frequency vectors. As the CountSketch produces an unbiased estimator, one can show that the average of the \(M\) local estimators is an unbiased estimator for the global frequency vector. We call this method _count sketch with fresh hash design_ (FreshSketch). We provide the following bound for FreshSketch. The proof of which is motivated by Huang et al. (2021).

**Theorem 3.1** (Instance-specific bound for FreshSketch).: _Let \((\hat{f}_{j})_{j=1}^{d}\) be estimates produced by FreshSketch. Then for each \(p\in(0,1)\), \(W\geq 1\) and \(L\geq\log(1/p)\), it holds that: for each \(j\in[d]\), with probability at least \(1-p\),_

\[|\hat{f}_{j}-f_{j}|<C\cdot\sqrt{\frac{\log(1/p)\log(M/p)}{L}\cdot\frac{1}{W} \cdot\sum_{i>W}(F_{i}^{*})^{2}},\]

_where \(C\) is an absolute constant, and \((F_{i}^{*})_{i=1}^{d}\) are defined as in Theorem 3.2._

**Hybrid Sketch.** Both SharedSketch and FreshSketch reduce a multi-round FFE problem into single-round FFE problem(s). In contrast, we show a more comprehensive sketching method, called _count sketch with hybrid hash design_ (HybridSketch), that solves a multi-round FFE problem as a whole. HybridSketch is presented as Algorithm 2. Specifically, HybridSketch generates \(M\) sketches that share a set of bucket hashes but use independent sets of sign hashes. Then in the \(m\)-th communication round, participating clients and the server communicate by the CountSketch algorithm based on the \(m\)-th sketch, so the server observes the summation of the sketched data through SecSum. After collecting \(M\) summations of the sketched local data, the server first computes averages over different rounds for _variance reduction_, then computes the median over different repeats

Figure 2: Shared vs. Hybrid vs. Fresh Sketches. We refer the reader to Section 3 for the definitions of the three methods. We compute the expected \(\ell_{\infty}\)-error for shared/hybrid/fresh sketches for a homogeneous, multi-round FFE problem. The domain size is \(d=10^{5}\). The number of rounds is \(M=10\). In all setups, the sketch length is fixed to \(L=5\). In every setting, the \(\ell_{\infty}\) error is averaged with \(1,000\) random repeats for simulating the expectation. In the case when the global frequency vector is a low-degree polynomial, hybrid sketch performs similarly to fresh sketch, and both are better than shared sketch. As long as the global frequency vector is a slightly higher degree polynomial (e.g., with a degree higher than 3), then hybrid sketch is significantly better than both shared and fresh sketches.

(or sketch rows) for _success probability amplification_. We provide the following problem-dependent bound for HybridSketch.

**Theorem 3.2** (Instance-specific bound for HybridSketch).: _Let \((\hat{f}_{j})_{j=1}^{d}\) be estimates produced by HybridSketch (see Algorithm 2). Define a heterogeneity vector \((F_{i})_{i=1}^{d}\) by_

\[F_{i}:=\frac{1}{M}\sqrt{\sum_{m=1}^{M}\big{(}f_{i}^{(m)}\big{)}^{2}},\quad i=1, \ldots,d.\]

_Clearly, it holds that \(F_{i}\leq f_{i}\) for every \(i\in[d]\). Let \((F_{i}^{*})_{i\geq 1}\) be \((F_{i})_{i\geq 1}\) sorted in non-increasing order. Then for each \(p\in(0,1)\), \(W\geq 1\) and \(L\geq\log(1/p)\), it holds that: for each \(j\in[d]\), with probability at least \(1-p\),_

\[|\hat{f}_{j}-f_{j}|<C\cdot\sqrt{\frac{\log(1/p)}{L}\cdot\frac{1}{W}\cdot\sum_{ i>W}(F_{i}^{*})^{2}},\]

_where \(C\) is an absolute constant._

We would like to point out that, although our HybridSketch algorithm is developed for multi-round frequency estimation problems, it can be adapted to multi-round vector recovery problems as well. Hence it could have broader applications in other federated learning scenarios.

**Hybrid Sketch vs. Fresh Sketch.** By comparing Theorem 3.2 with Theorem 3.1, we see that, with the same sketch size, the estimation error of HybridSketch is smaller than that of FreshSketch by a factor of \(\sqrt{\log(M/p)}\). This provides theoretical insights that HybridSketch is superior to FreshSketch in terms of adapting to the instance hardness in multi-round FFE settings. This is also verified empirically by Figure 2.

Hybrid Sketch vs. Shared Sketch.** We now compare the performance of HybridSketch and SharedSketch by comparing Theorem 3.2 and Proposition 2.1 (under a revision of replacing \(n\) with \(N=Mn\)). Note that

\[F_{i}=\frac{1}{M}\sqrt{\sum_{m=1}^{M}\left(f_{i}^{(m)}\right)^{2}}\leq\frac{1}{M }\sum_{m=1}^{M}f_{i}^{(m)}=f_{i}.\]

So with the same sketch size, HybridSketch achieves an error that is no worse than \(\csc\) in every case. Moreover, in the _homogeneous_ case where all local frequency vectors are equivalent to the global frequency vector, i.e., \(\mathbf{f}^{(m)}\equiv\mathbf{f}\) for all \(m\), then it holds that \(F_{i}=f_{i}/\sqrt{M}\). So in the homogeneous case, HybridSketch achieves an error that is smaller than that of \(\csc\) by a factor of \(1/\sqrt{M}\). In the general cases, the local frequency vectors are not perfectly homogeneous, then the improvement of HybridSketch over SharedSketch will depend on the _heterogeneity_ of these local frequency vectors.

Experiments.** We conduct three sets of experiments to verify our understandings about these sketches methods for multi-round FFE.

In the first sets of experiments, we simulate a multi-round FFE problem in homogeneous settings, where in every round the local frequency vectors are exactly the same. More specially, we set a domain size \(d=10^{5}\), a number of rounds \(M=10\) and test three different cases, where all the local frequency vectors are the same and (hence also the global frequency vector) are proportional to \((i^{-1.1})_{i=1}^{d}\), \((i^{-2})_{i=1}^{d}\) and \((i^{-5})_{i=1}^{d}\), respectively. In all the settings, we fix the sketch length to \(L=5\). In each experiment, we measure the expected \(\ell_{\infty}\)-error of each method with the averaging over \(1,000\) independent repeats. The results are plotted in Figure 2. We can observe that: for low-degree polynomials, HybridSketch is nearly as good as FreshSketch and both are better than SharedSketch. But for slightly high degree polynomials (with a degree of \(3\)), HybridSketch already outperforms both FreshSketch and SharedSketch. The numerical results are consistent with our theoretical analysis.

In the second sets of experiments, we simulate a multi-round FFE problem with the Gowalla dataset (Cho et al., 2011). Similar to previously, we construct a domain of size \(d=175,000\), which corresponds to a grid over the US map. Then we sample \(N=d=175,000\) lists of the location information (that all belong to the domain created) to represent the data of \(N\) clients, uniformly at random. We set the number of rounds to be \(M=10\). In each round, \(n=N/M=17,500\) clients participate. The results are presented in Figure 1(c). Here, the frequency and heterogeneity vectors have heavy tails, so HybridSketch and FreshSketch perform similarly and both are better than SharedSketch. This is consistent with our theoretical understanding.

In the third sets of experiments, we run simulations on the C4 (Bowman et al., 2020) dataset. Similar to the single round simulation, the domain size \(d=150,868\). We randomly sample \(N=150,000\) users from the dataset. The number of rounds \(M=10\), and in each round, \(n=N/10=15,000\) clients participate. The results are provided in Figures 1(f) and 3. Here, the frequency and heterogeneity vectors have moderately light tails, and Figure 3 already suggests that HybridSketch produces an estimate that has a better shape than that produced by FreshSketch and SharedSketch, verifying the advantages of HybridSketch.

## 4 Differentially Private Sketches

While SecSum provides security guarantees, it does not provide differential privacy guarantees. In this part, we discuss a simple modifications to the sketching algorithms to make them provably differentially private (DP).

Figure 3: The number of items with error greater than 0.1/width for Shared, Hybrid, and Fresh Sketches with C4 dataset. HybridSketch with a width of 200 achieves roughly the same error as SharedSketch with a width of 1200 and Fresh sketch with a width of 600.

**Definition 2** (\((\epsilon,\delta)\)-DP (Dwork et al., 2006)).: Let \(\mathtt{alg}(\cdot)\) be a randomized algorithm that takes a dataset \(\mathcal{D}\) as its input. Let \(\mathbb{P}\) be its probability measure. \(\mathtt{alg}(\cdot)\) is \((\epsilon,\delta)\)-DP if: for every pair of neighboring datasets \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\), it holds that

\[\mathbb{P}\{\mathtt{alg}(\mathcal{D})\in\mathcal{E}\}<e^{\epsilon}\cdot \mathbb{P}\{\mathtt{alg}(\mathcal{D}^{\prime})\in\mathcal{E}\}+\delta.\]

In our case, a dataset corresponds to all participated clients (or their data), and two neighboring datasets should be regarded as two sets of clients (local data) that only differ in a single client (local data). The algorithm refers to all procedures before releasing the final frequency estimate, and all the intermediate computation is considered private and is not released.

We work with _central DP_, that is, server releases data in a differentially private way while clients do not release data. We focus on HybridSketch as a representative algorithm. The DP mechanism can also be extended to the other sketching algorithms. Specifically, we use a DP mechanism that adds independent Gaussian noise to each entry of the sketching matrix, which is initially proposed for making CountSketch differentially private by Pagh and Thorup (2022); Zhao et al. (2022).

We provide the following theorem characterizing the trade-off between privacy and accuracy.

**Theorem 4.1** (DP-hybrid sketch).: _Consider a modified Algorithm 2, where we add to each entry of the sketching matrix an independent Gaussian noise, \(\mathcal{N}(0,c_{0}\cdot\sqrt{L\log(1/\delta)}/\epsilon)\), where \(c_{0}>0\) is a known constant. Suppose that \(L=\log(d/p)\) and \(W\geq 2\). Then the final output of the modified Algorithm 2, denoted by \((\hat{f}_{j})_{j=1}^{d}\), is \((\epsilon,\delta)\)-DP for \(\epsilon<1\) and \(\delta<0.1\). Moreover, with probability at most \(1-p\), it holds that_

\[\max_{j}|\hat{f}_{j}-f_{j}|<C\cdot\bigg{(}\sqrt{\frac{\sum_{i>W}(F_{i}^{*})^{2 }}{W}}+\frac{\sqrt{\log(d/p)\log(1/\delta)}}{n\sqrt{M}\epsilon}\bigg{)},\]

_where \(C>0\) is an absolute constant and \((F_{i}^{*})_{i=1}^{d}\) are as defined in Theorem 3.2._

It is worth noting that if the number of clients per round (\(n\)) is fixed, then a larger number of rounds \(M\) improves both the estimation error and the DP error in non-worst cases, e.g., when the local frequency vectors are nearly homogenous. However, if the total number of clients (\(N=Mn\)) is fixed, then a larger number of rounds \(M\) improves the estimation error but makes the DP error worse.

When \(M=1\), Theorem 4.1 recovers the bounds for differentially private CountSketch in Pagh and Thorup (2022); Zhao et al. (2022) and Theorem 5.1 in Chen et al. (2022). Moreover, Chen et al. (2022) shows that in single-round FFE, for any algorithm that achieves an \(\ell_{\infty}\)-error smaller than \(\tau:=\mathcal{O}(\sqrt{\log(d)\log(1/\delta)}/(n\epsilon))\), in the worse case, each client must communicate \(\Omega(n\cdot\min\{\sqrt{\log(d)/\log(1/\delta)},\log(d))\}\) bits (see Their Corollary 5.1). In comparison, According to Theorem 4.1 and Corollary 2.3, the differentially private CountSketch can achieve an \(\ell_{\infty}\)-error smaller than \(\tau\) with length \(L\asymp\log(d)\) and width

\[W=C\cdot\min\bigg{\{}\Big{(}\#\{f_{i}:f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\cdot \sum_{f_{i}<\tau}f_{i}^{2}\Big{)},\ n\bigg{\}}\leq C\cdot\min\{2/\tau,n\},\]

resulting in a per-client communication of \(\mathcal{O}(WL\log(n))\) bits, which matches the minimax lower bound in Chen et al. (2022) ignoring a \(\log(n)\) factor, but could be much smaller in non-worst cases where \((f_{i})_{i=1}^{d}\) decays fast.

## 5 Concluding Remarks

We make several novel extensions to the count sketch method for federated frequency estimation with one or more communication rounds. In the single round setting, we show that count sketch can achieve better communication efficiency when the underlying problem is simpler. We provide a two-phase approach to automatically select a sketch size that adapts to the hardness of the problem. In the multiple rounds setting, we show a new sketching method that provably achieves better accuracy than simple adaptions of count sketch. Finally, we adapt the Gaussian mechanism to make the hybrid sketching method differentially private.

We remark that the improvement of the instance-dependent method relies on the assumptions that the underlying frequency has a lighter tail, which might be unverifiable a priori due to constraints, e.g., limited communication and privacy budget. Finally, this work focuses on an offline setting where the frequency is considered to be fixed. Extending our results to an online setting where the frequency is varying is an interesting future direction.

## Acknolwdgement

We thank the anonymous reviewers for their helpful comments. We thank Brendan McMahan for insightful discussions during the project. VB has been partially supported by National Science Foundation Awards 2244899 and 2333887 and the ONR award N000142312737.

## References

* Bonawitz et al. (2016) Kallista Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. _arXiv preprint arXiv:1611.04482_, 2016.
* Bonawitz et al. (2019) Kallista Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In _MLSys_. mlsys.org, 2019.
* Bonawitz et al. (2021) Kallista Bonawitz, Peter Kairouz, Brendan McMahan, and Daniel Ramage. Federated learning and privacy: Building privacy-preserving systems for machine learning and data science on decentralized data. _Queue_, 19(5):87-114, 2021.
* Bowman et al. (2020) Samuel R. Bowman, Gabriel Angeli, Siddharth Jain, Jared Kaplan, Prafulla Dhariwal, Saurabh Neelakantan, Jonathon Shlens, and Dario Amodei. C4: Colossal clean crawled corpus. _arXiv preprint arXiv:2005.14165_, 2020.
* Cevher (2009) Volkan Cevher. Learning with compressible priors. _Advances in Neural Information Processing Systems_, 22, 2009.
* Charikar et al. (2002) Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In _International Colloquium on Automata, Languages, and Programming_, pages 693-703. Springer, 2002.
* Chen et al. (2021) Wei-Ning Chen, Christopher A Choquette-Choo, and Peter Kairouz. Communication efficient federated learning with secure aggregation and differential privacy. In _NeurIPS 2021 Workshop Privacy in Machine Learning_, 2021.
* Chen et al. (2022) Wei-Ning Chen, Ayfer Ozgur, Graham Cormode, and Akash Bharadwaj. The communication cost of security and privacy in federated frequency estimation. _arXiv preprint arXiv:2211.10041_, 2022.
* Cho et al. (2011) Eunjoon Cho, Seth A Myers, and Jure Leskovec. Friendship and mobility: user movement in location-based social networks. In _Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 1082-1090, 2011.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* Go et al. (2009) Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. _CS224N Project Report, Stanford_, 1(12), 2009.
* Huang et al. (2021) Ziyue Huang, Yuan Qiu, Ke Yi, and Graham Cormode. Frequency estimation under multiparty differential privacy: One-shot and streaming. _arXiv preprint arXiv:2104.01808_, 2021.
* Minton and Price (2014) Gregory T Minton and Eric Price. Improved concentration bounds for count-sketch. In _Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms_, pages 669-686. SIAM, 2014.
* Pagh and Thorup (2022) Rasmus Pagh and Mikkel Thorup. Improved utility analysis of private countsketch. _arXiv preprint arXiv:2205.08397_, 2022.
* Powers (1998) David M. W. Powers. Applications and explanations of Zipf's law. In _New Methods in Language Processing and Computational Natural Language Learning_, 1998. URL https://aclanthology.org/W98-1218.
* Pagh and Thorup (2015)Daniek Ramage and Stefano Mazzocchi. Federated analytics: Collaborative data science without data collection. https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html, 2020.
* Zhao et al. [2022] Fuheng Zhao, Dan Qiao, Rachel Redberg, Divyakant Agrawal, Amr El Abbadi, and Yu-Xiang Wang. Differentially private linear sketches: Efficient implementations and applications. _arXiv preprint arXiv:2205.09873_, 2022.
* Zhu et al. [2020] Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. Federated heavy hitters discovery with differential privacy. In _International Conference on Artificial Intelligence and Statistics_, pages 3837-3847. PMLR, 2020.

Missing Proofs for Section 2

### Proof of Proposition 2.1

Proof of Proposition 2.1.: We refer the reader to Theorem 4.1 in Minton and Price (2014). 

### Proof of Corollary 2.2

Proof of Corollary 2.2.: From Proposition 1 we know that

\[\text{for every }j\in[d],\quad\mathbb{P}\bigg{\{}|\texttt{dec}(j)-f_{j}|>C \cdot\sqrt{\frac{\log(1/\delta)}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}(f_{i}^{*})^ {2}}\bigg{\}}<\delta.\]

By union bound we have

\[\mathbb{P}\bigg{\{}\text{there exists }j\in[d],\ |\texttt{dec}(j)-f_{j}|>C \cdot\sqrt{\frac{\log(1/\delta)}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}(f_{i}^{*})^ {2}}\bigg{\}}<d\delta.\]

Replacing \(\delta\) with \(\delta/d\), setting \(L=\log(d/\delta)\), and using the definition of \(\ell_{\infty}\)-norm, we obtain

\[\mathbb{P}\bigg{\{}\|\texttt{dec}(\cdot)-\textbf{f}\|_{\infty}>C\cdot\sqrt{ \frac{1}{W}\cdot\sum_{i>W}(f_{i}^{*})^{2}}\bigg{\}}<\delta.\]

We next show that:

\[\sqrt{\frac{1}{W}\cdot\sum_{i>W}(f_{i}^{*})^{2}}\leq\frac{1}{W}.\]

To this end, we first show that \(f_{W}^{*}\leq\frac{1}{W}\). If not, we must have for \(i=1,\dots,W,\ f_{i}^{*}\geq f_{W}^{*}>\frac{1}{W}\), as \((f_{i}^{*})_{i=1}^{d}\) is sorted in non-increasing order. Then \(\sum_{i}^{d}f_{i}^{*}\geq\sum_{i=1}^{W}f_{i}^{*}>1\), which contradicts to the fact that \((f_{i}^{*})_{i=1}^{d}\) is a frequency vector. We have shown that \(f_{W}^{*}\leq\frac{1}{W}\), and this further implies that for any \(i\geq W\), \(f_{i}^{*}\leq f_{W}^{*}\leq\frac{1}{W}\). Then we can obtain

\[\sqrt{\frac{1}{W}\cdot\sum_{i>W}(f_{i}^{*})^{2}}\leq\sqrt{\frac{1}{W^{2}}\cdot \sum_{i>W}f_{i}^{*}}\leq\frac{1}{W},\]

since \((f_{i}^{*})_{i=1}^{d}\) is a frequency vector. We have completed all the proof. 

### Proof of Corollary 2.3

Proof of Corollary 2.3.: Define

\[E(W):=\sqrt{\frac{1}{W}\sum_{i>W}(f_{i}^{*})^{2}}.\]

We will show the following:

1. If \(W\geq\#\{f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\sum_{f_{i}<\tau}f_{i}^{2}\), then \(E(W)\leq\tau\).
2. Moreover, if \(E(W)\leq\tau\), then \(W\geq\frac{1}{2}\big{(}\#\{f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\sum_{f_{i}<\tau }f_{i}^{2}\big{)}\).

Then Corollary 2.3 follows by combining Corollary 2.2 with the above claims.

We first show the first part. First note that \(W\geq\#\{f_{i}\geq\tau\}\) and that \((f_{i}^{*})_{i=1}^{d}\) is sorted in non-increasing order, so for all \(i\geq W\) it holds that \(f_{i}^{*}<\tau\). Therefore,

\[E(W):=\sqrt{\frac{1}{W}\sum_{i>W}(f_{i}^{*})^{2}}\leq\sqrt{\frac{1}{W}\sum_{f _{i}<\tau}f_{i}^{2}}.\]

Moreover, note that \(W\geq\frac{1}{\tau^{2}}\sum_{f_{i}<\tau}f_{i}^{2}\), so we further have \(E(W)\leq\tau\).

To show that second part, we first note that, by definition, \(E(W)\leq\tau\) is equivalent to

\[2W\geq W+\frac{1}{\tau^{2}}\sum_{i>W}(f_{i}^{*})^{2}.\]

Consider the following function

\[F(k):=k+\frac{1}{\tau^{2}}\sum_{i>k}(f_{i}^{*})^{2},\quad k\geq 1,\]

one can directly verify that \(F(k)\) is minimized at \(k^{*}:=\#\{i:f_{i}\geq\tau\}\); moreover,

\[F(k^{*})=k^{*}+\frac{1}{\tau^{2}}\sum_{i>k^{*}}(f_{i}^{*})^{2}=\#\{f_{i}\geq \tau\}+\frac{1}{\tau^{2}}\sum_{f_{i}<\tau}f_{i}^{2}.\]

Therefore, we have

\[2W\geq F(W)\geq F(k^{*})=\#\{f_{i}\geq\tau\}+\frac{1}{\tau^{2}}\sum_{f_{i}< \tau}f_{i}^{2}.\]

This completes our proof. 

## Appendix B Missing Proofs for Section 3

### Proof of Theorem 3.1

Proof of Theorem 3.1.: The proof is motivated by Huang et al. (2021).

Define the following events

\[E_{j}^{(m)}:=\bigg{\{}|\hat{f}_{j}^{(m)}-f_{j}^{(m)}|\leq C\cdot\sqrt{\frac{ \log(1/p)}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}\big{(}f_{i}^{(m)}\big{)}^{2}} \bigg{\}},\ m\in[M],j\in[d].\]

Then by Proposition 2.1 we have

\[\mathbb{P}\big{\{}E_{j}^{(m)}\big{\}}\geq 1-p.\]

Then by union bound, we have

\[\mathbb{P}\bigg{\{}\bigcap_{m=1}^{M}E_{j}^{(m)}\bigg{\}}\geq 1-Mp.\]

Conditional on the event of \(\bigcap_{m=1}^{M}E_{j}^{(m)}\), we know that every random variable \(\hat{f}_{j}^{(m)}-f_{j}^{(m)}\) is bounded within

\[\big{(}-F^{(m)},\ F^{(m)}\big{)},\]

where

\[F^{(m)}:=C\cdot\sqrt{\frac{\log(1/p)}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}\big{(} f_{i}^{(m)}\big{)}^{2}}.\]

So by Hoeffding inequality, we have

\[\mathbb{P}\Bigg{\{}\bigg{|}\frac{1}{M}\sum_{m=1}^{M}\hat{f}_{j}^{(m)}-\frac{1 }{M}\sum_{m=1}^{M}f_{j}^{(m)}\bigg{|}\leq\sqrt{\frac{\log(2/p_{1})}{2M^{2}} \sum_{m=1}^{M}\big{(}F^{(m)}\big{)}^{2}}\bigg{|}\ \bigcap_{m=1}^{M}E_{j}^{(m)}\Bigg{\}}\geq 1-p_{1}\]

Then we have

\[\mathbb{P}\Bigg{\{}\bigg{|}\frac{1}{M}\sum_{m=1}^{M}\hat{f}_{j}^{(m)}-\frac{1 }{M}\sum_{m=1}^{M}f_{j}^{(m)}\bigg{|}\leq\sqrt{\frac{\log(2/p_{1})}{2M^{2}} \sum_{m=1}^{M}\big{(}F^{(m)}\big{)}^{2}}\Bigg{\}}\geq 1-p_{1}-Mp.\]Note that

\[\frac{\log(2/p_{1})}{2M^{2}}\sum_{m=1}^{M}\left(F^{(m)}\right)^{2} =\frac{\log(2/p_{1})}{2M^{2}}\sum_{m=1}^{M}C^{2}\cdot\frac{\log(1/p )}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}\left(f_{i}^{(m)}\right)^{2}\] \[=C^{2}\cdot\frac{\log(2/p_{1})\log(1/p)}{2L}\cdot\frac{1}{W}\cdot \sum_{i>W}\left(F_{i}\right)^{2}\]

So we have

\[\mathbb{P}\Bigg{\{}\bigg{|}\frac{1}{M}\sum_{m=1}^{M}\hat{f}_{j}^{( m)}-\frac{1}{M}\sum_{m=1}^{M}f_{j}^{(m)}\bigg{|}\leq\sqrt{C^{2}\cdot\frac{\log(2/p_{1 })\log(1/p)}{2L}\cdot\frac{1}{W}\cdot\sum_{i>W}\left(F_{i}\right)^{2}}\Bigg{\}}\] \[\geq 1-p_{1}-Mp.\]

Note replace \(p_{1}=p^{\prime}/2\) and \(p=p^{\prime}/(2M)\), we have that

\[\mathbb{P}\Bigg{\{}\bigg{|}\frac{1}{M}\sum_{m=1}^{M}\hat{f}_{j}^{(m)}-\frac{1 }{M}\sum_{m=1}^{M}f_{j}^{(m)}\bigg{|}\leq\sqrt{C^{\prime}\cdot\frac{\log(1/p^ {\prime})\log(M/p^{\prime})}{L}\cdot\frac{1}{W}\cdot\sum_{i>W}\left(F_{i} \right)^{2}}\Bigg{\}}\geq 1-p^{\prime}.\]

### Proof of Theorem 3.2

Proof of Theorem 3.2.: Let us consider the hybrid sketch approach in Algorithm 2. Recall that within a round, clients use the same set of hash functions to construct their sketching matrices. Across different rounds, clients use the same set of location hashes but a fresh set of sign hashes. Denote the hash functions by:

\[h_{\ell} :[d]\rightarrow[w],\quad\ell=1,\ldots,L;\] \[\sigma_{\ell}^{(m)} :[d]\rightarrow\{+1,-1\},\quad\ell=1,\ldots,L;\ m=1\ldots,M.\]

Recall the local frequency in each round is defined by

\[\mathbf{f}^{(m)}:=\frac{1}{n}\sum_{t=1}^{n}\mathbf{x}^{(m,\ell)},\quad m=1, \ldots,M.\]

And the global frequency vector is defined by

\[\mathbf{f}:=\frac{1}{M}\sum_{m=1}^{M}\mathbf{f}^{(m)}.\]

Then according to the communication protocol, the server receives \(M\) sketching matrices (each corresponds to a summation of clients' sketches within the same round). From the \(m\)-th sketch, we can extract \(L\) estimators for each index \(j\in[d]\), i.e.,

\[\tilde{\mathbf{f}}_{j}^{(m,\ell)} :=\sum_{i=1}^{d}\mathds{1}\left[h_{\ell}(i)=h_{\ell}(j)\right] \cdot\sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{f}_{i}^{(m )},\qquad j\in[d],\ m\in[M],\ \ell\in[L]\] \[=\mathbf{f}_{j}^{(m)}+\sum_{i\neq j}\mathds{1}\left[h_{\ell}(i)=h _{\ell}(j)\right]\cdot\sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot \mathbf{f}_{i}^{(m)}.\]

For each index, we will first average the estimators from different rounds to reduce the variance, then take the median over different rows to amplify the success probability. In particular, denote the round-wise averaging by

\[\tilde{\mathbf{f}}_{j}^{(\ell)} :=\frac{1}{M}\sum_{m=1}^{M}\tilde{\mathbf{f}}_{j}^{(m,\ell)}, \qquad j\in[d],\ \ell\in[L]\] \[=\frac{1}{M}\sum_{m=1}^{M}\mathbf{f}_{j}^{(m)}+\frac{1}{M}\sum_{m =1}^{M}\sum_{i\neq j}\mathds{1}\left[h_{\ell}(i)=h_{\ell}(j)\right]\cdot\sigma _{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{f}_{i}^{(m)}\]\[=\underbrace{\mathbf{f}_{j}}_{\text{signal}}+\underbrace{\frac{1}{M} \sum_{i\neq j}\mathds{1}\left[h_{\ell}(i)=h_{\ell}(j)\right]\cdot\sum_{m=1}^{M} \sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{f}_{i}^{(m)}}_{ \text{noise}}\] \[=\underbrace{\mathbf{f}_{j}}_{\text{signal}}+\underbrace{\frac{1} {M}\sum_{i\neq j,i\in\mathbb{W}}\mathds{1}\left[h_{\ell}(i)=h_{\ell}(j)\right] \cdot\sum_{m=1}^{M}\sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot \mathbf{f}_{i}^{(m)}}_{\text{headNoise}}\] \[\qquad\qquad+\underbrace{\frac{1}{M}\sum_{i\neq j,i\notin \mathbb{W}}\mathds{1}\left[h_{\ell}(i)=h_{\ell}(j)\right]\cdot\sum_{m=1}^{M} \sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{f}_{i}^{(m)}}_{ \text{tailNoise}}.\] (5)

Then we take the median over these estimators to obtain

\[\tilde{\mathbf{f}}_{j}:=\texttt{median}\{\tilde{\mathbf{f}}_{j}^{(\ell)},\ \ell\in[L]\},\quad j\in[d].\]

Head Noise.The only randomness comes from the algorithm. Note that the head noise contains at most \(|\mathbb{W}|\leq 0.1W\) independent terms, and each is zero with probability \(1-1/W\). Thus the head noise is zero with probability at least \((1-1/W)^{|\mathbb{W}|}\geq(1-1/W)^{0.1W}\geq 0.9\) provided that \(W>10\).

Tail Noise.Now consider the second noise term in (5). Fixing \(\ell\) and \(j\). Define

\[\xi_{i}^{(m)} :=\sigma_{\ell}^{(m)}(j)\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{ f}_{i}^{(m)}\] \[\xi_{i} :=\sum_{m=1}^{M}\xi_{i}^{(m)}=\sum_{m=1}^{M}\sigma_{\ell}^{(m)}(j )\cdot\sigma_{\ell}^{(m)}(i)\cdot\mathbf{f}_{i}^{(m)}\] \[\eta_{i} :=\mathds{1}\left[h(i)=h(j)\right]\] \[\texttt{tailNoise} :=\frac{1}{M}\sum_{i\neq j,i\notin\mathbb{W}}\eta_{i}\cdot\xi_{i}.\]

First notice that \(\left(\xi_{i}^{(m)}\right)_{m=1}^{M}\) are independent random variables and

\[\mathbb{E}[\xi_{i}^{(m)}]=0,\quad\mathrm{Var}[\xi_{i}^{(m)}]=\left(\mathbf{f} _{i}^{(m)}\right)^{2}.\]

These imply that

\[\mathbb{E}[\xi_{i}]=0,\quad\mathrm{Var}[\xi_{i}]=\sum_{m=1}^{M} \left(\mathbf{f}_{i}^{(m)}\right)^{2}.\]

Moreover, notice that \((\eta_{i},\xi_{i})_{i\neq j}\) are independent random variables, and

\[\mathbb{E}[\eta_{i}^{2}]=\frac{1}{W},\]

we then have

\[\mathbb{E}[\eta_{i}\xi_{i}] =0;\] \[\mathrm{Var}[\eta_{i}\xi_{i}] =\mathbb{E}[\eta_{i}^{2}]\cdot\mathrm{Var}[\xi_{i}]+\mathrm{Var}[ \eta_{i}]\cdot\left(\mathbb{E}[\xi_{i}]\right)^{2}\] \[=\frac{1}{W}\cdot\sum_{m=1}^{M}\left(\mathbf{f}_{i}^{(m)}\right) ^{2}.\]

Therefore we conclude that

\[\mathbb{E}[\texttt{tailNoise}] =\frac{1}{M}\sum_{i\neq j,i\notin\mathbb{W}}\mathbb{E}[\eta_{i} \xi_{i}]=0;\] \[\mathrm{Var}[\texttt{tailNoise}] =\frac{1}{M^{2}}\sum_{i\neq j,i\in\mathbb{W}}\mathrm{Var}[\eta_{i }\xi_{i}]\]\[=\frac{1}{M^{2}W}\cdot\sum_{i\neq j,i\neq\mathbb{W}}\sum_{m=1}^{M} \big{(}\mathbf{f}_{i}^{(m)}\big{)}^{2}\] \[\leq\frac{1}{M^{2}W}\cdot\sum_{i\notin\mathbb{W}}\sum_{m=1}^{M} \big{(}\mathbf{f}_{i}^{(m)}\big{)}^{2}.\]

Then by Chebyshev we see that: for fixed \(j\in[d]\) and \(\ell\in[L]\) it holds that

\[\mathbb{P}\bigg{\{}|\mathtt{tailNoise}|\geq\sqrt{\frac{10}{M^{2}W}\cdot\sum_ {i\notin\mathbb{W}}\sum_{m=1}^{M}\big{(}\mathbf{f}_{i}^{(m)}\big{)}^{2}}\bigg{\}} <0.1.\]

By a union bound we see that: for fixed \(j\in[d]\) and \(\ell\in[L]\) it holds that

\[\mathbb{P}\bigg{\{}|\tilde{\mathbf{f}}_{j}^{(\ell)}-\mathbf{f}_{j}|<\sqrt{ \frac{10}{M^{2}W}\cdot\sum_{i\notin\mathbb{W}}\sum_{m=1}^{M}\big{(}\mathbf{f} _{i}^{(m)}\big{)}^{2}}\bigg{\}}>0.8>0.5.\]

Probability Amplification.Fixing \(j\). Recall that \((\tilde{\mathbf{f}}_{j}^{(\ell)})_{\ell=1}^{L}\) are i.i.d. random variables and that \(\tilde{\mathbf{f}}_{j}:=\mathtt{median}\{\tilde{\mathbf{f}}_{j}^{(\ell)}: \ell\in[L]\}\). By Chernoff over \(\ell\) and union bound over \(j\) we see that:

\[\mathbb{P}\bigg{\{}\text{for each }j\in[d],\quad|\tilde{\mathbf{f}}_{j}- \mathbf{f}_{j}|\geq\sqrt{\frac{10}{M^{2}W}\cdot\sum_{i\notin\mathbb{W}}\sum_{m =1}^{M}\big{(}\mathbf{f}_{i}^{(m)}\big{)}^{2}}\bigg{\}}<2d\cdot\exp(\Omega(L)).\]

By choosing \(L=\Theta(\log(2d/\delta))\) we obtain that, with probability at least \(1-\delta\),

\[\text{for each }j\in[d],\quad|\tilde{\mathbf{f}}_{j}-\mathbf{f}_{j}|\lesssim \sqrt{\frac{10}{M^{2}W}\cdot\sum_{i\notin\mathbb{W}}\sum_{m=1}^{M}\big{(} \mathbf{f}_{i}^{(m)}\big{)}^{2}}.\]

## Appendix C Missing Proofs for Section 4

### Proof of Theorem 4.1

Proof of Theorem 4.1.: We follow the method of Pagh and Thorup (2022); Zhao et al. (2022) to add DP noise to all \(M\) sketches. Suppose \(\mathcal{F}=\big{(}\mathbf{f}^{(m)}\big{)}_{m=1}^{M}\) and \(\dot{\mathcal{F}}=\big{(}\hat{\mathbf{f}}^{(m)}\big{)}_{m=1}^{M}\) are the sets of local frequencies for two neighboring datasets respectively, then

\[\|\mathcal{F}-\dot{\mathcal{F}}\|_{2}\leq\frac{1}{n}.\]

Denote the sketches to be released by \(\mathcal{S}\circ\mathcal{F}:=\big{(}\mathbf{S}^{(m)}\circ\mathbf{f}^{(m)} \big{)}_{m=1}^{M}\). One can then calculate the \(\ell_{2}\)-sensitivity:

\[\|\mathcal{S}\circ\mathcal{F}-\mathcal{S}\circ\dot{\mathcal{F}}\|_{2}\leq\frac {\sqrt{L}}{n},\]

where \(L\eqsim\log(d/\delta)\) is the sketch length. Therefore the sketching will be \((\epsilon,\delta)\)-DP by adding Gaussian noise \(\mathcal{N}(0,\sigma^{2})\) to each bucket of each sketch, where

\[\sigma\eqsim\frac{\sqrt{L\log(1/\delta)}}{n\epsilon}.\]

The final released frequency estimator is obtained by post-processing the sketch, so it is also \((\epsilon,\delta)\)-DP.

We then calculate the error for the noisy sketch matrix. For each row estimator, we have that with probability at least \(2/3\):

\[\tilde{\mathbf{f}}_{j}^{\,(\ell)}-\mathbf{f}_{j}^{\ell}=\mathtt{tailNoise}+ \frac{1}{M}\sum_{m=1}^{M}\mathtt{rad}_{m}\cdot\mathcal{N}(0,\sigma^{2})\]\[=\mathtt{tailNoise}+\mathcal{N}(0,\sigma^{2}/M)\] \[\lesssim\sqrt{\frac{1}{M^{2}w}\cdot\sum_{i\notin\mathbb{W}}\sum_{m= 1}^{M}\left(\mathbf{f}_{i}^{(m)}\right)^{2}}+\frac{\sqrt{L\log(1/\delta)}}{ \sqrt{M}n\epsilon}.\]

By taking median over \(L\eqsim\log(d/\delta)\) repeats, we see that with probability at least \(1-\delta\), it holds that

\[\text{for each }j\in[d],\quad|\hat{\mathbf{f}}_{j}-\mathbf{f}_{j}|\lesssim \sqrt{\frac{1}{M^{2}w}\cdot\sum_{i\notin\mathbb{W}}\sum_{m=1}^{M}\left( \mathbf{f}_{i}^{(m)}\right)^{2}}++\frac{\sqrt{\log(d/\delta)\cdot\log(1/\delta )}}{\sqrt{M}n\epsilon}.\]