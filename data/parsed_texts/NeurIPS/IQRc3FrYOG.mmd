# Mutual-Information Regularized Multi-Agent

Policy Iteration

Jiangxing Wang

School of Computer Science

Peking University

jiangxiw@stu.pku.edu.cn &Deheng Ye

Tencent Inc.

dericye@tencent.com &Zongqing Lu

School of Computer Science

Peking University

BAAI

zongqing.lu@pku.edu.cn

Corresponding Author

###### Abstract

Despite the success of cooperative multi-agent reinforcement learning algorithms, most of them focus on a single team composition, which prevents them from being used in more realistic scenarios where dynamic team composition is possible. While some studies attempt to solve this problem via multi-task learning in a fixed set of team compositions, there is still a risk of overfitting to the training set, which may lead to catastrophic performance when facing dramatically varying team compositions during execution. To address this problem, we propose to use mutual information (MI) as an augmented reward to prevent individual policies from relying too much on team-related information and encourage agents to learn policies that are robust in different team compositions. Optimizing this MI-augmented objective in an off-policy manner can be intractable due to the existence of dynamic marginal distribution. To alleviate this problem, we first propose a multi-agent policy iteration algorithm with a fixed marginal distribution and prove its convergence and optimality. Then, we propose to employ the Blahut-Arimoto algorithm and an imaginary team composition distribution for optimization with approximate marginal distribution as the practical implementation. Empirically, our method demonstrates strong zero-shot generalization to dynamic team compositions in complex cooperative tasks.

## 1 Introduction

The cooperative multi-agent reinforcement learning (MARL) problem has attracted the attention of many researchers for being a well-abstracted model for many real-world problems, such as traffic signal control (Wang et al., 2021), autonomous warehouse (Zhou et al., 2021), and even AutoML (Wang et al., 2022) as the feedback to the machine learning community. In a cooperative MARL problem, we aim to train a group of agents that can cooperate to achieve a common goal. Such a common goal is often defined by a global reward function that is shared among all agents. Although this objective is naturally centralized, we want agents to be able to execute in a fully decentralized manner. Under such a requirement, Kraemer and Banerjee (2016) proposed the centralized training with decentralized execution (CTDE) framework, where a centralized critic is learned to evaluate the performance of the joint policy in terms of the global reward and a group of decentralized individual policies are learned via the centralized critic to realize decentralized execution.

With a centralized critic, multi-agent policy gradient methods directly use it to guide the update of each decentralized individual policy. Based on this idea, a series of studies (Lowe et al., 2017; Kuba et al., 2022; Yu et al., 2022; Ye et al., 2020) have been proposed with different optimizationtechniques for policy improvement. On the other hand, as the centralized critic is used to guide the learning of decentralized individual policies, many CTDE algorithms choose to factorize the centralized critic into decentralized individual utilities via the mixer network. This line of research is called value decomposition (Sunehag et al., 2018). Based on different design choices of the mixer network, a variety of value decomposition methods (Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2020; Zhang et al., 2021) has been proposed and achieved great success in cooperative MARL problems.

Despite the success of CTDE methods, previous research mainly focuses on training agents under a fixed team composition, which has been shown to exhibit serious overfitting issues (Wen et al., 2022), and leads to catastrophic performance when facing unseen team compositions. One natural way to solve this problem is to introduce multiple team compositions during training to alleviate overfitting. However, due to the existence of the mixer network and the network structure of individual utilities and policies, such a strategy cannot be simply applied to many existing CTDE methods.

REFIL (Iqbal et al., 2021) attempts to address this problem and proposes to use multi-head attention (Vaswani et al., 2017) in the mixer network and individual utilities to handle dynamic team compositions. It further proposes an imaginary objective based on random sub-group partitioning to accelerate the training process given a fixed set of team compositions. Although REFIL is able to handle dynamic team compositions, as it is still trained on a fixed set of team compositions, there is still a risk of overfitting to not a single team composition, but a set of team compositions, which has been demonstrated in several previous studies (Liu et al., 2021; Shao et al., 2022).

The overfitting issue can be attributed to the lack of robustness in different team compositions. When facing an arbitrary team composition during execution, one agent's observed information about team composition can arbitrarily vary from what it experienced during training. If the agent puts too much credit on this highly varied information to make decisions, it may fail to achieve robust behavior. As the variation of team composition is uncontrollable, one way to achieve robust behavior is to reduce the reliance on team-related information. Based on this intuition, we propose **MIPI** (**M**utual-**I**nformation Regularized Multi-Agent **P**olicy **I**eration), minimizing the mutual information between the policy of the agent and the team-related information to encourage robust behavior of each agent. Inspired by SAC (Haarnoja et al., 2018), we combine the global reward of environment and mutual information of each agent as a new objective and learn individual policies to optimize them at the same time. As the incorporation of mutual information imposes a challenge on optimization due to the existence of dynamic marginal distribution, we first propose a multi-agent policy iteration algorithm with a fixed marginal distribution and prove its convergence and optimality. Then, we propose to utilize the Blahut-Arimoto algorithm (Cover, 1999) and an imaginary team composition distribution for optimization under an approximate dynamic marginal distribution as the practical implementation.

To empirically justify our algorithm, we first evaluate the performance of MIPI in a simple yet challenging matrix game. Compared with the other two baselines using pure environmental reward and entropy-augmented reward, using mutual information as an augmented reward can help the agent find the policy that can achieve consistent performance across different team compositions. Then, we move to a more complicated scenario, StarCraft Micromanagement Tasks. While having the same level of performance in the training set, MIPI achieves better zero-shot generalization results in unseen team compositions during evaluation.

## 2 Related Work

### Centralized Training with Decentralized Execution (CTDE)

CTDE methods can be categorized into value decomposition and multi-agent policy gradient, depending on whether a centralized critic is decomposed or not. For value decomposition methods, a centralized critic is decomposed into decentralized utilities through the mixer network. Different mixers have been proposed as different interpretations of the Individual-Global-Maximum (Rashid et al., 2018) (IGM) principle or its equivalence, which ensures the consistency between optimal local actions and optimal joint action. VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) give sufficient conditions for IGM by additivity and monotonicity, respectively. QPLEX (Wang et al., 2020) and FOP (Zhang et al., 2021) take advantage of duplex dueling architecture to guarantee IGM.

In multi-agent policy gradient, a centralized critic function is directly used to guide the update of each decentralized individual policy. Most multi-agent policy gradient methods can be considered as an extension of the policy gradient method from RL to MARL. For example, MAPPDG (Lowe et al., 2017) extends DDPG (Lillicrap et al., 2016), HATRPO (Kuba et al., 2022) extends TRPO (Schulman et al., 2015), MAPPO (Yu et al., 2022) and CoPPO (Wu et al., 2021) extend PPO (Schulman et al., 2017).

### Dynamic Team Composition

While classical CTDE methods mainly focus on fixed team compositions, in real-world applications, agents that can adapt to dynamic team composition are preferable. To address this problem, REFIL (Iqbal et al., 2021) incorporates multi-head attention (Vaswani et al., 2017) into the networks and further introduces an imaginary objective based on random sub-group partitioning to accelerate the training process on a fixed set of team compositions. While REFIL learns policies that can handle dynamic team compositions, studies (Liu et al., 2021; Shao et al., 2022) suggest that it generalizes poorly on unseen team compositions. The necessity of training agents that can generalize to unseen team compositions is evident using the automated warehouse as an example, where it is very common to add more agents (more agents got purchased) or delete some agents (some agents got broken). Therefore, agents have to deal with different teams in the application scenario, which can not be fully covered during training. In order to learn policies that can adapt to unseen team compositions, many studies choose to sacrifice the requirement of decentralized execution. For example, a centralized agent is assumed in COPA (Liu et al., 2021), which has a global view of the environment and coordinates agents by distributing individual strategies. In SOG (Shao et al., 2022), a communication channel is assumed to elect conductors, so that the corresponding groups are constructed with conductor-follower consensus. Unlike these methods, we do not assume any kind of centralization during execution, such that the ability of decentralized execution is fully preserved. In CollaQ (Zhang et al., 2020), the decentralized utility function is decomposed into two terms: the self-term that only relies on the agent's own state, and the interactive term that is related to states of nearby agents. By using an additional MARA loss to constrain the contribution of the interactive term in the decentralized utility function, CollaQ solves the generalization on the dynamic team composition problem with CTDE being preserved. Unlike this method, MIPI uses mutual information to constrain the contribution of team-related information in the agent's policy.

### Information-Theoretic Principles in RL

In the standard RL problem, the objective is to solely optimize the environmental reward. However, in many problems, we not only want to optimize the cumulative rewards but also want the learned policy to exhibit some other properties. Therefore, a line of research across single-agent and multi-agent domains has been proposed using different information-theoretic principles for policy optimization. For example, SQL (Haarnoja et al., 2017) and SAC (Haarnoja et al., 2018) incorporate the maximum entropy principle to encourage exploration and diverse behavior. FOP (Zhang et al., 2021) further extends this idea to MARL and proves its convergence and optimality. As the augmented entropy term distorts the original objective in the original MDP, which may lead to undesirable behavior in some scenarios (Eysenbach and Levine, 2019). To solve this problem, DMAC (Su and Lu, 2022) proposes to use the divergence between the current policy and previous policy to replace entropy, which yields a bound of the discrepancy between the converged policy and optimal policy in the original MDP. Using divergence to guide the policy optimization is also very popular in offline RL (Levine et al., 2020), for example, F-BRC (Kostrikov et al., 2021) and ICQ (Yang et al., 2021), where the divergence is used to control the similarity between learned policy and behavior policy. While the entropy and KL divergence can be seen as a measurement of the distance to a fixed policy, the mutual information is about the distance to a dynamic marginal policy. In MIRL (Grau-Moya et al., 2019) and MIRACLE (Leibfried and Grau-Moya, 2020), the environmental reward is combined with mutual information to encourage the learned policy to be close to an optimal prior policy, which is also dynamically learned during the RL process instead of being fixed throughout. Unlike (Grau-Moya et al., 2019; Leibfried and Grau-Moya, 2020) that aim at a generalized version of SAC in single-agent RL, the purpose of our work is to solve generalization on dynamic team composition in MARL.

Mutual information (MI) has been widely used in previous MARL research for various purpose, e.g., exploration (Mahajan et al., 2019; Wang et al., 2019; Zheng et al., 2021), coordination (Konan et al., 2021; Kim et al., 2023), individuality (Jiang and Lu, 2021), diversity (Li et al., 2021) and social influence (Jaques et al., 2019). Unlike these works that mainly focus on the performance of agents in a fixed team, we focus on the generalization ability of agents over different or even unseen teams. Also, these works mainly try to increase the mutual information between two variables to enhance the dependency between variables. However, in our work, we try to decrease the mutual information between the agent's policy and team-related information to reduce the dependency between these two variables and avoid overfitting. Being an exception, PMIC (Li et al., 2022) maximizes the MI associated with the superior trajectories and minimizes the MI associated with the inferior trajectories at the same time. However, it still focuses on the training of a fixed team, while our work focuses on the training of a dynamic team and the generalization over unseen teams.

## 3 Background

In this paper, we formulate cooperative MARL with dynamic team composition as a multi-agent Markov decision process (MMDP) with entities. MMDP with entities can be defined by a tuple \(\langle\mathcal{E},S,A,U,P,r,\gamma\rangle\). \(\mathcal{E}\) is the set of entities in the environment, \(S\) is the set of states, and each state \(\mathbf{s}\) is composed by the state of each entity \(\mathbf{s}=\{s_{e}\}\). It is worth noting that except agents \(a\in A\), there are also other entities in the environments (e.g., landmarks, obstacles, agents with fixed behavior). \(U=U_{1}\times\cdots\times U_{|A|}\) is the joint action space, where \(U_{i}\) is the individual action space for each agent \(i\). For the agronowness of proof, we assume full observability such that at each state \(\mathbf{s}\in S\), each agent \(i\) receives state \(\mathbf{s}\), chooses an action \(u_{i}\in U_{i}\), and all actions form a joint action \(\mathbf{u}\in U\). The state transitions to the next state \(\mathbf{s}^{\prime}\) upon \(\mathbf{u}\) according to the transition function \(P(\mathbf{s}^{\prime}|\mathbf{s},\mathbf{u}):S\times U\times S\rightarrow[0,1]\), and all agents receive a shared reward \(r(\mathbf{s},\mathbf{u}):S\times U\rightarrow\mathbb{R}\). The objective is to learn an individual policy \(\pi_{i}(u_{i}|\mathbf{s})\) for each agent such that they can cooperate to maximize the expected cumulative discounted return, \(\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]\), where \(\gamma\in[0,1)\) is the discount factor. In CTDE, from a centralized perspective, a group of local policies can be viewed as a joint policy \(\pi_{\mathrm{jt}}(\mathbf{u}|\mathbf{s})\). For this joint policy, we can define the joint state-action value function \(Q_{\mathrm{jt}}(\mathbf{s}_{t},\mathbf{u}_{t})=\mathbb{E}_{\mathbf{s}_{t+1:\infty},\mathbf{u}_{ t+1:\infty}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t+k}|\mathbf{s}_{t},\mathbf{u}_{t}]\). Note that although we assume full observability for the rigorousness of proof, we use the trajectory of each agent \(\tau_{i}\in\mathcal{T}_{i}:(Y\times U_{i})^{*}\) to replace state \(\mathbf{s}\) for each agent to settle the partial observability in practice, where \(Y\) is the observation space.

Since we are discussing dynamic team composition in this paper, we further denote \(s_{i}^{+}\) as team-unrelated information for agent \(i\) (agent's own information), and use \(s_{i}^{-}\) to denote team-related information that varies along team composition (e.g., information of other agents, landmarks, obstacles). Although we assume full observability for each agent (i.e., \(\mathbf{s}\) is the same for all agents), \(s_{i}^{+}\) and \(s_{i}^{-}\) can be different as the circumstance of each agent per se is different. One can easily conclude that \(\mathbf{s}=\{s_{i}^{+},s_{i}^{-}\}\) for each agent \(i\).

## 4 Method

In this section, we present our method, MIPI, as follows. In Section 4.1, we introduce the mutual information (MI) augmented objective for regularizing the reliance on \(s_{i}^{-}\) for each agent. However, due to the existence of the dynamic marginal distribution, direct optimization on this objective can be intractable in practice. Therefore, in Section 4.2, we first discuss a multi-agent policy iteration with a fixed marginal distribution and prove its convergence and optimality. Then, in Section 4.3, we discuss how to use an imaginary team composition distribution to achieve an approximate dynamic marginal distribution and how to use the Blahut-Arimoto algorithm (Cover, 1999) to optimize the corresponding objective. Finally, in Section 4.4, we summarize the learning framework of MIPI.

### MI-Augmented Objective

The learning objective of standard MARL can be formulated as follows,

\[\operatorname*{arg\,max}_{\pi_{\mathrm{jt}}}\mathbb{E}_{\rho(\mathbf{s}_{0}),\pi_ {\mathrm{jt}},P}\Bigg{[}\sum_{t=0}^{T}\gamma^{t}r(\mathbf{s}_{t},\mathbf{u}_{t})\Bigg{]}, \tag{1}\]where \(\rho(\mathbf{s}_{0})\) is the distribution of initial state. We can further rewrite it as:

\[\operatorname*{arg\,max}_{\pi_{\rm jt}}\sum_{t=0}^{T}\mathbb{E}_{\rho_{\pi_{\rm jt }}(\mathbf{s}_{t})}\left[\,\mathbb{E}_{\pi_{\rm jt}(\mathbf{u}_{t}|\mathbf{s}_{t})}\left[ \gamma^{t}r(\mathbf{s}_{t},\mathbf{u}_{t})\right]\right], \tag{2}\]

where \(\rho_{\pi_{\rm jt}}(\mathbf{s}_{t})\) is the the marginal distribution over states at timestep \(t\). Recall that the conditional mutual information between \(x\) and \(y\) given \(z\) can be expressed as follows,

\[\mathbf{MI}(x;y|z)=\mathbb{E}_{p(y,z)}\left[\,\mathbb{E}_{p(x|y,z)}\left[\, \log\frac{p(x|y,z)}{p(x|z)}\right]\right].\]

The conditional mutual information can be used to measure the dependency between variable \(x\) and \(y\) with \(z\) given. Therefore, as our goal is to reduce the reliance of \(\pi_{i}(u_{i}|\mathbf{s})=\pi_{i}(u_{i}|s_{i}^{+},s_{i}^{-})\) on \(s_{i}^{-}\), we can formulate conditional mutual information as follows,

\[\mathbf{MI}(u_{i};s_{i}^{-}|s_{i}^{+}) =\mathbb{E}_{\rho(s_{i}^{+},s_{i}^{-})}\left[\,\mathbb{E}_{\pi_{i }(u_{i}|s_{i}^{+},s_{i}^{-})}\left[\,\log\frac{\pi_{i}(u_{i}|s_{i}^{+},s_{i}^{ -})}{\pi_{i}(u_{i}|s_{i}^{+})}\right]\right]\] \[=\mathbb{E}_{\rho(s_{i}^{+}),\rho(s_{i}^{-}|s_{i}^{+})}\left[\, \mathbb{E}_{\pi_{i}(u_{i}|\mathbf{s})}\left[\,\log\frac{\pi_{i}(u_{i}|\mathbf{s})}{\pi _{i}(u_{i}|s_{i}^{+})}\right]\right],\]

where \(\pi_{i}(u_{i}|s_{i}^{+})=\sum_{s_{i}^{-}}\rho(s_{i}^{-}|s_{i}^{+})\pi_{i}(u_{i }|s_{i}^{+},s_{i}^{-})\). Incorporating the conditional mutual information of all agents into the standard MARL objective, we now have the MI-augmented objective used in this paper:

\[\operatorname*{arg\,max}_{\pi_{\rm jt}} \sum_{t=0}^{T}\mathbb{E}_{\rho_{\pi_{\rm jt}}(\mathbf{s}_{t})}\left[ \,\mathbb{E}_{\pi_{\rm jt}(\mathbf{u}_{t}|\mathbf{s}_{t})}\left[\gamma^{t}\Big{(}r( \mathbf{s}_{t},\mathbf{u}_{t})-\alpha\sum_{i}\log\frac{\pi_{i}(u_{i,t}|\mathbf{s}_{t})}{\pi _{i}(u_{i,t}|s_{i,t}^{+})}\Big{)}\right]\right]\] (3) s.t. \[\pi_{i}(u_{i,t}|s_{i,t}^{+})=\sum_{s_{i,t}^{-}}\rho_{\pi_{\rm jt} }(s_{i,t}^{-}|s_{i,t}^{+})\pi_{i}(u_{i,t}|s_{i,t}^{+},s_{i,t}^{-}), \tag{4}\]

where the coefficient \(\alpha\) is used to determine the trade-off between maximizing global reward and minimizing mutual information.

### Multi-Agent Policy Iteration with a Fixed Marginal Distribution

As we can see in (4), the optimization of (3) is highly coupled with a dynamic marginal distribution \(\pi_{i}(u_{i,t}|s_{i,t}^{+})\). What's even worse is, this marginal distribution is determined by \(\rho_{\pi_{\rm jt}}(s_{i,t}^{-}|s_{i,t}^{+})\), which is related to \(\pi_{\rm jt}\), making the optimization problem even harder. However, one may notice that, if such a marginal distribution \(\pi_{i}(u_{i,t}|s_{i,t}^{+})\) is given and fixed, this problem becomes much easier and can be solved in an off-policy manner. Therefore, in this section, we introduce multi-agent policy iteration with a fixed marginal distribution and prove its convergence and optimality, and in the next section, we discuss how to approximate the dynamic marginal distribution by integrating constraints similar to (4) into this multi-agent policy iteration. First, let us define the joint value function \(V_{\rm jt}\) and joint state-action value function \(Q_{\rm jt}\) as follows,

\[V_{\rm jt}^{\pi_{\rm jt}}(\mathbf{s})=\mathbb{E}_{\pi_{\rm jt}}\left[ \,\sum_{t=0}\gamma^{t}\Big{(}r(\mathbf{s}_{t},\mathbf{u}_{t})-\alpha\sum_{i}\log\frac{ \pi_{i}(u_{i,t}|\mathbf{s}_{t})}{\pi_{i}(u_{i,t}|s_{i,t}^{+})}\Big{)}|\mathbf{s}_{0}= \mathbf{s}\right]\] \[Q_{\rm jt}^{\pi_{\rm jt}}(\mathbf{s},\mathbf{u})=r(\mathbf{s},\mathbf{u})+\gamma \,\mathbb{E}_{\mathbf{s}^{\prime}\sim P}\left[\,V_{\rm jt}(\mathbf{s}^{\prime})\right],\]

where \(\pi_{i}(u_{i,t}|s_{i,t}^{+})\) is the fixed marginal distribution for each agent \(i\). With the above definition, we can further deduce that:

\[V_{\rm jt}^{\pi_{\rm jt}}(\mathbf{s})=\mathbb{E}_{\pi_{\rm jt}}\left[Q_{\rm jt}^{ \pi_{\rm jt}}(\mathbf{s},\mathbf{u})-\alpha\sum_{i}\log\frac{\pi_{i}(u_{i}|\mathbf{s})}{\pi _{i}(u_{i}|s_{i}^{+})}\right].\]

We can then define the joint policy evaluation operator as

\[\Gamma_{\pi_{\rm jt}}\,Q_{\rm jt}(\mathbf{s},\mathbf{u}):=r(\mathbf{s},\mathbf{u})+\gamma\, \mathbb{E}_{\mathbf{s}^{\prime}}[V_{\rm jt}(\mathbf{s}^{\prime})] \tag{5}\]

and have the following lemma.

**Lemma 1** (Joint Policy Evaluation).: _Consider the modified Bellman backup operator \(\Gamma_{\pi_{\mathrm{jt}}}\) (5) and a mapping \(Q^{0}_{\mathrm{jt}}:S\times U\to\mathbb{R}\) with \(|U|<\infty\), and define \(Q^{k+1}_{\mathrm{jt}}=\Gamma_{\pi_{\mathrm{jt}}}\,Q^{k}_{\mathrm{jt}}\). Then, the sequence \(Q^{k}_{\mathrm{jt}}\) will converge to the joint \(\mathbf{Q}\)-function of \(\pi_{\mathrm{jt}}\) as \(k\to\infty\)._

Proof.: See Appendix A.1. 

Using Lemma 1, we can get \(Q_{\mathrm{jt}}\) for any joint policy \(\pi_{\mathrm{jt}}\). However, it is hard for us to use \(Q_{\mathrm{jt}}\) for individual policy improvement. To solve this problem, many value decomposition methods choose to factorize the joint state-action value function \(Q_{\mathrm{jt}}\) into the utility function \(Q_{i}\) of each agent and use \(Q_{i}\) to guide the individual policy improvement of \(\pi_{i}\). In this paper, we factorize the joint state-action value function into the following form, which is shared by many value decomposition methods (Zhang et al., 2021; Su and Lu, 2022; Wang et al., 2023):

\[Q^{\pi_{\mathrm{jt}}}_{\mathrm{jt}}(\mathbf{s},\mathbf{u})=\sum_{i}w_{i}(\mathbf{s})*Q^{ \pi_{i}}_{i}(\mathbf{s},u_{i})+b(\mathbf{s}). \tag{6}\]

After the evaluation of the joint policy and the decomposition of \(Q_{\mathrm{jt}}\), we construct the following optimization problem for individual policy improvement.

\[\pi^{\mathrm{new}}_{i}=\arg\max_{\pi^{\prime}_{i}}\mathbb{E}_{\pi^{\prime}_{i} }\left[Q^{\pi^{\mathrm{old}}_{i}}_{i}(\mathbf{s},u_{i})-\alpha\log\frac{\pi^{ \prime}_{i}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s^{+}_{i})}\right] \tag{7}\]

Based on the above optimization problem, we have the following lemma for individual policy improvement.

**Lemma 2** (Individual Policy Improvement).: _Let \(\pi^{\mathrm{new}}_{i}\) be the optimizer of the maximization problem in (7). Then, we have \(Q^{\pi^{\mathrm{new}}_{\mathrm{jt}}}_{\mathrm{jt}}(\mathbf{s},\mathbf{u})\geq Q^{\pi^{ \mathrm{old}}_{\mathrm{jt}}}_{\mathrm{jt}}(\mathbf{s},\mathbf{u})\) for all \((\mathbf{s},\mathbf{u})\in|S|\times|U|\) with \(|U|<\infty\), where \(\pi^{\mathrm{old}}_{\mathrm{jt}}(\mathbf{u}|\mathbf{s})=\prod_{i}\pi^{\mathrm{old}}_{i }(u_{i}|\mathbf{s})\) and \(\pi^{\mathrm{new}}_{\mathrm{jt}}(\mathbf{u}|\mathbf{s})=\prod_{i}\pi^{\mathrm{new}}_{i }(u_{i}|\mathbf{s})\)._

Proof.: See Appendix A.2. 

Combining Lemma 1 and 2, we can have the following theorem which proves the convergence and optimality of multi-agent policy iteration with a fixed marginal distribution.

**Theorem 1** (Multi-Agent Policy Iteration with a Fixed Marginal Distribution).: _For any joint policy \(\pi_{\mathrm{jt}}\), if we repeatedly apply joint policy evaluation and individual policy improvement. Then the joint policy \(\pi_{\mathrm{jt}}(\mathbf{u}|\mathbf{s})=\prod_{i=1}^{n}\pi_{i}(u_{i}|\mathbf{s})\) will eventually converge to \(\pi^{*}_{\mathrm{jt}}\), such that \(Q^{\pi^{\mathrm{jt}}_{\mathrm{jt}}}_{\mathrm{jt}}(\mathbf{s},\mathbf{u})\geq Q^{\pi_{ \mathrm{jt}}}_{\mathrm{jt}}(\mathbf{s},\mathbf{u})\) for all \(\pi_{\mathrm{jt}}\), assuming \(|U|<\infty\)._

Proof.: See Appendix A.3. 

### Approximation for Dynamic Marginal Distribution

With the multi-agent policy iteration above, we can have \(Q_{\mathrm{jt}}\) and corresponding \(Q_{i}\) for each agent, however, under a fixed marginal distribution. In this section, we discuss how to approximate the dynamic marginal distribution to decouple \(\rho_{\pi_{\mathrm{jt}}}(s^{-}_{i,t}|s^{+}_{i,t})\) from \(\pi_{\mathrm{jt}}\), and introduce the Blahut-Arimoto algorithm for the corresponding optimization.

Notice that the original objective (3) comes with a constraint (4). In Section 4.2, what we did is to remove this constraint for an easier optimization process. What we are going to do here, is to add a similar constraint back. First, consider the meaning of \(\rho_{\pi_{\mathrm{jt}}}(s^{-}_{i}|s^{+}_{i})\), it represents the potential team composition given team-unrelated information. Therefore, inspired by REFIL (Iqbal et al., 2021), we randomly partition team composition under \(\mathbf{s}=\{s^{+}_{i},s^{-}_{i}\}\) into different subgroups, yielding a set of imaginary team compositions and corresponding imaginary distribution \(\hat{\rho}(s^{*}_{i}|s^{+}_{i})\) for imaginary team-related information. With this imaginary distribution, we can propose the approximate objectivefor (3) as follows,

\[\arg\max_{\pi_{i}}\sum_{t=0}^{T}\mathbb{E}_{\rho_{\pi_{i}}(\mathbf{s}_{t} )}\left[\mathbb{E}_{\pi_{i}(\mathbf{u}_{t}|\mathbf{s}_{t})}\left[\gamma^{t}\Big{(}r( \mathbf{s}_{t},\mathbf{u}_{t})-\alpha\sum_{i}\log\frac{\pi_{i}(u_{i,t}|\mathbf{s}_{t})}{\pi_ {i}(u_{i,t}|s_{i,t}^{+})}\Big{)}\right]\right] \tag{8}\] \[\text{s.t.}\quad\pi_{i}(u_{i,t}|s_{i,t}^{+})=\sum_{s_{i,t}^{*}} \hat{\rho}(s_{i,t}^{*}|s_{i,t}^{+})\pi_{i}(u_{i,t}|s_{i,t}^{+},s_{i,t}^{*}). \tag{9}\]

Therefore, we can have the approximate optimization problem for (7) as follows,

\[\pi_{i}^{\text{new}}= \arg\max_{\pi_{i}^{\prime}}\mathbb{E}_{\pi_{i}^{\prime}}\left[Q_{ i}^{\pi_{i}^{\text{old}}}(\mathbf{s},u_{i})-\alpha\log\frac{\pi_{i}^{\prime}(u_{i}| \mathbf{s})}{\pi_{i}^{\prime}(u_{i}|s_{i}^{+})}\right] \tag{10}\] \[\text{s.t.}\quad\pi_{i}^{\prime}(u_{i}|s_{i}^{+})=\sum_{s_{i}^{*} }\hat{\rho}(s_{i}^{*}|s_{i}^{+})\pi_{i}^{\prime}(u_{i}|s_{i}^{+},s_{i}^{*}). \tag{11}\]

The objective above exhibits similarities with the rate-distortion problem (Cover, 1999), which could be solved using the Blahut-Arimoto algorithm. Although with the approximation above we break the convergence of Theorem 1, by using the Blahut-Arimoto algorithm we can have the following theorem, indicating the convergence of (10) as shown in Leibfried and Grau-Moya (2020).

**Theorem 2** (**Convergence of Constrained Individual Policy Improvement)**.: _The optimization problem induced by (10) can be solved by iterating in an alternate fashion through the following two equations:_

\[\pi_{i}^{m}(u_{i}|s_{i}^{+})=\sum_{s_{i}^{*}}\hat{\rho}(s_{i}^{*} |s_{i}^{+})\pi_{i}^{m}(u_{i}|s_{i}^{+},s_{i}^{*}) \tag{12}\] \[\pi_{i}^{m+1}(u_{i}|s_{i}^{+},s_{i}^{-})=\frac{\pi_{i}^{m}(u_{i}| s_{i}^{+})\exp(Q_{i}(\mathbf{s},u_{i})/\alpha)}{\sum_{u_{i}}\pi_{i}^{m}(u_{i}|s_{i}^{+}) \exp(Q_{i}(\mathbf{s},u_{i})/\alpha)}, \tag{13}\]

_where \(m\) refers to the iteration index. Denoting the total number of iterations as \(M\), the presented scheme converges at a rate of \(O(1/M)\) to an optimal policy \(\pi_{i}^{*}\) for any given bounded utility function \(Q_{i}\) and any initial policy \(\pi_{i}^{0}\)._

Proof.: See Appendix A.4. 

### MIPI Framework

In Section 4.2 and 4.3, we propose our learning algorithm in theory. In this section, we discuss how to implement our algorithm in practice, which can be summarized in Figure 1.

Figure 1: Learning framework of MIPI, where each agent \(i\) has three modules: a utility function \(Q_{i}(\mathbf{s},u_{i};\theta_{i})\), a policy \(\pi_{i}(u_{i}|\mathbf{s};\phi_{i})\), and a marginal policy \(\pi_{i}(u_{i}|s_{i}^{+};\omega_{i})\).

In MIPI, each agent has a utility function \(Q_{i}(\mathbf{s},u_{i};\theta_{i})\), a policy \(\pi_{i}(u_{i}|\mathbf{s};\phi_{i})\), and a marginal policy \(\pi_{i}(u_{i}|s_{i}^{+};\omega_{i})\). For joint policy evaluation, with the utilities of agents, we use a mixer network \(\mathrm{Mixer}(\cdot,\mathbf{s};\Theta)\) to get the joint state-action value function \(Q_{\mathrm{jt}}\) as follows,

\[Q_{\mathrm{jt}}(\mathbf{s},\mathbf{u}) =\mathrm{Mixer}([Q_{i}(\mathbf{s},u_{i};\theta_{i})]_{i=1}^{|A|},\mathbf{s };\Theta) \tag{14}\] \[=\sum_{i=1}^{|A|}w_{i}(\mathbf{s})Q_{i}(\mathbf{s},u_{i};\theta_{i})+b( \mathbf{s}), \tag{15}\]

Where \(w_{i}(\mathbf{s})\geq 0\) is a positive weight used to linearly decompose \(Q_{\mathrm{jt}}\) with the IGM principle being preserved. Same as REFIL, \(w_{i}(\mathbf{s})\) is computed via the attention mechanism to handle dynamic team composition. With \(Q_{\mathrm{jt}}\), we can update the utilities and the mixer network by minimizing the following TD error:

\[\mathcal{L}([\theta_{i}]_{i=1}^{|A|},\Theta)=\mathbb{E}_{\mathcal{D}}\left[Q_{ \mathrm{jt}}(\mathbf{s},\mathbf{u})-\left(r(\mathbf{s},\mathbf{u})+\gamma\Big{(}\hat{Q}_{ \mathrm{jt}}(\mathbf{s}^{\prime},\mathbf{u}^{\prime})-\alpha\sum_{i}^{|A|}\log\frac{ \pi_{i}(u^{\prime}_{i}|\mathbf{s}^{\prime})}{\pi_{i}(u^{\prime}_{i}|s^{\prime}_{i} )}\Big{)}\right)\right], \tag{16}\]

where \(\mathcal{D}\) is the replay buffer, \(\hat{Q}_{\mathrm{jt}}\) is the target network and \(u^{\prime}_{i}\) is sampled from the current policy \(\pi_{i}(u_{i}|\mathbf{s};\phi_{i})\) of each agent. To accelerate the training with a fixed set of team compositions, we also incorporate the same imaginary objective based on random sub-group partitioning as REFIL for joint policy evaluation.

As we described in Section 4.3, the constrained individual policy improvement is achieved via an iterative update of \(\pi_{i}(u_{i}|\mathbf{s};\phi_{i})\) and \(\pi_{i}(u_{i}|s^{+}_{i};\omega_{i})\). For \(\pi_{i}(u_{i}|s^{+}_{i};\omega_{i})\), we update it via the maximum likelihood estimation:

\[\mathcal{L}(\omega_{i})=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{s^{*}_{i} \sim\hat{\rho}(s^{*}_{i}|s^{+}_{i}),u_{i}\sim\pi_{i}(u_{i}|s^{+}_{i},s^{*}_{i} )}\left[\log\pi_{i}(u_{i}|s^{+}_{i})\right]\right]. \tag{17}\]

For \(\pi_{i}(u_{i}|\mathbf{s};\phi_{i})\), we update it by minimizing the KL-divergence as follows,

\[\mathcal{L}(\phi_{i})=\mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{u_{i}\sim\pi_{i }(u_{i}|\mathbf{s})}\left[\alpha\Big{(}\log\frac{\pi_{i}(u_{i}|\mathbf{s})}{\pi_{i}(u _{i}|s^{+}_{i})}\Big{)}-Q_{i}(\mathbf{s},u_{i})\right]\right]. \tag{18}\]

## 5 Experiments

In this section, we evaluate MIPI in two different scenarios. One is a simple yet challenging matrix game, which we use to illustrate how mutual information may help to learn generalizable policies. Then, we evaluate MIPI in a complicated cooperative MARL scenario: StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019), comparing it against REFIL, AQMIX (Iqbal et al., 2021), CollaQ (Zhang et al., 2020) and MAPPO (Yu et al., 2022). More details about experiments, hyperparameters, and the learning curve of each algorithm are included in Appendix B and C. All results are presented using the mean and standard deviation of five runs with different random seeds.

### An Illustrative Example: Matrix Game

We first use a matrix game to explain how mutual information works in solving generalization problems. In this game, we have two agents, and each of them can take two actions \(\{0,1\}\) and can take one of the two types \(\{A,B\}\). During training, we train these two agents under team compositions \((A,B)\) and \((B,A)\), where the corresponding payoff matrices are shown in Figure 2(a) and 2(b). However, during evaluation, we test their performance on team composition \((B,B)\), and have Figure 2(c) as the payoff matrix, which is different from training scenarios.

As we can see in Figure 2(a), 2(a), 2(c), we have different optimal joint actions in different team compositions. However, there exists a generalizable joint action \((a_{1}=0,a_{2}=0)\) that can achieve consistent performance regardless of team compositions, even if it is not an optimal joint action in any team composition.

In Figure 2(d), we plot the evaluation results of three algorithms during training, which is evaluated on team composition \((B,B)\). These three algorithms are all the same except they receive different rewards: pure environmental reward, environmental reward combined with entropy, and environmental reward combined with mutual information. As we can see in Figure 2(d), with the help of mutual information, agents are able to resist the temptation of overfitting to the specific team composition and learn behavior that can generalize across different team compositions.

### StarCraft Micromanagement Tasks

#### 5.2.1 Performance

Further, we evaluate MIPI on SMAC with the map designed by Iqbal et al. (2021). We customize three different types of scenarios (SZ, CSZ, and MMM) based on this map for our experiments. In SZ scenarios, agents can take two different unit types, in CSZ and MMM, agents can take three different unit types. During training, the maps randomly initialize 3-5 agents and the same number of enemies at the start of each episode. During the evaluation, we use 6-8 agents and 6-8 enemies. Results are shown in Table 1. In general, MIPI outperforms the baselines in **8 out of 9 evaluation scenarios**. When the evaluation scenario is similar to the training scenarios, the gap between MIPI and other baselines is relatively small, whereas, in the evaluation scenario that is very different from the training scenarios, the gap between MIPI and other baselines becomes larger. In terms of the training performance, REFIL achieves the best result in all three scenarios, as it does not consider the overfitting issue at all. However, the performance of MIPI is still at the same level as REFIL, which indicates that MIPI can achieve better zero-shot generalization without sacrificing the performance on the training set.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{Tasks\#Agent} & \multirow{2}{*}{Algorithms} & Training & \multicolumn{3}{c|}{Evaluation} \\ \cline{3-7}  & & & 3-5 & 6 & 7 & 8 \\ \hline \hline \multirow{5}{*}{SZ} & MIPI & 0.659\(\pm\)0.02 & **0.453\(\pm\)**0.08 & **0.404\(\pm\)**0.062 & **0.276\(\pm\)**0.076 \\ \cline{2-7}  & REFIL & **0.674\(\pm\)**0.038 & 0.441\(\pm\)0.103 & 0.352\(\pm\)0.078 & 0.236\(\pm\)0.103 \\ \cline{2-7}  & AQMIX & 0.528\(\pm\)0.044 & 0.343\(\pm\)0.105 & 0.291\(\pm\)0.084 & 0.182\(\pm\)0.058 \\ \cline{2-7}  & CollaQ & 0.588\(\pm\)0.03 & 0.366\(\pm\)0.086 & 0.314\(\pm\)0.076 & 0.198\(\pm\)0.097 \\ \cline{2-7}  & MAPPO & 0.256\(\pm\)0.01 & 0.129\(\pm\)0.019 & 0.148\(\pm\)0.031 & 0.036\(\pm\)0.015 \\ \hline \hline \multirow{5}{*}{CSZ} & MIPI & 0.548\(\pm\)0.032 & **0.42\(\pm\)**0.102 & **0.297\(\pm\)**0.112 & **0.261\(\pm\)**0.09 \\ \cline{2-7}  & REFIL & **0.568\(\pm\)**0.027 & 0.348\(\pm\)0.057 & 0.229\(\pm\)0.053 & 0.164\(\pm\)0.06 \\ \cline{2-7}  & AQMIX & 0.509\(\pm\)0.054 & 0.323\(\pm\)0.096 & 0.216\(\pm\)0.101 & 0.152\(\pm\)0.071 \\ \cline{2-7}  & CollaQ & 0.459\(\pm\)0.061 & 0.362\(\pm\)0.13 & 0.267\(\pm\)0.099 & 0.231\(\pm\)0.095 \\ \cline{2-7}  & MAPPO & 0.248\(\pm\)0.037 & 0.12\(\pm\)0.029 & 0.06\(\pm\)0.028 & 0.054\(\pm\)0.013 \\ \hline \hline \multirow{5}{*}{MMM} & MIPI & 0.548\(\pm\)0.023 & 0.495\(\pm\)0.054 & **0.447\(\pm\)**0.041 & **0.467\(\pm\)**0.067 \\ \cline{2-7}  & REFIL & **0.605\(\pm\)**0.057 & 0.437\(\pm\)0.118 & 0.329\(\pm\)0.171 & 0.224\(\pm\)0.163 \\ \cline{1-1} \cline{2-7}  & AQMIX & 0.501\(\pm\)0.036 & 0.447\(\pm\)0.043 & 0.344\(\pm\)0.071 & 0.251\(\pm\)0.089 \\ \cline{1-1} \cline{2-7}  & CollaQ & 0.589\(\pm\)0.027 & **0.513\(\pm\)**0.07 & 0.423\(\pm\)0.026 & 0.286\(\pm\)0.083 \\ \cline{1-1} \cline{2-7}  & MAPPO & 0.289\(\pm\)0.097 & 0.32\(\pm\)0.102 & 0.25\(\pm\)0.063 & 0.275\(\pm\)0.098 \\ \hline \end{tabular}
\end{table}
Table 1: Final performance on all SMAC maps. MIPI outperforms REFIL, AQMIX, and CollaQ in 8 out of 9 evaluation maps. We bold the best mean performance for each map.

Figure 2: A matrix game with different team compositions: (a) (b) (c) payoff matrices for different team compositions; (d) learning curves of different methods on team composition \((B,B)\).

#### 5.2.2 Ablation

Although MIPI uses the random sub-group partitioning as in REFIL, it is an actor-critic structure, whereas REFIL uses only a critic. Therefore, one may question whether the improved generalization of MIPI is due to the introduction of mutual information, or simply due to the introduction of the actor. To eliminate such a concern, we build two ablation baselines, Value and Entropy, where all other perspectives are the same as MIPI, except they use pure environmental reward and entropy-augmented reward, respectively. As we can see in Table 2, MIPI also outperforms these two baselines, which demonstrates the importance of MI-augmented reward in MIPI.

## 6 Conclusion

In this paper, we propose MIPI, an MI-regularized multi-agent policy iteration algorithm to improve the generalization ability of agents under unseen team compositions. We first prove the convergence and optimality of our algorithm given a fixed marginal distribution, then we propose to use an imaginary distribution to approximate the dynamic marginal distribution to better approximate the original objective and incorporate the Blahut-Arimoto algorithm into the multi-agent policy iteration to optimize this approximate objective. We evaluate our algorithm in complex coordination scenarios, SMAC, and demonstrate that MIPI can achieve better zero-shot generalization results, without sacrificing the performance on the training set.

One potential limitation of this work is the introduction of approximation distorts the original mutual information augmented objective and breaks the convergences of multi-agent policy iteration. One possible solution to this problem is to seek alternative solutions using on-policy optimization methods (Schulman et al., 2015, 2017; Grudzien et al., 2022) to optimize the augmented objective.

## Acknowledgments and Disclosure of Funding

This work was supported in part by NSF China under grant 62250068 and Tencent. The authors would like to thank the anonymous reviewers for their valuable comments.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Tasks\#Agent} & \multirow{2}{*}{Algorithms} & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c|}{Evaluation} \\ \cline{3-6}  & & 3-5 & 6 & 7 & 8 \\ \hline \hline \multirow{3}{*}{SZ} & MIPI & **0.659\(\pm\)**0.02 & **0.453\(\pm\)**0.08 & **0.404\(\pm\)**0.062 & **0.276\(\pm\)**0.076 \\ \cline{2-6}  & Value & 0.621\(\pm\)0.042 & 0.336\(\pm\)0.075 & 0.275\(\pm\)0.114 & 0.154\(\pm\)0.052 \\ \cline{2-6}  & Entropy & 0.105\(\pm\)0.035 & 0.024\(\pm\)0.011 & 0.015\(\pm\)0.011 & 0.01\(\pm\)0.01 \\ \hline \hline \multirow{3}{*}{CSZ} & MIPI & **0.548\(\pm\)**0.032 & **0.42\(\pm\)**0.102 & **0.297\(\pm\)**0.112 & **0.261\(\pm\)**0.09 \\ \cline{2-6}  & Value & 0.542\(\pm\)0.059 & 0.368\(\pm\)0.083 & 0.207\(\pm\)0.076 & 0.172\(\pm\)0.112 \\ \cline{2-6}  & Entropy & 0.316\(\pm\)0.04 & 0.237\(\pm\)0.051 & 0.076\(\pm\)0.041 & 0.066\(\pm\)0.041 \\ \hline \hline \multirow{3}{*}{MMM} & MIPI & **0.548\(\pm\)**0.023 & 0.495\(\pm\)0.054 & **0.447\(\pm\)**0.041 & **0.467\(\pm\)**0.067 \\ \cline{2-6}  & Value & 0.545\(\pm\)0.048 & **0.505\(\pm\)**0.058 & 0.391\(\pm\)0.083 & 0.319\(\pm\)0.105 \\ \cline{1-1} \cline{2-6}  & Entropy & 0.265\(\pm\)0.034 & 0.2\(\pm\)0.085 & 0.16\(\pm\)0.064 & 0.075\(\pm\)0.044 \\ \hline \end{tabular}
\end{table}
Table 2: Final performance on all SMAC maps. MIPI is compared with ablation baselines. We bold the best mean performance for each map.

## References

* Cover (1999) Thomas M Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* Eysenbach and Levine (2019) Benjamin Eysenbach and Sergey Levine. If maxent rl is the answer, what is the question? _arXiv preprint arXiv:1910.01913_, 2019.
* Grau-Moya et al. (2019) Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft q-learning with mutual-information regularization. In _International conference on learning representations_, 2019.
* Grudzien et al. (2022) Jakub Grudzien, Christian A Schroeder De Witt, and Jakob Foerster. Mirror learning: A unifying framework of policy optimisation. In _International Conference on Machine Learning_, pages 7825-7844, 2022.
* Haarnoja et al. (2017) Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR, 2017.
* Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Iqbal et al. (2021) Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin Bohmer, Shimon Whiteson, and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 4596-4606. PMLR, 2021.
* Jaques et al. (2019) Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In _International conference on machine learning_, pages 3040-3049. PMLR, 2019.
* Jiang and Lu (2021) Jiechuan Jiang and Zongqing Lu. The emergence of individuality. In _International Conference on Machine Learning_, pages 4992-5001. PMLR, 2021.
* Kim et al. (2023) Woojun Kim, Whiyoung Jung, Myungsik Cho, and Youngchul Sung. A variational approach to mutual information-based coordination for multi-agent reinforcement learning. _arXiv preprint arXiv:2303.00451_, 2023.
* Konan et al. (2021) Sachin G Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming. In _International Conference on Learning Representations_, 2021.
* Kostrikov et al. (2021) Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In _International Conference on Machine Learning_, pages 5774-5783. PMLR, 2021.
* Kraemer and Banerjee (2016) Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. _Neurocomputing_, 190:82-94, 2016.
* Kuba et al. (2022) Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In _International conference on learning representations_, 2022.
* Leibfried and Grau-Moya (2020) Felix Leibfried and Jordi Grau-Moya. Mutual-information regularization in markov decision processes and actor-critic learning. In _Conference on Robot Learning_, pages 360-373. PMLR, 2020.
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Li et al. (2021) Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 34:3991-4002, 2021.
* Liu et al. (2021)Pengyi Li, Hongyao Tang, Tianpei Yang, Xiaotian Hao, Tong Sang, Yan Zheng, Jianye Hao, Matthew E Taylor, Wenyuan Tao, and Zhen Wang. Pmic: Improving multi-agent reinforcement learning with progressive mutual information collaboration. In _International Conference on Machine Learning_, pages 12979-12997. PMLR, 2022.
* Lillicrap et al. (2016) Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In _International conference on learning representations_, 2016.
* Liu et al. (2021) Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar. Coach-player multi-agent reinforcement learning for dynamic team composition. In _International Conference on Machine Learning_, pages 6860-6870. PMLR, 2021.
* Lowe et al. (2017) Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.
* Mahajan et al. (2019) Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. _Advances in neural information processing systems_, 32, 2019.
* Papoudakis et al. (2021) Georgios Papoudakis, Filippos Christianoos, Lukas Schafer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021.
* Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 4295-4304. PMLR, 2018.
* Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. _CoRR_, abs/1902.04043, 2019.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Shao et al. (2022) Jianzhuru Shao, Zhiqiang Lou, Hongchang Zhang, Yuhang Jiang, Shuncheng He, and Xiangyang Ji. Self-organized group for cooperative multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 35:5711-5723, 2022.
* Su and Lu (2022) Kefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In _International Conference on Machine Learning_, pages 20580-20603. PMLR, 2022.
* Sunehag et al. (2018) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In _Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems_, pages 2085-2087, 2018.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT press, 2018.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wang et al. (2023) Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized execution: Multi-agent conditional policy factorization. In _International conference on learning representations_, 2023.
* Wang et al. (2020) Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. In _International Conference on Learning Representations_, 2020.
* Wang et al. (2021)Tong Wang, Jiahua Cao, and Azhar Hussain. Adaptive traffic signal control for large-scale scenario with cooperative group-based multi-agent reinforcement learning. _Transportation research part C: emerging technologies_, 125:103046, 2021.
* Wang et al. (2019) Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent exploration. _arXiv preprint arXiv:1910.05512_, 2019.
* Wang et al. (2022) Zhaozhi Wang, Kefan Su, Jian Zhang, Huizhu Jia, Qixiang Ye, Xiaodong Xie, and Zongqing Lu. Multi-agent automated machine learning. _arXiv preprint arXiv:2210.09084_, 2022.
* Wen et al. (2022) Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem. _Advances in Neural Information Processing Systems_, 35:16509-16521, 2022.
* Wu et al. (2021) Zifan Wu, Chao Yu, Deheng Ye, Junge Zhang, Hankz Hankui Zhuo, et al. Coordinated proximal policy optimization. _Advances in Neural Information Processing Systems_, 34:26437-26448, 2021.
* Yang et al. (2021) Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 34:10299-10312, 2021.
* Ye et al. (2020) Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. _Advances in Neural Information Processing Systems_, 33:621-632, 2020.
* Yu et al. (2022) Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. _Advances in Neural Information Processing Systems_, 35:24611-24624, 2022.
* Zhang et al. (2021) Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy of maximum-entropy multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 12491-12500. PMLR, 2021.
* Zhang et al. (2020) Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. Multi-agent collaboration via reward attribution decomposition. _arXiv preprint arXiv:2010.08531_, 2020.
* Zheng et al. (2021) Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven exploration. _Advances in Neural Information Processing Systems_, 34:3757-3769, 2021.
* Zhou et al. (2021) Tong Zhou, Dunbing Tang, Haihua Zhu, and Zequn Zhang. Multi-agent reinforcement learning for online scheduling in smart factories. _Robotics and Computer-Integrated Manufacturing_, 72:102202, 2021.

Proofs

### Proof of Lemma 1

**Lemma 1** (**Joint Policy Evaluation**).: _Consider the modified Bellman backup operator \(\Gamma_{\pi_{\mathrm{ji}}}\) (5) and a mapping \(Q^{0}_{\mathrm{jit}}:S\times U\to\mathbb{R}\) with \(|U|<\infty\), and define \(Q^{k+1}_{\mathrm{jt}}=\Gamma_{\pi_{\mathrm{ji}}}\,Q^{k}_{\mathrm{jt}}\). Then, the sequence \(Q^{k}_{\mathrm{jt}}\) will converge to the joint Q-function of \(\pi_{\mathrm{jt}}\) as \(k\to\infty\)._

Proof.: First, define the augmented reward2 as:

Footnote 2: We assume \(\pi_{i}(u_{i}|\mathbf{s})\) and \(\pi_{i}(u_{i}|_{i}^{+})\) to be \(\epsilon\)-soft policy (Sutton and Barto, 2018) to avoid the \(\log\) term being undefined.

\[r_{\pi_{\mathrm{ji}}}(\mathbf{s},\mathbf{u}):=r(\mathbf{s},\mathbf{u})-\alpha\,\mathbb{E}_{ \mathbf{s}^{\prime}}\Bigg{[}\,\mathbb{E}_{\pi_{\mathrm{ji}}}\,\Big{[}\,\sum_{i} \log\frac{\pi_{i}(u_{i}|\mathbf{s}^{\prime})}{\pi_{i}(u_{i}|s_{i}^{\prime+})}\, \Big{]}\Bigg{]}.\]

Then, rewrite the update rule as:

\[Q_{\mathrm{jit}}(\mathbf{s},\mathbf{u})\gets r_{\pi_{\mathrm{ji}}}(\mathbf{s},\mathbf{u}) +\gamma\,\mathbb{E}_{\mathbf{s}^{\prime},\mathbf{u}^{\prime}\sim\pi_{\mathrm{ji}}}[Q_{ \mathrm{jit}}(\mathbf{s}^{\prime},\mathbf{u}^{\prime})].\]

Last, apply the standard convergence results for policy evaluation (Sutton and Barto, 2018). 

### Proof of Lemma 2

**Lemma 2** (**Individual Policy Improvement**).: _Let \(\pi_{i}^{\mathrm{new}}\) be the optimizer of the maximization problem in (7). Then, we have \(Q^{\pi_{\mathrm{ji}}^{\mathrm{new}}}_{\mathrm{jit}}\)\((\mathbf{s},\mathbf{u})\geq Q^{\pi_{\mathrm{ji}}^{\mathrm{old}}}_{\mathrm{ji}}\)\((\mathbf{s},\mathbf{u})\) for all \((\mathbf{s},\mathbf{u})\in|S|\times|U|\) with \(|U|<\infty\), where \(\pi_{\mathrm{jit}}^{\mathrm{old}}(\mathbf{u}|\mathbf{s})=\prod_{i}\pi_{i}^{\mathrm{ old}}(u_{i}|\mathbf{s})\) and \(\pi_{\mathrm{jit}}^{\mathrm{new}}(\mathbf{u}|\mathbf{s})=\prod_{i}\pi_{i}^{\mathrm{ new}}(u_{i}|\mathbf{s})\)._

Proof.: As \(\pi_{i}^{\mathrm{new}}\) optimizes (7), we can have:

\[\mathbb{E}_{\pi_{i}^{\mathrm{new}}}\,\Big{[}Q^{\pi_{\mathrm{jit}}^{\mathrm{old }}}_{i}(\mathbf{s},\mathbf{u}_{i})-\alpha\log\frac{\pi_{i}^{\mathrm{new}}(u_{i}|\mathbf{s })}{\pi_{i}(u_{i}|s_{i}^{+})}\Big{]}\geq\mathbb{E}_{\pi_{\mathrm{ji}}^{\mathrm{ old}}}\,\Big{[}Q^{\pi_{\mathrm{jt}}^{\mathrm{old}}}_{i}(\mathbf{s},u_{i})-\alpha \log\frac{\pi_{i}^{\mathrm{old}}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{+})}\Big{]}. \tag{19}\]

Since we assume that:

\[Q^{\pi_{\mathrm{ji}}}_{\mathrm{jit}}(\mathbf{s},\mathbf{u})=\sum_{i}w_{i}(\mathbf{s})*Q^{ \pi_{i}}_{i}(\mathbf{s},u_{i})+b(\mathbf{s}),\]

we can have:

\[\mathbb{E}_{\mathbf{u}\sim\pi_{\mathrm{ji}}^{\mathrm{new}}}\,\Bigg{[} Q^{\pi_{\mathrm{jit}}^{\mathrm{old}}}_{\mathrm{jit}}(\mathbf{s},\mathbf{u})- \alpha\sum_{i}\log\frac{\pi_{i}^{\mathrm{new}}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_ {i}^{+})}\Bigg{]}\] \[=\mathbb{E}_{\mathbf{u}\sim\pi_{\mathrm{ji}}^{\mathrm{new}}}\,\Bigg{[} \,\sum_{i}w_{i}(\mathbf{s})*Q^{\pi_{\mathrm{jt}}^{\mathrm{old}}}_{i}(\mathbf{s},u_{i}) +b(\mathbf{s})-\alpha\sum_{i}\log\frac{\pi_{i}^{\mathrm{new}}(u_{i}|\mathbf{s})}{\pi_{i }(u_{i}|s_{i}^{+})}\,\Bigg{]}\] \[=\sum_{i}\mathbb{E}_{\mathbf{u}_{i}\sim\pi_{\mathrm{ji}}^{\mathrm{new} }}\,\Bigg{[}\,w_{i}(\mathbf{s})*Q^{\pi_{\mathrm{jt}}^{\mathrm{old}}}_{i}(\mathbf{s},u_{i })-\alpha\log\frac{\pi_{i}^{\mathrm{new}}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{ +})}\,\Bigg{]}+b(\mathbf{s})\] \[\geq\sum_{i}\mathbb{E}_{\mathbf{u}_{i}\sim\pi_{\mathrm{ji}}^{\mathrm{ old}}}\,\Bigg{[}\,w_{i}(\mathbf{s})*Q^{\pi_{\mathrm{jt}}^{\mathrm{old}}}_{i}(\mathbf{s},u_{i })-\alpha\log\frac{\pi_{i}^{\mathrm{old}}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{ +})}\,\Bigg{]}+b(\mathbf{s})\] \[=\mathbb{E}_{\mathbf{u}\sim\pi_{\mathrm{ji}}^{\mathrm{old}}}\,\Bigg{[} Q^{\pi_{\mathrm{ji}}^{\mathrm{old}}}_{\mathrm{jit}}(\mathbf{s},\mathbf{u})-\alpha\sum_{i} \log\frac{\pi_{i}^{\mathrm{old}}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{+})}\, \Bigg{]}\] \[=V^{\pi_{\mathrm{ji}}^{\mathrm{old}}}_Last, considering the modified Bellman equation, the following holds:

\[Q_{\rm jt}^{\pi_{\rm jt}^{\rm old}}\left(\mathbf{s},\mathbf{u}\right) =r(\mathbf{s},\mathbf{u})+\gamma\operatorname{\mathbb{E}}_{\mathbf{s}^{\prime} }\left[V_{\rm jt}^{\pi_{\rm jt}^{\rm old}}\left(\mathbf{s}^{\prime}\right)\right]\] \[\leq r(\mathbf{s},\mathbf{u})+\gamma\operatorname{\mathbb{E}}_{\mathbf{s}^{ \prime}}\left[\operatorname{\mathbb{E}}_{\mathbf{u}^{\prime}\sim\pi_{\rm jt}^{\rm new }}\left[Q_{\rm jt}^{\pi_{\rm jt}^{\rm old}}\left(\mathbf{s}^{\prime},\mathbf{u}^{ \prime}\right)-\alpha\sum_{i}\log\frac{\pi_{i}^{\rm new}(u_{i}^{\prime}|\mathbf{s} ^{\prime})}{\pi_{i}(u_{i}^{\prime}|s_{i}^{\prime+})}\right]\right]\] \[\vdots\] \[\leq Q_{\rm jt}^{\pi_{\rm jt}^{\rm new}}\left(\mathbf{s},\mathbf{u}\right),\]

where we have repeatedly expanded \(Q_{\rm jt}^{\pi_{\rm jt}^{\rm old}}\) on the RHS by applying the modified Bellman equation and the inequality in (20). 

### Proof of Theorem 1

**Theorem 1** (Multi-Agent Policy Iteration with a Fixed Marginal Distribution).: _For any joint policy \(\pi_{\rm jt}\), if we repeatedly apply joint policy evaluation and individual policy improvement. Then the joint policy \(\pi_{\rm jt}(\mathbf{u}|\mathbf{s})=\prod_{i=1}^{n}\pi_{i}(u_{i}|\mathbf{s})\) will eventually converge to \(\pi_{\rm jt}^{*}\), such that \(Q_{\rm jt}^{\pi_{\rm jt}^{*}}(\mathbf{s},\mathbf{u})\geq Q_{\rm jt}^{\pi_{\rm jt}}(\bm {s},\mathbf{u})\) for all \(\pi_{\rm jt}\), assuming \(|U|<\infty\)._

Proof.: First, by Lemma 2, the sequence \(\{\pi_{\rm jt}^{k}\}\) monotonically improves with \(Q_{\rm jt}^{\pi_{\rm jt}^{k+1}}\geq Q_{\rm jt}^{\pi_{\rm jt}^{k}}\). Since the augmented reward is bounded, then \(Q_{\rm jt}^{\pi_{\rm jt}^{k}}\) is bounded. Thus, this sequence must converge to some \(\pi_{\rm jt}^{*}\). Then, at convergence, we have the following inequality:

\[\operatorname{\mathbb{E}}_{\pi_{i}^{*}}\Big{[}Q_{i}^{\pi_{i}^{*}}(\mathbf{s},u_{i} )-\alpha\log\frac{\pi_{i}^{*}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{+})}\Big{]} \geq\operatorname{\mathbb{E}}_{\pi_{i}}\Big{[}Q_{i}^{\pi_{i}^{*}}(\mathbf{s},u_{i })-\alpha\log\frac{\pi_{i}(u_{i}|\mathbf{s})}{\pi_{i}(u_{i}|s_{i}^{+})}\Big{]}, \forall\pi_{i}\neq\pi_{i}^{*}.\]

Using the same iterative argument as in the proof of Lemma 2, we get \(Q_{\rm jt}^{\pi_{\rm jt}^{*}}(\mathbf{s},\mathbf{u})\geq Q_{\rm jt}^{\pi_{\rm jt}}(\bm {s},\mathbf{u})\) for all \((\mathbf{s},\mathbf{u})\in|S|\times|U|\). That is, the state-action value of any other policy \(\pi_{\rm jt}\) is lower than or equal to that of the converged policy \(\pi_{\rm jt}^{*}\). Therefore, \(\pi_{\rm jt}^{*}\) is the optimal joint policy. 

### Proof of Theorem 2

**Theorem 2** (Convergence of Constrained Individual Policy Improvement).: _The optimization problem in (10) can be solved by iterating in an alternate fashion through the following two equations:_

\[\pi_{i}^{m}(u_{i}|s_{i}^{+})=\sum_{s_{i}^{*}}\hat{\rho}(s_{i}^{*}|s_{i}^{+}) \pi_{i}^{m}(u_{i}|s_{i}^{+},s_{i}^{*})\]

\[\pi_{i}^{m+1}(u_{i}|s_{i}^{+},s_{i}^{-})=\frac{\pi_{i}^{m}(u_{i}|s_{i}^{+}) \exp(Q_{i}(\mathbf{s},u_{i})/\alpha)}{\sum_{u_{i}}\pi_{i}^{m}(u_{i}|s_{i}^{+}) \exp(Q_{i}(\mathbf{s},u_{i})/\alpha)},\]

_where \(m\) refers to the iteration index. Denoting the total number of iterations as \(M\), the presented scheme converges at a rate of \(O(1/M)\) to an optimal policy \(\pi_{i}^{*}\) for any given bounded utility function \(Q_{i}\) and any initial policy \(\pi_{i}^{0}\)._

Proof.: First, we notice that for a fixed \(\pi_{i}(u_{i}|s_{i}^{+},s_{i}^{*})\), we can have its optimal marginal as constrained in (11):

\[\pi_{i}(u_{i}|s_{i}^{+})=\sum_{s_{i}^{*}}\hat{\rho}(s_{i}^{*}|s_{i}^{+})\pi_{i }(u_{i}|s_{i}^{+},s_{i}^{*}).\]

Then, for a fixed marginal \(\pi_{i}(u_{i}|s_{i}^{+})\), we can have the optimal \(\pi_{i}(u_{i}|s_{i}^{+},s_{i}^{*})\) by solving (10) via standard variational calculus:

\[\pi(u_{i}|s_{i}^{+},s_{i}^{-})=\frac{\pi(u_{i}|s_{i}^{+})\exp(Q_{i}(\mathbf{s},u_{ i})/\alpha)}{\sum_{u_{i}}\pi(u_{i}|s_{i}^{+})\exp(Q_{i}(\mathbf{s},u_{i})/\alpha)}.\]

Lastly, with the above two equations, we can apply Theorem 1 in Leibfried and Grau-Moya (2020) to finish our proof.

Experiment Settings and Implementation Details

### Matrix Game

In the matrix game, we use a learning rate of \(3\times 10^{-4}\) for all algorithms. For the algorithm that uses mutual information as the augmented reward, we set the number of Blahut-Arimoto iterations to 1. For algorithms that use mutual information and entropy as the augmented reward, we fix \(\alpha\) as 0.5. The batch size used in the experiment is 64. Critics and polices used in the experiments consist of one hidden layer of 64 units with ELU non-linearity. For the mixer network, we use a hypernetwork similar to QMIX (Rashid et al., 2018), except no non-linearity is used. The environment and model are implemented in Python. All models are built by PyTorch and are trained via 1 Nvidia RTX 1060 GPU to conduct all the experiments. Each experiment takes roughly 1 hour.

### Smac

In StarCraft II, we use a learning rate of \(5\times 10^{-4}\) for all algorithms. The structure of the critic network and the mixer network of MIPI are the same as REFIL (Iqbal et al., 2021) except no non-linearity is used in the mixer of MIPI. The number of Blahut-Arimoto iterations is set to 4 for MIPI in this experiment. The policy network of MIPI shares all layers with the critic network except the last layer of the policy network being a different fully-connected layer. The target networks will be updated once every 200 training episodes for all algorithms. The temperature parameters \(\alpha\) and \(\alpha_{i}\) are fixed as 0.03 in SZ and CSZ and fixed as 0.1 in MMM for MIPI and Entropy. For REFIL, AQMIX, and CollaQ, we use their default settings. For CollaQ, as the original implementation is based on a different SMAC environment where the entity-level observation is not available, we re-implement CollaQ with minimum changes to adapt the entity-level observation based on the framework provided in REFIL to ensure fairness of comparison. For MAPPO, as there is no published version of MAPPO for dynamic team compositions, we choose to implement MAPPO following Papoudakis et al. (2021), with additional attention modules used in the policy and the critic to handle dynamic team compositions. The environment and model are implemented in Python. All models are built by PyTorch and are trained via a mixture of 4 Nvidia A100, 4 RTX 3090, and 1 RTX 2080 TI GPUs to conduct all the experiments. Each experiment takes 6 to 32 hours depending on the algorithms and scenarios. Our implementation of MIPI is based on REFIL (Iqbal et al., 2021) with MIT license. It is worth noting that, although we assume full observability for the rigorousness of proof, the trajectory of each agent is used to replace state \(s\) for each agent as input to settle the partial observability in all SMAC experiments.

### Resource Collection

In Resource Collection, we use a learning rate of \(5\times 10^{-4}\) for all algorithms. The structure of the critic network and the mixer network of MIPI are the same as REFIL (Iqbal et al., 2021) except no non-linearity is used in the mixer of MIPI. The number of Blahut-Arimoto iterations is set to 1 for MIPI in this experiment. The policy network of MIPI shares all layers with the critic network except the last layer of the policy network being a different fully-connected layer. The target networks will be updated once every 200 training episodes for all algorithms. The temperature parameters \(\alpha\) and \(\alpha_{i}\) are fixed as 0.05 in Resource Collection for MIPI. For REFIL, AQMIX, and CollaQ, we use their default settings. For CollaQ, as the original implementation is based on a different SMAC environment where the entity-level observation is not available, we re-implement CollaQ with minimum changes to adapt the entity-level observation based on the framework provided in REFIL to ensure fairness of comparison. For MAPPO, as there is no published version of MAPPO for dynamic team compositions, we choose to implement MAPPO following Papoudakis et al. (2021), with additional attention modules used in the policy and the critic to handle dynamic team compositions. The environment and model are implemented in Python. All models are built by PyTorch and are trained via 4 Nvidia RTX 3090 GPUs to conduct all the experiments. Each experiment takes roughly 20 hours. Our implementation of MIPI is based on REFIL (Iqbal et al., 2021) with MIT license. It is worth noting that, although we assume full observability for the rigorousness of proof, the trajectory of each agent is used to replace state \(s\) for each agent as input to settle the partial observability in all SMAC experiments. As suggested by previous research (Liu et al., 2021; Shao et al., 2022), random sub-group partitioning does not work well in Resource Collection, therefore we choose not to use it for MIPI in this experiment.

Training performance on SMAC

In this section, we additionally provide the learning curves of all algorithms used in Section 5.2. As we can see from Figure 3, these algorithms achieve similar training performance, except Entropy.

## Appendix D More Experiments

### Resource Collection

In this section, We further evaluate MIPI on Resource Collection, which is a more challenging scenario in terms of the level of collaboration used by COPA (Liu et al., 2021). During training, the map randomly initializes 3-5 agents, and during the evaluation, we will have 6-8 agents. We plot the curve of training and evaluation performance in Figure 4. As we can see, MIPI outperforms the baselines by a large margin, which indicates that MIPI can also perform well in scenarios requiring strong collaboration.

### Ablation on Alpha

In this section, We further include the ablation study on the impact of alpha. We train MIPI with different alpha on 3-5 agents scenarios and evaluate their performance on 6-8 agents scenarios. We use alpha=\(\infty\) to represent the case where team-related information is completely removed (it is worth noting how this is different from actually set alpha=\(\infty\) in MIPI). Results are summarized in Table 3. As we can see, unless alpha is set unreasonably, MIPI can always achieve better generalization ability without sacrificing the training performance. It's worth noting that alpha=\(\infty\) outperforms Value here, which further indicates that reducing the dependency on team-related information promotes generalization, even when the team-related information is completely removed. However, this strategy is not widely effective and sacrifices the training performance too much in some cases (see MMM), which further leads to a decay in both training and evaluation. In contrast, our method uses alpha to control the degree of dependency on team-related information, which provides more flexibility.

Figure 4: Learning curves of all the methods in Resource Collection, where the unit of x-axis is 1M timesteps and y-axis represents the return.

Figure 3: Learning curves of all the methods in SMAC, where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.

### Performance on Higher Level Driver

When working on the follow-up project of this paper, we noticed that REFIL can achieve better generalization results in MMM with a higher-level NVIDIA driver without any code-level change.

Figure 5: Learning curves of all the methods in SMAC, where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Tasks\#Agent} & \multirow{2}{*}{Alpha} & \multicolumn{2}{c|}{Training} & \multicolumn{3}{c|}{Evaluation} \\ \cline{3-6}  & & 3-5 & 6 & 7 & 8 \\ \hline \hline \multirow{8}{*}{SZ} & 0 (Value) & 0.621\(\pm\)0.042 & 0.336\(\pm\)0.075 & 0.275\(\pm\)0.114 & 0.154\(\pm\)0.052 \\ \cline{2-6}  & 0.01 & **0.672\(\pm\)**0.02 & 0.394\(\pm\)0.065 & 0.365\(\pm\)0.068 & 0.261\(\pm\)0.092 \\ \cline{2-6}  & 0.03 (MIPI) & 0.659\(\pm\)0.02 & **0.453\(\pm\)**0.08 & 0.404\(\pm\)0.062 & 0.276\(\pm\)0.076 \\ \cline{2-6}  & 0.05 & 0.643\(\pm\)0.02 & 0.447\(\pm\)0.062 & **0.408\(\pm\)**0.054 & **0.313\(\pm\)**0.069 \\ \cline{2-6}  & 0.1 & 0.475\(\pm\)0.073 & 0.277\(\pm\)0.125 & 0.26\(\pm\)0.093 & 0.146\(\pm\)0.075 \\ \cline{2-6}  & 0.5 & 0.175\(\pm\)0.053 & 0.056\(\pm\)0.015 & 0.129\(\pm\)0.026 & 0.043\(\pm\)0.021 \\ \cline{2-6}  & \(\infty\) & 0.546\(\pm\)0.069 & 0.429\(\pm\)0.036 & 0.389\(\pm\)0.038 & 0.221\(\pm\)0.004 \\ \hline \hline \multirow{8}{*}{CSZ} & 0 (Value) & 0.542\(\pm\)0.059 & 0.368\(\pm\)0.083 & 0.207\(\pm\)0.076 & 0.172\(\pm\)0.112 \\ \cline{2-6}  & 0.01 & **0.592\(\pm\)**0.02 & 0.378\(\pm\)0.033 & **0.364\(\pm\)**0.073 & **0.304\(\pm\)**0.056 \\ \cline{2-6}  & 0.03 (MIPI) & 0.548\(\pm\)0.032 & **0.42\(\pm\)**0.102 & 0.297\(\pm\)0.112 & 0.261\(\pm\)0.09 \\ \cline{2-6}  & 0.05 & 0.506\(\pm\)0.046 & 0.417\(\pm\)0.092 & 0.223\(\pm\)0.094 & 0.192\(\pm\)0.091 \\ \cline{2-6}  & 0.1 & 0.344\(\pm\)0.05 & 0.218\(\pm\)0.16 & 0.113\(\pm\)0.079 & 0.098\(\pm\)0.086 \\ \cline{2-6}  & \(\infty\) & 0.506\(\pm\)0.076 & 0.368\(\pm\)0.064 & 0.309\(\pm\)0.056 & 0.27\(\pm\)0.07 \\ \hline \hline \multirow{8}{*}{MMMM} & 0 (Value) & 0.545\(\pm\)0.048 & 0.505\(\pm\)0.058 & 0.391\(\pm\)0.083 & 0.319\(\pm\)0.105 \\ \cline{2-6}  & 0.05 & **0.59\(\pm\)**0.008 & **0.59\(\pm\)**0.053 & **0.526\(\pm\)**0.055 & 0.426\(\pm\)0.152 \\ \cline{1-1} \cline{2-6}  & 0.1 (MIPI) & 0.548\(\pm\)0.023 & 0.495\(\pm\)0.054 & 0.447\(\pm\)0.041 & **0.467\(\pm\)**0.067 \\ \cline{1-1} \cline{2-6}  & 0.5 & 0.277\(\pm\)0.042 & 0.158\(\pm\)0.094 & 0.18\(\pm\)0.105 & 0.139\(\pm\)0.056 \\ \cline{1-1} \cline{2-6}  & \(\infty\) & 0.396\(\pm\)0.07 & 0.432\(\pm\)0.018 & 0.383\(\pm\)0.041 & 0.315\(\pm\)0.061 \\ \hline \end{tabular}
\end{table}
Table 3: Final performance on all SMAC maps. MIPI is compared with the ablation baseline. We bold the best mean performance for each map.

The results are shown in Figure 5, where all algorithms are trained in a single platform that REFIL achieves better results. As we can see, although REFIL achieves better generalization results in some cases, MIPI can still outperform these baselines in terms of both speed and final performance by properly setting \(\alpha\) and \(\alpha_{i}\) (0.01 for sz, 0.015 for csz and 0.05 for MMM).