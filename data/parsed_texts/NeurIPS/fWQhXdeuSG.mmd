# Pretrained Optimization Model for Zero-Shot Black Box Optimization

 Xiaobin Li

Xidian University

22171214784@stu.xidian.edu.cn

&Kai Wu

Xidian University

kwu@xidian.edu.cn

&Yujian Betterrest Li

Xidian University

bebetterest@outlook.com

&Xiaoyu Zhang

Xidian University

xiaoyuzhang@xidian.edu.cn

&Handing Wang

Xidian University

hdwang@xidian.edu.cn

&Jing Liu

Xidian University

neouma@mail.xidian.edu.cn

Corresponding author

###### Abstract

Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/nnja-wm/POM/.

## 1 Introduction

Black box optimization, including tasks like hyperparameter optimization (HPO) [1], neuroevolution [2; 3; 4], neural architecture search (NAS) [5], and algorithm selection [6], is very important. In these scenarios, the algorithm can evaluate \(f(\mathbf{x})\) for any solution \(\mathbf{x}\); however, access to additional information about \(f\), such as the Hessian and gradients, is unavailable.

Addressing diverse BBO problems necessitates the tailored design of specific algorithms to achieve satisfactory performance. Crafting these algorithms typically demands substantial expertise. Therefore, it is crucial to ensure reliable and robust performance of the optimizer in various applications, called zero-shot optimization. Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer.

The studies [7, 8, 9] employed Transformer or diffusion models to pretrain model-based optimizers using offline datasets. While effective, these methods primarily fit optimization trajectories of other BBO algorithms to a specific task, potentially requiring retraining for new tasks, limiting their ability to zero-shot optimization. Subsequently, [10, 11] introduced two learned optimization frameworks for meta-learning evolution strategy (ES) and genetic algorithm (GA). However, the performance of these two methods on zero-shot optimization is weaker than that of CMA-ES [12] (see Section 4.2).

To address zero-shot optimization, especially for continuous optimization, we introduce a population-based Pretrained Optimization Model, called POM. Leveraging multiple individuals, population-based optimizers gain a better understanding of the fitness landscape. The core of the optimizer is how to design optimization strategies that sample better solutions. Inspired by the solution-producing mechanism of evolutionary computation, we design powerful POM blocks to form a general optimization strategy representation framework. Drawing inspiration from [13], we introduce an end-to-end gradient-based training method for POM, termed _MetaGBT_ (Meta Gradient-Based Training), ensuring stable and rapid training for POM. Pretraining POM on a set of training functions with _MetaGBT_ ensures good optimization strategy. Our contributions can be summarized as follows:

* **Excellent ability to solve zero-shot BBO**. We develop a efficient POM for zero-shot BBO, demonstrating a substantial performance advantage over state-of-the-art black-box optimizers.
* **Excellent ability to solve few-shot BBO**. Few-shot optimization is the existence of a small budget of function evaluations for the target task to tune the optimizer for better performance. More than 30% performance improvement can be obtained with 25 random function evaluations.

## 2 Related Work

**Heuristic Population-based BBO Algorithms**. Numerous metaheuristic population-based algorithms, such as genetic algorithms [14], evolution strategies [15, 16, 17], particle swarm optimization [18, 19], and differential evolution [20, 21], have been devised to address optimization problems. Notably, CMA-ES [12] and L-SHADE [22] stand out as state-of-the-art methods for BBO. However, these approaches rely on manually designed components, exhibiting inefficiency and fragility when confronted with new tasks. In contrast, the proposed POM can autonomously acquire optimization strategies from problem instances, mitigating the aforementioned limitations.

**Pretrained Population-based BBO Algorithms**. Pre-training BBO algorithms can be categorized into two types within the meta-learning framework. The first type frames meta-learning BBO algorithms as a bi-level optimization problem [23]. For instance, [24] leverages meta-learning to infer population-based black-box optimizers that automatically adapt to specific task classes. LES [25] designs a self-attention-based search strategy for discovering effective update rules for evolution strategies through meta-learning. Subsequent works like LGA [10] utilize this framework to discover the update rules of Gaussian genetic algorithms via Open-ES [26]. The second type models the meta-learning of a BBO algorithm as a reinforcement learning problem. [27] meta-learn a policy that adjusts the mutation step-size parameters of CMA-ES [12]. Category one faces the curse of dimensionality, where an escalating number of model parameters leads to skyrocketing training difficulty, impeding the development of intricate strategies. In contrast, category two, which models meta-learning optimizers as reinforcement learning tasks, grapples with training instability. POM, employing a gradient-based end-to-end training approach, successfully bypasses the curse of dimensionality, ensuring stable training.

**LLM for Optimization**. In line with POMs, various optimization approaches leveraging Large Language Models (LLMs) have emerged to address diverse problem domains, including NP-hard problems [28, 29], algorithm evolution [30, 31, 32, 33], reward design [34], and Neural Architecture Search (NAS) [35, 36]. Notably, LLMs play a role in sampling new solutions. However, their optimization strategies depend on externally introduced natural selection mechanisms and are less effective in numerical optimization scenarios [37]. LLMaMoCo [38] and EoH [39] use LLM to generate code to solve optimization problems, but the performance of LLMaMoCo depends on carefully designed instructions and prompts, and EoH has expensive evaluation costs. TNPs [40], ExPT [41] and LICO [42] use transformer structures to solve the BBO problem and have achieved good results. However, TNPs requires contextual information of the target problem, and neither ExPT nor LICO can be directly used to solve tasks with different dimensions from the training task. These methods lack the universal applicability as pretrained BBO models due to a deficiency in generating capabilities across tasks.

All the above methods cannot be the zero-shot optimizer. The first two categories need to adjust the hyperParameters when optimizing the new tasks, while the latter must fine-tune the instructions to achieve satisfactory results.

## 3 Pretrained Optimization Model

### Problem Definition

A black-box optimization problem can be transformed as a minimization problem, and constraints may exist for corresponding solutions: \(\min\limits_{\mathbf{x}}\ f(\mathbf{x}),s.t.\ x_{i}\in[l_{i},u_{i}]\), where \(\mathbf{x}=(x_{1},x_{2},\cdots,x_{d})\) represents the solution of optimization problem \(f\), the lower and upper bounds \(\mathbf{l}=(l_{1},l_{2},\cdots,l_{d})\) and \(\mathbf{u}=(u_{1},u_{2},\cdots,u_{d})\), and \(d\) is the dimension of \(\mathbf{x}\). For more background information on evolutionary algorithms, see Appendix A.

**Definition 1 Zero-shot Optimization**.: _Zero-shot optimization refers to an optimizer that is applied directly to solve a continuous black-box optimization problem \(f\) without any tuning. This means that the optimizer does not require any contextual information about \(f\) and can be directly used to handle problems of any dimensionality._

**Definition 2 Few-shot Optimization**.: _Alternatively, it is permissible to fine-tune the optimizer using a small portion of the function evaluation budget for the objective task, and then use the fine-tuned optimizer to solve \(f\)._

### Classic Population Optimization Algorithm

In this section, we use Differential Evolution (DE) as an example to review classic evolutionary algorithms. DE [20; 43] is a prominent family within evolutionary algorithms (EAs), known for its advantageous properties such as rapid convergence and robust performance [44; 45]. The optimization strategy of DE primarily involves mutation and crossover operations.

The classic DE/rand/1 crossover operator is illustrated in Eq. (1) (additional examples are listed in Appendix A.2). Each mutation strategy can be viewed as a specific instance of Eq. (2); Further details are provided in Appendix A.2. Additionally, we represent the mutation strategy in a matrix form, as shown in Eq. (3). The matrix \(\mathbf{S}\) evolves with the generation index \(t\), indicating that the mutation strategy adapts across different generations. Consequently, we propose a module to enhance the performance of the mutation operation, which leverages the information from the population of the \(t\)th generation to generate \(\mathbf{S}^{t}\). This serves as the motivation for our design of the LMM.

\[\mathbf{v}^{t}_{i}=\mathbf{x}^{t}_{r1}+F\cdot(\mathbf{x}^{t}_{r2}-\mathbf{x}^ {t}_{r3})\] (1)

In the crossover phase at step \(t\), DE uses a fixed crossover probability \(cr^{t}_{i}\in[0,1]\) for each individual \(\mathbf{x}^{t}_{i}\) in the population, as shown in Eq. (9). The crossover strategy for the entire population can then be expressed as a vector \(\mathbf{cr}^{t}=(cr^{t}_{1},cr^{t}_{2},\cdots,cr^{t}_{N})\). Our goal is to design a module that adaptively generates \(\mathbf{cr}^{t}\) using the information from the population. This approach allows for the automatic design of the crossover strategy by controlling the parameter \(cr\). This serves as the motivation for our design of LCM.

### Design of POM

A population consists of \(n\) individuals, denoted as \(\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}\}\). In this paper, \(\mathbf{X}\) is also treated as \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}]^{T}\) to support matrix operations. We feed POM an initial random population \(\mathbf{X}^{0}\) at step 0, specify the evolution generation \(T\) for it, and hope that it can generate a population \(\mathbf{X}^{T}\) close to the global optimum at step \(T\), as shown in \(\mathbf{X}^{T}=POM(\mathbf{X}^{0},T|\theta)\), where \(\theta\in\Omega\) is the parameters of POM, where \(\Omega\) stands for the strategy space. The goal of training POM is to find an optimal \(\theta\) in \(\Omega\). As shown in Fig. 1, POM consists of LMM, LCM and SM.

LMM

LMM generates candidate solutions \(\mathbf{v}_{i}^{t}\) for individual \(\mathbf{x}_{i}^{t}\) through Eq. (2), which enables the population information to be fully utilized in the process of generating candidate solutions \(\mathbf{v}_{i}^{t}\).

\[\mathbf{v}_{i}^{t}=\sum_{j}^{N}w_{i,j}\mathbf{x}_{j}^{t}\quad(\forall w_{i,j} \in\mathbb{R},w_{i,i}\neq 0)\] (2)

Further, we organize Eq. (2) into a matrix form, as shown in Eq. (3).

\[\mathbf{V}^{t}=\mathbf{S}^{t}\times\mathbf{X}^{t}\] (3)

\(\mathbf{X}^{t}\in\mathbb{R}^{N\times d}\) is the population in generation \(t\) and \(\mathbf{S}^{t}\in\mathbb{R}^{N\times N}\). \(\mathbf{S}\) evolves with each change in \(t\), signifying a mutation strategy that adapts across generations. Consequently, it is imperative to devise a module that leverages information from the population at generation \(t\) to generate \(\mathbf{S}^{t}\). Any mutation operator of differential evolution, such as the classic DE/rand/1 mutation operator, can be converted into Equation (3) in the specific case of \(\mathbf{S}\) (see Appendix A.2 for details). At the same time, the crossover operation of GAs can also be generalized into the form of Equation (3) [46].

The function of LMM is designed based on Multi-head self-attention (MSA) [47], as shown as follows:

\[\mathbf{S}^{t}=LMM(\mathbf{H}^{t}|\theta_{1})\] (4)

where \(\theta_{1}=\{\mathbf{W}_{m1},\mathbf{W}_{m2},\mathbf{W}_{m3},\mathbf{b}_{m1}, \mathbf{b}_{m2},\mathbf{b}_{m3}\}\) denotes the trainable parameters within LMM, while \(\mathbf{H}^{t}=[\mathbf{h}_{1}^{t},\mathbf{h}_{2}^{t},\cdots,\mathbf{h}_{N}^ {t}]\) serves as LMM's input, encapsulating population information. Each \(\mathbf{h}_{i}^{t}\) incorporates details about \(\mathbf{x}_{i}^{t}\), encompassing: 1) \(\hat{f}_{i}^{t}\): the normalized fitness \(f(\mathbf{x}_{i}^{t})\) of \(\mathbf{x}_{i}^{t}\); 2) \(\hat{r}_{i}^{t}\): the centralized ranking of \(\mathbf{x}_{i}^{t}\). The method for calculating \(\hat{f}_{i}^{t}\) is:

\[\hat{f}_{i}^{t}=\frac{f(\mathbf{x}_{i}^{t})-\mu^{t}}{\sigma^{t}}\] (5)

where \(\mu^{t}\) and \(\sigma^{t}\) denote the mean and standard deviation, respectively, of individual fitness values within the population at time \(t\). We build \(\hat{r}_{i}^{t}\) as follows:

\[\hat{r}_{i}^{t}=(\frac{rank(\mathbf{x}_{i}^{t},\mathbf{X}^{t})}{N}-0.5)\times 2\] (6)

where _rank_ yields the ranking of \(\mathbf{x}_{i}^{t}\) within the population \(\mathbf{X}^{t}\), with values ranging from 1 to \(N\). Thus, LMM utilizes information on the relative fitness of individuals to dynamically generate the strategy \(\hat{\mathbf{S}}^{t}\). \(\hat{r}_{i}^{t}\) serves as position encoding, explicitly offering the ranking information of individuals. Equation (7) details the computation of \(\hat{\mathbf{S}}^{t}\).

\[\begin{split}\hat{\mathbf{H}}^{t}&=Tanh(\mathbf{H}^ {t}\times\mathbf{W}_{m1}+\mathbf{b}_{m1}),\ \ \ \mathbf{Q}^{t}=Tanh(\hat{\mathbf{H}}^{t}\times\mathbf{W}_{m2}+\mathbf{b}_{m2})\\ \mathbf{K}^{t}&=Tanh(\hat{\mathbf{H}}^{t}\times \mathbf{W}_{m3}+\mathbf{b}_{m3}),\ \ \ \hat{\mathbf{S}}^{t}=Tanh(\frac{\mathbf{Q}^{t}\times(\mathbf{K}^{t})^{T}}{\sqrt{(d_ {m})}})\end{split}\] (7)

where _Tanh_ is an activation function. \(\mathbf{W}_{m1}\in\mathbb{R}^{2\times d_{m}}\) and \(\mathbf{W}_{m2},\mathbf{W}_{m3}\in\mathbb{R}^{d_{m}\times d_{m}}\). \(\mathbf{b}_{m1}\), \(\mathbf{b}_{m2}\), and \(\mathbf{b}_{m3}\) are vector with dimension \(d_{m}\). \(\hat{\mathbf{H}}^{t}\in\mathbb{R}^{N\times d_{m}}\), \(\mathbf{Q}^{t},\mathbf{K}^{t}\in\mathbb{R}^{N\times d_{m}}\), and \(\hat{\mathbf{S}}^{t}\in\mathbb{R}^{N\times N}\).

The topological structure of the population significantly influences their information exchange [48]. When all individuals engage in information exchange, the algorithm's convergence may suffer, diversity could diminish, and susceptibility to local optima increases. To address this, we introduce a _mask_ operation during both training and testing phases, where the probability of setting each element in \(\hat{\mathbf{S}}^{t}\) to 0 is \(r_{mask}\). This operation enhances POM's ability to learn efficient and robust strategies, as validated in our experiments. Consequently, \(\mathbf{S}^{t}\) is derived using Eq. (8).

\[\mathbf{S}^{t}=mask(\hat{\mathbf{S}}^{t}|r_{mask})\] (8)

Finally, we get \(\mathbf{V}^{t}\) via Eq. (3).

LcmFor each individual \(\mathbf{x}_{i}^{t}\) at step \(t\), a crossover probability \(cr_{i}^{t}\in[0,1]\) is established. Consequently, the population's crossover strategy is encapsulated in the vector \(\mathbf{cr}^{t}=(cr_{1}^{t},cr_{2}^{t},\cdots,cr_{N}^{t})\). The crossover operation, as depicted in Eq. (9), can be elucidated as follows:

\[\mathbf{u}_{i,k}^{t}=\left\{\begin{array}{ll}\mathbf{v}_{i,k}^{t},&\text{ if }\quad rand(0,1)\leq cr_{i}^{t}\\ \mathbf{x}_{i,k}^{t},&\text{otherwise}\end{array}\right.\quad\forall i\in[1,N]\] (9)The module design should facilitate the adaptive generation of \(\mathbf{cr}^{t}\) by leveraging population information. Executing the crossover operation with \(\mathbf{cr}^{t}\) yields \(\mathbf{U}^{t}=[\mathbf{u}_{1}^{t},\mathbf{u}_{2}^{t},\cdots,\mathbf{u}_{N}^{t}]\).

LCM is designed based on FFN [47], as shown in Eq. (10),

\[\mathbf{cr}^{t}=LCM(\mathbf{Z}^{t}|\theta_{2})\] (10)

where \(\theta_{2}=\{\mathbf{W}_{c1},\mathbf{b}_{c1},\mathbf{W}_{c2},\mathbf{b}_{c2}, \tau\}\) is the parameter of LCM and \(\mathbf{Z}^{t}\in\mathbb{R}^{N\times 3}\) is the population information used by LCM. Here, \(\mathbf{Z}^{t}=[\mathbf{z}_{1}^{t},\mathbf{z}_{2}^{t},\cdots,\mathbf{z}_{N}^ {t}]\). \(\mathbf{z}_{i}^{t}\) represents the relevant information of individual \(\mathbf{x}_{i}^{t}\) and \(\mathbf{X}^{t}\). For example, it can include the ranking information of \(\mathbf{x}_{i}^{t}\), the fitness information of \(\mathbf{x}_{i}^{t}\), the Euclidean distance between \(\mathbf{x}_{i}^{t}\) and \(\mathbf{V}_{i}^{t}\), and the distribution information of individuals within the population (such as the fitness distribution, the distance between pairs of individuals), etc. In this paper, \(\mathbf{z}_{i}^{t}\) includes the following information as a case study: 1) \(\hat{f}_{i}^{t}\): the normalized fitness \(f(\mathbf{x}_{i}^{t})\) of \(\mathbf{x}_{i}^{t}\); 2) \(\hat{r}_{i}^{t}\): the centralized ranking of \(\mathbf{x}_{i}^{t}\); 3) \(sim_{i}^{t}\): the cosine similarity between \(x_{i}^{t}\) and \(v_{i}^{t}\).

\[\mathbf{h}^{t}=Tanh(\mathbf{Z}^{t}\times\mathbf{W_{c1}}+\mathbf{b_{c1}}),\ \ \mathbf{\hat{h}^{t}}=layernorm(\mathbf{h}^{t}|\tau),\ \ \mathbf{cr}^{t}=Sigmoid(\mathbf{\hat{h}^{t}}\times\mathbf{W_{c2}}+\mathbf{b_{c2}})\] (11)

where the activation function _Sigmoid_ maps inputs to the range \((0,1)\). \(\mathbf{W_{c1}}\in\mathbb{R}^{3\times d_{c}}\), \(\mathbf{W_{c2}}\in\mathbb{R}^{d_{c}\times 1}\), \(\tau\) is the learnable parameters of _layernorm_[49]. \(\mathbf{b_{c1}}\) and \(\mathbf{b_{c2}}\) are vectors with dimensions \(d_{c}\) and \(1\), respectively.

Although we derive \(\mathbf{cr}^{t}\) from Eq. (11) as in Eq. (9), the discrete nature of the crossover operator renders it non-differentiable, impeding gradient-based training of the _LCM_ module. To address this limitation, we introduce the _gumbel_softmax_ method [50], providing an efficient gradient estimator that replaces non-differentiable samples from a categorical distribution with differentiable samples from a novel Gumbel-Softmax distribution.

Eq. (12) shows how to perform crossover operations between \(\mathbf{x}_{i}^{t}\) and \(\mathbf{v}_{i}^{t}\) in _LCM_ (\(\forall i\in[1,N]\)).

\[\begin{split}\mathbf{r}_{i}^{t}&=rand(d),\ \ \ \mathbf{cv}_{i}^{t}=gumbel\_softmax(cat(\mathbf{r}_{i}^{t},tile(c_{i}^{t},d))),\\ \mathbf{u}_{i}^{t}&=\mathbf{cv}_{i,0}^{t}\cdot \mathbf{x}_{i}^{t}+\mathbf{cv}_{i,1}^{t}\cdot\mathbf{v}_{i}^{t},\ \ \ \mathbf{U}^{t}=[\mathbf{u}_{1}^{t},\mathbf{u}_{2}^{t},\cdots,\mathbf{u}_{N}^{t}] \end{split}\] (12)

First, the _rand_ function samples uniformly from the range \([0,1]\) to obtain a vector \(\mathbf{r}_{i}^{t}\). Then get \(cr_{i}^{t}\) from \(\mathbf{cr}^{t}\) according to the index. The _tile_ function expands \(cr_{i}^{t}\) into a \(d\)-dimensional vector: \([cr_{i}^{t},cr_{i}^{t},\cdots,cr_{i}^{t}]\). The _cat_ function concatenates them into a matrix as shown below:

\[\begin{bmatrix}r_{i,1}^{t},&r_{i,2}^{t}&\cdots&r_{i,d}^{t}\\ cr_{i}^{t}&cr_{i}^{t}&\cdots&cr_{i}^{t}\end{bmatrix}\] (13)

Here, _gumbel_softmax_ is executed column-wise. For any column, the larger element becomes 1 after _gumbel_softmax_ and 0 otherwise. Therefore, \(\mathbf{cv}_{i}^{t}\in\mathbb{R}^{2\times d}\) may be a matrix like this:

\[\mathbf{cv}_{i}^{t}=\begin{bmatrix}1&0&0&0&1&1&\cdots&1&1\\ 0&1&1&1&0&0&\cdots&0&0\end{bmatrix}\] (14)

Over all FrameworkWe design LMM and LCM to achieve the generation of sample strategy (that is, generate \(\mathbf{S}^{t}\)) and crossover strategy (that is, generate \(\mathbf{cr}^{t}\)), respectively. The overall architecture of POM is shown in Fig. 1. The parameters that need to be trained in _POM_ are \(\theta=\{\theta_{1},\theta_{2}\}\). At time step \(t\), the population is \(\mathbf{X}^{t}\). Initially, we amalgamate the information from \(\mathbf{X}^{t}\) to construct descriptive representations of the population, \(\mathbf{H}^{t}\) and \(\mathbf{Z}^{t}\). _LMM_ adaptively generates \(\mathbf{S}^{t}\) based on \(\mathbf{H}^{t}\).

Figure 1: In the figure, \(\mathbf{X}^{0}\) is the initial random population. (a) The overall architecture of the POM. (b) POM training process. Here \(T\) is the size of the inner loop iteration step during training, and the training function should be differentiable. (c) POM testing process. Here, \(T\) is the number of iterations of the testing process and \(f\) is the target task. \(f\) does not have to be differentiable. Here we directly apply the trained POM to solve \(f\) without requiring gradient information.

The multiplication of \(\mathbf{X}^{t}\) and \(\mathbf{S}^{t}\) yields \(\mathbf{V}^{t}\) (see Eq. (3)). Next, _LCM_ adaptively generates \(\mathbf{cr}^{t}\) based on its input \(\mathbf{Z}^{t}\), and performs a crossover operation based on \(\mathbf{cr}^{t}\) to obtain \(\mathbf{U}^{t}\). Finally, _SM_[51], a _1-to-1_ selection strategy is executed between \(\mathbf{U}^{t}\) and \(\mathbf{X}^{t}\) to produce the next-generation population \(\mathbf{X}^{t+1}\).

\[\mathbf{X}^{t+1}=SM(\mathbf{X}^{t},\mathbf{U}^{t})=tile(l_{x>0}(\mathbf{M}_{F^ {\prime}}-\mathbf{M}_{F}))\odot\mathbf{X}^{t}+tile(1-l_{x>0}(\mathbf{M}_{F^{ \prime}}-\mathbf{M}_{F}))\odot\mathbf{U}^{t}\] (15)

where \(l_{x>0}(x)=1\) if \(x>0\) and \(l_{x>0}(x)=0\) if \(x<0\), and the _tile_ copy function extends the indication matrix to a tensor with size \((N,d)\), \(\mathbf{M}_{F}(\mathbf{M}_{F^{\prime}})\) denotes the fitness matrix of \(\mathbf{X}^{t}(\mathbf{U}^{t})\), and \(\odot\) indicates the pairwise multiplication between inputs.

```
0:\(T\), \(n\), training set \(TS\).
0: The optimal \(\theta\).
1: Randomly sample the parameter \(\theta\) of POM.
2:while not done do
3: Sample \(|TS|\) populations of size \(n\) to obtain \([\mathbf{X}_{0}^{0},\mathbf{X}_{0}^{0},\cdots,\mathbf{X}_{|TS|}^{t}]\).
4:for\(i=1,2,\ldots,|TS|\)do
5: Randomly sample \(\omega^{i}\) for the \(f_{i}\) in \(TS\).
6:endfor
7:for\(t=1,2,\ldots,T\)do
8:for\(i=1,2,\ldots,|TS|\)do
9:\(\mathbf{X}_{i}^{t}\gets POM(\mathbf{X}_{i}^{t-1},1|\theta)\).
10:\(loss_{i}^{t}\gets l_{i}(\mathbf{X}_{i}^{t},\mathbf{X}_{i}^{t-1},f_{i}, \omega^{i},\lambda)\).
11:endfor
12:\(\theta\leftarrow\) Update \(\theta\) based on \(\frac{1}{|TS|}\sum\limits_{i}loss_{i}^{t}\).
13:endfor
14:endwhile ```

**Algorithm 1** MetaGBT

### Tasks, Loss Function & MetaGBT

POM is meticulously crafted as a model amenable to end-to-end training based on gradients. While POM necessitates gradient information from the training task during the training phase, it exhibits the ability to tackle BBO problems in the testing phase without relying on any gradient information. To ensure the acquisition of an efficient, highly robust, and broadly generalizable optimization strategy, POM undergoes training on a diverse set of tasks. Training on these tasks sequentially poses the risk of domain overfitting, local optima entrapment, and diminished generalization performance. Consequently, we introduce a training methodology named _MetaGBT_.

**Tasks**. We form a training task set \(TS=\{f_{i}(\mathbf{X}|\omega^{j})\}\),where \(i\in[1,5]\) and \(j\in[1,N]\), comprising \(4N\) tasks derived from Table 3 in appendix, where \(\omega_{i}\) denotes the task parameter influencing the function's landscape offset. Our selection of these functions for the training task is motivated by their diverse landscape features. The specific landscape features encompassed in \(TS\) are detailed in Appendix B.

**Loss Function**. To avoid bias of different output scales in _TS_, for any function \(f_{i}\) in \(TS\), we design the normalized loss function \(l_{i}(\mathbf{X}^{t},\mathbf{X}^{t-1},f_{i},\omega^{i},\lambda)\). In Equation (16), \(l_{i}^{1}\) calculates the average fitness difference between the input and output of the POM, further normalized within \([0,1]\). This encourages convergence of the algorithm. \(l_{i}^{2}\) uses standard deviation to simulate the distribution of the output population, encouraging diversity in the output population. \(std(\mathbf{X}^{t},j)\) is the standard deviation of the jth dimension of the population. \(\lambda\) is a hyperparameter, and we find that setting it to 0.005 can make model training more stable.

\[\begin{split}\mathbf{X}^{t}&= POM(\mathbf{X}^{t-1},1| \theta)\\ l_{i}^{1}&=\frac{\frac{1}{|\mathbf{X}^{t}|}\sum \limits_{\mathbf{x}\in\mathbf{X}^{t}}f_{i}(\mathbf{x}|\omega^{i})-\frac{1}{| \mathbf{X}^{t-1}|}\sum\limits_{\mathbf{x}\in\mathbf{X}^{t-1}}f_{i}(\mathbf{x }|\omega^{i})}{\left|\frac{1}{|\mathbf{X}^{t-1}|}\sum\limits_{\mathbf{x}\in \mathbf{X}^{t-1}}f_{i}(\mathbf{x}|\omega^{i})\right|},\ \ l_{i}^{2}=\frac{\sum\limits_{j=1}^{d}std(\mathbf{X}^{t},j)}{d},\ \ l_{i}=l_{i}^{1}- \lambda l_{i}^{2}\end{split}\] (16)

**MetaGBT**. The pseudocode for _MetaGBT_ is presented in Algorithm 1. Initially, we sample the _POM_ parameter \(\theta\) from a standard normal distribution. The objective of _MetaGBT_ is to iteratively update \(\theta\) to bring it closer to the global optimum \(\theta^{*}\). In line 2, we sample a population for each task in _TS_. Lines 3, 4 and 5 involve the resampling of task parameters for all tasks in _TS_, thereby altering the task landscape, augmenting training complexity, and enhancing the learning of robust optimization strategies by POM. The final loss function (line 10) is determined by computing the average of the loss functions for all tasks. Subsequently, in line 12, we update \(\theta\) using a gradient-based optimizer, such as Adam [52]. The trained _POM_ is then ready for application in solving an unknown BBO problem, as depicted in Algorithm 1.

## 4 Experiments

### Experimental Setup

We test the performance of POM on the widely used BBO benchmark and two complex real-world problems (see Appendix C). Selected methods include DE (DE/rand/1/bin) [54] and ES ((\(\mu\),\(\lambda\))-ES) as population-based baselines, L-SHADE [22] and CMA-ES [12] as state-of-the-art population-based BBO methods, and LES [25] and LGA [10] as state-of-the-art POMs. POM is trained on \(TS\) with \(T=100\), \(n=100\), and \(d=10\). Detailed parameters for all compared methods are provided in Appendix E. Please refer to Appendix D for the reasons for choosing these algorithms.

### Results

**BBOB [55]**. We evaluate the generalization ability of POM across 24 BBOB functions with dimensions \(d=30\) and \(d=100\), where optimal solutions are located at **0**. Figure 2 presents the critical difference diagram comparing all algorithms (refer to Appendix Tables 4 and 6, and Figures 11, 12 and 13 for detailed results). POM significantly outperforms all methods, showcasing its efficacy across varying dimensions. Despite being trained solely on TF1-TF4 with \(d=10\), POM excels in higher dimensions (\(d=\{30,100,500\}\)), with its performance advantage becoming more pronounced with increasing dimensionality. Particularly on complex problems F21-F24, where global structure is weak, POM lags behind LSHADE but surpasses other methods, attributed to its adaptability through fine-tuning. TurBO [56] is the Bayesian optimization algorithm with the best performance on BBOB [57]. Under little budget conditions, the performance of POM outperforms that of TurBO in most cases (see Appendix G for details).

**Bipedal Walker [58]**. The Bipedal Walker task involves optimizing a fully connected neural network with \(d=874\) parameters over \(k=800\) time steps to enhance robot locomotion control. In Fig. 3(a), LSHADE shows ineffectiveness, while CMA-ES, LSHADE, and LGA suffer from premature convergence. Conversely, POM achieves stable and swift convergence, ultimately attaining the highest score.

**Enduro [58]**. Enduro task entails controlling a strategy with \(d=4149\) parameters across \(k=500\) steps, posing greater difficulty than Bipedal Walker. As depicted in Fig. 3(b), LGA and LES exhibit premature convergence and limited exploration. While CMA-ES initially converges slightly

Figure 3: Experimental results are presented for the Bipedal Walker (a) and Enduro (b), with the vertical axis denoted as \(R\), representing the strategy score. The score corresponds to the total reward acquired by the agent during interactions with the environment.

Figure 2: The critical difference diagram illustrates the performance ranking of seven algorithms across 24 BBOB problems with dimensions \(d=30,100\), employing Wilcoxon-Holm analysis [53] at a significance level of \(p=0.05\). Algorithm positions are indicative of their mean scores across multiple datasets, with higher scores signifying a method consistently outperforming competitors. Thick horizontal lines denote scenarios where there is no statistically significant difference in algorithm performance.

[MISSING_PAGE_EMPTY:8]

initially trained on \(TF1\)-\(TF5\). We calculate the relative performance improvement (RFI) achieved by the fine-tuned POM compared to the base POM, with results displayed in Figure 6. Experimental results indicate that fine-tuning POM leads to significant performance improvements even with a small sample size. The method for obtaining fine-tuning samples is not restricted; for black-box tasks, a surrogate model can be constructed to facilitate fine-tuning.

Size of Training DatasetAny complex problem can be simulated by a polynomial composed of simple basic function terms. To ensure that the optimization strategy learned by POM has robust generalization ability and performance, we should train POM on a set of basic functions.

First, we tested the impact of increasing the number of basic functions in the training set on model performance. Next, we examined the effect of introducing complex functions into the training set. Functions \(TF1-TF5\) are basic simple terms. For example, \(TF1\) is an absolute value term, and \(TF5\) is a square summation term. Functions \(TF6-TF8\) are composite terms composed of several basic functions. For instance, \(TF6\) includes both a cumulative multiplier term and a cosine term. The test results are shown in Figure 5 (a) and (b) (see Appendix Table 8 for details), respectively.

Experimental results indicate that increasing the number of basic functions leads to an overall improvement in POM performance, whereas the introduction of composite terms results in a significant performance decline. This aligns with our hypothesis.

Scale of POMWe explore the performance of POM at different scales, which is shown in Fig. 4 (b) (refer to Appendix Table 9 for additional details). We increase POM's parameter count by perturbing the hidden layers of each module (\(d_{m},d_{c}\)). Six models are constructed in ascending order of parameter count, labeled as _VS_ (very small), \(S\) (small), \(M\) (medium), \(L\) (large), _VL_ (very large), and _XL_ (extra large) (details in the Appendix Table 2). _XL_ achieves the best performance, while _VS_ and \(M\) also perform well. \(S\) exhibits the worst performance, and _VL_ performs worse than \(L\). Two core factors contribute to this phenomenon: the number of parameters and training. We observe a complex relationship between the number of parameters and training difficulty. _VS_, with the fewest

Figure 8: Displayed are visualized outcomes of LMM \(S^{t}\) in BBOB with \(d=100\) using \(n=10\) for clarity. Blank squares in the matrix denote masked portions from Eq. (8). Steps 1, 50, and 100 correspond to the 1st, 50th, and 100th generations in population evolution. The horizontal and vertical axes denote individual rankings, with 1 as the best and 10 as the worst in the population. Each row illustrates the weight assigned to other individuals when executing mutation operations for the respective individual.

Figure 9: Visual analysis results of LCM on BBOB F1, F11, and F24 with \(d=100\), employing \(n=100\), are presented. “Rank” signifies an individual’s position, with rank 5 representing the fifth-ranked individual in the population. Subgraphs depict the evolution of the probability that an individual will undergo crossover across three tasks as the population progresses. For example, (a) illustrates the crossover probability change for the top-ranked individual on F1, F11, and F24 with the number of generations.

parameters, is the easiest to train and performs well on BBOB. Conversely, XL, with a large number of parameters, exhibits the strongest capability to represent strategies, resulting in the best performance. The performance of XL aligns with our expectations. We obtain the following principles: 1) Larger models can have stronger capabilities but are more challenging to train; 2) Training difficulty and model scale do not exhibit a simple linear relationship, warranting further research; 3) Larger models require more functions for effective training.

Time BudgetWe assess the training and test time efficiency of POM across various architectures on BBOB (\(d=10\)) and BBOB (\(d=100\)) respectively, as illustrated in Figure 7. POM demonstrates remarkable efficiency in tackling BBO problems, with negligible training costs relative to its exceptional generalization ability and high performance.

### Visualization Analysis

LMM Learning AnalysisFigure 8 displays \(S^{t}\) for an in-depth analysis of the LMM strategy (refer to Appendix Figure 15-20 for additional details). Key observations and conclusions include: 1) Generally, superior individuals receive higher weights during LMM, showcasing POM's ability to balance exploration and exploitation as the population converges. 2) Across diverse function problems, POM dynamically generates optimization strategies, highlighting its adaptability and contributing to robust generalization. 3) Disadvantaged individuals exhibit a more uniform weight distribution, potentially aiding in their escape from local optima and enhancing algorithm convergence.

LCM Learning AnalysisWe visually examine the LCM strategy, presenting the results in Fig. 9 (refer to Appendix Figure 21-26 for additional details). LCM displays the capacity to adaptively generate diverse strategies for individuals across different ranks in the population, revealing distinct patterns among tasks and rankings. Notably, top-ranking individuals within the top 20, such as those ranked 1st, 5th, and 18th, exhibit a flexible crossover strategy. The dynamic adjustment of crossover probability with population evolution aids in preserving dominant genes and facilitating escape from local optima. Conversely, lower-ranking individuals show an increasing overall probability of crossover, promoting exploration of disadvantaged individuals and enhancing the algorithm's exploration capability. LCM proficiently generates adaptive crossover strategies across tasks, individuals, and convergence stages, significantly boosting both convergence and exploration capabilities.

## 5 Conclusions

We present POM, a novel Pretrained Optimization Model designed to address the inefficiencies of existing methods in zero-shot optimization. Evaluation on BBOB and robot control tasks demonstrates POM's superiority over other black-box optimizers, particularly in high-dimensional scenarios. Additionally, POM excels in solving few-shot optimization problems. Future research avenues include designing enhanced loss functions to optimize POM for both population convergence and diversity, thereby improving overall algorithm performance. In addition, the limitations of model scale and time performance deserve further study (see Appendix I for details).

## Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 62206205 and 62471371, in part by the Young Talent Fund of Association for Science and Technology in Shaanxi, China under Grant 20230129, in part by the Guangdong High-level Innovation Research Institution Project under Grant 2021B0909050008, and in part by the Guangzhou Key Research and Development Program under Grant 202206030003.

## References

* [1] Frank Hutter, Lars Kothoff, and Joaquin Vanschoren. _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019.

* [2] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. _arXiv preprint arXiv:1712.06567_, 2017.
* [3] Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, and Yue-Jiao Gong. SYMBOL: Generating flexible black-box optimizers through symbolic equation learning. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=vLJcd43U7a.
* [4] Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Zhenrui Li, Guojun Peng, Yue-Jiao Gong, Yining Ma, and Zhiguang Cao. Metabox: A benchmark platform for meta-black-box optimization with reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 10775-10795. Curran Associates, Inc., 2023.
* [5] Qing Ye, Yanan Sun, Jixin Zhang, and Jiancheng Lv. A distributed framework for ea-based nas. _IEEE Transactions on Parallel and Distributed Systems_, 32(7):1753-1764, 2021. doi: 10.1109/TPDS.2020.3046774.
* [6] Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, and Yue-Jiao Gong. Deep reinforcement learning for dynamic algorithm selection: A proof-of-principle study on differential evolution. _IEEE Transactions on Systems, Man, and Cybernetics: Systems_, pages 1-13, 2024. doi: 10.1109/TSMC.2024.3374889.
* [7] Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc'aurelio Ranzato, et al. Towards learning universal hyperparameter optimizers with transformers. _Advances in Neural Information Processing Systems_, 35:32053-32068, 2022.
* [8] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Generative pretraining for black-box optimization. _arXiv preprint arXiv:2206.10786_, 2022.
* [9] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion models for black-box optimization. _arXiv preprint arXiv:2306.07180_, 2023.
* [10] Robert Lange, Tom Schaul, Yutian Chen, Chris Lu, Tom Zahavy, Valentin Dalibard, and Sebastian Flennerhag. Discovering attention-based genetic algorithms via meta-black-box optimization. In _Proceedings of the Genetic and Evolutionary Computation Conference_, pages 929-937, 2023.
* [11] Robert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, and Sebastian Flennerhag. Discovering evolution strategies via meta-black-box optimization. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=mFDUOfP3EQH.
* [12] Nikolaus Hansen. The cma evolution strategy: A tutorial. _arXiv preprint arXiv:1604.00772_, 2016.
* [13] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [14] John H Holland. Genetic algorithms. _Scientific american_, 267(1):66-73, 1992.
* [15] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. _Evolutionary Computation_, 9(2):159-195, 2001.
* [16] Nikolaus Hansen, Sibylle D Muller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). _Evolutionary computation_, 11(1):1-18, 2003.
* [17] Raymond Ros and Nikolaus Hansen. A simple modification in cma-es achieving linear time and space complexity. In _International Conference on Parallel Problem Solving from Nature_, pages 296-305. Springer, 2008.

* Kennedy and Eberhart [1995] James Kennedy and Russell Eberhart. Particle swarm optimization. In _Proceedings of ICNN'95-International Conference on Neural Networks_, volume 4, pages 1942-1948. IEEE, 1995.
* Gong et al. [2015] Yue-Jiao Gong, Jing-Jing Li, Yicong Zhou, Yun Li, Henry Shu-Hung Chung, Yu-Hui Shi, and Jun Zhang. Genetic learning particle swarm optimization. _IEEE Transactions on Cybernetics_, 46(10):2277-2290, 2015.
* Storm and Price [1997] Rainer Storm and Kenneth Price. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. _Journal of global optimization_, 11:341-359, 1997.
* Stanovov et al. [2022] Vladimir Stanovov, Shakhnaz Akhmedova, and Eugene Semenkin. Nl-shade-lbc algorithm with linear parameter adaptation bias change for cec 2022 numerical optimization. In _2022 IEEE Congress on Evolutionary Computation (CEC)_, pages 01-08. IEEE, 2022.
* Tanabe and Fukunaga [2014] Ryoji Tanabe and Alex S. Fukunaga. Improving the search performance of shade using linear population size reduction. In _2014 IEEE Congress on Evolutionary Computation (CEC)_, pages 1658-1665, 2014. doi: 10.1109/CEC.2014.6900380.
* Liu et al. [2022] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10045-10067, 2022. doi: 10.1109/TPAMI.2021.3132674.
* Gomes et al. [2021] Hugo Siqueira Gomes, Benjamin Leger, and Christian Gagne. Meta learning black-box population-based optimizers. _arXiv preprint arXiv:2103.03526_, 2021.
* Lange et al. [2023] Robert Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, and Sebastian Flennerhag. Discovering evolution strategies via meta-black-box optimization. In _Proceedings of the Companion Conference on Genetic and Evolutionary Computation_, pages 29-30, 2023.
* Zhang et al. [2017] Xingwen Zhang, Jeff Clune, and Kenneth O Stanley. On the relationship between the openai evolution strategy and stochastic gradient descent. _arXiv preprint arXiv:1712.06564_, 2017.
* Shala et al. [2020] Gresa Shala, Andre Biedenkapp, Noor Awad, Steven Adriaensen, Marius Lindauer, and Frank Hutter. Learning step-size adaptation in cma-es. In _International Conference on Parallel Problem Solving from Nature_, pages 691-706. Springer, 2020.
* Romera-Paredes et al. [2023] Bernardino Romera-Paredes, Mohammadin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. _Nature_, pages 1-3, 2023.
* Meyerson et al. [2023] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. _arXiv preprint arXiv:2302.12170_, 2023.
* Liu et al. [2023] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. _arXiv preprint arXiv:2311.15249_, 2023.
* Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.
* Lehman et al. [2023] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley. Evolution through large models. In _Handbook of Evolutionary Machine Learning_, pages 331-366. Springer, 2023.
* Liu et al. [2024] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language mode, 2024.
* Ma et al. [2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. _arXiv preprint arXiv:2310.12931_, 2023.

* Chen et al. [2023] Angelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level neural architecture search. _arXiv preprint arXiv:2302.14838_, 2023.
* Nasir et al. [2023] Muhammad U Nasir, Sam Earle, Julian Togelius, Steven James, and Christopher Cleghorn. LImatic: Neural architecture search via large language models and quality-diversity optimization. _arXiv preprint arXiv:2306.01102_, 2023.
* Huang et al. [2024] Beichen Huang, Xingyu Wu, Yu Zhou, Jibin Wu, Liang Feng, Ran Cheng, and Kay Chen Tan. Exploring the true potential: Evaluating the black-box optimization capability of large language models. _arXiv preprint arXiv:2404.06290_, 2024.
* Ma et al. [2024] Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong. Llamoco: Instruction tuning of large language models for optimization code generation, 2024. URL https://arxiv.org/abs/2403.01131.
* Liu et al. [2024] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model, 2024. URL https://arxiv.org/abs/2401.02051.
* Nguyen and Grover [2023] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling, 2023. URL https://arxiv.org/abs/2207.04179.
* Nguyen et al. [2024] Tung Nguyen, Sudhanshu Agrawal, and Aditya Grover. Expt: synthetic pretraining for few-shot experimental design. _Advances in Neural Information Processing Systems_, 36, 2024.
* Nguyen and Grover [2024] Tung Nguyen and Aditya Grover. Lico: Large language models for in-context molecular optimization. _arXiv preprint arXiv:2406.18851_, 2024.
* Thangaraj et al. [2009] Radha Thangaraj, Millie Pant, and Ajith Abraham. A simple adaptive differential evolution algorithm. In _2009 world congress on nature & biologically inspired computing (nabic)_, pages 457-462. IEEE, 2009.
* Das et al. [2016] Swagatam Das, Sankha Subhra Mullick, and Ponnuthurai N Suganthan. Recent advances in differential evolution-an updated survey. _Swarm and evolutionary computation_, 27:1-30, 2016.
* Neri and Tirronen [2010] Ferrante Neri and Ville Tirronen. Recent advances in differential evolution: a survey and experimental analysis. _Artificial intelligence review_, 33:61-106, 2010.
* Zhang et al. [2021] Jiangning Zhang, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying Tai, Shuo Chen, Chengjie Wang, Feiyue Huang, and Yong Liu. Analogous to evolutionary algorithm: Designing a unified sequence model. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 26674-26688. Curran Associates, Inc., 2021.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Yu et al. [2021] Yang Yu, Zhenyu Lei, Yirui Wang, Tengfei Zhang, Chen Peng, and Shangce Gao. Improving dendritic neuron model with dynamic scale-free network-based differential evolution. _IEEE/CAA Journal of automatic sinica_, 9(1):99-110, 2021.
* Xu et al. [2019] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.
* Jang et al. [2016] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.
* Wu et al. [2023] Kai Wu, Penghui Liu, and Jing Liu. Decn: Automated evolutionary algorithms via evolution inspired deep convolution network, 2023.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.

* [53] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. Deep learning for time series classification: a review. _Data Mining and Knowledge Discovery_, 33(4):917-963, 2019.
* [54] Swagatam Das and Ponnuthurai Nagaratnam Suganthan. Differential evolution: A survey of the state-of-the-art. _IEEE transactions on evolutionary computation_, 15(1):4-31, 2010.
* [55] Nikolaus Hansen, Anne Auger, Raymond Ros, Olaf Mersmann, Tea Tusar, and Dimo Brockhoff. Coco: A platform for comparing continuous optimizers in a black-box setting. _Optimization Methods and Software_, 36(1):114-144, 2021.
* [56] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [57] Maria Laura Santoni, Elena Raponi, Renato De Leone, and Carola Doerr. Comparison of high-dimensional bayesian optimization algorithms on bob. _arXiv preprint arXiv:2303.00890_, 2023.
* [58] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
* [59] Steffen Finck, Nikolaus Hansen, Raymond Ros, and Anne Auger. Real-parameter black-box optimization benchmarking 2009: Presentation of the noiseless functions. Technical report, Citeseer, 2010.
* [60] Jazzbin. Geatpy: The genetic and evolutionary algorithm toolbox with high performance in python, 2020.

Preliminaries

### Genetic Algorithms

The crossover, mutation, and selection operators form the basic framework of GAs. GA starts with a randomly generated initial population. Then, genetic operations such as crossover and mutation will be carried out. After the fitness evaluation of all individuals in the population, a selection operation is performed to identify fitter individuals to undergo reproduction to generate offspring. Such an evolutionary process will be repeated until specific predefined stopping criteria are satisfied.

CrossoverThe crossover operator generates a new individual \(\mathbf{x}_{i}^{c}\in\mathbb{R}^{d}\) by Eq. (17), and \(cr\) is the probability of the crossover operator.

\[x_{k}^{c}=\begin{cases}x_{k}^{i}&rand(0,1)<cr\\ x_{k}^{j}&otherwise\end{cases}\] (17)

where \(k\in[1,\,\cdots,d]\), \(i\) and \(j\in[1,2,\ldots,n]\) (\(i\neq j\)). \(d\) represents the dimension of the problem. \(x_{k}^{i}\) and \(x_{k}^{j}\) represent the \(k\)-th element of \(\mathbf{x}^{i}\) and \(\mathbf{x}^{j}\) respectively. This operator is commonly conducted on \(n\) individuals; \(n\) represents the population size. After an expression expansion, we reformulate Eq. (17) as \(\sum_{i=1}^{n}\mathbf{x}_{i}\mathbf{W}_{i}^{c}\)[46]. \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) represents the \(i\)th individual in \(\mathbf{X}\), where \(\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}\}\) is a population. \(\mathbf{W}_{i}^{c}\in\mathbb{R}^{d\times d}\) is the diagonal matrix. If \(\mathbf{W}_{i}^{c}\) is full of zeros, the \(i\)th individual has no contribution.

MutationThe mutation operator brings about random changes in the population. Specifically, an individual \(\mathbf{x}_{i}\) in the population goes through the mutation operator to form the new individual \(\mathbf{x}_{i}^{m}\), formulated as follows:

\[x_{k}^{m}=\begin{cases}rand(l_{k},u_{k})&rand(0,1)<mr\\ \quad\quad x_{k}^{c}&otherwise\end{cases}\] (18)

where \(mr\) is the probability of mutation operator and \(k\in[1,\cdots,d]\). \(x_{k}^{m}\) and \(x_{k}^{c}\) represent the \(k\)-th element of \(\mathbf{x}^{m}\) and \(\mathbf{x}^{c}\) respectively. Similarly, Equation (18) can be reformulated as \(\mathbf{x}_{i}^{c}\mathbf{W}_{i}^{m}\), where \(\mathbf{W}_{i}^{m}\in\mathbb{R}^{d\times d}\) is the diagonal matrix.

SelectionWe introduce the binary tournament mating selection operator in Eq. (19). The selection operator survives individuals of higher quality for the next generation until the number of individuals is chosen. As shown in Eq. (19),

\[p_{i}=\begin{cases}1&f(\mathbf{x}_{i})<f(\mathbf{x}_{k})\\ 0&f(\mathbf{x}_{i})>f(\mathbf{x}_{k})\end{cases},\;\;(\mathbf{x}_{i},\mathbf{ x}_{k})\in\mathbf{X}\cup\mathbf{X}^{m}\] (19)

where \(p_{i}\) reflects the probability that \(\mathbf{x}_{i}\) is selected for the next generation, and \(\mathbf{X}^{m}=\{\mathbf{x}_{1}^{m},\mathbf{x}_{2}^{m},\cdots,\mathbf{x}_{n}^ {m}\}\).

### Mutation Strategy in DE

The core components of the optimization model include modules that generate solutions and modules that select solutions. GA and DE basically include crossover modules, mutation modules and selection modules. The evolutionary strategy represented by CMA-ES needs to sample a population from a certain distribution (such as Gaussian distribution), and further select individuals to update this distribution. In this paper, we design parameterized trainable LMM and LCM as modules for generating solutions. The function of LMM is to generate a candidate population, and LCM further performs crossover between the candidate population and the original population to obtain the offspring population.

We list some classic DE mutation strategies.

* DE/rand/1 \[\mathbf{v}_{i}^{t}=\mathbf{x}_{r1}^{t}+F\cdot(\mathbf{x}_{r2}^{t}-\mathbf{x}_ {r3}^{t})\] (20)
* DE/rand/2 \[\mathbf{v}_{i}^{t}=\mathbf{x}_{r1}^{t}+F\cdot(\mathbf{x}_{r2}^{t}-\mathbf{x}_ {r3}^{t}+\mathbf{x}_{r4}^{t}-\mathbf{x}_{r5}^{t})\] (21)* DE/best/1 \[\mathbf{v}_{i}^{t}=\mathbf{x}_{best}^{t}+F\cdot(\mathbf{x}_{r1}^{t}-\mathbf{x}_{r2 }^{t})\] (22)
* DE/current-to-rand/1 \[\mathbf{v}_{i}^{t}=(1-F)\mathbf{x}_{i}^{t}+F\cdot(\mathbf{x}_{r1}^{t}-\mathbf{ x}_{r2}^{t}+\mathbf{x}_{r3}^{t})\] (23)
* DE/current-to-best/1 \[\mathbf{v}_{i}^{t}=(1-F)\mathbf{x}_{i}^{t}+F\cdot\mathbf{x}_{best}^{t}+F\cdot( \mathbf{x}_{r1}^{t}-\mathbf{x}_{r2}^{t})\] (24)
* DE/current-to-pbest/1 \[\mathbf{v}_{i}^{t}=(1-F)\mathbf{x}_{i}^{t}+F\cdot\mathbf{x}_{pbest}^{t}+F \cdot(\mathbf{x}_{r1}^{t}-\mathbf{x}_{r2}^{t})\] (25)

The integer index \(r1\) (and similarly, \(r2\) and \(r3\)) is randomly selected from the range \([0,N]\). \(pbest\) is randomly selected from the indices of the best \(p\) individuals. \(x_{best}^{t}\) is the individual with the best fitness in the population at generation \(t\).

The generalized form of the mutation strategy is

\[\mathbf{v}_{i}^{t}=\sum_{j}^{N}w_{i,j}\mathbf{x}_{j}\quad(\forall w_{i,j}\in \mathbb{R})\] (26)

For example, when \(w_{i,q}=1\), \(w_{i,k}=-w_{i,j}\neq 0\), and \(w_{i,l}=0\) (\(\forall l\notin\{q,j,k\},q\neq k,k\neq j,q\neq j\)), it becomes DE/rand/1. If individuals of the population has been sorted from good to bad by fitness, when \(w_{i,0}=1\), \(w_{i,k}=-w_{i,j}\neq 0\), and \(w_{i,l}=0\) (\(\forall l\notin\{0,j,k\},k\neq j\)), it becomes DE/best/1.

## Appendix B Landscape Features of TF1-TF8

The landscape features included in \(TS\) are shown as follows:

* TF1: Unimodal
* TF2: Separable
* TF3: Unimodal, Separable
* TF4: Unimodal, Separable
* TF5: Multimodal, Non-separable, Having a very narrow valley from local optimum to global optimum, Ill-conditioned
* TF6: Multi-modal, Non-separable, Rotated
* TF7: Multimodal, Separable, Asymmetrical, Local optima's number is huge
* TF8: Multi-modal, Non-separable, Asymmetrical

## Appendix C Test Set

### Bbob

BBOB [59, 55] is a widely researched and recognized collection of benchmark test problems to evaluate the performance of optimization algorithms. The dataset consists of a series of high-dimensional continuous optimization functions, including single-peak, multi-peak, rotated, and distorted functions, as well as some functions with specific properties such as Lipschitz continuity and second-order differentiability.

### Robot Control Tasks

We test the performance of POM on two complex robot control tasks.

#### c.2.1 Bipedal Walker

The continuous control task Bipedal Walker [58], implemented within the Box2D physics engine, has been designed to test the ability of walking agents to navigate varying terrain by controlling their joints and maintaining balance. The challenge requires the agent to learn efficient walking strategies that enable it to traverse the intended path without falling or deviating from its trajectory. The robot's state comprises a range of variables, including the hull angle speed, angular velocity, horizontal speed, vertical speed, joint positions and angular speeds, legs contact with the ground, and lidar rangefinder measurements. The robot's actions involve determining motor speed values in the range of [-1, 1] for each of the four joints at the hips and knees. The performance of the agent is evaluated through a reward system, whereby it receives points for moving forward, with a maximum of 300+ points awarded upon successfully reaching the end of the designated course. However, the penalty of -100 points is imposed if the robot loses balance and falls. Furthermore, applying motor torque incurs a small cost in terms of points. The score accrued by the agent serves as a measure of its optimal performance. The Bipedal Walker task represents a challenging and dynamic environment that effectively evaluates the walking and balance control abilities of agents. As such, it provides a valuable benchmark for testing and comparing different reinforcement learning algorithms for robotic locomotion.

#### c.2.2 Enduro

Enduro [58] is one of the classic reinforcement learning environments provided by OpenAI Gym. It is a driving racing game based on the Atari 2600 game. In this environment, your goal is to drive as far as possible by controlling the car. The Enduro game is set on an endless highway where you need to avoid other vehicles and overtake as many other vehicles as possible within a limited time. You can avoid collisions with other vehicles by moving your car left and right, and be careful to control your speed to avoid accidents. The game rewards you based on how far you drive, so your goal is to learn a good driving strategy to maximize the distance traveled.

In these two test tasks, the agent interacts with the environment for \(k\) time steps, and the reward at the \(i\)-th step is \(r_{i}\). We evaluate strategy performance as follows:

\[R=\sum_{i=0}^{k}r_{i}\] (27)

In these two tasks we conduct 10 sets of experiments, each set of experiments consists of 5 independent runs. We finally take the best results of each set of experiments to calculate the mean and standard deviation.

Baselines

Our core is the population-based pre-training BBO algorithm, so we do not compare with non-population methods such as Bayesian optimization methods. Moreover, Bayesian optimization methods are difficult to deal with continuous optimization problems of more than 100 dimensions. We do not use LLM-based approaches [28; 29; 30; 31; 32; 34; 35; 36] as baselines because they can only be used for a specific type of task.

**Heuristic Population-based BBO Algorithm**. DE(DE/rand/1/bin) [54], ES((\(\mu\),\(\lambda\))-ES), L-SHADE [22], and CMA-ES [12], where DE [54] and ES are implemented based on Geatpy [60], CMA-ES and IPOP-CMA-ES are implemented by cmaes2, and L-SHADE is implemented by pyade3. The reasons for choosing these baselines are the following:

Footnote 2: https://github.com/CyberAgentAllLab

Footnote 3: https://github.com/xKuZZ/pyade

* DE(DE/rand/1/bin): A classic numerical optimization algorithm.
* ES((\(\mu\),\(\lambda\))-ES): A classic variant of the evolution strategy.
* CMA-ES: CMA-ES is often considered the state-of-the-art method for continuous domain optimization under challenging settings (e.g., ill-conditioned, non-convex, non-continuous, multimodal).
* L-SHADE: The state-of-the-art variant of DE.

**Pretrained BBO Algorithm**. We chose three state-of-the-art meta-learn BBO algorithms for comparison with POM.

* LES [25]: A recently proposed learnable ES. It uses a data-driven approach to discover new ES with strong generalization performance and search efficiency.
* LGA [10]: A recently proposed learnable GA that discovers new GA in a data-driven manner. The learned algorithm can be applied to unseen optimization problems, search dimensions, and evaluation budgets.
* We train POM on \(TS\). During training, the maximum number of evolution generations is 100, \(n=100\) and the problem dimension is set to 10.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_FAIL:22]

Figure 12: The log convergence curves of POM and other baselines. It shows the convergence curve of these algorithms on functions in BBOB with \(d=30\).

Figure 13: The log convergence curves of POM and other baselines. It shows the convergence curve of these algorithms on the functions in BBOB with \(d=100\).

Figure 14: The log convergence curves of POM and TurBO. It shows the convergence curve of these algorithms on functions in BBOB with \(d=100\).

[MISSING_PAGE_EMPTY:26]

Figure 15: Visualized results of mutation strategy \(S^{t}\) on BBOB (F1-F4) with \(d=100\). Here, \(n=10\) for the sake of clarity. The blank squares in the matrix indicate the masked parts in Eq. (8). Steps 1, 50 and 100 correspond to the 1st, 50th and 100th generations in the population evolution process. The horizontal and vertical axes show the ranking of individuals, with 1 being the best and 10 being the worst in the population. Each row represents the weight assigned to other individuals when performing mutation operations for the corresponding individual.

Figure 16: Visualized results of mutation strategy \(S^{t}\) on BBOB (F5-F8) with \(d=100\).

Figure 17: Visualized results of mutation strategy \(S^{t}\) on BBOB (F9-F12) with \(d=100\).

Figure 18: Visualized results of mutation strategy \(S^{t}\) on BBOB (F13-F16) with \(d=100\).

Figure 19: Visualized results of mutation strategy \(S^{t}\) on BBOB (F17-F20) with \(d=100\).

Figure 20: Visualized results of mutation strategy \(S^{t}\) on BBOB (F21-F24) with \(d=100\).

Figure 21: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 1. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**F** & **XS** & **S** & **S** & **M** & **L** & **VL** & **XL** \\ \hline F1 & 3.89E-16.63 & 3.89E-075.38E-07.60 & 6.08E-106.06E-19 & 7.88E-247.88E-24 & 1.97E-191.97E-19 & **4.99E-274.99E-27** \\ F2 & 7.66E-187.66E-18.9 & 9.97E-09.97E-09 & 2.43E-210.43E-21 & **6.96E-306.96E-30** & 1.04E-211.04E-21 & 2.58E-292.58E-29 \\ F3 & 3.11E-163.11E-16 & 4.60E+024.60E+02 & 1.39E-171.39E-17 & 2.89E-162.89E-16 & 4.17E+022 & **4.34E-263.34E-26** \\ F4 & 8.49E-050.49E-05 & 3.25E-025.02E+02 & 2.46E-024.66E+02 & 8.50E-010.56E-01 & 3.10E+023 & **6.15E-230.16E-23** \\ F5 & 4.00E+024.04E-020 & 3.22E-023.20E-02 & 3.38E-023.39E-02 & **0.00E-000.00E-00** & 4.05E+024.04E-02 & 2.56E-022Figure 22: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 5. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

Figure 23: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 18. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

Figure 24: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 51. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

Figure 25: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 75. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

## Appendix I Limitations

* Model size: In the experiment, we found that the relationship between the model size and the performance of POM is not a strict linear relationship. Although the larger the model, the more difficult it is to train, there is still no very quantitative design criterion between model size, training data volume and training difficulty.
* Time performance: We introduced an operation similar to the attention mechanism, whose time complexity is \(O(n^{2})\), which makes POM require a lot of time cost when processing large-scale populations. How to reduce and improve the time efficiency of POM is also worthy of further study.

## Appendix J Potential Impact

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

Figure 26: Results of a visual analysis of LCM on BBOB with \(d=100\). Here, \(n=100\). This is the crossover strategy of the individual ranked No. 100. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, we accurately reflect the contribution and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of work in the experimental analysis part. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Yes, we have a complete experimental proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We give all the details required to reproduce the main experimental results, see section 4.1 for details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide source code, and the data sets used are public data sets. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details (see section 3.3 for details). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes, we provide statistical experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, we provide device resources information and time analysis (see section 4). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research is in line with Neurips Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See the appendix J for specific content. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We respect the relevant original author and the open source agreement. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide anonymous code link. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.