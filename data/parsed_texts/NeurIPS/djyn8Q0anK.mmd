# Scalable Transformer for PDE Surrogate Modeling

Zijie Li, Dule Shu, Amir Barati Farimani

Carnegie Mellon University

Mechanical Engineering Department

{zijieli, dules}@andrew.cmu.edu & barati@cmu.edu

###### Abstract

Transformer has shown state-of-the-art performance on various applications and has recently emerged as a promising tool for surrogate modeling of partial differential equations (PDEs). Despite the introduction of linear-complexity attention, applying Transformer to problems with a large number of grid points can be numerically unstable and computationally expensive. In this work, we propose Factorized Transformer (FactFormer), which is based on an axial factorized kernel integral. Concretely, we introduce a learnable projection operator that decomposes the input function into multiple sub-functions with one-dimensional domain. These sub-functions are then evaluated and used to compute the instance-based kernel with an axial factorized scheme. We showcase that the proposed model is able to simulate 2D Kolmogorov flow on a \(256\times 256\) grid and 3D smoke buoyancy on a \(64\times 64\times 64\) grid with good accuracy and efficiency. The proposed factorized scheme can serve as a computationally efficient low-rank surrogate for the full attention scheme when dealing with multi-dimensional problems.

## 1 Introduction

Various physics processes are modeled by partial differential equations (PDEs), from the interaction between atoms in molecular systems to large-scale cosmological phenomena. Solving PDEs advances the understanding of complex physical phenomena, enabling people to make accurate predictions, and make informed decisions across a wide range of scientific and engineering disciplines. Numerical solvers provide a practical way to simulate and predict PDEs since many PDEs are often difficult to solve analytically. Most numerical solvers divide the continuous domain into a discretized grid and reduce the continuous differential equations to algebraic equations via methods like finite difference/element/volume methods or spectral method. Despite the theoretical guarantees behind them, their practical realization of specific problems can pose challenges that require careful expertise to overcome, such as a sufficient understanding of the underlying physics, or a fine-tailored mesh that resolves the necessary spatio-temporal scales. The interest in developing user-friendly and efficient PDE solvers, along with the success of deep learning models in many other areas [13; 44; 51; 103], has facilitated the emergence of neural-network-based PDE solvers, where the neural network can be used to parameterize the solution function of the target equation [96], or to approximate the solution operator[69; 76]. Compared to many numerical solvers, neural PDE solvers appear to be more tolerant with coarse discretization [104], and can be applied without explicit meshing [96]. In addition, knowing the underlying equations are not strictly necessary for neural PDE solvers, which gives them the potential to simplify and accelerate the process of physics simulation based on PDEs.

Among various neural network designs, attention-based models (Transformer) [115] have become state-of-the-art for a wide array of applications [13; 18; 27; 51], which gives rise to a recent surge of interest in applying the Transformer to PDE modeling [16; 29; 35; 41; 43; 54; 67; 73; 82; 86]. By viewing the input sequence as a function sampled on a discretization grid, attention can be interpreted as a learnable kernel integral [17; 35; 54; 60] or a learnable Galerkin projection [16], and the sequence-to-sequence Transformer [115] can be modified correspondingly to be better suited forPDE modeling [16; 35; 43; 54; 67; 74; 86]. In these works, attention is typically applied to every grid point in the domain to exploit both the local and non-local structure of the system, and therefore a linear-complexity variant of attention is usually necessary. As the number of grid points grows exponentially with respect to the number of dimensions, this results in a very large attention matrix that computes the interaction between every pair of the grid points (despite this attention matrix is not evaluated explicitly in linear attention). Consequently, cascading a deep stack of attention layers introduces instability and relatively high computational cost on high-resolution grid. To alleviate these issues and improve the scalability of Transformer in PDE modeling, we propose a modified attention mechanism. Our model is inspired by the kernel integral viewpoint of softmax-free attention, with a factorized integration scheme motivated by the inherent low-rank structure of dot-product kernel matrix. More specifically, we propose a multi-dimensional factorized kernel integral with each kernel function in the integral having only single-dimensional domains. To calculate these axial kernels, we propose a learnable integral operator that is able to project the input function with high-dimensional domain into a set of sub-functions with single-dimensional domain. The computation of each axial kernel is quadratic with respect to the number of grid points along that the corresponding axis but does not grow with the number of dimensions, which alleviates the curse of dimensionality in standard attention. With the modified attention mechanism, our proposed model can scale up to multi-dimensional problems with a large number of grid points and achieve competitive performance compared to state-of-the-art models. Moreover, we show that our factorized attention mechanism can reduce the computational cost and improve stability compared to softmax-free linear attention.1

Footnote 1: Code for this project is available at: https://github.com/BaratiLab/FactFormer.

## 2 Related works

Neural PDE solverBased on the emphases of model design, neural PDE solvers can generally be divided into the following groups. The first group of work focuses on using neural networks with mesh-specific architecture design (such as convolutional layers for uniform mesh, or a graph layer for irregular mesh) to learn the spatial and/or temporal correlation of the PDE data [10; 38; 48; 64; 65; 75; 88; 91; 92; 100; 104; 109; 114; 116]. With input-target data collected, the training process can be conducted without the knowledge of underlying PDEs. This can be appealing when the physics of the system is unknown or partially known, such as large-scale climate modeling [61; 83; 90; 97]. The second group of work, namely the Physics-Informed Neural Networks (PINNs)[15; 40; 42; 52; 77; 87; 96; 106; 127], treat neural networks as a parametrization of the underlying solution function. PINNs incorporate the knowledge of the governing equations into the construction of loss function, which includes the residual of the PDE, the consistency with given boundary condition and initial condition. Unlike the previous group of works, PINNs do not necessarily need input-target data and can be trained solely based on equation loss. The third group of works, often referred to as the neural operator, focuses on learning a mapping between the function spaces[5; 8; 9; 16; 36; 43; 50; 54; 60; 68; 70; 71; 76; 78; 86]. Neural operator has the generalization capability within a family of PDE and can potentially be adapted to different discretization without retraining. DeepONet [76] proposes a practical realization of the universal operator approximation theorem [21]. Concurrent work graph neural operator [69] proposes a learnable kernel integral to approximate the solution operator of parametric PDEs and the follow-up work Fourier Neural Operator (FNO) [68] achieves excelling accuracy and efficiency on certain types of problems. Broadly speaking, the

Figure 1: Modelâ€™s prediction (pred.) and reference ground truth (ref.). **Left**: 2D Kolmogorov flow on \(256\times 256\) grid; **Right**: 3D smoke buoyancy on \(64\times 64\times 64\) grid (\(zOy\) cross-section is shown).

operator learning can be conducted upon different types of function bases, such as the Fourier bases [34; 58; 59; 68; 95; 110; 121], wavelet bases [36], learned bases in an attention layer [16; 67], or based on approximation of the Green's function [7; 108]. The training of neural operators can also be combined with the principle of PINNs to yield a more physically consistent prediction [72; 117]. Our model is closely related to the neural operator, as the major building blocks in our proposed model are a learnable projection operator and a learnable kernel integral operator.

In addition to direct surrogate modeling, neural networks can also be combined with numerical solvers to improve their accuracy and efficiency. For example, using a trained neural network to correct the error of the solver on the fly [3; 28; 56; 89; 113], or doing offline high-fidelity reconstruction [24; 30; 49; 66; 102].

Transformer for Physics SimulationThe Transformer model [115] have gained outstanding popularity in natural language modeling [13; 25], imagery data processing [27] and beyond [51]. In the field of physics simulation, Transformer has drawn increasing research interest as a surrogate model for simulation, with its modeling capability demonstrated both as a neural PDE solver [16; 32; 35; 41; 43; 48; 54; 67; 82; 86] and as a pure data-driven model in the absence of a known governing PDE [14; 20; 31; 83]. The dot-product attention can be considered as an approximation of an integral transform with a non-symmetric learnable kernel function [16; 17; 35; 54; 60; 122], which relates Transformer to other popular operator learning models such the FNO [68]. We will expand the discussion of Transformer under the kernel viewpoint in Section 3.

Efficient TransformerFollowing the introduction of Transformer [115], various works have investigated ways of reducing the computational cost of standard scaled-dot product attention. The first line of work seeks to remove the softmax and make use of matrix associativity to derive linear complexity attention [23; 53; 101], which has also been explored for PDE modeling[16; 43; 67]. The second line of work tries to approximate the dot product between query and key matrix by exploiting the low-rank structure of it [4; 22; 45; 55; 120; 123; 126]. Our work is related to the first group of works with a softmax-free design, but still calculates the dot product between query and key first. Among the second line of work, Axial Transformer is closely related to our work, as both works have explored conducting attention in an axial fashion. However, the derivation of attention matrix is different in the two works (see Section 3.2 for detailed comparison). More generally, the exploitation of the multi-dimensional tensor structure in our proposed model can be related to tensor factorization methods [57; 85] and their applications in various deep learning models [58; 63; 80; 84; 124].

## 3 Method

### Attention mechanism

Standard attentionGiven three sets of vectors, namely the queries \(\left\{\mathbf{q}_{i}\right\}_{i=1}^{N_{q}}\), keys \(\left\{\mathbf{k}_{i}\right\}_{i=1}^{N_{b}}\), and values \(\left\{\mathbf{v}_{i}\right\}_{i=1}^{N_{v}}\) (assuming \(N_{k}=N_{v}\)), attention mechanism [2; 33; 79; 115] dynamically computes a weighted average of the values: \(\mathbf{z}_{i}=\sum_{j=1}^{N_{v}}h(\mathbf{q}_{i},\mathbf{k}_{j})\mathbf{v}_{j}\), where \(\mathbf{q}_{i},\mathbf{k}_{i},\mathbf{v}_{i}\in\mathbb{R}^{1\times d}\), \(h(\cdot)\) is the weight function that determines the contribution of a specific value to the final output. An example of \(h(\cdot)\) is the scaled-dot product with softmax [115]: \(h(\mathbf{q}_{i},\mathbf{k}_{j})=\exp\left(\mathbf{q}_{i}\mathbf{k}_{j}^{T}/ \tau\right)/\sum_{s}\exp\left(\mathbf{q}_{i}\mathbf{k}_{s}^{T}/\tau\right)\), and \(\tau\) is usually chosen as \(\tau=\sqrt{d}\). The queries/keys/values are usually obtained from inputs via learnable projection. In self-attention, all of them are computed from the same source as follow:

\[\mathbf{q}_{i}=\mathbf{u}_{i}W_{q},\mathbf{k}_{i}=\mathbf{u}_{i}W_{k}, \mathbf{v}_{i}=\mathbf{u}_{i}W_{v},\] (1)

where \(\mathbf{u}_{i}\in\mathbb{R}^{1\times d_{u}}\) is the input vector and \(\left\{W_{q},W_{k},W_{v}\right\}\in\mathbb{R}^{d_{u}\times d}\) are learnable projection matrices. In cross-attention, queries are derived from one input while keys and values are derived from another.

Attention as learnable integralUnder the hood of PDE modeling, the input sequence to the attention layer can be viewed as the sampling of input function on the discretization grid [16; 54; 60; 67]. Kovachki et al. [60] propose that the scaled-dot product attention [115] can be viewed as a special case of a Neural Operator [60], where the attention amounts to the Monte Carlo approximation of the learnable kernel integral. Cao [16] further proposes two interpretations of softmax-free attention. The first is to view attention as the Fredholm integral equation of the second kind with a learnable asymmetric dot-product kernel, and the second is to view it as a Peterov-Galerkin projection with learnable basis function. The softmax-free attention proposed by Cao [16] is later extended in OFormer [67], where Rotary Positional Encoding (RoPE) [105] is introduced to modulate the dot product and can be viewed as another special case of the kernel integral in Neural Operator style.

In this work, we continue on adopting the learnable kernel integral viewpoint of attention and view each channel of the hidden feature map as the sampling of a specific function on the discretization grid. Given query/key/value matrix \(\{Q,K,V\}\in\mathbb{R}^{N\times d}\), their row vectors: \(\mathbf{q}_{i}/\mathbf{k}_{i}/\mathbf{v}_{i}\), correspond to the sampling of a set of functions \(\{q_{l}(\cdot),k_{l}(\cdot),v_{l}(\cdot)\}_{l=1}^{d}\) on grid point \(x_{i}\), where \(\{x_{i}\}_{i=1}^{N}\) discretizes the underlying domain. As a more concrete example, the \(l\)-th column (channel) of \(\mathbf{q}_{i}\), represents the sampling of function \(q_{l}(\cdot)\) on a grid point, i.e. \(\left(\mathbf{q}_{i}\right)^{l}=q_{l}(x_{i})\). Furthermore, softmax-free attention is equivalent to the numerical quadrature of a kernel integral:

\[\left(\mathbf{z}_{i}\right)^{l}=\sum_{s=1}^{N}w_{s}(\mathbf{q}_{i}\cdot \mathbf{k}_{s})(\mathbf{v}_{s})^{l}\approx\int_{\Omega}\kappa\left(x_{i}, \xi\right)v_{l}(\xi)d\xi,\] (2)

where \(\mathbf{z}_{i}\) is the output vector, \(\kappa\left(x,\xi\right)=\sum_{l=1}^{d}q_{l}(x)k_{l}(\xi)\) is an instance-based kernel and \(w_{s}\) is the quadrature weight. Understanding attention from the perspective of the kernel has been an active topic of research [17, 23, 111, 122]. The theoretical approximation power of different kernel integrals has also been analyzed under the context of PDE learning [35, 54, 60].

Note that the above kernel does not explicitly depend on the spatial coordinates \(\left(x_{i},\xi\right)\). For this work, we opt for a modified kernel formulation proposed in OFormer [67], which modulates the dot product kernel with relative position. Assuming the underlying spatial domain is 1-D (which is sufficient for our proposed model, see next subsection), given query and key vectors \(\mathbf{q}_{i},\mathbf{k}_{j}\) and their corresponding spatial coordinates \(x_{i},x_{j}\), RoPE [105] (\(g(\cdot,\cdot):\mathbb{R}^{1\times d}\times\mathbb{R}\mapsto\mathbb{R}^{1 \times d}\)) is defined as:

\[g\left(\mathbf{q}_{i},x_{i}\right)=\mathbf{q}_{i}\mathbf{\Theta }\left(x_{i}\right),\ \ g\left(\mathbf{k}_{j},x_{j}\right)=\mathbf{k}_{j}\mathbf{\Theta} \left(x_{j}\right)\] (3) \[\text{where: }\mathbf{\Theta}\left(x_{i}\right)=\text{Diag}\left(R_{1}(x _{i}),\ldots,R_{d/2}(x_{i})\right),\ \ \ R_{l}=\begin{bmatrix}\cos\left(\lambda x_{i}\theta_{l}\right)&-\sin \left(\lambda x_{i}\theta_{l}\right)\\ \sin\left(\lambda x_{i}\theta_{l}\right)&\cos\left(\lambda x_{i}\theta_{l} \right)\end{bmatrix},\]

and \(\lambda,\theta_{l}\) are hyperparameters. \(\theta_{l}\) is usually chosen as \(10000^{-2(l-1)/d},l\in\{1,2,\ldots,d/2\}\) following Vaswani et al. [115] and Su et al. [105]. \(\lambda\) is a mesh-based weight that we heuristically set to \(64\) throughout most problems. The projection function \(\Theta(\cdot):\mathbb{R}\mapsto\mathbb{R}^{d}\times\mathbb{R}^{d}\) can explicitly modulate the dot product with relative position: \(g\left(\mathbf{q}_{i},x_{i}\right)g\left(\mathbf{k}_{j},x_{j}\right)^{T}= \mathbf{q}_{i}\Theta(x_{i}-x_{j})\mathbf{k}_{j}^{T}\), thanks to the following property of rotation matrix: \(R_{l}(x_{i})R_{l}(x_{j})^{T}=R_{l}(x_{i}-x_{j})\).

To summarize, we will adopt attention mechanism in the following form for the proposed model (with modification discussed in the next subsection):

\[Z=w\tilde{Q}\tilde{K}^{T}V,\] (4)

where \(\bar{\Box}\) denotes a matrix whose row vectors are RoPE encoded as in (3), e.g., \(\tilde{Q}_{i}=g(\mathbf{q}_{i},x_{i})\), \(w\) is the quadrature weight with a typical choice of \(1/N\) for uniform quadrature rule, \(Z\) is the output matrix. The query/key/value matrix \(Q/K/V\) is derived from the input via learnable projections defined in (1). The matrix product \(\tilde{Q}\tilde{K}^{T}\) evaluates the kernel function \(\kappa(\cdot,\cdot)\) on the discretization grid \(\{x_{i}\}_{i=1}^{N}\).

### Multidimensional factorized attention

Compared to the standard scaled dot product attention that has quadratic complexity with respect to the length of the input sequence, the attention in (4) can enjoy a linear complexity by making use of the associativity of matrix multiplication (calculate \(\tilde{K}^{T}V\) first). In PDE modeling, the length of the input sequence is equal to the number of points on the underlying discretization grid. Assuming the \(n\)-dimensional domain is discretized by \(S_{1}\times S_{2}\times\ldots\times S_{n}=N\) points, the softmax-free attention in (4) will compute the kernel in (2) with the dot product of \(Q\) and \(K\), which are \(N\) by \(d\) matrices with \(N\) usually much larger than \(d\). The kernel matrix computed is by design low-rank as it is the product of two tall and thin matrices. Meanwhile, attending a large number of grid points to each other can be unstable and the linear attention has a complexity that is quadratic to the channel dimension \(d\), which can limit the scalability of the model in terms of its width. To improve the numerical stability and reduce the computational cost of the aforementioned attention mechanism, we propose a simple yet efficient way to modify the kernel integral discussed in the previous section which is motivated by the low-rank structure of attention. Essentially, our model computes the kernel integral in an axial factorized manner instead of convolving over all the grid points in the domain.

For the following discussion, we will use tensor notation [57] to describe the operation. We assume the data is represented on a uniform Eulerian grid and can be treated as \(n\)_-way_ tensor \(U\in\mathbb{R}^{S_{1}\times S_{2}\times\ldots\times S_{n}}\)2. The product of it with a matrix \(W\in\mathbb{R}^{{}^{J\times S_{m}}}\) across the \(m\)-th mode will result in a tensor of shape \(S_{1}\times\ldots\times S_{m-1}\times J\times S_{m+1}\times\ldots\times S_{n}\), whose elements are defined as:

Footnote 2: In practice we often have an additional mode for the channel, resulting in a _(\(n+1\))-way_ tensor.

\[(U\times_{m}W)_{i_{1}i_{2}\ldots i_{m-1}ji_{m+1}\ldots i_{n}}=\sum_{i_{m}=1}^{S _{m}}U_{i_{1}i_{2}\ldots i_{m}\ldots i_{n}}W_{ji_{m}}.\] (5)

Learnable projectionThe first major component of the proposed framework is a set of learnable integral operators \(\{\mathcal{G}^{(1)},\mathcal{G}^{(2)},\ldots,\mathcal{G}^{(n)}\}\) that projects the input function \(u:\mathbb{R}^{n}\mapsto\mathbb{R}^{d}\) into a set of functions with one-dimensional domain \(\{\phi^{(1)},\phi^{(2)},\ldots,\phi^{(n)}\}\in\mathbb{R}\mapsto\mathbb{R}^{d}\), which is defined as:

\[\phi^{(m)}(x_{i}^{(m)})=\mathcal{G}^{(m)}(u)(x_{i}^{(m)})\] (6) \[=h^{(m)}\left(w\!\int_{\Omega_{1}}\!\cdots\!\int_{\Omega_{n}}\! \gamma^{(m)}\left(u\left(\xi_{1},\ldots,\xi_{m-1},x_{i}^{(m)},\xi_{m+1},\ldots,\xi_{n}\right)\right)d\xi_{1}\!\ldots\!d\xi_{m-1}d\xi_{m+1}\!\ldots\!d\xi_{n} \right),\]

where \(h^{(m)}(\cdot):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) and \(\gamma^{(m)}(\cdot):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) are pointwise learnable functions and \(w=1/(L_{1}L_{2}\cdots L_{m-1}L_{m+1}\cdots L_{n})\), with \(L_{m}\) being the size of domain \(\Omega_{m}\) discretized by \(\{x_{i}^{(m)}\}_{i=1}^{S_{m}}\). In practice, we implement \(h^{(m)}\) as a three-layer multi-layer perception (MLP) similar to the feedforward network in Transformer [115] and \(\gamma^{(m)}\) as a simple linear transformation. When the underlying grid is uniform, (6) simply amounts to first transforming the input with pointwise learnable functions \(\gamma^{(m)}(\cdot)\), applying mean pooling over all but the \(m\)-th spatial dimension, and then applying another pointwise learnable function \(h^{(m)}\).

Factorized kernel integralEquipped with the above projection module, we now introduce our factorized kernel integral scheme. More specifically, we propose to use the following integral to replace the kernel integral in (2):

\[\begin{split}& z\left(x_{i_{1}}^{(1)},x_{i_{2}}^{(2)},\ldots,x_{ i_{n}}^{(n)}\right)\\ &=\int_{\Omega_{1}}\kappa^{(1)}(x_{i_{1}}^{(1)},\xi_{1})\int_{ \Omega_{2}}\kappa^{(2)}(x_{i_{2}}^{(2)},\xi_{2})\cdots\int_{\Omega_{n}}\kappa ^{(n)}(x_{i_{n}}^{(n)},\xi_{n})v(\xi_{1},\xi_{2},\ldots,\xi_{n})d\xi_{1}d\xi_{ 2}\ldots d\xi_{n},\end{split}\] (7)

where kernels \(\{\kappa^{(1)},\ldots,\kappa^{(n)}\}:\mathbb{R}\times\mathbb{R}\mapsto \mathbb{R}\) are computed based on projected single-dimensional function along each axis, \(v(\cdot):\mathbb{R}^{n}\mapsto\mathbb{R}^{d}\) is derived from the input function \(u\) via linear transformation (just as the value in standard attention). Next, we will discuss how the above kernel integral is implemented in practice. Using the learnable projection operator defined in (6), we can obtain \(\{\hat{U}^{(1)},\ldots,\hat{U}^{(n)}\}\) (\(\hat{U}^{(m)}\in\mathbb{R}^{S_{m}\times\xi}\)) from the input \(U\in\mathbb{R}^{S_{1}\times\ldots\times S_{n}\times d}\), where the \(i_{m}\)-th row of \(\hat{U}\) is the evaluation of projected function \(\phi^{(m)}(\cdot)\) at \(x_{i_{m}}\): \(\hat{U}_{i_{n}}^{(m)}=\phi^{(m)}(x_{i_{m}})\). Then we apply linear transformation on them to obtain the query/key matrix just as standard attention: \(Q^{(m)}=\hat{U}^{(m)}W_{q}^{(m)},K^{(m)}=\hat{U}^{(m)}W_{k}^{(m)}\), where \(\{W_{q}^{(m)},W_{k}^{(m)}\}\in\mathbb{R}^{d\times d}\) are learnable matrices. The query and key are used to compute the kernel matrix \(A^{(m)}\in\mathbb{R}^{S_{m}\times S_{m}}\):

\[A^{(m)}=w_{m}\tilde{Q}^{(m)}\left(\tilde{K}^{(m)}\right)^{T},\] (8)

Figure 2: Schematic of the factorized kernel attention. **Upper path**: the input is transformed into the _Value_ via a linear transformation. **Lower path**: the input is first projected into multiple sub-functions with a one-dimensional domain. These sub-functions are then used to derive the _Query_ and _Key_ on each axis, and their dot products form the kernel function of the corresponding axis. The _Value_ is iteratively updated by the kernel integral transform along each axis and finally sent to an MLP.

where \(w_{m}\) is the mesh weight, \(\Box\) denotes the RoPE encoded matrix as discussed in (4), the \(i\)-th row and \(j\)-th column of \(A^{(m)}\) represents the kernel value \(\kappa^{(m)}(x_{i}^{(m)},x_{j}^{(m)})\). Despite (8) has a quadratic complexity with respect to the grid size \(S_{m}\), this is an affordable cost for most of the problems where the axial grid size \(S_{m}\) is mostly between \(64\) to \(512\). Meanwhile, the value \(V\in\mathbb{R}^{S_{1}\times\ldots\times S_{n}\times d}\) is derived from the input via a linear transformation (i.e. \((m+1)\)-th mode product): \(V=U\times_{n+1}W_{v}\), where \(W_{v}\in\mathbb{R}^{d\times d}\) is again a learnable matrix. The overall factorized kernel integral is numerically approximated with the following tensor-matrix product (Figure 2):

\[Z=\text{Att}(U)=V\times_{1}A^{(1)}\times_{2}A^{(2)}\times\ldots\times_{n}A^{( n)}.\] (9)

In (9), the computation of all kernel is of complexity \(O(S_{1}^{2}d+S_{2}^{2}d+\ldots+S_{n}^{2}d)\), and the time complexity of a single tensor-matrix product \(V\times_{m}A^{(m)}\) is \(O(NS_{m}d)\). After evaluating the tensor-matrix product, the output tensor \(Z\) will be sent to a pointwise feedforward network \(f(\cdot):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\). To sum up, the update protocol of a single layer in our proposed factorized Transformer is defined as follows:

\[U\gets f\left(\text{IN}\left(\text{Att}\left(U\right)\right)\right)+U,\] (10)

where \(\text{Att}(\cdot)\) is the attention from (9), \(\text{IN}(\cdot)\) is instance normalization [112] that normalizes each channel instance-wise.

It is worth pointing out that the axial factorized kernel proposed here shares some similarities with the Axial Transformer proposed in Ho et al. [45], but has two significant differences despite the connection. Firstly, Axial Transformer reduces the computational cost by constraining the context of attention along each axis (e.g. a pixel can only attend to other pixels on the same row), which amounts to moving all but one axis to the batch dimension. In this way computing the axial kernel matrix is of \(O(NS_{m}d)\) complexity (recall \(N=S_{1}\times\ldots\times S_{n}\)) instead of \(O(S_{m}^{2}d)\) as in our model. And its overall computation of attention is relatively more expensive due to the presence of softmax. Secondly, the decomposition in Axial Transformer is not layer-wise. For example, in the first layer, the attention is conducted in a row-wise manner and then the second block will conduct attention in a column-wise manner, whereas our model decomposes attention along all axes into a tensor-matrix product within every layer. We provide an illustrative example in Figure 27 of the Appendix.

### Training techniques

In this subsection, we will discuss several techniques used for training the model (including baselines) in our numerical experiments. In general, these techniques aim to alleviate the compounding error of autoregressive neural PDE solvers when applied to time-dependent PDEs.

Latent marchingIt is proposed in the Li et al. [67] that a simple pointwise learnable function \(\varepsilon(\cdot,\cdot)\in\mathbb{R}^{d}\times\mathbb{R}_{>0}\mapsto\mathbb{ R}^{d}\) can be used to propagate dynamics in the latent space with a fixed time interval \(\Delta t\): \(z\left(x,t+\Delta t\right)=z\left(x,t\right)+\varepsilon\left(z\left(x,t \right),t\right)\), where \(z\) is the output of the final attention layer. In practice, \(\varepsilon\) is implemented as a pointwise MLP and is efficient to compute. Leveraging this technique, with one call to the neural solver, we can forward the state for multiple time steps (by marching in the latent space for \(k\) steps), thus reducing the total number of calls by a ratio of \(k\). This is in principle similar to the _Temporal Bundling_ technique proposed in Message-Passing Neural PDE solver (MP-PDE) [12], yet different in practical realization. In MP-PDE, the multi-timestep prediction is implemented as first predicting the difference in time \(\{d_{1},d_{2},\ldots,d_{k}\}\) and then adding them to the input \(u_{0}\) by a forward Euler scheme in the physical space: \(\hat{u}_{k}=u_{0}+d_{k}\Delta t\). In this work, we opt for the latent marching to predict multi-timesteps as the forward Euler scheme (in the physical space) is less stable for fluid problems with relatively large time step sizes.

PushforwardNeural PDE solvers are observed to be unstable for time-dependent problems. A small error or perturbation that occurs at the beginning of neural PDE solvers' prediction, can easily result in an unbounded rollout error. While there is hardly a universal method for guaranteeing their stability, a wide array of techniques have been proposed to improve the stability of neural PDE solvers, such as adding physics constraints [72; 118; 119], rollout training [68] or adding random-walk noise [91; 100; 104]. For this work, we adopt the _pushforward_ technique from MP-PDE, which amounts to rolling out the model for two steps during training and then letting the gradient only flows through the last step. This allows training the model on error-corrupted samples and promotes the stability of the model. From a practical perspective, this is straightforward to implement and also computationally much cheaper than standard rollout training.

Experiment

In this section, we will investigate our proposed model numerically on several challenging problems. Furthermore, we compare our model against softmax-free attention [16; 67]. The baseline models we compared against are Fourier Neural Operator (FNO) [68], Factorized Fourier Neural Operator (F-FNO) [110] and Dilated ResNet (Dil-ResNet) [44; 125]. FNO has been shown to have good accuracy on a wide range of PDE problems and is computationally very efficient owing to the Fast Fourier Transformation (FFT). F-FNO factorizes the spectral convolution in FNO into separate spectral convolution along different axes and adopt an improved residual connection formulation like Transformer [115]. Dil-ResNet is recently introduced by Stachenfeld et al. [104] to learn the coarse-grained dynamics of turbulent flow and has demonstrated state-of-the-art performance across several problems. We adopt the implementation of Dil-ResNet with group normalization from PDEArena [37]. On 2D steady-state problem where linear attention's computational cost is affordable, we also include the result from Galerkin Transformer [16], which uses CNN to project the function onto a coarse grid and applies linear attention on the coarse grid. The implementation details of the proposed model and baselines are available in Section A, B of the Appendix.

### Benchmark problems

We first apply our model to three fluid-like systems, where the underlying physics patterns are sensitive to the spatiotemporal scale that discretization can resolve, and typically require fine discretization for classical numerical solvers. In these problems, the neural PDE solver is trained to predict the next frame (or multiple frames if using latent marching) given a context of previous frames. The number of context frames of Kolmogorov flow and isotropic turbulence is set to \(10\) following [68; 72], and \(4\) for smoke buoyancy similar to [37]. We also consider a well-known steady-state problem-2D Darcy flow, which has been studied in many of the previous works. Below we provide a brief description of each problem we studied. More details can be found in Section E of the Appendix.

2D Kolmogorov flowThe first example is 2D Kolmogorov flow governed by incompressible Navier-Stokes equation with a periodic boundary condition. The Reynolds number _Re_ determines how turbulent the system will be. We adopt the setting of forced turbulence following Kochkov et al. [56] and generate the data by using the pseudo-spectral method to simulate fluid flow with Reynolds number \(\textit{Re}=1000\). The objective is to predict the vorticity \(\omega\) of the flow field within an interval \([t_{0},t_{0}+T]\), where \(T=1\)s and \(t_{0}\) is a random starting point in the sequence. We use a spatial grid of \(256\times 256\) and temporal discretization of \(\Delta t=0.0625\)s (therefore \(1\)s corresponds to 16 frames) to train and evaluate the model.

3D isotropic turbulenceThe second example is 3D isotropic turbulence governed by incompressible Navier-Stokes equation with a periodic boundary condition. The major difference from the first example is that the vortex stretching term is non-zero for three-dimensional flow. We use the 3D spectral simulator from Mortensen and Langtangen [81], which simulates the forced turbulence described in Lamorgese et al. [62]. For generating the dataset, we simulate a system of Taylor Reynolds number \(\textit{Re}_{\lambda}=84\)[62]. The objective is to predict the pressure \(p\) and velocity \(\mathbf{u}\) from \(t=0.5\) to \(t=1\)s (10 frames). The model is trained and evaluated on a \(60\times 60\times 60\) spatial grid with \(\Delta t=0.05\)s.

3D smoke buoyancyThe third example is 3D buoyancy-driven flow, which depicts smoke volume rising in a closed domain. A similar system in 2D formulation has been studied in several previous works [11; 113]. The underlying governing equation is the incompressible Navier-Stokes equation coupled with an advection equation. The boundary condition for the smoke field is Dirichlet while the boundary condition for the flow field is Neumann. The advection equation describes the motion of smoke, which is transported along the flow field. We modify the solver from [37] that is implemented in _phiflow[46]_ to generate the data, with buoyancy factor set to \(0.5\) and viscosity \(\nu=0.003\). The objective is to predict the scalar density field of smoke \(d\) and velocity of flow \(\mathbf{u}\) from \(t=3\) to \(t=15\)s (16 frames). The model is trained and evaluated on a \(64\times 64\times 64\) spatial grid with \(\Delta t=0.75\)s. To account for non-periodic boundary conditions, we pad the domain for FNO variants and DilResNet following the original works. For FactFormer, we append a simple CNN block after the model, which comprises \(3\)-by-\(3\) convolutional layers with zero padding.

2D Darcy flowIn addition to the above time-depedendent systems, the fourth example is 2D steady-state problem from Li et al. [68]. Given the diffusion coefficient, the model predicts the steady-state flow field. The boundary condition is also Dirichlet so we adopt settings for all models similar to the 3D smoke problem.

### Results and discussion

For all the models, we study two protocols of training. The first is Latent Marching with Pushforward (denote as **LM**). The second is simply Autoregressive (denote as **AR**), where the model is rolled out for two steps during training. For LM models, each call to the model will output \(k\) future steps. On 2D Kolmogorov flow/3D smoke buoyancy, \(k\) is set to \(4\), and \(2\) for 3D isotropic turbulence. We interleave pushforward training with standard per-step training for LM models. The relative \(L^{2}\) norm is used to train and measure the error of each model following Li et al. [68]. The sequence-wise averaged error and the frame-wise error at the end frame are reported in Table 1, 2, 3. We also report the time cost of simulating a sequence and the number of parameters for each model. The frame-wise error trends are shown in Figure 6, 7, 8, 9, 10 in the Appendix. The visualization of predicted samples are provided in the Section F of Appendix.

We observe that Dil-ResNet has a slightly better per-frame fitting capability compared to the other models on 3D flow problems. As shown in the loss trend plots, it starts at a lower error compared to other models. This coincides with the observation in Stachenfeld et al. [104] where Dil-ResNet's performance is strong on 3D fluid problems. On 2D flow, F-FNO has the best accuracy compared to other models. Interestingly, FactFormer can catch up with Dil-ResNet on 2D Kolmogorov flow and 3D smoke buoyancy when the time duration becomes longer. Yet for shorter-term prediction - 3D isotropic turbulence, Dil-ResNet still has the best final accuracy. This suggests that the accuracy of long-term prediction can potentially benefit from exploiting the global structure that lies in the input. Nonetheless, compared to Dil-ResNet, FactFormer offers superior efficiency as indicated by the inference time (time cost of simulating a sequence). Since the training time is roughly proportional to the model forward time, on 3D problems Dil-ResNet generally takes 3-4 times longer to train. In terms of different training strategies, we find that AR models are less stable than multi-step training (LM) and computationally more expensive as it requires more calls to the neural solver. Despite the average error varies case by case, LM models' error generally accumulates slower on the problems we studied, whereas AR models quickly blow up in some cases.

Lastly, while Dil-ResNet has shown good accuracy for 3D flow problems, its performance is highly dependent on the training discretizations. As shown in the Figure 3, without changing model architecture, its evaluation errors increases significantly when the resolution increases, while Transformer-based models and FNO models' performance are roughly invariant to the resolution. This highlights a major difference between CNN-based models and neural operators.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{FNO2D} & \multicolumn{2}{c}{F-FNO2D} & \multicolumn{2}{c}{Dil-ResNet} & \multicolumn{2}{c}{FactFormer} \\ \cline{2-9}  & AR\({}^{*}\) & LM & AR\({}^{*}\) & LM & AR & LM & AR & LM \\ \hline \(\omega\) avg. error & 0.3177 & 0.2978 & **0.1486** & 0.2453 & 0.8156 & 0.1655 & 0.8835 & 0.1734 \\ \(\omega\) final error & 0.4423 & 0.4567 & **0.2811** & 0.3861 & 1.1692 & 0.3051 & 1.0963 & 0.3017 \\ \hline Inf. time (s) & 0.73 & 0.81 & 0.86 & 1.01 & 4.69 & 1.78 & 3.14 & 1.38 \\ \# params (M) & 85.1 & & 3.7 & & 2.4 & & 3.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation results of 2D Kolmogorov flow. A batch size of 10 is used for inference. LM models predict 4 steps with each call to the model. Total prediction length is 16 steps. **AR\({}^{*}\)**: Since for 2D problem FNO variants can afford to rollout more steps during training, AR FNO rollout for 12 steps, AR F-FNO rollout for 6 steps, whereas other AR models rollout for 2 steps during training. For model that has complex parameters, each cfloat parameter count as two paramaters.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{FNO3D} & \multicolumn{2}{c}{F-FNO3D} & \multicolumn{2}{c}{Dil-ResNet} & \multicolumn{2}{c}{FactFormer} \\ \cline{2-9}  & AR & LM & AR & LM & AR & LM & AR & LM \\ \hline \(p\) avg. error & 0.8080 & 0.4634 & 0.3151 & 0.3264 & **0.1725** & 0.1778 & 0.2989 & 0.2545 \\ \(p\) final error & 1.1285 & 0.6522 & 0.4250 & 0.4159 & 0.2573 & **0.2448** & 0.4407 & 0.3431 \\ \hline \(\mathbf{u}\) avg. error & 0.3967 & 0.3382 & 0.2298 & 0.2303 & **0.1143** & 0.1250 & 0.1775 & 0.1670 \\ \(\mathbf{u}\) final error & 0.6561 & 0.4735 & 0.2799 & 0.2850 & 0.1675 & **0.1671** & 0.2594 & 0.2218 \\ \hline Inf. time (s) & 1.01 & 0.91 & 2.77 & 1.37 & 12.67 & 6.89 & 2.68 & 1.31 \\ \# params (M) & 509.8 & & 3.0 & & 6.9 & & 5.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation results of 3D isotropic turbulence. A batch size of 4 is used for inference. LM models predict 2 steps with each call to the model. Total prediction length is 10 steps.

### Comparison against full attention

In this subsection, we will present an ablation study of the proposed factorized attention mechanism with softmax-free attention (denoted as "linear attention") previously applied to PDE modeling[16, 67]. More specifically, we employ the attention from Li et al. [67] (in the form of (4)) to replace factorized attention in (10), with \(\tilde{K},V\) normalized column-wise via instance normalization, e.g. \(\left|\left|V_{,j}\right|\right|_{2}=1\). To accommodate for the memory cost of linear attention, we further downsample the 2D Kolmogorov flow discussed in the last subsection to a \(128\times 128\) grid and train both linear and factorized attention models on it (with latent marching and pushforward trick).

Comparison of performanceWe compare the accuracy and computational cost of the two attention mechanisms in Figure 4 and Table 4. While in principle full attention could have better approximation capacity than factorized attention, in practice we find that it performs worse than factorized attention on this problem we studied. Specifically, its rollout is less stable and results in a degraded accuracy. We hypothesize that this is due to the instability of iteratively calculating the attention matrix of a large size, as rolling out the prediction requires recursively calling the model multiple times. In

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Avg. rel. \(L^{2}\) Norm} & \multicolumn{2}{c}{Fwd.} & \multicolumn{2}{c}{Fwd. + Bwd.} \\ \cline{3-5}  & & Enc. time (s) & Prop. time (s) & Time (s) & Mem. (MB) \\ \hline Factorized attention & 0.1529 & 0.0202 & & 0.0954 & 5217 \\ Linear attention (matmul) & 0.5853 & 0.1370 & 0.0112 & 0.3013 & 12029 \\ Linear attention (einsum) & 0.1333 & & 0.2938 & 12029 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison between factorized and linear attention on their forward/backward computational cost, Mem. denotes the peak memory usage. The benchmark is carried out using PyTorch 1.8.2 on an RTX 3090, with a batch size of 4. **Enc. time**: the time spent on obtaining the latent encoding, primarily includes attention layers and feedforward layers after each attention layer; **Prop. time**: the time used to propagate dynamics in the latent space with a 3-layer MLP.

addition to the accuracy improvement, the benchmark on computation empirically demonstrates the computational efficiency improvement of factorized attention over linear attention. We provide more detailed comparison between factorized attention and linear attention in the Section D of Appendix, where we observed consistent efficiency improvement with different grid sizes and model sizes.

Pattern of attention matricesWe also investigate the structure of different attention matrices. By construction, when using softmax-free attention to compute the kernel integral in (2), the kernel matrix \(A=QK^{T}\) is going to have a low-rank structure since \(\text{rank}(A)\leq\min(\text{rank}(Q),\text{rank}(K))\), \(Q,K\in\mathbb{R}^{N\times d}\) and usually\(N>>d\). After training, we compute the attention matrices based on \(100\) samples and conduct singular value decomposition (SVD) on them. We define the total energy of the spectrum as the sum of singular values \(E=\sum_{i}\sigma_{i}\), where \(\sigma_{i}\) is the \(i\)-th singular value and report the normalized cumulative energy histogram \(b_{k}=\sum_{i=1}^{k}\sigma_{i}/\sum_{i}\sigma_{i}\) in Figure 4(a), 4(b), 4(c). For each layer, the histogram is averaged across the attention matrices of all heads. Observe that for linear attention, its rank is relatively low as less than 5% of the singular values capture over 90% of the total energy, which is similar to the trend observed from previous works studying the rank of standard softmax-attention [6; 26; 120]. Note that the spectrum of linear attention is based on a truncated SVD and therefore its rank will be even lower if a full SVD is performed. The highly low-rank structure of the full attention matrix hints the potential to approximate with or decomposed into smaller and more compact matrices, and our proposed factorized scheme is one example.

## 5 Conclusion

In this work, we propose an end-to-end Transformer for PDE modeling, which features a learnable projection operator and a factorized kernel integral. We demonstrate that the proposed model balances efficiency and accuracy well, making it a promising and scalable solution for PDE surrogate modeling. However, the proposed attention mechanism is still not free from the curse of dimensionality. The computation of the factorized kernel integral requires evaluating the function on all \(S_{1}\times S_{2}\times\ldots\times S_{m}\) grid points. A future direction could be extending the factorization scheme to a more efficient tensor decomposition format like tensor-train. The proposed model currently exploits the uniform structure of the underlying grids and use mean pooling when doing projection, but non-uniform quadrature weight will be necessary when applying to non-uniform grids. It is also observed that the proposed model and other neural PDE solvers can be unstable due to the error accumulation when solving time-dependent systems.

## Acknowledgement

This work is supported by the National Science Foundation under Grant No. 1953222. The authors would like to thank the annonymous reviewers and area chair for their efforts and valuable feedback during the reviewing process. The authors would like to thank Dr. Shuhao Cao from University of Missouri - Kansas City for the comments regarding the backpropagation of feedforward layer and attention layer in Transformer. The authors would also like to thank Zhi Ye for the suggestions on benchmarking the computational cost of the model.

Figure 5: Spectrum of different attention matrices. **(a)**: Attention matrices from softmax-free attention. **(b), (c)**: Attention matrices for axis \(x\) and \(y\) from FactFormer. The vertical line indicates the fraction of singular values that capture \(90\%\) of the total energy. Since the full attention matrix \(A\) has a relatively large size (\(16384\times 16384\)). Therefore its computation is computed via TruncatedSVD [39] with top \(1024\) components truncated.

## References

* [1]_Large-Scale Dynamics and Transition to Turbulence in the Two-Dimensional Kolmogorov Flow_, pages 374-396. doi: 10.2514/5.9781600865831.0374.0396. URL https://arc.aiaa.org/doi/abs/10.2514/5.9781600865831.0374.0396.
* Bahdanau et al. [2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2014. URL https://arxiv.org/abs/1409.0473.
* Bar-Sinai et al. [2019] Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizations for partial differential equations. _Proceedings of the National Academy of Sciences_, 116(31):15344-15349, 2019.
* Beltagy et al. [2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.
* Bhattacharya et al. [2021] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. Model reduction and neural networks for parametric pdes, 2021.
* Bhojanapalli et al. [2021] Srinadh Bhojanapalli, Ayan Chakrabarti, Himanshu Jain, Sanjiv Kumar, Michal Lukasik, and Andreas Veit. Eigen analysis of self-attention and its reconstruction from partial computation, 2021.
* Boulle et al. [2022] Nicolas Boulle, Seick Kim, Tianyi Shi, and Alex Townsend. Learning green's functions associated with time-dependent partial differential equations. _Journal of Machine Learning Research_, 23(218):1-34, 2022.
* Brandstetter et al. [2022] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford neural layers for pde modeling. _arXiv preprint arXiv:2209.04934_, 2022.
* Brandstetter et al. [2022] Johannes Brandstetter, Max Welling, and Daniel E Worrall. Lie point symmetry data augmentation for neural PDE solvers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 2241-2256. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/brandstetter22a.html.
* Brandstetter et al. [2022] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. _arXiv preprint arXiv:2202.03376_, 2022.
* Brandstetter et al. [2023] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K. Gupta. Clifford neural layers for pde modeling, 2023.
* Brandstetter et al. [2023] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers, 2023.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Cachay et al. [2021] Salva Ruhling Cachay, Peetak Mitra, Haruki Hirasawa, Sookyung Kim, Subhashis Hazarika, Dipti Hingmire, Phil Rasch, Hansi Singh, and Kalai Ramea. Climformer-a spherical transformer model for long-term climate projections.
* Cai et al. [2021] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (pinns) for fluid mechanics: A review. _Acta Mechanica Sinica_, 37(12):1727-1738, 2021.
* Cao [2021] Shuhao Cao. Choose a transformer: Fourier or galerkin. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 24924-24940. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf.

* Cao et al. [2022] Shuhao Cao, Peng Xu, and David A. Clifton. How to understand masked autoencoders, 2022.
* Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020.
* Chandler and Kerswell [2013] Gary J. Chandler and Rich R. Kerswell. Invariant recurrent solutions embedded in a turbulent two-dimensional kolmogorov flow. _Journal of Fluid Mechanics_, 722:554-595, 2013. doi: 10.1017/jfm.2013.122.
* Chattopadhyay et al. [2020] Ashesh Chattopadhyay, Mustafa Mustafa, Pedram Hassanzadeh, and Karthik Kashinath. Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence. In _Proceedings of the 10th international conference on climate informatics_, pages 106-112, 2020.
* Chen and Chen [1995] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. _IEEE Transactions on Neural Networks_, 6(4):911-917, 1995. doi: 10.1109/72.392253.
* Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.
* Choromanski et al. [2022] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022.
* Chu and Thuerey [2017] Mengyu Chu and Nils Thuerey. Data-driven synthesis of smoke flows with CNN-based feature descriptors. _ACM Transactions on Graphics_, 36(4):1-14, jul 2017. doi: 10.1145/3072959.3073643. URL https://doi.org/10.1145/2F3072959.3073643.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dong et al. [2021] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth, 2021.
* Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Dresdner et al. [2022] Gideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-Nunez, Jamie A. Smith, Michael P. Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating turbulent flows, 2022.
* de O Fonseca et al. [2023] Antonio H de O Fonseca, Emanuele Zappala, Josue Ortega Caro, and David van Dijk. Continuous spatiotemporal transformers. _arXiv preprint arXiv:2301.13338_, 2023.
* Fukami et al. [2019] Kai Fukami, Koji Fukagata, and Kunihiko Taira. Super-resolution reconstruction of turbulent flows with machine learning. _Journal of Fluid Mechanics_, 870:106-120, 2019. doi: 10.1017/jfm.2019.238.
* Gao et al. [2022] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu, Yuyang Bernie Wang, Mu Li, and Dit-Yan Yeung. Earthformer: Exploring space-time transformers for earth system forecasting. _Advances in Neural Information Processing Systems_, 35:25390-25403, 2022.
* Geneva and Zabaras [2022] Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. _Neural Networks_, 146:272-289, 2022.
* Graves et al. [2014] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014. URL https://arxiv.org/abs/1410.5401.
* Guibas et al. [2021] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. _arXiv preprint arXiv:2111.13587_, 2021.

* Guo et al. [2022] Ruchi Guo, Shuhao Cao, and Long Chen. Transformer meets boundary value inverse problems. _arXiv preprint arXiv:2209.14977_, 2022.
* Gupta et al. [2021] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equations, 2021.
* Gupta and Brandstetter [2022] Jayesh K. Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling, 2022.
* Gupta and Brandstetter [2022] Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling. _arXiv preprint arXiv:2209.15616_, 2022.
* Halko et al. [2010] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010.
* Han et al. [2018] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. _Proceedings of the National Academy of Sciences_, 115(34):8505-8510, 2018.
* Han et al. [2022] Xu Han, Han Gao, Tobias Pfaff, Jian-Xun Wang, and Li-Ping Liu. Predicting physics in mesh-reduced space with temporal attention. _arXiv preprint arXiv:2201.09113_, 2022.
* Hao et al. [2023] Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, and Jun Zhu. Physics-informed machine learning: A survey on problems, methods and applications, 2023.
* Hao et al. [2023] Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. _arXiv preprint arXiv:2302.14376_, 2023.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* Ho et al. [2020] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers, 2020. URL https://openreview.net/forum?id=H1e5GJBtDr.
* Holl et al. [2020] Philipp Holl, Vladlen Koltun, and Kiwon Um. phillow: A differentiable pde solving framework for deep learning via physical simulations.
* Jagtap and Karniadakis [2020] Ameya D Jagtap and George Em Karniadakis. Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations. _Communications in Computational Physics_, 28(5):2002-2041, 2020.
* JANNY et al. [2023] Steeven JANNY, Aurelien Beneteau, Madiha Nadri, Julie Digne, Nicolas THOME, and Christian Wolf. EAGLE: Large-scale learning of turbulent fluid dynamics with mesh transformers. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=mIX4QpsARJ.
* Jiang et al. [2020] Chiyu "Max" Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A. Tchelepi, Philip Marcus, Mr Prabhat, and Anima Anandkumar. MeshfreeFlownet: A physics-constrained deep continuous space-time super-resolution framework. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15, 2020. doi: 10.1109/SC41405.2020.00013.
* Jin et al. [2022] Pengzhan Jin, Shuai Meng, and Lu Lu. Mionet: Learning multiple-input operators via tensor product. _SIAM Journal on Scientific Computing_, 44(6):A3490-A3514, 2022.
* Jumper et al. [2021] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.

* Karniadakis et al. [2021] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* Katharopoulos et al. [2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5156-5165. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html.
* Kissas et al. [2022] Georgios Kissas, Jacob Seidman, Leonardo Ferreira Guilhoto, Victor M. Preciado, George J. Pappas, and Paris Perdikaris. Learning operators with coupled attention, 2022.
* Kitaev et al. [2020] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rkgNKhttvB.
* Kochkov et al. [2021] Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, Michael P. Brenner, and Stephan Hoyer. Machine learning-accelerated computational fluid dynamics. _Proceedings of the National Academy of Sciences_, 118(21), may 2021. doi: 10.1073/pnas.2101784118. URL https://doi.org/10.1073%2Fpnas.2101784118.
* Kolda and Bader [2009] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. _SIAM Rev._, 51(3):455-500, aug 2009. ISSN 0036-1445. doi: 10.1137/07070111X. URL https://doi.org/10.1137/07070111X.
* Kossaifi et al. [2023] Jean Kossaifi, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, and Anima Anandkumar. Multi-grid tensorized fourier neural operator for high resolution PDEs, 2023. URL https://openreview.net/forum?id=po-oqRst4Xm.
* Kovachki et al. [2021] Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for fourier neural operators, 2021.
* Kovachki et al. [2021] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021.
* Lam et al. [2022] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, and Peter Battaglia. Graphcast: Learning skillful medium-range global weather forecasting, 2022.
* Lamorgese et al. [2004] A. G. Lamorgese, D. A. Caughey, and S. B. Pope. Direct numerical simulation of homogeneous turbulence with hyperviscosity. _Physics of Fluids_, 17(1), 12 2004. ISSN 1070-6631. doi: 10.1063/1.1833415. URL https://doi.org/10.1063/1.1833415. 015106.
* Lebedev et al. [2015] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition, 2015.
* Li et al. [2019] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids, 2019.
* Li and Farimani [2022] Zijie Li and Amir Barati Farimani. Graph neural network-accelerated lagrangian fluid simulation. _Computers & Graphics_, 103:201-211, 2022. ISSN 0097-8493. doi: https://doi.org/10.1016/j.cag.2022.02.004. URL https://www.sciencedirect.com/science/article/pii/S0097849322000206.
* Li et al. [2022] Zijie Li, Tianqin Li, and Amir Barati Farimani. TPU-GAN: Learning temporal coherence from dynamic point cloud sequences. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=FEBFJ98FFx.
* Li et al. [2023] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=EPPqt3uERT.

* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations, 2020.
* Li et al. [2020] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020.
* Li et al. [2023] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equations, 2023.
* Liu et al. [2022] Xinliang Liu, Bo Xu, and Lei Zhang. Ht-net: Hierarchical transformer based operator learning model for multiscale pdes. _arXiv preprint arXiv:2210.10890_, 2022.
* Liu et al. [2023] Xinliang Liu, Bo Xu, and Lei Zhang. Mitigating spectral bias for the multiscale operator learning with hierarchical attention, 2023.
* Lotzsch et al. [2022] Winfried Lotzsch, Simon Ohler, and Johannes Otterbach. Learning the solution operator of boundary value problems using graph neural networks. In _ICML 2022 2nd AI for Science Workshop_, 2022. URL https://openreview.net/forum?id=4vx9FQ47wiC.
* Lu et al. [2019] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Lu et al. [2021] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving differential equations. _SIAM review_, 63(1):208-228, 2021.
* Lu et al. [2022] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, apr 2022. doi: 10.1016/j.cma.2022.114778. URL https://doi.org/10.1016/j.cma.2022.114778.
* Luong et al. [2015] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation, 2015. URL https://arxiv.org/abs/1508.04025.
* Ma et al. [2019] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Dawei Song, and Ming Zhou. A tensorized transformer for language modeling, 2019.
* Mortensen and Langtangen [2016] Mikael Mortensen and Hans Petter Langtangen. High performance python for direct numerical simulations of turbulent flows. _Computer Physics Communications_, 203:53-65, jun 2016. doi: 10.1016/j.cpc.2016.02.005. URL https://doi.org/10.1016%2Fj.cpc.2016.02.005.
* Nguyen et al. [2022] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourierformer: Transformer meets generalized fourier integral theorem. _Advances in Neural Information Processing Systems_, 35:29319-29335, 2022.
* Nguyen et al. [2023] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, and Aditya Grover. Climax: A foundation model for weather and climate, 2023.
* Novikov et al. [2015] Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing neural networks, 2015.

* Oseledets [2011] I. V. Oseledets. Tensor-train decomposition. _SIAM Journal on Scientific Computing_, 33(5):2295-2317, 2011. doi: 10.1137/090752286. URL https://doi.org/10.1137/090752286.
* Ovadia et al. [2023] Oded Ovadia, Adar Kahana, Panos Stinis, Eli Turkel, and George Em Karniadakis. Vito: Vision transformer-operator. _arXiv preprint arXiv:2303.08891_, 2023.
* Pang et al. [2019] Guofei Pang, Lu Lu, and George Em Karniadakis. fpinns: Fractional physics-informed neural networks. _SIAM Journal on Scientific Computing_, 41(4):A2603-A2626, 2019.
* Pant et al. [2021] Pranshu Pant, Ruchit Doshi, Pranav Bahl, and Amir Barati Farimani. Deep learning for reduced order modelling and efficient temporal evolution of fluid simulations. _Physics of Fluids_, 33(10):107101, oct 2021. doi: 10.1063/5.0062546. URL https://doi.org/10.1063%2F5.0062546.
* Pathak et al. [2020] Jaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, and Marcus Day. Using machine learning to augment coarse-grid computational fluid dynamics simulations. _arXiv preprint arXiv:2010.00072_, 2020.
* Pathak et al. [2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators, 2022.
* Pfaff et al. [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning mesh-based simulation with graph networks, 2021.
* Prantl et al. [2022] Lukas Prantl, Benjamin Ummenhofer, Vladlen Koltun, and Nils Thuerey. Guaranteed conservation of momentum for learning particle-based fluid dynamics, 2022.
* Quarteroni and Valli [1999] Alfio Quarteroni and Alberto Valli. _Domain decomposition methods for partial differential equations_. Number BOOK. Oxford University Press, 1999.
* Rahimi and Recht [2007] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, _Advances in Neural Information Processing Systems_, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.
* Rahman et al. [2023] Md Ashiqur Rahman, Zachary E. Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators, 2023.
* Raissi et al. [2019] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* Rasp et al. [2020] Stephan Rasp, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. WeatherBench: A benchmark data set for data-driven weather forecasting. _Journal of Advances in Modeling Earth Systems_, 12(11), nov 2020. doi: 10.1029/2020ms002203. URL https://doi.org/10.1029%2F2020ms002203.
* Rogallo [1981] Robert S. Rogallo. Numerical experiments in homogeneous turbulence. 1981.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.
* Sanchez-Gonzalez et al. [2020] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks, 2020.
* Shen et al. [2020] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities, 2020.
* Shu et al. [2023] Dule Shu, Zijie Li, and Amir Barati Farimani. A physics-informed diffusion model for high-fidelity flow field reconstruction. _Journal of Computational Physics_, 478:111972, 2023.

* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* Stachenfeld et al. [2022] Kimberly Stachenfeld, Drummond B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned coarse models for efficient turbulence simulation, 2022.
* Su et al. [2022] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022.
* Sun et al. [2020] Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data. _Computer Methods in Applied Mechanics and Engineering_, 361:112732, 2020.
* Tancik et al. [2020] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains, 2020.
* Tang et al. [2022] Jingwei Tang, Vinicius C Azeyedo, Guillaume Cordonnier, and Barbara Solenthaler. Neural green's function for laplacian systems. _Computers & Graphics_, 107:186-196, 2022.
* Thuerey et al. [2020] Nils Thuerey, Konstantin Weissenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for reynolds-averaged navier-stokes simulations of airfoil flows. _AIAA Journal_, 58(1):25-36, 2020.
* Tran et al. [2023] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators, 2023.
* Tsai et al. [2019] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: A unified understanding of transformer's attention via the lens of kernel, 2019.
* Ulyanov et al. [2017] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization, 2017.
* Um et al. [2020] Kiwon Um, Robert Brand, Yun Raymond Fei, Philipp Holl, and Nils Thuerey. Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. _Advances in Neural Information Processing Systems_, 33:6111-6122, 2020.
* Ummenhofer et al. [2020] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation with continuous convolutions. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=B1lloJSYDH.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* Wang et al. [2020] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep learning for turbulent flow prediction. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, page 1457-1466, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403198. URL https://doi.org/10.1145/3394486.3403198.
* Wang et al. [2021] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. _Science Advances_, 7(40):eabi8605, 2021. doi: 10.1126/sciadv.abi8605. URL https://www.science.org/doi/abs/10.1126/sciadv.abi8605.

* [118] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets, 2021.
* [119] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality is all you need for training physics-informed neural networks, 2022.
* [120] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [121] Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-fno--an enhanced fourier neural operator-based deep-learning model for multiphase flow. _Advances in Water Resources_, 163:104180, 2022.
* [122] Matthew A Wright and Joseph E Gonzalez. Transformers are deep infinite-dimensional non-mercer binary kernel machines. _arXiv preprint arXiv:2106.01506_, 2021.
* [123] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A nystrom-based algorithm for approximating self-attention, 2021.
* [124] Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification, 2017.
* [125] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions, 2016.
* [126] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2021.
* [127] Min Zhu, Handi Zhang, Anran Jiao, George Em Karniadakis, and Lu Lu. Reliable extrapolation of deep neural operators informed by physics or sparse observations. _Computer Methods in Applied Mechanics and Engineering_, 412:116064, 2023.

## Appendix A Model implementation details

The major hyperparameters are listed in Table 5.

_Hidden dimension_ indicates the number of channels in the latent space. _Depth_ denotes the number of attention layers. Before entering each attention layer, we modulate the latent encoding with positional encoding: \(z_{i}\gets z_{i}+\psi(x_{i})\), where \(x_{i}\) is the Cartesian coordinate of latent encoding \(z_{i}\), \(\psi:\mathbb{R}^{n}\mapsto\mathbb{R}^{d}\) is a random Fourier feature mapping [94, 107] with a learnable linear transformation. _Kernel dimension_ is the dimension of each head, which is equivalent to \(d_{k}\), the number of function bases used to compute the kernel: \(\kappa(x,\xi)=\sum_{l}^{d_{k}}q_{l}(x)k_{l}(\xi)\). We train the model with AdamW optimizer and cyclic learning rate scheduler with a maximum learning rate \(3e-4\), similar to prior Transformer-for-PDE works [16, 43, 67].

For 3D smoke buoyancy and 2D Darcy flow problem, we append a CNN block after attention layers to better account for the boundary values. The CNN block has a U-shape arrangement with 4 CNN layers, with all layers using a kernel size of 3 and padding size of 1 (pad with zeros). The first CNN layer has a stride of 2, while other layers have a stride of 1. The stride-2 convolution will downsample the data by half, so nearest upsampling is applied between the second and third CNN layers to recover the spatial resolution.

On top of every model (including baselines we will discuss in the next section), we use a 2D convolutional layer to compress the temporal dimension if it is a time-dependent problem. Concretely, we first reshape the input into \((N,T_{\text{in}})\) where \(N\) is the number of spatial grid points and \(T_{\text{in}}\) is the number of input frames. Then we apply 2D convolution filters of size \((1,T_{\text{in}})\) to compress the temporal dimension to \(1\). At the bottom of every model, we use a three-layer MLP to project the latent encoding back to variables of interest such as pressure and velocities. In addition, we adopt a curriculum training strategy for all latent marching models, where we only march for 1 step at the beginning of the training and don't do any pushforward. Then we gradually increase the latent marching steps throughout the training and apply pushforward when the model has been trained for around \(6\%\) of the total epochs.

## Appendix B Baseline implementation details

In this section, we provide the full details of baseline models, namely FNO, F-FNO, Dil-ResNet, and linear attention Transformer.

For Fourier Neural Operator, the implementation is taken from Li et al. [72]'s official implementation: https://github.com/neuraloperator/physics_informed. And for Factorized Fourier Neural Operator the implementation is taken from https://github.com/alasdairtran/fourierflow. We add group normalization before the final fully-connected layer. We use a hidden size of 96, a mode number of 12 for 3D problems, 24 for 2D turbulence, 20 for Darcy flow, and a layer number of 4.

For Dil-ResNet, we adopt the implementation from Gupta and Brandstetter [37]:https://github.com/microsoft/pdearena. Compared to Gupta and Brandstetter [37] and Stachenfeld et al. [104], we simplify the setting by truncating the number of layers inside each block, where we use dilation layers \([1,3,8,3,1]\) (\([1,2,4,2,1]\) is chosen for the 3D problem as it has slightly better performance) inside each block instead of \([1,2,4,8,4,2,1]\). The primary reason is that without truncation, the training on 3D problems will take over a week (and cannot fit into a single A6000 GPU for 2-step

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Hyperparameter & 2D Kolmogorov & 3D turbulence & 3D smoke & 2D Darcy flow \\ \hline Hidden dimension & 128 & 128 & 128 & 128 \\ Depth & 4 & 4 & 3 & 3 \\ Heads & 8 & 6 & 6 & 12 \\ Kernel dimension & 128 & 192 & 192 & 128 \\ Input encoder & 2D Conv & 2D Conv & 2D Conv & MLP \\ Output decoder & MLP & MLP & MLP & MLP \\ \hline \hline \end{tabular}
\end{table}
Table 5: Major hyperparameters for FactFormer

[MISSING_PAGE_FAIL:20]

## Appendix D Further ablation study

This section includes further study and numerical experiments on the proposed model.

Note on backpropagationConsider a single (softmax-free) attention block that consists of a attention layer and a two layer feedforward network (for simplicity we consider single-headed case):

\[U_{1}=\text{Att}(U_{0}),\quad U_{2}=\sigma(U_{1}W_{1})W_{2},\] (11)

where \(U_{0}\in\mathbb{R}^{N\times d}\) is the input, \(W_{1},W_{2}\in\mathbb{R}^{d\times d}\) are learnable weights for the feedforward network. Given a loss function \(l(\cdot):\mathbb{R}^{N\times d}\mapsto\mathbb{R}\) (for example, mean squared error), define \(\tilde{U_{1}}:=U_{1}W_{1}\), \(\frac{\partial l}{\partial U_{2}}:=h\), \(\frac{\partial\sigma(U_{1})}{\partial U_{1}}:=g\), the gradient for the weights and input in the feedforward network are (\(\odot\) denotes element-wise multiplication):

\[\frac{\partial l}{\partial W_{2}} =\sigma(\tilde{U_{1}})^{T}h\] (12) \[\frac{\partial l}{\partial W_{1}} =U_{1}^{T}(hW_{2}^{T}\odot g)\] (13) \[\frac{\partial l}{\partial U_{1}} =(hW_{2}^{T}\odot g)W_{1}^{T}\] (14)

For linear (softmax-free) dot-product attention: \(U_{1}=\text{Att}(U_{0})=QK^{T}V\), where \(Q=U_{0}W_{q},K=U_{0}W_{k},V=U_{0}W_{v}\), and thus \(U_{1}=U_{0}W_{q}W_{k}^{T}U_{0}^{T}U_{0}W_{v}\), the gradient of weights are:

\[\frac{\partial l}{\partial W_{q}} =U_{0}^{T}\frac{\partial l}{\partial U_{1}}W_{v}^{T}U_{0}^{T}U_{0 }W_{k},\] (15) \[\frac{\partial l}{\partial W_{k}} =U_{0}^{T}U_{0}W_{v}(\frac{\partial l}{\partial U_{1}})^{T}U_{0}W_ {q},\] (16) \[\frac{\partial l}{\partial W_{v}} =U_{0}^{T}U_{0}W_{k}W_{q}^{T}U_{0}^{T}\frac{\partial l}{\partial U _{1}},\] (17)

where \(U_{0}\in\mathbb{R}^{N\times d}\) has appeared three times in each calculation and thus in backpropagation the computational cost of attention layer is generally more expensive than the feedforward layer where it involves more matrices that grow exponentially with respect to the spatial resolution.

Next we provide a comparison between the backpropagation of linear attention and the proposed factorized attention in scalar summation form. For simplicity, we only consider the attention layer. For linear attention, it can be written as:

\[Z_{i,c}=\sum_{m=1}^{d}Q_{i,m}(\sum_{j=1}^{N}K_{j,m}V_{j,c}),\] (18)where \(Z_{i,c}\) denotes the \(i\)-th row and \(j\)-th column of matrix \(Z\) and similar for other matrices, \(Q=XW_{q},K=XW_{k},V=XW_{v}\) and \(X\in\mathbb{R}^{N\times d}\) is input. The gradient of parameters are computed as:

\[\frac{\partial Z_{i,c}}{\partial(W_{q})_{r,s}} =X_{i,r}(\sum_{j=1}^{N}K_{j,s}V_{j,c}),\] (19) \[\frac{\partial Z_{i,c}}{\partial(W_{k})_{r,s}} =Q_{i,s}(\sum_{j=1}^{N}X_{j,r}V_{j,c}),\] (20) \[\frac{\partial Z_{i,c}}{\partial(W_{v})_{r,s}} =\begin{cases}\sum_{m=1}^{d}Q_{i,m}(\sum_{j=1}^{N}K_{j,m}X_{i,r})& \text{: if }s=c\\ 0&\text{: otherwise.}\end{cases}\] (21)

For the proposed factorized attention, it can written as:

\[Z_{i,c}=\sum_{j_{1}=1}^{S_{1}}\sum_{j_{2}=1}^{S_{2}}\ldots\sum_{j_{n}=1}^{S_{ n}}A_{i_{1},j_{1}}^{(1)}A_{i_{2},j_{2}}^{(2)}\ldots A_{i_{n},j_{n}}^{(n)}V_{j,c} \,,\quad j:=(j_{1},j_{2},\ldots,j_{n}),\;i:=i_{1},i_{2},\ldots,i_{n}\] (22)

where \(A^{(m)}=Q^{(n)}(K^{(m)})^{T}\) (\(A^{(m)}\in\mathbb{R}^{S_{m}\times S_{m}},N=S_{1}\times S_{2}\times\ldots\times S_{n}\)) is the axial kernel matrix as defined in (8), \(X^{(m)}\in\mathbb{R}^{S_{m}\times d}\) is the axial projection along \(m\)-th axis as defined in (6).

For \(W_{v}\), its gradient is computed as:

\[\frac{\partial Z_{i,c}}{\partial(W_{v})_{r,s}} =\begin{cases}\sum_{j_{1}=1}^{S_{1}}\sum_{j_{2}=1}^{S_{2}}\ldots \sum_{j_{n}=1}^{S_{n}}A_{i_{1},j_{1}}^{(1)}A_{i_{2},j_{2}}^{(2)}\ldots A_{i_{ n},j_{n}}^{(n)}X_{j,r}&\text{: if }s=c\\ 0&\text{: otherwise.}\end{cases}\] (23)

For \(W_{q}^{(n)},W_{k}^{(n)}\), their gradients are computed as:

\[\frac{\partial Z_{i,c}}{\partial(W_{q}^{(n)})_{r,s}} =X_{i_{n},r}^{(n)}\sum_{j_{1}=1}^{S_{1}}\sum_{j_{2}=1}^{S_{2}} \ldots\sum_{j_{n}=1}^{S_{n}}A_{i_{1},j_{1}}^{(1)}A_{i_{2},j_{2}}^{(2)}\ldots A _{i_{n-1},j_{n-1}}^{(n-1)}V_{j,c}K_{j_{n},s}^{(n)},\] (24) \[\frac{\partial Z_{i,c}}{\partial(W_{k}^{(n)})_{r,s}} =Q_{i_{n},s}^{(n)}\sum_{j_{1}=1}^{S_{1}}\sum_{j_{2}=1}^{S_{2}} \ldots\sum_{j_{n}=1}^{S_{n}}A_{i_{1},j_{1}}^{(1)}A_{i_{2},j_{2}}^{(2)}\ldots A _{i_{n-1},j_{n-1}}^{(n-1)}V_{j,c}X_{j_{n},r}^{(n)},\] (25)

The major difference is that for factorized attention the summation is taken over each axis separately while for linear attention is taken over all \(N\) grid points.

Runtime comparisonAs discussed in Section A, the kernel dimension indicates how many function bases are used to evaluate the kernel and a larger kernel dimension is beneficial to the learning capacity of the model. As shown in Figure 10(a), 10(b), linear attention's training cost increases more significantly than the factorized attention as kernel dimension increases, since its complexity is quadratic with respect to kernel dimension. Factorized attention's computational efficiency can be further improved by reducing the spatial resolution, leveraging techniques such as learning the mapping in the latent space (similar to latent diffusion model [99]), multi-scale network architecture that resembles multigrid methods [35, 74], or domain decomposition [47, 93]. Furthermore, the training cost of factorized attention is also relatively lower than linear attention on 3D domain as shown in Figure 11(a), 11(b).

Model scaling performanceWe study the impact of the number of heads and size of kernel dimension on prediction loss. For each direction of hyperparameter search, we fix the value of other hyperparameters to that shown in Table 5. The ablation experiments are conducted on 2D Kolmogorov flow (sampled from a \(128\times 128\) grid) with a splitting different from the Evaluation Section in the main body of the paper. As shown in Figure 12(b), the number of attention heads has a crucial impact on the final performance. The model's performance drops significantly when using fewer heads. This highlights the importance of multi-head mechanism in the factorized attention. In addition, we observe that the final accuracy of our model benefits from an increased kernel dimension (as shown in Figure 12(a)).

Figure 11: Benchmark of factorized attention and linear attention on 2D domain (with \(128\times 128\) grid) with varying kernel dimension. Benchmark is done on an RTX 3090 with PyTorch 1.8.2 and a batch size of 4. Hyperparameter setting is the same as in Table 5-2D Kolmogorov flow. â€œOOMâ€ denotes out of memory.

Figure 12: Benchmark of factorized attention and linear attention on 3D domain with varying grid size. Benchmark is done on an A6000 with PyTorch 1.8.2 and batch size of 1. Hyperparameter setting is the same as in Table 5-3D isotropic turbulence.

Figure 13: Ablation study on the key hyperparameters. Red color denotes the final choice of hyperparameter. Experiments are carried out on the validation fold.

Visualization of learned kernelsWe visualize the learned kernel as shown in Figure 14. Due to the presence of Rotary positional encoding [105], all kernels have a stationary pattern (the kernel value \(\kappa(\xi_{1},\xi_{2})\) depends only on the relative distance between two points, e.g. \(L^{2}\) distance: \(\left|\left|\xi_{1}-\xi_{2}\right|\right|_{2}\)). The kernel matrices also exhibits symmetric pattern despite the non-symmetric nature of dot product \(QK^{T}\) and all kernel matrices are diagonal dominated.

Influence of random seedWe investigate the influence of random seeds by training the model with three different seeds. As shown in Figure 15, all models converge to the similar level of loss with marginal difference.

## Appendix E Dataset details

In this section we provide the details for each dataset.

2D Kolmogorov flowThe incompressible Navier-Stokes equation under vorticity form reads as,

\[\frac{\partial\omega(\mathbf{x},t)}{\partial t}+\mathbf{u}( \mathbf{x},t)\cdot\nabla\omega(\mathbf{x},t) =\frac{1}{\textit{Re}}\nabla^{2}\omega(\mathbf{x},t)+f(\mathbf{x }), \mathbf{x} \in(0,2\pi)^{2},t\in(0,T],\] \[\nabla\cdot\mathbf{u}(\mathbf{x},t) =0, \mathbf{x} \in(0,2\pi)^{2},t\in[0,T],\] (26) \[\omega(\mathbf{x},0) =\omega_{0}(\mathbf{x}), \mathbf{x} \in(0,2\pi)^{2},\]

where \(\omega\) denotes vorticity, \(\mathbf{u}\) denotes velocity, _Re_ denotes the Reynolds number, \(\mathbf{x}=(x_{1},x_{2})\) denotes the spatial coordinates and \(f(\cdot)\) is the forcing term that is set to \(f(\mathbf{x})=-n\cos{(nx_{2})}-0.1\omega(\mathbf{x})\). The equation is periodic in all spatial directions. Compared to the cases discussed in Li et al. [72], we set the forcing factor \(n\) to 8 [1, 19] and introduce dragging force term \(0.1\omega(\mathbf{x})\) as described in Kochkov et al. [56]. The initial condition \(\omega_{0}\) is sampled from a prescribed Gaussian random field

Figure 14: Visualization of normalized attention kernel.

Figure 15: Averaged frame-wise loss trends. Each loss curve corresponds to a model initialized under a specific seed.

same as Li et al. [72]. The dataset consists of \(100\) trajectories for training and \(20\) trajectories for testing, with the length of each trajectory being \(10\) seconds and \(160\) frames.

We modify the pseudo-spectral solver (under Apache License 2.0) from https://github.com/neuraloperator/physics_informed/blob/master/solver/kolmogorov_flow.py to generate the data. The referenced direct numerical simulation is carried out with a spatial resolution of \(2048\times 2048\) and a temporal resolution of \(1e-4\).

3D isotropic turbulenceThe incompressible Navier-Stokes equation for this problem is given as:

\[\frac{\partial\mathbf{u}(\mathbf{x},t)}{\partial t}+\mathbf{u}( \mathbf{x},t)\cdot\nabla\mathbf{u}(\mathbf{x},t) =\nu\nabla^{2}\mathbf{u}(\mathbf{x},t)-\frac{1}{\rho}\nabla p( \mathbf{x},t)+\mathbf{f}(\mathbf{x}), \mathbf{x} \in(0,2\pi)^{3},t\in(0,T],\] \[\nabla\cdot\mathbf{u}(\mathbf{x},t) =0, \mathbf{x} \in(0,2\pi)^{3},t\in[0,T],\] (27) \[\mathbf{u}(\mathbf{x},0) =\mathbf{u}_{0}(\mathbf{x}), \mathbf{x} \in(0,2\pi)^{3},\]

where \(\mathbf{u}\) denotes velocity, \(p\) denotes the pressure, \(\nu\) is the viscosity parameter, \(\mathbf{x}=[x_{1},x_{2},x_{3}]\) denotes the spatial coordinates and \(f(\cdot)\) is the forcing term. The equation is periodic in all three spatial dimensions. The initialization of \(\mathbf{u}\) and the forcing settings follow Rogallo [98] and Lamorgese et al. [62] respectively, with Taylor Reynolds number set to \(84\)[62]. The dataset consists of \(1000\) trajectories for training and \(100\) trajectories for testing, with the length of each trajectory being \(1\) second and \(20\) frames.

We use the spectral Galerkin solver (under GNU GPL license 3.0) from https://github.com/spectralDNS. The referenced simulation is carried out with a spatial resolution of \(60\times 60\times 60\) and a temporal resolution of \(0.005s\).

3D smoke buoyancyThe governing equations for the 3D smoke buoyancy problem are incompressible Navier-Stokes equation (similar as above) coupled with advection equation:

\[\frac{\partial\mathbf{u}(\mathbf{x},t)}{\partial t}+\mathbf{u}( \mathbf{x},t)\cdot\nabla\mathbf{u}(\mathbf{x},t) =\nu\nabla^{2}\mathbf{u}(\mathbf{x},t)-\frac{1}{\rho}\nabla p( \mathbf{x},t)+\mathbf{f}(\mathbf{x},t), \mathbf{x} \in(0,L)^{3},t\in(0,T],\] \[\frac{\partial d(\mathbf{x},t)}{\partial t}+\mathbf{u}(\mathbf{x },t)\cdot\nabla d(\mathbf{x},t) =0, \mathbf{x} \in(0,L)^{3},t\in(0,T],\] \[\nabla\cdot\mathbf{u}(\mathbf{x},t) =0, \mathbf{x} \in(0,L)^{3},t\in[0,T],\] \[\mathbf{u}(\mathbf{x},0)=0, \mathbf{d}(\mathbf{x},0) =d_{0}(\mathbf{x}), \mathbf{x} \in(0,L)^{3},\]

where \(\mathbf{f}(\mathbf{x},t)=[0,0,\eta d(\mathbf{x},t)]\), \(\eta\) is the buoyancy factor, the velocity field \(\mathbf{u}\) has a Dirichlet boundary condition: \(\mathbf{u}(\mathbf{x},\cdot)=0,\forall\mathbf{x}\in\partial\Omega\), and the scalar density field for smoke has a Neumann boundary condition: \(\nabla d(\mathbf{x},\cdot)=0,\forall\mathbf{x}\in\partial\Omega\). The initial condition \(d_{0}(\mathbf{x})\) is a random field 3 with scaling of Fourier coefficient set to \(15.0\), smoothness factor set to \(4.0\). The length of the rectangular domain \(L\) is set to \(8\). The dataset consists of \(2000\) trajectories for training and \(200\) trajectories for testing, with the length of each trajectory being \(15\) seconds and \(20\) frames.

Footnote 3: Implemented with _phiflow_â€™s Noise class, see:https://tum-pbs.github.io/PhiFlow/phi/field/

We modify the 2D solver (under MIT license) from https://github.com/microsoft/pdearena to generate the data. The solver applies an advection-project scheme. The referenced simulation is carried out with a spatial resolution of \(64\times 64\times 64\) and a temporal resolution of \(0.75s\).

2D Darcy flowThe equation for the 2D Darcy flow is defined as:

\[-\nabla\cdot(a(x)\nabla u(x))=f(x), x\in(0,1)^{2},\] (28) \[u_{0}(x)=0, x\in\partial(0,1)^{2},\]

where \(f(x)\) is the forcing function that is set to constant \(1\). The coefficient function \(a(x)\) is sampled from Gaussian Random Field with zero Neumann boundary condition. The data is generated via second-order finite difference solver on a \(421\times 421\) resolution grid. We use the pre-generated dataset from Li et al. [68] (under MIT license). The dataset consists of \(1000\) samples for training and \(100\) samples for testing.

[MISSING_PAGE_EMPTY:26]

Figure 19: \(x\)-component of velocity in 3D isotropic turbulence.

Figure 20: \(y\)-component of velocity in 3D isotropic turbulence.

Figure 18: Pressure in 3D isotropic turbulence.

Figure 23: \(x\)-component of velocity in 3D smoke buoyancy.

Figure 21: \(z\)-component of velocity in 3D isotropic turbulence.

Figure 22: Smoke marker field in 3D smoke buoyancy.

Figure 24: \(y\)-component of velocity in 3D smoke buoyancy.

Figure 25: \(z\)-component of velocity in 3D smoke buoyancy.

Figure 26: Flow field of 2D Darcy flow.

Broader impact

The numerical simulation of PDEs is of extensive application in various fields, such as manufacturing, weather forecasting, and engineering design. Meanwhile, Transformer has shown promising performance on a wide range of data-driven applications including PDE modeling. Our work can help improve the stability and computational efficiency of the attention-based PDE surrogate models. Our experiments demonstrate that the proposed model serves as an efficient surrogate for numerical solvers, maintaining a balance between accuracy and efficiency, and thus pushing the Pareto front of accuracy-efficiency. However, as there exists a large variety of PDEs and each with very unique properties, there is no guarantee that one type of data-driven model can rule all. Additionally, just like most concurrent works on neural PDE solvers, the long-term stability of the proposed model cannot be guaranteed. Therefore it is important to acknowledge the limitations and potential risks associated with the application of neural PDE solvers.

Apart from enriching the existing architecture design choice of attention-based models, our work also has the potential to be combined with other neural PDE solvers design formulas (e.g., explicitly take into account the relationship between different output variables), or common neural network architectures (e.g., U-Net).

## Appendix H Schematic of Axial Transformer and FactFormer

Figure 27: An illustrative example of FactFormer and Axial Transformer applying to 2D input data, with some details such as positional encoding, multi-head mechanism and softmax in Axial Transformer omitted for simplicity. For Axial Transformer, column-wise attention block is shown as an example.