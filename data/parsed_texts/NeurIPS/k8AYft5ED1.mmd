# Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation

 Kaike Zhang\({}^{1,2}\), Qi Cao\({}^{1}\)1, Yunfan Wu\({}^{1,2}\), Fei Sun\({}^{1}\), Huawei Shen\({}^{1}\), Xueqi Cheng\({}^{1}\)

\({}^{1}\) CAS Key Laboratory of AI Safety, Institute of Computing Technology,

Chinese Academy of Sciences

\({}^{2}\) University of Chinese Academy of Sciences

Beijing, China

{zhangkaike21s, caoqi, wuyunfan19b, sunfei, shenhuawei, cxq}@ict.ac.cn

Corresponding author.

###### Abstract

Adversarial Collaborative Filtering (ACF), which typically applies adversarial perturbations at user and item embeddings through adversarial training, is widely recognized as an effective strategy for enhancing the robustness of Collaborative Filtering (CF) recommender systems against poisoning attacks. Besides, numerous studies have empirically shown that ACF can also improve recommendation performance compared to traditional CF. Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear. To bridge this gap, in this paper, we first theoretically show that ACF can achieve a lower recommendation error compared to traditional CF with the same training epochs in both clean and poisoned data contexts. Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness. Building on these theoretical understandings, we propose **P**ersonalized **M**agnitude **A**dversarial **C**llaborative **F**iltering (**PamaCF**). Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.

## 1 Introduction

Collaborative Filtering (CF) is widely recognized as a powerful tool for providing personalized recommendations [1, 2, 3] across various domains [4, 5]. However, the inherent openness of recommender systems allows attackers to inject fake users into the training data, aiming to manipulate recommendations, also known as poisoning attacks [6, 7]. Such manipulations can skew the distribution of item exposure, degrading the overall quality of the recommender system, thus harming the user experience and hindering the long-term development of the recommender system [8].

Existing methods for defending against poisoning attacks in CF can be categorized into two types [8]: (1) detecting and mitigating the influence of fake users [9, 10, 11, 12, 13, 14], and (2) developing robust models via adversarial training, also known as Adversarial Collaborative Filtering (ACF) [15, 16, 17, 18, 19, 20]. The first strategy focuses on detecting and removing fake users from the dataset before training [9, 10, 11, 14] or mitigating their impact during the training phase [12, 13]. These methods often rely on predefined assumptions about attacks [9, 12] or require labeled data related to attacks [10, 11, 12, 13]. Consequently, deviations from predefined attack patterns may lead to misclassification, failing to resist attacks while potentially harming genuine users' experience [13].

In contrast, ACF provides a more general defense paradigm without prior knowledge [15; 16; 17; 18; 19; 20]. Poisoning attacks in recommender systems mainly affect the learned embeddings of users and items, i.e., the system's parameters [6; 7]. Predominant ACF methods, particularly those aligned with Adversarial Personalized Ranking (APR) framework [15], heuristically incorporate adversarial perturbations at the parameter level during the training phase to mitigate these attacks [15; 17; 19; 20]. This approach employs a "min-max" paradigm, designed to minimize the recommendation error while contending with parameter perturbations aimed at maximizing this error within a specified magnitude [15], thus enhancing the robustness of CF.

It is interesting to note that adversarial training in the Computer Vision (CV) domain [21; 22; 23] has been observed to degrade model performance on clean samples [24; 25]. Several studies have also theoretically demonstrated a trade-off between robustness against evasion attacks and the performance of adversarial training in CV [26]. In contrast, ACF in recommender systems has been shown in numerous studies not only to enhance the robustness against poisoning attacks [8; 13; 18] but also to improve recommendation performance [15; 20; 27]. Despite the empirical evidence highlighting ACF's advantages, it still lacks a comprehensive theoretical understanding, which limits the ability to fully exploit the benefits and potential of ACF. To bridge this gap, in this paper, we propose the following research questions for further investigation:

1. _Why does ACF enhance both robustness and performance compared to traditional CF?_
2. _How can we further improve ACF?_

To answer these questions, we delve into a theoretical analysis of a simplified CF scenario. This analysis confirms that ACF can achieve a lower recommendation error at the same training epoch in both clean and poisoned data contexts, showing better performance and robustness compared to traditional CF. To investigate potential improvements to ACF, we establish upper and lower bounds for reductions in recommendation error during ACF's optimization process. Our findings indicate that (1) Users have varying constraints for perturbation magnitudes, i.e., different maximum perturbation magnitudes; (2) Within these constraints, applying personalized perturbation magnitudes as much as possible for each user can increase the error reduction bounds, further improving ACF's effectiveness.

Extending our theoretical results to practical CF scenarios, we establish a positive correlation between users' maximum perturbation magnitudes and their embedding scales. Building on these theoretical understandings, we introduce **P**ersonal**alized **M**agnitude **A**dversarial **C**ollaborative **F**iltering (**PamaCF**). PamaCF dynamically and personally assigns perturbation magnitudes based on users' embedding scales. Extensive experiments confirm that PamaCF outperforms baselines in both performance and robustness. Notably, PamaCF increases the average recommendation performance of the backbone model by 13.84% and reduces the average success ratio of attacks by 44.92% compared to the best baseline defense method. The main contributions of our study are summarized as follows:

* We provide theoretical evidence that ACF can achieve better performance and robustness compared to traditional CF in both clean and poisoned data contexts.
* We further identify upper and lower bounds of reduction in recommendation error for ACF during its optimization and demonstrate that applying personalized magnitudes of perturbation for each user can further improve ACF.
* Based on the above theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF), with extensive experiments confirming that PamaCF further improves both performance and robustness compared to state-of-the-art defense methods.

The code of our experiments is available at https://github.com/Kaike-Zhang/PamaCF.

## 2 Preliminary

**Collaborative Filtering** (CF) methods are widely employed in recommender systems. Following [1], we define a set of users \(\mathcal{U}=\{u\}\) and a set of items \(\mathcal{V}=\{v\}\). Using data from user-item interactions, our objective is to learn latent embeddings \(\bm{U}=[\bm{u}\in\mathbb{R}^{d}]_{u\in\mathcal{U}}\) for users and \(\bm{V}=[\bm{v}\in\mathbb{R}^{d}]_{v\in\mathcal{V}}\) for items. Then, we employ a preference function \(f:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\), which predicts user-item preference scores, denoted as \(\hat{r}_{u,v}=f(\bm{u},\bm{v})\).

**Adversarial Collaborative Filtering** (ACF) is acknowledged as an effective approach for enhancing both the performance and the robustness of CF recommender systems in the face of poisoning attacks. ACF methods, particularly within the framework of Adversarial Personalized Ranking (APR) [15], integrate adversarial perturbations at the parameter level (i.e., the latent embeddings \(\bm{U}\) and \(\bm{V}\)) during the training phase. Let \(\mathcal{L}(\Theta)\) denote the loss function of the CF recommender system, where \(\Theta=(\bm{U},\bm{V})\) represents the recommender system's parameters. ACF methods apply perturbations \(\Delta\) directly to the parameters as:

\[\mathcal{L}_{\mathrm{ACF}}(\Theta)= \mathcal{L}(\Theta)+\lambda\mathcal{L}(\Theta+\Delta^{\mathrm{adv} }),\] (1) \[\mathrm{where}\quad\Delta^{\mathrm{adv}}= \arg\max_{\Delta,\,\|\Delta\|\leq\epsilon}\mathcal{L}(\Theta+ \Delta),\]

where \(\epsilon>0\) defines the maximum magnitude of perturbations, and \(\lambda\) is the adversarial training weight. Due to constraints on space, a detailed discussion of related works is provided in Appendix A.

## 3 Theoretical Understanding of ACF

In this section, we provide a theoretical analysis of why ACF achieves superior performance and robustness compared to traditional CF from the perspective of recommendation error. Then, we explore mechanisms to further improve ACF's effectiveness based on its error reduction bounds. For clarity and simplicity, we initially focus on a Gaussian Single-item Recommender System, aligning with the frameworks presented in [18; 28]. It's important to **note that** the insights and analytical frameworks developed here are also applicable to more practical scenarios, as discussed in Section 4.

**Definition 1** (Gaussian Recommender System).: _Given a rating set \(\mathcal{R}=\{r_{1},r_{2},\ldots,r_{n}\}\) corresponding to \(n\) users, where each rating \(r\) is randomly selected from \(\{\pm 1\}\), an average embedding vector \(\bar{\bm{u}}\in\mathbb{R}^{d}\), and \(\sigma>0\), the Gaussian Recommender System initializes each user's embedding \(\bm{u}\) from the normal distribution \(\mathcal{N}(r\bar{\bm{u}},\sigma^{2}\bm{I})\). The item embedding \(\bm{v}\) is initialized as the average vector derived from these users: \(\bm{v}=\frac{1}{n}\sum_{i=1}^{n}r_{i}\bm{u}_{i}\). Then, a preference function \(f:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\{\pm 1\}\) is employed to predict user preferences: \(f(\bm{u},\bm{v})=\mathrm{sgn}(\langle\bm{v},\bm{u}\rangle)\), where \(\mathrm{sgn}(\cdot)\) denotes the sign function, returning 1 if \(\langle\bm{u},\bm{v}\rangle>0\) and -1 otherwise._

Based on Definition 1, we obtain \(\mathcal{I}=\{(\bm{u}_{1},r_{1}),\ldots,(\bm{u}_{n},r_{n})\}\), where \(\bm{u}\) represents the system-learned user embedding. With continued training, both each user embedding \(\bm{u}\) and item embedding \(\bm{v}\) are iteratively updated. Let \(\bm{u}_{(t)}\) and \(\bm{v}_{(t)}\) denote user and item embeddings at the \(t^{\mathrm{th}}\) epoch, respectively. For analytical simplicity and without loss of generality, we define the standard loss function \(\mathcal{L}(\Theta)\) (as traditional CF) used in the Gaussian Recommender System as follows [18]:

\[\mathcal{L}(\Theta_{(t)})=-\sum_{(\bm{u},r)\in\mathcal{I}}\left[r \cdot\langle\bm{u}_{(t)},\bm{v}_{(t)}\rangle\right],\] (2)

where the model parameters \(\Theta_{(t)}=\left(\bm{v}_{(t)},\left[\bm{u}_{1,(t)},\bm{u}_{2,(t)},\ldots,\bm {u}_{n,(t)}\right]\right)\). To integrate ACF into the Gaussian Recommender System, we introduce the adversarial loss [15], \(\mathcal{L}_{\mathrm{adv}}(\Theta)\), defined as:

\[\mathcal{L}_{\mathrm{adv}}(\Theta_{(t)})=\mathcal{L}(\Theta_{(t) })-\lambda\sum_{(\bm{u},r)\in\mathcal{I}}\left[r\cdot\langle\bm{u}_{(t)}+ \Delta_{\bm{u}},\bm{v}_{(t)}+\Delta_{\bm{v}}\rangle\right],\] (3)

where \(\lambda\) is the adversarial training weight. The perturbations \(\Delta_{\bm{u}}\) and \(\Delta_{\bm{v}}\) are applied to the user and item embeddings, respectively, as computed based on Equation 1.

### Why Does Adversarial Collaborative Filtering Benefit Recommender Systems?

To analyze the performance and robustness of traditional CF and ACF within the Gaussian Recommender System, we evaluate them from the perspective of recommendation error during the training process. For each user, both performance and robustness are reflected by the user's recommendation error. Specifically, attacks--whether item promotion attacks [6; 29] or performance damage attacks [8]--inevitably increase the user's recommendation error. Meanwhile, a smaller recommendation error means a higher recommendation performance. For a given user \(\bm{u}\), the initial item embedding \(\bm{v}_{(0)}\) in the Gaussian Recommender System can be approximately modeled2 as a sample from \(\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}\bm{I})\). Here, we provide the definition of recommendation error for user \(\bm{u}\).

Footnote 2: The precise form is \(\mathcal{N}(\bar{\bm{u}},\frac{(n-1)\sigma^{2}}{n^{2}}\bm{I})\), but we make this approximation for the sake of clarity and brevity. The approximation does not impact the subsequent theoretical results.

**Definition 2** (Recommendation Error).: _Given a Gaussian Recommender System \(f_{(t)}\) that has been trained for \(t\) epochs, the recommendation error for the user \(\bm{u}\) with rating \(r\) at the \(t^{\mathrm{th}}\) epoch is defined as the probability that the system's prediction does not align with the user's actual rating, as:_

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\coloneqq\mathbb{E}_{ \bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}\left[\mathbb{ I}\left(f_{(t)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r),\bm{v}_{(0)}\right)\right],\]

_where \(\mathbb{I}(\cdot)\) is an indicator function that returns 1 if the condition is true and 0 otherwise._

Based on the framework of ACF [15, 20], which includes \(t\) epochs of pre-training with standard loss before adversarial training, we derive a theorem that identifies the difference in recommendation error between standard and adversarial loss at the \((t+1)^{\mathrm{th}}\) epoch. To distinguish between the recommendation error of traditional CF and ACF, we define \(\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I )}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\) as the recommendation error following standard training (Equation 2) at the \((t+1)^{\mathrm{th}}\) epoch, and \(\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I )}^{\mathrm{adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\) as the recommendation error following adversarial training (Equation 3) at the \((t+1)^{\mathrm{th}}\) epoch.

**Theorem 1**.: _Consider a Gaussian Recommender System \(f_{(t)}\), pre-trained for \(t\) epochs using the standard loss function (Equation 2). Given a learning rate \(\eta\), an adversarial training weight \(\lambda\), and a perturbation magnitude \(\epsilon\), when \(\epsilon<\frac{\min(\|\bm{u}_{(t)}(\cdot)\|,\|\bm{u}\|)}{\eta}\), and \(\|\bar{\bm{u}}\|\gg\sigma^{3}\), the recommendation error for a user \(\bm{u}\) with rating \(r\) at the \((t+1)^{\mathrm{th}}\) epoch follows that:_

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I )}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]>\mathbb{P}_{\bm{v }_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}} \left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right].\]

For the proof, please refer to Appendix D.1.1. After the same epochs of pre-training, ACF at the next epoch achieves a lower recommendation error compared to traditional CF, thereby benefiting recommendation performance.

Next, our analysis extends to contexts where the recommender system is subject to poisoning attacks. These attacks involve injecting fake users into the system's training dataset to manipulate item exposure. We examine a Gaussian Recommender System with \(\mathcal{I}=\{(\bm{u}_{1},r_{1}),\ldots,(\bm{u}_{n},r_{n})\}\), where each tuple \((\bm{u},r)\in\mathbb{R}^{d}\times\{\pm 1\}\) represents the learned embedding and the rating of a genuine user. A _poisoning attack_ on this system injects a _poisoning user set_, \(\mathcal{I}^{\prime}=\{(\bm{u}^{\prime}_{1},r^{\prime}_{1}),(\bm{u}^{\prime}_ {2},r^{\prime}_{2}),\ldots,(\bm{u}^{\prime}_{n^{\prime}},r^{\prime}_{n^{ \prime}})\}\), with each tuple \((\bm{u}^{\prime},r^{\prime})\in\mathbb{R}^{d}\times\{\pm 1\}\) representing a fake user crrafted by attackers4. The _poisoned item embedding_\(\bm{v}^{\prime}\) is reinitialized to include both genuine and malicious contributions:

Footnote 4: To make poisoning attacks effective in single-item recommendation scenarios, attackers can directly inject users’ initialized embeddings, which is equivalent to constructing interactions for different items in multi-item scenarios.

\[\bm{v}^{\prime}=\frac{1}{n+n^{\prime}}\left(\sum_{(\bm{u},r)\in\mathcal{I}}r \bm{u}+\sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}r^{\prime} \bm{u}^{\prime}\right),\]

where \(n\) and \(n^{\prime}\) represent the number of genuine and fake users, respectively.

To evaluate the impact of these attacks, we introduce a formal definition of recommendation error in poisoned data.

**Definition 3** (\(\bm{\alpha}\)-Poisoned Recommendation Error).: _Given a boundary \(\alpha>0\), and a set of fake users injected by attackers within this boundary, i.e., \(\mathcal{I}^{\prime}\subseteq\mathcal{P}(\bm{u}^{\prime},\alpha)=\{(\bm{u}^{ \prime},r^{\prime})\mid(\bm{u}^{\prime},r^{\prime})\in\mathbb{R}^{d}\times\{ \pm 1\}\wedge\|\bm{u}^{\prime}\|_{\infty}\leq\alpha\}\), the \(\alpha\)-poisoned recommendation error for the genuine user \(\bm{u}\) with rating \(r\) at the \(t^{\mathrm{th}}\) epoch is defined as the probability:_

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)\right] \coloneqq\mathbb{E}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}} {n-1}I)}\left[\mathbb{I}\left(f_{(t),\alpha}(\bm{u},\bm{v}^{\prime})\neq r \mid(\bm{u},r),\bm{v}_{(0)}\right)\right],\]

_where \(f_{(t),\alpha}\) represents the Gaussian Recommender System under the \(\bm{\alpha}\)-poisoned condition, and \(\mathbb{I}(\cdot)\) is an indicator function that returns 1 if the condition is true and 0 otherwise._

For simplicity, we continue using the distribution of \(\bm{v}_{(0)}\) from the definition. This allows us to further analyze the \(\bm{\alpha}\)-poisoned recommendation error based on the distribution of \(\bm{v}^{\prime}_{(0)}=\frac{n}{n+n^{\prime}}\bm{v}_{(0)}+\frac{1}{n+n^{\prime}} \sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}r^{\prime}\bm{u}^{\prime}\).

Then we extend Theorem 1 to \(\bm{\alpha}\)-Poisoned Recommendation Error in the following theorem:

**Theorem 2**.: _Consider a poisoned Gaussian Recommender System \(f_{(t),\alpha}\), pre-trained for \(t\) epochs using the standard loss function (Equation 2). Given a learning rate \(\eta\), an adversarial training weight \(\lambda\), and a perturbation magnitude \(\epsilon\), when \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\bar{\bm{u}}\|)}{n\lambda}\), and \(\|\bar{\bm{u}}\|\gg\sigma\), the \(\bm{\alpha}\)-poisoned recommendation error for a genuine user \(\bm{u}\) with rating \(r\) at the \((t+1)^{\mathrm{th}}\) epoch follows that:_

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)\right]> \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} ^{\mathrm{adv}}\left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{ u},r)\right].\]

For the proof, please refer to Appendix D.1.2. Combining Theorem 1 and Theorem 2, we find that adversarial training, i.e., ACF, lowers recommendation errors compared to traditional CF in both clean and poisoned data contexts. Accordingly, ACF achieves better performance and robustness.

### How to Further Enhance Adversarial Collaborative Filtering

To explore mechanisms to further improve the effectiveness of ACF, we subsequently derive upper and lower bounds on the reduction of recommendation error between any two consecutive epochs after \(t\) epochs of pre-training.

**Theorem 3**.: _Consider a Gaussian Recommender System \(f_{(t)}\) which has been pre-trained for \(t\) epochs using standard loss (Equation 2) and subsequently trained on adversarial loss (Equation 3). For the \((t+k+1)^{\mathrm{th}}\) epoch, let the reduction in recommendation error of user \(\bm{u}\) with rating \(r\) relative to the \((t+k)^{\mathrm{th}}\) epoch from adversarial loss be denoted by:_

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm{u}, \bm{v})\neq r\mid(\bm{u},r)]=\] \[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}^{\mathrm{adv}}\left[f_{(t+k)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r) \right]-\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2} }{n-1}I)}^{\mathrm{adv}}\left[f_{(t+k+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r) \right].\]

_Given a learning rate \(\eta\), an adversarial training weight \(\lambda\), and a perturbation magnitude \(\epsilon\), when \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\|\bar{\bm{u}}\|)}{\eta\lambda}\), and \(\|\bar{\bm{u}}\|\gg\sigma\), it follows that:_

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm{u}, \bm{v})\neq r\mid(\bm{u},r)]\geq\] \[\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\|\bar{\bm{u}}\|+\eta(\| \bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1})\Psi(\bm{u},t+k)\right)\right)-\Phi \left(\frac{\sqrt{n-1}}{\sigma}\|\bar{\bm{u}}\|\right),\] \[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm{u}, \bm{v})\neq r\mid(\bm{u},r)]\leq 2\Phi\left(\frac{\sqrt{n-1}\eta}{2\sigma}(\|\bar{\bm{u}}\|^ {2}+\frac{d\sigma^{2}}{n-1})\Psi(\bm{u},t+k)\right)-1,\]

_where \(d\) is the embedding dimension, and \(\Phi(\cdot)\) denotes the cumulative distribution function (CDF) of the standard Gaussian distribution, and \(\Psi(\bm{u},t+k)\) is defined as:_

\[\Psi(\bm{u},t+k)=(1+\lambda)\gamma_{(t+k)}^{\bm{u}}\frac{C_{t+k}}{\|\bm{u}_{(t +k)}\|},\quad\text{where}\quad\gamma_{(t+k)}^{\bm{u}}=\left(1-\frac{\eta\lambda \epsilon}{\|\bm{u}_{(t+k)}\|}\right)^{-1},\] (4)

_where \(C_{t+k}\) is a constant at the \((t+k)^{\mathrm{th}}\) epoch._

For the proof, please refer to Appendix D.2.1. In light of Theorem 3, given a learning rate \(\eta\) and an adversarial training weight \(\lambda\), we can establish the following: (1) When the conditions, i.e., \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\bar{\bm{u}}\|)}{n\lambda}\) and \(\|\bar{\bm{u}}\|\gg\sigma\), are satisfied, the error reduction for ACF can be both upper and lower bounded. (2) Increasing the perturbation magnitude \(\epsilon\) under the above conditions can further improve these bounds, thus benefiting ACF's effectiveness.

Then, similarly, we extend Theorem 3 to the \(\bm{\alpha}\)-poisoned context.

**Theorem 4**.: _Consider a poisoned Gaussian Recommender System \(f_{(t),\alpha}\) which has been pre-trained for \(t\) epochs using standard loss (Equation 2) and subsequently trained on adversarial loss (Equation 3). For the \((t+k+1)^{\mathrm{th}}\) epoch, let the reduction in \(\bm{\alpha}\)-poisoned recommendation error of a genuine user \(\bm{u}\) with rating \(r\) relative to the \((t+k)^{\mathrm{th}}\) epoch from adversarial loss be denoted by \(\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{ \bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f_{\alpha}(\bm{u},\bm{v}^{ \prime})\neq r\mid(\bm{u},r)]\). Let \(\beta=\frac{n^{\prime}}{n}\sqrt{d}\alpha+\|\bar{\bm{u}}\|\) and \(\tau=2nn^{\prime}\alpha\|\bar{\bm{u}}\|_{0}\), where \(d\) is the embedding dimension, and given a learning rate \(\eta\), an adversarial training weight \(\lambda\)_and a perturbation magnitude \(\epsilon\), when \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\bar{\bm{u}}\|)}{\eta\lambda}\), and \(\|\bar{\bm{u}}\|\gg\sigma\), it follows that:_

\[\Delta_{(t+k+1)}^{\mathrm{adv}}\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{ N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f_{\alpha}(\bm{u},\bm{v}^{ \prime})\neq r\mid(\bm{u},r)]\quad>\] \[\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\beta+\eta\left(\frac{n^ {2}\|\bar{\bm{u}}\|^{2}-\tau}{n(n+n^{\prime})}+\frac{nd\sigma^{2}}{(n-1)(n+n^{ \prime})}\right)\Psi(\bm{u},t+k)\right)\right)-\Phi\left(\frac{\sqrt{n-1}}{ \sigma}\beta\right),\] \[\Delta_{(t+k+1)}^{\mathrm{adv}}\mathbb{P}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f_{\alpha}( \bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)]\quad\leq\] \[\qquad\qquad 2\Phi\left(\frac{\sqrt{n-1}\eta}{2\sigma}\left(\frac{n^ {2}\|\bar{\bm{u}}\|^{2}+(n^{\prime})^{2}d\alpha^{2}+\tau}{n(n+n^{\prime})}+ \frac{nd\sigma^{2}}{(n-1)(n+n^{\prime})}\right)\Psi(\bm{u},t+k)\right)-1,\]

_where \(\Phi()\) denotes the cumulative distribution function (CDF) of the standard Gaussian distribution, \(n^{\prime}\) is the number of fake users, and \(\Psi(\cdot)\) is defined in Equation 4._

For the proof, please refer to Appendix D.2.2. From Theorem 4, we understand that increasing \(\Psi(\bm{u},t+k)\) can further improve both the upper and lower bounds of error reduction, thereby mitigating the negative impact of poisons. Specifically, this involves the same mechanism as in the clean data context: increasing the perturbation magnitude \(\epsilon\) within \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\|\bm{u}\|)}{\eta\lambda}\).

In conclusion, the theorems in this section indicate that for each user \(\bm{u}\), when the user's perturbation magnitude meets \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\bm{u}\|)}{\eta\lambda}\), we have the following: (1) ACF is theoretically shown to be more effective than traditional CF, and (2) Increasing the user's perturbation magnitude during training as much as possible can further improve both the performance and robustness of ACF. These theoretical understandings can further benefit exploring and fully unleashing the potential of ACF.

## 4 Methodology

To extend theoretical understandings from the simple CF scenario to more practical scenarios, such as multi-item recommendations with Bayesian Personalized Ranking (BPR) [31], which is a mainstream loss function used in CF recommendations, we first conduct a preliminary experiment shown in Figure 1. Using Matrix Factorization [2] on the Gowalla dataset [32], we observe results similar to those in Theorem 3 and Theorem 4: NDCG@20 for users improves within their maximum magnitudes, i.e., constraints, but significantly declines once these constraints are surpassed. Based on the theoretical understandings provided in Section 3, we derive the following corollary to identify the maximum perturbation magnitude for each user in practical CF scenarios.

**Corollary 1**.: _Given any dot-product-based loss function \(\mathcal{L}(\Theta)\), within the framework of Adversarial Collaborative Filtering as defined in Equation 1, the maximum perturbation magnitude \(\epsilon^{(\bm{u})}_{(t),\max}\) for user \(\bm{u}\) at the \(t^{\mathrm{th}}\) epoch is positively related to \(\|\bm{u}_{(t)}\|\)._

For the proof of Corollary 1, please refer to Appendix D.3. According to Corollary 1, we observe that for a user \(\bm{u}\), the larger \(\|\bm{u}\|\), the greater the maximum perturbation magnitude. Considering that maximum perturbation magnitudes will be affected by other factors in the actual training process, to ensure training stability, we decompose \(\epsilon^{(\bm{u})}_{(t),\max}\) for a user \(\bm{u}\) at epoch \(t\) into two components: the uniform perturbation magnitude \(\rho\), applicable to all users, and a user-specific perturbation coefficient \(c(\bm{u},t)\), expressed as:

\[\epsilon^{(\bm{u})}_{(t),\max}=\rho\cdot c(\bm{u},t).\] (5)

According to Corollary 1, \(c(\bm{u},t)\) provides coefficients positively related to users' embedding scales. To avoid training instability caused by extreme scale values, we map \(c(\bm{u},t)\) into the interval \((0,1)\)

Figure 1: NDCG@20 across various perturbation magnitudes for five users (subject to Random Attacks [30]).

defined by:

\[c(\bm{u},t)=\mathrm{sig}\left(\frac{\|\bm{u}_{(t)}\|-\overline{\|\bm{u}_{(t)}\|}}{ \overline{\|\bm{u}_{(t)}\|}}\right),\]

where \(\overline{\|\bm{u}_{(t)}\|}\) represents the average norm of all user embeddings at epoch \(t\), and \(\mathrm{sig}(\cdot)\) denotes the sigmoid function. Consequently, the loss function for our method, Personalized Magnitude Adversarial Collaborative Filtering (PamaCF), is defined as:

\[\mathcal{L}_{\mathrm{PamaCF}}(\Theta)= \mathcal{L}(\Theta)+\lambda\mathcal{L}(\Theta+\Delta^{\mathrm{PamaCF }}),\] (6) \[\mathrm{where}\quad\Delta^{\mathrm{PamaCF}}= \arg\max_{\Delta,\,\|\bm{\lambda}_{u}\|\leq\rho c(\bm{u},t)} \mathcal{L}(\Theta+\Delta),\]

where \(\lambda\) is the weight of adversarial training, \(\rho\) represents the uniform perturbation magnitude for all users, and \(\Delta_{u}\) is the perturbation relative to user \(u\). To maximize the perturbation magnitude for each user within \(\rho c(\bm{u},t)\), we use the perturbation along the gradient direction of the user's adversarial loss with a step length of \(\rho c(\bm{u},t)\) as \(\Delta_{u}\). The specific algorithm process is detailed in Appendix B.

## 5 Experiments

In this section, we conduct extensive experiments to address the following research questions (**RQs**):

* **RQ1:** Can PamaCF further improve the performance and robustness of traditional ACF?
* **RQ2:** Why does PamaCF perform better than traditional ACF?
* **RQ3:** How do hyper-parameters affect PamaCF?

### Experimental Setup

In this section, we briefly introduce the experimental settings. For detailed information, including dataset preprocessing, comprehensive baseline descriptions, and implementation details, please refer to Appendix C.1.

**Datasets**. We employ three common benchmarks: the _Gowalla_ check-in dataset [32], the _Yelp2018_ business dataset, and the _MIND_ news recommendation dataset [33].

**Attack Methods**. We employ both heuristic (Random Attack [30], Bandwagon Attack [34]) and optimization-based (Rev Attack [7], DP Attack [6]) attack methods within a black-box context, where the attacker does not have access to the internal architecture or parameters of the target model.

**Defense Baselines**. We incorporate a variety of defense methods, including detection-based approaches (GraphRfi [12] and LLM4Dec [13]), adversarial collaborative filtering methods (APR [15] and SharpCF [20]), and a denoise-based strategy (StDenoise [35, 19]). In our study, we employ three common backbone recommendation models, Matrix Factorization (MF) [2], LightGCN [3], and NeurMF [36].

**Evaluation Metrics**. The primary metrics for assessing recommendation performance are the top-\(k\) metrics: \(\mathrm{Recall}@k\) and \(\mathrm{NDCG}@k\), as documented in [3, 8, 37]. To quantify the success ratio of attacks, we utilize \(\mathrm{T}\text{-}\mathrm{HR}@k\) and \(\mathrm{T}\text{-}\mathrm{NDCG}@k\) to measure the performance of target items within the top-\(k\) recommendations [7, 6, 13], as:

\[\mathrm{T}\text{-}\mathrm{HR}@k=\frac{1}{|\mathcal{T}|}\sum_{tar\in\mathcal{T} }\frac{\sum_{u\in\mathcal{U}\setminus\mathcal{U}_{tar}}\mathbb{I}\left(tar\in L _{u,1:k}\right)}{|\mathcal{U}\setminus\mathcal{U}_{tar}|},\] (7)

where \(\mathcal{T}\) is the set of target items, \(\mathcal{U}_{tar}\) denotes the set of genuine users who have interacted with target items \(tar\), \(L_{u,1:k}\) represents the top-\(k\) list of recommendations for user \(u\), and \(\mathbb{I}[\cdot)\) is the indicator function that returns 1 if the condition is true. The \(\mathrm{T}\text{-}\mathrm{NDCG}@k\) mirrors \(\mathrm{T}\text{-}\mathrm{HR}@k\), serving as the target item-specific version of \(\mathrm{NDCG}@k\).

### Performance Comparison (RQ1)

In this section, we answer **RQ1**. We focus on two key aspects: the recommendation performance and the robustness against poisoning attacks.

[MISSING_PAGE_FAIL:8]

Detection-based methods, such as GraphRfi and LLM4Dec, show robust defense against attacks similar to their training data, i.e., random attacks. However, the effectiveness of GraphRfi declines against other attack types. In contrast, ACF methods demonstrate stable defense capabilities across various attacks. Specifically, PamaCF significantly reduces the success ratio of attacks, decreasing \(\mathrm{T}\)-\(\mathrm{HR}@50\) and \(\mathrm{T}\)-\(\mathrm{NDCG}@50\) by 49.92% and 43.73% in average, respectively, compared to the best baseline. These results highlight PamaCF's advanced defense capabilities against various attacks.

Additionally, PamaCF's defense effectiveness against attacks targeting popular items is further evaluated. The corresponding results for LightGCN [3] and NeuMF [36], along with the recommendation performance at top-10, are also presented. All supplementary results are in Appendix C.2.

### Augmentation Analysis (RQ2)

In this section, we address **RQ2** by exploring why PamaCF can outperform traditional ACF (especially APR [15]) through embedding visualization and perturbation magnitude comparison.

**Embedding Visualization**. We randomly select a user and project the normalized embeddings of the user, real preference items, the target item given by attacks, and other items in the user's top-10 recommendation list into a two-dimensional space using T-SNE [38], as shown in Figure 2(a). We observe that PamaCF can bring real preference items closer, reducing the distance from the farthest real preference item from 0.736 to 0.365, while leading the target item farther away from all the real preference items. PamaCF's personalized perturbation magnitude lowers the ranking of both the target item and other items, thus improving robustness and performance.

**Perturbation Magnitude Comparison**. We compare the maximum perturbation magnitudes of APR and PamaCF, i.e., \(\epsilon\) in Equation 1 for APR and \(\rho\) in Equation 6 for PamaCF. Both \(\epsilon\) and \(\rho\) are selected through hyper-parameter tuning from \(\{0.1,0.2,\dots,1.0\}\). In the left part of Figure 2(b), we observe that PamaCF finds a higher perturbation magnitude. Additionally, the right portion of Figure 2(b) illustrates the distribution of personalized perturbation magnitudes across all users. These varying magnitudes for different users contribute to the improved effectiveness of PamaCF.

### Hyper-Parameters Analysis (RQ3)

In this section, we answer **RQ3** by exploring the effects of the hyperparameters, magnitude \(\rho\) and adversarial training weight \(\lambda\), as defined in Equation 6. The results are illustrated in Figure 3.

**Analysis of Hyper-Parameters \(\rho\)**. With \(\lambda\) fixed at 1.0, we vary \(\rho\) from 0.1 to 1.0 in increments of 0.1. Our findings demonstrate a significant improvement in both robustness and performance as \(\rho\) increases. Notably, even when \(\rho\) exceeds 0.1, there is an enhancement in recommendation

Figure 3: Left: Analysis of Hyper-Parameters \(\rho\); Right: Analysis of Hyper-Parameters \(\lambda\).

Figure 2: (a) PamaCF brings real preference items closer; (b) PamaCF achieves larger magnitudes.

performance compared to that of the backbone model, i.e., MF, with the range between 0.7 and 0.9 yielding the most significant enhancements.

**Analysis of Hyper-Parameters \(\lambda\).** With \(\rho\) set at 0.9, we adjust \(\lambda\) from 0.2 to 2.0 in increments of 0.2. The analysis indicates that the defensive ability becomes stable once \(\lambda\) surpasses 1.0 in most attacks. However, setting \(\lambda\) too high gradually diminishes the recommendation performance of PamaCF. Despite this, the performance of PamaCF remains considerably improved compared to MF.

## 6 Conclusion

In this work, we theoretically analyze why Adversarial Collaborative Filtering (ACF) enhances both the performance and robustness of Collaborative Filtering (CF) systems against poisoning attacks. Additionally, by establishing bounds for reductions in recommendation error during ACF's optimization process, we discover that applying personalized perturbation magnitudes for users based on their embedding scales can significantly improve ACF's effectiveness. Leveraging these theoretical understandings, we introduce Personalized Magnitude Adversarial Collaborative Filtering (PamaCF). Comprehensive experiments confirm that PamaCF effectively defends against various attacks and significantly enhances the quality of recommendations.

**Limitations**. Our study identifies several limitations that require further investigation. Firstly, our theoretical analysis is based on certain assumptions, specifically with the Gaussian Recommender System. We intend to relax these assumptions in future work. Secondly, this study only examines adversarial training within CF recommendations. In future research, we plan to extend our analysis to include more recommendation scenarios, such as sequential recommendations.

**Broader Impacts**. Our work focuses on enhancing both the performance and robustness of recommender systems against poisoning attacks, thereby benefiting the overall development of recommender systems. We do not foresee any negative impacts resulting from our work.

## Acknowledgements

This work is funded by the National Key R&D Program of China (2022YFB3103700, 2022YFB3103701), the Strategic Priority Research Program of the Chinese Academy of Sciences under Grant No. XDB0680101, and the National Natural Science Foundation of China under Grant Nos. 62102402, 62272125, U21B2046. Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI).

## References

* [1] Xiaoyuan Su and Taghi M Khoshgoftaar. A survey of collaborative filtering techniques. _Advances in Artificial Intelligence_, 2009, 2009.
* [2] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. _Computer_, 42(8):30-37, 2009.
* Simplifying and Powering Graph Convolution Network for Recommendation. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 639-648. ACM, 2020.
* [4] Brent Smith and Greg Linden. Two decades of recommender systems at amazon.com. _IEEE Internet Computing_, 21(3):12-18, 2017.
* [5] Carlos A Gomez-Uribe and Neil Hunt. The netflix recommender system: Algorithms, business value, and innovation. _ACM Transactions on Management Information Systems_, 6(4):1-19, 2015.
* [6] Hai Huang, Jiaming Mu, Neil Zhenqiang Gong, Qi Li, Bin Liu, and Mingwei Xu. Data Poisoning Attacks to Deep Learning Based Recommender Systems. In _Proceedings 2021 Network and Distributed System Security Symposium_, 2021.

* Tang et al. [2020] Jiaxi Tang, Hongyi Wen, and Ke Wang. Revisiting adversarially learned injection attacks against recommender systems. In _Proceedings of the 14th ACM Conference on Recommender Systems_, pages 318-327, 2020.
* Zhang et al. [2023] Kaike Zhang, Qi Cao, Fei Sun, Yunfan Wu, Shuchang Tao, Huawei Shen, and Xueqi Cheng. Robust recommender system: A survey and future directions. _arXiv preprint arXiv:2309.02057_, 2023.
* Chung et al. [2013] Chen-Yao Chung, Ping-Yu Hsu, and Shih-Hsiang Huang. \(\beta p\): A novel approach to filter out malicious rating profiles from recommender systems. _Decision Support Systems_, 55(1):314-325, 2013.
* Zhang and Zhou [2014] Fuzhi Zhang and Quanqiang Zhou. Hht-SVM: An online method for detecting profile injection attacks in collaborative recommender systems. _Knowledge-Based Systems_, 65:96-105, 2014.
* Yang et al. [2016] Zhihai Yang, Lin Xu, Zhongmin Cai, and Zongben Xu. Re-scale AdaBoost for attack detection in collaborative filtering recommender systems. _Knowledge-Based Systems_, 100:74-88, 2016.
* Zhang et al. [2020] Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. Gcn-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 689-698, 2020.
* Zhang et al. [2024] Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, and Xueqi Cheng. Lorec: Combating poisons with large language model for robust sequential recommendation. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1733-1742, 2024.
* Liu [2020] Yuli Liu. Recommending Inferior Results: A General and Feature-Free Model for Spam Detection. In _Proceedings of the 29th ACM International Conference on Information and Knowledge Management_, pages 955-974, 2020.
* He et al. [2018] Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. Adversarial Personalized Ranking for Recommendation. In _The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 355-364, 2018.
* Chen and Li [2019] Huiyuan Chen and Jing Li. Adversarial tensor factorization for context-aware recommendation. In _Proceedings of the 13th ACM Conference on Recommender Systems_, pages 363-367, 2019.
* Li et al. [2020] Ruirui Li, Xian Wu, and Wei Wang. Adversarial learning to compare: Self-attentive prospective customer recommendation in location based social networks. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 349-357, 2020.
* Wu et al. [2021] Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, Enhong Chen, and Senchao Yuan. Fight Fire with Fire: Towards Robust Recommender Systems via Adversarial Poisoning Training. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1074-1083, 2021.
* Ye et al. [2023] Haibo Ye, Xinjie Li, Yuan Yao, and Hanghang Tong. Towards robust neural graph collaborative filtering via structure denoising and embedding perturbation. _ACM Transactions on Information Systems_, 41(3):1-28, 2023.
* Chen et al. [2023] Huiyuan Chen, Xiaoting Li, Vivian Lai, Chin-Chia Michael Yeh, Yujie Fan, Yan Zheng, Mahashweta Das, and Hao Yang. Adversarial Collaborative Filtering for Free. In _Proceedings of the 17th ACM Conference on Recommender Systems_, pages 245-255. ACM, 2023.
* Singh et al. [2024] Naman Deep Singh, Francesco Croce, and Matthias Hein. Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models. In _Advances in Neural Information Processing Systems_, volume 36, pages 13931-13955, 2024.
* Tramer et al. [2017] Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. _arXiv preprint arXiv:1705.07204_, 2017.

* [23] Yian Deng and Tingting Mu. Understanding and improving ensemble adversarial defense. In _Advances in Neural Information Processing Systems_, volume 36, pages 58075-58087, 2023.
* [24] Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [25] Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung Brandon Wu. On the trade-off between adversarial and backdoor robustness. In _Advances in Neural Information Processing Systems_, volume 33, pages 11973-11983, 2020.
* [26] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International Conference on Machine Learning_, pages 7472-7482. PMLR, 2019.
* [27] Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, and Xueqi Cheng. Improving the shortest plank: Vulnerability-aware adversarial training for robust recommender system. In _Proceedings of the 18th ACM Conference on Recommender Systems_, pages 680-689, 2024.
* [28] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [29] Haoyang LI, Shimin DI, and Lei Chen. Revisiting Injective Attacks on Recommender Systems. In _Conference on Neural Information Processing Systems (NeurIPS)_, volume 35, pages 29989-30002, 2022.
* [30] Shyong K Lam and John Riedl. Shilling recommender systems for fun and profit. In _Proceedings of the 13th International Conference on World Wide Web_, pages 393-402, 2004.
* [31] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence_, pages 452-461, 2009.
* [32] Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. Modeling user exposure in recommendation. In _Proceedings of the 25th International Conference on World Wide Web_, pages 951-961, 2016.
* [33] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. Mind: A large-scale dataset for news recommendation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 3597-3606, 2020.
* [34] Robin Burke, Bamshad Mobasher, and Runa Bhaumik. Limited knowledge shilling attacks in collaborative filtering systems. In _Proceedings of 3rd International Workshop on Intelligent Techniques for Web Personalization, 19th International Joint Conference on Artificial Intelligence_, pages 17-24, 2005.
* [35] Changxin Tian, Yuexiang Xie, Yaliang Li, Nan Yang, and Wayne Xin Zhao. Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 122-132, 2022.
* [36] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In _Proceedings of the 26th International Conference on World Wide Web_, pages 173-182, 2017.
* [37] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative filtering. In _Proceedings of the 42nd international ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 165-174, 2019.
* [38] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of Machine Learning Research_, 9(11), 2008.

* [39] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In _Proceedings of the 10th ACM Conference on Recommender Systems_, pages 191-198, 2016.
* [40] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 974-983, 2018.
* [41] Yehuda Koren, Steffen Rendle, and Robert Bell. Advances in collaborative filtering. _Recommender Systems Handbook_, pages 91-142, 2021.
* [42] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning for recommender systems. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1235-1244, 2015.
* [43] Bamshad Mobasher, Robin Burke, Runa Bhaumik, and Chad Williams. Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness. _ACM Transactions on Internet Technology_, 7(4):23-es, 2007.
* [44] Carlos E Seminario and David C Wilson. Attacking item-based recommender systems with power items. In _Proceedings of the 8th ACM Conference on Recommender Systems_, pages 57-64, 2014.
* [45] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [46] Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, and Enhong Chen. Triple adversarial learning for influence-based poisoning attack in recommender systems. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1830-1840, 2021.
* [47] Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, and Yihua Huang. Knowledge-enhanced black-box attacks for recommendations. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 108-117, 2022.
* [48] Fulan Qian, Bei Yuan, Hai Chen, Jie Chen, Defu Lian, and Shu Zhao. Enhancing the transferability of adversarial examples based on nesterov momentum for recommendation systems. _IEEE Transactions on Big Data_, 2023.
* [49] Yanling Wang, Yuchen Liu, Qian Wang, Cong Wang, and Chenliang Li. Poisoning self-supervised learning based sequential recommendations. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 300-310, 2023.
* [50] Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang, and Gabriele Tolomei. The dark side of explanations: Poisoning recommender systems with counterfactual examples. _arXiv preprint arXiv:2305.00574_, 2023.
* [51] Chengzhi Huang and Hui Li. Single-user injection for invisible shilling attack against recommender systems. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 864-873, 2023.
* [52] Yunfan Wu, Qi Cao, Shuchang Tao, Kaike Zhang, Fei Sun, and Huawei Shen. Accelerating the surrogate retraining for poisoning attacks against recommender systems. In _Proceedings of the 18th ACM Conference on Recommender Systems_, pages 701-711, 2024.
* [53] Paul-Alexandru Chirita, Wolfgang Nejdl, and Cristian Zamfir. Preventing shilling attacks in online recommender systems. In _Proceedings of the 7th Annual ACM International Workshop on Web Information and Data Management_, pages 67-74, 2005.

Related Work

### Collaborative Filtering

Collaborative Filtering (CF) has become a cornerstone of modern recommender systems, evidenced by its widespread application in various studies [3; 39; 40]. The fundamental premise of CF is that users with similar preferences are likely to exhibit similar behaviors, which can be leveraged to predict future recommendations [41]. A principal approach within CF is Matrix Factorization, which learns latent embeddings of users and items by decomposing the observed interaction matrix [2].

With the advent of deep learning, neural CF models have emerged, designed to capture more complex patterns in user preferences. For example, CDL [42] merges auxiliary item information through neural networks into CF, addressing challenges associated with data sparsity. Additionally, NCF [36] replaces the traditional dot product with a neural network, enhancing the modeling of user-item interactions. More recently, Graph Neural Networks have prompted the development of graph-based CF models, such as NGCF [37] and LightGCN [3], which have shown remarkable efficacy in recommender systems. However, despite these technological advances, susceptibility to poisoning attacks remains a significant challenge, compromising the robustness of these systems [8].

### Poisoning Attacks against Recommender Systems

Poisoning attacks within recommender systems involve injecting fake users into the training data to manipulate the exposure of certain items. Initial research predominantly focused on rule-based heuristic attacks, where profiles for these fake users were constructed using predetermined heuristic rules [30; 34; 43; 44]. For example, the Random Attack [30] generated fake users interacting with targeted items alongside a random selection of other items. In contrast, the Bandwagon Attack [34] generated fake user interactions to include targeted items and others selected for their high popularity.

As the technique of attacks has evolved, recent studies have shifted towards optimization-based methods for generating fake users [7; 6; 29; 45; 46; 47; 48; 49; 50; 51; 52]. For instance, the Rev Attack [7] formalizes the attack as a bi-level optimization problem, addressed using gradient-based techniques. Similarly, the DP Attack [6] specifically targets deep learning-based recommender systems.

### Robust Recommender Systems

Mainstream strategies for enhancing the robustness of CF systems against poisoning attacks broadly categorize into two main approaches [8]: (1) detecting and removing fake users [9; 10; 11; 12; 13; 53], and (2) developing robust models via adversarial training, i.e., Adversarial Collaborative Filtering (ACF) [15; 16; 17; 18; 19; 20; 27].

Detection-based strategies focus either on pre-identifying and removing potential fake users from the dataset [9; 10; 11; 14] or on mitigating their influence during the training phase [12; 13]. These methods often rely on specific assumptions about the attacks [9; 12] or require supervised data regarding attacks [10; 11; 12; 13]. Among these, LoRec [13] utilizes large language models to enhance sequential recommendations, overcoming the limitations associated with specific knowledge in detection-based strategies. However, its scope is limited to sequential recommender systems and may not generalize well across different CF scenarios.

Conversely, ACF methodologies, particularly those aligned with the Adversarial Personalized Ranking (APR) framework [15], integrate adversarial perturbations at the parameter level (i.e., user and item embeddings) during the model training process [15; 17; 19; 20]. This approach follows a "min-max" optimization paradigm, designed to minimize the error in recommendations under parameter perturbations which aim to maximize the error [13]. Besides, numerous studies have demonstrated that ACF not only enhances the model's robustness but also improves its recommendation performance [15; 20; 27]. Nonetheless, despite its benefits in specific contexts through empirical validation, the intrinsic mechanisms of ACF's effectiveness and its universal applicability remain areas for further theoretical exploration.

```
0: Training set \(\mathcal{D}\), uniform perturbation magnitude \(\rho\), adversarial training weight \(\lambda\), pre-training epochs \(T_{\mathrm{pre}}\), batch size \(\mathbb{B}\)
0: Model parameters \(\Theta=[\bm{U},\bm{V}]\).
1: Pre-train \(\Theta=[\bm{U},\bm{V}]\) for \(T_{\mathrm{pre}}\) epochs using Equation 8.
2:while stopping criteria not met do
3: Draw batch of \(\mathbb{B}\) pairs \((u,i,j)\) from \(\mathcal{D}\).
4:for each \((u,i,j)\) in the batch do
5: Calculate \(\Delta_{u}^{\mathrm{PamaCF}}\), \(\Delta_{i}^{\mathrm{PamaCF}}\), and \(\Delta_{j}^{\mathrm{PamaCF}}\) using Equation 10.
6: Compute \(\mathcal{L}_{\mathrm{PamaCF}}((u,i,j)|\Theta)\) using Equation 9.
7:endfor
8: Update \(\Theta\) using the aggregated gradients from \(\mathcal{L}_{\mathrm{PamaCF}}(\Theta)\) in the batch.
9:endwhile
10:return\(\Theta=[\bm{U},\bm{V}]\) ```

**Algorithm 1** The Training Procedure of PamaCF-BPR

## Appendix B Methodology

For clarity, we present the PamaCF version of the widely used Bayesian Personalized Ranking (BPR) [31] loss function, which optimizes recommender models towards personalized ranking. Given the user set \(\mathcal{U}=\{u\}\), the item set \(\mathcal{V}=\{v\}\), and the training set \(\mathcal{D}=\{(u,i,j)\mid u\in\mathcal{U}\wedge i\in\mathcal{V}_{u}\wedge j \in\mathcal{V}\setminus\mathcal{V}_{u}\}\), where \(\mathcal{V}_{u}\) denotes the set of items with which user \(u\) has interacted. The objective function (to be minimized) of BPR is formally given by:

\[\mathcal{L}_{\mathrm{BPR}}(\Theta=[\bm{U},\bm{V}])=-\sum_{(u,i,j)\in\mathcal{D }}\ln\sigma\,(\langle\bm{U}_{u},\bm{V}_{i}\rangle-\langle\bm{U}_{u},\bm{V}_{ j}\rangle),\] (8)

where \(\bm{U}\) and \(\bm{V}\) represent the learned user and item embeddings, respectively.

The PamaCF version of the BPR loss function is defined as:

\[\mathcal{L}_{\mathrm{PamaCF}}(\Theta)= \mathcal{L}_{\mathrm{BPR}}(\Theta)+\lambda\mathcal{L}_{\mathrm{BPR} }(\Theta+\Delta^{\mathrm{PamaCF}}),\] \[\mathrm{where}\quad\Delta^{\mathrm{PamaCF}}= \arg\max_{\Delta,\,\|\Delta_{u/i/j}\|\leq p^{c}(\bm{u},t),(u,i, j)\in\mathcal{D}}\mathcal{L}_{\mathrm{BPR}}(\Theta+\Delta),\]

where \(\lambda\) is the weight of adversarial training, \(\rho\) represents the uniform perturbation magnitude for all users, and

\[c(\bm{u},t)=\text{sig}\left(\frac{\|\bm{u}_{(t)}\|-\|\bm{u}_{(t)}\|}{\|\bm{u}_ {(t)}\|}\right).\]

The specific handling of a pair \((u,i,j)\in\mathcal{D}\) is expressed by:

\[\mathcal{L}_{\mathrm{PamaCF}}((u,i,j)|\Theta)=-\ln\sigma\,( \langle\bm{U}_{u},\bm{V}_{i}\rangle-\langle\bm{U}_{u},\bm{V}_{j}\rangle)\] (9) \[-\lambda\ln\sigma\,\big{(}\langle\bm{U}_{u}+\Delta_{u}^{\mathrm{ PamaCF}},\bm{V}_{i}+\Delta_{i}^{\mathrm{PamaCF}}\rangle-\langle\bm{U}_{u}+ \Delta_{u}^{\mathrm{PamaCF}},\bm{V}_{j}+\Delta_{j}^{\mathrm{PamaCF}}\rangle \big{)},\]

where

\[\Delta_{u}^{\mathrm{PamaCF}}= \rho c(\bm{u},t)\frac{\Gamma_{u}}{\|\Gamma_{u}\|},\quad\text{ where}\quad\Gamma_{u}=\frac{\partial\mathcal{L}_{\mathrm{BPR}}((u,i,j)|\Theta+\Delta^{ \mathrm{PamaCF}})}{\partial\Delta_{u}},\] (10) \[\Delta_{i}^{\mathrm{PamaCF}}= \rho c(\bm{u},t)\frac{\Gamma_{i}}{\|\Gamma_{i}\|},\quad\text{ where}\quad\Gamma_{i}=\frac{\partial\mathcal{L}_{\mathrm{BPR}}((u,i,j)|\Theta+\Delta^{ \mathrm{PamaCF}})}{\partial\Delta_{i}},\] \[\Delta_{j}^{\mathrm{PamaCF}}= \rho c(\bm{u},t)\frac{\Gamma_{i}}{\|\Gamma_{j}\|},\quad\text{ where}\quad\Gamma_{j}=\frac{\partial\mathcal{L}_{\mathrm{BPR}}((u,i,j)|\Theta+\Delta^{ \mathrm{PamaCF}})}{\partial\Delta_{j}}.\]

The procedure of training with PamaCF is illustrated in Algorithm 1.

## Appendix C Experiments

### Supplements to Experimental Settings

**Datasets**. We employ three common benchmarks: the _Gowalla_ check-in dataset [32], the _Yelp2018_ business dataset, and the _MIND_ news recommendation dataset [33]. The Gowalla and Yelp2018datasets include interactions from all users. For the MIND dataset, we sample a subset of users following [13]. Following [3, 37], users and items with fewer than 10 interactions are excluded from our analysis. We allocate 80% of each user's historical interactions to the training set and the remainder for testing. Additionally, 10% of the interactions from the training set are randomly selected to form a validation set for hyperparameter tuning. Detailed statistics of the datasets are summarized in Table 3.

**Attack Methods**. We explore both heuristic (Random Attack [30], Bandwagon Attack [34]) and optimization-based (Rev Attack [7], DP Attack [6]) attack methods within a black-box context, where the attacker does not have access to the internal architecture or parameters of the target model.

* **Random Attack** (Heuristic Method) [30]: This method entails fake users including interactions with both the targeted items and a set of randomly chosen items.
* **Bandwagon Attack** (Heuristic Method) [34]: Fake users' interactions encompass the targeted items and those selected for their high popularity.
* **DP Attack** (Optimization-based Method) [6]: This approach is specifically designed to compromise deep learning-based recommender systems.
* **Rev Attack** (Optimization-based Method) [7]: The attack is conceptualized as a bi-level optimization problem, addressed through gradient-based methods.

**Defense Baselines**. We incorporate a variety of defense methods, including detection-based approaches (GraphRfi [12] and LLM4Dec [13]), adversarial collaborative filtering methods (APR [15] and SharpCF [20]), and a denoise-based strategy (StDenoise [35, 19]). In our study, we employ three common backbone recommendation models, MF [2], LightGCN [3], and NeurMF [36].

* **GraphRfi**[12]: Employs a combination of Graph Convolutional Networks and Neural Random Forests for identifying fake users.
* **LLM4Dec**[13]: Utilizes an LLM-based framework for fake users detection.
* **APR**[15]: Generates parameter perturbations and integrates these perturbations into training.
* **SharpCF**[20]: Adopts a sharpness-aware minimization approach to refine the adversarial training process proposed by APR.
* **StDenoise**[35, 19]: Applies a structural denoising technique that leverages the similarity between \(\bm{U}_{u}\) and \(\bm{V}_{i}\) for each \((u,i)\) pair, aiding in the removal of noise, as described in [35, 19].

Note that: With the need of item-side information, LLM4Dec is exclusively evaluated on the MIND dataset. Moreover, we observe that SharpCF, initially proposed for the MF model, exhibits unstable training performance when applied to the LightGCN model or the MIND dataset. Consequently, we present SharpCF results solely for the MF model on the Gowalla and Yelp2018 datasets.

**Implementation Details**. In our study, we employ three common backbone recommendation models, Matrix Factorization (MF) [2], LightGCN [3], and NeurMF [36]. To quantify the success ratio of attacks, we select \(k=50\) as the evaluation metric following [6, 7, 18], while for assessing recommendation performance, we utilize \(k=10,20\) following [3, 37]. For each attack setting, we conduct experiments five times, taking the average value as the result and the standard deviation as the error bar. The configuration of both the defense methods and the recommendation models involves selecting a learning rate from {0.1, 0.01,..., \(1\times 10^{-5}\)}, and a weight decay from {0, 0.1,..., \(1\times 10^{-5}\)}. The implementation of GraphRfi follows its paper. For the detection-based methods, we employ the Random Attack to generate supervised attack data. The magnitude parameter of adversarial perturbations in both APR and PamaCF is determined from a range of {0.1, 0.2,..., 1.0}. In terms of attack methods, we set the attack budget to \(1\%\) and target five specific items. The hyperparameters align with those detailed in their original publications.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline
**DATASET** & **\#Users** & **\#Items** & **\#Ratings** & **Avg.Inter.** & **Sparsity** \\ \hline Gowalla & 29,858 & 40,981 & 1,027,370 & 34.4 & 99.92\% \\ Yelp2018 & 31,668 & 38,048 & 1,561,406 & 49.3 & 99.88\% \\ MIND & 141,920 & 36,214 & 20,693,122 & 145.8 & 99.60\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Dataset statistics 

[MISSING_PAGE_FAIL:17]

## Appendix D Proofs

### Proofs for Section 3.1

#### d.1.1 Proof of Theorem 1

To investigate the recommendation error of each user during the training period, we first analyze how the item embeddings change. We discover a transformation function that accurately measures the change in the item embedding from its initial state to its state after a certain number of training epochs. This insight is formally expressed in the following proposition.

**Proposition 1**.: _Consider a Gaussian Recommender System \(f_{(t)}\), undergoing training for \(t\) epochs using the standard loss function specified in Equation 2. Given a learning rate \(\eta\), there exists a function \(M(t,\eta):\mathbb{N}^{+}\times\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\) that quantify the transformation of the item embedding due to training. Specifically, we have:_

\[\bm{v}_{(t)}=M(t,\eta)\bm{v}_{(0)},\]

_where \(\bm{v}_{(0)}\) denotes the initial item embedding and \(\bm{v}_{(t)}\) represents the item embedding after \(t\) epochs of training._

Proof of Proposition 1.: Consider the training process of the Gaussian Recommender System \(f_{(t)}\) over \(t\) epochs, with each update through the standard loss outlined in Equation 2. The update mechanism for user and item embeddings at the \(t^{\rm th}\) epoch can be described as follows:

\[\bm{u}_{(t)}= \bm{u}_{(t-1)}+\eta\cdot r\bm{v}_{(t-1)},\] (11) \[\bm{v}_{(t)}= \bm{v}_{(t-1)}+\eta\cdot\sum_{(\bm{u},r)\in\mathcal{I}}r\bm{u}_{ (t-1)}.\]

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline
**Model** & \multicolumn{3}{c}{**Chan** (\%)} & \multicolumn{3}{c}{**Random Attack** (\%)} & \multicolumn{3}{c}{**Bandwagen Attack** (\%)} & \multicolumn{3}{c}{**DP Attack** (\%)} & \multicolumn{3}{c}{**Rev Attack** (\%)} \\ \cline{2-13} (Dataset) & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** \\ \hline
**MF** & 7.49 \(\pm\) 0.08 & 5.85 \(\pm\) 0.03 & 7.47 \(\pm\) 0.03 & 5.98 \(\pm\) 0.05 & 5.81 \(\pm\) 0.06 & 5.81 \(\pm\) 0.04 & 7.24 \(\pm\) 0.01 & 7.23 \(\pm\) 0.08 & 7.24 \(\pm\) 0.04 & 7.26 \(\pm\) 0.02 \\
**+SDembles** & 7.03 \(\pm\) 0.08 & 7.77 \(\pm\) 1.00 & 6.99 \(\pm\) 0.08 & 7.16 \(\pm\) 0.06 & 6.95 \(\pm\) 0.09 & 7.19 \(\pm\) 0.03 & 7.20 \(\pm\) 0.09 & 7.11 \(\pm\) 0.02 & 7.22 \(\pm\) 0.09 & 7.14 \(\pm\) 0.04 & 7.29 \(\pm\) 0.03 \\
**+GraphR10** & 6.98 \(\pm\) 0.03 & 7.05 \(\pm\) 0.06 & 6.92 \(\pm\) 0.06 & 5.96 \(\pm\) 0.06 & 6.86 \(\pm\) 0.04 & 6.97 \(\pm\) 0.05 & 6.97 \(\pm\) 0.07 & 7.02 \(\pm\) 0.09 & 7.06 \(\pm\) 0.09 & 7.06 \(\pm\) 0.09 & 7.06 \(\pm\) 0.06 \\
**+APR** & 2.92 \(\pm\) 0.06 & 6.96 \(\pm\) 0.06 & 6.92 \(\pm\) 0.04 & 5.98 \(\pm\) 0.01 & 5.98 \(\pm\) 0.01 & 5.96 \(\pm\) 0.05 & 6.20 \(\pm\) 0.03 & 6.24 \(\pm\) 0.04 & 9.64 \(\pm\) 0.04 & 9.41 \(\pm\) 0.07 & 9.78 \(\pm\) 0.09 \\
**+ShapCP** & 8.81 \(\pm\) 0.09 & 8.83 \(\pm\) 0.10 & 8.80 \(\pm\) 0.09 & 8.84 \(\pm\) 0.07 & 8.71 \(\pm\) 0.08 & 8.72 \(\pm\) 0.06 & 8.93 \(\pm\) 0.11 & 8.91 \(\pm\) 0.09 & 8.92 \(\pm\) 0.05 & 8.93 \(\pm\) 0.03 \\
**+PamC** & **9.56 \(\pm\) 0.02** & **9.94 \(\pm\) 0.05** & **9.46 \(\pm\) 0.03** & **9.54 \(\pm\) 0.01** & **9.49 \(\pm\) 0.02** & **9.84 \(\pm\) 0.01** & **9.54 \(\pm\) 0.05** & **9.93 \(\pm\) 0.04** & **9.68 \(\pm\) 0.05** & **10.05 \(\pm\) 0.10** \\ Gain & 2.91\(\pm\) 0.17 & 4.26\(\pm\) 5.01 & 2.76\(\pm\) 5.01 & 2.74\(\pm\) 5.17 & 4.33\(\pm\) 1.93 & 7.46 \(\pm\) 5.01 & 4.25\(\pm\) 1.94 & -2.97\(\pm\) & -2.87\(\pm\) 1.7 & -2.78\(\pm\) 1.7 \\ Gain w.r.t. MF & +27.62\% & +7.00\% & +26.65\% & +66.69\% & +7.48\% & +28.10\% & +69.27\% & +31.78\% & +37.34\% & +33.380\% & +38.48\% \\ \hline \hline \end{tabular}

* The relative percentage increase of PamCPs’s metrics in the best value of other baselines’ metrics. Notably, only **three decimal places** are presented due to space limitations, through the actual ranking and calculations utilize the **full prediction** of the data.

\end{table}
Table 6: Recommendation Performance@10 on Gowalla

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline
**Model** & \multicolumn{3}{c}{**Chan** (\%)} & \multicolumn{3}{c}{**Random Attack** (\%)} & \multicolumn{3}{c}{**Bandwagen Attack** (\%)} & \multicolumn{3}{c}{**DP Attack** (\%)} \\ \cline{2-13} (Dataset) & **Recall@20** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** & **Recall@10** & **NDCG@10** \\ \hline
**LightGCN** & 12.54 \(\pm\) 0.03 & 8.27 \(\pm\) 0.02 & 12.46 \(\pm\) 0.05 & 8.26 \(\pm\) 0.03Considering the sum \(\sum_{(\bm{u},r)}r\bm{u}_{(t-1)}\), we have:

\[\sum_{(\bm{u},r)}r\bm{u}_{(t-1)} =\sum_{(\bm{u},r)}\left(r\bm{u}_{(t-2)}+\eta r^{2}\bm{v}_{(t-2)}\right)\] \[=\sum_{(\bm{u},r)}\left(r\bm{u}_{(t-3)}+\eta\left(\bm{v}_{(t-2)}+ \bm{v}_{(t-3)}\right)\right)\] \[\cdots\] \[=\sum_{(\bm{u},r)}\left(r\bm{u}_{(0)}+\eta\sum_{j=0}^{t-2}\bm{v}_ {(j)}\right)\] \[= n\bm{v}_{(0)}+n\eta\sum_{j=0}^{t-2}\bm{v}_{(j)},\]

where \(n\) is the number of users. This leads to the recursive update for \(\bm{v}_{(t)}\):

\[\bm{v}_{(t)}= \bm{v}_{(t-1)}+n\eta\bm{v}_{(0)}+n\eta^{2}\sum_{j=0}^{t-2}\bm{v}_ {(j)}\] \[= (1+n\eta^{2})\bm{v}_{(t-2)}+2n\eta\bm{v}_{(0)}+2n\eta^{2}\sum_{j =0}^{t-3}\bm{v}_{(j)}\] \[= (1+n\eta^{2}+2n\eta^{2})\bm{v}_{(t-3)}+\left(n\eta\cdot(1+n\eta^ {2})+2n\eta\right)\bm{v}_{(0)}+\left(n\eta^{2}\cdot(1+n\eta^{2})+2n\eta^{2} \right)\sum_{j=0}^{t-4}\bm{v}_{(j)}.\]

To simplify, we introduce \(a(k),b(k)\) and \(c(k)\) to represent the cumulative scaling factors as:

\[a(k) =\begin{cases}1&k=1\\ a(k-1)+c(k-1)&k\geq 2\end{cases},\] \[b(k) =\begin{cases}n\eta&k=1\\ n\eta\cdot a(k-1)+b(k-1)&k\geq 2\end{cases},\] (12) \[c(k) =\begin{cases}n\eta^{2}&k=1\\ n\eta^{2}\cdot a(k-1)+c(k-1)&k\geq 2\end{cases},\]

yielding a general form for \(\bm{v}_{(t)}\):

\[\bm{v}_{(t)}= a(k)\bm{v}_{(t-k)}+b(k)\bm{v}_{(0)}+c(k)\sum_{j=0}^{t-k-1}\bm{v} _{(j)}\] \[= a(t-1)\bm{v}_{(1)}+b(t-1)\bm{v}_{(0)}+c(t-1)\bm{v}_{(0)}.\]

Based on

\[\bm{v}_{(1)}=\bm{v}_{(0)}+\eta\sum_{(\bm{u},r)\in\mathcal{I}}r\bm{u}_{(0)}=(1+ n\eta)\bm{v}_{(0)},\]

let \(M(t,\eta):\mathbb{N}^{+}\times\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\) be the transformation function related to training epochs \(t\) and learning rate \(\eta\), which is defined as:

\[M(t,\eta)=\begin{cases}1+n\eta&t=1\\ (1+n\eta)a(t-1)+b(t-1)+c(t-1)&t>1\end{cases}.\]

Then we can get:

\[\bm{v}_{(t)}=M(t,\eta)\bm{v}_{(0)}.\]

This proves that the item embedding \(\bm{v}_{(t)}\) after \(t\) epochs of training is a scaled version of the initial embedding \(\bm{v}_{(0)}\), with the scaling factor \(M(t,\eta)\) being a function of the number of epochs \(t\) and the learning rate \(\eta\)

**Fact 1**.: _Let \(\bm{\varepsilon}\in\mathbb{R}^{d}\) be drawn from \(\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\), with \(\sigma>0\). Let \(\bm{w}\in\mathbb{R}^{d}\) represent any unit vector. Then, \(\langle\bm{\varepsilon},\bm{w}\rangle\) follows a normal distribution \(\mathcal{N}(0,\sigma^{2})\)._

Proof of Fact 1.: Consider \(\bm{\varepsilon}=[\varepsilon_{1},\ldots,\varepsilon_{d}]\), with each \(\varepsilon_{i}\) following \(\mathcal{N}(0,\sigma^{2})\). Let \(\bm{w}=[w_{1},\ldots,w_{d}]\), satisfying \(w_{1}^{2}+\cdots+w_{d}^{2}=1\). Then \(\langle\bm{\varepsilon},\bm{w}\rangle\) is distributed as \(\mathcal{N}\left(\mathbb{E}[\langle\bm{\varepsilon},\bm{w}\rangle],\mathbb{D} [\langle\bm{\varepsilon},\bm{w}\rangle]\right)\), where:

\[\mathbb{E}[\langle\bm{\varepsilon},\bm{w}\rangle] =\mathbb{E}[\varepsilon_{1}w_{1}+\cdots+\varepsilon_{d}w_{d}]=0,\] \[\mathbb{D}[\langle\bm{\varepsilon},\bm{w}\rangle] =\mathbb{D}[\varepsilon_{1}w_{1}+\cdots+\varepsilon_{d}w_{d}]\] \[=\mathbb{D}[\varepsilon_{1}w_{1}]+\cdots+\mathbb{D}[\varepsilon _{d}w_{d}]\] \[=(w_{1}^{2}+\cdots+w_{d}^{2})\sigma^{2}\] \[=\sigma^{2}.\]

Hence, Fact 1 is proved. 

Proof of Theorem 1.: By invoking Proposition 1, we establish the basis for evaluating the impact of training on the recommendation error at the \((t+1)^{\mathrm{th}}\) epoch. Proposition 1 specifies the scaling relationship between the initial and trained item embeddings, leading to the following update expressions for user and item embeddings. For the standard loss though Equation 2, we have:

\[\bm{u}_{(t+1)} =\bm{u}_{(t)}+\eta rM(t,\eta)\bm{v}_{(0)},\] \[\bm{v}_{(t+1)} =M(t+1,\eta)\bm{v}_{(0)}.\]

Considering the above update rules, the recommendation error at the \((t+1)^{\mathrm{th}}\) epoch is given by:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)} \left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] (13) \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t+1)},\bm{v}_{(t+1)}\rangle\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\left(\bm{u}_{(t)}+\eta rM(t,\eta)\bm{v }_{(0)}\right),M(t+1,\eta)\bm{v}_{(0)}\rangle\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\left(\bm{u}_{(t)}+\eta rM(t,\eta)\bm{v }_{(0)}\right),\bm{v}_{(0)}\rangle\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t)},\bm{v}_{(0)}\rangle\leq- \eta M(t,\eta)\|\bm{v}_{(0)}\|^{2}\right].\]

For adversarial loss as detailed by Equation 1, we have:

\[\Delta_{\mathrm{adv}}=\arg\max_{\Delta,\|\Delta\|\leq\epsilon}\mathcal{L}( \Theta+\Delta).\]

According to the first-order Taylor expansion, we have:

\[\Delta_{\mathrm{adv}}\approx \arg\max_{\Delta,\|\Delta\|\leq\epsilon}\mathcal{L}(\Theta)+ \langle\Delta,\nabla_{\Theta}\mathcal{L}(\Theta)\rangle\] \[= \arg\max_{\Delta,\|\Delta\|\leq\epsilon}\langle\Delta,\nabla_{ \Theta}\mathcal{L}(\Theta)\rangle\] \[= \epsilon\frac{\nabla_{\Theta}\mathcal{L}(\Theta)}{\|\nabla_{ \Theta}\mathcal{L}(\Theta)\|},\]

leading to specific perturbations \(\Delta_{\bm{u}}\) and \(\Delta_{\bm{v}}\) in Equation 3:

\[\Delta_{\bm{u}}=\epsilon\frac{\Gamma_{\bm{v}}}{\|\Gamma_{\bm{u}}\|}, \quad\text{where}\quad\Gamma_{\bm{u}}=\frac{\partial\mathcal{L}(\bm{u},\bm{v} |\Theta)}{\partial\bm{u}}=-r\bm{v},\] (14) \[\Delta_{\bm{v}}=\epsilon\frac{\Gamma_{\bm{v}}}{\|\Gamma_{\bm{v}}\|}, \quad\text{where}\quad\Gamma_{\bm{v}}=\frac{\partial\mathcal{L}(\bm{v},\bm{u} |\Theta)}{\partial\bm{v}}=-r\bm{u}.\]Subsequently, the updated embeddings through adversarial loss are expressed as:

\[\begin{split}\bm{u}_{(t+1)}^{\,\mathrm{adv}}&=\bm{u}_{(t )}+\eta\cdot\left(r\bm{v}_{(t)}+\lambda r\left(\bm{v}_{(t)}-\epsilon\frac{r\bm{u }_{(t)}}{\|\bm{u}_{(t)}\|}\right)\right)\\ &=\bm{u}_{(t)}+\eta(1+\lambda)r\bm{v}_{(t)}-\eta\lambda\epsilon \frac{\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|}\\ &=\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{(t)}\|}\right) \bm{u}_{(t)}+\eta(1+\lambda)r\cdot M(t,\eta)\bm{v}_{(0)}\\ \bm{v}_{(t+1)}^{\,\mathrm{adv}}&=\bm{v}_{(t)}+\eta \cdot\left(\sum r\bm{u}_{(t)}+\lambda\sum r\left(\bm{u}_{(t)}-\epsilon\frac{r \bm{v}_{(t)}}{\|\bm{v}_{(t)}\|}\right)\right)\\ &=\bm{v}_{(t)}+\eta(1+\lambda)\sum r\bm{u}_{(t)}-\frac{n\eta \lambda\epsilon}{\|\bm{v}_{(t)}\|}\bm{v}_{(t)}\\ &=\left(M(t,\eta)+n\eta(1+\lambda)\left(1+\eta+\eta\sum_{j=1}^{t -1}M(j,\eta)\right)-\frac{n\eta\lambda\epsilon}{\|\bm{v}_{(0)}\|}\right)\bm{v} _{(0)}.\end{split}\] (15)

Given \(\|\bar{\bm{u}}\|\gg\sigma\), we can approximate \(\|\bm{v}_{(0)}\|\) with \(\mathbb{E}[\|\bm{v}_{(0)}\|]\). Given \(\epsilon<\frac{\min(\|\bm{u}_{(t)}\|,\|\bar{\bm{u}}\|)}{\eta\lambda}\), according to the expansion of \(M(t,\eta)\) in Proposition 1, it follows that \(\left(M(t,\eta)+n\eta(1+\lambda)\left(1+\eta+\eta\sum_{j=1}^{t-1}M(j,\eta) \right)-\frac{n\eta\lambda\epsilon}{\|\bm{v}_{(0)}\|}\right)>0\). Considering these update rules, the recommendation error at the \((t+1)^{\mathrm{th}}\) epoch is determined by:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}^{\,\mathrm{adv}} \left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] (16) \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\left(1-\frac{\eta\lambda\epsilon}{\|\bm {u}_{(t)}\|}\right)\bm{u}_{(t)}+\eta(1+\lambda)r\cdot M(t,\eta)\bm{v}_{(0)} \right),\bm{v}_{(0)}\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\left(1-\frac{\eta\lambda\epsilon}{\|\bm {u}_{(t)}\|}\right)\bm{u}_{(t)},\bm{v}_{(0)}\rangle+\eta(1+\lambda)M(t,\eta) \langle\bm{v}_{(0)},\bm{v}_{(0)}\rangle\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\left(1-\frac{\eta\lambda\epsilon}{\|\bm {u}_{(t)}\|}\right)\bm{u}_{(t)},\bm{v}_{(0)}\rangle\leq-\eta(1+\lambda)M(t, \eta)\|\bm{v}_{(0)}\|^{2}\right].\]

Let \(\gamma_{(t)}^{(\bm{u})}=\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{(t)}\|} \right)^{-1}\). Given the condition \(\epsilon<\frac{\min(\|\bm{u}_{(t)}\|,\|\bar{\bm{u}}\|)}{\eta\lambda}\), it follows \(\gamma_{(t)}^{(\bm{u})}>1\). The final form of the recommendation error at the \((t+1)^{\mathrm{th}}\) epoch under adversarial training is:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}^{\,\mathrm{adv}} \left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t)},\bm{v}_{(0)}\rangle\leq- \eta(1+\lambda)\gamma_{(t)}^{(\bm{u})}M(t,\eta)\|\bm{v}_{(0)}\|^{2}\right]\]

This leads us to:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]- \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} ^{\,\mathrm{adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma_{(t)}^{\bm{u}}M(t,\eta)\|\bm{v}_{(0) }\|^{2}<r\langle\bm{u}_{(t)},\bm{v}_{(0)}\rangle\leq-\eta M(t,\eta)\|\bm{v}_{( 0)}\|^{2}\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma_{(t)}^{\bm{u}}M(t,\eta)\frac{\|\bm{v}_{( 0)}\|^{2}}{\|\bm{u}_{(t)}\|}<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{v} _{(0)}\rangle\leq-\eta M(t,\eta)\frac{\|\bm{v}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|} \right].\]Given that \(\|\bar{\bm{u}}\|\gg\sigma\), we can approximate the \(\|\bm{v}_{(0)}\|^{2}\) by using \(\mathbb{E}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}\left[ \|\bm{v}_{(0)}\|^{2}\right]\) as an estimate. Therefore, we have:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^ {2}}{n-1}I)}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]-\mathbb{P }_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\text{ adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] \[\approx\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}}, \frac{\sigma^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma_{(t)}^{\bm{u}}M(t,\eta) \frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}<\langle \frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{v}_{(0)}\rangle\leq-\eta M(t,\eta) \frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}\right],\]

where \(d\) is the dimension of \(\bm{v}_{(0)}\).

Let \(\bm{\varepsilon}\sim\mathcal{N}\left(\bm{0},\frac{\sigma^{2}}{n-1}\bm{I}\right)\), we have \(\bm{v}_{(0)}=\bar{\bm{u}}+\bm{\varepsilon}\). Thus:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]- \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I )}^{\text{ adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] \[=\mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N}\left(\bm{0},\frac{ \sigma^{2}}{n-1}I\right)}\left[-\eta(1+\lambda)\gamma_{(t)}^{\bm{u}}M(t,\eta) \frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}<\langle \frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bar{\bm{u}}+\bm{\varepsilon}\rangle \leq-\eta M(t,\eta)\frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u} _{(t)}\|}\right]\] \[=\mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N}\left(\bm{0},\frac{ \sigma^{2}}{n-1}I\right)}\left[-\eta(1+\lambda)\gamma_{(t)}^{\bm{u}}M(t,\eta) \frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}-\langle \frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bar{\bm{u}}\rangle\right.\] \[\left.\hskip 113.811024pt<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t) }\|},\bm{\varepsilon}\rangle\leq-\eta M(t,\eta)\frac{\|\bar{\bm{u}}\|^{2}+\frac {d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}-\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t )}\|},\bar{\bm{u}}\rangle\right].\] (17)

According to Fact 1, \(\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{\varepsilon}\rangle\sim \mathcal{N}\left(0,\frac{\sigma^{2}}{n-1}\right)\), then:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]- \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} ^{\text{ adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]\] \[= \Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\eta(1+\lambda)\gamma_{ (t)}^{\bm{u}}M(t,\eta)\frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\| \bm{u}_{(t)}\|}+\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bar{\bm{u}} \rangle\right)\right)\] \[\qquad\qquad-\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\eta M(t, \eta)\frac{\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t)}\|}+ \langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bar{\bm{u}}\rangle\right)\right),\]

where \(\Phi()\) is the CDF of standard Gaussian distribution. Given \(\epsilon<\frac{\min(\|\bm{u}_{(t)}\|,\|\bar{\bm{u}}\|)}{\eta\lambda}\), it follows that \((1+\lambda)\gamma_{(t)}^{(\bm{u})}>1+\lambda>1\). This condition implies a lower recommendation error at the \((t+1)^{\text{th}}\) epoch under adversarial training compared to standard training:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r)\right]-\mathbb{P}_{\bm{v}_{(0 )}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\text{ adv}}\left[f_{(t+1)}(\bm{u},\bm{v})\neq r\mid(\bm{u},r) \right]>0.\]

Therefore, Theorem 1 is proved. 

#### d.1.2 Proof of Theorem 2

Proof of Theorem 2.: In light of Theorem 1 and the impact of poisoning attacks, our objective is to measure the alteration in the recommendation error within a poisoning attack, i.e., \(\alpha\)-poisoned recommendation error.

A _poisoning attack_ on Gaussian Recommender System injects a _poisoning user set_\(\mathcal{I}^{\prime}=\{(\bm{u}_{1}^{\prime},r_{1}^{\prime}),(\bm{u}_{2}^{ \prime},r_{2}^{\prime}),\ldots,(\bm{u}_{n^{\prime}}^{\prime},r_{n^{\prime}}^{ \prime})\}\), with each tuple \((\bm{u}^{\prime},r^{\prime})\in\mathbb{R}^{d}\times\{\pm 1\}\) representing data maliciously crafted by attackers. Considering the initialized poisoned item embedding \(\bm{v}^{{}^{\prime}}\):

\[\bm{v}^{{}^{\prime}}_{(0)}=\frac{1}{n+n^{\prime}}\left(\sum_{(\bm{u},r)\in \mathcal{I}}r\bm{u}_{(0)}+\sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{ \prime}}r^{\prime}\bm{u}^{{}^{\prime}}_{(0)}\right),\] (18)by employing Theorem 1 as a basis (similar to Equations 13 and 16), we derive:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)\right]- \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} ^{\text{adv}}\left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)\right]\] \[= \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma^{\bm{u}}_{(t)}M(t,\eta)\frac{\|\bm{v} ^{\prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_ {(t)}\|},\bm{v}^{\prime}_{(0)}\rangle\leq-\eta M(t,\eta)\frac{\|\bm{v}^{\prime }_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}\right]\] \[= \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma^{\bm{u}}_{(t)}M(t,\eta)\frac{\|\bm{v} ^{\prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}\right.\] \[\qquad\left.<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\frac{ 1}{n+n^{\prime}}\left(n\bm{v}_{(0)}+\sum_{(\bm{u}^{\prime},r^{\prime})\in \mathcal{I}^{\prime}}r^{\prime}\bm{u}^{{}^{\prime}}_{(0)}\right)\rangle\leq- \eta M(t,\eta)\frac{\|\bm{v}^{\prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}\right]\] \[= \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta\frac{n+n^{\prime}}{n}(1+\lambda)\gamma^{\bm{u}}_{(t)}M (t,\eta)\frac{\|\bm{v}^{\prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}\right.\] \[\qquad\left.<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{v} _{(0)}\rangle+\frac{1}{n}\sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{ \prime}}rr^{\prime}\langle\frac{\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{u}^{{}^{ \prime}}_{(0)}\rangle\leq-\eta\frac{n+n^{\prime}}{n}M(t,\eta)\frac{\|\bm{v}^{ \prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}\right]\] \[= \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[-\eta(1+\lambda)\frac{n+n^{\prime}}{n}\gamma^{\bm{u}}_{(t)} M(t,\eta)\frac{\|\bm{v}^{\prime}_{(0)}\|^{2}}{\|\bm{u}_{(t)}\|}-\frac{1}{n}\sum_{( \bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{ \bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{u}^{{}^{\prime}}_{(0)}\rangle\right.\] \[\qquad\left.<\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{v} _{(0)}\rangle\leq-\eta\frac{n+n^{\prime}}{n}M(t,\eta)\frac{\|\bm{v}^{\prime}_{ (0)}\|^{2}}{\|\bm{u}_{(t)}\|}-\frac{1}{n}\sum_{(\bm{u}^{\prime},r^{\prime}) \in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{\bm{u}_{(t)}}{\|\bm{u}_{(t)} \|},\bm{u}^{{}^{\prime}}_{(0)}\rangle\right],\]

where \(\gamma^{(\bm{u})}_{(t)}=\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{(t)}\|} \right)^{-1}\).

Given \(\|\bar{\bm{u}}\|\gg\sigma\), we can use \(\mathbb{E}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[\|\bm{v}^{\prime}_{(0)}\|^{2}\right]\) to approximate the \(\|\bm{v}^{\prime}_{(0)}\|^{2}\) in the above. Similar to the proof of Theorem 1, specifically Equation 17, and under the precondition \(\epsilon<\frac{\min(\|\bm{u}_{(t)}\|,\|\bar{\bm{u}}\|)}{\eta\lambda}\), we have:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)\right]- \mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)} ^{\text{adv}}\left[f_{(t+1),\alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r )\right]>0.\]

Hence, Theorem 2 is proved. 

### Proofs for Section 3.2

#### d.2.1 Proof of Theorem 3

Extending Proposition 1 to ACF yields the following proposition, which captures the transformation of item embedding due to adversarial loss as defined in Equation 3.

**Proposition 2**.: _Consider a Gaussian Recommender System \(f_{(t)}\), per-trained on standard loss over \(t\) epochs, then trained by the adversarial loss specified in Equation 3 over \(k\) epochs. Given learning rate \(\eta\), adversarial training weight \(\lambda\), and perturbation magnitude \(\epsilon\), when \(\epsilon<\frac{\|\bar{\bm{u}}\|}{\eta\lambda}\), and \(\|\bar{\bm{u}}\|\gg\sigma\), there exists a transformation function \(M_{\text{adv}}(t,\eta,\lambda,\epsilon):\mathbb{N}^{+}\times\mathbb{R}^{+} \times\mathbb{R}^{+}\times\mathbb{R}^{+}\times\mathbb{R}^{+}\rightarrow\mathbb{R} ^{+}\), such that the item embedding at \((t+k)^{\text{th}}\) epoch, \(\bm{v}_{(t+k)}\), is related to the initial embedding \(\bm{v}_{(0)}\) by:_

\[\bm{v}_{(t+k)}=\frac{M_{\text{adv}}(t+k,\eta,\lambda,\epsilon)}{M_{\text{adv}}(t, \eta,\lambda,\epsilon)}M(t,\eta)\bm{v}_{(0)},\]

_where \(M(t,\eta)\) is the transformation function given by standard loss in Proposition 1._

The proof of Proposition 2 follows a reasoning analogous to that of Proposition 1. Due to the similarity, the detailed proof is omitted for brevity.

Proof of Theorem 3.: Given \(\epsilon<\frac{\|\bm{u}\|}{\eta\lambda}\), drawing from Proposition 2 and Theorem 1 (specifically Equation 15), the update rules for user and item embeddings in the ACF at the \((t+k+1)^{\mathrm{th}}\) epoch are presented as:

\[\begin{split}&\bm{u}_{(t+k+1)}=\left(1-\frac{\eta\lambda\epsilon}{ \|\bm{u}_{(t+k)}\|}\right)\bm{u}_{(t+k)}+\eta(1+\lambda)r\cdot\frac{M_{\mathrm{ adv}}(t+k,\eta,\lambda,\epsilon)}{M_{\mathrm{adv}}(t,\eta,\lambda,\epsilon)}M(t, \eta)\bm{v}_{(0)},\\ &\bm{v}_{(t+k+1)}=\frac{M_{\mathrm{adv}}(t+k+1,\eta,\lambda, \epsilon)}{M_{\mathrm{adv}}(t,\eta,\lambda,\epsilon)}M(t,\eta)\bm{v}_{(0)}. \end{split}\] (19)

Considering the above update rules, the recommendation error at the \((t+k+1)^{\mathrm{th}}\) epoch is given by:

\[\begin{split}&\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm {u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}\left[f_{(t+k+1)}(\bm{u},\bm{v}) \neq r\mid(\bm{u},r)\right]\\ &=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t+k+1)},\bm{v}_{(t+k+1)}\rangle \leq 0\right]\\ &=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\left\langle\left(1-\frac{\eta\lambda\epsilon}{ \|\bm{u}_{(t+k)}\|}\right)\bm{u}_{(t+k)},\bm{v}_{(0)}\right\rangle\leq-\eta(1+ \lambda)C_{t+k}\|\bm{v}_{(0)}\|^{2}\right],\end{split}\] (20)

where \(C_{t+k}=\frac{M_{\mathrm{adv}}(t+k,\eta,\lambda,\epsilon)}{M_{\mathrm{adv}}(t,\eta,\lambda,\epsilon)}M(t,\eta)\). Let \(\gamma_{(t+k)}^{(\bm{u})}=\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{(t+k)} \|}\right)^{-1}\). Given the condition \(\epsilon<\frac{\min(\|\bm{u}_{(t+k)}\|,\|\bm{u}\|)}{\eta\lambda}\), it follows \(\gamma_{(t+k)}^{(\bm{u})}>1\). The recommendation error at \((t+1)^{\mathrm{th}}\) epoch through adversarial loss can be expressed as:

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}^{\mathrm{adv}}\left[f_{(t+k+1)}(\bm{u},\bm{v})\neq r\mid( \bm{u},r)\right]\\ =\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t+k)},\bm{v}_{(0)}\rangle\leq- \eta(1+\lambda)\gamma_{(t+k)}^{(\bm{u})}C_{t+k}\|\bm{v}_{(0)}\|^{2}\right].\]

With

\[\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}^{\mathrm{adv}}\left[f_{(t+k)}(\bm{u},\bm{v})\neq r\mid( \bm{u},r)\right]=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac {\sigma^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t+k)},\bm{v}_{(0)}\rangle\leq 0 \right],\]

the change in recommendation error can be written as:

\[\begin{split}&\Delta_{(t+k+1)}^{\mathrm{adv}}\mathbb{P}_{\bm{v}_{(0)} \sim\mathcal{N}(\tilde{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm {u},\bm{v})\neq r\mid(\bm{u},r)]\\ =&\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm {u}},\frac{\sigma^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma_{(t+k)}^{(\bm{u})}C_ {t+k}\|\bm{v}_{(0)}\|^{2}<r\cdot\langle\bm{u}_{(t+k)},\bm{v}_{(0)}\rangle\leq 0\right] \\ =&\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm {u}},\frac{\sigma^{2}}{n-1}I)}\left[-\eta(1+\lambda)\gamma_{(t+k)}^{(\bm{u})} \frac{C_{t+k}}{\|\bm{u}_{(t+k)}\|}\|\bm{v}_{(0)}\|^{2}<r\cdot\langle\frac{\bm {u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{v}_{(0)}\rangle\leq 0\right]\end{split}\]

Considering \(\bm{v}_{(0)}\sim\mathcal{N}\left(\tilde{\bm{u}},\frac{\sigma^{2}}{n-1}I\right)\), let \(\bm{\varepsilon}\sim\mathcal{N}\left(\bm{0},\frac{\sigma^{2}}{n-1}I\right)\), we have \(\bm{v}_{(0)}=\tilde{\bm{u}}+\bm{\varepsilon}\). Then we obtain:

\[\begin{split}&\Delta_{(t+k+1)}^{\mathrm{adv}}\mathbb{P}_{\bm{v}_{(0)} \sim\mathcal{N}(\tilde{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm {u},\bm{v})\neq r\mid(\bm{u},r)]\\ =&\mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N}\left( \bm{0},\frac{\sigma^{2}}{n-1}I\right)}\Bigg{[}-\eta(1+\lambda)\gamma_{(t+k)}^{( \bm{u})}\frac{C_{t+k}}{\|\bm{u}_{(t+k)}\|}\|\bm{v}_{(0)}\|^{2}<\langle\frac{r \bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\tilde{\bm{u}}+\bm{\varepsilon}\rangle \leq 0\Bigg{]}\\ =&\mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N}\left( \bm{0},\frac{\sigma^{2}}{n-1}I\right)}\Bigg{[}-\eta(1+\lambda)\gamma_{(t+k)}^{( \bm{u})}\frac{C_{t+k}}{\|\bm{u}_{(t+k)}\|}\|\bm{v}_{(0)}\|^{2}-\langle\frac{r \bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\tilde{\bm{u}}\rangle<\langle\frac{r\bm{u }_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{\varepsilon}\rangle\leq-\langle\frac{r\bm{u }_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\tilde{\bm{u}}\rangle\Bigg{]}.\end{split}\]

Given that \(\|\tilde{\bm{u}}\|\gg\sigma\), we can approximate the \(\|\bm{v}_{(0)}\|^{2}\) by using \(\mathbb{E}_{\bm{v}_{(0)}\sim\mathcal{N}(\tilde{\bm{u}},\frac{\sigma^{2}}{n-1}I)} \left[\|\bm{v}_{(0)}\|^{2}\right]\) as an estimate. Thus, we have:

\[\begin{split}&\Delta_{(t+k+1)}^{\mathrm{adv}}\mathbb{P}_{\bm{v}_{(0)} \sim\mathcal{N}(\tilde{\bm{u}},\frac{\sigma^{2}}{n-1}I)}^{\mathrm{adv}}[f(\bm {u},\bm{v})\neq r\mid(\bm{u},r)]\\ \approx&\mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N} \left(\bm{0},\frac{\sigma^{2}}{n-1}I\right)}\Bigg{[}-\eta(1+\lambda)\gamma_{(t+k )}^{(\bm{u})}C_{t+k}\frac{\|\tilde{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\| \bm{u}_{(t+k)}\|}-\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\tilde{ \bm{u}}\rangle<\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{ \varepsilon}\rangle\leq-\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|}, \tilde{\bm{u}}\rangle\Bigg{]}.\end{split}\]By Fact 1, we have \(\langle\frac{r\bm{u}_{(t)}}{\|\bm{u}_{(t)}\|},\bm{\varepsilon}\rangle\sim\mathcal{ N}\left(0,\frac{\sigma^{2}}{n-1}\right)\), then:

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\,\mathrm{adv}}_{\bm{v }_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}[f(\bm{u},\bm{v}) \neq r\mid(\bm{u},r)]\] \[= \Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(-\langle\frac{r\bm{u}_{(t +k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\right)\right)-\Phi\left(\frac{ \sqrt{n-1}}{\sigma}\left(-\eta(1+\lambda)\gamma^{(\bm{u})}_{(t+k)}C_{t+k}\frac {\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1}}{\|\bm{u}_{(t+k)}\|}-\langle\frac {r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\right)\right),\]

where \(\Phi()\) is the CDF of standard Gaussian distribution.

Obviously,

\[\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\in[-\| \bar{\bm{u}}\|,\|\bar{\bm{u}}\|]\,.\]

Let

\[\Psi(\bm{u},t+k)=(1+\lambda)\gamma^{\bm{u}}_{(t+k)}\frac{C_{t+k}}{\|\bm{u}_{( t+k)}\|}.\]

Using the CDF properties of the standard normal distribution, we can conclude:

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\,\mathrm{adv}}_{\bm{v }_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}[f(\bm{u},\bm{v}) \neq r\mid(\bm{u},r)]\quad\geq\] \[\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\|\bar{\bm{u}}\|+\eta( \|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1})\Psi(\bm{u},t+k)\right)\right)- \Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\|\bar{\bm{u}}\|\right)\right),\]

equality holds when

\[\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle=\|\bar{ \bm{u}}\|.\]

Furthermore,

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\,\mathrm{adv}}_{\bm{v }_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma^{2}}{n-1}I)}[f(\bm{u},\bm{v}) \neq r\mid(\bm{u},r)]\quad\leq\quad 2\Phi\left(\frac{\sqrt{n-1}\eta}{2\sigma}(\|\bar{\bm{u}} \|^{2}+\frac{d\sigma^{2}}{n-1})\Psi(\bm{u},t+k)\right)-1.\]

Equality is achieved when

\[\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle=-\frac{1} {2}\eta(\|\bar{\bm{u}}\|^{2}+\frac{d\sigma^{2}}{n-1})\Psi(\bm{u},t+k).\]

Therefore, Theorem 3 is proved. 

#### d.2.2 Proof of Theorem 4

_Proof of Theorem 4._ Given \(\epsilon<\frac{\|\bar{\bm{u}}\|}{\eta\lambda}\), according to Proposition 2 and Theorem 1 (specifically Equation 15), we have:

\[\bm{u}_{(t+k+1)} =\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{(t+k)}\|}\right) \bm{u}_{(t+k)}+\eta(1+\lambda)r\cdot\frac{M_{\mathrm{adv}}(t+k,\eta,\lambda, \epsilon)}{M_{\mathrm{adv}}(t,\eta,\lambda,\epsilon)}M(t,\eta)\bm{v}^{{}^{ \prime}}_{(0)},\] (21) \[\bm{v}^{{}^{\prime}}_{(t+k+1)} =\frac{M_{\mathrm{adv}}(t+k+1,\eta,\lambda,\epsilon)}{M_{\mathrm{ adv}}(t,\eta,\lambda,\epsilon)}M(t,\eta)\bm{v}^{{}^{\prime}}_{(0)},\]

where \(\bm{v}^{{}^{\prime}}_{(0)}\) is the poisoned item embedding as given by Equation 18.

Considering the above update rules, the \(\alpha\)-poisoned recommendation error at the \((t+k+1)^{\mathrm{th}}\) epoch is given by:

\[\mathbb{P}^{\,\mathrm{adv}}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{ u}},\frac{\sigma^{2}}{n-1}I)}\left[f_{(t+k+1),\alpha}(\bm{u},\bm{v}^{\prime}) \neq r\mid(\bm{u},r)\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{\sigma ^{2}}{n-1}I)}\left[r\cdot\langle\bm{u}_{(t+k+1)},\bm{v}^{{}^{\prime}}_{(t+k+1) }\rangle\leq 0\right]\] \[=\mathbb{P}_{\bm{v}_{(0)}\sim\mathcal{N}(\bar{\bm{u}},\frac{ \sigma^{2}}{n-1}I)}\left[r\cdot\left\langle\left(1-\frac{\eta\lambda\epsilon}{ \|\bm{u}_{(t+k)}\|}\right)\bm{u}_{(t+k)},\bm{v}^{{}^{\prime}}_{(0)}\right\rangle \leq-\eta(1+\lambda)C_{t+k}\|\bm{v}^{{}^{\prime}}_{(0)}\|^{2}\right],\]

[MISSING_PAGE_EMPTY:26]

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\mathrm{adv}}_{\bm{v}_{(0)} \sim\mathcal{N}(\bar{\bm{u}}_{\cdot,\frac{\sigma^{2}}{n-1}I})}[f_{\alpha}(\bm{u },\bm{v}^{\prime})\neq r\mid(\bm{u},r)]\] \[\approx \mathbb{P}_{\bm{\varepsilon}\sim\mathcal{N}\left(\mathbf{0}, \frac{\sigma^{2}}{n-1}I\right)}\Bigg{[}-\eta\frac{n+n^{\prime}}{n}(1+\lambda) \gamma^{(\mathbf{u})}_{(t+k)}\frac{C_{t+k}}{\|\bm{u}_{(t+k)}\|}\mathbb{E} \left[\|\bm{v}^{\prime}_{(0)}\|^{2}\right]-\frac{1}{n}\sum_{(\bm{u}^{\prime}, r^{\prime})\in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k) }\|},\bm{u}^{{}^{\prime}}_{(0)}\rangle-\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_ {(t+k)}\|},\bar{\bm{u}}\rangle\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad<\langle\frac{r\bm{u}_{ (t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{\varepsilon}\rangle\leq-\frac{1}{n}\sum_{(\bm {u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{\bm{u} _{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}^{{}^{\prime}}_{(0)}\rangle-\langle\frac{ r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\Bigg{]}.\]

By Fact 1, we have \(\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{\varepsilon}\rangle\sim \mathcal{N}\left(0,\frac{\sigma^{2}}{n-1}\right)\), then:

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\mathrm{adv}}_{\bm{v }_{(0)}\sim\mathcal{N}(\bar{\bm{u}}_{\cdot,\frac{\sigma^{2}}{n-1}I})}[f_{ \alpha}(\bm{u},\bm{v}^{\prime})\neq r\mid(\bm{u},r)]\] \[= \Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(-\frac{1}{n}\sum_{(\bm{u }^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{\bm{u}_{ (t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}^{{}^{\prime}}_{(0)}\rangle-\langle\frac{r \bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\right)\right)\] \[-\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(-\eta\frac{n+n^{\prime }}{n}(1+\lambda)\gamma^{(\mathbf{u})}_{(t+k)}\frac{C_{t+k}}{\|\bm{u}_{(t+k)}\| }\mathbb{E}\left[\|\bm{v}^{\prime}_{(0)}\|^{2}\right]-\frac{1}{n}\sum_{(\bm{u }^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{\prime}\langle\frac{\bm{u}_ {(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}^{{}^{\prime}}_{(0)}\rangle-\langle\frac{ r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}}\rangle\right) \right),\]

where \(\Phi()\) is the CDF of standard Gaussian distribution.

Obviously,

\[\sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{ \prime}\langle\frac{\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}^{{}^{\prime}}_{( 0)}\rangle\in\left[-n^{\prime}\sqrt{d}\alpha,n^{\prime}\sqrt{d}\alpha\right],\] \[\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}} \rangle\in\left[-\|\bar{\bm{u}}\|,\|\bar{\bm{u}}\|\right],\] \[\mathbb{E}\left[\|\bm{v}^{\prime}_{(0)}\|^{2}\right]\in\left(\frac {n^{2}\|\bar{\bm{u}}\|^{2}-2nn^{\prime}\alpha\|\bar{\bm{u}}\|_{0}}{(n+n^{\prime })^{2}}+\frac{n^{2}d\sigma^{2}}{(n-1)(n+n^{\prime})^{2}},\ \frac{n^{2}\|\bar{\bm{u}}\|^{2}+(n^{\prime})^{2}d\alpha^{2}+2nn^{\prime}\alpha\| \bar{\bm{u}}\|_{0}}{(n+n^{\prime})^{2}}+\frac{n^{2}d\sigma^{2}}{(n-1)(n+n^{ \prime})^{2}}\right]\]

where \(n^{\prime}\) is the number of fake users, \(d\) is the dimension of \(\bm{u}^{\prime}\), and \(\alpha=\max_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}\|\bm{u}^{\prime} \|_{\infty}\).

Let

\[\Psi(\bm{u},t+k)= (1+\lambda)\gamma^{\bm{u}}_{(t+k)}\frac{C_{t+k}}{\|\bm{u}_{(t+k)} \|},\] \[\beta= \frac{n^{\prime}}{n}\sqrt{d}\alpha+\|\bar{\bm{u}}\|.\]

According to the CDF properties of the standard normal distribution, we can conclude that:

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\mathrm{adv}}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}}_{\cdot,\frac{\sigma^{2}}{n-1}I})}[f_{\alpha}(\bm{u}, \bm{v}^{\prime})\neq r\mid(\bm{u},r)]\quad>\]

\[\Phi\left(\frac{\sqrt{n-1}}{\sigma}\left(\beta+\eta\left(\frac{n^{2}\|\bar{\bm{u}} \|^{2}-2nn^{\prime}\alpha\|\bar{\bm{u}}\|_{0}}{n(n+n^{\prime})}+\frac{nd\sigma^{ 2}}{(n-1)(n+n^{\prime})}\right)\Psi(\bm{u},t+k)\right)\right)-\Phi\left(\frac{ \sqrt{n-1}}{\sigma}\left(\beta\right)\right),\]

reaches the minimum value when

\[\sum_{(\bm{u}^{\prime},r^{\prime})\in\mathcal{I}^{\prime}}rr^{ \prime}\langle\frac{\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}^{{}^{\prime}}_{(0)} \rangle=n^{\prime}\sqrt{d}\alpha,\] \[\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bar{\bm{u}} \rangle=\|\bar{\bm{u}}\|,\]

and \(\mathbb{E}\left[\|\bm{v}^{\prime}_{(0)}\|^{2}\right]\) reaches the minimum value \(\frac{n^{2}\|\bar{\bm{u}}\|^{2}-2nn^{\prime}\alpha\|\bar{\bm{u}}\|_{0}}{(n+n^{ \prime})^{2}}+\frac{n^{2}d\sigma^{2}}{(n-1)(n+n^{\prime})^{2}}\).

Moreover,

\[\Delta^{\mathrm{adv}}_{(t+k+1)}\mathbb{P}^{\mathrm{adv}}_{\bm{v}_{(0)}\sim \mathcal{N}(\bar{\bm{u}}_{\cdot,\frac{\sigma^{2}}{n-1}I})}[f_{\alpha}(\bm{u}, \bm{v}^{\prime})\neq r\mid(\bm{u},r)]\quad\leq\] \[2\Phi\left(\frac{\sqrt{n-1}\eta}{2\sigma}\left(\frac{n^{2}\|\bar{ \bm{u}}\|^{2}+(n^{\prime})^{2}\alpha+2nn^{\prime}\alpha\|\bar{\bm{u}}\|_{0}}{n(n+n ^{\prime})}+\frac{nd\sigma^{2}}{(n-1)(n+n^{\prime})}\right)\Psi(\bm{u},t+k) \right)-1,\]equality holds when

\[\mathbb{E}\left[\|\bm{v}_{(0)}^{\prime}\|^{2}\right] =\frac{n^{2}\|\bar{\bm{u}}\|^{2}+(n^{\prime})^{2}\alpha+2nn^{\prime }\alpha\|\bar{\bm{u}}\|_{0}}{(n+n^{\prime})^{2}}+\frac{n^{2}d\sigma^{2}}{(n-1)( n+n^{\prime})^{2}},\] \[\frac{1}{n}\sum_{(\bm{u}^{{}^{\prime}},r^{\prime})\in\mathcal{L}^ {\prime}}rr^{\prime}\langle\frac{\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|},\bm{u}_{ (0)}^{{}^{\prime}}\rangle+\langle\frac{r\bm{u}_{(t+k)}}{\|\bm{u}_{(t+k)}\|}, \bar{\bm{u}}\rangle =-\frac{1}{2}\eta\frac{n+n^{\prime}}{n}\mathbb{E}\left[\|\bm{v}_{ (0)}^{\prime}\|^{2}\right]\Psi(\bm{u},t+k).\]

Hence, Theorem 4 is proved. 

### Proofs for Section 4

Given any dot-product-based loss function \(\mathcal{L}(\Theta)\), characterized by its dependency on the product of user and item embeddings, the gradients of user and item embeddings at the \(t^{\mathrm{th}}\) epoch can be expressed as follows:

\[\nabla_{\bm{u}_{(t)}}\mathcal{L}(\bm{u},\bm{v}|\Theta_{(t)}) =\phi(r,\bm{u}_{(t)},\bm{v}_{(t)})\bm{v}_{(t)},\] (22) \[\nabla_{\bm{v}_{(t)}}\mathcal{L}(\bm{u},\bm{v}|\Theta_{(t)}) =\psi(r,\bm{u}_{(t)},\bm{v}_{(t)})\bm{u}_{(t)},\]

where \(\phi(\cdot)\) and \(\psi(\cdot)\) denote coefficient functions derived from \(\mathcal{L}(\Theta)\), mapping from the embeddings' space to the scalar values.

Considering the proofs of Theorem 3 and Theorem 4, there is a coefficient \(\gamma_{(t)}^{(\bm{u})}\) for the user \(\bm{u}\). When \(\gamma_{(t)}^{(\bm{u})}>1\) is satisfied, the effectiveness of ACF can be guaranteed. Here, we derive the coefficient \(\gamma_{(t)}^{(\bm{u})}\) in multi-item recommendation scenarios with dot-product-based loss through the following corollary.

**Corollary 2**.: _Assuming the incorporation of adversarial training as defined in Equation 1 with dot-product-based loss function \(\mathcal{L}(\Theta)\), and given the learning rate \(\eta\), the adversarial training weight \(\lambda\), and the perturbation scale \(\epsilon\), the \(\gamma_{(t)}^{(\bm{u})}\) for user \(\bm{u}\) is given by:_

\[\gamma_{(t)}^{(\bm{u})}=\left(1-\frac{\eta\lambda\epsilon}{\|\bm{u}_{t}\|} \sum_{\bm{v}\in\mathcal{N}_{\bm{u}}}|\psi(r,\bm{u}_{(t)},\bm{v}_{(t)})|\right) ^{-1},\] (23)

_where \(\mathcal{N}_{\bm{u}}\) is the item set that user \(\bm{u}\) interacts with._

Proof

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly articulate the primary contributions and scope of the paper. They accurately summarize the theoretical and experimental findings, explicitly matching the claims made throughout the document. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of our work are discussed in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions are presented in Section 3, and the related proofs are found in Appendix D. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experimental information, including datasets, baselines, and evaluation metrics, is provided in Section 5.1, while implementation details are found in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The experimental code and data are provided through an anonymous link in Section 1. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental settings are outlined in Section 5.1. Additional details can be found in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results presented in Section 5 include error bars. The method for calculating these error bars is explained in the implementation details provided in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The robust recommendation algorithm has a positive impact on the field of recommender systems. The impact is discussed in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The original papers that produced the code package or dataset are cited in both Section 5 and Appendix C.1. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the code via both an anonymized URL (in Section 1) and an anonymized zip file. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.