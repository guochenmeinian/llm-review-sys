# Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders

 Jan Dubinski

\({}^{1,2,}\)1

 Stanislaw Pawak

\({}^{1}\)

 Franzziska Boenisch

\({}^{4}\)2

**Tomasz Trzcinski**

\({}^{1,2,3}\)

**Adam Dziedzic**

\({}^{4}\)3

\({}^{1}\)Warsaw University of Technology

\({}^{2}\)IDEAS NCBR

\({}^{3}\)Tooploox

\({}^{4}\)CISPA Helmholtz Center for Information Security

Footnote 1: Corresponding authors: jan.dubinski.dokt@pw.edu.pl and adam.dziedzic@cispa.de

Footnote 2: Equal contribution.

Footnote 3: Project Lead.

Footnote 4: Code available at https://github.com/stapaw/b4b-active-encoder-defense

###### Abstract

Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose _Bucks for Buckets (B4B)_, the first _active defense_ that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task. B4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.5

Footnote 5: Code available at https://github.com/stapaw/b4b-active-encoder-defense

## 1 Introduction

In model stealing attacks, adversaries extract a machine learning model exposed via a public API by repeatedly querying it and updating their own stolen copy based on the obtained responses. Model stealing was shown to be one of the main threats to the security of machine learning models in practice [38]. Also in research, since the introduction of the first extraction attack against classifiers [40], a lot of work on improving stealing [27, 33, 40, 41], extending it to different model types [8, 37], and proposing adequate defenses [18, 25, 26, 31] has been put forward. With the recent shift in learning paradigms from supervised to self supervised learning (SSL), especially the need for new defenses becomes increasingly pressing. From an academic viewpoint, the urge arises because it was shown that SSL models (_encoders_) are even more vulnerable to model stealing [16, 29, 36] than their supervised counterparts. This is because whereas supervised models' output is low dimensional, _e.g.,_ per-class probabilities or pure labels, SSL encoders output high-dimensional representation vectors that encode a larger amount of information and thereby facilitate stealing. In addition, from a practical industry's viewpoint, defenses are required since many popular API providers, such as Coherence, OpenAI, or Clarify [1, 2, 3] already expose their high-value SSL encoders via APIs to a broad range of users.

Most of the current defenses against encoder stealing are _reactive, i.e.,_ they do not actively prevent the stealing but rather aim at detecting it by adding watermarks to the encoder [14, 16] or performing dataset inference to identify stolen copies [17]. Since at the point of detection, the damage of stealing has already been inflicted, we argue that reactive defenses intervene too late and we advocate for _active_ defenses that prevent stealing while it is happening. Yet, active defenses are challenging to implement because they not only need to prevent stealing but also should preserve the utility of representations for legitimate users. The only existing active defense against encoder stealing [29] falls short on this latter aspect since it significantly degrades the quality of representations for all users.

To close the gap between required and existing defenses, we propose _Bucks for Buckets (B4B)_, the first active defense against encoder stealing that does not harm utility for legitimate users. B4B leverages the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the full embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task. To turn this observation into a practical defense, B4B is equipped with three modular building blocks: (1) The first building block is a tracking mechanism that continuously estimates the fraction of the embedding space covered by the representations returned to each user. The intuition why this is relevant is that by covering large fractions of the embedding space, the representations will suffice for an adversary to reproduce the encoder's functionality, _i.e.,_ to successfully steal it. (2) B4B's second building block consists of a cost function to translate the covered fraction of the embedding space into a concrete penalty. We require this cost function to significantly penalize adversaries trying to steal the model while having only a minimal effect on legitimate users. (3) The third building block contains transformations that can be applied to the representations on a per-user basis to prevent adaptive attackers from circumventing our defense by creating multiple user accounts (sybils) and distributing their queries over these accounts such that they minimize the overall cost. We present the different building blocks of B4B in Figure 1.

While B4B's modularity enables different instantiations of the three building blocks, we propose a concrete end-to-end instantiation to showcase the practicability of our approach. To implement tracking of the covered embedding space, we employ _local sensitive hashing_ that maps any representation returned to a given user into a set of hash _buckets_. We base our cost function (_i.e.,_ the _"**bucks**"_) on utility and make B4B add noise to the representations with a magnitude that increases with the number of buckets occupied by the given user. While the scale of noise added to legitimate users'

Figure 1: **Overview of B4B. In the upper part, we present our B4B framework that consists of three modular building blocks: (1) A coverage estimation to track the fraction of embedding space covered by the representations returned to each user, (2) a cost function that serves to map the coverage to a concrete penalty to prevent stealing, and (3) per-user transformations that are applied to the returned representations to prevent sybil attacks. In the lower part, we present a concrete instantiation of B4B and the operation flow of our defense: The API calculates representations for the incoming queries. We instantiate the coverage estimation with local sensitive hashing and estimate the covered space as the fraction of _hash buckets_ occupied. We calibrate the costs by adding noise to the representations according to the coverage. We apply a set of transformations on a per-user basis. The noised and transformed representations are returned to the user.**

representations does not harm their downstream performance due to their small embedding space coverage, the representations returned to an adversary become increasingly noisy--significantly degrading the performance of their stolen encoder. Finally, we rely on a set of transformations (_e.g._, affine transformations, shuffling, padding) that preserve downstream utility [17]. While, as a consequence, legitimate users remain unaffected by these transformations, adversaries cannot directly combine the representations obtained through different sybil accounts anymore to train their stolen copy of the encoder. Instead, they first have to remap all representations into the same embedding space, which we show causes both query and computation overhead and still reduces the performance of the stolen encoder.

In summary, we make the following contributions:

1. We present B4B, the first active defense against encoder stealing that does not harm legitimate users' downstream performance. B4B's three building blocks enable penalizing adversaries whose returned representations cover large fractions of the embedding space and prevent sybil attacks.
2. We propose a concrete instantiation of B4B that relies on local sensitive hashing and decreases the quality of representations returned to a user once their representations fill too many hash buckets.
3. We provide an end-to-end evaluation of our defense to highlight its effectiveness in offering high utility representations for legitimate users and degrading the performance of stolen encoders in both the single and the sybil-accounts setup.

## 2 Related Work

**Model Extraction Attacks.** The goal of the model extraction attacks is to replicate the functionality of a victim model \(f_{v}\) trained on a dataset \(D_{v}\). An attacker has a black box access to the victim model and uses a stealing dataset \(D_{s}=\{q_{i},f_{v}(q_{i})\}_{i=1}^{n}\), consisting of queries \(q_{i}\) and the corresponding outputs \(f_{v}(q_{i})\) returned by the victim model, to train a stolen model \(f_{s}\). Model extraction attacks have been shown against various types of models including classifiers [24; 40] and encoders [16; 36].

**Self Supervised Learning and Encoders.** SSL is an increasingly popular machine learning paradigm. It trains encoder models to generate representations from complex inputs without relying on explicit labels. These representations encode useful features of a given input, enabling efficient learning for multiple downstream tasks. Many SSL frameworks have been proposed [9; 10; 12; 22; 23; 44]. In our work, we focus on the two popular SSL vision encoders, namely SimSiam [12] and DINO [9], which return high-quality representations that achieve state-of-the-art performance on downstream tasks when assessed by training a linear classifier directly on representations. SimSiam trains with two Siamese encoders with directly shared weights. A prediction MLP head is applied to one of the encoders \(f_{1}\), and the other encoder \(f_{2}\) has a stop-gradient, where both operations are used for avoiding collapsing solutions. In contrast, DINO shares only architecture (not weights) between a student \(f_{1}\) and a teacher model \(f_{2}\), also with the stop-gradient operation, but not the prediction head. While SimSiam uses convolutional neural networks (CNNs), DINO also employs vision transformers (ViTs). Both frameworks use a symmetrized loss of the form \(\frac{1}{2}g(f_{1}(x_{1}),f_{2}(x_{2}))+\frac{1}{2}g(f_{1}(x_{2}),f_{2}(x_{1}))\) in their optimization objectives, where \(g(\cdot,\cdot)\) is negative cosine similarity for SimSiam and cross-entropy for DINO. SimSiam and DINO's similarities and differences demonstrate our method's broad applicability across SSL frameworks. More details can be found in Appendix E.

**Stealing Encoders.** The stealing of SSL encoders was shown to be extremely effective [16; 29; 36]. The goal of extracting encoders is to maximize the similarity of the outputs from the stolen local copy and the original representations output by the victim encoder. Therefore, while training the stolen copy, the adversary either imitates a self-supervised training using a contrastive loss function, _e.g._, InfoNCE [10] or SoftNN [21] or directly matches both models' representations via the Mean Squared Error (MSE) loss. To reduce the number of queries sent to the victim encoder, the attack proposed in [29] leverages the key observation that the victim encoder returns similar representations for any image and its augmented versions. Therefore, a given image can be sent to the victim while the stolen copy is trained using many augmentations of this image, where the representation of a given augmented image is approximated as the one of the original image produced by the victim encoder.

**Defending Encoders.** Recently, watermarking [7; 25; 42] methods have been proposed to detect stolen encoders [14; 16; 43]. Many of these approaches use downstream tasks to check if a watermark embedded into a victim encoder is present in a suspect encoder. Dataset inference [30] is another type of encoder ownership resolution. It uses the victim's training dataset as a unique signature, leveraging the following observation: for a victim encoder trained on its private data as well as for its stolen copies, the distribution of the representations generated from the victim's training data differs from the distribution of the representations generated on the test data. In contrast, for an independently trained encoder, these two distributions cannot be distinguished, allowing the detection of stolen copies [17]. However, all the previous methods are _reactive_ and aim at detecting the stolen encoder instead of _actively_ preventing the attack. The only preliminary active defenses for encoders were proposed by [16; 29]. They either perturb or truncate the answers to poison the training objective of an attacker. These operations were shown to harm substantially the performance of legitimate users, which renders the defense impractical. In contrast, our B4B has negligible impact on the quality of representations returned to legitimate users.

## 3 Actively Defending against Model Stealing with B4B

B4B aims at actively preventing model stealing while preserving high-utility representations for legitimate users. Before introducing the three main building blocks of B4B, namely (1) the estimation of embedding space coverage, (2) the cost function, and (3) the transformation of representations (see Figure 1), we detail our threat model and the observation on embedding space coverage that represents the intuition behind our approach.

### Threat Model and Intuition

Our setup and the resulting threat model are inspired by public APIs, such as Cohere, OpenAI, or Clarify [1; 2; 3] that expose encoders to users through a pre-defined interface. These encoders are trained using SSL on large amounts of unlabeled data, often crawled from the internet, and therefore from diverse distributions. We notice that to provide rich representations to multiple users, the training dataset of the encoder needs to be significantly more diverse than the individual downstream tasks that the users query for representations. For instance, if the encoder behind the API is trained on the ImageNet dataset, then the legitimate users are expected to query the API for downstream tasks, such as CIFAR10 or SVHN. Similarly, if the encoder is trained on CIFAR10, the expected downstream tasks are MNIST or Fashion MNIST. Yet, in the design of our defense, we consider adversaries who can query the encoder with arbitrary inputs to obtain high-dimensional representation vectors from the encoder. Our defense is independent of the protected encoder's architecture and does not rely on any assumption about the adversary's data and query strategy.

We argue that even in this restricted setup, our defense can distinguish between adversaries and legitimate users by analyzing the distribution of representations returned to them. In Figure 2, by using PCA to project representations for different datasets to a two-dimensional space, we visualize that representations for different downstream tasks cluster in _disjoint_ and _small sub-spaces_ of the full embedding space. The representations were obtained from a SimSiam encoder originally trained on ImageNet (we observe similar clustering for DINO shown in Appendix F). As a result, legitimate users can be characterized by their representations' small coverage of the embedding space. In contrast, the adversary does not aim at solving a particular downstream task. They instead would want to obtain representations that cover large fractions of the embedding space. This enables reproducing the overall functionality of the encoder (instead of only learning some local task-specific behavior). Indeed, it has been empirically shown by prior work, such as [16], that stealing with multiple distributions, _e.g.,_ by relying on the complex ImageNet dataset, yields higher performance of the stolen encoder on various downstream tasks than stealing with a downstream dataset, such as CIFAR10. As a result, intuitively, we can identify and penalize adversaries based on their coverage of the embedding space, which will be significantly larger than the coverage of legitimate users. We leverage this intuition to build our B4B defense and present our three main building blocks in the following sections.

Figure 2: **Representations from Different Tasks Occupy Different Sub-Spaces of the Embedding Space. Presented for Fashion-MNIST, SVHN, CIFAR10, and STL10.**

### Building Block 1: Coverage Estimation of the Embedding Space

The first building block of our B4B serves to estimate and continuously keep track of the fraction of the embedding space occupied by any given user. Let \(\mathcal{E}\) denote our embedding space of dimension \(s\), further, let \(U\) be a user with a query dataset \(D=q_{1},\ldots,q_{n}\in\mathcal{D}\) and let \(f_{v}:\mathcal{D}\rightarrow\mathcal{E}\) be our protected victim encoder that maps data points from the input to the embedding space. Assume user \(U\) has, so far, queried a subset of their data points \(q_{1},\ldots,q_{j}\) with \(j\leq n\) to the encoder and obtained the representations \(r_{1},\ldots,r_{j}\) with each \(r_{i}\in\mathbb{R}^{s}\). We aim to estimate the true fraction of the embedding space \(\mathcal{E}^{U}_{f}\) that is covered by all returned representations \(r_{1},\ldots,r_{j}\) to user \(U\) and denote our estimate by \(\tilde{\mathcal{E}}^{U}_{f}\).

Local Sensitive Hashing.One of the methods to approximate the occupied space by representations returned to a given user is via Local Sensitive Hashing (LSH) [39]. We rely on this approach for the concrete instantiation of our B4B and use it to track per-user coverage of the embedding space. Standard (cryptographic) hash functions are characterized by high dispersion such that hash collisions are minimized. In contrast, LSH hashes similar data points into the same or proximal, so-called _hash buckets_. This functionality is desired when dealing with searches in high-dimensional spaces or with a large number of data points. Formally, an LSH function \(\mathcal{H}\) is defined for a metric space \(\mathcal{M}=(M,d)\), where \(d\) is a distance metric in space \(M\), with a given threshold \(T>0\), approximation factors \(f>1\), and probabilities \(P_{1}\) and \(P_{2}\), where \(P_{1}\gg P_{2}\). \(\mathcal{H}\) maps elements of the metric space to buckets \(b\in B\) and satisfies the following conditions for any two points \(q_{1},q_{2}\in M\): (1) If \(d(q_{1},q_{2})\leq T\), then \(\mathcal{H}(q_{1})=\mathcal{H}(q_{2})\) (_i.e.,_\(q_{1}\) and \(q_{2}\) collide in the same bucket \(b\)) with probability at least \(P_{1}\). (2) If \(d(q_{1},q_{2})\geq fT\), then \(\mathcal{H}(q_{1})=\mathcal{H}(q_{2})\) with probability at most \(P_{2}\).

### Building Block 2: Cost Function Design

Once we can estimate the coverage of an embedding space for a given user \(U\) as \(\tilde{\mathcal{E}}^{U}_{f}\), we need to design a cost function \(\mathcal{C}:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\) that maps from the estimated coverage to a cost. The cost function needs to be designed such that it does not significantly penalize legitimate users while imposing a severe penalty on adversaries to effectively prevent the encoder from being stolen. The semantics of the cost function's range depend on the type of costs that the defender wants to enforce. We discuss a broad range of options in Appendix C. These include monetary cost functions to adaptively charge users on a batch-query basis depending on their current coverage, costs in the form of additional computation that users need to perform in order to obtain their representations, similar to the proof of work in [18], costs in the form of delay in the interaction with the encoder behind the API [4], or costs in form of disk space that needs to be reserved by the user (similar to a proof of space [19; 20]). Which type of cost function is most adequate depends on the defender's objective and setup.

Exponential Cost Functions to Adjust Utility of Representations.In the concrete instantiation of B4B that we present in this work, we rely on costs in the form of the utility of the returned representations. We choose this concrete instantiation because it is intuitive, effective, and can be directly experimentally assessed. Moreover, it is even suitable for public APIs where, for example, no monetary costs are applicable. We adjust utility by adding Gaussian noise with different standard deviation \(\sigma\) to the returned representations. Since we do not want to penalize legitimate users with small coverage but make relating for adversaries with growing coverage increasingly prohibitive, we instantiate an exponential cost function that maps from the fraction of hash buckets occupied by the user to a value for \(\sigma\). We choose the general form of this function as

\[f_{\lambda,\alpha,\beta}(\tilde{\mathcal{E}}^{U}_{f})=\lambda\times(\exp^{ \ln\frac{\alpha}{\lambda}\times\tilde{\mathcal{E}}^{U}_{f}\times\beta^{-1}}-1)\] (1)

where \(\lambda<1\) compresses the curve of the function to obtain low function values for small fractions of occupied buckets, and then we set a target penalty \(\alpha\) for our cost function at a specified fraction of filled buckets \(\beta\). For instance, if we want to enforce a \(\sigma\) of \(5\) at \(90\%\) of filled buckets (_i.e.,_ for \(\tilde{\mathcal{E}}^{U}_{f}=0.9\)), we would need to set \(\alpha=5\) and \(\beta=0.9\).

### Building Block 3: Per-User Representation Transformations against Sybil Attacks

Given that our defense discourages users from querying with a wide variety of data points from different distributions, an adversary could create multiple fake user accounts (sybils) and querydifferent data subsets with more uniform representations from each of these accounts. By combining all the obtained representations and using them to train a stolen copy, the adversary could overcome the increased costs of stealing. To defend against such sybil attacks, we propose individually transforming the representations on a per-user level before returning them. As a result, the adversary would first have to map all the representations to one single unified space before being able to jointly leverage the representations from different accounts for their stolen copy.

Formally, for a given query \(q_{i}\), the protected victim encoder produces a representation \(r_{i}=f_{v}(q_{i})\), which is transformed by a user-specific transformation \(T_{U}(r_{i})\) before being returned to the querying user \(U\). For a new user \(U\), the defense randomly selects the transformation \(T_{U}\) from all possible choices. Note that the randomness is also added on a per-transformation basis, instead of only on the level of selecting the transformations. For example, a permutation of the elements in the output representations should be different for each user.

We formulate two concrete requirements for the transformations. First, they should preserve utility for legitimate users on their downstream tasks, and second, they should be costly to reverse for an adversary.

Utility Preserving Transformations.As a concrete instantiation for our B4B, we propose a set of transformations that fulfill the above-mentioned two requirements: (1) _Affine_ where we apply affine transformations to representations, (2) _Pad_ where we pad representations with constant values, (3) _Add_ where we add constant values at random positions within representations, (4) _Shuffle_ where we shuffle the elements in the representation vectors, and (5) _Binary_ where the original representations are mapped to binary vectors relying on a random partitioning of the representation space. To preserve the full amount of information contained in the original representations, in our binary transformations, we tune the length of binary representations. We visualize the operation of each of these transformations in Appendix C. All these transformations can additionally be combined with each other, which further increases the possible set of transformations applied per user. This renders it impossible for an adversary to correctly guess and reverse the applied representation. Instead, the adversary has to remap the representations over all accounts into a single embedding space in order to unify them and leverage them for training of their stolen encoder copy. We present an exhaustive list of strategies that adversaries can apply for the remapping in Appendix D. All the attack methods reduce to the minimum of remapping between representations of a pair of users, _i.e.,_ they are at least as complex as mapping between two separate accounts. In the next section, we show that our defense already impedes stealing for an adversary with two accounts.

## 4 Empirical Evaluation

We first empirically evaluate our instantiation of B4B's three building blocks and show how to calibrate each of them for our defense. Finally, we provide an end-to-end evaluation that highlights B4B's effectiveness in preserving downstream utility for legitimate users while successfully preventing the stealing by adversaries.

Experimental Setup.We conduct experiments on various kinds of downstream tasks and two popular SSL encoders. To test our defense, we use FashionMNIST, SVHN, STL10, and CIFAR10 as our downstream datasets, each with standard train and test splits. For stealing, we utilize training data from ImageNet and LAION-5B. We rely on encoder models from the SimSiam [12] and the DINO [9] SSL frameworks. As our victim encoders, we use the publicly available ResNet50 model from SimSiam trained for 100 epochs on ImageNet and the ViT Small DINO encoder trained for 800 epochs on ImageNet, each using batch size 256. The ViT architecture takes as input a grid of non-overlapping contiguous image patches of resolution \(N\)x\(N\). In this paper, we typically use \(N=16\). The Simsiam encoder has an output representation dimension of 2048, while DINO returns 1536 dimensional representations. We examine the utility of downstream classifiers using SimSiam's or DINO's representations obtained for the respective downstream datasets. To implement LSH, we rely on random projections [34] that we implement from scratch. For a detailed description of our stealing and downstream training parameters, we refer to Appendix F.

### Local Sensitive Hashing for Coverage Estimation

We first observe that the choice of the total number of hash buckets in the LSH influences the effectiveness of our method. In the extreme, if we have a too large number of buckets, the number of buckets filled will correspond to the number of queries posed by a user which fails to capture that similar representations cover similar sub-spaces of the embedding space, and hence does not serve to approximate the total fraction of the embedding space covered. However, if we have too few buckets, even the representations for simple downstream tasks will fill large fractions of buckets, making it impossible to calibrate the cost function such that it only penalizes adversaries. We experimentally find that for our evaluated encoders, \(2^{12}\) buckets represent a good trade-off. In Appendix F.6, we present an ablation study on the effect of the number of total buckets.

Our evaluation of the LSH to track coverage of the embedding space is presented in Figure 3. We observe that representations returned for standard downstream tasks (FashionMNIST, SVHN, CIFAR10) occupy a significantly smaller fraction of the total number of buckets than complex data from multiple distributions (ImageNet, LAION-5B). We present additional experimental results on measuring the coverage of the representation space in Appendix F.5. Specifically, we show that our method of measuring the embedding space coverage has broad applicability across various encoders and datasets used for pretraining. We further observe that the fraction of buckets occupied by the representations saturates over time. These findings highlight that LSH is successful in capturing the differences between legitimate users and adversaries--even in a low-query regime. Finally, we note that our total number of buckets (\(2^{12}\)) is well calibrated since, over all datasets, it successfully maps multiple representations to the same hash bucket while still filling various fractions of the total number of buckets.

### Calibrating the Cost Function

We experiment with different sets of hyperparameters to instantiate the cost function from Equation (1) in the previous section (3.3). As described there, we can calibrate the function (as shown in Figure 4) such that a desired penalty (in the form of a specific \(\sigma\)) will be assigned at a certain fraction of buckets occupied. For B4B, we aim at penalizing high embedding space coverage severely. Therefore, we need to identify and optimize for two components: 1) which value of \(\sigma\) leads to significant performance drops, and 2) for what

Figure 4: **Cost Function Calibration.**

Figure 3: **Estimating Embedding Space Coverage through LSH on SimSiam Encoder.** We present the fraction of buckets occupied by representations of different datasets as a function of the number of queries posed to the encoder _(left)_. We observe that representations for the downstream datasets (FashionMNIST, SVHN, CIFAR10) occupy a smaller fraction of buckets than representations from the complex ImageNet dataset. Our evaluation of the number of queries whose representations are mapped to the same bucket _(right)_ indicates that our total number of buckets (\(2^{12}\)) is well calibrated for the estimation of covered representation space: over all datasets, we experience hash collisions, _i.e.,_ queries whose representations are mapped to the same buckets. This indicates that our LSH is capable of representing similarities in the representations.

fraction of coverage do we want to impose this significant drop. We base both components on empirical observations. Our first observation is that for our four downstream tasks (FashionMNIST, SVHN, STL10, and CIFAR10), performance drops to 10% (_i.e.,_ random guessing) at roughly \(\sigma=0.5\). In Figure 3, we further see that with 50k queries, the downstream tasks occupy \(<30\%\) of the buckets. Ultimately, setting \(\alpha\) and \(\beta\) are design choices that an API provider needs to make in order to specify what type of query behavior they want to penalize. As very loose bounds (weak defense), based on our observation, we consider \(\sigma=1\) as a high penalty, which leads to \(\alpha=1\), and select \(\beta=0.8\). This \(\beta\) corresponds to considering 80% of buckets filled as a too-large coverage of the embedding space. We empirically observe that coverage of 80% of buckets occurs, for example, after around 100k of ImageNet queries. By choosing our target \(\beta\) so loose, _i.e.,_ significantly larger than the observed \(30\%\) for downstream tasks, we offer flexibility for the API to also provide good representations for more complex downstream tasks. Finally, to obtain a flat cost curve close to the origin--which serves to map small fractions of covered buckets to small costs--we find that we can set \(\lambda=10^{-6}\). In the Appendix, we evaluate our defense end-to-end with differently parameterized cost functions.

### Assessing the Effect of Transformations

Transformations Do Not Harm Utility for Legitimate Users.We evaluate the downstream accuracy for transformed representations based on training a linear classifier on top of them. To separate the effect of the noise added by our defense from the effect of the transformations, we perform the experiments in this subsection without adding noise to the returned representations. For example, on the CIFAR10 dataset and a SimSiam encoder pre-trained on ImageNet, without any transformations applied, we obtain a downstream accuracy of 90.41% (\(\pm\) 0.02), while, with transformations, we obtain 90.24% (\(\pm\) 0.11) for Affine, 90.40% (\(\pm\) 0.05) for Pad+Shuffle, 90.18% (\(\pm\) 0.06) for Affine+Pad+Shuffle, and 88.78% (\(\pm\) 0.2) for Binary. This highlights that the transformations preserve utility for legitimate users. This holds over all datasets we evaluate as we show in Appendix F.

Adversaries Cannot Perfectly Remap Representations over Multiple Sybil Accounts.To understand the impact of our per-user account transformations on sybil-attack based encoder stealing, we evaluate the difficulty of remapping representations between different sybil accounts. For simplicity, and since we established in Section 3.4 that multi-account attacks reduce to a two-account setup, we assume an adversary who queries from two sybil accounts and aims at learning to map the transformed representations from account #2 to the representation space of account #1. Using more accounts for the adversary causes a larger query overhead and potentially more performance loss from remapping. Our evaluation here, hence, represents a lower bound on the overhead caused to the adversary through our transformations.

We learn the mapping between different accounts' representations by training a linear model on overlapping representations between the accounts. We assess the fidelity of remapped representations as a function of the number of overlapping queries between the accounts. As a fidelity metric for our remapping, we compare the cosine distance between representations (\(a\) and \(b\) defined as: \(1-\frac{a^{T}b}{\|(a\|a\|^{2}\|b\|_{2})}\)). Once the remapping is trained, we evaluate by querying 10k data points from the test dataset through account #1 and then again through account #2. Then, we apply the learned remapping to the latter one and compute the pairwise cosine distances between the representations from account #1 and their remapped counterparts from account #2. Our results are depicted in Figure 5. We show that the largest cosine distance is achieved with the binary transformations, making them the most protective against the adversary since they best prevent perfect remapping, even with an overlap of as many as 10k queries between both accounts. However, these binary transformations also incur the highest drop in accuracy for legitimate users. The defender has the possibility of selecting their preferred types of transformations between representations taking into account the trade-offs between the effectiveness of the defense and the negative impact on legitimate users.

Figure 5: **Quality of Remappings.**

### End-to-End Stealing of an Encoder under our Defense

We perform an end-to-end study to showcase how our B4B defense affects legitimate users vs adversaries. The hyperparameters for B4B are chosen according to the empirical evaluation of the previous sections with \(2^{12}\) as the number of buckets, \(\alpha=1,\beta=0.8,\lambda=10^{-6}\) as the hyperparameter of the cost function, and different random affine transformations per-user account. Our main results are presented in Table 1. We observe that instantiating our framework with B4B has a negligible impact on legitimate users while substantially lowering the performance of stolen encoders in the case of single-user and sybil attackers.

**Legitimate Users.** We compare the accuracy of downstream classifiers trained on top of unprotected vs defended encoders. The victim encoder achieves high accuracy on the downstream tasks when no defense is employed. With B4B in place, we observe that across all the downstream tasks, the drop in performance is below 1%. For example, there is only a slight decrease in the accuracy of CIFAR10 from 90.41\(\pm 0.02\)% to 90.24\(\pm 0.11\)%. B4B's small effect on legitimate users stems from the fact that their downstream representations cover a relatively small part of the representations space. This results in a very low amount of noise added to their representations which preserves performance.

**Adversaries.** For adversaries who create a stolen copy of the victim encoder, we make two main observations. The most crucial one is that when our B4B is in place, the performance of the stolen copies over all downstream tasks significantly drops in comparison to when the victim encoder is unprotected (grey rows in Table 1). This highlights that our B4B effectively prevents stealing. Our next key observation concerns the number of stealing queries used by the adversary: When no defense is applied, the more queries are issued against the API (_e.g.,_ 100k instead of 50k), the higher performance of the stolen encoder on downstream tasks (_e.g.,_ CIFAR10 or FashionMNIST). In contrast, with B4B implemented as a defense, the performance decreases when using more stealing queries from a single account. This is because with more queries issued, the coverage of embedding space grows which renders the returned representations increasingly noisy and harms stealing performance.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline User & Defense & \# Queries & Dataset & Type & CIFAR10 & STL10 & SVHN & F-MNIST \\ \hline Legit & None & All & Task & Query & 90.41 \(\pm 0.02\) & 95.08\(\pm 0.13\) & 75.47\(\pm 0.04\) & 91.22\(\pm 0.11\) \\ Legit & B4B & All & Task & Query & 90.24\(\pm 0.11\) & 95.05\(\pm 0.1\) & 74.96\(\pm 0.13\) & 91.71\(\pm 0.01\) \\ Attack & None & 50K & Imgnet & Steal & 65.2\(\pm 0.03\) & 64.9\(\pm 0.01\) & 63.1\(\pm 0.01\) & 88.5 \(\pm 0.01\) \\ Attack & B4B & 50K & Imgnet & Steal & **35.72\(\pm 0.04\)** & **31.54\(\pm 0.02\)** & **19.74\(\pm 0.02\)** & **70.01\(\pm 0.01\)** \\ Attack & None & 100K & Imgnet & Steal & 66.1 \(\pm 0.03\) & 63.1 \(\pm 0.01\) & 61.5 \(\pm 0.01\) & 89.0 \(\pm 0.07\) \\ Attack & B4B & 100K & Imgnet & Steal & 12.01\(\pm 0.07\) & 13.94\(\pm 0.05\) & **19.96\(\pm 0.03\)** & **69.63\(\pm 0.07\)** \\ Attack & None & 100K & Ldison & Steal & 64.92\(\pm 0.03\) & 62.51\(\pm 0.03\) & 59.02\(\pm 0.02\) & 84.54\(\pm 0.01\) \\ Attack & B4B & 100K & LAION & Steal & 40.96\(\pm 0.03\) & **40.69\(\pm 0.05\)** & **34.43\(\pm 0.01\)** & 72.92\(\pm 0.01\) \\ \hline Sybil & B4B & 2x50K & Imgnet & Steal & 39.56\(\pm 0.06\) & 38.50\(\pm 0.04\) & 23.41\(\pm 0.02\) & 77.01\(\pm 0.08\) \\ Sybil & B4B & 3x33.3k & Imgnet & Steal & 33.87\(\pm 0.05\) & 38.57\(\pm 0.06\) & 21.16\(\pm 0.01\) & 72.95\(\pm 0.05\) \\ Sybil & B4B & 4x25k & Imgnet & Steal & 33.98\(\pm 0.04\) & 34.52\(\pm 0.08\) & 21.21\(\pm 0.02\) & 70.71\(\pm 0.05\) \\ Sybil & B4B & 5x20K & Imgnet & Steal & 32.65\(\pm 0.05\) & 32.45\(\pm 0.05\) & 29.63\(\pm 0.01\) & 70.12\(\pm 0.08\) \\ Sybil & B4B & 6x16.7k & Imgnet & Steal & 26.62\(\pm 0.04\) & 26.85\(\pm 0.05\) & 24.32\(\pm 0.02\) & 70.51\(\pm 0.04\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Stealing and Using Encoders With and Without our Defense.** The _USER_ column represents the type of the APIs’ user, where LEGIT denotes a legitimate user, ATTACKER stands for a standard single-account adversary, and SYBIL represents an adversary using two sybil accounts. We use InfoNCE loss for encoder extraction. # Queries stands for the number of queries used for stealing with ALL denoting that the entire downstream dataset was used. The _TYPE_ column expresses how the dataset is used. We follow the stealing setup from [17]. In the first row, we present the undefended victim encoder’s performance as the accuracy for downstream tasks trained on the encoder’s returned representations. In the following row, we show downstream utility for legitimate users when the victim encoder is defended by our B4B. Finally, (in the remaining rows) we assess the performance of stolen encoders on the downstream tasks. Our results highlight that while the performance of the encoder for legitimate users stays high, our B4B renders stealing inefficient with the stolen encoders obtaining significantly worse performance on downstream tasks.

Moreover, we show that B4B can also prevent model stealing attacks with data from a different distribution than the victim encoder's training set. We highlight this in Table 1 where we also use the LAION-5B dataset to steal an ImageNet pre-trained encoder. Our results highlight first that without any defense in place, the LAION dataset is highly effective to extract the ImageNet pre-trained encoder. Second, B4B effectively defends against such attacks, and yields a significant drop in downstream accuracy (on average above 20%) of the stolen encoder.

We also show that this performance drop cannot be prevented by sybil attacks. Therefore, we first consider an adversary who queries from two sybil accounts with 50k queries issued per account and the first 10k queries of both accounts used to learn the remapping of representations between them. When the adversary trains their stolen encoder copy on all the remapped representations, they increase downstream performance over querying from a single account. Yet, their performance is still significantly smaller than the performance of the victim encoder for legitimate users, or the encoder stolen from an undefended victim. Moreover, using more than two sybil accounts further reduces the attack performance as remapping complications accumulate. With ten sybils, remapping leaves no more usable data for training the stolen encoder. This demonstrates our method's advantage: increasing the number of sybil accounts makes encoder extraction impractical due to the growing remapping overhead. Overall, the results highlight that our B4B also successfully prevents sybil attacks.

### Baseline Comparison

Finally, we compare our B4B against the current state-of-the-art baseline defense, namely a static addition of noise to all the returned representations (as proposed in [16] (Section A.4),[29, 36]). For direct comparability, we evaluate the defense using the our end-to-end experiment setup from the previous section. We present our results in Table 6 in Appendix F.4. Confirming the findings from [16] our results also show that defenses that rely on static noise have the great disadvantage to harm legitimate users and attackers equally. When adding noise with a small standard deviation of \(\sigma=0.1\), we observe a negligible (<1%) performance drop for both attackers and legitimate users. Adding noise with a large standard deviation of, for example, \(\sigma=10\), we observe that both legitimate users' and attackers' performance drops between 15% and >40%. In summary, these defenses can either effectively defend stealing (but harm legitimate users), or keep utility for legitimate users high (but not defend well against stealing). In contrast, our B4B is able to provide high performance for legitimate users while effectively defending the encoder against stealing attacks.

## 5 Conclusions

We design B4B a new and modular active defense framework against stealing SSL encoders. All the previous approaches were either reactive, acting after the attack happened to detect stolen encoders, or lowered the quality of outputs substantially also for legitimate users which rendered such mechanisms impractical. We show that B4B successfully distinguishes between legitimate users and adversaries by tracking the embedding space coverage of users' obtained representations. B4B then leverages this tracking to apply a cost function that penalizes users based on the current space coverage, for instance, by lowering the quality of their outputs. Finally, B4B prevents sybil attacks by implementing per-user transformations for the returned representations. Through our experimental evaluation, we show that our defense indeed renders encoder stealing inefficient while preserving downstream utility for legitimate users. Our B4B is therefore a valuable contribution to a safer sharing and democratization of high-utility encoders over public APIs.

## Acknowledgments and Disclosure of Funding

This research was supported by Warsaw University of Technology within the Excellence Initiative Research University (IDUB) programme, National Science Centre, Poland grant no 2020/39/O/ST6/01478, grant no 2020/39/B/ST6/01511, grant no 2022/45/B/ST6/02817, and in part by PL-Grid Infrastructure grant nr PLG/2022/016058. The authors applied for a CC BY license to any Author Accepted Manuscript (AAM) version arising from this submission, in accordance with the grants' open access conditions.

## References

* [1] Clarifai, https://www.clarifai.com/. URL https://www.clarifai.com/.
* [2] Cohere, https://cohere.ai. URL https://cohere.ai/.
* [3] Openai, https://openai.com. URL https://openai.com/.
* [4] Poet2, https://labs.hyperledger.org/labs/archived/sawtooth-poet2.html. URL https://labs.hyperledger.org/labs/archived/sawtooth-poet2.html.
* [5] Google, https://google.github.io/snappy/. URL https://google.github.io/snappy/.
* [6] Facebook zstd, https://github.com/facebook/zstd. URL https://github.com/facebook/zstd.
* [7] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In _27th USENIX Security Symposium (USENIX Security 18)_, pages 1615-1631, 2018.
* [8] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In _USENIX Security Symposium_, volume 6, 2021.
* [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9650-9660, October 2021.
* [10] Ting Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. _International Conference on Machine Learning_, 2020.
* [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations, 2020.
* [12] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. 2020.
* [13] Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature Learning. In _AISTATS_, 2011. https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf.
* [14] Tianshuo Cong, Xinlei He, and Yang Zhang. Sslguard: A watermarking scheme for self-supervised learning pre-trained encoders. _CoRR_, abs/2201.11692, 2022. URL https://arxiv.org/abs/2201.11692.
* [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [16] Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, and Nicolas Papernot. On the difficulty of defending self-supervised learning against model extraction. In _International Conference on Machine Learning_, 2022.
* [17] Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, and Nicolas Papernot. Dataset inference for self-supervised models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=CCBJf9xJo2X.
* [18] Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, and Nicolas Papernot. Increasing the cost of model extraction with calibrated proof of work. In _International Conference on Learning Representations_, 2022. URL https://arxiv.org/abs/2201.09243.
* [19] Stefan Dziembowski, Sebastian Faust, Vladimir Kolmogorov, and Krzysztof Pietrzak. Proofs of space. 2013. URL https://eprint.iacr.

* Dziembowski et al. [2015] Stefan Dziembowski, Sebastian Faust, Vladimir Kolmogorov, and Krzysztof Pietrzak. Proofs of space. In _Advances in Cryptology-CRYPTO 2015: 35th Annual Cryptology Conference, Santa Barbara, CA, USA, August 16-20, 2015, Proceedings, Part II_, pages 585-605. Springer, 2015.
* Frosst et al. [2019] Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft nearest neighbor loss. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2012-2020. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/frosst19a.html.
* Grill et al. [2020] Jean-Bastien Grill, F. Strub, F. Altche, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new approach to self-supervised learning. _Computer Vision and Pattern Recognition_, 2020.
* He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16000-16009, June 2022.
* Jagielski et al. [2020] Matthew Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot. High accuracy and high fidelity extraction of neural networks. _USENIX Security Symposium_, 2020.
* Jia et al. [2021] Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, and Nicolas Papernot. Entangled watermarks as a defense against model extraction. In _30th USENIX Security Symposium (USENIX Security 21)_, pages 1937-1954, 2021.
* Juuti et al. [2019] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In _2019 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 512-527. IEEE, 2019.
* Kariyappa et al. [2021] Sanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi. Maze: Data-free model stealing attack using zeroth-order gradient estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13814-13823, 2021.
* Krizhevsky [2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* Liu et al. [2022] Yupei Liu, Jinyuan Jia, Hongbin Liu, and Neil Zhenqiang Gong. Stolenencoder: Stealing pre-trained encoders in self-supervised learning. In _Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security_, CCS '22, page 2115-2128, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450394505. doi: 10.1145/3548606.3560586. URL https://doi.org/10.1145/3548606.3560586.
* Maini et al. [2021] Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. In _Proceedings of ICLR 2021: 9th International Conference on Learning Representationsn_, 2021.
* Nagai et al. [2018] Yuki Nagai, Y. Uchida, S. Sakazawa, and Shin'ichi Satoh. Digital watermarking for deep neural networks. _International Journal of Multimedia Information Retrieval, 7:3-16_, 2018.
* Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011_, 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.
* Orekondy et al. [2019] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4954-4963, 2019.
* Pauleve et al. [2010] Loic Pauleve, Herve Jegou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash function types and querying mechanisms. _Pattern recognition letters_, 31(11):1348-1358, 2010.

* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.
* Sha et al. [2022] Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, and Yang Zhang. Can't steal? cont-steal! contrastive stealing attacks against image encoders. 2022. URL https://arxiv.org/abs/2201.07513.
* Shen et al. [2022] Yun Shen, Xinlei He, Yufei Han, and Yang Zhang. Model stealing attacks against inductive graph neural networks. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1175-1192. IEEE, 2022.
* Kumar et al. [2020] Ram Shankar Siva Kumar, Magnus Nystrom, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In _2020 IEEE Security and Privacy Workshops (SPW)_, pages 69-75, 2020. doi: 10.1109/SPW50608.2020.00028.
* Slaney and Casey [2008] Malcolm Slaney and Michael Casey. Locality-sensitive hashing for finding nearest neighbors [lecture notes]. _IEEE Signal processing magazine_, 25(2):128-131, 2008.
* Tramer et al. [2016] Florian Tramer, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart. Stealing machine learning models via prediction apis. _USENIX Security Symposium_, 2016.
* Truong et al. [2021] Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls, and Nicolas Papernot. Data-free model extraction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2021.
* Uchida et al. [2017] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh. Embedding watermarks into deep neural networks. In _Proceedings of the 2017 ACM on international conference on multimedia retrieval_, pages 269-277, 2017.
* Wu et al. [2022] Yutong Wu, Han Qiu, Tianwei Zhang, Lin Jiwei, and Meikang Qiu. Watermarking pre-trained encoders in contrastive learning. _ArXiv_, abs/2201.08217, 2022.
* Zbontar et al. [2021] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. _arXiv preprint arXiv:2103.03230_, 2021.

Broader Impacts

The goal of our work is to actively defend self-supervised encoders against model stealing attacks. Since we are directly defending encoders, any negative societal impacts of our work are minimal. One potentially negative impact could be the degradation of performance for legitimate users. However, as shown in our experimental results, we are able to preserve high utility for standard users.

## Appendix B Limitations

We show how our defense method is tuned for SimSiam and DINO. There are more types of SSL encoders that can be tested with our method. The B4B defense method requires tuning the parameters, such as the number of occupied buckets that is allowed without any penalty for the cost function, or the selection of the transformations. These steps are rather difficult to automate but can be replaced with more data-driven approaches. For example, instead of designing a cost function from scratch, one could create an ML model to obtain a cost for a given occupation of the representation space. We explain more details in the Appendix C.2.

## Appendix C Alternative Building Blocks to Instantiate B4B

While we present a reference implementation of B4B in our work that instantiates the three building blocks with (1) Local Sensitive Hashing, (2) Utility of the Representations, and (3) a set of concrete transformations, there exists a multitude of alternatives to concretely implement our B4B framework. In the following, we present these alternatives, grouped by building block.

### Alternative Estimation of the Coverage of Embedding Space

We also explore alternative methods to measure the distances between representations for queries sent to an API. One of them is to apply the cosine distance (where for two representations \(a\) and \(b\), it is defined as: \(1-\frac{a^{T}b}{||a||a||_{2}\cdot||b||_{2}}\)) since it can be measured between individual data points in a pair-wise fashion. If the total pair-wise cosine distance between representations for a given user is small, then the user queries presumably come from a single downstream task distribution. Otherwise, a user might be malicious and would like to cover a large part of the representation space, then the total pair-wise cosine distance for the user's representations would be high. Note that in this case, the cosine distance can be replaced with any other distance measure, such as the Minkowski distance. We opt for the LSH in our reference implementation, since it is much less expensive to compute than cosine distance. LSH requires only \(2^{12}=4096\) buckets that can be expressed as a binary table with the same number of elements, which requires in the worst case iterating over all of them to count how many are occupied. With more than \(4096\) queries sent by a given user, the computation on the LSH is sublinear \(<O(n)\) with respect to the number of user queries. For the cosine distance approach, the required computation grows quadratically \(O(n^{2})\) with the number of queries.

### Alternative Cost Functions

The cost functions can be designed from scratch manually or learned, for example, via an ML model, such as a neural network or SVM. In our initial version, the function was designed manually, where the underlying premise is that once a specified number of buckets is occupied, the cost should grow exponentially. Instead of defining such a function or providing the high-level parameters for functions that we contributed, one could learn an ML model that for a given number of buckets occupied, it should output an estimated cost, or even directly, the desired \(\sigma\) (standard deviation) of the noise added to representations. This method requires a relatively large number of data points to be provided for training the model, however, lowers the burden on a defender to either decide on the specific function or adjust its parameters. Thus, it could be more user-friendly, for example, not necessitating any mathematical background, but can be precise enough to obtain the desired behavior.

Note that instead of adding the calibrated noise (proportional to the estimated cost) to the representations, we could rather require a given user to pay a higher monetary cost for queries that cover a large fraction of the representation space, or force a user to solve a puzzle in a form of the proof-of-work [18], wait a specified amount of time via proof-of-elapsed time (PoET) [4], or prove that a specified amount of disk space was reserved [19; 20]. For example, consider the approach with PoET. A user sends queries to the API, which we cost based on their occupation of the embedding space. The user is sent a waiting time. The users' resource (e.g., a CPU) has to be occupied for this specific waiting time without performing any work. At the end of the specified amount of time, the user sends proof of the elapsed time, which can be easily verified by the server. PoET requires access to specialized hardware, for example, secure CPU instructions that are becoming widely available in consumer and enterprise processors. If a user does not own such hardware, proof of elapsed time could be produced using a service exposed by a cloud provider (e.g., Azure VMs that feature TEE 2). Note that if a server sends the time based on the calculated cost, the adversary might learn the cost function. Instead, the exact waiting time should be split in random _subwaiting_ times and sent to the user one by one. Thus, a server should rather have a few rounds of exchange with the client to incur the additional cost.

### Alternative to Transformations

As an alternative to the transformations used within this work (see Figure 6), one could use a different set of transformations or combinations thereof. The padding can be done with different constant values and combined with adding constant values within the representations. The padding and adding the constant values can be followed by shuffling the elements within the representations. We can apply the affine of binary transformations on top of the padding and shuffling. Additionally, we can also use other pre-defined linear transformations like rotations or shearing.

The representations could also be compressed to smaller vectors and the compression rate would depend on the occupation of the representation space, for example, the higher the number of occupied buckets in our hash table, the more compressed the output representations could be. Such representations could be compressed via FFT, a cosine transform, or standard compression techniques such as snappy [5]. If the information from the representations should not be lost, then the lossless compression techniques can be applied, for instance, zstd [6]. The only requirement of the compression techniques is to ensure that they do not decrease the accuracy on downstream tasks for legitimate users.

Another alternative is to incorporate an additional neural network layer for transforming the returned representations. The training of this supplementary layer should primarily focus on preserving the usability of the representations for legitimate users. This approach grants the API provider with additional capabilities, as it allows for the utilization of customized training objectives. For instance, if the API provider employs LSH (Locality-Sensitive Hashing) to estimate the coverage of the representation space, they can leverage buckets and train the additional layer to maintain high-quality representations exclusively for frequently-used buckets and their surrounding areas, while not prioritizing the rest of the representation space. This approach safeguards legitimate users

Figure 6: **Overview on Transformations.** We depict the inner-workings of the transformations considered in this work.

from any adverse effects, as their coverage of the representation space is minimal. Simultaneously, it ensures that adversaries are unable to exploit representations from the entire representation space.

## Appendix D Sybil Attacks

We consider an adversary who generates \(n\) sybil accounts to steal the encoder from the API. For each of the accounts, the representations are transformed in a different way. Therefore, to replicate the victim model using all the obtained representations, the adversary has to map these representations into one single space. This can be done, for example, by training a neural network to perform the mapping.

We assume the adversary obtains \(\{N_{1},N_{2},\dots,N_{n}\}\) many representations from the victim for each of the \(n\) sybil accounts. Without loss of generality, we assume the adversary maps them back to the embedding space of the first sybil account. To learn the mapping, the adversary can apply different strategies.

### Sybil Strategies

We present three potential approaches that Sybils might want to apply to circumvent our defense. Consider three users: \(A\), \(B\), and \(C\), with their respective datasets \(D_{A}\), \(D_{B}\), and \(D_{C}\), each with different distributions to maximize extraction effectiveness. First, user \(A\) is selected to unify representations from other users \(B\) and \(C\). User \(A\) would have to query from at least two different datasets \(D_{B}\) and \(D_{C}\), while other users would act legitimately. Sybil attackers want to deploy as many users as possible but with more fake accounts, user \(A\) incurs high coverage of the representation space, and this is prevented by our single-user defense. In all other cases neither of the sybil users can legitimately, thus they are already affected by the single-user defense. Second, user \(A\) would query from their own dataset \(D_{A}\) and partially from dataset \(D_{B}\). Then user \(B\) would query from their own dataset \(D_{B}\) and partially from dataset \(D_{C}\), and so on. This method is the most inconspicuous but requires a number of remappings that scales super-exponentially with the number of fake accounts, which is impractical. Finally, each user would query from their respective dataset, for example, user \(B\) would query from dataset \(D_{B}\) and additionally from a remapping dataset, _e.g.,_\(D_{A}\). Representations could be unified by mapping them to \(A\)'s representations. The last approach as well as all other cases reduce to the minimum of remapping between representations of a pair of users. We show that our defense cuts such attempts short by ensuring that the remapping between representations is prohibitive even for a pair of users.

## Appendix E Additional Related Work

One of the main workhorse techniques used in the encoders is contrastive learning, where the representations are trained so that the positive pairs (two augmented versions of the same image) have similar representations while negative pairs (augmentations of two different images) have representations which are far apart.

**SimSiam** utilizes Siamese networks (two encoders with shared weights) but with a simplified training process and architecture. In contrast to the previous frameworks, such as SimCLR [10], SimSiam's authors show that negative samples are unnecessary and collapsing solutions can be avoided by applying the projection head to of one of the encoders, and a stop-gradient operation to the other. SimSiam minimizes the negative cosine similarity between two randomly augmented views of the same image from the Siamese encoders, which is expressed via a symmetrized loss [22]. This creates a simple yet highly effective representation learning method.

**DINO** is another popular representation learning framework. While SimSiam uses CNNs, DINO employs vision transformers (ViTs). It trains a student and teacher encoder with the same architecture, updating the teacher with an (exponential moving) average of the student. Different random transformations of the same image are passed through both encoders. The student receives smaller image crops, forcing it to generate representations restoring parts of the original image. The training objective is minimizing cross-entropy loss between teacher and student representations.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

### B4B vs Static Noise Addition Defenses

We compare our B4B against the current state-of-the-art baseline defense, namely adding a static addition of noise to all the returned representations (as proposed in [16] (Section A.4),[29, 36]). For the Table 6, we use the same setup as in Table 1 (with an ImageNet pre-trained encoder).

Our results show the following insights:

1. If the amount of noise is small (\(\sigma=0.1\)) then the performance drop is negligible but for both a legitimate user (row 2) and an adversary (row 6). In this case, the defense does not affect the adversary at all (compare rows 5 & 6).
2. If the amount of noise is large (\(\sigma=10\)) then the performance drop is large for both a legitimate user (row 3) and an adversary (row 7). In this case, the encoder is worthless for legitimate users since the performance is too low.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline User & Defense & \# Queries & Dataset & Type & CIFAR10 & STL10 & SVHN & F-MNIST \\ \hline Legit & None & ALL & TASK & Query & 90.41\(\pm\)0.02 & 95.08\(\pm\)0.13 & 75.47\(\pm\)0.04 & 91.22\(\pm\)0.11 \\ Legit & Noise \(\sigma\)=0.1 & ALL & TASK & Query & 90.20\(\pm\)0.03 & 95.15\(\pm\)0.13 & 75.29\(\pm\)0.09 & 91.24\(\pm\)0.02 \\ Legit & Noise \(\sigma\)=10 & ALL & TASK & Query & 65.11\(\pm\)0.45 & 76.37\(\pm\)0.14 & 33.23\(\pm\)0.09 & 65.83\(\pm\)0.13 \\ Legit & B4B & ALL & TASK & Query & 90.24\(\pm\)0.11 & 95.05\(\pm\)0.1 & 74.96\(\pm\)0.13 & 91.77\(\pm\)0.01 \\ \hline Attack & None & 50K & ImageNet & Streal & 65.2\(\pm\)0.03 & 64.9\(\pm\)0.01 & 63.1\(\pm\)0.01 & 88.5 \(\pm\)0.01 \\ Attack & Noise \(\sigma\)=0.1 & 50K & ImGreNet & Streal & 64.92\(\pm\)0.04 & 64.61\(\pm\)0.02 & 62.35\(\pm\)0.01 & 88.41\(\pm\)0.01 \\ Attack & Noise \(\sigma\)=10 & 50K & ImGreNet & Steal & 36.32\(\pm\)0.2 & 32.59\(\pm\)0.06 & 20.59\(\pm\)0.01 & 74.94\(\pm\)0.02 \\ Attack & B4B & 50K & ImGreNet & Steal & 35.72\(\pm\)0.04 & 31.54\(\pm\)0.02 & 19.74\(\pm\)0.02 & 70.01\(\pm\)0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Stealing and Using Encoders with Static Noise Addition Defenses vs. Our B4B Defense.** Adding a small amount of noise results in negligible drop in performance for both legitimate user (row 2) and an adversary (row 6). Adding a large amount of noise defend stealing (row 7), but significantly harm legitimate users at the same time (row 3). Our B4B defense solves the above problem and provides high performance for legitimate users (row 4) while effectively defending the encoder against stealing attacks (row 8).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline User & Defense & \# Queries & Dataset & Type & CIFAR10 & STL10 & SVHN & F-MNIST \\ \hline Legit & None & ALL & TASK & Query & 90.41\(\pm\)0.02 & 95.08\(\pm\)0.13 & 75.47\(\pm\)0.04 & 91.22\(\pm\)0.11 \\ Legit & B4B & 50K & CIFAR10 & Query & 90.17\(\pm\)0.1 & 94.92\(\pm\)0.09 & 74.97\(\pm\)0.13 & 91.71 \(\pm\)0.08 \\ \hline Attack & None & 50K & ImageNet & Streal & 65.2\(\pm\)0.03 & 64.9\(\pm\)0.01 & 62.1 \(\pm\)0.01 & 88.5 \(\pm\)0.01 \\ Attack & B4B & 50K & ImageNet & Streal & 19.95\(\pm\)0.19 & 15.54\(\pm\)0.34 & 19.57 \(\pm\)0.01 & 23.50 \(\pm\)0.19 \\ Attack & None & 100K & ImageNet & Streal & 68.1 \(\pm\)0.03 & 63.1 \(\pm\)0.01 & 61.5 \(\pm\)0.01 & 89.0 \(\pm\)0.07 \\ Attack & B4B & 100K & ImGreNet & Steal & 10.35 \(\pm\)0.19 & 12.37 \(\pm\)0.69 & 19.34 \(\pm\)0.01 & 68.93 \(\pm\)0.17 \\ Sybil & B4B & 4\(\times\)25K & ImGreNet & Steal & 33.15 \(\pm\)0.04 & 30.23 \(\pm\)0.07 & 20.87 \(\pm\)0.01 & 72.19 \(\pm\)0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Stealing and Using Encoders With and Without our Defense**. The model used in the experiments is Simsiam, with the following parameters for the cost function \(\lambda=10^{-6}\), \(\alpha=0.1\), and \(\beta=30\)%, and the number of buckets equal to \(2^{12}\). Due to the lower performance on downstream tasks observed in Table 4 while keeping the parameter \(\beta\) fixed to 30% and \(\lambda\) fixed to \(10^{-6}\), we decrease the value of parameter \(\alpha\) to 0.1, which increases the performance of legitimate users on their downstream tasks. In this experiment, we also carry out a sybil attack with more accounts (4 instead of 2), but observe that this modification does not improve the performance of the attacker. With more accounts, a sybil has to sacrifice more queries for the remappings between the representations from different accounts. Additionally, note that each account introduces a different remapping error by the dint of different transformations applied to each account by B4B.

### Additional Embedding Space Coverage experiments

We present additional experiments on measuring the coverage of the representation space.

First, we use the same set-up as from Table 1 - SimSiam with ResNet50 pretrained on ImageNet. When querying the encoder with ImageNet-Full (includes all 1000 classes) and LAION-5B datasets, they both occupy a large fraction of the representation space of the victim encoder, as shown on Figure 8. In contrast, CIFAR10 covers the smallest portion of the representation space as the simplest dataset tested. ImageNet-Dogs (with only 118 classes for dog breeds) falls in the middle, occupying more space than CIFAR10 but less than ImageNet-Full and LAION-5B. Its intermediate coverage aligns with its mid-level difficulty compared to the other datasets. As indicated by representation space coverage, stealing the encoder is similarly effective with ImageNet-Full and LAION-5B datasets, as both datasets cover a large fraction of the representation space. Overall, Figure 8 demonstrates that: 1) our B4B can successfully protect the encoder model even from attackers stealing with data that was not used to train the model (LAION-5B in this case) and 2) while providing clean representation for users querying from downstream tasks that are part of more complicated datasets (ImageNet-Dogs).

Our method of measuring the embedding space coverage is not limited to a particular encoder or dataset used for pretraining. We demonstrate this in Figure 9, showing the fraction of occupied buckets for a SimCLR [11] Resnet34 encoder pretrained on CIFAR10.

Figure 8: **Fraction of Occupied Buckets (Embedding Space Coverage) for the ImageNet encoder.** Representations for the downstream datasets (CIFAR10, ImageNet - Dogs) occupy a smaller fraction of buckets than representations from the complex ImageNet or LAION-5B datasets. The underlying encoder is SimSiam pre-trained on ImageNet with ResNet50.

Figure 9: **Fraction of Occupied Buckets (Embedding Space Coverage) for the CIFAR10 encoder.** B4B can be applied to an encoder trained on CIFAR10. Representations for the downstream datasets (FashionMNIST, SVHN) occupy a smaller fraction of buckets than representations from CIFAR10, CIFAR100, and STL10 datasets. The underlying encoder is SimCLR pre-trained on CIFAR10 with ResNet34.

### Setting the number of buckets

We present our procedure to find an optimal number of buckets in Figure 10.

### Results for DINO

We show that our defense is also applicable to the DINO encoder. The occupation of the representations space is presented visually in Figure 11. We also show that the number of buckets \(2^{12}\) is optimal for DINO in Figure 12. The impact of transformation on the representations from DINO is shown Table 8. Finally, the end to end experiment for DINO is presented in Table 7.

Figure 11: **Representations from Different Tasks Occupy Different Sub-Spaces of the Embedding Space. Presented for FashionMNIST, SVHN, CIFAR10, and STL10. In this plot, we used the DINO ViT Small encoder trained on ImageNet.**

Figure 10: **Estimating Embedding Space Coverage through LSH on the SimSiam Encoder. We extend the results from Figure 3(a) and present the fraction of buckets occupied by representations of different datasets as a function of the number of queries posed to the encoder. We consider different number of buckets in the LSH table. We observe that \(2^{8}\) buckets is to small since queries from the ImageNet dataset saturate all the buckets after around 50k queries, while the number \(2^{14}\) of buckets is too large since it is never occupied more than 40%. Thus, the number \(2^{12}\) buckets is a good middle ground. Subfigure (c) corresponds to Figure 3 from the main paper. We also use the same notation and carry out our experiments in the same way as in Figure 3.**

### Additional evaluation of transformations

Additionally, we show the impact of transformations on the performance of legitimate users in Table 8 (for both SimSiam and DINO).

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline User & Defense & \# Queries & Dataset & Type & CIFAR10 & STL10 & SVHN & F-MNIST \\ \hline Legit & None & All & Task & Query & 94.51 \(\pm\) 0.08 & 97.98 \(\pm\) 0.04 & 70.66 \(\pm\) 0.16 & 89.98 \(\pm\) 0.01 \\ Legit & B4B & ALL & TASK & Query & 94.25 \(\pm\) 0.11 & 98.05 \(\pm\) 0.04 & 69.66 \(\pm\) 0.14 & 89.68 \(\pm\) 0.01 \\ Attack & None & 50K & ImNet & Steal. & 67.92 \(\pm\) 0.04 & 66.02 \(\pm\) 0.22 & 61.30 \(\pm\) 0.01 & 89.46 \(\pm\) 0.01 \\ Attack & B4B & 50K & ImNet & Steal & **42.02\(\pm\)**0.05 & **38.91\(\pm\)**0.06 & **19.94\(\pm\)**0.02 & 73.33\(\pm\)0.04 \\ Attack & None & 100K & ImNet & Steal & 75.07 \(\pm\) 0.01 & 76.32 \(\pm\) 0.02 & 71.79 \(\pm\) 0.06 & 89.76 \(\pm\) 0.01 \\ Attack & B4B & 100K & ImNet & Steal & 19.27\(\pm\)0.03 & 21.24\(\pm\)0.03 & 19.84\(\pm\)0.01 & 71.01\(\pm\)0.03 \\ Sybil & B4B & 50K+50K & ImNet & Steal & 45.56\(\pm\) 0.06 & 42.50\(\pm\)0.02 & 24.25\(\pm\)0.03 & 78.01\(\pm\) 0.08 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Stealing and Using Encoders With and Without our Defense. The model used in the experiments is DINO, with the following parameters for the cost function \(\lambda=10^{-6}\), \(\alpha=1000\), and \(\beta=60\)%, and the number of buckets equal to \(2^{12}\). We have to increase the value of parameter \(\alpha\) by \(\times 1000\) since the norms of the DINO representations are also around \(10^{3}\) higher than for SimSiam. We observe that B4B performs similarly on DINO as for SimSiam.**

Figure 12: **Estimating Embedding Space Coverage through LSH on the DINO Encoder. The number of buckets is set to \(2^{12}\). We also use the same notation and carry out our experiments in the same way as in Figure 3.**

Figure 13: **Protocol to Evaluate the Mapping Between Representations. We present the protocol of evaluating remappings for two sybil accounts. 1 API receives inputs from two sybil accounts and generates corresponding representations. 2 Representations are transformed on a per-user basis and returned. 3 Adversary trains a reference classifier on representations from account one. 4 Adversary trains a linear model to find mapping from representations of account two to representations of account one. 5 To check the quality of obtained mapping representations from test set of account two are mapped using the fixed mapper (from step 4) to representation space of account one. This enables the calculation of cosine distance between representations from account one and their counterparts from account two shown in Figure 5. Additionally, the fixed reference classifier (from step 3) can be used to measure the accuracy drop caused by remapping.**

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Transformation & Encoder & CIFAR10 & STL10 & SVHN & F-MNIST \\ \hline None & _Victim SimSiam_ & \(90.41\pm 0.02\) & \(95.08\pm 0.13\) & \(75.47\pm 0.04\) & \(91.22\pm 0.11\) \\ \hline Affine & SimSiam & \(90.24\pm 0.11\) & \(95.05\pm 0.1\) & \(74.96\pm 0.18\) & \(91.42\pm 0.15\) \\ Pad+Shuffle & SimSiam & \(90.4\pm 0.09\) & \(95.34\pm 0.06\) & \(75.47\pm 0.01\) & \(91.38\pm 0.15\) \\ Affine+Pad+Shuffle & SimSiam & \(90.18\pm 0.06\) & \(95.03\pm 0.05\) & \(74.86\pm 0.1\) & \(91.35\pm 0.1\) \\ Binary & SimSiam & \(88.78\pm 0.2\) & \(94.72\pm 0.02\) & \(68.42\pm 0.16\) & \(88.91\pm 0.34\) \\ \hline None & _Victim DINO_ & \(94.51\pm 0.08\) & \(97.98\pm 0.04\) & \(70.66\pm 0.16\) & \(89.98\pm 0.03\) \\ Affine & DINO & \(94.25\pm 0.11\) & \(98.05\pm 0.04\) & \(69.77\pm 0.11\) & \(89.68\pm 0.01\) \\ Pad+Shuffle & DINO & \(94.72\pm 0.02\) & \(98.07\pm 0.03\) & \(70.44\pm 0.1\) & \(89.91\pm 0.08\) \\ Affine+Pad+Shuffle & DINO & \(94.26\pm 0.06\) & \(98.02\pm 0.01\) & \(69.49\pm 0.2\) & \(89.70\pm 0.1\) \\ Binary & DINO & \(92.96\pm 0.1\) & \(98.03\pm 0.03\) & \(59.53\pm 0.27\) & \(88.26\pm 0.04\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Impact of Transformations on the Performance for Legitimate Users.** We show that the transformations applied per-account do not harm the performance of legitimate users on their downstream tasks. The victim encoders was trained on the ImageNet dataset using SimSiam and DINO frameworks.