# Loss Dynamics of Temporal Difference Reinforcement Learning

 Blake Bordelon, Paul Masset, Henry Kuo & Cengiz Pehlevan

John Paulson School of Engineering and Applied Sciences,

Center for Brain Science,

Kempner Institute for the Study of Natural & Artificial Intelligence,

Harvard University

Cambridge MA, 02138

blake_bordelon@g.harvard.edu, cpehlevan@g.harvard.edu

###### Abstract

Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent dynamics. We study how learning dynamics and plateaus depend on feature structure, learning rate, discount factor, and reward function. We then analyze how strategies like learning rate annealing and reward shaping can favorably alter learning dynamics and plateaus. To conclude, our work introduces new tools to open a new direction towards developing a theory of learning dynamics in reinforcement learning.

## 1 Introduction

Reinforcement learning (RL) is a general paradigm which allows agents to learn from experience the relative value of states in their environment and to take actions that maximize long term rewards [1]. RL algorithms have been successfully applied in a number of real world scenarios such as strategic games like backgammon and Go, autonomous vehicles, and fine tuning language models [2, 3, 4, 5, 6, 7].

Despite these empirical successes, a theoretical understanding of the learning dynamics and inductive biases of RL algorithms is currently lacking [8]. A large fraction of the theoretical work has focused on proving convergence and deriving bounds both in the asymptotic [9, 10, 11, 12, 13, 14] and non-asymptotic [15, 16, 17] limits, but do not provide a full picture of the evolution of the learning dynamics.

A desired feature of a candidate theory is to characterize the influence of function approximation to RL dynamics and its performance. Early versions of RL operated in a tabular setting, similar to dynamic programming [18], where all the states in the environment could be mapped one-to-one to a specific value and policy. In large and complex environments, it is not possible to enumerate all the states in the environment necessitating the use of function approximation for the target value and policy functions. Indeed, the recent success of many RL algorithms relies on deep reinforcementlearning architectures that combine an RL architecture with deep neural networks to build effective value estimators and policy networks [19].

One difficulty in analysing these algorithms compared to supervised learning settings is that the distribution of the data received at each time-step is not stationary. This non-stationarity arises from two principal sources: First, whether in an episodic or continuous setting, states visited within a learning trajectory are dependent on the recent past. Trajectories might be randomly sampled but points within a trajectory are correlated. Second, when the policy is updated it also changes the distribution of future visited states.

Here, we will focus on the first form of non-stationarity when learning a value function in the context of _policy evaluation_[1] using a classical RL algorithm, temporal difference (TD) learning [20]. We develop a theory of learning dynamics for RL in this setting in a high dimensional asymptotic limit with a focus on understanding the role of linear function approximation from a set of nonlinear and static features. In particular, we leverage ideas from recent work in application of statistical physics to machine learning theory to perform an average over the possible sequences of features encountered during learning. Our contributions are as follows:

* We introduce concepts from statistical physics, including a path integral approach to describe dynamics [21; 22; 23; 24; 25] and the Gaussian equivalence assumption [26; 27; 28; 29], to derive a theory of learning dynamics in TD learning (SS3) in an online setting. We provide an analytical formula for the typical case learning curve for TD learning.
* We show that our theory predicts scaling of the learning convergence speed and performance plateaus with parameters of the problem including task-feature alignment [30], learning rate, discount factor or batch size (SS4 and SS5). Task-feature alignment is a metric that quantifies how features allow fast or slow learning for a given task.
* We show our theory can be used to understand and guide design principles when choosing meta-parameters. Specifically, we show that we can use our theory to infer optimal schedules of learning rate annealing and the effects of reward shaping (SS5 and SS6).

## 2 Problem Setup and Related Works

### Problem Setup

We consider a set of states denoted by \(s\), possibly continuous, and a fixed policy \(\pi\) which generates a distribution over actions given the state. The state dynamics are defined by a distribution \(p(\tau)\) over trajectories through state space \(\tau=\{s_{1},s_{2},...,s_{T}\}\). Note that state transitions do not have to be Markovian, but each trajectory is i.i.d. sampled from \(p(\tau)\). We consider trajectories of length \(T\). Each state is represented by an \(N\)-dimensional feature vector \(\bm{\psi}(s)\in\mathbb{R}^{N}\), so that trajectory generates a collection of feature vectors \(\{\bm{\psi}(s_{t})\}_{t=1}^{T}\). The rewards are generated by a reward function \(R(s)\) which depends on the state. (In general, the features and rewards can depend on action as well: transition dynamics are still fixed as the policy is fixed, but variance over rewards at a given state may need to be modeled, see Appendix B.5).

At any time, we are interested in characterizing the _value function_ associated with a state, which measures the expected discounted sum of future rewards when starting in state \(s_{0}\)

\[V(s_{0})=R(s_{0})+\sum_{t\geq 1}\mathbb{E}_{s_{t}|s_{0}}\gamma^{t}R(s_{t})=R(s_ {0})+\gamma\mathbb{E}_{s_{1}|s_{0}}V(s_{1}).\] (1)

We use linear function approximation to learn the value function \(\hat{V}(s)=\bm{\psi}(s)\cdot\bm{w}\). Similar to kernel learning [31], the features \(\bm{\psi}\) should be high dimensional so that they can express a large set of possible value functions.

We study TD learning dynamics given this setup. At each step of the TD iteration, we sample a batch of \(B\) independent trajectories from the distribution and compute the TD update

\[\bm{w}_{n+1} =\bm{w}_{n}+\frac{\eta_{n}}{TB}\sum_{\mu=1}^{B}\sum_{t=1}^{T} \Delta_{n}^{\mu}(t)\bm{\psi}(s_{n}^{\mu}(t)),\] \[\Delta_{n}^{\mu}(t) \equiv R(s_{n}^{\mu}(t))+\gamma\hat{V}(s_{n}^{\mu}(t+1))-\hat{V}(s_{n }^{\mu}(t)).\] (2)We operate in a online batch regime as the trajectories in each batch are resampled at each iteration. This is distinct from an offline setting where the batches would be resampled from a finite-sized buffer [1]. Convergence considerations for infinite-batch online TD learning width different types of features \(\bm{\psi}\) are outlined in Appendix A. The specific form for the TD-error \(\Delta_{n}^{\mu}(t)\) depends on the precise variant of TD learning that is used. Here, we will focus on TD(0) but our approach can be extended to other TD learning rules and definitions of the return function. We see that the iterates \(\bm{w}_{n}\) will form a stochastic process as each sequence of states in an episode \(\{s_{n}^{\mu}(t)\}\) are drawn randomly from \(p(\tau)\). In general, we allow the learning rate \(\eta_{n}\) to depend on iteration, an important point we will revisit later. The distribution of features \(\{\bm{\psi}(s_{n}^{\mu}(t))\}\) over random trajectories \(\tau\) is in general quite complicated, depending on the details of the state transitions and the nonlinear feature maps, which motivates the following question:

**Question: _How can the stochastic dynamics of temporal difference learning be characterized for complicated trajectory distributions \(p(\tau)\) and feature maps \(\bm{\psi}(s)\)?_**

To address this question, in this work, we provide an analysis of TD learning that explicitly models the statistics of stochastic semi-gradient updates to \(\bm{w}_{n}\). Our framework is based on a Gaussian equivalence ansatz for TD learning and high dimensional mean field theory which predicts the statistics of TD errors \(\Delta_{n}^{\mu}(t)\) and the weight iterates \(\bm{w}_{n}\). The theory reveals a rich set of phenomena including plateaus unique to SGD noise in TD learning which can be ameliorated with learning rate annealing.

### Related Works

The dynamics of TD learning have been notoriously difficult to analyse. Unlike supervised learning settings, sampled states are correlated across a trajectory and the algorithms involve bootstrapping: using estimates of the value function for future states in the temporal difference update [1]. Some prior works study the least-square TD learning rule, which solves, at each step \(n\) of the algorithm, a linear system for the instantaneous best fit to \(n\) samples [32, 33, 34]. Alternatively, many works focus on the online SGD version of TD learning, where incremental updates are made to the parameters at each step, using fresh samples. This is the setting of our work. The focus of this literature has initially been to prove convergence and bounds on asymptotic behavior [11, 12, 13, 14, 35]. More recently, progress has been made in deriving bounds in the non-asymptotic regime. Initial work assumed that data samples were _i.i.d._[15, 16, 17, 36] and recent work has extended those approaches to Markovian noise [15, 37, 38, 39]. The majority of these proofs use the ODE-like method for stochastic approximation [11, 40], which corresponds to a limit of the stochastic semi-gradient dynamics where the effects of mini-batch noise are neglected. This is also known as the "mean-path" dynamics of TD learning and will correspond to the infinite batch limit of our theory. Furthermore, many of these methods require the use of iterative averaging of the learned value function, whereas we study the final iterate convergence. The approach we take here differs from many of these results as our goal is not to provide bounds on worst-case behavior but instead to provide a full description of the dynamics of the typical case scenario during learning.

Our approach also highlights the importance of the structure of the representations in controlling the dynamics of learning. This had been long been recognized in reinforcement learning and previous works proposed to improve feature representations to improve algorithmic performance [41, 42, 43]. This line of work has shown the importance of the relative smoothness of the representations and target functions in the ODE limit of TD dynamics [43, 44]. Similarly, several methods have been proposed to empirically learn a better shaping function [45, 46]. In _policy learning_ it has also been recognized that using a gradient aligned to the statistics of the tasks, such as the natural gradient [47] can greatly speed up convergence [48]. Our work does not explore such feature learning per se but could be used as a diagnostic tool to analyse how representations impact learning speed.

We adopt the perspective of statistical physics, by working with a simplified feature distribution which captures the learning dynamics and solving the theory in a high-dimensional limit [49, 50, 51]. We derive TD reinforcement learning curves from a mean field theory formalism which is exact for infinite dimensional features and batch size. Similar calculations for supervised learning on Gaussian data have been shown to provide an accurate description of high dimensional dynamics [52, 53, 54]. Further, even when data is not actually Gaussian, several algorithms, such as kernel or random-features regression, exhibit universality in their loss behavior, enabling analysis of the learning curve with a simpler Gaussian proxy [26, 27, 28, 30, 55]. We exploit this idea in the TD learning setting to some success. We note that Gaussian equivalence or universality is not a panacea, and in many cases the Gaussian proxy can fail to capture important machine learning phenomena [27, 56, 57].

Theoretical Results for Online TD Learning

### Computation of Learning Curves

We develop a dynamical mean field theory (DMFT) formalism can be utilized to compute the learning curves. We provide the full derivation of the DMFT in Appendix B. This computation consists of tracking the moment generating function for the iterates \(\bm{w}_{n}\) over the trajectories of randomly sampled features \(\{\bm{\psi}_{\mu}^{n}(t)\}_{t=1}^{T}\). In an appropriate high dimensional asymptotic limit, the results of our theory can be summarized as the following proposition.

**Proposition 3.1**.: _Let \(N,B\to\infty\) with \(B/N=\mathcal{O}(1)\) and episode length \(T=\mathcal{O}(1)\). Let the ground truth reward function be \(R(s)=\bm{w}_{R}\cdot\bm{\psi}(s)\) and value function \(V(s)=\bm{w}_{TD}\cdot\bm{\psi}(s)\) in the basis of our features. Define matrices_

\[\bm{\bar{\Sigma}}\equiv\frac{1}{T}\sum_{t}\bm{\Sigma}(t,t),\quad\bm{\bar{ \Sigma}}_{+}\equiv\frac{1}{T}\sum_{t}\bm{\Sigma}(t,t+1),\quad\bm{A}\equiv\bm{ \bar{\Sigma}}-\gamma\bm{\bar{\Sigma}}_{+},\] (3)

_and assume that the features are such that matrix \(\bm{A}\) is of extensive rank in \(N\). Then the typical value estimation error \(\mathcal{L}_{n}=\left\langle\left(V(s)-\hat{V}_{n}(s)\right)^{2}\right\rangle_ {s}\) after \(n\) steps has the form_

\[\mathcal{L}_{n} =\frac{1}{N}\text{Tr}\bm{\bar{\Sigma}}\bm{M}_{n},\] (4) \[\bm{M}_{n+1} =(\bm{I}-\eta\bm{A})\bm{M}_{n}(\bm{I}-\eta\bm{A})^{\top}+\frac{ \eta^{2}}{\alpha^{2}T^{2}}\sum_{tt^{\prime}}Q_{n}(t,t^{\prime})\bm{\Sigma}(t,t ^{\prime})\] (5) \[Q_{n}(t,t^{\prime}) =\frac{1}{N}\left\langle(\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma }(t,t^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma}{N}\left\langle (\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma}(t,t^{\prime}+1)\bm{w}_{n}\right\rangle\] \[\quad+\frac{\gamma}{N}\left\langle\bm{w}_{n}^{\top}\bm{\Sigma}(t +1,t^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma^{2}}{N}\left \langle\bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{\prime}+1)\bm{w}_{n}\right\rangle,\] (6)

_where \(\alpha=B/N\) and \(Q_{n}(t,t^{\prime})=\left\langle\Delta_{n}(t)\Delta_{n}(t^{\prime})\right\rangle\) is the correlation of randomly sampled TD-errors at episodic times \(t,t^{\prime}\) and iteration \(n\). The average over weights \(\left\langle\right\rangle\) denotes a Gaussian average whose moments are related to \(\bm{M}_{n}\). The correlation function \(Q_{n}(t,t^{\prime})\) depends on \(\bm{M}_{n}\) and the average weights \(\left\langle\bm{w}_{n}\right\rangle\); we provide its full formula in Appendix B.3, equation (B.17)._

Proof.: The full derivation is in Appendix B. At a high level, we track the moment generating function of the iterates \(\bm{w}_{n}\) over random draws of features \(\{\bm{\psi}_{n}^{\mu}(t)\}\), \(Z[\{\bm{j}_{n}\}]=\mathbb{E}_{\{\bm{\psi}_{n}^{\mu}(t)\}}\exp\left(i\sum_{n} \bm{j}_{n}\cdot\bm{w}_{n}\right)\propto\int\mathcal{D}q\exp\left(\frac{N}{2}S[ q,\{\bm{j}_{n}\}]\right)\) where \(S\) is a \(\mathcal{O}(1)\) action and \(q\) are a set of order parameters of the theory which include the following overlaps \(C_{n}(t,t^{\prime})=\frac{1}{N}\bm{w}_{n}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{ w}_{n}\) and \(Q_{n}(t,t^{\prime})=\frac{1}{B}\sum_{\mu=1}^{B}\Delta_{n}^{\mu}(t)\Delta_{n}^{\mu}( t^{\prime})\). In this high dimension \(N,B\to\infty\) limit with \(B/N=\mathcal{O}(1)\) and episode length \(T=\mathcal{O}(1)\), the order parameters can be obtained from saddle point integration, which requires solving \(\frac{\partial S}{\partial q}=0\). This procedure results in a deterministic learning curve given in equations (4),(5),(6) even though the realization of sampled states are disordered. The TD-error variables \(\Delta_{n}(t)\) become mean zero Gaussians and the \(\{\bm{w}_{n}\}\) also follow a Gaussian distribution with mean and variance determined by the order parameters. 

Before we explore the predictions of this theory, we first make a few remarks about this result.

_Remark 1_.: Though the theory is technically derived for large batch size \(B\), we will show that it provides an accurate description of the loss trajectory even for batches as small as \(B=1\). An alternative formulation in terms of recursive averaging reveals transparently which approximations lead to the same result as the mean field theory (Appendix B.6).

_Remark 2_.: The case where the reward function and/or the value function are inexpressible by the features \(\bm{\psi}\) can also be handled within this framework. In this case, the unlearnable components of the value function act as additional noise which limits performance [29]. These can also be handled by our theory, see Appendix A.

_Remark 3_.: The limit where \(\gamma=0\) recovers known results in online supervised learning with stochastic gradient methods [29, 58, 59]. In this limit, the dynamics will converge to zero loss provided the model features are sufficiently rich to represent the true value function.

_Remark 4_.: The TD learner with perfect coverage (infinite batch size) at each step will converge to the ground truth \(\bm{w}_{TD}=\left(\bm{\bar{\Sigma}}-\gamma\bm{\bar{\Sigma}}_{+}\right)^{-1}\bm{ \bar{\Sigma}}\bm{w}_{R}\) (see Appendix A).

_Remark 5_.: \(\bm{M}_{n}\) is equivalently defined as \(\bm{M}_{n}=\left\langle(\bm{w}-\bm{w}_{TD})(\bm{w}-\bm{w}_{TD})^{\top}\right\rangle _{\{\bm{\tau}_{n^{\prime}}^{\mu}\}_{n^{\prime}<n}}\), which measures deviation from the fixed point of gradient flow (vanishing learning rate) dynamics \(\bm{w}_{TD}\) over random sets of sampled episodes (Appendix B).

### Gaussian Approximation

The theory presented in Section 3.1 relies on an approximation of the feature distribution as Gaussian. Similar approximations have been successfully utilized in high dimensional regression problems even when the true features are non-Gaussian [26, 27, 28, 29]. We note that an exact, non-asymptotic theory for non-Gaussian features can be provided which closes under knowledge of the fourth cumulants of the features as we show in Appendix D, though this theory is especially cumbersome to analyze or evaluate compared to the theory of Section 3.1. Concretely, Proposition 3.1 relies on the following.

**Gaussian Feature Assumption**.: _The learning curves for a TD learner with high dimensional features \(\{\bm{\psi}(s_{t})\}_{t=1}^{T}\) over random \(\tau\) are well approximated by the learning curves of a TD learner trained with Gaussian features \(\bm{\psi}_{G}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma}+\bm{\mu\mu}^{\top})\) with matching mean and correlations_

\[\bm{\mu}(t)=\left\langle\bm{\psi}(s_{t})\right\rangle_{\tau\sim p(\tau)}\,\quad\bm{\Sigma}(t,t^{\prime})=\left\langle\bm{\psi}(s_{t})\bm{\psi}(s_{t^{ \prime}})^{\top}\right\rangle_{\tau\sim p(\tau)}.\] (7)

_where averages are taken over sequences of states \(\{s(t)\}\sim p(\tau)\)._

One interpretation of this ansatz is that the dependence of the learning curve on higher order cumulants of the features is negligible in high dimensional feature spaces under the square loss. This

Figure 1: An illustration of our theory for TD learning. (a) A diffusion process in a 2D grid world generates many possible trajectories through state space. Each colored line is a different trajectory. Reward function is shown in red, with darker red indicating higher reward. (b) When combined with nonlinear place cell feature representation, the state transitions generate a distribution over observed features \(\{\bm{\psi}(s_{t})\}\). (c) The value error associated with TD learning for a bump reward function on the true features generated from a single set of MDP trajectories (blue) is compared to training on sampled Gaussian vectors \(\{\bm{\psi}_{t}\}\) with matching within-episode covariance structure. These single runs of TD learning on either set of features are consistent with the typical case theory (black dashed). (d) The structure of the features alters learning dynamics. We consider, for simplicity, altering the bandwidth (BW) of the place cell features. (e) Varying place cell BW changes the dynamics for both large batch (\(B=30\)) and (f) small batch (\(B=3\)) TD learning. There is an optimal BW for a given step size. Small batch stochastic semi-gradient noise is more severe.

approximation has been shown to provide an accurate description on realistic supervised learning settings with non-Gaussian data with the square loss in prior works [26; 27; 29; 30; 55; 58]. As shown in these works, for standard supervised learning, even highly non-Gaussian features \(\{\bm{\psi}(s_{t})\}\) have least squares learning curves which are only sensitive to the first two cumulants of the distribution. We do not aim to provide a rigorous proof of this ansatz for TD learning but instead compute the learning curve implied by this assumption and compare to experiments on simple Markov Decision Processes (MDPs). The benefit of this hypothesis in the RL setting is that it abstracts away details of transitions in the state space and instead deals with the correlations of sampled features through time.

To illustrate an example of the Gaussian Equivalence idea, in Figure 1, we consider an MDP which is defined by diffusion through a 2-dimensional (2D) state space (Figure 1(a)). We choose the features \(\bm{\psi}(s)\) to be a collection of localized 2D Radial Basis Function (RBF) bumps which tile the 2D space, similarly to the "place cell" neurons found in the mammalian hippocampus [60; 61] (Figure 1(b)). The feature map is parameterized by the bandwidth of individual "place cells". In Figure 1(c), we show the value error learning curve as a function of the number of steps \(n\) (blue) and compare the value estimation error of the MDP with a Gaussian distribution for \(\bm{\psi}(t)\) with matching first and second moments (orange). Lastly, we plot the theoretical prediction of our theory (described in Section 3), which is computed under the Gaussian equivalence ansatz (black dashed). We see a remarkable match of the three curves. The equivalence can be used to predict the speed of TD learning for different features, such as place cells with varying bandwidth as we illustrated in Figure 1 (d)-(f). In Figure 1 (e) and (f), we plot the loss trajectories for a single run of TD for each feature set. We observe that bandwidth affects both the learning dynamics and the asymptotic error with an optimal bandwidth at any step. One of our goals will be to elucidate the role of feature quality in learning dynamics. While the large batch dynamics are approximately self-averaging, as shown by the fact that single runs of TD learning coincide with our theoretical typical case theory curves, there is significant semi-gradient variance in the value error at small batch sizes. While we expect Gaussian equivalence to hold for high dimensional features, in low dimensions non-Gaussian effects can significantly alter the learning curves as we show in Appendix D.1. However, for high dimensional features, the equivalence holds for many other feature distributions such as polynomial and fourier features (Appendix E).

## 4 Spectral Perspective on Hard Reward Functions

Our theory can provide some insights into the structure of tasks which can be learned easily and which require more sampled trajectories to estimate based on spectral decompositions of the feature covariances. We note that similar spectral arguments have been given in the ODE-limit [44] and are intimately related to the source conditions used in recent work to identify power-law rates in the large batch regime [39].

To build our argument, we diagonalize the matrix \(\bm{A}=\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+}\), obtaining \(\bm{A}\bm{u}_{k}=\lambda_{k}\bm{u}_{k}\), noting that eigenvalues \(\lambda_{k}\) can be complex. We then expand the TD solution in this basis \(\bm{w}_{TD}=\sum_{k}w_{k}\bm{u}_{k}\).

Figure 2: Reward functions and dynamics which lead to value functions with high spectral alignment to the features can be learned more quickly than those that do not. (a) A sparse and dense reward function in a 2D spatial navigation task can illustrate this effect. (b) The cumulative power distribution \(C(k)\) defined from the spectral decomposition of \(\bm{A}=\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+}\). Concretely we let \(\bm{A}\bm{u}_{k}=\lambda_{k}\bm{u}_{k}\) with \(\lambda_{k}\) ordered by real part and \(\bm{w}_{TD}=\sum_{k}w_{k}\bm{u}_{k}\). In the \(B\to\infty\) limit the task which has rapidly rising \(C(k)=\frac{\sum_{\ell<k}w_{\ell}^{2}}{\sum_{\ell}w_{\ell}^{2}}\) will converge more quickly than the task with slowly rising \(C(k)\). (c) Indeed, for large batch regime (\(B=20\)) the value error decreases more rapidly for \(R_{2}\) than for \(R_{1}\).

The theory predicts that, the average learned weights will be \(\langle\bm{w}_{n}\rangle=\sum_{k}|1-\eta\lambda_{k}|^{n}e^{i\theta_{k}n}w_{k}\bm{ u}_{k}\), where \(|\cdot|\) is complex modulus and \(\theta_{k}=\text{Arg}(1-\eta\lambda_{k})\). We can therefore order the modes by their convergence timescales \(|1-\eta\lambda_{k}|\). Given this ordering of timescales, we can order the modes \(k\) from those with smallest to largest timescales. Given this ordering, we see that tasks can be learned efficiently are those with most of the norm of \(\bm{w}_{k}\) in the modes with small timescales. We quantify how well aligned a task is to a given feature representation by computing a cumulative power distribution for the target weights \(C(k)=\frac{\sum_{\ell<k}w_{\ell}^{2}}{\sum_{\ell}w_{\ell}^{2}}\). If this quantity rises rapidly with \(k\) then the task can be learned from a small number of samples [30].

We consider again, the setting of Figure 1, the 2D exploration MDP but now contrast two different reward functions. In Figure 2 we show that this spectral decomposition can account for the gaps in loss for a place cell code in learning a sparse or dense reward function (Figure 2(a)). As expected the cumulative power rises more rapidly for the dense reward function \(R_{2}(s)\) (Figure 2(b)). As a consequence, the value error converges to zero more rapidly than for the sparse rewards.

## 5 Stochastic Semi-Gradient Learning Plateaus and Annealing Strategies

The stochastic noise from TD learning has striking qualitative differences from SGD noise in the standard supervised case. In standard supervised learning (such as \(\gamma=0\) version of this theory), the stochastic gradient noise does not prevent the model from fitting the target function with zero error provided the features are sufficiently rich to represent the target function. However, this is not the case in TD learning, where the predicted value \(\hat{V}(s)\) is bootstrapped using the model's weights \(\bm{w}_{n}\) at each iteration \(n\). This leads to asymptotic plateaus in learning curves. Our theory can predict these plateaus and their scaling whose proof is given in Appendix B.7.

**Proposition 5.1**.: _Our theoretical learning curves exhibit a fixed point for the value error dynamics for finite \(B\) and non-zero \(\eta\) and \(\gamma\). For small \(\frac{\eta\gamma^{2}}{B}\), we deduce that \(\bm{M}\) satisfies a self-consistent

Figure 3: Finite batch size, discount factor and learning rate all contribute to a stochastic semi-gradient plateau in the TD dynamics. The features are generated from a synthetic power law covariance with exponential temporal autocorrelation (see Appendix G). Dashed black lines are theory. In general, for fixed learning rate \(\eta\), the plateau scales as \(\mathcal{O}(\eta\gamma^{2}B^{-1})\). (a) Larger batch sizes \(B\) reduce SGD noise and leads to a lower plateau in the reducible value error for a decoupled power-law feature model. (b) Larger discount factor \(\gamma\) and (c) larger learning rate \(\eta\) lead to higher SGD plateau floor. (d) An annealing strategy \(\eta_{n}\sim\eta_{0}n^{-\chi}\) for \(\chi>0\) can allow one to avoid the plateau. For slow annealing (small \(\chi\)), the error scales as \(\mathcal{L}_{n}\sim\mathcal{O}(n^{-\chi})\). (e) The value error as a function of the learning rate annealing exponent \(\chi\) defined by \(\eta_{n}=\eta_{0}n^{-\chi}\). For this task, the optimal exponent balances the scale of the asymptote with the rate of convergence.

asymptotic scaling of the form \(\bm{M}=\mathcal{O}\left(\frac{\eta\gamma^{2}}{B}\right)\) implying an asymptotic value error scaling of \(\mathcal{L}\sim\frac{1}{N}\text{Tr}\bm{M}\bm{\bar{\Sigma}}\sim\mathcal{O}\left( \frac{\eta\gamma^{2}}{B}\right)\)._

In Figure 3, we demonstrate that our theory predicts the plateaus and their scaling as a function of finite batch size \(B\) (Figure 3(a)), non-zero discount factor \(\gamma>0\) (Figure 3(b)) and non-negligible learning rate (Figure 3(c)).

A strategy used in the literature to increase rates of convergence and improve asymptotic behavior is adaptation of the learning learning through an annealing schedule [1; 62; 63; 16]. To overcome this plateau in the loss, we consider annealing the learning rate \(\eta_{n}\) with iteration \(n\). In Figure 3(d), we show the effect of annealing the learning rate as a power law \(\eta_{n}=\eta_{0}n^{-\chi}\) for some non-negative exponent \(\chi\). For \(\chi=0\) the learning rate is constant and a fixed plateau is reached. For small nonzero \(\chi\), such as \(\chi=0.2\), the value error is, after an initial transient, always near its instantaneous fixed point plateau so the loss scales linearly with the learning rate, giving the asymptotic rate \(\mathcal{L}_{n}\sim\mathcal{O}(n^{-\chi})\). For large \(\chi\), the learning rate decreases very quickly and the plateau is never reached. Our approach can be used to find an optimal annealing exponent \(\chi\) and in Figure 3(e), we show that the optimal annealing exponent balances these effects and is well predicted by our theory.

## 6 Reward Shaping

Another strategy to improve the learning dynamics in reinforcement learning algorithms is reward shaping [64]. In standard supervised learning, the goal is to directly approximate the target objective given a cost function. However, in reinforcement learning, the objective is not to estimate rewards at each state directly but the discounted sum of future rewards, the value function. Importantly, many different reward schedules can lead to identical value functions. Reward shaping exploits this symmetry to speed up learning by altering the structure of TD updates and SGD noise. Here, we provide a theoretical description of the changes in the learning dynamics due to reward shaping which suggests they can be understood through a change of the alignment between the original rewards and the reshaped rewards in the space of the features used to represent the states.

The original ideas around reward shaping were inspired by work in experimental psychology and were closer to what is now studied as curriculum learning [65; 66; 67]. Reward shaping as currently used in reinforcement learning directly changes the reward function by adding a potential-based shaping function \(F\) such that \(F(s_{t},a,s_{t+1})=\gamma\phi(s_{t+1})-\phi(s_{t})\)[64]. In each step of the algorithm we feed

Figure 4: The theory can be used to understand how reward shaping decisions alter temporal difference learning dynamics. (a) A visualization of possible reward shaping potentials \(\phi(s)=\bm{w}_{\phi}\cdot\bm{\psi}(s)\) strategies in feature space. Probability density level curves for the features are depicted in blue. Reshaping with \(\bm{w}_{\phi}=\beta\bm{w}_{TD}\) for scale factor \(\beta\) merely changes the scale of weights which must be recovered (gold) and does not change timescales of TD dynamics. (b) The value error dynamics for the scale based reward shaping for the features in Figure 3. On the other hand, rotation based reward shaping where \(\bm{w}_{\phi}\) is not parallel to \(\bm{w}_{V}\) (red) leads to a potentially helpful mixture of timescales if the new target vector is more aligned with feature dimensions with high variance (purple). In (c), we plot loss curves for rotation angle \(\theta\) between the original mode \(\bm{w}_{V}\) and the top eigenvector of the feature covariance matrix \(\bm{\bar{\Sigma}}\). Dashed black lines are theory.

the following _reshaped rewards_\(\tilde{R}\) to the TD learner

\[\tilde{R}(s_{t})=\begin{cases}R(s_{t})-\gamma\phi(s_{t+1})&t=0\\ R(s_{t})+\phi(s_{t})-\gamma\phi(s_{t+1})&t>0\end{cases}.\] (8)

We note that this transformation simply offsets the target value function by \(\phi(s)\) as the series above telescopes with a cancellation of \(\phi(s_{t})\) between the \(t-1\) and \(t\)-th terms [64] (see Appendix C). However, the dynamics of TD learning with these reshaped rewards \(\tilde{R}\) is quite distinct from the dynamics with original rewards \(R\). Here, we study the case where we can express \(\phi(s)\) as a linear function of our features: \(\phi(s)=\bm{\psi}(s)\cdot\bm{w}_{\phi}\). This leads to a change in the dynamics for \(\bm{M}_{n}\) and \(\langle\bm{w}_{n}\rangle\) that we describe in the Appendix C.

In Figure 4, we illustrate the possible benefits of reward shaping. We explore two types of reward shaping. First, a scale based reward shaping where \(\bm{w}_{\phi}\) is parallel to the target TD weights \(\bm{w}_{TD}\). This merely changes the overall scale of the weights needed to converge in the dynamics, leading to similar timescales and an identical plateau for TD learning as we show in Figure 4 (b). On the other hand, reward shaping which rotates the fixed point of the TD dynamics into directions of higher feature variance can improve timescales of convergence. In Figure 4 (c), we show an example where we vary the angle \(\theta\) of the shaped-TD fixed point (see also Appendix C).

## 7 TD Learning Plateaus in More Realistic Settings

In this section, we test if some of the phenomena observed in our theory and experiments also hold in more realistic settings. We perform TD learning with Fourier features to evaluate a pre-trained policy on MountainCar-v0. As expected, we see that the value error plateaus to an error level determined by both the learning rate (Figure 4(a)) and batch size (Figure 4(b)) due to semigradient noise.

We show that the plateaus obey the predicted scalings of \(\mathcal{O}(\eta B^{-1})\) in Appendix F.

## 8 Discussion

Our work presents a new approach using concepts from statistical physics to derive average-case learning curve for _policy evaluation_ in TD-learning. However, it is only a first step towards a new theory of learning dynamics in reinforcement learning.

One major limitation of the present work is that it concerns linear function approximation where the features representing states/actions are fixed throughout learning. This limit can apply to neural networks in the "lazy" regime of training [68; 69], however it cannot account for neural networks that adapt their internal representations to the structure of the reward function. This differs from the setting of most practical algorithms, including in deep reinforcement learning, that specifically adapt their representations.

Figure 5: Policy evaluation in MountainCar-v0 environment. The policy was learned with tabular \(\epsilon\)-greedy Q-learning (see Appendix F for details). (a) Value error curves for different \(\eta\) when \(B=1\). (b) Value error curves for different \(B\) with \(\eta=0.1\). Shaded area denotes 95% confidence interval over 10 seeds.

Our theory provides a description of learning dynamics through a set of iterative equations (Proposition 3.1). In Figure 1 we evaluate these dynamics for a simple MDP but although the predicted dynamics present an excellent fit to the empirical simulations, the iterative equations can be difficult to interpret and computationally expensive to evaluate in a larger network and more realistic tasks. Nevertheless, our equations can be used to derive some scaling between key parameters of the algorithm for example by studying their fixed points as in Proposition 5.1.

Here, we considered the simplest form of temporal difference learning, batched online TD(0). In future work, it will be important to further characterize the behavior for online TD(0) with batch size \(B=1\) and to expand our approach to TD(\(\lambda\)) and other return distributions. Similarly, expanding our theory to the offline setting, in which the buffer of resampled trajectories would be of finite size, could provide an understanding of how the interactions between parameters govern convergence and divergence [1, 70, 71, 72].

Another limitation of our work is that we only considered the setting of _policy evaluation_ with a fixed policy. The goal of an RL agent is to learn how to act in the work and not merely to represent the value is its states. Unlike in supervised learning, the changes in the value function affect the policy but in many of RL algorithms, for example in _actor-critic_ architecture, there is a separation of the _policy evaluation_ (critic) and the _policy learning_ (actor) [73, 74]. Such algorithms estimate the value associated with state/action pairs under a given policy and then use this information to make beneficial updates to the policy, usually with the value and policy functions approximated by separate neural networks. In this paper, we only treated the first part of this process. Recently, a related approach has been used to analyse the dynamics of _policy learning_ in an "RL perceptron" setup [75]. A full theory of reinforcement learning combining _policy evaluation_ and _policy learning_ remains difficult due to the interaction between the two processes, but combining these approaches would be fruitful. One promising direction is in settings where the timescales of the two processes are different [76], such as when _policy learning_ occurring at a much slower rate which is often the case in practice.

Beyond developing a theory of learning dynamics in reinforcement learning, the approach could be used in neuroscience to understand how neural representation of space or value can shape the learning dynamics at the behavioral level. Ideas from reinforcement learning have been extremely influential to understand phenomena observed in neuroscience and have been mapped directly onto specific brain circuits [77, 78, 79]. The place cells of the hippocampus [60] exhibit localized tuning as the example in Figure 1 and together with grid cells in enthorinal cortex are thought to be crucial for navigation in spatial and cognitive spaces and their tuning is shaped by experience [79, 80, 61, 81]. Our theory specifically link the structure of representations, policy and reward to learning rates, which can all be experimentally measured simultaneously and could shed on light on how the spectral properties of representations govern learning and navigation [79, 82], similarly to how the mean field theories we have used here can explain learning of sensory features [83]. Future work could straightforwardly extend this DMFT formalism to deal with replay of sampled experiences during TD learning [84] at the cost of tracking correlations of weight updates across iterations of the algorithm [52].

To summarize, our work provide a new promising direction towards a theory of learning dynamics in reinforcement learning in artificial and biological agents.

## Acknowledgments and Disclosure of Funding

BB is supported by a Google PhD Fellowship. CP and BB were supported by NSF grant DMS-2134157. CP is further supported by NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. PM was supported by NIH grant 5R01DC017311 to Venkatesh Murthy and Naoshige Uchida. HK was supported by the Harvard College Research Program. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. We thank Jacob Zavatone-Veth for useful discussions and comments on this manuscript.

## References

* (1) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* (2) Gerald Tesauro. Temporal difference learning and td-gammon. _Communications of the ACM_, 38(3):58-69, 1995.
* (3) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* (4) David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* (5) Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. _Nature_, 602(7896):223-228, 2022.
* (6) B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2021.
* (7) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.
* (8) Matteo Hessel, Hado van Hasselt, Joseph Modayil, and David Silver. On inductive biases in deep reinforcement learning. _arXiv preprint arXiv:1907.02908_, 2019.
* (9) Peter Dayan. The convergence of td () for general. _Machine learning_, 8(3):341-362, 1992.
* (10) Christopher JCH Watkins and Peter Dayan. Q-learning. _Machine learning_, 8:279-292, 1992.
* (11) JN Tsitsiklis and B Vanroy. An analysis of temporal-difference learning with function approximation. _IEEE Transactions on Automatic Control_, 42(5):674-690, 1997.
* (12) Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region. _Advances in neural information processing systems_, 13, 2000.
* (13) Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Machine learning_, 49:209-232, 2002.
* (14) Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* (15) Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation, 2018. URL https://arxiv.org/abs/1806.02450.
* (16) Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with function approximation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* (17) Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In Amos Storkey and Fernando Perez-Cruz, editors, _Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics_, volume 84 of _Proceedings of Machine Learning Research_, pages 1347-1355. PMLR, 09-11 Apr 2018. URL https://proceedings.mlr.press/v84/lakshminarayanan18a.html.
* (18) Richard E Bellman. _Dynamic programming_. Princeton university press, 2010.

* Arulkumaran et al. [2017] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey. _IEEE Signal Processing Magazine_, 34(6):26-38, 2017.
* Sutton [1984] Richard Stuart Sutton. _Temporal credit assignment in reinforcement learning_. University of Massachusetts Amherst, 1984.
* Martin et al. [1973] Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. _Physical Review A_, 8(1):423, 1973.
* Crisanti and Sompolinsky [2018] A Crisanti and H Sompolinsky. Path integral approach to random neural networks. _Physical Review E_, 98(6):062120, 2018.
* Helias and Dahmen [2020] Moritz Helias and David Dahmen. _Statistical field theory for neural networks_, volume 970. Springer, 2020.
* Bordelon and Pehlevan [2022] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. _arXiv preprint arXiv:2205.09653_, 2022.
* Bordelon and Pehlevan [2022] Blake Bordelon and Cengiz Pehlevan. The influence of learning rule on representation dynamics in wide neural networks. _arXiv preprint arXiv:2210.02157_, 2022.
* Bordelon et al. [2020] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In _International Conference on Machine Learning_, pages 1024-1034. PMLR, 2020.
* Loureiro et al. [2021] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. _Advances in Neural Information Processing Systems_, 34:18137-18151, 2021.
* Hu and Lu [2022] Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 2022.
* Bordelon and Pehlevan [2022] Blake Bordelon and Cengiz Pehlevan. Learning curves for SGD on structured features. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=WPI2vbkAl3Q.
* Canatar et al. [2021] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. _Nature communications_, 12(1):2914, 2021.
* Scholkopf et al. [2002] Bernhard Scholkopf, Alexander J Smola, Francis Bach, et al. _Learning with kernels: support vector machines, regularization, optimization, and beyond_. MIT press, 2002.
* Tagorti and Scherrer [2015] Manel Tagorti and Bruno Scherrer. On the rate of convergence and error bounds for lstd (\(\lambda\)). In _International Conference on Machine Learning_, pages 1521-1529. PMLR, 2015.
* Pan et al. [2017] Yangchen Pan, Adam White, and Martha White. Accelerated gradient temporal difference learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* Geramifard et al. [2006] Alborz Geramifard, Michael Bowling, Martin Zinkevich, and Richard S Sutton. lstd: Eligibility traces and convergence analysis. _Advances in Neural Information Processing Systems_, 19, 2006.
* Pineda [1997] Fernando J Pineda. Mean-field theory for batched td (\(\lambda\)). _Neural computation_, 9(7):1403-1419, 1997.
* Patil et al. [2023] Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. In _International Conference on Artificial Intelligence and Statistics_, pages 5438-5448. PMLR, 2023.
* Srikant and Ying [2019] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In _Conference on Learning Theory_, pages 2803-2830. PMLR, 2019.

* [38] LA Prashanth, Nathaniel Korda, and Remi Munos. Concentration bounds for temporal difference learning with linear function approximation: the case of batch data and uniform sampling. _Machine Learning_, 110:559-618, 2021.
* [39] Eloise Berthier, Ziad Kobeissi, and Francis Bach. A non-asymptotic analysis of non-parametric temporal-difference learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 7599-7613. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/32246544c237164c365c0527b677a79a-Paper-Conference.pdf.
* [40] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and reinforcement learning. _SIAM Journal on Control and Optimization_, 38(2):447-469, 2000.
* [41] Ishai Menache, Shie Mannor, and Nahum Shimkin. Basis function adaptation in temporal difference reinforcement learning. _Annals of Operations Research_, 134(1):215-238, 2005.
* [42] Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. _Journal of Machine Learning Research_, 8(10), 2007.
* [43] Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal representations for reinforcement learning. _Advances in neural information processing systems_, 32, 2019.
* [44] Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning dynamics and generalization in deep reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14560-14581. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/lyle22a.html.
* [45] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping. _Advances in Neural Information Processing Systems_, 33:15931-15941, 2020.
* [46] Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu. Learning task-distribution reward shaping with meta-learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11210-11218, 2021.
* [47] Shun-Ichi Amari. Natural gradient works efficiently in learning. _Neural computation_, 10(2):251-276, 1998.
* [48] Sham M Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* [49] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from examples. _Physical review A_, 45(8):6056, 1992.
* [50] Andreas Engel and Christian Van den Broeck. _Statistical mechanics of learning_. Cambridge University Press, 2001.
* [51] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. _Annual Review of Condensed Matter Physics_, 11:501-528, 2020.
* [52] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova. Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification. _Advances in Neural Information Processing Systems_, 33:9540-9550, 2020.
* [53] Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborova. Rigorous dynamical mean field theory for stochastic gradient descent methods. _arXiv preprint arXiv:2210.06591_, 2022.

* Celentano et al. [2021] Michael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of first order methods with random data. _arXiv preprint arXiv:2112.07572_, 2021.
* Simon et al. [2023] James B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese. The eigen-learning framework: A conservation law perspective on kernel ridge regression and wide neural networks. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=FDbQGCAViI.
* Refinetti et al. [2023] Maria Refinetti, Alessandro Ingrosso, and Sebastian Goldt. Neural networks trained with sgd learn distributions of increasing complexity. In _International Conference on Machine Learning_, pages 28843-28863. PMLR, 2023.
* Ingrosso and Goldt [2022] Alessandro Ingrosso and Sebastian Goldt. Data-driven emergence of convolutional structure in neural networks. _Proceedings of the National Academy of Sciences_, 119(40):e2201854119, 2022.
* Varre et al. [2021] Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd for least-squares in the interpolation regime. _Advances in Neural Information Processing Systems_, 34:21581-21591, 2021.
* Velikanov et al. [2023] Maksim Velikanov, Denis Kuzmedelev, and Dmitry Yarotsky. A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=bzaPGEllsjE.
* O'Keefe [1976] John O'Keefe. Place units in the hippocampus of the freely moving rat. _Experimental neurology_, 51(1):78-109, 1976.
* Moser et al. [2008] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain's spatial representation system. _Annu. Rev. Neurosci._, 31:69-89, 2008.
* Jacobs [1988] Robert A Jacobs. Increased rates of convergence through learning rate adaptation. _Neural networks_, 1(4):295-307, 1988.
* Dabney and Barto [2012] William Dabney and Andrew Barto. Adaptive step-size for online temporal difference learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 26, pages 872-878, 2012.
* Ng et al. [1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In _Proceedings of the Sixteenth International Conference on Machine Learning_, pages 278-287, 1999.
* Skinner [1965] Burrhus Frederic Skinner. _Science and human behavior_. Number 92904. Simon and Schuster, 1965.
* Gullapalli and Barto [1992] Vijaykumar Gullapalli and Andrew G Barto. Shaping as a method for accelerating reinforcement learning. In _Proceedings of the 1992 IEEE international symposium on intelligent control_, pages 554-559. IEEE, 1992.
* Bengio et al. [2009] Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 41-48, 2009.
* Chizat et al. [2019] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31, pages 8571-8580. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
* Van Hasselt et al. [2018] Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. _arXiv preprint arXiv:1812.02648_, 2018.

* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Perdomo et al. [2023] Juan Perdomo, Akshay Krishnamurthy, Peter L Bartlett, and Sham Kakade. A sharp characterization of linear estimators for offline policy evaluation. _Journal of machine learning research_, 2023.
* Konda and Tsitsiklis [1999] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. _Advances in neural information processing systems_, 12, 1999.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937. PMLR, 2016.
* Patel et al. [2023] Nishil Patel, Sebastian Lee, Stefano Sarao Mannelli, Sebastian Goldt, and Andrew M Saxe. The rl perceptron: Dynamics of policy learning in high dimensions. In _ICLR 2023 Workshop on Physics for Machine Learning_, 2023.
* Konda and Tsitsiklis [2004] Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic approximation. _Annals of Applied Probability_, pages 796-819, 2004.
* Schultz et al. [1997] Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward. _Science_, 275(5306):1593-1599, 1997.
* Doya [2008] Kenji Doya. Modulators of decision making. _Nature neuroscience_, 11(4):410-416, 2008.
* Behrens et al. [2018] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowledge for flexible behavior. _Neuron_, 100(2):490-509, 2018.
* Stachenfeld et al. [2017] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as a predictive map. _Nature neuroscience_, 20(11):1643-1653, 2017.
* Sosa and Giocomo [2021] Marielena Sosa and Lisa M Giocomo. Navigating for reward. _Nature Reviews Neuroscience_, 22(8):472-487, 2021.
* McNamee et al. [2021] Daniel C McNamee, Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. Flexible modulation of sequence generation in the entorhinal-hippocampal system. _Nature neuroscience_, 24(6):851-862, 2021.
* Bordelon and Pehlevan [2022] Blake Bordelon and Cengiz Pehlevan. Population codes enable learning from few examples by shaping inductive bias. _Elife_, 11:e78606, 2022.
* Fedus et al. [2020] William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, and Will Dabney. Revisiting fundamentals of experience replay. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 3061-3071. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/fedus20a.html.

## Appendix A General Convergence Considerations for MDPs in Finite State Space

In this section, we will discuss the infinite batch limit and compare the value function obtained with TD to the ground truth value function. We will, for simplicity, consider in this section a Markov reward process with transition matrix \(p(s_{t+1}=s^{\prime}|s_{t}=s)=\Pi(s,s^{\prime})\). The general theory described in the main text does not only apply to MDPs, but the convergence analysis for MDPs is much more straightforward so we describe it here. In this case, the ground truth value function satisfies

\[V(s)=R(s)+\gamma\sum_{s^{\prime}}\Pi(s,s^{\prime})V(s^{\prime})\] (A.1)

which gives the vector equation \(\bm{V}=\left(\bm{I}-\gamma\bm{\Pi}\right)^{-1}\bm{R}\) for \(\bm{V},\bm{R}\in\mathbb{R}^{|\mathcal{S}|}\). Suppose the limiting distribution over states is \(\bm{p}\in\mathbb{R}^{|\mathcal{S}|}\) which has entries \(p(s)=\frac{1}{T}\sum_{t=1}^{T}p(s_{t}=s)\). The fixed point of TD dynamics is

\[\bm{\Psi}\text{diag}(\bm{p})\bm{\Psi}^{\top}\bm{w}_{TD}=\bm{\Psi}\text{diag}( \bm{p})\bm{R}+\gamma\bm{\Psi}\text{diag}(\bm{p})\bm{\Pi}\bm{\Psi}^{\top}\bm{w} _{TD}.\] (A.2)

We now consider the two possible cases for this fixed point condition.

Case 1: Underparameterized RegimeFirst, if the feature dimension \(N\) is smaller than the size of the state space \(|\mathcal{S}|\) and the features are maximal rank, then the TD learning fixed point is

\[\bm{w}_{TD}=\left(\bm{\Psi}\text{diag}(\bm{p})\bm{\Psi}^{\top}-\gamma\bm{\Psi }\text{diag}(\bm{p})\bm{\Pi}\bm{\Psi}^{\top}\right)^{-1}\bm{\Psi}\text{diag}( \bm{p})\bm{R}\] (A.3)

In this case, the value function is not learned perfectly, as can be seen by computing \(\hat{\bm{V}}=\bm{\Psi}^{\top}\bm{w}_{TD}\) and comparing to the ground truth \(\bm{V}=\left(\bm{I}-\gamma\bm{\Pi}\right)^{-1}\bm{R}\). In this case, we would say that TD learning has an _irreducible value error_ due to capturing only a \(N\) dimensional projection of the value function.

Case 2: Overparameterized RegimeAlternatively, if the feature dimension exceeds the total number of states, then the fixed point equation for TD is underspecified. However, throughout TD learning \(\bm{w}_{TD}\in\text{span}\{\bm{\psi}(s)\}_{s\in\mathcal{S}}\) so we can instead consider the decompostion \(\bm{w}_{V}=\sum_{s}\alpha(s)\bm{\psi}(s)\), where \(\bm{\alpha}\in\mathbb{R}^{|\mathcal{S}|}\) satisfies

\[\text{diag}(\bm{p})(\bm{I}-\gamma\bm{\Pi})\bm{K}\bm{\alpha}=\text{diag}(\bm{p })\bm{R}\] (A.4)

where \(\bm{K}\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) is the kernel computed with features \(K(s,s^{\prime})=\bm{\psi}(s)\cdot\bm{\psi}(s^{\prime})\). The solution to the above equation is unique and the learned value function \(\hat{\bm{V}}=\bm{\Psi}^{\top}\bm{w}_{TD}=\bm{K}\bm{K}^{-1}\left(\bm{I}-\gamma \bm{\Pi}\right)^{-1}\bm{R}=\left(\bm{I}-\gamma\bm{\Pi}\right)^{-1}\bm{R}=\bm{V}\). Therefore, in the over-parameterized limit, the irreducible value error for TD learning is zero. This limit was considered dynamically in the infinite batch (vanishing SGD noise) setting by [44].

## Appendix B Derivation of Learning Curves

In this section, we now consider the dynamics of TD learning when \(B\) random episodes are sampled at a time. In this calculation, the finite batch of episodes leads to non-negligible SGD effects which can cause undesirable plateaus in TD dynamics.

### Field Theory Derivation

In this section we use a Gaussian field theory formalism to compute the learning curve in the high dimensional asymptotic limit \(N,B\rightarrow\infty\) with \(B/N=\alpha\). The episode length \(T\) is treated as \(\mathcal{O}(1)\). While this paper focuses on the online setting, where fresh trajectories \(\{\tau_{n}^{\mu}\}\) are sampled at each iteration \(n\), this model can be straightforwardly extended to the case where a fixed number of experience trajectories \(\{\tau^{\mu}\}\) are replayed repeatedly during TD learning. We leave the experience

[MISSING_PAGE_FAIL:17]

For each of these order parameters, we enforce the definition of the order parameter using the Fourier representation of a Dirac-delta function

\[1 =B\int dQ_{n}(t,t^{\prime})\delta\left(BQ_{n}(t,t^{\prime})-\sum_{ \mu}\Delta_{n}^{\mu}(t)\Delta_{n}^{\mu}(t^{\prime})\right)\] \[=B\int\frac{dQ_{n}(t,t^{\prime})d\hat{Q}_{n}(t,t^{\prime})}{4\pi i }\exp\left(\frac{B}{2}\hat{Q}_{n}(t,t^{\prime})Q_{n}(t,t^{\prime})-\frac{1}{2} \sum_{\mu}\Delta_{n}^{\mu}(t)\Delta_{n}^{\mu}(t^{\prime})\hat{Q}_{n}(t,t^{ \prime})\right).\] (B.5)

Repeating this procedure for all order parameters \(q=\{Q,\hat{Q},C,\hat{C},C^{R},\hat{C}^{R},D,\hat{D},D^{R},\hat{D}^{R}\}\) and disregarding irrelevant prefactors, we have the following formula for the moment generating function

\[Z\propto\int\mathcal{D}q\exp\left(\frac{N}{2}S[q]\right)\] (B.6)

where the action \(S\) has the form

\[S =\sum_{n}\sum_{tt^{\prime}}\left[\alpha Q_{n}(t,t^{\prime})\hat{Q }_{n}(t,t^{\prime})+C_{n}(t,t^{\prime})\hat{C}_{n}(t,t^{\prime})+C_{n}^{R}(t,t ^{\prime})\hat{C}_{n}^{R}(t,t^{\prime})\right]\] \[-2\sum_{n}\sum_{tt^{\prime}}\left[D_{n}(t,t^{\prime})\hat{D}_{n}( t,t^{\prime})+D_{n}^{R}(t,t^{\prime})\hat{D}_{n}^{R}(t,t^{\prime})\right]+\frac{2}{N} \ln\mathcal{Z}_{w}+2\alpha\ln\mathcal{Z}_{\Delta}\] \[\mathcal{Z}_{w} =\int\mathcal{D}\bm{w}\mathcal{D}\hat{\bm{w}}\exp\left(-\frac{ \eta^{2}}{2T^{2}}\sum_{ntt^{\prime}}Q_{n}(t,t^{\prime})\hat{\bm{w}}_{n}^{\top} \bm{\Sigma}(t,t^{\prime})\hat{\bm{w}}_{n}+i\sum_{n}\hat{\bm{w}}_{n}\cdot(\bm{w }_{n+1}-\bm{w}_{n})\right)\] \[\exp\left(-\frac{1}{2}\sum_{ntt^{\prime}}\hat{C}_{n}(t,t^{\prime} )\bm{w}_{n}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{w}_{n}-\frac{1}{2}\hat{C}_{n}^ {R}(t,t^{\prime})\bm{w}_{R}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{w}_{n}\right)\] \[\exp\left(-i\sum_{ntt^{\prime}}\hat{D}_{n}(t,t^{\prime})\hat{\bm{ w}}_{n}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{w}_{n}-i\sum_{ntt^{\prime}}\hat{D}_{n}^ {R}(t,t^{\prime})\hat{\bm{w}}_{n}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{w}_{R}\right)\] \[\mathcal{Z}_{\Delta} =\int\mathcal{D}\Delta\mathcal{D}\hat{\Delta}\exp\left(-\frac{1}{2 }\sum_{ntt^{\prime}}\hat{Q}_{n}(t,t^{\prime})\Delta_{n}(t)\Delta_{n}(t^{\prime })+i\sum_{nt}\hat{\Delta}_{n}(t)\Delta_{n}(t)\right)\] \[\exp\left(-\frac{1}{2}\sum_{ntt^{\prime}}\hat{\Delta}_{n}(t)\hat {\Delta}_{n}(t^{\prime})\left[\frac{1}{N}\bm{w}_{R}^{\top}\bm{\Sigma}(t,t^{ \prime})\bm{w}_{R}+C(t,t^{\prime})\right]\right)\] \[\exp\left(\frac{1}{2}\sum_{ntt^{\prime}}\hat{\Delta}_{n}(t)\hat {\Delta}_{n}(t^{\prime})\left[C^{R}(t,t^{\prime})+C^{R}(t^{\prime},t)\right]\right)\] \[\exp\left(-\gamma\sum_{t,t^{\prime}}\hat{\Delta}_{n}(t)\hat{ \Delta}_{n}(t^{\prime}-1)C_{n}^{R}(t,t^{\prime})\right)\] \[\exp\left(-\frac{\gamma^{2}}{2}\sum_{t,t^{\prime}}\hat{\Delta}_{ n}(t-1)\hat{\Delta}_{n}(t^{\prime}-1)C_{n}(t,t^{\prime})\right)\] \[\exp\left(-\frac{\eta i}{\sqrt{\alpha}T}\sum_{nt,t^{\prime}}\hat{ \Delta}_{n}(t)\left[D_{n}^{R}(t^{\prime},t)-D_{n}(t^{\prime},t)+\gamma D_{n}(t ^{\prime},t+1)\right]\Delta_{n}(t^{\prime})\right)\] (B.7)

The function \(\mathcal{Z}\) has the interpretation of an effective partition function conditional on order parameters \(q\). To study the \(N\to\infty\) limit, we use the steepest descent method and analyze the saddle point 

[MISSING_PAGE_FAIL:19]

We therefore obtain a stochastic process of the form

\[\bm{w}_{n+1} =\bm{w}_{n}+\bm{u}_{n}^{w}+\frac{\eta\sqrt{\alpha}}{T}\sum_{t}\bm{ \Sigma}(t,t)\bm{w}_{R}-\frac{\eta\sqrt{\alpha}}{T}\sum_{t}\left[\bm{\Sigma}(t,t)- \gamma\bm{\Sigma}(t,t+1)\right]\bm{w}_{n}\] \[\bm{u}_{n} \sim\mathcal{N}\left(0,\frac{\eta^{2}}{T^{2}}\sum_{tt^{\prime}}Q _{n}(t,t^{\prime})\bm{\Sigma}(t,t^{\prime})\right)\;,\;\left\{\Delta_{n}(t) \right\}\sim\mathcal{N}(0,\bm{Q}_{n})\] \[Q_{n}(t,t^{\prime}) =\left\langle\Delta_{n}(t)\Delta_{n}(t^{\prime})\right\rangle= \frac{1}{N}\bm{w}_{R}\bm{\Sigma}(t,t^{\prime})\bm{w}_{R}-C^{R}(t,t^{\prime})- C^{R}(t^{\prime},t)+C(t,t^{\prime})\] \[C_{n}(t,t^{\prime}) =\frac{1}{N}\left\langle\bm{w}_{n}^{\top}\bm{\Sigma}(t,t^{\prime })\bm{w}_{n}\right\rangle\;,\;C_{n}^{R}(t,t^{\prime})=\frac{1}{N}\left\langle \bm{w}_{R}^{\top}\bm{\Sigma}(t,t^{\prime})\bm{w}_{n}\right\rangle\]

These are the final equations defining the stochastic evolution of \(\bm{w}_{n}\) and \(\Delta_{n}(t)\).

### Simplifying the Saddle Point Equations

Using the above saddle point equations, we see that the variables \(\{\Delta_{n}(t)\}\) and \(\{\bm{w}_{n}\}\) will be Gaussian random variables. It thus suffices to track their mean and covariance. The \(\{\Delta_{n}(t)\}\) variables have zero mean and covariance given by the \(Q_{n}(t,t^{\prime})\) function. The \(\{\bm{w}_{n}\}\) variables have the following mean evolution

\[\left\langle\bm{w}_{n+1}\right\rangle =\left\langle\bm{w}_{n}\right\rangle+\eta\sqrt{\alpha}\left[\bar {\bm{\Sigma}}\bm{w}_{R}-\left[\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+} \right]\left\langle\bm{w}_{n}\right\rangle\right]\] \[=\left\langle\bm{w}_{n}\right\rangle+\eta\sqrt{\alpha}\left[\bar {\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+}\right]\left[\bm{w}_{TD}-\left\langle \bm{w}_{n}\right\rangle\right]\] (B.14)

where \(\bm{w}_{TD}=\left[\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+}\right]^{-1} \bar{\bm{\Sigma}}\bm{w}_{R}\) is the fixed point of the TD dynamics. We next compute which admits the recursion

\[\bm{M}_{n+1}=\left(\bm{I}-\eta\sqrt{\alpha}\left[\bar{\bm{\Sigma}}-\gamma \bar{\bm{\Sigma}}_{+}\right]\right)\bm{M}_{n}\left(\bm{I}-\eta\sqrt{\alpha} \left[\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_{+}\right]\right)+\frac{\eta^{ 2}}{T^{2}}\sum_{tt^{\prime}}Q_{n}(t,t^{\prime})\bm{\Sigma}(t,t^{\prime})\] (B.15)

To obtain our formulas which hold for finite batch size, we rescale the learning rate by \(\eta\to\eta/\sqrt{\alpha}\) giving the following evolution

\[\left\langle\bm{w}_{n+1}\right\rangle =\left\langle\bm{w}_{n}\right\rangle+\eta\left[\bar{\bm{\Sigma}}- \gamma\bar{\bm{\Sigma}}_{+}\right]\left[\bm{w}_{TD}-\left\langle\bm{w}_{n} \right\rangle\right]\] \[\bm{M}_{n+1} =\left(\bm{I}-\eta\left[\bar{\bm{\Sigma}}-\gamma\bar{\bm{\Sigma}}_ {+}\right]\right)\bm{M}_{n}\left(\bm{I}-\eta\left[\bar{\bm{\Sigma}}-\gamma\bar {\bm{\Sigma}}_{+}\right]\right)^{\top}+\frac{\eta^{2}}{T^{2}\alpha^{2}}\sum_{ tt^{\prime}}Q_{n}(t,t^{\prime})\bm{\Sigma}(t,t^{\prime})\] (B.16)

After this rescaling, we see that the mean evolution for \(\bm{w}_{n}\) is independent of \(\alpha\) but that the variance picks up an additive term on each step on the order of \(\mathcal{O}(\eta^{2}\alpha^{-2})\) which vanishes in the infinite batch limit \(B/N\to\infty\). The error for value learning can be obtained from \(\bm{M}_{n}\) with \(\mathcal{L}_{n}=\frac{1}{N}\mathbf{Tr}\bm{M}_{n}\bar{\bm{\Sigma}}\). Lastly, we note that we can express the formula for \(Q_{n}(t,t^{\prime})\) entirely in terms of \(\bm{M}_{n}\) and \(\left\langle\bm{w}_{n}\right\rangle\). Thisgives the lengthy expression

\[Q_{n}(t,t^{\prime}) =\frac{1}{N}\left\langle(\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma}(t,t ^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma}{N}\left\langle(\bm {w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma}(t,t^{\prime}+1)\bm{w}_{n}\right\rangle\] \[+\frac{\gamma}{N}\left\langle\bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{ \prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma^{2}}{N}\left\langle \bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{\prime}+1)\bm{w}_{n}\right\rangle\] \[=\frac{1}{N}\text{Tr}\bm{M}_{n}\bm{\Sigma}(t,t^{\prime})+\frac{1} {N}\left(\bm{w}_{TD}-\left\langle\bm{w}_{n}\right\rangle\right)\left[\bm{ \Sigma}(t,t^{\prime})+\bm{\Sigma}(t^{\prime},t)\right](\bm{w}_{R}-\bm{w}_{TD})\] \[+\frac{1}{N}\left(\bm{w}_{R}-\bm{w}_{TD}\right)^{\top}\bm{\Sigma }(t,t^{\prime})\left(\bm{w}_{R}-\bm{w}_{TD}\right)\] \[-\frac{\gamma}{N}\text{Tr}\bm{M}_{n}\left[\bm{\Sigma}(t,t^{\prime }+1)+\bm{\Sigma}(t+1,t^{\prime})\right]\] \[+\frac{\gamma}{N}\left(\bm{w}_{TD}-\left\langle\bm{w}_{n}\right \rangle\right)\left[\bm{\Sigma}(t,t^{\prime}+1)+\bm{\Sigma}(t+1,t^{\prime}) \right]\bm{w}_{TD}\] \[+\frac{\gamma}{N}(\bm{w}_{R}-\bm{w}_{TD})^{\top}\left[\bm{\Sigma }(t,t^{\prime}+1)+\bm{\Sigma}(t+1,t^{\prime})\right]\left\langle\bm{w}_{n}\right\rangle\] \[+\frac{\gamma^{2}}{N}\text{Tr}\bm{M}_{n}\bm{\Sigma}(t+1,t^{\prime }+1)+\frac{2\gamma^{2}}{N}\left(\left\langle\bm{w}_{n}\right\rangle-\bm{w}_{ TD}\right)\bm{\Sigma}(t+1,t^{\prime}+1)\bm{w}_{TD}\] \[+\frac{\gamma^{2}}{N}\bm{w}_{TD}^{\top}\bm{\Sigma}(t+1,t^{\prime }+1)\bm{w}_{TD}\] (B.17)

### Final Result

Below we state in compact form the full final result for our TD learning curves. The below equations give the evolution of the first and second moments of \(\bm{w}_{n}\) obtained from the mean-field density of the previous section. Concretely, these moments obey dynamics

\[\left\langle\bm{w}_{n+1}\right\rangle =\left\langle\bm{w}_{n}\right\rangle+\eta\left[\bm{\bar{\Sigma}}- \gamma\bm{\bar{\Sigma}}_{+}\right]\left[\bm{w}_{V}-\left\langle\bm{w}_{n}\right\rangle\right]\] \[\bm{M}_{n+1} =\left[\bm{I}-\eta\bm{\bar{\Sigma}}+\eta\gamma\bm{\bar{\Sigma}}_{ +}\right]\bm{M}_{n}\left[\bm{I}-\eta\bm{\bar{\Sigma}}+\eta\gamma\bm{\bar{ \Sigma}}_{+}\right]^{\top}+\frac{\eta^{2}}{\alpha^{2}T^{2}}\sum_{tt^{\prime}}Q _{n}(t,t^{\prime})\bm{\Sigma}(t,t^{\prime})\] \[Q_{n}(t,t^{\prime}) =\frac{1}{N}\left\langle(\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma }(t,t^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma}{N}\left\langle (\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\Sigma}(t,t^{\prime}+1)\bm{w}_{n}\right\rangle\] \[+\frac{\gamma}{N}\left\langle\bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{ \prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma^{2}}{N}\left\langle \bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{\prime}+1)\bm{w}_{n}\right\rangle.\] (B.18)

These equations can be solved iteratively for \(\bar{\bm{w}}_{n},\bm{M}_{n},Q_{n}\). Finite dimensional versions of this result can be obtained by replacing \(\alpha\) with \(B/N\) as written in the main text. The value estimation error is

\[\mathcal{L}_{n}=\frac{1}{N}\text{Tr}\bm{M}_{n}\bm{\bar{\Sigma}}.\] (B.19)

### Non-Zero Mean Feature

We can also simply modify the DMFT equations if the mean feature is nonvanishing \(\bm{\mu}(s)\neq 0\). In this case, when averaging over all possible trajectories through state space, there is a mean feature vector at each episodic time \(\bm{\mu}(t)\). The above equations are exact for non-zero mean features if \(\bm{\Sigma}(t,t^{\prime})\) is regarded as the (non-centered) correlation matrix \(\left\langle\bm{\psi}(t)\bm{\psi}(t^{\prime})\right\rangle\).

### Action Dependent Rewards

#### b.5.1 Expected Q-Learning Reduces to Previous Model

In the case where we consider using features that depend on both states and actions \(\bm{\psi}(s,a)\) then we can use expected value learning to identify the expected value of a state-action pair under policy \(\pi\)

\[V(s,a)=R(s,a)+\gamma\left\langle V(s^{\prime},a^{\prime})\right\rangle_{s^{ \prime},a^{\prime}|s,a}\] (B.20)

This \(V\) function quantifies the expected reward associated with taking action \(a\) when in state \(s\) and subsequently following policy \(\pi\). This problem is structurally identical to the state dependent case by recognizing that state action pairs \((s,a)\) act as new states \(\tilde{s}\). As before, the policy defines the probability distribution over transitions on \(\tilde{s}\). We can thus use Equation (4) to calculate the learning curve for this problem.

#### b.5.2 Action Dependence Generates Target Noise in State Dependent Value Learning

In the case where the rewards depend on both state and action \(R(s,a)\) but features only depend on state \(\bm{\psi}(s)\), we need a slight modification of our theory which models the reward at each state as a mean value (over actions) plus a noise. For each state, we decompose the reward function into mean and fluctuation

\[R(s,a)=\bar{R}(s)+\epsilon(s,a)\;,\;\bar{R}(s)=\mathbb{E}_{a\sim\pi(a|s)}R(s,a)\] (B.21)

The function \(\bar{R}(s)\) can again be decomposed into the basis of features \(\bm{\psi}(s)\). However, we need to consider the correlation structure of \(\epsilon(s,a)\).

\[\mathbb{E}_{\tau}\epsilon(s_{t},a_{t})\epsilon(s_{t^{\prime}},a_ {t^{\prime}}) =\mathbb{E}_{s_{t}}\mathbb{E}_{a_{t}|s_{t}}\epsilon(s_{t},a_{t} )\left[\mathbb{E}_{s_{t^{\prime}}|s_{t},a_{t}}\left(\mathbb{E}_{a_{t^{\prime} }|s_{t^{\prime}}}\epsilon(s_{t^{\prime}},a_{t^{\prime}})\right)\right]\] \[=\delta_{t,t^{\prime}}\text{Var}_{a|s_{t}}R(s,a).\] (B.22)

The above average vanishes for \(t\neq t^{\prime}\) since \(\epsilon(s_{t^{\prime}},a_{t^{\prime}})\) is zero mean over \(a|s\). We introduce the notation \(\sigma_{t}^{2}=\text{Var}_{a|s_{t}}R(s_{t},a)\). Thus, we effectively have a model where our TD errors obey

\[\Delta(t)=\bar{R}(s_{t})+\epsilon(s_{t},a_{t})+\gamma\hat{V}(s_{t})-\hat{V}(s_ {t})\] (B.23)

The addition of this term leads to a simple modification of our \(Q(t,t)\) function

\[Q_{n}(t,t^{\prime}) =\frac{1}{N}\left\langle\left(\bm{w}_{R}-\bm{w}_{n}\right)^{\top }\bm{\Sigma}(t,t^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma}{ N}\left\langle\left(\bm{w}_{R}-\bm{w}_{n}\right)^{\top}\bm{\Sigma}(t,t^{\prime}+1) \bm{w}_{n}\right\rangle\] \[+\frac{\gamma}{N}\left\langle\bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t ^{\prime})(\bm{w}_{R}-\bm{w}_{n})\right\rangle+\frac{\gamma^{2}}{N}\left\langle \bm{w}_{n}^{\top}\bm{\Sigma}(t+1,t^{\prime}+1)\bm{w}_{n}\right\rangle\] \[+\delta_{t,t^{\prime}}\sigma_{t}^{2}.\] (B.24)

This change to the \(Q_{n}(t,t^{\prime})\) correlation function alters the dynamics of \(\bm{M}_{n}\). Lastly, our population risk for the value estimation takes the form

\[\mathcal{L}_{n}=\frac{1}{N}\text{Tr}\bm{M}_{n}\bm{\Sigma}+\frac{1}{T}\sum_{t} \sigma_{t}^{2}\] (B.25)

where \(\frac{1}{T}\sum_{t}\sigma_{t}^{2}\) exactly quantifies the variance in rewards unexplained by state-dependent features.

### Tracking Iterate Moments with Direct Recurrence Relation

In this section we give a direct calculation of the first two moments of \(\bm{w}\) over the collection of randomly sampled features \(\{\bm{\psi}_{n}^{\mu}(t)\}\) and show which terms are disregarded in the proportional limit examined in the main text.

Letting \(\bm{A}=\bm{\Sigma}-\gamma\bm{\Sigma}_{+}\), we note that the average evolution of \(\bm{w}\) has the form

\[\left\langle\bm{w}_{n+1}\right\rangle=\left(\bm{\Sigma}-\gamma\bm{\Sigma}_{+} \right)\left(\bm{w}_{TD}-\left\langle\bm{w}_{n}\right\rangle\right)\] (B.26)

Thus, if we disregarded fluctuations in \(\bm{w}_{n}\) due to SGD, the model will converge to the correct fixed point. Next, we look at \(\bm{M}_{n}=\left\langle\left(\bm{w}_{n}-\bm{w}_{TD}\right)\left(\bm{w}_{n}- \bm{w}_{TD}\right)\right\rangle\). Under the Gaussian equivalence ansatz, we have

\[\bm{M}_{n+1} =\bm{M}_{n}-\eta\bm{A}\bm{M}_{n}-\eta\bm{M}_{n}\bm{A}^{\top}+ \frac{\eta^{2}}{T^{2}B^{2}}\sum_{\mu\nu tt^{\prime}}\left\langle\Delta_{n}^{ \mu}(t)\Delta_{n}^{\nu}(t^{\prime})\bm{\psi}_{n}^{\mu}(t)\bm{\psi}_{n}^{\nu}(t ^{\prime})\right\rangle\] \[=(\bm{I}-\eta\bm{A})\bm{M}_{n}(\bm{I}-\eta\bm{A})^{\top}-\frac{ \eta^{2}}{B}\bm{A}\bm{M}_{n}\bm{A}^{\top}+\frac{\eta^{2}}{T^{2}B}\sum_{tt^{ \prime}}\left\langle\Delta_{n}(t)\Delta_{n}(t^{\prime})\bm{\psi}(t)\bm{\psi }(t^{\prime})^{\top}\right\rangle\] \[=(\bm{I}-\eta\bm{A})\bm{M}_{n}(\bm{I}-\eta\bm{A})^{\top}+\frac{ \eta^{2}}{T^{2}B}\sum_{tt^{\prime}}Q_{n}(t,t^{\prime})\bm{\Sigma}(t,t^{ \prime})\] \[+\frac{\eta^{2}}{T^{2}B}\sum_{tt^{\prime}}\left\langle\Delta_{n}(t ^{\prime})\bm{\psi}(t)\right\rangle\left\langle\Delta_{n}(t)\bm{\psi}(t^{ \prime})^{\top}\right\rangle\] (B.27)The mean field theory derived from saddle point integration consists of the first two terms in the final expression. Therefore mean field theory disregards the last term which computes cross time correlations of RPEs with features, effectively making the approximation

\[\frac{\eta^{2}}{T^{2}B}\sum_{tt^{\prime}}\left\langle\Delta_{n}(t^{\prime}) \boldsymbol{\psi}(t)\right\rangle\left\langle\Delta_{n}(t)\boldsymbol{\psi}(t ^{\prime})^{\top}\right\rangle\approx 0.\] (B.28)

After making this approximation, we recover the learning curve obtained in the previous Section B.3. We show in our experiments that dropping this term does not significantly alter the learning curves.

### Scaling of Asymptotic Fixed Points

To identify fixed points in the value error dynamics, we can seek non-vanishing fixed points for the weight error covariance \(\boldsymbol{M}=\left\langle(\boldsymbol{w}-\boldsymbol{w}_{TD})(\boldsymbol{w }-\boldsymbol{w}_{TD})\right\rangle\). We note that \(\left\langle\boldsymbol{w}\right\rangle\sim\boldsymbol{w}_{TD}\) asymptotically. Again, letting \(\boldsymbol{A}=\bar{\boldsymbol{\Sigma}}-\gamma\bar{\boldsymbol{\Sigma}}_{+}\), we obtain the following fixed point condition for \(\boldsymbol{M}\) under these assumptions

\[\boldsymbol{AM}+ \boldsymbol{MA}^{\top}-\eta\boldsymbol{AM}\boldsymbol{A}^{\top }=\frac{\eta}{BT^{2}}\sum_{tt^{\prime}}Q(t,t^{\prime})\boldsymbol{\Sigma}(t,t^ {\prime})\] \[Q(t,t^{\prime}) =\text{Tr}\boldsymbol{M}\boldsymbol{\Sigma}(t,t^{\prime})-\gamma \text{Tr}\boldsymbol{M}\left[\boldsymbol{\Sigma}(t,t^{\prime}+1)+\boldsymbol{ \Sigma}(t+1,t^{\prime})\right]+\gamma^{2}\text{Tr}\boldsymbol{M}\boldsymbol{ \Sigma}(t+1,t^{\prime}+1)\] \[+\gamma^{2}\boldsymbol{w}_{TD}^{\top}\boldsymbol{\Sigma}^{-1} \boldsymbol{\Sigma}_{+}\boldsymbol{\Sigma}(t,t^{\prime})\bar{\boldsymbol{ \Sigma}}_{+}\boldsymbol{\Sigma}^{-1}\boldsymbol{w}_{TD}+\gamma^{2}\boldsymbol {w}_{TD}^{\top}\boldsymbol{\Sigma}(t+1,t^{\prime}+1)\boldsymbol{w}_{TD}\] \[+\gamma^{2}\boldsymbol{w}_{TD}\bar{\boldsymbol{\Sigma}}^{-1} \bar{\boldsymbol{\Sigma}}_{+}\left[\boldsymbol{\Sigma}(t,t^{\prime}+1)+ \boldsymbol{\Sigma}(t+1,t^{\prime})\right]\boldsymbol{w}_{TD}.\] (B.29)

Where we used the formula for \(Q_{n}(t,t^{\prime})\) from Appendix B.6, evaluated at \(\left\langle\boldsymbol{w}\right\rangle=\boldsymbol{w}_{TD}\) and used the fact that \(\boldsymbol{w}_{R}=\boldsymbol{w}_{TD}-\gamma\bar{\boldsymbol{\Sigma}}_{-} \boldsymbol{\Sigma}_{+}\boldsymbol{w}_{TD}\). The solution \(\boldsymbol{M}=0\) is a valid fixed point for \(\boldsymbol{M}\) in the \(\eta\to 0\) and \(B\to\infty\) limits because the constant terms on the right-hand side vanish. Similarly, if \(\gamma=0\) (which corresponds to the standard supervised learning case), the right hand side is linear in \(\boldsymbol{M}\), allowing \(\boldsymbol{M}=0\) to be a valid fixed point.

However, for finite \(B\) and non-zero \(\eta\) and \(\gamma\), there exists a solution to the above fixed point equation. For small \(\frac{\eta\gamma^{2}}{B}\), we can deduce that \(\boldsymbol{M}\) must satisfy a self-consistent asymptotic scaling of the form

\[\boldsymbol{M}=\mathcal{O}\left(\frac{\eta\gamma^{2}}{B}\right)\] (B.30)

implying an asymptotic value error scaling of \(\mathcal{L}\sim\text{Tr}\boldsymbol{M}\bar{\boldsymbol{\Sigma}}\sim\mathcal{O }\left(\frac{\eta\gamma^{2}}{B}\right)\). These scalings are examined in Figure 3 where experiments obey the expected behavior.

## Appendix C Reward Shaping

In this section, we consider the role of reward shaping on the dynamics of TD learning. As discussed in the main text, we consider potential based shaping with potential function decomposable in the features \(\phi(s)=\boldsymbol{w}_{\phi}\cdot\boldsymbol{\psi}(s)\). We first describe the change to the average weight evolution \(\left\langle\boldsymbol{w}_{n}\right\rangle\) and then describe the dynamics of the correlations. In potential based shaping, the TD errors take the form

\[\Delta(t)=R(s(t))+\phi(s(t))-\gamma\phi(s(t+1))+\gamma\hat{V}(s(t+1))-\hat{V} (s(t))\] (C.1)

Computing from the DMFT equations the evolution of \(\left\langle\boldsymbol{w}_{n}\right\rangle\) we have

\[\left\langle\boldsymbol{w}_{n+1}\right\rangle =\left\langle\boldsymbol{w}_{n}\right\rangle+\eta\bar{\boldsymbol {\Sigma}}(\boldsymbol{w}_{R}+\boldsymbol{w}_{\phi}-\left\langle\boldsymbol{w}_{ n}\right\rangle)+\gamma\eta\bar{\boldsymbol{\Sigma}}_{+}(\left\langle \boldsymbol{w}_{n}\right\rangle-\boldsymbol{w}_{\phi})\] \[=\left\langle\boldsymbol{w}_{n}\right\rangle-\eta\boldsymbol{A} \left[\boldsymbol{w}_{TD}+\boldsymbol{w}_{\phi}-\left\langle\boldsymbol{w}_{ n}\right\rangle\right].\] (C.2)

We see that including the reward shaping function \(\phi\) offsets the fixed point of the algorithm to be \(\boldsymbol{w}_{TD}+\boldsymbol{w}_{\phi}\). This occurs precisely because the potential-based reward shaping generates an additive correction to the target value function by \(\phi(s)\)[64]. When we predict value at evaluation, we use the reshifted value \(\hat{V}(s)-\phi(s)\). The natural quantity to track at the level of the mean field equations is the adapted version of \(\boldsymbol{M}_{n}\)

\[\boldsymbol{M}_{n}=\left\langle\left(\boldsymbol{w}_{n}-\boldsymbol{w}_{TD}- \boldsymbol{w}_{\phi}\right)\left(\boldsymbol{w}_{n}-\boldsymbol{w}_{TD}- \boldsymbol{w}_{\phi}\right)^{\top}\right\rangle.\] (C.3)This correlation matrix has dynamics

\[\bm{M}_{n+1}=\left(\bm{I}-\eta\bm{A}\right)\bm{M}_{n}\left(\bm{I}-\eta\bm{A} \right)^{\top}+\frac{\eta^{2}}{BT^{2}}\sum_{tt^{\prime}}Q_{n}(t,t^{\prime})\bm{ \Sigma}(t,t^{\prime})\] (C.4)

and the TD-error correlations \(Q_{n}(t,t^{\prime})\) have the form

\[Q_{n}(t,t^{\prime})= \left\langle\left(\bm{w}_{R}+\bm{w}_{\phi}-\bm{w}_{n}\right)^{ \top}\bm{\Sigma}(t,t^{\prime})(\bm{w}_{R}+\bm{w}_{\phi}-\bm{w}_{n})\right\rangle\] \[+\gamma\left\langle\left(\bm{w}-\bm{w}_{\phi}\right)^{\top} \left[\bm{\Sigma}(t,t^{\prime})+\bm{\Sigma}(t^{\prime},t)\right](\bm{w}_{R}+ \bm{w}_{\phi}-\bm{w}_{n})\right\rangle\] \[+\gamma^{2}\left\langle\left(\bm{w}_{n}-\bm{w}_{\phi}\right)^{ \top}\bm{\Sigma}(t+1,t^{\prime}+1)(\bm{w}_{n}-\bm{w}_{\phi})\right\rangle\] (C.5)

The value estimation error is again \(\mathcal{L}_{n}=\text{Tr}\bm{M}_{n}\bar{\bm{\Sigma}}\). We see that the two primary ways that reward shaping alters the loss dynamics is

* A change in the initial condition for \(\bm{M}_{n}\) to be \(\bm{M}_{0}=(\bm{w}_{TD}+\bm{w}_{\phi})(\bm{w}_{TD}+\bm{w}_{\phi})^{\top}\)
* A change in the TD error covariance term \(Q_{n}(t,t^{\prime})\)

Both effects can generate significant changes in the dynamics and plateaus of the model.

## Appendix D Non-Gaussian Features

The full non-asymptotic theory (no assumptions on \(N,B\) large) of TD dynamics with linear function approximation closes under the fourth moments of the features. In this setting we do not incorporate explicit factors of \(N\) in the defintion of the value estimator \(\hat{V}(t)=\bm{w}\cdot\bm{\psi}(t)\). As before, we track the update to the \(\bm{M}\) matrix

\[\bm{M}_{n+1} =\bm{M}_{n}-\eta\bm{A}\bm{M}_{n}-\eta\bm{M}_{n}\bm{A}^{\top}+ \frac{\eta^{2}(B-1)}{B}\bm{A}\bm{M}_{n}\bm{A}^{\top}\] \[+\frac{\eta^{2}}{B}\sum_{tt^{\prime}}\left\langle\Delta_{n}(t) \Delta_{n}(t^{\prime})\bm{\psi}(t)\bm{\psi}(t^{\prime})^{\top}\right\rangle\] (D.1)

To calculate the last term, we introduce the following tensor of fourth moments

\[\kappa^{4}_{ijkl}(t_{1},t_{2},t_{3},t_{4})=\left\langle\psi_{i}(t_{1})\psi_{j }(t_{2})\psi_{k}(t_{3})\psi_{l}(t_{4})\right\rangle\] (D.2)

In the Gaussian case, this reduces to an expression involving only the correlations. For example, if the features are zero mean, then we can use Wick's theorem to obtain the decomposition

\[\kappa^{4,Gauss}_{ijkl}(t_{1},t_{2},t_{3},t_{4})=\Sigma_{ij}(t_{1},t_{2}) \Sigma_{kl}(t_{3},t_{4})+\Sigma_{ik}(t_{1},t_{3})\Sigma_{jl}(t_{2},t_{4})+ \Sigma_{ij}(t_{1},t_{2})\Sigma_{kl}(t_{3},t_{4})\] (D.3)

Now, using the fact that \(\Delta_{n}(t)=(\bm{w}_{R}-\bm{w}_{n})\cdot\bm{\psi}(t)+\gamma\bm{w}_{n}\cdot \bm{\psi}(t+1)\), we find

\[\left\langle\Delta_{n}(t)\Delta_{n}(t^{\prime})\psi_{i}(t)\psi_{ j}(t^{\prime})^{\top}\right\rangle\] \[=\left\langle\psi_{i}(t)\psi_{j}(t^{\prime})\;\bm{\psi}(t)^{\top} (\bm{w}_{R}-\bm{w}_{n})(\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\psi}(t^{\prime})\right\rangle\] \[+\gamma\left\langle\psi_{i}(t)\psi_{j}(t^{\prime})\;\bm{\psi}(t)^{ \top}(\bm{w}_{R}-\bm{w}_{n})\bm{w}_{n}^{\top}\bm{\psi}(t^{\prime}+1)\right\rangle\] \[+\gamma\left\langle\psi_{i}(t)\psi_{j}(t^{\prime})\;\bm{\psi}(t+1) ^{\top}\bm{w}_{n}(\bm{w}_{R}-\bm{w}_{n})^{\top}\bm{\psi}(t^{\prime})\right\rangle\] \[+\gamma^{2}\left\langle\psi_{i}(t)\psi_{j}(t^{\prime})\;\bm{\psi}( t+1)^{\top}\bm{w}_{n}\bm{w}_{n}^{\top}\bm{\psi}(t^{\prime}+1)\right\rangle\] (D.4)Putting this all together, we find the following recurrence for \(\bm{M}_{n}\) in the non-Gaussian case

\[\bm{M}_{n+1} =\bm{M}_{n}-\eta\bm{A}\bm{M}_{n}-\eta\bm{M}_{n}\bm{A}^{\top}+\frac{ \eta^{2}(B-1)}{B}\bm{A}\bm{M}_{n}\bm{A}^{\top}\] \[+\frac{\eta^{2}}{BT^{2}}\sum_{t,t^{\prime}}\text{Tr}\bm{\kappa}(t, t^{\prime},t,t^{\prime})\left<(\bm{w}_{R}-\bm{w}_{n})(\bm{w}_{R}-\bm{w}_{n})^{ \top}\right>\] \[+\frac{\gamma\eta^{2}T^{2}}{B}\sum_{t,t^{\prime}}\text{Tr}\,\bm{ \kappa}(t,t^{\prime},t,t^{\prime}+1)\left<\bm{w}_{n}(\bm{w}_{R}-\bm{w}_{n})^{ \top}\right>\] \[+\frac{\gamma^{2}\eta^{2}}{BT^{2}}\sum_{t,t^{\prime}}\text{Tr}\, \bm{\kappa}(t,t^{\prime},t+1,t^{\prime})\left<(\bm{w}_{R}-\bm{w}_{n})\bm{w}_{ n}^{\top}\right>\] \[+\frac{\gamma^{2}\eta^{2}}{BT^{2}}\sum_{t,t^{\prime}}\text{Tr}\, \bm{\kappa}(t,t^{\prime},t+1,t^{\prime}+1)\left<\bm{w}_{n}\bm{w}_{n}^{\top}\right>\] (D.5)

where all traces are taken against the last two time and feature indices. The averages over the weights close in terms of the average \(\left<\bm{w}_{n}\right>\) and the covariance \(\bm{M}_{n}\). This equation is exact for any feature distribution, but requires significant computational resources to evaluate and is less illuminating than the mean field limit analyzed in the previous sections.

### Breakdown of Gaussian Theory in Low Dimension

In this section, we discuss the breakdown of Gaussian theory at low dimension \(N\). In Figure 11 we provide an example where the non-Gaussian distributions exhibit noticeably different learning curves than the Gaussian approximate theory (dashed black) and Gaussian samples with matching covariance (dashed color). We use the features

### A Simple Solveable Example

We next examine a very simple case where we can exactly characterize the gap between the non-Gaussian and Gaussian distributions. In this section, we examine the special case of \(T=1\) and look at features which are independent \(p(\bm{\psi})=\prod_{i=1}^{N}p(\psi_{i})\) (form a factor distribution). In this case, we obtain the following exact learning curve

\[\mathcal{L}_{n}=\left[(1-\eta)^{2}+\frac{\eta^{2}(N+1+\kappa)}{B}\right]^{n} \,\ \kappa=\left<\psi^{4}\right>-3\left<\psi^{2}\right>^{2}\] (D.6)

Figure 11: The Gaussian theory can break down for non-Gaussian features in low dimension \(N\). Illustration of the possible gap between Gaussian and non-Gaussian performance in the power law features of Figure 3 and defined in Appendix G. (a) The learning curves for batchsize \(B=5\) and varying dimension \(N\). As \(N\) increases the gap between the non-Gaussian experiment (solid) and the Gaussian theory (black dashed) decreases.

which holds for any \(N,B\). We note that in the limit where \(N,B\to\infty\) with \(B/N=\alpha\), we see that the dependence on \(\kappa\) disappears and we arrive at the universal behavior

\[\mathcal{L}_{n}\sim\left[(1-\eta)^{2}+\frac{\eta^{2}}{\alpha}\right]^{n}\,\ N,B\to\infty\] (D.7)

For example, we can consider vectors on the hypercube where \(\psi_{k}\in\{\pm 1\}\) with equal probability for \(k\in\{1,...,N\}\) for the non-Gaussian distribution and compare to the Gaussian with identical covariance \(\bm{\psi}\sim\mathcal{N}(0,\bm{I})\).

\[\mathcal{L}_{n}=\begin{cases}\left[(1-\eta)^{2}+\frac{\eta^{2}(N+1)}{B}\right] ^{n}&\text{Gaussian }\bm{\psi}\\ \left[(1-\eta)^{2}+\frac{\eta^{2}(N-1)}{B}\right]^{n}&\text{Hypercube }\bm{\psi}\end{cases}\] (D.8)

The reason for the discrepancy between the Gaussian and Bernoulli/Hypercube loss curves is exactly the negative kurtosis of the hypercube features

\[\kappa_{\text{Gauss}}=0\] (D.9)

An example of this result for low and high dimensions \(N\) with \(B=\alpha N\) with \(\alpha=0.1\) is provided in Figure D.2. In low dimension (\(N=10\)) the Bernoulli/Hypercube feature have noticeably different dynamics than the Gaussian features. In the proportional limit \(N,B\to\infty\) with \(\alpha=B/N\) these learning curves are identical and all have the form \(\mathcal{L}_{n}\sim\left[(1-\eta)^{2}+\frac{\eta^{2}}{\alpha}\right]^{n}\).

## Appendix E Tests on Other Feature Distributions

In this section, we include additional tests of our theory on alternative features with the same random walk policy as in Figure 1.

## Appendix F Plateau Scaling in MountainCar-v0 Environment

We verified the results of the theory on the environment MountainCar-v0. First, we train a policy with tabular \(\epsilon\)-greedy Q-Learning (\(\epsilon=0.1,\gamma=0.99,\eta=0.01\)) to learn policy \(\pi\). The position and velocity are discretized into 42 and 28 states, respectively. The learned policy \(\pi\) is not optimal but consistently reaches goal within 350 timesteps. Therefore, each episode is set to have a length of 350 timesteps. Next, we take \(\pi\) and evaluate it with TD learning.

Since MountainCar-v0 is a continuous environment, there is no closed solution to the ground truth of the value function. To estimate the ground truth value function, we ran TD learning with a small learning rate for 10M batches (\(\eta=0.01,B=1,\gamma=0.99\)) to obtain \(V^{\pi}\approx\tilde{V}_{10M}\).

Figure D.2: A simple explicitly solveable case shows how non-Gaussian corrections appear at finite size but disappear in the proportional limit where \(N,B\to\infty\) with \(\alpha=B/N=\mathcal{O}(1)\).

## Appendix G Numerical methods and additional details

The code to generate the Figures is provided in the Supplementary Material as a Jupyter Notebook at the following Github repository https://github.com/Pehlevan-Group/TD-RL-dynamics. Here, we briefly highlight some of the parameter choices.

For Figures 3 and 4 we use diagonally decoupled, but temporally correlated power law features with \(\Sigma_{k\ell}(t,t^{\prime})=\delta_{k\ell}\ k^{-1.2}\exp{(-|t-t^{\prime}|/\tau_ {k})}\) with \(\tau_{k}=\frac{10}{k+1}\) and \(w_{k}^{R}=k^{-1.1}\) for \(k\in[N]\) with \(N=300\). This type of feature structure is especially easy to evaluate the theoretical learning curves for. Unless otherwise stated, these figures used \(\gamma=0.9\) and batch size \(B=10\).

For the 2D MDP grid world, we defined a discrete set of states on a \(17\times 17\) grid. The agent starts in the middle position and follows a random diffusion policy where each possible movement (up, down, left, right) is taken with equal probability. The features were generated as bell-shaped place cells (shown). We computed \(\bm{\Sigma}(t,t^{\prime})\) for the theory by sampling \(5000\) random draws of length \(T=50\). The Gaussian learning curve is obtained with TD learning with \(\bm{\psi}_{G}\sim\mathcal{N}(0,\bm{\Sigma})\).

Numerical experiments were performed on a NVIDIA SMX4-A100-80GB GPU using JAX to vectorize repetitive aspects of the experiments. With the exception of the MountainCar-v0 simulations, the numerical experiments (both preliminary experiments and those presented in the paper) took around \(1\) hour of compute time.

Figure F.1: Simulation in a MountainCar-v0 environment. (a) Value function learned by Tabular Q-Learning that approximates the value function of an optimal policy. (b) An example value function of a policy (\(V^{\pi}\)) learned by TD learning. Notice that the value function does not equate to that in (a) due to the policy \(\pi\) not reaching all states in the environment. (c-d) Linear scaling of convergence value error with the learning rate and the inverse of batch size. Target value function is the same across both experiments. Each dot represents a different seed. A total of 10 seeds were used. (c) Convergence value errors were computed by averaging the 100k batches before batch 10M. (d) Convergence value errors were computed by averaging the 100k batches before batch 1M.