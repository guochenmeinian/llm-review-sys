# BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens

 Hanxi Guo

Purdue University

guo778@purdue.edu

&Siyuan Cheng

Purdue University

cheng535@purdue.edu

&Xiaolong Jin

Purdue University

jin509@purdue.edu

&Zhuo Zhang

Purdue University

zhan3299@purdue.edu

&Kaiyuan Zhang

Purdue University

zhan4057@purdue.edu

&Guanhong Tao

University of Utah

guanhong.tao@utah.edu

&Guangyu Shen

Purdue University

shen447@purdue.edu

&Xiangyu Zhang

Purdue University

xyzhang@cs.purdue.edu

###### Abstract

Detecting text generated by Large Language Models (LLMs) is a pressing need in order to identify and prevent misuse of these powerful models in a wide range of applications, which have highly undesirable consequences such as misinformation and academic dishonesty. Given a piece of subject text, many existing detection methods work by measuring the difficulty of LLM predicting the next token in the text from their prefix. In this paper, we make a critical observation that how well the current token's output logits memorizes the closely preceding input tokens also provides strong evidence. Therefore, we propose a novel bi-directional calculation method that measures the cross-entropy losses between an output logits and the ground-truth token (forward) and between the output logits and the immediately preceding input token (backward). A classifier is trained to make the final prediction based on the statistics of these losses. We evaluate our system, named BiScope, on texts generated by five latest commercial LLMs across five heterogeneous datasets, including both natural language and code. BiScope demonstrates superior detection accuracy and robustness compared to nine existing baseline methods, exceeding the state-of-the-art non-commercial methods' detection accuracy by over \(0.30\) F1 score, achieving over \(0.95\) detection F1 score on average. It also outperforms the best commercial tool GPTZero that is based on a commercial LLM trained with an enormous volume of data. Code is available at [https://github.com/MarkGHK/BiScope](https://github.com/MarkGHK/BiScope).

## 1 Introduction

Given the superior performance of Large Language Models (LLMs) in understanding and generating text, they have become an integral part of human society, assisting people with daily activities such as summarizing articles, polishing emails, and more. However, the widespread use of LLMs also raises concerns about the misuse of AI-generated text. For instance, students and academics may utilize LLMs to produce content for their assignments and research [10, 37, 30], compromising academic integrity. Adversarial individuals could leverage LLMs to efficiently create inflammatory and fraudulent content on social media [22]. Additionally, the development of LLMs themselves faces challenges related to the quality of existing datasets, which may be compromised by the significantinclusion of AI-generated text [49, 27]. All of these issues underscore the urgent need to distinguish AI-generated text from human-written text [5, 14, 18, 50, 3, 12].

Despite this urgency, current AI-generated text detection techniques fall short as LLMs become increasingly diverse and advanced. Our experiments demonstrate that most of the existing approaches cannot achieve an F1 score exceeding 80% on the Yelp dataset [32] when using the latest LLMs (e.g., Claude-3-Opus) to generate content. A close examination of these approaches reveals inherent limitations by design. Specifically, there are three kinds of methods that do not need pre-training or additional information on the LLMs that generate the data (_e.g._, watermarking [7, 19, 44, 46, 20]). The first kind [5] directly prompts another LLM or NLP model to classify whether the subject text is AI-generated. While intuitive, the inevitable model hallucination [55, 26] consistently prevents it from achieving a high accuracy. The second kind of methods [28, 31, 9, 54] examines the linguistic features of the subject text, which are increasingly susceptible to deception as LLMs become more sophisticated and human-like in their responses. The third approach [11, 41, 17, 40, 16, 32, 35, 4, 53, 48] feeds partial or entire text to a surrogate model and checks how well the output text aligns with the surrogate model's preference via various metrics or downstream classifiers. While this method outperforms the first two approaches, it only examines the next token information in the output logits, representing just part of model behaviors, thereby naturally limiting its performance.

In this work, we explore the potential of leveraging internal model states to detect AI-generated text. Like existing methods, we hypothesize that since LLMs are trained on vast corpora of data from the Internet, their training data likely exhibit significant similarities, leading to similar behaviors across models. Therefore, we use a surrogate model to approximate the behaviors of the one used to generate the subject text. We also make a critical observation that, in causal language models (_e.g._, GPT), the current token's output logits encode information about both the next token (_i.e._, prediction) and its preceding input tokens (_i.e._, memorization), indicating a bidirectional relationship between the output logits and the input text. Specifically, when these causal language models encounter human data, they tend to memorize more preceding token information while predicting less next token information in their output logits.

To reveal this relationship, we calculate two kinds of cross-entropy losses by feeding different portions of the subject text into the surrogate model. One is the forward information, calculated as the cross-entropy loss between the output logits and the expected next token in the subject text. The other is the backward information, calculated as the cross-entropy loss between the output logits and the most preceding input token. We then train a binary classifier on the collected statistical loss features to make the final prediction. We also introduce several novel improvements in the prototype, such as providing a summary of the subject text to better guide the surrogate model, thereby enhancing its practical effectiveness and robustness, and using parallel model inference to enhance efficiency. As such, we propose BiScope, an effective and efficient AI-generated text detector by harnessing both the prediction and memorization features of the LLM through its output logits.

Our contributions are as follows:

* We propose a novel AI-generated text detection algorithm that exploits both the preceding token information (_i.e._, memorization) and the next token information (_i.e._, prediction) via an innovative bi-directional cross-entropy loss calculation method. Additionally, we are the first to utilize text summaries to guide the detection, further enhancing its effectiveness and robustness toward heterogeneous data.
* We extend existing datasets and craft a large-scale public dataset for more challenging AI-generated texts, consisting of \(25\) distinct groups and more than \(22,000\) samples. The dataset is sourced from five different text domains (both natural language and code) and generated by the five latest commercial LLMs. This dataset presents more challenging scenarios compared to existing datasets, which are typically sourced from open-source LLMs with fewer parameters and capabilities. We also craft a paraphrased version of our dataset.
* We develop a prototype named BiScope, a detection pipeline without any fine-tuning needed for the detection LLM. We evaluate it on our dataset and compare it with nine state-of-the-art baseline techniques. Our results show that BiScope can achieve an average F1 score of over \(0.95\), taking less than \(200\) milliseconds to detect a sample (when the summary procedure is disabled), while the baseline techniques achieve only \(0.70\)-\(0.85\) F1 scores and take up to \(27\) seconds per sample. BiScope also outperforms the best commercial tool,GPTZero, in 72% of cases. Additionally, we conduct a comprehensive ablation study to verify the effectiveness and robustness of each component of BiScope.

## 2 Background and Related Work

In addition to various watermarking techniques [7; 19; 44; 46; 20; 24; 15; 52] that require fine-tuning or additional information about the LLMs generating the text, several efforts have been directed towards the detection of AI-generated texts with minimal prior knowledge about the generative models. These efforts broadly fall into three categories. As stated in SS 1, the first two categories perform worse than the last category, hence we mainly focus on the methods in the third category that use a surrogate LLM in this paper. The methods in the third category can be further divided into two types: statistical methods and training-based methods.

**Statistical Methods.** These techniques [45; 34; 21; 33] primarily utilize pre-trained LLMs to simulate the generation process of the target generative AI, analyzing the statistical differences between AI-generated texts and human-written ones. These methods commonly serve as zero-shot approaches, assigning scores to indicate the probability of texts being AI-generated. For example, Zero-shot Query [32; 48] prompts a pre-trained LLM to score the input text. LogRank [11; 35] calculates the average probability rank of each token in a text processed through a pre-trained LLM, where higher ranks suggest the text is AI-generated. LRR [41] improves on LogRank by incorporating token confidence. DetectGPT [35] involves masking parts of the text to see how an LLM reconstructs them, and Raidar [32] employs the LLM to rewrite the text. Both methods assume that AI-generated texts are more likely to be preserved accurately in the process. Binoculars [13] analyzes the cross-entropy between the output logits from two surrogate models with different fine-tuning configurations.

**Training-based Methods.** This type includes methods train an NLP model to distinguish between AI-generated and human-written texts. For example, OpenAI [40] uses a RoBERTa-based model for training an AI-text classifier. Such methods can be susceptible to adversarial attacks or paraphrasing. The state-of-the-art technique RADAR [16] leverages adversarial training to improve the robustness of the classifier. There are also commercial services for the detection of AI-generated texts. For example, **GPTZero**[43] employs a multi-step statistical detection process and utilizes a pre-trained commercial LLM to deliver the prediction.

Our approach, BiScope, is a statistical method that uniquely incorporates meticulous statistical feature extraction via a bi-directional calculation method to ensure its general effectiveness and robustness against paraphrasing across five text domains and five of the latest commercial LLMs. To evaluate BiScope, we compare it with nine existing detection methods, including Zero-shot Query [48; 32], LogRank [11], LRR [41], DetectGPT [35], RADAR [16], Raidar [32], OpenAI Detector [40], Binoculars [13], and GhostBuster [48], as well as with the most renowned commercial detection API, **GPTZero**. We surpass these baseline methods in both effectiveness and efficiency.

## 3 Methodology of BiScope

### Design Motivation

Although existing AI-generated text detection methods have been proven to be robust against texts generated by various open-source LLMs, their performance degrades in more complex and real-world scenarios, especially when dealing with the latest commercial LLMs and heterogeneous text genres. The degradation can be attributed to the following two reasons: **feature insufficiency** and **contextual heterogeneity**.

**Feature Insufficiency.** Existing methods focus on analyzing the difficulty a surrogate LLM experiences in predicting the next token given the preceding input text. For example, these methods use the rank or the probability of the next token as a metric or compare the discrepancy between the input text and the surrogate model's generation. Figure 1(a) presents the detection F1 score of a toy example that uses the average next token rank from Llama-2-7B as the feature and a random forest model as the classification model on both human text (in blue) and GPT-4-Turbo's text (in orange). The detection F1 score only reaches 0.55, which is just slightly better than random guesses. This indicates the lack of a clear separation using only next token ranks. One may argue that the random forest may not be powerful enough. However, we will show later that using additional features proposed in the paper, the same random forest configuration could achieve much better results.

We observe that the internals of the surrogate LLM when used to predict the subject text have much richer information that can be used. Figure 2 illustrates how LLM encodes information. The arrows and texts in green illustrate the information related to the next token, while the arrows and texts in gray show the information related to the preceding tokens. In the auto-regressive generation mode, the LLM receives the tokens preceding to the current position as the input and outputs the logits that contains its prediction for the next token. During this procedure, its internal states encode the preceding tokens (i.e., memorization) [47] while implicitly "planning" for the next token [51], namely, as observed by researchers in [51], the internal states show similarities to the encodings of future tokens. The output logits, which can be considered as a reduced representation of the model's internal states, also contains both the information to predict the next token and the information of preceding tokens. Existing methods focus only on the former by comparing the output logits with the expected next token. In this paper, we propose to consider the preceding token information as well. In particular, we hypothesize the following: **for human-written text, the surrogate LLM has a poor prediction for the next token and a strong memory of the previous token, reflected in the output logits, whereas the behaviors for LLM-generated text are the opposite.** Intuitively, it's like when we humans are unsure of what to say next, and the last word tends to stay in our minds.

To validate our hypothesis, we conducted an experiment in which we compared the current output logits with the preceding token for a piece of a given text, leveraging the same random forest as before. Figure 1(b) and (c) present the detection F1 score when only using the preceding token's rank and when using both the preceding and next tokens' ranks (to distinguish human and LLM texts), reaching 0.73 and 0.78 F1 scores, respectively, denoting a 0.2 F1 score improvement compared to using the next token information alone. Observe that in (d) for next token prediction, the AI texts (in orange) tend to have a smaller CE loss than human texts (in blue), indicating the LLM has a better prediction for the AI texts. In (e) for previous token memorization, the human texts tend to have a smaller loss than the AI texts, indicating the LLM has poorer memory for AI texts. Figures (a) and (b) and an additional example in Appendix B show a similar trend. These support our hypothesis.

Thus, in BiScope, we design a novel bi-directional cross-entropy loss computation method that computes the cross-entropy losses between the output logits and the expected next token, and between the output logits and the preceding token. Figure 1(d)-(f) illustrate the F1 scores when using the cross-entropy losses for next token, previous token, and both. Observe they achieve better results compared to using plain ranks, due to the more wealthy information encoded. An additional example using GPT-Neo-2.7B in Appendix B shows a similar trend.

**Contextual Heterogeneity.** In addition to insufficient feature utilization, we also observe that contextual heterogeneity significantly influences detection accuracy. Existing methods directly use surrogate LLMs to generate the given text in an auto-regressive manner, without incorporating any

Figure 1: Comparison of detection F1 scores when utilizing the rank Figure 2: Comparison and cross-entropy loss regarding next token, preceding token or both. of output logits information The surrogate detection model is Llama-2-7B.

additional information of the context (for the text). As such, given a prefix part of the text, the LLM may have a diverse set of possible completions, limiting the ability to separate human and LLM texts.

To alleviate this problem, we formalize our detection as a guided completion task, using a surrogate LLM to first summarize the entire input text. These text summaries are then used to guide the completion, providing complementary contextual information and making the features more robust.

### Overview of BiScope

The entire workflow of BiScope can be summarized in four key steps, shown in Figure 3.

Step 1: **Completion Prompt Generation.** In the first step, we initialize the detection as a guided text completion task. We use a surrogate LLM to summarize the input text and generate a text summary as a guidance. We then divide the input text into two segments. The first segment, along with a completion request, is utilized to construct a text completion request. The text summary guidance and the text completion request form a completion prompt. Details are presented in SS 3.3.

Step 2: **Loss Computation In Text Completion.** Given the completion prompt and the second segment of the input text from Step 1, we then calculate our novel bi-directional cross-entropy losses for the tokens in the second text segment using multiple open-source LLMs in parallel. Details are presented in SS 3.4. The use of multiple LLMs is to reduce the uncertainty.

Step 3: **Statistical Feature Extraction.** We vary the separation of the two segments at different positions of the subject text (e.g., one-fourth, half, and three-fourth of the whole length). For each setup, we collect the statistics of the bi-directional cross-entropy loss values. The statistics are concatenated to form a feature vector. More details and justifications are presented in SS 3.5.

Step 4: **Feature Classification.** In the final step, we use the concatenated feature vector to train a binary classifier, which determines whether the input text is human-generated or AI-generated. Further details are presented in SS 3.6.

### Completion Prompt Generation

In BiScope, we calculate the bi-directional cross-entropy losses within a guided text completion scenario. This scenario involves providing a text summary guidance and a short sub-string of the input text to force LLMs to generate the remainder of the text. Specifically, to alleviate the impact of contextual heterogeneity during text generation, we first utilize a surrogate LLM to summarize the entire input text and obtain a summary as guidance. We then divide the input text into two segments (e.g., the first 10% and the remaining 90%). The first segment, referred to as Input Text Segment 1, serves as the sub-string in a text completion request, while the second segment, referred to as Input Text Segment 2, is used as the completion ground-truth in SS 3.4. By appending the text completion request after the summary guidance, we construct a completion prompt shown as follows:

Given the summary:  {Text Summary Guidance}  Complete the following text:  {Input Text Segment 1}

Figure 3: Overview of BiScope. Arrows and texts in brown indicate text summarization.

The text in black indicates the text completion request, while the text in brown indicates the guidance, which is summarized using the following prompt:

Write a title for this text: {Input Text}

The summary contains the aggregated contextual information of the entire text, providing complementary guidance to the LLM completion, in addition to the first segment. To balance the detection accuracy and efficiency, BiScope can also disable this text summary procedure to achieve faster AI-generated text detection with satisfactory accuracy.

### Loss Computation In Text Completion

After crafting the completion prompt, we then feed it into multiple open-source LLMs in parallel to obtain multiple output logits that correspond to the Input Text Segment 2 from SS 3.3 using the _teacher forcing pattern_[25], which feeds the ground-truth token prefixes (Input Text Segment 2) to compute the output logits at each token position. These output logits can be used to measure how likely the LLMs predict the next token given its prefix in the subject text and how well the LLMs memorize the preceding token, according to our discussion in SS 3.1. We hence propose a bi-directional cross-entropy calculation method in BiScope, which consists of both forward and backward cross-entropy calculations. The forward cross-entropy (\(\mathcal{FCE}\)) calculation is identical to the commonly used cross-entropy in most LLM training processes, utilizing the output logits and the next ground-truth token to capture the output logits' next-token-related information. In contrast, the backward cross-entropy (\(\mathcal{BCE}\)) is calculated between the output logits and the immediate preceding input token, capturing how much the logits memorizes the preceding token. The detailed \(\mathcal{FCE}\) and \(\mathcal{BCE}\) calculations at token position \(i\) with LLM \(\mathcal{M}\) are shown in Equation 1:

\[\mathcal{FCE}_{i}=-\sum_{z=1}^{||\mathcal{V}||}\tilde{P}_{i+1}^{z}\cdot\log( \mathcal{P}_{i}^{z}),\quad\mathcal{BCE}_{i}=-\sum_{z=1}^{||\mathcal{V}||} \tilde{P}_{i}^{z}\cdot\log(\mathcal{P}_{i}^{z}) \tag{1}\]

where \(\mathcal{V}\) indicates the vocabulary of the LLM, \(\mathcal{P}_{i}\) denotes the soft-maxed output logits from \(\mathcal{M}\) at position \(i\) of the generated text (when given the preceding tokens from the completion prompt). \(\tilde{\mathcal{P}}_{i}\) indicates the ground-truth token encoding at the same position.

### Statistical Feature Extraction

The aforementioned bi-directional cross-entropy loss values may have different characteristics when the text is partitioned at different positions. Intuitively, when the text is partitioned at a ratio of 1:9, meaning that we use the first 10% of the text to perform the completion, there tends to be a lot more uncertainty compared to a partition of 9:1. A naive design is to fix a partition ratio. However, finding the most effective partition is difficult. Another design is to use the loss values computed at all positions. However, it can hardly deal with length variations of input texts. Therefore, our design is to partition the whole text into \(n\) segments (\(n=10\) in our implementation). For each segment, we collect the bidirectional loss value statistics over all the positions with the segment, including the _mean_, _maximum_, _minimum_, and _standard deviation_ values. This allows us to align texts of various lengths and leverage the later classification to figure out the best partition positions (through learning). Additionally, this multi-segment analysis requires only a one-time inference of the input text, as the loss calculation at each position is independent of the others via teacher forcing. This allows BiScope to obtain various and sufficient features with high efficiency.

### Feature Classification

In the final step, we concatenate all the statistical features of both the \(\mathcal{FCE}\) and \(\mathcal{BCE}\) vectors from all the detection LLMs into a one-dimensional feature vector, which is then used to train a binary classifier to perform the classification. Due to the generality of these features, the binary classifier can be directly used to detect unseen data, whether from unknown LLMs or unfamiliar text domains.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

baselines with a \(0.29\) average F1 score increase. Additionally, existing baselines experience an overall \(0.03\) F1 detection score drop compared to their performance on the normal dataset. In contrast, our BiScope performs even better on the paraphrased dataset, with an overall \(0.02\) average F1 score increase. Under the out-of-distribution setting, we train the classifiers for all the methods on the normal dataset, while testing them on the paraphrased dataset, exploring the generality of the detection against unseen paraphrased data. Our BiScope outperforms existing baselines with an average \(0.29\) F1 detection score increase.

### Comparison with The Latest Commercial Detection Method

We also compare BiScope with the latest version (2024-01-09) of the most renowned commercial AI-generated text detection API, GPTZero [43], across all five datasets, as shown in Figure 6. BiScope outperforms GPTZero in 72% of the cases. Specifically, BiScope achieves a \(0.02\), \(0.01\), and \(0.01\) average F1 detection score increase on the Arxiv, Essay, and Creative datasets, respectively. On the Yelp dataset, BiScope's F1 detection score is \(0.04\) lower than GPTZero's. However, on the code dataset, BiScope performs significantly better than GPTZero, achieving a \(0.19\) average F1 score improvement, demonstrating BiScope's superior generality from natural language to code. Note that GPTZero's detection model is pre-trained on millions of data points, while our BiScope's classifier is trained on at most \(4,000\) test samples for each case.

### Efficiency Analysis

To address real-world challenges, efficiency is crucial for detection methods. We compared BiScope with nine baselines across five datasets, detailing average processing times for a single sample in Figure 8. RADAR is the fastest at \(0.01\)s per sample, while its adversarial training overhead may be considerable. Processing times for the other baselines are as follows: zero-shot query at \(0.32\)s, LogRank at \(0.05\)s, LRR at \(0.10\)s, DetectGPT at \(15.95\)s, Raidar at \(27.42\)s, OpenAI Detector at \(0.03\)s, Binoculars at \(0.19\)s, and GhostBuster at \(0.37\)s. Zero-shot query, LogRank, LRR, and OpenAI Detector are much quicker than DetectGPT, Raidar, Binoculars, and GhostBuster but offer lower detection performance and robustness. Our BiScope processes a sample in \(0.14\)s without summary guidance, matching the real-time levels of zero-shot query, LogRank, and LRR, while improving detection F1 score by over \(0.30\). With summary guidance, BiScope's processing time increases to \(1.35\)s per sample, still 12 to 20 times faster than DetectGPT and Raidar, and achieves the highest detection score.

### Ablation Study

We further evaluate the importance of each component in BiScope with two categories of ablation experiments. Since BiScope utilizes six open-source LLMs in parallel and ensemble their features, we first investigate the contribution of each LLM's feature in Figure 7 individually. Then, we further explore the contribution of BiScope's \(\mathcal{FCE}\) and \(\mathcal{BCE}\) losses respectively, compared with their aggregated performance, shown as Figure 8. More detailed results are shown in Appendix E. We also present more detailed ablation study on the impact of different segmentation strategies in multi-point splitting (Appendix E.3), and the impact of the completion prompt (Appendix E.4), in Appendix E.

**BiScope's Performance with Different Base Models.** We individually test all the detection base models in BiScope both with and without the summary procedure, including Gemma-2B, Gemma-7B, Llama-2-7B, Mistral-7B, Llama-3-8B, and Llama-2-13B. All the detection models help BiScope achieve over a \(0.84\) overall F1 detection score across all five datasets, demonstrating high consistency across different detection base models. As the size of the detection base model increases, BiScope performs progressively better, with F1 scores improving from \(0.84\) to \(0.95\).

**Importance of \(\mathcal{FCE}\) and \(\mathcal{BCE}\) in BiScope's Detection.** To demonstrate the rationality of the proposed bi-directional cross-entropy losses, we also evaluate BiScope by only using either \(\mathcal{FCE}\) or \(\mathcal{BCE}\) losses, and using both of them with the Llama-2-7B model, as shown in Figure 8. When using both the \(\mathcal{FCE}\) and \(\mathcal{BCE}\) losses, BiScope achieves the best average F1 detection score across the five datasets, which is \(0.94\), highlighting the necessity of combining both \(\mathcal{FCE}\) and \(\mathcal{BCE}\) loss features. When only using \(\mathcal{FCE}\) or \(\mathcal{BCE}\), BiScope reaches \(0.86\) and \(0.93\) average F1 detection scores, respectively, indicating that \(\mathcal{BCE}\) is more discriminative than \(\mathcal{FCE}\).

## 5 Limitations and Future Work

Our BiScope can achieve over a \(0.95\) F1 detection score across five datasets generated by the five latest commercial LLMs, both with and without intentional paraphrasing, illustrating the importance of the preceding token information in the output logits. However, in the OOD cross-dataset setting, there is a noticeable \(>0.10\) detection F1 score drop, highlighting the challenges in this setting. Thus, there is still room for future research to achieve more effective and robust detection in few-shot and cross-dataset settings, where the training set contains fewer samples and the test set comes from different text domains. Additionally, exploring ways to further exploit preceding token information and combine it with next token information should also be a future direction in the field of AI-generated text detection.

## 6 Conclusion

Existing methods to differentiate AI-generated texts from human-generated texts often analyze the difficulty for a surrogate LLM to generate the next token based on previous tokens from the text. We propose a more discriminative approach via a novel bi-directional cross-entropy calculation method, leveraging both the preceding token information and the next token information in the output logits. We integrate this method into a four-step detection pipeline, BiScope, which consists of Completion Prompt Generation, Loss Computation in Text Completion, Statistical Feature Extraction, and Feature Classification. We evaluate BiScope on five datasets, including both natural language and code, against six existing detection methods. BiScope surpasses all these methods, improving the average F1 detection score by \(0.30\) and also outperforming the well-known commercial API - GPTZero in 72% cases, while maintaining a real-time processing speed of less than 200ms per sample.

## Acknowledgments and Disclosure of Funding

We are grateful to the Center for AI Safety for providing computational resources. This work was funded in part by the National Science Foundation (NSF) Awards SHF-1901242, SHF-1910300, Proto-OKN 2333736, IIS-2416835, DARPA VSPELLS - HR001120S0058, IARPA TrojAI W911NF-19-S0012, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altschmidt, Sam Altman, Shyamal Anadkat, et al. Opt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Anthropic. Claude 3 api. _[https://www.anthropic.com/news/claude-3-family_](https://www.anthropic.com/news/claude-3-family_), 2023.
* [3] Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and Arthur Szlam. Real or fake? learning to discriminate machine from human generated text. _arXiv preprint arXiv:1906.03351_, 2019.
* [4] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In _International Conference on Learning Representations (ICLR)_, 2024.
* [5] Amrita Bhattacharjee and Huan Liu. Fighting fire with fire: can chatgpt detect ai-generated text? _ACM SIGKDD Explorations Newsletter_, 25(2):14-21, 2024.
* [6] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021.
* [7] Jack T Brassil, Steven Low, Nicholas F Maxmembuk, and Lawrence O'Gorman. Electronic marking and identification techniques to discourage document copying. _IEEE Journal on Selected Areas in Communications (JSAC)_, 13(8):1495-1504, 1995.
* [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [9] Xiuyuan Cheng and Alexander Cloninger. Classification logit two-sample testing by neural networks for differentiating near manifold densities. _IEEE Transactions on Information Theory_, 68(10):6631-6662, 2022.
* [10] Debby RE Cotton, Peter A Cotton, and J Reuben Shipway. Chatting and cheating: Ensuring academic integrity in the era of chatgpt. _Innovations in Education and Teaching International_, 61(2):228-239, 2024.
* [11] Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of generated text. In _Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)_, 2019.
* [12] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _arXiv preprint arXiv:2301.07597_, 2023.
* [13] Abhimanyu Hans, Avi Schwarzschild, Valeria Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Spotting llms with binoculars: Zero-shot detection of machine-generated text. In _International Conference on Machine Learning (ICML)_, 2024.
* [14] Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. Mgtbench: Benchmarking machine-generated text detection. _arXiv preprint arXiv:2303.14822_, 2023.
* [15] Abe Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and Yulia Tsvetkov. Semstamp: A semantic watermark with paraphrastic robustness for text generation. In _Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)_, 2024.
* [16] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Radar: Robust ai-text detection via adversarial learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.

* [17] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In _Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)_, 2020.
* [18] Ganesh Jawahar, Muhammad Abdul Mageed, and VS Laks Lakshmanan. Automatic detection of machine generated text: A critical survey. In _International Conference on Computational Linguistics (COLING)_, 2020.
* [19] Mohan S Kankanhalli and KF Hau. Watermarking of electronic text documents. _Electronic Commerce Research_, 2:169-187, 2002.
* [20] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In _International Conference on Machine Learning (ICML)_, 2023.
* [21] Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2024.
* [22] Sarah Kreps, R Miles McCain, and Miles Brundage. All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation. _Journal of Experimental Political Science_, 9(1):104-117, 2022.
* [23] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [24] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. _arXiv preprint arXiv:2307.15593_, 2023.
* [25] Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* [26] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. In _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2023.
* [27] Jiayi Liang, Xi Zhang, Yuming Shang, Sanchuan Guo, and Chaozhuo Li. Clean-label poisoning attack against fake news detection models. In _IEEE International Conference on Big Data (BigData)_, 2023.
* [28] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J Sutherland. Learning deep kernels for non-parametric two-sample tests. In _International Conference on Machine Learning (ICML)_, 2020.
* [29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [30] Chung Kwan Lo. What is the impact of chatgpt on education? a rapid review of the literature. _Education Sciences_, 13(4):410, 2023.
* [31] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. In _International Conference on Learning Representations (ICLR)_, 2016.
* [32] Chengzhi Mao, Carl Vondrick, Hao Wang, and Junfeng Yang. Raidar: generative ai detection via rewriting. _International Conference on Learning Representations (ICLR)_, 2024.
* [33] Hope McGovern, Rickard Stureborg, Yoshi Suhara, and Dimitris Alikaniotis. Your large language models are leaving fingerprints. _arXiv preprint arXiv:2405.14057_, 2024.
* [34] Niloofar Mireshgballah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better zero-shot machine-generated text detectors. In _Conference of the European Chapter of the Association for Computational Linguistics (EACL)_, 2024.
* [35] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In _International Conference on Machine Learning (ICML)_, 2023.
* [36] OpenAI. Gpt-3.5 turbo api. _[https://platform.openai.com/docs/models/gpt-3-5-turbo_](https://platform.openai.com/docs/models/gpt-3-5-turbo_), 2023.

* [37] Mike Perkins. Academic integrity considerations of ai large language models in the post-pandemic era: Chatgpt and beyond. _Journal of University Teaching & Learning Practice_, 20(2):07, 2023.
* [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 21(140):1-67, 2020.
* [39] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _arXiv preprint arXiv:2303.11156_, 2023.
* [40] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social impacts of language models. _arXiv preprint arXiv:1908.09203_, 2019.
* [41] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. _Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2023.
* [42] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [43] Edward Tian and Alexander Cui. Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods", 2023.
* [44] Umut Topkara, Mercan Topkara, and Mikhail J Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In _The 8th Workshop on Multimedia and Security_, 2006.
* [45] Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Sergey Nikolenko, Evgeny Burnaev, Serguei Barannikov, and Irina Piontkovskaya. Intrinsic dimension estimation for robust detection of ai-generated texts. _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [46] Honai Ueoka, Yugo Murawaki, and Sadao Kurohashi. Frustratingly easy edit-based linguistic steganography with a masked language model. In _The North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)_, 2021.
* [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [48] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten by large language models. _arXiv preprint arXiv:2305.15047_, 2023.
* [49] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on nlp models. In _Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)_, 2021.
* [50] Yuxia Wang, Jonbek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, et al. M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection. _arXiv preprint arXiv:2305.14902_, 2023.
* [51] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan ahead for future tokens? _arXiv preprint arXiv:2404.00859_, 2024.
* [52] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu. Watermarking text generated by black-box language models. _arXiv preprint arXiv:2305.08883_, 2023.
* [53] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. _International Conference on Learning Representations (ICLR)_, 2024.
* [54] Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, and Mingkui Tan. Detecting machine-generated texts by multi-population aware optimization for maximum mean discrepancy. In _International Conference on Learning Representations (ICLR)_, 2024.
* [55] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: a survey on hallucination in large language models. _arXiv preprint arXiv:2309.01219_, 2023.

To further illustrate our BiScope with more details, we present the following materials in the Appendix, shown as follows:

* Appendix A: Hyper-parameter settings for our method and all the baselines.
* Appendix B: An additional motivation example using a smaller LLM.
* Appendix C: More statistical details of the dataset crafted and utilized in the paper.
* Appendix D: More detailed results on both normal and paraphrased data under OOD setting.
* Appendix E: More details of the ablation study.

## Appendix A Hyper-parameter Settings

We use the default best-performance settings for all the baselines. Specifically, we use GPT-Neo-2.7B [6] as the surrogate model in LogRank and LRR, and as the scoring model in DetectGPT, where we use T5-3B [38] as the mask-filling model. For RADAR, we use its officially released detection model, which is a pre-trained RoBERTa-Large [29] model. For Zero-shot Query and Radiar, we use GPT-3.5-Turbo as the query model and the rewriting model. For other baselines, we strictly follow their official implementations with their best configurations. For our BiScope, we utilize six open-source LLMs in parallel, including Gemma-2B, Gemma-7B, Llama-2-7B, Mistral-7B, Llama-3-8B, and Llama-2-13B. We also test BiScope with (BiScope *) and without (BiScope) text summary guidance. For better reproducibility, we recommend using either Llama2-7B or Llama2-13B as the surrogate model, while the ensemble of more surrogate models is always welcomed. For the text split method, we recommend splitting the text at every 10% length, as used in our paper.

## Appendix B Additional Motivation Example

Figure 9 presents an additional example using a smaller language model: GPT-Neo-2.7B. Specifically, Figure 9(a) and (d) show the detection F1 score when only using the next token's rank and cross-entropy loss for detection. The F1 detection scores are less than 0.60 in both cases. However, when we use the preceding token's rank or cross-entropy loss as the feature, shown in Figure 9(b) and (e), the detection F1 scores increase to 0.63 and 0.76 respectively, indicating the higher reference value of the preceding token information. Figure 9(c) and (f) also show that using both the preceding token information and the next token information in the output logits achieves the best detection performance, with overall detection F1 scores of 0.67 and 0.82.

Moreover, when considering the preceding token information, AI text has a smaller rank score or higher cross-entropy loss, representing worse memorization of the AI text in the output logits.

Figure 9: Comparison between the detection F1 scores when utilizing the rank and cross-entropy loss regarding next token, last token or both. The detection surrogate model is GPT-Neo 2.7B.

Conversely, when considering the next token information, the result is the opposite, showing better prediction for AI text. This trend aligns with the results in Figure 1, demonstrating the generality of our observation.

## Appendix C Additional Details about Datasets

For all datasets, we reuse their human-generated data and craft AI-generated text using five of the latest commercial LLMs: GPT-3.5-Turbo [36], GPT-4-Turbo [1], Claude-3-Sonnet [2], Claude-3-Opus [2], and Gemini-1.0-Pro [42]. Thus, for each dataset, we have one piece of human-generated data and five pieces of AI-generated data. Additionally, we also create a corresponding paraphrased dataset using similar paraphrasing prompts from previous studies [32] on our newly crafted dataset to evaluate the robustness of the detection methods.

Table 2 presents more detailed information about the datasets used in our paper, for both the normal dataset and the paraphrased dataset. For the short natural language datasets (i.e., Arxiv and Yelp), the average text lengths are 187 and 477, respectively, with minimal lengths of 101 and 10. In contrast, the average text lengths of the long natural language datasets (i.e., Creative and Essay) are 2860 and 3899, which are 6-15 times larger than the short natural language dataset's sample length. The code dataset contains approximately 164 pieces of Python code for human-generated text and AI-generated text from each LLM, with an average length of 983, a minimum length of 41, and a maximum length of 1993. For all the datasets, the human-generated data and AI-generated data share similar lengths, preventing simple length-based detection.

## Appendix D Additional Comparison Results under OOD Setting

Table 3 illustrates more detailed OOD results on both the normal and paraphrased datasets across five datasets. Under the cross-model setting on the normal dataset, BiScope outperforms existing baselines in \(21\) out of \(25\) cases. Specifically, BiScope reaches over a \(0.95\) detection F1 score against five generative LLMs on the Arxiv, Creative, and Essay datasets, while the detection F1 scores of existing baselines are usually less than \(0.90\). On the Yelp dataset, BiScope achieves over a \(0.90\) detection F1 score against all five generative models, while the five baselines reach less than a \(0.85\) detection F1 score in most cases. On the code dataset, our BiScope outperforms all baselines except for Radar, achieving more than a \(0.15\) detection F1 score increase. Under the cross-dataset setting on the normal dataset, our BiScope performs worse than under the cross-model setting, outperforming existing methods in only \(9\) out of \(25\) cases. However, in most cases, BiScope is the second-best detection method, only trailing RADAR, which uses a detection model pre-trained on texts from multiple domains. As for the OOD setting on the paraphrased dataset, BiScope outperforms the five baselines in \(17\) out of \(20\) cases, achieving over a \(0.90\) detection F1 score compared to the \(<0.75\) average F1 score of the baselines.

\begin{table}
\begin{tabular}{c|c|c c c c|c c c c c} \hline \hline \multicolumn{1}{c}{} & \multicolumn{4}{c|}{Normal Dataset} & \multicolumn{4}{c}{Paraphrased Dataset} \\ \hline  & Data Type & Dataset Size & Average Len. & Min Len. & Max Len. & Median Len. & Dataset Size & Average Len. & Min Len. & Max Len. & Median Len. \\ \hline \multirow{4}{*}{Arxiv} & Human & 350 & 736.7 & 132 & 1736 & 7150 & - & - & - & - & - \\  & Machine & 1750 & 787.1 & 101 & 1701 & 810.5 & 1400 & 875.8 & 174 & 1874 & 931.5 \\  & All & 2100 & 787.0 & 101 & 1736 & 799.5 & 1400 & 875.8 & 174 & 1874 & 931.5 \\ \hline \multirow{4}{*}{Code} & Human & 164 & 631.5 & 132 & 1993 & 572.0 & - & - & - & - & - \\  & Machine & 819 & 411.3 & 41 & 1908 & 352.0 & 656 & 493.6 & 13 & 2333 & 382.5 \\  & All & 983 & 489.7 & 41 & 1993 & 387.0 & 656 & 493.6 & 13 & 2333 & 382.5 \\ \hline \multirow{4}{*}{Yelp} & Human & 2000 & 554.9 & 37 & 4959 & 407.0 & - & - & - & - & - \\  & Machine & 9740 & 461.1 & 10 & 2548 & 414.0 & 8000 & 586.5 & 54 & 2593 & 537.0 \\  & All & 11740 & 477.1 & 10 & 4895 & 413.0 & 8000 & 586.5 & 54 & 2593 & 537.0 \\ \hline \multirow{4}{*}{Easy} & Human & 1000 & 4249.9 & 1276 & 41470 & 3301.5 & - & - & - & - & - \\  & Machine & 4897 & 3827.7 & 515 & 21094 & 3486.0 & 3999 & 3666.8 & 129 & 19878 & 3284.0 \\  & All & 8987 & 3899.3 & 515 & 41470 & 3489.0 & 3999 & 3666.8 & 129 & 19878 & 3284.0 \\ \hline \multirow{4}{*}{Creative} & Human & 1000 & 2899.0 & 499 & 9933 & 2462.5 & - & - & - & - & - \\  & Machine & 4840 & 2851.9 & 176 & 13716 & 2620.0 & 4000 & 2924.4 & 85 & 16812 & 2674.5 \\ \cline{1-1}  & All & 3840 & 2860.0 & 176 & 13716 & 2588.5 & 4000 & 2924.4 & 85 & 16812 & 2674.5 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Statistical details of the five datasets used in our paper.

## Appendix E Additional Details of Ablation Study

### Results with Different Open-source LLMs

Table 4 shows additional details when different detection models are used by BiScope on both the normal and paraphrased datasets. In most cases, the ensemble results are better than the F1 scores derived using a single detection model. However, at least 90% of the detection performance can be preserved in most single-detection-model settings. Additionally, the detection F1 score consistently increases with the increasing model size of the detection model.

Among all the detection models, the Llama-2 series performs the best, outperforming its updated version, Llama-3. In contrast, the Gemma series performs the worst. A possible reason is that the Gemma and Llama-3 series models are still in their early development stages and are not fully capable of handling the AI-generated text detection task.

\begin{table}
\begin{tabular}{l l|c c c c|c c c c c|c c c c} \hline \hline  & & \multicolumn{4}{c|}{Normal-Loss Model} & \multicolumn{4}{c|}{Normal-Cross Dataset} & \multicolumn{4}{c}{Paraphrased-OOD} \\ \hline \multirow{3}{*}{Method} & \multicolumn{3}{c|}{GIFT3-5} & \multicolumn{3}{c|}{GIFT4-5} & \multicolumn{3}{c|}{Gault-3} & \multicolumn{3}{c|}{Gault-3} & \multicolumn{3}{c|}{Gault-3} & \multicolumn{3}{c|}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} & \multicolumn{3}{c}{Gault-3} \\  & \multicolumn{3}{c|}{Turbo} & \multicolumn{3}{c|}{Turbo} & \multicolumn{3}{c|}{Souter} & \multicolumn{3}{c|}{Ops} & \multicolumn{3}{c|}{I-0.10pu} & \multicolumn{3}{c|}{Turbo} & \multicolumn{3}{c|}{Souter} & \multicolumn{3}{c|}{Ops} & \multicolumn{3}{c|}{I-0.10pu} & \multicolumn{3}{c|}{Turbo} & \multicolumn{3}{c}{Turbo} & \multicolumn{3}{c}{Souter} & \multicolumn{3}{c}{Ops} \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & Log Rank & 0.6723 & 0.0703 & 0.7406 & 0.6805 & 0.6597 & 0.6727 & 0.5799 & 0.6476 & 0.6522 & 0.5590 & 0.5814 & 0.2821 & 0.2604 & 0.3382 \\  & LRR & 0.7920 & 0.7187 & 0.7589 & 0.7156 & 0.6742 & 0.6866 & 0.5238 & 0.6302 & 0.6650 & 0.5952 & 0.081 & 0.2089 & 0.2904 & 0.3537 \\  & DetectorGPT & 0.6677 & 0.6631 & 0.6637 & 0.6637 & 0.6637 & 0.6845 & 0.6962 & 0.7343 & 0.7421 & 0.0621 & 0.5774 & 0.6622 & 0.6609 \\  & RADAR & 0.5721 & 0.7949 & 0.7799 & 0.7854 & 0.7966 & 0.3315 & **0.7638** & 0.8037 & 0.8116 & **0.8071** & 0.9223 & 0.6546 & 0.7128 & 0.6654 \\  & Radar & 0.5700 & 0.6795 & 0.7604 & 0.6946 & 0.7693 & 0.5010 & 0.5766 & 0.6881 & 0.6888 & 0.7144 & 0.8039 & 0.7195 & 0.6266 & 0.7297 \\  & OpenAI Detector & 0.5239 & 0.6667 & 0.6670 & 0.6673 & 0.6671 & 0.6673 & 0.6653 & 0.6841 & 0.6679 & 0.6778 & 0.6916 & 0.6654 & 0.6608 & 0.6641 \\  & Binoculars & 0.9396 & 0.9272 & 0.9339 & 0.8968 & 0.8818 & **0.8340** & 0.7495 & **0.8489** & **0.8814** & 0.7766 & 0.4284 & 0.4547 & 0.7406 & 0.7105 \\  & GhostBluwer & 0.7684 & 0.3700 & 0.9617 & **0.9541** & 0.9702 & 0.8851 & 0.6914 & 0.7281 & 0.0991 & **0.7692** & **0.9541** & **0.7796** & **0.9562** & **0.9514** \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & BiScope & 0.9683 & 0.9496 & 0.9665 & 0.9126 & 0.9167 & 0.8706 & 0.6642 & 0.7548 & 0.796 & 0.7991 & 0.8610 & 0.7461 & 0.8776 & 0.9298 \\  & BiScope\({}^{\star}\) & **0.9599** & **0.9607** & **0.9881** & 0.9518 & **0.9510** & 0.8250 & 0.694 & 0.8285 & 0.7875 & 0.7930 & 0.9044 & 0.7678 & 0.8941 & 0.9296 \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & Log Rank & 0.6732 & 0.6722 & 0.6309 & 0.6345 & 0.6774 & 0.6695 & 0.6763 & 0.6836 & 0.6841 & 0.6427 & 0.6732 & 0.6417 & 0.5546 & 0.6318 \\  & LRR & 0.6600 & 0.6601 & 0.6475 & 0.6583 & 0.6686 & 0.6667 & 0.6664 & 0.6641 & 0.6662 & 0.6644 & 0.6163 & 0.6564 \\  & DetectorGPT & 0.6606 & 0.6417 & 0.6738 & 0.6254 & 0.6976 & 0.7124 & 0.6931 & 0.7363 & 0.7534 & 0.7998 & 0.6399 & 0.6345 & 0.7029 & 0.7085 \\  & RADAR & 0.7129 & 0.7180 & 0.7303 & 0.7164 & 0.6964 & 0.8760 & **0.8328** & 0.8129 & 0.7383 & 0.7845 & 0.7069 & 0.6982 & 0.6620 & 0.7023 \\  & Radar & 0.8339 & 0.8499 & 0.5940 & 0.8794 & 0.8680 & 0.4335 & 0.7538 & 0.7879 & 0.7699 & 0.7886 & 0.6993 & 0.7493 \\  & OpenAI Detector & 0.5876 & 0.6587 & 0.6587 & 0.6425 & 0.6267 & 0.6853 & 0.669 & 0.6663 & 0.6055 & 0.5299 & 0.7243 & 0.6668 & 0.6668 & 0.5674 \\  & Binoculars & 0.7076 & 0.6609 & 0.7028 & 0.6609 & 0.7188 & 0.8300 & 0.6688 & **0.8973** & **0.8306** & 0.8665 & 0.6193 & 0.6327 & 0.6705 & 0.7148 \\  & GhostBluwer & 0.7727 & 0.8176 & 0.8074 & 0.8076 & 0.7821 & 0.7372 & 0.7522 & 0.5575 & 0.7167 & 0.4294 & 0.7900 & 0.8359 & 0.8546 & 0.8754 \\ \hline \multirow{7}{*}{\begin{tabular}{} \end{tabular} } & BiScope & 0.9010 & 0.9257 & 0.8300 & 0.8908 & 0.9174 & **0.9189** & 0.8130 & 0.8309 & 0.8441 & **0.8890** & **0.9046** & **0.9552** & **0.9787** & **0.9736** \\  & BiScope\({}^{\star}\) & **0.9134** & **0.9266** & **0.8956** & **0.9499** & **0.9234** & 0.8912 & 0.8056 & 0.8670 & 0.8460 & 0.8175 & 0.9017 & 0.9517 & **0.9787** & 0.9787 & 0.9700 \\ \hline \multirow{7}{*}{
\begin{tabular}{} \end{tabular} } & Log Rank & 0.7408 & 0.7368 & 0.7386 & 0.8239 & 0.7906 & 0.8485 & 0.5514 & 0.5943 & 0.5641 & 0.5939 & 0.6205 & 0.5785 & 0.5132 & 0.3257 & 0.5324 & 0.7138 \\  & LRR & 0.6965 & 0.6864 & 0.7572 & 0.7277 & 0.7786 & 0.5647 & 0.5837 & 0.5935 & 0.5940 & 0.5601 & 0.5300 & 0.3255 & 0.365 & 0.5024 \\  & DetectorGPT & 0.7848 & 0.7486 & 0.7466 & 0.7592 & 0.7458 & 0.7800 & 0.6403 & 0.5690 & 0.593

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

### Detailed Contribution Comparison of Forward and Backward Cross-entropy Losses

Table 5 shows a more concrete comparison between the contributions of \(\mathcal{FCE}\) and \(\mathcal{BCE}\) to BiScope's detection effectiveness. In \(21\) of \(25\) cases, \(\mathcal{BCE}\) is more discriminative compared with \(\mathcal{FCE}\) (especially on code dataset), providing sufficient justifications for BiScope that introduce the preceding token information into the detection. Besides, In \(16\) of the \(25\) cases, the combination of \(\mathcal{FCE}\) and \(\mathcal{BCE}\) outperforms the \(\mathcal{FCE}\) only and \(\mathcal{BCE}\) only versions, supporting the necessity to use the bi-directional cross-entropy loss calculation method.

### Impact of Different Segmentation Strategies in Multi-point Splitting in BiScope

Table 6 presents the ablation results with different segmentation strategies in the multi-point splitting of BiScope. We tested three strategies: splitting at every 50% text length, every 25% text length, and every 10% text length (as used in our paper). The results indicate that a more fine-grained splitting interval generally improves BiScope's performance. However, in a small number of cases, a smaller splitting interval may degrade performance. We choose to use 10% as it achieves the highest detection scores in most cases while reaching low degradation in corner cases.

### Impact of The Completion Prompt in BiScope

Table 7 presents the comparison results when using and not using the completion prompt in BiScope. The results show that in 25 of 45 cases, using the completion prompt performs better. Additionally, the completion prompt is more compatible with the summary procedure. Thus, we chose to use the completion prompt in BiScope.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We summarize the procedure of our method and highlight both of our method and dataset contribution in the abstract and introduction sections. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We present the limitation of our work and potential future research directions in SS 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: We use empirical examples and results to justify our method. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We introduce the details of our method at each step in SS 3, and the hyperparameters we used in Appendix A. The code will be available at [https://github.com/MarkGHX/BiScope](https://github.com/MarkGHX/BiScope). Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will open-source our data and code at [https://github.com/MarkGHX/BiScope](https://github.com/MarkGHX/BiScope). Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We detailed all the dataset, metrics, and evaluation settings in SS 4. We also list the hyper-parameters we used for our methods and all the baselines in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not use error bars, but we use the F1 score to measure our method and all the baselines instead. Additionally, we also conduct a detailed ablation study for each specific case in our paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide detailed hyper-parameter settings and present the execution time for our method and all the baselines. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our paper is aligned with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We present the advantages and limitations of our method, and introduce future directions at SS5. We also compare our method with the latest commercial method, showing the positive impacts on the open-source community. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not involved in misusing. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all the baseline methods and data sources in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We present a detailed information about our new dataset in Appendix C. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.