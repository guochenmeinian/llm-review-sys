# Closing the Computational-Statistical Gap in Best Arm Identification for Combinatorial Semi-bandits

 Ruo-Chun Tzeng

EECS

KTH, Stockholm, Sweden

rctzeng@kth.se

&Po-An Wang

EECS

KTH, Stockholm, Sweden

wang9@kth.se

Alexandre Proutiere

EECS and Digital Futures

KTH, Stockholm, Sweden

alepro@kth.se

&Chi-Jen Lu

Institute of Information Science

Academia Sinica, Taiwan

cjlu@iis.sinica.edu.tw

###### Abstract

We study the best arm identification problem in combinatorial semi-bandits in the fixed confidence setting. We present Perturbed Frank-Wolfe Sampling (P-FWS), an algorithm that (i) runs in polynomial time, (ii) achieves the instance-specific minimal sample complexity in the high confidence regime, and (iii) enjoys polynomial sample complexity guarantees in the moderate confidence regime. To the best of our knowledge, even for the vanilla bandit problems, no algorithm was able to achieve (ii) and (iii) simultaneously. With P-FWS, we close the computational-statistical gap in best arm identification in combinatorial semi-bandits. The design of P-FWS starts from the optimization problem that defines the information-theoretical and instance-specific sample complexity lower bound. P-FWS solves this problem in an online manner using, in each round, a single iteration of the Frank-Wolfe algorithm. Structural properties of the problem are leveraged to make the P-FWS successive updates computationally efficient. In turn, P-FWS only relies on a simple linear maximization oracle.

## 1 Introduction

An efficient method to design statistically optimal algorithms solving active learning tasks (e.g., regret minimization or pure exploration in bandits and reinforcement learning) consists in the following two-step procedure. The first step amounts to deriving, through change-of-measure arguments, tight information-theoretical fundamental limits satisfied by a wide class of learning algorithms. These limits are often expressed as the solution of an optimization problem, referred in this paper to as the _lower-bound problem_. Interestingly, this solution specifies the instance-specific optimal exploration process: it characterizes the limiting behavior of the adaptive sampling rule any statistically optimal algorithm should implement. In the second step, the learning algorithm is designed so that its exploration process approaches the solution of the lower-bound problem. This design yields statistically optimal algorithms, but typically requires to repeatedly solve the lower-bound problem. This method has worked remarkably well for simple learning tasks such as regret minimization or best-arm identification with fixed confidence in classical stochastic bandits [14, 1, 1], but also in bandits whose arm-to-average reward function satisfies simple structural properties (e.g., Lipschitz, unimodal) [12, 2].

The method also provides a natural way of studying the computational-statistical gap [13] for active learning tasks. Indeed, if solving the lower-bound problem in polynomial time is possible, onemay hope to devise learning algorithms that are both statistically optimal and computationally efficient. As of now, however, the computational complexity of the lower-bound problem remains largely unexplored, except for simple learning tasks. For example, in the case of best policy identification in tabular Markov Decision Processes, the lower-bound problem is non-convex [1] and its complexity and approximability are unclear.

In this paper, we leverage the aforementioned two-step procedure to assess the computational-statistical gap for the best arm identification in combinatorial semi-bandits in the fixed confidence setting. We establish that, essentially, this gap does not exist (a result that was conjectured in [13]). Specifically, we present an algorithm that enjoys the following three properties: (i) it runs in polynomial time, (ii) its sample complexity matches the fundamental limits asymptotically in the high confidence regime, and (iii) its sample complexity is at most polynomial in the moderate confidence regime. Next, after formally introducing combinatorial semi-bandits, we describe our contributions and techniques in detail.

**Best arm identification in combinatorial semi-bandits.** In combinatorial semi-bandits [14, 15], the learner sequentially selects an action from a combinatorial set \(\mathcal{X}\subset\{0,1\}^{K}\). When in round \(t\), the action \(\bm{x}(t)=(x_{1}(t),\ldots,x_{K}(t))\in\mathcal{X}\) is chosen, the environment samples a \(K\)-dimensional vector \(\bm{y}(t)\) whose distribution is assumed to be Gaussian \(\mathcal{N}(\bm{\mu},\mathbf{I})\). The learner then receives the detailed reward vector \(\bm{x}(t)\odot\bm{y}(t)\) where \(\odot\) denotes the element-wise product (in other words, the learner observes the individual reward \(y_{k}(t)\) of the arm \(k\) if and only if this arm is selected in round \(t\), i.e., \(x_{k}(t)=1\)). The parameter \(\bm{\mu}\) characterizing the average rewards of the various arms is initially unknown. The goal of a learner is to identify the best action \(\bm{i}^{\star}(\bm{\mu})=\operatorname*{argmax}_{\bm{x}\in\mathcal{X}}\langle \bm{x},\bm{\mu}\rangle\) with a given level of confidence \(1-\delta\), for some \(\delta>0\) while minimizing the expected number of rounds needed. We assume that the best action is unique and denote by \(\Lambda=\{\bm{\mu}\in\mathbb{R}^{K}:|\bm{i}^{\star}(\bm{\mu})|=1\}\) the set of parameters satisfying this assumption. The learner strategy is defined by three components: (i) a sampling rule dictating the sequence of the selected actions; (ii) a stopping time \(\tau\) defining the last round where the learner interacts with the environment; (iii) a decision rule specifying the action \(\bm{i}\in\mathcal{X}\) believed to be optimal based on the data gathered until \(\tau\).

**The sample complexity lower-bound problem.** Consider the set of \(\delta\)-PAC algorithms such that for any \(\bm{\mu}\in\Lambda\), the best action is identified correctly with probability at least \(1-\delta\). We wish to find a \(\delta\)-PAC algorithm with minimal expected sample complexity \(\mathbb{E}_{\bm{\mu}}[\tau]\). To this aim, using classical change-of-measure arguments [16], we may derive a lower bound of the expected sample complexity satisfied by any \(\delta\)-PAC algorithm. This lower bound is given by1\(\mathbb{E}_{\bm{\mu}}[\tau]\geq T^{\star}(\bm{\mu})\mathsf{kl}(\delta,1-\delta)\). The characteristic time \(T^{\star}(\bm{\mu})\) is defined as the value of the following problem

Footnote 1: We present proof in Appendix K for completeness â€“ see also [13].

\[T^{\star}(\bm{\mu})^{-1}=\sup_{\bm{\omega}\in\Sigma}\inf_{\bm{\lambda}\in \mathsf{Alt}(\bm{\mu})}\left\langle\bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^ {2}}{2}\right\rangle,\] (1)

where2\(\Sigma=\{\sum_{\bm{x}\in\mathcal{X}}w_{\bm{x}}\bm{x}:\bm{w}\in\Sigma_{|\mathcal{ X}|}\}\), \(\mathsf{kl}(a,b)\) is the KL-divergence between two Bernoulli distributions with respective means \(a\) and \(b\), and \(\mathsf{Alt}(\bm{\mu})=\{\bm{\lambda}\in\Lambda:\bm{i}^{\star}(\bm{\lambda}) \neq\bm{i}^{\star}(\bm{\mu})\}\) is the set of _confusing_ parameters. As it turns out (see Lemma 1), \(T^{\star}(\bm{\mu})\) is at most quadratic in \(K\), and hence the sample complexity lower bound is polynomial. (1) is a concave program over \(\Sigma\)[17], and a point \(\bm{\omega}^{\star}\) in its solution set corresponds to an optimal allocation of action draws: an algorithm sampling actions according to \(\bm{\omega}^{\star}\) and equipped with an appropriate stopping rule would yield a sample complexity matching the lower bound. In this paper, we provide computationally efficient algorithms to solve (1) and show how these can be used to devise a \(\delta\)-PAC best action identification algorithm with minimal sample complexity and running in polynomial time. We only assume that we have access to a computationally efficient Oracle, referred to as the LM (Linear Maximization) Oracle, identifying the best action should \(\bm{\mu}\) be known (but for any possible \(\bm{\mu}\)). This assumption, made in all previous work on combinatorial semi-bandits (see e.g. [13, 14]), is crucial as indeed, if there is no computationally efficient algorithm solving the offline problem \(\operatorname*{argmax}_{\bm{x}\in\mathcal{X}^{\prime}}\langle\bm{x},\bm{\mu}\rangle\) with known \(\bm{\mu}\), there is no hope to solve its online version with unknown \(\bm{\mu}\) in a computationally efficient manner. The assumption holds for a large array of combinatorial sets of actions [S\({}^{+}\)03], including \(m\)-sets, matchings, (source-destination)-paths, spanning trees, matroids (refer to [16] for a thorough discussion).

**The Most-Confusing-Parameter (MCP) algorithm.** The difficulty of solving (1) lies in the inner optimization problem, i.e., in evaluating the objective function:

\[F_{\bm{\mu}}(\bm{\omega})=\inf_{\bm{\lambda}\in\text{\sc Att}(\bm{\mu})}\left< \bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^{2}}{2}\right>=\min_{\bm{x}\neq\bm{i} ^{*}(\bm{\mu})}f_{\bm{x}}(\bm{\omega},\bm{\mu})\] (2)

where \(f_{\bm{x}}(\bm{\omega},\bm{\mu})=\inf_{\bm{\lambda}\in\mathcal{C}_{\bm{x}}} \left<\bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^{2}}{2}\right>\) and \(\mathcal{C}_{\bm{x}}=\{\bm{\lambda}\in\mathbb{R}^{K}:\left<\bm{\lambda},\bm{i} ^{*}(\bm{\mu})-\bm{x}\right><0\}\). Evaluating \(F_{\bm{\mu}}(\bm{\omega})\) is required to solve (1), but also in the design of an efficient stopping rule. Our first contribution is MCP (Most-Confusing-Parameter), a polynomial time algorithm able to approximate \(F_{\bm{\mu}}(\bm{\omega})\) for any given \(\bm{\mu}\) and \(\bm{\omega}\). The algorithm's name refers to the fact that by computing \(F_{\bm{\mu}}(\bm{\omega})\), we implicitly identify the _most confusing parameter_\(\bm{\lambda}^{*}\in\arg\inf_{\bm{\lambda}\in\text{\sc Att}(\bm{\mu})}\langle \bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^{2}}{2}\rangle\). The design of MCP leverages a Lagrangian relaxation of the optimization problem defining \(f_{\bm{x}}(\bm{\omega},\bm{\mu})\) and exploits the fact that the Lagrange dual function linearly depends on \(\bm{x}\). In turn, this linearity allows us to make use of the LM Oracle. From these observations, we show that computing \(F_{\bm{\mu}}(\bm{\omega})\) boils down to solving a two-player game, for which one of the players can simply update her strategy using the LM Oracle. We prove the following informally stated theorem quantifying the performance of the MCP algorithm (see Theorem 3 for a more precise statement).

**Theorem 1**.: _For any \((\bm{\omega},\bm{\mu})\), the MCP algorithm with precision \(\epsilon\) and certainty parameter \(\theta\) returns \(\hat{F}\) and \(\hat{\bm{x}}\) satisfying \(\mathbb{P}_{\bm{\mu}}[F_{\bm{\mu}}(\bm{\omega})\leq\hat{F}\leq(1+\epsilon)F_{ \bm{\mu}}(\bm{\omega})]\geq 1-\theta\) and \(\hat{F}=f_{\hat{\bm{x}}}(\bm{\omega},\bm{\mu})\). The number of calls to the LM Oracle is, almost surely, at most polynomial in \(K\), \(\epsilon^{-1}\), and \(\ln\theta^{-1}\)._

**The Perturbed Frank-Wolfe Sampling (P-FWS) algorithm.** The MCP algorithm allows us to solve the lower-bound problem (1) for any given \(\bm{\mu}\). The latter is initially unknown, but could be estimated. A Track-and-Stop algorithm [1] solving (1) with this plug-in estimator in each round would yield asymptotically minimal sample complexity. It would however be computationally expensive. To circumvent this difficulty, as in [20], our algorithm, P-FWS, performs a single iteration of the Frank-Wolfe algorithm for the program (1) instantiated with an estimator of \(\bm{\mu}\). To apply the Frank-Wolfe algorithm, P-FWS uses stochastic smoothing techniques to approximate the non-differentiable objective function \(F_{\bm{\mu}}\) by a smooth function. To estimate the gradient of the latter, P-FWS leverages both the LM Oracle and the MCP algorithm (more specifically its second output \(\hat{\bm{x}}\)). Finally, P-FWS stopping rule takes the form of a classical Generalized Likelihood Ratio Test (GLRT) where the estimated objective function is compared to a time-dependent threshold. Hence the stopping rule also requires the MCP algorithm. We analyze the sample and computational complexities of P-FWS. Our main results are summarized in the following theorem (refer to Theorem 4 for details).

**Theorem 2**.: _For any \(\delta\in(0,1)\), P-FWS is \(\delta\)-PAC, and for any \((\epsilon,\tilde{\epsilon})\in(0,1)\) small enough, its sample complexity satisfies:_

\[\mathbb{E}_{\bm{\mu}}[\tau]\leq\frac{(1+\tilde{\epsilon})^{2}}{T^{*}(\bm{\mu} )^{-1}-\epsilon}\times H\bigg{(}\frac{1}{\delta}\cdot\frac{c(1+\tilde{ \epsilon})^{2}}{T^{*}(\bm{\mu})^{-1}-\epsilon}\bigg{)}+\Psi(\epsilon,\tilde{ \epsilon}),\]

_where \(H(x)=\ln(x)+\ln\ln(x)\), \(c>0\) is a universal constant, and \(\Psi(\epsilon,\tilde{\epsilon})\) is polynomial in \(\epsilon^{-1}\), \(\tilde{\epsilon}^{-1}\), \(K\), \(\left\lVert\bm{\mu}\right\rVert_{\infty}\), and \(\triangle_{\min}^{-1}\), where \(\triangle_{\min}=\min_{\bm{x}\neq\bm{i}^{*}(\bm{\mu})}\langle\bm{i}^{*}(\bm{ \mu})-\bm{x},\bm{\mu}\rangle\). Under P-FWS, the number of LM Oracle calls per round is at most polynomial in \(\ln\delta^{-1}\) and \(K\). The total expected number of these calls is also polynomial._

To the best of our knowledge, P-FWS is the first polynomial time best action identification algorithm with minimal sample complexity in the high confidence regime (when \(\delta\) tends to 0). Its sample complexity is also polynomial in \(K\) in the moderate confidence regime.

## 2 Preliminaries

We start by introducing some notation. We use bold lowercase letters (e.g., \(\bm{x}\)) for vectors, and bold uppercase letter (e.g., \(\mathbf{A}\)) for matrices. \(\odot\) (resp. \(\oplus\)) denotes the element-wise product (resp. sum over \(\mathbb{Z}_{2}\)). For \(\bm{x}\in\mathbb{R}^{K},i\in\mathbb{N}\), \(\bm{x}^{i}=(x_{k}^{i})_{k\in[K]}\) is the \(i\)-th element-wise power of \(\bm{x}\). \(D=\max_{\bm{x}\in\mathcal{X}}\left\lVert\bm{x}\right\rVert_{1}\) denotes the maximum number of arms part of an action. For any \(\bm{\mu}\in\Lambda\), we define the sub-optimality gap of \(\bm{x}\in\mathcal{X}\) as \(\triangle_{\bm{x}}(\bm{\mu})=\langle\bm{i}^{*}(\bm{\mu})-\bm{x},\bm{\mu}\rangle\), and the minimal gap as \(\triangle_{\min}(\bm{\mu})=\min_{\bm{x}\neq\bm{i}^{*}(\bm{\mu})}\triangle_{\bm{ x}}(\bm{\mu})\). \(\mathbb{P}_{\bm{\mu}}\) (resp. \(\mathbb{E}_{\bm{\mu}}\)) denotes the probability measure (resp. expectation) when the arm rewards are parametrized by \(\bm{\mu}\). Whenever it is clear from the context, we will drop \(\bm{\mu}\) for simplicity, e.g. \(\bm{i}^{*}=\bm{i}^{*}(\bm{\mu})\), \(\triangle_{\bm{x}}=\triangle_{\bm{x}}(\bm{\mu})\), and \(\triangle_{\min}=\min_{\bm{x}\neq\bm{i}^{*}}\triangle_{\bm{x}}\). Refer to Appendix A for an exhaustive table of notation.

### The lower-bound problem

Classical change-of-measure arguments lead to the asymptotic sample complexity lower bound \(\mathbb{E}_{\bm{\mu}}[\tau]\geq T^{\star}(\bm{\mu})\mathsf{k}\mathsf{l}(\delta,1-\delta)\) where the characteristic time is defined in (1). To have a chance to develop a computationally efficient best action identification algorithm, we need that the sample complexity lower bound grows at most polynomially in \(K\). This is indeed the case as stated in the following lemma, whose proof is provided in Appendix K.

**Lemma 1**.: _For any \(\bm{\mu}\in\Lambda\), \(T^{\star}(\bm{\mu})\leq 4KD\triangle_{\min}(\bm{\mu})^{-2}\)._

We will use first-order methods to solve the lower-bound problem, and to this aim, we will need to evaluate the gradient w.r.t. \(\bm{\omega}\) of \(f_{\bm{x}}(\bm{\omega},\bm{\mu})\). We can apply the envelop theorem [20] to show that for \((\bm{\omega},\bm{\mu})\in\Sigma_{+}\times\Lambda\),

\[\nabla_{\bm{\omega}}f_{\bm{x}}(\bm{\omega},\bm{\mu})=\frac{(\bm{\mu}-\bm{ \lambda}_{\bm{\omega},\bm{\mu}}^{\star}(\bm{x}))^{2}}{2},\]

where \(\Sigma_{+}=\Sigma\cap\mathbb{R}_{>0}^{K}\), \(\bm{\lambda}_{\bm{\omega},\bm{\mu}}^{\star}(\bm{x})=\operatorname*{argmin}_{ \bm{\lambda}\in\mathsf{cl}(\mathcal{C}_{\bm{x}})}\langle\bm{\omega},\frac{( \bm{\mu}-\bm{\lambda})^{2}}{2}\rangle\) and \(\mathsf{cl}\left(\mathcal{C}_{\bm{x}}\right)\) is the closure of \(\mathcal{C}_{\bm{x}}\) (refer to Lemma 19 in Appendix G.2).

### The Linear Maximization Oracle

As mentioned earlier, we assume that we have access to a computationally efficient Oracle, referred to as the LM (Linear Maximization) Oracle, identifying the best action if \(\bm{\mu}\) is known. More precisely, as in existing works in combinatorial semi-bandits [13, 14, 15], we make the following assumption.

**Assumption 1**.: _(i) There exists a polynomial-time algorithm identifying \(\bm{i}^{\star}(\bm{v})\) for any \(\bm{v}\in\mathbb{R}^{K}\); (ii) \(\mathcal{X}\) is inclusion-wise maximal, i.e., there is no \(\bm{x},\bm{x}^{\prime}\in\mathcal{X}\) s.t. \(\bm{x}<\bm{x}^{\prime}\); (iii) for each \(k\in[K]\), there exists \(\bm{x}\in\mathcal{X}\) such that \(x_{k}=1\); (iv) \(|\mathcal{X}|\geq 2\)._

Assumption 1 holds for combinatorial sets including \(m\)-sets, spanning forests, bipartite matching, \(s\)-\(t\) paths. For completeness, we verify the assumption for these action sets in Appendix J. In the design of our MCP algorithm, we will actually need to solve for some \(\bm{v}\in\mathbb{R}^{K}\) the linear maximization problem \(\max\langle\bm{x},\bm{v}\rangle\) over \(\mathcal{X}\setminus\{\bm{i}^{\star}(\bm{\mu})\}\); in other words, we will probably need to identify the second best action. Fortunately, this can be done in a computationally efficient manner under Assumption 1. The following lemma formalizes this observation. Its proof, presented in Appendix J, is inspired by Lawler's \(m\)-best algorithm [10].

**Lemma 2**.: _Let \(\bm{v}\in\mathbb{R}^{K}\) and \(\bm{x}\in\mathcal{X}\). Under Assumption 1, there exists an algorithm that solves \(\max_{\bm{x}^{\prime}\in\mathcal{X}:\bm{x}^{\prime}\neq\bm{x}}\langle\bm{v}, \bm{x}^{\prime}\rangle\) by only making at most \(D\) queries to the LM Oracle._

## 3 Solving the lower bound problem: the MCP algorithm

Solving the lower bound problem first requires to evaluate its objective function \(F_{\bm{\mu}}(\bm{\omega})\). A naive approach, enumerating \(f_{\bm{x}}(\bm{\omega},\bm{\mu})\) for all \(\bm{x}\in\mathcal{X}\setminus\{\bm{i}^{\star}\}\), would be computationally infeasible. In this section, we present and analyze MCP, an algorithm that approximates \(F_{\bm{\mu}}(\bm{\omega})\) by calling the LM Oracle a number of times growing at most polynomially in \(K\).

### Lagrangian relaxation

The first step towards the design of MCP consists in considering the Lagrangian relaxation of the optimization problem defining \(f_{\bm{x}}(\bm{\omega},\bm{\mu})=\inf_{\bm{\lambda}\in\mathcal{C}_{\bm{x}}} \langle\bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^{2}}{2}\rangle\) (see e.g., [14, 21]). For any \((\bm{\omega},\bm{\mu})\in\Sigma_{+}\times\Lambda\) and \(\bm{x}\neq\bm{i}^{\star}\), the Lagrangian \(\mathcal{L}_{\bm{\omega},\bm{\mu}}\) and Lagrange dual function \(g_{\bm{\omega},\bm{\mu}}\) of this problem are defined as, \(\forall\alpha\geq 0\),

\[\mathcal{L}_{\bm{\omega},\bm{\mu}}(\bm{\lambda},\bm{x},\alpha)=\left\langle \bm{\omega},\frac{(\bm{\mu}-\bm{\lambda})^{2}}{2}\right\rangle+\alpha\left\langle \bm{i}^{\star}-\bm{x},\bm{\lambda}\right\rangle\quad\text{and}\quad g_{\bm{ \omega},\bm{\mu}}(\bm{x},\alpha)=\inf_{\bm{\lambda}\in\mathbb{R}^{K}}\mathcal{L} _{\bm{\omega},\bm{\mu}}(\bm{\lambda},\bm{x},\alpha),\]

respectively. The following proposition, proved in Appendix C.1, provides useful properties of \(g_{\bm{\omega},\bm{\mu}}\):

**Proposition 1**.: _Let \((\bm{\omega},\bm{\mu})\in\Sigma_{+}\times\Lambda\) and \(\bm{x}\in\mathcal{X}\setminus\{\bm{i}^{\star}(\bm{\mu})\}\). (a) The Lagrange dual function is linear in \(\bm{x}\). More precisely, \(g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha)=c_{\bm{\omega},\bm{\mu}}(\alpha)+ \langle\bm{\ell}_{\bm{\omega},\bm{\mu}}(\alpha),\bm{x}\rangle\)__where \(c_{\bm{\omega},\bm{\mu}}(\bm{\alpha})=\alpha\left\langle\bm{\mu}-\frac{\alpha}{2} \bm{\omega}^{-1},\bm{i}^{\star}(\bm{\mu})\right\rangle\) and \(\bm{\ell}_{\bm{\omega},\bm{\mu}}(\alpha)=-\alpha\left(\bm{\mu}+\frac{\alpha}{2} \bm{\omega}^{-1}\odot(\bm{1}_{K}-2\bm{i}^{\star}(\bm{\mu}))\right)\). (b) \(g_{\bm{\omega},\bm{\mu}}(\bm{x},\cdot)\) is strictly concave (for any fixed \(\bm{x}\)). (c) \(f_{\bm{\omega}}(\bm{\omega},\bm{\mu})=\max_{\alpha\geq 0}g_{\bm{\omega},\bm{ \mu}}(\bm{x},\alpha)\) is attained by \(\alpha^{\star}_{\bm{x}}=\frac{\triangle_{\bm{x}}(\bm{\mu})}{\left\langle\bm{ \omega}\oplus\bm{i}^{\star}(\bm{\mu}),\bm{\omega}^{-1}\right\rangle}\). (d) \(\left\|\bm{\ell}_{\bm{\omega},\bm{\mu}}(\alpha^{\star}_{\bm{x}})\right\|_{1} \leq L_{\bm{\omega},\bm{\mu}}=4D^{2}K\left\|\bm{\mu}\right\|_{\infty}^{2} \left\|\bm{\omega}^{-1}\right\|_{\infty}\)._

From Proposition 1 (c), strong duality holds for the program defining \(f_{\bm{x}}(\bm{\omega},\bm{\mu})\), and we conclude:

\[F_{\bm{\mu}}(\bm{\omega})=\min_{\bm{x}\neq\bm{i}^{\star}}\max_{\alpha\geq 0 }g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha).\] (3)

\(F_{\bm{\mu}}(\bm{\omega})\) can hence be seen as the value in a two-player game. The aforementioned properties of the Lagrange dual function will help to compute this value.

### Solving the two-player game with no regret

There is a rich and growing literature on solving zero-sum games using no-regret algorithms, see for example [11, 10, 12]. Our game has the particularity that the \(\bm{x}\)-player has a discrete combinatorial action set whereas the \(\alpha\)-player has a convex action set. Importantly, for this game, we wish not only to estimate its value \(F_{\bm{\mu}}(\bm{\omega})\) but also an _equilibrium_ action \(\bm{x}_{c}\) such that \(F_{\bm{\mu}}(\bm{\omega})=\max_{\alpha\geq 0}g_{\bm{\omega},\bm{\mu}}(\bm{x}_{c },\alpha)\). Indeed, an estimate of \(\bm{x}_{c}\) will be needed when implementing the Frank-Wolfe algorithm and more specifically when estimating the gradient of \(F_{\bm{\mu}}(\bm{\omega})\). To return such an estimate, one could think of leveraging results from the recent literature on last-iterate convergence, see e.g. [1, 12, 13, 14, 15]. However, most of these results concern saddle-point problems only, and are not applicable in our setting. Here, we adopt a much simpler solution, and take advantage of the properties of the Lagrange dual function \(g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha)\) to design an iterative procedure directly leading to estimates of \((F_{\bm{\mu}}(\bm{\omega}),\bm{x}_{c})\). In this procedure, the two players successively update their actions until a stopping criterion is met, say up to the \(N\)-th iterations. The procedure generates a sequence \(\{(\bm{x}^{(n)},\alpha^{(n)})\}_{1\leq n\leq N}\), and from this sequence, estimates \((\hat{F},\hat{\bm{x}})\) of \((F_{\bm{\mu}}(\bm{\omega}),\bm{x}_{c})\). The details of the resulting MCP algorithm are presented in Algorithm 1.

\(\bm{x}\)**-player.** We use a variant of the Follow-the-Perturbed-Leader (FTPL) algorithm [16, 17]. The \(\bm{x}\)-player updates her action as follows:

\[\bm{x}^{(n)}\in\operatorname*{argmin}_{\bm{x}\neq\bm{i}^{\star}}\!\left(\sum \limits_{m=1}^{n-1}g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha^{(m)})+\left\langle \frac{\bm{\mathcal{Z}}_{n}}{\eta_{n}},\bm{x}\right\rangle\right)=\operatorname* {argmin}_{\bm{x}\neq\bm{i}^{\star}}\!\left(\left\langle\sum\limits_{m=1}^{n-1 }\bm{\ell}_{\bm{\omega},\bm{\mu}}(\alpha^{(m)})+\frac{\bm{\mathcal{Z}}_{n}}{ \eta_{n}},\bm{x}\right\rangle\right),\]

where \(\bm{\mathcal{Z}}_{n}\) is a random vector, exponentially distributed and with unit mean (\(\{\bm{\mathcal{Z}}_{n}\}_{n\geq 1}\) are i.i.d.). Compared to the standard FTPL algorithm, we vary learning rate \(\eta_{n}\) over time to get _anytime_ guarantees (as we do not know a priori when the iterative procedure will stop). This kind of time-varying learning rate was also used in [16] with a similar motivation. Note that thanks to the linearity of \(g_{\bm{\omega},\bm{\mu}}\) and Lemma 2, the \(\bm{x}\)-player update can be computed using at most \(D\) calls to the LM Oracle.

\(\alpha\)**-player and MCP outputs.** From Proposition 1, \(f_{\bm{x}}(\bm{\omega},\bm{\mu})=\max_{\alpha\geq 0}g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha)\). This suggests that the \(\alpha\)-player can just implement a best-response strategy: after the \(\bm{x}\)-player action \(\bm{x}^{(n)}\) is selected, the \(\alpha\)-player chooses \(\alpha^{(n)}=\alpha^{\star}_{\bm{x}^{(n)}}=\frac{\triangle_{\bm{x}^{(n)}}(\bm{ \mu})}{\left\langle\bm{x}^{(n)}\oplus\bm{i}^{\star}(\bm{\mu}),\bm{\omega}^{-1 }\right\rangle}\). This choice ensures that \(f_{\bm{x}^{(n)}}(\bm{\omega},\bm{\mu})=g_{\bm{\omega},\bm{\mu}}(\bm{x}^{(n)}, \alpha^{(n)})\), and suggests natural outputs for MCP: should it stops after \(N\) iterations, it can return \(\hat{F}=\min_{n\in[N]}g_{\bm{\omega},\bm{\mu}}(\bm{x}^{(n)},\alpha^{(n)})\) and \(\hat{\bm{x}}\in\operatorname*{argmin}_{n\in[N]}g_{\bm{\omega},\bm{\mu}}(\bm{x}^ {(n)},\alpha^{(n)})\).

**Stopping criterion.** The design of the MCP stopping criterion relies on the convergence analysis and regret from the \(\bm{x}\)-player perspective of the above iterative procedure, which we present in the next subsection. This convergence will be controlled by \(\bm{\ell}_{\bm{\omega},\bm{\mu}}(\alpha^{\star}_{\bm{x}})\) and its upper bound \(L_{\bm{\omega},\bm{\mu}}\) derived in Proposition 1. Introducing \(c_{\theta}=L_{\bm{\omega},\bm{\mu}}(4\sqrt{K(\ln K+1)}+\sqrt{\ln(\theta^{-1})/2})\), the MCP stopping criterion is: \(\sqrt{n}>c_{\theta}(1+\epsilon)/(\epsilon\hat{F})\). Since \(\sqrt{n}\) strictly increases with \(n\) and since \(\hat{F}\geq F_{\bm{\mu}}(\bm{\omega})\), this criterion ensures that the algorithm terminates in a finite number of iterates. Moreover, as shown in the next subsection, it also ensures that \(\hat{F}\) returned by MCP is an \((1+\epsilon)\)-approximation of \(F_{\bm{\mu}}(\bm{\omega})\) with probability at least \(1-\theta\).

### Performance analysis of the MCP algorithm

We start the analysis by quantifying the regret from the \(\bm{x}\)-player perspective of MCP before its stops. The following lemma is proved in Appendix C.3.

**Lemma 3**.: _Let \(N\in\mathbb{N}\). Under \((\epsilon,\theta)\text{-}\textsc{MCP}(\bm{\omega},\bm{\mu})\),_

\[\mathbb{P}\Bigg{[}\frac{1}{N}\sum_{n=1}^{N}g_{\bm{\omega},\bm{\mu}}(\bm{x}^{(n )},\alpha^{(n)})-\frac{1}{N}\min_{\bm{x}\neq\bm{i}^{\star}}\sum_{n=1}^{N}g_{ \bm{\omega},\bm{\mu}}(\bm{x},\alpha^{(n)})\leq\frac{c_{\theta}}{\sqrt{N}} \Bigg{]}\geq 1-\theta.\]

Observe that on the one hand,

\[\frac{1}{N}\sum_{n=1}^{N}g_{\bm{\omega},\bm{\mu}}(\bm{x}^{(n)},\alpha^{(n)}) \geq\min_{n\in[N]}g_{\bm{\omega},\bm{\mu}}(\bm{x}^{(n)},\alpha^{(n)})=\hat{F}\] (4)

always holds. On the other hand, if \(\bm{x}_{\varepsilon}\in\operatorname*{argmin}_{\bm{x}\neq\bm{i}^{\star}} \max_{\alpha\geq 0}g_{\bm{\omega},\bm{\mu}}(\bm{x},\alpha)\), then we have:

\[\frac{1}{N}\min_{\bm{x}\neq\bm{i}^{\star}}\sum_{n=1}^{N}g_{\bm{\omega},\bm{ \mu}}(\bm{x},\alpha^{(n)})\leq\frac{1}{N}\sum_{n=1}^{N}g_{\bm{\omega},\bm{\mu} }(\bm{x}_{\varepsilon},\alpha^{(n)})\leq\max_{\alpha\geq 0}g_{\bm{\omega},\bm{ \mu}}(\bm{x}_{\varepsilon},\alpha)=F_{\bm{\mu}}(\bm{\omega}).\] (5)

We conclude that for \(N\) such that \(\sqrt{N}\geq\frac{c_{\theta}(1+\epsilon)}{\epsilon\hat{F}}\), Lemma 3 together with the inequalities (4) and (5) imply that \(\hat{F}-F_{\bm{\mu}}(\bm{\omega})\leq\frac{c_{\theta}}{\sqrt{N}}\leq\frac{ \epsilon\hat{F}}{1+\epsilon}\) holds with probability at least \(1-\theta\). Hence \(\mathbb{P}\Big{[}\hat{F}\leq(1+\epsilon)F_{\bm{\mu}}(\bm{\omega})\Big{]}\geq 1-\theta\). From this observation, we essentially deduce the following theorem, whose complete proof is given in Appendix C.2.

**Theorem 3**.: _Let \(\epsilon,\theta\in(0,1)\). Under Assumption 1, for any \((\bm{\omega},\bm{\mu})\in\Sigma_{+}\times\Lambda\), the \((\epsilon,\theta)\text{-}\textsc{MCP}(\bm{\omega},\bm{\mu})\) algorithm outputs \((\hat{F},\hat{\bm{x}})\) satisfying \(\mathbb{P}\Big{[}F_{\bm{\mu}}(\bm{\omega})\leq\hat{F}\leq(1+\epsilon)F_{\bm{ \mu}}(\bm{\omega})\Big{]}\geq 1-\theta\) and \(\hat{F}=\max_{\alpha\geq 0}g_{\bm{\omega},\bm{\mu}}(\hat{\bm{x}},\alpha).\) Moreover, the number of LM Oracle calls the algorithm does is almost surely at most \(\Big{[}\frac{\epsilon_{\bm{\mu}}^{2}(1+\epsilon)^{2}}{\epsilon^{2}F_{\bm{ \mu}}(\bm{\omega})^{2}}\Big{]}=\mathcal{O}\bigg{(}\frac{\|\bm{\mu}\|_{\infty}^{ \epsilon_{\bm{\mu}}}^{\epsilon_{\bm{\mu}}}\|^{\epsilon_{\bm{\mu}}^{-1}}\big{]} _{\infty}^{2}K^{3}D^{5}\ln K\ln\theta^{-1}}{\epsilon^{2}F_{\bm{\mu}}(\bm{ \omega})^{2}}\bigg{)}\)._

## 4 The Perturbed Frank-Wolfe Sampling (P-Fws) algorithm

To identify an optimal sampling strategy, rather than solving the lower-bound problem in each round as a Track-and-Stop algorithm would [6], we devise P-FWS, an algorithm that performs a single iteration of the Frank-Wolfe algorithm for the lower-bound problem instantiated with an estimator of \(\bm{\mu}\). This requires us to first smooth the objective function \(F_{\bm{\mu}}(\bm{\omega})=\min_{\bm{x}\neq\bm{i}^{\star}}f_{\bm{x}}(\bm{\omega },\bm{\mu})\) (the latter is not differentiable at points \(\bm{\omega}\) where the \(\min\) is achieved for several sub-optimal actions \(\bm{x}\)). To this aim, we cannot leverage the same technique as in [12], where \(r\)-subdifferential subspaces are built from gradients of \(f_{\bm{x}}(\bm{\omega},\bm{\mu})\). These subspaces could indeed be generated by a number of vectors (here gradients) exponentially growing with \(K\). Instead, to cope with the combinatorialdecision sets, P-FWS applies more standard stochastic smoothing techniques as described in the next subsection. All the ingredients of P-FWS are gathered in SS4.2. By design, the algorithm just leverages the MCP algorithm as a subroutine, and hence only requires the LM Oracle. In SS4.3, we analyze the performance of P-FWS.

### Smoothing the objective function \(F_{\bm{\mu}}\)

Here, we present and analyze a standard stochastic technique to smooth a function \(\Phi\). In P-FWS, this technique will be applied to the objective function \(\Phi=F_{\bm{\mu}}\). Let \(\Phi:\mathbb{R}_{>0}^{K}\mapsto\mathbb{R}\) be a concave and \(\ell\)-Lipschitz function. Assume that the set of points where \(\Phi\) is not differentiable is of Lebesgue-measure zero. To smooth \(\Phi\), we can take its average value in a neighborhood of the point considered, see e.g. [13]. Formally, we define the _stochastic smoothed_ approximate of \(\Phi\) as:

\[\bar{\Phi}_{\eta}(\bm{\omega})=\mathbb{E}_{\bm{Z}\sim\text{Uniform}(B_{2})}[ \Phi(\bm{\omega}+\eta\bm{Z})]\,,\] (6)

where \(B_{2}=\{\bm{v}\in\mathbb{R}^{K}:\left\|\bm{v}\right\|_{2}\leq 1\}\) and \(\eta\in(0,\min_{k\in[K]}\omega_{k})\). The following proposition lists several properties of this smoothed function, and gathers together some of the results from [1], see Appendix H for more details.

**Proposition 2**.: _For any \(\bm{\omega}\in\Sigma_{+}\) and \(\eta\in(0,\min_{k\in[K]}\omega_{k})\), \(\bar{\Phi}_{\eta}\) satisfies: (i) \(\Phi(\bm{\omega})-\eta\ell\leq\bar{\Phi}_{\eta}(\bm{\omega})\leq\Phi(\bm{\omega})\); (ii) \(\nabla\bar{\Phi}_{\bm{\mu},\eta}(\bm{\omega})=\mathbb{E}_{\bm{Z}\sim\text{Uniform}(B_{2})}[ \nabla\Phi_{\bm{\mu}}(\bm{\omega}+\eta\bm{Z})]\); (iii) \(\bar{\Phi}_{\eta}\) is \(\frac{\ell K}{\eta}\)-smooth; (iv) if \(\eta>\eta^{\prime}>0\), then \(\bar{\Phi}_{\eta^{\prime}}(\bm{\omega})\geq\bar{\Phi}_{\eta}(\bm{\omega})\)._

Note that with (i), we may control the approximation error between \(\bar{\Phi}_{\eta}\) and \(\Phi\) by \(\eta\). (ii) and (iii) ensure the differentiability and smoothness of \(\bar{\Phi}_{\eta}\) respectively. (iii) is equivalent to \(\bar{\Phi}_{\eta}(\bm{\omega}^{\prime})\leq\bar{\Phi}_{\eta}(\bm{\omega})+ \left\langle\nabla\bar{\Phi}_{\eta}(\bm{\omega}),\bm{\omega}^{\prime}-\bm{ \omega}\right\rangle+\frac{\ell K}{2\eta}\left\|\bm{\omega}-\bm{\omega}^{ \prime}\right\|_{2}^{2},\ \forall\bm{\omega},\bm{\omega}^{\prime}\in\Sigma_{+}\). Finally, (iv) stems from the concavity of \(\Phi\), and implies that the value \(\Phi_{\eta}(\bm{\omega})\) monotonously increases while \(\eta\) decreases, and it is upper bounded by \(\Phi(\bm{\omega})\) thanks to (i). The above results hold for \(\Phi=F_{\bm{\mu}}\). Indeed, first it is clear that the definition (2) of \(F_{\bm{\mu}}\) can be extended to \(\mathbb{R}^{K}\); then, it can be shown that \(F_{\bm{\mu}}\) is Lipschitz-continuous and almost-everywhere differentiable - refer to Appendices I and H for formal proofs.

### The algorithm

Before presenting P-FWS, we need to introduce the following notation. For \(t\geq 1\), \(k\in[K]\), we define \(N_{k}(t)=\sum_{s=1}^{t}\mathbbm{1}\{x_{k}(s)=1\}\), \(\hat{\omega}_{k}(t)=N_{k}(t)/t\), and \(\hat{\mu}_{k}(t)=\sum_{s=1}^{t}y_{k}(s)\mathbbm{1}\{x_{k}(s)=1\}\,/N_{k}(t)\) when \(N_{k}(t)>0\).

**Sampling rule.** The design of the sampling rule is driven by the following objectives: (i) the empirical allocation should converge to the solution of the lower-bound problem (1), and (ii) the number of calls to the LM Oracle should be controlled. To meet the first objective, we need in the Frank-Wolfe updates to plug an accurate estimator of \(\bm{\mu}\) in. The accuracy of our estimator will be guaranteed by alternating between _forced exploration_ and _FW update_ sampling phases. Now for the second objective, we also use forced exploration phases when in a Frank-Wolfe update, the required number of calls to the LM Oracle predicted by the upper bound presented in Theorem 3 is too large. In view of Lemma 1 and Theorem 3, this happens in round \(t\) if \(\|\hat{\bm{\mu}}(t-1)\|_{\infty}\) or \(\triangle_{\min}(\hat{\bm{\mu}}(t-1))^{-1}\) is too large. Next, we describe the forced exploration and Frank-Wolfe update phases in detail.

_Forced exploration._ Initially, P-FWS applies the LM Oracle to compute the _forced exploration set_\(\mathcal{X}_{0}=\{\bm{i}^{\star}(\bm{e}_{k}):k\in[K]\}\), where \(\bm{e}_{k}\) is the \(K\)-dimensional vector whose \(k\)-th component is equal to one and zero elsewhere. P-FWS then selects each action in \(\mathcal{X}_{0}\) once. Note that Assumption 1 (iii) ensures that the \(k\)-th component of \(\bm{i}^{\star}(\bm{e}_{k})\) is equal to one. In turn, this ensures that \(\mathcal{X}_{0}\) is a \([K]\)-covering set, and that playing actions from \(\mathcal{X}_{0}\) is enough to estimate \(\bm{\mu}\). P-FWS starts an exploration phase at rounds \(t\) such that \(\sqrt{t/\left|\mathcal{X}_{0}\right|}\) is an integer or such that the maximum of \(\triangle_{\min}(\hat{\bm{\mu}}(t-1))^{-1}\) and \(\|\hat{\bm{\mu}}(t-1)\|_{\infty}\) is larger than \(\sqrt{t-1}\). Whenever this happens, P-FWS pulls each \(\bm{x}\in\mathcal{X}_{0}\) once.

_Frank-Wolfe updates._ When in round \(t\), the algorithm is not in a forced exploration phase, it implements an iteration of the Frank-Wolfe algorithm applied to maximize the smoothed function \(\bar{F}_{\hat{\bm{\mu}}(t-1),\eta_{t}}(\hat{\bm{\omega}}(t-1))=\mathbb{E}_{\bm{Z }\sim\text{Uniform}(B_{2})}\big{[}F_{\hat{\bm{\mu}}(t-1)}(\hat{\bm{\omega}}(t-1) +\eta_{t}\bm{Z})\big{]}\). The sequence of parameters \(\{\eta_{t}\}_{t\geq 1}\) is chosen to ensure that \(\eta_{t}\) chosen in \((0,\min_{k}\hat{\omega}_{k}(t))\), and hence \(\hat{\bm{\omega}}(t-1)+\eta_{t}\bm{Z}\in\mathbb{R}_{>0}^{K}\). Also note that in a round \(t\) where the algorithm is not in a forced exploration phase,by definition \(\triangle_{\min}(\hat{\bm{\mu}}(t-1))>0\). This implies that \(\hat{\bm{\mu}}(t-1)\in\Lambda\) and that \(F_{\hat{\bm{\mu}}(t-1)}\) and \(\tilde{F}_{\hat{\bm{\mu}}(t-1),\eta_{t}}(\hat{\bm{\omega}}(t-1))\) are well-defined. Now an ideal FW update would consist in playing an action \(\bm{i}^{\star}(\nabla\bar{F}_{\hat{\bm{\mu}}(t-1),\eta_{t}}(\hat{\bm{\omega}}(t -1)))=\operatorname*{argmax}_{\bm{x}\in\mathcal{X}}\left\langle\nabla\bar{F}_{ \hat{\bm{\mu}}(t-1),\eta_{t}}(\hat{\bm{\omega}}(t-1)),\bm{x}\right\rangle\), see e.g. [13]. Unfortunately, we do not have access to \(\nabla\bar{F}_{\hat{\bm{\mu}}(t-1),\eta_{t}}(\hat{\bm{\omega}}(t-1))\). But the latter can be approximated, as suggested in Proposition 2 (ii), by \(\nabla\bar{F}_{\hat{\bm{\mu}}(t-1),\eta_{t},n_{t}}(\hat{\bm{\omega}}(t-1))= \frac{1}{n_{t}}\sum_{m=1}^{n_{t}}\nabla f_{\hat{\bm{x}}_{m}}(\hat{\bm{\omega}} (t-1)+\eta_{t}\bm{Z}_{m},\hat{\bm{\mu}}(t-1))\), where \(\bm{Z}_{1},\cdots,\bm{Z}_{n_{t}}\overset{i,d.c.}{\sim}\text{Uniform}(B_{2})\), \(\hat{\bm{x}}_{m}\) is the action return by \((\rho_{t},\theta_{t})\text{-MCP}(\hat{\bm{\omega}}(t-1)+\eta_{t}\bm{Z}_{m}, \hat{\bm{\mu}}(t-1))\). P-FWS uses this approximation and the LM Oracle to select the action: \(\bm{x}(t)\in\bm{i}^{\star}\left(\nabla\bar{F}_{\hat{\bm{\mu}}(t-1),\eta_{t},n _{t}}(\hat{\bm{\omega}}(t-1))\right).\) The choices of the parameters \(\eta_{t}\), \(n_{t}\), \(\rho_{t}\) and \(\theta_{t}\) do matter. \(\eta_{t}\) impacts the sample complexity and should converge to 0 as \(t\to\infty\) so that \(\bar{F}_{\bm{\mu},\eta_{t}}(\bm{\omega})\to F_{\bm{\mu}}(\bm{\omega})\) at any point \(\bm{\omega}\in\Sigma_{+}\) (this is a consequence of Proposition 2 (i)(iv)). \(\eta_{t}\) should not decay too fast however as it would alter the smoothness of \(\bar{F}_{\bm{\mu},\eta_{t}}\). We will show that \(\eta_{t}\) should actually decay as \(1/\sqrt{t}\). \((n_{t},\rho_{t},\theta_{t})\) impact the trade-off between the sample complexity and the computational complexity of the algorithm. We let \(n_{t}\to\infty\) and \((\rho_{t},\theta_{t})\to 0\) as \(t\to\infty\) so that \(\left\langle\nabla\bar{F}_{\bm{\mu},\eta_{t},n_{t}}(\bm{\omega})-\nabla\bar{F }_{\bm{\mu},\eta_{t}}(\bm{\omega}),\bm{x}\right\rangle\to 0\) for any \((\bm{\omega},\bm{x})\in\Sigma_{+}\times\mathcal{X}\).

**Stopping and decision rule.** As often in best arm identification algorithms, the P-FWS stopping rule takes the form of a GLRT:

\[\tau=\inf\left\{t>4|\mathcal{X}_{0}|:\frac{t\hat{F}_{t}}{1+\epsilon_{t}}> \beta\bigg{(}t,\left(1-\frac{1}{4|\mathcal{X}_{0}|}\right)\delta\bigg{)},\max \left\{\triangle_{\min}(\hat{\bm{\mu}}(t))^{-1},\left\|\hat{\bm{\mu}}(t)\right\| _{\infty}\right\}\leq\sqrt{t}\right\},\] (7)

where \(\epsilon_{t}\in\mathbb{R}_{>0}\), \(\hat{F}_{t}\) is returned by the \((\epsilon_{t},\delta/t^{2})\text{-MCP}(\hat{\bm{\omega}}(t),\hat{\bm{\mu}}(t))\) algorithm. The function \(\beta\) satisfies

\[\forall t\geq 1,\quad\big{(}t\bar{F}_{\hat{\bm{\mu}}(t)}(\bm{\omega}(t))\geq \beta(t,\delta)\big{)}\implies\left(\mathbb{P}_{\bm{\mu}}[\bm{i}^{\star}(\hat{ \bm{\mu}}(t))\neq\bm{i}^{\star}(\bm{\mu})]\leq\delta\right),\] (8)

\[\exists c_{1},c_{2}>0:\quad\forall t\geq c_{1},\beta(t,\delta)\leq\ln\left( \frac{c_{2}t}{\delta}\right).\] (9)

Examples of function \(\beta\) satisfying the above conditions can be found in [16, 17, 18]. The condition (8) will ensure the \(\delta\)-correctness of P-FWS, whereas (9) will control its sample complexity. Finally, the action returned by P-FWS is simply defined as \(\hat{\bm{i}}=\bm{i}^{\star}(\hat{\bm{\mu}}(\tau))\). The complete pseudo-code of P-FWS is presented in Algorithm 2.3

### Non-asymptotic performance analysis of P-Fws

The following theorem provides an upper bound of the sample complexity of P-FWS valid for any confidence level \(\delta\), as well as the computational complexity of the algorithm.

**Theorem 4**.: _Let \(\boldsymbol{\mu}\in\Lambda\) and \(\delta\in(0,1)\). If P-FWS is parametrized using_

\[(\epsilon_{t},\eta_{t},n_{t},\rho_{t},\theta_{t})=\left(t^{-\frac{1}{3}},\, \frac{1}{4\sqrt{t|\mathcal{X}_{0}|}},\,\left[t^{\frac{1}{4}}\right],\,\frac{1} {16tD^{2}|\mathcal{X}_{0}|},\,\frac{1}{t^{\frac{1}{4}}e^{\sqrt{t}}}\right),\] (10)

_then (i) the algorithm finishes in finite time almost surely and \(\mathbb{P}_{\boldsymbol{\mu}}[\hat{\boldsymbol{i}}\neq\boldsymbol{i}^{\star}( \boldsymbol{\mu})]\leq\delta\); (ii) its sample complexity satisfies \(\mathbb{P}_{\boldsymbol{\mu}}\big{[}\limsup_{\delta\to 0}\frac{\tau}{\ln \delta^{-1}}\leq T^{\star}(\boldsymbol{\mu})\big{]}=1\) and for any \(\epsilon,\tilde{\epsilon}\in(0,1)\) with \(\epsilon<\min\{1,\frac{2D^{2}\triangle_{\min}^{2}}{K},\frac{D^{2}\left\| \boldsymbol{\mu}\right\|_{\infty}^{2}}{3}\}\),_

\[\mathbb{E}_{\boldsymbol{\mu}}[\tau]\leq\frac{(1+\tilde{\epsilon})^{2}}{T^{ \star}(\boldsymbol{\mu})^{-1}-6\epsilon}\times H\bigg{(}\frac{1}{\delta}\cdot \frac{4c_{2}}{3}\cdot\frac{(1+\tilde{\epsilon})^{2}}{T^{\star}(\boldsymbol{\mu })^{-1}-6\epsilon}\bigg{)}+\Psi(\epsilon,\tilde{\epsilon}),\]

_where \(H(x)=\ln x+\ln\ln x+1\) and \(\Psi(\epsilon,\tilde{\epsilon})\) (refer to (34) for a detailed expression) is polynomial in \(\epsilon^{-1}\), \(\tilde{\epsilon}^{-1}\), \(K\), \(\left\|\boldsymbol{\mu}\right\|_{\infty}\), and \(\triangle_{\min}(\boldsymbol{\mu})^{-1}\); (iii) the expected number of LM Oracle calls is upper bounded by a polynomial in \(\delta^{-1}\), \(K\), \(\left\|\boldsymbol{\mu}\right\|_{\infty}\), and \(\triangle_{\min}(\boldsymbol{\mu})^{-1}\)._

The above theorem establishes the statistical asymptotic optimality of P-FWS since it implies that \(\limsup_{\delta\to 0}\mathbb{E}_{\boldsymbol{\mu}}[\tau]\,/\ln(1/\delta) \leq(1+\tilde{\epsilon})^{2}/(T^{\star}(\boldsymbol{\mu})^{-1}-6\epsilon)\). This upper bound matches the sample complexity lower bound (1) when \(\epsilon\to 0\) and \(\tilde{\epsilon}\to 0\).

**Proof sketch.** The complete proof of Theorem 4 is presented in Appendix D.

_(i) Correctness._ To establish the \(\delta\)-correctness of the algorithm, we introduce the event \(\mathcal{G}\) under which \(\hat{F}_{t}\), computed by \((\epsilon_{t},\delta/t^{2})\)-MCP\((\hat{\boldsymbol{\omega}}(t),\hat{\boldsymbol{\mu}}(t))\), is an \((1+\epsilon_{t})\)-approximation of \(F_{\hat{\boldsymbol{\mu}}(t)}(\hat{\boldsymbol{\omega}}(t))\) in each round \(t\geq 4|\mathcal{X}_{0}|+1\). From Theorem 3, we deduce that \(\mathbb{P}_{\boldsymbol{\mu}}[\mathcal{G}^{c}]\leq\sum_{t=4|\mathcal{X}_{0}|+ 1}^{\infty}\delta/t^{2}\leq\delta/4|\mathcal{X}_{0}|\). In view of (8), this implies that \(\mathbb{P}_{\boldsymbol{\mu}}[\hat{\boldsymbol{i}}\neq\boldsymbol{i}^{\star} (\boldsymbol{\mu})]\leq\delta\).

_(ii) Non-asymptotic sample complexity upper bound._

Step 1. (Concentration and certainty equivalence) We first define two _good_ events, \(\mathcal{E}_{t}^{(1)}\) and \(\mathcal{E}_{t}^{(2)}\). \(\mathcal{E}_{t}^{(2)}\) corresponds to the event where \(\hat{\boldsymbol{\mu}}(t)\) is close to \(\boldsymbol{\mu}\), and its occurrence probability can be controlled using the forced exploration rounds and concentration inequalities. Under \(\mathcal{E}_{t}^{(1)}\), the selected action \(\boldsymbol{x}(t)\) is close to the ideal FW update. Again using concentration results and the performance guarantees of MCP given in Theorem 3, we can control the occurrence probability of \(\mathcal{E}_{t}^{(2)}\). Overall, we show that \(\sum_{t=1}^{\infty}\mathbb{P}_{\boldsymbol{\mu}}[(\mathcal{E}_{t}^{(1)}\cap \mathcal{E}_{t}^{(2)})^{c}]<\infty\). To this aim, we derive several important continuity results presented in Appendix G. These results essentially allow us to study the convergence of the smoothed FW updates as if the certainty equivalence principle held, i.e., as if \(\hat{\boldsymbol{\mu}}(t)=\boldsymbol{\mu}\).

Step 2. (Convergence of the smoothed FW updates) We study the convergence assuming that \((\mathcal{E}_{t}^{(1)}\cap\mathcal{E}_{t}^{(2)})\) holds. We first show that \(\tilde{F}_{\boldsymbol{\mu},\eta_{t}}\) is \(\ell\)-Lipschitz and smooth for \(\ell=2D^{2}\left\|\boldsymbol{\mu}\right\|_{\infty}^{2}\), see Appendices H and I. Then, in Appendix E, we establish that the dynamics of \(\phi_{t}=\max_{\boldsymbol{\omega}\in\Sigma}\,F_{\boldsymbol{\mu}}(\boldsymbol{ \omega})-F_{\boldsymbol{\mu}}(\hat{\boldsymbol{\omega}}(t))\) satisfy \(t\phi_{t}\leq(t-1)\phi_{t-1}+\ell\left(\eta_{t-1}+\frac{K^{2}}{2t\eta_{t}}\right)\). Observe that, as mentioned earlier, \(1/\sqrt{t}\) is indeed the optimal scaling choice for \(\eta_{t}\). We deduce that after a certain finite number \(T_{1}\) of rounds, \(\phi_{t}\) is sufficiently small and \(\max\{\triangle_{\min}(\hat{\boldsymbol{\mu}}(t))^{-1},\left\|\hat{\boldsymbol{ \mu}}(t)\right\|_{\infty}\}\leq\sqrt{t}\).

Step 3. Finally, we observe that \(\mathbb{E}_{\boldsymbol{\mu}}[\tau]\leq T_{1}+\sum_{t=T_{1}}^{\infty}\mathbb{P} _{\boldsymbol{\mu}}\Big{[}t\hat{F}_{t}\leq(1+\epsilon_{t})\beta\Big{(}t,(1- \frac{1}{4|\mathcal{X}_{0}|})\delta\Big{)}\Big{]}+\sum_{t=T_{1}+1}^{\infty} \mathbb{P}_{\boldsymbol{\mu}}\Big{[}(\mathcal{E}_{t}^{(1)}\cap\mathcal{E}_{t}^{ (2)})^{c}\Big{]}\,,\) and show that the second term in the r.h.s. in this inequality is equivalent to \(T^{\star}(\boldsymbol{\mu})\ln(1/\delta)\) as \(\delta\to 0\) using the property of the function \(\beta\) defining the stopping threshold and similar arguments as those used in [1, 16].

_(iii) Expected number of LM Oracle calls._ The MCP algorithm is called to compute \(\hat{F}_{t}\) and to perform the FW update only in rounds \(t\) such that \(\max\{\triangle_{\min}(\hat{\boldsymbol{\mu}}(t))^{-1},\left\|\hat{\boldsymbol{ \mu}}(t)\right\|_{\infty}\}\leq\sqrt{t}\). Thus, from Theorem 3 and Lemma 1, the number of LM Oracle calls per-round is a polynomial in \(t\) and \(K\). As the \(\mathbb{E}_{\boldsymbol{\mu}}[\tau]\) is polynomial (in \(\ln\delta^{-1}\), \(K\), \(\left\|\boldsymbol{\mu}\right\|_{\infty}\) and \(\triangle_{\min}^{-1}\)), the expected number of LM Oracle calls is also polynomial in the same variables.

Related Work

We provide an exhaustive survey of the related literature in Appendix B. To summarize, to the best of our knowledge, CombGame [17] is the state-of-the-art algorithm for BAI in combinatorial semi-bandits in the high confidence regimes. A complete comparison to P-FWS is presented in Appendix B. CombGame was initially introduced in [16] for classical bandit problems. There, the lower-bound problem is casted as a two-player game and the authors propose to use no-regret algorithms for each player to solve it. [17] adapts the algorithm for combinatorial semi-bandits, and provides a non-asymptotic sample complexity upper bound matching (1) asymptotically. However, the resulting algorithm requires to call an oracle solving the Most-Confusing-Parameter problem as our MCP algorithm. The authors of [17] conjectured the existence of such a computationally efficient oracle, and we establish this result here.

## 6 Conclusion

In this paper, we have presented P-FWS, the first computationally efficient and statistically optimal algorithm for the best arm identification problem in combinatorial semi-bandits. For this problem, we have studied the computational-statistical trade-off through the analysis of the optimization problem leading to instance-specific sample complexity lower bounds. This approach can be extended to study the computational-statistical gap in other learning tasks. Of particular interest are problems with an underlying structure (e.g. linear bandits [14, 15], or RL in linear / low rank MDPs [1]). Most results on these problems are concerned with statistical efficiency, and ignore computational issues.

## Acknowledgments and Disclosure of Funding

We thank Aristides Gionis and the anonymous reviewers for their valuable feedback. The research is funded by ERC Advanced Grant REBOUND (834862), the Wallenberg AI, Autonomous Systems and Software Program (WASP), and Digital Futures.

## References

* [AAS\({}^{+}\)23] Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, and Atsushi Iwasaki. Last-iterate convergence with full-and noisy-information feedback in two-player zero-sum games. In _Proc. of AISTATS_, 2023.
* [AKKS20] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. In _Proc. of NeurIPS_, 2020.
* [ALLW18] Jacob Abernethy, Kevin A Lai, Kfir Y Levy, and Jun-Kun Wang. Faster rates for convex-concave games. In _Proc. of COLT_, 2018.
* [AMP21] Aymen Al Marjani and Alexandre Proutiere. Adaptive sampling for best policy identification in markov decision processes. In _Proc. of ICML_, 2021.
* [APFS22] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In _Proc. of ICML_, 2022.
* [BBGS11] Andre Berger, Vincenzo Bonifaci, Fabrizio Grandoni, and Guido Schafer. Budgeted matching and budgeted matroid intersection via the gasoline puzzle. _Mathematical Programming_, 2011.
* [BGK22] Antoine Barrier, Aurelien Garivier, and Tomas Kocak. A non-asymptotic approach to best-arm identification for gaussian bandits. In _Proc. of AISTATS_, 2022.
* [BLM13] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. Oxford University Press, 2013.

* [BV04] Stephen Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* [CBL06] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* [CBL12] Nicolo Cesa-Bianchi and Gabor Lugosi. Combinatorial bandits. _Journal of Computer and System Sciences_, 2012.
* [CCG21a] Thibaut Cuvelier, Richard Combes, and Eric Gourdin. Asymptotically optimal strategies for combinatorial semi-bandits in polynomial time. In _Proc. of ALT_, 2021.
* [CCG21b] Thibaut Cuvelier, Richard Combes, and Eric Gourdin. Statistically efficient, polynomial-time algorithms for combinatorial semi-bandits. _Proc. of SIGMETRICS_, 2021.
* [CGL16] Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid constraints. In _Proc. of COLT_, 2016.
* [CGL\({}^{+}\)17] Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling algorithms for combinatorial pure exploration. In _Proc. of COLT_, 2017.
* [CLK\({}^{+}\)14] Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of multi-armed bandits. In _Proc. of NeurIPS_, 2014.
* [CMP17] Richard Combes, Stefan Magureanu, and Alexandre Proutiere. Minimal exploration in structured stochastic bandits. In _Proc. of NeurIPS_, 2017.
* [CTMSP\({}^{+}\)15] Richard Combes, Mohammad Sadegh Talebi Mazraeh Shahi, Alexandre Proutiere, et al. Combinatorial bandits revisited. In _Proc. of NeurIPS_, 2015.
* [DBW12] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. _SIAM Journal on Optimization_, 2012.
* [DFG21] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. In _Proc. of NeurIPS_, 2021.
* [DKC21] Yihan Du, Yuko Kuroki, and Wei Chen. Combinatorial pure exploration with full-bandit or partial linear feedback. In _Proc. of AAAI_, 2021.
* [DKM19] Remy Degenne, Wouter M Koolen, and Pierre Menard. Non-asymptotic pure exploration by solving games. In _Proc. of NeurIPS_, 2019.
* [DMSV20] Remy Degenne, Pierre Menard, Xuedong Shang, and Michal Valko. Gamification of pure exploration for linear bandits. In _Proc. of ICML_, 2020.
* [DP19] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. _Proc. of ITCS_, 2019.
* [FKM05] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In _Proc. of SODA_, 2005.
* [FKV14] Eugene A Feinberg, Pavlo O Kasyanov, and Mark Voorneveld. Berge's maximum theorem for noncompact image sets. _Journal of Mathematical Analysis and Applications_, 2014.
* [GC11] A. Garivier and O. Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In _Proc. of COLT_, 2011.
* [GH16] Dan Garber and Elad Hazan. A linearly convergent variant of the conditional gradient algorithm under strong convexity, with applications to online and stochastic optimization. _SIAM Journal on Optimization_, 2016.

* [GK16] Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In _Proc. of COLT_, 2016.
* [GPDO20] Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In _Proc. of COLT_, 2020.
* [H\({}^{+}\)16] Elad Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2016.
* [Han57] James Hannan. Approximation to bayes risk in repeated play. _Contributions to the Theory of Games_, 1957.
* [HK12] Elad Hazan and Satyen Kale. Projection-free online learning. In _Proc. of ICML_, 2012.
* [Jag13] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In _Proc. of ICML_, 2013.
* [JMKK21] Marc Jourdan, Mojmir Mutny, Johannes Kirschner, and Andreas Krause. Efficient pure exploration for combinatorial bandits with semi-bandit feedback. In _Proc. of ALT_, 2021.
* [JP20] Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. In _Proc. of NeurIPS_, 2020.
* [KCG16] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best-arm identification in multi-armed bandit models. _JMLR_, 2016.
* [KK21] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. _JMLR_, 2021.
* [KLLM22] Daniel Kane, Sihan Liu, Shachar Lovett, and Gaurav Mahajan. Computational-statistical gap in reinforcement learning. In _Proc. of COLT_, 2022.
* [KSJJ\({}^{+}\)20] Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. In _Proc. of NeurIPS_, 2020.
* [KTAS12] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In _Proc. of ICML_, 2012.
* [KV05] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. _Journal of Computer and System Sciences_, 2005.
* [KWA\({}^{+}\)14] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: fast combinatorial optimization with learning. In _Proc. of UAI_, 2014.
* [Lai87] Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. _The annals of statistics_, 1987.
* [Law72] Eugene L Lawler. A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem. _Management science_, 1972.
* [LNP\({}^{+}\)21] Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, et al. Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes. In _Proc. of AISTATS_, 2021.
* [MCP14] Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and optimal algorithms. In _Proc. of COLT_, 2014.
* [Neu15] Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In _Proc. of COLT_, 2015.

* [Oka73] Masashi Okamoto. Distinctness of the eigenvalues of a quadratic form in a multivariate sample. _The Annals of Statistics_, 1973.
* [PBVP20] Pierre Perrault, Etienne Boursier, Michal Valko, and Vianney Perchet. Statistical efficiency of thompson sampling for combinatorial semi-bandits. In _Proc. of NeurIPS_, 2020.
* [Per22] Pierre Perrault. When combinatorial thompson sampling meets approximation regret. In _Proc. of NeurIPS_, 2022.
* [PPV19] Pierre Perrault, Vianney Perchet, and Michal Valko. Exploiting structure of uncertainty for efficient matroid semi-bandits. In _Proc. of ICML_, 2019.
* [RG96] Ram Ravi and Michel X Goemans. The constrained minimum spanning tree problem. In _Scandinavian Workshop on Algorithm Theory_. Springer, 1996.
* [RS13] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In _Proc. of NeurIPS_, 2013.
* [S\({}^{+}\)03] Alexander Schrijver et al. _Combinatorial optimization: polyhedra and efficiency_, volume 24. Springer, 2003.
* [SN20] Arun Suggala and Praneeth Netrapalli. Follow the perturbed leader: Optimism and fast parallel algorithms for smooth minimax games. In _Proc. of NeurIPS_, 2020.
* [Vis21] Nisheeth K. Vishnoi. _Algorithms for Convex Optimization_. Cambridge University Press, 2021.
* [WLZL21] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In _Prof. of ICLR_, 2021.
* [WTP21] Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via frank-wolfe. In _Proc. of NeurIPS_, 2021.
* [ZODS21] Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. In _Proc. of NeurIPS_, 2021.