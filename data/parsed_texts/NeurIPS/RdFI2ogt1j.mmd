# A primer on in vitro biological neural networks

 Frithjof Gressmann Ashley Chen Lily Hexuan Xie

**Sarah Dowden Nancy Amato Lawrence Rauchwerger**

Siebel School of Computing and Data Science

University of Illinois at Urbana-Champaign

{fg14,ac136,hexuanx2,sodowden2,namato,rwerger}@illinois.edu

###### Abstract

Recent advances in bioengineering have enabled the creation of biological neural networks in vitro, raising the prospect of novel, unconventional platforms that can leverage genuine biological computation. The technology could help unlock computing paradigms that could be faster, more powerful, and more energy efficient than the silicon-based architectures that dominate today's computing landscape. However, engineering cell cultures for computing applications presents a radical departure from digital von Neumann architectures that computer scientists have grown accustomed to and will require a rethink of the entire stack. Here, we provide a brief overview of the key technologies, principles, and challenges of this emerging interdisciplinary field. We argue that seizing on its potential will require the development of new machine learning approaches that can process the vast observable activity of neuronal cell cultures and learn to control and make sense of their neural code. Such an effort could provide a pathway for leveraging biological neural networks and contribute to our understanding of what makes biological learning in neurons so incredibly efficient, holding broader lessons for the development of next-generation AI systems.

## 1 Introduction

In recent years, two mutually influential fields, artificial intelligence (AI) and neuroscience, have witnessed revolutionary developments leading to new synergistic opportunities. The remarkable success of large-scale neural networks in machine learning (ML) has enabled the effective modeling of complex data patterns and relationships across diverse domains such as natural language processing, computer vision, and time-series analysis [6]. In bioengineering, groundbreaking work in cellular reprogramming has enabled the conversion of ordinary human cells to stem cells [48], facilitating the in vitro cultivation of brain cell cultures for study and application outside of their natural biological context [46]. These advances make possible an emerging interdisciplinary effort to develop machine learning models that learn to interact with in vitro biological neural networks.

This primer presents a brief and introductory overview of this development and its potential. Reviewing long-standing efforts in multiple disciplines, we illustrate a possible strategy to leverage in vitro "wetware" that integrates work on neural simulation, spiking neural networks, surrogate optimization, neuromorphic computing, and machine learning models. Apart from delivering "wetware" substrates for computing, this undertaking can also serve as a vehicle for gaining a deeper understanding of the mechanisms that make biological neural processing so efficient. Thus, we expect that engineered living biological systems will continue to drive progress in ML and neuroscience and ultimately contribute to alternative hardware for artificial intelligence applications.

Principles, technologies, and challenges

To begin, this section 2 reviews key principles, technologies, and challenges. Although our review is far from comprehensive, it provides pointers to surveys in the respective disciplines that shape this emerging field. Against this background, in Section 3, we sketch out a possible pathway to leverage machine learning techniques to learn to harness in vitro systems. We discuss open challenges and future directions the community might help address, before concluding in Section 4.

### Biological neural networks

Given the significant success of neural networks in machine learning it can be easy to forget that _artificial_ neurons are a radical simplification of their biological counterparts that originally inspired them. Biological neurons do not only encode information in a fundamentally different way through spiking temporal dynamics [44], but also leverage processes such as synaptic, homeostatic and structural plasticity [3; 60], local error propagation via dendritic computation [39; 33], or neuromodulation [12] whose complexity far exceeds those of artificial neurons. Thus, it is no surprise that improvements in artificial neural networks continue to seek inspiration from neurophysiology and neuroscience [32; 65]. At the same time, despite significant progress, our understanding of biological neural computing leaves much to be desired. In particular, while the huge divergence in power requirements between artificial and biological neural networks suggests a greater energy efficiency of biological neural learning, exactly how it is achieved remains unclear. At a lower level, however, the general physiological principles that give rise to the complex neural dynamics have been uncovered. Put simply, biological neurons transmit signals in the form of potential differences between ions that are separated by the cell membrane [17]. The opening of ion channels in the membrane causes the cell to depolarize, a process that propagates along the membrane toward downstream cells. Upon reaching the synaptic terminal, neurotransmitters are released that diffuse to and induce a current in the post-synaptic neuron, which continues the signal transmission chain.

### Neural recording and stimulation

Fundamentally, these physiological processes can be manipulated in multiple ways [59]. For one, changing the extracellular potential can illicit depolarization and thus induce spiking activity. Alternatively, manipulation of the ion-channel permeability can alter the current flow and neural processing as a result. More fundamentally, neurotransmitter and blocker agents can interfere with the chemical balance at the synapses and modulate the synaptic neuro-transmission. In practice, experimental techniques leverage these principles to establish some level of control over neural dynamics.

Multi-electrode arraysAs one of the most established neuromodulation techniques [40; 50], electrical stimulation is commonly realized with extra-cellular electrodes that can detect and deliver potential differences in surrounding cells [43]. In particular, multi-electrode arrays (MEAs) that arrange electrodes in configurable mesh-like layouts allow high-resolution electrophysiological measurements with minimal disruption to cell tissues [10]. However, a major limitation of electrical stimulation is its inability to target specific cells and regions due to current spread [54].

Optogenetic stimulationOptogenetics has emerged as a promising alternative for neurostimulation, as it uses light to manipulate specific neurons and neuron groups [15; 57; 11]. The method involves introducing foreign light-sensitive transmembrane proteins, known as opsins, into target cell populations [35]. Opsins may, for instance, be delivered via viral infection, allowing the targeting of specific cells [63]. Subsequent light stimulation can then precisely activate or deactivate ion channels and neuronal activity without affecting neighboring cells. In particular, there is a wide variety of different microbial and genetically modified opsins that allow flexible experimental design and trade-offs [34]. For example, certain opsins may respond to low-intensity light, minimizing potential cell damage [42]. Optogenetic stimulation also works well with MEA-based systems, allowing for increasingly integrated experimentation platforms [45; 8; 53]. Thus, it is no surprise that optogenetic stimulation is seeing widespread adoption for in vitro experimentation [64; 36; 23; 67; 57; 11].

Continued developmentBesides these mainstream techniques, approaches such as magnetic stimulation [59; 9] or ultrasonic stimulation [28] continue to be developed and may emerge as additional options in the future.

#### 2.2.1 Neural coding and data processing

Decoding information from biological neurons is an essential yet challenging process. Modern recording devices, such as MEAs, allow the simultaneous recording of activity from hundreds to thousands of neurons [25] making neural data extremely high dimensional. Neural processes are also inherently stochastic. Synaptic vesicles, for example, are known to spontaneously release neurotransmitters even in the absence of evoked activity, causing random activity fluctuations as a result [2]. At the same time, recording devices and techniques introduce additional noise and uncertainty. For instance, since MEAs typically record extracellularly from a bunch of cells, complex post-processing algorithms that determine which neurons fired are required, adding another layer of uncertainty [21].

Several neural coding strategies have been proposed, such as rate, temporal, rank, and direct coding, to extract and represent the information content of neural activity [47, 62]. However, it remains unclear to what degree biological neural networks actually employ such encoding schemes. Although there is significant evidence that cognitive processes depend on the precise timing of neuronal activity [52], how information processing is reliably sustained in the presence of noise is an open question.

An alternative, higher-level approach to processing and decoding neural activity involves uncovering low-dimensional representations of high-dimensional neural data. Recent work suggests that neural activity can be effectively represented in fewer dimensions, indicating that the high-dimensional nature of current neural data might be highly redundant [26]. However, identifying these low-dimensional representations remains a challenge. Neural population activity exhibits inherent nonlinearity [19], yet many widely used dimensionality reduction techniques, such as Principal Component Analysis (PCA), make linear assumptions and may not capture data patterns effectively. At the same time, non-linear dimensionality reduction methods, such as autoencoders, often struggle with issues such as noise and overfitting in neural data [1].

As such, further advances are necessary in processing and decoding neural recordings. For example, the study of neural dynamics could help fill knowledge gaps and automatically uncover functional and structural properties of neural systems [41].

### In vitro neural networks

Notably, in vitro systems can increasingly support these efforts by providing a test bed for experimentation with realistic neural activity and extensive options for causal intervention. While the field was historically restricted to recording and stimulation technology in vivo [18], advances in bioengineering are opening up new possibilities for the study and use of neural systems and technologies in vitro.

Induced pluripotent stem cellsThe key to this development is induced pluripotent stem cells (iPS). They are a type of pluripotent cell derived from adult somatic cells that have been reprogrammed to an embryonic-like state, providing a virtually unlimited and less ethically problematic source of cells for biomedical research [48]. Initially developed using mouse fibroblasts, the development of human iPS cells (hiPSCs) shortly followed, with tremendous implications for biomedical and adjacent fields [61, 48, 29, 37]. In particular, the cheaper and more reliable production of cells with characteristics of embryonic stem cells makes it possible to consider the development of computing applications based on living neurons [46].

OrganoidsNotably, the so-called "organoid" technology is driving further progress to enable increasingly sophisticated applications [68]. Organoids are stem cell-derived, artificially generated three-dimensional (3D) cultures of cells. They can contain different cell types that self-organize through cell-sorting processes and spatial restrictions. Importantly, organoids can be generated in vitro from iPS cells. Researchers often opt for 3D cultures (organoids) over two-dimensional cultures (iPS cells) to obtain more physiologically realistic cellular compositions and achieve extensive culture growth, all while maintaining the potential for high-throughput screenings and analysis [51]. For instance, high-content imaging (HCI) and machine learning strategies allow for a fast analysis of data derived from organoids [13]. Overall, the technology has matured to the point where leveraging of cell cultures for computing applications is moving into the realm of distinct possibility [46].

Machine learning for leveraging in vitro neural networks

Despite the many advances and increasing technological sophistication, the engineering of cell cultures for computing applications faces, as we will argue, a bootstrapping problem. To illustrate this, consider that the development of conventional von Neumann type computers was driven by a theory of computation developed before any prototype of real-world computers would emerge. The question was not how a Turing-machine-like device could compute in theory, but how to solve a host of practical challenges to realize its real-world implementation. Engineering of biological tissue for computing, on the other hand, faces two problems at once: figuring out the _practical_ challenges of this effort while at the same time developing a _theory_ of how what has been developed works (or does not work). This has important methodological implications. Notably, data collection and algorithmic analysis of neural activity in itself may be of limited value as long as a formal framework for its interpretation is lacking [30]. Moreover, it is important to keep in mind that the engineered cell culture may not behave as it would in a healthy in vivo subject and is in this sense "functioning correctly". Furthermore, the high data dimensionality of recording and stimulation devices paired with the low signal-to-noise ratio makes systematic interactions with the system challenging (see Section 2). The extensive post-processing to sort and reconstruct spiking activity from the raw data adds another layer of uncertainty in itself [21; 16].

### Towards end-to-end optimization in vitro

In practice, however, despite the numerous challenges, there is a growing list of successes in learning to control and leverage neural systems in computing applications. For instance, brain-computer interfaces that are tested with human patients have been demonstrated to decode thought from neural activity recordings with remarkable accuracy [20]. For simpler organisms like the nematode Caenorhabditis elegans, optogenetic stimulation has been used to induce basic motor control [31]. Furthermore, the activity feedback of in vitro neurons has been used to realize basic video game play [27]. Arguably, the key to these successes has been effective machine learning methods that can build rich, implicit representations of the observed system dynamics. By framing the problem as a control problem amenable to optimization, it becomes possible to steer the neural activity toward desired states and dynamics despite the limited understanding and experimental control of the neural dynamics. The continued progress in machine learning supports these developments further. The rise of attention-based transformer architectures provides a scalable and effective way to build powerful representations from large-scale pre-training corpora that can be fine-tuned to specific applications [6]. To illustrate the potential of these developments and motivate further research, we sketch out a possible work in this area in the remainder of this paper. Crucially, it may be possible to sidestep bootstrapping issues by framing the bio-engineering task as a general, end-to-end optimization problem to improve the in vitro computing capabilities while uncovering the working principles of biological information processing.

Learning control modelWhile the technology and capabilities of experimental systems can vary significantly (Section 2), at a basic level, controlling in vitro cell cultures comes down to figuring out a stimulation sequence in response to observed activity. Specifically, a control model needs to learn to predict appropriate _stimulation_ of the available input channels at certain _times_. This may be, for instance, a sequence of times when to deliver stimulation through certain electrodes or via laser-induced optogenetic means. As such, the control model can be characterized as a mapping \(f:\mathds{R}^{N\times T}\rightarrow\mathds{R}^{j(x)\times T^{\prime}}\) that takes \(N\)-dimensional inputs and outputs a sequence of system stimulation times. The optimization objective is to find a set of parameters \(\theta\) such that the stimulation sequence \(f_{\theta}(x)=\vec{k}\) steers the observable neural dynamics of the biological neural network \(BNN(\vec{k})=\vec{y}\) in some desirable way. Note that this formulation does not assume anything about the internal characteristics of the neural systems. Training \(f\) successfully means not only overcoming the practical challenges of controlling a noisy, complex system, but it also implies uncovering some properties of the \(BNN\) that can be exploited to achieve the given objective. For instance, the model could simply use the \(BNN\) as a random projection into a higher dimensional space (this would be reminiscent of reservoir computing [14]). A more sophisticated model, however, may learn to exploit more intricate properties of the \(BNN\). For example, the model may leverage present plasticity by repeatedly delivering simulations to reconfigure the synaptic connectivity of the network.

Optimization approachHow to optimize \(f\) effectively is, in general, as much of an open question as what model and training approach would be most suitable. There are, however, principles that can guide the experimentation. First, the high cost and slow pace of lab experiments means that the training of \(f\) will likely rely on a pretraining scheme using synthetic data with subsequent fine-tuning on the more limited real-world data.

Notably, the long-standing developments in high-fidelity neural simulation present a rich resource for generating realistic synthetic data of neural dynamics. Thus, developing a simulation-driven pretraining corpus for a large-scale sequence model \(f\) is likely a worthwhile first step. One key question in this effort will be what level of simulation fidelity is required to allow \(f\) to represent relevant neural dynamics without over-fitting. Evidence from real-world data suggests that pre-trained representations may be able to bridge considerable transfer gaps. For instance, it has been demonstrated that pre-trained representations of neural activity can be general enough to transfer to different data domains, for example, between muscular electromyographic (EMG) signals to electroencephalographic (EEG) brain activity [4]. It may thus be sufficient to generate and train on synthetic data that only loosely match the lab data encountered at fine-tuning and inference time.

With suitable and sufficient data in place, the question becomes how to optimize \(f_{\theta}\). While non-continuous spiking dynamics are not differentiable in general, work on spiking neural networks (SNNs) has brought about a wide range of applicable optimization techniques [44; 49; 66]. In particular, surrogate gradient techniques offer a straightforward way to apply backpropagation-driven training to otherwise non-differentiable spiking dynamics [38]. Moreover, for the leaky-integrate and fire neuron model, several methods [5; 7; 58] provide exact gradients and can implement event-based gradient computation within the dynamical system [56]. These advances allow for a simulation framework that integrates the power of backpropagation-based machine learning models with theoretical and experimental models of biological neural networks [41].

With approximate or exact gradients available in simulation, it becomes possible to pre-train \(f\) in an end-to-end fashion using conventional gradient-descent optimization strategies. It is worth stressing that the objective in this setting is to find parameters such that \(f\)_exploits_ the biological neural network to minimize the loss, as opposed to minimizing the loss through \(f\) directly. This is reminiscent of a teacher-student [24] or a knowledge distillation setting [22], and work in this area may provide lessons for effective training.

Application in vitroFinally, with a pre-trained model \(f\) as a controller, it should be possible to "train" biological neural networks in vitro. To illustrate one possibility, consider the following approach. Training data \(x\) are encoded with the pre-trained \(f\) in a stimulation pattern \(\vec{k}\) that is fed into both the real-world lab system and the corresponding differentiable simulation. The cell culture output activity is recorded and used to compute the backward pass in the differentiable simulator with respect to \(f_{\theta}\). The resulting gradient that updates \(f_{\theta}\) will differ from the unknowable "true" gradient of the in vitro neural network, but it may be good enough to ensure forward progress in the iterative fine-tuning of \(f\). However, while a similar optimization approach has been used successfully to estimate the gradients of other real-world physical systems [55], how to effectively estimate such "good enough" gradients of the much more complicated in vitro systems presents an important open challenge. Leveraging simulation and data from lab experiments in such a way may produce data-driven training strategies whose effectiveness can be continually refined and experimentally verified. Overall, it is plausible that converging efforts, guided by feedback from real-world experiments, will help pave the way to increasingly sophisticated computing applications in vitro.

## 4 Conclusion

We have reviewed an emerging interdisciplinary endeavor to develop the technology to harness in vitro biological neural networks for computing applications. Key to this effort are machine learning models and optimization approaches that are able to learn to effectively interact with in vitro systems. Besides the direct practical motivations, it is likely that continued progress in this field will help uncover the working principles of biological neural processing. As such, engineered living biological networks may ultimately pave the way for next-generation hardware for artificial intelligence applications, be it in silico, in vitro, or both.

## References

* Altan et al. [2021] E. Altan, S. A. Solla, L. E. Miller, and E. J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. _PLOS Computational Biology_, 17(11):e1008591, Nov. 2021. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1008591. URL https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008591. Publisher: Public Library of Science.
* Andreae and Burrone [2018] L. C. Andreae and J. Burrone. The role of spontaneous neurotransmission in synapse and circuit development. _Journal of Neuroscience Research_, 96(3):354-359, Mar. 2018. ISSN 0360-4012. doi: 10.1002/jnr.24154. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5813191/.
* Billaudelle et al. [2021] S. Billaudelle, B. Cramer, M. A. Petrovici, K. Schreiber, D. Kappel, J. Schemmel, and K. Meier. Structural plasticity on an accelerated analog neuromorphic hardware system. _Neural Networks_, 133:11-20, Jan. 2021. ISSN 0893-6080. doi: 10.1016/j.neunet.2020.09.024. URL https://www.sciencedirect.com/science/article/pii/S0893608020303555.
* Bird et al. [2020] J. J. Bird, J. Kobylarz, D. R. Faria, A. Ekart, and E. P. Ribeiro. Cross-Domain MLP and CNN Transfer Learning for Biological Signal Processing: EEG and EMG. _IEEE Access_, 8:54789-54801, 2020. ISSN 2169-3536. doi: 10.1109/ACCESS.2020.2979074. URL https://ieeexplore.ieee.org/document/9027853. Conference Name: IEEE Access.
* Bohte and Kok [2000] S. M. Bohte and J. N. Kok. SpikeProp: Backpropagation for Networks of Spiking Neurons. _ESANN_, 2000.
* Bommasani et al. [2021] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Muniyiakwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Re, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramer, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the Opportunities and Risks of Foundation Models, July 2022. URL http://arxiv.org/abs/2108.07258. arXiv:2108.07258 [cs].
* Booij and tat Nguyen [2005] O. Booij and H. tat Nguyen. A gradient descent rule for spiking neurons emitting multiple spikes. _Information Processing Letters_, 95(6):552-558, Sept. 2005. ISSN 0020-0190. doi: 10.1016/j.ipl.2005.05.023. URL https://www.sciencedirect.com/science/article/pii/S0020019005001560.
* Brosch et al. [2020] M. Brosch, M. Deckert, S. Rathi, K. Takagaki, T. Weidner, F. W. Ohl, B. Schmidt, and M. T. Lippert. An optically transparent multi-electrode array for combined electrophysiology and optophysiology at the mesoscopic scale. _Journal of Neural Engineering_, 17(4):046014, July 2020. ISSN 1741-2552. doi: 10.1088/1741-2552/aba1a4.
* Bush [2020] J. Bush. Biophysical Neuromodulation: An Integrative Approach. 2020. URL https://hdl.handle.net/1920/11760.
* Chen et al. [2017] R. Chen, A. Canales, and P. Anikeeva. Neural recording and modulation technologies. _Nature Reviews Materials_, 2(2):1-16, Jan. 2017. ISSN 2058-8437. doi: 10.1038/natrevmats.2016.93. URL https://www.nature.com/articles/natrevmats201693. Publisher: Nature Publishing Group.

* Chen et al. [2022] W. Chen, C. Li, W. Liang, Y. Li, Z. Zou, Y. Xie, Y. Liao, L. Yu, Q. Lin, M. Huang, Z. Li, and X. Zhu. The Roles of Optogenetics and Technology in Neurobiology: A Review. _Frontiers in Aging Neuroscience_, 14:867863, Apr. 2022. ISSN 1663-4365. doi: 10.3389/fnagi.2022.867863. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9063564/.
* Cheong et al. [2022] W. H. Cheong, J. B. Jeon, J. H. In, G. Kim, H. Song, J. An, J. Park, Y. S. Kim, C. S. Hwang, and K. M. Kim. Demonstration of Neuromodulation-inspired Stashing System for Energy-efficient Learning of Spiking Neural Network using a Self-Rectifying Memristor Array. _Advanced Functional Materials_, 32(29):2200337, 2022. ISSN 1616-3028. doi: 10.1002/adfm.202200337. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/adfm.202200337. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adfm.202200337.
* Costamagna et al. [2021] G. Costamagna, G. P. Comi, and S. Corti. Advancing Drug Discovery for Neurological Disorders Using iPSC-Derived Neural Organoids. _International Journal of Molecular Sciences_, 22(5):2659, Mar. 2021. ISSN 1422-0067. doi: 10.3390/ijms22052659. URL https://www.mdpi.com/1422-0067/22/5/2659.
* Cucchi et al. [2022] M. Cucchi, S. Abreu, G. Ciccone, D. Brunner, and H. Kleemann. Hands-on reservoir computing: a tutorial for practical implementation. _Neuromorphic Computing and Engineering_, 2(3):032002, Aug. 2022. ISSN 2634-4386. doi: 10.1088/2634-4386/ac7db7. URL https://dx.doi.org/10.1088/2634-4386/ac7db7. Publisher: IOP Publishing.
* Deisseroth [2011] K. Deisseroth. Optogenetics. _Nature methods_, 8(1):26-29, Jan. 2011. ISSN 1548-7091. doi: 10.1038/nmeth.f.324. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6814250/.
* Durstewitz et al. [2023] D. Durstewitz, G. Koppe, and M. I. Thurm. Reconstructing computational system dynamics from neural data with recurrent neural networks. _Nature Reviews Neuroscience_, 24(11):693-710, Nov. 2023. ISSN 1471-0048. doi: 10.1038/s41583-023-00740-7. URL https://www.nature.com/articles/s41583-023-00740-7. Publisher: Nature Publishing Group.
* Ekeberg et al. [1991] O. Ekeberg, P. Wallen, A. Lansner, H. Traeven, L. Brodin, and S. Grillner. A computer based model for realistic simulations of neural networks: I. The single neuron and synaptic interaction. _Biological Cybernetics_, 65(2):81-90, June 1991. ISSN 0340-1200, 1432-0770. doi: 10.1007/BF00202382. URL http://link.springer.com/10.1007/BF00202382.
* Finger [1994] S. Finger. _Origins of neuroscience : a history of explorations into brain function_. New York : Oxford University Press, 1994. ISBN 978-0-19-506503-9. URL http://archive.org/details/originsofneurosc000ofing_p9s6.
* Fortunato et al. [2024] C. Fortunato, J. Bennasar-Vazquez, J. Park, J. C. Chang, L. E. Miller, J. T. Dudman, M. G. Perich, and J. A. Gallego. Nonlinear manifolds underlie neural population activity during behaviour. _bioRxiv_, page 2023.07.18.549575, Apr. 2024. doi: 10.1101/2023.07.18.549575. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370078/.
* Gao et al. [2021] X. Gao, Y. Wang, X. Chen, and S. Gao. Interface, interaction, and intelligence in generalized brain-computer interfaces. _Trends in Cognitive Sciences_, 25(8):671-684, Aug. 2021. ISSN 1364-6613, 1879-307X. doi: 10.1016/j.tics.2021.04.003. URL https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(21)00096-6. Publisher: Elsevier.
* Garcia et al. [2022] S. Garcia, A. P. Buccino, and P. Yger. How Do Spike Collisions Affect Spike Sorting Performance? _eNeuro_, 9(5):ENEURO.0105-22.2022, Sept. 2022. ISSN 2373-2822. doi: 10.1523/ENEURO.0105-22.2022. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9532018/.
* Gou et al. [2021] J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge Distillation: A Survey. _International Journal of Computer Vision_, 129(6):1789-1819, June 2021. ISSN 1573-1405. doi: 10.1007/s11263-021-01453-z. URL https://doi.org/10.1007/s11263-021-01453-z.
* Hallett et al. [2016] R. A. Hallett, S. P. Zimmerman, H. Yumerefendi, J. E. Bear, and B. Kuhlman. Correlating in Vitro and in Vivo Activities of Light-Inducible Dimers: A Cellular Optogenetics Guide. _ACS Synthetic Biology_, 5(1):53-64, Jan. 2016. doi: 10.1021/acssynbio.5b00119. URL https://doi.org/10.1021/acssynbio.5b00119. Publisher: American Chemical Society.

* Hu et al. [2022] C. Hu, X. Li, D. Liu, X. Chen, J. Wang, and X. Liu. Teacher-Student Architecture for Knowledge Learning: A Survey, Oct. 2022. URL https://arxiv.org/abs/2210.17332v1.
* Hurwitz et al. [2021] C. Hurwitz, A. Srivastava, K. Xu, J. Jude, M. G. Perich, L. E. Miller, and M. H. Hennig. Targeted Neural Dynamical Modeling, Oct. 2021. URL http://arxiv.org/abs/2110.14853. arXiv:2110.14853 [q-bio].
* Idesis et al. [2022] S. Idesis, M. Allegra, J. Vohryzek, Y. Sanz Perl, J. Faskowitz, O. Sporns, M. Corbetta, and G. Deco. A low dimensional embedding of brain dynamics enhances diagnostic accuracy and behavioral prediction in stroke. _Scientific Reports_, 13(1):15698, Sept. 2023. ISSN 2045-2322. doi: 10.1038/s41598-023-42533-z. URL https://www.nature.com/articles/s41598-023-42533-z. Publisher: Nature Publishing Group.
* Kagan et al. [2022] B. J. Kagan, A. C. Kitchen, N. T. Tran, F. Habibollahi, M. Khajehnejad, B. J. Parker, A. Bhat, B. Rollo, A. Razi, and K. J. Friston. In vitro neurons learn and exhibit sentience when embodied in a simulated game-world. _Neuron_, 110(23):3952-3969.e8, Dec. 2022. ISSN 0896-6273. doi: 10.1016/j.neuron.2022.09.001. URL https://www.cell.com/neuron/abstract/S0896-6273(22)00806-6. Publisher: Elsevier.
* Kamimura et al. [2020] H. A. S. Kamimura, A. Conti, N. Toschi, and E. E. Konofagou. Ultrasound Neuromodulation: Mechanisms and the Potential of Multimodal Stimulation for Neuronal Function Assessment. _Frontiers in Physics_, 8, May 2020. ISSN 2296-424X. doi: 10.3389/fphy.2020.00150. URL https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2020.00150/full. Publisher: Frontiers.
* Kegeles et al. [2020] E. Kegeles, A. Naumov, E. A. Karpulevich, P. Volchkov, and P. Baranov. Convolutional Neural Networks Can Predict Retinal Differentiation in Retinal Organoids. _Frontiers in Cellular Neuroscience_, 14:171, July 2020. ISSN 1662-5102. doi: 10.3389/fncel.2020.00171. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7350982/.
* Lazebnik [2004] Y. Lazebnik. Can a biologist fix a radio? -- or, what I learned while studying apoptosis. _Biochemistry (Moscow)_, 69(12):1403-1406, Dec. 2004. ISSN 0006-2979, 1608-3040. doi: 10.1007/s10541-005-0088-1. URL http://link.springer.com/article/10.1007/s10541-005-0088-1.
* Li et al. [2024] C. Li, G. Kreiman, and S. Ramanathan. Discovering neural policies to drive behaviour by integrating deep reinforcement learning agents with biological neural networks. _Nature Machine Intelligence_, 6(6):726-738, June 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00854-2. URL https://www.nature.com/articles/s42256-024-00854-2. Publisher: Nature Publishing Group.
* Li et al. [2024] G. Li, L. Deng, H. Tang, G. Pan, Y. Tian, K. Roy, and W. Maass. Brain-Inspired Computing: A Systematic Survey and Future Trends. _Proceedings of the IEEE_, 112(6):544-584, June 2024. ISSN 1558-2256. doi: 10.1109/JPROC.2024.3429360. URL https://ieeexplore.ieee.org/document/10636118/?arnumber=10636118. Conference Name: Proceedings of the IEEE.
* Liu et al. [2023] S. Liu, D. Akinwande, D. Kireev, and J. A. C. Incorvia. Graphene-Based Artificial Dendrites for Bio-Inspired Learning in Spiking Neuromorphic Systems, Oct. 2023. URL http://arxiv.org/abs/2310.02364. arXiv:2310.02364 [cond-mat, physics:physics].
* Masseck [2018] O. A. Masseck. A Guide to Optogenetic Applications, With Special Focus on Behavioral and _In Vivo_ Electrophysiological Experiments. In D. Manahan-Vaughan, editor, _Handbook of Behavioral Neuroscience_, volume 28 of _Handbook of Neural Plasticity Techniques_, pages 263-284. Elsevier, Jan. 2018. doi: 10.1016/B978-0-12-812028-6.00015-X. URL https://www.sciencedirect.com/science/article/pii/B978012812028600015X.
* Montagni et al. [2019] E. Montagni, F. Resta, A. L. A. Mascaro, and F. S. Pavone. Optogenetics in Brain Research: From a Strategy to Investigate Physiological Function to a Therapeutic Tool. _Photonics_, 6(3):92, Sept. 2019. ISSN 2304-6732. doi: 10.3390/photonics6030092. URL https://www.mdpi.com/2304-6732/6/3/92. Number: 3 Publisher: Multidisciplinary Digital Publishing Institute.

* Morton et al. [2019] A. Morton, C. Murawski, Y. Deng, C. Keum, G. B. Miles, J. A. Tello, and M. C. Gather. Photostimulation for In Vitro Optogenetics with High-Power Blue Organic Light-Emitting Diodes. _Advanced Biosystems_, 3(3):1800290, 2019. ISSN 2366-7478. doi: 10.1002/adbi.201800290. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/adbi.201800290. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adbi.201800290.
* Mossink et al. [2021] B. Mossink, A. H. Verboven, E. J. Van Hugte, T. M. Klein Gunnewiek, G. Parodi, K. Linda, C. Schoenmaker, T. Kleefstra, T. Kozicz, H. Van Bokhoven, D. Schubert, N. Nadif Kasri, and M. Frega. Human neuronal networks on micro-electrode arrays are a highly robust tool to study disease-specific genotype-phenotype correlations in vitro. _Stem Cell Reports_, 16(9):2182-2196, Sept. 2021. ISSN 22136711. doi: 10.1016/j.stemcr.2021.07.001. URL https://linkinghub.elsevier.com/retrieve/pii/S221367112100326X.
* Neftci et al. [2019] E. O. Neftci, H. Mostafa, and F. Zenke. Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks. _IEEE Signal Processing Magazine_, 36(6):51-63, Nov. 2019. ISSN 1558-0792. doi: 10.1109/MSP.2019.2931595. URL https://ieeexplore.ieee.org/abstract/document/8891809?casa_token=d6m_09356cEAAAA:r0v2W6cYvCgEAL8_cAT-Gy_HeedTOPkBDPHPLmCKb_H0k8cL0lgMqU4x6vp31YIps7FrLL7BEjw. Conference Name: IEEE Signal Processing Magazine.
* Pagkalos et al. [2023] M. Pagkalos, S. Chavlis, and P. Poirazi. Introducing the Dendrify framework for incorporating dendrites to spiking neural networks. _Nature Communications_, 14(1):131, Jan. 2023. ISSN 2041-1723. doi: 10.1038/s41467-022-35747-8. URL https://www.nature.com/articles/s41467-022-35747-8. Publisher: Nature Publishing Group.
* Rey et al. [2015] H. G. Rey, C. Pedreira, and R. Quian Quiroga. Past, present and future of spike sorting techniques. _Brain Research Bulletin_, 119:106-117, Oct. 2015. ISSN 0361-9230. doi: 10.1016/j.brainresbull.2015.04.007. URL https://www.sciencedirect.com/science/article/pii/S0361923015000684.
* Richards et al. [2019] B. A. Richards, T. P. Lillicrap, P. Beaudoin, Y. Bengio, R. Bogacz, A. Christensen, C. Clopath, R. P. Costa, A. de Berker, S. Ganguli, C. J. Gillon, D. Hafner, A. Kepecs, N. Kriegeskorte, P. Latham, G. W. Lindsay, K. D. Miller, R. Naud, C. C. Pack, P. Poirazi, P. Roelfsema, J. Sacramento, A. Saxe, B. Scellier, A. C. Schapiro, W. Senn, G. Wayne, D. Yamins, F. Zenke, J. Zylberberg, D. Therien, and K. P. Kording. A deep learning framework for neuroscience. _Nature Neuroscience_, 22(11):1761-1770, Nov. 2019. ISSN 1546-1726. doi: 10.1038/s41593-019-0520-2. URL https://www.nature.com/articles/s41593-019-0520-2. Publisher: Nature Publishing Group.
* Rodgers et al. [2021] J. Rodgers, B. Bano-Otalora, M. D. C. Belle, S. Paul, R. Hughes, P. Wright, R. McDowell, N. Milosavljevic, P. Orlowska-Feuer, F. P. Martial, J. Wynne, E. R. Ballister, R. Storchi, A. E. Allen, T. Brown, and R. J. Lucas. Using a bistable animal opsin for switchable and scalable optogenetic inhibition of neurons. _EMBO reports_, 22(5):e51866, May 2021. ISSN 1469-221X. doi: 10.15252/embr.202051866. URL https://www.embopress.org/doi/full/10.15252/embr.202051866. Publisher: John Wiley & Sons, Ltd.
* Ronchi et al. [2019] S. Ronchi, M. Fiscella, C. Marchetti, V. Viswam, J. Muller, U. Frey, and A. Hierlemann. Single-Cell Electrical Stimulation Using CMOS-Based High-Density Microelectrode Arrays. _Frontiers in Neuroscience_, 13, Mar. 2019. ISSN 1662-453X. doi: 10.3389/fnins.2019.00208. URL https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.00208/full. Publisher: Frontiers.
* Roy et al. [2019] K. Roy, A. Jaiswal, and P. Panda. Towards spike-based machine intelligence with neuromorphic computing. _Nature_, 575(7784):607-617, Nov. 2019. ISSN 0028-0836, 1476-4687. doi: 10.1038/s41586-019-1677-2. URL https://www.nature.com/articles/s41586-019-1677-2.
* Shin et al. [2021] H. Shin, S. Jeong, J.-H. Lee, W. Sun, N. Choi, and I.-J. Cho. 3D high-density microelectrode array with optical stimulation and drug delivery for investigating neural circuit dynamics. _Nature Communications_, 12(1):492, Jan. 2021. ISSN 2041-1723. doi: 10.1038/s41467-020-20763-3. URL https://www.nature.com/articles/s41467-020-20763-3. Publisher: Nature Publishing Group.

* Smirnova et al. [2023] L. Smirnova, B. S. Caffo, D. H. Gracias, Q. Huang, I. E. Morales Pantoja, B. Tang, D. J. Zack, C. A. Berlinicke, J. L. Boyd, T. D. Harris, E. C. Johnson, B. J. Kagan, J. Kahn, A. R. Muotri, B. L. Paulhamus, J. C. Schwamborn, J. Plotkin, A. S. Szalay, J. T. Vogelstein, P. F. Worley, and T. Hartung. Organoid intelligence (OI): the new frontier in biocomputing and intelligence-in-a-dish. _Frontiers in Science_, 0, 2023. ISSN 2813-6330. doi: 10.3389/fsci.2023.1017235. URL https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2023.1017235/full. Publisher: Frontiers.
* Taherkhani et al. [2020] A. Taherkhani, A. Belatreche, Y. Li, G. Cosma, L. P. Maguire, and T. M. McGinnity. A review of learning in biologically plausible spiking neural networks. _Neural Networks_, 122:253-272, Feb. 2020. ISSN 0893-6080. doi: 10.1016/j.neunet.2019.09.036. URL https://www.sciencedirect.com/science/article/pii/S0893608019303181.
* Takahashi et al. [2007] K. Takahashi, K. Tanabe, M. Ohnuki, M. Narita, T. Ichisaka, K. Tomoda, and S. Yamanaka. Induction of Pluripotent Stem Cells from Adult Human Fibroblasts by Defined Factors. _Cell_, 131(5):861-872, Nov. 2007. ISSN 0092-8674. doi: 10.1016/j.cell.2007.11.019. URL https://www.sciencedirect.com/science/article/pii/S0092867407014717.
* Tavanaei et al. [2019] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida. Deep learning in spiking neural networks. _Neural Networks_, 111:47-63, Mar. 2019. ISSN 0893-6080. doi: 10.1016/j.neunet.2018.12.002. URL https://www.sciencedirect.com/science/article/pii/S0893608018303332.
* Thornton et al. [2019] C. Thornton, F. Hutchings, and M. Kaiser. The Virtual Electrode Recording Tool for EXtracellular Potentials (VERTEX) Version 2.0: Modelling in vitro electrical stimulation of brain tissue. _Wellcome Open Research_, 4:20, Feb. 2019. ISSN 2398-502X. doi: 10.12688/wellcomeopenres.15058.1. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6439485/.
* van de Wetering et al. [2015] M. van de Wetering, H. E. Francies, J. M. Francis, G. Bounova, F. Iorio, A. Pronk, W. van Houdt, J. van Gorp, A. Taylor-Weiner, L. Kester, A. McLaren-Douglas, J. Blokker, S. Jaksani, S. Bartfeld, R. Volckman, P. van Sluis, V. S. Li, S. Seepo, C. Sekhar Pedamaluk, K. Cibulskis, S. L. Carter, A. McKenna, M. S. Lawrence, L. Lichtenstein, C. Stewart, J. Koster, R. Versteeg, A. van Oudenaarden, J. Saez-Rodriguez, R. G. Vries, G. Getz, L. Wessels, M. R. Stratton, U. McDermott, M. Meyerson, M. J. Garnett, and H. Clevers. Prospective derivation of a Living Organoid Biobank of colorectal cancer patients. _Cell_, 161(4):933-945, May 2015. ISSN 0092-8674. doi: 10.1016/j.cell.2015.03.053. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6428276/.
* VanRullen et al. [2005] R. VanRullen, R. Guyonneau, and S. J. Thorpe. Spike times make sense. _Trends in Neurosciences_, 28(1):1-4, Jan. 2005. ISSN 0166-2236, 1878-108X. doi: 10.1016/j.tins.2004.10.010. URL https://www.cell.com/trends/neurosciences/abstract/S0166-2236(04)00354-6. Publisher: Elsevier.
* Welkenhuysen et al. [2016] M. Welkenhuysen, L. Hoffman, Z. Luo, A. De Proft, C. Van den Haute, V. Baekelandt, Z. Debyser, G. Gielen, R. Puers, and D. Braeken. An integrated multi-electrode-optrode array for in vitro optogenetics. _Scientific Reports_, 6:20353, Feb. 2016. ISSN 2045-2322. doi: 10.1038/srep20353.
* Won et al. [2020] S. M. Won, E. Song, J. T. Reeder, and J. A. Rogers. Emerging Modalities and Implantable Technologies for Neuromodulation. _Cell_, 181(1):115-135, Apr. 2020. ISSN 0092-8674, 1097-4172. doi: 10.1016/j.cell.2020.02.054. URL https://www.cell.com/cell/abstract/S0092-8674(20)30231-2. Publisher: Elsevier.
* Wright et al. [2022] L. G. Wright, T. Onodera, M. M. Stein, T. Wang, D. T. Schachter, Z. Hu, and P. L. McMahon. Deep physical neural networks trained with backpropagation. _Nature_, 601(7894):549-555, Jan. 2022. ISSN 1476-4687. doi: 10.1038/s41586-021-04223-6. URL https://www.nature.com/articles/s41586-021-04223-6. Publisher: Nature Publishing Group.
* Wunderlich and Pehle [2021] T. C. Wunderlich and C. Pehle. Event-based backpropagation can compute exact gradients for spiking neural networks. _Scientific Reports_, 11(1):12829, June 2021. ISSN 2045-2322. doi: 10.1038/s41598-021-91786-z. URL https://www.nature.com/articles/s41598-021-91786-z. Publisher: Nature Publishing Group.

* Xu et al. [2023] S. Xu, M. Momin, S. Ahmed, A. Hossain, L. Veeramuthu, A. Pandiyan, C.-C. Kuo, and T. Zhou. Illuminating the Brain: Advances and Perspectives in Optoelectronics for Neural Activity Monitoring and Modulation. _Advanced Materials (Deerfield Beach, Fla.)_, 35(42):e2303267, Oct. 2023. ISSN 1521-4095. doi: 10.1002/adma.202303267.
* Xu et al. [2013] Y. Xu, X. Zeng, L. Han, and J. Yang. A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks. _Neural Networks_, 43:99-113, July 2013. ISSN 0893-6080. doi: 10.1016/j.neunet.2013.02.003. URL https://www.sciencedirect.com/science/article/pii/S0893608013000440.
* Yang et al. [2024] F. Yang, X. Wu, S. Cai, and G. Hong. Bioinspired nanotransducers for neuromodulation. _Nano Research_, 17(2):618-632, Feb. 2024. ISSN 1998-0000. doi: 10.1007/s12274-023-6136-6. URL https://doi.org/10.1007/s12274-023-6136-6.
* Yang et al. [2020] J. Yang, R. Wang, Y. Ren, J. Mao, Z. Wang, Y. Zhou, and S. Han. Neuromorphic Engineering: From Biological to Spike-Based Hardware Nervous Systems. _Advanced Materials_, 32(52):2003610, Dec. 2020. ISSN 0935-9648, 1521-4095. doi: 10.1002/adma.202003610. URL https://onlinelibrary.wiley.com/doi/10.1002/adma.202003610.
* Ye et al. [2013] L. Ye, C. Swingen, and J. Zhang. Induced pluripotent stem cells and their potential for basic and clinical sciences. _Current cardiology reviews_, 9(1):63-72, 2013. URL https://www.ingentaconnect.com/content/ben/ccr/2013/00000009/0000001/art00008. Publisher: Bentham Science Publishers.
* Yi et al. [2023] Z. Yi, J. Lian, Q. Liu, H. Zhu, D. Liang, and J. Liu. Learning rules in spiking neural networks: A survey. _Neurocomputing_, 531:163-179, Apr. 2023. ISSN 0925-2312. doi: 10.1016/j.neucom.2023.02.026. URL https://www.sciencedirect.com/science/article/pii/S0925231223001662.
* Yizhar et al. [2020] O. Yizhar, L. E. Fenno, T. J. Davidson, M. Mogri, and K. Deisseroth. Optogenetics in neural systems. _Neuron_, 71(1):9-34, July 2011. ISSN 1097-4199. doi: 10.1016/j.neuron.2011.06.004.
* Zabolocki et al. [2020] M. Zabolocki, K. McCormack, M. van den Hurk, B. Milky, A. P. Shoubridge, R. Adams, J. Tran, A. Mahadevan-Jansen, P. Reineck, J. Thomas, M. R. Hutchinson, C. K. H. Mak, A. Afonuevo, L. H. Chew, A. J. Hirst, V. M. Lee, E. Knock, and C. Bardy. BrainPhys neuronal medium optimized for imaging and optogenetics in vitro. _Nature Communications_, 11(1):5550, Nov. 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-19275-x. URL https://www.nature.com/articles/s41467-020-19275-x. Publisher: Nature Publishing Group.
* Zador et al. [2023] A. Zador, S. Escola, B. Richards, B. Olveczky, Y. Bengio, K. Boahen, M. Botvinick, D. Chklovskii, A. Churchland, C. Clopath, J. DiCarlo, S. Ganguli, J. Hawkins, K. Kording, A. Koulakov, Y. LeCun, T. Lillicrap, A. Marblestone, B. Olshausen, A. Pouget, C. Savin, T. Sejnowski, E. Simoncelli, S. Solla, D. Sussillo, A. S. Tolias, and D. Tsao. Catalyzing next-generation Artificial Intelligence through NeuroAI. _Nature Communications_, 14(1):1597, Mar. 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-37180-x. URL https://www.nature.com/articles/s41467-023-37180-x.
* Zenke and Vogels [2021] F. Zenke and T. P. Vogels. The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks. _Neural Computation_, 33(4):899-925, Mar. 2021. ISSN 0899-7667. doi: 10.1162/neco_a_01367. URL https://doi.org/10.1162/neco_a_01367.
* Zhang et al. [2023] X. Zhang, Z. Dou, S.-H. Kim, G. Upadhyay, D. Havert, S. Kang, K. Kazemi, K.-Y. Huang, O. Aydin, R. Huang, S. Rahman, A. Ellis-Mohr, H. A. Noblet, K. H. Lim, H. J. Chung, H. J. Gritton, M. T. A. Saif, H. J. Kong, J. M. Beggs, and M. Gazzola. 'Mind in Vitro' platforms: Versatile, scalable, robust and open solutions to interfacing with living neurons, Aug. 2023. URL https://www.biorxiv.org/content/10.1101/2023.08.21.554033v1. Pages: 2023.08.21.554033 Section: New Results.
* Zhao et al. [2022] Z. Zhao, X. Chen, A. M. Dowbaj, A. Sijukic, K. Bratlie, L. Lin, E. L. S. Fong, G. M. Balachandran, Z. Chen, A. Soragni, M. Huch, Y. A. Zeng, Q. Wang, and H. Yu. Organoids. _Nature Reviews Methods Primers_, 2(1):1-21, Dec. 2022. ISSN 2662-8449. doi: 10.1038/s43586-022-00174-y. URL https://www.nature.com/articles/s43586-022-00174-y. Publisher: Nature Publishing Group.