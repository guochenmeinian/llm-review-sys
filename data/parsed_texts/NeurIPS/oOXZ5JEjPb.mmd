# Activity Grammars for Temporal Action Segmentation

 Dayoung Gong1  Joonseok Lee1  Deunsol Jung  Suha Kwak  Minsu Cho

Pohang University of Science and Technology (POSTECH)

{dayoung.gong, jameslee, deunsol.jung, suha.kwak, mcho}@postech.ac.kr

http://cvlab.postech.ac.kr/research/KARI

Equal contribution.

###### Abstract

Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments, remains challenging for this reason. This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation. We propose a novel grammar induction algorithm that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.

## 1 Introduction

Human activities in videos do not proceed by accident; they are structured being subject to generative rules imposed by the goal of activities, the properties of individual actions, the physical environment, and so on. Comprehending such a compositional structure of multi-granular semantics in human activity poses a significant challenge in video understanding research. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments, remains challenging due to the reason. The recent methods based on deep neural networks [25, 9, 45, 2, 15, 16, 1] have shown remarkable improvement in learning temporal relations of actions in an implicit manner, but often face out-of-context errors that reveal the lack of capacity to capture the intricate structures of human activity, and the scarcity of annotated data exacerbates the issue in training. In this work, we address the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation.

Grammar is a natural and powerful way of explicitly representing the hierarchical structure of languages [14] and can also be applied to express the structure of activities. Despite the extensive body of grammar-based research for video understanding [23, 24, 33, 35, 32], none of these approaches have successfully integrated recursive rules. Recursive rules are indispensable for expressing complex and realistic structures found in action phrases and activities. To achieve this, we introduce a novel activity grammar induction algorithm, _Key-Action-based Recursive Induction_ (KARI), that extracts a powerful probabilistic context-free grammar while capturing the characteristics of the activity. Since an activity is composed of multiple actions, each activity exhibits a distinctive temporal structure based on pivotal actions, setting it apart from other activities. The proposed grammar inductionenables recursive rules with flexible temporal orders, which leads to powerful generalization capability. We also propose a novel activity grammar evaluation framework to evaluate the generalization and discrimination power of the proposed grammar induction algorithm. To incorporate the induced activity grammar into temporal action segmentation, we develop an effective parser, dubbed BEP, which searches the optimal rules according to the classification outputs generated by an off-the-shelf action segmentation model. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure.

The main contribution of this paper can be summarized as follows:

* We introduce a novel grammar induction algorithm that extracts a powerful context-free grammar with recursive rules based on key actions and temporal dependencies.
* We develop an effective parser that efficiently handles recursive rules of context-free grammar by using Breadth-first search and pruning.
* We propose a new grammar evaluation framework to assess the generalization and discrimination capabilities of the induced activity grammars.
* We show that the proposed method significantly improves the performance of temporal action segmentation models, as demonstrated through a comprehensive evaluation on two benchmarks, Breakfast and 50 Salads.

## 2 Related work

**Grammar for activity analysis.** Grammar is an essential tool to represent the compositional structure of language [14] and has been mainly studied in the context of natural language processing (NLP) [21; 22; 20; 37]. Grammar has been extensively studied in various research areas [28; 29; 32; 8; 43; 13; 42; 11; 27; 12]. Similarly, a grammatical framework can be used to express the structure of activities. Several work [23; 24; 33; 35] have defined context-free grammars based on possible temporal transitions between actions for action detection and recognition. Vo and Bovick [41] propose a stochastic grammar to model a hierarchical representation of activity based on AND-rules and OR-rules. Richard et al. [34] propose a context-free grammar defined on action sequences for weakly-supervised temporal action segmentation. Qi et al. [30; 32; 31] utilize a grammar induction algorithm named ADIOS [37] to induce grammar from action corpus. However, none of the proposed grammar for activity analysis includes recursive rules, which are a fundamental factor in expressing repetitions of actions or action phrases. In this paper, we propose a novel action grammar for temporal action segmentation based on key action and temporal dependency between actions considering recursive temporal structure.

**Temporal action segmentation (TAS).** Various methods have been proposed to address the task. Early work utilizes temporal sliding windows [36; 19] to detect action segments, and language-based methods [24; 23] has been proposed to utilize a temporal hierarchy of actions during segmentation. Recently, a deep-learning-based model named the temporal convolutional networks (TCN) has been proposed with an encoder-decoder architecture [25; 9]. Moreover, transformer-based models [45; 2] are recently introduced to leverage global temporal relations between actions based on self-attention and cross-attention mechanisms [40]. Other researches have been proposed to improve the accuracy of temporal action segmentation based on existing models [9; 45]. Huang _et al._[15] introduce a network module named Graph-based Temporal Reasoning Module (GTRM) that is applied on top of baseline models to learn temporal relations of action segments. Ishikawa _et al._[16] suggest an action segment refinement framework (ASRF) dividing a task into frame-wise action segmentation and boundary regression. They refine frame-level classification results with the predicted action boundaries. Gao _et al._[10] propose a global-to-local search scheme to find appropriate receptive field combinations instead of heuristic respective fields. Ahn and Lee [1] recently propose a hierarchical action segmentation refiner (HASR), which refines segmentation results by applying multi-granular context information from videos. A fast approximate inference method named FIFA for temporal action segmentation and alignment instead of dynamic programming is proposed by Souri _et al._[38]. Other researches [5; 6] reformulate TAS as a cross-domain problem with different domains of spatio-temporal variations, introducing self-supervised temporal domain adaptation. Xu _et al._[44] proposes differentiable temporal logic (DTL), which is a model-agnostic framework to give temporal constraints to neural networks. In this paper, we propose a neuro-symbolic approach where the activity grammar induced by the proposed grammar induction algorithm guides a temporal action segmentation model to refine segmental errors through parsing.

## 3 Our approach

Given a video of \(T\) frames \(\bm{F}=[F_{1},F_{2},...,F_{T}]\) and a predefined set of action classes \(\mathcal{A}\), the goal of temporal action segmentation is to translate the video into a sequence of actions \(\bm{a}=[a_{1},a_{2},...,a_{N}]\) and their associated frame lengths \(\bm{l}=[l_{1},l_{2},...,l_{N}]\) where \(N\) is unknown, \(a_{i}\in\mathcal{A}\) for \(1\leq i\leq N\), \(a_{i}\neq a_{i+1}\) for \(1\leq i\leq N-1\), and \(\sum_{i=1}^{N}l_{i}=T\).2 The resultant output of \(\bm{a}\) and \(\bm{l}\) indicates that the video consists of \(N\) segments and each pair \((a_{i},l_{i})\) represents the action and length of \(i_{\rm th}\) segment.

Footnote 2: In fact, this form of output is equivalent to that of frame-level action classification, which predict an action class for each frame, and the sequence of frame-level actions is easily converted to \((\bm{a},\bm{l})\) and vice versa.

In this work, we introduce an activity grammar that guides neural predictions for temporal action segmentation through parsing. We propose a novel activity grammar induction algorithm named KARI and an efficient parser called BEP. The overall pipeline of the proposed method consists of three steps, as illustrated in Fig. 1. First of all, KARI induces an activity grammar from action sequences in the training data. Using the KARI-induced grammar, BEP then takes the frame-level class prediction \(\bm{Y}\in\mathbb{R}^{T\times|\mathcal{A}|}\) from the off-the-shelf temporal action segmentation model [45; 9] and produces a grammar-consistent action sequence \(\bm{a}^{*}\). Finally, segmentation optimization is performed to obtain optimal action lengths \(\bm{l}^{*}\) based on \(\bm{a}^{*}\) and \(\bm{Y}\). In the following, we introduce the activity grammar as a probabilistic context-free grammar (Section 3.1), present KARI (Section 3.2) and BEP (Section 3.3), and describe a segmentation optimization method for final outputs (Section 3.4).

### Activity grammar

We define the _activity grammar_ as a probabilistic context-free grammar (PCFG) [17], designed to derive diverse action sequences pertaining to a specific activity class. The activity grammar, denoted as \(G=(\mathcal{V},\Sigma,\mathcal{P},S)\), follows the conventional PCFG which consists of four components: a finite set of variables \(\mathcal{V}\), a finite set of terminals \(\Sigma\), a finite set of production rules \(\mathcal{P}\), and the start symbol \(S\in\mathcal{V}\). In our context, the set of terminals \(\Sigma\) becomes the set of action classes \(\mathcal{A}\), and the production rules \(\mathcal{P}\) are used to generate action sequences from the start variable \(S\). We use two types of production rules, 'AND' and 'OR', defined as follows:

\[\mathrm{AND}: V\rightarrow\alpha \mathrm{where}\,V\in\mathcal{V}\,\mathrm{and}\,\alpha\in(\Sigma \cup\mathcal{V})^{*},\] (1) \[\mathrm{OR}: V\to V_{1}\left[p_{1}\right]\left|\,V_{2}\left[p_{2} \right]\,\right|\,\cdots\left|\,V_{n}\left[p_{n}\right]\right. \mathrm{where}\,V,V_{1},...,V_{n}\in\mathcal{V}.\] (2)

The AND rule replaces a head variable \(V\) with a sequence of variables and terminals \(\alpha\), determining the order of the terminals and variables. In contrast, the OR rule converts a head variable \(V\) to a sub-variable \(V_{i}\) with the probability \(p_{i}\), providing multiple alternatives for replacement; '!' denotes 'OR' operation. These two types of rules allow us to generate action sequences hierarchically.

### Grammar induction: Key-Action-based Recursive Induction (KARI)

Grammar induction refers to the process of learning grammars from data [37]. In our context, it takes action sequences of a specific activity in the training set and produces an activity grammar that is

Figure 1: **Overall pipeline of the proposed method.** (a) KARI induces an activity grammar \(G\) from action sequences in the training data, (b) BEP parses neural predictions \(\bm{Y}\) from the off-the-shelf temporal action segmentation model given a video \(\bm{F}\) by using the KARI-induced grammar \(G\), and (c) the final output of optimal action sequences and lengths \((\bm{a}^{*},\bm{l}^{*})\) is achieved through segmentation optimization. It is best viewed in color.

able to parse action sequences of the activity; the induced grammar should be able to parse unseen sequences of the activity as well as the sequences in the training set for generalization. To obtain an effective activity grammar avoiding under-/over-generalization, we introduce two main concepts for grammar induction: _key action_ and _temporal dependency_.

The key actions for a specific activity are those consistently present in every action sequence from the training dataset. Specifically, the top \(N^{\mathrm{key}}\) most frequently occurring actions among these are selected as the key actions. The hyperparameters of the number of key actions \(N^{\mathrm{key}}\) affects the degree of generalization achieved by the induced grammar. The temporal dependency refers to the relevance of temporal orders across actions. Temporally independent actions do not occur in a specific temporal order. This concept of temporal dependency can also be extended to groups of actions, meaning that some groups of actions can be temporally dependent on others.

We induce an activity grammar based on the key actions and the temporal dependency. Action sequences are divided into sub-sequences using the key actions as reference points, and the temporal dependencies between actions within the sub-sequences are established; temporally dependent actions are represented using AND rules (Eq. 1), while temporally independent actions are expressed with OR rules (Eq. 2). We give an example of grammar induction in Fig. 2; four action sequences are given in Fig. 2a, where the action class 'pour coffee' is chosen as the key action with the number of key actions \(N^{\mathrm{key}}\) set to 1.

Given the action sequences from the training dataset \(\mathcal{D}\), we begin grammar induction by identifying a set of key actions \(\mathcal{K}\subset\mathcal{A}\) with the pre-defined hyperparameter \(N^{\mathrm{key}}\). Using the key actions, each action sequence \(\bm{a}\in\mathcal{D}\) is divided into three parts: \(\bm{a}=[\bm{a}^{\mathrm{L}},\bm{a}^{\mathrm{M}},\bm{a}^{\mathrm{R}}]\). The sub-sequences \(\bm{a}^{\mathrm{L}},\bm{a}^{\mathrm{M}}\), and \(\bm{a}^{\mathrm{R}}\) denote the portions of the original action sequence that occurred _before_, _between_, and _after_ the key actions, respectively; the sub-sequence \(\bm{a}^{\mathrm{M}}\) starts from the first key action and includes up to the last key action in \(\mathcal{K}\). An example in Fig. 2b shows that the action sequence \(\bm{a}_{3}\) is divided into three sub-sequences using the key actions. For notational convenience, we will use the superscript \(\Omega\in\{\mathrm{L},\mathrm{M},\mathrm{R}\}\) to denote one of the three parts. All action sub-sequences \(\bm{a}^{\Omega}\) in a specific part \(\Omega\) are grouped to consist in a corresponding set of sub-sequences \(\mathcal{D}^{\Omega}\) (_cf._ Fig 2c). The action set \(\mathcal{A}^{\Omega}\subseteq\mathcal{A}\)

Figure 2: **Example of activity grammar induction of KARI. (a) Example action sequences are provided with ‘pour coffee’ as the key action with \(N^{\mathrm{key}}\) set to 1. (b) Action sequence, _e.g._, \(\bm{a}_{3}\), is segmented into sub-sequences based on key actions. (c) All action sub-sequences \(\bm{a}^{\Omega}\) consist in a set of sub-sequences \(\mathcal{D}^{\Omega}\). (d) The action set \(\mathcal{A}^{\Omega}\) contains all the actions occurring in \(\mathcal{D}^{\Omega}\). (e) Temporally independent actions are grouped, where each action group is temporally dependent in the action group sequence \(\bm{d}^{\Omega}\). (f) The resultant KARI-induced activity grammar is shown. For simplicity, we omit the probability, and it is best viewed in color.**

is then defined to contain all the actions occurring in \(\mathcal{D}^{\Omega}\) (_cf._ Fig 2d). To determine the temporal dependencies among the actions of \(\mathcal{A}^{\Omega}\), pairwise temporal orders are considered as follows. If one action always occurs before the other in \(\mathcal{D}^{\Omega}\), then the two actions are temporally dependent and otherwise temporally independent. Based on the concepts, we construct the action group sequence \(\bm{d}^{\Omega}\) by collecting the temporally independent actions as an action group and arranging such action groups according to their temporal dependencies (_cf._ Fig 2e).

In the following, we describe how to construct the production rules \(\mathcal{P}\) of the activity grammar \(G\).

**Start rule**. We first create the rule for the start variable \(S\):

\[S\to V^{\mathrm{L}}\,V^{\mathrm{M}}\,V^{\mathrm{R}}\,,\] (3)

where \(V^{\mathrm{L}}\), \(V^{\mathrm{M}}\), and \(V^{\mathrm{R}}\) are variables used to derive left, middle, and right parts of the action sequence, respectively.

**Rule for the variable \(V^{\Omega}\)**. For \(V^{\Omega}\), \(\Omega\in\{\mathrm{L},\mathrm{R}\}\), we construct an AND rule of action groups based on action group sequence \(\bm{d}^{\Omega}\):

\[V^{\Omega}\to V^{\Omega}_{1}\,V^{\Omega}_{2}\,\cdots\,V^{\Omega}_{|\bm{d}^{ \Omega}|},\] (4)

where the variable \(V^{\Omega}_{i}\) represents the \(i_{\mathrm{th}}\) action group in the action group sequence \(\bm{d}^{\Omega}_{i}\). Since actions in an action group are considered temporally independent, we construct an OR rule for each action group:

\[V^{\Omega}_{i}\to d^{\Omega}_{i,1}\,V^{\Omega}_{i}\,\left[p^{\Omega}_{i,1} \right]|\,d^{\Omega}_{i,2}\,V^{\Omega}_{i}\,\left[p^{\Omega}_{i,2}\right]|\, \cdots|\,d^{\Omega}_{i,|\bm{d}^{\Omega}|}\,V_{i}\,[p^{\Omega}_{i,|\bm{d}^{ \Omega}|}]\,|\,\epsilon\,[p^{\Omega}_{i,\epsilon}]\,,\] (5)

where \(d^{\Omega}_{i,j}\) denotes the \(j_{\mathrm{th}}\) action from the action group \(\bm{d}^{\Omega}_{i}\). The variable \(V^{\Omega}_{i}\) yields \(d^{\Omega}_{i,j}\,V^{\Omega}_{i}\) with the probability \(p^{\Omega}_{i,j}\). This rule can be recursively used to proceed to the variable \(V^{\Omega}_{i}\) in the next step. This recursive structure allows for repeated selection of actions within the same action group, leading to the generation of diverse action sequences, which is effective for generalization. To avoid an infinite loop of the recursion, the empty string \(\epsilon\) with the escape probability \(p^{\Omega}_{i,\epsilon}\) is added to Eq. 5. For the details, refer to the transition probability \(p^{\Omega}_{i,j}\) and the escape probability \(p^{\Omega}_{i,\epsilon}\) in Appendix A.1.

**Rule for the middle variable \(V^{\mathrm{M}}\).** Since the temporal order of key actions might vary, we consider all the possible temporal orders between key actions in \(\mathcal{K}\). A set of temporal permutations of actions is denoted as \(\Pi\), where each possible temporal permutation is represented by the OR rule:

\[V^{\mathrm{M}}\to V^{\mathrm{M}}_{1}\,[p^{\mathrm{M}}_{1}]\,|\,V^{\mathrm{M}} _{2}\,[p^{\mathrm{M}}_{2}]\,|\,\cdots\,|\,V^{\mathrm{M}}_{|\Pi|}\,[p^{\mathrm{ M}}_{\Pi}]\,|\,\epsilon\,[p^{\mathrm{M}}_{\epsilon}].\] (6)

The rule for the permutation variable \(V^{\mathrm{M}}_{i}\) is defined by the AND rule:

\[V^{\mathrm{M}}_{i}\to\pi_{i,1}\,V^{\mathrm{M}(i,1)}\,\cdots\,\pi_{i,|\bm{\pi} _{i}|}\,V^{\mathrm{M}(i,|\bm{\pi}_{i}|)}\,V^{\mathrm{M}}\,,\] (7)

where all the key actions are included. Note that \(\pi_{i,j}\) represents the \(j_{\mathrm{th}}\) action of the permutation \(\bm{\pi}_{i}\in\Pi\), and the variable \(V^{\mathrm{M}(i,j)}\) derives action sub-sequences between actions \(\pi_{i,j}\) and \(\pi_{i,j+1}\). The production rule for \(V^{\mathrm{M}(i,j)}\) adheres to the rules specified in Eq. 4 and 5. The resultant KARI-induced grammar from the example is shown in Fig. 2f, highlighting the compositional structure of actions.

### Parser: Breadth-first Earley Parser (BEP)

The goal of the parser is to identify the optimal action sequence \(\bm{\alpha}^{\star}\) by discovering the most likely grammatical structure based on the output of the action segmentation model [9; 45]. In other words, the parser examines the production rules of the activity grammar to determine whether the given neural prediction \(\bm{Y}\) can be parsed by the grammar \(G\). However, when the grammar includes recursive rules, the existing parser struggles to complete the parsing within a reasonable time due to the significant increase in branches from the parse tree. To address this challenge, we introduce an effective parser dubbed BEP, integrating Breadth-first search (BFS) and a pruning technique into a generalized Earley parser (GEP) [32]. Since the BFS prioritizes production rules closer to the start variable, it helps the parser understand the entire context of the activity before branching to recursive iterations. Simultaneously, pruning effectively reduces the vast search space generated by OR nodes and recursion, enabling the parser to focus on more relevant rules for the activity.

For parsing, we employ two heuristic probabilities introduced in [32] to compute the probability of variables and terminals within the parse tree. Specifically, let \(\bm{Y}_{t,x}\) denote the probability of frame being labeled as \(x\). In this context, we denote the last action in the action sequence \(\bm{a}\) as \(x\), _i.e._\(x=a_{N}\), where \(\bm{a}=[a_{1},a_{2},...,a_{N}]\), for simplicity. The transition probability \(g(x\,|\,\bm{a}_{1:N-1},G)\) determines the probability of parsing action \(x\) given the \(\bm{a}_{1:N-1}\) and the grammar \(G\).

The parsing probability \(p(\bm{F}_{1:T}\rightarrow\bm{a}\,|\,G)\) computes the probability of \(\bm{a}\) being the action sequence for \(\bm{F}_{1:T}\). The probability at \(t=1\) is initialized by:

\[p(F_{1}\rightarrow\bm{a}\,|\,G)=\begin{cases}g(x\,|\,\epsilon,G)\,\bm{Y}_{1,x }&\text{if $\bm{a}$ contains only $x$},\\ 0&\text{otherwise,}\end{cases}\] (8)

where \(\epsilon\) indicates an empty string.

Since we assume that the last action of \(\bm{a}\) is classified as \(x\), the parsing probability \(p(\bm{F}_{1:t}\rightarrow\bm{a}\,|\,G)\) can be represented with the probability of the previous frames:

\[p(\bm{F}_{1:t}\rightarrow\bm{a}\,|\,G)=\bm{Y}_{t,x}(p(\bm{F}_{1:t-1} \rightarrow\bm{a}\,|\,G)+g(x\,|\,\bm{a}_{1:N-1},G)\,p(\bm{F}_{1:t-1} \rightarrow\bm{a}_{1:N-1}\,|\,G)\,).\] (9)

The prefix probability \(p(\bm{F}_{1:T}\rightarrow\bm{a}...\,|\,G)\) represents the probability of \(\bm{a}\) being the prefix of \(\bm{a}^{*}\). This probability is computed by measuring the probability that \(\bm{a}\) is the action sequence for the frame \(\bm{F}_{1:t}\) with \(t\) in the range \([1,T]\):

\[p(\bm{F}_{1:T}\rightarrow\bm{a}...\,|\,G)=p(F_{1}\rightarrow\bm{a}\,|\,G)+g (x\,|\,\bm{a}_{1:N-1},G)\sum_{t=2}^{T}\bm{Y}_{t,x}\,p(\bm{F}_{1:t-1} \rightarrow\bm{a}_{1:N-1}\,|\,G).\] (10)

The parsing operation is structured following the original Earley parser [7], consisting of three key operations: prediction, scanning, and completion. These operations involve the update and generation of states, where every state comprises the rule being processed, the parent state, the parsed action sequence denoted as \(\bm{a}\), and the prefix probability denoted as \(p(\bm{a}...)\). The states are enqueued and prioritized by their depth \(d\) within the parse tree.

* **Prediction**: for every state \(Q(m,n,d)\) of the form \((A\rightarrow\alpha\cdot B\beta,Q(i,j,k),\bm{a},p(\bm{a}...))\), add \((B\rightarrow\cdot\Gamma,Q(m,n,d),\bm{a},p(\bm{a}...))\) to \(Q(m,n,d+1)\) for every production rule in the grammar with \(B\) on the left-hand side.
* **Scanning**: for every state in \(Q(m,n,d)\) of the form \((A\rightarrow\alpha\cdot w\beta,Q(i,j,k),\bm{a},p(\bm{a}...))\), append the new terminal \(w\) to \(\bm{a}\) and compute the probability \(p((\bm{a}+w)...)\). Create a new set \(Q(m+1,n^{\prime},d)\) where \(n^{\prime}\) is the current size of \(Q(m+1)\). Add \((A\rightarrow\alpha w\cdot\beta,Q(i,j,k),\bm{a}+w,p((\bm{a}+w)...))\) to \(Q(m+1,n^{\prime},d)\).
* **Completion**: for every state in \(Q(m,n,d)\) of the form \((A\rightarrow\Gamma\cdot Q,(i,j,k),\bm{a},p(\bm{a}...))\), find states in \(Q(i,j,k)\) of the form \((B\rightarrow\alpha\cdot A\beta,Q(i^{\prime},j^{\prime},k^{\prime}),\bm{a}^{ \prime},p(\bm{a}^{\prime}...))\) and add \((B\rightarrow\alpha A\cdot\beta,Q(i^{\prime},j^{\prime},k^{\prime}),\bm{a},p( \bm{a}...))\) to \(Q(m,n,d-1)\).

The symbols \(\alpha\), \(\beta\), and \(\Gamma\) represent arbitrary strings consisting of terminals and variables, _i.e._\(\alpha,\beta,\Gamma\in(\Sigma\cup V)^{*}\). The symbols \(A\) and \(B\) refer to the variables, while \(w\) denotes a single terminal. The symbol \(Q\) represents the set of states, and the dot (\(\cdot\)) denotes the current position of the parser within the production rule.

Additionally, we introduce a pruning technique of limiting the queue size to reduce the vast search space in the parse tree, similar to the beam search. Specifically, the parser preserves only the top \(N^{\rm queue}\) elements from the queue in order of the parsing probability of each state. The parsing process terminates when the parser identifies that the parsed action sequence \(\bm{a}^{*}\) has a higher parsing probability than the prefix probabilities of any other states in the queue. For the further details, refer to Appendix B.

### Segmentation optimization

The objective of segmentation optimization is to determine the optimal alignment between the input classification probability matrix \(\bm{Y}\) and the action sequence \(\bm{a}^{*}\). In other words, the entire frames are allocated within the action sequences \(\bm{a}^{*}=\big{[}a_{1}^{*},a_{2}^{*},...,a_{N}^{*}\big{]}\), obtained from the parser, to determine the optimal action lengths \(\bm{l}^{*}=[l_{1}^{*},l_{2}^{*},...,l_{N}^{*}]\). In this work, we utilize dynamic programming-based Viterbi-like algorithm [35] for activity parsing. Similar to [26; 32], the optimizer explores all possible allocations and selects the one with the maximum product of probabilities:

\[\bm{l}^{*}=\arg\max_{\bm{l}}(p(\bm{l}\,|\,\bm{a}^{*},\bm{Y}_{1:T})),\] (11)

\[p(\bm{l}\,|\,\bm{a},\bm{Y}_{1:t})=\max_{i<t}(p(\bm{l}_{1:N-1}\,|\,\bm{a}_{1:N- 1},\bm{Y}_{1:i})\prod_{j=i}^{t}\bm{Y}_{j,a_{N}}).\] (12)

## 4 Experimental evaluation and analysis

### Datasets and evaluation metrics

**Datasets.** We conduct experiments on two widely used benchmark datasets for temporal action segmentation: Breakfast [23] and 50 Salads [39]. The Breakfast dataset, consisting of 1,712 videos, involves 52 individuals preparing 10 different breakfast activities comprised of 48 actions in 18 different kitchens. Similarly, the 50 Salads dataset comprises 50 egocentric videos of people preparing salads of a single activity with 17 fine-grained actions from 25 people. We used I3D [4] features provided by [9].

**Evaluation metrics.** For evaluation metrics, we report edit score, F1@\(\{10,25,50\}\) scores, and frame-wise accuracy following the previous work [9; 45].

### Implementation details

For KARI, we set the hyperparameters of the number of key actions \(N^{\mathrm{key}}\) to 4 for Breakfast, and 3 for 50 Salads. We individually induce separate activity grammar for the ten activity classes within Breakfast and subsequently merge them into a unified grammar. For the comparison with the existing grammar used in the previous work [32; 31], we induce activity grammars of ADIOS [37] provided by [31]. Two types of ADIOS-induced grammar are induced: ADIOS-AND-induced grammar, primarily composed of AND rules with limited generalization capabilities, and ADIOS-OR-induced grammar, predominantly incorporating OR rules, offering improved generalization. Please refer to Appendix C.1 for grammar induction details.

For BEP, we configured the queue size \(N^{\mathrm{queue}}\) to be 20. For efficiency, we adjust the sampling rate of the input video features to 50 for Breakfast and 100 for 50 Salads. We use two widely used models for the temporal action segmentation: ASFormer [45] based on Transformer and MS-TCN [9] based on CNNs. Since we apply the proposed method to the reproduced temporal action segmentation models, we directly compare and evaluate the performance based on the reproduced results.

### Evaluation framework for activity grammar

We propose a novel evaluation framework to assess the generalization and discrimination capabilities of the activity grammar. Figure 3 shows the overall process of the grammar evaluation framework. We first generate a set of synthetic activity grammars \(\mathcal{G}^{\mathrm{S}}\) randomly. Action sequences \(\bm{a}\in\mathcal{D}_{i}^{\mathrm{all}}\) are generated from each synthetic grammar \(G^{\mathrm{S}}_{i}\in\mathcal{G}^{\mathrm{S}}\), and these sequences are randomly divided into two sets: seen _seen_ and _unseen_. For each seen set, a grammar induction algorithm is applied, resulting in the induced grammar \(G^{\mathrm{I}}_{i}\) consisting in a corresponding set of induced grammars \(\mathcal{G}^{\mathrm{I}}\). For grammar evaluation, the induced grammar \(G^{\mathrm{I}}_{i}\in\mathcal{G}^{\mathrm{I}}\) parses action sequences from the entire unseen sets. The induced grammar should accurately parse the action sequences generated by the original synthetic grammar from which it was induced, while also effectively discriminating those generated by other synthetic grammars.

To simulate real-world video action sequences, we generate the synthetic activity grammars assuming temporal dependencies across actions. This indicates that certain actions follow a temporal order

\begin{table}
\begin{tabular}{l c c} \hline \hline grammar & precision & recall \\ \hline ADIOS-AND & 0.97 & 0.08 \\ ADIOS-OR & **0.99** & 0.25 \\ KARI (ours) & 0.93 & **0.98** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Synthetic \(G\) I**

Figure 4: **Confusion matrix of activity grammars. The results of KARI-induced grammar are similar to the synthetic grammar, showing high recall with comparable precision.**

Figure 3: **Grammar evaluation**

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

and 'add saltnpepper', (blue bar in Fig. 4(a)), which are omitted in the results obtained by using the ADIOS-OR induced grammar. The results show that KARI-induced grammar allows a more flexible temporal structure between actions. Furthermore, our method effectively removes actions such as 'put pancake2plate' that do not correspond to the intended activity. Similarly, qualitative results on 50 Salads in Fig. 4(b) show the effectiveness of the proposed method with complex action sequences. The overall results show that activity grammar-based refinement for the temporal action segmentation model is effective for correcting the neural predictions by using the grammar as a guide.

## 5 Conclusion

We have shown that the proposed approach enhances the sequence prediction and discovers its compositional structure, significantly improving temporal action segmentation in terms of both performance and interpretability. However, the improvement is limited by the initial output of the action segmentation network, which remains further research in the future. We believe that the grammar induction and parsing methods can be easily applied to other sequence prediction tasks.

Figure 5: **Qualitative results.** KARI-induced grammar efficiently insert missing actions and removes out-of-context actions in ASFormer [45].

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Grammar induction} & scrambled & pancake & salad & fried egg & juice & coffee & sandwich & cereal & milk & tea & total \\  & egg (11.9) & (11.1) & (9.9) & (9.5) & (7.2) & (6.7) & (6.0) & (5.1) & (5.0) & (5.0) & (7.7) \\ \hline Kuehne _et al._[23] & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.53 \\ Richard _et al._[35] & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.54 \\ ADIOS-AND [37] & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.54 \\ ADIOS-OR [37] & 0.39 & 0.30 & 0.37 & 0.53 & 0.55 & 0.80 & 0.73 & 0.96 & 0.78 & 0.92 & 0.63 \\ KARI & **0.84** & **0.71** & **0.90** & **0.70** & **0.77** & **1.00** & **0.91** & **0.96** & **0.90** & **0.98** & **0.87** \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Grammar evaluation on real data.** We evaluate the proposed KARI-induced-grammar on Breakfast, demonstrating the superior high recall on unseen action sequences from each activity. The average length of action sequences of each activity is shown in parentheses.

Acknowledgements

This work was supported by the IITP grants (2022-0-00264: Comprehensive video understanding and generation with knowledge-based deep logic (\(50\%\)), 2022-0-00290: Visual intelligence for space-time understanding and generation based on multi-layered visual common sense (\(20\%\)), 2022-0-00959: Few-shot learning of causal inference in vision and language (\(20\%\)), and 2019-0-01906: AI graduate school program at POSTECH (\(10\%\))) funded by the Korea government (MSIT).

## References

* [1] H. Ahn and D. Lee. Refining action segmentation with hierarchical video representations. In _Proc. IEEE International Conference on Computer Vision (ICCV)_, pages 16302-16310, 2021.
* [2] N. Behrmann, S. A. Golestaneh, Z. Kolter, J. Gall, and M. Noroozi. Unified fully and timestamp supervised temporal action segmentation via sequence to sequence translation. In _Proc. European Conference on Computer Vision (ECCV)_, pages 52-68. Springer, 2022.
* [3] P. Belcak, D. Hofer, and R. Wattenhofer. A neural model for regular grammar induction. _arXiv preprint arXiv:2209.11628_, 2022.
* [4] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6299-6308, 2017.
* [5] M.-H. Chen, B. Li, Y. Bao, and G. AlRegib. Action segmentation with mixed temporal domain adaptation. In _Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 605-614, 2020.
* [6] M.-H. Chen, B. Li, Y. Bao, G. AlRegib, and Z. Kira. Action segmentation with joint self-supervised temporal domain adaptation. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9454-9463, 2020.
* [7] J. Earley. An efficient context-free parsing algorithm. _Communications of the ACM_, 13(2):94-102, 1970.
* [8] H.-S. Fang, Y. Xu, W. Wang, X. Liu, and S.-C. Zhu. Learning pose grammar to encode human body configuration for 3d pose estimation. In _Proc. AAAI Conference on Artificial Intelligence (AAAI)_, volume 32, 2018.
* [9] Y. A. Farha and J. Gall. Ms-tcn: Multi-stage temporal convolutional network for action segmentation. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3575-3584, 2019.
* [10] S.-H. Gao, Q. Han, Z.-Y. Li, P. Peng, L. Wang, and M.-M. Cheng. Global2local: Efficient structure search for video action segmentation. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16805-16814, 2021.
* [11] M. Guo, V. Thost, B. Li, P. Das, J. Chen, and W. Matusik. Data-efficient graph grammar learning for molecular generation. In _Proc. International Conference on Learning Representations (ICLR)_, 2021.
* [12] Y. Hong, Q. Li, R. Gong, D. Ciao, S. Huang, and S.-C. Zhu. Smart: A situation model for algebra story problems via attributed grammar. In _Proc. AAAI Conference on Artificial Intelligence (AAAI)_, pages 13009-13017, 2021.
* [13] Y. Hong, Q. Li, S.-C. Zhu, and S. Huang. Vlgrammar: Grounded grammar induction of vision and language. In _Proc. IEEE International Conference on Computer Vision (ICCV)_, pages 1665-1674, 2021.
* [14] J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to automata theory, languages, and computation. _Acm Sigact News_, 32(1):60-65, 2001.
** [15] Y. Huang, Y. Sugano, and Y. Sato. Improving action segmentation via graph-based temporal reasoning. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 14024-14034, 2020.
* [16] Y. Ishikawa, S. Kasai, Y. Aoki, and H. Kataoka. Alleviating over-segmentation errors by detecting action boundaries. In _Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 2322-2331, 2021.
* [17] F. Jelinek, J. D. Lafferty, and R. L. Mercer. _Basic methods of probabilistic context free grammars_. Springer, 1992.
* [18] D. Jurafsky. _Speech & language processing_. Pearson Education India, 2000.
* [19] S. Karaman, L. Seidenari, and A. Del Bimbo. Fast saliency based pooling of fisher encoded dense trajectories. In _ECCV THUMOS Workshop_, volume 1, page 5, 2014.
* [20] Y. Kim. Sequence-to-sequence learning with latent neural grammars. _Advances in Neural Information Processing Systems_, 34:26302-26317, 2021.
* [21] Y. Kim, C. Dyer, and A. M. Rush. Compound probabilistic context-free grammars for grammar induction. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2369-2385, 2019.
* [22] Y. Kim, A. M. Rush, L. Yu, A. Kuncoro, C. Dyer, and G. Melis. Unsupervised recurrent neural network grammars. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1105-1117, 2019.
* [23] H. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 780-787, 2014.
* [24] H. Kuehne, A. Richard, and J. Gall. Weakly supervised learning of actions from transcripts. _Computer Vision and Image Understanding_, 163:78-89, 2017.
* [25] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networks for action segmentation and detection. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 156-165, 2017.
* [26] J. Li, P. Lei, and S. Todorovic. Weakly supervised energy-based learning for action segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, October 2019.
* [27] Q. Li, S. Huang, Y. Hong, Y. Chen, Y. N. Wu, and S.-C. Zhu. Closed loop neural-symbolic learning via integrating neural perception, grammar parsing, and symbolic reasoning. In _Proc. International Conference on Machine Learning (ICML)_, pages 5884-5894. PMLR, 2020.
* [28] A. Piergiovanni, A. Angelova, and M. S. Ryoo. Differentiable grammars for videos. In _Proc. AAAI Conference on Artificial Intelligence (AAAI)_, pages 11874-11881, 2020.
* [29] A. Piergiovanni, A. Angelova, A. Toshev, and M. S. Ryoo. Adversarial generative grammars for human activity prediction. In _Proc. European Conference on Computer Vision (ECCV)_, pages 507-523. Springer, 2020.
* [30] S. Qi, S. Huang, P. Wei, and S.-C. Zhu. Predicting human activities using stochastic grammar. In _Proc. IEEE International Conference on Computer Vision (ICCV)_, pages 1164-1172, 2017.
* [31] S. Qi, B. Jia, S. Huang, P. Wei, and S.-C. Zhu. A generalized earley parser for human activity parsing and prediction. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(8):2538-2554, 2020.
* [32] S. Qi, B. Jia, and S.-C. Zhu. Generalized earley parser: Bridging symbolic grammars and sequence data for future prediction. In _Proc. International Conference on Machine Learning (ICML)_, pages 4171-4179. PMLR, 2018.

* [33] A. Richard, H. Kuehne, and J. Gall. Weakly supervised action learning with rnn based fine-to-coarse modeling. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 754-763, 2017.
* [34] A. Richard, H. Kuehne, and J. Gall. Action sets: Weakly supervised action segmentation without ordering constraints. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5987-5996, 2018.
* [35] A. Richard, H. Kuehne, A. Iqbal, and J. Gall. Neuralnetwork-viterbi: A framework for weakly supervised video learning. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7386-7395, 2018.
* [36] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for fine grained activity detection of cooking activities. In _Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1194-1201. IEEE, 2012.
* [37] Z. Solan, D. Horn, E. Ruppin, and S. Edelman. Unsupervised learning of natural languages. _Proceedings of the National Academy of Sciences_, 102(33):11629-11634, 2005.
* [38] Y. Souri, Y. A. Farha, F. Despinoy, G. Francesca, and J. Gall. Fifa: Fast inference approximation for action segmentation. In _Pattern Recognition: 43rd DAGM German Conference, DAGM GCPR 2021, Bonn, Germany, September 28-October 1, 2021, Proceedings_, pages 282-296. Springer, 2022.
* [39] S. Stein and S. J. McKenna. Combining embedded accelerometers with computer vision for recognizing food preparation activities. In _Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing_, pages 729-738, 2013.
* [40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Proc. Neural Information Processing Systems (NeurIPS)_, 30, 2017.
* [41] N. N. Vo and A. F. Bobick. From stochastic grammar to bayes network: Probabilistic parsing of complex activity. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2641-2648, 2014.
* [42] B. Wan, W. Han, Z. Zheng, and T. Tuytelaars. Unsupervised vision-language grammar induction with shared structure modeling. In _Proc. International Conference on Learning Representations (ICLR)_, 2021.
* [43] Y. Xu, W. Wang, T. Liu, X. Liu, J. Xie, and S.-C. Zhu. Monocular 3d pose estimation via pose grammar and data augmentation. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 2021.
* [44] Z. Xu, Y. S. Rawat, Y. Wong, M. Kankanhalli, and M. Shah. Don't pour cereal into coffee: Differentiable temporal logic for temporal action segmentation. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* [45] F. Yi, H. Wen, and T. Jiang. Asformer: Transformer for action segmentation. In _Proc. British Machine Vision Conference (BMVC)_, 2021.

## Appendices

In this supplement, we provide detailed descriptions of the proposed method and additional results, which are omitted in the main paper due to the lack of space. In Section A, we will describe the formulation of the probability of activity grammar. Algorithmic details of BEP is included in Section B. Section C compares KARI with the existing grammar induction algorithms for activity grammar and Section D presents additional qualitative results. We conclude this Appendix by discussing the broader impact of our research in Section E.

## Appendix A Formulation of the probabilities in KARI

In this section, we describe the formulation of transition probability \(p_{i,j}^{\Omega}\) and the escape probability \(p_{i,\epsilon}^{\Omega}\) and \(p_{e}^{\mathrm{M}}\) in Eq. 5 and 6 in Section A.1. In the following, the derivation of the expectation of the escape probability is described in Section A.2.

### Formulation of the escape and transition probability

We first introduce and escape probabilities and the transition probabilities introduced in Eq. 5.

**Pre-processing.** Let \(\bm{h}_{i}^{\Omega}\) represent a list of action sub-sequences, where the sub-sequence from \(\bm{h}_{i}^{\Omega}\) removes actions that does not exist in the action group \(\bm{d}_{i}^{\Omega}\) from the action sub-sequence in \(\mathcal{D}^{\Omega}\). The empty string \(\epsilon\) remains when the action sub-sequence does not include actions within the action group \(\bm{d}_{i}^{\Omega}\). For example in Fig. 2, a list of sub-sequences \(\bm{h}_{1}^{\mathrm{R}}\) can be structured as \(\bm{h}_{1}^{\mathrm{R}}=[[\mathrm{pour\,milk}],[\mathrm{spoon\,sugar},\, \mathrm{pour\,milk}],[\mathrm{pour\,milk},\,\mathrm{spoon\,sugar}],[\mathrm{ spoon\,sugar}]]\) with the corresponding action group \(\bm{d}_{1}^{\mathrm{R}}=\{\mathrm{pour\,milk},\,\mathrm{spoon\,sugar}\}\). Similary, a list of sub-sequences \(\bm{h}_{2}^{\mathrm{R}}\) is structured as \(\bm{h}_{2}^{\mathrm{R}}=[\epsilon,\epsilon,[\mathrm{stir\,coffee}],[\mathrm{ stir\,coffee}]]\) with the action group \(\bm{d}_{2}^{\mathrm{R}}=\{\mathrm{stir\,coffee}\}\). This pre-processing step of generating \(\bm{h}_{i}^{\Omega}\) enables us to consider the statistical probabilities associated with actions.

**Formulation of the escape probability.** The escape probability \(p_{i,\epsilon}^{\Omega}\) and the transition probability \(p_{i,j}^{\Omega}\) are both defined based on the number of recursion \(n^{\mathrm{rec}}\) of the current timestep; thereby these probabilities are represented as functions of \(n^{\mathrm{rec}}\). We first define the escape probability function:

\[p_{i,\epsilon}^{\Omega}(n^{\mathrm{rec}})=\begin{cases}\frac{\left|\left[\bm{a }\in\bm{h}_{i}^{\Omega}\,|\,\bm{a}=\epsilon\right]\right|}{\left|\bm{h}_{i}^{ \Omega}\right|}&\mathrm{if}\,n^{\mathrm{rec}}=1\,,\\ \frac{1}{\bar{N}^{\bm{h}_{i}^{\Omega}}}&\mathrm{otherwise}\,,\end{cases}\] (13)

where \(\bar{N}^{\bm{h}_{i}^{\Omega}}\) is the average length of the sub-sequences in \(\bm{h}_{i}^{\Omega}\). In the first recursion, _i.e._, \(n^{\mathrm{rec}}=1\), the probability calculation solely considers statistics of the actions. Otherwise, the probability is calculated based on the expected number of recursions, which will be introduced in Appendix A.2. In Fig. 2, \(p_{1,\epsilon}^{\mathrm{R}}(1)=0\), since none of the action sub-sequence \(\bm{a}\) from \(\bm{h}_{1}^{\mathrm{R}}\) is equal to the empty sequence, and \(p_{1,\epsilon}^{\mathrm{R}}(n^{\mathrm{rec}}>1)=\frac{2}{3}\), since the average length of sub-strings in \(\bm{h}_{1}^{\mathrm{R}}\) is \(1.5\).

**Formulation of the transition probability.** The action sequence \(\bm{a}=[a_{1},a_{2},...,a_{N}]\) represents the distinct action labels for the video segments, where \(a_{i}\neq a_{i+1}\) as described in Section 3. In order to prevent the repetition of the same action in Eq. 5, we introduce an additional input \(q\) when defining the transition probability. Here, \(q\) refers to the index of the actions selected by the rule in the previous step, specifically at \((n^{\mathrm{rec}}-1)_{\mathrm{th}}\) step where \(n^{\mathrm{rec}}>1\). We simply put \(q\) to \(0\) in the first recursion, _i.e._\(n^{\mathrm{rec}}=1\), which does not affect the results. The transition probability \(p_{i,j}^{\Omega}\) is defined by:

\[p_{i,j}^{\Omega}(n^{\mathrm{rec}},q)=\begin{cases}\frac{\left|\left[\bm{a}\in \bm{h}_{i}^{\Omega}\,|\,a_{1}=d_{i,j}^{\Omega}\right]\right|}{\left|\bm{h}_{i} ^{\Omega}\right|}&\mathrm{if}\,n^{\mathrm{rec}}=1\,,\\ 0&\mathrm{if}\,n^{\mathrm{rec}}>1\,\text{and}\,j=q,\\ \frac{p_{i,j}^{\Omega}(1,0)\left(1-p_{i,\epsilon}^{\Omega}(n^{\mathrm{rec}}) \right)}{\sum_{l\neq q}p_{i,l}^{\Omega}(1,0)}&\mathrm{otherwise}.\end{cases}\] (14)

The escape probability \(p_{e}^{\mathrm{M}}\) and the transition probability \(p_{i,j}^{\mathrm{M}}\) for the middle variable \(V^{\mathrm{M}}\) in Eq. 6 is defined in the same way as Eq. 13 and Eq. 14, respectively.

### Derivation of the escape probability

We introduce the formulation of the escape probability \(p_{i,e}^{\Omega}\) in Appendix A.1. The escape probability is required to avoid an infinite loop of the rules and guarantee the length of sequences from the recursive rules in Eq. 5. Since the number of recursions directly determines the sequence lengths, we determine the escape probability regarding the length of action sequences. For notational simplicity, we denote the escape probabilities \(p_{i,e}^{\Omega}\) by \(p\), omitting superscripts and subscripts. The expectation of the number of recursions is calculated by:

\[\lim_{n\to\infty}\sum_{k=1}^{n}kp(1-p)^{k} =\lim_{n\to\infty}\frac{(1-p)(1+(1-p)^{n}-np(1-p)^{n})}{p},\] (15) \[=\frac{1-p}{p}.\] (16)

Since we derive the escape probability when \(n^{\mathrm{rec}}>1\), the expected number of recursions is equal to \(\bar{N}-1\):

\[\frac{1-p}{p}=\bar{N}-1,\] (17)

, where \(\bar{N}\) is the average length of action sequences. Finally, we obtain the escape probability by

\[p=\frac{1}{\bar{N}},\] (18)

where this equation is used in Eq. 13 when \(n^{\mathrm{rec}}>1\). The derivation of the escape probability \(p_{e}^{\mathrm{M}}\) of the middle variable \(V^{\mathrm{M}}\) in Eq. 6 is also formulated as the same.

## Appendix B Breadth-first Earley Parser (BEP)

### Earley parser

The Earley parser [7] is a classic algorithm that efficiently parses strings for context-free grammar. It operates by maintaining a set of states of the parsing process. Each state consists of a production rule, a position within that rule, and a position in the input string. The parser builds a parse tree for the input string, which records the structure of parsing. The Earley parser is commonly used for natural language processing tasks, such as syntactic analysis and semantic parsing.

The Earley parser consists of three main operations: scanning, prediction, and completion.

* **Scanning:** The parser matches a terminal symbol in the input string with the current position in the production rule. This operation moves the parser forward in the input string.
* **Prediction:** The parser expands a variable in the production rule based on the current position. It adds new states to the set of states for possible future matches.
* **Completion:** When the parser reaches the end of the production rule, it searches for other states predicting the head variable of the current rule. Subsequently, the parser update the positions within the rule of the searched states.

By iterating these three operations, the Earley parser builds a parse chart that represents all possible parse trees for the input string.

### Implementation details

The parsing probability \(p(\bm{F}_{1:t}\to\bm{a}\,|\,G)\) (Eq. 9) can suffer from numerical underflow due to its exponential decrease as \(t\) increases. To overcome the issue, we compute the probabilities in logarithmic space, following [31]. For simplicity, we denote \(\log(p(\bm{F}_{1:t-1}\to\bm{a}\,|\,G))\) as \(P_{N}\) and \(\log(p(\bm{F}_{1:t-1}\to\bm{a}_{1:N-1}\,|\,G))\) as \(P_{N-1}\) below:

\[P_{N-1}^{\prime} =\log(g(x|\bm{a}_{1:N-1},G))+P_{N-1},\] (19) \[z =\max(P_{N},P_{N-1}^{\prime}),\] (20) \[\log(p(\bm{F}_{1:t}\to\bm{a}\,|\,G)) =\,\log(\bm{Y}_{t,x})+z+\log(\exp(P_{N}-z)+\exp(P_{N-1}^{\prime}- z)).\] (21)For the computational efficiency, we set the sampling stride of input matrix \(\bm{Y}\) as 50 for Breakfast and 100 for 50Salads. Additionally, we set the maximum length of the refined action sequence as 20 for Breakfast and 25 for 50Salads.

### Parsing algorithm

Algorithm 1 shows the parsing procedure of BEP. We utilize a priority queue that sorts the elements in ascending order. The \(currentSet\) stores multiple states with the same \(m\), \(n\), and \(d\). See B.4 for the examples. BEP stops parsing when the probability of \(\bm{a}^{*}\) has the highest probability compared to states in the queue while ensuring the current state can reach depth 1 with only completions.

### Parsing example

In this section, we provide an example to help understand how BEP works. For simplicity, we assume that frame-wise class probabilities from the segmentation model are identical across all action classes. First of all, we define toy grammar as shown in Figure 6.

In the context of grammar, \(S\) indicates the starting variable. \(A\), \(B\), \(C\), and \(A_{i}\) for \(i=[1,2,3,4]\) represent the variables, while \(x_{j}\) for \(j=[1,2,3,4,5,6,7]\) represent terminals.

Table 9 is the history of parsing with the toy grammar. It shows the currently popped state, the visiting order, the parsed prefix, the previous state, and which states are currently in the queue. The three consecutive numbers in the column pop, from, and queue indicate \(m\), \(n\), and \(d\) of the state. The column \(p\) represents the prefix probability excluding the frame-wise probability, which can be considered a parsing probability since all frame-wise probabilities are assumed to be the same. Note that the table includes some history after the parsed sequence satisfied the early stop constraint to illustrate how BEP prioritizes the states. Returning to the subject, the table shows BEP preferentially searches for states with a small depth. In order 14, even though the probability of state \(Q(1,1,3)\) is higher, BEP visits the state \(Q(3,0,2)\) with a lower depth.

Figure 6: Toy grammar used for the example of the BEP parsing

``` Input :probability matrix \(\bm{Y}\), grammar \(G\), queue size \(N^{\rm queue}\) Output : Best parsed sequence \(\bm{a}^{*}\)
1functionBreadth-first Earley Parser
2\(q\gets priorityQueue()\) ; // init priority queue
3\(Q(0,0,0)\leftarrow(\Gamma\to R,Q(0,0,0),\epsilon,1.0)\) ; // set initial state
4\(q.push(0,(1.0,0,0,\epsilon,Q(0,0,0)))\) ; // push initial state to queue
5\(\bm{a}^{*}\leftarrow\epsilon\) ; // init \(\bm{a}^{*}\)
6while\((d,(p(\bm{a}_{1:|\bm{a}|-1}),m,\bm{a}_{1:|\bm{a}|-1},currentSet))\gets q.pop()\)do
7for\((r,(Q(i,j,k),\bm{a},p(\bm{a}_{...}))\in currentSet\)do
8// update \(\bm{a}^{*}\) when \(\bm{a}\) has higher probability
9if\(p(\bm{a})>p(\bm{a}^{*})\)then
10\(\bm{a}^{*}\leftarrow\bm{a}\)
11 endif
12// prediction
13if\(r\) is \((A\rightarrow\alpha\cdot B\beta)\)then
14foreach\((B\rightarrow\Gamma)\)in\(G\)do
15\(r^{\prime}\leftarrow(B\rightarrow\cdot\Gamma)\)
16\(Q^{\prime}\leftarrow(r^{\prime},Q(m,n,d),\bm{a},p(\bm{a}...))\)
17\(Q(m,n,d+1).add(Q^{\prime})\)\(q.push(d+1,(p(\bm{a}...),m,n,\bm{a},Q(m,n,d+1)))\)
18 endfor
19
20 endif
21// scanning
22if\(r\) is \((A\rightarrow\alpha\cdot x\beta)\)then
23\(r^{\prime}\leftarrow(A\rightarrow\alpha x\cdot\beta)\)
24\(n^{\prime}\leftarrow|Q(m+1)|\)
25\(Q^{\prime}\leftarrow(r^{\prime},Q(i,j,k),\bm{a}+x,p((\bm{a}+x)...))\)
26\(Q(m+1,n^{\prime},d).add(Q^{\prime})\)
27\(q.push(d,(p(\bm{a}+x)...,m+1,n^{\prime},d,Q(i,j,k)))\)
28 endif
29// completion
30if\(r\) is \((B\rightarrow\Gamma\cdot)\)then
31for each\(((A\rightarrow\alpha\cdot B\beta),Q(i^{\prime},j^{\prime},k^{\prime}),\bm{a},p( \bm{a}...))\)in\(Q(i,j,k)\)do
32\(r^{\prime}\leftarrow(A\rightarrow\alpha B\cdot\beta)\)
33\(Q^{\prime}\leftarrow(r^{\prime},Q(i^{\prime},j^{\prime},k^{\prime}),\bm{a},p( \bm{a}...))\)
34\(Q(m,n,d-1).add(Q^{\prime})\)
35\(q.push(d-1,(p(\bm{a}...),m,n,\bm{a},Q(m,n,d-1)))\)
36 endfor
37 endif
38 endfor
39// early stop when \(\bm{a}^{*}\) has the highest probability and finished parsing
40if\(p(\bm{a}^{*})>p(\bm{a}^{\prime})\)for all \(\bm{a}^{\prime}\)in \(q\)then
30if\(\bm{a}^{*}\) has parsed then
31return\(\bm{a}^{*}\)
32 endif
33 endif
34// queue pruning
35if\(|q|>N^{\rm queue}\)then // sort \(q\) in probability descending order
36\(q^{\prime}\leftarrow\text{sorted}(q,\text{key}=p(\bm{a}...),\text{reverse}=True)\)
37\(q.clear()\)
38for\(i\gets 1\)to\(N^{\rm queue}\)do
39\(q.push(q^{\prime}.pop())\)
40 endfor
410 endif
42
43 endwhile
44return\(\bm{a}^{*}\)
45endfunction ```

**Algorithm 1**Breadth-first Earley Parser (BEP)

## Appendix C Comparison with the existing grammar induction algorithms

### Existing grammar induction algorithms

Kuehne _et al._[23] introduce a hierarchical context-free grammar induction algorithm. The root rule with the starting variable \(S\) is induced as \(S\to V_{1}\,|\,V_{2}\,|\,...\,|\,V_{N^{\lambda}}\), where the variable \(V_{i}\) represents a single activity and \(N^{\Lambda}\) is the number of activities from the dataset. Then each \(V_{i}\) expands into action sequences from each activity, _i.e._, the rule is formed as: \(V_{i}\rightarrow\mathcal{A}_{i,1}\,|\,\mathcal{A}_{i,2}\,|\,...\,|\,\mathcal{ A}_{i,|\mathcal{A}_{i}|}\), where \(\mathcal{A}_{i}\) is a set of action sequences from the \(i\)-th activity and each \(\mathcal{A}_{i,j}\) represents a \(j\)-th action sequence in \(\mathcal{A}_{i}\). Richard _et al._[35] propose a grammar induction method for a probabilistic right-regular grammar, where every rule has the form of \(\tilde{H}\to c\,H\). The algorithm is motivated by n-gram models [18] and finite grammars [14]. Specifically, the variable \(\tilde{H}\) represents an action sequence \(\bm{a}_{1:n}\), \(H\) represents an action sequence \(\bm{a}_{1:n-1}\), and a terminal \(c\) is an action class of \(a_{n}\). The induced grammar can express the intermediate action sequences \(\bm{a}_{1:n}\) and expands its rules based on the sequential order of actions.

Recently, Qi _et al._[32] adopt the Automatic Distillation of Structure (ADIOS) [37] algorithm to induce a probabilistic context-free grammar. The ADIOS algorithm finds the significant patterns (AND rules) and equivalence action classes (OR rules) from the given action sequences. The algorithm identifies repetitive patterns in action sequences to minimize redundant sequences and find potential candidates for generalized action classes. Following the grammar induction methods of ADIOS, we

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline pop & order & \(m\) & \(n\) & \(d\) & rule & prefix & operation & from & \(p\) & queue \\ \hline - & 1 & 0 & 0 & 0 & \(\Gamma\rightarrow\cdot S\) & - & ROOT & - & 1 & 000 \\ \hline
000 & 2 & 0 & 0 & 1 & \(S\rightarrow\cdot ABC\) & - & PRED & 000 & 1 & 001 \\ \hline
001 & 3 & 0 & 0 & 2 & \(A\rightarrow\cdot A_{1}A_{2}\) & - & PRED & 001 & 1 & 002 \\ \hline
002 & 4 & 0 & 0 & 3 & \(A_{1}\rightarrow\cdot x_{1}\) & - & PRED & 002 & 0.7 & 003 \\  & 0 & 0 & 0 & 3 & \(A_{1}\rightarrow\cdot x_{2}\) & - & PRED & 002 & 0.3 & \\ \hline
003 & 5 & 1 & 0 & 3 & \(A_{1}\to x_{1}\cdot\) & \(x_{1}\) & SCAN & 003 & 0.7 & 103, 113 \\  & 19 & 1 & 1 & 3 & \(A_{1}\to x_{2}\cdot\) & \(x_{2}\) & SCAN & 003 & 0.3 & \\ \hline
103 & 6 & 1 & 0 & 2 & \(A\to A_{1}\cdot A_{2}\) & \(x_{1}\) & COMP & 103 & 0.7 & 113, 102 \\ \hline
102 & 7 & 1 & 0 & 3 & \(A_{2}\rightarrow\cdot A_{3}A_{4}\) & \(x_{1}\) & PRED & 102 & 0.35 & 103, 113 \\  & & 1 & 0 & 3 & \(A_{2}\rightarrow\cdot e\) & \(x_{1}\) & PRED & 102 & 0.35 & \\ \hline  & & 1 & 0 & 4 & \(A_{3}\rightarrow\cdot a4A_{4}\) & \(x_{1}\) & PRED & 103 & 0.175 & 203, 113, 104 \\
103 & - & 1 & 0 & 4 & \(A_{3}\rightarrow\cdot e\) & \(x_{1}\) & PRED & 103 & 0.175 & \\  & 8 & 2 & 0 & 3 & \(A_{2}\to e\cdot\) & \(x_{1}\) & SCAN & 103 & 0.35 & \\ \hline
203 & 9 & 2 & 0 & 2 & \(A\to A_{1}A_{2}\cdot\) & \(x_{1}\) & COMP & 203 & 0.35 & 202, 113, 104 \\ \hline
202 & 10 & 2 & 0 & 1 & \(S\to A\cdot BC\) & \(x_{1}\) & COMP & 202 & 0.35 & 201, 113, 104 \\ \hline
201 & 11 & 2 & 0 & 2 & \(B\rightarrow\cdot x_{5}\) & \(x_{1}\) & PRED & 201 & 0.35 & 202, 113, 104 \\ \hline
202 & 12 & 3 & 0 & 2 & \(B\to x_{5}\cdot\) & \(x_{1}\,x_{5}\) & SCAN & 202 & 0.35 & 302, 113, 104 \\ \hline
302 & 13 & 3 & 0 & 1 & \(S\to AB\cdot C\) & \(x_{1}\,x_{5}\) & COMP & 302 & 0.35 & 301, 113, 104 \\ \hline
301 & 14 & 3 & 0 & 2 & \(C\rightarrow\cdot x_{6}\) & \(x_{1}\,x_{5}\) & PRED & 301 & 0.245 & 302, 113, 104 \\  & 3 & 0 & 2 & \(C\rightarrow\cdot x_{7}\) & \(x_{1}\,x_{5}\) & PRED & 301 & 0.105 & \\ \hline
302 & 15 & 4 & 0 & 2 & \(C\rightarrow\cdot x_{6}\cdot\) & \(x_{1}\,x_{5}\,x_{6}\) & SCAN & 302 & 0.245 & 402, 412, 113, 104 \\  & 17 & 4 & 1 & 2 & \(C\rightarrow\tau\cdot\) & \(x_{1}\,x_{5}\,x_{7}\) & SCAN & 302 & 0.105 & \\ \hline
402 & 16 & 4 & 0 & 1 & \(S\to ABC\cdot\) & \(x_{1}\,x_{5}\,x_{6}\) & COMP & 402 & 0.245 & 401, 412, 113, 104 \\ \hline
401 & - & - & - & - & \(\Gamma\to S\cdot\) & \(x_{1}\,x_{5}\,x_{6}\) & COMP & 401 & 0.245 & 412, 113, 104 \\ \hline
412 & 18 & 4 & 1 & 1 & \(S\to ABC\cdot\) & \(x_{1}\,x_{5}\,x_{7}\) & COMP & 412 & 0.105 & 411, 113, 104 \\ \hline
411 & - & - & - & - & \(\Gamma\to S\cdot\) & \(x_{1}\,x_{5}\,x_{7}\) & COMP & 411 & 0.105 & 113, 104 \\ \hline
113 & - & 1 & 1 & 2 & \(A\to A_{1}\cdot A_{2}\) & \(x_{2}\) & COMP & 113 & 0.3 & 112, 104 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Parsing log for the given toy grammar through BEP.

set a decreasing ratio of the motif extraction algorithm \(\eta\) to 1, a significance level for the decrease ratio \(\gamma\) to 0.1, and the context window size 1 for ADIOS-AND-induced grammar. For ADIOS-OR-induced grammar, we set \(\eta\) to 0.9, \(\gamma\) to 0.1, and the context window size to 4.

However, none of these approaches have managed to effectively integrate recursive rules, which are crucial for representing intricate and lifelike structures of action phrases and activities. The proposed KARI algorithm introduces a probabilistic context-free grammar that allows for the expression of complex activity structures, which captures a distinctive temporal structure based on key actions.

### Performance on temporal action segmentation

In Table 10, we compare the performance of each grammar induction algorithm on refining temporal action segmentation models [45]. The overall results show that the KARI-induced grammar demonstrates the best refinement performance compared to the other grammar induction algorithms, showing a significant performance gap in both datasets. The induced grammar of Richard _et al_.also shows better performance than other grammar induction algorithms except for KARI, indicating that the ability to represent intermediate action sequences by production rules helps improve refinement performance. In conclusion, the generalization capabilities and variability of expressing action sequences are essential to guide the temporal action segmentation network to better refinement results.

## Appendix D Qualitative results

We provide additional qualitative results for the Breakfast and 50Salads. Figure 7 shows examples of successful output refinements by the KARI-induced grammar, demonstrating its ability to cover various sequences comprising combinations of multiple actions. This is further evident in Figure 6(a), where ADIOS-OR falls short in covering the _'add dressing'_ action following the _'serve salad'_ action, while KARI handles it proficiently.

We also show failure cases of the KARI-induced grammar in Figure 8, where further improvement is needed. We acknowledge that the KARI-induced grammar sometimes deletes certain actions. This deletion of actions, along with the challenges posed by inaccurate identification of actions by the segmentation model, show areas for improvement in the refinement process. We recognize these as opportunities for future work to enhance the performance of the grammar induction algorithm and address these limitations.

## Appendix E Broader Impact

The research presented in this paper holds significant potential for impact across multiple domains. The development of efficient and effective grammar induction algorithms for activity grammar, coupled with the Breadth-first Earley parser, has the potential to greatly enhance human activity recognition and understanding systems. This, in turn, can have far-reaching implications in various

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline dataset & reprod. & refinement & grammar induction & edit & F1@10 & F1@25 & F1@50 & acc. \\ \hline \multirow{4}{*}{50Salads} & ✓ & - & - & 76.5 & 83.8 & 81.7 & 74.8 & **86.1** \\  & ✓ & ✓ & Kuehne _et al_. [23] & 62.9 & 73.0 & 70.6 & 63.0 & 78.6 \\  & ✓ & ✓ & Richard _et al.[35]_ & 63.1 & 73.1 & 70.5 & 62.9 & 78.7 \\
[39] & ✓ & ✓ & ADIOS-AND & 58.3 & 70.0 & 68.0 & 59.4 & 76.2 \\  & ✓ & ✓ & ADIOS-OR & 61.1 & 72.0 & 70.1 & 62.4 & 78.9 \\  & ✓ & ✓ & KARI & **79.9** & **85.4** & **83.8** & **77.4** & 85.3 \\ \hline \multirow{4}{*}{Breakfast} & ✓ & - & - & 75.6 & 77.3 & 72.0 & 59.4 & **74.3** \\  & ✓ & ✓ & Kuehne _et al_. [23] & 72.8 & 74.0 & 69.1 & 55.5 & 72.9 \\ \cline{1-1}  & ✓ & ✓ & Richard _et al.[35]_ & 77.3 & 77.2 & 72.2 & 59.4 & 74.1 \\ \cline{1-1}  & ✓ & ✓ & ADIOS-AND & 69.2 & 69.8 & 64.9 & 52.2 & 72.4 \\ \cline{1-1}  & ✓ & ✓ & ADIOS-OR & 70.3 & 71.8 & 66.8 & 54.2 & 71.8 \\ \cline{1-1}  & ✓ & ✓ & KARI & **77.8** & **78.8** & **73.7** & **60.8** & 74.0 \\ \hline \hline \end{tabular}
\end{table}
Table 10: The performance comparison with other grammar induction algorithms on two benchmark datasets.

## 6 Conclusion

Figure 7: Qualitative results on successful casesapplications, such as video surveillance, human-computer interaction, robotics, and healthcare monitoring. By improving the accuracy and efficiency of activity recognition systems, our research contributes to advancements in these domains, enabling more robust and intelligent systems. The broader implications of this research extend beyond activity grammar induction itself, fostering innovation and enhancing the capabilities of intelligent systems in diverse fields.

Figure 8: Qualitative results on failure cases