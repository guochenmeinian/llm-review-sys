# Evaluating Cognitive Maps and planning in Large Language Models with CogEval

 Ida Momennejad

Microsoft Research

New York, NY

idamo

Hosein Hasanbeig

Microsoft Research

New York, NY

New York, NY

hosein.hasanbeig

&Felipe Vieira Frujeri

Microsoft

Redmond, WA

felipe.frujeri

&Hiteshi Sharma

Microsoft

Redmond, WA

hiteshi.sharma

Robert Osazuwa Ness

Microsoft Research

Redmond, WA

robertness

&Nebojsa Jojic

Microsoft Research

Redmond, WA

jojic

&Hamid Palangi

Microsoft Research

Redmond, WA

hpalangi

&Jonathan Larson

Microsoft Research

Redmond, WA

jolarso

@microsoft.com

Equal contribution

###### Abstract

Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate _cognitive maps_ and _planning ability_ across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davnic-003-175B, Google Bard, Coherence-slarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.

## 1 Introduction

Large language models (LLMs) are generatively pre-trained and display apparent competence on some cognitive tasks [8]. This has led to a recent surge in studies claiming LLMs have emergent human-level cognitive abilities, which may encourage applications that interact with LLMs in a zero-shot or few-shot manner with expectations of human-level cognition. However, most claims of competence are based on anecdotes rather than systematic evaluation. In response, we make two contributions. First, we propose CogEval, a Cognitive Science-Inspired [12, 5, 39] protocol for Measurement and Evaluation of cognitive abilities in LLMs (Figure 1, top), such as planning, theoryof mind, causal inference, or other abilities. Second, we apply this evaluation protocol to the domain of cognitive maps and planning, and systematically evaluate these capacities across eight LLMs. We build our task prompts according to established human experiments, but our goal is not a comparison with human performance nor any assumptions of LLMs being "human-like" [27]. We evaluate LLMs' _functional_ as opposed to _formal linguistic_ abilities [21], and by that we have both a functionalist and multiple-realizability-based notion of cognitive ability [9] in mind.

We investigated whether LLMs (OpenAI GPT-4, GPT-3.5-175B, and davinci-003-175B, Google Bard, Coherence-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B) understand the latent structure of planning problems (cognitive maps). We hypothesized that failure in planning may relate to cognitive map deficits. To address these questions, we followed the CogEval protocol (Figure 1). First, we operationalized the latent ability (cognitive map and planning) in terms of multiple tasks with variations in three factors: (a) the latent structure of the tasks' environment (different Markov decision processes (MDPs) or graphs), (b) the domain (spatial vs. social ties vs. object relations), and (c) multiple planning tasks for each latent graph structure (c.f. Section 2 for detail). These domains were selected due to their prevalence in everyday problems as well as the cognitive science literature on cognitive maps [4, 44, 35]. We then generated repeated measurements across small and large LLMs (c.f. Section 2 for choice of LLMs) and conducted statistical analysis to compare the results. We found that LLMs only show apparent competence in simpler tasks, where route memorization was sufficient to find a solution, but fail on closer systematic observation. Our evidence suggests against out-of-the-box emergent planning capacities in recently introduced LLMs.

_What is a cognitive map?_ A cognitive map is a representation of latent relational structures that underlies a task or environment, and facilitates planning, reasoning, and inference in biological and artificial problems [46, 4, 26, 7]. The concept originated from Tolman's latent learning experiments, demonstrating rodents' ability to learn maze structures without rewards [46]. This challenged the dominant behavioral view that learning only occurs with reinforcement; and paved the way for a cognitive revolution. Decades later, discoveries of hippocampal place cells [31, 30, 32] and entorhinal cortex grid cells [13, 15, 29], together referred to as "the brain's GPS," further substantiated cognitive maps and earned the 2014 Nobel Prize [1]. Cognitive maps have since been studied behaviorally, computationally, and neurally; revealing that multi-step, multi-scale, and compressed neural representations are crucial for inference in both memory and planning [4, 26, 7]. Over the past decades, a number of reinforcement learning (RL) and deep neural network models have been proposed to capture the computations involved in cognitive maps and planning in the hippocampus and the prefrontal cortex of humans, rodents, bats, monkeys, and birds [4, 37, 7].

_Why would LLMs plan with a cognitive map?_ It has been suggested that the transformer architecture and its learned representations, which lie at the heart of modern LLMs, are comparable to the hippocampus of the brain and the representations it learns [55]. Other studies show that GPT-3 is capable of event segmentation of narrative transcripts similar to human evaluators [23], and evaluate some cognitive capacities of GPT-3 using cognitive science and psychological methods applied in the evaluation of human cognition [5, 38, 48, 56]. Other cognitive scientists have distinguished _formal_ linguistic ability (e.g., the ability to form grammatically correct sentences) from _functional_ cognitive capacities (e.g., theory of mind, sequential planning, etc) and call for a meticulous evaluation of LLMs' _functional_ competence without conflating them with their _formal_ linguistic competence - much like the dissociation of language and thought [21]. Taken together, these studies raise the hypothesis that LLMs would be able to extract and use cognitive maps from text, and second, that LLMs' failure in capturing cognitive maps could underlie failure modes in planning.

To test these hypotheses, we designed prompts to measure behavioral signatures of extraction and use of cognitive maps in a set of tasks adapted from existing human behavioral experiments [25, 24, 28, 34, 36]. We operationalized cognitive maps and planning with a number of tasks (Figure 1) with variations in environmental structures or graphs, varying items from different domains (spatial, social, object relations), and across a number of different conditions (e.g., value-based planning, reward and transition revaluation, shortcut, and detour).

Notably, the corresponding human experiments that inspired our prompts were never in linguistic form, and this is the first adaptation of them to prompts to the best of our knowledge. This is an important consideration since contamination of the training data with the test set is one of the most challenging obstacles to testing LLM capacities. To prevent any possible contamination, we avoided BIG-bench [40], which has been flagged by OpenAI for contamination [2], and a planning benchmarkfor GPT-3 [49] as both pre-date GPT-4 and raise data contamination issues. Here we introduce and generate novel prompts inspired by human experiments with established validity in cognitive science. To our knowledge, a systematic evaluation of planning and cognitive map capacities in GPT-4 and comparison to other LLMs remain unexplored. In what follows we elaborate on a protocol and two related experiments to address this.

Figure 1: **The CogEval protocol, task structure, and example task prompt. (top) In the CogEval protocol, a latent ability can be evaluated by first, being operationalized as tasks, and second, be measured multiple times and with variations and controls. We followed this protocol to evaluate cognitive map and planning. To robustly evaluate these abilities, multiple task prompts were generated with varying task structures (graph), the item domains (e.g., spatial or social), and task conditions (e.g., value-based path, detour). LLM responses were generated 30 times per task prompt and temperature for the three OpenAI models studied in this work and once per task and temperature for other LLMs. The results were compared across task configurations, LLMs, and temperatures using statistical analysis. (middle) The promptsâ€™ underlying task structures were six graphs based on human experiments. A: simple line graph from [25]. B: simple tree graphs based on [24]. C: graph A with double depth and stochastic transitions. D, E, and F represent community graphs from [36], [28], and [34] respectively. (bottom) An example prompt for graph A. This procedure evaluates planning behavior in value-based navigation (see Table 1). The colored transitions in the figure are for clarity, showing different stages of the latent transition structure (cognitive map or graph).**

Methods

**The CogEval protocol.** In order to evaluate cognitive-map-related planning and navigation in LLMs, we propose and use the CogEval protocol (Figure 1). Please note that CogEval is not a benchmark nor limited to cognitive maps, it is a general protocol for evaluating any cognitive capacity, such as planning, theory of mind, causal reasoning, etc. As an example, here we have applied it to the domain of cognitive maps and planning.

CogEval adheres to four methodological best practices suggested by cognitive scientists [12]. First, the _latent construct or ability_: here we evaluate cognitive maps, which are representations that capture a model of the task structure, and adaptive planning, which requires an internal representation of task structures (similar to model-based RL [42] or task-oriented model-free RL [16; 17; 18; 19]). Second, _operationalization with construct validity_: we operationalize planning ability by generating unique variations of established experimental tasks that measure the comprehension and use of cognitive maps in multi-step planning behavior [25; 36; 24]. Third, _multiple tasks and multiple response generations_: we generated many tasks for each condition of interest varying graph structure, and domain (spatial with ordered states such as room numbers, spatial with unordered states, social ties, object relations). Most task conditions include a partial change in the environment to test adaptive planning (e.g., changing the location of rewards or the structure of the environment, see Table 1). Collectively, these tasks allow us to robustly measure the latent construct: cognitive map and planning ability. Fourth, including multiple task conditions allows us to _control_ for multiple factors when making inference about the ability.

Thus, we evaluate the construct using multiple environments with different graph structures (based on existing human experiments on cognitive maps [25; 36; 24], see graphs in Figure 1), controlling for robustness to variations in graphs, task conditions, and item domains (e.g., rooms, people, objects, random letters), using multiple generations (30 generations per condition), and across different temperatures (\(0\), \(0.5\), and \(1\)).

**LLMs evaluated.** We compared the following LLMs: GPT-4-* [2], GPT-3.5-turbo-175B [33], text-Davinci-3-175B [6] (Azure OpenAI API), Bard-* [45], Anthropic Claude-1-52B [3], LLaMA-13B [47], Cohere-52.4B [10], Alpaca-7B [43] (nat.dev API), where * means the number of parameters is undisclosed.

**Experiments.** We conducted planning experiments to systematically compare the performance of all LLMs across task conditions created with 3 factors of graph structure (6 graphs), domain (3 domains), and tasks (15 tasks) over 3 temperatures (0, 0.5, 1)).

### A cognitive science inspired evaluation of cognitive maps and planning capacity in LLMs

We designed our experiment prioritizing robustness and control conditions. Model performance on cognitive tasks can be influenced by various factors beyond the primary cognitive capacity, such as the specific prompts, the temperature parameter, experimental conditions (Table 1, Figure 1, bottom), the specific items the task is presented with or domain (e.g., spatial connections vs. social ties), and the specific relational graph underlying the problem (e.g., this could be a graph structure such as line graphs, trees, community graphs with different size and density). For instance, perhaps an LLM performs better when the items in a task are rooms that are numbered in order in a line graph (item or domain effect), or when the graph structure is finite rather than a community graph with potential loops (graph effect). Thus, we implemented measures to mitigate such effects, like potential performance variations due to task item selection or its underlying graph structure. We measured the results for each combination of factors and parameters 30 times for OpenAI models (for which we had API access) and once for the remaining models with no API access. We compared the results across 10 LLMs.

_Why vary temperature?_ Temperature in LLMs determines randomness in the generated response, by manipulating the probabilities of the next word in a sequence. Thus, temperature can be thought of as a parameter controlling the diversity of the output. A temperature of 0 results in deterministic or greedy responses with less variance (Note: OpenAI has made it known that even a temperature of 0 is not entirely deterministic). With higher temperatures, especially closer to 1, the LLM creates more diverse and varied text upon repetition, akin to exploration. While a higher temperature may be helpful for tasks that require varied responses or creativity, it could go either way for planning: on the one hand, precision in planning trajectories may seem more in line with a deterministic temperature, and on the other, a higher temperature leads to exploration, which may improve behavior by getting out of local minima. Repeating the experiments with varying temperature can help address its possible effect in either direction.

**Statistical analysis.** We evaluated the robustness of each LLM's performance by applying a statistical model of how each of the factors and their combinations contribute to variance in performance. Specifically, we fit a logistic regression analysis with domain, condition, and graph types as categorical regressors, and included second and third-order interaction terms between these three terms. We made sure that each combination of domain, condition, and graph had several replicates, though the approach is robust to imbalance issues. We included model version and temperature as separate independent variables that account for technical variation distinct from our conditions of interest. See supplement for full details on analysis and results.

#### 2.1.1 Task prompts

Navigating cognitive maps requires adaptive multi-step planning using compressed representations of the environment, not mere memor memorization of all routes. Thus, cognitive map experiments test flexible adaptivity to local changes in the environment to evaluate biological and reinforcement learning agents [25, 25, 26, 14]. Latent learning experiments found that rodents who explored a maze with no reward could quickly find the shortest route to a newly introduced reward, i.e., find an optimal policy in RL context. This was taken as their ability to learn the cognitive maps of their mazes [46], but various additional experimental conditions were then designed and evaluated to confirm that they could flexibly adapt their cognitive map and planning to local environment alterations such as reward relocation (revaluation), changes to the map (transition revaluation) [25], or the introduction of shortcuts and detours [41]. Previous research has adapted these experiments to investigating the robustness and flexibility of deep model-based RL in the face of local changes to the reward structure (LoCA), and shown that deep model-based RL agents such as Dreamer v2, muZero, and PlaNet failed at flexible planning in reward revaluation scenarios [52]. Here we operationalized our tasks inspired by similar conditions in human reinforcement learning and deep MBRL experiments on learning, updating, and using cognitive maps for adaptive and flexible planning [25, 52].

Importantly, the corresponding human experiments were never conducted using texts, but were presented either as videos or a sequence of images that human participants moved forward by choosing an action (e.g. pressing left, right, up, or down). We believe this mitigates the risks of contamination. Moreover, when possible, we set the date earlier than our pilot studies to avoid potential contamination due to our experiments in the past month. To also ensure that the model cannot infer any answers from the papers, we asked GPT-4 to explain the experimental paradigm and draw the map of the environments after providing it a reference to a specific figure in a corresponding paper, and it failed. Thus, we believe our prompts have a negligible to no chance of having contaminated the training sets.

Below we provide examples of task prompts for graph A and a spatial domain (number ordered rooms). All prompts are available in the supplementary material and on https://github.com/cogeval/cogmaps.

**I. Value-based or goal-driven planning.** Below is an example prompt for value-driven or goal-directed planning in graph A in Figure 1. Success requires an understanding of the start and goal positions, comparison of the paths to find the shortest path that leads to the highest rewards, and planning a multi-step navigation or traversal of the underlying graph structure of the task.

_Imagine a world with six rooms. From the lobby you have two choices, room 1 and room 2. You enter room 1, at the end there's a door that leads to room 3, and room 3 leads to room 5. There's a chest in room 5. You open it and there's 10 dollars. Then you exit and start over. This time in the lobby you choose room 2, then enter room 4, which leads to room 6. There's a chest with 50 dollars. You return to the lobby. Which room will you choose to make the most money?_

**II. Transition Revaluation, after prompt I.** This condition occurs when the structure of the environment (e.g., an edge of the graph or Markov decision process) locally changes, and planning requires integrating or 'piecing together' different parts of the cognitive map to update one's plan or policy.

_Now you're dropped in room 3 and the door at its end suddenly leads to room 6, and then you're dropped in room 4 and the door at its end suddenly leads to room 5. you return to the lobby. Which room will lead to more rewards?_

**III. Reward Revaluation, after prompt I.** A common local change in any environment is when the location of rewards or goals change, without any changes to the map or structure of the states (or the cognitive map). This is known as Reward Revaluation or retrospective revaluation of rewards [25].

_Now you're dropped into room 3, then you enter room 5 and the chest has 100 dollars. Then you're taken out, and dropped into room 4, then you enter room 6 and the chest has the same amount as before. When you return to the lobby, which room do you choose to make the most reward?_

**V. Shortcut prompts with and without teleportation, after prompt I.** Tolman's experiments on cognitive maps [46] included a condition evaluating the animal's ability to discover shortcuts. Since the early 1990s, evaluating the ability of various Dyna architectures [42] in discovering shortcuts has been an important part of evaluating planning behavior. Below are two different shortcut prompts.

_In the lobby you're presented with a portal, and you can choose which room to teleport into. Which room do you choose to maximize rewards?_

_In the lobby you're presented with a new door which leads to a new room, room 7. Room 7's door leads directly to room 6. Remember that you will only be able to choose one path that leads to the most money. Which room from the lobby will lead to the path where one can make the most money?_

**V. Detour prompts with and without Teleportation, after prompt I.**

_You enter the lobby and this time you encounter a new room, room 7. Room 7's door leads to room 8, and room 8 leads to room 9. From room 9 you can teleport anywhere. You return to the lobby, and choose the room that leads to the most reward, but the door to the next room is blocked. You go back to the lobby. Which room do you choose to reach the most rewards?_

_You enter the lobby and this time you encounter a new room, room 7. Room 7's door leads to room 8, and room 8 leads to room 6. When you return to the lobby and choose the previous path that led to the most reward, you discover that the regular door to the room with the most money is now blocked. You go back to the lobby. You will only be able to choose one path that leads to the most money. Which room from the lobby will lead to the path where one can make the most money?_

## 3 Results

### Repeated measures comparison of planning across LLMs

We evaluated out-of-the-box emergent or native ability of different LLMs on the cognitive map tasks. Table 2 shows the statistical analysis highlighting the contributions of each factor to a logistic regression model's fit of LLM model performance. The magnitude of chi-square test statistics indicate contribution to overall model fit. Figure 2 compares the performance of all LLMs across all latent graph structures. Table 3 shows mean and standard error for planning performance across tasks and LLMs.

The results in Table 2 indicate that the LLM (\(\chi^{2}(11)=2357.87\), p <.001), graph (\(\chi^{2}(11)=3431.53\), p <.001), condition (\(\chi^{2}(11)=2080.04\), p <.001), and domain (\(\chi^{2}(11)=304.06\), p <.001) each yielded significant chi-squared statistics. This means that not only did different LLMs performeddifferently, but performance varied as a result of varying graphs, domains, and conditions. Conversely, the temperature showed a non-significant chi-squared statistic (\(\chi^{2}(11)=1.28,p=.53\)) and the interaction between the LLM and temperature was also non-significant (\(\chi^{2}(11)=10.69,p=.71\)). Noteworthy, the interactions among graph-domain, graph-condition, domain-condition, and graph-domain-condition were all significant (all p's <.001). The interactions among graph-domain (\(\chi^{2}(11)=334.41,p<.001\)), graph-condition (\(\chi^{2}(50)=1651.33,p<.001\)), domain-condition (\(\chi^{2}(39)=310.53,p<.001\)), and graph-domain-condition (\(\chi^{2}(108)=1133.16,p<.001\)) were all significant. A full table of regression coefficient estimates is included in the supplement.

In summary, while the 'temperature' and the interaction of 'LLM' and 'temperature' do not show significant effects on planning tasks, all other factors and their interactions significantly contribute to the variations in the dependent variable. These effects show that LLM performance on cognitive map and planning tasks was not robust to the graph structure of the problems, the domain, nor the task conditions, and it also varied across models (see Tables 2 and 3 and Figure 2).

### Failure modes

We note three main _failure modes_ when the task had an underlying graph with a dense community structure. Notably, when we probe the LLMs to list connected rooms or items as \((\mathrm{state},\mathrm{actions},\mathrm{state})\) tuples, they do well (e.g., \((\mathrm{room1},\mathrm{opendoor},\mathrm{room3})\) is a tuple for graph A in Figure 1). However, when asked to do any tasks with a community graph structure using this tuple knowledge, LLMs display the following failure modes; (1) hallucinate edges that do not exist, or (2) produce longer trajectories instead of shortest paths, or (3) produce trajectories that fall in loops. For example in the task of finding the shortest path to a state that is 1 cluster away, out of \(30\) runs GPT-4 has a success rate of \(0\) at temperature \(0\). Even with changing the temperature to \(0.5\) or

\begin{table}
\begin{tabular}{l l} \hline \hline
**Condition** & Description & Group \\ \hline
**valuePath** & The optimal solution is to find the optimal policy, or shortest path, which yields the highest reward \\
**1stepPath** & The optimal solution is a 1-step policy, i.e., goal is adjacent to the starting state \\
**2stepPath** & The optimal solution is a 2-step policy \\
**3stepPath** & The optimal solution is a 3-step policy \\
**netapPath** & The optimal solution is an a-step policy, where max is the diameter of the graph (longest shortest path) \\ \hline
**rewardLevel** & Upon a local change in the _reward structure_, the goal has changed and the optimal solution requires finding a new path \\
**policyReal** & Upon a local change in the _reward structure_, the optimal solution requires finding a new policy \\
**tramLevelStochastic** & Upon a local change in the _transition structure_, the goal is the same but the optimal solution requires finding a new policy \\
**tramLevel** & Upon a local change in the _transition structure_, the goal is the same but the optimal solution requires finding a new policy \\
**tramLevel** & **TransLevel** \\ \hline
**noticeShortcut** & Upon a change in the graph structure, the optimal solution requires finding a shortest path \\
**noticeShortcut** & Upon a change in the _transition structure_, the optimal solution requires finding a shortest using a teleportation portal \\
**tackEventCnT** & Upon a local change in the _regular and transition structure_, the optimal solution requires finding a shortcut using a teleportation portal, an additional CoT prompt is given \\
**noticeDetoor** & Upon a change in the graph structure, the optimal solution requires finding a shortcut using a teleportation step \\ \hline \hline \end{tabular}
\end{table}
Table 1: Brief descriptions of the task conditions applied to varying graphs and domains

Figure 2: **Results for planning experiments in 8 LLMs. (left) Mean and standard error of performance on all tasks for each of the different graphs (see Figure 1 for graph details) across different LLMs studied in this work. (right) Mean performance compared across per main task category (see Table 3 for details).**

\(1\) and repeating the same \(30\) runs its success rate can not exceed \(10\%\). Please refer to Figure 3 for examples of above failure modes.

## 4 Discussion and future directions

This paper makes two main contributions. First, we introduce CogEval, a cognitive science inspired protocol for systematic and robust evaluation of functional [21] cognitive abilities in LLMs. Second, we follow the CogEval protocol to evaluate multiple LLMs' native or emergent ability to extract cognitive maps for sequential planning, navigation, or graph inference. All tasks and prompts are based on non-linguistic human cognitive science experiments that we adapted into text prompts for the first time. We test for robustness of the findings by varying task conditions, graph structure, domains (spatial, social), and LLM temperature. Our systematic evaluation reveals that while LLMs display apparent competence on some tasks in simpler graphs, they do not have out-of-the-box zero-shot emergent cognitive map comprehension or planning competence for other graphs.

_Methodological contribution_. **CogEval**, our proposed cognitive-science inspired protocol [12] for systematic evaluation of LLMs, affords the following contributions. (1) We avoid the reuse of contaminated standardized benchmarks by creating novel prompts based on non-text-based experiments that are known to evaluate cognitive maps and planning in humans, animals, and RL. (2) We use multiple tasks to probe the cognitive constructs (cognitive maps and planning) and repeat each interaction multiple times and across different temperatures. (3) We use statistical analysis to evaluate the robustness and reliability of each effect, with three main factors of graph structure, item domain (spatial vs. social), and task condition (e.g., value-based decision making, shortcut, detour, see Table 1). (4) We employ chain of thought and instruction prompts to evaluate the limits of out-of-the-box cognitive abilities of LLMs (Supplementary Experiment 2), and (5) analyze and categorize different types of failure modes. Please note that CogEval is not a benchmark nor limited to evaluating cognitive maps and planning, it is a general protocol for evaluating any cognitive capacity in LLMs. As an example, in this paper we have applied it to the domain of cognitive maps and planning.

_No evidence for understanding cognitive maps or planning_. Our systematic and incremental evaluations reveal limited to no cognitive map capacities in the current generation of LLMs - including

\begin{table}
\begin{tabular}{l l r r r} \hline  & term & Chi-squared Stat (Deviance) & df & p value \\ \hline
1 & LLM & 2357.87 & 7 & \textless{}0.001 \\
2 & graph & 3431.53 & 5 & \textless{}0.001 \\
3 & domain & 458.74 & 2 & \textless{}0.001 \\
4 & temperature & 1.28 & 2 & 0.53 \\
5 & condition & 2080.04 & 4 & \textless{}0.001 \\
6 & LLM and temperature & 10.69 & 14 & 0.71 \\
7 & graph and domain & 334.41 & 10 & \textless{}0.001 \\
8 & graph and condition & 1651.33 & 20 & \textless{}0.001 \\
9 & domain and condition & 310.53 & 8 & \textless{}0.001 \\
10 & graph, domain, condition & 1133.16 & 44 & \textless{}0.001 \\ \hline \end{tabular}
\end{table}
Table 2: Step-wise contribution of adding each factor to the logistic regression fit of LLM model performance (number of successes out of max possible successes in dialog).

\begin{table}
\begin{tabular}{l r r r r r r r r r r} \hline
**Condition** & & gpt-4.3-3k & gpt-35 & davinci-003 & clande-v1 & pythia-20b & cohere & Ilama-13b & alpaca-7b & hard \\ \hline
**IstepPath** & **0.99,0.08** & 0.76, 0.32 & 0.52, 0.45 & 0.57, 0.37 & 0.64, 0.41 & 0.27, 0.42 & 0.23, 0.38 & 0.27, 0.41 & 0.05, 0.10 \\
**2stepPath** & **0.82,0.35** & 0.73, 0.38 & 0.16, 0.25 & 0.61, 0.41 & 0.67, 0.42 & 0.29, 0.44 & 0.22, 0.37 & 0.35, 0.47 & 0.25, 0.50 \\
**3stepPath** & 0.55, 0.38 & 0.73, 0.37 & **0.58,0.43** & 0.27, 0.31 & 0.35, 0.49 & 0.04 & 0.07, 0.06 & 0.02, 0.11, 0.10 \\
**nonteleDetour** & **0.55, 0.39** & 0.51, 0.35 & 0.35, 0.43 & 0.50, 0.41 & 0.51, 0.57 & 0.21, 0.35 & 0.19, 0.32 & 0.26, 0.38 & 0.29, 0.48 \\
**nonteleShortcut** & 0.56,0.40 & 0.25, 0.39 & 0.49, 0.40 & **0.62,0.43** & 0.36, 0.16 & 0.27, 0.11, 0.18 & 0.20, 0.30 & 0.29, 0.48 \\
**nonteleStructcutC1** & **1.00,0.00** & **1.00,0.00** & 0.09, 0.07 & 0.58, 0.38 & 0.36, 0.38 & 0.37, 0.40 & 0.17, 0.29 & 0.37, 0.37 & 0.37 & 0.37 \\
**nstepPath** & **0.47,0.38** & 0.31, 0.34 & 0.17, 0.27 & 0.33, 0.37 & 0.27, 0.42 & 0.05, 0.11 & 0.06, 0.08 & 0.12, 0.32 & 0.00, 0.00 \\
**policyReval** & 0.21, 0.18 & 0.12, 0.23 & 0.13, 0.04 & **0.28,0.30** & 0.00 & 0.00, 0.00 & 0.00, 0.04 & 0.07, 0.05 & 0.22 & 0.00, 0.00 \\
**rewardReval** & **0.67,0.40** & 0.57, 0.36 & 0.34, 0.25 & 0.48, 0.36 & 0.45, 0.34 & 0.41, 0.28 & 0.43, 0.33 & 0.44, 0.14 & 0.14, 0.14 \\
**teleDetour** & 0.47, 0.35 & 0.34, 0.30 & **0.53,0.44** & 0.37, 0.33 & 0.44, 0.41 & 0.21, 0.35 & 0.23, 0.37 & 0.23, 0.38 & 0.29, 0.48 \\
**testRobortcutC4,0.54** & **0.39,** 0.35, 0.33 & 0.44, 0.41 & 0.48, 0.39 & 0.27, 0.33 & 0.16, 0.27 & 0.16, 0.22 & 0.12, 0.24 & 0.29, 0.48 \\
**teleShortcutCoT** & 0.50,00.00 & 0.50, 0.00 & 0.04, 0.01 & 0.50, 0.50 & 0.39, 0.36 & 0.19, 0.40 & **0.83,0.29** & 0.35, 0.36 & - \\
**trankEval** & **0.60,0.42** & 0.59, 0.40 & 0.49, 0.38 & 0.55, 0.36 & 0.47, 0.42 & 0.19, 0.28 & 0.22, 0.33 & 0.27, 0.37 & 0.08, 0.17 \\
**transRevalStochastic** & 0.73, 0.36 & 0.52, 0.36 & **0.91,0.24** & **0.78,** 0.34 & 0.36, 0.32 & 0.00, 0.00 & 0.11, 0.19 & 0.22, 0.39 & - \\
**valuePath** & 0.58, 0.41 & 0.66, 0.40 & **0.66,0.39** & 0.44, 0.41 & 0.49, 0.46 & 0.31, 0.40 & 0.27, 0.39 & 0.33, 0.45 & 0.29, 0.48 \\ \hline \end{tabular}
\end{table}
Table 3: Mean and standard errors for planning performance across all task conditions in all 10 LLMs. ARI scores closer to zero represent poor performance by the LLM and ARI scores reaching 1.0 represent performance matching Leiden.

GPT-4. Specifically, we find that LLMs only show apparent competence on simple sequential inference tasks where route memorization can help, and given LLMs have received all possible trajectories in the text prompt. We also observe that the sparsity of graph structure drove performance. However, when 1-step and multi-step traversal and planning require understanding the underlying relational structure of the environment graph, LLMs including GPT-4 fail by hallucinations, suboptimally long routes, or falling in loops.

_How did LLMs solve the simpler tasks?_ Without access to full architectures or training sets, we can only form hypotheses based on our behavioral observations. We observe that LLMs do better in problems where the entire trajectories are explicitly available in the text prompts, and they only need to retrieve and piece together partial changes. However, planning behavior in more complex graphs is far worse, and this is not just due to graph size: performance was worse for graph B (7-node tree) than C (12 node, parallel lines). Performance on the 15-node graph with 3 dense clusters was worse than the 16-node (\(4\)-cluster) graph that has better cross-cluster connectivity and more paths among clusters.

These observations suggest that LLMs may fail at planning problems where they need to use the transition structure to unroll the trajectories and find the correct path, which is closer to the notion of planning in model-based RL and in cognitive science. Capturing the underlying structure and using it to unroll trajectories are quintessential to cognitive maps and planning ability. Thus, the apparent competence in simpler tasks may be due to using cached or memorized routes rather than understanding the cognitive map, planning, or inference ability.

LLMs may do better in smaller and simpler graphs because the prompt already expands all the possible paths or trajectories. When there is a change in the rewards or transition structure, LLMs only need to change one step in an already laid out path. However, in more complex graphs only the one-step connections are laid out in the prompt, but not all paths or trajectories between any given two nodes. We observed that failures significantly increase in tasks with these larger graphs with community structures, even when an LLM can list the pairwise tuples of connected states (see failure modes, Figure 3).

_Interpreting the results._ The experiments in the paper are not meant to be interpreted as a benchmark for planning. They probe the same construct in different ways, evaluating the ability to use information about \((\mathrm{state},\mathrm{action},\mathrm{state})\) tuples, e.g., \((\mathrm{room1},\mathrm{opelletdoor},\mathrm{room3})\), to piece together policies in response to task prompts. A reader may wonder why we claim that LLMs do not display emergent planning in spite of non-zero performance for some tasks in our experiments (Figure 2. We interpret the findings as such due to failure modes and various inconsistencies in success cases (Figure 3). For instance, a common failure mode is generating sequences with hallucinated \((\mathrm{state},\mathrm{actions},\mathrm{state})\) tuples that do not exist. For GPT-4 this constitutes over 20% of 4-step plans. Another common failure mode is that they fall into loops when prompted to find the shortest path between two states (Figure 3, left and right). LLMs sometimes even fail to identify 1-step paths or suggest multi-hop trajectories for traversing to an adjacent state (Figure 3, middle).

Figure 3: **Examples of three failure modes. (left) Edge hallucination. (middle) Failure at finding a 1-step policy within the same cluster. (right) Failure at multi-hop path by both falling in a loop and hallucinating edges. In each example the blue box is the _task prompt_, the grey box shows the _model response_, and the green arrows demonstrate the correct response on the graph.**

These observations seem counter-intuitive given some LLMs can generate a list of tuples when asked, but fail to use the tuples to make (even 1-step) valid plans. This shows that, while LLMs appear to solve planning problems when given simple routes that can be explicitly memorized, they cannot _generalize_ from route memory solutions (trajectories directly in the prompt) to using the tuples to adaptively generate branching plans. Together, these inconsistent observations are in line with the hypothesis that _LLMs do not understand cognitive maps_ and therefore cannot consistently plan. Elsewhere propose black box architectures that improve planning performance [54]. However, our findings point to a lack of out-of-the-box zero-shot emergent planning ability.

_Limitations_. First, we lack knowledge of LLMs like GPT-4's architecture or training. To address this limitation, we did not use existing text-based benchmarks and instead generated novel prompts not in their training sets. Second, in the human experiments that influenced our prompts participants learn gradually, experiencing states one-by-one, only tested after they showed signs of learning, similar to a model-based RL agent having the transition structure and using it for inference and planning. To address this difference, we present the environment's structure in linguistic format. The LLM had to extract the cognitive map, identify the goal location based on instructions. and infer the policy towards the goal. Third, we do not have a human baseline for the language-based tasks, so a comparison with human behavior on the exact language prompts remains the topic of future studies.

_Implication for applications_. LLMs are expected to be applied in fields like gaming, planning, and social reasoning, with tasks that require understanding the inherent relational structure of the problem from the input for flexible reasoning and planning. However, here we show various failure modes in the understanding of the underlying cognitive maps or planning abilities, including hallucination and falling in loops. Even when provided instructions and Chain of Thought (CoT) prompts like breadth-first search (BFS), we observe that GPT-4 struggles to process multi-hop paths it has not experienced (Supplementary Experiment 2). These findings suggest caution in the application of LLMs in problems that involve planning or complex structures. Below we discuss future directions that may mitigate these challenges for problems with simpler structures.

_LLMs as programmable machines rather than emergent intelligence?_ While some regard LLMs as agents with emergent intelligence comparable to humans and animals, our findings are more consistent with the view that LLMs are programmable machines where natural language is their programming language [20]. Thus, here we evaluated planning in LLMs in a functionalist and multiple-realizability sense rather than making any assumptions of them being "human-like" [27].

_Future directions_. A future direction is to analyze embedding representations and attention in LLMs, and test hypotheses about representations underlying success and failure in planning. This mirrors how neuroscience analyzes neural data to understand representations in model-based and predictive planning and decision-making [25, 7]. A recent study suggests a promising future for this direction [57]. Another interesting direction is to study the limits of LLMs' transitive inference using pairwise associations [37, 34], given we observed that while some LLMs could list pairwise tuples or recognize the goal, they still struggled with planning. A further direction is to study whether the use of _schemas_, i.e., overused, generalized cognitive maps such as "airport" [50, 22, 11, 51], can improve performance on real-world scenarios by evoking helpful structures, given LLMs have shown promise with analogical reasoning tasks [53]. Finally, some have suggested ways to improve planning by augmenting LLMs with algorithms that enable executive control, an interesting direction that can contribute to the future of augmenting both larger and especially smaller language models (see [54]).

_LLMs need a hippocampus and prefrontal cortex._ The hippocampus and prefrontal cortex in the brain extract the relational structure or cognitive maps from sequential data to flexibly plan at multiple scales [7, 26]. Their functions can inspire memory, planning, and executive control augmentations to LLMs in order to mitigate failure modes such as hallucinating edges. Ongoing research shows that indeed prefrontal cortex-inspired solutions can improve planning performance in an LLM-based architecture with multiple calls to GPT-4 [54], pointing at a promising future direction.

_Summary_. We introduced CogEval, a cognitive science inspired protocol for systematic and robust evaluation of LLMs. We applied CogEval to evaluate planning performance across 8 LLMs and found poor performance.

Acknowledgement

We are extremely grateful to Peter Lee, Michael Frank, John Krakauer, Joshua Tenenbaum, Paul Bennett, Alison Gopnik, and Melanie Mitchell for early feedback on methods and results. We would also like to acknowledge Derek Worthen and Richard Ciapala for engineering support.

## References

* [1] The nobel prize in physiology or medicine 2014. https://www.nobelprize.org/prizes/medicine/2014/press-release/. Accessed: 2023-5-10.
* [2] OpenAI 2023. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [3] Anthropic. Introducing Claude. https://www.anthropic.com/index/introducing-claude, 2023. [Online].
* [4] Timothy E J Behrens, Timothy H Muller, James C R Whittington, Shirley Mark, Alon B Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowledge for flexible behavior. _Neuron_, 100(2):490-509, October 2018.
* [5] Marcel Binz and Eric Schulz. Using cognitive psychology to understand GPT-3. _Proceedings of the National Academy of Sciences_, 120(6):e2218523120, 2023.
* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [7] Iva K Brunec and Ida Momennejad. Predictive representations in hippocampal and prefrontal hierarchies. _J. Neurosci._, November 2021.
* [8] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. March 2023.
* [9] Rosa Cao. Multiple realizability and the spirit of functionalism. _Synthese_, 200(6):506, December 2022.
* [10] Cohere. Introducing Cohere. https://txt.cohere.com/cohere-launches-extremely-large-beta-2, 2022. [Online].
* [11] Delaram Farzanfar, Hugo J Spiers, Morris Moscovitch, and R Shayna Rosenbaum. From cognitive maps to spatial schemas. _Nat. Rev. Neurosci._, 24(2):63-79, February 2023.
* [12] Michael C Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams. Experimentology. https://experimentology.io/, 2023. Accessed: 2023-5-9.
* [13] Marianne Fyhn, Sturla Molden, Menno P Witter, Edvard I Moser, and May-Britt Moser. Spatial representation in the entorhinal cortex. _Science_, 305(5688):1258-1264, August 2004.
* [14] Mona M Garvert, Raymond J Dolan, and Timothy Ej Behrens. A map of abstract relational knowledge in the human hippocampal-entorhinal cortex. _Elife_, 6, 2017.
* [15] Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. Microstructure of a spatial map in the entorhinal cortex. _Nature_, 436(7052):801-806, August 2005.
* [16] Hosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Logically-constrained neural fitted Q-iteration. In _Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems_, pages 2012-2014. International Foundation for Autonomous Agents and Multiagent Systems, 2019.
* [17] Hosein Hasanbeig, Yiannis Kantaros, Alessandro Abate, Daniel Kroening, George J Pappas, and Insup Lee. Reinforcement learning for temporal logic control synthesis with probabilistic satisfaction guarantees. In _Proceedings of the 58th Conference on Decision and Control_, pages 5338-5343. IEEE, 2019.
* [18] Hosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Certified reinforcement learning with logic guidance. _Artificial Intelligence_, page 103949, 2023.

* [19] Hosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel Kroening. Symbolic task inference in deep reinforcement learning. _Journal of Artificial Intelligence Research (JAIR)_, 2023.
* [20] Ana Jojic, Zhen Wang, and Nebojsa Jojic. GPT is becoming a turing machine: Here are some ways to program it. March 2023.
* [21] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. Dissociating language and thought in large language models: a cognitive perspective. January 2023.
* [22] Rolando Massis-Obando, Kenneth A Norman, and Christopher Baldassano. Schema representations in distinct brain networks support narrative memory during encoding and retrieval. _Elife_, 11, April 2022.
* [23] Sebastian Michelmann, Manoj Kumar, Kenneth A Norman, and Mariya Toneva. Large language models can segment narrative events similarly to humans. _ArXiv_, January 2023.
* [24] I Momennejad, A R Otto, N D Daw, and K A Norman. Offline replay supports planning in human reinforcement learning. _Elife_, 2018.
* [25] I Momennejad, E M Russek, J H Cheong, M M Botvinick, N D Daw, and S J Gershman. The successor representation in human reinforcement learning. _Nat Hum Behav_, 1(9):680-692, September 2017.
* [26] Ida Momennejad. Learning structures: Predictive representations, replay, and generalization. _Current Opinion in Behavioral Sciences_, 32:155-166, April 2020.
* [27] Ida Momennejad. A rubric for human-like agents and NeuroAI. _Philos. Trans. R. Soc. Lond. B Biol. Sci._, 378(1869):20210446, December 2022.
* [28] Ida Momennejad, Ajua Duker, and Alan Coman. Bridge ties bind collective memories. _Nat. Commun._, 10(1):1578, April 2019.
* [29] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain's spatial representation system. _Annu. Rev. Neurosci._, 31:69-89, 2008.
* [30] J O'Keefe. Place units in the hippocampus of the freely moving rat. _Exp. Neurol._, 51(1):78-109, April 1976.
* [31] J O'Keefe and J Dostrovsky. The hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat. _Brain Res._, 34(1):171-175, November 1971.
* [32] John O'Keefe and Lynn Nadel. _The Hippocamp as a Cognitive Map_. Oxford: Clarendon Press, 1978.
* [33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [34] Athula Pudhiyidath, Neal W Morton, Rodrigo Viveros Duran, Anna C Schapiro, Ida Momennejad, Demitrius M Hinojosa-Rowland, Robert J Molitor, and Alison R Preston. Representations of temporal community structure in hippocampus and precuneus predict inductive reasoning decisions. _J. Cogn. Neurosci._, 34(10):1736-1760, September 2022.
* [35] Matthew Schafer and Daniela Schiller. Navigating social space. _Neuron_, 100(2):476-489, October 2018.
* [36] Anna C Schapiro, Timothy T Rogers, Natalia I Cordova, Nicholas B Turk-Browne, and Matthew M Botvinick. Neural representations of events arise from temporal community structure. _Nat. Neurosci._, 16(4):486-492, April 2013.

* [37] Anna C Schapiro, Nicholas B Turk-Browne, Matthew M Botvinick, and Kenneth A Norman. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. _Philos. Trans. R. Soc. Lond. B Biol. Sci._, 372(1711), 2017.
* [38] Richard Shiffrin and Melanie Mitchell. Probing the psychology of AI models. _Proc. Natl. Acad. Sci. U. S. A._, 120(10):e2300963120, March 2023.
* [39] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. March 2023.
* [40] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartomiej Bojanowski, Bathuan Ozyurt, Behnam Hedayatnia, Beham Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri Ramirez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegui Gonzalez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Draxard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwek Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuek Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martinez-Plumed, Francesca Happe, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, German Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schutze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hillton, Jaehoon Lee, Jaime Fernandez Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jaa Kocon, Jana Thompson, Jared Kaplan, Jarena Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, Jorg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkarruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colton, Luke Metz, Luft Kern Seneel, Maarten Bosma, Maarten Sap, Maarife ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, Matyas Schubert, Medina Orduna Baitenirova, Melody Arnaud, Melvin McElrath, Michael A Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michal Swedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal, Moin Aminaseri, Mor Geva, Mozhdeh Gheini, Varma T Mukund, Nanyun Peng,Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegahi Aliporomolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Hutt, Pinyu Hwang, Piotr Mikowski, Piyush Patil, Pouya Pezeshkpour, Prti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco Delgado, Raphael Milliere, Rhythm Garg, Richard Barnes, Rif A Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R Bowman, Samuel S Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Segpideh Sadeghi, Shaadi Hamdan, Sharon Zhou, Shashashar Srivastava, Sherry Shi, Shikhar Singh, Shima Asadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T Piantadosi, Stuart M Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Ramasesh, Vianay Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadch, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. June 2022.
* [41] K L Stachenfeld, M Botvinick, and others. Design principles of the hippocampal cognitive map. _Adv. Neural Inf. Process. Syst._, 2014.
* [42] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT Press, November 2018.
* [43] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following LLaMA model, 2023.
* [44] Rita Morais Tavares, Avi Mendelsohn, Yael Grossman, Christian Hamilton Williams, Matthew Shapiro, Yaacov Trope, and Daniela Schiller. A map for social navigation in the human brain. _Neuron_, 87(1):231-243, July 2015.
* [45] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.
* [46] E C Tolman. Cognitive maps in rats and men. _Psychol. Rev._, 55(4):189-208, July 1948.
* [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [48] Tomer Ullman. Large language models fail on trivial alterations to Theory-of-Mind tasks. February 2023.
* [49] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). June 2022.

* [50] Marlieke T R van Kesteren, Sarah F Beul, Atsuko Takashima, Richard N Henson, Dirk J Ruiter, and Guillen Fernandez. Differential roles for medial prefrontal and medial temporal cortices in schema-dependent encoding: from congruent to incongruent. _Neuropsychologia_, 51(12):2352-2359, October 2013.
* [51] Marlieke Tina Renee van Kesteren and Martijn Meeter. How to optimize knowledge construction in the brain. _NPJ Sci Learn_, 5:5, May 2020.
* [52] Yi Wan, Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Sarath Chandar, and Harm van Seijen. Towards evaluating adaptivity of Model-Based reinforcement learning methods. April 2022.
* [53] Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. December 2022.
* [54] Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, and Ida Momennejad. A prefrontal cortex-inspired architecture for planning in large language models, 2023.
* [55] James C R Whittington, Joseph Warren, and Timothy E J Behrens. Relating transformers to models and neural representations of the hippocampal formation. December 2021.
* [56] Eunice Yiu, Eliza Kosoy, and Alison Gopnik. Imitation versus innovation: What children can do that large language and language-and-vision models cannot (yet)? May 2023.
* [57] Safoora Yousefi, Leo Bethhauser, Hosein Hasanbeig, Akanksha Saran, Raphael Milliere, and Ida Momennejad. In-context learning in large language models: A neuroscience-inspired analysis of representations, 2023.