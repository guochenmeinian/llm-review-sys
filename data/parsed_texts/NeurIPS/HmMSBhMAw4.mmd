# Periodic agent-state based Q-learning for POMDPs

Amit Sinha

McGill University, Mila

Matthieu Geist

Cohere

Aditya Mahajan

McGill University, Mila

###### Abstract

The standard approach for Partially Observable Markov Decision Processes (POMDPs) is to convert them to a fully observed belief-state MDP. However, the belief state depends on the system model and is therefore not viable in reinforcement learning (RL) settings. A widely used alternative is to use an agent state, which is a model-free, recursively updateable function of the observation history. Examples include frame stacking and recurrent neural networks. Since the agent state is model-free, it is used to adapt standard RL algorithms to POMDPs. However, standard RL algorithms like Q-learning learn a stationary policy. Our main thesis that we illustrate via examples is that because the agent state does not satisfy the Markov property, non-stationary agent-state based policies can outperform stationary ones. To leverage this feature, we propose PASQL (periodic agent-state based Q-learning), which is a variant of agent-state-based Q-learning that learns periodic policies. By combining ideas from periodic Markov chains and stochastic approximation, we rigorously establish that PASQL converges to a cyclic limit and characterize the approximation error of the converged periodic policy. Finally, we present a numerical experiment to highlight the salient features of PASQL and demonstrate the benefit of learning periodic policies over stationary policies.

## 1 Introduction

Recent advances in reinforcement learning (RL) have successfully integrated algorithms with strong theoretical guarantees and deep learning to achieve significant successes [16, 17]. However, most RL theory is limited to models with perfect state observations [14, 15]. Despite this, there is substantial empirical evidence showing that RL algorithms perform well in partially observed settings [18, 19, 20, 1, 13, 21, 15, 16, 17, 18, 19, 22]. Recently, there has been a significant advances in the theoretical understanding of different RL algorithms for POMDPs [14, 13, 15, 16] but a complete understanding is still lacking.

**Planning in POMDPs.** When the system model is known, the standard approach [1, 16, 17] is to construct an equivalent MDP with the belief state (which is the posterior distribution of the environment state given the history of observations and actions at the agent) as the information state. The belief state is policy independent and has time-homogeneous dynamics, which enables the formulation of a belief-state based dynamic program (DP). There is a rich literature which leverages the structure of the resulting DP to propose efficient algorithms to solve POMDPs [16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. See [20] for a review. However, the belief state depends on the system model, so the belief-state based approach does not work for RL.

**RL in POMDPs.** An alternative approach for RL in POMDPs is to consider policies which depend on an _agent state_\(\{z_{t}\}_{t\geq 1}\), where \(Z_{t}\in\mathsf{Z}\), which is a recursively updateable compression of the history: the agent starts at an initial state \(z_{0}\) and recursively updates the agent state as some function of the current agent-state, next observation, and current action. A simple instance of agent-state is _frame stacking_, where a window of previous observations is used as state [14, 15, 16]. Another example is to use a recurrent neural network such as LSTM or GRU to compress the history of observations and actions into an agent state [18, 19, 20, 17, 16]. Infact, as argued in [15, 16] such an agent state is present in most deep RL algorithms for POMDPs. We refer to such a representation as an "agent state" because it captures the agent's internal state that it uses for decision making.

When the agent state is an information state, i.e., satisfies the Markov property, i.e., \(\mathds{P}(z_{t+1}|z_{1:t},a_{1:t})=\mathds{P}(z_{t+1}|z_{t},a_{t})\) and is sufficient for reward prediction, i.e., \(\mathds{E}[R_{t}|y_{1:t},a_{1:t}]=\mathds{E}[R_{t}|z_{t},a_{t}]\) (where \(y_{t}\) is the observation, \(a_{t}\) is the action, and \(R_{t}\) is the per-step reward), the optimal agent-state based policy can be obtained via a dynamic program (DP) [14]. An example of such an agent state is the belief state. But, in general, the agent state is not an information state. For example, frame stacking and RNN do not satisfy the Markov property, in general. It is also possible to have agent-states that satisfy the Markov property but are not sufficient for reward prediction (e.g., when the agent state is always a constant). In all such settings, the best agent-state policy cannot be obtained via a DP. Nonetheless, there has been considerable interest to use RL to find a good agent-state based policy.

One of the most commonly used RL algorithms is off-policy Q-learning, which we call agent-state Q-learning (ASQL). In ASQL for POMDPs, the Q-learning iteration is applied as if the agent state satisfied the Markov property even though it does not. The agent starts with an initial \(Q_{1}(z,a)\), acts according to a behavior policy \(\mu\), i.e., chooses \(a_{t}\sim\mu(z_{t})\), and recursively updates

\[Q_{t+1}(z,a)=Q_{t}(z,a)+\alpha_{t}(z,a)\Big{[}R_{t}+\gamma\max_{a^{\prime}\in \mathsf{A}}Q_{t}(z_{t+1},a^{\prime})-Q_{t}(z,a)\Big{]}\] (ASQL)

where \(\gamma\in[0,1)\) is the discount factor and the learning rates \(\{\alpha_{t}\}_{t\geq 1}\) are chosen such that \(\alpha_{t}(z,a)=0\) if \((z,a)\neq(z_{t},a_{t})\). The convergence of ASQL has been recently presented in [13, 14] which show that under some technical assumptions, ASQL converges to a limit. The policy determined by ASQL is the greedy policy w.r.t. this limit.

**Limitation of Q-learning with agent state.** The greedy policy determined by ASQL is stationary (i.e., uses the same control law at every time). In infinite horizon MDPs (and, therefore, also in POMDPs when using the belief state as an agent state), stationary policies perform as well as non-stationary policies. This is because the agent-state satisfies the Markov property. However, in ASQL the agent state generally does not satisfy the Markov property. Therefore, _restricting attention to stationary policies may lead to a loss of optimality!_

As an illustration, consider the POMDP shown in Fig. 1, which is described in detail in App. A.2 as Ex. 2. Suppose the system starts in state \(1\). Since the dynamics are deterministic, the agent can infer the current state from the history of past actions and can take the action to increment the current state and receive a per-step reward of \(+1\). Thus, the performance \(J^{\star}_{\text{BD}}\) of belief-state based policies is \(J^{\star}_{\text{BD}}=1/(1-\gamma)\). Contrast this with the performance \(J^{\star}_{\text{SD}}\) of deterministic agent-state base policies with agent state equal to current observation, which is given by \(J^{\star}_{\text{SD}}=(1+\gamma-\gamma^{2})/(1-\gamma^{3})<J^{\star}_{\text{BD}}\). In particular, for \(\gamma=0.9\), \(J^{\star}_{\text{BD}}=10\) which is larger than \(J^{\star}_{\text{SD}}=4.022\).

We show that the gap between \(J^{\star}_{\text{SD}}\) and \(J^{\star}_{\text{BD}}\) can be reduced by considering non-stationary policies. Ex. 2 has deterministic dynamics, so the optimal policy can be implemented in _open-loop_ via a sequence of control actions \(\{a^{\star}_{t}\}_{t\geq 1}\), where \(a^{\star}_{t}=\mathds{1}\{t\in\mathsf{D}_{1}\}\). This open-loop policy can be implemented via any information structure, including agent-state based policies. _Thus, a non-stationary deterministic agent-state based policy performs better than stationary deterministic agent-state based policies._ A similar conclusion also holds for models with stochastic dynamics.

**The main idea.** Arbitrary non-stationary policies cannot be used in RL because such policies have countably infinite number of parameters. In this paper, we consider a simple class of non-stationary

Figure 1: The cells indicate the state of the environment. Cells with the same background color have the same observation. The cells with a thick red boundary correspond to elements of the set \(\mathsf{D}_{0}\coloneqq\{n(n+1)/2+1:n\in\mathds{N}\}\), where the action \(0\) gives a reward of \(+1\) and moves the state to the right, while the action \(1\) gives a reward of \(-1\) and resets the state to \(1\). The cells with a thin black boundary correspond to elements of the set \(\mathsf{D}_{1}=\mathds{N}\setminus\mathsf{D}_{0}\), where the action \(1\) gives the reward of \(+1\) and moves the state to the right while the action \(0\) gives a reward of \(-1\) and resets the state to \(1\). Discount factor \(\gamma=0.9\).

policies with finite number of parameters: _periodic policies_. An agent-state based policy \(\pi=(\pi_{1},\pi_{2},\dots)\) is said to be periodic with period \(L\) if \(\pi_{t}=\pi_{t^{\prime}}\) whenever \(t\equiv t^{\prime}\pmod{L}\).

To highlight the salient feature of periodic policies, we perform a brute force search over all deterministic periodic policies of period \(L\), for \(L=\{1,\dots,10\}\), in 2. Let \(J^{\star}_{L}\) denote the optimal performance for policies of period \(L\). The result is shown below (see App. A.2 for details):

\begin{tabular}{l l l l l l l l l l} \hline \hline \(L\) & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \(J^{\star}_{L}\) & 4.022 & 4.022 & 7.479 & 6.184 & 8.810 & 7.479 & 9.340 & 8.488 & 9.607 & 8.810 \\ \hline \hline \end{tabular}

The above example highlights some salient features of periodic policies: (i) Periodic deterministic agent-state based policies may outperform stationary deterministic agent-state based policies. (ii) \(\{J^{\star}_{L}\}_{L\geq 1}\) is not a monotonically increasing sequence. This is because \(\Pi_{L}\), the set of all periodic deterministic agent-state based policies of period \(L\), is not monotonically increasing. (iii) If \(L\) divides \(M\), then \(J^{\star}_{L}\leq J^{\star}_{M}\). This is because \(\Pi_{L}\subseteq\Pi_{M}\). In other words, if we take any integer sequence \(\{L_{n}\}_{n\geq 1}\) that has the property that \(L_{n}\) divides \(L_{n+1}\), then the performance of the policies with period \(L_{n}\) is monotonically increasing in \(n\). For example, periodic policies with period \(L\in\{n!:n\in\mathds{N}\}\) will have monotonically increasing performance. (iv) In the above example, the set \(\mathsf{D}_{0}\) is chosen such that the optimal sequence of actions1 is not periodic. Therefore, even though periodic policies can achieve a performance that is arbitrarily close to the optimal belief-based policies, they are not necessarily globally optimal (in the class of non-stationary agent-state based policies). Thus, the periodic deterministic policy class is a middle ground between the stationary deterministic and non-stationary policy classes and provides us a simple way of leveraging the benefits of non-stationarity while trading-off computational and memory complexity.

Footnote 1: Recall that the system dynamics are deterministic, so optimal policy can be implemented in open loop.

The main contributions of this paper are as follows.

1. Motivated by the fact that non-stationary agent-state based policies outperform stationary ones, we propose a variant of agent-state based Q-learning (ASQL) that learns periodic policies. We call this algorithm periodic agent-state based Q-learning (PASQL).
2. We rigorously establish that PASQL converges to a cyclic limit. Therefore, the greedy policy w.r.t. the limit is a periodic policy. Due to the non-Markovian nature of the agent-state, the limit (of the Q-function and the greedy policy) depends on the behavioral policy used during learning.
3. We quantify the sub-optimality gap of the periodic policy learnt by PASQL.
4. We present numerical experiments to illustrate the convergence results, highlight the salient features of PASQL, and show that the periodic policy learned by PASQL indeed performs better than stationary policies learned by ASQL.

## 2 Periodic agent-state based Q-learning (PASQL) with agent state

### Model for POMDPs

A POMDP is a stochastic dynamical system with state \(s_{t}\in\mathsf{S}\), input \(a_{t}\in\mathsf{A}\), and output \(y_{t}\in\mathsf{Y}\), where we assume that all sets are finite. The system operates in discrete time with the dynamics given as follows: The initial state \(s_{1}\sim\rho\) and for any time \(t\in\mathds{N}\), we have

\[\mathds{P}(s_{t+1},y_{t+1}\mid s_{1:t},y_{1:t},a_{1:t})=\mathds{P}(s_{t+1},y _{t+1}\mid s_{t},a_{t})\eqqcolon P(s_{t+1},y_{t+1}\mid s_{t},a_{t})\]

where \(P\) is a probability transition matrix. In addition, at each time the system yields a reward \(R_{t}=r(s_{t},a_{t})\). We will assume that \(R_{t}\in[0,R_{\max}]\). The discount factor is denoted by \(\gamma\in[0,1)\).

Let \(\vec{\pi}=(\vec{\pi}_{1},\vec{\pi}_{2},\dots)\) denote any (history dependent and possibly randomized) policy. Then the action at time \(t\) is given by \(a_{t}\sim\vec{\pi}_{t}(y_{1:t},a_{1:t-1})\). The performance of policy \(\vec{\pi}\) is given by

\[J^{\vec{\pi}}\coloneqq\mathds{E}_{\begin{subarray}{c}a_{t}\sim\vec{\pi}_{t}( y_{1:t},a_{t-1})\\ (s_{t+1},y_{t+1})\sim P(s_{t},a_{t})\end{subarray}}\biggl{[}\sum_{t=1}^{ \infty}\gamma^{t-1}r(s_{t},a_{t})\biggm{|}s_{1}\sim\rho\biggr{]}.\]

The objective is to find a (history dependent and possibly randomized) policy \(\vec{\pi}\) to maximize \(J^{\vec{\pi}}\).

Agent state for Pomdps.An agent-state is model-free recursively updateable function of the history of observations and actions. In particular, let \(\mathsf{Z}\) denote agent-state space. Then, the agent state process \(\{z_{t}\}_{t\geq 0}\), \(z_{t}\in\mathsf{Z}\), starts with an initial value \(z_{0}\), and is then recursively computed as \(z_{t+1}=\phi(z_{t},y_{t+1},a_{t})\) for a pre-specified agent-state update function \(\phi\).

We use \(\pi=(\pi_{1},\pi_{2},\dots)\) to denote an agent-state based policy,2 i.e., a policy where the action at time \(t\) is given by \(a_{t}\sim\pi_{t}(z_{t})\). An agent-state based policy is said to be **stationary** if for all \(t\) and \(t^{\prime}\), we have \(\pi_{t}(a|z)=\pi_{t^{\prime}}(a|z)\) for all \((z,a)\in\mathsf{Z}\times\mathsf{A}\). An agent-state based policy is said to be **periodic** with period \(L\) if for all \(t\) and \(t^{\prime}\) such that \(t\equiv t^{\prime}\pmod{L}\), we have \(\pi_{t}(a|z)=\pi_{t^{\prime}}(a|z)\) for all \((z,a)\in\mathsf{Z}\times\mathsf{A}\).

Footnote 2: We use \(\vec{\pi}\) to denote history dependent policies and \(\pi\) to denote agent-state based policies.

### PASQL: Periodic agent-state based Q-learning algorithm for Pomdps

We now present a periodic variant of agent-state based Q-learning, which we abbreviate as PASQL. PASQL is an online, off-policy learning algorithm in which the agent acts according to a behavior policy \(\mu=(\mu_{1},\mu_{2},\dots)\) which is a periodic (stochastic) agent-state based policy \(\mu\) with period \(L\).

Let \([\![t]\!]\coloneqq(t\bmod L)\) and \(\mathsf{L}\coloneqq\{0,1,\dots,L-1\}\). Let \((z_{1},a_{1},R_{1},z_{2},a_{2},R_{2},\dots)\) be a sample path of agent-state, action, and reward observed by the agent. In PASQL, the agent maintains an \(L\)-tuple of Q-functions \((Q_{t}^{0},Q_{t}^{1},\dots,Q_{t}^{L-1})\), \(t\geq 1\). The \(\ell\)-th component, \(\ell\in\mathsf{L}\), is updated at time steps when \([\![t]\!]=\ell\). In particular, we can write the update as

\[Q_{t+1}^{\ell}(z,a)=Q_{t}^{\ell}(z,a)+\alpha_{t}^{\ell}(z,a)\Big{[}R_{t}+ \gamma\max_{a^{\prime}\in\mathsf{A}}Q_{t}^{[\ell+1]}(z_{t+1},a^{\prime})-Q_{t} ^{\ell}(z,a)\Big{]},\quad\forall\ell\in\mathsf{L},\] (PASQL)

where the learning rate sequence \(\{(\alpha_{t}^{0},\dots,\alpha_{t}^{L-1})\}_{t\geq 1}\) is chosen such that \(\alpha_{t}^{\ell}(z,a)=0\) if \((\ell,z,a)\neq([\![t]\!],z_{t},a_{t})\) and satisfies Assm. 1. PASQL differs from ASQL in two aspects: (i) The behavior policy \(\mu\) is periodic. (ii) The update of the Q-function is periodic. When \(L=1\), PASQL collapses to ASQL.

The standard convergence analysis of Q-learning for MDPs shows that the Q-function convergences to the unique solution of the MDP dynamic program (DP). The key challenge in characterizing the convergence of PASQL is that the agent state \(\{Z_{t}\}_{t\geq 1}\) does not satisfy the Markov property. Therefore, a DP to find the best agent-state based policy does not exist. So, we cannot use the standard analysis to characterize the convergence of PASQL. In Sec. 2.3, we provide a complete characterization of the convergence of PASQL.

The quality of the converged solution depends on the expressiveness of the agent state. For example, if the agent state is not expressive (e.g., agent state is always constant), then even if PASQL converges to a limit, the limit will be far from optimal. Therefore, it is important to quantify the degree of sub-optimality of the converged limit. We do so in Sec. 2.4.

### Establishing the convergence of tabular PASQL

To characterize the convergence of tabular PASQL, we impose two assumptions which are standard for analysis of RL algorithms [1, 1]. The first assumption is on the learning rates.

**Assumption 1**: For all \((\ell,z,a)\), the learning rates \(\{\alpha_{t}^{\ell}(z,a)\}_{t\geq 1}\) are measurable with respect to the sigma-algebra generated by \((z_{1:t},a_{1:t})\) and satisfy \(\alpha_{t}^{\ell}(z,a)=0\) if \((\ell,z,a)\neq([\![t]\!],z_{t},a_{t})\). Moreover, \(\sum_{t\geq 1}\alpha_{t}^{\ell}(z,a)=\infty\) and \(\sum_{t\geq 1}(\alpha_{t}^{\ell}(z,a))^{2}<\infty\), almost surely.

The second assumption is on the behavior policy \(\mu\). We first state an immediate property.

**Lemma 1**: _For any behavior policy \(\mu\), the process \(\{(S_{t},Z_{t})\}_{t\geq 1}\) is Markov. Therefore, the processes \(\{(S_{t},Z_{t},A_{t})\}_{t\geq 1}\) and \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t\geq 1}\) are also Markov._

**Assumption 2**: The behavior policy \(\mu\) is such that the Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t\geq 1}\) is time-periodic3 with period \(L\) and converges to a cyclic limiting distribution \((\zeta_{\mu}^{0},\dots,\zeta_{\mu}^{L-1})\), where \(\sum_{(s,y)}\zeta_{\mu}^{\ell}(s,y,z,a)>0\) for all \((\ell,z,a)\) (i.e., all \((\ell,z,a)\) are visited infinitely often).

[MISSING_PAGE_FAIL:5]

Many commonly used metrics on probability spaces are IPMs. For example, (i) Total variation distance for which \(\mathfrak{F}=\{\mathrm{span}(f)\leq 1\}\), where \(\mathrm{span}(f)=\max f-\min f\) is the span seminorm of \(f\). In this case, \(\rho_{\mathfrak{F}}(f)=\mathrm{span}(f)\). (ii) Wasserstein distance for which \(\mathfrak{F}=\{\mathrm{Lip}(f)\leq 1\}\), where \(\mathrm{Lip}(f)\) is the Lipschitz constant of \(f\). In this case, \(\rho_{\mathfrak{F}}(f)=\mathrm{Lip}(f)\). Other examples include Kantorovich metric, bounded Lipschitz metric, and maximum mean discrepancy. See [15, 16] for more details.

**Sub-optimality gap.** Let \(\mathsf{T}(t,\ell)\coloneqq\{\tau\geq t:[\![\tau]\!]=\ell\}\). Furthermore, for any \(\ell\in\mathsf{L}\) and \(t\), define

\[\varepsilon^{\ell}_{t} \coloneqq\sup_{\tau\in\mathsf{T}(t,\ell)}\sup_{h_{\tau},a_{\tau} }\Bigl{|}\mathsf{E}[R_{\tau}\mid h_{\tau},a_{\tau}]-\sum_{s\in\mathsf{S}}r(s,a _{\tau})\zeta^{\ell}_{\mu}(s\mid\sigma_{\tau}(h_{\tau}),a_{\tau})\Bigr{|},\] \[\delta^{\ell}_{t} \coloneqq\sup_{\tau\in\mathsf{T}(t,\ell)}\sup_{h_{\tau},a_{\tau} }d_{\mathfrak{F}}(\mathds{P}(Z_{\tau+1}=\cdot\mid h_{\tau},a_{\tau}),P^{\ell} _{\mu}(Z_{\tau+1}=\cdot\lvert\sigma_{\tau}(h_{\tau}),a_{\tau})).\]

Then, we have the following sub-optimality gap for \(\mathfrak{F}_{\mu}\).

**Theorem 2**: _Let \(V^{\ell}_{\mu}(z)\coloneqq\max_{a\in\mathsf{A}}Q^{\ell}_{\mu}(z,a)\). Then,_

\[\sup_{h_{\ell}}\bigl{[}V^{\star}_{t}(h_{t})-V^{\mathfrak{F}_{\mu}}_{t}(h_{t}) \bigr{]}\leq\frac{2}{(1-\gamma^{L})}\sum_{\ell\in\mathsf{L}}\gamma^{\ell} \Bigl{[}\varepsilon^{[t+\ell]}_{t+\ell}+\gamma\delta^{[t+\ell]}_{t+\ell}\rho_ {\mathfrak{F}}(V^{[t+\ell+1]}_{\mu})\Bigr{]}.\] (3)

See App. F for proof. The salient features of the sub-optimality gap of Thm. 2 are as follows.

* We can recover some existing results as special cases of Thm. 2. When we take \(L=1\), Thm. 2 recovers the sub-optimality gap for ASQL obtained in [23, Thm. 3]. In addition, when the agent state is a sliding window memory, Thm. 2 is similar to the sub-optimality gap obtained in [11, Thm. 4.1]. Note that the results of Thm. 2 for these special cases is more general because the previous results were derived under a restrictive assumption on the learning rates.
* The sub-optimality gap in Thm. 2 is on the sub-optimality w.r.t. the optimal _history-dependent_ policy rather than the optimal non-stationary agent-state policy. Thus, it inherently depends on the quality of the agent state. Consequently, even if \(L\to\infty\), the sub-optimality gap does not go to zero.
* It is not easy to characterize the sensitivity of the bound to the period \(L\). In particular, increasing \(L\) means changing behavioral policy \(\mu\), and therefore changing the converged limit \((\zeta^{0}_{\mu},\ldots,\zeta^{L-1}_{\mu})\), which impacts the right hand side of (3) in a complicated way. So, it is not necessarily the case that increasing \(L\) reduces the sub-optimality gap. This is not surprising, as we have seen earlier in Ex. 2 presented in the introduction that even the performance of periodic agent-state based policies is not monotone in \(L\).

## 3 Numerical experiments

In this section, we present a numerical example to highlight the salient features of our results. We use the following POMDP model.

**Example 1**: Consider a POMDP with \(\mathsf{S}=\{0,1,\ldots,5\}\), \(\mathsf{A}=\{0,1\}\), \(\mathsf{Y}=\{0,1\}\) and \(\gamma=0.9\). The dynamics are as shown in Fig. 2. The observation is \(0\) in states \(\{0,1,2\}\) which are shaded white and is \(1\) in states \(\{3,4,5\}\) which are shaded gray. The transitions shown in green give a reward of \(+1\); those in in blue give a reward of \(+0.5\); others give no reward.

We consider a family of models, denoted by \(\mathcal{M}(p)\), \(p\in[0,1]\), which are similar to Ex. 1 except the controlled state transition matrix is \(pI+(1-p)P\), where \(P\) is the controlled state transition matrix of Ex. 1 shown in Fig. 2. In the results reported below, we use \(p=0.01\). The hyperparameters for the experiments are provided in App. H.

**Convergence of PASQL with \(L=2\).** We assume that the agent state \(Z_{t}=Y_{t}\) and take period \(L=2\). We consider three behavioral policies: \(\mu_{k}=(\mu^{0}_{k},\mu^{1}_{k})\), \(k\in\mathsf{K}\coloneqq\{1,2,3\}\), where \(\mu^{\ell}_{k}\colon\{0,1\}\to\mathsf{K}\), and \(\mu^{\ell}_{k}\colon\{0,1\}\to\mathsf{K}\). The policy \(\mu^{\ell}_{k}\colon\{0,1\}\to\mathsf{K}\) is given by

\[\mu^{\ell}_{k}\colon\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\} \to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to \mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K }\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup \{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\} \to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\}\] (4)

where \(\mathsf{K}\) is the set of all possible states in \(\mathsf{K}\). The policy \(\mu^{\ell}_{k}\colon\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to \mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K }\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup \{0,1\}\to\mathsf{K}\cup\{0,1\}\to\mathsf{K}\cup\{0,1\}\to\(\Delta(\{0,1\})\), \(\ell\in\{0,1\}\). The policy \(\mu_{k}\) is completely characterized by four numbers which we write in matrix form as: \(\left[\mu_{k}^{0}(0|0),\mu_{k}^{1}(0|0);\mu_{k}^{0}(0|1),\mu_{k}^{1}(0|1)\right]\). With this notation, the three policies are given by \(\mu_{1}:=\left[0.2,0.8;0.8,0.2\right]\), \(\mu_{2}\coloneqq\left[0.5,0.5;0.5,0.5\right]\), \(\mu_{3}\coloneqq\left[0.8,0.2;0.2,0.8\right]\).

For each behavioral policy \(\mu_{k}\), \(k\in\mathsf{K}\), run PASQL for 25 random seeds. The median + interquantile range of the iterates \(\{Q_{t}^{t}(z,a)\}_{t\geq 1}\) as well as the theoretical limits \(Q_{\mu_{k}}(z,a)\) (computed using Thm. 1) are shown in Fig. 3. The salient features of these results are as follows:

* PASQL converges close to the theoretical limit predicted by Thm. 1.
* As highlighted earlier, the limiting value \(Q_{\mu_{k}}^{\ell}\) depends on the behavioral policy \(\mu_{k}\).
* When the aperiodic behavior policy \(\mu_{2}\) is used, the Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t\geq 1}\) is aperiodic, and therefore the limiting distribution \(\zeta_{\mu_{2}}^{\ell}\) and the corresponding Q-functions \(Q_{\mu_{2}}^{\ell}\) do not depend on \(\ell\). This highlights the fact that we have to choose a periodic behavioral policy to converge to a non-stationary policy (PASQL-policy).

**Comparison of converged policies.** Finally, we compute the periodic greedy policy \(\pi_{\mu_{k}}=(\pi_{\mu_{k}}^{0},\pi_{\mu_{k}}^{1})\) given by (PASQL-policy), \(k\in\mathsf{K}\), and compute its performance \(J^{\pi_{\mu_{k}}}\) via policy evaluation on the product space \(\mathsf{S}\times\mathsf{Z}\) (see App. G). We also do a brute force search over all \(L=2\) periodic deterministic agent-state policies to compute the optimal performance \(J_{2}^{\star}\) over all such policies. The results, displayed in Table 1, illustrate the following:

* The greedy policy \(\pi_{\mu_{k}}\) depends on the behavioral policy. This is not surprising given the fact that the limiting value \(Q_{\mu_{k}}^{\ell}\) depends on \(\mu_{k}\).
* The policy \(\pi_{\mu_{1}}\) achieves the optimal performance, whereas the policies \(\pi_{\mu_{2}}\) and \(\pi_{\mu_{3}}\) do not perform well. This highlights the importance of starting with a good behavioral policy. See Sec. 5 for a discussion on variants such as \(\epsilon\)-greedy.

**Advantage of learning periodic policies.** As stated in the introduction, the main motivation of PASQL is that it allows us to learn non-stationary policies. To see why this is useful, we run ASQL (which is effectively PASQL with \(L=1\)). We again consider three behavioral policies: \(\bar{\mu}_{k}\), \(k\in\mathsf{K}\coloneqq\{1,2,3\}\), where \(\bar{\mu}_{k}\colon\{0,1\}\to\Delta(\{0,1\})\), where (using similar notation as for \(L=2\) case) \(\bar{\mu}_{1}\coloneqq\left[0.2;0.8\right]\), \(\bar{\mu}_{2}\coloneqq\left[0.5;0.5\right]\), \(\bar{\mu}_{3}\coloneqq\left[0.8;0.2\right]\).

For each behavioral policy \(\bar{\mu}_{k}\), \(k\in\mathsf{K}\), run ASQL for 25 random seeds. The results are shown in App. A.1. The performance of the greedy policies \(\pi_{\bar{\mu}_{k}}\) and the performance of the best period

\begin{table}
\begin{tabular}{c c c c} \hline \(J_{2}^{\star}\) & \(J^{\pi_{\mu_{1}}}\) & \(J^{\pi_{\mu_{2}}}\) & \(J^{\pi_{\mu_{3}}}\) \\
6.793 & 6.793 & 1.064 & 0.532 \\ \hline \end{tabular}
\end{table}
Table 1: Performance of converged periodic policies.

Figure 3: PASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red).

\(L=1\) deterministic agent-state-based policy computed via brute force is shown in Table 2. The key implications are as follows:

* As was the case for PASQL, the greedy policy \(\pi_{\bar{\mu}_{k}}\) depends on the behavioral policy. As mentioned earlier, this is a fundamental consequence of the fact that the agent state is not an information state. Adding (or removing) periodicity does not change this feature.
* The best performance of ASQL is worse than the best performance of PASQL. This highlights the potential benefits of using periodicity. However, at the same time, if a bad behavioral policy is chosen (e.g., policy \(\mu_{3}\)), the performance of PASQL can be worse than that of ASQL for a nominal policy (e.g., policy \(\bar{\mu}_{2}\)). This highlights that periodicity is not a magic bullet and some care is needed to choose a good behavioral policy. Understanding what makes a good periodic behavioral policy is an unexplored area that needs investigation.

## 4 Related work

**Policy search for agent state policies.** There is a rich literature on planning with agent state-based policies that build on the policy evaluation formula presented in App. G. See [13] for review. These approaches rely on the system model and cannot be used in the RL setting.

**State abstractions for POMDPs** are related to agent-state based policies. Some frameworks for state abstractions in POMDPs include predictive state representations (PSR) [12, 13, 14, 15, 16], approximate bisimulation [17, 18], and approximate information states (AIS) [19] (which is used in our proof of Thm. 2). Although there are various RL algorithms based on such state abstractions, the key difference is that all these frameworks focus on stationary policies in the infinite horizon setting. Our key insight that non-stationary/periodic policies improve performance is also applicable to these frameworks.

**ASQL for POMDPs.** As stated earlier, ASQL may be viewed as the special case of PASQL when \(L=1\). The convergence of the simplest version of ASQL was established in [14] for \(Z_{t}=Y_{t}\) under the assumption that the actions are chosen i.i.d. (and do not depend on \(z_{t}\)). In [17] it was established that \(Q^{0}_{\mu}\) is the fixed point of (ASQL), but convergence of \(\{Q_{t}\}_{t\geq 1}\) to \(Q^{0}_{\mu}\) was not established. The convergence of ASQL when the agent state is a finite window memory was established in [14]. These results were generalized to general agent-state models in [19]. The regret of an optimistic variant of ASQL was presented in [16]. However, all of these papers focus on stationary policies.

Our analysis is similar to the analysis of [14, 19] with two key differences. First, their convergence results were derived under the assumption that the learning rates are the reciprocal of visitation counts. We relax this assumption to the standard learning rate conditions of Assm. 1 using ideas from stochastic approximation. Second, their analysis is restricted to stationary policies. We generalize the analysis to periodic policies using ideas from time-periodic Markov chains.

**Q-learning for non-Markovian environments.** As highlighted earlier, a key challenge in understanding the convergence of PASQL is that the agent-state is not Markovian. The same conceptual difficulty arises in the analysis of Q-learning for non-Markovian environments [15, 16, 17]. Consequently, our analysis has stylistic similarities with the analysis in [15, 16, 17] but the technical assumptions and the modeling details are different. And more importantly, they restrict attention to stationary policies. Given our results, it may be worthwhile to explore if periodic policies can help in non-Markovian environments as well.

**Continual learning and non-stationary MDPs.** Non-stationarity is an important consideration in continual learning (see [1] and references therein). However, in these settings, the environment is non-stationary. Our setting is different: the environment is stationary, but non-stationary policies help because the agent state is not Markov.

**Hierarchical learning.** The options framework [18, 19, 15, 14] is a hierarchical approach that learns temporal abstractions in MDPs and POMDPs. Due to temporal abstraction, the policy learned by the options framework is non-stationary. The same is true for other hierarchical learning approaches proposed in [13, 12, 18]. In principle, PASQL could be considered as a form of temporal abstraction where time is split into trajectories of length \(L\) and then a policy of length \(L\) is learned. However, the theoretical analysis for options is mostly restricted to MDP setting and the convergence guarantees for options in POMDPs are weaker [11, 12, 13]. Nonetheless, the algorithmic tools developed for options might be useful for PASQL as well.

**Double Q-learning.** The update equation of PASQL are structurally similar to the update equations used in double Q-learning [14, 15]. However, the motivation and settings are different: the motivation for Double Q-learning is to reduce overestimation bias in off-policy learning in MDPs, while the motivation for PASQL is to induce non-stationarity while learning in POMDPs. Therefore, the analysis of the two algorithms is very different. More importantly, the end goals differ: double Q-learning learns a stationary policy while PASQL learns a periodic policy.

**Use of non-stationary/periodic policies in MDPs** is investigated in [10, 11, 12] in the context of approximate dynamic programming (ADP). Their main result was to show that using non-stationary or periodic policies can improve the approximation error in ADP. Although these results use periodic policies, the setting of ADP in MDPs is very different from ours.

## 5 Discussion

**Deterministic vs. stochastic policies.** In this work, we restricted attention to periodic deterministic policies. In principle, we could have also considered periodic _stochastic_ policies. For stationary policies (i.e., when period is one), stochastic policies can outperform deterministic policies [10] as illustrated by Ex. 3 in App. A.3. However, we do not consider stochastic policies in this work because we are interested in understanding Q-learning with agent-state and Q-learning results in a deterministic policy. There are two options to obtain stochastic policies: using regularization [11], which changes the objective function; or using policy gradient algorithms [12, 13], which are a different class of algorithms than Q-learning.

However, as illustrated in the motivating Ex. 2 presented in the introduction, non-stationary policies can do better than stationary stochastic policies as well. So, adding non-stationarity via periodicity remains an interesting research direction when learning stochastic policies as well.

**PASQL is a special case of ASQL with state augmentation.** In principle, PASQL could be considered as a special case of ASQL with an augmented agent state \(\bar{Z}_{t}=(Z_{t},[\![t]\!])\). However, the convergence analysis of ASQL in [11, 12] does not imply the convergence of PASQL because the results of [11, 12] are derived under the assumption that Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t\geq 1}\) is irreducible and aperiodic, while we assume that the Markov chain is _periodic_. Due to our weaker assumption, we are able to establish convergence of PASQL to time-varying periodic policies.

**Non-stationary policies vs. memory augmentation.** Non-stationarity is a fundamentally different concept than memory augmentation. As an illustration, consider the T-shaped grid world (first considered in [1]) shown in Fig. 4, which has a corridor of length \(2n\). In App. A.4, we show that for this example, a stationary policy which uses a sliding window of past \(m\) observations and actions as the agent state needs a memory of at least \(m>2n\) to reach the goal state. In contrast, a periodic policy with period \(L=3\) can reach the goal state for every \(n\). This example shows that periodicity is a different concept from memory augmentation and highlights the fact that mechanisms other than memory augmentation can achieve optimal behavior.

The analysis of this paper is applicable to general memory augmented policies, so we do not need to choose between memory augmentation and periodicity. Our main message is that once the agent's memory is fixed based on practical considerations, adding periodicity could improve performance.

**Choice of the period \(L\).** If the agent state \(Z_{t}\) is a good approximation to the belief state, then ASQL (or, equivalently, PASQL with \(L=1\)) would converge to an approximately optimal policy. So, using PASQL a period \(L>1\) is useful when the agent state is not a good approximation of the belief state.

As shown by Ex. 2 in the introduction, the performance of the best periodic policy does not increase monotonically with the period \(L\). However, if we consider periods in the set \(\{n!:n\in\mathds{N}\}\), then the performance increases monotonically. However, PASQL does not necessarily converge to the best periodic policy. The quality of the converged policy (PASQL-policy) depends on the behavior

Figure 4: A T-shaped grid world. Agent starts at s, where it learns whether the goal state is \(\mathrm{G}_{1}\) or \(\mathrm{G}_{2}\). It has to go through the corridor \(\{1,\dots,2n\}\), without knowing where it is, reach \(\mathrm{T}\) and go up or down to reach the goal state.

policy \(\mu\). The difficulty of finding a good behavioral policy increases with \(L\). In addition, increasing the period increases the memory required to store the tuple \((Q^{0},\ldots,Q^{L})\) and the number of samples needed to converge (because each component is updated only once every \(L\) samples). Therefore, the choice of the period \(L\) should be treated as a hyperparameter that needs to be tuned.

**Choice of the behavioral policy.** The behavioral policy impacts the converged limit of PASQL, and consequently it impacts the periodic greedy policy that is learned. As we pointed out in the discussion after Thm. 1, this dependence is a fundamental consequence of using an agent state that is not Markov and cannot be avoided. Therefore, it is important to understand how to choose behavioral policies that lead to convergence to good policies.

**Generalization to other variants.** Our analysis is restricted to tabular off-policy Q-learning where a fixed behavioral policy is followed. Our proof fundamentally depends on the fact that the behavioral policy induces a cyclic limiting distribution on the periodic Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t\geq 1}\). Such a condition is not satisfied in variants such as \(\epsilon\)-greedy Q-learning and SARSA. Generalizing the technical proof to cover these more practical algorithms (including function approximation) is an important future direction.

## Acknowledgments

The work of AS and AM was supported in part by a grant from Google's Institutional Research Program in collaboration with Mila. The numerical experiments were enabled in part by support provided by Calcul Quebec and Compute Canada.

## References

* [Abe+24] David Abel, Andre Barreto, Benjamin Van Roy, Doina Precup, Hado P van Hasselt, and Satinder Singh. "A definition of continual reinforcement learning". In: _Advances in Neural Information Processing Systems_ 36 (2024).
* [Ast65] K.J Astrom. "Optimal Control of Markov Processes with Incomplete State Information". In: _Journal of Mathematical Analysis and Applications_ 10.1 (Feb. 1965), pp. 174-205. issn: 0022247X.
* [BHP17] Pierre-Luc Bacon, Jean Harb, and Doina Precup. "The option-critic architecture". In: _Proceedings of the AAAI conference on artificial intelligence_. Vol. 31. 1. 2017.
* [Bak01] Bram Bakker. "Reinforcement Learning with Long Short-Term Memory". In: _Advances in Neural Information Processing Systems_. Ed. by T. Dietterich, S. Becker, and Z. Ghahramani. Vol. 14. MIT Press, 2001. url: https://proceedings.neurips.cc/paper_files/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf.
* [BB01] Jonathan Baxter and Peter L Bartlett. "Infinite-horizon policy-gradient estimation". In: _Journal of Artificial Intelligence Research_ 15 (2001), pp. 319-350.
* [BMP12] Albert Benveniste, Michel Metivier, and Pierre Priouret. _Adaptive algorithms and stochastic approximations_. Vol. 22. Springer Science & Business Media, 2012.
* [BT96] Dimitri Bertsekas and John N Tsitsiklis. _Neuro-dynamic programming_. Athena Scientific, 1996.
* [Ber13] Dimitri P Bertsekas. _Abstract Dynamic Programming_. Athena Scientific, 2013.
* [BP24] Shalabh Bhatnagar and L.A. Prashanth. Personal communication. 2024.
* [BSG11] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. "Closing the learning-planning loop with predictive state representations". In: _The International Journal of Robotics Research_ 30.7 (2011), pp. 954-966.
* [Bor08] Vivek S Borkar. _Stochastic approximation: a dynamical systems viewpoint_. Hindustan Book Agency, 2008.
* [CLZ97] Anthony Cassandra, Michael L. Littman, and Nevin L. Zhang. "Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes". In: _Uncertainty in Artificial Intelligence_. 1997.
* [CKL94] Anthony R Cassandra, Leslie Pack Kaelbling, and Michael L Littman. "Acting optimally in partially observable stochastic domains". In: _AAAI Conference on Artificial Intelligence_. Vol. 94. 1994, pp. 1023-1028.
* [Cas98] Anthony Rocco Cassandra. "Exact and approximate algorithms for partially observable Markov decision processes". PhD thesis. Brown University, 1998.
* [Cas+21] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. "Mico: Improved representations via sampling-based state similarity for Markov decision processes". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 30113-30126.
* [CPP09] Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. "Equivalence Relations in Fully and Partially Observable Markov Decision Processes". In: _International Joint Conference on Artificial Intelligence_. 2009, pp. 1653-1658.
* [Cha+24] Siddharth Chandak, Pratik Shah, Vivek S Borkar, and Parth Dodhia. "Reinforcement learning in non-Markovian environments". In: _Systems & Control Letters_ 185 (2024), p. 105751.
* [CSL21] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. "Goal-conditioned reinforcement learning with imagined subgoals". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 1430-1440.
* [Cha07] Joseph Chang. _Stochastic Processes_. Unpublished. Available at http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf. 2007.
* [DY24] Ali Devran Kera and Serdar Yuksel. "Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments". In: _Transactions on Machine Learning Research_ (2024).

* [Die00] Thomas G Dietterich. "Hierarchical reinforcement learning with the MAXQ value function decomposition". In: _Journal of Artificial Intelligence Research_ 13 (2000), pp. 227-303.
* [DRZ22] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. "Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States". In: _Journal of Machine Learning Research_ 23.255 (2022), pp. 1-54.
* [DVZ22] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. "Simple agent, complex environment: Efficient reinforcement learning with agent states". In: _Journal of Machine Learning Research_ 23.255 (2022), pp. 1-54.
* [Dur19] Rick Durrett. _Probability: Theory and Examples_. Cambridge University Press, Apr. 2019. isbn: 9781108473682. doi: 10.1017/9781108591034.
* [GSP19] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. "A theory of regularized markov decision processes". In: _International Conference on Machine Learning_. PMLR. 2019, pp. 2160-2169.
* [Gru+18] Audrunas Gruslys, Remi Munos, Ivo Danihelka, Marc G. Bellemare, and Alex Graves. "The Reactor: A Sample-Efficient Actor-Critic Architecture". In: _Proceedings of the International Conference on Learning Representations (ICLR)_. 2018. url: https://arxiv.org/abs/1704.04651.
* [Haf+20] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. "Dream to Control: Learning Behaviors by Latent Imagination". In: _International Conference on Learning Representations_. 2020.
* [Haf+21] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. "Mastering Atari with Discrete World Models". In: _International Conference on Learning Representations_. 2021.
* [HFP14] William Hamilton, Mahdi Milani Fard, and Joelle Pineau. "Efficient learning and planning with compressed predictive states". In: _The Journal of Machine Learning Research_ 15.1 (2014), pp. 3395-3439.
* [Han98] Eric A. Hansen. "Solving POMDPs by searching in policy space". In: _Uncertainty in Artificial Intelligence_. Madison, Wisconsin, 1998, pp. 211-219. isbn: 155860555X.
* [Has10] Hado Hasselt. "Double Q-learning". In: _Advances in neural information processing systems_ 23 (2010).
* [HS15] Matthew Hausknecht and Peter Stone. "Deep recurrent Q-learning for partially observable MDPs". In: _AAAI Fall Symposium Series_. 2015.
* [Hau97] Milos Hausknecht. "Planning and control in stochastic domains with imperfect information". PhD thesis. Massachusetts Institute of Technology, 1997.
* [Hau00] Milos Hausknecht. "Value-function approximations for partially observable Markov decision processes". In: _Journal of artificial intelligence research_ 13 (2000), pp. 33-94.
* [JSJ94] Tommi Jaakkola, Satinder Singh, and Michael Jordan. "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems". In: _Advances in Neural Information Processing Systems_. Vol. 7. MIT Press, 1994, pp. 345-352.
* [JKS16] Nan Jiang, Alex Kulesza, and Satinder P Singh. "Improving Predictive State Representations via Gradient Descent." In: _AAAI Conference on Artificial Intelligence_. 2016, pp. 1709-1715.
* [Kap+19] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. "Recurrent Experience Replay in Distributed Reinforcement Learning". In: _International Conference on Learning Representations_. 2019.
* [KY22] Ali Devran Kara and Serdar Yuksel. "Convergence of Finite Memory Q Learning for POMDPs and Near Optimality of Learned Policies Under Filter Stability". In: _Mathematics of Operations Research_ (Nov. 2022). issn: 1526-5471. doi: 10.1287/moor.2022.1331.
* [KWW22] Mykel J Kochenderfer, Tim A Wheeler, and Kyle H Wray. _Algorithms for decision making_. MIT press, 2022.
* [KJS15a] Alex Kulesza, Nan Jiang, and Satinder Singh. "Low-rank spectral learning with weighted loss functions". In: _Artificial Intelligence and Statistics_. 2015, pp. 517-525.

* [KJS15b] Alex Kulesza, Nan Jiang, and Satinder P Singh. "Spectral Learning of Predictive State Representations with Insufficient Statistics." In: _AAAI Conference on Artificial Intelligence_. 2015, pp. 2715-2721.
* [KY97] Harold J. Kushner and G. George Yin. _Stochastic Approximation Algorithms and Applications_. Springer New York, 1997. doi: 10.1007/978-1-4899-2696-8.
* [LVC18] Tuyen P Le, Ngo Anh Vien, and TaeChoong Chung. "A deep hierarchical reinforcement learning algorithm in partially observable Markov decision processes". In: _Ieee Access_ 6 (2018), pp. 49089-49102.
* [LS15] Boris Lesner and Bruno Scherrer. "Non-Stationary Approximate Modified Policy Iteration". In: _International Conference on Machine Learning_. Vol. 37. Proceedings of Machine Learning Research. Lille, France: PMLR, July 2015, pp. 1567-1575.
* [Lit96] Michael Lederman Littman. "Algorithms for sequential decision-making". PhD thesis. Brown University, 1996.
* [Lu+23] Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen, et al. "Reinforcement learning, bit by bit". In: _Foundations and Trends in Machine Learning_ 16.6 (2023), pp. 733-865.
* [MH+18] Sultan Javed Majeed, Marcus Hutter, et al. "On Q-learning Convergence for Non-Markov Decision Processes." In: _IJCAI_. Vol. 18. 2018, pp. 2546-2552.
* [Mni+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. "Playing atari with deep reinforcement learning". In: _arXiv preprint arXiv:1312.5602_ (2013).
* [Mul97] Alfred Muller. "Integral probability metrics and their generating classes of functions". In: _Advances in Applied Probability_ 29.2 (1997), pp. 429-443.
* [Nor98] James R Norris. _Markov chains_. Cambridge University Press, 1998.
* [PP02] Theodore J. Perkins and Mark D. Pendrith. "On the Existence of Fixed Points for Q-Learning and Sarsa in Partially Observable Domains". In: _International Conference on Machine Learning_. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2002, pp. 490-497.
* [PGT+03] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. "Point-based value iteration: An anytime algorithm for POMDPs". In: _International Joint Conference on Artificial Intelligence_. Vol. 3. 2003, pp. 1025-1032.
* [Pla77] Loren Kerry Platzman. "Finite Memory Estimation and Control of Finite Probabilistic Systems." PhD thesis. Massachusetts Institute of Technology, 1977.
* [PB24] L.A. Prashanth and Shalabh Bhatnagar. _Gradient-based algorithms for zeroth-order optimization_. Now publishers, 2024. url: http://www.cse.iitm.ac.in/~prashla/bookstuff/GBSO_book.pdf.
* [Pre00] Doina Precup. _Temporal abstraction in reinforcement learning_. University of Massachusetts Amherst, 2000.
* [Qia+18] Zhiqian Qiao, Katharina Muelling, John Dolan, Praveen Palanisamy, and Priyantha Mudalige. "Pomdp and hierarchical options mdp with continuous actions for autonomous driving at intersections". In: _2018 21st International Conference on Intelligent Transportation Systems (ITSC)_. IEEE. 2018, pp. 2377-2382.
* [Rii65] Jens Ove Riis. "Discounted Markov Programming in a Periodic Process". In: _Operations Research_ 13.6 (Dec. 1965), pp. 920-929. issn: 1526-5463. doi: 10.1287/opre.13.6.920.
* [RM51] Herbert Robbins and Sutton Monro. "A Stochastic Approximation Method". In: _The Annals of Mathematical Statistics_ 22.3 (1951), pp. 400-407.
* [RGT04] Matthew Rosencrantz, Geoff Gordon, and Sebastian Thrun. "Learning low dimensional predictive representations". In: _International Conference on Machine Learning_. 2004.
* [Sch16] Bruno Scherrer. _On Periodic Markov Decision Processes_. European Workshop on Reinforcement Learning. Dec. 2016. url: https://ewrl.files.wordpress.com/2016/12/scherrer.pdf.
* [SL12] Bruno Scherrer and Boris Lesner. "On the use of non-stationary policies for stationary infinite-horizon Markov decision processes". In: _Advances in Neural Information Processing Systems_ 25 (2012).

* [Sey+23] Erfan SeyedSalehi, Nima Akbarzadeh, Amit Sinha, and Aditya Mahajan. "Approximate information state based convergence analysis of recurrent Q-learning". In: _European conference on reinforcement learning_. 2023. url: https://arxiv.org/abs/2306.05991.
* [Sil+16] David Silver et al. "Mastering the game of Go with deep neural networks and tree search". In: _Nature_ 529 (2016), pp. 484-489.
* [SJJ94] Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. "Learning without state-estimation in partially observable Markovian decision processes". In: _Machine Learning_. Elsevier, 1994, pp. 284-292.
* [SS73] Richard D. Smallwood and Edward J. Sondik. "The Optimal Control of Partially Observable Markov Processes over a Finite Horizon". In: _Operations Research_ 21.5 (Oct. 1973), pp. 1071-1088. doi: 10.1287/opre.21.5.1071.
* [SS04] Trey Smith and Reid Simmons. "Heuristic search value iteration for POMDPs". In: _Conference on Uncertainty in Artificial Intelligence_. Banff, Canada, 2004, pp. 520-527.
* [SV05] Matthijs TJ Spaan and Nikos Vlassis. "Perseus: Randomized point-based value iteration for POMDPs". In: _Journal of Artificial Intelligence Research_ 24 (2005), pp. 195-220.
* [Ste+18] Denis Steckelmacher, Diederik Roijers, Anna Harutyunyan, Peter Vrancx, Helene Plisnier, and Ann Nowe. "Reinforcement learning in POMDPs with memoryless options and option-observation initiation sets". In: _Proceedings of the AAAI conference on artificial intelligence_. Vol. 32. 1. 2018.
* [Sub+22] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. "Approximate information state for approximate planning and reinforcement learning in partially observed systems". In: _Journal of Machine Learning Research_ 23.12 (2022), pp. 1-83.
* [Sut+99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. "Policy gradient methods for reinforcement learning with function approximation". In: _Advances in Neural Information Processing Systems_. Vol. 12. 1999.
* [SPS99] Richard S Sutton, Doina Precup, and Satinder Singh. "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning". In: _Artificial intelligence_ 112.1-2 (1999), pp. 181-211.
* [SB08] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2008.
* [VGS16] Hado Van Hasselt, Arthur Guez, and David Silver. "Deep reinforcement learning with double q-learning". In: _Proceedings of the AAAI conference on artificial intelligence_. Vol. 30. 1. 2016.
* [Vez+17] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. "Feudal networks for hierarchical reinforcement learning". In: _International conference on machine learning_. PMLR. 2017, pp. 3540-3549.
* [WS94] Chelsea C White III and William T Scherer. "Finite-memory suboptimal design for partially observed Markov decision processes". In: _Operations Research_ 42.3 (1994), pp. 439-455.
* [WS97] Marco Wiering and Jurgen Schmidhuber. "HQ-learning". In: _Adaptive behavior_ 6.2 (1997), pp. 219-246.
* [Wie+07] Daan Wierstra, Alexander Foerster, Jan Peters, and Juergen Schmidhuber. "Solving deep memory POMDPs with recurrent policy gradients". In: _International Conference on Artificial Neural Networks (ICANN)_. Springer. 2007, pp. 697-706.
* [Wie+10] Daan Wierstra, Alexander Forster, Jan Peters, and Jurgen Schmidhuber. "Recurrent policy gradients". In: _Logic Journal of the IGPL_ 18.5 (2010), pp. 620-634.
* [Wit75] Hans S Witsenhausen. "On policy independence of conditional expectations". In: _Information and Control_ 28.1 (1975), pp. 65-75.
* [Zha09] H. Zhang. "Partially Observable Markov Decision Processes: A Geometric Technique and Analysis". In: _Operations Research_ (2009).

###### Contents of Appendix

* A Illustrative examples
* A.1 Ex. 1: Learning curves for ASQL
* A.2 Ex. 2: non-stationary policies can outperform stationary policies
* A.3 Ex. 3: stochastic policies can outperform deterministic policies
* A.4 Ex. 4: conceptual difference between state-augmentation and periodic policies
* B Periodic Markov chains
* B.1 Time-homogeneous Markov chains and their properties
* B.2 Time-varying with periodic transition matrix
* B.3 Constructing an equivalent time-homogeneous Markov chain
* B.4 Limiting behavior of periodic Markov chain
* C Periodic Markov decision processes
* D Stochastic Approximation with Markov noise
* E Thm. 1: Convergence of periodic Q-learning
* E.1 Step 1: State splitting of the error function
* E.2 Step 2: Convergence of component \(X_{t}^{\ell,0}\)
* E.3 Step 3: Convergence of component \(X_{t}^{\ell,1}\)
* E.4 Step 4: Convergence of component \(X_{t}^{\ell,2}\)
* E.5 Putting everything together
* F Thm. 2: Sub-optimality gap
* G Policy evaluation of an agent-state based policy
* H Reproducibility information

## Appendix A Illustrative examples

### Ex. 1: Learning curves for ASQL

For each behavioral policy \(\bar{\mu}_{k}\), \(k\in\mathsf{K}\), we run PASQL for 25 random seeds. The median + interquantile range of the iterates \(\{Q_{t}(z,a)\}_{t\geq 1}\) as well as the theoretical limits \(Q_{\bar{\mu}_{k}}(z,a)\) (computed as per Thm. 1 for \(L=1\)) are shown in Fig. 5. These curves show that the result of Thm. 1 is valid for the stationary case (\(L=1\)) as well.

### Ex. 2: non-stationary policies can outperform stationary policies

**Example 2**: Consider a POMDP with \(\mathsf{S}=\mathds{Z}_{>0}\), \(\mathsf{A}=\{0,1\}\), and \(\mathsf{Y}=\{0,1\}\). The system starts in an initial state \(s_{1}=1\) and has deterministic dynamics. To describe the dynamics and the reward function, we define \(\mathsf{D}_{0}\coloneqq\{n(n+1)/2+1:n\in\mathds{Z}_{\geq 0}\}\), \(\mathsf{D}_{1}=\mathds{N}\backslash\mathsf{D}_{0}\), and \(\mathsf{D}=\mathsf{D}_{0}\times\{0\}\cup\mathsf{D}_{1}\times\{1\}\subset \mathsf{S}\times\mathsf{A}\). Then, the dynamics, observations, and rewards are given by

\[s_{t+1}=\begin{cases}s_{t}+1,&(s_{t},a_{t})\in\mathsf{D},\\ 1,&\text{otherwise},\end{cases}\quad y_{t}=\begin{cases}0,&s_{t}\text{ is odd,}\\ 1,&s_{t}\text{ is even},\end{cases}\quad r(s,a)=\begin{cases}+1,&(s,a)\in \mathsf{D},\\ -1&\text{otherwise}.\end{cases}\]

Thus, the state is incremented if the agent takes action \(0\) when the state is in \(\mathsf{D}_{0}\) and takes action \(1\) when the state is in \(\mathsf{D}_{1}\). Taking these actions yield a reward of \(+1\). Not taking such an action results in a reward of \(-1\) and resets the state to \(1\). The agent does not observe the state, but only observes whether the state is odd or even. A graphical representation of the model is shown in Fig. 1.

**For policy class \(\Pi_{\mathsf{BD}}\) (the class of all belief-based deterministic policies),** since the system starts in a known initial state and the dynamics are deterministic, the agent can compute the current state

Figure 5: ASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red).

(thus, the belief is a delta function on the current state). Thus, the agent can always choose the correct action depending on whether the state is in \(\mathsf{D}_{0}\) and \(\mathsf{D}_{1}\). Hence \(J^{*}_{\text{\tiny BD}}=1/(1-\gamma)\), which is the highest possible reward.

**For policy class \(\Pi_{\text{\tiny SD}}\) (the class of all agent-state based deterministic policies),** there are four possible deterministic policies. For odd observations, the agent may take action \(0\) and \(1\). Similarly, for even observations, the agent may take action \(0\) or \(1\). Note that the system starts in state \(1\), which is in \(\mathsf{D}_{0}\). Therefore, if the agent chooses action \(1\) when the observation is odd, it receives a reward of \(-1\) and stays at state \(1\). Therefore, the discounted total reward is \(-1/(1-\gamma)\), which is the least possible value. Therefore, any policy that chooses \(1\) on odd observations cannot be optimal. Therefore, the optimal (deterministic) action on odd observations is to pick action \(0\). Thus, there are two policies that we need to evaluate.

* If the agent chooses action \(0\) at both odd and even observations, the state cycles between \(1\to 2\to 3\to 1\to 2\to 3\cdots\) with the reward sequence \((+1,+1,-1,+1,+1,-1,\dots)\). Thus, the cumulative total reward of this policy is \((1+\gamma-\gamma^{2})/(1-\gamma^{3})\).
* If the agent chooses action \(0\) at odd observations and action \(1\) at even observations, the state cycles between \(1\to 2\to 1\to 2\cdots\) with the reward sequence \((+1,-1,+1,-1,\dots)\). Thus, the cumulative total reward of this policy is \(1/(1+\gamma)\).

It is easy to verify that for \(\gamma\in(0,1)\), \(1/(1+\gamma)<(1+\gamma-\gamma^{2})/(1-\gamma^{3})\). Thus,

\[J^{*}_{\text{\tiny SD}}=\frac{1+\gamma-\gamma^{2}}{1-\gamma^{3}}.\]

We also consider **policy class \(\Pi_{\text{\tiny SS}}\): the class of all stationary stochastic agent-state based policies.** For policy class \(\Pi_{\text{\tiny SS}}\), the policy is characterized by two numbers \((p_{0},p_{1})\in[0,1]^{2}\), where \(p_{y}\) denotes the probability of choosing action \(1\) when the observation is \(y\), \(y\in\{0,1\}\). We compute the approximately optimal policy by doing a brute force search over \((p_{0},p_{1})\) by discretizing them two decimal places and for each choice, running a Monte Carlo simulation of length \(1,000\) and averaging it over \(100\) random seeds. We find that there is negligible difference between the performance of stochastic and deterministic policies.

Finally, we consider **policy class \(\Pi_{L}\)**, which is the class of periodic deterministic agent-state based policies. A policy \(\pi\in\Pi_{L}\) is characterized by two vectors \(p_{0},p_{1}\in\{0,1\}^{L}\), where \(p_{y,\ell}\) denotes the action chosen when \(t\bmod L=\ell\) and the observation is \(y\). We do an exhaustive search over all deterministic policies of length \(L\), \(L\in\{1,\dots,10\}\) to compute the numbers shown in the main text.

### Ex. 3: stochastic policies can outperform deterministic policies

When the agent state is not an information state, the optimal stochastic stationary policy will perform better than (or equal to) the optimal deterministic stationary policy as observed in [10]. Here is an example to illustrate this for a simple toy POMDP.

**Example 3**: Consider a POMDP with \(\mathsf{S}=\{0,1,2\}\), \(\mathsf{A}=\{0,1\}\) and \(\mathsf{Y}=\{0\}\). The system starts at an initial state \(s_{1}=0\) and the dynamics under the two actions are shown in Fig. 6. The agent does not observe the state, i.e., \(Y_{t}\equiv 0\). The rewards under action \(0\) are \(r(\cdot,0)=[-1,0,2]\) and the rewards under action \(1\) are \(r(s,1)=-0.5\), for all \(s\in\mathsf{S}\).

Figure 6: The dynamics for Ex. 3.

We consider agent state \(Z_{t}=Y_{t}\). Let \(\Pi_{\textsc{S}{\textsc{S}}}\) denote the of all stationary stochastic policies and \(\Pi_{\textsc{S}{\textsc{D}}}\) denote the class of of all stationary deterministic policicic A policy \(\pi\in\Pi_{\textsc{S}{\textsc{S}}}\) is parameterized by a single parameter \(p\in[0,1]\), which indicates the probability of choosing action \(1\). We denote such a policy by \(\pi_{p}\). Note that \(p\in\{0,1\}\), \(\pi_{p}\in\Pi_{\textsc{S}{\textsc{D}}}\). Let \((P_{a},r_{a})\) denote the probability transition matrix and reward function when \(a\in\mathsf{A}\) is chosen and let \((P_{p},r_{p})=(1-p)(P_{0},r_{0})+p(P_{1},r_{1})\). Then, the performance of policy \(\pi_{p}\) is given by \(J^{\pi_{p}}=[(1-\gamma P_{p})^{-1}r_{p}]_{0}\). The performance for all \(p\in[0,1]\) for \(\gamma=0.9\) is shown in Fig. 7, which shows that the best performance is achieved by the stochastic policy \(\pi_{p}\) with \(p\approx 0.39\).

Thus, stochastic policies can outperform deterministic policies.

### Ex. 4: conceptual difference between state-augmentation and periodic policies

**Example 4**: Consider a T-shaped grid world showed in Fig. 4 with state space \(\mathsf{P}\times\mathsf{G}\), where \(\mathsf{P}=\{\mathsf{s},1,2,\ldots,2n,\mathsf{T}\}\) is the position of the agent and \(\mathsf{G}=\{\textsc{G}_{1},\textsc{G}_{2}\}\) is the location of the goal. The observation space is \(\mathsf{Y}=\{0,1,2,3\}\). The observation is a deterministic function of the state and is given as follows:

* At state \((\textsc{s},\textsc{G}_{i})\), \(i\in\{1,2\}\), the observation is \(i\) and reveals the location of the goal state to the agent.
* At states \(\{1,\ldots,2n\}\times\mathsf{G}\), the observation is \(0\), so the agent cannot distinguish between these states.
* At states \(\{\mathsf{T}\}\times\mathsf{G}\), the observation is \(3\), so the agent knows when it reaches the \(\mathsf{T}\) state.

The action space depends on the current state: actions \(\{\textsc{left},\textsc{right},\textsc{stay}\}\) are available when the agent is at \(\{s,1,\ldots,2n\}\) and actions \(\{\textsc{up},\textsc{down}\}\) are available at position \(\mathsf{T}\).

The agent receives a reward of \(+1\) if it reaching the goal state and \(-1\) if it reaches the wrong goal state (i.e., reaches \(\textsc{G}_{2}\) when the goal state is \(\textsc{G}_{1}\)). The discount factor \(\gamma=1\).

We consider two classes of policies:

1. [label=()]
2. \(\Pi_{\textsc{S}{\textsc{D}}}(m)\): Stationary policies with agent state equal to a sliding window of the last \(m\) observations and actions.
3. \(\Pi_{L}\): Periodic policies with agent state equal to the last observation and periodic \(L\).

It is easy to see that as long as the window length \(m\leq 2n\), any policy in \(\Pi_{\textsc{S}{\textsc{D}}}(m)\) yields an average return of \(0\); for window lengths \(m>2n\), the agent can remember the first observation, and therefore it is possible to construct a policy that yields a return of \(+1\).

We now consider a deterministic periodic policy with period \(L=3\) given as follows:6\(\pi=(\pi^{0},\pi^{1},\pi^{2})\) where \(\pi^{\ell}\colon\mathsf{Y}\to\mathsf{A}\). We denote each \(\pi^{\ell}\) as a column vector, where the \(y\)-th component indicates the action \(\pi^{\ell}(y)\), where - means that the choice of the action for that observation is

Figure 4: A T-shaped grid world for Ex. 4. In state s, the agent learns about the goal state. In states \(\{1,2,\ldots,2n\}\), the agent simply knows that it is in the gray corridor, but does not know which cell it is in. In state \(\mathsf{T}\), it knows that it has reached the end of corridor and must decide whether to go up or down. The agent gets a reward of \(+1\) for reaching the correct goal state and a reward of \(-1\) for reaching the wrong goal state.

Figure 7: Performance of stationary stochastic policies \(\pi_{p}\) for \(p\in[0,1]\) for Ex. 3.

irrelevant for performance. The policy is given by

\[\pi^{0}=\begin{bmatrix}\texttt{RIGHT}\\ \texttt{RIGHT}\\ \texttt{stay}\\ \end{bmatrix},\quad\pi^{1}=\begin{bmatrix}\texttt{RIGHT}\\ -\\ \texttt{RIGHT}\\ \texttt{up}\\ \end{bmatrix},\quad\pi^{2}=\begin{bmatrix}\texttt{stay}\\ -\\ -\\ -\\ \texttt{down}\\ \end{bmatrix}.\]

It is easy to verify if the system starts in state \((0,\mathrm{G}_{1})\), then by following policy \((\pi^{0},\pi^{1},\pi^{2})\), the agent reaches state \(\mathrm{G}_{1}\) at time \(3n+3\). Moreover, when the system starts in state \((0,\mathrm{G}_{2})\), then by following the policy \((\pi^{0},\pi^{1},\pi^{2})\), the agent reaches \(\mathrm{G}_{2}\) at time \(3n+4\). Thus, in both cases, the policy \((\pi^{0},\pi^{1},\pi^{2})\) yields the maximum reward of \(+1\).

## Appendix B Periodic Markov chains

In most of the standard reference material on Markov chains, it is assumed that the Markov chain is aperiodic and irreducible. In our analysis, we need to work with periodic Markov chains. In this appendix, we review some of the basic properties of Markov chains and then derive some fundamental results for periodic Markov chains.

Let \(\mathsf{S}\) be a finite set. A stochastic process \(\{S_{t}\}_{t\geq 0}\), \(S_{t}\in\mathsf{S}\), is called a **Markov chain** if it satisfies the _Markov property_: for any \(t\in\mathds{Z}_{\geq 0}\) and \(s_{1:t+1}\in\mathsf{S}^{t+1}\), we have

\[\mathds{P}(S_{t+1}=s_{t+1}\mid S_{1:t}=s_{1:t})=\mathds{P}(S_{t+1}=s_{t+1} \mid S_{t}=s_{t}).\] (4)

If is often convenient to assume that \(\mathsf{S}=\{1,\dots,n\}\). We can define an \(n\times n\) transition probability matrix \(P_{t}\) given by \([P_{t}]_{ij}=\mathds{P}(S_{t+1}=j\mid S_{t}=i)\). Then, all the probabilistic properties of the Markov chain is described by the transition matrices \((P_{0},P_{1},\dots)\).

In particular, suppose the Markov chain starts at the initial PMF (probability mass function) \(\xi_{0}\) and let \(\xi_{t}\) denote the PMF at time \(t\). We will view \(\xi_{t}\) as a \(n\)-dimensional row vector. Then, Eq. (4) implies \(\xi_{t+1}=\xi_{t}P_{t}\) and, therefore,

\[\xi_{t+1}=\xi_{0}P_{0}P_{1}\cdots P_{t}.\]

### Time-homogeneous Markov chains and their properties

A Markov chain is said to be **time-homogeneous** if the transition matrix \(P_{t}\) is the same for all time \(t\). In this section, we state some standard results for time-homogeneous Markov chains [10].

#### b.1.1 Classification of states

The states of a time-homogeneous Markov chain can be classified as follows.

1. We say that a state \(j\) is **accessible from \(i\)** (abbreviated as \(i\rightsquigarrow j\)) if there is exists an \(m\in\mathds{Z}_{\geq 0}\) (which may depend on \(i\) and \(j\)) such that \([P^{m}]_{ij}>0\). The fact that \([P^{m}]_{ij}>0\) implies that there exists an ordered sequence of states \((i_{0},\dots,i_{m})\) such that \(i_{0}=i\) and \(i_{m}=j\) such that \(P_{i_{k}i_{k+1}}>0\); thus, there is a path of positive probability from state \(i\) to state \(j\). Accessibility is an transitive relationship, i.e., if \(i\rightsquigarrow j\) and \(j\rightsquigarrow k\) implies that \(i\rightsquigarrow k\).
2. Two distinct states \(i\) and \(j\) are said to **communicate** (abbreviated to \(i\rightsquigarrow j\)) if \(i\) is accessible from \(j\) (i.e., \(j\rightsquigarrow i\)) and \(j\) is accessible from \(i\) (\(i\rightsquigarrow j\)). Alternatively, we say that \(i\) and \(j\) communicate if there exist \(m,m^{\prime}\in\mathds{Z}_{\geq 0}\) such that \([P^{m}]_{ij}>0\) and \([P^{m^{\prime}}]_{ji}>0\). Communication is an equivalence relationship, i.e., it is reflexive (\(i\rightsquigarrow i\)), symmetric (\(i\rightsquigarrow j\) if and only if \(j\rightsquigarrow i\)), and transitive (\(i\rightsquigarrow j\) and \(j\rightsquigarrow k\) implies \(i\rightsquigarrow k\)).
3. The states in a finite-state Markov chain can be partitioned into two sets: **recurrent states** and **transient states**. A state is recurrent if it is accessible from all states that are from it (i.e., \(i\) is recurrent if \(i\rightsquigarrow j\) implies that \(j\rightsquigarrow i\)). States that are not recurrent are **transient**. It can be shown that a state \(i\) is recurrent if and only if \[\sum_{t=1}^{\infty}[P^{t}]_{ii}=\infty.\]4. States \(i\) and \(j\) are said to belong to the same **communicating class** if \(i\) and \(j\) communicate. Communicating classes form a partition the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class). A Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called **irreducible**.
5. The **period** of a state \(i\), denoted by \(d(i)\), is defined as \[d(i)=\gcd\{t\in\mathds{Z}_{\geq 1}:[P^{t}]_{ii}>0\}.\] If the period is \(1\), the state is **aperiodic**, and if the period is \(2\) or more, the state is **periodic**. It can be shown that all states in the same class have the same period. A Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state \(i\) such that \(P_{ii}>0\). In general, for a finite and aperiodic Markov chain, there exists a positive integer \(T\) such that \[[P^{t}]_{ii}>0,\quad\forall t\geq T,i\in\mathsf{S}.\]

#### b.1.2 Limit behavior of Markov chains

We now state some special distributions for a time-homogeneous Markov chain.

1. A PMF \(\zeta\) on \(\mathsf{S}\) is called a **stationary distribution** if \(\zeta=\zeta P\). Thus, if a (time-homogeneous) Markov chain starts in a stationary distribution, it stays in a stationary distribution. A finite irreducible Markov chain has a unique stationary distribution. Moreover, when the Markov chain is also aperiodic, the stationary distribution is given by \(\zeta(j)=1/m_{j}\), where \(m_{j}\) is the expected return time to state \(j\).
2. A PMF \(\zeta\) on \(\mathsf{S}\) is called a **limiting distribution** if \[\lim_{t\to\infty}[P^{t}]_{ij}=\zeta(j),\quad\forall i,j\in\mathsf{S}.\] A finite irreducible Markov chain has a limiting distribution if and only if it is aperiodic. Therefore, for an aperiodic Markov chain, the limiting distribution is the same as the stationary distribution.

**Theorem 3** (Strong law of large numbers for Markov chains, Theorem 5.6.1 of [4]): _Suppose \(\{S_{t}\}_{t\geq 1}\) is an irreducible Markov chain that starts in state \(i\in\mathsf{S}\). Then,_

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}\mathds{1}\{S_{t}=j\}=\frac{1}{m_ {j}}.\]

_Therefore, for any function \(h\colon\mathsf{S}\to\mathds{R}\),_

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}h(S_{t})=\sum_{j\in\mathsf{S}} \frac{h(j)}{m_{j}}.\] (5)

_If, in addition, the Markov chain \(\{S_{t}\}_{t\geq 1}\) is aperiodic, and has a limiting distribution \(\zeta\), then we have that_

\[\lim_{T\to\infty}\frac{1}{T}\sum_{t=0}^{T-1}h(S_{t})=\sum_{j\in\mathsf{S}} \zeta(j)h(j).\] (6)

#### b.2 Time-varying with periodic transition matrix

In this section, we consider time-varying Markov chains where the transition matrices \((P_{0},P_{1},\dots)\) are periodic with period \(L\). Let \(\llbracket t\rrbracket=(t\bmod L)\) and \(\mathsf{L}=\{0,\dots,L-1\}\). Then, the transition matrix \(P_{t}\) is the same as \(P_{\llbracket t\rrbracket}\). Thus, the system dynamics are completely described by the transition matrices \(\{P_{\ell}\}_{\ell\in\mathsf{L}}\). With a slight abuse of notation, we will call such a Markov chain as \(L\)**-periodic Markov chain**. We will show later that the notion of _time-periodicity_ that we are considering is equivalent to the notion of _state-periodicity_ for time-homogeneous Markov chains defined earlier.

### Constructing an equivalent time-homogeneous Markov chain

Since the Markov chain is not time-homogeneous, the classification and results of the previous section are not directly applicable. There are two ways to construct a time-homogeneous Markov chain: using state augmentation or viewing the process after every \(L\) steps.

#### b.3.1 Method 1: State augmentation

The original time-varying Markov chain \(\{S_{t}\}_{t\geq 0}\) is equivalent to the time-homogeneous Markov chain \(\{(S_{t},[\![t]\!])\}_{t\geq 0}\) defined on \(\mathsf{S}\times\mathsf{L}\) with transition matrix \(\bar{P}\) given by

\[\bar{P}((s^{\prime},\ell^{\prime})\mid(s,\ell))=P_{\ell}(s^{\prime}\mid s) \mathds{1}\{\ell^{\prime}=[\![\ell+1]\!]\}.\]

**Example 5**: Consider a \(2\)-periodic Markov chain with state space \(\mathsf{S}=\{1,2\}\) and transition matrices

\[P_{0}=\begin{bmatrix}\frac{1}{4}&\frac{3}{4}\\ \frac{1}{2}&\frac{1}{2}\end{bmatrix}\quad\text{and}\quad P_{1}=\begin{bmatrix} \frac{3}{4}&\frac{1}{4}\\ \frac{1}{4}&\frac{3}{4}\end{bmatrix}.\]

The time-periodic Markov chain of Ex. 5 may be viewed as a time-homogeneous Markov chain with state space \(\{1,2\}\times\{0,1\}\) and transition matrix

\[\bar{P}=\begin{smallmatrix}(1,0)\\ (2,0)\\ (1,1)\\ (2,1)\\ (2,1)\\ \end{smallmatrix}\begin{bmatrix}(1,0)\\ 0\\ 0\\ 0\\ \frac{3}{4}\\ \frac{1}{4}\\ \frac{3}{4}\\ \frac{3}{4}\\ \end{bmatrix} \begin{matrix}(1,0)\\ 0\\ 0\\ 0\\ 0\\ 0\\ \end{matrix}\begin{bmatrix}(2,0)\\ (1,1)\\ (2,1)\\ (2,1)\\ \end{bmatrix}=\begin{bmatrix}0&I\\ I&0\end{bmatrix}\begin{bmatrix}P_{0}&0\\ 0&P_{1}\end{bmatrix}\]

where \(0\) denotes the all zero matrix and \(I\) denotes the identity matrix (both of size \(2\times 2\)). Note that the time-homogeneous Markov chain is periodic.

Define the following:

* \(L\) block diagonal matrices \(\Lambda_{0},\ldots,\Lambda_{L-1}\in\mathds{R}^{nL\times nL}\) as follows: \[\Lambda_{0}=\mathrm{blkdiag}(P_{0},P_{1},\ldots,P_{L-1}),\quad\Lambda_{1}= \mathrm{blkdiag}(P_{L-1},P_{0},\ldots,P_{L-2}),\quad\text{ etc.}\]
* A permutation matrix \(\Pi\in\{0,1\}^{nL\times nL}\) as follows \[\Pi=\begin{bmatrix}0&I&\cdots&0\\ \vdots&\ddots&\ddots&\vdots\\ 0&0&\cdots&I\\ I&0&\cdots&0\end{bmatrix}\] where each block is \(n\times n\).

The permutation matrix \(\Pi\) satisfies the following properties (which can be verified by direct algebra):

1. \(\Pi\Pi^{\intercal}=I\) and therefore \(\Pi^{-1}=\Pi^{\intercal}\).
2. \(\Pi^{L}=I\).
3. \(\Lambda_{\ell}\Pi=\Pi\Lambda_{[\![\ell+1]\!]}\), \(\ell\in\mathsf{L}\).

In general, the transition matrix of the Markov chain \(\{(S_{t},[\![t]\!])\}_{t\geq 0}\) is

\[\bar{P}=\begin{bmatrix}0&P_{0}&\cdots&0\\ \vdots&\ddots&\ddots&\vdots\\ 0&0&\cdots&P_{L-2}\\ P_{L-1}&0&\cdots&0\end{bmatrix}_{nL\times nL}=\Lambda_{0}\Pi.\]

#### b.3.2 Method 2: Viewing the process every \(L\) steps

The original Markov chain viewed every \(L\)-steps, i.e., the process \(\{S_{kL+\ell}\}_{k\geq 0}\), \(\ell\in\mathsf{L}\), is a time-homogeneous Markov chain with transition probability matrix \(\mathcal{P}_{\ell}\) given by

\[\mathcal{P}_{\ell}=P_{[\ell]}P_{[\ell+1]}\cdots P_{[\ell+L-1]}\]

that is,

\[\mathcal{P}_{0}=P_{0}P_{1}\cdots P_{L-2}P_{L-1},\quad\mathcal{P}_{1}=P_{1}P_{2 }\cdots P_{L-1}P_{0},\quad\text{etc.}\]

#### b.3.3 Relationship between the two constructions

The two constructions are related as follows.

**Proposition 1**: _We have that \(\bar{P}^{L}=\mathrm{blkdiag}(\mathcal{P}_{0},\ldots,\mathcal{P}_{L})\)._

Proof From (P3), we get that \(\bar{P}=\Pi\Lambda_{1}\). Therefore,

\[\bar{P}^{2}=\Lambda_{0}\Pi\Lambda_{0}\Pi=\Lambda_{0}\Lambda_{1}\Pi^{2}\]

Similarly

\[\bar{P}^{3}=\Lambda_{0}\Pi\bar{P}^{2}=\Lambda_{0}\Pi\Lambda_{0}\Lambda_{1}\Pi ^{2}=\Lambda_{0}\Lambda_{1}\Pi\Lambda_{1}\Pi^{2}=\Lambda_{0}\Lambda_{1}\Lambda _{2}\Pi^{3}\]

Continuing this way, we get

\[\bar{P}^{L}=\Lambda_{0}\Lambda_{1}\ldots\Lambda_{L-1}\Pi^{L}=\Lambda_{0} \Lambda_{1}\ldots\Lambda_{L-1}.\]

where the last equality follows from (P2). The result then follows from the definitions of \(\Lambda_{\ell}\) and \(\mathcal{P}_{\ell}\), \(\ell\in\mathsf{L}\). \(\Box\)

### Limiting behavior of periodic Markov chain

In the subsequent discussion, we consider the following assumptions.

**Assumption 3**: Every \(\{\mathcal{P}_{\ell}\}\), \(\ell\in\mathsf{L}\), is irreducible and aperiodic

Suppose Assm. 3 holds. Define \(\zeta^{\ell}\) to be the unique stationary distribution for Markov chain \(\mathcal{P}_{\ell}\), \(\ell\in\mathsf{L}\), i.e., \(\zeta^{\ell}\) is the unique PMF that satisfies \(\zeta^{\ell}=\zeta^{\ell}\mathcal{P}_{\ell}\).

**Proposition 2**: _The PMFs \(\{\zeta^{\ell}\}_{\ell\in\mathsf{L}}\) satisfy_

\[\zeta^{\ell}P_{\ell}=\zeta^{[\ell+1]},\quad\ell\in\mathsf{L}.\]

Proof We prove the result for \(\ell=0\). The analysis is the same for general \(\ell\). By assumption, we have that

\[\zeta^{0}=\zeta^{0}\mathcal{P}_{0}=\zeta^{0}P_{0}P_{1}\cdots P_{L-1}.\]

Let \(\bar{\zeta}^{1}\coloneqq\zeta^{0}P_{0}\). Then, we have

\[\bar{\zeta}^{1}=\zeta^{0}P_{0}=\zeta^{0}P_{0}P_{1}\cdots P_{L-1}P_{0}=\bar{ \zeta}^{1}P_{1}\cdots P_{L-1}P_{0}=\bar{\zeta}^{1}\mathcal{P}_{1}.\]

Thus \(\bar{\zeta}^{1}\) is a stationary distribution. Since \(\mathcal{P}_{1}\) is irreducible, the stationary distribution is unique, hence \(\bar{\zeta}^{1}\) must equal \(\zeta^{1}\). \(\Box\)

We can verify this result for Ex. 5. For this model, we have

\[\mathcal{P}_{0}=P_{0}P_{1}=\begin{bmatrix}\frac{3}{8}&\frac{5}{8}\\ \frac{1}{2}&\frac{1}{2}\end{bmatrix}\quad\text{and}\quad\mathcal{P}_{1}=P_{1}P _{0}=\begin{bmatrix}\frac{5}{16}&\frac{11}{16}\\ \frac{7}{16}&\frac{9}{16}\end{bmatrix}.\]

Thus,

\[\zeta^{0}=\begin{bmatrix}4&\frac{5}{9}\\ \end{bmatrix}\quad\text{and}\quad\zeta^{1}=\begin{bmatrix}\frac{7}{18}&\frac{ 11}{18}\end{bmatrix}\]

And we can verify that \(\zeta^{0}P_{0}=\zeta^{1}\) and \(\zeta^{1}P_{1}=\zeta^{0}\).

**Proposition 3**: _Under Assm. 3, the limiting distribution of the Markov chain \(\{S_{t}\}_{t\geq 0}\) is cyclic. In particular, for any initial distribution \(\xi_{0}\),_

\[\lim_{k\to\infty}\xi_{kL+\ell}=\zeta^{\ell}\] (7)_Furthermore,_

\[\limsup_{K\to\infty}\frac{1}{K}\sum_{k=0}^{K-1}\mathds{1}\{S_{kL+\ell}=i\}=[\zeta^ {\ell}]_{i},\quad\forall i\in\mathsf{S},\ell\in\mathsf{S}.\]

_Consequently, for any function \(h\colon\mathsf{S}\to\mathds{R}\),_

\[\limsup_{K\to\infty}\frac{1}{K}\sum_{k=0}^{K-1}h(S_{kL+\ell})=\sum_{s\in \mathsf{S}}h(s)[\zeta^{\ell}]_{s},\quad\ell\in\mathsf{S}.\] (8)

Proof The results follow from standard results for the time-homogeneous Markov chain \(\{S_{kL+\ell}\}_{k\geq 0}\). 

Proof (Alternative) We present an alternative proof that uses the state augmented Markov chain \(\bar{P}\). We first prove that under Assm. 3, the chain \(\bar{P}\) is irreducible periodic with period \(L\).

The proof of irreducibility relies on two observations.

1. Fix an \(\ell\in\mathsf{L}\) and consider \(i,j\in\mathsf{S}\). Since \(\mathcal{P}_{\ell}\) is irreducible, we have that there exists a positive integer \(m\) (depending on \(i\), \(j\), and \(\ell\)) such that \([\mathcal{P}_{\ell}^{m}]_{ij}>0\). Note that Prop. 1 implies that \([\bar{P}^{mL}]_{(i,\ell),(j,\ell)}=[\mathcal{P}_{\ell}]_{ij}>0\). Therefore, in the Markov chain \(\bar{P}\), states \((i,\ell)\leadsto(j,\ell)\). Since \(i\) and \(j\) were arbitrary, all states \(\mathsf{S}\times\{\ell\}\) belong to the same communicating class.
2. Now consider two \(\ell,\ell^{\prime}\in\mathsf{L}\). Suppose we start at some state \((i,\ell)\in\mathsf{S}\times\{\ell\}\), then in \([\ell^{\prime}-\ell]\) steps, we will reach some state \((j,\ell^{\prime})\in\mathsf{S}\times\{\ell^{\prime}\}\). Thus, \((j,\ell^{\prime})\) is accessible from \((i,\ell)\). But, we have already argued that all states in \(\mathsf{S}\times\{\ell\}\) belong to the same communicating class, therefore all states in \(\mathsf{S}\times\{\ell^{\prime}\}\) are accessible from all states in \(\mathsf{S}\times\{\ell\}\). By interchanging the roles of \(\ell\) and \(\ell^{\prime}\), we have that all states in \(\mathsf{S}\times\{\ell\}\) are accessible from all starts in \(\mathsf{S}\times\{\ell^{\prime}\}\). Therefore, the states \(\mathsf{S}\times\{\ell\}\) and \(\mathsf{S}\times\{\ell^{\prime}\}\) belong to the same communicating class. Since \(\ell\) and \(\ell^{\prime}\) were arbitrary, we have that all states of \(\bar{P}\) belong to the same communicating class. Hence, \(\bar{P}\) is irreducible.

We now show that \(\bar{P}\) is periodic. First observe that the Markov chain starting in the set \(\mathsf{S}\times\{\ell\}\) does not return to the same set for the first \(L-1\) steps. Thus, \([\bar{P}^{t}]_{(i,\ell),(i,\ell)}=0\) for \(t\in\{1,2,\ldots,L-1\}\). Therefore, the only possible values of \(t\) for which \([\bar{P}^{t}]_{(i,\ell),(i,\ell)}>0\) are those that are multiples of \(L\). Hence, for any \((i,\ell)\in\mathsf{S}\times\mathsf{L}\),

\[d(i,\ell)=\gcd\{t\in\mathds{Z}_{\geq 1}:[\bar{P}^{t}]_{(i,\ell),(i,\ell)}>0\} =L\gcd\{k\in\mathds{Z}_{\geq 1}:[\mathcal{P}_{\ell}^{k}]_{ii}>0\}\] (9)

Moreover, since \(\mathcal{P}_{\ell}\) is aperiodic, \(\gcd\{k\in\mathds{Z}_{\geq 1}:[\mathcal{P}_{\ell}^{k}]_{ii}>0\}=1\). Substituting in (9), we get that \(d(i,\ell)=L\) for all \((i,\ell)\). Thus, all states have a period of \(L\).

Now, from Prop. 1, we know that \(\bar{P}^{L}=\mathrm{blkdiag}(\mathcal{P}_{0},\ldots,\mathcal{P}_{L-1})\). Therefore

\[\lim_{k\to\infty}[\bar{P}^{kL}]_{(i,\ell),(j,\ell)}=[\zeta^{\ell}]_{j},\quad( i,\ell)\in\mathsf{S}\times\mathsf{L}.\]

Consequently, if we start with an initial distribution \(\bar{\xi}_{0}\) such that \(\bar{\xi}_{0}(\mathsf{S}\times\{0\})=1\), then,

\[\lim_{k\to\infty}\bar{\xi}_{kL}=\mathrm{vec}(\zeta_{0},0,\ldots,0)\]

where the \(0\) vectors are of size \(n\). Consequently, Prop. 2 implies that

\[\lim_{k\to\infty}\bar{\xi}_{kL+\ell}=\mathrm{vec}(0,\ldots,0,\zeta_{\ell},0, \ldots,0),\quad\forall\ell\in\mathsf{L}\]

where \(\zeta^{\ell}\) is the \(\ell\)-th place. This completes the proof of (7).

Now consider the function \(\bar{h}\colon\mathsf{S}\times\mathsf{L}\to\mathds{R}\) defined as \(\bar{h}(s,\ell^{\prime})=h(s)\mathds{1}\{\ell^{\prime}=\ell\}\). Then, by taking \(T=KL\), we have

\[\lim_{K\to\infty}\frac{1}{K}\sum_{t=0}^{K-1}h(S_{kL+\ell})=\lim_{T\to\infty} \frac{L}{T}\sum_{t=0}^{T-1}\bar{h}(S_{t},\llbracket t\rrbracket)=L\sum_{s\in \mathsf{S}}\frac{h(s)}{m_{(s,\ell)}}\]

where the last equation uses (5) from Thm. 3. Now, (8) follows from observing that mean return time to state \((s,\ell)\) in Markov chain \(\bar{P}\) is \(L\) times the mean-return time to state \(s\) in Markov chain \(\mathcal{P}_{\ell}\), which equals \(1/[\zeta^{\ell}]_{s}\) since \(\mathcal{P}_{\ell}\) is irreducible and aperiodic.

Periodic Markov decision processes

Periodic MDPs are a special class time non-stationary MDPs where the dynamics and rewards are periodic. In particular, let \(\mathcal{M}\) be a time-varying MDP with state space \(\mathsf{S}\), action space \(\mathsf{A}\), and dynamics and reward at time \(t\) given by \(P_{t}\colon\mathsf{S}\times\mathsf{A}\to\Delta(\mathsf{S})\) and \(r_{t}\colon\mathsf{S}\times\mathsf{A}\to\mathds{R}\).

As before, we use \(\llbracket t\rrbracket\) to denote \(t\bmod L\) and \(\mathsf{L}\) to denote \(\{0,\dots,L-1\}\). The MDP \(\mathcal{M}\) is periodic with period \(L\) if there exist \((P^{\ell},r^{\ell})\), \(\ell\in\mathsf{L}\) such that for all \(t\):

\[P_{t}(S_{t+1}\mid S_{t},A_{t})=P^{\llbracket t\rrbracket}(S_{t+1}\mid S_{t},A_{ t})\quad\text{and}\quad r_{t}(S_{t},A_{t})=r^{\llbracket t\rrbracket}(S_{t},A_{t}).\]

Periodic MDPs were first considered in [14]. Periodic MDPs may be viewed as stationary MDPs by considering the augmented state \((S_{t},\llbracket t\rrbracket)\). By this equivalence, it can be shown that there is no loss of optimality in restricting attention to periodic policies. In particular, let \((V^{0},\dots,V^{L-1})\) denote the fixed point of the following system of equations

\[V^{\ell}(s)=\max_{a\in\mathsf{A}}\Bigl{\{}r^{\ell}(s,a)+\gamma\sum_{s^{\prime }\in\mathsf{S}}P^{\ell}(s^{\prime}|s,a)V^{\llbracket t+1\rrbracket}(s^{\prime })\Bigr{\}},\quad\forall(\ell,s,a)\in\mathsf{L}\times\mathsf{S}\times\mathsf{A}.\] (10)

Define \(\pi_{*}^{\ell}(s)\) to be the arg-max or the right hand side of (10). Then the time-varying policy \(\pi=(\pi_{1},\pi_{2},\dots)\) given by \(\pi_{t}=\pi_{*}^{\llbracket t\rrbracket}\) is optimal.

See [14] for a discussion of how to modify standard MDP algorithms to solve periodic dynamic program (10).

## Appendix D Stochastic Approximation with Markov noise

We now state a generalization of Thm. 3 to stochastic approximation style iterations.

**Theorem 4**: _Let \(\{S_{t}\}_{t\geq 1}\), \(\mathsf{S}\), be an irreducible and aperiodic finite Markov chain with unique limiting distribution \(\zeta\). Let \(\mathcal{F}_{t}\) denote the natural filtration w.r.t. \(\{S_{t}\}_{t\geq 1}\) and \(\{\alpha_{t}\}_{t\geq 1}\) be a non-negative real-valued process adapted to \(\{\mathcal{F}_{t}\}\) that satisfies_

\[\sum_{t\geq 1}\alpha_{t}=\infty\quad\text{and}\quad\sum_{t\geq 1}\alpha_{t}^{2 }<\infty.\] (11)

_Let \(\{M_{t+1}\}_{t\geq 1}\) be a square-integrable margingale difference sequence w.r.t. \(\{\mathcal{F}_{t}\}_{t\geq 1}\) such that \(\mathds{E}[M_{t+1}^{2}\mid\mathcal{F}_{t}]\leq K(1+\|X_{t}\|^{2})\) for some constant \(K\). Consider the iterative process \(\{X_{t}\}_{t\geq 1}\), where \(X_{1}\) is arbitrary and for \(t\geq 1\), we have_

\[X_{t+1}=(1-\alpha_{t})X_{t}+\alpha_{t}\bigl{[}h(S_{t})+M_{t+1}\bigr{]}.\] (12)

_Then, the sequence \(\{X_{t}\}_{t\geq 1}\) converges almost surely to limit. In particular,_

\[\lim_{T\to\infty}X_{T}=\sum_{s\in\mathsf{S}}h(s)\zeta(s),\quad a.s.\] (13)

Eq. (12) is similar to standard stochastic approximation iteration [13, 14, 15], which the "noise sequence" \(h(S_{t})\) is assumed to be a martingale difference sequence. The setting considered above is sometimes referred to as stochastic approximation with Markov noise. In fact, more general version of this result where the noise sequence is allowed to depend on the state \(X_{t}\) are typically established in the literature [1, 15, 16, 17]. For the sake of completeness, we will show that Thm. 4 is a special case of these more-general results.

Before presenting the proof, we point out that Thm. 4 is a generalization of Thm. 3, Eq. (6). In particular, suppose the learning rates are \(\alpha_{t}=1/(1+t)\). Then, simple algebra shows that

\[X_{T}=\frac{1}{T}\sum_{t=1}^{T}h(S_{t}).\]

Then, (6) of Thm. 3 implies that the limit is given by the right had side of (13). Therefore, Thm. 4 is a generalization of Thm. 3 to general learning rates which satisfy (11).

Proof To establish the result, we will show that the iteration \(\{X_{t}\}_{t\geq 1}\) satisfies the assumptions for the convergence of stochastic approximation with (state dependent) Markov noise and stochastic recursive inclusions given in [4, Theorem 2.7]. The proof is due to [1]. In particular, we can rewrite (12) as

\[X_{t+1}=X_{t}+\alpha_{t}g(X_{t},S_{t})\]

where \(g(x,s)=-x+h(s)\). Moreover, for ease of notation, define \(\bar{h}=\sum_{s\in\mathsf{S}}h(s)\zeta(s)\). Then, we have

* \(g(x,s)\) is Lipschtiz continuous in the first argument, so A2.14 of [4] holds.
* From (6), the ergodic occupation measure of \(\{h(S_{t})\}_{t\geq 1}\) is \(\{\bar{h}\}\), which is compact and convex. So, A2.15 of [4] is satisfied.
* The conditions on the martingale noise sequence \(\{M_{t}\}_{t\geq 1}\) imply that A2.16 of [4] holds.
* Eq. (11) is equivalent to A2.17 of [4].
* To check A2.18 of [4], for any measure \(\nu\) on \(\mathsf{S}\), define \[\tilde{h}(x,\nu)=\int g(x,s)\nu(ds)=-x+\bar{h}.\] Also define \[\tilde{h}_{c}(x,\nu)=\frac{\tilde{h}(cx,c\nu)}{c}=-x+\frac{\bar{h}}{c}\] Let \(\tilde{h}_{\infty}(x,\nu)=\lim_{c\to\infty}h_{c}(x,\nu)=x\). Thus, the differential inclusion in A2.18(ii) is actually an ODE \[\dot{x}=-x\] which has origin as the unique global asymptotically stable equilibrium point. Thus, A2.18 of [4] is satisfied.

Therefore, all assumptions of Theorem 2.7 of [4] are satisfied. Therefore, by that result, the iterates \(\{X_{t}\}_{t\geq 1}\) converge to solution of the ODE (note that the differential inclusion in Theorem 2.7 of [4] is an ODE in our setting)

\[\dot{x}=-x+\bar{h}.\] (14)

Note that \(x=\bar{h}\) is the unique asymptotically stable attractor of the ODE (14). Therefore, Theorem 2.7 of [4] implies (13). 

Thm. 4 also implies the following generalization of Prop. 3.

**Proposition 4**: _Suppose \(\{S_{t}\}_{t\geq 1}\) is a time-periodic Markov chain with period \(L\) that satisfies Assm. 3 with the unique limiting distribution \(\{\zeta^{\ell}\}_{\ell\in\mathsf{L}}\). Let \(\{\mathcal{F}_{t}\}_{t\geq 1}\) denote the natural filtration w.r.t. \(\{S_{t}\}_{t\geq 1}\) and \(\{\alpha_{t}^{\ell}\}_{t\geq 1}\), \(\ell\in\mathsf{L}\), be non-negative real-valued processes adapted to \(\{\mathcal{F}_{t}\}_{t\geq 1}\) such that \(\alpha_{t}^{\ell}=0\) when \(\ell\neq[\![t]\!]\) and_

\[\sum_{t\geq 1}\alpha_{t}^{\ell}=\infty\quad\text{and}\quad\sum_{t\geq 1}( \alpha_{t}^{\ell})^{2}<\infty.\]

_Let \(\{M_{t+1}\}_{t\geq 1}\) be a square-integrable martingale difference sequence w.r.t. \(\{\mathcal{F}_{t}\}_{t\geq 1}\) such that \(\mathbb{E}[M_{t+1}^{2}\mid\mathcal{F}_{t}]\leq K(1+\|X_{t}\|^{2})\) for some constant \(K\). Fix any \(\ell\in\mathsf{L}\), Consider the iterative process \(\{X_{k}^{\ell}\}_{k\geq 1}\), where \(X_{1}\) is arbitrary and for \(k\geq 1\), we have_

\[X_{t+1}^{\ell}=(1-\alpha_{t}^{\ell})X_{t}^{\ell}+\alpha_{t}^{\ell}\big{[}h(S_{ t})+M_{t+1}\big{]}.\] (15)

_Then, the sequence \(\{X_{t}^{\ell}\}_{t\geq 1}\) converges almost surely to the following limit_

\[\lim_{t\to\infty}X_{t}^{\ell}=\sum_{s\in\mathsf{S}}h(s)\zeta^{\ell}(s),\quad a.s.\]

Proof Note that the learning rates used here can be viewed as the learning rates of \(L\) separated stochastic iterations on a common timescale \(t\). Each separate stochastic iteration \(\ell\in\mathsf{L}\) is actually only updated once every \(L\) steps on the timescale \(t\). Because of the condition \(\alpha_{t}^{\ell}=0\) when \(\ell\neq[\![t]\!]\), each update is followed by \(L-1\) "pseudo"-updates where the learning rate is \(0\). Therefore, each \(X^{\ell}\) is updated only once every \(L\) steps on timescale \(t\).

The result then follows immediately from Thm. 4 by considering the process \(\{S_{t}\}_{t\geq 1}\) every \(L\) steps for each \(\ell\in\mathsf{L}\).

Thm. 1: Convergence of periodic Q-learning

The high-level idea of the proof is similar to [14] for ASQL when the agent state is a finite window of past observations and action. The key observation of [14] is the following: Consider an iterative process \(X_{t+1}=(1-\alpha_{t})X_{t}+\alpha_{t}U_{t}\) with the learning rates \(\alpha_{t}=1/(1+t)\). Then, \(X_{t+1}=(X_{0}+\sum_{\tau=1}^{t}U_{t})/(1+t)\). Then, if the process \(\{U_{t}\}_{t\geq 1}\) has an ergodic limit (e.g., when \(\{U_{t}\}_{t\geq 1}\) is a function of a Markov chain, see Thm. 3), the process \(\{X_{t}\}_{t\geq 1}\) converges to the ergodic limit of \(\{U_{t}\}_{t\geq 1}\). We follow a similar idea but with the following changes:

* Instead of assuming "averaging" learning rates (i.e., reciprocal of the number of visits), we allow for general learning rates of Assm. 1.
* We account for the fact that that the "noise" is periodic.

The rest of the analysis then follows along the standard argument of convergence of Q-learning [14; 14].

Define the error function \(\Delta_{t+1}^{\ell}\coloneqq Q_{t+1}^{\ell}-Q_{\mu}^{\ell}\), for all \(\ell\in\mathsf{L}\). To prove Thm. 1, it suffices to prove that \(\|\Delta_{t}^{\ell}\|\to 0\) for all \(\ell\in\mathsf{L}\), where \(\|\!\cdot\!\|\) is the supremum-norm. The proof proceeds in three steps.

### Step 1: State splitting of the error function

Define \(V_{t}^{\ell}(z)\coloneqq\max_{a\in\mathsf{A}}Q_{t}^{\ell}(z,a)\) and \(V_{\mu}^{\ell}(z)\coloneqq\max_{a\in\mathsf{A}}Q_{\mu}^{\ell}(z,a)\), for all \(\ell\in\mathsf{L}\), \(z\in\mathsf{Z}\). We can combine (PASQL), (1), and (2) as follows

\[\Delta_{t+1}^{\ell}(z,a)=(1-\alpha_{t}^{\ell}(z,a))\Delta_{t}^{\ell}(z,a)+ \alpha_{t}^{\ell}(z,a)\big{[}U_{t}^{\ell,0}(z,a)+U_{t}^{\ell,1}(z,a)+U_{t}^{ \ell,2}(z,a)\big{]}\] (16)

where

\[U_{t}^{\ell,0}(z,a) \coloneqq\big{[}r(S_{t},A_{t})-r_{\mu}^{\ell}(z,a)\big{]}\, \mathds{1}_{\{Z_{t}=z,A_{t}=a\}},\] \[U_{t}^{\ell,1}(z,a) \coloneqq\Big{[}\gamma V_{\mu}^{[\ell+1]}(Z_{t+1})-\gamma\sum_{z ^{\prime}\in\mathsf{Z}}P_{\mu}^{\ell}(z^{\prime}|z,a)V_{\mu}^{[\ell+1]}(z^{ \prime})\Big{]}\mathds{1}_{\{Z_{t}=z,A_{t}=a\}},\] \[U_{t}^{\ell,2}(z,a) \coloneqq\gamma V_{t}^{[\ell+1]}(Z_{t+1})-\gamma V_{\mu}^{[\ell+1 ]}(Z_{t+1}).\]

Note that we have added extra indicator functions in the \(U_{t}^{\ell,i}(z,a)\) terms, \(i\in\{0,1\}\). This does not change the value of \(\alpha_{t}^{\ell}(z,a)U_{t}^{\ell,i}(z,a)\) because the learning rates have the property that \(\alpha_{t}^{\ell}(z,a)=0\) if \((\ell,z,a)\neq([\![t]\!],z_{t},a_{t})\) (see Assm. 1).

For each \(\ell\in\mathsf{L}\), Eq. (16) may be viewed as a linear system with state \(\Delta_{t+1}^{\ell}\) and three inputs \(U_{t}^{\ell,0}\),\(U_{t}^{\ell,1}\) and \(U_{t}^{\ell,2}\). We exploit the linearity of the system and split the state into three components: \(\Delta_{t+1}^{\ell}=X_{t+1}^{\ell,0}+X_{t+1}^{\ell,1}+X_{t+1}^{\ell,2}\), where the three components evolve as follows:

\[X_{t+1}^{\ell,i}(z,a)=(1-\alpha_{t}^{\ell}(z,a))X_{t}^{\ell,i}(z,a)+\alpha_{t}^ {\ell}(z,a)U_{t}^{\ell,i}(z,a),\quad i\in\{0,1,2\}\] (17)

Linearity implies that (16) is equivalent to (17). We will now separately show that \(\|X_{t}^{\ell,0}\|\to 0\), \(\|X_{t}^{\ell,1}\|\to 0\) and \(\|X_{t}^{\ell,2}\|\to 0\).

### Step 2: Convergence of component \(X_{t}^{\ell,0}\)

Fix \((\ell,z_{\circ},a_{\circ})\in\mathsf{L}\times\mathsf{Z}\times\mathsf{A}\) and define

\[h_{r}(S_{t},Z_{t},A_{t};\ell,z_{\circ},a_{\circ})=\big{[}r(S_{t},A_{t})-r_{\mu} ^{\ell}(z_{\circ},a_{\circ})\big{]}\mathds{1}_{\{Z_{t}=z_{\circ},A_{t}=a_{ \circ}\}}.\]

Then the process \(\{X_{t}^{\ell,0}(z_{\circ},a_{\circ})\}_{t\geq 1}\) is given by the stochastic iteration

\[X_{t+1}^{\ell,0}(z_{\circ},a_{\circ})=(1-\alpha_{t}^{\ell}(z_{\circ},a_{\circ})) X_{t}^{\ell,0}(z_{\circ},a_{\circ})+\alpha_{t}^{\ell}(z_{\circ},a_{\circ})h_{r}(S_{t},Z_{t},A_{t };\ell,z_{\circ},a_{\circ}),\]

which is of the form (15). The process \(\{(S_{t},Z_{t},A_{t})\}_{t\geq 1}\) is a periodic Markov chain and the learning rates \(\{\alpha_{t}^{\ell}(z_{\circ},a_{\circ})\}_{t\geq 1}\) satisfy the conditions of Prop. 4 due to Assm. 1. Therefore, Prop. 4 implies

[MISSING_PAGE_FAIL:27]

### Step 4: Convergence of component \(X_{t}^{\ell,2}\)

The remaining analysis is similar to corresponding step in the standard convergence proof of Q-learning and its variations [13, 14, 15]. In this section, we use \(\|\cdot\|\) to denote the supremum norm, i.e., \(\|\cdot\|_{\infty}\).

In the previous step, we have shown that \(\|X_{t}^{\ell,i}\|\to 0\) a.s., for \(i\in\{0,1\}\). Thus, we have that \(\|X_{t}^{\ell,0}+X_{t}^{\ell,1}\|\to 0\) a.s. Arbitrarily fix an \(\epsilon>0\). Therefore, there exists a set \(\Omega^{1}\) of measure one and a constant \(T(\omega,\epsilon)\) such that for \(\omega\in\Omega^{1}\), all \(t>T(\omega,\epsilon)\), and \((\ell,z,a)\in\mathsf{L}\times\mathsf{Z}\times\mathsf{A}\), we have

\[X_{t}^{\ell,0}(z,a)+X_{t}^{\ell,1}(z,a)<\epsilon.\] (18)

Now pick a constant \(C\) such that

\[\kappa\coloneqq\gamma\left(1+\frac{1}{C}\right)<1\] (19)

Suppose for some \(t>T(\omega,\epsilon)\), \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|>C\epsilon\). Then, for \((z,a)\in\mathsf{Z}\times\mathsf{A}\),

\[U_{t}^{\ell,2}(z,a) =\gamma V_{t}^{[\ell+1]}(Z_{t+1})-\gamma V_{\mu}^{[\ell+1]}(Z_{t+ 1})\] \[=\gamma\max_{a\in\mathsf{A}}Q_{t}^{[\ell+1]}(Z_{t+1},a)-\max_{a^{ \prime}\in\mathsf{A}}\gamma Q_{\mu}^{[\ell+1]}(Z_{t+1},a^{\prime})\] \[\leq\gamma\max_{a\in\mathsf{A}}\left\{Q_{t}^{[\ell+1]}(Z_{t+1},a) -\gamma Q_{\mu}^{[\ell+1]}(Z_{t+1},a)\right\}\] \[\stackrel{{(a)}}{{\leq}}\gamma\|Q_{t}^{[\ell+1]}-Q_{ \mu}^{[\ell+1]}\|=\gamma\|\Delta_{t}^{[\ell+1]}\|\] \[\leq\gamma\|X_{t}^{[\ell+1],0}+X_{t}^{[\ell+1],1}\|+\gamma\|X_{t} ^{[\ell+1],2}\|\stackrel{{(b)}}{{\leq}}\gamma\epsilon+\gamma\|X _{t}^{[\ell+1],2}\|\] (20a) \[\stackrel{{(c)}}{{\leq}}\gamma\left(1+\frac{1}{C} \right)\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\stackrel{{(d)} }{{=}}\kappa\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\stackrel{{ (d)}}{{<}}\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|.\] (20b)

where \((a)\) follows from the fact that an upper bound is obtained by maximizing over all realizations of \(Z_{t+1}\), \((b)\) follows from (18), \((c)\) follows from the fact that \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|>C\epsilon\), \((d)\) follows from (19). Thus, for any \(t>T(\omega,\epsilon)\) and \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|>C\epsilon\), we have

\[X_{t+1}^{\ell,2}(z,a) =(1-\alpha_{t}^{\ell}(z,a))X_{t}^{\ell,2}(z,a)+\alpha_{t}^{\ell}( z,a)U_{t}^{\ell,2}(z,a)<\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\] \[\implies\max_{\ell\in\mathsf{L}}\|X_{t+1}^{\ell,2}\|<\max_{\ell \in\mathsf{L}}\|X_{t}^{\ell,2}\|.\]

Hence, when \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|>C\epsilon\), it decreases monotonically with time. Hence, there are two possibilities: either (i) \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\) always remains above \(C\epsilon\); or (ii) it goes below \(C\epsilon\) at some stage. We consider these two possibilities separately.

e.4.1 Possibility (i): \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\) always remains above \(C\epsilon\)

We will show that \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\) cannot remain above \(C\epsilon\) forever. We first start with a basic result for random iterations. This is a self-contained result, so we reuse some of the variables used in the rest of the paper.

**Lemma 2**: _Let \(\{X_{t}\}_{t\geq 1}\), \(\{Y_{t}\}_{t\geq 1}\), and \(\{\alpha_{t}\}_{t\geq 1}\) be non-negative sequences adapted to a filtration \(\{\mathcal{F}_{t}\}_{t\geq 1}\) that satisfy the following:_

\[X_{t+1} \leq(1-\alpha_{t})X_{t},\] (21a) \[Y_{t+1} \leq(1-\alpha_{t})Y_{t}+\alpha_{t}c,\] (21b)

_where \(c\) is a constant. Suppose_

\[\sum_{t=1}^{\infty}\alpha_{t}=\infty\] (22)

_Then, the sequence \(\{X_{t}\}_{t\geq 1}\) converges to zero almost surely and the sequence \(\{Y_{t}\}_{t\geq 1}\) converges to \(c\) almost surely._Proof The iteration (21a) implies that

\[X_{t+1}\leq\Big{[}(1-\alpha_{1})\cdots(1-\alpha_{t})\Big{]}X_{1}\]

Condition (22) implies that the term in the square brackets converges to zero. Therefore, \(X_{t}\to 0\).

Observe that the iteration (21b) can be rewritten as

\[Y_{t+1}-c\leq(1-\alpha_{t})(Y_{t}-c)\]

which is of the form (21a). Therefore, \(Y_{t}-c\to 0\). 

We will now prove that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\) cannot remain above \(C\epsilon\) forever. The proof is by contradiction. Suppose \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\) remains above \(C\epsilon\) forever. As argued earlier, this implies that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\), \(t\geq T(\omega,\epsilon)\), is a strictly decreasing sequence, so it must be bounded from above. Let \(B^{(0)}\) be such that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\leq B^{(0)}\) for all \(t\geq T(\omega,\epsilon)\). Eq. (20b) implies that \(\lVert U_{t}^{\ell,2}\rVert<\kappa B^{(0)}\). Then, we have that

\[\max_{\ell\in\mathbb{L}}X_{t+1}^{\ell,2}(z,a) \leq(1-\alpha_{t}^{\ell}(z,a))\max_{\ell\in\mathbb{L}}\lVert X_{ t}^{\ell,2}\rVert+\alpha_{t}^{\ell}(z,a)\max_{\ell\in\mathbb{L}}\lVert U_{t}^{\ell,2}\rVert\] \[\leq(1-\alpha_{t}^{\ell}(z,a))\max_{\ell\in\mathbb{L}}\lVert X_{ t}^{\ell,2}\rVert+\alpha_{t}^{\ell}(z,a)\kappa\max_{\ell\in\mathbb{L}} \lVert X_{t}^{\ell,2}\rVert\]

which implies that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\leq\lVert M_{t}^{\ell,(0)}\rVert\), where \(\{M_{t}^{\ell,(0)}\}_{t\geq T(\omega,\epsilon)}\) is a sequence given by

\[M_{t+1}^{\ell,(0)}(z,a)\leq(1-\alpha_{t}^{\ell}(z,a))M_{t}^{\ell,(0)}(z,a)+ \alpha_{t}^{\ell}(z,a)\kappa B^{(0)},\quad\forall(z,a)\in\mathsf{Z}\times \mathsf{A}.\]

Lem. 2 implies that \(M_{t}^{\ell,(0)}(z,a)\to\kappa B^{(0)}\) and hence \(\lVert M_{t}^{\ell,(0)}\rVert\to\kappa B^{(0)}\). Now pick an arbitrary \(\bar{\epsilon}\in(0,(1-\kappa)C\epsilon)\). Thus, there exists a time \(T^{(1)}=T^{(1)}(\omega,\epsilon,\bar{\epsilon})\) such that for all \(t>T^{(1)}\), \(\lVert M_{t}^{\ell,(0)}\rVert\leq B^{(1)}\coloneqq\kappa B^{(0)}+\bar{\epsilon}\). Since \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\) is bounded by \(\lVert M_{t}^{\ell,(0)}\rVert\), this implies that for all \(t>T^{(1)}\), \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\leq B^{(1)}\) and, by (20b), \(\lVert U_{t}^{\ell,2}\rVert\leq\kappa B^{(1)}\). By repeating the above argument, there exists a time \(T^{(2)}\) such that for all \(t\geq T^{(2)}\),

\[\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\leq B^{(2)}\coloneqq\kappa B ^{(1)}+\bar{\epsilon}=\kappa^{2}B^{(0)}+\kappa\bar{\epsilon}+\bar{\epsilon},\]

and so on. By (19), \(\kappa<1\) and \(\bar{\epsilon}\) is chosen to be less than \(C\epsilon\). So eventually, \(B^{(m)}\coloneqq\kappa^{m}B^{(0)}+\kappa^{m-1}\bar{\epsilon}+\cdots+\bar{\epsilon}\) must get below \(C\epsilon\) for some \(m\), contradicting the assumption that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\) remains above \(C\epsilon\) forever.

e.4.2 Possibility (ii): \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\) goes below \(C\epsilon\) at some stage

Suppose that there is some \(t>T(\omega,\epsilon)\) such that \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert<C\epsilon\). Then (20a) implies that

\[\lVert U_{t}^{\ell,2}\rVert\leq\gamma\lVert X_{t}^{[\ell+1],0}+X_{t}^{[\ell+1],1}\rVert+\gamma\lVert X_{t}^{[\ell+1],2}\rVert\leq\gamma\epsilon+\gamma C \epsilon<C\epsilon\]

where the last inequality uses (19). Therefore,

\[\max_{\ell\in\mathbb{L}}X_{t+1}^{\ell,2}(z,a)\leq(1-\alpha_{t}^{\ell}(z,a)) \max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert+\alpha_{t}^{\ell}(z,a) \max_{\ell\in\mathbb{L}}\lVert U_{t}^{\ell,2}\rVert<C\epsilon\]

where the last inequality uses the fact that both \(\lVert U_{t}^{\ell,2}\rVert\) and \(\max_{\ell\in\mathbb{L}}\lVert X_{t+1}^{\ell,2}\rVert\) are both below \(C\epsilon\). Thus, we have that

\[\max_{\ell\in\mathbb{L}}X_{t+1}^{\ell,2}(z,a)<C\epsilon.\]

Hence, once \(\max_{\ell\in\mathbb{L}}\lVert X_{t+1}^{\ell,2}\rVert\) goes below \(C\epsilon\), it stays there.

#### e.4.3 Implication

We have show that for sufficiently large \(t>T(\omega,\epsilon)\), \(\max_{\ell\in\mathbb{L}}X_{t}^{\ell,2}(z,a)<C\epsilon\). Since \(\epsilon\) is arbitrary, this means that for all realizations \(\omega\in\Omega^{1}\), \(\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert\to 0\). Thus,

\[\lim_{t\to\infty}\max_{\ell\in\mathbb{L}}\lVert X_{t}^{\ell,2}\rVert=0,\quad a.s.\] (23)

### Putting everything together

Recall that we defined \(\Delta_{t}^{\ell}=Q_{t}^{\ell}-Q_{\mu}\) and in Step \(1\), we split \(\Delta_{t}^{\ell}=X_{t}^{\ell,0}+X_{t}^{\ell,1}+X_{t}^{\ell,2}\). Steps \(2\) and \(3\) together show that \(\|X_{t}^{\ell,0}+X_{t}^{\ell,1}\|\to 0\), a.s. and Step \(3\) (23) shows us that \(\max_{\ell\in\mathsf{L}}\|X_{t}^{\ell,2}\|\to 0\), a.s. Thus, by the triangle inequality,

\[\lim_{t\to\infty}\lVert\Delta_{t}^{\ell}\rVert\leq\lim_{t\to\infty}\lVert X _{t}^{\ell,0}+X_{t}^{\ell,1}\rVert+\lim_{t\to\infty}\lVert X_{t}^{\ell,2} \rVert=0,\]

which establishes that \(Q_{t}^{\ell}\to Q_{\mu}\), a.s.

## Appendix F Thm. 2: Sub-optimality gap

The high-level idea of proving Thm. 2 is as follows. Thm. 1 shows that PASQL converges to a cyclic limit, which is the solution to a periodic MDP. Thus, the question of characterizing the sub-optimality gap is equivalent to the following. Given a PODMP \(\mathcal{P}\), let \(\mathcal{M}\) be a periodic agent-state based model that approximates the reward and the dynamics of \(\mathcal{P}\) (in the sense of an approximate information state, as defined in [15]). Let \(\hat{\pi}^{\star}\) be the optimal policy of model \(\mathcal{M}\). What is the sub-optimality gap when \(\hat{\pi}^{\star}\) is used in the original POMDP \(\mathcal{P}\)?

To answer such questions, a general framework of approximate information states was developed in [15] for both finite and infinite horizon models. However, we cannot directly used the results of [15] because the infinite horizon results there were restricted to stationary policies, while we are interested in the sub-optimality gap of periodic policies.

Nonetheless, Thm. 2 can be proved by building on the existing results of [15]. In particular, we start by looking at finite horizon model rather than infinite horizon model. Then, as per [15, Definition 7], the agent state process may be viewed as an approximate information state with approximation errors \(\{(\varepsilon_{t},\delta_{t})\}_{t\geq 1}\), where

\[\varepsilon_{t} =\sup_{h_{t},a_{t}}\big{|}\mathds{E}[R_{t}\mid h_{t},a_{t}]- \sum_{s\in\mathsf{S}}r(s,a)\zeta_{\mu}^{[\mathsf{f}]}(s\mid z,a)\big{|},\] \[\delta_{t} =\sup_{h_{t},a_{t}}d_{\mathsf{S}}(\mathds{P}(Z_{t+1}=\cdot\mid h_ {t},a_{t}),P_{\mu}^{[\mathsf{f}]}(Z_{t+1}=\cdot|\sigma_{t}(h_{t}),a_{t})).\]

Let \(V_{t,T}^{\pi}(h_{t})=\mathds{E}^{\vec{\pi}}\big{[}\sum_{\tau=t}^{T}\gamma^{ \tau-1}R_{\tau}\mid h_{t}\big{]}\) denote the value function of policy \(\vec{\pi}\) for the finite horizon model starting at history \(h_{t}\) at time \(t\). Let \(V_{t,T}^{\star}(h_{t})\coloneqq\sup_{\vec{\pi}}V_{t,T}^{\vec{\pi}}(h_{t})\) denote the optimal value function, where the optimization is over all history dependent policies. Moreover, let \(\hat{V}_{t,T}(z_{t})\) denote the optimal value function for the periodic MDP model constructed in Thm. 1. Let \(\vec{\pi}_{\mu}\) denote the history-based policy defined in Sec. 2.4.

Then, from [15, Theorem 9] we have

\[\sup_{h_{t}}\bigl{[}V_{t,T}^{\star}(h_{t})-V_{t,T}^{\vec{\pi}_{\mu}}(h_{t}) \bigr{]}\leq 2\sum_{\tau=t}^{T}\gamma^{\tau-t}\bigl{[}\varepsilon_{\tau}+ \gamma\delta_{\tau}\rho_{\bar{\mathfrak{g}}}(\hat{V}_{\tau+1,T})\bigr{]}\] (24)

where we set \(\hat{V}_{T+1,T}(z)\equiv 0\) for convenience.

The following hold when we let \(T\to\infty\).

* Since \(R_{t}\) is uniformly bounded, \(V_{t,T}^{\star}(h_{t})\to V_{t}^{\star}(h_{t})\) as \(T\to\infty\).
* By the same argument, \(V_{t,T}^{\vec{\pi}_{\mu}}(h_{t})\to V_{t}^{\vec{\pi}_{\mu}}(h_{t})\) as \(T\to\infty\).
* By standard results for periodic MDPs (see App. C), \(\hat{V}_{t,T}\to V_{\mu}^{[\mathsf{f}]}\) as \(T\to\infty\).
* By definition, \(\varepsilon_{t}\leq\varepsilon_{t}^{[\mathsf{f}]}\) and \(\delta_{t}\leq\delta_{t}^{[\mathsf{f}]}\).

Therefore, by taking \(T\to\infty\) in (24), we get

\[\sup_{h_{t}}\bigl{[}V_{t}^{\star}(h_{t})-V_{t}^{\vec{\pi}_{\mu}}(h_{t})\bigr{]} \leq 2\sum_{\tau=t}^{\infty}\gamma^{\tau-t}\bigl{[}\varepsilon_{\tau}^{ \llbracket\mathsf{f}\rrbracket}+\gamma\delta_{\tau}^{\llbracket\mathsf{f} \rrbracket}\rho_{\bar{\mathfrak{g}}}(\hat{V}^{\llbracket\tau+1\rrbracket}) \bigr{]}.\]

The result then follows from observing that for \(\tau\in\mathsf{T}(t,\ell)\), \(\epsilon_{t}^{\ell}\) and \(\delta_{t}^{\ell}\) are non-decreasing sequences.

Policy evaluation of an agent-state based policy

The performance of any agent-state based policy can be evaluated via a slight generalization of "cross-product MDP" method originally presented in [14]. This method has been rediscovered in slightly different forms multiple times [13, 12, 11, 15].

The key intuition is Lem. 1. Thus, for any agent-state based policy, \(\{(S_{t},Z_{t})\}_{t\geq 1}\) is a Markov chain. The only difference in our setting is that the Markov chain is time-periodic. Thus, for any periodic agent-state based policy \((\pi^{0},\dots,\pi^{L-1})\), we can identify the periodic rewards \((\bar{r}^{0},\dots,\bar{r}^{L-1})\) and periodic dynamics \((\bar{P}^{0},\dots,\bar{P}^{L-1})\) (which depend on \(\pi\) but we are not carrying that dependence in our notation) as follows:

\[\bar{r}^{\ell}(s,z) =\sum_{a\in\mathsf{A}}\pi^{\ell}(a|z)r(s,a),\] \[\bar{P}^{\ell}(s^{\prime},z^{\prime}|s,z) =\sum_{(y,a)\in\mathsf{Y}\times\mathsf{A}}\pi^{\ell}(a|z)P(s^{ \prime},y^{\prime}|s,a)\mathds{1}_{\{z^{\prime}=\phi(z,y^{\prime},a)\}}.\]

We can then evaluate the performance of this time-periodic Markov chain via performance evaluation formulas for periodic MDPs (App. C). In particular, define

\[\tilde{r} =\bar{r}^{0}+\gamma\bar{P}^{0}\bar{r}^{1}+\dots+\gamma^{L-1}\bar{ P}^{0}\bar{P}^{1}\dots\bar{P}^{L-2}\bar{r}^{L-1},\] \[\tilde{P} =\bar{P}^{0}\bar{P}^{1}\dots\bar{P}^{L-1},\]

to be the \(L\)-step cumulative rewards and dynamics for the time-periodic Markov chain. Then define

\[\tilde{V}=(1-\gamma^{L}\tilde{P})^{-1}\tilde{r}.\]

Thus, \(\tilde{V}(s,z)\) gives the performance of periodic policy \(\pi\) when starting at initial state \((s,z)\). If the initial state is stochastic, we can average over the initial distribution.

## Appendix H Reproducibility information

The hyperparameters for the numerical experiments presented in Sec. 3 are shown in Table 3. The experiments were run on a computer cluster by running jobs that requested \(2\)-CPU nodes with \(<8\)GB memory. Each seed typically took less than \(10\) minutes to execute.

\begin{table}
\begin{tabular}{l l} \hline \hline Parameter & Value \\ \hline Training steps & \(10^{6}\) \\ Start learn rate & \(10^{-3}\) \\ End learn rate & \(10^{-5}\) \\ Learn rate schedule & Exponential \\ Exponential decay rate & 1.0 \\ Number of random seeds & 25 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters used in Ex. 1

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made have been proved theoretically in detail in the appendix. Illustrative examples have also been shown to provide more clarity on the ideas involved. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper includes the limitations in the discussion section (Sec. 5). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the theoretical results in an organized manner with numbered and cross-referenced equations, theorems, lemmas, assumptions etc. We have considered the proofs in rigorous detail and explain all the details in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The information needed to reproduce the main experimental results of the paper are disclosed in further detail in App. G and App. H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We intend to make the code open access after the review process is complete. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The main details are provided in the paper. Additional details are provided in the appendix (app. H). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The example mentioned in the main text is run with 25 random seeds and the median and interquantile range are plotted. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The relevant information about compute resources is provided in the appendix (App. H). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms with every aspect of the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The research conducted in this paper is foundational research that focuses on developing theoretical results and as such, does not directly have a societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA] Justification: Since the paper is based on foundational research, it does not pose any direct risks that require safeguarding. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.