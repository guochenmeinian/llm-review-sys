# Causal Discovery from Subsampled Time Series

with Proxy Variables

 Mingzhou Liu\({}^{1,2}\)  Xinwei Sun\({}^{3}\) Lingjing Hu\({}^{4}\)  Yizhou Wang\({}^{1,2,5}\)

\({}^{1}\) School of Computer Science, Peking University

\({}^{2}\) Center on Frontiers of Computing Studies (CFCS), Peking University

\({}^{3}\) School of Data Science, Fudan University

\({}^{4}\) Yanjing Medical College, Capital Medical University

\({}^{5}\) Institute for Artificial Intelligence, Peking University

Correspondence to sunxinwei@fudan.edu.cn

###### Abstract

Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, _i.e._, the frequency of measurement is much lower than that of causal influence. To overcome this problem, numerous methods have been proposed, yet either was limited to the linear case or failed to achieve identifiability. In this paper, we propose a constraint-based algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. Our observation is that the challenge of subsampling arises mainly from hidden variables at the unobserved time steps. Meanwhile, every hidden variable has an observed proxy, which is essentially itself at some observable time in the future, benefiting from the temporal structure. Based on these, we can leverage the proxies to remove the bias induced by the hidden variables and hence achieve identifiability. Following this intuition, we propose a proxy-based causal discovery algorithm. Our algorithm is nonparametric and can achieve full causal identification. Theoretical advantages are reflected in synthetic and real-world experiments. Our code is available at https://github.com/lmz123321/proxy_causal_discovery.

## 1 Introduction

Temporal systems are the primary subject of causal modeling in many sciences, such as pathology, neuroscience, and economics. For these systems, a common issue is the difficulty of collecting sufficiently refined timescale data. That is, we can only observe a _subsampled_ version of the true causal interaction. This can cause serious problems for causal identification, since full observation (_i.e._, causal sufficiency [1]) is believed to be an important condition for causal identification, and the violation of which (_e.g._, the existence of hidden variables) can induce strong bias [2].

**Example 1.1**.: Take the pathology study of Alzheimer's disease (AD) as an example. In AD, patients suffer from memory loss due to the atrophy of memory-related brain regions such as the Hippocamp [3]. To monitor such atrophy, the standard protocol is to perform a MRI examination on the brain every six months [4]. However, many studies have shown that the disease can progress much more rapidly than this [5, 6]. For this reason, when recovering the causal interactions between brain regions, those at the unobserved time steps constitute hidden variables and induce spurious edges in the causal graph, as shown in Figure 1 (b).

To resolve this problem, many attempts have been made to recover the correct causal relations from subsampled data. However, none of them achieved general identifiability results. For example, in[10, 11, 12], identifiability was achieved only for linear data. As for nonlinear data, only a small part of the causal information (_i.e._, an equivalence class) could be identified [13, 14, 15, 16]. Indeed, encountering such difficulties is no coincidence, considering the fact that there are many hidden variables in the system. Particularly, the hidden mediators prevent us from distinguishing the direct causation from possible mediation and hence only allow us to recover some ancestral information.

In this paper, we propose a constraint-based algorithm that can identify the entire causal structure from subsampled data, without any parametric assumption. Our departure here is that in time series, every hidden variable has an observed descendent, which is essentially itself at some observable time in the future1. Therefore, we can use the observed descendent as its _proxy variable_[17] and adjust for the bias induced by it. For this purpose, we first represent the ancestral information, which is composed of both direct causation and hidden mediation, with the Maximal Ancestral Graph (MAG) [18]. Then, to distinguish causation from mediation, we use the observed descendants of the hidden mediators as their proxy variables and adjust for their induced mediation bias. We show that our strategy can achieve a complete identification of the causal relations entailed in the time series. It is essentially nonparametric and only relies on the smoothness and completeness of the structural equation to conduct valid proxy-based adjustment [19].

Footnote 1: For example, the hidden variable \(X(t_{u})\) at the unobserved time step \(t_{u}\) with \(X(t_{u})\to X(t_{u}+1)\to X(t_{u}+2)\rightarrow\cdots\rightarrow X(t_{o})\) has an observable descendent \(X(t_{o})\) at some observable time \(t_{o}\) in the future.

Back to the AD example, as shown in Figure 1, our method can recover causal pathways that align well with existing clinical studies [8, 9], while compared baselines fail to do so.

Our contributions are summarized below:

1. We establish nonparametric causal identification in subsampled time series.
2. We propose an algorithm to practically recover the causal graph.
3. We achieve more accurate causal identification than others on synthetic and real data.

The rest of the paper is organized as follows. First, in Section 2, we introduce the problem setting, basic assumptions, and related literature needed to understand our work. Second, in Section 3, we establish theories on structural identifiability. Then, based on these theories, in Section 4, we introduce the proposed causal discovery algorithm and verify its effectiveness in Section 5. Finally, we conclude the paper and discuss future works in Section 6.

Figure 1: Recovered causal pathways in Alzheimer’s disease, with (a) our method that explicitly models the subsampling and (b) Dynotears [7] that neglects subsampling. Spurious edges that contradict clinical studies [8, 9] are marked red. Please refer to Section 5.2 and Figure 7 for details.

## 2 Preliminary

We start by introducing the causal framework and related literature needed to understand our work.

**Time series model.** Let \(\mathbf{X}(t):=[X_{1}(t),...,X_{d}(t)]\) be a multivariate time series with \(d\) variables defined at discrete time steps \(t=1,...,T\). We assume the data is generated by a first-order structural vector autoregression (SVAR) process [16]:

\[X_{i}(t)=f_{i}(\mathbf{PA}_{i}(t-1),N_{i}),\] (1)

where \(f_{i}\) is the structural function, \(\mathbf{PA}_{i}\) is the parent of \(X_{i}\), and \(N_{i}\) is the exogenous noise.

Implicit in (1) is the assumptions that cause precedes effect, and that causation is invariant to time, _i.e._, the structural function \(f_{i}\) and the causal parents \(\mathbf{PA}_{i}\) keeps unchanged across time steps. These assumptions carry the fundamental beliefs of temporal precedence and stability on causality [1], therefore are widely adopted by existing works [10; 11; 12; 14; 16].

**Subsampling.** The subsampling problem means that model (1) can be only observed every \(k\) steps [10; 12; 16]. That is, we can only observe \(\mathbf{X}(1),\mathbf{X}(k+1),...,\mathbf{X}([\frac{T}{k}]k+1)\).

**Graph terminologies.** Causal relations entailed in model (1) can be represented by causal graphs (Figure 2). We first introduce the _full time Directed Acyclic Graph (DAG)_[20], which provides a complete description of the dynamics in the system.

**Definition 2.1** (Full time DAG).: Let \(G:=(\mathbf{V},\mathbf{E})\) be the associated full time DAG of model (1). The vertex set \(\mathbf{V}:=\{\mathbf{X}(t)\}_{t=1}^{T}\), the edge set \(\mathbf{E}\) contains \(X_{i}(t-1)\to X_{j}(t)\) iff \(X_{i}\in\mathbf{PA}_{j}\).

We assume the full time DAG is Markovian and faithful to the joint distribution \(\mathbb{P}(\mathbf{X}(1),...,\mathbf{X}(T))\).

**Assumption 2.2** (Markovian and faithfulness).: For disjoint vertex sets \(\mathbf{A},\mathbf{B},\mathbf{Z}\subseteq\mathbf{V}\), \(\mathbf{A}\perp\mathbf{B}|\mathbf{Z}\Leftrightarrow\mathbf{A}\perp_{G}\mathbf{ B}|\mathbf{Z}\), where \(\perp_{G}\) denotes \(d\)-separation in \(G\).

In practice, it is often sufficient to know the causal relations between time series as a whole, without knowing precisely the relations between time instants [21]. In this regard, we can summarize the causal relations with the _summary graph_[20].

**Definition 2.3** (Summary graph).: Let \(G:=(\mathbf{V},\mathbf{E})\) be the associated summary graph of model (1). The vertex set \(\mathbf{V}:=\mathbf{X}\), the edge set \(\mathbf{E}\) contains \(X_{i}\to X_{j}(i\neq j)\) iff \(X_{i}\in\mathbf{PA}_{j}\).

In this paper, our goal is to recover the summary graph, given data observed at \(t=1,k+1,...,\lfloor\frac{T}{k}\rfloor k+1\).

For this purpose, we need a structure to represent the (marginal) causal relations between observed variables and hence link the observation distribution to the summary graph. Here, we use the _Maximal Ancestral Graph (MAG)_[22], since it can represent casual relations when unobserved variables exist.

Specifically, for the variable set \(\mathbf{V}:=\{\mathbf{X}(t)\}_{t=1}^{T}\), let \(\mathbf{O}:=\{\mathbf{X}(1),...,\mathbf{X}([\frac{T}{k}]k+1)\}\) be the observed subset and \(\mathbf{L}:=\mathbf{V}\backslash\mathbf{O}\) be the unobserved subset. Let \(\mathbf{An}(A)\) be the ancestor set of \(A\). Then, given any full time DAG \(G\) over \(\mathbf{V}\), the corresponding MAG \(M_{G}\) over \(\mathbf{O}\) is defined as follows:

Figure 2: Graph terminologies: (a) Summary graph. (b) Full time DAG with subsampling factor \(k=2\). Variables at observed time steps \(t_{1},t_{3},t_{5}\) are marked gray, while those at unobserved time steps \(t_{2},t_{4}\) are dashed. (c) MAG over observed variables. The edges \(A(t_{1})\to M(t_{3})\) and \(A(t_{3})\leftrightarrow M(t_{3})\) are (_resp._) induced by the inducing paths \(A(t_{1})\to A(t_{2})\to M(t_{3})\) and \(A(t_{3})\gets A(t_{2})\to M(t_{3})\) in the full time DAG. Please refer to the appendix for detailed explanations.

[MISSING_PAGE_EMPTY:4]

Next, we first introduce Proposition 3.1, which shows that the MAG is identifiable.

**Proposition 3.1** (Identifiability of the MAG).: _Assuming model (1) and Assumption 2.2, then the MAG over the observed variable set \(\mathbf{O}\) is identifiable, i.e., its skeleton and edge orientations can be uniquely derived from the joint distribution \(\mathbb{P}(\mathbf{O})\)._

Since the identifiability of the MAG's skeleton under the faithfulness assumption is a well-known result [18], we focus on explaining the identification of edge orientations. Specifically, we will divide the edges into two classes: the instantaneous edges and the lagged edges, and discuss their orientations respectively.

The _instantaneous edge_, _e.g._, \(A(t_{3})\leftrightarrow M(t_{3})\), is an edge that connects two vertices at the same time. Since we assume cause must precede effect, the instantaneous edge does not represent causation but latent confounding, therefore is bidirectional according to Definition 2.4. For example, in Figure 2 (c), the instantaneous edge \(A(t_{3})\leftrightarrow M(t_{3})\) represents the latent confounding \(A(t_{3})\gets A(t_{2})\to M(t_{3})\) between \(A(t_{3})\) and \(M(t_{3})\). On the other hand, the _lagged edge_, _e.g._, \(A(t_{1})\to M(t_{3})\), is an edge that connects two vertices at different time. The lagged edge represents ancestral information and hence is directed (\(\rightarrow\)) from the past to the future.

To connect the identified MAG to the summary graph, we first define the following graph structures, examples of which are shown in Figure 3:

**Definition 3.2** (Graph structures).: In the summary graph,

1. A _directed path_\(p_{AB}\) from \(A\) to \(B\) with length \(l\) is a sequence of distinct vertices \(A,V_{1},...,V_{l},B\) where each vertex points to its successor. Two directed paths \(p_{AB_{1}},p_{AB_{2}}\) are called _disjoint_ if they do not share any non-startpoint vertex.
2. A _confounding structure_\(c_{AB}\) between \(A\) and \(B\) with lengths \((r,q)\) consists of a vertex \(U\), a directed path \(p_{UA}\) from \(U\) to \(A\) with length \(r\), and a directed path \(p_{UB}\) from \(U\) to \(B\) with length \(q\), where \(p_{UA}\) and \(p_{UB}\) are disjoint.

In the following, we explain the relationship between the identified MAG and the summary graph. In particular, we will discuss what the directed and bidirected edges in the MAG (_resp._) imply about the summary graph.

According to Definition 2.4, the _directed edge_\(A(t)\to B(t+k)\) in the MAG means \(A\) is the ancestor of \(B\). Therefore, in the summary graph, there is either \(A\to B\) or a directed path from \(A\) to \(B\). On the other hand, the _bidirected edge_\(A(t+k)\leftrightarrow B(t+k)\) in the MAG means there is a latent confounder between them, which can be either \(A(t+1)\), _i.e._, \(A(t+k)\leftarrow\cdots\gets A(t+1)\rightarrow\cdots\to B(t+k)\), or a third variable \(U(t+1)\), _i.e._, \(A(t+k)\leftarrow\cdots\gets U(t+1)\rightarrow\cdots\to B(t+k)\). Hence, in the summary graph, there is either a directed path from \(A\) to \(B\) or a confounding structure between them.

Summarizing the above observations, we have the following proposition:

**Proposition 3.3** (MAG to summary graph).: _If there are \(A(t)\to B(t+k)\) and \(A(t+k)\leftrightarrow B(t+k)\) in the MAG, then, in the summary graph, there is either_

1. \(A\to B\)_,_ 2. _a directed path from_ \(A\) _to_ \(B\) _with length_ \(l\leq k-2\)_, or_
2. _a directed path from_ \(A\) _to_ \(B\) _with length_ \(l=k-1\) _and a confounding structure between them with lengths_ \((r\leq k-2,q\leq k-2)\)

Figure 3: Illustration of Definition 3.2. (a) A directed path from \(A\) to \(B\) with length \(l\). (b) A confounding structure between \(A\) and \(B\) with lengths \((r,q)\).

_Remark 3.4_.: In Proposition 3.3, the maximum length of the directed path is \(k-1\), this is because any path longer than this can not be an inducing path and therefore can not induce \(A(t)\to B(t+k)\) in the MAG. Please refer to the appendix for details.

Proposition 3.3 provides a necessary condition for having \(A\to B\) in the summary graph, _i.e._, there are both \(A(t)\to B(t+k)\) and \(A(t+k)\leftrightarrow B(t+k)\) in the MAG. It also inspires us that, to make the condition sufficient, we need to distinguish the direct effect (\(A\to B\)) from the indirect one (\(A\rightarrow\cdots\to B\)).

For this purpose, we propose to use the proximal causal discovery method. Before preceding any technical detail, we first brief our idea below. The indirect effect, unlike the direct one, relies on mediation and therefore can be \(d\)-separated by the (unobserved) mediator. To test the \(d\)-separation, we can use the observed proxy of the mediator, thanks to the self causation assumption in Assumption 2.11. For example, in Figure 4 (b), the indirect effect \(A(t_{1})\to M(t_{2})\to B(t_{3})\) from \(A\) to \(B\) can be \(d\)-separated by the mediator \(M(t_{2})\), who has an observed proxy \(M(t_{3})\).

For the general case, the separation set is a bit more complicated to ensure all paths except the one representing direct effect are \(d\)-separated. Specifically, for two vertices \(A(t)\) and \(B(t+k)\), the separation set is the union of two sets \(\mathbf{M}(t+1)\cup\mathbf{S}(t)\). The set \(\mathbf{M}(t+1)\) contains possible mediators, namely \(A(t+1)\) and any vertex \(M_{i}(t+1)\) (\(M_{i}\)\(B\)) such that \(A(t)\to M_{i}(t+k)\) in the MAG and \(M_{i}\) is not \(B\)'s descendant3. The set \(\mathbf{S}(t)\) is used to \(d\)-separate possible back-door paths between \(A(t)\) and \(B(t+k)\), it contains any vertex \(S_{i}(t)\) (\(S_{i}\)\(\ast\)\(A\)) such that \(S_{i}(t)\to B(t+k)\) or \(S_{i}(t)\to M_{j}(t+k)\) for some \(M_{j}\in\mathbf{M}\) in the MAG.

Footnote 3: This can be justified from the MAG. Specifically, if \(M_{i}\) is the descendant of \(B\), then there is \(B(t)\to V_{1}(t+k),V_{1}(t)\to V_{2}(t+k),...,V_{l}(t)\to M_{i}(t+k)\) in the MAG.

Equipped with the separation set, we then have the following identifiability result:

**Theorem 3.5** (Identifiability of the summary graph).: _Assuming model (1), Assumption 2.2, and Assumptions 2.6, 2.7, 2.11, then the summary graph is identifiable. Specifically,_

1. _There is_ \(A\to B\) _in the summary graph iff there are_ \(A(t)\to B(t+k),A(t+k)\leftrightarrow B(t+k)\) _in the MAG, and the set_ \(\mathbf{M}(t+1)\cup\mathbf{S}(t)\) _is not sufficient to_ \(d\)_-separate_ \(A(t),B(t+k)\) _in the full time DAG._
2. _The condition "the set_ \(\mathbf{M}(t+1)\cup\mathbf{S}(t)\) _is not sufficient to_ \(d\)_-separate_ \(A(t),B(t+k)\) _in the full time DAG" can be tested by the proxy variable_ \(\mathbf{M}(t+k)\) _of the unobserved set_ \(\mathbf{M}(t+1)\)_._

Figure 4: Distinguishing the direct effect (\(A\to B\)) from the indirect one (\(A\rightarrow\cdots\to B\)) with proxy variables. Note that though (a) and (b) have the same MAG, only the indirect effect \(A(t_{1})\to M(t_{2})\to B(t_{3})\) in (b) can be \(d\)-separated by \(M(t_{2})\). To test the \(d\)-separation, we can use \(M(t_{3})\) as the proxy variable.

Theorem 3.5 provides a necessary and sufficient condition for having \(A\to B\) in the summary graph. This condition is testable with the proxy \(\mathbf{M}(t+k)\) of the unobserved set \(\mathbf{M}(t+1)\) (\(\mathbf{S}(t)\) is observable). The validity of such proxying, _i.e._, \(A(t)\perp_{G}\mathbf{M}(t+k)|\mathbf{M}(t+1),\mathbf{S}(t)\) is proved in the appendix.

## 4 Discovery algorithm

Equipped with the identifiability results in Proposition 3.1 and Theorem 3.5, we can practically discover the summary graph with Algorithm 1. Specifically, given the observed data points, Algorithm 1 first recovers the MAG according to Proposition 3.1. Then, for each pair of vertices in the summary graph, it first uses the necessary condition of "having \(A(t)\to B(t+k)\) and \(A(t+k)\leftrightarrow B(t+k)\) in the MAG" to construct underdetermined edges. Finally, among these underdetermined edges, it uses the proxy variable approach to identify direct causation and remove the indirect one, according to Theorem 3.5.

Below, we first introduce the _Partially Determined Directed Acyclic Graph (PD-DAG)_, which will be used to represent the intermediate result of the algorithm.

**Definition 4.1** (PD-DAG).: A PD-DAG is a DAG with two kinds of directed edges: solid (\(\rightarrow\)) and dashed (\(\dashdot\)). A solid edge represents a determined causal relation, while a dashed one means the causal relation is still underdetermined. We use a meta-symbol, asterisk (\(\star\)\(\rightarrow\)), to denote any of the two edges. In a PD-DAG \(G=(\mathbf{V},\mathbf{E})\), the vertex \(A\) points to the vertex \(B\) if the edge \(A\)\(\star\to B\in\mathbf{E}\). The definition of the directed path and confounding structure are the same as in the summary graph.

``` Data: Data observed at \(t=1,k+1,...,\lfloor\frac{T}{k}\rfloor k+1\) Result: The summary graph
1 Construct the skeleton of the MAG with [26], orient edges according to Proposition 3.1;
2 For every vertex pair \(A,B\), set \(A\)\(\leftarrow\)\(B\) if there are \(A(t)\)\(\rightarrow\)\(B(t+k)\) and \(A(t+k)\)\(\leftrightarrow\)\(B(t+k)\) in the MAG. Called the resulted PD-DAG \(G_{0}\) ; /* Proposition 3.3 */ Set \(G\) = \(G_{0}\) and iteratively execute: 1. For every \(A\)\(\rightarrow\)\(B\), check 1. a directed path from \(A\) to \(B\) with length \(l\leq k-2\); 2. a directed path from \(A\) to \(B\) with length \(l\leq k-1\) and a confounding structure between \(A\) and \(B\) with lengths (\(r\leq k-2,q\leq k-2\)); 2. If \(\exists\) (i) or (ii), pass. Otherwise, set \(A\)\(\rightarrow\)\(B\) ; /* Proposition 3.3 */ 2. Randomly pick a dashed edge \(A\)\(\leftarrow\)\(B\), check whether \(A(t)\perp_{G}B(t+k)|\mathbf{M}(t+1),\mathbf{S}(t)\); 2. If the \(d\)-separation holds, remove \(A\)\(\leftrightarrow\)\(B\). Otherwise, set \(A\)\(\rightarrow\)\(B\) ; /* Theorem 3.5 */ until all edges in \(G\) are solid ones; return\(G\) ```

**Algorithm 1**Discover the summary graph

_Remark 4.2_.: Algorithm 1 can be generalized to cases where the subsampling factor \(k\) is unknown, by skipping the step (a) below line 3.

## 5 Experiment

In this section, we evaluate our method on synthetic data and a real-world application, _i.e._, discovering causal pathways in Alzheimer's disease.

**Compared baselines.**_Methods that account for subsampling_: 1. SVAR-FCI [16] that extended the FCI algorithm to time series and recovered a MAG over observed variables; 2. NG-EM [10] that achieved identifiability with linear non-Gaussianity and used Expectation Maximization (EM) for estimation. _Methods that neglect subsampling_: 1. Dynotears [7] that extended the score-based Notears [27] algorithm to the time series; 2. PC-GCE [21] that modified the PC algorithm for temporal data with a new information-theoretic conditional independence (CI) test.

**Metrics.** We use the \(\mathrm{F}_{1}\)-score, precision, and recall, where precision and recall (_resp._) measure the accuracy and completeness of identified causal edges, and \(\mathrm{F}_{1}:=2\cdot\frac{\mathrm{precision}\cdot\mathrm{recall}}{\mathrm{ precision}+\mathrm{recall}}\).

**Implementation details.** The significance level is set to \(0.05\). For the MAG recovery, we use the FCI algorithm implemented in the \(\mathrm{causallearn}\) package4. Temporal constraints, _e.g._, causal precedence, time invariance, and self causation are added as the background knowledge.

Footnote 4: https://github.com/py-why/causal-learn

### Synthetic study

**Data generation.** We generate radnom summary graphs with the Erdos-Renyi model [28], where the vertex number is set to \(5\), the probability of each edge is set to \(0.3\). For each graph, we generate temporal data with the structural equation \(X_{i}(t)=\sum_{j\in\mathbf{PA}_{i}}f_{ij}(X_{j}(t-1))+N_{i}\), where the function \(f_{ij}\) is randomly chosen from \(\{linear,sin,tanh,sqrt\}\), the exogenous noise \(N_{i}\) is randomly sampled from \(\{uniform,gauss.,exp.,gamma\}\). We consider different subsampling factors \(k=\{2,3,4,5\}\) and sample sizes \(n=\{600,800,1000,1200\}\). For each setting5, we replicate over \(100\) random seeds.

Footnote 5: We also consider different graph scales \(d=\{5,15,25,35,45\}\). Please refer to the appendix for details.

**Comparison with baselines.** Figure 5 shows the performance of our method and baselines under different subsampling factors (upper row) and sample sizes (lower row). As we can see, our method significantly outperforms the others in all settings. Specifically, compared with methods that account for subsampling (SVAR-FCI and NG-EM), our method achieves both higher precision and recall, indicating less detection error and missing edges. This advantage can be attributed to the fact that our method enjoys an identifiability guarantee (_v.s._ SVAR-FCI) and meanwhile requires no parametric assumption on the structural model (_v.s._ NG-EM). Compared with methods that ignore subsampling (Dynotears and PC-GCE), our method achieves higher \(\mathrm{F}_{1}\)-score, precision, and is comparable in recall. This result shows that our method can effectively reduce the spurious detection induced by unobserved time steps. Besides, we can observe that Dynotears and PC-GCE slightly outperform SVAR-FCI and NG-EM, which again demonstrates the necessity of establishing nonparametric identification in subsampling problems.

**Intermediate results.** We evaluate the intermediate results of Algorithm 1 (MAG, edges identified by proxies, summary graph) and report the results in Figure 6. As shown, both the recovered MAG and edges identified by proxies have high accuracy under moderate subsampling factors and sample sizes, hence explaining the effectiveness of our algorithm in recovering the summary graph. Meanwhile, we can also observe that the performance slightly decreases when the subsampling factor \(k\) is large. This

Figure 5: Performance of our method and baselines under different subsampling factors (upper row) and sample sizes (lower row).

is explained as follows. When \(k\) is large, there are many unobserved steps between two observations, which weakens the correlation pattern in data and therefore breaks our faithfulness assumption.

### Discovering causal pathways in Alzheimer's disease

**Background.** Alzheimer's disease (AD) is one of the most common neuro-degenerative diseases. In AD, patients suffer from memory loss due to atrophy of memory-related brain regions, such as the Hippocamp [3] and Temporal lobe [29]. A widely accepted explanation for such atrophy is: the disease releases toxic proteins, _e.g._, \(A\beta\)[30] and \(tau\)[31]; along anatomical pathways, these proteins spread from one brain region to another, eventually leading to atrophy of the whole brain [32]. Recovering these anatomical pathways, _i.e._, underlying causal mechanisms, will benefit the understanding of AD pathology and inspire potential treatment methods.

**Dataset and preprocessing.** We consider the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset [4], in which the imaging data is acquired from structural Magnetic Resonance Imaging (sMRI) scans. We apply the Dartel VBM [33] for preprocessing and the Statistical Parametric Mapping (SPM) [34] for segmenting brain regions. Then, we implement the Automatic Anatomical Labeling (AAL) atlas [35] to partition the whole brain into \(90\) regions. In total, we use \(n=558\) subjects with baseline and month-6 follow-up visits enrolled in ADNI-GO/1/2/3 periods.

**Results.** Figure 7 shows the recovered summary graph over \(90\) brain regions. For illustration, in Figure 1 (a), we further group the brain regions into \(8\) meta-regions according to their anatomical structures. We can observe that **i)** most causal pathways between meta-regions are unidirectional; **ii)** the identified sources of atrophy are the Hippocamp, Amygdala, and Temporal lobe, which are all early-degenerate regions [3; 29]; **iii)** the topology order induced from our result, _i.e._, \(\{Hip.,Amy.,Tem.,Cin.,Ins.,Fro.,Par.,Occ.\}\) coincides with the temporal degeneration order found in [8; 9]. These results constitute an important finding that can be supported by existing studies: the atrophy (releasing of toxic proteins) is sourced from the Hippocamp and gradually propagates to other brain structures along certain anatomical pathways [32; 36].

In contrast, in Figure 1 (b), the causal relations recovered by the Dynotears baseline6 are less clear. For example, most of the identified interactions are bidirectional, which may be due to the spurious edges induced by subsampling. Besides, the identified atrophy source is the Frontal lobe, with AD-related regions identified as outcomes, which is inconsistent with clinical studies. These results, from another perspective, show the importance of modeling subsampling in time series causal discovery.

Footnote 6: Please refer to the appendix for results of the other baselines.

Figure 6: Evaluation of intermediate results. We report \(F_{1}\)-score, precision, and recall of the recovered MAG, edges identified by proxies, and the recovered summary graph.

## 6 Conclusion

In this paper, we propose a causal discovery algorithm for subsampled time series. Our method leverages the recent progress of proximal causal discovery and can achieve complete identifiability without any parametric assumption. The proposed algorithm can outperform baselines on synthetic data and recover reasonable causal pathways in Alzheimer's disease.

**Limitation and future work.** Our method relies on an accurate test of the conditional independence (CI) and therefore may suffer from low recall when the CI patterns in data are weak. To solve this problem, we will investigate theories on high-efficiency CI tests and pursue a dedicated solution.

## Acknowledgments

This work was supported by National Key R&D Program of China (2022ZD0114900).

## References

* [1] Judea Pearl. _Causality_. Cambridge University Press, 2009.
* [2] Patrick Suppes. Causal analysis of hidden variables. In _PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association_, volume 1980, pages 563-571. Cambridge University Press, 1980.
* [3] Mark J West, Paul D Coleman, Dorothy G Flood, and Juan C Troncoso. Differences in the pattern of hippocampal neuronal loss in normal ageing and alzheimer's disease. _The Lancet_, 344(8925):769-772, 1994.
* [4] Ronald Carl Petersen, Paul S Aisen, Laurel A Beckett, Michael C Donohue, Anthony Collins Gamst, Danielle J Harvey, Clifford R Jack, William J Jagust, Leslie M Shaw, Arthur W Toga, et al. Alzheimer's disease neuroimaging initiative (adni): clinical characterization. _Neurology_, 74(3):201-209, 2010.
* [5] Rachelle Smith Doody, Paul Massman, and J Kay Dunn. A method for estimating progression rates in alzheimer disease. _Archives of neurology_, 58(3):449-454, 2001.
* [6] Craig J Thalhauser and Natalia L Komarova. Alzheimer's disease: rapid and slow progression. _Journal of the Royal Society Interface_, 9(66):119-126, 2012.
* [7] Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series

Figure 7: Recovered summary graph in Alzheimer’s disease.

data. In _International Conference on Artificial Intelligence and Statistics_, pages 1595-1605. PMLR, 2020.
* [8] Alexandra L Young, Razvan V Marinescu, Neil P Oxtoby, Martina Bocchetta, Keir Yong, Nicholas C Firth, David M Cash, David L Thomas, Katrina M Dick, Jorge Cardoso, et al. Uncovering the heterogeneity and temporal complexity of neurodegenerative diseases with subtype and stage inference. _Nature communications_, 9(1):4273, 2018.
* [9] Jacob W Vogel, Alexandra L Young, Neil P Oxtoby, Ruben Smith, Rik Ossenkoppele, Olof T Strandberg, Renaud La Jotie, Leon M Aksman, Michel J Grothe, Yasser Iturria-Medina, et al. Four distinct trajectories of tau deposition identified in alzheimer's disease. _Nature medicine_, 27(5):871-881, 2021.
* [10] Mingming Gong, Kun Zhang, Bernhard Schoelkopf, Dacheng Tao, and Philipp Geiger. Discovering temporal causal relations from subsampled data. In _International Conference on Machine Learning_, pages 1898-1906. PMLR, 2015.
* [11] Mingming Gong, Kun Zhang, Bernhard Scholkopf, Clark Glymour, and Dacheng Tao. Causal discovery from temporally aggregated time series. In _Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence_, volume 2017. NIH Public Access, 2017.
* [12] Alex Tank, Emily B Fox, and Ali Shojaie. Identifiability and estimation of structural vector autoregressive models for subsampled and mixed-frequency time series. _Biometrika_, 106(2):433-452, 2019.
* [13] David Danks and Sergey Plis. Learning causal structure from undersampled time series. 2013.
* [14] Sergey Plis, David Danks, Cynthia Freeman, and Vince Calhoun. Rate-agnostic (causal) structure learning. _Advances in neural information processing systems_, 28, 2015.
* [15] Antti Hyttinen, Sergey Plis, Matti Jarvisalo, Frederick Eberhardt, and David Danks. Causal discovery from subsampled time series data by constraint optimization. In _Conference on Probabilistic Graphical Models_, pages 216-227. PMLR, 2016.
* [16] Daniel Malinsky and Peter Spirtes. Causal structure learning from multivariate time series in settings with unmeasured confounding. In _Proceedings of 2018 ACM SIGKDD workshop on causal discovery_, pages 23-47. PMLR, 2018.
* [17] Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference. _Biometrika_, 101(2):423-437, 2014.
* [18] P Spirtes, C Meek, and T Richardson. Computation, causation and discovery, 1999.
* [19] Mingzhou Liu, Xinwei Sun, Yu Qiao, and Yizhou Wang. Causal discovery with unobserved variables: a proxy variable approach. _arXiv preprint arXiv:2305.05281_, 2023.
* [20] Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, and Jingping Bi. Causal discovery from temporal data: An overview and new perspectives. _arXiv preprint arXiv:2303.10112_, 2023.
* [21] Charles K Assaad, Emilie Devijver, and Eric Gaussier. Discovery of extended summary graphs in time series. In _Uncertainty in Artificial Intelligence_, pages 96-106. PMLR, 2022.
* [22] Jiji Zhang. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. _Artificial Intelligence_, 172(16-17):1873-1896, 2008.
* [23] Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal effects with proxy variables of an unmeasured confounder. _Biometrika_, 105(4):987-993, 2018.
* [24] Maurice S Bartlett. On the theoretical specification and sampling properties of autocorrelated time-series. _Supplement to the Journal of the Royal Statistical Society_, 8(1):27-41, 1946.
* [25] ZA Lomnicki and SK Zaremba. On the estimation of autocorrelation in time series. _The Annals of Mathematical Statistics_, 28(1):140-158, 1957.
* [26] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. _Causation, prediction, and search_. MIT press, 2000.
* [27] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. _Advances in neural information processing systems_, 31, 2018.

* Erdos et al. [1960] Paul Erdos, Alfred Renyi, et al. On the evolution of random graphs. _Publ. Math. Inst. Hung. Acad. Sci_, 5(1):17-60, 1960.
* Jack et al. [1998] Clifford R Jack, Ronald C Petersen, Yuecheng Xu, Peter C O'Brien, Glenn E Smith, Robert J Ivnik, Eric G Tangalos, and Emre Kokmen. Rate of medial temporal lobe atrophy in typical aging and alzheimer's disease. _Neurology_, 51(4):993-999, 1998.
* LaFerla et al. [2007] Frank M LaFerla, Kim N Green, and Salvatore Oddo. Intracellular amyloid-\(\beta\) in alzheimer's disease. _Nature Reviews Neuroscience_, 8(7):499-509, 2007.
* Kolarova et al. [2012] Michala Kolarova, Francisco Garcia-Sierra, Ales Bartos, Jan Ricny, and Daniela Ripova. Structure and pathology of tau protein in alzheimer disease. _International journal of Alzheimer's disease_, 2012, 2012.
* Bloom [2014] George S Bloom. Amyloid-\(\beta\) and tau: the trigger and bullet in alzheimer disease pathogenesis. _JAMA neurology_, 71(4):505-508, 2014.
* Ashburner [2010] John Ashburner. Vbm tutorial. _Wellcome Trust Centre for Neuroimaging, London, UK_, 2010.
* Friston [2003] Karl J Friston. Statistical parametric mapping. _Neuroscience databases: a practical guide_, pages 237-250, 2003.
* Tzourio-Mazoyer et al. [2002] Nathalie Tzourio-Mazoyer, Brigitte Landeau, Dimitri Papathanassiou, Fabrice Crivello, Octave Etard, Nicolas Delcroix, Bernard Mazoyer, and Marc Joliot. Automated anatomical labeling of activations in spm using a macroscopic anatomical parcellation of the mni mri single-subject brain. _Neuroimage_, 15(1):273-289, 2002.
* Liu et al. [2023] Mingzhou Liu, Xiangyu Zheng, Xinwei Sun, Fang Fang, and Yizhou Wang. Which invariance should we transfer? A causal minimax learning approach. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 22488-22527. PMLR, 23-29 Jul 2023.