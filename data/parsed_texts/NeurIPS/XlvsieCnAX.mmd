# Exact Representation of Sparse Networks with

Symmetric Nonnegative Embeddings

Sudhanshu Chanpuriya\({}^{1}\), Ryan A. Rossi\({}^{2}\), Anup Rao\({}^{2}\), Tung Mai\({}^{2}\),

Nedim Lipka\({}^{2}\), Zhao Song\({}^{2}\), and Cameron Musco\({}^{3}\)

\({}^{1}\)University of Illinois Urbana-Champaign, schariya@illinois.edu

\({}^{2}\)Adobe Research, {ryrossi,anuproa,tumai,lipka,zsong}@adobe.com

\({}^{3}\)University of Massachusetts Amherst, cmusco@cs.umass.edu

###### Abstract

Graph models based on factorization of the adjacency matrix often fail to capture network structures related to links between dissimilar nodes (heterophily). We introduce a novel graph factorization model that leverages two nonnegative vectors per node to interpretably account for links between both similar and dissimilar nodes. We prove that our model can exactly represent any graph with low _arboricity_, a property that many real-world networks satisfy; our proof also applies to related models but has much greater scope than the closest prior bound, which is based on low _max degree_. Our factorization also has compelling properties besides expressiveness: due to its symmetric structure and nonnegativity, fitting the model inherently finds node communities, and the model's link predictions can be interpreted in terms of these communities. In experiments on real-world networks, we demonstrate our factorization's effectiveness on a variety of tasks, including community detection and link prediction.

## 1 Introduction

Graphs data naturally arises in a variety of fields including sociology (Mason & Verwoerd, 2007), biology (Scott, 1988), and computer networking (Bonato, 2004). A key task in machine learning for graph data is forming models of graphs that can predict edges between nodes, form useful representations of nodes, and reveal interpretable structure in the graph, such as detecting clusters of nodes. Many graph models fall under the framework of edge-independent graph generative models, which output the probabilities of edges existing between any pair of nodes. The parameters of such models can be trained iteratively on the network, or some fraction of the network which is known, in the link prediction task, i.e., by minimizing a predictive loss. To choose among these models, one must consider two criteria: 1) whether the model can express structures of interest in the graph, 2) whether the model expresses these structures in an interpretable way.

Expressiveness of low-dimensional embeddingsAs real-world graphs are high-dimensional objects, graph models generally compress information about the graph. For example, dot product models associate each node with a real-valued "embedding" vector; the predicted probability of a link between two nodes increases with the similarity of their embeddings. These models can alternatively be seen as factorizing the graph's adjacency matrix to approximate it with a low-rank matrix. Recent work of Seshadhri et al. (2020) has shown that dot product models are limited in their ability to model common structures in real-world graphs, such as triangles incident only on low-degree nodes. In response, Chanpuriya et al. (2020) showed that with the logistic principal components analysis (LPCA) model, which has two embeddings per node (i.e., using the dot product of the 'left' embedding of one node and the 'right' embedding of another), not only can such structures be represented, but further, any graph can be exactly represented with embedding vectors whose lengths are linear in themax degree of the graph. There are two keys to this result. First is the presence of a nonlinear linking function in LPCA; since adjacency matrices are generally not low-rank, exact low-rank factorization is impossible without a linking function. Second is that having two embeddings rather than one allows for expression of non-positive semidefinite (PSD) matrices. As discussed in Peysakhovich & Bottou (2021), that the single-embedding models can only represent PSD matrices precludes representation of 'heterophilous' structures in graphs; heterophilous structures are those wherein dissimilar nodes are linked, in contrast to more intuitive 'homophilous' linking between similar nodes.

Interpretability and node clusteringBeyond being able to capture a given network accurately, it is often desirable for a graph model to form interpretable representations of nodes and to produce edge probabilities in an interpretable fashion. Dot product models can achieve this by restricting the node embeddings to be nonnegative. Nonnegative factorization has long been used to decompose data into parts (Donoho & Stodden, 2003). In the context of graphs, this entails decomposing the set of nodes of the network into clusters or communities. In particular, each entry of the nonnegative embedding vector of a node represents the intensity with which the node participates in a community. This allows the edge probabilities output by dot product models to be interpretable in terms of coparicipation in communities. Depending on the model, these vectors may have restrictions such as a sum-to-one requirement, meaning the node is assigned a categorical distribution over communities. The least restrictive and most expressive case is that of soft assignments to overlapping communities, where the entries can vary totally independently. In such models, which include the BigClam model of Yang & Leskovec (2013), the output of the dot product may be mapped through a nonlinear link function (as in LPCA) to produce a probability for each edge, i.e., to ensure the values lie in \([0,1]\).

Summary of contributionsThe key contributions of this work are as follows:

* We prove that LPCA admits exact low-rank factorizations of graphs with bounded _arboricity_, which is the minimum number of forests into which a graph's edges can be partitioned. By the Nash-Williams theorem, arboricity is a measure of a graph's density in that, letting \(S\) denote an induced subgraph and \(n_{S}\) and \(m_{S}\) denote the number of nodes and edges in \(S\), arboricity is the maximum over all \(S\) of \(\lceil\frac{m_{S}}{n_{S}-1}\rceil\). Our result is more applicable to real-world graphs than the prior result of Chanpuruis et al. (2020) for graphs with bounded max degree, since sparsity is a common feature of real networks, whereas low max degree is not.
* We introduce a graph model that extends LPCA and is both highly expressive and interpretable. Our model incorporates two embeddings per node and a nonlinear linking function, and hence is able to express both heterophily and overlapping communities. At the same time, our model is based on symmetric nonnegative matrix factorization, so it outputs link probabilities that are interpretable in terms of the communities it detects. While prior graph factorizations incorporate some aspects of nonnegativity, heterophily, and/or nonlinearity, our proposed model lies at the intersection of all three.
* We show how any graph with a low-rank factorization in the LPCA model also admits a low-rank factorization in our community-based model. This means that the guarantees on low-rank representation for bounded max degree and arboricity also apply to our model.
* In experiments, we show that our method is competitive with and often outperforms other comparable models on real-world graphs in terms of representing the network, doing interpretable link prediction, and detecting communities that align with ground-truth.

## 2 Motivating Example

To demonstrate how heterophily can manifest in networks, as well as how models which assume homophily can fail to represent such networks, we provide a simple synthetic example involving a graph of matches between users of a dating app. Suppose that users of this app are generally seeking partners of a different gender (to simplify this example, we assume that each user of the app is either a man or a woman), and suppose that each user comes from one of ten cities. Members from the same city are likely to match with each other; this typifies homophily, wherein links occur between similar nodes. Furthermore, users having the same gender are unlikely to match with each other; this typifies heterophily. Figure 1 shows an instantiation of such an adjacency matrix with \(1000\) nodes, which are randomly assigned to man or woman and to one of the ten cities. We recreate this network with our proposed embedding model and with BigClam, which explicitly assumeshomophily. We also compare with the SVD of the adjacency matrix, which outputs the best (lowest Frobenius error) low-rank approximation that is possible without a nonlinear linking function. Since SVD lacks nonnegativity constraints on the factors, we do not expect intepretability. In Figure 2, we show how BigClam captures only the ten communities based on city, i.e., only the homophilous structure, and fails to capture the heterophilous distinction between men and women. We also plot the error of the reconstructions as the embedding length increases. There are \(10\cdot 2=20\) different kinds of nodes, meaning the expected adjacency matrix is rank-\(20\), and our model maintains the lowest error up to this embedding length; by contrast, BigClam is unable to decrease error after capturing city information with length-\(10\) embeddings. In Figure 3, we visualize the features generated by the three methods, i.e., the factors returned by each factorization. Our model's factors capture the relevant latent structure in an interpretable way. By contrast, SVD's factors are harder to interpret, and BigClam does not represent the heterophilous structure.

Figure 1: The motivating synthetic graph. The expected adjacency matrix (left) and the sampled matrix (right); the latter, which is passed to the training algorithms, is produced by treating the entries of the former as parameters of Bernoulli distributions and sampling. The network is approximately a union of ten bipartite graphs, each of which correspond to men and women in one of the ten cities.

Figure 3: Factors resulting from decomposition of the motivating synthetic graph of Figure 1 with the three models, using 12 communities or singular vectors. The top/bottom rows represent the positive/negative eigenvalues corresponding to homophilous/heterophilous communities (note that BigClam does not include the latter). The homophilous factors from BigClam and our model reflect the 10 cities, and the heterophilous factor from our model reflect men and women. The factors from SVD are harder to interpret. Note that the order of the communities in the factors is arbitrary.

Figure 2: Left: Reconstructions of the motivating synthetic graph of Figure 1 with SVD, BigClam, and our model, using 12 communities or singular vectors. Note the lack of the small diagonal structure in BigClam’s reconstruction; this corresponds to its inability to capture the heterophilous interaction between men and women. Right: Frobenius error when reconstructing the motivating synthetic graph of Figure 1 with SVD, BigClam, and our model, as the embedding length is varied. The error is normalized by the sum of the true adjacency matrix (i.e., twice the number of edges).

## 3 Community-Based Graph Factorization

Consider the set of undirected, unweighted graphs on \(n\) nodes, i.e., the set of graphs with symmetric adjacency matrices in \(\{0,1\}^{n\times n}\). We propose an edge-independent generative model for such graphs. Given nonnegative parameter matrices \(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\) and \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\), we set the probability of an edge existing between nodes \(i\) and \(j\) to be the \((i,j)\)-th entry of matrix \(\tilde{\bm{A}}\):

\[\tilde{\bm{A}}:=\sigma(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}),\] (1)

where \(\sigma\) is the logistic function. Here \(k_{B}\), \(k_{C}\) are the number of homophilous/heterophilous clusters. Intuitively, if \(\bm{b}_{i}\in\mathbb{R}_{+}^{k_{B}}\) is the \(i\)-th row of matrix \(\bm{B}\), then \(\bm{b}_{i}\) is the affinity of node \(i\) to each of the \(k_{B}\) homophilous communities. Similarly, \(\bm{c}_{i}\in\mathbb{R}_{+}^{k_{C}}\) is the affinity of node \(i\) to the \(k_{C}\) heterophilous communities. As an equivalent statement, for each pair of nodes \(i\) and \(j\), \(\tilde{\bm{A}}_{i,j}:=\sigma(\bm{b}_{i}\bm{b}_{j}-\bm{c}_{i}\bm{c}_{j}^{\top})\). We will soon discuss the precise interpretation of this model, but the idea is roughly similar to the attract-repel framework of Peysakhovich & Bottou (2021). When nodes \(i\) and \(j\) have similar 'attractive' \(\bm{b}\) embeddings, i.e., when \(\bm{b}_{i}\bm{b}_{j}^{\top}\) is high, the likelihood of an edge between them increases, hence why the \(\bm{B}\) factor is homophilous. By contrast, the \(\bm{C}\) factor is'repulsive'/heterophilous since, when \(\bm{c}_{i}\bm{c}_{j}^{\top}\) is high, the likelihood of an edge between \(i\) and \(j\) decreases.

Alternate expressionWe note that the model above can also be expressed in a form that normalizes cluster assignments and is more compact, in that it combines the homophilous and heterophilous cluster assignments. Instead of \(\bm{B}\) and \(\bm{C}\), this form uses a matrix \(\bm{V}\in[0,1]^{n\times k}\) and a diagonal matrix \(\bm{W}\in\mathbb{R}^{k\times k}\), where \(k=k_{B}+k_{C}\) is the total number of clusters. In particular, let \(\bm{m}_{B}\) and \(\bm{m}_{C}\) be the vectors containing the maximums of each column of \(\bm{B}\) and \(\bm{C}\). By setting

\[\bm{V} =\left(\bm{B}\cdot\mathrm{diag}\left(\bm{m}_{B}^{-1}\right);\; \;\;\bm{C}\cdot\mathrm{diag}\left(\bm{m}_{C}^{-1}\right)\right)\] (2) \[\bm{W} =\mathrm{diag}\left(\left(+\bm{m}_{B}^{2};\;\;\;-\bm{m}_{C}^{2} \right)\right),\]

the constraint on \(\bm{V}\) is satisfied. Further, \(\bm{V}\bm{W}\bm{V}^{\top}=\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}\), so

\[\tilde{\bm{A}}:=\sigma(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})=\sigma(\bm{V} \bm{W}\bm{V}^{\top}).\] (3)

Here, if \(\mathbf{v}_{i}\in[0,1]^{k}\) is the \(i\)-th row of matrix \(\bm{V}\), then \(\mathbf{v}_{i}\) is the soft (normalized) assignment of node \(i\) to the \(k\) communities. The diagonal entries of \(\bm{W}\) represent the strength of the homophily (if positive) or heterophily (if negative) of the communities. For each entry, \(\tilde{\bm{A}}_{i,j}=\sigma(\mathbf{v}_{i}\bm{W}\mathbf{v}_{j}^{\top})\). We use these two forms interchangeably throughout this work.

InterpretationThe edge probabilities output by this model have an intuitive interpretation. Recall that there are bijections between probability \(p\in[0,1]\), odds \(o=\frac{p}{1-p}\in[0,\infty)\), and logit \(\ell=\log(o)\in(-\infty,+\infty)\). The logit of the link probability between nodes \(i\) and \(j\) is \(\mathbf{v}_{i}^{\top}\bm{W}\mathbf{v}_{j}\), which is a summation of terms \(\mathbf{v}_{ic}\mathbf{v}_{jc}\bm{W}_{cc}\) over all communities \(c\in[k]\). If the nodes both fully participate in community \(c\), that is, \(\mathbf{v}_{ic}=\mathbf{v}_{jc}=1\), then the edge logit is changed by \(\bm{W}_{cc}\) starting from a baseline of \(0\), or equivalently, the odds of an edge is multiplied by \(\exp(\bm{W}_{cc})\) starting from a baseline odds of \(1\); if either of the nodes participates only partially in community \(c\), then the change in logit and odds is accordingly prorated. Homophily and heterophily also have a clear interpretation in this model: homophilous communities, which are expressed in \(\bm{B}\), are those with \(\bm{W}_{cc}>0\), where two nodes both participating in the community increases the odds of a link, whereas communities with \(\bm{W}_{cc}<0\), which are expressed in \(\bm{C}\), are heterophilous, and copparticipation decreases the odds of a link.

## 4 Related Work

Community detection via interpretable factorizationsThere is extensive prior work on community detection and node clustering (Schaeffer, 2007; Aggarwal & Wang, 2010; Nascimento & De Carvalho, 2011), perhaps the most well-known being the normalized cuts algorithm of Shi & Malik (2000), which produces a clustering based on the entrywise signs of an eigenvector of the graph Laplacian matrix. However, the clustering algorithms which are most relevant to our work are those based on non-negative matrix factorization (NMF) (Lee & Seung, 1999; Berry et al., 2007; Wang & Zhang, 2012; Gillis, 2020), many of which can be seen as integrating nonnegativity constraints intothe broader, well-studied _random dot product graph_ (RDPG) model (Young and Scheinerman, 2007; Scheinerman and Tucker, 2010; Athreya et al., 2017). One such algorithm is that of Yu et al. (2005), which approximately factors a graph's adjacency matrix \(\bm{A}\in\{0,1\}^{n\times n\times n}\) into two positive matrices \(\bm{H}\) and \(\bm{\Lambda}\), where \(\bm{H}\in\mathbb{R}_{+}^{n\times k}\) is left-stochastic and \(\bm{\Lambda}\in\mathbb{R}_{+}^{k\times k}\) is diagonal, such that \(\bm{H}\bm{\Lambda}\bm{H}^{\top}\approx\bm{A}\). Here \(\bm{H}\) represents a soft clustering of the \(n\) nodes into \(k\) clusters, while the diagonal entries of \(\bm{\Lambda}\) represent the prevalence of edges within clusters. Note the similarity of the factorization to our model, save for the lack of a nonlinearity. Other NMF approaches include those of Ding et al. (2008); Yang et al. (2012); Kuang et al. (2012), and Kuang et al. (2015) (SymNMF).

Modeling heterophilyMuch of the existing work on graph models has an underlying assumption of network homophily (Johnson et al., 2010; Noldus and Van Mieghem, 2015). There has been significant recent interest in the limitations of graph neural network (GNN) models (Duvenaud et al., 2015; Kipf and Welling, 2017; Hamilton et al., 2017) at addressing network heterophily (NT & Maehara, 2019; Zhu et al., 2020; Zheng et al., 2022), as well as proposed solutions (Pei et al., 2020; Yan et al., 2021), but relatively less work for models applicable to clustering. Some existing NMF approaches to clustering do naturally model heterophilous structure in networks. The model of Nourbakhsh et al. (2014), for example, is similar to that of Yu et al. (2005), but allows the cluster affinity matrix \(\bm{\Lambda}\) to be non-diagonal; this allows for inter-cluster edge affinity to exceed intra-cluster edge affinity, so heterophily can arise in this model, though it is not a focus of their work. Another example is the model in Miller et al. (2009), which is similar to ours, though it restricts the cluster assignment matrix \(\bm{V}\) to be binary; additionally, their training algorithm is not based on gradient descent as ours is, and it does not scale to larger networks. More recently, Rubin-Delanchy et al. (2017) and Peysakhovich and Bottou (2021) propose simple decompositions which allow for representation of non-PSD adjacency matrices. The model in the latter work is a factorization of the form \(\bm{A}\approx\bm{D}+\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}\), where \(\bm{D}\in\mathbb{R}^{n\times n}\) is diagonal and \(\bm{B},\bm{C}\in\mathbb{R}^{n\times k}\) are low-rank; excluding the diagonal \(\bm{D}\) term, the model in the former work is algebraically identical. The authors discuss how, interestingly, this model separates the homophilous and heterophilous structure into different factors, namely \(\bm{B}\) and \(\bm{C}\), corresponding to positive and negative eigenvalues, respectively. Note that these decompositions do not include a nonlinear linking function, which is crucial to our exact factorization results, and the respective works do not investigate constraining the factors to be nonnegative.

Overlapping communities and exact embeddingsMany models discussed above focus on the single-label clustering task and thus involve highly-constrained factorizations (e.g., sum-to-one conditions). We are interested in the closely related but distinct task of multi-label clustering, also known as overlapping community detection (Xie et al., 2013; Javed et al., 2018), which involves less constrained, more expressive factorizations. The BigClam algorithm of Yang and Leskovec (2013) uses the following generative model for this task: the probability of a link between two nodes \(i\) and \(j\) is given by \(1-\exp(-\bm{f}_{i}\cdot\bm{f}_{j})\), where \(\bm{f}_{i},\bm{f}_{j}\in\mathbb{R}_{+}^{k}\) represent the intensities with which the nodes participate in each of the \(k\) communities. Note that BigClam assumes strict homophily of the communities: two nodes participating in the same community always increases the probability of a link. However, this model allows for expression of very dense intersections of communities, which the authors observe is generally a characteristic of real-world networks. To ensure that output entries are probabilities, BigClam's factorization includes a nonlinear linking function (namely, \(f(x)=1-e^{x}\)), like our model and LPCA. Recent work outside clustering and community detection on graph generative models (Rendsburg et al., 2020; Chanpuriya et al., 2020) suggests that incorporating a nonlinear linking function can greatly increase the expressiveness of factorization-based graph models, to the point of being able to exactly represent a graph. This adds to a growing body of literature on expressiveness guarantees for embeddings on relational data (Sala et al., 2018; Bhattacharjee and Dasgupta, 2020; Boratko et al., 2021). Most relevant to our work, as previously discussed, Chanpuriya et al. (2020) provide a guarantee for exact low-rank representation of graphs with bounded max degree when using the LPCA factorization model. In this work, we provide a new such guarantee, except for bounded arboricity, which is more applicable to real-world networks, and extend these guarantees to our community-based factorization.

## 5 Theoretical Results

We first restate the main result from Chanpuriya et al. (2020) on exact representation of graphs with bounded max degree using the logistic principal components analysis (LPCA) model, which reconstructs a graph \(\bm{A}\in\{0,1\}^{n\times n}\) using logit factors \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times k}\) via

\[\bm{A}\approx\sigma(\bm{X}\bm{Y}^{\top}).\] (4)

Note that unlike our community-based factorization, the factors of the LPCA model are not nonnegative, and the factorization does not reflect the symmetry of the undirected graph's adjacency matrix. Regardless of the model's interpretability, the following theorem provides a significant guarantee on its expressiveness. We use the following notation: given a matrix \(\bm{M}\), let \(H(\bm{M})\) denote the matrix resulting from entrywise application of the Heaviside step function to \(\bm{M}\), that is, setting all positive entries to \(1\), negative entries to \(0\), and zero entries to \(\sfrac{1}{2}\).

**Theorem 5.1** (Exact LPCA Factorization for Bounded-Degree Graphs ).: _Let \(\bm{A}\in\{0,1\}^{n\times n}\) be the adjacency matrix of a graph \(G\) with maximum degree \(c\). Then there exist matrices \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times(2c+1)}\) such that \(\bm{A}=H(\bm{X}\bm{Y}^{\top})\)._

This corresponds to arbitrarily small approximation error in the LPCA model (Equation 4) because, provided such factors \(\bm{X},\bm{Y}\) for some graph \(\bm{A}\), we have that \(\lim_{s\to\infty}\sigma\left(s\bm{X}\bm{Y}^{\top}\right)=H(\bm{X}\bm{Y}^{\top} )=\bm{A}\). That is, we can scale the factors larger to reduce the error to an arbitrary extent.

We expand on this result in two ways. First, give a new bound for exact embedding in terms of arboricity, rather than max degree. This increases the applicability to real-world networks, which often are sparse (i.e., low arboricity) and have right-skewed degree distributions (i.e., high max degree). Second, we show that any rank-\(k\) LPCA factorization can be converted to our model's symmetric nonnegative factorization with \(O(k)\) communities. This extends the guarantees on the LPCA model's power for exact representation of graphs, both the prior guarantee in terms of max degree and our new one in terms of arboricity, to our community-based model as well. In Appendix A.1, we also introduce a natural family of graphs - Community Overlap Threshold (COT) graphs - for which our model's community-based factorization not only exactly represents the graph, but also must capture some latent structure to do so with sufficiently low embedding dimensionality.

Arboricity bound for exact representationWe will use the following well-known fact: the rank of the entrywise product of two matrices is at most the product of their individual ranks, that is,

\[\text{rank}(\bm{X}\circ\bm{Y})\leq\text{rank}(\bm{X})\cdot\text{rank}(\bm{Y}).\]

**Theorem 5.2** (Exact LPCA Factorization for Bounded-Arboricity Graphs).: _Let \(\bm{A}\in\{0,1\}^{n\times n}\) be the adjacency matrix of an undirected graph \(G\) with arboricity \(\alpha\). Then there exist embeddings \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times(4\alpha^{2}+1)}\) such that \(\bm{A}=H(\bm{X}\bm{Y}^{\top})\)._

Proof.: Let the undirected graph \(\bm{A}\) have arboricity \(\alpha\), i.e., the edges can be partitioned into \(\alpha\) forests. We produce a directed graph \(\bm{B}\) from \(\bm{A}\) by orienting the edges in these forests so that each node's edges point towards its children. Now \(\bm{A}=\bm{B}+\bm{B}^{\top}\), and every node in \(\bm{B}\) has in-degree at most \(\alpha\).

Let \(\bm{V}\in\mathbb{R}^{n\times 2\alpha}\) be the Vandermonde matrix with \(\bm{V}_{t,j}=t^{j-1}\). For any \(\bm{c}\in\mathbb{R}^{2\alpha}\), \([\bm{V}\bm{c}](t)=\sum_{j=1}^{2\alpha}\bm{c}(j)\cdot t^{j-1}\), that is, \(\bm{V}\bm{c}\in\mathbb{R}^{n}\) is a degree-\((2\alpha)\) polynomial with coefficients \(\bm{c}\) evaluated at the integers \(t\in[n]=\{1,\ldots,n\}\). Let \(\bm{b}_{i}\) be the \(i^{\text{th}}\) column of \(\bm{B}\). We seek to construct a polynomial such that for \(t\) with \(\bm{b}_{i}(t)=1\), \([\bm{V}\bm{c}_{i}](t)=0\), and \([\bm{V}\bm{c}_{i}](t)<0\) elsewhere; that is, when inputting an index \(t\in[n]\) such that the \(t^{\text{th}}\) node is an in-neighbor of the \(i^{\text{th}}\) node, we want the polynomial to output \(0\), and for all other indices in \([n]\), we want it to have a negative output. Letting \(N(i)\) denote the in-neighbors of the \(i^{\text{th}}\) node, a simple instantiation of such a polynomial in \(t\) is \(-1\cdot\prod_{j\in N(i)}(t-j)^{2}\). Note that since all nodes have in-degree at most \(\alpha\), this polynomial's degree is at most \(2\alpha\), and hence there exists a coefficient vector \(\bm{c}_{i}\in\mathbb{R}^{2\alpha}\) encoding this polynomial.

Let \(\bm{C}\in\mathbb{R}^{n\times 2\alpha}\) be the matrix resulting from stacking such coefficient vectors for each of the \(n\) nodes. Consider \(\bm{P}=\bm{V}\bm{C}\in\mathbb{R}^{n\times n}\): \(\bm{P}_{i,j}\) is \(0\) if \(\bm{B}_{i,j}=1\) and negative otherwise. Then \((\bm{P}\circ\bm{P}^{\top})_{i,j}\) is \(0\) when either \(\bm{B}_{i,j}=1\) or \((\bm{B}^{\top})_{i,j}=1\) and positive otherwise; equivalently, since \(\bm{A}=\bm{B}+\bm{B}^{\top}\), \((\bm{P}\circ\bm{P}^{\top})_{i,j}=0\) iff \(\bm{A}_{i,j}=1\). Take any positive \(\epsilon\) less than the smallest positive entry of \(\bm{P}\circ\bm{P}^{\top}\). Letting \(\bm{J}\) be an all-ones matrix, define \(\bm{M}=\epsilon\bm{J}-(\bm{P}\circ\bm{P}^{\top})\). Note that \(\bm{M}_{i,j}>0\) if \(\bm{A}=1\) and \(\bm{M}_{i,j}<0\) if \(\bm{A}=0\), that is, \(\bm{M}=H(\bm{A})\) as desired. Since \(\text{rank}(\bm{J})=1\) and \(\text{rank}(\bm{P})\leq 2\alpha\), by the bound on the rank of entrywise products of matrices, the rank of \(\bm{M}\) is at most \((2\alpha)^{2}+1\).

Exact representation with community factorizationLPCA factors \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times k}\) can be processed into nonnegative factors \(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\) and \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\) such that \(k_{B}+k_{C}=6k\) and

\[\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}=\tfrac{1}{2}\left(\bm{X}\bm{Y}^{\top}+ \bm{Y}\bm{X}^{\top}\right).\] (5)

As a rough outline of the argument that follows, we will need \(6k\) columns in the new factors \(\bm{B},\bm{C}\), up from the \(k\) columns in \(\bm{X},\bm{Y}\), because accounting for the possible asymmetry in \(\bm{X}\bm{Y}^{\top}\) will double the required columns, and accounting for the nonnegativity of \(\bm{B},\bm{C}\) will triple the required columns. Observe that the left-hand side can only represent symmetric matrices, but \(\bm{X}\bm{Y}^{\top}\) is not necessarily symmetric even if \(H(\bm{X}\bm{Y}^{\top})=\bm{A}\) for a symmetric \(\bm{A}\). For this reason, we use a symmetrization: let \(\bm{L}=\tfrac{1}{2}\left(\bm{X}\bm{Y}^{\top}+\bm{Y}\bm{X}^{\top}\right)\). Note that \(H(\bm{L})=H(\bm{X}\bm{Y}^{\top})\), so if \(\bm{X}\bm{Y}^{\top}\) constitutes an exact representation of \(\bm{A}\) in that \(H(\bm{X}\bm{Y}^{\top})=\bm{A}\), so too both expressions for \(\bm{L}\) in Equation 5. Pseudocode for the procedure of constructing \(\bm{B},\bm{C}\) given \(\bm{X},\bm{Y}\) is given in Algorithm 1. The concept of this algorithm is to first separate the logit matrix \(\bm{L}\) into a sum and difference of rank-\(1\) components via eigendecomposition. Each of these components can be written as \(+\mathbf{v}\mathbf{v}^{\top}\) or \(-\mathbf{v}\mathbf{v}^{\top}\) with \(\mathbf{v}\in\mathbb{R}^{n}\), where the sign depends on the sign of the eigenvalue. Each component is then separated into a sum and difference of three outer products of nonnegative vectors, via the following Lemma 5.3.

**Lemma 5.3**.: _Let \(\phi:\mathbb{R}\to\mathbb{R}\) denote the ReLU function, i.e., \(\phi(z)=\max\{z,0\}\). For any vector \(\mathbf{v}\),_

\[\mathbf{v}\mathbf{v}^{\top}=2\phi(\mathbf{v})\phi(\mathbf{v})^{\top}+2\phi(- \mathbf{v})\phi(-\mathbf{v})^{\top}-|\mathbf{v}||\mathbf{v}|^{\top}.\]

Proof.: Take any \(\mathbf{v}\in\mathbb{R}^{k}\). Then

\[\mathbf{v}\mathbf{v}^{\top} =(\phi(\mathbf{v})-\phi(-\mathbf{v}))\cdot(\phi(\mathbf{v})^{\top }-\phi(-\mathbf{v})^{\top})\] \[=+\phi(\mathbf{v})\phi(\mathbf{v})^{\top}+\phi(-\mathbf{v})\phi(- \mathbf{v})^{\top}-\phi(\mathbf{v})\phi(-\mathbf{v})^{\top}-\phi(-\mathbf{v}) \phi(\mathbf{v})^{\top}\] \[=+2\phi(\mathbf{v})\phi(\mathbf{v})^{\top}+2\phi(-\mathbf{v})\phi (-\mathbf{v})^{\top}-(\phi(\mathbf{v})+\phi(-\mathbf{v}))\cdot(\phi(\mathbf{ v})+\phi(-\mathbf{v}))^{\top}\] \[=+2\phi(\mathbf{v})\phi(\mathbf{v})^{\top}+2\phi(-\mathbf{v})\phi (-\mathbf{v})^{\top}-|\mathbf{v}||\mathbf{v}|^{\top},\]

where the first step follows from \(\mathbf{v}=\phi(\mathbf{v})-\phi(-\mathbf{v})\), and the last step from \(|\mathbf{v}|=\phi(\mathbf{v})+\phi(-\mathbf{v})\). 

Algorithm 1 follows from Lemma 5.3 and constitutes a constructive proof of the following theorem:

**Theorem 5.4** (Exact Community Factorization from Exact LPCA Factorization).: _Given a symmetric matrix \(\bm{A}\in\{0,1\}\) and \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times k}\) such that \(\bm{A}=H(\bm{X}\bm{Y}^{\top})\), there exist nonnegative matrices \(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\) and \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\) such that \(k_{B}+k_{C}=6k\) and \(\bm{A}=H(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})\)._

**Input** logit factors \(\bm{X},\bm{Y}\in\mathbb{R}^{n\times k}\)

**Output**\(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\), \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\) such that \(k_{B}+k_{C}=6k\)

and \(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}=\tfrac{1}{2}\left(\bm{X}\bm{Y}^{\top}+ \bm{Y}\bm{X}^{\top}\right)\)

```
1: Set \(\bm{Q}\in\mathbb{R}^{n\times 2k}\) and \(\bm{\lambda}\in\mathbb{R}^{2k}\) by truncated eigendecomposition such that \(\bm{Q}\times\operatorname{diag}(\bm{\lambda})\times\bm{Q}^{\top}=\tfrac{1}{2}( \bm{X}\bm{Y}^{\top}+\bm{Y}\bm{X}^{\top})\)
2:\(\bm{B}^{*}\leftarrow\bm{Q}^{+}\times\operatorname{diag}(\sqrt{+\bm{\lambda}^{ \top}})\), where \(\bm{\lambda}^{\top}\), \(\bm{Q}^{+}\) are the positive eigenvalues/vectors
3:\(\bm{C}^{*}\leftarrow\bm{Q}^{-}\times\operatorname{diag}(\sqrt{-\bm{\lambda}^{ -}})\), where \(\bm{\lambda}^{-}\), \(\bm{Q}^{-}\) are the negative eigenvalues/vectors
4:\(\bm{B}\leftarrow\left(\sqrt{2}\phi(\bm{B}^{*});\quad\sqrt{2}\phi(-\bm{B}^{*}); \quad|\bm{C}^{*}|\right)\ \ \triangleright\phi\) and \(|\cdot|\) are entrywise ReLU and absolute value
5:\(\bm{C}\leftarrow\left(\sqrt{2}\phi(\bm{C}^{*});\quad\sqrt{2}\phi(-\bm{C}^{*}); \quad|\bm{B}^{*}|\right)\)
6:return\(\bm{B},\bm{C}\) ```

**Algorithm 1** Converting LPCA Factors to Community Factors

As stated in the introduction to this section, Theorem 5.4 extends any upper bound on the exact factorization dimensionality from the LPCA model to our community-based model. That is, up to a constant factor, the bound in terms of max degree from Theorem 5.1 and the bound in terms of arboricity from Theorem 5.2 also apply to our model; for brevity, we state just the latter here.

**Corollary 5.5** (Exact Community Factorization for Bounded-Arboricity Graphs).: _Let \(\bm{A}\in\{0,1\}^{n\times n}\) be the adjacency matrix of an undirected graph \(G\) with arboricity \(\alpha\). Then there exist non-negative embeddings \(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\) and \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\) such that \(k_{B}+k_{C}=6(4\alpha^{2}+1)\) and \(\bm{A}=H(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})\)._Note that Corollary 5.5 is purely a statement about the capacity of our model; Theorem 5.2 stems from a constructive proof based on polynomial interpolation, and therefore so too does this corollary. We do not expect this factorization to be informative about the graph's latent structure. In the following Section 6, we will fit the model with an entirely different algorithm for downstream applications.

## 6 Experiments

We now present a training algorithm to fit our model, then evaluate our method on a benchmark of five real-world networks.

### Training Algorithm

Given an input graph \(\bm{A}\in\{0,1\}^{n\times n}\), we find low-rank nonnegative matrices \(\bm{B}\) and \(\bm{C}\) such that the model produces \(\tilde{\bm{A}}=\sigma(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})\in(0,1)^{n \times n}\) as in Equation 1 which approximately matches \(\bm{A}\). In particular, we train the model to minimize the sum of binary cross-entropies of the link predictions over all pairs of nodes:

\[R=-\sum\nolimits_{ij}\left(\bm{A}_{ij}\log(\tilde{\bm{A}}_{ij})+(1-\bm{A}_{ij}) \log(1-\tilde{\bm{A}}_{ij})\right).\] (6)

We fit the parameters by gradient descent over this loss, as well as \(L_{2}\) regularization of the factors \(\bm{B}\) and \(\bm{C}\), subject to the nonnegativity of \(\bm{B}\) and \(\bm{C}\). This algorithm is fairly straightforward; pseudocode is given in Algorithm 2. This is quite similar to the training algorithm of Chanpuriya et al. (2020), but in contrast to that work, which only targets an exact fit, we explore the expression of graph structure in the factors and their utility in downstream tasks. Regularization of the factors is implemented to this end to avoid overfitting. Though in the main paper we outline and evaluate a non-stochastic version of the training algorithm, it can generalize straightforwardly to a more scalable stochastic version, e.g., by sampling links and non-links for the loss function and using projected SGD. In Appendix A.2, we discuss an industry application of our model to tabular dataset completion, for which we employ such stochastic training. Here, we use the simpler non-stochastic training to isolate the impact of model capacity, which is the focus of this work, as opposed to optimization.

```
0: adjacency matrix \(\bm{A}\in\{0,1\}^{n\times n}\), regularization weight \(\lambda\geq 0\), # of iterations \(I\), # of homo/heterophilous communities \(k_{B}/k_{C}\)
0: fitted factors \(\bm{B}\in\mathbb{R}_{+}^{n\times k_{B}}\), \(\bm{C}\in\mathbb{R}_{+}^{n\times k_{C}}\) such that \(\sigma(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})\approx\bm{A}\)
1: Initialize \(\bm{B}\), \(\bm{C}\) by setting entries to independent samples of \(\text{Unif}(0,\nicefrac{{1}}{{\sqrt{k_{B}}}}),\text{Unif}(0,\nicefrac{{ 1}}{{\sqrt{k_{C}}}})\)
2:for\(i\gets 1\) to \(I\)do
3:\(\tilde{\bm{A}}\leftarrow\sigma(\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top})\)
4:\(R\leftarrow-\sum_{ij}\left(\bm{A}_{ij}\log(\tilde{\bm{A}}_{ij})+(1-\bm{A}_{ij} )\log(1-\tilde{\bm{A}}_{ij})\right)\)
5:\(R\gets R+\lambda\left(\|\bm{B}\|_{F}^{2}+\|\bm{C}\|_{F}^{2}\right)\)
6: Calculate \(\partial_{\bm{B},\bm{C}}R\), the gradient of \(R\) w.r.t. \(\bm{B},\bm{C}\), via differentiation through Steps 2 to 4
7: Update \(\bm{B},\bm{C}\) to minimize \(R\) using \(\partial_{\bm{B},\bm{C}}R\), subject to \(\bm{B},\bm{C}\geq 0\)
8:endfor
9:return\(\bm{B},\bm{C}\) ```

**Algorithm 2** Fitting the Constrained Model

Implementation detailsOur implementation uses PyTorch (Paszke et al., 2019) for automatic differentiation and minimizes loss using the SciPy (Jones et al., 2001) implementation of the L-BFGS (Liu and Nocedal, 1989; Zhu et al., 1997) algorithm with default hyperparameters and up to a max of 200 iterations of optimization. We set regularization weight \(\lambda=10\) as in Yang and Leskovec (2013). We include code in the form of a Jupyter notebook (Perez and Granger, 2007) demo.

### Datasets

We use five fairly common mid-size datasets ranging from around 1K to 10K nodes. The selection of these five datasets is partly based on the presence of ground-truth multi-labels, which allows for evaluating the overlapping clustering methods. Statistics for these datasets, including the degeneracy of each network, are given in Table 1. Note that degeneracy is an upper bound on arboricity. We note that the mid-sized networks used in our empirical work actually underemphasize the significance of our theoretical arboricity bound: see, e.g., the real-world networks in Pashanasangi & Seshadhri (2021), which have up to tens of millions of nodes but still have degeneracies at most in the hundreds.

**Blog** is a social network of relationships between online bloggers; the node labels represent interests of the bloggers. Similarly, **YouTube** is a social network of YouTube users, and the labels represent groups that the users joined.

**POS** is a word co-occurrence network: nodes represent words, and there are edges between words which are frequently adjacent in a section of the Wikipedia corpus. Each node label represents the part-of-speech of the word. **PPI** is a subgraph of the protein-protein interaction network for Homo Sapiens. Labels represent biological states. Finally, **Amazon** is a co-purchasing network: nodes represent products, and there are edges between products which are frequently purchased together. Labels represent categories of products.

While social networks like the former two in this list are generally dominated by homophily (McPherson et al., 2001), the latter three should exhibit significant heterophily. For co-purchasing networks like Amazon, depending on the product, two of the same kind of product are generally not co-purchased, e.g., Pepsi and Coke, as discussed in Pepsakhovich & Bottou (2021). Though less intuitively accessible, there is also prior discussion of disassortativity in word adjacencies (Foster et al., 2010; Zweig, 2016), as well as in PPI networks (Newman, 2002; Hase et al., 2010).

### Results

ExpressivenessFirst, we investigate the expressiveness of our generative model, that is, the fidelity with which it can reproduce an input network. In Section 1, we used a simple synthetic network to show that our model is more expressive than others due to its ability to represent heterophilous structures in addition to homophilous ones. We now evaluate the expressiveness of our model on real-world networks. As with the synthetic graph, we fix the number of communities or singular vectors, fit the model, then evaluate the reconstruction error. In Figure 4, we compare the results of our model with those of SVD, BigClam (which is discussed in detail in Section 4), and SymNMF (Kuang et al., 2015). SymNMF simply factors the adjacency matrix as \(\bm{A}\approx\bm{H}\bm{H}^{\top}\), where \(\bm{H}\in\mathbb{R}_{+}^{n\times k}\); note that, like SVD, SymNMF does not necessarily output a matrix whose entries are probabilities (i.e., bounded in \([0,1]\)), and hence it is not a natural graph generative model like ours and BigClam.

\begin{table}
\begin{tabular}{l l r r r r r} \hline \hline
**Name** & **Reference** & **Nodes** & **Edges** & **Labels** & **Max Degree** & **Degeneracy** \\ \hline Blog & Tang \& Liu (2009) & 10,312 & 333,983 & 39 & 3,992 & 114 \\ YouTube & Yang \& Leskovec (2015) & 5,346 & 24,121 & 5 & 628 & 19 \\ POS & Qiu et al. (2018) & 4,777 & 92,406 & 40 & 3,644 & 49 \\ PPI & Breitkreutz et al. (2007) & 3,852 & 76,546 & 50 & 593 & 29 \\ Amazon & Yang \& Leskovec (2015) & 794 & 2,109 & 5 & 29 & 6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of datasets used in our experiments. As in Sun et al. (2019), for YouTube and Amazon, we take only nodes that participate in at least one of the largest \(5\) ground-truth communities.

Figure 4: Reconstruction error on real-world networks, relative to our model’s error.

For each method, we fix the number of communities or singular vectors at the ground-truth number. For this experiment only, we are not concerned with learning the latent structure of the graph; the only goal is accurate representation of the network with limited parameters. So, for a fair comparison with SVD, we do not regularize the training of the other methods. Our method consistently has the lowest reconstruction error, both in terms of Frobenius error and entrywise cross-entropy (Equation 6). We particularly highlight the improvement over BigCLam; the salient difference between these models, both of which are factorization-based, include a nonlinear link function, and assign node communities using nonnegative factors, is the presence of the heterophilous \(-\bm{CC}^{\top}\) term in our model. Thus, the improvement directly reflects the value of incorporating heterophily into the model. Interestingly, we find the most significant improvement exactly on the three datasets which have been noted to exhibit significant heterophily: POS, PPI, and Amazon.

Similarity to ground-truth communitiesTo assess the interpretability of clusters generated by our method, we evaluate the similarity of these clusters to ground-truth communities (i.e., class labels), and we compare other methods for overlapping clustering. We additionally compare to another recent but non-generative approach, the vGraph method of Sun et al. (2019), which is based on link clustering; the authors found their method to generally achieve state-of-the-art results in this task. For all methods, we set the number of communities to be detected as the number of ground-truth communities. We report F1-Score as computed in Yang and Leskovec (2013). See Figure 5 (left): the performance of our method is competitive with SymNMF, BigCLam, and vGraph.

Interpretable link predictionWe assess the predictive power of our generative model on the link prediction task. As discussed in Section 3, the link probabilities output by our model are interpretable in terms of a clustering of nodes that it generates; we compare results with our method to those with other models which permit similar interpretation, namely BigCLAM and SymNMF. We randomly select 10% of node pairs to hold out, fit the models on the remaining 90%, then use the trained models to predict links between node pairs in the held out 10%. As a baseline, we also show results for randomly predicting link or no link with equal probability. See Figure 5 (right). The performance of our method is competitive with or exceeds those of the other methods in terms of F1 Score.

## 7 Conclusion

We introduce a community-based graph generative model based on symmetric nonnegative matrix factorization which can represent both homophily and heterophily. We add to a prior guarantee of exact representation for bounded degree graphs with a broader guarantee for bounded _arboricity_ graphs, and we show that both of these guarantees apply to our more interpretable graph model. We illustrate our model's capabilities with experiments on a synthetic motivating example. Experiments on real-world networks show its effectiveness on several key tasks. Broadly, our results suggest that incorporating heterophily into methods for networks can improve both theoretical grounding and empirical performance, while maintaining interpretability. Future directions include deeper understanding of the expressiveness of low-rank logit models and convergence of training algorithms.

Figure 5: Left: Similarity of recovered communities to ground-truth labels of real-world datasets. (Note: vGraph is omitted on Blog due to memory limitations.) Right: Accuracy of link prediction.

## Acknowledgments and Disclosure of Funding

This project was partially supported by an Adobe Research grant, along with NSF Grants 2046235 and 1763618.

## References

* Aggarwal and Wang (2010) Aggarwal, C. C. and Wang, H. A survey of clustering algorithms for graph data. In _Managing and Mining Graph Data_, pp. 275-301. Springer, 2010.
* Athreya et al. (2017) Athreya, A., Fishkind, D. E., Tang, M., Priebe, C. E., Park, Y., Vogelstein, J. T., Levin, K., Lyzinski, V., and Qin, Y. Statistical inference on random dot product graphs: a survey. _The Journal of Machine Learning Research_, 18(1):8393-8484, 2017.
* Berry et al. (2007) Berry, M. W., Browne, M., Langville, A. N., Pauca, V. P., and Plemmons, R. J. Algorithms and applications for approximate nonnegative matrix factorization. _Computational Statistics & Data Analysis_, 52(1):155-173, 2007.
* Bhattacharjee and Dasgupta (2020) Bhattacharjee, R. and Dasgupta, S. What relations are reliably embeddable in euclidean space? In _Algorithmic Learning Theory_, pp. 174-195. PMLR, 2020.
* Bonato (2004) Bonato, A. A survey of models of the web graph. In _Workshop on Combinatorial and Algorithmic Aspects of Networking_, pp. 159-172. Springer, 2004.
* Boratko et al. (2021) Boratko, M., Zhang, D., Monath, N., Vilnis, L., Clarkson, K. L., and McCallum, A. Capacity and bias of learned geometric embeddings for directed graphs. _Advances in Neural Information Processing Systems_, 34:16423-16436, 2021.
* Breitkreutz et al. (2007) Breitkreutz, B.-J., Stark, C., Reguly, T., Boucher, L., Breitkreutz, A., Livstone, M., Oughtred, R., Lackner, D. H., Bahler, J., Wood, V., et al. The biogrid interaction database: 2008 update. _Nucleic acids research_, 36(suppl_1):D637-D640, 2007.
* Chanpuriya et al. (2020) Chanpuriya, S., Musco, C., Sotiropoulos, K., and Tsourakakis, C. Node embeddings and exact low-rank representations of complex networks. _Advances in Neural Information Processing Systems_, 33, 2020.
* Ding et al. (2008) Ding, C., Li, T., and Jordan, M. I. Nonnegative matrix factorization for combinatorial optimization: Spectral clustering, graph matching, and clique finding. In _2008 Eighth IEEE International Conference on Data Mining_, pp. 183-192. IEEE, 2008.
* Donoho and Stodden (2003) Donoho, D. L. and Stodden, V. When does non-negative matrix factorization give a correct decomposition into parts? In _Advances in Neural Information Processing Systems 16_, pp. 1141-1148. MIT Press, 2003.
* Duvenaud et al. (2015) Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J., Gomez-Bombarelli, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Convolutional networks on graphs for learning molecular fingerprints. In _Advances in Neural Information Processing Systems 28_, pp. 2224-2232, 2015.
* Foster et al. (2010) Foster, J. G., Foster, D. V., Grassberger, P., and Paczuski, M. Edge direction and the structure of networks. _Proceedings of the National Academy of Sciences_, 107(24):10815-10820, 2010.
* Gillis (2020) Gillis, N. _Nonnegative Matrix Factorization_. SIAM, 2020.
* Hamilton et al. (2017) Hamilton, W., Ying, Z., and Leskovec, J. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Hase et al. (2010) Hase, T., Niimura, Y., and Tanaka, H. Difference in gene duplicability may explain the difference in overall structure of protein-protein interaction networks among eukaryotes. _BMC Evolutionary Biology_, 10(1):1-15, 2010.
* Javed et al. (2018) Javed, M. A., Younis, M. S., Latif, S., Qadir, J., and Baig, A. Community detection in networks: A multidisciplinary review. _Journal of Network and Computer Applications_, 108:87-111, 2018.
* Javed et al. (2018)Johnson, S., Torres, J. J., Marro, J., and Munoz, M. A. Entropic origin of disassortativity in complex networks. _Physical Review Letters_, 104(10):108702, 2010.
* Jones et al. (2001) Jones, E., Oliphant, T., Peterson, P., et al. SciPy: Open source scientific tools for Python, 2001. URL http://www.scipy.org/.
* Kipf & Welling (2017) Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. _International Conference on Learning Representations_, 2017.
* Kuang et al. (2012) Kuang, D., Ding, C., and Park, H. Symmetric nonnegative matrix factorization for graph clustering. In _Proceedings of the 2012 SIAM International Conference on Data Mining_, pp. 106-117. SIAM, 2012.
* Kuang et al. (2015) Kuang, D., Yun, S., and Park, H. Symnmf: nonnegative low-rank approximation of a similarity matrix for graph clustering. _Journal of Global Optimization_, 62(3):545-574, 2015.
* Lee & Seung (1999) Lee, D. D. and Seung, H. S. Learning the parts of objects by non-negative matrix factorization. _Nature_, 401(6755):788-791, 1999.
* Liu & Nocedal (1989) Liu, D. C. and Nocedal, J. On the limited memory BFGS method for large scale optimization. _Mathematical Programming_, 45(1-3):503-528, 1989.
* Mason & Verwoerd (2007) Mason, O. and Verwoerd, M. Graph theory and networks in biology. _IET Systems Biology_, 1(2):89-119, 2007.
* McPherson et al. (2001) McPherson, M., Smith-Lovin, L., and Cook, J. M. Birds of a feather: Homophily in social networks. _Annual review of sociology_, 27(1):415-444, 2001.
* Miller et al. (2009) Miller, K. T., Griffiths, T. L., and Jordan, M. I. Nonparametric latent feature models for link prediction. In _Advances in Neural Information Processing Systems 22_, pp. 1276-1284. Curran Associates, Inc., 2009.
* Nascimento & De Carvalho (2011) Nascimento, M. C. and De Carvalho, A. C. Spectral methods for graph clustering-a survey. _European Journal of Operational Research_, 211(2):221-231, 2011.
* Newman (2002) Newman, M. E. Assortative mixing in networks. _Physical Review Letters_, 89(20):208701, 2002.
* Noldus & Van Mieghem (2015) Noldus, R. and Van Mieghem, P. Assortativity in complex networks. _Journal of Complex Networks_, 3(4):507-542, 2015.
* Nourbakhsh et al. (2014) Nourbakhsh, F., Bulo, S. R., and Pelillo, M. A matrix factorization approach to graph compression. In _2014 22nd International Conference on Pattern Recognition_, pp. 76-81. IEEE, 2014.
* NT & Maehara (2019) NT, H. and Maehara, T. Revisiting graph neural networks: All we have is low-pass filters. _arXiv preprint arXiv:1905.09550_, 2019.
* Pashansangi & Seshadhri (2021) Pashansangi, N. and Seshadhri, C. Faster and generalized temporal triangle counting, via degeneracy ordering. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pp. 1319-1328, 2021.
* Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pp. 8024-8035. Curran Associates, Inc., 2019.
* Pei et al. (2020) Pei, H., Wei, B., Chang, K. C., Lei, Y., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In _8th International Conference on Learning Representations, ICLR 2020_, 2020.
* Perez & Granger (2007) Perez, F. and Granger, B. E. IPython: a system for interactive scientific computing. _Computing in Science and Engineering_, 9(3):21-29, May 2007. ISSN 1521-9615. doi: 10.1109/MCSE.2007.53. URL https://ipython.org.
* Peysakhovich & Bottou (2021) Peysakhovich, A. and Bottou, L. An attract-repel decomposition of undirected networks. _arXiv preprint arXiv:2106.09671_, 2021.
* Pashansangi & Seshadhri (2015)Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In _Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining_, pp. 459-467. ACM, 2018.
* Rendsburg et al. (2020) Rendsburg, L., Heidrich, H., and Von Luxburg, U. Netgan without gan: From random walks to low-rank approximations. In _International Conference on Machine Learning_, pp. 8073-8082. PMLR, 2020.
* Rubin-Delanchy et al. (2017) Rubin-Delanchy, P., Cape, J., Tang, M., and Priebe, C. E. A statistical interpretation of spectral embedding: the generalised random dot product graph. _arXiv preprint arXiv:1709.05506_, 2017.
* Sala et al. (2018) Sala, F., De Sa, C., Gu, A., and Re, C. Representation tradeoffs for hyperbolic embeddings. In _International Conference on Machine Learning_, pp. 4460-4469. PMLR, 2018.
* Schaeffer (2007) Schaeffer, S. E. Graph clustering. _Computer Science Review_, 1(1):27-64, 2007.
* Scheinerman & Tucker (2010) Scheinerman, E. R. and Tucker, K. Modeling graphs using dot product representations. _Computational statistics_, 25(1):1-16, 2010.
* Scott (1988) Scott, J. Social network analysis. _Sociology_, 22(1):109-127, 1988.
* Seshadhri et al. (2020) Seshadhri, C., Sharma, A., Stolman, A., and Goel, A. The impossibility of low-rank representations for triangle-rich complex networks. _Proceedings of the National Academy of Sciences_, 117(11):5631-5637, 2020.
* Shi & Malik (2000) Shi, J. and Malik, J. Normalized cuts and image segmentation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 22(8):888-905, 2000.
* Sun et al. (2019) Sun, F., Qu, M., Hoffmann, J., Huang, C., and Tang, J. vgraph: A generative model for joint community detection and node representation learning. In _Advances in Neural Information Processing Systems 32_, pp. 512-522, 2019.
* Tang & Liu (2009) Tang, L. and Liu, H. Relational learning via latent social dimensions. In _Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pp. 817-826. ACM, 2009.
* Wang & Zhang (2012) Wang, Y.-X. and Zhang, Y.-J. Nonnegative matrix factorization: A comprehensive review. _IEEE Transactions on Knowledge and Data Engineering_, 25(6):1336-1353, 2012.
* Xie et al. (2013) Xie, J., Kelley, S., and Szymanski, B. K. Overlapping community detection in networks: The state-of-the-art and comparative study. _ACM Computing Surveys_, 45(4):43:1-43:35, 2013.
* Yan et al. (2021) Yan, Y., Hashemi, M., Swersky, K., Yang, Y., and Koutra, D. Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks. _arXiv preprint arXiv:2102.06462_, 2021.
* Yang & Leskovec (2013) Yang, J. and Leskovec, J. Overlapping community detection at scale: a nonnegative matrix factorization approach. In _Proceedings of the Sixth ACM International Conference on Web Search and Data Mining_, pp. 587-596, 2013.
* Yang & Leskovec (2015) Yang, J. and Leskovec, J. Defining and evaluating network communities based on ground-truth. _Knowledge and Information Systems_, 42(1):181-213, 2015.
* Yang et al. (2012) Yang, Z., Hao, T., Dikmen, O., Chen, X., and Oja, E. Clustering by nonnegative matrix factorization using graph random walk. In _Advances in Neural Information Processing Systems_, pp. 1079-1087, 2012.
* Young & Scheinerman (2007) Young, S. J. and Scheinerman, E. R. Random dot product graph models for social networks. In _International Workshop on Algorithms and Models for the Web-Graph_, pp. 138-149. Springer, 2007.
* Yu et al. (2005) Yu, K., Yu, S., and Tresp, V. Soft clustering on graphs. In _Advances in Neural Information Processing Systems_, pp. 1553-1560, 2005.

Zheng, X., Liu, Y., Pan, S., Zhang, M., Jin, D., and Yu, P. S. Graph neural networks for graphs with heterophily: A survey. _arXiv:2202.07082_, 2022.
* Zhu et al. (1997) Zhu, C., Byrd, R. H., Lu, P., and Nocedal, J. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. _ACM Transactions on Mathematical Software (TOMS)_, 23(4):550-560, 1997.
* Zhu et al. (2020) Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D. Beyond homophily in graph neural networks: Current limitations and effective designs. _Advances in Neural Information Processing Systems_, 33, 2020.
* Zweig (2016) Zweig, K. A. Are word-adjacency networks networks? In _Towards a theoretical framework for analyzing complex linguistic networks_, pp. 153-163. Springer, 2016.

Appendix

### COT Graph Exact Representation

As a theoretical demonstration of the capability of our model to learn latent structure, we additionally show that our model can exactly represent a natural family of graphs, which exhibits both homophily and heterophily, with small \(k\) and interpretably. The family of graphs is specified below in Definition 1; roughly speaking, nodes in such graphs share an edge iff they coparticipate in some number of homophilous communities and don't coparticipate in a number of heterophilous communities. For example, the motivating graph described in Section 2 would be an instance of such a graph if an edge occurs between two users iff the two users are from the same city and have different genders.

**Definition 1** (Community Overlap Threshold (COT) Graph).: _An unweighted, undirected graph whose edges are determined by an overlapping clustering and a "thresholding" integer \(t\in\mathbb{Z}\) as follows: for each vertex \(i\), there are two latent binary vectors \(\bm{b}_{i}\in\{0,1\}^{k_{b}}\) and \(\bm{c}_{i}\in\{0,1\}^{k_{c}}\), and there is an edge between vertices \(i\) and \(j\) iff \(\bm{b}_{i}\cdot\bm{b}_{j}-\bm{c}_{i}\cdot\bm{c}_{j}\geq t\)._

**Theorem A.1** (Compact Representation of COT Graphs).: _Suppose \(\bm{A}\) is the adjacency matrix of a COT graph on \(n\) nodes with latent vectors \(\bm{b}_{i}\in\{0,1\}^{k_{b}}\) and \(\bm{c}_{i}\in\{0,1\}^{k_{c}}\) for \(i\in\{1,2,\ldots,n\}\). Let \(k=k_{b}+k_{c}\). Then, for any \(\epsilon>0\), there exist \(\bm{V}\in[0,1]^{n\times(k+1)}\) and diagonal \(\bm{W}\in\mathbb{R}^{(k+1)\times(k+1)}\) such that \(\left\|\sigma(\bm{V}\bm{W}\bm{V}^{\top})-\bm{A}\right\|_{F}<\epsilon\)._

Proof.: Let \(t\) be the thresholding integer of the graph, and let the rows of \(\bm{B}\in\{0,1\}^{n\times k_{b}}\) and \(\bm{C}\in\{0,1\}^{n\times k_{c}}\) contain the vectors \(\bm{b}\) and \(\bm{c}\) of all nodes. Via Equation 2, we can find \(\bm{V}^{*}\in[0,1]^{n\times k}\) and diagonal \(\bm{W}^{*}\in\mathbb{R}^{k\times k}\) such that \(\bm{V}^{*}\bm{W}^{*}{\bm{V}^{*}}^{\top}=\bm{B}\bm{B}^{\top}-\bm{C}\bm{C}^{\top}\). Now let

\[\bm{V}=(\bm{V}^{*}\quad\bm{1})\qquad\bm{W}=\begin{pmatrix}\bm{W}^{*}&0\\ 0&\frac{1}{2}-t\end{pmatrix}.\]

Then \((\bm{V}\bm{W}\bm{V}^{\top})_{ij}=\bm{b}_{i}\cdot\bm{b}_{j}-\bm{c}_{i}\cdot \bm{c}_{j}+\frac{1}{2}-t\). Hence \((\bm{V}\bm{W}\bm{V}^{\top})_{ij}>0\) iff \(\bm{b}_{i}\cdot\bm{b}_{j}-\bm{c}_{i}\cdot\bm{c}_{j}>t-\frac{1}{2}\), which is true iff \(\bm{A}_{ij}=1\) by the assumption on the graph. Similarly, \((\bm{V}\bm{W}\bm{V}^{\top})_{ij}<0\) iff \(\bm{A}_{ij}=0\). It follows that

\[\lim_{s\to\infty}\sigma\left(\bm{V}(s\bm{W})\bm{V}^{\top}\right)=\lim_{s\to \infty}\sigma\left(s\bm{V}\bm{W}\bm{V}^{\top}\right)=\bm{A}.\qed\]

### Application: Tabular Data Completion

We apply our link prediction algorithm to the task of data completion for categorical tabular data. We first transform such data to a graph as follows: We create a node for each row, as well as a node for each unique category for each of the columns. For each entry of the table, a link occurs between the node for the row and the node for the value in the entry, which is one of the possible categories of the column (e.g., if Bob is American, then a link occurs between the row node for 'Bob' and the category node for 'American'). No links occur between two nodes for rows, or between two nodes for category values. For the negative samples, we select only edges between nodes for rows and nodes for category values; the negative samples would otherwise be dominated by pairs of nodes for rows.

We use two proprietary company datasets datasets which we call Company A and Company B. We use the same experimental setup as described in Section 6.3. Unlike in Section 6.3, we have a restriction on the possible links: each row node must be linked to exactly one of the value nodes for each of the categories. Hence, for each combination of row node and column, we predict a link only with the value node with the highest predicted link probability. We set the number of communities for all methods to \(20\) and \(50\) for Company A and Company B, respectively. We employ the stochastic version of our algorithm, based on projected SGD, for which code is also provided. We generally use a batch size of \(100\); we find that the optimization of BigClam often diverges on Company B with this batch size, so we instead use batches of size \(1000\) for its optimization. The results are provided in Figure 6. Notably, our proposed approach outperforms comparable (community-based factorization) methods in terms of accuracy. As a baseline, we also show the accuracy when completing the tables by simply selecting the most common categorical value for each column ("Plurality"). All methods outperform this baseline on both datasets.

Figure 6: Test accuracy of tabular data completion on two proprietary company datasets (Company A and Company B).