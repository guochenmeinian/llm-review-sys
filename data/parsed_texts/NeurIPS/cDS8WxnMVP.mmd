Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods

 Felix Dangel

Vector Institute

Toronto, Canada

fdangel@vectorinstitute.ai

###### Abstract

Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the transfer of theoretical and algorithmic ideas to convolutions. We simplify convolutions by viewing them as tensor networks (TNs) that allow reasoning about the underlying tensor multiplications by drawing diagrams, manipulating them to perform function transformations like differentiation, and efficiently evaluating them with einsum. To demonstrate their simplicity and expressiveness, we derive diagrams of various autodiff operations and popular curvature approximations with full hyper-parameter support, batching, channel groups, and generalization to any convolution dimension. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to simplify diagrams before evaluation. Finally, we probe performance. Our TN implementation accelerates a recently-proposed KFAC variant up to 4.5 x while removing the standard implementation's memory overhead, and enables new hardware-efficient tensor dropout for approximate backpropagation.

## 1 Introduction

Convolutional neural networks [CNNs, 39] mark a milestone in the development of deep learning architectures as their'sliding window' approach represents an important inductive bias for vision tasks. Their intuition is simple to explain with graphical illustrations [e.g. 21]. Yet, convolutions are more challenging to analyze than dense layers in multi-layer perceptrons (MLPs) or transformers [71]. One reason is that they are hard to express in matrix notation and--even in index notation--compact expressions that are convenient to work with only exist for special hyper-parameters [e.g. 27, 2]. Many hyper-parameters (stride, padding,...) and additional features like channel groups [36] introduce even more complexity that is inherited by related routines, e.g. for autodiff. We observe a delay of analytic and algorithmic developments between MLPs vs. CNNs, e.g.

* Approximate Hessian diagonal: 1989 vs. 2024
* Hessian rank: 2021 vs. 2023
* Gradient descent learning dynamics: 2014 vs. 2023
* Neural tangent kernel (NTK): 2018 vs. 2019
* Kronecker-factored quasi-Newton methods: 2021 vs. 2022
* Kronecker-factored curvature (KFAC, KFRA, KFLR): (2015, 2017, 2017) vs. (2016, 2020, 2020)

The software support for less standard routines some of these methods require also reflects this gap. Some functions only support special dimensions [15]. Others use less efficient workarounds (SS5.1) or are not provided at all (SSB.4). And they are hard to modify as the code is either closed-source [12] or written in a low-level language. This complicates the advance of existing, and the exploration of new, algorithmic ideas for convolutions.

Here, we seek to reduce this complexity gap by viewing convolutions as tensor networks [TNs, 53, 6, 9] which express the underlying tensor multiplications as diagrams. These diagrams are simpler to parse than mathematical equations and can seamlessly be (i) manipulated to take derivatives, add batching, or extract sub-tensors, (ii) merged with other diagrams, and (iii) evaluated with einsum. This yields simple, modifiable implementations that benefit from automated under-the-hood-optimizations for efficient TN contraction developed by the quantum simulation community [e.g. 66, 25, 74, 13], like finding a high-quality contraction order or distributing computations:

1. We use the TN format of convolution from Hayashi et al. [29] to derive diagrams and einsum formulas for autodiff and less standard routines for curvature approximations with support for all hyper-parameters, batching, groups, and any dimension (Table 1).
2. We present transformations based on the convolution's connectivity pattern to re-wire and symbolically simplify TNs before evaluation (example in Figure 1).
3. We compare default and TN implementations, demonstrating optimal peak memory reduction and run time improvements up to 4.5 x for a recent KFAC variant, and showcase their flexibility to impose hardware-efficient dropout for randomized backpropagation.

Our work not only provides simpler perspectives and implementations that facilitate the exploration of algorithmic ideas for convolutions, but also directly advances second-order methods like KFAC: It enables more frequent pre-conditioner updates, using larger batches without going out of memory, and extending KFAC to transpose convolution. These improvements are important for second-order optimization and other applications like Laplace approximations [20] and influence functions [28].

## 2 Preliminaries

We briefly review 2d convolution (SS2.1), tensor multiplication and einsum (SS2.2), then introduce the graphical TN notation and apply it to convolution (SS2.3). Bold lower-case (\(\bm{a}\)), upper-case (\(\bm{A}\)), and upper-case sans-serif (\(\bm{\mathsf{A}}\)) symbols indicate vectors, matrices, and tensors. Entries follow the same convention but use regular font weight; \([\cdot]\) denotes slicing \((\,|\bm{A}|_{i,j}=A_{i,j})\). Parenthesized indices mean reshapes, e.g. \([\bm{a}]_{(i,j)}=[\bm{A}]_{i,j}\) with \(\bm{a}\) the flattened matrix \(\bm{A}\).

Figure 1: Many convolution-related routines can be expressed as TNs and evaluated with einsum. We illustrate this for the input-based factor of KFAC for convolutions [KFC, 27], whose standard implementation (_top_) requires unfolding the input (high memory). The TN (_middle_) enables internal optimizations inside einsum (e.g. with contraction path optimizers like opt_einsum [66]). (_Bottom_) In many cases, the TN further simplifies due to structures in the index pattern, which reduces cost.

### Convolution

2d convolutions process channels of 2d signals \(\textbf{X}\in\mathbb{R}^{C_{\text{in}}\times I_{1}\times I_{2}}\) with \(C_{\text{in}}\) channels of spatial dimensions1\(I_{1},I_{2}\) by sliding a collection of \(C_{\text{out}}\) filter banks, arranged in a kernel \(\textbf{W}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times K_{1}\times K _{2}}\) with kernel size \(K_{1},K_{2}\), over the input. The sliding operation depends on various hyperparameters [padding, stride, \(\ldots\), see 21]. At each step, the filters are contracted with the overlapping area, yielding the channel values of a pixel in the output \(\textbf{Y}\in\mathbb{R}^{C_{\text{out}}\times O_{1}\times O_{2}}\) with spatial dimensions \(O_{1},O_{2}\). Optionally, a bias from \(\bm{b}\in\mathbb{R}^{C_{\text{out}}}\) is added per channel.

Footnote 1: We prefer \(I_{1},I_{2}\) over the more common choice \(H,W\) to simplify the generalization to higher dimensions.

One way to implement convolution is via matrix multiplication [10], similar to fully-connected layers. First, one extracts the overlapping patches from the input for each output, then flattens and column-stacks them into a matrix \([\textbf{X}]\in\mathbb{R}^{C_{\text{in}}K_{1}K_{2}\times O_{1}O_{2}}\), called the _unfolded input_ (or im2col). Multiplying a matrix view \(\bm{W}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}}K_{1}K_{2}}\) of the kernel onto the unfolded input then yields a matrix view \(\bm{Y}\) of **Y** (the vector of ones, \(\bm{1}_{O_{1}O_{2}}\), copies the bias for each channel),

\[\bm{Y}=\bm{W}[\textbf{X}]+\bm{b}\:\bm{1}_{O_{1}O_{2}}^{\top}\in\mathbb{R}^{C_ {\text{out}}\times O_{1}O_{2}}\,.\] (1)

We can also view convolution as an affine map of the flattened input \(\bm{x}\in\mathbb{R}^{C_{\text{in}}I_{1}I_{2}}\) into a vector view \(\bm{y}\) of **Y** with a Toeplitz-structured matrix \(\bm{A}(\textbf{W})\in\mathbb{R}^{C_{\text{out}}O_{1}O_{2}\times C_{\text{in}} I_{1}I_{2}}\),

\[\bm{y}=\bm{A}(\textbf{W})\bm{x}+\bm{b}\otimes\bm{1}_{O_{1}O_{2}}\in\mathbb{R}^{C_{ \text{out}}O_{1}O_{2}}\,.\] (2)

This perspective is uncommon in code, but used in theoretical works [e.g. 65] as it highlights the similarity between convolutions and dense layers.

### Tensor Multiplication

Tensor multiplication unifies outer (Kronecker), element-wise (Hadamard), and inner products and uses the input-output index relation to infer the multiplication type. We start with the binary case, then generalize to more inputs: Consider \(\textbf{A},\textbf{B},\textbf{C}\) whose index names are described by the index tuples \(S_{1},S_{2},S_{3}\) where \(S_{3}\subseteq(S_{1}\cup S_{2})\) (converting tuples to sets if needed). Any product of **A** and **B** can be described by the multiplication operator \(*_{(S_{1},S_{2},S_{3})}\) with

\[\textbf{C}=*_{(S_{1},S_{2},S_{3})}(\textbf{A},\textbf{B})\quad\Leftrightarrow \quad[\textbf{C}]_{S_{3}}=\sum_{(S_{1}\cup S_{2})\setminus S_{3}}[\textbf{A}] _{S_{1}}[\textbf{B}]_{S_{2}}\] (3)

summing over indices that are not present in the output. E.g., for two matrices \(\bm{A},\bm{B}\), their product is \(\bm{A}\bm{B}=*_{(i,j),(j,k),(i,k))}(\bm{A},\bm{B})\) (see SS1.2), their Hadamard product \(\bm{A}\odot\bm{B}=*_{((i,j),(i,j),(i,j))}(\bm{A},\bm{B})\), and their Kronecker product \(\bm{A}\otimes\bm{B}=*_{((i,j),(k,I),((i,k),(j,I)))}(\bm{A},\bm{B})\). Libraries support this functionality via einsum, which takes a string encoding of \(S_{1},S_{2},S_{3}\), followed by **A**, **B**. It also accepts longer sequences \(\textbf{A}_{1},\ldots,\textbf{A}_{N}\) with index tuples \(S_{1},S_{2},\ldots,S_{N}\) and output index tuple \(S_{N+1}\),

\[\textbf{A}_{N+1}=*_{(S_{1},\ldots,S_{N},S_{N+1})}(\textbf{A}_{1},\ldots,\textbf {A}_{N})\:\Leftrightarrow\:[\textbf{A}_{N+1}]_{S_{N+1}}=\sum_{(\bigcup_{n=1}^{N }S_{n})\setminus S_{N+1}}\!\!\!\left(\prod_{n=1}^{N}[\textbf{A}_{n}]_{S_{n}} \right).\] (4)

Figure 2: TNs of (a) 2d convolution and (b,c) connections to its matrix multiplication view. The connectivity along each dimension is explicit via an index pattern tensor \(\bm{\mathsf{\Pi}}\).

[MISSING_PAGE_EMPTY:4]

## 3 TNs for Convolution Operations

We now demonstrate the elegance of TNs for computing derivatives (SS3.1), autodiff operations (SS3.2), and approximate second-order information (SS3.3) by graphical manipulation. For simplicity, we exclude batching (vmap-ing like in JAX [8]) and channel groups, and provide the diagrams with full support in SSB. Table 1 summarizes our derivations (with batching and groups). As a warm-up, we identify the unfolded input and kernel from the matrix-multiplication view (Equations (1) and (2)). They follow by contracting the index patterns with either the input or kernel (Figures 1(b) and 1(c)),

\[[\textbf{X}]]_{(c_{\text{in}},k_{1},k_{2}),(c_{\text{in}},0_{1})} =\sum_{i_{1},i_{2}}\mathcal{X}_{c_{\text{in}},i_{1},i_{2}}\sqcap ^{(1)}_{i_{1},o_{1},k_{1}}\sqcap^{(2)}_{i_{2},o_{2},k_{2}},\] \[[\textbf{A}(\textbf{W})]_{(c_{\text{in}},o_{1},o_{2}),(c_{\text{in }},i_{1},i_{2})} =\sum_{k_{1},k_{2}}\sqcap^{(1)}_{i_{1},o_{1},k_{1}}\sqcap^{(2)}_{i_ {2},o_{2},k_{2}}\textbf{W}_{c_{\text{out}},c_{\text{in}},k_{1},k_{2}}\;.\]

### Tensor Network Differentiation

Derivatives play a crucial role in theoretical and practical ML. First, we show that differentiating a TN diagram amounts to a simple graphical manipulation. Then, we derive the Jacobians of convolution. Consider an arbitrary TN represented by the tensor multiplication from Equation (4). The Jacobian tensor \([\textbf{J}_{\textbf{A}_{j}}\textbf{A}_{N+1}]_{S_{N+1},S^{\prime}_{j}}=\nicefrac{{ \partial[\textbf{A}_{N+1}]_{S_{N+1}}}}{{\partial[\textbf{A}_{j}]_{S^{\prime}}}}\) w.r.t. an input \(\textbf{A}_{j}\) collects all partial derivatives and is addressed through indices \(S_{n+1}\times S^{\prime}_{j}\) with \(S^{\prime}_{j}\) an independent copy of \(S_{j}\). Assume that \(\textbf{A}_{j}\) only enters once in the tensor multiplication. Then, taking the derivative of Equation (4) w.r.t. \([\textbf{A}_{j}]_{S^{\prime}_{j}}\) simply replaces the tensor by a Kronecker delta \(\delta_{S_{j},S^{\prime}_{j}}\),

\[\frac{\partial[\textbf{A}_{N+1}]_{S_{N+1}}}{\partial[\textbf{A}_{j}]_{S^{ \prime}_{j}}}=\sum_{(\bigcup_{n=1}^{N}S_{n})\setminus S_{n+1}}[\textbf{A}_{1} ]_{S_{1}}\cdots[\textbf{A}_{j-1}]_{S_{j-1}}\prod_{i\in S_{j}}\delta_{i,i^{ \prime}}[\textbf{A}_{j+1}]_{S_{j+1}}\cdots[\textbf{A}_{N}]_{S_{N}}\] (6)

If an index \(i\in S_{j}\) is summed, \(i\not\in S_{n+1}\), we can sum the Kronecker delta \(\delta_{i,i^{\prime}}\), effectively replacing all occurrences of \(i\) by \(i^{\prime}\). If instead \(i\) is part of the output index, \(i\in S_{n+1}\), the Kronecker delta remains part of the Jacobian and imposes structure. Figure 2(a) illustrates this process in diagrams for differentiating a convolution w.r.t. its kernel. Equation (6) amounts to cutting out the argument of differentiation and assigning new indices to the resulting open legs. For the weight Jacobian \(\textbf{J}_{\textbf{W}}\textbf{Y}\), this introduces structure: If we re-interpret the two sub-diagrams in Figure 2(a) as matrices, compare with the Kronecker diagram from Equation (5) and use Figure 1(b), we find \([\textbf{X}]^{\top}\otimes\textbf{I}_{C_{\text{out}}}\) for the Jacobian's matrix view [e.g. 16]. Figure 2(b) shows the input Jacobian \(\textbf{J}_{\textbf{X}}\textbf{Y}\) which is a tensor view of \(\textbf{A}(\textbf{W})\), as expected from the matrix-vector perspective of Equation (2).

Differentiating a TN is more convenient than using matrix calculus [44] as it amounts to a simple graphical manipulation, does not rely on a flattening convention, and therefore preserves the full index structure. The resulting TN can still be translated back to matrix language, if desired. It also simplifies the computation of higher-order derivatives (e.g. \(\nicefrac{{\partial^{2}\textbf{Y}}}{{\partial\textbf{w}\partial\textbf{x}}}\)), since differentiation yields another TN and can thus be repeated. If a tensor occurs more than once in a TN, the product rule applies and the derivative is a sum of TNs with one occurrence removed.

Figure 3: TN differentiation as graphical manipulation. (a) Differentiating convolution w.r.t. **W** is cutting it out of the diagram and yields the weight Jacobian. (b) Same procedure applied to the Jacobian w.r.t. **X**. (c) VJP for the weight and (d) input Jacobian (transpose convolution). Jacobians are shaded, only their contraction with \(\textbf{V}^{(\textbf{Y})}\) is highlighted.

### Autodiff & Connections to Transpose Convolution

Although Jacobians are useful, crucial routines for autodiff are vector-Jacobian and Jacobian-vector products (VJPs, JVPs). Both are simple to realize with TNs due to access to full Jacobians. VJPs are used in backpropagation to pull back a tensor \(\textbf{V}^{(\textbf{Y})}\in\mathbb{R}^{C_{\text{out}}\times C_{1}\times C_{2}}\) from the output to the input or weight space. The VJP results \(\textbf{V}^{(\textbf{X})}\in\mathbb{R}^{C_{\text{in}}\times I_{1}\times I_{2}}\) and \(\textbf{V}^{(\textbf{W})}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}} \times K_{1}\times K_{2}}\) are

\[\begin{split}\textbf{V}^{(\textbf{X})}_{c_{\text{in}}^{\prime},c_ {1}^{\prime},c_{2}^{\prime}}=\sum_{c_{\text{out}},c_{1},c_{2}}V^{(\textbf{Y})}_ {c_{\text{out}},c_{1},c_{2}}\frac{\partial\,Y_{c_{\text{out}},c_{1},c_{2}}}{ \partial\,X_{c_{\text{in}}^{\prime},c_{1}^{\prime},c_{2}^{\prime}}}\,,\qquad V ^{(\textbf{W})}_{c_{\text{out}}^{\prime},c_{1}^{\prime},c_{2}^{\prime}}=\sum_{ c_{\text{out}},c_{1},c_{2}}V^{(\textbf{Y})}_{c_{\text{out}},c_{1},c_{2}} \frac{\partial\,Y_{c_{\text{out}},c_{1},c_{2}}}{\partial\,W_{c_{\text{out}}^{ \prime},c_{\text{in}}^{\prime},c_{1}^{\prime},c_{2}^{\prime}}}\,.\end{split}\]

Both are simply new TNs constructed from contracting the vector with the respective Jacobian, see Figures 2(c) and 2(d) (VJPs are analogous). The input VJP is often used to define transpose convolution [21]. In the matrix-multiplication perspective (Equation (2)), this operation is defined relative to a convolution with kernel **W** by multiplication with \(\textbf{A}(\textbf{W})^{\top}\), i.e. using the same connectivity pattern but mapping from the convolution's output to input space. The TN in Figure 2(d) makes this sharing explicit and cleanly defines transpose convolution.4

Footnote 4: Standalone implementations of transpose convolution require another parameter to unambiguously reconstruct the convolution’s input dimension (see §D) for how to compute **P** in this case).

### Kronecker-factored Approximate Curvature

The Jacobian diagrams allow us to construct the TNs of second-order information like the Fisher/generalized Gauss-Newton (GGN) matrix and sub-tensors like its diagonal (SSC). Here, we focus on the popular Kronecker-factored approximation of the GGN [47; 27; 23; 48] whose input-based Kronecker factor relies on the unfolded input \([\textbf{X}]\) which requires large memory. State-of-the-art libraries that provide access to KFAC [17; 51] also use this approach. Using TNs, we can often avoid expanding \([\mathbbm{X}]\) explicitly and save memory. Here, we describe the existing KFAC approximations and their TNs (see SS5.1 for their run time evaluation).

**KFC (KFAC-expand):** Grosse & Martens [27] introduce a Kronecker approximation for the kernel's GGN, \(\textbf{G}\approx\boldsymbol{\Omega}\otimes\boldsymbol{\Gamma}\) where \(\boldsymbol{\Gamma}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{out}}}\) and the input-based factor \(\boldsymbol{\Omega}=[\mathbbm{X}][\mathbbm{X}]^{\top}\in\mathbb{R}^{C_{\text{in }}K_{1}K_{2}\times C_{\text{in}}K_{1}K_{2}}\) (Figure 3(a)), the unfolded input's self-inner product (averaged over a batch).

**KFAC-reduce:** Eschenhagen et al. [23] generalized KFAC to graph neural networks and transformers based on the concept of weight sharing, also present in convolutions. They identify two approximations: KFAC-expand and KFAC-reduce. The former corresponds to KFC [27]. The latter shows similar performance in downstream tasks, but is cheaper to compute. It relies on the column-averaged unfolded input, i.e. the average over all patches sharing the same weights. KFAC-reduce approximates \(\textbf{G}\approx\boldsymbol{\hat{\Omega}}\otimes\boldsymbol{\hat{\Gamma}}\) with \(\boldsymbol{\hat{\Gamma}}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{out}}}\) and \(\boldsymbol{\hat{\Omega}}=\nicefrac{{1}}{{({O_{1}O_{2}})^{2}}}\,\boldsymbol{ \hat{\Omega}}^{\top}[\mathbbm{X}](1^{\top}_{O_{1}O_{2}}[\textbf{X}])^{\top}\in \mathbb{R}^{C_{\text{in}}K_{1}K_{2}\times C_{\text{in}}K_{1}K_{2}}\) (Figure 3(b); averaged over a batch). For convolutions, this is arguably a'more natural' approximation as it becomes exact in certain limits [23], in contrast to the expand approximation.

**KFAC for transpose convolution:** Our approach enables us to derive KFAC for transpose convolutions. We are not aware of previous works doing so. This seems surprising because, similar to SS2.1, transpose convolution can be seen as matrix multiplication between the kernel and an unfolded input. From this formulation we can immediately obtain KFAC through the weight sharing view of Eschenhagen et al. [23]. The Kronecker factor requires unfolding the input similar to im2col, but for transpose convolutions. This operation is currently not provided by ML libraries. We can overcome this limitation, express the unfolding operation as TN, and--for the first time--establish KFAC (expand and reduce) for transpose convolutions (see SSB.4 for details).

Figure 4: TNs of input-based Kronecker factors for KFAC approximations of the Fisher/GGN (no batching, no groups). The unfolded input is shaded, only additional contractions are highlighted. (a) \(\boldsymbol{\Omega}\) (KFC/KFAC-expand) from Grosse & Martens [27], (b) \(\boldsymbol{\hat{\Omega}}\) (KFAC-reduce) from Eschenhagen et al. [23] (vectors of ones effectively amount to sums).

## 4 TN Simplifications & Implementation

Many convolutions in real-world CNNs use structured connectivity patterns that allow for simplifications which we describe here along with implementation aspects.

### Index Pattern Structure & Simplifications

The index pattern \(\boldsymbol{\mathsf{\Pi}}\) encodes the connectivity of a convolution and depends on its hyper-parameters. Along one dimension, \(\boldsymbol{\mathsf{\Pi}}=\boldsymbol{\mathsf{\Pi}}(I,K,S,P,D)\) with input size \(I\), kernel size \(K\), stride \(S\), padding \(P\), and dilation \(D\). We provide pseudo-code for computing \(\boldsymbol{\mathsf{\Pi}}\) in \(\mathbb{S}\mathbb{D}\) which is easy to implement efficiently with standard functions from any numerical library (Algorithm D1). Its entries are

\[\left[\boldsymbol{\mathsf{\Pi}}(I,K,S,P,D)\right]_{i,o,k}=\delta_{i,1+(k-1)D+(o -1)S-P}\,,\] (7)

with \(i=1,\ldots,I,o=1,\ldots,O,k=1,\ldots,K\) and output size \(O(I,K,S,P,D)=1+\lfloor(I+2P-(K+(K-1)(D-1)))/S\rfloor\). Since \(\boldsymbol{\mathsf{\Pi}}\) is binary and has size linear in \(I,O,K\), it is cheap to pre-compute and cache. The index pattern's symmetries allow for re-wiring a TN. For instance, the symmetry of \((k,D)\) and \((o,S)\) in Equation (7) and \(O(I,K,S,P,D)\) permits a _kernel-output swap_, exchanging the role of kernel and output dimension (Figure 4(a)). Rochette et al. [58] used this to phrase the per-example gradient computation (Figure 4(c)) as convolution.

For many convolutions of real-world CNNs (see SSE for a hyper-parameter study) the index pattern possesses structure that simplifies its contraction with other tensors into either smaller contractions or reshapes: _Dense convolutions_ use a shared kernel size and stride, and thus process non-overlapping adjacent tiles of the input. Their index pattern's action can be expressed as a cheap reshape (Figure 4(b)). Such convolutions are common in DenseNets [33], MobileNets [31; 60], ResNets [30], and ConvNeXts [42]. InceptionV3 [69] has 2d _mixed-dense convolutions_ that are dense along one dimension. _Down-sampling convolutions_ use a larger stride than kernel size, hence only process a sub-set of their input, and are used in ResNet18 [30], ResNext101 [72], and WideResNet101 [73]. Their pattern contracts with a tensor \(\boldsymbol{\mathsf{V}}\) like that of a dense convolution with a sub-tensor \(\boldsymbol{\tilde{\mathsf{V}}}\) (Figure 4(c)). SS5.1 shows that those simplifications accelerate computations.

### Practical Benefits of the TN Abstraction & Limitations for Convolutions

**Contraction order optimization:** There exist various orders in which to carry out the summations in a TN and their performance can vary by orders of magnitude. One extreme approach is to carry out all summations via nested for-loops. This so-called Feynman path integral algorithm requires little memory, but many FLOPS since it does not re-cycle intermediate results. The other extreme is sequential pair-wise contraction. This builds up intermediate results and can greatly reduce FLOPS. The schedule is represented by a binary tree, but the underlying search is in general at least #P-hard [14]. Fortunately, there exist heuristics to find high-quality contraction trees for TNs with hundreds of tensors [32; 25; 13], implemented in packages like opt_einsum[66].

**Index slicing:** A common problem with high-quality schedules is that intermediates exceed memory. Dynamic slicing [32] (e.g. cotengra[25]) is a simple method to decompose a contraction until it becomes feasible by breaking it up into smaller identical sub-tasks whose aggregation adds a small overhead. This enables peak memory reduction and distribution.

**Sparsity:**\(\boldsymbol{\mathsf{\Pi}}\) is sparse as only a small fraction of the input contributes to an output element. For a convolution with stride \(S<K\) and default parameters (\(P=0,D=1\)), for fixed output and kernel indices \(k,o\), there is exactly one non-zero entry in \(\left[\boldsymbol{\mathsf{\Pi}}\right]_{:,o,k}\). Hence \(\mathtt{nnz}(\boldsymbol{\mathsf{\Pi}})=OK\), which corresponds to a sparsity of \(\nicefrac{{1}}{{l}}\). Padding leads to more kernel elements that do not contribute to an

Figure 5: TN illustrations of index pattern simplifications and transformations. See § D.3 for the math formulation.

output pixel, and therefore a sparser \(\boldsymbol{\mathsf{\Pi}}\). For down-sampling and dense convolutions, we showed how \(\boldsymbol{\mathsf{\Pi}}\)'s algebraic structure allows to simplify its contraction. However, if that is not possible, \(\boldsymbol{\mathsf{\Pi}}\) contains explicit zeros that add unnecessary FLOPS. One way to circumvent this is to match a TN with that of an operation with efficient implementation (like im2col, (transpose) convolution) using transformations like the _kernel-output swap_ or by introducing identity tensors to complete a template, as done in Rochette et al. [58], Dangel [15] for per-sample gradients and im2col.

**Approximate contraction & structured dropout:** TNs offer a principled approach for stochastic approximation via Monte-Carlo estimation to save memory and run time at the cost of accuracy. The basic idea is best explained on a matrix product \(\boldsymbol{C}:=\boldsymbol{A}\boldsymbol{B}=\sum_{n=1}^{N}\left[\boldsymbol{A }\right]_{:,n}\left[\boldsymbol{B}\right]_{n,:}\) with \(\boldsymbol{A}\in\mathbb{R}^{I\times N},\boldsymbol{B}\in\mathbb{R}^{N,O}\). To approximate the sum, we introduce a distribution over \(n\)'s range, then use column-row-sampling [CRS, 1] to form an unbiased Monte-Carlo approximation with sampled indices, which only requires the sub-matrices with active column-row pairs. Bernoulli-CRS samples without replacement by assigning a Bernoulli random variable \(\mathrm{Bernoulli}(\pi_{n})\) with probability \(\pi_{n}\) for column-row pair \(n\) to be included in the contraction. The Bernoulli estimator is \(\tilde{\boldsymbol{C}}:=\sum_{n=1}^{N}\nicefrac{{z_{n}}}{{\pi_{n}}}\left[ \boldsymbol{A}\right]_{n,:}\left[\boldsymbol{B}\right]_{n,:}\) with \(z_{n}\sim\mathrm{Bernoulli}(\pi_{n})\). With a shared keep probability, \(\pi_{n}:=p\), this yields the unbiased estimator \(\boldsymbol{C}^{\prime}=\nicefrac{{1}}{{p}}\sum_{n=1,\ldots,N}\boldsymbol{A} ^{\prime}\boldsymbol{B}^{\prime}\) where \(\boldsymbol{A}^{\prime}=\boldsymbol{A}\boldsymbol{K}\) and \(\boldsymbol{B}^{\prime}=\boldsymbol{K}\boldsymbol{B}\) with \(\boldsymbol{K}=\mathrm{diag}(z_{1},\ldots,z_{N})\) are the sub-matrices of \(\boldsymbol{A},\boldsymbol{B}\) containing the active column-row pairs. CRS applies to a single contraction. For TNs with multiple sums, we can apply it individually, and also impose a distribution over the result indices, which computes a (scaled) sub-tensor.

## 5 Experiments

Here, we demonstrate computational benefits of TNs for less standard routines of second-order methods and showcase their flexibility to perform stochastic autodiff in novel ways.

### Run Time Evaluation

We implement the presented TNs' contraction strings and operands5 in PyTorch [52]. The simplifications from SS4 can be applied on top and yield a modified einsum expression. To find a contraction schedule, we use opt_einsum[66] with default settings. We extract the unique convolutions of 9 architectures for ImageNet and smaller data sets, then compare some operations from Table 1 with their standard implementation on an Nvidia Tesla T4 GPU (16 GB); see SSF for all details. Due to space constraints, we highlight important insights here and provide references to the corresponding material in the appendix. In general, the performance gap between standard and TN implementation decreases the less common an operation is (Figure F17); from forward pass (inference & training), to VJPs (training), to KFAC (training with a second-order method). This is intuitive as more frequently used routines have been optimized more aggressively.

Footnote 5: einsum does not yet support index un-grouping, so we must reshape manually before and after.

**Impact of simplifications:** While general convolutions remain unaffected (Figure F18d) when applying the transformations of SS4, mixed dense, dense, and down-sampling convolutions consistently enjoy significant run time improvements (Figures F18a to F18c). As an example, we show the performance comparison for dense convolutions in Figure 6: The performance ratio's median between TN and standard forward and input VJP is close to 1, that is both require almost the same time. In the median, the TN even outperforms PyTorch's highly optimized weight VJP, also for down-sampling

Figure 6: Run time ratios of TN (w/o simplifications) vs. standard implementation for dense convolutions of 9 CNNs. With simplifications, convolution and input VJP achieve median ratios slightly above 1, and the TN implementation is faster for weight VJP, KFC & KFAC-reduce. The code in Figure 1 corresponds to default, TN, and simplified TN KFC implementation.

convolutions (Figure F21). For KFC, the median performance ratios are well below 1 for dense, mixed dense & sub-sampling convolutions (Figure F22).

**KFAC-reduce:** For all convolution types, the TN implementation achieves its largest improvements for \(\hat{\mathbf{\Omega}}\) and consistently outperforms the PyTorch implementation in the median when simplifications are enabled (Figure F23). The standard implementation unfolds the input, takes the row-average, then forms its outer product. The TN does not need to expand \(\llbracket\mathbf{X}\rrbracket\) in memory and instead averages the index pattern tensors, which reduces peak memory and run time. We observe performance ratios down to 0.22 x (speed-ups up to \(\approx 4.5\) x, Table F9) and consistently lower memory consumption with savings up to 3 GiB (Figure 7). Hence, our approach not only significantly reduces the overhead of 2nd-order optimizers based on KFAC-reduce, but also allows them to operate on larger batches without exceeding memory (Eschenhagen et al. [23] specifically mention memory as important limitation of their method). Other examples for KFAC algorithms where computing the input-based Kronecker factor adds significant time and memory overhead are that of Petersen et al. [54], Benzing [5] which only use \(\mathbf{\Omega}\) (setting \(\mathbf{\Gamma}\propto\bm{I}\)), or Lin et al. [41, 40] which remove matrix inversion.

**Downstream improvements with KFAC-reduce:** To demonstrate the speed-ups of KFAC-reduce in practical algorithms, we apply our work to the SINGD optimizer [41] and benchmark the impact of our TN implementation on its memory and run time in comparison to SGD without momentum. Concretely, we investigate SINGD with KFAC-reduce and diagonal pre-conditioners on ResNet18 and VGG19 on ImageNet-like synthetic data \((3,256,256)\) using a batch size of 128. We measured per-iteration time and peak memory on an NVIDIA A40 with 48 GiB of RAM. For SINGD, we compare computing the Kronecker factors with the standard approach ('SINGD') via input unfolding versus our TN implementation ('SINGD+TN'). Table 2 summarizes the results.

On both nets, our TN implementation halves SINGD's run time, and almost completely eliminates the memory, overhead compared to SGD. On VGG19, it dramatically lowers the memory overhead, cutting it down by a factor of 2 from 32 GiB to 16 GiB. This enables using larger batches or more frequently updating the pre-conditioner, underlining the utility of our approach for reducing the computational gap between approximate second-order and first-order methods.

### Randomized Autodiff via Approximate Contraction

CRS is an alternative to checkpointing [26] to lower memory consumption of backpropagation [50, 11, 1]. Here, we focus on unbiased gradient approximations by applying the exact forward pass, but CRS when computing the weight VJP, which requires storing a sub-tensor of \(\mathbf{X}\). For convolutions, the approaches of existing works are limited by the supported functionality of ML libraries. Adelman et al. [1] restrict to sampling \(\mathbf{X}\) along \(c_{\text{in}}\), which eliminates many gradient entries as the index is part of the gradient. The randomized gradient would thus only train a sub-tensor of the kernel per step. Oktay et al. [50], Chen et al. [11] apply unstructured dropout to \(\mathbf{X}\), store it in sparse form, and restore the sparsified tensor during the backward pass. This reduces memory, but not computation.

Our TN implementation is more flexible and can, for example, tackle spatial dimensions with CRS. This reduces memory to the same extent, but also run time due to fewer contractions. Importantly, it

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & **Optimizer** & **Per iter.[s]** & **Peak mem. [GiB]** \\ \hline \multirow{4}{*}{\begin{tabular}{l} \multirow{2}{*}{\begin{tabular}{l} \multirow{2}{*}{\begin{tabular}{l} SGD \\ SINGD \\ SINGD+TN \\ \end{tabular} } } \\ \end{tabular} } & SGD & \(0.12\) (\(1.0\) x) & \(3.6\) (\(1.0\) x) \\  & SINGD & \(0.19\) (\(1.7\) x) & \(4.5\) (\(1.3\) x) \\  & SINGD+TN & \(0.16\) (\(1.3\) x) & \(3.6\) (\(1.0\) x) \\ \hline \multirow{4}{*}{\begin{tabular}{l} \multirow{2}{*}{\begin{tabular}{l} \multirow{2}{*}{
\begin{tabular}{l} SGD \\ SINGD+TN \\ \end{tabular} } } \\ \end{tabular} } & SGD & \(0.69\) (\(1.0\) x) & \(14\) (\(1.0\) x) \\  & SINGD & \(1.0\) (\(1.5\) x) & \(32\) (\(2.3\) x) \\  & SINGD+TN & \(0.80\) (\(1.2\) x) & \(16\) (\(1.1\) x) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Impact of our TN implementation on SINGD’s run time and peak memory compared to SGD.

Figure 7: Extra memory used by the standard versus our TN implementation (simplifications enabled) of KFAC-reduce. Each point represents a convolution from 9 CNNs, clipped below by 1 MiB. TNs consistently use less memory than the standard implementation (one exception), and often no extra memory at all. We observe memory savings up to 3 GiB.

does not zero out the gradient for entire filters. In Figure 8 we compare the gradient approximation errors of channel and spatial sub-sampling. For the same memory reduction, spatial sub-sampling yields a smaller approximation error on both real & synthetic data. E.g., instead of keeping 75 % of channels, we achieve the same approximation quality using only 35 % of pixels.

## 6 Related Work

**Structured convolutions:** We use the TN formulation of convolution from Hayashi et al. [29] who focus on connecting kernel factorizations to existing (depth-wise separable [31; 60], factored [69], bottleneck [30], flattened/CP decomposed, low-rank filter [67; 57; 70]) convolutions and explore new factorizations. Our work focuses on operations related to convolutions, diagram manipulations, the index pattern structure, and computational performance/flexibility. Structured convolutions integrate seamlessly with our framework by replacing the kernel with its factorized TN.

**Higher-order autodiff:** ML frameworks focus on differentiating scalar-valued objectives once. Recent works [37; 38; 43] developed a tensor calculus to compute higher-order derivatives of tensor-valued functions and compiler optimizations through linear algebra and common sub-expression elimination. Phrasing convolution as einsum, we allow it to be integrated into such frameworks, benefit from their optimizations, and complement them with our convolution-specific simplifications.

## 7 Conclusion

We used tensor networks (TNs), a diagrammatic representation of tensor multiplications, to simplify convolutions and many related operations. We derived the diagrams of autodiff and less standard routines for curvature approximations like KFAC with support for all hyper-parameters, channel groups, batching, and generalization to arbitrary dimensions. All amount to simple einsum expressions that can easily be modified--e.g. to perform stochastic backpropagation--and benefit from under-the-hood optimizations before evaluation. We complemented those by convolution-specific symbolic simplifications based on structure in the connectivity pattern and showed their effectiveness to advance second-order methods. Our TN implementation accelerates the computation of KFAC up to 4.5 x and uses significantly less memory. Beyond performance improvements, the simplifying perspective also allowed us to formulate KFAC for transpose convolution. More broadly, our work underlines the elegance of TNs for reasoning about tensor multiplications and function transformations (differentiation, batching, slicing, simplification) in terms of diagrams at less cognitive load without sacrificing rigour. We believe they are a powerful tool for the ML community that will open up new algorithmic possibilities due to their simplicity & flexibility.

Figure 8: Sampling spatial axes is more effective than channels on both (a) real-world and (b) synthetic data. We take the untrained All-CNN-C [68] for CIFAR-100 with cross-entropy loss, disable dropout, and modify the convolutions to use a fraction \(p\) of **X** when computing the weight gradient via Bernoulli-CRS. For mini-batches of size 128, we compute the deterministic gradients for all kernels, then flatten and concatenate them into a vector \(\bm{g}\); likewise for its proxy \(\hat{\bm{g}}\). CRS is described by \((p_{c_{i}},p_{i_{1}},p_{i_{2}})\), the keep rates along the channel and spatial dimensions. We compare channel and spatial sampling with same memory reduction, i.e. \((p,1,1)\) and \((1,\sqrt{p},\sqrt{p})\). To measure approximation quality, we use the normalized residual norm \(\nicefrac{{\|\bm{g}-\bm{g}\|_{2}}}{{\|\bm{g}\|_{2}}}\) and report mean and standard deviation of 10 different model and batch initializations.

## Acknowledgments and Disclosure of Funding

The author would like to thank Luca Thiede, Andres Fernandez Rodriguez, and Kirill Neklyudov for providing feedback to the manuscript. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.

## References

* [1] Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [2] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely wide neural net. _Advances in neural information processing systems (NeurIPS)_, 2019.
* [3] Bahamou, A., Goldfarb, D., and Ren, Y. A mini-block fisher method for deep neural networks. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* [4] Becker, S. and Lecun, Y. Improving the convergence of back-propagation learning with second-order methods. 1989.
* [5] Benzing, F. Gradient descent on neurons and its link to approximate second-order optimization. In _International Conference on Machine Learning (ICML)_, 2022.
* [6] Biamonte, J. and Bergholm, V. Tensor networks in a nutshell, 2017.
* [7] Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In _International Conference on Machine Learning (ICML)_, 2017.
* [8] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. JAX: composable transformations of Python+NumPy programs, 2018.
* [9] Bridgeman, J. C. and Chubb, C. T. Hand-waving and interpretive dance: an introductory course on tensor networks. _Journal of Physics A: Mathematical and theoretical_, 2017.
* [10] Chellapilla, K., Puri, S., and Simard, P. High performance convolutional neural networks for document processing. In _International Workshop on Frontiers in Handwriting Recognition_, 2006.
* [11] Chen, J., Xu, K., Wang, Y., Cheng, Y., and Yao, A. DropIT: Dropping intermediate tensors for memory-efficient DNN training. In _International Conference on Learning Representations (ICLR)_, 2023.
* [12] Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., and Shelhamer, E. cudnn: Efficient primitives for deep learning. 2014.
* [13] cuQuantum development team, T. cuQuantum SDK: A high-performance library for accelerating quantum information science, 2023.
* [14] Damm, C., Holzer, M., and McKenzie, P. The complexity of tensor calculus. _computational complexity_, 2002.
* [15] Dangel, F. unfoldNd: (n=1,2,3)-dimensional unfold (im2col) and fold (col2im) in pytorch, 2021.
* [16] Dangel, F., Harmeling, S., and Hennig, P. Modular block-diagonal curvature approximations for feed-forward architectures. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2020.
* [17] Dangel, F., Kunstner, F., and Hennig, P. BackPACK: Packing more into backprop. In _International Conference on Learning Representations (ICLR)_, 2020.
* [18] Dangel, F., Tatzel, L., and Hennig, P. ViViT: Curvature access through the generalized gauss-newton's low-rank structure. _Transactions on Machine Learning Research (TMLR)_, 2022.
* [19] Dangel, F. J. Backpropagation beyond the gradient. 2023.
* effortless bayesian deep learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [21] Dumoulin, V. and Visin, F. A guide to convolution arithmetic for deep learning. 2016.

* [22] Elsayed, M., Farrahi, H., Dangel, F., and Mahmood, A. R. Revisiting scalable hessian diagonal approximations for applications in reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2024.
* [23] Eschenhagen, R., Immer, A., Turner, R. E., Schneider, F., and Hennig, P. Kronecker-factored approximate curvature for modern neural network architectures. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [24] Goldfarb, D., Ren, Y., and Bahamou, A. Practical quasi-newton methods for training deep neural networks, 2021.
* [25] Gray, J. and Kourtis, S. Hyper-optimized tensor network contraction. _Quantum_, 2021.
* [26] Griewank, A. and Walther, A. _Evaluating derivatives: principles and techniques of algorithmic differentiation_. SIAM, 2008.
* [27] Grosse, R. and Martens, J. A kronecker-factored approximate Fisher matrix for convolution layers. In _International Conference on Machine Learning (ICML)_, 2016.
* [28] Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukosute, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R. Studying large language model generalization with influence functions, 2023.
* [29] Hayashi, K., Yamaguchi, T., Sugawara, Y., and Maeda, S.-i. Exploring unexplored tensor network decompositions for convolutional neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [30] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _IEEE conference on computer vision and pattern recognition (CVPR)_, 2016.
* [31] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. 2017.
* [32] Huang, C., Zhang, F., Newman, M., Ni, X., Ding, D., Cai, J., Gao, X., Wang, T., Wu, F., Zhang, G., et al. Efficient parallelization of tensor network contraction for simulating quantum computation. _Nature Computational Science_, 2021.
* [33] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In _IEEE conference on computer vision and pattern recognition (CVPR)_, 2017.
* [34] Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks, 2020.
* [35] Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho*, K., and Geras*, K. The break-even point on optimization trajectories of deep neural networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* [36] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2012.
* [37] Laue, S., Mitterreiter, M., and Giesen, J. Computing higher order derivatives of matrix and tensor expressions. _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [38] Laue, S., Mitterreiter, M., and Giesen, J. A simple and efficient tensor calculus. In _AAAI Conference on Artificial Intelligence_, 2020.
* [39] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. _Neural Computation_, 1989.
* [40] Lin, W., Duruisseaux, V., Leok, M., Nielsen, F., Khan, M. E., and Schmidt, M. Simplifying momentum-based riemannian submanifold optimization. 2023.
* [41] Lin, W., Dangel, F., Eschenhagen, R., Neklyudov, K., Kristiadi, A., Turner, R. E., and Makhzani, A. Structured inverse-free natural gradient descent: Memory-efficient & numerically-stable KFAC. In _International Conference on Machine Learning (ICML)_, 2024.
* [42] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.

* [43] Ma, L., Ye, J., and Solomonik, E. Autohoot: Automatic high-order optimization for tensors. In _International Conference on Parallel Architectures and Compilation Techniques (PACT)_, 2020.
* [44] Magnus, J. R. and Neudecker, H. _Matrix Differential Calculus with Applications in Statistics and Econometrics_. Probabilistics and Statistics. 1999.
* [45] Martens, J. Deep learning via Hessian-free optimization. In _International Conference on Machine Learning (ICML)_, 2010.
* [46] Martens, J. New insights and perspectives on the natural gradient method, 2020.
* [47] Martens, J. and Grosse, R. Optimizing neural networks with Kronecker-factored approximate curvature. In _International Conference on Machine Learning (ICML)_, 2015.
* [48] Martens, J., Ba, J., and Johnson, M. Kronecker-factored curvature approximations for recurrent neural networks. In _International Conference on Learning Representations (ICLR)_, 2018.
* [49] Novak, R., Sohl-Dickstein, J., and Schoenholz, S. S. Fast finite width neural tangent kernel. In _International Conference on Machine Learning (ICML)_, 2022.
* [50] Oktay, D., McGreivy, N., Aduol, J., Beatson, A., and Adams, R. P. Randomized automatic differentiation. In _International Conference on Learning Representations (ICLR)_, 2021.
* [51] Osawa, K., Ishikawa, S., Yokota, R., Li, S., and Hoefler, T. Asdl: A unified interface for gradient preconditioning in pytorch, 2023.
* [52] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems (NeurIPS)_. 2019.
* [53] Penrose, R. Applications of negative dimensional tensors. _Combinatorial Mathematics and its Applications_, 1971.
* [54] Petersen, F., Sutter, T., Borgelt, C., Huh, D., Kuehne, H., Sun, Y., and Deussen, O. ISAAC newton: Input-based approximate curvature for newton's method. In _International Conference on Learning Representations (ICLR)_, 2023.
* [55] Pinson, H., Lenaerts, J., and Ginis, V. Linear cnns discover the statistical structure of the dataset using only the most dominant frequencies. In _International Conference on Machine Learning (ICML)_, 2023.
* [56] Ren, Y., Bahamou, A., and Goldfarb, D. Kronecker-factored quasi-newton methods for deep learning, 2022.
* [57] Rigamonti, R., Sironi, A., Lepetit, V., and Fua, P. Learning separable filters. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2013.
* [58] Rochette, G., Manoel, A., and Tramel, E. W. Efficient per-example gradient computations in convolutional neural networks, 2019.
* [59] Rogozhnikov, A. Einops: Clear and reliable tensor manipulations with einstein-like notation. In _International Conference on Learning Representations (ICLR)_, 2022.
* [60] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In _IEEE conference on computer vision and pattern recognition (CVPR)_, 2018.
* [61] Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014.
* [62] Schneider, F., Balles, L., and Hennig, P. DeepOBS: A deep learning optimizer benchmark suite. In _International Conference on Learning Representations (ICLR)_, 2019.
* [63] Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient descent. _Neural Computation_, 2002.
* [64] Singh, S. P., Bachmann, G., and Hofmann, T. Analytic insights into structure and rank of neural network hessian maps. _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [65] Singh, S. P., Hofmann, T., and Scholkopf, B. The hessian perspective into the nature of convolutional neural networks. 2023.

- A python package for optimizing contraction order for einsum-like expressions. _Journal of Open Source Software (JOSS)_, 2018.
* [67] Smith, S. W. The scientist and engineer's guide to digital signal processing. 1997.
* [68] Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net, 2015.
* [69] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In _IEEE conference on computer vision and pattern recognition (CVPR)_, 2016.
* [70] Tai, C., Xiao, T., Zhang, Y., Wang, X., et al. Convolutional neural networks with low-rank regularization. 2015.
* [71] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [72] Xie, S., Girshick, R., Dollar, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In _IEEE conference on computer vision and pattern recognition (CVPR)_, 2017.
* [73] Zagoruyko, S. and Komodakis, N. Wide residual networks. 2016.
* [74] Zhang, F. A parallel tensor network contraction algorithm and its applications in quantum computation. 2020.

Conclusions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods (Supplementary Material)

[MISSING_PAGE_POST]

[MISSING_PAGE_FAIL:16]

\(\bm{A}^{(k_{1},k_{2})}\in\mathbb{R}^{I\times J},k_{i}=1\ldots,K\). We can extract the diagonal blocks by restoring the sub-structure,then taking the diagonal along the \(K\)-dimensional index,

We can apply this procedure to the GGN from Figure 16(a). Assume we want to divide the output channel, input channel, and spatial dimensions into \(G_{C_{\text{out}}},G_{C_{\text{in}}},G_{K_{1}},G_{K_{2}}\) groups. A group will thus be indexed with a tuple \((g_{C_{\text{out}}},g_{C_{\text{in}}},g_{K_{1}},g_{K_{2}})\) and the corresponding GGN block will be of dimension \(C_{\text{out}}/G_{C_{\text{out}}}\times C_{\text{in}}/G_{C_{\text{in}}}\times K _{1}/G_{K_{1}}\times K_{2}/G_{K_{2}}\times C_{\text{out}}/G_{C_{\text{in}}}\times C _{\text{in}}/G_{C_{\text{in}}}\times K_{1}/G_{K_{1}}\times K_{2}/G_{K_{2}}\) and correspond to the GGN for \([\textbf{M}]_{(g_{C_{\text{out}}},:),(g_{C_{\text{in}}},:),(g_{K_{1}},:),(g_{K _{2}},:)}\). This process of un-grouping the output dimensions, then taking the diagonal along the group indices, is illustrated in Figure 11. Note that if we choose \(G_{C_{\text{out}}}=\tilde{C}_{\text{out}}\), \(G_{C_{\text{in}}}=C_{\text{in}},G_{K_{1}}=K_{1},G_{K_{2}}=K_{2}\), each block will be a single number and hence we recover the GGN diagonal from Figure 16(b). If instead we \(G_{C_{\text{out}}}=G_{C_{\text{in}}}G_{K_{1}}G_{K_{2}}=1\), we obtain the full GGN from Figure 16(a). The outlined schemes allows to extract mini-blocks of arbitrary size along the diagonal (subject to the total dimension).

### Kronecker-factored Approximate Curvature (KFAC) for Grouped Convolutions

We were unable to find a definition of KFAC for grouped convolutions. Hence, we derive it here and present the TN diagrams. We use the perspective that grouped convolutions are independent convolutions over channel groups which are then concatenated. For each of those convolutions, we can then apply established the KFAC approximation for convolutions without groups. For a group \(g\) we have the kernel \(\textbf{M}_{g}=[\textbf{M}]_{(g,:),:,:,:}\) and the unfolded input of its associated input channels, \([\textbf{X}_{g}]=[\textbf{X}]_{(g,:),:,:}=[\![\textbf{X}]_{(g,:),:,:}]\) (or \([\textbf{X}_{n,g}]=[\![\textbf{X}_{n}]_{(g,:),:,:}]\) in the batched setting).

KFC/KFAC-expand for grouped convolutions:Applying the regular KFC approximation to the kernel of group \(g\), this yields the Fisher approximation \(\boldsymbol{\Omega}_{g}\otimes\boldsymbol{\Gamma}_{g}\) with \(\boldsymbol{\Gamma}_{g}\in\mathbb{R}^{\tilde{C}_{\text{out}}\times\tilde{C}_{ \text{out}}}\) and

Figure 11: TN of a GGN mini-block diagonal without batching and channel groups.

Figure 12: TN diagrams of input-based factors in Kronecker approximations of the GGN for convolutions with batching and channel groups. They extend Figure 4 from the main text.

\(\bm{\Omega}_{g}=\nicefrac{{1}}{{N}}\sum_{n=1}^{N}\llbracket\bm{X}_{n,g}\rrbracket[\! \llbracket\bm{X}_{n,g}\rrbracket\!]^{\top}\in\mathbb{R}^{C_{\text{in}}K_{1}K_{2} \times C_{\text{in}}K_{1}K_{2}}\) where \(\bm{X}_{n,g}\) is the input tensor for sample \(n\) and group \(g\) (remember the index structure \(\bm{\mathsf{X}}_{n,(g,\hat{c}_{\text{in}}),i_{1},i_{2}}\)). Figure 11(a) shows the diagram for \(\{N\bm{\Omega}_{g}\}_{g=1}^{G}\).

KFAC-reduce for grouped convolutions:Proceeding in the same way, but using the unfolded input averaged over output locations, we obtain the Fisher approximation \(\hat{\bm{\Omega}}_{g}\otimes\hat{\bm{\Gamma}}_{g}\) with \(\hat{\bm{\Gamma}}_{g}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{out}}}\) and \(\hat{\bm{\Omega}}_{g}=\nicefrac{{1}}{{N(O_{1}O_{2})^{2}}}\sum_{n=1}^{N} \bm{1}_{O_{1}O_{2}}^{\top}\llbracket\bm{X}_{n,g}\rrbracket(\bm{1}_{O_{1}O_{2} }^{\top}\llbracket\bm{X}_{n,g}\rrbracket)^{\top}\in\mathbb{R}^{C_{\text{in}}K_{ 1}K_{2}\times C_{\text{in}}K_{1}K_{2}}\) for the kernel of group \(g\). Figure 11(b) shows the diagram for \(\{N(O_{1}O_{2})^{2}\hat{\bm{\Omega}}_{g}\}_{g=1}^{G}\).

### Kronecker-factored Approximate Curvature (KFAC) for Transpose Convolution

Here we derive the KFAC approximation for transpose convolutions.

We describe transpose convolution in terms of its associated convolution from an input space \(\mathcal{X}=\mathbb{R}^{C_{\text{in}}\times I_{1}\times I_{2}}\) to an output space \(\mathcal{Y}=\mathbb{R}^{C_{\text{out}}\times O_{1}\times O_{2}}\). The convolution has hyper-parameters \(K_{1,2},S_{1,2},P_{1,2},D_{1,2}\) with index patterns \(\bm{\mathsf{\Pi}}^{(1)}=\bm{\mathsf{\Pi}}(I_{1},K_{1},S_{1},P_{1},D_{1})\in \mathbb{R}^{I_{1}\times O_{1}\times K_{1}}\) and \(\bm{\mathsf{\Pi}}^{(2)}=\bm{\mathsf{\Pi}}(I_{2},K_{2},S_{2},P_{2},D_{2})\in \mathbb{R}^{I_{2}\times O_{2}\times K_{2}}\).

Transpose convolution as matrix multiplication:Transpose convolution maps a \(\bm{\mathsf{V}}\in\mathcal{Y}\) into an \(\bm{\mathsf{X}}\in\mathcal{X}\). In ML frameworks like PyTorch, its kernel \(\bm{\mathsf{\tilde{W}}}\) is stored as \(C_{\text{out}}\times C_{\text{in}}\times K_{1}\times K_{2}\) tensor. The relation \(\bm{\mathsf{X}}=\bm{\mathsf{\tilde{W}}}\star_{\text{T}}\bm{\mathsf{Y}}\) where \(\star_{\text{T}}\) denotes transpose convolution is given by Figure 2(d),

\[\bm{\mathsf{X}}_{c_{\text{in}},i_{1},i_{2}}=\sum_{c_{\text{out}}=1}^{C_{\text{ out}}}\sum_{k_{1}=1}^{K_{1}}\sum_{k_{2}=1}^{K_{2}}\sum_{o_{1}=1}^{O_{1}}\sum_{o_{2}= 1}^{O_{2}}\bm{\mathsf{\Pi}}^{(1)}_{i_{1},o_{1},k_{1}}\bm{\mathsf{\Pi}}^{(2)}_{ i_{2},o_{2},k_{2}}\,\bm{Y}_{c_{\text{out}},k_{1},k_{2}}\,\bm{\mathsf{\tilde{W}}}_{c_{ \text{out}},c_{\text{in}},k_{1},k_{2}}\] (111)

Our goal is to turn the express the above as matrix multiplication. To do that, we first define the matrix reshape \(\bm{X}\) of \(\bm{\mathsf{X}}\) via \(\bm{X}\in\mathbb{R}^{C_{\text{in}}\times I_{1}I_{2}}\) such that \([\bm{X}]_{c_{\text{in}},(i_{1},i_{2})}=\bm{\mathsf{X}}_{c_{\text{in}},i_{1},i_{ 2}}\). Next, we consider a transposed kernel \(\bm{\mathsf{W}}\) of \(\bm{\mathsf{\tilde{W}}}\) with changed order of the first two indices, i.e.\(\bm{\mathsf{W}}\in\mathbb{R}^{C_{\text{in}}\times C_{\text{out}}\times K_{1} \times K_{2}}\) such that

\[\bm{W}_{c_{\text{in}},c_{\text{out}},k_{1},k_{2}}=\bm{\mathsf{\tilde{W}}}_{c_{ \text{out}},c_{\text{in}},k_{1},k_{2}}\,.\] (112)

This transposition is necessary to convert the kernel's layout in the ML framework to a layout that admits Equation (111) to be expressed as matrix multiplication. Using a matrix reshape \(\bm{W}\) of \(\bm{\mathsf{W}}\) via \(\bm{W}\in\mathbb{R}^{C_{\text{in}}\times C_{\text{out}}K_{1}K_{2}}\) such that \([\bm{W}]_{c_{\text{in}},(c_{\text{out}},k_{1},k_{2})}=\bm{\mathsf{\tilde{W}}}_{c_ {\text{in}},c_{\text{out}},k_{1},k_{2}}\), we can express Equation (111) as matrix multiplication

\[\bm{X}=\bm{W}[\![\bm{\mathsf{\tilde{V}}}]\!]_{\text{T}}\] (113)

where \([\![\bm{\mathsf{\tilde{V}}}]\!]_{\text{T}}\in\mathbb{R}^{C_{\text{out}}K_{1}K_{ 2}\times I_{1}I_{2}}\) is the _transpose-unfolded input_ to the transpose convolution (note that \([\![\cdot]\!]\neq[\![\cdot]\!]_{\text{T}}\)!)

\[[[\![\bm{\mathsf{\tilde{V}}}]\!]_{\text{T}}]_{(c_{\text{out}},k_{1},k_{2}),(i_{1}, i_{2})}=\sum_{o_{1}=1}^{O_{1}}\sum_{o_{2}=1}^{O_{2}}\bm{\mathsf{\Pi}}^{(1)}_{i_{1},o_{1},k_{1}} \bm{\mathsf{\Pi}}^{(2)}_{i_{2},o_{2},k_{2}}\,\bm{Y}_{c_{\text{out}},o_{1},o_{2}}\,.\] (114)

To the best of our knowledge there is no API for \([\![\cdot]\!]_{\text{T}}\) in existing ML frameworks. Our approach can provide a simple and efficient implementation of \([\![\cdot]\!]\) through the TN shown in Figure 12(a) which corresponds to Equation (114). As Equation (113) is of the same form as Equation (1), it is now straightforward to write down the KFAC approximations for transpose convolution.

KFAC-expand:We will define the KFAC-expand approximation for the GGN w.r.t. the flattened kernel \(\bm{w}\) of \(\bm{W}\). Note that, in practise, this approximation must be properly transformed back to the layout \(\bm{\mathsf{\tilde{W}}}\) of the ML framework. We have \(\bm{G}(\bm{w})\approx\bm{\Omega}\otimes\bm{\Gamma}\), with \(\bm{\Gamma}\in\mathbb{R}^{C_{\text{in}}\times C_{\text{in}}}\) computed from backpropagated gradients, and the input-based Kronecker factor

\[\bm{\Omega}=[\![\bm{\mathsf{\tilde{V}}}]\!]_{\text{T}}[\![\bm{\mathsf{\tilde{V}}}]\!] _{\text{T}}^{\top}\in\mathbb{R}^{C_{\text{out}}K_{1}K_{2}\times C_{\text{out}}K_{1} K_{2}}\,.\] (115)

See Figure 12(b) for the corresponding TN.

KFAC-reduce:For KFAC-reduce, we have \(\bm{G}(\bm{w})\approx\hat{\bm{\Omega}}\otimes\hat{\bm{\Gamma}}\), with \(\hat{\bm{\Gamma}}\in\mathbb{R}^{C_{\text{in}}\times C_{\text{in}}}\) computed from backpropagated gradients, and the input-based Kronecker factor

\[\hat{\bm{\Omega}}=\frac{1}{(I_{1}I_{2})^{2}}\left(\bm{1}_{I_{1}I_{2}}^{\top}[ \![\bm{\mathsf{V}}]\!]_{\text{T}}\right)\left(\bm{1}_{I_{1}I_{2}}^{\top}[\![ \bm{\mathsf{V}}]\!]_{\text{T}}\right)^{\top}\in\mathbb{R}^{C_{\text{out}}K_{1 }K_{2}\times C_{\text{out}}K_{1}K_{2}}\;.\] (B14)

See Figure B13c for the corresponding TN.

With batching and groups:In the presence of \(G\) groups, we have per-group kernels \(\hat{\bm{W}}_{g}=[\hat{\bm{W}}]_{(g,:),:,:,:}\in\mathbb{R}^{C_{\text{out}}/ \sigma\times C_{\text{in}}/\sigma\times K_{1}\times K_{2}}\) and \(\bm{W}_{g}\in\mathbb{R}^{C_{\text{in}}/\sigma\times C_{\text{out}}/\sigma \times K_{1}\times K_{2}}\), as well as per-group transpose-unfolded inputs \([\![\bm{\mathsf{V}}_{g}]\!]_{\text{T}}=[\![\bm{\mathsf{V}}]\!]_{\text{T}(g,: ),:,:}=[\![\bm{\mathsf{V}}]\!]_{(g,:),:,:}]\!]_{\text{T}}\in\mathbb{R}^{C_{ \text{out}}/GK_{1}K_{2}\times I_{1}I_{2}}\). Each group corresponds to a transpose convolution in itself. With batching, we have an additional leading batch dimension, i.e. \([\![\bm{\mathsf{V}}_{n,g}]\!]_{\text{T}}\). Applying the same steps from above, we can define the KFAC approximation for the GGN w.r.t. the flattened per-group kernel \(\bm{w}_{g}\) of \(\bm{W}_{g}\).

For KFAC-expand, we have \(\bm{G}(\bm{w}_{g})\approx\bm{\Omega}_{g}\otimes\bm{\Gamma}_{g}\), with \(\bm{\Gamma}_{g}\in\mathbb{R}^{C_{\text{in}}/\sigma\times C_{\text{in}}/G}\) computed from backpropagated gradients, and the input-based Kronecker factor

\[\bm{\Omega}_{g}=\frac{1}{N}\sum_{n=1}^{N}[\![\bm{\mathsf{V}}_{n,g}]\!]_{\text {T}}[\![\bm{\mathsf{V}}_{n,g}]\!]_{\text{T}}^{\top}\in\mathbb{R}^{C_{\text{ out}}/GK_{1}K_{2}\times C_{\text{out}}/GK_{1}K_{2}}\;.\]

For KFAC-reduce, we have \(\bm{G}(\bm{w}_{g})\approx\hat{\bm{\Omega}}_{g}\otimes\hat{\bm{\Gamma}}_{g}\), with \(\hat{\bm{\Gamma}}_{g}\in\mathbb{R}^{C_{\text{in}}/\sigma\times C_{\text{in}}/G}\) computed from backpropagated gradients, and the input-based Kronecker factor

\[\hat{\bm{\Omega}}_{g}=\frac{1}{N(O_{1}O_{2})^{2}}\sum_{n=1}^{N}\left(\bm{1}_{ I_{1}I_{2}}^{\top}[\![\bm{\mathsf{V}}_{n,g}]\!]_{\text{T}}\right)\left(\bm{1}_{I_{1}I_{2 }}^{\top}[\![\bm{\mathsf{V}}_{n,g}]\!]_{\text{T}}\right)^{\top}\in\mathbb{R}^{ C_{\text{out}}/\sigma GK_{1}K_{2}\times C_{\text{out}}/GK_{1}K_{2}}\;.\]

### Further Operations & Extensive Overview

Consecutive convolutions:We can chain two, or more, convolutions into a single TN diagram (Figure B14) to obtain a deep linear CNN [65] similar to deep linear networks which are popular for analytical studies.

Convolution weight/input JVPs:In the main text, we derived the Jacobians of convolution (SS3.1) which can be used to derive the JVPs. A JVP propagates perturbations \(\bm{\mathsf{V}}^{(\bm{\mathsf{W}})}\in\mathbb{R}^{C_{\text{out}}\times C_{\text {in}}\times K_{1}\times K_{2}}\) and \(\bm{\mathsf{V}}^{(\bm{\mathsf{X}})}\in\mathbb{R}^{C_{\text{in}}\times I_{1} \times I_{2}}\) in the input space to perturbations in the output space by contracting the perturbation with the Jacobian. See Table B3 for the general einsum expressions.

Batched convolution weight VJP:To obtain per-sample gradients, the weight VJP must be carried out without summing over the batch axis which amounts to keeping the batch index in the output index tuple.

VJPs and JVPs of im2col:With the TN differentiation technique described in SS3.1 we can compute the Jacobian of the unfolding operation, then contract it with perturbations \(V^{(\bm{\mathsf{X}})}\in\mathbb{R}^{C_{\text{in}}\times K_{1}\times K_{2}}\) in input space to obtain the JVP, or with perturbations \(V^{([\bm{\mathsf{X}}])}\in\mathbb{R}^{O_{1}O_{2}\times C_{\text{in}}K_{1}K_{2}}\) to obtain the VJP.

Approximate Hessian diagonals (HesScale/BL89):Becker & Lecun [4], Elsayed et al. [22] proposed approximate procedures for the Hessian diagonal which cost roughly a gradient. They can be understood as modifications of the Hessian backpropagation equations from Dangel et al. [16].

Consider a layer with input \(\bm{x}\), output \(\bm{y}\), and weights \(\bm{w}\) inside a sequential feedforward neural network (for a convolutional layer, these correspond to the flattened input, output, and kernel). To compute per-layer Hessians of a loss \(\ell\), each layer backpropagates its incoming Hessian \(\nabla^{2}_{\bm{y}}\ell\) according to [16]

\[\nabla^{2}_{\bm{x}}\ell =(\bm{J}_{\bm{x}}\bm{y})^{\top}\nabla^{2}_{\bm{y}}\ell(\bm{J}_{ \bm{x}}\bm{y})+\sum_{i}\frac{\partial\ell}{\partial y_{i}}\nabla^{2}_{\bm{x}}y _{i}\,,\] (B15) \[\nabla^{2}_{\bm{w}}\ell =(\bm{J}_{\bm{w}}\bm{y})^{\top}\nabla^{2}_{\bm{y}}\ell(\bm{J}_{ \bm{w}}\bm{y})+\sum_{i}\frac{\partial\ell}{\partial y_{i}}\nabla^{2}_{\bm{w}}y _{i}\,.\]

The scheme of [4, 22] imposes diagonal structure on the backpropagated quantity. A layer receives a backpropagated diagonal \(\bm{d}^{(\bm{y})}\) such that \(\operatorname{diag}(\bm{d}^{(\bm{y})})\approx\nabla^{2}_{\bm{y}}\ell\), and backpropagates it according to Equation (B15), but with a post-processing step to obtain a diagonal backpropagated quantity,

\[\bm{d}^{(\bm{x})} =\operatorname{diag}\left((\bm{J}_{\bm{x}}\bm{y})^{\top} \operatorname{diag}(\bm{d}^{(\bm{y})})(\bm{J}_{\bm{x}}\bm{y})\right)+ \operatorname{diag}\left(\sum_{i}\frac{\partial\ell}{\partial y_{i}}\nabla^{2 }_{\bm{x}}y_{i}\right)\,,\] (B16) \[\bm{d}^{(\bm{w})} =\operatorname{diag}\left((\bm{J}_{\bm{w}}\bm{y})^{\top} \operatorname{diag}(\bm{d}^{(\bm{w})})(\bm{J}_{\bm{w}}\bm{y})\right)+ \operatorname{diag}\left(\sum_{i}\frac{\partial\ell}{\partial y_{i}}\nabla^{2 }_{\bm{w}}y_{i}\right)\,,\]

where \(\operatorname{diag}(\bm{d}^{(\bm{x})})\approx\nabla^{2}_{\bm{x}}\ell\) and \(\operatorname{diag}(\bm{d}^{(\bm{w})})\approx\nabla^{2}_{\bm{w}}\ell\) is an approximation to the Hessian diagonal.

For convolutional layers, which are linear in the input and weight, the second summands are zero due to \(\nabla^{2}_{\bm{x}}y_{i}=\bm{0}=\nabla^{2}_{\bm{w}}y_{i}\). The first terms of Equation (B16) require (i) embedding a diagonal vector into a matrix, (ii) applying MJPs and JMPs, and (iii) extracting the result's diagonal. Those can be expressed as a single TN. We show the diagrams in Figure 15, using tensors rather than their flattened versions, that is \((\bm{x},\bm{y},\bm{w},\bm{d}^{(\bm{x})},\bm{d}^{(\bm{y})},\bm{d}^{(\bm{w})}) \rightarrow(\mathbf{X},\mathbf{Y},\mathbf{W},\mathbf{D}^{(\mathbf{X})}, \mathbf{D}^{(\mathbf{Y})},\mathbf{D}^{(\mathbf{W})})\).

Figure B15: TN diagrams for HesScale/BL89 [4, 22] backpropagations through convolutional layers to approximate the Hessian diagonals \(\mathbf{D^{(X)}},\mathbf{D^{(W)}}\). JMPs and MJPs are shaded. (a, b) show the simple versions without batching and without channel groups. (c, d) include batching and channel groups.

[MISSING_PAGE_EMPTY:23]

## Appendix C Exact Second-Order Information

Here, we look at computing second-order information of a loss w.r.t. to the kernel of a convolution. Its computation can be phrased as backpropagation with a final extraction step [19] which contains less standard operations like Jacobian-matrix products (JMPs) and sub-tensor extraction. TNs can express this extraction step in a single diagram.

Consider a datum \((\bm{x},\bm{t})\) and its loss \(\ell(\bm{w})=\ell(\bm{f},\bm{t})\) where \(\bm{f}:=f_{\bm{w}}(\bm{x})\in\mathbb{R}^{C}\) is the prediction of a CNN with a convolution with flattened kernel \(\bm{w}\) and flattened output \(\bm{y}\) (derivations carry over to a batch loss). The kernel's generalized Gauss-Newton (GGN) matrix [63]\(\bm{G}(\bm{w})=(\bm{J_{w}}\bm{f})^{\top}\nabla_{\bm{f}}^{2}\ell(\bm{J_{w}}\bm{ f})\in\mathbb{R}^{C_{\text{out}}\text{Ca}_{\text{in}}K_{1}K_{2}\times C_{ \text{out}}\text{Ca}_{\text{in}}K_{1}K_{2}}\) is a positive semi-definite Hessian proxy preferred by many applications [e.g. 20, 45] and coincides with the Fisher information matrix for many common losses [46]. It is the self-outer product of a backpropagated symmetric factorization \(\bm{S}^{(\bm{y})}=(\bm{J_{y}}\bm{f})^{\top}\bm{S}^{(\bm{f})}\in\mathbb{R}^{C_ {\text{out}}O_{1}O_{2}\times C}\) of the loss Hessian, \(\nabla_{\bm{f}}^{2}\ell(\bm{f},\bm{y})=\bm{S}^{(\bm{f})}(\bm{S}^{(\bm{f})})^{\top}\). During backpropagation, the convolution extracts information about \(\bm{G}(\bm{w})=(\bm{J_{w}}\bm{y})^{\top}\bm{S}^{(\bm{y})}(\bm{S}^{(\bm{y})})^ {\top}\bm{J_{w}}\bm{y}\).

In TN notation, this is easy to express without flattening: We simply compose two VJP diagrams from Figure 2(c) with an extra leg (MJP) and add the outer-product contraction to obtain the tensor version \(\bm{\mathsf{G}}(\bm{W})\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times K _{1}\times K_{2}\times C_{\text{out}}\times C_{\text{in}}\times K_{1}\times K _{2}}\) of \(\bm{G}(\bm{w})\) (Figure 9(a)). The GGN is often further approximated by sub-tensors as it is too large. These slicing operations are also easy to integrate into the diagrams, e.g. to extract diagonal elements (Figure 9(b); [51]), or mini-block diagonals (Figure 10(b); [16]). This also removes redundant computations compared to computing, then slicing, the matrix. The same ideas apply to the GGN Gram matrix \((\bm{S}^{(\bm{w})})^{\top}\bm{S}^{(\bm{w})}\in\mathbb{R}^{C\times C}\) (Figure 9(c)). It contains the GGN spectrum [18] and is related to the empirical NTK for square loss [49].

Implementation Details

Here we present details on the index pattern computation, and additional transformations.

### Index Pattern Tensor Computation for Convolutions

Algorithm D1 lists pseudo-code for the index pattern computation from the convolution hyper-parameters \(K,S,P,D\), and the spatial input dimension \(I\), that is \(\boldsymbol{\mathsf{\Pi}}(I,K,S,P,D)\). _Unlike in the main text, we use 0-based indexing which is more common in numerical libraries._ For self-consistency, we re-state the relation of the hyper-parameters to output dimension from [21, Relationship 15],

\[O(I,K,S,P,D)=1+\left\lfloor\frac{I+2P-K-(K-1)(D-1)}{S}\right\rfloor\,.\] (D17)

```
0: Input size \(I\in\mathbb{N}^{+}\), kernel size \(K\in\mathbb{N}^{+}\), stride \(S\in\mathbb{N}^{+}\), padding \(P\in\mathbb{N}^{+}_{0}\), dilation \(D\in\mathbb{N}^{+}\) \(O\gets 1+\left\lfloor\frac{I+2P-K-(K-1)(D-1)}{S}\right\rfloor\)\(\triangleright\)Compute output dimension [21, Relationship 15] \(\boldsymbol{\mathsf{\Pi}}\leftarrow\boldsymbol{0}_{I\times O\times K}\)\(\triangleright\)Initialize index pattern tensor for\(o=0,\dots,O-1\), \(k=0,\dots,K-1\)do\(\triangleright\)Use 0-based indexing! \(i\gets KD+oS-P\)\(\triangleright\)Reconstruct contributing input element if\(0\leq i\leq I-1\)then\(\triangleright\)Check in bounds \(\overline{\Pi}_{i,o,k}\gets 1\) endif endfor return Index pattern tensor \(\boldsymbol{\mathsf{\Pi}}\in\{0,1\}^{I\times O\times K}\) ```

**Algorithm D1** Computing the convolution index pattern tensor \(\boldsymbol{\mathsf{\Pi}}\) for a spatial dimension.

### Index Pattern Tensor for Standalone Transpose Convolution

Although a transpose convolution is defined w.r.t. a reference convolution with hyper-parameters \(K,S,P,D\), most libraries offer standalone implementations of transpose convolution. We describe the transpose convolution by its associated convolution, that is as a mapping from \(\mathbb{R}^{C_{\text{out}}\times O_{1}\times O_{2}}\) (the convolution's output space) to \(\mathbb{R}^{C_{\text{in}}\times I_{1}\times I_{2}}\) (the convolution's input space). For convolution with \(S>1\), we cannot infer \(I\) from \(O,K,S,P,D\), as multiple \(I\)s map to the same \(O\) if \((I+2P-K-(K-1)(D-1))\mod S\neq 0\) (see the floor operation in Algorithm D1). We need to either supply \(I\) directly, or the remainder

\[A=I+2P-K-(K-1)(D-1)-S(O-1)\]

(often called output_padding) to make \(I\) unambiguous. Then, we compute

\[I=(O-1)S-2P+K+(K-1)(D-1)+A\,.\] (D18)

to get \(I(O,A)\) and call Algorithm D1 to obtain \(\boldsymbol{\mathsf{\Pi}}(I(O,A),K,S,P,D)\).

### Details on Index Pattern Simplifications

In the following, we will assume the absence of boundary pixels that don't overlap with the kernel, that is

\[I+2P-(K+(K-1)(D-1))\mod S=0\,,\] (D19)

where the floor operation in \(O(I,K,S,P,D)\) is obsolete. This can always be assured by narrowing \(\boldsymbol{\mathsf{X}}\) before a convolution. Based on our hyper-parameter analysis of real-world CNNs (SSE), we identify:

**Transformation D1 (Dense convolutions)**: _Assume Equation (D19). For \(K=S\) with default padding and dilation (\(P=0\), \(D=1\)), patches are adjacent non-overlapping tiles, accessible by un-grouping the input index \(i\) into a tuple index \((\tilde{i},\tilde{k})\) of size \({}^{I}\!/_{K}\times K\):_

\[[\boldsymbol{\mathsf{\Pi}}(I,K,K,0,1)]_{i,o,k}=[\boldsymbol{\mathsf{\Pi}}(I,K, K,0,1)]_{(\tilde{i},\tilde{k}),o,k}=\delta_{i,o}\delta_{\tilde{k},k}\,.\]

_Point-wise convolutions (\(K=S=1\)) are a special case with pattern \([\boldsymbol{\mathsf{\Pi}}(I,1,1,0,1)]_{i,o,k}=\delta_{i,o}\)._Point-wise convolutions with \(K=S=1\) are common in DenseNets [33], MobileNets [31, 60] and ResNets [30]. InceptionV3 [69] has 2d'mixed dense' convolutions that are point-wise along one spatial dimension. ConvNeXt [42] uses dense convolutions with \(K=S\in\{2,4\}\).

**Transformation D2 (Down-sampling convolutions)** _For \(S>K\) with default padding and dilation (\(P=0\), \(D=1\)), some elements do not overlap with the kernel. If the input dimension \(i\) is summed, all participating tensors can be pruned to remove the explicit zeros. Assume \(I\mod S=0\). Then, pruning amounts to un-grouping \(i\) into \((i^{\prime},s)\) of size \(\nicefrac{{1}}{{s}}\times S\), narrowing \(s\) to \(K\) entries, and grouping back into an index \(\tilde{i}\) of size \(\nicefrac{{K1}}{{S}}\). After pruning, the index pattern represents a dense convolution with input size \(\nicefrac{{K1}}{{S}}\), kernel size \(K\), and stride \(K\). In a contraction with some tensor **V**,_

\[\sum_{i=1}^{I}\left[\textbf{V}\right]_{\ldots,i,\ldots}[\textbf{\Pi}(I,K,S>K, 0,1)]_{i,o,k}=\nicefrac{{\sum_{i=1}^{\prime/S}}}{{|\tilde{\textbf{V}}|}}_{ \ldots,\tilde{i},\ldots}[\textbf{\Pi}(\nicefrac{{KI}}{{S}},K,K,0,1)]_{i,o,k}\]

_with sub-tensor \([\tilde{\textbf{V}}]_{\ldots,\tilde{i},\ldots}=[[\textbf{V}]_{\ldots,(i^{ \prime},s),\ldots}]_{\ldots,(:,:K),\ldots}\) where \(:K\) means narrowing to \(K\) elements._

Transformation D2 converts down-sampling convolutions to dense convolutions, which can be further simplified with Transformation D1. We find down-sampling convolutions with \(S=2>K=1\) in ResNet18 [30], ResNext101 [72], and WideResNet101 [73]. Those convolutions discard 75 % of their input! Knowledge that an operation only consumes a fraction of its input could be used to eliminate those 'dead' computations in preceding operations, reducing FLOPS and memory.

**Transformation D3 (Kernel-output dimension swap)** _Assume Equation_ (D19). _Transposing kernel and output dimensions in an index pattern yields another index pattern with same input size, kernel size \(O(I,K,S,P,D)\), and swapped stride and dilation:_

\[[\textbf{\Pi}(I,K,S,P,D)]_{i,o,k}=[\textbf{\Pi}(I,O,D,P,S)]_{i,k,o}\.\]

This transformation is easy to see from the symmetry of \((k,D)\) and \((o,S)\) in Equation (7) and \(O(I,K,S,P,D)\). It converts index pattern contractions over output into kernel dimensions, like in convolutions. An example is the weight VJP from Figure (c)c, which--after swapping kernel and output dimensions--resembles the TN for convolution from Figure 2 with kernel **V**. Rochette et al. [58] use this to phrase the computation of per-example gradients as convolution.

SSD.3 presents more properties of **\(\boldsymbol{\Pi}\)** based on the sub-sampling interpretation of stride and dilation along the output and kernel dimensions. We also provide a transformation for swapping input and output dimensions, relating convolution and transpose convolution as described in [21].

For completeness, we state additional index pattern tensor properties here (using 1-based indexing):

**Transformation D4 (Sub-sampling interpretation of stride)** _Strided convolutions (\(S>1\)) sub-sample non-strided convolutions along the output dimension, ignoring all but every \(S\)th output [21]. In other words, \([\textbf{\Pi}(I,K,S,P,D)]_{i,o,k}=[\textbf{\Pi}(I,K,1,P,D)]_{i,1+S(o-1),k}\) or, in tensor notation \((\cdot]_{::S}\) denotes slicing with steps of \(S\)),_

\[\textbf{\Pi}(I,K,S,P,D)=[\textbf{\Pi}(I,K,1,P,D)]_{:,:,:S,:}\.\]

**Transformation D5 (Sub-sampling interpretation of dilation)** _Dilated convolutions (\(D>1\)) with kernel size \(K\) sub-sample the kernel of a non-dilated convolution of kernel size \(K+(D-1)(K-1)\), ignoring all but every \(D\)th kernel element. In other words, \([\textbf{\Pi}(I,K,S,P,D)]_{i,o,k}=[\textbf{\Pi}(I,K+(K-1)(D-1),S,P,1)]_{i,o,1+ D(k-1)}\) or, in tensor notation,_

\[\textbf{\Pi}(I,K,S,P,D)=[\textbf{\Pi}(I,K+(K-1)(D-1),S,P,1)]_{:,:,::D}\.\]

**Transformation D6 (Transpose convolution as convolution)** _Assume Equation_ (D19). _Consider a non-strided (\(S=1\)), non-dilated (\(D=1\)) convolution with index pattern \(\textbf{\Pi}(I,K,1,P,1)\) and output dimension \(O(I,K,1,P,1)\). Transposing the spatial dimensions and flipping the kernel dimension yields another index pattern with modified padding \(P^{\prime}=K-P-1\). In other words, for all \(i=1,\ldots,I\), \(k=1,\ldots,K\), \(o=1,\ldots,O\)_

\[[\textbf{\Pi}(I,K,1,P,1)]_{i,o,k}=[\textbf{\Pi}(O,K,1,P^{\prime},1)]_{o,i,K+1-k }\.\]Convolution Layer Hyper-parameter Analysis

Here we give an overview of and characterize convolutions in popular architectures (see Table 10). We include moderately deep CNNs on Fashion MNIST, CIFAR-10, and CIFAR-100 from the DeepOBS benchmark [62], and deep CNNs on ImageNet (AlexNet, ResNet18, InceptionV3, MobileNetV2, ResNext101). Regarding the hyper-parameters, we make the following observations:

* Many CNNs do not use a bias term. This is because the output of those layers feeds directly into a batch normalization layer, which is invariant under the addition of a bias term.
* All investigated convolutions use default dilation.
* Group convolutions are rarely used. MobileNetV2 and ConvNeXt-base (Tables 11(g) and 12) use group convolutions that interpret each individual channel as a group. ResNext101 (Table 12) uses group convolutions that interpret a collection of channels as a group. ConvNeXt-base (Table 11(g)) uses dense convolutions with \(P=0\) and \(S=K\in\{2,4\}\).
* Many networks use dense convolutions, that is convolutions with unit kernel size \((K=1)\), unit stride \((S=1)\), and no padding \((P=0)\). These convolutions have a trivial index pattern and can therefore be simplified.
* InceptionV3 (Table 12(h)) uses two-dimensional convolutions with one trivial dimension ('mixed dense') with unit kernel size, unit stride, and no padding along one direction. For this spatial dimension, the index pattern can be simplified.
* ResNet18 (Table 11(e)) and ResNext101 (Table 12) use convolutions with \(S>K\) for down-sampling whose kernel only overlaps with a fraction of the input. The index pattern can be simplified.

\begin{table}

\end{table}
Table E4: Hyper-parameters of convolutions in different CNNs. For convolutions with identical hyper-parameters, we only show one instance and its multiplicity.

\begin{table}

\end{table}
Table E4: Hyper-parameters of convolutions in different CNNs. For convolutions with identical hyper-parameters, we only show one instance and its multiplicity.

## Appendix F Run Time Evaluation Details (GPU)

Here we provide all details on the run time evaluation from the main text. We consider the convolutions from the CNNs from SSE. Experiments were carried out on an Nvidia Tesla T4 (16 GB memory). We use a batch size of 32 for the ImageNet architectures, and 128 for the others.

### Protocol & Overview

We compare different implementations of the same operations in PyTorch. The base line (referenced by 'PT') uses PyTorch's built-in functionalities for convolutions and related operations, such as torch.nn.functional.conv2d (forward), torch.nn.functional.unfold (KFC, KFAC-reduce), and PyTorch's built-in automatic differentiation torch.autograd.grad (VJPs).

Our TN implementation (referenced by 'TN') sets up operands and the string-valued equation for each routine. Optionally, we can apply the simplifications from SS4 as a post-processing step before contraction, which yields a modified equation and operand list ('TN + opt'). Finally, we determine the contraction path using opt_einsum.contract_path and perform the contraction with its PyTorch back-end (opt_einsum.contract). We only measure the contraction time as in practical settings, the contraction path search would be executed once, then cached. We also exclude final operations to obtain the correct shape or scale (flattening, reshaping, scaling by constant) in all implementations (including the base line).

For each operation and each convolution layer, we perform 50 independent repetitions and report the minimum time in tables. To summarize those tables, we extract the performance ratios, that is the TN implementation's run time divided by the base line's. Ratios larger than 1 mean that the TN implementation is slower, ratios smaller than 1 indicate that it is faster than the base line. We collect those ratios for the different convolution types (general, mixed dense, dense, sub-sampling) and display them separately using box plots. Each operation has two boxes, corresponding to the un-simplified (TN), and the simplified (TN + opt) implementation. For the box plots, we use matplotlib's default settings (a box extends from the first quartile to the third quartile of the data, with a line at the median; whiskers extend from the box by 1.5x the inter-quartile range; flier points are those past the end of the whiskers). Figure F17 summarizes the entire GPU benchmark. Figure F18 shows the same information with each convolution type as an individual plot.

Figure F18: Impact of TN simplifications (non-simplified performance ratios shown in blue). TN simplifications improve performance on (a) mixed dense, (b) dense, and (c) down-sampling convolutions. (d) General convolutions are not affected by TN simplifications.

[MISSING_PAGE_EMPTY:32]

\begin{table}

\end{table}
Table 5: Forward pass performance comparison on GPU. (a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)

[MISSING_PAGE_EMPTY:35]

\begin{table}

\end{table}
Table 6: Input VJP performance comparison on GPU.

[MISSING_PAGE_FAIL:38]

\begin{table}

\end{table}
Table F7: Weight VJP performance comparison on GPU.

[MISSING_PAGE_FAIL:41]

\begin{table}

\end{table}
Table 10: KFC (KFAC-expand) factor performance comparison on GPU.

(a) 3c3d, CIFAR-10, input shape (128, 3, 32, 32)

\begin{table}

\end{table}
Table 11: KFC (KFAC-expand) factor performance comparison on GPU.

[MISSING_PAGE_FAIL:44]

\begin{table}

\end{table}
Table 10: KFAC-reduce factor performance comparison on GPU.

Memory Evaluation Details (CPU)

Here, we investigate the peak memory consumption of our proposed TN implementations.

### Theoretical & Empirical Analysis for KFAC-reduce Factor

We assume a two-dimensional convolution with input **X** of shape \((C_{\text{in}},I_{1},I_{2})\), output of shape \((C_{\text{out}},O_{1},O_{2})\) and kernel of shape \((C_{\text{out}},C_{\text{in}},K_{1},K_{2})\). The analysis with a batch dimension is analogous; hence we suppress it here to de-clutter the notation.

The main difference between the default and our proposed TN implementation of \(\hat{\boldsymbol{\Omega}}\) from SS3.3 lies in the computation of the averaged unfolded input which consists of \(C_{\text{in}}K_{1}K_{2}\) numbers. In the following, we will look at the extra memory on top of storing the input **X**, the averaged unfolded input \(\llbracket\textbf{X}\rrbracket^{(\text{avg})}\), and the result \(\hat{\boldsymbol{\Omega}}\).

Default implementation:The standard implementation computes \(\llbracket\textbf{X}\rrbracket^{(\text{avg})}\) via the unfolded input \(\llbracket X\rrbracket\) and thus requires extra storage of \(C_{\text{in}}K_{1}K_{2}O_{1}O_{2}\) numbers.

TN implementation (general case):The TN implementation requires storing the averaged index patterns \(\boldsymbol{\mathsf{\Pi}}^{(i,\text{avg})}:=\nicefrac{{1}}{{O_{i}}}\sum_{o= 1}^{O_{i}}[\boldsymbol{\mathsf{\Pi}}^{(i)}]_{:,o,:}\) for \(i=1,2\). These are directly computed via a slight modification of Algorithm 1 and require storing \(I_{1}K_{1}+I_{2}K_{2}\) numbers. In contrast to the default implementation, spatial dimensions are de-coupled and there is no dependency on \(C_{\text{in}}\).

TN implementation (structured case):For structured convolutions (Figure 5) we can describe the action of the index pattern tensor through reshape and narrowing operations. ML libraries usually perform these without allocating additional memory. Hence, our symbolic simplifications completely eliminate the allocation of temporary intermediates to compute \(\llbracket\textbf{X}\rrbracket^{(\text{avg})}\).

Empirical results:To demonstrate the memory reduction inside the computation of \(\hat{\boldsymbol{\Omega}}\) we measure its peak memory with the memory-profiler library and subtract the memory required to store **X** and \(\hat{\boldsymbol{\Omega}}\). This approximates the extra internal memory requirement of an implementation. With the setup of \(\lx@sectionsign\)F we report the minimum additional memory over 50 independent runs in Table 10. We consistently observe that the TN implementation has lower peak memory, which is further reduced by our symbolic simplifications (see for example the effect on ResNext101's dense and down-sampling convolutions in Table 10).

Our theoretical analysis from above suggests that the peak memory difference becomes most visible for many channels with large kernel and output sizes. One example are ConxNeXt-base's features.1.0.block.0 convolutions with \(K_{1}=K_{2}=7\), \(O_{1}=O_{2}=64\), and \(C_{\text{in}}=128\) (Table 4g). For those convolutions, we observe that the default implementation requires an additional 3,140 MiB (\(\approx 3\) GiB!) of memory, whereas the TN implementation has zero extra memory demand (Table 10g). This is consistent with our theoretical analysis in that the overhead is storing the unfolded input, which has \((N=32)\cdot(C_{\text{in}}=128)\cdot(O_{1}=64)\cdot(O_{2}=64)\cdot(K_{1}=7) \cdot(K_{2}=7)=822,083,584\) float32 entries, corresponding to 3,136 MiB.

\begin{table}

\end{table}
Table G10: Additional internally required memory to compute the KFAC-reduce factor (measured on CPU). The value 0 indicates that an implementation’s peak memory matches the memory consumption of its input **X** and result \(\hat{\bm{\Omega}}\).

Miscellaneous

### Example: Associativity of Tensor Multiplication

Here, we demonstrate associativity of tensor multiplication through an example. The technical challenge is that an index can only be summed once there are no remaining tensors sharing it. Therefore, we must carry indices that are summed in later multiplications in the intermediate results, which requires some set arithmetic on the index sets.

Let \(S_{1},S_{2},S_{3}\) be index tuples of the input tensors \(\boldsymbol{\mathsf{A}}\), \(\boldsymbol{\mathsf{B}}\), \(\boldsymbol{\mathsf{C}}\), and \(S_{4}\subseteq(S_{1}\cup S_{2}\cup S_{3})\) a valid output index tuple of their tensor multiplication \(\boldsymbol{\mathsf{D}}=*_{(S_{1},S_{2},S_{3},S_{4})}(\boldsymbol{\mathsf{A}},\boldsymbol{\mathsf{B}},\boldsymbol{\mathsf{C}})\). We can either first multiply \(\boldsymbol{\mathsf{A}}\) with \(\boldsymbol{\mathsf{B}}\) to obtain an intermediate tensor of index structure \(S_{1,2}\), or \(\boldsymbol{\mathsf{B}}\) with \(\boldsymbol{\mathsf{C}}\) to obtain an intermediate tensor of index structure \(S_{2,3}\), before carrying out the remaining multiplications. To construct the intermediate index structures, we divide the indices \(\tilde{S}=(S_{1}\cup S_{2}\cup S_{3})\setminus S_{4}\) that are summed over into those only shared between \(\boldsymbol{\mathsf{A}}\), \(\boldsymbol{\mathsf{B}}\) given by \(\tilde{S}_{1,2}=(S_{1}\cup S_{2})\setminus(S_{4}\cup S_{3})\), and those only shared among \(\boldsymbol{\mathsf{B}}\), \(\boldsymbol{\mathsf{C}}\) given by \(\tilde{S}_{2,3}=(S_{2}\cup S_{3})\setminus(S_{4}\cup S_{1})\). This yields the intermediate indices \(S_{1,2}=(S_{1}\cup S_{2})\setminus\tilde{S}_{1,2}\) and \(S_{2,3}=(S_{2}\cup S_{3})\setminus\tilde{S}_{2,3}\), and the parenthesizations

\[\begin{split}&[\boldsymbol{\mathsf{D}}]_{S_{4}}=\left(\sum_{ \tilde{S}_{\tilde{S}_{1,2}}}\left(\sum_{\tilde{S}_{1,2}}[\boldsymbol{\mathsf{ A}}]_{S_{1}}[\boldsymbol{\mathsf{B}}]_{S_{2}}\right)[\boldsymbol{\mathsf{C}}]_{S_{3}} \right)=\left(\sum_{\tilde{S}_{\tilde{S}}\setminus\tilde{S}_{2,3}}[\boldsymbol {\mathsf{A}}]_{S_{1}}\left(\sum_{\tilde{S}_{2,3}}[\boldsymbol{\mathsf{B}}]_{S_ {2}}[\boldsymbol{\mathsf{C}}]_{S_{3}}\right)\right)\\ &\Leftrightarrow\ \boldsymbol{\mathsf{D}}=*_{(S_{1,2},S_{3},S_{4})} \left(*_{(S_{2},S_{3},S_{2,3})}(\boldsymbol{\mathsf{A}},\boldsymbol{\mathsf{ B}}),\boldsymbol{\mathsf{C}}\right)=*_{(S_{1},S_{2},S_{3},S_{4})}\left( \boldsymbol{\mathsf{A}},*_{(S_{1},S_{2},S_{1,2})}(\boldsymbol{\mathsf{B}}, \boldsymbol{\mathsf{C}})\right)\,.\end{split}\] (120)

This generalizes to \(n\)-ary multiplication, allowing to break it down into smaller multiplications. However, the index notation and set arithmetic from Equation (120) quickly becomes impractical.

### Example: Matrix-matrix Multiplication as Tensor Multiplication

Here we provide a small self-contained example that demonstrates Equation (3) for matrix-matrix multiplication.

Consider two matrices \(\boldsymbol{A},\boldsymbol{B}\) which are compatible for multiplication and let \(\boldsymbol{C}=\boldsymbol{A}\boldsymbol{B}\). In index notation, we have

\[[\boldsymbol{C}]_{i,k}=\sum_{j}[\boldsymbol{A}]_{i,j}[\boldsymbol{B}]_{j,k}\,.\]

The index tuples are \(S_{\boldsymbol{A}}=(i,j)\), \(S_{\boldsymbol{B}}=(j,k)\), and \(S_{\boldsymbol{C}}=(i,k)\). Next, we evaluate which indices are summed over. Since the order of those indices does not matter, we can interpret the tuples as sets and use set arithmetic:

\[(S_{\boldsymbol{A}}\cup S_{\boldsymbol{B}})\setminus S_{\boldsymbol{C}}=((i,j )\cup(j,k))\setminus(i,k)=(j)\setminus(i,k)=(j)\,.\]

Now we see that matrix-matrix multiplication is a case of tensor multiplication (Equation (3)),

\[[\boldsymbol{C}]_{S_{\boldsymbol{C}}}=\sum_{(S_{\boldsymbol{A}}\cup\tilde{S} _{\boldsymbol{B}})\setminus S_{\boldsymbol{C}}}[\boldsymbol{A}]_{S_{\boldsymbol {A}}}[\boldsymbol{B}]_{S_{\boldsymbol{B}}}=*_{(S_{\boldsymbol{A}},S_{ \boldsymbol{B}},S_{\boldsymbol{C}})}(\boldsymbol{A},\boldsymbol{B})\,.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide a bullet point list of our contributions in SS1, each of which references the part of the paper that outlines the contribution. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See SS4.2 and SSA. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The TN simplifications we provide in SS4.1 follow straightforward from the index pattern's structure Equation (7) and are stated rigorously in SSD.3, including their assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide implementation details in SSD, experimental and hardware details in SSF and G. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will open-source the code to reproduce all our experiments, as well as the raw data containing the results shown in the manuscript. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See \(\lx@sectionsign\)F and G for details on the experimental setting. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our run time plots contain box plots with medians and quartiles reported over different convolutions, and the randomized backpropagation results show mean and standard deviations for 10 different model and batch initializations. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All results were obtained on a single GPU to be comparable in terms of run time. See SSF for the details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the Code of Ethics and believe that our work conforms to it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This work aims to provide a simplifying perspective and implementations of otherwise hard-to-access operations for convolutions to facilitate the exploration of algorithmic ideas and advance existing second-order methods. We don't see any direct negative societal impacts.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release any data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the papers introducing the neural network architectures and data sets used in our experiments. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.