# Locally Private and Robust Multi-Armed Bandits

 Xingyu Zhou

Wayne State University

xingyu.zhou@wayne.edu

Work done during research intern at Wayne State University

Wei Zhang

Texas A&M University

komo@tamu.edu

###### Abstract

We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data. To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings. Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively. Our theoretical results in both settings are also corroborated by a set of systematic simulations. One key message in this paper is that LTC is a more difficult setting that leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). Our sharp understanding of LTC and CTL also naturally allows us to give the first tight performance bounds for the most practical setting where corruption could happen both before and after the LDP mechanism. As an important by-product, we also give the first correct and tight regret bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption, by identifying a fundamental flaw in the state-of-the-art.

## 1 Introduction

The Multi-Armed Bandit (MAB) problem (Berry and Fristedt, 1985) offers a fundamental approach for sequential decision-making under uncertainty based on only bandit feedback. Take online advertising as an illustrative example, where the advertising platform (i.e., the central learner) sequentially and adaptively displays ads (i.e., arm) based on users' reward feedback (e.g., engagement score) so as to maximize the cumulative rewards. In practice, several important factors have to be considered when designing real-world MAB algorithms, as illustrated below using online advertising.

**Privacy.** The raw engagement score (which is calculated based on clicks, purchases, and time spent viewing the ad, etc.) from a user's device may lead to privacy leakage. For instance, when the ad is about medicine on some rare or uncommon disease, a high engagement score might imply interest or association with the uncommon disease. Such privacy leakage may lead to unintended personal and social consequences as well as trust issues on the platform. One principled way to mitigate it is via local differential privacy (LDP) (Kasiviswanathan et al., 2011; Duchi et al., 2018), i.e., each user's device locally adds a suitable amount of noise (depending on the privacy mechanism and budget) to obfuscate the raw feedback before sending it out from the device (see the yellow region in Fig. 1).

**Robustness.** Another important factor in real-world scenarios is the robustness of MAB algorithms under both possibly heavy-tailed feedback and adversary corruption.

_Heavy-tailed feedback._ The engagement score in our example could often be heavy-tailed, i.e., non-negligible probabilities of observing extremely high values. This might happen due to some special events and seasons (e.g., Black Friday) or influencer interaction.

_Adversary corruption._ There could be malicious attacks on the engagement scores during the collection of users' feedback, e.g., with some probability, each score could be replaced by any _arbitrary_ value, i.e., Huber corruption (Huber, 1964). On the other hand, corruption can also happen on each user's side before transmission, e.g., one could manipulate or spoof interactions to skew scores. Most practically, corruption can also happen both before and after the data transmission.

To tackle the above privacy and robustness issues in MABs, there has been a large related literature, which, however, mainly investigates the two issues in an isolated way (see Appendix B for details). Motivated by this, in this work, we are particularly interested in the following question:

_Is there any interesting interplay between privacy and robustness in MABs?_

**Our contributions.** We give an affirmative answer to the above question by unveiling a fundamental interplay between privacy protection (in particular, local differential privacy (LDP)) and robustness under Huber corruption and heavy-tailedness. Our main message is a separation result between two MAB settings that differ in the order of privacy protection and corruption, i.e., LDP-then-corruption (LTC) vs. Corruption-then-LDP (CTL). That is, under LTC, corruption happens after LDP mechanism while under CTL, corruption happens before the LDP mechanism (see Fig. 1). To obtain our separation result for the two settings, we take the following principled approach:

**1.** We first study the mean estimation problem - a cornerstone step in the analysis of stochastic MABs - under both LTC and CTL settings. We give the first tight characterization of the estimation error in high probability, in terms of privacy budget, corruption level, and heavy-tailedness. Specifically, we first establish lower bounds on the minimax error rate in high probability and then propose a unified optimal algorithm that achieves matching worst-case upper bounds for both settings. The key observation here is that the mean estimation error under LTC is larger than that under CTL and moreover the gap becomes larger as the privacy requirement becomes stronger. Further, our sharp results on LTC and CTL also naturally enable us to give tight performance bounds for the most practical setting, C-LDP-C, where corruption happens both before and after LDP, see (3) in Fig. 1.

**2.** Leveraging the above tight mean estimation results, we then study both online MABs and offline MABs under both LTC and CTL. We present an almost tight characterization (up to log factor) of the corresponding minimax performances (i.e., regret in online MABs and sub-optimality in offline MABs) by deriving lower bounds and proposing almost optimal algorithms. As in mean estimation, there is a separation between LTC and CTL, i.e., LTC is a more difficult setting that leads to worse performance in the minimax sense, highlighting the interesting interplay between privacy and robustness in MABs. All of these results also allow us to easily handle the C-LDP-C setting.

**3.** Along the way, several results could be of independent interest. First, our optimal locally private and robust mean estimators can be applied to many other applications beyond MABs. Moreover, as an important by-product, we identify a fundamental flaw in the regret upper bound of state-of-the-art locally private online MABs with heavy tails (i.e., without corruption), and give the first correct one.

**Related Work.** We discuss the most relevant related work in the main body and relegate a detailed discussion to Appendix B. LDP with bounded/sub-Gaussian reward is first introduced to MABs in Ren et al. (2020) and later it was generalized to the heavy-tailed rewards (Tao et al., 2022). Robust MABs under Huber corruption have been recently studied in Kapoor et al. (2019); Mukherjee et al. (2021); Basu et al. (2022); Agrawal et al. (2023) while robust MABs concerning heavy-tailed reward date back to Bubeck et al. (2013). However, these work only study privacy and robustness separately. To the best of our knowledge, there are only two very recent work that consider privacy and robustness in MABs simultaneously. In Wu et al. (2023), the authors consider the central DP model where the raw non-private feedback received by the central learner can be first corrupted under Huber model. This is in sharp contrast to our local DP model, which is not only stronger but allows us to study

Figure 1: The interplay between privacy and robustness (heavy-tailed data and corruption).

the order of corruption and privacy. In Charisopoulos et al. (2023), the authors study linear bandits (which includes MAB as a special case) under LDP and then Huber corruption (i.e., LTC setting). As will be discussed in Section 4, their regret bound is sub-optimal and worse than ours when reduced to the MAB case. Note that we also study the CTL setting, which in turn allows us to study the most practical setting C-LDP-C. Finally, our work is inspired by recent advances in (locally) private and robust mean estimation (Li et al., 2022; Cheu et al., 2021; Chhor and Sentenac, 2023). Our key contributions are the first _high-probability_ concentration bounds for both CTL and LTC settings.

## 2 Problem Setup

In this section, we formally introduce the three problems considered in this paper: mean estimation, online and offline MABs, under the constraints of both LDP and robustness (including heavy tails and Huber corruption). To start with, we introduce the privacy and corruption models.

**Definition 1** (\(\varepsilon\)-LDP, Duchi et al. (2018)).: For a privacy parameter \(\varepsilon\in[0,1]\), the random variable \(\widetilde{X}\) is an \(\varepsilon\)_-locally differentially private_ view of \(X\) via privacy channel/mechanism \(Q\) if

\[\sup_{S\in\sigma(\widetilde{\mathcal{X}}),x,x^{\prime}\in\mathcal{X}}\frac{Q( \widetilde{X}\in S\mid X=x)}{Q(\widetilde{X}\in S\mid X=x^{\prime})}\leq e^{ \varepsilon},\]

where \(\sigma(\widetilde{\mathcal{X}})\) denotes an appropriate \(\sigma\)-field on \(\widetilde{\mathcal{X}}\). In this case, we also say that the conditional distribution (privacy channel) \(Q\) is an \(\varepsilon\)-LDP privacy mechanism. We write \(\mathcal{Q}_{\varepsilon}\) as the set of all \(\varepsilon\)-LDP mechanisms (channels).

**Definition 2** (\(\alpha\)-Huber corruption, Huber (1964)).: Given a parameter \(\alpha\in[0,1/2)\) and a distribution \(D\) on inliers, the output distribution under \(\alpha\)-Huber model is \(O=(1-\alpha)D+\alpha E\). That is, a sample from \(O\) returns a sample from \(D\) with probability \(1-\alpha\) and otherwise returns a sample from some (unconstrained and unknown) corruption distribution \(E\). We write \(\mathcal{C}_{\alpha}(D)\) as the set of all possible \(\alpha\)-Huber corruptions (channels) of inlier distribution \(D\).

With the two definitions in hand, we can introduce the two main settings in this paper: (i) LDP-then-Corruption (LTC) vs. (ii) Corruption-then-LDP (CTL), as also illustrated in Fig. 1.

**Definition 3** (LTC vs. CTL).: We consider the following interplay between privacy and corruption.

(i) **LDP-then-Corruption (LTC):** Each user \(i\in[n]\) first generates an \(\varepsilon\)-LDP view of raw data \(X_{i}\). Then, the private data \(Y_{i}\) from each device is independently corrupted by an \(\alpha\)-Huber channel that outputs \(Z_{i}\) to the central analyzer/agent.

(ii) **Corruption-then-LDP (CTL):** Each user's raw data \(X_{i}\) is first independently corrupted by an \(\alpha\)-Huber model. Then, the corrupted data \(Y_{i}\) passes through an \(\varepsilon\)-LDP mechanism at each device that outputs \(Z_{i}\) to the central analyzer/agent.

Under both settings, we aim to design \(\varepsilon\)-LDP mechanisms for user devices and central analyzers that ensure local privacy and robustness against \(\alpha\)-Huber corruption and heavy-tailed data distributions. The two settings also naturally enable us to study the most practical setting C-LDP-C.

**Mean estimation.** As in Duchi et al. (2018), given a real number \(k>1\), we consider the following class of possibly heavy-tailed distributions

\[\mathcal{P}_{k}:=\{\text{distributions }P\text{ such that }\mathbb{E}_{X\sim P}[X]\in[-1,1]\text{ and }\mathbb{E}_{X\sim P}[|X|^{k}]\leq 1\}. \tag{1}\]

That is, \(k\) controls the tail behavior of the distribution with smaller \(k\) meaning heavier of the tails. Given any distribution \(P\in\mathcal{P}_{k}\), our goal is to estimate its mean \(\mu(P)\) as accurately as possible. In contrast to the standard case where the analyzer has access to \(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P\), the analyzer in this paper now only observes samples \(\{Z_{i}\}_{i=1}^{n}\) that are both private and corrupted view of \(\{X_{i}\}_{i=1}^{n}\). Specifically, we are interested in the _high probability_ error under our two different settings (LTC vs. CTL), as formally defined below.

**Definition 4** (Minimax mean estimation error rate).: Given \(\delta>0\) and sample size \(n>0\), the minimax mean estimation error rate of the class \(\mathcal{P}_{k}\) under \(\varepsilon\)-LDP and \(\alpha\)-Huber corruption is defined as follows

\[\phi_{\delta}^{*}(k,\varepsilon,\alpha,n)\!:=\!\inf\{\phi>0\mid\inf_{Q\in \mathcal{Q}_{\varepsilon}}\inf_{\widetilde{\mu}_{n}}\sup_{P\in\mathcal{P}_{k} }\sup_{C\in\mathcal{C}_{\alpha}(P)}\mathbb{P}\left[|\widehat{\mu}_{n}-\mu(P)|> \phi\right]\leq\delta\}, \tag{2}\]where \(\widehat{\mu}_{n}\) is a measurable function of \(\{Z_{i}\}_{i=1}^{n}\), i.e., private and corrupted view of \(n\)\(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P\in\mathcal{P}_{k}\) that pass through \(\varepsilon\)-LDP channel \(Q\) and \(\alpha\)-Huber corruption channel \(C\). We write \(\phi_{\delta,\text{\rm{ITC}}}^{*}(k,\varepsilon,\alpha,n)\) and \(\phi_{\delta,\text{\rm{CTL}}}^{*}(k,\varepsilon,\alpha,n)\) for the settings of LTC and CTL.

Intuitively speaking, \(\phi_{\delta}^{*}\) represents the minimal error rate that any \(\varepsilon\)-LDP estimator can achieve with high probability \(1-\delta\) for all distributions \(P\in\mathcal{P}_{k}\) and all \(\alpha\)-Huber corruption models, hence taking inf over \(Q\) and \(\widehat{\mu}_{n}\) and sup over distribution and corruption. Thus, the goal in our mean estimation problem is to design an optimal \(\varepsilon\)-LDP mechanism \(Q^{\star}\) at each user's side and an optimal analyzer \(\widehat{\mu}_{n}^{*}\) at the central analyzer in order to attain the minimax mean estimate error rate in (2).

**Online MABs.** At each round \(t\in[T]\), the central learner/analyzer chooses an action/arm \(a_{t}\in[K]\) according to a policy \(\pi\) and receives a reward sample \(X_{t}\) that is drawn from some distribution \(P_{a_{t}}\) with unknown mean \(r(a_{t}):=\mu(P_{a_{t}})\). Here, the policy is \(\pi=\{\pi_{t}\}_{t=1}^{T}\) and \(\pi_{t+1}\) is a measurable function of the data received by the end of round \(t\), i.e., for each \(t\in[T]\), \(\mathcal{D}_{t}=\{(a,X^{(a)}(t))\}_{a\in[K]}\) where \(X^{(a)}(t):=\{X^{(a)}_{1},\ldots,X^{(a)}_{N_{a}(t)}\}\) and \(\sum_{a\in K}N_{a}(t)=t\). That is, for each round \(t\), \(X^{(a)}(t)\) groups together all \(N_{a}(t)\) rewards from each arm \(a\in[K]\) where \(N_{a}(t)\) is the total number of times that arm \(a\) has been pulled by time \(t\). The goal in online MABs is to characterize the minimax _clean_ regret under our LTC and CTL settings defined below.

**Definition 5** (Minimax clean regret).: Let \(\text{MAB}(k):=\{\{P_{a}\}_{a\in K}\mid P_{a}\in\mathcal{P}_{k}\}\) be the class of \(K\)-armed MAB instances with inlier distributions for each arm in \(\mathcal{P}_{k}\). Then, the minimax clean regret is defined as

\[R^{*}(k,\varepsilon,\alpha,T):=\inf_{Q\in\mathcal{Q}_{\star}}\inf_{\pi}\sup_{I \in\text{MAB}(k)}\sup_{C\in\mathcal{C}_{\alpha}(I)}\mathbb{E}\left[T\cdot r(a ^{\star})-\sum_{t=1}^{T}r(a_{t})\right], \tag{3}\]

where \(a_{t+1}\) is a measurable function (via \(\pi\)) of private and corrupted dataset \(\{(a,Z^{(a)}(t))\}_{a\in[K]}\). Here, for any arm \(a\in[K]\) and \(t\in[T]\), \(Z^{(a)}(t):=\{Z^{(a)}_{1},\ldots,Z^{(a)}_{N_{a}(t)}\}\) is the private and corrupted view of \(N_{a}(t)\) samples of \(P_{a}\) that pass through \(\varepsilon\)-LDP channel \(Q\) and \(\alpha\)-Huber corruption channel \(C\). We write \(R^{*}_{\text{\rm{LTC}}}(k,\varepsilon,\alpha,T)\) and \(R^{*}_{\text{\rm{CTL}}}(k,\varepsilon,\alpha,T)\) for the settings of LTC and CTL, respectively.

The goal in online MABs is to design an optimal \(\varepsilon\)-LDP mechanism \(Q^{\star}\) and optimal learning policy \(\pi^{\star}\) so as to attain the minimax clean regret in (3).

_Remark 1_.: As standard in the literature (Wu et al., 2023; Chen et al., 2022; Niss and Tewari, 2020), \(r(\cdot)\) in (3) is the mean of inlier distributions while the randomness in the expectation is generated by both privacy and corruption.

**Offline MABs.** In the offline case, the analyzer cannot interact with users and instead, it is given a batch pre-collected dataset \(\mathcal{D}=\{(a_{i},X_{i})\}_{i=1}^{N}\) sampled from some joint distribution of a behavior policy \(\pi\) and reward distributions \(\{P_{a}\}_{a\in[K]}\). As in Rashidinejad et al. (2021), we assume a finite concentrability coefficient \(\beta^{\star}\) such that \(1/\pi(a^{\star})\leq\beta^{\star}\), where \(a^{\star}\) is the optimal arm that has the largest mean and \(\beta^{\star}\) captures deviation between the behavior distribution \(\pi\) and the distribution induced by the _optimal_ policy. The goal here is to characterize the minimax sub-optimality under our LTC and CTL settings defined below.

**Definition 6** (Minimax sub-optimality).: Let

\[\text{MAB}(\beta^{\star},k):=\{(\pi,\{P_{a}\}_{a\in K})\,|\,P_{a}\in \mathcal{P}_{k}\text{ and }1/\pi(a^{\star})\leq\beta^{\star}\}\]

be the class of \(K\)-armed MAB instances with distributions in \(\mathcal{P}_{k}\) and concentrability coefficient \(\beta^{\star}\). Then, the minimax sub-optimality is defined as

\[\mathrm{SubOpt}^{*}(\beta^{\star},k,\varepsilon,\alpha,N):=\inf_{Q\in \mathcal{Q}_{\star}}\inf_{\widehat{a}}\sup_{I\in\text{MAB}(\beta^{\star},k)} \sup_{C\in\mathcal{C}_{\alpha}(I)}\mathbb{E}\left[|r(a^{\star})-r(\widehat{a} )|\right], \tag{4}\]

where \(\widehat{a}\) is a measurable function of private and corrupted dataset \(\{(a,Z^{(a)})\}_{a\in[K]}\) and \(Z^{(a)}:=\{Z^{(a)}_{1},\ldots,Z^{(a)}_{N_{a}}\}\) is the private and corrupted view of \(N_{a}\) samples of \(P_{a}\) that pass through \(\varepsilon\)-LDP channel \(Q\) and \(\alpha\)-Huber corruption channel \(C\). We write \(\mathrm{SubOpt}^{*}_{\text{\rm{LTC}}}(\beta^{\star},k,\varepsilon,\alpha,N)\) and \(\mathrm{SubOpt}^{*}_{\text{\rm{CTL}}}(\beta^{\star},k,\varepsilon,\alpha,N)\) for LTC and CTL, respectively.

We remark that we assume the batch data is collected by an \(\varepsilon\)-LDP mechanism that can be specified by the learner. Note that as in the standard case, we do not control the behavior policy \(\pi\) other than a finite \(\beta^{\star}\). The goal here is to design an optimal \(\varepsilon\)-LDP mechanism \(Q^{\star}\) (which protects local privacy for any users offering batch data) and optimal offline learning algorithm \(\widehat{a}^{\star}\).

Mean Estimation

We start with our first problem - mean estimation under privacy and robustness constraints. Our main result in this section is the following theorem that characterizes the minimax error rate (cf. Def. 4)

**Theorem 1** (Mean Estimation).: _Given any fixed \(\delta\in(0,1/2)\)2, \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\) and \(k>1\), we have that for all large enough \(n\),_

Footnote 2: We assume \(\delta\) does not depends on \(n\); otherwise, \(\delta\in(\delta_{\min},1/2)\) where \(\delta_{\min}=e^{-cn}\) for some \(c>0\).

\[\phi^{*}_{\delta,\mathrm{LTC}}(k,\varepsilon,\alpha,n) =\Theta\left(\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}+\left( \frac{1}{\varepsilon}\sqrt{\frac{\log(1/\delta)}{n}}\right)^{1-1/k}\right),\] \[\phi^{*}_{\delta,\mathrm{CTL}}(k,\varepsilon,\alpha,n) =\Theta\left(\alpha^{1-1/k}+\left(\frac{1}{\varepsilon}\sqrt{ \frac{\log(1/\delta)}{n}}\right)^{1-1/k}\right).\]

_Remark 2_.: To the best of our knowledge, this is the first high-probability concentration bound for mean estimation under both LTC and CTL, which tightly captures the dependence on the corruption level \(\alpha\), privacy budget \(\varepsilon\) and heavy-tail parameter \(k\), simultaneously. It can be seen that for LTC setting, there is an additional \((1/\varepsilon)^{1-1/k}\) factor, which implies that introducing LDP guarantee first would make it more vulnerable to corruption/data manipulation attacks. Interestingly, for a fixed \(\varepsilon\), this additional vulnerability due to LDP decreases as the tail becomes heavier, which offers additional insight into the interplay of privacy, heavy-tailedness, and robustness. Our LTC result also complements the result in Cheu et al. (2021), which considers the bounded case (i.e., \(k=\infty\)) under constant probability only rather than our high probability guarantee. On the other hand, for CTL, we note that the impact of corruption and privacy is _separable_. Our high probability bound for CTL complements the error bound in terms of mean-square error (MSE) only in Li et al. (2022).

To establish Theorem 1, we first establish the following lower bounds, with full proof in Appendix E.

**Proposition 1** (Lower Bounds).: _Given any fixed \(\delta\in(0,1/2)\), \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\), \(k>1\) and \(n\) large enough, for all \(\varepsilon\)-LDP mechanism \(Q\) and all estimator \(\widehat{\mu}_{n}\), there exists a distribution \(P\in\mathcal{P}_{k}\) and \(\alpha\)-Huber corruption channel \(C\in\mathcal{C}_{\alpha}(P)\) such that with probability at least \(\delta\)_

_(i) For LTC:_ \(|\widehat{\mu}_{n}-\mu(P)|\geq\Omega\left(\left(\frac{\alpha}{\varepsilon} \right)^{1-1/k}+(\frac{1}{\varepsilon}\sqrt{\frac{\log(1/\delta)}{n}})^{1-1/k }\right)\)_,_

_(ii) For CTL:_ \(|\widehat{\mu}_{n}-\mu(P)|\geq\Omega\left(\alpha^{1-1/k}+(\frac{1}{\varepsilon }\sqrt{\frac{\log(1/\delta)}{n}})^{1-1/k}\right)\)_,_

_where recall that \(\widehat{\mu}_{n}\) is a measurable function of \(\{Z_{i}\}_{i=1}^{n}\), i.e., private and corrupted view of \(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P\in\mathcal{P}_{k}\) obtained from \(\varepsilon\)-LDP channel \(Q\) and \(\alpha\)-Huber corruption channel \(C\)._

Proof sketch.: We provide a summary of the key steps in the proof. Essentially, we divide the proof into two parts. First, we consider the case without corruption and aim to establish the second term in the bound. To this end, we will leverage tools from information theory in an novel way, e.g., maximal coupling, strong data processing inequality of LDP, and Bretagnolle-Huber inequality between TV and KL distance. Then, we turn to give the first term related to corruption. To this end, we will leverage a folklore but important fact about Huber model. Roughly speaking, this fact says that given two inlier distributions \(D_{1}\) and \(D_{2}\) that satisfy \(\mathrm{TV}\left(D_{1},D_{2}\right)\leq O(\alpha)\), then after \(\alpha\)-Huber channel, one cannot distinguish between \(D_{1}\) and \(D_{2}\). Another important fact is that \(\varepsilon\)-LDP channel is a "contraction" channel in terms of TV distance, i.e., \(\mathrm{TV}\left(M_{1},M_{2}\right)\leq O(\varepsilon)\mathrm{TV}\left(P_{1},P_{2}\right)\) where \(M_{1}\), \(M_{2}\) are induced marginals of \(P_{1},P_{2}\) after any \(\varepsilon\)-LDP channel. 

**Key intuition behind the separation between LTC and CTL.** Building upon the above proof, one can immediately see that under the LTC setting, due to the "contraction" of LDP, one can choose two distributions that have a larger mean difference by a factor of \(1/\varepsilon\), while still guaranteeing that after \(\alpha\)-Huber corruption, they are indistinguishable, _hence explaining the key difference of \(1/\varepsilon\) between LTC and CTL._ We also provide another understanding of the separation from the attack perspective (see more details in Appendix A). The key idea here is that each single data attack in the LTC setting will lead to an additional \(1/\varepsilon\) factor compared to CTL setting. This is mainly because any \(\varepsilon\)-LDP mechanism on binary data can be simulated by random response mechanism (Kairouz et al., 2015).

We now turn to upper bounds, centering around the following key question: _Can we design a simple algorithm that can achieve optimal errors for all LTC, CTL, and even C-LDP-C in a unified way?_ We give an affirmative answer via Algorithm 1. It consists of a local randomizer at each user's side and an analyzer at the central side. The task of \(Q\) is to guarantee that its output is an \(\varepsilon\)-LDP view of its input. To this end, for each input \(U_{i}\), it first truncates it into \(\bar{U}_{i}\) using a properly chosen threshold \(M\). Then, it converts the real number to binary data via random rounding. Next, it applies random response technique to generate the final output \(\widetilde{U}_{i}\), i.e., with probability \(\frac{e^{\varepsilon}}{e^{\varepsilon}+1}\), outputs a number of the same sign (with additional scaling for unbiasedness); otherwise flips the sign. Upon receiving the final input \(\{Z_{i}\}_{i=1}^{n}\), the analyzer \(\mathcal{A}\) first simply filters out the data if it is out of the bounded range and then returns the sample mean.

For LTC and CTL, the only difference in Algorithm 1 would be the truncation value \(M\). The performance bounds for both settings under Algorithm 1 are given below. See Appendix F for proof.

**Proposition 2** (Upper Bounds).: _Given any fixed \(\delta\in(0,1)\), \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\) and \(k>1\), for any distribution \(P\in\mathcal{P}_{k}\) and any \(\alpha\)-Huber channel \(C\in\mathcal{C}_{\alpha}\), Algorithm 1 satisfies that the mechanism \(Q\) is \(\varepsilon\)-LDP and each returned estimate \(\widehat{\mu}_{n}\) guarantees that with probability at least \(1-\delta\)_

_(i) For LTC:_ \(|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\alpha}{\varepsilon}\right) ^{1-1/k}+\left(\frac{1}{\varepsilon}\sqrt{\frac{\log(1/\delta)}{n}}\right)^{1- 1/k}\right)\)_,_

_(ii) For CTL:_ \(|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\alpha^{1-1/k}+\left(\frac{1}{\varepsilon }\sqrt{\frac{\log(1/\delta)}{n}}\right)^{1-1/k}\right)\)_,_

_where (i) holds for \(M\!=\!\min\left\{\left(\frac{\varepsilon}{\alpha}\right)^{1/k},\left(\frac{ \varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\right\}\) and all \(n\geq 3\log(1/\delta)/\alpha\), if \(\alpha>0\); otherwise for all \(n\) and \(M=\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\).(ii) holds for \(M=\min\left\{\left(\frac{1}{\alpha}\right)^{1/k},\left(\frac{\varepsilon\sqrt {n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\right\}\) and \(n\geq 3\log(1/\delta)/\alpha\), if \(\alpha>0\); otherwise for \(n\geq\log(1/\delta)\) and \(M=\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\)._

**Corruption-LDP-Corruption (C-LDP-C).** Our tight characterization of LTC and CTL immediately helps us understand the C-LDP-C setting, where corruption happens both before and after LDP. In particular, it is easy to see that the minimax lower bound for LTC would be a valid lower bound for the more difficult C-LDP-C setting. It turns out that this lower bound is also tight since it is matched by Algorithm 1 with the same parameter choice \(M\) as in the LTC setting, see Appendix G.

```
1:Procedure:\(\varepsilon\)-LDP mechanism \(Q\)
2://Input:\(U_{i}\), parameters:\(M\), \(\varepsilon\)
3://Output: private view \(\widetilde{U}_{i}\)
4: Truncate: \(\widetilde{U}_{i}=U_{i}\mathds{1}(|U_{i}|\leq M)\)
5: Random rounding: \(U^{\prime}_{i}=\begin{cases}M&w.p.\frac{1+\widetilde{U}_{i}/M}{2}\\ -M&w.p.\frac{1-\widetilde{U}_{i}/M}{2}\end{cases}\)
6: Random response: \(\widetilde{U}_{i}=\begin{cases}\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1}U^{ \prime}_{i}&w.p.\frac{e^{\varepsilon}}{e^{\varepsilon}+1}\\ -\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1}U^{\prime}_{i}&w.p.\frac{1}{e^{ \varepsilon}+1}\end{cases}\)
7:Return\(\widetilde{U}_{i}\)
8:Procedure: Analyzer \(\mathcal{A}\)
9://Input:\(\{Z_{i}\}_{i=1}^{n}\), parameters:\(M\), \(\varepsilon\)
10://Output: estimator \(\widehat{\mu}_{n}\)
11:Return\(\widehat{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}Z_{i}\mathds{1}(|Z_{i}|\leq M \cdot\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1})\)
```

**Algorithm 1** A Unified Algorithm

**How to choose parameter \(M\) in practice.** First, we note that for the bounded case (\(k=\infty\)), \(M=1\) across all three settings, independent of other parameters. This implies that Algorithm 1 can adaptively guarantee optimal minimax rates for LTC, CTL, and C-LDP-C without prior knowledge of the specific setting and other parameter like \(\alpha\). Second, for certain applications, one may have prior knowledge of the underlying setting (see Appendix C.3). In this case, one can have a performance gain if it is under the CTL setting. Also, as mentioned above, we see that choosing the \(M\) as in LTC can automatically help to handle the C-LDP-C setting. Finally, the dependence on \(\varepsilon\) in \(M\) is fine since it is a known privacy parameter while the dependence on the unknown parameter \(\alpha\) is a little bit annoying. A quick practical fix is to use an estimated upper bound on \(\alpha\). In theory, the story of whether one can remove it in our case is complicated, see the discussion in Appendix C.2.

_Remark 3_ (Burn-in period).: Under Algorithm 1, when \(\alpha>0\), the concentration kicks in when the sample size \(n\) is larger than a threshold. This type of burn-in period also exists in previous concentration results under the Huber model, though in different contexts (e.g., non-private case in Chen et al. (2022) or central model of DP in Wu et al. (2023)) or with different estimators (e.g., trimmed mean in Mukherjee et al. (2021)).

_Remark 4_ (Random response vs. Laplace mechanism).: One may wonder if the standard Laplace mechanism can be applied in replace of the random response for \(\varepsilon\)-LDP in \(Q\). The answer depends on the setting and the analyzer \(\mathcal{A}\). For CTL, one can still derive a similar optimal concentration bound as in Proposition 2 by the concentration of Laplace noise. On the other hand, for LTC, simply replacing random response with Laplace mechanism in \(Q\) will lead to an additional \(\log(1/\alpha)\) factor. This aligns with the fact that truncation-based estimators even cannot achieve optimal mean estimation for Gaussians under corruption (Diakonikolas and Kane, 2023). The above discussion indicates another difference between LTC and CTL, i.e., the choice of \(\varepsilon\)-LDP mechanisms.

As two interesting applications of our mean estimation results, we will study both online MABs and offline MABs in the next two sections, highlighting again the sharp differences between LTC and CTL settings, in terms of regret and sub-optimality performance, respectively.

## 4 Online MABs

For online MABs, our main result is the following theorem that gives an almost tight characterization (up to log factor) of its minimax clean regret (cf. Def. 5) for both LTC and CTL settings.

**Theorem 2** (Online MABs).: _Given any \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\) and \(k>1\), we have for all large enough \(T\),_

\[R^{*}_{\delta,\mathrm{LTC}}(k,\varepsilon,\alpha,T) =\widetilde{\Theta}\left(T\cdot\left(\frac{\alpha}{\varepsilon} \right)^{1-1/k}+T^{\frac{k+1}{2k}}\left(\frac{K}{\varepsilon^{2}}\right)^{ \frac{k-1}{2k}}\right),\] \[R^{*}_{\delta,\mathrm{CTL}}(k,\varepsilon,\alpha,T) =\widetilde{\Theta}\left(T\cdot\alpha^{1-1/k}+T^{\frac{k+1}{2k}} \left(\frac{K}{\varepsilon^{2}}\right)^{\frac{k-1}{2k}}\right).\]

_Remark 5_.: For both settings, due to corruption, the minimax clean regret (i.e., problem-independent regret) has a linear dependence on \(T\), as in previous works under Huber corruption (Wu et al., 2023; Chen et al., 2022). The key here is to capture the tight factor in front of \(T\), where the additional \(1/\varepsilon\) factor in LTC again demonstrates the sharp difference between the two settings as in the mean estimation problem. As before, one can obtain the same rate for C-LDP-C from the LTC setting.

To prove the above theorem, we start with the corresponding lower bounds (see App. H for proof).

**Proposition 3** (Regret Lower bounds).: _Let \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\), \(k>1\) and \(T\) be large enough. Then, the minimax clean regrets satisfy the following results._

_(i) LTC:_ \(R^{*}_{\mathrm{LTC}}(k,\varepsilon,\alpha,T)\geq\Omega\left(T\cdot\left(\frac{ \alpha}{\varepsilon}\right)^{1-1/k}+T^{\frac{k+1}{2k}}\left(\frac{K}{ \varepsilon^{2}}\right)^{\frac{k-1}{2k}}\right)\)_;_

_(ii) CTL:_ \(R^{*}_{\mathrm{CTL}}(k,\varepsilon,\alpha,T)\geq\Omega\left(T\cdot\alpha^{1- 1/k}+T^{\frac{k+1}{2k}}\left(\frac{K}{\varepsilon^{2}}\right)^{\frac{k-1}{2k} }\right)\)_._

**Comparisons to related work.** We first remark that Tao et al. (2022) studied a similar case but without corruption (i.e., \(\alpha=0\)) and established a lower bound on the order of \(\Omega\left(\left(\frac{K}{\varepsilon^{2}}\right)^{1-1/k}T^{1/k}\right)\) (for \(k\in(1,2]\) when adapted to our setting), which is weaker concerning \(T\) compared to our lower bound. In Tao et al. (2022), the authors also claimed to achieve their lower bound via some arm-elimination algorithm, which now becomes _ungrounded_ given our tighter lower bound. That is, since for a large enough \(T\), our lower bound is even larger than their upper bound for fixed \(\varepsilon\), \(k\) and \(K\) (e.g., \(T^{3/4}\) vs. \(\sqrt{T}\) for \(k=2\), see further discussion in Appendix C.3). Another recent work Wu et al. (2023) also studies online MABs with both privacy and Huber corruption but under the _weaker_ central model of DP. In particular, the true reward from each user may be first corrupted before being observed by the central learner, who is then responsible for taking care of privacy guarantees. That is, the central learner has access to users' raw (corrupted) data rather than only a private view of data as in our LDP case. Under this strictly weaker privacy model, Wu et al. (2023) establish the following lower bound on the minimax clean regret: \(\Omega\left(\sqrt{KT}+(K/\varepsilon)^{1-\frac{1}{k}}\,T^{\frac{1}{k}}+T\alpha^{1- \frac{1}{k}}\right)\). Compared to our CTL setting, one can see that our stronger LDP privacy incurs a larger privacy cost.

Now, let us turn to our proposed algorithm (i.e., Algorithm 2) for achieving matching regret upper bounds (up to log factor). Algorithm 2 is a variant of upper confidence bound (UCB)-based algorithm (cf. Auer et al. (2002)), which computes the UCB index for each arm at each round \(t\in[T]\) and then selects the one with the highest UCB, i.e., optimism in the face of uncertainty. To construct a valid UCB, we resort to our mean estimation results in the last section. In particular, we will need Algorithm 1 to compute the private and robust sample mean \(\widehat{\mu}_{a,N_{a}(t)}(t)\) for each arm \(a\in[K]\) at each round \(t\), where \(N_{a}(t)\) be the number of pulls of arm \(i\) by the beginning of time \(t\). Then, the bonus term (i.e., radius of the confidence bound) \(\beta_{a}(t)\) comes from the high probability mean estimation error established in Proposition 2. Note that due to burn-in period of the concentration results, Algorithm 2 has an additional exploration period to guarantee that the number of arm pulls is larger than a threshold (line 4). The following proposition formally states the regret guarantees of Algorithm 2 with the proof given in Appendix I.

**Proposition 4** (Regret Upper Bounds).: _Let \(\varepsilon\in[0,1]\), \(\alpha\in(0,1/2)\), \(k>1\) and \(T\) be large enough. Then, for any \(1/2>\bar{\alpha}\geq\alpha\), the expected clean regret of Algorithm 2 satisfies the following guarantees._

_(i) LTC:_ \(R_{\mathrm{LTC}}(k,\varepsilon,\alpha,T)\leq O\left(T\left(\frac{\bar{\alpha}} {\varepsilon}\right)^{1-1/k}+\left(\frac{K\log T}{\varepsilon^{2}}\right)^{ \frac{k-1}{2k}}T^{\frac{k+1}{2k}}+\frac{K\log T}{\bar{\alpha}}\right)\)_;_

_(ii) CTL:_ \(R_{\mathrm{CTL}}(k,\varepsilon,\alpha,T)\leq O\left(T\bar{\alpha}^{1-1/k}+ \left(\frac{K\log T}{\varepsilon^{2}}\right)^{\frac{k-1}{2k}}T^{\frac{k+1}{2k} }+\frac{K\log T}{\bar{\alpha}}\right)\)_._

**Comparisons to related work.** First, for \(\alpha=0\), our result with a direct modification of the burn-in period gives a regret bound that only has the term \(O\left(\left(\frac{K\log T}{\varepsilon^{2}}\right)^{\frac{k-1}{2k}}T^{\frac{k +1}{2k}}\right)\). This is the first _correct_ regret bound for locally private heavy-tailed MABs, i.e., without corruption, fixing the aforementioned issue in the state-of-the-art in Tao et al. (2022) (see more discussions in Appendix C.3). Second, it is worth comparing our result to a recent similar result in Charisopoulos et al. (2023), where the authors present regret for linear bandits under LTC setting. Their result is worse than ours when reduced to MAB with bounded rewards, as the scaling with respect to \(\alpha\) is \(\sqrt{\alpha}\) in the first linear term rather than our \(\alpha\). Another minor difference is that our algorithm is anytime while their algorithm is not.

**Other extensions.** Although we mainly focus on minimax regret (i.e., problem-independent bound) in this paper, under some conditions of corruption level and the minimum mean gap, Algorithm 2 is also able to offer some problem-dependent bounds (see Appendix I). In the case that the corruption parameter \(\alpha\) is very small but not equal to zero, one can tune the choice of \(\bar{\alpha}\) (hence truncation threshold \(M\)) to balance the first and third terms in the bound. Similar comments and observations have been made in related work as in Chen et al. (2022); Wu et al. (2023).

## 5 Offline MABs

In this section, we study offline MABs as another application of our high probability mean estimation results developed in Section 3. We establish both lower bounds and almost matching upper bounds for locally private offline MABs with corruptions. To the best of our knowledge, this is the first result on offline MABs with heavy-tailed rewards, even without privacy and corruption.

**Proposition 5** (Sub-optimality Lower Bounds).: _Let \(\varepsilon\in[0,1]\), \(\alpha\in[0,1/2)\), \(k>1\) and \(N\) be large enough. Then, for \(\beta^{\star}\geq 2\), the minimax expected sub-optimality satisfies the following results._

_(i) LTC:_ \(\mathrm{SubOpt}^{*}_{\mathrm{LTC}}(\beta^{\star},k,\varepsilon,\alpha,N)\!\geq\! \Omega\left(\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\!+\!(\frac{1}{ \varepsilon}\sqrt{\frac{\beta^{\star}}{N}}\right)^{1-1/k})\)_;_

_(ii) CTL:_ \(\mathrm{SubOpt}^{*}_{\mathrm{CTL}}(\beta^{\star},k,\varepsilon,\alpha,N)\geq \Omega\left(\alpha^{1-1/k}+(\frac{1}{\varepsilon}\sqrt{\frac{\beta^{\star}}{N}} \right)^{1-1/k})\)_;_

Now, let us turn to our proposed algorithm, which is able to achieve a matching expected sub-optimality (up to log factor) for both LTC and CTL settings. Our algorithm is a simple variant of the classic Lower Confidence Bound (LCB)-based algorithm as in Rashidinejad et al. (2021), i.e., pessimism in the offline setting. The key difference compared to Rashidinejad et al. (2021) is our new private and robust estimator (line 8) and penalty term (line 10), which come from our high probability mean estimation error. Another modification is due to our burn-in period of concentration result (line 4). Putting all of these together, Algorithm 3 is able to achieve the following guarantees on the expected sub-optimality, which almost matches the lower bound in Proposition 5. See the App. K and J for proofs of the upper and lower bounds.

**Proposition 6** (Sub-optimality Upper Bounds).: _Let \(\varepsilon\in[0,1]\), \(\alpha\in(0,1/2)\), \(k>1\) and \(\delta=1/N\). Then, for all finite \(\beta^{\star}\geq 1\) and large enough \(N\) and \(N_{a}\). \(\geq 3\log(1/\delta)/\alpha\), the expected sub-optimality of Algorithm 3 satisfies_

_(i) LTC:_ \(\mathrm{SubOpt}_{\mathrm{LTC}}(\beta^{\star},k,\varepsilon,\alpha,N)\!\leq\!O \left(\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\!+\!\left(\frac{1}{ \varepsilon}\sqrt{\frac{\beta^{\star}\log(KN)}{N}}\right)^{1-1/k}\right)\)_;_

_(ii) CTL:_ \(\mathrm{SubOpt}_{\mathrm{CTL}}(\beta^{\star},k,\varepsilon,\alpha,N)\leq O\left( \alpha^{1-1/k}+\left(\frac{1}{\varepsilon}\sqrt{\frac{\beta^{\star}\log(KN)}{ N}}\right)^{1-1/k}\right)\)_._

For the case of \(\alpha=0\), as before one can simply choose to use the mean estimate result for \(\alpha=0\) as shown in Proposition 2 and adjust the burn-in period accordingly. This will lead to a bound that only has the second term in the above upper bounds. For \(\beta^{\star}\geq 2\), one can observe that the upper bound of Algorithm 3 almost matches the lower bounds in Proposition 5 for both LTC and CTL settings. However, when \(\beta^{\star}\in[1,2)\) (i.e., good coverage case), it is known that the performance of LCB is worse than imitation learning, i.e., simply returning the most frequently selected arm in the offline dataset (when there is no privacy and corruption) (Rashidinejad et al., 2021). We leave it to future work to give a tight characterization of the sub-optimality when \(\beta^{\star}\in[1,2)\). Moreover, the proof of Proposition 6 also naturally gives us high-probability bounds without specifying \(\delta=1/N\) in the end.

## 6 Simulations

Beyond our theoretical results, we have also conducted a set of simulations for our three problems. Our theoretical results capture the worst-case performance (i.e., minimax rates). Thus, for simulations, we are particularly interested in the following two questions: (i) _Can we simulate the worst-case scenario and test the performance of our proposed algorithms?_ and (ii) _How about their performance in non-worst-case scenarios?_ We give detailed answers to both questions for all three problems in Appendix A, which offers additional insights into the interplay between privacy and robustness.

## 7 Concluding Remarks

To conclude, we have demonstrated an interesting interplay between privacy and robustness in three problems: mean estimation, online and offline MABs. The punchline across three problems is thatcorruption after any LDP mechanism becomes easier, i.e., the same amount of corruption leads to a worse performance when compared to the case where Huber corruption happens before LDP mechanisms. We also give the first set of results for the most practical C-LDP-C setting.

Some interesting future directions include (i) improving the sub-optimal result for linear bandit in Charisopoulos et al. (2023) by following existing private linear bandits (Li et al., 2022, 2024) along with the assumption of bounded reward; (ii) generalizing it to other privacy models such as shuffle DP (Chowdhury and Zhou, 2022c); (iii) studying the case where the heavy-tailedness is characterized by the central moment rather than the raw moment currently considered in our paper; (iv) extending the results to locally private and robust reinforcement learning by building upon existing results such as Chowdhury and Zhou (2022a); Liao et al. (2023); Zhou (2022).

## Acknowledgements

XZ is supported in part by NSF CNS-2153220 and CNS2312835. XZ would like to thank Daniel Kane for the helpful discussions.

## References

* Agrawal et al. (2021) Shubhada Agrawal, Sandeep K Juneja, and Wouter M Koolen. Regret minimization in heavy-tailed bandits. In _Conference on Learning Theory_, pp. 26-62. PMLR, 2021.
* Agrawal et al. (2023) Shubhada Agrawal, Timothee Mathieu, Debabrota Basu, and Odalric-Ambrym Maillard. Crimed: Lower and upper bounds on regret for bandits with unbounded stochastic corruption. _arXiv preprint arXiv:2309.16563_, 2023.
* Agrawal et al. (2024) Shubhada Agrawal, Timothee Mathieu, Debabrota Basu, and Odalric-Ambrym Maillard. Crimed: Lower and upper bounds on regret for bandits with unbounded stochastic corruption. In _International Conference on Algorithmic Learning Theory_, pp. 74-124. PMLR, 2024.
* Altschuler et al. (2019) Jason M Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best arm identification for contaminated bandits. _J. Mach. Learn. Res._, 20(91):1-39, 2019.
* Asi et al. (2023) Hilal Asi, Jonathan Ullman, and Lydia Zakynthinou. From robustness to privacy and back. _arXiv preprint arXiv:2302.01855_, 2023.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47:235-256, 2002.
* Basu et al. (2022) Debabrota Basu, Odalric-Ambrym Maillard, and Timothee Mathieu. Bandits corrupted by nature: Lower bounds on regret and robust optimistic algorithm. _arXiv preprint arXiv:2203.03186_, 2022.
* Berry and Fristedt (1985) Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments (monographs on statistics and applied probability). _London: Chapman and Hall_, 5(71-87):7-7, 1985.
* Bhatt et al. (2022) Sujay Bhatt, Guanhua Fang, Ping Li, and Gennady Samorodnitsky. Minimax m-estimation under adversarial contamination. In _International Conference on Machine Learning_, pp. 1906-1924. PMLR, 2022.
* Bubeck et al. (2013) Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gabor Lugosi. Bandits with heavy tail. _IEEE Transactions on Information Theory_, 59(11):7711-7717, 2013.
* Charisopoulos et al. (2023) Vasileios Charisopoulos, Hossein Esfandiari, and Vahab Mirrokni. Robust and differentially private stochastic linear bandits. _arXiv preprint arXiv:2304.11741_, 2023.
* Chen et al. (2018) Mengjie Chen, Chao Gao, and Zhao Ren. Robust covariance and scatter matrix estimation under huber's contamination model. _The Annals of Statistics_, 46(5):1932-1960, 2018.
* Chen et al. (2022) Sitan Chen, Frederic Koehler, Ankur Moitra, and Morris Yau. Online and distribution-free robustness: Regression and contextual bandits with huber contamination. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pp. 684-695. IEEE, 2022.
* Chen et al. (2020)Xiaoyu Chen, Kai Zheng, Zixin Zhou, Yunchang Yang, Wei Chen, and Liwei Wang. (locally) differentially private combinatorial semi-bandits. In _International Conference on Machine Learning_, pp. 1757-1767. PMLR, 2020.
* Cheu et al. (2021) Albert Cheu, Adam Smith, and Jonathan Ullman. Manipulation attacks in local differential privacy. In _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 883-900. IEEE, 2021.
* Chhor & Sentenac (2023) Julien Chhor and Flore Sentenac. Robust estimation of discrete distributions under local differential privacy. In _International Conference on Algorithmic Learning Theory_, pp. 411-446. PMLR, 2023.
* Chowdhury & Zhou (2022a) Sayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pp. 6375-6383, 2022a.
* Chowdhury & Zhou (2022b) Sayak Ray Chowdhury and Xingyu Zhou. Distributed differential privacy in multi-armed bandits. _arXiv preprint arXiv:2206.05772_, 2022b.
* Chowdhury & Zhou (2022c) Sayak Ray Chowdhury and Xingyu Zhou. Shuffle private linear contextual bandits. _arXiv preprint arXiv:2202.05567_, 2022c.
* Diakonikolas & Kane (2023) Ilias Diakonikolas and Daniel M Kane. _Algorithmic high-dimensional robust statistics_. Cambridge University Press, 2023.
* Duchi et al. (2018) John C Duchi, Michael I Jordan, and Martin J Wainwright. Minimax optimal procedures for locally private estimation. _Journal of the American Statistical Association_, 113(521):182-201, 2018.
* Georgiev & Hopkins (2022) Kristian Georgiev and Samuel Hopkins. Privacy induces robustness: Information-computation gaps and sparse mean estimation. _Advances in Neural Information Processing Systems_, 35:6829-6842, 2022.
* Gupta et al. (2019) Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial corruptions. In _Conference on Learning Theory_, pp. 1562-1578. PMLR, 2019.
* Hopkins et al. (2023) Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pp. 497-506, 2023.
* Huber (1964) Peter J Huber. Robust estimation of a location parameter. _Ann. Math. Statist._, 35(4):73-101, 1964.
* Jain et al. (2022) Ayush Jain, Alon Orlitsky, and Vaishakh Ravindrakumar. Robust estimation algorithms don't need to know the corruption level. _arXiv preprint arXiv:2202.05453_, 2022.
* Kairouz et al. (2014) Peter Kairouz, Sewoong Oh, and Pramod Viswanath. Extremal mechanisms for local differential privacy. _Advances in neural information processing systems_, 27, 2014.
* Kairouz et al. (2015) Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In _International conference on machine learning_, pp. 1376-1385. PMLR, 2015.
* Kamath et al. (2020) Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distributions. In _Conference on Learning Theory_, pp. 2204-2235. PMLR, 2020.
* Kapoor et al. (2019) Sayash Kapoor, Kumar Kshitij Patel, and Purushottam Kar. Corruption-tolerant bandit learning. _Machine Learning_, 108(4):687-715, 2019.
* Kasiviswanathan et al. (2011) Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? _SIAM Journal on Computing_, 40(3):793-826, 2011.
* Li et al. (2022a) Fengjiao Li, Xingyu Zhou, and Bo Ji. Differentially private linear bandits with partial distributed feedback. In _2022 20th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)_, pp. 41-48. IEEE, 2022a.
* Li et al. (2024) Fengjiao Li, Xingyu Zhou, and Bo Ji. Distributed linear bandits with differential privacy. _IEEE Transactions on Network Science and Engineering_, 2024.
* Li et al. (2022b)Mengchu Li, Thomas B Berrett, and Yi Yu. On robustness and local differential privacy. _arXiv preprint arXiv:2201.00751_, 2022b.
* Liao et al. (2023) Chonghua Liao, Jiafan He, and Quanquan Gu. Locally differentially private reinforcement learning for linear mixture markov decision processes. In _Asian Conference on Machine Learning_, pp. 627-642. PMLR, 2023.
* Lugosi and Mendelson (2019) Gabor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. _Foundations of Computational Mathematics_, 19(5):1145-1190, 2019.
* Lykouris et al. (2018) Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pp. 114-122, 2018.
* Mishra and Thakurta (2015) Nikita Mishra and Abhradeep Thakurta. (nearly) optimal differentially private stochastic multi-arm bandits. In _Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence_, pp. 592-601, 2015.
* Mukherjee et al. (2021) Arpan Mukherjee, Ali Tajer, Pin-Yu Chen, and Payel Das. Mean-based best arm identification in stochastic bandits under reward contamination. _Advances in Neural Information Processing Systems_, 34:9651-9662, 2021.
* Niss and Tewari (2020) Laura Niss and Ambuj Tewari. What you see may not be what you get: Ucb bandit algorithms robust to \(\varepsilon\)-contamination. In _Conference on Uncertainty in Artificial Intelligence_, pp. 450-459. PMLR, 2020.
* Qiao and Wang (2022) Dan Qiao and Yu-Xiang Wang. Offline reinforcement learning with differential privacy. _arXiv preprint arXiv:2206.00810_, 2022.
* Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716, 2021.
* Ren et al. (2020) Wenbo Ren, Xingyu Zhou, Jia Liu, and Ness B Shroff. Multi-armed bandits with local differential privacy. _arXiv preprint arXiv:2007.03121_, 2020.
* Sajed and Sheffet (2019) Touqir Sajed and Or Sheffet. An optimal private stochastic-mab algorithm based on optimal private stopping rule. In _International Conference on Machine Learning_, pp. 5579-5588. PMLR, 2019.
* Tao et al. (2022) Youming Tao, Yulian Wu, Peng Zhao, and Di Wang. Optimal rates of (locally) differentially private heavy-tailed multi-armed bandits. In _International Conference on Artificial Intelligence and Statistics_, pp. 1546-1574. PMLR, 2022.
* Tenenbaum et al. (2021) Jay Tenenbaum, Haim Kaplan, Yishay Mansour, and Uri Stemmer. Differentially private multi-armed bandits in the shuffle model. _Advances in Neural Information Processing Systems_, 34, 2021.
* Tossou and Dimitrakakis (2016) Aristide CY Tossou and Christos Dimitrakakis. Algorithms for differentially private multi-armed bandits. In _Thirtieth AAAI Conference on Artificial Intelligence_, 2016.
* Wu et al. (2023) Yulian Wu, Xingyu Zhou, Youming Tao, and Di Wang. On private and robust bandits. _arXiv preprint arXiv:2302.02526_, 2023.
* Zheng et al. (2020) Kai Zheng, Tianle Cai, Weiran Huang, Zhenguo Li, and Liwei Wang. Locally differentially private (contextual) bandits learning. _Advances in Neural Information Processing Systems_, 33:12300-12310, 2020.
* Zhou (2022) Xingyu Zhou. Differentially private reinforcement learning with linear function approximation. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 6(1):1-27, 2022.
* Zhou and Tan (2021) Xingyu Zhou and Jian Tan. Local differential privacy for bayesian optimization. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(12):11152-11159, 5 2021.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims (i.e., there exists a fundamental interplay between LDP and robustness under Huber corruption and heavy-tailedness) in abstract and introduction reflect the contributions and scope of our paper. We also provide a list of our core contributions directly in our introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: One limitation is the algorithm's dependence on the unknown parameter \(\alpha\). A practical fix suggested is to use an estimated upper bound on \(\alpha\), but the theoretical aspect of completely removing this dependence is complex, see the discussion in Appendix C.2. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We state all assumptions of our theoretical results under three problems: mean estimation, online MABs, and offline MABs. For mean estimation, we offer a tight characterization of high-probability mean estimation errors under both LTC and CTL settings, with detailed proofs provided in Appendix E and Appendix F. Building on this, the paper presents an almost tight (up to a log factor) characterization of the minimax regret in online MABs (Appendix I, Appendix H) and the sub-optimality in offline MABs (Appendix J, Appendix K). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a comprehensive discussion of the experiment details for each problem and various types of corruption settings at the beginning of our simulation section (Appendix A). Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. *3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code with detailed instructions for the experiments discussed in Appendix A. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We thoroughly list all training details and demonstrate the results in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: We present different metrics to assess the performance of our algorithms with error bars. Mean estimation errors are shown under different corruption scenario (Fig. 2, Fig. 3). Similarly, the online algorithm's performance under various corruption scenarios is displayed (Fig. 4, Fig. 6). Comparisons between our specific algorithm and other algorithms under Online MABs are also provided (Fig. 5). Lastly, in offline MABs, the suboptimality of our algorithm is illustrated (Fig. 7). For full information, please check Appendix A. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Our experiments are designed primarily to support the theoretical results and are relatively simple in their settings. They do not require high-performance hardware and can be run on most standard computers. Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our experiments are relatively simple in their settings, and no privacy information is leaked through our code. Guidelines:* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broad implications in Appendix D. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

Simulations

In this section, we conduct numerical simulations to assess the performance of our algorithms in three problems (i.e., mean estimation, online MABs and offline MABs), under both LTC and CTL settings. Recall that our performance metrics for all three problems are minimax ones, which capture the worst-case performance. As a result, we are particularly interested in the following two questions in our simulations:

1. _Can we simulate the worst-case scenario and test the performance of our proposed algorithms?_
2. _How about their performance in non-worst-case scenarios?_

Note that (i) essentially sheds further light on how to design the most powerful adversary Huber corruption model, which in turn could explain the separation result between LTC and CTL from the perspective of attacking. On the other hand, (ii) would help to illustrate our algorithms' performance in some mild/real-world non-adversary Huber corruption. For example, although the minimax regret for online MABs has a linear term in the worst case, the actual performance under the non-adversary corruption model can be sub-linear as we will show later.

### Mean estimation

We start with the worst-case scenario for the mean estimation under a large sample size regime where the minimax error rate is dominated by the corruption part, i.e., the separation result \((\alpha/\varepsilon)^{1-1/k}\) under LTC vs. \(\alpha^{1-1/k}\) under CTL. To this end, we need to design the most powerful adversary corruption for both LTC and CTL. Here, we allow the (white-box) adversary to choose inlier distribution over \(X\) and can adaptively choose Huber corruption distribution based on inlier distribution and the knowledge of our algorithm, e.g., LDP mechanism \(Q\) in the LTC setting.

In particular, the adversary chooses the following inlier distribution:

\[P(X=1/\gamma)=\frac{1}{2}\gamma^{k},\quad P(X=-1/\gamma)=\frac{1}{2}\gamma^{ k},\quad P(X=0)=1-\gamma^{k} \tag{5}\]

where \(\gamma=(\alpha/\varepsilon)^{1/k}\) under LTC and \(\gamma=(\alpha)^{1/k}\) under CTL. One can clearly see that \(\mathbb{E}\left[|X|^{k}\right]\leq 1\) for all \(k>1\), hence \(P\in\mathcal{P}_{k}\) for any \(k>1\) and \(\alpha\leq\varepsilon\). Moreover, we have \(\mathbb{E}\left[X\right]=0\).

Now, we first consider the following strong Huber corruption model.

**Definition 7** (Strong Huber corruption for mean estimation).: Let the inlier distribution over \(X\) be given by (5). Under LTC: for each input \(Y_{i}\), with probability \(\alpha\), replace it with \(M\cdot\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1}\); Under CTL: for each input \(X_{i}\), with probability \(\alpha\), replace it with \(M\);

Note that, the white-box adversary knows our algorithm and hence \(M\). We are going to show that no matter how large the sample size is, the mean error has to be large for both LTC and CTL under the above strong Huber corruption.

Let us start with CTL and consider the sample size \(n\) to be large. Then, according to Algorithm 1, \(M=(1/\alpha)^{1/k}=1/\gamma\), which leads to the fact that the mean of \(Y\) is now \(\alpha M=\alpha^{1-1/k}\) (note \(\mathbb{E}\left[X\right]=0\)). Then, our estimator will essentially at best return the mean of \(Y\), hence leading to the error of \(\Omega(\alpha^{1-1/k})\). For LTC, with the choice of \(\gamma\) and \(M\), we also have \(M=1/\gamma\). By our design of LDP mechanism \(Q\) in Algorithm 1, the mean of \(Y\) is still zero and hence after the corruption, the mean of \(Z\) becomes \(\alpha\cdot M\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1}\), which is the best outcome of our estimator, hence the error of \(\Omega((\alpha/\varepsilon)^{1-1/k})\). Note that in both cases, the choice of corruption distribution needs care (i.e., adaptation to our algorithm), since otherwise, our estimator may still have an accurate estimate, as some other outlier values can be simply filtered out by our algorithm. More importantly, an alternative explanation of our separation result becomes evident: under LTC, the error is larger because the adversary has the capability to select a corruption value that is magnified by a factor of \(1/\varepsilon\).

In our experiments, we choose \(k=2\) and consider various corruption level \(\alpha\in\{0,0.02,0.05\}\) and privacy budget \(\varepsilon\in\{0.3,0.5,1\}\). For each set of parameters, we conduct \(300\) runs and plot the average of the estimation error and corresponding confidence region. Fig. 2 illustrates our simulation results under strong Huber corruption in Definition 7. A common pattern behind all the plots in Fig. 2 is that due to strong corruption, the estimation error will only converge to a plateau and almost match the lower bounds. Specifically, from the two plots in column (a), we see that when \(\alpha=0\) or \(\varepsilon=1\), the performance under LTC and CTL is close (i.e., no-separation), which aligns with our theoretical results. In the two plots of column (b), we see that LTC has a larger error than CTL and as \(\varepsilon\) decreases (i.e., stronger privacy), the difference becomes larger, which matches our theoretical separation results. Finally, comparing the plots in column (c) with those in (b), we see that as the corruption level increases, the performance becomes worse.

We also consider the following weak corruption model, which simply flips the sign of the data.

**Definition 8** (Weak Huber corruption for mean estimation).: Let the inlier distribution over \(X\) be given by (5). Under LTC: for each input \(Y_{i}\), with probability \(\alpha\), replace it with \(-Y_{i}\); Under CTL: for each input \(X_{i}\), with probability \(\alpha\), replace it with \(-X_{i}\);

In Fig.3, we can see that under weak Huber corruption, the estimation error under our estimators can indeed decrease as the sample size increases. This demonstrates that in some real-world mild corruption scenarios, our estimators can yield promising performance.

### Online MABs

#### a.2.1 Non-adversary Corruption

In this section, we first consider some classic heavy-tailed distributions under non-adversary corruption. The main purpose is to show that our proposed algorithm (i.e., Algorithm 2) can indeed achieve sublinear regret under certain scenarios. Moreover, our simulations will also provide some insights into our proof.

Figure 3: Mean estimation error with weak Huber corruption in Definition 8 under LTC and CTL settings

Figure 2: Mean estimation error with strong Huber corruption in Definition 7 under LTC and CTL settings

**Settings.** As in previous works Tao et al. (2022); Wu et al. (2023), we consider Pareto distribution, whose probability distribution is given by

\[f(x;x_{m},s)=\begin{cases}\frac{sx_{m}^{s}}{x^{s+1}},&\text{if }x\geq x_{m}\\ 0,&\text{otherwise}\end{cases}\]

where \(s>0\) is the shape parameter and \(x_{m}>0\) is the scale parameter. In our experiments, we consider there are \(K=10\) arms, and for each arm \(i\in[K]\), the distribution is Pareto with \(x_{m}=i\) and \(s=11\). To ensure that each arm's reward distribution is in \(\mathcal{P}_{k}\) (i.e., \(\mathbb{E}_{X\sim P}[|X|^{k}]\leq 1\)), we normalize the reward by the \(k\)-th moment, which is \(\frac{sx_{m}^{k}}{s-k}\). Consequently, the mean of each arm is \(\frac{s-k}{x_{m}^{k-1}(s-1)}\). We consider \(k=2\), which along with our choices of \(s\) and \(x_{m}\), yields that arm \(1\) is the best arm with a mean of \(0.9\) while arm \(10\) is the worst arm with a mean of \(0.09\). For the corruption, we consider the following Huber model.

**Definition 9** (Huber corruption for online/offline MABs).: Let each arm's inlier distribution be Pareto with the parameters described above. Under LTC, for each private view of reward from each \(a\in[K]\), with probability \(\alpha\), replace it with \(M\cdot\frac{e^{s}+1}{e^{s}-1}\). Under CTL, for each raw reward from each arm \(a\in[K]\), with probability \(\alpha\), replace it with \(M\).

_Remark 6_.: It is worth noting that even though the above corruption values are the same as in Definition 7, it is not necessarily the worst-case as the inliers are now Pareto. That is, even after corruption, the agent can possibly still distinguish between different arms. We also consider strong corruption cases where after corruption, the agent cannot distinguish the distributions of two arms, hence a linear regret, see Fig. 6 for details.

Fig. 4 illustrates the regret performance of our proposed algorithm (i.e., Algorithm 2) for online MABs under LTC and CTL settings, with the specific corruption given by Definition 9. The two plots in column (a) capture the LTC setting while the two plots in column (b) denote the CTL setting. In both settings, we can see that for small corruption level \(\alpha\), our algorithm can achieve sublinear regret, even though in the _worst-case_ our minimax bounds are linear. In column (c), we also directly compare the regret performance under LTC and CTL with different sets of parameters of \(\alpha\) and \(\varepsilon\). As expected, the regret performance under LTC is worse than that under CTL, and as \(\alpha\) increases or \(\varepsilon\) decreases, the gap becomes larger. This demonstrates separation results in terms of actual performance rather than only in terms of theoretical upper bounds. As a baseline, we also compare with one classic robust MAB algorithm under heavy-tailed rewards proposed in Bubeck et al. (2013).

Fig. 5 compares our specific algorithms with the algorithm proposed in Tao et al. (2022), namely LDPRSE, which is proposed for the setting of LDP and heavy-tailed rewards in online MABs. Hence, our comparisons were made in the online MAB setting under weak corruption. The findings are organized into two columns, demonstrating the impact of varying \(\alpha\) (corruption) values on

Figure 4: Regret performance with weak Huber corruption in Definition 9 under LTC and CTL settings.

performance as \(\varepsilon\) (privacy) increases. These results highlight the advantages of our algorithms over LDPRSE in situations where there exist additional corruptions.

Note that our purpose in this section is not to demonstrate the superior performance of our proposed algorithm over all existing robust or/and private algorithms (given a large number of different existing ones). Rather, one of the goals is to use simulations to highlight the separation between LTC and CTL. Another important goal is to provide more insights into our proof of the regret upper bounds. Specifically, in our proof of the LTC setting (similar in CTL setting), we will divide the set of all sub-optimal arms \(\mathcal{G}\) into two groups \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) where \(\mathcal{G}_{2}=\{a\in[K]\setminus a^{*}:c\left(\frac{\alpha}{\varepsilon} \right)^{1-1/k}\geq\frac{1}{2}\Delta_{a}\}\) for some constant \(c\). Then, we argue that if \(\mathcal{G}_{2}\) is empty, then one can still derive the standard logarithmic problem-dependent regret bound. This can also be somehow validated partially by our simulation results. In particular, under our problem instances described above, when \(\alpha=0.02\), \(\varepsilon=0.1\), and \(c=0.5\), we have \(|\mathcal{G}_{2}|=0\) under LTC (i.e., no sub-optimal arms in \(\mathcal{G}_{2}\)). In this case, as illustrated in the top plot of column (a) in Fig. 4, we can observe logarithmic order regret. This naturally extends to the larger \(\varepsilon\) case, as illustrated in the bottom plot in column (a).

#### a.2.2 Strong Huber Corruption

As mentioned above, we also create a strong Huber corruption for online MABs, in this case, the regret becomes linear which matches our minimax lower bound. In this scenario, our goal is to create an adversary strong Huber corruption for online MABs, where the agent cannot distinguish the distributions of two arms by utilizing the following probability distribution:

\[P(X=1/\gamma)=\gamma^{k},\quad P(X=0)=1-\gamma^{k}\] \[P^{\prime}(X=1/\gamma)=\gamma^{k}/2,\quad P^{\prime}(X=0)=1- \gamma^{k}/2\]

where \(\gamma\) adopts the form \(c_{1}\cdot(\alpha/\varepsilon)^{1/k}\) under LTC and \(c_{1}\cdot(\alpha)^{1/k}\) under CTL, with \(c_{1}\) configured as 0.1 to ensure \(\gamma^{k}\leq 1\) for an expansive \(\alpha\). As before, \(P,P^{\prime}\in\mathcal{P}_{k}\) for any \(k>1\) and \(\mu(P)=\gamma^{k-1}\), \(\mu(P)=\gamma^{k-1}/2\). Let \(P\) and \(P^{\prime}\) represent the distributions for arms 0 and 1 respectively. We define the corruption distribution under CTL settings as:

**Definition 10** (Strong Huber corruption under CTL Settings).: \[C(X=1/\gamma)=\gamma^{k}/2, C(X=0)=1-\gamma^{k}/2\] \[C^{\prime}(X=1/\gamma)=\gamma^{k}/(2\alpha), C^{\prime}(X=0)=1-\gamma^{k}/(2\alpha)\]

Figure 5: Comparison of Our Algorithms vs. LDPRSE in the online MAB setting under weak corruption.

According to 2, it is apparent that the agent cannot differentiate between \(P\) and \(P^{\prime}\) upon executing the operation:

\[(1-\alpha)P+\alpha C=(1-\alpha)P^{\prime}+\alpha C^{\prime}\]

This outcome emerges from the CTL's inherent nature of initially introducing contamination, which perseveres in maintaining indistinguishability, even post-transmission through the LDP channel and the Huber model.

In the context of LTC settings, the distinctiveness arises from the fact that the distributions of \(P\) and \(P^{\prime}\) undergo alterations after passing through LDP, necessitating corresponding corruptions. Let \(R\) and \(R^{\prime}\) be the post-LDP transformation distributions over variable Y, defined as:

\[R(Y=S)=\frac{1}{2}+\frac{\gamma^{k}}{2}\cdot\frac{e^{\varepsilon }-1}{e^{\varepsilon}+1},\quad R(Y=-S)=\frac{1}{2}-\frac{\gamma^{k}}{2}\cdot \frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}\] \[R^{\prime}(Y=S)=\frac{1}{2}+\frac{\gamma^{k}}{4}\cdot\frac{e^{ \varepsilon}-1}{e^{\varepsilon}+1},\quad R^{\prime}(Y=-S)=\frac{1}{2}-\frac{ \gamma^{k}}{4}\cdot\frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}\]

where \(S=M\cdot\frac{e^{\varepsilon}+1}{e^{\varepsilon}-1}\).

Additionally, we define the corruption distribution as:

**Definition 11** (Strong Huber corruption under LTC Settings).: \[N(Y=S)=\frac{\gamma^{k}}{4}\cdot\frac{e^{\varepsilon}-1}{e^{ \varepsilon}+1},\quad N(Y=-S)=1-\frac{\gamma^{k}}{4}\cdot\frac{e^{\varepsilon }-1}{e^{\varepsilon}+1}\] \[N^{\prime}(Y=S)=\frac{\gamma^{k}}{4}\cdot\frac{e^{\varepsilon}-1 }{e^{\varepsilon}+1}\cdot\frac{1}{\alpha},\quad N^{\prime}(Y=-S)=1-\frac{ \gamma^{k}}{4}\cdot\frac{e^{\varepsilon}-1}{e^{\varepsilon}+1}\cdot\frac{1}{\alpha}\]

Now we also have \((1-\alpha)R+\alpha N=(1-\alpha)R^{\prime}+\alpha N^{\prime}\), indicating our continued inability to distinguish between \(P\) and \(P^{\prime}\) in the LTC setting.

Fig. 6 illustrates the regret performance of our proposed algorithm (i.e., Algorithm 2) for online MABs under LTC and CTL settings, with the strong Huber corruption in Definition 10 and Definition 11. A common pattern behind all the plots in Fig. 6 is that due to strong huber corruption, the agent cannot distinguish the distributions of two arms, hence linear regret. Based on the analysis above, we anticipate that the regret will scale linearly by a factor of \(c_{1}\) with respect to our minimax clean regret and Fig. 6 aligned with our discussion. The two plots in column (a) capture the LTC setting while the two plots in column (b) denote the CTL setting. As expected, the regret performance under LTC is worse than that under CTL, highlighting separation results in terms of actual performance rather than only in terms of theoretical upper bounds.

### Offline MABs

In the offline case, the analyzer/agent is given a batch of pre-collected data with private and corrupted view. In our experiments, we again consider the case that there are \(K=10\) arms and each arm's raw reward distribution is Pareto with the same parameters as in the online case. For corruption, we again consider the one given by Definition 9.

One difference here is that we need to specify the behavior policy \(\pi\) that is used to collect the data. To this end, we consider the following policy \(\pi\) in our simulation results: for each sample size \(N\), we pulled the best arm (i.e., arm \(1\)) \(\frac{N}{3}\) times and each other arm \(i\neq 1\) uniformly, i.e., \(\frac{2N}{3(K-1)}\) times. That is, roughly speaking, we approximately have \(1/\pi(a^{\star})=3\), which aligns with our theoretical assumption (i.e., the finite concentrability coefficient \(\beta^{\star}\geq 2\) when our upper bounds are tight in minimax sense).

Fig. 7 illustrates the suboptimality of our algorithm (i.e., Algorithm 3) under both LTC and CTL settings. We can see that in both settings, the sub-optimality could approach zero under several values of privacy parameters. This again highlights that under mild/non-adversary corruption, the algorithm could yield reasonably good performance, rather than the pessimistic worst-case one. Also, we observe that even in this non-adversary corruption case, suboptimality under LTC in general is still worse than that under CTL. Finally, it is not surprising that for both LTC and CTL, as \(\alpha\) increases or \(\varepsilon\) decreases, sub-optimality will increase.

Figure 6: Regret performance with strong Huber corruption in Definition 10 unde CTL settings and Definition 11 under LTC settings.

Figure 7: Suboptimality performance with Huber corruption in Definition 9 under LTC and CTL settings.

Additional Related Work

**Private MABs.** To offer mathematically rigorous privacy protection, LDP is first introduced to MABs in Ren et al. (2020) where the authors establish private lower bounds on both problem-dependent and problem-independent (minimax) regrets as well as several LDP mechanisms and learning algorithms that achieve nearly-optimal performance. Later, it is generalized to the heavy-tailed setting in Tao et al. (2022). LDP has also been considered in various other bandit settings (Chen et al., 2020; Zheng et al., 2020; Zhou and Tan, 2021). In addition to LDP, other strictly weaker privacy models have also been considered in MABs to achieve a better regret, such as central DP where users need to trust the central learner (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016; Sajed and Sheffet, 2019) and distributed DP where users need to trust the intermediate third-party (Tenenbaum et al., 2021; Chowdhury and Zhou, 2022b). In addition to the above online MABs, recent work (Qiao and Wang, 2022) also considers offline RL (hence MABs) under central DP with bounded rewards.

**Robust MABs.** Robust MABs under Huber corruption have been recently studied in Kapoor et al. (2019); Mukherjee et al. (2021); Basu et al. (2022); Agrawal et al. (2024). Several other corruption models have also been considered in MABs, such as budgeted-corruption model where the cumulative difference between observed reward and true reward is bounded by some constant budget (Lykouris et al., 2018; Gupta et al., 2019) and strong contamination model (Niss and Tewari, 2020; Altschuler et al., 2019). Robust regret minimization in MABs under heavy-tailed rewards have also been studied, e.g., Bubeck et al. (2013); Agrawal et al. (2021).

**Private and Robust MABs.** As mentioned above, the existing literature largely investigate privacy and robustness in MABs separately. To the best of our knowledge, there are only two very recent works that consider privacy and robustness in MABs simultaneously. In Wu et al. (2023), the authors consider the central DP model where the raw non-private feedback received by the central learner can be first corrupted under Huber model. This is in sharp contrast to our local DP model, which is not only stronger but allows us to study the order of corruption and privacy. In Charisopoulos et al. (2023), the authors study linear bandits (which includes MAB as a special case) under LDP and then Huber corruption (i.e., LTC setting). As discussed in Section 4, their regret bound is sub-optimal and worse than ours when reduced to the MAB case. Note that we also study the CTL setting, which in turn highlights the interplay between privacy and corruption. Moreover, the results for both LTC and CTL allow us to give the first results for the C-LDP-C setting.

**Private and Robust Mean Estimation.** Our work is inspired by recent advances in (locally) private and robust mean estimation. In particular, for the CTL setting, the authors of Li et al. (2022) give the tight characterization in terms of mean-square-error (MSE). In contrast, we derive the high probability concentration. For LTC, both Cheu et al. (2021); Chhor and Sentenac (2023) give constant-probability concentration when the inlier distribution is bounded. Instead, we present the high-probability version even for heavy-tailed inlier distribution, which requires new analysis and design of the estimators. We would also like to point out some other related private and/or robust mean estimation results. For instance, under central DP, Kamath et al. (2020) gives the first high probability mean concentration for heavy-tailed distributions. For standard non-private mean estimation under heavy tails, we refer readers to the nice survey by Lugosi and Mendelson (2019). For non-private mean estimation under corruption in general high-dimension space, we refer readers to the nice book by Diakonikolas and Kane (2023). We finally remark that there are recent exciting advances in understanding the connection between robustness and privacy in mean estimation (e.g., robustness induces privacy Hopkins et al. (2023); Asi et al. (2023) and vice versa Georgiev and Hopkins (2022)), which, however, mainly focus on the central DP model.

## Appendix C Discussions

### Discussions on Practical Scenarios for LTC and CTL

In the introduction, we have motivated our paper using the example of online recommendation/advertising via MABs. Here we give two more concrete examples. The key difference between LTC and CTL in practice is that LTC mainly models the situation where the data transmission is vulnerable to manipulation while CTL models the situation where the data source is more vulnerable to manipulation.

**CTL:** Consider a healthcare recommendation system that suggests personalized health interventions based on patient data. In this case, the data might first be corrupted (intentionally or unintentionally) before being subjected to LDP mechanisms, such as when data is collected from various sources with different levels of reliability or when users self-report their health information with errors or falsifications. However, the data transmission is often well-controlled in this case and is not likely vulnerable to manipulation due to strong federal regulations.

**LTC:** Consider a wireless IoT (Internet-of-Things) smart-home application where sensors are deployed to monitor/control the temperature or other metrics in homes. These sensors often have built-in checks to ensure that the data is collected correctly. However, after the LDP mechanism at each sensor from each home, the data transmission process through wireless networks (channels) is often more vulnerable to manipulation attacks, e.g., man-in-the-middle attacks, packet sniffing, or spoofing.

In addition to the above two examples, we do believe that there are many other practical scenarios that motivate our study of the interplay between LDP and Huber corruption.

**Key implication of "LTC is harder":** If our recommendation system requires LDP protection, then the adversary can tailor its manipulation attack (corruption) based on the LDP mechanism (hence \(\varepsilon\)) to amplify the error by the order of \(1/\varepsilon\). In other words, LDP protocols are highly vulnerable to manipulation - poisoning the private messages can be far more destructive than poisoning the data itself. As a result, it is important to keep our private protocol "secret" as the adversary needs to tailor its attack according to the LDP protocol to create the worst-case scenario (strongest attack).

### Robust Estimators without Knowing Corruption Parameter

Currently, whether it is possible to derive a tight error bound without knowing \(\alpha\) is still unclear to us. In particular, on the one hand, there are some positive results (Jain et al., 2022; Bhatt et al., 2022) for some estimators. On the other hand, some work suggests some negative results regarding MAB problems (Agrawal et al., 2024). Note that all Jain et al. (2022); Bhatt et al. (2022); Agrawal et al. (2024) only consider corruption, i.e., no privacy protection. Thus, one interesting future work is to settle down this problem, which is beyond the scope of our current paper.

### Ungrounded Regret Upper Bound in State-of-the-Art

In Tao et al. (2022), the authors consider a simpler setting - locally private heavy-tailed online MABs, i.e., without corruption. They claimed to achieve a regret upper bound on the order of \(O\left(\left(\frac{K}{\varepsilon^{2}}\right)^{1-1/k}T^{1/k}\right)\). However, given our tighter lower bound \(\Omega\left(T^{\frac{k+1}{2k}}\left(\frac{K}{\varepsilon^{2}}\right)^{\frac{k -1}{2k}}\right)\) in Proposition 3, their upper bound becomes ungrounded as it contradicts our lower bound for large \(T\). In particular, considering \(k=2\) (with only a bounded second moment), our lower bound gives a regret on the order of \(\Omega(T^{3/4})\) while their upper bound is \(O(\sqrt{T})\).

_Remark 7_.: In fact, our lower bound also gives another interesting interplay between privacy and robustness (in particular, heavy-tailed rewards). Specifically, in the non-private case, as shown in the Bubeck et al. (2013), one can still achieve \(\Theta(\sqrt{T})\) regret when the reward distributions have only bounded second moments. However, in the locally private case, our lower bound indicates that the regret is at least \(\Omega(T^{3/4})\).

## Appendix D Broader Impact Statement

This research presents novel insights into the interplay between local differential privacy and robustness in the context of Multi-Armed Bandits (MABs), with a focus on two distinct settings: Local Differential Privacy then Corruption (LTC) and Corruption then Local Differential Privacy (CTL). The findings have broad implications in various domains, particularly in online advertising and recommendation systems, where privacy preservation and data integrity are paramount. By enhancing the robustness of MAB algorithms against corruption and heavy-tailed feedback while ensuring local privacy, our work can significantly contribute to the development of more secure and reliable decision-making systems. We show that the mean estimation error under LTC is larger than under CTL, emphasizing that LTC is a more challenging setting. This separation is critical forpractical applications like healthcare recommendation systems (CTL) and wireless IoT smart-home applications (LTC). Additionally, our algorithms can adaptively guarantee optimal minimax rates across different settings without prior knowledge, which is crucial for real-world scenarios where the specific setting may not be known in advance. However, the complexity and computational demands of these advanced algorithms might limit their accessibility to smaller organizations, potentially widening the gap between large and small entities. Moreover, while our approach reduces privacy leakage and data manipulation risks, it does not completely eliminate them. This is particularly important because adversaries can tailor their attacks based on the LDP mechanism to amplify errors. Thus, ongoing efforts should focus on further improving these algorithms to address potential ethical issues, including data bias and privacy concerns, and enhancing their accessibility and fairness. Furthermore, deriving tight error bounds without knowing the corruption parameter \(\alpha\) remains an open challenge, suggesting the need for future research in this area.

## Appendix E Proof of Proposition 1

Proof.: We first focus on the LTC setting and divide the proof into two steps.

**Step 1: Without corruption.** By definition, it suffices to establish a lower bound on the concentration even without corruption. That is, under LTC, \(Z_{i}=Y_{i}\) for all \(i\in[n]\). This will give us the second term in the bound.

Consider the following two distributions \(P\) and \(P^{\prime}\). Let \(\gamma>0\), specified later and

\[P(X=1/\gamma)=\gamma^{k},\quad P(X=0)=1-\gamma^{k}\] \[P^{\prime}(X=1/\gamma)=1/2\cdot\gamma^{k},\quad P^{\prime}(X=0)= 1-1/2\cdot\gamma^{k}. \tag{6}\]

It is easy to see that for both \(P,P^{\prime}\), \(\mathbb{E}\left[|X|^{k}\right]\leq 1\) for all \(k>1\), hence \(P,P^{\prime}\in\mathcal{P}_{k}\) for any \(k>1\). Moreover, we have \(|\mu(P)-\mu(P^{\prime})|=1/2\cdot\gamma^{k-1}\) and \(\mathrm{TV}\left(P,P^{\prime}\right)=1/2\cdot\gamma^{k}\). For any \(\varepsilon\)-LDP channel \(Q\), let \(M\) and \(M^{\prime}\) be the induced marginal distribution from \(P\) and \(P^{\prime}\), respectively. That is, for \(i\in[n]\), \(Y_{i}\sim M\) and \(Y_{i}^{\prime}\sim M^{\prime}\). Let \(Y_{[n]}=\{Y_{i}\}_{i=1}^{n}\) and \(Y_{[n]}^{\prime}=\{Y_{i}^{\prime}\}_{i=1}^{n}\), i.e., \(Y_{[n]}\sim M^{\otimes n}\) and \(Y_{[n]}^{\prime}\sim M^{\prime\otimes n}\).

The high-level idea behind our proof is as follows: Given any sample size \(n\), if there exists at least probability \(2\delta\) such that \(Y_{[n]}=Y_{[n]}^{\prime}\), then one has to incur \(\Omega(\gamma^{k-1})\) estimation error with probability \(\delta\). This naturally reminds us to think about maximal coupling, since it maximizes the probability that \(Y_{[n]}=Y_{[n]}^{\prime}\) and is also closely related to \(\mathrm{TV}\) distance. In particular, we have the following textbook facts.

**Lemma 1**.: _Let \(P_{1}\) and \(P_{2}\) be two distributions on \(\mathcal{X}\) that share the same \(\sigma\)-algebra. There exists a coupling \(\omega^{*}(P_{1},P_{2})\), which is a distribution over \(\mathcal{X}^{2}\) such that_

\[\mathbb{P}_{(X_{1},X_{2})\sim\omega^{*}(P_{1},P_{2}))}(X_{1}\neq X _{2})=\mathrm{TV}(P_{1},P_{2})\] \[\forall S\text{ measurable},\mathbb{P}_{(X_{1},X_{2})\sim\omega^{* }(P_{1},P_{2}))}(X_{1}\in S)=P_{1}(X_{1}\in S)\] \[\forall S\text{ measurable},\mathbb{P}_{(X_{1},X_{2})\sim\omega^{* }(P_{1},P_{2}))}(X_{2}\in S)=P_{2}(X_{2}\in S).\]

_This coupling is called maximal coupling._

Based on this fact, fix some \(n\), if \((Y_{[n]},Y_{[n]}^{\prime})\) is sampled from the maximal coupling \(\omega^{*}(M^{\otimes n},M^{\prime\otimes n})\), then we know that there exists a probability \(p=1-\mathrm{TV}(M^{\otimes n},M^{\prime\otimes n})\) such that \(Y_{[n]}=Y_{[n]}^{\prime}\). To lower bound \(p\), we need to upper bound the \(\mathrm{TV}\) distance. To this end, we will leverage Bretagnolle-Huber inequality and strong data processing inequality (i.e., Corollary 3 in Duchi et al. (2018)). In particular,we have

\[\mathrm{TV}(M^{\otimes n},M^{\prime\otimes n}) \stackrel{{(a)}}{{\leq}}1-\frac{1}{2}\exp\left(- \mathrm{KL}\left(\left.M^{\otimes n}\right\|M^{\prime\otimes n}\right)\right)\] \[\stackrel{{(b)}}{{=}}1-\frac{1}{2}\exp\left(-4(e^{ \varepsilon}-1)^{2}\cdot n\cdot(\mathrm{TV}\left(P,P^{\prime}\right))^{2}\right)\] \[=1-\frac{1}{2}\exp\left(-4(e^{\varepsilon}-1)^{2}\cdot n\cdot \gamma^{2k}\right)\] \[\stackrel{{(c)}}{{\leq}}1-\frac{1}{2}\exp\left(-16 \varepsilon^{2}\cdot n\cdot\gamma^{2k}\right),\]

where (a) holds by Bretagnolle-Huber inequality; (b) holds by Corollary 3 in Duchi et al. (2018); (c) is true since \(e^{\varepsilon}-1\leq 2\varepsilon\) for \(\varepsilon\in[0,1]\). Thus, let \(\gamma=c_{1}\left(\frac{\sqrt{\log(1/\delta)}}{\varepsilon\sqrt{n}}\right)^{1 /k}\) for some constant \(c\). Then, for large enough \(n\), \(\gamma^{k-1}<1\) and \(\mathrm{TV}(M^{\otimes n},M^{\prime\otimes n})\leq 1-2\delta\), which implies that with probability at least \(\delta\), the error is \(\Omega(\gamma^{k-1})=\Omega\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}\right)\).

**Step 2: Corruption part.** Recall that under \(\alpha\)-Huber, for each private view \(Y_{i}\), it is independently corrupted with probability \(\alpha\), and when it happens, \(Z_{i}\) is sampled from an arbitrary noise distribution \(N\); otherwise, \(Z_{i}=Y_{i}\). To proceed, we will utilize the following useful fact.

**Lemma 2** (Theorem 5.1 in Chen et al. (2018)).: _Let \(R_{1}\) and \(R_{2}\) be two distributions on \(\mathcal{X}\); If for some \(\alpha\in[0,1/2)\), we have that \(\mathrm{TV}\left(R_{1},R_{2}\right)\leq\frac{\alpha}{1-\alpha}\), then there exist two distributions \(N_{1}\) and \(N_{2}\) on the same probability space such that_

\[(1-\alpha)R_{1}+\alpha N_{1}=(1-\alpha)R_{2}+\alpha N_{2}.\]

This result says that the Huber model with parameter \(\alpha\) can corrupt two distributions that are close in \(\mathrm{TV}\) distance so that the outputs are essentially sampled from the same distribution, hence indistinguishable.

Another fact we will leverage is that LDP mechanism is a "contraction" in that it will make the \(\mathrm{TV}\) distance closer.

**Lemma 3** (Corollary 2.9 in Kairouz et al. (2014)).: _For any \(\varepsilon>0\), let \(Q\) be any \(\varepsilon\)-LDP mechanism. Then, for any pair of distributions \(P_{1}\) and \(P_{2}\), the induced marginals \(M_{1}\) and \(M_{2}\) satisfy_

\[\mathrm{TV}\left(M_{1},M_{2}\right)\leq\frac{e^{\varepsilon}-1}{e^{\varepsilon }+1}\mathrm{TV}\left(P_{1},P_{2}\right).\]

The above fact indicates that for \(\varepsilon\in[0,1]\), \(\mathrm{TV}\left(M_{1},M_{2}\right)\leq O(\varepsilon)\mathrm{TV}\left(P_{1}, P_{2}\right)\). With the above two facts, it suffices for us to find two distributions \(P\) and \(P^{\prime}\) for \(X_{i}\) with a "large" mean difference, such that the induced marginal distributions for \(Y_{i}\) is \(O(\alpha)\). To this end, we again consider the two distributions in (6) with a different choice of \(\gamma\). Since \(\mathrm{TV}\left(P,P^{\prime}\right)=1/2\cdot\gamma^{k}\), by Lemma 3, choosing \(\gamma=c^{\prime}\cdot(\alpha/\varepsilon)^{1/k}\) for some small constant \(c^{\prime}>0\) yields that \(\mathrm{TV}\left(M,M^{\prime}\right)\leq\alpha\leq\alpha/(1-\alpha)\). Hence, by Lemma 2, there exists Huber contamination such that it is impossible to distinguish the final outputs. Hence, with a probability of at least \(1/2\), the error is \(\Omega\left(\gamma^{k-1}\right)=\Omega\left((\alpha/\varepsilon)^{1-1/k}\right)\). We finally conclude that for any \(\delta\in(0,1/2)\), with probability at least \(\delta\), for all large enough \(n\), estimation error is \(\Omega\left(\gamma^{k-1}\right)=\Omega\left((\alpha/\varepsilon)^{1-1/k}\right)\). This finishes the proof for the LTC setting.

As for the CTL setting, the second term in the lower bound follows the same proof as in Step 1. The key difference lies in Step 2, i.e., the first term in the bound. In particular, since the contamination is before LDP, one can now only choose \(\gamma=c^{\prime}\alpha^{1/k}\), i.e., no "contraction" from LDP anymore. As a result, the estimation error is \(\Omega\left(\gamma^{k-1}\right)=\Omega\left(\alpha^{1-1/k}\right)\). 

## Appendix F Proof of Proposition 2

Proof.: Let us start with the LTC setting. As for privacy, it builds on the privacy guarantee of random response.

Privacy.By definition, we need to show that for any two inputs \(x,x^{\prime}\in\mathcal{X}\) and \(y\in\left\{M\frac{e^{x}+1}{e^{x}-1},-M\frac{e^{x}+1}{e^{x}-1}\right\}\)

\[\frac{\mathbb{P}\left[Y=y|X=x\right]}{\mathbb{P}\left[Y=y|X=x^{\prime}\right]} \leq e^{\varepsilon}.\]

Consider the case \(y=M\frac{e^{x}+1}{e^{x}-1}\) and similar analysis applies to the other case. Let \(P_{x\to M^{+}}\) be the probability that \(x\) is translated to \(M\) in our mechanism \(Q\) and \(P_{x\to M^{-}}\) be the probability that \(x\) is translated to \(-M\) in our mechanism \(Q\). Similarly defines \(P_{x^{\prime}\to M^{+}}\) and \(P_{x^{\prime}\to M^{-}}\).

Thus, according to our \(Q\) in Algorithm 1) and let \(P_{\varepsilon}:=\frac{e^{\varepsilon}}{e^{x}+1}\), we have

\[\mathbb{P}\left[Y=y|X=x\right]=P_{x\to M^{+}}P_{\varepsilon}+P_{x \to M^{-}}(1-P_{\varepsilon})\] \[\mathbb{P}\left[Y=y|X=x^{\prime}\right]=P_{x^{\prime}\to M^{+}}P_{ \varepsilon}+P_{x^{\prime}\to M^{-}}(1-P_{\varepsilon})\]

As a result,

\[\frac{\mathbb{P}\left[Y=y|X=x\right]}{\mathbb{P}\left[Y=y|X=x^{\prime}\right]} =\frac{P_{x\to M^{+}}P_{\varepsilon}+P_{x\to M^{-}}(1-P_{\varepsilon})}{P_{x ^{\prime}\to M^{+}}P_{\varepsilon}+P_{x^{\prime}\to M^{-}}(1-P_{\varepsilon})} \leq\frac{P_{\varepsilon}}{1-P_{\varepsilon}}\leq e^{\varepsilon}.\]

**Utility.** For the utility part, we will divide the proof into four steps.

We draw the following informal diagram for an illustration of Algorithm 1.

\[X_{i}\xrightarrow{\text{Tunc.}(M)}\bar{X}_{i}\xrightarrow{\text{ Random Rounding}}X^{\prime}_{i}\xrightarrow{\text{Random Response}}Y_{i}\xrightarrow{\text{Corruption}}Z_{i}\xrightarrow{\text{Tunc.}(M\frac{e^{x}+1}{e^{x}-1})}\bar{Z}_{i} \xrightarrow{\text{Sample Mean}}\widehat{\mu}_{n}\]

**Step 1:** Bound the number of corrupted points.

By Chernoff bound for the binomial distribution, we have that for \(n\geq 3\log(1/\delta)/\alpha\)

\[|\mathcal{B}|\leq 2\alpha n,\quad w.p.\quad 1-\delta,\]

where \(|\mathcal{B}|\) denotes the total number of corrupted ("bad") points. Let this event be \(\mathcal{E}\), and in the following steps, we will condition on this event.

**Step 2:** Bound the distance \(|\mathbb{E}\left[X^{\prime}_{i}\right]-\mathbb{E}\left[X_{i}\right]|\).

\[|\mathbb{E}\left[X_{i}\right]-\mathbb{E}\left[X^{\prime}_{i}\right]| \leq|\mathbb{E}\left[X_{i}\right]-\mathbb{E}\left[\bar{X}_{i} \right]|+|\mathbb{E}\left[\bar{X}_{i}\right]-\mathbb{E}\left[X^{\prime}_{i} \right]|\] \[\stackrel{{(a)}}{{=}}|\mathbb{E}\left[X_{i}\right]- \mathbb{E}\left[\bar{X}_{i}\right]|+0\] \[\leq\mathbb{E}\left[|X_{i}|\mathds{1}(|X_{i}|\geq M)\right]\] \[\stackrel{{(b)}}{{\leq}}\frac{1}{M^{k-1}}\]

where (a) holds by the property of random rounding. Recall that, for any \(\bar{X}_{i}\in[-M,M]\), \(X^{\prime}_{i}=M\) w.p. \(\frac{1+\bar{X}_{i}/M}{2}\) and \(X^{\prime}_{i}=-M\) w.p. \(\frac{1-\bar{X}_{i}/M}{2}\). Thus, one can see \(\mathbb{E}\left[X^{\prime}_{i}|\bar{X}_{i}\right]=\bar{X}_{i}\), hence \(\mathbb{E}\left[\bar{X}_{i}\right]=\mathbb{E}\left[X^{\prime}_{i}\right]\); (b) holds by Holder's inequality and the fact \(k\)-th moment of \(X_{i}\) is upper bounded by one.

**Step 3:** Bound the distance \(|\mathbb{E}\left[X^{\prime}_{i}\right]-\widehat{\mu}_{n}|\).

\[|\widehat{\mu}_{n}-\mathbb{E}\left[X^{\prime}_{i}\right]| =|\frac{1}{n}\sum_{i}\bar{Z}_{i}-\mathbb{E}\left[X^{\prime}_{i} \right]|\] \[=|\frac{1}{n}\sum_{i}\bar{Z}_{i}-\frac{1}{n}\sum_{i}Y_{i}+\frac{ 1}{n}\sum_{i}Y_{i}-\mathbb{E}\left[X^{\prime}_{i}\right]|\] \[\stackrel{{(a)}}{{\leq}}2\alpha\cdot M\cdot\frac{e^ {\varepsilon}+1}{e^{\varepsilon}-1}+|\frac{1}{n}\sum_{i}Y_{i}-\mathbb{E}\left[X^ {\prime}_{i}\right]|\] \[\stackrel{{(b)}}{{\leq}}2\alpha\cdot M\cdot\frac{e^ {\varepsilon}+1}{e^{\varepsilon}-1}+O\left(M\cdot\frac{e^{\varepsilon}+1}{e^{ \varepsilon}-1}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right)\quad w.p.\quad 1-\delta\]where (a) holds by triangle inequality, the event \(\mathcal{E}\) in step 1, and the fact that \(\bar{Z}_{i}\), \(Y_{i}\) are both bounded; (b) holds by Hoeffding inequality. Note that \(Y_{i}=\frac{e^{\epsilon}+1}{e^{\epsilon}-1}X_{i}^{\prime}\) w.p. \(\frac{e^{\epsilon}}{e^{\epsilon}+1}\) and \(Y_{i}=-\frac{e^{\epsilon}+1}{e^{\epsilon}-1}X_{i}^{\prime}\) w.p. \(\frac{1}{e^{\epsilon}+1}\). That is, \(\mathbb{E}\left[Y_{i}\right]=\mathbb{E}\left[X_{i}^{\prime}\right]\) and \(Y_{i}=\{M\cdot\frac{e^{\epsilon}+1}{e^{\epsilon}-1},-M\cdot\frac{e^{\epsilon}+ 1}{e^{\epsilon}-1}\}\).

**Step 4:** Put the above two parts together.

For any \(\varepsilon\in[0,1]\), any \(\delta\in(0,1)\) and any \(P\in\mathcal{P}_{k}\), we have with probability at least \(1-\delta\),

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\frac{1}{M^{k-1}}+\frac{\alpha M}{ \varepsilon}+\frac{M}{\varepsilon}\sqrt{\frac{\log(1/\delta)}{n}}\right).\]

Thus, choosing \(M=\min\left\{\left(\frac{\varepsilon}{\alpha}\right)^{1/k},\left(\frac{\sqrt{ n}\varepsilon}{\sqrt{\log(1/\delta)}}\right)^{1/k}\right\}\), yields that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\alpha}{\varepsilon}\right)^ {1-1/k}+\left(\frac{1}{\varepsilon}\sqrt{\frac{\log(1/\delta)}{n}}\right)^{1- 1/k}\right),\]

which finishes the proof for the LTC setting.

Now, let us move to the CTL setting. For privacy, it follows from the same idea as in the LTC setting. For utility, we will divide the proof into five steps and leverage the following informal diagram for an illustration of Algorithm 1.

\[X_{i}\xrightarrow{\text{Corruption}}Y_{i}\xrightarrow{\text{Trunc}\left(M \right)}\bar{Y}_{i}\xrightarrow{\text{Random Rounding}}Y_{i}^{\prime} \xrightarrow{\text{Random Response}}Z_{i}\xrightarrow{\text{Sample Mean}}\widehat{ \mu}_{n}\]

**Step 1:** Bound the number of corrupted points.

By Chernoff bound for the binomial distribution, we have that for \(n\geq 3\log(1/\delta)/\alpha\)

\[|\mathcal{B}|\leq 2\alpha n,\quad w.p.\quad 1-\delta,\]

where \(|\mathcal{B}|\) denotes the total number of corrupted ("bad") points. Let this event be \(\mathcal{E}\), and in the following steps, we will condition on this event.

**Step 2:** Bound the distance \(|\widehat{\mu}_{n}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i}\right]|\).

\[|\widehat{\mu}_{n}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i }\right]| \stackrel{{(a)}}{{=}}|\frac{1}{n}\sum_{i}Z_{i}-\frac{1 }{n}\sum_{i}\mathbb{E}\left[Y_{i}^{\prime}\right]|\] \[\stackrel{{(b)}}{{\leq}}O\left(M\cdot\frac{e^{ \epsilon}+1}{e^{\epsilon}-1}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right)\quad w.p.\quad 1-\delta,\]

where (a) holds by property of random rounding, i.e., \(\mathbb{E}\left[\bar{Y}_{i}\right]=\mathbb{E}\left[Y_{i}^{\prime}\right]\); (b) holds by property of random response, i.e., \(\mathbb{E}\left[Z_{i}\right]=\mathbb{E}\left[Y_{i}^{\prime}\right]\) and Hoeffding inequality.

**Step 3:** Bound the distance \(|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i }\right]|\).

\[|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i }\right]|\leq O\left(M\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right),\quad w.p. \quad 1-\delta\]

where it simply follows from Hoeffding's inequality.

**Step 4:** Bound the distance \(|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\mathbb{E}\left[X_{i}\right]|\).

\[|\frac{1}{n}\sum_{i\in[n]}\bar{Y}_{i}-\mathbb{E}\left[X_{i}\right]| \stackrel{{(a)}}{{=}} |\frac{1}{n}\sum_{i\in\mathcal{G}}\bar{Y}_{i}-\mathbb{E}\left[X_{i }\right]+\frac{1}{n}\sum_{i\in\mathcal{B}}\bar{Y}_{i}|\] \[\stackrel{{(b)}}{{\leq}} |\frac{1}{n}\sum_{i\in\mathcal{G}}\bar{Y}_{i}-\mathbb{E}\left[X_{i }\right]|+2\alpha M\] \[=|\frac{1}{n}\sum_{i\in[n]}X_{i}\mathds{1}(|X_{i}|\leq M)-\mathbb{ E}\left[X_{i}\right]-\frac{1}{n}\sum_{i\in[\mathcal{B}]}X_{i}\mathds{1}(|X_{i}| \leq M)|+2\alpha M\] \[\leq|\frac{1}{n}\sum_{i\in[n]}X_{i}\mathds{1}(|X_{i}|\leq M)- \mathbb{E}\left[X_{i}\right]|+4\alpha M\] \[\leq|\underbrace{1}_{n}\sum_{i\in[n]}X_{i}\mathds{1}(|X_{i}|\leq M )-\mathbb{E}\left[X_{i}\mathds{1}(|X_{i}|\leq M)\right]|+\underbrace{\mathbb{ E}\left[X_{i}\mathds{1}(|X_{i}|\leq M)\right]-\mathbb{E}\left[X_{i}\right]}_{ \mathcal{T}_{2}}+4\alpha M\]

where in (a), \(\mathcal{G}\) represents all "good" indexes that are not corrupted and \(\mathcal{B}\) represents all "bad" indexes that are corrupted; (b) follows from the boundedness of \(\bar{Y}_{i}\) and the event \(\mathcal{E}\) in step 1.

For \(\mathcal{T}_{2}\), by Holder's inequality and the fact \(k\)-th moment of \(X_{i}\) is upper bounded by one, we have

\[\mathcal{T}_{2}\leq O\left(\frac{1}{M^{k-1}}\right).\]

For \(\mathcal{T}_{1}\), we consider two cases: (i) \(k\in(1,2)\) and (ii) \(k\geq 2\) when applying Bernstein's inequality.

For case (i), we note that \(\mathbb{E}\left[X_{i}^{2}\mathds{1}(|X_{i}|\leq M)\right]=\mathbb{E}\left[X_{i }|^{k}|X_{i}|^{2-k}\mathds{1}(|X_{i}|\leq M)\right]\stackrel{{(a) }}{{\leq}}\mathbb{E}\left[|X_{i}|^{k}M^{2-k}\right]\leq M^{2-k}\), where (a) follows from \(k<2\). Thus, by Bernstein's inequality, we have

\[\mathcal{T}_{1}\leq O\left(\sqrt{\frac{M^{2-k}\log(1/\delta)}{n}}+\frac{M\log (1/\delta)}{n}\right).\]

For case (ii), we note that \(\mathbb{E}\left[X_{i}^{2}\mathds{1}(|X_{i}|\leq M)\right]\leq\mathbb{E}\left[X _{i}^{2}\right]\leq 1\). Thus, by Bernstein's inequality, we have

\[\mathcal{T}_{1}\leq O\left(\sqrt{\frac{\log(1/\delta)}{n}}+\frac{M\log(1/ \delta)}{n}\right).\]

**Step 5:** Put everything together. Case (i): for any \(k\in(1,2)\), \(\varepsilon\in[0,1]\), any \(\delta\in(0,1)\) and any \(P\in\mathcal{P}_{k}\), we have with probability at least \(1-\delta\),

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\sqrt{\frac{M^{2-k}\log(1/\delta)}{n}} \right)+O\left(\frac{M\log(1/\delta)}{n}\right)+O\left(\frac{1}{M^{k-1}} \right)+O(\alpha M)+O\left(\frac{M}{\varepsilon}\cdot\sqrt{\frac{\log(1/\delta )}{n}}\right).\]

Thus, choosing \(M=\min\left\{\left(\frac{n}{\log(1/\delta)}\right)^{1/k},\left(\frac{1}{\alpha }\right)^{1/k},\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}} \right)^{1/k}\right\}\), yields that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\log(1/\delta)}{n}\right)^{ 1-1/k}+\alpha^{1-1/k}+\left(\frac{\sqrt{\log(1/\delta)}}{\varepsilon\sqrt{n} }\right)^{1-1/k}\right).\]

Hence, when \(n\geq\log(1/\delta)\), we have

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+\alpha^{1-1/k}\right).\]Case (ii): for any \(k\geq 2\), \(\varepsilon\in[0,1]\), any \(\delta\in(0,1)\) and any \(P\in\mathcal{P}_{k}\), we have with probability at least \(1-\delta\),

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)+O \left(\frac{M\log(1/\delta)}{n}\right)+O\left(\frac{1}{M^{k-1}}\right)+O(\alpha M )+O\left(\frac{M}{\varepsilon}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right).\]

Thus, choosing \(M=\min\left\{\left(\frac{n}{\log(1/\delta)}\right)^{1/k},\left(\frac{1}{\alpha }\right)^{1/k},\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}}\right)^ {1/k}\right\}\), yields that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\sqrt{\frac{\log(1/\delta)}{n}}+\left( \frac{\log(1/\delta)}{n}\right)^{1-1/k}+\alpha^{1-1/k}+\left(\frac{\sqrt{\log (1/\delta)}}{\varepsilon\sqrt{n}}\right)^{1-1/k}\right).\]

Hence, when \(n\geq\log(1/\delta)\), we have

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+\alpha^{1-1/k}\right).\]

Finally, combining the above two cases, we see that when \(n\geq\log(1/\delta)\), for any \(k>1\), it suffices to choose \(M=\min\left\{\left(\frac{1}{\alpha}\right)^{1/k},\left(\frac{\varepsilon\sqrt{ n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\right\}\) and obtain that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+\alpha^{1-1/k}\right).\]

which finishes the proof for the CTL setting.

## Appendix G Proof of the Upper Bound for the C-LDP-C Setting

After the proofs for the previous two settings, we can easily establish the upper bound for the C-LDP-C setting. For completeness, we also provide a detailed proof. To elucidate the utility of our approach, we will structure the proof into four distinct steps, building upon the derivation outlined previously.

\[X_{i}\xrightarrow{\text{Corruption}}Y_{i}\xrightarrow{\text{Trunc.}(M)}\bar{Y }_{i}\xrightarrow{\text{Random Rounding}}Y_{i}^{\prime}\xrightarrow{\text{ Random Response}}Z_{i}\xrightarrow{\text{Corruption}}Z_{i}^{\prime}\xrightarrow{\text{Trunc.}(M \frac{\varepsilon^{\prime}+1}{\varepsilon^{\prime}-1})}\bar{Z}_{i}\xrightarrow{ \text{Sample Mean}}\widehat{\mu}_{n}\]

**Step 1:** Bound the distance \(|\mathbb{E}\left[Y_{i}^{\prime}\right]-\widehat{\mu}_{n}|\).

According to the analysis in the LTC setting, we can directly derive

\[|\widehat{\mu}_{n}-\mathbb{E}[Y_{i}^{\prime}]| =\left|\frac{1}{n}\sum_{i}\bar{Z}_{i}-\mathbb{E}[Y_{i}^{\prime}]\right|\] \[\leq 2\alpha\cdot M\cdot\frac{e^{e}+1}{e^{e}-1}+O\left(M\cdot \frac{e^{e}+1}{e^{e}-1}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right)\quad w.p. \quad 1-\delta\]

**Step 2:** Bound the distance \(|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i }\right]|\).

\[|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\frac{1}{n}\sum_{i}\mathbb{E}\left[\bar{Y}_{i }\right]|\leq O\left(M\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right),\quad w.p. \quad 1-\delta\]

where it simply follows from Hoeffding's inequality.

**Step 3:** Bound the distance \(|\frac{1}{n}\sum_{i}\bar{Y}_{i}-\mathbb{E}\left[X_{i}\right]|\).

From the analysis in the CTL setting, we once again derive

\[|\frac{1}{n}\sum_{i\in[n]}\bar{Y}_{i}-\mathbb{E}\left[X_{i}\right]|\leq \underbrace{|\frac{1}{n}\sum_{i\in[n]}X_{i}\mathbb{I}(|X_{i}|\leq M)-\mathbb{E }\left[X_{i}\mathds{1}(|X_{i}|\leq M)\right]|}_{\mathcal{T}_{1}}+\underbrace{ \left[\mathbb{E}\left[X_{i}\mathds{1}(|X_{i}|\leq M)\right]-\mathbb{E}\left[X _{i}\right]\right]+4\alpha M}_{\mathcal{T}_{2}}+4\alpha M\]

**Step 4:** Put everything together.

Case (i): for any \(k\in(1,2)\), \(\varepsilon\in[0,1]\), any \(\delta\in(0,1)\) and any \(P\in\mathcal{P}_{k}\), we have with probability at least \(1-\delta\),

\[|\widehat{\mu}_{n}-\mu(P)| \leq O\left(\sqrt{\frac{M^{2-k}\log(1/\delta)}{n}}\right)+O\left( \frac{M\log(1/\delta)}{n}\right)+O\left(\frac{1}{M^{k-1}}\right)\] \[+O(\alpha M)+O(\frac{\alpha M}{\varepsilon})+O\left(\frac{M}{ \varepsilon}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right).\]

Thus, choosing \(M=\min\left\{\left(\frac{n}{\log(1/\delta)}\right)^{1/k},\left(\frac{ \varepsilon}{\alpha}\right)^{1/k},\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log( 1/\delta)}}\right)^{1/k}\right\}\), yields that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\log(1/\delta)}{n}\right)^{1 -1/k}+(\frac{\alpha}{\varepsilon})^{1-1/k}+\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}\right).\]

Hence, when \(n\geq\log(1/\delta)\), we have

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+(\frac{\alpha}{\varepsilon})^{1-1/k}\right).\]

Case (ii): for any \(k\geq 2\), \(\varepsilon\in[0,1]\), any \(\delta\in(0,1)\) and any \(P\in\mathcal{P}_{k}\), we have with probability at least \(1-\delta\),

\[|\widehat{\mu}_{n}-\mu(P)| \leq O\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)+O\left(\frac{M \log(1/\delta)}{n}\right)+O\left(\frac{1}{M^{k-1}}\right)\] \[+O(\alpha M)+O(\frac{\alpha M}{\varepsilon})+O\left(\frac{M}{ \varepsilon}\cdot\sqrt{\frac{\log(1/\delta)}{n}}\right).\]

Thus, choosing \(M=\min\left\{\left(\frac{n}{\log(1/\delta)}\right)^{1/k},\left(\frac{ \varepsilon}{\alpha}\right)^{1/k},\left(\frac{\varepsilon\sqrt{n}}{\sqrt{\log( 1/\delta)}}\right)^{1/k}\right\}\), yields that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\sqrt{\frac{\log(1/\delta)}{n}}+\left( \frac{\log(1/\delta)}{n}\right)^{1-1/k}+(\frac{\alpha}{\varepsilon})^{1-1/k} +\left(\frac{\sqrt{\log(1/\delta)}}{\varepsilon\sqrt{n}}\right)^{1-1/k}\right).\]

Hence, when \(n\geq\log(1/\delta)\), we have

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+(\frac{\alpha}{\varepsilon})^{1-1/k} \right).\]

Finally, combining the above two cases, we see that when \(n\geq\log(1/\delta)\), for any \(k>1\), it suffices to choose \(M=\min\left\{\left(\frac{\varepsilon}{\alpha}\right)^{1/k},\left(\frac{ \varepsilon\sqrt{n}}{\sqrt{\log(1/\delta)}}\right)^{1/k}\right\}\) and obtain that

\[|\widehat{\mu}_{n}-\mu(P)|\leq O\left(\left(\frac{\sqrt{\log(1/\delta)}}{ \varepsilon\sqrt{n}}\right)^{1-1/k}+(\frac{\alpha}{\varepsilon})^{1-1/k}\right).\]Proof of Proposition 3

Proof.: As in the section for mean estimation, we first focus on the LTC setting and divide the lower bound proof into two steps.

**Step 1: Without corruption.** In this case, we aim to establish the second term in the lower bound. We consider the first MAB instance \(I\) as follows. Let \(\gamma>0\) be determined later and

\[P_{1}(X=1/\gamma)=1/2\cdot\gamma^{k},\quad P_{1}(X=0)=1-1/2\cdot \gamma^{k}\] \[P_{a}(X=1/\gamma)=1/4\cdot\gamma^{k},\quad P_{a}(X=0)=1-1/4\cdot \gamma^{k}.\quad\forall a\neq 1. \tag{7}\]

Thus, one can see that \(I\in\text{MAB}(k)\) for a proper choice of \(\gamma\) and arm \(1\) is the optimal arm for instance \(I\). We let \(M_{a}\) be the induced marginal distribution of \(P_{a}\) via any \(\varepsilon\)-LDP channel and \(\mathbb{E}_{I}\left[\cdot\right]\) denote the expectation over \(\mathbb{P}_{I}\), which is over the randomness in the marginal distributions \(\{M_{a}\}_{a\in[K]}\) and policy \(\pi\).

Then, we construct a "coupled" instance \(I^{\prime}\) of \(I\) as follows. Let \(i=\operatorname*{argmin}_{j>1}\mathbb{E}_{I}\left[N_{j}(T)\right]\), i.e., the arm between \(a_{2}\) and \(a_{K}\) that has the minimum number of pulls under instance \(I\). Define the second instance \(I^{\prime}\) that only differs in the distribution for arm \(i\) compared to instance \(I\)

\[P_{i}(X=1/\gamma)=3/4\cdot\gamma^{k},\quad P_{i}(X=0)=1-3/4\cdot \gamma^{k}. \tag{8}\]

Thus, \(I^{\prime}\in\text{MAB}(k)\) and arm \(i\) is the optimal arm for instance \(I^{\prime}\). By definition, we also have \(\mathbb{E}_{I}\left[N_{i}(T)\right]\leq T/(K-1)\).

For any instance \(I\) and policy \(\pi\), we let \(\mathcal{R}_{T}(\pi,I)\) be its corresponding expected regret. Then, by standard argument and noting that the mean gap is \(\Delta:=1/4\cdot\gamma^{k-1}\), we have

\[\mathcal{R}_{T}(\pi,I)+\mathcal{R}_{T}(\pi,I^{\prime}) \geq\frac{T}{2}\cdot\Delta\cdot\left(\mathbb{P}_{I}\left[N_{1}(T) \leq T/2\right]+\mathbb{P}_{I^{\prime}}\left[N_{1}(T)\geq T/2\right]\right)\] \[\overset{(a)}{\geq}\frac{T\Delta}{4}\exp(-\mathrm{KL}\left( \mathbb{P}_{I}\|\,\mathbb{P}_{I^{\prime}}\right))\] \[\overset{(b)}{=}\frac{T\Delta}{4}\exp(-\mathbb{E}_{I}\left[N_{i} (T)\right]\cdot\mathrm{KL}\left(\,M_{i}\|\,M^{\prime}_{i}\right))\] \[\overset{(c)}{\geq}\frac{T\Delta}{4}\exp(-\mathbb{E}_{I}\left[N_ {i}(T)\right]\cdot 4(e^{\varepsilon}-1)^{2}\cdot(\mathrm{TV}\left(P_{i},P^{ \prime}_{i}\right))^{2})\] \[\overset{(d)}{\geq}\frac{T\Delta}{4}\exp\left(-\frac{T}{K-1}\cdot 4 (e^{\varepsilon}-1)^{2}\cdot(\mathrm{TV}\left(P_{i},P^{\prime}_{i}\right))^{2}\right)\] \[\overset{(e)}{=}\frac{T\Delta}{4}\exp\left(-\frac{T}{K-1}\cdot 4 (e^{\varepsilon}-1)^{2}\cdot\frac{\gamma^{2k}}{4}\right)\]

where (a) holds by Bretagnolle-Huber inequality; (b) follows from chain rule of KL divergence; (c) holds by Theorem 1 in Duchi et al. (2018); (d) is true since \(\mathbb{E}_{I}\left[N_{i}(T)\right]\leq T/(K-1)\); (e) holds by definition of TV distance.

Thus, putting everything together and noting that for \(\varepsilon\in[0,1]\), \(e^{\varepsilon}-1\leq 2\varepsilon\), yields that

\[\mathcal{R}_{T}(\pi,I)+\mathcal{R}_{T}(\pi,I^{\prime})\geq\frac{T\Delta}{4} \exp\left(-4\frac{\varepsilon^{2}T\gamma^{2k}}{K-1}\right).\]

Thus, suppose \(T\geq K/\varepsilon^{2}\) and choosing \(\gamma=(K/(\varepsilon^{2}T))^{1/2k}\), one can check that all the required conditions on \(\gamma\) are satisfied and we finally have that \(\max\{\mathcal{R}_{T}(\pi,I),\mathcal{R}_{T}(\pi,I^{\prime})\}\geq\Omega\left(T \gamma^{k-1}\right)=\Omega\left(T\frac{k+1}{2k}\left(\frac{K}{\varepsilon^{2}} \right)^{\frac{k-1}{2k}}\right)\).

**Step 2: Corruption part.** In this case, we aim to establish the first term in the lower bound.

This part basically shares the same argument as before for mean estimation. Note that the only difference between \(I\) and \(I^{\prime}\) is the distribution for arm \(i\). Then, we apply the same argument as in the proof of Proposition 1 to \(P_{i}\) and \(P^{\prime}_{i}\). Hence, we have that there exists Huber corruptions so that one cannot distinguish between \(P_{i}\) and \(P^{\prime}_{i}\), and hence \(I\) and \(I^{\prime}\). As a result, the total expected regret is \(\Omega\left(T\gamma^{k-1}\right)=\Omega(T(\alpha/\varepsilon)^{1-1/k})\).

Finally, for the CTL setting, the first step is the same and second step only differs in that there is no "contraction" effect as in the proof of Proposition 1.

Proof of Proposition 4

Proof.: Let us start with the LTC case. We divide the set of all sub-optimal arms \(\mathcal{G}\) into two groups \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}:=\mathcal{G}\setminus\mathcal{G}_{1}\), where \(\mathcal{G}_{1}=\{a\in[K]\setminus a^{*}:c^{\prime}\left(\frac{\alpha}{ \varepsilon}\right)^{1-1/k}<\frac{1}{2}\Delta_{a}\}\) for some universal constant \(c^{\prime}\) chosen later. Hence, \(\mathcal{G}_{2}=\{a\in[K]\setminus a^{*}:c^{\prime}\left(\frac{\alpha}{ \varepsilon}\right)^{1-1/k}\geq\frac{1}{2}\Delta_{a}\}\), which implies that the total expected regret from suboptimal arms in \(\mathcal{G}_{2}\) is upper bounded by \(O\left(T\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\right)\). Thus, it remains to bound the total expected regret of pulling suboptimal arms in \(\mathcal{G}_{1}\). To this end, for each \(i\in\mathcal{G}_{1}\), we aim to show that

\[\mathbb{E}\left[N_{i}(T)\right]\leq O\left(\frac{\log T}{ \varepsilon^{2}(\Delta_{i})^{\frac{2k}{k-1}}}+\frac{\log T}{\alpha}\right). \tag{9}\]

Let us first assume (9) holds and see how we can arrive at our claimed upper bound. By the definition of expected regret, we have

\[R(k,\varepsilon,\alpha,T) =\sum_{i\in\mathcal{G}_{1}}\Delta_{i}\mathbb{E}\left[N_{i}(T) \right]+\sum_{i\in\mathcal{G}_{2}}\Delta_{i}\mathbb{E}\left[N_{i}(T)\right]\] \[\leq\sum_{i\in\mathcal{G}_{1}}\Delta_{i}\mathbb{E}\left[N_{i}(T) \right]+O\left(T\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\right),\]

where inequality holds by the definition of \(\mathcal{G}_{2}\). It remains to translate the first term into a problem-independent one. To this end, we further divide the arms in \(\mathcal{G}_{1}\) into two groups: one group consists of all arms that satisfy \(\Delta_{i}<\eta\) for some constant \(\eta>0\) and another one contains all arms that satisfy \(\Delta_{i}\geq\eta\). Thus, by (9), we have

\[\sum_{i\in\mathcal{G}_{1}}\Delta_{i}\mathbb{E}\left[N_{i}(T)\right] \leq\eta T+O\left(\frac{K\log T}{\varepsilon^{2}\eta^{\frac{k+1}{k-1}}}+\frac {K\log T}{\alpha}\right).\]

Choosing \(\eta=\left(\frac{K\log T}{\varepsilon^{2}T}\right)^{\frac{k-1}{2k}}\), yields that the total expected regret satisfies

\[R(k,\varepsilon,\alpha,T)\leq O\left(\left(\frac{K\log T}{ \varepsilon^{2}}\right)^{\frac{k-1}{2k}}T^{\frac{k+1}{2k}}+\frac{K\log T}{ \alpha}+T\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\right).\]

Finally, for very small \(\alpha\), one can replace it with its upper bound \(\bar{\alpha}\) to optimize the regret.

It remains to establish (9). First note that \(O(\log T/\alpha)\) basically follows from the burn-in period. Thus, we only need to bound the total number of pulls after the burn-in period. We denote by \(N_{i}^{\prime}(t)\) the total number of by time \(t\) after the burn-in period, i.e., it is equal to \(N_{i}(t)\) minus the total number of burn-in plays of arm \(i\). In the following, we will show that

\[\mathbb{E}\left[N_{i}^{\prime}(T)\right]\leq C_{1}\frac{\log T}{ \varepsilon^{2}(\Delta_{i})^{\frac{2k}{k-1}}}+C_{2}, \tag{10}\]

for some constants \(C_{1}\) and \(C_{2}\).

To this end, for \(t\) that is after the burn-in period of arm \(i\in\mathcal{G}_{1}\), if \(a_{t}=i\), then one of the following must be true:

\[\text{UCB}_{a^{*}}(t) \leq\mu(P_{a^{*}}) \tag{11}\] \[\widehat{\mu}_{i,N_{i}(t)}>\mu(P_{i})+c\left(\frac{\alpha}{ \varepsilon}\right)^{1-1/k}+c\left(\frac{1}{\varepsilon}\sqrt{\frac{\log(t^{ 4})}{N_{i}(t)}}\right)^{1-1/k}\] (12) \[N_{i}^{\prime}(t)<C\frac{\log T}{\varepsilon^{2}(\Delta_{i})^{ \frac{2k}{k-1}}} \tag{13}\]This is because if all three are not true, then we have

UCB\[{}_{a}.(t) >\mu(P_{a^{*}})\] \[=\mu(P_{i})+\Delta_{i}\] \[\stackrel{{(a)}}{{\geq}}\mu(P_{i})+\frac{1}{2}\Delta_{ i}+c^{\prime}\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\] \[\stackrel{{(b)}}{{\geq}}\mu(P_{i})+2c\left(\frac{1} {\varepsilon}\sqrt{\frac{\log(t^{4})}{N_{i}^{\prime}(t)}}\right)^{1-1/k}+c^{ \prime}\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\] \[\stackrel{{(c)}}{{\geq}}\mu(P_{i})+2c\left(\frac{1} {\varepsilon}\sqrt{\frac{\log(t^{4})}{N_{i}(t)}}\right)^{1-1/k}+c^{\prime} \left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\] \[\stackrel{{(d)}}{{\geq}}\widehat{\mu}_{i,N_{i}(t)}+c \left(\frac{1}{\varepsilon}\sqrt{\frac{\log(t^{4})}{N_{i}(t)}}\right)^{1-1/k} +c\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\] \[=\text{UCB}_{i}(t)\]

where (a) holds by the fact that \(i\in\mathcal{G}_{1}\); (b) holds by choosing a large constant \(C\) in (13); (c) is true since \(N_{i}(t)>N_{i}^{\prime}(t)\); (d) holds by the inverse direction of (12) and choosing \(c^{\prime}=2c\).

Let \(t^{\prime}\) be the time just after the burn-in period, then we have

\[\mathbb{E}\left[N_{i}^{\prime}(T)\right]=\mathbb{E}\left[\sum_{t \geq t^{\prime}}\mathds{1}(a_{t}=i)\right] \leq C\frac{\log T}{\varepsilon^{2}(\Delta_{i})^{\frac{2k}{k-1}}} +\sum_{t\geq t^{\prime}}\mathbb{E}\left[\mathds{1}(a_{t}=i\text{ and (\ref{eq:t-1}) is false})\right]\] \[\stackrel{{(a)}}{{\leq}}C\frac{\log T}{\varepsilon^{ 2}(\Delta_{i})^{\frac{2k}{k-1}}} +\sum_{t\geq t^{\prime}}\mathbb{E}\left[\mathds{1}((\ref{eq:t-1})\text{ is true or (\ref{eq:t-1}) is true})\right]\]

where (a) holds by the above claim, i.e., if \(a_{t}=i\) and (13) is false, then one of (11) and (12) must be true. Then, by our mean concentration result and union bounds, we can upper bound the second term above as

\[\mathbb{E}\left[\mathds{1}((\ref{eq:t-1})\text{ is true or (\ref{eq:t-1}) is true})\right]\leq 2\sum_{s=1}^{t}\frac{1}{t^{4}}=\frac{2}{t^{3}}.\]

Putting them together, we have established (10), hence the result. The proof for CTL setting is essentially the same with the only difference in the definition of \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\). 

## Appendix J Proof of Proposition 5

Proof.: **Without corruption.** We consider two instances in \(\operatorname{MAB}(\beta^{\star},k)\). In particular, we consider two-arm MABs \(I\) and \(I^{\prime}\):

\[\text{For }I:\mu_{1}^{I}:=\mu(P_{1}^{I})=1/2\cdot\gamma^{k-1},\;\mu_{ 2}^{I}:=\mu(P_{2}^{I})=1/4\cdot\gamma^{k-1}\] \[\text{For }I^{\prime}:\mu_{1}^{I^{\prime}}:=\mu(P_{1}^{I})=1/2 \cdot\gamma^{k-1},\;\mu_{2}^{I^{\prime}}:=\mu(P_{2}^{I})=3/4\cdot\gamma^{k-1} \tag{14}\]

These distributions can be constructed in the same way as in the proof of Proposition 3 (cf. (7)). Moreover, for the behavior policy \(\pi\), we have \(\pi(2)=1/\beta^{\star}\) and \(\pi(1)=1-1/\beta^{\star}\). We now verify that both \((\pi,\mu^{I})\) and \((\pi,\mu^{I^{\prime}})\) are in \(\operatorname{MAB}(\beta^{\star},k)\). By construction, each distribution is belonging to \(\mathcal{P}_{k}\). It remains to verify that \(1/\pi(a^{*})\leq\beta^{\star}\). For \(I^{\prime}\), we have \(1/\pi(2)=\beta^{\star}\). And for \(I\), we have \(1/\pi(1)=1/(1-1/\beta^{\star})\leq\beta^{\star}\) when \(\beta^{\star}\geq 2\).

Now, we proceed to apply classic Le Cam's method. Let loss/sub-optimality of any final chosen arm \(\widehat{a}\in\{1,2\}\) under \(I\) and \(I^{\prime}\) be \(\ell(\widehat{a};I)\), \(\ell(\widehat{a};I^{\prime})\). Then, by our construction, we have

\[\ell(\widehat{a};I)+\ell(\widehat{a};I^{\prime})\geq 1/4\cdot\gamma^{k-1}.\]Thus, by Le Cam's method and Bretagnolle-Huber inequality, we have

\[\text{SubOpt}^{*}(\beta^{*},k,\varepsilon,\alpha,N)\geq\frac{\gamma^{k-1}}{16} \exp\left(-\mathrm{KL}\left(M_{\pi}^{I}\big{\|}\,M_{\pi}^{I^{\prime}}\right) \right),\]

where \(\mathrm{KL}\left(M_{\pi}^{I}\big{\|}\,M_{\pi}^{I^{\prime}}\right)\) is the private KL divergence between two MAB instances. By chain rule of KL divergence and Theorem 1 in Duchi et al. (2018), we have

\[\mathrm{KL}\left(M_{\pi}^{I}\big{\|}\,M_{\pi}^{I^{\prime}}\right)\leq\frac{N}{ \beta^{*}}4(e^{\varepsilon}-1)^{2}\left(\mathrm{TV}\left(P_{2}^{I},P_{2}^{I^{ \prime}}\right)\right)^{2}.\]

Thus, noting that for \(\varepsilon\in[0,1]\), \(e^{\varepsilon}-1\leq 2\varepsilon\) and \(\left(\mathrm{TV}\left(P_{2}^{I},P_{2}^{I^{\prime}}\right)\right)^{2}=\frac{ \gamma^{2k}}{4}\), we have that

\[\text{SubOpt}^{*}(\beta^{*},k,\varepsilon,\alpha,N)\geq\frac{\gamma^{k-1}}{16 }\exp\left(-\frac{\varepsilon^{2}N\gamma^{2k}}{4\beta^{*}}\right).\]

Finally, for a large enough \(N\), choosing \(\gamma=(\beta^{*}/(\varepsilon^{2}N))^{1/2k}\), yields that

\[\text{SubOpt}^{*}(\beta^{*},k,\varepsilon,\alpha,N)\geq\Omega\left(\left(\frac {1}{\varepsilon}\sqrt{\frac{\beta^{*}}{N}}\right)^{1-1/k}\right).\]

**Corruption part.** By our construction (cf. (14) (7), (8)) we have that \(\mathrm{TV}\left(P_{2}^{I},P_{2}^{I^{\prime}}\right)=\frac{\gamma^{k}}{2}\). Then, a similar idea as in the proof of Proposition 1 applies here. That is, for the LTC setting, by the contraction of LDP, we can set \(\gamma^{k}=\Theta(\frac{\alpha}{\varepsilon})\) so that \(\mathrm{TV}\left(M_{2}^{I},M_{2}^{I^{\prime}}\right)\leq\alpha\). Thus, one cannot distinguish \(I\) and \(I^{\prime}\) under \(\alpha\)-Huber model. Thus, one has to incur a sub-optimality gap as \(\Omega(\gamma^{k})=\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}\). In contrast, due to no contraction by LDP first, one can only set \(\gamma^{k}=\Theta(\alpha)\), which leads to the final result. 

## Appendix K Proof of Proposition 6

Proof.: We will focus on the LTC case, since the CTL case is nearly the same with a minor change in the confidence bound. Let \(\mathcal{E}=\mathcal{E}_{1}\cap\mathcal{E}_{2}\) where

\[\mathcal{E}_{1}:=\{\forall a\in[K],|\widehat{r}(a)-r(a)|\leq b(a)\}\]

\[\mathcal{E}_{2}:=\{N(a^{*})\geq\frac{1}{2}N\pi(a^{*})\}.\]

Let us first assume that \(\mathbb{P}\left[\mathcal{E}\right]\geq 1-2\delta\) and see how we can prove the final result. Then, we will establish this high-probability event in the end. Hence, condition the event \(\mathcal{E}\) and define LCB(a)\(:=\widehat{r}(a)-b(a)\), we have

\[r(a^{*})-r(\widehat{a}) =r(a^{*})-\text{LCB}(a^{*})+\text{LCB}(a^{*})-\text{LCB}(\widehat {a})+\text{LCB}(\widehat{a})-r(\widehat{a})\] \[\leq 2b(a^{*})\] \[\leq 2c\left(\frac{\alpha}{\varepsilon}\right)^{1-1/k}+2c\left( \frac{1}{\varepsilon}\sqrt{\frac{\log(2K/\delta)}{N_{a^{*}}}}\right)^{1-1/k}.\]

Then, by the definition of \(\beta^{*}\) and \(\mathcal{E}\), we can further lower bound \(N_{a^{*}}\) by \(\frac{N}{2\beta^{*}}\). Then, by the bounded mean of each arm, and choosing \(\delta=1/N\), we have the claimed expected sub-optimality result.

It remains to bound the probability of \(\mathcal{E}\). For \(\mathcal{E}_{2}\), by standard Chernoff bound, we have \(\mathbb{P}\left[\mathcal{E}_{2}\right]\geq 1-\delta\) when \(N\geq 8\beta^{*}\log(1/\delta)\). For \(\mathcal{E}_{1}\), we have the following argument. For any arm \(a\) such that \(N_{a}\) is larger than the burn-in threshold, the concentration in \(\mathcal{E}_{1}\) follows from our high-probability mean estimation result. For all other arms, by construction and the condition that all arms have mean between \([-1,1]\), we have

\[\widehat{r}(a)-b(a)=-1\leq r(a)\leq\widehat{r}(a)+b(a)=1,\]

which enables us to establish our claim \(\mathbb{P}\left[\mathcal{E}\right]\geq 1-2\delta\).