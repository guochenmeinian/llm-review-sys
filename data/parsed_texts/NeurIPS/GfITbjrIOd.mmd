# Human-Aligned Calibration for AI-Assisted Decision Making

Nina L. Corvelo Benz

Max Planck Institute for Software Systems

ETH Zurich

ninacobe@mpi-sws.org &Manuel Gomez Rodriguez

Max Planck Institute for Software Systems

manuel@mpi-sws.org

###### Abstract

Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values--an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. However, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. Further, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for alignment. Experiments on four different AI-assisted decision making tasks where a classifier provides decision support to real human experts validate our theoretical results and suggest that alignment may lead to better decisions.

## 1 Introduction

In recent years, there has been an increasing excitement on the potential of machine learning models to improve decision making in a variety of high-stakes domains such as medicine, education or criminal justice [1, 2, 3]. One of the main focus has been binary classification tasks, where a classifier helps a decision maker by predicting a binary label of interest using a set of observable features [4, 5, 6, 7]. For example, in medical treatment, the classifier may help a doctor by predicting whether a patient may benefit from a treatment. In college admissions, it may help an admissions committee by predicting whether a candidate may successfully complete an undergraduate program. In loan decisions, it may help a bank by predicting whether a prospective customer may default on a loan. In all these scenarios, the decision maker--the doctor, the committee or the bank--aim to use these predictions, together with their own predictions, to take good decisions that maximize a given utility function. In this context, since the predictions are unlikely to always match the truth, it has been widely agreed that the classifier should also provide a confidence value together with each prediction [8, 9].

While the conventional wisdom is that the confidence value should be a well calibrated estimate of the probability that the predicted label matches the true label [10, 11, 12, 13, 14, 15, 16], multiple lines of empirical evidence have recently shown that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values [17, 18, 19]. Therein, Vodrahali et al. [17] have shown that, in certain scenarios, decision makers take better decisions using uncalibrated probability estimates rather than calibrated ones. However, a theoretical framework explaining this puzzling observation has been missing and it is yet unclear what properties we should be looking for to guarantee that confidence values are useful for AI-assisted decision making. In our work, we aim to bridge this gap.

**Our contributions.** We start by formally characterizing AI-assisted decision making using a structural causal model (SCM) [20], as seen in Figure 1. Building upon this characterization, we first argue that, if a decision maker is rational, the level of trust she places on predictions will be monotone on the confidence values--she will place more (less) trust on predictions with higher (lower) confidence values. Then, we show that, for a broad class of utility functions, there are data distributions for which a rational decision maker can never take optimal decisions using calibrated estimates of the probability that the predicted label matches the true label as confidence values. However, we further show that, if the confidence values a decision maker uses satisfy a natural alignment property with respect to the confidence she has on her own predictions, which we refer to as human-alignment, then the decision maker can both be rational and take optimal decisions. In addition, we demonstrate that human-alignment can be achieved via multicalibration [11], a statistical notion introduced in the context of algorithmic fairness. In particular, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for human-alignment. Finally, we validate our theoretical framework using real data from four different AI-assisted decision making tasks where a classifier provides decision support to human decision makers in four different binary classification tasks. Our results suggest that, comparing across tasks, classifiers providing human-aligned confidence values facilitate better decisions than classifiers providing confidence values that are not human-aligned. Moreover, our results also suggest that rational decision makers' trust level increases monotonically with the classifier's provided confidence.

**Further related work.** Our work builds upon a rapidly increasing literature on AI-assisted decision making (refer to Lai et al. [21] for a recent review). More specifically, it is motivated by several empirical studies showing that decision makers have difficulties at modulating trust using confidence values [17, 18, 19], as discussed previously. In this context, it is also worth noting that other empirical studies have analyzed how other factors such as model explanations and accuracy modulate trust [22, 23, 24, 25, 26]. However, except for a very recent notable exception [27], theoretical frameworks, which could be used to better understand the mixed findings found by these empirical studies, have been missing. More broadly, our work also relates to a flurry of recent work on reinforcement learning with human feedback [28, 29, 30], which aims to better align the outputs of large language models (LLMs) with human preferences. However, our formulation is fundamentally different and our technical contributions are orthogonal to theirs.

## 2 A Causal Model of AI-Assisted Decision Making

We consider an AI-assisted decision making process where, for each realization of the process, a decision maker first observes a set of features \((x,v)\in\mathcal{X}\times\mathcal{V}\), then takes a binary decision \(t\in\{0,1\}\) informed by a classifier's prediction \(\hat{y}=\operatorname*{argmax}_{y}f_{y}(x)\), as well as confidence \(f_{\hat{y}}(x)\in[0,1]\), of a binary label of interest \(y\in\{0,1\}\), and finally receives a utility \(u(t,y)\in\mathbb{R}\). Such an AI-assisted decision making process fits a variety of real-world applications. For example, in medical treatment, the features \((x,v)\) may comprise multiple sources of information regarding a patient's health1, the label \(y\) may indicate whether a patient would benefit from a specific treatment, the decision \(t\) may indicate whether the doctor applies the specific treatment to the patient, and the utility \(u(t,y)\) may quantify the trade-off between health benefit to the patient and economic cost to the decision maker.

Footnote 1: Our formulation allows for a subset of the features \(v\) to be available only to the decision maker but not to the classifier.

In what follows, rather than working with both \(\hat{y}\) and \(f_{\hat{y}}(x)\), we will work with just \(b=f_{1}(x)\), which we will refer to as classifier's confidence, without loss of generality2. Moreover, we will assume that the utility \(u(t,y)\) is greater if the value of \(t\) and \(y\) coincide, _i.e._,

\[u(1,1)>u(1,0),\quad u(1,1)>u(0,1),\quad u(0,0)>u(1,0),\quad\text{and}\quad u(0,0 )\geq u(0,1), \tag{1}\]

a condition that we think it is natural under an appropriate choice of label and decision values. For example, in medical diagnosis, if \(t=1\) means the patient is tested early for a disease and \(y=1\) means the patient suffers the disease, the above condition implies that the utility of either testing a patient who suffers the disease or not testing a patient who does not suffer the disease are greater than the utility of either not testing a patient who suffers the disease or testing a patient who does not suffer the disease. In condition 1, we allow for a non-strict inequality \(u(0,0)\geq u(0,1)\) because, in settings in which the label \(Y\) is only realized whenever the decision \(t=1\) (_e.g._, in our previous example on medical treatment, we can only observe if a treatment is eventually beneficial or not if the patient is treated), it has been argued that, whenever \(t=0\), any choice of utility must be independent of the label value [4, 5, 6], _i.e._, \(u(0,0)=u(0,1)=u(0)\).

Next, we characterize the above AI-assisted decision making process using a structural causal model (SCM) [20], which we denote as \(\mathcal{M}\). The SCM \(\mathcal{M}\) is defined by a set of assignments, which entail a distribution \(P^{\mathcal{M}}\) and divide naturally into two subsets. One subset comprises the features and the label3, _i.e._,

Footnote 3: We denote random variables with capital letters and realizations of random variables with lower case letters.

\[X=f_{X}(D)\quad V=f_{V}(D)\quad\text{and}\quad Y=f_{Y}(D), \tag{2}\]

where \(D\) is an independent exogenous random variable, often called exogenous noise, characterizing the data generating process and \(f_{X}\), \(f_{V}\) and \(f_{Y}\) are given functions4. The second subset comprises the decision maker and the classifier, _i.e._,

Footnote 4: Our model allows both for causal and anticausal features [31].

\[H=f_{H}(X,V,Q),\quad B=f_{B}(X,H),\quad T=\pi(H,B,W)\quad\text{and}\quad U=u(T,Y), \tag{3}\]

where \(f_{H}\) and \(f_{B}\) are given functions, which determine the decision maker's confidence \(H\) and classifier's confidence \(B\) that the value of the label of interest is \(Y=1\), \(\pi\) is a given AI-assisted decision policy, which determines the decision maker's decision \(T\), \(u\) is a given utility function, which determines the utility \(U\), and \(Q\) and \(W\) are independent exogenous variables modeling the decision maker's individual characteristics influencing her own confidence \(H\) and her decision \(T\), respectively. By distinguishing both sources of noise, we allow for the presence of uncertainty on the decision \(T\) even after conditioning on fixed confidence values \(h\) and \(b\). This accounts for the fact that, in reality, a decision maker may take different decisions \(T\) for instances with the same confidence values \(h\) and \(b\). For example, in medical treatment, for two different patients with the same confidence \(h\) and \(b\), a doctor's decision may differ due to limited resources.

In our SCM \(\mathcal{M}\), the decision maker's confidence \(H\) refers to the confidence the decision maker has that the label \(Y=1\)_before_ observing the classifier's confidence \(B\). Moreover, following previous behavioral studies showing that human's confidence \(H\) is discretized in a few distinct levels [32, 33], we assume \(H\) takes values \(h\) from a totally ordered discrete set \(\mathcal{H}\). We say that the decision maker's

Figure 1: Our structural causal model \(\mathcal{M}\). Orange circles represent endogenous random variables and blue boxes represent exogenous random variables. The value of each endogenous variable is given by a function of the values of its ancestors in the structural causal model, as defined by Eqs. 2 and 3. The value of each exogenous variable is sampled independently from a given distribution.

confidence \(f_{H}\) is monotone (with respect to the probability distribution \(P(Y=1)\)) if, for all \(h,h^{\prime}\in\mathcal{H}\) such that \(h\leq h^{\prime}\), it holds that \(P(Y=1\;\mid\;H=h)\leq P(Y=1\;\mid\;H=h^{\prime})\). Further, we allow the classifier's confidence \(B\) to depend on the decision maker's confidence \(H\) because this will be necessary to achieve human-alignment via multicalibration in Section 5. However, our negative result in Section 3 also holds if the classifier's confidence \(f_{B}(X,H)=f_{B}(X)\) only depends on the features, as usual in most classifiers designed for AI-assisted decision making. In the remainder, we will use \(Z=(X,H)\) and denote the space of features and human confidence values as \(\mathcal{Z}=\mathcal{X}\times\mathcal{H}\). Figure 1 shows a visual representation of our SCM \(\mathcal{M}\).

Under this characterization, we argue that, if a rational decision maker has decided \(t\) under confidence values \(b\) and \(h\), then she would have decided \(t^{\prime}\geq t\) had the confidence values been \(b^{\prime}\geq b\) and \(h^{\prime}\geq h\) while holding "everything else fixed" [20]. For example, in medical treatment, assume a doctor's and a classifier's confidence that a patient would benefit from treatment is \(b=h=0.7\) and the doctor decides to treat the patient, then we argue that, if the doctor is rational, she would have treated the patient had the doctor's and the classifier's confidence been \(b^{\prime}=h^{\prime}=0.8>0.7\). Further, we say that any AI-assisted decision policy \(\pi\) that satisfies this property is monotone, _i.e._,

**Definition 1** (Monotone AI-assisted decision policy).: _An AI-assisted decision policy \(\pi\) is monotone if and only if, for any \(b,b^{\prime}\in[0,1]\) and \(h,h^{\prime}\in\mathcal{H}\) such that \(b\leq b^{\prime}\) and \(h\leq h^{\prime}\), it holds that \(\pi(h,b,w)\leq\pi(h^{\prime},b^{\prime},w)\) for any \(w\sim P^{\mathcal{M}}(w)\)._

Finally, note that, under any monotone AI-assisted decision policy, it trivially follows that

\[\mathbb{E}[T\;\mid\;H=h,B=b]\leq\mathbb{E}[T\;\mid\;H=h^{\prime},B=b^{\prime}], \tag{4}\]

where the expectation is over the uncertainty on the decision maker's individual characteristics and the data generating process.

## 3 Impossibility of AI-Assisted Decision Making Under Calibration

In AI-assisted decision making, classifiers are usually demanded to provide calibrated confidence values [10, 11, 12, 13, 14, 15, 16]. A confidence function \(f_{B}:\mathcal{Z}\rightarrow[0,1]\) is said to be perfectly calibrated if, for any \(b\in[0,1]\), it holds that \(P(Y=1\;\mid\;f_{B}(Z)=b)=b\). Unfortunately, using finite amounts of (calibration) data, one can only hope to construct approximately calibrated confidence functions. There exist many different notions of approximate calibration, which have been proposed over the years. Here, for concreteness, we adopt the notion of \(\alpha\)-calibration5 introduced by Hebert-Johnson et al. [11], however, our theoretical results can be easily adapted to other notions of approximate calibration6.

Footnote 5: Note that, if \(\alpha=0\) and \(\mathcal{S}=\mathcal{Z}\), the confidence function \(f\) is perfectly calibrated.

**Definition 2** (Calibration).: _A confidence function \(f_{B}:\mathcal{Z}\rightarrow[0,1]\) satisfies \(\alpha\)-calibration with respect to \(\mathcal{S}\subseteq\mathcal{Z}\) if there exists some \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\), with \(|\mathcal{S}^{\prime}|\geq(1-\alpha)|\mathcal{S}|\), such that, for any \(b\in[0,1]\), it holds that_

\[|P(Y=1\;\mid\;f_{B}(Z)=b,Z\in\mathcal{S}^{\prime})-b|\leq\alpha, \tag{5}\]

If the decision maker's decision \(T\) only depends on the classifier's confidence \(B\), _i.e._, \(\pi(H,B,W)=\pi(B)\) and \(f_{B}\) satisfies \(\alpha\)-calibration with respect to \(\mathcal{Z}\), then, it readily follows from previous work that, for any utility function that satisfies Eq. 1, a simple monotone AI-assisted decision policy \(\pi_{B}^{*}\) that takes decisions by thresholding the confidence values is optimal [4, 5, 6, 7], _i.e._, \(\pi_{B}^{*}=\operatorname*{argmax}_{\pi\in\Pi(B)}\mathbb{E}_{\pi}[u(T,Y)]\), where the expectation is with respect to the probability distribution \(P^{\mathcal{M}}\) and \(\Pi(B)\) denotes the class of AI-assisted decision policies using \(B\). However, one of the main motivations to favor AI-assisted decision making over fully automated decision making is that the decision maker may have access to additional features \(V\) and may like to weigh the classifier's confidence \(B\) against her own confidence \(H\). Hence, the decision maker may seek for the optimal decision policy \(\pi^{*}\) over the class \(\Pi(H,B)\) of AI-assisted decision policies using \(H\) and \(B\), _i.e._, \(\pi^{*}=\operatorname*{argmax}_{\pi\in\Pi(H,B)}\mathbb{E}_{\pi}[u(T,Y)]\), since it may offer greater expected utility than \(\pi_{B}^{*}\).

Unfortunately, the following negative result shows that, in general, a rational decision maker may be unable to discover such an optimal decision policy \(\pi^{*}\) using (perfectly) calibrated confidence values and this is true even if \(f_{H}\) is monotone:

**Theorem 3**.: _There exist (infinitely many) AI-assisted decision making processes \(\mathcal{M}\) satisfying Eqs. 2 and 3, with utility functions \(u(T,Y)\) satisfying Eq. 1, such that \(f_{B}\) is perfectly calibrated and \(f_{H}\) is monotone but any AI-assisted decision policy \(\pi\in\Pi(H,B)\) that satisfies monotonicity is suboptimal, i.e., \(\mathbb{E}_{\pi}[u(T,Y)]<\mathbb{E}_{\pi^{*}}[u(T,Y)]\)._

In the proof of the above result in Appendix A.2, we show that there always exist a perfectly calibrated \(f_{B}(Z)=f_{B}(X,H)\) that depends on both \(X\) and \(H\) for which any monotone AI-assisted decision policy is suboptimal. This is due to the fact that \(f_{B}(Z)\) is calibrated on average over \(H\), however, it may not be calibrated, nor even monotone, after conditioning on a specific value \(H=h\). Further, we also show that, even if \(f_{B}(Z)=P^{\mathcal{M}}(Y=1\;\mid\;X)\) matches the true distribution of the label \(Y\) given the features \(X\), which has been typically the ultimate goal in the machine learning literature, there always exists a monotone \(f_{H}\) for which any monotone AI-assisted decision policy is suboptimal. This is due to the fact that the decision maker's confidence \(H\) can differ across instances with the same value for features \(X\) because it also depends on the features \(V\) and noise \(Q\). Hence, \(f_{H}\) may not be monotone after conditioning on a specific value \(X=x\). In both cases, when a rational decision maker compares pairs of confidence values \(h,b\) and \(h^{\prime},b^{\prime}\), the rate of positive outcomes \(Y=1\) for each pair may appear _contradictory_ with the magnitude of confidence. In what follows, we will show that, if \(f_{B}\) satisfies a natural alignment property with respect to \(f_{H}\), which we refer to as human-alignment, there always exists an optimal AI-assisted decision policy that is monotone.

## 4 AI-Assisted Decision Making Under Human-Aligned Calibration

Intuitively, to avoid that pairs of confidence values \(B\) and \(H\) appear as contradictory to a rational decision maker, we need to make sure that, with high probability, both \(f_{B}\) and \(f_{H}\) are monotone after conditioning on specific values of \(H\) and \(B\), respectively. Next, we formalize this intuition by means of the following property, which we refer to as \(\alpha\)-alignment:

**Definition 4** (Human-alignment).: _A confidence function \(f_{B}\) satisfies \(\alpha\)-alignment with respect to a confidence function \(f_{H}\) if, for any \(h\in\mathcal{H}\), there exists some \(\tilde{\mathcal{S}}_{h}\subseteq\mathcal{S}_{h}\), with \(\mathcal{S}_{h}=\{(x,H)\in\mathcal{Z}\;\mid\;H=h\}\) and \(|\tilde{\mathcal{S}}_{h}|\geq(1-\alpha/2)|\mathcal{S}_{h}|\), such that, for any \(b^{\prime},b^{\prime\prime}\in[0,1]\) and \(h^{\prime},h^{\prime\prime}\in\mathcal{H}\) such that \(b^{\prime}\leq b^{\prime\prime}\) and \(h^{\prime}\leq h^{\prime\prime}\), it holds that_

\[P(Y=1\;\mid\;f_{B}(X,H)=b^{\prime},(X,H)\in\tilde{\mathcal{S}}_{h^{\prime}})- P(Y=1\;\mid\;f_{B}(X,H)=b^{\prime\prime},(X,H)\in\tilde{\mathcal{S}}_{h^{ \prime\prime}})\leq\alpha \tag{6}\]

The above definition just means that, if \(f_{B}\) is \(\alpha\)-aligned with respect to \(f_{H}\) then, for any \(h,h^{\prime}\in\mathcal{H}\), we can bound any violation of monotonicity by \(f_{B}\) between at least a \((1-\alpha/2)\) fraction of the subspaces of features \(\mathcal{S}_{h}\) and \(\mathcal{S}_{h^{\prime}}\). Moreover, note that, if \(f_{B}\) is \(0\)-aligned with respect to \(f_{H}\), then there are no violations of monotonicity, _i.e._, \(P(Y=1\;\mid\;f_{B}(X,H)=b^{\prime},(X,H)\in\tilde{\mathcal{S}}_{h^{\prime}}) \leq P(Y=1\;\mid\;f_{B}(X,H)=b^{\prime\prime},(X,H)\in\tilde{\mathcal{S}}_{h^ {\prime\prime}})\), and we say that \(f_{B}\) is perfectly aligned with respect to \(f_{H}\).

Given the above definition, we are now ready to state our main result, which shows that human-alignment allows for AI-assisted decision policies that satisfy monotonicity and (near-)optimality:

**Theorem 5**.: _Let \(\mathcal{M}\) be any AI-assisted decision making process satisfying Eqs. 2 and 3, with an utility function \(u(T,Y)\) satisfying Eq. 1 If \(f_{B}\) satisfies \(\alpha\)-alignment w.r.t. \(f_{H}\), then there always exists an AI-assisted decision policy \(\pi\in\Pi(H,B)\) that satisfies monotonicity and is near-optimal, i.e.,_

\[\mathbb{E}_{\pi^{*}}[u(T,Y)]\leq\mathbb{E}_{\pi}[u(T,Y)]+\alpha\cdot\left[u(1,1 )-u(0,1)+\frac{3}{2}(u(0,0)-u(1,0))\right] \tag{7}\]

_where \(\pi^{*}=\operatorname*{argmax}_{\pi\in\Pi(H,B)}\mathbb{E}_{\pi}[u(T,Y)]\) is the optimal policy._

**Corollary 1**.: _If \(f_{B}\) is perfectly aligned with respect to \(f_{H}\), then there always exists an AI-assisted decision policy \(\pi\in\Pi(H,B)\) that satisfies monotonicity and is optimal._

Finally, in many high-stakes applications, we may like to make sure that the confidence values provided by \(f_{B}\) are both useful and interpretable [34]. Hence, we may like to seek for confidence functions \(f_{B}\) that satisfy human-aligned calibration, which we define as follows:

**Definition 6** (Human-aligned calibration).: _A confidence function \(f_{B}\) satisfies \(\alpha\)-aligned calibration with respect to a confidence function \(f_{H}\) if and only if \(f_{B}\) satisfies \(\alpha\)-alignment with respect to \(f_{H}\) and it satisfies \(\alpha\)-calibration with respect to \(\mathcal{Z}\)._

In the next section, we will show how to achieve human-alignment and human-aligned calibration via multicalibration, a statistical notion introduced in the context of algorithmic fairness [11].

Achieving Human-Aligned Calibration via Multicalibration

Multicalibration was introduced by Hebert-Johnson et al. [11] as a notion to achieve fairness in supervised learning. It strengthens the notion of calibration by requiring that the confidence function is calibrated simultaneously across a large collection of subspaces of features \(\mathcal{C}\subseteq 2^{\mathcal{Z}}\) which may or may not be disjoint. More formally, it is defined as follows:

**Definition 7** (Multicalibration).: _A confidence function \(f_{B}:\mathcal{Z}\rightarrow\mathcal{B}\) satisfies \(\alpha\)-multicalibration with respect to \(\mathcal{C}\subseteq 2^{\mathcal{Z}}\) if \(f_{B}\) satisfies \(\alpha\)-calibration with respect to every \(\mathcal{S}\in\mathcal{C}\)._

Then, we can show that, for an appropriate choice of \(\mathcal{C}\), if \(f_{B}\) satisfies \(\alpha\)-multicalibration with respect to \(\mathcal{C}\), then it satisfies \(\alpha\)-aligned calibration with respect to \(f_{H}\). More specifically, we have the following result:

**Theorem 8**.: _If \(f_{B}\) satisfies \((\alpha/2)\)-multicalibration with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\), with \(\mathcal{S}_{h}=\{(x,H)\in\mathcal{Z}\ \mid\ H=h\}\), then \(f_{B}\) satisfies \(\alpha\)-aligned calibration with respect to \(f_{H}\)._

The above theorem suggests that, given a classifier's confidence function \(f_{B}\), we can multicalibrate \(f_{B}\) with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\) to achieve \(\alpha\)-aligned calibration with respect to \(f_{H}\). To achieve multicalibration guarantees using finite amounts of (calibration) data, multicalibration algorithms need to discretize the range of \(f_{B}\)[9, 11, 12]. In what follows, we briefly revisit two algorithms, which carry out this discretization differently, and discuss their complexity and data requirements with respect to achieving \(\alpha\)-aligned calibration.

**Multicalibration algorithm via \(\lambda\)-discretization.** This algorithm, which was introduced by Hebert-Johnson et al. [11], discretizes the range of \(f_{B}\), _i.e._, the interval \([0,1]\), into bins of fixed size \(\lambda>0\) with values \(\Lambda[0,1]=\{\frac{\lambda}{2},\frac{3\lambda}{2},\ldots,1-\frac{\lambda}{2}\}\).

Let \(\lambda(b)=[b-\lambda/2,b+\lambda/2)\). The algorithm partitions each subspace \(\mathcal{S}_{h}\) into \(1/\lambda\) groups \(\mathcal{S}_{h,\lambda(b)}=\{(x,h)\in\mathcal{S}_{h}\ |\ f_{B}(x,h)\in\lambda(b)\}\), with \(b\in\Lambda[0,1]\). It iteratively updates the confidence values of function \(f_{B}\) for these groups until \(f_{B}\) satisfies a discretized notion of \(\alpha^{\prime}\)-multicalibration over these groups. The algorithm then returns a discretized confidence function \(f_{B,\lambda}(x,h)=\mathbb{E}[f_{B}(X,H)\ |\ f_{B}(X,H)\in\lambda(b)]\), with \(b\in\Lambda[0,1]\) such that \(f_{B}(x,h)\in\lambda(b)\), which is guaranteed to satisfy \((\alpha^{\prime}+\lambda)\)-multicalibration. Refer to Algorithm 1 in Appendix B for a pseudocode of the algorithm.

Then, as a direct consequence of Theorem 8, we can obtain a (discretized) confidence function \(f_{B,\lambda}\) that satisfies \(\alpha\)-aligned calibration by setting \(\alpha^{\prime}=\lambda=\alpha/4\). However, the following proposition shows that, to satisfy just \(\alpha\)-alignment, it is enough to set \(\alpha^{\prime}=\frac{3}{8}\alpha>\alpha/4\) and \(\lambda=\alpha/4\):

**Proposition 1**.: _The discretized confidence function \(f_{B,\lambda}\) returned by Algorithm 1 satisfies \((2\alpha^{\prime}+\lambda)\)-alignment with respect to \(f_{H}\)._

Finally, it is worth noting that, to implement Algorithm 1, we need to compute empirical estimates of the expectations and probabilities above using a calibration set \(\mathcal{D}\). In this context, Theorem 2 in Hebert-Johnson et al. [11] shows that, if we use a calibration set of size \(O(\log(|\mathcal{H}|/(\alpha\gamma\xi))/\alpha^{11/2}\gamma^{3/2})\), with \(P((X,H)\in\mathcal{S}_{h})>\gamma\) for all \(h\in\mathcal{H}\), then \(f_{B,\lambda}\) is guaranteed to satisfy \(\alpha\)-multicalibration with probability at least \(1-\xi\) in time \(O(|\mathcal{H}|\cdot\text{poly}(1/\alpha,1/\gamma))\).

**Multicalibration algorithm via uniform mass binning.** Uniform mass binning (UMD) [9, 12] has been originally designed to calibrate \(f_{B}\) with respect to \(\mathcal{Z}\) using a calibration set \(\mathcal{D}\). However, since the subspaces \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\) are disjoint, _i.e._, \(\mathcal{S}_{h}\cap\mathcal{S}_{h^{\prime}}=\emptyset\) for every \(h\neq h^{\prime}\), we can multicalibrate \(f_{B}\) with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\) by just running \(|\mathcal{H}|\) instances of UMD, each using the subset of samples \(\mathcal{D}\cap\mathcal{S}_{h}\). Here, we would like to emphasize that we can use UMD to achieve multicalibration because, in our setting, the subspaces \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\) are disjoint.

Each instance of UMD discretizes the range of \(f_{B}\), _i.e._, the interval \([0,1]\), into \(N=1/\lambda\) bins with values \(\Lambda_{h}[0,1]=\{\hat{P}(Y=1\ |\ f_{B}(X,h)\in[0,\hat{q}_{1})],\ldots,\hat{P}(Y=1\ |\ f_{B}(X,h)\in[\hat{q}_{N-1},\hat{q}_{N}])\}\), where \(\hat{q}_{i}\) denotes the \((i/N)\)-th empirical quantile of the confidence values \(f_{B}(x,h)\) of the samples \((x,h)\in\mathcal{D}\cap\mathcal{S}_{h}\) and \(\hat{P}\) denotes an empirical estimate of the probability using samples from \(\mathcal{D}\cap\mathcal{S}_{h}\), aswell. Here, note that, by construction, the bins have similar probability mass. Then, for each \((x,h)\in\mathcal{Z}\), the corresponding instance of UMD provides the value of the discretized confidence function \(f_{B,\lambda}(x,h)=b\), where \(b\in\Lambda_{h}[0,1]\) denotes the value of the bin whose corresponding defining interval includes \(f_{B}(x,h)\). Finally, we have the following theorem, which guarantees that, as long as the calibration set is large enough, the discretized confidence function \(f_{B,\lambda}\) satisfies \(\alpha\)-aligned calibration with respect to \(f_{H}\) with high probability:

**Theorem 9**.: _The discretized confidence function \(f_{B,\lambda}\) returned by \(|\mathcal{H}|\) instances of UMD, one per \(\mathcal{S}_{h}\), satisfies \(\alpha\)-aligned calibration with respect to \(f_{H}\) with probability at least \(1-\xi\) as long as the size of the calibration set \(|\mathcal{D}|=O\left(\frac{|\mathcal{H}|}{\alpha^{2}\lambda\gamma}\log\left( \frac{|\mathcal{H}|}{\lambda\xi}\right)\right)\), with \(P((X,H)\in\mathcal{S}_{h})\geq\gamma\)._

## 6 Experiments

In this section, we validate our theoretical results using a dataset with real expert predictions in an AI-assisted decision making scenario comprising four different binary classification tasks7.

Footnote 7: We release the code to reproduce our analysis at [https://github.com/Networks-Learning/human-aligned-calibration](https://github.com/Networks-Learning/human-aligned-calibration).

**Data description.** We experiment with the publicly available Human-AI Interactions dataset [35]. The dataset comprises \(34,\!783\) unique predictions from \(1,\!088\) different human participants on four different binary prediction tasks ("Art", "Sarcasm", "Cities" and "Census"). Overall, there are approximately \(32\) different instances per task. In the "Art" task, participants need to determine the art period of a painting given two choices and, overall, there are paintings from four art periods. In the "Sarcasm" task, participants need to detect if sarcasm is present in text snippets from the Reddit sarcasm dataset [36]. In the "Cities" task, participants need to determine which large US city is depicted in an image given two choices and, overall, there are images of four different US cities. Finally, in the "Census" task, participants need to determine if an individual earns more than \(50\)k a year based on certain demographic information in tabular form. For "Sarcasm", \(x\) is a representation of the text snippets and we set \(y=1\) if sarcasm is present, for "Art" and "Cities", \(x\) is a representation of the images and we set \(y=1\) and \(y=0\) at random for each different instance and, for "Census", \(x\) summarizes demographic information and we set \(y=1\) if an individual earns more than \(50\)k a year. In each of the tasks, human participants provide confidence values about their predictions before (\(h\)) and after (\(h\)+AI) receiving AI advice from a classifier in form of the classifier's confidence values \(b\).8 The original dataset contains predictions by participants from different, but overlapping, sets

Figure 2: Empirical estimate of the probabilities \(P(Y=1\mid(X,Y)\in\mathcal{S}_{h,\lambda(b)})\), where \(b\in\Lambda[0,1]\) and \(h\in\{\text{low},\text{mid},\text{high}\}\) are the discretized confidence values for the classifiers and human participants, respectively. Error bars represent \(90\)% confidence intervals and hatched bars mark alignment violations between confidence pairs \((h,b)\) with \(|\mathcal{S}_{h,\lambda(b)}|\geq 30\).

of countries across tasks, who were told the AI advice had different values of accuracy.9 In our experiments, to control for these confounding factors, we focus on participants from the US who were told the AI advice was \(80\)% accurate, resulting in \(15{,}063\) unique predictions from \(471\) different human participants.

Footnote 9: Participants were also either told that the advice is from a “Human” or from an “AI” based on a random assignment of participants to a treatment or control group. Since the actual advice received in both groups was identical for the same instance and the “perceived advice source” is randomized, we use data from both treatment and control groups in the experiments.

**Experimental setup and evaluation metrics.** For each of the tasks, we first measure (i) the degree of misalignment between the classifiers' confidence values \(b\) and the participants' confidence values \(h\) before receiving AI advice \(b\) and (ii) the difference \((h_{\star\text{AI}}-h)\) between the human participant's confidence values before and after receiving AI advice \(b\). Then, we compare the utility achieved by a AI-assisted decision policy \(\pi_{H_{\star\text{AI}}}\) that predicts the value of \(y\) by thresholding the humans' confidence values \(h_{\star\text{AI}}\) after observing the classifier's confidence values against two baselines: (i) a decision policy \(\pi_{B}\) that predicts the value of \(y\) by thresholding the classifier's confidence values \(b\) and (ii) a decision policy \(\pi_{H}\) that predicts the value of \(y\) by thresholding the humans' confidence values \(h\) before observing the classifier's confidence values.

To measure the degree of misalignment, we discretize the confidence values \(b\) and \(h\) into bins. For the classifiers' confidence \(b\), we use \(8\) uniform sized bins per task with (centered) values \(\Lambda[0,1]\), where \(\lambda=1/8\). For the human participants' confidence \(h\) before receiving AI advice \(b\), we use three bins per task ('low','mid' and 'high'), where we set the bin boundaries so that each bin contains approximately the same probability mass and set the bin values to the average confidence value within each bin. In what follows, we refer to the pairs of discretized confidence values \((h,b)\) as cells, where samples \((x,y)\in\mathcal{Z}\) whose confidence values lie in the cell \((h,b)\) define the group \(\mathcal{S}_{h,\lambda(b)}\), and note that we choose a rather low number of bins for both \(b\) and \(h\) so that most cells have sufficient data samples to reliable estimate several misalignment metrics, which we describe next.

We use three different misalignment metrics: (i) the number of alignment violations between cell pairs, (ii) the expected alignment error (EAE) and (iii) the maximum alignment error (MAE). There is an alignment violation between cells pairs \((h,b)\) and \((h^{\prime},b^{\prime})\), with \(h\leq h^{\prime}\) and \(b\leq b^{\prime}\), if

\[P(Y=1|(X,Y)\in S_{h,\lambda(b)})>P(Y=1|(X,Y)\in S_{h^{\prime},\lambda(b^{ \prime})}).\]

Moreover, we have that:

\[\text{EAE} =\frac{1}{N}\cdot\sum_{h\leq h^{\prime},b\leq b^{\prime}}\left[P( Y=1\,\mid\,(X,Y)\in\mathcal{S}_{h,\lambda(b)})-P(Y=1\,\mid\,(X,Y)\in\mathcal{S} _{h^{\prime},\lambda(b^{\prime})})\right]_{+},\] \[\text{MAE} =\max_{h\leq h^{\prime},b\leq b^{\prime}}P(Y=1\,\mid\,(X,Y)\in \mathcal{S}_{h,\lambda(b)})-P(Y=1\,\mid\,(X,Y)\in\mathcal{S}_{h^{\prime}, \lambda(b^{\prime})}),\]

where \(N=|\{h\leq h^{\prime},b\leq b^{\prime}\}|\). Here, note that the number of alignment violations tells us how frequently is the left hand side of Eq. 6 positive across cell pairs given \(\tilde{S}_{h}=\tilde{S}_{h}\) and the EAE and MAE quantify the average and maximum value of the left hand side of Eq. 6 across cells violating alignment. To obtain reliable estimates of the above metrics, we only consider cells \((h,b)\) with \(|\mathcal{S}_{h,\lambda(b)}|\geq 30\) samples. Moreover, we also report the expected calibration error (ECE) and maximum calibration error (MCE) [12, 37], which are natural counterparts to EAE and MAE, respectively.

As a measure of utility, we estimate the true positive rate (TPR) and false positive rate (FPR) of the decision policies \(\pi_{B}\), \(\pi_{H}\) and \(\pi_{H_{\star\text{AI}}}\) for all possible choices of threshold values, which we summarize using the area under the ROC curve (AUC) and, in Appendix C, we also report ROC curves.

\begin{table}
\begin{tabular}{r|c c|c c|c c c} \hline \hline \multirow{2}{*}{**Task**} & \multicolumn{2}{c|}{**Misalignment**} & \multicolumn{2}{c|}{**Miscalibration**} & \multicolumn{3}{c}{**AUC**} \\  & **EAE** & **MAE** & **ECE** & **MCE** & \(\pi_{B}\) & \(\pi_{H}\) & \(\pi_{H_{\star\text{AI}}}\) \\ \hline Art & \(4.5\cdot 10^{-4}\) & \(0.058\) & \(0.084\) & \(0.186\) & \(86.7\)\% & \(72.7\)\% & \(82.0\)\% \\ Sarcasm & \(3.8\cdot 10^{-3}\) & \(0.224\) & \(0.085\) & \(0.310\) & \(89.9\)\% & \(82.5\)\% & \(86.5\)\% \\ Cities & \(6.2\cdot 10^{-5}\) & \(0.013\) & \(0.066\) & \(0.158\) & \(84.4\)\% & \(79.0\)\% & \(84.7\)\% \\ Census & \(9.0\cdot 10^{-3}\) & \(0.298\) & \(0.109\) & \(0.270\) & \(80.0\)\% & \(77.3\)\% & \(79.9\)\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Misalignment, miscalibration and AUC.

**Results.** We start by looking at the empirical estimates of the probabilities \(P(Y=1\mid(X,Y)\in\mathcal{S}_{h,\lambda(b)})\) and of our measures of misalignment (EAE, MAE) and miscalibration (ECE, MCE) in Figure 2 and Table 1 (left and middle columns). The results show that, for "Cities", the probabilities \(P(Y=1\mid(X,Y)\in\mathcal{S}_{h,\lambda(b)})\) are (approximately) monotonically increasing with respect to the classifier's confidence values \(b\). More specifically, as shown in Figure 2, there is only one alignment violation between cell pairs and, hence, our metrics of misalignment acquire also very low values. In contrast, for "Art", "Sarcasm" and especially "Census", there is an increasing number of alignment violations and our misalignment metrics acquire higher values, up to several orders of magnitude higher for "Census". These results also show that misalignment and miscalibration go hand in hand, however, in terms of miscalibration, "Census" does not stand up so strongly.

Next, we look at the difference \(h_{\text{+AI}}-h\) between the human participant's recorded confidence values before and after receiving AI advice \(b\) across samples in each of the subsets \(\mathcal{S}_{h,\lambda(b)}\) induced by the discretized confidence values used above. Figure 3 summarizes the results, which reveal that the difference \(h_{\text{+AI}}-h\) increases monotonically with respect to the classifier's confidence \(b\). This suggests that participants always expect \(b\) to reflect the probability of a positive outcome irrespectively of their confidence value \(h\) before receiving AI advice, providing support for our hypothesis that (rational) decision makers implement monotone AI-assisted decisions policies. Further, this finding also implies that, for "Art", "Sarcasm" and "Census", any policy \(\pi_{H_{\text{+AI}}}\) that predicts the value of the label \(y\) by thresholding the confidence value \(h_{\text{+AI}}\) will be necessarily suboptimal because the probabilities \(P(Y=1\mid(X,Y)\in\mathcal{S}_{h,\lambda(b)})\) are not monotone increasing with \(b\).

Finally, we look at the AUC achieved by decision policies \(\pi_{B}\), \(\pi_{H}\) and \(\pi_{H_{\text{+AI}}}\). Table 1 (right columns) summarize the results, which shows that \(\pi_{H_{\text{+AI}}}\) outperforms \(\pi_{H}\) consistently across all tasks but it only outperforms \(\pi_{B}\) in a single task ("Cities") out of four. These findings provide empirical support for Theorem 3, which predicts that, in the presence of human-alignment violations as those observed in "Art", "Sarcasm" and "Census", any monotone AI-assisted decision policy will be suboptimal, and they also provide support for Theorem 5, which predicts that, under human-alignment, there exist near-optimal AI-assisted decision policies satisfying monotonicity.

## 7 Discussion and Limitations

In this section, we discuss the intended scope of our work and identify several limitations of our theoretical and experimental results, which may serve as starting points for future work.

**Decision making setting.** We have focused on decision making settings where both decisions and outcomes are binary. However, we think that it may be feasible to extend our theoretical analysis to settings with multi-categorical (or real-valued) outcome variables and decisions. One of the main challenges would be to identify which natural conditions utility functions may satisfy in such settings. Further, we also think that it would be significantly more challenging to extend our theoretical analysis to sequential settings--multicalibration in sequential settings is an open area of research--but our ideas may still be a useful starting point. In addition, our theoretical analysis assumes that the decision makers aim to maximize the average utility of their decisions. However, whenever human decisions are consequential to individuals, the decision maker may have fairness desiderata.

**Confidence values.** In our causal model of AI-assisted decision making, we allow the classifier's confidence values to depend on the decision maker's confidence values because this is necessary to achieve human-alignment via multicalibration as described in Section 5. However, we would like to clarify that both Theorems 3 and 5 still hold if the classifier's confidence values do not depend on the decision maker's confidence, as it is typically the status quo today. Looking into the future, our work questions this status quo by showing that, by allowing the classifier's confidence values to depend on the decision maker's confidence values, a decision maker may end up taking decisions with higher utility. Moreover, we would also like to clarify that, while the motivation behind our work is AI-assisted human decision making, our theoretical results do not depend on who--be it a classifier or another human--gives advice. As long as the advice comes in the form of confidence values, our results are valid. Finally, while we have shown that human-alignment can be achieved via multicalibration, we hypothesize that algorithms specifically designed to achieve human-alignment may have lower data and computational requirements than multicalibration algorithms.

**Experimental results.** Our experimental results demonstrate that, _across tasks_, the average utility achieved by decision makers is relatively higher if the classifier they use satisfies human-alignment. However, they do not empirically demonstrate that, _for a fixed task_, there is an improvement in average utility achieved by decision makers if the classifier they use satisfies human-alignment. The reason why we could not demonstrate the latter is because, in our experiments, we used an observational dataset gathered by others [35]. Looking into the future, it would be very important to run a human subject study to empirically demonstrate the latter and, for now, treat our conclusions with caution.

## 8 Conclusions

We have introduced a theoretical framework to investigate what properties confidence values should have to help decision makers take better decisions. We have shown that there exists data distribution for which a rational decision maker using calibrated confidence values will always take suboptimal decisions. However, we have further shown that, if the confidence values satisfy a natural alignment property, which can be achieved via multicalibration, then a rational decision maker using these confidence values can take optimal decisions. Finally, we have illustrated our theoretical results using real human predictions on four AI-assisted decision making tasks.

**Acknowledgements.** We would like to thank Nastaran Okati for fruitful discussions at an early stage of the project. Gomez-Rodriguez acknowledges support from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 945719).

## References

* [1] Wei Jiao, Gurnit Atwal, Paz Polak, Rosa Karlic, Edwin Cuppen, Alexandra Daniy, Jeroen de Ridder, Carla van Herpen, Martijn P Lolkema, Neeltje Steeghs, et al. A deep learning system accurately classifies primary and metastatic cancers using passenger mutation patterns. _Nature communications_, 11(1):1-12, 2020.
* [2] Jacob Whitehill, Kiran Mohan, Daniel Seaton, Yigal Rosen, and Dustin Tingley. Mooc dropout prediction: How to measure accuracy? In _Proceedings of the fourth (2017) acm conference on learning@ scale_, pages 161-164, 2017.
* [3] Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. _Science advances_, 4(1):eaao5580, 2018.

* [4] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In _Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining_, pages 797-806, 2017.
* [5] Niki Kilbertus, Manuel Gomez Rodriguez, Bernhard Scholkopf, Krikamol Muandet, and Isabel Valera. Fair decisions despite imperfect predictions. In _International Conference on Artificial Intelligence and Statistics_, pages 277-287. PMLR, 2020.
* [6] Isabel Valera, Adish Singla, and Manuel Gomez-Rodriguez. Enhancing the accuracy and fairness of human decision making. In _Advances in Neural Information Processing Systems_, 2018.
* [7] Guy N Rothblum and Gal Yona. Decision-making under miscalibration. In _Innovations in Theoretical Computer Science_, 2023.
* [8] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, 2017.
* [9] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In _Proceedings of the 18th International Conference on Machine Learning_, 2001.
* [10] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian Raftery. Probabilistic forecasts, calibration and sharpness. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 2007.
* [11] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In _Proceedings of the 35th International Conference on Machine Learning_, 2018.
* [12] Chirag Gupta and Aaditya K. Ramdas. Distribution-free calibration guarantees for histogram binning without sample splitting. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* [13] Roshni Sahoo, Shengjia Zhao, Alyssa Chen, and Stefano Ermon. Reliable decisions with threshold calibration. _Advances in Neural Information Processing Systems_, 34:1831-1844, 2021.
* [14] Shengjia Zhao, Michael P Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating predictions to decisions: A novel approach to multi-class calibration. In _Advances in Neural Information Processing Systems_, 2021.
* [15] Yingxiang Huang, Wentao Li, Fima Macheret, Rodney A Gabriel, and Lucila Ohno-Machado. A tutorial on calibration measurements and calibration models for clinical prediction models. _Journal of the American Medical Informatics Association_, 27(4):621-633, 2020.
* [16] Lequn Wang, Thorsten Joachims, and Manuel Gomez-Rodriguez. Improving screening processes via calibrated subset selection. In _Proceedings of the 39th International Conference on Machine Learning_, 2023.
* [17] Kailas Vodrahalli, Tobias Gerstenberg, and James Zou. Uncalibrated models can improve human-ai collaboration. In _Advances in Neural Information Processing Systems_, 2022.
* [18] Gal Yona, Amir Feder, and Itay Laish. Useful confidence measures: Beyond the max score. _arXiv preprint arXiv:2210.14070_, 2022.
* [19] Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel Gomez-Rodriguez. Improving expert predictions with conformal prediction. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [20] Judea Pearl. _Causality_. Cambridge university press, 2009.
* [21] Vivian Lai, Chacha Chen, Q Vera Liao, Alison Smith-Renner, and Chenhao Tan. Towards a science of human-ai decision making: a survey of empirical studies. In _Proceedings of the ACM Conference on Fairness, Accountability, and Transparency_, 2023.

* [22] Andrea Papenmeier, Gwenn Englebienne, and Christin Seifert. How model accuracy and explanation fidelity influence user trust. _arXiv preprint arXiv:1907.12652_, 2019.
* [23] Xinru Wang and Ming Yin. Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making. In _26th International Conference on Intelligent User Interfaces_, pages 318-328, 2021.
* [24] Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. Understanding the effect of accuracy on trust in machine learning models. In _Proceedings of the 2019 chi conference on human factors in computing systems_, pages 1-12, 2019.
* [25] Mahsan Nourani, Joanie T. King, and Eric D. Ragan. The role of domain expertise in user trust and the impact of first impressions with intelligent systems. _ArXiv_, abs/2008.09100, 2020.
* [26] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. Effect of confidence and explanation on accuracy and trust calibration in ai-assisted decision making. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, pages 295-305, 2020.
* [27] Chacha Chen, Shi Feng, Amit Sharma, and Chenhao Tan. Machine explanations and human understanding. _Transactions of Machine Learning Research_, 2023.
* [28] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 2017.
* [29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 2022.
* [30] Banghua Zhu, Jiantao Jiao, and Michael Jordan. Principled reinforcement learning with human feedback from pairwise or \(k\)-wise comparisons. _arXiv preprint arXiv:2301.11270_, 2023.
* [31] Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, 2012.
* [32] Matteo Lisi, Gianluigi Mongillo, Georgia Milne, Tessa Dekker, and Andrei Gorea. Discrete confidence levels revealed by sequential decisions. _Nature Human Behaviour_, 5(2):273-280, 2021.
* [33] Hang Zhang, Nathaniel D Daw, and Laurence T Maloney. Human representation of visuo-motor uncertainty as mixtures of orthogonal basis distributions. _Nature neuroscience_, 18(8):1152-1158, 2015.
* [34] Umang Bhatt, Javier Antoran, Yunfeng Zhang, Q Vera Liao, Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Melancon, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, et al. Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 401-413, 2021.
* [35] Kailas Vodrahalli, Roxana Daneshjou, Tobias Gerstenberg, and James Zou. Do humans trust advice more if it comes from ai? an analysis of human-ai interactions. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 763-777, 2022.
* [36] Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A large self-annotated corpus for sarcasm. _arXiv preprint arXiv:1704.05579_, 2017.
* [37] Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, and Peter Flach. Classifier calibration: How to assess and improve predicted class probabilities: a survey. _arXiv e-prints_, pages arXiv-2112, 2021.

Proofs

### Additional Lemmas

**Lemma 1** (Monotonicity).: _If a utility function \(u\) satisfies Eq. 1, then \(u\) is monotone with respect to the probability that \(Y=1\), i.e., for any \(P,P^{\prime}\in\mathcal{P}(\{0,1\})\) such that \(P(Y=1)\leq P^{\prime}(Y=1)\), it holds that \(\mathbb{E}_{Y\sim P}[u(1,Y)]\leq\mathbb{E}_{Y\sim P^{\prime}}[u(1,Y)]\)._

Proof.: We readily have that

\[\mathbb{E}_{Y\sim P}[u(1,Y)] =P(Y=1)\cdot u(1,1)+(1-P(Y=1))\cdot u(1,0)\] \[\leq P^{\prime}(Y=1)\cdot u(1,1)+(1-P^{\prime}(Y=1))\cdot u(1,0)\] \[=\mathbb{E}_{Y\sim P^{\prime}}[u(1,Y)],\]

where, in the above inequality, we use that \(u(1,1)>u(1,0)\) and \(P(Y=1)\leq P^{\prime}(Y=1)\). 

**Lemma 2** (Trivial policies are not always optimal).: _If a utility function \(u\) satisfies Eq. 1, then there exist \(P,P^{\prime}\in\mathcal{P}(\{0,1\})\) such that the trivial policies \(\pi\) that either always decide \(T=1\) or always decide \(T=0\) are suboptimal. In particular, for any \(P,P^{\prime}\in\mathcal{P}(\{0,1\})\) such that \(P(Y=1)<c\) and \(P^{\prime}(Y=1)>c\), where_

\[c=\frac{u(0,0)-u(1,0)}{u(1,1)-u(1,0)+u(0,0)-u(0,1)}\in(0,1), \tag{8}\]

_it holds that_

\[\mathbb{E}_{Y\sim P}[u(1,Y)]<\mathbb{E}_{Y\sim P}[u(0,Y)]\quad\text{and}\quad \mathbb{E}_{Y\sim P^{\prime}}[u(1,Y)]>\mathbb{E}_{Y\sim P^{\prime}}[u(0,Y)]. \tag{9}\]

Proof.: Let \(P\) be any distribution such that

\[P(Y=1)<c=\frac{u(0,0)-u(1,0)}{u(1,1)-u(1,0)+u(0,0)-u(0,1)},\]

where \(c\in(0,1)\) because, by assumption, \(u\) satisfies Eq. 1. Now, by rearranging the above inequality, we have that

\[P(Y=1)\cdot u(1,1)+(1-P(Y=1))\cdot u(1,0)<P(Y=1)\cdot u(0,1)+(1-P(Y=1))\cdot u (0,0),\]

and, using the definition of the expectation, it immediately follows that

\[\mathbb{E}_{Y\sim P}[u(1,Y)]<\mathbb{E}_{Y\sim P}[u(0,Y)].\]

The same argument can be used to show that, for any distribution \(P^{\prime}\) such that \(P^{\prime}(Y=1)>c\), it holds that \(\mathbb{E}_{Y\sim P^{\prime}}[u(1,Y)]>\mathbb{E}_{Y\sim P^{\prime}}[u(0,Y)]\). Finally, note that, since \(c\in(0,1)\), we know that such distributions \(P\) and \(P^{\prime}\) exist. 

### Proof of Theorem 3

Before proving Theorem 3, we rewrite the expected utility with respect to the probability distribution \(P^{\mathcal{M}}\) in terms of confidence \(H\) and \(B\) by using the law of total expectation,

\[\mathbb{E}_{\pi}[u(T,Y)]=\mathbb{E}_{H,B\sim P^{\mathcal{M}}(H,B)}\left[ \mathbb{E}_{\pi}[u(T,Y)|H,B]\right].\]

Here, to simplify notation, we will write

\[\mathbb{E}_{H,B}\left[\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H,B]\right],\]

where note that, using the law of total expectation, we can write the inner expectation in the above expression in terms of the utilities of the trivial policies, _i.e._,

\[\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H,B]=\mathbb{E}[u(1,Y)\,\mid\,H,B ]\cdot P_{\pi}(T=1\,\mid\,H,B)\\ +\mathbb{E}[u(0,Y)\,\mid\,H,B]\cdot P_{\pi}(T=0\,\mid\,H,B), \tag{10}\]

and we will use \(P\) to refer to probabilities induced by SCM \(\mathcal{M}\), _e.g._, \(P(H,B)\) to denote \(P^{\mathcal{M}}(H,B)\). Now, we restate and prove Theorem 3.

**Theorem 3**.: There exist (infinitely many) AI-assisted decision making processes \(\mathcal{M}\) satisfying Eqs. 2 and 3, with utility functions \(u(T,Y)\) satisfying Eq. 1, such that \(f_{B}\) is perfectly calibrated and \(f_{H}\) is monotone but any AI-assisted decision policy \(\pi\in\Pi(H,B)\) that satisfies monotonicity is suboptimal, _i.e._, \(\mathbb{E}_{\pi}[u(T,Y)]<\mathbb{E}_{\pi^{*}}[u(T,Y)]\).

Proof.: To prove the above claim, we construct a monotone confidence function \(f_{H}\), perfectly calibrated confidence function \(f_{B}\) and distribution \(P^{\mathcal{M}}\) for which any monotone AI-assisted decision policy \(\pi\in\Pi(H,B)\) achieves strictly lower utility than a carefully constructed non monotone AI-assisted decision policy \(\tilde{\pi}\in\Pi(H,B)\).

We will present the proof in three parts. First, we will introduce the main building block and idea behind the proof by a small construction of \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\) with \(|\mathcal{H}|=|\mathcal{B}|=3\), where \(\mathcal{B}\subseteq[0,1]\) denotes the (discrete) output space of the classifier's confidence function. We then construct examples of \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\) for arbitrary \(|\mathcal{H}|=k\) and \(|\mathcal{B}|=m\) with \(m,k\in\mathbb{N}\), \(m>k\geq 2\). Lastly, we construct examples where \(\mathcal{B}\) is non-discrete and \(|\mathcal{H}|=k\) with \(k>2\).

**Main building block and small example.**

We start by presenting the main idea of the proof using an example with a small set of confidence values \(\mathcal{H}\) and \(\mathcal{B}\). Let the values of the decision maker's confidence \(H\) be in \(\mathcal{H}=\{h_{1},h_{2},h_{3}\}\) and the values of the classifier's confidence \(B\) be in \(\mathcal{B}=\{b_{1},b_{2},b_{3}\}\), with order \(h_{i}<(h_{i}+1)\) and \(b_{i}<(b_{i}+1)\) respectively.

Our main building block, consists of two distributions \(P^{-},P^{+}\in\mathcal{P}(\{0,1\})\) with \(P^{-}(Y=1)<c\) and \(P^{+}(Y=1)>c\), where \(c\) depends on utility \(u\) as described by Eq. 8 in Lemma 2. We use these distributions for our constructions of \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\), so that for some realizations of \(H,B\) distribution \(P(Y=1\;\mid\;H,B)\) is either \(P^{-}\) or \(P^{+}\). Using Lemma 2 and from Eq. 10, we have that:

* For any \(h_{i},b_{i}\) such that \(P(Y\;\mid\;H=h_{i},B=b_{i})=P^{-}\), it holds that \[\mathbb{E}[u(1,Y)\;\mid\;H=h_{i},B=b_{i}]<\mathbb{E}[u(0,Y)\;\mid\;H=h_{i},B=b _{i}].\] Hence, _decreasing_\(P_{\pi}(T=1\;\mid\;H,B)\)_increases_\(\mathbb{E}[u(T,Y)\;\mid\;H=h_{i},B=b_{i}]\).
* For any \(h_{i},b_{i}\) such that \(P(Y\;\mid\;H=h_{i},B=b_{i})=P^{+}\), it holds that \[\mathbb{E}[u(1,Y)\;\mid\;H=h_{i},B=b_{i}]>\mathbb{E}[u(0,Y)\;\mid\;H=h_{i},B=b _{i}].\] Hence, _increasing_\(P_{\pi}(T=1\;\mid\;H,B)\)_increases_\(\mathbb{E}[u(T,Y)\;\mid\;H=h_{i},B=b_{i}]\).

Intuitively, suppose we now have that, for confidence values \(h_{2},b_{2}\), \(Y\sim P^{+}\) and, for confidence values \(h_{3},b_{2},Y\sim P^{-}\), _i.e._, \(P(Y\;\mid\;H=h_{2},B=b_{2})=P^{+}\) and \(P(Y\;\mid\;H=h_{3},B=b_{2})=P^{-}\). Then, any non-monotone AI-assisted decision policy \(\tilde{\pi}\) with \(P_{\tilde{\pi}}(T=1\;\mid\;H=h_{2},B=b_{2})>P_{\tilde{\pi}}(T=1\;\mid\;H=h_{3},B=b_{2})\) will have higher expected utility than any monotone AI-assisted decision policy given confidence values \(h_{2},b_{2}\) and \(h_{3},b_{2}\). Finally, under an appropriate choice of distribution \(P(H,B)\), such non-monotone AI-assisted decision policies \(\tilde{\pi}\) will offer higher overall utility in expectation.

We formalize this intuition with the following lemma:

**Lemma 3**.: _Let \(\mathcal{M}\) be any AI-assisted decision making process satisfying Eqs. 2 and 3, with utility function \(u(T,Y)\) satisfying Eq. 1. If \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\) are such that there exists confidence values \(b\in\mathcal{B}\), \(h_{i},h_{j}\in\mathcal{H}\), with \(h_{i}<h_{j}\), which satisfy_

\[\begin{split} P(H=h_{i},B=b)>0,&\quad P(H=h_{j},B= b)>0,\\ P(Y\;\mid\;H=h_{i},B=b)=P^{+}&\quad\text{and}\quad P (Y\;\mid\;H=h_{j},B=b)=P^{-},\end{split} \tag{11}\]

_for some distributions \(P^{-},P^{+}\) with \(P^{-}(Y=1)<c\) and \(P^{+}(Y=1)>c\), where_

\[c=\frac{u(0,0)-u(1,0)}{u(1,1)-u(1,0)+u(0,0)-u(0,1)}. \tag{12}\]

_Then, for any monotone AI-assisted decision policy \(\pi\in\Pi(H,B)\), there exists an AI-assisted decision policy \(\tilde{\pi}\in\Pi(H,B)\) which is not monotone and achieves a stricly greater utility than \(\pi\), i.e., \(\mathbb{E}_{\pi}[u(T,Y)]<\mathbb{E}_{\tilde{\pi}}[u(T,Y)]\)._

Proof.: Let \(\pi\) be a monotone AI-assisted decision policy, then it must hold that \(P_{\pi}(T=1\;\mid\;H=h_{i},B=b)\leq P_{\pi}(T=1\;\mid\;H=h_{j},B=b)\) (see Eq. 4). Let \(\tilde{\pi}\) be an identical AI-assisted decision policy to \(\pi\) up to the decision for confidence values \(h_{i},b\) and \(h_{j},b\). We distinguish between three cases.

**-- Case 1**: \(P_{\pi}(T=1\;\mid\;H=h_{i},B=b)<P_{\pi}(T=1\;\mid\;H=h_{j},B=b)\).

Let the probability of \(T=1\) under \(\tilde{\pi}\) for confidence values \(h_{i},b\) and \(h_{j},b\) be switched compared to \(\pi\), _i.e._,

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{i},B=b)=P_{\pi}(T=1\;\mid\;H=h_{j},B=b),\]

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{j},B=b)=P_{\pi}(T=1\;\mid\;H=h_{i},B=b).\]

Then, \(\tilde{\pi}\) is not monotone, as Eq. 4 is not satisfied, and it holds that

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{i},B=b)>P_{\pi}(T=1\;\mid\;H=h_{i},B=b),\]

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{j},B=b)<P_{\pi}(T=1\;\mid\;H=h_{j},B=b).\]

As we decreased \(P(T=1\;\mid\;H=h_{j},B=b)\) and increased \(P(T=1\;\mid\;H=h_{i},B=b)\), by properties (I) and (II), it must hold that the expected utility of \(\tilde{\pi}\) given confidence values \(h_{i},b\) and \(h_{j},b\) is higher than the one of \(\pi\), _i.e._,

\[\mathbb{E}_{\tilde{\pi}}[u(T,Y)\;\mid\;H=h_{i},B=b]>\mathbb{E}_{ \pi}[u(T,Y)\;\mid\;H=h_{i},B=b]\quad\text{and} \tag{13}\] \[\mathbb{E}_{\tilde{\pi}}[u(T,Y)\;\mid\;H=h_{j},B=b]>\mathbb{E}_{ \pi}[u(T,Y)\;\mid\;H=h_{j},B=b]. \tag{14}\]

**-- Case 2**: \(0<P_{\pi}(T=1\;\mid\;H=h_{i},B=b)=P_{\pi}(T=1\;\mid\;H=h_{j},B=b)\leq 1\).

Let the probability of \(T=1\) under \(\tilde{\pi}\) for confidence values \(h_{j},b\) be strictly lower compared to \(\pi\) and be the same as \(\pi\) for \(h_{i},b\). Then, \(\tilde{\pi}\) is not monotone, since by case assumption

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{i},B=b)=P_{\pi}(T=1\;\mid\;H=h_{j},B=b)>P_{ \tilde{\pi}}(T=1\;\mid\;H=h_{j},B=b)\]

and the inequality in Eq. 14 holds by property (I).

**-- Case 3**: \(P_{\pi}(T=1\;\mid\;H=h_{i},B=b)=P_{\pi}(T=1\;\mid\;H=h_{j},B=b)=0\).

Let the probability of \(T=1\) under \(\tilde{\pi}\) for confidence values \(h_{i},b\) be strictly higher compared to \(\pi\) and be the same as \(\pi\) for \(h_{j},b\). Then, \(\tilde{\pi}\) is not monotone, since by case assumption

\[P_{\tilde{\pi}}(T=1\;\mid\;H=h_{j},B=b)=P_{\pi}(T=1\;\mid\;H=h_{i},B=b)<P_{ \tilde{\pi}}(T=1\;\mid\;H=h_{i},B=b)\]

and the inequality in Eq. 13 holds by property (II).

As in all three cases at least one of the strict inequalities in Eqs. 13 or 14 holds and \(\tilde{\pi}\) is equivalent to \(\pi\) (_i.e._, it has the same expected conditional utility) given any other pair of confidence values \(h^{\prime}\in\mathcal{H}\), \(b^{\prime}\in\mathcal{B}\), we have that

\[\mathbb{E}_{\tilde{\pi}}[u(T,Y)]=\mathbb{E}[\mathbb{E}_{\tilde{\pi}}[u(T,Y)]|H,B]>\mathbb{E}[\mathbb{E}_{\pi}[u(T,Y)|H,B]=\mathbb{E}_{\pi}[u(T,Y)].\]

Before proceeding further, we would like to note that we may also state Lemma 3 using \(h\in\mathcal{H}\), \(b_{i},b_{j}\in\mathcal{B}\), with \(b_{i}<b_{j}\), the proof would follow analogously.

Now, we construct an AI-decision making process \(\mathcal{M}\), with \(\mathcal{H}=\{h_{1},h_{2},h_{3}\}\) and \(\mathcal{B}=\{b_{1},b_{2},b_{3}\}\), such the decision maker's confidence \(f_{H}\) is monotone, the classifier's confidence \(f_{B}\) is perfectly calibrated, and the conditions of Lemma 3 are satisfied. First, let \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\) be such that

\[P(f_{B}(Z)=b_{j})=\begin{cases}3/6&\text{if }j=1\\ 2/6&\text{if }j=2\\ 1/6&\text{if }j=3\\ 0&\text{otherwise}\end{cases}\quad\text{and}\]

\[P(H=h_{i}\;\mid\;B=b_{j}):=P_{X,V}(H=h_{i}\;\mid\;f_{B}(Z)=b_{j})=\begin{cases} \frac{1}{4-j}&\text{if }i\geq j\\ 0&\text{otherwise}.\end{cases}\]

Then, it readily follows that \(P(H=h_{i},B=b_{j})=1/6\) for \(i\geq j\) and \(P(H=h_{i},B=b_{j})=0\) otherwise. Moreover, for each pair of confidence values \((h_{i},b_{j})\) with positive probability \(P(H=h_{i},B=b_{j})\), we set

\[P(Y=1\;\mid\;H=h_{i},B=b_{j})=\begin{cases}P^{+}&\text{if }i=j=2\text{ or (}i=3\text{ and }j\in\{1,3\})\\ P^{-}&\text{if (}j=2\text{ and }i=3\text{) or (}j=1\text{ and }i\in\{1,2\}),\end{cases}\]as shown in Figure 4 (left). Then, it readily follows that \(f_{H}\) is monotone with respect to the probability that \(Y=1\), _i.e._, \(P(Y=1\,\mid\,H=h_{i})\leq P(Y=1\,\mid\,H=h_{i+1})\)), and we have that the classifier's confidence values

\[b_{j} :=\sum_{i:i\geq j}P(H=h_{i}\,\mid\,B=b_{j})\cdot P(Y=1\,\mid\,H=h _{i},B=b_{j})\] \[=\begin{cases}2/3\cdot P^{-}+1/3\cdot P^{+}&\text{if }j=1\\ 1/2\cdot P^{-}+1/2\cdot P^{+}&\text{if }j=2\\ P^{+}&\text{if }j=3\\ 0&\text{otherwise}\end{cases}\]

are perfectly calibrated and satisfy that \(b_{j}<b_{j+1}\).

Finally, using Lemma 3 with \(b=b_{2}\), \(h_{i}=h_{2}\), \(h_{j}=h_{3}\), we have that any monotone AI-assisted decision policy is suboptimal for any \(\mathcal{M}\) with \(f_{H}\), \(f_{B}\) and \(P^{\mathcal{M}}\) as defined above.

#### Construction with arbitrary \(|\mathcal{H}|=k\) and \(|\mathcal{B}|=m\), \(m>k\geq 2\).

In this second part of the proof, we construct an AI-assisted decision making processes \(\mathcal{M}\), with \(|\mathcal{H}|=k\) and \(|\mathcal{B}|=m\) such that \(m>k\geq 2\), such that the decision maker's confidence \(f_{H}\) is monotone, the classifier's confidence \(f_{B}\) is perfectly calibrated and the conditions of Lemma 3 are satisfied.

First, let the space of confidence values be \(\mathcal{H}=\{h_{i}\}_{i\in[k]}\) and \(\mathcal{B}=\{b_{j}\}_{j\in[m]}\), with order \(h_{i}<h_{i+1}\) and \(b_{i}<b_{i+1}\), respectively, and \(f_{H},f_{B}\) and \(P^{\mathcal{M}}\) be such that \(P(f_{B}(Z)=b_{j})=1/m\) and

\[P(H=h_{i}\,\mid\,B=b_{j}):=P_{X,V}(H=h_{i}\,\mid\,f_{B}(Z)=b_{j})=\begin{cases} \frac{m-j+1}{m}&\text{if }j=i\\ \frac{m-j+1}{m}&\text{if }i=1,j>k\\ \frac{j-1}{m}&\text{if }j=i+1,j\leq k\\ \frac{j-1}{m}&\text{if }i=k,j>k\\ 0&\text{otherwise}.\end{cases} \tag{15}\]

Figure 4: Nonzero values of \(P(Y=1|H=h_{i},B=b_{j})\) and \(P(H=h_{i},B=b_{j})\) for every \(h_{i}\in\mathcal{H}\) and \(b_{j}\in\mathcal{B}\) used in the first (left) and second (right) part of the proof of Theorem 3. In each cell \((h_{i},b_{j})\) in both panels, \(P^{+}\) or \(P^{-}\) is the value of \(P(Y=1|H=h_{i},B=b_{j})\) and lighter color means lower value of \(P(H=h_{i},B=b_{j})\), where white means \(P(Y=1|h=h_{i},B=b_{j})=0\) and \(P(H,B)=0\). In both panels, the assignment of values is very stylized to facilitate the proof—the classifier’s confidence function \(f_{B}\) partitions the feature space in a way such that a rational decision maker is unable to take decisions that maximize utility for almost all confidence values. However, less stylized examples also satisfy the conditions of Lemma 3. For example, as long as there is one triplet of confidence values \(b_{2},h_{2},h_{3}\) (or \(h_{3},b_{1},b_{2}\) in the left example) for which a rational decision maker is unable to take decisions that maximize utility, Lemma 3 can be applied.

[MISSING_PAGE_EMPTY:17]

and let

\[P(Y=1\,\mid\,X,H=h_{i},X\in I_{j})=\begin{cases}f^{-}(X)&\text{if $j=i$ or ($i=j=1$ )}\\ f^{+}(X)&\text{if $j=i+1$ or ($i=k$ and $j=k+1$),}\end{cases} \tag{20}\]

as shown in Figure 5. Next, we define

\[f_{B}(Z)=f_{B}(X):=P(Y=1\,\mid\,X)=\begin{cases}f^{-}(X)&\text{if $X\in I_{1}$}\\ f^{+}(X)&\text{if $X\in I_{k+1}$}\\ (f^{-}(X)+f^{+}(X))/2&\text{otherwise,}\end{cases}\]

which, by construction, is perfectly calibrated.

To show that the decision maker's confidence function \(f_{H}\) is monotone with respect to the probability that \(Y=1\), we first note that, using Eq. 19, we have that

\[P(X\in I_{j}\,\mid\,H=h_{i})=\begin{cases}1/2&\text{if $1<i<k$ and $j\in\{i,i+1\}$ and}\\ 1&\text{if $i=j=1$}\\ 1&\text{if $i=k$ and $j=k+1$}\\ 0&\text{otherwise.}\end{cases} \tag{21}\]

Hence, using Eq. 21 and the law of total probability, for any \(i\in\{2,\ldots,k-2\}\), we have that

\[P(Y=1\,\mid\,H=h_{i}) =\frac{1}{2}\left[P(Y=1\,\mid\,H=h_{i},X\in I_{i})+P(Y=1\,\mid\,H =h_{i},X\in I_{i+1})\right]\] \[\leq\frac{1}{2}\left[f^{-}(q_{i})+f^{+}(q_{i+1})\right]\] \[=\frac{1}{2}\left[f^{-}\left(\inf I_{i+1}\right)+f^{+}\left(Finally, using Lemma 3 with any choice of confidence values \(h_{i}=h_{j-1}\)\(h_{j}=h_{j}\), \(j\in\{2,\cdots,k-1\}\) and \(b=f_{B}(X)\) with \(X\in I_{j}\), we have that any monotone AI-assisted decision policy \(\pi\) is suboptimal for any \(\mathcal{M}\) with \(|\mathcal{B}|\subseteq[0,1]\) and \(|\mathcal{H}|=k\), \(k\geq 2\) and \(f_{H}\), \(f_{B}\) and \(P^{\mathcal{M}}\) as defined above. 

### Proof of Theorem 5

We prove the statement by contraposition. Let \(\mathcal{M}\) be an AI-assisted decision making process satisfying Eqs. 2 and 3, with a utility function \(u(T,Y)\) satisfying Eq. 1 and let \(\mathcal{M}\) be such that \(f_{B}\) satisfies \(\alpha\)-alignment with respect to \(f_{H}\) and \(f_{B}\) has output space \(\mathcal{B}\subseteq[0,1]\). Assume there exists no (near-)optimal monotone AI-assisted decision policy for utility \(u\). Thus, there must exist an optimal AI-assisted decision policy \(\pi\in\Pi(H,B)\) which is not monotone and has strictly greater expected utility than any monotone policy. However, we show that we can modify \(\pi\) to a monotone AI-assisted decision policy \(\hat{\pi}\in\Pi(H,B)\) with near-optimal expected utility.

As \(\pi\) is not monotone, there must exist confidence values \(h_{1},h_{2}\in\mathcal{H}\), \(h_{1}\leq h_{2}\), and \(b_{1},b_{2}\in\mathcal{B}\), \(b_{1}\leq b_{2}\), such that

\[\pi(h_{1},b_{1},w)>\pi(h_{2},b_{2},w)\quad\text{for some}\quad w\in\mathcal{W}, \tag{22}\]

where \(\mathcal{W}\) denotes the space of noise values. In what follows, let \(\tilde{\mathcal{W}}_{h_{1},b_{1}}^{(\pi,h_{2},b_{2})}\subseteq\mathcal{W}\) denote the set containing any such \(w\) and let \(\tilde{\mathcal{W}}^{(\pi,h_{2},b_{2})}=\bigcup_{h,b\in\mathcal{H}\times \mathcal{B}}\tilde{\mathcal{W}}_{h,b}^{(\pi,h_{2},b_{2})}\).

For any confidence value \(h^{\prime},b^{\prime}\in\mathcal{H}\times[0,1]\), we modify policy \(\pi\) to a policy \(\hat{\pi}\) as follows. Let \(\{\tilde{\mathcal{S}}_{h}\}_{h\in\mathcal{H}}\) denote the sets satisfying the \(\alpha\)-alignment condition for \(f_{B}\) with respect to \(f_{H}\) and, given confidence \(h^{\prime}\), let \(\hat{b}_{h^{\prime}}\) denote the smallest confidence value of \(f_{B}\), such that there exist \(h\leq h^{\prime}\) with \(P(Y=1\ \mid\ B=\hat{b}_{h^{\prime}},Z\in\tilde{\mathcal{S}}_{h})\geq c\), _i.e._,

\[\hat{b}_{h^{\prime}}:=\min\{b\in\mathcal{B}\ \mid\ P(Y=1\ \mid\ B=b,Z\in\tilde{ \mathcal{S}}_{h})\geq c\ \text{for}\ h\leq h^{\prime}\}. \tag{23}\]

Now, we define a new AI-assisted policy \(\hat{\pi}\) from \(\pi\) as follows,

\[\hat{\pi}(h^{\prime},b^{\prime},w):=\begin{cases}1&\text{if}\ b^{\prime}\geq \hat{b}_{h}\ \text{and}\ w\in\bigcup_{h\leq h^{\prime},b\in\left[\hat{b}_{h^{\prime}},b^{ \prime}\right]}\tilde{\mathcal{W}}^{(\pi,h,b)}\\ 0&\text{if}\ b^{\prime}<\hat{b}_{h}\ \text{and}\ w\in\bigcup_{h\geq h^{\prime},b \in\left[\hat{b}_{h^{\prime}},\hat{b}_{h^{\prime}}\right)}\tilde{\mathcal{W}} ^{(\pi,h,b)}\\ \pi(h^{\prime},b^{\prime},w)&\text{otherwise}.\end{cases} \tag{24}\]

Next, we show that \(\hat{\pi}\) is monotone and \(\mathbb{E}_{\hat{\pi}}[u(T,Y)]\geq\mathbb{E}_{\pi}[u(T,Y)]+\alpha\cdot a\) for some constant \(a\).

**Proof \(\hat{\pi}\) is a monotone assisted policy.**

To prove that \(\hat{\pi}\in\Pi(H,B)\) is a monotone AI-assisted decision policy, we show that, for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H},b^{\prime},b^{\prime\prime}\in \mathcal{B}\), with \(h^{\prime}\leq h^{\prime\prime},b^{\prime}\leq b^{\prime\prime}\), it holds that \(\tilde{\mathcal{W}}_{h^{\prime},b^{\prime}}^{(\hat{\pi},h^{\prime\prime},b^{ \prime\prime})}=\emptyset\). We distinguish between three cases.

**-- Case 1**: \(b^{\prime}\geq\hat{b}_{h^{\prime}}\) and \(b^{\prime\prime}\geq\hat{b}_{h^{\prime\prime}}\).

Since \(h^{\prime}\leq h^{\prime\prime}\), \(b^{\prime}\leq b^{\prime\prime}\) and, by definition, \(\hat{b}_{h^{\prime\prime}}\leq\hat{b}_{h^{\prime}}\) since \(h^{\prime}\leq h^{\prime\prime}\), we have that

\[\bigcup_{h\leq h^{\prime},b\in\left[\hat{b}_{h^{\prime}},b^{\prime}\right]} \tilde{\mathcal{W}}^{(\pi,h,b)}\subseteq\bigcup_{h\leq h^{\prime\prime},b\in \left[\hat{b}_{h^{\prime\prime}},b^{\prime\prime}\right]}\tilde{\mathcal{W}}^{( \pi,h,b)}.\]

Hence, we can conclude that

\[\hat{\pi}(h^{\prime},b^{\prime},w)\leq 1=\hat{\pi}(h^{\prime\prime},b^{\prime\prime},w) \ \text{for all}\ w\in\bigcup_{h\leq h^{\prime\prime},b\in\left[\hat{b}_{h^{\prime \prime}},b^{\prime\prime}\right]}\tilde{\mathcal{W}}^{(\pi,h,b)}. \tag{25}\]

Further, for any other \(w\in\mathcal{W}-\bigcup_{h\leq h^{\prime\prime},b\in\left[\hat{b}_{h^{\prime \prime}},b^{\prime}\right]}\tilde{\mathcal{W}}^{(\pi,h,b)}\subseteq\mathcal{W}- \tilde{\mathcal{W}}_{h^{\prime},b^{\prime\prime}}^{(\pi,h^{\prime\prime},b^{ \prime\prime})}\), we have that \(\hat{\pi}(h^{\prime},b^{\prime},w)=\pi(h^{\prime},b^{\prime},w)\) and \(\hat{\pi}(h^{\prime\prime},b^{\prime\prime},w)=\pi(h^{\prime\prime},b^{\prime \prime},w)\) and, by definition of \(\tilde{\mathcal{W}}_{h^{\prime},b^{\prime}}^{(\pi,h^{\prime\prime},b^{\prime\prime})}\), it follows that

\[\hat{\pi}(h^{\prime},b^{\prime},w)\leq\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\ \text{for all}\ w\in\mathcal{W}-\bigcup_{h\leq h^{\prime\prime},b\in\left[ \hat{b}_{h^{\prime\prime}},b^{\prime\prime}\right]}\tilde{\mathcal{W}}^{(\pi,h,b)}. \tag{26}\]From Eqs. 25 and 26, it follows that \(\tilde{\mathcal{W}}^{(\hat{\pi},h^{\prime\prime},b^{\prime\prime})}_{h^{\prime},b^{ \prime}}=\emptyset\).

**-- Case 2:**\(b^{\prime}<\hat{b}_{h^{\prime}}\) and \(b^{\prime\prime}\geq\hat{b}_{h^{\prime\prime}}\).

By definition of \(\hat{\pi}\), we have that

\[\hat{\pi}(h^{\prime},b^{\prime},w)\leq 1=\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\text{ for all }w\in\bigcup_{h\leq h^{\prime\prime},b\in\llbracket b_{h^{ \prime\prime}},b^{\prime\prime}\rrbracket}\tilde{\mathcal{W}}^{(\pi,h,b)} \tag{27}\]

and

\[\hat{\pi}(h^{\prime},b^{\prime},w)=0\leq\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\text{ for all }w\in\bigcup_{h\geq h^{\prime},b\in\llbracket b^{ \prime},\hat{b}_{h^{\prime}}\rangle}\tilde{\mathcal{W}}^{(\pi,h,b)} \tag{28}\]

Analogously to case 1, since the values of \(w\) below are also in \(\mathcal{W}-\tilde{\mathcal{W}}^{(\pi,h^{\prime\prime},b^{\prime\prime})}_{h^ {\prime},b^{\prime}}\) and \(\hat{\pi}\) is equivalent to \(\pi\) for these values, we have that

\[\hat{\pi}(h^{\prime},b^{\prime},w)\leq\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\text{ for all }w\in\mathcal{W}-\bigcup_{h\leq h^{\prime\prime},b \in\llbracket b_{h^{\prime\prime}},b^{\prime\prime}\rrbracket}\tilde{\mathcal{ W}}^{(\pi,h,b)}-\bigcup_{h\geq h^{\prime},b\in\llbracket b^{\prime},\hat{b}_{h^{ \prime}}\rangle}\tilde{\mathcal{W}}^{(\pi,h,b)} \tag{29}\]

From Eqs. 27 28 and 29, it follows that \(\tilde{\mathcal{W}}^{(\hat{\pi},h^{\prime\prime},b^{\prime\prime})}_{h^{ \prime},b^{\prime}}=\emptyset\).

**-- Case 3:**\(b^{\prime}<\hat{b}_{h^{\prime}}\) and \(b^{\prime\prime}<\hat{b}_{h^{\prime\prime}}\).

Since \(h^{\prime}\leq h^{\prime\prime}\), \(b^{\prime}\leq b^{\prime\prime}\) and, by definition, \(\hat{b}_{h^{\prime\prime}}\leq\hat{b}_{h^{\prime}}\) since \(h^{\prime}\leq h^{\prime\prime}\), we have that

\[\bigcup_{h\geq h^{\prime\prime},b\in\llbracket b^{\prime\prime},\hat{b}_{h^{ \prime\prime}}\rangle}\tilde{\mathcal{W}}^{(\pi,h,b)}\subseteq\bigcup_{h\geq h ^{\prime},b\in\llbracket b^{\prime},\hat{b}_{h^{\prime}}\rangle}\tilde{ \mathcal{W}}^{(\pi,h,b)}.\]

Hence, we can conclude that

\[\hat{\pi}(h^{\prime},b^{\prime},w)=0\leq\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\text{ for all }w\in\bigcup_{h\geq h^{\prime},b\in\llbracket b^{ \prime},\hat{b}_{h^{\prime}}\rangle}\tilde{\mathcal{W}}^{(\pi,h,b)} \tag{30}\]

Again analogously to case 1, since the values of \(w\) below are also in \(\mathcal{W}-\tilde{\mathcal{W}}^{(\pi,h^{\prime\prime},b^{\prime\prime})}_{h^ {\prime},b^{\prime}}\) and \(\hat{\pi}\) is equivalent to \(\pi\) for these values, we have that

\[\hat{\pi}(h^{\prime},b^{\prime},w)\leq\hat{\pi}(h^{\prime\prime},b^{\prime \prime},w)\text{ for all }w\in\mathcal{W}-\bigcup_{h\geq h^{\prime},b\in \llbracket b^{\prime},\hat{b}_{h^{\prime}}\rangle}\tilde{\mathcal{W}}^{(\pi,h,b)} \tag{31}\]

From Eqs. 30 and 31, it follows that \(\tilde{\mathcal{W}}^{(\hat{\pi},h^{\prime\prime},b^{\prime\prime})}_{h^{\prime },b^{\prime\prime}}=\emptyset\).

Note that, we cannot have a case where \(b^{\prime}\geq\hat{b}_{h^{\prime}}\) and \(b^{\prime\prime}<\hat{b}_{h^{\prime\prime}}\), as this would imply \(b^{\prime\prime}<b^{\prime}\). Since, in all three possible cases, we have shown that \(\tilde{\mathcal{W}}^{(\hat{\pi},h^{\prime\prime},b^{\prime\prime})}_{h^{ \prime},b^{\prime}}=\emptyset\), we can conclude that \(\hat{\pi}\in\Pi(H,B)\) is monotone.

**Proof \(\hat{\pi}\) is near optimal.**

First, we rewrite the inner expectation in Eq. 10 as

\[\mathbb{E}_{\pi}[u(T,Y)\ \mid\ H,B]=\mathbb{E}[u(0,Y)\ \mid\ H,B]+( \mathbb{E}[u(1,Y)\ \mid\ H,B]\\ -\mathbb{E}[u(0,Y)\ \mid\ H,B])\cdot P_{\pi}(T=1\ \mid\ H,B).\]

Further, recall that \(|\tilde{\mathcal{S}}_{h}|\geq(1-\alpha/2)|\mathcal{S}_{h}|\) for all \(h\in\mathcal{H}\) and, for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H}\), \(h^{\prime}\leq h^{\prime\prime}\) and all \(b^{\prime},b^{\prime\prime}\in[0,1]\), \(b^{\prime}\leq b^{\prime\prime}\), we have that

\[P(Y=1\ \mid\ f_{B}(Z)=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})-P(Y=1\ \ \mid\ f_{B}(Z)=b^{\prime\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime\prime}})\leq\alpha \tag{32}\]

Now, for any \(h^{\prime}\in\mathcal{H},b^{\prime}\in\mathcal{B}\), we show an upper bound on \(\mathbb{E}_{\pi}[u(T,Y)\ \mid\ H=h^{\prime},B=b^{\prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y)\ \mid\ H=h^{\prime},B=b^{\prime}]\). We distinguish between three cases.

**-- Case 1**: \(b^{\prime}\geq\hat{b}_{h^{\prime}}\) and \(P(Y=1\ \mid\ H=h^{\prime},B=b^{\prime})\geq c\).

Using Lemma 2, we have that

\[(\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{E}[u(0,Y)\,\mid\,H=h^{ \prime},B=b^{\prime}])\geq 0 \tag{33}\]

Moreover, as \(b^{\prime}\geq\hat{b}_{h^{\prime}}\), the distribution of positive decisions in \(\hat{\pi}\) may also increases for \(h^{\prime},b^{\prime}\) compared to \(\pi\) (see Eq. 24), _i.e._,

\[P_{\pi}(T=1\,\mid\,H=h^{\prime},B=b^{\prime})-P_{\hat{\pi}}(T=1\,\mid\,H=h^{ \prime},B=b^{\prime})\leq 0\]

Hence, it follows that

\[\begin{split}\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H=h^{\prime},B=b^{ \prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]\\ =(\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{E}[ u(0,Y)\,\mid\,H=h^{\prime},B=b^{\prime}])\\ \times(P_{\pi}(T=1\,\mid\,H=h^{\prime},B=b^{\prime})-P_{\hat{\pi} }(T=1\,\mid\,H=h^{\prime},B=b^{\prime}))\leq 0.\end{split} \tag{34}\]

**-- Case 2**: \(b^{\prime}\geq\hat{b}_{h^{\prime}}\) and \(P(Y=1\,\mid\,H=h^{\prime},B=b^{\prime})<c\).

Since \(b^{\prime}\geq\hat{b}_{h^{\prime}}\), there exists \(h,b\in\mathcal{H}\times\mathcal{B}\), with \(h\leq h^{\prime}\), \(b\leq b^{\prime}\), such that \(P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})\geq c\). Moreover, using the definition of \(\alpha\)-alignment, we have that

\[P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})\leq P(Y=1\,\mid\,B=b^{\prime},Z \in\tilde{\mathcal{S}}_{h^{\prime}})+\alpha \tag{35}\]

Then, we can use this to lower bound the expected utility of \(T=1\) given \(B=b^{\prime}\) and \(Z\in\tilde{\mathcal{S}}_{h^{\prime}}\) as follows:

\[\begin{split}\mathbb{E}[u(1,Y)\,\mid\,B&=b,Z\in \tilde{\mathcal{S}}_{h}]-\mathbb{E}[u(1,Y)\,\mid\,B=b^{\prime},Z\in\tilde{ \mathcal{S}}_{h^{\prime}}]\\ &=u(1,1)\cdot(P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})-P(Y=1 \,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})\\ &+u(1,0)\cdot(P(Y=1\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{ h^{\prime}})-P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h}))\\ &\leq(u(1,1)-u(1,0))\cdot\alpha,\end{split} \tag{36}\]

where the last inequality due to Eq. 35 and the assumption that \(u(1,1)-u(1,0)>0\). Analogously, we can also upper bound the expected utility of \(T=0\) given \(H=h^{\prime},B=b^{\prime}\) and \(Z\in\tilde{\mathcal{S}}_{h^{\prime}}\) as follows:

\[\begin{split}\mathbb{E}[u(0,Y)\,\mid\,B&=b,Z\in \tilde{\mathcal{S}}_{h}]-\mathbb{E}[u(0,Y)\,\mid\,B=b^{\prime},Z\in\tilde{ \mathcal{S}}_{h^{\prime}}]\\ &=u(0,1)\cdot(P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})-P(Y= 1\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})\\ &+u(0,0)\cdot(P(Y=1\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{ h^{\prime}})-P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h}))\\ &\geq(u(0,1)-u(0,0))\cdot\alpha,\end{split} \tag{37}\]

where the last inequality holds due to Eq. 35 and the assumption that \(u(0,1)-u(0,0)<0\).

Now, as \(P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})\geq c\), by Lemma 2, we have that

\[\mathbb{E}[u(1,Y)\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h}]\geq\mathbb{E}[u(0,Y) \,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h}] \tag{38}\]

Combining Eqs. 36, 37 and 38, we obtain

\[\begin{split}\mathbb{E}[u(1,Y)\,\mid\,B&=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}]+\alpha(u(1,1)-u(1,0))\\ &\geq\mathbb{E}[u(0,Y)\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{ S}}_{h^{\prime}}]+\alpha(u(0,1)-u(0,0))\end{split} \tag{39}\]

In addition, note that we have following trivial bound for the expectation when \(H=h^{\prime}\) but \(Z\notin\tilde{\mathcal{S}}_{h^{\prime}}\)

\[u(1,0)\leq\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]\leq u(1,1), \tag{40}\]

\[u(0,1)\leq\mathbb{E}[u(0,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]\leq u(0,0) \tag{41}\]

Moreover, since \(b^{\prime}\geq\hat{b}_{h^{\prime}}\), the distribution of positive decisions in \(\hat{\pi}\) may also increase for \(h^{\prime},b^{\prime}\) compared to \(\pi\), _i.e._,

\[P_{\pi}(T=1\,\mid\,H=h^{\prime},B=b^{\prime})-P_{\hat{\pi}}(T=1\,\mid\,H=h^{ \prime},B=b^{\prime})\leq 0\]

Hence, we have that

\[\begin{split}\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H&=h ^{\prime},B=b^{\prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y)\,\mid\,H=h^{\prime},B=b^ {\prime}]\\ &\leq(-1)\cdot(\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{ \prime}]-\mathbb{E}[u(0,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]),\end{split} \tag{42}\]where the inequality follows since \(\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{E}[u(0,Y)\,\mid\,H=h^{ \prime},B=b^{\prime}]\leq 0\) by Lemma 2 as \(P(Y=1\,\mid\,H=h^{\prime},B=b^{\prime})<c\).

Finally, combining Eqs. 39, 40, 41 and 42 and using the law of total expectation, we obtain

\[\mathbb{E}_{\pi}[u(T,Y) \,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y) \,\mid\,H=h^{\prime},B=b^{\prime}] \tag{43}\] \[\leq(1-\beta_{(h^{\prime},b^{\prime})})(\mathbb{E}[u(0,Y)\,\mid\, B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}]-\mathbb{E}[u(1,Y)\,\mid\,B=b^{ \prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}])\] \[+\beta_{(h^{\prime},b^{\prime})}(\mathbb{E}[u(0,Y)\,\mid\,H=h^{ \prime},B=b^{\prime}]-\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}])\] \[\leq(1-\beta_{(h^{\prime},b^{\prime})})\alpha(u(1,1)-u(1,0)+u(0, 0)-u(0,1))+\beta_{(h^{\prime},b^{\prime})}(u(0,0)-u(1,0)),\]

where \(\beta_{(h^{\prime},b^{\prime})}\) denotes the probability of \(Z\notin\tilde{\mathcal{S}}_{h^{\prime}}\) given \(H=h^{\prime},B=b^{\prime}\), _i.e._, \(\beta_{(h^{\prime},b^{\prime})}=P(Z\notin\tilde{\mathcal{S}}_{h^{\prime}}|H=h^{ \prime},B=b^{\prime})\).

**-- Case 3**: \(b^{\prime}<\hat{b}_{h^{\prime}}\).

For all \(h,b\), with \(h\leq h^{\prime}\), \(b\leq b^{\prime}\), we have that \(P(Y=1\,\mid\,B=b,Z\in\tilde{\mathcal{S}}_{h})<c\). In particular, \(P(Y=1\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})<c\). Thus, by Lemma 2,

\[\mathbb{E}[u(1,Y)\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}]< \mathbb{E}[u(0,Y)\,\mid\,B=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}] \tag{44}\]

In this case, since \(b^{\prime}<\hat{b}_{h^{\prime}}\), the distribution of positive decisions in \(\hat{\pi}\) may decrease for \(h,b\) compared to \(\pi\), _i.e._,

\[0\leq P_{\pi}(T=1\,\mid\,H=h,B=b)-P_{\hat{\pi}}(T=1\,\mid\,H=h,B=b)\]

Combining Eqs.44, 40 and 41 and using the law of total expectation, we obtain

\[\mathbb{E}_{\pi}[u(T,Y) \,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y) \,\mid\,H=h^{\prime},B=b^{\prime}] \tag{45}\] \[\leq(\mathbb{E}[u(1,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]-\mathbb{ E}[u(0,Y)\,\mid\,H=h^{\prime},B=b^{\prime}])\cdot 1\] \[=(1-\beta_{(h^{\prime},b^{\prime})})(\mathbb{E}[u(1,Y)\,\mid\,B=b ^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}]-\mathbb{E}[u(0,Y)\,\mid\,B=b^{ \prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}}])\] \[+\beta_{(h^{\prime},b^{\prime})}(\mathbb{E}[u(1,Y)\,\mid\,H=h^{ \prime},B=b^{\prime}]-\mathbb{E}_{Y}[u(0,Y)\,\mid\,H=h^{\prime},B=b^{\prime}])\] \[\leq\beta_{(h^{\prime},b^{\prime})}(u(1,1)-u(0,1)),\]

where again \(\beta_{(h^{\prime},b^{\prime})}=P(Z\notin\tilde{\mathcal{S}}_{h^{\prime}}|H=h ^{\prime},B=b^{\prime})\).

Now, for a fixed \(h^{\prime}\in\mathcal{H}\), since \(|\tilde{\mathcal{S}}_{h^{\prime}}|\geq(1-\alpha/2)|\mathcal{S}_{h^{\prime}}|\), we know that \(0\leq\sum_{b\in\mathcal{B}}\beta_{(h^{\prime},b)}\leq\alpha/2\). Hence, combining Eqs. 34, 43 and 45 from the three cases above, we have that

\[\mathbb{E}_{B}[\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H=h^{\prime},B=b^{ \prime}]]-\mathbb{E}_{B}[\mathbb{E}_{\hat{\pi}}[u(T,Y)\,\mid\,H=h^{\prime},B=b ^{\prime}]]\\ =\mathbb{E}_{B}[\mathbb{E}_{\pi}[u(T,Y)\,\mid\,H=h^{\prime},B=b ^{\prime}]-\mathbb{E}_{\hat{\pi}}[u(T,Y)\,\mid\,H=h^{\prime},B=b^{\prime}]]\\ \leq\max\{\alpha(u(1,1)-u(1,0)+u(0,0)-u(0,1))+\frac{\alpha}{2} \cdot(u(0,0)-u(1,0)),\,\frac{\alpha}{2}\cdot(u(1,1)-u(0,1))\}\\ \leq\alpha\cdot(u(1,1)-u(0,1)+\frac{3}{2}\cdot(u(0,0)-u(1,0))).\]

Finally, since by assumption \(\pi\) is optimal, _i.e._, \(\mathbb{E}_{\pi}[u(T,Y)]=\mathbb{E}_{\pi^{*}}[u(T,Y)]=\max_{\pi^{\prime}\in \Pi(H,B)}\mathbb{E}_{\pi^{\prime}}[u(T,Y)]\), we can conclude by the law of total expectation that

\[\mathbb{E}_{\pi^{*}}[u(T,Y)] =\mathbb{E}_{H}\mathbb{E}_{B}[\mathbb{E}_{Y,\,T\,\mid\,\pi}[u( T,Y)\,\mid\,H,B]]\] \[\leq\mathbb{E}_{\hat{\pi}}[u(T,Y)]+\alpha\cdot(u(1,1)-u(0,1)+\frac {3}{2}\cdot(u(0,0)-u(1,0)))\,.\]

This concludes the proof.

### Proof of Theorem 8

If \(f_{B}\) is \(\alpha/2\)-multicalibrated with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\), then, by definition, for any \(h\in\mathcal{H}\), there exists \(\tilde{\mathcal{S}}_{h}\subset\mathcal{S}_{h}\) with \(|\mathcal{S}|\geq(1-\alpha/2)|\mathcal{S}_{h}|\) such that, for any \(b\in[0,1]\), it holds that

\[|P(Y=1\,\mid\,f_{B}(Z)=b,Z\in\tilde{\mathcal{S}}_{h})-b|\leq\alpha/2.\]This directly implies that, for any \(h^{\prime},h^{\prime\prime}\in\mathcal{H}\) and \(b^{\prime},b^{\prime\prime}\in[0,1]\), we have that

\[P(Y=1\,\mid\,f_{B}(Z)=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})-b^{ \prime}-P(Y=1\,\mid\,f_{B}(Z)=b^{\prime\prime},Z\in\tilde{\mathcal{S}}_{h^{ \prime\prime}})-b^{\prime\prime}\leq\alpha \tag{46}\]

and, using linearity of expectation, we further have that

\[P(Y=1\,\mid\,f_{B}(Z)=b^{\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime}})-P(Y=1\, \mid\,f_{B}(Z)=b^{\prime\prime},Z\in\tilde{\mathcal{S}}_{h^{\prime\prime}}) \leq\alpha+b^{\prime}-b^{\prime\prime}, \tag{47}\]

showing that, whenever \(b^{\prime}\leq b^{\prime\prime}\), the \(\alpha\)-alignment condition is met. This proves that \(f_{B}\) is \(\alpha\)-aligned with respect to \(f_{H}\).

Finally, if \(f_{B}\) is \(\alpha/2\)-multicalibrated with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\), then, it is \(\alpha/2\)-calibrated with respect to any of the sets \(\mathcal{S}_{h}\). Since \(\mathcal{Z}=\cup_{h\in\mathcal{H}}\mathcal{S}_{h}\), this implies that \(f_{B}\) is \(\alpha/2\)-calibrated with respect to \(\mathcal{Z}\). This concludes the proof.

### Proof of Proposition 1

Given a discretization parameter \(\lambda\), Algorithm 1 works with a discretized notion of \(\alpha\)-multicalibration, namely \((\alpha,\lambda)\)-multicalibration:

**Definition 10**.: _Let \(\mathcal{C}\subseteq 2^{Z}\) be a collection of subsets of \(\mathcal{Z}\). For any \(\alpha,\lambda>0\), confidence function \(f_{B}:\mathcal{Z}\rightarrow[0,1]\) is \((\alpha,\lambda)\)-multicalibrated with respect to \(\mathcal{C}\) if, for all \(S\in\mathcal{C}\), \(b\in\Lambda[0,1]\), and all \(S_{h,\lambda(b)}(g)\) such that \(|\mathcal{S}_{h,\lambda(b)}|\geq\alpha\lambda|\mathcal{S}_{h}|\), it holds that_

\[|\mathbb{E}[f_{B}(X,H)-P(Y=1\,\mid\,X,H)\,\mid\,(X,H)\in\mathcal{S}_{h, \lambda(b)}]|\leq\alpha\,. \tag{48}\]

Here, we can analogously define a discretized notion of \(\alpha\)-alignment, namely \((\alpha,\lambda)\)-alignment.

**Definition 11**.: _For \(\alpha,\lambda>0\), a confidence function \(f_{B}:\mathcal{Z}\rightarrow[0,1]\) is \((\alpha,\lambda)\)-aligned with respect to \(f_{H}\) if, for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H}\), \(h^{\prime}\leq h^{\prime\prime}\), and all \(b^{\prime},b^{\prime\prime}\in\Lambda[0,1]\), \(b^{\prime}\leq b^{\prime\prime}\), with \(|\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}|>\alpha/2\cdot\lambda|S_{h^{ \prime}}|\) and \(|\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}|>\alpha/2\cdot \lambda|S_{h^{\prime\prime}}|\), we have_

\[P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime})})-P(Y=1\, \mid\,(X,H)\in\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})})\leq \alpha\,. \tag{49}\]

In what follows, we first show that \((\alpha,\lambda)\)-multicalibration with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\) implies \((2\alpha+\lambda,\lambda)\)-alignment with respect to \(f_{H}\).

**Theorem 12**.: _For \(\alpha,\lambda>0\), if \(f_{B}\) is \((\alpha,\lambda)\)-multicalibrated with respect to \(\{\mathcal{S}_{h}\}_{h\in\mathcal{H}}\), then \(f_{B}\) is \((2\alpha+\lambda,\lambda)\)-aligned with respect to \(f_{H}\)._

Proof.: If \(f_{B}\) is \((\alpha,\lambda)\)-multicalibrated with respect to \(\{S_{h}\}_{h\in\mathcal{H}}\), then, by definition, for all \(h\in\mathcal{H}\), \(b\in\Lambda[0,1]\), and all \(\mathcal{S}_{h,\lambda(b)}\) such that \(|\mathcal{S}_{h,\lambda(b)}|\geq\alpha\cdot\lambda|\mathcal{S}_{h}|\), it holds that

\[|\mathbb{E}[f_{B}(X,H)-P(Y=1\,\mid\,X,H)\,\mid\,(X,H)\in\mathcal{S}_{h, \lambda(b)}]|\leq\alpha. \tag{50}\]

This directly implies that, for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H},b^{\prime},b^{\prime\prime}\in\Lambda [0,1]\) with \(|\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}|\geq\alpha\cdot\lambda|S_{h^{ \prime}}|\) and \(|\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}|\geq\alpha\cdot \lambda|S_{h^{\prime\prime}}|\), it holds that

\[\mathbb{E}[f_{B}(X,H)-P(Y=1\,\mid\,X,H)\,\mid\,(X,H)\in\mathcal{ S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}] \tag{51}\] \[\quad-\mathbb{E}[f_{B}(X,H)-P(Y=1\,\mid\,X,H)\,\mid\,(X,H)\in \mathcal{S}_{h^{\prime},\lambda(b^{\prime})}]\leq 2\alpha\]

and, using the linearity of expectation, we have that

\[P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime} )})-P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime \prime})}) \tag{52}\] \[\leq 2\alpha+\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in\mathcal{S}_{h^{ \prime},\lambda(b^{\prime})}]-\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in\mathcal{S}_{h ^{\prime\prime},\lambda(b^{\prime\prime})}].\]

Whenever \(b^{\prime}\leq b^{\prime\prime}\), due to the \(\lambda\)-discretization, we have that

\[\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime \prime})}]-\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime\prime}, \lambda(b^{\prime\prime})}]\leq\lambda \tag{53}\]

Hence, we have shown that if \(f_{B}\) is \(\alpha\)-multicalibrated, then for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H},b^{\prime},b^{\prime\prime}\in\Lambda[0,1]\) with \(|\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}|\geq\alpha\cdot\lambda|S_{h^{ \prime}}|\) and \(|\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}|\geq\alpha\cdot\lambda|S _{h^{\prime\prime}}|\), we have

\[P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime})})-P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})})\leq 2\alpha+\lambda\,. \tag{54}\]

Further, note that \((2\alpha+\lambda)/2\cdot\lambda>\alpha\cdot\lambda\) as \(\lambda>0\). This concludes the proof.

Next, we show that, if \(f_{B}\) is \((\alpha,\lambda)\)-aligned, then \(f_{B,\lambda}\) is \(\alpha\)-aligned with respect to \(f_{H}\).

**Theorem 13**.: _For \(\alpha,\lambda>0\), if \(f_{B}\) is \((\alpha,\lambda)\)-aligned with respect to \(f_{H}\), then \(f_{B,\lambda}\) is \(\alpha\)-aligned with respect to \(f_{H}\)._

Proof.: The proof is similar to the proof of Lemma 1 in Hebert-Johnson et al. [11]. Consider all \(\mathcal{S}_{h,\lambda(b)}\) such that \(|\mathcal{S}_{h,\lambda(b)}|<\alpha\lambda|\mathcal{S}_{h}|\). By the \(\lambda\)-discretization, there are at most \(1/\lambda\) such sets, thus, the cardinality of their union is at most \(1/\lambda\alpha\lambda|\mathcal{S}_{h}|=\alpha|\mathcal{S}_{h}|\). Hence, for all \(h\in\mathcal{H}\), there exists a subset \(\tilde{\mathcal{S}}_{h}\subset\mathcal{S}_{h}\) with \(|\tilde{\mathcal{S}}_{h}|\geq(1-\alpha)|\mathcal{S}_{h}|\) such that, for all \(h^{\prime},h^{\prime\prime}\in\mathcal{H}\), with \(h^{\prime}\leq h^{\prime\prime}\), and all \(b^{\prime},b^{\prime\prime}\in\Lambda[0,1]\), with \(b^{\prime}\leq b^{\prime\prime}\), it holds that

\[P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}\cap\tilde{ \mathcal{S}}_{h^{\prime}})-P(Y=1\,\mid\,(X,H)\in\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}\cap\tilde{\mathcal{S}}_{h^{\prime\prime}})\leq \alpha\,. \tag{55}\]

The \(\lambda\)-discretization sets all values of \((x,h)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}\) to \(f_{B,\lambda}(x,h)=\mathbb{E}[f_{B}(X,H)\,\mid\,f_{B}(X,H)\in\lambda(b^{\prime })]\). Note that, for \((x,h)\in\mathcal{S}_{h^{\prime},\lambda(b^{\prime})}\), \(f_{B,\lambda}(x,h)\in\lambda(b^{\prime})\) and for \((x,h)\in\mathcal{S}_{h^{\prime\prime},\lambda(b^{\prime\prime})}\), \(f_{B,\lambda}(x,h)\in\lambda(b^{\prime\prime})\), so it still holds that \(\mathbb{E}[f_{B}(X,H)\,\mid\,f_{B}(X,H)\in\lambda(b^{\prime})]\leq\mathbb{E}[ f_{B}(X,H)\,\mid\,f_{B}(X,H)\in\lambda(b^{\prime\prime})]\). Thus, using Eq. 55, we have that

\[P(Y=1\,\mid\,f_{B}(X,H)=\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in \lambda(b^{\prime})],(X,H)\in\tilde{\mathcal{S}}_{h^{\prime}}) \tag{56}\] \[-P(Y=1\,\mid\,f_{B}(X,H)=\mathbb{E}[f_{B}(X,H)\,\mid\,(X,H)\in \lambda(b^{\prime\prime})],(X,H)\in\tilde{\mathcal{S}}_{h^{\prime\prime}})\leq\alpha\]

This concludes the proof. 

Finally, using Theorems 12 and 13, it readily follows that, given a parameter \(\alpha^{\prime}\), the discretized confidence function \(f_{B,\lambda}\) returned by Algorithm 1 satisfies \((2\alpha^{\prime}+\lambda)\)-aligned calibration with respect to \(f_{H}\).

### Proof Theorem 9

We structure the proof in three parts. We first explain the calibration guarantee that UMD provides and how it relates to human-aligned calibration. Then, we derive a lower bound on the size of the subsets \(\mathcal{D}\cap\mathcal{S}_{h}\) so that the discretized confidence function \(f_{B,\lambda}\) satisfies \(\alpha\)-aligned calibration with respect to \(f_{H}\) with high probability. Finally, building on this result, we derive an upper bound on \(|\mathcal{D}|\) so that \(f_{B,\lambda}\) satisfies \(\alpha\)-aligned calibration with high probability as long as there exists \(\gamma>0\) so that \(P((X,H)\in\mathcal{S}_{h})\geq\gamma\) for all \(h\in\mathcal{H}\).

**Conditional Calibration implies Human-Aligned Calibration.** Running UMD on a dataset \(\mathcal{D}\in(\mathcal{Z}\times\mathcal{Y})^{n}\), where each datapoint is sampled from \(P^{\mathcal{M}}\), guarantees \((\alpha,\xi)\)-conditional calibration, a PAC-style calibration guarantee [12]. Given a dataset \(\mathcal{D}\), a confidence function \(f_{B}\) satisfies \((\alpha,\xi)\)-conditional calibration if, with probability at least \(1-\xi\) over the randomness in \(\mathcal{D}\),

\[\forall b\in[0,1],\quad|P(Y=1|f_{B}(X,H)=b)-b|\leq\alpha\,.\]

This stands in contrast to the definition of \(\alpha\)-calibration, which requires only that the confidence \(f_{B}(X,H)\) is at most \(\alpha\) away from the true probability for \(1-\alpha\) fraction of \(\mathcal{Z}\).

Similarly, using an union bound over all \(h\in\mathcal{H}\), \((\alpha/2,\xi/|\mathcal{H}|)\)-conditional calibration of \(f_{B}\) on each \(\mathcal{S}_{h}\), \(h\in\mathcal{H}\), implies that, with probability at least \(1-\xi\) over the randomness in \(\mathcal{D}\), \(f_{B}\) satisfies that

\[\forall h\in\mathcal{H},\quad\forall b\in[0,1],\quad|P(Y=1|f_{B}(X,H)=b,H=h)- b|\leq\alpha/2\,. \tag{57}\]

Hence, analogously to the proof of Theorem 8, this implies that, with probability at least \(1-\xi\) over the randomness in \(\mathcal{D}\), \(f_{B}\) also satisfies that

\[\forall h,h^{\prime}\in\mathcal{H}, h\leq h^{\prime},\,\,\,\forall b,b^{\prime}\in\mathcal{G},b\leq b^{ \prime}, \tag{58}\] \[P(Y=1|f_{B}(X,H)=b,H=h)-P(Y=1|f_{B}(X,H)=b^{\prime},H=h^{\prime}) \leq\alpha\,.\]

In summary, from Eqs. 57 and 58, we can conclude that \((\alpha/2,\xi/|\mathcal{H}|)\)-conditional calibration of \(f_{B}\) on each \(\mathcal{S}_{h}\), \(h\in\mathcal{H}\), implies that, with probability at least \(1-\xi\), \(f_{B}\) satisfies \(\alpha\)-aligned calibration, where, for all \(h\in\mathcal{H}\), we have that \(\tilde{\mathcal{S}}_{h}=\mathcal{S}_{h}\).

**Lower bound on \(|\mathcal{D}\cap\mathcal{S}_{h}|\) to achieve conditional calibration with UMD.** Running UMD on each partition \(\mathcal{D}\cap\mathcal{S}_{h}\) of \(\mathcal{D}\) induced by \(h\in\mathcal{H}\) achieves \((\alpha/2,\xi/|\mathcal{H}|)\)-conditional calibration as long as each subset \(\mathcal{D}\cap\mathcal{S}_{h}\) of the data is large enough. More specifically, the following lower bound on the size of the subsets \(\mathcal{D}\cap\mathcal{S}_{h}\) readily follows from Theorem 3 in Gupta et al. [12].

**Lemma 4**.: _The discretized confidence function \(f_{B,\lambda}\) returned by \(|\mathcal{H}|\) instances of UMD, one per \(\mathcal{S}_{h}\), is \((\alpha/2,\xi/|\mathcal{H}|)\)-conditional calibrated on \(\mathcal{S}_{h}\) for any \(\xi\in(0,1)\) if_

\[|\mathcal{D}\cap\mathcal{S}_{h}|\geq n_{\text{min}}:=\left(\frac{2\log\left( \frac{2|\mathcal{H}|}{\xi}\cdot\left\lceil\frac{1}{\lambda}\right\rceil\right) }{\alpha^{2}}+2\right)\cdot\left\lceil\frac{1}{\lambda}\right\rceil \tag{59}\]

Proof.: Let \(B\) denote the number of bins in UMD. Theorem 3 in Gupta et al. [12] states that, if \(f_{B}(X,H)\) is absolutely continuous with respect to the Lebesgue measure11 and \(|\mathcal{D}\cap\mathcal{S}_{h}|\geq 2B\), then the discretized confidence function output by UMD is \((\epsilon,\xi^{\prime})\)-conditionally calibrated for any \(\xi^{\prime}\in(0,1)\) and

Footnote 11: If \(f_{B}\) is not continuous with respect to the Lebesgue measure (or equivalently put, \(f_{B}\) does not have a probability density function), a randomization trick can be used to ensure that the results of the theorem hold.

\[\epsilon=\sqrt{\frac{\log(2B/\xi^{\prime})}{2(\lfloor\mathcal{D}\cap\mathcal{ S}_{h}\rfloor/B\rfloor-1)}}\,. \tag{60}\]

Then, for a given \(\alpha\), setting \(\epsilon=\alpha/2\), \(B=\lceil 1/\lambda\rceil\) and \(\xi^{\prime}=\xi/|\mathcal{H}|\), we can solve Eq. 60 for the lower bound on \(|\mathcal{D}\cap\mathcal{S}_{h}|\geq n_{\text{min}}\) with \(n_{\text{min}}\) as defined in Eq. 59. 

**Upper bound on \(|\mathcal{D}|\) to achieve conditional calibration with UMD.** Suppose \(P((X,H)\in\mathcal{S}_{h})\geq\gamma\) for all \(h\in\mathcal{H}\). When \(|\mathcal{H}|\geq 2\), we give an upper bound on \(|\mathcal{D}|\) so that with high probability \(|\mathcal{D}\cap\mathcal{S}_{h}|\geq n_{\text{min}}\) for all \(h\in\mathcal{H}\).

In the process of sampling \(\mathcal{D}\in(\mathcal{Z}\times\mathcal{Y})^{n}\) from \(P^{\mathcal{M}}\), let \(R_{i}^{(h)}=1\) denote the event that the \(i\)-th datapoint \((x_{i},h_{i},y_{i})\) has confidence value \(h\), _i.e._, \(h_{i}=h\). Then, we can express \(|\mathcal{D}\cap\mathcal{S}_{h}|\) in terms of random variable \(R^{(h)}\), defined as

\[R^{(h)}=\sum_{i=1}^{|\mathcal{D}|}R_{i}^{(h)}. \tag{61}\]

Since \(R_{i}^{(h)}\) is a Bernoulli-distributed variable with \(P(R_{i}^{(h)})=P((X,H)\in\mathcal{S}_{h})\), the expected value of \(R^{(h)}\) is \(\mu(h):=\mathbb{E}[R^{(h)}]=P((X,H)\in\mathcal{S}_{h})\cdot|\mathcal{D}|\geq \gamma\cdot|\mathcal{D}|\).

Let \(|\mathcal{D}|=2\cdot|\mathcal{H}|\cdot\log(2/\xi)\cdot 1/\gamma\cdot n_{\text{min}}\), observe that in this case

\[P(R^{(h)}\leq n_{\text{min}})=P\left(R^{(h)}\leq\frac{\gamma}{2|\mathcal{H}| \cdot\log(2/\xi)}\cdot|\mathcal{D}|\right).\]

For \(|\mathcal{H}|\geq 2\) and \(\xi\in(0,1)\), we have \(1/(2|\mathcal{H}|\cdot\log(2/\xi))\in(0,1)\) and we can use a variation of the Chernoff bound to show

\[P(R^{(h)}\leq n_{\text{min}}) \leq P\left(R^{(h)}\leq\frac{1}{2|\mathcal{H}|\cdot\log(2/\xi)} \cdot\mu(h)\right)\] \[\leq e^{-\mu(h)\left(\frac{2|\mathcal{H}|\cdot\log(2/\xi)-1}{2| \mathcal{H}|\cdot\log(2/\xi)}\right)^{2}\cdot\frac{1}{2}}\] \[=e^{-\mu(h)\cdot\frac{1}{2}\left(1-\frac{1}{|\mathcal{H}|\cdot \log(2/\xi)}+\frac{1}{2(2|\mathcal{H}|\cdot\log(2/\xi))^{2}}\right)}\] \[\leq\frac{\xi}{2}\cdot e^{-|\mathcal{H}|\cdot n_{\text{min}}\left( \frac{1}{2}-\frac{1}{|\mathcal{H}|\cdot\log(2/\xi)}+\frac{1}{2(2|\mathcal{H}| \cdot\log(2/\xi))^{2}}\right)},\]

where the first and last inequality results from using \(\mu(h)>\gamma\cdot|\mathcal{D}|\). We can now use a union bound to obtain a lower bound on the probability that for any \(h\in\mathcal{H}\), \(|\mathcal{D}\cap\mathcal{S}_{h}|\leq n_{\text{min}}\), _i.e._,

\[P(\exists h\in\mathcal{H}:\ |\mathcal{D}\cap\mathcal{S}_{h}|\leq n_{\text{min}}) \leq\frac{\xi}{2}\cdot|\mathcal{H}|\cdot e^{-|\mathcal{H}|\cdot n_{\text{min }}\cdot\left(\frac{1}{2}-\frac{1}{2|\mathcal{H}|\cdot\log(2/\xi)}+\frac{1}{2( 2|\mathcal{H}|\cdot\log(2/\xi))^{2}}\right)} \tag{62}\]

One can verify that for \(|\mathcal{H}|\geq 2\) and \(n_{\text{min}}\geq 1\), we have \(P(\exists h\in\mathcal{H}:\ |\mathcal{D}\cap\mathcal{S}_{h}|\leq n_{\text{min}})\leq \frac{\xi}{2}\). Hence, if \(|\mathcal{D}|=2\cdot|\mathcal{H}|\cdot\log(2/\xi)\cdot 1/\gamma\cdot n_{\text{min}}\), then, for all \(h\in\mathcal{H}\), \(|\mathcal{D}\cap\mathcal{S}_{h}|\leq n_{\text{min}}\) with probability \(1-\xi/2\).

Combining this result and Lemma 4, we have that the discretized confidence function \(f_{B,\lambda}\) returned by \(|\mathcal{H}|\) instances of UMD, one per \(\mathcal{S}_{h}\), is \((\alpha/2,\xi/(2|\mathcal{H}|))\)-conditional calibrated on each \(\mathcal{S}_{h}\) with probability at least \(1-\xi/2\) for any \(\xi\in(0,1)\) if

\[|\mathcal{D}|=2\cdot|\mathcal{H}|\cdot\frac{\log(2/\xi)}{\gamma}\cdot\left( \frac{2\log\left(\frac{4|\mathcal{H}|}{\xi}\cdot\left\lceil\frac{1}{\lambda} \right\rceil\right)}{\alpha^{2}}+2\right)\cdot\left\lceil\frac{1}{\lambda}\right\rceil \tag{63}\]

Finally, using a union bound, we can conclude that \(f_{B,\lambda}\) achieves \(\alpha\)-aligned calibration with respect to \(f_{H}\) with probability at least \(1-\xi\) from

\[|\mathcal{D}|=O\left(|\mathcal{H}|\cdot\frac{\log(|\mathcal{H}|/\xi\lambda)}{ \alpha^{2}\cdot\lambda\cdot\gamma}\right)\]

samples. This concludes the proof.

Multicalibration Algorithm

In this section, we give a high-level description of the post-processing algorithm for multicalibration introduced by Hebert-Johnson et al. [11]. The algorithm works with a discretization of \([0,1]\) into uniform sized bins of size \(\lambda\), for a \(\lambda>0\). Formally the \(\lambda\)-discretization of \([0,1]\), is defined as

**Definition 14** (\(\lambda\)-discretization [11]).: _Let \(\lambda>0\). The \(\lambda\)-discretization of \([0,1]\), denoted by \(\Lambda[0,1]=\{\frac{\lambda}{2},\frac{3\lambda}{2},\ldots,1-\frac{\lambda}{2}\}\), is the set of \(1/\lambda\) evenly spaced real values over \([0,1]\). For \(b\in\Lambda[0,1]\), let_

\[\lambda(b)=[b-\lambda/2,v+\lambda/2) \tag{64}\]

_be the \(\lambda\)-interval centered around \(b\) (except for the final interval, which will be \([1-\lambda,1]\))._

It starts by partitioning each subspace \(\mathcal{S}_{h}\) into \(1/\lambda\) groups \(\mathcal{S}_{h,\lambda(b)}=\{(x,h)\in\mathcal{S}_{h}\;\mid\;f_{B}(x,h)\in \lambda(b)\}\), with \(b\in\Lambda[0,1]\). Then, it repeatedly looks for a large enough group \(\mathcal{S}_{h,\lambda(b)}\) such that the absolute difference between the average confidence value \(\mathbb{E}[f_{B}(X,H)\;\mid\;(X,H)\in\mathcal{S}_{h,\lambda(b)}]\) and the probability \(P(Y=1\;\mid\;(X,H)\in\mathcal{S}_{h,\lambda(b)})\) is larger than \(\alpha\) and, if it finds it, it updates the confidence value \(f_{B}(x,h)\) of each \((x,h)\in\mathcal{S}_{h,\lambda(b)}\) by this difference. Once the algorithm cannot find any more such a group, it returns a discretized confidence function \(f_{B,\lambda}(x,h)=\mathbb{E}[f_{B}(X,H)\;\mid\;f_{B}(X,H)\in\lambda(b)]\), with \(b\in\Lambda[0,1]\) such that \(f_{B}(x,h)\in\lambda(b)\), which is guaranteed to satisfy \((\alpha+\lambda)\)-multicalibration.

Algorithm 1 provides a pseudocode implementation of the overall algorithm. Within the implementation, it is worth noting that the expectations and probabilities can be estimated with fresh samples from the distribution or from a fixed dataset using tools from differential privacy and adaptive data analysis, as discussed in Hebert-Johnson et al. [11].

```
1:Input: confidence function \(f_{B}\), parameters \(\alpha,\lambda>0\)
2:Output: confidence function \(f_{B,\lambda}\)
3:repeat
4: updated \(\leftarrow\)false
5:for\(\mathcal{S}_{h}\in\mathcal{C}\)\(\&\)\(b\in\Lambda[0,1]\)do
6:\(\mathcal{S}_{h,\lambda(b)}\leftarrow\mathcal{S}_{h}\cap\{(x,h)\in\mathcal{Z} \;\mid\;f_{B}(x,h)\in\lambda(b)\}\)
7:if\(P((X,H)\in\mathcal{S}_{h,\lambda(b)})<\alpha\lambda\cdot P((X,H)\in\mathcal{S} _{h})\)then
8:continue
9:\(\bar{b}_{h,\lambda(b)}\leftarrow\mathbb{E}[f_{B}(X,H)\;\mid\;(X,H)\in\mathcal{S }_{h,\lambda(b)}]\)
10:\(r_{h,\lambda(b)}\gets P(Y=1\;\mid\;(X,H)\in\mathcal{S}_{h,\lambda(b)})\)
11:if\(|r_{h,\lambda(b)}-\bar{b}_{h,\lambda(b)}|>\alpha\)then
12: updated \(\leftarrow\)true
13:for\((x,h)\in\mathcal{S}_{h,\lambda(b)}\)do
14:\(f_{B}(x,h)\gets f_{B}(x,h)+(r_{h,\lambda(b)}-\bar{b}_{h,\lambda(b)})\) {project into \([0,1]\) if necessary}
15:until updated \(=\)false
16:for\(b\in\Lambda[0,1]\)do
17:\(\bar{b}_{\lambda(b)}\leftarrow\mathbb{E}[f_{B}(X,H)|f_{B}(X,H)\in\lambda(b)]\)
18:for\((x,h)\in\mathcal{Z}\;:f_{B}(x,h)\in\lambda(b)\)do
19:\(f_{B,\lambda}(x,h)\gets b_{\lambda(b)}\)
20:return\(f_{B,\lambda}\)
```

**Algorithm 1** Post-processing algorithm for \((\alpha+\lambda)\)-multicalibration

[MISSING_PAGE_EMPTY:28]