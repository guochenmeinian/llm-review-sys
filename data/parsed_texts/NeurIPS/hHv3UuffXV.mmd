# Block Broyden's Methods for Solving Nonlinear Equations

 Chengchang Liu

Department of Computer Science \(\&\) Engineering

The Chinese University of Hong Kong

7liuchengchang@gmail.com

&Cheng Chen\({}^{*}\)

Shanghai Key Laboratory of Trustworthy Computing

East China Normal University

chchen@sei.ecnu.edu.cn

&Luo Luo

School of Data Science

Fudan University

luoluo@fudan.edu.cn

&John C.S. Lui

Department of Computer Science \(\&\) Engineering

The Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

The corresponding author

###### Abstract

This paper studies quasi-Newton methods for solving nonlinear equations. We propose block variants of both good and bad Broyden's methods, which enjoy explicit local superlinear convergence rates. Our block good Broyden's method has a faster condition-number-free convergence rate than existing Broyden's methods because it takes the advantage of multiple rank modification on Jacobian estimator. On the other hand, our block bad Broyden's method directly estimates the inverse of the Jacobian provably, which reduces the computational cost of the iteration. Our theoretical results provide some new insights on why good Broyden's method outperforms bad Broyden's method in most of the cases. The empirical results also demonstrate the superiority of our methods and validate our theoretical analysis.

## 1 Introduction

In this paper, we consider solving the following nonlinear equation systems:

\[\mathbf{F}(\mathbf{x})=\mathbf{0},\] (1)

where \(\mathbf{x}\in\mathbb{R}^{d}\), \(\mathbf{F}(\mathbf{x})\stackrel{{\text{def}}}{{=}}[F_{1}( \mathbf{x}),\cdots,F_{d}(\mathbf{x})]^{\top}:\mathbb{R}^{d}\rightarrow\mathbb{ R}^{d}\) and each \(F_{i}(\mathbf{x})\) is differentiable. Solving nonlinear equations is one of the most important problems in scientific computing [38]. It has various applications including machine learning [3, 4, 12, 15, 44], game theory [18, 39], economics [2] and control systems [5, 37].

Newton's method and its variants [16, 25, 26] such as the Gauss-Newton method [19, 38], the Levenberg-Marquart method [17, 28, 34, 36] and the trust region method [40, 53] are widely adopted to solve the systems of nonlinear equations. These methods usually enjoy fast local superlinear rates.

Newton's method takes iterates of form

\[\mathbf{x}_{t+1}=\mathbf{x}_{t}-(\mathbf{J}(\mathbf{x}_{t}))^{-1}\mathbf{F}( \mathbf{x}_{t}),\]

where \(\mathbf{J}(\mathbf{x})\in\mathbb{R}^{d\times d}\) is the Jacobian at \(\mathbf{x}\). Since computing the inverse of the exact Jacobian matrix requires \(\mathcal{O}(d^{3})\) running time, Newton's method suffers from expensive computation especially when solving the large-scale nonlinear equations [20, 47, 54].

Quasi-Newton methods have been proposed for avoiding the heavy computational cost of Newton-type methods while preserving good local convergence behaviour [7, 8, 9, 10, 11, 13, 24, 31, 32, 41, 42, 43, 45, 51]. Among these quasi-Newton methods, the Broyden's methods [6], including the good and the bad schemes [1, 30, 35], are considered to be the most effective methods for solving nonlinear equations. The Broyden's good method2 approximates the Jacobian \(\mathbf{J}(\mathbf{x}_{t})\) by an estimator \(\mathbf{B}_{t}\) and updates the Jacobian estimator in each round as \(\mathbf{B}_{t+1}=\mathbf{B}_{t}+\mathbf{\Delta}_{t}\). Here \(\mathbf{\Delta}_{t}\) is a rank-\(1\) updating matrix constructed by the curvature information. Broyden et al. [11], Kelley and Sachs [27] proved that the good Broyden's method can achieve asymptotic local superlinear rates.

Footnote 2: We use the names “good Broyden’s method” and “bad Broyden’s method” by following the previous literature [1, 10, 22, 35].

The bad Broyden's method approximates the inverse of the Jacobian by \(\mathbf{H}_{t}\) and updates the approximate matrix directly. Although the bad Broyden's method enjoys less computational cost than good Broyden's method in each iteration, it does not perform as well as the good method in most cases [11]. Lin et al. [29] show that both the good and bad Broyden's methods have superlinear rates of \(\mathcal{O}((1/\sqrt{t})^{t})\) and provide some insights on the difference between their empirical performance.

Ye et al. [50] proposed a new variant of good Broyden's method by conducting \(\mathbf{\Delta}_{t}\) with a greedy or random strategy. Their method achieves a better explicit convergence rate of \(\mathcal{O}((1-1/d)^{t(t-1)/4})\). However, it remains unknown whether this convergence rate can be further improved by leveraging block updates which increase the reuse rate of the data in cache and take advantage of parallel computing [14]. Gower and Richtarik [21] studied several random quasi-Newton updates including the Broyden's updates for approximating the inverse of matrices, but they only provide implicit linear rates for their methods. Liu et al. [33] established explicit convergence rates for several block quasi-Newton updates, but they focus on approximating positive definite matrices.

In this paper, we propose two random block Broyden's methods for solving nonlinear equations and provide their explicit superlinear convergence rates. We compare the theoretical results of proposed methods with existing Broyden's methods in Table 1 and summarize our contribution as follows:

* We provide explicit convergence rates for the block good Broyden's udpate and the block bad Broyden's update proposed by Gower and Richtarik [21]. Our results show that the block good Broyden's update can approximate a nonsingular matrix \(\mathbf{A}\) with a linear rate of \((1-k/d)^{t}\) which improves the previous rate of \((1-1/d)^{t}\) where \(k\stackrel{{\mathrm{def}}}{{=}}\mathrm{rank}(\mathbf{\Delta}_{t})\). We also show that the "bad" update can approximate the inverse matrix \(\mathbf{A}^{-1}\) with an linear rate of \((1-k/(d\hat{\kappa}^{2}))^{t}\) where \(\hat{\kappa}\) is the condition number of \(\mathbf{A}\). To the best of our knowledge, this is the first explicit convergence rate for the block bad Broyden's update.
* We propose the block good Broyden's method with convergence rate \(\mathcal{O}((1-k/d)^{t(t-1)/4})\) where \(k\) is the rank of the updating matrix \(\mathbf{\Delta}_{t}\). This rate reveals the advantage of block update and improves previous results. Our method also relaxes the initial conditions stated in Ye et al. [50].
* We propose the block bad Broyden's method with convergence rate \(\mathcal{O}((1-k/(4dk^{2}))^{t(t-1)/4})\). We also study the initial conditions of two proposed block variants. Our analysis shows that bad Broyden's method is only suitable for the cases where the condition number of the Jacobian is small, while good Broyden's method performs well in most cases.

Paper OrganizationIn Section 2, we introduce the notation and assumptions as the preliminaries of this paper. In Section 3, we introduce the block good or bad Broyden's updates for approximating the general matrix. In Section 4, we propose the block good or bad Broyden's methods with explicit local superlinear rates. In Section 5, we discuss the behavior difference of the good and bad methods. We validate our methods by numerical experiments in Section 6. Finally, we conclude our results in Section 7. All proofs are deferred to appendix.

## 2 Preliminaries

We let \([d]\stackrel{{\text{def}}}{{=}}\{1,2\cdots,d\}\). We use \(\|\cdot\|_{F}\) to denote the Frobenius norm of a given matrix, \(\|\cdot\|_{2}\) to denote the spectral norm of a vector and Euclidean norm of a matrix respectively. The standard basis for \(\mathbb{R}^{d}\) is presented by \(\{\mathbf{e}_{1},\cdots,\mathbf{e}_{d}\}\) and \(\mathbf{I}_{d}\) is the identity matrix. We denote the trace, the largest singular value, and the smallest singular value of a matrix by \(\operatorname{tr}\left(\cdot\right)\), \(\sigma_{\min}(\cdot)\), and \(\sigma_{\max}(\cdot)\) respectively.

We use \(\mathbf{x}_{*}\) to denote the solution of the nonlinear equation (1) and \(\mathbf{J}_{*}\) to denote the Jacobian matrix at \(\mathbf{x}_{*}\), i.e., \(\mathbf{J}_{*}\stackrel{{\text{def}}}{{=}}\mathbf{J}(\mathbf{x}_ {*})\). We let \(\mu\stackrel{{\text{def}}}{{=}}\sigma_{\min}(\mathbf{J}( \mathbf{x}_{*}))\), \(L\stackrel{{\text{def}}}{{=}}\sigma_{\max}(\mathbf{J}(\mathbf{x}_ {*}))\) and then define the condition number of \(\mathbf{J}_{*}\) as \(\kappa\stackrel{{\text{def}}}{{=}}L/\mu\). We also use \(\hat{\kappa}\stackrel{{\text{def}}}{{=}}\sigma_{\max}(\mathbf{A} )/\sigma_{\min}(\mathbf{A})\) to present the condition number of given matrix \(\mathbf{A}\).

Then we present two standard assumptions on the nonlinear equations (1), which is widely used in previous works [16; 29; 50].

**Assumption 2.1**.: The solution \(\mathbf{x}_{*}\) of the nonlinear equation (1) is unique and nondegenerate, i.e.,

\[\mu\stackrel{{\text{def}}}{{=}}\sigma_{\min}(\mathbf{J}_{*})>0.\]

**Assumption 2.2**.: The Jacobian \(\mathbf{J}(\mathbf{x})\) satisfies

\[\|\mathbf{J}(\mathbf{x})-\mathbf{J}_{*}\|_{2}\leq M\|\mathbf{x}-\mathbf{x}_{*} \|_{2}\ \ \text{for all}\ \ \ \mathbf{x}\in\mathbb{R}^{d}.\] (2)

The following proposition shows that if \(\mathbf{x}\) is in some local region of \(\mathbf{x}_{*}\), the Jacobian matrix \(\mathbf{J}(\mathbf{x})\) has a bounded condition number.

**Proposition 2.3**.: _Suppose Assumptions 2.1 and 2.2 hold. For all \(\mathbf{x}\) satisfies \(\|\mathbf{x}-\mathbf{x}_{*}\|_{2}\leq\mu^{2}/(6LM)\), we have_

\[\sigma_{\min}(\mathbf{J}(\mathbf{x}))\geq\frac{\mu}{\sqrt{2}}\qquad\text{and} \qquad\sigma_{\max}(\mathbf{J}(\mathbf{x}))\leq\sqrt{2}L.\]

We present two notations for the block Broyden's Update.

**Definition 2.4** (Block Good Broyden's Update).: Let \(\mathbf{A}\), \(\mathbf{B}\in\mathbb{R}^{d\times d}\). For any full column rank matrix \(\mathbf{U}\in\mathbb{R}^{d\times k}\), we define

\[\text{Block-G-Broyden}(\mathbf{B},\mathbf{A},\mathbf{U})\triangleq\mathbf{B}+( \mathbf{A}-\mathbf{B})\mathbf{U}\left(\mathbf{U}^{\top}\mathbf{U}\right)^{-1} \mathbf{U}^{\top}.\] (3)

**Definition 2.5** (Block Bad Broyden's Update).: Let \(\mathbf{A}\), \(\mathbf{H}\in\mathbb{R}^{d\times d}\). For any full column rank matrix \(\mathbf{U}\in\mathbb{R}^{d\times k}\), we define

\[\text{Block-B-Broyden}(\mathbf{H},\mathbf{A},\mathbf{U})\triangleq\mathbf{H}+( \mathbf{I}_{d}-\mathbf{H}\mathbf{A})\mathbf{U}(\mathbf{U}^{\top}\mathbf{A}^{ \top}\mathbf{A}\mathbf{U})^{-1}\mathbf{U}^{\top}\mathbf{A}^{\top}.\] (4)

## 3 The Block Broyden's Updates for Approximating Matrices

In this section, we provide the linear convergence rates of the block good and bad Broyden's updates for approximating matrices. The theoretical results is summarized in Table 2.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Methods** & \(\mathbf{rank}(\boldsymbol{\Delta}_{t})\) & **Convergence Rate** \\ \hline Good/Bad Broyden’s Method [1; 6; 29] & \(1\) & \(\mathcal{O}\left(1/t^{t/2}\right)\) \\ \hline Greedy/Randomized Good Broyden’s Method [50] & \(1\) & \(\mathcal{O}\big{(}(1-1/d)^{t(t-1)/4}\big{)}\) \\ \hline Block Good Broyden’s Method & \(k\in[d-1]\) & \(\mathcal{O}\big{(}(1-k/d)^{t(t-1)/4}\big{)}\) \\ Algorithm 1 & \(k\in[d]\) & \(\mathcal{O}\big{(}(1-k/(4\kappa^{2}d))^{t(t-1)/4}\big{)}\) \\ \hline Block Bad Broyden’s Method & \(k\in[d]\) & \(\mathcal{O}\big{(}(1-k/(4\kappa^{2}d))^{t(t-1)/4}\big{)}\) \\ Algorithm 2 & \(k\in[d]\) & \(\mathcal{O}\big{(}(1-k/(4\kappa^{2}d))^{t(t-1)/4}\big{)}\) \\ \hline \end{tabular}
\end{table}
Table 1: We summarize the properties of Broyden’s methods for solving the Nonlinear equationsThe block good Broyden's update, which aims to compute an approximation of matrix \(\mathbf{A}\), can be written as:

\[\mathbf{B}_{t+1}=\text{Block-G-Broyden}(\mathbf{B}_{t},\mathbf{A},\mathbf{U}_{t}).\]

The following theorem presents a linear convergence rate of \((1-k/d)^{t}\) which is better than the rate \((1-1/d)^{t}\) provided by Gower and Richtarik [21], Ye et al. [50].

**Theorem 3.1**.: _Assume that \(\mathbf{A}\in\mathbb{R}^{d\times d}\) and \(\mathbf{B}_{0}\in\mathbb{R}^{d\times d}\). If we select \(\mathbf{U}_{t}=[\mathbf{e}_{i_{1}},\mathbf{e}_{i_{2}},\cdots,\mathbf{e}_{i_{ k}}]\in\mathbb{R}^{d\times k}\), where \(\{i_{1},\cdots,i_{k}\}\) are uniformly chosen from \(\{1,2,\cdots,d\}\) without replacement at each round, then for any nonsingular matrix \(\mathbf{C}\in\mathbb{R}^{d\times d}\), the block good Broyden's update satisfies_

\[\|\mathbf{C}(\mathbf{B}_{t+1}-\mathbf{A})\|_{F}^{2}\leq\| \mathbf{C}(\mathbf{B}_{t}-\mathbf{A})\|_{F}^{2},\] (5)

_and_

\[\mathbb{E}\big{[}\|\mathbf{C}(\mathbf{B}_{t}-\mathbf{A})\|_{F}^ {2}\big{]}\leq\left(1-\frac{k}{d}\right)^{t}\|\mathbf{C}(\mathbf{B}_{0}- \mathbf{A})\|_{F}^{2}.\] (6)

On the other hand, the bad Broyden's update which targets to approximate \(\mathbf{A}^{-1}\) can be written as:

\[\mathbf{H}_{t+1}=\text{Block-B-Broyden}(\mathbf{H}_{t},\mathbf{A}, \mathbf{U}_{t}).\]

Gower and Richtarik [21] provide an implicit rate of \((1-\rho)^{t}\) for the above scheme with \(\rho\in[0,k/d]\), but their analysis cannot guarantee an explicit \(\rho\). In the following theorem, we show that the block bad Broyden's update can approximate \(\mathbf{H}_{t}\) to \(\mathbf{A}^{-1}\) with an explicit linear rate of \((1-k/(\hat{\kappa}^{2}d))^{t}\).

**Theorem 3.2**.: _Assume that \(\mathbf{A}\in\mathbb{R}^{d\times d}\) and \(\mathbf{H}_{0}\in\mathbb{R}^{d\times d}\). If we select \(\mathbf{U}_{t}=[\mathbf{e}_{i_{1}},\mathbf{e}_{i_{2}},\cdots,\mathbf{e}_{i_{ k}}]\in\mathbb{R}^{d\times k}\) where \(\{i_{1},\cdots,i_{k}\}\) are uniformly chosen from \(\{1,2,\cdots,d\}\) without replacement at each round, then for any nonsingular matrix \(\mathbf{C}\in\mathbb{R}^{d\times d}\), the block bad Broyden's update satisfies_

\[\|\mathbf{C}(\mathbf{H}_{t+1}-\mathbf{A}^{-1})\|_{F}^{2}\leq \|\mathbf{C}(\mathbf{H}_{t}-\mathbf{A}^{-1})\|_{F}^{2},\] (7)

_and_

\[\mathbb{E}\big{[}\|\mathbf{C}(\mathbf{H}_{t}-\mathbf{A}^{-1})\|_ {F}^{2}\big{]}\leq\left(1-\frac{k}{d\hat{\kappa}^{2}}\right)^{t}\|\mathbf{C}( \mathbf{H}_{0}-\mathbf{A}^{-1})\|_{F}^{2}.\] (8)

_Remark 3.3_.: If we choose \(\mathbf{C}=\mathbf{I}_{d}\) in Theorem 3.1 and Theorem 3.2, then the measures in these two theorems are exactly the same as the one in Section 8.5 and Section 8.3 of [21]. Besides, the rate of Theorem 3.1 recovers the convergent rates of Section 8.5 in [21] and Lemma 4.1 in [50] when we take \(k=1\).

## 4 The Block Broyden's Methods

In this section, we propose two block Broyden's methods for solving the nonlinear equation (1). We present our algorithms in section 4.1 and the corresponding convergence results in Section 4.2.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Updates** & **Previous Results** & 
\begin{tabular}{c} **Improved Results** \\ Theorem 3.1/3.2 \\ \end{tabular} & **Measure** \\ \hline Block Good & \(\left(1-\frac{1}{d}\right)^{t}\)[21, 50]\({}^{\text{(a)}}\) & \(\left(1-\frac{k}{d}\right)^{t}\) & \(\mathbb{E}\left[\|(\mathbf{B}_{t}-\mathbf{A})\|_{F}^{2}\right]\) \\ \hline Block Bad & \(\left(1-\rho\right)^{t}\)[21]\({}^{\text{(b)}}\) & \(\left(1-\frac{k}{d\hat{\kappa}^{2}}\right)^{t}\) & \(\mathbb{E}\left[\|(\mathbf{H}_{t}-\mathbf{A}^{-1})\|_{F}^{2}\right]\) \\ \hline \multicolumn{4}{l}{(a). the result holds for \(k=1\) and it is unknown when \(k>1\).} \\ \multicolumn{4}{l}{(b). Gower and Richtarik [21] only prove that \(\rho\in[0,\frac{k}{d}]\), but do not provide the explicit value of \(\rho\).} \\ \end{tabular}
\end{table}
Table 2: We summarize the properties of Broyden’s updates for approximating a given nonsingular matrix \(\mathbf{A}\) or \(\mathbf{A}^{-1}\).

### Algorithms

By using the block Broyden's updates in Section 3, we propose two novel algorithms called Block Good Broyden's Method (BGB) and Block Bad Broyden's Method (BBB) for solving nonlinear equations.

We present the BGB algorithm in Algorithm 1 which updates the Jacobian estimator \(\mathbf{B}_{t}\) by the block good Broyden's update in each iteration. Notice that the inverse of \(\mathbf{B}_{t}\) can be computed efficiently by adopting Sherman-Morrison-Woodbury formula [46]. On the other hand, the BBB algorithm, which is presented in Algorithm 2, approximates the inverse of the Jacobian directly by using the block bad Broyden's update. It usually has a lower computational cost than the BGB algorithm in each round because the BBB algorithm does not need to compute the inverse of the estimator \(\mathbf{H}_{t}\).

```
1:Input: Initial estimator \(\mathbf{B}_{0}\), initial point \(\mathbf{x}_{0}\) and block size \(k\).
2:for\(t=0,1\ldots\)
3:\(\mathbf{x}_{t+1}=\mathbf{x}_{t}-\mathbf{H}_{t}\mathbf{F}(\mathbf{x}_{t})\).
4: Choose \(\{i_{1},\cdots,i_{k}\}\) by uniformly select \(k\) items from \(\{1,\cdots,d\}\) without replacement.
5:\(\mathbf{U}_{t}=[\mathbf{e}_{i_{1}},\cdots,\mathbf{e}_{i_{k}}]\in\mathbb{R}^{d \times k}\).
6:\(\mathbf{H}_{t+1}=\text{Block-B-Broyden}(\mathbf{H}_{t},\mathbf{J}(\mathbf{x}_{t +1}),\mathbf{U}_{t})\).
7:endfor ```

**Algorithm 2** Block Bad Broyden's Method (BBB)

_Remark 4.1_.: Algorithms 1 and 2 do not require full information of the Jacobian. We construct \(\mathbf{U_{t}}\) by subsampling the columns of the identity matrix. When updating the Jacobian estimator by the block updates, we need to calculate \(\mathbf{J}_{t+1}\mathbf{U}_{t}\) which is only the partial information of \(\mathbf{J}_{t+1}\) (columns of \(\mathbf{J}_{t+1}\)). Since we have \(k\ll d\), it is not expensive to access the partial information of the Jacobian.

### Convergence Analysis for the Block Broyden's Methods

We provide the convergence analysis for Algorithm 1 and Algorithm 2 in Section 4.2.1 and Section 4.2.2 respectively. We denote the Jabocbian matrix at \(\mathbf{x}_{t}\) as \(\mathbf{J}_{t}\). As previous works [16; 29; 50], we make an assumption on the estimator matrices in Algorithm 1 and Algorithm 2 as follows:

**Assumption 4.2**.: We assume the sequence \(\{\mathbf{B}_{t}\}_{t=0}^{\infty}\) generated by Algorithm 1 (and \(\{\mathbf{H}_{t}\}_{t=0}^{\infty}\) generated by Algorithm 2) are well-defined and nonsingular.

#### 4.2.1 Analysis for Block Good Broyden's Methods

In this subsection, we use the following measures for our convergence analysis,

\[r_{t}\stackrel{{\text{def}}}{{=}}\|\mathbf{x}_{t}-\mathbf{x}_{*} \|_{2}\quad\text{ and }\quad\sigma_{t}\stackrel{{\text{def}}}{{=}}\| \mathbf{J}_{*}^{-1}(\mathbf{B}_{t}-\mathbf{J}_{*})\|_{F}.\]

The \(r_{t}\) measures the distance between \(\mathbf{x}_{t}\) and the solution \(\mathbf{x}_{*}\) and \(\sigma_{t}\) measures how well does the estimator matrix \(\mathbf{B}_{t}\) approximate the Jacobian at \(\mathbf{x}_{*}\).

The following lemma provides upper bound of \(\sigma_{t}\) after one block Broyden's update.

**Lemma 4.3**.: _Performing Algorithm 1 under Assumptions 2.1, 2.2 and 4.2, we have_

\[\sigma_{t+1}\leq\sigma_{t}+\frac{2M\sqrt{d}}{\mu}r_{t+1}\qquad\text{ and}\qquad\mathbb{E}[\sigma_{t+1}]\leq\sqrt{1-\frac{k}{d}}\cdot\sigma_{t}+\frac{2M \sqrt{d}}{\mu}\cdot r_{t+1}.\] (9)

Based on Lemma 4.3, we present the superlinear convergence rate for Algorithm 1.

**Theorem 4.4**.: _Suppose Assumptions 2.1, 2.2 and 4.2 hold and the initial condition of Algorithm 1 satisfies_

\[\frac{2M\sqrt{d}r_{0}}{\mu}\leq\min\left\{\frac{(1-q)(d-k)}{4(1+q)d},\frac{q}{ 4(1+q)}\right\}\quad\text{and}\quad\sigma_{0}\leq\frac{q}{2(1+q)}\] (10)

_for arbitary \(q\in(0,1)\). Then for any \(k\in[d-1]\), the output of Algorithm 1 satisfies_

\[\mathbb{E}\left[\|\mathbf{J}_{*}^{-1}(\mathbf{B}_{t}-\mathbf{J}_{*})\|_{F} \right]\leq 2\mathrm{e}\left(1-\frac{k}{d}\right)^{t/2},\]

_and_

\[\mathbb{E}\left[\frac{\|\mathbf{x}_{t+1}-\mathbf{x}_{*}\|_{2}}{\|\mathbf{x}_ {t}-\mathbf{x}_{*}\|_{2}}\right]\leq 4\mathrm{e}\left(1-\frac{k}{d}\right)^{t/2}.\]

Theorem 4.4 implies the following high probability bound for Algorithm 1.

**Corollary 4.5**.: _Performing Algorithm 1 under the same assumption and initial condition as Theorem 4.4, with probability at least \(1-\delta\), we have_

\[\|\mathbf{J}_{*}^{-1}(\mathbf{B}_{t}-\mathbf{J}_{*})\|_{F}\leq\frac{4 \mathrm{e}d^{2}}{k^{2}\delta}\left(1-\frac{k}{d+k}\right)^{t/2},\] (11)

_and_

\[\|\mathbf{x}_{t}-\mathbf{x}_{*}\|_{2}\leq\left(\frac{8\mathrm{e}d^{2}}{k^{2} \delta}\right)^{t}\left(1-\frac{k}{d+k}\right)^{t(t-1)/4}\|\mathbf{x}_{0}- \mathbf{x}_{*}\|_{2}.\] (12)

Comparison with [50]Compare Theorem 4.4 with Theorem 4.3 of [50], we can find that the convergence rate of our BGB algorithm is better than greedy and randomized good Broyden's methods [50] if we choose \(k>1\).

On the other hand, the initial condition of greedy and randomized good Broyden's methods [50] is

\[\|\mathbf{x}_{0}-\mathbf{x}_{*}\|_{2}=\mathcal{O}\left(\frac{\mu}{M\sqrt{d}} \right)\ \ \text{and}\ \ \|\mathbf{B}_{0}-\mathbf{J}_{0}\|_{F}=\mathcal{O}\left(\mu\right),\] (13)

while the condition of Theorem 4.4 can be reformulated as

\[\|\mathbf{x}_{0}-\mathbf{x}_{*}\|_{2}=\mathcal{O}\left(\frac{\mu}{M\sqrt{d}} \right)\ \ \text{and}\ \ \|\mathbf{J}_{*}^{-1}(\mathbf{B}_{0}-\mathbf{J}_{*})\|_{F}=\mathcal{O} \left(1\right).\] (14)

Since

\[\|\mathbf{J}_{*}^{-1}(\mathbf{B}_{0}-\mathbf{J}_{*})\|_{F} \leq\|\mathbf{J}_{*}^{-1}(\mathbf{J}_{*}-\mathbf{J}_{0})\|_{F}+ \|\mathbf{J}_{*}^{-1}(\mathbf{B}_{0}-\mathbf{J}_{0})\|_{F}\] \[\leq\frac{M\sqrt{d}}{\mu}\|\mathbf{x}_{0}-\mathbf{x}_{*}\|_{2}+ \frac{1}{\mu}\|\mathbf{B}_{0}-\mathbf{J}_{0}\|_{F}=\mathcal{O}(1),\]

condition (13) can implies condition (14). However, the reverse is not always true. For example, we can choose \(\mathbf{B}_{0}=1.5\mathbf{J}_{*}\) and suppose

\[\mathbf{J}_{0}=\mathbf{J}_{*}=\begin{bmatrix}3&0\\ 0&10^{-10}\end{bmatrix}.\]

Then we have \(\|\mathbf{J}_{*}^{-1}(\mathbf{B}_{0}-\mathbf{J}_{*})\|_{F}=\|\frac{1}{2} \mathbf{I}_{2}\|_{F}=\mathcal{O}(1)\) while \(\|\mathbf{B}_{0}-\mathbf{J}_{0}\|_{F}=\|\frac{1}{2}\mathbf{J}_{*}\|_{F}\gg 10 ^{-10}=\mu\).

Overall, compared with the greedy or randomized good Broyden's method [50], Theorem 4.4 not only gives a faster convergence superlinear rate by leveraging the idea of block update, but also weakens the initial condition by using different measures in the analysis.

#### 4.2.2 Analysis for Block Bad Broyden's Methods

This subsection gives the convergence analysis for Algorithm 2. We use the following measures to describe the convergent behavior

\[R_{t}\stackrel{{\text{def}}}{{=}}\|\mathbf{J}_{*}( \mathbf{x}_{t}-\mathbf{x}_{*})\|_{2}\qquad\text{and}\qquad\tau_{t}\stackrel{{ \text{def}}}{{=}}\|\mathbf{J}_{*}(\mathbf{H}_{t}-\mathbf{J}_{*}^{-1})\|_{F}.\]

The \(R_{t}\) measures the distance between \(\mathbf{x}_{t}\) and the solution \(x_{*}\) and \(\tau_{t}\) measures how well does the estimator \(\mathbf{H}_{t}\) approximate the matrix \(\mathbf{J}_{*}^{-1}\).

Using the convergence results for the block bad Broyden's update in Theorem 3.2, we are able to tackle the difference between the estimator \(\mathbf{H}_{t}\) and the matrix \(\mathbf{J}_{*}^{-1}\) after one block update in Algorithm 2.

**Lemma 4.6**.: _Performing Algorithm 2 under Assumptions 2.1, 2.2 and 4.2 and suppose the sequence \(\{\mathbf{x}_{t}\}_{t=0}^{\infty}\) generated by Algorithm 2 satisfies that \(\|\mathbf{x}_{t}-\mathbf{x}_{*}\|_{2}\leq\mu^{2}/(6LM)\), we have_

\[\tau_{t+1}\leq\tau_{t}+\frac{4M\sqrt{d}}{\mu^{2}}\cdot R_{t+1}^{2 }\quad\text{and}\quad\mathbb{E}[\tau_{t+1}]\leq\sqrt{1-\frac{k}{4\kappa^{2}d} \cdot\tau_{t}+\frac{4M\sqrt{d}}{\mu^{2}}}\cdot R_{t+1}.\] (15)

We can establish the superlinear convergence of the block bad Broyden's method based on Lemma 4.6.

**Theorem 4.7**.: _Suppose Assumptions 2.1, 2.2 and 4.2 hold and the initial condition of Algorithm 2 satisfies_

\[\frac{4M\sqrt{d}R_{0}}{\mu^{2}}\leq\min\left\{\frac{1-q}{4},\frac {q}{2},\frac{\sqrt{d}}{3\kappa}\right\}\quad\text{and}\quad\tau_{0}\leq\frac{ q}{2}\] (16)

_for arbitrary \(q\in(0,1)\). Then for \(k\in[d]\), the output of Algorithm 2 satisfies_

\[\mathbb{E}\left[\|\mathbf{J}_{*}(\mathbf{H}_{t}-\mathbf{J}_{*}^{- 1})\|_{F}\right]\leq\mathrm{e}\left(1-\frac{k}{4d\kappa^{2}}\right)^{t/2},\]

_and_

\[\mathbb{E}\left[\frac{\|\mathbf{J}_{*}(\mathbf{x}_{t+1}-\mathbf{ x}_{*})\|_{2}}{\|\mathbf{J}_{*}(\mathbf{x}_{t}-\mathbf{x}_{*})\|_{2}}\right] \leq 2\mathrm{e}\left(1-\frac{k}{4d\kappa^{2}}\right)^{t/2}.\]

Similar to Corollary 4.5, we can also obtain the high probability bound for Algorithm 2.

**Corollary 4.8**.: _Performing Algorithm 2 under the same assumption and initial condition as Theorem 4.7, with probability at least \(1-\delta\), we have_

\[\|\mathbf{J}_{*}(\mathbf{H}_{t}-\mathbf{J}_{*}^{-1})\|_{F}\leq \frac{8\mathrm{e}d^{2}\kappa^{4}}{\delta k^{2}}\left(1-\frac{k}{4d\kappa^{2}+k }\right)^{t/2},\] (17)

_and_

\[\|\mathbf{J}_{*}(\mathbf{x}_{t}-\mathbf{x}_{*})\|_{2}\leq\left( \frac{16\mathrm{e}d^{2}\kappa^{4}}{k^{2}\delta}\right)^{t}\left(1-\frac{k}{4d \kappa^{2}+k}\right)^{t(t-1)/4}\|\mathbf{J}_{*}(\mathbf{x}_{0}-\mathbf{x}_{*}) \|_{2}.\] (18)

## 5 Discussion

In this section, we discuss the performance difference between the good and bad Broyden's methods which is considered as an important open problem in the field of nonlinear equations [35].

We first discuss the different performance of the block Broyden's methods (Algorithm 1 and 2). Notice that the "good" method enjoys a condition-number-free superlinear rate of \(\mathcal{O}((1-k/d)^{t(t-1)/4})\) and the initial conditions of \(\mathbf{B}_{0}\) and \(\mathbf{x}_{0}\) are \(\|\mathbf{J}_{*}^{-1}(\mathbf{B}_{0}-\mathbf{J}_{*})\|_{F}=\mathcal{O}(1) \qquad\text{and}\qquad\|\mathbf{x}_{0}-\mathbf{x}_{*}\|_{2}=\mathcal{O}\left( \frac{\mu}{M\sqrt{d}}\right)\) respectively. On the other hand, both the superlinear rate \(\mathcal{O}((1-k/(4d\kappa^{2}))^{t(t-1)/4})\) and initial conditions \(\|\mathbf{J}_{*}(\mathbf{H}_{0}-\mathbf{J}_{*}^{-1})\|_{F}=\mathcal{O}(\min\{ 1,\sqrt{d}/\kappa\})\), \(\|\mathbf{J}_{*}(\mathbf{x}_{0}-\mathbf{x}_{*})\|_{2}=\mathcal{O}(\mu^{2}/(M \sqrt{d}))\) for \(\mathbf{H}_{0}\), \(\mathbf{x}_{0}\) of the "bad" method depend on \(\kappa\) heavily. Thus we think these two block Broyden's methods are suitable for different scenarios:* The "good" method is more suitable for the cases of large condition number (\(\kappa\gg 1\)) because its convergence rate is condition-number-free and its initial condition has weaker dependency on \(\kappa\) than the "bad" method.
* The 'bad" method may have better performance when \(\kappa=\mathcal{O}(1)\) because under this case the convergence rates do not differ much between the "good" and "bad" method while the latter one usually has a cheaper computational cost per iteration.

The condition number is very large in most of the cases which means the "good" method generally outperforms the "bad" one. We summarize the different convergence rates, initial conditions and suitable scenes of the block good and bad Broyden's methods in Table 3.

The similar phenomenon also holds for the classical good and bad Broyden's methods [29], whose iterations can be reformulated as

\[\begin{cases}\mathbf{x}_{t+1}=\mathbf{x}_{t}-\mathbf{B}_{t}^{-1} \mathbf{F}(\mathbf{x}_{t}),\\ \mathbf{B}_{t+1}=\text{Block-G-Broyden}\left(\mathbf{B}_{t},\hat{\mathbf{J}}_{ t+1},\mathbf{u}_{t}\right)=\mathbf{B}_{t}+\frac{(\mathbf{y}_{t}-\mathbf{B}_{t} \mathbf{u}_{t})\mathbf{u}_{t}^{\top}}{\mathbf{u}_{t}^{\top}\mathbf{u}_{t}} \end{cases},\] (19)

and

\[\begin{cases}\mathbf{x}_{t+1}=\mathbf{x}_{t}-\mathbf{H}_{t} \mathbf{F}(\mathbf{x}_{t}),\\ \mathbf{H}_{t+1}=\text{Block-B-Broyden}\left(\mathbf{H}_{t},\hat{ \mathbf{J}}_{t+1},\mathbf{u}_{t}\right)=\mathbf{H}_{t}+\frac{(\mathbf{u}_{t}- \mathbf{H}_{t}\mathbf{y}_{t})\mathbf{y}_{t}^{\top}}{\mathbf{y}_{t}^{\top} \mathbf{y}_{t}}\end{cases}\] (20)

respectively, where \(\mathbf{u}_{t}=\mathbf{x}_{t+1}-\mathbf{x}_{t}\), \(\hat{\mathbf{J}}_{t+1}=\int_{0}^{1}\mathbf{J}(\mathbf{x}_{t}+s\mathbf{u}_{t}) \mathrm{d}s\) and \(\mathbf{y}_{t}=\mathbf{F}(\mathbf{x}_{t+1})-\mathbf{F}(\mathbf{x}_{t})\). The different convergent behavior of the block Broyden's updates helps us understand the performance difference between the classical good and bad Broyden's methods for the similarity of their frameworks.

## 6 Experiments

We validate our methods on the Chandrasekhar H-equation which is well studied in the previous literature [25; 29; 50] as follows

\[F_{i}(\mathbf{x})=x_{i}-\left(1-\frac{c}{2N}\sum_{j=1}^{N}\frac {\mu_{i}x_{j}}{\mu_{i}+\mu_{j}}\right)^{-1},\] (21)

where \(\mathbf{x}=[x_{1},\cdots,x_{N}]^{\top}\in\mathbb{R}^{N}\) and \(\mathbf{F}(\mathbf{x})=[F_{1}(\mathbf{x}),\cdots,F_{N}(\mathbf{x})]^{\top}\in \mathbb{R}^{N}\). We denote GB-Cl and BB-Cl as the classical good and bad Broyden's methods respectively [1; 29]. We denote GB-Gr and GB-Ra as the greedy and randomized Broyden's methods [50] respectively. Our experiments are conducted on a PC with Apple M1 and all algorithms are implemented in Python 3.8.12.

Our first experiment considers three cases: \(N=200\), \(N=300\), \(N=400\). We set \(c=1-10^{-12}\) for the H-equation and choose the block size \(k=N/10\) for the proposed methods. In all cases, we use the same inputs \(\mathbf{B}_{0}=0.1\mathbf{I}_{N}\) (\(\mathbf{H}_{0}=10\mathbf{I}_{N}\)) for all algorithms. We use classical Newton method as the warm-up algorithm to obtain \(\mathbf{x}_{0}\) which satisfies the local condition and take it as the initial point for all methods. We compare the proposed BGB and BBB algorithm with baselines and present the results of iteration number against \(\|\mathbf{F}(\mathbf{x})\|_{2}\) and running time against \(\|\mathbf{F}(\mathbf{x})\|_{2}\) in Figure 1. We observe that the proposed block good Broyden's method (BGB) outperforms the baselines in all cases, but the block bad Broyden's method (BBB) does not perform very well. This is mainly because \(\kappa\) is very large in this setting (\(\kappa\approx 10^{6}\)). We also note that the classical Broyden's methods (GB-Cl and BB-Cl) are numerical unstable. Specifically, they do not guarantee the descent of \(\|\mathbf{F}(\mathbf{x}_{t})\|_{2}\) and encounter nan value during the iterations. The BB-Cl algorithm even fails to converge after some iterations. Such instability of the classical Broyden's methods is also observed in the previous literature [29, 50].

Our second experiment explores the performance of the proposed block Broyden's methods with different block size. We also study whether BBB algorithm has good performance for the nonlinear equation which Jacobian of the solution small condition number. By fixing \(N=400\) and setting \(c=\{1-10^{-1},1-10^{-3},1-10^{-5}\}\), we obtain different condition numbers of (21) as \(\kappa=2,31,327\). We present the results in Figure 2. For each \(\kappa\), we also vary the block size \(k=\{1,10,100\}\) for BBB and BBB algorithms. We observe that when \(\kappa=\mathcal{O}(1)\), BBB outperforms BGB in terms of the CPU time (Figure 2 (d), (e)). which matches our analysis in section 5. We also find that larger block size \(k\) will lead to faster convergence in terms of the iterations ((a), (b), (c) of Figure 2), which verifies our theoretical results in section 4.2.

## 7 Conclusion

In this paper, we have proposed the block Broyden's methods for solving nonlinear equations. The proposed block good Broyden's method enjoys a faster superlinear rate than all of the existing Broyden's methods. We have also shown that the block bad Broyden's update approximates the inverse of the object matrix with an explicit linear rate and proposed the block bad Broyden's method accordingly. The established convergence results for the block good and bad methods bring us new understanding on the performance difference between the good and bad Broyden's methods. Especially, they can explain why good Broyden's method generally outperforms the "bad" one.

For the future work, it is possible to incorporate the safeguard mechanism in Wang et al. [48] to remove the assumption on the Jacobian estimator (Assumption 4.2). It will also be interesting to study the global behavior based on the recent advance in Jiang et al. [23] and design efficient stochastic or sketched algorithms [48, 49, 52] for solving nonlinear equations.

Figure 1: We demonstrate iteration numbers vs. \(\|\mathbf{F}(\mathbf{x})\|_{2}\) and CPU time (second) vs. \(\|\mathbf{F}(\mathbf{x})\|_{2}\) for H-equation with different equation numbers \(N\).

## Acknowledgement

We would like to thank Haishan Ye for valuable discussion. Cheng Chen is supported by National Natural Science Foundation of China (No. 62306116) and the Dean's fund of Shanghai Key Laboratory of Trustworthy Computing. Luo Luo is supported by National Natural Science Foundation of China (No. 62206058) and Shanghai Sailing Program (22YF1402900). John C.S. Lui is supported in part by the Hong Kong GRC 14215722.

## References

* [1] Mehiddin Al-Baali, Emilio Spedicato, and Francesca Maggioni. Broyden's quasi-Newton methods for a nonlinear system of equations and unconstrained optimization: a review and open problems. _Optimization Methods and Software_, 29(5):937-954, 2014.
* [2] Lucian-Liviu Albu. Non-linear models: applications in economics. _Available at SSRN 1565345_, 2006.
* [3] Meysam Alizamir, Sungwon Kim, Ozgur Kisi, and Mohammad Zounemat-Kermani. A comparative study of several machine learning based non-linear regression methods in estimating solar radiation: Case studies of the usa and turkey regions. _Energy_, 197:117239, 2020.
* [4] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. _Advances in Neural Information Processing Systems_, 32, 2019.
* [5] Elouse Berthier, Justin Carpentier, and Francis Bach. Fast and robust stability region estimation for nonlinear dynamical systems. In _2021 European Control Conference (ECC)_, pages 1412-1419. IEEE, 2021.
* [6] Charles G. Broyden. A class of methods for solving nonlinear simultaneous equations. _Mathematics of computation_, 19(92):577-593, 1965.
* [7] Charles G. Broyden. Quasi-Newton methods and their application to function minimisation. _Mathematics of Computation_, 21(99):368-381, 1967.
* [8] Charles G. Broyden. The convergence of a class of double-rank minimization algorithms 1. general considerations. _IMA Journal of Applied Mathematics_, 6(1):76-90, 1970.
* [9] Charles G. Broyden. The convergence of a class of double-rank minimization algorithms: 2. the new algorithm. _IMA journal of applied mathematics_, 6(3):222-231, 1970.

Figure 2: We demonstrate iteration numbers vs. \(\|\mathbf{F}(\mathbf{x})\|_{2}\) and CPU time (second) vs. \(\|\mathbf{F}(\mathbf{x})\|_{2}\) for H-equation with different condition number \(\kappa\).

* [10] Charles G. Broyden. On the discovery of the "good Broyden" method. _Mathematical programming_, 87:209-213, 2000.
* [11] Charles G. Broyden, J. E. Dennis, and Jorge J. More. On the local and superlinear convergence of quasi-Newton methods. _IMA Journal of Applied Mathematics_, 12(3):223-245, 1973.
* [12] Caterina Buizza, Cesar Quilodran Casas, Philip Nadler, Julian Mack, Stefano Marrone, Zainab Titus, Clemence Le Cornec, Evelyn Heylen, Tolga Dur, Luis Baca Ruiz, et al. Data learning: integrating data assimilation and machine learning. _Journal of Computational Science_, 58:101525, 2022.
* [13] Richard H. Byrd, Jorge Nocedal, and Ya-Xiang Yuan. Global convergence of a cass of quasi-Newton methods on convex problems. _SIAM Journal on Numerical Analysis_, 24(5):1171-1190, 1987.
* [14] Timothy A. Davis. Block matrix methods: Taking advantage of high-performance computers. Technical report, Technical Report TR-98-024, 1998.
* [15] Alexandre Defossez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions. In _Artificial Intelligence and Statistics_, pages 205-213. PMLR, 2015.
* [16] John E. Dennis Jr and Robert B. Schnabel. _Numerical methods for unconstrained optimization and nonlinear equations_. SIAM, 1996.
* [17] Jin-yan Fan and Ya-Xiang Yuan. On the quadratic convergence of the Levenberg-Marquardt method without nonsingularity assumption. _Computing_, 74:23-39, 2005.
* [18] J. Frehse and A. Bensoussan. Nonlinear elliptic systems in stochastic game theory. _Journal fur die reine und angewandte Mathematik_, 350:23-67, 1984.
* [19] Philip E. Gill and Walter Murray. Algorithms for the solution of the nonlinear least-squares problem. _SIAM Journal on Numerical Analysis_, 15(5):977-992, 1978.
* [20] Nick Gould, Dominique Orban, and Philippe Toint. Numerical methods for large-scale nonlinear optimization. _Acta Numerica_, 14:299-361, 2005.
* [21] Robert M. Gower and Peter Richtarik. Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms. _arXiv preprint arXiv:1602.01768_, 2016.
* [22] Andreas Griewank. Broyden updating, the good and the bad. _Optimization Stories, Documenta Mathematica. Extra Volume: Optimization Stories_, pages 301-315, 2012.
* [23] Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari. Online learning guided curvature approximation: A quasi-Newton method with global non-asymptotic superlinear convergence. In _Annual Conference Computational Learning Theory_, 2023.
* [24] Qiujiang Jin and Aryan Mokhtari. Non-asymptotic superlinear convergence of standard quasi-Newton methods. _Mathematical Programming_, pages 1-49, 2022.
* [25] Carl T. Kelley. _Iterative methods for linear and nonlinear equations_. SIAM, 1995.
* [26] Carl T. Kelley. _Solving nonlinear equations with Newton's method_. SIAM, 2003.
* [27] Carl T. Kelley and Ekkehard W. Sachs. A new proof of superlinear convergence for Broyden's method in Hilbert space. _SIAM Journal on Optimization_, 1(1):146-150, 1991.
* [28] Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. _Quarterly of applied mathematics_, 2(2):164-168, 1944.
* [29] Dachao Lin, Haishan Ye, and Zhihua Zhang. Explicit superlinear convergence rates of Broyden's methods in nonlinear equations. _arXiv preprint arXiv:2109.01974_, 2021.
* [30] Dachao Lin, Haishan Ye, and Zhihua Zhang. Explicit convergence rates of greedy and random quasi-Newton methods. _Journal of Machine Learning Research_, 23(162):1-40, 2022.

* [31] Chengchang Liu and Luo Luo. quasi-Newton methods for saddle point problems. _Advances in Neural Information Processing Systems_, 35:3975-3987, 2022.
* [32] Chengchang Liu, Shuxian Bi, Luo Luo, and John CS Lui. Partial-quasi-Newton methods: Efficient algorithms for minimax optimization problems with unbalanced dimensionality. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1031-1041, 2022.
* [33] Chengchang Liu, Cheng Chen, and Luo Luo. Symmetric rank-\(k\) methods. _arXiv preprint arXiv:2303.16188_, 2023.
* [34] Donald W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. _Journal of the society for Industrial and Applied Mathematics_, 11(2):431-441, 1963.
* [35] Jose Mario Martinez. Practical quasi-Newton methods for solving nonlinear systems. _Journal of computational and Applied Mathematics_, 124(1-2):97-121, 2000.
* [36] Konstantin Mishchenko. Regularized Newton method with global \(O(1/k^{2})\) convergence. _arXiv preprint arXiv:2112.02089_, 2021.
* [37] Jorge J. More. A collection of nonlinear model problems. Technical report, Argonne National Lab., IL (USA), 1989.
* [38] Yu. Nesterov. Modified Gauss-Newton scheme with worst case guarantees for global performance. _Optimisation methods and software_, 22(3):469-483, 2007.
* [39] Mojtaba Nourian and Peter E. Caines. \(\epsilon\)-Nash mean field game theory for nonlinear stochastic dynamical systems with major and minor agents. _SIAM Journal on Control and Optimization_, 51(4):3302-3331, 2013.
* [40] M.J.D. Powell. A hybrid method for nonlinear equations. _Numerical Methods for Nonlinear Algebraic Equations_, 1970.
* [41] Anton Rodomanov and Yurii Nesterov. Greedy quasi-Newton methods with explicit superlinear convergence. _SIAM Journal on Optimization_, 31(1):785-811, 2021.
* [42] Anton Rodomanov and Yurii Nesterov. New results on superlinear convergence of classical quasi-Newton methods. _Journal of optimization theory and applications_, 188(3):744-769, 2021.
* [43] Anton Rodomanov and Yurii Nesterov. Rates of superlinear convergence for classical quasi-Newton methods. _Mathematical Programming_, pages 1-32, 2021.
* [44] Damien Scieur, Edouard Oyallon, Alexandre d'Aspremont, and Francis Bach. Online regularized nonlinear acceleration. _arXiv preprint arXiv:1805.09639_, 2018.
* [45] David F. Shanno. Conditioning of quasi-Newton methods for function minimization. _Mathematics of computation_, 24(111):647-656, 1970.
* [46] Jack Sherman and Winfred J. Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. _The Annals of Mathematical Statistics_, 21(1):124-127, 1950.
* [47] Ph L. Toint. On large scale nonlinear least squares calculations. _SIAM Journal on Scientific and Statistical Computing_, 8(3):416-435, 1987.
* [48] Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. _SIAM Journal on Optimization_, 27(2):927-956, 2017.
* [49] Xiaoyu Wang, Xiao Wang, and Ya-Xiang Yuan. Stochastic proximal quasi-Newton methods for non-convex composite optimization. _Optimization Methods and Software_, 34(5):922-948, 2019.

* [50] Haishan Ye, Dachao Lin, and Zhihua Zhang. Greedy and random Broyden's methods with explicit superlinear convergence rates in nonlinear equations. _arXiv preprint arXiv:2110.08572_, 2021.
* [51] Haishan Ye, Dachao Lin, Xiangyu Chang, and Zhihua Zhang. Towards explicit superlinear convergence rate for SR1. _Mathematical Programming_, pages 1-31, 2022.
* [52] Rui Yuan, Alessandro Lazaric, and Robert M. Gower. Sketched Newton-Raphson. _SIAM Journal on Optimization_, 32(3):1555-1583, 2022.
* [53] Ya-Xiang Yuan. Trust region algorithms for nonlinear equations. _Information_, 1:7-20, 1998.
* [54] Ya-Xiang Yuan. Recent advances in numerical methods for nonlinear equations and nonlinear least squares. _Numerical algebra, control & optimization_, 1(1):15, 2011.