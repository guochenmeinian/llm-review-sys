# Selective Sampling and Imitation Learning

via Online Regression

 Ayush Sekhari\({}^{1}\)

Authors are listed in alphabetical order of their last names.

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Karthik Sridharan\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Wen Sun\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

&Runzhe Wu\({}^{2}\)

\({}^{1}\)MIT, \({}^{2}\)Cornell University

###### Abstract

We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries. Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t. the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.

## 1 Introduction

From the classic supervised learning setting to the more complex problems like interactive Imitation Learning (IL) (Ross et al., 2011), high-quality labels or supervision is often expensive and hard to obtain. Thus, one wishes to develop algorithms that do not require a label for every data sample presented during the learning process. Active learning or selective sampling is a learning paradigm that is designed to reduce query complexity by only querying for labels at selected data points, and has been extensively studied in both theory and practice (Agarwal, 2013; Dekel et al., 2012; Hanneke and Yang, 2021; Zhu and Nowak, 2022; Cesa-Bianchi et al., 2005; Hanneke and Yang, 2015).

In this work, we study selective sampling and its application to interactive Imitation Learning (Ross et al., 2011). Our goal is to design algorithms that can leverage general function approximation and online regression oracles to achieve small regret on predicting the correct labels, and at thesame time minimize the number of expert queries made (query complexity). Towards this goal, we first study selective sampling which is an online active learning framework, and provide regret and query complexity bounds for general function classes (used to model the experts). Our key results in selective sampling are obtained by developing a connection between the regret of the online regression oracles and the regret of predicting the correct labels. Additionally, we bound the query complexity using the eluder dimension (Russo and Van Roy, 2013) of the underlying function class used to model the expert. We complement our results with a lower bound indicating that a dependence on an eluder dimension like complexity measure is unavoidable in the query complexity in the worst case. In particular, we provide lower bounds in terms of the star number of the function class--a quantity closely related to the eluder dimension. Our new selective sampling algorithm, called SAGE, can operate under fairly general modeling assumptions, loss functions, and allows for multiple labels (i.e., multi-class classification).

We then extend our selective sampling algorithm to the interactive IL framework proposed by Ross et al. (2011) to reduce the query complexity. While the DAgger algorithm proposed by Ross et al. (2011) has been extensively used in various robotics applications (e.g., Ross et al. (2013); Pan et al. (2018)), it often requires a large number of expert queries. There have been some efforts on reducing the expert query complexity by leveraging ideas from active learning (e.g., Laskey et al. (2016); Brantley et al. (2020)), however, these prior attempts do not have theoretical guarantees on bounding expert's query complexity. In this work, we provide the first provably correct algorithm for interactive IL with general function classes, called RAVIOLI, which not only achieves strong regret bounds in terms of maximizing the underlying reward functions, but also enjoys a small query complexity. Furthermore, we note that RAVIOLI operates under significantly weaker assumptions as compared to the prior works, like Ross et al. (2011), on interactive IL. In particular, we only assume access to a noisy expert, as compared to the prior works that assume that the expert is noiseless. In fact, for the noisy setting, we show that one can not even hope to learn from purely offline expert demonstrations unless one has exponentially in horizon \(H\) many samples. Such a strong separation does not hold in the noiseless setting.

Our bounds depend on the margin of the noisy expert, which intuitively quantifies the confidence level of the expert. In particular, the margin is large for states where the expert is very confident in terms of providing the correct labels, while on the other hand, the margin is small on the states where the expert is less confident and subsequently provides more noisy labels as feedback. Such kind of margin condition was missing in prior works, like Ross et al. (2011), which assumes that the expert can provide confident labels everywhere. Additionally, we note that our margin assumption is quite mild as we only assume that the expert has a large margin under the states that could be visited by the noiseless expert (however, the states visited by the learner, or by following the noisy expert, may not have a small margin).

We then extend our results to the multiple expert setting where the learner has access to \(M\) many experts/teachers who may have different expertise at different parts of the state space. In particular, there is no expert who can singlehandedly perform well on the underlying environment, but an aggregation of their policies can lead to good performance. Such an assumption holds in various applications and has been recently explored in continuous control tasks like robotics and discrete tasks like chess and Minigrid (Beliaev et al., 2022). Similar to the single expert setting, we model the expertise of the experts in multiple expert setting using the concept of margins. Different experts have different margin functions, capturing the fact that experts may have different expertise at different parts of the state space. Prior work from Cheng et al. (2020) also considers multiple experts in IL and provides meaningful regret bounds, however, their assumption on the experts is much stronger than us: they assume that for any state, there at least exists one expert who can achieve high reward-to-go if the expert took over the control starting from this state till the end of the episode. Furthermore, Cheng et al. (2020) considers the setting where one can also query for the reward signals, whereas we do not require access to any reward signals.

## 2 Contributions and Overview of Results

### Selective Sampling

Online selective sampling models the interaction between a learner and an adversary over \(T\) rounds. At the beginning of each round of the interaction, the adversary presents a context \(x_{t}\) to the learner. After receiving the context, the learner makes a prediction \(\hat{y}_{t}\in\left[K\right]\), where \(K\) denotes the number of actions. Then, the learner needs to make a choice of whether or not to query an _expert_ who is assumed to have some knowledge about the true label for all the presented contexts. The experts knowledge about the true label is modeled via the ground truth modeling function \(f^{\star}\), which is assumed to belong to a given function class \(\mathcal{F}\) but is unknown to the learner. If the learner decides to query for the label, then the expert will return a noisy label \(y_{t}\) sampled using \(f^{\star}\). If the learner does not query, then the learner does not receive any feedback in this round. The learner makes an update based on the latest information it has, and moves on to the next round of the interaction. The goal of the learner is to compete with the expert policy \(\pi^{\star}\), that is defined using the experts model \(f^{\star}\). In the selective sampling setting, we are concerned with two things: the total regret of the learner w.r.t. the policy \(\pi^{\star}\), and the number of expert queries that the learner makes. Our key contributions are as follows:

* We provide a new selective sampling algorithm (Algorithm 1) that relies on an online regression oracle w.r.t. \(\mathcal{F}\) (where \(\mathcal{F}\) is the given model class) to make predictions and to decide whether to query for labels. Our algorithm can handle multiple actions, adversarial contexts, arbitrary model class \(\mathcal{F}\), and fairly general modeling assumptions (that we discuss in more detail in Section 3), and enjoys the following regret bound and query complexity: \[\mathrm{Reg}_{T}=\widetilde{\mathcal{O}}\left(\inf_{\varepsilon}\left\{ \varepsilon T_{\varepsilon}+\frac{\mathrm{Reg}\left(\mathcal{F};T\right)}{ \varepsilon}\right\}\right)\quad\text{and}\quad N_{T}=\widetilde{\mathcal{O}} \!\left(\inf_{\varepsilon}\!\left\{T_{\varepsilon}+\frac{\mathrm{Reg}\left( \mathcal{F};T\right)\cdot\mathfrak{E}\left(\mathcal{F},\varepsilon;f^{\star} \right)}{\varepsilon^{2}}\right\}\right)\!\!.\] (1) where \(\mathrm{Reg}\left(\mathcal{F};T\right)\) denotes the regret bound for the online regression oracle on \(\mathcal{F}\), \(\mathfrak{E}\left(\mathcal{F},\varepsilon;f^{\star}\right)\) denotes the eluder dimension of \(\mathcal{F}\), and \(T_{\varepsilon}\) denotes the number of rounds at which the margin of the experts predictions is smaller than \(\varepsilon\) (the exact notion of margin is defined in Section 3).
* We show via a lower bound that, without additional assumptions, the dependence on the eluder dimension in the query complexity bound (1) is unavoidable if we desire a regret bound of the form (1), even when \(T_{\varepsilon}=0\). The details are located in Section 3.2.
* For the stochastic setting, where the context \(\{x_{t}\}_{t\leq T}\) are sampled i.i.d. from a fixed unknown distribution, we provide an alternate algorithm (Algorithm 3) that enjoys the same regret bound as (1) but whose query complexity scales with the disagreement coefficient of \(\mathcal{F}\) instead of the eluder dimension (Theorem 2). Since the disagreement coefficient is always smaller than the eluder dimension, Theorem 2 yields an improvement in the query complexity.

### Imitation Learning

We then move to the more challenging Imitation Learning (IL) setting, where the learner operates in an episodic finite horizon Markov Decision Process (MDP), and can query a noisy expert for feedback (i.e. the expert action) on the states that it visits. The interaction proceeds in \(T\) episodes of length \(H\) each. In episode \(t\), at each time step \(h\in[H]\) and on the state \(x_{t,h}\), the learner chooses an action \(\widetilde{y}_{t,h}\) and transitions to state \(x_{t,h+1}\). However, the learner does not receive any reward signal. Instead, the learner can actively choose to query an _expert_ who has some knowledge about the correct action to be taken on \(x_{t,h}\), and gives back noisy feedback \(y_{t,h}\) about this action. Similar to the selective sampling setting, the experts knowledge about the true label is modeled via the ground truth modeling function \(f_{h}^{\star}\), which is assumed to belong to a given function class \(\mathcal{F}_{h}\) but is unknown to the learner. The goal of the learner is to compete with the optimal policy \(\pi^{\star}\) of the (noiseless) expert. Our key contributions in IL are:

* In Section 4, we first demonstrate an exponential separation in terms of task horizon \(H\) in the sample complexity, for learning via offline expert demonstration only vs interactive querying of experts, when the feedback from the expert is noisy.
* We then provide a general IL algorithm (in Algorithm 2) that relies on online regression oracles w.r.t. \(\{\mathcal{F}_{h}\}_{h\leq H}\) to predict actions, and to decide whether to query for labels. Similar to the selective sampling setting, the regret bound for our algorithm scales with the regret of the online regression oracles, and the query complexity bound has an additional dependence on the eluder dimension. Furthermore, our algorithm can handle multiple actions, adversarially changing dynamics, arbitrary model class \(\mathcal{F}\), and fairly general modeling assumptions.
* A key difference from our results in selective sampling is that the term \(T_{\varepsilon}\) that appears in our regret and query complexity bounds in IL denote the number of time steps in which the expert policy \(\pi^{\star}\) has a small margin (instead of the number of time steps when the learner's policy has a small margin). In fact, the learner and the expert trajectories could be completely different from each other, and we only pay in the margin term if the expert trajectory at that time step would have a low margin. See Section 4 for the exact definition of margin.
* In Section 4.1, we provide extensions to our algorithm when the learner can query \(M\) experts at each round. In particular, we do not assume that any of the experts is singlehandedly optimal for the entire state space, but that there exist aggregation functions of these experts' predictions that perform well in practice, and with which we compete.

## 3 Selective Sampling

In the problem of selective sampling, on every round \(t\), nature (or an adversary) produces a context \(x_{t}\). The learner then receives this context and predicts a label \(\widehat{y}_{t}\in[K]\) for that context. The learner also computes a query condition \(Z_{t}\in\{0,1\}\) for that context. If \(Z_{t}=1\), the learner requests for label \(y_{t}\in[K]\) corresponding to the \(x_{t}\), and if not, the learner receives no feedback on the label for that round. Let \(\mathcal{F}\) be a model class such that each model \(f\in\mathcal{F}\) maps contexts \(x\) to scores \(f(x)\in\mathbb{R}^{K}\). In this work we assume that while contexts can be chosen arbitrarily, the label \(y_{t}\) corresponding to a context \(x_{t}\) is drawn from a distribution over labels specified by the score \(f^{\star}(x_{t})\) where \(f^{\star}\in\mathcal{F}\) is a fixed model unknown to the learner. We assume that a link function \(\phi:\mathbb{R}^{K}\mapsto\Delta(K)\) maps scores to distributions and assume that the noisy label \(y_{t}\) is sampled as

\[y_{t}\sim\phi(f^{\star}(x_{t})).\] (2)

In this work, we assume that the link function \(\phi(v)=\nabla\Phi(v)\) for some \(\Phi:\mathbb{R}^{K}\mapsto\mathbb{R}\)(see Agarwal (2013) for more details) which satisfies the following assumption.

**Assumption 1**.: _The function \(\Phi\) is \(\lambda\)-strongly-convex and \(\gamma\)-smooth, i.e. for all \(u,u^{\prime}\in\mathbb{R}^{K}\),_

\[\frac{\lambda}{2}\big{\|}u^{\prime}-u\big{\|}_{2}^{2}\leq\Phi(u^{\prime})-\Phi (u)-\big{\langle}\nabla\Phi(u),u^{\prime}-u\big{\rangle}\leq\frac{\gamma}{2} \big{\|}u^{\prime}-u\big{\|}_{2}^{2}.\]

Our main contribution in this section is a selective sampling algorithm that uses online non-parametric regression w.r.t. the model class \(\mathcal{F}\) as a black box. Specifically, define the loss function corresponding to the link function \(\phi\) as \(\ell_{\phi}(v,y)=\Phi(v)-v[y]\) where \(v\in\mathbb{R}^{K}\) and \(y\in[K]\). We assume that the learner has access to an online regression oracle for the loss \(\ell_{\phi}\) (which is a convex loss) w.r.t. the class \(\mathcal{F}\), that for any sequence \(\{(x_{1},y_{1}),\ldots,(x_{T},y_{T})\}\) guarantees the regret bound

\[\sum_{s=1}^{T}\ell_{\phi}(f_{s}(x_{s}),y_{s})-\inf_{f\in\mathcal{F}}\sum_{s=1 }^{T}\ell_{\phi}(f(x_{s}),y_{s})\leq\operatorname{Reg}^{\ell_{\phi}}(\mathcal{ F};T).\] (3)

When \(\phi\) is identity (under which the models in \(\mathcal{F}\) directly map to distributions over the labels), then \(\ell_{\phi}\) denotes the standard square loss, and we need a bound on the regret w.r.t. the square loss, denoted by \(\operatorname{Reg}^{\mathrm{sq}}(\mathcal{F};T)\). When \(\phi\) is the Boltzman distribution mapping (given by \(\Phi\) being the softmax function) then \(\ell_{\phi}\) is the logistic loss, and we need an online logistic regression oracle for \(\mathcal{F}\). Minimax rates for the regret bound in (3) are well known:

* _Square-loss regression:_Rakhlin and Sridharan (2014) characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity of \(\mathcal{F}\), which for example, leads to regret bound \(\operatorname{Reg}^{\mathrm{sq}}(\mathcal{F};T)=O(\log|\mathcal{F}|)\) for finite function classes \(\mathcal{F}\), and \(\operatorname{Reg}^{\mathrm{sq}}(\mathcal{F};T)=O(d\log(T))\) when \(\mathcal{F}\) is a \(d\)-dimensional linear class. More examples can be found in Rakhlin and Sridharan (2014, Section 4). We refer the readers to Krishnamurthy et al. (2017), Foster et al. (2018) for efficient implementations.
* _Logistic-loss regression:_ When \(\mathcal{F}\) is finite, we have the regret bound \(\operatorname{Reg}(\mathcal{F};T)\leq O(\log|\mathcal{F}|)\)(Cesa-Bianchi and Lugosi, 2006, Chapter 9). For learning linear predictors, there exists efficient improper learner with regret bound \(\operatorname{Reg}(\mathcal{F};T)\leq O(d\log|T|)\)(Foster et al., 2018). More examples can be found in Foster et al. (2018, Section 7) and Rakhlin and Sridharan (2015).

When one deals with complex model classes \(\mathcal{F}\) such that the labeling concept class corresponding to \(\mathcal{F}\) could possibly have infinite VC dimension (like it is typically the case), then one needs to naturally rely on a margin-based analysis (Tsybakov, 2004; Shalev-Shwartz and Ben-David, 2014; Dekel et al., 2012). For \(p\in\mathbb{R}^{K}\), we use the following well-known notion of margin for multiclass settings3:

Footnote 3: Throughout the paper, we assume that the ties in \(\operatorname*{argmax}\) or \(\operatorname*{argmin}\) are broken arbitrarily, but consistently.

\[\mathtt{Margin}(p)=\phi(p)[k^{*}]-\max_{k^{\prime}\neq k^{*}}\phi(p)[k^{\prime }]\qquad\text{where}\quad k^{*}\in\operatorname*{argmax}_{k}\phi(p)[k],\] (4)

A key quantity that appears in our results is the number of \(x_{t}\)'s that fall within an \(\varepsilon\) margin region,

\[T_{\varepsilon}=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{\star}(x_{t})) \leq\varepsilon\}.\]

\(T_{\varepsilon}\) denotes the number of times where even the Bayes optimal classifier is confused about the correct label on \(x_{t}\), and has confidence less than \(\varepsilon\). The algorithm relies on an online regression oracle mentioned above to produce the predictor \(f_{t}\) at every round. The predicted label \(\widehat{y}_{t}=\mathtt{SelectAction}(f_{t}(x_{t}))=\operatorname*{argmax}_{k} \phi(f_{t}(x_{t}))[k]\) is picked based on the score \(f_{t}(x_{t})\) (where \(\widehat{y}_{t}\) is the label with the largest score). The learner updates the regression oracle on only those rounds in which it makes a query. Our main algorithm for selective sampling is provided in Algorithm 1.4

Footnote 4: Unless explicitly specified, the action set is given by \(\mathcal{A}=[K]=\{1,\ldots,K\}\) where \(K\geq 2\).

```
0: Parameters \(\delta,\gamma,\lambda,T\), function class \(\mathcal{F}\), and online regression oracle \(\mathsf{Oracle}\) w.r.t \(\ell_{\phi}\).
1: Set \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)=\frac{4}{\lambda}\mathrm{Reg}^{ \ell_{\phi}}(\mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta)\), Compute \(f_{1}\leftarrow\mathsf{Oracle}_{1}(\varnothing)\).
2:for\(t=1\) to \(T\)do
3: Nature chooses \(x_{t}\).
4: Learner plays the action \(\widehat{y}_{t}=\mathtt{SelectAction}(f_{t}(x_{t}))\).
5: Learner computes \[\Delta_{t}(x_{t}):=\max_{f\in\mathcal{F}}\,|f(x_{t})-f_{t}(x_{t})\|\quad \text{s.t.}\quad\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{ \delta}^{\ell_{\phi}}(\mathcal{F},T).\] (5)
6: Learner decides whether to query: \(Z_{t}=\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))\leq 2\gamma\Delta_{t}(x_{t})\}\).
7:if\(Z_{t}=1\)then
8: Learner queries the label \(y_{t}\) on \(x_{t}\).
9:\(f_{t+1}\leftarrow\mathsf{Oracle}_{t}(\{x_{t},y_{t}\})\).
10:else
11:\(f_{t+1}\gets f_{t}\). ```

**Algorithm 1** Selective \(\mathbf{SAmplinG}\) with \(\mathbf{Expert}\) Feedback (SAGE)

Our goal in this work is twofold: Firstly, we would like Algorithm 2 to have a low regret w.r.t. the optimal model \(f^{\star}\), defined as

\[\mathrm{Reg}_{T}=\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t}\neq y_{t}\}-\sum_{ t=1}^{T}\mathbf{1}\{\mathtt{SelectAction}(f^{\star}(x_{t}))\neq y_{t}\}\]

Simultaneously, we also aim to make as few label queries \(N_{T}=\sum_{t=1}^{T}Z_{t}\) as possible. Before delving into our results, we first recall the following variant of eluder-dimension (Russo and Van Roy, 2013; Foster et al., 2020; Zhu and Nowak, 2022).

**Definition 1** (Scale-sensitive eluder dimension (normed version)).: _Fix any \(f^{\star}\in\mathcal{F}\), and define \(\widetilde{\mathfrak{E}}(\mathcal{F},\beta;f^{\star})\) to be the length of the longest sequence of contexts \(x_{1},x_{2},\ldots x_{m}\) such that for all \(i\), there exists \(f_{i}\in\mathcal{F}\) such that_

\[\|f_{i}(x_{i})-f^{\star}(x_{i})\|>\beta,\quad\text{and}\quad\sum_{j<i}\|f_{i} (x_{j})-f^{\star}(x_{j})\|^{2}\leq\beta^{2}.\]

_The value function eluder dimension is defined as \(\mathfrak{E}(\mathcal{F},\beta^{\prime};f^{\star})=\sup_{\beta\geq\beta^{ \prime}}\widetilde{\mathfrak{E}}(\mathcal{F},\beta;f^{\star})\)._

Bounds on the eluder dimension for various function classes are well known, e.g. when \(\mathcal{F}\) is finite, \(\mathfrak{E}(\mathcal{F},\beta^{\prime};f^{\star})\leq|\mathcal{F}|-1\), and when \(\mathcal{F}\) is the set of \(d\)-dimensional function with bounded norm, then \(\mathfrak{E}(\mathcal{F},\beta^{\prime};f^{\star})=O(d)\). We refer the reader to Russo and Van Roy (2013); Mou et al. (2020); Li et al. (2022) for more examples. The following theorem is our main result for selective sampling:

**Theorem 1**.: _Let \(\delta\in(0,1)\). Under the modeling assumptions above (in (2), (3) and (4)), with probability at least \(1-\delta\), Algorithm 1 obtains the regret bound_

\[\mathrm{Reg}_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{ \varepsilon T_{\varepsilon}+\frac{\gamma^{2}}{\lambda\varepsilon}\mathrm{ Reg}^{\ell_{\phi}}\big{(}\mathcal{F};T\big{)}+\frac{\gamma^{2}}{\lambda^{2} \varepsilon}\log(1/\delta)\right\}\right)\!,\qquad\text{and,}\] \[N_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{T_{ \varepsilon}+\frac{\gamma^{2}}{\lambda\varepsilon^{2}}\cdot\!\mathrm{Reg}^{ \ell_{\phi}}\big{(}\mathcal{F};T\big{)}\cdot\mathfrak{E}\big{(}\mathcal{F}, \nicefrac{{\varepsilon}}{{4}}\gamma;f^{\star}\big{)}+\frac{\gamma^{2}}{ \lambda^{2}\varepsilon^{2}}\log(1/\delta)\right\}\right)\!.\]

A few points are in order:

* It must be noted that for most settings we consider, as an example if model class \(\mathcal{F}\) is finite, one typically has that \(\mathrm{Reg}(\mathcal{F};T)\leq\log|\mathcal{F}|\). Thus, in the case where one has a hard margin condition i.e. \(T_{\varepsilon_{0}}=0\) for some \(\varepsilon_{0}>0\), we get \(\mathrm{Reg}_{T}\leq O\left(\frac{\log|\mathcal{F}|}{\varepsilon_{0}}\right)\) and \(N_{T}\leq O\left(\frac{\mathfrak{E}\big{(}\mathcal{F},\varepsilon;f^{\star} \big{)}\log|\mathcal{F}|}{\varepsilon_{0}^{2}}\right)\).
* Our regret bound does not depend on the eluder dimension. However, the query complexity bound has a dependence on eluder dimension. Thus, for function classes for which the eluder dimension is large, the regret bound is still optimal while the number of label queries may be large.

### Selective Sampling in the Stochastic Setting

So far we assumed that the contexts \(\{x_{t}\}_{t\geq 0}\) could be chosen in a possibly adversarial fashion, and thus our bound on the number of label queries scales with the eluder dimension. However, it turns out that if the contexts are drawn i.i.d. from some (unknown) distribution \(\mu\), then one can improve the query complexity to scale with the value function disagreement coefficient of \(\mathcal{F}\) (defined below) which is always smaller than the eluder dimension (Lemma 6).

**Definition 2** (Scale sensitive disagreement coefficient (normed version), Foster et al. (2020)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\). For any \(f^{\star}\in\mathcal{F}\), and \(\beta_{0},\varepsilon_{0}>0\), the value function disagreement coefficient \(\theta^{\mathrm{val}}\big{(}\mathcal{F},\varepsilon_{0},\beta_{0};f^{\star} \big{)}\) is defined as_

\[\sup_{\mu}\sup_{\beta\beta_{0},\varepsilon>0}\sup_{\varepsilon \geq\varepsilon_{0}}\left\{\frac{\varepsilon^{2}}{\beta^{2}}\cdot\mathrm{Pr} _{x\sim\mu}\big{(}\exists f\in\mathcal{F}\mid|f(x)-f^{\star}(x)|>\varepsilon, \|f-f^{\star}\|_{\mu}\leq\beta\big{)}\right\}\lor 1\]

_where \(\|f\|_{\mu}=\sqrt{\mathbb{E}_{x\sim\mu}\big{[}|f(x)|^{2}\big{]}}\)._

The key idea that gives us the above improvement, of replacing the eluder dimension by disagreement coefficient in the query complexity bound, is to use epoching for the query condition, while still using an online regression oracle to make predictions. The exact algorithm is given in Appendix E.4.

**Theorem 2**.: _Let \(\delta\in(0,1)\), and consider the modeling assumptions in (2), (3) and (4). Furthermore, suppose that \(x_{t}\) is sampled i.i.d. from \(\mu\), where \(\mu\) is a fixed distribution. Then, with probability at least \(1-\delta\), Algorithm 3 obtains the bounds5_

Footnote 5: In the rest of the paper, the notation \(\widetilde{\mathcal{O}}\) hides additive \(\log(1/\delta)\)-factors which, for constant \(\delta\) and in all the results, are asymptotically dominated by the other terms presented in the displayed bounds.

\[\mathrm{Reg}_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{ \varepsilon T_{\varepsilon}+\frac{\gamma^{2}}{\lambda\varepsilon}\mathrm{ Reg}^{\ell_{\phi}}\big{(}\mathcal{F};T\big{)}\right\}\right)\!,\qquad\text{and,}\] \[N_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{T_{ \varepsilon}+\frac{\gamma^{2}}{\lambda\varepsilon^{2}}\cdot\mathrm{Reg}^{ \ell_{\phi}}\big{(}\mathcal{F};T\big{)}\cdot\theta^{\mathrm{val}}\big{(} \mathcal{F},\nicefrac{{\varepsilon}}{{8}\gamma,\,\mathrm{Reg}^{\ell_{\phi}} \big{(}\mathcal{F};T\big{)}}\big{/}T;f^{\star}\big{)}\right\}\right)\!.\]

We note that Algorithm 3 automatically adapts to Tsybakov noise condition with respect to \(\mu\).

**Corollary 1** (Tsybakov noise condition, Tsybakov (2004)).: _Suppose there exists constants \(c,\rho\geq 0\) s.t. \(\mathrm{Pr}_{x\sim\mu}\big{(}\mathtt{Margin}\big{(}f^{\star}(x)\big{)}\leq \varepsilon\big{)}\leq c\varepsilon^{\rho}\) for all \(\varepsilon\in(0,1)\), and consider the same modeling assumptions as in Theorem 2. Then, with probability at least \(1-\delta\), Algorithm 3 obtains the bound_

\[\mathrm{Reg}_{T} =\widetilde{\mathcal{O}}\!\left(\left(\mathrm{Reg}^{\ell_{\phi}} \big{(}\mathcal{F};T\big{)}\right)^{\frac{\rho\varepsilon+1}{\rho+2}}\cdot(T)^{ \frac{1}{\rho+2}}\right)\!,\qquad\text{and,}\] \[N_{T} =\widetilde{\mathcal{O}}\!\left(\left(\mathrm{Reg}^{\ell_{\phi}} \big{(}\mathcal{F};T\big{)}\cdot\theta^{\mathrm{val}}\big{(}\mathcal{F}, \nicefrac{{\varepsilon}}{{8}\gamma,\,\mathrm{Reg}^{\ell_{\phi}}\big{(} \mathcal{F};T\big{)

### Lower Bounds (Binary Action Case)

We supplement the above upper bound with a lower bound in terms of the star number of \(\mathcal{F}\) (defined below). The star number is bounded from above by the eluder dimension which appears in our upper bounds (Lemma 6). While star number may not be lower bounded by eluder dimension in general, for many commonly considered classes, star number is of the same order as the eluder dimension (Foster et al., 2020). For the sake of a clean presentation, we restrict our lower bound to the binary actions case, although one can easily extend the lower bound to the multiple actions case.

**Definition 3** (scale-sensitive star number).: _For any \(\zeta\in(0,1)\) and \(\beta\in(0,\zeta/2)\), define \(\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta,\beta)\) as the largest \(m\) such that there exists target function \(f^{\star}\in\mathcal{F}\) and sequence \(x_{1},\ldots,x_{m}\in\mathcal{X}\) s.t. \(\forall i\in[m]\), \(|f^{\star}(x_{i})|>\zeta\), \(\exists f_{i}\in\mathcal{F}\) s.t.,_

\[\mathbf{(1)}\sum_{j\neq i}(f_{i}(x_{j})-f^{\star}(x_{j}))^{2}<\beta^{2}\quad \mathbf{(2)}\left|f_{i}(x_{i})\right|>\zeta/2\text{ and }f_{i}(x_{i})f^{\star}(x_{i})<0\quad \mathbf{(3)}\left|f_{i}(x_{i})-f^{\star}(x_{i})\right|\leq 2\zeta\]

The below theorem provides a lower bound on number of queries, in terms of star number for any algorithm that guarantees a non-trivial regret bound.

**Theorem 3**.: _Given a function class \(\mathcal{F}\) and some desired margin \(\zeta>0\), define \(\beta\in(0,\zeta/2)\) be the largest number such that \(\beta^{2}\leq\min\{\zeta^{2}/\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta, \beta),\zeta^{2}/16\}\). Then, for any algorithm that guarantees regret bound of \(\mathbb{E}[\mathrm{Reg}_{T}]\leq 64\frac{\zeta T}{\mathfrak{s}^{\mathrm{val}}( \mathcal{F},\zeta,\beta)}\) on all instances with margin \(\zeta/2\), there exists a distribution \(\mu\) over \(\mathcal{X}\) and a target function \(f^{\star}\in\mathcal{F}\) with margin6\(\zeta\) such that the number of queries \(N_{T}\) made by the algorithm on that instance in \(T\) rounds of interaction satisfy_

Footnote 6: When \(\mathcal{A}=\{1,2\}\), recall that \(\mathtt{Margin}(f(x))=|\Pr(y=2\mid f(x))-\Pr(y=1\mid f(x))|=|f(x)|\).

\[\mathbb{E}[N_{T}]=\Omega\Bigg{(}\frac{\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta,\beta)}{40\zeta^{2}}\Bigg{)}.\]

The above lower bound demonstrates that for any algorithm that has a sublinear regret guarantee, a dependence on an additional complexity measure like the star number (or the eluder dimension) is unavoidable in the number of queries in the worst case. This suggests that our upper bound cannot be further improved beyond the discrepancy between the star number and eluder dimension. The following corrolary illustrates the above lower bound.

**Corollary 2**.: _There exists a class \(\mathcal{F}\) with \(|\mathcal{F}|=\sqrt{T}\), and \(\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta,\beta)=O(\sqrt{T})\) for any \(\beta=O(1)\) and \(\zeta=O(1)\), such that any algorithm that makes less than \(\sqrt{T}\) number of label queries, will have a regret of at least \(\mathbb{E}[\mathrm{Reg}_{T}]\geq\sqrt{T}\) on some instance with margin \(\zeta\)._

## 4 Imitation Learning (\(H>1\)) with Selective Queries to an Expert

The problem of Imitation Learning (IL) consists of learning policies in MDPs when one has access to an expert (aka the teacher) that can make suggestions on which actions to take at a given state. IL has enjoyed tremendous empirical success, and various different interaction models have been considered. In the simplest IL setting, studied under the umbrella of offline RL (Levine et al., 2020) or Behavior Cloning (Ross and Bagnell, 2010; Torabi et al., 2018), the learner is given an offline dataset of trajectories (state and action pairs) from an expert and aims to output a well-performing policy. Here, the learner is not allowed any interaction with the expert, and can only rely on the provided dataset of expert demonstrations for learning. A much stronger IL setting is the one where the learner can interact with the expert, and rely on its feedback on states that it reaches by executing its own policies.

In their seminal work, Ross et al. (2011) proposed a framework for interactive imitation learning via reduction to online learning and classification tasks. This has been extensively studied in the IL literature (e.g., Ross and Bagnell (2014); Sun et al. (2017); Cheng and Boots (2018)). The algorithm DAgger from (Ross et al., 2011) has enjoyed great empirical success. On the theoretical side, however, performance guarantees for DAgger only hold under the assumption that, when queried, the expert makes action suggestions from a very good policy \(\pi^{\star}\) that we would like to compete with. However, in practice, human demonstrators are far from being optimal and suggestions from experts should be modeled as noisy suggestions that only correlate with \(\pi^{\star}\). It turns out that IL where one only hasaccess to noisy expert suggestions is drastically different from the noiseless setting. For instance, in the sequel, we show that there can be an exponential separation in terms of the dependence on horizon \(H\) in the sample complexity of learning purely from offline demonstration vs learning with online interactions.

Formally, we consider interactive IL in an episodic finite horizon Markov Decision Process (MDP), where the learner can query a noisy expert for feedback (i.e., action) on the states that it visits. The game proceeds in \(T\) episodes. In each episode \(t\), the nature picks the initial state \(x_{t,1}\) for \(h=1\); then for every time step \(h\in[H]\), the learner proposes an action \(\hat{y}_{t,h}\in[K]\) given the current state \(x_{t,h}\); then the system proceeds by selecting the next state \(x_{t,h+1}\leftarrow\mathbb{T}_{t,h}(x_{t,h},\hat{y}_{t,h})\), where \(\mathbb{T}_{t,h}:\mathcal{X}\times\mathcal{Y}\mapsto\mathcal{X}\) denotes the deterministic dynamics at timestep \(h\) of round \(t\) and is unknown to the learner. The learner then decides whether to query the expert for feedback. If the learner queries, it receives a recommended action from the expert, and otherwise the learner does not receive any additional information. The game moves on to the next time step \(h+1\), and moves to the next episode \(t+1\) when it reaches to time step \(H\) in the current episode. We now describe the expert model. With \(f_{h}^{*}\) being the underlying score function at time step \(h\), the expert feedback is sampled from a distribution \(\phi(f_{h}^{*}(x))\in\Delta(K)\), with \(\phi:\mathbb{R}^{K}\mapsto\mathbb{R}^{K}\) being some link function (e.g., \(\phi(p)[i]\propto\exp(p[i])\)). The goal of the leaner is to perform as well as the Bayes optimal policy7 defined as \(\pi_{h}^{*}(x):=\operatorname*{argmax}_{a\in[K]}\phi(f_{h}^{*}(x))\). In particular, the learner aims to find a sequence of policies \(\{\pi_{t}\}_{t\leq T}\) that have a small cumulative regret defined w.r.t. some (unknown) reward function under possibly adversarial (and unknown) transition dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H,t\leq T}\). At the same time, the learner wants to minimize the number of queries made to the expert. Formally, we consider counterfactual regret defined as

Footnote 7: Note that the comparator policy \(\pi^{*}\) reflects the experts models, and may not be the optimal policy for the underlying MDP.

\[\operatorname{Reg}_{T}=\sum_{t=1}^{T}\sum_{h=1}^{H}r(x_{t,h}^{ \pi^{*}},\pi_{h}^{*}(x_{t,h}^{\pi^{*}}))-\sum_{t=1}^{T}\sum_{h=1}^{T}r(x_{t,h},\hat{y}_{t,h})\]

where \(x_{t,h}\) are the states reached by the learner corresponding to the chosen actions and the dynamics, and \(x_{t,h}^{\pi^{*}}\) denotes the states that would have been generated if we executed \(\pi^{*}\) from the beginning of the episode under the same dynamics. The query complexity \(N_{T}\) is the total number of queries to the expert across all \(H\) steps in \(T\) episodes.

Given the selective sampling results we provided in the earlier section, one may be tempted to apply them to the imitation learning problem. However, there is a caveat. A key to the reduction in Ross et al. (2011) is to apply Performance Difference Lemma (PDL) to reduce the problem of IL to online classification under the sequence of state distributions induced by the policies played by the learning algorithm. Hence, if one blindly applied this reduction, then in the margin term, one would need to account for the states that the learner visits (which could be arbitrary). Thus, for DAgger to have meaningful bounds, we would require a large margin over the entire state space. This is too much to ask for in practical applications. Consider the example of learning autonomous driving from a human driver as the expert. It is reasonable to believe that human drivers can confidently provide the right actions when they are driving themselves or are faced with situations they are more familiar with. However, assuming that the human driver is going to be confident in an unfamiliar situation (e.g., an emergency situation that is not often encountered by the human driver), is a strong assumption. Towards that end, we make a significantly weaker, and much more realistic, margin assumption that the expert has a large margin only on the state distribution induced by \(\pi^{*}\), and not on the state distribution of the learner or the noisy expert.8 In particular, we define \(T_{\varepsilon,h}\) to denote the total number of episodes where the comparator policy \(\pi^{*}\) visits a state with low margin at time step \(h\), i.e., \(T_{\varepsilon,h}=\sum_{t=1}^{T}\mathbf{1}\{\texttt{Margin}(f_{h}^{*}(x_{t,h} ^{\pi^{*}}))\leq\varepsilon\}\).

Footnote 8: The precise definition of the Margin for IL is given in the appendix.

We now proceed to our main results in this section. Learning from a noisy expert is indeed very challenging. In fact, learning from noisy expert feedback may even be statistically intractable in the non-interactive IL setting, where the learner is only limited to accessing offline noisy expert demonstrations for learning, e.g. in offline RL, Behavior Cloning, etc. The following lower bound formalizes this. In fact, the same lower bound also shows that AggreVaTe (Ross and Bagnell, 2014) style algorithms would not succeed under noisy expert feedback, AggreVaTe relies on roll-outs obtained by running the (noisy) expert suggestions.

**Proposition 1** (Lower bound for learning from non-interactive noisy demonstrations).: _There exists an MDP, for every \(h\leq H\), a function class \(\mathcal{F}_{h}\) with \(|\mathcal{F}_{h}|\leq 2^{H}\), a noisy expert whose optimal policy \(\pi^{\star}(x)=\operatorname*{argmax}_{a}(f_{h}^{\star}(x)[a])\) for some \(f_{h}^{\star}\in\mathcal{F}_{h}\) with \(T_{\varepsilon,h}=0\) for any \(\varepsilon\leq 1/4\), such than any non-interactive algorithm needs \(\Omega(2^{H})\) many noisy expert trajectory demonstrations to learn, with probability at least \(3/4\), a policy \(\widehat{\pi}\) that is \(1/8\)-suboptimal w.r.t. \(\pi^{\star}\)._

Proposition 1 implies that in order to learn with a reasonable sample complexity (that is polynomial in \(H\)), a learner must be able to interactively query the expert. In Algorithm 2, we provide an interactive imitation learning algorithm (with selective querying) that can learn from noisy expert feedback. The regret bound and query complexity bounds for Algorithm 2 are:

```
0: Params \(\delta,\gamma,\lambda,T\), function classes \(\{\mathcal{F}_{h}\}_{h\leq H}\), online regression oracle \(\mathsf{Oracle}_{h}\) w.r.t. \(\ell_{\phi}\) for \(h\in[H]\).
1: Set \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h},T)=\frac{4}{\lambda}\mathrm{Reg}^{ \ell_{\phi}}(\mathcal{F}_{h};T)+\frac{112}{\lambda^{2}}\log(4H\log^{2}(T)/ \delta)\).
2: Compute \(f_{1,h}=\mathsf{Oracle}_{1,h}(\varnothing)\) for \(h\in[H]\).
3:for\(t=1\) to \(T\)do
4: Nature chooses the state \(x_{t,1}\).
5:for\(h=1\) to \(H\)do
6: Learner plays \(\widehat{y}_{t,h}=\mathsf{SelectAction}(f_{t,h}(x_{t,h}))\)
7: Learner transitions to the next state in this round \(x_{t,h+1}\leftarrow\mathbb{T}_{t,h}(x_{t,h},\widehat{y}_{t,h})\).
8: Learner computes \[\Delta_{t,h}:=\max_{f\in\mathcal{F}_{h}}|f(x_{t,h})-f_{t,h}(x_{t,h})|\text{ s.t. }\sum_{s=1}^{t-1}Z_{s,h}\|f(x_{s,h})-f_{s,h}(x_{s,h})\|^{2}\leq\Psi_{\delta}^{ \ell_{\phi}}(\mathcal{F}_{h},T).\] (6)
9: Learner decides whether to query: \(Z_{t,h}=\mathbf{1}\{\mathsf{Margin}(f_{t,h}(x_{t,h}))\leq 2\gamma\Delta_{t,h}\}\).
10:if\(Z_{t,h}=1\)then
11: Learner queries the label \(y_{t,h}\) for \(x_{t,h}\).
12:\(f_{t+1,h}\leftarrow\mathsf{Oracle}_{t+1,h}(\{x_{t,h},y_{t,h}\})\)
13:else
14:\(f_{t+1,h}\gets f_{t,h}\) ```

**Algorithm 2** \(\mathsf{IneRAciVe}\) **ImitatiOn Learning VIa Active Expert Querying (RAVIOLI)**

**Theorem 4**.: _Let \(\delta\in(0,1)\). Under the modeling assumptions above, with probability at least \(1-\delta\), Algorithm 2 obtains:_

\[\mathrm{Reg}_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{H\sum_ {h=1}^{H}T_{\varepsilon,h}+\frac{H\gamma^{2}}{\lambda\varepsilon^{2}}\sum_{h=1 }^{H}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F}_{h};T)\right\}\right)\!,\qquad \text{and,}\] \[N_{T} =\widetilde{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{H\sum_ {h=1}^{H}T_{\varepsilon,h}+\frac{H\gamma^{2}}{\lambda\varepsilon^{2}}\sum_{h=1 }^{H}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F}_{h};T)\cdot\mathfrak{E}(\mathcal{F }_{h},\varepsilon\!/\!s\gamma;f_{h}^{\star})\right\}\right)\!.\]

Since the above bound holds for any sequence of dynamics \(\{\mathbb{T}_{h,t}\}_{h\leq H,t\leq T}\), the result of Theorem 4 also holds for the stochastic IL setting where the transition dynamic is stochastic but fixed during the interaction. In particular, setting \(\mathbb{T}_{h,t}\sim\widehat{\mathcal{T}}_{h}\) sampled i.i.d. from a fixed stochastic dynamics \(\{\widehat{\mathcal{T}}_{h}\}_{h\leq H}\) recovers a similar bound for the stochastic setting.

### Learning from Multiple Experts

In Dekel et al. (2012), the problem of selective sampling from multiple experts is considered with the main motivation being that we can consider each expert as being confident (and correct) in certain states or scenarios, and we would like to learn from their joint feedback. The goal there is to perform not only as well as the best of them individually but even as well as the best combination of them. Consider the example of learning to drive from human demonstrations, we might have one human demonstrator who is an expert in highway driving, another human who is an expert in city driving, and the third one in off-road conditions. Each expert is confident in their own terrain, but we would like to learn a policy that can perform well in all terrains.

The formal model is similar to the single-expert case, but we now have \(M\) experts. For every time step \(h\leq H\), the \(m\)-th expert has an underlying ground truth model \(f_{h}^{\star,m}\in\mathcal{F}_{h}^{m}\) that it uses to produceits label, i.e. for a given state \(x_{h}\) it draws its label as \(y_{h}^{m}\sim\phi(f_{h}^{\star,m}(x_{h}))\), where \(\phi\) is the link function. On rounds in which the learner queries for the experts feedback, it gets back a label from each of the \(M\) experts, i.e. \(\{y_{h}^{1},\ldots,y_{h}^{M}\}\). While on every query the learner gets a different label from each expert, its objective is to perform as well as a comparator policy that is defined w.r.t. some ground truth aggregation function that we define next.

The aggregation function \(\mathscr{A}:\Delta([K])^{M}\mapsto\Delta([K])\), known to the learner, combines the recommendation of the \(M\) experts to obtain a ground truth label for the corresponding state. In particular, on a given state \(x_{h}\), the label \(y_{h}\) is samples as:

\[y_{h}\sim\mathscr{A}\big{(}\phi(f_{h}^{\star,1}(x_{h})),\ldots,\phi(f_{h}^{ \star,M}(x_{h}))\big{)}.\] (7)

Given the aggregation function \(\mathscr{A}\) and the above label generation process, the policy \(\pi^{\star}\) that we wish to compete with in our regret bound is simply the Bayes optimal predictor given by

\[\pi^{\star}(x_{h})=\texttt{SelectAction}(\mathscr{A}\big{(}(\phi(f_{h}^{\star,1 }(x_{h})),\ldots,\phi(f_{h}^{\star,M}(x_{h})))\big{)}),\] (8)

where \(\texttt{SelectAction}:\Delta(K)\mapsto[K]\) is given by \(\texttt{SelectAction}(p)=\operatorname*{argmax}_{k\in[K]}p[K]\). Some illustrative examples of aggregation functions are given in Appendix F.5. Our main Theorem 5 below bounds the number of label queries to the experts, and regret with respect to this \(\pi^{\star}\), and is obtained using the imitation learning algorithm given in Algorithm 4 in Appendix F.5.

Our bounds depend on a margin term \(T_{\varepsilon,h}\), that captures the number of rounds in which the Bayes optimal predictor \(\pi^{\star}\) can flip its label if our estimates of the \(M\) experts are off by at most \(\varepsilon\) (in \(\ell_{\infty}\) norm). Similar to the single expert case, we only pay in the margin term for time steps in which the counterfactual trajectory w.r.t. the policy \(\pi^{\star}\) has a small-margin. We note that while the trajectories taken by the learner or the noisy experts may go through states that have a large-margin, the margin term \(T_{\varepsilon,h}\) that appears in our bounds only accounts for time steps when the comparator policy \(\pi^{\star}\) (the optimal aggregation of expert recommendations) would go to a small-margin region, which could be much smaller. For the ease of notation, we defer the exact definition of margin, and the term \(T_{\varepsilon,h}\) to Appendix F.5, and state the main result below:

**Theorem 5**.: _Let \(\delta\in(0,1)\). Under the modeling assumptions above for the multiple experts setting, with probability at least \(1-\delta\), the imitation learning Algorithm 4 (given in the appendix) obtains:_

\[\operatorname{Reg}_{T} =\bar{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{H\sum_{h=1} ^{H}T_{\varepsilon,h}+\frac{H}{\lambda\varepsilon^{2}}\sum_{m=1}^{M}\sum_{h=1 }^{H}\operatorname{Reg}^{\ell_{\phi}}(\mathcal{F}_{h}^{m};T)\right\}\right), \qquad\text{and,}\] \[N_{T} =\bar{\mathcal{O}}\!\left(\inf_{\varepsilon}\!\left\{H\sum_{h=1} ^{H}T_{\varepsilon,h}+\frac{H}{\lambda\varepsilon^{2}}\sum_{h=1}^{H}\sum_{m=1 }^{M}\operatorname{Reg}^{\ell_{\phi}}(\mathcal{F}_{h}^{m};T)\cdot\mathfrak{E} (\mathcal{F}_{h}^{m},\varepsilon\!/\!8;f_{h}^{\star,m})\right\}\right).\]

In Appendix A, we evaluate our IL algorithm on the Cartpole environment, with single and multiple experts. We found that our algorithm can match the performance of passive querying algorithms while making a significantly lesser number of expert queries. Finally, note that setting \(H=1\) in the above result, recovers an algorithm, and a similar result for selective sampling with multiple experts.

## Conclusion

In this paper, or goal is to develop algorithms for online IL with active queries with small regret and query complexity bounds. Towards that end, we started by considering the selective sampling setting (IL with \(H=1\)), and provided a selective sampling algorithm that can work with general function classes \(\mathcal{F}\) and modeling assumptions, and relies on access to an online regression oracle w.r.t. \(\mathcal{F}\) to make its predictions (Section 3). The provided regret and query complexity bounds depend on the margin of the expert model. We then extended our selective sampling algorithm to interactive IL (Section 4). For IL, we showed that the margin term that appears in the regret and the query complexity depends on the margin of the expert on counterfactual trajectories that would have been observed on following the expert policy (that we wish to compare to), instead of the trajectories that the learner observes. Thus, if the expert always chooses actions that leads to states where it is confident (i.e. has less margin), the margin term will be smaller. We also considered extensions to learning with multiple experts.

## Acknowledgements

AS thanks Sasha Rakhlin and Dylan Foster for helpful discussions. AS acknowledges support from the Simons Foundation and NSF through award DMS-2031883, as well as from the DOE through award DE-SC0022199. WS acknowledges support from NSF grant IIS-2154711. KS acknowledges support from NSF CAREER Award 1750575, and LinkedIn-Cornell grant.

## References

* Agarwal (2013) Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In _International Conference on Machine Learning_, pages 1220-1228. PMLR, 2013.
* Barto et al. (1983) Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. _IEEE transactions on systems, man, and cybernetics_, (5):834-846, 1983.
* Beliaev et al. (2022) Mark Beliaev, Andy Shih, Stefano Ermon, Dorsa Sadigh, and Ramtin Pedarsani. Imitation learning by estimating expertise of demonstrators. In _International Conference on Machine Learning_, pages 1732-1748. PMLR, 2022.
* Brantley et al. (2019) Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In _International Conference on Learning Representations_, 2019.
* Brantley et al. (2020) Kiante Brantley, Amr Sharaf, and Hal Daume III. Active imitation learning with noisy guidance. _arXiv preprint arXiv:2005.12801_, 2020.
* Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Cao et al. (2022) Zhangjie Cao, Zihan Wang, and Dorsa Sadigh. Learning from imperfect demonstrations via adversarial confidence transfer. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 441-447. IEEE, 2022.
* Cesa-Bianchi and Lugosi (2006) Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Cesa-Bianchi et al. (2005) Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. _IEEE Transactions on Information Theory_, 51(6):2152-2162, 2005.
* Chang et al. (2015) Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume III, and John Langford. Learning to search better than your teacher. In _International Conference on Machine Learning_, pages 2058-2066. PMLR, 2015.
* Cheng and Boots (2018) Ching-An Cheng and Byron Boots. Convergence of value aggregation for imitation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1801-1809. PMLR, 2018.
* Cheng et al. (2020) Ching-An Cheng, Andrey Kolobov, and Alekh Agarwal. Policy improvement via imitation of multiple oracles. _Advances in Neural Information Processing Systems_, 33:5587-5598, 2020.
* Dekel et al. (2012) Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single and multiple experts. _The Journal of Machine Learning Research_, 13(1):2655-2697, 2012.
* Du et al. (2023) Maximilian Du, Suraj Nair, Dorsa Sadigh, and Chelsea Finn. Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets. _arXiv preprint arXiv:2304.08742_, 2023.
* Foster et al. (2018a) Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 1539-1548. PMLR, 2018a.
* Foster et al. (2018b) Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. In _Conference On Learning Theory_, pages 167-208. PMLR, 2018b.
* Foster et al. (2018c)Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. _arXiv preprint arXiv:2010.03104_, 2020.
* Hanneke and Yang (2015) Steve Hanneke and Liu Yang. Minimax analysis of active learning. _J. Mach. Learn. Res._, 16(1):3487-3602, 2015.
* Hanneke and Yang (2021) Steve Hanneke and Liu Yang. Toward a general theory of online selective sampling: Trading off mistakes and queries. In _International Conference on Artificial Intelligence and Statistics_, pages 3997-4005. PMLR, 2021.
* Hao et al. (2022) Yilun Hao, Ruinan Wang, Zhangjie Cao, Zihan Wang, Yuchen Cui, and Dorsa Sadigh. Masked imitation learning: Discovering environment-invariant modalities in multimodal demonstrations. _arXiv preprint arXiv:2209.07682_, 2022.
* Hejna and Sadigh (2023) Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward function. _arXiv preprint arXiv:2305.15363_, 2023.
* Kakade and Langford (2002) Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _In Proc. 19th International Conference on Machine Learning_. Citeseer, 2002.
* Krishnamurthy et al. (2017) Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, and John Langford. Active learning for cost-sensitive classification. In _International Conference on Machine Learning_, pages 1915-1924. PMLR, 2017.
* Laskey et al. (2016) Michael Laskey, Sam Staszak, Wesley Yu-Shu Hsieh, Jeffrey Mahler, Florian T Pokorny, Anca D Dragan, and Ken Goldberg. Shiv: Reducing supervisor burden in dagger using support vectors for efficient learning from demonstrations in high dimensional state spaces. In _2016 IEEE International Conference on Robotics and Automation (ICRA)_, pages 462-469. IEEE, 2016.
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Li et al. (2022) Gene Li, Pritish Kamath, Dylan J Foster, and Nati Srebro. Understanding the eluder dimension. _Advances in Neural Information Processing Systems_, 35:23737-23750, 2022.
* Mendelson (2002) Shahar Mendelson. Rademacher averages and phase transitions in glivenko-cantelli classes. _IEEE transactions on Information Theory_, 48(1):251-263, 2002.
* Mou et al. (2020) Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning with policy space generalization. _arXiv preprint arXiv:2008.07353_, 2020.
* Nguyen and III (2020) Khanh Nguyen and Hal Daume III. Active imitation learning from multiple non-deterministic teachers: Formulation, challenges, and algorithms. _arXiv preprint arXiv:2006.07777_, 2020.
* Osband and Van Roy (2014) Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27, 2014.
* Pan et al. (2018) Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keutak Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end deep imitation learning. In _Robotics: science and systems_, 2018.
* Rakhlin and Sridharan (2014) Alexander Rakhlin and Karthik Sridharan. Online non-parametric regression. In _Conference on Learning Theory_, pages 1232-1264. PMLR, 2014.
* Rakhlin and Sridharan (2015) Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss functions. _arXiv preprint arXiv:1501.06598_, 2015.
* Ross and Bagnell (2010) Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 661-668. JMLR Workshop and Conference Proceedings, 2010.
* Ross and Bagnell (2014) Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. _arXiv preprint arXiv:1406.5979_, 2014.
* Ross et al. (2015)Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
* Ross et al. (2013) Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J Andrew Bagnell, and Martial Hebert. Learning monocular reactive uav control in cluttered natural environments. In _2013 IEEE international conference on robotics and automation_, pages 1765-1772. IEEE, 2013.
* Russo and Van Roy (2013) Daniel Russo and Benjamin Van Roy. eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Shalev-Shwartz and Ben-David (2014) Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* Srebro et al. (2010) Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. _Advances in neural information processing systems_, 23, 2010.
* Sun et al. (2017) Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In _International conference on machine learning_, pages 3309-3318. PMLR, 2017.
* Torabi et al. (2018) Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. _arXiv preprint arXiv:1805.01954_, 2018.
* Tsybakov (2004) Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. _The Annals of Statistics_, 32(1):135-166, 2004.
* Zhu and Nowak (2022) Yinglun Zhu and Robert Nowak. Efficient active learning with abstention. _arXiv preprint arXiv:2204.00043_, 2022.

###### Contents of Appendix

* A Experiments
* B Further Discussion on Related Works
* C Discussion on Computationally Efficiency
* D Useful Tools and Notation
* D.1 Basic Probabilistic Tools
* D.2 Online Learning
* D.3 Eluder Dimension, Disagreement Coefficient, and Star Number
* E Selective Sampling: Learning from Single Expert
* E.1 Comparison to Related Works
* E.2 Proof Sketch for Selective Sampling and Binary Labels
* E.3 Proof of Theorem 1
* E.3.1 Supporting Technical Results
* E.3.2 Regret Bound
* E.3.3 Total Number of Queries
* E.4 Proof of Theorem 2
* E.4.1 Supporting Technical Results
* E.4.2 Regret Bound
* E.4.3 Total Number of Queries
* E.5 Proof of Corollary 1
* E.6 Proofs for Lower Bounds in Section 3.2
* F Imitation Learning: Learning from Single Expert
* F.1 Imitation Learning Tools
* F.2 Proof of Proposition 1
* F.3 Proof of Theorem 4
* F.3.1 Supporting Technical Results
* F.3.2 Regret Bound
* F.3.3 Total Number of Queries
* F.4 Proof for the Stochastic Setting
* F.5 Proof of Theorem 5
* F.5.1 Supporting Technical Results
* F.5.2 Regret Bound
* F.5.3 Total Number of Queries
Experiments

We conduct experiments to verify our theory. To this end, we first introduce the simulator, _Cart Pole_(Barto et al., 1983; Brockman et al., 2016), and then explain the implementation of our algorithm and the baselines. Finally, we present the results.

Cart Pole.Cart Pole is a classical control problem, in which a pole is attached by an un-actuated joint to a cart. The goal is to balance the pole by applying force to the cart either towards the left or towards the right (so binary action). The episode is terminated once either the pole is out of balance or the cart deviates too far from the origin. A reward of 1 is obtained in each time step (however, the algorithm does not get any reward signal). The observations are four-dimensional, with the values representing the cart's position, velocity, the pole's angle, and angular velocity. The action is binary, indicating the force is either to the left or to the right.

Expert policies generation.We first generate an optimal policy \(\pi^{*}\) (that attains the maximum possible reward of 500) by policy gradient. We notice that when running the optimal policy \(\pi^{*}\), the absolute value of the cart's position only lies in \([0,2]\). Hence, to generate \(M\) experts, we first divide this interval into \(M\) sub-intervals \([a_{0},a_{1}]\),\([a_{1},a_{2}]\),\(\ldots\),\([a_{n-1},a_{M}]\left(a_{0}=0\text{ and }a_{M}=2\right)\) by geometric progression. For the \(i\)-th expert, it plays the same action as \(\pi^{*}\) when the absolute value of the cart's position is in the interval \([a_{i-1},a_{i}]\) and plays uniformly at random outside of this interval. We find that using such generation, each expert individually cannot achieve a good performance (when \(M>1\)), while a proper combination of them can still be as strong as \(\pi^{*}\). We conduct experiment for \(M=1,2,3,\) and \(5\), respectively. Given this design of expert generation, when the cart is in the sub-interval \([a_{i-1},a_{i}]\), the only expert with non-zero margin is exactly the \(i\)-th expert.

Implementation.The algorithm is similar to Algorithm 4 but with some modification for practical purpose. First, we use a neural network (single hidden layer neural network,with 4 neurons in the hidden layer) as our function class \(\{\mathcal{F}_{h}^{m}\}_{h\leq H,m\leq M}\). Second, we specify SelectAction to pick the action of the most confident expert, i.e.,

\[\texttt{SelectAction}(f_{t,h}^{1}(x),\ldots,f_{t,h}^{M}(x)):=\text{sign}(f_{t,h}^{i}(x))\quad\text{where}\quad\hat{i}=\operatorname*{arg\,max}_{i\in[M]}| f_{t,h}^{i}(x)|.\]

Since we are considering binary action, we assume \(f_{t,h}^{i}(x)\in[-1,1]\), and the action space is \(\{-1,1\}\). Third, to compute \(\Delta_{t,h}^{m}\) efficiently, we apply the Lagrange multiplier to (60) to arrive at the following equivalent problem:

\[\Delta_{t,h}^{m}(x_{t,h}):=\min_{f\in\mathcal{F}_{h}^{m}}\max_{ \alpha\geq 0}\,-\|f(x_{t,h})-f_{t,h}^{m}(x_{t,h})\|\] \[+\alpha\left(\sum_{s=1}^{t-1}Z_{s,h}\big{\|}f(x_{s,h})-f_{s,h}^{ m}(x_{s,h})\right\|^{2}-\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T) \right).\]

Then we treat the Lagrange multiplier \(\alpha\) as a constant, which converts the problem into the following:

\[\Delta_{t,h}^{m}(x_{t,h}):=\min_{f\in\mathcal{F}_{h}^{m}}\,-\|f(x_{t,h})-f_{t,h}^{m}(x_{t,h})\|+\alpha\sum_{s=1}^{t-1}Z_{s,h}\big{\|}f(x_{s,h})-f_{s,h}^{m} (x_{s,h})\big{\|}^{2}.\] (9)

The study of varying \(\alpha\) is shown in Figure 1. We found that small values (e.g., \(\alpha=1\)) mostly lead to poor performance, while the results are fairly similar for large values. In our key experiments, we choose \(\alpha=50\) when the number of experts is 1, 2 or 3, and choose 200 for 5-expert experiments. We note that since computing (9) for each time step involves repetitively fitting neural networks, which is time-consuming, we do a warm start at each round. In particular, we set the initial weights for the neural network of each round to be the weights of the trained network from the previous round. We also implemented _early stopping_ that stops the iteration if the loss does not significantly decrease for multiple consecutive iterations. The online regression oracle Oracle is instantiated as applying gradient descent for certain steps on the mean squared loss over all data collected so far, using warm start for speedup as well.

We first conduct experiments on a single expert setting. In Figure 2 we plot the curves of return and number of queries with respect to iterations for our method, and compare to DAgger (which passively makes queries at every time step; Ross and Bagnell (2014)). We note that while our algorithm does not converge to the optimal value as fast as DAgger, the number of queries made by our algorithm is significantly fewer, which means that our method is indeed balancing the speed of learning and the number of queries.

In additional to DAgger, we also compare to the following baselines:

* **Passive learning.** By passive learning, we mean running our algorithms with \(Z_{t,h}=1\), i.e., making queries whenever possible. Based on different styles of expert feedback, we divide the passive learning baselines into two: _noisy experts_ and _noiseless experts_. For the former we get the noisy label \(y^{m}_{t,h}\) for \(x_{t,h}\) (generated by \(y^{m}_{t,h}\sim\phi(f^{*,m}_{h}(x_{t,h}))\)), and for the latter we directly get the action of the optimal policy (i.e. the action \(\pi^{*}_{h}(x_{t,h})\)). Intuitively, noiseless feedback is more helpful than the noisy one.
* **MAMBA.** We compare our algorithm with (a slight variant of) MAMBA (Cheng et al., 2020). At each time step, it creates copies of the environment and run each expert policy on these copies, and then it selects the action of the expert policy with the highest return. For simplicity, we refer to this algorithm as MAMBA. Note that MAMBA assumes that one has access to the underlying reward function. Thus this baseline is using significantly more information than our approach.
* **Best expert.** We also compared our algorithm with the best expert policy.

The main results are shown in Figure 3. We first noticed that our algorithm outperforms passive learning with noisy experts in all settings. Moreover, we beat the noiseless version when there is only one expert. Intuitively, getting feedback from noiseless experts is a very strong assumption and it is not surprising to see that the performance is improved with this stronger feedback. Note that our algorithm is only getting noisy labels as feedback. We also note that, despite the fact that MAMBA achieves better results than the best expert policy (in terms of the value function), it is still worse than our algorithm. Indeed, MAMBA does not even learn a policy that can solve the task when \(M\geq 2\). This is because by our construction of experts, there is no single expert that is capable of solving the task alone. Note that MAMBA performs well in the one expert case because in that case, the (single) expert can reliably solve the control task.

Figure 1: Learning curves of return with respect to the number of queries for different values of \(\alpha\) and different numbers of experts.

Figure 2: Learning curves of the return and the number of queries for 1 expert.

Further Discussion on Related Works

Selective Sampling.There is a large bank of both theoretical and empirical work for active learning and selective sampling. Perhaps the work closest to ours is the work of Zhu and Nowak (2022). In this paper, the authors consider binary classification problem and provide bounds on number of queries and bound on excess risk in the active learning framework. Their algorithm also relies on regression oracle. However, there are many key differences: Firstly, their guarantees for regret for selective sampling problem (see for instance Theorem 10 on page 28 of Zhu and Nowak (2022)) has a dependence on disagreement coefficient in the regret bound as well as number of queries. On the contrary, as we show in our work, one only needs to pay for eluder dimension or disagreement coefficient in query complexity and not in regret bound. Furthermore, we supplement our result with lower bound showing that unless one has label complexity that depends on star number (and hence can be also related to worst case disagreement coefficient), one can not get a small enough regret bound. So the separation between regret bound (that is independent of eluder dimension/star number/disagreement coefficient) and query complexity (that depends on those quantities) is real. Secondly, the results in (Zhu and Nowak, 2022) font automatically adapt to the margin region and in general there is no way to estimate the parameters of Tsybakov's noise condition. Finally, their regret bounds depend on pseudo dimension and are thus generally suboptimal for complex \(\mathcal{F}\).

Imitation Learning.IL has enjoyed tremendous research from both theoretical and empirical perspective in the last decade; notable references include Ross et al. (2011), Ross and Bagnell (2014), Sun et al. (2017), Chang et al. (2015), Brantley et al. (2019, 2020), Nguyen and Daume III (2020). Ross et al. (2011) initiated research on using online regression oracles to model the expert feedback, and provided regret bounds for IL. The key differences between our work and prior theoretical works on IL are as follows: Firstly, we consider active querying, and provide query complexity bounds for our algorithms. Secondly, and more importantly, we consider interactive IL with noisy expert feedback whereas prior works was restricted to exact expert feedback. Finally, our regret and query complexity bounds scale with the number of times when the comparator policy (induced by the expert) goes to the states where expert has a small margin (instead of the number of times when the learner goes to such states). In many cases, the margin error term corresponding to the comparator policy could be much smaller. On the empirical side, there is a long line of research that provided algorithms and empirical heuristics for making IL sample efficient by modeling the experts in both single expert and multiple expert settings (Beliaev et al., 2022; Cao et al., 2022; Hejna and Sadigh, 2023; Du et al., 2023; Hao et al., 2022); however most of these algorithm do not come with any rigorous guarantees.

## Appendix C Discussion on Computationally Efficiency

Given the theoretical nature of this paper, our focus is to understand the statistically efficient (query complexity), and to develop algorithms, for selective sampling and imitation learning with general model classes \(\mathcal{F}\), given access to an online regression oracle w.r.t. \(\mathcal{F}\). For a general function class, our algorithm is computationally inefficient. However, in many cases, our algorithm (or its slight modification) can also be implemented efficiently. We describe some scenarios below:

Figure 3: Learning curves of return with respect to the number of queries for different algorithms and numbers of experts.

* **1D-Linear models**: When \(f:\mathcal{X}\mapsto\mathbb{R}\) is linear, the optimization objective in (5) in Algorithm 1 (or (6) in Algorithm 2) can be efficiently by instead solving the objectives \[\Delta_{t}^{(1)}(x_{t})=\max_{f\in\mathcal{F}}f(x_{t})-f_{t}(x_{t})\text{ s.t. }\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|\leq\Psi,\] and \[\Delta_{t}^{(2)}(x_{t})=\max_{f\in\mathcal{F}}-(f(x_{t})-f_{t}(x_{t}))\text{ s.t. }\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|\leq\Psi\] and then picking the maximum absolute value. Both of these new objectives are linear functions, with convex constraints, and thus can be solved efficiently using a standard solver (e.g. CvXOPT).
* **Differentiable parameterizations**: When class \(\mathcal{F}\) could be parameterized in a differentiable way, e.g. using a neural network, we can simply add the constraints as a penalty (with the appropriate multiplicative scale parameter) to convert (5) into an unconstrained optimization problem w.r.t the parameters \(\theta\) (of the differentiable parameterization) and then solve it using SGD algorithm. While this is only a heuristic, it works well in practice and is precisely what we do for our experiments in A with 2 layer neural networks.
* **Efficient Implementation of (5) via calls to a Regression Oracles w.r.t. \(\mathcal{F}\)**. Below we discuss some scenarios and minor modifications of our algorithms under which the computation of \(\Delta_{t}\) (as in (5) in Algorithm 1 or (6) in Algorithm 2) can be performed efficiently via calls to a regression oracle w.r.t. \(\mathcal{F}\). Suppose that \(\mathcal{F}\) is closed under convexification. We consider two scenarios:
* **Binary Actions Setting**: In this case, we can simply choose \(\mathcal{F}\) to be a class of 1D functions, since we can model the expert using a function \(f^{*}:\mathcal{X}\mapsto\mathbb{R}\in\mathcal{F}\) where for any \(x\), \(f^{*}(x)\) denotes the probability of choosing the first action. The probability of choosing the second action would then be \(1-f^{*}(x)\). In this case, (5) in Algorithm 1 simply reduces to \[\Delta_{t}^{\prime}(x_{t})=\max_{f\in\mathcal{F}}|f(x_{t})-f_{t}(x_{t})|\text { s.t. }\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|\leq\Psi.\] The above can be implemented efficiently using the techniques from Foster et al. (2018). In particular, let \(r_{\max}\) denote the maximum value that \(f(x)\) can take. We can solve the above objective using the BINSEARCH procedure in Foster et al. (2018) where we perform a binary search over a weight parameter \(w\), by solving for each \(w\) the optimization problems: \[\arg\min_{f\in\mathcal{F}}\left(w\cdot\left(f(x_{t})-f_{t}(x_{t})-2r_{\max} \right)^{2}+\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|\right)\] and \[\arg\min_{f\in\mathcal{F}}\left(w\cdot\left(f(x_{t})-f_{t}(x_{t})+2r_{\max} \right)^{2}+\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|\right)\] both of which can be efficiently implemented using square loss regression oracles to \(\mathcal{F}\). We refer to Foster et al. (2018) for more details.
* **Multiple Actions Setting:** Suppose that \(\mathcal{F}\) is a product class of \(\{\mathcal{F}_{k}\}\) for different actions \(k\in[K]\). We can get an oracle-efficient algorithm for a slight modification of (5), at a price of an extra multiplicative \(K\) factor in the query complexity bound. Consider the \(\Delta_{t}(x_{t})\) given by \(\max_{f\in\mathcal{F}}|f(x_{t})-f_{t}(x_{t})|_{\infty}\text{ s.t. }\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|_{2}\leq\Psi\), which is equal to \(\max_{k\in[K]}\max_{f\in\mathcal{F}}|f_{k}(x_{t})-f_{t,k}(x_{t})|\text{ s.t. }\sum_{s=1}^{t-1}Z_{s}|f(x_{s})-f_{s}(x_{s})|_{2}\leq\Psi\), which can again be implemented via calls to a square loss regression oracle w.r.t. \(\mathcal{F}\) by using BINSEARCH procedure in Foster et al. (2018) (similar to what we did for the Binary actions case above).

Useful Tools and Notation

Additional notation.Throughout the paper, we assume that the ties are broken arbitrarily but consistently. Vector-valued variables are denoted with small alphabets like \(u,v\), etc, and matrix-valued variables are denoted with capital alphabets like \(F,G\), etc. For any two distributions \(D_{1}\) and \(D_{2}\), we define \(\mathrm{KL}(D_{1}\|D_{2})\) to denote the KL divergence between \(D_{1}\) and \(D_{2}\). Furthermore, \(\mathrm{kl}(b_{1}\|b_{2})\) denotes the KL divergence between \(\mathrm{Bernoulli}(b_{1})\) and \(\mathrm{Bernoulli}(b_{2})\). Finally, we assume that \(\|f(x)\|\leq B\leq 1\) for any \(f\in\mathcal{F}\) and \(x\in\mathcal{X}\).

The following lemma is used throughout the appendix, and its proof is trivial.

**Lemma 1**.: _Let \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\) be any two events such that \(\mathcal{E}_{1}\implies\mathcal{E}_{2}\) then \(\mathbf{1}\{\mathcal{E}_{1}\}\leq\mathbf{1}\{\mathcal{E}_{2}\}\)._

### Basic Probabilistic Tools

**Lemma 2** (Theorem 1 in Srebro et al. (2010)).: _Let \(T>0\), and let \(\mathcal{F}=\{\mathcal{X}\times\mathcal{Y}\}\) be an arbitrary function class, and \(\ell\) be an \(\gamma\)-smooth and non-negative loss such that \(|\ell(f(x),y)|\leq B\) for all \(x\in\mathcal{X},y\in\mathcal{Y},f\in\mathcal{F}\). For any \(\delta>0\), we have with probability at least \(1-\delta\) over a random sample of size \(T\), for any \(f\in\mathcal{F}\),_

\[T\operatorname{\mathbb{E}}_{(x,y)-\mu}[\ell(f(x),y)]\leq 2\sum_{t=1}^{T} \ell(f(x_{t}),y_{t})+c_{1}\big{(}HT\log^{3}(T)\mathsf{Rad}_{T}^{2}(\mathcal{F} )+B\log(1/\delta)\big{)}\]

_where \(c_{1}<10^{5}\) is a numeric constant, and \(\mathsf{Rad}_{T}(\mathcal{F})\) denotes the Rademacher complexity of the class \(\mathcal{F}\)._

The precise value of the numeric constant \(c_{1}\) in the above can be derived from Srebro et al. (2010) and Mendelson (2002). Note that for finite function classes, we have \(\mathsf{Rad}_{T}(\mathcal{F})=\mathcal{O}\Big{(}\sqrt{\log(|\mathcal{F}|)/T} \Big{)}\) and thus the second term above is bounded by \(\widetilde{\mathcal{O}}(\log(|\mathcal{F}|))\). In general, we have that \(T\mathsf{Rad}_{T}^{2}(\mathcal{F})=\widetilde{\mathcal{O}}(\mathrm{Reg}^{ \mathrm{sq}}(\mathcal{F};T))\)(Rakhlin and Sridharan, 2014), and thus the second term is always dominated by the other terms in our regret and query complexity bounds.

The following inequalities are well-known; we use the version stated in Zhu and Nowak (2022).

**Lemma 3** (Freedman's inequality).: _Let \(\{X_{t}\}_{t\leq T}\) be a real-valued martingale different sequence adapted to the filtration \(\mathfrak{F}_{t}\), and let \(\operatorname{\mathbb{E}}_{t}[\cdot]:=\operatorname{\mathbb{E}}[\cdot| \mathfrak{F}_{t-1}]\). If \(|X_{t}|\leq B\) almost surely, then for any \(\eta\in(0,1/B)\), the following holds with probability at least \(1-\delta\):_

\[\sum_{t=1}^{T}X_{t}\leq\eta\sum_{t=1}^{T}\operatorname{\mathbb{E}}_{t}[X_{t}^ {2}]+\frac{B\log(1/\delta)}{\eta}.\]

**Lemma 4**.: _Let \(\{X_{t}\}_{t\leq T}\) be a sequence of positive valued random variables adapted to the filtration \(\mathfrak{F}_{t}\), and and let \(\operatorname{\mathbb{E}}_{t}[\cdot]:=\operatorname{\mathbb{E}}[\cdot| \mathfrak{F}_{t-1}]\). If \(X_{t}\leq B\) almost surely, then with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}X_{t}\leq\frac{3}{2}\sum_{t=1}^{T}\operatorname{\mathbb{E}}_{t} [X_{t}]+4B\log(2/\delta),\]

_and_

\[\sum_{t=1}^{T}\operatorname{\mathbb{E}}_{t}[X_{t}]\leq 2\sum_{t=1}^{T}X_{t}+8B \log(2/\delta).\]

### Online Learning

**Lemma 5**.: _Suppose that the labels are generated according to the (2) where the link function satisfies Assumption 1. Additionally, assume that the regression oracle satisfies the guarantee (3). Then, for any \(\delta\leq 1/e\) and \(T\geq 3\), with probability at least \(1-\delta\), we have for all \(t\leq T\),_

\[\sum_{s=1}^{t}\lVert f_{s}(x_{s})-f^{\star}(x_{s})\rVert^{2}\leq\Psi_{\delta} ^{\ell_{\phi}}(\mathcal{F},T):=\frac{4}{\lambda}\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta),\]

_where \(B\) is defined such that \(\sup_{x}f(x)\leq B\)._Proof.: Using Agarwal (2013, Lemma 2) along with an Union bound implies that for all \(t\leq T\),

\[\sum_{s=1}^{t}\|f_{s}(x_{s})-f^{\star}(x_{s})\|^{2}\leq\frac{4}{\lambda}\sum_{s= 1}^{t}(\ell_{\phi}(f_{s}(x_{s}),y_{s})-\ell_{\phi}(f^{\star}(x_{s}),y_{s}))+ \frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta).\]

Plugging in the regret bound (3) in the above, we get that

\[\sum_{s=1}^{t}\|f_{s}(x_{s})-f^{\star}(x_{s})\|^{2}\leq\sum_{s=1}^{T}\|f_{s}(x_{ s})-f^{\star}(x_{s})\|^{2}\leq\frac{4}{\lambda}\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta).\]

### Eluder Dimension, Disagreement Coefficient, and Star Number

For the sake of completeness, we recall the scalar versions of scale-sensitive eluder dimension, and disagreement coefficient introduced in Russo and Van Roy (2013), Foster et al. (2020), which is defined for a class \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}\}\) of scalar valued functions.

**Definition 4** (Scale-sensitive eluder dimension (scalar version), Russo and Van Roy (2013), Foster et al. (2020)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}\}\). Fix any \(f^{\star}\in\mathcal{F}\), and define \(\mathfrak{E}^{\prime}(\mathcal{F},\beta;f^{\star})\) to be the length of the longest sequence of contexts \(x_{1},x_{2},\ldots x_{m}\) such that for all \(i\), there exists \(f_{i}\in\mathcal{F}\) such that_

\[|f_{i}(x_{i})-f^{\star}(x_{i})|>\beta,\quad\text{and}\quad\sum_{j<\mathrm{t}} (f_{i}(x_{j})-f^{\star}(x_{j}))^{2}\leq\beta^{2}.\]

_We define the scale-sensitive eluder dimension as \(\mathfrak{E}(\mathcal{F},\beta_{0};f^{\star}):=\sup_{\beta_{0}\geq\beta} \mathfrak{E}^{\prime}(\mathcal{F},\beta;f^{\star})\)._

We next provide some examples of function classes with bounded eluder dimension. The examples \((a)\) - \((c)\) first appeared in Russo and Van Roy (2013), and \((d)\) first appeared in Osband and Van Roy (2014).

1. For any function class \(\mathcal{F}:\{\mathcal{X}\mapsto\mathbb{R}\}\) and \(f^{\star}\in\mathcal{F}\), \(\mathfrak{E}(\mathcal{F},\beta_{0};f^{\star})\leq O(|\mathcal{X}|)\).
2. For the class \(\mathcal{F}\) of linear functions on a known feature map \(\phi\) i.e., \(\mathcal{F}=\{f\mid f(x)=(\theta_{f},\phi(x))\,\ \theta_{f}\in\mathbb{R}^{d},\| \theta_{f}\|\leq 1\}\), we have \(\mathfrak{E}(\mathcal{F},\beta_{0};f^{\star})\leq O(d\log(1/\varepsilon))\).
3. For the class \(\mathcal{F}\) of generalized linear functions on a known feature map \(\phi\) i.e., \(\mathcal{F}=\{f\mid f(x)=g((\theta_{f},\phi(x)))\,\ \theta_{f}\in\mathbb{R}^{d},\| \theta_{f}\|\leq 1\}\) where \(g\) is an increasing continuously differentiable function, we have \(\mathfrak{E}(\mathcal{F},\beta_{0};f^{\star})\leq O(dr^{2}\log(L/\varepsilon))\) where \(r=\sup_{\theta_{x},g^{\prime}}(\{(\theta,x)\})_{\inf_{\theta_{x},g^{\prime}}( \{(\theta,x)\}}\) and \(L=\sup_{\theta,x}g^{\prime}(\{(\theta,\phi(x)\}))\).
4. For the class \(\mathcal{F}\) of quadratic functions on a known feature map \(\phi\) i.e. \(\mathcal{F}=\{f\mid f(x)=\phi(x)^{\top}\Sigma_{f}\phi(x)\,\ \Sigma_{f}\in \mathbb{R}^{d\times d},\|\Sigma_{f}\|_{F}\leq 1\}\).

We next recall the definition of scale-sensitive disagreement coefficient which appears in our bounds for the case of stochastic contexts.

**Definition 5** (Scale-sensitive disagreement coefficient (scalar version), Foster et al. (2020)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}\}\). For any \(f^{\star}\in\mathcal{F}\), and \(\gamma_{0},\varepsilon_{0}>0\), the value function disagreement coefficient \(\theta^{\mathrm{val}}(\mathcal{F},\varepsilon_{0},\gamma_{0};f^{\star})\) is defined as_

\[\sup_{\mu}\sup_{\gamma>\gamma_{0},\varepsilon>\varepsilon_{0}}\biggl{\{}\frac{ \varepsilon^{2}}{\gamma^{2}}\cdot\Pr_{x\sim\mu}(\exists f\in\mathcal{F}\mid|f (x)-f^{\star}(x)|>\varepsilon,\|f-f^{\star}\|_{\mu}\leq\gamma)\biggr{\}}\lor 1\]

_where \(\|f\|=\sqrt{\mathbb{E}_{x\sim\mu}[f^{2}(x)]}\)._

As we will show in Lemma 6 below, the scale-sensitive disagreement coefficient of \(\mathcal{F}\) is always bounded by the eluder dimension of \(\mathcal{F}\) upto a constant factor on the dependence on \(\varepsilon_{0}\) and \(\gamma_{0}\). However, the disagreement coefficient can be significantly smaller than the eluder dimension because it can leverage additional distributional structure. We refer the reader to Foster et al. (2020) for bounds on the eluder dimension, and the disagreement coefficient for various function classes. In the following, we extend the above definitions to vector-valued functions to account for the vector-valued function classes that we consider in this work.

**Definition 6** (Scale-sensitive eluder dimension (normed version)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\). Fix any \(f^{\star}\in\mathcal{F}\), and define \(\widetilde{\mathfrak{E}}(\mathcal{F},\beta;f^{\star})\) to be the length of the longest sequence of contexts \(x_{1},x_{2},\ldots x_{m}\) such that for all \(i\), there exists \(f_{i}\in\mathcal{F}\) such that_

\[\|f_{i}(x_{i})-f^{\star}(x_{i})\|>\beta,\quad\text{and}\quad\sum_{j<i}\|f_{i}( x_{j})-f^{\star}(x_{j})\|^{2}\leq\beta^{2}.\]

_We define the scale-sensitive eluder dimension as \(\mathfrak{E}(\mathcal{F},\beta^{\prime};f^{\star})=\sup_{\beta\geq\beta^{ \prime}}\widetilde{\mathfrak{E}}(\mathcal{F},\beta;f^{\star})\)._

We note that the normed eluder dimension can be lower bounded in terms of the eluder dimension of scalar-valued function class obtained by projecting the output of the functions in \(\mathcal{F}\) along different coordinates. Let \(\mathcal{F}_{j}=\{P_{j}f\mid P_{j}f(x)=f(x)[j],\ f\in\mathcal{F}\}\), then clearly, for any \(f^{\star}\in\mathcal{F}\), \(\mathfrak{E}(\mathcal{F},\beta;f^{\star})\geq\sup_{j\in[d]}\mathfrak{E}(P_{j }\mathcal{F},\beta;P_{j}f^{\star})\). Furthermore, we always have that \(\mathfrak{E}(\mathcal{F},\beta;f^{\star})\lesssim\kappa\sum_{j=1}^{d} \mathfrak{E}(P_{j}\mathcal{F},\beta;P_{j}f^{\star})\), where \(\kappa\) hides \(\mathrm{poly}(d)\) factors. We next define the normed version of disagreement coefficient for vector-valued functions.

**Definition 7** (Scale sensitive disagreement coefficient (normed version), Foster et al. (2020)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\). For any \(f^{\star}\in\mathcal{F}\), and \(\beta_{0},\varepsilon_{0}>0\), the value function disagreement coefficient \(\theta^{\mathrm{val}}(\mathcal{F},\varepsilon_{0},\beta_{0};f^{\star})\) is defined as_

\[\sup_{\mu}\sup_{\beta>\beta_{0},\varepsilon>\varepsilon_{0}}\left\{\frac{ \varepsilon^{2}}{\beta^{2}}\cdot\Pr_{x\sim\mu}(\exists f\in\mathcal{F}\mid|f( x)-f^{\star}(x)|>\varepsilon,\|f-f^{\star}\|_{\mu}\leq\beta)\right\}\lor 1\]

_where \(\|f\|_{\mu}=\sqrt{\mathbb{E}_{x\sim\mu}[|f(x)\|^{2}]}\)._

We additionally also define the following bivariate version of eluder dimension for vector-valued functions.

**Definition 8** (Scale-sensitive eluder dimension (bivariate version)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\). Fix any \(f^{\star}\in\mathcal{F}\), and define \(\tilde{\mathfrak{E}}^{\prime}(\mathcal{F},\beta;f^{\star})\) to be the length of the longest sequence of contexts and actions \((x_{1},y_{1}),(x_{2},y_{2})\ldots(x_{m},y_{m})\) such that for all \(i\), there exists \(f_{i}\in\mathcal{F}\) such that_

\[|f_{i}(x_{i})[y_{i}]-f^{\star}(x_{i})[y_{i}]|>\beta,\quad\text{and}\quad\sum_{ j<i}(f_{i}(x_{j})[y_{j}]-f^{\star}(x_{j})[y_{j}])^{2}\leq\beta^{2}.\]

_We define the scale sensitive eluder dimension (mixed version) as \(\tilde{\mathfrak{E}}(\mathcal{F},\beta;f^{\star}):=\sup_{\beta\geq\beta_{0}} \tilde{\mathfrak{E}}^{\prime}(\mathcal{F},\beta_{0};f^{\star})\)._

We next define the strong variant of scale-sensitive star number.

**Definition 9** (scale-sensitive star number (strong version), Foster et al. (2020)).: _Let \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\). For any \(f^{\star}\in\mathcal{F}\) and \(\beta>0\), let \(\tilde{\mathfrak{E}}^{\mathrm{val}}(\mathcal{F},\beta)\) denote the length of the longest sequence of contexts \(\{x_{1},\ldots,x_{m}\}\) such that for all \(i\), there exists \(f_{i}\in\mathcal{F}\) such that_

\[\|f_{i}(x_{i})-f^{\star}(x_{i})\|>\beta,\quad\text{and}\quad\sum_{j=i}\|f_{i} (x_{j})-f^{\star}(x_{j})\|^{2}\leq\beta^{2}.\]

_We define the scale-sensitive star number as \(\tilde{\mathfrak{s}}^{\mathrm{val}}(\mathcal{F},\beta):=\sup_{\beta>\beta_{0}} \tilde{\mathfrak{E}}^{\mathrm{val}}(\mathcal{F},\beta_{0})\)._

The next result provides a relation between the star number, disagreement coefficient and the eluder dimension.

**Lemma 6** (Foster et al. (2020)).: _Suppose \(\mathcal{F}\subseteq\{\mathcal{X}\mapsto\mathbb{R}^{K}\}\) is a uniform Glivenko-Cantelli class. For any \(f^{\star}\in\mathcal{F}\) and \(\gamma,\varepsilon>0\), we have \(\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\beta;f^{\star})\leq\mathfrak{E}( \mathcal{F},\beta;f^{\star})\), \(\theta^{\mathrm{val}}(\mathcal{F},\varepsilon,\gamma;f^{\star})\leq 4( \mathfrak{s}^{\mathrm{val}}(\mathcal{F},\gamma;f^{\star}))^{2}\) and \(\theta^{\mathrm{val}}(\mathcal{F},\varepsilon,\gamma;f^{\star})\leq 4 \mathfrak{E}(\mathcal{F},\gamma;f^{\star})\)._

The following two technical lemmas are useful in bounding the total number of queries made by our selective sampling and imitation learning algorithms. We first provide a technical result which bounds the number of times we can find a function \(f^{\prime}\) in a refinement \(\mathcal{F}_{t}\) of \(\mathcal{F}\), such that \(f^{\prime}\) is sufficiently far away from \(f^{\star}\in\mathcal{F}\). This result is a variant of Russo and Van Roy (2013, Lemma 3), and first appears in Foster et al. (2020, Lemma E.4).

**Lemma 7**.: _Let \(\{x_{t},y_{t},Z_{t}\}_{t=1}^{T}\) be sequence of tuples, where \(x_{t}\in\mathcal{X}\) and \(Z_{t}\in\{0,1\}\). Fix any \(f^{\star}\in\mathcal{F}\), and define the set \(\mathcal{F}_{t}=\{f\in\mathcal{F}\mid\sum_{s=1}^{t-1}Z_{s}(f(x_{s})[y_{s}]-f^{ \star}(x_{s})[y_{s}])^{2}\leq\beta^{2}\}\). Then, for any \(\zeta>0\),_

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\exists f^Proof.: We first note that we can always remove a tuple \(\{(x_{t},y_{t}),Z_{t}\}\) whenever \(Z_{t}=0\) without any effect on the conclusion. Hence, we can assume \(Z_{t}=1\) for all \(t\in[T]\) without loss of generality. Then the rest of the proof essentialy follows from Foster et al. (2020, Lemma E.4). For completeness, we state the full proof here.

For simplicity of presentation, we say \((x_{t},y_{t})\) is \(\zeta\)-independent of \((x_{1},y_{1}),\ldots,(x_{t-1},y_{t-1})\) if there exists \(f\in\mathcal{F}\) such that \(|f(x_{t})[y_{t}]-f^{*}(x_{t})[y_{t}]|\geq\zeta\) and \(\sum_{s=1}^{t-1}(f(x_{s})[y_{s}]-f^{*}(x_{s})[y_{s}])^{2}\leq\zeta^{2}\). Otherwise, we say \(x\) is \(\zeta\)-dependent. The proof consists of the following two claims.

First, we claim that for any \(t\in[T]\), if there exists \(f\in\mathcal{F}_{t}\) such that \(|f(x_{t})[y_{t}]-f^{*}(x_{t})[y_{t}]|\geq\zeta\), then \(x_{t}\) is \(\zeta\)-dependent on at most \(\beta^{2}/\zeta^{2}\) disjoint sequences of \((x_{1},y_{1}),\ldots,(x_{t-1},y_{t-1})\). To show this, let's say \(x_{t}\) is \(\zeta\)-dependent on a particular subsequence \((x_{i_{1}},y_{i_{1}}),\ldots,(x_{i_{k}},y_{i_{k}})\) while \(|f(x)[y]-f^{*}(x)[y]|\geq\zeta\). Then it must holds that

\[\sum_{j=1}^{k}\left(f(x_{i_{j}})[y_{i_{j}}]-f^{*}(x_{i_{j}})[y_{i_{j}}]\right) ^{2}\geq\zeta^{2}.\]

If there are \(M\) such disjoint subsequence, then we can add them up and obtain the following:

\[\sum_{s=1}^{t-1}\left(f(x_{s})[y_{s}]-f^{*}(x_{s})[y_{s}]\right)^{2}\geq M \zeta^{2}.\]

By the construction of \(\mathcal{F}_{t}\), the left-hand side above is at most \(\beta^{2}\). Hence we conclude that \(\beta^{2}\geq M\zeta^{2}\), which implies \(M\leq\beta^{2}/\zeta^{2}\).

Second, we claim that for any \(k\) and any sequence \((x_{1},y_{1}),\ldots,(x_{k},y_{k})\), there exists \(j\leq k\) such that \(x_{j}\) is \(\zeta\)-dependent on at least \(N:=\lfloor k\,/\,\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\rfloor\) disjoint subsequences of \((x_{1},y_{1}),\ldots,(x_{j-1},y_{j-1})\). This can be proved by construction. Let \(B_{1},\ldots,B_{N}\) be \(N\) subsequences of \((x_{1},y_{1}),\ldots,(x_{k},y_{k})\) and are initialized with \(B_{i}=\{(x_{i},y_{i})\}\). Then we repeat the following process for \(j=N+1,N+2\ldots,k\).

* We first check if \(x_{j}\) is \(\zeta\)-dependent on \(B_{i}\) for all \(i\in[N]\). If so, we are done.
* Otherwise, pick an arbitrary \(i\in[N]\) for which \(x_{j}\) is \(\zeta\)-independent of \(B_{i}\) and append \((x_{j},y_{j})\) to \(B_{i}\), i.e., \(B_{i}\gets B_{i}\cup\{(x_{j},y_{j})\}\).

If we don't reach any \(j\) while running the above process for which the first statement above is satisfied, we should end up with \(\sum_{i=1}^{N}|B_{i}|=k\geq N\cdot\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\). We note that by construction \(|B_{i}|\leq\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\) and thus \(|B_{i}|=\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\) for all \(i\in[N]\), which implies \(x_{k}\) must be \(\zeta\)-dependent on all \(B_{i}\).

Finally, let \(x_{i_{1}},\ldots,x_{i_{k}}\) be the subsequence where, for all \(s\in[k]\), there exists \(f\in\mathcal{F}_{i_{s}}\) such that \(|f(x_{i_{s}})[y_{i_{s}}]-f^{*}(x_{i_{s}})[y_{i_{j}}]|\geq\zeta\). By our first claim we know each element of this subsequence is \(\zeta\)-dependent on at most \(\beta^{2}/\zeta^{2}\) disjoint subsequences. By the second claim, we know that there exists an element that is \(\zeta\)-dependent on at least \(\lfloor k\,/\,\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\rfloor\) disjoint subsequences. So we must have \(\lfloor k\,/\,\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\rfloor\leq\beta^ {2}/\zeta^{2}\). Hence, \(k\leq(\beta^{2}/\zeta^{2}+1)\cdot\tilde{\mathfrak{E}}(\mathcal{F},\zeta;f^{*})\). 

The following is an extension of Lemma 7 that holds for the normed version of eluder dimension given in Definition 1. The proof is essentially the same so we skip it for conciseness.

**Lemma 8**.: _Let \(\{x_{t},Z_{t}\}_{t=1}^{T}\) be sequence of tuples, where \(x_{t}\in\mathcal{X}\) and \(Z_{t}\in\{0,1\}\). Fix any \(f^{*}\in\mathcal{F}\), and define the set \(\mathcal{F}_{t}=\{f\in\mathcal{F}\mid\sum_{s=1}^{t-1}Z_{s}\|f(x_{s})-f^{*}(x_{s} )\|^{2}\leq\beta^{2}\}\). Then, for any \(\zeta>0\),_

\[\sum_{t=1}^{T}Z_{t}\mathbbm{1}\{\exists f^{\prime}\in\mathcal{F}_{t}:|f^{\prime }(x_{t})-f^{*}(x_{t})\|\geq\zeta\}\leq\left(\frac{\beta^{2}}{\zeta^{2}}+1 \right)\mathfrak{E}(\mathcal{F},\zeta;f^{*}).\]Slective Sampling: Learning from Single Expert

### Comparison to Related Works

Selective Sampling.There is a large bank of both theoretical and empirical work for active learning and selective sampling. Perhaps the work closest to ours is the work of Zhu and Nowak (2022). In this paper, the authors consider binary classification problem and provide bounds on number of queries and bound on excess risk in the active learning framework. Their algorithm also relies on regression oracle. However, there are many key differences: Firstly, their guarantees for regret for selective sampling problem (see for instance Theorem 10 on page 28 of Zhu and Nowak (2022)) has a dependence on disagreement coefficient in the regret bound as well as number of queries. On the contrary, as we show in our work, one only needs to pay for eluder dimension or disagreement coefficient in query complexity and not in regret bound. Furthermore, we supplement our result with lower bound showing that unless one has label complexity that depends on star number (and hence can be also related to worst case disagreement coefficient), one can not get a small enough regret bound. So the separation between regret bound (that is independent of eluder dimension/star number/disagreement coefficient) and query complexity (that depends on those quantities) is real. Secondly, the results in Zhu and Nowak (2022) dont automatically adapt to the margin region and in general there is no way to estimate the parameters of Tsybakov's noise condition. Finally, their regret bounds depend on pseudo dimension and are thus generally suboptimal for complex \(\mathcal{F}\).

### Proof Sketch for Selective Sampling and Binary Labels

Let \(\mathcal{A}=\{1,2\}\), and the link function \(\phi(z)=z\) corresponding to square-loss \(\ell_{\phi}=(v-y)^{2}/2\); here \(\lambda=\gamma=1\).

Let \(\bar{\mathcal{F}}\subseteq\{\mathcal{X}\mapsto[-1,1]\}\) be a function class, and \(\bar{f}^{*}\in\mathcal{F}\). We assume that for any context \(x\), the label \(y\) is drawn according to the distribution \(\Pr(y_{t}=2)=1+\bar{f}^{*}(x)/2\). Using \(\bar{\mathcal{F}}\), we can define the score function class \(\mathcal{F}=\{f_{\bar{f}}\mid\bar{f}\in\bar{\mathcal{F}}\}\) where \(f(x)=\frac{1}{2}(1-\bar{f}(x),1+\bar{f}(x))^{\top}\in[0,1]^{2}\), and additionally define \(f^{*}=f_{\bar{f}^{*}}\). Clearly, the Bayes optimal predictor that chooses the action with the largest score is given by \(\texttt{SelectAction}(f^{*}(x))=1+\text{sign}(\bar{f}^{*}(x))\). Furthermore, \(\texttt{Margin}(f^{*}(x)):=|\Pr(y=2\mid x)-\Pr(y=1\mid x)|=|\bar{f}^{*}(x)|\) which implies that \(T_{\varepsilon}=\sum_{t=1}^{T}\mathbf{1}\{|\bar{f}^{*}(x_{t})|\leq\varepsilon\}\). Finally, the oracle in (3) reduces to a square-loss online regression oracle, which implies that with probability at least \(1-\delta\), for all \(t\leq T\),

\[\sum_{s=1}^{t}Z_{s}(\bar{f}_{s}(x_{s})-\bar{f}^{*}(x_{s}))^{2} \lesssim\sum_{s=1}^{t}Z_{s}(\bar{f}_{s}(x_{s})-y_{s})^{2}-\sum_{s=1}^{t}Z_{s}( \bar{f}^{*}(x_{s})-y_{s})^{2}\lesssim\mathrm{Reg}^{\mathrm{sq}}(\bar{ \mathcal{F}};T)+\log(\nicefrac{{T}}{{\delta}}),\] (10)

The above implies that \(\bar{f}^{*}\) satisfies the constraints in (5) with the right choice of constants, \(\lambda\), and \(\gamma\), and thus \(|\bar{f}_{t}(x_{t})-\bar{f}^{*}(x_{t})|\leq\Delta_{t}(x_{t})\) (see Lemma 10 for proof). However, since the query condition in Algorithm 1 is \(Z_{t}=1\{|\bar{f}_{t}(x_{t})|\leq\Delta_{t}(x_{t})\}\), we have that if \(Z_{t}=0\), then \(|\bar{f}_{t}(x_{t})|>\Delta_{t}(x_{t})\) which implies that \(\text{sign}(\bar{f}^{*}(x_{t}))=\text{sign}(\bar{f}_{t}(x_{t}))\). Thus,

\[\sum_{s=1}^{t}\bar{Z}_{s}\mathbf{1}\{\text{sign}(\bar{f}^{*}(x_{t}))\neq\text{ sign}(\bar{f}_{t}(x_{t}))\}=0.\] (11)

_Regret bound._ Using the fact that \(y_{t}\sim 1+\mathrm{Ber}(\nicefrac{{1+\bar{f}^{*}(x_{t})}}{{2}})\), \(\widehat{y}_{t}=\texttt{SelectAction}(f_{t}(x_{t}))=1+\text{sign}(\bar{f}_{t}(x _{t}))\), we have

\[\mathrm{Reg}_{T} =\sum_{t=1}^{T}\Pr(\widehat{y}_{t}\neq y_{t})-\Pr(\texttt{SelectAction }(f^{*}(x_{t}))\neq y_{t})\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\text{sign}(\bar{f}_{t}(x_{t}))\neq \text{sign}(\bar{f}^{*}(x_{t}))\}\cdot|2\Pr(y_{t}=1)-1|\] \[=\sum_{t=1}^{T}\mathbf{1}\{\text{sign}(\bar{f}_{t}(x_{t}))\neq \text{sign}(\bar{f}^{*}(x_{t}))\}\cdot|\bar{f}^{*}(x_{t})|\]

The right hand side above can be split and upper bound via the following three terms:

\[\mathrm{Reg}_{T}\leq\varepsilon\sum_{t=1}^{T}\mathbf{1}\{|\bar{f}^{*}(x_{t})| \leq\varepsilon\}+\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\text{sign}(\bar{f}_{t}(x_{t }))\neq\text{sign}(\bar{f}^{*}(x_{t})),|\bar{f}^{*}(x_{t})|>\varepsilon\} \cdot|\bar{f}^{*}(x_{t})|\]\[+\sum_{t=1}^{T}\bar{Z}_{t}\mathbf{1}\{\mathrm{sign}(\bar{f}_{t}(x_{t})) \neq\mathrm{sign}(\bar{f}^{*}(x_{t}))\}\cdot|\bar{f}^{*}(x_{t})|.\] \[=\varepsilon T_{\varepsilon}+\underbrace{\sum_{t=1}^{T}Z_{t} \mathbf{1}\{\mathrm{sign}(\bar{f}_{t}(x_{t}))\neq\mathrm{sign}(\bar{f}^{*}(x_{ t})),|\bar{f}^{*}(x_{t})|>\varepsilon\}\cdot|\bar{f}^{*}(x_{t})|}_{:=\tau_{A}},\]

where the first term is \(T_{\varepsilon}\), and the last term is zero due to (11). The term \(\mathrm{T}_{A}\) denotes the regret for the rounds in which the learner queries for the label, and the margin for \(\bar{f}^{*}(x_{t})\) is larger than \(\varepsilon\). We note that

\[\mathrm{T}_{A}\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{|\bar{f}^{*}(x_{t})-\bar{f}_{ t}(x_{t})|>\varepsilon\}\cdot|\bar{f}^{*}(x_{t})-\bar{f}_{t}(x_{t})|\]

where the inequality holds because \(|\bar{f}^{*}(x_{t})-\bar{f}_{t}(x_{t})|\geq|\bar{f}^{*}(x_{t})|\) since they have opposite signs. Using the fact that \(\mathbf{1}\{a\geq b\}\leq a/b\) for all \(a,b\geq 0\), and the bound in (10), we get

\[\mathrm{T}_{A}\leq\frac{1}{\varepsilon}\sum_{t=1}^{T}Z_{t}(\bar{f}^{*}(x_{t}) -\bar{f}_{t}(x_{t}))^{2}\lesssim\frac{1}{\varepsilon}\mathrm{Reg}^{\mathrm{sq }}(\mathcal{F};T)+\frac{1}{\varepsilon}\log(\nicefrac{{1}}{{\delta}}),\]

Gathering all the terms, we get

\[\mathrm{Reg}_{T}=\widetilde{\mathcal{O}}\bigg{(}\varepsilon T_{\varepsilon}+ \frac{1}{\varepsilon}\mathrm{Reg}^{\mathrm{sq}}(\mathcal{F};T)+\frac{1}{ \varepsilon}\log(\nicefrac{{1}}{{\delta}})\bigg{)}.\]

_Query complexity._ Plugging in the query rule, and splitting as in the regret bound, we get

\[N_{T} =\sum_{t=1}^{T}Z_{t}=\sum_{t=1}^{T}\mathbf{1}\{|\bar{f}_{t}(x_{t} )|\leq\Delta_{t}(x_{t})\}\] \[\leq\underbrace{\sum_{t=1}^{T}\mathbf{1}\{|\bar{f}^{*}(x_{t})| \leq\varepsilon\}}_{=T_{\varepsilon}}+\underbrace{\sum_{t=1}^{T}\mathbf{1}\{| \bar{f}_{t}(x_{t})|\leq\Delta_{t}(x_{t}),|\bar{f}^{*}(x_{t})|>\varepsilon, \Delta_{t}(x_{t})\leq\nicefrac{{\varepsilon}}{{\delta}}\}}_{:=\tau_{C}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\underbrace{\sum_{t=1}^{T} \mathbf{1}\{|\bar{f}_{t}(x_{t})|\leq\Delta_{t}(x_{t}),|\bar{f}^{*}(x_{t})|> \varepsilon,\Delta_{t}(x_{t})>\nicefrac{{\varepsilon}}{{\delta}}\}}_{:=\tau_{D}}\]

\(\mathrm{T}_{C}\) denotes the rounds in which we make a query, \(\Delta_{t}(x_{t})\leq\nicefrac{{\varepsilon}}{{3}}\), and the margin for \(\bar{f}^{*}(x_{t})\) is larger than \(\varepsilon\). Since \(|\bar{f}_{t}(x_{t})-\bar{f}^{*}(x_{t})|\leq\Delta_{t}(x_{t})\) (as shown above), we have

\[|\bar{f}^{*}(x_{t})|\leq|\bar{f}_{t}(x_{t})-\bar{f}^{*}(x_{t})|+|\bar{f}_{t}(x _{t})|\leq\Delta_{t}(x_{t})+|\bar{f}_{t}(x_{t})|.\]

Thus,

\[\mathrm{T}_{C}\leq\sum_{t=1}^{T}\mathbf{1}\{|\bar{f}^{*}(x_{t})|\leq 2 \Delta_{t}(x_{t}),|\bar{f}^{*}(x_{t})|>\varepsilon,\Delta_{t}(x_{t})\leq \nicefrac{{\varepsilon}}{{3}}\}=0.\]

\(\mathrm{T}_{D}\) is bounded by the number of rounds for which we make a query and \(\Delta_{t}(x_{t})\geq\nicefrac{{\varepsilon}}{{3}}\). Using the properties of eluder dimension, we get that

\[\mathrm{T}_{D}\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq \nicefrac{{\varepsilon}}{{3}}\}\lesssim\frac{1}{\varepsilon^{2}}\mathrm{Reg}^{ \mathrm{sq}}(\mathcal{F};T)\cdot\mathfrak{E}(\mathcal{F},\nicefrac{{\varepsilon }}{{6}};\bar{f}^{*})+\log(\nicefrac{{1}}{{\delta}}).\]

Gathering all the terms, we conclude

\[N_{T}=\widetilde{\mathcal{O}}\bigg{(}T_{\varepsilon}+\frac{1}{ \varepsilon^{2}}\mathrm{Reg}^{\mathrm{sq}}(\mathcal{F};T)\cdot\mathfrak{E}( \mathcal{F},\nicefrac{{\varepsilon}}{{6}};\bar{f}^{*})+\frac{1}{\varepsilon^{2 }}\log(\nicefrac{{1}}{{\delta}})\bigg{)}.\]

In Appendix E.3, we provide the complete proof and show how to generalize it for multiple actions, link function \(\phi\) and corresponding regression oracles w.r.t. \(\ell_{\phi}\).

### Proof of Theorem 1

Before delving into the proof, we recall the relevant notation. In Algorithm 1,

* The label \(y_{t}\sim\phi(f^{\star}(x_{t}))\), where \(\phi\) denotes the link-function given in (2).
* The function \(\texttt{SelectAction}(f_{t}(x_{t})):=\operatorname*{argmax}_{k}\phi(f_{t}(x_{t}) )[k]\).
* For any vector \(v\in\mathbb{R}^{K}\), the margin is given by the gap between the value at the largest and the second largest coordinate, i.e. \[\texttt{Margin}(v)=\phi(v)[k^{\star}]-\max_{k\neq k^{\star}}\phi(v)[k],\] where \(k^{\star}\in\operatorname*{argmax}_{k\in[K]}\phi(v)[k]\).
* We also define \(T_{\varepsilon}=\sum_{t=1}^{T}\mathbf{1}\{\texttt{Margin}(f^{\star}(x_{t})) \leq\varepsilon\}\) to denote the number of samples within \(T\) rounds of interaction for which the margin w.r.t. \(f^{\star}\) is smaller than \(\varepsilon\).
* We define the function \(\operatorname*{Gap}:\mathbb{R}^{K}\times[K]\mapsto\mathbb{R}^{+}\) as \[\operatorname*{Gap}(v,k)=\max_{k^{\prime}}\phi(v)[k^{\prime}]-\phi(v)[k],\] (12) to denote the gap between the largest and the \(k\)-th coordinate of \(v\).

#### e.3.1 Supporting Technical Results

**Lemma 9**.: _For any \(u\), and \(k^{\prime}\neq\operatorname*{argmax}_{k}\phi(u)[k]\),_

\[\texttt{Margin}(u)\leq\operatorname*{Gap}(u,k^{\prime}).\]

Proof.: Let \(k^{\star}=\operatorname*{argmax}_{k}\phi(u)[k]\). By definition,

\[\operatorname*{Gap}(u,k^{\prime}) =\phi(u)[k^{\star}]-\phi(u)[k^{\prime}]\] \[\geq\phi(u)[k^{\star}]-\max_{k^{\prime}\neq k}\phi(u)[k^{\prime}] =\texttt{Margin}(u).\]

The following technical result establishes a certain favorable property for the function \(f^{\star}\), whose proof follows from the regret bound of the online oracle used in Algorithm 1.

**Lemma 10**.: _With probability at least \(1-\delta\), the function \(f^{\star}\in\mathcal{F}\) satisfies the following for all \(t\leq T\):_

\[\sum_{s=1}^{t}Z_{s}\|f^{\star}(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{\delta}^{ \ell_{o}}(\mathcal{F},T),\]

_where \(\Psi_{\delta}^{\ell_{o}}(\mathcal{F},T):=\frac{4}{\lambda}\mathrm{Reg}^{\ell_ {o}}(\mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta)\)._

Proof.: The desired result follows from an application of Lemma 5, where we note that we do not query oracle when \(Z_{s}=0\), and thus do not count the time steps for which \(Z_{s}=0\). 

Throughout the proof, we condition on the \(1-\delta\) probability event that Lemma 10 holds. The next technical lemma allows us to bound the number of times when we query for the label and \(\Delta_{t}(x_{t})\geq\zeta\) in terms of the eluder dimension (normed version) of the function class \(\mathcal{F}\). Note that Lemma 11 holds even if the sequence \(\{x_{t}\}_{t\leq T}\) could be adversarially generated.

**Lemma 11**.: _Let \(f^{\star}\) satisfy Lemma 10, and let \(\Delta_{t}(x_{t})\) be defined in (5) in Algorithm 1. Then, for any \(\zeta>0\), with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\zeta\}\leq\widehat{O} \Bigg{(}\frac{\Psi_{\delta}^{\ell_{o}}(\mathcal{F},T)}{\zeta^{2}}\cdot \mathfrak{E}(\mathcal{F},\zeta/\!z;f^{\star})\Bigg{)}.\]

_where \(\mathfrak{E}\) denotes the eluder dimension is given in Definition 1._Proof.: Let \(f_{t}^{*}\) denote the maximizer of (5) at round \(t\) on point \(x_{t}\). Thus,

\[\Delta_{t}(x_{t})=\|f_{t}^{*}(x_{t})-f_{t}(x_{t})\|,\qquad\text{and} \qquad\sum_{s=1}^{t-1}Z_{s}\|f_{t}^{*}(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{ \delta}^{\ell_{\phi}}(\mathcal{F},T).\] (13)

However, recall that Lemma 10 implies that, with probability at least \(1-\delta\), the function \(f^{*}\) satisfies the bound

\[\sum_{s=1}^{t-1}Z_{s}\|f^{*}(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{ \delta}^{\ell_{\phi}}(\mathcal{F},T).\] (14)

Using (13), (14) and Triangle inequality, we get that

\[\sum_{s=1}^{t-1}Z_{s}\|f_{t}^{*}(x_{s})-f^{*}(x_{s})\|^{2} \leq 2\sum_{s=1}^{t-1}Z_{s}\|f_{t}^{*}(x_{t})-f_{s}(x_{s})\|^{2}+2 \sum_{s=1}^{t-1}Z_{s}\|f^{*}(x_{t})-f_{s}(x_{s})\|^{2}\] \[\leq 4\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T).\] (15)

Next, note that, an application of Triangle inequality implies that \(\|f_{t}^{*}(x_{t})-f_{t}(x_{t})\|\leq\|f_{t}^{*}(x_{t})-f^{*}(x_{t})\|+\|f^{*} (x_{t})-f_{t}(x_{t})\|\). Thus,

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\zeta\} =\sum_{t=1}^{T}Z_{t}\mathbf{1}\{|f_{t}^{*}(x_{t})-f_{t}(x_{t})\| \geq\zeta\}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{|f_{t}^{*}(x_{t})-f^{*}(x_{t}) |+\|f^{*}(x_{t})-f_{t}(x_{t})\|\geq\zeta\}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\|f_{t}^{*}(x_{t})-f^{*} (x_{t})\|\geq\frac{\zeta}{2}\bigg{\}}+\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\| f_{t}(x_{t})-f^{*}(x_{t})\|\geq\frac{\zeta}{2}\bigg{\}}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\|f_{t}^{*}(x_{t})-f^{*} (x_{t})\|\geq\frac{\zeta}{2}\bigg{\}}+\frac{4\zeta^{2}}{\zeta^{2}}\sum_{t=1}^ {T}Z_{t}(f_{t}(x_{t})-f^{*}(x_{t}))^{2}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\|f_{t}^{*}(x_{t})-f^{*} (x_{t})\|\geq\frac{\zeta}{2}\bigg{\}}+\frac{4\Psi_{\delta}^{\ell_{\phi}}( \mathcal{F},T)}{\zeta^{2}},\] (16)

where in the last line we used Lemma 10 to bound the second term. In the following, we show how to bound the first term. Recall that for any \(t\leq T\), the function \(f_{t}^{*}\) satisfies (15). Thus, we wish to bound

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\|f_{t}^{*}(x_{t})-f^{*}(x_ {t})\|\geq\frac{\zeta}{2}\bigg{\}}\quad\text{s.t.}\quad\sum_{s=1}^{t-1}Z_{s}(f _{t}^{*}(x_{s})-f^{*}(x_{s}))^{2}\leq 4\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T),\]

for all \(t\leq T\). An application of Lemma 8 in the above implies that

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\bigg{\{}\|f_{t}^{*}(x_{t})-f^{*}(x_ {t})\|\geq\frac{\zeta}{2}\bigg{\}}\leq\frac{17\Psi_{\delta}^{\ell_{\phi}}( \mathcal{F},T)}{\zeta^{2}}\cdot\mathfrak{E}(\mathcal{F},\zeta\!/_{2};f^{*}).\] (17)

where in the last line, we used the fact that \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)\!/\zeta^{2}\geq 1\), for our parameter setting.

Plugging in the bound (17) in (16), and using the fact that \(\mathfrak{E}(\mathcal{F},\zeta\!/_{2};f^{*})\geq 1\), we get that

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\zeta\}\leq \frac{20\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)}{\zeta^{2}}\cdot\mathfrak{E }(\mathcal{F},\zeta\!/_{2};f^{*}).\]

The next two technical lemma's relate the margin to the gap between functions, and are useful in the analysis for regret / total number of queries.

**Lemma 12**.: _Suppose the functions \(\pi_{1}\) and \(\pi_{2}\) are defined such that \(\pi_{i}(x)=\operatorname*{argmax}_{k\in[K]}\phi(f_{i}(x))[k]\). Then, for any \(x\) for which \(\pi_{1}(x)\neq\pi_{2}(x)\), we have_

\[\mathtt{Margin}(f_{1}(x))\leq\phi(f_{1}(x))[\pi_{1}(x)]-\phi(f_{1}(x))[\pi_{2} (x)]\leq 2\gamma|f_{1}(x)-f_{2}(x)\|_{2},\]

_where \(\gamma\)-denotes the Lipschitz parameter of the link function \(\phi\)._Proof.: First note \(\phi(f_{2}(x))[\pi_{2}(x)]\geq\phi(f_{2}(x))[\pi_{1}(x)]\) by the definition of \(\pi_{2}\). Thus,

\[\phi(f_{1}(x))[\pi_{1}(x)]-\phi(f_{1}(x))[\pi_{2}(x)]\] \[\leq\phi(f_{1}(x))[\pi_{1}(x)]-\phi(f_{2}(x))[\pi_{1}(x)]+\phi(f_{ 2}(x))[\pi_{2}(x)]-\phi(f_{1}(x))[\pi_{2}(x)]\] \[\leq 2|\phi(f_{1}(x))-\phi(f_{2}(x))\|_{\infty}\] \[\leq 2|\phi(f_{1}(x))-\phi(f_{2}(x))\|_{2}.\]

Using the fact that \(\phi\) is \(\gamma\)-Lipschitz, we immediately get that

\[\phi(f_{1}(x))[\pi_{1}(x)]-\phi(f_{1}(x))[\pi_{2}(x)]\leq 2\gamma\|f_{1}(x)-f_{ 2}(x)|_{2}.\]

**Lemma 13**.: _For any two function \(f_{1},f_{2}\in\mathcal{F}\), and \(x\in\mathcal{X}\),_

\[\mathtt{Margin}(f_{1}(x))-\mathtt{Margin}(f_{2}(x))\leq 2\gamma\|f_{1}(x)-f_{ 2}(x)\|.\]

Proof.: For the ease of notation, define

\[k_{1}=\operatorname*{argmax}_{k\in\{k\}}\phi(f_{1}(x))[k]\qquad\text{and} \qquad k_{1}^{\prime}=\operatorname*{argmax}_{k^{\prime}\neq k_{1}}\phi(f_{1} (x))[k^{\prime}],\]

where ties are broken arbitrarily but consistently. Similarly, we define

\[k_{2}=\operatorname*{argmax}_{k\in\{k\}}\phi(f_{2}(x))[k]\qquad\text{and} \qquad k_{2}^{\prime}=\operatorname*{argmax}_{k^{\prime}\neq k_{2}}\phi(f_{2} (x))[k^{\prime}].\] (18)

Thus, we have that

\[\mathtt{Margin}(f_{1}(x))=\phi(f_{1}(x))[k_{1}]-\phi(f_{1}(x))[k_{1}^{\prime}],\]

and

\[\mathtt{Margin}(f_{2}(x))=\phi(f_{2}(x))[k_{2}]-\phi(f_{2}(x))[k_{2}^{\prime}].\] (19)

Finally, also note that for any coordinate \(k\),

\[\phi(f_{1}(x))[k]-\phi(f_{2}(x))[k]\leq\|\phi(f_{2}(x))-\phi(f_{1}(x))\|.\] (20)

We now proceed with the proof. Plugging in the form in (19), we get that

\[\mathtt{Margin}(f_{1}(x))-\mathtt{Margin}(f_{2}(x))\] \[\qquad=\phi(f_{1}(x))[k_{1}]-\phi(f_{1}(x))[k_{1}^{\prime}]-(\phi (f_{2}(x))[k_{2}]-\phi(f_{2}(x))[k_{2}^{\prime}])\] \[\qquad=(\phi(f_{1}(x))[k_{1}]-\phi(f_{2}(x))[k_{2}])+(\phi(f_{2}( x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}])\] \[\qquad\leq(\phi(f_{1}(x))[k_{1}]-\phi(f_{2}(x))[k_{1}])+(\phi(f_{2 }(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}])\] \[\qquad\leq\|\phi(f_{2}(x))-\phi(f_{1}(x))\|+(\phi(f_{2}(x))[k_{2}^ {\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}]),\]

where the first inequality uses the fact that \(k_{2}\) is the maximizer coordinate of \(\phi(f_{2}(x))\) and the last inequality uses (20). In the following, we bound the second term in the right hand side above under the following three cases:

* _Case 1:_\(k_{2}^{\prime}\neq k_{1}\)_: Since \(k_{2}^{\prime}+k_{1}\), we note that replacing \(k_{1}^{\prime}\) by \(k_{2}^{\prime}\) in the second term will only increase the value (see the definition in (18)). Thus, \[\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}] \leq\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{2}^{\prime}]\] \[\leq\|\phi(f_{2}(x))-\phi(f_{1}(x))\|,\] where the last line uses (20).
* _Case 2a:_\(k_{2}^{\prime}=k_{1},k_{2}=k_{1}^{\prime}\)_: Using definition of \(k_{2}\) in (18), we note that \[\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}] =\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{2}]\] \[\leq\phi(f_{2}(x))[k_{2}]-\phi(f_{1}(x))[k_{2}]\] \[\leq\|\phi(f_{2}(x))-\phi(f_{1}(x))\|,\] where the last line uses (20).

* _Case 2b:_\(k_{2}^{\prime}=k_{1},k_{2}\neq k_{1}^{\prime}\): Using the fact that \(k_{2}^{\prime}=k_{1}\) and that \(k_{2}\neq k_{2}^{\prime}\), we get that \(k_{2}\neq k_{1}\). Thus using the definition of \(k_{1}^{\prime}\) along with the fact that \(k_{2}\neq k_{1}\), we get that \[\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{1}^{\prime}] \leq\phi(f_{2}(x))[k_{2}^{\prime}]-\phi(f_{1}(x))[k_{2}]\] \[\leq\phi(f_{2}(x))[k_{2}]-\phi(f_{1}(x))[k_{2}]\] \[\leq\|\phi(f_{2}(x))-\phi(f_{1}(x))\|,\] where the second last line uses definition of \(k_{2}\) and the last line uses (20).

Combining all the above bounds together implies that

\[\mathtt{Margin}(f_{1}(x))-\mathtt{Margin}(f_{2}(x))\leq 2\|\phi(f_{2}(x))- \phi(f_{1}(x))\|.\]

The final statement follows since \(\phi\) is \(\gamma\)-Lipschitz. 

#### e.3.2 Regret Bound

For the ease of notation, for the rest of the proof in this section we define the function \(\pi^{\star}\) such that

\[\pi^{\star}(x)=\operatorname*{argmax}_{k}\phi(f^{\star}(x))[k].\]

Additionally, we recall that for any time \(t\), \(\widehat{y}_{t}=\mathtt{SelectAction}(f_{t}(x_{t}))=\operatorname*{argmax}_{k} \phi(f_{t}(x))[k]\). Starting from the definition of the regret, we have

\[\operatorname{Reg}_{T} =\sum_{t=1}^{T}\Pr(\widehat{y}_{t}\neq y_{t})-\Pr(\pi^{\star}(x_ {t})\neq y_{t})\] \[=\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_{t}) \}\cdot|\Pr(y_{t}=\pi^{\star}(x_{t}))-\Pr(y_{t}=\widehat{y}_{t})|\] \[=\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_{t}) \}\cdot|\phi(f^{\star}(x_{t}))[\pi^{\star}(x_{t})]-\phi(f^{\star}(x_{t}))[ \widehat{y}_{t}]|\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_ {t})\}\cdot\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t}),\]

where the second last line uses the probabilistic model from which labels are generated, and the last inequality plugs in the definition of \(\operatorname{Gap}\) from (45). Let \(\varepsilon>0\) be a free parameter. We can decompose the above regret bound further as:

\[\operatorname{Reg}_{T}\leq\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_ {t}\neq\pi^{\star}(x_{t}),\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t} )\leq\varepsilon\}\cdot\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})\] \[\qquad\qquad\qquad\qquad+\sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t }\neq\pi^{\star}(x_{t}),\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})> \varepsilon\}\cdot\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})\]

Using the fact that \(\mathsf{y}_{t}(x_{t})=\operatorname*{argmax}_{k\in[K]}\phi(f_{t}(x))[k]\) along with the definition of \(\operatorname{Gap}\) and Lemma 12, we get that

\[\operatorname{Reg}_{T}\leq \sum_{t=1}^{T}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_{t}), \operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})\leq\varepsilon\}\cdot\varepsilon\]\[+2\gamma\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_{t}), \operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})>\varepsilon\}\cdot|f^{ \star}(x_{t})-f_{t}(x_{t})\|\]

\[+2\gamma\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_{t}) \}\cdot|f^{\star}(x_{t})-f_{t}(x_{t})\|\] (21)

\[=T_{\varepsilon}\cdot\varepsilon+2\gamma\cdot T_{A}+2\gamma\cdot T_{B}\cdot \|f^{\star}(x_{t})-f_{t}(x_{t})\|,\]

where the second inequality holds because \(\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})\leq\varepsilon\) implies that \(\mathtt{Margin}(f^{\star}(x_{t}))\leq\varepsilon\) whenever \(\widehat{y}_{t}\neq\pi^{\star}(x_{t})\). In the last line above, we plugged in the definition of \(T_{\varepsilon}\), and defined \(\mathtt{T}_{A}\) and \(\mathtt{T}_{B}\) as the second term and the last term respectively (upto constants). We bound them separately below:

* _Bound on \(\mathtt{T}_{A}\):_ We note that \[\mathtt{T}_{A} =\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\widehat{y}_{t}\neq\pi^{\star}(x_ {t}),\operatorname{Gap}(f^{\star}(x_{t}),\widehat{y}_{t})>\varepsilon\}\cdot \|f^{\star}(x_{t})-f_{t}(x_{t})\|\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\|f^{\star}(x_{t})-f_{t}(x_{t} )\|>\varepsilon/2\gamma\}\cdot\|f^{\star}(x_{t})-f_{t}(x_{t})\|\] where the second line follows from Lemma 12 and because \(\widehat{y}_{t}\neq\pi^{\star}(x_{t})\). Using the fact that \(\mathbf{1}\{a\geq b\}\leq\nicefrac{{a}}{{b}}\) for all \(a,b\geq 0\), we get that \[\mathtt{T}_{A}\leq 4\gamma\sum_{t=1}^{T}Z_{t}\frac{\|f^{\star}(x_{t})-f_{t}(x_ {t})\|^{2}}{\varepsilon}.\] (22)
* _Bound on \(\mathtt{T}_{B}\):_ Fix any \(t\leq T\), and note that Lemma 10 implies that \(\sum_{s=1}^{t}\|f^{\star}(x_{t})-f_{t}(x_{t})\|^{2}\leq\Psi_{\delta}^{\ell_{ \phi}}(\mathcal{F},T)\). Thus \(f^{\star}\) satisfies the constraint in the definition of \(\Delta_{t}\) in (5) and we must have that \[\|f^{\star}(x_{t})-f_{t}(x_{t})\|\leq\Delta_{t}(x_{t}).\] (23) Plugging in the definition of \(Z_{t}\), we note that \[\mathtt{T}_{B} =\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))>2\gamma \Delta_{t}(x_{t}),\widehat{y}_{t}\neq\pi^{\star}(x_{t})\}\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\|f_{t}(x_{t})-f^{\star}(x_{t})|> \Delta_{t}(x_{t})\},\] where the second inequality is due Lemma 12. However, note that the term inside the indicator contradicts (23) (which always holds). Thus, \[\mathtt{T}_{B}=0.\] (24)

Combining the bounds (22) and (24), we get that

\[\operatorname{Reg}_{T} \leq\varepsilon T_{\varepsilon}+8\gamma^{2}\sum_{t=1}^{T}Z_{t} \frac{\|f_{t}(x_{t})-f^{\star}(x_{t})\|^{2}}{\varepsilon}\] \[\leq\varepsilon T_{\varepsilon}+\frac{8\gamma^{2}}{\varepsilon} \Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T),\]

where the last inequality is due to Lemma 10.

Since \(\varepsilon\) is a free parameter above, the final bound follows by choosing the best parameter \(\varepsilon\), and by plugging in the form of \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)\).

#### e.3.3 Total Number of Queries

We use the notation \(N_{T}\) to denote the total number of expert queries made by the learner within \(T\) rounds of interactions. Let \(\varepsilon>0\) be a free parameter. Using the definition of \(Z_{t}\), we have that

\[N_{T}=\sum_{t=1}^{T}Z_{t}\]\[=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t})\}\] \[=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))\leq\varepsilon\}\] \[\qquad\qquad\qquad+\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t }(x_{t}))\leq 2\gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))>\varepsilon\}\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{*}(x_{t}))\leq \varepsilon\}\] \[\qquad\qquad\qquad+\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t }(x_{t}))\leq 2\gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))> \varepsilon,\Delta_{t}(x_{t})>\varepsilon/4\gamma\}\] (25) \[=T_{\varepsilon}+\mathtt{T}_{D}+\mathtt{T}_{E},\]

where in the last line we use the definition of \(T_{\varepsilon}\), and defined \(\mathtt{T}_{D}\) and \(\mathtt{T}_{E}\) respectively. We bound them separately below:

* _Bound on \(\mathtt{T}_{D}\)._ Recall (23) which implies that \(f^{\star}\) satisfies the bound \(\|f_{t}(x_{t})-f^{\star}(x_{t})\|\leq\Delta_{t}(x_{t})\). Thus, using Lemma 13, we get that \[\mathtt{Margin}(f^{\star}(x_{t}))\leq 2\gamma|f_{t}(x_{t})-f^{\star}(x_{t}) \|+t\mathtt{Margin}(f_{t}(x_{t}))\leq 2\gamma\Delta_{t}(x_{t})+\mathtt{Margin}(f_{t }(x_{t})).\] The above implies that \[\mathtt{T}_{D} =\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))>\varepsilon,\Delta_{t}( x_{t})\leq\varepsilon/4\gamma\}\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{*}(x_{t}))\leq 4 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))>\varepsilon,\Delta_{t}( x_{t})\leq\varepsilon/4\gamma\}\] \[\leq 0,\] where the last line follows from the fact that all the conditions inside the indictor can not hold simultaneously.
* _Bound on \(\mathtt{T}_{E}\)._ We note that \[\mathtt{T}_{E} =\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{*}(x_{t}))>\varepsilon,\Delta_{t}( x_{t})>\varepsilon/4\gamma\}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq \varepsilon/4\gamma\}\] \[\leq\frac{320\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)}{ \varepsilon^{2}}\cdot\mathfrak{E}(\mathcal{F},\varepsilon/4\gamma;f^{*}).\] where the last line follows from setting \(\zeta=\varepsilon/4\gamma\) in Lemma 11.

Gathering the bounds above, we get that

\[N_{T}\leq T_{\varepsilon}+\frac{640\gamma^{2}\Psi_{\delta}^{\ell_{\phi}}( \mathcal{F},T)}{\varepsilon^{2}}\cdot\mathfrak{E}(\mathcal{F},\varepsilon/4 \gamma;f^{*}).\]

Since \(\varepsilon\) is a free parameter above, the final bound follows by choosing the best parameter \(\varepsilon\), and by plugging in the form of \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)\).

### Proof of Theorem 2

Before delving into the proof, we recall the relevant notation. In Algorithm 3,* The label \(y_{t}\sim\phi(f^{*}(x_{t}))\), where \(\phi\) denotes the link-function given in (2).
* The function \(\texttt{SelectAction}(f_{t}(x_{t})):=\operatorname*{argmax}_{k}\phi(f_{t}(x_{t}))[k]\).
* For any vector \(v\in\mathbb{R}^{K}\), the margin is given by the gap between the value at the largest and the second largest coordinate, i.e. \[\texttt{Margin}(v)=\phi(v)[k^{*}]-\max_{k\neq k^{*}}\phi(v)[k],\] where \(k^{*}\in\operatorname*{argmax}_{k\in[K]}\phi(v)[k]\).
* We also define \(T_{\varepsilon}=\sum_{t=1}^{T}\mathbf{1}\{\texttt{Margin}(f^{*}(x_{t}))\leq\varepsilon\}\) to denote the number of samples within \(T\) rounds of interaction for which the margin w.r.t. \(f^{*}\) is smaller than \(\varepsilon\).
* We define the function \(\operatorname*{Gap}:\mathbb{R}^{K}\times[K]\mapsto\mathbb{R}^{+}\) as \[\operatorname*{Gap}(v,k)=\max_{k^{\prime}}\phi(v)[k^{\prime}]-\phi(v)[k],\] (28) to denote the gap between the largest and the \(k\)-th coordinate of \(v\). Recall that Lemma 9 holds.
* Additionally, we define the function \(Z^{e}\) to denote the query condition \[Z^{e}(x)=\mathbf{1}\{\inf_{g\in\mathcal{F}_{e}}\texttt{Margin}(g(x))\leq 2 \gamma\sup_{f,f^{\prime}\in\mathcal{F}_{e}}\lVert f(x)-f^{\prime}(x)\rVert\}.\] (29) The definition in (29) suggests that for all \(t\in[\tau_{e},\tau_{e+1})\), \(Z_{t}=Z^{e}(x_{t})\), for all \(e\leq E-1\).

Intuition for epoching.We next provide intuition on why epoching in needed in Algorithm 3 to get the improved query complexity bound. From the proof sketch in Section E.2, the term \(\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\varepsilon\}\) appearing in the query complexity bound is handled using the eluder dimension of \(\mathcal{F}\). When \(x_{t}\) is sampled i.i.d. we wish to bound this using disagreement-coefficient instead. However, note that in Algorithm 1 the query condition \(Z_{t}\) depends on the samples \(\{x_{s}\}_{sct}\) drawn in all previous time steps and the corresponding query conditions \(\{Z_{s}\}_{s<t}\). This introduces a bias, and thus the terms \(Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\varepsilon\}\) are no longer independent to each other. Thus, we can not directly used distributional properties like the disagreement coefficient to bound the query complexity. Algorithm 3 fixes this issue by defining epochs of doubling length such that the query condition in epoch \(e\) only depends on the samples presented to the learner at time steps before this epoch (i.e. in time steps \(1\leq t\leq\tau_{e}-1\). Thus, the terms \(Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\varepsilon\}\) for \(\tau_{e}\leq t<\tau_{e+1}\) are i.i.d. allowing us to get bounds in terms of distributional properties like the disagreement coefficient of \(\mathcal{F}\).

However, note that whenever we query in Algorithm 3, we still choose the labels according to the estimate from the online regression oracle and thus the regret bound remains unchanged.

#### e.4.1 Supporting Technical Results

The following lemma establishes useful technical properties of the function \(f^{\star}\) and the sets \(\mathcal{F}_{e}\).

**Lemma 14**.: _Suppose Algorithm 3 is run on the sequence \(\{x_{t}\}_{t\leq T}\) drawn i.i.d. from the unknown distribution \(\mu\). Then, with probability at least \(1-\delta\), each of the following holds:_

1. _For all_ \(t\leq T\)_, the function_ \(f^{\star}\in\mathcal{F}\) _satisfies_ \[\sum_{s=1}^{t}Z_{s}\|f^{\star}(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{\delta}^{ \ell_{\phi}}(\mathcal{F},T),\] _where_ \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)=\frac{4}{\lambda}\mathrm{Reg}^{ \ell_{\phi}}(\mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2}(T)/\delta)\)_. Thus,_ \(f^{\star}\in\mathcal{F}_{e}\) _for all_ \(e\leq E-1\)_, and_ \(\|f^{\star}(x_{t})-g_{t}(x_{t})\|\leq\Delta_{e}(x_{t})\) _for all_ \(\tau_{e}\leq t\leq\tau_{e+1}-1\)_._
2. _For any function_ \(f\in\mathcal{F}_{e}\)_, we have_ \[\mathbb{E}\biggl{[}\sum_{s=1}^{\tau_{e}-1}Z_{s}\|f(x_{s})-f^{\star}(x_{s})\|^ {2}\biggr{]}\leq\widehat{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T),\] _where_ \(\widehat{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T):=2\Psi_{\delta}^{\ell_{ \phi}}(\mathcal{F},T)+4c_{2}\bigl{(}\log^{4}(T)\sup_{\tau\leq T}\bigl{(}\tau \mathsf{Rad}_{\tau}^{2}(\mathcal{F})\bigr{)}+2\log(T)\log(E/\delta)\bigr{)}\)__
3. _For any_ \(e\leq E\)_, and any function_ \(f\in\mathcal{F}_{e}\)_, we have_ \[\|f(x_{s})-f^{\star}(x_{s})\|_{\bar{\nu}_{e}}\leq\sqrt{\frac{\widehat{\Psi}_ {\delta}^{\ell_{\phi}}(\mathcal{F};T)}{\tau_{e}-1}}\] _where the sub-distributions_ \(\bar{\mu}_{\bar{e}}(x):=Z^{\bar{e}}(x)\mu(x)\) _and_ \(\bar{\nu}_{e}:=\frac{1}{\tau_{e}-\tau_{1}}\sum_{\bar{e}=1}^{e-1}(\tau_{\bar{e }+1}-\tau_{\bar{e}})\bar{\mu}_{\bar{e}}\)_._
4. _For any_ \(\bar{e}<e\)_, the corresponding sets_ \(\mathcal{F}_{e}\) _and_ \(\mathcal{F}_{\bar{e}}\) _satisfy the relation_ \(\mathcal{F}_{e}\subseteq\mathcal{F}_{\bar{e}}\)_._
5. _For any_ \(\bar{e}\leq e\)_, we have_ \(\bar{\mu}_{e}\lessdot\bar{\mu}_{\bar{e}}\)_._

Proof.: We prove each part separately below:

1. An application of Lemma 5, where we note that we do not query oracle when \(Z_{s}=0\), and thus do not count the time steps for which \(Z_{s}=0\), implies that \[\sum_{s=1}^{t}Z_{s}\|f^{\star}(x_{s})-f_{s}(x_{s})\|^{2}\leq\frac{4}{\lambda} \mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T)+\frac{112}{\lambda^{2}}\log(4\log^{2 }(T)/\delta)=:\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)\] for all \(t\leq T\) with probability at least \(1-\delta\). Using the above for \(t=\tau_{e+1}-1\) implies that \(f^{\star}\in\mathcal{F}_{e}\) for all \(e\leq E-1\). Since, we also have that \(g_{t}\in\mathcal{F}_{e}\) (by construction) for all \(\tau_{e}\leq t\leq\tau_{e+1}-1\), plugging in the definition of \(\Delta_{e}(x_{t})\), we immediately get that \(\|f^{\star}(x_{t})-g_{t}(x_{t})\|\leq\Delta_{e}(x_{t})\).
2. Fix any epoch number \(\bar{e}\leq E-1\), and consider the time steps \(\tau_{\bar{e}}\leq t<\tau_{\bar{e}+1}\). Define the loss function \[\ell_{\bar{e}}(f(x),f^{\star}(x))=Z^{\bar{e}}(x)\|f(x)-f^{\star}(x)\|^{2}\] where \(Z^{\bar{e}}\) denotes the query conditions at epoch \(\bar{e}\) (defined in (29)), and recall that \(Z^{\bar{e}}\) does not depend on any samples that are drawn at epoch \(\bar{e}\) (by definition). Furthermore, note that 

[MISSING_PAGE_EMPTY:33]

* The argument follows from the definition of the set \(\mathcal{F}_{e}\) as any function \(f\in\mathcal{F}_{e}\) that satisfies \[\sum_{s=1}^{\tau_{e}-1}Z_{s}\|f(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{\delta}^{ \ell_{\phi}}(\mathcal{F},T),\] also satisfies the constraint \[\sum_{s=1}^{\tau_{e}-1}Z_{s}\|f(x_{s})-f_{s}(x_{s})\|^{2}\leq\Psi_{\delta}^{ \ell_{\phi}}(\mathcal{F},T),\] for any \(\bar{e}\leq e\), since the left hand side consists of lesser number of terms and all terms are non-negative. Thus, \(\mathcal{F}_{e}\in\mathcal{F}_{\bar{e}}\).
* Recall that for any \(e\leq E-1\), the sub-probability measure \(\bar{\mu}_{e}(x):=Z^{e}(x)\mu(x)\) where \(Z^{e}(x)=\mathbf{1}\{\min_{g\in\mathcal{F}_{e}}\|g(x)\|\leq\Delta_{e}(x)\}\), and \(\Delta_{e}(x)=\max_{f^{\prime},f\in\mathcal{F}_{e}}\left|\,f(x)-f^{\prime}(x)\right|\). First note that for any \(\bar{e}\leq e\), \[\Delta_{e}(x)=\max_{f^{\prime},f\in\mathcal{F}_{e}}\|f(x)-f^{\prime}(x)\|\leq \max_{f^{\prime},f\in\mathcal{F}_{e}}\|f(x)-f^{\prime}(x)\|=\Delta_{\bar{e}}(x),\] where the inequality above holds because \(\mathcal{F}_{e}\subseteq\mathcal{F}_{\bar{e}}\) due to part-(d) above. Furthermore, \[\min_{g\in\mathcal{F}_{e}}\|g(x)\|\geq\min_{g\in\mathcal{F}_{\bar{e}}}\|g(x)\|,\] again because \(\mathcal{F}_{e}\subseteq\mathcal{F}_{\bar{e}}\). Thus, \[Z^{e}(x)=\mathbf{1}\{\min_{g\in\mathcal{F}_{e}}\|g(x)\|\leq\Delta_{e}(x)\} \leq\mathbf{1}\{\min_{g\in\mathcal{F}_{e}}\|g(x)\|\leq\Delta_{\bar{e}}(x)\} \leq Z^{\bar{e}}(x).\] The above implies that \(\bar{\mu}_{e}\leq\bar{\mu}_{\bar{e}}\).

**Lemma 15**.: _Let \(\varepsilon_{0},\gamma_{0}\geq 0\), and \(f^{\star}\in\mathcal{F}\). Then, for any sub distribution \(\bar{\mu}\) such that \(\mathbb{E}_{x-\bar{\mu}}[\mathbf{1}\{x\in\mathcal{X}\}]>0\), \(\varepsilon\geq\varepsilon_{0}\) and \(\gamma\geq\gamma_{0}\),_

\[\frac{\varepsilon^{2}}{\gamma^{2}}\Pr_{x-\bar{\mu}}\bigl{(}\exists f\in \mathcal{F}\;:\;\|f(x)-f^{\star}(x)\|>\varepsilon,\|f(x_{s})-f^{\star}(x_{s}) \|_{\bar{\mu}}\leq\gamma\bigr{)}\leq\theta^{\mathrm{val}}\bigl{(}\mathcal{F}, \varepsilon_{0},\gamma_{0};f^{\star}\bigr{)}.\]

Proof.: The key idea in the proof is to go from sub distributions to distributions, and then invoking the definition of \(\theta\) from Definition 2. Define \(\kappa=\mathbb{E}_{x-\bar{\mu}}[\mathbf{1}\{x\in\mathcal{X}\}]\). Since \(0<\kappa\leq 1\), we can define a probability measure \(\mu\) such that \(\mu(x)=\bar{\mu}(x)/\kappa\). Thus, for any \(\varepsilon\geq\varepsilon_{0}\) and \(\gamma\geq\gamma_{0}\),

\[\frac{\varepsilon^{2}}{\gamma^{2}}\Pr_{x-\bar{\mu}}\bigl{(}\exists f \in\mathcal{F}\;:\;\|f(x)-f^{\star}(x)\|>\varepsilon,\|f(x_{s})-f^{\star}(x_{ s})\|_{\bar{\mu}}\leq\gamma\bigr{)}\] \[\qquad\qquad=\frac{\varepsilon^{2}}{\gamma^{2}/k}\Pr_{x-\mu} \bigl{(}\exists f\in\mathcal{F}\;:\;\|f(x)-f^{\star}(x)\|>\varepsilon,\|f(x_{ s})-f^{\star}(x_{s})\|_{\bar{\mu}}\leq\gamma\bigr{)}\] \[\qquad\qquad\leq\frac{\varepsilon^{2}}{\gamma^{2}/k}\Pr_{x-\mu} \bigl{(}\exists f\in\mathcal{F}\;:\;\|f(x)-f^{\star}(x)\|>\varepsilon,\|f(x_{ s})-f^{\star}(x_{s})\|_{\mu}\leq\gamma\bigr{)}\] \[\qquad\qquad\leq\frac{\varepsilon^{2}}{\gamma^{2}/k}\Pr_{x-\mu} \bigl{(}\exists f\in\mathcal{F}\;:\;\|f(x)-f^{\star}(x)\|>\varepsilon,\|f(x_{ s})-f^{\star}(x_{s})\|_{\mu}\leq\bar{\gamma}\bigr{)},\]

where in the second last line, we defined \(\bar{\gamma}=\nicefrac{{\gamma}}{{\sqrt{\kappa}}}\), and the last line used the fact that both \(\varepsilon\geq\varepsilon_{0}\) and \(\bar{\gamma}\geq\gamma_{0}\). The final statement follows by noting the fact that \(\mu\) is a distribution and the definition of the disagreement coefficient \(\theta^{\mathrm{val}}(;)\) from Definition 2. 

The following technical result will be useful in bounding the query complexity for Algorithm 3.

**Lemma 16**.: _For any \(t\leq T\), let \(e(t)\) denotes the epoch number such that \(\tau_{e(t)}\leq t<\tau_{e(t)+1}\). Let \(f^{*}\in\mathcal{F}\) satisfy Lemma 14, and let \(\Delta_{e(t)}(x_{t})\) be defined in Algorithm 3. Then, for any \(\zeta>0\), with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\big{\{}\Delta_{e(t)}(x_{t})\geq\zeta\big{\}}\leq 1 2\log(T)\cdot\frac{\widehat{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{\zeta ^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac{\zeta}{2},\frac{ \widehat{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{T};f^{*}\Bigg{)}+4\log(2 /\delta).\]

Proof.: Recall the definition of the query rule \(Z^{e}\) given in (29), and note that the function \(Z^{e}\) is independent of the samples \(\{x_{t}\}_{t\neq\tau_{e}}^{\tau_{e+1}-1}\) chosen by the nature for time steps at epoch \(e\). Additionally, also recall that at every time step, \(x_{t}\) is sampled independently from the distribution \(\mu\). Thus, using the query condition \(Z^{e}\), we can define the sub-probability measure

\[\bar{\mu}_{e}:=\mu(x)Z^{e}(x),\] (32)

such that \(\bar{\mu}_{e}(x)=\mu(x)\) whenever \(Z_{e}(x)=1\) and is \(0\) otherwise. Furthermore, for any \(e\in E-1\), we define the sub-probability measure \(\bar{\nu}_{e}\) as

\[\bar{\nu}_{e}=\frac{1}{\tau_{e}-\tau_{1}}\sum_{\bar{e}=1}^{e-1}(\tau_{\bar{e}+ 1}-\tau_{\bar{e}})\bar{\mu}_{\bar{e}}.\] (33)

We now move to the main proof. First fix any epoch \(e\leq E-1\), and consider any round \(t\in[\tau_{e},\tau_{e+1}-1]\). Using the definition of \(\Delta_{e}(x_{t})\) and definition of \(Z^{e}\) from (29) in the above, we get that

\[\mathbb{E}_{x_{t}\sim\mu}[Z_{t}\mathbf{1}\{\Delta_{e}(x_{t})> \zeta\}] =\mathbb{E}_{x\sim\mu}\Bigg{[}Z^{e}(x_{t})\mathbf{1}\Bigg{\{} \sup_{f,f^{*}\in\mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|>\zeta\Bigg{\}}\Bigg{]}\] \[\leq\mathbb{E}_{x\sim\mu}\Bigg{[}Z^{e}(x_{t})\mathbf{1}\Bigg{\{} \sup_{f\in\mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|>\frac{\zeta}{2}\Bigg{\}} \Bigg{]}\]

where the second line follows because \(f^{*}\in\mathcal{F}_{e}\), and because \(\sup_{f,f^{*}\in\mathcal{F}_{e}}\|f(x_{t})-f^{\prime}(x_{t})\|\leq 2\sup_{f\in \mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|\) due to Triangle inequality. Plugging in the definition of \(\bar{\mu}_{e}\) from (32) in the above we get that

\[\mathbb{E}_{x_{t}\sim\mu}[Z_{t}\mathbf{1}\{\Delta_{e}(x_{t})> \zeta\}] \leq\mathbb{E}_{x\sim\bar{\mu}_{e}}\Bigg{[}\mathbf{1}\Bigg{\{} \sup_{f\in\mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|>\frac{\zeta}{2}\Bigg{\}} \Bigg{]}\] \[\leq\mathbb{E}_{x\sim\bar{\mu}_{e}}\Bigg{[}\mathbf{1}\Bigg{\{} \sup_{f\in\mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|>\frac{\zeta}{2}\Bigg{\}} \Bigg{]}.\] (34)

for all \(\bar{e}\leq e\), where the last inequality follows from Lemma 14-\((e)\). Since the above holds for all \(\bar{e}\leq e\), we immediately get that

\[\mathbb{E}_{x_{t}\sim\mu}[Z_{t}\mathbf{1}\{\Delta_{e}(x_{t})> \zeta\}] \leq\mathbb{E}_{x\sim\bar{\nu}_{e}}\Bigg{[}\mathbf{1}\Bigg{\{} \sup_{f\in\mathcal{F}_{e}}\|f(x_{t})-f^{*}(x_{t})\|>\frac{\zeta}{2}\Bigg{\}} \Bigg{]}\] \[=\mathbb{E}_{x\sim\bar{\nu}_{e}}\Bigg{[}\mathbf{1}\Bigg{\{} \exists f\in\mathcal{F}_{e}\;:\;\|f(x)-f^{*}(x)\|>\frac{\zeta}{2}\Big{\}} \Bigg{]},\] (35)

where the sub-probability measure \(\bar{\nu}_{\bar{e}}\) is defined in (33). Additionally, recall that Lemma 14-\((b)\) implies that with probability at least \(1-\delta\) any \(f\in\mathcal{F}_{e}\) satisfies

\[\|f(x_{s})-f^{*}(x_{s})\|_{\bar{\nu}_{e}}\leq\sqrt{\frac{\widehat{\Psi}_{ \delta}^{\ell_{\phi}}(\mathcal{F};T)}{\tau_{e}-1}}.\] (36)

Conditioning on the above event, and plugging it in (35), we get that

\[\mathbb{E}_{x_{t}\sim\bar{\nu}_{e}}\Bigg{[}\mathbf{1}\Bigg{\{} \exists f\in\mathcal{F}_{e}\;:\;\|f(x)-f^{*}(x)\|>\frac{\zeta}{2},\|f(x_{s})-f ^{*}(x_{s})\|_{\bar{\nu}_{e}}\leq\sqrt{\frac{\widehat{\Psi}_{\delta}^{\ell_{ \phi}}(\mathcal{F};T)}{\tau_{e}-1}}\Bigg{\}}\Bigg{]}\]\[\leq 4\cdot\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{ (\tau_{e}-1)\zeta^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac{ \zeta}{2},\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{T};f^{ \star}\Bigg{)},\] (37)

where the last inequality uses Lemma 15.

Summing up the bound in (37) for each term \(t=1\) to \(T\), we get that

\[\sum_{t=1}^{T}\mathbb{E}_{x_{t}}[Z_{t}\mathbf{1}\{\Delta_{e}(x_{t })>\zeta\}] =\sum_{e=1}^{E-1}\tau_{e+1}^{\tau_{e+1}-1}\mathbb{E}_{x_{t}}[Z_{t }\mathbf{1}\{\Delta_{e}(x_{t})>\zeta\}]\] \[\leq 4\sum_{e=1}^{E-1}(\tau_{e+1}-\tau_{e})\frac{\widetilde{\Psi} _{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{(\tau_{e}-1)\zeta^{2}}\cdot\theta^{ \mathrm{val}}\Bigg{(}\mathcal{F},\frac{\zeta}{2},\frac{\widetilde{\Psi}_{ \delta}^{\ell_{\phi}}(\mathcal{F};T)}{\tau_{e}-1};f^{\star}\Bigg{)}\] \[\leq 8\sum_{e=1}^{E-1}\frac{\widetilde{\Psi}_{\delta}^{\ell_{ \phi}}(\mathcal{F};T)}{\zeta^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac{\zeta}{2},\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T) }{\tau_{e}-1};f^{\star}\Bigg{)}\] \[\leq 8\sum_{e=1}^{E-1}\frac{\widetilde{\Psi}_{\delta}^{\ell_{ \phi}}(\mathcal{F};T)}{\zeta^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac{\zeta}{2},\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T )}{T};f^{\star}\Bigg{)}\] \[\leq 8\log(T)\cdot\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}( \mathcal{F};T)}{\zeta^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac {\zeta}{2},\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{T};f ^{\star}\Bigg{)}\]

where the second inequality uses the fact that \(\tau_{e+1}=2\tau_{e}\) and that \(\tau_{1}=1\), the third inequality holds due to monotonicity of \(\theta^{\mathrm{val}}\big{(}\mathcal{F},\frac{\zeta}{2},\cdot;f^{\star}\big{)}\) and the last line simply plugs in the value of \(E=\log(T)\).

Using Lemma 4 with the above bound for the sequence of random variable \(X_{t}=Z_{t}\mathbf{1}\{\Delta_{e}(x_{t})>\zeta\}\), we get that with probability at least \(1-\delta\),

\[\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{e}(x_{t})>\zeta\} \leq\frac{3}{2}\sum_{t=1}^{T}\mathbb{E}_{x_{t}}[Z_{t}\mathbf{1}\{ \Delta_{e}(x_{t})>\zeta\}]+4\log(2/\delta)\] \[\leq 12\log(T)\cdot\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}( \mathcal{F};T)}{\zeta^{2}}\cdot\theta^{\mathrm{val}}\Bigg{(}\mathcal{F},\frac {\zeta}{2},\frac{\widetilde{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{T};f ^{\star}\Bigg{)}+4\log(2/\delta).\]

The final result follows by taking a union bound of the above and the event in (36). 

#### e.4.2 Regret Bound

For the ease of notation, through the proofs in this section we define the operators \(\pi^{\star}\) as

\[\pi^{\star}(x)=\operatorname*{argmax}_{k}\phi(f^{\star}(x))[k].\]

Furthermore, recall that \(\widetilde{y}_{t}\) denotes the action chosen by the learner at round \(t\) of interaction. Starting from the definition of the regret, we get that

\[\operatorname{Reg}_{T} =\sum_{t=1}^{T}\Pr(\widetilde{y}_{t}\neq y_{t})-\Pr(\pi^{\star}( x_{t})\neq y_{t})\] \[=\sum_{t=1}^{T}\mathbf{1}\{\widetilde{y}_{t}\neq\pi^{\star}(x_{t}) \}\cdot|\Pr(y_{t}=\pi^{\star}(x_{t}))-\Pr(y_{t}=\widehat{y}_{t})|\] \[=\sum_{t=1}^{T}\mathbf{1}\{\widetilde{y}_{t}\neq\pi^{\star}(x_{t} )\}\cdot|\phi(f^{\star}(x_{t}))[\pi^{\star}(x_{t})]-\phi(f^{\star}(x_{t}))[ \widetilde{y}_{t}]|\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\widetilde{y}_{t}\neq\pi^{\star}(x_ {t})\}\cdot\operatorname{Gap}(f^{\star}(x_{t}),\widetilde{y}_{t}),\]

where the last inequality plugs in the definition of \(\operatorname{Gap}\) from (45). Let \(\varepsilon>0\) be a free parameter. We can decompose the above regret bound further as:

\[\operatorname{Reg}_{T}\leq\sum_{t=1}^{T}\mathbf{1}\{\widetilde{y}_{t}\neq\pi^{ \star}(x_{t}),\operatorname{Gap}(f^{\star}(x_{t}),\widetilde{y}_{t})\leq \varepsilon\}\cdot\operatorname{Gap}(f^{\star}(x_{t}),\widetilde{y}_{t})\]\[\mathbb{T}_{B}=0.\] (40)Combining the bounds (38) and (40), we get that:

\[\mathrm{Reg}_{T} \leq\varepsilon T_{e}+8\gamma^{2}\sum_{t=1}^{T}Z_{t}\frac{\|f_{t}(x _{t})-f^{\star}(x_{t})\|^{2}}{\varepsilon}\] \[\leq\varepsilon T_{e}+\frac{8\gamma^{2}}{\varepsilon}\Psi_{\delta} ^{\ell_{\phi}}(\mathcal{F},T),\]

where the last inequality is due to Lemma 14.

Since \(\varepsilon\) is a free parameter above, the final bound follows by choosing the best parameter \(\varepsilon\), and by plugging in the form of \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F},T)\).

#### e.4.3 Total Number of Queries

Let \(N_{T}\) denote the total number of expert queries made by the learner within \(T\) rounds of interactions. For the ease of notation, define \(\Delta_{t}(x_{t})=\Delta_{e(t)}(x_{t})\) where \(e(t)\) denotes the epoch number for which \(\tau_{e(t)}\leq t<\tau_{e(t)+1}\). Additionally, let \(\varepsilon>0\) be a free parameter. Thus,

\[N_{T} =\sum_{t=1}^{T}Z_{t}\] \[=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t})\}\] \[=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))\leq\varepsilon\}\] \[\qquad\qquad\qquad+\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t }(x_{t}))\leq 2\gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))>\varepsilon\}\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{\star}(x_{t})) \leq\varepsilon\}\] \[\qquad\qquad\qquad+\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t }(x_{t}))\leq 2\gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))> \varepsilon,\Delta_{t}(x_{t})>\varepsilon/4\gamma\}\] \[=T_{\varepsilon}+\mathtt{T}_{D}+\mathtt{T}_{E},\]

where in the last line we used the definition of \(T_{\varepsilon}\) and defined \(\mathtt{T}_{D}\) and \(\mathtt{T}_{E}\) respectively, which we bound separately below.

* _Bound on \(\mathtt{T}_{D}\)._ From Lemma 14 recall that \(\|f^{\star}(x_{t})-g_{t}(x_{t})\|\leq\Delta_{e}(x_{t})\). Thus, for any \(x_{t}\) for which \(\|g_{t}(x_{t})\|\leq\Delta_{e}(x_{t})\), Lemma 13 implies that \[\mathtt{Margin}(f^{\star}(x_{t}))\leq 2\gamma|f_{t}(x_{t})-f^{\star}(x_{t})\|+ \mathtt{Margin}(g_{t}(x_{t}))\leq 2\gamma\Delta_{t}(x_{t})+\mathtt{Margin}(f_{t}(x_{t})).\] The above implies that \[\mathtt{T}_{D} =\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))>\varepsilon,\Delta_ {t}(x_{t})\leq\varepsilon/4\gamma\}\] \[\leq\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{\star}(x_{t})) \leq 4\gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))>\varepsilon, \Delta_{t}(x_{t})\leq\varepsilon/4\gamma\}\] \[\leq 0,\] where the last line follows from the fact that all the conditions inside the indictor can not hold simultaneously for any \(\varepsilon>0\).
* _Bound on \(\mathtt{T}_{E}\)._ We note that \[\mathtt{T}_{E}=\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(g_{t}(x_{t}))\leq 2 \gamma\Delta_{t}(x_{t}),\mathtt{Margin}(f^{\star}(x_{t}))>\varepsilon,\Delta_ {t}(x_{t})>\varepsilon/4\gamma\}\] \[\leq 0,\] where the last line follows from the fact that all the conditions inside the indictor can not hold simultaneously for any \(\varepsilon>0\).

\[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{t}(x_{t})\geq\nicefrac{{ \varepsilon}}{{4\gamma}}\}\] \[\leq\sum_{t=1}^{T}Z_{t}\mathbf{1}\{\Delta_{e(t)}(x_{t})\geq \nicefrac{{\varepsilon}}{{4\gamma}}\}.\]

Using Lemma 16 with \(\zeta=\nicefrac{{\varepsilon}}{{4\gamma}}\) to bound the term on the right hand side above, we get that with probability at least \(1-2\delta\),

\[\mathtt{T}_{E}\leq O\Bigg{(}\log(T)\gamma^{2}\cdot\frac{\widehat{\Psi}_{ \delta}^{\ell_{\phi}}(\mathcal{F};T)}{\varepsilon^{2}}\cdot\theta^{\mathrm{val} }\Bigg{(}\mathcal{F},\frac{\varepsilon}{8\gamma},\frac{\widehat{\Psi}_{\delta }^{\ell_{\phi}}(\mathcal{F};T)}{T};f^{\star}\Bigg{)}+\log(2/\delta)\Bigg{)}.\]

Gathering the bounds above, we get that

\[N_{T}\leq T_{\varepsilon}+O\Bigg{(}\log(T)\gamma^{2}\cdot\frac{\widehat{\Psi }_{\delta}^{\ell_{\phi}}(\mathcal{F};T)}{\varepsilon^{2}}\cdot\theta^{\mathrm{ val}}\Bigg{(}\mathcal{F},\frac{\varepsilon}{8\gamma},\frac{\widehat{\Psi}_{ \delta}^{\ell_{\phi}}(\mathcal{F};T)}{T};f^{\star}\Bigg{)}+\log(2/\delta) \Bigg{)}.\]

Since \(\varepsilon\) is a free parameter above, the final bound follows by choosing the best parameter \(\varepsilon\), and by plugging in the form of \(\widehat{\Psi}_{\delta}^{\ell_{\phi}}(\mathcal{F};T)\).

### Proof of Corollary 1

Note that the Tsybakov noise condition implies that there exists constants \(c,\rho\geq 0\) such that for all \(\varepsilon\in(0,1)\):

\[\Pr_{x\sim\mu}(\mathtt{Margin}(f^{\star}(x_{t}))\leq\varepsilon)\leq c \varepsilon^{\rho}.\]

Thus, using Lemma 4, we get that

\[T_{\varepsilon} =\sum_{t=1}^{T}\mathbf{1}\{\mathtt{Margin}(f^{\star}(x_{t}))\leq\varepsilon\}\] \[\leq\frac{3T}{2}\Pr_{x\sim\mu}(\mathtt{Margin}(f^{\star}(x))\leq \varepsilon)+4\log(2/\delta)\] \[\leq 2cT\varepsilon^{\rho}+4\log(2/\delta).\]

Using the above in the bound for Theorem 2, we get that for any \(\varepsilon>0\),

\[\mathrm{Reg}_{T}\lesssim cT\varepsilon^{\rho+1}+\frac{\gamma^{2}}{\lambda \varepsilon}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T)+\log(1/\delta),\]

Setting \(\varepsilon=\left(\frac{\gamma^{2}}{\lambda cT}\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F};T)\right)^{\frac{1}{\rho+2}}\) in the above implies that

\[\mathrm{Reg}_{T}\lesssim\left(\frac{\gamma^{2}}{\lambda}c^{\frac{1}{\rho+1}} \right)^{\frac{\rho+1}{\rho+2}}\left(\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T )\right)^{\frac{\rho+1}{\rho+2}}\cdot(T)^{\frac{1}{\rho+2}}+\log(1/\delta).\]

Similarly, we can bound the query complexity bound for any \(\varepsilon>0\) as:

\[N_{T}\lesssim T\varepsilon^{\rho}+\frac{\gamma^{2}}{\lambda\varepsilon^{2}} \cdot\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T)\cdot\theta^{\mathrm{val}} \big{(}\mathcal{F},\nicefrac{{\varepsilon}}{{8\gamma}},\mathrm{Reg}^{\ell_{ \phi}}(\mathcal{F};T)\big{/}\!T;f^{\star}\big{)}+\log(1/\delta).\]

Setting \(\varepsilon=\left(\frac{\gamma^{2}}{\lambda T}\cdot\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F};T)\cdot\theta^{\mathrm{val}}\big{(}\mathcal{F},\nicefrac{{ \varepsilon}}{{8\gamma}},\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T)\big{/}\!T; f^{\star}\big{)}\right)^{\frac{1}{\rho+2}}\) in the above implies that

\[N_{T}\leq\left(\frac{\gamma^{2}}{\lambda}\cdot\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F};T)\cdot\theta^{\mathrm{val}}\big{(}\mathcal{F},\nicefrac{{ \varepsilon}}{{8\gamma}},\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F};T)\big{/}\!T; f^{\star}\big{)}\right)^{\frac{\rho}{\rho+2}}\cdot T^{\frac{2}{\rho+2}}.\]

### Proofs for Lower Bounds in Section 3.2

The proof of Theorem 3 below follows along the lines of the proof of the lower bound in Theorem 28 of Foster et al. (2020) with minor changes.

Proof of Theorem 3.: Let \(\beta\leq\zeta/2\) be the largest number such that \(\beta^{2}\leq\min\{\zeta^{2}/\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta, \beta),\zeta^{2}/16\}\). Given the function class \(\mathcal{F}\), assume that \(m=\mathfrak{s}^{\mathrm{val}}(\mathcal{F},\zeta,\beta)>0\) (the lower bound is obvious when the star number is \(0\)). Let the target function \(f^{\star}\), the data sequence \(x^{1},\ldots,x^{m}\), and the function \(f_{1},\ldots,f_{m}\in\mathcal{F}\) be the witnesses for the fact that the star number is \(m\) (see Definition 3). First, for any \(i\in[m]\), we have that \(|f^{\star}(x^{i})|>\zeta\) by definition of the star number. Next note that, for each \(f_{i}\), we have that, for any \(j\neq i\),

\[|f_{i}(x^{j})|\geq|f^{\star}(x^{j})|-|f_{i}(x^{j})-f^{\star}(x^{j})|\geq\zeta- \beta\geq\zeta/2\]

On the other hand, from our definition of star number we have that,

\[|f_{i}(x^{i})|\geq\zeta/2.\]

Hence we are guaranteed that each \(f_{i}\) has a margin of at least \(\zeta/2\) on \(x^{1},\ldots,x^{m}\). Now consider the distribution \(\mu\) over the context to be the uniform distribution over \(\{x^{1},\ldots,x^{m}\}\). Also let \(P^{i}(y=1\mid x)=\nicefrac{{1}}{{f_{i}(x)}}\nicefrac{{2}}{{2}}\) be the conditional probability of label given context \(x\). Let \(D_{i}\) denote the joint distribution over \(\mathcal{X}\times\{\pm 1\}\) given by drawing \(x\)'s from \(\mu\) and labels from \(P^{i}\). Additionally, let \(D_{0}\) be given by drawing \(x\)'s from \(\mu\) and \(y\) conditioned on \(x\) as \(P(y=1\mid x)=\nicefrac{{1}}{{f^{\star}(x)}}\nicefrac{{2}}{{2}}\). Finally, let \(\nu=0\) with probability \(1/2\) and \(\nu\sim\mathrm{Uniform}([m])\) with probability \(1/2\). Note that by our premise,

\[\frac{1}{2}\mathbb{E}_{D_{0}}\left[\mathrm{Reg}_{T}\right]+\frac{1}{2m}\sum_{ i=1}^{m}\mathbb{E}_{D_{i}}\left[\mathrm{Reg}_{T}\right]=\mathbb{E}_{\nu} \left[\mathbb{E}_{D_{\nu}}\left[\mathrm{Reg}_{T}\right]\right]\leq c\frac{ \zeta T}{m},\] (41)

where the value of the constant \(c\) will be set later. Furthermore, let \(p_{t}:\mathcal{X}\mapsto\Delta(\mathcal{Y})\) denote the distribution over the label chosen by the given algorithm at round \(t\) given the history \((x_{1},y_{1},\ldots,x_{t-1},y_{t-1})\). We note that for any \(i\in[m]\),

\[\mathbb{E}_{D_{i}}\left[\mathrm{Reg}_{T}\right] \geq\mathbb{E}_{D_{i}}\left[\sum_{t=1}^{T}\frac{\zeta}{2}\mathbb{ E}_{x\sim\mu}\mathbb{E}_{\nicefrac{{j}}{{\sim p_{t}(x)}}}\left[\mathbf{1} \{f_{i}(x)\hat{y}<0\}\right]\right]\] \[=\frac{T\zeta}{2}\mathbb{E}_{D_{i}}\left[\frac{1}{T}\sum_{t=1}^{T }\mathbb{E}_{x\sim\mu}\mathbb{E}_{\nicefrac{{j}}{{\sim p_{t}(x)}}}\mathbf{1} \{\text{sign}(f_{i}(x))\neq\hat{y}\}\right]\] \[\geq\frac{T\zeta}{4}\mathbb{E}_{D_{i}}\left\|\frac{1}{T}\sum_{t=1 }^{T}p_{t}-\text{sign}(f_{i})\right\|_{L_{1}(\mu)}\]

where in the first inequality we used that \(f_{i}\) has a margin of at least \(\zeta/2\) for any \(x\in\{x^{1},\ldots,x^{m}\}\) as shown above, and the last inequality simply follows from the fact that

\[|\mathbb{E}_{\nicefrac{{j}}{{\sim p_{t}(x)}}}\mathbf{1}\{\text{sign}(f_{i}(x ))\neq\hat{y}\}|=|\mathbb{E}_{\nicefrac{{j}}{{\sim p_{t}(x)}}}(\mathbf{1}\{ \text{sign}(f_{i}(x))=1\}-\mathbf{1}\{\widehat{y}=0\})|=\frac{1}{2}|p_{t}(x)-f _{i}(x)|,\]

and via an application of the Jensen's inequality. Using the property that for any distribution with margin at least \(\zeta/2\), the algorithm satisfies \(\mathbb{E}_{D_{i}}[\mathrm{Reg}_{T}]\leq\nicefrac{{c}}{{7}}\nicefrac{{m}}{{m}}\), the above implies that

\[\mathbb{E}_{D_{i}}\left\|\frac{1}{T}\sum_{t=1}^{T}p_{t}-\text{sign}(f_{i}) \right\|_{L_{1}(\mu)}\leq\frac{8c}{m},\]

which implies that

\[\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{D_{i}}\left\|\frac{1}{T}\sum_{t=1}^{T}p_{ t}-\text{sign}(f_{i})\right\|_{L_{1}(\mu)}\leq\frac{8c}{m}.\] (42)

Hence, setting \(c=64\) and using Markov's inequality, we have that

\[\frac{1}{m}\sum_{i=1}^{m}\Pr_{D_{i}}\left(\left\|\frac{1}{T}\sum_{t=1}^{T}p_{ t}-\text{sign}(f_{i})\right\|_{L_{1}(\mu)}>\frac{2}{m}\right)\leq\frac{1}{16}.\]Further note that \(\left\|\text{sign}(f_{j})\right.\) - \(\text{sign}(f_{i})\right\|_{L_{1}(\mu)}>\frac{4}{m}\) and thus, condition on the fact that \(\nu\) is not equal to \(0\), we can can identify \(\nu\) correctly with probability at least \(1-1/16\). Thus, considering the reference measure \(D_{0}\) and using Fano's inequality, we must have that

\[\log(2)\leq\frac{1}{m}\sum_{i=1}^{m}\operatorname{KL}(D_{0}\|D_{i})\]

Now we are left with bounding \(\operatorname{KL}(D_{0}\|D_{i})\). To this end, we first make a simple observation that the distribution on the \(x_{t}\)'s is the same under \(D_{0}\) and \(D_{i}\). Hence on rounds \(t\) where \(Z_{t}=0\) we do not query for the labels in these rounds, and thus we do not glean any new information to distinguish \(D_{i}\) from \(D_{0}\). In other words, we only need to consider rounds when \(Z_{t}=1\). Hence we have:

\[\operatorname{KL}(D_{0}\|D_{i})=\mathbb{E}_{D_{0}}\left[\sum_{t=1}^{T}Z_{t} \cdot\operatorname{kl}(P_{0}(y_{t}=1|x_{t})\|P_{i}(y_{t}=1|x_{t}))\right]\]

Assuming \(\zeta>1/4\) and using the bound on KL between Bernoulli variables we get,

\[\operatorname{KL}(D_{0}\|D_{i})\leq\mathbb{E}_{D_{0}}\left[32N_{i}\zeta^{2}+8 \left(\max_{j\neq i}N_{j}\right)\beta^{2}\right]\]

where \(N_{i}=|\{t:Z_{t}=1,x_{t}=x^{i}\}\), and we used item-**(3)** in the definition of star number for the \(\zeta^{2}\) term and item-**(1)** for the \(\beta^{2}\) term. Hence we have that,

\[\log(2) \leq\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}_{D_{0}}\left[32N_{i}\zeta ^{2}+8\left(\max_{j\neq i}N_{j}\right)\beta^{2}\right]\] \[\leq\frac{32}{m}\mathbb{E}_{D_{0}}\left[\sum_{i=1}^{m}N_{i}\right] \zeta^{2}+8\mathbb{E}_{D_{0}}\left[\max_{j}N_{j}\right]\beta^{2}\] \[\leq\frac{32}{m}\mathbb{E}_{D_{0}}\left[N_{T}\right]\zeta^{2}+8 \mathbb{E}_{D_{0}}\left[N_{T}\right]\beta^{2}\]

Since \(\beta^{2}\leq\zeta^{2}/m\), we have that

\[\log(2)\leq\frac{40}{m}\mathbb{E}_{D_{0}}\left[N_{T}\right]\zeta^{2}\]

Hence we conclude that,

\[\mathbb{E}_{D_{0}}\left[N_{T}\right]\geq\frac{\log(2)m}{40\zeta^{2}}\]

which yields the desired lower bound. 

Proof of Corollary 2.: Consider the function class \(\mathcal{F}=\{f_{0},f_{1},\ldots,f_{\sqrt{T}}\}\) where \(f_{0}(x_{i})=1/2+\zeta\) for every \(x_{1},\ldots,x_{\sqrt{T}}\), and where \(f_{i}(x_{i})=1/2-\zeta\) and \(f_{i}(x_{j})=1/2+\zeta\) for any \(j\neq i\). Note that selecting \(f^{\star}=f_{0}\) and \(f_{1},\ldots,f_{m}\) on \(x_{1},\ldots,x_{\sqrt{T}}\), we can show that the star number \(\mathfrak{s}^{\text{val}}(\mathcal{F},1/8,1/2)=O(\sqrt{T})\) (the disagreement coefficient of \(\mathcal{F}\) is also \(O(\sqrt{T})\)). Thus, using the converse of Theorem 3 we get that if the number of queries is smaller than \(\sqrt{T}\), then there must exist some data distribution under which the regret bound of the algorithm is larger that \(\sqrt{T}\).

Imitation Learning: Learning from Single Expert

### Imitation Learning Tools

We first recall useful additional notation. Recall that a policy \(\pi\) is a mapping from the states \(\mathcal{X}\) to actions \(\mathcal{A}\). For any \(h\leq H\), and random variable \(\mathsf{Z}(x_{h},a_{h})\), we use the notation \(\mathbb{E}_{\pi}\big{[}\mathsf{Z}(x_{h},a_{h})\big{]}\) to denote the expectation w.r.t. trajectories \(\{x_{1},a_{1}\ldots,x_{H},a_{H}\}\) sampled using the policy \(\pi\).

The following lemma is the standard performance difference lemma, well known in the imitation learning and reinforcement learning literature.

**Lemma 17** (Performance Difference Lemma; Kakade and Langford (2002); Ross and Bagnell (2014)).: _For any MDP \(M\), and any two arbitrary stationary policies \(\pi\) and \(\widetilde{\pi}\), we have_

\[V^{\pi}-V^{\widetilde{\pi}}=\sum_{h=1}^{H}\mathbb{E}_{x_{h},a_{h} \prec d_{h}^{\pi}}[-A_{h}^{\pi}(x_{h},a_{h})],\]

_where \(A^{\pi}\) is the advantage function of the policy \(\pi\) in MDP \(M\), i.e., \(A_{h}^{\pi}(x,a)=Q_{h}^{\pi}(x,a)-V_{h}^{\pi}(x)\)._

### Proof of Proposition 1

MDP construction.The underlying MDP is a binary tree of depth \(H\). In particular, we construct the deterministic MDP \(M=\big{(}\mathcal{X},\mathcal{A},P,r,x_{1}\big{)}\) where state space \(\mathcal{X}=\cup_{h=1}^{H}\mathcal{X}_{h}\) with \(\mathcal{X}_{h}=\{x_{h,i}\}_{i=1}^{2^{n-1}}\) (we assume that \(x_{1}=x_{1,1}\)), action space \(\mathcal{A}=\{0,1\}\), reward \(r\) is such that \(r(x,a)=\mathrm{Bern}\big{(}\frac{1}{2}+\frac{1}{4}\mathbf{1}\{x=x^{*}\}\big{)}\) for some special state \(x^{*}\in\mathcal{X}_{H}\). The transition dynamics \(P\) is deterministic and defines a binary tree over \(\mathcal{X}\), i.e. for any \(h\) and \(x_{h,i}\), \(P(x^{\prime}\mid x_{h,i},a)=1\) if \(x^{\prime}=x_{h+1,2i-1}\) and \(a=0\), or \(x^{\prime}=x_{h+1,2i+1}\) and \(a=1\), else \(P(x^{\prime}\mid x,a)=0\).

We next define the expert policy \(\pi^{*}\), expert model \(f^{*}\) and the class \(\mathcal{F}\). First, for any path \(\tau=(x_{1},a_{1},\ldots,x_{H},a_{H})\) from the root state \(x_{1}\) to a terminal state \(x_{H}\) at the layer \(H\), define the policy \(\pi_{\tau}\) as

\[\pi_{\tau}(x_{h})=\begin{cases}a_{h}&\text{if}\quad(x_{h},a_{h} )\in\tau\\ \bar{a}_{h}\leftarrow\text{Uniform}(\{0,1\})&\text{otherwise}\end{cases}.\]

In particular, \(\pi_{\tau}\) is defined such that for any state on the path \(\tau\), we choose the corresponding action in \(\tau\), and for any state outside of \(\tau\), we choose an arbitrary (deterministic) action. Let \(\mathcal{T}\) denote the set of all \(2^{H}\) many paths from the root note \(x_{1}\) to a leaf node \(x_{H}\in\mathcal{X}_{H}\). We define the class \(\Pi=\{\pi_{\tau}\mid\tau\in\mathcal{T}\}\), and \(\mathcal{F}=\{f_{\tau}\mid\tau\in\mathcal{T}\}\), where for any \(\tau\), we define \(f_{\tau}:\mathcal{X}\mapsto\mathbb{R}^{2}\) as

\[f_{\tau}(x)=\begin{cases}(\nicefrac{{3}}{{4}},\nicefrac{{1}}{{4}})&\text{ if}\qquad\pi_{\tau}(x)=0\\ (\nicefrac{{1}}{{4}},\nicefrac{{3}}{{4}})&\text{if}\qquad\pi_{\tau}(x)=1\end{cases}.\]

Next, let \(\tau^{*}=(x_{1},a_{1}^{\prime},x_{2}^{\prime},\ldots,x_{H-1}^{\prime},a_{H-1} ^{\prime},x^{*})\) be the path from the root \(x_{1}\) to the special state \(x^{*}\in\mathcal{X}_{H}\) on the underlying binary tree. We finally define \(\pi^{*}=\pi_{\tau^{*}}\) and \(f^{*}=f_{\tau^{*}}\).

Lower bound.Given the MDP construction, the class \(\mathcal{F}\), \(f^{*}\) and \(\pi^{*}\) above, we now proceed to the desired lower bound for non-interactive imitation learning. First, note that \(\pi^{*}(x)=\operatorname*{argmax}_{a}(f^{*}(x)[a])\) for any \(x\in\mathcal{X}\). Furthermore, \(\mathtt{Margin}(f^{*}((x))=\frac{1}{2}|f(x)[0]-f(x)[1]|=\frac{1}{4}\) for all \(x\in\mathcal{X}\). Thus, for any \(\varepsilon\leq\frac{1}{4}\), \(T_{\varepsilon,h}=0\).

Next, for any policy \(\pi\), note that \(V^{\pi}=\frac{1}{2}+\frac{1}{4}\mathbf{1}\{\pi=\pi^{*}\}\). Thus, \(\pi^{*}\) is the unique \(1/8\)-suboptimal policy. Additionally, consider a noisy expert that draws its label according to (2) with the link function \(\phi(z)=z\), i.e. on the state \(x\), the expert draws its label from \(a\sim f^{*}(x)\). Now, suppose that the learner is given a dataset \(\mathcal{D}\) of \(m\) many trajectories drawn this noisy expert. There are two scenarios: either \(\mathcal{D}\) does not contain \(\tau^{*}\), or \(\mathcal{D}\) contains the trajectory \(\tau^{*}\).

* In the first case, the learner is restricted to finding \(\pi^{*}\) by eliminating all other \(\pi\neq\pi^{*}\) using the observations \(\mathcal{D}\). Since, \(|\Pi|=2^{H}\) and each policy in the class is associated with a different path on the tree, we must have that \(m=O(2^{H})\).

* In the second case, we need \(\tau^{*}\in\mathcal{D}\). However, note that probability of observing the trajectory \(\tau^{*}\) when following the actions proposed by the noisy expert is \(\Pr(\tau^{*}\mid a_{h}\sim f^{*}(x_{h}))=(3/4)^{H}\). Thus, in order to observe \(\tau^{*}\) with probability at least \(3/4\) in the dataset \(\mathcal{D}\), we need \(m=O((4/3)^{H})\).

In both the scenarios above, we need to collect exponentially many samples.

### Proof of Theorem 4

Before delving into the proof, we recall the relevant notation. In Algorithm 2, for any \(h\leq H\),

* The label \(y_{t,h}\sim\phi(f^{*}(x_{t,h}))\),, where \(\phi\) denotes the link-function given in (2).
* The function \(\texttt{SelectAction}(f_{t,h}(x_{t,h}))=\operatorname*{argmax}_{k}\phi(f_{t,h}( x_{t,h}))[k]\).
* For any vector \(v\in\mathbb{R}^{K}\), the margin is given by the gap between the value at the largest and the second largest coordinate (under the link function \(\phi\)), i.e. \[\texttt{Margin}(v)=\phi(v)[k^{*}]-\max_{k\neq k^{*}}\phi(v)[k],\] where \(k^{*}\in\operatorname*{argmax}_{k\in[K]}\phi(v)[k]\).
* We define \(T_{\varepsilon}=\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbf{1}\{\texttt{Margin}(f_{ h}^{*}(x_{t,h}))\leq\varepsilon\}\) to denote the number of samples within \(T\) rounds of interaction for which the margin w.r.t. \(f_{h}^{*}\) is smaller than \(\varepsilon\).
* The trajectory at round \(t\) is generated using the dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\) to determine the states that the learner observes, starting from the state \(x_{1}\).
* At round \(t\), the learner collects data using the policy \(\pi_{t}\) such that at time \(h\), and state \(x\), the action \(\pi_{t}(x)=\texttt{SelectAction}(f_{t,h}(x_{t,h}))\).
* For any policy \(\pi\), let \(\tau_{t}^{\pi}\) denote the (counterfactual) trajectory that one would obtain by running \(\pi\) on the deterministic dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\) with the start state \(x_{t,1}\), i.e. \[\tau_{t}^{\pi}=\big{\{}x_{t,1}^{\pi},\pi(x_{t,1}^{\pi}),\ldots,x_{t,H}^{\pi}, \pi(x_{t,H}^{\pi})\big{\}}\] (43) where \(x_{t,1}^{\pi}=x_{t,1}\) and \(x_{t,h+1}^{\pi}=\mathbb{T}_{t,h}(x_{t,h}^{\pi},\pi(x_{t,h}^{\pi}))\).
* For a trajectory \(\tau=\{x_{1},a_{1},\ldots,x_{H},a_{H}\}\), we define the total return \[R(\tau)=\sum_{h=1}^{H}r(x_{h},a_{h}).\] (44)
* Additionally, for any policy \(\pi\) and dynamics \(\{\mathbb{T}_{h}\}_{h\leq H}\), we define the trajectory obtained by running the policy \(\pi\) as \[\tau^{\pi}=\{x_{1}^{\pi},\pi_{1}(x_{1}^{\pi}),x_{2}^{\pi},\ldots\}.\] (45)
* We define the function \(\operatorname*{Gap}:\mathbb{R}^{K}\times[K]\mapsto\mathbb{R}^{+}\) as \[\operatorname*{Gap}(v,k)=\max_{k^{\prime}}\phi(v)[k^{\prime}]-\phi(v)[k],\] to denote the gap between the largest and the \(k\)-th coordinate of \(v\), and note that \(\texttt{Margin}(v)\leq\operatorname*{Gap}(v,k)\) for all \(k\neq k^{\star}\) (due to Lemma 9).

#### f.3.1 Supporting Technical Results

We first define a useful technical lemma which allows us to bound the gap between the total returns for policies \(\pi_{1}\) and \(\pi_{2}\), under the dynamics \(\{\mathbb{T}_{h}\}_{h\leq H}\). Recall that for a policy \(\pi\), we define the trajectory \(\tau^{\pi}\) under \(\{\mathbb{T}_{h}\}_{h\leq H}\) and the start state \(x_{1}\) as the trajectory \(\{x_{1}^{\pi},\pi(x_{1}^{\pi}),\ldots,x_{H}^{\pi},\pi(x_{H}^{\pi})\}\) where \(x_{1}^{\pi}=x_{1}\), and \(x_{H+1}^{\pi}\leftarrow\mathbb{T}_{h}(x_{h}^{\pi},\pi(x_{h}^{\pi}))\).

**Lemma 18**.: _Let \(\{\mathbb{T}_{h}\}_{h\leq H}\) be a deterministic dynamics, and let \(x_{1}\) be the start state. Let \(\pi_{1}\) and \(\pi_{2}\) be any two deterministic policies, and let \(\tau^{\pi_{1}}=\{x_{1}^{\pi_{1}},\pi_{1}(x_{1}^{\pi_{1}}),x_{2}^{\pi_{1}},\ldots\}\) and \(\tau^{\pi_{2}}=\{x_{1}^{\pi_{2}},\pi_{1}(x_{1}^{\pi_{2}}),x_{2}^{\pi_{2}},\ldots\}\) be two trajectories drawn using \(\pi_{1}\) and \(\pi_{2}\) on \(\{\mathbb{T}_{h}\}_{h\leq H}\) with start state \(x_{1}\). Then, for any set \(\mathds{X}\in\mathcal{X}\), the total trajectory rewards satisfy_

\[R(\tau^{\pi_{1}})-R(\tau^{\pi_{2}})\leq 2H\sum_{h=1}^{H}\mathbf{1}\{x_{h}^{\pi_{1}} \in\mathds{X}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{2}(x_{h}^{\pi_{2}})\neq\pi_{1 }(x_{h}^{\pi_{2}}),x_{h}^{\pi_{2}}\notin\mathds{X}\}.\]Proof.: Let \(\mathfrak{h}\leq H\) denote the first timestep at which the policies \(\pi_{1}\) and \(\pi_{2}\) choose different actions under \(\{\mathbb{T}_{h}\}_{h\leq H}\). Since the trajectories \(\tau^{\pi_{1}}=\{x_{1}^{\pi_{1}},\pi_{1}(x_{1}^{\pi_{1}}),x_{2}^{\pi_{1}},\ldots\}\) and \(\tau^{\pi_{2}}=\{x_{1}^{\pi_{2}},\pi_{1}(x_{1}^{\pi_{2}}),x_{2}^{\pi_{2}},\ldots\}\) are obtained by evolving through (the deterministic dynamics) \(\{\mathbb{T}_{h}\}_{h\leq H}\) using policies \(\pi_{1}\) and \(\pi_{2}\) respectively, and with the same state state \(x_{1}\), we have that

\[x_{h}^{\pi_{1}}=x_{h}^{\pi_{2}}\qquad\qquad\text{for all $h\leq\mathfrak{h}$},\]

and

\[\pi_{1}(x_{h}^{\pi_{1}})=\pi_{2}(x_{h}^{\pi_{2}})\qquad\qquad\text{ for all $h\leq\mathfrak{h}-1$}.\] (46)

Starting from the definition of the cumulative reward \(R(\cdot)\), we have that

\[R(\tau^{\pi_{1}})-R(\tau^{\pi_{2}}) =\sum_{h=1}^{H}\bigl{(}r(x_{h}^{\pi_{1}},\pi_{1}(x_{h}^{\pi_{1}}) )-r(x_{h}^{\pi_{2}},\pi_{2}(x_{h}^{\pi_{2}}))\bigr{)}\] \[=\sum_{h=1}^{\mathfrak{h}-1}\bigl{(}r(x_{h}^{\pi_{1}},\pi_{1}(x_{ h}^{\pi_{1}}))-r(x_{h}^{\pi_{2}},\pi_{2}(x_{h}^{\pi_{2}}))\bigr{)}+\sum_{h= \mathfrak{h}}^{H}\bigl{(}r(x_{h}^{\pi_{1}},\pi_{1}(x_{h}^{\pi_{1}}))-r(x_{h}^ {\pi_{2}},\pi_{2}(x_{h}^{\pi_{2}}))\bigr{)}\] \[=\sum_{h=\mathfrak{h}}^{H}\bigl{(}r(x_{h}^{\pi_{1}},\pi_{1}(x_{h}^ {\pi_{1}}))-r(x_{h}^{\pi_{2}},\pi_{2}(x_{h}^{\pi_{2}}))\bigr{)},\]

where the last line uses the fact that the trajectories (and thus the rewards) \(\tau^{\pi_{1}}\) and \(\tau^{\pi_{2}}\) are identical for the first \(\mathfrak{h}-1\) states and actions (see (46)). Since (46) also implies that \(x_{\mathfrak{h}}^{\pi_{1}}=x_{\mathfrak{h}}^{\pi_{2}}\), for the ease of notation we define \(x_{\mathfrak{h}}=x_{\mathfrak{h}}^{\pi_{1}}=x_{\mathfrak{h}}^{\pi_{2}}\). Using the fact that \(|r(x,a)|\leq 1\) and that \(\pi_{1}(x_{\mathfrak{h}})\neq\pi_{2}(x_{\mathfrak{h}})\) (by definition of \(\mathfrak{h}\)), we can bound the above as

\[R(\tau^{\pi_{1}})-R(\tau^{\pi_{2}}) \leq 2(H-\mathfrak{h}+1)\mathbf{1}\{\pi_{1}(x_{\mathfrak{h}})\neq \pi_{2}(x_{\mathfrak{h}})\}\] \[\leq 2H\mathbf{1}\{\pi_{1}(x_{\mathfrak{h}})\neq\pi_{2}(x_{ \mathfrak{h}})\}\] \[=2H\mathbf{1}\{\pi_{1}(x_{\mathfrak{h}})\neq\pi_{2}(x_{ \mathfrak{h}}),x_{\mathfrak{h}}\in\mathbf{X}\}+2H\mathbf{1}\{\pi_{1}(x_{ \mathfrak{h}})\neq\pi_{2}(x_{\mathfrak{h}}),x_{\mathfrak{h}}\notin\mathbf{X}\}\] \[\leq 2H\mathbf{1}\{x_{\mathfrak{h}}\in\mathbf{X}\}+2H\mathbf{1} \{\pi_{1}(x_{\mathfrak{h}})\neq\pi_{2}(x_{\mathfrak{h}}),x_{\mathfrak{h}}\notin \mathbf{X}\}\] \[=2H\mathbf{1}\{x_{\mathfrak{h}}^{\pi_{1}}\in\mathbf{X}\}+2H \mathbf{1}\{\pi_{2}(x_{\mathfrak{h}}^{\pi_{2}})\neq\pi_{1}(x_{\mathfrak{h}}^{ \pi_{2}}),x_{\mathfrak{h}}^{\pi_{2}}\notin\mathbf{X}\}\] \[\leq 2H\sum_{h=1}^{H}\mathbf{1}\{x_{h}^{\pi_{1}}\in\mathbf{X}\}+2H \sum_{h=1}^{H}\mathbf{1}\{\pi_{2}(x_{h}^{\pi_{2}})\neq\pi_{1}(x_{h}^{\pi_{2}}),x_{h}^{\pi_{2}}\notin\mathbf{X}\},\]

where the equality in second last line plugs in the fact that \(x_{\mathfrak{h}}=x_{\mathfrak{h}}^{\pi_{1}}=x_{\mathfrak{h}}^{\pi_{2}}\), and the last inequality is a straightforward upper bound. 

We will also be using Lemma 9 and Lemma 13 from Appendix E.3 for bounding the Margin in the regret bound proofs. Finally, we note the following properties of the function \(f_{h}^{*}\).

**Lemma 19**.: _With probability at least \(1-\delta\), the function \(f_{h}^{*}\) satisfies for any \(h\leq H\) and \(t\leq T\),_

* \(\sum_{s=1}^{t-1}Z_{s,h}\|f_{h}^{*}(x_{s,h})-f_{s,h}(x_{s,h})\|^{2}\leq\Psi_{ \delta}^{\ell_{\phi}}(\mathcal{F}_{h},T),\)__
* \(\|f_{h}^{*}(x_{t,h})-f_{t,h}(x_{t,h})\|\leq\Delta_{t,h}(x_{t,h}),\)__

_where \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h},T)=\frac{4}{\lambda}\mathrm{Reg}^{ \ell_{\phi}}(\mathcal{F}_{h};T)+\frac{112}{\lambda^{2}}\log(4H\log^{2}(T)/\delta)\)._

Proof.:
* We first note that we do not query oracle when \(Z_{s,h}=0\), and thus we can ignore the time steps for which \(Z_{s,h}=0\). Hence, for each \(h\in[H]\), applying Lemma 5 along with the fact that \(\sup_{x,f\in\mathcal{F}_{h}}|f(x)|\leq 1\) yields \[\sum_{s=1}^{t-1}Z_{s,h}\|f_{h}^{*}(x_{s,h})-f_{s,h}(x_{s,h})\|^{2}\leq\frac{4}{ \lambda}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F}_{h};T)+\frac{112}{\lambda^{2}} \log(4\log^{2}(T)/\delta)\] for all \(t\leq T\). Then, we take the union bound for all \(h\in[H]\), which completes the proof.
* The second part follows from using the observation in part-(a) that \(f_{h}^{*}\) satisfies the constraint in the definition of \(\Delta_{t,h}\) given in (6), and thus \(\left\|f_{h}^{*}(x_{t,h})-f_{t,h}(x_{t,h})\right\|\leq\Delta_{t,h}(x_{t,h})\).

The next technical lemma bounds the number of times when \(\Delta_{t,h}(x_{t,h})\geq\zeta\) and we query the expert. Note that Lemma 20 holds even if the sequence \(\{x_{t,h}\}_{t\leq T}\) was adversarially generated.

**Lemma 20**.: _Let \(f^{\star}\) satisfy Lemma 19, and let \(\Delta_{t,h}(x_{t})\) be defined in (6). Suppose we run Algorithm 2 on data sequence \(\{(x_{t,h})_{h\leq H}\}_{t\leq T}\), and let \(Z_{t,h}\) be as defined in line 9. Then, for any \(\zeta>0\), with probability at least \(1-H\delta\), for any \(h\leq H\),_

\[\sum_{t=1}^{T}Z_{t,h}\mathbf{1}\{\Delta_{t,h}(x_{t,h})\geq\zeta\}\leq\frac{20 \Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h},T)}{\zeta^{2}}\cdot\mathfrak{E}( \mathcal{F}_{h},\frac{\zeta}{2};f_{h}^{\star}),\]

_where \(\mathfrak{E}\) denotes the eluder dimension is given in Definition 1._

Proof.: The proof is identical to the proof of Lemma 11 by replacing all \(|\cdot|\) with \(\|\cdot|\), and substitute the corresponding bounds for \(f_{h}^{\star}\) via Lemma 19 (instead of using Lemma 10). We skip the proof for conciseness. 

#### f.3.2 Regret Bound

Recall that the trajectory at round \(t\) is generated using the dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\). Define the policy \(\pi_{t}\) and \(\pi^{\star}\) such that for any \(h\leq H\) and \(x\in\mathcal{X}_{h}\),

\[\pi_{t}(x_{h})=\texttt{SelectAction}(f_{t,h}(x_{h})),\quad\text{and,}\quad\pi^ {\star}(x_{h})=\texttt{SelectAction}(f_{h}^{\star}(x_{h})).\] (47)

Furthermore, for any policy \(\pi\), let \(\tau_{t}^{\pi}\) denote the trajectory that one would obtain by running \(\pi\) on the deterministic dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\) with the start state \(x_{t,1}\), i.e.

\[\tau_{t}^{\pi}=\big{\{}x_{t,1}^{\pi},\pi(x_{t,1}^{\pi}),\ldots,x_{t,H}^{\pi}, \pi(x_{t,H}^{\pi})\big{\}}\] (48)

where \(x_{t,1}^{\pi}=x_{t,1}\) and \(x_{t,h+1}^{\pi}=\mathbb{T}_{t,h}(x_{t,h}^{\pi},\pi(x_{t,h}^{\pi}))\). Note that Algorithm 2 collects trajectories using the policy \(\pi_{t}\) at round \(t\). Thus, we have that

\[x_{t,h}^{\pi_{t}}=x_{t,h},\] (49)

where \(x_{t,h}\) denotes the state at time step \(h\) in round \(t\) of Algorithm 2. Finally, let \(\varepsilon>0\) be a free parameter. We now have all the notation to proceed to the proof on our regret bound.

**Step 1: Bounding the difference in return at round \(t\).** Fix any \(t\leq T\), and let \(\tau_{t}^{\pi_{t}}\) and \(\tau_{t}^{\pi^{\star}}\) denote the trajectories that would have been sampled using the policies \(\pi_{t}\) and the policy \(\pi^{\star}\) at round \(t\). Furthermore, define the set \(\mathtt{X}_{\varepsilon}\) as

\[\mathtt{X}_{\varepsilon}:=\bigcup_{h=1}^{H}\{x\in\mathcal{X}_{h}\mid\texttt{ Margin}(f_{h}^{\star}(x))\leq\varepsilon\}\] (50)

Using Lemma 18 for the policies \(\pi_{t}\) and \(\pi^{\star}\), and the set \(\mathtt{X}_{\varepsilon}\) defined above, we get that9

Footnote 9: The key advantage of using Lemma 18 is that the first term \(\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathtt{X}_{\varepsilon}\}\) accounts for the number steps at which a counterfactual trajectory sampled using \(\pi^{\star}\) goes to the state space with margin less than \(\varepsilon\). Thus, we only pay for the number of times when the comparator policy \(\pi^{\star}\) would go to states with \(\varepsilon\)-margin (instead of when \(\pi_{t}\) does to such states).

\[R(\tau_{t}^{\pi^{\star}})-R(\tau_{t}^{\pi_{t}}) \leq 2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathtt{X} _{\varepsilon}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{t}(x_{t,h}^{\pi_{t}})\neq\pi^ {\star}(x_{t,h}^{\pi_{t}}),x_{t,h}^{\pi_{t}}\notin\mathtt{X}_{\varepsilon}\}\] (51) \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathtt{X} _{\varepsilon}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star}(x _{t,h}),x_{t,h}\notin\mathtt{X}_{\varepsilon}\}\] \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathtt{X} _{\varepsilon}\}+2H\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{ \star}(x_{t,h}),x_{t,h}\notin\mathtt{X}_{\varepsilon}\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2H \sum_{h=1}^{H}\bar{Z}_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})+\pi^{\star}(x_{t,h}),x _{t,h}\notin\mathtt{X}_{\varepsilon}\}\] \[\leq 2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathtt{X} _{\varepsilon}\}+2H\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{ \star}(x_{t,h}),x_{t,h}\notin\mathtt{X}_{\varepsilon}\}\]\[+2H\sum_{h=1}^{H}\bar{Z}_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{ \star}(x_{t,h})\}\] \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathbb{X}_{ \varepsilon}\}+2H\mathsf{T}_{A}+2H\mathsf{T}_{B},\] (52)

where the second line is obtained by plugging in (49) and the last line simply defines \(\mathsf{T}_{A}\) and \(\mathsf{T}_{B}\) to be the second and the third terms in the previous line without the \(2H\) multiplicative factor.

We bound \(\mathsf{T}_{A}\) and \(\mathsf{T}_{B}\) separately below.

* _Bound on term \(\mathsf{T}_{A}\)._ Using the definition of \(\mathbb{X}_{\varepsilon}\) from (50), we note that \[\mathsf{T}_{A} =\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star} (x_{t,h}),x_{t,h}\notin\mathbb{X}_{\varepsilon}\}\] \[=\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star} (x_{t,h}),\mathsf{Margin}(f_{h}^{\star}(x_{t,h}))>\varepsilon\}\] \[\leq\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi^{\star}(x_{t,h})+\pi_{t }(x_{t,h}),\phi(f_{h}^{\star}(x_{t,h}))[\pi^{\star}(x_{t,h})]-\phi(f_{h}^{ \star}(x_{t,h}))[\pi_{t}(x_{t,h})]\geq\varepsilon\}\] \[\leq\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\phi(f_{h}^{\star}(x_{t,h})) [\pi^{\star}(x_{t,h})]-\phi(f_{h}^{\star}(x_{t,h}))[\pi_{t}(x_{t,h})]\geq \varepsilon\},\] where in the second last line we used the definition of \(\mathsf{Margin}(f_{h}^{\star}(x_{t,h}))\) along with the fact that \(\pi^{\star}(x_{t,h})\neq\pi_{t}(x_{t,h})\). Using the relation in Lemma 12 for the term inside the indicator, we can further bound the above as \[\mathsf{T}_{A} \leq\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{2\gamma\|f_{h}^{\star}(x_{t, h})-f_{t,h}(x_{t,h})\|\geq\varepsilon\}\] \[\leq\frac{4\gamma^{2}}{\varepsilon^{2}}\sum_{h=1}^{H}Z_{t,h}\|f_{h}^ {\star}(x_{t,h})-f_{t,h}(x_{t,h})\|^{2},\] where in the second inequality we used: \(\mathbf{1}\{a\geq b\}\leq\nicefrac{{a^{2}}}{{b^{2}}}\) for any \(a,b\geq 0\).
* _Bound on term \(\mathsf{T}_{B}\)._ Before delving into the proof, first note that Lemma 19-\((b)\) implies that \[\|f_{h}^{\star}(x_{t,h})-f_{t,h}(x_{t,h})\|\leq\Delta_{t,h}(x_{t,h}).\] (53) Next, note that \[\mathsf{T}_{B} =\sum_{h=1}^{H}\bar{Z}_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{ \star}(x_{t,h})\}\] \[=\sum_{h=1}^{H}\mathbf{1}\{\mathsf{Margin}(f_{t,h}(x_{t,h}))>2 \gamma\Delta_{t,h}(x_{t,h}),\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h})\},\] where in the last line we just plugged in the query condition under which \(Z_{t,h}=0\). However note that the above two conditions inside the indicator imply that \[2\gamma\Delta_{t,h}(x_{t,h}) <\mathsf{Margin}(f_{t,h}(x_{t,h}))\] \[\leq\phi(f_{t,h}(x_{t,h}))[\pi_{t}(x_{t,h})]-\phi(f_{t,h}(x_{t,h}) )[\pi^{\star}(x_{t,h})]\] \[\leq 2\gamma\|f_{t,h}(x_{t,h})-f_{h}^{\star}(x_{t,h})\|,\] where the second line uses the definition of \(\mathsf{Margin}(\cdot)\) and the fact that \(\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h})\), the last line is due to Lemma 12. Thus, \[\mathsf{T}_{B}\leq\sum_{h=1}^{H}\mathbf{1}\{\|f_{t,h}(x_{t,h})-f_{h}^{\star}(x_ {t,h})\|>\Delta_{t,h}(x_{t,h})\},\] but the conditions inside the indicator in the above contradicts (53) (which holds with probability \(1-\delta\)). Thus, with probability at least \(1-\delta\), \[\mathsf{T}_{B}=0.\] (54)

[MISSING_PAGE_FAIL:47]

\[\leq\sum_{h=1}^{h_{t}}Z_{t,h_{t}}\mathbf{1}\{\pi^{\star}(x_{t,h}) \neq\pi_{t}(x_{t,h})\}\] \[\leq\sum_{h=1}^{h_{t}}\bar{Z}_{t,h}\mathbf{1}\{\pi^{\star}(x_{t,h} )\neq\pi_{t}(x_{t,h})\}\]

where in second inequality above, we used the fact that \(Z_{t,h}=0\) (and thus \(\bar{Z}_{t,h}=1\)) for all \(h\leq h_{t}\), by the definition of \(h_{t}\). However note that the right hand side in the last inequality is equivalent to the term \(\mathsf{T}_{B}\) defined above (where sum is now till \(h_{t}\) instead of \(H\)). Thus, using the bound in (54) in the above, we immediately get that

\[Z_{t,h_{t}}\mathbf{1}\{x_{t,h_{t}}\in\mathsf{X}_{\varepsilon}\}\mathbf{1}\{ \exists h<h_{t}:\pi^{\star}(x_{t,h})\neq\pi_{t}(x_{t,h})\}=0.\]

For the first term in (58), using the condition that \(\pi^{\star}(x_{t,h})=\pi_{t}(x_{t,h})\) for all \(h\leq h_{t}\), we get that \(x_{t,h}=x_{t,h}^{\pi^{\star}}\) and thus

\[HZ_{t,h_{t}}\mathbf{1}\{x_{t,h_{t}}\in\mathsf{X}_{\varepsilon} \}\mathbf{1}\{\forall h\leq h_{t}:\pi^{\star}(x_{t,h})=\pi_{t}(x_{t,h})\} \leq HZ_{t,h}\mathbf{1}\{x_{t,h_{t}}^{\pi^{\star}}\in\mathsf{X}_{ \varepsilon}\}\] \[\leq H\mathbf{1}\{x_{t,h_{t}}^{\pi^{\star}}\in\mathsf{X}_{ \varepsilon}\}\] \[\leq H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathsf{ X}_{\varepsilon}\}.\]

Gathering the two terms above, and plugging in the definition of \(T_{\varepsilon,h}\), we get that

\[\mathsf{T}_{C}\leq H\sum_{h=1}^{H}\sum_{t=1}^{T}\mathbf{1}\{x_{t,h}^{\pi^{ \star}}\in\mathsf{X}_{\varepsilon}\}=H\sum_{h=1}^{H}T_{\varepsilon,h}.\]

* _Bound on_ \(\mathsf{T}_{D}\)_. Using the definition of the set \(\mathsf{X}_{\varepsilon}\) and \(Z_{t,h}\), we note that \[(\mathsf{T}_{D})_{t} =\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{x_{t,h}\notin\mathsf{X}_{ \varepsilon},\Delta_{t,h}(x_{t,h})\leq\frac{\varepsilon}{4\gamma}\}\] \[=\sum_{h=1}^{H}\mathbf{1}\{\mathsf{Margin}(f_{t,h}(x_{t,h}))\leq 2 \gamma\Delta_{t,h}(x_{t,h}),\mathsf{Margin}(f_{h}^{\star}(x_{t,h}))> \varepsilon,\Delta_{t,h}(x_{t,h})\leq\frac{\varepsilon}{4\gamma}\}\] (59) Recall that Lemma 19 implies that with probability at least \(1-\delta\), \[\|f_{h}^{\star}(x_{t,h})-f_{t,h}(x_{t,h})\|\leq\Delta_{t,h}(x_{t,h}),\] using which with Lemma 13 implies that \[\mathsf{Margin}(f_{h}^{\star}(x_{t,h})) \leq\mathsf{Margin}(f_{t,h}(x_{t,h}))+2\gamma\|f_{h}^{\star}(x_{ t,h})-f_{t,h}(x_{t,h})\|\] \[\leq\mathsf{Margin}(f_{t,h}(x_{t,h}))+2\gamma\Delta_{t,h}(x_{t,h}).\] Using the above bound with the conditions in (59) implies that \[(\mathsf{T}_{D})_{t} =\sum_{h=1}^{H}\mathbf{1}\{\mathsf{Margin}(f_{h}^{\star}(x_{t,h}) )\leq 4\gamma\Delta_{t,h}(x_{t,h}),\mathsf{Margin}(f_{h}^{\star}(x_{t,h}))> \varepsilon,\Delta_{t,h}(x_{t,h})\leq\frac{\varepsilon}{4\gamma}\}\] \[=\sum_{h=1}^{H}\mathbf{1}\{\mathsf{Margin}(f_{h}^{\star}(x_{t,h}) )\leq\varepsilon,\mathsf{Margin}(f_{h}^{\star}(x_{t,h}))>\varepsilon\}\] \[=0,\] where the last equality holds because the two conditions in the indicator in the previous line can never occur simultaneously.
* _Bound on_ \(\mathsf{T}_{E}\)_. Note that \[\mathsf{T}_{E}=\sum_{t=1}^{T}\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{x_{t,h}\notin \mathsf{X}_{\varepsilon},\Delta_{t,h}(x_{t,h})>\frac{\varepsilon}{4\gamma}\}\]\[\leq\sum_{t=1}^{T}\mathbb{E}_{\iota_{t}}\bigg{[}2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h }^{\pi^{*}}\in\mathbf{X}_{\varepsilon}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{t}(x_{t,h}^{\pi_{t}})+\pi^{*}(x_{t,h}^{\pi_{t}}),x_{t,h}^{\pi_{t}}\notin\mathbf{X}_{ \varepsilon}\}\bigg{]},\]

where the last holds due to Lemma 18 and the set \(\mathbf{X}_{\varepsilon}\) is defined in (50). An application of Lemma 4 in the above implies that with probability at least \(1-\delta\),

\[\sum_{t=1}^{T}V^{\pi}-V^{\pi_{t}}\leq 4H\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{*}}\in\mathbf{X}_{\varepsilon}\}+4H\sum_{t=1}^{T}\sum_{h=1}^{H} \mathbf{1}\{\pi_{t}(x_{t,h}^{\pi_{t}})+\pi^{*}(x_{t,h}^{\pi_{t}}),x_{t,h}^{\pi_ {t}}\notin\mathbf{X}_{\varepsilon}\}+32H^{2}\log(2/\delta).\]

The rest of the proof is identical to the proof of Theorem 4 from (51) onwards. They query complexity can be similarly computed.

We next provide the proofs for learning from multiple experts.

### Proof of Theorem 5

```
0: Parameters \(\delta,\gamma,\lambda,T\), function classes \(\{\mathcal{F}_{h}^{m}\}_{h\leq H,m\leq M}\), online oracles \(\{\mathsf{ Oracle}_{h}^{m}\}_{h\leq H,m\leq M}\) w.r.t. \(\ell_{\phi}\).
1: Set \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T)=\frac{4}{\lambda}\mathrm{ Reg}^{\ell_{\phi}}(\mathcal{F}_{h}^{m};T)+\frac{112}{\lambda^{2}}\log(4MH\log^{2}(T)/ \delta)\).
2: Compute \(f_{1,h}^{m}=\mathsf{ Oracle}_{1,h}(\varnothing)\) for each \(h\in[H]\) and \(m\in[M]\).
3:for\(t=1\) to \(T\)do
4: Nature chooses the state \(x_{t,1}\).
5:for\(h=1\) to \(H\)do
6: Define \(F_{t,h}^{m}(x):=[f_{t,h}^{1}(x),\ldots,f_{t,h}^{M}(x)]\).
7: Learner plays \(\widehat{y}_{t,h}=\mathsf{SelectAction}(F_{t,h}(x_{t,h}))\).
8: Learner transitions to the next state in this round \(x_{t,h+1}\leftarrow\mathbb{T}_{t,h}(x_{t,h},\widehat{y}_{t,h})\).
9: For each \(m\in[M]\), learner computes \[\Delta_{t,h}^{m}(x_{t,h}):=\max_{f\in\mathcal{F}_{h}^{m}} \left\|f(x_{t,h})-f_{t,h}^{m}(x_{t,h})\right\|\] s.t. \[\sum_{s=1}^{t-1}Z_{s,h}\left\|f(x_{s,h})-f_{s,h}^{m}(x_{s,h}) \right\|^{2}\leq\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T).\] (60) and defines \(\tilde{\Delta}_{t,h}(x_{t,h})=[\Delta_{t,h}^{1}(x_{t,h}),\ldots,\Delta_{t,h}^ {M}(x_{t,h})]\).
10: Learner decides whether to query: \(Z_{t,h}=\mathsf{Query}(F_{t,h}(x_{t,h}),\tilde{\Delta}_{t,h}(x_{t,h}))\)
11:if\(Z_{t,h}=1\)then
12:for\(m=1\) to \(M\)do
13: Learner queries expert \(m\) for its label \(y_{t,h}^{m}\) for \(x_{t,h}\).
14:\(f_{t+1,h}^{m}\leftarrow\mathsf{ Oracle}_{t+1,h}^{m}(\{x_{t,h},y_{t,h}\})\)
15:else
16:\(f_{t+1,h}^{m}\gets f_{t,h}^{m}\) for each \(m\in[M]\). ```

**Algorithm 4**\(\mathsf{IntRAActiVe}\)\(\mathsf{InitialOn}\)\(\mathsf{Learning}\)\(\mathsf{VIa}\) Active Queries to \(\boldsymbol{M}\) Experts (RAVIOLI-M)

We first discuss the setup and the relevant notation. We are in the imitation learning setup introduced in Appendix F.3. In particular, the learner interacts with \(M\) experts in \(T\) episodes/rounds, each consisting of \(H\) timesteps. For any \(h\leq H\), each of the \(M\) experts have a ground truth model \(f_{h}^{*,m}\), respectively. Given the context \(x_{t,h}\) for time step \(h\) in round \(t\), the learner plays the actions \(\widehat{y}_{t,h}\in[K]\), and can additionally choose to query the experts (by setting \(Q_{t,h}=1\)) to receive noisy feedback \(y_{t,h}^{m}\sim\phi(f_{h}^{*,m}(x_{t}))\) from each expert \(m\in[M]\). After playing the chosen action \(\widehat{y}_{t,h}\), the learner then transitions to state \(x_{t,h+1}\leftarrow\mathbb{T}_{t,h}(x_{t,h},\widehat{y}_{t,h})\) where \(\{\mathbb{T}_{t,h}\}_{h\in[H]}\) is a sequence of deterministic dynamics (unknown to the learner).

In Algorithm 4, for any round \(t\leq T\) and \(h\leq H\):

* The aggregation function \(\mathscr{A}:\mathbb{R}^{K\times M}\mapsto\mathbb{R}^{K}\), known to the learner, maps the predictions of the estimated experts to distributions over actions. Some illustrative examples are given below to illustrate the generality of our setup:
* _Random aggregation:_ Given a state \(x_{h}\), the aggregation rule chooses an expert uniformly at random and returns the label \(y_{h}\) sampled from its model. In particular, \[y_{h}\sim\phi(f_{h}^{*,\widehat{m}}(x_{h})),\qquad\text{where}\qquad\widehat{m }\sim\mathrm{Uniform}([M]).\] Here, the distribution \(\mathscr{A}\big{(}\phi(f_{h}^{*,1}(x_{h})),\ldots,\phi(f_{h}^{*,M}(x_{h})) \big{)}=\frac{1}{M}\sum_{m=1}^{M}\phi(f^{*,m}(x_{h}))\).
* _Majority label:_\(\mathscr{A}\) is deterministic. Given a state \(x_{t}\), the aggregation rule chooses the label \(y_{h}\in[K]\) which is the top preference for the majority of the experts. In particular, \[y_{h}=\mathscr{A}\big{(}\phi(f_{h}^{*,1}(x_{h})),\ldots,\phi(f_{h}^{*,M}(x_{h}) )\big{)}=\operatorname*{argmax}_{k\in[K]}\sum_{m=1}^{M}\mathbf{1}\{k= \operatorname*{argmax}_{\overline{k}\in[K]}\phi(f_{h}^{*,m}(x_{h})[\widetilde{ k}])\}.\]
* _Majority-of-confident-experts:_ This aggregation rule is also deterministic, and was first introduced in Dekel et al. (2012). Given a state \(x_{t}\), the aggregation rule chooses the label \(y_{h}\in[K]\) which is the top preference for the majority of the \(\rho\)_-confident_ experts on \(x_{h}\) i.e. the experts whose margin on \(x_{h}\) is larger than \(\rho\). In particular, \[y_{h} =\mathscr{A}\big{(}\phi(f_{h}^{\star,1}(x_{h})),\ldots,\phi(f_{h} ^{\star,M}(x_{h}))\big{)}\] \[=\operatorname*{argmax}_{k\in[K]}\sum_{m=1}^{M}\mathbf{1}\{k= \operatorname*{argmax}_{\mathbb{E}\in[K]}\phi(f_{h}^{\star,m}(x_{h})[ \widetilde{k}])\text{ and }\mathtt{Margin}(\phi(f_{h}^{\star,m}(x_{h}))>\rho)\},\] where \(\mathtt{Margin}(f_{h}^{\star,m}(x_{h})>\rho)=\max_{k_{1}}\bigl{(}\phi(f_{h}^{ \star,m}(x_{h}))[k_{1}]-\bigl{(}\max_{k_{2}\neq k_{1}}\phi(f_{h}^{\star,m}(x_{h} ))[k_{2}]\bigr{)}\bigr{)}\). This aggregation rule is useful when there may be many experts that give equal weights to the top and the second-to-top coordinates w.r.t. their respective models, and hence can not be confidently accounted for in the majority rule. Furthermore, instead of choosing the majority label, similar to Dekel et al. (2012), one can also return the label sampled according to a uniform distribution over \(\rho\)-confident experts.
* The function \(\mathtt{SelectAction}:\mathbb{R}^{K\times M}\mapsto[K]\) chooses the action to play at round \(t\), and is defined as: \[\mathtt{SelectAction}(F_{t,h}(x_{t,h}))=\operatorname*{argmax}_{k}\mathscr{A} \big{(}\phi(F_{t,h}(x_{t,h}))\big{)}[k],\] (61) where \(F_{t,h}(x_{t,h}))=[f_{t,h}^{1}(x_{t,h})),\ldots,f_{t,h}^{M}(x_{t,h}))]\), and \(\phi\) denotes the link-function given in (2).
* Our goal in Algorithm 4 is to compete with the policy \(\pi^{\star}\) defined such that for any \(x\in\mathcal{X}_{h}\), \[\pi^{\star}(x)=\mathtt{SelectAction}(F_{h}^{\star}(x))\] (62) where \(F_{h}^{\star}(x)=[f_{h}^{\star,1}(x),\ldots,f_{h}^{\star,1}(x)]\in\mathbb{R}^{ K\times M}\).
* Given the context \(x_{t,h}\) and the function \(F_{t,h}:\mathcal{X}\mapsto\mathbb{R}^{K\times M}\), the learner decided whether to query via \(Z_{t,h}=\mathtt{Query}(F_{t,h}(x_{t,h}),\widetilde{\Delta}_{t,h}(x_{t,h}))\) where we define the function \(\mathtt{Query}:\mathbb{R}^{K\times M}\times\mathbb{R}^{M}\) as \[\mathtt{Query}(U;\bar{\varepsilon}):=\sup_{V\in\mathbb{R}^{K \times M}}\mathbf{1}\{\mathtt{SelectAction}(U)\neq\mathtt{SelectAction}(V)\}\] s.t. \[\|U[;m]-V[;m]\|_{2}\leq\bar{\varepsilon}[m]\qquad\forall m\leq M.\] (63)

At round \(t\), the learner interactions with transition dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\) and collects data. Without loss of generality, we assume that the learner always starts from the state \(x_{t,1}\). We next recall the interaction at round \(t\):

* The learner collects data using the policy \(\pi_{t}\), defined such that \[\pi_{t}(x)=\mathtt{SelectAction}(f_{t,h}(x_{t,h})).\] for any \(h\leq H\), and state \(x\in\mathcal{X}_{h}\).
* For any policy \(\pi\), we use the notation \(\tau_{t}^{\pi}\) to denote the (counterfactual) trajectory that would have been generated by running \(\pi\) on the deterministic dynamics \(\{\mathbb{T}_{t,h}\}_{h\leq H}\) with the start state \(x_{t,1}\), i.e. \[\tau_{t}^{\pi}=\bigl{\{}x_{t,1}^{\pi},\pi(x_{t,1}^{\pi}),\ldots,x_{t,H}^{\pi}, \pi(x_{t,H}^{\pi})\bigr{\}},\] (64) where \(x_{t,1}^{\pi}=x_{t,1}\) and \(x_{t,h+1}^{\pi}=\mathbb{T}_{t,h}(x_{t,h}^{\pi},\pi(x_{t,h}^{\pi}))\).
* For any trajectory \(\tau=\{x_{1},a_{1},\ldots,x_{H},a_{H}\}\), we define the total return \[R(\tau)=\sum_{h=1}^{H}r(x_{h},a_{h}).\] (65)The goal of the learner is to minimize its regret which is given by

\[\mathrm{Reg}_{T}=\sum_{t=1}^{T}R(\tau_{t}^{\pi^{*}})-\sum_{t=1}^{T}R(\tau_{t}^{ \pi_{t}}).\] (66)

Finally, our bounds depend on the notation of margin defined w.r.t. the function Query defined above. In particular, for a sequence of contexts \(\{\{T_{t,h}\}_{h\leq H}\}_{t\leq T}\), we define \(T_{\varepsilon,h}\) as

\[T_{\varepsilon,h}=\sum_{t=1}^{T}\mathbf{1}\{\texttt{Query}(F_{h}^{*}(x_{t,h}^ {\pi^{*}}),\varepsilon\bar{\texttt{I}})=1\}.\] (67)

In the above we count for the number of time steps when the counterfactual states \(\{x_{t,h}^{\pi^{*}}\}_{t\leq T}\), reached under the given dynamics if we had executed \(\pi^{*}\), are within the \(\varepsilon\)-margin region. Note that even though the observed states \(\{\{x_{t,h}\}_{h\leq H}\}_{t\leq T}\) are such that \(\sum_{t=1}^{T}\mathbf{1}\{\texttt{Query}(F_{h}^{*}(x_{t,h}),\varepsilon\bar{ \texttt{I}})=1\}\) is large, we would not pay for this in our margin term \(T_{\varepsilon,h}\).

#### f.5.1 Supporting Technical Results

**Lemma 21**.: _With probability at least \(1-\delta\), for any \(m\leq M\), and \(t\leq T\) and \(h\leq H\), the function \(f^{\star,m}\) satisfies_

* \(\sum_{s=1}^{t-1}Z_{s,h}\|f_{h}^{\star,m}(x_{s,h})-f_{s,h}^{m}(x_{s,h})\|^{2} \leq\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T)\) _,_
* \(\|f_{h}^{\star,m}(x_{t,h})-f_{t,h}^{m}(x_{t,h})\|\leq\Delta_{t,h}^{m}(x_{t,h})\)_,_

_where \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T)=\frac{4}{\lambda}\mathrm{ Reg}^{\ell_{\phi}}(\mathcal{F}_{h}^{m};T)+\frac{112}{\lambda^{2}}\log(4MH \log^{2}(T)/\delta)\)._

Proof.:
* We first note that we do not query oracle when \(Z_{s,h}=0\), and thus we can ignore the time steps for which \(Z_{s,h}=0\). Hence, for each \(h\in[H]\) and \(m\in[M]\), applying Lemma 5 yields \[\sum_{s=1}^{t-1}Z_{s,h}\|f_{h}^{\star,m}(x_{s,h})-f_{s,h}(x_{s,h})\|^{2}\leq \frac{4}{\lambda}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F}_{h}^{m};T)+\frac{112}{ \lambda^{2}}\log(4\log^{2}(T)/\delta)\] for all \(t\leq T\). Then, we take the union bound for all \(h\in[H]\) and \(m\in[M]\), which completes the proof.
* The second part follows from using part-(a) along with the definition in (60).

The next lemma bound the number of times when \(\Delta_{t,h}^{m}(x_{t,h})\geq\zeta\), and we query. Note that Lemma 22 holds even if the sequence \(\{x_{t,h}\}_{t\leq T}\) was adversarially generated.

**Lemma 22**.: _Let \(f^{\star,m}\) satisfy Lemma 21, and let \(\Delta_{t,h}^{m}(x_{t,h})\) be defined in Algorithm 4. Suppose Algorithm 4 is run on the data sequence \(\{x_{t,h}\}_{t\leq 1}\), and let \(Z_{t,h}\) be defined in line 10. Then, for any \(\zeta>0\), with probability at least \(1-M\delta\), for any \(m\in[M]\), and \(h\leq H\),_

\[\sum_{t=1}^{T}Z_{t,h}\mathbf{1}\{\Delta_{t,h}^{m}(x_{t})\geq\zeta\}\leq\frac{ 20\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T)}{\zeta^{2}}\cdot\texttt{ \mathfrak{E}}(\mathcal{F}_{h}^{m},\zeta\!/_{2};f_{h}^{\star,m}),\]

_where \(\mathfrak{E}\) denotes the eluder dimension given in Definition 1._

Proof.: The proof is identical to the proof of Lemma 11 where we handle each \(m\in[M]\) and \(h\in[H]\) separately, and substitute the corresponding bounds for \(f_{h}^{\star,m}\) via Lemma 21 (instead of using Lemma 10). We skip the proof for conciseness.

#### f.5.2 Regret Bound

Suppose the trajectories at round \(t\) are generated using the deterministic dynamics \(\{\mathbb{T}_{t,1},\ldots,\mathbb{T}_{t,H}\}=\mathscr{T}(\cdot\ ;\iota_{t})\) where \(\iota_{t}\) denotes the random seed that captures all of the stochasticity at round \(t\)10.

Footnote 10: We use random seed \(\iota_{t}\) to capture all the stochasticity in the choice of \(\{\mathbb{T}_{t,h}\}_{h\leq H,t\leq T}\). However, all our proofs extend to IL learning with an arbitrary, and possibly adversarial, choice of \(\{\mathbb{T}_{t,h}\}_{h\leq H,t\leq T}\)

Recall that the policies \(\pi^{\star}\) and \(\pi_{t}\) such that for any \(h\leq H\) and \(x\in\mathcal{X}_{h}\), \(\pi^{\star}(x)=\mathtt{SelectAction}(F_{h}^{\star}(x))\), and,\(\pi_{t}(x)=\mathtt{SelectAction}(F_{t,h}(x))\). Note that Algorithm 4 collects trajectories using the policy \(\pi_{t}\) at round \(t\). Thus, we have

\[x_{t,h}^{\pi_{t}}=x_{t,h},\] (68)

where \(x_{t,h}\) denotes the state at time step \(h\) in round \(t\) of Algorithm 4. Finally, let \(\varepsilon>0\) be a free parameter. We start with the bound on the regret at time \(t\).

Step 1: Bounding the difference in cumulative return at round \(t\).Fix any \(t\leq T\), and let \(\tau_{t}^{\pi_{t}}\) and \(\tau_{t}^{\pi^{\star}}\) denote the trajectories that would have been sampled using the policies \(\pi_{t}\) and the policy \(\pi^{\star}\) at round \(t\). Furthermore, define the set \(\mathsf{X}_{\varepsilon}\) as

\[\mathsf{X}_{\varepsilon}:=\bigcup_{h=1}^{H}\{x\in\mathcal{X}_{h}\mid\mathtt{ Query}(F_{h}^{\star}(x),\varepsilon\tilde{\mathsf{I}})=1\}\] (69)

Using Lemma 18 for the policies \(\pi_{t}\) and \(\pi^{\star}\), and the set \(\mathsf{X}_{\varepsilon}\) defined above, we get that

\[R(\tau_{t}^{\pi^{\star}})-R(\tau_{t}^{\pi_{t}}) \leq 2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in \mathsf{X}_{\varepsilon}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{t}(x_{t,h}^{\pi_{ t}})\neq\pi^{\star}(x_{t,h}^{\pi_{t}}),x_{t,h}^{\pi_{t}}\notin\mathsf{X}_{ \varepsilon}\}\] \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathsf{X}_{ \varepsilon}\}+2H\sum_{h=1}^{H}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h}),x_{t,h}\notin\mathsf{X}_{\varepsilon}\}\] \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathsf{X}_{ \varepsilon}\}+2H\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{ \star}(x_{t,h}),x_{t,h}\notin\mathsf{X}_{\varepsilon}\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2H\sum_{ h=1}^{H}\bar{Z}_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h}),x_{t,h} \notin\mathsf{X}_{\varepsilon}\}\] \[=2H\sum_{h=1}^{H}\mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathsf{X}_{ \varepsilon}\}+2H\mathbb{T}_{A}+2H\mathbb{T}_{B},\] (70)

where the second line is obtained by using the relation (68) in the second line. The last line simply defines \(\mathsf{T}_{A}\) and \(\mathbb{T}_{B}\) to be the second and the third term in the previous line, respectively, without the \(2H\) multiplicative factor. We bound these two terms separately below:

* _Bound on \(\mathsf{T}_{A}\)._ Using the definition of \(\mathsf{X}_{\varepsilon}\) from (69), we note that \[\mathsf{T}_{A} =\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star}( x_{t,h}),x_{t,h}\notin\mathsf{X}_{\varepsilon}\}\] \[=\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq\pi^{\star}( x_{t,h}),\mathtt{Query}(F_{h}^{\star}(x_{t,h}),\varepsilon\tilde{\mathsf{I}})=0\}\] \[=\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{\exists m\in[m]\ :\ |f_{t,h}^{m}(x_{t,h})-f_{h}^{ \star,m}(x_{t,h})\|>\varepsilon\},\] where the last line follows from the fact that the definition of \(\mathtt{Query}\) and the fact that \(\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h})\) implies that there exists some \(m\in[M]\) for which \(\|f_{t,h}^{m}(x_{t,h})-f_{h}^{\star,m}(x_{t,h})\|>\varepsilon\). The above implies that \[\mathsf{T}_{A}\leq\sum_{m=1}^{M}\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{|f_{t,h}^{m} (x_{t,h})-f_{h}^{\star,m}(x_{t,h})\|>\varepsilon\}.\]* _Bound on_ \(\mathsf{T}_{B}\)_. First note that Lemma_ 21 _implies that with probability at least_ \(1-\delta\)_, for all_ \(m\leq M\) _and_ \(h\leq H\)_,_ \[\|f_{h}^{\star,m}(x_{t,h})-f_{t,h}^{m}(x_{t,h})\|\leq\Delta_{t,h}^{m}(x_{t,h}).\] (71)

_Next, note that_

\[\mathsf{T}_{B} \leq\sum_{h=1}^{H}\bar{Z}_{t,h}\mathbf{1}\{\pi_{t}(x_{t,h})\neq \pi^{\star}(x_{t,h})\}\] (72) \[=\sum_{h=1}^{H}\mathbf{1}\{\mathtt{Query}(F_{t,h}(x_{t,h}),\tilde {\Delta}_{t,h}(x_{t,h}))=0,\pi_{t}(x_{t,h})+\pi^{\star}(x_{t,h})\},\]

_where in the last line follows from plugging in the query condition under which_ \(Z_{t,h}=0\)_. However note that for any_ \(h\leq H\) _for which_ \(\mathtt{Query}(F_{t,h}(x_{t,h}),\tilde{\Delta}_{t,h}(x_{t,h}))=0\)_, by the definition of_ \(\mathtt{Query}\) _and the fact that_ \(\pi_{t}(x_{t,h})\neq\pi^{\star}(x_{t,h})\)_, there must exist some_ \(m\in[M]\) _such that_

\[\|f_{h}^{\star,m}(x_{t,h})-f_{t,h}^{m}(x_{t,h})\|>\Delta_{t,h}^{m}(x_{t,h}).\]

_However, the above contradicts (_71_), and thus with probability at least_ \(1-\delta\)_,_

\[\mathsf{T}_{B}=0.\] (73)

Plugging the above bounds on \(\mathsf{T}_{A}\) and \(\mathsf{T}_{B}\) in (70), we get that

\[R(\tau_{t}^{\pi^{\star}})-R(\tau_{t}^{\pi_{t}})\leq 2H\sum_{h=1}^{H} \mathbf{1}\{x_{t,h}^{\pi^{\star}}\in\mathsf{X}_{\varepsilon}\}+2H\sum_{m=1}^{ M}\sum_{h=1}^{H}Z_{t,h}\mathbf{1}\{|f_{t,h}^{m}(x_{t,h})-f_{h}^{\star,m}(x_{t,h}) \|>\varepsilon\}.\] (74)

Step 2: Aggregating over all time steps.Using the bound in (74) for each round \(t\), we get that

\[\mathrm{Reg}_{T} =\sum_{t=1}^{T}\Bigl{(}R(\tau_{t}^{\pi^{\star}})-R(\tau_{t}^{\pi _{t}})\Bigr{)}\] \[\leq 2H\sum_{h=1}^{H}\sum_{t=1}^{T}\mathbf{1}\{x_{t,h}^{\pi^{ \star}}\in\mathsf{X}_{\varepsilon}\}+2H\sum_{t=1}^{T}\sum_{m=1}^{M}\sum_{h=1}^ {H}Z_{t,h}\mathbf{1}\{\|f_{t,h}^{m}(x_{t,h})-f_{h}^{\star,m}(x_{t,h})\|> \varepsilon\}.\]

Using the fact that \(\mathbf{1}\{a\geq b\}\leq{}^{a^{2}}\!/\!{b^{2}}\) for any \(a,b\geq 0\), and the definition of \(T_{\varepsilon,h}\) in the above, we get that

\[\mathrm{Reg}_{T} \leq 2H\sum_{h=1}^{H}T_{\varepsilon,h}+2H\sum_{t=1}^{T}\sum_{m=1}^ {M}\sum_{h=1}^{H}Z_{t,h}\frac{\|f_{t,h}^{m}(x_{t,h})-f_{h}^{\star,m}(x_{t,h}) \|^{2}}{\varepsilon^{2}}\] \[\leq 2H\sum_{h=1}^{H}T_{\varepsilon,h}+\frac{2H}{\varepsilon^{2}} \sum_{m=1}^{M}\sum_{h=1}^{H}\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T).\] (75)

where the last line follows from using the bound in Lemma 21.

Plugging in the form of \(\Psi_{\delta}^{\ell_{\phi}}(\mathcal{F}_{h}^{m},T)\) and ignoring \(\log\) factors and constants, we get that

\[\mathrm{Reg}_{T}\lesssim H\sum_{h=1}^{H}T_{\varepsilon,h}+\frac{H}{\lambda \varepsilon^{2}}\sum_{m=1}^{M}\sum_{h=1}^{H}\mathrm{Reg}^{\ell_{\phi}}( \mathcal{F}_{h}^{m};T)+\frac{MH^{2}}{\lambda^{2}\varepsilon^{2}}\log(1/\delta).\]

Notice that \(\varepsilon\) is a free parameter above so the final bound follows by taking \(\inf\) over all feasible \(\varepsilon\).

#### f.5.3 Total Number of Queries

Fix any \(t\leq T\), and let \(h_{t}\) denote the first time step at round \(t\) for which \(Z_{t,h_{t}}=1\), if such a time-step exists (and is set to be \(H+1\) otherwise). We first observe that for all \(h\leq h_{t}\), we have \(\pi^{\star}(x_{t,h})=\pi_{t}(x_{t,h})\). To see this, note that

\[Z_{t,h_{t}}\mathbf{1}\{\exists h<h_{t}:\pi^{\star}(x_{t,h})\neq\pi_{t}(x_{t,h} )\}\leq\sum_{h=1}^{h_{t}-1}Z_{t,h_{t}}\mathbf{1}\{\pi^{\star}(x_{t,h})\neq\pi_{ t}(x_{t,h})\}\]\[\leq\sum_{h=1}^{h_{t-1}-1}Z_{t,h}\mathbbm{1}\{\pi^{*}(x_{t,h})+\pi_{t}(x _{t,h})\}\] \[\leq\sum_{h=1}^{h_{t-1}-1}\bar{Z}_{t,h}\mathbbm{1}\{\pi^{*}(x_{t,h} )+\pi_{t}(x_{t,h})\}\]

where in second inequality above, we used the fact that \(Z_{t,h}=0\) (and thus \(\bar{Z}_{t,h}=1\)) for all \(h<h_{t}\), by the definition of \(h_{t}\). Observe that the right hand side in the last inequality above is equivalent to the term (72) in the bound on \(\mathbb{T}_{B}\) above (where sum is now till \(h_{t}\) instead of \(H\)). Thus, using the bound in (73), we get that

\[Z_{t,h_{t}}\mathbbm{1}\{\exists h<h_{t}:\pi^{*}(x_{t,h})+\pi_{t}(x _{t,h})\}=0,\]

and thus

\[\pi^{*}(x_{t,h})=\pi_{t}(x_{t,h})\qquad\text{for all }h\leq h_{t}.\] (76)

Next, let \(\varepsilon>0\) be a free parameter, and note plugging in the definition of \(h_{t}\), we get that the total number of samples is bounded as:

\[N_{T} =\sum_{t=1}^{T}\sum_{h=1}^{H}Z_{t,h}\] \[\leq H\sum_{t=1}^{T}Z_{t,h_{t}}\] \[=H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\in\mathbb{X} _{\varepsilon}\}+H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\notin \mathbb{X}_{\varepsilon}\}\] \[=H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\in\mathbb{X} _{\varepsilon}\}+H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\notin \mathbb{X}_{\varepsilon},\|\Delta_{t,h_{t}}^{m}(x_{t,h_{t}})\|_{\infty}\leq \frac{\varepsilon}{4}\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+H\sum_{t=1}^{T}Z _{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\notin\mathbb{X}_{\varepsilon},\|\Delta_{t, h_{t}}^{m}(x_{t,h_{t}})\|_{\infty}>\frac{\varepsilon}{4}\}\] \[=\mathbb{T}_{C}+\mathbb{T}_{D}+\mathbb{T}_{E},\]

where \(\mathbb{T}_{C}\), \(\mathbb{T}_{D}\) and \(\mathbb{T}_{E}\) are the first, second and the third term respectively in the previous line. We bound them separately below.

* _Bound on \(\mathbb{T}_{C}\)._ Fix any \(t\leq T\). Using the relation in (76), note that \(\pi^{*}(x_{t,h})=\pi_{t}(x_{t,h})\) for all \(h<h_{t}\). Thus, the corresponding trajectories would be identical till time step \(h_{t}\), which implies that \(x_{t,h_{t}}=x_{t,h_{t}}^{\pi^{*}}\). Using this property in the \(\mathbb{T}_{C}\), we get that \[(\mathbb{T}_{C})_{t} =H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}\in\mathbb{X}_{ \varepsilon}\}\] \[=H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbbm{1}\{x_{t,h_{t}}^{\pi^{*}}\in \mathbb{X}_{\varepsilon}\}\] \[\leq H\sum_{t=1}^{T}\sum_{h=1}^{H}Z_{t,h}\mathbbm{1}\{x_{t,h}^{ \pi^{*}}\in\mathbb{X}_{\varepsilon}\}\] \[=H\sum_{h=1}^{H}T_{\varepsilon,h},\] where the last line plugs in the definition of \(T_{\varepsilon,h}\).
* _Bound on \(\mathbb{T}_{D}\)._ First note that \[(\mathbb{T}_{D})_{t}=H\mathbbm{1}\{\texttt{Query}\big{(}F_{t,h_{t}}(x_{t,h_{t} }),\tilde{\Delta}_{t,h_{t}}(x_{t,h_{t}})\big{)}=1,\texttt{Query}(F^{*}(x_{t,h_{ t}}),\varepsilon\tilde{\mathbbm{1}})=0,\sup_{m\in[M]}\Delta_{t}^{m}(x_{t,h_{t}})\leq \varepsilon/4\}.\] In the following, we will show that all the conditions in the above indicator can not hold simultaneously. First note that since \(\texttt{Query}\big{(}F_{t,h_{t}}(x_{t,h_{t}}),\tilde{\Delta}_{t,h_{t}}(x_{t,h _{t}})\big{)}=1\), there exists an \(\widetilde{F}\) such that \[\texttt{SelectAction}(\widetilde{F}(x_{t,h_{t}})))\neq\texttt{ SelectAction}(F_{t,h_{t}}(x_{t,h_{t}}))\] (77)\[\forall m\in[M]:\qquad\|\widetilde{F}(x_{t,h_{t}})[:,m]-F_{t,h_{t}}(x_{t,h_{t}})[:,m ]\|\leq\Delta^{m}_{t,h_{t}}(x_{t,h_{t}}).\] (78) On the other hand, recall that Lemma 21 implies that \[\forall m\in[M]:\qquad\|F^{\star}(x_{t,h_{t}})[:,m]-F_{t,h_{t}}(x_{t,h_{t}})[:,m ]\|\leq\Delta^{m}_{t,h_{t}}(x_{t,h_{t}}).\] (79) Since, \(\sup_{m}\Delta^{m}_{t,h_{t}}(x_{t,h_{t}})\leq\nicefrac{{\varepsilon}}{{4}}\), an application of Triangle inequality along with the bounds (78) and (79) imply that \[\forall m\in[M]:\qquad\|F^{\star}(x_{t,h_{t}})[:,m]-\widetilde{F}(x_{t,h_{t}})[ :,m]\|\leq 2\Delta^{m}_{t,h_{t}}(x_{t,h_{t}})<\varepsilon.\] (80) But the above contradicts the fact that \(\mathsf{Query}\big{(}F^{\star}(x_{t,h_{t}}),\varepsilon\,\mathbb{I}\big{)}=0\) since both \(\widetilde{F}\) and \(F_{t}\) satisfy the norm constraints in the definition of \(\mathsf{Query}\), but we can not simultaneously have that \[\mathsf{SelectAction}(F^{\star}(x_{t,h_{t}})))=\mathsf{SelectAction}(F_{t,h_{t}}( x_{t,h_{t}}))=\mathsf{SelectAction}(\widetilde{F}(x_{t,h_{t}})),\] due to (77). Thus, we must have that \[(\mathsf{T}_{D})_{t}=0.\]
* _Bound on_ \(\mathsf{T}_{E}\). We note that \[\mathsf{T}_{E} \leq H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbf{1}\{\lfloor\|\tilde{ \Delta}_{t,h_{t}}(x_{t,h_{t}})\rfloor_{\infty}>\nicefrac{{\varepsilon}}{{4}}\}\] \[=H\sum_{t=1}^{T}Z_{t,h_{t}}\mathbf{1}\{\lfloor\exists m\in[M]: \Delta^{m}_{t,h_{t}}(x_{t,h_{t}})>\nicefrac{{\varepsilon}}{{4}}\}\] \[\leq H\sum_{m=1}^{M}\sum_{t=1}^{T}Z_{t,h_{t}}\mathbf{1}\{\lfloor \Delta^{m}_{t,h_{t}}(x_{t,h_{t}})>\nicefrac{{\varepsilon}}{{4}}\},\] where the last line simply upper bound the term for \(h_{t}\) by the corresponding terms for all \(h\leq H\). Using Lemma 22 to bound the term in the right hand side for each \(m\in[M]\) and \(h\leq H\), we get that \[\mathsf{T}_{E}\leq\sum_{h=1}^{H}\sum_{m=1}^{M}\frac{320H\Psi^{\ell_{\phi}}_{ \delta}(\mathcal{F}^{m}_{h},T)}{\varepsilon^{2}}\cdot\mathfrak{E}(\mathcal{F} ^{m}_{h},\frac{\varepsilon}{8};f^{\star,m}_{h}).\]

Gathering the bound above, we get that

\[N_{T}\leq H\sum_{h=1}^{H}T_{\varepsilon,h}+\frac{320H}{\varepsilon^{2}}\sum_{ h=1}^{H}\sum_{m=1}^{M}\Psi^{\ell_{\phi}}_{\delta}(\mathcal{F}^{m}_{h},T)\cdot \mathfrak{E}(\mathcal{F}^{m}_{h},\frac{\varepsilon}{8};f^{\star,m}_{h}).\]

Plugging in the form of \(\Psi^{\ell_{\phi}}_{\delta}(\mathcal{F}^{m}_{h},T)\) and ignoring \(\log\) factors and constants, we get that

\[N_{T}\lesssim H\sum_{h=1}^{H}T_{\varepsilon,h}+\frac{H}{\lambda\varepsilon^{2 }}\sum_{h=1}^{H}\sum_{m=1}^{M}\mathrm{Reg}^{\ell_{\phi}}(\mathcal{F}^{m}_{h};T) \cdot\mathfrak{E}(\mathcal{F}^{m}_{h},\nicefrac{{\varepsilon}}{{8}};f^{ \star,m}_{h})+\frac{MH^{2}}{\lambda^{2}\varepsilon^{2}}\log(1/\delta).\]

Notice that \(\varepsilon\) is a free parameter above so the final bound follows by taking \(\inf\) over all feasible \(\varepsilon\).