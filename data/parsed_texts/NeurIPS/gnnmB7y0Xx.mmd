[MISSING_PAGE_EMPTY:1]

labels in shallow transformer layers, and then uses the ICL function in deeper transformer layers to make predictions (Hendel et al., 2023). However, while these compressed vectors encapsulate learned information in a more condensed form and show significant promise in applying ICL, there still exists a considerable gap in understanding the operational mechanisms and optimization strategies of these vectors. This significant gap hinders the further grasping and utilization of ICL.

In this paper, we aim to bridge the existing gap by presenting a comprehensive analysis of compressed vectors. Specifically, we investigate their similarities with parameters trained via gradient descent and introduce the formulation of state vector that encapsulates the processing state of ICL stored in the attention activations. Building on the concept of state vector, and drawing insights from the model soup (Wortsman et al., 2022) and Polyak momentum-based gradient optimization algorithms (Qian, 1999, Sutskever et al., 2013), we propose inner optimization and momentum optimization strategies which are progressively applied to enhance the state vector. Moreover, we further exploit the demonstration compression capabilities of the state vector to address the practical challenges encountered when applying ICL in settings with multiple examples, where demonstrations are typically too lengthy for standard ICL, such as in the 100-shot setting which is prevalent in practice. Specifically, we introduce a divide-and-conquer aggregation method that effectively aggregates the ICL functions of these extensive examples. This approach enables us to scale up for processing extended examples by compressing them into a single state vector. We conduct extensive experiments using Llama-2 (Touvron et al., 2023) and GPT-J (Wang and Komatsuzaki, 2021) in both zero-shot and few-shot settings. The experimental results show that our method effectively enhances the state vector and achieves state-of-the-art performance on diverse tasks. This not only manifests the effectiveness of our approach but also paves the way for a more comprehensive understanding of ICL.

Our contributions are summarized as follows:

* We delve into the working mechanism of compressed vectors in ICL and highlight their similarities with parameters trained via gradient descent. Building on this observation, we propose the formulation of the state vector.
* We propose inner and momentum optimization to progressively refine the state vector as an efficient test-time adaptation. Additionally, we introduce a divide-and-conquer aggregation to effectively scale up to large numbers of examples.
* We show the practicality of our proposed methods across a wide range of tasks through extensive experiments. Our results also offer insights for future research aiming to fully understand the functionalities of ICL.

## 2 Related Work

Mechanistic Interpretability.Recent works have focused on the working mechanisms of ICL (Chan et al., 2022, Xie et al., 2022, Wang et al., 2023). Olsson et al. (2022) argue that induction heads may be the mechanistic source of general ICL in transformers. Akyurek et al. (2022) show that transformer-based in-context learners can implicitly implement standard optimization algorithms on linear models. A mainstream assumption posits that ICL has a similarity with the gradient descent. von Oswald et al. (2023) demonstrate how a linear attention-only transformer model can perform a gradient descent-like procedure implicitly. Dai et al. (2023) compare standard gradient descent based fine-tuning and ICL, and figure out that the transformer attention of ICL exhibits a dual form of gradient descent-based optimization. Moreover, some works revisit and modify this theory on the layer causality dependence (Natan et al., 2023) or training batch size (Shen et al., 2023). In contrast, we focus on the application of the dual form of gradient descent and ICL and present optimization methods with inspiration from the dual form.

Task Representation.Numerous studies have extensively explored the concept of compressing various tasks into task representations as a means of effectively manipulating tasks within ICL ability. Notably, Shao et al. (2023) and Mu et al. (2023) have successfully yielded compositional task representations by training a composition model. In a slightly different vein, some researchers have delved into the art of devising methodologies to compose minor parameter adjustments acquired through task fine-tuning (Ilharco et al., 2022, Panigrahi et al., 2023, Yu et al., 2023, Hu et al., 2024, Merullo et al., 2023). An alternative line of research finds that the task representation could be extracted in ICL (Liu et al., 2023, Hendel et al., 2023, Todd et al., 2023, Yang et al., 2023). Differentfrom these approaches, our work avoids the need for additional training and focuses more on analysing why these compressed vectors work and how to improve their performance.

## 3 Formalization

In this section, we first provide a detailed examination of attention activation which is found to contain the compressed ICL function by previous works (Hendel et al., 2023; Todd et al., 2023). Then, we highlight its inherent similarities with parameters trained through gradient descent. Finally, we introduce the concept of the state vector drawing inspiration from these observations.

A classic template of ICL has the following necessary components: (1) \(N\) examples that are used to form the demonstrations and each example contains an input query \(\mathcal{X}\) and its corresponding label \(\mathcal{Y}\). (2) Separate tokens \(\mathcal{S}\) that separate the input query and the label for each example (e.g., \(\rightarrow\)). (3) A query \(\mathcal{X}_{q}\) for prediction. With the above components, the contextual model input of ICL could be written as follows:

\[\mathcal{X}_{1},\mathcal{S},\mathcal{Y}_{1},\mathcal{X}_{2},\mathcal{S}, \mathcal{Y}_{2},\cdots,\mathcal{X}_{N},\mathcal{S},\mathcal{Y}_{N},\mathcal{ X}_{q},\mathcal{S}.\]

Here we analyse the attention activation of the last separate token. In the \(l\)-th transformer layer, the output activation \(\mathbf{a}^{l}\) of the attention heads of the last separate token is:

\[\mathbf{a}^{l}=W_{V}[X^{\prime};X]\operatorname{softmax}\left(\frac{\left(W_{ K}[X^{\prime};X]\right)^{T}\mathbf{q}}{\sqrt{d}}\right),\] (1)

where \(X^{\prime}\) denotes the hidden state of demonstrations, \(X\) denotes the hidden state of the query and the last separate token (called zero-shot input), \(q\) denotes the attention query vector of the last separate token, \([X^{\prime};X]\) denotes the matrix concatenation, \(\sqrt{d}\) is the scaling factor, \(W_{K}\) and \(W_{V}\) are parameter weight matrix.

Consistent with previous works (Dai et al., 2023; Natan et al., 2023), we omit the softmax operation and the scaling factor to approximate standard attention as relaxed linear attention for qualitative analysis. Consequently, the activation can be simplified as follows:

\[\mathbf{a}^{l} \approx W_{V}[X^{\prime};X]\left(W_{K}[X^{\prime};X]\right)^{T} \mathbf{q}\] (2) \[=\left(W_{V}X\left(W_{K}X\right)^{T}+W_{V}X^{\prime}\left(W_{K}X ^{\prime}\right)^{T}\right)\mathbf{q}\] \[=\left(W_{\text{ZSL}}+\sum_{i}\left((W_{V}\mathbf{x}_{i}^{\prime })\otimes(W_{K}\mathbf{x}_{i}^{\prime})\right)\right)\mathbf{q}.\]

We define \(W_{\text{ZSL}}=W_{V}X\left(W_{K}X\right)^{T}\) as the initialized parameters since it is the attention result in the Zero-Shot Learning (ZSL) setting.

To draw a meaningful comparison between attention activation and parameters trained through gradient descent, we now shift our focus towards analyzing a simple linear transformation represented by \(\mathbf{y}_{i}=W\mathbf{x}_{i}\). Given a loss function \(\mathcal{L}\) and the learning rate \(\eta\), the gradient of linear weight is:

\[\nabla_{W}\mathcal{L}(\mathbf{y}_{i})=\frac{\partial\mathcal{L}(\mathbf{y}_{ i})}{\partial\mathbf{y}_{i}}\frac{\partial\mathbf{y}_{i}}{\partial W}=\nabla_{ \mathbf{y}_{i}}\mathcal{L}(\mathbf{y}_{i})\mathbf{x}_{i}^{T}.\] (3)

Denoting the back-propagated errors as \(\mathbf{e}_{i}=-\eta\nabla_{\mathbf{y}_{i}}\mathcal{L}\), we can get the full batch gradient with training examples:

\[\Delta W_{GD}=\sum_{i}\mathbf{e}_{i}\otimes\mathbf{x}_{i}^{\prime},\] (4)

where \(\mathbf{x}_{i}^{\prime}\) is the input training examples. Hence, in the previous Eqn. 2, if we substitute \(W_{K}\mathbf{x}_{i}^{\prime}\) as training examples, and take \(W_{V}\mathbf{x}_{i}^{\prime}\approx\mathbf{e}_{i}\) corresponding to some meta gradients (Dai et al., 2023; Natan et al., 2023). The activation can be written as:

\[\mathbf{a}^{l}=\left(W_{\text{ZSL}}+\sum_{i}\mathbf{e}_{i}\otimes W_{K} \mathbf{x}_{i}^{\prime}\right)\mathbf{q}=\left(W_{\text{ZSL}}+\Delta W_{GD} \right)\mathbf{q}.\] (5)

Hence, it can be inferred that the output activation \(\mathbf{a}^{l}\) can be regarded as parameters trained via gradient descent which utilizes the demonstrations as training instances.

With the above dual form between activation and trained parameters, and in light of observations that transformers tend to learn the ICL function primarily in their first \(L\) layers (Wang et al., 2023), we have the following hypothesis: During the process of ICL, the first \(L\) layers progressively update the flow of information using each example in the demonstration through forward computation. The processing state of ICL is then stored within the activation of the attention head. The subsequent layers access and utilize the processing state to reinstate the ICL function, which is used implicitly for predicting the queries. Therefore we concatenate the activation in the initial \(L\) layers and introduce the notation of the state vector:

\[\mathcal{V}_{N}^{L}=\Big{\|}_{l=1}^{L}\ \mathbf{a}^{l},\] (6)

where \(L\) is the number of layers and \(N\) is the number of examples in the demonstration. \(\|\) denotes the concatenation operation. Note that we have a completely different construction strategy and usage compared to the function vector (Todd et al., 2023). Although the task vector (Hendel et al., 2023) may be functionally equivalent in the forward process, the proposed state vector differs significantly in its integration into the model, making it easier and more effective to analyse and interpret.

## 4 Method

### Overview

As illustrated in Figure 1, our approach initially extracts the state vector from the attention head that corresponds to the final separate token in the first \(L\) layers using a demonstration and a dummy query. Then, with the view of treating the state vector as trained parameters, coupled with drawing inspiration from the model soup and the momentum-based gradient optimization algorithm, we introduce two methods that progressively optimize the state vector as test-time adaptation (Liang et al., 2023): (1) inner optimization (SS4.2) and (2) momentum optimization (SS4.3). Moreover, we propose a divide-and-conquer (D&C) state vector aggregation method for efficiently compressing the ICL function in the multiple example setting (SS4.4).

After the state vector optimization or aggregation, we utilize the processed state vector to intervene the model during the forward inference pass. In particular, we first input a test query in the zero-shot setting or with the demonstration in the few-shot setting. During the forward pass in the first \(L\) layers, we replace the attention activation of the last separate token with the corresponding activation in the state vector. In other words, the state vector is leveraged to intervene in the output of the first \(L\) transformer layers, blocking the attention of the last separate token to the previous context. With state vector intervention, the transformer learns the ICL function from the processing state stored in the state vector, and continues to make the prediction on the test query.

Figure 1: The overall framework of the proposed state vector. The state vectors are extracted from the output activations of attention heads. These state vectors are progressively optimized by _inner optimization_ and _momentum optimization_, or be aggregated through a _divide-and-conquer (D&C) aggregation_. Finally, the processed state vector is utilized to intervene the inference forward pass.

### Inner Optimization

Inspired by the works on the model soup (Wortsman et al., 2022; Chronopoulou et al., 2023) which show that weight-space averaging not only yields performance improvement but also often enhances robustness, we thus ask the following research question (**RQ1**): _Is it possible to optimize our state vector using the model soup approach?_ To explore this question, we propose an inner optimization method to improve the effectiveness and robustness of state vector. Specifically, we not only extract the state vector in each separate token of the dummy query but also extract the state vector from each example. Formally, with a forward pass in an \(N\) shot ICL setting, we extract the \(N\) state vector \(\mathcal{V}_{i}^{L}\) (\(1\leq i\leq N\)) from last \(N\) separate token. Subsequently, we apply a uniform averaging process to these state vectors as follows:

\[\overline{\mathcal{V}}_{N}^{L}=\frac{1}{N}\sum_{i=1}^{N}\mathcal{V}_{i}^{L},\] (7)

where \(\overline{\mathcal{V}}_{N}^{L}\) is the inner optimized state vector, which can be directly utilized for inference intervention or serves as the initial state vector for later momentum optimization.

### Momentum Optimization

Since we view the state vector as parameters trained gradually through demonstration examples, the difference between two state vectors with adjacent corresponding separate tokens can also be regarded as the influence of the middle example, akin to the gradient. Motivated by this understanding, coupled with extensive studies of the gradient optimization algorithm (Sutskever et al., 2013; Duchi et al., 2010; Loshchilov and Hutter, 2019), we direct our focus toward a simple momentum-based gradient optimization algorithm, seeking to answer the following research question (**RQ2**): _Can our state vector be optimized using momentum-based optimization algorithm?_ To answer this question, we propose a momentum optimization. Formally, we first extract the influence of each example by subtracting two adjacent state vectors:

\[E_{i}^{L}=\mathcal{V}_{i}^{L}-\mathcal{V}_{i-1}^{L},\] (8)

where \(E_{i}^{L}\) is the influence of \(i\)-th (\(1<i\leq N\)) example in the early \(L\) layer. Then, we apply the momentum gradient optimization algorithm to obtain optimized influence \(\widetilde{E}_{i}^{L}\), and add it to the last state vector:

\[\widehat{\mathcal{V}}_{N}^{L}=\overline{\mathcal{V}}_{N}^{L}+\widetilde{E}^{L }=\overline{\mathcal{V}}_{N}^{L}+\texttt{opt}([E_{i}^{L}]_{i=1}^{N}),\] (9)

where \(\widehat{\mathcal{V}}_{N}^{L}\) is the momentum optimized state vector and \(\overline{\mathcal{V}}_{N}^{L}\) is the inner optimized state vector. \(\texttt{opt}(\cdot)\) denotes the momentum gradient optimization algorithm. We also explore various other gradient optimization algorithms in SS6.1.

### Divide-and-Conquer Aggregation

In addition to optimizing the state vector to more effectively represent the ICL function from a small number of examples, we also explore its capacity to encapsulate multiple examples within a single vector. However, regular ICL can not be directly used on multiple examples due to the context length limitation of current LLMs. This leads us to investigate the following question (**RQ3**): _Can we use the state vector to represent multiple examples that are unmanageable for regular ICL?_ To address this question, we propose a divide-and-conquer method for state vector aggregation. As depicted in Figure 1, our approach involves distinct aggregation processes (i.e. the divide stage and the conquer stage). In the divide stage, examples are randomly divided into groups, termed grouped demonstrations. Within each group, a random example is selected to serve as a dummy query, which allows us to extract a group-specific state vector. In the conquer stage, these dummy queries are paired with their corresponding labels to form input-label pairs. From these input-label pairs, we form an aggregated demonstration, add an additional dummy query, and subsequently extract the aggregated state vector. It is worth noting that during the forward pass of aggregated state vector extraction, we utilise the group-specific state vector to intervene the attention activation of the separate tokens of their corresponding examples. The divide and conquer approach allows us to aggregate the ICL function of each grouped demonstration into its respective group-specific state vector, and subsequently aggregate the ICL function of each group-specific state vector into a single,comprehensive aggregated state vector. This aggregated vector is then utilized for interventions during inference, similarly to the optimized state vector discussed in SS4.2 and SS4.3. Moreover, in the few-shot setting, the aggregated demonstrations are treated as inference demonstrations. The divide-and-conquer approach effectively circumvents the context-length constraints inherent in LLMs, thereby enabling a more effective and efficient aggregation of information across multiple examples.

## 5 Experiment

### Setup

We conduct the evaluation across 12 datasets that encompass different domains.

* **Linguistics** includes Antonym (Nguyen et al., 2017), Capitalize, Present-Past, and Singular-Plural (Todd et al., 2023), focusing on transformations in the form or meaning of words.
* **Translation** is represented by the English-French (Lample et al., 2018) dataset, which involves translating English words into their French counterparts.
* **Knowledge** comprises Country-Capital (Todd et al., 2023), AG News (Zhang et al., 2015), Person-Sport, Person-Instrument, Person-Occupation, Product-Company, and Landmark-Country (Hernandez et al., 2023), which are centred around question-to-answer mappings for commonsense knowledge queries.

We employ _Llama-2-7B_ and _GPT-J-6B_ as our LLMs, chosen for their moderate model sizes, open-source and capability for ICL. We also provide the results with larger models (i.e., Llama-2-13B) in the Appendix H. We use Llama-2-7B as the default model unless otherwise specified. Our method is orthogonal to the choice of transformer-based decoder-only autoregressive LLMs.

For simplicity evaluation, we restrict to single-token output and use first output token accuracy as the evaluation metric as in previous work (Hendel et al., 2023; Todd et al., 2023).

### Baseline

In the paper, we compare with the following methods:

* **Regular** is the baseline for the zero-shot setting that uses only the given query as input, while **ICL baseline**(Wei et al., 2022) makes predictions on the label by taking both the demonstrations and the given query.
* **Function vector**(Todd et al., 2023) is extracted from attention activation using the causal mediation method and is then added to the hidden state of certain transformer layers during inference.
* **Task vector**(Hendel et al., 2023) is extracted from the hidden state of the separate token and is leveraged for blocking the layer when inference.

### Inner Optimization(RQ1)

As shown in Table 1, the performance of our inner optimized state vector has a significant improvement comparing the task vector and function vector in both zero-shot and few-shot settings. Our state vector with inner optimization. In the zero-shot setting, the inner optimization shows an average improvement of 10.2% on Llama-2 and 5.9% on GPT-J across six datasets. In the few-shot setting, the inner optimization also achieves a 1.2% improvement on Llama-2 and 1.7% on GPT-J. The improvement demonstrates the effectiveness of inner optimization. However, although state vector (inn.) outperforms task vector, its few-shot performance on some datasets is inferior to the ICL baseline. We attribute this primarily to the introduction of query information from examples. While inner optimization enhances task-relevant information for the state vector, it also introduces noise of other dummy queries, hindering the model's ability to focus on the current predictive query, thereby reducing performance. In addition to the performance improvements, our inner optimization approach also effectively alleviates the phenomenon of high variance in the original task vector in the zero-shot setting. In practical use, the performance of the task vector is influenced by demonstrations and dummy queries, leading to weaker robustness. Our proposed inner optimization approach effectively mitigates this issue, similarly motivated as the model averaging method, thereby enhancing the robustness of the state vector.

### Momentum Optimization (RQ2)

As depicted in Table 1, building upon the inner optimized state vector, our proposed momentum optimization algorithm further enhances the effectiveness of the state vector, achieving the best performance on average in all settings. In the zero-shot setting, the momentum optimization boosts the performance of the inner-optimized state vector with an average increase of 1.3% on Llama-2 and 2.4% on GPT-J. In the few-shot setting, state vector with momentum optimization achieves a 0.8% average increase on Llama-2 and 1.0% on GPT-J. This reveals the effectiveness of our momentum optimization. With the combination of inner optimization and momentum optimization, our state

\begin{table}
\begin{tabular}{c|c|c c c c c c c} \hline
**Model** & **Method** & **Anym** & **Eng-F** & **Pers-Inst** & **Pers-Occ** & **Prod-Comp** & **Land-Cout** & **Average** \\ \hline \multirow{6}{*}{Llama-2-7B} & \multirow{6}{*}{Zero-shot} & Regular & 1.0\(\pm\) 0.2 & 0.1\(\pm\)0.0 & 0.0\(\pm\)0.0 & 0.4\(\pm\)0.2 & 0.0\(\pm\)0.0 & 0.3 \\  & & Function vector & 45.1\(\pm\)1.20 & 2.1\(\pm\)0.13 & 11.0\(\pm\)0.1 & 0.1\(\pm\)0.1 & 25.6\(\pm\)4.3 & 32.9\(\pm\)21.6 & 22.8 \\  & & Task vector & 56.2\(\pm\)2.8 & 63.2\(\pm\)3.6 & 61.8\(\pm\)8.4 & 27.9\(\pm\)15.2 & 55.5\(\pm\)20.1 & 57.8\(\pm\)26.3 & 53.7 \\  & & State vector (inn.) & 61.0\(\pm\)1.0 & 66.5\(\pm\)2.2 & 67.4\(\pm\)2.6 & 42.7\(\pm\)2.4 & 64.5\(\pm\)10.6 & 81.0\(\pm\)1.7 & 63.9 \\  & & State vector (norm) & 60.4\(\pm\)0.7 & 67.5\(\pm\)1.8 & 68.7\(\pm\)1.6 & 45.6\(\pm\)5.9 & 71.3\(\pm\)3.6 & 77.7\(\pm\)1.8 & 65.2 \\ \cline{2-10}  & \multirow{6}{*}{Few-shot} & \multirow{6}{*}{ICL baseline} & 64.8\(\pm\)4.8 & 74.3\(\pm\)0.8 & 71.7\(\pm\)3.7 & 56.1\(\pm\)2.7 & 80.8\(\pm\)0.8 & 87.0\(\pm\)0.3 & 72.5 \\  & & Function vector & 54.5\(\pm\)0.9 & 65.2\(\pm\)1.4 & 60.8\(\pm\)5.6 & 54.2\(\pm\)2.2 & 76.0\(\pm\)1.3 & 84.2\(\pm\)2.9 & 65.8 \\  & & Task vector & 65.7\(\pm\)1.8 & 73.8\(\pm\)0.9 & 66.6\(\pm\)5.2 & 56.4\(\pm\)2.3 & 81.9\(\pm\)1.8 & 86.7\(\pm\)0.9 & 71.8 \\  & & State vector (norm) & 66.2\(\pm\)1.6 & 74.6\(\pm\)0.7 & 70.4\(\pm\)1.3 & 57.0\(\pm\)2.9 & **82.8\(\pm\)**1.6 & 87.5\(\pm\)0.9 & 73.0 \\  & & State vector (norm) & 65.8\(\pm\)3.7 & 74.3\(\pm\)1.1 & **74.9\(\pm\)**2.9 & **58.2\(\pm\)**0.4 & 82.0\(\pm\)1.0 & **87.6\(\pm\)**0.3 & **73.8** \\ \hline \multirow{6}{*}{GPT-J-6B} & \multirow{6}{*}{Zero-shot} & Regular & 8.1\(\pm\)0.6 & 7.2\(\pm\)0.6 & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 1.9\(\pm\)0.5 & 0.9\(\pm\)0.2 & 3.0 \\  & & Function vector & 33.1\(\pm\)1.8 & 32.9\(\pm\)1.8 & 41.5\(\pm\)8.1 & 31.1\(\pm\)2.3 & 46.3\(\pm\)5.7 & 22.5\(\pm\)10.2 & 24.4 \\  & & Task vector & 22.6\(\pm\)3.8 & 32.2\(\pm\)5.1 & 44.4\(\pm\)2.0 & 28.3\(\pm\)18.6 & 43.8\(\pm\)5.7 & 41.3\(\pm\)12.3 & 35.6 \\  & & State vector (inn.) & 33.4\(\pm\)1.9 & 31.7\(\pm\)3.8 & 49.3\(\pm\)2.0 & 30.0\(\pm\)6.2 & **42.8\(\pm\)**4.3 & 61.9\(\pm\)1.6 & 41.5 \\  & & State vector (norm) & 31.1\(\pm\)1.0 & 35.1\(\pm\)2.4 & 50.3\(\pm\)3.0 & 42.4\(\pm\)1.5 & 44.2\(\pm\)1.5 & 60.3\(\pm\)0.9 & 43.9 \\ \cline{2-10}  & \multirow{6}{*}{Few-shot} & \multirow{6}{*}{ICL baseline} & 59.2\(\pm\)1.4 & 69.9\(\pm\)2.0 & 47.4\(\pm\)6.7 & 29.3\(\pm\)1.0 & 62.5\(\pm\)1.0 & **69.3\(\pm\)**0.5 & 55.8 \\  & & Function vector & 56.4\(\pm\)1.9 & 65.8\(\pm\)1.9 & 49.1\(\pm\)1.2 & 30.3\(\pm\)1.9 & 58.5\(\pm\)3.3 & 69.2\(\pm\)0.6 & 54.9 \\ \cline{2-10}  & & Task vector & 58.5\(\pm\)1.6 & 70.6\(\pm\)1.2 & 42.3\(\pm\)6.4 & 27.8\(\pm\)3.3 & 66.0\(\pm\)2.6 & 63.1\(\pm\)5.3 & 54.7 \\ \cline{2-10}  & & State vector (inn.) & 58.7\(\pm\)2.2 & **70.9\(\pm\)**1.3 & 46.5\(\pm\)4.9 & 29.4\(\pm\)1.7 & **66.3\(\pm\)**2.1 & 66.4\(\pm\)2.8 & 56.4 \\ \cline{2-10}  & & State vector (norm) & **59.6\(\pm\)**1.4 & 70.1\(\pm\)2.2 & **51.9\(\pm\)**2.4 & **30.4\(\pm\)**1.1 & 63.8\(\pm\)**0.8 & 68.6\(\pm\)0.3 & **57.4** \\ \hline \end{tabular}
\end{table}
Table 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in **bold**. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (\(p<.05\)).

Figure 2: Performance of aggregation across number of examples. _Avg._ denotes the average aggregation baseline and _D&C._ denotes the divide-and-conquer aggregation. The **X** axis represents the number of examples, and the **Y** axis represents the accuracy.

vector (mom.) surpasses the original variant, showcasing a remarkable improvement of 11.5% for Llama-2 and 8.3% for GPT-J in the zero-shot setting. In the few-shot setting, our state vector (mom.) still outperforms the task vector with a 2.0% improvement for Llama-2 and 2.7% for GPT-J. Furthermore, without inputting demonstration during inference, the state vector (mom.) achieves an impressive 90% ICL performance on Llama-2 and 78% ICL performance on GPT-J. When compared to ICL with the same examples as the demonstration, state vector (mom.) outperforms ICL in both Llama-2 and GPT-J. These improvements verify the effectiveness of our progressive optimization strategy. Note that applying momentum optimization directly to task vectors does not yield average improvements across tasks in our preliminary experiment. We speculate that this inconsistency stems from the poor robustness of the task vectors, which hinders the stable optimization by momentum optimization and leads to poor performance in some tasks.

### Divide-and-Conquer Aggregation (RQ3)

In this experiment, we explore the performance of D&C state vector aggregation across varying numbers of examples. Besides the regular and ICL baseline mentioned, we introduce average aggregation as a strong baseline. This approach first extracts state vectors from the example group and subsequently employs their mathematical average for aggregation. We compare our D&C aggregation method with the baseline ranging from 10 to 100 examples across two models. Due to limited computational resources, we were not able to do an exhaustive search over all datasets. Thus, we only present the results for four tasks.

As illustrated in the Figure 2, both the D&C aggregation and average aggregation exhibit similar trends in both few-shot and zero-shot settings. The performance of both aggregation methods initially falls short of the ICL baseline. However, their performance boosts when examples increase. The initial poor performance can be attributed to the limited number of state vectors. Additionally, although the performance of the D&C aggregation initially falls behind that of the average aggregation, it exhibits a more substantial performance improvement when examples increase, ultimately outperforming average aggregation in the multiple example setting, highlighting the efficiency of D&C aggregation.

## 6 Analysis

### Ablation with Other Optimization Methods

We present an ablation study to investigate various classical gradient optimization algorithms, aiming to delve deeper into the inner state vector optimization. We compare the momentum-based gradient optimization algorithm with following additional first-order gradient optimization algorithms: Adagrad (adag.) (Duchi et al., 2010), RMSprop (rms.) (Graves, 2013) and Adam(adam.) (Kingma and Ba, 2015).

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Method** & **Zero-shot** & **Few-shot** \\ \hline ICL baseline & 0.2\(\pm\) 0.4 & 71.0\(\pm\) 10.8 \\ Task vector & 52.9\(\pm\) 9.4 & 68.5\(\pm\) 10.5 \\ State vector (mom.) & 65.2\(\pm\) 10.2 & 72.2\(\pm\) 10.6 \\ State vector (adag.) & 11.7\(\pm\) 12.0 & 16.1\(\pm\) 10.2 \\ State vector (rms.) & 0.8\(\pm\) 0.9 & 1.5\(\pm\) 1.0 \\ State vector (adam.) & 6.7\(\pm\) 6.1 & 10.6\(\pm\) 8.5 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison of gradient optimization algorithms. The method means the optimization algorithm applied to the \(\texttt{opt}(\cdot)\) in Eqn. 9.

Figure 3: Average zero-shot performance across six datasets for each choice of the intermediate layer \(L\). The solid line means the average value, while the shaded area indicates the standard deviation.

As shown in Table 2, we observe a significant decrease in state vector performance when using first-order gradient optimization algorithms, in contrast to the more stable results obtained with momentum-based optimization. This discrepancy suggests that current first-order gradient optimization algorithms may not be well-suited for state vector optimization. We believe there are two main reasons for this. First, first-order gradient optimizers typically rely on adaptive learning rates, which depend on a significant amount of historical information. This can lead to instability and reduced effectiveness, especially when the available data is limited. Second, first-order gradient optimizers involve more complex hyper-parameters compared to the Momentum Optimizer, making it more difficult to identify optimal settings. Our experimental results confirm that directly applying hyper-parameter configurations commonly used in gradient descent leads to suboptimal performance in state vector optimization.

### Layer Selection

We investigate the impact of layer selection on the extraction of state vectors in transformer models. We evaluate the average performance across different datasets in the zero-shot setting, as illustrated in Figure 3. Our results reveal a dual-phase trend: initially, increasing the number of layers for state vector extraction improves performance, but this improvement reverses beyond the 14th layer. We correlate this with the dynamics of ICL function processing in transformers in line with previous works (Voita et al., 2019; Wang et al., 2023). In the initial layers, transformers are primarily engaged in learning and encapsulating the ICL function within state vector, where additional layers enhance the richness of the functional information in the state vector. In contrast, the later layers prioritize applying this learned information for prediction tasks. Here, additional layers tend to introduce noise, especially from predicted labels of dummy queries, which may negatively impact performance.

### Qualitative Study

We provide the visualization by Principal Component Analysis (PCA) of the original state vector in the Antonym, English-French and Product-Company task. As depicted in Figure 4, we have three observations: (1) State vectors corresponding to the examples occupying the same position tend to form distinct clusters. This clustering pattern suggests a high degree of similarity among state vectors within each example position, despite different contexts. (2) A notable separation is evident between the state vectors originating from the first example and other position examples. This demarcation implies that ICL may begin to effectively function with a few examples. (3) An interesting trend is observable in the movement of these clusters as the example position increases. This trend may be indicative of an accumulation of task-specific information, where each additional example contributes to a more nuanced understanding of the model. These findings suggest a progressive enhancement in the ability of model to internalize and reflect the subtleties of the task at hand. Moreover, these observations reflect the efficacy of momentum optimization to leverage the observed clustering trend.

Figure 4: The 2D PCA visualization of the state vector in the Antonym, English-French, and Product-Company tasks, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration, with the position of each point indicated by the adjacent number.

Conclusion

In this paper, we reveal that ICL compressed vector can be viewed as parameters trained through gradient descent on the demonstrations. Then, we introduce the concept of state vector coupled with two optimization methods to enhance the capability of ICL and conduct comprehensive experiments across two popular LLMs and multiple tasks to support our claim. Furthermore, our approach demonstrates the ability to compress context while maintaining lower variance. In the future, we aim to extend our methods to more complex ICL scenarios and apply them to larger LLMs and call for more nuanced and realistic studies of ICL.

## Acknowledgements

This work is jointly supported by grants: National Natural Science Foundation of China (No. 62376067), National Natural Science Foundation of China (No. 62406088) and Guangdong Basic and Applied Basic Research Foundation (2023A1515110078).

## References

* Akyurek et al. [2022] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _ArXiv preprint_, abs/2211.15661, 2022.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Chan et al. [2022] Stephanie Chan, Adam Santoro, Andrew K. Lampinen, Jane Wang, Aaditya Singh, Pierre H. Richemond, James L. McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.
* Chronopoulou et al. [2023] Alexandra Chronopoulou, Matthew Peters, Alexander Fraser, and Jesse Dodge. AdapterSoup: Weight averaging to improve generalization of pretrained language models. In _Findings of the Association for Computational Linguistics: EACL 2023_, pages 2054-2063, 2023.
* Dai et al. [2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 4005-4019, 2023.
* December 9, 2022_, 2022.
* Dong et al. [2023] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _ArXiv preprint_, abs/2301.00234, 2023.
* The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010_, pages 257-269, 2010.
* Graves [2013] Alex Graves. Generating sequences with recurrent neural networks. _ArXiv_, abs/1308.0850, 2013.
* Hendel et al. [2023] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. _ArXiv preprint_, abs/2310.15916, 2023.
* Heng et al. [2014]Evan Hernandez, Arnab Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. Linearity of relation decoding in transformer language models. _ArXiv preprint_, abs/2308.09124, 2023.
* Hu et al. (2024) Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, and Min Zhang. Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation. In _Proc. of AAAI_, 2024.
* Ilharco et al. (2022) Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. _ArXiv preprint_, abs/2212.04089, 2022.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Machine Learning_, 2015.
* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, _Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023_, pages 611-626. ACM, 2023. doi: 10.1145/3600006.3613165.
* Lample et al. (2018) Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Word translation without parallel data. In _International Conference on Machine Learning_, 2018.
* Liang et al. (2023) Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under distribution shifts. _ArXiv preprint_, abs/2303.15361, 2023.
* Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023a.
* Liu et al. (2023b) Sheng Liu, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. _ArXiv preprint_, abs/2311.06668, 2023b.
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Machine Learning_, 2019.
* Merullo et al. (2023) Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style vector arithmetic. _ArXiv preprint_, abs/2305.16130, 2023.
* Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. Learning to compress prompts with gist tokens. _ArXiv preprint_, abs/2304.08467, 2023.
* Natan et al. (2023) Tomer Bar Natan, Gilad Deutch, Nadav Magar, and Guy Dar. In-context learning and gradient descent revisited. _ArXiv preprint_, abs/2311.07772, 2023.
* Nguyen et al. (2017) Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. Distinguishing antonyms and synonyms in a pattern-based neural network. In _Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers_, pages 76-85, 2017.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _ArXiv preprint_, abs/2209.11895, 2022.
* Panigrahi et al. (2023) Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill localization in fine-tuned language models. In _International Conference on Machine Learning_, 2023.
* Qian (1999) Ning Qian. On the momentum term in gradient descent learning algorithms. _Neural Networks_, 12(1):145-151, 1999.
* Qian et al. (2018)Nan Shao, Zefan Cai, Hanwei Xu, Chonghua Liao, Yanan Zheng, and Zhilin Yang. Compositional task representations for large language models. In _International Conference on Learning Representations_, 2023.
* Shen et al. (2023) Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? _ArXiv preprint_, abs/2310.08540, 2023.
* Sutskever et al. (2013) Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In _Proc. of ICML_, volume 28 of _JMLR Workshop and Conference Proceedings_, pages 1139-1147, 2013.
* Todd et al. (2023) Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. Function vectors in large language models. _ArXiv preprint_, abs/2310.15213, 2023.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, and Kevin R. Stone et al. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, abs/2307.09288, 2023.
* Voita et al. (2019) Elena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In _Proc. of EMNLP_, pages 4396-4406, 2019.
* Oswald et al. (2023) Johannes von Oswald, Eyvind Niklasson, E. Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, 2023.
* Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, 2021.
* Wang et al. (2023) Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 9840-9855, 2023.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. _Trans. Mach. Learn. Res._, 2022, 2022.
* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 23965-23998, 2022.
* Xie et al. (2022) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In _International Conference on Machine Learning_, 2022.
* Yang et al. (2023) Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward tuning boosts in-context learning in language models. _ArXiv preprint_, abs/2305.13016, 2023.
* Yu et al. (2023) Le Yu, Yu Bowen, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. _ArXiv preprint_, abs/2311.03099, 2023.
* Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015.
* Zhang et al. (2016)Implementation Details

In this paper, we use random sampling to create subsets for each dataset. Each subset consists of 10 instances for demonstrations and one instance for a dummy query since we employ a 10-shot as the default ICL setting. The remaining instances are split into test and development sets with a 7:3 ratio. For experiments with multiple examples, we sample 100 instances instead of 10. We use "\(\rightarrow\)" as the separate token similar to previous works. We tried other tokens but no significant difference. All the experiments are reported over 5 random seeds. The inference mechanism with state vector we describe in SS4.1 has a key hyper-parameter (i.e.the layer \(L\)). Previous studies (Hendel et al., 2023) have shown that the choice of \(L\) has an influence on performance. We find the best layer for different tasks via the accuracy of the development set. For the inner optimization in SS4.2, we choose the last seven state vectors to optimize. This is because the early state vectors yield subpar performance, primarily due to limitations in the available examples. For the momentum optimization, we choose 0.5 as the retention rate for historical momentum from the options of 0.25, 0.5 and 0.75. We run all the experiments on a single NVIDIA A100 80G GPUs. Each of our experiments consumes between 10 minutes to 8 hours of GPU time, depending on the dataset.

## Appendix B More Details about Baseline

In this section, we present an in-depth and comprehensive analysis of two baselines (i.e. task vector (Hendel et al., 2023) and function vector (Todd et al., 2023)). Furthermore, we offer a more nuanced comparison with our proposed state vector, highlighting the distinct differences and advantages of our approach.

The task vector is designed to extract the ICL function from a specific layer's hidden state within the transformer model. This is achieved by directly replacing the corresponding hidden state during inference for intervention. On the other hand, Todd et al. (2023) first extracts the ICL function from the output activations across all attention heads in all transformer layers. These activations are then prioritized based on their causal effect, quantified by the variance in the model's output space with or without individual activation interventions. The mathematical average of the top 10 causal effect activations is the function vector, which is subsequently added to the hidden state of a specific layer during the inference stage.

In contrast to these methods, our approach for state vector extraction focuses on procuring the ICL procession state from the output activations of the attention heads within the first \(L\) layers. During inference, we replace the corresponding activations with optimized ones. While functionally equivalent to the forward process of the task vector when disregarding state vector optimization (i.e., the vanilla state vector), our approach offers enhanced mechanical explainability. This is attributable to its motivation from the dual form of in-context learning and gradient descent, as explicated in previous work (Dai et al., 2023; Natan et al., 2023). Furthermore, inspired by the dual form, we focus on the further optimization process. On the other hand, unlike the function vector which extracts activations based on the causal effects resulting from individual interventions, our method is rooted in the underlying mechanisms of ICL. This strategy not only improves mechanical explainability but also demonstrates greater performance as evidenced by extensive experiments. Experiments also show notably poor performance of the function vector on certain knowledge-based datasets, such as Person-Occupation.

## Appendix C More Details about Datasets

Here, we describe in detail the tasks that we use to evaluate the state vectors.

* **Antonym**(Nguyen et al., 2017) contains 2398 word pairs that are antonyms of each other (e.g. "massive" \(\rightarrow\) "tiny"). We apply the dataset processed version from the function vector (Todd et al., 2023). They filter the word pairs where both words can be tokenized as a single token.
* **Capitalize**(Todd et al., 2023) contains 813 word pairs that capitalize the first letter of the given input word (e.g. "plan" \(\rightarrow\) "Plan").
* **Present-Past**(Todd et al., 2023) contains 293 word pairs, where simple past tense verbs are output when given simple present tense verbs (e.g. "adapt" \(\rightarrow\) "adapted").

* **Singular-Plural**(Todd et al., 2023) contains 205 word pairs, where the plural form of a given singular word (e.g., "wallet" \(\rightarrow\) "wallets").
* **English-French**(Lample et al., 2018) contains 4698 pairs of words, which consists of a word in English and its translation into French (e.g., "circle" \(\rightarrow\) "cercle"). We apply the processed version from the function vector(Todd et al., 2023).
* **Country-Capital**(Todd et al., 2023) contains 197 instances, which output the name of the capital city of the given country (e.g. "Luanda" \(\rightarrow\) "Angola").
* **AG News**(Zhang et al., 2015) contains 7600 instances. Each instance contains the news headlines and the first few sentences of an article as input, and output corresponding labels include Business, Science, Sports, and World.
* **Person-Sport**(Hernandez et al., 2023) contains 318 instances. Each instance contains the name of a professional athlete and the sport that they play (e.g. "Hank Aaron" \(\rightarrow\) "basketball").
* **Person-Instrument**(Hernandez et al., 2023) contains 510 instances. Each instance contains the name of a professional musician and the instrument they play (e.g. "Tom Fletcher" \(\rightarrow\) "guitar").
* **Person-Occupation**(Hernandez et al., 2023) contains 821 instances. Each instance contains the name of a well-known individual and their occupation (e.g. "Tom Fletcher" \(\rightarrow\) "guitar").
* **Product-Company**(Hernandez et al., 2023) contains 522 instances. Each instance contains the name of a commercial product and the company that sells the product (e.g. "Tom Fletcher" \(\rightarrow\) "guitar").
* **Landmark-Country**(Hernandez et al., 2023) contains 836 instances. Each instance contains the name of a landmark and the country in which it is located.

## Appendix D Efficiency Analysis

In this section, we present an efficiency analysis of two proposed optimization methods. We evaluate the average inference time using 1000 test data on a single NVIDIA A100 (80G) GPU, covering six main datasets and 10 random seeds per dataset. The results are illustrated in Figure 5. In the zero-shot setting, we compress the ICL function into the state vector which eliminates the need to concatenate demonstrations during inference. As shown in the Figure 5, the proposed inner optimization and momentum optimization, which, while tripling the inference speed, achieve 89% of the regular ICL performance on Llama-2-7B and 78% on GPT-J-6B (see Table 1 in the paper). In the few-shot setting, the proposed inner optimization and momentum optimization achieve better results than standard ICL at the cost of a minimal loss in inference speed (e.g., 99% and 96%). Moreover, our method is orthogonal to attention speedup techniques, such as flash attention (Dao et al., 2022) and page attention (Kwon et al., 2023). Therefore, our approach can also benefit from the achievements of these works and achieve further efficiency improvement. We leave the exploration of alternative enhancement as future work.

Figure 5: Time efficiency analysis of Llama-2-7B and GPT-J-6B. Inn denotes our state vector with inner optimization. Mom denotes our state vector with momentum optimization

## Appendix E Natural Text Completions

In this study, we evaluate the effectiveness of the momentum optimized state vector on natural text completions. Given a natural text template, we instruct the model to greedily generate 5 tokens with or without intervention in the zero-shot setting. We use exact match accuracy as the metric. Table 3 shows the result of natural text completions on Llama-2. The performance boosts observed with the momentum-optimized state vector on the separate tokens indicate that it can guide the model to generate answers correctly. We include more examples of natural text completions in the Appendix.

## Appendix F Case Study

In this section, we present a case study shown in Table 4, to demonstrate the efficacy of the momentum-optimized state vector in natural text completions. Consider the query: "What is the meaning of biography?", The vanilla Llama-2 model would directly answer this question. However, when influenced by an English-French state vector, Llama-2 changes its response, translating the question into French instead. Similarly, when presented with the sentence "When I think of upright, I think of". Influenced by an Antonym state vector, Llama-2 completes the sentence with an anonymous pattern. These instances exemplify the model learning the ICL function stored in the momentum optimized state vector, enabling it to generate context relevant to the specified task.

## Appendix G Full Result

In this section, we provide the additional result with llama-2-7B GPT-J model. We first present the main result of optimization on the other six tasks except the main result, and the average performance across all tasks. As shown in Table 5, our inner optimization and momentum optimization effectively enhance the state vector.

Moreover, we provide the result of state vector aggregation on two additional datasets. As shown in Figure 6, the trends of both DSC and average aggregation follow a similar pattern to the main result shown in Figure 2 as the number of examples increases, illustrating the effectiveness of our aggregation methods.

\begin{table}
\begin{tabular}{l l} \hline \hline
**English-French** & \\ \hline
**Prompt** & What is the meaning of biography? \\ \hline Llama-2-7B & A written account of someone’s life. \\  + state vector & It is biographie. \\ \hline
**Antonym** & \\ \hline
**Prompt** & When I think of upright, I think of \\ \hline Llama-2-7B & I think of a person who is standing up \\  + state vector & for what they believe in. \\ \hline \hline \end{tabular}
\end{table}
Table 4: Natural prompt cases with momentum optimized state vector on Antonym task and English-French task.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Prompt** & **Llama-2-7B** & **+SV** \\ \hline What instrument did X play? & 8.7\(\pm\) 0.7 & 67.3\(\pm\) 2.8 \\ Can you tell me which musical instrument was played by X? & 25.1\(\pm\) 0.7 & 69.0\(\pm\) 4.3 \\ What was the primary instrument of X in their music career? & 17.3\(\pm\) 1.4 & 70.3\(\pm\) 2.9 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Text portability of momentum optimized state vector. The templates are provided with “X” replaced by a query word. “+SV” denotes adding momentum optimized state vector

[MISSING_PAGE_EMPTY:16]

diverse demonstrations or dummy queries. As illustrated in Figure 10, our analysis yields three key observations:

* The task vector and state vector exhibit greater sensitivity to dummy queries than to demonstrations. This finding suggests that dummy queries have a greater impact on performance compared to demonstrations, underscoring the importance of reducing the noise from dummy queries to enhance state vector performance.
* In the few-shot setting, both the task vector and the state vector (inn.) indicate significantly greater robustness compared to their performance in the zero-shot setting. There is a noticeable reduction in the standard deviation across diverse demonstrations or dummy queries when applying demonstrations during ICL inference. This improvement may be attributed to the richer ICL function information provided by demonstrations, which in turn bolsters performance stability.
* Compared to the task vector, our inner optimized state vector shows markedly enhanced robustness to the variations in demonstrations and dummy queries, in both zero-shot and few-shot settings. This highlights the effectiveness of our proposed inner optimization in improving the robustness of the state vector.

\begin{table}
\begin{tabular}{c|l|l|c c c|c} \hline
**Model** & \multicolumn{1}{c|}{**Method**} & **Antonym** & **English-French** & **Person-Instrument** & **Average** \\ \hline \multirow{8}{*}{Llama-2-13B} & \multirow{4}{*}{Zero-shot} & Regular & 1.2\(\pm\) 0.7 & 0.2\(\pm\) 0.2 & 0.0\(\pm\) 0.0 & 0.5 \\  & & Function vector & 47.1\(\pm\) 1.6 & 23.2\(\pm\)4.3 & 0.1\(\pm\) 0.1 & 23.5 \\  & & Task vector & 46.0\(\pm\) 2.4 & 43.1\(\pm\)7.2 & 58.2\(\pm\)6.3 & 49.1 \\  & & State vector (inn.) & 47.0\(\pm\) 1.2 & 50.5\(\pm\)1.9 & 66.6\(\pm\)3.1 & 54.7 \\  & & State vector (mom.) & 47.9\(\pm\) 1.1 & 55.9\(\pm\)3.4 & 68.5\(\pm\)2.0 & 57.4 \\ \cline{2-6}  & \multirow{4}{*}{Few-shot} & ICL baseline & **67.0\(\pm\) 0.1** & 74.5\(\pm\)1.3 & 75.0\(\pm\)0.2 & 72.2 \\  & & Function vector & 65.7\(\pm\) 1.7 & 75.2\(\pm\)2.6 & 72.2\(\pm\)0.4 & 71.3 \\  & & Task vector & 64.8\(\pm\) 1.2 & 70.5\(\pm\)3.5 & 70.6\(\pm\)3.1 & 68.6 \\  & & State vector (inn.) & 65.5\(\pm\) 0.8 & **75.8\(\pm\)1.6** & 77.0\(\pm\)1.3 & 72.8 \\  & & State vector (mom.) & 65.9\(\pm\) 0.7 & 75.6\(\pm\)0.4 & **78.6\(\pm\)1.1** & **73.4** \\ \hline \multirow{8}{*}{Llama-2-70B} & \multirow{4}{*}{Zero-shot} & Regular & 2.4\(\pm\) 1.5 & 0.6\(\pm\)0.5 & 0.0\(\pm\) 0.0 & 1.0 \\  & & Function vector & 48.3\(\pm\)9.5 & 29.9\(\pm\)0.3 & 5.3\(\pm\)0.3 & 27.8 \\ \cline{1-1}  & & Task vector & 63.3\(\pm\)2.1 & 48.8\(\pm\)9.5 & 63.5\(\pm\)6.9 & 58.5 \\ \cline{1-1}  & & State vector (inn.) & 65.5\(\pm\)1.0 & 56.0\(\pm\)6.6 & 67.3\(\pm\)5.6 & 62.9 \\ \cline{1-1}  & & State vector (mom.) & 66.7\(\pm\)1.1 & 57.8\(\pm\)5.3 & 71.9\(\pm\)4.9 & 65.5 \\ \cline{1-1} \cline{2-6}  & \multirow{4}{*}{Few-shot} & ICL baseline & **68.6\(\pm\)2.8** & 81.6\(\pm\)1.4 & 80.6\(\pm\)1.9 & 76.9 \\ \cline{1-1}  & & Function vector & 64.8\(\pm\)2.3 & 81.7\(\pm\)2.1 & 82.8\(\pm\)4.1 & 76.4 \\ \cline{1-1}  & & Task vector & 67.1\(\pm\)2.8 & 81.5\(\pm\)1.8 & 82.2\(\pm\)4.7 & 76.9 \\ \cline{1-1}  & & State vector (inn.) & 68.0\(\pm\)2.8 & **82.9\(\pm\)1.9** & 82.6\(\pm\)2.3 & 77.8 \\ \cline{1-1}  & & State vector (mom.) & 68.5\(\pm\)2.2 & 82.5\(\pm\)2.1 & **83.2\(\pm\)2.6** & **78.1** \\ \hline \end{tabular}
\end{table}
Table 6: Performance of state vector optimization across three tasks on Llama-2-13B and Llama-2-70B. The best results in the zero shot setting are in underline and the best results in the few shot setting are in **bold**. The result of basic state vector is mathematically equivalent to task vector.

Figure 7: Performance of aggregation on Llama-2-13B across number of examples. _Avg._ denotes the average aggregation baseline and _D&C_ denotes the divide-and-conquer aggregation. The **X** axis represents the number of examples, and the **Y** axis represents the accuracy.

## Appendix K Limitation

The definition of state vectors is contingent upon specific assumptions and lacks a rigorous theoretical foundation, which may impact its generalizability and reliability across different NLP tasks. Additionally, the experiments were conducted on a limited scale with moderate-sized models and datasets. These constraints may affect the applicability of the results to larger models or more complex datasets. Further research will explore these aspects to establish a more robust validation of the proposed methods.

Figure 8: Performance of aggregation on Llama-2-70B across number of examples. _Avg_. denotes the average aggregation baseline and _D&C_. denotes the divide-and-conquer aggregation. The **X** axis represents the number of examples, and the **Y** axis represents the accuracy.

Figure 10: Standard deviation of performance on Llama-2 across three datasets.

Figure 9: The 2D PCA visualization of the state vector in the Antonym task and English-French task of **GPT-J**, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration and the outlier is of the first order.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: we provide our discussion of limitations in the Appendix K. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: the paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: the paper fully disclose all the information and details of main experimental results in the Section 5.1 and Appendix A Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: we provide the instructions, data, codes and scripts of the main experimental results in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: we provide all the training and test detailed description of the experimental setup and details in Section 5.1 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: we provide the standard error of the main results in Table 1, Table 5, Table 6, Table 3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: we provide the information on the computer resources in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the research conducted in the paper fully conforms to the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: there is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: all creators and original owners of assets used in the paper, including code, data, and models, are properly credited, and the licenses and terms of use are explicitly mentioned and fully respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: this paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.