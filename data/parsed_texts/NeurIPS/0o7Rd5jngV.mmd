# Understanding the Expressive Power and Mechanisms

of Transformer for Sequence Modeling

Mingze Wang

School of Mathematical Sciences, Peking University, Beijing, China

mingzewang@stu.pku.edu.cn

&Weinan E \(\dagger\)

Center for Machine Learning Research and School of Mathematical Sciences, Peking University, Beijing, China

AI for Science Institute, Beijing, China

weinan@math.pku.edu.cn

###### Abstract

We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads. These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures.

## 1 Introduction

In recent years, Transformer networks (Vaswani et al., 2017) have emerged as foundational models, setting new benchmarks across various domains, including natural language processing (NLP), computer vision (CV), and protein folding. Despite their impressive practical achievements, the underlying mechanisms and theoretical foundations of Transformer networks remain largely elusive.

Transformer networks encompass various components, posing challenges to their comprehensive understanding. A typical Transformer comprises multiple layers, each consisting of a multi-head self-attention (Attn) sub-layer and a feed-forward network (FFN) sub-layer, integrated with residual blocks. FFN is a two-layer nonlinear network, while Attn includes dot-product (DP) and positional encoding (PE). To get a better understanding of how Transformer works in practice, we need to study several key issues. These include:

**(i)**_How do the key hyper-parameters, for example, the number of layers, the number of Attn heads and the with of FFN layers, affect the performance of the Transformer network?_
**(ii)**_How do the Attn and FFN layers contribute differently to the overall performance?_
**(iii)**_How does DP attention work, and is the DP structure necessary?_
**(iv)**_How efficient is PE in modeling long-range correlations?_

Extensive empirical research on Transformer components has led to the proposal of numerous alternatives to the current structure of Transformer. For example, several relative positional encodings (RPE) (Shaw et al., 2018; Raffel et al., 2020; Su et al., 2024; Press et al., 2022) have been proposedto substitute the original absolute positional encoding (APE), yielding superior performance in challenging tasks like length generalization (Ontanon et al., 2022; Csordas et al., 2021; Anil et al., 2022). Additionally, the necessity of the computationally expensive DP in Attn layers has been widely questioned, and researchers proposed numerous alternatives of DP that show considerable efficacy in specific tasks (Kitaev et al., 2020; Wang et al., 2020; Choromanski et al., 2020; Tay et al., 2021; Allen-Zhu and Li, 2023). Nonetheless, these explorations have not yielded a satisfactory theoretical understanding of the mechanisms of these components.

**In this work,** we investigate the expressive power of Transformer and the underlying mechanisms of its components for sequence modeling. **Our contributions** are summarized as follows:

We categorize three types of sequence modeling tasks with varying complexity, which are relevant to a broad spectrum of application areas. _Task I: Modeling fixed, long but sparse memories_. This is relevant to sparse Boolean functions and the traditional \(n\)-gram model in NLP. _Task II: Modeling adaptive, long but sparse memories_. This is relevant to multi-step reasoning tasks as well as various NLP tasks such as dependency parsing, sentiment analysis, and continuation writing. _Task III: Modeling essentially sparse memories_. Examples include feature representation in CV and wavelet analysis in classical signal processing.

For these sequence modeling tasks, we theoretically investigate the expressive power of Transformer and its variants, establishing explicit approximation rates. Our meticulous analysis provides _theoretical insights_ into the underlying mechanisms of Transformer components. Specifically,

* **The distinct roles of the number of layers, the number of Attn heads, and the width of FFN layers.** Deeper Transformer are capable of handling memories with more intricate interrelationships, such as nested relationships (Thm 4.4). In contrast, for memories lacking such interrelationships, single-layer Transformer with sufficient number of Attn heads and FFN width should suffice (Thm 4.1). This is quite intuitive: If the content of the next token relies on a few previous tokens in an independent way, we can treat each such dependence by a separate attention head. There is no need for many layers. Additionally, increasing the depth can also alleviate the reliance on the number of heads and width (Prop 4.5).
* **The different roles of Attn layers and FFN layers.** Our results consistently suggest that: FFN layers are tasked with approximating nonlinear memory functions and the readout function, while Attn layers are responsible for extracting the tokens from these memory locations.
* **The functionality and necessity of DP.** For the relatively simple Task I, DP is not necessary and can be omitted (Thm 3.1). However, for the more complex Task II, the cooperation between DP and RPE provides the needed interaction between the temporal space and the token space, crucial for the extraction of adaptive memories (Thm 4.1 and 4.4). Additionally, for Task II, while the nonlinearity provided by DP is necessary (Prop 4.2), a computationally efficient alternative to DP exists, as we show in Prop 4.3.
* **The efficiency of RPE in modeling long-range correlations.** Our results consistently suggest that the primary role of RPE is to approximate the memory kernels. Specifically, for Task III, we demonstrate that Transformer with suitable RPE can handle heavy-tailed memories, thus overcoming the Curse of Memory faced by recurrent neural networks (Thm 5.1). Moreover, our findings give theoretical support to the choice of RPE in practice.

Finally, we conduct experiments to validate our theoretical insights.

## 2 Preliminaries

**Basic notations.** We use bold-faced letters for vectors or matrices and lowercase letters for scalars, e.g. \(\mathbf{x}=(x_{1},\cdots,x_{d})^{\top}\in\mathbb{R}^{d}\) and \(\mathbf{W}=(W_{ij})_{m\times n}\in\mathbb{R}^{m\times n}\). The standard Euclidean inner product between two vectors is denoted by \(\langle\cdot,\cdot\rangle\), and the \(l_{p}\) norm of a vector is represented by \(\left\lVert\cdot\right\rVert_{p}\). We employ standard big-O notations \(\mathcal{O},\Omega,\Theta\) to hide absolute positive constants and use \(\tilde{\mathcal{O}},\tilde{\Omega},\tilde{\Theta}\) to further hide logarithmic constants. For any positive integer \(n\), let \([n]=\{1,\cdots,n\}\). Denote by \(\mathbb{I}\{E\}\) the indicator function for an event \(E\). Denote by \(a\lor b=\max\{a,b\}\) for real number \(a,b\).

### Sequence modeling with long but sparse memories

**Sequence modeling.** For convenience, we consider input sequences of infinite length \((t\in\mathbb{Z})\). It is important to note, however, that our theoretical framework can be adapted to finite-length input sequences by masking distant tokens. Formally, the output sequence \(\mathbf{Y}=(\mathbf{y}_{t})_{t\in\mathbb{Z}}\in\mathbb{R}^{c\times\mathbb{Z}}\) is generated from the input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\in\mathcal{X}\subset\mathbb{R}^{d\times \mathbb{Z}}\) via an unknown mapping \(\mathbf{H}\left(\cdot\right)\) dependent on the input sequence up to the prediction time, and this can be expressed as:

\[\mathbf{y}_{t}=\mathbf{H}_{t}(\mathbf{X})=\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-1},\mathbf{x}_{t-2}, \cdots),\quad t\in\mathbb{Z}. \tag{1}\]

Our objective is to learn the mapping \(\mathbf{H}\left(\cdot\right)\). Additionally, we define the norm \(\left\|\mathbf{H}\right\|:=\sup_{t\in\mathbb{Z}}\sup_{\mathbf{X}\in\mathcal{X}} \left\|\mathbf{H}_{t}(\mathbf{X})\right\|\). Without loss of generality, we assume \(\left\|\mathbf{x}_{t}\right\|_{2}\leq 1\) for any \(\mathbf{X}\in\mathcal{X}\) and set the output dimension \(c=1\) for simplicity.

**Long but sparse memories.** To model such sequences, we define three types of memories: fixed, long but sparse memories; adaptive, long but sparse memories; and essentially sparse memories. These memory types are prevalent in sequence modeling tasks across diverse domains such as NLP, CV, signal processing, and sparse function representation. In Section 3, 4, and 5, we will formally define these different types and investigate Transformer's capacity to model them.

### Transformer architecture

**Transformer network.** Transformer (Vaswani et al., 2017) is a network architecture designed for processing sequences and generating predictions. Given an input sequence \(\mathbf{X}\), Transformer executes the following steps. Initially, each \(d\)-dimensional (dim) input token is transformed into a \(D\)-dim vector through an embedding mapping such as \(\mathbf{x}_{t}^{(0)}=\mathbf{W}_{E}\mathbf{x}_{t}+\mathbf{b}_{E}\), where \(\mathbf{W}_{E}\in\mathbb{R}^{D\times d},\mathbf{b}_{E}\in\mathbb{R}^{D}\). Subsequently, a typical \(L\)-layer Transformer with residual block operates according to the formulation:

\[\begin{split}\mathbf{X}^{(l-\frac{1}{2})}&=\mathbf{X}^{(l-1 )}+\mathbf{Attn}^{(l)}(\mathbf{X}^{(l-1)}),\quad l\in[L];\\ \mathbf{X}^{(l)}&=\mathbf{X}^{(l-\frac{1}{2})}+\mathbf{FFN}^{ (l)}(\mathbf{X}^{(l-\frac{1}{2})}),\quad l\in[L].\end{split} \tag{2}\]

At the \(l\)-th layer, \(\mathbf{FFN}^{(l)}(\cdot)\) denotes a standard (point-wise) two-layer ReLU networks with \(m\) neurons: for a given input \(\mathbf{x}\in\mathbb{R}^{D}\), \(\mathbf{FFN}^{(l)}(\mathbf{x})=\sum_{k=1}^{m}\mathbf{a}_{k}^{(l)}\sigma\big{(}\mathbf{b}_ {k}^{(l)\top}\mathbf{x}+c_{k}^{(l)}\big{)}\), where \(\sigma(\cdot)\) is the activation function such as ReLU. Additionally, in the final (\(L\)-th) FFN layer, the residual block is omitted, commonly referred to as the readout function. Moreover, \(\mathbf{Attn}^{(l)}(\cdot)\) refers to a multi-head self-attention, as elaborated below.

**Multi-head self-attention.** Our focus lies on standard dot-product Attn, denoted as \(\mathbf{Attn}^{(l)}(\cdot)\) and consisting of \(H\) heads. When applied to an input sequence \(\mathbf{X}\), Attn operates as follows:

\[\mathbf{Attn}^{(l)}(\mathbf{X})=\mathbf{W}_{O}^{(l)}\sum_{h=1}^{H}\mathbf{W}_{V}^{(l,h)} \mathbf{X}\mathrm{softmax}_{c}\left(\left\langle\mathbf{W}_{Q}^{(l,h)}\mathbf{X},\mathbf{W}_{ K}^{(l,h)}\mathbf{X}\right\rangle+\mathbf{R}^{(l,h)}\right). \tag{3}\]

Here, the parameters \(\mathbf{W}_{Q}^{(l,h)},\mathbf{W}_{K}^{(l,h)},\mathbf{W}_{V}^{(l,h)},\mathbf{W}_{O}^{(l,h)}\) correspond to the query, key, value, output matrices of the \((l,h)\)-th head, respectively. \(\mathrm{softmax}_{c}\) represents taking softmax normalization across column. Furthermore, \(\mathbf{R}^{(l,h)}\in\mathbb{R}^{2\times\mathbb{Z}}\) denotes the relative positional encoding matrix, which satisfies \(R_{t,s}^{(l,h)}=-\infty\) for \(t<s\) in the next-token prediction paradigm. Consequently, the \(t\)-th output of Attn is expressed as:

\[\mathbf{Attn}_{t}^{(l)}(\mathbf{X})=\mathbf{W}_{O}^{(l)}\sum_{h=1}^{H}\sum_{s=0}^{+ \infty}\frac{\mathbf{W}_{V}^{(l,h)}\mathbf{x}_{t-s}\exp\left(\left\langle\mathbf{W}_{Q}^{( l,h)}\mathbf{x}_{t},\mathbf{W}_{K}^{(l,h)}\mathbf{x}_{t-s}\right\rangle+R_{t,t-s}^{(l,h)} \right)}{\sum_{j=0}^{+\infty}\exp\left(\left\langle\mathbf{W}_{Q}^{(l,h)}\mathbf{x}_{t },\mathbf{W}_{K}^{(l,h)}\mathbf{x}_{t-j}\right\rangle+R_{t,t-j}^{(l,h)}\right)}.\]

**Logarithmic and Power relative positional encoding.** As highlighted in Section A, among various types of RPEs, the RPEs used in T5 and KERPLE(log) demonstrate superior performance over Alibi, significantly outperforming other RPEs and APEs in the length generalization task (Kazemmejad et al., 2023; Chi et al., 2022). This finding motivates our focus on the T5-type, KERPLE(log), and Alibi-type RPEs throughout this paper. All of these RPE matrices are Toeplitz, with the form of\(R_{t,s}=r(t-s)\). Notably, for T5 and KERPLE(log), \(r(t-s)\) undergoes an initial linear decrease followed by a logarithmic decrease as the relative distance \(t-s\) increases (Please refer to Section G.1 for more details). In contrast, for Alibi, \(r(t-s)\) decreases linearly. Inspired by these discussions, we examine the following RPEs with different decay rates:

\[\phi_{\log}(z)=\begin{cases}-\log z,&z\geq 1\\ -\infty,&\text{otherwise}\end{cases};\quad\phi_{\lim}(z)=\begin{cases}-z,&z\geq 0 \\ -\infty,&\text{otherwise}\end{cases}.\]

We will study Transformer with \(\phi_{\texttt{type}}\) RPE (\(\texttt{type}\in\{\text{log},\text{lin}\}\)). Specifically, the RPE in the \((l,h)\)-th head (3) is as follows:

\[R_{t,s}^{(l,h)}:=p^{(l,h)}\phi_{\texttt{type}}(t-s), \tag{4}\]

where \(p^{(l,h)}\in\mathbb{R}_{+}\) is a trainable parameter.

**Remark 2.1**.: For standard Transformer (2) incorporating Attn (3) with RPE (4), the parameters are: the embedding matrix \(\mathbf{W}_{E}\); \(\mathbf{a}_{k}^{(l)},\mathbf{b}_{k}^{(l)},c_{k}^{(l)}\) in the FFN layers; \(\mathbf{W}_{Q}^{(l,h)},\mathbf{W}_{K}^{(l,h)},\mathbf{W}_{V}^{(l,h)},\)\(p^{(l,h)},\mathbf{W}_{O}^{(l)}\) in the Attn layers. Notably, the number of parameters is independent of the sequence length, thus enabling the model to handle input sequences of arbitrary length.

**Remark 2.2**.: In the subsequent sections, we will analyze Transformer and its variants. For the sake of brevity, some shorthand notations are introduced here. For examples, Transformer (2) using \(\phi_{\log}\)/\(\phi_{\text{lin}}\) RPE (4) is referred to as "Transformer with \(\log\)/lin-RPE"; Transformer with \(\mathbf{W}_{Q}^{(l,h)},\mathbf{W}_{K}^{(l,h)}=\mathbf{0}\) is called "dot-product-free Transformer".

### Expressive power via approximation theory

This paper delves into the expressive power of Transformer through the lens of approximation theory, with a specific focus on establishing explicit approximation rates for Transformers in modeling long but sparse memories.

**Approximation rates v.s. universal approximation.** In approximation theory, results are generally categorized into two types: universal approximation (density-type) and approximation rates (Jackson-type) (Jackson, 1930). Universal approximation investigates whether the hypothesis class is dense in the target class. Although this property is fundamental, it does not offer detailed insights into approximation efficiency. In contrast, approximation rates go deeper, emphasizing the efficiency of the approximation. A typical example within this framework is the approximation theory of two-layer neural networks (2NNs).

**Barron space of 2NNs**. The well-known universal approximation result for 2NNs asserts that 2NNs can approximate any continuous function (Barron, 1992, 1993, 1994). Nonetheless, this result lacks a characterization of the approximation efficiency, i.e., how many neurons are needed to achieve a certain approximation accuracy? This gap was addressed by the Barron space theory (E et al., 2019, 2021, Ma et al., 2020). It is established that for any function within Barron space \(f\in\mathcal{B}\) (Appendix G.2), 2NNs with \(m\) neurons (denoted by \(\mathcal{H}_{m}\)) can approximate them efficiently, at a rate of \(\inf_{f_{m}\in\mathcal{H}_{m}}\|f-f_{m}\|\leq\mathcal{O}(\|f\|_{\text{g}}/ \sqrt{m})\), remarkably independent of the input dimension \(d\), thus avoiding the _Curse of Dimensionality_(Bellman, 1966, Bach, 2017).

## 3 Fixed, long but \(M\)-sparse memories

### Problem formulation

**Fixed, long but \(M\)-sparse memories.** In this section, we investigate a fundamental category of long but sparse memories. Our focus is on scenarios where the positions of the sparse memories remain fixed and are independent of the tokens. The target function is represented by:

\[y_{t}=f(\mathbf{x}_{t},\mathbf{x}_{t-T_{1}},\cdots,\mathbf{x}_{t-T_{M}}), \tag{5}\]

where \(1\leq T_{1}<\cdots<T_{M}<+\infty\) signify the fixed positions of the memories. Despite the memories being fixed (token-independent) and sparse (finite \(M\)), the task can still be complex due to the potentially long-range memories (\(T_{1},\cdots,T_{M}\) can be large enough).

**Examples.** (**I**) For Boolean inputs, (5) aligns with _sparse Boolean functions_, also studied in (Edelman et al., 2022; Bhattamisha et al., 2022). Notably, Bhattamisha et al. (2022) observed that Transformers outperform LSTMs in learning sparse parities. (II) Selecting the simplest case of \(T_{i}=i\) in (5) corresponds to the traditional _\(n\)-gram model_, which consists of short and sparse memories.

**Target class.** We focus on target functions described in (5). The readout function \(f\) is considered within the standard Barron space \(\mathcal{B}\), i.e., which can be effectively approximated by 2NNs. Moreover, we assume that \(f\) is Lipschitz, denoted by \(f\in\mathcal{L}\). Thus, we can focus more on investigating the memory extraction power of Transformer. Formally, we define the target class for modeling fixed, long but \(M\)-sparse memories as:

\[\mathcal{H}^{\mathrm{Fix}}:=\big{\{}\mathbf{H}:\ \mathbf{H}_{t}(\mathbf{X})= \eqref{eq:2},\ \text{where}\ 1\leq T_{1}<\cdots<T_{M}<+\infty,f\in\mathcal{B} \cap\mathcal{L}\big{\}}. \tag{6}\]

**Transformer hypothesis class.** As mentioned in Section 1, one of our main aims is to study the necessity and roles of different components in Transformer, such as DP and RPE. This section focuses on the "simplest" one-layer Transformer and investigates whether it can effectively model this task. Formally, our hypothesis class includes all one-layer _DP-free_ Transformers, configured with \(H\)Attn heads and FFN width \(m\):

\[\mathcal{T}\mathcal{F}^{\mathrm{DPF},\mathtt{type}}_{(1,H,m)}:= \big{\{}\mathbf{TF}:\mathbf{TF}\text{ is a $1$-layer, $H$-head, $m$-width} \tag{7}\] \[\text{ dot-product-free Transformer with type-RPE}\big{\}}.\]

### Theoretical results and insights

**Theorem 3.1** (Approximation rate).: _For any target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Fix}}\) (6), rate \(n\in\mathbb{N}_{+}\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\mathrm{DPF},\mathtt{type}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}-\mathbf{TF}\right| \kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\right|\leq\mathcal{E}_{ \mathrm{FFN}}+\left|\kern-1.075pt\left|\kern-1.075pt\left|f\right|\kern-1.075pt \right|\kern-1.075pt\right|_{\mathrm{Lip}}\mathcal{E}_{\mathrm{Attn}}(\mathtt{ type}),\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left|\kern-1.075pt \left|\kern-1.075pt\left|f\right|\kern-1.075pt\right|\kern-1.075pt\right| \kern-1.075pt\right|_{\mathrm{B}}}{\sqrt{m}}\right)\) and \(\mathcal{E}_{\mathrm{Attn}}(\mathtt{type})=\begin{cases}\mathcal{O}\Big{(} \frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}e^{0.01T_{i}}\right)_{+}^{n+1}\Big{)}, \mathtt{type}=\mathrm{lin}\\ \mathcal{O}\Big{(}\frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}T_{i}^{1.01}\right)_{ +}^{n+1}\Big{)},\mathtt{type}=\mathrm{log}\end{cases}.\)_

Theorem 3.1 establishes the _approximation rate_ of one-layer DP-free Transformer for modeling fixed, long but sparse memories. Here, the model complexity is governed by the number of Attn heads \(H\) and the width of FFN layers \(m\), while the target complexity arises from the lengths of the memories \(T_{1},\cdots,T_{M}\) and the complexity of the readout function \(f\). The approximation error comprises two components: the error in the FFN component \(\mathcal{E}_{\mathrm{FFN}}\) and the error in the Attn component \(\mathcal{E}_{\mathrm{Attn}}(\mathtt{type})\). The error \(\mathcal{E}_{\mathrm{FFN}}\) aligns with classical results, showcasing its effectiveness in approximating Barron functions. On the other hand, \(\mathcal{E}_{\mathrm{Attn}}(\mathtt{type})\) hinges on the capacity of the Attn block for modeling long-range memories. Specifically, with increasing memory length, the necessary number of Attn heads grows at a small exponential rate for \(\mathrm{lin}\)-RPE and at a polynomial rate for \(\mathrm{log}\)-RPE.

The proof of Theorem 3.1 is deferred to Appendix B. We can draw some insights from Theorem 3.1 and its proof.

**Different roles of the Attn layer and the FFN layer.** The Attn and FFN layers fulfill distinct roles in this task. Specifically, the FFN layer efficiently approximates the nonlinear readout function \(f\), while the Attn layer is responsible for extracting the token \(\mathbf{x}_{t-T_{i}}\) by approximating the memory kernel \(\mathbb{I}\{\cdot=T_{i}\}\). These components together enable effective modeling of fixed, long, but sparse memories.

**Non-necessity of DP.** Theorem 3.1 suggests that the DP component in Attn is not necessary and can be omitted for modeling fixed, long but sparse memories. This is due to the relative simplicity of modeling fixed memory kernels. In a more complex scenario in Section 4, the role of the dot-product becomes important. In contrast to Edelman et al. (2022), which utilizes the property of DP to prove that Transformer can model sparse Boolean functions, our result reveals that one-layer Transformer can successfully tackle the same task _even without the dot product in the attention layer_.

**Effect of RPE types on expressivity.** Our result indicates that the type of the RPE used in the Attn layer subtly influences the Transformer's ability to model long-range memories. As the range of the memory increases, the required head number grows at a slightly exponential rate for \(\mathrm{lin}\)-RPEand at a polynomial rate for \(\log\)-RPE. The subtle difference is attributed to the relative simplicity of approximating the memory kernel \(\mathbb{I}\{\cdot=T_{i}\}\). We will explore a more complex task in Section 5, where the impact of different types of RPE becomes even more pronounced.

## 4 \(K\)-Adaptive, long but \(M\)-sparse memories

### Problem formulation

In this section, we delve into a more complex modeling scenario closely aligned with typical language processing tasks.

\(K\)**-Adaptive, long but \(M\)-sparse memories.** This section investigates the scenario where the positions of the sparse memories are "adaptive", meaning they depend on the input tokens. The target function is formulated as:

\[y_{t}=f(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}}), \tag{8}\]

where the positions of the memory tokens \(t_{1},\cdots,t_{M}\) follow a nested relationship:

\[\begin{split} t_{1}=g_{1}(\mathbf{x}_{t});t_{2}=g_{2}(\mathbf{x}_{t}, \mathbf{x}_{t-t_{1}});\cdots;t_{K+1}=g_{K+1}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots, \mathbf{x}_{t-t_{K}});\\ \cdots;t_{M}=g_{M}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t _{K}}).\end{split}\]

Here, \(M\) denotes the number of memory tokens, and \(K\) measures the nesting complexity in the memory structure. We assume that memory functions \(g_{i}\) generate positive integers for the input tokens, and there exist maximum values \(T_{i}\) such that \(g_{i}\leq T_{i}\). In this adaptive framework, each position of the memory token depends on multiple input tokens and is nested within other memory structures, leading to potential influence of later memory tokens by the earlier ones.

To facilitate understanding, we first consider a warm-up case, i.e., \(K=0\) in (8). In this case, the positions of memories only depend on the current token, without interaction with each other. It can be represented as:

\[y_{t}=f(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}}), \tag{9}\]

where \(t_{i}=g(\mathbf{x}_{i}),i\in[M]\).

**Target class.** The target classes for modeling adaptive, long but sparse memories in both warm-up and general cases are as follows:

\[\mathcal{H}^{\rm Adap}_{(1,M)}:=\big{\{}\mathbf{H}:\ \mathbf{H}_{t}(\mathbf{X})= \eqref{eq:model},\ \text{where}\ g_{i}\in\mathcal{B},1\leq g_{i}\leq T_{i},i\in[M];f \in\mathcal{B}\cap\mathcal{L}\big{\}}. \tag{10}\]

\[\mathcal{H}^{\rm Adap}_{(K,M)}:=\big{\{}\mathbf{H}:\ \mathbf{H}_{t}(\mathbf{X})= \eqref{eq:model},\ \text{where}\ g_{i}\in\mathcal{B},1\leq g_{i}\leq T_{i},i\in[M];f \in\mathcal{B}\cap\mathcal{L}\big{\}}. \tag{11}\]

**Examples.** Adaptive memories are commonly encountered in practical scenarios. (I) _Adaptive sparse Boolean functions_, e.g., \(y_{t}=x_{t}\cdot x_{t-g(x_{t})}\cdot x_{t-g(x_{t-g(x_{t})})}\), where \(\mathbf{X}\in\{\pm 1\}^{\mathbb{Z}}\), \(g(x)=1\) for \(x=1\) and \(g(x)=2\) for \(x=-1\). This fits within our framework (8) with \(K=M=2\). (II) _Multi-step reasoning_, e.g., modeling the \(K\)-adaptive, long, but \(K\)-sparse memories contains a complicated \(K\)-step reasoning task, which require the sequential search following the rule \(((\cdots((x_{t}\mapsto x_{t-t_{1}})\mapsto x_{t-t_{2}}\cdots)\mapsto x_{t-t_{ K-1}})\mapsto x_{t-t_{K}}\). (III) In _NLP tasks_ like dependency parsing, part-of-speech tagging, sentiment analysis, or continuation writing, the positions of relevant prefix tokens usually depend on the context itself, and can vary depending the content. Additionally, the nested structure is a fundamental characteristic of natural language (Hawkins, 2021).

**Transformer hypothesis class.** Some previous works Yun et al. (2019); Kim et al. (2022) treated the softmax with normalization as an approximation of hardmax, suggesting the potential importance of the normalization. In contrast, in this section, we remove the normalization in the denominator of softmax and investigate its ability for sequence modeling. Additionally, to address the discreteness of time and memory values, we consider Transformer with specific precision, as detailed in Appendix C. The precision technique is widely used in LLM training (Kalamkar et al., 2019), such as BFloat16. Formally, the hypothesis class is defined as follows, encompassing all normalization-free \(L\)-layer Transformer, configured with \(H\) Attn heads and FFN width \(m\) and using type-RPE and specific precision.

\[\mathcal{T}\mathcal{F}^{\text{type}}_{(L,H,m)}:=\big{\{}\mathbf{TF}: \mathbf{TF}\ \text{is an}\ L\text{-layer,}\ H\text{-head,}\ m\text{-width} \tag{12}\] \[\text{Transformer with type-RPE and specific precision}\big{\}}.\]

### Theoretical results and insights: The warm-up case

**Theorem 4.1** (Approximation rate, warm-up case).: _For any target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Adap}}_{(1,M)}\) (8), rate \(n\in\mathbb{N}_{+}\), and \(H,m\in\mathbb{N}_{+}\), there exists a two-layer Transformer \(\mathbf{TF}\in\mathcal{TF}^{\mathsf{type}}_{(2,H,m)}\) (12) and a constant \(C(n)\) such that: if the width satisfies \(m\geq\begin{cases}\tilde{\Omega}\big{(}\sum_{i=1}^{M}\left\lVert g_{i}\right\rVert _{\mathcal{B}}^{2}\big{)}&,\mathsf{type}=\ln\\ \tilde{\Omega}\big{(}\sum_{i=1}^{M}\left\lVert\log g_{i}\right\rVert_{\mathcal{ B}}^{2}T_{i}^{2}\big{)}&,\mathsf{type}=\log\end{cases}\), then the following approximation rate holds:_

\[\left\lVert\!\left\lVert\mathbf{H}-\mathbf{TF}\right\rVert\!\right\rVert\leq \mathcal{E}_{\mathrm{FFN}}+\left\lVert f\right\rVert_{\mathrm{Lip}}\mathcal{E }_{\mathrm{Attn}}(\mathsf{type}),\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\lVert f\right \rVert_{\mathcal{B}}}{\sqrt{m}}\right)\) and \(\mathcal{E}_{\mathrm{Attn}}(\mathsf{type})=\begin{cases}\mathcal{O}\Big{(} \frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}e^{0.01T_{i}}\right)^{n+1}\Big{)}&, \mathsf{type}=\ln\\ \mathcal{O}\Big{(}\frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}T_{i}^{1.01}\right)^{n +1}\Big{)}&,\mathsf{type}=\log\end{cases}\)._

In Theorem 4.1, we present the _approximation rate_ of two-layer Transformer for the warm-up case: modeling \(1\)-adaptive, long but \(M\)-sparse memories. This theorem reveals that the approximation error comprises two distinct components: the error in the FFN component \(\mathcal{E}_{\mathrm{FFN}}\) and the error in the Attn component \(\mathcal{E}_{\mathrm{Attn}}(\mathsf{type})\). A critical difference from 3.1 is the presence of the condition related to the width \(m\) of FFN layers. This term arises from using the FFN layer to approximate the memory function \(g_{i}\). Owing to the discreteness of memory \(g_{i}\) and the implementation of rounding operations, the approximation within rounding accuracy all achieves zero error after rounding, while it can not get correct rounding beyond this accuracy. In contrast, the error \(\mathcal{E}_{\mathrm{FFN}}\) is caused by using FFN to approximate the readout function \(f\), the same as \(\mathcal{E}_{\mathrm{FFN}}\) in Theorem 3.1.

The proof of Theorem 4.1 can be found in Appendix C.1. Theorem 4.1 and its proof offer several critical insights into the underlying mechanism of Transformer.

**Distinct roles of Attn layers and FFN layers.** Our proof elucidates that the FFN layers are tasked with approximating the readout function \(f\) and memory functions \(g_{i}\), while the Attn layers are responsible for the extraction of the adaptive memories. It is essential to clarify the difference between "approximating memory functions" and "memory extraction". The former refers to utilizing some function to estimate the memory function \(g_{i}\), whereas the latter pertains to extracting the token \(\mathbf{x}_{t-g_{i}(\mathbf{x}_{t})}\) from the memory location.

**Cooperation between DP and RPE.** In the \(2\)-nd Attn layer, the extraction of the memory functions is achieved through an interplay between DP and RPE. Specifically, this is done through _a nice interaction between the temporal space (provided by RPE) and the token space (provided by DP)_. Please refer to Appendix C.1 for more details.

**Rethinking DP in Attn.** Our proof highlights that the core mechanism of Attn is to provide a nice interaction between the temporal space and the token space through the cooperation of DP and RPE. This leads us to the following question: _Is DP in Attn necessary and replaceable?_ The following two propositions provide some hints.

**Proposition 4.2** (DP vs. DP-free (informal)).: _There exists a target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Adap}}_{(1,1)}\) (10) such that:_

_(A) For any \(\epsilon>0\), there exists a \(1\)-layer Attn \(\mathbf{Attn}^{\mathrm{DP}}\) such that \(\left\lVert\!\left\lVert\mathbf{H}-\mathbf{Attn}^{\mathrm{DP}}\right\rVert\! \right\rVert\leq\epsilon\)._

_(B) For any \(1\)-layer DP-free Attn \(\mathbf{Attn}^{\mathrm{DPF}}\), a uniform lower bound holds: \(\left\lVert\!\left\lVert\mathbf{H}-\mathbf{Attn}^{\mathrm{DPF}}\right\rVert\! \right\rVert\geq\frac{2}{3}\)._

Proposition 4.2 reveal a significant distinction in the expressiveness of two network types for modeling adaptive, long, but sparse memories. Specifically, \(1\)-layer Attn with DP can effectively model this task, while \(1\)-layer DP-free Attn provably fails. This finding underscores the essential role of DP in providing the necessary nonlinearity for Attn to model adaptive memories. The formal version of Proposition 4.2 and its proof can be found in Appendix C.2.

**Proposition 4.3** (Substitute for DP (informal)).: _There exists a substitute structure for DP, requiring only \(\mathcal{O}(D)\) parameters (compared to \(\mathcal{O}(D^{2})\) in standard DP) that can effectively model \(\mathbf{H}\in\mathcal{H}^{\mathrm{adap}}_{(1,M)}\) (10). Specifically, if we substitute DP with this structure, \(1\)-layer Transformer can achieve the same approximation rate as stated in Section 4.1._

[MISSING_PAGE_FAIL:8]

to the _sum_ of all the memory functions' complexity (\(\left\|g_{i}\right\|_{\mathcal{B}},\left\|\log g_{i}\right\|_{\mathcal{B}},T_{i}\) for memory function \(g_{i}\)). In contrast, for \(M+1\)-layer Transformer, the required width correlates with the _maximum_ complexity of the memory functions, much lower than that for \(2\)-layer Transformer. Similarly, the required number of heads for \(M+1\)-layer Transformer is much fewer than that for \(2\)-layer Transformer. Please refer to Appendix D.2 for a detailed comparison. The observation suggests that increased depth can significantly reduce the demands on the number of heads and the width. The underlying reason is that deep networks can distribute the memories across different layers for processing, with each layer focusing on approximating only a single memory function.

## 5 Essentially \(M\)-sparse memories

### Problem formulation

In language tasks, each token possesses clear semantic meaning. As a result, the structure of the memory is sparse in the original space. This aligns well with our modeling assumptions discussed in Section 3 and 4. However, in other machine learning tasks, we may encounter situations where the input tokens lack distinct semantic meaning. This might happen in image processing or classical signal processing. In these situations, the memory structure could potentially be dense in the original space. Nonetheless, the memory structure might exhibit sparsity in some transformed domain. We call such memory structure "essentially sparse". In this section, we study the situation in which the memory structure in long-ranged but essentially sparse. For simplicity, we consider the situation in which the positions of the memory kernels are fixed. The analysis can be easily extended to the situation with an adaptive memory structure.

**Fixed, essentially \(M\)-sparse memory.** Consider the following situation:

\[y_{t}=f\left(\left(\mathbf{X}*\rho_{1}\right)(t),\cdots,\left(\mathbf{X}*\rho_{M} \right)(t)\right), \tag{13}\]

where \(\rho_{1}(\cdot),\cdots,\rho_{M}(\cdot)\in\ell^{1}(\mathbb{N})\) serve as memory kernels, and \((\mathbf{X}*\rho_{k})(t)=\sum_{s=0}^{+\infty}\mathbf{x}_{t-s}\rho_{k}(s)\) denotes the convolution of the inputs with kernel \(\rho_{k}\).

**Target class** and **Transformer hypothesis class.** The target class for modeling essentially sparse memories is defined as:

\[\mathcal{H}^{\text{Ess}}:=\big{\{}\mathbf{H}:\ \mathbf{H}_{t}(\mathbf{X})=(\ref{eq: 13}),\text{ where }\rho_{1},\cdots,\rho_{M}\in\ell^{1}(\mathbb{N}),f\in \mathcal{B}\cap\mathcal{L}\big{\}}. \tag{14}\]

For the hypothesis class, we consider one-layer dot-product-free Transformer with Attn head number \(H\) and FFN width \(m\), as defined in (7).

**Examples.** Essentially sparse memories are prevalent in real-world scenarios:

(I) _Image Tasks._ In CV, a fundamental objective is identifying and representing meaningful "features", such as ears, nose, etc. These features can often be modeled using convolution kernels, leading to a task in the form \(y=f\left(\mathbf{X}*\rho_{\text{eye}},\mathbf{X}*\rho_{\text{nose}},\mathbf{X}*\rho_{ \text{ear}}\right)\). This is an extension of the task we discussed above, in which the kernel functions \(\{\rho_{j}\}\) are data-dependent ("adaptive" in the terminology used in the previous section).

(II) _Signal processing._ In signal processing, it is commonly the case that the signals are highly sparse under Wavelet or Fourier transforms. For instance, let \(\psi(\cdot)\) be a wavelet function and define \(\psi_{a,b}(t):=\psi(\frac{t-b}{a})/\sqrt{\left|a\right|}\). Then we have \(y=f\left(\mathbf{X}*\psi_{a_{1},b_{1}},\cdots,\mathbf{X}*\psi_{a_{M},b_{M}}\right)\) where \((a_{1},b_{1}),\cdots,(a_{M},b_{M})\) might be data-dependent.

(III) _Mathematical calculation._ Consider algebraic operations where memory exhibits sparsity under specific linear transformations. For example, \(y_{t}=10x_{t}+x_{t-4}/(\sum_{s=0}^{100}w_{s}x_{t-10-s})-\sum_{s=0}^{+\infty}v_{ s}x_{t-100-s}\) can be represented in our framework as \(y=f\left(\mathbf{X}*\rho_{1},\cdots,\mathbf{X}*\rho_{4}\right)\), where each \(\rho_{i}\) represents a specific linear transformation.

### Theoretical results and insights

**Theorem 5.1** (Approximation rates).:

_(A) Consider \(\mathcal{H}^{\text{Ess}}\) (14) with exponentially decayed memory kernels, i.e., there exists \(\beta>0\) such that \(\rho_{1}(t),\cdots,\rho_{M}(t)=\mathcal{O}(e^{-\beta t})\). Then for any target \(\mathbf{H}\in\mathcal{H}^{\text{Ess}}\), rate \(n\in[\left\lfloor 99\beta\right\rfloor]\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer DP-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\mathrm{DPF,lin}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left\|\!\left|\mathbf{H}-\mathbf{TF}\right|\!\right\|\leq\mathcal{E}_{ \mathrm{FFN}}+\left\|f\right\|_{\mathrm{Lip}}\cdot\mathcal{E}_{\mathrm{Attn}};\]

_(B) Consider \(\mathcal{H}^{\text{Ess}}\) (14) with polynomially decayed memory kernels, i.e., there exists \(\beta>1\) such that \(\rho_{1}(t),\cdots,\rho_{M}(t)=\mathcal{O}(t^{-\beta})\). Then for any target \(\mathbf{H}\in\mathcal{H}^{\text{Ess}}\), rate \(n\in[\left\lfloor 0.99\beta\right\rfloor-1]\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer DP-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\mathrm{DPF,log}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left\|\!\left|\mathbf{H}-\mathbf{TF}\right|\!\right\|\leq\mathcal{E}_{ \mathrm{FFN}}+\left\|f\right\|_{\mathrm{Lip}}\cdot\mathcal{E}_{\mathrm{Attn}};\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{ \mathrm{B}}}{\sqrt{m}}\right),\mathcal{E}_{\mathrm{Attn}}=\mathcal{O}\left( \frac{C(n)M^{n+1}}{H^{n}}\right).\)_

Theorem 5.1 illustrates that one-layer DP-free Transformer with lin-RPE is effective in modeling essentially sparse memories with exponentially decayed kernels, and one-layer DP-free Transformer with log-RPE can efficiently model the memories with polynomially decayed kernels. A key difference between Theorem 5.1 and Theorem 3.1 lies in the memory kernels they address. In Theorem 5.1, the Attn layer should approximate general memory kernels \(\rho_{i}(\cdot)\), instead of approximating indicator kernels \(\mathbb{I}\{\cdot=T_{i}\}\) in Theorem 3.1. The proof of Theorem 5.1 can be found in Appendix E.

**Overcoming the Curse of Memory (CoM).** For recurrent neural networks (RNN), it was discovered (Li et al., 2021, 2022) that both approximation and optimization become exceedingly difficult when the target has long-term memory. This phenomenon is referred as the "_curse of memory_", or "CoM". It was shown in (Li et al., 2021, 2022) that RNN requires an exponentially large number of neurons to approximate targets with heavy-tailed memory kernels, such as the ones that exhibit polynomial decay. In contrast, Theorem 5.1 reveals that Transformer with log-RPE efficiently handles polynomial decaying memory kernels, requiring only a polynomial number of neurons for effective approximation. This finding theoretically elucidates the superior performance of T5's RPE and KERPLE(log) in length generalization task in practice (Section G.1).

## 6 Experimental Validation

As summarized in Section 1, our theoretical analysis reveals novel insights into the expressive power and mechanisms of Transformer. To validate these insights, we conduct experiments ranging from simple toy models to more complex language model pre-training. Due to space constraints, detailed experimental validation and practical implications of our insights are presented in Appendix H.

## 7 Conclusion and Future Work

In this work, we investigate theoretically the expressive power and the mechanisms of Transformer for modeling long but sparse memories. Our analysis establishes explicit approximation rates and offers much-needed insights into the functionalities of the various components of Transformer. However, we still have a long way to go for a full theoretical understanding of Transformer. For instance, although we have investigated the mechanisms of Transformer in terms of expressive power, the evolution of the mechanisms during the training process remains elusive. Recent studies revealed that Transformer exhibits multi-phase learning dynamics (Boix-Adsera et al., 2023) and undergoes phase transitions (Olsson et al., 2022) during training, akin to the phenomenon of learning with increasing complexity in classical neural networks (Kalimeris et al., 2019; Xu et al., 2019; Rahaman et al., 2019; Abbe et al., 2023; Wang and Ma, 2023). These and other issues will be studied in future work.

## Acknowledgments

This work is supported in part by the National Key Basic Research Program of China (No. 2015CB856000). We thank Prof. Qianxiao Li, Prof. Lei Wu, Dr. Zhong Li, and Dr. Hongkang Yang for helpful discussions and anonymous reviewers for their valuable suggestions.

## References

* Abbe et al. (2023) Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023a.
* Abbe et al. (2023b) Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. _International Conference on Machine Learning_, 2023b.
* Akyurek et al. (2022) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* Allen-Zhu and Li (2023) Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. _arXiv preprint arXiv:2305.13673_, 2023.
* Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. _Advances in Neural Information Processing Systems_, 35:38546-38556, 2022.
* Bach (2017) Francis Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* Bai et al. (2023) Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _arXiv preprint arXiv:2306.04637_, 2023.
* Barron (1992) Andrew R Barron. Neural net approximation. In _Proc. 7th Yale Workshop on Adaptive and Learning Systems_, volume 1, pages 69-72, 1992.
* Barron (1993) Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.
* Barron (1994) Andrew R Barron. Approximation and estimation bounds for artificial neural networks. _Machine Learning_, 14(1):115-133, 1994.
* Bellman (1966) Richard Bellman. Dynamic programming. _Science_, 153(3731):34-37, 1966.
* Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* Bhattacharya et al. (2020) Satwik Bhattacharya, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2020.
* Bhattacharya et al. (2022) Satwik Bhattacharya, Arkil Patel, Varun Kanade, and Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse boolean functions. _arXiv preprint arXiv:2211.12316_, 2022.
* Boix-Adsera et al. (2023) Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. _arXiv preprint arXiv:2306.07042_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Candes and Wakin (2008) Emmanuel J Candes and Michael B Wakin. An introduction to compressive sampling. _IEEE signal processing magazine_, 25(2):21-30, 2008.
* Chi et al. (2022) Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. _Advances in Neural Information Processing Systems_, 35:8386-8399, 2022.
* Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. _arXiv preprint arXiv:2009.14794_, 2020.
* Choromanski et al. (2020)Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.
* Csordas et al. (2021) Robert Csordas, Kazuki Irie, and Jurgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021.
* Dehghani et al. (2019) Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. _International Conference on Learning Representations_, 2019.
* Donoho (2006) David L Donoho. Compressed sensing. _IEEE Transactions on information theory_, 52(4):1289-1306, 2006.
* E et al. (2019) Weinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural networks. _Communications in Mathematical Sciences_, 17(5):1407-1425, 2019.
* E et al. (2021) Weinan E, Chao Ma, and Lei Wu. The barron space and the flow-induced function spaces for neural network models. _Constructive Approximation_, pages 1-38, 2021.
* Edelman et al. (2022) Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html).
* Feng et al. (2023) Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. _arXiv preprint arXiv:2305.15408_, 2023.
* Francis and Kucera (1979) W Nelson Francis and Henry Kucera. Brown corpus manual. _Letters to the Editor_, 5(2):7, 1979.
* Garg et al. (2022) Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Giannou et al. (2023) Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. _International Conference on Machine Learning_, 2023.
* Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus), 2019.
* Hahn (2020) Michael Hahn. Theoretical limitations of self-attention in neural sequence models. _Transactions of the Association for Computational Linguistics_, 8:156-171, 2020.
* Hawkins (2021) Jeff Hawkins. _A thousand brains: A new theory of intelligence_. Basic Books, 2021.
* Jackson (1930) Dunham Jackson. _The theory of approximation_, volume 11. American Mathematical Soc., 1930.
* Jiang and Li (2023) Haotian Jiang and Qianxiao Li. Approximation theory of transformer networks for sequence modeling. _arXiv preprint arXiv:2305.18475_, 2023.
* Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. _arXiv preprint arXiv:1905.12322_, 2019.
* Kucera et al. (2021)Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. _Advances in neural information processing systems_, 32, 2019.
* Kazemmejad et al. (2023) Amirhossein Kazemmejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. _Advances in Neural Information Processing Systems_, 2023.
* Kim et al. (2022) Junghwan Kim, Michelle Kim, and Barzan Mozafari. Provable memorization capacity of transformers. In _The Eleventh International Conference on Learning Representations_, 2022.
* Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* Li et al. (2021) Zhong Li, Jiequn Han, Qianxiao Li, and Weinan E. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. _International Conference on Learning Representations_, 2021.
* Li et al. (2022) Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. _Journal of Machine Learning Research_, 23(42):1-85, 2022.
* Li et al. (2023) Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. In _Conference on Parsimony and Learning (Recent Spotlight Track)_, 2023.
* Liu et al. (2022) Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* Ma and Ying (2022) Chao Ma and Lexing Ying. Why self-attention is natural for sequence-to-sequence problems? a perspective from symmetries. _arXiv preprint arXiv:2210.06741_, 2022.
* Ma et al. (2020) Chao Ma, Stephan Wojtowytsch, Lei Wu, and Weinan E. Towards a mathematical understanding of neural network-based machine learning: what we know and what we don't. _arXiv preprint arXiv:2009.10713_, 2020.
* Mahankali et al. (2023) Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_, 2023.
* Merrill and Sabharwal (2023a) William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. _arXiv preprint arXiv:2310.07923_, 2023a.
* Merrill and Sabharwal (2023b) William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023b.
* Merrill et al. (2022) William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold circuits. _Transactions of the Association for Computational Linguistics_, 10:843-856, 2022.
* Meyer (1992) Yves Meyer. _Wavelets and Operators: Volume 1_. Cambridge university press, 1992.
* Nasukawa and Yi (2003) Tetsuya Nasukawa and Jeonghee Yi. Sentiment analysis: Capturing favorability using natural language processing. In _Proceedings of the 2nd international conference on Knowledge capture_, pages 70-77, 2003.
* Nivre and Scholz (2004) Joakim Nivre and Mario Scholz. Deterministic dependency parsing of english text. In _COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics_, pages 64-70, 2004.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* O'Connor et al. (2019)Santiago Ontanon, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks. _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_, 2022.
* OpenAI (2023) OpenAI. Gpt-4 technical report. _[https://cdn.openai.com/papers/gpt-4.pdf_](https://cdn.openai.com/papers/gpt-4.pdf_), 2023.
* Perez et al. (2021) Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 22(1):3463-3497, 2021.
* Press et al. (2022) Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. _International Conference on Learning Representations_, 2022.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* Rahaman et al. (2019) Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In _International Conference on Machine Learning_, pages 5301-5310. PMLR, 2019.
* Shannon (1948) Claude Elwood Shannon. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423, 1948.
* Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics_, 2018.
* Shen et al. (2023) Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? _arXiv preprint arXiv:2310.08540_, 2023.
* Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.
* Tay et al. (2021) Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In _International Conference on Machine Learning_, pages 10183-10192. PMLR, 2021.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Oswald et al. (2023) Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Oswald et al. (2023) Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. _arXiv preprint arXiv:2309.05858_, 2023.
* Wang et al. (2023) Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2023.
* Wang and Ma (2023) Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks. _Advances in Neural Information Processing Systems_, 2023.
* Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* Wang et al. (2020)Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022a.
* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022b.
* Weiss et al. (2021) Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In _International Conference on Machine Learning_, pages 11080-11090. PMLR, 2021.
* Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* Xu et al. (2019) Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle: Fourier analysis sheds light on deep neural networks. _Communications in Computational Physics_, 2019.
* Yun et al. (2019) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. _Advances in neural information processing systems_, 33:17283-17297, 2020.
* Zhang et al. (2024) Zhongwang Zhang, Zhiwei Wang, Junjie Yao, Zhangchen Zhou, Xiaolong Li, Zhi-Qin John Xu, et al. Anchor function: a type of benchmark functions for studying language models. _arXiv preprint arXiv:2401.08309_, 2024.

## Appendix A Detailed Related Works

* [leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
* Proof of Section 3
* B.1 Proof of Theorem 3.1
* Proof of Section 4.2
* C.1 Proof of Theorem 4.1
* C.2 Proof of Proposition 4.2
* C.3 Proof of Proposition 4.3
* Proof of Section 4.3
* D.1 Proof of Theorem 4.4
* D.2 Proof of Proposition 4.5
* Proof of Section 5
* E.1 Proof of Theorem 5.1
* Key Lemmas about Approximation
* F.1 Approximation by the sum of exponential decay
* F.2 Approximation by the sum of polynomial decay
* Some Background and Proof Preparation
* G.1 T5's relative positional encoding
* G.2 Barron space theory
* G.3 Useful approximation lemmas

* H.1 Restatement of our theoretical insights
* H.2 Experimental Validation
* H.3 Practical Implications

## Appendix A Detailed Related Works

**Theoretical results of Transformer.** We first review the expressive power results of Transformer. Yun et al. (2019) first proved the universal approximation property (UAP) of Transformer, highlighting the crucial role of PE in breaking permutation invariance. Edelman et al. (2022) demonstrated that Transformer can approximate fixed sparse functions. Dehghani et al. (2019); Perez et al. (2021); Wei et al. (2022) explored the Turing-completeness of infinite-precision and finite-precision Transformer. Giannou et al. (2023) showed that looped Transformer can implement practical computer programs.

Jiang and Li (2023) provided explicit approximation rates for Transformer in sequences modeling with inherent graph structures. Liu et al. (2022) found that Transformer can execute finite-state automata. Ma and Ying (2022) asserted the natural suitability of Attn for achieving permutation equivariance. Besides these affirmative results, several studies characterized the expressivity limitation of Transformers, particularly in modeling formal languages or simulating circuits (Hahn, 2020; Weiss et al., 2021; Bhattacharya et al., 2020; Merrill and Sabharwal, 2023; Merrill et al., 2022). Additionally Feng et al. (2023); Merrill and Sabharwal (2023a) examined the expressivity of Transformer using Chain of Thought prompting (Wei et al., 2022b). Moreover, some studies showed that the in-context learning ability of Transformer is attainable by simulating gradient-based iterations across various layers (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023; Von Oswald et al., 2023; Mahankali et al., 2023; Bai et al., 2023; Shen et al., 2023). Besides, experimental studies also provide insights into the mechanisms of Transformer through induction head (Elhage et al., 2021; Olsson et al., 2022), information flow (Wang et al., 2023), anchor functions (Zhang et al., 2024), etc.

**Positional encoding.** One core component of Transformer is the PE, which facilitates the representation of input sequence order. Theoretically, Transformer without PE lacks UAP and is restricted to representing permutation-invariant functions. PE was first introduced in Vaswani et al. (2017). It has limitations in encoding unseen positions. To overcome this difficulty, Shaw et al. (2018) introduced RPE. Subsequent studies proposed various different RPE types. Notable examples include T5's RPE (Raffel et al., 2020), Rotary RPE (Su et al., 2024) (utilized in PaLM (Chowdhery et al., 2023) and LlaMA (Touvron et al., 2023)), Alibi RPE (Press et al., 2022) (employed in BLOOM (Workshop et al., 2022)), and KERPLE (Chi et al., 2022). A prevailing belief is that RPEs can outperform APEs in the "length generalization task" (Ontanon et al., 2022; Csordas et al., 2021)- the ability to generalize from smaller training contexts to larger ones, a critical challenge for Large Language Models (Anil et al., 2022; Abbe et al., 2023b). However, Press et al. (2022) revealed that the commonly used Rotary RPE may exhibit suboptimal performance in this task. The recent work (Kazemmejad et al., 2023; Chi et al., 2022) conducted systematic experiments comparing the length generalization capabilities of Transformers with various RPEs and APEs, suggesting that _the RPEs used in T5 and KERPLE(log) demonstrate superior performance over other types_.

**Rethinking dot-product.** Another critical component of Transformer is the DP structure. Due to its quadratic cost as a function of the sequence length, the necessity of DP has always been questioned. Numerous variants of DP have been proposed, demonstrating competitive performance across diverse tasks. Representative examples include Longformer (Beltagy et al., 2020), Big Bird (Zaheer et al., 2020), Reformer (Kitaev et al., 2020), Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020), Synthesizer (Tay et al., 2021), etc. In particular, a recent study (Allen-Zhu and Li, 2023) compared standard and DP-free Transformers in modeling "context-free grammar". Their findings suggested that the presence of DP has a marginal impact on performance. _These evidences motivate us to rethink the necessity of DP in Transformer_.

**Sparsity**(Donoho, 2006; Candes and Wakin, 2008) has gained considerable attention in sequence modeling. In classical signal processing, there is a prevailing notion that valuable signals are extremely sparse. For example, when representing an image, one often finds that only a few wavelet coefficients hold significant values in wavelet space (Meyer, 1992). In NLP, the starting point off the traditional \(n\)-gram model (Shannon, 1948) is that the next token only relies on a few previous tokens. Such models, however, overlook long-range information, often resulting in suboptimal performance. For NLP tasks such as dependency parsing (Nivre and Scholz, 2004), sentiment analysis (Nasukawa and Yi, 2003), part-of-speech tagging (Francis and Kucera, 1979), and continuation writing (Brown et al., 2020; OpenAI, 2023), it is indeed often the case that only a limited subset of preceding information is crucial for accurate prediction. However, these relevant information can be quite distant. For instance, the resolution of a mystery novel may hinge on elements introduced at the outset. Moreover, for Transformer networks, extensive research into the visualization and interpretability has revealed that (i) the learned activation maps of FFN layers are extremely sparse (Li et al., 2023); (ii) the learned self-attention matrices exhibit notable sparsity, yet it does not closely resemble a diagonal configuration (Elhage et al., 2021). These observations suggest that the prediction of the next token is influenced by a small number of previous tokens which might be far away. Therefore, _being able to represent sparse but long-range dependence is important for sequence modeling_.

Proof of Section 3

### Proof of Theorem 3.1

In this subsection, we give the detailed proofs of fixed, long but sparse memory:

\[\mathbf{y}_{t}=\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-T_{1}},\cdots,\mathbf{x}_{t-T_{M}}),\]

where \(1\leq T_{1}<\cdots<T_{M}<+\infty\) signify the fixed positions of the memories.

**Theorem B.1** (Restatement of Theorem 3.1).: _For any target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Fix}}\) (6), rate \(n\in\mathbb{N}_{+}\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer Transformer \(\mathbf{TF}\in\mathcal{T}^{\mathrm{DPF},\mathsf{type}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}-\mathbf{TF}\right| \kern-1.075pt\right|\kern-1.075pt\right|\leq\mathcal{E}_{\mathrm{FFN}}+\left| \kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left| \kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left| \kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}-\mathbf{TF} \right|\right|\right|\right|\right|\right|\right|\right|\right|\leq\mathcal{E}_{ \mathrm{FFN}}+\left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left| \left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}- \mathbf{TF}\right|\right|\right|\right|\right|\right),\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left|\kern-1.075pt \left|\kern-1.075pt\left|\kern-1.075pt\left|\left|\kern-1.075pt\left|\left| \mathbf{f}\right|\right|\right|\right|\right|\right)}{\sqrt{m}}\right)\) and_

\[\mathcal{E}_{\mathrm{Attn}}(\mathsf{type})=\begin{cases}\mathcal{O}\left(\frac{ C(n)}{H^{n}}\left(\sum_{i=1}^{M}e^{0.01T_{i}}\right)^{n+1}\right),&\mathsf{ type}=\mathrm{lin}\\ \mathcal{O}\left(\frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}T_{i}^{1.01}\right)^{n+1 }\right),&\mathsf{type}=\mathrm{log}\end{cases}.\]

Proof of Theorem b.1.: First, we choose the embedding dimension \(D=(M+1)d\), and select the simple embedding \(\mathbf{W}_{E}=(\mathbf{I}_{d\times d},\mathbf{0})^{\top}\in\mathbb{R}^{D\times d},\mathbf{b}_ {E}=\mathbf{0}\in\mathbb{R}^{D}\).

Then for any input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\), the token after embedding satisfies:

\[\mathbf{x}_{t}^{E}=\mathbf{W}_{E}\mathbf{x}_{t}+\mathbf{b}_{E}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top })^{\top}\in\mathbb{R}^{D}.\]

For one-layer Dot-product-free Transformer \(\mathbf{TF}\in\mathcal{T}^{\mathrm{DPF},\mathsf{type}}_{(1,H,m)}\) with \(\phi_{\mathsf{type}}\), the output token \(\mathbf{TF}_{t}(\mathbf{X})\) of \(t\)-th input token \(\mathbf{x}_{t}\) satisfies:

\[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}+\mathbf{W}_{O}^{(1)}\sum_{h=1}^{H}\mathbf{Attn}_{t}^ {(1,h)}(\mathbf{X}^{(0)}),\] \[\mathbf{x}_{t}^{(1)} =\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(1/2)})\]

where

\[\mathbf{Attn}_{t}^{(1,h)}(\mathbf{X})=\mathbf{W}_{V}^{(1,h)}\sum_{s=0}^{+\infty}\frac{ \mathbf{x}_{t-s}\exp\left(p^{(1,h)}\phi_{\mathsf{type}}(s)\right)}{\sum_{j=0}^{+ \infty}\exp\left(p^{(1,h)}\phi_{\mathsf{type}}(j)\right)}.\]

This proof can be summarized as the following process:

\[\cdots\quad\mathbf{x}_{t}^{E}\quad\cdots\] Step I. Attn layer \[\downarrow\] \[\cdots\quad\mathbf{x}_{t}^{(1/2)} \approx(\mathbf{x}_{t}^{\top},\mathbf{x}_{t-T_{1}}^{\top},\cdots,\mathbf{x}_{ t-T_{M}}^{\top})^{\top}\quad\cdots\] Step II. FFN layer \[\downarrow\] \[\cdots\quad\mathbf{x}_{t}^{(1)} \approx\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-T_{1}},\cdots,\mathbf{x}_{t-T_{M}})\quad\cdots\]

Now we give the formal proof.

**Step I.** Extract the memory locations by (Dot-product-free) Attn layer.

We consider to use \(H_{k}\) attention heads (from \(\sum_{i=1}^{k-1}H_{i}+1\)-th head to \(\sum_{i=1}^{k}H_{i}\)-th head) to extract it, and it satisfies to \(\sum_{k=1}^{M}H_{k}=H\).

For simplicity, we denote the following projection matrices:\[\mathbf{P}^{(k)}:=(\mathbf{0}_{d\times kd}\quad\mathbf{I}_{d\times d}\quad\mathbf{0}) \in\mathbb{R}^{d\times D},\quad 1\leq k\leq M.\] \[\mathbf{P}^{(k)}_{\perp}:=\begin{pmatrix}\mathbf{I}_{kd\times kd}&\mathbf{0}_{d \times d}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}_{d\times d}&\mathbf{I}_{(M-k)d\times(M-k)d}\end{pmatrix}\in\mathbb{R }^{Md\times D},\quad 1\leq k\leq M.\]

Now we consider the extraction of \(k\)-th memory \(\mathbf{x}_{t-T_{k}}\) (\(1\leq k\leq M\)).

* **Case \(\mathsf{type}=\lim\).** By Lemma F.1, for any rate \(n\in\mathbb{N}_{+}\), there exists an constant \(C(n)\) and a function \[\phi_{k}^{\exp}(t)=\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}} \alpha_{h}e^{-\beta_{h}t}\] such that \(\beta_{h}>0\) and \[\left\|\mathbb{I}\{\cdot=T_{k}\}-\phi_{k}^{\exp}(\cdot)\right\|_{\ell_{1}( \mathbb{N})}=\sum_{s=0}^{+\infty}\left|\mathbb{I}\{s=T_{k}\}-\phi_{k}^{\exp}( s)\right|\leq C(n)\frac{e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.\] Therefore, for these attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can choose \[p^{(1,h)}=\beta_{h},\quad\mathbf{W}^{(1,h)}_{V}=\alpha_{h}\left(\sum_{j=0}^{+\infty }\exp(-\beta_{h}j)\right)\mathbf{\delta}^{d\times d}_{(k+1,1)},\] where \(\mathbf{\delta}^{(k+1,1)}\in\mathbb{R}^{D\times D}\) means that: it equals to \(\mathbf{I}_{d\times d}\) for the \((k+1,1)\)-th \(d\times d\) blocks, and \(\mathbf{0}_{d\times d}\) for the other \(d\times d\) blocks. Then it holds that: \[\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\mathbf{Attn}^{(1,h)}_{t} (\mathbf{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\alpha_{h }\sum_{s=0}^{+\infty}e^{-\beta_{h}s}\begin{pmatrix}\mathbf{0}_{kd}\\ \mathbf{x}_{t-s}\\ \mathbf{0}\end{pmatrix}\in\mathbb{R}^{D}.\] This implies: \[\mathbf{P}^{(k)}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}} \mathbf{Attn}^{(1,h)}_{t}(\mathbf{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum _{i=1}^{k}H_{i}}\alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}s}\mathbf{x}_{t-s},\] \[\mathbf{P}^{(k)}_{\perp}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^ {k}H_{i}}\mathbf{Attn}^{(1,h)}_{t}(\mathbf{X}^{(0)})=\mathbf{0},\] moreover, the following estimate holds: \[\left\|\mathbf{P}^{(k)}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^ {k}H_{i}}\mathbf{Attn}^{(1,h)}_{t}(\mathbf{X}^{(0)})-\mathbf{x}_{t-T_{k}}\right\|_{2}\] \[= \left\|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}} \alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}s}\mathbf{x}_{t-s}-\mathbf{x}_{t-T_{k}} \right\|_{2}\]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

Proof of Section 4.2

In this section, we give the detailed proofs of the approximation theory of Transformer for modeling the warm-up case of adaptive, long but sparse memory:

\[\mathbf{y}_{t}=\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}}),\]

where the adaptive memory satisfies to:

\[t_{k}=g_{k}(\mathbf{x}_{t}),\quad k\in[M].\]

Moreover, \(g_{k}(\cdot)\) generate positive integers for the input tokens, and there exist maximum values \(T_{k}\) such that \(1\leq g_{k}(\mathbf{x}_{t})\leq T_{k}\) holds for any \(\mathbf{x}_{t}\) and \(k\in[M]\).

To tackle the discrete values of the time and the memory values \(g_{k}(\mathbf{x}_{t})\), a modified version of standard FFN, termed "FFN with precision", us cibsudered. This approach ensures that the output of FFN undergoes a simple rounding operation. Notably, the precision technique is widely used in LLM training (Kalamkar et al., 2019), such as BFloat16. Specifically, for Transformer using RPE with type, we use the following FFN with precision:

\[\widehat{\text{FFN}}(\mathbf{x}) :=[\text{FFN}(\mathbf{x})], \texttt{type} =\text{lin}; \tag{15}\] \[\widehat{\text{FFN}}(\mathbf{x}) :=\log\left[\exp\left(\text{FFN}(\mathbf{x})\right)\right], \texttt{type} =\log,\]

where \([\cdot]\) signifies rounding to the nearest integer, i.e., \([x]=\underset{n\in\mathbb{Z}}{\arg\min}\left|n-x\right|\) (\(x\in\mathbb{R}\)).

It is important to note that the rounding obtained by using the operator \(\log[\exp(z)]\), used in (15), is _quite fine_, which is much finer than the vanilla rounding obtained by \([z]\). To elaborate, the following proposition is presented:

**Proposition C.1**.: _For any \(z\geq 1\), the following holds:_

\[\text{(i)}\left|\log[\exp(z)]-z\right|\leq\frac{1}{2\min\{e^{z},[e^{z}]\}}; \quad\text{(ii)}\left|[z]-z\right|\leq\frac{1}{2}.\]

### Proof of Theorem 4.1

**Theorem C.2** (Restatement of Theorem 4.1).: _For any target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Adap}}_{(1,M)}\) (8), rate \(n\in\mathbb{N}_{+}\), and \(H,m\in\mathbb{N}_{+}\), there exists a two-layer Transformer \(\mathbf{TF}\in\mathcal{TF}^{\mathrm{NF},\texttt{type}}_{(2,H,m)}\) (12) and a constant \(C(n)\) such that: if the width satisfies_

\[m\geq\begin{cases}\tilde{\Omega}\left(\sum_{i=1}^{M}\left\|g_{i}\right\|_{ \mathcal{B}}^{2}\right),&\texttt{type}=\text{lin}\\ \tilde{\Omega}\left(\sum_{i=1}^{M}\left\|\log g_{i}\right\|_{\mathcal{B}}^{2} T_{i}^{2}\right),&\texttt{type}=\text{log}\end{cases},\]

_then the following approximation rate holds:_

\[\left\|\!\left\|\mathbf{H}-\mathbf{TF}\right\|\!\right\|\leq\mathcal{E}_{ \mathrm{FFN}}+\left\|f\right\|_{\mathrm{Lip}}\mathcal{E}_{\mathrm{Attn}}( \texttt{type}),\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{ \mathcal{B}}}{\sqrt{m}}\right)\) and_

\[\mathcal{E}_{\mathrm{Attn}}(\texttt{type})=\begin{cases}\mathcal{O}\left( \frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}e^{0.01T_{i}}\right)^{n+1}\right),& \texttt{type}=\text{lin}\\ \mathcal{O}\left(\frac{C(n)}{H^{n}}\left(\sum_{i=1}^{M}T_{i}^{1.01}\right)^{n +1}\right),&\texttt{type}=\text{log}\end{cases}.\]

Proof of Theorem c.2.: First, we choose the embedding dimension \(D=(M+1)(d+1)\), and select a simple embedding \(\mathbf{W}_{E}=(\mathbf{I}_{d\times d},\mathbf{0})^{\top}\in\mathbb{R}^{D\times d},\mathbf{b}_ {E}=\mathbf{0}\in\mathbb{R}^{D}\).

Then for any input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\), the token after embedding satisfies:

\[\mathbf{x}_{t}^{(0)}=\mathbf{W}_{E}\mathbf{x}_{t}+\mathbf{b}_{E}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{ \top})^{\top}\in\mathbb{R}^{D}.\]To tackle the discrete values of \(g_{m}(\mathbf{x}_{t})\), we utilize \(\widehat{\text{FFN}}\), FFN with precision (15). It ensures that the output of FFN undergoes a simple rounding operation.

Thus, for two-layer normalization-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\text{NF},\text{type}}_{(2,H,m)}\) with \(\phi_{\text{type}}\), the output token \(\mathbf{x}_{t}^{(2)}\) of \(t\)-th input token satisfies:

\[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}+\mathbf{W}_{O}^{(1)}\sum_{h=1}^{H}\mathbf{Attn}_{t} ^{(1,h)}(\mathbf{X}^{(0)}),\] \[\mathbf{x}_{t}^{(1)} =\mathbf{x}_{t}^{(1/2)}+\widehat{\mathbf{FFN}}^{(1)}(\mathbf{x}_{t}^{(1/ 2)}),\] \[\mathbf{x}_{t}^{(3/2)} =\mathbf{x}_{t}^{(1)}+\mathbf{W}_{O}^{(2)}\sum_{h=1}^{H}\mathbf{Attn}_{t} ^{(2,h)}(\mathbf{X}^{(1)}),\] \[\mathbf{x}_{t}^{(2)} =\mathbf{FFN}^{(2)}(\mathbf{x}_{t}^{(3/2)}),\]

where

\[\mathbf{Attn}_{t}^{(l,h)}(\mathbf{X})=\mathbf{W}_{V}^{(l,h)}\sum_{s=0}^{+\infty}\mathbf{x} _{t-s}\exp\left(\left\langle\mathbf{W}_{Q}^{(l,h)}\mathbf{x}_{t},\mathbf{W}_{K}^{(l,h)}\bm {x}_{t-s}\right\rangle+p^{(l,h)}\phi_{\text{type}}(s)\right).\]

This proof can be summarized as the following process:

* **Case**\(\text{type}=\text{lin}\)**.** \[\mathbf{x}_{t}^{(0)}\] Step I. 1-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}\] Step II. 1-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(1)} =(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}( \mathbf{x}_{t}),1)^{\top}\] Step III. 2-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(3/2)} \approx(\mathbf{x}_{t}^{\top},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}^{\top}, \cdots,\mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}(\bm {x}_{t}),1)^{\top}\] Step IV. 2-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(2)} \approx\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})},\cdots,\mathbf{x}_{t-g_{M }(\mathbf{x}_{t})})\]
* **Case**\(\text{type}=\text{log}\)**.** \[\mathbf{x}_{t}^{(0)}\] Step I. 1-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}\] Step II. 1-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(1)} =(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots, \log g_{M}(\mathbf{x}_{t}),\log 2)^{\top}\] Step III. 2-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(3/2)} \approx(\mathbf{x}_{t}^{\top},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}^{\top}, \cdots,\mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g _{M}(\mathbf{x}_{t}),\log 2)^{\top}\] Step IV. 2-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(2)} \approx\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})},\cdots,\mathbf{x}_{t-g_{M }(\mathbf{x}_{t})})\]

Now we give the formal proof.

**Step I.** Identity map.

For the first Attn layer, we only need to do the identity map by taking \(\mathbf{W}_{0}^{(1)}=\mathbf{0}\). Then \(\mathbf{x}_{t}^{(1/2)}=\mathbf{x}_{t}^{(0)}\).

**Step II.** Approximate the adaptive memory function by the first FFN layer.

* **Case \(\texttt{type}=\texttt{lin}\).** Our main idea is that using the first FFN layer to express \((\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}(\mathbf{x}_{t}),1)^ {\top}\) exactly. First, we consider to approximate the \(r\)-th memory function \(g_{r}(\mathbf{x})\) by standard FFN. For any \(r\in[M]\), by Lemma G.6, there exists a two-layer neural network with \(m_{r}\) neurons \[f_{(1,r)}^{2\text{NN}}(\mathbf{x})=\sum_{k=1}^{m_{r}}a_{k}^{(1,r)}\sigma\left(\mathbf{ b}_{k}^{(1,r)\ \top}\mathbf{x}+c_{k}^{(1,r)}\right)\] defined on \(\mathbb{R}^{d}\) such that \[\left\|g_{r}-f_{(1,r)}^{2\text{NN}}\right\|_{L^{\infty}([-1,1]^{D})}\leq \tilde{\mathcal{O}}\left(\frac{\left\|g_{r}\right\|_{\mathbf{\beta}}}{\sqrt{m_{r}} }\right).\] Therefore, if we choose \[\tilde{\mathcal{O}}\left(\frac{\left\|g_{r}\right\|_{\mathbf{\beta}}}{\sqrt{m_{r}} }\right)<\frac{1}{2},\] the following holds: \[\left|g_{r}(\mathbf{x}_{t})-f_{(1,r)}^{2\text{NN}}(\mathbf{x}_{t})\right|\leq\left\|g _{r}-f_{(1,r)}^{2\text{NN}}\right\|_{L^{\infty}([-1,1]^{d})}<\frac{1}{2},\] Noticing \(g_{r}(\mathbf{x}_{t})\in\mathbb{N}_{+}\), we have \(\left[f_{(1,r)}^{2\text{NN}}(\mathbf{x}_{t})\right]=g_{r}(\mathbf{x}_{t})\), which implies: \[\widehat{f_{(1,r)}^{2\text{NN}}}(\mathbf{x}_{t})=\left[f_{(1,r)}^{2\text{NN}}(\bm {x}_{t})\right]=g_{r}(\mathbf{x}_{t}).\] Consequently, in order to construct the form \((\mathbf{0}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}(\mathbf{x}_{t}),1)^{\top}\in\mathbb{ R}^{D}\), we need to arrange the parameters \(a_{k}^{(1,r)}\), \(\mathbf{b}_{k}^{(1,r)}\), and \(c_{k}^{(1,r)}\) (\(k\in[m_{r}],r\in[M]\)) appropriately. Denote \(\bar{\mathbf{b}}_{k}^{(1,r)}=(\mathbf{b}_{k}^{(1,r)\ \top},\mathbf{0}^{\top})^{\top}\in \mathbb{R}^{D}\) for \(k\in[m_{r}],r\in[M]\). Consider the following two-layer neural network with \(1+\sum_{r=1}^{M}m_{r}\) neurons defined on \(\mathbb{R}^{D}\): \[\mathbf{FFN}^{(1)}(\mathbf{x})= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^{ r}m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\bar{\mathbf{b}}_{k}^{(1,r)\ \top}\mathbf{x}+c_{k}^{(1,r)}\right)\] \[+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+1).\] It is easy to verify that for any \(\mathbf{x}_{t}^{(1/2)}\), it holds that \[\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(1/2)})=\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(0)})\] \[= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^ {r}m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\bar{\mathbf{b}}_{k}^{(1,r)\ \top}\mathbf{x}_{t}^{(0)}+c_{k}^{(1,r)}\right)+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+1)\] \[= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^ {r}m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\mathbf{b}_{k}^{(1,r)\ \top}\mathbf{x}_{t}+c_{k}^{(1,r)}\right)+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+1)\] \[= \sum_{r=1}^{M}\mathbf{e}_{D-M+r-1}f_{r}^{2\text{NN}}(\mathbf{x}_{t})+\bm {e}_{D}\] \[= (\mathbf{0}_{d}^{\top},f_{(1,1)}^{2\text{NN}}(\mathbf{x}_{t}),\cdots,f_{(1,M)}^{2\text{NN}}(\mathbf{x}_{t}),1)^{\top}\in\mathbb{R}^{D}.\] Moreover, it satisfies that \[\widetilde{\mathbf{FFN}}^{(1)}(\mathbf{x}_{t}^{(1/2)})=\left[\mathbf{FFN}^{(1)}( \mathbf{x}_{t}^{(1/2)})\right]\]\[= (\mathbf{0}_{d}^{\top},\left[f_{(1,1)}^{\rm 2NN}(\mathbf{x}_{t}) \right],\cdots,\left[f_{(1,M)}^{\rm 2NN}(\mathbf{x}_{t})\right],1)^{\top}\] \[= (\mathbf{0}_{d}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}(\mathbf{x}_{t}),1)^{\top}\in\mathbb{R}^{D}.\]

Thus, we have achieved our goal in this step:

\[\mathbf{x}_{t}^{(1)}=\mathbf{x}_{t}^{(1/2)}+\widetilde{\mathbf{FFN}}^{(1)}(\mathbf{x}_{t}^ {(1/2)})=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g_{M}( \mathbf{x}_{t}),1)^{\top}.\]
* **Case type \(=\log\).** Our main idea is that using the first FFN layer to express \((\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g_{M} (\mathbf{x}_{t}),\log 2)^{\top}\) exactly. First, we consider to approximate the \(r\)-th memory function \(\log g_{r}(\mathbf{x})\) by standard FFN. For any \(r\in[M]\), by Lemma G.6, there exists a two-layer neural network with \(m_{r}\) neurons \[f_{(1,r)}^{\rm 2NN}(\mathbf{x})=\sum_{k=1}^{m_{r}}a_{k}^{(1,r)}\sigma\left(\mathbf{b} _{k}^{(1,r)\top}\mathbf{x}+c_{k}^{(1,r)}\right)\] defined on \(\mathbb{R}^{d}\) such that \[\left\|\log g_{r}-f_{(1,r)}^{\rm 2NN}\right\|_{L^{\infty}([-1,1]^{D})}\leq \tilde{\mathcal{O}}\left(\frac{\|\log g_{r}\|_{B}}{\sqrt{m_{r}}}\right).\] Therefore, if we choose \[\tilde{\mathcal{O}}\left(\frac{\|\log g_{r}\|_{B}}{\sqrt{m_{r}}} \right)<\frac{1}{4T_{r}},\] the following holds: \[\left|\log g_{r}(\mathbf{x}_{t})-f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t})\right| \leq\left\|g_{r}-f_{(1,r)}^{\rm 2NN}\right\|_{L^{\infty}([-1,1]^{d})}< \frac{1}{4T_{r}},\] which ensures \[\left|\exp\left(f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t})\right)-g_{r}(\mathbf{x}_{ t})\right|=\left|\exp\left(f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t})\right)-\exp\left( \log\left(g_{r}(\mathbf{x}_{t})\right)\right)\right|\] \[\leq \exp\left(\max\left\{f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t}),\log\left(g_{ r}(\mathbf{x}_{t})\right)\right\}\right)\left|f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t})-\log\left(g_{r}(\mathbf{x}_{t}) \right)\right|\] \[\leq \exp\left(\log g_{r}(\mathbf{x}_{t})+\frac{1}{4}\right)\frac{1}{4T_ {r}}\] \[\leq e^{1/4}\cdot T_{r}\cdot\frac{1}{4T_{r}}<\frac{1}{2}.\]

Noticing \(g_{r}(\mathbf{x}_{t})\in\mathbb{N}_{+}\), we have \(\left[\exp\left(f_{(1,r)}^{\rm 2NN}(\mathbf{x}_{t})\right)\right]=g_{r}(\mathbf{x}_{t})\), which implies:

\[\widetilde{f_{(1,r)}^{\rm 2NN}}(\mathbf{x}_{t})=\log\left[\exp\left(f_{(1,r)}^{\rm 2NN }\right)\right]=\log g_{r}(\mathbf{x}_{t}).\]

Consequently, in order to construct the form \((\mathbf{0}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g_{M}(\mathbf{x}_{t}),\log 2)^{\top}\), we need to arrange the parameters \(a_{k}^{(1,r)}\), \(\mathbf{b}_{k}^{(1,r)}\), and \(c_{k}^{(1,r)}\) (\(k\in[m_{r}],r\in[M]\)) appropriately. Denote \(\bar{\mathbf{b}}_{k}^{(1,r)}=\left(\mathbf{b}_{k}^{(1,r)\top},\mathbf{0}^{\top} \right)^{\top}\in\mathbb{R}^{D}\) for \(k\in[m_{r}],r\in[M]\). Consider the following two-layer neural network with \(1+\sum_{r=1}^{M}m_{r}\) neurons defined on \(\mathbb{R}^{D}\):

\[\mathbf{FFN}^{(1)}(\mathbf{x})= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^{r }m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\bar{\mathbf{b}}_{k}^{(1,r)\top}\bm {x}+c_{k}^{(1,r)}\right)\] \[+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+\log 2).\]It is easy to verify that for any \(\mathbf{x}_{t}^{(1/2)}\), it holds that \[\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(1/2)})=\mathbf{FFN}^{(1)}(\mathbf{x}_{t} ^{(0)})\] \[= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^{r} m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\bar{\mathbf{b}}_{k}^{(1,r)}\,{}^{ \top}\mathbf{x}_{t}^{(0)}+c_{k}^{(1,r)}\right)+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+\log 2)\] \[= \sum_{r=1}^{M}\sum_{1+\sum_{j=0}^{r-1}m_{j}\leq k\leq\sum_{j=0}^{r }m_{j}}\mathbf{e}_{D-M+r-1}a_{k}^{(1,r)}\sigma\left(\mathbf{b}_{k}^{(1,r)}\,{}^{\top} \mathbf{x}_{t}+c_{k}^{(1,r)}\right)+\mathbf{e}_{D}\cdot 1\cdot\sigma(0+\log 2)\] \[= \sum_{r=1}^{M}\mathbf{e}_{D-M+r-1}f_{r}^{2\mathrm{NN}}(\mathbf{x}_{t})+ \mathbf{e}_{D}\log 2\] \[= (\mathbf{0}_{d}^{\top},f_{(1,1)}^{2\mathrm{NN}}(\mathbf{x}_{t}),\cdots,f_{ (1,M)}^{2\mathrm{NN}}(\mathbf{x}_{t}),\log 2)^{\top}\in\mathbb{R}^{D}.\] Moreover, it satisfies that \[\widetilde{\mathbf{FFN}}^{(1)}(\mathbf{x}_{t}^{(1/2)})=\log\left[\exp \left(\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(1/2)})\right)\right]\] \[= (\mathbf{0}_{d}^{\top},\log\left[\exp\left(f_{(1,1)}^{2\mathrm{NN}}( \mathbf{x}_{t})\right)\right],\cdots,\log\left[\exp\left(f_{(1,M)}^{2\mathrm{NN}}( \mathbf{x}_{t})\right)\right],\log 2,\mathbf{0}^{\top})^{\top}\] \[= (\mathbf{0}_{d}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g_{M}(\mathbf{x }_{t}),\log 2)^{\top}.\] Thus, we have achieved our goal in this step: \[\mathbf{x}_{t}^{(1)}=\mathbf{x}_{t}^{(1/2)}+\widehat{\mathbf{FFN}}^{(1)}(\mathbf{x}_{t} ^{(1/2)})=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g _{M}(\mathbf{x}_{t}),\log 2)^{\top}.\] As established in the proof above, the width \(m\) must satisfy: \[m\geq 1+\sum_{r=1}^{M}m_{r}=\begin{cases}\tilde{\Omega}\left(\sum_{ r=1}^{M}\left\|g_{r}\right\|_{B}^{2}\right),&\texttt{type}=\lim\\ \tilde{\Omega}\left(\sum_{r=1}^{M}\left\|\log g_{r}\right\|_{B}^{2}T_{r}^{2} \right),&\texttt{type}=\log\end{cases}.\]

**Step III.** Extract the adaptive memories by the second Attn layer.

We consider to use \(H_{k}\) attention heads (from \(\sum_{i=1}^{k-1}H_{i}+1\)-th head to \(\sum_{i=1}^{k}H_{i}\)-th head) to extract it, and it satisfies to \(\sum_{k=1}^{M}H_{k}=H\).

For simplicity, we denote the following projection matrices in \(\mathbb{R}^{D\times D}\):

\[\mathbf{P}^{(k)}:=(\mathbf{0}_{d\times kd}\quad\mathbf{I}_{d\times d}\quad\bm {0})\in\mathbb{R}^{d\times D},\quad 1\leq k\leq M;\] \[\mathbf{P}^{(k)}_{\perp}:=\begin{pmatrix}\mathbf{I}_{kd\times kd}&\mathbf{0} _{d\times d}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}_{d\times d}&\mathbf{I}_{(D-(k+1)d)\times(D-(k+1)d)}\end{pmatrix}\in \mathbb{R}^{(D-d)\times D},\quad 1\leq k\leq M;\] \[\mathbf{Q}^{(M)}:=\left(\mathbf{I}_{(M+1)d\times(M+1)d}\quad\mathbf{0}\right) \in\mathbb{R}^{(M+1)d\times D}.\]

Now we consider the extraction of \(k\)-th adaptive memory \(\mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\) (\(1\leq k\leq M\)).

* \(\textbf{Case type}=\lim\). By Lemma F.2, for any rate \(n\in\mathbb{N}_{+}\), there exists a constant \(C(n)\) and a function \[\phi_{k}^{\exp}(t;B) =\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}}\alpha _{h}\exp(-\beta_{h}(t-B))\] \[=\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}}\alpha _{h}\exp\left(\beta_{h}B-\beta_{h}t\right)\]such that \(\beta_{h}>0\) and

\[\sup_{1\leq B\leq T_{k}}\left\|\mathbb{I}\{\cdot=B\}-\phi_{k}^{\exp}(\cdot;B) \right\|_{\ell_{1}(\mathbb{N})}\leq\frac{C(n)e^{0.01(n+1)T_{k}}}{H_{k}^{n}}.\]

Moreover, Noticing that \(1\leq g_{k}(\mathbf{x}_{t})\leq T_{k}\) holds for any \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\in\mathcal{X}\), the following holds:

\[\sup_{\mathbf{X}}\left\|\mathbb{I}\{\cdot=g_{k}(\mathbf{x}_{t})\}-\phi_{k }^{\exp}(\cdot;g_{k}(\mathbf{x}_{t}))\right\|_{\ell_{1}(\mathbb{N})}\] \[\leq \sup_{1\leq B\leq T_{k}}\left\|\mathbb{I}\{\cdot=B\}-\phi_{k}^{ \exp}(\cdot;B)\right\|_{\ell_{1}(\mathbb{N})}\leq\frac{C(n)e^{0.01(n+1)T_{k}} }{H_{k}^{n}}.\]

Therefore, for these attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can choose:

\[p^{(2,h)}=\beta_{h},\quad\mathbf{W}_{O}^{(1)}=\mathbf{I}_{D\times D}, \quad\mathbf{W}_{V}^{(2,h)}=\alpha_{h}\mathbf{\delta}_{(k+1,1)}^{(d\times d)}\in \mathbb{R}^{D\times D},\] \[\mathbf{W}_{Q}^{(2,h)}=\sqrt{\beta_{h}}\mathbf{\delta}_{(D-M+k-1,1)}^{(1 \times 1)}\in\mathbb{R}^{D\times D},\quad\mathbf{W}_{K}^{(2,h)}=\sqrt{\beta_{h}} \mathbf{\delta}_{(D,1)}^{(1\times 1)}\in\mathbb{R}^{D\times D},\]

where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks.

Then it is easy to verify:

\[\Big{\langle}\mathbf{W}_{Q}^{(2,h)}\mathbf{x}_{t}^{(1)},\mathbf{W}_{K}^{(2,h)}\mathbf{x}_{t-s }^{(1)}\Big{\rangle}+p^{(2,h)}\phi_{\rm lin}(s)=-\beta_{h}\Big{(}s-g_{k}(\mathbf{ x}_{t})\Big{)},\]

which implies:

\[\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\mathbf{Attn}_{t}^{(2,h) }(\mathbf{X}^{(1)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\alpha_ {h}\sum_{s=0}^{+\infty}e^{-\beta_{h}(s-g_{k}(\mathbf{x}_{t}))}\begin{pmatrix}\mathbf{ 0}_{kd}\\ \mathbf{x}_{t-s}\\ \mathbf{0}\end{pmatrix}\in\mathbb{R}^{D},\]

Then it holds that:

\[\mathbf{P}^{(k)}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i} }\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{ \sum_{i=1}^{k}H_{i}}\alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}(s-g_{k}(\mathbf{ x}_{t}))}\mathbf{x}_{t-s},\] \[\mathbf{P}^{(k)}_{\perp}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^ {k}H_{i}}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X}^{(0)})=\mathbf{0},\]

moreover, the following estimate holds:

\[\left\|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\bm {P}^{(k)}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X})-\mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\right\| _{2}\] \[= \left\|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}} \alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}(s-g_{k}(\mathbf{x}_{t}))}\mathbf{x}_{t-s}- \mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\right\|_{2}\]\[= \left\|\phi_{k}^{\exp}(\cdot;g_{k}(\mathbf{x}_{t}))-\mathbb{I}\{\cdot=g_{ k}(\mathbf{x}_{t})\}\right\|_{\ell_{1}(\mathbb{N})}\leq\frac{C(n)e^{0.01(n+1)T_{k}}}{H_{ k}^{n}}.\]
* **Case \(\texttt{type}=\log\).** By Lemma F.5, for any rate \(n\in\mathbb{N}_{+}\), there exists a constant \(C(n)\) and a function \[\phi_{k}^{\mathrm{poly}}(t;B) =\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}} \alpha_{h}(t/B)^{-\beta_{h}}\] \[=\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}} \alpha_{h}\exp\Big{(}\beta_{h}\log B-\beta_{h}\log t\Big{)}\] such that \(\beta_{h}>1\) and \[\sup_{1\leq B\leq T_{k}}\Big{\|}\mathbb{I}\{\cdot=B\}-\phi_{k}^{\mathrm{poly }}(\cdot;B)\Big{\|}_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)T_{k}^{1.01(n+1)} }{H_{k}^{n}}.\] Moreover, Noticing that \(1\leq g_{k}(\mathbf{x}_{t})\leq T_{k}\) holds for any \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\in\mathcal{X}\), the following holds: \[\sup_{\mathbf{X}}\Big{\|}\mathbb{I}\{\cdot=g_{k}(\mathbf{x}_{t})\}-\phi_{ k}^{\mathrm{poly}}(\cdot;g_{k}(\mathbf{x}_{t}))\Big{\|}_{\ell_{1}(\mathbb{N}_{+})}\] \[\leq \sup_{1\leq B\leq T_{k}}\Big{\|}\mathbb{I}\{\cdot=B\}-\phi_{k}^{ \mathrm{poly}}(\cdot;B)\Big{\|}_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)T_{k}^ {1.01(n+1)}}{H_{k}^{n}}.\] Therefore, for these attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can choose: \[p^{(2,h)}=\beta_{h},\quad\mathbf{W}_{O}^{(1)}=\mathbf{I}_{D\times D},\quad\mathbf{W}_{V}^{ (2,h)}=\alpha_{h}\mathbf{\delta}_{(k+1,1)}^{(d\times d)}\in\mathbb{R}^{D\times D},\] \[\mathbf{W}_{Q}^{(2,h)}=\sqrt{\beta_{h}}\mathbf{\delta}_{(D-M+k-1,1)}^{(1 \times 1)}\in\mathbb{R}^{D\times D},\quad\mathbf{W}_{K}^{(2,h)}=\frac{\sqrt{\beta_ {h}}}{\log 2}\mathbf{\delta}_{(D,1)}^{(1\times 1)}\in\mathbb{R}^{D\times D},\] where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks. Then it is easy to verify: \[\Big{\langle}\mathbf{W}_{Q}^{(2,h)}\mathbf{x}_{t}^{(1)},\mathbf{W}_{K}^{(2,h)}\mathbf{x}_{t-s} ^{(1)}\Big{\rangle}+p^{(2,h)}\phi_{\log}(s)=-\beta_{h}\log\Big{(}s/g_{k}(\mathbf{ x}_{t})\Big{)},\] which implies: \[\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\mathbf{Attn}_{t}^{(2,h)}( \mathbf{X}^{(1)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\alpha_{h} \sum_{s=0}^{+\infty}(s/g_{k}(\mathbf{x}_{t}))^{-\beta_{h}}\begin{pmatrix}\mathbf{0}_{kd} \\ \mathbf{x}_{t-s}\\ \mathbf{0}\end{pmatrix}\in\mathbb{R}^{D},\]Then it holds that:

\[\mathbf{P}^{(k)}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}} \mathbf{Attn}_{t}^{(2,h)}(\mathbf{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{ i=1}^{k-1}H_{i}}\alpha_{h}\sum_{s=0}^{+\infty}(s/g_{k}(\mathbf{x}_{t}))^{-\beta_{h}} \mathbf{x}_{t-s},\] \[\mathbf{P}^{(k)}_{\perp}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{ k-1}H_{i}}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X}^{(0)})=\mathbf{0},\]

moreover, the following estimate holds:

\[\left\|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\mathbf{P }^{(k)}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X})-\mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\right\|_ {2}\] \[= \left\|\sum_{h=\sum_{i=1}^{k}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\alpha _{h}\sum_{s=0}^{+\infty}(s/g_{k}(\mathbf{x}_{t}))^{-\beta_{h}}\mathbf{x}_{t-s}-\mathbf{x}_{ t-g_{k}(\mathbf{x}_{t})}\right\|_{2}\] \[= \left\|\sum_{s=0}^{+\infty}\left(\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^ {\sum_{i=1}^{k}H_{i}}\alpha_{h}(s/g_{k}(\mathbf{x}_{t}))^{-\beta_{h}}-\mathbb{I}\{s =g_{k}(\mathbf{x}_{t})\}\right)\mathbf{x}_{t-s}\right\|_{2}\] \[\leq \sum_{s=0}^{+\infty}\left|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{ i=1}^{k}H_{i}}\alpha_{h}(s/g_{k}(\mathbf{x}_{t}))^{-\beta_{h}}-\mathbb{I}\{s=g_{k}( \mathbf{x}_{t})\}\right|\] \[= \left\|\phi_{k}^{\text{poly}}(\cdot;g_{k}(\mathbf{x}_{t}))-\mathbb{I} \{\cdot=g_{k}(\mathbf{x}_{t})\}\right\|_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)T_ {k}^{1.01(n+1)}}{H_{k}^{n}}.\]

Then we combine the estimate for all \(k\in[M]\) for these two cases. It holds that

\[\left\|\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}-\begin{pmatrix}\mathbf{x}_{t}\\ \mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}\\ \vdots\\ \mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}\end{pmatrix}\right\|_{2}\] \[= \left\|\begin{pmatrix}\mathbf{x}_{t}\\ \mathbf{0}_{Md}\end{pmatrix}+\sum_{h=1}^{M}\mathbf{Q}^{(M)}\mathbf{Attn}_{t}^{(2,h)}( \mathbf{X}^{(1)})-\begin{pmatrix}\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}\\ \vdots\\ \mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}\end{pmatrix}\right\|_{2}\] \[= \left\|\sum_{h=1}^{M}\mathbf{Q}^{(M)}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X} )-\begin{pmatrix}\mathbf{0}_{d}\\ \mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}\\ \vdots\\ \mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}\end{pmatrix}\right\|_{2}\] \[= \left\|\sum_{k=1}^{M}\left(\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum _{i=1}^{k}H_{i}}\mathbf{Q}^{(M)}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X})-\begin{pmatrix} \mathbf{0}_{kd}\\ \mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\\ \mathbf{0}\end{pmatrix}\right)\right\|_{2}\] \[\leq \sum_{k=1}^{M}\left\|\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^ {k}H_{i}}\mathbf{Q}^{(M)}\mathbf{Attn}_{t}^{(2,h)}(\mathbf{X})-\begin{pmatrix}\mathbf{0}_{ kd}\\ \mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\\ \mathbf{0}\end{pmatrix}\right\|_{2}\]\[= \sum_{k=1}^{M}\left\|\sum_{h=\sum_{i=1}^{k=1}H_{i}+1}^{\sum_{i=1}^{k= 1}H_{i}}\mathbf{P}^{(k)}\mathbf{Attn}_{i}^{(2,h)}(\mathbf{X})-\mathbf{x}_{t-g_{k}(\mathbf{x}_{t })}\right\|_{2}\] \[\leq \mathcal{E}_{\mathrm{Attn}}(\mathtt{type}):=\begin{cases}\frac{C (n)e^{0.01(n+1)T_{k}}}{H_{k}^{n}},&\mathtt{type}=\mathrm{lin}\\ \frac{C(nT_{k}^{1.01(n+1)}}{H_{k}^{n}},&\mathtt{type}=\mathrm{log}\end{cases}.\]

Similar to the proof of Theorem B.1, we choose the head number:

\[H_{k} =\frac{e^{0.01T_{k}}}{\sum_{j=1}^{M}e^{0.01T_{j}}}H,\quad k\in[M],\quad\mathtt{type}=\mathrm{lin};\] \[H_{k} =\frac{T_{k}^{1.01}}{\sum_{j=1}^{M}T_{j}^{1.01}}H,\quad k\in[M], \quad\mathtt{type}=\mathrm{log}\,.\]

Thus, we obtain the final bound in Step III:

\[\mathcal{E}_{\mathrm{Attn}}^{\mathrm{Soft}}(\mathtt{type})\leq \begin{cases}\frac{C(n)}{H^{n}}\left(\sum_{k=1}^{M}e^{0.01T_{k}}\right)^{n+1},\ \mathtt{type}=\mathrm{lin}\\ \frac{C(n)}{H^{n}}\left(\sum_{k=1}^{M}T_{k}^{1.01}\right)^{n+1},\ \mathtt{ type}=\mathrm{log}\end{cases}.\]

Furthermore, by choosing \(\mathcal{E}_{\mathrm{Attn}}(\mathtt{type})\leq 1\), it holds that

\[\left\|\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}\right\|_{\infty}\leq \left\|\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}-\begin{pmatrix}\mathbf{x}_{t}\\ \mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}\\ \vdots\\ \mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}\end{pmatrix}\right\|_{\infty}+\left\|\begin{pmatrix} \mathbf{x}_{t}\\ \mathbf{x}_{t-g_{1}(\mathbf{x}_{t})}\\ \vdots\\ \mathbf{x}_{t-g_{M}(\mathbf{x}_{t})}\end{pmatrix}\right\|_{\infty}\] \[\leq \mathcal{E}_{\mathrm{Attn}}(\mathtt{type})+1\leq 2.\]

**Step IV.** Approximate the nonlinear function by \(2\)-nd FFN layer.

In this step, we aim to approximate the function \(f\) using two-layer network. By Lemma G.6, there exists a two-layer neural network with \(m\) neurons

\[f_{(2)}^{2\mathrm{NN}}(\mathbf{x})=\sum_{k=1}^{m}a_{k}^{(2)}\sigma\left(\mathbf{b}_{k} ^{(2)\ \top}\mathbf{x}+c_{k}^{(2)}\right)\]

defined on \(\mathbb{R}^{(M+1)d}\) such that

\[\left\|f-f_{(2)}^{2\mathrm{NN}}\right\|_{L^{\infty}([-2,2]^{(M+1)d})}\leq \tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{\mathcal{B}}}{\sqrt{m}}\right).\]

Denote \(\bar{\mathbf{b}}_{k}^{(2)}=({\mathbf{b}_{k}^{(2)}}^{\top},\mathbf{0}^{\top})^{\top}\in \mathbb{R}^{D}\) for \(k\in[m]\). And we consider the following two-layer neural network with \(m\) neurons defined on \(\mathbb{R}^{D}\):

\[\mathrm{FFN}^{(2)}(\mathbf{x}):=\sum_{k=1}^{m}a_{k}^{(2)}\sigma\left(\bar{\mathbf{b}}_ {k}^{(2)\ \top}\mathbf{x}+c_{k}^{(2)}\right).\]

It is easy to verify:\[\text{FFN}^{(2)}\left(\mathbf{x}_{t}^{(3/2)}\right)=f_{(2)}^{2\text{NN}}\left(\mathbf{Q}^{ (M)}\mathbf{x}_{t}^{(3/2)}\right).\]

Moreover,

\[\mathcal{E}_{\text{FFN}}^{(2)}:=\left\|f-\text{FFN}^{(2)}\right\|_{L^{\infty}([ -2,2]^{(M+1)t})}\leq\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{\mathcal{ B}}}{\sqrt{m}}\right).\]

**The final bound.**

For any \(t\) and \(\left\|\mathbf{X}\right\|\in\mathcal{X}\), it holds that

\[\left\|\mathbf{H}_{t}(\mathbf{X})-\mathbf{x}_{t}^{(2)}\right\|=\left|f \left(\mathbf{x}_{t},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})},\cdots\mathbf{x}_{t-g_{M}(\mathbf{x}_{t} )}\right)-\text{FFN}^{(2)}\left(\mathbf{x}_{t}^{(3/2)}\right)\right|\] \[= \left|f\left(\mathbf{x}_{t},\mathbf{x}_{t-g_{1}(\mathbf{x}_{t})},\cdots\mathbf{x} _{t-g_{M}(\mathbf{x}_{t})}\right)-f\left(\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}\right)\right|\] \[\quad+f\left(\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}\right)-f_{(2)}^{2 \text{NN}}\left(\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}\right)\Big{|}\] \[\leq \left|f\left(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots\mathbf{x}_{t-t_{M}} \right)-f\left(\mathbf{Q}^{(M)}\mathbf{x}_{t}^{(3/2)}\right)\right|+\left|f\left(\mathbf{Q }^{(M)}\mathbf{x}_{t}^{(3/2)}\right)-f_{(2)}^{2\text{NN}}\left(\mathbf{Q}^{(M)}\mathbf{x} _{t}^{(3/2)}\right)\right|\] \[\leq \left\|f\right\|_{\text{Lip}}\left\|\left(\mathbf{x}_{t}^{\top},\mathbf{ x}_{t-t_{1}}^{\top},\cdots\mathbf{x}_{t-t_{M}}^{\top}\right)^{\top}-\mathbf{Q}^{(M)} \mathbf{x}_{t}^{(3/2)}\right\|_{2}+\left\|f-f_{(2)}^{2\text{NN}}\right\|_{L^{ \infty}([-2,2]^{(M+1)D})}\] \[\leq \left\|f\right\|_{\text{Lip}}\cdot\mathcal{E}_{\text{Attn}}+ \mathcal{E}_{\text{FFN}},\]

where

\[\mathcal{E}_{\text{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{ \mathcal{B}}}{\sqrt{m}}\right);\]

Moreover, recalling our proof in Step II, we further need the hard condition:

\[m\geq\begin{cases}\tilde{\Omega}\left(\sum_{r=1}^{M}\left\|g_{r}\right\|_{ \mathcal{B}}^{2}\right),&\texttt{type}=\lim\\ \tilde{\Omega}\left(\sum_{r=1}^{M}\left\|\log g_{r}\right\|_{\mathcal{B}}^{2 }T_{r}^{2}\right),&\texttt{type}=\log\end{cases}.\]

Due to the arbitrariness of \(t\) and \(\mathbf{X}\), the proof is completed.

**Remark C.3**.: **The core step** in this proof is **Step III**, where the extraction of the memory functions is achieved through a _a nice interaction between the temporal space (provided by RPE) and the token space (provided by DP)_. Specifically, the memory functions \(g_{i}(\mathbf{x}_{t})\) (in token space) are mapped into the temporal space \(s\), resulting in the form of \(\mathbf{x}_{s-g_{i}(\mathbf{x}_{t})}\) for DP Attn with \(\lim\)-RPE or \(\log(s/g_{i}(\mathbf{x}_{t}))\) for DP Attn with \(\log\)-RPE.

### Proof of Proposition 4.2

For Proposition 16, we denote the following one-layer Attn hypothesis class:

\[\mathcal{ATTN}_{(1,H)}^{\mathsf{type}} :=\big{\{}\mathbf{Attn}:\mathbf{TF}\text{ is a $1$-layer, $H$-head (normalization-free) Attn with type-RPE}\big{\}};\] \[\mathcal{ATTN}_{(1,H)}^{\mathsf{DPF},\mathsf{type}} :=\big{\{}\mathbf{TF}:\mathbf{Attn}\text{ is a $1$-layer, $H$-head DP-free Attn with type-RPE}\big{\}}. \tag{16}\]

**Proposition C.4** (The formal version of Proposition 4.2).: _Consider \(1\)-layer Attn. Then, there exists a target \(\mathbf{H}\in\mathcal{H}_{(1,\,1)}^{\text{adap}}\) (10) such that:_

_(A) (Attn with DP) For any \(\epsilon>0\), there exists a \(1\)-layer Attn \(\mathbf{Attn}^{\mathrm{DP}}\in\mathcal{ATTN}_{(1,H)}^{\mathsf{type}}\) such that_

\[\big{\|}\hskip-1.0pt\big{\|}\hskip-1.0pt\mathbf{H}-\mathbf{Attn}^{\mathrm{DP} }\big{\|}\hskip-1.0pt\big{\|}\leq\epsilon.\]

_(B) (Attn without DP) For any \(1\)-layer DP-free Attn \(\mathbf{Attn}^{\mathrm{DPF}}\in\mathcal{ATTN}_{(1,H)}^{\mathrm{DPF},\mathsf{ type}}\), a uniform lower bound holds:_

\[\big{\|}\hskip-1.0pt\big{\|}\hskip-1.0pt\mathbf{H}-\mathbf{Attn}^{\mathrm{DPF }}\big{\|}\hskip-1.0pt\big{\|}\geq\frac{2}{3}.\]

Proof of Proposition c.4.: Consider the following target function \(\mathrm{H}\in\mathcal{H}_{1,1}^{\text{Adap}}\). Let the input sequence \(\mathbf{X}\in\mathcal{X}=\{-1,0,1\}^{\mathbb{Z}}\), and we consider the target

\[y_{t}=\mathrm{H}_{t}(\mathbf{X}):=x_{t-g(x_{t})},\]

where the adaptive memory is

\[g(x)=\begin{cases}0,&x=-1\\ 1,&x=0\\ 2,&x=1\end{cases}.\]

**Part (A).** The Efficiency of Attn with DP.

First, we choose the embedding dimension \(D=2\), and select simple embedding \(\mathbf{W}_{E}=\mathbf{b}_{E}=(1,0)^{\top}\in\mathbb{R}^{2\times 1}\). Then for any input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\), the token after embedding satisfies:

\[x_{t}^{(0)}=\mathbf{W}_{E}x_{t}+\mathbf{b}_{E}=(x_{t},1)^{\top}.\]

We consider one-layer normalization-free Self-attention with \(\phi_{\mathrm{exp}}\), which has the form:

\[\mathrm{Attn}_{t}^{\mathrm{DP}}(\mathbf{X})=\mathbf{W}_{O}\sum_{h=1}^{H}\mathbf{W}_{V}^{(1,h)}\sum_{s=0}^{+\infty}\mathbf{x}_{t-s}^{(0)}\exp\left(\left\langle\mathbf{W}_{Q}^{( l,h)}\mathbf{x}_{t}^{(0)},\mathbf{W}_{K}^{(1,h)}\mathbf{x}_{t-s}^{(0)}\right\rangle+p^{(1,h )}s\right).\]

By Lemma F.2 (for \(n=1\)), there exists a constant \(C>0\) and a function

\[\phi^{\mathrm{exp}}(t;B)=\sum_{h=1}^{H}\alpha_{h}\exp(-\beta_{h}(t-B))=\sum_{h =1}^{H}\alpha_{h}\exp\left(\beta_{h}B-\beta_{h}t\right)\]

such that \(\beta_{h}>0\) and

\[\sup_{0\leq B\leq 2}\left\|\mathbb{I}\{\cdot=B\}-\phi^{\mathrm{exp}}(\cdot;B) \right\|_{\ell_{1}(\mathbb{N})}\leq\frac{Ce^{0.01\cdot 2\cdot 2}}{H}= \mathcal{O}\left(\frac{1}{H}\right).\]Moreover, Noticing that \(0\leq g(x_{t})\leq 2\) holds for any \(\mathbf{X}=(x_{t})_{t\in\mathbb{Z}}\in\mathcal{X}\), the following holds:

\[\sup_{\mathbf{X}}\left\|\mathbb{I}\{\cdot=g(x_{t})\}-\phi^{\text{exp}} (\cdot;g(x_{t}))\right\|_{\ell_{1}(\mathbb{N})}\] \[\leq\sup_{0\leq B\leq 2}\left\|\mathbb{I}\{\cdot=B\}-\phi^{\text{ exp}}(\cdot;B)\right\|_{\ell_{1}(\mathbb{N})}\leq\mathcal{O}\left(\frac{1}{H}\right).\]

Therefore, for attention heads (\(1\leq h\leq H\)), we can choose:

\[p^{(1,h)}=\beta_{h},\quad\mathbf{W}_{O}^{(1)}=(1,0)^{\top}\in\mathbb{ R}^{2\times 1},\quad\mathbf{W}_{V}^{(1,h)}=\alpha_{h}\mathbf{\delta}_{(1,1)}^{(1\times 1)} \in\mathbb{R}^{2\times 2},\] \[\mathbf{W}_{Q}^{(1,h)}=\sqrt{\beta_{h}}(1,1)^{\top}\in\mathbb{R}^{2 \times 1},\quad\mathbf{W}_{K}^{(1,h)}=\sqrt{\beta_{h}}(0,1)^{\top}\in\mathbb{R}^{2 \times 1},\]

where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks.

Then it is easy to verify:

\[\Big{\langle}\mathbf{W}_{Q}^{(1,h)}\mathbf{x}_{t}^{(0)},\mathbf{W}_{K}^{(1,h) }\mathbf{x}_{t-s}^{(0)}\Big{\rangle}+p^{(2,h)}s=-p^{(1,h)}\Big{(}s-(x_{t}+1)\Big{)} =-p^{(1,h)}\Big{(}s-g(x_{t})\Big{)}.\]

Thus, the following estimate holds:

\[\left\|\mathrm{Attn}_{t}^{\text{DP}}(\mathbf{X})-x_{t-g(x_{t})}\right\| _{2}\] \[= \left\|\sum_{h=1}^{H}\alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h} (s-g(x_{t}))}x_{t-s}-x_{t-g(x_{t})}\right\|_{2}\] \[= \left\|\sum_{s=0}^{+\infty}\left(\sum_{h=1}^{H}\alpha_{h}e^{- \beta_{h}(s-g(x_{t}))}-\mathbb{I}\{s=g(x_{t})\}\right)x_{t-s}\right\|_{2}\] \[\leq \sum_{s=0}^{+\infty}\left|\sum_{h=1}^{H}\alpha_{h}e^{-\beta_{h} (s-g_{k}(x_{t}))}-\mathbb{I}\{s=g(x_{t})\}\right|\] \[= \left\|\phi^{\text{exp}}(\cdot;g(x_{t}))-\mathbb{I}\{\cdot=g(x_{t })\}\right\|_{\ell_{1}(\mathbb{N})}\leq\mathcal{O}\left(\frac{1}{H}\right).\]

Due to the arbitrariness of \(t\) and \(\mathbf{X}\), the proof is completed: for any \(\epsilon>0\), we only need to use \(H=\Omega(1/\epsilon)\) heads to approximate it.

**Part (B).** The Hardness Result of Attn without DP.

We consider one-layer Dot-product-free Self-attention with \(\phi_{\text{exp}}\) or \(\phi_{\text{log}}\). For any input \(\mathbf{X}\), the corresponding output can be written as:

\[\mathrm{Attn}_{t}^{\text{DPF}}(\mathbf{X})=\sum_{s=0}^{+\infty}\rho_{s}(W_{E}x_{t- s}+b_{E}).\]

For simplicity, we denote:

\[x_{t}^{(0)}=W_{E}x_{t}+b_{E},\]

then we have the following estimate:

\[\left|\left|\mathrm{H}-\mathrm{Attn}^{\text{DPF}}\right|\right| \right|= \sup_{t}\sup_{\mathbf{X}}\left|\mathrm{H}_{t}^{\text{DPF}}(\mathbf{X})- \mathrm{Attn}_{t}(\mathbf{X})\right|\geq\sup_{\mathbf{X}}\left|\mathrm{H}_{0}(\mathbf{X})- \mathrm{Attn}_{0}^{\text{DPF}}(\mathbf{X})\right|\] \[= \left\|\left|\mathrm{H}-\mathrm{Attn}^{\text{DPF}}\right|\right| \right|\leq\left\|\left|\mathrm{H}-\mathrm{Attn}^{\text{DPF}}\right\|\right\|\] \[\leq \left\|\mathrm{H

[MISSING_PAGE_EMPTY:35]

where the parameters \(\mathbf{w}^{(l,h)}\in\mathbb{R}^{D\times 1}\).

Notice that in TMX Transformer, the revised term requires only \(\mathcal{O}(D)\) parameters, much less than \(\mathcal{O}(D^{2})\) in standard Dot-product Transformer.

Consequently, we define the following TMX Transformer hypothesis class.

\[\mathcal{TF}_{(1,H,m)}^{\mathrm{TMX,\,type}}:=\big{\{}\mathbf{TF}: \mathbf{TF}\text{ is a }1\text{-layer, }H\text{-head, }m\text{-width} \tag{17}\] \[\text{(normalization-free) TMX Transformer with type-RPE}\big{\}}.\]

**Proposition C.6** (The formal version of Proposition 4.3).: _Under the same conditions in Theorem 4.1, there exists a two-layer TMX Transformer \(\mathbf{TF}\in\mathcal{TF}_{(2,H,m)}^{\mathrm{TMX,\,type}}\) (17) such that it can achieve the same approximation rate as standard Transformer presented in Theorem 4.1._

Proof of Proposition c.6.: It is worth noting that TMX Transformer only replaces \(\left\langle\mathbf{W}_{Q}^{(l,h)}\mathbf{x}_{t},\mathbf{W}_{K}^{(l,h)}\mathbf{x}_{t-s} \right\rangle+p^{(l,h)}\phi_{\texttt{type}}(s)\) in standard Transformer with \(p^{(l,h)}\Big{(}\phi_{\texttt{type}}\left(s\right)-\phi_{\texttt{type}}\left( \mathbf{w}^{(l,h)}\ ^{\top}\mathbf{x}_{t}\right)\Big{)}\). Therefore, the proof is highly similar to that of Theorem C.2. We only need to prove that TMX Attn can also achieve **Step I** and **Step III** in the proof of Theorem C.2.

**Step L**. Step I is trivial due to the same use of the residual block.

**Step III. Extract the adaptive memories by the second Attn layer.**

We still consider to use \(H_{k}\) attention heads (from \(\sum_{i=1}^{k-1}H_{i}+1\)-th head to \(\sum_{i=1}^{k}H_{i}\)-th head) to extract it, and it satisfies to \(\sum_{k=1}^{M}H_{k}=H\).

Now we consider the extraction of \(k\)-th adaptive memory \(\mathbf{x}_{t-g_{k}(\mathbf{x}_{t})}\) (\(1\leq k\leq M\)).

* **Case \(\texttt{type}=\mathrm{lin}\).** For the proof of standard Transformer (the proof of Theorem C.2), for the attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can construct specific \(p^{(2,h)},\mathbf{W}_{Q}^{(2,h)},\mathbf{W}_{K}^{(2,h)},\mathbf{W}_{V}^{(2,h)}\) such that \[\Big{\langle}\mathbf{W}_{Q}^{(2,h)}\mathbf{x}_{t}^{(1)},\mathbf{W}_{K}^{(2,h )}\mathbf{x}_{t-s}^{(1)}\Big{\rangle}+p^{(2,h)}\phi_{\mathrm{lin}}(s)=-\beta_{h} \Big{(}s-g_{k}(\mathbf{x}_{t})\Big{)}.\] In this proof, we only need to prove that we can also construct specific \(\mathbf{w}^{(l,h)},\mathbf{W}_{V}^{(2,h)}\) such that \[p^{(2,h)}\left(\phi_{\mathrm{lin}}(s)-\phi_{\mathrm{lin}}\left( \mathbf{w}^{(2,h)}\ ^{\top}\mathbf{x}_{t}^{(1)}\right)\right)=-\beta_{h}\Big{(}s-g_{k}( \mathbf{x}_{t})\Big{)}.\] Recalling the proof of Theorem C.2, \[\mathbf{x}_{t}^{(1)}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},g_{1}(\mathbf{x}_{t}),\cdots,g _{M}(\mathbf{x}_{t}),1)^{\top}\in\mathbb{R}^{D}.\] Therefore, we can choose \[p^{(2,h)}=\beta_{h},\quad\mathbf{w}^{(2,h)}=\mathbf{\delta}_{(D-M+k-1,1)}^{(1\times 1 )}\in\mathbb{R}^{D},\quad\mathbf{W}_{V}^{(2,h)}=\alpha_{h}\mathbf{\delta}_{(k+1,1)}^{(d \times d)}\in\mathbb{R}^{D\times D}.\] where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks. The the following holds: \[p^{(2,h)}\left(\phi_{\mathrm{lin}}(s)-\phi_{\mathrm{lin}}\left( \mathbf{w}^{(2,h)}\ ^{\top}\mathbf{x}_{t}^{(1)}\right)\right)\]\[= -p^{(2,h)}\left(s-\mathbf{w}^{(2,h)}\,{}^{\top}\mathbf{x}_{t}^{(1)}\right)=- \beta_{h}\Big{(}s-g_{k}(\mathbf{x}_{t})\Big{)}.\]
* **Case \(\mathtt{type}=\log\).** For the proof of standard Transformer (the proof of Theorem C.2), for the attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can construct specific \(p^{(2,h)},\mathbf{W}_{Q}^{(2,h)},\mathbf{W}_{K}^{(2,h)},\mathbf{W}_{V}^{(2,h)}\) such that \[\Big{\langle}\mathbf{W}_{Q}^{(2,h)}\mathbf{x}_{t}^{(1)},\mathbf{W}_{K}^{(2,h)}\mathbf{x}_{t-s} ^{(1)}\Big{\rangle}+p^{(2,h)}\phi_{\log}(s)=-\beta_{h}\log\Big{(}s/g_{k}(\mathbf{ x}_{t})\Big{)}.\] In this proof, we only need to prove that we can also construct specific \(\mathbf{w}^{(l,h)},\mathbf{W}_{V}^{(2,h)}\) such that \[p^{(2,h)}\left(\phi_{\log}(s)-\phi_{\log}\left(\mathbf{w}^{(2,h)}\,{}^{\top}\mathbf{x }_{t}^{(1)}\right)\right)=-\beta_{h}\log\Big{(}s/g_{k}(\mathbf{x}_{t})\Big{)}.\] Recalling the proof of Theorem C.2, \[\mathbf{x}_{t}^{(1)}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top},\log g_{1}(\mathbf{x}_{t}),\cdots,\log g_{M}(\mathbf{x}_{t}),\log 2)^{\top}.\] Therefore, we can choose \[p^{(2,h)}=\beta_{h},\quad\mathbf{w}^{(2,h)}=\mathbf{\delta}_{(D-M+k-1,1)}^{(1\times 1 )}\in\mathbb{R}^{D},\quad\mathbf{W}_{V}^{(2,h)}=\alpha_{h}\mathbf{\delta}_{(k+1,1)}^{(d \times d)}\in\mathbb{R}^{D\times D}.\] where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks. The the following holds: \[p^{(2,h)}\left(\phi_{\log}(s)-\phi_{\log}\left(\mathbf{w}^{(2,h)}\,{}^ {\top}\mathbf{x}_{t}^{(1)}\right)\right)\] \[= -p^{(2,h)}\log\Big{(}s/\left(\mathbf{w}^{(2,h)}\,{}^{\top}\mathbf{x}_{t}^ {(1)}\right)\Big{)}=-\beta_{h}\log\Big{(}s/g_{k}(\mathbf{x}_{t})\Big{)}.\]

The rest of the proof is exactly the same as the proof of Theorem C.2, and we do not repeat it.

Proof of Section 4.3

### Proof of Theorem 4.4

In this subsection, we give the detailed proofs for the general case of \(K\)-adaptive, long but \(M\)-sparse memory:

\[\mathbf{y}_{t}=\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}}),\]

where the adaptive memories satisfy:

\[t_{1} =g_{1}(\mathbf{x}_{t});\] \[t_{2} =g_{2}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}});\] \[\cdots\] \[t_{K+1} =g_{K+1}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{K}});\] \[\cdots\] \[t_{K+2} =g_{K+2}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{K}});\] \[\cdots\] \[t_{M} =g_{K+1}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{K}}),\]

where \(1\leq t_{k}\leq T_{k}\) holds for any \(k\in[M]\).

**Theorem D.1** (Restatement of Theorem 4.4).: _For any target \(\mathbf{H}\in\mathcal{H}^{\mathrm{Adap}}_{(K,M)}\), rate \(n\in\mathbb{N}_{+}\), and \(H,m\in\mathbb{N}_{+}\), there exists an \(L\)-layer (\(L=K+1+\mathbb{I}\{M\geq K+1\}\)) Transformer \(\mathbf{TF}\in\mathcal{TF}^{\mathrm{NF},\mathtt{type}}_{(L,H,m)}\) (12) and a constant \(C(n)\) such that: if the width satisfies_

\[m\geq\begin{cases}\tilde{\Omega}\Big{(}\max\limits_{i\in[K]}\vee\sum\limits_{i =K+1}^{M}\left\|g_{i}\right\|_{\mathcal{B}}^{2}\Big{)},&\mathtt{type}=\mathrm{ lin},\\ \tilde{\Omega}\Big{(}\max\limits_{i\in[K]}\vee\sum\limits_{i=K+1}^{M}\left\| \log g_{i}\right\|_{\mathcal{B}}^{2}T_{i}^{2}\Big{)},&\mathtt{type}=\mathrm{ log}\end{cases},\]

_then the following approximation rate holds:_

\[\left\|\mathbf{H}-\mathbf{TF}\right\|\leq\mathcal{E}_{\mathrm{FFN}}+\mathcal{ E}_{\mathrm{Attn}}(\mathtt{type}),\]

_where \(\mathcal{E}_{\mathrm{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{ \mathcal{B}}}{\sqrt{m}}\right)\) and_

\[\mathcal{E}_{\mathrm{Attn}}(\mathtt{type})=\begin{cases}\mathcal{O}\left( \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}+\left(\sum_{l=K+1}^{ M}e^{0.01T_{l}}\right)^{2n+2}}\right),\ \mathtt{type}=\mathrm{lin}\\ \mathcal{O}\left(\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}T_{l}^{2.02(n+1)}+\left( \sum_{l=K+1}^{M}T_{l}^{1.01}\right)^{2n+2}}\right),\ \mathtt{type}=\mathrm{ log}\end{cases}.\]

Proof of Theorem D.1.:

First, we choose the embedding dimension \(D=(M+1)(d+1)\), and select the same embedding matrix \(\mathbf{W}_{E}=(\mathbf{I}_{d},\mathbf{0})^{\top}\in\mathbb{R}^{D\times d},\mathbf{b}_{E}=\mathbf{0 }=\in\mathbb{R}^{D}\) as the proof of Theorem C.2. Moreover, we still use the network with precision \(\widehat{\mathrm{FFN}}\) defined in Appendix C.1 to tackle the discrete values of memories.

Then for any input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\), the token after embedding satisfies:

\[\mathbf{x}_{t}^{(0)}=\mathbf{W}_{E}\mathbf{x}_{t}+\mathbf{b}_{E}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{ \top})^{\top}\in\mathbb{R}^{D}.\]

Thus, for \(L\)-layer (\(L=K+1+\mathbb{I}\{M\geq K+1\}\)) normalization-free Transformer \(\mathbf{TF}\in\mathcal{TF}^{\mathrm{NF},\mathtt{type}}_{(L,H,m)}\) with \(\phi_{\mathtt{type}}\), the output token \(\mathbf{x}_{t}^{(K+1)}\) of \(t\)-th input token satisfies:

\[\mathbf{x}_{t}^{(l-1/2)}=\mathbf{x}_{t}^{(l)}+\mathbf{W}_{O}^{(l)}\sum_{h=1}^{H}\mathbf{Attn }_{t}^{(l,h)}(\mathbf{X}^{(l-1)}),\ 1\leq l\leq L+1\]\[\mathbf{x}_{t}^{(l)} =\mathbf{x}_{t}^{(l-1/2)}+\widehat{\mathbf{FFN}}^{(l)}(\mathbf{x}_{t}^{(l-1/2 )}),\,1\leq l\leq L\] \[\mathbf{x}_{t}^{(L+1)} =\mathbf{FFN}^{(L+1)}(\mathbf{x}_{t}^{(L+1/2)}),\]

where

\[\mathbf{Attn}_{t}^{(l,h)}(\mathbf{X})=\mathbf{W}_{V}^{(l,h)}\sum_{s=0}^{+\infty}\mathbf{x}_ {t-s}\exp\left(\left\langle\mathbf{W}_{Q}^{(l,h)}\mathbf{x}_{t},\mathbf{W}_{K}^{(l,h)}\mathbf{ x}_{t-s}\right\rangle+p^{(l,h)}\phi_{\texttt{type}}(s)\right).\]

Since the proof of this theorem is similar to the proof of Theorem C.2, we mainly discuss the differences.

The proof can be summarized as the following process:

* **Case \(\texttt{type}=\ln\).*
* **Regime \(M\geq K+1\).*
* \[\mathbf{x}_{t}^{(0)}\] Step 1. 1-st Att \(\downarrow\) \[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}\] Step 2. 1-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(1)} =\mathbf{x}_{t}^{(1/2)}+(\mathbf{0}^{\top},t_{1},\mathbf{0}_{M-1}^{\top},1)^{\top}\] Step 3. 2-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(3/2)} \approx\mathbf{x}_{t}^{(1)}+(\mathbf{0}_{d}^{\top},\mathbf{x}_{t-t_{1}}^{\top },\mathbf{0}^{\top})^{\top}\] Step 4. 2-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(2)} =\mathbf{x}_{t}^{(3/2)}+(\mathbf{0}^{\top},t_{2},\mathbf{0}_{M-1}^{\top})^{\top}\] Step 5. 3-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(5/2)} \approx\mathbf{x}_{t}^{(2)}+(\mathbf{0}_{2d}^{\top},\mathbf{x}_{t-t_{2}}^{\top },\mathbf{0}^{\top})^{\top}\] \[\ldots\] Step \(2K+1\). \(K+1\)-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(K+1/2)} \approx\mathbf{x}_{t}^{(K)}+(\mathbf{0}_{Kd}^{\top},\mathbf{x}_{t-t_{K}}^{\top },\mathbf{0}^{\top})^{\top}\] Step \(2K+2\). \(K+1\)-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(K+1)} =\mathbf{x}_{t}^{(K+1/2)}+(\mathbf{0}^{\top},t_{K+1},\cdots,t_{M},0)^{\top}\] Step \(2K+3\). \(K+2\)-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(K+3/2)} \approx\mathbf{x}_{t}^{(K+1)}+(\mathbf{0}_{(K+1)d}^{\top},\mathbf{x}_{t-t_{K+ 1}}^{\top},\cdots,\mathbf{x}_{t-t_{M}}^{\top},0)^{\top}\] Step \(2K+4\). \(K+2\)-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(K+2)} \approx\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}})\]
* **Regime \(M=K\).*
* \[\mathbf{x}_{t}^{(0)}\] Step 1. 1-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(1/2)} =\mathbf{x}_{t}^{(0)}\] Step 2. 1-st FFN \(\downarrow\) \[\mathbf{x}_{t}^{(1)} =\mathbf{x}_{t}^{(1/2)}+(\mathbf{0}^{\top},t_{1},\mathbf{0}_{M-1}^{\top},1)^{\top}\] Step 3. 2-st Attn \(\downarrow\) \[\mathbf{x}_{t}^{(3/2)} \approx\mathbf{x}_{t}^{(1)}+(\mathbf{0}_{d}^{\top},\mathbf{x}_{t-t_{1}}^{\top },\mathbf{0}^{\top})^{\top}\]

[MISSING_PAGE_EMPTY:40]

\[\mathbf{x}_{t}^{(2)} =\mathbf{x}_{t}^{(3/2)}+(\mathbf{0}^{\top},\log t_{2},\mathbf{0}_{M-1}^{\top})^{\top}\] Step 5. 3-st Attn \[\downarrow\] \[\mathbf{x}_{t}^{(5/2)} \approx\mathbf{x}_{t}^{(2)}+(\mathbf{0}_{2d}^{\top},\mathbf{x}_{t-t_{2}}^{\top },\mathbf{0}^{\top})^{\top}\] \[\ldots\] Step \[2K+1\]. \[K+1\] -st Attn \[\downarrow\] \[\mathbf{x}_{t}^{(K+1/2)} \approx\mathbf{x}_{t}^{(K)}+(\mathbf{0}_{Kd}^{\top},\mathbf{x}_{t-t_{K}}^{\top },\mathbf{0}^{\top})^{\top}\] Step \[2K+2\]. \[K+1\] -st FFN \[\downarrow\] \[\mathbf{x}_{t}^{(K+1)} \approx\mathbf{f}(\mathbf{x}_{t},\mathbf{x}_{t-t_{1}},\cdots,\mathbf{x}_{t-t_{M}})\]

For simplicity, we denote the following projection matrices:

\[\mathbf{P}^{(k)} :=(\mathbf{0}_{d\times kd}\quad\mathbf{I}_{d\times d}\quad\mathbf{0})\in\mathbb{ R}^{d\times D},\quad 1\leq k\leq M;\] \[\mathbf{P}_{\perp}^{(k)} :=\begin{pmatrix}\mathbf{I}_{kd\times kd}&\mathbf{0}_{d\times d}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}_{d\times d}&\mathbf{I}_{(D-(k+1)d)\times(D-(k+1)d)}\end{pmatrix}\in \mathbb{R}^{(D-d)\times D},\quad 1\leq k\leq M;\] \[\mathbf{Q}^{(k)} :=\begin{pmatrix}\mathbf{I}_{(k+1)d\times(k+1)d}&\mathbf{0})\in\mathbb{R}^ {(k+1)d\times D},\quad 1\leq k\leq M;\end{pmatrix}\] \[\mathbf{Q}_{\perp}^{(k)} :=\begin{pmatrix}\mathbf{0}&\mathbf{I}_{(D-(k+1)d)\times(D-(k+1)d)}\end{pmatrix} \in\mathbb{R}^{(D-(k+1)d)\times D},\quad 1\leq k\leq M;\] \[\mathbf{R} :=\begin{pmatrix}\mathbf{0}_{(M-K)d\times(K+1)d}&\mathbf{I}_{(M-K)d\times( M-K)d}&\mathbf{0}\end{pmatrix}\in\mathbb{R}^{(M-K)d\times D};\] \[\mathbf{R}_{\perp} :=\begin{pmatrix}\mathbf{I}_{(K+1)d\times(K+1)d}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}&\mathbf{I}_{(M+1)\times(M+1)}\end{pmatrix}\in\mathbb{R}^{(D-(M-K)d) \times D}.\]

**Step 1** is trivial due to the use of the residual block.

**Step 2.** In the same way as Step II in the proof of Theorem C.2, we obtain the conclusion in this step: If the width of FFN satisfies

\[m\geq\begin{cases}\tilde{\Omega}\left(\left\|g_{1}\right\|_{\mathsf{B}}^{2} \right),&\texttt{type}=\text{lin}\\ \tilde{\Omega}\left(\left\|\log g_{1}\right\|_{\mathsf{B}}^{2}T_{1}^{2}\right),&\texttt{type}=\text{log}\end{cases},\]

then the following holds:

\[\mathbf{x}_{t}^{(1)}=\mathbf{x}_{t}^{(1/2)}+(\mathbf{0}^{\top},t_{1},\mathbf{0}_{M}^{\top},1)^ {\top}.\]

Thus, **(E1)** holds for \(l=2\).

**Step 3 \(\sim\) Step \(2K+1\).**

* **FFN layers.*
* We use \(l\)-th (\(2\leq l\leq K\)) FFN layer to express \(l\)-th memory \(t_{l}\) exactly. By Lemma G.6, there exists a two-layer neural network with \(m\) neurons defined on \(\mathbb{R}^{ld}\) \[f_{(l)}^{\rm 2NN}(\mathbf{x})=\sum_{k=1}^{m}a_{k}^{(l)}\sigma(\mathbf{b}_{k}^{(l) \top}\mathbf{x}+c_{k}^{(l)})\] such that \[\left\|g_{l}-f_{(l)}^{\rm 2NN}\right\|_{L^{\infty}([-2,2]^{ld})}\leq \tilde{\mathcal{O}}\left(\frac{\left\|g_{l}\right\|_{\mathsf{B}}}{\sqrt{m}} \right).\] For \(l\)-th FFN layer, we only need to arrange the parameters \(a_{k}^{(l)}\), \(\mathbf{b}_{k}^{(l)}\), and \(c_{k}^{(l)}\) (\(k\in[m],2\leq l\leq M\)).

[MISSING_PAGE_FAIL:42]

Therefore, for the attention heads \(h\) (\(h\in[H]\)) in each layer \(l\) (\(1\leq l\leq K\)), we can choose: \[p^{(l+1,h)}=\beta_{l,h},\quad\mathbf{W}_{O}^{(l+1)}=\mathbf{I},\quad\mathbf{W} _{V}^{(l+1,h)}=\alpha_{l,h}\mathbf{\delta}_{(l+1,1)}^{(d\times d)}\in\mathbb{R}^{D \times D},\] \[\mathbf{W}_{Q}^{(l+1,h)}=\sqrt{\beta_{l,h}}\mathbf{\delta}_{(D-M+1,1)}^{(1 \times 1)}\in\mathbb{R}^{D\times(D/H)},\] \[\mathbf{W}_{K}^{(l+1,h)}=\sqrt{\beta_{l,h}}\mathbf{\delta}_{(D,1)}^{(1 \times 1)}\in\mathbb{R}^{D\times(D/H)},\] where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks.
* **Case \(\mathtt{type}=\log\).*
* **FFN layers.*
* We use \(l\)-th (\(2\leq l\leq K\)) FFN layer to express \(l\)-th memory \(t_{l}\) exactly. By Lemma G.6, there exists a two-layer neural network with \(m\) neurons defined on \(\mathbb{R}^{ld}\) \[f_{(l)}^{2\text{NN}}(\mathbf{x})=\sum_{k=1}^{m}a_{k}^{(l)}\sigma(\mathbf{b}_{k}^{(l)} \,^{\top}\mathbf{x}+c_{k}^{(l)})\] such that \[\left\|\log g_{l}-f_{(l)}^{2\text{NN}}\right\|_{L^{\infty}}\leq\tilde{\mathcal{ O}}\left(\frac{\left\|\log g_{l}\right\|_{\mathcal{B}}}{\sqrt{m}}\right).\] For \(l\)-th FFN layer, we only need to arrange the parameters \(a_{k}^{(l)}\), \(\mathbf{b}_{k}^{(l)}\), and \(c_{k}^{(l)}\) (\(k\in[m],2\leq l\leq M\)). Denote \(\bar{\mathbf{b}}_{k}^{(l)}={({\mathbf{b}_{k}^{(l)}}^{\top},\mathbf{0}^{\top})}^{\top}\in \mathbb{R}^{D}\) for \(k\in[m],2\leq l\leq K-1\). We consider the following \(l\)-th layer 2NN with \(m\) neurons defined on \(\mathbb{R}^{D}\): \[\mathbf{FFN}^{(l)}(\mathbf{x})= \sum_{k=1}^{m}\mathbf{e}_{D-M+l-1}a_{k}^{(r)}\sigma\left(\bar{\mathbf{b}} _{k}^{(r)}\,^{\top}\mathbf{x}+c_{k}^{(r)}\right).\] It is easy to verify \[\mathbf{FFN}^{(l)}(\mathbf{x})=\left(\mathbf{0}^{\top},f_{(l)}^{2\text{NN}}\left(\mathbf{ Q}^{(l)}\mathbf{x}\right),\mathbf{0}_{D-M+l-1}^{\top}\right)^{\top}\in\mathbb{R}^{D}, \quad\forall\mathbf{x}\in\mathbb{R}^{D}.\] Notice that if the width \(m\) satisfies \[\tilde{\mathcal{O}}\left(\frac{\left\|\log g_{l}\right\|_{\mathcal{B}}}{\sqrt{ m}}\right)<\frac{1}{8T_{l}},\] and the input \(\mathbf{x}_{t}^{(l-1/2)}\) satisfies \[\left\|\log g_{l}\right\|_{\text{Lip}}\cdot\left\|(\mathbf{x}_{t}^{\top},\cdots, \mathbf{x}_{t-t_{l-1}}^{\top})^{\top}-\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)}\right\|_{2} \leq\frac{1}{8T_{l}},\] the following holds: \[\left|\log g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})-f_{(l)}^{ 2\text{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)})\right|\] \[\leq \left|\log g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})-\log g_{l} (\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)})\right|\] \[\quad+\left|g_{l}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)})-f_{(l)}^{2 \text{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)})\right|\]
\[\leq \left\|\log g_{l}\right\|_{\mathrm{Lip}}\left\|(\mathbf{x}_{t}^{\top}, \cdots,\mathbf{x}_{t-t_{l-1}}^{\top})^{\top}-\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l-1/2)}\right\| _{2}+\left\|\log g_{l}-f_{(l)}^{2\mathrm{NN}}\right\|_{L^{\infty}}\] \[< \frac{1}{8T_{l}}+\frac{1}{8T_{l}}=\frac{1}{4T_{l}},\]

which ensures

\[\left|\exp\left(f_{(l)}^{2\mathrm{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l -1/2)})\right)-g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})\right|\] \[= \left|\exp\left(f_{(l)}^{2\mathrm{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l -1/2)})\right)-\exp\left(\log\left(g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}}) \right))\right|\right.\] \[\leq \exp\left(\max\left\{f_{(l)}^{2\mathrm{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{ t}^{(l-1/2)}),\log\left(g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})\right) \right\}\right)\] \[\quad\cdot\left|f_{(l)}^{2\mathrm{NN}}(\mathbf{Q}^{(l)}\mathbf{x}_{t}^{(l -1/2)})-\log\left(g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})\right)\right|\] \[\leq \exp\left(\log\left(g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}}) \right)+\frac{1}{8}\right)\frac{1}{4T_{r}}\] \[\leq e^{1/8}\cdot T_{r}\cdot\frac{1}{4T_{r}}<\frac{1}{2}.\]

Noticing \(t_{l}=g_{l}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})\in\mathbb{N}_{+}\), we have

\[\widetilde{f_{(l)}^{2\mathrm{NN}}}(\mathbf{Q}_{l}\mathbf{x}_{t}^{(l-1/2)})=\log\left( \exp\left[\mathrm{FFN}^{(l)}(\mathbf{x}_{t},\cdots,\mathbf{x}_{t-t_{l-1}})\right] \right)=\log t_{l}.\]

Thus, it holds that:

\[\widetilde{\mathbf{FFN}}^{(l)}(\mathbf{x}_{t}^{(l-1/2)})=(\mathbf{0}^{\top},\log t_{l },\mathbf{0}_{D-M+l-1}^{\top})^{\top}.\]

**- Attn layers.**

By Lemma F.5, for any rate \(n\in\mathbb{N}_{+}\), there exists a constant \(C(n)\) and \(K\) functions

\[\phi_{l}^{\mathrm{poly}}(t;B)= \sum_{h=1}\alpha_{l,h}(t/B)^{-\beta_{l,h}}\] \[= \sum_{h=1}^{H}\alpha_{l,h}\exp\Big{(}-\beta_{l,h}\log(t/B)\Big{)},\quad 1\leq l\leq K\]

such that \(\beta_{l,h}>0\) and

\[\sup_{1\leq B\leq T_{l}}\left\|\mathbb{I}\{\cdot=B\}-\phi_{l}^{\mathrm{poly}} (\cdot;B)\right\|_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)T_{l}^{1.01(n+1)}}{ H^{n}},\;1\leq l\leq K.\]

Moreover, Noticing that \(1\leq g_{l}(\cdot)\leq T_{l}\) holds for any \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\in\mathcal{X}\) and \(1\leq l\leq K\), the following holds:

\[\sup_{\mathbf{X}}\left\|\mathbb{I}\{\cdot=t_{l}\}-\phi_{l}^{\mathrm{poly}}(\cdot;t_ {l})\right\|_{\ell_{1}(\mathbb{N}_{+})}\]

Therefore, for the attention heads \(h\) (\(h\in[H]\)) in each layer \(l\) (\(1\leq l\leq K\)), we can choose:

\[p^{(l+1,h)}=\beta_{l,h},\quad\mathbf{W}_{O}^{(l+1)}=\mathbf{I},\quad\mathbf{W}_{V}^{(l+1,h )}=\alpha_{l,h}\mathbf{\delta}_{(l+1,1)}^{(d\times d)}\in\mathbb{R}^{D\times D},\]

\[\mathbf{W}_{Q}^{(l+1,h)}=\sqrt{\beta_{l,h}}\mathbf{\delta}_{(D-M+l,1)}^{(1\times 1)} \in\mathbb{R}^{D\times(D/H)},\]

\[\mathbf{W}_{K}^{(l+1,h)}=\sqrt{\beta_{l,h}}\mathbf{\delta}_{(D,1)}^{(1\times 1)}\in \mathbb{R}^{D\times(D/H)},\]

where \(\mathbf{\delta}_{(p_{1},p_{2})}^{(r\times r)}\) means that: it equals to \(\mathbf{I}_{r\times r}\) for the \((p_{1},p_{2})\)-th \(r\times r\) blocks, and \(\mathbf{0}_{r\times r}\) for the other \(r\times r\) blocks.

Similar to the estimate in Step II and Step III in the proof of Theorem C.2, it is easy to prove the following estimates by induction.

If the width satisfies

\[m\geq\begin{cases}\tilde{\Omega}\left(\max_{l\in[K]}\left\|g_{l}\right\|_{\mathcal{ B}}^{2}\right)=\tilde{\Omega}\left(\left\|g_{K}\right\|_{\mathcal{B}}^{2}\right),& \mathtt{type}=\ln\\ \tilde{\Omega}\left(\max_{l\in[K]}\left\|\log g_{l}\right\|_{\mathcal{B}}^{2}T _{l}^{2}\right)=\tilde{\Omega}\left(\left\|\log g_{K}\right\|_{\mathcal{B}}^{2 }T_{K}^{2}\right),&\mathtt{type}=\log\end{cases},\]

and the head number satisfies

\[\begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}e^{0.02(n+1)T_{l}}}\leq \frac{1}{4\max\limits_{l\in[K-1]}\left\|g_{l}\right\|_{\mathrm{Lip}}},&\mathtt{ type}=\ln\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\leq\frac{1}{4\max \limits_{l\in[K-1]}\left\|\log g_{l}\right\|_{\mathrm{Lip}}},&\mathtt{type}= \log\end{cases},\]

then the following estimates hold:

* **(E1)** for any \(2\leq l\leq K\), \[\mathbf{x}_{t}^{(l)}=\mathbf{x}_{t}^{(l-1/2)}+(\mathbf{0}^{\top},t_{l},\mathbf{0}_{M-l+1}^{\top })^{\top};\]
* **(E2)** for any \(1\leq l\leq K\), \[\mathbf{P}_{\perp}^{(l)}\mathbf{x}_{t}^{(l+1/2)}=\mathbf{P}_{\perp}^{(l)}\mathbf{x}_{t}^{(l)};\]
* **(E3)** for any \(1\leq l\leq K\), \[\left\|\mathbf{P}^{(l)}\left(\mathbf{x}_{t}^{(l+1/2)}-\mathbf{x}_{t}^{(l)}+(\mathbf{0}_{ld}^{ \top},\mathbf{x}_{t-t_{l}}^{\top},\mathbf{0}^{\top})^{\top}\right)\right\|_{2}\leq \begin{cases}\frac{C(n)e^{0.01(n+1)T_{l}}}{H^{n}},&\mathtt{type}=\ln\\ \frac{C(n)T_{l}^{1.01(n+1)}}{H^{n}},&\mathtt{type}=\log\end{cases};\] \[\left\|\mathbf{Q}^{(l)}\left(\mathbf{x}_{t}^{(l+1/2)}-\mathbf{x}_{t}^{(0)} \right)\right\|_{2}\leq\begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{j=1}^{l}e^{0.02(n+1)T_{l}}},&\mathtt{type}=\ln\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{j=1}^{l}T_{j}^{2.02(n+1)}},&\mathtt{type}=\log \end{cases}.\]

**The Remained Steps.**

* **Regime \(M\geq K+1\).** **Step \(2K+2\) and \(2K+3\).** In the similar way as **Step \(3\sim\) Step \(2K-1\)** in this proof and **Step II, Step III** in the proof of Theorem C.2, it is easy to verify the following estimate. If the width satisifes \[m\geq\begin{cases}\tilde{\Omega}\left(\sum_{l=K+1}^{M}\left\|g_{l}\right\|_{ \mathcal{B}}^{2}\right),&\mathtt{type}=\ln\\ \tilde{\Omega}\left(\sum_{l=K+1}^{M}\left\|\log g_{l}\right\|_{\mathcal{B}}^{ 2}T_{l}^{2}\right),&\mathtt{type}=\log\end{cases},\] and the head number satisfies \[\begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\leq \frac{1}{4\max\limits_{l\in[K]}\left\|g_{l}\right\|_{\mathrm{Lip}}},&\mathtt{ type}=\ln\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}T_{l}^{2.02(n+1)}}\leq\frac{1}{4\max \limits_{l\in[K]}\left\|\log g_{l}\right\|_{\mathrm{Lip}}},&\mathtt{type}

[MISSING_PAGE_EMPTY:46]

\[m\geq\begin{cases}\tilde{\Omega}\left(\max_{l\in[K]}\vee\sum_{l=K+1}^{M}\left\|g_{l} \right\|_{\mathcal{B}}^{2}\right),&\text{type}=\text{lin}\\ \tilde{\Omega}\left(\max_{l\in[K]}\vee\sum_{l=K+1}^{M}\left\|\log g_{l}\right\|_ {\mathcal{B}}^{2}T_{l}^{2}\right),&\text{type}=\log\end{cases},\]
* **Regime \(M=K\).** **Step \(2K+2\) and the final bound.** In the same way as Step IV in the proof of Theorem C.2, there exists **FFN**, such that the following estimate holds for any \(t\) and \(\mathbf{X}\): \[\left\|\mathbf{H}_{t}(\mathbf{X})-\mathbf{x}_{t}^{(K+1)}\right\| \leq \left\|f\right\|_{\text{Lip}}\cdot\left\|\mathbf{Q}^{(M)}\left(\mathbf{x }_{t}^{(K+1/2)}-\mathbf{x}_{t}^{(0)}\right)\right\|_{2}+\mathcal{E}_{\text{FFN}}\] \[= \left\|f\right\|_{\text{Lip}}\cdot\mathcal{E}_{\text{Attn}}( \text{type})+\mathcal{E}_{\text{FFN}},\] where \[\mathcal{E}_{\text{FFN}}=\mathcal{O}\left(\frac{\left\|f\right\|_{\mathcal{B}} }{\sqrt{m}}\right),\] \[\mathcal{E}_{\text{Attn}}(\text{type})=\left\|\mathbf{Q}_{M}\left(\mathbf{x }_{t}^{(K+1/2)}-\mathbf{x}_{t}^{(0)}\right)\right\|_{2}\] \[= \begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}e^{0.02(n+1)T_ {l}}},&\text{type}=\text{lin}\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K}T_{l}^{2.02(n+1)}},&\text{type}=\log\end{cases}.\] Recalling our analysis, we need the head number satisfies \[\begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}e^{0.02(n+1)T_ {l}}}\leq\frac{1}{t_{\frac{\max}{l\in[K-1]}\left\|g_{l}\right\|_{\text{Lip}}}},&\text{type}=\text{lin}\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\leq\frac{1}{4}\frac {1}{t_{\frac{\max}{l\in[K-1]}\left\|\log g_{l}\right\|_{\text{Lip}}}},&\text{ type}=\log\end{cases}.\] Due to \[\begin{cases}\frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}e^{0.02(n+1)T_ {l}}}\leq\mathcal{E}_{\text{Attn}}(\text{type}),&\text{type}=\text{lin}\\ \frac{C(n)}{H^{n}}\sqrt{\sum_{l=1}^{K-1}T_{l}^{2.02(n+1)}}\leq\mathcal{E}_{ \text{Attn}}(\text{type}),&\text{type}=\log\end{cases},\] when \(H\) is large enough, this condition holds naturally and do not affect the approximation rate. Moreover, we need the following condition on the width: \[m\geq\begin{cases}\tilde{\Omega}\left(\max_{l\in[K]}\left\|g_{l}\right\|_{ \mathcal{B}}^{2}\right),&\text{type}=\text{lin}\\ \tilde{\Omega}\left(\max_{l\in[K]}\left\|\log g_{l}\right\|_{\mathcal{B}}^{2} T_{l}^{2}\right),&\text{type}=\log\end{cases},\] Combining these two regimes, we complete our proof.

[MISSING_PAGE_FAIL:48]

for \(2\)-layer Transformer, the required number of Attn heads \(H_{\text{need}}^{(2)}\) satisfies: \[\epsilon=\begin{cases}\mathcal{O}\left(\frac{C(n)}{\left(H_{\text{need}}^{(M+1 )}\right)^{n}}\sqrt{\sum_{l=1}^{K}e^{0.02(n+1)T_{l}}}\right),&\text{type}=\lim \\ \mathcal{O}\left(\frac{C(n)}{\left(H_{\text{need}}^{(M+1)}\right)^{n}}\sqrt{ \sum_{l=1}^{K}T_{l}^{1.02(n+1)}}\right),&\text{type}=\log\end{cases}.\] * for \(M+1\)-layer Transformer, the required number of Attn heads \(H_{\text{need}}^{(M+1)}\) satisfies: \[\epsilon=\begin{cases}\mathcal{O}\left(\frac{C(n)}{\left(H_{\text{need}}^{(M +1)}\right)^{n}}\left(\sum_{i=1}^{M}e^{0.01T_{i}}\right)^{n+1}\right),&\text{ type}=\lim\\ \mathcal{O}\left(\frac{C(n)}{\left(H_{\text{need}}^{(M+1)}\right)^{n}}\left( \sum_{i=1}^{M}T_{i}^{1.01}\right)^{n+1}\right),&\text{type}=\log\end{cases}.\] It is easy to see that: \[\left(\frac{H_{\text{need}}^{(M+1)}}{H_{\text{need}}^{(2)}} \right)^{2n}=\frac{b_{1}^{2}+\cdots+b_{M}^{2}}{(b_{1}+\cdots b_{M})^{2}},\] \[b_{1}^{2}+\cdots+b_{M}^{2}\leq(b_{1}+\cdots b_{M})^{2}.\]

This finding suggests that increased depth can significantly reduce the demands on the number of heads and the width. The underlying reason is that deep networks can distribute memories across different layers for processing, with each layer focusing on approximating only a single memory function.

Proof of Section 5

### Proof of Theorem 5.1

In this subsection, we give the detailed proofs of the warm-up case of (fixed) essentially sparse memories as follows:

\[y_{t}=f\left(\left(\mathbf{X}*\rho_{1}\right)\left(t\right),\cdots,\left(\mathbf{X}*\rho _{M}\right)\left(t\right)\right),\]

where \(\rho_{1}(\cdot),\cdots,\rho_{M}(\cdot)\in\ell^{1}(\mathbb{N})\) serve as memory kernels, and \(\left(\mathbf{X}*\rho_{k}\right)(t)=\sum_{s=0}^{+\infty}\mathbf{x}_{t-s}\rho_{k}(s)\) denotes the convolution of the inputs with kernel \(\rho_{k}\).

**Theorem E.1** (Restatement of Theorem 5.1).:

_(A) Consider \(\mathcal{H}^{\text{Ess}}\) (14) with exponentially decayed memory kernels, i.e., there exists \(\beta>0\) such that \(\rho_{1}(t),\cdots,\rho_{M}(t)=\mathcal{O}(e^{-\beta t})\). Then for any target \(\mathbf{H}\in\mathcal{H}^{\text{Ess}}\), rate \(n\in[\left|99\beta\right|]\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer DP-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\text{DP},\mathsf{exp}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}-\mathbf{TF}\right| \kern-1.075pt\right|\leq\mathcal{E}_{\text{FFN}}+\left|\kern-1.075pt\left| \kern-1.075ptf\right|\right|\kern-1.075pt\right|_{\text{Lip}}\mathcal{E}_{ \text{Attn}}(\mathsf{type});\]

_(B) Consider \(\mathcal{H}^{\text{Ess}}\) (14) with polynomially decayed memory kernels, i.e., there exists \(\beta>1\) such that \(\rho_{1}(t),\cdots,\rho_{M}(t)=\mathcal{O}(t^{-\beta})\). Then for any target \(\mathbf{H}\in\mathcal{H}^{\text{Ess}}\), rate \(n\in[\left|0.99\beta\right|-1]\), and \(H,m\in\mathbb{N}_{+}\), there exists a \(1\)-layer DP-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\text{DP},\mathsf{poly}}_{(1,H,m)}\) (7) and a constant \(C(n)\) such that_

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{H}-\mathbf{TF}\right| \kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\leq \mathcal{E}_{\text{FFN}}+\left|\kern-1.075pt\left|\kern-1.075ptf\right|\kern-1.0 75pt\right|_{\text{Lip}}\mathcal{E}_{\text{Attn}}(\mathsf{type});\]

_where \(\mathcal{E}_{\text{FFN}}=\tilde{\mathcal{O}}\left(\frac{\left\|\kern-1.075pt \left|\kern-1.075ptf\right|\kern-1.075pt\right|\kern-1.075pt\right|_{\text{ BF}}}{\sqrt{m}}\right)\) and_

\[\mathcal{E}_{\text{Attn}}(\mathsf{type})=\mathcal{O}\left(\frac{C(n)M^{n+1}}{H ^{n}}\right).\]

Proof of Theorem E.1.: The proof of this theorem is highly similar to the proof of Theorem B.1. The only difference is that the Attn layer needs to be used to approximate general memory kernel \(\rho_{k}(\cdot)\) instead of simple \(\mathbb{I}\{\cdot=T_{k}\}\). But for the completeness of the proof in this section, we still provide the detailed proof.

First, we choose the embedding dimension \(D=Md\), and select the simple embedding \(\mathbf{W}_{E}=(\mathbf{I}_{d\times d},\mathbf{0})^{\top}\in\mathbb{R}^{D\times d},\mathbf{b} _{E}=\mathbf{0}\in\mathbb{R}^{D}\).

For any input sequence \(\mathbf{X}=(\mathbf{x}_{t})_{t\in\mathbb{Z}}\), the token after embedding satisfies:

\[\mathbf{x}_{t}^{E}=\mathbf{W}_{E}\mathbf{x}_{t}+\mathbf{b}_{E}=(\mathbf{x}_{t}^{\top},\mathbf{0}^{\top })^{\top}\in\mathbb{R}^{D}.\]

Then for one-layer Dot-product-free Transformer \(\mathbf{TF}\in\mathcal{T}\mathcal{F}^{\text{DPF},\mathsf{type}}_{(1,H,m)}\) without residual blocks, the output token \(\mathbf{TF}_{t}(\mathbf{X})\) of \(t\)-th input token \(\mathbf{x}_{t}\) satisfies:

\[\mathbf{x}_{t}^{(1/2)} =\mathbf{W}_{O}^{(1)}\sum_{h=1}^{H}\mathbf{Attn}_{t}^{(1,h)}(\mathbf{X}^{( 0)}),\] \[\mathbf{x}_{t}^{(1)} =\mathbf{FFN}^{(1)}(\mathbf{x}_{t}^{(1/2)})\]

where

\[\mathbf{Attn}_{t}^{(1,h)}(\mathbf{X})=\mathbf{W}_{V}^{(1,h)}\sum_{s=0}^{+\infty}\frac{ \mathbf{x}_{t-s}\exp\left(p^{(1,h)}\phi_{\mathsf{type}}(s)\right)}{\sum_{j=0}^{+ \infty}\exp\left(p^{(1,h)}\phi_{\mathsf{type}}(j)\right)}.\]

This proof can be summarized as the following process:

\[\cdots \mathbf{x}_{t}^{E} \cdots\]Step I. Attn layer \(\downarrow\)

\[\begin{array}{ccc}\cdots&\boldsymbol{x}_{t}^{(1/2)}\approx\left((\boldsymbol{X} \ast\rho_{1})(t),\cdots,(\boldsymbol{X}\ast\rho_{M})(t)\right)^{\top}&\cdots\\ \mbox{Step II. FFN layer }\downarrow\\ \cdots&\boldsymbol{x}_{t}^{(1)}\approx\boldsymbol{f}\left((\boldsymbol{X}\ast \rho_{1})(t),\cdots,(\boldsymbol{X}\ast\rho_{M})(t)\right)&\cdots\end{array}\]

Now we give the formal proof.

**Step I.** Extract the memory locations by (Dot-product-free) Attn layer.

We consider to use \(H_{k}\) attention heads (from \(\sum_{i=1}^{k-1}H_{i}+1\)-th head to \(\sum_{i=1}^{k}H_{i}\)-th head) to extract it, and it satisfies to \(\sum_{k=1}^{M}H_{k}=H\).

\[\begin{array}{ccc}\boldsymbol{P}^{(k)}:=\left(\boldsymbol{0}_{d\times(k-1)d} \quad\boldsymbol{I}_{d\times d}\quad\boldsymbol{0}\right)\in\mathbb{R}^{d \times D},&1\leq k\leq M.\\ \boldsymbol{P}_{\perp}^{(k)}:=\begin{pmatrix}\boldsymbol{I}_{(k-1)d\times(k-1 )d}&\boldsymbol{0}_{d\times d}&\boldsymbol{0}\\ \boldsymbol{0}&\boldsymbol{0}_{d\times d}&\boldsymbol{I}_{(M-k-1)d\times(M-k- 1)d}\end{pmatrix}\in\mathbb{R}^{(M-1)d\times D},&1\leq k\leq M.\end{array}\]

Now we consider the extraction of \(k\)-th memory \((\boldsymbol{X}\ast\rho_{k})(t)\) (\(1\leq k\leq M\)).

* **Case (A). Approximating exponentially decayed memories by \(\mbox{type}=\lim\).** Because there exists \(\beta>0\) such that \(\rho_{k}(t)=\mathcal{O}(e^{-\beta t})\), by Lemma F.3, for any \(n\in[\lfloor 99\beta\rfloor]\) and \(m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a function \[\phi_{k}^{\exp}(t)=\sum_{\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}} \alpha_{h}e^{-\beta_{h}t}\] such that \(\beta_{h}>0\) and \[\left\|\rho_{k}(\cdot)-\phi_{k}^{\exp}(\cdot)\right\|_{\ell_{1}(\mathbb{N})}= \sum_{s=0}^{+\infty}\left|\rho_{k}(s)-\phi_{k}^{\exp}(s)\right|\leq\frac{C(n) }{m^{n}}.\] Therefore, for these attention heads (\(\sum_{i=1}^{k-1}H_{i}+1\leq h\leq\sum_{i=1}^{k}H_{i}\)), we can choose \[p^{(1,h)}=\beta_{h},\quad\boldsymbol{W}_{V}^{(1,h)}=\alpha_{h}\left(\sum_{j=0} ^{+\infty}\exp(-\beta_{h}j)\right)\boldsymbol{\delta}_{(k,1)}^{d\times d},\] where \(\boldsymbol{\delta}^{(k,1)}\in\mathbb{R}^{D\times D}\) means that: it equals to \(\boldsymbol{I}_{d\times d}\) for the \((k,1)\)-th \(d\times d\) blocks, and \(\boldsymbol{0}_{d\times d}\) for the other \(d\times d\) blocks. Then it holds that: \[\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}}\mathbf{Attn}_{t}^{(1,h )}(\boldsymbol{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i }}\alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}s}\begin{pmatrix}\boldsymbol{0}_ {(k-1)d}\\ \boldsymbol{x}_{t-s}\\ \boldsymbol{0}\end{pmatrix}\in\mathbb{R}^{D},\] This implies: \[\boldsymbol{P}^{(k)}\sum_{h=\sum_{i=1}^{k-1}H_{i}+1}^{\sum_{i=1}^{k}H_{i}} \mathbf{Attn}_{t}^{(1,h)}(\boldsymbol{X}^{(0)})=\sum_{h=\sum_{i=1}^{k-1}H_{i} +1}^{\sum_{i=1}^{k}H_{i}}\alpha_{h}\sum_{s=0}^{+\infty}e^{-\beta_{h}s} \boldsymbol{x}_{t-s},\]

[MISSING_PAGE_FAIL:52]

[MISSING_PAGE_EMPTY:53]

Furthermore, by choosing \(\mathcal{E}_{\mathrm{Attn}}\leq 1\), it holds that

\[\left\|\mathbf{x}_{t}^{(1/2)}\right\|_{\infty}\leq\left\|\mathbf{x}_{t}^{(1/2)}-\begin{pmatrix} (\mathbf{X}*\rho_{1})(t)\\ \vdots\\ (\mathbf{X}*\rho_{M})(t)\end{pmatrix}\right\|_{\infty}+\left\|\begin{pmatrix}(\bm {X}*\rho_{1})(t)\\ \vdots\\ (\mathbf{X}*\rho_{M})(t)\end{pmatrix}\right\|_{\infty}\leq\mathcal{E}_{\mathrm{ Attn}}+1\leq 2.\]

**Step II.** Approximate the readout function by FFN layer.

In this step, we aim to approximate the function \(f\) using two-layer network. By Lemma G.6, there exists a two-layer neural network with \(m\) neurons defined on \(\mathbb{R}^{\mathcal{D}}\)

\[\mathrm{FFN}^{(1)}(\mathbf{y})=\sum_{k=1}^{m}a_{k}\sigma(\mathbf{b}_{k}^{\top}\mathbf{y}+c _{k})\]

such that

\[\mathcal{E}_{\mathrm{FFN}}:=\left\|\mathrm{FFN}^{(1)}-f\right\|_{L^{\infty}([ -2,2]^{D})}\leq\tilde{\mathcal{O}}\left(\frac{\left\|f\right\|_{\mathcal{B}}}{ \sqrt{m}}\right).\]

**The final bound.**

For any \(t\) and \(\mathbf{X}\in\mathcal{X}\), it holds that

\[\left\|\mathbf{H}_{t}(\mathbf{X})-\mathbf{x}_{t}^{(1)}\right\|=\left|f(( \mathbf{X}*\rho_{1})(t),\cdots,(\mathbf{X}*\rho_{M})(t))-\mathrm{FFN}^{(1)}\left(\mathbf{x} _{t}^{(1/2)}\right)\right|\] \[= \left|f((\mathbf{X}*\rho_{1})(t),\cdots,(\mathbf{X}*\rho_{M})(t))-f\left( \mathbf{x}_{t}^{(1/2)}\right)+f\left(\mathbf{x}_{t}^{(1/2)}\right)-\mathrm{FFN}^{(1)} \left(\mathbf{x}_{t}^{(1/2)}\right)\right|\] \[\leq \left|f((\mathbf{X}*\rho_{1})(t),\cdots,(\mathbf{X}*\rho_{M})(t))-f\left( \mathbf{x}_{t}^{(1/2)}\right)\right|+\left|f\left(\mathbf{x}_{t}^{(1/2)}\right)- \mathrm{FFN}^{(1)}\left(\mathbf{x}_{t}^{(1/2)}\right)\right|\] \[\leq \left\|f\right\|_{\mathrm{Lip}}\left\|((\mathbf{X}*\rho_{1})(t)^{\top },\cdots,(\mathbf{X}*\rho_{M})(t)^{\top})^{\top}-\mathbf{x}_{t}^{(1/2)}\right\|_{2}+ \left\|f-\mathrm{FFN}^{(1)}\right\|_{L^{\infty}([-2,2]^{D})}\] \[\leq \left\|f\right\|_{\mathrm{Lip}}\cdot\mathcal{E}_{\mathrm{Attn}}+ \mathcal{E}_{\mathrm{FFN}},\]

where

\[\mathcal{E}_{\mathrm{FFN}}=\frac{\left\|f\right\|_{\mathcal{B}}}{\sqrt{m}}; \quad\mathcal{E}_{\mathrm{Attn}}=\frac{C(n)M^{n+1}}{H^{n}},\quad\text{ for both \bf Case (A) and \bf Case (B)}.\]

Due to the arbitrariness of \(t\) and \(\mathbf{X}\), the proof is completed.

[MISSING_PAGE_EMPTY:55]

It is easy to verify that \(\Phi_{T}\) satisfies that

\[\Phi_{T}(t)\big{|}_{\mathbb{N}}=\mathbb{I}(t=T).\]

Moreover, we consider the function

\[P_{m}(t):=e^{-\gamma t}Q_{m}(e^{-\alpha t}),\quad t\in[0,+\infty).\]

Then by choosing \(\alpha=\gamma=0.01\), the following error estimate holds:

\[\left\|P_{m}(\cdot)-\mathbb{I}(\cdot=T)\right\|_{\ell_{1}(\mathbb{ N})}=\sum_{t=0}^{+\infty}|P_{m}(t)-\Phi_{T}(t)|\] \[= \sum_{t=0}^{+\infty}e^{-\gamma t}|Q_{m}(e^{-\alpha t})-\Psi_{T}(e ^{-\alpha t})|\leq\sum_{t=0}^{+\infty}e^{-\gamma t}\frac{M_{T}(n)}{m^{n}}\] \[\leq \frac{C(n,\alpha)e^{(\gamma+n\alpha)T}}{m^{n}}\sum_{t=0}^{+\infty }e^{-\gamma t}\leq\frac{C(n)e^{0.01(n+1)T}}{m^{n}}\frac{1}{1-e^{-\gamma}}\] \[= \frac{\tilde{C}(n)e^{0.01(n+1)T}}{m^{n}}.\]

Finally, notice that \(P_{m}(t)=e^{-\gamma t}Q_{m}\left(e^{-\alpha t}\right)=\sum\limits_{k=0}^{m-1} \alpha_{k}e^{-(0.01+0.01k)}\), so we can select \(\phi_{m}^{\text{exp}}(t):=P_{m}(t)\).

**Lemma F.2** (Exp decay, adaptive Delta function).: _For any \(T\in\mathbb{N}\), \(n,m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a \(\phi_{m}^{\text{exp}}(t;B)=\sum\limits_{k=1}^{m}\alpha_{k}e^{-\beta_{k}(t-B)}\) such that_

\[\max_{1\leq B\leq T}\left\|\mathbb{I}(\cdot=B)-\phi_{m}^{\text{exp}}(\cdot;B) \right\|_{\ell_{1}(\mathbb{N})}\leq\frac{C(n)e^{0.01(n+1)T}}{m^{n}}.\]

_where \(\beta_{k}>0\) holds for any \(k\in[m]\)._

Proof of Lemma f.2.: The **key point** of the proof is to note that the adaptability of \(B\) can be eliminated by the **translation operator**\(t-B\).

First, recall our proof of Lemma F.1. For the same \(\Psi_{T}(\cdot)\), for any \(n,m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a polynomial \(Q_{m}(x)=\sum\limits_{k=0}^{m-1}\alpha_{k}x^{k}\) such that

\[\sup_{x\in[0,1]}\left|\Psi_{T}(x)-Q_{m}(x)\right|\leq\frac{C(n)e^{0.01(n+1)T} }{m^{n}}.\]

Moreover, using the transform \(x=e^{-0.01(t-B+T)}\)\((t\geq 0)\) on the function \(\Psi\) and consider

\[\Phi_{T}(t;B):=e^{-0.01(t-B+T)}\Psi_{T}\left(e^{-0.01(t-B+T)}\right),\quad t\in [0,+\infty).\]

It is easy to verify that \(\Phi_{T}(\cdot;\cdot)\) satisfies that

\[\Phi_{T}(t;B)\big{|}_{\mathbb{N}}=\mathbb{I}(t=B).\]

And we consider the function

\[P_{m}(t;B):=e^{-0.01(t-B+T)}Q_{m}\left(e^{-0.01(t-B+T)}\right),\quad t\in[0,+ \infty).\]Then, for any \(1\leq B\leq T\), the following error estimate holds:

\[\left\|P_{m}(\cdot;B)-\mathbb{I}(\cdot=B)\right\|_{\ell_{1}(\mathbb{N })}=\sum\limits_{t=0}^{+\infty}|P_{m}(t;B)-\Phi_{T}(t;B)|\] \[= \sum\limits_{t=0}^{+\infty}e^{-0.01(t-B+T)}\left|Q_{m}\left(e^{-0.01(t-B+T)}\right)-\Psi_{T}\left(e^{-0.01(t-B+T)}\right)\right|\] \[\leq \sum\limits_{t=0}^{+\infty}e^{-0.01t}\sup\limits_{x\in[0,1]}|Q_{m }(x)-\Psi_{T}(x)|\] \[\leq \frac{C(n)e^{0.01(n+1)T}}{m^{n}}\sum\limits_{t=0}^{+\infty}e^{-0. 01t}=\frac{\tilde{C}(n)e^{0.01(n+1)T}}{m^{n}}.\]

Due to the arbitrariness of \(B\), the proof is completed.

**Lemma F.3** (Exp decay, fixed Delta function).: _Consider a exponentially decayed memory \(\rho(\cdot)\): there exists \(\beta>0\) such that \(\rho(t)=\mathcal{O}(e^{-\beta t})\). Then for any \(n\in[\lfloor 99\beta\rfloor]\) and \(m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a \(\phi_{m}^{\mathrm{exp}}(t)=\sum\limits_{k=1}^{m}\alpha_{k}e^{-\beta_{k}t}\) such that_

\[\left\|\rho(\cdot)-\phi_{m}^{\mathrm{exp}}(\cdot)\right\|_{\ell_{1}(\mathbb{N })}\leq\frac{C(n)}{m^{n}},\]

_where \(\beta_{k}>0\) holds for any \(k\in[m]\)._

Proof of Lemma F.3.:

There exists \(C>0\) such that \(|\rho(t)|\leq Ce^{-\beta t}\).

Let \(\alpha,\gamma>0\) be constants, and they will take specific values at the end of the proof.

First, recall the standard bump function on \([-1,1]\):

\[\Psi(x):=\begin{cases}\exp\left(-\frac{1}{1-x^{2}}\right),\;x\in(-1,1)\\ 0,\text{ otherwise}\end{cases},\]

and we can define the following constants for \(T\geq 1\):

\[\mu_{T}=e^{-\alpha T},\quad\sigma_{T}=\frac{1}{2}\left(e^{-\alpha T}-e^{- \alpha(T+1)}\right),\]

and we consider the following bump function \(\Psi_{T}\in\mathcal{C}^{\infty}([0,1])\):

\[\Psi_{T}(x)=\begin{cases}V_{T}\Psi\left(\frac{x-\mu_{T}}{\sigma_{T}}\right), \;x\in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T})\\ 0,\text{ otherwise}\end{cases},\]

where \(V_{T}\) is a scaling constant such that \(\Psi_{T}(e^{-\alpha T})=e^{\gamma T}\rho(T)\).

Consequently, we consider the sum of bump functions on \([0,1]\):

\[\varphi(x):=\sum\limits_{T=1}^{+\infty}\Psi_{T}(x).\]

It is easy to verify that \((\mu_{T_{1}}-\sigma_{T_{1}},\mu_{T_{1}}+\sigma_{T_{1}})\cap(\mu_{T_{2}}-\sigma _{T_{2}},\mu_{T_{2}}+\sigma_{T_{2}})=\varnothing\) for any \(T_{1}\neq T_{2}\) and

\[\varphi(x)=\begin{cases}\Psi_{T}(x),\;\mu_{T}-\sigma_{T}\leq x\leq\mu_{T}+ \sigma_{T}\\ 0,\text{ otherwise}\end{cases}.\]

[MISSING_PAGE_FAIL:58]

Then for any \(n<\frac{\beta-\gamma}{\alpha}\) (\(n\in\mathbb{N}\)), the following error estimate holds:

\[\left\|P_{m}(\cdot)-\rho(\cdot)\right\|_{\ell_{1}(\mathbb{N})}=\sum \limits_{t=0}^{+\infty}\left|P_{m}(t)-\Phi(t)\right|\] \[= \sum\limits_{t=0}^{+\infty}e^{-\gamma t}\left|Q_{m}\left(e^{- \alpha t}\right)-\Psi_{T}\left(e^{-\alpha t}\right)\right|\leq\frac{C(n,\alpha )}{m^{n}}\sum\limits_{t=0}^{+\infty}e^{-\gamma t}.\]

By choosing \(\alpha=5\cdot 10^{-3}\) and \(\gamma=10^{-2}\beta\), it holds that \(99\beta<\frac{\beta-\gamma}{2\alpha}=\frac{\beta-\gamma}{\alpha}\).

Thus, we obtain our result: for any \(n\in[\lfloor 99\beta\rfloor]\) (\(\beta\geq 1/99\)), the following error estimate holds:

\[\left\|P_{m}(\cdot)-\rho(\cdot)\right\|_{\ell_{1}(\mathbb{N})}\leq\frac{C(n)}{ m^{n}}\sum\limits_{t=0}^{+\infty}e^{-\gamma t}=\frac{C(n)}{m^{n}}\frac{1}{1-e^{ -10^{-2}\beta}}=\frac{\tilde{C}(n)}{m^{n}}.\]

### Approximation by the sum of polynomial decay

**Lemma F.4** (Poly decay, fixed Delta function).: _For any \(T,n,m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a \(\phi_{m}^{\mathrm{poly}}(t)=\sum\limits_{k=1}^{m}\alpha_{k}t^{-\beta_{k}}\) such that_

\[\left\|\mathbb{I}(\cdot=T)-\phi_{m}^{\mathrm{poly}}(\cdot)\right\|_{\ell_{1}( \mathbb{N}_{+})}\leq\frac{C(n)T^{1.01(n+1)}}{m^{n}},\]

_where \(\beta_{k}>1\) holds for any \(k\in[m]\)._

Proof of Lemma F.4.:

Let \(\alpha,\gamma>0\) be constants, and they will take specific values at the end of the proof

First, recall the standard bump function on \([-1,1]\):

\[\Psi(x):=\begin{cases}\exp\left(-\frac{1}{1-x^{2}}\right),\ x\in(-1,1)\\ 0,\ \text{otherwise}\end{cases},\]

and we can define the following constants for \(T\geq 1\):

\[\mu_{T}=\frac{1}{T^{\alpha}},\quad\sigma_{T}=\frac{1}{T^{\alpha}}-\frac{1}{(T+ 1)^{\alpha}}.\]

Then we consider the following bump function \(\Psi_{T}\in\mathcal{C}^{\infty}([0,1])\):

\[\Psi_{T}(x)=\begin{cases}V_{T}\Psi\left(\frac{x-\mu_{T}}{\sigma_{T}}\right), \ x\in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T})\\ 0,\ \text{otherwise}\end{cases},\]

where \(V_{T}\) is a scaling constant such that \(\Psi_{T}(\frac{1}{T^{\alpha}})=T^{1+\gamma}\).

First, we consider the approximation of \(\Psi_{T}\) on \([0,1]\).

Notice that \(\Psi_{T}\in\mathcal{C}^{\infty}([0,1])\), and \(\Psi_{T}^{(k)}(0)=0\) for any \(k\in\mathbb{N}\). For the standard bump function \(\Psi\), for any \(n\in\mathbb{N}_{+}\), there exists an absolute constant \(M(n)>0\) only depending on \(n\), such that \(\max\limits_{0\leq k\leq n}\sup\limits_{x\in[-1,1]}\left|\Psi^{(k)}(x)\right| \leq M(n)\).

Notice that for any \(k\in\mathbb{N}\) and \(x\in[0,1]\),

\[\Psi_{T}^{(k)}(x)=\frac{V_{T}}{\sigma_{T}^{k}}\Psi^{(k)}\left(\frac{x-\mu_{T}} {\sigma_{T}}\right).\]Therefore, the following upper bound holds:

\[M_{T}(n)=\max_{0\leq k\leq n}\frac{V_{T}}{\sigma_{T}^{k}}M(n)=\frac{ V_{T}}{\sigma_{T}^{n}}M(n)=\frac{T^{1+\gamma}e}{\left(1/T^{\alpha}-1/(T+1)^{ \alpha}\right)^{n}}M(n)\] \[\leq \frac{T^{1+\gamma}(T+1)^{n(1+\alpha)}M(n)e}{\alpha^{n}}\leq\frac{ 2^{n}M(n)e}{\alpha^{n}}T^{1+\gamma+n(1+\alpha)}:=C(n,\alpha)T^{1+\gamma+n(1+ \alpha)}.\]

By Lemma G.5, for any \(m\in\mathbb{N}_{+}\), there exists a polynomial \(Q_{m}(x)=\sum\limits_{k=0}^{m-1}\alpha_{k}x^{k}\) such that

\[\sup_{x\in[0,1]}\left|\Psi_{T}(x)-Q_{m}(x)\right|\leq\frac{M_{T}(n)}{m^{n}}\leq \frac{C(n,\alpha)T^{1+\gamma+n(1+\alpha)}}{m^{n}}.\]

Now we use the transform \(x=\frac{1}{t^{\alpha}}\)\((t\geq 1)\) on the function \(\Psi\) and consider

\[\Phi_{T}(t):=\frac{1}{t^{1+\gamma}}\Psi_{T}\left(\frac{1}{t^{\alpha}}\right), \quad t\in[1,+\infty).\]

It is easy to verify that \(\Phi_{T}\) satisfies that

\[\left.\Phi_{T}(t)\right|_{\mathbb{N}_{+}}=\mathbb{I}(t=T).\]

Moreover, we consider the function

\[P_{m}(t):=\frac{1}{t^{1+\gamma}}Q_{m}\left(\frac{1}{t^{\alpha}}\right),\quad t \in[1,+\infty).\]

Then by choosing \(\alpha=\gamma=0.01\), the following error estimate holds:

\[\left\|P_{m}(\cdot)-\mathbb{I}(\cdot=T)\right\|_{\ell_{1}(\mathbb{ N}_{+})}=\sum\limits_{t=1}^{+\infty}|P_{m}(t)-\Phi_{T}(t)|\] \[= \sum\limits_{t=1}^{+\infty}\frac{1}{t^{1+\gamma}}\left|Q_{m} \left(\frac{1}{t^{\alpha}}\right)-\Psi_{T}\left(\frac{1}{t^{\alpha}}\right) \right|\leq\sum\limits_{t=1}^{+\infty}\frac{1}{t^{1+\gamma}}\frac{M_{T}(n)}{m ^{n}}\] \[\leq \frac{C(n,\alpha)T^{1+\gamma+n(1+\alpha)}}{m^{n}}\sum\limits_{t=1 }^{+\infty}\frac{1}{t^{1+\gamma}}=\frac{C(n)T^{1.01(n+1)}}{m^{n}}\sum\limits _{t=1}^{+\infty}\frac{1}{t^{1+0.01}}\] \[= \frac{\tilde{C}(n)T^{1.01(n+1)}}{m^{n}}.\]

Finally, notice that \(P_{m}(\cdot)\) satisfies to \(P_{m}(t)=\frac{1}{t^{1+\gamma}}Q_{m}\left(\frac{1}{t^{\alpha}}\right)=\sum \limits_{k=0}^{m-1}\alpha_{k}t^{-(1.01+0.01k)}\), so we can select \(\phi_{m}^{\mathrm{poly}}(t):=P_{m}(t)\).

**Lemma F.5** (Poly decay, adaptive Delta function).: _For any \(T,n,m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a \(\phi_{m}^{\mathrm{poly}}(t;B)=\sum\limits_{k=1}^{m}\alpha_{k}(t/B)^{-\beta_{k}}\) such that_

\[\max_{1\leq B\leq T}\left\|\mathbb{I}(\cdot=B)-\phi_{m}^{\mathrm{poly}}(\cdot;B )\right\|_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)T^{1.01(n+1)}}{m^{n}},\]

_where \(\beta_{k}>1\) holds for any \(k\in[m]\)._

Proof of Lemma F.5.: The **key point** of the proof is to note that the adaptability of \(B\) can be eliminated by the **rescaling operator**\(t/B\).

First, recall our proof of Lemma F.4. For the same \(\Psi_{T}(\cdot)\), for any \(n,m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a polynomial \(Q_{m}(x)=\sum\limits_{k=0}^{m-1}\alpha_{k}x^{k}\) such that

\[\sup\limits_{x\in[0,1]}|\Psi_{T}(x)-Q_{m}(x)|\leq\frac{C(n)T^{1.01(n+1)}}{m^{n }}.\]

We use the transform \(x=\frac{1}{t^{0.01}}\ (t\geq 1)\) on the function \(\Psi\) and consider

\[\Phi_{T}(t;B):=\left(\frac{B}{tT}\right)^{1.01}\Psi_{T}\left(\left(\frac{B}{tT }\right)^{0.01}\right),\quad t\in[1,+\infty).\]

It is easy to verify that \(\Phi_{T}(\cdot;\cdot)\) satisfies that

\[\Phi_{T}(t;B)\big{|}_{\mathbb{N}_{+}}=\mathbb{I}(t=B).\]

And we consider the function

\[P_{m}(t;B):=\left(\frac{B}{tT}\right)^{1.01}Q_{m}\left(\left(\frac{B}{tT} \right)^{0.01}\right),\quad t\in[1,+\infty).\]

Then, for any \(1\leq B\leq T\), the following error estimate holds:

\[\|P_{m}(\cdot;B)-\mathbb{I}(\cdot=B)\|_{\ell_{1}(\mathbb{N}_{+}) }=\sum\limits_{t=1}^{+\infty}|P_{m}(t;B)-\Phi_{T}(t;B)|\] \[= \sum\limits_{t=1}^{+\infty}\left(\frac{B}{tT}\right)^{1.01}\left| Q_{m}\left(\left(\frac{B}{tT}\right)^{0.01}\right)-\Psi_{T}\left(\left(\frac{B}{ tT}\right)^{0.01}\right)\right|\] \[\leq \sum\limits_{t=1}^{+\infty}\frac{1}{t^{1.01}}\sup\limits_{x\in[0,1]}|Q_{m}(x)-\Psi_{T}(x)|\] \[\leq \frac{C(n)T^{1.01(n+1)}}{m^{n}}\sum\limits_{t=1}^{+\infty}\frac{ 1}{t^{1.01}}=\frac{\tilde{C}(n)T^{1.01(n+1)}}{m^{n}}.\]

Due to the arbitrariness of \(B\), the proof is completed.

**Lemma F.6** (Poly decay, fixed Delta function).: _Consider a polynomially decayed memory \(\rho(\cdot)\): there exists \(\beta>1\) such that \(\rho(t)=\mathcal{O}(t^{-\beta})\). Then for any \(n\in[\lfloor 0.99\beta\rfloor-1]\) and \(m\in\mathbb{N}_{+}\), there exists an absolute constant \(C(n)\) only depending on \(n\) and a \(\phi_{m}^{\mathrm{poly}}(t)=\sum\limits_{k=1}^{m}\alpha_{k}t^{-\beta_{k}}\) such that_

\[\left\|\rho(\cdot)-\phi_{m}^{\mathrm{poly}}(\cdot)\right\|_{\ell_{1}(\mathbb{N }_{+})}\leq\frac{C(n)}{m^{n}},\]

_where \(\beta_{k}>1\) holds for any \(k\in[m]\)._

Proof of Lemma F.6.:

There exists \(C>0\) such that \(|\rho(t)|\leq C/t^{\beta}\).

Let \(\alpha,\gamma>0\) be constants, and they will take specific values at the end of the proof

First, recall the standard bump function on \([-1,1]\):

\[\Psi(x):=\begin{cases}\exp\left(-\frac{1}{1-x^{2}}\right),\ x\in(-1,1)\\ 0,\ \text{otherwise}\end{cases},\]and we can define the following constants for \(T\geq 1\):

\[\mu_{T}=\frac{1}{T^{\alpha}},\quad\sigma_{T}=\frac{1}{2}\left(\frac{1}{T^{\alpha} }-\frac{1}{(T+1)^{\alpha}}\right),\]

and we consider the following bump function \(\Psi_{T}\in\mathcal{C}^{\infty}([0,1])\):

\[\Psi_{T}(x)=\begin{cases}V_{T}\Psi\left(\frac{x-\mu_{T}}{\sigma_{T}}\right),\;x \in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T})\\ 0,\;\text{otherwise}\end{cases},\]

where \(V_{T}\) is a scaling constant such that \(\Psi_{T}(\frac{1}{T^{\alpha}})=T^{1+\gamma}\rho(T)\).

Consequently, we consider the sum of bump functions on \([0,1]\):

\[\varphi(x):=\sum_{T=1}^{+\infty}\Psi_{T}(x).\]

It is easy to verify that \((\mu_{T_{1}}-\sigma_{T_{1}},\mu_{T_{1}}+\sigma_{T_{1}})\cap(\mu_{T_{2}}-\sigma _{T_{2}},\mu_{T_{2}}+\sigma_{T_{2}})=\varnothing\) for any \(T_{1}\neq T_{2}\) and

\[\varphi(x)=\begin{cases}\Psi_{T}(x),\;\mu_{T}-\sigma_{T}\leq x\leq\mu_{T}+ \sigma_{T}\\ 0,\;\text{otherwise}\end{cases}.\]

First, we study the property of \(\varphi(\cdot)\).

We denote the absolute constants \(M_{k}=\sup_{x}|\varphi^{(k)}(x)|\). Notice that for any \(k\in\mathbb{N}\),

\[\Psi_{T}^{(k)}(x)=\frac{V_{T}}{\sigma_{T}^{k}}\Psi^{(k)}\left(\frac{x-\mu_{T}} {\sigma_{T}}\right).\]

Therefore, it holds that

\[\sup_{x\in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T})}|\varphi^{(k)}( x)|=\sup_{x\in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T})}|\Psi_{T}^{(k)}(x)|\] \[\leq \frac{V_{T}}{\sigma_{T}^{k}}M_{k}=\frac{T^{1+\gamma}\rho(T)}{ \left(\frac{1}{T^{\alpha}}-\frac{1}{(T+1)^{\alpha}}\right)^{k}}2^{k}M_{k}e\] \[\leq \frac{(T+1)^{k(1+\alpha)}T^{1+\gamma-\beta}C2^{k}M_{k}e}{\alpha^ {k}}\leq\frac{2^{k(2+\alpha)}CM_{k}e}{\alpha^{k}}T^{1+\gamma+k(1+\alpha)-\beta}.\]

Therefore, if \(k\leq\frac{\beta-(1+\gamma)}{1+\alpha}\), the following uniform bounds hold:

\[\sup_{x\in(0,1]}|\varphi^{(k)}(x)|=\sup_{T\geq 1}\sup_{x\in(\mu_{T} -\sigma_{T},\mu_{T}+\sigma_{T})}|\varphi^{(k)}(x)|\] \[\leq \sup_{T\geq 1}\frac{2^{k(2+\alpha)}CM_{k}e}{\alpha^{k}}T^{1+ \gamma+k(1+\alpha)-\beta}\leq\frac{2^{k(2+\alpha)}CM_{k}e}{\alpha^{k}}:=C(k, \alpha).\]

Consequently, we consider the smoothness of \(\Phi\) at \(x=0\).

Recalling the previous results, for any \(x\in(0,1]\), we have

\[\frac{|\varphi^{(k)}(x)|}{x} \leq C(k,\alpha)\frac{T^{1+\gamma+k(1+\alpha)-\beta}}{\mu_{T}- \sigma_{T}}\leq\frac{C(k,\alpha)2^{2+\alpha}}{\alpha}T^{1+\gamma+(k+1)(1+ \alpha)-\beta},\;x\in(\mu_{T}-\sigma_{T},\mu_{T}+\sigma_{T});\] \[\frac{|\varphi^{(k)}(x)|}{x} =0,\;\text{otherwise}\]

Thus, by induction, it is easy to verify that for any \(i<\frac{\beta-(1+\gamma)}{1+\alpha}\) (\(i\in\mathbb{N}\)),

\[\varphi^{(i)}(0)=0.\]Therefore, for any \(n<\frac{\beta-(1+\gamma)}{1+\alpha}\) (\(n\in\mathbb{N}_{+}\)), \(\varphi^{(k)}(0)=0\) holds for any \(0\leq k\leq n\). Moreover, the following uniform bound holds:

\[\max_{0\leq k\leq n}\sup_{x\in[0,1]}|\varphi^{(k)}(x)|\leq C(n,\alpha).\]

By Lemma G.5, for any \(m\in\mathbb{N}_{+}\), there exists a polynomial \(Q_{m}(x)=\sum\limits_{k=0}^{m-1}\alpha_{k}x^{k}\) such that

\[\sup_{x\in[0,1]}|\varphi(x)-Q_{m}(x)|\leq\frac{C(n,\alpha)}{m^{n}}.\]

Now we use the transform \(x=\frac{1}{t^{\alpha}}\)\((t\geq 1)\) on the function \(\varphi\) and consider

\[\Phi(t):=\frac{1}{t^{1+\gamma}}\varphi\left(\frac{1}{t^{\alpha}}\right),\quad t \in[1,+\infty).\]

It is easy to verify that \(\Phi\) satisfies that

\[\left.\Phi(t)\right|_{\mathbb{N}_{+}}=\rho(t)\right|_{\mathbb{N}_{+}}.\]

Moreover, we consider the function

\[P_{m}(t):=\frac{1}{t^{1+\gamma}}Q_{m}\left(\frac{1}{t^{\alpha}}\right),\quad t \in[1,+\infty).\]

Then for any \(n<\frac{\beta-(1+\gamma)}{1+\alpha}\) (\(n\in\mathbb{N}\)), the following error estimate holds:

\[\|P_{m}(\cdot)-\rho(\cdot)\|_{\ell_{1}(\mathbb{N}_{+})}=\sum_{t=1} ^{+\infty}|P_{m}(t)-\Phi(t)|\] \[= \sum_{t=1}^{+\infty}\frac{1}{t^{1+\gamma}}\left|Q_{m}\left(\frac{ 1}{t^{\alpha}}\right)-\Psi_{T}\left(\frac{1}{t^{\alpha}}\right)\right|\leq \frac{C(n,\alpha)}{m^{n}}\sum_{t=1}^{+\infty}\frac{1}{t^{1+\gamma}}.\]

By choosing \(\alpha=10^{-2}\) and \(\gamma=10^{-4}\beta\), we have \(0.99\beta-1=\frac{\beta-\gamma}{1+\alpha}-1=\frac{\beta-(1+\gamma+\alpha)}{1+ \alpha}<\frac{\beta-(1+\gamma)}{1+\alpha}\). Thus, we obtain our result: for any \(n\in[\lfloor 0.99\beta\rfloor-1]\) (\(\beta\geq 2/0.99\)), the following error estimate holds:

\[\|P_{m}(\cdot)-\rho(\cdot)\|_{\ell_{1}(\mathbb{N}_{+})}\leq\frac{C(n)}{m^{n}} \sum_{t=1}^{+\infty}\frac{1}{t^{1+\gamma}}\leq\frac{C(n)}{m^{n}}\sum_{t=1}^{+ \infty}\frac{1}{t^{1+10^{-4}}}=\frac{\tilde{C}(n)}{m^{n}}.\]Some Background and Proof Preparation

### T5's relative positional encoding

The T5's Relative Positional Encoding is primary focus of this study. Its standard form in practical applications (Raffel et al., 2020) adheres to \(R_{t,s}=r(t-s)\), where

\[-r(n)=\begin{cases}n,&\text{ if }n<\mathcal{B}\\ \mathcal{B}+\lfloor\mathcal{B}\cdot\frac{\log(n/\mathcal{B})}{\log(\mathcal{D }/\mathcal{B})}\rfloor,&\text{ if }\mathcal{B}\leq n<\mathcal{D}\\ 2B-1,&\text{ if }n\geq\mathcal{D}\end{cases}.\]

Here, \(\mathcal{D}\) is a large integer, signifying the longest distance of concern, while \(\mathcal{B}\) is a small integer. One can see that for \(n<\mathcal{B}\), \(r(\cdot)\) exhibits polynomial decay, whereas for \(\mathcal{B}<n<\mathcal{D}\), \(r(\cdot)\) demonstrates logarithmic decay. Consequently, the _overall decay rate_ of \(r(\cdot)\) is _logarithmic_.

The following Table further provides an example of standard T5's Relative Positional Encoding.

### Barron space theory

The well-known universal approximation result for 2NNs asserts that 2NNs can approximate any continuous function (Barron, 1992, 1993, 1994). Nonetheless, this result lacks a characterization of the approximation efficiency, i.e., how many neurons are needed to achieve a certain approximation accuracy? This gap was addressed by the Barron space theory (E et al., 2019, 2021). It is established that for any function within Barron space \(f\in\mathcal{B}\), 2NNs with \(m\) neurons (denoted by \(\mathcal{H}_{m}\)) can approximate them efficiently, at a rate of \(\inf_{f_{m}\in\mathcal{H}_{m}}\left\|f-f_{m}\right\|\leq\mathcal{O}(\left\|f \right\|_{\mathcal{B}}/\sqrt{m})\), remarkably independent of the input dimension \(d\), thus avoiding the _Curse of Dimensionality_(Bellman, 1966; Bach, 2017). Specifically, the Barron space is defined by:

**Definition G.1** (Barron space (E et al., 2019, 2021; Ma et al., 2020)).: Consider functions \(f:X\to\mathbb{R}\) that admit the following representation:

\[f(\mathbf{x})=\int_{\Omega}a\sigma(\mathbf{b}^{\top}\mathbf{x}+c)\rho(\mathrm{d}a,\mathrm{ d}\mathbf{b},\mathrm{d}c),\;\mathbf{x}\in X.\]

For any \(p\in[1,+\infty]\), we define the Barron norm:

\[\left\|f\right\|_{\mathcal{B}_{p}}:=\inf_{\rho}\Big{(}\mathbb{E}_{p}\left[|a|^ {p}(\left\|\mathbf{b}\right\|_{1}+|c|)^{p}\right]\Big{)}^{1/p}.\]

Then the Barron space are defined as:

\[\mathcal{B}_{p}:=\{f\in\mathcal{C}:\left\|f\right\|_{\mathcal{B}_{p}}<+\infty\}.\]

**Proposition G.2**.: _For any \(p\in[1,+\infty]\), \(\mathcal{B}_{p}=\mathcal{B}_{\infty}\) and \(\left\|f\right\|_{\mathcal{B}_{p}}=\left\|f\right\|_{\mathcal{B}_{\infty}}\)._

**Remark G.3**.: From the Proposition above, the Barron spaces \(\mathcal{B}_{p}\) are equivalent for any \(p\in[1,+\infty]\). Consequently, in this paper, we use \(\mathcal{B}\) and \(\left\|\cdot\right\|_{\mathcal{B}}\) to denote the Barron space and Barron norm.

**Remark G.4**.: For Barron space \(\mathcal{B}\), both Direct and Inverse Approximation Theorems hold (E et al., 2021). In this paper, we mainly utilize the Direct Approximation Theorem, stated in Lemma G.6.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \(t-s\) & \(0\) & \(1\) & \(2\) & \(3\) & \(4\) & \(5\) & \(6\) & \(7\) & \(8\) & \(9\) & \(10\) & \(11\) & \(12\) & \(13\) & \(14\) & \(15\) \\ \hline \(-r(t-s)\) & \(0\) &

### Useful approximation lemmas

**Lemma G.5** (Jackson (1930)).: _Let \(f\in\mathcal{C}^{n}([0,1])\) with \(f(0)=f^{\prime}(0)=\cdots=f^{(n)}(0)=0\). Then for any \(m\in\mathbb{N}_{+}\), there exists a polynomial \(Q_{m}(x)=\sum\limits_{k=0}^{m-1}\alpha_{k}x^{k}\) such that_

\[\left\|f-Q_{m}\right\|_{L^{\infty}([0,1])}\leq\frac{M(n)}{m^{n}},\]

_where \(M(n)=\max\limits_{k\leq n}\left\|f^{(k)}\right\|_{L^{\infty}([0,1])}\)._

**Lemma G.6** (Ma et al. (2020)).: _For any \(f\in\mathcal{B}\) and \(m\in\mathbb{N}\), there exists a two-layer ReLU neural network \(f_{m}(\mathbf{x})=\sum\limits_{k=1}^{m}a_{k}\sigma(\mathbf{b}_{k}^{\top}\mathbf{x}+c_{k})\) with \(m\) neurons such that_

\[\left\|f-f_{m}\right\|_{L^{\infty}([0,1]^{4})}\leq\tilde{\mathcal{O}}\left( \frac{\left\|f\right\|_{\mathcal{B}}}{\sqrt{m}}\right).\]Experiments

### Restatement of our theoretical insights

As detailed in Section 1, our theoretical analysis reveals the following novel insights into the expressive power and mechanisms of Transformer:

**Insight (1). The distinct roles of the number of layers, the number of Attn heads, and the width of FFN layers. (1a)** Deeper Transformers can handle tasks with memories with more intricate interrelationships, such as nested relationships (Type II). (1b) In contrast, for tasks with memories lacking such interrelationships (Type I), a single-layer Transformer with sufficient Attn heads and FFN width should suffice.

**Insight (2). The different roles of Attn layers and FFN layers. (2a)** FFN layers are tasked with approximating nonlinear memory functions and the readout function, (2b) while Attn layers are responsible for extracting the tokens from the memory locations.

**Insight (3). The functionality and necessity of Dot-product (DP). (3a)** For the relatively simple Task I, DP is not necessary and can be omitted. (3b) However, for the more complex Task II, DP provides necessary nonlinearity: the cooperation between DP and RPE provides the needed interaction between the temporal space and the token space.

**Insight (4). The efficiency of Relative Positional Encoding (RPE) in modeling long-range correlations.** The primary role of RPE is to approximate the memory kernels. (4a) Transformer with log-type RPE can handle heavy-tailed memories. (4b) Transformer with lin-type RPE can handle light-tailed memories.

### Experimental Validation

To validation of our theoretical **insights (1a)\(\sim\)(4b)**, we conduct 8 experiments, from simple toy models to more complex LLM pre-training. The experiments are conducted on 1 A100.

#### h.2.1 Validation of Insight (1a)

_Objective._ As indicated in Section 4, numerous NLP tasks exhibit complex interrelationships among tokens and belong to our Task II. This experiment aims to verify our Insight (1a): for such tasks, increasing the number of layers \(L\) is more efficient than increasing the number of Attn heads \(H\).

_Setup._ Specifically, we pretrain decoder-only Transformers (Vaswani et al., 2017) with different \(L\) and \(H\) on the OpenWebText dataset (Gokaslan and Cohen, 2019) for 10,000 iterations (approximately 1B tokens) on 1 A100, using cross-entropy loss and AdamW with the same hyperparameters. To ensure comparability, we meticulously balance the total number of parameters across both experimental setups.

_Results and conclusion._ The final validation losses are shown in Table 2. By comparing these two subtables, the benefits brought by increasing \(L\) are much greater than the benefits brought by increasing \(H\) (\(0.802>0.136\)), thereby corroborating our Insight (1a).

#### h.2.2 Validation of Insight (1b)

_Objective._ As mentioned in Section 3, sparse Boolean functions have no interactions among the memories and belong to our Task I. This experiment aims to verify our Insight (1b): for such tasks, a single-layer Transformer equipped with a sufficient number of Attn heads \(H\) and FFN width \(m\) suffices, and there is no need to increase the number of layers \(L\).

\begin{table}
\begin{tabular}{c|c|c} \hline \hline \(L=1,H=8\) (26M) & \(L=1,H=12\) (29M) & \(L=1,H=12\) (32M) \\ \hline
5.796 (baseline) & 5.689 (\(\downarrow\) 0.107) & 5.660 (\(\downarrow\) 0.136) \\ \hline \hline \(L=1,H=8\) (26M) & \(L=4,H=8\) (29M) & \(L=8,H=8\) (32M) \\ \hline
5.796 (baseline) & 5.374 (\(\downarrow\) 0.422) & 4.994 (\(\downarrow\) 0.802) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of the experiment supporting Insight (1a).

Setup._ Specifically, we train single-layer DP-free Transformers with different \(H\) and \(m\) to learn a sparse Boolean target function \(f^{*}\): \(f^{*}(\mathbf{x}):=g^{*}(x_{48},x_{56},x_{99}):=\sum_{k=1}^{64}\mathrm{ReLU}(\langle w _{k}^{*},(x_{48},x_{56},x_{99})\rangle)\) for input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\), where \(\mathbf{w}_{k}^{*}\) are generated by \(\mathbf{w}_{k}^{*}\sim N(0,I_{3})\). Training proceeds for 10,000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters.

Results and conclusion.The final validation losses are shown in Table 3. As shown in this table, a single-layer Transformer equipped with a sufficient \(H\) (32) and \(m\) (256) is adequate for representing this sparse Boolean function. This empirical evidence supports our Insight (1b).

#### h.2.3 Validation of Insight (2a)

_Objective._ This experiment aims to verify our Insight (2a): to learn a sparse Boolean function with a "complex" readout function and "simple" memories, increasing the FFN width \(m\) can significantly improve the performance, whereas increasing the number of Attn heads \(H\) brings almost no benefit.

Setup.Specifically, we train single-layer DP-free Transformers with different \(H\) and \(m\) to learn a sparse Boolean function with a "complex" readout function (\(g^{*}\)) and a "simple" single memory (\(x_{99}\)): \(f^{*}(\mathbf{x}):=g^{*}(x_{99}):=\sum_{k=1}^{64}\mathrm{ReLU}(\mathbf{w}_{k}^{*}\cdot x _{99})\) for any input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\), where \(\mathbf{w}_{k}^{*}\) are generated by \(\mathbf{w}_{k}^{*}\sim N(0,1)\). Training proceeds for 10,000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters.

Results and conclusion.The final validation losses are shown in Table 4. The tables indicate that, for learning a sparse Boolean function with a "complex" readout function and "simple" memories, increasing \(m\) can significantly improve the performance (\(0.49\to 0.002\)), almost completing this task perfectly. Conversely, increasing \(H\) fails to yield substantial improvement. This empirical evidence supports our Insight (2a).

#### h.2.4 Validation of Insight (2b)

_Objective._ Contrasting with Experiment (2a), this experiment aims to verify our Insight (2b): for learning a sparse Boolean function with a "simple" readout function and "complex" memories, increasing the number of Attn headers \(H\) can substantially improve the performance while increasing FFN width \(m\) will offer almost no benefit.

Setup.Specifically, we train single-layer DP-free Transformers with different \(H\) and \(m\) to learn a sparse Boolean function with a "simple" linear readout function (\(g^{*}\)) and relatively "complex" memories (\(x_{48},x_{56},x_{99}\)): \(f^{*}(\mathbf{x}):=g^{*}(x_{48},x_{56},x_{99}):=x_{48}+x_{56}+x_{99}\) for any input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\). Training processes for 10,000 iterations (1M samples), using squared loss and AdamW with the same hyperparameters.

Results and conclusion.The final validation losses are presented in Table 5. The tables indicate that, for learning a sparse Boolean function with a "simple" readout function and "complex" memories, increasing \(m\) can significantly improve the performance (\(1.16\to 10^{-6}\)), closely achieving task perfection. In contrast, increasing \(m\) brings almost no benefits. This empirical evidence supports our Insight 2(b).

#### h.2.5 Validation of Insight (3a)

_Objective._ As mentioned in Section 3, learning sparse Boolean functions has no interactions among the memories and belongs to our Task I. This experiment aims to verify our insight (3a): for such

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline  & \(m=8\) & \(m=64\) & \(m=512\) & \multicolumn{1}{c}{\(H=8\)} & \multicolumn{1}{c}{\(H=64\)} & \multicolumn{1}{c}{\(H=512\)} \\ \hline \(H=8\) & 0.49 & 0.006 & 0.002 & \multicolumn{1}{c}{\(m=8\)} & 0.49 & 0.49 & 0.52 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of the experiment supporting Insight (2a).

\begin{table}
\begin{tabular}{c|c|c} \hline \hline \(H=2,m=16\) & \(H=8,m=64\) & \(H=32,m=256\) \\ \hline
0.21 & 0.04 & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of the experiment supporting Insight (1b).

tasks, a DP-free Transformer equipped with a sufficient number of Attn heads \(H\) and FFN width \(m\) is sufficiently capable. Moreover, there is no need to use DP structure in Attn.

_Setup._ Specifically, we train single-layer DP-free Transformers with different \(H\) and \(m\) and with DP or without DP to learn a sparse Boolean target function \(f^{*}\): \(f^{*}(\mathbf{x}):=g^{*}(x_{48},x_{56},x_{99}):=\sum_{k=1}^{64}\mathrm{ReLU}((\mathbf{w }_{k}^{*},(x_{48},x_{56},x_{99})))\) for input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\), where \(\mathbf{w}_{k}^{*}\) are generated by \(\mathbf{w}_{k}^{*}\sim N(0,I_{3})\). Training proceeds for 10,000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters.

_Results and conclusion._ The final validation losses are shown in Table 6. The findings illustrate that a DP-free Transformer equipped with a sufficient \(H\) (32) and \(m\) (256) is adept at accurately representing the given sparse Boolean function. Additionally, the Incorporation of the DP structure into the layers contributes marginally to performance enhancement. This substantiates our Insight (3a).

#### h.2.6 Validation of Insight (3b)

_Objective._ As indicated in Section 4, numerous NLP tasks exhibit complex interrelationships among tokens and belong to our Task II. This experiment aims to verify our Insight (3b): for such tasks, the utilization of DP structure in Attn layers is necessary.

_Setup._ Specifically, we pre-train Transformers with DP or without DP on the OpenWebText dataset for 10,000 iterations (approximately 1B tokens) on 1 A100, using cross-entropy loss and AdamW with the same hyperparameters.

_Results and conclusion._ The final validation losses are presented in Table 7. As shown in the table, for NLP pre-training tasks, Transformer incorporating DP structure is more efficient than Transformer without DP (\(5.796<5.830,5.374<5.486,4.994<5.274\)), thereby supporting our Insight 3(b).

#### h.2.7 Validation of Insight (4a)

_Objective._ This experiment aims to verify our Insight (4a): for learning Task III with heavy-tailed memories, Transformers with log-type RPE are efficient, whereas those with lin-type RPE fail.

_Setup._ Specifically, we train single-layer, FFN-free, DP-free Transformers with log-type RPE or lin-type RPE and varying numbers of Attn heads \(H\). The target function involves a heavy-tailed memory kernel \(\rho(t)=t^{-0.5}\): \(f^{*}(\mathbf{x}):=\sum_{s=1}^{1000}x_{s}\rho(1000-s)\) for any input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\). Training processes for 10,000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters.

_Results and conclusion._ The final validation losses are shown in Table 8. As shown in the table, to learn heavy-tailed memories, even single-head Transformer with log-type RPE can complete it

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline  & \(H=2,m=16\) & \(H=8,m=64\) & \(H=32,m=256\) \\ \hline with DP & 0.21 & 0.04 & 0.01 \\ \hline without DP & 0.17 & 0.11 & 0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of the experiment supporting Insight (3a).

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline  & \(m=2\) & \(m=64\) & \(m=256\) \\ \hline \(H=2\) & 1.16 & 0.81 & 1.23 \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c|c|c} \hline \hline  & \(H=2\) & \(H=64\) & \(H=256\) \\ \hline \(m=2\) & 1.16 & \textless{}1e-6 & \textless{}1e-6 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of the experiment supporting Insight (2b).

perfectly (\(<10^{-5}\)). Conversely, Transformers employing lin-type RPE exhibit limited improvement even with up to 64 heads (\(0.19\)). This empirical evidence supports our Insight (4a).

#### h.2.8 Validation of Insight (4b)

_Objective._ In contrast to Experiment (4a), this experiment aims to verify that for learning our Task III with light-tailed memories, Transformers with lin-type RPE are efficient, whereas those with log-type RPE fail.

_Setup._ Specifically, we train single-layer, FFN-free, DP-free Transformers with log-type RPE or lin-type RPE and varying numbers of Att heads \(H\). The target function involves a heavy-tailed memory kernel \(\rho(t)=e^{-5t}\): \(f^{*}(\mathbf{x}):=\sum_{s=1}^{1000}x_{s}\rho(1000-s)\) for any input sequence \(\mathbf{x}=(x_{1},\cdots,x_{1000})\in\{\pm 1\}^{1000}\). Training processes for 10,000 iterations (1M samples) using squared loss and AdamW with the same hyperparameters.

_Results and conclusion._ The final validation losses are shown in Table 9. As shown in the table, to learn light-tailed memories, even single-head Transformer with lin-type RPE can complete it perfectly (\(<10^{-7}\)). Conversely, Transformers employing log-type RPE exhibit limited improvement even with up to 64 heads (\(5.3\times 10^{-4}\)). This empirical evidence supports our Insight (4b).

### Practical Implications

Our theoretical insights and empirical evidence can directly lead to the following 8 practical implications, such as the strategic selection of Transformer hyperparameters for specific tasks.

* **Implication (1a).** (supported by Insight (1a) and Experiment (1a)) For sequence modeling tasks with complex interrelations between memories, such as many NLP applications, enhancing the number of layers \(L\) is more beneficial than increasing the number of Attn heads \(H\) and FFN width \(m\).
* **Implication (1b).** (supported by Insight (1b) and Experiment (1b)) For simple sequence modeling tasks with almost no memory intercorrelation, such as the learning of sparse Boolean function, improving performance necessitates only sufficient \(H\) and \(m\) in a single-layer Transformer, without a need to increase \(L\).
* **Implication (2a).** (supported by Insight (2a) and Experiment (2a)) For sequence modeling tasks with complex readout or memory functions, increasing \(m\) can significantly improve performance.
* **Implication (2b).** (supported by Insight (2b) and Experiment (2b)) For sequence modeling tasks with multiple memories, increasing \(H\) can markedly improve performance.
* **Implication (3a).** (supported by Insight (3a) and Experiment (3a)) For simple sequence modeling tasks with almost no memory correlations, such as learning sparse Boolean functions, omitting the DP structure in Attn layers can still perform well.
* **Implication (3b).** (supported by Insight (3b) and Experiment (3b))

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & \(H=1\) & \(H=4\) & \(H=16\) & \(H=64\) \\ \hline type=log & **\textless{}1e-5** & **\textless{}1e-5** & **\textless{}1e-5** & **\textless{}1e-5** \\ \hline type=lin & 0.73 & 0.68 & 1.16 & 0.19 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results of the experiment supporting Insight (4a).

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & \(H=1\) & \(H=4\) & \(H=16\) & \(H=64\) \\ \hline type=log & 9.1e-4 & 3.7e-3 & 2.6e-3 & 5.3e-4 \\ \hline type=lin & **\textless{}1e-7** & **\textless{}1e-7** & **\textless{}1e-7** & **\textless{}1e-7** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results of the experiment supporting Insight (4b).

For sequence modeling tasks with complex correlations between memories, such as many NLP tasks, preserving the DP structure in attention layers is crucial for achieving high performance due to its indispensable nonlinearity.
* **Implication (4a).** (supported by Insight (4a) and Experiment (4a)) For sequence modeling tasks with heavy-tailed memories, the employment of log-type RPE (such as T5's RPE and KERPLE (log)) is recommended over lin-type RPE (such as Alibi).
* **Implication (4b).** (supported by Insight (4b) and Experiment (4b)) For sequence modeling tasks with light-tailed memories, the employment of lin-type RPE (such as Alibi) is recommended over log-type RPE (such as T5's RPE and KERPLE (log)).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We believe that the abstract and introduction reflect the contributions and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: In Section 2, 3, 4, and 5; Appendix B, C, D, E, and F. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We believe that all of the experimental results are reproducible in our work. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: The code or data of the experiments are simple and easy to reproduce following the description in the paper. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Appendix H. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: the approximation error is deterministic and there is no need to consider the error bars here. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Section H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have confirmed that the research is conducted with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: [The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines: [The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. *