# Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space

Xiran Fan

Department of Statistics

University of Florida

fanxiran@ufl.edu

&Chun-Hao Yang

Institute of Statistics and Data Science

National Taiwan University

chunhaoy@ntu.edu.tw

&Baba C. Vemuri

Department of CISE

University of Florida veemuri@ufl.edu

###### Abstract

Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.

## 1 Introduction

Hyperbolic space, a non-Euclidean space with constant negative curvature, has been shown [25, 23, 29, 24] to be effective for representing hierarchically organized data. For example, authors in [25] showed that a tree can be embedded in a hyperbolic space with arbitrarily small distortion. The main reason for this is that a hyperbolic space can be regarded as a continuous version of trees - the volume of the space grows _exponentially_ as one moves away from the center in hyperbolic space. This matches the growth pattern of the number of nodes in a tree which grows _exponentially_ as the depth of the tree increases. Hyperbolic space embedding has been shown to be a promising approach for representing data with a (latent) hierarchical structure [23, 24, 29, 15].

Recently, representation of data in hyperbolic space for the fundamental tasks of unsupervised and supervised learning has been popularized in various contexts, e.g., dimensionality reduction [10, 13], clustering [22], large-margin classifier [12, 32, 11], regression [20], etc. Existing 'linear' classifiers in hyperbolic spaces are predominantly based on _geodesics_ i.e., using geodesics as decision boundaries. In [12], the decision boundary is chosen to be the intersection of the hyperboloid model and a hyperplane in the ambient space, which in this case is the Minkowski space. Then, the support vector machine (SVM) in hyperbolic space is formulated as a nonconvex optimization problem. In [32], authors followed the same parameterization of the hyperbolic geodesic decision plane and provided a series of algorithms to provably learn large margin classifiers in hyperbolic space. However, as pointed out by [11], the algorithm in [32] fails to converge in practice. Authors in [11] used the Poincare ball model and parameterized the geodesic decision plane as a hyperplane mapped using the exponential map from the _tangent space_ at some reference point. They first constructed convex hulls for each data cluster in the hyperbolic space and the reference point is then chosen to be the midpointbetween different convex hulls. Then they apply a _Euclidean_ perceptron/SVM algorithm to data lifted into the tangent space at the aforementioned reference point. Although the optimization problem in the tangent space is convex, the procedure of the tangent space approximation introduces inaccuracies and distortions. Moreover, convex hull learning is highly unstable and their implementation is only applicable to the 2-dimensional hyperbolic space.

Finally, it is worth mentioning that linear classification within hyperbolic space, which can be considered as the last layer, referred to as the hyperbolic logistic regression (LR), is a fundamental component of hyperbolic neural networks (HNNs) [16; 27]. The calculation of logits in this layer is based on the distances between samples and the geodesic decision boundary. Notably, this hyperbolic LR employs a geodesic decision boundary but is not a large-margin classifier.

### Horospherical Decision Boundaries for Classification in Hyperbolic Sapce

_Horospheres_, which are the level sets of the _Busemann function_ in hyperbolic spaces, are the analogs of Euclidean hyperplanes [4]. Horospheres (horocycles) are contained in the Poincare ball (disk) and are tangential to the ball (disk) at an ideal point as shown in Figure 1. A collection of horospheres centered at the same ideal point are parallel to each other and the lengths of geodesic segments between two horospheres are all equal, just as the lengths of line segments between parallel hyperplanes in Euclidean space are all equal. This property of horospheres was explored by [9] to develop a dimensionality reduction method for data in hyperbolic space. By using horospherical projection, they are able to preserve the distance information in the original data. However, there is no literature on constructing a 'linear' classifier in hyperbolic space using horospheres as the decision boundaries although the horospheres are the hyperbolic equivalent of Euclidean hyperplanes. Therefore, it is natural to consider the use of horospheres as decision boundaries for classification in hyperbolic spaces. In this work, we propose a novel _hyperbolic large-margin classifier using horospheres as decision boundaries in the Poincare model_. We term this classifier as a _HoroSVM_. The horospheres are well-defined in the Poincare ball model. A toy example as shown in Figure 2 demonstrates the advantage of horospherical decision boundaries over geodesic decision boundaries. As the tree-structured data grows in depth, leaf nodes are embedded closer to each other within a subtree and among different subtrees. One of the classification problems in hyperbolic space is to determine whether a node belongs to a chosen subtree given the embedding. For comparison purposes, the decision boundaries of HoroSVM (Figure 2(a)) and hyperboloid SVM [12] (Figure 2(b)) are shown in the figure. As evident, the horospherical decision boundary perfectly separates (the root node is excluded in training) the data while the geodesic decision boundary makes several mistakes on both positive and negative samples. We present a novel formulation of the classification problem in the hyperbolic space as a geodesically convex optimization problem on a Riemannian manifold. This optimization problem can be easily solved using any Riemannian gradient descent technique guaranteeing global optimality. Gradient-based optimizations for geodesically convex problems guaranteeing global optimal solutions are the topic of investigation in optimization literature and we refer the reader to [35] for detailed convergence analysis of several such optimization methods. Further, we empirically validate our method on several real and synthetic data sets.

It should be noted that a horosphere decision boundary has been used in some recent works [31; 28] in constructing HNNs. For example, authors in [31] proposed hyperbolic neuron models using the Busemann function as a generalization of the Euclidean inner product to extract horosphere features from data. Authors in [28] proposed a shallow fully-connected continuous network spanned by (hyperbolic) neurons, on noncompact symmetric space (including hyperbolic space) using the Helgason-Fourier transform. Authors in [33] produced Euclidean features from hyperbolic embeddings via the eigenfunctions of the Laplace operator in the hyperbolic space where the eigenfunctions involve horosphere features. Note that none of the above works developed a large margin classifier using the horosphere as a decision boundary. To the best of our knowledge, our work is the first in the

Figure 1: A 2-d Poincaré disk model \(\mathbb{B}^{2}\) and its boundary \(\partial\mathbb{B}^{2}=\mathbb{S}^{1}\). Given an ideal point \(\bm{\omega}\in\partial\mathbb{B}^{2}\), the black lines and curves are hyperbolic geodesics starting (ending) at \(\bm{\omega}\) and the red circles are horocycles centered at \(\bm{\omega}\).

literature to present a convex optimization formulation of a large-margin classifier using a horosphere decision boundary in a hyperbolic space.

The rest of this paper is organized as follows. In Section 2, we present some background on hyperbolic geometry pertinent to the work presented here. In Section 3, we present our horospherical boundary-based classification methods. Experimental results are presented in Section 4 to demonstrate the advantage of our HoroSVM over competing hyperbolic classifiers. Finally, we conclude in Section 5.

## 2 Background

In this section, we review some basic concepts of hyperbolic geometry including the generalization of the Euclidean hyperplane to the hyperbolic space namely, the horosphere.

### Hyperbolic Space and the Poincare Ball Model

There are five isometric models of the hyperbolic space: the Poincare ball model, the Lorentz model, the Klein model, the upper-half space model, and the Hemisphere model [7]. We choose the Poincare Ball model in this paper as it is easy to visualize and the Busemann function has a nice closed-form expression in this model. Note that the decision boundary of choice in our work is the level-set of the Busemann function namely, the horosphere.

An \(n\)-dimensional Poincare Ball model, denoted by \((\mathbb{B}^{n},g_{\mathbb{B}})\), consists of all points in an open ball of radius 1, i.e., \(\mathbb{B}^{n}=\{\bm{x}\in\mathbb{R}^{n}:\|\bm{x}\|<1\}\), and equipped with the Riemannian metric \(g_{\mathbb{B}}(\bm{x})=4(1-\|\bm{x}\|^{2})^{-2}g_{\mathbb{R}}\), where \(\|\cdot\|\) is the Euclidean \(L_{2}\) norm, and \(g_{\mathbb{R}}\) is the Euclidean metric. The geodesic distance between points \(\bm{x},\bm{y}\in\mathbb{B}^{n}\) is \(d_{\mathbb{B}}(\bm{x},\bm{y})=\cosh^{-1}\Big{(}1+2\frac{\|\bm{x}-\bm{y}\|^{2 }}{(1-\|\bm{x}\|^{2})(1-\|\bm{y}\|^{2})}\Big{)}\).

### Horospheres

Geodesics, geodesic rays, and ideal pointsThe shortest path that connects two points in the Poincare Ball model is called a _geodesic segment_. A _geodesic ray_ is a geodesic segment that can be infinitely extended in one direction. We call the endpoint at infinity of a geodesic ray an _ideal point_. For \(\mathbb{B}^{n}\), ideal points form the boundary of the ball: \(\partial\mathbb{B}^{n}=\mathbb{S}^{n-1}=\{x\in\mathbb{R}^{n}:\|\bm{x}\|=1\}\), where \(\mathbb{S}^{n-1}\) is the \((n-1)\)-dimensional hypersphere. The hypersphere \((\mathbb{S}^{n-1},g_{\mathbb{S}})\) is a Riemannian manifold equipped with the Riemannian metric \(g_{\mathbb{S}}=4(1+\|\bm{x}\|^{2})^{-2}g_{\mathbb{R}}\).

Busemann function [6]Let \(\bm{\omega}\in\partial\mathbb{B}^{n}\) be an ideal point and \(\gamma_{\bm{\omega}}:[0,\infty)\to\mathbb{B}^{n}\) a geodesic ray pointing \(\bm{\omega}\). The _Busemann function_ is defined as

\[b_{\bm{\omega}}(\bm{x})=\lim_{t\to\infty}(d(\gamma_{\bm{\omega}}(t),\bm{x})-t ),\quad\bm{x}\in\mathbb{B}^{n}.\] (1)

Figure 2: A balanced tree with depth 6 and spread 4 embedded in a 2-d Poincaré ball model using [15] is depicted in the figure. The orange plus node is the root of a chosen subtree, the blue dots are positive samples (nodes of the subtree) and the red dots are negative samples. (a) depicts a HoroSVM performance on the classification of positive and negative samples/nodes along with a zoomed-in version on its right. (b) depicts the geodesic boundary from the competing method, Hyperboloid SVM [12], along with the zoomed-in version to its right.

In the Poincare Ball model, Eq. (1) has a closed form \(:b_{\bm{\omega}}(\bm{x})=-\log\frac{1-\|\bm{x}\|^{2}}{\|\bm{\omega}-\bm{x}\|^{2}}\).

Horospheres [6]In \(\mathbb{B}^{n}\), a horosphere is a \((n-1)\)-dimensional sphere that is internally tangent to \(\partial\mathbb{B}^{n}\) at an ideal point. For a given \(\bm{\omega}\in\partial\mathbb{B}^{n}\), the level sets of Busemann function \(b_{\bm{\omega}}(\bm{x})\) in the Poincare ball model is a series of horospheres tangent at \(\bm{\omega}\). Horospheres are hyperbolic hyperplanes in the sense that the corresponding construction in Euclidean space gives a hyperplane.

Given an ideal point \(\omega\in\partial\mathbb{B}^{n}\), the function defined by \(\langle\omega,x\rangle_{B}:x\mapsto-b_{\omega}(x)\) is constant over horosphere tangent at \(\omega\).1 Hence any horosphere can be parameterized with an ideal point \(\omega\) and an offset value \(b\) of the level set.

Footnote 1: Note that the Euclidean inner product \(\langle\bm{w},\cdot\rangle\) is constant over a hyperplane that is perpendicular to a given direction \(\bm{w}\).

Let \(\Pi\) denote the set of horospheres of \(\mathbb{B}^{n}\). A horosphere \(\pi\in\Pi\) can be parameterized by \(0<\mu\in\mathbb{R}^{+}\)2, \(\bm{\omega}\in\mathbb{S}^{n-1}\), and \(b\in\mathbb{R}\) as

Footnote 2: Parameter \(\mu\) is included for simplicity in analysis later on (Eq 3, 12)

\[\pi_{\mu,\bm{\omega},b}:=\{\bm{z}\in\mathbb{B}^{n}|\mu\langle\bm{\omega},\bm{z }\rangle_{\mathbb{B}}-b=0\}.\] (2)

We will use \(\pi\) or \(\pi\). to represent a horosphere in different parameterizations hereafter.

## 3 Horospherical Boundary-based Classification

In this section, we present the key theoretical contributions of our work namely, a horosphere-based SVM classifier that involves formulating and solving a geodesically convex optimization problem. First, we present some preliminary facts and results about the horospheres. Then, we present the optimization problems for the horospherical perceptron and SVM along with analysis.

### Point to Horosphere Distance

While the distance from the origin \(\bm{o}\) to a horosphere has been known for decades [18] (Introduction 4.1, p.31), we present a natural generalization of previous results by providing a closed-form expression for measuring the hyperbolic distance from any arbitrary point \(\bm{x}\in\mathbb{B}^{n}\) to a given horosphere \(\pi_{\mu,\bm{\omega},b}\). The following remark provides this result.

**Proposition 3.1**.: _Let \(\pi_{\mu,\bm{\omega},b}\) be a horosphere. The hyperbolic distance of a point \(\bm{x}\in\mathbb{B}^{n}\) to a horosphere \(\pi_{\mu,\bm{\omega},b}\) is given by_

\[d_{\mathbb{B}}(\bm{x},\pi_{\mu,\bm{\omega},b})=\frac{|\mu\langle\bm{\omega}, \bm{x}\rangle_{\mathbb{B}}-b|}{\mu}.\] (3)

Notice that it shares a similarity to the Euclidean distance of a point to a hyperplane. Before presenting the proof for Proposition 3.1, we recall the following Fact 3.2 and Lemma 3.3 from [31].

**Fact 3.2**.: _Given an ideal point \(\bm{\omega}\) and a point \(\bm{x}\in\mathbb{B}^{n}\), there is a unique horosphere passing through \(\bm{x}\) and tangent at \(\bm{\omega}\)._

**Lemma 3.3**.: _[_31_]_ _Let \(\Pi_{\bm{\omega}}\) be the set of horocycles of \(\mathbb{B}^{n}\) tangent at \(\bm{\omega}\). Given \(\lambda\in\mathbb{R}\), let \(\pi_{\lambda,\bm{\omega}}\) be the unique horosphere that passes through \(\tanh(\lambda/2)\cdot\bm{\omega}\) and tangent at \(\bm{\omega}\). Note that \(\Pi_{\bm{\omega}}=\cup_{\lambda\in\mathbb{R}}\{\pi_{\lambda,\bm{\omega}}\}\). We have the following two results: (i) the hyperbolic lengths of geodesic (that pass through \(\bm{\omega}\)) segments between \(\pi_{\lambda_{1},\bm{\omega}}\) and \(\pi_{\lambda_{2},\bm{\omega}}\) are equal to \(|\lambda_{1}-\lambda_{2}|\); (ii) \(\langle\bm{\omega},x\rangle_{\mathbb{B}}=\lambda\) for any \(\bm{x}\in\pi_{\lambda,\bm{\omega}}\)._

Figure 3 shows a 2D Poincare disk model \(\mathbb{B}^{2}\) and its boundary \(\mathbb{S}^{1}\). The point \(o\) is the origin of the disk, \(\bm{x}\in\mathbb{B}^{2}\) is a point, and \(\bm{\omega}\in\mathbb{S}^{1}\) is a point at infinity (an ideal point). Two geodesics ending at the same \(\bm{\omega}\) from \(\bm{x}\) and \(o\) respectively are shown in the figure (black solid line/curve). The circle \(\pi_{\mu,\bm{\omega},b}\) is a given horocycle tangent (red solid circle) at \(\bm{\omega}\). The hyperbolic distance \(d_{\mathbb{B}}(\bm{x},\pi_{\mu,\bm{\omega},b})\) between \(\bm{x}\) and \(\pi_{\mu,\bm{\omega},b}\) is identified as the distance between \(\bm{x}\) and \(\bm{y}_{\bm{x}}\) where \(\bm{y}_{\bm{x}}\) is the projection of \(\bm{x}\) to \(\pi_{\mu,\bm{\omega},b}\) along the geodesic ending at \(\bm{\omega}\). Let \(\pi^{\bm{x}}\) (red dashed circle) be the unique horocycle that passes through \(\bm{x}\) and is tangent at \(\bm{\omega}\). Note that the lengths of all geodesic segments between two horocycles are the same. That is, \(d_{\mathbb{B}}(\bm{x},\pi_{\mu,\bm{\omega},b})=d_{\mathbb{B}}(\bm{x},\bm{y}_{ \bm{x}})=d_{\mathbb{B}}(\bm{x}_{0},\bm{y}_{0})\), where \(\bm{x}_{0},\bm{y}_{0}\) are _horocyclic projections_[9] of \(\bm{x},\bm{y}_{\bm{x}}\) along \(\pi^{\bm{x}}\) and \(\pi_{\mu,\bm{\omega},b}\) respectively.

Proof of Proposition 3.1.: Given a point \(\bm{x}\in\mathbb{B}^{n}\) and an ideal point \(\bm{\omega}\in\mathbb{S}^{n-1}\), let \(\pi\in\Pi_{\bm{\omega}}\) be a horosphere tangent at \(\bm{\omega}\) and \(\pi^{\bm{x}}\in\Pi_{\bm{\omega}}\) be the unique horosphere that passes through \(\bm{x}\). Write \(\langle\bm{\omega},\bm{x}\rangle_{\mathbb{B}}=\lambda_{\bm{x}}\) and then \(\bm{x}_{0}=\tanh(\lambda_{\bm{x}}/2)\cdot\bm{\omega}\) is the horospherical projection of \(\bm{x}\) along \(\pi_{\bm{x}}\). For consistency in notation, we write \(\pi^{\bm{x}}\) as \(\pi_{\lambda_{\bm{x}},\bm{\omega}}\). Since the horosphere \(\pi_{\mu,\bm{\omega},b}\) can be reparameterized as \(\pi_{\lambda,\bm{\omega}}\), where \(\lambda=\frac{b}{\mu}\), the hyperbolic distance from \(\bm{x}\) to \(\pi_{\mu,\bm{\omega},b}\) is the hyperbolic distance between \(\pi_{\lambda_{\bm{x}},\bm{\omega}}\) and \(\pi_{\lambda,\bm{\omega}}\) which is \(|\lambda_{\bm{x}}-\lambda|=\frac{|\mu\langle\bm{\omega},\bm{x}\rangle_{\mathbb{ B}}-b|}{\mu}\) (by Lemma 3.3). This completes the proof. 

### Horospherical Decision Boundaries

We consider classification problems in hyperbolic space of the following form: \(\mathcal{X}\subset\mathbb{B}^{n}\) denotes the feature space and \(\mathcal{Y}=\{\pm 1\}\) denotes the binary label space. In the following, we denote the training set by \(S\subset\mathcal{X}\times\mathcal{Y}\). The decision rule using a horosphere as its decision boundary can be written as the following function \(f:\mathcal{X}\mapsto\mathcal{Y}\) where

\[f(\bm{x};\mu,\bm{\omega},b)=\text{sign}\left(\mu\langle\bm{\omega},\bm{x} \rangle_{\mathbb{B}}-b\right).\] (4)

The positive samples are expected to lie inside a horosphere while the negative samples are expected to lie outside a horosphere. This is analogous to the linear decision boundary in Euclidean space and we will build a horospherical perceptron and a horospherical SVM based on this decision boundary.

It should be noted that in \(\mathbb{R}^{n}\) the hyperplane \(\xi_{a,\bm{w},b}=\{\bm{z}\in\mathbb{R}^{n}|a\langle\bm{w},\bm{z}\rangle-b=0\}\) where \(a\in\mathbb{R}^{+},b\in\mathbb{R}\), \(\bm{w}\in\mathbb{S}^{n-1}\) is the hyperplane \(\xi_{a,-\bm{w},-b}\). However, the horospheres \(\pi_{\mu,\bm{\omega},b}\) and \(\pi_{\mu_{\bm{\omega}},-b}\) respectively represent two distinct horospheres, centered at \(\bm{\omega}\) and \(-\bm{\omega}\) respectively. Let \(\Pi^{+}=\{\pi_{\mu,\bm{\omega},b}\in\Pi|b>0\}\) and \(\Pi^{-}=\{\pi_{\mu,\bm{\omega},b}\in\Pi|b<0\}\). Thus, \(\Pi^{+},\Pi^{-}\subset\Pi\) and the radius of \(\pi\in\Pi^{+}\) is less than \(1/2\) and the radius of \(\pi\in\Pi^{-}\) is greater than \(1/2\). In most cases of classification in hyperbolic space, the positive samples are clustered near the boundaries. Hence, we restrict ourselves to finding a horosphere \(\pi\in\Pi^{+}\) that separates data, instead of searching over \(\Pi\). Intuitively, we are looking for a'small' horosphere that captures the positive samples. We are now ready to present the Horospherical Perceptron followed by the Horospherical SVM.

### Horospherical Perceptron

The loss function for the proposed horospherical perceptron is given by

\[l(\mu,\bm{\omega},b;\bm{x},y)=\max(0,-y\cdot(\mu\langle\bm{\omega},\bm{x} \rangle_{\mathbb{B}}-b))\,,\quad(\mu,\bm{\omega},b)\in\mathbb{R}^{+}\times \mathbb{S}^{n-1}\times\mathbb{R}^{+},\] (5)

which is zero when the instance is classified correctly and is proportional to the signed distance of the instance from the horosphere when it is misclassified. The empirical loss for a given data set \(S\) is

\[L(\mu,\bm{\omega},b)=\frac{1}{|S|}\sum_{\{\bm{x},y\}\in S}l(\mu,\bm{\omega},b; \bm{x},y)\] (6)

Hence, the optimal horosphere is learned by solving the above optimization problem on the manifold \(\mathbb{R}^{+}\times\mathbb{S}^{n-1}\times\mathbb{R}^{+}\), i.e.,

\[\mu^{*},\bm{\omega}^{*},b^{*}=\arg\min_{(\mu,\bm{\omega},b)}L(\mu,\bm{\omega},b)\] (7)

To further analyze this optimization problem, we first recall some facts about geodesic convexity.

**Definition 3.4**.: (Geodesically convex sets [30]). Let \((\mathcal{M},g)\) be a Riemannian manifold. A set \(\bm{A}\subseteq\mathcal{M}\) is said to be a geodesically convex set if, for any two points \(\bm{p},\bm{q}\in\bm{A}\), the geodesic \(\gamma_{\bm{p}\bm{q}}\) that connects them is contained in \(\bm{A}\).

**Definition 3.5**.: (Geodesically convex/concave functions [30]) Let \(\bm{A}\subseteq\mathcal{M}\) be a geodesically convex set. A function \(f:\bm{A}\rightarrow\mathbb{R}\) is said to be a geodesically convex function if, for any \(\bm{p},\bm{q}\in\bm{A}\) the composition \(f\circ\gamma_{\bm{p}\bm{q}}:[0,1]\rightarrow\mathbb{R}\) is a convex function, where \(\gamma_{\bm{p}\bm{q}}:[0,1]\rightarrow\mathcal{M}\) is a geodesic that connects \(\bm{p},\bm{q}\). \(f\) is said to be a geodesically concave function if \(-f\) is a geodesically convex function.

Figure 3: The relationship between horocycles, geodesic, and the horocyclic projections in \(\mathbb{B}^{2}\).

**Theorem 3.6**.: _[_30_]_ _Let \(\bm{A}\subseteq\mathcal{M}\) be a geodesically convex set. A function \(f:\bm{A}\to\mathbb{R}\) is geodesically convex if and only if its epigraph \(\text{epi}(f)=\{(\bm{p},c)|f(\bm{p})\leq c\}\subset\bm{A}\times\mathbb{R}\) is a convex set._

Now we present the main theoretical result of this paper.

**Theorem 3.7**.: _For a given training data sample \(\{\bm{x},y\}\in S,0<\|\bm{x}\|<R<1\) (the hyperbolic feature \(\bm{x}\) neither lie on the center nor lie on the boundary), \(l(\mu,\bm{\omega},b;\bm{x},y)\) is a geodesically convex function on \(\mathbb{R}^{+}\times\bm{A}\times\mathbb{R}^{+}\) and is a geodesically concave function on \(\mathbb{R}^{+}\times\bm{B}\times\mathbb{R}^{+}\), where_

\[\bm{A}=\left\{\bm{\nu}\in\mathbb{S}^{n-1}\middle|y\cdot\frac{\bm{x}^{T}\bm{ \nu}}{\|\bm{x}\|}>0\right\}\subset\mathbb{S}^{n-1},\quad\bm{B}=\left\{\bm{\nu }\in\mathbb{S}^{n-1}\middle|y\cdot\frac{\bm{x}^{T}\bm{\nu}}{\|\bm{x}\|}<0 \right\}\subset\mathbb{S}^{n-1}.\] (8)

_Note that both \(\bm{A}\) and \(\bm{B}\) are geodesically convex sets._

Proof.: Let \(l(\mu,\bm{\omega},b;\bm{x},y)=\max(0,-g(\mu,\bm{\omega},b;\bm{x},y))\) where \(g(\mu,\bm{\omega},b;\bm{x},y)=y\cdot(\mu\langle\bm{\omega},\bm{x}\rangle_{ \mathbb{B}}-b)\). Since \(\max(0,-a)\) is a convex function in \(a\in\mathbb{R}\) and \(g(\cdot)\) is linear in \(\mu\) and \(b\), we only need to show that \(g(\cdot)\), as a function of \(\bm{\omega}\in\mathbb{S}^{n-1}\), is geodesically convex (concave). Without loss of generality, let \(\mu=1\) and \(b=0\). Also note that \(y\) is the label of data that takes values from \(\{-1,+1\}\) which may flip the inequality. It suffices to validate results for the positive sample, i.e, \(y=1\).

With a slight abuse of notation, let \(g(\bm{\omega};\bm{x})=g(1,\bm{\omega},0;\bm{x},1)=\langle\bm{\omega},\bm{x} \rangle_{\mathbb{B}}=\ln\frac{1-\|\bm{x}\|^{2}}{\|\bm{\omega}-\bm{x}\|^{2}}\) defined on \(\bm{A}=\left\{\bm{\nu}\in\mathbb{S}^{n-1}\middle|\frac{\bm{x}^{T}\bm{\nu}}{ \|\bm{x}\|}>0\right\}\subset\mathbb{S}^{n-1}\). Since \(-\ln(\cdot)\) is decreasing and convex, we only need to check \(h(\bm{\omega};\bm{x})=\|\bm{\omega}-\bm{x}\|^{2}\) is geodesically convex on \(\bm{A}\), i.e, check that the epigraph of \(h\) is a convex set. Note that \(\frac{\bm{x}^{T}\bm{\omega}}{\|\bm{x}\|}=\cos(\theta_{\bm{\omega}})\) for \(\bm{\omega}\in\mathbb{S}^{n-1}\) where \(\theta_{\bm{\omega}}=\angle(\bm{\omega},\bm{x})\).

\[\text{epi}(h) =\left\{(\bm{\omega},c)\in\bm{A}\times\mathbb{R}\middle|\|\bm{ \omega}-\bm{x}\|^{2}\leq c\right\}\] (9) \[=\left\{(\bm{\omega},c)\middle|\frac{\bm{x}^{T}\bm{\omega}}{\| \bm{x}\|}\geq\frac{1}{2\|\bm{x}\|}(1+\|\bm{x}\|^{2}-c)\right\}\] \[=\left\{(\bm{\omega},c)|\cos(\theta_{\bm{\omega}})\geq d(c) \right\}=\begin{cases}\bm{A}\times[d(c),\infty)&\text{if }d(c)\leq 0\\ \bm{A}_{d}\times[d(c),\infty)&\text{if }d(c)>0\end{cases}\]

where \(d(c)\) is a real number depending on \(c\) and \(\|\bm{x}\|\) and \(\bm{A}_{d}=\{(\bm{\omega},c)|\cos(\theta_{\bm{\omega}})\geq d(c)\}\) is the collection of unit vectors where the angle between the vectors given the data \(\bm{x}\) is small i.e., restricted to a small region on the sphere. The last equality follows from the definition of \(\bm{A}\) and \(\bm{A}_{d}\): if \(d(c)\leq 0\), then \(\{\omega:\cos(\theta_{\bm{\omega}})\geq d(c)\}\cap\bm{A}=\bm{A}\). Similarly, if \(d(c)>0\), \(\{\omega:\cos(\theta_{\bm{\omega}})\geq d(c)\}\cap\bm{A}=\{\bm{\omega}\in \mathbb{S}^{n-1}\middle|\cos(\theta_{\bm{\omega}})\geq d(c)\}:=\bm{A}_{d(c)}\). Both \(\bm{A}\) and \(\bm{A}_{d}\) are geodesically convex sets and this completes the proof. 

The convex sets \(\bm{A},\bm{B}\) are the hemispheres of \(\mathbb{S}^{n-1}\) separated by the hyperplane \(\{\bm{z}\in\mathbb{R}^{n}|\bm{x}^{T}\bm{z}=0\}\) (the hyperplane has \(\bm{x}\) as its normal vector) in its ambient space \(\mathbb{R}^{n}\). Theorem 3.7 tells us that given one data sample \((\bm{x},y)\), the optimal value described in Eq. (7) exists in \(\mathbb{R}^{+}\times\bm{A}\times\mathbb{R}^{+}\), and it is globally optimal. For a collection of training samples \(S=\{(\bm{x}_{i},y_{i})\}_{i=1}^{N}\), let \(\bm{A}_{i}=\left\{\bm{\nu}\in\mathbb{S}^{n-1}\middle|y_{i}\cdot\frac{\bm{x}^{T} \bm{\nu}}{\|\bm{x}_{i}\|}>0\right\}\). If the data are separable by a horosphere, it follows that \(\cap_{i=1}^{N}\bm{A}_{i}\) is non-empty and convex. Then the loss function given \(S\) is geodesically convex on \(\mathbb{R}^{+}\times\cap_{i=1}^{N}\bm{A}_{i}\times\mathbb{R}^{+}\) and the global optimum can be obtained using any gradient-based optimization. Numerically, we apply a Riemannian gradient descent method on the entire space \(\mathbb{R}^{+}\times\mathbb{S}^{n-1}\times\mathbb{R}^{+}\) since \(g(\mu,\bm{\omega},b;\bm{x},y)\) is continuous.

### Horospherical SVM

Given a horospherical decision boundary \(\pi_{\mu,\bm{\omega},b}\) parameterized by \(\bm{\omega}\in\mathbb{S}^{n-1},\mu\in\mathbb{R}^{+}\), and \(b\in\mathbb{R}\), the margin \(\gamma\) is the minimal distance from training samples \(S\) to the decision boundary:

\[\gamma(\mu,\bm{\omega},b)=\inf_{\{\bm{x},y\}\in S}y\cdot f(\bm{x};\mu,\bm{ \omega},b)\cdot d_{\mathbb{B}}(\bm{x},\pi_{\mu,\bm{\omega},b})=\inf_{\{\bm{x},y \}\in S}y\cdot\frac{(\mu\langle\bm{\omega},\bm{x}\rangle_{\mathbb{B}}-b)}{\mu}.\] (10)

The maximum margin classifier can be obtained by solving the following optimization problem:

\[\max_{\mu,\bm{\omega},b}\quad\gamma(\mu,\bm{\omega},b)\qquad s.t.\quad y\cdot \frac{(\mu\langle\bm{\omega},\bm{x}\rangle_{\mathbb{B}}-b)}{\mu}\geq\gamma \quad\text{for all }(\bm{x},y)\in S.\] (11)

**Theorem 3.8**.: _The maximum margin classification problem in hyperbolic space with horosphere as its decision boundary described in Eq. (11) is equivalent to the following optimization problem:_

\[\min_{\mu,\bm{\omega},b}\quad\frac{1}{2}\mu^{2}\qquad s.t.\quad y\cdot(\mu \langle\bm{\omega},\bm{x}\rangle_{\mathbb{B}}-b)\geq 1\quad\text{for all }(\bm{x},y)\in S.\] (12)

The proof is analogous to that in the Euclidean case [3]. Note that the margin is unchanged if we apply the following scale transformation: \(\mu\to\mu/\gamma\) and \(b\to b/\gamma\). We can also build a soft-margin horospherical SVM, dubbed HoroSVM, by minimizing the following loss function:

\[l(\mu,\bm{\omega},b;\bm{x},y)=\frac{1}{2}\mu^{2}+C\sum_{i=1}^{|S|}\max(0,1-y_{ i}\cdot(\mu\langle\bm{\omega},\bm{x}_{i}\rangle_{\mathbb{B}}-b)).\] (13)

where \(C\) is a hyperparameter that controls the tradeoff between minimizing misclassification and maximizing margin.

It is easy to see that the same result as in Theorem 3.7 holds for the loss function in Eq. (13). That is, the loss function is a geodesically convex function on one geodesically convex subset of the parameter space and a geodesically concave function on the other geodesically convex subset of the parameter space. Recall that the idea behind proving that the loss function in the horospherical perceptron, \(\max(0,-g(\cdot))\), where \(g(\cdot)=y\cdot(\mu\langle\bm{\omega},\bm{x}\rangle_{\mathbb{B}}-b)\), is geodesically convex is based on the important fact that \(\max(0,-a)\) is a convex function in \(a\in\mathbb{R}\). Similarly, the same idea applies to HoroSVM, where the hinge loss is used, and the loss function becomes \(\frac{1}{2}\mu^{2}+\max(0,1-g(\cdot))\). Note that \(\max(0,1-a)\) is also a convex function in \(a\in\mathbb{R}\) and \(\frac{1}{2}\mu^{2}\) is a convex function in \(\mu\), these facts complete the proof of the desired property for the HoroSVM, which is analogous to Theorem 3.7 for horospherical perceptron.

We can then apply any Riemannian gradient descent optimization methods for updating the parameters in HoroSVM since the problem is an optimization problem over a product space of Riemannian manifolds, \(\mathbb{R}^{+}\times\mathbb{S}^{n-1}\times\mathbb{R}^{+}\). We refer the readers to [5] and [1] for more details about optimization techniques on Riemannian manifolds.

## 4 Experiments

In this section, we present several experimental results obtained from an application of our HoroSVM to synthetic data as well as real data sets used in published literature. Our implementation is based on Pymanopt [19] using the Riemannian conjugate gradient method [26] on Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz.

### Network Data Set

Here, we follow the experimental setup in [12], and evaluate our HoroSVM over four real-world network data sets used by [8]: karate[34] (2 classes, 34 nodes ), polblogs[2] (2 classes, 1224 nodes ), polbooks3 (3 classes, 105 nodes ), and football[17] (12 classes, 115 nodes ).

Footnote 3: http://www-personal.umich.edu/~mejn/netdata/

The network data is embedded in a 2D hyperbolic space using the method of [8]. Given the hyperbolic embeddings, we compare our HoroSVM with three other competing large margin classifiers: Euclidean SVM (even though it violates the hyperbolic geometry), hyperboloid SVM [12], and Poincare SVM [11]. The absence of comparison with [32] in this experiment is due to two reasons. First, [32] aims to provide a theoretical understanding of hyperbolic spaces in classification, focusing on 'linearly' separable data (data that can be separated by a geodesic). They do not address extensions to nonlinearly separable data, which limits their applicability to many practical datasets. Second, for datasets that are linearly separable in hyperbolic space, the approach in [32] aligns with the hyperboloid SVM when adversarial training is not applied in [32].

For multiclass classification, a one-verses-rest strategy is applied. We conducted a five-fold cross-validation on each data set, where we chose the hyperparameter \(C\) from \(\{1,5,10\}\) during the cross-validation procedure. Note that a more extensive search space for \(C\) may lead to potential performance improvements. The mean of the F1 score followed by the standard deviation over fivetrials are summarized in Table 1. As evident from the table, our method yields the best results on all the data sets.

HoroSVM outperformed other methods on all four data sets. The data in karate are well-separated and thus both Euclidean SVM and hyperboloid SVM performed equally well. Our method outperforms the others since the horospheres have several nice properties, the most important of which is that the Busemann function whose level sets are the horospheres is a convex function that guarantees global optimality in the optimization. Notice that the performance of Poincare SVM on karate is inferior to others by a significant amount. The reason is that the performance of Poincare SVM is sensitive to the choice of the reference point. We demonstrate our performance gain on the remaining data sets, and our method is more consistent, compared to Euclidean SVM and hyperboloid SVM, in terms of lower standard deviation, on football data set where data exhibit a larger variance/spread.

### Subtree Classification in WordNet

A task of considerable interest in hyperbolic space classification problems is to determine whether a node belongs to a given subtree in the hyperbolic embedding. We obtained hyperbolic embeddings in various dimensions using the approach in [15] for WordNet 4 noun hierarchy (82,115 nodes). We consider four subtrees whose roots are the following synsets: ANIMAL.N.01, GROUP.N.01, WORKER.N.01, and MAMMAL.N.01.

Footnote 4: https://wordnet.princeton.edu/

We split all nodes in a subtree into positive training (80%) and test (20%) nodes and applied the same process to the remaining WordNet nodes to create negative training and test sets. The average F1 scores and the standard deviations over 3 trials are shown in Table 2. The number of positive training/test samples of each data set are listed as well. We exclude Poincare SVM from the comparisons in this task. The reason being, data are highly imbalanced in this task and the positive samples are clustered near the boundary. The reference point learned in Poincare SVM will be close to the boundary where the tangent approximation of data at this reference point is highly distorted, as opposed to the original hyperbolic embeddings. It is therefore hard to locate a hyperplane in the

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & \begin{tabular}{c} Kanate \\ \end{tabular} & \begin{tabular}{c} Polblogs \\ \end{tabular} & \begin{tabular}{c} Polbooks \\ \end{tabular} & 
\begin{tabular}{c} Football \\ \end{tabular} \\ \hline Euclidean SVM & 0.95 \(\pm\) 0.06 & 0.92 \(\pm\) 0.02 & 0.83 \(\pm\) 0.03 & 0.29 \(\pm\) 0.12 \\ Hyperboloid SVM & 0.95 \(\pm\) 0.06 & 0.92 \(\pm\) 0.01 & 0.83 \(\pm\) 0.03 & 0.30 \(\pm\) 0.14 \\ Poincaré SVM & 0.78 \(\pm\) 0.16 & 0.92 \(\pm\) 0.02 & 0.84 \(\pm\) 0.03 & 0.32 \(\pm\) 0.04 \\ HoroSVM (Ours) & **0.98 \(\pm\) 0.04** & **0.93 \(\pm\) 0.01** & **0.85 \(\pm\) 0.04** & **0.34 \(\pm\) 0.06** \\ \hline \hline \end{tabular}
\end{table}
Table 1: F1 scores for node classification on network datasets. Boldface indicates best performance.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & \begin{tabular}{c} animal.n.01 \\ 3218/798 \\ \end{tabular} & \begin{tabular}{c} group.n.01 \\ 6649/1727 \\ \end{tabular} & \begin{tabular}{c} worker.n.01 \\ 861/254 \\ \end{tabular} & 
\begin{tabular}{c} mammal.n.01 \\ 953/228 \\ \end{tabular} \\ \hline Hyperboloid SVM (D = 2) & 0.53 \(\pm\) 0.07 & 0.52 \(\pm\) 0.01 & 0.54 \(\pm\) 0.04 & 0.39 \(\pm\) 0.03 \\ \hline Hyperbolic LR (D = 2) & 0.46 \(\pm\) 0.08 & 0.52 \(\pm\) 0.04 & 0.54 \(\pm\) 0.07 & 0.32 \(\pm\) 0.10 \\ Hyperbolic LR (D = 5) & 0.95 \(\pm\) 0.03 & 0.76 \(\pm\) 0.07 & 0.80 \(\pm\) 0.08 & 0.78 \(\pm\) 0.04 \\ Hyperbolic LR (D = 10) & 0.96 \(\pm\) 0.01 & 0.86 \(\pm\) 0.05 & 0.84 \(\pm\) 0.04 & 0.94 \(\pm\) 0.04 \\ \hline Euclidean SVM (D = 2) & 0.39 \(\pm\) 0.01 & 0.39 \(\pm\) 0.00 & 0.32 \(\pm\) 0.02 & 0.20 \(\pm\) 0.01 \\ Euclidean SVM (D = 5) & 0.95 \(\pm\) 0.00 & 0.79 \(\pm\) 0.01 & 0.38 \(\pm\) 0.02 & 0.44 \(\pm\) 0.01 \\ Euclidean SVM (D = 10) & 0.97 \(\pm\) 0.00 & 0.91 \(\pm\) 0.00 & 0.46 \(\pm\) 0.04 & 0.72 \(\pm\) 0.05 \\ \hline HoroSVM (D = 2) & **0.57 \(\pm\) 0.07** & **0.65 \(\pm\) 0.01** & **0.62 \(\pm\) 0.01** & **0.42 \(\pm\) 0.01** \\ HoroSVM (D = 5) & 0.93 \(\pm\) 0.01 & 0.88 \(\pm\) 0.00 & 0.82 \(\pm\) 0.04 & 0.88 \(\pm\) 0.01 \\ HoroSVM (D = 10) & 0.95 \(\pm\) 0.02 & 0.91 \(\pm\) 0.01 & 0.86 \(\pm\) 0.01 & 0.93 \(\pm\) 0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 2: F1 scores for subtree classification on four subtrees of WordNet. Boldface indicates the best performance on 2D embeddings of each dataset.

tangent space that separates the lifted (mapped) data. In addition, the learning of the reference point is only applicable to 2D hyperbolic space.

As well known in the Euclidean SVM literature, a vanilla (unweighted) implementation of SVM performs poorly on extremely imbalanced data, we observed the same behavior in training HoroSVM on this task. We preprocessed the data by downsampling the majority class of samples (the negative samples) to train a robust model. Note that our HoroSVM can be naturally extended to a class/instance-weighted version by assigning a class/instance weight to the penalty term \(C\) for each sample, allowing us to address more general imbalanced data. Since there is no protocol for dealing with unbalanced data in training a hyperboloid SVM, we presented the Euclidean SVM results using the same preprocessed data for reference. In addition, we presented results of hyperbolic logistic regression (LR) [15], which is not a large-margin classifier on this task, where the imbalanced data is handled by sampling the equal number of negative and positive nodes in each mini-batch of size 16 during training.

Now, we highlight several results in Table 2. The superior performance of hyperboloid SVM over hyperbolic LR is expected, as both methods use geodesic decision boundaries but hyperboloid SVM aims to maximize the margin. However, the training of hyperboloid SVM is highly unstable as we mentioned earlier due to the non-convex optimization process. Euclidean SVM under-performs as it does not take into account the hyperbolic geometry. Our HoroSVM exhibits a significant improvement in predicting words in a subtree, as evidenced by higher F1 scores across all the subtrees. The small number of nodes within a subtree, compared to the whole WordNet, causes the nodes to cluster near the boundary in their hyperbolic embeddings. Thus, horosphere is an ideally suited decision boundary (in comparison to the geodesic boundary in [12]) to isolate the subtree.

### Synthetic Data with Noisy Labels

To demonstrate the robustness of our HoroSVM, we apply it to synthetic data with noisy labels at varying levels/amounts of noise. Specifically, we generated 100 synthetic datasets by sampling from a Gaussian mixture model defined on the Poincare disk model as in [12]. The isotropic Gaussian distribution in hyperbolic space is referred to as the _Riemannian normal_ distribution, and we used the sampling method presented in [21]. For each dataset, we sampled two centroids from a zero-mean Riemannian normal distribution with a variance of 1.5. We then sampled 200 data points from a unit-variate Riemannian normal distribution centered at each centroid, resulting in a dataset of 400 points classified into positive and negative classes. We split the dataset into training and test sets with 100 positive/negative samples in the training set and 100 positive/negative samples in the test set. We then generated datasets with noisy labels at noise levels: \(\eta\in\{0,0.05,0.1,\dots,0.5\}\) by flipping the labels of a proportion \(\eta\) of the training (not test) samples, with an equal number of positive and negative samples flipped. The train/test average F1 scores of each method across all datasets at varying noise levels are shown in Fig 4. We compared our HoroSVM with hyperboloid SVM, hyperbolic LR, and a two-layer HNN [15] (with a hidden dimension of 5). While all methods depict decreasing training F1 scores as the noise level increases, HoroSVM outperforms the others consistently throughout the training process. Hyperbolic LR and HNN exhibit the least resistance to label noise, with test F1 scores dropping (faster) with increasing noise level. Both hyperboloid SVM and HoroSVM demonstrate consistent performance across different noise levels, owing to the inherent robustness of large-margin classifiers. However, the training of hyperboloid SVM is highly unstable resulting in its inferior performance. HoroSVM demonstrates its superiority in accuracy and robustness as evidenced in the results. In addition, we present the average training times for each method on one dataset (200 samples) as follows: 6.57 seconds (Hyperboloid SVM), 3.98 seconds (Hyperbolic LR), 9.06 seconds (HNN), and 3.73 seconds (HoroSVM). Notably, HoroSVM stands out as the fastest.

Figure 4: Training (left) and test (right) F1 scores of several methods on synthetic data with noisy labels at different noise levels.

Discussion and Conclusions

In this paper, we presented a novel large margin classifier, dubbed HoroSVM, whose decision boundaries are horospheres that are the level sets of a Busemann function. We presented a novel formulation leading to the optimization of a geodesically convex loss performed using a Riemannian gradient-based method and guaranteeing a globally optimal solution. We demonstrated superior to competitive performance of the HoroSVM over SOTA large margin classifiers.

In Euclidean space, a kernel SVM is usually favored over the linear SVM due to its ability to cope with non-linearly separable data. In Hyperbolic space, the challenge lies in developing valid positive definite kernels (see [14] for details on validity of kernels on Riemannian manifolds). The only reported work on KSVM in hyperbolic space that we are aware of is [12], which uses a kernel that violates the positive definiteness property of RKHS kernels. Thus the problem of interest is primarily defining a valid family of kernels in hyperbolic space. We will address this in our future work.

## Acknowledgements

This research was in part funded by the NSF grant IIS 1724174 and the NIH NINDS and NIA grant RF1NS121099 to Vemuri.

## References

* [1] P. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. In _Optimization Algorithms on Matrix Manifolds_. Princeton University Press, 2009.
* [2] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided they blog. In _Proceedings of the 3rd international workshop on Link discovery_, pages 36-43, 2005.
* [3] Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
* [4] Roberto Bonola. _Non-Euclidean geometry: A critical and historical study of its development_. Courier Corporation, 1955.
* [5] Nicolas Boumal. An introduction to optimization on smooth manifolds. _Available online, May_, 3, 2020.
* [6] Martin R Bridson and Andre Haefliger. _Metric spaces of non-positive curvature_, volume 319. Springer Science & Business Media, 2013.
* [7] James W. Cannon, William J. Floyd, Richard Kenyon, and Walter R. Parry. _Hyperbolic Geometry_, volume 31. MSRI Publications, 1997.
* [8] Benjamin Paul Chamberlain, James Clough, and Marc Peter Deisenroth. Neural embeddings of graphs in hyperbolic space. _arXiv preprint arXiv:1705.10359_, 2017.
* [9] Ines Chami, Albert Gu, Dat P Nguyen, and Christopher Re. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In _International Conference on Machine Learning_, pages 1419-1429. PMLR, 2021.
* [10] Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional neural networks. _Advances in Neural Information Processing Systems_, 32:4868-4879, 2019.
* [11] Eli Chien, Chao Pan, Puoya Tabaghi, and Olgica Milenkovic. Highly scalable and provably accurate classification in poincare balls. In _2021 IEEE International Conference on Data Mining (ICDM)_, pages 61-70. IEEE, 2021.
* [12] Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic space. In _The 22nd international conference on artificial intelligence and statistics_, pages 1832-1840. PMLR, 2019.
* [13] Xiran Fan, Chun-Hao Yang, and Baba C Vemuri. Nested hyperbolic spaces for dimensionality reduction and hyperbolic nn design. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 356-365, 2022.

* [14] Aasa Feragen, Francois Lauze, and Soren Hauberg. Geodesic exponential kernels: When curvature and linearity conflict. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3032-3042, 2015.
* [15] Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In _International Conference on Machine Learning_, pages 1646-1655. PMLR, 2018.
* [16] Octavian-Eugen Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. _Advances in Neural Information Processing Systems 31_, pages 5345-5355, 2019.
* [17] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. _Proceedings of the national academy of sciences_, 99(12):7821-7826, 2002.
* [18] Sigurdur Helgason. _Groups and geometric analysis: integral geometry, invariant differential operators, and spherical functions_, volume 83. American Mathematical Society, 2022.
* [19] Niklas Koep and Sebastian Weichwald. Pymanopt: A python toolbox for optimization on manifolds using automatic differentiation. _Journal of Machine Learning Research_, 17:1-5, 2016.
* [20] Gian Marconi, Carlo Ciliberto, and Lorenzo Rosasco. Hyperbolic manifold regression. In _International Conference on Artificial Intelligence and Statistics_, pages 2570-2580. PMLR, 2020.
* [21] Emile Mathieu. Charline le lan, chris j maddison, ryota tomioka, and yee whey teh. continuous hierarchical representations with poincare variational auto-encoders. _Advances in Neural Information Processing Systems_, pages 12544-12555, 2019.
* [22] Nicholas Monath, Manzil Zaheer, Daniel Silva, Andrew McCallum, and Amr Ahmed. Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 714-722, 2019.
* [23] Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. _Advances in neural information processing systems_, 30:6338-6347, 2017.
* [24] Frederic Sala, Chris De Sa, Albert Gu, and Christopher Re. Representation tradeoffs for hyperbolic embeddings. In _International conference on machine learning_, pages 4460-4469. PMLR, 2018.
* [25] Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In _International Symposium on Graph Drawing_, pages 355-366. Springer, 2011.
* [26] Hiroyuki Sato. Riemannian conjugate gradient methods: General framework and specific algorithms with convergence analyses. _SIAM Journal on Optimization_, 32(4):2690-2717, 2022.
* [27] Ryohei Shimizu, YUSUKE Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. In _International Conference on Learning Representations_, 2020.
* [28] Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda. Fully-connected network on noncompact symmetric space and ridgelet transform based on helgason-fourier analysis. In _International Conference on Machine Learning_, pages 20405-20422. PMLR, 2022.
* [29] Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word embeddings. In _International Conference on Learning Representations_, 2018.
* [30] Constantin Udriste. _Convex functions and optimization methods on Riemannian manifolds_, volume 297. Springer Science & Business Media, 2013.
* [31] Ming-Xi Wang. Laplacian eigenspaces, horocycles and neuron models on hyperbolic spaces. 2021.
* [32] Melanie Weber, Manzil Zaheer, Ankit Singh Rawat, Aditya K Menon, and Sanjiv Kumar. Robust large-margin learning in hyperbolic space. _Advances in Neural Information Processing Systems_, 33:17863-17873, 2020.
* [33] Tao Yu and Christopher De Sa. Hyla: Hyperbolic laplacian features for graph learning. _arXiv preprint arXiv:2202.06854_, 2022.
* [34] Wayne W Zachary. An information flow model for conflict and fission in small groups. _Journal of anthropological research_, 33(4):452-473, 1977.
* [35] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In _Conference on Learning Theory_, pages 1617-1638. PMLR, 2016.