# No "Zero-Shot" Without Exponential Data:

Pretraining Concept Frequency Determines

Multimodal Model Performance

Vishaal Udandarao\({}^{*}\)\({}^{1,2}\)  Ameya Prabhu\({}^{*}\)\({}^{1,3}\)  Adhiraj Ghosh\({}^{1}\)  Yash Sharma\({}^{1}\)

Philip H.S. Torr\({}^{3}\)  Adel Bibi\({}^{3}\)  Samuel Albanie\({}^{2}\)\({}^{\dagger}\)  Matthias Bethge\({}^{1}\)\({}^{\dagger}\)

\({}^{1}\)Tubingen AI Center, University of Tubingen \({}^{2}\)University of Cambridge \({}^{3}\)University of Oxford

###### Abstract

Web-crawled datasets underlie the impressive "zero-shot" performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" _generalization_ is for such models because the extent to which their pretraining datasets encompass downstream concepts used in "zero-shot" evaluation is unknown. In this work, we ask: _How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?_ We comprehensively investigate this question across 34 models and 5 standard pretraining datasets, generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and evaluation datasets [81], and testing on purely synthetic data distributions [52]. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test dataset as the _Let it Wag!_ benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training data and compute paradigms remains to be found.

## 1 Introduction

Multimodal models like CLIP [98] and Stable Diffusion [104] have revolutionized performance on downstream tasks. CLIP is now the _de facto_ standard for "zero-shot" image recognition [143; 74; 136; 49; 142] and image-text retrieval [47; 64; 25; 127; 139], while Stable Diffusion is now the _de facto_ standard for "zero-shot" text-to-image (T2I) generation [100; 18; 104; 42]. In this work, we investigate this empirical success through the lens of zero-shot generalization [70], which refers to the ability of models to apply their learned knowledge to new unseen concepts (not seen during training). Accordingly, we ask: _Are current multimodal models truly capable of "zero-shot" generalization?_

To tackle this question, we conduct a comparative analysis involving two main factors: (1) the performance of models across various downstream tasks, and (2) the frequency of test concepts within their pretraining datasets. We compile a comprehensive list of \(4,029\) concepts2 from 27 downstreamtasks spanning classification, retrieval, and image generation, assessing model performance against these concepts. Our analysis spanned five large-scale image-text pretraining datasets with different scales, data curation methods and sources (CC-3M [115], CC-12M [28], YFCC-15M [123], LAION-Aesthetics [111], LAION-400M [110]), and evaluated the performance of 10 CLIP models and 24 T2I models, spanning different architectures and parameter scales. We consistently find across our experiments that, across concepts, the frequency of a concept in the pretraining dataset is _a strong predictor_ of the model's performance on test examples containing that concept (see Fig. 2). Notably, _model performance scales linearly as the concept frequency in pretraining data grows exponentially i.e._, we observe a consistent log-linear scaling trend. We find that this log-linear trend is robust to controlling for correlated factors (similar samples in pretraining and test data [81]) and testing across different concept distributions along with samples generated entirely synthetically [52].

Our findings indicate that the impressive empirical performance of multimodal models like CLIP and Stable Diffusion can be largely attributed to the presence of test concepts within their vast pretraining datasets, thus their reported empirical performance does not constitute "zero-shot" generalization. Quite the contrary, these models require exponentially more data on a concept to linearly improve their performance on tasks pertaining to that concept, suggesting significant sample inefficiency.

We additionally document the distribution of concepts encountered in pretraining data and find that:

* **Concept Distribution:** Across all pretraining datasets, the distribution of concepts is long-tailed (see Fig. 5), indicating that a large fraction of concepts are rare. Given the extreme sample inefficiency observed, these rare concepts are not properly learned during pretraining.
* **Concept Correlation across Pretraining Datasets:** The distributions of concepts across different pretraining datasets are strongly correlated (see Tab. 4), suggesting that web crawls yield surprisingly similar concept distributions across very diverse data curation strategies. This necessitates explicit concept rebalancing efforts explored in prior work [11; 135].
* **Image-Text Misalignment in Pretraining Data:** Concepts often appear in one modality but not the other, implying significant misalignment (see Tab. 3). Our released data artifacts can help image-text alignment efforts at scale by precisely indicating examples where modalities misalign. Note that the log-linear trend across both modalities is robust to this misalignment.

To provide a simple benchmark for multimodal generalization that controls for concept frequency in the pretraining set, we introduce a new long-tailed test set, "_Let It Wag:_". Current models trained on both openly available datasets (_e.g._, LAION-2B [111], DataComp-1B [47]) and closed-source datasets (_e.g._, OpenAI-WIT [98], WebLI [30]) have significant drops in performance (see Fig. 6), suggesting that our findings may also transfer to closed-source datasets. We publicly release all data artifacts, amortising the cost of analyzing image-text pretraining datasets for future efforts focused on a more data-centric understanding of the properties of multimodal models.

**Situating our Contributions in Broader Literature.** Our comprehensive analysis of several image-text datasets significantly adds to prior investigations on the role of pretraining data in affecting performance for both CLIP [92; 81; 43] and language models [62; 102; 82], by (1) showing that concept frequency determines zero-shot performance, and (2) pinpointing the exponential need for training data as a fundamental issue for current multimodal foundation models. We conclude that the key to "zero-shot" generalization under large-scale training paradigms remains to be found.

## 2 Concepts in Pretraining Data and Quantifying Frequency

In this section, we discuss how to estimate concept frequencies within pretraining datasets. We first define our concepts of interest, then describe algorithms for extracting their individual frequencies from images and text captions of pretraining datasets independently, and describe how we aggregate them to compute matched image-text concept frequencies. For a schematic overview, see Fig. 1.

**Defining Concepts.** We define "concepts" as the specific objects/relations we seek to analyze in pretraining datasets. Since our goal is to analyze downstream performance of models, we source concepts from 27 target evaluation datasets. For zero-shot classification datasets, extracted concepts are class names, such as the \(1,000\) object classes in ImageNet [36] (_e.g._, touch, goldfish). We also include relational verbs and verb-noun combinations since they are the classes of the UCF101 dataset [116] (_e.g._, diving, brushing teeth) as well as background nouns from the SUN397 dataset [133] (_e.g._, abbey, sky). For retrieval and image generation datasets, concepts are all nouns in test set captions or generation prompts. For example, from, "A man is wearing a hat", we extract "man" and "hat" as concepts. We filter out ambiguous or irrelevant concepts that are present in less than five downstream evaluation samples. In sum, we collate \(4,029\) concepts sourced from \(17\) classification, \(2\) image-text retrieval, and \(8\) text-to-image generation downstream datasets (see Tab. 1 and Sec. 3.1 for details).

**Concept Frequency from Captions.** For efficient concept searches, we pre-index all captions from the pretraining datasets, _i.e._, construct a mapping from concepts to captions. We first use part-of-speech tagging to isolate common and proper nouns, and lemmatize them with SpaCy [58] (lemmatization helps standardize verbs, enabling the estimation of their frequencies too [65]). These lemmatized terms are then cataloged in inverted unigram dictionaries, mapping each term to all sample indices in the pretraining dataset containing that term. To determine the frequency of a concept, we examine the concept's unigrams within these dictionaries. For multi-word concepts, we split them into their constituent unigrams, and then independently search for all unigrams before intersecting their hit lists to get a list of matched sample indices. The frequency of the concept in text captions is the count of these intersecting sample indices. This algorithm hence allows scalable \(\mathcal{O}(1)\) search with respect to the number of captions for any concept in pretraining dataset captions.

**Concept Frequency from Images.** Unlike text captions, we do not have a finite vocabulary for pre-indexing pretraining images. Instead, we collect all the \(4,029\) downstream concepts and verify their presence in images using a pretrained image tagging model. We tested various open-vocabulary object detectors, multi-tagging models and image-text matching models, for this concept tagging task. We found that RAM++ [59]--an open-set tagging model that tags images based on a predefined list of concept descriptions, in a multi-label manner--performs the best. We automatically consider the relationship between concepts (like synonyms) and concept hierarchies [84], since RAM++ uses descriptions generated by a language model (Appx. I.3) for each concept, to tag each image with certain concepts. This approach generates a list of pretraining images, each tagged with whether the downstream concepts are present or not, from which we can compute concept frequencies (Appx. I).

**Image-Text Matched Concept Frequencies.** Finally, we combine the frequencies obtained from both text and image searches to compute _matched image-text frequencies_. This involves identifying pretraining samples where both the image and its associated caption correspond to the concept. By intersecting the lists from our image and text searches, we determine the count of samples that align in both modalities, offering a comprehensive view of concept representation in the pretraining datasets. This step is necessary as we observed significant image-text misalignment between concepts in pretraining datasets (see Tab. 3), hence captions may not reflect what is present in the image and vice-versa. This behaviour has also been alluded to in prior work investigating data curation strategies [78, 77, 134, 89]. We provide a more detailed analysis of image-text misalignment in Sec. 5.

## 3 Comparing Pretraining Frequency & "Zero-Shot" Performance

Equipped with frequency estimates for downstream concepts, we now establish the relationship between image-text-matched pretraining concept frequencies and zero-shot performance across classification, retrieval, and generation tasks. We first detail our setup and then discuss key results.

Figure 1: **Concept Extraction and Frequency Estimation. (_left_) We compile \(4,029\) concepts from \(27\) evaluation datasets. (_right_) We construct efficient indices for text-search (unigram indexing (1)) and image-search (RAM++ (2)); intersecting hits from both gives (3) image-text matched frequencies.**

### Experimental Setup

We analyze two classes of multimodal models: Image-Text and Text-to-Image. For both, we detail the pretraining and testing datasets, along with their associated evaluation parameters.

#### 3.1.1 Image-Text (CLIP) Models

**Datasets.** We use 4 pretraining, 2 downstream retrieval, and 17 downstream classification datasets, covering a broad spectrum of objects, scenes, camera-types, and fine-grained distinctions (see Tab. 1).

**Note on Pretraining Dataset Diversity.** Each analyzed pretraining dataset significantly differs in data collection, filtering, and cleaning operations. CC-3M [115], originally intended to be used for training image captioning models, explicitly has no real-world entities or proper nouns present, and is cleaned only to have common nouns in its captions. CC-12M [28] and YFCC-15M [123], collected from Flickr, have user-provided metadata. Finally, LAION-400M [110] and LAION-Aesthetics [111] contain raw images downloaded from Common-Crawl [101] with alt-texts as the captions, which can be inherently very noisy as they are uploaded by non-expert humans as a placeholder for images.

**Models.** We test CLIP [98] models with both ResNet [54] and Vision Transformer [37] architecture, with ViT-B-16 [87] and RN50 [49, 88] trained on CC-3M and CC-12M, ViT-B-16, RN50, and RN101 [61] trained on YFCC-15M, and ViT-B-16, ViT-B-32, and ViT-L-14 trained on LAION400M [110]. We follow open_clip[61], slip[87] and cyclip[49] for our implementation.

**Prompting.** For zero-shot classification, we experiment with three prompting strategies: {classname} only, "A photo of a {classname}" and prompt-ensembles [98], which averages over \(80\) different prompt variations of {classname}. For retrieval, we use the image or the caption as input corresponding to I2T (image-to-text) or T2I (text-to-image) retrieval respectively.

**Metrics.** We compute mean accuracy for classification tasks [98]. For retrieval, we measure Recall@1, Recall@5, and Recall@10 for both text-to-image and image-to-text retrieval tasks [98].

#### 3.1.2 Text-to-Image Models

**Datasets.** Our pretraining dataset is LAION-Aesthetics [111], with downstream evaluations done on subsampled versions of eight datasets: CUB200 [131], Daily-DALLE [34], Detection [31], Parti-Prompts [140], DrawBench [106], COCO-Base [75], Relational Understanding [33] and Winoground [124]. Please refer to HEIM [72] for more details on the evaluation datasets.

**Models.** We evaluate 24 T2I models, detailed in Tab. 2. Their sizes range from 0.4B parameters (DeepFloyd-IF-M [9] and DALL-E Mini [35]) to 4.3B parameters (DeepFloyd-IF-XL [9]). We include various Stable Diffusion models [104] as well as variants tuned for specific visual styles [6, 4, 5].

**Prompting.** Text prompts from the evaluation datasets are used directly to generate images, with 4 image samples generated for each prompt.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Dataset Type** & \multicolumn{3}{c}{**Datasets**} \\ \hline
**Pretraining** & CC-3M [115] & CC-12M [28] & YFCC-15M [123] & LAION-400M [110] \\ \hline \multirow{3}{*}{**Classification-Eval**} & ImageNet [36] & SUN397 [133] & UCF101 [116] & Catelton101 [45] & EuroSAT [56] \\  & Caltech256 [50] & Fluores102 [90] & DDTD [32] & Birdsample [16] & Food101 [21] & Stanford-Cars [66] \\  & FGVCAircraft [79] & Oxford-Pets [93] & Country211 [98] & CIFAR-10 [68] & CIFAR100 [68] \\ \hline
**Retrieval-Eval** & \multicolumn{3}{c}{Flickr-1K [138]} & COCO-5K [75] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Pretraining and downstream datasets used in Image-Text (CLIP) experiments.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Category** & \multicolumn{3}{c}{**Models**} \\ \hline \multirow{4}{*}{**Models**} & M-Vader [15] & DeepFloyd-IF-M [9] & DeepFloyd-IF-L-[9] & DeepFloyd-IF-XL [9] \\  & GigGAN [63] & DALL-E Mini [35] & DALL-E Mega [35] & Promptist+SD-v1.4 [53] \\ \cline{1-1}  & Dreamlike-Diffusion-v1.0 [2] & Dreamlike Photorealv2.0 [3] & OpenJourney-v1 [4] & OpenJourney-v2 [5] \\ \cline{1-1}  & SD-Safe-Max [104] & SD-Safe-Medium [104] & SD-Safe-Strong [104] & SD-Safe-Weak [104] \\ \cline{1-1}  & SD-v1.4 [104] & SD-v1.5 [104] & SD-v2-Base [104] & SD-v2-1-base [104] \\ \cline{1-1}  & Vinetdois-Diffusion-v0.1 [7] & minDALLE [105] & Lexica-SD-v1.5 [1] & Redshift-Diffusion [6] \\ \hline \hline \end{tabular}
\end{table}
Table 2: Models used in text-to-image (T2I) experiments.

**Metrics.** Evaluation consists of image-text alignment and aesthetic scores. For automated metrics [72], we use expected and max CLIP-score [57] to measure image-text alignment along with expected and max aesthetics-score [110] to measure aesthetics. To verify reliability of automated metrics, we compare them with human-rated scores (measured on a 5-point grading scale) for both image-text alignment and aesthetics [72]. To supplement the human-rated scores provided by HEIM [72], we confirm our findings by performing our own small-scale human evaluation (Appx. C).

### Result: Pretraining Concept Frequency is Predictive of "Zero-Shot" Performance

We now probe the impact of pretraining concept frequency on "zero-shot" performance of models. Our main results, across various tasks and model types, are shown in Figs. 2 and 3.

**Understanding the Plots.** The plots in the main paper present text-image (CLIP) models' zero-shot classification results using accuracy and text-to-image retrieval performance using Recall@10. Similarly, we present T2I generative models' performance on image generation tasks using the expected aesthetics score. For the other aforementioned metrics for retrieval as well as other automated generation metrics along with human-rated scores, we find that they show similar trends,

Figure 3: **Log-linear relationships between concept frequency and T2I aesthetic scores. Across all tested T2I models pretrained on LAION-Aesthetics, we observe a consistent linear relationship between aesthetic score (averaged across 8 datasets) on a concept and the log-scaled concept frequency.**

Figure 2: **Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP’s zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (\(p<0.05\) with a two-tailed t-test [118]), and thus we show Pearson correlation (\(\rho\)) [73] as well.**and we provide them for reference in Apps. B and C. For clarity, the data presentation is simplified from scatter plots to a cohesive line, similar to work from Kandpal et al. [62] and Razeghi et al. [102]. The x-axis is log-scaled, and performance metrics are averaged within bins along this axis for ease-of-visualization of the log-linear correlation. We removed bins containing very few concepts per bin by standard IQR removal [132] following Kandpal et al. [62]. We additionally compute Pearson correlation \(\rho\)[73] for each line and provide significance results based on a two-tailed t-test [118].

_Key Finding: Log-linear scaling between concept frequency and zero-shot performance._ Across all the 16 different plots, we observe a clear log-linear relationship between pretraining concept frequency and zero-shot performance. These plots vary in (i) model type (discriminative vs. generative), (ii) task (classification vs. retrieval), (iii) model architecture and parameter count, (iv) pretraining dataset (curation methods and scales), (v) evaluation metrics, (vi) prompting strategies, and (vii) concept frequencies isolated only from image or text caption (additional experiments for (v) are presented in Apps. B and C, for (vi) are presented in Appx. A, and for (vii) are presented in Appx. D). The observed log-linear scaling trend persists _across all seven presented dimensions_. In some plots, we notice a slight dip at the high-frequency concepts--we analyse this in greater detail in Appx. L. Thus, taken together, our results reveal data-hungry learning, _i.e_, a lack in current multimodal models' ability to learn concepts from pretraining datasets in a sample-efficient manner.

## 4 Stress-Testing Frequency-Performance Trends with Distributional Controls

In this section, we perform two control experiments to account for different confounding distributional factors of pretraining datasets, to ensure the robustness of our log-linear frequency-performance scaling trends: (1) we control for sample-level similarity in distribution between pretraining and evaluation datasets [137; 81], and (2) we investigate effects of pretraining data with radically different controlled concept distributions, with entirely synthetically-generated image-text pairs [52].

### Controlling for Similar Samples in Pretraining and Downstream Data

**Motivation.** Prior work has suggested that sample-level similarity between pretraining and downstream datasets impacts model performance [62; 81; 137; 102]. This leaves open the possibility that our frequency-performance results are simply an artifact of this factor, _i.e_., as concept frequency increases, it is likely that the pretraining dataset also contains more similar samples to the test sets. We hence investigate the frequency-performance trends when controlling for sample-level similarity.

**Setup.** We use the LAION-200M [10] dataset for this experiment. We first verify that a CLIP-ViT-B-32 model pretrained on the LAION-200M dataset (used to study sample similarity in prior work [81]) exhibits a similar log-linear trend between concept frequency and zero-shot performance. Then, we use the near_pruning method from Mayilvahanan et al. [81] to eliminate 50 million samples most similar to the test sets from the pretraining LAION-200M dataset. We provide details for this in Appx. G.1. This procedure removes the most similar samples between pretraining and test sets. We verify that this procedure influences the performance of the model drastically across our aggregate classification and retrieval tasks respectively, replicating the findings of Mayilvahanan et al. [81].

**Key Finding: _Concept Frequency is still Predictive of Performance._** We repeat our analysis on models trained with this controlled pretraining dataset with 150M samples, and report results on the

Figure 4: **Stress-testing the log-linear scaling trends. We provide further evidence for the log-linear relationship between performance and concept frequency, across different scenarios: (_left_) we control for “similarity” between downstream test sets and pretraining datasets, and (_right_) we conduct experiments on an entirely synthetic pretraining distribution with no real-world images or captions.**

same downstream classification and retrieval datasets, in Fig. 4 (left). Despite removing the most similar samples between pretraining and test sets, we still consistently observe a clear log-linear relationship between pretraining frequency of test set concepts and zero-shot model performance.

**Conclusion.** This analysis reaffirms that, despite removing pretraining samples closely related to the downstream evaluation datasets, the log-linear relationship between concept frequency and zero-shot performance persists. Note that this is despite substantial decreases in absolute performance, highlighting the robustness of concept frequency as a performance indicator for CLIP models.

### Testing Generalization to Purely Synthetic Concept and Data Distributions

**Motivation.** Sampling across real-world data might not result in significant differences in concept distribution, as we will later show in Sec. 5. Hence, we repeat our analysis on a synthetic dataset designed with an explicitly different concept distribution [52]. This evaluation aims to understand if pretraining concept frequency remains a significant performance predictor within a synthetic concept distribution, generalizing even for models pretrained on entirely synthetic images and text captions.

**Setup.** The SynthCI-30M dataset [52] introduces a novel concept distribution, generating 30 million synthetic image-text pairs. Utilizing their publicly available data and models, we explore the relationship between concept frequency and model performance in this purely synthetic data regime.

**Key Finding:**_Concept Frequency is still Predictive of Performance._ We report results for models pretrained with the controlled SynthCI-30M dataset in Fig. 4 (right). We still consistently observe a clear log-linear relationship between concept frequency and zero-shot model performance.

**Conclusion.** This consistency highlights that concept frequency is a robust indicator of model performance, extending even to entirely synthetic datasets and pretraining concept distributions.

## 5 Additional Insights from Pretraining Concept Frequencies

**Finding 1:**_Pretraining Datasets Exhibit Long-tailed Concept Distributions._ Our analysis in Fig. 5 reveals an extremely long-tailed distribution of concept frequencies in pretraining datasets, with over two-thirds of concepts occurring at almost negligible frequencies relative to the size of the datasets (we highlight the "head" part of this distribution in boxes). Our observations extend the findings of past work that have noted the long-tailed distribution of large-scale language datasets [14; 26; 94; 146]. As we observed with the log-linear trend, this distribution directly reflects disparities in performance.

**Finding 2:**_Misalignment Between Concepts in Image-Text Pairs._ Our concept frequency estimation pipeline enables us to investigate the alignment of concepts within paired pretraining image-text data. Perfect image-text alignment is defined as every image-text pair containing the same concepts. Previous studies have qualitatively discussed the problem of misalignment in large image-text datasets [77; 134; 78]. Our analysis enables us to quantify this _misalignment degree_--for each image-text pair in the pretraining dataset, we find concepts that are matched to the image and the text caption independently. If there are no intersecting concepts from the independent image and text hits, we mark that pair as misaligned (detailed algorithm shown in Appx. K). Tab. 3 shows the high degree of misalignment in all image-text pairs (\(5-36\%\)). To the best of our knowledge, this is the first

Figure 5: **Concept distribution of pre-training datasets is highly long-tailed. We showcase the distribution of pretraining frequencies of all concepts aggregated across all 17 of our downstream classification datasets. Across all the pretraining datasets, we observe very heavy tails. We normalize the concept frequencies and remove concepts with 0 counts for improved readability of the plots.**

attempt to quantify the misalignment degree in pretraining image-text datasets explicitly. We release the precise misaligned image-text samples from pretraining datasets to enable better data curation.

**Finding 3: _Concept Frequencies Across Datasets are Correlated._** Despite vast differences in the size (3M-400M samples) and curation strategies of the pretraining datasets, we discovered a surprisingly high correlation in concept frequencies across them (see Tab. 4). This suggests that the internet, as the common source of these datasets, naturally exhibits a long-tailed distribution, influencing any dataset derived from it to display similar long-tailed behavior also. This result inspired "_Let It Wag!"_.

## 6 Testing the Tail: _Let It Wag!_

**Motivation.** In the previous sections, we identified a consistent long-tailed concept distribution across pretraining datasets, highlighting the scarcity of certain concepts on the web. This observation forms the basis of our hypothesis that models likely underperform when tested against data distributions that are heavily long-tailed. To test this, we carefully curate 290 concepts identified as the least frequent across all pretraining datasets. This includes concepts like eggnog, wormsnake, and tropical kingbird. We then use these concepts to create an evaluation dataset, "_Let It Wag!"_.

**Dataset Details.** The "_Let It Wag!"_ classification dataset comprises 130K test samples downloaded from the web using the method of Prabhu et al. [96]. The test samples are evenly distributed across 290 categories of long-tailed concepts. From the list of curated concepts, we download test set images, deduplicate them, remove outliers, and finally manually clean and hand-verify the class labels.

**Analysis Details.** We run both classification and image generation experiments on "_Let It Wag!"_. For classification, we evaluate 40 text-image (CLIP) models on the _"Let It Wag!"_ classification dataset, using an ensemble of 80 prompts from Radford et al. [98]. For the generation task, we utilize SD-XL [95], SD-v2 [104], and Dreamlike-Photoreal-v2.0 [3], to generate images for the long-tailed concepts. For each model, we run \(50\) diffusion steps, maintaining default settings for all other parameters.

**Text-Image Classification Results.** We showcase the results of our long-tailed classification task in Fig. 6--we plot results of all models on both "_Let It Wag!"_ (y-axis) and ImageNet (x-axis). We observe that all models underperform by large margins on the long-tailed "_Let It Wag!"_ dataset (upto 20% lower absolute accuracies compared to ImageNet). This performance drop-off generalises across all model scales and 10 different pretraining data distributions, reinforcing the notion that all web-sourced pretraining datasets are inherently constrained to be long-tailed. With that said, note that the higher capacity models (fitted line with slope=1.58 in Fig. 6) seem to be closing the gap to ImageNet performance, indicating improved performance on the long-tailed concepts.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Correlations** & **CC-3M** & **CC-12M** & **YFCC-15M** & **LAION-400M** \\ \hline
**CC-3M** & 1.00 & 0.79 & 0.96 & 0.63 \\
**CC-12M** & – & 1.00 & 0.97 & 0.74 \\
**YFCC-15M** & – & – & 1.00 & 0.76 \\
**LAION-400M** & – & – & – & 1.00 \\ \hline \hline \end{tabular}
\end{table}
Table 4: We compute correlation in concept frequency across pretraining datasets, observing strong correlations, despite major differences in scale and curation.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Dataset/** & **Number of** & **Misalignment** \\
**Misalignment** & **Misaligned pairs** & **Degree (\%)** \\ \hline
**CC-3M** & 557,683 & 16.81\% \\
**CC-12M** & 2,143,784 & 17.25\% \\
**YFCC-15M** & 5,409,248 & 36.48\% \\
**LAION-A** & 23,104,076 & 14.34\% \\
**LAION-400M** & 21,996,097 & 5.31\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: For each pretraining dataset, we present the number of misaligned image-text pairs and the _misalignment degree_: fraction of misalignment pairs.

Figure 6: **Large-drops in accuracy on “_Let It Wag!"_. Across 40 tested CLIP models, we note large performance drops compared to ImageNet. Further, the performance gap seems to decrease for high-capacity models as demonstrated by larger positive slope (1.58) for those models.**

**T2I Generation Results.** We provide a qualitative analysis on image generation for assessing T2I models on the rare "_Let It Wag!_"concepts in Fig. 7. For enhancing image diversity, we generate prompts using Gemini [121] (top row of generated images) and GPT-4 [12] (bottom row of generated images). Green borders represent correct generations, red borders represent incorrect generations and yellow borders represent ambiguous generations. While descriptive prompting generally aids in improving the quality of generated images [53], we still observe T2I models failing to comprehend and accurately represent many concepts in our "_Let It Wag!_" dataset. Some failure cases involve misrepresenting activities (such as Pizza Tossing or Cricket Bowling as shown in Fig. 29), generating the wrong concept (Chuck-will's-widow as shown in Fig. 7 (top)), as well as not comprehending the concept at all (Ocarina in Fig. 7 (bottom)). We hence show that Stable Diffusion models are prone to the long tail qualitatively--we also provide quantitative results in Appx. N.1.

**Conclusion.** Across both the classification and generation experiments, we have showcased that current multimodal models predictably underperform, regardless of their model scale or pretraining datasets. This suggests a need for better strategies for sample-efficient learning on the long-tail.

## 7 Related Work

We discuss the most relevant prior works to ours here, and defer an extended literature review to Appx. F. Past works [98; 47; 88; 43; 76; 39; 82; 81] have highlighted the importance of pretraining data for improved downstream model performance. Fang et al. [43] demonstrated that pretraining data diversity is key to CLIP's strong out-of-distribution generalisation. Nguyen et al. [88] extended this analysis to show that differences in data distributions can change model performance, enabling effective data mixing strategies for pretraining. Mayilvahanan et al. [81] complemented these works by showing that CLIP's performance is correlated with the similarity between pretraining and test datasets. Our findings further pinpoint that the frequency of concept occurrences is a key indicator of performance. This complements existing work in areas like question-answering [62] and numerical reasoning [102] in LLMs. Concurrent to our work, Parashar et al. [92] also explore the problem of long-tailed concepts in LAION-2B and how it affects CLIP performance, supporting our findings. In contrast to their work, our demonstration that the long tail yields a log-linear trend, explicitly indicates exponential sample inefficiency in pretrained multimodal models. Additionally, contrary to their work, we index both image and text modalities, as well as span across several scales of diverse pretraining datasets. Our frequency estimation procedure on both texts and images independently, enables us to provide a more finer-grained analysis of pretraining datasets than previously studied in the literature, like (1) quantifying the misalignment between images and text captions, (2) assessing

Figure 7: **Qualitative results on “_Let It Wag!_” concepts demonstrate failure cases of T2I models on the long-tail_. We created 4 prompts for each concept using Gemini [121] and GPT-4 [12] which are fed to 3 Stable Diffusion [104] models. Generations with red border are incorrect, green border are correct and yellow border are ambiguous. Despite advances in high-fidelity image generation, there is large scope for improvement for such long-tail concepts (quantitative results in Appx. N.1).**the similarity of the different pretraining data concept distributions, and (3) doing a number of control experiments to thoroughly stress-test the robustness of our log-linear scaling results.

## 8 Conclusion

In this work, we studied 5 pretraining datasets of 34 multimodal models, analyzing the distribution and composition of concepts within them, generating over 300GB of data artifacts that we publicly release. Our findings reveal that across concepts, significant improvements in zero-shot performance require exponentially more data, following a sample-inefficient log-linear scaling trend. This pattern persists despite controlling for similarities between pretraining and downstream datasets or even when testing models on entirely synthetic data distributions. Further, all tested models consistently underperformed on the _"Let it Wag!"_ dataset, which we systematically constructed from our findings to test for long-tail concepts. This underlines a critical reassessment of what "zero-shot" generalization entails for multimodal models, highlighting the limitations in their current generalization capabilities.

## Acknowledgements

The authors would like to thank (in alphabetic order of first name): Gyungin Shin, Jonathan Roberts, Karsten Roth, Mehdi Cherti, Prasanna Mayilvahanan, Shyamgopal Karthik, Thao Nguyen and Vlad Bogolin for helpful feedback and providing access to various resources throughout the project. YS would like to thank Wieland Brendel, Nicholas Carlini, Daphne Ippolito, Katherine Lee, Matthew Jagielski, and Milad Nasr. AP is funded by Meta AI Grant No. DFR05540. VU and YS thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS). VU also thanks the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VU was supported by a Google PhD Fellowship in Machine Intelligence. PT thanks the Royal Academy of Engineering for their support. AB acknowledges the funding from the KAUST Office of Sponsored Research (OSR-CRG2021-4648) and the support from Google Cloud through the Google Gemma 2 Academic Program GCP Credit Award. SA is supported by a Newton Trust Grant. MB acknowledges financial support via the Open Philanthropy Foundation funded by the Good Ventures Foundation. This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP4, project number: 276693517 and the UKRI grant: Turing AI Fellowship EP/W002981/1. MB is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645.

## References

* [1] Lexica search with stable diffusion v1.5 (1b). https://lexica.art/?q=stable+diffusion+1.5.
* [2] Dreamlike diffusion v1.0. https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0,.
* [3] Dreamlike photoreal v2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0,.
* [4] Openjourney v1. https://huggingface.co/prompthero/openjourney,.
* [5] Openjourney v2. https://huggingface.co/prompthero/openjourney-v4,.
* [6] Redshift diffusion. https://huggingface.co/nitroosocke/redshift-diffusion.
* [7] Vintedois (22h) diffusion model v0.1. https://huggingface.co/22h/vintedois-diffusion-v0-1.
* [8] Human (q5). https://www.wikidata.org/wiki/Q5.
* [9] Deepfloyd if. https://github.com/deep-floyd/IF, 2023.
* [10] Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semedup: Data-efficient learning at web-scale through semantic deduplication. _Advances in Neural Information Processing Systems_, 2023.
* [11] Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and Ari S Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters. In _International Conference on Learning Representations (ICLR)_, 2024.

* [12] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [13] Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Gua. Tracing knowledge in language models back to the training data. _In Findings of the Association for Computational Linguistics: EMNLP_, 2022.
* [14] Stefan Baack and Mozilla Insights. Training data for the price of a sandwich1. 2024.
* [15] Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Bjorn Deiseroth, Constantin Eichenberg, Andrew M Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, et al. Multifusion: Fusing pre-trained models for multi-lingual, multi-modal image generation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [16] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* [17] Ian Berlot-Attwell, A Michael Carrell, Kumar Krishna Agrawal, Yash Sharma, and Naomi Saphra. Attribute diversity determines the systematicity gap in vqa. _arXiv preprint arXiv:2311.08695_, 2023.
* [18] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. In _Computer Science_, 2023.
* [19] Abeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In _2021 IEEE Winter Conference on Applications of Computer Vision (WACV)_, 2021.
* [20] Abeba Birhane, Sanghyun Han, Vishnu Boddeti, Sasha Luccioni, et al. Into the laion's den: Investigating hate in multimodal datasets. _Advances in Neural Information Processing Systems_, 2023.
* [21] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _European Conference on Computer Vision (ECCV)_, 2014.
* [22] Max F Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. Image retrieval outperforms diffusion models on data augmentation. _Transactions on Machine Learning Research (TMLR)_, 2023.
* [23] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In _The Eleventh International Conference on Learning Representations_, 2023.
* [24] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In _32nd USENIX Security Symposium (USENIX Security 23)_, pages 5253-5270, 2023.
* [25] Santiago Castro and Fabian Caba Heilbron. Fitclip: Refining large-scale pretrained image-text models for zero-shot video understanding tasks. _British Machine Vision Conference (BMVC)_, 2022.
* [26] Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [27] Kent Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman. Speak, memory: An archaeology of books known to chatgrpt/gpt-4. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 7312-7327, 2023.
* [28] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* [29] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegp4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [30] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Pallewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-scaled multilingual language-image model. In _The Eleventh International Conference on Learning Representations_, 2023.
* [31] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3043-3054, 2023.
* [32] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* [33] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. _arXiv preprint arXiv:2208.00005_, 2022.

* [34] dailydale2023. Instagram account of daily dall-e. https://www.instagram.com/dailydall.e/, 2024. Accessed: 2024-04-03.
* [35] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas, and Ribrata Ghosh. Dall-e mini, 7 2021. URL. https://github.com/borisdayma/dalle-mini.
* [36] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2009.
* [37] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [38] Ling Du, Anthony TS Ho, and Runmin Cong. Perceptual hashing for image authentication: A survey. _Signal Processing: Image Communication_, 2020.
* [39] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schutze, and Yoav Goldberg. Measuring causal effects of data statistics on language model'factual'predictions. _arXiv preprint arXiv:2207.14251_, 2022.
* [40] Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, and Jesse Dodge. What's in my big data? In _The Twelfth International Conference on Learning Representations_, 2024.
* [41] Rahim Entezari, Mitchell Wortsman, Olga Saukh, M Moein Shariatnia, Hanie Sedghi, and Ludwig Schmidt. The role of pre-training data in transfer learning. _arXiv preprint arXiv:2302.13602_, 2023.
* [42] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* [43] Alex Fang, Gabriel Illarco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In _International Conference on Machine Learning_, pages 6216-6234. PMLR, 2022.
* [44] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks, 2023.
* [45] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _Conference on Computer Vision and Pattern Recognition Workshop (CVPR-W)_, 2004.
* [46] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. _Cognition_, 28(1-2):3-71, 1988.
* [47] Samir Yitzhak Gadre, Gabriel Illarco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* [48] Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta Nakashima. Uncurated image-text datasets: Shedding light on demographic bias. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6957-6966, 2023.
* [49] Shashank Gool, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. Cyclip: Cyclic contrastive language-image pretraining. _Advances in Neural Information Processing Systems_, 35:6704-6719, 2022.
* [50] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
* [51] Dylan Jasper Hadfield-Menell. _The Principal-Agent Alignment Problem in Artificial Intelligence_. University of California, Berkeley, 2021.
* [52] Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem. Synthclip: Are we ready for a fully synthetic clip training? _arXiv preprint arXiv:2402.01832_, 2024.
* [53] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [55] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? In _The Eleventh International Conference on Learning Representations_, 2023.
* [56] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 12(7):2217-2226, 2019.

* [57] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7514-7528, 2021.
* [58] Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. _To appear_, 7(1):411-420, 2017.
* [59] Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. _arXiv e-prints_, pages arXiv-2310, 2023.
* [60] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? _Journal of Artificial Intelligence Research_, 67:757-795, 2020.
* [61] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.
* [62] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In _International Conference on Machine Learning (ICML)_, pages 15696-15707. PMLR, 2023.
* [63] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [64] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In _International Conference on Machine Learning (ICML)_, 2021.
* [65] Kimmo Koskenniemi. A general computational model for word-form recognition and production. In _10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics_. The Association for Computational Linguistics, 1984.
* [66] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _International Conference on Computer Vision Workshop (ICCV-W)_, 2013.
* [67] Kundan Krishna, Saurabh Garg, Jeffrey P Bigham, and Zachary C Lipton. Downstream datasets make surprisingly good pretraining corpora. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 12207-12222, 2023.
* [68] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [69] Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, et al. From scarcity to efficiency: Improving clip training via visual-enriched captions. _arXiv preprint arXiv:2310.07699_, 2023.
* [70] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. _IEEE transactions on pattern analysis and machine intelligence_, 36(3):453-465, 2013.
* [71] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8424-8445, 2022.
* [72] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [73] Joseph Lee Rodgers and W Alan Nicewander. Thirteen ways to look at the correlation coefficient. _The American Statistician_, 42(1):59-66, 1988.
* [74] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* [75] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision (ECCV)_, 2014.
* [76] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. _arXiv preprint arXiv:2305.13169_, 2023.
* [77] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and Ari Morcos. Sieve: Multimodal dataset pruning using image captioning models. _arXiv preprint arXiv:2310.02110_, 2023.

* [78] Pratyush Maini, Sachin Goyal, Zachary Chase Lipton, J Zico Kolter, and Aditi Raghunathan. T-MARS: Improving visual representations by circumventing text feature learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [79] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [80] Daniela Massicei, Camilla Longden, Agnieszka Slowik, Samuel Wills, Martin Grayson, and Cecily Morrison. Explaining clip's performance disparities on data from blind/low vision users. _arXiv preprint arXiv:2311.17315_, 2023.
* [81] Prasanna Mayilvahanan, Thaddaus Wiedemer, Evgenia Rusak, Matthias Bethge, and Wieland Brendel. Does CLIP's generalization performance mainly stem from high train-test similarity? In _The Twelfth International Conference on Learning Representations_, 2024.
* [82] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. _arXiv preprint arXiv:2309.13638_, 2023.
* [83] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. _arXiv preprint arXiv:2210.07183_, 2022.
* [84] George A Miller. _WordNet: An electronic lexical database_. MIT press, 1998.
* [85] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In _International conference on machine learning_, pages 7721-7735. PMLR, 2021.
* [86] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. _Advances in Neural Information Processing Systems_, 36, 2023.
* [87] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. In _European conference on computer vision_, pages 529-544. Springer, 2022.
* [88] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. _Advances in Neural Information Processing Systems_, 35:21455-21469, 2022.
* [89] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. _Advances in Neural Information Processing Systems_, 2023.
* [90] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729. IEEE, 2008.
* [91] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _Transactions on Machine Learning Research_, 2023.
* [92] Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails of vision-language models. _CVPR_, 2024.
* [93] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2012.
* [94] Steven T Piantadosi. Zipf's word frequency law in natural language: A critical review and future directions. _Psychonomic bulletin & review_, 21:1112-1130, 2014.
* [95] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In _The Twelfth International Conference on Learning Representations_, 2024.
* [96] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip HS Torr, and Adel Bibi. From categories to classifier: Name-only continual learning by exploring the web. _arXiv preprint arXiv:2311.11293_, 2023.
* [97] Sarah Pratt, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. _arXiv preprint arXiv:2209.03320_, 2022.
* [98] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 2021.
* [99] Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ali Farhadi, and Ludwig Schmidt. On the connection between pre-training data diversity and fine-tuning robustness. _Advances in Neural Information Processing Systems_, 36, 2024.

* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning (ICML)_, 2021.
* building an open web-scale crawl using hadoop, 2010. URL https://www.slideshare.net/hadoopusergroup/common-crawlpresentation.
* Razeghi et al. [2022] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 840-854, 2022.
* Razeghi et al. [2022] Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L Logan Iv, Matt Gardner, and Sameer Singh. Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot lm performance. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 389-395, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* Saehoon et al. [2021] Kim Saehoon, Cho Sanghun, Kim Chiehon, Doyup Lee, and Woonhyuk Baek. mindall-e on conceptual captions. https://github.com/kakaobrain/minDALL-E, 2021.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Samuel et al. [2024] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. Generating images of rare concepts using pre-trained diffusion models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 4695-4703, 2024.
* Santurkar et al. [2022] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Is a caption worth a thousand images? a controlled study for representation learning. _arXiv preprint arXiv:2207.07635_, 2022.
* Sarryldtz et al. [2023] Mert Bulent Sarryldtz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* Schuhmann et al. [2021] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* Seshadri et al. [2023] Preethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image generation. _arXiv preprint arXiv:2308.00755_, 2023.
* Shahmohammadi et al. [2023] Hassan Shahmohammadi, Adhiraj Ghosh, and Hendrik Lensch. Vipe: Visualise pretty-much everything. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 5477-5494, 2023.
* Shao et al. [2023] Hanyin Shao, Jie Huang, Shen Zheng, and Kevin Chen-Chuan Chang. Quantifying association capabilities of large language models and its implications on privacy leakage. _arXiv preprint arXiv:2305.12707_, 2023.
* Sharma et al. [2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.
* Soomro et al. [2012] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. _Advances in Neural Information Processing Systems_, 35:19523-19536, 2022.
* Student [118] Student. Probable error of a correlation coefficient. _Biometrika_, pages 302-310, 1908.
* Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2015.

* [120] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. _Advances in Neural Information Processing Systems_, 33:18583-18599, 2020.
* [121] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [122] David Thiel. Identifying and eliminating csam in generative ml training data and models. Technical report, Technical report, Stanford University, Palo Alto, CA, 2023. URL https://purl..., 2023.
* [123] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfc100m: The new data in multimedia research. _Communications of the ACM_, 59 (2):64-73, 2016.
* [124] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [125] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to-image models make strong visual representation learners. _Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [126] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. _Advances in Neural Information Processing Systems_, 36, 2024.
* [127] Vishaal Udandarao, Abhishek Maiti, Deepak Srivatsav, Suryatej Reddy Vyalla, Yifang Yin, and Rajiv Ratn Shah. Cobra: Contrastive bi-modal representation algorithm. _arXiv preprint arXiv:2005.03687_, 2020.
* [128] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. In _International Conference on Computer Vision (ICCV)_, 2023.
* [129] Vishaal Udandarao, Max F Burg, Samuel Albanie, and Matthias Bethge. Visual data-type understanding does not emerge from scaling vision-language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [130] Pavan Kumar Anassoalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileip: Fast image-text models through multi-modal reinforced training. _arXiv preprint arXiv:2311.17049_, 2023.
* [131] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.
* [132] Steven Walfish. A review of statistical outlier methods. _Pharmaceutical technology_, 30(11):82, 2006.
* [133] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2010.
* [134] Hu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data, 2023.
* [135] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data, 2023.
* [136] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [137] Gregory Yauney, Emily Reif, and David Mimno. Data similarity is not enough to explain language model performance. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.
* [138] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.
* [139] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Moitaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.
* [140] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* [141] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. _arXiv preprint arXiv:2310.20550_, 2023.

* [142] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [143] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11975-11986, 2023.
* [144] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [145] Yunhua Zhang, Hazel Doughty, and Cees GM Snoek. Low-resource vision challenges for foundation models. _arXiv preprint arXiv:2401.04716_, 2024.
* [146] George Kingsley Zipf. _Human behavior and the principle of least effort: An introduction to human ecology_. Ravenio books, 2016.

## Part I Appendix

### Table of Contents

* A Concept Frequency is Predictive of Performance Across Prompting Strategies
* B Concept Frequency is Predictive of Performance Across Retrieval Metrics
* C Concept Frequency is Predictive of Performance for T2I Models
* D Concept Frequency is Predictive of Performance across Concepts only from Image and Text Domains
* E Generalization of findings to improved VLM training objectives
* F Extended Related Work
* G Experimental Details
* G.1 Setup of Mayilvahanan et al. [81]
* H _Let It Wag!_ Test Set
* H.1 Final Set of Concepts in _Let It Wag!_
* H.2 _Let It Wag!_: Classification Test Set Curation
* I Why and How Do We Use RAM++?
	* I.1 Why RAM++ and not CLIP or open-vocabulary detectors?
	* I.2 How: Optimal RAM++ threshold for calculating concept frequencies
	* I.3 GPT-4 Descriptions for each extracted concept
* J Clarification regarding \(0\)-frequency points
* K Misalignment Degree Results and Human Verification
* L Analysis of dips in high frequency concepts
* M Variance in performance per point in the zero-shot classification plots
* N T2I Models: Evaluation
* N.1 Quantitative Results by Retrieval
* N.2 Qualitative Results
* O Classification Results: _Let It Wag!_
* P Compute and Storage Resources
* Q Licenses and Attributions
* R Limitations, Open Questions and Future Directions
* S Broader ImpactsConcept Frequency is Predictive of Performance Across Prompting Strategies

We extend the zero-shot classification results from Fig. 2 in Fig. 8 with two different prompting strategies: the results in the main paper used the {classname} only as the prompts, here we showcase both (1) "A photo of a {classname} " prompting and (2) 80 prompt ensembles as used by Radford et al [98]. We observe that _the strong log-linear trend between concept frequency and zero-shot performance consistently holds across different prompting strategies_.

Figure 8: **Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP’s zero-shot classification accuracy on a concept and the log-scaled concept pretraining frequency. This trend holds for both “A photo of a {classname}” prompting style and 80 prompt ensembles [98]. ** indicates that the result is significant (\(p<0.05\) with a two-tailed t-test.), and thus we show Pearson correlation (\(\rho\)) as well.**

[MISSING_PAGE_FAIL:20]

## 4 Conclusion

Figure 10: **Log-linear relationships between concept frequency and CLIP T2I retrieval performance.** Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP’s retrieval performance (measured using text-to-image metrics) on a concept and the log-scaled concept pretraining frequency. ** indicates that the result is significant (\(p<0.05\) with a two-tailed t-test.), and thus we show Pearson correlation (\(\rho\)) as well.

[MISSING_PAGE_FAIL:22]

Figure 14: **Log-linear relationships between concept frequency and T2I Avg. CLIP scores. Across all tested models pretrained on the LAION-Aesthetics dataset, we observe a consistent linear relationship between T2I zero-shot performance on a concept and the log-scaled concept pretraining frequency.**

Figure 12: **Log-linear relationships between concept frequency and T2I human aesthetic scores. Across all tested models pretrained on the LAION-Aesthetics dataset, we observe a consistent linear relationship between T2I zero-shot performance on a concept and the log-scaled concept pretraining frequency.**

Figure 13: **Log-linear relationships between concept frequency and T2I human alignment scores. Across all tested models pretrained on the LAION-Aesthetics dataset, we observe a consistent linear relationship between T2I zero-shot performance on a concept and the log-scaled concept pretraining frequency.**

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_FAIL:26]

Extended Related Work

Supplementing the discussion in the main paper, we further provide a broad overview of the surrounding literature within which our paper is mainly positioned.

**Effect of Pre-training Data on Downstream Data.** Several data-centric prior works [98; 47; 88; 43; 89; 76; 134; 135; 145; 117; 80; 99; 107; 108; 39; 27; 103] have highlighted the importance of pretraining data in affecting performance. Fang et al. [43] robustly demonstrated that pretraining data diversity is the key property underlying CLIP's strong out-of-distribution generalisation behaviour. Similarly, Berlot-Attwell et al. [17] showed that attribute diversity is crucial for compositional generalization [60], namely systematicity [46]. Nguyen et al. [88] extended the Fang et al. [43] analysis to show that differences in data distributions can predictably change model performance and that this behaviour can lead to effective data mixing strategies at pretraining time. Mayilvahanan et al. [81] complemented this research direction by showing that CLIP's performance is correlated with the similarity between training and test datasets. Udandaraco et al. [129] further showed that the frequency of certain visual data-types in the LAION-2B dataset was roughly correlated to the performance of CLIP models in identifying visual data-types. McCoy et al. [82] introduced the teleological approach to understanding model generalization, showing that LLMs are more reliable on tasks and input/output instances that are more probable based on their pretraining datasets. Our findings further pinpoint that the frequency of concept occurrences is a key indicator of performance. This complements existing research in specific areas like question-answering [62] and numerical reasoning [102] in large language models, where high train-test set similarity does not fully account for observed performance levels [137]. Concurrent to our work, Parashar et al. [92] also explore the problem of long-tailed concepts in the LAION-2B dataset and how it affects performance of CLIP models, supporting our findings. In contrast to their work, we look at count separately in image and text modalities, as well as across pretraining sets, and do a number of control experiments to thoroughly test the robustness of our result. Further, our frequency estimation procedure on text captions and images independently enables us to provide a more fine-grained analysis of the pretraining data distribution like quantifying misalignment between images and texts, and assessing similarity of the different pretraining data concept distributions. Finally, our demonstration that the long tail yields a log-linear trend explicitly indicates exponential sample inefficiency in large-scale pretrained models.

**Detailed Differences and Contributions compared to Parashar et al. [92]**. We emphasise that we complement this prior work and point out that our work comprehensively tests the strength of the log-linear scaling trend across several datasets, spanning varying levels of data curation and dataset sizes. Further, we note that in Parashar et al. [92], the estimated frequencies are computed using only the text captions of LAION-2B. These estimated frequencies are then used as the canonical frequencies for plotting the performance-frequency curves for all the tested models (despite these models being trained on different pretraining datasets other than LAION-2B). Our work strongly showcases why this apparent asymmetry in their frequency estimation methodology should work--from Tab. 4, we show that different VLM pretraining datasets are strongly correlated in their concept distributions. Hence, in spite of Parashar et al. [92] using only LAION-2B as their source dataset for frequency estimation, their results roughly hold true because of this strong correlation across pretraining datasets. Our methodology of incorporating both images and text captions when computing the frequency estimates is crucial for explaining this. Hence, we believe that our work comprehensively generalizes and explains the findings of prior work while also providing insights into the pretraining datasets (_e.g._, misalignment degree and correlation of concept distributions in datasets).

**Data-centric analyses.** Our work also adds to the plethora of work that aims to understand and explore the composition of large-scale datasets, and uses data as a medium for improving downstream tasks. Prior work has noted the importance of data for improving model performance on a generalised set of tasks [47; 11; 41; 13; 14]. For instance, several works utilise retrieved and synthetic data for adapting foundation models on a broad set of downstream tasks [128; 55; 125; 22; 109; 96; 144]. Maini et al. [78] observed the existence of "text-centric" clusters in LAION-2B and measured its impact on downstream performance. Other work has seeked to target the misalignment problem that we quantified in Tab. 3 by explicit recapointing of pretraining datasets [69; 29; 130; 141; 89; 18]. Further, studies have also shown that by better data pruning strategies, neural scaling laws can be made more efficient than a power-law [117; 10]. Prior work has also showcased that large-scale datasets suffer from extreme redundancy in concepts, and high degrees of toxic and biased content [40; 126]. Further research has showcased the downstream effects that such biases during pretraining induce in state-of-the art models [20, 112, 19, 48]. Our work tackles the issue of long-tailed concepts in pretraining datasets, and shows that this is an important research direction to focus efforts on.

Experimental Details

### Setup of Mayilvahanan et al. [81]

LAION-200M is a dataset obtained by deduplicating LAION-400M by pruning exact duplicates, near duplicates, and semantically similar samples within LAION-400M [10]. The control pretraining set is created by pruning 50 million highly similar samples from LAION in the order of decreasing perceptual similarity to datapoints in ImageNet-val set. We use the 150M pretraining set for obtaining the concept distribution. We evaluate the performance of a ViT-B-32 CLIP model trained on this dataset on our downstream tasks and present our analysis on those tasks.

_Let It Wag!_ Test Set

### Final Set of Concepts in _Let It Wag!_

Based on our frequency estimation pipeline from Sec. 2, we carefully curate \(290\) of the least frequent concepts across LAION-400M pretraining dataset (out of the \(4,029\)). We then remove all the concepts that have 0 counts to ensure that our final dataset consists of concepts that have been detected atleast once in LAION-400M, this method has also been used in Kandpal et al. [62] to ensure robustness to noise in the estimation process. We then add them as our set of concepts in _Let It Wag!_. A few example concepts from our final list are: {Beechcraft_1900, Black_Rosy_Finch, Irish_Wolfhound, Japanese_Chin, Kentucky_Warbler, eastern_hog-nosed_snake, eel, eggnog, flatfish, isopod, kingsnake, ladle, lakeshore, letter_opener}. We release our full concept list publicly here.

**High-level insights about long-tail concepts.** The broad categories of the most long-tailed concepts with a few examples for each are as follows (a majority of them are also highlighted in Figs. 28 to 31):

* _Birds:_ Western Scrub Jay, Cassins Finch, Prairie Warbler, Red eyed Vireo, Veery
* _Animals:_ flatworm, Tibetan Mastiff, Scottish Terrier, vine snake, newt
* _Aircrafts:_ A300B4, A310, Falcon 900, DHC-8-300, MD-11
* _Objects:_ guillotine, letter opener, ladle, dust jacket
* _Plants and fungi:_ mexican aster, gyromitra, great masterwort, thorn apple, cape flower
* _Misc.:_ consomme, stratified texture, eggnog

**Further statistics of _Let-It-Wag!_**

We provide some further statistics of the test-set below.

* _Most frequent concepts:_ partridge (count=9489), Bank Swallow (count=9489), eel (7907)
* _Least frequent concepts:_ Red-necked Grebe (count=0), SR-20 aircraft (count=0), Globeflower (count=0)
* _Median frequency of concepts:_ 97.5
* _Mean frequency of concepts:_ 1096.2

We also show the full histogram of concept frequencies for the \(290\) concepts in _Let-It-Wag!_ in Fig. 21. From the histogram, it is evident that most of the concepts in _Let-It-Wag!_ have frequency less than \(2000\). About half of the concepts in _Let-It-Wag!_ (approx. 140) have a frequency less than \(1000\). Hence, this histogram sufficiently establishes _that our Let-It-Wag! dataset truly captures the long tail_.

### _Let It Wag!_: Classification Test Set Curation

To ensure our "_Let It Wag!_" classification dataset is thoroughly cleaned and diverse, we follow a meticulous process consisting of several cleaning, filtering and verification steps:

Figure 21: **Histogram of concept frequencies for _Let-It-Wag!_ Dataset**

**1. Diverse Sourcing:** We gather images from three different online sources--Flickr, DuckDuckGo, and Bing Search--to maximize the variety of our dataset while retaining very easy-to-classify images3.

Footnote 3: we use the image sourcing pipeline of C2C [96]

**2. Temporal Filtering:** We applied a filter to only retrieve images after January 2023 to minimize overlap with images used in the pretraining datasets of multimodal models. Note this helps mitigate but does not ensure that the overlap problem is resolved.

**3. Outlier Removal:** We employ a pre-trained InceptionNet [119] to remove outliers from the entire image pool. We do this by taking all pairwise cosine-similarities between all images in the pool, and removing the images that are in the bottom 5% of the similarity values4.

Footnote 4: We use the fastdup library for outlier removal: https://github.com/visual-layer/fastdup

**4. Initial De-duplication with an InceptionNet:** We employ a pre-trained InceptionNet [119] model to identify and remove duplicates. This step involves setting high thresholds for soft de-duplication (0.9 for common classes and 0.95 for fine-grained classes) to ensure only minor, precise exclusions. A threshold of 0.9/0.95 means that we consider images to be duplicates if the cosine similarity of that image's embedding (from InceptionNet) with any other image's embedding in the image pool is larger than 0.9/0.95.

**5. Manual Verification:** Following the automated cleaning, we manually inspect and verify the accuracy of the remaining images for each class to ensure they meet quality standards.

**6. Second-level De-duplication with Perceptual Hashing:** Post-verification, we use perceptual hashing [38] with a threshold of 10 bits to identify and remove duplicate images within each class, ensuring uniqueness across our dataset5.

Footnote 5: We use the imagededup library for de-duplication: https://github.com/idealo/imagededup

**7. Class Balancing:** Finally, we balance the dataset to ensure an equal representation of classes.

This process was followed for increased quality and reliability of our dataset for image recognition tasks.

Why and How Do We Use RAM++?

We detail why we use the RAM++ model [59] instead of CLIPScore [57] or open-vocabulary detection models [86]. Furthermore, we elaborate on how we selected the threshold hyperparameter used for identifying concepts in images.

### Why RAM++ and not CLIP or open-vocabulary detectors?

We provide some qualitative examples to illustrate why we chose RAM++. Our input images do not often involve complex scenes suitable for object detectors, but many fine-grained classes on which alongside CLIP, even powerful open-world detectors like OWL-v2 [86] have poor performance.

### How: Optimal RAM++ threshold for calculating concept frequencies

We ablate the choice of the threshold we use for assigning concepts to images using the RAM++ model. For the given set of concepts, RAM++ provides a probability value (by taking a sigmoid over raw logits) for each concept's existence in a particular image. To tag an image as containing a particular concept, we have to set a threshold for deciding this assignment. We test over three thresholds: \(\{0.5,0.6,0.7\}\), showcasing quantitative and qualitative results for all thresholds in Figs. 23 and 24.

We observe best frequency estimation results using the highest threshold of 0.7. This is due to the high precision afforded by this threshold, leading to us counting only the "most aligned images" per concept as hits. With lower thresholds (0.5, 0.6), we note that noisier images that do not align well with the concept can be counted as hits, leading to degraded precision and thereby poorer frequency estimation. To ensure that we do not incorrectly tag images with erroneous concepts, our primary objective is to optimize hit precision, even if it means occasionally missing out on tagging some images with correct concepts. Hence, we use 0.7 as the threshold for all our main results.

### GPT-4 Descriptions for each extracted concept

In addition to providing a list of concepts to the RAM++ model, we also provide a set of GPT-4 generated responses that describe each concept (please refer to Tab. 5 for examples). This ensures that we adequately cover synonyms of concepts and take into account concept hierarchies [84]. This further improves tagging precision by using visual descriptions to better identify concepts (this has been shown to enhance performance in previous works [97, 83]. We open-source these descriptions.

Figure 22: **Qualitative Results comparing OWL-v2, RAM++ and CLIP. We show qualitative examples across three different models: OWL-v2, RAM++ and CLIP on fine-grained concepts.**

Figure 23: **Qualitative Results with different RAM++ thresholds.** We show qualitative examples across three different thresholds: {0.5, 0.6, 0.7} for estimating concept frequency using the RAM++ model. We note the significantly better concepts identified by the higher threshold (0.7) compared to the lower thresholds (0.5, 0.6). The images are sourced from the CC-3M dataset.

Figure 24: **Effect of different thresholds for determining concept frequency using RAM++.** We test three different thresholds: {0.5, 0.6, 0.7} for estimating concept frequency using the RAM++ model. Note that the lower the threshold, the lower precision are the tagged concepts since lower thresholds (0.5, 0.6) lead to noisier images being counted as hits, hence reducing the hit precision for determining frequency. Despite this added noise at lower thresholds, we note that all the correlations are significantly positive across all thresholds. This further signifies the robustness of our log-linear scaling trends inspite of our frequency estimates being noisy. ** indicates that the result is significant (\(p<0.05\) with two-tailed t-test.), and thus we show Pearson correlation (\(\rho\)) too.

Figure 23: **Qualitative Results with different RAM++ thresholds.** We show qualitative examples across three different thresholds: {0.5, 0.6, 0.7} for estimating concept frequency using the RAM++ model. We note the significantly better concepts identified by the higher threshold (0.7) compared to the lower thresholds (0.5, 0.6). The images are sourced from the CC-3M dataset.

\begin{table}
\begin{tabular}{c|c|l} \hline
**Evaluation Dataset** & **Concept** & **GPT-4 Description** \\ \hline ImageNet [36] & Tench & A tench is a freshwater fish that typically has a greenish or brownish body with reddish fins, small scales, and a pair of barbels near its mouth. It can grow up to 70 cm long. \\ \hline SUN397 [133] & Alley & An alley is a narrow passageway or lane between or behind buildings, which is often used for access or for parking. \\ \hline UCF101 [116] & Blowing\_Candles & A Blowing\_Candles moment can be identified concisely as a moment or event where a person is blowing out the candles on a cake, typically at a birthday celebration. \\ \hline Caltech101 [45] & Butterfly & A butterfly is a small, flying insect known for its colorful and symmetrical wings. It has a slender body, antennae and three pairs of legs. \\ \hline CUB [131] & Least\_Auklet & A Least Auklet is a small seabird with a black back and wings, white underparts, and a stubby orange bill. They also have white eye-rings and a small, rounded tail. \\ \hline EuroSAT [56] & Pasture Land & Pasture land can be concisely identified as an open or cleared land covered with grass, clover, or the like, suitable for grazing by livestock, with little to no trees and is primarily used for agricultural purposes. \\ \hline Flowers102 [90] & pink primrose & A pink primrose is a perennial flower featuring delicate, soft pink petals arranged around a yellow center, with bright green leaves at the base. \\ \hline DTD [32] & bumpy & A bumpy object has an uneven or rough surface with lots of small raised areas or protuberances \\ \hline Food101 [21] & churro & A churro is a long, thin, golden-brown pastry that is typically ridged and may be dusted with sugar. \\ \hline FGVCAircraft [79] & 707320 & A 707320 is a model of the Boeing 707, which is a mid-size, long-range, narrow-body four-engine jet airliner. \\ \hline Stanford-Cars [66] & 1993 Volvo 240 Sedan & The 1993 Volvo 240 Sedan can be identified by these features:1. Manufacturer: Volvo 2. Production Year: 1993 3. Model: 240 4. Body Style: 4-door sedan 5. Engine: 2.3L 4-cylinder 6. Transmission: 5-speed manual or 4-speed automatic \\ \hline CIFAR100 [68] & bowl & A bowl is a round dish or container typically used to hold food, often deeper than a plate with a wide open top. \\ \hline COCO-5K [75] & metro & A metro can be identified concisely as an urban railway system that operates within large cities, offering high-frequency services and utilizing multiple cars and stations. \\ \hline \end{tabular}
\end{table}
Table 5: **Example GPT-4 Descriptions fed to RAM++ on a subset of downstream datasets and concepts.**

[MISSING_PAGE_FAIL:35]

Misalignment Degree Results and Human Verification

In Tab. 3 in the main paper, we quantified the _misalignment degree_, and showcased that a large number of image-text pairs in all pretraining datasets are misaligned. In Alg. 1, we describe the method used for quantifying this _misalignment degree_ for each pretraining dataset. We also showcase some qualitative examples of a few image-text pairs from the CC-3M dataset that are identified as misaligned using our analysis in Fig. 26.

``` Data: Pretraining dataset \(\mathcal{D}=\{(i_{1},t_{1}),(i_{2},t_{2}),\ldots,(i_{N},t_{N})\}\), Image Index \(I_{\text{img}}\), Text Index \(I_{\text{text}}\) Result:mis_degree \(\textit{mis\_degree}\gets 0\) for\((i,t)\in\mathcal{D}\)do img_concepts \(\gets I_{\text{img}}[i]\) // extract all concepts from this image text_concepts \(\gets I_{\text{text}}[t]\) // extract all concepts from this text caption hits \(\leftarrow\) set_intersection\((\text{img\_concepts},\text{text\_concepts})\) iflen(\(\textit{hits})=0\)then mis_degree \(\leftarrow\) mis_degree \(+1\)  end if return \(mis\_degree/N\)  end for ```

**Algorithm 1**Extracting _misalignment degree_ from pretraining datasets

**Human verification for misalignment results**. To verify the misalignment results from Tab. 3, we manually annotated \(200\) random image-text pairs from each dataset as aligned or misaligned. An image-text pair is misaligned if the text caption was irrelevant to the image. Previous work also found a similarly small random subset over large-scale web-datasets to be representative [78]. Our estimated misalignment results from Tab. 3 were in line with the human-verified results (see Tab. 7 below), corroborating our findings. Further, from our human-verification experiment, we found that the high misalignment degree in YFCC-15M is likely due to the lack of text quality filtering. YFCC-15M images are sourced directly from Flickr, where captions often provide high-level context rather than accurately describing the image content.

Figure 26: **Qualitative examples of misaligned image-text pairs identified. We present 4 samples from the CC-3M pretraining dataset that are identified as misaligned by our analysis. Here, the text captions clearly do not entail the images, and hence do not provide a meaningful signal for learning.**

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Dataset** & **Results from Tab. 3** & **Human-verified results** \\ \hline CC-3M & 16.81\% & 18.00\% \\ CC-12M & 17.25\% & 14.50\% \\ YFCC-15M & 36.48\% & 40.50\% \\ LAION-400M & 5.31\% & 7.00\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Human verification of mis-alignment results.**Analysis of dips in high frequency concepts

We provide some intuitions on why there are some drops in the trend at high frequencies for the CC-3M and CC-12M classification plots in Fig. 2. We investigated which concepts occur at such high frequencies, specifically above \(10^{4}\). From our analysis, we hypothesize two key reasons for these performance dips:

* _Concept ambiguity:_ We observe many concepts that are homonyms / polysemous (same spelling but different meaning _i.e._, can represent multiple concepts at once). Some examples are watch, bear, house, fly, bridge, cloud, park, face, bar, tower, wave, _etc_.
* _Broad concepts:_ A concept with a broader scope of definition supersedes a narrower one (concept 'dog' vs the specific breeds of dogs seen in ImageNet ('yorkshire terrier', 'boston terrier','scottish terrier', 'golden retriever', etc)). These concepts are too coarse-grained and hence can be visually represented by a diverse set of images. Performance variance of these concepts can be quite high based on the specific set of images given for testing.

These ambiguities become more prevalent the more ubiquitous a concept is, which is directly tied to its frequency obtained from pretraining datasets. Some more examples for a deeper understanding of the diversity of concepts are: 'cucumber','mushroom', 'Granny Smith', 'camera', 'chair', 'cup', 'laptop', 'hammer', 'jeep', 'lab coat', 'lipstick', 'american-flag', 'bear', 'cake', 'diamond-ring', _etc_.

[MISSING_PAGE_FAIL:39]

T2I Models: Evaluation

We provide additional quantitative and qualitative results in this section for T2I models evaluated on the "_Let It Wag?_" dataset.

### Quantitative Results by Retrieval

We analyse how state-of-the-art T2I models perform on the long-tailed concepts comprising the "_Let It Wag?_" dataset. As detailed in Sec. 6, we generate 4 images for each concept using Stable Diffusion XL [95], Stable Diffusion v2 [104] and Dreamlike Photoreal [1].

**Prompting Strategy.** The prompting strategy (system role) used, adapted from Shahmohammadi et al. [113], was:

With this pool of generated images, we conduct a controlled experiment on the long-tailed concepts using nearest-neighbor retrieval as the evaluation metric by querying a generated image and retrieving the top-k results from a gallery of images taken from the "_Let It Wag._" dataset. The overall pipeline is as follows:

**Setup.** We define the query and gallery set for head and tail concepts. For tail concepts, we sample the 25 concepts with the lowest frequency from the "_Let It Wag._" dataset. For head concepts, we sample the 25 most frequent concepts for comparison. We use the same prompting strategy with the selected 25 concepts across all 3 T2I models. To create the gallery set, we randomly sample 100 images for each of these concepts. We use DINOv2 [91] ViT-S/14 as the feature extractor.

**Results.** In Table 8, we provide the Cumulative Matching Characteristic (CMC@k) results for all 3 T2I models used in our experiment. CMC@k was chosen as we are interested in measuring the performance delta between head and tail concepts for successful retrievals within the top-k retrieved real images for a given generated image. We observe a large performance gap between _Head_ and _Tail_ concepts, providing a quantitative evaluation of generation performance of T2I models.

### Qualitative Results

In Fig. 7 of the main text, we provide an initial insight into the qualitative performance of T2I models on _"Let It Wag!"_ concepts. For ease of comprehension and comparison, we segregate concepts

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Model & \multicolumn{3}{c}{\(\Delta\)CMC} \\  & k=1 & k=2 & k=5 \\ \hline Stable Diffusion XL & 13.0 & 16.0 & 16.8 \\ Stable Diffusion v2 & 11.0 & 10.0 & 10.4 \\ Dreamlike Photoreal & 8.0 & 9.0 & 9.4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Generated-real retrieval scores.** We compare retrieval results of DINOv2 ViT-S/14 when using generated images as query images. We report \(\Delta\) CMC@k results where k={1,2,5} between head and tail concepts.

into 4 clusters: Aircraft (Fig. 28), Activity (Fig. 29), Animal (Fig. 30) and others (Fig. 31). **Please note that we compress the aforementioned images to a lower quality due to the file size limitation of our submission. We will replace them with the original, high quality image files for the final version.**

**Results.** Fig. 28 shows T2I models having difficulty in representing an aircraft in its full form in a majority of cases in addition to misrepresenting the specific model in the generated images. Fig. 29 showcases the difficulty T2I models face when representing actions or activities from prompts. Fig. 30 exemplifies the same inability of T2I models to accurately represent animal species. Finally, the remainder of the query set is shown in Fig. 31 and includes the inability to classify and subsequently generate certain species of flowers and objects.

Figure 28: **Qualitative results on the Aircraft cluster**.

Figure 29: **Qualitative results on the Activity cluster.**

Figure 30: **Qualitative results on the Animal cluster.**

Figure 31: **Qualitative results for other selected failure cases.**

[MISSING_PAGE_EMPTY:46]

Compute and Storage Resources

We run all our RAM++ image index construction and search experiments using NVIDIA A-100-80GB, 2080-TI and A-100-40GB GPU nodes. For the text index construction and search experiments, we use a CPU server with a 48-core Intel Xeon Platinum 8268 CPU and 392GB of RAM. We document the precise storage and compute costs for all our experiments, pertaining to each pretraining dataset used, in Tab. 10.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline
**Pretraining Dataset** & **Disk-Space** & **Number of Shards** & **RAM++ WCT** & **Text-Index Const. WCT** \\ \hline CC-3M & 243GB & 332 & 16h & 54h \\ CC-12M & 1.2TB & 1100 & 55h & 216h \\ YFCC-15M & 1.1TB & 1500 & 75h & 270h \\ LAION-400M & 9.4TB & 41408 & 2070h & 7200h \\ LAION-A & 5.4TB & 16110 & 805h & 2700h \\ SynthCI30M & 527GB & 3040 & 101h & 540h \\ \hline \end{tabular}
\end{table}
Table 10: **Compute and Storage Resources Utilized**. We report the total disk space required for storing all pretraining datasets along with the number of shards stored. Further, we also report the exact wall-clock runtimes (WCT) for running the RAM++ image tagging scripts and the text-index construction across all downstream datasets, on a single GPU/CPU node.

[MISSING_PAGE_FAIL:48]

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Model** & **Source** & **License** \\ \hline ViT-B-16, ViT-B-32, ViT-L-14 & [37] & Apache-2.0 license \\ ResNet50, ResNet101 & [54] & MIT License \\ \hline M-Vader & [15] & Unknown \\ DeepFloyd-IF-M, DeepFloyd-IF-L, DeepFloyd-IF-XL & [9] & DeepFloyd IF License Agreement \\ GigaGAN & [63] & Unknown \\ DALL-E Mini,DALL-E Mega & [35] & Apache-2.0 license \\ Promptist+SD-v1.4 & [53] & MIT \\ Dreamlike-Diffusion-v1.0 & [2] & Unknown \\ Dreamlike Photoreal v2.0 & [3] & Unknown \\ OpenJourney-v1 & [4] & CreativeML OpenRAIL License \\ OpenJourney-v2 & [5] & CreativeML OpenRAIL License \\ SD-Safe-Max,SD-Safe-Medium,SD-Safe-Strong,SD-Safe-Weak,SD-v1.4,SD-v1.5, & [104] & CreativeML OpenRAIL License \\ SD-v2-Base,SD-v2-1-Base & [7] & CreativeML OpenRAIL License \\ Vintedes-Diffusion-v0.1 & [105] & Apache-2.0 license \\ minDALL.E & [1] & CreativeML OpenRAIL License \\ Lexica-SD-v1.5 & [6] & CreativeML OpenRAIL License \\ Redshift-Diffusion & [6] & CreativeML OpenRAIL License \\ \hline \hline \end{tabular}
\end{table}
Table 12: **License for all models used in this work.**Limitations, Open Questions and Future Directions

We highlight a few limitations and open questions of our work, leading to some possible exciting avenues for future research.

**Understanding Image-Text Misalignments.** One can explore the origins of misalignments between images and texts, such as the limitations of exact matching for concept identification in captions, inaccuracies from the RAM++ tagging model, or captions that are either too noisy or irrelevant. A few potential mitigating strategies are to explicitly recaption the images [29, 89] or to utilize the grounded concepts from the images as additional feedback signal.

**Investigating Compositional Generalization.** In our work, we only analyse concepts in isolation, and do not take into account the combination of concepts. "Zero-shot generalization" often refers to models' ability for compositional generalization (understanding new combinations of concepts not previously encountered). This is distinct from traditional zero-shot learning and presents an intriguing, yet unresolved challenge: analyzing compositional generalization from a data-centric perspective.

**Methods for Bridging the Generalization Gap.** Addressing the challenges posed by the long-tail distribution involves improving model generalization to overcome the limited improvement from pretraining we found in our study. Retrieval mechanisms can compensate for the inherent generalization shortcomings of pretrained models, providing a viable path to mitigating the effects of long-tailed pretraining data distributions.

**Towards a Theoretical Model for the Log-Linear Scaling Trends.** Our experiments comprehensively showcase the log-linear scaling trend of model performance with pretraining concept frequency empirically, across several diverse pretraining datasets and models. However, our analysis lacks a detailed theoretical framework explaining why such a trend exists. Building such a framework can help get better intuitions about the underlying mechanics of data dependence in multimodal models, which could be crucial for developing more efficient training strategies or algorithms.

**On the Interaction of Model Scale and Concept Frequency.** An important aspect of the current recipe for building robust foundation models is model scale. Despite investigating models across different scales, a key open question is what the effect of model scaling would be on the slope of the log-linear fit in our plots. Precisely studying the rate of change of the slope across model scales would enable making stronger claims on the optimal capacity-data-frequency tradeoffs.

**Potential Mitigating Solutions.** While our paper does not propose specific solutions, we believe its primary contribution is in thoroughly highlighting the issues with current pretraining strategies for multimodal models across various datasets, pretraining methods, architectures, training objectives, and tasks. Additionally, by releasing the _"Let it Wag!"_ testbed, we provide a straightforward test set for future research to build upon, aiming to improve the generalization of multimodal models to long-tail scenarios. However, we suggest a few potential methods that could be explored to enhance multimodal long-tail:

* _Retrieval Augmentation:_ Enhancing generalization to long-tail concepts can be achieved by utilizing the "world-knowledge" of LLMs to provide detailed descriptions for these concepts. This approach transforms the task from simply recognizing long-tail concepts by name to recognizing them by both names and descriptions.
* _Curriculum Learning:_ Our tested models used random IID sampling during training. However, research into better sequencing of data samples could potentially improve model generalization to long-tail concepts by inducing more transferable feature representations in VLMs.
* _Synthetic Data:_ Addressing the issue of long-tail concepts in web-sourced datasets may not be feasible by merely increasing data samples. There will likely always be low-data density regions in the pretraining data distribution. Using synthetic data, either through procedurally generated samples or text-to-image models, could be a viable mitigation strategy.

We hope these suggestions provide valuable directions for future research and contribute to the development of multimodal models capable of better generalization.

Broader Impacts

Our work uses large-scale image-text pretraining datasets and models. The broad societal implications of both of these artifacts have been comprehensively discussed in prior work [98; 19; 20]. By extensively studying the composition of these large-scale datasets via principled methods, our work tries to gain a better understanding of their composition. A key result from our work that has serious potential implications for the broader society is the poor performance of multimodal models on the long-tail. From Tab. 4 and Fig. 5, it is clear that web-sourced datasets all exhibit the same long-tailed biases. This suggests that current models will predictably underperform on digitally marginalized communities and societies that are underrepresented on the web. Our results call for improved algorithms for training such multimodal models, such that they are more inclusive and performant on the long-tail. We also publicly release all of our data artifacts. Since the multimodal datasets we analyze in our work are extremely biased and can contain hateful, harmful and toxic content [20], our publicly released data artifacts potentially reflect these biases too. However, we hope that, by facilitating analysis of such large-scale datasets via our artifacts, future research efforts focus on gaining a better understanding of how to make these datasets fairer and more inclusive.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have reiterated our main claim of the log-linear scaling trend between pretraining concept frequency and downstream model performance several times in the main paper. We further back this up with extensive empirical evidence in Sec. 3 and Sec. 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have added a limitations section and potential future research directions that can be explored to mitigate these limitations in Appx. R. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]

Justification: We do not include theoretical results. We validate our main research questions with extensive experimentation and ablation studies.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We not only disclose all experimental details in the main paper but also, to ensure reproducibility, make available our code and data artifacts (over 300GB) across experiments, which includes GPT-4 descriptions per concept fed to the RAM++ model, search counts for each concept in all downstream datasets in each pretaining dataset, evaluation results per concept for all models used across T2I and zero-shot experiments, among others. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We are providing our codebase, with clear instructions to reproduce experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiments for comparing concept frequency to model performance (Sec. 3) include datasets used (both pretraining and downstream), models used along with their implementation details, prompting strategies for all 3 tasks and the evaluation metrics we analyse. We also provide the setup for experiments where we stress-test the scaling trend we observe in our main experiments (Sec. 4). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide significance tests with a two-tailed t-test for our main experimental results (Fig. 2). We further provide 95% CI results in Appx. M Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We incorporate statistics of compute and storage resources in Appx. P, which includes total disk space required, number of shards stored as well as the wall-clock runtimes (WCT) for RAM++ image tagging and text-index construction. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We uniformly conform to the Code of Ethics and, in particular, all data-related concerns about our "_Let it Way!_" benchmark. We communicate the details of our benchmark with a license, allow access to research artifacts, make our work reproducible, and carefully consider all societal impacts and harmful consequences of our research output. Note that we use the LAION-400M and LAION-Aesthetics datasets. LAION-400M has not been shown to contain any harmful child-sexual abuse material (CSAM). However, LAION-Aesthetics is a subset of the LAION-5B dataset, which has been shown to contain CSAM [122]. We are in contact with the LAION-5B dataset authors, and will reproduce results on their cleaned set once released. However, note that our set of \(4,029\) concepts do not contain any harmful content, and hence we are confident that our main results should hold even across the cleaned datasets. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader societal impacts of our work in Appx. S Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: For the _Let It Wag!_ dataset we introduce, we take considerable steps to ensure responsible release, all of which are detailed in Appx. H.2. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: We acknowledge previous works that we refer to in our usage of open-source multimodal models and all datasets. For licenses pertaining to models and datasets, please refer to Appx. Q. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduce a new long-tailed dataset, "_Let It Wag?_"--we detail the entire dataset sourcing pipeline in depth in Sec. 6 and Appx. H.2. We source all the image samples from publicly available sources, and release our dataset under the MIT license. We also release all our pretraining data artifacts publicly under the MIT license. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: Our human experiments in Apps. C and K are not crowd-sourced. The human participants in the experiment were colleagues of one of the authors. Screenshots of the user interface for which the participants were linked to is in Fig. 16, which contains the instructions given to participants. Further details are provided in Appx. C. Regarding compensation, participants volunteered to contribute to the study. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes] Justification: Our broader impact statement in Appx. S describes the potential risks incurred by exposure to models trained on large-scale image-text pretraining datasets, such as Stable Diffusion [104] (v1.4). Appx. C indicates that volunteers provided informed consent, and that IRB approval was not obtained. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.