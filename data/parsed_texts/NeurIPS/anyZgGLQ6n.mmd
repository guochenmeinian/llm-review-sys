# Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression

 Yixiu Mao\({}^{1}\), Qi Wang\({}^{1}\), Chen Chen\({}^{1}\), Yun Qu\({}^{1}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Department of Automation, Tsinghua University

myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn

###### Abstract

In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.

## 1 Introduction

Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [42, 57, 63, 53, 7]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [13] and time-consuming episode collection [27]. Recent advances view offline RL as a hopeful solution to these challenges [34]. Offline RL aims to learn a policy from a fixed dataset without further interactions [32]. It can tap into existing large-scale datasets for safe and efficient learning [23, 37, 50].

In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [12], which can be exacerbated by bootstrapping and result in severe value overestimation [34]. To address this issue, a large body of work has emerged to directly or indirectly _suppress OOD actions_ during training, employing various techniques such as policy constraint [12, 30, 10], value penalization [31, 2, 6], and in-sample learning [29, 14, 71].

Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an _OOD state issue_ that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [34, 75].

In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [75, 22]. Technically, Zhang et al. [75] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, whileJiang et al. [22] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement.

In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as _value-aware_ OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL [9] and NeoRL [49]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations.

To summarize, the main contributions of this work are:

* We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS _unifying OOD state correction and OOD action suppression_.
* Our approach achieves _value-aware_ OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods.
* Empirically1, our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments _without additional hyperparameter tuning_. Footnote 1: Our code is available at https://github.com/maoyixiu/SCAS.

## 2 Preliminaries

In reinforcement learning, we generally characterize the environment as a Markov Decision Process (MDP) \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,\gamma,d_{0})\), with state space \(\mathcal{S}\), action space \(\mathcal{A}\), transition dynamics \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\), reward function \(R:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), discount factor \(\gamma\in[0,1)\), and initial state distribution \(d_{0}\)[61]. The agent interacts with the environment and seeks a policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) to maximize the expected discounted return \(\eta(\pi)\):

\[\eta(\pi)=\mathbb{E}_{s_{0}\sim d_{0},a_{t}\sim\pi(\cdot|s_{t}),s_{t+1}\sim P( \cdot|s_{t},a_{t})}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})\right].\] (1)

Figure 1: **The resulting state distributions of offline RL algorithms and optimal values of states.** (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. _SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas_, while CQL and TD3BC tend to produce OOD states with extremely low values.

For any policy \(\pi\), we define the value function as \(V^{\pi}(s)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})|s_{0 }=s\right]\) and the state-action value function (\(Q\)-value function) as \(Q^{\pi}(s,a)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})| s_{0}=s,a_{0}=a\right]\).

Offline RL.In offline RL, the agent can only access a static dataset \(\mathcal{D}=\{(s_{i}^{t},a_{i}^{i},s_{i+1}^{i},r_{i}^{i})\}\). We denote the empirical behavior policy of \(\mathcal{D}\) by \(\beta(a|s)\) and the empirical dynamics model by \(M(s^{\prime}|s,a)\), both of which depict the conditional distributions observed in the dataset [12]. Typical actor-critic algorithms [56; 18] evaluate policy \(\pi\) by minimizing Bellman loss:

\[L_{Q}(\theta)=\mathbb{E}_{(s,a,s^{\prime})\sim\mathcal{D}}[(Q_{\theta}(s,a)-R (s,a)-\gamma\mathbb{E}_{a^{\prime}\sim\pi_{\phi}(\cdot|s^{\prime})}Q_{\theta ^{\prime}}(s^{\prime},a^{\prime}))^{2}],\] (2)

where \(\pi_{\phi}\) and \(Q_{\theta}\) are the parameterized policy and \(Q\) function, and \(Q_{\theta^{\prime}}\) is a target network whose parameters are updated via Polyak averaging [42].

Simultaneously, policy improvement in policy iteration is achieved via maximizing the Q-value:

\[L_{\pi}(\phi)=-\mathbb{E}_{s\sim\mathcal{D},a\sim\pi_{\phi}}\left[Q_{\theta} \left(s,a\right)\right].\] (3)

OOD action issue.In offline RL, _OOD actions_ refer to actions outside the support of the behavior policy \(\beta(\cdot|s)\) at a specific state \(s\in\mathcal{D}\). Since the Q-values of OOD actions can be poorly estimated and the policy improvement is towards maximizing the estimated \(Q_{\theta}\), the resulting policy tends to prioritize the OOD actions with overestimated values, leading to poor performance [12].

## 3 OOD State Correction

The following focuses on the OOD state issue and OOD state correction in offline RL. In Section 3.1, we systematically analyze the OOD state issue, introduce the concept of OOD state correction, and point out limitations of prior methods. Then we present the proposed approach SCAS in Section 3.2.

### OOD State Issue in Offline RL

In offline RL, _OOD states_ refer to states not in the offline dataset. The OOD state issue (Definition 1) pertains to scenarios where the agent enters OOD states during the test phase, potentially resulting in catastrophic failure [34]. However, such a topic is rarely investigated in the literature, and existing studies lack deep insights. We mathematically formulate the OOD state issue as follows.

**Definition 1** (OOD state issue).: _There exists \(s\in\mathcal{S}\), such that \(d_{\mathcal{M}\mathcal{T}}^{\pi}(s)>0\) and \(d_{\mathcal{D}}(s)=0\), where \(\mathcal{M}_{\mathcal{T}}\) is the MDP of the test environment, \(\pi\) is any learned policy, \(\bar{d}_{\mathcal{M}\mathcal{T}}^{\pi}\) is the state probability density induced by \(\pi\) in \(\mathcal{M}_{\mathcal{T}}\), and \(d_{\mathcal{D}}\) is the state probability density in the offline dataset._

Origins and consequence of OOD states.During the test phase, the OOD states occur primarily in three scenarios: (i) OOD actions: the learned policy, not perfectly constrained within the support of the behavior policy, executes unreliable OOD actions, leading to OOD states. (ii) Stochastic environment: the initial state of the actual environment may fall outside the offline dataset. In addition, stochastic dynamics can also lead to states outside the dataset, even when taking ID actions in ID states. (iii) Perturbations: commonly seen in real-world robot applications, some unexpected perturbations can propel the agent into OOD states (e.g., wind, human interference).

During offline training, the typical Bellman updates involve only ID states, and the policies in OOD states are not trained. As a result, when encountering OOD states in the test phase, the agent would exhibit uncontrolled behavior, and the state deviation from the offline dataset can be further exacerbated over time steps, severely degrading performance [34].

OOD state correction.To mitigate this OOD state issue, an intuitive solution is to train a policy capable of correcting the agent from OOD states to ID states, a concept known as _OOD state correction_[75]. Specifically, during offline training, we can perturb the original state \(s\) in the dataset into \(\hat{s}\) to generate substantial OOD states. Then consider the scenario where the agent starts from \(\hat{s}\), follows the trained policy \(\pi\), and transitions to the next state \(\hat{s}^{\prime}\). To reduce state deviation, \(\hat{s}^{\prime}\) is expected to be close to the offline dataset. Thus we can align the distribution of \(\hat{s}^{\prime}\) with an ID state distribution to regularize the policy and achieve OOD state correction.

Continuing the above train of thought, SDC [75] generates the ID state distribution by feeding the original state \(s\) into a trained state transition model \(N(s^{\prime}|s)\) of the dataset. This model characterizesthe conditional state transition distribution in the dataset and is implemented by a conditional variational auto-encoder (CVAE) [58]. After pretraining a dynamics model \(M(s^{\prime}|s,a)\) and the state transition model \(N(s^{\prime}|s)\), SDC introduces the following policy regularizer for OOD state correction:

\[\min_{\pi}\operatorname*{\mathbb{E}}_{s\sim\mathcal{D}}\operatorname*{ \mathbb{E}}_{\hat{s}\sim\mathcal{N}_{\sigma}(s)}\left[\operatorname*{MMD}(M( \cdot|\hat{s},\pi(\cdot|\hat{s})),N(\cdot|s))\right],\] (4)

where \(\hat{s}\) is a Gaussian noise perturbed version of the original state \(s\), \(\sigma\) is the standard deviation of the Gaussian, \(M(\cdot|\hat{s},\pi(\cdot|\hat{s}))\) is shorthand for \(\operatorname*{\mathbb{E}}_{\hat{a}\sim\pi(\cdot|\hat{s})}M(\cdot|\hat{s},\hat {a})\), and MMD is the maximum mean discrepancy measure. More recently, OSR [22] directly aligns the trained policy distribution at the perturbed state with a CVAE inverse dynamics model to constrain the policy in OOD states.

Limitations.However, the regularizers of prior methods are only designed to deal with this OOD state issue. To mitigate OOD actions, they require an additional conservative Q learning (CQL) term [31] in value estimation to penalize Q-values of OOD actions. In addition, the state transition distribution and the inverse dynamics distribution are multi-modal in many scenarios [43]. The necessity of extra OOD action suppression components and complex distribution modeling compromises their computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, particularly when the offline dataset contains a large portion of suboptimal states. In such cases, vanilla OOD state correction can lead to suboptimal behaviors. Consequently, there is also significant potential for improvement in the performance of prior methods.

For a more comprehensive discussion of related work, please refer to Appendix A.

### Value-aware OOD State Correction

The objective of this work is to formulate a simple yet effective policy regularizer for offline RL that unifies OOD state correction and OOD action suppression. Moreover, we aim to achieve _value-aware_ OOD state correction, involving the correction of the agent from OOD states to high-value ID states.

Value-aware state transition.For the ID state distribution to which the agent is corrected, we expect a value-aware state transition distribution \(N^{*}(\cdot|s)\) that lies within the support of the dataset state transition distribution \(N(\cdot|s)\) but is skewed toward high-value states \(s^{\prime}\). To ensure stability and, more importantly, to enable our subsequently designed algorithm to circumvent modeling complex distributions, we seek a soft optimal version of it. To this end, we consider the following problem2:

Footnote 2: Note that the regularizer \(\operatorname*{D_{KL}}(N^{*}(\cdot|s)\|N(\cdot|s))\) can constrain the support of \(N^{*}(\cdot|s)\) within that of \(N(\cdot|s)\), because if \(\operatorname*{supp}(N^{*}(\cdot|s))\nsubseteq\operatorname*{supp}(N(\cdot|s))\) at some state \(s\), then \(\operatorname*{D_{KL}}(N^{*}(\cdot|s)\|N(\cdot|s))=\infty\).

\[\max_{N^{*}}\operatorname*{\mathbb{E}}_{s\sim\mathcal{D}}\left[\alpha\operatorname *{\mathbb{E}}_{s^{\prime}\sim N^{*}(\cdot|s)}V(s^{\prime})-\operatorname*{D_{ KL}}(N^{*}(\cdot|s)\|N(\cdot|s))\right],\] (5)

where \(\alpha\) is a hyperparameter to balance the two terms.

The optimization problem above has a closed-form solution:

\[N^{*}(s^{\prime}|s)=\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right) \right)N(s^{\prime}|s),\] (6)

where \(Z(s)=\sum_{s^{\prime}}\exp\left(\alpha V\left(s^{\prime}\right)\right)N(s^{ \prime}|s)\) is a normalization factor. It can be seen from Eq. (6) that \(\operatorname*{supp}(N^{*}(\cdot|s))\subseteq\operatorname*{supp}(N(\cdot|s))\). Note that \(\alpha\) is a key hyperparameter that controls the significance of the values of next states in SCAS's OOD state correction. As \(\alpha\) increases, \(N^{*}(\cdot|s)\) becomes more skewed toward the optimal \(s^{\prime}\) in the support of \(N(\cdot|s)\).

OOD state correction.In order to produce substantial OOD states, we perturb each state \(s\in\mathcal{D}\) with Gaussian noise \(\mathcal{N}(0,\sigma^{2})\), resulting in perturbed state \(\hat{s}\). It is worth noting that the dataset used for RL training remains unchanged. We perturb the states solely to formulate the regularizer.

We anticipate the following value-aware OOD state correction scenario, where the agent starts from OOD state \(\hat{s}\), follows the trained policy \(\pi\), and transitions to the high-value ID state \(s^{\prime}\) in the distribution of \(N^{*}(\cdot|s)\). To this end, we train the policy \(\pi\) to align the dynamics induced by \(\pi\) on the perturbed state \(\hat{s}\) with the value-aware state transition distribution at the original state \(s\) via KL divergence. That is, we regularize \(\pi\) by minimizing:

\[\min_{\pi}\operatorname*{\mathbb{E}}_{s\sim\mathcal{D}}\operatorname*{\mathbb{E }}_{\hat{s}\sim\mathcal{N}_{\sigma}(s)}\operatorname*{D_{KL}}(N^{*}(\cdot|s) \|M(\cdot|\hat{s},\pi(\cdot|\hat{s}))).\] (7)By substituting the analytical solution of \(N^{*}\) from Eq. (6) into the KL divergence, we have

\[\operatorname*{argmin}_{\pi}\operatorname*{D_{KL}}(N^{*}(\cdot|s)||M(\cdot|\hat{s },\pi(\cdot|\hat{s})))=\operatorname*{argmax}_{\pi}\operatorname*{\mathbb{E}}_{s ^{\prime}\sim N(\cdot|s)}\,\left[\frac{\exp\left(\alpha V\left(s^{\prime} \right)\right)}{Z(s)}\log M(s^{\prime}|\hat{s},\pi(\cdot|\hat{s}))\right].\]

Note that \(N\) is the state transition distribution in the dataset, and \(s\sim\mathcal{D},s^{\prime}\sim N(\cdot|s)\) is equivalent to \((s,s^{\prime})\sim\mathcal{D}\). Thus minimizing Eq. (7) is equivalent to maximizing following regularizer:

\[R(\pi)=\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\operatorname* {\mathbb{E}}_{\hat{s}\sim\mathcal{N}_{\pi}(s)}\left[\frac{\exp\left(\alpha V \left(s^{\prime}\right)\right)}{Z(s)}\log M(s^{\prime}|\hat{s},\pi(\cdot|\hat {s}))\right].\] (8)

As a result, \(R(\pi)\) effectively eliminates the need for a pre-trained multi-modal state transition model (\(N\) or \(N^{*}\)) and enables direct sampling from the dataset for optimization.

However, the normalization factor \(Z(s)\) in \(R(\pi)\) can be challenging to compute. We note that the regularizer \(R(\pi)\) is derived from the minimization of the KL divergence in Eq. (7). Since we aim to minimize this KL at every state \(s\) in \(\mathcal{D}\) and \(Z(s)\) only affects the relative weights at different \(s\), it matters less to precisely restore the correct state weights in \(\mathcal{D}\) by computing \(Z(s)\), which is empirically hard to estimate and may bring more instability. Thus, we replace \(Z(s)\) in \(R(\pi)\) with an empirical normalizer \(\exp(\alpha V(s))\) for computational stability:

\[R_{1}(\pi)=\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}} \operatorname*{\mathbb{E}}_{\hat{s}\sim\mathcal{N}_{\pi}(s)}\left[\frac{\exp \left(\alpha V\left(s^{\prime}\right)\right)}{\exp\left(\alpha V\left(s\right) \right)}\log M(s^{\prime}|\hat{s},\pi(\cdot|\hat{s}))\right].\] (9)

We provide further rationale behind this choice of the empirical normalizer in Appendix C.1.

Tractable optimization.Now we shift focus to the optimization of \(R_{1}(\pi)\). The expectation with respect to \(\pi\) can be moved outside the logarithm by Jensen's inequality:

\[R_{1}(\pi)\geq\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}} \operatorname*{\mathbb{E}}_{\hat{s}\sim\mathcal{N}_{\sigma}(s)}\operatorname* {\mathbb{E}}_{\hat{s}\sim\pi(\cdot|s)}\left[\frac{\exp\left(\alpha V\left(s^ {\prime}\right)\right)}{\exp\left(\alpha V\left(s\right)\right)}\log M(s^{ \prime}|\hat{s},a)\right],\] (10)

where the equality holds when \(\pi\) is deterministic. In general, it is convenient to maximize the lower bound in Eq. (10) using the reparameterization trick. However, to ensure the equality case in Eq. (10), we opt to train a deterministic policy \(\pi\). In this case, we can directly maximize \(R_{1}(\pi)\) by computing the gradient of \(\pi\) using automatic differentiation [46].

In contrast to model-based RL methods that typically use the learned dynamics model to roll out multi-step trajectories for policy training [20, 73], our algorithm utilizes the dynamics model to propagate the gradient of policy and regularize policy training, resulting in significantly enhanced computational efficiency. Moreover, the nature of one-step dynamics prediction in our method is advantageous for maintaining relatively high prediction accuracy.

## 4 Analysis of OOD Action Suppression

This section focuses on the OOD action issue and shows that the proposed regularizer also exhibits the effect of _OOD action suppression_. In other words, it can also prevent the policy from taking OOD actions, thereby simultaneously addressing the fundamental OOD action issue in offline RL. In offline RL, OOD actions are exclusively defined on ID states. This is because actor-critic training is limited to ID states, and any actions on OOD states would not affect training and cause the OOD action issue mentioned in Section 2. Consequently, for the analysis of OOD actions, it is essential to consider ID states. We define \(\bar{R},\bar{R}_{1}\) as the ID state version of \(R,R_{1}\), where \(\hat{s}=s\). \(\bar{R}\) and \(\bar{R}_{1}\) can be regarded as special cases of \(R\) and \(R_{1}\), when \(\hat{s}\) sampled from \(\mathcal{N}(s,\sigma^{2})\) is equal to \(s\):

\[\bar{R}(\pi) =\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[ \frac{\exp\left(\alpha V\left(s^{\prime}\right)\right)}{Z(s)}\log M(s^{\prime} |s,\pi(\cdot|s))\right],\] (11) \[\bar{R}_{1}(\pi) =\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[ \frac{\exp\left(\alpha V\left(s^{\prime}\right)\right)}{\exp\left(\alpha V \left(s\right)\right)}\log M(s^{\prime}|s,\pi(\cdot|s))\right].\] (12)The proposed regularizer functions as follows: when the agent encounters OOD states, it drives the agent to choose actions leading to ID states, as discussed in Section 3.2. When the agent is in ID states, the ID state part of it comes into play. In the following, we show that it helps circumvent taking OOD actions by analyzing the maximizer of \(\bar{R},\bar{R}_{1}\) in tabular MDPs.

**Proposition 1**.: _Suppose that the environment dynamics is deterministic, then both \(\bar{R}(\pi)\) and \(\bar{R}_{1}(\pi)\) achieve their global maximum at the policy \(\pi^{*}\), where3_

Footnote 3: Here for clarity, we use the notation \(M\) with slightly different meanings in different cases: in the stochastic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\); in the deterministic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\).

\[\pi^{*}(a|s)=\frac{1}{Z(s)}\exp\left(\alpha V\left(M(s,a)\right)\right)\beta( a|s)\] (13)

_The support of \(\pi^{*}\) is within that of the behavior policy \(\beta\):_

\[\mathrm{supp}(\pi^{*}(\cdot|s))\subseteq\mathrm{supp}(\beta(\cdot|s)),\; \forall s\sim\mathcal{D}\] (14)

_and \(\pi^{*}\) makes the following equation hold:_

\[N^{*}(\cdot|s)=M(\cdot|s,\pi^{*}(\cdot|s)),\;\forall s\sim\mathcal{D}\] (15)

Under the deterministic dynamics condition, Proposition 1 shows that \(\pi^{*}\) is constrained within the support of the behavior policy. Thus, our regularizer helps to keep the policy from taking OOD actions. Moreover, \(\pi^{*}\) is able to exactly align \(M(\cdot|s,\pi^{*}(\cdot|s))\) with \(N^{*}(\cdot|s)\), indicating the guidance of the agent to the high-value ID state distributions.

Furthermore, we show in Proposition 2 that even under stochastic dynamics, the optimization of \(\bar{R}\) and \(\bar{R}_{1}\) still yields policies constrained within the support of \(\beta\). Hence, SCAS also exhibits the effect of OOD action suppression in this more general scenario.

**Proposition 2**.: _When the dynamics is stochastic, the maximizers of both \(\bar{R}(\pi)\) and \(\bar{R}_{1}(\pi)\) are constrained within the support of the behavior policy:_

\[\mathrm{supp}(\pi^{*}(\cdot|s))\subseteq\mathrm{supp}(\beta( \cdot|s)),\;\forall s\sim\mathcal{D}\] (16) \[\mathrm{supp}(\pi^{*}_{1}(\cdot|s))\subseteq\mathrm{supp}(\beta( \cdot|s)),\;\forall s\sim\mathcal{D}\] (17)

## 5 Implementation Details

SCAS is easy to implement and we design the practical algorithm to be as simple as possible, retaining algorithmic simplicity and improving computational efficiency.

Dynamics model.We employ a deterministic dynamics model \(M_{\omega}\). The loss for training the model is

\[L_{M}(\omega)=\mathop{\mathbb{E}}_{(s,a,s^{\prime})\sim\mathcal{D}}[\|M_{ \omega}(s,a)-s^{\prime}\|^{2}_{2}]\] (18)

Policy improvement.With a deterministic model, we replace the log-likelihood in \(R_{1}(\pi)\) with mean squared error. It is a common approach in RL algorithms to convert a maximum likelihood estimation problem into a regression problem when dealing with Gaussians with fixed variance [10]. As discussed in Section 3.2, we also adopt a deterministic policy model \(\pi_{\phi}\). Thus, we have the following policy regularizer:

\[R_{2}(\pi_{\phi})=\mathop{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\mathop{ \mathbb{E}}_{\hat{s}\sim\mathcal{N}_{\sigma}(s)}\left[\frac{\exp\left(\alpha V _{\theta}\left(s^{\prime}\right)\right)}{\exp\left(\alpha V_{\theta}\left(s \right)\right)}\|M_{\omega}(\hat{s},\pi_{\phi}(\hat{s}))-s^{\prime}\|^{2}_{2} \right],\] (19)

where \(V_{\theta}(s)=Q_{\theta}(s,\bar{\pi}_{\phi}(s))\) and \(\bar{\pi}_{\phi}\) means \(\pi_{\phi}\) with detached gradients. Using deterministic policy also simplifies the training process without learning a \(V\)-function. Combining \(R_{2}(\pi_{\phi})\) with the standard policy improvement objective, we update the policy by maximizing:

\[J_{\pi}(\phi)=(1-\lambda)\mathbb{E}_{s\sim\mathcal{D}}\left[Q_{\theta}\left(s,\pi_{\phi}(s)\right)\right]+\lambda R_{2}(\pi_{\phi}),\] (20)where \(\lambda\) is a hyperparameter to balance the two terms. Additionally, following TD3+BC [10], we also normalize \(Q_{\theta}\) in the first term in each mini-batch to maintain a balanced scale across tasks.

**Overall algorithm.** Putting everything together, we present our final algorithm in Algorithm 1.

## 6 Experiments

In this section, we conduct several experiments to examine the performance and properties of SCAS. Please refer to Appendices D and E for experimental details and additional results.

### Empirical Evidence of OOD State Correction and OOD Action Suppression

OOD state correction.To examine the OOD state correction ability, we compare the state distributions generated by the learned policies of different algorithms with the state distribution of the offline dataset. In detail, we first train SCAS, CQL [31], and TD3+BC [10], and then collect 50,000 samples by running the trained policies separately. We also sample 50,000 states randomly from the offline dataset for comparison. Figures 1(a) to 1(c) plot the state distributions in halfcheetah-medium-expert [9] with t-SNE [62], and Figure 1(d) visualizes the optimal value of each state. We access these values from the learned value function obtained by running TD3 [11] online to convergence.

In Figures 1(a) and 1(b), we observe that the policies learned by CQL and TD3+BC tend to produce OOD states. As depicted in Figure 1(d), these OOD states have extremely low values, so entering them can be detrimental to performance. In contrast, the state distribution induced by SCAS is almost entirely within the support of the offline distribution, demonstrating the OOD state correction ability of SCAS. Moreover, we also note that in the low-value area of the offline state distribution (the grey circle in Figure 1(d)), SCAS exhibits a very low state density, which could be attributed to SCAS's value-aware OOD state correction. We refer the reader to Appendix E.2 for additional experiments validating the OOD state correction effects.

OOD action suppression.We empirically evaluate the OOD action suppression effects through the lens of value estimates. We compare SCAS with three baselines: (1) ordinary off-policy RL which is SCAS with \(\lambda=0\) (all other implementations are the same); (2) SDC [75] without additional CQL [31] term to suppress OOD actions; (3) OSR [22] without additional CQL term. We conduct experiments on D4RL datasets [9]. Since value over-estimation (divergence) is the main consequence and evidence of OOD actions [12], we plot the learned Q-values of SCAS and the baselines in Figure 2. We also include the oracle Q-values of SCAS by rollouting the trained policy for \(1,000\) episodes and evaluating the Monte-Carlo return. Additional results are provided in Appendix E.1.

The results show that the learned Q-values of ordinary off-policy RL, SDC without CQL, and OSR without CQL diverge at early learning stages, suggesting that the algorithms suffer from severe OOD actions. By contrast, the learned Q-values of SCAS stay close to the oracle Q-values. This indicates that SCAS regularization alone is able to suppress OOD actions.

### Comparisons on Offline RL Benchmarks

**Tasks.** We evaluate SCAS on D4RL [9] and NeoRL [49] benchmarks. In D4RL, we conduct experiments on Gym locomotion tasks and much more challenging AntMaze tasks. Due to the space limit, _the results on NeoRL_ are deferred to Table 4 in Appendix E.3.

Figure 2: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Only SCASâ€™s OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence).

Baselines.We compare SCAS with prior state-of-the-art offline RL methods as well as the ones specifically designed for OOD state correction, including BC [48], MOPO [73], OneStep RL [5], CQL [31], TD3+BC [10], IQL [29], SDC [75] and OSR [22].

Hyperparamter tuning.Offline RL methods are appealing for their ability to generate effective policies without online interaction. Nevertheless, many existing offline RL works involve dataset-specific hyperparameter tuning. The reduction of hyperparameter tuning is crucial for improving practical applicability. In this work, SCAS uses _a single set of hyperparameters for all datasets_ in D4RL and NeoRL benchmarks to obtain the reported results.

Comparisons with baselines.On D4RL, comparisons of performance, runtime, and hyperparameter tuning information are shown in Table 1. We refer the reader to Appendix E.8 for learning curve details of SCAS. On the Gym locomotion tasks, SCAS outperforms prior methods on most datasets and achieves the highest total score with a single set of hyperparameters. On the challenging AntMaze tasks, SCAS performs better than IQL and outperforms other methods by a very large margin. In NeoRL (Table 4), SCAS performs comparably to MOBILE [59] and outperforms other baselines.

Runtime.We present the runtime of algorithms at the bottom of Table 1. SCAS exhibits significantly lower runtime than MOPO, SDC, and OSR and is comparable to other model-free baselines.

Generality.SCAS is a generic model-based regularizer that can be easily integrated into existing offline RL algorithms. The corresponding results and analysis are provided in Appendix E.5.

### Comparisons in Perturbed Environments

In this section, we evaluate the algorithms in a more real-world setting where the agent receives uncertain perturbations during test time. OOD state correction is even more critical in such scenarios since the agent can enter OOD states after perturbation. To simulate this scenario, we add varying steps of Gaussian noise with a magnitude of \(0.5\) to the actions conducted by the policy during test time. Specifically, the policy is trained on standard D4RL datasets but is tested in the perturbed environments. We control the strength of perturbations by adjusting the number of perturbation steps.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Dataset (v2) & BC & MOPO & OneStep & TD3BC & CQL & IQL & OSR & SDC & SCAS (Ours) \\ \hline halfcheetah-med & 42.0 & **73.1** & 50.4 & 48.3 & 47.0 & 47.4 & 45.1\(\pm\)0.8 & 45.9\(\pm\)0.5 & 46.6\(\pm\)0.2 \\ hopper-med & 56.2 & 38.3 & 87.5 & 59.3 & 53.0 & 66.2 & 62.0\(\pm\)3.6 & 64.7\(\pm\)3.5 & **102.5\(\pm\)0.3** \\ walker2d-med & 71.0 & 41.2 & **84.8** & **83.7** & 73.3 & **80.1\(\pm\)1.8** & **82.7\(\pm\)1.9** & **82.3\(\pm\)3.0** \\ halfcheetah-med-rep & 36.4 & **69.2** & 42.7 & 44.6 & 45.5 & 44.2 & 43.3\(\pm\)0.2 & 45.1\(\pm\)0.5 & 44.0\(\pm\)0.3 \\ hopper-med-rep & 21.8 & 32.7 & **98.5** & 60.9 & 88.7 & 94.7 & 42.1\(\pm\)1.2 & 94.8\(\pm\)6.5 & **101.6\(\pm\)1.0** \\ walker2d-med-rep & 24.9 & 73.7 & 61.7 & **81.8** & **81.8** & 73.8 & **78.1\(\pm\)1.8** & **78.5\(\pm\)6.0** & **78.1\(\pm\)4.5** \\ halfcheetah-med-exp & 59.6 & 70.3 & 75.1 & **90.7** & 75.6 & 86.7 & 63.7\(\pm\)1.45 & 76.3\(\pm\)5.2 & **91.7\(\pm\)2.7** \\ hopper-med-exp & 51.7 & 60.6 & **108.6** & 98.0 & 105.6 & 91.5 & 78.9\(\pm\)1.64 & 99.9\(\pm\)8.5 & **109.7\(\pm\)3.5** \\ walker2d-med-exp & 101.2 & 77.4 & **111.3** & **110.1** & **107.9** & **109.6** & **108.1\(\pm\)4.4** & **109.2\(\pm\)1.4** & **108.4\(\pm\)3.7** \\ halfcheetah-rand & 2.6 & **35.9** & 2.3 & 11.0 & 17.5 & 13.1 & 1.6\(\pm\)0.1 & 14.2\(\pm\)0.7 & 12.2\(\pm\)3.2 \\ hopper-rand & 4.1 & 16.7 & 5.6 & 8.5 & 7.9 & 7.9 & 3.7\(\pm\)2.6 & 3.1\(\pm\)2.8 & **31.4\(\pm\)0.1** \\ walker2d-rand & 1.2 & 4.2 & **6.9** & 1.6 & 5.1 & 5.4 & -0.1\(\pm\)0.0 & 0.2\(\pm\)0.4 & 1.4\(\pm\)1.1 \\ \hline locomotion total & 472.7 & 593.3 & 735.4 & 698.5 & 708.9 & 718.8 & 606.7 & 714.6 & **810.1** \\ \hline \hline antmaze-umaze & 66.8 & 0.0 & 54.0 & 73.0 & 82.6 & **89.6** & 87.4\(\pm\)5.0 & 81.4\(\pm\)3.8 & **90.4\(\pm\)4.3** \\ antmaze-umaze-div & 56.8 & 0.0 & 57.8 & 47.0 & 10.2 & **65.6** & 55.6\(\pm\)8.0 & 49.6\(\pm\)10.4 & **63.8\(\pm\)16.7** \\ antmaze-med-play & 0.0 & 0.0 & 0.0 & 0.0 & 59.0 & **76.4** & 22.6\(\pm\)7.6 & 55.0\(\pm\)9.6 & **76.6\(\pm\)3.9** \\ antmaze-med-div & 0.0 & 0.0 & 0.6 & 0.2 & 46.6 & 72.8 & 19.6\(\pm\)5.8 & 56.6\(\pm\)10.3 & **80.4\(\pm\)5.4** \\ antmaze-large-play & 0.0 & 0.0 & 0.0 & 0.0 & 16.4 & 42.0 & 0.0\(\pm\)0.0 & 20.8\(\pm\)8.0 & **49.0\(\pm\)4.0** \\ antmaze-large-div & 0.0 & 0.0 & 0.2 & 0.0 & 3.2 & 46.0 & 0.0\(\pm\)0.0 & 25.8\(\pm\)7.5 & **50.6\(\pm\)7.2** \\ \hline antmaze total & 123.6 & 0.0 & 112.6 & 120.2 & 218 & 392.4 & 185.2 & 289.2 & **410.8** \\ \hline \hline runtime & 30m & 900m & 120m & 60m & 250m & 100m & 300m & 420m & 140m \\ \hline hyperparameter tuning & **w/o** & w/ & **w/o** & w/ & **w/o** & w/ & w/ & **w/o** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Averaged normalized scores on Gym locomotion and AntMaze tasks over five random seeds.

Figure 3 shows the results of TD3+BC, CQL, SDC, and SCAS on various datasets over five random seeds. We observe that SCAS consistently outperforms previous methods across different perturbation levels and also exhibits less performance degradation against perturbations. Therefore, SCAS enjoys better robustness against perturbations in the complex and unpredictable environments.

### Parameter Study

We examine the effects of the inverse temperature \(\alpha\), the balance coefficient \(\lambda\), and the noise scale \(\sigma\). Due to the space limit, _the results for \(\sigma\) and on additional datasets_ are deferred to Appendix E.6. A sensitivity analysis on dynamics model errors is also provided in Appendix E.7.

**Inverse temperature \(\alpha\).**\(\alpha\) is the key hyperparameter in SCAS for achieving value-aware OOD state correction. If \(\alpha=0\), the effect degenerates to vanilla OOD state correction. Figure 4 displays the learning curves of SCAS with different \(\alpha\). The results show that **a large \(\alpha\) is _crucial_** for achieving good performance (also verified on more tasks in Figure 6), clearly demonstrating the effectiveness of our value-aware OOD state correction. However, too large \(\alpha\) (\(\alpha=10\)) induces less satisfying performance, probably due to the increased variance of the learning objective.

**Balance coefficient \(\lambda\).**\(\lambda\) in Eq. (20) controls the balance between vanilla policy improvement and SCAS regularization. We vary \(\lambda\) within the range \([0,1]\) and present the learning curves of SCAS in Figure 4. Notably, SCAS is able to converge to good performance over a very wide range of \(\lambda\) (also verified on more tasks in Figure 7). An interesting finding is that even when \(\lambda=1\) and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states.

## 7 Conclusion and Limitations

In this paper, we systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies _OOD state correction_ and _OOD action suppression_. SCAS also achieves _value-aware_ OOD state correction, significantly improving performance over vanilla

Figure 4: Parameter study on the inverse temperature \(\alpha\) and the balance coefficient \(\lambda\). (a) An appropriately large \(\alpha\) is crucial for achieving good performance. (b) The proposed SCAS regularization is essential and demonstrates robustness to changes in \(\lambda\).

Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.

OOD state correction. Empirical results validate the properties of SCAS, showcasing its superior performance on the offline RL benchmarks and its enhanced robustness in perturbed environments.

However, our work also has some limitations. For example, current SCAS primarily focuses on continuous control tasks. In discrete settings, algorithmic components like state perturbation strategy would be different, which would be an interesting direction for future work. Moreover, we anticipate employing more advanced dynamics models, such as ensembles [73] and diffusion models [21], to further improve the performance of our method.

## Acknowledgment

We thank the anonymous reviewers for feedback on an early version of this paper. This work was supported by the National Key R&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61827804.

## References

* [1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017.
* [2] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in neural information processing systems_, 34:7436-7447, 2021.
* [3] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Y4cs1Z3HnqL.
* [4] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. _arXiv preprint arXiv:2301.08028_, 2023.
* [5] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. _Advances in Neural Information Processing Systems_, 34:4933-4946, 2021.
* [6] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. _arXiv preprint arXiv:2202.02446_, 2022.
* [7] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. _Nature_, 602(7897):414-419, 2022.
* [8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [10] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [11] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [12] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, pages 2052-2062. PMLR, 2019.

* Garcia and Fernandez [2015] Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_, 16(1):1437-1480, 2015.
* Garg et al. [2023] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent RL without entropy. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=SJ0Lde3tRL.
* Ghasemipour et al. [2021] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In _International Conference on Machine Learning_, pages 3682-3691. PMLR, 2021.
* Gronauer and Diepold [2022] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial Intelligence Review_, 55(2):895-943, 2022.
* Gu et al. [2022] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Hong et al. [2023] Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, and Pulkit Agrawal. Beyond uniform sampling: Offline reinforcement learning with imbalanced datasets. _Advances in Neural Information Processing Systems_, 36:4985-5009, 2023.
* Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_, 32, 2019.
* Janner et al. [2022] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In _International Conference on Machine Learning_, pages 9902-9915. PMLR, 2022.
* Jiang et al. [2023] Ke Jiang, Jia-Yu Yao, and Xiaoyang Tan. Recovering from out-of-sample states via inverse dynamics in offline reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* Kaiser et al. [2019] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. _arXiv preprint arXiv:1903.00374_, 2019.
* Kidambi et al. [2020] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. _Advances in neural information processing systems_, 33:21810-21823, 2020.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kober et al. [2013] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Kostrikov et al. [2021] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In _International Conference on Machine Learning_, pages 5774-5783. PMLR, 2021.
* Kostrikov et al. [2022] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=68n2s9ZJNF8.

* Kumar et al. [2019] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Lange et al. [2012] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In _Reinforcement learning_, pages 45-73. Springer, 2012.
* Lee et al. [2021] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offline policy optimization via stationary distribution correction estimation. In _International Conference on Machine Learning_, pages 6120-6130. PMLR, 2021.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Lowe et al. [2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.
* Lyu et al. [2022] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=VYYf6567pQc.
* Maddern et al. [2017] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The oxford robotcar dataset. _The International Journal of Robotics Research_, 36(1):3-15, 2017.
* Mao et al. [2024] Liyuan Mao, Haoran Xu, Weinan Zhang, and Xianyuan Zhan. ODICE: Revealing the mystery of distribution correction estimation via orthogonal-gradient update. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=L8UUn7Llt4.
* Mao et al. [2023] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In _International Conference on Machine Learning_, pages 23829-23851. PMLR, 2023.
* Mao et al. [2023] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Matsushima et al. [2021] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=3hGNqpI4W8.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* Moerland et al. [2023] Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement learning: A survey. _Foundations and Trends(r) in Machine Learning_, 16(1):1-118, 2023.
* Nachum et al. [2019] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Al-gaedice: Policy gradient from arbitrary experience. _arXiv preprint arXiv:1912.02074_, 2019.
* Nair et al. [2020] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* Paszke et al. [2017] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.

* Peng et al. [2019] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* Pomerleau [1988] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. _Advances in neural information processing systems_, 1, 1988.
* Qin et al. [2022] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:24753-24765, 2022.
* Qu et al. [2023] Yun Qu, Boyuan Wang, Jianzhan Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Linc Liu, Junfeng Yang, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its offline reinforcement learning benchmarks. In _Thirty-seventh Conference on Neural Information Processing Systems Track on Datasets and Benchmarks_, 2023.
* Qu et al. [2024] Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhan Shao, Yixiu Mao, Cheems Wang, Chang Liu, and Xiangyang Ji. Choices are more important than efforts: Llm enables efficient multi-agent exploration. _arXiv preprint arXiv:2410.02511_, 2024.
* Rashid et al. [2020] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. _Journal of Machine Learning Research_, 21(178):1-51, 2020.
* Schrittwieser et al. [2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Shao et al. [2023] Jianzhan Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=62zm04mv8X.
* Shao et al. [2023] Jianzhan Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 30776-30793. PMLR, 2023.
* Silver et al. [2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In _International conference on machine learning_, pages 387-395. PMLr, 2014.
* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Sohn et al. [2015] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. _Advances in neural information processing systems_, 28, 2015.
* Sun et al. [2023] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In _International Conference on Machine Learning_, pages 33177-33194. PMLR, 2023.
* Sutton [1991] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _ACM Sigart Bulletin_, 2(4):160-163, 1991.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Van der Maaten and Hinton [2008] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.

* Wang et al. [2024] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation from adversarially explicit task distribution generation. _arXiv preprint arXiv:2407.19523_, 2024.
* Wang and Van Hoof [2022] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search. In _International Conference on Machine Learning_, pages 23055-23077. PMLR, 2022.
* Wang et al. [2024] Qi Wang, Yiqin Lv, Zheng Xie, Jincai Huang, et al. A simple yet effective strategy to robustify the meta learning paradigm. _Advances in Neural Information Processing Systems_, 36, 2024.
* Wang et al. [2020] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. _Advances in Neural Information Processing Systems_, 33:7768-7778, 2020.
* Wu et al. [2022] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=KCXQ5HoM-fy.
* Wu et al. [2019] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* Xie et al. [2021] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* Xu et al. [2023] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline RL with no OOD actions: In-sample learning via implicit value regularization. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ueYYgo2pSSU.
* Yang et al. [2022] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: Robust offline reinforcement learning via conservative smoothing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=_QzJJGH_KE.
* Yu et al. [2020] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. _Advances in Neural Information Processing Systems_, 33:14129-14142, 2020.
* Yu et al. [2021] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in neural information processing systems_, 34:28954-28967, 2021.
* Zhang et al. [2022] Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng He, Guanwen Zhang, and Xiangyang Ji. State deviation correction for offline reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9022-9030, 2022.
* Zhang et al. [2023] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. In-sample actor critic for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=dfDvOWUB53R.

Related Work

Model-free offline RL.In offline RL, extrapolation error and overestimation caused by OOD actions pose significant challenges. Among model-free solutions, value regularization methods penalize the \(Q\)-values of OOD actions [31, 2, 28, 36, 3, 72, 40], while policy constraint approaches compel the trained policy to be close to the behavior policy, either explicitly via divergence penalties [69, 30, 10], implicitly by weighted behavior cloning [47, 45, 67, 39], or directly through specific parameterization of the policy [12, 15]. Relatively independently, in-sample learning methods formulate the Bellman target using only the actions in the dataset to avoid OOD actions [5, 29, 76, 71]. Recently, some works aim to learn the optimal policy within the support of the dataset (known as in-support or in-sample optimal policy) in a theoretically sound way and are less affected by the average quality of the dataset [39, 40, 68]. However, existing popular offline RL approaches primarily focus on the OOD action issue during training and often neglect the OOD state issue during the test phase.

Model-based offline RL.Model-based RL methods learn a model of the environment and generate synthetic data from that model to optimize the policy [60, 20, 24]. To ensure conservatism in offline RL, Kidambi et al. [25] and Yu et al. [73] estimate the uncertainty in the model and apply reward penalties for state-action pairs with high uncertainty. Some model-based approaches also introduce conservatism similarly to model-free ones, employing techniques like value regularization [74] and policy constraint [41]. Recently, Sun et al. [59] conducts uncertainty quantification through the inconsistency of Bellman estimations under the learned dynamics ensemble. However, model-based methods often come with a high computational burden [20], and their effectiveness relies heavily on the quality of the trained model [43]. In contrast, our algorithm leverages the dynamics model to propagate policy gradients, make one-step predictions, and regularize policy training, leading to significantly improved computational efficiency and relatively high prediction accuracy.

OOD state correction.In offline RL, OOD state correction deserves more attention as the state deviation during the test phase can accumulate over time steps, severely degrading performance [34]. Existing limited solutions aim to train the policy to correct the agent from OOD states to ID states [75, 22]. Specifically, SDC [75] builds a dynamics model and a state transition model, and aligns the policy-induced next state distributions at OOD states with the state transition model. On the other hand, OSR [22] utilizes an inverse dynamics model to constrain the policy at OOD states. Compared with prior methods, our proposed SCAS efficiently unifies OOD state correction and OOD action suppression in offline RL and additionally achieves _value-aware_ OOD state correction. The DICE series of works [44, 33, 38] share similar motivations with SCAS to some extent; however, there are significant differences between the two. Firstly, DICE is based on a linear programming framework of RL, while SCAS is based on a dynamic programming framework. Therefore, the theoretical foundations and learning paradigms of the two are inherently different. Secondly, SCAS only corrects encountered OOD states, whereas DICE algorithms require the policy-induced occupancy distribution to align with the dataset distribution. Therefore, DICE's constraints are stricter, potentially making it more susceptible to the average quality of the dataset. Lastly, theoretical and empirical evidence indicate that DICE algorithms have a problem of gradient cancellation [38], which imposes certain limitations on their practical effectiveness.

## Appendix B Proofs

In this section, we present the proofs for the theories in the paper.

### Derivation of the Value-aware State Transition Distribution

We show that Eq. (6) is the optimal solution of the optimization problem Eq. (5):

\[\max_{N^{*}}\ \mathop{\mathbb{E}}_{s\sim\mathcal{D}}\left[\alpha\mathop{ \mathbb{E}}_{s^{\prime}\sim N^{*}(\cdot|s)}V(s^{\prime})-\mathrm{D}_{\mathrm{ KL}}(N^{*}(\cdot|s)\|N(\cdot|s))\right]\] (21)We can optimize \(N^{*}\) at each \(s\in\mathcal{D}\) separately. Thus we consider the following optimization problem:

\[\max_{\tilde{N}}\alpha\mathop{\mathbb{E}}_{s^{\prime}\sim\tilde{N}( \cdot|s)}V(s^{\prime})-\mathrm{D_{KL}}(\tilde{N}(\cdot|s)\|N(\cdot|s))\] (22) \[s.t.\,\sum_{s^{\prime}}\tilde{N}(s^{\prime}|s)=1,\;\forall s\in \mathcal{D}\]

This constrained optimization problem is convex, and the Lagrangian is:

\[\mathcal{L}(\tilde{N})=\alpha\mathop{\mathbb{E}}_{s^{\prime}\sim\tilde{N}( \cdot|s)}V(s^{\prime})-\mathrm{D_{KL}}(\tilde{N}(\cdot|s)\|N(\cdot|s))+\nu \left(\sum_{s^{\prime}}\tilde{N}(s^{\prime}|s)-1\right)\] (23)

The KKT condition gives:

\[\frac{\partial\mathcal{L}}{\partial\tilde{N}(s^{\prime}|s)}=\alpha V(s^{\prime })-\log\tilde{N}(s^{\prime}|s)-1+\log N(s^{\prime}|s)+\nu=0\] (24)

Solving for \(\tilde{N}\) gives the closed form solution \(N^{*}\):

\[N^{*}(s^{\prime}|s)=\exp\left(\alpha V\left(s^{\prime}\right)-1+\nu\right)N(s ^{\prime}|s),\;\forall s\sim\mathcal{D}\] (25)

By the condition \(\sum_{s^{\prime}}N^{*}(s^{\prime}|s)=1\), we can directly solve the Lagrangian multiplier \(\nu\) and replace \(\exp(\nu-1)\) with a normalization factor:

\[N^{*}(s^{\prime}|s)=\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right) \right)N(s^{\prime}|s),\;\forall s\sim\mathcal{D}\] (26)

where \(Z(s)=\sum_{s^{\prime}}\exp\left(\alpha V\left(s^{\prime}\right)\right)N(s^{ \prime}|s)\) is the normalization factor.

### Proof of Proposition 1

**Proposition 3** (Proposition 1 in the main paper).: _Suppose that the environment dynamics is deterministic, then both \(\bar{R}(\pi)\) and \(\bar{R}_{1}(\pi)\) achieve their global maximum at the policy \(\pi^{*}\), where4_

Footnote 4: Here for clarity, we use the notation \(M\) with slightly different meanings in different cases: in the stochastic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\); in the deterministic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\).

\[\pi^{*}(a|s)=\frac{1}{Z(s)}\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a |s)\] (27)

_The support of \(\pi^{*}\) is within that of the behavior policy \(\beta\):_

\[\mathrm{supp}(\pi^{*}(\cdot|s))\subseteq\mathrm{supp}(\beta(\cdot|s)),\; \forall s\sim\mathcal{D}\] (28)

_and \(\pi^{*}\) makes the following equation hold:_

\[N^{*}(\cdot|s)=M(\cdot|s,\pi^{*}(\cdot|s)),\;\forall s\sim\mathcal{D}\] (29)

Proof.: We start with \(\bar{R}(\pi)\).

\[\mathop{\mathrm{argmax}}_{\pi}\;\bar{R}(\pi)\] (30) \[= \mathop{\mathrm{argmax}}_{\pi}\mathop{\mathbb{E}}_{(s,s^{\prime })\sim\mathcal{D}}\;\left[\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime} \right)\right)\log M(s^{\prime}|s,\pi(\cdot|s))\right]\] (31) \[= \mathop{\mathrm{argmax}}_{\pi}\mathop{\mathbb{E}}_{s\sim\mathcal{ D}}\mathop{\mathbb{E}}_{s^{\prime}\sim N(\cdot|s)}\;\left[\frac{1}{Z(s)}\exp \left(\alpha V\left(s^{\prime}\right)\right)\log M(s^{\prime}|s,\pi(\cdot|s))\right]\] (32) \[= \mathop{\mathrm{argmax}}_{\pi}\mathop{\mathbb{E}}_{s\sim\mathcal{ D}}\mathop{\mathbb{E}}_{s^{\prime}\sim N^{*}(\cdot|s)}\;\left[\log M(s^{\prime}|s,\pi( \cdot|s))\right]\] (33) \[= \mathop{\mathrm{argmin}}_{\pi}\mathop{\mathbb{E}}_{s\sim\mathcal{ D}}\mathop{\mathbb{E}}_{s^{\prime}\sim N^{*}(\cdot|s)}\;\left[\log N^{*}(s^{ \prime}|s)-\log M(s^{\prime}|s,\pi(\cdot|s))\right]\] (34) \[= \mathop{\mathrm{argmin}}_{\pi}\mathop{\mathbb{E}}_{s\sim\mathcal{ D}}\mathrm{D_{KL}}(N^{*}(\cdot|s)||M(\cdot|s,\pi(\cdot|s)))\] (35)The third equality holds because of the relationship between \(N^{*}\) and \(N\) in Eq. (6):

\[N^{*}(s^{\prime}|s)=\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right) \right)N(s^{\prime}|s),\;\forall s\sim\mathcal{D}\] (36)

Therefore, the maximizer of \(\bar{R}(\pi)\) is equal to the solution of the minimization problem in Eq. (35). Now consider the two distributions \(N^{*}(\cdot|s)\) and \(M(\cdot|s,\pi(\cdot|s))\) in Eq. (35).

\[N^{*}(s^{\prime}|s) =\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right)\right)N (s^{\prime}|s)\] (37) \[=\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right)\right) \sum_{a}\beta(a|s)M(s^{\prime}|s,a)\] (38)

For analytical clarity, we use the notation \(M\) with slightly different meanings in different cases: in the stochastic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\); in the deterministic setting, \(M:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\). With the deterministic dynamics assumption,

\[N^{*}(s^{\prime}|s) =\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right)\right) \sum_{a}\beta(a|s)\mathbb{I}\left[M(s,a)=s^{\prime}\right]\] (39) \[=\sum_{a}\frac{1}{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right) \right)\beta(a|s)\mathbb{I}\left[M(s,a)=s^{\prime}\right]\] (40) \[=\sum_{a}\frac{1}{Z(s)}\exp\left(\alpha V\left(M(s,a)\right) \right)\beta(a|s)\mathbb{I}\left[M(s,a)=s^{\prime}\right]\] (41)

On the other hand,

\[M(s^{\prime}|s,\pi(\cdot|s)) =\sum_{a}M(s^{\prime}|s,a)\pi(a|s)\] (42) \[=\sum_{a}\pi(a|s)\mathbb{I}\left[M(s,a)=s^{\prime}\right]\] (43)

Now we define \(\pi^{*}(a|s)\) as

\[\pi^{*}(a|s):=\frac{1}{Z(s)}\exp\left(\alpha V\left(M(s,a)\right)\right)\beta( a|s)\] (44)

We first show that \(\pi^{*}\) is a valid policy, that is, \(\pi^{*}\) is normalized.

\[\pi^{*}(a|s) =\frac{1}{Z(s)}\exp\left(\alpha V\left(M(s,a)\right)\right)\beta( a|s)\] (45) \[=\frac{\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a|s)}{ \sum_{s^{\prime}}\exp\left(\alpha V\left(s^{\prime}\right)\right)N(s^{\prime }|s)}\] (46) \[=\frac{\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a|s)}{ \sum_{s^{\prime}}\exp\left(\alpha V\left(s^{\prime}\right)\right)\sum_{a} \beta(a|s)M(s^{\prime}|s,a)}\] (47) \[=\frac{\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a|s)}{ \sum_{a}\sum_{s^{\prime}}\exp\left(\alpha V\left(s^{\prime}\right)\right) \beta(a|s)M(s^{\prime}|s,a)}\] (48) \[=\frac{\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a|s)}{ \sum_{a}\exp\left(\alpha V\left(M(s,a)\right)\right)\beta(a|s)}\] (49)

Therefore, \(\sum_{a}\pi^{*}(a|s)=1\).

Substitute Eq. (44) into Eq. (41),

\[N^{*}(s^{\prime}|s)=\sum_{a}\pi^{*}(a|s)\mathbb{I}\left[M(s,a)=s^{\prime}\right]\] (50)

Comparing Eq. (43) with Eq. (50), it holds that \(N^{*}(s^{\prime}|s)=M(s^{\prime}|s,\pi^{*}(\cdot|s)),\forall s\sim\mathcal{D}\). As a result,

\[\mathop{\mathbb{E}}_{s\sim\mathcal{D}}\,\mathrm{D}_{\mathrm{KL}}(N^{*}(\cdot|s )\|M(\cdot|s,\pi^{*}(\cdot|s)))=0\] (51)

[MISSING_PAGE_FAIL:18]

For \(\forall s\in\mathcal{D}\), there exists at least one action \(a\) such that \((s,a)\in\mathcal{D}\). Thus it holds that \(n(s)>0,\forall s\in\mathcal{D}\). Then for \(\forall s\in\mathcal{D},\forall\pi\), define \(\pi_{\text{in}}\) as follows:

\[\pi_{\text{in}}(a|s)=\left\{\begin{array}{ll}\pi(a|s)+\frac{\epsilon(s)}{n( s)},&\beta(a|s)>0,\\ 0,&\beta(a|s)=0.\end{array}\right.\] (68)

\(\pi_{\text{in}}\) can be seen as a projection of \(\pi\) onto \(\beta\)'s support. Besides, for \(\forall s\in\mathcal{D}\),

\[\sum_{a}\pi_{\text{in}}(a|s) =\sum_{a}\mathbb{I}[\beta(a|s)>0]\left(\pi(a|s)+\frac{\epsilon(s) }{n(s)}\right)\] (69) \[=\sum_{a}\mathbb{I}[\beta(a|s)>0]\pi(a|s)+\epsilon(s)\] (70) \[=\sum_{a}\mathbb{I}[\beta(a|s)>0]\pi(a|s)+\sum_{a}\mathbb{I}[ \beta(a|s)=0]\pi(a|s)\] (71) \[=\sum_{a}\pi(a|s)\] (72) \[=1\] (73)

Thus \(\pi_{\text{in}}\) is a valid policy.

Now we compare \(\bar{R}(\pi_{\text{in}})\) with \(\bar{R}(\pi)\). For \(\forall(s,s^{\prime})\in\mathcal{D}\),

\[\sum_{a}M(s^{\prime}|s,a)\pi_{\text{in}}(a|s)-\sum_{a}M(s^{\prime }|s,a)\pi(a|s)\] (74) \[= \sum_{a}M(s^{\prime}|s,a)\left(\pi_{\text{in}}(a|s)-\pi(a|s)\right)\] (75) \[= \sum_{\{a|\beta(a|s)>0\}}M(s^{\prime}|s,a)\left(\pi_{\text{in}}( a|s)-\pi(a|s)\right)\] (76) \[= \sum_{\{a|\beta(a|s)>0\}}M(s^{\prime}|s,a)\frac{\epsilon(s)}{n(s)}\] (77) \[\geq 0\] (78)

The second equality holds because, in tabular MDPs, the empirical dynamics model \(M\) exactly computes the conditional distribution observed in the dataset. For transitions not contained in the dataset, \(M=0\)[12]. The final inequality holds because \(\epsilon(s)\geq 0\).

Therefore,

\[\bar{R}(\pi_{\text{in}})-\bar{R}(\pi)\] (79) \[= \mathop{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[\frac{1 }{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right)\right)\log\left(\frac{\sum_{ a}M(s^{\prime}|s,a)\pi_{\text{in}}(a|s)}{\sum_{a}M(s^{\prime}|s,a)\pi(a|s)} \right)\right]\] (80) \[\geq \mathop{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[\frac{1 }{Z(s)}\exp\left(\alpha V\left(s^{\prime}\right)\right)\log\left(1\right)\right]\] (81) \[\geq 0\] (82)

Now suppose \(\pi\) is not constrained within the support of the behavior policy at some state \(s_{1}\in\mathcal{D}\): \(\operatorname{supp}(\pi(\cdot|s_{1}))\nsubseteq\operatorname{supp}(\beta( \cdot|s_{1}))\). That is, \(\exists\tilde{a}_{1}\) such that \(\pi(\tilde{a}_{1}|s_{1})>0\) and \(\beta(\tilde{a}_{1}|s_{1})=0\). Thus it holds that \(\epsilon(s_{1})=\sum_{a}\mathbb{I}[\beta(a|s_{1})=0]\pi(a|s_{1})>0\). On the other hand, since \(s_{1}\in\mathcal{D}\), there exists at least one action \(a_{1}\) and one state \(s^{\prime}_{1}\) such that \((s_{1},a_{1},s^{\prime}_{1})\in\mathcal{D}\). Thus it holds that \(\beta(a_{1}|s_{1})>0\) and \(M(s^{\prime}_{1}|s_{1},a_{1})>0\). As a result,

\[\sum_{a}M(s^{\prime}_{1}|s_{1},a)\pi_{\text{in}}(a|s_{1})-\sum_{a }M(s^{\prime}_{1}|s_{1},a)\pi(a|s_{1})\] (83) \[= \sum_{\{a|\beta(a|s_{1})>0\}}M(s^{\prime}_{1}|s_{1},a)\frac{ \epsilon(s_{1})}{n(s_{1})}\] (84) \[> 0\] (85)In such case, \(\bar{R}(\pi_{\text{in}})>\bar{R}(\pi)\). Therefore, if \(\pi\) is not constrained within the support of the behavior policy at some state \(s_{1}\in\mathcal{D}\), we can find another policy \(\pi_{\text{in}}\) that is constrained within the support of the behavior policy and achieves higher objective function \(\bar{R}(\pi_{\text{in}})\). Consequently, \(\bar{R}(\pi)\) must achieve its maximum at support constrained policy \(\pi^{*}\): \(\operatorname{supp}(\pi^{*}(\cdot|s))\subseteq\operatorname{supp}(\beta(\cdot |s)),\ \forall s\sim\mathcal{D}\).

Now we consider \(\bar{R}_{1}(\pi)\).

\[\bar{R}_{1}(\pi): =\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[ \exp\left(\alpha\left(V\left(s^{\prime}\right)-V\left(s\right)\right)\right) \log M(s^{\prime}|s,\pi(\cdot|s))\right]\] (86) \[=\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[ \exp\left(\alpha\left(V\left(s^{\prime}\right)-V\left(s\right)\right)\right) \log\left(\sum_{a}M(s^{\prime}|s,a)\pi(a|s)\right)\right]\] (87)

With the same definition of \(\epsilon(s)\), \(n(s)\) and \(\pi_{\text{in}}\) as in Eq. (66), Eq. (67) and Eq. (68), it also holds that

\[\bar{R}_{1}(\pi_{\text{in}})-\bar{R}_{1}(\pi)\] (88) \[=\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}}\left[ \exp\left(\alpha\left(V\left(s^{\prime}\right)-V\left(s\right)\right)\right) \log\left(\frac{\sum_{a}M(s^{\prime}|s,a)\pi_{\text{in}}(a|s)}{\sum_{a}M(s^{ \prime}|s,a)\pi(a|s)}\right)\right]\] (89) \[\geq\operatorname*{\mathbb{E}}_{(s,s^{\prime})\sim\mathcal{D}} \left[\exp\left(\alpha\left(V\left(s^{\prime}\right)-V\left(s\right)\right) \right)\log\left(1\right)\right]\] (90) \[\geq 0\] (91)

As before, when supposing \(\pi\) is not constrained within the support of the behavior policy at some state \(s_{1}\in\mathcal{D}\), it holds that \(\bar{R}_{1}(\pi_{\text{in}})>\bar{R}_{1}(\pi)\). Therefore, \(\bar{R}_{1}(\pi)\) must achieve its maximum at support constrained policy \(\pi_{1}^{*}\): \(\operatorname{supp}(\pi_{1}^{*}(\cdot|s))\subseteq\operatorname{supp}(\beta( \cdot|s)),\ \forall s\sim\mathcal{D}\).

In conclusion, when the environment dynamics is stochastic, the maximizers of both \(\bar{R}(\pi)\) and \(\bar{R}_{1}(\pi)\) are constrained within the support of the behavior policy:

\[\operatorname{supp}(\pi^{*}(\cdot|s))\subseteq\operatorname{supp}(\beta( \cdot|s)),\ \operatorname{supp}(\pi_{1}^{*}(\cdot|s))\subseteq\operatorname{supp}(\beta( \cdot|s)),\ \forall s\sim\mathcal{D}\] (92)

## Appendix C Further Discussions

### Rationale for Choosing \(\exp(\alpha V(s))\) as the Empirical Normalizer

Firstly, choosing \(\exp(\alpha V(s))\) is intended to obtain something similar to the advantage function. With this normalizer, the weight of our regularizer is \(\exp(\alpha(V(s^{\prime})-V(s)))\), which is comparable to the weight \(\exp(\alpha A(s,a))\) in Advantage Weighted Regression (AWR) [47]. Here, \(V(s^{\prime})-V(s)\) represents the relative advantage of the next state \(s^{\prime}\) compared to the current state \(s\), while \(A(s,a)\) reflects the relative advantage of taking action \(a\) in \(s\) compared to following the current policy. Comparison of the objectives of SCAS and AWR:

\[\text{SCAS:}\quad\exp(\alpha(V(s^{\prime})-V(s)))\log(M(s^{\prime }|\hat{s},\pi(\hat{s})))\] (93) \[\text{AWR:}\quad\exp(\alpha A(s,a))\log\pi(a|s)\] (94)

Secondly, as discussed in the paper, introducing any normalizer that depends only on \(s\) (independent of \(s^{\prime}\)) does not affect the development and analysis of our method; it is merely for computational stability. In AWR-based methods, there also exists a normalizer \(Z(s)\) and they usually disregard it [47, 45]. The rationale behind this is similar.

### Pessimism and Robustness in SCAS

In a specific sense, SCAS, which unifies OOD state correction and OOD action suppression, also integrates pessimism and state robustness. (1) Regarding pessimism: The OOD action suppression effect of SCAS aligns with the pessimism commonly discussed in offline RL work (being pessimistic about OOD actions) [31, 70, 30, 3, 54]. Unlike traditional policy constraint methods [69, 30, 10, 47], our approach does not require the training policy to align with the behavior policy; it only requires the successor states to be within the dataset support, which is a more relaxed constraint. (2) Regarding state robustness: The OOD state correction effect of SCAS is aimed at improving the agent'srobustness to OOD states during the test phase. Compared with previous works, SCAS unifies OOD state correction and OOD action suppression and additionally achieves value-aware OOD state correction. Some offline RL literature on state robustness differs from our approach; they typically consider noisy observations [72], such as sensor errors. In contrast, SCAS addresses state robustness concerning actual OOD states encountered during test time, rather than noisy observations.

### Regularization Effect at ID States

In SCAS, there is regularization on the policy's output actions at ID states. In our regularizer, the perturbed states \(\hat{s}\) are sampled from \(\mathcal{N}(s,\sigma^{2})\), and a large portion of \(\hat{s}\) will fall near the original ID state \(s\) or even be approximately equal to \(s\). Therefore, the policy's output actions at ID states are also regularized. For this part of the regularization, its role is equivalent to the ID state regularizer analyzed in Section 4, which has been theoretically shown to have the effect of suppressing OOD actions. Moreover, the experimental results in Section 6 also demonstrate that our OOD state correction regularizer addresses the traditional issue with OOD actions.

### Differences between the OOD Action Issue and the OOD State Issue

We further elucidate the differences between the well-known OOD action issue and the OOD state issue we analyzed. Most offline RL works focus on the OOD action issue in the training phase. That is, the trained policy outputs OOD actions to compute the target Q, which results in extrapolation error and value divergence during training [12]. In contrast, the OOD state issue we defined and analyzed is in the test phase. That is, the agent can enter states out of the offline dataset during test, potentially resulting in catastrophic failure.

## Appendix D Experimental Details

All hyperparameters of SCAS are included in Table 2. Note that we use this same set of hyperparameters to obtain all the results reported in this paper (except for parameter study). Following TD3+BC [10], we normalize the states in all datasets except for antmaze-large. We clip the exponentiated weight \(\exp\left(\alpha V_{\theta}\left(s^{\prime}\right)-\alpha V_{\theta}\left(s \right)\right)\) in Eq. (19) to \((-\infty,50]\). Following the suggestions in the benchmark [9], we subtract 1 from the rewards for the Antmaze datasets.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{8}{*}{Policy training} & Optimizer & Adam [26] \\  & Critic learning rate & \(3\times 10^{-4}\) \\  & Actor learning rate & \(2\times 10^{-4}\) with cosine schedule \\  & Batch size & 256 \\  & Discount factor & 0.99 \\  & Gradient Steps & \(10^{6}\) \\  & Target network update rate & 0.005 \\  & Policy update frequency & 2 \\  & Number of Critics & 4 \\  & Inverse temperature \(\alpha\) & 5 \\  & Balance coefficient \(\lambda\) & 0.25 \\  & Noise scale \(\sigma\) & 0.003 \\ \hline \multirow{4}{*}{Dynamics training} & Optimizer & Adam \\  & Learning rate & \(1\times 10^{-3}\) \\ \cline{1-1}  & Batch size & 256 \\ \cline{1-1}  & Gradient Steps & \(5\times 10^{5}\) \\ \hline \multirow{3}{*}{Architecture} & Actor & input-256-256-output \\ \cline{1-1}  & Critic & input-256-256-1 \\ \cline{1-1}  & Dynamics & input-256-256-256-256-output \\ \hline \hline \end{tabular}
\end{table}
Table 2: Hyperparameters in SCAS.

Our evaluation criteria follow those used in most previous works. For the Gym locomotion tasks, we average returns over 10 evaluation trajectories and 5 random seeds, while for the Ant Maze tasks, we average over 100 evaluation trajectories and 5 random seeds. The reported results are the normalized scores, which are offered by the D4RL benchmark [9] to measure how the learned policy compared with random and expert policy:

\[\text{D4RL score}=100\times\frac{\text{learned policy return}-\text{random policy return}}{\text{expert policy return}-\text{random policy return}}\]

The results of baselines reported in Table 1 are obtained as follows. We re-run OSR [22] on all datasets using their official codebase5 and tune the hyperparameters for each dataset as specified in their paper. We implement SDC [75] and re-run it on all datasets. We use the SDC-related hyperparameters as specified in their paper, and sweep the CQL-related hyperparameters in {1,2,5,10,20} for each dataset. We re-run OneStep RL [5] on all datasets using their official codebase6 and the default hyperparameters. We implement BC [48] based on the TD3+BC repository7 and re-run it on all datasets. The results of other baselines are taken from [3] and [68]. The runtime in Table 1 is obtained by running offline RL algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090.

Footnote 5: https://github.com/Jack10843/OSR

Footnote 6: https://github.com/davidbrandfonbrener/onestep-rl

Footnote 7: https://github.com/sfujim/TD3_BC

Figures 1(a) to 1(d) share the same embedding function obtained by running t-SNE on the set of all 200,000 samples (50,000 samples each from the dataset, CQL, TD3+BC, and SCAS). This ensures a clear visual comparison. Figure 1(d) contains all the 200,000 samples, which is the union of the points in Figures 1(a) to 1(c).

## Appendix E Additional Experimental Results

### Additional Value Estimation Results

Under the same setting of Figure 2, we conduct experiments on the additional datasets. The results are shown in Figure 5. We omit the Q values of Off-policy RL, SDC w/o CQL, and OSR w/o CQL at higher numbers of optimization steps, because these Q values diverge in the early learning stage, and plotting their Q values at later optimization steps would result in an excessive range on the vertical axis. The additional results also show that only SCAS's OOD state correction term can achieve OOD action suppression and prevent value over-estimation.

### Additional Results on OOD State Correction

To further examine the OOD state correction effects of SCAS, we conduct experiments on a modified D4RL maze2d-open-v0 [9]. It is a 2D point robot navigation task in a rectangle map with vertices \((0,0)\) and \((3,5)\). The agent needs to reach the goal at \((2,3)\). We modify the dataset by removing all

Figure 5: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Here Off-policy RL is SCAS with weight \(\lambda=0\) in Eq. (20). Only SCASâ€™s OOD state correction term can achieve OOD action suppression and prevent value over-estimation.

the transitions containing states in a rectangle with vertices \((0,0)\) and \((1.5,2.5)\). During test, we let the initial state be randomly distributed in this OOD region. We train algorithms over \(10^{6}\) gradient steps and average returns over 1000 evaluation trajectories.

The results of BC [48], TD3+BC [10], CQL [31], MOPO [73], IQL [29], and SCAS are reported in Table 3. With the OOD state correction signals, SCAS corrects the agent out of the OOD region more quickly and stably, achieving significantly better performance than typical offline RL methods.

### Comparisons on the NeoRL Benchmark

We also evaluate SCAS on the NeoRL benchmark [49]. NeoRL is a benchmark designed to simulate real-world scenarios by collecting datasets using a more conservative policy, aligning closely with realistic data collection scenarios. The narrow and limited data makes it challenging for offline RL algorithms. The results are shown in Table 4. The results of baselines are taken directly from the MOBILE paper [59]. According to Appendix C in [59], these results are obtained by tuning hyperparameters per dataset. For SCAS, we use the same fixed set of hyperparameters as specified in Appendix D. Without additional hyperparameter tuning, SCAS still performs comparably to MOBILE and outperforms other baselines in total scores.

### Comparisons with Additional Baselines

The original SCAS requires only one single hyperparameter configuration in implementations. For a fair comparison with DW [19], EDAC [2], RORL [72], SQL [71], and EQL [71], we roughly select \(\lambda\) from {0.025, 0.25} for each dataset, referring to this variant as SCAS-ht. The results of SCAS-ht and these methods are reported in Table 5. Among the ensemble-free methods, SCAS-ht achieves the highest performance in both mujoco locomotion and antmaze domains. Compared with ensemble-based methods, SCAS-ht also performs better on antmaze tasks. DW [19] reweights ID data points by their values for behavior regularization and does not account for OOD states during the test phase. In contrast, our approach considers an OOD state correction scenario, resulting in enhanced robustness during the test phase and better performance.

### Results of Combining SCAS Regularizer into Various Offline RL Objectives

The SCAS regularizer is compatible with various offline RL objectives. We conduct experiments to combine SCAS with CQL [31], IQL [29], and TD3BC [10]. Comparisons between these combined

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & BC & TD3BC & CQL & EDAC & MOPO & MOBILE & SCAS \\ \hline Hopper-High & 43.1 & 75.3 & 76.6 & 52.5 & 11.5 & 87.8 & **100.5\(\pm\)7.8** \\ Hopper-Med & 51.3 & 70.3 & 64.5 & 44.9 & 1.0 & 51.1 & **94.6\(\pm\)9.3** \\ Hopper-Low & 15.1 & 15.8 & 16.0 & 18.3 & 6.2 & 17.4 & **19.7\(\pm\)1.2** \\ Walker2d-High & 72.6 & 69.6 & **75.3** & **75.5** & 18.0 & **74.9** & **74.6\(\pm\)0.7** \\ Walker2d-Med & 48.7 & 58.5 & 57.3 & 57.6 & 39.9 & 62.2 & **63.4\(\pm\)1.0** \\ Walker2d-Low & 28.5 & 43.0 & **44.7** & 40.2 & 11.6 & 37.6 & 34.4\(\pm\)1.3 \\ HalfCheetah-High & 71.3 & 75.3 & 77.4 & 81.4 & 65.9 & **83.0** & 77.0\(\pm\)0.5 \\ HalfCheetah-Med & 49.0 & 52.3 & 54.6 & 54.9 & 62.3 & **77.8** & 53.1\(\pm\)0.1 \\ HalfCheetah-Low & 29.1 & 30.0 & 38.2 & 31.3 & 40.1 & **54.7** & 31.5\(\pm\)0.2 \\ \hline total & 408.7 & 490.1 & 504.6 & 456.6 & 256.5 & **546.5** & **548.7** \\ \hline hyperparameter tuning & **w/o** & w/ & w/ & w/ & w/ & w/ & **w/o** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Averaged normalized scores on the NeoRL benchmark over four random seeds.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & BC & TD3+BC & CQL & MOPO & IQL & SCAS \\ \hline Steps out of OOD & 84.7\(\pm\)44.7 & 58.0\(\pm\)35.7 & 63.8\(\pm\)33.0 & 50.6\(\pm\)25.4 & 37.7\(\pm\)6.7 & **22.8\(\pm\)3.1** \\ D4RL score & 38.5\(\pm\)25.4 & 63.9\(\pm\)39.3 & 41.2\(\pm\)42.0 & 110.1\(\pm\)78.8 & 335.0\(\pm\)114.9 & **571.9\(\pm\)2.7** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparisons in modified maze2d-open-v0 over five random seeds.

algorithms and the original ones are shown in Table 6. We find that applying the SCAS regularizer leads to improved performance for these popular algorithms, which could be attributed to the OOD state correction effects of SCAS. However, we also find that these combined methods do not achieve better performance than the original SCAS (comparable on most tasks and worse on some tasks). We hypothesize that this is because SCAS already has the effect of OOD action suppression, and when combined with offline RL objectives that also aim for OOD action suppression, it may become overly conservative. As a result, the combined algorithms may perform worse than the original SCAS on some sub-optimal datasets.

### Additional Parameter Study Results

In this section, we present additional parameter study results conducted on four challenging Antmaze tasks, including antmaze-large-play-v2, antmaze-large-diverse-v2, antmaze-medium-play-v2, and antmaze-medium-diverse-v2.

**Inverse Temperature \(\alpha\).** The inverse temperature \(\alpha\) is the key hyperparameter in SCAS for achieving value-aware OOD state correction. It controls the significance of the values of next states in SCAS's

\begin{table}
\begin{tabular}{l|c c|c c|c c|c} \hline \hline Dataset & \multirow{2}{*}{CQL} & \multicolumn{2}{c|}{CQL} & \multirow{2}{*}{TD3BC} & \multicolumn{2}{c|}{TD3BC} & \multicolumn{2}{c|}{IQL} & \multirow{2}{*}{SCAS} \\  & & +SCAS & & +SCAS & & +SCAS & \\ \hline halfcheetah-med & **47.0** & 46.5 & **48.3** & 44.1 & **47.4** & 46.8 & 46.6 \\ hopper-med & 53.0 & **96.1** & 59.3 & **66.6** & 66.2 & **76.8** & **102.5** \\ walker2d-med & 73.3 & **84.9** & **83.7** & 81.9 & 78.3 & **84.0** & **82.3** \\ halfcheetah-med-rep & **45.5** & 43.6 & **44.6** & 40.5 & **44.2** & **44.2** & **44.0** \\ hopper-med-rep & 88.7 & **100.2** & 60.9 & **79.4** & 94.7 & **102.3** & **101.6** \\ walker2d-med-rep & **81.8** & 78.6 & **81.8** & 76.2 & 73.8 & **76.2** & 78.1 \\ halfcheetah-med-exp & 75.6 & **92.9** & **90.7** & 89.6 & 86.7 & **92.7** & 91.7 \\ hopper-med-exp & 105.6 & **108.2** & 98.0 & **108.9** & 91.5 & **101.9** & **109.7** \\ walker2d-med-exp & **107.9** & 104.3 & **110.1** & 106.0 & **109.6** & 105.4 & 108.4 \\ \hline total & 678.4 & **755.5** & 677.4 & **693.2** & 692.4 & **730.3** & **764.9** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparisons on the D4RL benchmark. Here +SCAS means adding the SCAS regularizer. The results are averaged over 5 random seeds.

\begin{table}
\begin{tabular}{l|c c c c c c|c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{6}{c}{Ensemble-free} & \multicolumn{3}{c}{Ensemble-based} \\ \cline{2-9}  & DW+CQL & DW+IQL & SQL & EQL & DQL & SCAS-ht & EDAC & RORL \\ \hline halfcheetah-med & 46.5 & 47.7 & 48.3 & 47.2 & 51.1 & **58.5\(\pm\)1.1** & 65.9 & **66.8** \\ hopper-med & 66.1 & 62.5 & 75.5 & 74.6 & 90.5 & **102.5\(\pm\)0.3** & 101.6 & **104.8** \\ walker2d-med & 82.1 & 80.8 & 84.2 & 83.2 & 87 & **90.8\(\pm\)2.6** & 92.5 & **102.4** \\ halfcheetah-med-rep & 45.1 & 44.6 & 44.8 & 44.5 & 47.8 & **52.9\(\pm\)1.4** & 61.3 & **61.9** \\ hopper-med-rep & 88.6 & 79.7 & 99.7 & 98.1 & 101.3 & **101.6\(\pm\)1.0** & 101.0 & **102.8** \\ walker2d-med-rep & 75.3 & 65.1 & 81.2 & 76.6 & **95.5** & 88.1\(\pm\)4.2 & 87.1 & **90.4** \\ halfcheetah-med-exp & 86.1 & 93.7 & 94.0 & 90.6 & **96.8** & 91.7\(\pm\)2.7 & 106.3 & **107.8** \\ hopper-med-exp & 92.9 & 81.0 & **111.8** & 105.5 & 111.1 & 109.7\(\pm\)3.5 & 110.7 & **112.7** \\ walker2d-med-exp & 109.7 & 109.7 & 110.0 & 110.2 & 110.1 & **110.8\(\pm\)1.0** & 114.7 & **121.2** \\ locomotion total & 692.4 & 664.8 & 749.5 & 730.5 & 791.2 & **806.6** & 841.1 & **870.8** \\ \hline antmaze-umaze & 72.7 & 81.3 & 92.2 & 93.2 & **93.4** & 90.4\(\pm\)3.6 & 0.0 & **96.7** \\ antmaze-umaze-div & 34.0 & 61.0 & **74.0** & 65.4 & 66.2 & 66.4\(\pm\)14.3 & 0.0 & **90.7** \\ antmaze-med-play & 4.0 & 78.7 & 80.2 & 77.5 & 76.6 & **83.6\(\pm\)3.1** & 0.0 & **76.3** \\ antmaze-med-div & 1.3 & 64.7 & 79.1 & 70.0 & 78.6 & **84.6\(\pm\)5.0** & 0.0 & **69.3** \\ antmaze-large-play & 2.0 & 40.0 & 53.2 & 45.6 & 46.4 & **59.4\(\pm\)5.0** & 0.0 & **16.3** \\ antmaze-large-div & 0.0 & 42.0 & 52.3 & 42.5 & **56.6** & 56.2\(\pm\)5.4 & 0.0 & **41.0** \\ \hline antmaze total & 114.0 & 367.7 & 431.0 & 394.2 & 417.8 & **440.6** & 0.0 & **390.3** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparisons with additional baselines on the D4RL benchmark. Here SCAS-ht means SCAS with slight hyperparameter tuning, selecting \(\lambda\) from \(\{0.025,0.25\}\). The results of SCAS-ht are averaged over 5 random seeds and the others are taken from their papers.

OOD state correction. If \(\alpha=0\), the effect corresponds to vanilla OOD state correction. As \(\alpha\) gets larger, SCAS is more inclined to correct the agent to the high-value ID states. Thus we can assess the effectiveness of value-aware OOD state correction compared to vanilla OOD state correction by varying \(\alpha\). Here we test SCAS with different \(\alpha\) and the results are shown in Figure 6. We observe that a large \(\alpha\) is crucial for achieving good performance on all the antmaze tasks, clearly demonstrating the effectiveness of our _value-aware_ OOD state correction. However, too large \(\alpha\) (\(\alpha=10\)) induces less satisfying performance, probably due to the increased variance of the learning objective. In general, we find that choosing \(\alpha=5\) leads to the best performance.

**Balance Coefficient \(\lambda\).** The balance coefficient \(\lambda\) controls the balance between vanilla policy improvement and SCAS regularization. If we set \(\lambda=0\), SCAS degenerates into the vanilla off-policy RL algorithm. Here we vary \(\lambda\) in \(\{0,0.25,0.5,0.75,1\}\) and present the corresponding learning curves of SCAS in Figure 7. Notably, SCAS is able to converge to good performance over a very wide range of \(\lambda\). However, if \(\lambda=0\), the vanilla off-policy RL suffers from extrapolation error and overestimation, demonstrating poor performance. We also observe a very interesting fact that even when \(\lambda=1\) and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states.

**Noise Scale \(\sigma\).** The noise scale \(\sigma\) is the standard deviation of the Gaussian noise added to the original states for formulating the SCAS regularizer. Here we test SCAS with different \(\sigma\) and present the corresponding learning curves in Figure 8. We observe a significant performance drop with too large \(\sigma\) (\(\sigma=1\)) on all the tasks, due to the heavily corrupted learning signal. On the other hand, when \(\sigma=0\) (without noise), the performance is also less satisfying. With \(\sigma=0\), SCAS is still able to prevent the agent at ID states from entering OOD states, maintaining the agent in safe regions, but it cannot correct the agent from OOD states to ID states as reliably as the original SCAS. In general, we find that choosing \(\sigma=0.001\) or \(0.01\) leads to the best performance.

Figure 6: Additional results from the parameter study on the inverse temperature \(\alpha\). The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

Figure 7: Additional results from the parameter study on the balance coefficient \(\lambda\). The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

[MISSING_PAGE_EMPTY:26]

performed by humans, like factory automation or autonomous driving. Addressing these challenges will contribute to the responsible development and deployment of offline RL algorithms.

From an academic standpoint, this research systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression. This work potentially offers researchers a new perspective on analyzing the OOD state issue and enhancing test-time robustness in offline RL. Besides, SCAS also holds the promise to be extended to safe RL [1, 17, 13], meta RL [8, 65, 66, 64, 4], and multi-agent RL [35, 52, 55, 51, 16].

Figure 10: Learning curves of SCAS on Gym locomotion tasks. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

Figure 11: Learning curves of SCAS on AntMaze tasks. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please refer to Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Please refer to the code in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results in the paper are accompanied by standard deviations across multiple seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use are explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is well documented and anonymized. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.