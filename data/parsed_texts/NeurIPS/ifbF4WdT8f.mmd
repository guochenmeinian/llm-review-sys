# EvoPrompting: Language Models for Code-Level Neural Architecture Search

 Angelica Chen

New York University

angelica.chen@nyu.edu

Work done while a Student Researcher at Google DeepMind.

David M. Dohan

OpenAI

david@ddohan.com

Work done while at Google DeepMind.

Darid R. So

Jane Street

david.r.so.ai@gmail.com

Work done while a Student Researcher at Google DeepMind.

###### Abstract

Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as general adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design _novel_ architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.

## 1 Introduction

Scaling of Transformers (Vaswani et al., 2017) has produced language models (LM) with impressive performance. Beyond achieving state-of-the-art results on conventional natural language processing tasks, these LMs demonstrate breakthrough technical capabilities, such as learning how to code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving reasoning problems (Wei et al., 2022). Yet, despite these strides, several works have noted LMs' current limitations in solving complex problems and creating novel solutions (Qian et al., 2022; Dakhel et al., 2022). In this work, we improve upon a base LM's ability to propose novel and diverse solutions to complex reasoning problems by iteratively evolving in-context prompts and prompt-tuning the LM. We call this technique EvoPrompting and demonstrate its success on the difficult task of deep learning architecture design. Our key finding is that, while LMs perform poorly at designing novel and effective neural architectures via naive few-shot prompting, EvoPrompting enables LMs to create novel and effective deep neural architectures, particularly when combined with prompt-tuning methods.

EvoPrompting is based on the recently popularized practice of in-context prompting. Prompting is the technique of conditioning a LM's decoded output on a custom prefix known as a _prompt_, which can include natural language task instructions or a few input-output examples. The prompt is used only at inference time and requires no gradient updates (Brown et al., 2020). In past work, prompting has been demonstrated to elicit impressive performance on a wide variety of tasks without requiring task-specific fine-tuning (Sanh et al., 2021; Wei et al., 2022; Kojima et al., 2022). Here, we leverage LM prompting for the task of designing improved deep learning architectures.

To engineer adequately powerful prompts, we draw inspiration from existing ideas in the field of neural architecture search. There, evolution has long been used to search over discrete spaces to efficiently discover improved deep learning architectures (Yao, 1999; Real et al., 2017). However, evolutionary approaches typically require careful manual design of a discrete search space (_e.g._ a small set of known convolutional neural network components, as in Real et al. (2017) or TensorFlow primitives, as in So et al. (2021)). As a result, the performance of the evolutionary algorithm is then sensitive to and possibly limited by the design of the search space. In EvoPrompting the LM's vocabulary replaces the search space, which both increases the flexibility of the search and reduces reliance on manual design. The LM is also an _adaptive_ mutation/crossover operator, in the sense that it can be improved round over round via prompt-tuning. Furthermore, EvoPrompting also improves on naive few-shot prompting by using an evolutionary search approach to iteratively improve the in-context examples for few-shot prompting.

To demonstrate the effectiveness of this method, we first do extensive testing and analyses on the relatively low-compute problem of MNIST-1D (Greydanus, 2020). The key finding of these experiments is that EvoPrompting is capable of producing conventional convolutional architectures superior to published manually designed models (Section 4.1). In Section 4.2 we then apply our method to the more challenging task of designing graph neural networks using problems from the CLRS Algorithmic Reasoning Benchmark (Velickovic et al., 2022), where EvoPrompting generates novel architectures that outperform state-of-the-art models on 21 out of 30 algorithmic reasoning tasks (Appendix 3).

The contributions of this work are summarized as follows:

1. We propose EvoPrompting, a method that utilizes evolutionary search to create and curate data to improve LM in-context prompting examples. Although this work focuses on the specific task of neural architecture design to develop this method, EvoPrompting is generally applicable to LM tasks that rely on in-context learning (ICL) or prompt-tuning.
2. A study applying LMs to code-level neural architecture design. Our experiments demonstrate that applying few-shot prompting alone to neural architecture design is unsuccessful, but few

Figure 1: An overview of EvoPrompting. After _initializing_ the search with a handful of manually designed program seeds, the meta-learning loop begins. First, our code-pretrained LM uses the seeds as in-context prompt examples to _generate_ candidate architectures. Those candidate architectures are then _trained_ on the task training data and _evaluated_ on the task validation set. Next, the most fit members of the population are _selected_ as in-context examples for the next meta-learning loop and all evaluated individuals are used as training data for prompt-tuning the LM. From there, the meta-learning loop begins again.

shot prompting with EvoPrompting enables LMs to create architectures that outperform those designed by human experts.
3. Novel graph neural network architectures that were discovered using EvoPrompting. These architectures outperform the current state-of-the-art architecture, TripletGMPNN (Ibarz et al., 2022), on 21 out of 30 CLRS Algorithmic Reasoning Benchmark tasks (Appx. 3).

## 2 Related Work

LMs for code generationScaling Transformers (Vaswani et al., 2017) is currently a popular route for reliably creating state-of-the-art natural language systems (Brown et al., 2020; Du et al., 2021; BigScience Workshop et al., 2022; Zhang et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). Many works have observed that large LMs are capable of performing technical tasks such as writing code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving complex reasoning problems (Wei et al., 2022). Our work is most closely related to efforts that have applied LMs to coding tasks (Chen et al., 2021; Odena et al., 2021; Xu et al., 2022; Wang et al., 2021; Ahmad et al., 2021; Feng et al., 2020), since our technique proposes architectures in code.

PromptingBrown et al. (2020) demonstrated that LMs can be prompted with in-context examples to steer LM decoding towards solving problems in-context without gradient updates. Numerous works have utilized this prompting to further boost LM abilities (Sanh et al., 2021; Wei et al., 2022; Kojima et al., 2022). Others have focused on optimizing these prompts (Min et al., 2022; Liu et al., 2021) as via approaches such as augmentation with retrieval systems (Rubin et al., 2021), permutations of few-shot examples (Lu et al., 2021; Zhao et al., 2021), generating prompts via LMs (Zhou et al., 2022), and instruction-tuning (Wei et al., 2021; Ouyang et al., 2022; Sanh et al., 2021). From the perspective of Dohan et al. (2022), prompts are parameters that can be tuned using probabilistic inference techniques. Brooks et al. (2022) proposes using few-shot prompts to implement both the rollout policy and world model of a policy iteration algorithm. Our EvoPrompting method extends these efforts by proposing evolutionary search as a means to both better design prompts for ICL and tune the base LM to use the prompt more effectively.

Evolutionary AlgorithmsOur method is closely related to evolutionary neural architecture search (NAS) (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019; Liu et al., 2020), in which architectures are represented as discrete DNAs, and evolved and filtered based on fitness metrics that assess architecture performance. However, our method can search over arbitrary strings of code, whereas conventional evolutionary NAS algorithms rely on hand-crafted search spaces that can strongly bias and contrain the search (Li and Talwalkar, 2019; Sciuto et al., 2019; Bender et al., 2020; Real et al., 2020; So et al., 2021). A work close to ours is Lehman et al. (2022), in which an LM is fine-tuned to produce Python code diffs given one of three fixed messages that describe what should be changed, and then used as the mutation operator in an evolutionary algorithm. Their work is validated on the Sodarace domain. Our work differs in that we use an LM as a crossover operator, without specifying the class of changes to make, which may offer greater flexibility. Furthermore, we evaluate our approach on the real-world task of NAS, rely on mixed temperature sampling of the LM for diversity instead of using a QD algorithm, and also use prompt-tuning in our algorithm. We choose not to use a QD algorithm such as MAP-Elites since this approach requires the design and discretization of a descriptor space, which is complex and difficult to hand-design for the space of all possible neural networks.

Another concurrent work is Meyerson et al. (2023), which uses an LM as a crossover operator to produce variations of text-based genotypes in the domains of symbolic regression, text sentiment, images, and Sodaracer programs. Like Lehman et al. (2022), they use MAP-Elites to trade off quality with diversity in two of the domains and demonstrate that their overall algorithm reliably produces a diverse range of outputs. They additionally demonstrated performance comparable to state-of-the-art approaches on the toy task of symbolic regression. Their study varies from ours in a number of ways - we apply our algorithm to the real-world task of NAS, we optimize for a tradeoff between state-of-the-art task performance and model size, we condition on target performance in our prompts, we do not use MAP-Elites, and we use prompt-tuning to iteratively improve the LM's crossover abilities instead.

EvoPrompting Method

### Architecture search problem formulation

Let our target task be denoted by \(\mathcal{T}\) and \(\mathcal{D}\) be a dataset consisting of input-output pairs \((x,y)\in\mathcal{D}\) for task \(\mathcal{T}\). Define the probability distribution \(\pi_{\theta}:\mathcal{V}\rightarrow\{0,1\}\) over vocabulary \(\mathcal{V}\) as a language/code model parameterized by \(\theta\), from which we can sample code segments \(c\in\mathcal{V}^{*}\) (for \(\mathcal{V}^{*}\) the Kleene closure of \(\mathcal{V}\), _i.e._ the set of all concatenations of symbols in \(\mathcal{V}\)). We also have an evaluation function \(\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D}):\mathcal{V}^{*}\times\mathcal{D} \rightarrow\mathbb{R}\) that trains the model architecture given by code \(c\) on \(\mathcal{D}\) and outputs some real-valued fitness score \(s\in\mathbb{R}\), which can be a function of model accuracy and other model characteristics. Our ultimate goal is to identify some set of code samples \(c\sim\mathcal{V}^{*}\) that define neural network architectures that, when trained on \(\mathcal{D}\), maximize the reward \(\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D})\).

### LMs for evolutionary crossover and mutation

The goal of our algorithm is to generate a set \(C\) consisting of \(k\) neural network architectures that maximize the reward \(\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D})\) for arbitrary pairs of \((\mathcal{D},\mathcal{T})\):

\[\arg\max_{\begin{subarray}{c}C=\{c\mid c\sim\pi_{\theta}\}\\ \mid C\mid=k\end{subarray}}\mathbb{E}_{c\in C}\mathbb{E}_{(x,y)\in\mathcal{D}} \left[\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D})\right]\] (1)

Since this optimization problem is generally intractable, we turn to a black-box evolutionary approach for iteratively generating, scoring, and selecting the best neural network architectures. Indeed, evolution has been demonstrated to perform particularly well in this domain because of how sparse high quality solutions tend to be (Real et al., 2017, 2018). Although evolution has been used for architecture search many times before (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019), we improve upon this approach by using an LM for crossover and mutation operations.

Using an LM in this manner has multiple appealing properties. While past evolutionary approaches for neural architecture search have required careful design and specification of a discrete search space (_e.g._ the space of high level modules (Real et al., 2018; So et al., 2019), TensorFlow statements (So et al., 2021), or basic mathematical operations (Real et al., 2020)), our algorithm's search space includes any neural network architecture that can be represented in Python. This allows for greater flexibility and diversity of the output architectures, and reduces the amount of manual design and human bias involved in the algorithm. Furthermore, modern pre-trained LMs are typically trained on massive datasets containing a significant number of source code files. This pre-training process encodes useful knowledge about code structure and functionality that is not otherwise available in evolutionary algorithms. Lastly, LMs can also be used as _self-adaptive crossover operators_, in which the crossover operator is incrementally trained round after round to generate higher reward crossovers.

### EvoPrompting meta-learning algorithm

Our complete algorithm is described in Algorithm 1. At the core of our algorithm is a scoring function, which describes the general "fitness" of a model on the task at hand. Since higher accuracy can often be achieved simply by increasing the number of parameters in a model, we use the negative product of the validation error and the model size as the fitness (see step 6 in Algorithm 3). More complicated objective functions have previously been used for dual objective neural architecture search (Bender et al., 2020), but we find this simple product works best in our case and requires minimal tuning. Generally the higher the fitness, the better (with some caveats, noted in our description of fitness-based selection below).

The end-to-end meta-learning algorithm has several stages, which we describe below:

InitializationWe start by setting our global historical population \(G\) to the empty list and initializing our current population \(P\) with a few seed architectures that are known to be well-designed (step 3 in Algorithm 1), which _warm-starts_ the search (So et al., 2019). These seed models are evaluated using the same \(\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D})\) function that is used to evaluate new candidate models (see below).

```
1:Input: LM \(\pi_{\theta_{0}}\), dataset \(\mathcal{D}\), task \(\mathcal{T}\), \(T\) number of rounds, \(m\) number of few-shot prompts per round, \(n\) number of samples to generate per prompt, \(k\) number of in-context examples per prompt, \(p\) number of survivors to select per generation, \(\alpha\) the upper threshold for the test error
2:\(G\leftarrow[]\)
3:\(P\leftarrow\textsc{InitializePopulation}(p)\)
4:\(t\gets 0\)
5:while\(t<T\)do
6:\(C\leftarrow\textsc{CrossMut}(\pi_{\theta_{t}},P,m,k,n)\)
7:\(C_{\textsc{Evaled}}\leftarrow\textsc{FilterAndEval}(C,\mathcal{T},\mathcal{D},\alpha)\)
8:\(G\gets G+C_{\textsc{Evaled}}\)
9:if\(t<T-1\)then
10:\(P\leftarrow\textsc{GetTop}(G,p)\)
11:\(\theta_{t+1}\leftarrow\textsc{Train}(\theta_{t},C_{\textsc{Evaled}}\setminus P)\)
12:endif
13:\(t\gets t+1\)
14:endwhile
15:Return \(\textsc{GetTop}(G,p)\) ```

**Algorithm 2** The crossover and mutation algorithm, \(\textsc{CrossMut}(\pi_{\theta_{t}},P,m,k,n)\), where \(\textsc{Uniform}(P)\) denotes the uniform distribution over the set \(P\). The set of potential parents \(P\) consists of the top examples from the previous round.

```
1:Input: LM \(\pi_{\theta}\), population of code samples and fitnesses \(P=\{(c,s)\,|\,c\in\mathcal{V}^{*},\textsc{Eval}_{\mathcal{T}}(c,\mathcal{D})=s\}\), \(m\) number of few-shot prompts to create, \(k\) number of in-context examples in each prompt, and \(n\) number of samples to sample per prompt.
2:\(C\leftarrow[]\)
3:\(i\gets 0\)
4:while\(i<m\)do
5:\(E\leftarrow\{x_{j}\}_{j=1}^{k}\), where \(x_{j}\overset{i.i.d.}{\sim}\textsc{Uniform}(P)\)
6:\(p\leftarrow\textsc{MakeFewShotPrompt}(E)\)
7:\(C_{i}\leftarrow\{c_{j}\}_{j=1}^{n}\), where \(c_{j}\overset{i.i.d.}{\sim}\pi_{\theta}(\cdot\,|\,p)\)
8:\(C\gets C+C_{i}\)
9:\(i\gets i+1\)
10:endwhile
11:Output:\(C\) ```

**Algorithm 3** The algorithm for filtering and scoring child models, \(\textsc{FilterAndEval}(C,\mathcal{T},\mathcal{D},\alpha)\).

Crossing over and mutating the parent modelsTo mutate and apply crossover to the parents \(P\) selected in the last step, we use both the source code and the evaluation metrics of each model in \(P\) to create few-shot prompts.

In the last line of the prompt, we create a target set of metrics to condition \(\pi_{\theta}\)'s generations on that indicate the desired validation accuracy and model size of the proposed architecture. We set the target model size as \(90\%\) of the minimum model size of the parent models, rounded to the nearest 100 parameters, and the target validation accuracy as \(102\%\) of the maximum validation accuracy of the parent models, rounded to the nearest tenth of a percent. We create \(m\) such prompts per round, each with \(k\) in-context examples selected uniformly at random from \(P\). An example of a prompt is shown in Listing 1.

```
1"""Metrics:
2{'num_params':'4800','val_accuracy':'0.865'}
3"""
4classModel(nn.Module):
5@nn.compact
6def__call__(self,x):
7x=nn.Dense(features=10)(x)
8returnx
9
10"""Metrics:
1{'num_params':'4300','val_accuracy':'0.880'}
11"""
12classModel(nn.Module): ```

Listing 1: The format of our few-shot prompts. In practice we use 2-shot prompts but we omit the second in-context example here for brevity.

Finally, we use \(\pi_{\theta}\) to generate \(n\) samples per prompt, yielding a total of \(n\times m\) child samples per round of evolution. We denote this portion of the algorithm as CrossMut\((\pi_{\theta_{1}},P,m,k,n)\) (Algorithm 2 and step 6 of Algorithm 1).

Filtering and scoring child samplesTo score and filter child samples \(c\) generated by \(\pi_{\theta}\), we use the evaluation function Eval\({}_{\mathcal{T}}(c,\mathcal{D})\), which trains the model encoded by \(c\) on the dataset \(\mathcal{D}\) and returns the lowest validation error encountered during training. All child models are trained for the same number of steps, with the same optimizer hyperparameters. Since our fitness function can potentially be gamed by generating arbitrarily small models, we also add a validation error threshold \(\alpha\), which is the upper limit of the validation error that a model can incur without being removed from \(G\), the global population. We refer to this function as FilterAndEval\((C,\mathcal{T},\mathcal{D},\alpha)\) (Algorithm 3 and step 7 of Algorithm 1). Lastly, we add the remaining trainable models and their associated fitness scores into \(G\) (step 8 of Algorithm 1).

Fitness-based selectionAfter evaluating all child models in the current round, we apply fitness-based selection to identify top candidate models for crossover (step 10 of Algorithm 1). We denote this as GetTop\((G,p)\), which refers simply to selecting the \(p\) models with the highest fitness scores from \(G\). Once these models have been selected, they are permanently removed from the population and cannot be used again as parents for crossover.

Training \(\pi_{\theta_{1}}\)Lastly, all child models generated in the current round that were not previously selected for crossover (_i.e._\(C_{\textsc{Evaled}}\setminus P\)) are used to prompt-tune \(\pi_{\theta}\) for the next round (step 11 of Algorithm 1).

## 4 Experiments and Results

We evaluate our meta-learning algorithm on two datasets - MNIST-1D (Greydanus, 2020) and the CLRS algorithmic reasoning benchmark (Velickovic et al., 2022). While the former benchmark is lightweight and permits us to do a more thorough analysis of our algorithm, the latter is a newer benchmark that covers 30 different algorithms with more headroom for discovering novel architectures with better performance.

In all of our experiments, our \(\pi_{\theta_{0}}\) (_i.e._ the crossover operator) is a 62B parameter PALM model (Chowdhery et al., 2022) pre-trained on 1.3T tokens of conversational, web, and code documents. It was additionally fine-tuned on a corpus of 64B tokens containing near-deduplicated, permissively-licensed Python source code files from Github. We always sample from \(\pi_{\theta_{0}}\) with mixed temperature sampling, in which the sampling temperature is selected uniformly from \([0.2,0.6,0.8,1.0]\). Between each round, the model is prompt-tuned (Lester et al., 2021) for 5 epochs with a soft prompt length of 16, batch size of 16, and learning rate of 0.1 (as described in Section 3.3 and Step 11 of Algorithm 1). Unless stated otherwise, we run 10 rounds of evolution with 10 prompts per round and 16 samples generated per prompt, yielding a total of 160 models generated per round and 1600 models generated during the entire search. Duplicate models and un-trainable models are not scored, but do count into the 1600. All other EvoPrompting hyperparameters are listed in Appendix A.1.

### Mnist-1d

DatasetWe apply our method first to MNIST-1D (Greydanus, 2020), a one-dimensional, scaled-down version of the MNIST-1D dataset containing examples that are 20 times smaller than the original MNIST dataset. Each example is only 40-dimensional, with 4000 examples in the training dataset and 1000 in test. Since there is no validation dataset, we randomly set aside 500 examples from the training dataset to use as the validation dataset. Despite being more lightweight, MNIST-1D distinguishes more between different architecture types (Greydanus, 2020) than its larger counterpart MNIST (LeCun et al., 1998).

Meta-learning set-upThroughout the model search we use the AdamW optimizer (Loshchilov and Hutter, 2019) to train each child model on a single NVIDIA Tesla P100 GPU for 8000 steps, with learning rate 0.01 and batch size 128. We score child models according to the best validation accuracy achieved during training. We also seed the search with 4 seed models - the 3 hand-designed neural baselines from the original MNIST-1D paper (Greydanus, 2020) (GRU, CNN, and MLP) and a fourth, larger CNN model of our own design. All four are implemented with Flax (Heek et al., 2020). We refer the reader to Appendix A.2 for the source code of these seed models.

BaselinesWe compare EvoPrompting with the following baselines:

* Naive few-shot prompting: This baseline simply generates code samples \(c\sim\pi_{\theta_{0}}(\cdot|p)\), where \(p\) is a 2-shot prompt constructed using in-context examples randomly selected from the seed models (Listing 1). This is essentially an ablation of steps 7-12 in Algorithm 1 with \(T=1\). We increase the number of samples generated per prompt for the naive prompting baseline such that the total number of samples generated by \(\pi_{\theta}\) matches that of the other baselines.
* prompt-tuning): We run the entire algorithm as is, but without prompt-tuning between each round. This is an ablation of step 11 from Algorithm 1
* EvoPrompting (random parents): Instead of selecting the most fit models from the last round as parents for the next round, we select parents randomly. This is an ablation of Step 10 in Algorithm 1, which is the GetTop\((G,p)\) step.

EvoPrompting finds smaller and more accurate modelsFigure 1(a) shows a comparison of the test error and model size of the top 20 models discovered by EvoPrompting compared with those of our seed models and three baselines. The points approximate a Pareto frontier, below which each algorithm cannot improve on one dimension without hurting the other. EvoPrompting possesses the Pareto frontier closest to the origin, indicating that it finds more optimal models in terms of accuracy and size. In fact, many models in EvoPrompting's top 20 discovered models are orders of magnitude smaller than those of the other baselines, while still having lower test error.

We also note that - on this task in particular - EvoPrompting excels especially at optimizing convolutional architectures. Many of the top 20 models are narrower and deeper convolutional architectures, with smaller strides, less padding, and no dense layers. These models consistently perform better than the shallower, denser, and wider convolutional architectures seen in earlier rounds of the model search.

Another important aspect of a meta-learning algorithm is the relationship between the number of individuals evaluated and the maximum fitness observed so far, _i.e._ the sample efficiency. Neural architecture search can be an expensive process, with the most open-ended searches requiring the evaluation of trillions of individuals (Real et al., 2020). Thus, it is crucial to identify fit candidatesusing as few samples as possible. Figure 1(b) compares how the fitness of the best-performing child model improves as a function of the number of child samples generated thus far. The random parents baseline plateaus the quickest, reaching a maximum fitness by the time approximately 200 individuals have been generated. Furthermore, the maximum fitness it reaches is significantly worse than that of the other experiments. On the other hand, EvoPrompting without prompt-tuning and normal EvoPrompting do not plateau until much later on. EvoPrompting's plateau is the highest and therefore fitter on average than the individuals discovered by any of the other experiments.

It is also evident from both Figure 1(a) and 1(b) that performance suffers when any individual component is removed. Interestingly, Figure 1(a) indicates that prompting with randomly selected parents combined with prompt-tuning is no more effective than naive prompting alone. This highlights the importance of selecting helpful in-context examples, particularly in a task for which we assume that less training signal exists in the pre-training data. However, selecting more fit models as in-context examples without prompt-tuning also does not perform nearly as well as our full method.

Trajectory over meta-learning roundsWe also explored the trajectory of our meta-learning algorithm round over round, as shown in Appendix A.3. In general, we observe that EvoPrompting starts out further away from the origin (in round 0) and ends up closest to the origin in round 10, which signifies that it discovers - on average - the smallest and most accurate models in the last round. However, the search does not always yield improvements on both axes between consecutive rounds. In rounds 0-2 and 6-10, EvoPrompting improves test error while trading off model size. On the other hand, both dimensions are simultaneously improved upon in rounds 3-5.

Figure 3: Number of child models generated versus maximum fitness of top model seen so far (as estimated using 100 bootstrap samples of size 20 for each point along the x-axis) when searching over neural network models for three CLRS tasks. As mentioned in Section 4.2, these algorithms were selected because our preliminary analyses indicated that they had the most headroom for architectural improvements.

Figure 2: EvoPrompting discovers smaller and better performing architectures on MNIST-1D than alternative search methods.

### Clrs

Although the MNIST-1D task offers an efficient and practical setting for evaluating a meta-learning algorithm, CNN architectures already perform fairly well on this task and neural image classification architectures have been extensively studied as a whole. There also exists the possibility that our LM has seen many convolutional architectures in its pre-training data. Instead, we turn to a different learning task and class of neural network architectures in order to assess whether our meta-learning framework generalizes to other tasks, datasets, and neural architectures.

DatasetThe CLRS algorithmic reasoning benchmark (Velickovic et al., 2022) evaluates the ability of neural networks to learn algorithmic reasoning across a set of 30 classical algorithms covered in the _Introduction to Algorithms_ textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009). This benchmark is useful not only as a difficult logical reasoning task for neural networks, but also as a measure of a neural network's _algorithmic alignment_(Xu et al., 2020). In brief, algorithmic alignment refers to a model's ability to reason like an algorithm (_i.e._ using the computation graph for a task), rather than relying upon memorization or other less sample efficient learning strategies. Although a model can approximate an algorithm by pattern-matching against similar inputs or relying on other shortcuts, it cannot generalize to arbitrarily long inputs or edge cases without learning the computation graph underlying the algorithm.

Accordingly, the CLRS benchmark represents the algorithms' inputs and outputs as graphs, and the steps of the algorithm as a _trajectory_ of operations over the input graph. This problem setup can be straightforwardly processed by graph neural networks, which is explored in Ibarz et al. (2022). They find that a Triplet-GMPNN model (a message-passing neural network (Gilmer et al., 2017) with gating and triplet edge processing) exhibits the best performance when trained and evaluated across all 30 algorithms at once.

Meta-learning set-upSimilar to our MNIST-1D set-up, we use the AdamW optimizer to train each child model on a single NVIDIA Tesla P100 GPU. However, since most of the explored child models were much larger than the MNIST-1D models, we only trained each child model for 2000 steps. Anecdotally, we observed that the performance of different models often diverged by 2000 steps, which provided sufficient signal for the model search process. We otherwise followed the hyperparameters for single-task training in Ibarz et al. (2022) and evaluated models using validation accuracy.

Unlike our MNIST-1D set-up, we only search over the triplet representations of a Triplet-GMPNN model (see Ibarz et al. (2022) for more details), rather than the entire graph processor. We also seed the search with nine different seed models - each a variant of a Triplet-GMPNN model with a different triplet representation. Each seed triplet representation incorporates a minor tweak of a single component of the original triplet representation designed by Ibarz et al. (2022). These include a fully-connected output layer, a sum aggregation, fully-connected node/edge/graph representations,

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{CLRS Task} & \multirow{2}{*}{Best Performing Model} & \multicolumn{2}{c}{Model Size \(\downarrow\)} & \multicolumn{2}{c}{OOD Accuracy \(\uparrow\)} \\  & & Ours & Baseline & Ours & Baseline \\ \hline Articulation Points & QuadNodeMinMax & 497969 & 531913 & **93.5 \(\pm\) 1.8\%** & 88.3\(\pm\) 2.0\% \\ BFS & MaxMean & 522931 & 523963 & **100.0 \(\pm\) 0.0\%** & 99.7\(\pm\) 0.0\% \\ Bubble Sort & ConcatRep & 568533 & 524477 & **88.9 \(\pm\) 2.8\%** & 67.7\(\pm\) 5.5\% \\ DFS & Div2Max & 660158 & 661190 & **68.1 \(\pm\) 1.4\%** & 47.8\(\pm\) 4.2\% \\ Floyd Warshall & ConcatRep & 669145 & 625089 & **61.4 \(\pm\) 0.8\%** & 48.5\(\pm\) 1.0\% \\ Heapsort & ConcatRep & 703710 & 659654 & **69.9 \(\pm\) 4.2\%** & 31.0\(\pm\) 5.8\% \\ Insertion Sort & Div2Mean & 523445 & 524477 & **89.5 \(\pm\) 2.6\%** & 78.1\(\pm\) 4.6\% \\ Quicksort & Div2Mean & 524727 & 525759 & **85.2 \(\pm\) 4.3\%** & 64.6\(\pm\) 5.1\% \\ Task Scheduling & TanhExpandTriplets & 262333 & 262333 & **88.2 \(\pm\) 0.4\%** & 87.3\(\pm\) 0.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: A comparison of OOD accuracy and model size (in number of parameters) of models newly discovered by EvoPrompting on select CLRS tasks where EvoPrompting has discovered more accurate architectures without large increases in model size, compared with the baseline model (the Triplet-GMPNN from Ibarz et al. (2022)). OOD accuracy numbers for the baseline model are from Ibarz et al. (2022). For the full table of results on all CLRS tasks, including accuracies of our own implementation of the Triplet-GMPNN, see Appendix 3.

a simple linear triplet representation, and a bilinear representation (Mnih and Hinton, 2007). All nine are implemented with Haiku (Hennigan et al., 2020), an object-oriented neural network library for Jax (see Appendix A.5 for the source code of the seed models.)

Generalizing beyond image classification modelsWe search using EvoPrompting on 3 individual algorithms in the CLRS benchmark - the articulation points, Graham scan, and Kruskal's minimum spanning tree algorithms. We select these algorithms because our preliminary analyses with hand-designed architectures showed that they had the most headroom for improvement, although we found that the discovered architectures transfer well to other CLRS benchmark tasks as well (Appx. 3). Our search results are shown in Figure 3. EvoPrompting continues to find models that are more "fit" than our other two baselines, though we observed that the results also show more variation than our results for MNIST-1D did.

Analyzing newly discovered modelsOur search across triplet representations yielded several new designs that we sought to evaluate across all algorithms in the CLRS benchmark. Although these new models were discovered in model searches over single algorithms, they oftentimes generalized to other algorithms that were unseen during the model search. Figure 5 shows the trajectory of validation accuracy during training and Table 1 provides OOD accuracies for these models on a few select algorithms. (We defer the reader to Appendix A.4 for the full source code of each newly discovered model and Table A.6 for the full list of OOD accuracies for every algorithm in the CLRS benchmark.)

We note that the model search suggested several simple but effective changes. For example, instead of taking the maximum of the triplet representation, the QuadNodeMinMax model uses quadruplet node representations instead of triplets, and it subtracts the minimum of the quad representation from the max instead. ConcatRep represents the node, edge, and graph representations as a concatenation of a projection feedforward layer, and MaxMean takes the maximum of the triplet representations prior to taking the mean and passing it through the output dense layer. Div2Mean scales each of the node representations by \(1/2\) and uses a mean aggregation of the triplet representations instead of the max aggregation. TanhExpandTriplets applies additional dimension expansion to the triplet representations and applies a hyperbolic tangent function after the max aggregation. See Appx. A.4 for the full code of each discovered model.

Of the 5 newly discovered models that we chose to analyze, ConcatRep is the only one that increases model size. However, as shown in Table 1, ConcatRep frequently yielded improvements in OOD accuracy that far exceeded the percent increase in model size. For instance, on the heapsort algorithm ConcatRep increased OOD accuracy by 125.19% while only increasing model size by 6.68% over the baseline. The other four newly discovered models shown in Table 1 simultaneously improved OOD accuracy while decreasing model size on the articulation points, BFS, DFS, insertion sort, quicksort, and task scheduling algorithms. On the rest of the CLRS algorithms (Table A.6), our newly discovered models typically achieved OOD accuracy comparable to or better than the baseline, while maintaining similar model size.

## 5 Conclusion

We have shown that embedding a pre-trained LM in an evolutionary algorithm significantly improves the LM's performance on the task of neural architecture design. Our approach has demonstrated success at not only optimizing convolutional architectures for the MNIST-1D task, but also at developing new kinds of GNNs for the CLRS algorithmic benchmark. This demonstrates: 1) using evolutionary techniques can vastly improve the in-context capabilities of pre-trained LMs, and 2) EvoPrompting can discover novel and state-of-the-art architectures that optimize for both accuracy and model size. Furthermore, EvoPrompting is general enough to be easily adapted to search for solutions to other kinds of reasoning tasks beyond NAS. We leave the adaptation of EvoPrompting for other tasks to future work.

However, our study is limited by the lack of an extensive comparison against other standard NAS techniques because EvoPrompting was designed for open-ended search, whereas other techniques were not, which would introduce a potential confounder. We include one such comparison on NATS-Bench in Appendix A.7, as well as a discussion of the confounders thereof.

## 6 Acknowledgements

We thank Maarten Bosma, Kefan Xiao, Yifeng Lu, Quoc Le, Ed Chi, Borja Ibarz, Petar Velickovic, Chen Liang, Charles Sutton, and the Google Brain AutoML team for providing valuable discussions and feedback that influenced the direction of this project. We also thank the Google Student Researcher program for providing the resources and opportunities necessary for this project to take place.

## References

* Ahmad et al. (2021) Ahmad, W. U., Chakraborty, S., Ray, B., and Chang, K.-W. Unified pre-training for program understanding and generation. _ArXiv_, abs/2103.06333, 2021.
* Bender et al. (2020) Bender, G., Liu, H., Chen, B., Chu, G., Cheng, S., Kindermans, P.-J., and Le, Q. V. Can weight sharing outperform random architecture search? an investigation with tunas. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 14311-14320, 2020.
* BigScience Workshop (2021) BigScience Workshop, :, Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanachi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurencon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D. I., Radev, D., Ponferrada, E. G., Levkovizh, E., Kim, E., Natan, E. B., De Toni, F., Dupont, G., Kruszewski, G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Von Werra, L., Weber, L., Phan, L., allal, L. B., Tanguy, L., Dey, M., Munoz, M. R., Masoud, M., Grandury, M., Sasko, M., Huang, M., Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M. C., Jauhar, M. A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., de Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amoux, P., Lhoest, Q., Harliman, R., Bommassani, R., Lopez, R. L., Ribeiro, R., Osel, S., Pyysalo, S., Nagel, S., Bose, S., Muhammad, S. H., Sharma, S., Longpre, S., Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T. T., Schick, T., Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu, V., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si, C., Tasar, D. E., Salesky, E., Mielke, S. J., Lee, W. Y., Sharma, A., Santilli, A., Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Strobelt, H., Fries, J. A., Rozen, J., Gao, L., Sutawika, L., Bari, M. S., Al-shaibani, M. S., Manica, M., Nayak, N., Tehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S. H., Kim, T., Bers, T., Fevry, T., Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.-X., Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H. W., Tae, J., Phang, J., Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrouinte, M., Patry, N., Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallee, P. F., Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.-L., Subramonian, A., Neveol, A., Lovering, C., Garrette, D., Tunuguntla, D., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G. I., Schoelkopf, H., Kalo, J.-C., Novikova, J., Forde, J. Z., Clive, J., Kasai, J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal, O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S., Shavrina, T., Scialom, T., Yun, T., Limisiewicz, T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A., Khan, A., Faranak, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A., Tammour, A., HajiHosseini, A., Behroozi, A., Ajibade, B., Saxena, B., Ferrandis, C. M., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen, D. A., Tan, E., Baylor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J. B., Dutra, L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinlou, M., McKenna, M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An, R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S., Roy, S., Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap, A. R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A., Beilharz, B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C.,Fourrier, C., Perinan, D. L., Molano, D., Yu, D., Manjavacas, E., Barth, F., Fuhrmann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H. U., Bello, I., Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J. D., Sivaraman, K. R., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovetz, M. H., Takeuchi, M., Pamies, M., Castillo, M. A., Nezhurnia, M., Sanger, M., Samwald, M., Cullan, M., Weinberg, M., De Wolf, M., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam, N., Dahlberg, N., Broad, N. M., Mueller, N., Fung, P., Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R., Su, R., Su, R., Cahyawijaya, S., Garda, S., Deshmukh, S. S., Mishra, S., Kiblawi, S., Ott, S., Sang-aroonsiri, S., Kumar, S., Schweter, S., Bharati, S., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y. S., Venkatraman, Y., Xu, Y., Xu, Y., Yan, Z., Xie, Z., Ye, Z., Bras, M., Belkada, Y., and Wolf, T. Bloom: A 176b-parameter open-access multilingual language model, 2022. URL https://arxiv.org/abs/2211.05100.
* Brooks et al. (2022) Brooks, E., Walls, L., Lewis, R. L., and Singh, S. In-context policy iteration, 2022. URL https://arxiv.org/abs/2210.03821.
* Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.
* Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S. A., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. _ArXiv_, abs/2107.03374, 2021.
* Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsyvashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.
* Cormen et al. (2009) Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. _Introduction to Algorithms, Third Edition_. The MIT Press, 3rd edition, 2009. ISBN 0262033844.
* Dakhel et al. (2022) Dakhel, A. M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., and Jiang, Z. M. Github copilot ai pair programmer: Asset or liability? _ArXiv_, abs/2206.15331, 2022.
* Dohan et al. (2021) Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., and Sutton, C. Language model cascades, 2022. URL https://arxiv.org/abs/2207.10342.
* Dong et al. (2021) Dong, X., Liu, L., Musial, K., and Gabrys, B. NATS-Bench: Benchmarking nas algorithms for architecture topology and size. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 2021. doi: 10.1109/TPAMI.2021.3054824. doi: 10.1109/TPAMI.2021.3054824.
* Du et al. (2021) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905.
* Elsken et al. (2018) Elsken, T., Metzen, J. H., and Hutter, F. Efficient multi-objective neural architecture search via lamarckian evolution. _arXiv: Machine Learning_, 2018.
* Elsken et al. (2019)Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Codebert: A pre-trained model for programming and natural languages. _ArXiv_, abs/2002.08155, 2020.
* Volume 70_, ICML'17, pp. 1263-1272. JMLR.org, 2017.
* Greydanus (2020) Greydanus, S. Scaling *down* deep learning. _CoRR_, abs/2011.14439, 2020. URL https://arxiv.org/abs/2011.14439.
* Heek et al. (2020) Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., and van Zee, M. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax.
* Hennigan et al. (2020) Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku.
* Ibarz et al. (2022) Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Bennani, M. A., Csordas, R., Dudzik, A., Bovsnjak, M., Vitvitskyi, A., Rubanova, Y., Deac, A., Bevilacqua, B., Ganin, Y., Blundell, C., and Velickovic, P. A generalist neural algorithmic learner. _ArXiv_, abs/2209.11142, 2022.
* Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. _ArXiv_, abs/2205.11916, 2022.
* LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. _Proc. IEEE_, 86:2278-2324, 1998.
* Lehman et al. (2022) Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. Evolution through large models. _ArXiv_, abs/2206.08896, 2022.
* Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning, 2021. URL https://arxiv.org/abs/2104.08691.
* Li & Talwalkar (2019) Li, L. and Talwalkar, A. S. Random search and reproducibility for neural architecture search. _ArXiv_, abs/1902.07638, 2019.
* Liu et al. (2020) Liu, H., Brock, A., Simonyan, K., and Le, Q. V. Evolving normalization-activation layers. _ArXiv_, abs/2004.02967, 2020.
* Liu et al. (2021) Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? In _Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out_, 2021.
* Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* Lu et al. (2021) Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _Annual Meeting of the Association for Computational Linguistics_, 2021.
* Meyerson et al. (2023) Meyerson, E., Nelson, M. J., Bradley, H., Moradi, A., Hoover, A. K., and Lehman, J. Language model crossover: Variation through few-shot prompting, 2023. URL https://arxiv.org/abs/2302.12170.
* Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? _ArXiv_, abs/2202.12837, 2022.
* Mnih & Hinton (2007) Mnih, A. and Hinton, G. Three new graphical models for statistical language modelling. In _Proceedings of the 24th International Conference on Machine Learning_, ICML '07, pp. 641-648, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595937933. doi: 10.1145/1273496.1273577. URL https://doi.org/10.1145/1273496.1273577.
* Noorbakhsh et al. (2021) Noorbakhsh, K., Sulaiman, M., Sharifi, M., Roy, K., and Jamshidi, P. Pretrained language models are symbolic mathematics solvers too! _ArXiv_, abs/2110.03501, 2021.
* Nester et al. (2020)* Odena et al. (2021) Odena, A., Sutton, C., Dohan, D. M., Jiang, E., Michalewski, H., Austin, J., Bosma, M. P., Nye, M., Terry, M., and Le, Q. V. Program synthesis with large language models. In _n/a_, pp. n/a, n/a, 2021. n/a.
* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. _ArXiv_, abs/2203.02155, 2022.
* Qian et al. (2022) Qian, J., Wang, H., Li, Z., LI, S., and Yan, X. Limitations of language models in arithmetic and symbolic induction. _ArXiv_, abs/2208.05051, 2022.
* Real et al. (2017) Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Tan, J., Le, Q. V., and Kurakin, A. Large-scale evolution of image classifiers. _ArXiv_, abs/1703.01041, 2017.
* Real et al. (2018) Real, E., Aggarwal, A., Huang, Y., and Le, Q. V. Regularized evolution for image classifier architecture search. In _AAAI Conference on Artificial Intelligence_, 2018.
* Real et al. (2020) Real, E., Liang, C., So, D. R., and Le, Q. V. Automl-zero: Evolving machine learning algorithms from scratch. In _International Conference on Machine Learning_, 2020.
* Rubin et al. (2021) Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning. _ArXiv_, abs/2112.08633, 2021.
* Sanh et al. (2019) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot task generalization. _ArXiv_, abs/2110.08207, 2021.
* Sciuto et al. (2019) Sciuto, C., Yu, K., Jaggi, M., Musat, C. C., and Salzmann, M. Evaluating the search phase of neural architecture search. _ArXiv_, abs/1902.08142, 2019.
* So et al. (2019) So, D. R., Liang, C., and Le, Q. V. The evolved transformer. _ArXiv_, abs/1901.11117, 2019.
* So et al. (2021) So, D. R., Marke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Primer: Searching for efficient transformers for language modeling, 2021. URL https://arxiv.org/abs/2109.08668.
* Thoppilan et al. (2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegal, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y., Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern, K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le, Q. Lamda: Language models for dialog applications, 2022. URL https://arxiv.org/abs/2201.08239.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017. URL https://arxiv.org/abs/1706.03762.
* Velickovic et al. (2022) Velickovic, P., Badia, A. P., Budden, D., Pascanu, R., Banino, A., Dashevskiy, M., Hadsell, R., and Blundell, C. The clrs algorithmic reasoning benchmark. In _International Conference on Machine Learning_, 2022.
* Wang et al. (2021) Wang, Y., Wang, W., Joty, S. R., and Hoi, S. C. H. Coded5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. _ArXiv_, abs/2109.00859, 2021.
* Wei et al. (2021) Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. _ArXiv_, abs/2109.01652, 2021.
* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., hsin Chi, E. H., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. _ArXiv_, abs/2201.11903, 2022.

Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A systematic evaluation of large language models of code. _Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming_, 2022.
* Xu et al. (2020) Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. What can neural networks reason about? In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rJxbJeHFPS.
* Yao (1999) Yao, X. Evolving artificial neural networks. _Proc. IEEE_, 87:1423-1447, 1999.
* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068.
* Zhao et al. (2021) Zhao, T., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. _ArXiv_, abs/2102.09690, 2021.
* Zhou et al. (2022) Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. _ArXiv_, abs/2211.01910, 2022.

## Appendix A Appendix

### EvoPrompting Hyperparameters

### MNIST-1D Seed Models

Below we provide the source code for the four seed models used in the MNIST-1D model search.

```
1classModel(nn.Module):
2features:int=32
3nlayer:int=3
4
5@nn.compact
6def__call__(self,x):
7x=x[...,None]
8x=nn.Conv(features=self.features,kernel_size=(3,))(x)
9x=nn.relu(x)
10
11x=nn.avg_pool(x,window_shape=(2,),strides=(2,))
12for_inrange(self.nlayer-1):
13xp=nn.Conv(features=self.features,kernel_size=(3,),
14)(x)
15xp=nn.relu(xp)
16x=x+xp
17x=nn.avg_pool(x,window_shape=(2,),strides=(2,))
18x=x.reshape((x.shape[0],-1))#flatten
19x=nn.Dense(features=256)(x)
20x=nn.relu(x)
21x=nn.Dense(features=10)(x)
22returnx ```

Listing 2: A hand-designed convolutional model.

```
1classModel(nn.Module):
2features:int=25
3
4@nn.compact
5def__call__(self,x):
6x=x[...,None]
7x=nn.Conv(features=self.features,kernel_size=(5,),strides=(2,),padding=(1,)
8)(x)
9x=nn.relu(x)
10for_inrange(2):x = nn.Conv( features=self.features,kernel_size=(3,),strides=(2,),padding=(1,))(x) x = nn.relu(x) x = x.reshape((x.shape[0],-1)) x = nn.Dense(features=10)(x) returnx ```

Listing 3: A Flax implementation of the convolutional baseline from Greydanus (2020).

```
1classModel(nn.Module):
2"""AsimpleGRUmodel."""
3
4hidden_size:int=6
5seed:int=42
6
7@nn.compact
8def__call__(self,x):
9x=jnp.expand_dims(x,-1)
10rng=jax_random.PRNGKey(self.seed)
11gru=recurrent.GRU(
12hidden_size=self.hidden_size,num_layers=1,dropout_rate=0.0,bidirectional=True,
13)
14lengths=np.full([x.shape[0]],x.shape[1]) initialized_params=gru.init(rng,x,lengths)
15params=initialized_params['params']
16outputs,_=gru.apply({'params':params},x,lengths)
17outputsoutputs.reshape((outputs.shape[0],-1))
18x=nn.Dense(features=10)(outputs) returnx ```

Listing 4: A Flax implementation of the GRU baseline from Greydanus (2020).

```
1classModel(nn.Module):
2hidden_size:int=100
3
4@nn.compact
5def__call__(self,x):
6x=nn.Dense(features=self.hidden_size)(x)
7x=nn.relu(x)
8x=x+nn.relu(nn.Dense(features=self.hidden_size)(x))
9x=nn.Dense(features=10)(x) returnx
10}
11
12returnModel ```

Listing 5: A Flax implementation of the fully connected baseline from Greydanus (2020).

### Newly Discovered CLRS GNNs

Below we list the Python source code of five of the newly discovered GNNs.

```
1defget_triplet_msgs_quad(z,edge_fts,graph_fts,nb_triplet_fts
2,out_size):
3node_reps=[hk.Linear(nb_triplet_fts)for_inrange(4)]
4triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
5node_pair_inversions=[(1,2),(1,3),(2,3),(3,1)]
6triplets=functools.reduce(
7lambdax,y:x+y,
8[
9jnp.expand_dims(tri_node_rep,axis=perm)
10fortri_node_rep,permimzip(

Figure 4: The average model size and test error of the child models produced in each round of the model search. Data points closer to the origin represent rounds that yielded more fit models.

Figure 5: Maximum fitness scores of five of the newly discovered models, compared against the baseline, on eight of the CLRS tasks.

triplet_node_reps,node_pair_inversions
11 )
12 ),
13 )
14 returnjnp.max(triplets,axis=1) - jnp.min(triplets,axis=1) ```

Listing 6: The triplet representation that we refer to as QuadNodeMinMax.

```
1defget_triplet_msgs_concatrep(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2defrep_fn(x,size):
3proj=hk.nets.MLP([size])
4ff=hk.nets.MLP([size*4,size])
5returnjnp.concatenate([
6proj(x),ff(x),
7],axis=-1)
9
10triplet_node_reps=[rep_fn(z,nb_triplet_fts)for_inrange(3)]
11triplet_edge_reps=[rep_fn(edge_fts,nb_triplet_fts)for_inrange(3)]
12triplet_graph_rep=rep_fn(graph_fts,nb_triplet_fts)
13node_pair_permutations=[(2,3),(1,3),(1,2)]
14triplets=functools.reduce(
15lambdax,y:x+y,
16[
17jnp.expand_dims(tri_node_rep,axis=perm)
18fortri_node_rep,perminzip(
19triplet_node_reps,node_pair_permutations
20)
21],
22
23triplets+=functools.reduce(
24lambdax,y:x+y,
25[
26jnp.expand_dims(tri_edge_rep,axis=i)
27fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
28],
29) triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
30output_layer=hk.Linear(out_size)
31returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 7: The triplet representation that we refer to as ConcatRep.

```
1defget_triplet_msgs_tanhexplandtriplets(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4graph_rep=nk.nets.MLP([nb_triplet_fts])
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fts)foredge_repinedge_reps]
7node_pair_permutations=[(2,3),(1,3),(1,2)]
8triplets=functools.reduce(
9lambdax,y:x+y,
10[
11jnp.expand_dims(tri_node_rep,axis=perm)
12fortri_node_rep,perminzip(triplet_node_reps,node_pair_permutations
*),
*) triplets+=functools.reduce( lambdax,y:x+y,
*jnp.expand_dims(tri_edge_rep,axis=i) fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
*),
*) triplets+=jnp.expand_dims(graph_rep(graph_fts),axis=(1,2,3)) triplets+=jnp.expand_dims(graph_rep(graph_fts),axis=(2,3,1)) triplets+=jnp.expand_dims(graph_rep(graph_fts),axis=(3,1,2))
*output_layer=hk.Linear(out_size) returnoutput_layer(jnp.tanh(jnp.max(triplets,axis=1))) ```

Listing 8: The triplet representation that we refer to as TanhExpandTriplets.

```
1defget_triplet_msgs_div2mean(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4triplet_node_reps=[node_rep(z/2)fornode_repinnode_reps]
5triplet_edge_reps=[edge_rep(edge_fts)foredge_repin edge_reps]
6node_pair_permutations=[(2,3),(1,3),(1,2)]
7triplets=functools.reduce(
8lambdax,y:x+y,
9[
10jp.expand_dims(tri_node_rep,axis=perm) fortri_node_rep,perminzip(
11triplet_node_reps,node_pair_permutations) )
12],
13}
14}
15triplets+=functools.reduce(
16lambdax,y:x+y,
17[
18jnp.expand_dims(tri_edge_rep,axis=perm) fortri_edge_rep,perminzip(triplet_edge_reps,range(3,0,-1))
19],
20}
21output_layer=hk.Linear(out_size) returnoutput_layer(jnp.mean(triplets,axis=1)) ```

Listing 9: The triplet representation that we refer to as Div2Mean.

```
1defget_triplet_msgs_maxmean(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4graph_rep=hk.nets.MLP([nb_triplet_fts])
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]triplet_edge_reps=[edge_rep(edge_fits)foredge_repinedge_reps]node_pair_permutations=[(2,3),(1,3),(1,2)]triplets=functools.reduce(lambdax,y:x+y,
10[ jnp.expand_dims(tri_node_rep,axis=perm)fortri_node_rep,permizip(triplet_node_reps,node_pair_permutations))
15],
16)
17triplets+=functools.reduce(lambdax,y:x+y,
18[ jnp.expand_dims(tri_edge_rep,axis=i)fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1)))
19],
20)
21triplets=jnp.maximum(triplets,-100.0)output_layer=hk.Linear(out_size)returnoutput_layer(jnp.mean(triplets,axis=1)) ```

Listing 10: The triplet representation that we refer to as MaxMean.

### CLRS Seed Models

Below we provide the source code for the nine seed models used in the CLRS model search.

```
1defget_triplet_msgs_v1(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4graph_rep=hk.Linear(nb_triplet_fts)
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fts)foredge_repinedge_reps]
7triplet_graph_rep=graph_rep(graph_fts)
8node_pair_permutations=[(2,3),(1,3),(1,2)]
9triplets=functools.reduce(lambda,y:x+y,
10[
11jnp.expand_dims(tri_node_rep,axis=perm)
12fortri_node_rep,perminzip(triplet_node_reps,node_pair_permutations
13)
14],
15}
16}
17}
18triplets+=functools.reduce(
19lambda,y:x+y,
20[
21jnp.expand_dims(tri_edge_rep,axis=i)fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
23],
24}
25triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
26output_layer=hk.Linear(out_size)
27returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 11: The triplet representation belonging to the first seed model - the standard triplet representation from Ibarz et al. (2022).

```
1defget_triplet_msgs_v2(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4graph_rep=hk.Linear(nb_triplet_fts)
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fits)foredge_repinedge_reps]
7triplet_graph_rep=graph_rep(graph_fts)
8node_pair_permutations=[(2,3),(1,3),(1,2)]
9triplets=functools.reduce(lambda,y:x+y,
11[
12jnp.expand_dims(tri_node_rep,axis=perm)
13fortri_node_rep,perminzip(triplet_node_reps,node_pair_permutations
14)
15],
16}
17}
18triplets+=functools.reduce(
19lambda,y:x+y,
20[
21jnp.expand_dims(tri_edge_rep,axis=i)
22fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))),
23},
24} triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
26output_layer=hk.nets.MLP([out_size,out_size])
27returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 12: The triplet representation belonging to the second seed model, with the output layer replaced by a fully-connected multi-layer perceptron.

```
1defget_triplet_msgs_v3(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_imrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_imrange(3)]
4graph_rep=hk.Linear(nb_triplet_fts)
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fits)foredge_repin
7edge_reps]
8triplet_graph_rep=graph_rep(graph_fts)
9roleps=[(2,3),(1,3),(1,2)]
10lambdax,y:x+y,
11[
12jnp.expand_dims(tri_node_rep,axis=perm)
13fortri_node_rep,perminzip(triplet_node_reps,node_pair_permutations
15)
16],
17}
18triplets+=functools.reduce(
19lambdax,y:x+y,
20[
21jnp.expand_dims(tri_edge_rep,axis=i)
22fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
23],
24} triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
25output_layer=hk.Linear(out_size)
26returnoutput_layer(jnp.sum(triplets,axis=1)) ```

Listing 13: The triplet representation belonging to the third seed model, which uses sum instead of max aggregation.

```
1defget_triplet_msgs_v4(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.nets.MLP([nb_triplet_fts,nb_triplet_fts])for_imrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_imrange(3)]
4graph_rep=hk.Linear(nb_triplet_fts)
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fts)foredge_repin
7edge_reps]
8triplet_graph_rep=graph_rep(graph_fts)
9node_pair_permutations=[(2,3),(1,3),(1,2)]
10triplets=functools.reduce(
11lambdax,y:x+y,
12[
13jnp.expand_dims(tri_node_rep,axis=perm)
14fortri_node_rep,perminzip(triplet_node_reps,node_pair_permutations},
1} triplets+=functools.reduce(
2lambdax,y:x+y,
3[ ] jnp.expand_dims(tri_edge_rep,axis=i)
4fortri_edge_rep,iinzip(triplet_edge_reps,range(3,
5, 0, -1))
6],
7} triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
8output_layer=hk.Linear(out_size)
9returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 14: The triplet representation of the 4th seed model, which uses fully-connected multi-layer perceptron node representations.

```
1defget_triplet_msgs_v5(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.nets.MLP([nb_triplet_fts,nb_triplet_fts])for_inrange(3)]
4graph_rep=hk.Linear(nb_triplet_fts)
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fts)foredge_repinedge_reps)
7triplet_graph_rep=graph_rep(graph_fts)
8node_pair_permutations=[(2,3),(1,3),(1,2)]
9triplets=functools.reduce(
10lambda,y:x+y,
11[ ] jnp.expand_dims(tri_node_rep,axis=perm)
12fortri_node_rep,perminzip(
13triplet_node_reps,node_pair_permutations
14)
15],
16),
17) triplets+=functools.reduce(
18lambdax,y:x+y,
19[ ] jnp.expand_dims(tri_edge_rep,axis=i)
20fortri_edge_rep,iinzip(triplet_edge_reps,range(3,
20,-1))
21],
22} triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
23output_layer=hk.Linear(out_size)
24returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 15: The triplet representation of the 5th seed model, which uses fully-connected multi-layer perceptron edge representations.

```
1defget_triplet_msgs_v6(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2node_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_inrange(3)]
4graph_rep=hk.nets.MLP([nb_triplet_fts,nb_triplet_fts])
5triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
6triplet_edge_reps=[edge_rep(edge_fts)foredge_repinedge_reps]
7triplet_graph_rep=graph_rep(graph_fts) ```

Listing 16: The triplet representation of the 5th seed model, which uses fully-connected multi-layer perceptron node representations.

node_pair_permutations=[(2,3),(1,3),(1,2)] triplets=functools.reduce( lambdax,y:x+y,
11[ jnp.expand_dims(tri_node_rep,axis=perm) fortri_node_rep,perminzip( triplet_node_reps,node_pair_permutations
15)
16],
17} triplets+=functools.reduce( lambdax,y:x+y,
18[ jnp.expand_dims(tri_node_rep,axis=i) fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
19],
20} triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3)) output_layer=hk.Linear(out_size) returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 16: The triplet representation of the 6th seed model, which uses fully-connected multi-layer perceptron graph representations.

```
1defget_triplet_msgs_v7(z,edge_fts,graph_fts,nb_triplet_fts, out_size):
2node_reps=[hk.nets.MLP([nb_triplet_fts])for_imrange(3)]
3edge_reps=[hk.Linear(nb_triplet_fts)for_imrange(3)]
4triplet_node_reps=[node_rep(z)fornode_repinnode_reps]
5triplet_edge_reps=[edge_rep(edge_fts)foredge_repin edge_reps]
6node_pair_permutations=[(2,3),(1,3),(1,2)]
7triplets=functools.reduce( lambdax,y:x+y,
8[
9jnp.expand_dims(tri_node_rep,axis=perm) fortri_node_rep,perminzip( triplet_node_reps,node_pair_permutations
13)
14],
15} triplets+=functools.reduce( lambdax,y:x+y,
16[ jnp.expand_dims(tri_edge_rep,axis=i) fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
17],
18),
19} output_layer=hk.Linear(out_size) returnoutput_layer(jnp.max(triplets,axis=1)) ```

Listing 17: The triplet representation of the 7th seed model, which uses fully-connected multi-layer perceptron node representations and does not have a graph representation.

```
1defget_triplet_msgs_v8(z,edge_fts,graph_fts,nb_triplet_fts, out_size):
2output_layer=hk.nets.MLP([out_size]) returnjnp.tile(jnp.expand_dims(output_layer(z),axis=(1)),[1,z.shape[1],1,1]
5) ```

Listing 18: The triplet representation of the 8th seed model, which simply applies a linear layer and tiles the output to maintain dimensional consistency.

```
1defget_triplet_msgs_v9(z,edge_fts,graph_fts,nb_triplet_fts,out_size):
2defrep_fn(x,size):
3proj=hk.nets.MLP([size])
4ff=hk.nets.MLP([size*8,size])
5returnproj(x)*ff(x)
6
7triplet_node_reps=[rep_fn(z,nb_triplet_fts)for_inrange(3)]
8triplet_edge_reps=[rep_fn(edge_fts,nb_triplet_fts)for_inrange(3)]
9triplet_graph_rep=rep_fn(graph_fts,nb_triplet_fts)
10node_pair_permutations=[(2,3),(1,3),(1,2)]
11triplets=functools.reduce(
12lambdax,y:x+y,
13[
14jnp.expand_dims(tri_node_rep,axis=perm)
15fortri_node_rep,perminzip(
16triplet_node_reps,node_pair_permutations
17)
18],
19}
20triplets+=functools.reduce(
21lambdax,y:x+y,
22[
23jnp.expand_dims(tri_edge_rep,axis=i)
24fortri_edge_rep,iinzip(triplet_edge_reps,range(3,0,-1))
25],
26}
27triplets+=jnp.expand_dims(triplet_graph_rep,axis=(1,2,3))
28returnrep_fn(jnp.max(triplets,axis=1),out_size) ```

Listing 19: The triplet representation of the 9th seed model, which uses a bilinear representation for the node, edge, and graph representations.

### OOD Evaluation of Newly Discovered Models on CLRS

\begin{table}
\begin{tabular}{l l c c c c c} Algorithm & Best & Model & Size \(\downarrow\) & \multicolumn{3}{c}{OOD Accuracy \(\uparrow\)} \\  & Performing & Best & Best & Best performing & Baseline & Baseline model & Baseline model \\  & Model & Performance & Baseline & newly discovered & (our & (from \\  & & & Model & & & \\  & & & Model & & & \\  & & & & & & \\ \hline \hline Activity & Baseline & & & & 95.05 \(\pm\) & 93.96\(\pm\) & 95.18\(\pm\) \\ Selector & Baseline & 262204 & 262204 & 0.53\% & 0.29\% & 0.45\% \\ Articulation & & & & & 93.46 \(\pm\) & 91.40\(\pm\) & 88.32\(\pm\) \\ Points & QuadNodeMinMax & 497969 & 531913 & 1.77\% & 1.74\% & 2.01\% \\ Bellman Ford & ConcatRep & & & & 97.50 \(\pm\) & 97.08\(\pm\) & 97.39\(\pm\) \\  & & & & & 97.05 \(\pm\) & 97.08\(\pm\) & 97.39\(\pm\) \\ BFS & MaxMean & 568660 & 524604 & 0.31\% & 0.24\% & 0.19\% \\  & & & & 99.99 \(\pm\) & 99.80\(\pm\) & 99.73\(\pm\) \\ BFS & MaxMean & 522931 & 523963 & 0.01\% & 0.04\% & 0.04\% \\  & & & & 77.98 \(\pm\) & 79.57\(\pm\) & 77.58\(\pm\) \\ Binary Search & Baseline & 262204 & 262204 & 2.49\% & 1.73\% & 2.35\% \\ Bridges & ConcatRep & & & 97.57 \(\pm\) & 97.31\(\pm\) & 93.99\(\pm\) \\  & & & & 97.30 \(\pm\) & 95.94\(\pm\) & 96.05\(\pm\) \\ Bubble Sort & ConcatRep & & & & 88.87 \(\pm\) & 83.20\(\pm\) & 67.68\(\pm\) \\ DAG Shortest & & 568533 & 524477 & 2.77\% & 4.27\% & 5.50\% \\  & & & & 98.01 \(\pm\) & 97.48\(\pm\) & 98.19\(\pm\) \\ Paths & Baseline & & & & 98.01 \(\pm\) & 97.48\(\pm\) & 98.19\(\pm\) \\  & & & & & 98.14 \(\pm\) & 46.78\(\pm\) & 47.79\(\pm\) \\ DFS & Div2Mean & & & & 97.05 \(\pm\) & 97.30 \(\pm\) & 95.94\(\pm\) & 96.05\(\pm\) \\ Dijkstra & Div2Mean & & & & 98.77 \(\pm\) & 98.66\(\pm\) & 0.60\% \\ Find & & & & & 98.01 \(\pm\) & 97.48\(\pm\) & 98.19\(\pm\) \\ Maximum & Baseline & & & & 98.01 \(\pm\) & 97.48\(\pm\) & 98.19\(\pm\) \\ Subarray & Baseline & & & & 98.01 \(\pm\) & 97.48\(\pm\) & 98.19\(\pm\) \\ Kadane & & & & & 98.01 \(\pm\) & 98.19\(\pm\) & 98.19\(\pm\) \\ Floyd & ConcatRep & & & & 98.01 \(\pm\) & 98.19\(\pm\) & 98.19\(\pm\) \\ Warshall & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Graham Scan & MaxMean & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\  & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Heapsort & ConcatRep & & & & 98.01 \(\pm\) & 98.19\(\pm\) & 98.19\(\pm\) \\ Insertion Sort & Div2Mean & & & & 98.01 \(\pm\) & 98.19\(\pm\) & 98.19\(\pm\) \\  & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\  & & & & 98.01 \(\pm\) & 98.19\(\pm\) & 98.19\(\pm\) \\ Knuth- & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Morris-Pratt & Baseline & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\  & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ LCS Length & Baseline & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Matrix Chain & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Order & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\  & & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\ Minimum & Div2Mean & & & & 98.01 \(\pm\) & 98.19\(\pm\) \\  & & & & & 98.01 \(\pm\) & 98.

\begin{tabular}{l l l l l l l} \multicolumn{7}{c}{} & \multicolumn{5}{c}{Continuation of Table 3} \\ Algorithm & Best & \multicolumn{2}{c}{Model Size \(\downarrow\)} & \multicolumn{2}{c}{OOD Accuracy \(\uparrow\)} \\  & Performing & Best & \multicolumn{2}{c}{Base-} & \multicolumn{2}{c}{Best per-} & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{Baseline} \\  & Model & Perform- & \multicolumn{2}{c}{Base-} & \multicolumn{2}{c}{forming} & \multicolumn{2}{c}{model} & \multicolumn{2}{c}{model} \\  & &forming & \multicolumn{2}{c}{model} & \multicolumn{2}{c}{model} & \multicolumn{2}{c}{overly dis-} & \multicolumn{2}{c}{(our & (from \\  & & & Model & & model & & \\ \hline \hline MST Kruskal & ConcatRep & \multirow{2}{*}{443747} & \multirow{2}{*}{399691} & 91.47 \(\pm\) & 90.60\(\pm\) & 89.80\(\pm\) \\ MST Prim & ConcatRep & & & 0.48\% & 0.32\% & 0.77\% \\  & & & & 88.74 \(\pm\) & 85.18\(\pm\) & 86.39\(\pm\) \\ Naive String & \multirow{2}{*}{569942} & \multirow{2}{*}{252886} & \multirow{2}{*}{1.67\%} & \multirow{2}{*}{2.24\%} & \multirow{2}{*}{1.33\%} \\ Matcher & & & & 79.77 \(\pm\) & 73.39\(\pm\) & 78.67\(\pm\) \\  & & inMax & & & & \\ Optimal BST & Div2Mean & & & 78.66 \(\pm\) & 78.08\(\pm\) & 73.77\(\pm\) \\  & & 624955 & 625987 & 0.46\% & 0.96\% & 1.48\% \\ Quickselect & \multirow{2}{*}{377130} & \multirow{2}{*}{395714} & \multirow{2}{*}{0.41\%} & \multirow{2}{*}{0.08\%} & \multirow{2}{*}{0.25\%} \\ inMax & & & & 0.79 \(\pm\) & 0.13\(\pm\) & 0.47\(\pm\) \\  & & 377130 & 395714 & 0.41\% & 0.08\% & 0.25\% \\ Quicksort & Div2Mean & & & 85.23 \(\pm\) & 84.71\(\pm\) & 64.64\(\pm\) \\ Segments & \multirow{2}{*}{524727} & \multirow{2}{*}{525759} & \multirow{2}{*}{4.26\%} & \multirow{2}{*}{2.66\%} & \multirow{2}{*}{5.12\%} \\ Intersect & & & & 98.15 \(\pm\) & 97.40\(\pm\) & 97.64\(\pm\) \\ Strongly & & & & & \\ Connected & & 262327 & 263359 & 0.00\% & 0.00\% & 0.09\% \\ Components & & & & 41.86 \(\pm\) & 43.71\(\pm\) & 43.43\(\pm\) \\ Task & \multirow{2}{*}{707299} & \multirow{2}{*}{663243} & \multirow{2}{*}{3.39\%} & \multirow{2}{*}{5.94\%} & \multirow{2}{*}{3.15\%} \\ Scheduling & & & & 88.23 \(\pm\) & 88.10\(\pm\) & 87.25\(\pm\) \\ Topological & \multirow{2}{*}{707299} & \multirow{2}{*}{626333} & \multirow{2}{*}{262333} & \multirow{2}{*}{0.44\%} & \multirow{2}{*}{0.31\%} & \multirow{2}{*}{0.35\%} \\ Sort & & & & & & \\ \hline \multicolumn{7}{c}{} & \multicolumn{7}{c}{} & \multicolumn{7}{c}{} & \multicolumn{7}{c}{} & \multicolumn{7}{c}{} \\ End of Table 3 & & & & \\ \hline \end{tabular}

[MISSING_PAGE_FAIL:29]

### Broader Impacts

Our work may have a number of ethical, societal, and other broader impacts. Since we focus on automatic improvement of large language models, the implications of our research are largely similar to those of LMs in general. On the one hand, improving the abilities and decreasing the sizes of LMs may increase their accessibility (Kopf et al., 2023), improve energy efficiency (McDonald et al., 2022; Chen et al., 2023), and expand educational and professional opportunities (Kasneci et al., 2023; Eysenbach, 2023). On the other, LMs have long been known to give rise to unjust and toxic language that may hurt and amplify stereotypes (Nadeem et al., 2020; Lucy and Bamman, 2021), exclusionary norms, and allocational harms to marginalized groups (Bender et al., 2021). LMs may also present information hazards, often generating realistic-sounding misinformation (Bickmore et al., 2018; Quach, 2022) or revealing private personal information (Carlini et al., 2021). Lastly, other harms may arise from the ways that humans interact with LMs - either by inadvertently relying too much on unreliable LM outputs (McKee et al., 2021) or via malicious uses (Ranade et al., 2021; Boiko et al., 2023).

## References

* Bender et al. (2021) Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big?. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.
* Bickmore et al. (2018) Bickmore, T. W., Trinh, H., Olafsson, S., O'Leary, T. K., Asadi, R., Rickles, N. M., and Cruz, R. Patient and consumer safety risks when using conversational assistants for medical information: An observational study of siri, alexa, and google assistant. _J Med Internet Res_, 20(9), Sep 2018. ISSN 1438-8871. doi: 10.2196/11510. URL http://www.jmir.org/2018/9/e11510/.
* Boiko et al. (2023) Boiko, D. A., MacKnight, R., and Gomes, G. Emergent autonomous scientific research capabilities of large language models, 2023.
* Carlini et al. (2021) Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C. Extracting training data from large language models. In _USENIX Security Symposium_, 2021. URL https://arxiv.org/abs/2012.07805.
* Chen et al. (2023) Chen, L., Zaharia, M., and Zou, J. Frugalgrt: How to use large language models while reducing cost and improving performance, 2023.
* Eysenbach (2023) Eysenbach, G. The role of chatgpt, generative language models, and artificial intelligence in medical education: A conversation with chatgpt and a call for papers. _JMIR Med Educ_, 9:e46885, Mar 2023. ISSN 2369-3762. doi: 10.2196/46885. URL https://mededu.jmir.org/2023/1/e46885.
* Kasneci et al. (2023) Kasneci, E., Sessler, K., Kuchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Gunnemann, S., Hullermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., Stadler, M., Weller, J., Kuhn, J., and Kasneci, G. Chatgpt for good? on opportunities and challenges of large language models for education. _Learning and Individual Differences_, 103:102274, 2023. doi: https://doi.org/10.1016/j.lindif.2023.102274.
* democratizing large language model alignment, 2023.
* Lucy and Bamman (2021) Lucy, L. and Bamman, D. Gender and representation bias in GPT-3 generated stories. In _Proceedings of the Third Workshop on Narrative Understanding_, pp. 48-55, Virtual, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.nuse-1.5. URL https://aclanthology.org/2021.nuse-1.5.

McDonald, J., Li, B., Frey, N., Tiwari, D., Gadepally, V., and Samsi, S. Great power, great responsibility: Recommendations for reducing energy for training language models. In _Findings of the Association for Computational Linguistics: NAACL 2022_. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.151. URL https://doi.org/10.18653%2Fv1%2F2022.findings-naacl.151.
* McKee et al. (2021) McKee, K. R., Bai, X., and Fiske, S. Humans perceive warmth and competence in artificial intelligence, Feb 2021. URL psyarxiv.com/Sursp.
* Nadeem et al. (2020) Nadeem, M., Bethke, A., and Reddy, S. Stereoset: Measuring stereotypical bias in pretrained language models, 2020.
* Quach (2022) Quach, K. Researchers made an openai gpt-3 medical chatbot as an experiment. it told a mock patient to kill themselves, Aug 2022. URL https://www.theregister.com/2020/10/28/gpt3_medical_chatbot_experiment/.
* Ranade et al. (2021) Ranade, P., Piplai, A., Mittal, S., Joshi, A., and Finin, T. Generating fake cyber threat intelligence using transformer-based models, 2021.