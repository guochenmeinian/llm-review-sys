# A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm

 Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

&Daniel M. Kane

University of California, San Diego

dakane@cs.ucsd.edu

Jasper C.H. Lee

University of Wisconsin-Madison

jasper.lee@wisc.edu

&Ankit Pensia

IBM Research

ankitp@ibm.com

&Thanasis Pittas

University of Wisconsin-Madison

pittas@wisc.edu

###### Abstract

We study the problem of list-decodable Gaussian covariance estimation. Given a multiset \(T\) of \(n\) points in \(\mathbb{R}^{d}\) such that an unknown \(\alpha<1/2\) fraction of points in \(T\) are i.i.d. samples from an unknown Gaussian \(\mathcal{N}(\mu,\Sigma)\), the goal is to output a list of \(O(1/\alpha)\) hypotheses at least one of which is close to \(\Sigma\) in relative Frobenius norm. Our main result is a \(\operatorname{poly}(d,1/\alpha)\) sample and time algorithm for this task that guarantees relative Frobenius norm error of \(\operatorname{poly}(1/\alpha)\). Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [1] on robustly learning arbitrary GMMs. Combined with the other components of [1], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs, resolving an open problem proposed by Vempala [27] and Kothari [19]. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings.

## 1 Introduction

Robust statistics studies the efficient (parameter) learnability of an underlying distribution given samples some fraction of which might be corrupted, perhaps arbitrarily. While the statistical theory of these problems has been well-established for some time [10, 11], only recently has the algorithmic theory of such estimation problems begun to be understood [1, 1, 2].

A classical problem in robust estimation is that of multivariate robust mean estimation -- that is, estimating the mean of an unknown distribution in the presence of a small constant fraction of outliers. One of the original results in the field is that given samples from a Gaussian \(\mathcal{N}(\mu,I)\) with an \(\epsilon\)-fraction of outliers (for some \(\epsilon<1/2\)), the unknown mean \(\mu\) can be efficiently estimated to an error of \(O(\epsilon\sqrt{\log 1/\epsilon})\) in the \(\ell_{2}\)-norm [1]. Note that the use of the \(\ell_{2}\)-norm here is quite natural, as the total variation distance between two identity-covariance Gaussians is roughly proportional to the \(\ell_{2}\)-distance between their means; thus, this estimator learns the underlying distribution to total variation distance error \(O(\epsilon\sqrt{\log 1/\epsilon})\). This bound cannot be substantially improved, as learning to error \(o(\epsilon)\) in total variation distance is information-theoretically impossible.

While the above algorithmic result works when \(\epsilon\) is small (less than \(1/2\)), if more than half of the samples are corrupted, it becomes impossible to learn with only a single returned hypothesis--the corruption might simply simulate other Gaussians, and no algorithm can identify which Gaussian is the original one. This issue can be circumvented using the _list-decodable_ mean estimationparadigm [10], where the algorithm is allowed to output a small _list of hypotheses_ with the guarantee that at least one of them is relatively close to the target. List-decodable learning is closely related to _semi-verified learning_[10, 11], where a learner can choose to audit a small amount of data. The framework has been shown to capture a multitude of applications, including _crowdsourcing_[10, 11], _semi-random community detection_ in stochastic block models [10] and _clustering_ (as we also show in this work). For Gaussian mean estimation in particular, if an \(\alpha\)-fraction of the samples are clean (i.e., uncorrupted), for some \(\alpha<1/2\), there exist polynomial-time algorithms that return a list of \(O(1/\alpha)\) hypotheses such that (with high probability) at least one of them is within \(\tilde{O}(\sqrt{1/\alpha})\) of the mean in \(\ell_{2}\)-distance [10]. Note that while this \(\ell_{2}\)-norm distance bound does not imply a good bound on the total variation distance between the true distribution and the learned hypothesis, it does bound their distance away from one, ensuring some non-trivial overlap.

Another important, and arguably more complex, problem in robust statistics (the focus of this work) is that of robustly estimating the covariance of a multivariate Gaussian. It was shown in [1] that given \(\epsilon\)-corrupted samples from \(\mathcal{N}(0,\Sigma)\) (for \(\epsilon<1/2\)) there is a polynomial-time algorithm for estimating \(\Sigma\) to error \(O(\epsilon\log 1/\epsilon)\) in the relative Frobenius norm, i.e., outputting a hypothesis covariance \(\widetilde{\Sigma}\) satisfying \(\|\widetilde{\Sigma}^{-1/2}\Sigma\widetilde{\Sigma}^{-1/2}-I\|_{\mathrm{F}}\leq\epsilon\). This is again the relevant metric, since, if \(\|\widetilde{\Sigma}^{-1/2}\Sigma\widetilde{\Sigma}^{-1/2}-I\|_{\mathrm{F}} \leq 2/3\), then the total variation distance between \(\mathcal{N}(0,\Sigma)\) and \(\mathcal{N}(0,\widetilde{\Sigma})\) is proportional to the relative Frobenius norm \(\|\widetilde{\Sigma}^{-1/2}\Sigma\widetilde{\Sigma}^{-1/2}-I\|_{\mathrm{F}}\)[12].

A natural goal, with a number of applications, is to extend the above algorithmic result to the list-decodable setting. That is, one would like a polynomial-time algorithm that given corrupted samples from \(\mathcal{N}(0,\Sigma)\) (with an \(\alpha\)-fraction of clean samples, for some \(\alpha<1/2\)), returns a list of \(O(1/\alpha)\) many hypotheses with the guarantee that at least one of them has some non-trivial overlap with the true distribution in total variation distance. We start by noting that the sample complexity of list-decodable covariance estimation is \(\mathrm{poly}(d/\alpha)\), albeit via an exponential time algorithm. The only known algorithm for list-decodable covariance estimation (with total variation error guarantees) is due to [13]. This algorithm essentially relies on the Sum-of-Squares method and has (sample and computational) complexity \(d^{\mathrm{poly}(1/\alpha)}\). Intriguingly, there is compelling evidence that this complexity bound cannot be improved. Specifically, [1, 1] showed that any Statistical Query (SQ) algorithm for this task requires complexity \(d^{\mathrm{poly}(1/\alpha)}\). Combined with the reduction of [1], this implies a similar lower bound for low-degree polynomial tests. These lower bounds suggest an intrinsic information-computation gap for the problem.

The aforementioned lower bound results [1, 1] establish that it is SQ (and low-degree) hard to distinguish between a standard multivariate Gaussian, \(\mathcal{N}(0,I)\), and a distribution \(P\) that behaves like \(\mathcal{N}(0,I)\) (in terms of low-degree moments) but \(P\) contains an \(\alpha\)-fraction of samples from a Gaussian \(\mathcal{N}(0,\Sigma)\), where \(\Sigma\) is _very thin_ in some hidden direction \(v\). Since the distribution \(\mathcal{N}(0,\Sigma)\) is very thin along \(v\), i.e., has very small variance, the obvious choice of \(\mathcal{N}(0,I)\) essentially has no overlap with \(\mathcal{N}(0,\Sigma)\) -- making this kind of strong list-decoding guarantee (closeness in total variation distance) likely computationally intractable.

Interestingly, there are two possible ways that a pair of mean zero Gaussians can be separated [1, 1]: (1) one could be much thinner than the other in some direction, or (2) they could have many orthogonal directions in which their variances differ, adding up to something more substantial. While the lower bounds of [1, 1] seem to rule out being able to detect deviations of the former type in fully polynomial time (i.e., \(\mathrm{poly}(d/\alpha)\)), it does not rule out efficiently detecting deviations of the latter. In particular, we could hope to find (in \(\mathrm{poly}(d/\alpha)\) time) a good hypothesis \(\widetilde{\Sigma}\) such that \(\|\widetilde{\Sigma}^{-1/2}\Sigma\widetilde{\Sigma}^{-1/2}-I\|_{\mathrm{F}}\) is not too big. While this does not exclude the possibility that \(\Sigma\) is much thinner than \(\widetilde{\Sigma}\) in a small number of independent directions, it does rule out the second kind of difference between the two. The main goal of this paper is to provide an elementary (relying only on spectral techniques) list-decoding algorithm of this form. Given corrupted samples from a Gaussian \(\mathcal{N}(0,\Sigma)\), we give a \(\mathrm{poly}(d/\alpha)\)-time algorithm that returns a small list of hypotheses \(\widetilde{\Sigma}\) such that for at least one of them we have that \(\|\widetilde{\Sigma}^{-1/2}\Sigma\widetilde{\Sigma}^{-1/2}-I\|_{\mathrm{F}}< \mathrm{poly}(1/\alpha)\).

In addition to providing the best qualitative guarantee we could hope to achieve in fully polynomial time, the above kind of "weak" list-decoding algorithm has interesting implications for the well-studied problem of robustly learning Gaussian mixture models (GMMs). [1] gave a polynomial-time algorithm to robustly learn arbitrary mixtures of Gaussians (with a constant number of components). One of the two main ingredients of their approach is a subroutine that could perform partial clustering of points into components that satisfy exactly this kind of weak closeness guarantee. [1] developed such a subroutine by making essential use of a Sum-of-Squares (SoS) relaxation, for the setting that the samples come from a mildly corrupted mixture of Gaussians (with a small constant fraction of outliers). As a corollary of our techniques, we obtain an elementary spectral algorithm for this partial clustering task (and, as already stated, our results work in the more general list-decoding setting). This yields the first SoS-free algorithm for robustly learning GMMs, answering an open problem in the literature [21, 13].

### Our Results

Our main result is a polynomial-time algorithm for list-decoding the covariance of a Gaussian in relative Frobenius norm, under adversarial corruption where more than half the samples could be outliers. [1] makes precise the corruption model, and [1] states the guarantees of our main algorithm (Algorithm 1).

**Definition 1.1** (Corruption model for list-decoding).: _Let the parameters \(\epsilon,\alpha\in(0,1/2)\) and a distribution family \(\mathcal{D}\). The statistician specifies the number of samples \(m\). Then a set of \(n\geq\alpha m\) i.i.d. points are sampled from an unknown \(D\in\mathcal{D}\). We call these \(n\) samples the inliers. Upon inspecting the \(n\) inliers, a (malicious and computationally unbounded) adversary can replace an arbitrary \(\ell\leq\epsilon n\) of the inliers with arbitrary points, and further add \(m-n\) arbitrary points to the dataset, before returning the entire set of \(m\) points to the statistician. The parameter \(\alpha\) is also known a-priori to the statistician, but the number \(n\) chosen by the adversary is unknown. We refer to this set of \(m\) points as an \((\alpha,\epsilon)\)-corrupted set of samples from \(D\)._

In our context, the notion of list-decoding is as follows: our algorithm will return a polynomially-sized list of matrices \(H_{i}\), such that at least one \(H_{i}\) is an "approximate square root" of the true covariance \(\Sigma\) having bounded _dimension-independent_ error \(\|H_{i}^{-1/2}\Sigma H_{i}^{-1/2}-I\|_{\mathrm{F}}\). As discussed earlier, the bound we guarantee is \(\mathrm{poly}(1/\alpha)\), which is in general larger than \(1\) and thus does not lead to non-trivial total variation bounds, thus circumventing related SQ lower bounds. Our main theorem is:

**Theorem 1.2** (List-Decodable Covariance Estimation in Relative Frobenius Norm).: _Let \(C^{\prime}>0\) be a sufficiently large constant and \(\epsilon_{0}>0\) be a sufficiently small constant. Let the parameters \(\alpha\in(0,1/2)\), \(\epsilon\in(0,\epsilon_{0})\), and failure probability \(\delta\in(0,1/2)\) be known to the algorithm. Let \(D\) be the Gaussian distribution \(\mathcal{N}(\mu,\Sigma)\) with mean \(\mu\in\mathbb{R}^{d}\) and full-rank covariance \(\Sigma\in\mathbb{R}^{d\times d}\). There is an \(\tilde{O}(m^{2}d^{2})\)-time algorithm (Algorithm 1) such that, on input \(\alpha,\delta\) and an \((\alpha,\epsilon)\)-corrupted set of \(m\) points from \(D\) ([1]) for any \(m>C^{\prime}\frac{d^{2}\log^{5}(d/\alpha\delta)}{\alpha^{6}}\), with probability at least \(1-\delta\), the algorithm returns a list of at most \(O(1/\alpha)\) many sets \(T_{i}\) which are disjoint subsets of samples, each of size at least \(0.5\alpha m\), and there exists a \(T_{i}\) in the output list such that:_

* _Recall the notation in the corruption model (_Definition 1.1_) where_ \(n\) _is the size of the original inlier set_ \(S\) _and_ \(\ell\) _is the number of points in_ \(S\) _that the adversary replaced--_\(n\) _and_ \(\ell\) _are unknown to the algorithm except that_ \(n\geq\alpha m\) _and_ \(\ell\leq\epsilon n\)_. The set_ \(T_{i}\) _in the returned list satisfies that_ \(|T_{i}\cap S|\geq(1-0.01\alpha)(n-\ell)\)_._
* _Denote_ \(H_{i}:=\mathbf{E}_{X\sim T_{i}}[XX^{\top}]\)_. The matrix_ \(H_{i}\) _satisfies_ \(\|H_{i}^{-1/2}\Sigma H_{i}^{-1/2}-I\|_{\mathrm{F}}\lesssim\frac{1}{\alpha^{4} }\log(\frac{1}{\alpha})\)_._

Algorithm 1 is an iterative spectral algorithm, as opposed to involving large convex programs based on the sum-of-squares hierarchy. We state [1] and give a high-level proof sketch of why it satisfies Theorem 1.2 in Section 3. The formal proof of Theorem 1.2 appears in [1]F (where we allow \(\Sigma\) to be rank-deficient as well).

We also briefly remark that, if we wish to list-decode pairs of (mean, covariance), then we can run the following straightforward augmentation to [1]: after running Algorithm 1, for each \(\hat{\Sigma}\) in the output list, whiten the data using \(\hat{\Sigma}\) and run a standard list-decoding algorithm for the mean [15, 13]. These algorithms work if the data is distributed with a bounded covariance: by the guarantees of Theorem 1.2, after whitening, the data has covariance bounded by \(\mathrm{poly}(1/\alpha)\cdot I\).

As a corollary of our main result, the same algorithm (but with a slightly higher sample complexity) also achieves outlier-robust list-decoding of the covariances for the components of a Gaussian mixture model, in relative Frobenius norm. [1] and Theorem 1.4 state the corresponding corruption model and theoretical guarantees on Algorithm 1.

**Definition 1.3** (Corruption model for samples from Gaussian Mixtures).: _Let \(\epsilon\in(0,1/2)\). Consider a Gaussian mixture model \(\sum_{p}\alpha_{p}\mathcal{N}(\mu_{p},\Sigma_{p})\), where the parameters \(\alpha_{p},\mu_{p}\) and \(\Sigma_{p}\) are unknown and satisfy \(\alpha_{p}\geq\alpha\) for some known parameter \(\alpha\). The statistician specifies the number of samples \(m\), and \(m\) i.i.d. samples are drawn from the Gaussian mixture, which are called the \(\mathrm{inliers}\). The malicious and computationally unbounded adversary then inspects the \(m\) inliers and is allowed to replace an arbitrary subset of \(\epsilon\alpha m\) many inlier points with arbitrary outlier points, before giving the modified dataset to the statistician. We call this the \(\epsilon\)-corrupted set of samples._

**Theorem 1.4** (Outlier-Robust Clustering and Estimation of Covariances for GMM).: _Let \(C^{\prime}>0\) be a sufficiently large constant and \(\epsilon_{0}>0\) be a sufficiently small constant. Let the parameters \(\alpha\in(0,1/2)\), \(\epsilon\in(0,\epsilon_{0})\), and failure probability \(\delta\in(0,1/2)\) be known. There is an \(\tilde{O}(m^{2}d^{2})\)-time algorithm such that, on input \(\alpha,\delta\), and \(m>C^{\prime}\frac{d^{2}\log^{2}(d/\alpha)}{\alpha^{\delta}}\) many \(\epsilon\)-corrupted samples from an unknown \(k\)-component Gaussian mixture \(\sum_{p=1}^{k}\alpha_{p}\mathcal{N}(\mu_{p},\Sigma_{p})\) over \(\mathbb{R}^{d}\) as in Definition 1.3, where all \(\Sigma_{p}\)'s are full-rank and all \(\alpha_{p}\) satisfies \(\alpha_{p}\geq\alpha\) and \(k\leq\frac{1}{\alpha}\) is unknown to the algorithm, with probability at least \(1-\delta\) over the corrupted samples and the randomness of the algorithm, the algorithm returns a list of at most \(k\) many disjoint subsets of samples \(\{T_{i}\}\) such that:_

* _For the_ \(p^{\text{th}}\) _Gaussian component, denote the set_ \(S_{p}\) _as the samples in the inlier set_ \(S\) _that were drawn from component_ \(p\)_. Let_ \(n_{p}\) _be the size of_ \(S_{p}\)_, and let_ \(\ell_{p}\) _be the number of points in_ \(S_{p}\) _that the adversary replaced--_\(n_{p}\) _and_ \(\ell_{p}\) _are both unknown to the algorithm except that_ \(\mathbf{E}[n_{p}]=\alpha_{p}m\geq\alpha m\) _for each_ \(p\) _and_ \(\sum_{p}\ell_{p}\leq\epsilon\alpha m\)_. Then, for every Gaussian component_ \(p\) _in the mixture, there exists a set_ \(T_{i_{p}}\) _in the returned list such that_ \(|T_{i_{p}}\cap S_{p}|\geq(1-0.01\alpha)(n_{p}-\ell_{p})\)_._
* _For every component_ \(p\)_, there is a set of samples_ \(T_{i_{p}}\) _in the returned list such that, defining_ \(H_{i_{p}}=\mathbf{E}_{X\sim T_{i_{p}}}[XX^{\top}]\) _, we have_ \(H_{i_{p}}\) _satisfying_ \(\|H_{i_{p}}^{-1/2}\Sigma_{p}H_{i_{p}}^{-1/2}-I\|_{\mathrm{F}}\lesssim(1/\alpha ^{4})\log(1/\alpha)\)_._
* _Let_ \(\Sigma\) _be the (population-level) covariance matrix of the Gaussian mixture. For any two components_ \(p\neq p^{\prime}\) _with_ \(\|\Sigma^{-1/2}(\Sigma_{p}-\Sigma_{p^{\prime}})\Sigma^{-1/2}\|_{\mathrm{F}}>C( 1/\alpha)^{5}\log(1/\alpha)\) _for a sufficiently large constant_ \(C\)_, the sets_ \(T_{i_{p}}\) _and_ \(T_{i_{p^{\prime}}}\) _from the previous bullet are guaranteed to be different._

The theorem states that, not only does Algorithm 1 achieve list-decoding of the Gaussian component covariances, but it also clusters samples according to separation of covariances in relative Frobenius norm. The recent result of [1] on robustly learning GMMs also involves an algorithmic component for clustering. Their approach is based on the sum-of-squares hierarchy (and thus requires solving large convex programs) while, Algorithm 1 is a purely spectral algorithm.

We also emphasize that the list size returned by Algorithm 1, in the case of a \(k\)-component Gaussian mixture model, is at most \(k\) -- instead of a weaker result such as \(O(1/\alpha)\) or polynomial/exponentially large in \(1/\alpha\). This is possible because Algorithm 1 keeps careful track of samples and makes sure that no more than a \(0.01\alpha\)-fraction of samples is removed from each component or mis-clustered into another component. We prove Theorem 1.4 in Appendix G.

### Overview of Techniques

At a high level, our approach involves integrating robust covariance estimation techniques from [1] with the multifilter for list-decoding of [1]. For a set \(S\), we use \(SS^{\top}\) to denote the set \(\{xx^{\top}:x\in S\}\). [1] states that in order to estimate the true covariance \(\Sigma\), it suffices to find a large subset \(S\) of points with large overlap with the original set of good points, so that if \(\Sigma^{\prime}=\mathbf{Cov}(S)\) then \(\mathbf{Cov}((\Sigma^{\prime})^{-1/2}SS^{\top}(\Sigma^{\prime})^{-1/2})\) has no large eigenvalues. This will imply that \(\|(\Sigma^{\prime})^{-1/2}(\Sigma-\Sigma^{\prime})(\Sigma^{\prime})^{-1/2}\|_ {\mathrm{F}}\) is not too large (Lemma E.2).

As in [1], our basic approach for finding such a set \(S\) is by the iteratively repeating the following procedure: We begin by taking \(S\) to be the set of all samples and repeatedly check whether or not \(\mathbf{Cov}((\Sigma^{\prime})^{-1/2}SS^{\top}(\Sigma^{\prime})^{-1/2})\) has any large eigenvalues. If it does not, we are done. If it has a large eigenvalue corresponding to a matrix \(A\) (normalized to have unit Frobenius norm), we consider the values of \(f(x)=\langle A,(\Sigma^{\prime})^{-1/2}xx^{\top}(\Sigma^{\prime})^{-1/2}\rangle\), for \(x\) in \(S\), and attempt to use them to find outliers. Unfortunately, as the inliers might comprise a minority of the sample, the values we get out of this formula might end-up in several reasonably large clusters, any one of which could plausibly contain the true samples; thus, not allowing us to declare any particular points to be outlierswith any degree of certainty. We resolve this issue by using the multifilter approach of [10]--we either (i) iteratively remove outliers, or (ii) partition the data into clusters and recurse on each cluster. In particular, we note that if there is large variance in the \(A\)-direction, one of two things must happen, either: (i) The substantial majority of the values of \(f(x)\) lie in a single cluster, with some extreme outliers. In this case, we can be confident that the extreme outliers are actual errors and remove them (Section 3.2). (ii) There are at least two clusters of values of \(f(x)\) that are far apart from each other. In this case, instead of simply removing obvious outliers, we replace \(S\) by two subsets \(S_{1}\) and \(S_{2}\) with the guarantee that at least one of the \(S_{i}\) contains almost all of the inliers. Naively, this can be done by finding a value \(y\) between the two clusters so that very few samples have \(f(x)\) close to \(y\), and letting \(S_{1}\) be the set of points with \(f(x)<y\) and \(S_{2}\) the set of points with \(f(x)>y\) (Section 3.3). In either case, we will have cleaned up our set of samples and can recurse on each of the returned subsets of \(S\). Iterating this technique recursively on all of the smaller subsets returned ensures that there is always at least one subset containing the majority of the inliers, and that eventually once it stops having too large of a covariance, we will return an appropriate approximation to \(\Sigma\).

We want to highlight the main point of difference where our techniques differ notably from [10]. In order to implement the algorithm outlined above, one needs to have good a priori bounds for what the variance of \(f(x)\) over the inliers ought to be. Since \(f(\cdot)\) is a quadratic polynomial, the variance of \(f\) over the inliers, itself depends on the covariance \(\Sigma\), which is exactly what we are trying to estimate. This challenge of circular dependence does not appear in [10]: their goal was to estimate the unknown mean of an identity-covariance Gaussian, and thus it sufficed to use a linear polynomial \(f\) (instead of a quadratic polynomial). Importantly, the covariance of a linear polynomial does not depend on the (unknown) mean (it depends only on the covariance, which was known in their setting). In order to overcome this challenge, we observe that if \(S\) contains most of the inliers, then the covariance of \(S\) cannot be too much smaller than the true covariance \(\Sigma\). This allows us to find an upper bound on \(\Sigma\), which in turn lets us upper bound the variance of \(f(x)\) over the good samples (Lemma 3.2).

Related WorkWe refer the reader to [10] for an overview of algorithmic robust statistics. We mention the most relevant related work here and discuss additional related work in Appendix A. Algorithms for list-decodable covariance estimation were developed in the special cases of subspace estimation and linear regression in [14, 15, 16, 17, 18, 19]. On the other hand, [10, 18] present SQ lower bounds for learning Gaussian mixture models and list-decodable linear regression (and thus list-decodable covariance estimation), respectively.

[15] gave the first algorithm for general list-decodable covariance estimation that achieves non-trivial bounds in total variation distance using the powerful sum-of-squares hierarchy. Their algorithm outputs an \(\exp(\mathrm{poly}(1/\alpha))\)-sized list of matrices containing an \(H_{i}\) that is close to the true \(\Sigma\) in two metrics (i) relative Frobenius norm: \(\|H_{i}^{-1/2}\Sigma H_{i}^{-1/2}-I\|_{\mathrm{F}}=\mathrm{poly}(1/\alpha)\)_and_ (ii) multiplicative spectral approximation: \(\mathrm{poly}(\alpha)\Sigma\preceq H_{i}\preceq\mathrm{poly}(1/\alpha)\Sigma\). Their algorithm uses \(d^{\mathrm{poly}(1/\alpha)}\) samples, which seems to be necessary for efficient (statistical query) algorithms achieving multiplicative spectral approximation [10, 18]. In comparison, Theorem 1.2 uses only \(\mathrm{poly}(d/\alpha)\) samples, returns a list of \(O(1/\alpha)\) matrices, but approximates only in the relative Frobenius norm: \(\|H_{i}^{-1/2}\Sigma H_{i}^{-1/2}-I\|_{\mathrm{F}}=\mathrm{poly}(1/\alpha)\).

## 2 Preliminaries

For a vector \(v\), we let \(\|v\|_{2}\) denote its \(\ell_{2}\)-norm. We use \(I_{d}\) to denote the \(d\times d\) identity matrix; We will drop the subscript when it is clear from the context. For a matrix \(A\), we use \(\|A\|_{\mathrm{F}}\) and \(\|A\|_{\mathrm{op}}\) to denote the Frobenius and spectral (or operator) norms, respectively. We denote by \(\langle v,u\rangle\), the standard inner product between the vectors \(u,v\). For matrices \(U,V\in\mathbb{R}^{d\times d}\), we use \(\langle U,V\rangle\) to denote the trace inner product \(\sum_{ij}U_{ij}V_{ij}\). For a matrix \(A\in\mathbb{R}^{d\times d}\), we use \(A^{\flat}\) to denote the flattened vector in \(\mathbb{R}^{d^{2}}\), and for a \(v\in\mathbb{R}^{d^{2}}\), we use \(v^{\sharp}\) to denote the unique matrix \(A\) such that \(A^{\flat}=v^{\sharp}\). For a matrix \(A\), we let \(A^{\dagger}\) denote its pseudo-inverse. We use \(\otimes\) to denote the Kronecker product. For a matrix \(A\), we use \(\ker(A)\) for the null space of \(A\). We use \(X\sim D\) to denote that a random variable \(X\) is distributed according to the distribution \(D\). We use \(\mathcal{N}(\mu,\Sigma)\) for the Gaussian distribution with mean \(\mu\) and covariance matrix \(\Sigma\). For a set \(S\), we use \(X\sim S\) to denote that \(X\) is distributed uniformly at random from \(S\). We use \(a\lesssim b\) to denote that there exists an absolute universal constant \(C>0\) (independent of the variables or parameters on which \(a\) and \(b\) depend) such that \(a\leq Cb\).

### Deterministic Conditions

Our algorithm will rely on the uncorrupted inliers satisfying a set of properties, similar to the "stability conditions" from [1]. Intuitively, these are are concentration properties for sets of samples, but with the added requirement that every large subset of the samples also satisfies these properties.

**Definition 2.1** (\((\eta,\epsilon)\)-Stable Set).: _Let \(D\) be a distribution with mean \(\mu\) and covariance \(\Sigma\). We say a set of points \(A\subset\mathbb{R}^{d}\) is \((\eta,\epsilon)\)-stable with respect to \(D\), if for any subset \(A^{\prime}\subseteq A\) with \(|A^{\prime}|\geq(1-\epsilon)|A|\), the following hold: for every \(v\in\mathbb{R}^{d}\), symmetric \(U\in\mathbb{R}^{d\times d}\), and every even degree-\(2\) polynomial \(p\):_

1. \(\big{|}\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}v^{\top}(x-\mu)\big{|}\leq 0.1\sqrt{(v^{\top}\Sigma v)}\)_._
2. \(\big{|}\big{\langle}\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}(x-\mu)(x-\mu)^ {\top}-\Sigma,U\big{\rangle}\big{|}\leq 0.1\,\big{\|}\Sigma^{1/2}U\Sigma^{1/2} \big{\|}_{\mathrm{F}}\)__
3. \(\underset{X\sim A^{\prime}}{\mathbf{Pr}}\left[\big{|}p(X)\!-\!\underset{X\sim D }{\mathbf{E}}[p(X)]\big{|}\!\!>\!\!10\sqrt{\underset{Y\sim D}{\mathbf{Var}}[p( Y)]}\ln\left(\frac{2}{\eta}\right)\right]\!\!\leq\!\!\eta\)_._
4. \(\mathbf{Var}_{X\sim A^{\prime}}[p(X)]\leq 4\,\mathbf{Var}_{X\sim D}[p(X)]\)_._
5. _The null space of second moment matrix of_ \(A^{\prime}\) _is contained in the null space of_ \(\Sigma\)_, i.e.,_ \(\ker\left(\sum_{x\in A^{\prime}}xx^{\top}\right)\subseteq\ker(\Sigma)\)_._

A Gaussian dataset is "stable" with high probability [1]; formally, we have Lemma2.2, proved in AppendixD.1. Moreover, Lemma2.2 can be extended to a variety of distributions (cf. RemarkD.1).

**Lemma 2.2** (Deterministic Conditions Hold with High Probability).: _For a sufficiently small positive constant \(\epsilon_{0}\) and a sufficiently large absolute constant \(C\), a set of \(m>Cd^{2}\log^{5}(d/(\eta\delta))/\eta^{2}\) samples from \(\mathcal{N}(\mu,\Sigma)\), with probability \(1-\delta\), is \((\eta,\epsilon)\)-stable set with respect to \(\mu,\Sigma\) for all \(\epsilon\leq\epsilon_{0}\)._

## 3 Analysis of a Single Recursive Call of the Algorithm

Our algorithm, Algorithm1, filters and splits samples into multiple sets recursively, until we can certify that the empirical second moment matrix of the "current data set" is suitable to be included in the returned list of covariances. As a reference point, we define the notations and assumptions necessary to analyze each recursive call below. However, before moving to the formal analysis we will first give an informal overview of the algorithm's steps and the high-level ideas behind them.

**Assumption 3.1** (Assumptions and notations for a single recursive call of the algorithm).:
* \(S=\{x_{i}\}_{i=1}^{n}\) is a set of \(n\) uncontaminated samples, which is assumed to be \((\eta,2\epsilon_{0})\)-stable with respect to the inlier distribution \(D\) having mean and covariance \(\mu,\Sigma\) (c.f. Definition2.1). We assume \(\eta\leq 0.001\), \(\epsilon_{0}=0.01\), and \(\mathbf{Var}_{X\sim D}[X^{\top}AX]\leq C_{1}(\|\Sigma^{1/2}A\Sigma^{1/2}\|_{F }^{2}+\|\Sigma^{1/2}A\mu\|_{2}^{2})\) for all symmetric \(d\times d\) matrices \(A\) and a constant \(C_{1}\).
* \(T\) is the input set to the current recursive call of the algorithm (after the adversarial corruptions), which satisfies \(|S\cap T|\geq(1-2\epsilon_{0})|S|\) and \(|T|\leq(1/\alpha)|S|\).
* We denote \(H=\mathbf{E}_{X\sim T}\left[XX^{\top}\right]\).
* We denote by \(\widetilde{S},\widetilde{T}\) the versions of \(S\) and \(T\) normalized by \(H^{\dagger/2}\): \(\widetilde{S}=\{H^{\dagger/2}x:x\in S\}\) and \(\widetilde{T}=\{H^{\dagger/2}x:x\in T\}\). We use the notation \(\widetilde{x}\) for elements in \(\widetilde{S}\) and \(\widetilde{T}\), and \(x\) for elements in \(S\) and \(T\). Similarly, we use the notation \(\widetilde{X}\) for random variables with support in \(\widetilde{S}\) or \(\widetilde{T}\).
* The mean and covariance of the inlier distribution \(D\) after transformation with \(H^{\dagger/2}\) are denoted by \(\widetilde{\mu}:=H^{\dagger/2}\mu\), \(\widetilde{\Sigma}:=H^{\dagger/2}\Sigma H^{\dagger/2}\). We denote the empirical mean and covariance of the transformed inliers in \(T\) by \(\hat{\mu}:=\mathbf{E}_{\widetilde{X}\sim\widetilde{S}\cap\widetilde{T}}[ \widetilde{X}]\) and \(\hat{\Sigma}:=\mathbf{Cov}_{\widetilde{X}\sim\widetilde{S}\cap\widetilde{T}} [\widetilde{X}]\).

Much of the algorithm uses the fact that, for a Gaussian, even quadratic polynomials have a small variance. We will leverage this for filtering and clustering samples. See E for the proof.

**Lemma 3.2**.: _Make Assumption 3.1 and recall that \(H=\mathbf{E}_{X\sim T}[XX^{\top}]\), where \(T\) is the corrupted version of a stable inlier set \(S\). For every symmetric matrix \(A\) with \(\|A\|_{\mathrm{F}}=1\), we have that \(\mathbf{Var}_{X\sim D}[(H^{1/2}X)^{\top}A(H^{\dagger/2}X)]\leq 18C_{1}/\alpha^{2}\)._

Armed with Lemma3.2, we can now give a high-level overview of a recursive call of Algorithm1:

1. In our notation, we call the current data set \(T\). Denoting \(H=\mathbf{E}_{X\sim T}[XX^{\top}]\) for its the empirical second moment matrix, we construct the normalized data set \(\widetilde{T}=\{H^{-1/2}x\ :\ x\in T\}\). The normalization allows us to bound the covariance \(\Sigma\) in terms of \(H\).
2. Since we are trying to estimate a covariance, consider the vectors \(\tilde{s}=\{(\tilde{x}\tilde{x}^{\top})^{\flat}:\tilde{x}\in\widetilde{T}\}\), which are the second moment matrices of each data point flattened into vectors.
3. The first step is standard in filtering-based outlier-robust estimation: we test whether the covariance of the \(\tilde{s}\) vectors is small. If so, we are able to prove that the current \(H\) is a good approximate square root of \(\Sigma\) (c.f. Section3.1) hence we just return \(H\).
4. If the first test fails, that would imply that the empirical covariance of the \(\tilde{s}\) vectors is large in some direction. We want to leverage this direction to make progress, either by removing outliers through filtering or by bi-partitioning our samples into two clear clusters.
5. To decide between the 2 options, consider projecting the \(\tilde{s}\) vectors onto their largest variance direction. Specifically, let \(A\) be the matrix lifted from the largest eigenvector of the covariance of the \(\tilde{s}\) vectors. Define the vectors \(\tilde{y}=\tilde{x}^{\top}A\tilde{x}=\langle\tilde{x}\tilde{x}^{\top},A\rangle\) for \(\tilde{x}\in\widetilde{T}\), corresponding to the 1-dimensional projection of \(\tilde{s}\) onto the \(A^{\flat}\) direction. Since we have failed the first test, these \(\tilde{y}\) elements must have a large variance. We will decide to filter or divide our samples, based on whether the \(\alpha m\)-smallest and \(\alpha m\)-largest elements of the \(\tilde{y}\)s are close to each other.
6. If they are close, yet we have large variance, we will use this information to design a _score function_ and perform filtering that removes a random sample with probability proportional to its score. We will then go back to Step1. This would work because by Lemma3.2 and by stability (Definition2.1), the (unfiltered) inliers have a small empirical variance within themselves, meaning that the large total empirical variance is mostly due to the outlier.

Ideally, the score of a sample would be (proportional to) the squared distance between the sample and the mean of the inliers--the total inlier score would then be equal to the inlier variance. However, since we do not know which points are the inliers, we instead use the median of all the projected samples as a proxy for the unknown inlier mean. We show that the distance between the \(\alpha m\)-smallest and largest \(\tilde{y}\)s bounds the difference between the ideal and proxy scores.
7. Otherwise, \(\alpha m\)-smallest and \(\alpha m\)-largest elements of the \(\tilde{y}\)s are far apart. By the stability condition Definition2.1 (specifically, Condition(L.3)), most of the inliers must be close to each other under this 1-dimensional projection. Therefore, the large quantile range necessarily means there is a threshold under this projection to divide the samples into two sets, such that each set has at least \(\alpha m\) points and most of the inliers are kept within a single set.

The score function mentioned in Step6 upper bounds the maximum variance we check in Step3. For simplicity, in the actual algorithm (Algorithm1) we use the score directly for the termination check instead of checking the covariance, but it does not matter technically which quantity we use.

**Remark 3.3** (Runtime of Algorithm1).: We claim that each "loop" in Algorithm1 takes \(\tilde{O}(md^{2})\) time to compute. The number of times we run the "loop" is at most \(O(m)\), since each loop either ends in termination, removes 1 element from the dataset, or splits the dataset, all of which can happen at most \(O(m)\) times. From this, we can conclude a runtime of \(\tilde{O}(m^{2}d^{2})\). The sample complexity of our algorithm is also explicitly calculable to be \(\tilde{O}(d^{2}/\alpha^{6})\), which follows from Lemma2.2 and the choice of parameter \(\eta=\Theta(\alpha^{3})\) from TheoremF.1 (the formal version of Theorem1.2).

To see the runtime of a single loop: the most expensive operations in each loop are to compute \(H_{t}\), its pseudo-inverse, and to compute the symmetric matrix \(A\) in Line7 that is the top eigenvector of a \(d^{2}\times d^{2}\) matrix. Computing \(H_{t}\) trivially takes \(O(md^{2})\) time, resulting in a \(d\times d\) matrix. Its pseudoinverse can be computed in \(O(d^{\omega})\) time, which is dominated by \(O(md^{2})\) since \(m\gtrsim d\). Lastly, we observe that, instead of actually computing the top eigenvector in Line7 to yield the matrix \(A\), it suffices in our analysis to compute a matrix \(B\) whose Rayleigh quotient \((B^{\flat})^{\top}\left(\mathbf{Cov}_{\widetilde{X}\sim\widetilde{T}_{t}}[ \widetilde{X}^{\otimes 2}]\right)B^{\flat}/((B^{\flat})^{\top}B^{\flat})\) is at least \(\frac{1}{2}\) times \((A^{\flat})^{\top}\left(\mathbf{Cov}_{\widetilde{X}\sim\widetilde{T}_{t}}[ \widetilde{X}^{\otimes 2}]\right)A^{\flat}/((A^{\flat})^{\top}A^{\flat})\). We can do this via \(O(\log d)\) manypower iterations. Since

\[\mathop{\mathbf{Cov}}_{\widetilde{X}\sim T_{t}}[\widetilde{X}^{\otimes 2}]=\frac{1}{| \widetilde{T}_{t}|}\sum_{z\in\widetilde{T}_{t}}zz^{\top}-\left(\frac{1}{| \widetilde{T}_{t}|}\sum_{z\in\widetilde{T}_{t}}z\right)^{\top}\left(\frac{1}{| \widetilde{T}_{t}|}\sum_{z\in\widetilde{T}_{t}}z\right)\,\]

we can compute each matrix-vector product in \(O(|T_{t}|d^{2})\leq O(md^{2})\) time, thus yielding an \(\tilde{O}(m^{2}d^{2})\) runtime for the power iteration.

### Certificate Lemma: Bounded Fourth Moment Implies Closeness

The first component of the analysis is our certificate lemma, which states that, if the empirical covariance of the (flattened) second moment matrices of current data set (after normalization) is bounded, then the empirical second moment matrix \(H\) of the current data set is a good approximation to the covariance of the Gaussian component we want to estimate.

**Lemma 3.4** (Case when we stop and return).: _Make Assumption 3.1. Let \(w\in\mathbb{R}^{d^{2}}\) be the leading eigenvector of the \(\mathop{\mathbf{Cov}}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{\otimes 2}]\) with \(\|w\|_{2}=1\), and let \(A\in\mathbb{R}^{d\times d}\) be \(w^{\sharp}\). Note that \(\|w\|_{2}=1\) implies \(\|A\|_{\mathrm{F}}=1\). Then, we have \(\left\|H^{1/2}\Sigma H^{\dagger/2}-I\right\|_{\mathrm{F}}^{2}\lesssim(1/ \alpha)\mathop{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{ \top}A\widetilde{X}]+1/\alpha^{2}\)._

See Appendix E.2 for the proof. Our termination check of creftype 12 uses the score \(f(\widetilde{x})=(\widetilde{x}^{\top}A\widetilde{x}-\widetilde{y}_{\mathrm{ median}})^{2}\) where \(\widetilde{y}_{\mathrm{median}}\) is the median of \(\{\widetilde{x}^{\top}A\widetilde{x}\ :\ \widetilde{x}\in\widetilde{T}\}\). Since \(\mathop{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{\top}A \widetilde{X}]\leq\mathop{\mathbf{E}}_{\widetilde{X}\sim\widetilde{T}}[f( \widetilde{X})]\) our check ensures that \(\mathop{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{\top} A\widetilde{X}]\leq\mathop{\mathrm{poly}}(1/\alpha)\) before returning.

### Filtering: Removing Extreme Outliers

As discussed in the algorithm outline, if the termination check fails, namely the expected score over the entire set of \(\widetilde{T}\) is large, then we proceed to either filter or bi-partition our samples. This subsection states the guarantees of the filtering procedure, which assumes that the \(\alpha m\)-smallest and largest elements in the set \(\{\tilde{x}^{\top}A\tilde{x}\ :\ \tilde{x}\in\tilde{T}\}\) have distance at most \(R\) for some \(R=\mathrm{poly}(1/\alpha)\).

Recall from Lemma3.2 that \(\mathbf{Var}_{\tilde{X}\sim H^{1/2}D}[\tilde{X}^{\top}A\tilde{X}]=\mathbf{E}_ {\tilde{X}\sim H^{1/2}D}[(\tilde{X}^{\top}A\tilde{X}\ -\mathbf{E}_{\tilde{X}\sim H^{1/2}D}[\tilde{X}^{\top}A\tilde{X}])^{2}]\) is bounded by \(O(1/\alpha^{2})\). By stability, the same is true for \(\mathbf{E}_{\tilde{X}\sim\tilde{S}\cap\tilde{T}}[(\tilde{X}^{\top}A\tilde{X} -\mathbf{E}_{\tilde{X}\sim\tilde{S}\cap\tilde{T}}[\tilde{X}^{\top}A\tilde{X}])^ {2}]\). Notice that this looks almost like our score function \(f(X)\), except that in \(f(\tilde{X})\) we use \(\widetilde{y}_{\mathrm{median}}\) for centering instead of \(\mathbf{E}_{\tilde{X}\sim\tilde{S}\cap\tilde{T}}[\tilde{X}^{\top}A\tilde{X}]\), since the latter quantity is by definition unknown to the algorithm. In LemmaE.3, we show that the two quantities have distance upper bounded by \(O(R)\), where \(R\) is the quantile distance in our assumption, which in turn implies that the inliers in \(\tilde{S}\cap\tilde{T}\) contribute very little to \(\mathbf{E}_{\tilde{X}\sim\tilde{T}}[f(\tilde{X})]\). Given that \(\mathbf{E}_{\tilde{X}\sim\tilde{T}}[f(\tilde{X})]\) is large, by virtue of having failed the termination check, we can then conclude that most of the score contribution comes from the outliers. Thus, we can safely use the score to randomly pick an element in the dataset for removal, with probability proportional to its score, and the element will be overwhelmingly more likely to be an outlier rather than an inlier. Lemma3.5 below states the precise guarantees on the ratio of the total score of the inliers versus the total score over the entire dataset.

**Lemma 3.5** (Filtering).: _Make Assumption3.1. Let \(A\) be an arbitrary symmetric matrix with \(\|A\|_{\mathrm{F}}=1\). Let \(R=C(1/\alpha)\log(1/\eta)\) for \(C\geq 100\sqrt{C_{1}}\). Define \(\widetilde{y}_{\mathrm{median}}=\mathrm{Median}(\{\tilde{x}^{\top}A\tilde{x} \mid\tilde{x}\in\tilde{T}\})\). Define the function \(f(\tilde{x}):=(\tilde{x}^{\top}A\tilde{x}-\widetilde{y}_{\mathrm{median}})^{2}\). Let \(m_{1}\) be a number less than \(|S|/3\). Denote by \(q_{i}\) the \(i\)-th smallest point of \(\{\tilde{x}^{\top}A\tilde{x}\mid\tilde{x}\in\tilde{T}\}\). If \(q_{|T|-m_{1}}-q_{m_{1}}\leq R\) and \(\mathbf{E}_{X\sim T}\left[f(x)\right]>C^{\prime}R^{2}/\alpha^{3}\) for \(C^{\prime}\geq 720/\epsilon_{0}\), that is, in the case where the check in Line12 fails, then, the function \(f(\cdot)\) satisfies \(\sum_{\tilde{x}\in\tilde{T}}f(\tilde{x})>\frac{40}{\epsilon_{0}}\frac{1}{ \alpha^{3}}\sum_{\tilde{x}\in\tilde{S}\cap\tilde{T}}f(\tilde{x})\)._

The score ratio determines (in expectation) the ratio between the number of outliers and inliers removed. In Lemma3.5, the ratio is in the order of \(1/\alpha^{3}\)--this will allow us to guarantee that at the end of the entire recursive execution of Algorithm1, we would have removed at most a \(0.01\alpha\) fraction of the inliers. See RemarkF.5 in AppendixF for more details.

### Divider: Identifying Multiple Clusters and Recursing

The previous subsections covered the cases where (i) the expected score is small, or (ii) the expected score over \(\widetilde{T}\) is large and the \(\alpha\) and \(1-\alpha\) quantiles of \(\{\widetilde{X}^{\top}A\widetilde{X}\ :\ \widetilde{x}\in\widetilde{T}\}\) are close to each other. What remains is the case when both the expected score is large yet the quantiles are far apart. In this instance, we will not be able to make progress via filtering using the above argument. This is actually an intuitively reasonable scenario, since the outliers may in fact have another \(\approx\alpha m\) samples that are distributed as a different Gaussian with a very different covariance--the algorithm would not be able to tell which Gaussian is supposed to be the inliers. We will argue that, when both the expected score is large and the quantiles are far apart, the samples are in fact easy to bipartition into 2 clusters, such that the most of the inliers fall within 1 side of the bipartition. This allows us to make progress outside of filtering, and this clustering mechanism also allows us to handle Gaussian mixture models and make sure we (roughly) handle components separately.

The key intuition is that, by the stability Conditions (L.3) and (L.4), we know that the inliers under the 1-dimensional projection \(\{\widetilde{X}^{\top}A\widetilde{X}\ :\ \widetilde{X}\in\widetilde{S}\cap \widetilde{T}\}\) must be well-concentrated, in fact lying in an interval of length \(\widetilde{O}(1/\alpha)\). The fact that the quantile range is wide implies that there must be some point within the range that is close to very few samples \(\widetilde{X}^{\top}A\widetilde{X}\), by an averaging argument. We can then use the point as a threshold to bipartition our samples. See below for the precise statement.

```
1:functionFindDivider(\(T\),\(n^{\prime}\),\(m_{1}\))
2: Let the \(m_{1}\)-th smallest point be \(q_{m_{1}}\) and \(m_{1}\)-th largest point be \(q_{|T|-m_{1}}\).
3: Divide the interval \([q_{m_{1}},q_{|T|-m_{1}}]\) into \(2m^{\prime}/n^{\prime}\) equally-sized subintervals.
4: Find a subinterval \(I^{\prime}\) with at most \(n^{\prime}/2\) points and return its midpoint. ```

**Algorithm 2** Divider for list decoding

**Lemma 3.6** (Divider Algorithm).: _Let \(T=(y_{1},\ldots,y_{m^{\prime}})\) be a set of \(m^{\prime}\) points in \(\mathbb{R}\). Let \(S_{\mathrm{proj}}\subset T\) be a set of \(n^{\prime}\) points such that \(S_{\mathrm{proj}}\) is supported on an interval \(I\) of length \(r\). For every \(i\in[m^{\prime}]\), let the \(i\)-th smallest point of the set \(T\) be \(q_{i}\). Suppose \(q_{|T|-m_{1}}-q_{m_{1}}\geq R\) such that \(R\geq 10(m^{\prime}/n^{\prime})r\). Then, given \(T\) and \(n^{\prime},m_{1}\), Algorithm 2 returns a point \(t\) such that if we define \(T_{1}=\{x\in T:x\leq t\}\) and \(T_{2}=T\setminus T_{1}\) then: (i) \(\min(|T_{1}|,|T_{2}|)\geq m_{1}\) and (ii) \(S_{\mathrm{proj}}\subseteq T_{1}\) or \(S_{\mathrm{proj}}\subseteq T_{2}\)._

Proof.: The last step of the algorithm must succeed by an averaging argument. Consider the midpoint \(t\) of the returned subinterval \(I^{\prime}\), which is at least \(q_{m_{1}}\) and at most \(q_{|T|-m_{1}}\). Since \(T_{1}\) contains all points at most \(q_{m_{1}}\), and \(T_{2}\) contains all points at most \(q_{|T|-m_{1}}\), we must have \(\min(|T_{1}|,|T_{2}|)\geq m_{1}\). Lastly, we verify the second desideratum, which holds if \(t\notin I\). For the sake of contradiction, if \(t\in I\), then since \(I\) has length \(r\) and \(I^{\prime}\) has length at least \(R/(2m/n^{\prime})\geq 5r\), then \(I\subseteq I^{\prime}\). However, since \(|I^{\prime}\cap T|\leq n^{\prime}/2\), we know that \(I\) cannot be strictly contained in \(I^{\prime}\), reaching the desired contradiction. 

## 4 High-Level Proof Sketch of Theorem 1.2

We discuss the proof strategy of Theorem 1.2 at a high level. See F for the complete proof.

**Proof Sketch** Recall that Algorithm 1 is a recursive algorithm: each call repeatedly filters out samples before either terminating or splitting the dataset into two and recursively calling itself. The execution of Algorithm 1 can thus be viewed as a binary tree, with each node being a recursive call.

The high-level idea for proving Theorem 1.2 is straightforward, though involving technical calculations to implement. Consider the subtree grown from the root recursive call, up to and including a certain level \(j\). We proceed by induction on the height \(j\) of such subtree, and claim there must exists a leaf node in this subtree such that most of the inliers remain in the input dataset of the leaf node.

Concretely, let \(\mathcal{T}_{j}\) be the subtree of height \(j\) grown from the root node. We claim that there must be a leaf in this subtree, whose input set \(T\) satisfies

\[\alpha^{3}(\epsilon_{0}/40)|T|+|S\setminus T|\leq(j+1)\alpha^{3}(\epsilon_{0} /20)m+\ell\,\] (1)

recalling that \(\alpha\) is the proportion of inliers, \(\epsilon_{0}\) is the maximum fraction of inliers removed by the adversary, \(\ell\) is the actual (unknown) number of inliers removed and \(m\) is the size of the original dataset returned by the adversary. The left hand side keeps track of both a) how many inliers we have accidentally removed, through the \(|S\setminus T|\) term, and b) the relative proportions of the outliers we have removed versus the inliers we have removed, by comparing both terms on the left hand side.

For the induction step, we need to analyze the execution of a single recursive call. We show that (1) implies Assumption 3.1, and so the inductive step can follow the case analysis outlined in Section 3--either we terminate, or we decide to either filter or bipartition the sample set. To convert this into a high-probability statement, we use a standard (sub-)martingale argument in F.1. 

## Acknowledgements

We thank the anonymous reviewers for insightful comments and suggestions on this work. Ilias Diakonikolas is supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), and a DARPA Learning with Less Labels (LwLL) grant. Daniel M. Kane is supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER). Jasper C.H. Lee is supported in part by the generous funding of a Croucher Fellowship for Postdoctoral Research, NSF award DMS-2023239, NSF Medium Award CCF-2107079 and NSF AiTF Award CCF-2006206. Ankit Pensia is supported by NSF Awards CCF-1652862, and CCF-1841190, and CCF-2011255; The majority of this work was done while Ankit Pensia was at UW-Madison. Thanksi Pittas is supported by NSF Medium Award CCF-2107079 and NSF Award DMS-2023239 (TRIPODS).

## References

* [AK05] S. Arora and R. Kannan. "Learning mixtures of separated nonspherical Gaussians". In: _The Annals of Applied Probability_ 15.1A (2005), pp. 69-92.
* [BBHLT20] M. Brennan, G. Bresler, S. B. Hopkins, J. Li, and T.Schramm. "Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent". In: _CoRR_ abs/2009.06107 (2020).

* [BBV08] M.-F. Balcan, A. Blum, and S. Vempala. "A discriminative framework for clustering via similarity functions". In: _STOC_. 2008, pp. 671-680.
* [BDHKKK20] A. Bakshi, I. Diakonikolas, S. B. Hopkins, D. M. Kane, S. Karmalkar, and P. K. Kothari. "Outlier-Robust Clustering of Gaussians and Other Non-Spherical Mixtures". In: _Proc. 61st IEEE Symposium on Foundations of Computer Science (FOCS)_. 2020.
* [BDJKKV22] A. Bakshi, I. Diakonikolas, H. Jia, D. M. Kane, P. K. Kothari, and S. S. Vempala. "Robustly Learning Mixtures of k Arbitrary Gaussians". In: _Proc. 54th Annual ACM Symposium on Theory of Computing (STOC)_. 2022.
* [Ber18] D. S. Bernstein. _Scalar, Vector, and Matrix Mathematics: Theory, Facts, and Formulas_. Revised and expanded edition. Princeton: Princeton University Press, 2018. isbn: 978-0-691-15120-5 978-0-691-17653-6.
* [BK21] A. Bakshi and P. Kothari. "List-Decodable Subspace Recovery: Dimension Independent Error in Polynomial Time". In: _Proc. 32nd Annual Symposium on Discrete Algorithms (SODA)_. 2021.
* [BS10] M. Belkin and K. Sinha. "Polynomial Learning of Distribution Families". In: _FOCS_. 2010, pp. 103-112.
* [CMY20] Y. Cherapanamjeri, S. Mohanty, and M. Yau. "List decodable mean estimation in nearly linear time". In: _Proc. 61st IEEE Symposium on Foundations of Computer Science (FOCS)_. 2020.
* [CSV17] M. Charikar, J. Steinhardt, and G. Valiant. "Learning from untrusted data". In: _Proceedings of STOC 2017_. 2017, pp. 47-60.
* [Das99] S. Dasgupta. "Learning mixtures of Gaussians". In: _Proceedings of the 40th Annual Symposium on Foundations of Computer Science_. 1999, pp. 634-644.
* [DHKK20] I. Diakonikolas, S. B. Hopkins, D. Kane, and S. Karmalkar. "Robustly Learning any Clusterable Mixture of Gaussians". In: _CoRR_ abs/2005.06417 (2020).
* [DJKS22] A. Das, A. Jain, W. Kong, and R. Sen. "Efficient List-Decodable Regression using Batches". In: _CoRR_ abs/2211.12743 (2022).
* [DK23] I. Diakonikolas and D. M. Kane. _Algorithmic High-Dimensional Robust Statistics_. Cambridge University Press, 2023.
* [DKK20a] I. Diakonikolas, D. M. Kane, and D. Kongsgaard. "List-decodable mean estimation via iterative multi-filtering". In: _Advances in Neural Information Processing Systems 33 (NeurIPS)_. 2020.
* [DKK20b] Ilias Diakonikolas, Daniel Kane, and Daniel Kongsgaard. "List-decodable mean estimation via iterative multi-filtering". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 9312-9323.
* [DKKLMS16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. "Robust Estimators in High Dimensions without the Computational Intractability". In: _Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS)_. 2016, pp. 655-664.
* [DKKLT21] I. Diakonikolas, D. M. Kane, D. Kongsgaard, J. Li, and K. Tian. "List-decodable mean estimation in nearly-pca time". In: _Advances in Neural Information Processing Systems 34 (NeurIPS)_. Vol. 34. 2021.
* [DKKLT22] I. Diakonikolas, D. M. Kane, D. Kongsgaard, J. Li, and K. Tian. "Clustering Mixture Models in Almost-Linear Time via List-Decodable Mean Estimation". In: _Proc. 54th Annual ACM Symposium on Theory of Computing (STOC)_. 2022.
* [DKKPP22] I. Diakonikolas, D. M. Kane, S. Karmalkar, A. Pensia, and T. Pittas. "List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering". In: _Advances in Neural Information Processing Systems 35 (NeurIPS)_. 2022.
* [DKPPS21] I. Diakonikolas, D. M. Kane, A. Pensia, T. Pittas, and A. Stewart. "Statistical query lower bounds for list-decodable linear regression". In: _Advances in Neural Information Processing Systems 34 (NeurIPS)_. 2021.
* [DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. "Statistical Query Lower Bounds for Robust Estimation of High-Dimensional Gaussians and Gaussian Mixtures". In: _Proc. 58th IEEE Symposium on Foundations of Computer Science (FOCS)_. 2017, pp. 73-84.

* [DKS18] I. Diakonikolas, D. M. Kane, and A. Stewart. "List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians". In: _Proc. 50th Annual ACM Symposium on Theory of Computing (STOC)_. 2018.
* [DMR18] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. "The total variation distance between high-dimensional Gaussians with the same mean". In: _arXiv preprint arXiv:1810.08693_ (2018).
* [DS07] S. Dasgupta and L. J. Schulman. "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians". In: _Journal of Machine Learning Research_ 8 (2007), pp. 203-226.
* [HR09] P. J. Huber and E. M. Ronchetti. _Robust Statistics_. Wiley Series in Probability and Statistics. Hoboken, NJ, USA: John Wiley & Sons, Inc., 2009. isbn: 978-0-470-43469-7 978-0-470-12990-6.
* [Hub64] P. J. Huber. "Robust Estimation of a Location Parameter". In: _The Annals of Mathematical Statistics_ 35.1 (Mar. 1964), pp. 73-101. issn: 0003-4851.
* [IK22] M. Ivkov and P. K. Kothari. "List-decodable covariance estimation". In: _Proc. 54th Annual ACM Symposium on Theory of Computing (STOC)_. 2022.
* [KK10] A. Kumar and R. Kannan. "Clustering with spectral norm and the k-means algorithm". In: _2010 IEEE 51st Annual Symposium on Foundations of Computer Science_. IEEE. 2010, pp. 299-308.
* [KKK19] S. Karmalkar, A. Klivans, and P. Kothari. "List-decodable linear regression". In: _Advances in Neural Information Processing Systems_. 2019, pp. 7423-7432.
* [Kot21] P. Kothari. _Robustly Learning Mixtures of Gaussians_. TCS+ Talk. Available online at: https://youtu.be/Yjk0Xzvjy0w7t=2593. 2021.
* [KS17] P. K. Kothari and J. Steinhardt. "Better Agnostic Clustering Via Relaxed Tensor Norms". In: _CoRR_ abs/1711.07465 (2017).
* [Li18] J. Li. "Principled approaches to robust machine learning and beyond". PhD thesis. Massachusetts Institute of Technology, 2018.
* [LM21] A. Liu and A. Moitra. "Settling the Robust Learnability of Mixtures of Gaussians". In: _Proc. 53rd Annual ACM Symposium on Theory of Computing (STOC)_. 2021.
* [LM22] A. Liu and A. Moitra. "Learning GMMs with Nearly Optimal Robustness Guarantees". In: _Proc. 35th Annual Conference on Learning Theory (COLT)_. 2022.
* [LRV16] K. A. Lai, A. B. Rao, and S. Vempala. "Agnostic Estimation of Mean and Covariance". In: _Proceedings of FOCS'16_. 2016.
* [MV10] A. Moitra and G. Valiant. "Settling the polynomial learnability of mixtures of Gaussians". In: _FOCS_. 2010, pp. 93-102.
* [MV18] M. Meister and G. Valiant. "A data prism: Semi-verified learning in the small-alpha regime". In: _Proc. 31st Annual Conference on Learning Theory (COLT)_. PMLR. 2018, pp. 1530-1546.
* [RY20a] P. Raghavendra and M. Yau. "List decodable learning via sum of squares". In: _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_. SIAM. 2020, pp. 161-180.
* [RY20b] P. Raghavendra and M. Yau. "List Decodable Subspace Recovery". In: _Proc. 33rd Annual Conference on Learning Theory (COLT)_. 2020.
* [SVC16] J. Steinhardt, G. Valiant, and M. Charikar. "Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction". In: _Advances in Neural Information Processing Systems 29 (NeurIPS)_. 2016.
* [Vem22] S. Vempala. _Robustly Learning Mixtures of Arbitrary Gaussians in Polynomial Time_. Bangalore Theory Seminar Series. Available online at: https://youtu.be/eF9XYJRDFTg7t=3107. 2022.
* [VW04] S. Vempala and G. Wang. "A spectral algorithm for learning mixture models". In: _Journal of Computer and System Sciences_ 68.4 (2004), pp. 841-860.
* [ZS22] S. Zeng and J. Shen. "List-Decodable Sparse Mean Estimation". In: _Advances in Neural Information Processing Systems 35 (NeurIPS)_. 2022.

Additional Related Work

Gaussian Mixture ModelsGaussian mixture models (GMM) have a long history in statistics and theoretical computer science. In theoretical computer science, a system investigation of efficient algorithms for GMM was initiated by [11], and then improved upon in a series of works, see, for example, [12, 13, 14]. However, these algorithms imposed additional assumptions on the parameters of GMM, for example, separation between the components. Later, [12, 13] gave algorithms without any assumptions on the components but these algorithms were sensitive to outliers. The field of algorithmic robust statistics has developed in the last decade to develop outlier-robust algorithms. A recent line of work [1, 10, 11, 12] has developed efficient algorithms for learning GMM robustly.

List-decodable LearningThe model of list-decodable learning was first introduced in [1] and was first used in the context of high-dimensional estimation in [10]. Regarding mean estimation in particular, a body of work presented algorithms with increasingly better guarantees [1, 1, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22].

## Appendix B Omitted Preliminaries

### Notation

We use \(\mathbb{Z}_{+}\) to denote positive integers. For \(n\in\mathbb{Z}_{+}\), we denote \([n]\stackrel{{\mathrm{def}}}{{=}}\{1,\ldots,n\}\) and use \(\mathcal{S}^{d-1}\) for the \(d\)-dimensional unit sphere. For a vector \(v\), we let \(\|v\|_{2}\) denote its \(\ell_{2}\)-norm. We use \(I_{d}\) to denote the \(d\times d\) identity matrix. We will drop the subscript when it is clear from the context. For a matrix \(A\), we use \(\|A\|_{\mathrm{F}}\) and \(\|A\|_{\mathrm{op}}\) to denote the Frobenius and spectral (or operator) norms respectively. We say that a symmetric \(d\times d\) matrix \(A\) is PSD (positive semidefinite) and write \(A\succeq 0\) if for all vectors \(x\in\mathbb{R}^{d}\) we have that \(x^{\top}Ax\geq 0\). We denote \(\lambda_{\max}(A):=\max_{x\in\mathcal{S}^{d-1}}x^{\top}Ax\). We write \(A\preceq B\) when \(B-A\) is PSD. For a matrix \(A\in\mathbb{R}^{d\times d}\), \(\mathrm{tr}(A)\) denotes the trace of the matrix A. We denote by \(\langle v,u\rangle\), the standard inner product between the vectors \(u,v\). For matrices \(U,V\in\mathbb{R}^{d\times d}\), we generalize the inner product as follows: \(\langle U,V\rangle=\sum_{ij}U_{ij}V_{ij}\). For a matrix \(A\in\mathbb{R}^{d\times d}\), we use \(A^{\flat}\) to denote the flattened vector in \(\mathbb{R}^{d^{2}}\), and for a \(v\in\mathbb{R}^{d^{2}}\), we use \(v^{\sharp}\) to denote the unique matrix \(A\) such that \(A^{\flat}=v^{\sharp}\). For a matrix \(A\), we let \(A^{\dagger}\) denote its pseudo-inverse. We use \(\otimes\) to denote the Kronecker product. For a matrix \(A\), we use \(\ker(A)\) for the null space of \(A\) and use \(\mathbb{P}_{A}\) to denote the projection onto the range of \(A\), i.e., \(\mathbb{P}_{A}=AA^{\dagger}\)[1, Proposition 8.1.7 (xi)].

We use the notation \(X\sim D\) to denote that a random variable \(X\) is distributed according to the distribution \(D\). For a random variable \(X\), we use \(\mathbf{E}[X]\) for its expectation. We use \(\mathcal{N}(\mu,\Sigma)\) to denote the Gaussian distribution with mean \(\mu\) and covariance matrix \(\Sigma\). For a set \(S\), we use \(\mathcal{U}(S)\) to denote the uniform distribution on \(S\) and use \(X\sim S\) as a shortcut for \(X\sim\mathcal{U}(S)\).

We use \(a\lesssim b\) to denote that there exists an absolute universal constant \(C>0\) (independent of the variables or parameters on which \(a\) and \(b\) depend) such that \(a\leq Cb\).

### Miscellaneous Facts

In this section, we state standard facts that will be used in our analysis.

**Fact B.1**.: _For any matrix \(A\in\mathbb{R}^{d\times d}\), \(\mathbf{Var}_{X\sim\mathcal{N}(\mu,\Sigma)}[X^{\top}AX]\leq 4\|\Sigma^{1/2}A \Sigma^{1/2}\|_{\mathrm{F}}^{2}+8\|\Sigma^{1/2}A\mu\|_{2}^{2}\)._

Proof.: We write \(X\sim\mathcal{N}(\mu,\Sigma)\) as \(X=\Sigma^{1/2}Z+\mu\) for \(Z\sim\mathcal{N}(0,I)\). We have that

\[\mathop{\mathbf{Var}}_{X\sim\mathcal{N}(\mu,\Sigma)}[X^{\top}AX] =\mathop{\mathbf{Var}}_{Z\sim\mathcal{N}(0,I)}[(\Sigma^{1/2}Z+ \mu)^{\top}A(\Sigma^{1/2}Z+\mu)]\] \[=\mathop{\mathbf{Var}}_{Z\sim\mathcal{N}(0,I)}[Z^{\top}\Sigma^{1/ 2}A\Sigma^{1/2}Z+2\mu^{\top}A\Sigma^{1/2}Z+\mu^{\top}A\mu]\] \[\leq 2\left(\mathop{\mathbf{Var}}_{Z\sim\mathcal{N}(0,I)}[Z^{ \top}\Sigma^{1/2}A\Sigma^{1/2}Z]+4\mathop{\mathbf{Var}}_{Z\sim\mathcal{N}(0,I) }[\mu^{\top}A\Sigma^{1/2}Z]\right)\] \[\leq 4\|\Sigma^{1/2}A\Sigma^{1/2}\|_{\mathrm{F}}^{2}+8\|\Sigma^{1/ 2}A\mu\|_{2}^{2}\,\] (2)where the first inequality line uses that \(\mathbf{Var}[A+B]=\mathbf{Var}[A]+\mathbf{Var}[B]+2\,\mathbf{Cov}[A,B]\leq\mathbf{ Var}[A]+\mathbf{Var}[B]+2\sqrt{\mathbf{Var}[A]\,\mathbf{Var}[B]}=(\sqrt{\mathbf{Var}[A]}+ \sqrt{\mathbf{Var}[B]})^{2}\leq 2(\mathbf{Var}[A]+\mathbf{Var}[B])\), and the last inequality uses the following: The first term uses the fact that for a symmetric matrix \(B\) and a vector \(v\), \(\mathbf{Var}_{Z\sim\mathcal{N}(0,I)}[Z^{\top}BZ]=2\|B\|_{\text{F}}^{2}\) and \(\mathbf{Var}_{Z\sim\mathcal{N}(0,I)}[v^{\top}Z]=\|v\|_{2}^{2}\), with \(B:=\Sigma^{1/2}A\Sigma^{1/2}\) and \(v:=\Sigma^{1/2}A\mu\). 

**Fact B.2** (Azuma-Hoeffding Inequality).: _Let \(\{X_{n}\}_{n\in\mathbb{N}}\) be a submartingale or supermatingale with respect to a sequence \(\{Y_{n}\}_{n\in\mathbb{N}}\). If for all \(n=1,2,\ldots\) it holds \(|X_{n}-X_{n-1}|\leq c_{n}\) almost surely, then for any \(\epsilon>0\) and \(n\in\mathbb{N}\)_

\[\Pr\left[X_{n}-X_{t}>\epsilon\right]\leq\exp\left(-\frac{2\epsilon^{2}}{\sum_{ t=1}^{\infty}c_{t}}\right)\.\]

**Fact B.3** (See, e.g., [1, Proposition 11.10.34]).: _Let \(R\) and \(M\) be two square matrices, then \(\|RM\|_{\text{F}}\leq\min\left((\|R\|_{\text{F}}\|M\|_{\text{op}}\,,\|R\|_{ \text{op}}\|M\|_{\text{F}})\right)\)._

**Fact B.4** (Frobenius norm and projection matrices).: _Let \(A\) be a symmetric matrix. If \(L_{1}\) and \(L_{2}\) are two PSD projection matrices satisfying \(L_{1}\preceq L_{2}\), then \(\|L_{1}AL_{1}^{\top}\|_{\text{F}}\leq\|L_{2}AL_{2}^{\top}\|_{\text{F}}\)._

**Fact B.5** (Pseudo-inverse of a matrix).: _Let \(G\) be a square matrix and define \(R=G^{\dagger}\). Let the SVD of \(G\) be \(U\Lambda V^{\top}\). Then \(R=V\Lambda^{\dagger}U^{\top}\), \(\mathbb{P}_{G}=U\Lambda\Lambda^{\dagger}U^{\top}\), and \(R=R\mathbb{P}_{G}\). Moreover, Let \(\Delta\) be any arbitrary diagonal matrix such that if \(\Lambda_{i,i}=0\), then \(\Delta_{i,i}=0\), and define \(L=V\Delta V^{\top}\). Then \(L=LRG\)._

Proof.: The first follows from [1, Chapter 8], the second follows from [1, Proposition 8.1.7 (xii)], and the third follows from the first two and the fact that \(\Lambda^{\dagger}\Lambda\Lambda^{\dagger}=\Lambda\). The final claim follows from the following series of equalities:

\[LRG =V\Delta V^{\top}V\Lambda^{\dagger}U^{\top}U\Lambda V^{\top}\] \[=V\Delta V^{\top}V\Lambda^{\dagger}U^{\top}U\Lambda V^{\top}\] \[=V\Delta\Lambda^{\dagger}\Lambda V^{\top}\] \[=V\Delta V^{\top}=L,\]

where we use that \(\Delta\) is diagonal and for each \(i\) such that \(\Delta_{i,i}\neq 0\), \(\Lambda_{i,i}^{\dagger}\Lambda_{i,i}=1\).

**Fact B.6** ([1, Fact 8.4.16]).: _Let \(A\) and \(B\) be two square matrices and suppose that \(A\) is full rank. Then \(\mathbb{P}_{BA}=\mathbb{P}_{B}\)._

## Appendix C Linear Algebraic Results

In this section, we prove the proofs of certain linear algebraic results that we need. In particular, we provide the proofs of Lemmata C.1 and C.3 that will be used later.

### Relative Frobenius Norm to Frobenius Norm: Proof of Lemma c.1

In this section, we provide the proof of Lemma c.1.

**Lemma C.1**.: _Consider two arbitrary full rank, positive definite matrices \(\Sigma_{1}\) and \(\Sigma_{2}\), and suppose there exists a full rank, positive definite matrix \(H\) such that (i) \(\|I-H^{\dagger/2}\Sigma_{1}H^{\dagger/2}\|_{\text{F}}\leq\rho\) and (ii) \(\|I-H^{\dagger/2}\Sigma_{2}H^{\dagger/2}\|_{\text{F}}\leq\rho\). Then, for an arbitrary full rank, positive definite matrix \(\Sigma\), we have_

\[\|\Sigma^{-1/2}\Sigma_{1}\Sigma^{-1/2}-\Sigma^{-1/2}\Sigma_{2}\Sigma^{-1/2}\|_ {\text{F}}\leq 5\rho\max(\|\Sigma^{-1/2}\Sigma_{1}\Sigma^{-1/2}\|_{\text{op}},\| \Sigma^{-1/2}\Sigma_{2}\Sigma^{-1/2}\|_{\text{op}}).\]

Proof of Lemma c.1.: Let \(G=H^{-1/2}\Sigma^{1/2}\). Let \(\widetilde{\Sigma}_{1}=\Sigma^{-1/2}\Sigma_{1}\Sigma^{-1/2}\) and \(\widetilde{\Sigma}_{2}=\Sigma^{-1/2}\Sigma_{2}\Sigma^{-1/2}\), both of which are positive definite matrices. Since \(\Sigma\) is full rank, we obtain that

\[\Sigma_{1} =\Sigma^{1/2}\Sigma^{-1/2}\Sigma_{1}\Sigma^{-1/2}\Sigma^{1/2}= \Sigma^{1/2}\widetilde{\Sigma}_{1}\Sigma^{1/2}\] \[\Sigma_{2} =\Sigma^{1/2}\Sigma^{-1/2}\Sigma_{2}\Sigma^{-1/2}\Sigma^{1/2}= \Sigma^{1/2}\widetilde{\Sigma}_{2}\Sigma^{1/2}.\]This directly gives us that \(G\widetilde{\Sigma}_{1}G^{\top}=I-H\Sigma_{1}H\) and \(G\widetilde{\Sigma}_{2}G^{\top}=I-H\Sigma_{2}H\). Overall, we have obtained the following:

\[\|I-G\widetilde{\Sigma}_{1}G^{\top}\|_{\mathrm{F}}\leq\rho\ \ \ \text{and}\ \ \ \|I-G \widetilde{\Sigma}_{2}G^{\top}\|_{\mathrm{F}}\leq\rho.\]

We first state a result, proved in the end of this section, that transfers closeness in relative Frobenius norm to Frobenius norm of the difference (along most of the directions).

**Lemma C.2**.: _Let \(G\) be a \(d\times d\) matrix and let \(B\) be an arbitrary symmetric matrix of the same dimension. Recall that \(\mathbb{P}_{G}\) denotes the projection matrix onto range of \(G\). Suppose that_

\[\|\mathbb{P}_{G}-GBG^{\top}\|_{\mathrm{F}}\leq\rho\]

_for some \(\rho\geq 1\). Let the SVD of \(G\) be \(G=U\Lambda V^{\top}\) for some orthonormal matrices \(U\) and \(V\) and a diagonal matrix \(\Lambda\)._

_Then there exists a \(d\times d\) matrix \(L\) such that_

1. _(_\(L\) captures directions of_ \(G\)_)_ \(L=V\Delta_{B}V^{\top}\) _for some binary diagonal matrix_ \(\Delta_{B}\preceq\Lambda\Lambda^{\dagger}\)_. That is, the diagonal entries of_ \(\Delta_{B}\) _are all in_ \(\{0,1\}\)_, and furthermore, all the 1 diagonal entries of_ \(\Delta_{B}\) _are also non-zero diagonal entries in_ \(\Lambda\)_._
2. _(rank of_ \(L\)_) The rank of_ \(L\) _satisfies_ \(\|\Lambda-\Delta_{B}\|_{\mathrm{F}}\leq 2\rho\) _and_
3. _(closeness of_ \(G^{\dagger}\) _and_ \(B\) _along_ \(L\)_)_ \(\left\|L\left(RR^{\top}-B\right)L^{\top}\right\|_{\mathrm{F}}\leq 2\rho\|B\|_{ \mathrm{op}}\)_, where_ \(R=G^{\dagger}\)_._

We will now apply Lemma C.2 twice following the same notation as in its statement. In particular, the SVD decomposition of \(G=U\Delta V^{\top}\) and \(R=G^{\dagger}\). Lemma C.2 implies that there exist two binary diagonal matrices \(\Delta_{1}\) and \(\Delta_{2}\), where \(\Delta_{1}\) is for \(\widetilde{\Sigma}_{1}\) and \(\Delta_{2}\) is for \(\widetilde{\Sigma}_{2}\) such that (i) both \(\|I-\Delta_{1}\|_{\mathrm{F}}\) and \(\|I-\Delta_{1}\|_{\mathrm{F}}\) are at \(2\rho\), and (ii) \(V\Delta_{1}V^{\top}(RR^{\top}-\widetilde{\Sigma}_{1})V\Delta_{1}V^{\top}\) and \(V\Delta_{2}V^{\top}(RR^{\top}-\widetilde{\Sigma}_{2})V\Delta_{2}V^{\top}\) have Frobenius norms at most \(2\rho\|\widetilde{\Sigma}_{1}\|_{\mathrm{op}}\) and \(2\rho\|\widetilde{\Sigma}_{2}\|_{\mathrm{op}}\), respectively. Let \(\Delta=\Delta_{1}\times\Delta_{2}\) and define \(L=V\Delta V^{\top}\). By (i), we have that \(\|I-L\|_{\mathrm{F}}=\|I-\Delta\|_{\mathrm{F}}\leq 4\rho\). Since \(L\preceq V\Delta_{1}V^{\top}\) and \(L\) is a projection matrix, Fact B.4 along with (ii) above implies that

\[\|L(RR^{\top}-\widetilde{\Sigma}_{1})L^{\top}\|_{\mathrm{F}}\leq 2\rho\| \widetilde{\Sigma}_{1}\|_{\mathrm{op}}.\]

Similarly for \(\widetilde{\Sigma}_{2}\). By triangle inequality, we obtain that

\[\|L(\widetilde{\Sigma}_{1}-\widetilde{\Sigma}_{2})L^{\top}\|_{\mathrm{F}}\leq 4 \rho\max\left(\|\widetilde{\Sigma}_{1}\|_{\mathrm{op}},\|\widetilde{\Sigma}_{2 }\|_{\mathrm{op}}\right).\]

Since \(L\) is a projection matrix, triangle inequality implies that

\[\left\|\left(\widetilde{\Sigma}_{1}-\widetilde{\Sigma}_{2}\right)\right\|_{ \mathrm{F}} \leq\left\|L\left(\widetilde{\Sigma}_{1}-\widetilde{\Sigma}_{2} \right)L^{\top}\right\|_{\mathrm{F}}+\left\|(I-L)\left(\widetilde{\Sigma}_{1}- \widetilde{\Sigma}_{2}\right)(I-L)^{\top}\right\|_{\mathrm{F}}\] \[\lesssim\left\|L\left(\widetilde{\Sigma}_{1}-\widetilde{\Sigma}_{2 }\right)L^{\top}\right\|_{\mathrm{F}}+\|I-L\|_{\mathrm{F}}\cdot\left\|\left( \widetilde{\Sigma}_{1}-\widetilde{\Sigma}_{2}\right)\right\|_{\mathrm{op}}\] \[\lesssim\rho\cdot\max\left(\|\widetilde{\Sigma}_{1}\|_{\mathrm{op }},\|\widetilde{\Sigma}_{2}\|_{\mathrm{op}}\right)\.\]

This completes the proof.

We now provide the proof of Lemma C.2.

**Lemma C.2**.: _Let \(G\) be a \(d\times d\) matrix and let \(B\) be an arbitrary symmetric matrix of the same dimension. Recall that \(\mathbb{P}_{G}\) denotes the projection matrix onto range of \(G\). Suppose that_

\[\|\mathbb{P}_{G}-GBG^{\top}\|_{\mathrm{F}}\leq\rho\]

_for some \(\rho\geq 1\). Let the SVD of \(G\) be \(G=U\Lambda V^{\top}\) for some orthonormal matrices \(U\) and \(V\) and a diagonal matrix \(\Lambda\)._

_Then there exists a \(d\times d\) matrix \(L\) such that_1. _(_\(L\) _captures directions of_ \(G\)_)_ \(L=V\Delta_{B}V^{\top}\) _for some binary diagonal matrix_ \(\Delta_{B}\preceq\Lambda\Lambda^{\dagger}\)_. That is, the diagonal entries of_ \(\Delta_{B}\) _are all in_ \(\{0,1\}\)_, and furthermore, all the 1 diagonal entries of_ \(\Delta_{B}\) _are also non-zero diagonal entries in_ \(\Lambda\)_._
2. _(rank of_ \(L\)_) The rank of_ \(L\) _satisfies_ \(\|\Lambda-\Delta_{B}\|_{\mathrm{F}}\leq 2\rho\) _and_
3. _(closeness of_ \(G^{\dagger}\) _and_ \(B\) _along_ \(L\)_)_ \(\left\|L\left(RR^{\top}-B\right)L^{\top}\right\|_{\mathrm{F}}\leq 2\rho\|B\|_{ \text{op}}\)_, where_ \(R=G^{\dagger}\)_._

Proof.: Let \(u_{1},u_{2},\ldots,u_{d}\) be the columns of \(U\) and let \(v_{1},v_{2},\ldots,v_{d}\) be the columns of \(V\). Let the entries of \(\Lambda\) be \(\lambda_{1},\lambda_{2},\ldots,\lambda_{d}\), which are non-negative by SVD property. Let \(\mathcal{J}\) be the set of \(i\)'s in \([d]\) such that \(\lambda_{i}>0\).

Define the matrices \(F:=U^{\top}GBG^{\top}U\) and \(B^{\prime}:=V^{\top}BV\). Then since \(G^{\top}=V\Lambda U^{\top}\), we have

\[F=U^{\top}GBG^{\top}U=U^{\top}U\Lambda V^{\top}BV\Lambda U^{\top}U=\Lambda V^{ \top}BV\Lambda=\Lambda B^{\prime}\Lambda.\]

Thus the \((i,j)\)-th entry of \(F\) is \(B^{\prime}_{i,j}\lambda_{i}\lambda_{j}\).

Now define the matrix \(M:=U^{\top}\mathbb{P}_{G}U\). From Fact B.5, we have that \(\mathbb{P}_{G}=U\Lambda\Lambda^{\dagger}U^{\top}\). And thus, \(M=\Lambda\Lambda^{\dagger}\), which is the diagonal matrix with entries \(M_{ii}=1\) for each \(i\in\mathcal{J}\), and \(M_{ii}=0\) otherwise.

Using the invariance of Frobenius norm under orthogonal transform, we obtain the following series of equalities:

\[\|\mathbb{P}_{G} -GBG^{\top}\|_{\mathrm{F}}^{2}\] \[=\left\|U^{\top}\left(\mathbb{P}_{G}-GBG^{\top}\right)U\right\|_ {\mathrm{F}}^{2}\] \[=\|M-F\|_{\mathrm{F}}^{2}\] \[\geq\sum_{i\in\mathcal{J}}\left(M_{i,i}-F_{i,i}\right)^{2}\] \[=\sum_{i\in\mathcal{J}}\left(1-B^{\prime}_{i,i}\cdot\lambda_{i}^{ 2}\right)^{2}\] (3)

Let \(\mathcal{I}\subseteq\mathcal{J}\) be the set of \(i\)'s in \(\mathcal{J}\) such that \(\lambda_{i}^{2}B^{\prime}_{i,i}<(1/2)\). Then \(\sum_{i\in\mathcal{I}}\left(1-B^{\prime}_{i,i}\cdot\lambda_{i}^{2}\right)^{2}> |\mathcal{I}|/4\). Thus if \(|\mathcal{I}|\) is larger than \(4\rho^{2}\), then the Frobenius norm squared is larger than \(r^{2}\), which is a contradiction.

If \(i\in\mathcal{J}\setminus\mathcal{I}\), then \(\lambda_{i}^{2}\geq 1/(2B^{\prime}_{i,i})\) and not just \(\lambda_{i}>0\).

Define the matrix \(L\) to be \(L:=\sum_{i:i\in\mathcal{J}\setminus\mathcal{I}}v_{i}v_{i}^{\top}\), which is equal to \(V\Delta_{B}V^{\top}\) where \(\Delta_{B}\) is a diagonal matrix with the \(i\)-th diagonal entry equal to \(1\) if \(i\in\mathcal{J}\setminus\mathcal{I}\) and \(0\) otherwise. By definition, \(L\) has rank \(|\mathcal{J}|-|\mathcal{I}|\) and furthermore, the 1 diagonal entries of \(\Delta_{B}\) are always non-zero diagonal entries in \(\Lambda\).

Let \(R=G^{\dagger}\), and by Fact B.5, we have that \(R=V\Lambda^{\dagger}U^{\top}\). Thus \(LR=V\Delta_{B}V^{\top}V\Lambda^{\dagger}U^{\top}=V\Delta_{B}\Lambda^{\dagger}U ^{\top}\), and thus the singular values of \(LR\) satisfy that they are at most

\[\frac{1}{\min_{i\in\mathcal{J}\setminus\mathcal{I}}\lambda_{i}}\leq\max_{i} \sqrt{2B^{\prime}_{i,i}}\leq\sqrt{2\|B^{\prime}\|_{\text{op}}}=\sqrt{2\|V^{ \top}BV\|_{\text{op}}}=\sqrt{2\|B\|_{\text{op}}}.\]

We first need two more observations: \(L=LRG\) and \(LRR^{\top}L^{\top}=LR\mathbb{P}_{G}R^{\top}L^{\top}\), both of which hold by Fact B.5. Finally, we show that \(RR^{\top}\) and \(B\) are close along the directions in \(L\).

\[\|L (RR^{\top}-B)L^{\top}\|_{\mathrm{F}}\] \[=\|LRR^{\top}L^{\top}-LBL^{\top}\|_{\mathrm{F}}\] \[=\|LR\mathbb{P}_{G}R^{\top}L^{\top}-LRGBG^{\top}R^{\top}L^{\top} \|_{\mathrm{F}}\] \[=\|LR(\mathbb{P}_{G}-GBG^{\top})R^{\top}L^{\top}\|_{\mathrm{F}}\] \[\leq\|LR\|_{\text{op}}\|\mathbb{P}_{G}-GBG^{\top}\|_{\mathrm{F}} \|R^{\top}L^{\top}\|_{\text{op}}\] \[=\|LR\|_{\text{op}}^{2}\|\mathbb{P}_{G}-GBG^{\top}\|_{\mathrm{F}}\] \[\leq 2\|B\|_{\text{op}}\cdot\rho.\] (4)

This completes the proof.

### Proof of Lemma c.3

We now restate and prove Lemma c.3.

**Lemma C.3**.: _Let \(A\) and \(B\) be two PSD matrices with \(\mathbb{P}_{B}\preceq\mathbb{P}_{A}\) or equivalently \(\ker(A)\subseteq\ker(B)\). Then \(\|ABA-\mathbb{P}_{B}\|_{\mathrm{F}}\leq 2\|ABA-\mathbb{P}_{A}\|_{\mathrm{F}}\)._

Proof.: Applying triangle inequality, we have that

\[\|ABA-\mathbb{P}_{B}\|_{\mathrm{F}}\leq\|ABA-\mathbb{P}_{A}\|_{ \mathrm{F}}+\|\mathbb{P}_{A}-\mathbb{P}_{B}\|_{\mathrm{F}}.\]

It thus suffices to show that the second term, \(\|\mathbb{P}_{B}-\mathbb{P}_{A}\|_{\mathrm{F}}\), is less than \(\|ABA-\mathbb{P}_{A}\|_{\mathrm{F}}\). Let \(d_{A}\) be the rank of \(A\) and \(d_{B}\) be the rank of \(B\). We will show the following two results, which imply the desired bound:

\[\|\mathbb{P}_{B}-\mathbb{P}_{A}\|_{\mathrm{F}}^{2}=|d_{B}-d_{A}| \leq\|ABA-\mathbb{P}_{B}\|_{\mathrm{F}}^{2}\] (5)

In the rest of the proof, we will establish the above two inequalities.

We begin with the second inequality in (5). By the lemma assumptions, we have that the null space of \(A\) being a subspace of the null space of \(B\). This is equivalent to saying that the column space of \(B\) being a subspace of the column space of \(A\). In particular, this also implies that \(d_{B}\leq d_{A}\). It is easy to see that \(\mathrm{rank}(ABA)\leq\mathrm{rank}(B)\leq\mathrm{rank}(A)=\mathrm{rank}( \mathbb{P}_{A})\), and the eigenvalues of \(\mathbb{P}_{A}\) are either \(0\) or \(1\). We now state a simple result lower bounding the Frobenius norm by the difference of the ranks for such matrices:

**Claim C.4** (Bound on rank difference).: _Let \(A^{\prime}\) be a symmetric matrix such that the eigenvalues of \(A^{\prime}\) belong to \(\{0,1,-1\}\). Let \(B^{\prime}\) be an arbitrary matrix. Then \(\|B^{\prime}-A^{\prime}\|_{\mathrm{F}}^{2}\geq\mathrm{rank}(A^{\prime})- \mathrm{rank}(B^{\prime})\)._

Proof.: Let \(u_{1},\ldots u_{d}\) be the orthonormal eigenvectors of \(A^{\prime}\) with eigenvalues \(\lambda_{1},\lambda_{2},\ldots,\lambda_{d}\) such that \(|\lambda_{i}|=1\) for \(i\leq\mathrm{rank}(A^{\prime})\) and \(\lambda_{i}=0\) otherwise. Without loss of generality, we assume \(\mathrm{rank}(B^{\prime})\leq\mathrm{rank}(A^{\prime})\), otherwise the result is trivial. Let \(\mathcal{I}\) be the set of \(i\)'s such that \(i\leq\mathrm{rank}(A^{\prime})\) and \(B^{\prime}u_{i}=0\).

\[\|B^{\prime}-A^{\prime}\|_{\mathrm{F}}^{2} =\sum_{i=1}^{d}\|B^{\prime}u_{i}-A^{\prime}u_{i}\|_{2}^{2}\geq \sum_{i=1}^{\mathrm{rank}(A^{\prime})}\|B^{\prime}u_{i}-A^{\prime}u_{i}\|_{2}^ {2}\] \[=\sum_{i=1}^{\mathrm{rank}(A^{\prime})}\|B^{\prime}u_{i}-\lambda_ {i}u_{i}\|_{2}^{2}.\geq\sum_{i\in\mathcal{I}}\|\lambda_{i}u_{i}\|_{2}^{2}=| \mathcal{I}|.\]

Since the rank of \(B^{\prime}\) is less than the rank of \(A^{\prime}\), it follows that \(B^{\prime}u_{i}\) is nonzero for at most \(\mathrm{rank}(B^{\prime})\) many \(u_{i}\)'s in \(\{u_{i}:i\leq\mathrm{rank}(A^{\prime})\}\). Thus \(|\mathcal{I}|\) is at least \(\mathrm{rank}(A^{\prime})-\mathrm{rank}(B^{\prime})\). 

By Claim C.4, we have that \(\mathrm{rank}(\mathbb{P}_{B})-\mathrm{rank}(ABA)\leq\|\mathbb{P}_{B}-ABA\|_ {\mathrm{F}}^{2}\). Thus, it implies the desired inequality, \(|d_{B}-d_{A}|\leq\|ABA-\mathbb{P}_{A}\|_{\mathrm{F}}^{2}\).

We will now establish the first inequality in (5). Observe that both \(\mathbb{P}_{A}\) and \(\mathbb{P}_{B}\) are PSD matrices, whose eigenvalues are either \(1\) or \(0\). The following result upper bounds the Frobenius norm of \(\mathbb{P}_{A}-\mathbb{P}_{B}\) in terms of the difference of their ranks.

**Claim C.5**.: _If \(A\) and \(B\) are two PSD matrices such that \(\mathbb{P}_{B}\preceq\mathbb{P}_{A}\), then \(\|\mathbb{P}_{A}-\mathbb{P}_{B}\|_{\mathrm{F}}^{2}=\mathrm{rank}(A)-\mathrm{ rank}(B)\)._

Proof.: Let \(d_{A}\) be the rank of \(A\) and \(d_{B}\) be the rank of \(B\). We have that \(d_{B}=\mathrm{rank}(\mathbb{P}_{B})\leq\mathrm{rank}(\mathbb{P}_{A})=d_{A}\). Consider any orthonormal basis \(v_{1},\ldots,v_{d}\) in \(\mathbb{R}^{d}\), such that \(v_{1},\ldots,v_{d_{A}}\) spans the column space of \(\mathbb{P}_{A}\), and \(v_{1},\ldots,v_{d_{B}}\) spans the column space of \(B\). Since the column space of \(\mathbb{P}_{B}\) is a subspace of that of \(\mathbb{P}_{B}\), such an orthonormal basis always exists.

Moreover, since the eigenvalues of \(\mathbb{P}_{A}\) are either \(0\) or \(1\), we have that \(\mathbb{P}_{A}v_{i}=v_{i}\) for \(i\leq d_{A}\) and \(0\) otherwise. Similarly, \(\mathbb{P}_{B}v_{i}=v_{i}\) for \(i\leq d_{B}\) and \(0\) otherwise.

We will also use the following basic fact: for any square matrix \(J\) and any set of orthonormal basis vectors \(u_{1},\dots,u_{d}\), we have that \(\|J\|_{\mathrm{F}}^{2}=\sum_{i=1}^{d}\|Ju_{i}\|_{2}^{2}\). Applying this fact to \(\mathbb{P}_{A}-\mathbb{P}_{B}\) and the orthonormal basis \(v_{1},\dots,v_{d}\) above, we get the following:

\[\|\mathbb{P}_{A}-\mathbb{P}_{B}\|_{\mathrm{F}}^{2} =\sum_{i=1}^{d}\|\mathbb{P}_{A}v_{i}-\mathbb{P}_{B}v_{i}\|_{2}^{2} =\sum_{i=1}^{d_{B}}\|\mathbb{P}_{A}v_{i}-\mathbb{P}_{B}v_{i}\|_{2}^{2}+\sum_{d _{B}+1}^{d_{A}}\|\mathbb{P}_{A}v_{i}-\mathbb{P}_{B}v_{i}\|_{2}^{2}=\sum_{d_{B}+ 1}^{d_{A}}\|v_{i}\|_{2}^{2}\] \[=d_{A}-d_{B},\]

where the second equality uses that \(\mathbb{P}_{A}v_{i}=\mathbb{P}_{B}v_{i}=0\) for \(i\geq d_{A}\), and the third equality uses that \(\mathbb{P}_{A}v_{i}=\mathbb{P}_{B}v_{i}=v_{i}\) for \(i\leq d_{B}\) and \(\mathbb{P}_{B}v_{i}=0\) and \(\mathbb{P}_{A}v_{i}=v_{i}\) for \(i\in\{d_{B}+1,\dots,d_{A}\}\). 

By Claim C.5, we have that \(\|\mathbb{P}_{A}-\mathbb{P}_{B}\|_{\mathrm{F}}^{2}=|\operatorname{rank}(A)- \operatorname{rank}(B)|\).

## Appendix D Stability: Sample Complexity and Its Consequences

In this section, we prove results related to stability. Appendix D.1 proves the sample complexity for the stability condition to hold for Gaussians and Appendix D.2 focuses on some consequences of stability.

**Remark D.1**.: We note that the deterministic conditions of Definition 2.1 are not specific to the Gaussian distribution but hold with polynomial sample complexity \(\operatorname{poly}(d/\eta)\) for broader classes of distributions, roughly speaking, distributions with light tails for degree-4 polynomials.

* Condition (L.1) is satisfied with polynomial sample complexity by distributions that have bounded second moment.
* Condition (L.3) at the population level is a direct implication of hypercontractivity of degree-2 polynomials. The \(\log(1/\eta)\) factor in the condition as stated currently is tailored to the Gaussian distribution but it is not crucial for the algorithm and could be relaxed to some polynomial of \(1/\eta\); this would translate to incurring some \(\operatorname{poly}(1/\eta)\) to the final error guarantee of the main theorem. After establishing the condition at the population level, it can be transferred to sample level with polynomial sample complexity.
* Regarding Condition (L.2), every entry of the Frobenius norm is a degree 2 polynomial, thus the property holds for distributions with light tails for these polynomials, e.g., hypercontractive degree-2 distributions.
* Condition (L.4) can also be derived by hypercontractivity of degree 4 polynomials similarly.
* Condition (L.5) holds almost surely for continuous distributions.

### Sample Complexity of Stability Conditions

**Lemma 2.2** (Deterministic Conditions Hold with High Probability).: _For a sufficiently small positive constant \(\epsilon_{0}\) and a sufficiently large absolute constant \(C\), a set of \(m>Cd^{2}\log^{5}(d/(\eta\delta))/\eta^{2}\) samples from \(\mathcal{N}(\mu,\Sigma)\), with probability \(1-\delta\), is \((\eta,\epsilon)\)-stable set with respect to \(\mu,\Sigma\) for all \(\epsilon\leq\epsilon_{0}\)._

Proof.: Let \(A\) be a set of i.i.d. samples from \(\mathcal{N}(\mu,\Sigma)\). We check the four conditions from Definition 2.1 separately. Since \(A\) is a set of i.i.d. samples from a Gaussian, it satisfies Condition (L.1) with high probability (see, e.g., [11, Corollary 2.1.13]).

We start with Condition (L.2). Let \(X\sim\mathcal{N}(\mu,\Sigma)\), then \(X-\mu\sim\mathcal{N}(0,\Sigma)\). Equivalently, \(X-\mu=\Sigma^{1/2}Z\) with \(Z\sim\mathcal{N}(0,I)\). Let \(A\) be a set of \(m\) i.i.d. samples from \(\mathcal{N}(\mu,\Sigma)\) and like before, for every \(x\in A\), write \(x-\mu=\Sigma^{1/2}z\), where \(z\) are corresponding i.i.d. samples from \(\mathcal{N}(0,I)\). Let a subset \(A^{\prime}\subset A\) with \(|A^{\prime}|\geq|A|(1-\epsilon)\). Then, we have the following

\[\operatorname{tr}\left(\left(\frac{1}{n}\sum_{x\in A^{\prime}}(x-\mu)(x-\mu)^ {\top}-\Sigma\right)U\right)=\operatorname{tr}\left(\Sigma^{1/2}\left(\frac{1 }{n}\sum_{z}zz^{\top}-I\right)\Sigma^{1/2}U\right)\]\[=\operatorname{tr}\left(\left(\frac{1}{n}\sum_{z}zz^{\top}-I \right)\Sigma^{1/2}U\Sigma^{1/2}\right)\] \[\leq C\epsilon\log(1/\epsilon)\|\Sigma^{1/2}U\Sigma^{1/2}\|_{ \mathrm{F}}\,,\]

where the first line uses the rewriting that we introduced before, the second line uses the cyclic property of the trace operator, and the last line uses [11, Corollary 2.1.13]. As stated in that corollary, the inequality holds with probability \(1-\delta/4\) as long as the sample complexity is a sufficiently large multiple of \((d^{2}/\epsilon^{2})\log(4/\delta)\). Fixing \(\epsilon=\epsilon_{0}\) a sufficiently small positive absolute constant can make the right hand side of the above inequality \(0.1\|\Sigma^{1/2}U\Sigma^{1/2}\|_{\mathrm{F}}\).

Condition Condition (L.1) can be shown identically using [11, Corollary 2.1.9].

We now turn our attention to Condition (L.3). [1, Lemma 5.17] implies that with the stated number of samples, with high probability, it holds that

\[\operatorname*{\mathbf{Pr}}_{X\sim A}\left[|p(x)|>\sqrt{\operatorname*{ \mathbf{Var}}_{Y\sim\mathcal{N}(\mu,\Sigma)}[p(Y)]}10\ln\left(\frac{2}{\eta} \right)\right]\leq\frac{\eta}{2}.\]

Since \(|A^{\prime}|\geq|A|(1-\epsilon_{0})\geq|A|/2\), Condition (L.3) holds. Finally, Condition (L.4) follows by noting that \(\operatorname*{\mathbf{Var}}_{X\sim A^{\prime}}[p(X)]\leq 2\operatorname*{ \mathbf{Var}}_{X\sim A}[p(X)]\) and using [16, Lemma 5.17] again with \(\epsilon=\epsilon_{0}\). In order for the conclusion of that lemma to hold with probability \(1-\delta/4\), the number of samples of the Gaussian component is a sufficiently large multiple of \(d^{2}\log^{5}(4d/(\eta\delta))/\eta^{2}\). A union bound over the events corresponding to each of the four conditions concludes the proof.

Lastly, we show Condition (L.5). We will in fact show a stronger statement: suppose the rank of \(\Sigma\) is \(d_{\Sigma}\). Then, if we take \(m\geq d_{\Sigma}+1\) i.i.d. samples \(\{\mu+z_{1},\mu+z_{2},\ldots,\mu+z_{m}\}\) from \(\mathcal{N}(\mu,\Sigma)\), it must be the case that with probability 1, for all \(A^{\prime}\subseteq A\) with \(|A^{\prime}|\geq d_{\Sigma}+1\), we have \(\sum_{x\in A^{\prime}}(v^{\top}x)^{2}=0\) (or equivalently, \(v\) being orthogonal to all vectors \(x\in A^{\prime}\)) implying \(v^{\top}\Sigma v=0\) for all vectors \(v\in\mathbb{R}^{d}\).

We will need the following straightforward claim: if \(A^{\prime}=\{\mu+z^{\prime}_{0},\ldots,\mu+z^{\prime}_{d_{\Sigma}}\}\) (of size \(d_{\Sigma}+1\)) is such that 1) \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{d_{\Sigma}}-z^{\prime}_{0}\}\) are linearly independent and 2) \(z^{\prime}_{i}\) all lie within the column space of \(\Sigma\), then for any vector \(v\in\mathbb{R}^{d}\), \(v\) being orthogonal to all of \(A^{\prime}\) implies that \(v^{\top}\Sigma v=0\). To see why this is true, observe that \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{d_{\Sigma}}-z^{\prime}_{0}\}\), which are linearly independent vectors that all lie in the column space of \(\Sigma\), must span the entirety of the column space of \(\Sigma\) by dimension counting. Thus, if \(v\) is orthogonal to all the points in \(A^{\prime}\), then \(v\) is orthogonal to all of \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{d_{\Sigma}}-z^{\prime}_{0}\}\), which implies \(v^{\top}\Sigma v=0\).

We can now show the following inductive claim on the size of \(A\), which combined with the above claim implies the strengthened version of Condition (L.5). Specifically, we show inductively that, with probability 1 over the sampling of \(A\), for any subset \(A^{\prime}\subset A\) of size at most \(d_{\Sigma}+1\), the claim conditions hold for \(A^{\prime}\). Namely, if \(A^{\prime}=\{\mu+z^{\prime}_{0},\ldots,\mu+z^{\prime}_{j}\}\) for some \(j\leq d_{\Sigma}\), for some arbitrary ordering of the elements in \(A^{\prime}\), then 1) the set \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{j}-z^{\prime}_{0}\}\) is linearly independent and 2) \(z^{\prime}_{i}\) all lie in the column space of \(\Sigma\).

The base case of \(|A|=1\) is trivial. Now suppose the above statement is true for \(|A_{\ell}|=\ell\), and consider sampling a new point \(\mu+z_{\ell}\) and adding it to \(A_{\ell}\) to form \(A_{\ell+1}\). Take any subset \(A^{\prime}\) of \(A_{\ell+1}\) of size at most \(d_{\Sigma}+1\). If \(A^{\prime}\subset A_{\ell}\) we are already done by the induction hypothesis. Otherwise, \(A^{\prime}=\{\mu+z_{\ell}\}\cup A^{\prime}_{\ell}\), where \(A^{\prime}_{\ell}=\{\mu+z^{\prime}_{0},\ldots,\mu+z^{\prime}_{j}\}\) has size at most \(d_{\Sigma}\) and satisfies the conditions in the induction by the inductive hypothesis. The space of \(\mu+z\) such that \(z-z^{\prime}_{0}\) lies in the span of \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{j}-z^{\prime}_{0}\}\) has measure 0 under \(\mathcal{N}(\mu,\Sigma)\), given there are strictly fewer than \(d_{\Sigma}\) many linearly independent vectors in \(\{z^{\prime}_{1}-z^{\prime}_{0},\ldots,z^{\prime}_{j}-z^{\prime}_{0}\}\), all of which lie in the column space of \(\Sigma\). Furthermore, there is only a finite number of such \(A^{\prime}_{\ell}\). Thus, with probability 1, the new point \(\mu+z_{\ell}\) will be sampled such that for all such \(A^{\prime}_{\ell}\), \(\{\mu+z_{\ell}\}\cup A^{\prime}_{\ell}\) remains linearly independent. Additionally, also with probability 1, \(z_{\ell}\) will lie in the column space of \(\Sigma\). This completes the proof of the inductive claim, which as we have shown implies Condition (L.5).

### Facts Regarding Stability

The following corollaries will be useful later on. In particular, since our algorithm will perform transformations of the dataset, we will need CorollaryD.2 to argue about the stability properties of these sets. Moreover, the next lemma (LemmaD.3) will be used to extract the final Frobenius norm guarantee of our theorem given the first bullet of that theorem. The proofs are deferred to AppendixD.4.

**Corollary D.2** (Stability of Transformed Sets).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\) with mean \(\mu\) and covariance \(\Sigma\). Let \(A\) be a set of points in \(\mathbb{R}^{d}\) that satisfies Conditions(L1) and (L2) of Definition2.1 of \((\eta,\epsilon)\)-stability with respect to \(D\). Then we have that for any symmetric transformation \(P\in\mathbb{R}^{d\times d}\) and every \(A^{\prime}\subset A\) with \(|A^{\prime}|\geq(1-\epsilon)|A|\), the following holds:_

1. \(\left\|P\left(\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}x-\mu\right)\right\| _{2}\leq 0.1\sqrt{\|P\Sigma P\|_{\mathsf{op}}}\)_._
2. \(\left\|P\left(\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}(x-\mu)(x-\mu)^{\top }-\Sigma\right)P\right\|_{\mathrm{F}}\leq 0.1\|P\Sigma P\|_{\mathsf{op}}\)_._

The following lemma (proved in AppendixD.4) will be needed to prove our certificate lemma to transfer the final guarantees of our algorithm from the empirical covariance of the sample set to the actual covariance of the Gaussians.

**Lemma D.3**.: _Let \(A\) be a set of points in \(\mathbb{R}^{d}\) satisfying Conditions(L1) and (L2) of \((\eta,\epsilon)\)-stability (Definition2.1) with respect to \(\mu,\Sigma\). Let \(A^{\prime}\subseteq A\) such that \(|A^{\prime}|\geq(1-\epsilon)|A|\). If \(P,L\in\mathbb{R}^{d\times d}\) are symmetric matrices with the property that \(\|\operatorname{\mathbf{Cov}}_{X\sim A^{\prime}}[PX]-L\|_{\mathrm{F}}\leq r\), then_

\[\left\|P\Sigma P-L\right\|_{\mathrm{F}}=O(r+\|L\|_{\mathsf{op}})\.\]

### Certificate Lemma

The following result shows that if the covariance matrix of \(X^{\otimes 2}\) over the corrupted set of points is bounded in operator norm, then the empirical second moment matrix of \(T\) is a good estimate of covariance of inliers in Frobenius norm. We prove a more general version below that allows for arbitrary linear transformation matrix \(P\) and since we are interested in guarantees for relative Frobenius norm, our algorithm will chose \(P\) so that \(\operatorname{\mathbf{E}}_{X\sim T}[PXX^{\top}P]\) is equal to identity.

**Lemma D.4** (Bounded fourth-moment and second moment imply closeness of covariance).: _Let \(T\) be a set of \(n\) points in \(\mathbb{R}^{d}\). Let \(P\) be an arbitrary symmetric matrix. Let \(w\in\mathbb{R}^{d^{2}}\) be the leading eigenvector of the \(\operatorname{\mathbf{Cov}}_{X\sim T}[(PX)^{\otimes 2}]\) with \(\|w\|_{2}=1\), and let \(A\in\mathbb{R}^{d\times d}\) be \(w^{\sharp}\). Note that \(\|w\|_{2}=1\) implies \(\|A\|_{\mathrm{F}}=1\). For every subset \(T^{\prime}\subseteq T\) with \(|T^{\prime}|\geq(\alpha/2)|T|\), we have that_

\[\left\|\operatorname{\mathbf{Cov}}_{X\sim T}[PXX^{\top}P]- \operatorname{\mathbf{E}}_{x\sim T}[PXX^{\top}P]\right\|_{\mathrm{F}}^{2} \leq\frac{4}{\alpha}\operatorname{\mathbf{Var}}_{X\sim T}[X^{\top }PAPX]\quad\text{and}\] \[\left\|\operatorname{\mathbf{Cov}}_{X\sim T}[PXX]- \operatorname{\mathbf{E}}_{x\sim T}[PXX^{\top}P]\right\|_{\mathrm{F}}^{2} \leq\frac{4}{\alpha}\operatorname{\mathbf{Var}}_{X\sim T}[X^{\top }PAPX]+\frac{8}{\alpha^{2}}\left\|\operatorname{\mathbf{E}}_{X\sim T}[PXX^{ \top}P]\right\|_{\mathsf{op}}^{2}\.\]

_In particular, suppose that there exists a set \(S\) that satisfies Conditions(L1) and (L2) of (\(\eta,\epsilon\))-stability (Definition2.1) with respect to \(\mu\) and \(\Sigma\) for some \(\eta\) and \(\epsilon\), and suppose that \(|S\cap T^{\prime}|\geq\min(\alpha|T|/2,(1-\epsilon)|S|)\). Then_

\[\left\|P\Sigma P-\operatorname{\mathbf{E}}_{X\sim T^{\prime}}[PXX^{\top}P] \right\|_{\mathrm{F}}\lesssim\frac{1}{\alpha}\operatorname{\mathbf{Var}}_{X \sim T}[X^{\top}PAPX]+\frac{1}{\alpha^{2}}\left\|\operatorname{\mathbf{E}}_{X \sim T}[PXX^{\top}P]\right\|_{\mathsf{op}}^{2}\.\]

Proof.: Let \(\widetilde{T}\) define the set \(\{PX:X\in T\}\). In this notation, \(w\) is the leading eigenvector of \(\operatorname{\mathbf{Cov}}_{\widetilde{T}\sim\widetilde{T}}[\widetilde{X}^{ \otimes 2}]\) with \(\|w\|_{2}=1\), and \(A=w^{\sharp}\). Then, for any \(\widetilde{T}^{\prime}\subseteq\widetilde{T}\) with \(|\widetilde{T}^{\prime}|\geq\alpha|\widetilde{T}^{\prime}|/2\), we have the following

\[\operatorname{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{T}}[ \widetilde{X}^{\top}A\widetilde{X}] =\sup_{v\in\mathbb{R}^{d^{2}},\|v\|_{2}^{2}=1}\operatorname{ \mathbf{E}}_{\widetilde{X}\sim\widetilde{T}}\left[\left\langle\widetilde{X}^ {\otimes 2}-\operatorname{\mathbf{E}}_{\widetilde{Y}\sim\widetilde{T}}[\widetilde{Y}^{ \otimes 2}],v\right\rangle^{2}\right]\] \[=\sup_{v\in\mathbb{R}^{d^{2}},\|v\|_{2}^{2}=1}\frac{|\widetilde{T} ^{\prime}|}{|\widetilde{T}|}\operatorname{\mathbf{E}}_{\widetilde{X}\sim \widetilde{T}^{\prime}}\left[\left\langle\widetilde{X}^{\otimes 2}-\operatorname{\mathbf{E}}_{ \widetilde{Y}\sim\widetilde{T}}[\widetilde{Y}^{\otimes 2}],v\right\rangle^{2}\right]\]\[\geq\frac{\alpha}{2}\sup_{v\in\mathbb{R}^{d^{2}},\|v\|_{2}^{2}=1} \left\langle\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[ \widetilde{X}\widetilde{X}^{\top}]-\underset{\widetilde{Y}\sim\widetilde{T}}{ \mathbf{E}}[\widetilde{Y}\widetilde{Y}^{\top}],J\right\rangle^{2}\] \[=\frac{\alpha}{2}\left\|\underset{\widetilde{X}\sim\widetilde{T}^ {\prime}}{\mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]-\underset{ \widetilde{X}\sim\widetilde{T}}{\mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}] \right\|_{\text{F}}^{2}\,,\]

where the penultimate step above holds by lifting the flattened matrix inner products in the previous line to the matrix inner product, and the last line uses the variation characterization of the Frobenius norm. This implies the first inequality since \(\mathbf{E}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}\widetilde{X}^{\top }]=\mathbf{E}_{X\sim T}[PXX^{\top}P]\).

We will now establish the second inequality. Using the relation between the second moment matrix and the covariance, we have the following:

\[\left\|\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{ \mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]-\underset{\widetilde{X}\sim \widetilde{T}}{\mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]\right\|_{\text{F }}^{2} =\left\|\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[ \widetilde{X}\widetilde{X}^{\top}]-\underset{\widetilde{X}\sim\widetilde{T}^ {\prime}}{\mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]\right\|_{\text{F}}^{2}\] \[\leq 2\left\|\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{ \mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]-\underset{\widetilde{X}\sim \widetilde{T}}{\mathbf{E}}[\widetilde{X}\widetilde{X}^{\top}]\right\|_{\text{F }}^{2}+2\left\|\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[ \widetilde{X}]\,\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}} [\widetilde{X}^{\top}]\right\|_{\text{F}}^{2}\] \[\leq\frac{4}{\alpha}\underset{\widetilde{X}\sim\widetilde{T}^{ \prime}}{\mathbf{Var}}[\widetilde{X}^{\top}A\widetilde{X}]+2\left\|\underset{ \widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[\widetilde{X}]\, \underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[\widetilde{X} ^{\top}]\right\|_{\text{op}}^{2}\,,\]

where the last step uses that \(\mathbf{E}_{\widetilde{X}\sim\widetilde{T}^{\prime}}[\widetilde{X}]\,\mathbf{E }_{\widetilde{X}\sim\widetilde{T}^{\prime}}[\widetilde{X}^{\top}]\) has rank one and thus its Frobenius and operator norm match. Thus to establish the desired inequality, it suffices to prove that \(\mathbf{E}_{\widetilde{X}\sim\widetilde{T}^{\prime}}[\widetilde{X}]\,\mathbf{E }_{\widetilde{X}\sim\widetilde{T}^{\prime}}[\widetilde{X}^{\top}]\preceq(2/ \alpha)\cdot\mathbf{E}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}\widetilde {X}^{\top}]\). Indeed, we have that

\[\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}}[ \widetilde{X}]\,\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E}} [\widetilde{X}^{\top}] \preceq\underset{\widetilde{X}\sim\widetilde{T}^{\prime}}{\mathbf{E }}[\widetilde{X}\widetilde{X}^{\top}]=\frac{1}{|\widetilde{T}^{\prime}|}\, \sum_{\widetilde{X}\in\widetilde{T}^{\prime}}{\mathbf{X}\widetilde{X}^{\top }}=\frac{|\widetilde{T}|}{|\widetilde{T}^{\prime}|}\frac{1}{|\widetilde{T}|} \sum_{\widetilde{X}\in\widetilde{T}^{\prime}}\widetilde{X}\widetilde{X}^{\top}\] \[\preceq\frac{2}{\alpha}\frac{1}{|\widetilde{T}|}\sum_{\widetilde{X }\in\widetilde{T}^{\prime}}{\mathbf{X}\widetilde{X}^{\top}}\preceq\frac{2}{ \alpha}\frac{1}{|\widetilde{T}|}\sum_{\widetilde{X}\in\widetilde{T}}\widetilde{ X}\widetilde{X}^{\top},\]

where the last expression is in fact equal to \((2/\alpha)\cdot\mathbf{E}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X} \widetilde{X}^{\top}]\). This completes the proof for the second statement.

We now consider the case when \(T\) contains a large stable subset. Let \(T^{\prime}=T\cap S\), which satisfies that \(|T^{\prime}|\geq(\alpha/2)|T|\) and \(|T^{\prime}|\geq(1-\epsilon)|S|\). By applying Lemma D.3 with \(L=\mathbf{E}_{X\sim T}[PXX^{\top}P]\) to the second inequality in the statement with \(T^{\prime}\), we obtain the desired result.

### Proofs of Corollary D.2 and lemma D.3

**Corollary D.2** (Stability of Transformed Sets).: _Let \(D\) be a distribution over \(\mathbb{R}^{d}\) with mean \(\mu\) and covariance \(\Sigma\). Let \(A\) be a set of points in \(\mathbb{R}^{d}\) that satisfies Conditions (L.1) and (L.2) of Definition 2.1 of \((\eta,\epsilon)\)-stability with respect to \(D\). Then we have that for any symmetric transformation \(P\in\mathbb{R}^{d\times d}\) and every \(A^{\prime}\subset A\) with \(|A^{\prime}|\geq(1-\epsilon)|A|\), the following holds:_

1. \(\left\|P\left(\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}x-\mu\right)\right\|_ {2}\leq 0.1\sqrt{\|P\Sigma P\|_{\text{op}}}\)_._
2. \(\left\|P\left(\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}(x-\mu)(x-\mu)^{\top} -\Sigma\right)P\right\|_{\text{F}}\leq 0.1\|P\Sigma P\|_{\text{op}}\)_._Proof.: We explain each condition separately. We begin with the first condition as follows: For any unit vector \(u\) in \(\mathbb{R}^{d}\), we have that

\[\left|u^{\top}P\left(\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}x-\mu\right) \right|\leq 0.1\sqrt{u^{\top}P\Sigma Pu}\leq\sqrt{\|P\Sigma P\|_{\text{op}}}\,\]

where the first inequality uses Condition (L.1) of Definition 2.1 for \(v=Pu\).

We now focus our attention to the second condition. Denote \(\hat{\Sigma}:=\frac{1}{|A^{\prime}|}\sum_{x\in A^{\prime}}(x-\mu)(x-\mu)^{\top}\) for saving space. Let \(Q\) be any symmetric matrix \(Q\in\mathbb{R}^{d\times d}\) with \(\|Q\|_{\mathrm{F}}\leq 1\). Using the cyclical property of trace, we obtain

\[\mathrm{tr}\left(P\left(\hat{\Sigma}-\Sigma\right)PQ\right)=\mathrm{tr}\left( \left(\hat{\Sigma}-\Sigma\right)PQP\right)\leq 0.1\left\|\Sigma^{1/2}PQP\Sigma^{1/2} \right\|_{\mathrm{F}}\,\]

where the last step applies Condition (L.2) of Definition 2.1 with \(V=PQP\). Finally, using the cyclic properties of the trace operator,

\[\left\|\Sigma^{1/2}PQP\Sigma^{1/2}\right\|_{\mathrm{F}}^{2} =\mathrm{tr}\left(\Sigma^{1/2}PQP\Sigma PQP\Sigma^{1/2}\right)= \mathrm{tr}(QP\Sigma PQP\Sigma P)\] \[=\|QP\Sigma P\|_{\mathrm{F}}^{2}\leq\|Q\|_{\mathrm{F}}^{2}\|P \Sigma P\|_{\text{op}}^{2}\leq\|P\Sigma P\|_{\text{op}}^{2},\]

where the first inequality uses Fact B.3 and the second inequality uses \(\|Q\|_{\mathrm{F}}\leq 1\). Thus we have the following:

\[\left\|P\left(\hat{\Sigma}-\Sigma\right)P\right\|_{\mathrm{F}} =\sup_{Q:\|Q\|_{\mathrm{F}}\leq 1}\mathrm{tr}\left(P\left(\hat{ \Sigma}-\Sigma\right)PQ\right)\] \[=\sup_{\text{symmetric }Q:\|Q\|_{\mathrm{F}}\leq 1}\mathrm{tr} \left(P\left(\hat{\Sigma}-\Sigma\right)PQ\right)\] \[\leq 0.1\|P\Sigma P\|_{\text{op}}.\]

where the second equality is due to the fact that \(P\left(\hat{\Sigma}-\Sigma\right)P\) is itself symmetric.

**Lemma D.3**.: _Let \(A\) be a set of points in \(\mathbb{R}^{d}\) satisfying Conditions (L.1) and (L.2) of \((\eta,\epsilon)\)-stability (Definition 2.1) with respect to \(\mu,\Sigma\). Let \(A^{\prime}\subseteq A\) such that \(|A^{\prime}|\geq(1-\epsilon)|A|\). If \(P,L\in\mathbb{R}^{d\times d}\) are symmetric matrices with the property that \(\|\mathbf{Cov}_{X\sim A^{\prime}}[PX]-L\|_{\mathrm{F}}\leq r\), then_

\[\|P\Sigma P-L\|_{\mathrm{F}}=O(r+\|L\|_{\text{op}})\.\]

Proof.: To simplify notation, let \(\widehat{\Sigma}:=\mathbf{Cov}_{X\sim A^{\prime}}[X]\) and \(\overline{\Sigma}:=\mathbf{E}_{X\sim A^{\prime}}[(X-\mu)(X-\mu)^{\top}]\). Observe that \(\overline{\Sigma}=\widehat{\Sigma}+(\mu-\hat{\mu})(\mu-\hat{\mu})^{\top}\). We apply triangle inequality and Corollary D.2 with to obtain the following:

\[\|P(\widehat{\Sigma}-\Sigma)P\|_{\mathrm{F}} \leq\|P(\overline{\Sigma}-\Sigma)P\|_{\mathrm{F}}+\|P(\mu-\hat{ \mu})(\mu-\hat{\mu})^{\top}P\|_{\mathrm{F}}\] \[=\|P(\overline{\Sigma}-\Sigma)P\|_{\mathrm{F}}+\|P(\mu-\hat{\mu} )\|_{2}^{2}\] \[\leq 0.1\|P\Sigma P\|_{\text{op}}+0.1\|P\Sigma P\|_{\text{op}}\] \[\leq 0.2\|P\Sigma P\|_{\text{op}}\,\] (6)

where the first inequality follows from the triangle inequality and the second inequality uses Corollary D.2.

We now analyze the desired expression by using the triangle inequality and the lemma assumption as follows:

\[\|P\Sigma P-L\|_{\mathrm{F}}\leq\|P\widehat{\Sigma}P-L\|_{\mathrm{F}}+\|P( \widehat{\Sigma}-\Sigma)P\|_{\mathrm{F}}\leq r+0.2\|P\Sigma P\|_{\text{op}},\]

where the first term was bounded by \(r\) by assumption and the second term uses the bound (6). Thus it suffices to show that \(\|P\Sigma P\|_{\text{op}}=O(r+\|L\|_{\text{op}})\). To this end, we again use triangle inequality as follows:

\[\|P\Sigma P\|_{\text{op}} \leq\|P\widehat{\Sigma}P-L\|_{\text{op}}+\|L\|_{\text{op}}+\|P( \widehat{\Sigma}-\Sigma)P\|_{\text{op}}\] \[\leq r+\|L\|_{\text{op}}+0.2\|P\Sigma P\|_{\text{op}},\] (7)

where the second inequality uses the bound (6). Rearranging the above display inequality, we obtain \(\|P\Sigma P\|_{\text{op}}=O(r+\|L\|_{\text{op}})\)

[MISSING_PAGE_EMPTY:23]

By applying (8), we obtain

\[\hat{\Sigma}=\frac{1}{\left|\widetilde{S}\cap\widetilde{T}\right|}\sum_{ \widetilde{x}\in\widetilde{S}\cap\widetilde{T}}(\widetilde{x}-\hat{\mu})( \widetilde{x}-\hat{\mu})^{\top}\preceq\frac{1}{\left|\widetilde{S}\cap \widetilde{T}\right|}\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}} \widetilde{x}\widetilde{x}^{\top}\preceq\frac{2}{\alpha}\mathbb{P}_{H},\]

implying that \(\|\hat{\Sigma}\|_{\text{op}}\leq 2/\alpha\), since \(\mathbb{P}_{H}\) is just a projection matrix with \(0/1\) eigenvalues.

Establishing Item 2Since, for any random vector \(X\), we have that \(\mathbf{E}[X]\,\mathbf{E}[X]^{\top}\preceq\mathbf{E}[XX^{\top}]\) (which is equivalent to \(\mathbf{Cov}[X]\succeq 0\)), we obtain

\[\hat{\mu}\hat{\mu}^{\top}\preceq\frac{1}{\left|\widetilde{S}\cap\widetilde{T} \right|}\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}}\widetilde{x} \widetilde{x}^{\top}\preceq\frac{2}{\alpha}\mathbb{P}_{H},\]

where the last inequality uses (8). This implies that \(\|\hat{\mu}\|_{2}^{2}\leq 2/\alpha\).

Establishing Item 3The goal is to bound the population-level covariance \(\|\widetilde{\Sigma}\|_{\text{op}}\). To this end, we will use the bounds from Items 1 and 2 which bounds their empirical versions and relate the empirical versions to the population ones via the deterministic conditions.

Consider an arbitrary subset \(\widetilde{S}_{1}\) of \(\widetilde{S}\) satisfying \(\left|\widetilde{S}_{1}\right|\geq\left|\widetilde{S}\right|(1-2\epsilon_{0})\). We first note that by Corollary D.2, we have that

\[\left\|\widetilde{\Sigma}-\underset{\widetilde{X}\sim S_{1}}{\mathbf{E}}[( \widetilde{X}-\widetilde{\mu})(\widetilde{X}-\widetilde{\mu})^{\top}]\right\| _{\text{F}}\leq 0.1\left\|\widetilde{\Sigma}\right\|_{\text{op}}\quad\text{ and}\quad\left\|\widetilde{\mu}-\underset{\widetilde{X}\sim\widetilde{S}_{1}}{ \mathbf{E}}[\widetilde{X}]\right\|_{2}\leq 0.1\sqrt{\left\|\widetilde{\Sigma}\right\|_{ \text{op}}}.\] (9)

Now define \(\widetilde{\Sigma}_{1}:=\mathbf{E}_{\widetilde{X}\sim\widetilde{S}\cap \widetilde{T}}[(\widetilde{X}-\widetilde{\mu})(\widetilde{X}-\widetilde{\mu} )^{\top}]\), the centered second moment matrix of \(\widetilde{S}\cap\widetilde{T}\), which satisfies \(\widetilde{\Sigma}_{1}=\hat{\Sigma}+(\widetilde{\mu}-\hat{\mu})(\widetilde{ \mu}-\hat{\mu})^{\top}\). We have that

\[\|\widetilde{\Sigma}\|_{\text{op}} \leq\|\widetilde{\Sigma}-\hat{\Sigma}\|_{\text{F}}+\|\hat{\Sigma }\|_{\text{op}}\] (triangle inequality) \[\leq\|\widetilde{\Sigma}-\widetilde{\Sigma}_{1}\|_{\text{F}}+\| \widetilde{\mu}-\hat{\mu}\|_{2}^{2}+\|\hat{\Sigma}\|_{\text{op}}\] (triangle inequality and \[\widetilde{\Sigma}_{1}=\hat{\Sigma}+(\mu-\hat{\mu})(\mu-\hat{\mu})^{\top}\] ) \[\leq 0.2\|\widetilde{\Sigma}\|_{\text{op}}+\|\hat{\Sigma}\|_{ \text{op}},\]

where in the last line we use (9) for \(\widetilde{S}_{1}=\widetilde{S}\cap\widetilde{T}\) and the fact that \(\left|\widetilde{S}\cap\widetilde{T}\right|\geq(1-2\epsilon_{0})|\widetilde{S }|\). Rearranging, we have that \(\|\widetilde{\Sigma}\|_{\text{op}}<1.25\|\hat{\Sigma}\|_{\text{op}}\) for which we can use Item 1 to further upper bound it by \(3/\alpha\).

Establishing Item 4We use a similar argument as in Item 3:

\[\|\widetilde{\mu}\|_{2}\leq\|\hat{\mu}\|_{2}+\|\widetilde{\mu}-\hat{\mu}\|_{2 }\leq\|\hat{\mu}\|_{2}+0.1\sqrt{\|\widetilde{\Sigma}\|_{\text{op}}}\leq\sqrt{ 3/\alpha},\] (10)

where the first step uses the triangle inequality, the second inequality uses Equation (9) for \(\widetilde{S}_{1}=\widetilde{S}\cap\widetilde{T}\), and the last inequality uses Items 2 and 3.

Establishing Item 5For the covariance condition, we have the following:

\[\left\|\underset{\widetilde{X}\sim\widetilde{S}}{\mathbf{Cov}[ \widetilde{X}]}\right\|_{\text{op}} \leq\left\|\underset{\widetilde{X}\sim\widetilde{S}}{\mathbf{E}} \left[(\widetilde{X}-\widetilde{\mu})(\widetilde{X}-\widetilde{\mu})^{\top} \right]\right\|_{\text{op}}\] (using \[\mathbf{Cov}(\widetilde{X})\preceq\mathbf{E}[\widetilde{X}\widetilde{X} \widetilde{X}^{\top}]\] ) \[\leq\left\|\underset{\widetilde{X}\sim\widetilde{S}}{\mathbf{E}} \left[(\widetilde{X}-\widetilde{\mu})(\widetilde{X}-\widetilde{\mu})^{\top} \right]-\widetilde{\Sigma}\right\|_{\text{op}}+\|\widetilde{\Sigma}\|_{\text{op}}\] (triangle inequality) \[\leq 1.1\|\widetilde{\Sigma}\|_{\text{op}},\]

where the last step upper bounds the first term using Equation (9) for \(\widetilde{S}_{1}=\widetilde{S}\), which trivially satisfies \(|\widetilde{S}|\geq|\widetilde{S}|(1-2\epsilon_{0})\). The overall expression is upper bounded by \((1.1\times 3)/\alpha\) by Item 3.

The mean condition has a similar proof:

\[\left\|\underset{\widetilde{X}\sim\widetilde{S}}{\mathbf{E}}[\widetilde{X}] \right\|_{2}\leq\left\|\underset{\widetilde{X}\sim\widetilde{S}}{\mathbf{E}}[ \widetilde{X}]-\widetilde{\mu}\right\|_{2}+\|\widetilde{\mu}\|_{2}\leq 0.1\sqrt{\|\widetilde{\Sigma}\|_{\text{op}}}+\sqrt{3/ \alpha}\leq 2/\sqrt{\alpha},\]

where we use Equations (9) and (10).

The previous result implies that the mean and covariance of the inlier distribution \(D\) after transformation with \(H^{\dagger/2}\) is bounded. Since the variance of degree-two polynomials under \(D\) is bounded by assumption, we obtain the following bound on the variance:

**Lemma 3.2**.: _Make Assumption 3.1 and recall that \(H=\mathbf{E}_{X\sim T}[XX^{\top}]\), where \(T\) is the corrupted version of a stable inlier set \(S\). For every symmetric matrix \(A\) with \(\|A\|_{\mathrm{F}}=1\), we have that \(\mathbf{Var}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)]\leq 18C_{1}/ \alpha^{2}\)._

Proof.: By Assumption 3.1, we have that \(\mathbf{Var}_{X\sim D}[X^{\top}BX]\leq C_{1}(\|\Sigma^{1/2}B\Sigma^{1/2}\|_{F}^ {2}+\|\Sigma^{1/2}B\mu\|_{2}^{2})\) for all \(B\in\mathbb{R}^{d\times d}\). Applying this to \(B=H^{\dagger/2}AH^{\dagger/2}\), we have that

\[\mathop{\mathbf{Var}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{ \dagger/2}X)] \leq C_{1}(\|\Sigma^{1/2}H^{\dagger/2}AH^{\dagger/2}\Sigma^{1/2} \|_{F}^{2}+\|\Sigma^{1/2}H^{\dagger/2}AH^{\dagger/2}\mu\|_{2}^{2})\] \[=C_{1}(\|\tilde{\Sigma}^{1/2}A\tilde{\Sigma}^{1/2}\|_{F}^{2}+\| \tilde{\Sigma}^{1/2}A\tilde{\mu}\|_{2}^{2})\,\] (11)

where we use that \(\|\tilde{\Sigma}^{1/2}A\tilde{\Sigma}^{1/2}\|_{F}^{2}=\|\Sigma^{1/2}H^{\dagger /2}AH^{\dagger/2}\Sigma^{1/2}\|_{F}^{2}\) and \(\|\Sigma^{1/2}H^{\dagger/2}AH^{\dagger/2}\mu\|_{2}^{2}=\|\tilde{\Sigma}^{1/2} A\tilde{\mu}\|_{2}^{2}\) since \(\tilde{\Sigma}=H^{\dagger/2}\Sigma H^{\dagger/2}\) and \(\tilde{\mu}=H^{\dagger/2}\mu\). By Lemma E.1, \(\|\tilde{\Sigma}\|_{\mathrm{op}}\leq 3/\alpha\). Since \(\|A\|_{\mathrm{F}}=1\), we have that the first term in (11) is bounded as follows: \(\|\tilde{\Sigma}^{1/2}A\tilde{\Sigma}^{1/2}\|_{\mathrm{F}}^{2}\leq\|\tilde{ \Sigma}\|_{\mathrm{op}}^{2}\|A\|_{\mathrm{F}}^{2}\leq 9/\alpha^{2}\), using the fact that \(\|AB\|_{\mathrm{F}}\leq\|A\|_{\mathrm{op}}\|B\|_{\mathrm{F}}\) (Fact B.3). We also have that \(\|\tilde{\Sigma}^{1/2}A\tilde{\mu}\|_{2}^{2}\leq\|\tilde{\Sigma}\|_{\mathrm{ op}}\|A\|_{\mathrm{op}}^{2}\|\tilde{\mu}\|_{2}^{2}\leq 9/\alpha^{2}\), where we the bounds from Lemma E.1. The desired conclusion then follows by applying these upper bounds in (11).

### Certificate Lemma: Proof of Lemma 3.4

**Lemma E.2** (Case when we stop and return; Formal version of Lemma 3.4).: _Make Assumption 3.1. Let \(w\in\mathbb{R}^{d^{2}}\) be the leading eigenvector of the \(\mathbf{Cov}_{\widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{\otimes 2}]\) with \(\|w\|_{2}=1\), and let \(A\in\mathbb{R}^{d\times d}\) be \(w^{\sharp}\). Note that \(\|w\|_{2}=1\) implies \(\|A\|_{\mathrm{F}}=1\). For every subset \(\widetilde{T}^{\prime}\subseteq\widetilde{T}\) with \(|\widetilde{T}^{\prime}|\geq(\alpha/2)|\widetilde{T}|\), we have that_

\[\left\|\operatorname*{\mathbf{Cov}}_{\widetilde{X}\sim\widetilde{T}^{\prime} }[\widetilde{X}]-\mathbb{P}_{H}\right\|_{\mathrm{F}}^{2}\lesssim\frac{1}{ \alpha}\operatorname*{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{T}}[ \widetilde{X}^{\top}A\widetilde{X}]+\frac{1}{\alpha^{2}}\.\]

_In particular, we have that \(\left\|H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{H}\right\|_{\mathrm{F}}^ {2}\lesssim\frac{1}{\alpha}\operatorname*{\mathbf{Var}}_{\widetilde{X}\sim \widetilde{T}}[\widetilde{X}^{\top}A\widetilde{X}]+\frac{1}{\alpha^{2}}\) and \(\left\|H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{\Sigma}\right\|_{ \mathrm{F}}^{2}\lesssim\frac{1}{\alpha}\operatorname*{\mathbf{Var}}_{ \widetilde{X}\sim\widetilde{T}}[\widetilde{X}^{\top}A\widetilde{X}]+\frac{1}{ \alpha^{2}}\)._

Proof.: We will apply the main certificate lemma, Lemma D.4, with \(P=H^{\dagger/2}\). The desired result then follows by noting that \(\operatorname*{\mathbf{E}}_{X\sim T}[PXX^{\top}P]=H^{\dagger/2}HH^{\dagger/2} =\mathbb{P}_{H}\) and the operator norm of \(\mathbb{P}_{H}\) is at most \(1\). We will use the following result, proved in Appendix C, to upper bound the frobenius norm of \(H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{\Sigma}\):

**Lemma C.3**.: _Let \(A\) and \(B\) be two PSD matrices with \(\mathbb{P}_{B}\preceq\mathbb{P}_{A}\) or equivalently \(\ker(A)\subseteq\ker(B)\). Then \(\|ABA-\mathbb{P}_{B}\|_{\mathrm{F}}\leq 2\|ABA-\mathbb{P}_{A}\|_{\mathrm{F}}\)._

Before providing the proof sketch of Lemma C.3 below, we show that it suffices to prove the desired result. Since \(H\) is the second moment matrix of \(T\), we have that \(\ker(H)\subseteq\ker(\sum_{x\in S\cap T}xx^{\top})\), which is further contained in \(\ker(\Sigma)\) by Condition (L.5) in Definition 2.1. We apply Lemma C.3 with \(B=\Sigma\) and \(A=H\), which gives the desired result.

We now give a brief proof sketch of Lemma C.3. By triangle inequality, it suffices to show that \(\|\mathbb{P}_{A}-\mathbb{P}_{B}\|_{\mathrm{F}}\leq\|ABA-\mathbb{P}_{A}\|_{ \mathrm{F}}\). Since \(\ker(A)\subset\ker(B),\mathbb{P}_{A}-\mathbb{P}_{B}\) is again a projection matrix of rank equal to \(\operatorname*{rank}(A)-\operatorname*{rank}(B)\) and thus its frobenius norm is equal to square root of \(\operatorname*{rank}(A)-\operatorname*{rank}(B)\). On the other hand, the matrices \(ABA\) and \(\mathbb{P}_{A}\) have a rank difference of at least \(\operatorname*{rank}(A)-\operatorname*{rank}(B)\). Combining this observation with the fact that \(\mathbb{P}_{A}\) has binary eigenvalues, we can lower bound \(\|ABA-\mathbb{P}_{A}\|_{\mathrm{F}}\) by square root of \(\operatorname*{rank}(A)-\operatorname*{rank}(B)\).

### Filtering: Proof of Lemma 3.5

We first show that if we take the median of a quadratic polynomial \(p\) over the corrupted set, then the sample mean of \(p\) over the inliers is not too far from the median if the left and right quantiles are close.

**Lemma E.3** (Quantiles of quadratics after normalization).: _Make Assumption 3.1. Let \(A\) be an arbitrary symmetric matrix with \(\|A\|_{\mathrm{F}}=1\). Define \(\widetilde{y}_{\mathrm{median}}=\mathrm{Median}\left(\left\{\widetilde{x}^{ \top}A\widetilde{x}\mid\widetilde{x}\in\widetilde{T}\right\}\right)\). Let \(m_{1}\) be any number less than \(|S|/3\). Denote by \(q_{i}\) the \(i\)-th smallest point of \(\left\{\widetilde{x}^{\top}A\widetilde{x}\mid\widetilde{x}\in\widetilde{T}\right\}\)._

_Suppose that \(q_{|T|-m_{1}}-q_{m_{1}}\leq R\) for \(R:=C(1/\alpha)\log(1/\eta)\) with \(C>100\sqrt{C_{1}}\) (recall that the absolute constant \(C_{1}\) is defined in Assumption 3.1). Then, \(\left|\operatorname*{\mathbf{E}}_{\widetilde{X}\sim\widetilde{S}}[\widetilde{X }^{\top}A\widetilde{X}]-\widetilde{y}_{\mathrm{median}}\right|\leq 2R\)._

Proof.: Let \(\mu^{\prime}=\operatorname*{\mathbf{E}}_{\widetilde{X}\sim\widetilde{S}}[ \widetilde{X}^{\top}A\widetilde{X}]\). (Not to be confused with \(\hat{\mu}\) and \(\widetilde{\mu}\) in Assumption 3.1, which are expectations of \(\widetilde{X}\) in the samples and at the population level, instead of the quadratic form in the definition of \(\mu^{\prime}\).)

Given that \(\widetilde{y}_{\mathrm{median}}\) (by definition) lies within the interval \([q_{m_{1}},q_{|T|-m_{1}}]\) which has length at most \(R\), it suffices to argue that \(\mu^{\prime}\) also lies within distance \(R\) of that interval. Namely, \(\mu^{\prime}\geq q_{m_{1}}-R\) and \(\mu^{\prime}\leq q_{|T|-m_{1}}+R\).

Let \(\sigma^{2}:=\operatorname*{\mathbf{Var}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{ \dagger/2}X)]\). The \((\eta,2\epsilon_{0})\)-stability of \(S\) in Assumption 3.1 (see Condition (L.3)) implies that all but an \(\eta\)-fraction of \(\left\{\widetilde{x}^{\top}A\widetilde{x}:\widetilde{x}\in\widetilde{S}\cap \widetilde{T}\right\}\) lies in the interval\(\mathbf{E}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)]\pm 10\sigma\log(1/\eta)\) since the size of the set \(\widehat{S}\cap\widetilde{T}\)) is at least \(|S|(1-2\epsilon_{0})\) (by Assumption 3.1).

Thus, for at least \((1-\eta)[S\cap T]\geq|S|(1-2\epsilon_{0}-\eta)|S|\) points in \(T\), the value of \(\widetilde{x}^{\top}A\widetilde{x}\) lies in the interval \(\mathbf{E}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)]\pm 10\sigma\log(1/\eta)\). Since \(\eta\leq 0.01\) and \(\epsilon_{0}<1/4\), the number of such points is at least \(|S|/3\).

Rephrasing, there is an interval \([y_{\mathrm{left}},y_{\mathrm{right}}]\) that contains at least \(|S|/3\) points in \(\widetilde{T}\), where

\[y_{\mathrm{left}} =\mathop{\mathbf{E}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{ \dagger/2}X)]-10\sigma\log(1/\eta)\,\] \[y_{\mathrm{right}} =\mathop{\mathbf{E}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{ \dagger/2}X)]+10\sigma\log(1/\eta)\.\]

Therefore, there are at most \(|T|-|S|/3\) points in \(\widetilde{T}\) that are less than \(y_{\mathrm{left}}\), implying that \(q_{|T|-|S|/3}\geq y_{\mathrm{left}}\). Furthermore, since, by assumption we have \(m_{1}\leq|S|/3\), this implies that \(q_{|T|-m_{1}}\geq y_{\mathrm{left}}\). By a symmetric argument, we also have that \(q_{m_{1}}\leq y_{\mathrm{right}}\).

Next, recall that by Item 6 of Lemma E.1, we have that \(|\mathop{\mathbf{E}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)]-\mu^ {\prime}|=|\mathop{\mathbf{E}}_{X\sim D}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2} X)]-\mathop{\mathbf{E}}_{\widetilde{X}\sim\widetilde{S}}[\widetilde{X}^{\top}A \widetilde{X}]|<1/\alpha\). Thus, we have \(q_{|T|-m_{1}}\geq y_{\mathrm{left}}\geq\mu^{\prime}-10\sigma\log(1/\eta)-1/\alpha\). Rearranging, we get \(\mu^{\prime}\leq q_{|T|-m_{1}}+10\sigma\log(1/\eta)+1/\alpha\). Symmetrically, we get that \(\mu^{\prime}\geq q_{m_{1}}-10\sigma\log(1/\eta)-1/\alpha\).

Finally, we argue that \(10\sigma\log(1/\eta)+1/\alpha\leq R\), showing that \(\mu^{\prime}\leq q_{|T|-m_{1}}+R\) and \(\mu^{\prime}\geq q_{m_{1}}-R\), which as we argued at the beginning of the proof is sufficient to show the lemma statement. To argue this, by Lemma 3.2, we have that \(\sigma\leq 5\sqrt{C_{1}}/\alpha\). Thus, it suffices to choose, as in the lemma statement, that \(R=(C/\alpha)\log(1/\eta)\) for some sufficiently large \(C>0\).

The above result implies that if we calculate the "squared deviation" of the projections of inliers centered around the _empirical median of the corrupted set_, then it will be of the order of the empirical variance of inliers up to a factor of \(R^{2}\) (by triangle inequality). Moreover, the empirical variance of the inliers is of the order \(O(1/\alpha^{2})\) by Lemma 3.2. Thus if the variance of the corrupted set is much larger than the upper bound, then we can assign score to each point, the function \(f\) below, such that the contribution from outliers is much larger than the inliers.

**Lemma E.4** (Filtering; Formal version of Lemma 3.5).: _Make Assumption 3.1. Let \(A\) be an arbitrary symmetric matrix with \(\|A\|_{\mathrm{F}}=1\). Let \(R=C(1/\alpha)\log(1/\eta)\) for \(C\geq 100\sqrt{C_{1}}\). Define \(\widetilde{y}_{\mathrm{median}}=\mathrm{Median}(\{\widetilde{x}^{\top}A \widetilde{x}\mid\widetilde{x}\in\widetilde{T}\})\). Define the function \(f(\widetilde{x}):=(\widetilde{x}^{\top}A\widetilde{x}-\widetilde{y}_{\mathrm{ median}})^{2}\). Let \(m_{1}\) be a number less than \(|S|/3\). Denote by \(q_{i}\) the \(i\)-th smallest point of \(\{\widetilde{x}^{\top}A\widetilde{x}\mid\widetilde{x}\in\widetilde{T}\}\)._

_If \(q_{|T|-m_{1}}-q_{m_{1}}\leq R\) and \(\mathop{\mathbf{E}}_{X\sim T}\left[f(x)\right]>C^{\prime}R^{2}/\alpha^{3}\) for \(C^{\prime}\geq 720/\epsilon_{0}\), that is, in the case where the check in Line 12 fails, then, the function \(f(\cdot)\) satisfies_

\[\sum_{\widetilde{x}\in\widetilde{T}}f(\widetilde{x})>\frac{40}{\epsilon_{0}} \frac{1}{\alpha^{3}}\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}}f( \widetilde{x})\.\]

Proof.: The core argument for the proof is that \(\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}}f(\widetilde{x})\) can be upper bounded under the assumption that \(q_{|T|-m_{1}}-q_{m_{1}}\leq R\), as shown in the following claim.

**Claim E.5**.: _Assuming the conditions in Lemma E.4, we have \(\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}}f(\widetilde{x})\leq 9|S|R^{2}\)._

Proof.: Let \(\mu^{\prime}=\mathop{\mathbf{E}}_{\widetilde{X}\sim\widetilde{S}}[\widetilde{X} ^{\top}A\widetilde{X}]\). We have the following series of inequalities:

\[\sum_{\widetilde{x}\in\widetilde{S}\cap\widetilde{T}}f(\widetilde{x}) \leq\sum_{\widetilde{x}\in\widetilde{S}}f(\widetilde{x})\] (since \[f(x)\geq 0\] ) \[=\sum_{\widetilde{x}\in\widetilde{S}}(\widetilde{x}^{\top}A \widetilde{x}-\widetilde{y}_{\mathrm{median}})^{2}\]\[\leq 2\sum_{\widetilde{x}\in S}\left(\left(x^{\top}Ax-\mu^{\prime} \right)^{2}+\left(\mu^{\prime}-\widetilde{y}_{\mathrm{median}}\right)^{2} \right)\qquad\text{(by $(a+b)^{2}\leq 2a^{2}+2b^{2}$)}\] \[=2|S|\underbrace{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{S}}[ \widetilde{X}^{\top}A\widetilde{X}]+2\sum_{\widetilde{x}\in\widetilde{S}}(\mu^ {\prime}-\widetilde{y}_{\mathrm{median}})^{2}\] \[\leq 2|S|\underbrace{\mathbf{Var}}_{\widetilde{X}\sim\widetilde{S}} [\widetilde{X}^{\top}A\widetilde{X}]+8R^{2}|S|\] (by Lemma E.3) \[\leq 2|S|\left(\frac{72C_{1}}{\alpha^{2}}\right)+8R^{2}|S|\] (by Lemma 3.2 and Condition (L.4)) \[\leq 9|S|R^{2}\,\] (by definition of \[R\] )

where the last line uses that \(R\geq 12\sqrt{C_{1}}/\alpha\), which is satisfied for \(C\geq 12\sqrt{C_{1}}\). This completes the proof of Claim E.5. 

On the other hand, we have that

\[\sum_{\widetilde{x}\in\widetilde{T}}f(\widetilde{x}) >C^{\prime}R^{2}|T|/\alpha^{3}\] (by assumption) \[\geq 0.5C^{\prime}R^{2}|S|/\alpha^{3}\] (since

\[|T|\geq|S\cap T|\geq|S|(1-2\epsilon_{0})\geq|S|/2\] \[>(360/\epsilon_{0})R^{2}|S|/\alpha^{3}\]

(since

\[C^{\prime}\geq 720/\epsilon_{0}\]

) \[\geq 40/(\epsilon_{0}\alpha^{3})\sum_{\widetilde{x}\in\widetilde{S} \cap\widetilde{T}}f(\widetilde{x})\.\] (using Claim E.5)

This completes the proof of Lemma E.4.

## Appendix F Proof of Theorem 1.2

In this section, we present the main algorithmic result of the paper. As previously stated, the theorem holds for distributions beyond Gaussians, as long as the input set of points satisfies the deterministic conditions from Section 2.1 and the distribution meets a mild requirement. We now restate and prove the theorem in this more general form.

**Theorem F.1** (Weak List Decodable Covariance Estimation).: _Let the ambient dimension be \(d\in\mathbb{Z}_{+}\), let \(C\) be a sufficiently large absolute constant, and let the parameters \(\alpha\in(0,1/2)\), \(\epsilon\in(0,\epsilon_{0})\) for a sufficiently small positive constant \(\epsilon_{0}\), and failure probability \(\delta\in(0,1/2)\) be known to the algorithm. Let \(D\) be a distribution with mean \(\mu\in\mathbb{R}^{d}\), covariance \(\Sigma\in\mathbb{R}^{d\times d}\), and assume that \(\mathbf{Var}_{\chi\sim D}[X^{\top}AX]=O(|[\Sigma^{1/2}A\Sigma^{1/2}\|_{F}^{2} +\|\Sigma^{1/2}A\mu\|_{2}^{2})\). There is a polynomial-time algorithm such that, on input \(\alpha,\delta\) as well as an \((\alpha,\epsilon)\)-corruption of a set \(S\) (Definition 1.1) with \(|S|>C/(\alpha^{6}\epsilon_{0}^{2})\log(1/(\alpha\delta))\) which is \(((\epsilon_{0}/40)\alpha^{3},2\epsilon_{0}))\)-stable with respect to \(D\) (Definition 2.1), with probability at least \(1-\delta\) over the randomness of the algorithm, the algorithm returns a list of at most \(O(1/\alpha)\) many sets \(T_{i}\) which are disjoint subsets of samples in the input, each \(T_{i}\) has size at least \(0.5\alpha m\), and there exists a \(T_{i}\) in the output list such that:_

* _Recall the notation in the corruption model (_Definition 1.1_) where_ \(n\) _is the size of the original inlier set_ \(S\) _and_ \(\ell\) _is the number of points in_ \(S\) _that the adversary replaced--_\(n\) _and_ \(\ell\) _are unknown to the algorithm except that_ \(n\geq\alpha m\) _and_ \(\ell\leq\epsilon n\)_. The set_ \(T_{i}\) _satisfies that_ \(|T_{i}\cap S|\geq(1-0.01\alpha)(n-\ell)\)_._
* _Denote_ \(H_{i}:=\mathbf{E}_{X\sim T_{i}}[XX^{\top}]\)_. The matrix_ \(H_{i}\) _satisfies_ \[\max\left(\|(H_{i}^{\dagger})^{1/2}\Sigma(H_{i}^{\dagger})^{1/2}-\mathbb{P}_{ H}\|_{\mathrm{F}}\,\ \|(H_{i}^{\dagger})^{1/2}\Sigma(H_{i}^{\dagger})^{1/2}-\mathbb{P}_{ \Sigma}\|_{\mathrm{F}}\right)\leq O((1/\alpha^{4})\log(1/\alpha)).\]

The Gaussian distribution \(\mathcal{N}(\mu,\Sigma)\) satisfies the distributional assumption of Theorem F.1 (c.f. Fact B.1), and a set of size \(m=C^{\prime}\frac{d^{2}\log^{5}(d/\alpha/\delta)}{\alpha^{6}}\) from \(\mathcal{N}(\mu,\Sigma)\) is \(((\epsilon_{0}/40)\alpha^{3},2\epsilon_{0}))\)-stable with probability \(1-\delta\) for a constant \(\epsilon_{0}\) (c.f. Lemma 2.2). Thus, Theorem F.1 theorem covers the Gaussian case \(D=\mathcal{N}(\mu,\Sigma)\) with polynomial sample complexity, and it directly implies Theorem1.2, the main result of this work.

We achieve the guarantee of TheoremF.1 by combining the procedure of the previous section in a recursive algorithm (Algorithm1). We maintain the notation of Assumption3.1 with the addition of subscripts that indicate the number of filtering steps performed.

We restate the algorithm below, before proving TheoremF.1:

```
1:Constants: \(m,\alpha,\eta,R:=C(1/\alpha^{2})\log(1/(\epsilon_{0}\alpha))\) for \(C>6000\sqrt{C_{1}}\), \(C^{\prime}>720/\epsilon_{0}\) (where \(C_{1},\epsilon_{0}\) are defined in Assumption3.1).
2:functionCovListDecoding(\(T_{0}\))
3:\(t\gets 0\).
4:loop
5: Compute \(H_{t}=\mathbf{E}_{X\sim T_{t}}[XX^{\top}]\).
6: Let \(\widetilde{T}_{t}=\{H_{t}^{1/2}x\ :\ x\in T_{t}\}\) be the transformed set of samples.
7: Let \(A\) be the symmetric matrix corresponding to the top eigenvector of \(\mathbf{Cov}_{\widetilde{X}\sim\widetilde{T}_{t}}[\widetilde{X}^{\otimes 2}]\).
8: Normalize \(A\) so that \(\|A\|_{\mathrm{F}}=1\).
9: Compute the set \(\widetilde{Y}_{t}=\{\widetilde{x}^{\top}A\widetilde{x}\ :\ \widetilde{x}\in \widetilde{T}_{t}\}\)
10: Compute the \(\alpha m/9\)-th smallest element \(q_{\mathrm{left}}\) as well as the \(\alpha m/9\)-th largest element \(q_{\mathrm{right}}\), as well as the median \(\widetilde{y}_{\mathrm{median}}\) of \(\widetilde{Y}_{t}\).
11: Define the function \(f(\widetilde{x})=(\widetilde{x}^{\top}A\widetilde{x}-\widetilde{y}_{\mathrm{ median}})^{2}\).
12:if\(\mathbf{E}_{\widetilde{X}\sim\widetilde{T}_{t}}[f(\widetilde{X})]\leq C^{ \prime}R^{2}/\alpha^{3}\)then\(\triangleright\) c.f. Lemma3.4
13:if\(|T_{t}|\geq 0.5\alpha m\)thenreturn\(\{T_{t}\}\) elsereturn the empty list.
14:elseif\(q_{\mathrm{right}}-q_{\mathrm{left}}\leq R\)then\(\triangleright\) c.f. Lemma3.5
15: Let the probability mass function \(p(\widetilde{x}):=f(\widetilde{x})/\sum_{\widetilde{x}\in\widetilde{T}_{t}}f( \widetilde{x})\).
16: Pick \(x_{\mathrm{removed}}\in T_{t}\) according to \(p(H_{t}^{\dagger/2}x)\).
17:\(T_{t+1}\gets T_{t}\setminus\{x_{\mathrm{removed}}\}\).
18:else
19:\(\tau\leftarrow\mathrm{FindDivider}(\widetilde{Y}_{t},\alpha m/9)\). \(\triangleright\) c.f. Lemma3.6
20:\(T^{\prime}\leftarrow\{H_{t}^{1/2}\widetilde{x}:\widetilde{x}\in\widetilde{T} _{t},\widetilde{x}^{\top}A\widetilde{x}\leq\tau\}\), \(T^{\prime\prime}\leftarrow\{H_{t}^{1/2}\widetilde{x}:\widetilde{x}\in T_{t}, \widetilde{x}^{\top}A\widetilde{x}>\tau\}\).
21:\(L_{1}\leftarrow\textsc{CovListDecoding}(T^{\prime}),L_{2}\leftarrow \textsc{CovListDecoding}(T^{\prime\prime})\).
22:return\(L_{1}\cup L_{2}\).
23:\(t\gets t+1\). ```

**Algorithm 3** List-Decodable Covariance Estimation in Relative Frobenius Norm

Proof of TheoremF.1.: Since the final error guarantees do not depend on \(\epsilon\), without loss of generality we use the maximum level of corruptions \(\epsilon=\epsilon_{0}=0.01\) in the following proof. We will also use the letter \(\eta\) to denote the expression \((\epsilon_{0}/40)\alpha^{3}\) as in the stability assumption in the theorem statement on the inlier set. We will proceed to prove TheoremF.1 by induction, and crucially use the following fact that "union bounds also work under conditioning".

**Fact F.2**.: _If event \(A\) happens with probability \(1-\tau_{1}\) and event \(B\) happens with probability \(1-\tau_{2}\) conditioned on event \(A\), then the probability of both \(A\) and \(B\) happening is at least \(1-\tau_{1}-\tau_{2}\)._

The technical bulk for proving TheoremF.1 is LemmaF.4, an inductive claim over the levels of recursion, that (in addition to some other basic properties) with high probability over any level, either (i) there was a recursive call at a prior level whose input contains most of the inliers, and by definition that call has terminated, or (ii) there exists some recursive call at this current level whose input contains most of the inliers. We will use the following notation (DefinitionF.3) to denote various objects in the recursive execution of our algorithm.

**Definition F.3** (Notation for Recursion Tree).: _We define a rooted binary tree \(\mathcal{T}\) that corresponds to the execution of our recursive algorithm (Algorithm1). The root node at level 0 corresponds to the top-level call of CovListDecoding (Algorithm1) and for every non-leaf node, its left child corresponds to a call of CovListDecoding from Line2 and its right child corresponds to a call from Line 21. Thus, every node is uniquely specified by the level in the tree and its position within that level. We denote by \(\mathcal{T}_{i,j}\) the \(i^{\text{th}}\) node of level \(j\), where the node numbers are also 0-indexed (e.g., the root node is \(\mathcal{T}_{0,0}\)). For the node \(\mathcal{T}_{i,j}\), we denote by \(T_{(0)}^{(i,j)}\) the input data set to the corresponding recursive call. In order to refer to the working data set at the \(t^{\text{th}}\) iteration of the main loop in the execution of node \(\mathcal{T}_{i,j}\), we use the notation \(T_{(t)}^{(i,j)}\) and \(\widetilde{T}_{(t)}^{(i,j)}\) exactly in the same way as \(T_{(t)}\) and \(\widetilde{T}_{(t)}\) are used in Algorithm 1. Finally, \(\mathcal{T}_{j}\) (i.e., using a single subscript) refers to the subtree growing from the root and including all the nodes up to and including the \(j^{\text{th}}\) level._

**Lemma F.4**.: _In the context of Theorem F.1, consider the recursion tree of CovListDecoding (Definition F.3). Then, for any \(j\in\{0,\ldots,9/\alpha\}\), we have that with probability at least \(1-0.01\alpha j\delta\) the following holds:_

1. _(Size decrease under recursion) The input of every node_ \(\mathcal{T}_{i,j}\) _of level_ \(j\) _satisfies_ \(|T_{(0)}^{(i,j)}|\leq m-j\alpha m/9\)__
2. _(Disjoint inputs) Consider the subtree_ \(\mathcal{T}_{j}\) _growing from the root and truncated at (but including) level_ \(j\)_. All the leaves in_ \(\mathcal{T}_{j}\) _have disjoint input sets. Note that the leaves may not all be at level_ \(j\)_, and some might be at earlier levels._
3. _(Bound on inliers removed) Consider the subtree_ \(\mathcal{T}_{j}\) _growing from the root and truncated at (but including) level_ \(j\)_. There exists a leaf_ \(\mathcal{T}_{i^{\prime},j^{\prime}}\) _in_ \(\mathcal{T}_{j}\) _(but the level of the leaf_ \(j^{\prime}\) _is not necessarily equal to_ \(j\)_) such that its input_ \(T_{(0)}^{(i^{\prime},j^{\prime})}\) _satisfies_ \[\alpha^{3}(\epsilon_{0}/40)|T_{(0)}^{(i^{\prime},j^{\prime})}|+|S\setminus T_ {(0)}^{(i^{\prime},j^{\prime})}|\leq(j+1)\alpha^{3}(\epsilon_{0}/20)m+\ell\] (12) _where as in_ Definition 1.1_,_ \(m\) _is the size of the original input set of the root node_ \(\mathcal{T}_{0,0}\) _and_ \(\ell\leq\epsilon_{0}n\) _is the number of samples that the adversary replaced in_ \(S\)_. In particular, the number of inliers removed by the algorithm until_ \(T_{(0)}^{(i^{\prime},j^{\prime})}\) _is at most_ \((j+1)\alpha^{3}(\epsilon_{0}/20)m\)_._

**Remark F.5**.: We remark on why (12) uses a cubic power of \(\alpha\), which is the same cubic power appearing in Lemma E.4 that states that the total score of all the outliers is a \(1/\alpha^{3}\) factor more than the total score of the inliers. The three powers of \(\alpha\) are due to the following reasons: 1) the inliers can be as few as an \(\alpha\)-fraction of all the samples, 2) the depth of the recursion tree can be as much as \(O(1/\alpha)\), meaning that a set can be filtered \(O(1/\alpha)\) times, and 3) Theorem F.1 guarantees that we remove at most an \(O(\alpha)\) fraction of inliers. We also remark that, by tweaking parameters in the algorithm and increasing the power of \(\operatorname{poly}(1/\alpha)\) in our covariance estimation guarantees, we can make the score function ratio (and hence the \(\operatorname{poly}(\alpha)\) factor in (12)) as large a power of \(\alpha\) as we desire. This would guarantee that we remove at most a smaller \(\operatorname{poly}(\alpha)\) fraction of inliers.

Proof.: We prove Lemma F.4 by induction on the level number \(j\). For the base case of \(j=0\), Conditions 1 and 2 are trivial. As for Condition 3, at \(j=0\) we have \(|T_{(0)}^{(0,0)}|=m\) and \(|S\setminus T_{(0)}^{(0,0)}|\leq\ell\) by the definition of the contamination model (Definition 1.1), which directly implies \(\alpha^{3}(\epsilon_{0}/40)|T_{(0)}^{(0,0)}|+|S\setminus T_{(0)}^{(0,0)}|\leq \alpha^{3}(\epsilon_{0}/40)m+\ell\leq\alpha^{3}(\epsilon_{0}/20)m+\ell\) as desired.

To show the inductive case, we assume the lemma statement is true for some \(j\) and we will show the statement for the case \(j+1\).

Conditions 1 and 2These are trivially true, by virtue of the fact that the recursive algorithm, after it is done with removing points via filtering, it partitions the input set using FindDivider which guarantees that both output sets have size at least \(m_{1}=\alpha m/9\) (Lemma 3.6).

Condition 3Recall the notation \(T_{(t)}^{(i,j)}\) from Definition F.3 that is used to denote the set of points in the variable \(T_{t}\) after the \(t^{\text{th}}\) iteration in the \(i^{\text{th}}\) recursive call in level \(j\).

By Fact F.2, we will condition on Condition 3 being true for \(\mathcal{T}_{j}\), the subtree truncated at (but including) level \(j\) and show that Condition 3 holds also for \(\mathcal{T}_{j+1}\), the subtree truncated at level \(j+1\), except with probability \(0.01\alpha\delta\). Concretely, the conditioning implies that there exists a leaf \(\mathcal{T}_{i^{\prime},j^{\prime}}\) of the subtree\(\mathcal{T}_{j}\) whose input set \(T_{(0)}^{(i^{\prime},j^{\prime})}\) satisfies the inequality in Condition 3 for \(j^{\prime}\leq j\). If \(j^{\prime}<j\), then we are done (since it continues to be a leaf and the desired bound on the right hand size in Equation (12) is only larger). Otherwise, in the case of \(j^{\prime}=j\), we have to analyze the execution of the recursive call associated with this leaf node in the recursion tree \(\mathcal{T}_{j}\).

To begin with the analysis, we verify that the input set \(T_{(0)}^{(i^{\prime},j)}\) satisfies Assumption 3.1. We first check that \(|S\cap T_{(0)}^{(i^{\prime},j)}|\geq n(1-2\epsilon_{0})\):

\[|S\cap T_{(0)}^{(i^{\prime},j)}| =|S|-|S\setminus T_{(0)}^{(i^{\prime},j)}|\] \[\geq n-(j+1)\alpha^{3}(\epsilon_{0}/20)m-\ell\] (by the inductive hypothesis) \[\geq n-(9/\alpha+1)\alpha^{3}(\epsilon_{0}/20)m-\ell\] ( \[j\leq 9/\alpha\] ) \[\geq n-\alpha^{2}\epsilon_{0}m-\epsilon_{0}n\] ( \[\ell\leq\epsilon_{0}n\] ) \[\geq n-\alpha\epsilon_{0}n-\epsilon_{0}n\] ( \[n\geq\alpha m\] ) \[\geq n-1.5\epsilon_{0}n\,\] (13)

where the last line uses that \(\alpha\leq 1/2\). Then it remains to check that \(|T_{(0)}^{(i^{\prime},j)}|\leq(1/\alpha)|S|\), but this is trivially satisfied since it holds for the input set \(T_{(0)}^{(0,0)}\) that we start with: \(|T_{(0)}^{(i^{\prime},j)}|\leq|T_{(0)}^{(0,0)}|=m\leq(1/\alpha)n=(1/\alpha)|S|\).

In a recursive call, the algorithm iteratively removes points (through Line 16) before either terminating (Line 12) or calling FindDivider and recursing (Line 21). Denote the iteration just before either terminating or running FindDivider by \(t_{i^{\prime}}^{*}\). We need to argue that the iterative filtering of points prior to iteration \(t_{i^{\prime}}^{*}\) still roughly preserves Condition 3.1.2, that is stated below and proved in Lemma F.6, captures the standard martingale argument for filtering-based robust algorithms, and it informally states that if the input set \(T_{(0)}^{(i^{\prime},j)}\) satisfies Assumption 3.1, then Condition 3 is roughly preserved.

**Lemma F.6**.: _Recall the notations and assumptions from Assumption 3.1. Consider an execution of \(\textsc{CovListDecoding}(T_{0})\) with \(T_{0}=T\) (with \(T\) satisfying the assumptions from Assumption 3.1), and suppose that \(|T_{0}|\geq C^{\prime}/(\alpha^{6}\epsilon_{0}^{2})\log{(1/(\alpha\delta))}\). Further assume that \(|S\setminus T_{0}|\leq 1.5\epsilon_{0}|S|\) (which is a strengthening over Assumption 3.1). Moreover, denote by \(T_{t}\) the dataset through the loop iterations of Algorithm 1. Then, with probability at least \(1-0.01\alpha\delta\), it holds that_

\[\alpha^{3}(\epsilon_{0}/40)|T_{t}|+|S\setminus T_{t}|\leq\alpha^{3}(\epsilon_ {0}/40)|T_{0}|+|S\setminus T_{0}|+\alpha^{3}(\epsilon_{0}/40)|T_{0}|\,\]

_simultaneously for all iterations \(t\) until the execution of \(\textsc{CovListDecoding}\) enters either Line 13 or Line 19._

Concretely, applying Lemma F.6 for input set \(T_{(0)}^{(i^{\prime},j)}\) and with failure probability \(0.01\alpha\delta\) in place of \(\delta\). The lemma is applicable since its first requirement \(|S\setminus T_{(0)}^{(i^{\prime},j)}|\leq 1.5\epsilon_{0}|S|\) has been already checked in (13), and for its second requirement we have that \(|T_{(0)}^{(i^{\prime},j)}|\geq|T_{(0)}^{(i^{\prime},j)}\cap S|\geq(1-1.5 \epsilon_{0})|S|\geq C^{\prime}/(\alpha^{6}\epsilon_{0}^{2})\log(1/(\alpha \delta))\), where the last one uses (13) and the next step uses our assumption \(|S|>C/(\alpha^{6}\epsilon_{0}^{2})\log(1/(\alpha\delta))\). The lemma then yields that, with probability at least \(1-0.01\alpha\delta\), \(T_{(t_{i^{\prime}}^{*})}^{(i^{\prime},j)}\) is such that

\[\alpha^{3}(\epsilon_{0}/40)|T_{(t_{i^{\prime}}^{*})}^{(i^{\prime},j)}|+|S \setminus T_{(t_{i^{\prime}}^{*})}^{(i^{\prime},j)}|\leq(j+1)\alpha^{3}( \epsilon_{0}/20)m+\ell+(\epsilon_{0}/40)\alpha^{3}m\.\] (14)

Now, either the recursive call terminates at iteration \(t_{i^{\prime}}^{*}\) or it goes into Line 19. In the former case, we are done (since it is now a leaf node). Otherwise, the recursive call uses FindDivider to partition \(T_{(t_{i^{\prime}}^{*})}^{(i^{\prime},j)}\) into \(T^{\prime}\) and \(T^{\prime\prime}\). We need to show that at least one of \(T^{\prime}\) and \(T^{\prime\prime}\) satisfies Condition 3 (for case \(j+1\)), using Lemma 3.6. Let us now derive what parameters and set to invoke Lemma 3.6 with.

Using (i) Condition (L.3) of the stability property of the original inlier set \(S\), and (ii) the fact that \((H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)-\mathbf{E}_{X\sim\mathcal{N}(\mu, \Sigma)}[(H^{\dagger/2}X)^{\top}A(H^{\dagger/2}X)]\) is an even quadratic in \(X\) for any 

[MISSING_PAGE_FAIL:32]

\[=(j+2)\alpha^{3}(\epsilon_{0}/20)m+\ell\;.\] (16)

Thus Condition 3 is satisfied by \(T^{\prime}\) for case \(j+1\), completing the inductive proof of this lemma.

We now use the lemma to conclude the proof of Theorem F.1.

By Lemma F.4, we know that with probability at least \(1-0.09\delta\), the recursion tree generated up to level \(9/\alpha\), namely \(\mathcal{T}_{9/\alpha}\), satisfies the lemma guarantees. In particular, since (i) by Line 19 in Algorithm 1, the input to any recursive call must have size at least \(m_{1}=\alpha m/9\), and (ii) Condition 1 of Lemma F.4 applied to \(j=m/m_{1}=9/\alpha\) yields an input size upper bound of 0, we can conclude that the execution of the algorithm must have completely terminated by level \(9/\alpha\). We know by Lemma F.4 that there exists a leaf node \(i\) in the recursion tree whose input satisfies Condition 3 of the third part of the lemma statement. That is, letting \(j\) denote the level of that leaf, we have that

\[\alpha^{3}(\epsilon_{0}/40)|T^{(i,j)}_{(0)}|+|S\setminus T^{(i,j)}_{(0)}|\leq (j+1)\alpha^{3}(\epsilon_{0}/20)m+\ell\;.\] (17)

The recursion node \(\mathcal{T}_{i,j}\) starts with that input set \(T^{(i,j)}_{(0)}\) and, before terminating, it may perform some filtering steps. As in the proof of Lemma F.4, we will use Lemma F.6 to analyze the "working set" right before the recursive call terminates. If we denote by \(t^{*}\) the number of filtering steps before termination, Lemma F.6 yields that except with probability \(0.01\alpha\delta\), we have

\[\alpha^{3}(\epsilon_{0}/40)|T^{(i,j)}_{(t^{*})}|+|S\setminus T^{(i,j)}_{(t^{*} )}|\leq(j+1)\alpha^{3}(\epsilon_{0}/20)m+\ell+(\epsilon_{0}/40)\alpha^{3}m\;.\] (18)

Note that by Fact F.2, the total failure probability is upper bounded by \(0.09\delta+0.01\alpha\delta\) which is less than \(\delta\).

We are now ready to prove the first bullet of Theorem F.1. The above inequality implies that

\[|T^{(i,j)}_{(t^{*})}\cap S| =|S|-|S\setminus T^{(i,j)}_{(t^{*})}|\] \[\geq n-\ell-(\epsilon_{0}/20)(j+1)\alpha^{3}m-(\epsilon_{0}/40) \alpha^{3}m\] (by ( 18 )) \[\geq n-\ell-(\epsilon_{0}/20)(j+2)\alpha^{3}m\] \[\geq n-\ell-(\epsilon_{0}/20)(j+2)\alpha^{2}n\] ( \[n\geq\alpha m\] ) \[\geq n-\ell-(\epsilon_{0}/2)\alpha n\] ( \[j\leq 9/\alpha\] and \[\alpha<1/2\] ) \[\geq n-\ell-\epsilon_{0}(1-\epsilon_{0})\alpha n\] ( \[1/2\leq 1-\epsilon_{0}\] ) \[\geq n-\ell-\epsilon_{0}\alpha(n-\ell)\] ( \[\ell\leq\epsilon_{0}n\] ) \[=(n-\ell)(1-\epsilon_{0}\alpha)\;.\] (19)

(19) further implies that \(|T^{(i,j)}_{(t^{*})}\cap S|\geq(n-\ell)(1-\epsilon_{0}\alpha)\geq n(1- \epsilon_{0})(1-\epsilon_{0}\alpha)\geq(\alpha/2)m\) (since \(n\geq\alpha m\) and \(\epsilon_{0}\) and \(\alpha\) are small), meaning that the set \(T^{(i,j)}_{(t^{*})}\) will indeed be returned. This completes the proof of the first bullet of Theorem F.1.

We now move to the second bullet. Defining \(H=\mathbf{E}_{X\sim T^{(i,j)}_{(t^{*})}}[XX^{\top}]\), we need to show that \(\|H^{\dagger/2}\Sigma H^{\dagger/2}-HH^{\dagger}\|_{\mathrm{F}}\lesssim(1/ \alpha^{4})\log^{2}(1/\alpha)\). We will apply Lemma E.2 to show the bounds in the second bullet, which requires checking that Assumption 3.1 is satisfied by \(T^{(i,j)}_{(t^{*})}\). Earlier, right below (19), we have already checked that \(|T^{(i,j)}_{(t^{*})}\cap S|\geq(\alpha/2)|T^{(i,j)}_{(t^{*})}|\). The only remaining condition to check is \(|T^{(i,j)}_{(t^{*})}|\leq|S|/\alpha\), but this is trivially true since \(T^{(i,j)}_{(t^{*})}\subset T^{(0,0)}_{(0)}\) (the original input set to the recursive algorithm) and \(|T^{(0,0)}_{(0)}|\leq|S|/\alpha\) by Definition 1.1. Thus, Lemma E.2 yields that

\[\left\|H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{H}\right\|_ {\mathrm{F}}^{2} \lesssim\frac{1}{\alpha}\operatorname*{\mathbf{Var}}_{\widehat{X} \sim T^{(i,j)}_{(t^{*})}}[\widehat{X}^{\top}A\widehat{X}]+\frac{1}{\alpha^{2}}\] \[\leq\frac{1}{\alpha}\operatorname*{\mathbf{E}}_{\widehat{X} \sim T^{(i,j)}_{(t^{*})}}[f(\widehat{X})]+\frac{1}{\alpha^{2}}\qquad( \mathbf{Var}(Y)\leq\mathbf{E}[(Y-c)^{2}]\text{ for any }c\in\mathbb{R})\]\[\lesssim\frac{R^{2}}{\alpha^{4}}\] (c.f. Line 12 of Algorithm 1) \[\lesssim\frac{1}{\alpha^{8}}\log^{2}\left(\frac{1}{\alpha}\right)\,\quad \text{($R=\Theta((1/\alpha^{2})\log(1/\alpha))$, noting that $\epsilon_{0}=0.01$)}\]

where the penultimate inequality uses the fact that, if the algorithm terminated, it must be the case that \(\mathbf{E}_{\widetilde{X}\sim T_{\mathrm{end}}^{(i,j)}}[f(\widetilde{X})]\leq C ^{\prime}R^{2}/\alpha^{3}\) by Line 12 in Algorithm 1. Taking a square root on both sides implies that

\[\left\|H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{H}\right\|_{ \mathrm{F}}\lesssim\frac{1}{\alpha^{4}}\log\left(\frac{1}{\alpha}\right)\]

The same guarantee holds for \(\left\|H^{\dagger/2}\Sigma H^{\dagger/2}-\mathbb{P}_{\Sigma}\right\|_{ \mathrm{F}}\), also following from Lemma E.2.

Lastly, we check that we only return \(O(1/\alpha)\) sets in the output list. This is true by construction: (i) Line 13 only allows sets to be output if they have size at least \(\Omega(\alpha m)\), (ii) there were only \(m\) points to begin with and (iii) all the leaves have disjoint input sets by Lemma F.4. 

### Proof of Lemma F.6

We now prove that filtering does not remove too many inliers using a standard martingale argument.

**Lemma F.6**.: _Recall the notations and assumptions from Assumption 3.1. Consider an execution of \(\textsc{CovListDecoding}(T_{0})\) with \(T_{0}=T\) (with \(T\) satisfying the assumptions from Assumption 3.1), and suppose that \(|T_{0}|\geq C^{\prime}/(\alpha^{6}\epsilon_{0}^{2})\log\left(1/(\alpha\delta)\right)\). Further assume that \(|S\setminus T_{0}|\leq 1.5\epsilon_{0}|S|\) (which is a strengthening over Assumption 3.1). Moreover, denote by \(T_{t}\) the dataset through the loop iterations of Algorithm 1. Then, with probability at least \(1-0.01\alpha\delta\), it holds that_

\[\alpha^{3}(\epsilon_{0}/40)|T_{t}|+|S\setminus T_{t}|\leq\alpha^{3}(\epsilon _{0}/40)|T_{0}|+|S\setminus T_{0}|+\alpha^{3}(\epsilon_{0}/40)|T_{0}|\,\]

_simultaneously for all iterations \(t\) until the execution of \(\textsc{CovListDecoding}\) enters either Line 13 or Line 19._

Proof.: We will use the notation of Assumption 3.1 for a single call of Algorithm 1. Denote by \(t\) the iteration count. Also define the stopping time \(t_{\mathrm{end}}\) to be the first iteration when \(\alpha^{3}(\epsilon_{0}/40)|T_{t_{\mathrm{end}}}|+|S\setminus T_{t_{\mathrm{ end}}}|>2\epsilon_{0}|S|\) or when the iteration goes into Line 12 or Line 19 instead of Line 16 (that is, when \(\mathbf{E}_{X\sim T_{\mathrm{end}}}[f(H_{t}^{\dagger/2}X)]\leq C^{\prime}R^{2} /\alpha^{3}\) or when \(q_{\mathrm{right}}-q_{\mathrm{left}}>R\) for the iteration \(t_{\mathrm{end}}\)). Now, define \(\Delta_{t}=\alpha^{3}(\epsilon_{0}/40)|T_{\min\{t,t_{\mathrm{end}}\}}|+|S \setminus T_{\min\{t,t_{\mathrm{end}}\}}|\).

In order to prove the lemma, we will show that at the first \(t^{*}\) (if one exists) such that \(\alpha^{3}(\epsilon_{0}/40)|T_{t^{*}}|+|S\setminus T_{t^{*}}|>\alpha^{3}( \epsilon_{0}/40)|T_{0}|+|S\setminus T_{0}|+\alpha^{3}(\epsilon_{0}/40)|T_{0}|\), then \(\Delta_{t^{*}}>\alpha^{3}(\epsilon_{0}/40)|T_{0}|+|S\setminus T_{0}|+\alpha^{3 }(\epsilon_{0}/40)|T_{0}|\) as well. Afterwards, we will show that \(\Delta_{t}\) is a sub-martingale and use sub-martingale tail bounds to show that the sequence \(\Delta_{t}\) remains small over an entire trajectory of \(|T_{0}|\) steps with high probability.

The first step is easy to show, since the threshold of \(\alpha^{3}(\epsilon_{0}/40)|T_{0}|+|S\setminus T_{0}|+\alpha^{3}(\epsilon_{0}/ 40)|T_{0}|<\alpha^{3}(\epsilon_{0}/20)|T_{0}|+1.5\epsilon_{0}|S|\leq\alpha^{2} (\epsilon_{0}/20)|S|+1.5\epsilon_{0}|S|\leq 2\epsilon_{0}|S|\), where the first inequality is by the lemma assumption. Therefore, \(t^{*}\leq t_{\mathrm{end}}\) if it exists, meaning that \(\Delta_{t^{*}}=\alpha^{3}(\epsilon_{0}/40)|T_{t^{*}}|+|S\setminus T_{t^{*}}|\).

Now we need to show that \(\Delta_{t}\) is a sub-martingale with respect to the sequence \(T_{\min\{t,t_{\mathrm{end}}\}}\). If \(t\geq t_{\mathrm{end}}\), then \(\Delta_{t+1}\) is by definition equal to \(\Delta_{t}\) and thus \(\mathbf{E}[\Delta_{t+1}\mid T_{\min\{t,t_{\mathrm{end}}\}}]=\Delta_{t}\). Otherwise, we have that \(\mathbf{E}[\Delta_{t+1}\mid T_{t}]\) is equal to \(\Delta_{t}\) plus the expected number of inliers removed minus \(\alpha^{3}(\epsilon_{0}/40)\) times the expected number of all points removed by our filter. Since the stopping condition is not satisfied, we have \(|S\setminus T_{t}|\leq 2\epsilon_{0}|S|\), meaning that Assumption 3.1 continues to hold for \(T_{t}\), as well as that the other conditions for Lemma E.4 hold. We can therefore apply Lemma E.4 to obtain

\[\mathbf{E}[\Delta_{t+1}\mid T_{t}]=\Delta_{t}+\sum_{\widetilde{x}\in \widetilde{S}\cap\widetilde{T}_{t}}f(\widetilde{x})-\alpha^{3}\frac{\epsilon_ {0}}{40}\sum_{\widetilde{x}\in\widetilde{T}_{t}}f(\widetilde{x})\leq\Delta_{t}\,\]

Summarizing, the above case analysis shows that \(\{\Delta_{t}\}_{t\in\mathbb{N}}\) is a sub-martingale with respect to \(T_{\min\{t,t_{\mathrm{end}}\}}\).

We note also that for every \(t\), \(|\Delta_{t}-\Delta_{t+1}|\leq 1\) with probability 1. Thus, using the standard Azuma-Hoeffding inequality for sub-martingales (Fact B.2), we have that for every \(t\leq|T_{0}|\),

\[\mathbf{Pr}\left[\Delta_{t}-\Delta_{0}>(\epsilon_{0}/40)\alpha^{3}|T_{0}| \right]\leq e^{-(2/40^{2})\cdot\epsilon_{0}^{2}\alpha^{6}|T_{0}|}\;.\]

By a union bound over all \(t\in[|T_{0}|]\), we have

\[\mathbf{Pr}\left[\exists t\in[m]:\Delta_{t}>\Delta_{0}+\alpha^{3}(\epsilon_{ 0}/40)|T_{0}|\right]\leq|T_{0}|e^{-(2/40^{2})\cdot\epsilon_{0}^{2}\alpha^{6}|T _{0}|}\leq 0.01\alpha\delta\;,\] (20)

where the last inequality uses that \(|T_{0}|>\frac{C^{\prime}}{\alpha^{\epsilon}\epsilon_{0}^{2}}\log\left(\frac{1 }{\alpha\delta}\right)\) for a sufficiently large absolute constant \(C^{\prime}\).

## Appendix G Applications to Learning Gaussian Mixture Models

In this section, we prove the applications of our result to robustly learning Gaussian mixture models.

**Theorem 1.4** (Outlier-Robust Clustering and Estimation of Covariances for GMM).: _Let \(C^{\prime}>0\) be a sufficiently large constant and \(\epsilon_{0}>0\) be a sufficiently small constant. Let the parameters \(\alpha\in(0,1/2)\), \(\epsilon\in(0,\epsilon_{0})\), and failure probability \(\delta\in(0,1/2)\) be known. There is an \(\tilde{O}(m^{2}d^{2})\)-time algorithm such that, on input \(\alpha,\delta\), and \(m>C^{\prime}\frac{d^{2}\log\epsilon^{(d/\alpha\delta)}}{\alpha^{\delta}}\) many \(\epsilon\)-corrupted samples from an unknown \(k\)-component Gaussian mixture \(\sum_{p=1}^{k}\alpha_{p}\mathcal{N}(\mu_{p},\Sigma_{p})\) over \(\mathbb{R}^{d}\) as in Definition 1.3, where all \(\Sigma_{p}\)'s are full-rank and all \(\alpha_{p}\) satisfies \(\alpha_{p}\geq\alpha\) and \(k\leq\frac{1}{\alpha}\) is unknown to the algorithm, with probability at least \(1-\delta\) over the corrupted samples and the randomness of the algorithm, the algorithm returns a list of at most \(k\) many disjoint subsets of samples \(\{T_{i}\}\) such that:_

* _For the_ \(p^{\text{th}}\) _Gaussian component, denote the set_ \(S_{p}\) _as the samples in the inlier set_ \(S\) _that were drawn from component_ \(p\)_. Let_ \(n_{p}\) _be the size of_ \(S_{p}\)_, and let_ \(\ell_{p}\) _be the number of points in_ \(S_{p}\) _that the adversary replaced-_\(n_{p}\) _and_ \(\ell_{p}\) _are both unknown to the algorithm except that_ \(\mathbf{E}[n_{p}]=\alpha_{p}m\geq\alpha m\) _for each_ \(p\) _and_ \(\sum_{p}\ell_{p}\leq\epsilon\alpha m\)_. Then, for every Gaussian component_ \(p\) _in the mixture, there exists a set_ \(T_{i_{p}}\) _in the returned list such that_ \(|T_{i_{p}}\cap S_{p}|\geq(1-0.01\alpha)(n_{p}-\ell_{p})\)_._
* _For every component_ \(p\)_, there is a set of samples_ \(T_{i_{p}}\) _in the returned list such that, defining_ \(H_{i_{p}}=\mathbf{E}_{X\sim T_{i_{p}}}[XX^{\top}]\) _, we have_ \(H_{i_{p}}\) _satisfying_ \(\|H_{i_{p}}^{-1/2}\Sigma_{p}H_{i_{p}}^{-1/2}-I\|_{\mathrm{F}}\lesssim(1/\alpha ^{4})\log(1/\alpha)\)_._
* _Let_ \(\Sigma\) _be the (population-level) covariance matrix of the Gaussian mixture. For any two components_ \(p\neq p^{\prime}\) _with_ \(\|\Sigma^{-1/2}(\Sigma_{p}-\Sigma_{p^{\prime}})\Sigma^{-1/2}\|_{\mathrm{F}}>C( 1/\alpha)^{5}\log(1/\alpha)\) _for a sufficiently large constant_ \(C\)_, the sets_ \(T_{i_{p}}\) _and_ \(T_{i_{p^{\prime}}}\) _from the previous bullet are guaranteed to be different._

We show the first two bullets as a direct corollary of Theorem 1.2 applied to each Gaussian component \(D=\mathcal{N}(\mu_{p},\Sigma_{p})\) as the inlier distribution. The proof of the final bullet involves some more involved linear algebraic lemmas. Here we give only the high-level proof of the final bullet, and defer the proof of these individual lemmas to Appendix C.

Proof.: First observe that, with a sufficiently large \(\mathrm{poly}(d,\frac{1}{\alpha},\log\frac{1}{\delta})\) samples, with probability at least \(1-\delta/4\), the number of inliers \(n_{p}\) from component \(p\) is at least \(0.99\alpha m\) for all components \(p\). We will condition on this event.

Furthermore, noting that in the corruption model defined in Definition 1.3, at most \(\epsilon\alpha m\) points can be corrupted total, this implies that \(\ell_{p}\leq\epsilon\alpha m\leq(\epsilon/0.99)n_{p}\) (since \(n_{p}\geq 0.99\alpha m\)).

Therefore, we apply Theorem 1.2 to all the \(k\) Gaussian components, using parameter \(0.99\alpha\) in place of \(\alpha\), \(\epsilon/0.99\) in place of \(\epsilon\), and failure probability \(\delta/(4k)\). This implies that, with a union bound, using \(m=\mathrm{poly}(d,1/\alpha,\log k/\delta)=\mathrm{poly}(d,1/\alpha,\log 1/\delta)\) samples (since \(k\lesssim 1/\alpha\), for otherwise the Gaussian mixture will have weight greater than 1), with probability at least \(1-\delta/4\), the two bullet points in the theorem holds for all the Gaussian components, directly guaranteed by Theorem 1.2.

It remains to show that the algorithm will return at most \(k\) subsets in its output list. Noting that, across the subsets output by the algorithm for each of the components, there is already a total of \((1-0.01\alpha)\sum_{p=1}^{k}(n_{p}-\ell_{p})\) points being output. Recall that \(\sum_{p}n_{p}=m\) and \(\sum_{p}\ell_{p}\leq\epsilon\alpha m\). Thus, in the output subsets corresponding to the components, there are a total of at least \(m-(\epsilon+0.01)\alpha m\geq m-0.4\alpha m\) points (where we used that \(\epsilon\) is bounded by sufficiently small constant. Since (i) there were only \(m\) points in the input and (ii) all the output subsets are disjoint subsets of the input, this means that there are at most \(0.4\alpha m\) points that can be output in any additional subsets not corresponding to any Gaussian component. However, by Theorem1.2, any output subset must have size at least \(0.5(0.99\alpha)m\), and so there cannot be any extra subset output in the list that does not correspond to any Gaussian component. This shows that there can be at most \(k\) subsets in the output list.

We turn to the final bullet of the theorem. We will show that any two components \(p\) and \(p^{\prime}\) that belong to the same set \(T_{i}\) in the output of the algorithm satisfy that \(\|\Sigma^{-1/2}\Sigma_{p}\Sigma^{-1/2}-\Sigma^{-1/2}\Sigma_{p^{\prime}}\Sigma^{ -1/2}\|_{\mathrm{F}}\lesssim\frac{1}{\alpha^{5}}\log(1/\alpha)\). Taking the contrapositive yields the final theorem bullet.

Let \(H\) be the second moment matrix of this set, that is, \(H=\mathbf{E}_{X\sim T_{i}}[XX^{\top}]\). Then \(H\) satisfies that \(\|I-H^{\dagger/2}\Sigma_{p}H^{\dagger/2}\|_{\mathrm{F}}\leq r\) and \(\|I-H^{\dagger/2}\Sigma_{p^{\prime}}H^{\dagger/2}\|_{\mathrm{F}}\leq r\) for \(r\lesssim(1/\alpha^{4})\log(1/\alpha)\), by the second bullet point of the theorem. Furthermore, \(H\) is full rank because \(H\) is the second moment matrix of a large subset of empirical points of component \(p\) and thus Condition (L.5) implies that \(\ker(H)\) is contained in \(\ker(\Sigma_{p})\), which is empty since \(\Sigma_{p}\) is full rank by the theorem assumption.

We will now apply the following result (LemmaC.1) to infer that \(\Sigma_{p}\) and \(\Sigma_{p^{\prime}}\) must be close, in the sense of the final theorem bullet point. We defer the proof of this lemma to AppendixC.

We will now apply LemmaC.1 to \(\Sigma_{1}=\Sigma_{p}\) and \(\Sigma_{2}=\Sigma_{p^{\prime}}\), and upper bound the right hand side of the lemma to give the final theorem bullet point.

We claim that the operator norm of \(\Sigma^{-1/2}\Sigma_{p}\Sigma^{-1/2}\) is bounded by \(1/\alpha\), due to the fact that component \(p\) has mass at least \(\alpha\). Indeed, we have that

\[\Sigma_{p}\preceq\underset{X\sim\mathcal{N}(\mu_{p},\Sigma_{p})}{\mathbf{E}} [(X-\mu)(X-\mu)^{\top}]\preceq\frac{1}{\alpha_{p}}\cdot\underset{X\sim\sum_{i }\alpha_{i}\mathcal{N}(\mu_{i},\Sigma_{i})}{\mathbf{E}}[(X-\mu)(X-\mu)^{\top} ]=\frac{1}{\alpha_{p}}\cdot\Sigma\.\]

This in particular implies that \(\Sigma^{-1/2}\Sigma_{p}\Sigma^{-1/2}\preceq\frac{1}{\alpha_{p}}I\preceq\frac{ 1}{\alpha}I\), and hence \(\|\Sigma^{-1/2}\Sigma_{p}\Sigma^{-1/2}\|_{\mathsf{op}}\leq\frac{1}{\alpha}\). The same conclusion holds for the component \(p^{\prime}\). Recalling that \(r\lesssim(1/\alpha^{4})\log(1/\alpha)\), the right hand side of LemmaC.1 can be upper bounded by \(O((1/\alpha^{5})\log(1/\alpha))\), thus yielding the final bullet in the theorem, as discussed above.

We end this section with a brief discussion on the proof of LemmaC.1. The lemma states that, if there exists a single matrix \(H\) that is an approximate square root to both \(\Sigma_{1}\) and \(\Sigma_{2}\), in the sense that \(\|I-H^{\dagger/2}\Sigma_{1}H^{\dagger/2}\|_{\mathrm{F}}\) and \(\|I-H^{\dagger/2}\Sigma_{1}H^{\dagger/2}\|_{\mathrm{F}}\) are both upper bounded, then \(\Sigma_{1}\) and \(\Sigma_{2}\) cannot be too far from each other, in that \(\Sigma_{1}-\Sigma_{2}\) must also have bounded Frobenius norm. In fact, this can be generalized to the normalized version \(\Sigma^{-1/2}(\Sigma_{1}-\Sigma_{2})\Sigma^{-1/2}\) for any positive definite matrix \(\Sigma\).

For simplicity of the discussion, let us assume for now that \(\Sigma=I\). To show LemmaC.1, a natural first idea is to show that, if \(\|I-H^{\dagger/2}\Sigma_{1}H^{\dagger/2}\|_{\mathrm{F}}\) is bounded, say by some quantity \(\rho\), then so must be \(\|HH^{\top}-\Sigma_{1}\|_{\mathrm{F}}\) by some quantity related to \(O(\rho)\), times the operator norm of \(\Sigma_{1}\) for appropriate scaling. Unfortunately, that is not true: Even for \(\Sigma_{1}=I\), we can have \(H\) being infinity in the first diagonal element and 1s in the other diagonal elements, and \(\|I-H^{\dagger/2}\Sigma_{1}H^{\dagger/2}\|_{\mathrm{F}}\) would only be 1 yet \(HH^{\top}-\Sigma_{1}\) is infinite in the first dimension. The crucial observation, then, is that a slightly weaker statement is true: On an orthonormal basis that depends only on \(H\) and not on \(\Sigma_{1}\), \(HH^{\top}\) approximates \(\Sigma_{1}\) in most dimensions, namely \(d-O(\rho^{2})\) many dimensions. This statement is formally captured by LemmaC.2. Thus, in these dimensions, we can bound the difference between \(\Sigma_{1}\) and \(\Sigma_{2}\) (in Frobenius norm). In the rest of the dimensions where \(HH^{\top}\) does not approximate either \(\Sigma_{1}\) or \(\Sigma_{2}\) well, we can bound these dimensions' contribution to the Frobenius norm of \(\Sigma_{1}-\Sigma_{2}\) simply by \(\max(\|\Sigma_{1}\|_{\mathsf{op}},\|\Sigma_{2}\|_{\mathsf{op}})\) times the number of such dimensions, which is small. Combining these bounds yields LemmaC.1 as desired.