# A Separation in Heavy-Tailed Sampling:

Gaussian vs. Stable Oracles for Proximal Samplers

 Ye He

Georgia Institute of Technology

yhe367@gatech.edu &Alireza Mousavi-Hosseini

University of Toronto, and Vector Institute

mousavi@cs.toronto.edu &Krishnakumar Balasubramanian

University of California, Davis

kbala@ucdavis.edu &Murat A. Erdogdu

University of Toronto, and Vector Institute

erdogdu@cs.toronto.edu

###### Abstract

We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only \(\mathcal{O}(\log(1/\varepsilon))\) versus \(\Omega(\text{poly}(1/\varepsilon))\) iterations to output a sample which is \(\varepsilon\)-close to the target in \(\chi^{2}\)-divergence. Our results are presented for proximal samplers that are based on Gaussian versus stable oracles. We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.

## 1 Introduction

The task of sampling from heavy-tailed targets arises in various domains such as Bayesian statistics [1, 1], machine learning [1, 2, 13, 14], robust statistics [15, 16, 17, 18], multiple comparison procedures [1, 19], and study of geophysical systems [2, 18, 19]. This problem is particularly challenging when using gradient-based Markov Chain Monte Carlo (MCMC) algorithms due to diminishing gradients, which occurs when the tails of the target density decay at a slow (e.g. polynomial) rate. Indeed, canonical algorithms like Langevin Monte Carlo (LMC) have been empirically observed to perform poorly [1, 16, 17] when sampling from such heavy-tailed targets.

Several approaches have been proposed in the literature to overcome these limitations of LMC and related algorithms. The predominant ones include (i) transformation-based approaches, where a diffeomorphic (invertible) transformation is used to first map the heavy-tailed density to a light-tailed one so that a light-tailed sampling algorithm can be used [10, 14, 15], (ii) discretizing general Ito diffusions with non-standard Brownian motion that have heavy-tailed densities as their equilibrium density [1, 16, 15], and (iii) discretizing stable-driven stochastic differential equations [18]. However, the few theoretical results available on the analysis of algorithms based on approaches (i) and (ii) provide only low-accuracy heavy-tailed samplers; such algorithms require \(\text{poly}(1/\varepsilon)\) iterations to obtain a sample that is \(\varepsilon\)-close to the target in a reasonable metric of choice. Furthermore, quantitative complexity guarantees for the sampling approach used in (iii) are not yet available; thus, existing comparisons are mainly based on empirical studies.

In stark contrast, when the target density is light-tailed it is well-known that algorithms like proximal samplers based on Gaussian oracles and the Metropolis Adjusted Langevin Algorithm (MALA) have high-accuracy guarantees; these algorithms require only \(\text{polylog}(1/\varepsilon)\) iterations to obtain a sample which is \(\varepsilon\)-close to the target in some metric. See, for example, the works by [10, 14, 15].

WSC22a, CCSW22, CG23]. Specifically, [1] analyzed the proximal sampling algorithm to sample from a class of strongly log-concave densities and obtained high-accuracy guarantees. [13] established similar high-accuracy guarantees for the proximal sampler to sample from target densities that satisfy a certain functional inequality, covering a range of light-tailed densities with exponentially fast tail decay (e.g. log-Sobolev and Poincare inequalities). However, it is not clear if the proximal sampler achieves the same desirable performance when the target is not light-tailed.

In light of existing results, in this work, we first consider the following question:

**Q1.** _What are the fundamental limits of proximal samplers under the Gaussian oracle when sampling from heavy-tailed targets?_

To answer this question, we construct lower bounds showing that Gaussian-based samplers necessarily require \(\text{poly}(1/\varepsilon)\) iterations to sample from a class of heavy-tailed targets. These results complement the lower bounds on the complexity of sampling from heavy-tailed densities using the LMC algorithm established in [16]. With this lower bound in hand, we next consider the following question:

**Q2.** _Is it possible to design high-accuracy samplers for heavy-tailed targets?_

We answer this in the affirmative by constructing proximal samplers that are based on stable oracles (see Definition 1 and Algorithm 2) by leveraging the fractional heat-flow corresponding to a class of stable-driven SDEs. We analyze the complexity of this algorithm when sampling from heavy-tailed densities that satisfy a fractional Poincare inequality, and establish that they require only \(\log(1/\varepsilon)\) iterations. Together, our answers to **Q1** and **Q2** provide a clear separation between samplers based on Gaussian and stable oracles. Our contributions can be summarized as follows.

* _Lower bounds for the Gaussian oracle_: In Section 2, we focus on **Q1** and establish in Theorems 1 and 2 respectively that the Langevin diffusion and the proximal sampler based on the Gaussian oracle necessarily have a fundamental barrier when sampling from heavy-tailed densities. Our proof technique builds on [15], and provides a novel perspective for obtaining algorithm-dependent lower bounds for sampling, which may be of independent interest.
* _A proximal sampler based on the stable oracle:_ In Section 3, we introduce a proximal sampler based on the \(\alpha\)-stable oracle, which fundamentally relies on the exact implementations of the fractional heat flow that correspond to a stable-driven SDE. Here, the parameter \(\alpha\) determines the allowed class of heavy-tailed targets which could be sampled with high-accuracy. In Theorem 3 and Proposition 1, we provide upper bounds on the iteration complexity that are of smaller order than the corresponding lower bounds established for the Gaussian oracle. We provide a rejection-sampling based implementation of the \(\alpha\)-stable oracle for the case \(\alpha=1\) and prove complexity upper bounds in Corollary 3. Finally, in Theorem 4, considering a sub-class of Cauchy-type targets, we prove lower bounds showing that our upper bounds cannot be fundamentally improved.

An illustration of our results for Cauchy target densities, \(\pi_{\nu}\propto(1+|x|^{2})^{-(d+\nu)/2}\) where \(\nu\) is the degrees of freedom, is provided in Table 1. We specifically consider the practical version of the stable proximal sampler with \(\alpha=1\) (i.e., Algorithm 2 with the stable oracle implemented by Algorithm 3), and show that it always outperforms the Gaussian proximal sampler (Algorithm 1). Indeed, when \(\nu\geq 1\), the separation between these algorithms is obvious. In the case \(\nu\in(0,1)\), Algorithm 2 & 3 has a \(\text{poly}(1/\varepsilon)\) complexity, nevertheless, it still improves the complexity of the Gaussian proximal sampler by a factor of \(\varepsilon\). We also show via lower bounds (in Section 3.4) that the \(\text{poly}(1/\varepsilon)\) complexity for Algorithm 2 & 3, when \(\nu\in(0,1)\), can only be improved up to certain factors. We remark that for the ideal proximal sampler (Algorithm 2), the upper bound when \(\nu\in(0,1)\) is also \(\mathcal{O}(\log(1/\varepsilon))\). These results demonstrate a clear separation between Gaussian and stable proximal samplers.

**Related works.** We first discuss works analyzing the complexity of heavy-tailed sampling as characterized by a functional inequality assumption. [1] analyzed the connection between sampling

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \cline{2-5} \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\(\nu\geq 1\)} & \multicolumn{2}{c|}{\(\nu\in(0,1)\)} \\ \hline Oracle & Gaussian (Alg. 1) & Stable (Alg. 2 \& 3) & Gaussian (Alg. 1) & Stable (Alg. 2 \& 3) \\ \hline Complexity & \(\tilde{\Omega}(\varepsilon^{-\frac{1}{\nu}})\) (Cor. 2) & \(\mathcal{O}(\log(\varepsilon^{-1}))\) (Cor. 5) & \(\tilde{\Omega}(\varepsilon^{-\frac{1}{\nu}})\) (Cor. 2) & \(\tilde{\mathcal{O}}(\varepsilon^{-\frac{1}{\nu}+1})\) (Cor. 5) \\ \hline \end{tabular}
\end{table}
Table 1: **Separation for Proximal Samplers: Gaussian vs. practical Stable oracles (\(\alpha=1\)): Upper and lower iteration complexity bounds to generate an \(\varepsilon\)-accurate sample in \(\chi^{2}\)-divergence from the generalized Cauchy target densities with degrees of freedom \(\nu\), i.e. \(\pi_{\nu}\propto(1+|x|^{2})^{-(d+\nu)/2}\). Here, \(\tilde{\Omega}\), \(\tilde{\mathcal{O}}\) hide constants depending on \(\nu\) and \(\text{polylog}(d,1/\varepsilon)\). For the proximal sampler with a general \(\alpha\)-Stable oracle (Algorithm 2), the upper bound for \(\nu\in(0,1)\) is \(\mathcal{O}(\log(1/\varepsilon))\) when \(\alpha=\nu\). The lower bounds are from Corollary 2 via \(2\mathrm{TV}^{2}\leq\chi^{2}\).**algorithms for a class of \(s\)-concave densities satisfying a certain isoperimetry condition related to weighted Poincare inequalities. [10] undertook a mean-square analysis of discretization of a specific Ito diffusion that characterizes a class of heavy-tailed densities satisfying a weighted Poincare inequality. [10] and [11] analyzed the complexity of pseudo-marginal MCMC algorithms and the random-walk Metropolis algorithm respectively, under weak Poincare inequalities. As mentioned before, [10] showed lower bounds for the LMC algorithm when the target density satisfies a weak Poincare inequality. [12] and [13] analyzed a transformation based approach for heavy-tailed sampling under conditions closely related to the same functional inequality. This transformation methodology is also used to demonstrate asymptotic exponential ergodicity for other sampling algorithms like the bouncy particle sampler and the zig-zag sampler, in the heavy-tailed settings [14, 15, 16]. These works provide only low-accuracy guarantees for heavy-tailed sampling and do not consider the use of weak Fractional Poincare inequalities.

Recent years have witnessed a significant focus on (strongly) log-concave sampling, leading to an extensive body of work that is challenging to encapsulate succinctly. In the context of (strongly) log-concave or light-tailed distributions, a plethora of non-asymptotic investigations have been conducted on LMC variations, including advanced integrators [13, 12, 14], underdamped LMC [11, 12, 13, 15], and MALA [1, 13, 14, 15]. Outside the realm of log-concavity, the dissipativity assumption, which regulates the growth of the potential, has been used in numerous studies to derive convergence guarantees [16, 17, 18, 19, 20, 21, 22, 23].

While research on upper bounds of sampling algorithms' complexity has advanced considerably, the exploration of lower bounds is still nascent. [12] explored the query complexity of sampling from strongly log-concave distributions in one-dimensional settings. [12] established lower bounds for LMC in sampling from strongly log-concave distributions. [1] presented lower bounds for sampling from strongly log-concave distributions with noisy gradients. [1] focused on lower bounds for estimating normalizing constants of log-concave densities. Contributions by [12] and [20] provide lower bounds in the metropolized algorithm category, including Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [1] contributed to lower bounds in Fisher information for non-log-concave sampling.

## 2 Lower Bounds for Sampling with the Gaussian Oracle

In this section, we focus on **Q1** for both the Langevin diffusion (in continuous time) and the proximal sampler (in discrete time), where both procedures have the target density as their invariant measures. Our results below illustrate the limitation of the Gaussian oracle1 for heavy-tailed sampling in both continuous and discrete time, showing that the phenomenon is not because of the discretization effect, but is inherently related to the use of Gaussian oracles.

Footnote 1: Here, for the sake of unified presentation, we refer the use of Brownian motion in (LD) as Gaussian oracle.

**Langevin diffusion.** We first start with the overdamped Langevin diffusion (LD):

\[\mathrm{d}X_{t}=-\nabla V(X_{t})\mathrm{d}t+\sqrt{2}\mathrm{d}B_{t}.\] (LD)

LD achieves high-accuracy "sampling" in continuous time, i.e. a \(\mathrm{polylog}(1/\varepsilon)\) convergence rate in the light-tailed setting. We make the following dissipativity-type assumption.

**Assumption 1**.: _The target density is given by \(\pi^{X}(x)\propto\exp(-V(x))\), where \(V:\mathbb{R}^{d}\to\mathbb{R}\) satisfies_

\[\forall x\in\mathbb{R}^{d},\quad\frac{(d+\nu_{1})|x|^{2}}{1+|x|^{2}}\leq \langle x,\nabla V(x)\rangle\leq\frac{(d+\nu_{2})|x|^{2}}{1+|x|^{2}}\quad\text {for some }\nu_{2}\geq\nu_{1}\geq 0.\]

**Remark 1**.: _The upper bound on \(\langle x,\nabla V(x)\rangle\) ensures that \(V\) grows at most logarithmically in \(|x|\). Consequently, \(\pi^{X}\) is heavy-tailed and in fact does not satisfy a Poincare inequality. The lower bound on \(\langle x,\nabla V(x)\rangle\) is only needed for deriving the dimension dependency in our guarantees. If one is only interested in the \(\varepsilon\) dependency, this condition can be replaced with \(0\leq\langle x,\nabla V(x)\rangle\)._

A classical example of a density satisfying the above assumption is the generalized Cauchy density with degrees of freedom \(\nu=\nu_{1}=\nu_{2}>0\), where the potential is given by

\[V_{\nu}(x)\coloneqq\frac{d+\nu}{2}\ln(1+|x|^{2}).\] (1)

The following result, proved in Appendix A, provides a lower bound on the performance of LD.

**Theorem 1**.: _Suppose \(\pi^{X}\propto\exp(-V)\) satisfies Assumption 1. Let \(X_{t}\) be the solution of the Langevin diffusion, and \(\mu_{t}\coloneqq\mathrm{Law}(X_{t})\). Then, for any \(\delta>0\),_

\[\mathrm{TV}(\pi^{X},\mu_{t})\geq C_{\nu_{1},\nu_{2}}d^{\frac{\nu_{1}-\nu_{2}}{2 }(1+\delta)}\left(C_{\delta}(\mu_{0})+\kappa_{\delta}t\right)^{-\frac{\nu_{2} (1+\delta)}{2}},\]

_where \(\kappa_{\delta}\coloneqq 1\vee\frac{2}{d+\nu_{2}}\vee\frac{\nu_{5}(1+\delta)}{(d+ \nu_{2})\delta}\), \(C_{\delta}(\mu_{0})\coloneqq\frac{1}{d+\nu_{2}}\mathbb{E}[(1+|X_{0}|^{2})^{ \gamma}]^{1/\gamma}\) with \(\gamma=\kappa_{\delta}(d+\nu_{2})/2\), and \(C_{\nu_{1},\nu_{2}}\) is a constant depending only on \(\nu_{1}\) and \(\nu_{2}\)._

If we assume \(|X_{0}|\leq\mathcal{O}(\sqrt{d})\) for simplicity, then by choosing \(\delta=\frac{2\ln\ln t}{\nu_{2}\ln t}\wedge\frac{2\ln\ln d}{(\nu_{2}-\nu_{1}) \ln d}\), we obtain

\[\mathrm{TV}(\pi^{X},\mu_{t})\geq\tilde{\Omega}_{\nu_{1},\nu_{2}}(d^{\frac{\nu _{1}-\nu_{2}}{2}}t^{-\frac{\nu_{2}}{2}}).\]

Thus, LD requires at least \(T=\tilde{\Omega}_{\nu_{1},\nu_{2}}\big{(}d^{\frac{\nu_{1}-\nu_{2}}{2}}(1/ \varepsilon)^{2/\nu_{2}}\big{)}\) to reach \(\varepsilon\) error in total variation. While this bound may be small in high dimensions when \(\nu_{2}>\nu_{1}\), for the canonical model of Cauchy-type potentials with \(\nu_{2}=\nu_{1}=\nu\), it will be independent of dimension, as stated by the following result. Note that Assumption 1 can also cover a general scaling by replacing \(|x|\) with \(c|x|\) for some constant \(c\), which would introduce a multiplicative factor of \(1/c^{2}\) for the lower bound on \(T\). This is expected as e.g., mixing to the Gibbs potential \(c^{2}|x|^{2}\) can be faster than mixing to \(|x|^{2}\) by a factor of \(1/c^{2}\).

**Corollary 1**.: _Consider the generalized Cauchy density \(\pi^{X}_{\nu}\propto\exp(-V_{\nu})\) where \(V_{\nu}\) is as in (1). Let \(X_{t}\) be the solution of the Langevin diffusion, and \(\mu_{t}\coloneqq\mathrm{Law}(X_{t})\). For simplicity, assume the initialization satisfies \(|X_{0}|\leq\mathcal{O}(\sqrt{d})\). Then, achieving \(\mathrm{TV}(\pi^{X}_{\nu},\mu_{T})\leq\varepsilon\) requires \(T\geq\tilde{\Omega}_{\nu}\big{(}\varepsilon^{-\frac{\varepsilon}{\nu}}\big{)}\)._

The above lower bound implies that LD is a low-accuracy "sampler" for this target density in the sense that it depends polynomially on \(1/\varepsilon\); this dependence gets worse with smaller \(\nu\) as the tails get heavier. It is worth highlighting the gap between the upper bound of [11, Corollary 8], which is \(\tilde{\mathcal{O}}\big{(}1/\varepsilon^{4/\nu}\big{)}\), and the lower bound in Corollary 1.

**Gaussian proximal sampler.** In the remainder of this section, we prove that the Gaussian proximal sampler, described in Algorithm 1, also suffers from a \(\mathrm{poly}(1/\varepsilon)\) rate when the target density is heavy-tailed. In each iteration of Algorithm 1, the first step involves sampling a standard Gaussian random variable \(y_{k}\) centered at the current iterate \(x_{k}\) with variance \(\eta I\); this is a one-step isotropic Brownian random walk. Alternatively, since the Fokker-Planck equation of the standard Brownian motion is the classical heat equation, this step could also be interpreted as an exact simulation of the heat flow; see, for example, [10] and [14]. Specifically, the density of \(y_{k}\) is the solution to the heat flow at time \(\eta\) with the initial condition being the density of \(x_{k}\). The second step is called the restricted Gaussian oracle (RGO) as coined by [17]; under which \((x_{k},y_{k})\) is a reversible Markov chain whose stationary density has \(x\)-marginal \(\pi^{X}\).

**Assumption 2**.: _For some \(\nu_{2}\geq\nu_{1}\geq 0\), the target \(\pi^{X}(x)\propto\exp(-V(x))\) with \(V:\mathbb{R}^{d}\to\mathbb{R}\) satisfies_

\[\forall x\in\mathbb{R}^{d}\quad\frac{(d+\nu_{1})|x|^{2}}{1+|x|^{2}}\leq\langle x,\nabla V(x)\rangle,\quad|\nabla V(x)|\leq\frac{(d+\nu_{2})|x|}{1+|x|^{2}}, \quad\Delta V(x)\leq\frac{(d+\nu_{2})^{2}}{1+|x|^{2}}.\]

The first condition above also appears in Assumption 1 and the second condition implies the upper bound of Assumption 1; thus, the above assumption is stronger. Note that the generalized Cauchy measure (1) satisfies this assumption with \(\nu_{1}=\nu_{2}=\nu\). Under Assumption 2, we state the following lower bound on the Gaussian proximal sampler and defer its proof to Appendix A.

**Theorem 2**.: _Suppose \(\pi^{X}\propto\exp(-V)\) satisfies Assumption 2. Let \(x_{k}\) denote the \(k^{\text{th}}\) iterate of the Gaussian proximal sampler (Algorithm 1) with step \(\eta\) and let \(\rho^{X}_{k}\coloneqq\mathrm{Law}(x_{k})\). Then, for any \(\delta>0\),_

\[\mathrm{TV}(\pi^{X},\rho^{X}_{k})\geq C_{\nu_{1},\nu_{2}}d^{\frac{\nu_{1}-\nu_ {2}}{2}(1+\delta)}\left(C_{\delta}(\mu_{0})+\kappa_{\delta}\eta k\right)^{- \frac{\nu_{2}(1+\delta)}{2}},\]

_where \(\kappa_{\delta}\), \(C_{\delta}(\mu_{0})\), and \(C_{\nu_{1},\nu_{2}}\) are defined in Theorem 1._Above, assuming \(|X_{0}|\leq\mathcal{O}(\sqrt{d})\) with the same choice of \(\delta\) as in Theorem 1 yields \(\mathrm{TV}(\pi_{\nu}^{X},\rho_{k}^{X})\geq\tilde{\Omega}_{\nu_{1},\nu_{2}}\big{(} \frac{\nu_{1}-\nu_{2}}{2}\big{)}\). Note that in order for the RGO step to be efficiently implementable, we need to have a sufficiently small \(\eta\). The state-of-the-art implementation of RGO requires a step size of order \(\eta=\tilde{\mathcal{O}}(1/(Ld^{1/2}))\) when \(V\) has \(L\)-Lipschitz gradients [10]. With this choice of step size, the above lower bound requires at least \(N=\tilde{\Omega}_{\nu_{1},\nu_{2}}\big{(}Ld^{1/2+(\nu_{1}-\nu_{2})/\nu_{2}} (1/\varepsilon)^{2/\nu_{2}}\big{)}\) iterations. The assumptions in Theorem 2 once again cover the canonical examples of generalized Cauchy densities, where we have \(L=d+\nu\), which simplifies the lower bound as follows.

**Corollary 2**.: _Consider the generalized Cauchy density \(\pi_{\nu}^{X}\propto\exp(-V_{\nu})\) where \(V_{\nu}\) is as in (1). Let \(x_{k}\) denote the \(k^{\text{th}}\) iterate of the Gaussian proximal sampler, and define \(\rho_{k}^{X}\coloneqq\mathrm{Law}(x_{k})\), and choose the step size \(\eta=\tilde{\mathcal{O}}(1/(Ld^{1/2}))\). If we assume \(|X_{0}|\leq\mathcal{O}(\sqrt{d})\) for simplicity, then achieving \(\mathrm{TV}(\pi_{\nu}^{X},\rho_{N}^{X})\leq\varepsilon\) requires \(N\geq\tilde{\Omega}_{\nu}\big{(}d^{\frac{3}{2}}\varepsilon^{-\frac{2}{\nu}} \big{)}\) iterations._

We emphasize that the above lower bound is of order \(\mathrm{poly}(1/\varepsilon)\) as advertised. Thus, the RGO-based proximal sampler can only yield a low-accuracy guarantee in this setting.

## 3 Stable Proximal Sampler and the Restricted \(\alpha\)-Stable Oracle

Having characterized the limitations of Gaussian oracles for heavy-tailed sampling, thereby answering **Q1**, in what follows, we will focus on **Q2** and construct proximal samplers based on the \(\alpha\)-stable oracle, and prove that they achieve high-accuracy guarantees when sampling from heavy-tailed targets. First, we provide a basic overview of \(\alpha\)-stable processes and fractional heat flows.

**Isotropic \(\alpha\)-stable process.** For \(t\geq 0\), let \(X_{t}^{(\alpha)}\) be the isotropic stable Levy process in \(\mathbb{R}^{d}\), starting from \(x\in\mathbb{R}^{d}\), with the index of stability \(\alpha\in(0,2]\), defined uniquely via its characteristic function \(\mathbb{E}_{x}e^{i(\xi,X_{t}^{(\alpha)}-x)}=e^{-t|\xi|^{\alpha}}\). When \(\alpha=2\), \(X_{t}^{(2)}\) is a scaled Brownian motion, and when \(0<\alpha<2\), it becomes a pure Levy jump process in \(\mathbb{R}^{d}\). The transition density of \(X_{t}^{(\alpha)}\)is then given by

\[p^{(\alpha)}(t;x,y)=p_{t}^{(\alpha)}(y-x)\quad\text{ with }\quad p_{t}^{( \alpha)}(y)=(2\pi)^{-d}\int_{\mathbb{R}^{d}}\exp(-t|\xi|^{\alpha})e^{-i(\xi,y )}\mathrm{d}\xi,\] (2)

where the second equation above is the inverse Fourier transform of the characteristic function, thus returns the density. The transition kernel and the density in (2) have closed-form expressions for the special cases \(\alpha=1,2\). In particular, when \(\alpha=1\), \(p_{t}^{(1)}\) reduces to a Cauchy density with degrees of freedom \(\nu=1\), i.e. \(p_{t}^{(1)}(y)\propto(|y|^{2}+t^{2})^{-(d+1)/2}\). We finally note that the isotropic stable Levy process \(X_{t}^{(\alpha)}\) displays self-similarity like the Brownian motion; the processes \(X_{at}^{(\alpha)}\) and \(a^{1/\alpha}X_{t}^{(\alpha)}\) have the same distribution. This property is crucial in the development of the stable proximal sampler.

**Fractional heat flow.** The equation \(\partial_{t}u(t,x)=-(-\Delta)^{\alpha/2}u(t,x)\) with the condition \(u(0,x)=u_{0}(x)\) is an extension of the classical heat flow, and is referred to as the fractional heat flow. Here, \(-(-\Delta)^{\alpha/2}\) is the fractional Laplacian operator with \(\alpha\in(0,2]\), which is the infinitesimal generator of the isotropic \(\alpha\)-stable process. For \(\alpha=2\), it reduces to the standard Laplacian operator \(\Delta\).

**Stable proximal sampler.** Let \(\pi(x,y)\) be a joint density such that \(\pi(x,y)\propto\pi^{X}(x)p^{(\alpha)}(\eta;x,y)\), where \(\pi^{X}\) is the target and \(p^{(\alpha)}(\eta;x,y)\) is the transition density of the \(\alpha\)-stable process, introduced in (2). It is easy to verify that (i) the \(X\)-marginal of \(\pi\) is \(\pi^{X}\), (ii) the conditional density of \(Y\) given \(X\) is \(\pi^{Y|X}(\cdot|x)=p^{(\alpha)}(\eta;x,\cdot)\), (iii) the \(Y\)-marginal is \(\pi^{Y}=\pi^{X}\ast p_{\eta}^{(\alpha)}\), i.e. \(\pi^{Y}\) is obtained by evolving \(\pi^{X}\) along the \(\alpha\)-fractional heat flow for time \(\eta\), and (iv) the conditional density of \(X\) given \(Y\) is \(\pi^{X|Y}(\cdot|y)\propto\pi^{X}(\cdot)p^{(\alpha)}(\eta;\cdot,y)\). Based on these, we introduce the following stable oracle.

**Definition 1** (Restricted \(\alpha\)-Stable Oracle).: _Given \(y\in\mathbb{R}^{d}\), an oracle that outputs a random vector distributed according to \(\pi^{X|Y}(\cdot|y)\), is called the Restricted \(\alpha\)-Stable Oracle (R\(\alpha\)SO)._

Note that when \(\alpha=2\), the R\(\alpha\)SO reduces to the RGO of [10]. The Stable Proximal Sampler (Algorithm 2) with parameter \(\alpha\) is initialized at a point \(x_{0}\in\mathbb{R}^{d}\) and performs Gibbs sampling on the joint density \(\pi\). In each iteration, the first step involves sampling an isotropic \(\alpha\)-stable random vector \(y_{k}\) centered at the current iterate \(x_{k}\), which is a one-step isotropic \(\alpha\)-stable random walk. This could also be interpreted as an exact simulation of the fractional heat flow. Indeed, due to the relation between the fractional heat flow and the isotropic stable process, the density of \(y_{k}\) is exactly the solution to the \(\alpha\)-fractional heat flow at time \(\eta\) with the initial condition being the density of \(x_{k}\)When \(\alpha=2\), the first step reduces to an isotropic Brownian random walk and a simulation of the classical heat flow. The second step calls the R\(\alpha\)SO at the point \(y_{k}\).

### Convergence guarantees

We next provide convergence guarantees for the stable proximal sampler in \(\chi^{2}\)-divergence assuming access to the R\(\alpha\)SO. Similar results for a practical implementation are presented in Section 3.2. To proceed, we introduce the fractional Poincare inequality, first introduced in [13] to characterize a class of heavy-tailed densities including the canonical Cauchy class.

**Definition 2** (Fractional Poincare Inequality).: _For \(\vartheta\in(0,2)\), a probability density \(\mu\) satisfies a \(\vartheta\)-fractional Poincare inequality (FPI) if there exists a positive constant \(C_{\text{FPI}(\vartheta)}\) such that for any function \(\phi:\mathbb{R}^{d}\to\mathbb{R}\) in the domain of \(\mathcal{E}_{\mu}^{(\vartheta)}\), we have_

\[\text{Var}_{\mu}(\phi)\leq C_{\text{FPI}(\vartheta)}\mathcal{E}_{\mu}^{( \vartheta)}(\phi).\] (FPI)

_where \(\mathcal{E}_{\mu}^{(\vartheta)}\) is a non-local Dirichlet form associated with \(\mu\) defined as_

\[\mathcal{E}_{\mu}^{(\vartheta)}(\phi):=c_{d,\vartheta}\iint_{\{x\neq y\}} \frac{(\phi(x)-\phi(y))^{2}}{|x-y|^{(d+\vartheta)}}\mathrm{d}x\mu(y)\mathrm{ d}y\quad\text{ with }\quad c_{d,\vartheta}=\frac{2^{\vartheta}\Gamma((d+\vartheta)/2)}{\pi^{d/2}| \Gamma(-\vartheta/2)|}.\]

**Remark 2**.: _FPI is a weaker condition than Assumption 2. In fact, any density satisfying the first 2 conditions in Assumption 2 satisfies \(\vartheta\)-FPI for all \(\vartheta<\nu_{1}\)[13, Theorem 1.1]. In Proposition 2, we show that as \(\vartheta\to 2^{-}\), FPI becomes equivalent to the standard Poincare inequality._

In the sequel, \(\rho_{k}^{X}\) denotes the law of \(x_{k}\), \(\rho_{k}^{Y}\) denotes the law of \(y_{k}\), and \(\rho_{k}=\rho_{k}^{X,Y}\) is the joint law of \((x_{k},y_{k})\). We provide the following convergence guarantee under an FPI, proved in Appendix B.2.

**Theorem 3**.: _Assume that \(\pi^{X}\) satisfies the \(\alpha\)-FPI with parameter \(C_{\text{FPI}(\alpha)}\) for \(\alpha\in(0,2)\). For any step size \(\eta>0\) and initial density \(\rho_{0}^{X}\), the \(k^{\text{th}}\) iterate of Algorithm 2, with parameter \(\alpha\), satisfies_

\[\chi^{2}(\rho_{k}^{X}|\pi^{X})\leq\exp\left(-k\eta\left(C_{\text{FPI}(\alpha) }+\eta\right)^{-1}\right)\chi^{2}(\rho_{0}^{X}|\pi^{X}).\]

As a consequence of Remark 2 and Proposition 2, we recover the result in [10, Theorem 4], by letting \(\alpha\to 2^{-}\). While our results in Theorem 3 are based on Algorithm 2 which requires exact calls to R\(\alpha\)SO, the next result, proved in Appendix B.3, shows that even with an inexact implementation of R\(\alpha\)SO, the error accumulation is at most linear, and Algorithm 2 still converges quickly.

**Proposition 1**.: _Suppose the R\(\alpha\)SO in Algorithm 2 is implemented inexactly, i.e. there exists a positive constant \(\varepsilon_{\mathrm{TV}}\) such that \(\mathrm{TV}(\tilde{\rho}_{k}^{X|Y}(\cdot|y),\rho_{k}^{X|Y}(\cdot|y))\leq \varepsilon_{\mathrm{TV}}\) for all \(y\in\mathbb{R}^{d}\) and \(k\geq 1\), where \(\tilde{\rho}_{k}^{X|Y}(\cdot|y)\) is the density of the inexact R\(\alpha\)SO sample conditioned on \(y\). Let \(\tilde{\rho}_{k}^{X}\) be the density of the output of the \(k^{\text{th}}\) step of Algorithm 2 with the inexact R\(\alpha\)SO and \(\rho_{k}^{X}\) be the density of the output of \(k^{\text{th}}\) step Algorithm 2 with the exact R\(\alpha\)SO. Then, for all \(k\geq 0\),_

\[\mathrm{TV}(\tilde{\rho}_{k}^{X},\rho_{k}^{X})\leq\mathrm{TV}(\tilde{\rho}_{0 }^{X},\rho_{0}^{X})+k\,\varepsilon_{\mathrm{TV}}.\]

_Further, if \(\tilde{\rho}_{0}^{X}=\rho_{0}^{X}\), for any \(K\geq K_{0}\), we get \(\mathrm{TV}(\tilde{\rho}_{k}^{X},\pi^{X})\leq\varepsilon\), if \(\varepsilon_{\mathrm{TV}}\leq\varepsilon/2K\), where the constant \(K_{0}=(1+C_{\text{FPI}(\alpha)}\eta^{-1})\log\left(\chi^{2}(\tilde{\rho}_{0}^ {X}|\pi^{X})/\varepsilon^{2}\right)\) with \(C_{\text{FPI}(\alpha)}\) being the \(\alpha\)-FPI parameter of \(\pi^{X}\)._

### A practical implementation of R\(\alpha\)SO

In the sequel, we introduce a practical implementation of R\(\alpha\)SO when \(\alpha=1\). For this, we consider the case when the target density \(\pi^{X}\propto e^{-V}\) satisfies the \(1\)-FPI with parameter \(C_{\text{FPI}(1)}\). A more thorough implementation of R\(\alpha\)SO for other values of \(\alpha\) will be investigated in future work.

**Assumption 3**.: _There exist constants \(\beta,L>0\) such that for any minimizer \(x^{*}\in\arg\min_{y\in\mathbb{R}^{d}}V(y)\) and for all \(x\in\mathbb{R}^{d}\), \(V\) satisfies \(V(x)-V(x^{*})\leq L|x-x^{*}|^{\beta}\)._Algorithm 3 provides an exact implementation of R\(\alpha\)SO for \(\alpha=1\) via rejection sampling. Inputs to this algorithm are the intermediate points \(y_{k}\) in the stable proximal sampler (Algorithm 2). Note that Algorithm 3 requires a global minimizer of \(V\), which is always assumed to exist, which guarantees that the acceptance probability is non-trivial. It generates proposals with density \(p^{(1)}(\eta;\cdot,y)\) and utilizes that \(p^{(1)}\) is a Cauchy density and Cauchy random vectors can be generated via ratios between a Gaussian random vector and square-root of a \(\chi^{2}\) random variable. Finally, the accept-reject step ensures that the output \(x\) has density \(\pi^{X|Y}(\cdot|y)\propto e^{-V}p^{(1)}(\eta;\cdot,y)\). This makes Algorithm 3 a zeroth-order algorithm requiring only access to function evaluations of \(V\). Under Assumption 3, by choosing a small step-size, we can control the expected number of rejections in Algorithm 3. We now state the iteration complexity of our stable proximal sampler with this R\(\alpha\)SO implementation in the following result, whose proof is provided in Appendix B.3.

**Corollary 3**.: _Assume \(V\) satisfies Assumption 3. If we choose the step-size \(\eta=\Theta(d^{-\frac{1}{2}}L^{-\frac{1}{3}})\), then Algorithm 3 implements the R\(\alpha\)SO with \(\alpha=1\), with the expected number of zeroth-order calls to \(V\) of order \(\mathbb{E}[\exp(L|y_{k}|^{\beta})]\). Further assume \(\pi^{X}\) satisfies \(1\)-FPI with parameter \(C_{\text{FPI}(1)}\). Suppose we run Algorithm 2 with R\(\alpha\)SO implemented for with \(\alpha=1\) by Algorithm 3. Then, to return a sample which is \(\varepsilon\)-close in \(\chi^{2}\)-divergence to the target, the expected number of iterations required by Algorithm 2 is_

\[\mathcal{O}\big{(}C_{\text{FPI}(1)}d^{\frac{1}{2}}L^{\frac{1}{3}}\log(\chi^{2} (\rho_{0}^{X}|\pi^{X})/\varepsilon)\big{)}.\]

Note that the above result provides a high-accuracy guarantee for the implementable version of the stable proximal sampler (Algorithm 3) for a class of heavy-tailed targets, overcoming the fundamental barrier established in Theorem 2 for the Gaussian proximal sampler (i.e., Algorithm 1). A numerical illustration of this improvement is provided in Appendix D by sampling from student-t distributions.

**Remark 3**.: _(1) Finding a global minimizer of the potential \(V\) can be hard, which could be avoided if a lower bound on the potential \(V\) is available; see Appendix B.3. (2) A trivial bound for \(\mathbb{E}[\exp(L|y_{k}|^{\beta})]\) is \(\exp(LM)\) for \(M=\mathbb{E}_{\pi^{X}}[|X|^{\beta}]+\chi^{2}(\rho_{0}^{X}|\pi^{X})\mathbb{E}_{ \pi^{X}}[|X|^{2\beta}]^{\frac{1}{2}}\). Since our main focus is high vs low accuracy samplers, deriving a sharper bound is beyond the scope of the current paper._

### Illustrative examples

To illustrate our results, we now apply the proximal algorithms to sample from Cauchy densities and discuss the complexity of both the ideal sampler (Algorithm 2) in which we can choose any \(\alpha\in(0,2)\) and the implementable version with \(\alpha=1\) (Algorithm 3). For the ideal sampler, we can choose \(\alpha\leq\nu\) for any degrees of freedom \(\nu>0\), and apply Theorem 3 since \(\pi_{\nu}\) satisfies a \(\alpha\)-FPI [20].

**Corollary 4**.: _For any \(\nu>0\), consider the generalized Cauchy target \(\pi_{\nu}\propto\exp(-V_{\nu})\) with \(V_{\nu}\) defined in (1). For the stable proximal sampler with parameter \(\alpha\in(0,2)\) and \(\alpha\leq\nu\) (i.e., Algorithm 2), suppose we set the step-size \(\eta\in(0,1)\) and draw the initial sample from the standard Gaussian density. Then, the number of iterations required by Algorithm 2 to produce an \(\varepsilon\)-accurate sample in \(\chi^{2}\)-divergence is \(\mathcal{O}(C_{\text{FPI}(\alpha)}\eta^{-1}\log(d/\varepsilon))\), where \(C_{\text{FPI}(\alpha)}\) is the \(\alpha\)-FPI parameter of \(\pi_{\nu}\)._

For the implementable sampler, since the parameter \(\alpha\) is fixed to be \(1\), whether a suitable FPI is satisfied or not depends on the degrees of freedom \(\nu\). Specifically, when \(\nu\geq 1\), \(1\)-FPI is satisfied and Corollary 5 applies. When \(\nu\in(0,1)\), on the other hand, \(1\)-FPI is not satisfied. To tackle this issue, we prove convergence guarantees for the proximal sampler under a weak fractional Poincare inequality; the next corollary, proved in Appendix B.4, summarizes these results.

**Corollary 5**.: _For the Cauchy target \(\pi_{\nu}\propto\exp(-V_{\nu})\) where \(V_{\nu}\) is defined in (1), we consider Algorithm 2 with \(\alpha=1\), a standard Gaussian initialization, and R\(\alpha\)SO implemented by Algorithm 3._

1. _When_ \(\nu\geq 1\)_, if we set the step-size_ \(\eta=\Theta\big{(}d^{-\frac{1}{2}}(d+\nu)^{-4}\big{)}\)_, the expected number iterations required by Algorithm_ 2 _to output a sample which is_ \(\varepsilon\)_-close in_ \(\chi^{2}\)_-divergence to the target is of order_ \(\mathcal{O}\big{(}C_{\text{FPI}(1)}d^{\frac{1}{2}}(d+\nu)^{4}\log(d/\varepsilon) \big{)}\)_, where_ \(C_{\text{FPI}(1)}\) _is the_ \(1\)_-FPI parameter of_ \(\pi_{\nu}\)2. _When_ \(\nu\in(0,1)\)_, if we set the step-size_ \(\eta=\Theta\big{(}d^{-\frac{1}{2}}(d+\nu)^{-\frac{1}{\varepsilon}}\big{)}\)_, the expected number of iterations required by Algorithm_ 2_, to output a sample which is_ \(\varepsilon\)_-close in_ \(\chi^{2}\)_-divergence to the target is of order_ \(\tilde{\mathcal{O}}\big{(}\max\big{\{}c^{\frac{1}{2}}d^{\frac{1}{2\nu}+\frac{ 1}{\varepsilon}},cd^{\frac{1}{2}+\frac{1}{\varepsilon}}\varepsilon^{-\frac{1} {\varepsilon}+1}\big{\}}\big{)}\)_, where_ \(c\) _is the positive constant given in (_16_). Here,_ \(\tilde{\mathcal{O}}\) _hides the polylog factors on_ \(d\) _and_ \(1/\varepsilon\)_._

The stable proximal sampler (Algorithm 2) is a high accuracy sampler for the class of generalized Cauchy targets, as long as \(\alpha\leq\nu\), meaning that it achieves log(\(1/\varepsilon\)) iteration complexity. The improvement from poly(\(1/\varepsilon\)) to log(\(1/\varepsilon\)) separates the stable proximal sampler and the Gaussian proximal sampler in the task of heavy-tailed sampling. When we use the rejection-sampling implementation with parameter \(\alpha=1\) (Algorithm 3), iteration complexity goes through a phase transition as the tails get heavier. When the generalized Cauchy density has a finite mean (\(\nu>1\)), we achieve a high-accuracy sampler with log(\(1/\varepsilon\)) iteration complexity. However, without a finite mean (i.e., \(\nu\in(0,1)\)), the algorithm becomes a low-accuracy sampler with poly(\(1/\varepsilon\)) complexity. Even in this low-accuracy regime, the implementable stable proximal sampler outperforms the Gaussian one, as originally highlighted in Table 1. Last, we claim that the poly(\(1/\varepsilon\)) complexity of Algorithms 2 and 3 is not due to a loose analysis, as we show \(\mathrm{poly}(1/\varepsilon)\) lower bounds in the following section.

### Lower bounds for the stable proximal sampler

We now study lower bounds on the stable proximal sampler to sample from the class of target densities satisfying Assumption 2, which includes the generalized Cauchy target. Recall that Assumption 2 implies the FPI used in Theorem 3. The result below, proved in Appendix C, complements Theorem 3, showing the impossibility of achieving \(\log(1/\varepsilon)\) rates for a sufficiently large \(\alpha\).

**Theorem 4**.: _Suppose \(\pi^{X}\propto\exp(-V)\) with \(V\) satisfying Assumption 2 and \(\frac{\nu_{2}(d+\nu_{2})}{d+\nu_{1}}<\alpha\leq 2\). Let \(x_{k}\) denote the \(k^{\text{th}}\) iterate of Algorithm 2 with parameter \(\alpha\) and step size \(\eta\), and let \(\rho_{k}^{X}\coloneqq\mathrm{Law}(x_{k})\). Then for any \(\tau\in\big{(}\frac{\nu_{2}(d+\nu_{2})}{d+\nu_{1}},\alpha\big{)}\), and \(g(d,\nu_{1},\nu_{2},\tau)=\nu_{2}/\{\tau(d+\nu_{1})-\nu_{2}(d+\nu_{2})\}\), we have_

\[\mathrm{TV}(\pi^{X},\rho_{k}^{X})\geq C_{\nu_{1},\nu_{2},\alpha}d^{\frac{\tau (d+\nu_{1})g(d,\nu_{1},\nu_{2},\tau)}{2}}\big{(}\mathbb{E}[(1+|x_{0}|^{2})^{ \frac{\tau}{2}}]+m_{\tau}^{(\alpha)}k^{\frac{\tau}{2}+1}\eta^{\frac{\tau}{ \alpha}}\big{)}^{-(d+\nu_{2})g(d,\nu_{1},\nu_{2},\tau)},\]

_where \(C_{\nu_{1},\nu_{2},\alpha}\) is a constant depending only on \(\nu_{1},\nu_{2},\alpha\), and \(m_{\tau}^{(\alpha)}\) is the \(\tau^{\text{th}}\) absolute moment of the \(\alpha\)-stable random variable with density \(p_{1}^{(\alpha)}\) defined in (2)._

**Remark 4**.: _The parameter \(\tau\) in Theorem 4 can be chosen arbitrarily close to \(\alpha\). Specifically, if we assume \(|X_{0}|\leq\mathcal{O}(\sqrt{d})\), then with the choice of \(\tau=\alpha-\big{(}\frac{\log(\log d)}{\log d}\wedge\frac{\log\log(\eta^{-1})} {\log(\eta^{-1})}\big{)}\), we have_

\[\mathrm{TV}(\pi^{X},\rho_{k}^{X})\geq\tilde{\Omega}_{\nu_{1},\nu_{2},\alpha} \big{(}d^{\frac{\tau(d+\nu_{1})g(d,\nu_{1},\nu_{2},\alpha)}{2}}\big{(}d^{ \alpha}+m_{\tau}^{(\alpha)}k^{\frac{\alpha}{2}+1}\eta\big{)}^{-(d+\nu_{2})g(d,\nu_{1},\nu_{2},\alpha)}\big{)},\]

_where \(\tilde{\Omega}\) hides \(\text{polylog}(d/\eta)\) factors._

The \(\tau^{\text{th}}\) absolute moment of the \(\alpha\)-stable random variable depends on the choice of \(\alpha\) and the dimension \(d\). It is hard to find an explicit formula of \(m_{\tau}^{(\alpha)}\) in general. An explicit formula is only available in some special cases, such as \(\alpha=1,2\). Specializing Theorem 4 for the generalized Cauchy potential (i.e., \(\nu_{1}=\nu_{2}\)) we obtain the following explicit result.

**Corollary 6**.: _Let \(\alpha\in(0,2]\). Suppose \(\pi_{\nu}\propto\exp(-V_{\nu})\) where \(V_{\nu}(x)\) is as in (1) for some \(\nu\in(0,\alpha)\). Let \((x_{k})_{k\geq 0}\) be the output of Algorithm 2 with parameter \(\alpha\) and step-size \(\eta>0\), and \(\rho_{k}^{X}\coloneqq\mathrm{Law}(x_{k})\) for all \(k\geq 0\). Then for any \(\tau\in(\nu,\alpha)\),_

\[\mathrm{TV}(\rho_{k}^{X},\pi_{\nu})\geq C_{\nu,\alpha}d^{\frac{\nu\tau}{2(\tau- \nu)}}\big{(}\mathbb{E}[(1+|x_{0}|^{2})^{\frac{\tau}{2}}]+m_{\tau}^{(\alpha)}k ^{\frac{\tau}{2}+1}\eta^{\frac{\tau}{\alpha}}\big{)}^{-\frac{\nu}{\tau-\nu}}.\]

_where \(m_{\tau}^{(\alpha)}\) is the \(\tau^{\text{th}}\) absolute moment of the \(\alpha\)-stable random variable with density \(p_{1}^{(\alpha)}\) as in (2)._

For the rejection sampling implementation in Algorithm 3, \(\alpha=1\) and \(m_{\tau}^{(1)}=\Theta(d^{\frac{\tau}{\alpha}})\) for all \(\tau<1\) (see Appendix B.1). Notice that to implement the R\(\alpha\)SO in the Stable proximal sampler efficiently, we need a sufficiently small step-size \(\eta\). When the target potential satisfies Assumption 3, i.e. \(V\) is \(\beta\)-Holder continuous with parameter \(L\), we require \(\eta=\Theta(d^{-\frac{1}{2}}L^{-\frac{1}{\alpha}})\) to ensure R\(\alpha\)SO can be implemented with \(\mathcal{O}(1)\) queries. Therefore, if we choose \(\eta=\Theta(d^{-\frac{1}{2}}L^{-\frac{1}{\beta}})\), the minimum number of iterations we need to get an \(\varepsilon\)-error in \(\mathrm{TV}\) is

\[\Omega_{\nu,\tau}\Big{(}\varepsilon^{-\frac{2(\tau-\nu)}{(2+\tau)\nu}}d^{\frac{ \tau}{2+\tau}}L^{\frac{2\tau}{\beta(2+\tau)}}\Big{)}.\]For the generalized Cauchy potential with \(\nu\in(0,1)\), we have \(\beta=\nu/4\) and \(L=(d+\nu)/\nu\), which leads to the following corollary.

**Corollary 7**.: _Suppose \(\pi_{\nu}^{X}\propto\exp(-V_{\nu})\) is the generalized Cauchy density with \(\nu\in(0,1)\). Let \(x_{k}\) denote the \(k\)-th iterate of the stable proximal sampler with \(\alpha=1\) (Algorithm 3), and \(\rho_{k}^{X}\coloneqq\mathrm{Law}(x_{k})\). If we choose the step size \(\eta=\Theta(L^{-\frac{4}{\nu}}d^{-\frac{1}{2}})\) where \(L=\frac{d+\nu}{\nu}\) is the \(\nu/4\)-Holder constant of \(V_{\nu}\), and assume, for simplicity, \(|x_{0}|\leq\mathcal{O}(\sqrt{d})\), then, \(\mathrm{TV}(\pi_{\nu}^{X},\rho_{N}^{X})\leq\varepsilon\) requires \(N\geq\Omega_{\nu,\tau}\big{(}d^{\frac{+k\pi/\nu}{2+\tau}}\varepsilon^{-\frac{ 2(\tau-\nu)}{\nu(2+\tau)}}\big{)}\), for any \(\tau\in(\nu,1)\). Further, by choosing \(\tau=\max(\nu,1-\frac{\log(\log(d/\varepsilon))}{\log(d/\varepsilon)})\), we obtain_

\[N\geq\tilde{\Omega}_{\nu}\Big{(}d^{\frac{\nu+8}{3\nu}}\varepsilon^{-\frac{2(1 -\nu)}{3\nu}}\Big{)},\quad\text{in order for}\quad\mathrm{TV}(\pi_{\nu}^{X},\rho_{N}^{X}) \leq\varepsilon.\]

The above result shows that when implementing the R\(\alpha\)SO in Algorithm 2 with Algorithm 3, to sample from generalized Cauchy targets with \(\nu\in(0,1)\), we can at best have an iteration complexity of order \(\text{poly}(1/\varepsilon)\), matching the upper bounds in Corollary 5 up to certain factors.

## 4 Overview of Proof Techniques

**Lower bounds.** We build on the techniques developed in [1]. Let \(\mu_{t}\) denotes the law of LD along its trajectory. To proceed, we need some \(G:\mathbb{R}^{d}\to\mathbb{R}\) for which we can upper bound \(\mu_{t}(G)\coloneqq\int G\mathrm{d}\mu_{t}\), and some \(f:\mathbb{R}^{d}\to\mathbb{R}\) that satisfies \(\pi^{X}(G\geq y)\geq f(y)\) for all \(y\in\mathbb{R}_{+}\). After finding the candidates \(G\) and \(f\), Lemma 1 in Appendix A guarantees \(\mathrm{TV}(\pi^{X},\mu_{t})\geq\sup_{y\in\mathbb{R}_{+}}f(y)-\mu_{t}(G)/y\). This technique relies on choosing \(G\) such that it has heavy tails under \(\pi^{X}\) leading to a large \(f(y)\), while having light tails along the trajectory, thus small \(\mu_{t}(G)\). By picking \(G=\exp(\kappa V)\) with \(\kappa\geq 1\), one can immediately observe that \(\pi^{X}(G)=\infty\), thus \(G\) indeed has heavy tails under \(\pi^{X}\).

To control \(\mu_{t}(G)\) along the trajectory, one can use the generator of LD to bound \(\partial_{t}\mu_{t}(G)\). Recall the generator of LD, \(\mathcal{L}_{\mathrm{LD}}(\cdot)=\Delta(\cdot)-\langle\nabla V,\nabla\cdot\rangle\). Therefore, with a choice of \(G=\exp(\kappa V)\), controlling \(\partial_{t}\mu_{t}(G)\) requires bounding the first and second derivatives of \(V\). To avoid making extra assumptions for \(V\) in the analysis of LD, we instead construct \(G\) based on a surrogate potential \(\tilde{V}(x)=\frac{d+\mu_{t}}{2}\ln(1+|x|^{2})\), which is an upper bound to the potential \(V\). We then estimate \(f\) based on this surrogate potential in Lemma 2, and control the growth of \(\mu_{t}(G)\) in Lemma 3. Combined with Lemma 1, this leads to the proof of Theorem 1, with the details provided in Appendix A.

For the Gaussian proximal sampler, bounding \(\rho_{k}^{X}(G)\) requires controlling the expectation of \(G\) along the forward and backward heat flow. For the particular choice of \(G=\exp(\kappa V)\), we show in Lemma 4 that the growth of \(\rho_{k}^{X}(G)\) can be controlled only by considering a forward heat flow with the corresponding generator \(\mathcal{L}_{\mathrm{HF}}=\frac{1}{2}\Delta\). Therefore, given additional estimates on the second derivatives of \(V\), we bound the growth of \(\rho_{k}^{X}(G)\) in Lemma 5. Once this bound is achieved, we can invoke Lemma 1 to finish the proof of Theorem 2.

**Upper bounds.** Our upper bound analysis builds on that by [1] in the specific ways discussed next. We consider the change in \(\chi^{2}\) divergence when we apply the two operations to the law \(\rho_{k}^{X}\) to the iterates and the target \(\pi^{X}\colon(i)\) evolving the two densities along the \(\alpha\)-fractional heat flow for time \(\eta\) and \((ii)\) applying the R\(\alpha\)SO to the resulting densities. For the step (i), it is required to show that the solution along the fractional heat flow of the stable proximal sampler at any time, satisfies FPI. To show this, \((a)\) the convolution property of the FPI is proved in Lemma 6, and \((b)\) the FPI parameter for the stable process follows from [1, Theorem 23]. In Proposition 3, it is then shown that the \(\chi^{2}\) divergence decays exponentially fast along the fractional heat flow under the assumption of FPI. The aforementioned results enable us to prove the exponential decay of \(\chi^{2}\) divergence along the fractional heat flow under FPI in Proposition 3. To deal with the step (ii) above, we use the data processing inequality; see Proposition 3. These two steps together, enable us to derive the stated upper bounds for the stable proximal sampler.

## 5 Discussion

We showed the limitations of Gaussian proximal samplers for high-accuracy heavy-tailed sampling, and proposed and analyzed stable proximal samplers, establishing that they are indeed high-accuracy algorithms. We now list a few important limitations and problems for future research: (i) It is important to develop efficiently implementable versions of the stable proximal sampler for all values of \(\alpha\in(0,2)\), and characterize their complexity in terms of problem parameters, (ii) Gaussian proximal samplers can be interpreted as a proximal point method for approximating the entropic regularized Wasserstein gradient flow of the KL objective [13]. This leads to the question, _can we provide a variational interepretation of the stable proximal sampler?_ A potential approach is to leverage the results by [15] on gradient flow interpretation of jump processes corresponding to the fractional heat equation, (iii) It is possible to use a non-standard Ito process in the proximal sampler (in place of the \(\alpha\)-stable diffusion); see, for example, [12, 14, 15]. With this modification, it is interesting to examine the rates under weighted Poincare inequalities that also characterize heavy-tailed densities. There are two difficulties to overcome here: \((a)\) How to generate an exact non-standard Ito process? \((b)\) How to implement the corresponding Restricted non-standard Gaussian Oracle, which requires the zeroth order information of the transition density of the Ito process? In certain cases, non-standard Ito diffusion can be interpreted as a Brownian motion on an embedded sub-manifold; thus, the approach in [16] might be useful.

### Acknowledgements

KB was supported in part by NSF grants DMS-2053918 and DMS-2413426.

## References

* [ALPW22] Christophe Andrieu, Anthony Lee, Sam Power, and Andi Q Wang, _Comparison of Markov chains via weak Poincare inequalities with application to pseudo-marginal MCMC_, The Annals of Statistics **50** (2022), no. 6, 3592-3618.
* [ALPW23], _Weak Poincare Inequalities for Markov chains: Theory and Applications_, arXiv preprint arXiv:2312.11689 (2023).
* [App09] David Applebaum, _Levy processes and stochastic calculus_, Cambridge university press, 2009.
* [BCE\({}^{+}\)22] Krishnakumar Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang, _Towards a theory of non-log-concave sampling: First-order stationarity guarantees for Langevin Monte Carlo_, Conference on Learning Theory, PMLR, 2022, pp. 2896-2923.
* [BHJ08] Krzysztof Bogdan, Wolfhard Hansen, and Tomasz Jakubowski, _Time-dependent Schrodinger perturbations of transition densities_, Studia Mathematica **189** (2008), no. 3, 235-254.
* [BRZ19] Joris Bierkens, Gareth O Roberts, and Pierre-Andre Zitt, _Ergodicity of the zigzag process_, The Annals of Applied Probability **29** (2019), no. 4, 2266-2301.
* [BZ17] Maria-Florina F Balcan and Hongyang Zhang, _Sample and computationally efficient learning algorithms under \(s\)-concave distributions_, Advances in Neural Information Processing Systems **30** (2017).
* [CBL22] Niladri S Chatterji, Peter L Bartlett, and Philip M Long, _Oracle lower bounds for stochastic gradient sampling algorithms_, Bernoulli **28** (2022), no. 2, 1074-1092.
* [CCBJ18] Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan, _Underdamped Langevin MCMC: A non-asymptotic analysis_, Conference on learning theory, PMLR, 2018, pp. 300-323.
* [CCSW22] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono, _Improved analysis for a proximal algorithm for sampling_, Conference on Learning Theory, PMLR, 2022, pp. 2984-3014.
* [CDV09] Karthekeyan Chandrasekaran, Amit Deshpande, and Santosh Vempala, _Sampling s-concave functions: The limit of convexity based isoperimetry_, International Workshop on Approximation Algorithms for Combinatorial Optimization, Springer, 2009, pp. 420-433.
* [CG03] Eric A Carlen and Wilfrid Gangbo, _Constrained steepest descent in the 2-Wasserstein metric_, Annals of mathematics (2003), 807-846.

* [CG23] Yuansi Chen and Khashayar Gatmiry, _A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry_, arXiv preprint arXiv:2304.04095 (2023).
* [CGL\({}^{+}\)22] Sinho Chewi, Patrik R Gerber, Chen Lu, Thibaut Le Gouic, and Philippe Rigollet, _The query complexity of sampling from strongly log-concave distributions in one dimension_, Proceedings of Thirty Fifth Conference on Learning Theory, vol. 178, PMLR, 2022, pp. 2041-2059.
* [CGLL22] Sinho Chewi, Patrik Gerber, Holden Lee, and Chen Lu, _Fisher information lower bounds for sampling_, arXiv preprint arXiv:2210.02482 (2022).
* [Cha04] Djalil Chafai, _Entropies, convexity, and functional inequalities, on \(\Phi\)-entropies and \(\Phi\)-sobolev inequalities_, Journal of Mathematics of Kyoto University **44** (2004), no. 2, 325-363.
* [CLA\({}^{+}\)21] Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic, and Philippe Rigollet, _Optimal dimension dependence of the Metropolis-Adjusted Langevin Algorithm_, Conference on Learning Theory, PMLR, 2021, pp. 1260-1300.
* [CLW23] Yu Cao, Jianfeng Lu, and Lihan Wang, _On explicit \(L_{2}\)-convergence rate estimate for underdamped Langevin dynamics_, Archive for Rational Mechanics and Analysis **247** (2023), no. 5, 90.
* [DBCD19] George Deligiannidis, Alexandre Bouchard-Cote, and Arnaud Doucet, _Exponential ergodicity of the Bouncy Particle Sampler_, Annals of Statistics **47** (2019), no. 3.
* [DCWY19] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu, _Log-concave sampling: Metropolis-Hastings algorithms are fast_, Journal of Machine Learning Research **20** (2019), no. 183, 1-42.
* [DGM20] Alain Durmus, Arnaud Guillin, and Pierre Monmarche, _Geometric ergodicity of the Bouncy Particle Sampler_, Annals of applied probability **30** (2020), no. 5, 2069-2098.
* [DKTZ20] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis, _Learning halfspaces with Massart noise under structured distributions_, Conference on Learning Theory, PMLR, 2020, pp. 1486-1513.
* [DM17] Alain Durmus and Eric Moulines, _Nonasymptotic convergence analysis for the unadjusted Langevin algorithm_, The Annals of Applied Probability **27** (2017), no. 3, 1551-1587 (en).
* [DRD20] Arnak S Dalalyan and Lionel Riou-Durand, _On sampling from a log-concave density using kinetic Langevin diffusions_, Bernoulli **26** (2020), no. 3, 1956-1988.
* [EGZ19] Andreas Eberle, Arnaud Guillin, and Raphael Zimmer, _Couplings and quantitative contraction rates for Langevin dynamics_, The Annals of Probability **47** (2019), no. 4, 1982-2010.
* [EH21] Murat A Erdogdu and Rasa Hosseinzadeh, _On the convergence of Langevin Monte Carlo: The interplay between tail growth and smoothness_, Conference on Learning Theory, PMLR, 2021, pp. 1776-1822.
* [EHZ22] Murat A Erdogdu, Rasa Hosseinzadeh, and Shunshi Zhang, _Convergence of Langevin Monte Carlo in chi-squared and Renyi divergence_, International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 8151-8175.
* [EMS18] Murat A Erdogdu, Lester Mackey, and Ohad Shamir, _Global non-convex optimization with discretized diffusions_, Advances in Neural Information Processing Systems **31** (2018).
* [Erb14] Matthias Erbar, _Gradient flows of the entropy for jump processes_, Annales de l'IHP Probabilites et statistiques, vol. 50, 2014, pp. 920-945.

* [FYC23] Jiaojiao Fan, Bo Yuan, and Yongxin Chen, _Improved dimension dependence of a proximal algorithm for sampling_, arXiv preprint arXiv:2302.10081 (2023).
* [GB09] Alan Genz and Frank Bretz, _Computation of multivariate normal and t-probabilities_, vol. 195, Springer Science & Business Media, 2009.
* [GBH04] Alan Genz, Frank Bretz, and Yosef Hochberg, _Approximations to multivariate \(t\) integrals with application to multiple comparison procedures_, Recent Developments in Multiple Comparison Procedures, Institute of Mathematical Statistics, 2004, pp. 24-32.
* [GJPS08] Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su, _A weakly informative default prior distribution for logistic and other regression models_, The annals of applied statistics **2** (2008), no. 4, 1360-1383.
* [GLL20] Rong Ge, Holden Lee, and Jianfeng Lu, _Estimating normalizing constants for log-concave distributions: Algorithms and lower bounds_, Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, 2020, pp. 579-586.
* [GLL\({}^{+}\)23] Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, and Kevin Tian, _Algorithmic aspects of the log-Laplace transform and a non-Euclidean proximal sampler_, arXiv preprint arXiv:2302.06085 (2023).
* [GLM18] Joycee Ghosh, Yingbo Li, and Robin Mitra, _On the use of Cauchy prior distributions for Bayesian logistic regression_, Bayesian Analysis **13** (2018), no. 2, 359-383.
* [Hai10] Martin Hairer, _Convergence of Markov processes_, Lecture notes (2010).
* [HBE20] Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu, _On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method_, Advances in Neural Information Processing Systems **33** (2020), 7366-7376.
* [HBE24], _An analysis of Transformed Unadjusted Langevin Algorithm for Heavy-tailed Sampling_, IEEE Transactions on Information Theory (2024).
* [HFBE24] Ye He, Tyler Farghly, Krishnakumar Balasubramanian, and Murat A Erdogdu, _Mean-square analysis of discretized Ito diffusions for heavy-tailed sampling_, Journal of Machine Learning Research (to appear) (2024).
* [HMW21] Lu-Jing Huang, Mateusz B Majka, and Jian Wang, _Approximation of heavy-tailed distributions via stable-driven SDEs_, Bernoulli **27** (2021), no. 3, 2040-2068.
* [JG12] Leif T Johnson and Charles J Geyer, _Variable transformation to obtain geometric ergodicity in the Random-Walk Metropolis algorithm_, The Annals of Statistics **40** (2012), no. 6, 3050-3076.
* [JR07] Soren Jarner and Gareth Roberts, _Convergence of heavy-tailed Monte Carlo Markov Chain algorithms_, Scandinavian Journal of Statistics **34** (2007), no. 4, 781-815.
* [Kam18] Kengo Kamatani, _Efficient strategy for the Markov chain Monte Carlo in high-dimension with heavy-tailed target probability distribution_, Bernoulli **24** (2018), no. 4B, 3711-3750.
* [KN04] Samuel Kotz and Saralees Nadarajah, _Multivariate t-distributions and their applications_, Cambridge University Press, 2004.
* [Kwa17] Mateusz Kwasnicki, _Ten equivalent definitions of the fractional Laplace operator_, Fractional Calculus and Applied Analysis **20** (2017), no. 1, 7-51.
* [LST20] Yin Tat Lee, Ruoqi Shen, and Kevin Tian, _Logsmooth gradient concentration and tighter runtimes for Metropolized Hamiltonian Monte Carlo_, Conference on learning theory, PMLR, 2020, pp. 2565-2597.
* [LST21a], _Lower bounds on Metropolized sampling methods for well-conditioned distributions_, Advances in Neural Information Processing Systems **34** (2021), 18812-18824.

* [LST21b], _Structured logconcave sampling with a Restricted Gaussian Oracle_, Conference on Learning Theory, PMLR, 2021, pp. 2993-3050.
* [LWME19] Xuechen Li, Yi Wu, Lester Mackey, and Murat A Erdogdu, _Stochastic Runge-Kutta accelerates Langevin Monte Carlo and beyond_, Advances in neural information processing systems **32** (2019).
* [LZT22] Ruilin Li, Hongyuan Zha, and Molei Tao, _Sqrt(d) Dimension Dependence of Langevin Monte Carlo_, The International Conference on Learning Representations, 2022.
* [MFWB22] Wenlong Mou, Nicolas Flammarion, Martin J Wainwright, and Peter L Bartlett, _Improved bounds for discretization of Langevin diffusions: Near-optimal rates without convexity_, Bernoulli **28** (2022), no. 3, 1577-1601.
* [MHFH\({}^{+}\)23] Alireza Mousavi-Hosseini, Tyler K. Farghly, Ye He, Krishna Balasubramanian, and Murat A. Erdogdu, _Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincare Inequality_, Proceedings of Thirty Sixth Conference on Learning Theory, vol. 195, 2023, pp. 1-35.
* [NoL20] John P Nolan, _Univariate stable distributions_, Springer, 2020.
* [NSR19] Than Huy Nguyen, Umut Simsekli, and Gael Richard, _Non-asymptotic analysis of Fractional Langevin Monte Carlo for non-convex optimization_, International Conference on Machine Learning, 2019, pp. 4810-4819.
* [PBEM23] Mathieu Le Provost, Ricardo Baptista, Jeff D Eldredge, and Youssef Marzouk, _An adaptive ensemble filter for heavy-tailed distributions: Tuning-free inflation and localization_, arXiv preprint arXiv:2310.08741 (2023).
* [QM16] Di Qi and Andrew J Majda, _Predicting fat-tailed intermittent probability distributions in passive scalar turbulence with imperfect models through empirical information theory_, Communications in Mathematical Sciences **14** (2016), no. 6, 1687-1722.
* [RRT17] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky, _Non-convex learning via stochastic gradient Langevin dynamics: A nonasymptotic analysis_, Conference on Learning Theory, PMLR, 2017, pp. 1674-1703.
* [SL19] Ruoqi Shen and Yin Tat Lee, _The randomized midpoint method for log-concave sampling_, Advances in Neural Information Processing Systems **32** (2019).
* [SP15] Prashant D Sardeshmukh and Cecile Penland, _Understanding the distinctively skewed and heavy tailed character of atmospheric and oceanic probability distributions_, Chaos: An Interdisciplinary Journal of Nonlinear Science **25** (2015), no. 3.
* [SZTG20] Umut Simsekli, Lingjiong Zhu, Yee Whye Teh, and Mert Gurbuzbalaban, _Fractional underdamped Langevin dynamics: Retargeting SGD with momentum under heavy-tailed gradient noise_, International Conference on Machine Learning, 2020, pp. 8970-8980.
* [Wib18] Andre Wibisono, _Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem_, Conference on Learning Theory, PMLR, 2018, pp. 2093-3027.
* [WSC22a] Keru Wu, Scott Schmidler, and Yuansi Chen, _Minimax mixing time of the Metropolis-adjusted Langevin algorithm for log-concave sampling_, The Journal of Machine Learning Research **23** (2022), no. 1, 12348-12410.
* [WSC22b], _Minimax Mixing Time of the Metropolis-Adjusted Langevin Algorithm for Log-Concave Sampling_, Journal of Machine Learning Research **23** (2022), no. 270, 1-63.
* [WW15] Feng-Yu Wang and Jian Wang, _Functional inequalities for stable-like Dirichlet forms_, Journal of Theoretical Probability **28** (2015), no. 2, 423-448.

* [YLR22] Jun Yang, Krzysztof Latuszynski, and Gareth Roberts, _Stereographic Markov Chain Monte Carlo_, arXiv preprint arXiv:2205.12112 (2022).
* [ZZ23] Xiaolong Zhang and Xicheng Zhang, _Ergodicity of supercritical SDEs driven by \(\alpha\)-stable processes and heavy-tailed sampling_, Bernoulli **29** (2023), no. 3, 1933-1958.

Lower Bound Proofs for the Langevin Diffusion and the Gaussian Proximal Sampler

While research on upper bounds of sampling algorithms' complexity has advanced considerably, the exploration of lower bounds is still nascent. [12] explored the query complexity of sampling from strongly log-concave distributions in one-dimensional settings. [13] established lower bounds for LMC in sampling from strongly log-concave distributions. [12] presented lower bounds for sampling from strongly log-concave distributions with noisy gradients. [12] focused on lower bounds for estimating normalizing constants of log-concave densities. Contributions by [13] and [14] provide lower bounds in the metropolized algorithm category, including Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [12] contributed to lower bounds in Fisher information for non-log-concave sampling. In what follows, we take a different approach and rely on the arguments developed in [15].

We begin by stating the following result which drives our lower bound strategy.

**Lemma 1** ([15, Theorem 5.1]).: _Suppose \(\mu\) and \(\nu\) are probability measures on \(\mathbb{R}^{d}\). Consider some \(G:\mathbb{R}^{d}\to\mathbb{R}_{+}\) and \(f:\mathbb{R}_{+}\to\mathbb{R}_{+}\) satisfying \(\mu(G\geq y)\geq f(y)\) for all \(y\in\mathbb{R}_{+}\). Then,_

\[\mathrm{TV}(\mu,\nu)\geq\sup_{y\in\mathbb{R}_{+}}f(y)-\frac{\int G\mathrm{d} \nu}{y}.\]

_In particular, suppose \(\mathrm{Id}\cdot f:\mathbb{R}_{+}\ni y\mapsto yf(y)\in\mathbb{R}_{+}\) is a bijection, then_

\[\mathrm{TV}(\mu,\nu)\geq\frac{1}{2}f\Big{(}(\mathrm{Id}\cdot f)^{-1}\big{(}2 m\big{)}\Big{)},\]

_for any \(m\geq\int G\mathrm{d}\nu\)._

Proof.: By the definition of total variation and Markov's inequality, for any \(y>0\)

\[\mathrm{TV}(\mu,\nu)\geq\mu(G\geq y)-\nu(G\geq y)\geq f(y)-\frac{\int G \mathrm{d}\nu}{y}.\]

When \(\mathrm{Id}\cdot f\) is invertible, choosing \(y=(\mathrm{Id}\cdot f)^{-1}(2m)\) implies \(yf(y)=2m\) and yields the desired result. 

To apply Lemma 1 when the target density satisfies Assumption 1, we need to establish tail lower bounds for this density, which we do so via the following lemma. In the following, let \(\omega_{d}:=\frac{\pi^{d/2}}{\Gamma((d+2)/2)}\) denote the volume of the unit \(d\)-ball.

**Lemma 2**.: _Suppose \(\pi^{X}(x)\propto\exp(-V(x))\) satisfies Assumption 1. Then, for all \(R>0\),_

\[\pi^{X}(|x|\geq R)\geq\frac{2de^{-\nu_{1}/d}}{(d+\nu_{1})\Gamma(\nu_{1}/2)}(d/ 2)^{\nu_{1}/2}(1+R^{-2})^{-(d+\nu_{2})/2}R^{-\nu_{2}}.\]

_When focusing on dependence on \(R\) and \(d\), we obtain,_

\[\pi^{X}(|x|\geq R)\geq C_{\nu_{1}}d^{\nu_{1}/2}(1+R^{-2})^{-(d+\nu_{2})/2}R^{- \nu_{2}},\]

_where \(C_{\nu_{1}}=\frac{2^{1-\nu_{1}/2}e^{-\nu_{1}}}{(1+\nu_{1})\Gamma(\nu_{1}/2)}\)._

Proof.: Without loss of generality assume \(V(0)=0\). Via Assumption 1, we have the estimates for \(V\),

\[V(x)=\int_{t=0}^{1}\langle x,\nabla V(tx)\rangle\mathrm{d}t\leq(d+\nu)\int_{t =0}^{1}\frac{t|x|^{2}\mathrm{d}t}{1+|tx|^{2}}=\frac{d+\nu_{2}}{2}\ln(1+|x|^{2}),\]

and similarly

\[V(x)\geq\frac{d+\nu_{1}}{2}\ln(1+|x|^{2}).\]Consequently, using the spherical coordinates,

\[\pi^{X}(|x|\geq R) \geq\frac{1}{Z}\int_{|x|\geq R}(1+|x|^{2})^{-(d+\nu_{2})/2}\mathrm{d}x\] \[=\frac{d\omega_{d}}{Z}\int_{r\geq R}(1+r^{2})^{-(d+\nu_{2})/2}r^{ d-1}\mathrm{d}r\] \[\geq\frac{d\omega_{d}(1+R^{-2})^{-(d+\nu_{2})/2}}{Z}\int_{r\geq R }r^{-\nu_{2}-1}\mathrm{d}r\] \[=\frac{d\omega_{d}(1+R^{-2})^{-(d+\nu_{2})/2}}{Z_{d}}R^{-\nu_{2}}.\]

Next, using the lower bound established on \(V\) and spherical coordinates, we obtain,

\[Z \leq\int_{\mathbb{R}^{d}}(1+|x|^{2})^{-(d+\nu_{1})/2}\mathrm{d}x\] \[=d\omega_{d}\int_{0}^{\infty}(1+r^{2})^{-(d+\nu_{1})/2}r^{d-1} \mathrm{d}r\] \[=\frac{1}{2}d\omega_{d}\int_{0}^{\infty}u^{\nu_{1}/2-1}(1-u)^{d/ 2-1}\mathrm{d}u\] \[=\frac{1}{2}d\omega_{d}\mathrm{B}(\nu_{1}/2,d/2)\] \[=\frac{d\omega_{d}\Gamma(\nu_{1}/2)\Gamma(d/2)}{2\Gamma((d+\nu_{ 1})/2)},\]

where \(\mathrm{B}\) denotes the beta function. Plugging back into our tail lower bound, we obtain,

\[\pi^{X}(|x|\geq R)\geq\frac{2\Gamma((d+\nu_{1})/2)}{\Gamma(\nu_{1}/2)\Gamma(d/ 2)}(1+R^{-2})^{-(d+\nu_{2})/2}R^{-\nu_{2}}.\]

Moreover, by [13, Lemma 32] we have

\[\frac{\Gamma((d+\nu_{1})/2)}{\Gamma(d/2)}=\frac{d}{d+\nu_{1}}\frac{\Gamma((d+ \nu_{1}+2)/2)}{\Gamma((d+2)/2)}\geq\frac{2de^{-\nu_{1}/d}}{d+\nu_{1}}(d/2)^{ \nu_{1}/2},\]

which completes the proof. 

Another element of Lemma 1 is controlling the growth of \(\mathbb{E}[G(X_{t})]\) throughout the process. The following lemma achieves such control under the Langevin diffusion.

**Lemma 3**.: _Suppose \((X_{t})_{t\geq 0}\) is the solution to the Langevin diffusion starting at \(X_{0}\) with the corresponding potential \(V(x)\) satisfying Assumption 1. Let \(G(x)=\exp(\kappa\tilde{V}(x))\) where \(\tilde{V}(x)=\frac{d+\nu_{2}}{2}\ln(1+|x|^{2})\) and \(\kappa\geq\frac{2}{d+\nu_{2}}\lor 1\). Then,_

\[\mathbb{E}[G(X_{t})]\leq\left(\mathbb{E}[G(X_{0})]^{\frac{2}{\kappa(d+\nu_{2} )}}+4\kappa(d+\nu_{2})t\right)^{\frac{\kappa(d+\nu_{2})}{2}}.\]

Proof.: Recall the generator of the Langevin diffusion \(\mathcal{L}(\cdot)=\Delta\cdot-\langle\nabla V,\nabla\cdot\rangle\). Then,

\[\frac{\mathrm{d}\mathbb{E}[G(X_{t})]}{\mathrm{d}t} =\mathbb{E}[\mathcal{L}G(X_{t})]\] \[=\kappa\mathbb{E}\left[\left(\kappa|\nabla\tilde{V}|^{2}+\Delta \tilde{V}-\langle\nabla\tilde{V},\nabla V\rangle\right)G\right]\] \[\leq\kappa\mathbb{E}\left[\left(\kappa|\nabla\tilde{V}|^{2}+ \Delta\tilde{V}\right)G\right]\qquad\text{(Assumption \ref{eq:Langevin diffusion})}\] \[\leq 2\kappa^{2}(d+\nu_{2})^{2}\mathbb{E}\left[\frac{G(X_{t})}{1+| X_{t}|^{2}}\right]\] \[=2\kappa^{2}(d+\nu_{2})^{2}\mathbb{E}\left[G(X_{t})^{1-\frac{2}{ \kappa(d+\nu_{2})}}\right]\] \[\leq 2\kappa^{2}(d+\nu_{2})^{2}\mathbb{E}[G(X_{t})]^{1-\frac{2}{ \kappa(d+\nu_{2})}}\qquad\text{(Jensen's Inequality)}.\]

Integrating the above inequality completes the proof.

With the above lemmas in hand, we are ready to present the proof of Theorem 1.

Proof of Theorem 1.: To apply Lemma 1 we choose \(G(x)=\exp(\kappa\tilde{V}(x))\) where \(\tilde{V}(x)=\frac{d+\nu_{2}}{2}\ln(1+|x|^{2})\) with \(\kappa\geq 1\vee\frac{2}{d+\nu_{2}}\). By Lemma 2 we have

\[\pi^{X}(G(x)\geq y)\geq\pi^{X}\left(|x|\geq y^{\frac{1}{\kappa(d+\nu_{2})}} \right)\geq C_{\nu_{1}}d^{\nu_{1}/2}\left(1+y^{\frac{-2}{\kappa(d+\nu_{2})}} \right)^{-(d+\nu_{2})/2}y^{\frac{-\nu_{2}}{\kappa(d+\nu_{2})}}.\]

Moreover, define

\[g(t)\coloneqq\left(g(0)^{\frac{2}{\kappa(d+\nu_{2})}}+4\kappa(d+\nu_{2})t \right)^{\frac{\kappa(d+\nu_{2})}{2}},\]

with \(g(0)\coloneqq\mathbb{E}[G(X_{0})]\). Then by Lemma 3 we have \(\mathbb{E}[G(X_{t})]\leq g(t)\) and we can invoke Lemma 1 to obtain

\[\mathrm{TV}(\pi^{X},\mu_{t}) \geq\sup_{y\in\mathbb{R}_{+}}C_{\nu_{1}}d^{\nu_{1}/2}\left(1+y^{ \frac{-2}{\kappa(d+\nu_{2})}}\right)^{-(d+\nu_{2})/2}y^{\frac{-\nu_{2}}{ \kappa(d+\nu_{2})}}-\frac{g(t)}{y}.\] \[\geq\sup_{y\in\mathbb{R}_{+}}C_{\nu_{1}}d^{\nu_{1}/2}\exp\left(- \frac{(d+\nu_{2})y^{\frac{-2}{\kappa(d+\nu_{2})}}}{2}\right)y^{\frac{-\nu_{2}} {\kappa(d+\nu_{2})}}-\frac{g(t\lor 1)}{y},\]

where we used the fact that \(1+x\leq e^{x}\) for all \(x\in\mathbb{R}\) and \(g(t)\) is non-decreasing in \(t\). Choose

\[y^{*}\coloneqq C^{\prime}_{\nu_{1},\nu_{2}}\left(\frac{g(t\lor 1)}{d^{\nu_{1} /2}}\right)^{\frac{\kappa(d+\nu_{2})}{\kappa(d+\nu_{2})-\nu_{2}}},\]

for a sufficiently large constant \(C^{\prime}_{\nu_{1},\nu_{2}}\geq 1\). For simplicity, let

\[\tilde{g}(t)\coloneqq\frac{g(t\lor 1)^{\frac{2}{\kappa(d+\nu_{2})}}}{4\kappa( d+\nu_{2})},\]

and notice that

\[y^{*}=C^{\prime}_{\nu_{1},\nu_{2}}d^{\frac{\kappa(d+\nu_{2})}{2}\frac{\kappa( d+\nu_{2})-\nu_{1}}{\kappa(d+\nu_{2})-\nu_{2}}}\left(4\kappa(1+\nu_{2}/d) \tilde{g}(t)\right)^{\frac{\kappa^{2}(d+\nu_{2})^{2}}{2(\kappa(d+\nu_{2})- \nu_{2})}}.\] (3)

Using the fact that

\[y^{*}\geq(4\kappa)^{\frac{\kappa^{2}(d+\nu_{2})^{2}}{4\kappa(d+\nu_{2})-\nu_ {2}}}d^{\frac{\kappa(d+\nu_{2})}{2}\frac{\kappa(d+\nu_{2})-\nu_{1}}{\kappa(d+ \nu_{2})-\nu_{2}}},\]

we have

\[\mathrm{TV}(\pi^{X},\mu_{t}) \geq C_{\nu_{1}}\exp\Big{(}-\frac{1+\nu_{2}/d}{8\kappa}\cdot d^{ \frac{\nu_{1}-\nu_{2}}{\kappa(d+\nu_{2})-\nu_{2}}}\Big{)}d^{\nu_{1}/2}y^{*} ^{\frac{-\nu_{2}}{\kappa(d+\nu_{2})}}-\frac{g(t\lor 1)}{y^{*}}\] \[\geq\tilde{C}_{\nu_{1},\nu_{2}}d^{\nu_{1}/2}y^{*}^{\frac{-\nu_{2} }{\kappa(d+\nu_{2})}}-\frac{g(t\lor 1)}{y^{*}},\]

where \(\tilde{C}_{\nu_{1},\nu_{2}}=C_{\nu_{1}}e^{-\frac{1+\nu_{2}/d}{8}}\). By plugging in the value of \(y^{*}\) from (3), we obtain,

\[\mathrm{TV}(\pi^{X},\mu_{t})\] \[\geq \left\{\tilde{C}_{\nu_{1},\nu_{2}}C^{\prime}_{\nu_{1},\nu_{2}} \frac{-\nu_{2}}{\kappa(d+\nu_{2})}-C^{\prime}_{\nu_{1},\nu_{2}}-^{-1}\right\} \left\{d^{\frac{\nu_{1}-\nu_{2}}{2}}\left(2\kappa(1+\nu_{2}/d)\tilde{g}(t) \right)^{\frac{-\nu_{2}}{2}}\right\}^{1+\frac{\nu_{2}}{\kappa(d+\nu_{2})-\nu _{2}}}.\]

Thus for sufficiently large \(C^{\prime}_{\nu_{1},\nu_{2}}\), there exists \(C^{\prime\prime}_{\nu_{1},\nu_{2}}\) such that

\[\mathrm{TV}(\pi^{X},\mu_{t})\geq C^{\prime\prime}_{\nu_{1},\nu_{2}}\left\{d^{ \frac{\nu_{1}-\nu_{2}}{2}}\left(4\kappa(1+\nu/d)\tilde{g}(t)\right))^{\frac{- \nu_{2}}{2}}\right\}^{1+\frac{\nu_{2}}{\kappa(d+\nu_{2})-\nu_{2}}}.\]

Choosing \(\kappa\) according to the statement of the theorem completes the proof. 

In order to prove a similar theorem for the Gaussian proximal sampler, we control the growth of \(\mathbb{E}[G(x_{k})]\) for the iterates of the proximal sampler via the following lemmas.

**Lemma 4**.: _Suppose \((x_{k},y_{k})_{k}\) are the iterates of the Gaussian proximal sampler with step size \(\eta\) and target density \(\pi^{X}\propto\exp(-V)\) for some \(V:\mathbb{R}^{d}\to\mathbb{R}\). Let \(G(x)=\exp(\kappa V(x))\) with \(\kappa\geq 1\). Then, for every \(k\geq 0\),_

\[\mathbb{E}[G(x_{k+1})]\leq\mathbb{E}[G(x_{k}+\sqrt{2\eta}z)],\]

_where \(z\sim\mathcal{N}(0,I_{d})\) is sampled independently from \(x_{k}\)._

Proof.: Recall that \(\pi^{X|Y}(x|y)\propto\exp\big{(}-V(x)-\frac{|x-y|^{2}}{2\eta}\big{)}\). Therefore,

\[\mathbb{E}[G(x_{k+1})\mid y_{k}] =C_{y_{k}}\int\frac{\exp\big{(}(\kappa-1)V(x)-\frac{|x-y_{k}|^{2}} {2\eta}\big{)}}{(2\pi\eta)^{d/2}}\mathrm{d}x\] \[=C_{y_{k}}\mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})^{1-1/\kappa}\mid y _{k}],\]

where \(z_{1}\sim\mathcal{N}(0,I_{d})\). Furthermore,

\[C_{y_{k}} =\frac{1}{(2\pi\eta)^{d/2}}\int\exp\Big{(}-V(x)-\frac{|x-y_{k}|^{ 2}}{2\eta}\Big{)}\mathrm{d}x\] \[=\mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})^{-1/\kappa}\mid y_{k}].\]

Therefore,

\[\mathbb{E}[G(x_{k+1})\mid y_{k}]\] \[= \frac{\mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})^{1-1/\kappa}\mid y_{k} ]}{\mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})^{-1/\kappa}\mid y_{k}]}\] \[\leq \mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})\mid y_{k}]^{1-1/\kappa} \mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})\mid y_{k}]^{1/\kappa}\qquad\text{( Jensen's Inequality)}\] \[= \mathbb{E}[G(y_{k}+\sqrt{\eta}z_{1})\mid y_{k}].\]

Recall \(y_{k}=x_{k}+\sqrt{\eta}z_{2}\) where \(z_{2}\sim\mathcal{N}(0,I_{d})\) is independent from \(x_{k}\). By the towering property of conditional expectation,

\[\mathbb{E}[G(x_{k+1})] \leq\mathbb{E}[G(x_{k}+\sqrt{\eta}z_{1}+\sqrt{\eta}z_{2})]\] \[=\mathbb{E}[G(x_{k}+\sqrt{2\eta}z)],\]

where \(z\sim\mathcal{N}(0,I_{d})\) is independent from \(x_{k}\), which completes the proof. 

In order to provide a more refined control over \(\mathbb{E}[G(x_{k})]\), we need additional assumptions on \(V\). In particular, when considering the generalized Cauchy density, we arrive at the following lemma.

**Lemma 5**.: _Suppose \((x_{k},y_{k})_{k}\) are the iterates of the Gaussian proximal sampler with step size \(\eta\) and target density \(\pi^{X}\propto\exp(-V)\) satisfies_

\[|\nabla V(x)|\leq\frac{(d+\nu_{2})|x|}{1+|x|^{2}}\quad\text{and}\quad\Delta V( x)\leq\frac{(d+\nu_{2})^{2}}{1+|x|^{2}},\]

_for all \(x\in\mathbb{R}^{d}\). Let \(G(x)=\exp(\kappa V(x))\) with \(\kappa\geq 1\vee\frac{2}{d+\nu_{2}}\). Then, for every \(k\geq 0\),_

\[\mathbb{E}[G(x_{k+1})]^{\frac{2}{|x(d+\nu_{2})}}\leq\mathbb{E}[G(x_{k})]^{ \frac{2}{|x(d+\nu_{2})}}+4\kappa\eta k(d+\nu_{2}).\]

Proof.: From Lemma 4, we have

\[\mathbb{E}[G(x_{k+1})]\leq\mathbb{E}[G(x_{k}+\sqrt{2\eta}z)],\]

where \(z\sim\mathcal{N}(0,I_{d})\) is independent from \(x_{k}\). Consider the Brownian motion starting at \(x_{k}\), denoted by \(Z_{t}=B_{t}+x_{k}\) where \((B_{t})\) is a standard Brownian motion in \(\mathbb{R}^{d}\). Notice that the generator for theprocess \(\mathrm{d}Z_{t}=\mathrm{d}B_{t}\) is \(\mathcal{L}=\frac{1}{2}\Delta\). Therefore,

\[\frac{\mathrm{d}\mathbb{E}[G(Z_{t})]}{\mathrm{d}t} =\mathbb{E}[\mathcal{L}G(Z_{t})]\] \[=\frac{\kappa}{2}\mathbb{E}\big{[}G(Z_{t})\big{(}\kappa|\nabla V|^ {2}+\Delta V\big{)}\big{]}\] \[\leq\frac{\kappa(\kappa+1)}{2}\mathbb{E}\Big{[}G(Z_{t})\frac{(d+ \nu_{2})^{2}}{1+|Z_{t}|^{2}}\Big{]}\] \[\leq 2\kappa^{2}(d+\nu_{2})^{2}\mathbb{E}\big{[}G(Z_{t})^{1- \frac{2}{\kappa(d+\nu_{2})}}\big{]}\] \[\leq 2\kappa^{2}(d+\nu_{2})^{2}\mathbb{E}[G(Z_{t})]^{1-\frac{2}{ \kappa(d+\nu_{2})}}\qquad\text{(Jensen's Inequality)}.\]

Integrating the above inequality yields

\[\mathbb{E}[G(Z_{t})]^{\frac{2}{\kappa(d+\nu_{2})}}\leq\mathbb{E}[G(Z_{0})]^{ \frac{2}{\kappa(d+\nu_{2})}}+2\kappa(d+\nu_{2})t.\]

The proof is complete by noticing that \(Z_{0}=x_{k}\) and \(Z_{t}=x_{k}+\sqrt{2\eta}z\) for \(t=2\eta\). 

Proof of Theorem 2.: Notice that the statements of Lemmas 3 and 5 are virtually the same by changing \(t\) to \(2k\eta\). Using this fact, the rest of the proof follows exactly the same as the proof of Theorem 1. 

## Appendix B Proofs for the Stable Proximal Sampler

### Preliminaries

In this section, we introduce additional preliminaries on the isotropic \(\alpha\)-stable process, the fractional Poincare-type inequalities, the fractional Laplacian and the fractional heat flow.

The Levy process is a stochastic process that is stochastically continuous with independent and stationary increments. Due to the stochastic continuity, the Levy processes have cadlag trajectories, which allows jumps in the paths. A Levy process \(Y_{t}\) is uniquely determined by a triple \((b,A,\nu)\) through the following Levy-Khinchine formula: for all \(t\geq 0\) and \(\xi\in\mathbb{R}^{d}\),

\[\mathbb{E}\big{[}e^{i\langle\xi,Y_{t}\rangle}\big{]}=\exp\bigg{(}t\big{(}i \langle b,\xi\rangle-\xi^{\intercal}A\xi+\int_{\mathbb{R}^{d}\setminus\{0\}} (e^{i\langle\xi,y\rangle}-1-i\langle\xi,y\rangle 1_{\{|y|\leq 1\}}(y))\nu( \mathrm{d}y)\big{)}\bigg{)},\] (4)

where \(b\in\mathbf{R}^{d}\) is a drift vector. \(A\in\mathbb{R}^{d\times d}\) is the covariance matrix of the Brownian motion in the Levy-Ito decomposition[1, 10] and \(\nu\) is the Levy measure related to the jump parts in the Levy-Ito decomposition.

The rotationally invariant(isotropic) stable process is a special case for the Levy process when \(b=0\), \(A=0\) and \(\nu\) is the measure given by

\[\nu(\mathrm{d}y)=c_{d,\alpha}|y|^{-(d+\alpha)},\quad c_{d,\alpha}=2^{\alpha} \Gamma((d+\alpha)/2)/(\pi^{d/2}|\Gamma(-\alpha/2)|).\] (5)

Based on the Levy-Khinchine formula (4), if we initialize the process at \(x\in\mathbb{R}^{d}\), its characteristic function is given by

\[\mathbb{E}_{x}e^{i\langle\xi,X_{t}^{(\alpha)}-x\rangle}=e^{-t|\xi|^{\alpha}}, \qquad x,\xi\in\mathbb{R}^{d},\;t\geq 0.\] (6)

The index of stability \(\alpha\in(0,2]\) determines the tail-heaviness of the densities: the smaller is \(\alpha\), the heavier is the tail. The parameter \(t\) in (6) measures the spread of \(X_{t}\) around the center. When \(\alpha=2\), the stable process pertains to the Brownian motion running with a time clock twice as fast as the standard one and hence it has continuous paths. When \(\alpha\in(0,2)\), the stable process paths contain discontinuities, which are often referred as jumps. At each fixed time, unlike the Brownian motion, the \(\alpha\)-stable process density only has a finite \(p^{\text{th}}\)-moment for \(p<\alpha\), i.e.

\[\mathbb{E}[|X_{1}^{(\alpha)}|^{p}]=\begin{cases}+\infty&p\in[\alpha,+\infty), \alpha\in(0,2),\\ m_{p}^{(\alpha)}<+\infty&p\in(0,\alpha),\alpha\in(0,2).\end{cases}\]When \(d=1\), the fractional absolute moment formula for \(m_{p}^{(\alpha)}\) can be derived explicitly, see [20, Chapter 3.7]. When \(d>1\), the explicit formula for \(m_{p}^{(\alpha)}\) is only known in some special cases. For example, when \(\alpha=1\), \(m_{p}^{(1)}=\frac{\Gamma((d+p)/2)\Gamma((1-p)/2)}{\Gamma(d/2)\Gamma(1/2)}\) for all \(p<1\). Another good property of \(\alpha\)-stable process is the self-similarity. By examining the characteristic functions, it is easy to verify that the isotropic \(\alpha\)-stable process is self-similar with the Hurst index \(1/\alpha\), i.e. \(X_{at}^{(\alpha)}\) and \(a^{1/\alpha}X_{t}^{(\alpha)}\) have the same distribution. Or equivalently, \(p_{t}^{(\alpha)}(x)=t^{-\frac{d}{\alpha}}p_{1}^{(1)}(t^{-\frac{1}{\alpha}}x)\) for all \(x\in\mathbb{R}^{d}\) and \(t>0\).

The fractional Laplacian operator in \(\mathbb{R}^{d}\) of order \(\alpha\) is denoted by \(-(-\Delta)^{\alpha/2}\) for \(\alpha\in(0,2]\). It was introduced as a non-local generalization of the Laplacian operator to model various physical phenomenons. In [14], ten equivalent definitions of the fractional Laplacian operator are introduced. Here we recall two of them:

* Distributional definition: For all Schwartz functions \(\phi\) defined on \(\mathbb{R}^{d}\), we have \[\int_{\mathbb{R}^{d}}-(-\Delta)^{\alpha/2}f(y)\phi(y)\mathrm{d}y=\int_{ \mathbb{R}^{d}}f(x)\left(-(-\Delta)^{\alpha/2}\phi(x)\right)dx.\]
* Singular integral definition: For a limit in the space \(L^{p}(\mathbb{R}^{d})\), \(p\in[1,\infty)\), we have \[-(-\Delta)^{\alpha/2}f(x)=\lim_{r\to 0^{+}}\frac{2^{\alpha}\Gamma(\frac{d+ \alpha}{2})}{\pi^{d/2}|\Gamma(-\frac{\alpha}{2})|}\int_{\mathbb{R}^{d}\setminus B _{r}}\frac{f(x+z)-f(x)}{|z|^{d+\alpha}}\mathrm{d}z.\] where \(B_{r}\) is the unit ball with radius \(r\) centered at the origin.

The fractional Laplacian can be understood as the infinitesimal generator of the stable Levy process. More explicitly, the semigroup defined by the transition probability \(p_{t}^{(\alpha)}\) in (2) has the infinitesimal generator \(-(-\Delta)^{\alpha/2}\), i.e. the density function \(p_{t}^{(\alpha)}\) satisfies the following equation in the sense of distribution, [1]:

\[\partial_{t}p_{t}^{(\alpha)}(x)=-(-\Delta)^{\alpha/2}p_{t}^{(\alpha)}(x).\] (7)

(7) is usually referred as the \(\alpha\)-fractional heat flow. When \(\alpha=2\), \(-(-\Delta)^{\alpha/2}\) is the Laplacian operator and (7) becomes the heat flow.

**Proposition 2** (From FPI to PI).: _When \(\vartheta\to 2^{-}\), the \(\vartheta\)-FPI reduces to the classical Poincare inequality with Dirichlet form \(\mathcal{E}_{\mu}(\phi)=\int|\nabla\phi(x)|^{2}\mathrm{d}x\) for any smooth bounded \(\phi:\mathbb{R}^{d}\to\mathbb{R}^{d}\)._

Proof.: It suffices to prove that \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\) converges to \(\mathcal{E}_{\mu}(\phi)\) as \(\vartheta\to 2^{-}\) for any smooth function \(\phi\). Recall the definition of \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\):

\[\mathcal{E}_{\mu}^{(\vartheta)}(\phi):=c_{d,\vartheta}\iint_{\{x\neq y\}} \!\!\frac{(\phi(x)-\phi(y))^{2}}{|x-y|^{(d+\vartheta)}}\mathrm{d}x\mu(y) \mathrm{d}y\quad\text{ with }\quad c_{d,\vartheta}=\frac{2^{\vartheta}\Gamma((d+ \vartheta)/2)}{\pi^{d/2}|\Gamma(-\vartheta/2)|},\]

where \(c_{d,\vartheta}=\mathcal{O}(2-\vartheta)\) as \(\vartheta\to 2^{-}\). Now we rewrite the inside integral in \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\) and split the integral region into a centered unit ball, denoted as \(B_{1}\), and its complement:

\[\int_{x\neq y}\frac{(\phi(x)-\phi(y))^{2}}{|x-y|^{(d+\vartheta)}} \mathrm{d}x =\int_{z\neq 0}\frac{(\phi(y+z)-\phi(y))^{2}}{|z|^{(d+\vartheta)}} \mathrm{d}z\] \[=\underbrace{\int_{B_{1}}\frac{(\phi(y+z)-\phi(y))^{2}}{|z|^{(d+ \vartheta)}}\mathrm{d}z}_{I_{1}}+\underbrace{\int_{\mathbb{R}^{d}\setminus B_{1 }}\frac{(\phi(y+z)-\phi(y))^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z}_{I_{2}}.\]

For \(I_{2}\), we have

\[I_{2}\leq 4\left\|\phi\right\|_{\infty}^{2}\int_{\mathbb{R}^{d}\setminus B_{1}} \frac{1}{|z|^{d+\vartheta}}\mathrm{d}z=\frac{4\left\|\phi\right\|_{\infty}^{2}d \pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2}+1)}\int_{1}^{\infty}r^{-\vartheta+1} \mathrm{d}r=\frac{4\left\|\phi\right\|_{\infty}^{2}d\pi^{\frac{d}{2}}}{\vartheta \Gamma(\frac{d}{2}+1)}.\]

As a result, the term in \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\) that is induced by \(I_{2}\) satisfies

\[c_{d,\vartheta}\int_{\mathbb{R}^{d}}I_{2}\mu(y)\mathrm{d}y\leq c_{d,\vartheta} \frac{4\left\|\phi\right\|_{\infty}^{2}d\pi^{\frac{d}{2}}}{\vartheta\Gamma(\frac{ d}{2}+1)}\to 0\quad\text{as }\vartheta\to 2^{-}.\]For \(I_{1}\), we have when \(\vartheta>1\),

\[I_{1}-\int_{B_{1}}\frac{|\langle\nabla\phi(y),z\rangle|^{2}}{|z|^{d +\vartheta}}\mathrm{d}z\] \[=\int_{B_{1}}\frac{\big{(}\phi(y+z)-\phi(y)-\langle\nabla\phi(y),z \rangle\big{)}\big{(}\phi(y+z)-\phi(y)+\langle\nabla\phi(y),z\rangle\big{)}}{|z |^{d+\vartheta}}\mathrm{d}z\] \[\leq\|\phi\|_{C^{2}(\mathbb{R}^{d})}\,\|\phi\|_{C^{1}(\mathbb{R}^ {d})}\int_{B_{1}}|z|^{-(d+\vartheta-3)}\mathrm{d}z\] \[=\|\phi\|_{C^{2}(\mathbb{R}^{d})}\,\|\phi\|_{C^{1}(\mathbb{R}^{d} )}\,\frac{d\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2}+1)}\int_{0}^{1}r^{\vartheta- 2}\mathrm{d}r\] \[=\|\phi\|_{C^{2}(\mathbb{R}^{d})}\,\|\phi\|_{C^{1}(\mathbb{R}^{d} )}\,\frac{d\pi^{\frac{d}{2}}}{(\vartheta-1)\Gamma(\frac{d}{2}+1)},\]

where \(\|\phi\|_{C^{i}(\mathbb{R}^{d})}:=\sup_{x\in\mathbb{R}^{d}}|\phi^{(i)}(x)|\) for \(i=1,2\). As a result, the term in \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\) that is induced by \(I_{1}\) satisfies

\[c_{d,\vartheta}\int_{\mathbb{R}^{d}}\big{(}I_{2}-\int_{B_{1}}\frac{|\langle \nabla\phi(y),z\rangle|^{2}}{|z|^{d+\vartheta}}\mathrm{d}z\big{)}\mu(y) \mathrm{d}y\leq c_{d,\vartheta}\frac{\|\phi\|_{C^{2}(\mathbb{R}^{d})}\,\|\phi \|_{C^{1}(\mathbb{R}^{d})}\,d\pi^{\frac{d}{2}}}{(\vartheta-1)\Gamma(\frac{d} {2}+1)}\to 0\quad\text{as }\vartheta\to 2^{-}.\]

Therefore we have \(\mathcal{E}_{\mu}^{(\vartheta)}(\phi)\to c_{d,\vartheta}\int_{\mathbb{R}^{d}} \int_{B_{1}}\frac{|\langle\nabla\phi(y),z\rangle|^{2}}{|z|^{d+\vartheta}}\mu (y)\mathrm{d}z\mathrm{d}y\) as \(\vartheta\to 2^{-}\). Last, we prove the limit is equivalent to \(2\mathcal{E}_{\mu}(\phi)\). For \(i\neq j\), we have

\[\int_{B_{1}}\partial_{i}\phi(y)\partial_{j}\phi(y)z_{i}z_{j}\mathrm{d}z=-\int_ {B_{1}}\partial_{i}\phi(y)\partial_{j}\phi(y)\tilde{z}_{i}\tilde{z}_{j} \mathrm{d}\tilde{z},\]

where \(\tilde{z}_{k}=z_{k}\) for all \(k\neq j\) and \(\tilde{z}_{j}=-z_{j}\). Therefore, \(\int_{B_{1}}\partial_{i}\phi(y)\partial_{j}\phi(y)z_{i}z_{j}\mathrm{d}z=0\). As a result,

\[\int_{B_{1}}\frac{|\langle\nabla\phi(y),z\rangle|^{2}}{|z|^{d+ \vartheta}}\mathrm{d}z =\int_{B_{1}}\frac{\sum_{i=1}^{d}(\partial_{i}\phi(y))^{2}z_{i}^ {2}}{|z|^{d+\vartheta}}\mathrm{d}z\] \[=\sum_{i=1}^{d}(\partial_{i}\phi(y))^{2}\frac{1}{d}\int_{B_{1}} \frac{|z|^{2}}{|z|^{d+\vartheta}}\mathrm{d}z\] \[=|\nabla\phi(y)|^{2}\frac{\pi^{\frac{d}{2}}}{(2-\vartheta)\Gamma( \frac{d}{2}+1)},\]

and the proof follows from \(c_{d,\vartheta}\frac{\pi^{\frac{d}{2}}}{(2-\vartheta)\Gamma(\frac{d}{2}+1)}\to 2\) as \(\vartheta\to 2^{-}\). 

### \(\chi^{2}\) convergence under FPI

In this section, we study the decaying property of \(\chi^{2}\)-divergence from \(\rho_{k}^{X}\) to \(\pi^{X}\), where \(\rho_{k}^{X}\) is the law of \(x_{k}\). In the following analysis, we denote \(\rho_{k}=\rho_{k}^{X,Y}\) as the law of \((x_{k},y_{k})\), \(\rho_{k}^{Y}\) the law of \(y_{k}\). We will analyze the two steps in the stable proximal sampler separately.

**Step 1.** In the following proposition, we study the decay of \(\chi^{2}\)-divergence in step 1.

**Proposition 3**.: _Assume that \(\pi^{X}\) satisfies the \(\alpha\)-FPI with parameter \(C_{\text{FPI}(\alpha)}\), then for each \(k\geq 0\),_

\[\chi^{2}(\rho_{k}^{Y}|\pi^{Y})\leq\exp\left(-\eta\left(C_{\text{FPI}(\alpha)}+ \eta\right)^{-1}\right)\chi^{2}(\rho_{k}^{X}|\pi^{X}).\]

Proof of Proposition 3.: For the simplicity of notations, we will write \(p^{(\alpha)}\) and \(p_{t}^{(\alpha)}\) as \(p\) and \(p_{t}\) respectively in this proof. Since \(x_{k}\sim\rho_{k}^{X}\) and \(y_{k}|x_{k}\sim p(\eta;x,\cdot)\), we have

\[\rho_{k}^{Y}(y)=\int_{\mathbb{R}^{d}}p(\eta;x,y)\rho_{k}^{X}(x)\mathrm{d}x=\int_ {\mathbb{R}^{d}}\rho_{k}^{X}(x)p_{\eta}(y-x)\mathrm{d}x=\rho_{k}^{X}\ast p_{ \eta}(y).\]Therefore, we can view \(\rho_{k}^{Y}\) as \(\rho_{k}^{X}\) evolving along the following factional heat flow

\[\partial_{t}\tilde{\rho}_{t}=-(-\Delta)^{\frac{\alpha}{2}}\tilde{\rho}_{t}.\]

That is if \(\tilde{\rho}_{0}=\rho_{k}^{X}\), then \(\tilde{\rho}_{\eta}=\rho_{k}^{Y}\). Similarly, since \(\pi^{Y}=\pi^{X}*p_{\eta}\), if \(\tilde{\rho}_{0}=\pi^{X}\), then \(\tilde{\rho}_{\eta}=\pi^{Y}\). For any \(t\in[0,\eta]\), define \(\pi_{t}^{X}=\pi^{X}*p_{t}\) and \(\rho_{t}^{X}=\rho_{k}^{X}*p_{t}\). The derivative of \(\phi\)-divergence from \(\rho_{t}^{X}\) to \(\pi_{t}^{X}\) can be calculated as

\[\frac{d}{dt}\int_{\mathbb{R}^{d}}\phi(\frac{\rho_{t}^{X}}{\pi_{t}^ {X}})\pi_{t}^{X}\mathrm{d}x\] \[= \int_{\mathbb{R}^{d}}\partial_{t}\pi_{t}^{X}\phi(\frac{\rho_{t}^{ X}}{\pi_{t}^{X}})+\phi^{\prime}(\frac{\rho_{t}^{X}}{\pi_{t}^{X}})\left(\partial_{t} \rho_{t}^{X}-\partial_{t}\pi_{t}^{X}\frac{\rho_{t}^{X}}{\pi_{t}^{X}}\right) \mathrm{d}x\] \[= -\int_{\mathbb{R}^{d}}\phi(\frac{\rho_{t}^{X}}{\pi_{t}^{X}})(- \Delta)^{\frac{\alpha}{2}}\pi_{t}^{X}\mathrm{d}x+\int_{\mathbb{R}^{d}}\phi^{ \prime}(\frac{\rho_{t}^{X}}{\pi_{t}^{X}})\left(\frac{\rho_{t}^{X}}{\pi_{t}^{X} }(-\Delta)^{\frac{\alpha}{2}}\pi_{t}^{X}-(-\Delta)^{\frac{\alpha}{2}}\rho_{t}^ {X}\right)\mathrm{d}x\] \[= \int_{\mathbb{R}^{d}}\left[-\frac{\rho_{t}^{X}}{\pi_{t}^{X}}(- \Delta)^{\frac{\alpha}{2}}\phi^{\prime}(\frac{\rho_{t}^{X}}{\pi_{t}^{X}})+(- \Delta)^{\frac{\alpha}{2}}\left(\frac{\rho_{t}^{X}}{\pi_{t}^{X}}\phi^{\prime}( \frac{\rho_{t}^{X}}{\pi_{t}^{X}})\right)-(-\Delta)^{\frac{\alpha}{2}}\phi( \frac{\rho_{t}^{X}}{\pi_{t}^{X}})\right]\pi_{t}^{X}\mathrm{d}x,\]

where in the second identity we used the distributional definition of the fractional Laplacian. Next according to the singular integral definition of fractional Laplacian, we have

\[-(-\Delta)^{\frac{\alpha}{2}}f(x):=c_{d,\alpha}\lim_{r\to 0^{+}}\int_{ \mathbb{R}^{d}\setminus B_{r}}\frac{f(x+z)-f(x)}{|z|^{d+\alpha}}\mathrm{d}z,\] (8)

where \(B_{r}=\{x\in\mathbb{R}^{d}:|x|\leq r\}\) and \(c_{d,\alpha}\) is given in (5). With (8), we have

\[\frac{d}{dt}\int_{\mathbb{R}^{d}}\phi(\frac{\rho_{t}^{X}}{\pi_{t}^ {X}})\pi_{t}^{X}\mathrm{d}x\] \[= c_{d,\alpha}\lim_{r\to 0^{+}}\int_{\mathbb{R}^{d}\setminus B_{r}} \frac{\phi(\frac{\rho_{t}^{X}(x+z)}{\pi_{t}^{X}(x+z)})-\phi(\frac{\rho_{t}^{X} (x)}{\pi_{t}^{X}(x+z)})-\frac{\rho_{t}^{X}(x+z)}{\pi_{t}^{X}(x+z)}\phi^{\prime }(\frac{\rho_{t}^{X}(x+z)}{\pi_{t}^{X}(x+z)})+\frac{\rho_{t}^{X}(x)}{\pi_{t}^{ X}(x)}\phi^{\prime}(\frac{\rho_{t}^{X}(x+z)}{\pi_{t}^{X}(x+z)})}{|z|^{d+\alpha}} \mathrm{d}z\pi_{t}^{X}(x)\mathrm{d}x.\]

When \(\phi(r)=(r-1)^{2}\), \(\int_{\mathbb{R}^{d}}\phi(\frac{\rho_{t}^{X}}{\pi_{t}^{X}})\pi_{t}^{X}\mathrm{d }x=\chi^{2}(\rho_{t}^{X}|\pi_{t}^{X})\) and we have

\[\frac{d}{dt}\chi^{2}(\rho_{t}^{X}|\pi_{t}^{X})=-c_{d,\alpha}\lim_{r\to 0^{+}} \int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}\setminus B_{r}}\frac{\left(\frac{ \rho_{t}^{X}(x+z)}{\pi_{t}^{X}(x+z)}-\frac{\rho_{t}^{X}(x)}{\pi_{t}^{X}(x)} \right)^{2}}{|z|^{d+\alpha}}\mathrm{d}z\pi_{t}^{X}\mathrm{d}x:=-\mathcal{E}_{ \pi_{t}^{X}}(\frac{\rho_{t}^{X}}{\pi_{t}^{X}}).\]

According to [10, Theorem 23], \(p_{t}\) satisfies \(\alpha\)-FPI with parameter \(t\) for all \(t\in(0,\eta]\). Since \(\pi^{X}\) also satisfies the \(\alpha\)-FPI with parameter \(C_{\text{FPI}(\alpha)}\), Lemma 6 implies that \(\pi_{t}^{X}=\pi^{X}*p_{t}\) satisfies the \(\alpha\)-FPI with parameter \(C_{\text{FPI}(\alpha)}+\eta\) for all \(t\in(0,\eta]\). Therefore we have

\[\frac{d}{dt}\chi^{2}(\rho_{t}^{X}|\pi_{t}^{X})=-\mathcal{E}_{\pi_{t}^{X}}( \frac{\rho_{t}^{X}}{\pi_{t}^{X}})\leq-\left(C_{\text{FPI}(\alpha)}+\eta\right) ^{-1}\chi^{2}(\rho_{t}^{X}|\pi_{t}^{X}).\]

Last, according to Gronwall's inequality we have

\[\chi^{2}(\rho_{k}^{Y}|\pi^{Y})=\chi^{2}(\rho_{\eta}^{X}|\pi_{\eta}^{X})\leq \exp\left(-\eta\left(C_{\text{FPI}(\alpha)}+\eta\right)^{-1}\right)\chi^{2}( \rho_{k}^{X}|\pi^{X}).\]

**Step 2.** In this step, we study the decay of \(\chi^{2}\)-divergence in step 2. building on the work by [12]. According to the R\(\alpha\)SO, we have \(\rho_{k+1}^{X}(x)=\int_{\mathbb{R}^{d}}\pi^{X|Y}(x|y)\rho_{k}^{Y}(y)\mathrm{d}y\). Also notice that \(\pi^{X}(x)=\int_{\mathbb{R}^{d}}\pi^{X|Y}(x|y)\pi^{Y}(y)\mathrm{d}y\). According to the data processing inequalities, \(\chi^{2}\) divergence won't increase after step 2, i.e. \(\chi^{2}(\rho_{k+1}^{X}|\pi^{X})\leq\chi^{2}(\rho_{k}^{Y}|\pi^{Y})\).

Combining our results in **Step 1** and **Step 2**, we prove Theorem 3.

**Lemma 6**.: _Let \(\mu_{1},\mu_{2}\) be two probability densities satisfying the \(\vartheta\)-FPI with parameters \(C_{1},C_{2}\) respectively. Then \(\mu_{1}*\mu_{2}\) satisfies the \(\vartheta\)-FPI with parameter \(C_{1}+C_{2}\)._

Proof of Lemma 6.: Let \(X,Y\) be two independent random variables such that \(X\sim\mu_{1}\) and \(Y\sim\mu_{2}\). Then \(X+Y\sim\mu_{1}*\mu_{2}\). According to variance decomposition, we have for any function \(\phi\),

\[\text{Var}_{\mu_{1}*\mu_{2}}\left(\phi\right)=\text{Var}\left(\phi(X+Y) \right)=\mathbb{E}\left[\text{Var}\left(\phi(X+Y)|Y\right)\right]+\text{Var} \left(\mathbb{E}\left[\phi(X+Y)|Y\right]\right).\]Since \(X\sim\mu_{1}\) and \(\mu_{1}\) satisfies the \(\vartheta\)-FPI with parameter \(C_{1}\), we have

\[\text{Var}\left(\phi(X+Y)|Y\right)\leq C_{1}c_{d,\alpha}\iint_{\{z\neq 0\}} \frac{\left(\phi(x+Y+z)-\phi(x+Y)\right)^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z \mu_{1}(x)\mathrm{d}x,\]

therefore we have

\[\mathbb{E}\left[\text{Var}\left(\phi(X+Y)|Y\right)\right]\] (9) \[\leq C_{1}c_{d,\alpha}\iiint_{\{z\neq 0\}}\frac{\left(\phi(x+y+z)- \phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{1}(x)\mathrm{d}x\mu_ {2}(y)\mathrm{d}y.\]

Since \(Y\sim\mu_{2}\) and \(\mu_{2}\) satisfies the \(\vartheta\)-FPI with parameter \(C_{2}\), we have

\[\text{Var}\left(\mathbb{E}\left[\phi(X+Y)|Y\right]\right)\] (10) \[\leq C_{2}c_{d,\alpha}\iint_{\{z\neq 0\}}\frac{\left(\int\phi(x+y+z) \mu_{1}(x)\mathrm{d}x-\int\phi(x+y)\mu_{1}(x)\mathrm{d}x\right)^{2}}{|z|^{(d+ \vartheta)}}\mathrm{d}z\mu_{2}(y)\mathrm{d}y\] \[\leq C_{2}c_{d,\alpha}\iint_{\{z\neq 0\}}\int\frac{(\phi(x+y+z)- \phi(x+y))^{2}}{|z|^{(d+\vartheta)}}\mu_{1}(x)\mathrm{d}x\mathrm{d}z\mu_{2}(y) \mathrm{d}y\]

where the last inequality follows from Jensen's inequality. Combining (9) and (10), we have

\[\text{Var}_{\mu_{1}\ast\mu_{2}}(\phi) \leq C_{1}c_{d,\alpha}\iiint_{\{z\neq 0\}}\frac{\left(\phi(x+y+z)- \phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{1}(x)\mathrm{d}x\mu _{2}(y)\mathrm{d}y\] \[\quad+C_{2}c_{d,\alpha}\iint_{\{z\neq 0\}}\int\frac{\left(\phi(x+y+ z)-\phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mu_{1}(x)\mathrm{d}x\mathrm{d}z \mu_{2}(y)\mathrm{d}y\] \[\leq(C_{1}+C_{2})\,c_{d,\alpha}\iiint_{\{z\neq 0\}}\frac{(\phi(x+y+ z)-\phi(x+y))^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{1}(x)\mathrm{d}x\mu _{2}(y)\mathrm{d}y\] \[=(C_{1}+C_{2})\,c_{d,\alpha}\iint_{\{z\neq 0\}}\frac{(\phi(x+z)- \phi(u))^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{1}\ast\mu_{2}(u)\mathrm{d}u\] \[=(C_{1}+C_{2})\,\mathcal{E}_{\mu_{1}\ast\mu_{2}}(\phi),\]

where the second inequality follows from Fatou's lemma. 

### Implementation of the Stable Proximal Sampler

In this section we discuss the implementation of the R\(\alpha\)SO step in our stable proximal sampler. We introduce an exact implementation of the R\(\alpha\)SO step without optimizing the target potential and the proofs for Corollary 3 and Proposition 1.

**Rejection sampling without optimization**. Suppose a uniform lower bound of the target potential is known, i.e. there is a constant \(C_{\text{Low}}\) such that \(\inf_{x\in\mathbb{R}^{d}}V(x)\geq C_{\text{Low}}>-\infty\), R\(\alpha\)SO at each step can be implemented exactly via a rejection sampler with proposals \(\tilde{x}_{k+1}\) following \(p_{\eta}^{(\alpha)}(\cdot-y_{k})\) and the acceptance probability \(\exp(-V(\tilde{x}_{k+1})+C_{\text{Low}})\). Then the expected number of rejections, \(N\), satisfies

\[N=\big{(}\int_{\mathbb{R}^{d}}e^{-V(x)+C_{\text{Low}}}p(\eta;x,y_{k})\mathrm{d }x\big{)}^{-1}\quad\text{and}\quad\log N=-C_{\text{Low}}-\log\big{(}\int_{ \mathbb{R}^{d}}e^{-V(x)}p^{(\alpha)}(\eta;x,y_{k})\mathrm{d}x\big{)}.\]

Without loss of generality, we assume \(x^{*}=0\), which always hold if we translate the potential \(V\) by \(V(0)\). Then we have

\[\log N \leq-C_{\text{Low}}+\int_{\mathbb{R}^{d}}\big{(}V(x)-V(0)\big{)}p ^{(\alpha)}(\eta;x,y_{k})\mathrm{d}x\] \[\leq-C_{\text{Low}}+L\int_{\mathbb{R}^{d}}\big{|}x+y_{k}\big{|}^ {\beta}p_{\eta}^{(\alpha)}(x)\mathrm{d}x\] \[\leq-C_{\text{Low}}+L\mathbb{E}_{X\sim\pi^{X}}[|X|^{\beta}]+L\eta ^{\beta}d^{\frac{\beta}{2}}+L\mathbb{E}_{X\sim\pi^{X}}[|X|^{2\beta}]^{\frac{1} {2}}\chi^{2}(\rho_{0}^{X}|\pi^{X})^{\frac{1}{2}}+\frac{\Gamma(\frac{d+1}{2}) \Gamma(\frac{1-\beta}{2})L}{\Gamma(\frac{d+1-\beta}{2})\pi^{\frac{1}{2}}}\eta^ {\beta},\]where the second inequality follows from Assumption 3 and the last inequality follows from the proof of Corollary 3. With the above estimation, we can pick \(\eta=\Theta(C_{\text{Low}}^{\frac{1}{3}}d^{-\frac{1}{2}}L^{-\frac{1}{3}})\) and the expected number of rejections satisfies \(\log N=\mathcal{O}(C_{\text{Low}}+LM)\) with \(M=\mathbb{E}_{\pi^{X}}[|X|^{\beta}]+\chi^{2}(\rho_{0}^{X}|\pi^{X})\mathbb{E}_{ \pi^{X}}[|X|^{2\beta}]^{\frac{1}{2}}\).

Proof of Corollary 3.: The expected number of iterations conditioned on \(y_{k}\) in the rejection sampling is

\[N =\bigg{(}\int_{\mathbb{R}^{d}}e^{-V(x)+V(x^{*})}p^{(\alpha)}( \eta;x,y_{k})\mathrm{d}x\bigg{)}^{-1}\] \[\text{and}\quad\log N =-V(x^{*})-\log\big{(}\int_{\mathbb{R}^{d}}e^{-V(x)}p^{(\alpha)} (\eta;x,y_{k})\mathrm{d}x\big{)}\] \[\leq\int_{\mathbb{R}^{d}}\big{(}V(x)-V(x^{*})\big{)}p^{(\alpha)} (\eta;x,y_{k})\mathrm{d}x\] \[=\int_{\mathbb{R}^{d}}\big{(}V(x+y_{k})-V(x^{*})\big{)}p^{(\alpha )}_{\eta}(x)\mathrm{d}x.\]

WLOG, assume \(x^{*}=0\). Since \(V\) satisfies Assumption 3, we have

\[\log N \leq L\int_{\mathbb{R}^{d}}\big{|}x+y_{k}\big{|}^{\beta}p^{( \alpha)}_{\eta}(x)\mathrm{d}x=\frac{L\Gamma(\frac{d+1}{2})}{\pi^{\frac{d+1}{2} }}\eta\int_{\mathbb{R}^{d}}\big{|}x+y_{k}\big{|}^{\beta}(\big{|}x\big{|}^{2}+ \eta^{2})^{-\frac{d+1}{2}}\mathrm{d}x\] \[\leq L|y_{k}|^{\beta}+\frac{L\Gamma(\frac{d+1}{2})}{\pi^{\frac{d+ 1}{2}}}\eta\int_{\mathbb{R}^{d}}\big{(}\big{|}x\big{|}^{2}+\eta^{2})^{-\frac{d +1-\beta}{2}}\mathrm{d}x\] \[=L|y_{k}|^{\beta}+\frac{\Gamma(\frac{d+1}{2})\Gamma(\frac{1-\beta }{2})L}{\Gamma(\frac{d+1-\beta}{2})\pi^{\frac{1}{2}}}\eta^{\beta}.\]

Therefore, when \(\eta=\Theta(d^{-\frac{1}{2}}L^{-\frac{1}{3}})\), the expected number of rejections N is of order \(\mathbb{E}[\exp(L|y_{k}|^{\beta}]\). Since \(\pi^{X}\) satisfies a \(1\)-FPI with parameter \(C_{\text{FPI}(1)}\), according to [1], \(p_{t}\) satisfies the \(1\)-FPI with parameter \(\eta\) for any \(t\in(0,\eta)\). Last it follows from Theorem 9 that for any \(\eta>0\), to achieve a \(\varepsilon\)-accuracy in \(\chi^{2}\) divergence, we need to perform the stable proximal sampler \(K\) steps with

\[K\geq\big{(}C_{\text{FPI}(1)}\eta^{-1}+1\big{)}\log\bigg{(}\frac{\chi^{2}(\rho_ {0}^{X}|\pi^{X})}{\varepsilon}\bigg{)}=\mathcal{O}\big{(}C_{\text{FPI}(1)}d^{ \frac{1}{2}}L^{\frac{1}{3}}\log\big{(}\frac{\chi^{2}(\rho_{0}^{X}|\pi^{X})}{ \varepsilon}\big{)}\big{)}.\]

Proof of Proposition 1.: For all \(k\geq 0\), we have

\[\mathrm{TV}(\tilde{\rho}_{k+1}^{X},\rho_{k+1}^{X}) =\mathrm{TV}\big{(}\int\tilde{\rho}_{k+1}^{X|Y}(\cdot|y)\tilde{ \rho}_{k}^{Y}(y)\mathrm{d}y,\int\rho_{k+1}^{X|Y}(\cdot|y)\rho_{k}^{Y}(y) \mathrm{d}y\big{)}\] \[\leq\mathrm{TV}\big{(}\int\tilde{\rho}_{k+1}^{X|Y}(\cdot|y)\tilde {\rho}_{k}^{Y}(y)\mathrm{d}y,\int\rho_{k+1}^{X|Y}(\cdot|y)\tilde{\rho}_{k}^{Y}( y)\mathrm{d}y\big{)}\] \[\quad+\mathrm{TV}\big{(}\int\rho_{k+1}^{X|Y}(\cdot|y)\tilde{\rho }_{k}^{Y}(y)\mathrm{d}y,\int\rho_{k+1}^{X|Y}(\cdot|y)\rho_{k}^{Y}(y)\mathrm{d}y \big{)}\] \[\leq\mathbb{E}_{\tilde{\rho}_{k}^{X}}[\mathrm{TV}(\tilde{\rho}_{k +1}^{X|Y}(\cdot,y)],\rho_{k+1}^{X|Y}(\cdot|y))+\mathrm{TV}(\tilde{\rho}_{k}^{Y},\rho_{k}^{Y})\] \[\leq\varepsilon_{\mathrm{TV}}+\mathrm{TV}(\tilde{\rho}_{k}^{X}, \rho_{k}^{X}),\]

where the last two inequalities follow from the data processing inequality. Therefore, \(\mathrm{TV}(\tilde{\rho}_{k}^{X},\rho_{k}^{X})\leq k\varepsilon_{\mathrm{TV}}+ \mathrm{TV}(\tilde{\rho}_{0}^{X},\rho_{0}^{X})\) for all \(k\geq 1\).

[MISSING_PAGE_FAIL:25]

heat flow by time \(\eta\) respectively. For any \(t\in[0,\eta]\), define \(\pi^{X}_{t}=\pi^{X}*p^{(\alpha)}_{t}\) and \(\rho^{X}_{t}=\rho^{X}_{k}*p^{(\alpha)}_{t}\). We have

\[\frac{\mathrm{d}}{\mathrm{d}t}\chi^{2}(\rho^{X}_{t}|\pi^{X}_{t})=-\mathcal{E}_{ \pi^{X}_{t}}(\frac{\rho^{X}_{t}}{\pi^{X}_{t}})=-\mathcal{E}_{\pi^{X}_{t}}( \frac{\rho^{X}_{t}}{\pi^{X}_{t}}-1).\]

According to [10, Theorem 23], \(p^{(\alpha)}_{t}\) satisfies \(\alpha\)-FPI with parameter \(\eta\) for all \(t\in(0,\eta]\). According to Lemma 7, \(\pi^{X}_{t}\) satisfies the \(\alpha\)-wPI with \(\beta_{\text{WFPI}(\alpha)}(r)+\eta\). Therefore we get

\[\frac{\mathrm{d}}{\mathrm{d}t}\chi^{2}(\rho^{X}_{t}|\pi^{X}_{t}) \leq\big{(}\beta_{\text{WFPI}(\alpha)}(r)+\eta\big{)}^{-1}\chi^{2 }(\rho^{X}_{t}|\pi^{X}_{t})+r\big{(}\beta_{\text{WFPI}(\alpha)}(r)+\eta\big{)} ^{-1}\left\|\rho^{X}_{t}/\pi^{X}_{t}-1\right\|_{\infty}^{2}\] \[\leq\big{(}\beta_{\text{WFPI}(\alpha)}(r)+\eta\big{)}^{-1}\chi^{ 2}(\rho^{X}_{t}|\pi^{X}_{t})+4r\big{(}\beta_{\text{WFPI}(\alpha)}(r)+\eta \big{)}^{-1}\exp\big{(}2R_{\infty}(\rho^{X}_{k}|\pi^{X})\big{)},\]

where the last inequality follows from the definition of Renyi-divergence and the data processing inequality. Last, (11) follows from Gronwall's inequality. 

Proof of Theorem 5.: According to Proposition 4, the \(\chi^{2}\) decaying property in step 1 of the algorithm is as follows,

\[\chi^{2}(\rho^{Y}_{k}|\pi^{Y}) \leq\exp\big{(}-(\beta_{\text{WFPI}(\alpha)}(r)+\eta)^{-1}\eta \big{)}\chi^{2}(\rho^{X}_{k}|\pi^{X})\] \[\quad+4r\big{(}1-\exp\big{(}-(\beta_{\text{WFPI}(\alpha)}(r)+ \eta)^{-1}\eta\big{)}\exp\big{(}2R_{\infty}(\rho^{X}_{k}|\pi^{X})\big{)}.\]

In step 2, we have \(\rho^{X}_{k+1}=\rho^{Y}_{k}*\pi^{X|Y}\) and \(\pi^{X}=\pi^{Y}*\pi^{X|Y}\). Therefore according to the data processing inequality, we get

\[\chi^{2}(\rho^{X}_{k+1}|\pi^{X}) \leq\chi^{2}(\rho^{Y}_{k}|\pi^{Y})\] \[\leq\exp\big{(}-(\beta_{\text{WFPI}(\vartheta)}(r)+\eta)^{-1} \eta\big{)}\chi^{2}(\rho^{X}_{k}|\pi^{X})\] \[\quad+4r\big{(}1-\exp\big{(}-(\beta_{\text{WFPI}(\alpha)}(r)+\eta )^{-1}\eta\big{)}\exp\big{(}2R_{\infty}(\rho^{X}_{k}|\pi^{X})\big{)}\] \[\leq\exp\big{(}-k(\beta_{\text{WFPI}(\alpha)}(r)+\eta)^{-1}\eta \big{)}\chi^{2}(\rho^{X}_{0}|\pi^{X})\] \[\quad+4r\big{(}1-\exp\big{(}-(\beta_{\text{WFPI}(\alpha)}(r)+\eta )^{-1}(k+1)\eta\big{)}\big{)}\exp\big{(}2R_{\infty}(\rho^{X}_{0}|\pi^{X})\big{)},\]

where the last inequality follows from the data processing inequality. Last, apply the above iterative relation \(k\) times and we prove (11). 

**Lemma 7**.: _Let \(\mu_{1}\) be a probability density on \(\mathbb{R}^{d}\) satisfying the \(\vartheta\)-wFPI with parameter \(\beta_{\text{WFPI}(\vartheta)}(r)\). Let \(\mu_{2}\) be a probability density on \(\mathbb{R}^{d}\) satisfying the \(\vartheta\)-FPI with parameter \(C_{\text{FPI}(\vartheta)}\). Then \(\mu_{1}*\mu_{2}\) satisfies \(\vartheta\)-wFPI with parameter \(\beta_{\text{WFPI}(\vartheta)}(r)+C_{\text{FFI}(\vartheta)}\)._

Proof of Lemma 7.: Let \(X,Y\) be two independent random variables such that \(X\sim\mu_{2}\) and \(Y\sim\mu_{1}\). According to variance decomposition, we have for any function \(\phi\) such that \(\mu_{1}*\mu_{2}(\phi)=0\),

\[\text{Var}_{\mu_{1}*\mu_{2}}(\phi)=\text{Var}\left(\phi(X+Y)\right)=\mathbb{E} \left[\text{Var}\left(\phi(X+Y)|Y\right)\right]+\text{Var}\left(\mathbb{E} \left[\phi(X+Y)|Y\right]\right).\]

Since \(X\sim\mu_{2}\) and \(\mu_{2}\) satisfies the \(\vartheta\)-FPI with parameter \(C_{\text{FFI}(\vartheta)}\), we have

\[\mathbb{E}[\text{Var}\left(\phi(X+Y)|Y\right)]\] (12) \[\leq C_{\text{FFI}(\vartheta)}c_{d,\alpha}\ii\iint_{\{z\neq 0\}}\frac{ \left(\phi(x+y+z)-\phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{2}( x)\mathrm{d}x\mu_{1}(y)\mathrm{d}y.\] (13)

Since \(Y\sim\mu_{1}\) and \(\mu_{1}\) satisfies the \(\vartheta\)-wFPI with parameter \(\beta_{\text{WFPI}(\vartheta)}\), following the proof of Lemma 6, we have

\[\text{Var}\left(\mathbb{E}\left[\phi(X+Y)|Y\right]\right)\] (14) \[\leq \beta_{\text{WFPI}(\vartheta)}c_{d,\alpha}\iint_{(z\neq 0)}\int\frac{ \left(\phi(x+y+z)-\phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mu_{2}(x)\mathrm{d} x\mathrm{d}z\mu_{1}(y)\mathrm{d}y\] \[\quad+r\left\|\int\phi(x+\cdot)\mu_{2}(x)\mathrm{d}x-\iint\phi(x+ y)\mu_{2}(x)\mathrm{d}x\mu_{1}(y)\mathrm{d}y\right\|_{\infty}^{2}\] \[\leq \beta_{\text{WFPI}(\vartheta)}c_{d,\alpha}\iint_{(z\neq 0)}\int\frac{ \left(\phi(x+y+z)-\phi(x+y)\right)^{2}}{|z|^{(d+\vartheta)}}\mu_{2}(x)\mathrm{d} x\mathrm{d}z\mu_{1}(y)\mathrm{d}y+r\left\|\phi\right\|_{\infty}^{2},\]where the last inequality follows from the fact that \(\mu_{1}\ast\mu_{2}(\phi)=0\) and the convexity \(\left\|\cdot\right\|_{\infty}\). Combining (12) and (14), we have

\[\operatorname{Var}_{\mu_{1}\ast\mu_{2}}(\phi)\] \[\leq C_{\text{FPI}(\phi)}c_{d,\alpha}\iiint_{\{z\neq 0\}}\frac{( \phi(x+y+z)-\phi(x+y))^{2}}{|z|^{(d+\vartheta)}}\mathrm{d}z\mu_{2}(x)\mathrm{ d}x\mu_{1}(y)\mathrm{d}y\] \[\qquad+\beta_{\text{WFI}(\vartheta)}(r)c_{d,\alpha}\iint_{(z\neq 0 )}\int\frac{(\phi(x+y+z)-\phi(x+y))^{2}}{|z|^{(d+\vartheta)}}\mu_{2}(x)\mathrm{ d}x\mathrm{d}z\mu_{1}(y)\mathrm{d}y+r\left\|\phi\right\|_{\infty}^{2}\] \[=\bigl{(}\beta_{\text{WFI}(\vartheta)}(r)+C_{\text{FHI}( \vartheta)}\bigr{)}\,c_{d,\alpha}\iint_{(z\neq 0)}\frac{(\phi(u+z)-\phi(u))^{2}}{|z|^{(d+ \vartheta)}}\mathrm{d}z\mu_{1}\ast\mu_{2}(u)\mathrm{d}u+r\left\|\phi\right\|_ {\infty}^{2}\] \[=\bigl{(}\beta_{\text{WFI}(\vartheta)}(r)+C_{\text{FHI}( \vartheta)}\bigr{)}\,\mathcal{E}_{\mu_{1}\ast\mu_{2}}(\phi)+r\left\|\phi \right\|_{\infty}^{2}.\]

Lemma 7 is hence proved. 

### Proofs for the Generalized Cauchy Examples

In this section, we provide proofs for the two corollaries in Section 3.2.

Proof of Corollary 4.: According to [25, Corollary 1.2], \(\pi_{\nu}\) satisfies \(\alpha\)-FPI with parameter \(C_{\text{FPI}(\vartheta)}\) for any \(\alpha\leq\min(2,\nu)\). Therefore it follows from Theorem 3 that

\[\chi^{2}(\rho_{k}^{X}|\pi_{\nu})\leq\exp\left(-k\eta\left(C_{\text{FPI}( \alpha)}+\eta\right)^{-1}\right)\chi^{2}(\rho_{0}^{X}|\pi_{\nu}).\] (15)

According to [25, Corollary 22], when \(\rho_{0}^{X}=\mathcal{N}(0,I_{d})\) and \(d\geq 2\), \(R_{\infty}(\rho_{0}^{X}|\pi_{\nu})\leq\ln(2^{\nu/2}\Gamma(\nu/2))+\ln(\frac{ d+\nu}{2e})\) which implies \(\chi^{2}(\rho_{0}^{X}|\pi_{\nu})=\Theta(d)\). Therefore Corollary 4 follows from (15) and \(\eta\in(0,1)\). 

Proof of Corollary 5.: We prove the two part in the Corollary separately:

**(i) When \(\nu\geq 1\)**, according to [25, Corollary 1.2]\(\pi_{\nu}\) satisfies the \(1\)-FPI with parameter \(C_{\text{FPI}(1)}\). Corollary 3 applies with \(L=4(d+\nu)\) and \(\beta=1/4\) and the iteration complexity of Algorithm 2 is of order \(\mathcal{O}\bigl{(}C_{\text{FPI}(1)}d^{\frac{1}{2}}(d+\nu)^{4}\ln(\chi^{2}( \rho_{0}^{X}|\pi_{\nu})/\varepsilon)\bigr{)}\).

**(ii) When \(\nu\in(0,1)\)**, according to [25, Corollary 1.2], there exists a positive constant \(c\) such that \(\pi_{\nu}\) satisfies the \(1\)-wFPI with parameter

\[\beta_{\text{WFPI}(1)}(r)=c(1+r^{-(1-\nu)/\nu}).\] (16)

Theorem 5 implies that

\[\chi^{2}(\rho_{k}^{X}|\pi_{\nu}) \leq\exp\big{(}-\frac{k\eta}{\eta+c(1+r^{-(1-\nu)/\nu})}\big{)} \chi^{2}(\rho_{0}^{X}|\pi_{\nu})\] \[\qquad+r\bigl{(}1-\exp\big{(}-\frac{(k+1)\eta}{\eta+c(1+r^{-(1- \nu)/\nu})}\big{)}\big{)}\exp\big{(}2R_{\infty}(\rho_{0}^{X}|\pi^{X})\big{)}\] \[\leq\exp\big{(}-\frac{k\eta}{\eta+c(1+r^{-(1-\nu)/\nu})}\big{)} \chi^{2}(\rho_{0}^{X}|\pi_{\nu})\] \[\qquad+\frac{(k+1)\eta r}{\eta+c(1+r^{-(1-\nu)/\nu})}\exp\big{(}2 R_{\infty}(\rho_{0}^{X}|\pi^{X})\big{)}.\]

For any \(\varepsilon>0\) and \(k\geq 1\), pick \(r=\frac{\exp\big{(}-2\nu R_{\infty}(\rho_{0}^{X}|\pi_{\nu})\big{)}c^{\nu}\pi^{ \nu}}{(k+1)^{\nu}\pi^{\nu}}\), we have \(\chi^{2}(\rho_{k}^{X}|\pi_{\nu})\leq\varepsilon\) if

\[k\geq\bigl{[}1+c^{\frac{1}{2}}\eta^{-\frac{1}{2}}+2^{1/\nu}c\eta^{-1}\varepsilon ^{-(1-\nu)/\nu}\exp\big{(}\frac{2(1-\nu)R_{\infty}(\rho_{0}^{X}|\pi_{\nu})}{ \nu}\big{)}\bigr{]}\ln^{1/\nu}(\frac{2\chi^{2}(\rho_{0}^{X}|\pi_{\nu})}{ \varepsilon}).\]

Corollary 3 applies with \(L=(d+\nu)/\nu\) and \(\beta=\nu/4\). Therefore, by choosing \(\eta=\Theta(d^{-\frac{1}{2}}(d+\nu)^{-\frac{4}{\nu}})\), the iteration complexity in Algorithm 2 is of order

\[\mathcal{O}\biggl{(}\max\left\{c^{\frac{1}{d}}d^{\frac{1}{2}+\frac{4}{\nu^{2}}}, cd^{\frac{1}{2}+\frac{4}{\nu}}\varepsilon^{-\frac{1-\nu}{\nu}}}\exp\big{(} \frac{2(1-\nu)R_{\infty}(\rho_{0}^{X}|\pi_{\nu})}{\nu}\big{)}\right\}\ln^{ \frac{1}{\nu}}\big{(}\frac{2\chi^{2}(\rho_{0}^{X}|\pi_{\nu})}{\varepsilon} \big{)}\biggr{)}.\]Proofs for the Lower Bounds on the Stable Proximal Sampler

In this section we introduce the proofs for the lower bounds for the stable proximal sampler with parameter \(\alpha\) when the target is the generalized Cauchy density with degrees of freedom strictly smaller than \(\alpha\). The lower bound is proved following the idea introduced in Section 2.

**Lemma 8**.: _Suppose \((x_{k},y_{k})_{k}\) are the iterates of the stable proximal sampler with parameter \(\alpha\), step size \(\eta\) and target density \(\pi^{X}\propto\exp(-V)\) for some \(V:\mathbb{R}^{d}\to\mathbb{R}\). Let \(G(x)=\exp(\kappa V(x))\) with \(\kappa\in(0,1)\). Then, for every \(k\geq 0\),_

\[\mathbb{E}[G(x_{k+1})]\leq\mathbb{E}[G(x_{k}+2^{\frac{1}{\alpha}}\eta^{\frac{1 }{\alpha}}z_{k})],\]

_where \(z_{k}\), with density \(p_{1}^{(\alpha)}\), is sampled independently from \(x_{k}\)._

Proof of Lemma 8.: Recall that \(\pi^{X|Y}(x|y)\propto\pi^{X}(x)p^{(\alpha)}(\eta;x,y)\). We have

\[\mathbb{E}[G(x_{k+1})] =\mathbb{E}\big{[}\mathbb{E}[G(x_{k+1})|y_{k}]\big{]}=\mathbb{E} \big{[}Z_{y_{k}}^{-1}\int G(x)\pi^{X}(x)p_{\eta}^{(\alpha)}(x-y_{k})\mathrm{d }x\big{]}\] \[=\mathbb{E}\big{[}Z_{y_{k}}^{-1}\mathbb{E}[G(y_{k}+\eta^{\frac{1 }{\alpha}}z_{k})\pi^{X}(y_{k}+\eta^{\frac{1}{\alpha}}z_{k})|y_{k}]\big{]},\]

where \(Z_{y_{k}}=\int\pi^{X}(x)p_{\eta}^{(\alpha)}(x-y_{k})\mathrm{d}x=\mathbb{E}[\pi ^{X}(y_{k}+\eta^{\frac{1}{\alpha}}z_{k})|y_{k}]\) and \(z_{k}\) is the \(\alpha\)-stable random vector with density \(p_{1}^{(\alpha)}\), which is independent to \(y_{k},x_{k}\). Let \(T:\mathbb{R}_{+}\to\mathbb{R}\) be \(T(r)=r^{-\kappa}\). Since \(\kappa\in(0,1)\), \(T\) is convex and \(r\mapsto rT(r)\) is concave. According to the fact that \(G(x)=T(\pi^{X})(x)\) and Jensen's inequality, we have

\[\mathbb{E}[G(x_{k+1})] =\mathbb{E}\bigg{[}\frac{\mathbb{E}\big{[}(\pi^{X}T(\pi^{X}))(y_{ k}+\eta^{\frac{1}{\alpha}}z_{k})|y_{k}\big{]}}{\mathbb{E}\big{[}\pi^{X}(y_{k}+ \eta^{\frac{1}{\alpha}}z_{k})|y_{k}\big{]}}\bigg{]}\] \[\leq\mathbb{E}\big{[}T\big{(}\mathbb{E}[\pi^{X}(y_{k}+\eta^{\frac {1}{\alpha}}z_{k})|y_{k}]\big{)}\big{]}.\]

Since \(T\) is convex, apply Jensen's inequality again and we get

\[\mathbb{E}[G(x_{k+1})] \leq\mathbb{E}[G(y_{k}+\eta^{\frac{1}{\alpha}}z_{k})]=\mathbb{E} \big{[}\mathbb{E}[G(x_{k}+\eta^{\frac{1}{\alpha}}z^{\prime}_{k}+\eta^{\frac{1} {\alpha}}z_{k})|x_{k}]\big{]}\] \[=\mathbb{E}[G(x_{k}+2^{\frac{1}{\alpha}}\eta^{\frac{1}{\alpha}} \bar{z}_{k})|x_{k}],\]

where \(z^{\prime}_{k}\) is the \(\alpha\)-stable random vector with density \(p_{1}^{(\alpha)}\), which is independent to \(x_{k},z_{k}\) and the last identity follows from the self-similarity of \(\alpha\)-stable process with \(\bar{z}_{k}\sim p_{1}^{(\alpha)}\) which is independent to \(x_{k}\). 

**Lemma 9**.: _Suppose \((x_{k},y_{k})_{k}\) are the iterates of the stable proximal sampler with parameter \(\alpha\), step size \(\eta\) and target density \(\pi^{X}\propto\exp(-V)\) satisfies_

\[|\nabla V(x)|\leq\frac{(d+\nu_{2})|x|}{1+|x|^{2}}\quad\text{and}\quad\Delta V( x)\leq\frac{(d+\nu_{2})^{2}}{1+|x|^{2}},\]

_for some \(\nu_{2}\in(0,\alpha)\) and for all \(x\in\mathbb{R}^{d}\). Let \(G(x)=\exp(\kappa V(x))\) with_

\[\kappa\in(\nu_{2}(d+\nu_{2})^{-1},\alpha(d+\nu_{2})^{-1}).\]

_Then, for every \(k\geq 0\) and for all \(r>0\),_

\[\mathbb{E}[G(x_{k+1})]\leq(1+r)^{\frac{\kappa(d+\nu_{2})}{2}}\mathbb{E}[G(x_{ k})]+2^{\frac{\kappa(d+\nu_{2})}{\alpha}}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}}(1+r^{-1 })^{\frac{\kappa(d+\nu_{2})}{2}}m_{\kappa(d+\nu_{2})}^{(\alpha)},\] (17)

_where \(m_{\kappa(d+\nu_{2})}^{(\alpha)}=\mathbb{E}[|z_{k}|^{\kappa(d+\nu_{2})}]\) with \(z_{k}\) being an \(\alpha\)-stable random vector with density \(p_{1}^{(\alpha)}\). Moreover, for every \(N\geq 0\),_

\[\mathbb{E}[G(x_{N})]\lesssim\mathbb{E}[G(x_{0})]+m_{\kappa(d+\nu_{2})}^{( \alpha)}N^{\frac{\kappa(d+\nu_{2})}{2}+1}\eta^{\frac{\kappa(d+\nu_{2})}{ \alpha}},\] (18)

_where \(\lesssim\) is hiding a uniform positive constant factor._Proof of Lemma 9.: Without loss of generality assume \(V(0)=0\). Then, we have that,

\[V(x)=\int_{0}^{1}\langle x,\nabla V(tx)\rangle\mathrm{d}t\leq(d+\nu_{2})\int_{0}^ {1}\frac{t|x|}{1+|tx|^{2}}\mathrm{d}t=\frac{d+\nu_{2}}{2}\ln(1+|x|^{2}).\]

Therefore \(G(x)=\exp(\kappa V(x))\leq(1+|x|^{2})^{\kappa(d+\nu_{2})/2}\), Since \(\kappa\in(\nu_{2}(d+\nu_{2})^{-1},\alpha(d+\nu_{2})^{-1})\), \(G(x)=\mathcal{O}(|x|^{\kappa(d+\nu_{2})})\) when \(|x|\gg 1\) and \(\mathbb{E}[G(x_{k}+2^{\frac{1}{\alpha}}\eta^{\frac{1}{\alpha}}z_{k})]\) in Lemma 8 is finite. We have

\[\mathbb{E}[G(x_{k}+2^{\frac{1}{\alpha}}\eta^{\frac{1}{\alpha}}z_{ k})]\] \[\leq \mathbb{E}[(1+|x_{k}+2^{\frac{1}{\alpha}}\eta^{\frac{1}{\alpha}} z_{k}|^{2})^{\frac{\kappa(d+\nu_{2})}{2}}]\] \[\leq \mathbb{E}[(1+(1+r)|x_{k}|^{2}+4^{\frac{1}{\alpha}}\eta^{\frac{ \alpha}{\alpha}}(1+r^{-1})|z_{k}|^{2})^{\frac{\kappa(d+\nu_{2})}{2}}]\] \[\leq (1+r)^{\frac{\kappa(d+\nu_{2})}{2}}\mathbb{E}[G(x_{k})]+2^{\frac{ \kappa(d+\nu_{2})}{\alpha}}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}}(1+r^{-1})^ {\frac{\kappa(d+\nu_{2})}{2}}\mathbb{E}[|z_{k}|^{\kappa(d+\nu_{2})}]\] \[\leq (1+r)^{\frac{\kappa(d+\nu_{2})}{2}}\mathbb{E}[G(x_{k})]+2^{\frac{ \kappa(d+\nu_{2})}{\alpha}}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}}(1+r^{-1})^ {\frac{\kappa(d+\nu_{2})}{2}}m_{\kappa(d+\nu_{2})}^{(\alpha)},\]

where the first inequality follows from the Young's inequality and \(m_{\kappa(d+\nu_{2})}^{(\alpha)}=\mathbb{E}[|z_{k}|^{\kappa(d+\nu_{2})}]\) with \(z_{k}\) being an \(\alpha\)-stable random vector with density \(p_{1}^{(\alpha)}\). (17) follows from Lemma 8. Furthermore, by induction we have

\[\mathbb{E}[G(x_{N})] \leq(1+r)^{\kappa(d+\nu_{2})N/2}\mathbb{E}[G(x_{0})]\] \[\quad+\frac{(1+r)^{\kappa(d+\nu_{2})N/2}-1}{(1+r)^{\kappa(d+\nu_ {2})/2}-1}2^{\frac{\kappa(d+\nu_{2})}{\alpha}}\eta^{\frac{\kappa(d+\nu_{2})}{ \alpha}}(1+r^{-1})^{\frac{\kappa(d+\nu_{2})}{2}}m_{\kappa(d+\nu_{2})}^{(\alpha )}.\]

Pick \(r=\frac{2}{\kappa(d+\nu_{2})N}\) and (18) is proved. 

Proof of Theorem 4.: To apply Lemma 1, we choose \(G(x)=\exp(\kappa V(x))\) with \(\kappa\in(\nu_{2}(d+\nu_{2})^{-1},\alpha(d+\nu_{2})^{-1})\subset(0,1)\). Without loss of generality assume \(V(0)=0\). Via Assumption 1, we have the estimates for \(V\),

\[V(x)=\int_{0}^{1}\langle x,\nabla V(tx)\rangle\mathrm{d}t\geq(d+\nu_{1})\int_ {0}^{1}\frac{t|x|}{1+|tx|^{2}}\mathrm{d}t=\frac{d+\nu_{1}}{2}\ln(1+|x|^{2}).\]

By Lemma 2 we have

\[\pi^{X}(G(x)\geq y)\geq\pi^{X}\left(|x|\geq y^{\frac{1}{\kappa(d+\nu_{1})}} \right)\geq C_{\nu_{1}}d^{\frac{\nu_{1}}{2}}\left(1+y^{\frac{-2}{\kappa(d+\nu_ {1})}}\right)^{-\frac{d+\nu_{2}}{2}}y^{\frac{-\nu_{2}}{\kappa(d+\nu_{1})}}.\]

We then invoke Lemma 1 and Lemma 9 to obtain

\[\mathrm{TV}(\rho_{N}^{X},\pi^{X})\] \[\gtrsim\sup_{y\geq 1}C_{\nu_{1}}d^{\frac{\nu_{1}}{2}}\left(1+y^{ \frac{-2}{\kappa(d+\nu_{1})}}\right)^{-\frac{d+\nu_{2}}{2}}y^{\frac{-\nu_{2}} {\kappa(d+\nu_{1})}}-\frac{\mathbb{E}[G(x_{0})]+m_{\kappa(d+\nu_{2})}^{(\alpha) }N^{\frac{\kappa(d+\nu_{2})}{2}+1}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}}}{y}.\]

The fact that \(\kappa\in(\nu_{2}(d+\nu_{1})^{-1},\alpha(d+\nu_{2})^{-1})\) ensures that the supremum on the right side is always positive. In particular, picking \(y\) such that

\[y^{1-\frac{\nu_{2}}{\kappa(d+\nu_{1})}}=\Theta\Big{(}C_{\nu_{1}}^{-1}d^{- \frac{\nu_{2}}{2}}\big{(}\mathbb{E}[G(x_{0})]+m_{\kappa(d+\nu_{2})}^{(\alpha) }N^{\frac{\kappa(d+\nu_{2})}{2}+1}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}} \big{)}\Big{)},\]

we obtain that

\[\mathrm{TV}(\rho_{N}^{X},\pi^{X})\] \[\gtrsim \,C_{\nu_{1}}^{\frac{\kappa(d+\nu_{1})}{\kappa(d+\nu_{1})-\nu_{2} }}d^{\frac{\kappa(d+\nu_{1})\nu_{2}}{2\kappa(d+\nu_{1})-2\nu_{2}}}\big{(} \mathbb{E}[G(x_{0})]+m_{\kappa(d+\nu_{2})}^{(\alpha)}N^{\frac{\kappa(d+\nu_{2}) }{2}+1}\eta^{\frac{\kappa(d+\nu_{2})}{\alpha}}\big{)}^{-\frac{\nu_{2}}{\kappa(d +\nu_{1})-\nu_{2}}},\]

where \(\gtrsim\) is hiding a uniform positive constant factor. Therefore, for any \(\alpha\in(\frac{\nu_{2}(d+\nu_{2})}{d+\nu_{1}},2]\) and \(\delta\in(0,\alpha-\frac{\nu_{2}(d+\nu_{2})}{d+\nu_{1}})\), we can choose \(\kappa=\frac{\alpha-\delta}{d+\nu_{2}}\in(\frac{\nu_{2}}{d+\nu_{1}},\frac{ \alpha}{d+\nu_{2}})\) and get that

\[\mathrm{TV}(\rho_{N}^{X},\pi^{X})\] \[\geq \,C_{\nu_{1},\nu_{2},\delta}d^{\frac{\nu_{2}(\alpha-\delta)(d+\nu _{1})}{2(\alpha-\delta)(d+\nu_{1})-2\nu_{2}(d+\nu_{2})}}\big{(}\mathbb{E}[G(x_ {0})]+m_{\alpha-\delta}^{(\alpha)}N^{\frac{\alpha-\delta}{2}+1}\eta^{\frac{ \alpha-\delta}{\alpha}}\big{)}^{-\frac{\nu_{2}(d+\nu_{2})}{(\alpha-\delta)(d+ \nu_{1})-\nu_{2}(d+\nu_{2})}}.\]

Theorem 4 then follows by taking \(\tau=\alpha-\delta\).

### Further Discussions on Lower bounds of the stable proximal sampler

To derive a lower bound for the stable proximal sampler with parameter \(\alpha\), it is worth mentioning that there is an extra difficulty applying our method when \(\nu\geq\alpha\). Recall that when \(\nu\in(0,\alpha)\), \(\pi_{\nu}\) has heavier tail than \(\rho_{k}^{X}\) does. Therefore, when we apply

\[\mathrm{TV}(\rho_{k}^{X},\pi_{\nu})\geq|\pi_{\nu}(G\geq y)-\rho_{k}^{X}(G\geq y )|,\] (19)

to study the lower bound, it suffices to derive a lower bound on \(\pi_{\nu}(G\geq y)\), and an upper bound on \(\rho_{k}^{X}(G\geq y)\) which is smaller than the lower bound on \(\pi_{\nu}(G\geq y)\). Deriving these bounds is not too hard: the lower bound can be obtained by looking at an explicit integral against \(\pi_{\nu}\) directly and the upper bound is derived based on the fractional absolute moment accumulation of the isotropic \(\alpha\)-stable random variables along the stable proximal sampler.

However, when \(\nu\geq\alpha\), we expect that \(\rho_{k}^{X}\) has heavier tail than \(\pi_{\nu}\). Therefore, to apply (19), we need to find an upper bound on \(\pi_{\nu}(G\geq y)\), and a lower bound on \(\rho_{k}^{X}(G\geq y)\) which is smaller than the upper bound on \(\pi_{\nu}(G\geq y)\). Notice that \(\rho_{k}^{X}(G\geq y)\) is a quantity varying along the trajectory of the stable proximal sampler. Deriving a lower bound along the trajectory is essentially more challenging than deriving an upper bound.

In order to derive a satisfying lower bound in this case, it hence remains to characterize the stable proximal sampler as an approximation of an appropriate gradient flow, just as that the Brownian-driven proximal sampler can be interpret as the entropy-regularized JKO scheme in [20]; see also Section 5. To understand this kind of gradient flow approximations itself is an interesting future work as it may help us to understand and characterize the class of MCMC samplers that utilize heavy-tail samples to approximate lighter-tail target densities, which is non-standard compared to commonly used MCMC samplers such as ULA, MALA, etc.

## Appendix D Numerical Illustrations

In this section, we present numerical results that illustrate the improved performance of the proximal sampler with stable oracles (\(\alpha=1\)) compared to that with Gaussian oracles. We first sample from the one-dimensional student-t distribution with center zero and \(4\) degrees of freedom by running the proximal samplers with different oracles in parallel for 100 times. Each individual chain is run for 100 iterations with step-size \(\eta=0.1\). Figures 1,2,3 present the convergence results for different initializations \(x_{0}=20,5,-5\) respectively. In each figure, the first column shows the means and variances of the iterates along the trajectories; the center column shows the histograms of the last iterates and the target density (red curve); the last column shows the convergence of Wasserstein-\(2\) distance along the trajectories. We also sample from the two-dimensional student-t distribution with center at the origin and \(4\) degrees of freedom by running the proximal samplers with different oracles in parallel for 30 times. Each individual chain is run for 20 iterations with step-size \(\eta=0.1\) with the initialization at \(x_{0}=[5,1]\). In Figure 4, we present the convergence results, the first column showing the means and variances of the first-coordinates along the trajectories, the center column showing the histograms of first-coordinate in the last iterates and the first-coordinate marginal density of the target distribution (red curve), and the last column showing the convergence of Wasserstein-\(2\) distance along the trajectories.

Figure 1: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-dimensional student-t with center \(0\) and \(4\) degrees of freedom; initialization is chosen \(x_{0}=20\).

Figure 4: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be two-dimensional student-t with center \((0,0)\) and \(4\) degrees of freedom; initialization is chosen \(x_{0}=[5,1]\).

Figure 3: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-dimensional student-t with center \(0\) and \(4\) degrees of freedom; initialization is chosen \(x_{0}=-5\).

Figure 2: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be one-dimensional student-t with center \(0\) and \(4\) degrees of freedom; initialization is chosen \(x_{0}=5\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claim in the abstract and introduction is the separation result between Gaussian and Proximal Sampler. The rest of the sections are exactly stating (and proving) the aforementioned separation result. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions are listed in the respective theorem. The (correct) proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have read the Ethics Guideline and followed it in the paper preperation. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work is primarily theoretical. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.