# Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice

 Aiwen Xu

Department of Computer Science

University of California, Santa Barbara

Santa Barbara, CA 93117

aiwenxu@ucsb.edu

&Yuchen Hou

Department of Computer Science

University of California, Santa Barbara

Santa Barbara, CA 93117

yuchenhou@ucsb.edu

&Cristopher M. Niell

Department of Biology, Institute of Neuroscience

University of Oregon

Eugene, OR 97403

cniell@uoregon.edu

&Michael Beyeler

Department of Computer Science

Department of Psychological & Brain Sciences

University of California, Santa Barbara

Santa Barbara, CA 93117

mbeyeler@ucsb.edu

###### Abstract

Despite their immense success as a model of macaque visual cortex, deep convolutional neural networks (CNNs) have struggled to predict activity in visual cortex of the mouse, which is thought to be strongly dependent on the animal's behavioral state. Furthermore, most computational models focus on predicting neural responses to static images presented under head fixation, which are dramatically different from the dynamic, continuous visual stimuli that arise during movement in the real world. Consequently, it is still unknown how natural visual input and different behavioral variables may integrate over time to generate responses in primary visual cortex (V1). To address this, we introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity in freely moving mice. We show that the model achieves state-of-the-art predictions of V1 activity during free exploration and demonstrate the importance of each component in an extensive ablation study. Analyzing our model using maximally activating stimuli and saliency maps, we reveal new insights into cortical function, including the prevalence of mixed selectivity for behavioral variables in mouse V1. In summary, our model offers a comprehensive deep-learning framework for exploring the computational principles underlying V1 neurons in freely-moving animals engaged in natural behavior.

## 1 Introduction

Computational models have been crucial in providing insight into the underlying mechanisms by which neurons in the visual cortex respond to external stimuli. Deep convolutional neural networks (CNNs) have had immense success as predictive models of the primate ventral stream, in cases wherethe animal was passively viewing stimuli or simply maintaining fixation [1; 2; 3; 4; 5]. Despite their success, these CNNs are poor predictors of neural responses in mouse visual cortex [6], which is thought to be shallower and more parallel than that of primates [7; 8]. According to the best models in the literature [9; 10; 11; 12; 13; 14], the mouse visual system is more broadly tuned and operates on relatively low-resolution inputs to support a variety of behaviors [15]. However, these models were limited to predicting neural responses to controlled (and potentially ethologically irrelevant) stimuli that were passively viewed by head-fixed animals.

Movement is a critical element of natural behavior. In the visual system, eye and head movements during locomotion and orienting transform the visual scene in potentially both beneficial (e.g., by providing additional visual cues) and detrimental ways (e.g., by introducing confounds due to self-movement) [16; 17; 18]. Movement-related activity is widespread in mouse cortex [19; 20] and prevalent in primary visual cortex (V1) [21; 22]. For instance, V1 neurons of freely moving mice show robust responses to head and eye position [23; 24], which may contribute a multiplicative gain to the visual response [25] that cannot be replicated under head fixation. V1 activity may be further modulated by variables that depend on the state of the animal and its behavioral goals [20; 22; 26; 27]. However, how these behavioral variables may integrate to modulate visual responses in V1 is unknown. Furthermore, a comprehensive predictive model of V1 activity in freely moving animals is still lacking.

To address these challenges, we make the following contributions:

* We introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity during natural vision in freely moving mice.
* We show that the model achieves state-of-the-art predictions of V1 activity during free exploration based on visual input and behavior, demonstrating the ability to accurately model neural responses in the dynamic regime of movement through the visual scene.
* We uncover new insights into cortical neural coding by analyzing our model with maximally activating stimuli and saliency maps, and demonstrate that mixed selectivity of visual and behavioral variables is prevalent in mouse V1.

## 2 Related Work

The mouse, as a model organism, offers unparalleled experimental access to the mammalian cerebral cortex [28]. Computational models of mouse V1, including generalized linear models (GLMs) [25; 29] and customized models mimicking the mouse visual hierarchy [30], have been crucial in providing deeper insights into the range of computations performed by visual cortex. More recently, deep CNNs have also been used to model mouse V1 [9; 10; 11; 12; 13; 31; 32; 33; 34].

Despite their success in predicting neural activity in the macaque visual cortex [35], deep CNNs trained on ImageNet have had limited success in predicting mouse visual cortical activity [6]. This is perhaps not surprising, as most ImageNet stimuli belong to static images of human-relevant semantic categories and may thus be of low ethological relevance for rodents. More importantly, these deep CNNs may not be the ideal architecture to model mouse visual cortex, which is known to be shallower and more parallel than primate visual cortex [36; 37]. In addition, mice are known to have lower visual acuity than that of primates [7; 8], and much of their visual processing may be devoted to active, movement-based behavior rather than passive analysis of the visual scene [38; 22; 39]. Although the majority of V1 neurons is believed to encode low-level visual features [40], their activity is often strongly modulated by behavioral variables related to eye and head position [23; 24; 25], locomotion [21; 22; 18], arousal [27; 41], and the recent history of the animal [26]. Furthermore, mouse V1 is highly interconnected with both cortical and subcortical brain areas, which contrasts with feedforward, hierarchical models of visual processing [22].

A common architectural approach that has proved quite successful is to split the network into different components (first introduced by [11]):

* a "core" network, which typically consist of a CNN used to extract convolutional features from the visual stimulus [11; 12; 25; 42], sometimes in combination with a recurrent network [11];
* a "shifter" network, which mimics gaze shifts by learning a (typically affine) transformation from head- to eye-centered coordinates, either applied to the pixel input [11; 25] or a CNN layer [12];
* a "readout" network, which learns a mapping from artificial to biological neurons [11; 12; 42].

Owing to the difficulty of developing a predictive model of mouse cortex, Willeke _et al._[14] recently invited submissions to the Sensorium competition held at NeurIPS '22. The competition introduced a benchmark dataset of V1 neural activity recorded from head-fixed mice on a treadmill viewing static images, with simultaneous measurements of running speed, pupil size, and eye position. A baseline model was provided as well, which consisted of a 4-layer CNN core in combination with a shifter and readout network [12]. Even though 26 teams submitted 194 different models, the overall improvement to the baseline performance was modest, raising the single trial correlation from \(.287\) to \(.325\) in the Sensorium and from \(.384\) to \(.453\) in the Sensorium+ competition. Architectural innovations (e.g., Transformers, Normalizing Flows, YOLO, and knowledge distillation), were unable to make an impact, as most improvements were gained from ensemble methods. A promising direction was taken by the winning model, which attempted to learn a latent representation of the "brain state" from the various behavioral variables, inspired by [20]. However, the model utilized the timestamps of the test set to estimate recent neuronal activities, which the other competitors did not have access to.

Taken together, we identified three main limitations of previous work that this study aims to address:

* **Head-fixed preparations.** Most previous models operated on data from animals in head-fixed conditions with static stimuli, which do not mirror natural behavior and thus provide limited insight into visual processing in real-world environments. In contrast, the present work is applied to state-of-the-art neurophysiological recordings of V1 activity in freely moving mice. This represents a dramatic shift in the "parameter space" of visual input, from static images to dynamic, real-world visual input. One could imagine that this will make the modeling process more difficult, because the stimulus set is more complex, or easier, because it is more matched to the computational challenge the brain evolved for.
* **Limited influence of behavioral state.** Previous models often limited the influence of behavioral state to eye measurements and treadmill running speed, which were either concatenated with the visual features [14; 41], utilized in the shifter network to determine the gaze-contingent retinal input [11; 14], or used to predict a multiplicative gain factor [11].
* **Missing temporal dynamics.** Most previous modeling works ignored the temporal factors that might influence V1 activity and overlooked the dynamic nature of visual processing (but see [11]). We overcome this limitation by utilizing approximately 1-hour-long recordings of three mice freely exploring an arena, and our model is capable of handling continuous data streams of any length.

## 3 Methods

Head-mounted recording systemWe had access to data from three adult mice who were freely exploring \(48\,\mathrm{cm}\) long \(\times\)\(37\,\mathrm{cm}\) wide \(\times\)\(30\,\mathrm{cm}\) high arena (Fig. 1A), collected with a state-of-the-art recording system [25] that combined high-density silicon probes with miniature head-mounted cameras (Fig. 1B). One camera was aimed outwards to capture the visual scene from the mouse's perspective ("worldcam") at \(16\,\mathrm{ms}\) per frame (downsampled to \(60\times 80\) pixels). A second camera, aimed at the eye, was used to extract eye position (\(\theta\), \(\phi\)) and pupil radius (\(\sigma\)) at \(30\,\mathrm{Hz}\) using DeepLabCut [43]. Pitch (\(\rho\)) and roll (\(\omega\)) of the mouse's head position were extracted at \(30\,\mathrm{kHz}\) from the inertial measurement unit (IMU). \(\theta\), \(\phi\), \(\rho\), and \(\omega\) allowed for the worldcam video to be corrected for eye movements: A 3-layer fully-connected shifter network (where each linear layer was accompanied by Tanh and BatchNorm) was trained to predict a rotation (bounded by \(\pm 36^{\circ}\)) and a shift (bounded by \(\pm 16\) pixels horizontally and \(\pm 12\) pixels vertically) based on \(\theta\), \(\phi\), \(\rho\), and \(\omega\) to convert each frame to head- to eye-centered coordinates. Locomotion speed (\(s\)) was estimated from the top-down camera feed using DeepLabCut [43]. Electrophysiology data was acquired at \(30\,\mathrm{kHz}\) using a \(11\,\mathrm{\SIUnitSymbolMicro m}\times 15\,\mathrm{\SIUnitSymbolMicro m}\) multi-shank linear silicon probe (128 channels) implanted in the center of the left monocular V1, then bandpass-filtered between \(0.01\,\mathrm{Hz}\) and \(7.5\,\mathrm{kHz}\), and spike-sorted with Kilosort 2.5 [44]. Single units were selected using Phy2 (see [45]) and inactive units (mean firing rate \(<3\,\mathrm{Hz}\)) were removed. This yielded 68, 32, and 49 active units for Mouse 1-3, respectively. To prepare the data for machine learning, all data streams were deinterlaced and resampled at \(20.83\,\mathrm{Hz}\) (\(48\,\mathrm{ms}\) per frame; Fig. 1C). For a more detailed description of the dataset, see Appendix A and Ref. [25].

Model architectureWe used a 3-layer CNN (kernel size 7, \(128\times 64\times 32\) channels) to encode the visual stimulus. Each convolutional layer was followed by a BatchNorm layer, a ReLU, and a Dropout layer (0.5 rate). A fully-connected layer transformed the learned visual features into a visual feature vector, \(\bm{v}\) (Fig. 2, _top-right_). In a purely visual version of the model, \(\bm{v}\) was fed into a fully-connected layer, followed by a softplus layer, to yield a neuronal response prediction.

To encode behavioral state, we constructed an input vector from different sets of behavioral variables:

* \(\mathcal{S}\): all behavioral variables used in the Sensorium+ competition [14], consisting of running speed (\(s\)), pupil size (\(\sigma\)), and its temporal derivative (\(\dot{\sigma}\));
* \(\mathcal{B}\): all behavioral variables used in [25], consisting of eye position (\(\theta\), \(\phi\)), head position (\(\rho\), \(\omega\)), pupil size (\(\sigma\)), and running speed (\(s\));
* \(\mathcal{D}\): the first-order derivatives of the variables in \(\mathcal{B}\), namely \(\dot{\theta}\), \(\dot{\phi}\), \(\dot{\omega}\), \(\dot{\rho}\), \(\dot{\sigma}\), and \(s\).

To test for interactions between behavioral variables, these sets could also include the pairwise multiplication of their elements; e.g., \(\mathcal{B}_{\times}=\{b_{i}b_{j}\ \forall\ (b_{i},b_{j})\in\mathcal{B}\}\). The input vector was then passed through a batch normalization layer and a fully connected layer (subjected to a strong L1 norm for feature selection) to produce a behavioral vector, \(\bm{b}\).

We then concatenated the vectors \(\bm{v}\), \(\bm{b}\), and their element-wise product \(\bm{v}\odot\bm{b}\) (all calculated for each individual input frame), fed them through a batch normalization layer, and input them to a 1-layer gated recurrent unit (GRU) (hidden size of 512). To incorporate temporal dynamics, we constructed different versions (GRU\({}_{k}\)) of the model that had access to \(k\) previous frames. A fully-connected layer and a softplus activation function were applied to yield the neuronal response prediction.

Training and model evaluationSince the visual input depended on the movement of the mouse and the mouse could be in very different behavioral states over the length of the recording, the data was highly inhomogeneous across time. To deal with the continuous and dynamic nature of the data, we therefore split the \(\sim 1\,\mathrm{h}\)-long recording into 10 consecutive segments. The first \(70\,\mathrm{\char 37}\) of each segment were then reserved for training (including an 80-20 validation split) and the remaining \(30\,\mathrm{\char 37}\) for testing.

Note that it is unlikely for data to "leak" from the train segment into the test segment. While it is possible that the mouse could have been exploring the same part of the arena at different segments of the recording, it was free to move its head, eyes, and body as it saw fit. Thus two duplicate data points could only be produced by the animal exactly duplicating the time courses of its eye, head, and body movement in the exact same location of the arena.

Figure 1: Schematic of the head-mounted recording system for freely moving mice (adapted from [25]). A) Three mice freely explored a \(48\,\mathrm{cm}\) long \(\times\)\(37\,\mathrm{cm}\) wide \(\times\)\(30\,\mathrm{cm}\) high arena. B) Preparation included a silicon probe for electrophysiological recording in V1 (yellow), miniature cameras for recording the mouse’s eye position and pupil size (\(\theta\), \(\phi\), and \(\sigma\); magenta), and visual scene (blue), and inertial measurement unit for measuring head orientation (\(\rho\) and \(\omega\); green). C) Sample data from a \(9.6\,\mathrm{s}\) period during free movement showing (from top) visual scene, horizontal and vertical eye position, pupil size, head pitch and roll, locomotor speed, and a raster plot of 64 units.

Models were separately trained on the data from each mouse. Model parameters were optimized with Adam (batch size: 256, CNN learning rate: \(.0001\), full model: \(.0002\)) to minimize the Poisson loss between predicted neuronal response (\(\hat{r}\)) and ground truth (\(r\)) : \(\frac{1}{N}\sum_{i=1}^{N}(\hat{r}_{i}-r_{i}\log\hat{r}_{i})\), where \(N\) denotes the number of recorded neurons for each mouse. We used early stopping on the validation set (patience: 5 epochs), which led all models to converge in less than 50 epochs. Due to the large number of hyper-parameters, the specific network and training settings were determined using a combination of grid search and manual exploration on a validation set (see Appendix B).

To evaluate model performance, we calculated the cross-correlation (\(cc\)) between a smoothed version (\(2\,\mathrm{s}\) boxcar filter) of the predicted and ground-truth response for each recorded neuron [25].

All models were implemented in PyTorch and trained on an NVIDIA RTX 3090 with 24GB memory. All code, data used to train the models, and weights of the trained model can be found at https://github.com/bionicvisionlab/2023-Xu-Multimodal-Mouse-V1.

Maximally activating stimuliWe used gradient ascent [42] to discover the visual stimuli that most strongly activate a particular model neuron in our network. The visual input was initialized with noise sampled in \(\mathcal{N}(.5,2)\). The behavioral variables were initialized to a vector of all ones, and updated in the loop with the visual stimuli. We used the Adam optimizer to repeatedly add the gradient of the target neuron's activity with respect to its inputs. We also applied L2 regularization (weight of.02) and Laplacian regularization (weight of 0.01) [46] on the image. This procedure was repeated 6400 times. The resulting, maximally activating visual stimuli were smoothed with a Butterworth filter (low-pass,.05 cutoff frequency ratio) to reduce the impact of high-frequency noise.

Saliency mapWe computed a saliency map [47] of the behavioral vector for each neuron to discover which behavioral variables contributed most strongly to each model neuron's activity. We iterated through the test dataset, recorded the gradient of each behavioral input with respect to each neuron's prediction, and then averaged the gradients per neuron to obtain the saliency map.

## 4 Results

Mouse V1 activity is best predicted with a 3-layer CNNTo determine the purely visual contribution to V1 responses, we experimented with a large number of vision architectures (see Appendix B). In the end, a vanilla 3-layer CNN (kernel size 7, \(128\times 64\times 32\) channels) yielded

Figure 2: Model architecture diagram. The vision-only network (top-right) was a CNN network, predicting the neural activity at time \(t\) given the visual input at time \(t-1\) (\(48\,\mathrm{ms}\) bins). The full model combined the CNN with a behavioral encoder and a gated recurrent unit (GRU), predicting the neural activity at time \(t\) given the visual and behavioral inputs from time \(t-1\) to \(t-n\).

the best cross-correlation between predicted and ground-truth responses (Table 1), outperforming the best autoencoder architecture (kernel size: 7, encoder: \(64\times 128\times 256\) channels, decoder: \(256\times 128\times 64\) channels), ResNet-18 [48] (a 20-layer CNN with the first input channel being replaced by 1), EfficientNet-B0 [49] (a 65-layer CNN with the first input channel being replaced by 1), and the Sensorium baseline [12] (a 4-layer CNN with a readout network). The greatest improvement in cross-correlation was achieved for Mouse 1.

Behavioral variables improve most neuronal predictionsOnce we identified the 3-layer CNN as the best visual encoder, we added the different sets of behavioral variables to the network. To allow for a fair comparison with the Sensorium+ baseline [14], we first limited ourselves to \(\mathcal{S}=\{\sigma,\hat{\sigma},s\}\), but then gradually added more behavioral variables (\(\mathcal{B}\)) [25] as well the derivatives of these variables (\(\mathcal{D}\)) and multiplicative pairs (\(\mathcal{B}_{\times}\) and \(\{\mathcal{B}\cup\mathcal{D}\}_{\times}\)).

The results are shown in Table 2. All models were able to outperform the Sensorium+ baseline, and the addition of behavioral variables and their interactions further improved model performance. Note that although the full model used a GRU to combine visual and behavioral features, the input sequence length was always 1 (i.e., \(\text{GRU}_{1}\)). That being said, it is possible that the GRU learned long-term correlations that the Sensorium+ baseline model did not have access to. Nevertheless, the biggest performance improvements were gained through the addition of behavioral variables related to head and eye position (which are present in \(\mathcal{B}\) but not in \(\mathcal{S}\)), their derivatives (\(\mathcal{D}\)), and multiplicative interactions between these variables (\(\{\mathcal{B}\cup\mathcal{D}\}_{\times}\)).

We also wondered whether the prediction of only some V1 neurons would benefit from the addition of these behavioral variables. To our surprise, the cross-correlation between predicted and ground-truth responses improved for almost all recorded V1 neurons (Fig. 3).

\begin{table}
\begin{tabular}{l l|c c c c c c c}  & & \multicolumn{3}{c}{Mouse 1} & \multicolumn{3}{c}{Mouse 2} & \multicolumn{3}{c}{Mouse 3} \\ Model & & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) \\ \hline CNN & \(\mid\) & \(\mathbf{.596\pm.134}\) & \(\mathbf{.0626}\) & \(\mathbf{.424\pm.141}\) & \(\mathbf{.100}\) & \(\mid\) & \(\mathbf{.552\pm.138}\) & \(\mathbf{.0908}\) \\ Autoencoder & \(\mid\) & \(.555\pm.140\) & \(.0710\) & \(\mid\) & \(.370\pm.145\) & \(.112\) & \(\mid\) & \(.521\pm.144\) & \(.0974\) \\ ResNet-18 [48] & \(\mid\) & \(.517\pm.159\) & \(.0782\) & \(.366\pm.171\) & \(.107\) & \(\mid\) & \(.511\pm.138\) & \(.0944\) \\ EfficientNet-B0 [49] & \(\mid\) & \(.542\pm.153\) & \(.0694\) & \(.393\pm.165\) & \(.103\) & \(\mid\) & \(.510\pm.127\) & \(.0965\) \\ Sensorium [12] & \(\mid\) & \(.519\pm.149\) & \(.0754\) & \(.381\pm.128\) & \(.119\) & \(\mid\) & \(.497\pm.136\) & \(.100\) \\ \end{tabular}
\end{table}
Table 1: Best-performing vision models, compared to the Sensorium baseline [12] (see Appendix B for more). Best-performing networks are indicated in bold. \(cc\): cross-correlation, mean \(\pm\) standard deviation across neurons (\(\uparrow\): the higher the better), MSE: mean-squared error (\(\downarrow\): the lower the better).

Access to longer series of data in time further improves predictive performanceAfter we identified the full behavioral feature set (\(\{\mathcal{B}\cup\mathcal{D}\}_{\times}\)) as the one yielding the best model performance, we extended the GRU's temporal dependence by allowing the input to vary from one frame (\(48\,\mathrm{ms}\)) to a total of eight frames (\(384\,\mathrm{ms}\)), and assessed the model's performance.

The results are shown in Table 3. The amount of temporal information needed by the model to reach peak predictive performance was similar across mice (\(288\,\mathrm{ms}\), \(192\,\mathrm{ms}\), and \(192\,\mathrm{ms}\) in terms of cross-correlation, \(192\,\mathrm{ms}\), \(192\,\mathrm{ms}\), and \(192\,\mathrm{ms}\) in terms of mean-squared error, respectively). This indicates that temporal information is important for predicting dynamic neural activity. However, the dependence on temporal information has a limit, and different neurons in V1 might possess different temporal capacities.

Well-defined visual receptive fields emergeTo assess whether the CNN+GRU\({}_{1}\) model learned meaningful visual receptive fields, we used gradient ascent (see Methods) to find the maximally activating stimulus for each neuron. Receptive fields for the 32 best-predicted neurons are shown in Fig. 4. Interestingly, most of them had well-defined excitatory and inhibitory subregions, often resembling receptive fields of orientation-selective neurons. Most excitatory and inhibitory subregions spanned approximately \(30^{\circ}\) of visual angle (the full width of the frame, 80 pixels, roughly corresponding to \(120^{\circ}\) of visual angle), which is roughly on the same order of magnitude compared to receptive field sizes typically observed in mouse V1, varying from \(10^{\circ}\) to \(30^{\circ}\)[8, 25, 50].

Receptive fields were noticeably different across mice. Whereas Mouse 1 and 3 had visual receptive fields with strongly excitatory subregions, most model neurons for Mouse 2 appeared to be inhibited

Figure 3: The integration of behavioral variables improved the cross-correlation (\(cc\)) for the majority of neurons. Each dot represents a neuron. A dot above the dashed diagonal indicates a higher \(cc\) with the inclusion of behavioral variables. Histograms (small insets) illustrate the distribution of the improvement in \(cc\) across the neuronal population.

\begin{table}
\begin{tabular}{l c|c c c|c c c}  & \multicolumn{3}{c}{Mouse 1} & \multicolumn{2}{c}{Mouse 2} & \multicolumn{2}{c}{Mouse 3} \\ Model & History & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) \\ \hline CNN+GRU\({}_{1}\) & \(48\,\mathrm{ms}\) & \(.646\pm.136\) & \(.0543\) & \(.508\pm.166\) & \(.0917\) & \(.607\pm.132\) & \(.0801\) \\ CNN+GRU\({}_{2}\) & \(96\,\mathrm{ms}\) & \(.649\pm.139\) & \(.0528\) & \(.506\pm.174\) & \(.0898\) & \(.607\pm.133\) & \(.0811\) \\ CNN+GRU\({}_{3}\) & \(144\,\mathrm{ms}\) & \(.653\pm.139\) & \(.0528\) & \(.528\pm.160\) & \(.0843\) & \(.604\pm.134\) & \(.0790\) \\ CNN+GRU\({}_{4}\) & \(192\,\mathrm{ms}\) & \(.650\pm.142\) & \(.0525\) & \(.566\pm.169\) & \(.0799\) & \(.614\pm.136\) & \(.0773\) \\ CNN+GRU\({}_{5}\) & \(240\,\mathrm{ms}\) & \(.645\pm.144\) & \(.0556\) & \(.519\pm.177\) & \(.0933\) & \(.598\pm.134\) & \(.0807\) \\ CNN+GRU\({}_{6}\) & \(288\,\mathrm{ms}\) & \(.654\pm.142\) & \(.0526\) & \(.549\pm.175\) & \(.0823\) & \(.598\pm.138\) & \(.0798\) \\ CNN+GRU\({}_{7}\) & \(336\,\mathrm{ms}\) & \(.644\pm.148\) & \(.0534\) & \(.533\pm.169\) & \(.0931\) & \(.596\pm.133\) & \(.0806\) \\ CNN+GRU\({}_{8}\) & \(384\,\mathrm{ms}\) & \(.646\pm.146\) & \(.0547\) & \(.546\pm.179\) & \(.0840\) & \(.594\pm.141\) & \(.0825\) \\ \end{tabular}
\end{table}
Table 3: CNN+GRU\({}_{k}\) model trained with input from \(k\) timesteps on the full feature set (\(\{\mathcal{B}\cup\mathcal{D}\}_{\times}\)). Best performing networks are indicated in bold. \(cc\): cross-correlation, mean \(\pm\) standard deviation (\(\uparrow\): the higher the better), MSE: mean-squared error (\(\downarrow\): the lower the better).

by visual signals (same colorbar across panels). In addition, several model neurons lacked pronounced visual receptive fields, indicating that they were more strongly driven by behavioral variables. Even though model fits were repeated with different initial values for the behavioral variables, the resulting visual receptive fields looked qualitatively the same (see Appendix C), thus demonstrating the validity of the generated receptive fields. In addition, even some of the best-predicted neurons lack a pronounced or spatially structured receptive field, implying that these neurons could be primarily driven by behavioral variables.

Analysis of behavioral saliency maps reveals different types of neuronsIntrigued by the fact that some neurons lacked pronounced visual receptive fields, we aimed to analyze the influence of behavioral state on the predicted neuronal response by performing a saliency map analysis on the behavioral inputs (see Methods). Since different behavioral variables operate on different input ranges, we first standardized the saliency map activities for each behavioral variable across the model neuron population. Saliency map activities further than 1 standard deviation from the mean were then interpreted as "driving" the neuron, allowing us to categorize each neuron as being driven by one or multiple behavioral variables (Fig. 5).

We first asked which neurons in our model were driven by which behavioral variables (Fig. 5, _top_). Consistent with [25], we found a large fraction of model neurons driven by eye and head position, and smaller fractions driven by locomotion speed and pupil size. Approximately 20-30% of neurons were not driven by any of these behavioral variables, rendering their responses purely visual.

However, a particular neuron could be driven by multiple behavioral variables. Repeating the above analysis, we found that most model neurons showed mixed selectivity (i.e., responding to different categories of information, such as visual and motor signals, or stimulus and reward signals), with only a minority of cells responding exclusively to a single behavioral variable, (Fig. 5, _middle_). Adding the interaction terms between behavioral variables (Fig. 5, _bottom_) did not change the fact that most model V1 neurons encoded combinations of multiple behavioral variables, often relating information about the animal's eye position to head position and locomotor speed.

## 5 Discussion

In this paper, we propose a deep recurrent neural network that achieves state-of-the-art predictions of V1 activity in freely moving mice. We discovered that our model outperforms previous models under these more naturalistic conditions, which could be attributed to the better alignment of this data with the computations performed by the mouse visual system, based on its natural visual environment and

Figure 4: The maximally activating stimuli learned in CNN+GRU\({}_{1}\), generated via gradient ascent. The 32 neurons with the highest cross-correlation (\(cc\)) from each mouse are shown, sorted by \(cc\).

behavioral characteristics. Similar to previous models, we found that a simple CNN architecture is sufficient to predict the visual response properties of cells in mouse V1.

In addition, mouse V1 is known to be strongly modulated by signals related to the movement of the animal's eyes, head, and body [21, 22, 25], which are severely restricted in head-fixed preparations. Models trained on head-fixed preparations may thus be limited in their predictive power. In contrast, our model was able to predict V1 activity on a 1-hour continuous data stream, during which the animal freely explored a real-world arena. Our analyses demonstrate the impact of the animal's behavioral state on V1 activity and reveal that most model V1 neurons exhibit mixed selectivity to multiple behavioral variables.

Accurate predictions of mouse V1 activity under natural conditionsOur brains did not evolve to view stationary stimuli on a computer screen. However, most research on neural coding in vision has been conducted under head-fixed conditions, which do not mirror natural behavior and thus provide limited insight into visual processing in real-world environments. Some visual functions mediated by the ventral stream, such as identifying faces and objects, resemble this condition, but the real visual environment is constantly shifting due to self-motion, leading to dynamic activities such as navigation or object reaching, typically mediated by the dorsal stream. To truly understand visual perception in natural environments, we need to capture the computational principles when the subjects are in motion.

In this research, we take the initial steps towards this by modeling a novel data type encompassing neural activity coupled with a visual scene captured from a freely moving animal's perspective. This represents a dramatic (but, in our opinion, crucial) shift in the "parameter space" of visual input, from static images projected on a screen to dynamic, real-world visual input.

Figure 5: Effect of behavioral variables on model neuron activity, inferred by the saliency analysis. A) Fraction of neurons that are “driven by” (i.e., their saliency map activation is further than 1 standard deviation from the mean) different behavioral variables (similar to [25]). A neuron that responds to (e.g.) both position and speed may be counted twice. Neurons without a strong behavioral drive are categorized as “vision only”. B) Fraction of neurons that are uniquely driven by a specific behavioral variable. Still, a large fraction of neurons are driven by multiple behavioral variables. C) Same as B), but split with interaction terms.

Surprisingly, visual responses were best predicted with a standard three-layer ("vanilla") CNN (Table 1), as compared to a multitude of more sophisticated models that included autoencoders, variational autoencoders, filter bank models, and pre-trained ResNet and EfficientNet architectures. One possible explanation might be that the neurons in our dataset were selective for other behavioral inputs that we did not have access to, and that the vanilla CNN architecture imposed the fewest assumptions about how visual input contributed to the neural activity. In addition, visual receptive fields for Mouse 2 were noticeably different from the other two mice, exhibiting pronounced inhibitory subregions (Fig. 4, _center_). This is consistent with the fact that the cortical probes of Mouse 2 were more superficial compared to the other two mice [25], so the recorded neurons may have both different anatomical inputs and different visual responses.

Mixed selectivity of behavioral variablesOur experiments demonstrated that the models incorporating behavioral variables and their interactions performed substantially better than the models relying exclusively on visual inputs. Moreover, our saliency map analysis showed that only around 25% of model neurons could be considered purely visual, with the majority of model neurons driven by multiple behavioral variables.

This widespread mixed selectivity is consistent with previous literature suggesting that V1 neurons may be modulated by a high-dimensional latent representation of several behavioral variables related to the animal's movement, recent experiences, and behavioral goals [20]. It is also consistent with the idea of a basis function representation [51, 52], which allows a population of neurons to conjunctively represent multiple behaviorally relevant variables. Such representations are often employed by higher-order visual areas in primate cortex to implement sensorimotor transformations [17, 51, 53]. It is intriguing to find computational evidence for such a representation as early as V1 in the mouse. Future computational studies should therefore aim to study the mechanisms by which V1 neurons might construct a nonlinear combination of behavioral signals.

Limitations and future work.While our study opens a new perspective on modeling neural activity during natural conditions, there are a few limitations that need to be acknowledged. First, our data was relatively limited (around 50 neurons per animal, for 3 animals). The development of a Sensorium-style standardized dataset [14] for freely-moving mice would significantly benefit future research in this area, enabling more robust comparisons between different modeling approaches. Second, it would be beneficial to integrate other modalities that are known to be encoded in mouse V1 into the model. One such example is reward signals [54], which could provide additional information about the animal's decision-making processes and motivations during exploration.

## Acknowledgments

This work was supported by the National Institute of Neurological Disorders and Stroke of the National Institutes of Health under Award Number R01-NS121919. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

## References

* [1] Daniel L. K. Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and James J. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. _Proceedings of the National Academy of Sciences_, 111(23):8619-8624, June 2014. Publisher: Proceedings of the National Academy of Sciences.
* [2] Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte. Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation. _PLOS Computational Biology_, 10(11):e1003915, November 2014. Publisher: Public Library of Science.
* [3] Umut Guclu and Marcel A. J. van Gerven. Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream. _Journal of Neuroscience_, 35(27):10005-10014, July 2015. Publisher: Society for Neuroscience Section: Articles.
* [4] Santiago A. Cadena, George H. Denfield, Edgar Y. Walker, Leon A. Gatys, Andreas S. Tolias, Matthias Bethge, and Alexander S. Ecker. Deep convolutional models improve predictions of macaque V1 responses to natural images. _PLOS Computational Biology_, 15(4):e1006897, April 2019. Publisher: Public Library of Science.
* [5] William F. Kindel, Elijah D. Christensen, and Joel Zylberberg. Using deep learning to probe the neural code for images in primary visual cortex. _Journal of Vision_, 19(4):29, April 2019.
* [6] Santiago A. Cadena, Fabian H. Sinz, Taliah Muhammad, Emmanouil Froudarakis, Erick Cobos, Edgar Y. Walker, Jake Reimer, Matthias Bethge, Andreas Tolias, and Alexander S. Ecker. How well do deep neural networks trained on object recognition characterize the mouse visual system? October 2019.
* [7] Glen T Prusky, Paul W. R West, and Robert M Douglas. Behavioral assessment of visual acuity in mice and rats. _Vision Research_, 40(16):2201-2209, July 2000.
* [8] Cristopher M. Niell and Michael P. Stryker. Highly Selective Receptive Fields in Mouse Visual Cortex. _The Journal of Neuroscience_, 28(30):7520-7536, July 2008.
* [9] David Klindt, Alexander S Ecker, Thomas Euler, and Matthias Bethge. Neural system identification for large populations separating " what" and " where". In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [10] Alexander S. Ecker, Fabian H. Sinz, Emmanouil Froudarakis, Paul G. Fahey, Santiago A. Cadena, Edgar Y. Walker, Erick Cobos, Jacob Reimer, Andreas S. Tolias, and Matthias Bethge. A rotation-equivariant convolutional neural network model of primary visual cortex. December 2018.
* [11] Fabian Sinz, Alexander S Ecker, Paul Fahey, Edgar Walker, Erick Cobos, Emmanouil Froudarakis, Dimitri Yatsenko, Zachary Pitkow, Jacob Reimer, and Andreas Tolias. Stimulus domain transfer in recurrent models for large scale cortical population prediction on video. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [12] Konstantin-Klemens Lurz, Mohammad Bashiri, Konstantin Willeke, Akshay Jagadish, Eric Wang, Edgar Y. Walker, Santiago A. Cadena, Taliah Muhammad, Erick Cobos, Andreas S. Tolias, Alexander S. Ecker, and Fabian H. Sinz. Generalization in data-driven models of primary visual cortex. January 2021.
* [13] Shahab Bakhtiari, Patrick Mineault, Timothy Lillicrap, Christopher Pack, and Blake Richards. The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning. In _Advances in Neural Information Processing Systems_, volume 34, pages 25164-25178. Curran Associates, Inc., 2021.
* [14] Konstantin F. Willeke, Paul G. Fahey, Mohammad Bashiri, Laura Pede, Max F. Burg, Christoph Blessing, Santiago A. Cadena, Zhiwei Ding, Konstantin-Klemens Lurz, Kayla Ponder, Taliah Muhammad, Saumil S. Patel, Alexander S. Ecker, Andreas S. Tolias, and Fabian H. Sinz. The Sensorium competition on predicting large-scale mouse primary visual cortex activity, June 2022. arXiv:2206.08666 [cs, q-bio].
* [15] Andrew D. Huberman and Cristopher M. Niell. What can mice tell us about how vision works? _Trends in Neurosciences_, 34(9):464-473, September 2011.
* [16] James J. Gibson. _The Ecological Approach to Visual Perception: Classic Edition_. Psychology Press, November 2014. Google-Books-ID: 8BSLBQAAQBAJ.
* [17] M. Beyeler, N. Dutt, and J. L. Krichmar. 3D Visual Response Properties of MSTd Emerge from an Efficient, Sparse Population Code. _Journal of Neuroscience_, 36(32):8399-415, August 2016.

* [18] Philip R. L. Parker, Morgan A. Brown, Matthew C. Smear, and Cristopher M. Niell. Movement-Related Signals in Sensory Areas: Roles in Natural Behavior. _Trends in Neurosciences_, 43(8):581-595, August 2020.
* [19] Laura Busse, Jessica A. Cardin, M. Eugenia Chiappe, Michael M. Halassa, Matthew J. McGinley, Takayuki Yamashita, and Aman B. Saleem. Sensation during Active Behaviors. _Journal of Neuroscience_, 37(45):10826-10834, November 2017. Publisher: Society for Neuroscience Section: Symposium and Mini-Symposium.
* [20] Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Charu Bai Reddy, Matteo Carandini, and Kenneth D. Harris. Spontaneous behaviors drive multidimensional, brainwide activity. _Science_, 364(6437):eaaw7893, April 2019. Publisher: American Association for the Advancement of Science.
* [21] Cristopher M. Niell and Michael P. Stryker. Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex. _Neuron_, 65(4):472-479, February 2010.
* [22] Emmanouil Froudarakis, Paul G. Fahey, Jacob Reimer, Stelios M. Smirnakis, Edward J. Tehovnik, and Andreas S. Tolias. The Visual Cortex in Context. _Annual Review of Vision Science_, 5(1):317-339, 2019. _eprint: https://doi.org/10.1146/annurev-vision-091517-034407.
* [23] Arne F. Meyer, Jasper Poott, John O'Keefe, Maneesh Sahani, and Jennifer F. Linden. A Head-Mounted Camera System Integrates Detailed Behavioral Monitoring with Multichannel Electrophysiology in Freely Moving Mice. _Neuron_, 100(1):46-60.e7, October 2018.
* [24] Grigori Guitchounts, Javier Masis, Steffen B. E. Wolff, and David Cox. Encoding of 3D Head Orienting Movements in the Primary Visual Cortex. _Neuron_, 108(3):512-525.e4, November 2020.
* [25] Philip R. L. Parker, Elliott T. T. Abe, Emmalyn S. P. Leonard, Dylan M. Martins, and Cristopher M. Niell. Joint coding of visual input and eye/head position in V1 of freely moving mice. _Neuron_, September 2022.
* [26] Laura Busse, Asli Ayaz, Neel T. Dhruv, Steffen Katzner, Aman B. Saleem, Marieke L. Scholvinck, Andrew D. Zaharia, and Matteo Carandini. The Detection of Visual Contrast in the Behaving Mouse. _Journal of Neuroscience_, 31(31):11351-11361, August 2011. Publisher: Society for Neuroscience Section: Articles.
* [27] Sylvia Schroder, Nicholas A. Steinmetz, Michael Krumin, Marius Pachitariu, Matteo Rizzi, Leon Lagnado, Kenneth D. Harris, and Matteo Carandini. Arousal Modulates Retinal Output. _Neuron_, 107(3):487-495.e9, August 2020.
* [28] Cristopher M. Niell and Massimo Scanziani. How Cortical Circuits Implement Cortical Computations: Mouse Visual Cortex as a Model. _Annual Review of Neuroscience_, 44(1):517-546, 2021. _eprint: https://doi.org/10.1146/annurev-neuro-102320-085825.
* [29] Pieter M. Goldstein, Sandra Reinert, Tobias Bonhoeffer, and Mark Hubener. Mouse visual cortex areas represent perceptual and semantic features of learned visual categories. _Nature Neuroscience_, 24(10):1441-1451, October 2021. Number: 10 Publisher: Nature Publishing Group.
* [30] Jan Antolik, Sonja B. Hofer, James A. Bednar, and Thomas D. Mrsic-Flogel. Model Constrained by Visual Hierarchy Improves Prediction of Neural Responses to Natural Scenes. _PLOS Computational Biology_, 12(6):e1004927, June 2016. Publisher: Public Library of Science.
* [31] Mohammad Bashiri, Edgar Walker, Konstantin-Klemens Lurz, Akshay Jagadish, Taliah Muhammad, Zhiwei Ding, Zhuokun Ding, Andreas Tolias, and Fabian Sinz. A flow-based latent state generative model of neural population responses to natural images. In _Advances in Neural Information Processing Systems_, volume 34, pages 15801-15815. Curran Associates, Inc., 2021.
* [32] Aran Nayebi, Nathan C. L. Kong, Chengxu Zhuang, Justin L. Gardner, Anthony M. Norcia, and Daniel L. K. Yamins. Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation. _PLOS Computational Biology_, 19(10):e1011506, October 2023. Publisher: Public Library of Science.
* [33] Jianghong Shi, Eric Shea-Brown, and Michael Buice. Comparison Against Task Driven Artificial Neural Networks Reveals Functional Properties in Mouse Visual Cortex. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [34] Jianghong Shi, Bryan Tripp, Eric Shea-Brown, Stefan Mihalas, and Michael A. Buice. MouseNet: A biologically constrained convolutional neural network model for the mouse visual cortex. _PLOS Computational Biology_, 18(9):e1010427, September 2022. Publisher: Public Library of Science.

* [35] Galen Pogoncheff, Jacob Granley, and Michael Beyeler. Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., November 2023.
* [36] Julie A. Harris, Stefan Mihalas, Karla E. Hirokawa, Jennifer D. Whitesell, Hannah Choi, Amy Bernard, Phillip Bohn, Shiella Caldejon, Linzy Casal, Andrew Cho, Aaron Feiner, David Feng, Nathalie Gaudreault, Charles R. Gerfen, Nile Graddis, Peter A. Groblewski, Alex M. Henry, Anh Ho, Robert Howard, Joseph E. Knox, Leonard Kuan, Xiuli Kuang, Jerome Lecoq, Phil Lesnar, Yaoyao Li, Jennifer Luviano, Stephen McConoughey, Marty T. Mortvud, Maitham Naememi, Lydia Ng, Seung Wook Oh, Benjamin Ouellette, Elise Shen, Staci A. Sorensen, Wayne Wakeman, Quanzin Wang, Yun Wang, Ali Willoriford, John W. Phillips, Allan R. Jones, Christof Koch, and Hongkui Zeng. Hierarchical organization of cortical and thalamic connectivity. _Nature_, 575(7781):195-202, November 2019. Number: 7781 Publisher: Nature Publishing Group.
* [37] Joshua H. Siegle, Xiaoxuan Jia, Severine Durand, Sam Gale, Corbett Bennett, Nile Graddis, Greggory Heller, Tamina K. Ramirez, Hannah Choi, Jennifer A. Luviano, Peter A. Groblewski, Ruweida Ahmed, Anton Arkhipov, Amy Bernard, Yazan N. Billeh, Dillan Brown, Michael A. Buice, Nicolas Cain, Shiella Caldejon, Linzy Casal, Andrew Cho, Maggie Chvilicek, Timothy C. Cox, Kael Dai, Daniel J. Denman, Saskia E. J. de Vries, Roald Dietzman, Luke Esposito, Colin Farrell, David Feng, John Galbraith, Marina Garrett, Emily C. Gelfand, Nicole Hancock, Julie A. Harris, Robert Howard, Brian Hu, Ross Hytmen, Ramakrishnan Iyer, Erika Jesset, Katelyn Johnson, India Kato, Justin Kiggins, Sophie Lambert, Jerome Lecoq, Peter Ledcodhowitsch, Jung Hoon Lee, Arielle Leon, Yang Li, Elizabeth Liang, Fuhui Long, Kyla Mace, Jose Melchior, Daniel Millman, Tyler Mollenkopf, Chelsea Nayan, Lydia Ng, Kiet Ngo, Thuyahn Nguyen, Philip R. Nicovich, Kat North, Gabriel Koch Ocker, Doug Ollerenshaw, Michael Oliver, Marius Pachitariu, Jed Perkins, Melissa Reding, David Reid, Miranda Robertson, Kara Ronellenfitch, Sam Seid, Cliff Slaughterbeck, Michelle Stoecklin, David Sullivan, Ben Sutton, Jackie Swapp, Carol Thompson, Kristen Turner, Wayne Wakeman, Jennifer D. Whitesell, Derric Williams, Ali Williford, Rob Young, Hongkui Zeng, Sarah Naylor, John W. Phillips, R. Clay Reid, Stefan Mihalas, Shawn R. Olsen, and Christof Koch. Survey of spiking in the mouse visual system reveals functional hierarchy. _Nature_, 592(7852):86-92, April 2021. Number: 7852 Publisher: Nature Publishing Group.
* [38] Angie M Michaiel, Elliott TT Abe, and Cristopher M Niell. Dynamics of gaze control during prey capture in freely moving mice. _eLife_, 9:e57458, July 2020. Publisher: eLife Sciences Publications, Ltd.
* [39] Aman B Saleem. Two stream hypothesis of visual processing for navigation in mouse. _Current Opinion in Neurobiology_, 64:70-78, October 2020.
* [40] Cristopher M. Niell. Cell Types, Circuits, and Receptive Fields in the Mouse Visual Cortex. _Annual Review of Neuroscience_, 38(1):413-431, 2015. _eprint: https://doi.org/10.1146/annurev-neuro-071714-033807.
* [41] Katrin Franke, Konstantin F. Willeke, Kayla Ponder, Mario Galdamez, Na Zhou, Taliah Muhammad, Saumil Patel, Emmanouil Froudarakis, Jacob Reimer, Fabian H. Sinz, and Andreas S. Tolias. State-dependent pupil dilation rapidly shifts visual feature selectivity. _Nature_, 610(7930):128-134, October 2022. Number: 7930 Publisher: Nature Publishing Group.
* [42] Edgar Y. Walker, Fabian H. Sinz, Erick Cobos, Taliah Muhammad, Emmanouil Froudarakis, Paul G. Fahey, Alexander S. Ecker, Jacob Reimer, Xaq Pitkow, and Andreas S. Tolias. Inception loops discover what excites neurons most using deep predictive models. _Nature Neuroscience_, 22(12):2060-2065, December 2019. Number: 12 Publisher: Nature Publishing Group.
* [43] Alexander Mathis, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. _Nature Neuroscience_, 21(9):1281-1289, September 2018. Number: 9 Publisher: Nature Publishing Group.
* [44] Nicholas A. Steinmetz, Cagatay Aydin, Anna Lebedeva, Michael Okun, Marius Pachitariu, Marius Bauza, Maxime Beau, Jai Bhagat, Claudia Bohm, Martijn Broux, Susu Chen, Jennifer Colonell, Richard J. Gardner, Bill Karsh, Fabian Kloosterman, Dimitar Kostadinov, Carolina Mora-Lopez, John O'Callaghan, Junchol Park, Jan Putzeyis, Britton Sauerbrei, Rik J. J. van Daal, Abraham Z. Vollan, Shiwei Wang, Marleen Welkenhuysen, Zhiwen Ye, Joshua T. Dudman, Barundeb Dutta, Adam W. Hantman, Kenneth D. Harris, Albert K. Lee, Edvard I. Moser, John O'Keefe, Alfonso Renart, Karel Svoboda, Michael Hausser, Sebastian Haesler, Matteo Carandini, and Timothy D. Harris. Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings. _Science_, 372(6539):eabf4588, April 2021. Publisher: American Association for the Advancement of Science.

* [45] Philip R. L. Parker, Dylan M. Martins, Emmalyn S. P. Leonard, Nathan M. Casey, Shelby L. Sharp, Elliott T. T. Abe, Matthew C. Smear, Jacob L. Yates, Jude F. Mitchell, and Cristopher M. Niell. A dynamic sequence of visual processing initiated by gaze shifts, August 2022. Pages: 2022.08.23.504847 Section: New Results.
* [46] Pouya Bashivan, Kohitij Kar, and James J. DiCarlo. Neural population control via deep image synthesis. _Science_, 364(6439):eaav9436, May 2019. Publisher: American Association for the Advancement of Science.
* [47] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, April 2014. arXiv:1312.6034 [cs].
* [48] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition, December 2015. arXiv:1512.03385 [cs].
* [49] Mingxing Tan and Quoc V. Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, September 2020. arXiv:1905.11946 [cs, stat] version: 5.
* [50] Gert Van den Bergh, Bin Zhang, Lutgarde Arckens, and Yuzo M. Chino. Receptive-field properties of V1 and V2 neurons in mice and macaque monkeys. _Journal of Comparative Neurology_, 518(11):2051-2070, 2010. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.22321.
* [51] Michael Beyeler, Emily L. Rounds, Kristofor D. Carlson, Nikil Dutt, and Jeffrey L. Krichmar. Neural correlates of sparse coding and dimensionality reduction. _PLOS Computational Biology_, 15(6):e1006908, June 2019.
* [52] S. Fusi, E. K. Miller, and M. Rigotti. Why neurons mix: high dimensionality for higher cognition. _Curr Opin Neurobiol_, 37:66-74, April 2016.
* [53] A. Pouget and L. H. Snyder. Computational approaches to sensorimotor transformations. _Nat Neurosci_, 3 Suppl:1192-8, November 2000.
* [54] Marshall G. Shuler and Mark F. Bear. Reward Timing in the Primary Visual Cortex. _Science_, 311(5767):1606-1609, March 2006. Publisher: American Association for the Advancement of Science.

## Appendix A Data Acquisition

The full procedure of data collection and data preprocessing is described in detail in Ref. [25], but are briefly described below for the reader's convenience.

Visual scenes from the mouse's perspective, neural activity in V1, and various behavioral variables were simultaneously recorded from three adult mice who were freely exploring an arena with a state-of-the-art head-mounted recording system [25]. The recording system consisted of high-density silicon probes, two miniature cameras and an inertial measurement unit (IMU). Electrophysiology data was acquired at \(30\,\mathrm{kHz}\) using a \(11\,\mathrm{\SIUnitSymbolMicro m}\times 15\,\mathrm{\SIUnitSymbolMicro m}\) multi-shank linear silicon probe (128 channels) implanted in the center of the left monocular V1. One wide-angled camera (around \(120^{\circ}\)) was aimed outwards to capture the visual scene available to the right eye of the mouse at \(16\,\mathrm{ms}\) per frame ("worldcam"). A second camera was aimed at the right eye (illuminated by an infrared-LED) to record a video feed of the right eye at \(30\,\mathrm{Hz}\). The IMU acquired three-axis gyroscope and accelerometer data at \(30\,\mathrm{kHz}\). In addition, a top-down camera recorded the mouse in the arena at \(60\,\mathrm{Hz}\).

During experiments, mice were placed in an arena where they could move around freely for about 1 hour. The arena was approximately \(48\,\mathrm{cm}\) long \(\times\,37\,\mathrm{cm}\) wide \(\times\,30\,\mathrm{cm}\) high. The gray floor was covered with black-and-white Legos to provide visual contrast. One wall of the arena was a monitor displaying a moving black-and-white spots stimulus, and the other three walls were covered with wallpaper with static stimuli including white noise, black-and-white high-spatial-frequency gratings, and black-and-white low-spatial-frequency gratings. In order to encourage foraging behavior during the recording sessions, small fragments of tortilla chips were sparsely distributed across the arena.

The worldcam video was downsampled to \(60\times 80\) pixels. DeepLabCut [43] was used to extract extract eye position (\(\theta\), \(\phi\)) and pupil radius (\(\sigma\)). Pitch (\(\rho\)) and roll (\(\omega\)) of the mouse's head position were extracted from the IMU. Locomotion speed (\(s\)) was estimated from the top-down camera feed using DeepLabCut [43]. Electrophysiology data bandpass-filtered between \(0.01\,\mathrm{Hz}\) and \(7.5\,\mathrm{kHz}\), and spike-sorted with Kilosort 2.5 [44]. Single units were selected using Phy2 (see [45]) and inactive units (mean firing rate \(<3\,\mathrm{Hz}\)) were removed. This yielded 68, 32, and 49 active units for Mouse 1-3, respectively. To prepare the data for machine learning, all data streams were deinterlaced and resampled at \(20.83\,\mathrm{Hz}\) (\(48\,\mathrm{ms}\) per frame; Fig. 1C).

Vision-Only Models

### Hyperparameter Tuning

We performed a grid search to find the optimal CNN kernel size (3, 5, 7, 9), number of channels (32, 64, 128, 256, 512; in various combinations), and dropout rate (0, 0.25, 0.5). While other models often rely on kernel size 3 for their CNN, we found these small kernels to lead to worse performance, perhaps due to the mouse's low-resolution vision. Kernel size 7 performed best.

We repeated the grid search for CNNs with different number of convolutional layers. The resulting 3-layer CNN with 0.5 dropout rate outperformed many differently sized networks, such as a 1-layer CNN with 1024 channels (i.e., a shallow but wide network), a 2-layer CNN, or a 4-layer CNN. Choice of learning rates and optimizers had no notable effect on the final performance of the networks.

### Autoencoder

We initially hypothesized that an autoencoder could provide regularization benefits over a "vanilla" CNN, because the reconstruction loss might encourage the model to learn visual features that are useful for decoding. We used an encoder \(\phi\) to map the original frame \(\mathcal{F}\) to a vector \(\mathcal{V}\) in the latent space, which was present at the bottleneck, while the decoder \(\psi\) then mapped the vector \(\mathcal{V}\) from the latent space to the output.

\[\phi:\ \ \mathcal{F}\rightarrow\mathcal{V},\] (1) \[\psi:\ \ \mathcal{V}\rightarrow\mathcal{F},\] (2) \[\phi,\psi=\operatorname{argmin}_{\phi,\psi}||\mathcal{F}-(\psi \cdot\phi)\mathcal{F}||^{2}.\] (3)

After hyperparameter search, we settled on size 256 for the latent space vector, and the weight of the reconstruction loss relative to the Poisson loss was fixed at 0.5. Both the encoder and the decoder were 3-layer CNNs, and their numbers of channels were symmetric. However, after testing a number of autoencoders with different configurations (Table 4), we found that a simple 3-layer CNN outperformed any and all of them.

\begin{table}
\begin{tabular}{l c c c|c c c}  & \multicolumn{3}{c}{Mouse 1} & \multicolumn{2}{c}{Mouse 2} & \multicolumn{2}{c}{Mouse 3} \\ Kernel size, encoder \#channels & & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) & \(cc\uparrow\) & MSE \(\downarrow\) \\ \hline
3, \(16\times 32\times 64\) & \(\downarrow.539\pm.149\) & \(.0728\) & \(.389\pm.128\) & \(.10\mathbf{77}\) & \(.502\pm.129\) & \(.0996\) \\
5, \(16\times 32\times 64\) & \(\downarrow.550\pm.147\) & \(.0728\) & \(.363\pm.116\) & \(.109\) & \(.508\pm.135\) & \(.0983\) \\
7, \(16\times 32\times 64\) & \(\downarrow.525\pm.152\) & \(.0732\) & \(.353\pm.121\) & \(.117\) & \(.509\pm.131\) & \(.0980\) \\
9, \(16\times 32\times 64\) & \(\downarrow.518\pm.147\) & \(.0752\) & \(.315\pm.101\) & \(.119\) & \(.492\pm.135\) & \(.0997\) \\
3, \(32\times 64\times 128\) & \(\downarrow.543\pm.144\) & \(.0737\) & \(.367\pm.128\) & \(.109\) & \(.503\pm.131\) & \(.100\) \\
5, \(32\times 64\times 128\) & \(\downarrow.551\pm.149\) & \(.0723\) & \(.361\pm.109\) & \(.119\) & \(.514\pm.132\) & \(.0984\) \\
7, \(32\times 64\times 128\) & \(\downarrow.539\pm.145\) & \(.0739\) & \(\mathbf{.390\pm.118}\) & \(.109\) & \(.492\pm.129\) & \(.100\) \\
9, \(32\times 64\times 128\) & \(\downarrow.510\pm.155\) & \(.0758\) & \(.331\pm.119\) & \(.112\) & \(.500\pm.134\) & \(.101\) \\
3, \(64\times 128\times 256\) & \(\downarrow.541\pm.146\) & \(.0758\) & \(.374\pm.123\) & \(.110\) & \(.514\pm.127\) & \(.0990\) \\
5, \(64\times 128\times 256\) & \(\downarrow.552\pm.145\) & \(.0777\) & \(.362\pm.119\) & \(.110\) & \(.508\pm.134\) & \(.104\) \\
7, \(64\times 128\times 256\) & \(\downarrow.553\pm.134\) & \(.0688\) & \(.369\pm.104\) & \(.111\) & \(\mathbf{.530\pm.136}\) & \(.0992\) \\
9, \(64\times 128\times 256\) & \(\downarrow.537\pm.146\) & \(.0811\) & \(.355\pm.109\) & \(.119\) & \(.500\pm.128\) & \(.105\) \\ \end{tabular}
\end{table}
Table 4: Performance of different autoencoders. The numbers of channels in the decoder were symmetric with those of the encoder. Best performing networks are indicated in bold. \(cc\): cross-correlation, mean \(\pm\) standard deviation across neurons (\(\uparrow\): the higher the better), MSE: mean-squared error (\(\downarrow\): the lower the better).

[MISSING_PAGE_EMPTY:17]