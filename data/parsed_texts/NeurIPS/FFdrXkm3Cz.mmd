# On the spectral bias of two-layer linear networks

 Aditya Varre

EPFL

aditya.varre@epfl.ch &Maria-Luiza Vladarean

EPFL

maria-luiza.vladarean@epfl.ch &Loucas Pillaud-Vivien

Courant Institute of Mathematics, NYU / Flatiron Institute

lpiillaudvivien@flatironinstitute.org &Nicolas Flammarion

EPFL

nicolas.flammarion@epfl.ch

###### Abstract

This paper studies the behaviour of two-layer fully connected networks with linear activations trained with gradient flow on the square loss. We show how the optimization process carries an _implicit bias_ on the _parameters_ that depends on the scale of its initialization. The main result of the paper is a variational characterization of the loss minimizers retrieved by the gradient flow for a specific initialization shape. This characterization reveals that, in the small scale initialization regime, the linear neural network's _hidden layer_ is biased toward having a low-rank structure. To complement our results, we showcase a hidden mirror flow that tracks the dynamics of the singular values of the weights matrices and describe their time evolution. We support our findings with numerical experiments illustrating the phenomena.

## 1 Introduction

The most forceful driver of advancements in the field of Machine Learning over the past decades has been the success of deep neural networks. Amongst the striking qualities of these models is the fact that, despite being heavily overparametrized, their optimization consistently yields minima with good generalization properties. A beckoning research direction is thus to unravel the process through which neural networks learn internal representations for a given task (Bengio et al., 2013). Understanding such phenomena is crucial for lowering the interpretability barrier of these models and developing a principled approach to their training and deployment in practice.

Recent experimental evidence identified one of the likely paths towards achieving these goals as the study of the inherent regularization properties (or implicit biases) of training algorithms (Neyshabur et al., 2014; Zhang et al., 2016). These observations laid the foundation for a new line of work (see, e.g., Vardi, 2022) whose driving question is which minimum, amongst the many, awaits at the tail end of optimization.

One of the determining factors for the implicit bias of gradient methods is the initialization scale, which controls their operational regime as shown by empirical studies (Chizat et al., 2019). More precisely, gradient descent with a low-scale initialization is capable of learning rich feature representations from the data. Strikingly, despite overparameterization, the hidden-layer neurons align in the direction of the features (Chizat et al., 2019; Atanasov et al., 2022) and learning of representations reflects in the low-rank structure of the hidden layers. Our work aims to precisely explain this phenomenon and quantify the impact of the initialization scale on feature learning.

Unfortunately, studying such phenomena for the types of neural networks used in practice is mathematically challenging at present due to the non-linearity of their activations. Their less expressive _linear_ counterparts, however, are more tractable and represent a good proxy due to their non-convexloss landscape and non-linear learning dynamics. Consequently, the study of deep linear networks has received significant amounts of attention over the past years, and spans several important directions, including convergence (Arora et al., 2019, 2018; Min et al., 2021), learning dynamics (Saxe et al., 2014; Braun et al., 2022) and the implicit bias of optimization algorithms (Azulay et al., 2021). This work complements these previous approaches by mathematically describing the properties of their parameters at convergence, highlighting the implicit bias phenomenon, and further analyzing the evolution of weight matrices throughout the optimization process.

Specifically, this paper studies overparameterized vector regression problems on two-layer fully-connected linear neural networks. We show the following results when the network is trained with gradient flow (GF).

1. In Theorem 3.1, we prove that the zero-loss solutions retrieved by the gradient flow are the minimizers of a potential that depends on the initialization scale. Additionally, we provide explicit expressions for the singular values of the hidden layer weights, also as a function of the initialization scale. These characterizations reveal how low-magnitude initialization induces a low-rank structure of the hidden layer.
2. In Theorem 3.2, we show that gradient flow on the parameters induces a mirror flow on the singular values. In the specific case of scalar regression, we show that the gradient flow on the weights is equivalent to a mirror flow on the linear predictor. These characterizations give the geometrical structure of the training dynamics of linear neural networks.
3. In Proposition 4.1, we design a simple process to analytically describe how stochastic noise in the training algorithm can likewise induce low-rank structures in the weights _regardless of the initialization scale_.

We proceed by presenting related work in Section 1.1, formalizing the problem setup and assumptions in Section 2, stating and discussing our results in Section 3, and finally, we provide supporting numerical evidence in Section 4.

### Related Work

The first pillar of our work addresses the implicit bias of GF and its stochastic variant in regression problems. One of the hallmarks of bias in this setting is the impact of initialization scale: large initial weights induce a learning regime in which the parameters travel a short distance to convergence and feature learning fails to happen (_lazy_ regime), while small initialization effects a polar opposite behaviour of the system (_rich_ regime) Chizat et al. (2019); Woodworth et al. (2020). Training and generalization in the lazy regime are well-studied (Jacot et al., 2018; Du et al., 2019; Arora et al., 2019; Soltanolkotabi et al., 2017), however this scenario fails to capture the observed behaviour of neural networks in practice (Ghorbani et al., 2019). While the rich regime more faithfully approximates the feature learning abilities of these models, it is comparatively more challenging to analyze and few results are known. Amongst them are those concerning diagonal linear networks, where a preference towards sparse representations is shown (Woodworth et al., 2020), and a restricted setting of the matrix factorization problem, where the implicit bias leads to low-rank representations (Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2018). We similarly study the rich representation learning regime and provide initialization scale-dependent statements on implicit bias for two-layer fully connected linear networks.

Most theoretical results on the implicit bias of GF in overparametrized models rely on the identification of a related mirror flow in a reparametrized space (Gunasekar et al., 2018). Diagonal linear networks are amenable to this technique and therefore well-studied (Woodworth et al., 2020; Pesme et al., 2021). For linear fully-connected networks, however, the existence of a mirror flow is not always guaranteed (Li et al., 2022). To partly alleviate this issue, Azulay et al. (2021) introduce a nonlinear time-rescaling technique and show that for scalar least-squares regression on a two-layer fully connected network with zero-balance initialization, the implicit bias selects low \(\ell_{2}\)-norm predictors. We prove a similar result under _imbalanced_ initialization controlled by a scale parameter, and characterize the weight matrices independently at convergence, thus presenting a higher-resolution view of the problem.

Other works on linear networks include (Min et al., 2021) where convergence is studied in the presence of weight imbalance and implicit bias results are provided in the functional space; and (Yunet al., 2021) where tensor networks are studied with the goal of unifying the implicit bias results for linear parameterization. In the case of linear networks, Yun et al. (2021) further show an implicit bias towards minimum \(\ell_{2}\) linear predictors for vanishingly small initializations. For classification in the case of linear networks, Ji and Telgarsky (2019) show that the weights grow to infinity and the layers of the deep linear network align during the course of optimization. Timor et al. (2023) show a similar phenomenon happens for two-layer ReLU networks.

The second pillar concerns the learning dynamics of linear neural networks. The two-layer case optimized with GF on the square loss has been studied by Fukumizu (1998); Saxe et al. (2014, 2019); Braun et al. (2022). The common setup of these works is that of zero-balance initialization and whitened data. First, Saxe et al. (2014, 2019) provide expressions for the temporal evolution of singular values of the predictor by assuming decoupled dynamics and a specific data-dependent initialization of the weights. This latter condition is alleviated by the approach of Fukumizu (1998) and Braun et al. (2022); Tarmoun et al. (2021) who solve a matrix Ricatti equation yielding solutions for the weight dynamics in the case where the network initialization has full rank. Finally, Gidel et al. (2019) loosen the whitened data assumption through a perturbation analysis and provide the time-evolution of singular values of the weight matrices. Our work removes the requirement of zero-balanced initialization and full-rank network initialization, and gives formulas for the weights' evolution as a function of the initialization scale. We further provide mirror flows on the weights' singular values and show that components are learned in a hierarchical manner for the case of whitened data.

Related work is further addressed in the following sections, as part of the discussion of results.

## 2 Preliminaries and problem setup

Notation.Time-dependent variables are written in bold fonts: we drop the \(t\) in \(A(t)\) and simply denote it as \(\mathbf{A}\). The time derivative of such variables is denoted \(\frac{d}{dt}A(t)\) as \(\overset{\boldsymbol{\cdot}}{\mathbf{A}}\).

Vector Regression.The set-up is that of standard vector regression problems with inputs \((x_{1},\ldots,x_{n})\in\left(\mathbb{R}^{d}\right)^{n}\) and outputs \((y_{1},\ldots y_{n})\in\left(\mathbb{R}^{k}\right)^{n}\) in the so-called overparametrized regime where \(d\geq n\). Regarding the output dimension, the reader may keep in mind throughout the article that \(k\ll d\), though the analysis holds for any \(k,d\) pair. In order to learn the input/output rule, we minimize the square loss over a class of parametric models \(\mathcal{H}=\{f_{\boldsymbol{\theta}}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{ R}^{k}\mid\boldsymbol{\theta}\in\mathbb{R}^{p}\}\) which we specify in the next paragraph. The train loss therefore can be written as

\[\mathcal{L}\left(\boldsymbol{\theta}\right)=\frac{1}{2n}\sum_{i=1}^{n}\left(y _{i}-f_{\boldsymbol{\theta}}(x_{i})\right)^{2}. \tag{2.1}\]

Parameterization with a Linear Network.We consider the parametric model of _two-layer linear neural networks_ of width \(l\in\mathbb{N}^{*}\): this corresponds to the parametrization \(\boldsymbol{\theta}=(\boldsymbol{W}_{1},\boldsymbol{W}_{2})\), \(\boldsymbol{W}_{1}\in\mathbb{R}^{d\times l},\boldsymbol{W}_{2}\in\mathbb{R}^{l \times k}\) and \(f_{\boldsymbol{\theta}}(x)=\boldsymbol{W}_{2}^{\top}\boldsymbol{W}_{1}^{\top}x\). The model is linear in the input \(x\), and in terms of expressivity, it is equivalent to the linear class of predictors given by \(f_{\boldsymbol{\theta}}(x)=\boldsymbol{\beta}^{\top}x\), with \(\boldsymbol{\beta}=\boldsymbol{W}_{1}\boldsymbol{W}_{2}\). We henceforth use the symbol \(\boldsymbol{\beta}\) to denote the associated linear predictor of the network. An important consequence of this reparametrization is that the prediction function \(f_{\boldsymbol{\theta}}\) is positively homogeneous of degree 2 in \(\boldsymbol{\theta}\): \(\forall\lambda\in\mathbb{R}\), it holds that \(f_{\lambda\boldsymbol{\theta}}=\lambda^{2}f_{\boldsymbol{\theta}}\), as it is the case for two-layer ReLU networks. This property has important consequences in the loss landscape through which \(\boldsymbol{\theta}\) goes.

Train loss.Assume momentarily that \(k=1\) and denote \(\boldsymbol{\phi}(x)=\boldsymbol{W}_{1}^{\top}x\in\mathbb{R}^{l}\). It is then clear that the predictor rewrites as \(f_{\boldsymbol{\theta}}(x)=\langle\boldsymbol{\phi}(x),\boldsymbol{W}_{2}\rangle\). For this reason, we call the hidden layer \(\boldsymbol{W}_{1}\) the _feature layer_ and the last layer \(\boldsymbol{W}_{2}\) the _weight matrix_. We study the _overparametrized setting_ where \(l\gg d\). Letting \(X^{\top}\coloneqq[x_{1},\ldots,x_{n}]\) and \(Y^{\top}\coloneqq[y_{1},\ldots,y_{n}]\), the loss becomes

\[\mathcal{L}\left(\boldsymbol{W}_{1},\boldsymbol{W}_{2}\right)=\frac{1}{2N}\|X \boldsymbol{W}_{1}\boldsymbol{W}_{2}-Y\|^{2}. \tag{2.2}\]

For brevity, we ignore the \(N\) in Eq.(2.2) by implicitly rescaling the data as \((X,Y)\leftarrow(\nicefrac{{X}}{{\sqrt{N}}},\nicefrac{{Y}}{{\sqrt{N}}})\).

Interpolators.Note that when \(Y\in\operatorname{span}(X)\) and \(X\) is non-degenerate (which occurs with probability one if e.g., \(X,Y\) are Gaussian and \(d\geq n\)), there always exists a solution which attains _zero_ loss, i.e., \(\mathbf{\beta}^{*}\in\mathbb{R}^{d\times k}\) such that \(X\mathbf{\beta}^{*}=Y\). We emphasize the fact that there are two levels of overparametrization here: on one hand, when \(d>n\), the set of zero loss linear predictors \(\mathcal{I}_{\mathbf{\beta}}:=\{\mathbf{\beta}\in\mathbb{R}^{d\times k}\mid\ X\mathbf{\beta} =Y\}\) is typically an affine set of dimension \((d-n)k\). On the other hand, since we also reparametrize \(\mathbf{\beta}\) as a linear network of width \(l\gg d\), the manifold of interpolators in the reparametrized space of \(\mathbf{\theta}\), defined by \(\mathcal{I}_{\mathbf{\theta}}:=\{\mathbf{\theta}=(\mathbf{W}_{1},\mathbf{W}_{2})\mid\mathbf{W}_{1 }\mathbf{W}_{2}\in\mathcal{I}_{\mathbf{\beta}}\},\) is of dimension \(l(d+k)-nk\). A natural question, therefore, is to which of these interpolators \(\mathbf{\theta}^{*}\in\mathcal{I}_{\mathbf{\theta}}\) does a given optimization algorithm converge. This concept is referred to as the _implicit bias_ of an algorithm. The aim of this work is to study that of gradient flow.

Gradient Flow.The dynamics induced in parameter space by running gradient flow on (2.2) is given by

\[\mathbf{\overset{*}{\mathbf{\theta}}}=-\nabla_{\mathbf{\theta}}\mathcal{L}( \mathbf{\theta}). \tag{2.3}\]

We wish to describe the implicit regularization properties of this continuous-time process, which is the vanishing stepsize limit of (stochastic) gradient descent. While the latter algorithms incur additional regularization properties from using non-zero stepsizes (Keskar et al., 2017), the study of GF is an important stepping stone to understanding the implicit bias of gradient-based methods in practice. In terms of \(\mathbf{W}_{1},\mathbf{W}_{2}\) the dynamics translates to

\[\mathbf{\overset{*}{W}}_{1} =X^{\top}\left(Y-X\mathbf{W}_{1}\mathbf{W}_{2}\right)\mathbf{W}_{2}^{\top}\,, \tag{2.4a}\] \[\mathbf{\overset{*}{W}}_{2} =\mathbf{W}_{1}^{\top}X^{\top}\left(Y-X\mathbf{W}_{1}\mathbf{W}_{2}\right). \tag{2.4b}\]

We emphasize a crucial point: even if the function \(\mathbf{\beta}\to\|X\mathbf{\beta}-Y\|^{2}\) is convex, its reparametrization in terms of \(\mathbf{W}_{1},\mathbf{W}_{2}\) is not. Non-convexity and non-linearity makes the analysis challenging and a priori it is not even clear whether the time evolution of \(\mathbf{\beta}\) can be expressed as a closed system.

Initialization.One of our primary objects of study is the impact of initialization on the behaviour of GF. We describe here our initialization choice, to which we henceforth refer as \(I_{\gamma}\).

* **Orthogonal feature layer:** We initialize the inner layer such that the rows of \(\mathbf{W}_{1}\) are orthogonal and scale with parameter \(\gamma>0\). Mathematically, this translates to \(\mathbf{W}_{1}(0)=\sqrt{2\gamma}P\) for \(P\in\mathbb{R}^{d\times l}\) in the Stiefel manifold \(V_{d}(\mathbb{R}^{l}):=\{P\in\mathbb{R}^{d\times l},\text{ such that }PP^{\top}=I_{d}\}\). Initializing with an orthogonal matrix is studied by Pennington et al. (2018); Hu et al. (2020), however from an optimization perspective. Note that when \(l\) is very large, this setting approximates the real-world scenario of initializing the hidden neurons with \(d\) i.i.d. Gaussian vectors in \(\mathbb{R}^{l}\), which are known to be almost orthogonal.
* **Zero weight layer:** In order to remove any initialization bias from the linear layer, we initialize it at \(\mathbf{W}_{2}(0)=0\). This can be seen as the limiting case of initializing the weight layer with a very small _relative_ scale \(\overline{\gamma}\ll\gamma\).

As already mentioned in Section 1.1, existing studies on linear networks assume a "zero-balance initialization", namely that \(\mathbf{W}_{1}^{\top}(0)\mathbf{W}_{1}(0)=\mathbf{W}_{2}(0)\mathbf{W}_{2}^{\top}(0)\)(Saxe et al., 2014; Arora et al., 2019, Azulay et al., 2021). This condition introduces the invariant \(\mathbf{W}_{1}^{\top}\mathbf{W}_{1}=\mathbf{W}_{2}\mathbf{W}_{2}^{\top}\)(Du et al., 2018), which holds for all times \(t\geq 0\). This balancedness can be seen as a degeneracy assumption on the flow, since it implies that \(\mathbf{W}_{1}\) has at most rank \(k\) during the entire process, irrespective of the scale of initialization \(\gamma\). In contrast, we show that _depending on \(\gamma\)_ the feature layer \(\mathbf{W}_{1}\) is biased (or not) toward a low-rank predictor, thus unveiling a truly rich representation learning regime.

## 3 Main result: implicit bias and dynamics description

### Implicit bias on the parameters

Non-convex gradient flows are generally not guaranteed to reach _global_ minimizers of the objective and even when they do, such results are difficult to formally prove. Moreover, the existence of many zero-loss solutions with different generalization properties raises the question of which interpolatingnetwork is yielded by training. An elegant answer to such questions is to express the resulting predictor as the _optimum_ among all the possible interpolators of some new, a priori unspecified cost. In addition to the descriptive power of such variational formulations, they express a form of capacity control over the estimator which can be further used to describe its generalization abilities (Bartlett et al., 2020). The following theorem adds to this series of works, by precisely deriving such a characterization for GF in the setting of linear networks.

**Theorem 3.1**.: _Let \((\mathbf{W}_{1},\mathbf{W}_{2})\) be the process that follows the GF equations (2.4a)-(2.4b), initialized according to condition \(I_{\gamma}\), for some \(\gamma>0\). Then_

1. _The parameters converge to a global optimum of the loss_ \[\lim_{t\to\infty}\left(\mathbf{W}_{1}(t),\mathbf{W}_{2}(t)\right)=\left(\mathbf{W}_{1}^{ \infty},\mathbf{W}_{2}^{\infty}\right)\in\mathcal{I}_{\mathbf{\theta}}.\]
2. _The linear predictor_ \(\mathbf{\beta}\) _converges to the minimum_ \(\ell_{2}\)_-norm interpolator_ \[\lim_{t\to\infty}\mathbf{\beta}(t)=\operatorname*{argmin}_{X\mathbf{\beta}=Y}\ \left\|\mathbf{\beta}\right\|_{F}\stackrel{{\mathrm{def}}}{{=}}\mathbf{ \beta}_{*}.\]
3. _We have the following variational characterization of the limiting parameters_ \[(\mathbf{W}_{1}^{\infty},\mathbf{W}_{2}^{\infty})\in\operatorname*{argmin}_{X\mathbf{W}_{ 1}\mathbf{W}_{2}=Y}\frac{1}{2}\big{\|}\mathbf{W}_{2}\big{\|}_{F}^{2}+\frac{1}{2}\big{\|} \mathbf{W}_{1}\big{\|}_{F}^{2}-\gamma\log\big{(}\mathrm{det}\ \big{(}\mathbf{W}_{1}\mathbf{W}_{1}^{\top}\big{)}\big{)}.\] (3.1)

Interpretation of the theorem.The theorem is divided into three parts which state that (i) the matrices converge to a zero loss solution, which is a priori non-trivial since the loss in non-convex; (ii) among all the interpolators in \(\mathcal{I}_{\mathbf{\beta}}\), \(\mathbf{\beta}\) converges to the minimum \(\ell_{2}\)-norm interpolator for all \(\gamma>0\); and (iii) among all the interpolators in \(\mathcal{I}_{\theta},(\mathbf{W}_{1},\mathbf{W}_{2})\) converge to the ones that minimize a \(\gamma\)-dependent potential. To fully capture the richness of this result, we observe that in the limit of \(\gamma\to 0\), problem (3.1) informally translates to (Attouch, 1996)

\[\lim_{\gamma\to 0}\left(\mathbf{W}_{1}^{\infty},\mathbf{W}_{2}^{\infty}\right)\in \operatorname*{argmin}_{X\mathbf{W}_{1}\mathbf{W}_{2}=Y}\frac{1}{2}\big{\|}\mathbf{W}_{2} \big{\|}_{F}^{2}+\frac{1}{2}\big{\|}\mathbf{W}_{1}\big{\|}_{F}^{2}.\]

This is equivalent, in the space of linear predictors \(\mathbf{\beta}\) to the minimum nuclear norm solution \(\mathbf{\beta}\in\operatorname*{argmin}_{X\mathbf{\beta}=Y}\ \big{\|}\mathbf{\beta} \big{\|}_{*}\) (which is also the minimum \(\ell_{2}\)-norm interpolator for the problem we study). We informally derived this interpretation by taking _first_ the limit \(t\to\infty\)_and only after \(\gamma\to 0\). The theorem naturally does not hold if the two limits are reversed, since \(\gamma=0\) places the initialization at a saddle point of the loss, which is a stationary point of the flow.

With increasing \(\gamma\), we move towards solutions with a large \(\log\mathrm{det}\left(\mathbf{W}_{1}\mathbf{W}_{1}^{\top}\right)\), which is a smooth approximation of the rank (Fazel et al., 2003). Intuitively, this means that solutions with increasing rank are preferred as \(\gamma\) grows. This scale-induced implicit bias is reminiscent of the _rich_ and _lazy_ regimes (Choizat et al., 2019; Woodworth et al., 2020), albeit visible in the space of representations rather than in that of predictors. As such, our result for linear networks is akin to Woodworth et al. (2020)'s, which characterizes the rich and lazy regimes for simpler diagonal linear networks.

Comparison with works on implicit bias of \(\mathbf{\beta}\).Azulay et al. (2021); Min et al. (2021) also study the implicit bias phenomenon in linear networks, however, these results only address the structure of the final predictor \(\mathbf{\beta}\) and not that of the factorized problem \((\mathbf{W}_{1},\mathbf{W}_{2})\). As shown in Theorem 3.1, these works fall short of unveiling all the nuances of the implicit regularization induced by GF in the case of linear networks.

To give a precise example, consider the simplest case of scalar regression (\(k=1\)) for which both Theorem 3.1 and (Azulay et al., 2021) show that \(\mathbf{\beta}\) is biased towards low-\(\ell_{2}\) interpolators. This view is not complete, since there exist many pairs \((\mathbf{W}_{1},\mathbf{W}_{2})\) such that \(\mathbf{W}_{1}\mathbf{W}_{2}=\mathbf{\beta}\). Theorem 3.1 goes one step further and provides variational characterization of \((\mathbf{W}_{1},\mathbf{W}_{2})\) at convergence. Moreover, it shows that when \(\gamma\to 0\) all columns of \(\mathbf{W}_{1}\) align in the direction of \(\mathbf{\beta}\), thus creating a rank one hidden layer. This is an example of rich representation learning, where \(\mathbf{W}_{1}\) is learning the only feature needed to make a prediction.

[MISSING_PAGE_FAIL:6]

(Appendix C.12) by solving a matrix Ricatti equation (Bittanti et al., 1991). In the limit of \(\gamma\to 0\), we can show that, beyond the case of balanced initialization (Saxe et al., 2014; Gidel et al., 2019), the singular values are learned in a hierarchical manner. When \(\gamma\to 0\) and with appropriately rescaled time, the limiting trajectory for the \(i^{th}\) singular value \(\sigma_{i,\mathbf{\beta}}\) can be seen as the _jump process_

\[\sigma_{i,\mathbf{\beta}}\left(\ln\left(\frac{1}{\gamma}\right)\!t\right)=\sigma_{i,\mathbf{\beta}_{\star}}\mathbbm{1}\left(t>\frac{1}{2\sigma_{i,\mathbf{\beta}_{\star}}} \right),\]

where \(\sigma_{i,\mathbf{\beta}_{\star}}\) is the \(i^{th}\) singular value of \(\mathbf{\beta}^{\star}\). Each singular value is activated at time \(-\ln\left(\gamma\right)\left(2\sigma_{i,\mathbf{\beta}_{\star}}\right)^{-1}\). Therefore, we observe an incremental learning process, where the activation begins with the largest singular value and proceeds accordingly.

Mirror descent for scalar regression.The result (b) states that the GF on the parameters \((\mathbf{W}_{1},\mathbf{W}_{2})\) implies a mirror flow on the predictor \(\mathbf{\beta}\) with the potential \(\psi_{\delta}\). To be more precise, the evolution is governed by a mirror flow with the time scaled as a function of \(\left\|\mathbf{\beta}\right\|\). This technique of time-warping was proposed in Azulay et al. (2021) for the case of a linear network with a single neuron (\(l\) = 1) with balanced initializations. In contrast, with a specific initialization shape, we show the existence of a mirror flow for an arbitrary number of neurons and unbalanced initialization of any scale. The existence of a mirror flow is surprising since the reparametrization defining linear networks is not commutative in general (Li et al., 2022). However, due to the specific initialization we use, this problem can be circumvented by preserving certain commutative properties.

The equivalence with mirror descent enables us to show that \(\mathcal{L}\left(\mathbf{\beta}(t)\right)=O(1/\gamma t)\) (see Appendix C.8), thus providing a convergence rate for the training loss independent of the conditioning of data, in contrast to Min et al. (2021); Du et al. (2019). Note that with decreasing initialization scale \(\gamma\), the convergence speed diminishes, while according to the results in Theorems 3.1 better implicit bias is achieved. This suggests the existence of a trade-off between optimization and implicit bias already observed in several works (Woodworth et al., 2020), where achieving better quality solutions is linked to slower optimization. In contrast to this behaviour, for the case of balanced initialization (Braun et al., 2022) emphasizes a decoupling between the learning speed and the quality of solutions. Conversely, we stress that in the general setting (e.g., under imbalance) such a decoupling is absent.

### Sketch of the proofs

In this section, we give a short description of the proofs of the main results from the previous sections. The common theme of the following intermediate results is to identify natural invariants of the dynamics, which can be leveraged to understand the hidden mirror structure of the flow.

**Lemma 3.1**.: _Consider the dynamics of the gradient flow (2.4) initialized at \((\mathbf{W}_{1}(0),\mathbf{W}_{2}(0))=\left(\sqrt{2\gamma}P,0\right)\). Let \(\mathbf{Z}_{1}:=\mathbf{W}_{1}P^{\top},\mathbf{Z}_{2}:=PW_{2}\) and the residual \(\mathbf{R}:=X^{\top}(Y-X\mathbf{Z}_{1}\mathbf{Z}_{2})\), then the evolution of \((\mathbf{Z}_{1},\mathbf{Z}_{2})\) is governed by the following ODE_

\[\mathbf{\dot{Z}}_{1}=\mathbf{R}\mathbf{Z}_{2}^{\top}\quad,\quad\mathbf{\dot{Z}}_{2}=\mathbf{Z}_{1} ^{\top}\mathbf{R}. \tag{3.4}\]

_Furthermore, the dynamics of gradient flow (2.4) is equivalent to (3.4), i.e., \((\mathbf{W}_{1}(t),\mathbf{W}_{2}(t))=(\mathbf{Z}_{1}(t)P,P^{\top}\mathbf{Z}_{2}(t))\) at any time \(t\)._

Lemma 3.1 derives an equivalent dynamics to equations (2.4). It shows that weights \(\mathbf{W}_{1}^{\top},\mathbf{W}_{2}\) always stay in the column span of the initialization \(P\), thus restricting their evolution to a subspace. Going forward, we derive the invariants of the dynamics (3.4).

**Lemma 3.2**.: _For the projected matrices given in (3.4), we have the following invariant,_

\[\mathbf{Z}_{1}^{\top}\mathbf{Z}_{1}-\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}=2\gamma\mathbf{I}.\]

This invariant ensures that \(\mathbf{Z}_{1}^{\top}\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\) commute which is a crucial ingredient in the proofs of Theorems 3.1, 3.2. Now, we derive the evolution of \(\mathbf{\alpha}:=\mathbf{Z}_{1}^{-\top}\mathbf{Z}_{2}\), which turns out to be the central quantity enabling our result. The lemma below describes certain properties of the evolution of \(\mathbf{\alpha}\).

**Lemma 3.3**.: _Let \(\mathbf{\alpha}:=\mathbf{Z}_{1}^{-\top}\mathbf{Z}_{2}\), we have the following time evolution of parameters:_

\[\mathbf{\dot{\alpha}}=\mathbf{R}-\mathbf{\alpha}\mathbf{R}^{\top}\mathbf{\alpha}\;,\quad\text{ and }\quad\mathbf{\beta}=\left(1-\mathbf{\alpha}\mathbf{\alpha}^{\top}\right)^{-1}\mathbf{\alpha}.\]An outline of the proof of Theorem 3.1.With an _ansatz_ on the potential that it is decomposable in terms of \(\mathbf{Z}_{1},\mathbf{Z}_{2}\), we derive KKT conditions for the constrained optimization problem

\[\operatorname*{argmin}_{X\mathbf{Z}_{1}\mathbf{Z}_{2}=Y}\psi_{1}(\mathbf{Z}_{1})+\psi_{2}( \mathbf{Z}_{2}).\]

Using Lemmas 3.3, we show that \(\mathbf{\alpha}\) stays in \(\operatorname*{span}(X)\). We use the isotropic property of the imbalance from Lemma 3.2 to find appropriate functions \(\psi_{1},\psi_{2}\) and finally prove Theorem 3.1. The proofs for theorem 3.1, 3.2 and corollary 3.1 can be found in Appendix B.

## 4 Further thoughts and perspectives

The previous section provided a deep-dive into the dynamics of the gradient flow, which we complement here with a few steps in the direction of understanding the dynamics with stochastic gradients. We investigate stochastic gradient descent (SGD) by studying its simpler counterpart, label noise gradient descent (LNGD) Blanc et al. (2020).

### The role of noise

It was observed that the noise in stochastic gradient descent has a parameter-dependent shape that induces certain regularization properties (HaoChen et al., 2021; Blanc et al., 2020). Here, we study the properties of the noise shape induced in the case of parameterization with linear neural networks.

Inspired from the analysis of HaoChen et al. (2021) and the large noise regime described by Pillaud-Vivien et al. (2022) in the context of diagonal neural networks, we design a process driven purely by noise and which carries the same geometric properties as SGD's noise. We consider the scalar (\(k=1\)) regression problem with \(l=d\) and, through an abuse of notation, denote \(\mathbf{W}_{1}=\mathbf{W}\), and \(\mathbf{W}_{2}=\mathbf{a}\). The noise-driven process which we consider is:

\[\mathbf{d}\mathbf{W}=\left(\mathbf{d}\mathbf{B}_{t}\right)\mathbf{a}^{\top}\quad,\quad \mathbf{d}\mathbf{a}=\mathbf{W}^{\top}\mathbf{d}\mathbf{B}_{t}, \tag{4.1}\]

where \(\mathbf{B}_{t}\) is a \(d\)-dimensional Brownian motion. Details on how this SDE captures the noise of SGD are deferred to Appendix D. We show that, similarly to the rich regime of the gradient flow (\(\gamma\to 0\)), this noise also carries a rich spectral bias _but for any initialization_. Indeed, we have the following result on the SDE dynamics. The proof can be found in Appendix D.

**Proposition 4.1**.: _The dynamics (4.1) has the following convergence properties_

1. _Variance explosion__. The variance of the norms of_ \(\mathbf{W},\mathbf{a}\) _explode, i.e.,_ \[\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{W}(t)\right\|^{2}\right]\to\infty \quad,\quad\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{a}(t)\right\|^{2} \right]\to\infty.\]
2. _Scale divergence__. For_ \(d\geq 5\)_, for any_ \(\alpha>0\)_, we have that,_ \[\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{W}(t)\right\|^{\alpha}\right]\to \infty\quad,\quad\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{a}(t)\right\| ^{\alpha}+\left\|\bar{\mathbf{a}}(t)\right\|^{\alpha}\right]\to\infty.\] _where_ \(\bar{\mathbf{a}}:=e^{-t}\int\limits_{0}^{t}e^{s}\mathbf{a}(s)\mathbf{d}s\) _is the exponential moving average of_ \(\mathbf{a}\)_._
3. _Alignment - spectral bias__. Denote the_ \(i^{\text{th}}\) _row of_ \(\mathbf{W}\) _as_ \(\mathbf{w}_{i}\)_. Using_ \([\mathbf{w}_{i},\mathbf{a}]\stackrel{{\text{def}}}{{=}}\mathbf{w}_{i} \mathbf{a}^{\top}-\mathbf{a}\mathbf{w}_{i}^{\top}\)_,_ \[\lim_{t\to\infty}\mathbb{E}\left[\left|[\mathbf{w}_{i},\mathbf{a}]\right|\right]\to 0.\]

For any two vectors \(u,v\), \([u,v]\) denotes the commutator of the vectors: remark that if \([u,v]=0\), then \(u,v\) are aligned, i.e, \(u=cv\), for some scalar \(c\in\mathbb{R}\). First, notice that for \(d=1\), the SDE in fact corresponds to the geometric Brownian motion with no drift and the dynamics collapses to zero (Oksendal, 2013). For dimension \(d\geq 2\), the proposition states that the system diverges and the weights grow towards infinity. However, despite the fact that the norm grows, the commutator \([\mathbf{w}_{i},\mathbf{a}]\) goes to zero, indicating that all the rows of \(\mathbf{w}_{i}\) align towards \(\mathbf{a}\). Overall, similarly to the gradient flow in the rich regime, this induces a low rank structure in \(\mathbf{W}\). This phenomenon can be further seen through the evolution of singular values, where the top singular value of \(\mathbf{W}\) grows unboundedly, whereas the remaining singular values decay to \(0\) as depicted in Figure 2(a) in the Appendix. This sheds some light on how SGD induces a particular parameter-dependent noise which implicitly biases the solutions towards having a low-rank structure of the hidden layer (Andriushchenko et al., 2022).

**Intricate dynamics in presence of drift.** Proposition 4.1 focuses on the process that is solely driven by noise. However, in general SGD also encompasses a drift term which corresponds to the dynamics studied in Section 3. The continuous-time SDE describing the process is

\[\mathrm{d}\mathbf{W}=-\nabla_{\mathbf{W}}\mathcal{L}\left(\mathbf{W},\mathbf{a}\right) \mathrm{d}t+\delta\left(\mathrm{d}\mathbf{B}_{t}\right)\mathbf{a}^{\top}\quad, \quad\mathrm{d}\mathbf{a}=-\nabla_{\mathbf{a}}\mathcal{L}\left(\mathbf{W},\mathbf{ a}\right)\mathrm{d}t+\delta\mathbf{W}^{\top}\mathrm{d}\mathbf{B}_{t},\]

where \(\delta>0\) indicates the scale of the noise. The presence of drift quickly complicates the analysis, but intuitively, the noise simplifies the model by inducing a rank reduction, whereas the drift terms prevent the weights from growing unbounded. This noise-driven mechanism relaxes the role of initialization. Empirically this is illustrated in Figure 0(b). Gradient descent already exhibits a regularization effect as it increases only one singular value while keeping the others constant. However, gradient descent with label noise (Blanc et al., 2020) enhances this regularization effect by decaying the singular values and promoting low-rank representations. As a result, even for larger initialization scales, we observe the presence of low-rank structures in the hidden layer, unlike in gradient descent. The precise characterization of the this phenomenon is left for future research.

**Experiments.** We consider a regression problem on synthetic data, with \(n=5\) samples of Gaussian data in \(\mathbb{R}^{10}\) (\(d=10\)) and the labels in \(\mathbb{R}^{3}\) (\(k=3\)) generated by a ground truth \(\mathbf{\beta}_{*}\in\mathbb{R}^{d\times k}\). We consider a network with width \(l=200\). In Figure 0(a), we show the evolution of the top-\(4\) singular values of the hidden layer \(\mathbf{W}_{1}\). We use orthogonal initialization for the network with the two scales of initialization \(\gamma=1,10^{-4}\). Note that, as depicted by Corollary 3.1, for the smaller scale only the first \(k=3\) singular values are significant in comparison to the remaining \(d-k\) singular values. This shows that the matrix is approximately rank \(k\) and the neurons align along three directions. In contrast, for the larger scale \(\gamma=1\), the final weight matrix has rank \(d\). To complement this, we also consider a Gaussian initialization with variance \(0.01\) - specifically, we initialize the inner layer with \(d=10\) Gaussian random vectors in \(\mathbb{R}^{l}\). As described when \(l\gg d\), the initialization is close to the orthogonal initialization. Hence, in this case, we can see that only \(k\) singular values grow and the final model has an approximately rank \(k\) hidden layer. In figure 0(b), we depict the time evolution of singular values for GD and LNGD on a scalar regression problem with orthogonal data in \(\mathbb{R}^{5}\) (\(n,d=5\)) and a network with \(l=200\). Further details on hyper-parameters can be found in the Appendix.

**Extension to non-linear activations.** Huh et al. (2023), Andriushchenko et al. (2022) empirically demonstrate a low-rank phenomenon through extensive experiments on deep networks with non-linear activations. However, a comprehensive theoretical comprehension of this behavior remains elusive, despite some efforts addressing these issues (Boursier et al., 2022). To show that our analysis extends beyond linear activations, we present a toy experiment for ReLU networks (see Figure 2 and further details in Figure 5). Consider a scalar regression problem in a ReLU teacher-student setup. We generate a training set of size \(10\) sampled from a random Gaussian distribution in \(\mathbb{R}^{5}\). The training data \((x_{i},y_{i})_{i=1}^{10}\in\mathbb{R}^{5}\times\mathbb{R}\) is generated by a teacher ReLU network with \(2\) neurons \((w_{0},w_{1})\), i.e.,

\[y_{i}=a_{0}\sigma(w_{0}^{\top}x_{0})+a_{1}\sigma(w_{1}^{\top}x_{1}),\]

Figure 1: (a) Vector regression with orthogonal initialization and scales \(\gamma=1,10^{-4}\) and Gaussian initialization with entries from \(N(0,0.01)\) (b) Scalar regression with Gradient Descent (GD) and Label Noise Gradient Descent (LNGD).

where \(\sigma\) is the ReLU non-linearity. We train a student network with 20 hidden neurons. Note that there are two relevant directions \(w_{0},w_{1}\) for the student network to learn, therefore we expect the hidden layer to represent these two directions (i.e., a rank 2 hidden layer, and a singular value decomposition with two non-zero singular values). This property is empirically verified in Figure 2. We plot the time evolution of singular values and when initialized at low-scale the network converges to an approximately rank-2 matrix. When initialized at a larger scale, the network weight matrix is high rank and the neurons do not learn the teacher directions.

Perspectives.Learning representations which can be transferred to downstream tasks is a key attribute for the success of deep learning (Bengio et al., 2013; LeCun et al., 2015). In this work, we present an archetypal problem where for the same predictor in functional space, there exist multiple representations in parameter space, some of which can exhibit a rich structure. This scenario presents a case for going beyond the characterization of implicit bias in the functional space (Parhi and Nowak, 2022) and further studying the implicit bias in the parameter space. Such characterizations facilitate the identification of crucial ingredients in training algorithms that enable effective feature learning.

Limitations and Future Work.This paper tackles the phenomenon of implicit bias, with the aim of furthering the understanding of how neural networks learn in practice. Unfortunately, practical models are highly nonlinear due to their activations and rely on various heuristics to achieve state-of-the-art performance, thus being difficult to grasp mathematically. This work therefore studies the simplified setting of two-layer linear neural networks. In terms of the assumptions we make, the orthogonality of initialization is only approximately faithful to practical settings where small random weights are used. Nevertheless, we are confident that this requirement can be loosened through a perturbation analysis in the vein of Gidel et al. (2019). Finally, our dynamical description of the system is yet to be completed in the vector regression case with non-whitened data. A careful set of assumptions is necessary here, and hopefully ones that are weaker than the restricted isometry property used in related works (Li et al., 2018). Finally, we only partially describe the dynamics in the presence of stochastic noise and giving a full characterization remains a desired objective of future investigations. Further discussion on these aspects is presented in Appendix C.1.

## Acknowledgments and Disclosure of Funding

AV is supported by Swiss data science fellowship.

## References

* Alvarez et al. (2004) F. Alvarez, J. Bolte, and O. Brahic. Hessian riemannian gradient flows in convex programming. _SIAM journal on control and optimization_, 43(2):477-501, 2004.
* Andriushchenko et al. (2022) M. Andriushchenko, A. Varre, L. Pillaud-Vivien, and N. Flammarion. Sgd with large step sizes learns sparse features. _arXiv preprint arXiv:2210.05337_, 2022.
* Arora et al. (2018) S. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In _Proceedings of the 35th International Conference on Machine Learning_, 2018.
* Arora et al. (2019a) S. Arora, N. Cohen, N. Golowich, and W. Hu. A convergence analysis of gradient descent for deep linear neural networks. In _International Conference on Learning Representations_, 2019a. URL [https://openreview.net/forum?id=SkMQg3C5K7](https://openreview.net/forum?id=SkMQg3C5K7).
* Arora et al. (2019b)

Figure 2: The time evolution of singular values of the hidden layer weights of a 2-layer ReLU network when trained with gradient flow initialized with Gaussian random variables with different scales. We consider a scalar regression problem in a teacher-student setup.

S. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_, 32, 2019b.
* Arora et al. (2019c) S. Arora, S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019c.
* Atanasov et al. (2022) A. Atanasov, B. Bordelon, and C. Pehlevan. Neural networks as kernel learners: The silent alignment effect. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=1Nvf1q&doom](https://openreview.net/forum?id=1Nvf1q&doom).
* Attouch (1996) H. Attouch. Viscosity solutions of minimization problems. _SIAM Journal on Optimization_, 6(3):769-806, 1996.
* Azulay et al. (2021) S. Azulay, E. Moroshko, M. S. Nacson, B. E. Woodworth, N. Srebro, A. Globerson, and D. Soudry. On the implicit bias of initialization shape: Beyond infinitesimal mirror descent. In _International Conference on Machine Learning, ICML 2021_, 2021.
* Bartlett et al. (2020) P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 2020.
* Bengio et al. (2013) Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. _IEEE Trans. Pattern Anal. Mach. Intell._, 2013.
* Berthier (2022) R. Berthier. Incremental learning in diagonal linear networks. _arXiv preprint arXiv:2208.14673_, 2022.
* Bittanti et al. (1991) S. Bittanti, A. J. Laub, and J. C. Willems. The riccati equation. 1991.
* Blanc et al. (2020) G. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In _Conference on Learning Theory, COLT 2020_, Proceedings of Machine Learning Research. PMLR, 2020.
* Boursier et al. (2022) E. Boursier, L. Pillaud-Vivien, and N. Flammarion. Gradient flow dynamics of shallow reLU networks for square loss and orthogonal inputs. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=L74c-iUX01I](https://openreview.net/forum?id=L74c-iUX01I).
* Braun et al. (2022) L. Braun, C. C. J. Domine, J. E. Fitzgerald, and A. M. Saxe. Exact learning dynamics of deep linear networks with prior knowledge. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=1Jx2vng-KiC](https://openreview.net/forum?id=1Jx2vng-KiC).
* Chizat et al. (2019) L. Chizat, E. Oyallon, and F. Bach. _On Lazy Training in Differentiable Programming_. Curran Associates Inc., Red Hook, NY, USA, 2019.
* Du et al. (2019a) S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019a.
* Du et al. (2018) S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In _Advances in Neural Information Processing Systems_, 2018.
* Du et al. (2019b) S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations_, 2019b.
* Fazel et al. (2003) M. Fazel, H. Hindi, and S. Boyd. Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices. In _Proceedings of the 2003 American Control Conference, 2003._, volume 3, pages 2156-2162 vol.3, 2003. doi: 10.1109/ACC.2003.1243393.
* Fukumizu (1998) K. Fukumizu. Effect of batch learning in multilayer neural networks. _Gen_, 1(04):1E-03, 1998.
* Ghai et al. (2020) U. Ghai, E. Hazan, and Y. Singer. Exponentiated gradient meets gradient descent. In _Proceedings of the 31st International Conference on Algorithmic Learning Theory_, volume 117 of _Proceedings of Machine Learning Research_, 2020.
* Ghaemi et al. (2019)B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers neural network. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gidel et al. (2019) G. Gidel, F. Bach, and S. Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gunasekar et al. (2017) S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems_, 30, 2017.
* Gunasekar et al. (2018) S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of optimization geometry. In _International Conference on Machine Learning_, pages 1832-1841. PMLR, 2018.
* HaoChen et al. (2021) J. Z. HaoChen, C. Wei, J. D. Lee, and T. Ma. Shape matters: Understanding the implicit bias of the noise covariance. In M. Belkin and S. Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, 2021.
* Hu et al. (2020) W. Hu, L. Xiao, and J. Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In _International Conference on Learning Representations_, 2020.
* Huh et al. (2023) M. Huh, H. Mobahi, R. Zhang, B. Cheung, P. Agrawal, and P. Isola. The low-rank simplicity bias in deep networks. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=bCiNWDmlY2](https://openreview.net/forum?id=bCiNWDmlY2).
* Jacot et al. (2018) A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Jacot et al. (2022) A. Jacot, F. Ged, B. Simsek, C. Hongler, and F. Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.
* Ji and Telgarsky (2019) Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=HJflg30qKX](https://openreview.net/forum?id=HJflg30qKX).
* Keskar et al. (2017) N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017. URL [https://openreview.net/forum?id=HloyRlYgg](https://openreview.net/forum?id=HloyRlYgg).
* Lasserre (1995) J. Lasserre. A trace inequality for matrix product. _IEEE Transactions on Automatic Control_, 1995.
* LeCun et al. (2015) Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* Li et al. (2018) Y. Li, T. Ma, and H. Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In _Conference On Learning Theory_, pages 2-47. PMLR, 2018.
* Li et al. (2021) Z. Li, Y. Luo, and K. Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In _International Conference on Learning Representations_, 2021.
* Li et al. (2022) Z. Li, T. Wang, J. D. Lee, and S. Arora. Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=k4KHXS6_zOV](https://openreview.net/forum?id=k4KHXS6_zOV).
* Min et al. (2021) H. Min, S. Tarmoun, R. Vidal, and E. Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Neyshabur et al. (2014) B. Neyshabur, R. Tomioka, and N. Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Oksendal (2013) B. Oksendal. _Stochastic differential equations: an introduction with applications_. Springer Science & Business Media, 2013.
* Oksendal (2014)R. Parhi and R. D. Nowak. What kinds of functions do deep neural networks learn? insights from variational spline theory. _SIAM Journal on Mathematics of Data Science_, 2022.
* Pennington et al. [2018] J. Pennington, S. Schoenholz, and S. Ganguli. The emergence of spectral universality in deep networks. In _International Conference on Artificial Intelligence and Statistics_, pages 1924-1932. PMLR, 2018.
* Pesme and Flammarion [2023] S. Pesme and N. Flammarion. Saddle-to-saddle dynamics in diagonal linear networks, 2023.
* Pesme et al. [2021] S. Pesme, L. Pillaud-Vivien, and N. Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. _Advances in Neural Information Processing Systems_, 34:29218-29230, 2021.
* Pillaud-Vivien et al. [2022] L. Pillaud-Vivien, J. Reygner, and N. Flammarion. Label noise (stochastic) gradient descent implicitly solves the lasso for quadratic parametrisation. In _Conference on Learning Theory_, pages 2127-2159. PMLR, 2022.
* Saxe et al. [2014] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In _2nd International Conference on Learning Representations, ICLR 2014_, 2014.
* Saxe et al. [2019] A. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019.
* Soltanolkotabi et al. [2017] M. Soltanolkotabi, A. Javanmard, and J. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. _IEEE Transactions on Information Theory_, 65:742-769, 2017.
* Tarmoun et al. [2021] S. Tarmoun, G. Franca, B. D. Haeffele, and R. Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10153-10161. PMLR, 18-24 Jul 2021.
* Timor et al. [2023] N. Timor, G. Vardi, and O. Shamir. Implicit regularization towards rank minimization in relu networks. In _Proceedings of The 34th International Conference on Algorithmic Learning Theory_, Proceedings of Machine Learning Research, 2023.
* Vardi [2022] G. Vardi. On the implicit bias in deep-learning algorithms. _arXiv preprint arXiv:2208.12591_, 2022.
* Woodworth et al. [2020] B. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Srebro. Kernel and rich regimes in overparametrized models. In J. Abernethy and S. Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 3635-3673. PMLR, 09-12 Jul 2020.
* Yun et al. [2021] C. Yun, S. Krishnan, and H. Mobahi. A unifying view on implicit bias in training linear neural networks. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=ZsZM-4iMQ&H](https://openreview.net/forum?id=ZsZM-4iMQ&H).
* Zhang et al. [2016] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. _arXiv preprint arXiv:1611.03530_, 2016.

### Organization

The appendix is organized as follows,

* In section A, we present the experiment details.
* In section B, we present the proof of Theorems 3.1, 3.2 and Corollary 3.1.
* section C contains the proofs of the supporting lemmas.
* In the final section D, we discuss our choice on the noise model and present the proof of Proposition 4.1.
* For the results referenced in the main section, the convergence rate of mirror flow can be found at C.8, the time evolution of singular values and their limiting jump process is available at C.12, the stationarity of singular vectors in the orthogonal case at C.11, the discussion on the noise model at \(\,\mathsf{D}\).

Notations.For matrices of appropriate dimensions, we use \([A,B]\) to denote \(AB-BA\).

## Appendix A Experiment Details

Experiments.We discretize the SDE (4.1) with a step-size \(\sim\nicefrac{{1}}{{\sqrt{d}}}\). We simulate three parallel runs and track the evolution of singular values and the evolution of alignment using the commutator of the row \(\mathbf{w}_{i},\mathbf{a}\), i.e., \([\mathbf{w}_{i},\mathbf{a}]\coloneqq\left(\mathbf{w}_{i}\mathbf{a}^{\top}-\mathbf{a} \mathbf{w}_{i}^{\top}\right)\). We consider the SDE for dimemsion \(d=2\). The evolution is initialized at \(\mathbf{W}(0)=\mathbf{I}_{2}\) and \(\mathbf{a}(0)=0\). As seen in Figure 2(a), the noise shape inherently induces a low-rank structure where it intensifies a singular value and significantly diminishes the other singular value. As predicted by our proposition (4.1), figure 2(b) shows that the rows of \(\mathbf{W}\) align with \(\mathbf{a}\), thus giving a rank \(1\) structure. The experiments were run on a 16-GB RAM Apple M1 mac with OS Ventura 13.3.1.

## Appendix B Main Proofs

In this section, we present the proofs of the theorem discussed in Section 3.

**Theorem 3.1**.: _Let \((\mathbf{W}_{1},\mathbf{W}_{2})\) be the process that follows the GF equations (2.4a)-(2.4b), initialized according to condition \(I_{\gamma}\), for some \(\gamma>0\). Then_

1. _The parameters converge to a global optimum of the loss_ \[\lim_{t\to\infty}\left(\mathbf{W}_{1}(t),\mathbf{W}_{2}(t)\right)=(\mathbf{W}_{1}^{\infty},\mathbf{W}_{2}^{\infty})\in\mathcal{I}_{\mathbf{\theta}}.\]

Figure 3: Three parallel runs of the noise dynamics Eq. (4.1) for \(d=2\). (a) The evolution of singular values with \(\sigma_{0}\) increasing and \(\sigma_{1}\) decaying. (b) Measuring the norm of the commutator again as predicted by Proposition 4.1.

_._
2. _The linear predictor_ \(\mathbf{\beta}\) _converges to the minimum_ \(\ell_{2}\)_-norm interpolator_ \[\lim_{t\to\infty}\mathbf{\beta}(t)=\operatorname*{argmin}_{X\mathbf{\beta}=Y}\ \left\| \mathbf{\beta}\right\|_{F}\stackrel{{\mathrm{def}}}{{=}}\mathbf{\beta}_{*}.\]
3. _We have the following variational characterization of the limiting parameters_ \[(\mathbf{W}_{1}^{\infty},\mathbf{W}_{2}^{\infty})\in\operatorname*{argmin}_{X\mathbf{W}_{1 }\mathbf{W}_{2}=Y}\frac{1}{2}\big{\|}\mathbf{W}_{2}\big{\|}_{F}^{2}+\frac{1}{2}\big{\|} \mathbf{W}_{1}\big{\|}_{F}^{2}-\gamma\log\big{(}\det\,\big{(}\mathbf{W}_{1}\mathbf{W}_{1}^ {\top}\big{)}\big{)}.\] (3.1)

Proof.: We initialize such that \(\mathbf{W}_{1}=\sqrt{2\gamma}P,\mathbf{W}_{2}=0\). Lemma 3.1 states that the dynamics of gradient flow is restricted to a subspace and can be equivalently described by,

\[\overset{\mathbf{\cdot}}{\mathbf{Z}}_{1}^{\star}=\mathbf{R}\mathbf{Z}_{2}^{\top}\quad,\quad \overset{\mathbf{\cdot}}{\mathbf{Z}}_{2}^{\star}=\mathbf{Z}_{1}^{\top}\mathbf{R}.\] (B.1)

where \(\mathbf{R}:=X^{\top}(Y-X\mathbf{Z}_{1}\mathbf{Z}_{2})\).

1. To show the convergence, we track the evolution of \(\operatorname{tr}\big{(}\mathbf{R}^{\top}\mathbf{R}\big{)}\) and use the following descent inequality to show that it converges to \(0\). With \(\lambda_{\min}(X^{T}X)\) denoting the smallest eigenvalue of the \(X^{\top}X\), the descent inequality (C.13) is as follows, \[\overset{\mathbf{\cdot}}{\operatorname{tr}\,(\mathbf{R}^{\top}\mathbf{R})}\leq-2\gamma \lambda_{\min}(X^{\top}X)\operatorname{tr}\big{(}\mathbf{R}^{\top}\mathbf{R}\big{)}.\] Refer to Lemma C.6 for the detailed proof.
2. To show that \(\mathbf{\beta}\to\mathbf{\beta}_{*}\) in the limit, we show that \(\mathbf{\beta}\in span\big{(}X^{\top}\big{)}\), i.e., \(\mathbf{\beta}=X^{\top}\lambda\), for some \(\lambda\). This satisfies the KKT conditions required for the following minimization problem. \[\mathbf{\beta}_{\infty}\in\operatorname*{argmin}_{X\mathbf{\beta}=Y}\frac{1}{2}\big{\|} \mathbf{\beta}\big{\|}^{2}=\mathbf{\beta}_{*}.\] The complete proof can be found at Lemma C.6.
3. For the limit of the projected dynamics \((\mathbf{Z}_{1}^{\infty},\mathbf{Z}_{2}^{\infty})\coloneqq\lim_{t\to\infty}\big{(}\bm {Z}_{1}(t),\mathbf{Z}_{2}(t)\big{)}\), Lemma C.4 shows the following, \[(\mathbf{Z}_{1}^{\infty},\mathbf{Z}_{2}^{\infty})\in\operatorname*{argmin}_{X\mathbf{Z}_{1 }\mathbf{Z}_{2}=y}\frac{1}{2}\big{\|}\mathbf{Z}_{2}\big{\|}_{F}^{2}+\frac{1}{2}\big{\|} \mathbf{Z}_{1}\big{\|}_{F}^{2}-\gamma\log\big{(}\det\,\big{(}\mathbf{Z}_{1}\mathbf{Z}_{1} ^{\top}\big{)}\big{)}.\] Using the transformation from Eq. C.7, we have, \[\mathbf{W}_{1}=\mathbf{Z}_{1}P,\quad\mathbf{W}_{2}=P^{\top}\mathbf{Z}_{2}.\] Thus, \[\big{\|}\mathbf{W}_{1}\big{\|}_{F}=\big{\|}\mathbf{Z}_{1}\big{\|}_{F}, \quad\big{\|}\mathbf{W}_{2}\big{\|}_{F}=\big{\|}\mathbf{Z}_{2}\big{\|}_{F},\] \[\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top}=\mathbf{W}_{1}\mathbf{W}_{1}^{\top},\quad\mathbf{ Z}_{1}\mathbf{Z}_{2}=\mathbf{W}_{1}\mathbf{W}_{2}.\] Therefore, \[(\mathbf{W}_{1}^{\infty},\mathbf{W}_{2}^{\infty})\in\operatorname*{argmin}_{X\mathbf{W}_{1 }\mathbf{W}_{2}=Y}\frac{1}{2}\big{\|}\mathbf{W}_{2}\big{\|}_{F}^{2}+\frac{1}{2}\big{\|} \mathbf{W}_{1}\big{\|}_{F}^{2}-\gamma\log\big{(}\det\,\big{(}\mathbf{W}_{1}\mathbf{W}_{1}^ {\top}\big{)}\big{)}.\] This hold on the set \(\{(\mathbf{W}_{1},\mathbf{W}_{2}):\mathbf{W}_{1}P_{\perp}^{\top}=0,P_{\perp}\mathbf{W}_{2}=0\}\) which is ensured from gradient flow from Lemma 3.1.

**Corollary 3.1**.: _[Singular values at the limit] Using the same quantities as in Theorem 3.1, denote \((\sigma_{1}(\mathbf{W}_{1}^{\infty}),\ldots,\sigma_{d}(\mathbf{W}_{1}^{\infty}))\) and \((\sigma_{1}(\mathbf{\beta}_{*}),\ldots,\sigma_{k}(\mathbf{\beta}_{*}))\) the singular values of \(\mathbf{W}_{1}^{\infty},\mathbf{\beta}_{*}\) are_

\[\sigma_{i}\left(\mathbf{W}_{1}^{\infty}\right) =\left(\sqrt{\sigma_{i}\left(\mathbf{\beta}_{*}\right)^{2}+\gamma^{2 }}+\gamma\right)^{1/2},\text{ for }1\leq i\leq k.\] \[\sigma_{i}\left(\mathbf{W}_{1}^{\infty}\right) =\left(2\gamma\right)^{1/2},\text{ for }k<i\leq d.\]

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

Proof.: From Lemma C.3, we have that the \(\left(\mathbf{Z}_{1}^{\infty}\right)^{-\top}\mathbf{Z}_{2}^{\infty}\in\text{span}(X)\), so the condition (**P2**) from Proposition C.5 holds. Note that from (**P2**) of Proposition C.5, we have,

\[\nabla\Psi_{1}\left(\mathbf{Z}_{1}^{\infty}\right) =\left(\mathbf{Z}_{1}^{\infty}\right)^{-\top}\mathbf{Z}_{2}^{\infty}\left( \mathbf{Z}_{2}^{\infty}\right)^{\top},\] \[=\left(\mathbf{Z}_{1}^{\infty}\right)^{-\top}\left(\left(\mathbf{Z}_{1}^{ \infty}\right)^{\top}\mathbf{Z}_{1}^{\infty}-2\gamma\mathbf{I}\right)\!,\] \[=\mathbf{Z}_{1}^{\infty}-2\gamma\left(\mathbf{Z}_{1}^{\infty}\right)^{- \top}.\]

which is satisfied by the potential

\[\Psi_{1}\left(\mathbf{Z}_{1}\right)=\frac{1}{2}\big{\|}\mathbf{Z}_{1}\big{\|}_{F}^{2}- \gamma\log\big{(}\det\,\left(\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top}\right)\!\big{)}.\]

When the imbalance is not isotropic, i.e., \(\mathbf{Z}_{1}^{\top}\mathbf{Z}_{1}-\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}=D\), where \(D\) is some diagonal matrix (\(\neq c\mathbf{I}\), for any constant \(c\)). It this case,

\[\nabla\Psi_{1}\left(\mathbf{Z}_{1}^{\infty}\right)=\mathbf{Z}_{1}^{\infty}-D\left(\mathbf{ Z}_{1}^{\infty}\right)^{-\top},\]

and there exists no such function \(\Psi_{1}\) and the proof breaks. 

**Proposition C.5**.: _Let \(\left(\mathbf{Z}_{1}^{*},\mathbf{Z}_{2}^{*}\right)\) satisfy the following minimization problem_

\[\left(\mathbf{Z}_{1}^{*},\mathbf{Z}_{2}^{*}\right)=\operatorname*{argmin}_{X\mathbf{Z}_{1} \mathbf{Z}_{2}=y}\Psi_{1}\left(\mathbf{Z}_{1}\right)+\Psi_{2}\left(\mathbf{Z}_{2}\right),\]

_for some non-negative potential functions \(\Psi_{1}\left(\mathbf{Z}_{1}\right),\Psi_{2}\left(\mathbf{Z}_{2}\right)\). Then, \(\left(\mathbf{Z}_{1}^{*},\mathbf{Z}_{2}^{*}\right)\) satisfies_

* \(\left(\mathbf{Z}_{1}^{*}\right)^{-\top}\nabla\Psi_{2}(\mathbf{Z}_{2}^{*})\in\operatorname {span}\left(X\right)\)_,_
* \(\nabla\Psi_{1}\left(\mathbf{Z}_{1}^{*}\right)=\left(\mathbf{Z}_{1}^{*}\right)^{-\top} \nabla\Psi_{2}(\mathbf{Z}_{2}^{*})(\mathbf{Z}_{2}^{*})^{\top}\)_._

Proof.: The Lagrangian for the minimization problem above is,

\[\mathcal{L}\left(\mathbf{Z}_{1},\mathbf{Z}_{2},\lambda\right)=\Psi_{1}\left(\mathbf{Z}_{1} \right)+\Psi_{2}\left(\mathbf{Z}_{2}\right)+\left\langle\lambda,X\mathbf{Z}_{1}\mathbf{Z} _{2}-y\right\rangle.\]

Taking derivatives w.r.t. to \(\mathbf{Z}_{1},\mathbf{Z}_{2}\), we get,

\[\nabla_{\mathbf{Z}_{1}}\mathcal{L}\left(\mathbf{Z}_{1},\mathbf{Z}_{2},\lambda \right)=\nabla\Psi_{1}\left(\mathbf{Z}_{1}\right)+\left(X^{\top}\lambda\right)\mathbf{ Z}_{2}^{\top}.\] \[\nabla_{\mathbf{Z}_{2}}\mathcal{L}\left(\mathbf{Z}_{1},\mathbf{Z}_{2},\lambda \right)=\nabla\Psi_{2}\left(\mathbf{Z}_{2}\right)+\mathbf{Z}_{1}^{\top}\left(X^{\top} \lambda\right)\!.\]

As \(\mathbf{Z}_{1}^{*},\mathbf{Z}_{2}^{*}\) should satisfy \(\nabla\mathcal{L}=0\).

\[\left(\mathbf{Z}_{1}^{*}\right)^{-\top}\nabla\Psi_{2}(\mathbf{Z}_{2}^{*})=-X^{\top}\lambda,\] \[\nabla\Psi_{1}\left(\mathbf{Z}_{1}^{*}\right)=\left(\mathbf{Z}_{1}^{*} \right)^{-\top}\nabla\Psi_{2}(\mathbf{Z}_{2}^{*})(\mathbf{Z}_{2}^{*})^{\top}\]

**Lemma C.6**.: _Let \(\left(\mathbf{Z}_{1},\mathbf{Z}_{2}\right)\) be the process that follows the GF equations (3.1), initialized according to condition \(I_{\gamma}\), for some \(\gamma>0\). Then_

* _The parameters converge to a global optimum of the loss_ \[\lim_{t\to\infty}Y-X\mathbf{Z}_{1}(t)\mathbf{Z}_{2}(t)=0.\]
* _The linear predictor_ \(\mathbf{\beta}\) _converges to the minimum_ \(\ell_{2}\)_-norm interpolator_ \[\lim_{t\to\infty}\mathbf{\beta}(t)=\operatorname*{argmin}_{X\mathbf{\beta}=Y}\ \left\|\mathbf{\beta}\right\|_{2}\stackrel{{\text{def}}}{{=}}\mathbf{ \beta}_{*}.\]

Proof.: The evolution of \(\mathbf{R}\) writes,

\[\begin{CD}\mathbf{\overset{\cdot}{R}}&=-(X^{\top}X)\left[\mathbf{Z}_{1}\mathbf{Z}_{1}^{ \top}\mathbf{R}+\mathbf{R}\mathbf{Z}_{2}^{\top}\mathbf{Z}_{2}\right],\\ \mathbf{\overset{\cdot}{\operatorname{tr}\,\left(\mathbf{R}^{\top}\mathbf{R}\right)}}&=-2 \mathrm{tr}\left(\mathbf{R}^{\top}(X^{\top}X)\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top}\mathbf{R} \right)-2\mathrm{tr}\left(\mathbf{R}^{\top}(X^{\top}X)\mathbf{R}\mathbf{Z}_{2}^{\top}\mathbf{Z}_{ 2}\right).\end{CD}\]

[MISSING_PAGE_FAIL:20]

Recall from Lemma 3.3 that

\[\mathbf{\beta}=2\gamma(\mathbf{I}-\mathbf{\alpha}\mathbf{\alpha}^{\top})^{-1}\mathbf{\alpha}.\]

Using Sherman-Morrison,

\[(\mathbf{I}-\mathbf{\alpha}\mathbf{\alpha}^{\top})^{-1} =\mathbf{I}+\frac{\mathbf{\alpha}\mathbf{\alpha}^{\top}}{1-\left\|\mathbf{ \alpha}\right\|^{2}}.\] \[\mathbf{\beta} =2\gamma\left[(\mathbf{I}-\mathbf{\alpha}\mathbf{\alpha}^{\top})^{-1} \right]\mathbf{\alpha}=2\gamma\left[\mathbf{I}+\frac{\mathbf{\alpha}\mathbf{\alpha}^{\top}} {1-\left\|\mathbf{\alpha}\right\|^{2}}\right]\mathbf{\alpha}=2\gamma\frac{\mathbf{\alpha}} {1-\left\|\mathbf{\alpha}\right\|^{2}}.\]

Taking the norms, we get,

\[\left\|\mathbf{\beta}\right\| =2\gamma\frac{\left\|\mathbf{\alpha}\right\|}{1-\left\|\mathbf{\alpha} \right\|^{2}},\quad\left\|\mathbf{\alpha}\right\|^{2}+\frac{2\gamma}{\left\|\mathbf{ \beta}\right\|}\left\|\mathbf{\alpha}\right\|-1=0.\] \[\left\|\mathbf{\alpha}\right\| =-\frac{\gamma}{\left\|\mathbf{\beta}\right\|}+\sqrt{\left(\frac{ \gamma}{\left\|\mathbf{\beta}\right\|}\right)^{2}+1}.\] \[1-\left\|\mathbf{\alpha}\right\|^{2} =1-\left[-\frac{\gamma}{\left\|\mathbf{\beta}\right\|}+\sqrt{\left( \frac{\gamma}{\left\|\mathbf{\beta}\right\|}\right)^{2}+1}\right]^{2},\] \[=2\frac{\gamma}{\left\|\mathbf{\beta}\right\|}\sqrt{\left(\frac{ \gamma}{\left\|\mathbf{\beta}\right\|}\right)^{2}+1}-2\left(\frac{\gamma}{\left\| \mathbf{\beta}\right\|}\right)^{2},\] \[=\frac{2\gamma}{\sqrt{\gamma^{2}+\left\|\mathbf{\beta}\right\|^{2}}+ \gamma}.\]

Now consider the evolution of \(\mathbf{\alpha}\)

\[\overset{\mathbf{\cdot}}{\mathbf{\alpha}} =\left[\mathbf{I}-\mathbf{\alpha}\mathbf{\alpha}^{\top}\right]\mathbf{r},\] \[\left[\mathbf{I}-\mathbf{\alpha}\mathbf{\alpha}^{\top}\right]^{-1} \overset{\mathbf{\cdot}}{\mathbf{\alpha}} =\mathbf{r},\] \[\left[\mathbf{I}+\frac{\mathbf{\alpha}\mathbf{\alpha}^{\top}}{1-\left\| \mathbf{\alpha}\right\|^{2}}\right]\overset{\mathbf{\cdot}}{\mathbf{\alpha}} =\mathbf{r}.\]

One cannot write \(\left[\mathbf{I}+\frac{\mathbf{\alpha}\mathbf{\alpha}^{\top}}{1-\left\|\mathbf{\alpha} \right\|^{2}}\right]\) as a Hessian of any function of \(\mathbf{\alpha}\). However, multiplying on both sides with \((1-\left\|\mathbf{\alpha}\right\|^{2})^{-1/2}\), we obtain,

\[\left[\frac{\mathbf{I}}{\left(1-\left\|\mathbf{\alpha}\right\|^{2} \right)^{1/2}}+\frac{\mathbf{\alpha}\mathbf{\alpha}^{\top}}{\left(1-\left\|\mathbf{\alpha }\right\|^{2}\right)^{3/2}}\right]\overset{\mathbf{\cdot}}{\mathbf{\alpha}} =\frac{\mathbf{r}}{\left(1-\left\|\mathbf{\alpha}\right\|^{2}\right)^{1/2}},\] \[\frac{\mathrm{d}}{\mathrm{d}t}\ \left[\frac{\mathbf{\alpha}}{\left(1- \left\|\mathbf{\alpha}\right\|^{2}\right)^{1/2}}\right] =\frac{\mathbf{r}}{\left(1-\left\|\mathbf{\alpha}\right\|^{2}\right)^{1/2}}.\]

We tackle the left hand side as follows,

\[\mathbf{\beta} =\frac{2\gamma\mathbf{\alpha}}{1-\left\|\mathbf{\alpha}\right\|^{2}},\] \[\frac{\mathbf{\alpha}}{\left(1-\left\|\mathbf{\alpha}\right\|^{2}\right)^ {1/2}} =\frac{\mathbf{\beta}}{2\gamma}\left(1-\left\|\mathbf{\alpha}\right\|^{2} \right)^{1/2}=\frac{\mathbf{\beta}}{2\gamma}\left(\frac{2\gamma}{\sqrt{\gamma^{2} +\left\|\mathbf{\beta}\right\|^{2}}+\gamma}\right)^{1/2},\] \[=\frac{\mathbf{\beta}}{\sqrt{2\gamma}\left(\sqrt{\gamma^{2}+\left\| \mathbf{\beta}\right\|^{2}}+\gamma\right)^{1/2}}.\]Substituting the above expression and also \(\left(1-\left\|\mathbf{\alpha}\right\|^{2}\right)^{1/2}\) on the RHS gives us,

\[\frac{\mathrm{d}}{\mathrm{d}t}\,\left[\frac{\mathbf{\beta}}{\left(\sqrt{ \gamma^{2}+\left\|\mathbf{\beta}\right\|^{2}}+\gamma\right)^{1/2}}\right]=\left( \sqrt{\gamma^{2}+\left\|\mathbf{\beta}\right\|^{2}}+\gamma\right)^{1/2}\mathbf{r}.\]

Define the mirror potential,

\[\psi_{\gamma}\left(\mathbf{\beta}\right)\overset{\mathrm{def}}{=}\frac{2}{3}\left[ \sqrt{\left\|\mathbf{\beta}\right\|^{2}+\gamma^{2}}+\gamma\right]^{3/2}-2\gamma \left[\sqrt{\left\|\mathbf{\beta}\right\|^{2}+\gamma^{2}}+\gamma\right]^{1/2}.\] (C.15)

Using the fact that,

\[\frac{\mathrm{d}}{\mathrm{d}x}\left[\frac{2}{3}\left[\sqrt{x^{2}+p^{2}}+q \right]^{3/2}-2q\left[\sqrt{x^{2}+p^{2}}+q\right]^{1/2}\right]=\left[\sqrt{x^{ 2}+p^{2}}+q\right]^{-1/2}x.\]

We have,

\[\nabla\psi_{\gamma}\left(\mathbf{\beta}\right)=\left(\gamma+\sqrt{\left\|\mathbf{\beta }\right\|^{2}+\gamma^{2}}\right)^{-1/2}\mathbf{\beta}.\]

Thus, we can write it as a continuous-time mirror descent.

\[\widehat{\nabla\psi_{\gamma}\left(\mathbf{\beta}\right)}=-\left(\sqrt{\gamma^{2}+ \left\|\mathbf{\beta}\right\|^{2}}+\gamma\right)^{1/2}\nabla\mathcal{L}\left(\mathbf{ \beta}\right).\]

**Lemma C.8** (**Convergence rate of mirror flow)**.: _For the dynamics of the mirror flow given by Eq. (C.14), we have the following rate of convergence,_

\[\mathcal{L}\left(\mathbf{\beta}\left(\mathbf{t}\right)\right)\leq\frac{D_{\psi_{ \gamma}}\left(\mathbf{\beta}_{*},\mathbf{\beta}_{0}\right)}{\sqrt{2\gamma}\;\mathbf{t}}.\]

Proof.: Note that \(\psi_{\gamma}\) is convex. Let \(D_{\psi_{\gamma}}\left(.,.\right)\) be the Bregman divergence defined with the potential \(\psi_{\gamma}(.)\). Taking the time derivative of the Bregman divergence, we get,

\[\frac{\mathrm{d}}{\mathrm{d}t}\;D_{\psi_{\gamma}}\left(\mathbf{\beta} _{*},\mathbf{\beta}\right) =\left\langle\widehat{\nabla\psi_{\gamma}(\mathbf{\beta})},\mathbf{\beta} -\mathbf{\beta}*\right\rangle,\] \[=-\left(\sqrt{\gamma^{2}+\left\|\mathbf{\beta}\right\|^{2}}+\gamma \right)^{1/2}\left\langle\nabla\mathcal{L}\left(\mathbf{\beta}\right),\mathbf{\beta}- \mathbf{\beta}*\right\rangle.\]

We have,

\[-\left(\sqrt{\gamma^{2}+\left\|\mathbf{\beta}\right\|^{2}}+\gamma \right)^{1/2}\leq-\sqrt{2\gamma}\]

Using the convexity of \(\mathcal{L}\left(.\right)\) in \(\mathbf{\beta}\), we have

\[\left\langle\nabla\mathcal{L}\left(\mathbf{\beta}\right),\mathbf{\beta}-\mathbf{\beta}* \right\rangle\leq\mathcal{L}\left(\mathbf{\beta}\right)-\mathcal{L}\left(\mathbf{\beta }_{*}\right)=\mathcal{L}\left(\mathbf{\beta}\right).\]

The last step is using the existence of an interpolating solution. Substituting the above two inequalities, we get,

\[\frac{\mathrm{d}}{\mathrm{d}t}\;D_{\psi_{\gamma}}\left(\mathbf{\beta}_{*},\mathbf{ \beta}\right)\leq-\sqrt{2\gamma}\mathcal{L}\left(\mathbf{\beta}\right).\]

Integrating, we get,

\[\int\limits_{0}^{t}\mathcal{L}\left(\mathbf{\beta}(s)\right)ds\leq \frac{1}{\sqrt{2\gamma}}\left[D_{\psi_{\gamma}}\left(\mathbf{\beta}_{*},\mathbf{\beta}_ {0}\right)-D_{\psi_{\gamma}}\left(\mathbf{\beta}_{*},\mathbf{\beta}_{t}\right)\right] \leq\frac{D_{\psi_{\gamma}}\left(\mathbf{\beta}_{*},\mathbf{\beta}_{0}\right)}{\sqrt{2 \gamma}}.\]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

Since \((\mathbf{B}(0)-\mathbf{B}^{\top}(0))=0\) is a fixed point of the above equation, identity (C.18) is proven.

Finally, we show \(\mathbf{U}_{\mathbf{R}}=U_{\mathbf{\beta}^{*}}\) and \(\mathbf{V}_{\mathbf{R}}=V_{\mathbf{\beta}^{*}}\) using yet another invariance, namely

\[\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^{\top}=\mathbf{Z}_{2}\mathbf{R}^ {\top}\mathbf{Z}_{1},\forall t>0.\] (C.22)

We proceed by computing the associated time derivatives and showing that their difference is null:

\[\widetilde{\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^{\top}}- \widetilde{\mathbf{Z}_{2}\mathbf{R}^{\top}\mathbf{Z}_{1}} =\mathbf{Z}_{1}^{\top}\mathbf{Z}_{2}^{\top}-\mathbf{Z}_{2} \mathbf{\overset{\boldsymbol{\cdot}}{\mathbf{R}}}\mathbf{Z}_{1}\] (C.23) \[=-\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^{\top}\mathbf{Z}_ {2}\mathbf{Z}_{2}^{\top}-2\gamma\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^ {\top}-\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\mathbf{Z}_{1}^{\top}\mathbf{R} \mathbf{Z}_{2}^{\top}\] (C.24) \[\qquad+\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\mathbf{Z}_{2}\mathbf{ R}^{\top}\mathbf{Z}_{1}+2\gamma\mathbf{Z}_{2}\mathbf{R}^{\top}\mathbf{Z}_{1}+ \mathbf{Z}_{2}\mathbf{R}^{\top}\mathbf{Z}_{1}+\mathbf{Z}_{2}\mathbf{R}^{\top} \mathbf{Z}_{1}\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\] (C.25) \[=\left(\mathbf{Z}_{2}\mathbf{R}^{\top}\mathbf{Z}_{1}-\mathbf{Z}_{ 1}^{\top}\mathbf{R}\mathbf{Z}_{2}^{\top}\right)\mathbf{Z}_{2}\mathbf{Z}_{2}^{ \top}+\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\left(\mathbf{Z}_{2}\mathbf{R}^{\top} \mathbf{Z}_{1}-\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^{\top}\right)\] (C.26) \[\qquad-2\gamma\left(\mathbf{Z}_{1}^{\top}\mathbf{R}\mathbf{Z}_{2}^ {\top}-\mathbf{Z}_{2}\mathbf{R}^{\top}\mathbf{Z}_{1}\right),\] (C.27)

where we used (C.20) and the invariance (C.8). Since \(\mathbf{Z}_{2}(0)\mathbf{R}^{\top}(0)\mathbf{Z}_{1}(0)-\mathbf{Z}_{1}(0)^{\top }\mathbf{R}(0)\mathbf{Z}_{2}(0)^{\top}=0\), it is a fixed point of the above equation and we have shown (C.22).

Finally, it holds that

\[\mathbf{Z}_{1}^{\top}R\mathbf{Z}_{2}^{\top}=\mathbf{Z}_{2}\mathbf{ R}^{\top}\mathbf{Z}_{1} \Longleftrightarrow\mathbf{Z}_{1}^{\top}\mathbf{\beta}^{*}\mathbf{Z}_{2}^{ \top}-\mathbf{Z}_{1}^{\top}\mathbf{\beta}\mathbf{Z}_{2}^{\top}=\mathbf{Z}_{2}\mathbf{ \beta}^{*\top}\mathbf{Z}_{1}-\mathbf{Z}_{2}\mathbf{\beta}^{\top}\mathbf{Z}_{1}\] \[\Longleftrightarrow\mathbf{Z}_{1}^{\top}\mathbf{\beta}^{*}\mathbf{Z} _{2}^{\top}=\mathbf{Z}_{2}\mathbf{\beta}^{*\top}\mathbf{Z}_{1}\] \[\Longleftrightarrow\mathbf{\beta}^{*}\mathbf{\alpha}^{\top}=\mathbf{\alpha} \mathbf{\beta}^{*\top},\] (C.28)

where the second equivalence comes from the fact that \(\mathbf{Z}_{1}^{\top}\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\mathbf{Z}_{2}^{\top}\) are simultaneously diagonalizable and thus commute (from invariance (C.8)), and the second equivalence comes from multiplying left and right with \(\mathbf{Z}_{1}^{-\top}\) and \(\mathbf{Z}_{1}^{-1}\), respectively and the definition of \(\mathbf{\alpha}\). Furthermore,

\[R^{\top}\mathbf{\alpha}=\mathbf{\alpha}^{\top}R \Longleftrightarrow\mathbf{\beta}^{*\top}\mathbf{\alpha}-\beta^{\top}\mathbf{ \alpha}=\mathbf{\alpha}^{\top}\mathbf{\beta}^{*}-\mathbf{\alpha}^{\top}\beta\] \[\Longleftrightarrow\mathbf{\beta}^{*\top}\mathbf{\alpha}=\mathbf{\alpha}^{ \top}\mathbf{\beta}^{*},\] (C.29)

where the second line comes from the fact that \(\mathbf{\beta}^{\top}\mathbf{\alpha}=\mathbf{\alpha}^{\top}\mathbf{Z}_{1}\mathbf{Z}_{1}^{ \top}\mathbf{\alpha}=\mathbf{\alpha}^{\top}\mathbf{\beta}\).

From repeated applications of (C.28) and (C.29) we obtain that

\[\left(\mathbf{\alpha}^{\top}\mathbf{\alpha}\right)\left(\mathbf{\beta}^{*\top}\mathbf{\beta}^ {*}\right)=\left(\mathbf{\beta}^{*\top}\mathbf{\beta}^{*}\right)\left(\mathbf{\alpha}^{ \top}\mathbf{\alpha}\right)\quad\text{and}\quad\left(\mathbf{\alpha}\mathbf{\alpha}^{\top }\right)\left(\mathbf{\beta}^{*}\mathbf{\beta}^{*\top}\right)=\left(\mathbf{\beta}^{*}\bm {\beta}^{*\top}\right)\left(\mathbf{\alpha}\mathbf{\alpha}^{\top}\right),\] (C.30)

and the final identity follows. 

**Theorem C.12** (**Time evolution of singular values - orthogonal data)**.: _Assume that \(X^{\top}X=I_{d}\). Then, the \(i^{th}\) singular value of \(\mathbf{\beta}\) evolves over time as_

\[\sigma_{\mathbf{\beta},i}(t)=\frac{2\gamma\left(\sqrt{1+\frac{\gamma^{2}}{\sigma_{*, i}^{2}}}\left[\frac{\gamma+\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}-\exp\left(-2t\sqrt{ \sigma_{*,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}- \gamma\right)}{\gamma+\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}+\exp\left(-2t\sqrt{ \sigma_{*,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}- \gamma\right)}\right]-\frac{\gamma}{\sigma_{*,i}}\right)}{1-\left(\sqrt{1+ \frac{\gamma^{2}}{\sigma_{*,i}^{2}}}\left[\frac{\gamma+\sqrt{\sigma_{*,i}^{2}+ \gamma^{2}}-\exp\left(-2t\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}\right)\left(\sqrt{ \sigma_{*,i}^{2}+\gamma^{2}}-\gamma\right)}{\gamma+\sqrt{\sigma_{*,i}^{2}+ \gamma^{2}}+\exp\left(-2t\sqrt{\sigma_{*,i}^{2}+\gamma^{2}}\right)\left(\sqrt{ \sigma_{*,i}^{2}+\gamma^{2}}-\gamma\right)}\right]-\frac{\gamma}{\sigma_{*,i} }\right)^{2}}.\]

_As a consequence, under vanishing initialization \(\gamma\to 0\) and with a rescaling of time as \(t\to\ln(1/\gamma)t\), the \(i^{th}\) singular value is learned at time \(T_{i}=1/2\sigma_{\star,i}\):_

\[\lim_{\gamma\to 0}\sigma_{\mathbf{\beta},i}(\ln(1/\gamma)t)=\begin{cases}0,\;if\;t<1/2 \sigma_{\star,i}\\ \sigma_{\star,i},\;otherwise.\end{cases}\]

Proof.: From Lemma 3.3 we have that \(\overset{\boldsymbol{\cdot}}{\mathbf{\alpha}}=\mathbf{R}-\alpha\mathbf{R}\mathbf{\alpha} ^{\top}\). In light of Theorem C.11, identity (C.18) and it holds that:

\[\overset{\boldsymbol{\cdot}}{\mathbf{D}}_{\mathbf{\alpha}}=\left(I_{d}-\tilde{ \mathbf{D}}_{\mathbf{\alpha}}^{2}\right)\mathbf{D}_{\mathbf{\beta}^{*}}-2\gamma\mathbf{D }_{\mathbf{\alpha}}.\]For the \(i^{th}\) singular value of \(\mathbf{\alpha}\) it holds that:

\[\overbrace{\sigma_{\mathbf{\alpha},i}(t)}^{\bullet} =-\sigma_{\star,i}\sigma_{\mathbf{\alpha},i}^{2}(t)-2\gamma\sigma_{\mathbf{ \alpha},i}(t)+\sigma_{\star,i}\] \[=-\sigma_{\star,i}\left[\left(\sigma_{\mathbf{\alpha},i}(t)+\frac{ \gamma}{\sigma_{\star,i}}\right)^{2}-\left(1+\frac{\gamma^{2}}{\sigma_{\star,i }^{2}}\right)\right],\] (C.31)

where \(\sigma_{\star,i}\) is the \(i^{th}\) singular value of \(\mathbf{\beta}^{*}\). Equation (C.31) is a Ricatti ODE and is separable. For ease of notation, let \(p\coloneqq\frac{\gamma}{\sigma_{\star,i}}\), \(q\coloneqq 1+\frac{\gamma^{2}}{\sigma_{\star,i}^{2}}\) and \(r\coloneqq-\sigma_{\star,i}\). Then, we need to solve the IVP:

\[\begin{cases}\overbrace{\sigma_{\mathbf{\alpha},i}(t)}^{\bullet}&=r \left[\left(\sigma_{\mathbf{\alpha},i}(t)+p\right)^{2}-q\right]\\ \sigma_{\mathbf{\alpha},i}(0)&=0,\end{cases}\] (C.32)

for which we get \(\sigma_{\mathbf{\alpha},i}(t)=\sqrt{q}\left[\frac{1-\exp\left(2r\sqrt{q}(t+c_{1}) \right)}{1+\exp\left(2r\sqrt{q}(t+c_{1})\right)}\right]-p\) and solving for the initial value gives us \(c_{1}=\frac{1}{2r\sqrt{q}}\log\left(\frac{\sqrt{q}-p}{\sqrt{q}+p}\right)\). Replacing \(p,q,r\) and rearranging we finally obtain:

\[\sigma_{\mathbf{\alpha},i}(t)=\sqrt{1+\frac{\gamma^{2}}{\sigma_{\star,i}^{2}}}\left[\frac{\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\exp\left(-2 t\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{\star,i}^{2}+ \gamma^{2}}-\gamma\right)}{\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}+\exp \left(-2t\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{ \star,i}^{2}+\gamma^{2}}-\gamma\right)}\right]-\frac{\gamma}{\sigma_{\star,i}}.\] (C.33)

In order to obtain the dynamics for the singular values of \(\mathbf{\beta}=\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top}\mathbf{\alpha}\) we recall the relation given in Lemma C.10:

\[\mathbf{\tilde{D}}_{\mathbf{Z}_{1}}^{2}=\gamma I_{d}+\sqrt{\gamma ^{2}I_{d}+\mathbf{\tilde{D}}_{\mathbf{\beta}}^{2}},\]

where the appropriate dimensionality of the diagonal matrices is evident from the indexing of the identity matrices. Therefore, the singular values \(\sigma_{\mathbf{\beta},i}(t)\) are the solutions to the equation

\[\sigma_{\mathbf{\beta},i}(t)=\sigma_{\mathbf{Z}_{1},i}^{2}(t)\sigma_{ \mathbf{\alpha},i}(t)=\sigma_{\mathbf{\alpha},i}(t)\left[\gamma+\sqrt{\gamma^{2}+ \sigma_{\mathbf{\beta},i}^{2}(t)}\right],\]

and have the following expression:

\[\sigma_{\mathbf{\beta},i}(t) =\frac{2\gamma\sigma_{\mathbf{\alpha},i}(t)}{1-\sigma_{\mathbf{\alpha},i }^{2}(t)}\] \[=\frac{2\gamma\left(\sqrt{1+\frac{\gamma^{2}}{\sigma_{\star,i}^{ 2}}}\left[\frac{\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\exp\left(-2t \sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{\star,i}^{2} +\gamma^{2}}-\gamma\right)}{\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}+ \exp\left(-2t\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_ {\star,i}^{2}+\gamma^{2}}-\gamma\right)}\right]-\frac{\gamma}{\sigma_{\star,i} }\right)}{1-\left(\sqrt{1+\frac{\gamma^{2}}{\sigma_{\star,i}^{2}}}\left[\frac {\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\exp\left(-2t\sqrt{\sigma_{ \star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}- \gamma\right)}{\gamma+\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}+\exp\left(-2t\sqrt {\sigma_{\star,i}^{2}+\gamma^{2}}\right)\left(\sqrt{\sigma_{\star,i}^{2}+ \gamma^{2}}-\gamma\right)}\right]-\frac{\gamma}{\sigma_{\star,i}}\right)^{2}}.\]

Note that \(\lim_{t\to\infty}\sigma_{\mathbf{\alpha},i}(t)=\frac{1}{\sigma_{\star,i}}\left( \sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\gamma\right)\) and therefore we can verify that \(\mathbf{\beta}\overset{t\to\infty}{\longrightarrow}\mathbf{\beta}^{*}\) by looking at the singular values, since the singular vectors are static (Lemma C.11).

\[\lim_{t\to\infty}\sigma_{\mathbf{\beta},i}(t) =\frac{2\gamma\left(\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\gamma \right)}{\frac{2\gamma}{\sigma_{\star,i}}\left(\sqrt{\sigma_{\star,i}^{2}+ \gamma^{2}}-\gamma\right)}\] \[=\sigma_{\star,i}.\]

We can now derive asymptotic transition times at which the singular values are learned. Now, we further process expression (C.33). Let \(v\coloneqq\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}-\gamma\), \(w\coloneqq\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}+\gamma\), then \(v+w=2\sqrt{\sigma_{\star,i}^{2}+\gamma^{2}}\) and \(w-v=2\gamma\). We re-write:

\[\sigma_{\boldsymbol{\alpha},i}(t) =\frac{1}{2\sigma_{\star,i}}\left[(w+v)\left[\frac{w-\exp\left(-t (v+w)\right)v}{w+\exp(-t(v+w))v}\right]-(w-v)\right]\] \[=\frac{1}{\sigma_{\star,i}}\left[\frac{-vw\exp\left(-t(v+w)\right) v}{w+\exp(-t(v+w))v}+\frac{vw}{w+\exp(-t(v+w))v}\right]\] \[=\frac{v}{\sigma_{\star,i}}\left[1-\underbrace{\frac{(v+w)\exp \left(-t(v+w)\right)}{w+v\exp(-t(v+w))}}_{=:h(t)}\right].\] (C.34)

Since from before we have that \(\sigma_{\boldsymbol{\beta},i}(t)=\frac{2\gamma\sigma_{\boldsymbol{\alpha},i}(t )}{1-\sigma_{\boldsymbol{\alpha},i}^{2}(t)}\) and we can write \(1-\sigma_{\boldsymbol{\alpha},i}^{2}(t)=\frac{2\gamma v}{\sigma_{\star,i}^{2} }+\frac{v^{2}h(t)\left(2-h(t)\right)}{\sigma_{\star,i}^{2}}\), the singular values of \(\boldsymbol{\beta}\) become:

\[\sigma_{\boldsymbol{\beta},i}(t)=\frac{2\gamma\sigma_{\boldsymbol{\alpha},i}(t )}{1-\sigma_{\boldsymbol{\alpha},i}^{2}(t)} = \frac{\frac{2\gamma v}{\sigma_{\star,i}}\left[1-h(t)\right]}{ \frac{2\gamma v}{\sigma_{\star,i}^{2}}+\frac{v^{2}h(t)\left(2-h(t)\right)}{ \sigma_{\star,i}^{2}}} = \frac{\sigma_{\star,i}\left[1-h(t)\right]}{1+\frac{vh(t)\left(2-h( t)\right)}{2\gamma}}.\]

We wish to study the limit of infinitesimal initialization \(\gamma\to 0\). We introduce a constant \(c>0\) and rewrite \(\gamma=\exp(-c)\) and have \(c=\ln(1/\gamma)\). Rescaling time \(t\to ct\) and taking the limit \(c\to\infty\) we have

\[\lim_{c\to\infty}\sigma_{\boldsymbol{\beta},i}(ct) =\lim_{c\to\infty}\frac{2\sigma_{\star,i}\left[1-h(ct)\right]}{2 +vh(ct)\exp(c)\left(2-h(ct)\right)}\] \[=\frac{\sigma_{\star,i}}{1+\lim_{c\to\infty}vh(ct)\exp(c)}\] \[=\begin{cases}0,\;\text{if}\;t<1/2\sigma_{\star,i},\\ \frac{\sigma_{\star,i}}{1+2\sigma_{\star,i}},\;\text{if}\;t=1/2\sigma_{\star,i }\\ \sigma_{\star,i},\;\text{if}\;t>1/2\sigma_{\star,i},\end{cases}\]

since \(\lim_{c\to\infty}\exp(c)h(ct)=\lim_{c\to\infty}\frac{(v+w)\exp\left(c[1-t(v+w)] \right)}{w+v\exp(-ct(v+w))}\)

\[=\lim_{c\to\infty}\frac{2\sqrt{\sigma_{\star,i}^{2}+\exp(-2c)} \exp\left(c[1-2t\sigma_{\star,i}]\right)\exp\left(\frac{-2ct\exp(-2c)}{\sigma _{\star,i}+\sqrt{\sigma_{\star,i}^{2}+\exp(-2c)}}\right)}{\sqrt{\sigma_{\star,i}^{2}+\exp(-2c)}+\exp(-c)+v\exp(-ct(v+w))}\]

\[=\begin{cases}\infty,\;\text{if}\;t<1/2\sigma_{\star,i}\\ 2,\;\text{if}\;t=1/2\sigma_{\star,i}\\ 0,\;\text{if}\;t>1/2\sigma_{\star,i}.\end{cases}\qed\]

**Lemma C.13** (**Mirror on singular values)**.: _With the same notations as in Theorem 3.1, The singular values of \(\boldsymbol{\beta}\), denoted by \(\mathbf{D}_{\boldsymbol{\beta}}\), follow the mirror flow_

\[\mathbf{d}\nabla\Psi_{\gamma}\left(\mathbf{D}_{\boldsymbol{\beta}}\right)=- \nabla_{\mathbf{D}_{\boldsymbol{\beta}}}\mathcal{L}(\boldsymbol{\beta})\; \mathrm{d}t,\]

_where the potential is \(\Psi_{\gamma}\left(\mathbf{D}_{\boldsymbol{\beta}}\right)\coloneqq\operatorname {tr}\left(\frac{1}{2}\mathbf{D}_{\boldsymbol{\beta}}\sinh^{-1}\left(\mathbf{D} _{\boldsymbol{\beta}}/\gamma\right)-\sqrt{\mathbf{D}_{\boldsymbol{\beta}}^{2} +\gamma^{2}}\right)\)._Proof.: We recall the dynamics induces on \(\boldsymbol{\alpha}=\mathbf{Z}_{1}^{-\top}\mathbf{Z}_{2}\) given in Lemma 3.3:

\[\boldsymbol{\dot{\alpha}}=\mathbf{R}-\boldsymbol{\alpha}\mathbf{R}^{\top} \boldsymbol{\alpha}.\] (C.35)

Writing the SVD decomposition of \(\boldsymbol{\alpha}\) as \(\mathbf{U}_{\boldsymbol{\alpha}}\mathbf{D}_{\boldsymbol{\alpha}}\mathbf{V}_{ \boldsymbol{\alpha}}^{\top}=\mathbf{U}_{\boldsymbol{\mathbb{Z}}_{1}}\mathbf{D}_ {\boldsymbol{\alpha}}\mathbf{V}_{\boldsymbol{\mathbb{Z}}_{2}}^{\top}\) we have that:

\[\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{\alpha}}\mathbf{V}_{ \mathbf{Z}_{2}} =\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z}_{ 1}}\mathbf{D}_{\boldsymbol{\alpha}}\mathbf{V}_{\mathbf{Z}_{2}}^{\top}\mathbf{V }_{\mathbf{Z}_{2}}}^{\top}\mathbf{V}_{\mathbf{Z}_{2}}\] \[=\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z} _{1}}}\mathbf{U}_{\mathbf{Z}_{2}}\mathbf{D}_{\boldsymbol{\alpha}}\mathbf{V}_{ \mathbf{Z}_{2}}^{\top}\mathbf{V}_{\mathbf{Z}_{2}}+\mathbf{U}_{\mathbf{Z}_{1}}^ {\top}\mathbf{U}_{\mathbf{Z}_{1}}\boldsymbol{\dot{\Omega}}_{\boldsymbol{\alpha }}\mathbf{V}_{\mathbf{Z}_{2}}^{\top}\mathbf{V}_{\mathbf{Z}_{2}}+\mathbf{U}_{ \mathbf{Z}_{1}}^{\top}\mathbf{U}_{\mathbf{Z}_{1}}\mathbf{D}_{\boldsymbol{\alpha }}\boldsymbol{\dot{\nabla}}_{\mathbf{Z}_{2}}^{\top}\mathbf{V}_{\mathbf{Z}_{2}}\] \[=\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z} _{1}}}\mathbf{D}_{\boldsymbol{\alpha}}+\boldsymbol{\dot{D}_{\boldsymbol{\alpha }}}\boldsymbol{\dot{\nabla}}_{\mathbf{Z}_{2}}^{\top}\mathbf{V}_{\mathbf{Z}_{2}}.\]

Since \(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z}_{1}}}^{\top} +\boldsymbol{\dot{U}_{\mathbf{Z}_{1}}^{\top}}\mathbf{U}_{\mathbf{Z}_{1}}=0\) and \(\mathbf{V}_{\mathbf{Z}_{2}}^{\top}\boldsymbol{\dot{\nabla}}\mathbf{V}_{ \mathbf{Z}_{2}}+\boldsymbol{\dot{\nabla}}_{\mathbf{Z}_{2}}^{\top}\mathbf{V}_{ \mathbf{Z}_{2}}=0\), the matrices \(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z}_{1}}}^{\top}\) and \(\mathbf{V}_{\mathbf{Z}_{2}}^{\top}\boldsymbol{\dot{\nabla}}_{\mathbf{Z}_{2}}\) are skew-symmetric (have \(0\) diagonal), and therefore the principal diagonals of the products \(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{U}_{\mathbf{Z}_{1}}}^{\top} \mathbf{D}_{\boldsymbol{\alpha}}\) and \(\mathbf{D}_{\boldsymbol{\alpha}}\boldsymbol{\dot{\nabla}}_{\mathbf{Z}_{2}}^{\top} \mathbf{V}_{\mathbf{Z}_{2}}\) are also \(0\).

We define the linear operator \(\mathrm{diag}:\mathbb{R}^{d\times k}\rightarrow\mathbb{R}^{d\times k}\) which maps any \(A\in\mathbb{R}^{d\times k}\) to a matrix of the same dimensions whose principal diagonal contains the elements on the principal diagonal of \(A\), and zeros otherwise. With this notation, we have that \(\mathrm{diag}(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{\alpha}} \mathbf{V}_{\mathbf{Z}_{2}})=\boldsymbol{\dot{\Omega}}_{\boldsymbol{\alpha}}\).

We similarly apply the orthogonal matrices to the left and right of the RHS of C.35, letting \(\mathbf{R}^{\prime}\coloneqq\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\mathbf{R} \mathbf{V}_{\mathbf{Z}_{2}}\in\mathbb{R}^{d\times k}\):

\[\mathrm{diag}\left(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\left( \mathbf{R}-\boldsymbol{\alpha}\mathbf{R}^{\top}\boldsymbol{\alpha}\right) \mathbf{V}_{\mathbf{Z}_{2}}\right) =\mathrm{diag}\left(\mathbf{R}^{\prime}-\mathbf{D}_{\boldsymbol{ \alpha}}\mathbf{R}^{\prime\top}\mathbf{D}_{\boldsymbol{\alpha}}\right)\] \[=\mathrm{diag}\left(\mathbf{R}^{\prime}\right)-\mathrm{diag} \left(\mathbf{D}_{\boldsymbol{\alpha}}\mathbf{R}^{\prime\top}\mathbf{D}_{ \boldsymbol{\alpha}}\right)\] \[=\mathrm{diag}\left(\mathbf{R}^{\prime}\right)-\mathbf{D}_{ \boldsymbol{\alpha}}\mathrm{diag}\left(\mathbf{R}^{\prime}\right)^{\top} \mathbf{D}_{\boldsymbol{\alpha}}\] \[=\left(I_{d}-\boldsymbol{\dot{\Omega}}_{\boldsymbol{\alpha}}^{2} \right)\mathrm{diag}\left(\mathbf{R}^{\prime}\right).\]

Therefore it holds that:

\[\boldsymbol{\dot{\Omega}}_{\boldsymbol{\alpha}}=\left(I_{d}-\boldsymbol{\dot{ \Omega}}_{\boldsymbol{\alpha}}^{2}\right)\mathrm{diag}\left(\mathbf{R}^{\prime}\right)\] (C.36)

We now wish to arrive at an expression in \(\boldsymbol{\beta}\). From the definition of \(\boldsymbol{\alpha}\) it holds that \(\boldsymbol{\alpha}=(\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top})^{-1}\boldsymbol{\beta}\) and, in conjunction with the first part of Theorem C.11 it holds that \(\mathbf{D}_{\boldsymbol{\alpha}}=(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^ {-1}\mathbf{D}_{\boldsymbol{\beta}}\). Therefore, the LHS of (C.36) becomes:

\[\boldsymbol{\dot{\Omega}}_{\boldsymbol{\alpha}}=(\boldsymbol{\dot{\Omega}}_{ \mathbf{Z}_{1}}^{2})^{-1}\mathbf{D}_{\boldsymbol{\beta}}+(\boldsymbol{\dot{ \Omega}}_{\mathbf{Z}_{1}}^{2})^{-1}\boldsymbol{\dot{\Omega}}_{\boldsymbol{ \beta}}.\] (C.37)

For computing \((\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^{-1}\) it is perhaps easiest to proceed as we did with \(\boldsymbol{\alpha}\).

\[\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{ \Omega}}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}} ^{\top}\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}} =-\mathbf{U}_{\mathbf{Z}_{1}}^{\top}(\mathbf{Z}_{1}\mathbf{Z}_{1}^{ \top})^{-1}(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}\mathbf{Z}_{1}^{\top}) (\mathbf{Z}_{1}\mathbf{Z}_{1}^{\top})^{-1}\mathbf{U}_{\mathbf{Z}_{1}}\] \[=-(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^{-1}\left(\mathbf{D }_{\boldsymbol{\beta}}\mathbf{R}^{\prime\top}+\mathbf{R}^{\prime}\mathbf{D}_{ \boldsymbol{\beta}}^{\top}\right)(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^ {-1},\]

and therefore

\[(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^{-1} =\mathrm{diag}\left(\mathbf{U}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{ \dot{\Omega}}_{\mathbf{Z}_{1}}^{\top}\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{ \top}\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}\right)\] \[=-(\boldsymbol{\dot{\Omega}}_{\mathbf{Z}_{1}}^{2})^{-1}\left( \mathbf{D}_{\Finally, for dealing with the RHS of (C.36), we recall equation (C.12) which is implies that \(2\gamma(\tilde{\mathbf{D}}_{\mathbf{Z}_{1}}^{2})^{-1}=I_{d}-\tilde{\mathbf{D}}_{ \boldsymbol{\alpha}}^{2}\). Putting together (C.39) and (C.36), we get that:

\[-2(\tilde{\mathbf{D}}_{\mathbf{Z}_{1}}^{2})^{-2}\tilde{\mathbf{D}}_{ \boldsymbol{\beta}}^{2}\operatorname{diag}\left(\mathbf{R}^{\prime}\right)+( \tilde{\mathbf{D}}_{\mathbf{Z}_{1}}^{2})^{-1}\boldsymbol{\overleftarrow{ \mathbf{D}}_{\boldsymbol{\beta}}}\ =\ 2\gamma(\tilde{\mathbf{D}}_{\mathbf{Z}_{1}}^{2})^{-1} \operatorname{diag}\left(\mathbf{R}^{\prime}\right),\] (C.40)

Therefore, we have the following string of implications:

\[\eqref{eq:C.40} \Longleftrightarrow\frac{1}{2}\left[\gamma I+\tilde{\mathbf{D}}_{ \boldsymbol{\beta}}^{2}(\tilde{\mathbf{D}}_{\mathbf{Z}_{1}}^{2})^{-1}\right]^{ -1}\boldsymbol{\overleftarrow{\mathbf{D}}_{\boldsymbol{\beta}}}\ =\ \operatorname{diag}\left(\mathbf{R}^{\prime}\right)\] \[\Longleftrightarrow\frac{1}{2}(\gamma I_{d}+\sqrt{\gamma^{2}I_{d} +\tilde{\mathbf{D}}_{\boldsymbol{\beta}}^{2}})\left[\gamma^{2}I_{d}+\tilde{ \mathbf{D}}_{\boldsymbol{\beta}}^{2}+\gamma\sqrt{\gamma^{2}I_{d}+\tilde{ \mathbf{D}}_{\boldsymbol{\beta}}^{2}}\right]^{-1}\boldsymbol{\overleftarrow{ \mathbf{D}}_{\boldsymbol{\beta}}}\ =\ \operatorname{diag}\left(\mathbf{R}^{\prime}\right)\] \[\Longleftrightarrow\frac{1}{2}\left[\sqrt{\gamma^{2}I_{d}+\tilde {\mathbf{D}}_{\boldsymbol{\beta}}^{2}}\right]^{-1}\boldsymbol{\overleftarrow{ \mathbf{D}}_{\boldsymbol{\beta}}}\ =\ \operatorname{diag}\left(\mathbf{R}^{\prime}\right)\] \[\Longrightarrow\frac{d\frac{1}{2}\sinh^{-1}(\frac{1}{\gamma} \mathbf{D}_{\boldsymbol{\beta}})}{dt}\ =\ \operatorname{diag}\left(\mathbf{R}^{ \prime}\right).\]

As a final step, we remark that \(\operatorname{diag}\left(\mathbf{R}^{\prime}\right)=-\nabla_{\mathbf{D}_{ \boldsymbol{\beta}}}\mathcal{L}(\boldsymbol{\beta})\) and that \(\nabla_{\mathbf{D}_{\boldsymbol{\beta}}}\operatorname{tr}\left(\frac{1}{2} \mathbf{D}_{\boldsymbol{\beta}}\sinh^{-1}\left(\mathbf{D}_{\boldsymbol{\beta }}/\gamma\right)-\sqrt{\mathbf{D}_{\boldsymbol{\beta}}^{2}+\gamma^{2}}\right) =\sinh^{-1}(\frac{1}{\gamma}\mathbf{D}_{\boldsymbol{\beta}})\). 

### Extensions and further experiments

In this subsection, we describe how the insights from our analysis can be extend to relaxed assumptions on initialization, discrete gradient descent and non-linear activations.

Perturbations from assumption on initialization.Our analysis is contingent upon two assumptions regarding the initialization shape. The first assumption concerns the orthogonality of the initial feature layer \(\boldsymbol{W}_{1}(0)\). In Figure 0(a), we explored a scenario where the feature layer is initialized with a random Gaussian matrix, yet the evolution of singular values closely aligns with our theoretical analysis for orthogonal initialization. In Figure 4, we demonstrate the impact of zero initialization for the weight layer \(\boldsymbol{W}_{2}\). We maintain the same experimental setup as in Figure 0(a), but employ a random Gaussian initialization for \(\boldsymbol{W}_{2}\) with variance scales of \(10^{-2}\) and \(10^{-3}\), while initializing \(\boldsymbol{W}_{1}\) with a variance of \(10^{-3}\). In this context, the evolution of singular values continues to adhere to the predicted trend from our analysis.

Discrete step size.Here we present a simpler problem to show how we can go beyond continuous time analysis. Consider the problem with \(l=d,k=1,\boldsymbol{W}_{1},\boldsymbol{W}_{2}=\boldsymbol{W},\mathbf{a}\) and \(\boldsymbol{W}\) is initialized with

Figure 4: The time evolution of singular values of the hidden layer weights of a 2-layer linear network when trained with gradient flow initialized with Gaussian random variables and non-zero \(\boldsymbol{W}_{2}\).

\(I\). The evolution of \(W,a\) with a learning rate \(\eta\) can be written as

\[\mathbf{W}_{t+1} =\mathbf{W}_{t}+\eta\mathbf{R}_{t}\mathbf{a}_{t}^{\top},\] \[\mathbf{a}_{t+1} =\mathbf{a}_{t}+\eta\mathbf{W}_{t}^{\top}\mathbf{a}_{t},\] \[\mathbf{R}_{t+1} =\mathbf{R}_{t}-\eta\left(X^{\top}X\right)(\mathbf{W}_{t}\mathbf{W}_{t}^{\top} +\|\mathbf{a}_{t}\|^{2})R_{t}-\eta^{2}\left(\mathbf{a}_{t}^{\top}\mathbf{W}_{t}\mathbf{R}_{t} \right)\left(X^{\top}X\right)\mathbf{R}_{t}.\]

If we further assume that \(X^{\top}X=I\), then it can be shown that \(\mathbf{R}_{t},\mathbf{a}_{t}\) only grow in norm and do not change in direction. So the update of \(\mathbf{W}_{t}\) is always aligned with the rank-1 matrix \(\mathbf{R}_{0}\mathbf{a}_{0}^{\top}\). Hence for small initialization, the final \(\mathbf{W}_{\infty}\) is approximately a rank-1 matrix. This presents a way forward for the discrete step-size case and orthogonal data. With further analysis, we think this can be generalized to any data matrix satisfying the RIP conditions. This is not included in the main paper due to restrictive assumptions on the data but we will include these comments in the appendix.

Non-linear activations.We consider the same teacher-student setup as in the Figure 2. To completely characterize the dynamics and the final weight parameters is a challenging problem. The characterization of the dynamics at the small scale of initialization is also absent (for any general data matrix), Boursier et al. (2022) solves this in the case of orthogonal data. To study even this simple case of two neurons, one has to study the dynamics in two phases where one jumps from zero initialization (a saddle point) to another saddle and then further converge to zero train loss as seen in the Figure 4(a). It is difficult to precisely characterize this intermediate saddle. With some careful additional work, we believe that our analysis can capture Phase 1 (where you jump to the first saddle) of the dynamics where you approximately learn rank 1 matrix, see Figure 4(b), for general data matrices extending the current understanding. We hope this briefly sketches a way forward for ReLU networks.

## Appendix D Noise Dynamics

Noise Model.Here we consider the scalar case \((k=1)\), abusing the notation \(\mathbf{W}=\mathbf{W}_{1},\mathbf{a}=\mathbf{W}_{2}\). The gradients of the loss \(\mathcal{L}\left(\mathbf{W},\mathbf{a}\right)\) are

\[\nabla_{\mathbf{W}}\mathcal{L}=-X^{\top}\left(Y-X\mathbf{W}\mathbf{a}\right) \mathbf{a}^{\top},\quad\nabla_{\mathbf{a}}\mathcal{L}=-\mathbf{W}^{\top}X^{\top}\left(Y-X \mathbf{W}\mathbf{a}\right).\]

When the labels or outputs are doped with a noise of magnitude \(\delta>0\), i.e., adding \(\varepsilon\sim\delta\mathrm{N}(0,\mathbf{I}_{n})\). Now the gradients computed after doping with this label noise are

\[\nabla_{\mathbf{W}}\tilde{\mathcal{L}} =-X^{\top}\left(Y+\varepsilon-X\mathbf{W}\mathbf{a}\right)\mathbf{a}^{\top}, \quad\nabla_{\mathbf{a}}\tilde{\mathcal{L}}=-\mathbf{W}^{\top}X^{\top}\left(Y+ \varepsilon-X\mathbf{W}\mathbf{a}\right).\] \[\nabla_{\mathbf{W}}\tilde{\mathcal{L}} =\nabla_{\mathbf{W}}\mathcal{L}-X^{\top}\varepsilon\mathbf{a}^{\top}, \quad\nabla_{\mathbf{a}}\tilde{\mathcal{L}}=\nabla_{\mathbf{a}}\mathcal{L}-\mathbf{W}^{ \top}X^{\top}\varepsilon.\]

Figure 5: (a) The training curve of the teacher-student network which follows a saddle-to-saddle dynamics. (b) The time evolution of singular values of the hidden layer weights of a 2-layer ReLU network when trained with gradient flow. The plot represents Phase 1 of the training where you first learn a (approximately) rank-1 hidden layer.

Thus label noise gradient descent with step size \(\eta\) and with added label noise \(\varepsilon_{t}\) at each iteration writes

\[\mathbf{W}_{t+1}=\mathbf{W}_{t}-\eta\left(\nabla_{\mathbf{W}}\mathcal{L}\left(\mathbf{W}_{t}, \mathbf{a}_{t}\right)-X^{\top}\varepsilon_{t}\mathbf{a}_{t}^{\top}\right).\]

The continuous time version of this SDE writes,

\[\mathrm{d}\mathbf{W}=-\eta\nabla_{\mathbf{W}}\mathcal{L}\left(\mathbf{W},\mathbf{a}\right) \mathrm{d}t+\eta\delta X^{\top}\mathrm{d}\mathbf{B}_{t}\mathbf{a}^{\top}.\]

Now, we consider the large noise regime, where the dominating term in the SDE is the diffusion term. Therefore we consider the SDE,

\[\mathrm{d}\mathbf{W}=\eta\delta X^{\top}\mathrm{d}\mathbf{B}_{t}\mathbf{a}^{\top},\]

where \(\mathbf{B}_{t}\) is a \(n\)-dimensional Brownian motion. Note that we can get rid of the term \(\eta\delta\) by re-scaling time by a constant factor. Similar steps for the evolution of \(\mathbf{a}\) gives the SDE,

\[\mathrm{d}\mathbf{W}=X^{\top}\mathrm{d}\mathbf{B}_{t}\mathbf{a}^{\top},\quad\mathrm{d }\mathbf{a}=\mathbf{W}^{\top}X^{\top}\mathrm{d}\mathbf{B}_{t}.\]

Now consider the compact SVD decomposition of \(X\), i.e., \(X=UDV^{\top}\), where \(U,D\in\mathbb{R}^{n\times n},V\in\mathbb{R}^{d\times n}\) and \(UU^{\top}=U^{\top}U=V^{\top}V=\mathbf{I}_{n}\).

\[\mathrm{d}\mathbf{W}=VDU^{\top}\mathrm{d}\mathbf{B}_{t}\mathbf{a}^{\top}, \quad\mathrm{d}\mathbf{a}=\mathbf{W}^{\top}VDU^{\top}\mathrm{d}\mathbf{B}_{t},\] \[\mathrm{d}V^{\top}\mathbf{W}=D\left(U^{\top}\mathrm{d}\mathbf{B}_{t} \right)\mathbf{a}^{\top},\quad\mathrm{d}\mathbf{a}=\left(V^{\top}\mathbf{W}\right) ^{\top}D\left(U^{\top}\mathrm{d}\mathbf{B}_{t}\right).\]

Using Levy's characterization \(\mathbf{Y}_{t}=U^{\top}\mathbf{B}_{t}\) is a Brownian motion, since \(U\) is an orthogonal matrix. Let \(\tilde{\mathbf{W}}=V^{\top}\mathbf{W}\), then

\[\mathrm{d}\tilde{\mathbf{W}}=D\mathrm{d}\mathbf{Y}_{t}\mathbf{a}^{\top}, \quad\mathrm{d}\mathbf{a}=\tilde{\mathbf{W}}^{\top}D\mathrm{d}\mathbf{Y}_{t}.\]

Here we consider \(D=\mathbf{I}\) and change the notation to get the SDE 4.1 below. Our results can be extended to any diagonal matrix \(D\).

\[\mathrm{d}\mathbf{W}=\left(\mathrm{d}\mathbf{B}_{t}\right)\mathbf{a}^{\top}\quad, \quad\mathrm{d}\mathbf{a}=\mathbf{W}^{\top}\mathrm{d}\mathbf{B}_{t}.\]

**Proposition 4.1**.: _The dynamics (4.1) has the following convergence properties_

1. _Variance explosion._ _The variance of the norms of_ \(\mathbf{W},\mathbf{a}\) _explode, i.e.,_ \[\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{W}(t)\right\|^{2}\right]\to\infty \quad,\quad\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{a}(t)\right\|^{2} \right]\to\infty.\]
2. _Scale divergence._ _For_ \(d\geq 5\)_, for any_ \(\alpha>0\)_, we have that,_ _where_ \(\bar{\mathbf{a}}\coloneqq e^{-t}\int\limits_{0}^{t}e^{s}\mathbf{a}(s)\mathrm{d}s\) _is the exponential moving average of_ \(\mathbf{a}\)_._
3. _Alignment - spectral bias._ _Denote the_ \(i^{\text{th}}\) _row of_ \(\mathbf{W}\) _as_ \(\mathbf{w}_{i}\)_. Using_ \([\mathbf{w}_{i},\mathbf{a}]\stackrel{{\mathrm{def}}}{{=}}\mathbf{w}_{i} \mathbf{a}^{\top}-\mathbf{a}\mathbf{w}_{i}^{\top}\)_,_ \[\lim_{t\to\infty}\mathbb{E}\left[\left\|\mathbf{w}_{i},\mathbf{a}\right\|\right] \to 0.\]

Proof.: Consider the noise model Eq. (4.1),

\[\mathrm{d}\mathbf{W}=\left(\mathrm{d}\mathbf{B}_{t}\right)\mathbf{a}^{\top}\quad,\quad \mathrm{d}\mathbf{a}=\mathbf{W}^{\top}\mathrm{d}\mathbf{B}_{t},\] (D.1)

Variance.Let \(\mathbf{w}_{i}\) be the \(i^{th}\) column of \(\mathbf{W}\) and \(\mathbf{a}_{i}\) be the \(i^{th}\) coordinate of \(\mathbf{a}\). For any \(i\in[n]\), the diffusion of the quantities can be separately written as

\[\mathrm{d}\mathbf{w}_{i}=\mathbf{a}_{i}\mathrm{d}\mathbf{B}_{t},\quad\mathrm{d}\mathbf{a}_{i}= \left\langle\mathbf{w}_{i},\mathrm{d}\mathbf{B}_{t}\right\rangle.\]

Using the Ito chain rule,

\[\mathrm{d}\big{\|}\mathbf{w}_{i}\big{\|}^{2} =2\left\langle\mathbf{w}_{i},\mathrm{d}\mathbf{w}_{i}\right\rangle+ \left\langle\mathrm{d}\mathbf{w}_{i},\mathrm{d}\mathbf{w}_{i}\right\rangle,\] \[=d\big{\|}\mathbf{a}_{i}\big{\|}^{2}\mathrm{d}t+2\mathbf{a}_{i}\left\langle \mathbf{w}_{i},\mathrm{d}\mathbf{B}_{t}\right\rangle.\]Similarly,

\[\mathsf{d}\mathbf{a}_{i}^{2}=2\mathbf{a}_{i}\mathsf{d}\mathbf{a}_{i}+\mathsf{d}\mathbf{a}_{i} \mathsf{d}\mathbf{a}_{i}=2\mathbf{a}_{i}\left\langle\mathbf{w}_{i},\mathsf{d}\mathbf{B}_{t} \right\rangle+\big{\|}\mathbf{w}_{i}\big{\|}^{2}\mathsf{d}t.\]

Note that \(\left\langle\mathbf{w}_{i},\mathsf{d}\mathbf{B}_{t}\right\rangle\sim\big{\|}\mathbf{w}_{i} \big{\|}\mathsf{d}\tilde{\mathbf{B}}_{t}\) for some one-dimensional Brownian motion \((\mathbf{\tilde{B}}_{t})_{t\geq 0}\).

\[\mathsf{d}\mathbf{a}_{i}^{2}=\big{\|}\mathbf{w}_{i}\big{\|}^{2}\mathsf{d}t+2\mathbf{a}_{i} \big{\|}\mathbf{w}_{i}\big{\|}\mathsf{d}\tilde{\mathbf{B}}_{t},\quad\mathsf{d}\big{\|} \mathbf{w}_{i}\big{\|}^{2}=d\big{\|}\mathbf{a}_{i}\big{\|}^{2}\mathsf{d}t+2\mathbf{a}_{i} \big{\|}\mathbf{w}_{i}\big{\|}\mathsf{d}\mathbf{\tilde{B}}_{t}.\]

Let \(\mathbf{v}\coloneqq\mathbf{a}_{i},\mathbf{u}\coloneqq\big{\|}\mathbf{w}_{i}\big{\|}\), using this notation we get,

\[\mathsf{d}\mathbf{u}^{2}=d\mathbf{v}^{2}\mathsf{d}t+2\mathbf{u}\mathbf{v}\mathsf{d}\mathbf{\tilde{ B}}_{t},\quad\mathsf{d}\mathbf{v}^{2}=\mathbf{u}^{2}\mathsf{d}t+2\mathbf{u}\mathbf{v}\mathsf{d} \mathbf{\tilde{B}}_{t}.\] (D.2)

Let \(\mathbf{u}_{0}\coloneqq\mathbb{E}\left[\mathbf{u}^{2}\right],\mathbf{v}_{0}\coloneqq \mathbb{E}\left[\mathbf{v}^{2}\right]\). Using the Dynkins formula, taking the expectation, we get,

\[\mathsf{d}\mathbb{E}\left[\mathbf{u}^{2}\right] =d\mathbb{E}\left[\mathbf{v}^{2}\right]\mathsf{d}t,\quad\mathsf{d} \mathbb{E}\left[\mathbf{v}^{2}\right]=\mathbb{E}\left[\mathbf{u}^{2}\right],\] \[\mathsf{d}\mathbf{u}_{0} =d\mathbf{v}_{0}\mathsf{d}t,\quad\mathsf{d}\mathbf{v}_{0}=\mathbf{u}_{0} \mathsf{d}t.\]

This system can be transformed into,

\[\mathsf{d}\big{(}\mathbf{u}_{0}+\sqrt{d}\mathbf{v}_{0}\big{)}=\sqrt{d}\left(\mathbf{u}_{0} +\sqrt{n}\mathbf{v}_{0}\right)\mathsf{d}t.\]

Solving the above ODE we get,

\[\left(\mathbf{u}_{0}+\sqrt{d}\mathbf{v}_{0}\right)=c_{0}e^{\sqrt{d}t},\quad\text{where }c_{0}=\mathbf{u}_{0}(0)+\sqrt{d}\mathbf{v}_{0}(0).\]

Similarly,

\[\mathsf{d}\big{(}\mathbf{u}_{0}-\sqrt{d}\mathbf{v}_{0}\big{)} =-\sqrt{d}\left(\mathbf{u}_{0}-\sqrt{d}\mathbf{v}_{0}\right)\mathsf{d}t,\] \[\left(\mathbf{u}_{0}-\sqrt{d}\mathbf{v}_{0}\right) =c_{1}e^{-\sqrt{d}t},\quad\text{where }c_{1}=\mathbf{u}_{0}(0)-\sqrt{d}\mathbf{v}_{0}(0).\]

\[\mathbf{u}_{0}=\frac{1}{2}\left[c_{0}e^{\sqrt{d}t}+c_{1}e^{-\sqrt{d}t}\right], \quad\mathbf{v}_{0}=\frac{1}{2\sqrt{d}}\left[c_{0}e^{\sqrt{d}t}-c_{1}e^{-\sqrt{d}t }\right].\]

Taking the limit proves the first part of the result.

Scale.From Eq. D.2, we have,

\[\mathsf{d}\big{(}\mathbf{u}^{2}+\sqrt{n}\mathbf{v}^{2}\big{)}=\sqrt{n}(\mathbf{u}^{2}+ \sqrt{d}\mathbf{v}^{2})+2\left(\sqrt{d}+1\right)\mathbf{u}\mathbf{v}\mathsf{d}\mathbf{\tilde{B }}_{t}.\]

Again using the Ito chain rule, we get,

\[\mathsf{d}\big{(}\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\big{)}^{\mathbf{\alpha}} =\mathbf{\alpha}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}-1}\mathsf{d}\big{(}\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\big{)}\] \[\qquad\qquad+\frac{1}{2}\mathbf{\alpha}(\mathbf{\alpha}-1)\left(\mathbf{u}^{2 }+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{\alpha}-2}\mathsf{d}\big{(}\mathbf{u}^{2}+\sqrt{n }\mathbf{v}^{2}\big{)}\mathsf{d}\big{(}\mathbf{u}^{2}+\sqrt{n}\mathbf{v}^{2}\big{)},\] \[=\mathbf{\alpha}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}-1}\left[\sqrt{d}(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2})\mathsf{d}t+2\left( \sqrt{d}+1\right)\mathbf{u}\mathbf{v}\mathsf{d}\mathbf{\tilde{B}}_{t}\right]\] \[\qquad\qquad\qquad+\frac{1}{2}\mathbf{\alpha}(\mathbf{\alpha}-1)\left( \mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{\alpha}-2}\left(4\left(\sqrt{d}+1 \right)^{2}\mathbf{u}^{2}\mathbf{v}^{2}\right)\mathsf{d}t,\]

The drift term is

\[\mathbf{\alpha}\sqrt{n}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}} +2\mathbf{\alpha}(\mathbf{\alpha}-1)\left(\sqrt{d}+1\right)^{2}\left(\mathbf{u}^{2}+ \sqrt{n}\mathbf{v}^{2}\right)^{\mathbf{\alpha}-2}\mathbf{u}^{2}\mathbf{v}^{2}\] \[=\mathbf{\alpha}\left(\mathbf{u}^{2}+\sqrt{n}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}}\left[\sqrt{d}+2(\mathbf{\alpha}-1)\left(\sqrt{d}+1\right)^{2}\frac{\mathbf{u}^{ 2}\mathbf{v}^{2}}{\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{2}}\right].\]

Again using the Dynkins formula for the evolution of expectation, we have,

\[\mathsf{d}\mathbb{E}\left[\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}}\right]=\mathbb{E}\left[\mathbf{\alpha}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2} \right)^{\mathbf{\alpha}}\left[\sqrt{d}+2(\mathbf{\alpha}-1)\left(\sqrt{d}+1\right)^{2} \frac{\mathbf{u}^{2}\mathbf{v}^{2}}{\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{2}} \right]\right]\mathsf{d}t.\] (D.3)For any function,

\[g(y)=\frac{y}{\left(y+\sqrt{d}\right)^{2}},\]

attains its maximum value at \(y=\sqrt{d}\), i.e., \(g(\sqrt{n})=1/(4\sqrt{d})\). Note that \(\mathbf{\alpha}<1\) and we have

\[2(\mathbf{\alpha}-1)\left(\sqrt{n}+1\right)^{2}\frac{\mathbf{u}^{2}\mathbf{v}^{2}}{\left( \mathbf{u}^{2}+\sqrt{n}\mathbf{v}^{2}\right)^{2}}\geq 2(\mathbf{\alpha}-1)\left(\sqrt{n}+1 \right)^{2}g(\sqrt{n})=(\mathbf{\alpha}-1)\frac{\left(\sqrt{n}+1\right)^{2}}{2\sqrt {n}}.\]

The drift can be lower bounded as the following,

\[\mathbf{\alpha}\sqrt{n}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{ \mathbf{\alpha}} +2\mathbf{\alpha}(\mathbf{\alpha}-1)\left(\sqrt{d}+1\right)^{2}\left(\mathbf{u }^{2}+\sqrt{n}\mathbf{v}^{2}\right)^{\mathbf{\alpha}-2}\mathbf{u}^{2}\mathbf{v}^{2}\] \[\geq\mathbf{\alpha}\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{ \alpha}}\left[\sqrt{d}+(\mathbf{\alpha}-1)\frac{\left(\sqrt{d}+1\right)^{2}}{2 \sqrt{d}}\right].\]

For \(d\geq 5\) and \(0<\mathbf{\alpha}<1\), we have \(c_{0}>0\),

\[\left[\sqrt{d}+(\mathbf{\alpha}-1)\frac{\left(\sqrt{d}+1\right)^{2}}{2\sqrt{d}} \right]>c_{0}.\]

Using the above expression in Eq. D.3,

\[\text{d}\mathbb{E}\left[\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2}\right)^{\mathbf{\alpha }}\right]\geq\mathbf{\alpha}c_{0}\mathbb{E}\left[\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{ 2}\right)^{\mathbf{\alpha}}\right]\text{d}t.\]

Taking the limit,

\[\lim_{t\rightarrow\infty}\mathbb{E}\left[\left(\mathbf{u}^{2}+\sqrt{d}\mathbf{v}^{2} \right)^{\mathbf{\alpha}}\right]\rightarrow\infty.\]

From the SDE (D.2), we obtain the following process with only diffusion,

\[\text{d}\left(\mathbf{u}^{2}-\mathbf{v}^{2}\right) =(d\mathbf{v}^{2}-\mathbf{u}^{2})\text{d}t,\] \[\text{d}\left(\mathbf{u}^{2}-\mathbf{v}^{2}\right)+\left(\mathbf{u}^{2}-\mathbf{ v}^{2}\right)\text{d}t =(n-1)\mathbf{v}^{2},\] \[e^{t}\text{d}\left(\mathbf{u}^{2}-\mathbf{v}^{2}\right)+e^{t}\left(\mathbf{u }^{2}-\mathbf{v}^{2}\right)\text{d}t =(n-1)e^{t}\mathbf{v}^{2}\text{d}t,\] \[\text{d}e^{t}\left(\mathbf{u}^{2}-\mathbf{v}^{2}\right) =(n-1)e^{t}\mathbf{v}^{2}\text{d}t,\] \[\left(\mathbf{u}^{2}(t_{2})-\mathbf{v}^{2}(t_{2})\right) =e^{-(t_{2}-t_{1})}\left(\mathbf{u}^{2}(t_{1})-\mathbf{v}^{2}(t_{1}) \right)+(n-1)\int\limits_{t_{1}}^{t_{2}}e^{-(t_{2}-t)}\mathbf{v}^{2}(t)\text{d}t,\] \[\left(\mathbf{u}^{2}(t_{2})+\sqrt{n}\mathbf{v}^{2}(t_{2})\right) =e^{-(t_{2}-t_{1})}\left(\mathbf{u}^{2}(t_{1})-\mathbf{v}^{2}(t_{1}) \right)+\sqrt{n}\mathbf{v}^{2}(t_{2})+(n-1)e^{-t_{2}}\int\limits_{t_{1}}^{t_{2}}e ^{t}\mathbf{v}^{2}(t)\text{d}t,\]

Denote \(C(t)\coloneqq\mathbf{u}^{2}(t)+\sqrt{d}\mathbf{v}^{2}(t)\), using the fact that \(\mathbf{u}^{2}(t_{1})-\mathbf{v}^{2}(t_{1})\leq C(t_{1})\),

\[C\left(t_{2}\right)\leq e^{-(t_{2}-t_{1})}C\left(t_{1}\right)+\sqrt{d}\mathbf{v}^{ 2}(t_{2})+(d-1)e^{-t_{2}}\int\limits_{t_{1}}^{t_{2}}e^{t}\mathbf{v}^{2}(t),\]

With \(t_{1}=0\), using the notation \(\tilde{\mathbf{v}}^{2}(t)\coloneqq e^{-t}\int\limits_{0}^{t}e^{s}\mathbf{v}^{2}(s) \text{d}s\), to denote the exponential moving average.

For any time \(t>0\), we have,

\[C\left(t\right)\leq e^{-(t)}C\left(0\right)+d\left(\mathbf{v}^{2}(t)+\tilde{\mathbf{v} }^{2}(t)\right).\]

Raising to the power of \(\alpha\),

\[C\left(t\right)^{\alpha}\leq\left[e^{-t}C\left(0\right)+d\left(\mathbf{v}^{2}(t)+ \tilde{\mathbf{v}}^{2}(t)\right)\right]^{\alpha}.\]For \(a,b>0\) and \(0<p<1\), we have \(\left(a+b\right)^{p}\leq a^{p}+b^{p}\). Using the inequality,

\[C\left(t\right)^{\alpha}\leq e^{-\alpha t}C\left(0\right)^{\alpha}+d^{\alpha} \left(\mathbf{v}^{2}(t)\right)^{\alpha}+d^{\alpha}\left(\tilde{\mathbf{v}}^{2}(t) \right)^{\alpha}.\]

Taking the expectation,

\[\mathbb{E}\left[C\left(t\right)^{\alpha}\right]\leq e^{-\alpha t}\mathbb{E} \left[C\left(0\right)^{\alpha}\right]+d^{\alpha}\mathbb{E}\left[\left(\mathbf{v}^{2 }(t)\right)^{\alpha}\right]+d^{\alpha}\mathbb{E}\left[\left(\tilde{\mathbf{v}}^{2} (t)\right)^{\alpha}\right].\]

Now, we proceed by taking the limit,

\[\lim_{t\rightarrow\infty}\mathbb{E}\left[C\left(t\right)^{\alpha}\right]\leq e^ {-\alpha t}\lim_{t\rightarrow\infty}\mathbb{E}\left[C\left(0\right)^{\alpha} \right]+d^{\alpha}\lim_{t\rightarrow\infty}\left[\mathbb{E}\left[\left(\mathbf{v} ^{2}(t)\right)^{\alpha}\right]+\mathbb{E}\left[\left(\tilde{\mathbf{v}}^{2}(t) \right)^{\alpha}\right]\right].\]

Thus, we obtain,

\[\lim_{t\rightarrow\infty}\left(\mathbb{E}\left[\left(\mathbf{v}^{2}(t)\right)^{ \alpha}\right]+\mathbb{E}\left[\left(\tilde{\mathbf{v}}^{2}(t)\right)^{\alpha} \right]\right)\rightarrow\infty,\]

where \(\tilde{\mathbf{v}}^{2}(t)\coloneqq e^{-t}\int\limits_{0}^{t}e^{s}\mathbf{v}^{2}(s) \mathrm{d}s\) is the exponential moving average.

A similar computation for \(\mathbf{u}\) will yield,

\[\lim_{t\rightarrow\infty}\ \mathbb{E}\left[\left(\mathbf{u}^{2}(t)\right)^{\alpha} \right]\rightarrow\infty.\]

Therefore for the limit of \(\mathbf{a}_{i},\left\|\mathbf{w}_{i}\right\|\), we have,

\[\lim_{t\rightarrow\infty}\ \mathbb{E}\left[\left\|\mathbf{w}_{i}(t)\right\|^{2 \alpha}\right]\rightarrow\infty,\quad\lim_{t\rightarrow\infty}\left(\mathbb{E }\left[\left(\mathbf{a}_{i}^{2}(t)\right)^{\alpha}\right]+\mathbb{E}\left[\left( \tilde{\mathbf{a}_{i}}^{2}(t)\right)^{\alpha}\right]\right)\rightarrow\infty.\]

Now we proceed to combine the above results and obtain the result on \(\mathbf{a}\), \(\mathbf{W}\). Note that for \(0<\alpha<1\), \(x^{\alpha}\) is concave. Further using the Jensen's inequality, we have,

\[\left(\left\|\mathbf{W}\right\|^{2}\right)^{\alpha}=\left(\sum\limits_{i=1}^{d} \left\|\mathbf{w}_{i}\right\|^{2}\right)^{\alpha}\geq d^{1-\alpha}\sum\limits_{i=1 }^{d}\left\|\mathbf{w}_{i}(t)\right\|^{2\alpha}.\]

Similar expression for \(\mathbf{a}\) and taking the limit, we obtain, the result

\[\lim_{t\rightarrow\infty}\mathbb{E}\left[\left\|\mathbf{W}(t)\right\|^{\alpha} \right]\rightarrow\infty\quad,\quad\lim_{t\rightarrow\infty}\mathbb{E}\left[ \left\|\mathbf{a}(t)\right\|^{\alpha}+\left\|\tilde{\mathbf{a}}(t)\right\|^{ \alpha}\right]\rightarrow\infty.\]

where \(\tilde{\mathbf{a}}\coloneqq e^{-t}\int\limits_{0}^{t}e^{s}\mathbf{a}(s) \mathrm{d}s\) is the exponential moving average of \(\mathbf{a}\).

Alignment.Let \(\mathbf{z}_{1},\mathbf{z}_{2}\ldots\mathbf{z}_{d}\) be the rows of the matrix \(\mathbf{W}\). Now the evolution of the rows can be written as

\[\mathbf{z}_{i}^{\star} =\mathbf{a}(\mathrm{d}\mathbf{B}_{t}^{i}),\] \[\mathrm{d}\mathbf{a} =\sum\limits_{j=1}^{d}\mathbf{z}_{j}\mathrm{d}\mathbf{B}_{t}^{j}.\]

For any two matrices, with same dimensions define \([u,v]\stackrel{{\mathrm{def}}}{{=}}uv^{\top}-vu^{\top}\).

\[\mathrm{d}[\mathbf{z}_{i},\mathbf{a}]=\mathrm{d}\big{(}\mathbf{z}_{i}\mathbf{a}^{\top }\big{)}-\mathrm{d}\big{(}\mathbf{a}\mathbf{z}_{i}^{\top}\big{)},\]Using the Ito chain rule,

\[\mathrm{d}\!\left(\mathbf{z}_{i}\mathbf{\mathrm{a}}^{\top}\right) =\mathrm{d}\mathbf{z}_{i}\mathbf{\mathrm{a}}^{\top}+\mathbf{z}_{i}\mathrm{d}\bm {b}^{\top}+\mathrm{d}\mathbf{z}_{i}\mathrm{d}\mathbf{\mathrm{a}},\] \[=\mathbf{\mathrm{a}}\mathbf{\mathrm{a}}^{\top}(\mathrm{d}\mathbf{B}_{t}^{i})+ \mathbf{z}_{i}\left(\sum_{j=1}^{d}\mathbf{z}_{j}^{\top}\mathrm{d}\mathbf{B}_{t}^{j}\right)+ \left(\mathbf{\mathrm{a}}(\mathrm{d}\mathbf{B}_{t}^{i})\right)\left(\sum_{j=1}^{d}\mathbf{ z}_{j}^{\top}\mathrm{d}\mathbf{B}_{t}^{j}\right),\] \[=\mathbf{\mathrm{a}}\mathbf{\mathrm{a}}^{\top}(\mathrm{d}\mathbf{B}_{t}^{i})+ \sum_{j=1}^{d}\mathbf{z}_{i}\mathbf{z}_{j}^{\top}\mathrm{d}\mathbf{B}_{t}^{j}+\mathbf{\mathrm{ a}}\mathbf{z}_{i}^{\top}\mathrm{d}t,\] \[\mathrm{d}\!\left(\mathbf{\mathrm{a}}\mathbf{z}_{i}^{\top}\right) =\mathbf{\mathrm{a}}\mathbf{\mathrm{a}}^{\top}\left(\mathrm{d}\mathbf{B}_{t}^ {i}\right)+\mathbf{z}_{i}\mathbf{\mathrm{a}}^{\top}\mathrm{d}t+\sum_{j=1}^{d}\mathbf{z}_{ j}\mathbf{z}_{i}^{\top}\mathrm{d}\mathbf{B}_{t}^{j},\] \[\mathrm{d}\!\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right] =-\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right]\mathrm{d}t+\sum_{i=1}^{ d}\left[\mathbf{z}_{i},\mathbf{z}_{j}\right]\mathrm{d}\mathbf{B}_{t}^{j},\]

From the above evolution, we have that,

\[\mathrm{d}\!\mathbb{E}\left[\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right]\right]=- \mathbb{E}\left[\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right]\right]\mathrm{d}t.\]

Hence, we have,

\[\mathbb{E}\left[\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right]\right]=\left[\mathbf{z}_{i }(0),\mathbf{\mathrm{a}}(0)\right]e^{-t}.\]

Let \(e_{i}\stackrel{{\mathrm{def}}}{{=}}\left[\mathbf{z}_{i},\mathbf{\mathrm{ a}}\right]\left[k,l\right]\), be any \((kl)^{th}\) entry of the matrix. Similarly, let \(c_{ij}\stackrel{{\mathrm{def}}}{{=}}\left[\mathbf{z}_{i},\mathbf{z}_{j} \right]\left[k,l\right]\).

\[\mathrm{d}e_{i}=-e_{i}\mathrm{d}t+\sum_{j}c_{ij}\mathrm{d}\mathbf{B}_{t}^{j},\] \[\mathrm{d}e_{i}^{2}=-2e_{i}\left(e_{i}\mathrm{d}t+\sum_{j}c_{ij} \mathrm{d}\mathbf{B}_{t}^{j}\right)+\sum_{j}c_{ij}^{2}\mathrm{d}t,\] \[\mathrm{d}e_{i}^{2}=-2e_{i}^{2}\mathrm{d}t+\sum_{j}c_{ij}^{2} \mathrm{d}t-2e_{i}\sum_{j}c_{ij}\mathrm{d}\mathbf{B}_{t}^{j}.\]

Again using the Ito formula and computing \((e_{i}^{2})^{\mathbf{\alpha}}\) for some \(\mathbf{\alpha}\in(0,1)\), we get,

\[\mathrm{d}\!\left(e_{i}^{2}\right)^{\mathbf{\alpha}}=\mathbf{\alpha}\left(e_{i}^{2} \right)^{\mathbf{\alpha}-1}\mathrm{d}e_{i}^{2}+\frac{1}{2}\mathbf{\alpha}(\mathbf{\alpha}- 1)\left(e_{i}^{2}\right)^{\mathbf{\alpha}-2}\mathrm{d}e_{i}^{2}\mathrm{d}e_{i}^{2},\]

\[\mathrm{d}\!\left(e_{i}^{2}\right)^{\mathbf{\alpha}}=\mathbf{\alpha}\left(e_{i}^{2} \right)^{\mathbf{\alpha}-1}\left[-2e_{i}^{2}\mathrm{d}t+\sum_{j}c_{ij}^{2}\mathrm{ d}t-2e_{i}\sum_{j}c_{ij}\mathrm{d}\mathbf{B}_{t}^{j}\right]+\frac{1}{2}\mathbf{\alpha}(\mathbf{ \alpha}-1)\left(e_{i}^{2}\right)^{\mathbf{\alpha}-2}4e_{i}^{2}\left[\sum_{j}c_{ij }^{2}\right],\]

Taking \(\mathbf{\alpha}=0.5\),

\[\mathrm{d}|e_{i}|=-|e_{i}|\mathrm{d}t-|e_{i}|^{-1}e_{i}\sum_{j}c_{ij}\mathrm{ d}\mathbf{B}_{t}^{j}.\]

Taking expectation, we get,

\[\mathrm{d}\!\mathbb{E}\left[|e_{i}|\right]=-\mathbb{E}\left[|e_{i}|\right] \mathrm{d}t.\]

Hence,

\[\mathbb{E}\left[|\!\left[\mathbf{z}_{i},\mathbf{\mathrm{a}}\right]|\right]=|\!\left[ \mathbf{z}_{i}(0),\mathbf{\mathrm{a}}(0)\right]|\!|e^{-t}.\]