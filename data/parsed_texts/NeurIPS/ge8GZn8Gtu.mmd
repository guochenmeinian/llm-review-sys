# Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures

 Xin Chen

Princeton University

xc5557@princeton.edu

&Anderson Ye Zhang

University of Pennsylvania

ayz@wharton.upenn.edu

###### Abstract

We study clustering under anisotropic Gaussian Mixture Models (GMMs), where covariance matrices from different clusters are unknown and are not necessarily the identity matrix. We analyze two anisotropic scenarios: homogeneous, with identical covariance matrices, and heterogeneous, with distinct matrices per cluster. For these models, we derive minimax lower bounds that illustrate the critical influence of covariance structures on clustering accuracy. To solve the clustering problem, we consider a variant of Lloyd's algorithm, adapted to estimate and utilize covariance information iteratively. We prove that the adjusted algorithm not only achieves the minimax optimality but also converges within a logarithmic number of iterations, thus bridging the gap between theoretical guarantees and practical efficiency.

## 1 Introduction

Clustering is a fundamentally important task in statistics and machine learning [7, 2]. The most widely recognized and extensively studied model for clustering is the Gaussian Mixture Model (GMM) [17, 19], which is formulated as

\[Y_{j}=\theta_{z_{j}^{*}}^{*}+\epsilon_{j},\;\text{where}\;\epsilon_{j}\; \overset{ind}{\sim}\;\mathcal{N}(0,\Sigma_{z_{j}^{*}}^{*}),\;\forall j\in[n].\]

Here \(Y=(Y_{1},\dots,Y_{n})\) are the observations with \(n\) being the sample size. We define the set \([n]=\{1,2,\dots,n\}\). Assume \(k\) is the known number of clusters. Let \(\{\theta_{a}^{*}\}_{a\in[k]}\) represent the unknown centers, and \(\Sigma_{a}^{*}\) denote the corresponding unknown covariance matrices. Define \(z^{*}\in[k]^{n}\) as the cluster assignment vector, where for each index \(j\in[n]\), the value of \(z_{j}^{*}\) specifies which cluster the \(j\)-th data point is assigned to. The goal is to recover \(z^{*}\) from \(Y\). For any estimator \(\hat{z}\), its clustering performance is measured by the misclustering error rate \(h(\hat{z},z^{*})\), which will be introduced later in (4).

There has been increasing interest in theoretical and algorithmic analysis of clustering under GMMs. In a scenario where a GMM is isotropic, meaning that all covariance matrices \(\{\Sigma_{a}^{*}\}_{a\in[k]}\) are equal to the identity matrix, [15] obtained the minimax rate for clustering, which takes the form of \(\exp(-(1+o(1))(\min_{a\neq b}\|\theta_{a}^{*}-\theta_{b}^{*}\|)^{2}/8)\), with respect to the misclustering error rate. A diverse range of methods has been explored in the context of the isotropic setting. Among these, Lloyd's algorithm [13] stands out as a particularly effective clustering algorithm, renowned for its extensive success in a myriad of disciplines. [15, 8] establish computational and statistical guarantees for the Lloyd's algorithm. Specifically, they showed it achieves the minimax optimal rates after a few iterations provided with some decent initialization. Another popular approach to clustering especially for high dimensional data is the spectral clustering [21, 18, 20], which is an umbrella term for clustering after a dimension reduction through a spectral decomposition. [14] proves the spectral clustering also achieves the optimality under the isotropic GMM. Semidefinite programming (SDP)is also used for clustering by exploiting its low-rank structure, and its statistical properties have been studied in literature, for example, [5].

Despite the numerous compelling findings, most existing research primarily focuses on isotropic GMMs. The understanding of clustering in an anisotropic context, where the covariance matrices are not constrained to be identity matrices, remains relatively limited. Some studies, including [15, 5, 16, 1, 9, 24], present results for sub-Gaussian mixture models, wherein the errors \(\epsilon_{j}\) are assumed to follow some sub-Gaussian distributions with the variance proxy \(\sigma^{2}\). At first glance, it might appear that these results encompass the anisotropic case, as distributions of the form \(\{\mathcal{N}(0,\Sigma_{a}^{*})\}_{a\in[k]}\) are indeed sub-Gaussian distributions. However, from a minimax perspective, the least favorable scenario among all sub-Gaussian distributions with variance proxy \(\sigma^{2}\)--and thus the most challenging for clustering--is when the errors are distributed as \(\mathcal{N}(0,\sigma^{2}I)\). Therefore, the minimax rate for clustering under the sub-Gaussian mixture model essentially equals the one under the isotropic GMM, and methods like Lloyd's algorithm, which require no covariance matrix information, can be rate-optimal. As a result, the aforementioned findings primarily pertain to isotropic GMMs.

A few studies have explored the direction of clustering under anisotropic GMMs. [3] presents a polynomial-time clustering algorithm that provably performs well when Gaussian distributions are well-separated by hyperplanes. This idea is further developed in [11], which extends the approach to allow overlapping Gaussians, albeit only in two-cluster scenarios. [22] proposes a novel method for clustering under a balanced mixture of two elliptical distributions. They establish a provable upper bound on their clustering performance. Nevertheless, the fundamental limit of clustering under anisotropic GMMs, and whether a polynomial-time procedure can achieve it, remains unknown.

In this paper, we investigate the clustering task under two anisotropic GMMs. In Model 1, all covariance matrices are equal (i.e., homogeneous) to some unknown matrix \(\Sigma^{*}\). Model 2 offers more flexibility, with covariance matrices that are unknown and not necessarily identical (i.e., heterogeneous). The contribution of this paper is two-fold, summarized as follows:

* Our first contribution is on the minimax rates. We obtain minimax lower bounds for clustering under anisotropic GMMs with respect to the misclustering error rate. We show they take the form of \[\inf_{\hat{z}}\sup_{z^{*}}\mathbb{E}h(\hat{z},z^{*})\geq\exp\biggl{(}-(1+o(1)) \frac{\text{(signal-to-noise ratio)}^{2}}{8}\biggr{)}\,,\] where the signal-to-noise ratio under Model 1 is equal to \(\min_{a,b\in[k]:a\neq b}\|(\theta_{a}^{*}-\theta_{b}^{*})^{T}\Sigma^{*-\frac{1 }{2}}\|\). The signal-to-noise ratio for Model 2 is more intricate and will be introduced in Section 3. For both models, we can see the minimax rates depend not only on the centers but also on the covariance matrices. This is different from the isotropic case, whose signal-to-noise ratio is \(\min_{a\neq b}\|\theta_{a}^{*}-\theta_{b}^{*}\|\). Our results precisely capture the role that covariance matrices play in the clustering problem. This shows that covariance matrices impact the fundamental limits of the clustering problem through complex interactions with the centers, especially in Model 2. We obtain the minimax lower bounds by drawing connections with Linear Discriminant Analysis (LDA) [6] and Quadratic Discriminant Analysis (QDA).
* Our second and more important contribution is on the computational side. We give a computationally feasible procedure and rate-optimal algorithm for the anisotropic GMM. Lloyd's algorithm, developed for the isotropic case, is no longer optimal as it only considers distances among centers [3]. We study an adjusted Lloyd's algorithm which estimates the covariance matrices in each iteration and adjusts the clusters accordingly. It can also be seen as a hard EM algorithm [4]. Here, we modify the E-step of the soft EM by implementing a maximization step that directly assigns data points to clusters, rather than calculating probabilities. As an iterative algorithm, we demonstrate that it achieves the minimax lower bound within \(\log n\) iterations. This offers both statistical and computational guarantees, serving as valuable guidance for practitioners. Specifically, if we let \(z^{(t)}\) denote the output of the algorithm after \(t\) iterations, it holds with high probability that \[h(z^{(t)},z^{*})\leq\exp\biggl{(}-(1+o(1))\frac{\text{(signal-to-noise ratio)}^{2}}{8}\biggr{)}\,,\] for all \(t\geq\log n\). The algorithm can be initialized using popular methods like spectral clustering or Lloyd's algorithm. In our numerical studies, we demonstrate that our algorithm significantly improves over the two aforementioned methods under anisotropic GMMs, and matches the optimal exponent specified in the minimax lower bound.

Paper Organization.The remaining paper is organized as follows. In Section 2, we study Model 1 where the covariance matrices are unknown but homogeneous. In Section 3, we consider Model 2 where covariance matrices are unknown and heterogeneous. For both cases, we establish the minimax lower bound for the clustering and give a computationally feasible and rate-optimal procedure. In Section 4, we provide a numerical comparison with other popular methods. Proofs are included in the supplement.

Notation.For any matrix \(X\in\mathbb{R}^{d\times d}\), we denote \(\lambda_{1}(X)\) as its smallest eigenvalue and \(\lambda_{d}(X)\) as its largest eigenvalue. In addition, we denote \(\|X\|\) as its operator norm. For any two vectors \(u,v\) of the same dimension, we denote \(\langle u,v\rangle=u^{T}v\) as their inner product. For any positive integer \(d\), we denote \(I_{d}\) as the \(d\times d\) identity matrix. We denote \(\mathcal{N}(\mu,\Sigma)\) as the normal distribution with mean \(\mu\) and covariance matrix \(\Sigma\). We denote \(\mathbb{I}\left\{\cdot\right\}\) as the indicator function. For two positive sequences \(\{a_{n}\}\) and \(\{b_{n}\}\), \(a_{n}\preceq b_{n}\) and \(a_{n}=O(b_{n})\) both mean \(a_{n}\leq Cb_{n}\) for some constant \(C>0\) independent of \(n\). We also write \(a_{n}=o(b_{n})\) or \(\frac{b_{n}}{a_{n}}\to\infty\) when \(\limsup_{n}\frac{a_{n}}{b_{n}}=0\).

## 2 GMM with Unknown but Homogeneous Covariance Matrices

### Model

We first consider the GMM where the covariance matrices of different clusters are unknown but are assumed to be equal to each other. Then the data-generating process can be displayed as follows:

**Model 1:** \[Y_{j}=\theta_{z_{j}^{*}}^{*}+\epsilon_{j},\text{ where }\epsilon_{j} \overset{ind}{\sim}\mathcal{N}(0,\Sigma^{*}),\forall j\in[n].\] (1)

Throughout the paper, we call it _Model 1_ for simplicity and to distinguish it from a different and more complicated one that will be introduced in Section 3. The goal is to recover the underlying cluster assignment vector \(z^{*}\). If \(\Sigma^{*}\) were known, then (1) can be converted into an isotropic GMM by a linear transformation \((\Sigma^{*})^{-\frac{1}{2}}Y_{j}\). However, the unknown nature of \(\Sigma^{*}\) makes clustering under this model more challenging than under isotropic GMMs.

Signal-to-noise Ratio.Define the signal-to-noise ratio

\[\text{SNR}=\min_{a,b\in[k]:a\neq b}\|(\theta_{a}^{*}-\theta_{b}^{*})^{T} \Sigma^{*-\frac{1}{2}}\|,\] (2)

which is a function of all the centers \(\{\theta_{a}^{*}\}_{a\in[k]}\) and the covariance matrix \(\Sigma^{*}\). As we will show later in Theorem 2.1, SNR captures the difficulty of the clustering problem and determines the minimax rate. We defer the geometric interpretation of SNR until after presenting Theorem 2.2.

A quantity closely related to SNR is the minimum distance among the centers. Define \(\Delta\) as

\[\Delta=\min_{a,b\in[k]:a\neq b}\left\|\theta_{a}^{*}-\theta_{b}^{*}\right\|.\] (3)

Then we can see SNR and \(\Delta\) are of the same order if all eigenvalues of the covariance matrix \(\Sigma^{*}\) are assumed to be constants. If \(\Sigma^{*}\) is further assumed to be \(\sigma^{2}I_{d}\), then SNR equals \(\Delta/\sigma\). As a result, in [15; 8; 14] where the isotropic GMMs are studied, \(\Delta/\sigma\) plays the role of signal-to-noise ratio and appears in their rates. Since (2) represents a direct generalization, we refer to it as the signal-to-noise ratio for Model 1.

Loss Function.To measure the clustering performance, we consider the following loss function. For any \(z,z^{*}\in[k]^{n},\) we define

\[h(z,z^{*})=\min_{\psi\in\Psi}\frac{1}{n}\sum_{j=1}^{n}\mathbb{I}\left\{\psi(z_ {j})\neq z_{j}^{*}\right\},\] (4)

where \(\Psi=\{\psi:\psi\text{ is a bijection from }[k]\text{ to }[k]\}\). Here, the minimum is taken over all permutations of \([k]\) to address the identifiability issues of the labels \(1,2,\ldots,k\). The loss function measures the proportion of coordinates where \(z\) and \(z^{*}\) differ, modulo any permutation of label symbols. Thus, it is referred to as the misclustering error rate in this paper. Another loss that will be used is \(\ell(z,z^{*})\) defined as

\[\ell(z,z^{*})=\sum_{j=1}^{n}\left\|\theta_{z_{j}}^{*}-\theta_{z_{j} ^{*}}^{*}\right\|^{2}.\] (5)

It measures the clustering performance of \(z\) considering the distances among the true centers. It is related to \(h(z,z^{*})\) as \(h(z,z^{*})\leq\ell(z,z^{*})/(n\Delta^{2})\) and provides more information than \(h(z,z^{*})\). We will mainly use \(\ell(z,z^{*})\) in the technical analysis but will present results using \(h(z,z^{*})\) which is more interpretable.

### Minimax Lower Bound

We first establish the minimax lower bound for the clustering problem under Model 1.

**Theorem 2.1**.: _Under the assumption \(\frac{\text{SNR}}{\sqrt{\log k}}\rightarrow\infty\), we have_

\[\inf_{\hat{z}}\sup_{z^{*}\in[k]^{n}}\mathbb{E}h(\hat{z},z^{*}) \geq\exp\!\left(-(1+o(1))\frac{\text{SNR}^{2}}{8}\right).\] (6)

_If \(\text{SNR}=O(1)\) instead, we have \(\inf_{\hat{z}}\sup_{z^{*}\in[k]^{n}}\mathbb{E}h(\hat{z},z^{*})\geq c\) for some constant \(c>0\)._

Theorem 2.1 allows the cluster numbers \(k\) to grow with \(n\) and shows that \(\text{SNR}\rightarrow\infty\) is a necessary condition to have a consistent clustering. If \(k\) is a constant, then \(\text{SNR}\rightarrow\infty\) is also a sufficient condition. Theorem 2.1 holds for any arbitrary configurations of \(\{\theta_{a}^{*}\}_{a\in[k]}\) and \(\Sigma^{*}\), with the minimax lower bound depending on these through SNR. The parameter space is only for \(z^{*}\) while \(\{\theta_{a}^{*}\}_{a\in[k]}\) and \(\Sigma^{*}\) are held fixed. Hence, (6) can be interpreted as a case-specific result, precisely capturing the explicit dependence of the minimax rates on \(\{\theta_{a}^{*}\}_{a\in[k]}\) and \(\Sigma^{*}\).

Theorem 2.1 is closely related to the LDA. If there are only two clusters with known centers and a covariance matrix, then estimating each \(z_{j}^{*}\) becomes exactly the task of the LDA: we aim to determine from which of two normal distributions, each with a different mean but the same covariance matrix, the observation \(Y_{\hat{y}}\) is generated. In fact, this approach is also how Theorem 2.1 is proved: We first reduce the estimation problem of \(z^{*}\) to two-point hypothesis testing for each individual \(z_{j}^{*}\). The error of these tests is analyzed in Lemma A.1 using the LDA, and we then aggregate all these testing errors together.

With the help of Lemma A.1, we have a geometric interpretation of SNR. In the left panel of Figure 1, we have two normal distributions \(\mathcal{N}(\theta_{1}^{*},\Sigma^{*})\) and \(\mathcal{N}(\theta_{2}^{*},\Sigma^{*})\) that \(X\) follows. The black line represents the optimal testing procedure \(\phi\) displayed in Lemma A.1, dividing the space into two half-spaces. To calculate the testing error, we can make the transformation \(X^{\prime}=(\Sigma^{*})^{-\frac{1}{2}}(X-\theta_{1}^{*})\) so that the two normal distributions become isotropic: \(\mathcal{N}(0,I_{d})\) and \(\mathcal{N}((\Sigma^{*})^{-\frac{1}{2}}(\theta_{2}^{*}-\theta_{1}^{*}),I_{d})\) as displayed in the right panel. Then the distance between the two centers is \(\|(\Sigma^{*})^{-\frac{1}{2}}(\theta_{2}^{*}-\theta_{1}^{*})\|\), and the distance from a center to the black curve is half of that. Then, the probability that \(\mathcal{N}(0,I_{d})\) falls within the

Figure 1: A geometric interpretation of SNR.

grayed area equals \(\exp(-(1+o(1))\|(\Sigma^{*})^{-\frac{1}{2}}(\theta_{2}^{*}-\theta_{1}^{*})\|^{2}/8)\), according to Gaussian tail probability. As a result, \(\|(\Sigma^{*})^{-\frac{1}{2}}(\theta_{2}^{*}-\theta_{1}^{*})\|\) is the effective distance between the two centers of \(\mathcal{N}(\theta_{1}^{*},\Sigma^{*})\) and \(\mathcal{N}(\theta_{2}^{*},\Sigma^{*})\) for the clustering problem, taking into account the geometry of the covariance matrix. Since we have multiple clusters, SNR defined in (2) can be interpreted as the minimum effective distance among the centers \(\{\theta_{a}^{*}\}_{a\in[k]}\), considering the anisotropic structure of \(\Sigma^{*}\). This measure captures the intrinsic difficulty of the clustering problem.

### Rate-Optimal Adaptive Procedure

In this section, we give a computationally feasible and rate-optimal procedure for clustering under Model 1. Summarized in Algorithm 1, it is a variant of Lloyd's algorithm. Starting with an initial setup, it iteratively updates the estimates of the centers \(\{\theta_{a}^{*}\}_{a\in[k]}\) (in (7)), the covariance matrix \(\Sigma^{*}\) (in (8)), and the cluster assignment vector \(z^{*}\) (in (9)). This algorithm differs from Lloyd's algorithm in that the latter is designed for isotropic GMMs and does not incorporate the covariance matrix update outlined in (8). Furthermore, (9) updates the estimation of \(z_{j}^{*}\) using \(\operatorname*{argmin}_{a\in[k]}(Y_{j}-\theta_{a}^{(t)})^{T}(Y_{j}-\theta_{a} ^{(t)})\) instead. To differentiate clearly, we refer to the classic form as the _vanilla Lloyd's algorithm_ and our modified version, which accommodates the unknown and anisotropic covariance matrix, as the _adjusted Lloyd's algorithm_.

Algorithm 1 can also be interpreted as a hard EM algorithm. When applying Expectation Maximization (EM) to Model 1, the M step estimates the parameters \(\{\theta_{a}^{*}\}_{a\in[k]}\) and \(\Sigma^{*}\), while the E step estimates \(z^{*}\). It turns out the updates on the parameters (7) - (8) are identical to those in the EM's M step. However, the update of \(z^{*}\) in Algorithm 1 differs from that in the EM. Instead of computing a conditional expectation typical of the E step, the algorithm performs maximization in (9). As a result, Algorithm 1 effectively consists solely of M steps for both parameters and \(z^{*}\), characterizing it as a hard EM algorithm.

``` Input: Data \(Y\), number of clusters \(k\), an initialization \(z^{(0)}\), number of iterations \(T\). Output:\(z^{(T)}\)
1for\(t=1,\ldots,T\)do
2 Update the centers: \[\theta_{a}^{(t)}=\frac{\sum_{j\in[n]}Y_{j}\mathbb{I}\left\{z_{j}^{(t-1)}=a \right\}}{\sum_{j\in[n]}\mathbb{I}\left\{z_{j}^{(t-1)}=a\right\}},\quad\forall a \in[k].\] (7)
3 Update the covariance matrix: \[\Sigma^{(t)}=\frac{\sum_{a\in[k]}\sum_{j\in[n]}(Y_{j}-\theta_{a}^{(t)})(Y_{j} -\theta_{a}^{(t)})^{T}\mathbb{I}\left\{z_{j}^{(t-1)}=a\right\}}{n}.\] (8)
4 Update the cluster assignment vector: \[z_{j}^{(t)}=\operatorname*{argmin}_{a\in[k]}(Y_{j}-\theta_{a}^{(t)})^{T}( \Sigma^{(t)})^{-1}(Y_{j}-\theta_{a}^{(t)}),\quad\forall j\in[n].\] (9)

**Algorithm 1**Adjusted Lloyd's Algorithm for Model 1.

In Theorem 2.2, we give a computational and statistical guarantee of Algorithm 1. We show that starting from a decent initialization, within \(\log n\) iterations, Algorithm 1 achieves the error rate \(\exp\bigl{(}-(1+o(1))\text{SNR}^{2}/8\bigr{)}\) which matches the minimax lower bound given in Theorem 2.1. As a result, Algorithm 1 is a rate-optimal procedure. In addition, the algorithm is fully adaptive to the unknown \(\{\theta_{a}^{*}\}_{a\in[k]}\) and \(\Sigma^{*}\). The sole piece of information presumed to be known is \(k\), the number of clusters, as commonly assumed in clustering literature [15; 8; 14]. The theorem also shows that the number of iterations needed to achieve the optimal rate is at most \(\log n\), providing implementation guidance to practitioners.

**Theorem 2.2**.: _Assume \(k=O(1)\), \(d=O(\sqrt{n})\), and \(\min_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\geq\frac{\alpha n}{k}\) for some constant \(\alpha>0\). Assume SNR \(\rightarrow\infty\) and \(\lambda_{d}(\Sigma^{*})/\lambda_{1}(\Sigma^{*})=O(1)\). For Algorithm 1, suppose \(z^{(0)}\) satisfies \(\ell(z^{(0)},z^{*})=o(n)\) with probability at least \(1-\eta\). Then with probability at least \(1-\eta-n^{-1}-\exp(-\text{SNR})\), we have_

\[h(z^{(t)},z^{*})\leq\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8}\biggr{)}, \quad\text{for all }t\geq\log n.\]

We make the following remarks on the assumptions of Theorem 2.2: When \(k\) is constant, the assumption that \(\text{SNR}\rightarrow\infty\) is a necessary condition for consistent recovery of \(z^{*}\), as outlined in the minimax lower bound presented in Theorem 2.1. The assumption on \(\Sigma^{*}\) ensures that the covariance matrix is well-conditioned. The dimensionality \(d\) is assumed to be \(O(\sqrt{n})\), a stronger assumption than in [15; 8; 14], where \(d=O(n)\) is sufficient. This is because, unlike these studies, our work requires estimating the covariance matrix \(\Sigma^{*}\) and controlling the estimation error \(\|\Sigma^{(t)}-\Sigma^{*}\|\).

Theorem 2.2 needs a decent initialization \(z^{(0)}\) in the sense that it is sufficiently close to the ground truth such that \(\ell(z^{(0)},z^{*})=o(n)\). This is because our theoretical analysis requires the initialization being within a specific proximity to the true parameters. The requirement can be fulfilled by simple procedures. An example is the vanilla Lloyd's algorithm whose performance is studied in [15; 8]. Though [15; 8] are for isotropic GMMs, their results can be extended to sub-Gaussian mixture models with nearly identical proof. Since \(\epsilon_{j}\) are sub-Gaussian random variables with proxy variance \(\lambda_{d}(\Sigma^{*})\), [8] implies the vanilla Lloyd's algorithm output \(\hat{z}\) satisfies \(\ell(\hat{z},z^{*})\leq n\exp(-(1+o(1))\Delta^{2}/(8\lambda_{d}(\Sigma^{*})))\) with probability at least \(1-\exp(-\Delta/\sqrt{\lambda_{d}(\Sigma^{*})})-n^{-1}\), under the assumption that \(\Delta^{2}/(k^{2}(kd/n+1)\lambda_{d}(\Sigma^{*}))\rightarrow\infty\). Then we have \(\ell(\hat{z},z^{*})=o(n)\) with high probability under the assumptions of Theorem 2.2, and hence it can be used as an initialization for the algorithm.

## 3 GMM with Unknown and Heterogeneous Covariance Matrices

### Model

In this section, we study the GMM where the covariance matrices of each cluster are unknown and not necessarily equal to each other. The data-generation process can be displayed as follows,

**Model 2:**\[Y_{j}=\theta_{z_{j}^{*}}^{*}+\epsilon_{j},\text{ where }\epsilon_{j} \stackrel{{ ind}}{{\sim}}\mathcal{N}(0,\Sigma_{z_{j}^{*}}^{*}), \forall j\in[n].\] (10)

We refer to this as _Model 2_ throughout the paper to distinguish it from Model 1, as discussed in Section 2. The key difference between (10) and (1) is that here we have distinct covariance matrices \(\{\Sigma_{a}^{*}\}_{a\in[k]}\) for each cluster, instead of a single shared \(\Sigma^{*}\). We use the same loss function as defined in (4).

Signal-to-noise Ratio.The signal-to-noise ratio for Model 2 is defined as follows. We use the notation \(\text{SNR}^{\prime}\) to distinguish it from the SNR used for Model 1. Compared to SNR, \(\text{SNR}^{\prime}\) is much more complicated and does not have an explicit formula. We first define a set \(B_{a,b}\subset\mathbb{R}^{d}\) for any \(a,b\in[k]\) such that \(a\neq b\):

\[B_{a,b}=\Bigg{\{}x\in\mathbb{R}^{d}: x^{T}\Sigma_{a}^{*\frac{1}{2}}\Sigma_{b}^{*-1}(\theta_{a}^{*}- \theta_{b}^{*})+\frac{1}{2}x^{T}\Bigl{(}\Sigma_{a}^{*\frac{1}{2}}\Sigma_{b}^{* -1}\Sigma_{a}^{*\frac{1}{2}}-I_{d}\Bigr{)}\,x\] \[\leq-\frac{1}{2}(\theta_{a}^{*}-\theta_{b}^{*})^{T}\Sigma_{b}^{*-1 }(\theta_{a}^{*}-\theta_{b}^{*})+\frac{1}{2}\log|\Sigma_{a}^{*}|-\frac{1}{2} \log|\Sigma_{b}^{*}|\,\Bigg{\}}.\]

We then define \(\text{SNR}^{\prime}_{a,b}=2\min_{x\in B_{a,b}}\|x\|\) and

\[\text{SNR}^{\prime}=\min_{a,b\in[k]:a\neq b}\text{SNR}^{\prime}_{a,b}.\] (11)

The form of \(\text{SNR}^{\prime}\) is closely connected to the testing error of the QDA, which we will give in Lemma 3.1. The interpretation of the \(\text{SNR}^{\prime}\), particularly from a geometric perspective, will be

[MISSING_PAGE_FAIL:7]

calculate this, we transform \(X\) to \(X^{\prime}=(\Sigma_{1}^{*})^{-\frac{1}{2}}(X-\theta_{1}^{*})\), standardizing the first distribution. Then, as displayed in the right panel of Figure 2, the two distributions become \(\mathcal{N}(0,I_{d})\) and \(\mathcal{N}((\Sigma_{1}^{*})^{-\frac{1}{2}}(\theta_{2}^{*}-\theta_{1}^{*}),( \Sigma_{1}^{*})^{-\frac{1}{2}}\Sigma_{2}^{*}(\Sigma_{1}^{*})^{-\frac{1}{2}})\), and the optimal testing procedure \(\phi\) becomes \(\mathbb{I}\left\{X^{\prime}\in B_{1,2}\right\}\). As a result, in the right panel of Figure 2, \(B_{1,2}\) represents the space colored by gray, and the black curve is its boundary. Then \(\mathbb{P}_{\mathbb{H}_{0}}(\phi=1)\) is equal to \(\mathbb{P}(\mathcal{N}(0,I_{d})\in B_{1,2})\). Under the assumption \(d=O(1)\) and \(\max_{a,b\in\{1,2\}}\lambda_{d}(\Sigma_{a}^{*})/\lambda_{1}(\Sigma_{b}^{*})=O (1)\), in Lemma C.10, we can show \(\mathbb{P}(\mathcal{N}(0,I_{d})\in B_{1,2})=\exp(-(1+o(1))\text{SNR}_{1,2}^{ \prime}/8)\). As a result, \(\text{SNR}^{\prime}\) can be interpreted as the minimum effective distance among the centers \(\{\theta_{a}^{*}\}_{a\in[k]}\), considering the anisotropic and heterogeneous structure of \(\{\Sigma_{a}^{*}\}_{a\in[k]}\), and it captures the intrinsic difficulty of the clustering problem under Model 2.

### Optimal Adaptive Procedure

In this section, we give a computationally feasible and rate-optimal procedure for clustering under Model 2. Similar to Algorithm 1, Algorithm 2 is a variant of Lloyd's algorithm, adjusted to accommodate unknown and heterogeneous covariance matrices. It can also be interpreted as a hard EM algorithm under Model 2. Algorithm 2 differs from Algorithm 1 in (13) and (14), as now there are \(k\) covariance matrices instead of a common one.

``` Input: Data \(Y\), number of clusters \(k\), an initialization \(z^{(0)}\), number of iterations \(T\). Output:\(z^{(T)}\)
1for\(t=1,\ldots,T\)do
2 Update the centers: \[\theta_{a}^{(t)}=\frac{\sum_{j\in[n]}Y_{j}\mathbb{I}\left\{z_{j}^{(t-1)}=a \right\}}{\sum_{j\in[n]}\mathbb{I}\left\{z_{j}^{(t-1)}=a\right\}},\quad\forall a \in[k].\] (12) Update the covariance matrices: \[\Sigma_{a}^{(t)}=\frac{\sum_{j\in[n]}(Y_{j}-\theta_{a}^{(t)})(Y_{j}-\theta_{a} ^{(t)})^{T}\mathbb{I}\left\{z_{j}^{(t-1)}=a\right\}}{\sum_{j\in[n]}\mathbb{I} \left\{z_{j}^{(t-1)}=a\right\}},\quad\forall a\in[k].\] (13) Update the cluster assignment vector: \[z_{j}^{(t)}=\operatorname*{argmin}_{a\in[k]}(Y_{j}-\theta_{a}^{(t)})^{T}( \Sigma_{a}^{(t)})^{-1}(Y_{j}-\theta_{a}^{(t)})+\log|\Sigma_{a}^{(t)}|,\quad \forall j\in[n].\] (14) ```

**Algorithm 2**Adjusted Lloyd's Algorithm for Model 2.

In Theorem 3.2, we give a computational and statistical guarantee for Algorithm 2. We demonstrate that, with proper initialization, Algorithm 2 achieves the minimax lower bound within \(\log n\) iterations. The assumptions needed in Theorem 3.2 are similar to those in Theorem 2.2, except that we require stronger assumptions on the dimensionality \(d\) since now we have \(k\) (instead of one) covariance matrices to be estimated. In addition, by assuming \(\max_{a,b\in[k]}\lambda_{d}(\Sigma_{a}^{*})/\lambda_{1}(\Sigma_{b}^{*})=O(1)\), we ensure not only that each of the \(k\) covariance matrices is well-conditioned but also that they are comparable to one another.

**Theorem 3.2**.: _Assume \(k=O(1)\), \(d=O(1)\), and \(\min_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\geq\frac{\alpha n}{k}\) for some constant \(\alpha>0\). Assume \(\text{SNR}^{\prime}\rightarrow\infty\) and \(\max_{a,b\in[k]}\lambda_{d}(\Sigma_{a}^{*})/\lambda_{1}(\Sigma_{b}^{*})=O(1)\). For Algorithm 2, suppose \(z^{(0)}\) satisfies \(\ell(z^{(0)},z^{*})=o(n)\) with probability at least \(1-\eta\). Then with probability at least \(1-\eta-5n^{-1}-\exp(-\text{SNR}^{\prime})\), we have_

\[h(z^{(t)},z^{*})\leq\exp\!\left(-(1+o(1))\frac{\text{SNR}^{\prime} {}^{2}}{8}\right),\quad\text{for all }t\geq\log n.\]

The vanilla Lloyd's algorithm can be used as the initialization for Algorithm 2. This is because Model 2 is also a sub-Gaussian mixture model. By the same argument as in Section 2.3, the output of the vanilla Lloyd's algorithm \(\hat{z}\) satisfies \(\ell(\hat{z},z^{*})=o(n)\) with high probability under the assumptions of Theorem 3.2.

We conclude this section with a time complexity analysis of Algorithm 2. Compared to the vanilla Lloyd's algorithm, our method introduces additional computational overhead due to the need for computing the inverse and determinant of covariance matrices. Specifically, the time complexity of Algorithm 2 is \(O(nkd^{3}T)\). In contrast, the vanilla Lloyd's algorithm has a lower time complexity of \(O(nkdT)\). The increase in complexity stems from matrix operations in \(d\) dimensions, as both matrix inversion and determinant computation scale as \(O(d^{3})\).

## 4 Numerical Studies

In this section, we compare the performance of our methods with other popular clustering methods on synthetic and real datasets under different settings.

Model 1.The first simulation is designed for the GMM with unknown but homogeneous covariance matrices (i.e., Model 1). We independently generate \(n=1200\) samples with dimension \(d=50\) from \(k=30\) clusters. Each cluster has 40 samples. We set \(\Sigma^{*}=U^{T}\Lambda U\), where \(\Lambda\) is a \(50\times 50\) diagonal matrix with diagonal elements selected from 0.5 to 8 with equal space and \(U\) is a randomly generated orthogonal matrix. The centers \(\{\theta_{a}^{*}\}_{a\in[n]}\) are orthogonal to each other with \(\|\theta_{1}^{*}\|=\ldots=\|\theta_{30}^{*}\|=9\). We consider four popular clustering methods: (1) the spectral clustering method in [14] (denoted as "spectral"), (2) the vanilla Lloyd's algorithm in [15] (denoted as "vanilla Lloyd"), (3) Algorithm 1 initialized by the spectral clustering (denoted as "spectral + Alg 1"), and (4) Algorithm 1 initialized by the vanilla Lloyd (denoted as "vanilla Lloyd + Alg 1"). The comparison is presented in the left panel of Figure 3.

Model 2.We also compare the performances of four methods (spectral, vanilla Lloyd, spectral + Alg 2, and vanilla Lloyd + Alg 2) for the GMM with unknown and heterogeneous covariance matrices (i.e., Model 2). In this case, we take \(n=1200\), \(k=2\), and \(d=9\). We set \(\Sigma_{1}^{*}=I_{d}\) and \(\Sigma_{2}^{*}=\Lambda_{2}\), a diagonal matrix where the first diagonal entry is 0.5 and the remaining entries are 5. We set the cluster sizes to be 900 and 300, respectively. To simplify the calculation of SNR\({}^{\prime}\), we set \(\theta_{1}^{*}=0\) and \(\theta_{2}^{*}=5e_{1}\), with \(e_{1}\) being the vector that has a 1 in its first entry and 0s elsewhere. The comparison is presented in the right panel of Figure 3.

In Figure 3, the \(x\)-axis is the number of iterations and the \(y\)-axis is the logarithm of the misclustering error rate, i.e., \(\log(h)\). Each of the curves plotted is an average of 100 independent trials. We can see both Algorithm 1 and Algorithm 2 outperform the spectral clustering and the vanilla Lloyd's algorithm significantly. Additionally, the dashed lines in the left and right panels represent the optimal exponents \(-\text{SNR}^{2}/8\) and \(-\text{SNR}^{\prime 2}/8\) of the minimax bounds, respectively. It is observed that both Algorithm 1 and Algorithm 2 meet these benchmarks after three iterations. This justifies the conclusion that both algorithms are rate-optimal.

Figure 3: Left: Performance of Algorithm 1 compared with other methods under Model 1. Right: Performance of Algorithm 2 compared with other methods under Model 2.

Real Data.To further demonstrate the effectiveness of our methods, we conduct experiments using the Fashion-MNIST dataset [23]. In the first analysis, we use a total of 12,000 28\(\times\)28 grayscale images, consisting of 6,000 images each from the T-shirt/top class and the Trouser class. The left panel of Figure 4 gives a visualization of the data points using their first two principal components, showing the anisotropic and heterogeneous covariance structures. Since a large number of pixels have zero across most images, we apply PCA to reduce dimensionality from 784 to 50 by retaining the top 50 principal components. Our Algorithm 2 achieves a misclustering error of 5.71%, outperforming the vanilla Lloyd's algorithm, which has an error of 8.24%. In the second analysis, we incorporate an additional class, the Ankle boot class, increasing the total to 18,000 images across three classes. Following the same preprocessing steps, the visualization of the dataset's structure in the right panel of Figure 4 again confirms the presence of anisotropic and heterogeneous covariances. Here, Algorithm 2 achieves an error of 3.97%, an improvement over the 5.64% error rate observed with the vanilla Lloyd's algorithm.

## 5 Conclusion

This paper focuses on clustering methods and theory for GMMs, with anisotropic covariance structures, presenting new minimax bounds and an adjusted Lloyd's algorithm tailored for varying covariance structures. Our theoretical and empirical analyses demonstrate the algorithm's ability to achieve optimality within a logarithmic number of iterations. Despite these advances, our results have some limitations that are worth addressing in future work:

1. **High-Dimensional Settings:** Current results are restricted to dimensions \(d\) growing at a rate slower than \(n\), specifically \(d=O(\sqrt{n})\) as stated in Theorem 2.2. Section 3 further requires a stronger assumption \(d=O(1)\). These constraints stem from technical challenges in estimating covariance matrices accurately and in controlling matrix determinant. Adopting more sophisticated analytical tools could potentially relax these bounds to \(d=O(n)\). In scenarios where \(d\) exceeds \(n\), the misclustering error deviates from the simpler exponential decay observed under isotropic GMMs, as shown in [16]. This suggests that our model might also exhibit similar complexities, warranting further exploration into the technique used in [16] for potential extensions.
2. **Ill-Conditioned Covariance Structures:** Our analysis relies on the assumption of well-conditioned covariance matrices, where \(\max_{a,b\in[k]}\lambda_{d}(\Sigma_{a}^{*})/\lambda_{1}(\Sigma_{b}^{*})=O(1)\). This condition is crucial for the current analytical framework, as it helps manage the estimation errors of covariance matrices and their inverses. While more advanced techniques may allow for a relaxation of this assumption, handling ill-conditioned or degenerate covariance matrices remains challenging, particularly due to the difficulty of working with matrix inverses in such cases. While minimax lower bounds suggest that clustering is still possible even when the covariance matrix is degenerate, it raises computational challenges for our current algorithms. This highlights the need for developing new algorithms that can function effectively under less restrictive conditions.

Figure 4: Visualization of the Fashion-MNIST dataset using the first two principal components. The data points are color-coded to indicate class membership: Red represents the T-shirt/top class, green denotes the Trouser class, and blue signifies the Ankle boot class. This illustration shows the existence of anisotropic and heterogeneous covariance structures.

## References

* [1] Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An \(\ell_{p}\) theory of PCA and spectral clustering. _The Annals of Statistics_, 50(4):2359-2385, 2022.
* [2] Christopher M Bishop. _Pattern recognition and machine learning_. springer, 2006.
* [3] S Charles Brubaker and Santosh S Vempala. Isotropic PCA and affine-invariant clustering. In _Building Bridges_, pages 241-281. Springer, 2008.
* [4] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, 1977.
* [5] Yingjie Fei and Yudong Chen. Hidden integrality of SDP relaxations for sub-Gaussian mixture models. In _Conference On Learning Theory_, pages 1931-1965. PMLR, 2018.
* [6] Ronald A Fisher. The use of multiple measurements in taxonomic problems. _Annals of eugenics_, 7(2):179-188, 1936.
* [7] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. _The elements of statistical learning_, volume 1. Springer series in statistics New York, 2001.
* [8] Chao Gao and Anderson Y Zhang. Iterative algorithm for discrete structure recovery. _The Annals of Statistics_, 50(2):1066-1094, 2022.
* [9] Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed \(k\)-means. _Mathematical Statistics and Learning_, 1(3):317-374, 2019.
* [10] Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian random vectors. _Electronic Communications in Probability_, 17, 2012.
* [11] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two Gaussians. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 553-562, 2010.
* [12] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. _The Annals of Statistics_, pages 1302-1338, 2000.
* [13] Stuart Lloyd. Least squares quantization in PCM. _IEEE transactions on information theory_, 28(2):129-137, 1982.
* [14] Matthias Loffler, Anderson Y Zhang, and Harrison H Zhou. Optimality of spectral clustering in the Gaussian mixture model. _The Annals of Statistics_, 49(5):2506-2530, 2021.
* [15] Yu Lu and Harrison H Zhou. Statistical and computational guarantees of Lloyd's algorithm and its variants. _arXiv preprint arXiv:1612.02099_, 2016.
* [16] Mohamed Ndaoud. Sharp optimal recovery in the two component Gaussian mixture model. _The Annals of Statistics_, 50(4):2096-2126, 2022.
* [17] Karl Pearson. Contributions to the mathematical theory of evolution. _Philosophical Transactions of the Royal Society of London. A_, 185:71-110, 1894.
* [18] Daniel A Spielman and Shang-Hua Teng. Spectral partitioning works: Planar graphs and finite element meshes. In _Proceedings of 37th Conference on Foundations of Computer Science_, pages 96-105. IEEE, 1996.
* [19] D Michael Titterington, Adrian FM Smith, and Udi E Makov. _Statistical analysis of finite mixture distributions_. Wiley,, 1985.
* [20] S. Vempala and G. Wang. A spectral algorithm for learning mixture models. _J. Comput. Syst. Sci._, 68(4):841-860, 2004.
* [21] Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17(4):395-416, 2007.

* [22] Kaizheng Wang, Yuling Yan, and Mateo Diaz. Efficient clustering for stretched mixtures: Landscape and optimality. _Advances in Neural Information Processing Systems_, 33:21309-21320, 2020.
* [23] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
* [24] Anderson Y Zhang and Harrison H Zhou. Leave-one-out singular subspace perturbation analysis for spectral clustering. _arXiv preprint arXiv:2205.14855_, 2022.

The appendices are organized as follows. Appendix A is dedicated to proving the results in Section 2. To be more specific, we prove the lower bound, Theorem 2.1, in Appendix A.1 and the upper bound, Theorem 2.2, in Appendix A.2. For the upper bound proof, we first give a high-level idea in Appendix A.2.1, followed by a detailed proof in Appendix A.2.2. Appendix B includes proofs of the results in Section 3: the proof of the lower bound, Theorem 3.1, is in Appendix B.1, and the proof of the upper bound, Theorem 3.2, is in Appendix B.2. We include all technical lemmas and their proofs in Appendix C.

## Appendix A Proofs in Section 2

### Proofs for the Lower Bound

In the following lemma, we give a sharp and explicit formula for the testing error of the LDA. Here we have two normal distributions \(\mathcal{N}(\theta_{1}^{*},\Sigma^{*})\) and \(\mathcal{N}(\theta_{2}^{*},\Sigma^{*})\) and an observation \(X\) that is generated from one of them. We are interested in estimating from which distribution the observation is drawn. By the Neyman-Pearson lemma, it is known that the likelihood ratio test \(\mathbb{I}\left\{2(\theta_{2}^{*}-\theta_{1}^{*})^{T}(\Sigma^{*})^{-1}X\geq \theta_{2}^{*T}(\Sigma^{*})^{-1}\theta_{2}^{*}-\theta_{1}^{*T}(\Sigma^{*})^{- 1}\theta_{1}^{*}\right\}\) is the optimal testing procedure. Then by using the Gaussian tail probability, we are able to obtain the optimal testing error, with its lower bound given in Lemma A.1.

**Lemma A.1** (Testing Error for the LDA).: _Consider two hypotheses \(\mathbb{H}_{0}:X\sim\mathcal{N}(\theta_{1}^{*},\Sigma^{*})\) and \(\mathbb{H}_{1}:X\sim\mathcal{N}(\theta_{2}^{*},\Sigma^{*})\). Define a testing procedure_

\[\phi=\mathbb{I}\left\{2(\theta_{2}^{*}-\theta_{1}^{*})^{T}(\Sigma^{*})^{-1}X \geq\theta_{2}^{*T}(\Sigma^{*})^{-1}\theta_{2}^{*}-\theta_{1}^{*T}(\Sigma^{*}) ^{-1}\theta_{1}^{*}\right\}.\]

_Then \(\inf_{\hat{\phi}}(\mathbb{P}_{\mathbb{H}_{0}}(\hat{\phi}=1)+\mathbb{P}_{ \mathbb{H}_{1}}(\hat{\phi}=0))=\mathbb{P}_{\mathbb{H}_{0}}(\phi=1)+\mathbb{P}_ {\mathbb{H}_{1}}(\phi=0)\,.\) If \(\|(\theta_{2}^{*}-\theta_{1}^{*})^{T}(\Sigma^{*})^{-\frac{1}{2}}\|\to\infty\), we have_

\[\inf_{\hat{\phi}}(\mathbb{P}_{\mathbb{H}_{0}}(\hat{\phi}=1)+\mathbb{P}_{ \mathbb{H}_{1}}(\hat{\phi}=0))\geq\exp\!\left(-(1+o(1))\frac{\|(\theta_{2}^{* }-\theta_{1}^{*})^{T}(\Sigma^{*})^{-\frac{1}{2}}\|^{2}}{8}\right).\]

_Otherwise, \(\inf_{\hat{\phi}}(\mathbb{P}_{\mathbb{H}_{0}}(\hat{\phi}=1)+\mathbb{P}_{ \mathbb{H}_{1}}(\hat{\phi}=0))\geq c\) for some constant \(c>0\)._

Proof.: Note that \(\phi\) is the likelihood ratio test. By the Neyman-Pearson lemma, it is the optimal procedure. That is, \(\inf_{\hat{\phi}}(\mathbb{P}_{\mathbb{H}_{0}}(\hat{\phi}=1)+\mathbb{P}_{ \mathbb{H}_{1}}(\hat{\phi}=0))=\mathbb{P}_{\mathbb{H}_{0}}(\phi=1)+\mathbb{P}_ {\mathbb{H}_{1}}(\phi=0)\,.\) Let \(\epsilon\sim\mathcal{N}(0,I_{d})\). By Gaussian tail probability, we have

\[\mathbb{P}_{\mathbb{H}_{0}}(\phi=1)+\mathbb{P}_{\mathbb{H}_{1}} (\phi=0) =\mathbb{P}\big{(}2(\theta_{2}^{*}-\theta_{1}^{*})^{T}(\Sigma^{*}) ^{-1}(\theta_{1}^{*}+\epsilon)\geq\theta_{2}^{*T}(\Sigma^{*})^{-1}\theta_{2}^ {*}-\theta_{1}^{*T}(\Sigma^{*})^{-1}\theta_{1}^{*}\big{)}\] \[\quad+\mathbb{P}\big{(}2(\theta_{2}^{*}-\theta_{1}^{*})^{T}( \Sigma^{*})^{-1}(\theta_{2}^{*}+\epsilon)<\theta_{2}^{*T}(\Sigma^{*})^{-1} \theta_{2}^{*}-\theta_{1}^{*T}(\Sigma^{*})^{-1}\theta_{1}^{*}\big{)}\] \[=2\mathbb{P}\big{(}2(\theta_{2}^{*}-\theta_{1}^{*})^{T}(\Sigma^{* })^{-1}(\theta_{1}^{*}+\epsilon)\geq\theta_{2}^{*T}(\Sigma^{*})^{-1}\theta_{2} ^{*}-\theta_{1}^{*T}(\Sigma^{*})^{-1}\theta_{1}^{*}\big{)}\] \[=2\mathbb{P}\bigg{(}\epsilon>\frac{1}{2}\|(\theta_{2}^{*}-\theta_ {1}^{*})^{T}(\Sigma^{*})^{-\frac{1}{2}}\|\bigg{)}\] \[\geq C\min\left\{1,\frac{1}{\|(\theta_{2}^{*}-\theta_{1}^{*})^{T} (\Sigma^{*})^{-\frac{1}{2}}\|}\exp\!\left(-\frac{\|(\theta_{2}^{*}-\theta_{1 }^{*})^{T}(\Sigma^{*})^{-\frac{1}{2}}\|^{2}}{8}\right)\right\},\]

for some constant \(C>0\). The proof is complete. 

Proof of Theorem 2.1.: We adopt the idea from [15]. Without loss of generality, assume the minimum in (2) is achieved at \(a=1,b=2\) so that \(\text{SNR}=(\theta_{1}^{*}-\theta_{2}^{*})^{T}(\Sigma^{*})^{-1}(\theta_{1}^{*}- \theta_{2}^{*})\). Consider an arbitrary \(\bar{z}\in[k]^{n}\) such that \(|\{i\in[n]:\bar{z}_{i}=a\}|\geq\lceil\frac{n}{k}-\frac{n}{8k^{2}}\rceil\) for any \(a\in[k]\). Then for each \(a\in[k]\), we can choose a subset of \(\{i\in[n]:\bar{z}_{i}=a\}\) with cardinality \(\lceil\frac{n}{k}-\frac{n}{8k^{2}}\rceil\), denoted by \(T_{a}\). Let \(T=\cup_{a\in[k]}T_{a}\). Then we can define a parameter space

\[\mathcal{Z}=\left\{z\in[k]^{n}:z_{i}=\bar{z}_{i}\text{ for all }i\in T\text{ and }z_{i}\in\{1,2\}\text{ if }i\in T^{c}\right\}.\]

Notice that for any \(z\neq\tilde{z}\in\mathcal{Z}\), we have \(\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}\{z_{i}\neq\tilde{z}_{i}\}\leq\frac{k}{n} \frac{n}{8k^{2}}=\frac{1}{8k}\) and \(\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}\{\psi(z_{i})\neq\tilde{z}_{i}\}\geq\frac{1} {n}(\frac{n}{2k}-\frac{n}{8k^{2}})\geq\frac{1}{4k}\) for any permutation \(\psi\) on \([k]\). Thus we can conclude

\[h(z,\tilde{z})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}\{z_{i}\neq\tilde{z}_{i}\}, \quad\text{for all }z,\tilde{z}\in\mathcal{Z}.\]We notice that

\[\inf_{\hat{z}}\sup_{z^{*}\in[k]^{n}}\mathbb{E}h(\hat{z},z^{*}) \geq\ \inf_{\hat{z}}\sup_{z^{*}\in\mathcal{Z}}\mathbb{E}h(\hat{z},z^{*})\] \[\geq\ \inf_{\hat{z}}\frac{1}{|\mathcal{Z}|}\sum_{z^{*}\in\mathcal{Z}} \mathbb{E}h(\hat{z},z^{*})\] \[\geq\ \frac{1}{n}\sum_{i\in T^{c}}\inf_{\hat{z}_{i}}\frac{1}{| \mathcal{Z}|}\sum_{z^{*}\in\mathcal{Z}}\mathbb{P}_{z^{*}}(\hat{z}_{i}\neq z_{i}).\]

Now consider a fixed \(i\in T^{c}\). Define \(\mathcal{Z}_{a}=\{z\in\mathcal{Z}:z_{i}=a\}\) for \(a=1,2\). Then we can see \(\mathcal{Z}=\mathcal{Z}_{1}\cup\mathcal{Z}_{2}\) and \(\mathcal{Z}_{1}\cap\mathcal{Z}_{2}=\emptyset\). What is more, there exists a one-to-one mapping \(f(\cdot)\) between \(\mathcal{Z}_{1}\) and \(\mathcal{Z}_{2}\), such that for any \(z\in\mathcal{Z}_{1}\), we have \(f(z)\in\mathcal{Z}_{2}\) with \([f(z)]_{j}=z_{j}\) for any \(j\neq i\) and \([f(z)]_{i}=2\). Hence, we can reduce the problem to a two-point testing probe and then apply Lemma A.1. We first consider the case that \(\text{SNR}\to\infty\). We have

\[\inf_{\hat{z}_{i}}\frac{1}{|\mathcal{Z}|}\sum_{z^{*}\in\mathcal{Z }}\mathbb{P}_{z^{*}}(\hat{z}_{i}\neq z_{i}) =\inf_{\hat{z}_{i}}\frac{1}{|\mathcal{Z}|}\sum_{z^{*}\in\mathcal{Z }_{1}}\bigl{(}\mathbb{P}_{z^{*}}(\hat{z}_{i}\neq 1)+\mathbb{P}_{f(z^{*})}(\hat{z}_{i} \neq 2)\bigr{)}\] \[\geq\frac{1}{|\mathcal{Z}|}\sum_{z^{*}\in\mathcal{Z}_{1}}\inf_{ \hat{z}_{i}}\bigl{(}\mathbb{P}_{z^{*}}(\hat{z}_{i}\neq 1)+\mathbb{P}_{f(z^{*})}( \hat{z}_{i}\neq 2)\bigr{)}\] \[\geq\frac{|\mathcal{Z}_{1}|}{\mathcal{Z}}\exp\biggl{(}-(1+\eta) \frac{\text{SNR}^{2}}{8}\biggr{)}\] \[\geq\frac{1}{2}\exp\biggl{(}-(1+\eta)\frac{\text{SNR}^{2}}{8} \biggr{)}\,,\]

for some \(\eta=o(1)\). Here the second inequality is due to Lemma A.1. Then,

\[\inf_{\hat{z}}\sup_{z^{*}\in[k]^{n}}\mathbb{E}h(\hat{z},z^{*}) \geq\frac{|T^{c}|}{2n}\exp\biggl{(}-(1+\eta)\frac{\text{SNR}^{2}}{ 8}\biggr{)}=\frac{1}{16k}\exp\biggl{(}-(1+\eta)\frac{\text{SNR}^{2}}{8}\biggr{)}\] \[=\exp\biggl{(}-(1+\eta^{\prime})\frac{\text{SNR}^{2}}{8}\biggr{)}\,,\]

for some other \(\eta^{\prime}=o(1)\), where we use \(\text{SNR}^{2}/\log k\to\infty\).

The proof for the case \(\text{SNR}=O(1)\) is similar and hence is omitted here. 

### Proofs for the Upper Bound

#### a.2.1 High-level Idea

In this section, we provide a high-level idea for the proof of Theorem 2.2. The detailed proof is technical and is given later in Appendix A.2.2.

The key idea for establishing the statistical guarantees of Algorithm 1, an iterative algorithm, is to perform a "one-step" analysis [8]. That is, assume we have an estimation \(z\) for \(z^{*}\). Then we can apply (7), (8), and (9) on \(z\) to obtain \(\{\hat{\theta}_{a}(z)\}_{a\in[k]}\), \(\hat{\Sigma}(z)\), and \(\hat{z}(z)\) sequentially, which all depend on \(z\). Thus, \(\hat{z}(z)\) can be seen as a refined estimate of \(z^{*}\). We will first build the connection between \(\ell(z,z^{*})\) with \(\ell(\hat{z}(z),z^{*})\) as in Lemma A.2, which informally states that under certain conditions, with high probability, we have

\[\ell(\hat{z}(z),z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{2}\ell(z,z^{*})\]

holds for any \(z\in[k]^{n}\) such that \(\ell(z,z^{*})\) is small. Here \(\xi_{\text{ideal}}(\delta)\) refers to the ideal error, which eventually leads to the upper bound in Theorem 2.2. Lemma A.2 tells us \(\hat{z}(\cdot)\) has a "contraction" property. That is, after one iteration of (7), (8), and (9), \(\ell(\hat{z}(z),z^{*})\) is at most a half of \(\ell(z,z^{*})\), up to an additive term \(\xi_{\text{ideal}}(\delta)\).

To establish Lemma A.2, we decompose the loss \(\ell(\hat{z}(z),z^{*})\) into several errors according to the difference in their behaviors. Next, we will introduce several conditions (Conditions 1 - 3), under which we demonstrate that these errors are either negligible, well-controlled by \(\ell(z,z^{*})\), or connected to \(\xi_{\text{ideal}}(\delta)\). Once Lemma A.2 is established, we will show in Lemma A.3 that the connection can be extended to multiple iterations, under two more conditions (Conditions 4 - 5). Lemma A.3 states informally that, under certain conditions and with high probability, we have

\[\ell(z^{(t)},z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{2}\ell(z^{(t-1)},z^ {*})\]

for all \(t\geq 1\). This implies \(\ell(z^{(t)},z^{*})\) is eventually at most \(\xi_{\text{ideal}}(\delta)\), up to some constant factor. Last, we will show all these conditions hold with high probability. Although the algorithmic guarantees in Lemma A.2 and Lemma A.3 are established with respect to the \(\ell(\cdot,\cdot)\) loss, we will use the relationship between \(h(\cdot,\cdot)\) and \(\ell(\cdot,\cdot)\) to convert this result to one involving \(h(\cdot,\cdot)\). Hence, we prove Theorem 2.2.

#### a.2.2 Detailed Proofs

In the statement of Theorem 2.2, the covariance matrix \(\Sigma^{*}\) is assumed to satisfy \(\lambda_{d}(\Sigma^{*})/\lambda_{1}(\Sigma^{*})=O(1)\). Without loss of generality, we can replace it by assuming \(\Sigma^{*}\) satisfies

\[\lambda_{\min}\leq\lambda_{1}(\Sigma^{*})\leq\lambda_{d}(\Sigma^{*})\leq \lambda_{\max}\] (15)

where \(\lambda_{\min},\lambda_{\max}>0\) are two constants. This is due to the following simple argument using the scaling properties of normal distributions. Let \(\{Y_{j}\}\) be some dataset generated according to Model 1 with parameters \(\{\theta_{a}^{*}\}_{a\in[k]}\), \(\Sigma^{*}\), and \(z^{*}\). The assumption \(\lambda_{d}(\Sigma^{*})/\lambda_{1}(\Sigma^{*})=O(1)\) is equivalent to assuming there exist some constants \(\lambda_{\min},\lambda_{\max}>0\) and some quantity \(\sigma>0\) that may depend on \(n\) such that \(\lambda_{\min}\sigma^{2}\leq\lambda_{1}(\Sigma^{*})\leq\lambda_{d}(\Sigma^{*} )\leq\lambda_{\max}\sigma^{2}\). By performing a scaling transformation, we obtain another dataset \(Y_{j}^{\prime}=Y_{j}/\sigma\). Note that: 1) \(\{Y_{j}^{\prime}\}\) can be seen as generated from Model 1 with parameters \(\{\theta_{a}^{*}/\sigma\}_{a\in[k]}\), \(\Sigma^{*}/\sigma^{2}\), and \(z^{*}\). 2) Clustering on \(\{Y_{j}\}\) is equivalent to clustering on \(\{Y_{j}^{\prime}\}\). 3) By the definition in (2), the SNRs that are associated with the data-generating processes of \(\{Y_{j}^{\prime}\}\) and \(\{Y_{j}\}\) are exactly equal to each other. 4) We have \(\lambda_{\min}\leq\lambda_{1}(\Sigma^{*}/\sigma^{2})\leq\lambda_{d}(\Sigma^{* }/\sigma^{2})\leq\lambda_{\max}\). Thus, for the remainder of this section, we assume that (15) holds without any loss of generality.

In the proof, we will mainly use the loss \(\ell(\cdot,\cdot)\) for convenience. Recall \(\Delta\) is defined as the minimum distance among centers in (3). We have

\[h(z,z^{*})\leq\frac{\ell(z,z^{*})}{n\Delta^{2}}.\] (16)

The algorithmic guarantees Lemma A.2 and Lemma A.3 are established with respect to the \(\ell(\cdot,\cdot)\) loss. Eventually, we will use (16) to convert it into a result with respect to \(h(\cdot,\cdot)\) in the proof of Theorem 2.2.

Error Decomposition for the One-step Analysis:Consider an arbitrary \(z\in[k]^{n}\). Apply (7), (8), and (9) on \(z\) to obtain \(\{\hat{\theta}_{a}(z)\}_{a\in[k]}\), \(\hat{\Sigma}(z)\), and \(\hat{z}(z)\):

\[\hat{\theta}_{a}(z) =\frac{\sum_{j\in[n]}Y_{j}\mathbb{I}\left\{z_{j}=a\right\}}{\sum _{j\in[n]}\mathbb{I}\left\{z_{j}=a\right\}},\quad\forall a\in[k]\] \[\hat{\Sigma}(z) =\frac{\sum_{a\in[k]}\sum_{j\in[n]}(Y_{j}-\hat{\theta}_{a}(z))(Y_ {j}-\hat{\theta}_{a}(z))^{T}\mathbb{I}\left\{z_{j}=a\right\}}{n},\] \[\hat{z}_{j}(z) =\operatorname*{argmin}_{a\in[k]}(Y_{j}-\hat{\theta}_{a}(z))^{T} (\hat{\Sigma}(z))^{-1}(Y_{j}-\hat{\theta}_{a}),\quad\forall j\in[n].\]

For simplicity, we denote \(\hat{z}\) as shorthand for \(\hat{z}(z)\). Let \(j\in[n]\) be an arbitrary index with \(z_{j}^{*}=a\). According to (9), \(z_{j}^{*}\) will be incorrectly estimated after one iteration in \(\hat{z}\) if \(a\neq\operatorname*{argmin}_{b\in[k]}(Y_{j}-\hat{\theta}_{b}(z))^{T}(\hat{ \Sigma}(z))^{-1}(Y_{j}-\hat{\theta}_{b}(z))\). Therefore, it is important to analyze the event

\[\langle Y_{j}-\hat{\theta}_{b}(z),(\hat{\Sigma}(z))^{-1}(Y_{j}-\hat{\theta}_{b} (z))\rangle\leq\langle Y_{j}-\hat{\theta}_{a}(z),(\hat{\Sigma}(z))^{-1}(Y_{j}- \hat{\theta}_{a}(z))\rangle,\] (17)

for any \(b\in[k]\setminus\{a\}\). Note that \(Y_{j}=\theta_{a}^{*}+\epsilon_{j}\). After some rearrangements, we can see (17) is equivalent to

\[\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}(\hat{\theta}_{a}(z^ {*})-\hat{\theta}_{b}(z^{*}))\rangle\] \[\leq -\frac{1}{2}\langle\theta_{a}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1 }(\theta_{a}^{*}-\theta_{b}^{*})\rangle+F_{j}(a,b,z)+G_{j}(a,b,z)+H_{j}(a,b,z),\]where

\[F_{j}(a,b,z) =\langle\epsilon_{j},(\hat{\Sigma}(z))^{-1}(\hat{\theta}_{b}(z)- \hat{\theta}_{b}(z^{*}))\rangle-\langle\epsilon_{j},(\hat{\Sigma}(z))^{-1}( \hat{\theta}_{a}(z-\hat{\theta}_{a}(z^{*}))\rangle\] \[\quad+\langle\epsilon_{j},((\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z ^{*}))^{-1})(\hat{\theta}_{b}(z^{*})-\hat{\theta}_{a}(z^{*}))\rangle,\]

\[G_{j}(a,b,z) =\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{a}(z),(\hat{ \Sigma}(z))^{-1}(\theta_{a}^{*}-\hat{\theta}_{a}(z))\rangle-\frac{1}{2} \langle\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}),(\hat{\Sigma}(z))^{-1}(\theta_ {a}^{*}-\hat{\theta}_{a}(z^{*}))\rangle\] \[+\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}),(\hat{ \Sigma}(z))^{-1}(\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}))\rangle-\frac{1}{2} \langle\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}),(\hat{\Sigma}(z^{*}))^{-1}( \theta_{a}^{*}-\hat{\theta}_{a}(z^{*}))\rangle\] \[-\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{b}(z),(\hat{ \Sigma}(z))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z))\rangle+\frac{1}{2} \langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z))^{-1}(\theta_ {a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle\] \[-\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{ \Sigma}(z))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle+\frac{1}{2} \langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z^{*}))^{-1}( \theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle,\]

\[H_{j}(a,b,z)= -\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{ \Sigma}(z^{*}))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle+\frac{1} {2}\langle\theta_{a}^{*}-\theta_{b}^{*},(\hat{\Sigma}(z))^{-1}(\theta_{a}^{*} -\theta_{b}^{*})\rangle\] \[-\frac{1}{2}\langle\theta_{a}^{*}-\theta_{b}^{*},(\hat{\Sigma}( z^{*}))^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle+\frac{1}{2}\langle\theta_{a}^{*}- \theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\] \[+\frac{1}{2}\langle\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}),(\hat{ \Sigma}(z^{*}))^{-1}(\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}))\rangle.\]

In the above decomposition, the expression \(\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}(\hat{\theta}_{a}(z^{*})-\hat{ \theta}_{b}(z^{*}))\rangle\leq-\frac{1}{2}\langle\theta_{a}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\) does not involve \(z\). Roughly speaking, it corresponds to the event that \(z_{j}^{*}\) will be incorrectly estimated in \(\hat{z}(z^{*})\). This is considered the main part of (17) and will contribute to \(\xi_{\text{ideal}}\). The difference between (17) and the main term is expressed through the terms \(F_{j},G_{j},H_{j}\): \(F_{j}\) includes terms related to noise \(\epsilon_{j}\), illustrating the impact of measurement noise; \(G_{j}\) covers estimation errors for cluster centers (\(\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*})\)) and covariance matrices (\(\hat{\Sigma}(z)-\hat{\Sigma}(z^{*})\)), showing the effect of the parameter estimation inaccuracies; \(H_{j}\) contains all other terms from additional error sources. Readers can refer to [8] for more information about the decomposition.

Conditions and Guarantees for One-step Analysis.We continue to analyze the event (17). We first define a quantity independent of \(z\), which we refer to as the ideal error:

\[\xi_{\text{ideal}}(\delta)=\sum_{j=1}^{n}\sum_{b\in[k]\setminus \{z_{j}^{*}\}}\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\Bigg{\{} \langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}(\hat{\theta}_{a}(z^{*})-\hat{ \theta}_{b}(z^{*}))\rangle\] \[\leq-\frac{1-\delta}{2}\langle\theta_{a}^{*}-\theta_{b}^{*},( \Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\Bigg{\}}.\]

When \(\delta=0\), it is determined by the main term in (17), namely \(\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}(\hat{\theta}_{a}(z^{*})-\hat{ \theta}_{b}(z^{*}))\rangle\leq-\frac{1}{2}\langle\theta_{a}^{*}-\theta_{b}^{*},( \Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\). Roughly speaking, \(\xi_{\text{ideal}}(0)\) relates to the performance of \(\hat{z}(z^{*})\). Due to the presence of the terms \(F_{j},G_{j},H_{j}\) in the decomposition of (17), what appears in the analysis of (17) is \(\xi_{\text{ideal}}(\delta)\) instead of \(\xi_{\text{ideal}}(0)\) where hopefully \(\delta>0\) is some small number.

To establish the guarantee for one-step analysis, we next give several conditions on the error terms \(F_{j}(a,b;z),G_{j}(a,b;z)\) and \(H_{j}(a,b;z)\).

**Condition 1**.: _Assume that_

\[\max_{\{z:l(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k]\setminus\{z_{j}^{*} \}}\frac{|H_{j}(z_{j}^{*},b,z)|}{\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},( \Sigma^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle}\leq\frac{ \delta}{4}\]

_holds with probability at least \(1-\eta_{1}\) for some \(\tau,\delta,\eta_{1}>0\)._

**Condition 2**.: _Assume that_

\[\max_{\{z:l(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z_{j}^{*} \}}\frac{F_{j}(z_{j}^{*},b,z)^{2}\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}}{ \langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z_{j}^{*}} ^{*}-\theta_{b}^{*})\rangle^{2}\ell(z,z^{*})}\leq\frac{\delta^{2}}{128}\]

_holds with probability at least \(1-\eta_{2}\) for some \(\tau,\delta,\eta_{2}>0\)._

**Condition 3**.: _Assume that_

\[\max_{\{z:l\{z,z^{*}\}\leq\tau\}}\max_{j\in[n]}\max_{b\in[k]\setminus\{z^{*}_{j} \}}\frac{|G_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},( \Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle}\leq\frac{ \delta}{8}\]

_holds with probability at least \(1-\eta_{3}\) for some \(\tau,\delta,\eta_{3}>0\)._

**Lemma A.2**.: _Assumes Conditions 1 - 3 hold for some \(\tau,\delta,\eta_{1},\eta_{2},\eta_{3},>0\). We then have_

\[\mathbb{P}\bigg{(}\ell(\hat{z},z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{ 2}\ell(z,z^{*})\text{ for any }z\in[k]^{n}\text{ such that }\ell(z,z^{*})\leq\tau\bigg{)}\geq 1-\eta,\]

_where \(\eta=\sum_{i=1}^{3}\eta_{i}\)._

Proof.: Consider any \(j\in[n]\) such that \(z^{*}_{j}=a\). We notice that for any \(b\in[k]\) such that \(b\neq a\),

\[\mathbb{I}\left\{\hat{z}_{j}=b\right\} \leq\mathbb{I}\left\{\langle Y_{j}-\hat{\theta}_{b}(z),(\hat{ \Sigma}(z))^{-1}(Y_{j}-\hat{\theta}_{b}(z))\rangle\leq\langle Y_{j}-\hat{ \theta}_{a}(z),(\hat{\Sigma}(z))^{-1}(Y_{j}-\hat{\theta}_{a}(z))\rangle\right\}\] \[= \mathbb{I}\bigg{\{}\langle\epsilon_{j},(\hat{\Sigma}(z^{*})^{-1 })(\hat{\theta}_{z^{*}_{j}}(z^{*})-\hat{\theta}_{b}(z^{*}))\rangle\] \[\leq-\frac{1}{2}\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},( \Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle+F_{j}(z^{*}_{j},b,z)+G_{j}(z^{*}_{j},b,z)+H_{j}(z^{*}_{j},b,z)\bigg{\}}\] \[\leq \mathbb{I}\left\{\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}( \hat{\theta}_{z^{*}_{j}}(z^{*})-\hat{\theta}_{b}(z^{*}))\rangle\leq-\frac{1- \delta}{2}\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle\right\}\] \[\quad+\mathbb{I}\left\{\frac{\delta}{2}\langle\theta^{*}_{z^{*}_{ j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle \leq F_{j}(z^{*}_{j},b,z)+G_{j}(z^{*}_{j},b,z)+H_{j}(z^{*}_{j},b,z)\right\}\] \[\leq \mathbb{I}\left\{\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{-1}( \hat{\theta}_{z^{*}_{j}}(z^{*})-\hat{\theta}_{b}(z^{*}))\rangle\leq-\frac{1- \delta}{2}\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle\right\}\] \[\quad+\frac{64F_{j}(z^{*}_{j},b,z)^{2}}{\delta^{2}\langle\theta^{* }_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{ *}_{b})\rangle^{2}},\]

where the second inequality comes from Conditions 1 and 3. Note that we can multiply \(\mathbb{I}\left\{\hat{z}_{j}=b\right\}\) on both sides of the above display and the inequality still holds. Hence,

\[\mathbb{I}\left\{\hat{z}_{j}=b\right\} \leq\mathbb{I}\left\{\langle\epsilon_{j},(\hat{\Sigma}(z^{*}))^{- 1}(\hat{\theta}_{z^{*}_{j}}(z^{*})-\hat{\theta}_{b}(z^{*}))\rangle\leq-\frac{1 -\delta}{2}\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle\right\}\] \[\quad+\frac{64F_{j}(z^{*}_{j},b,z)^{2}}{\delta^{2}\langle\theta^{* }_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{ *}_{b})\rangle^{2}}\mathbb{I}\left\{\hat{z}_{j}=b\right\}.\]

Thus, we have

\[\ell(\hat{z},z^{*})\] \[= \sum_{j=1}^{n}\sum_{b\in[k]\setminus\{a\}}\left\|\theta^{*}_{b}- \theta^{*}_{z^{*}_{j}}\right\|^{2}\mathbb{I}\left\{\hat{z}_{j}=b\right\}\] \[\leq \xi_{\text{ideal}}(\delta)+\sum_{j=1}^{n}\sum_{b\in[k]\setminus \{z^{*}_{j}\}}\left\|\theta^{*}_{b}-\theta^{*}_{z^{*}_{j}}\right\|^{2}\mathbb{I} \left\{\hat{z}_{j}=b\right\}\frac{64F_{j}(z^{*}_{j},b,z)^{2}}{\delta^{2} \langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*} _{j}}-\theta^{*}_{b})\rangle^{2}}\] \[\leq \xi_{\text{ideal}}(\delta)+\sum_{j=1}^{n}\max_{b\in[k]\setminus \{z^{*}_{j}\}}\left\|\theta^{*}_{b}-\theta^{*}_{z^{*}_{j}}\right\|^{2}\frac{64F_{ j}(z^{*}_{j},b,z)^{2}}{\delta^{2}\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},( \Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}}\] \[\leq \xi_{\text{ideal}}(\delta)+\frac{\ell(z,z^{*})}{2},\]

which implies Lemma A.2. Here the last inequality uses Condition 2.

Conditions and Guarantees for Multiple Iterations.In the above, we establish a statistical guarantee for the one-step analysis. Now we will extend the result to multiple iterations. That is, starting from some initialization \(z^{(0)}\), we will characterize how the losses \(\ell(z^{(0)},z^{*})\), \(\ell(z^{(1)},z^{*})\), \(\ell(z^{(2)},z^{*})\),..., decay. We impose conditions on \(\xi_{\text{ideal}}(\delta)\) and the initialization \(z^{(0)}\).

**Condition 4**.: _Assume that_

\[\xi_{\text{ideal}}(\delta)\leq\frac{3\tau}{8}\]

_holds with probability at least \(1-\eta_{4}\) for some \(\tau,\delta,\eta_{4}>0\)._

Finally, we need a condition on the initialization.

**Condition 5**.: _Assume that_

\[\ell(z^{(0)},z^{*})\leq\tau\]

_holds with probability at least \(1-\eta_{5}\) for some \(\tau,\eta_{5}>0\)._

With these conditions satisfied, we can give a lemma that shows the convergence of our algorithm.

**Lemma A.3**.: _Assume Conditions 1 - 5 hold for some \(\tau,\delta,\eta_{1},\eta_{2},\eta_{3},\eta_{4},\eta_{5}>0\). We then have_

\[\ell(z^{(t)},z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{2}\ell(z^{(t-1)}, z^{*})\]

_for all \(t\geq 1\), with probability at least \(1-\eta\), where \(\eta=\sum_{i=1}^{5}\eta_{i}\)._

Proof.: By Conditions 4, 5 and a mathematical induction argument, we can easily conclude \(\ell(z^{(t)},z^{*})\leq\tau\) for any \(t\geq 0\). Thus, Lemma A.3 is a direct extension of Lemma A.2. 

With-high-probability Results for the Conditions and Proof of Theorem 2.2.Recall the definition of \(\Delta\) in (3). Recall that in (15) we assume \(\lambda_{\min}\leq\lambda_{1}(\Sigma^{*})\leq\lambda_{d}(\Sigma^{*})\leq \lambda_{\max}\) for two constants \(\lambda_{\min},\lambda_{\max}>0\). Hence we have \(\Delta\) is of the same order as SNR. Specifically, we have

\[\frac{1}{\sqrt{\lambda_{\max}}}\Delta\leq\text{SNR}\leq\frac{1}{\sqrt{\lambda _{\min}}}\Delta.\] (18)

Hence the assumption \(\text{SNR}\to\infty\) in the statement of Theorem 2.2 is equivalently \(\Delta\to\infty\). Next, we give two with-high-probability lemmas. The first lemma is for Conditions 1-3, providing upper bounds for the quantities involved in these conditions, showing that \(\delta\) can be taken as some \(o(1)\) term. The second lemma shows that for any \(\delta=o(1)\), \(\xi_{\text{ideal}}(\delta)\) is upper bounded by the desired minimax rate multiplied by the sample size \(n\).

**Lemma A.4**.: _Under the same conditions as in Theorem 2.2, for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(\alpha\) and \(C^{\prime}\) such that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{|H_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*} _{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b}) \rangle} \leq C\sqrt{\frac{k(d+\log n)}{n}}\] (19) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{F_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}}- \theta^{*}_{b}\|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{* })^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})} \leq Ck^{3}\bigg{(}\frac{\tau}{n}+\frac{1}{\Delta^{2}}+\frac{d^{2}}{n \Delta^{2}}\bigg{)}\] (20) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{|G_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*} _{j}}-\theta^{*}_{b},(\Sigma^{*})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b}) \rangle} \leq Ck\bigg{(}\frac{\tau}{n}+\frac{1}{\Delta}\sqrt{\frac{\tau}{n} }+\frac{d\sqrt{\tau}}{n\Delta}\bigg{)}\] (21)

_with probability at least \(1-n^{-C^{\prime}}\). As a result, Conditions 1-3 hold for some \(\delta=o(1)\)._

Proof.: Under the conditions of Theorem 2.2, the inequalities (33)-(38) hold with probability at least \(1-n^{-C^{\prime}}\). In the remaining proof, we will work on the event these inequalities hold. Denote\(\hat{\Sigma}_{a}(z)=\frac{\sum_{j\in[n]}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}-\hat{ \theta}_{a}(z))^{T}\mathbb{I}\{z_{j}=a\}}{\sum_{j\in[n]}\mathbb{I}\{z_{j}=a\}}\) and \(\Sigma_{a}^{*}=\Sigma^{*}\) for any \(a\in[k]\). Then we have the equivalence

\[\hat{\Sigma}(z^{*})-\Sigma^{*}=\sum_{a=1}^{k}\frac{\sum_{j=1}^{n}\mathbb{I}\{z _{j}^{*}=a\}}{n}(\hat{\Sigma}_{a}(z^{*})-\Sigma_{a}^{*}).\]

Hence, we can use the results from Lemma C.7 and Lemma C.8.

By (43) and (44), we have

\[\|\hat{\Sigma}(z^{*})-\Sigma^{*}\|\preceq\sqrt{\frac{k(d+\log n)}{n}},\]

and

\[\|\hat{\Sigma}(z)-\hat{\Sigma}(z^{*})\| = \left\|\sum_{a=1}^{k}\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}{ n}\hat{\Sigma}_{a}(z)-\sum_{a=1}^{k}\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}{ n}\hat{\Sigma}_{a}(z^{*})\right\|\] \[\preceq \left\|\sum_{a=1}^{k}\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}} {n}(\hat{\Sigma}_{a}(z)-\hat{\Sigma}(z^{*}))\right\|+\left\|\sum_{a=1}^{k} \frac{\sum_{j=1}^{n}(\mathbb{I}\{z_{j}=a\}-\mathbb{I}\{z_{j}^{*}=a\})}{n}\hat {\Sigma}_{a}(z^{*})\right\|\] \[\preceq \frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*})+ \frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}+\frac{k}{n\Delta^{2}}\ell(z,z^{*})\] \[\preceq \frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*})+ \frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\]

By the assumption that \(kd=O(\sqrt{n})\), \(\frac{\Delta}{k}\to\infty\) and \(\tau=o(n/k)\), we have \(\|\hat{\Sigma}(z^{*})-\Sigma^{*}\|,\|\hat{\Sigma}(z)-\hat{\Sigma}(z^{*})\|=o(1)\), which implies \(\|(\hat{\Sigma}(z^{*}))^{-1}\|,\|(\hat{\Sigma}(z))^{-1}\|\preceq 1\). Thus, we have

\[\|(\hat{\Sigma}(z^{*}))^{-1}-(\Sigma^{*})^{-1}\|\leq\|(\hat{\Sigma}(z^{*}))^{- 1}\|\|\hat{\Sigma}(z^{*})-\Sigma^{*}\|\|(\Sigma^{*})^{-1}\|\preceq\sqrt{\frac{ k(d+\log n)}{n}},\] (22)

and similarly

\[\|(\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{*}))^{-1}\|\preceq\frac{k}{n}\ell(z, z^{*})+\frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\] (23)

Now we start to prove (19)-(21). Let \(F_{j}(a,b,z)=F_{j}^{(1)}(a,b,z)+F_{j}^{(2)}(a,b,z)+F_{j}^{(3)}(a,b,z)\) where

\[F_{j}^{(1)}(a,b,z) :=\langle\epsilon_{j},(\hat{\Sigma}(z))^{-1}(\hat{\theta}_{b}(z)- \hat{\theta}_{b}(z^{*}))\rangle-\langle\epsilon_{j},(\hat{\Sigma}(z))^{-1}( \hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}))\rangle,\] \[F_{j}^{(2)}(a,b,z) :=-\langle\epsilon_{j},((\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{* }))^{-1})(\theta_{a}^{*}-\theta_{b}^{*})\rangle,\] \[F_{j}^{(3)}(a,b,z) :=-\langle\epsilon_{j},((\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{* }))^{-1})(\theta_{b}^{*}-\hat{\theta}_{b}(z^{*}))\rangle+\langle\epsilon_{j}, ((\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{*}))^{-1})(\theta_{a}^{*}-\hat{ \theta}_{a}(z^{*}))\rangle.\]

Notice that

\[\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{*}\}}\frac{F_{j}^{(2)}(z ^{*}_{j},b,z)^{2}\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}}{\langle\theta_{z ^{*}_{j}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z^{*}_{j}}^{*}-\theta_{ b}^{*})\rangle^{2}\ell(z,z^{*})}\] \[\preceq \sum_{j=1}^{n}\sum_{b=1}^{k}\frac{\left|\langle\epsilon_{j},(( \hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{*}))^{-1})(\theta_{z^{*}_{j}}^{*}-\theta_ {b}^{*})\rangle\right|^{2}}{\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}\ell(z, z^{*})}\] \[\leq \sum_{b=1}^{k}\sum_{a\in[k]\setminus\{b\}}\sum_{j=1}^{n}\mathbb{ I}\{z_{j}^{*}=a\}\frac{\left|\langle\epsilon_{j},((\hat{\Sigma}(z))^{-1}-(\hat{\Sigma}(z^{* }))^{-1})(\theta_{a}^{*}-\theta_{b}^{*})\rangle\right|^{2}}{\|\theta_{a}^{*}- \theta_{b}^{*}\|^{2}\ell(z,z^{*})}\] \[\leq \sum_{b=1}^{k}\sum_{a\in[k]\setminus\{b\}}\frac{\|((\hat{\Sigma}(z) )^{-1}-(\hat{\Sigma}(z^{*}))^{-1})(\theta_{a}^{*}-\theta_{b}^{*})\|^{2}}{\| \theta_{a}^{*}-\theta_{b}^{*}\|^{2}\ell(z,z^{*})}\bigg{\|}\sum_{j=1}^{n} \mathbb{I}\{z_{j}^{*}=a\}\epsilon_{j}\epsilon_{j}^{T}\bigg{\|}\] \[\preceq k^{3}(\frac{\tau}{n}+\frac{1}{\Delta^{2}}+\frac{d^{2}}{n\Delta^{2}}),\]where we use (34), (23), and the fact that \(\ell(z,z^{*})\leq\tau\) and \(kd=O(\sqrt{n})\) for the last inequality. Here the second to last inequality is due to the following argument: for any \(w\in\mathbb{R}^{d}\), we have \(\sum_{j}|\langle\epsilon_{j},w\rangle|^{2}=\sum_{j}w^{T}\epsilon_{j}\epsilon_{ j}^{T}w=w^{T}(\sum_{j}\epsilon_{j}\epsilon_{j}^{T})w\leq\|w\|^{2}\|\sum_{j} \epsilon_{j}\epsilon_{j}^{T}\|\). From (41) we have \(\max_{a\in[k]}\|\theta_{a}^{*}-\hat{\theta}_{a}(z^{*})\|=o(1)\) under the assumption \(kd=O(\sqrt{n})\). By the similar analysis as in \(F_{j}^{(2)}(a,b,z)\), we have

\[\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{*}_{j}\}}\frac{F_{j}^{(3)}(z^{*}_{j}, b,z)^{2}\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}}{\langle\theta_{z^{*}_{j}}^{*}- \theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}) \rangle^{2}\ell(z,z^{*})}\preceq k^{3}(\frac{\tau}{n}+\frac{1}{\Delta^{2}}+ \frac{d^{2}}{n\Delta^{2}}).\]

Similarly, we have

\[\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{*}_{j}\}}\frac{F_{j}^{( 1)}(z^{*}_{j},b,z)^{2}\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}}{\langle \theta_{z^{*}_{j}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z^{*}_{j}}^{* }-\theta_{b}^{*})\rangle^{2}\ell(z,z^{*})}\] \[\preceq \sum_{b=1}^{k}\sum_{a\in[k]\setminus\{b\}}\frac{\|(\hat{\Sigma} (z))^{-1}(\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}))\|^{2}\|}{\|\theta_{a}^ {*}-\theta_{b}^{*}\|^{2}\ell(z,z^{*})}\bigg{\|}\sum_{j=1}^{n}\mathbb{I}\{z^{*} _{j}=a\}\epsilon_{j}\epsilon_{j}^{T}\bigg{\|}\] \[\preceq \frac{k^{3}}{\Delta^{4}},\]

where we use (42) and the fact that \((\hat{\Sigma}(z))^{-1}\) has bounded operator norm. Combining these terms together, we obtain (20).

Next, for (19), by (41) we have

\[|-\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z^ {*}))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle+\langle\theta_{a}^{ *}-\theta_{b}^{*},(\hat{\Sigma}(z^{*}))^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle|\] \[\leq |\langle\theta_{b}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z^ {*}))^{-1}(\theta_{b}^{*}-\hat{\theta}_{b}(z^{*}))\rangle|+2|\langle\theta_{b }^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z^{*}))^{-1}(\theta_{a}^{*}- \theta_{b}^{*})\rangle|\] \[\preceq \frac{k(d+\log n)}{n}+\sqrt{\frac{k(d+\log n)}{n}}\|\theta_{a}^ {*}-\theta_{b}^{*}\|,\]

and

\[|\langle\theta_{a}^{*}-\hat{\theta}_{a}(z^{*}),(\hat{\Sigma}(z^{*}))^{-1}( \theta_{a}^{*}-\hat{\theta}_{a}(z^{*}))\rangle|\preceq\frac{k(d+\log n)}{n}.\]

By (22) we have

\[|-\langle\theta_{a}^{*}-\theta_{b}^{*},(\hat{\Sigma}(z^{*}))^{-1}(\theta_{a}^ {*}-\theta_{b}^{*})\rangle+\langle\theta_{a}^{*}-\theta_{b}^{*},(\Sigma^{*})^{ -1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle|\preceq\sqrt{\frac{k(d+\log n)}{n} }\|\theta_{a}^{*}-\theta_{b}^{*}\|^{2}.\]

Using the results above we can get (19).

Finally we are going to establish (21). Recall the definition of \(G_{j}(a,b,z)\) which has four terms. For the third and fourth terms, we have

\[|-\langle\theta_{a}^{*}-\hat{\theta}_{b}(z),(\hat{\Sigma}(z))^{-1}(\theta_{a}^ {*}-\hat{\theta}_{b}(z))\rangle+\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}), (\hat{\Sigma}(z))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle|\]

\[\preceq \|\hat{\theta}_{b}(z)-\hat{\theta}_{b}(z^{*})\|^{2}+\|\hat{\theta}_{b}(z)- \hat{\theta}_{b}(z^{*})\|\|\theta_{a}^{*}-\theta_{b}^{*}\|,\]

and

\[|-\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}(z))^{-1}(\theta_{ a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle+\langle\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}),( \hat{\Sigma}(z^{*}))^{-1}(\theta_{a}^{*}-\hat{\theta}_{b}(z^{*}))\rangle|\]

\[\preceq \|\theta_{a}^{*}-\theta_{b}^{*}\|^{2}\|(\hat{\Sigma}(z))^{-1}-(\hat{ \Sigma}(z^{*}))^{-1}\|.\]

We can easily verify that the other two terms are smaller than the above two terms. Then, by using (42) and (23), we have

\[\frac{|G_{j}(z^{*}_{j},b,z)|}{\langle\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*},( \Sigma^{*})^{-1}(\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*})\rangle}\] \[\preceq \frac{\|\hat{\theta}_{b}(z)-\hat{\theta}_{b}(z^{*})\|^{2}+\|\hat{ \theta}_{b}(z)-\hat{\theta}_{b}(z^{*})\|\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*} \|+\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}\|(\hat{\Sigma}(z))^{-1}-(\hat{ \Sigma}(z^{*}))^{-1}\|}{\|\theta_{z^{*}_{j}}^{*}-\theta_{b}^{*}\|^{2}}\] \[\preceq \frac{k\tau}{n}+\frac{k}{\Delta}\sqrt{\frac{\tau}{n}}+\frac{kd\sqrt {\tau}}{n\Delta}.\]

**Lemma A.5**.: _With the same conditions as in Theorem 2.2, for any \(\delta=o(1)\), we have_

\[\xi_{\text{ideal}}(\delta)\leq n\exp\!\left(-(1+o(1))\frac{\text{SNR}^{2}}{8} \right).\]

_with probability at least \(1-n^{-C^{\prime}}-\exp(-\text{SNR})\)._

Proof.: Under the conditions of Theorem 2.2, the inequalities (33)-(38) hold with probability at least \(1-n^{-C^{\prime}}\). In the remaining proof, we will work on the event these inequalities hold. Recall the definition of \(\xi_{\text{ideal}}\). We can write

\[\xi_{\text{ideal}}(\delta) =\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{\langle\epsilon_{j},(\hat{ \Sigma}(z^{*}))^{-1}(\hat{\theta}_{z_{j}^{*}}(z^{*})-\hat{\theta}_{b}(z^{*})) \rangle\leq-\frac{1-\delta}{2}\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},( \Sigma^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\right\}\] \[\leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{\langle\epsilon_{j},(\Sigma ^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\leq-\frac{1-\delta- \bar{\delta}}{2}\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1} (\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\right\}\] \[+\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{\langle\epsilon_{j},((\hat{ \Sigma}(z^{*}))^{-1}-(\Sigma^{*})^{-1})(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}) \rangle\leq-\frac{\bar{\delta}}{6}\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\right\}\] \[+\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{\langle\epsilon_{j},(\hat{ \Sigma}(z^{*}))^{-1}(\hat{\theta}_{z_{j}^{*}}(z^{*})-\theta_{z_{j}^{*}}^{*}) \rangle\leq-\frac{\bar{\delta}}{6}\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\right\}\] \[+\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{-\langle\epsilon_{j},(\hat{ \Sigma}(z^{*}))^{-1}(\hat{\theta}_{b}(z^{*})-\theta_{b}^{*})\rangle\leq-\frac {\bar{\delta}}{6}\langle\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1} (\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle\right\}\] \[=M_{1}+M_{2}+M_{3}+M_{4}.\]

where \(\bar{\delta}=\bar{\delta}_{n}\) is some sequence to be chosen later. We bound the four terms sequentially. Suppose \(\epsilon_{j}=(\Sigma^{*})^{1/2}w_{j}\), where \(w_{j}\overset{ind}{\sim}\mathcal{N}(0,I_{d})\). By (22), we know

\[M_{2} \leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{\frac{\bar{\delta}}{6\lambda _{\max}}\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\leq\lambda_{\max}\|w_{j} \|\|(\hat{\Sigma}(z^{*}))^{-1}-(\Sigma^{*})^{-1}\|\|\theta_{z_{j}^{*}}^{*}- \theta_{b}^{*}\|\right\}\] \[\leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{C\bar{\delta}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|\sqrt{\frac{n}{d+\log n}}\leq\|w_{j}\|\right\}\] \[\leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\mathbb{I}\left\{C\bar{\delta}^{2}\|\theta_{z_ {j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\frac{n}{d+\log n}-2d\leq\|w_{j}\|^{2}-2d \right\},\]

where \(C\) is a constant which may vary from line by line. Recall that \(kd=O(\sqrt{n})\), \(\min_{a\neq b}\|\theta_{a}^{*}-\theta_{b}^{*}\|\rightarrow\infty\), and \(\Delta/k\rightarrow\infty\) by assumption. Let \(n^{-\frac{1}{4}}=o(\bar{\delta})\). Using the \(\chi^{2}\) tail probability in Lemma C.1, we have for any \(a\neq b\in[k]\),

\[\mathbb{E}M_{2}\leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}\exp\!\left(-C\bar{\delta}^{2}\|\theta_{z_{j }^{*}}^{*}-\theta_{b}^{*}\|^{2}\sqrt{n}\right)\leq n\exp\!\left(-(1+o(1)) \frac{\text{SNR}^{2}}{8}\right).\]

We can obtain similar bounds on \(M_{3}\) and \(M_{4}\) by using (41). For \(M_{1}\), the Gaussian tail bound leads to the inequality

\[\mathbb{P}\!\left\{\langle\epsilon_{j},(\Sigma^{*})^{-1}(\theta_{a }^{*}-\theta_{b}^{*})\rangle\leq-\frac{1-\delta-\bar{\delta}}{2}\langle\theta_{a}^{*} -\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\right\}\] \[= \mathbb{P}\!\left\{\langle w_{j},(\Sigma^{*})^{-1/2}(\theta_{a }^{*}-\theta_{b}^{*})\rangle\leq-\frac{1-\delta-\bar{\delta}}{2}\langle\theta_{a}^{* }-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle\right\}\] \[\leq \exp\!\left(-\frac{(1-\delta-\bar{\delta})^{2}}{8}\langle\theta_{a }^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_{b}^{*})\rangle \right).\]Thus,

\[\mathbb{E}M_{1} \leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\|\theta_{z_{j} ^{*}}^{*}-\theta_{b}^{*}\|^{2}\exp\biggl{(}-\frac{(1-\delta-\bar{\delta})^{2}}{8 }\langle\theta_{a}^{*}-\theta_{b}^{*},(\Sigma^{*})^{-1}(\theta_{a}^{*}-\theta_ {b}^{*})\rangle\biggr{)}\] \[\leq n\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8}\biggr{)}\,.\]

Overall, we have \(\mathbb{E}\xi_{\text{ideal}}\preceq n\exp\Bigl{(}-(1+o(1))\frac{\text{SNR}^{2 }}{8}\Bigr{)}\). By the Markov's inequality, we have

\[\mathbb{P}(\xi_{\text{ideal}}(\delta_{n})\geq\mathbb{E}\xi_{\text{ideal}}\exp (\text{SNR}))\leq\exp(-\text{SNR}).\]

In other words, with probability at least \(1-\exp(-\text{SNR})\), we have

\[\xi_{\text{ideal}}(\delta_{n})\leq\mathbb{E}\xi_{\text{ideal}}(\delta_{n}) \exp(\text{SNR})\leq n\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8}\biggr{)}\,.\]

Proof of Theorem 2.2.: By Lemmas A.3 - A.5, we have that Conditions 1 - 5 are satisfied with probability at least \(1-\eta-n^{-1}-\exp(-\text{SNR})\). Then applying Lemma A.3, we have

\[\ell(z^{(t)},z^{*})\leq n\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8} \biggr{)}+\frac{1}{2}\ell(z^{(t-1)},z^{*}),\quad\text{for all }t\geq 1.\]

By (16), and since there exists a constant \(C\) such that \(\Delta\leq C\text{SNR}\), we can conclude

\[h(z^{(t)},z^{*})\leq\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8}\biggr{)}+2 ^{-t},\quad\text{for all }t\geq 1.\]

Notice that \(h(\cdot,\cdot)\) takes value in the set \(\{j/n:j\in[n]\cup\{0\}\}\), the term \(2^{-t}\) in the above inequality should be negligible as long as \(2^{-t}=o(n^{-1})\). Thus, we can claim

\[h(z^{(t)},z^{*})\leq\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{2}}{8}\biggr{)}\,,\quad\text{for all }t\geq\log n.\]

## Appendix B Proofs in Section 3

### Proofs for the Lower Bound

Proof of Lemma 3.1.: The Neyman-Pearson lemma tells us the likelihood ratio test \(\phi\) is the optimal procedure. Following the proof of Lemma A.1, we have

\[\mathbb{P}_{\mathrm{H}_{0}}(\phi=1)+\mathbb{P}_{\mathrm{H}_{1}}( \phi=0) =\mathbb{P}(\epsilon\in B_{1,2})+\mathbb{P}(\epsilon\in B_{2,1})\] \[\geq\exp\biggl{(}-\frac{1+o(1)}{8}\text{SNR}^{{}^{\prime}2}_{1,2} \biggr{)}+\exp\biggl{(}-\frac{1+o(1)}{8}\text{SNR}^{{}^{\prime}2}_{2,1}\biggr{)}\,,\]

where the last inequality is by Lemma C.10. 

Proof of Theorem 3.1.: The proof is identical to the proof of Theorem 2.1 and is omitted here. 

### Proofs for the Upper Bound

We adopt a similar proof idea as in Section 2 for Model 1. We first present an error decomposition for the one-step analysis for Algorithm 2. In Lemma B.1, we show the loss decays after a one-step iteration under Conditions 6 - 11. Then in Lemma B.2 we extend the result to multiple iterations, under two extra Conditions 12 - 13. Finally, we show that all the conditions are satisfied with high probability and thus prove Theorem 3.2.

In the statement of Theorem 3.2, we assume \(\max_{a,b\in[k]}\lambda_{d}(\Sigma^{*}_{a})/\lambda_{1}(\Sigma^{*}_{b})=O(1)\) for the covariance matrices \(\{\Sigma^{*}_{a}\}_{a\in[k]}\). Without loss of generality, we can replace it by assuming \(\{\Sigma^{*}_{a}\}_{a\in[k]}\) satisfy

\[\lambda_{\min}\leq\min_{a\in[k]}\lambda_{1}(\Sigma^{*}_{a})\leq\max_{a\in[k]} \lambda_{d}(\Sigma^{*}_{a})\leq\lambda_{\max}\] (24)

where \(\lambda_{\min},\lambda_{\max}>0\) are two constants. This is due to the scaling properties of the normal distributions. The reasoning is the same as that in (15) for Model 1 and is omitted here. For the remainder of this section, we will assume that (24) holds for the covariance matrices.

Error Decomposition for the One-step Analysis:Consider an arbitrary \(z\in[k]^{n}\). Apply (12), (13), and (14) on \(z\) to obtain \(\{\hat{\theta}_{a}(z)\}_{a\in[k]}\), \(\{\hat{\Sigma}_{a}(z)\}_{a\in[k]}\), and \(\hat{z}(z)\):

\[\hat{\theta}_{a}(z) =\frac{\sum_{j\in[n]}Y_{j}\mathbb{I}\left\{z_{j}=a\right\}}{\sum _{j\in[n]}\mathbb{I}\left\{z_{j}=a\right\}},\] \[\hat{\Sigma}_{a}(z) =\frac{\sum_{j\in[n]}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}-\hat{ \theta}_{a}(z))^{T}\mathbb{I}\left\{z_{j}=a\right\}}{\sum_{j\in[n]}\mathbb{I} \left\{z_{j}=a\right\}},\quad\forall a\in[k],\] \[\hat{z}_{j}(t) =\operatorname*{argmin}_{a\in[k]}(Y_{j}-\hat{\theta}_{a}(z))^{T} (\hat{\Sigma}_{a}(a))^{-1}(Y_{j}-\hat{\theta}_{a}(z))+\log|\hat{\Sigma}_{a}(z )|,\quad\forall j\in[n].\]

For simplicity, we denote \(\hat{z}\) as shorthand for \(\hat{z}(z)\). Let \(j\in[n]\) be an arbitrary index with \(z^{*}_{j}=a\). According to (14), \(z^{*}_{j}\) will be incorrectly estimated after one iteration in \(\hat{z}\) if \(a\neq\operatorname*{argmin}_{b\in[k]}(Y_{j}-\hat{\theta}_{b}(z))^{T}(\hat{ \Sigma}_{b}(z))^{-1}(Y_{j}-\hat{\theta}_{b}(z))+\log|\hat{\Sigma}_{b}(z)|\). That is, it is important to analyze the event

\[\langle Y_{j}-\hat{\theta}_{b}(z),(\hat{\Sigma}_{b}(z))^{-1}(Y_{j}-\hat{ \theta}_{b}(z))\rangle+\log|\hat{\Sigma}_{b}(z)|\leq\langle Y_{j}-\hat{\theta }_{a}(z),(\hat{\Sigma}_{a}(z))^{-1}(Y_{j}-\hat{\theta}_{a}(z))\rangle+\log| \hat{\Sigma}_{a}(z)|,\] (25)

for any \(b\in[k]\setminus\{a\}\). After some rearrangements, we can see (25) is equivalent to

\[\langle\epsilon_{j},(\hat{\Sigma}_{b}(z^{*}))^{-1}(\theta^{*}_{a} -\hat{\theta}_{b}(z^{*}))\rangle-\langle\epsilon_{j},(\hat{\Sigma}_{a}(z^{*}) )^{-1}(\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle\] \[+\frac{1}{2}\langle\epsilon_{j},((\hat{\Sigma}_{b}(z^{*}))^{-1}-( \hat{\Sigma}_{a}(z^{*}))^{-1})\epsilon_{j}\rangle-\frac{1}{2}\log|\Sigma^{*}_{ a}|+\frac{1}{2}\log|\Sigma^{*}_{b}|\] \[\leq -\frac{1}{2}\langle\theta^{*}_{a}-\theta^{*}_{b},(\Sigma^{*}_{b}) ^{-1}(\theta^{*}_{a}-\theta^{*}_{b})\rangle\] \[+F_{j}(a,b,z)+Q_{j}(a,b,z)+G_{j}(a,b,z)+H_{j}(a,b,z)+K_{j}(a,b,z)+L _{j}(a,b,z),\]

where

\[F_{j}(a,b,z) =\langle\epsilon_{j},(\hat{\Sigma}_{b}(z))^{-1}(\hat{\theta}_{b}( z)-\hat{\theta}_{b}(z^{*}))\rangle-\langle\epsilon_{j},(\hat{\Sigma}_{a}(z))^{-1}( \hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}))\rangle\] \[-\langle\epsilon_{j},((\hat{\Sigma}_{b}(z))^{-1}-(\hat{\Sigma}_{b }(z^{*}))^{-1})(\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle\] \[+\langle\epsilon_{j},((\hat{\Sigma}_{a}(z))^{-1}-(\hat{\Sigma}_{a }(z^{*}))^{-1})(\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle,\]

\[Q_{j}(a,b,z) =-\frac{1}{2}\langle\epsilon_{j},((\hat{\Sigma}_{b}(z))^{-1}-( \hat{\Sigma}_{b}(z^{*}))^{-1})\epsilon_{j}\rangle+\frac{1}{2}\langle\epsilon_{ j},((\hat{\Sigma}_{a}(z))^{-1}-(\hat{\Sigma}_{a}(z^{*}))^{-1})\epsilon_{j}\rangle,\]

\[G_{j}(a,b,z) =\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{a}(z),(\hat{ \Sigma}_{a}(z))^{-1}(\theta^{*}_{a}-\hat{\theta}_{a}(z))\rangle-\frac{1}{2} \langle\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}),(\hat{\Sigma}_{a}(z))^{-1}( \theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle\] \[+\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}),(\hat{ \Sigma}_{a}(z))^{-1}(\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle-\frac{1}{2} \langle\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}),(\hat{\Sigma}_{a}(z^{*}))^{-1}( \theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle\] \[-\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{b}(z),(\hat{ \Sigma}_{b}(z))^{-1}(\theta^{*}_{a}-\hat{\theta}_{b}(z))\rangle+\frac{1}{2} \langle\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}_{b}(z))^{-1}( \theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle\] \[-\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}),(\hat{ \Sigma}_{b}(z))^{-1}(\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle+\frac{1}{2} \langle\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}),(\hat{\Sigma}_{b}(z))^{-1}( \theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle,\]

\[H_{j}(a,b,z)= -\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}),(\hat{ \Sigma}_{b}(z^{*}))^{-1}(\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle+\frac{1}{2} \langle\theta^{*}_{a}-\theta^{*}_{b},(\hat{\Sigma}_{b}(z^{*}))^{-1}(\theta^{*}_{a }-\theta^{*}_{b})\rangle\] \[-\frac{1}{2}\langle\theta^{*}_{a}-\theta^{*}_{b},(\hat{\Sigma}_{b }(z^{*}))^{-1}(\theta^{*}_{a}-\theta^{*}_{b})\rangle+\frac{1}{2}\langle\theta^{*}_{a }-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{a}-\theta^{*}_{b})\rangle\] \[+\frac{1}{2}\langle\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}),(\hat{ \Sigma}_{a}(z^{*}))^{-1}(\theta^{*}_{a}-\hat{\theta}_{a}(z^{*}))\rangle,\]\[K_{j}(a,b,z)=\frac{1}{2}(\log|\hat{\Sigma}_{a}(z)|-\log|\hat{\Sigma}_{a}(z^{*})|)- \frac{1}{2}(\log|\hat{\Sigma}_{b}(z)|-\log|\hat{\Sigma}_{b}(z^{*})|),\]

\[L_{j}(a,b,z)=\frac{1}{2}(\log|\hat{\Sigma}_{a}(z^{*})|-\log|\Sigma_{a}^{*}|)- \frac{1}{2}(\log|\hat{\Sigma}_{b}(z^{*})|-\log|\Sigma_{b}^{*}|).\]

Among these terms, \(F_{j},G_{j},H_{j}\) are nearly identical to their counterparts in Section A.2.2 with \(\hat{\Sigma}(z)\) replaced by \(\hat{\Sigma}_{a}(z)\) or \(\hat{\Sigma}_{b}(z)\). There are three extra terms not appearing in Section A.2.2: \(Q_{j}\) is a quadratic term of \(\epsilon_{j}\) and \(K_{j},L_{j}\) are terms involving matrix determinants.

Conditions and Guarantees for One-step Analysis.To establish the guarantee for the one-step analysis, we first give several conditions on the error terms.

**Condition 6**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{|H_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{* }_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle} \leq\frac{\delta}{12}\]

_holds with probability at least \(1-\eta_{1}\) for some \(\tau,\delta,\eta_{1}>0\)._

**Condition 7**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{F_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b} \|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})}\leq\frac{ \delta^{2}}{288}\]

_holds with probability at least \(1-\eta_{2}\) for some \(\tau,\delta,\eta_{2}>0\)._

**Condition 8**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{|G_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{* }_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle} \leq\frac{\delta}{12}\]

_holds with probability at least \(1-\eta_{3}\) for some \(\tau,\delta,\eta_{3}>0\)._

**Condition 9**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{Q_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b} \|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})}\leq\frac{ \delta^{2}}{288}\]

_holds with probability at least \(1-\eta_{4}\) for some \(\tau,\delta,\eta_{4}>0\)._

**Condition 10**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{K_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b} \|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}( \theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})}\leq\frac{ \delta^{2}}{288}\]

_holds with probability at least \(1-\eta_{5}\) for some \(\tau,\delta,\eta_{5}>0\)._

**Condition 11**.: _Assume that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k]\setminus\{z^{* }_{j}\}}\frac{|L_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*} _{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle} \leq\frac{\delta}{12}\]

_holds with probability at least \(1-\eta_{6}\) for some \(\tau,\delta,\eta_{6}>0\)._

We next define a quantity referred to as the ideal error,

\[\xi_{\text{ideal}}(\delta)=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z^{*}_{j}\}} \|\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b}\|^{2}\mathbb{I}\{\langle\epsilon_{j},( \hat{\Sigma}_{b}(z^{*}))^{-1}(\theta^{*}_{a}-\hat{\theta}_{b}(z^{*}))\rangle- \langle\epsilon_{j},(\hat{\Sigma}_{a}(z^{*}))^{-1}(\theta^{*}_{a}-\hat{\theta}_ {a}(z^{*}))\rangle\]

\[+\frac{1}{2}\langle\epsilon_{j},((\hat{\Sigma}_{b}(z^{*}))^{-1}-(\hat{\Sigma}_{a }(z^{*}))^{-1})\epsilon_{j}\rangle-\frac{1}{2}\log|\Sigma^{*}_{a}|+\frac{1}{2} \log|\Sigma^{*}_{b}|\leq-\frac{1-\delta}{2}\langle\theta^{*}_{a}-\theta^{*}_{b},( \Sigma^{*}_{b})^{-1}(\theta^{*}_{a}-\theta^{*}_{b})\rangle\}.\]

**Lemma B.1**.: _Assumes Conditions 6 - 11 hold for some \(\tau,\delta,\eta_{1},\ldots,\eta_{6}>0\). We then have_

\[\mathbb{P}\bigg{(}\ell(\hat{z},z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{2} \ell(z,z^{*})\text{ for any }z\in[k]^{n}\text{ such that }\ell(z,z^{*})\leq\tau\bigg{)}\geq 1-\eta,\]

_where \(\eta=\sum_{i=1}^{6}\eta_{i}\)._

Proof.: The proof of this lemma is quite similar to the proof of Lemma A.2. The additional terms \(Q_{j}\) and \(K_{j}\) can be handled in the same way as \(F_{j}\) while \(L_{j}\) can be handled similarly to \(H_{j}\). We omit the details here.

Conditions and Guarantees for Multiple Iterations.In the above, we establish a statistical guarantee for the one-step analysis. Now we will extend the result to multiple iterations. That is, starting from some initialization \(z^{(0)}\), we will characterize how the losses \(\ell(z^{(0)},z^{*})\), \(\ell(z^{(1)},z^{*})\), \(\ell(z^{(2)},z^{*})\),..., decay. We impose conditions on \(\xi_{\text{ideal}}(\delta)\) and the initialization \(z^{(0)}\).

**Condition 12**.: _Assume that_

\[\xi_{\text{ideal}}(\delta)\leq\frac{\tau}{2}\]

_holds with probability at least \(1-\eta_{7}\) for some \(\tau,\delta,\eta_{7}>0\)._

Finally, we need a condition on the initialization.

**Condition 13**.: _Assume that_

\[\ell(z^{(0)},z^{*})\leq\tau\]

_holds with probability at least \(1-\eta_{8}\) for some \(\tau,\eta_{8}>0\)._

With these conditions satisfied, we can give a lemma that shows the convergence of our algorithm.

**Lemma B.2**.: _Assumes Conditions 6 - 13 hold for some \(\tau,\delta,\eta_{1},\dots,\eta_{8}>0\). We then have_

\[\ell(z^{(t)},z^{*})\leq\xi_{\text{ideal}}(\delta)+\frac{1}{2}\ell(z^{(t-1)},z ^{*})\]

_for all \(t\geq 1\), with probability at least \(1-\eta\), where \(\eta=\sum_{i=1}^{8}\eta_{i}\)._

Proof.: The proof of this lemma is the same as the proof of Lemma A.3. 

With-high-probability Results for the Conditions and Proof of Theorem 3.2.Lemma B.3 and Lemma B.4 are the counterparts of Lemmas A.4 and A.5 in Appendix A.2. Recall that (24) is assumed. By Lemma C.10, we have \(\Delta\) is of the same order as \(\text{SNR}^{\prime}\), which will play a similar role as (18) in Section A.2.2.

Lemma B.3 and Lemma B.4 are counterparts of Lemmas A.4 and A.5 in Section A.2.2. The first lemma is for Conditions 6-11, providing upper bounds for the quantities involved in these conditions, showing that \(\delta\) can be taken as some \(o(1)\) term. The second lemma shows that for any \(\delta=o(1)\), \(\xi_{\text{ideal}}(\delta)\) is upper bounded by the desired minimax rate multiplied by the sample size \(n\).

**Lemma B.3**.: _Under the same conditions as in Theorem 3.2, for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(\alpha,C^{\prime},\lambda_{\min},\lambda_{\max}\) such that_

\[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k] \setminus\{z^{*}\}}\frac{|H_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j} }-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b}) \rangle} \leq C\sqrt{\frac{k(d+\log n)}{n}}\] (26) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{F_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}} -\theta^{*}_{b}\|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^ {*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})} \leq Ck^{3}\bigg{(}\frac{\tau}{n}+\frac{1}{\Delta^{2}}+\frac{d^{2} }{n\Delta^{2}}\bigg{)}\] (27) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{|G_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*} _{j}}-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{ b})\rangle} \leq Ck\bigg{(}\frac{\tau}{n}+\frac{1}{\Delta}\sqrt{\frac{\tau}{n}}+\frac{d\sqrt{ \tau}}{n\Delta}\bigg{)}\] (28) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{Q_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}} -\theta^{*}_{b}\|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^ {*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})} \leq C\frac{k^{3}d^{2}}{\Delta^{2}}\bigg{(}\frac{\tau}{n}+\frac{1}{ \Delta^{2}}+\frac{d^{2}}{n\Delta^{2}}\bigg{)}\] (29) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\sum_{j=1}^{n}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{K_{j}(z^{*}_{j},b,z)^{2}\|\theta^{*}_{z^{*}_{j}} -\theta^{*}_{b}\|^{2}}{\langle\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b},(\Sigma^ {*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b})\rangle^{2}\ell(z,z^{*})} \leq C\frac{k^{3}d^{2}}{\Delta^{2}}\bigg{(}\frac{\tau}{n}+\frac{1}{ \Delta^{2}}+\frac{d^{2}}{n\Delta^{2}}\bigg{)}\] (30) \[\max_{\{z:\ell(z,z^{*})\leq\tau\}}\max_{j\in[n]}\max_{b\in[k] \setminus\{z^{*}_{j}\}}\frac{|L_{j}(z^{*}_{j},b,z)|}{\langle\theta^{*}_{z^{*}_{j} }-\theta^{*}_{b},(\Sigma^{*}_{b})^{-1}(\theta^{*}_{z^{*}_{j}}-\theta^{*}_{b}) \rangle} \leq C\frac{d}{\Delta^{2}}\sqrt{\frac{k(d+\log n)}{n}}\] (31)

_with probability at least \(1-n^{-C^{\prime}}-\frac{4}{nd}\). As a result, Conditions 6-11 hold for some \(\delta=o(1)\)._Proof.: Under the conditions of Theorem 3.2, the inequalities (33)-(38) hold with probability at least \(1-n^{-C^{\prime}}\). In the remaining proof, we will work on the event these inequalities hold. Hence, we can use the results from Lemma C.7 and C.8. Using the same arguments as in the proof of Lemma A.4, we can get (26), (27) and (28).

As for (29), we first use Lemma C.2 to have \(\sum_{j=1}^{n}\|\epsilon_{j}\|^{4}\leq 3nd\) with probability at least \(1-4/(nd)\). Then, we have

\[\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z_{j}^{*}\}}\frac{Q_{j}(z_{ j}^{*},b,z)^{2}\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}}{\langle \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*} }^{*}-\theta_{b}^{*})\rangle^{2}\ell(z,z^{*})} \preceq \sum_{j=1}^{n}\sum_{b=1}^{k}\frac{Q_{j}(z_{j}^{*},b,z)^{2}}{ \Delta^{2}\ell(z,z^{*})}\] \[\leq k\sum_{j=1}^{n}\|\epsilon_{j}\|^{4}\frac{\max_{a\in[k]}\|(\hat{ \Sigma}_{a}(z))^{-1}-(\hat{\Sigma}_{a}(z^{*}))^{-1}\|^{2}}{\Delta^{2}\ell(z,z ^{*})}\] \[\preceq \frac{k^{3}d^{2}}{\Delta^{2}}\bigg{(}\frac{\tau}{n}+\frac{1}{ \Delta^{2}}+\frac{d^{2}}{n\Delta^{2}}\bigg{)}\,,\]

where the last inequality is due to (53) and the fact that \(\ell(z,z^{*})\leq\tau\).

Next for (30), notice that by (43), (44), and \(\text{SNR}^{\prime}\rightarrow\infty\), we have for any \(1\leq i\leq d\), \(\frac{\lambda_{\min}}{2}\leq\lambda_{i}(\hat{\Sigma}_{a}(z^{*}))\leq 2\lambda_{ \max}\) and

\[\left|\log(1+\max_{a\in[k]}\frac{\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*}) \|_{2}}{\lambda_{i}(\hat{\Sigma}_{a}(z^{*}))})\right|\leq\left|\log(1-\max_{a \in[k]}\frac{\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\|_{2}}{\lambda_{i} (\hat{\Sigma}_{a}(z^{*}))})\right|.\]

Thus by Weyl's inequality, we know

\[\max_{a\in[k]}\left|\log|\hat{\Sigma}_{a}(z)|-\log|\hat{\Sigma}_ {a}(z^{*})|\right|\] \[= \max_{a\in[k]}\left|\log\frac{|\hat{\Sigma}_{a}(z)|}{|\hat{\Sigma }_{a}(z^{*})|}\right|\] \[\leq \left|\sum_{i=1}^{d}\log(1-\frac{\max_{a\in[k]}\|\hat{\Sigma}_{a} (z)-\hat{\Sigma}_{a}(z^{*})\|_{2}}{\lambda_{i}(\hat{\Sigma}_{a}(z^{*}))})\right|\] \[\preceq \sum_{i=1}^{d}\log\left(1+\max_{a\in[k]}\frac{\|\hat{\Sigma}_{a} (z)-\hat{\Sigma}_{a}(z^{*})\|_{2}}{\lambda_{i}(\hat{\Sigma}_{a}(z^{*}))}+ \frac{\max_{a\in[k]}\frac{\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\|_{2} ^{2}}{\lambda_{i}^{2}(\hat{\Sigma}_{a}(z^{*}))}}{1-\max_{a\in[k]}\frac{\|\hat {\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\|_{2}}{\lambda_{i}(\hat{\Sigma}_{a}(z^ {*}))}}\right)\] \[\preceq d\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\|_{2},\] (32)

where the last inequality is due to the fact that \(\lambda_{i}(\hat{\Sigma}_{a}(z^{*}))\) is at the constant rate, \(\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\|_{2}=o(1)\) and the inequality \(\log(1+x)\leq x\) for any \(x>0\). (32) yields to the inequality

\[\sum_{j=1}^{n}\max_{b\in[k]\setminus\{z_{j}^{*}\}}\frac{K_{j}(z_{ j}^{*},b,z)^{2}\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\|^{2}}{\langle\theta_{z_{j}^{*}}^{*} -\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}) \rangle^{2}\ell(z,z^{*})} \preceq \sum_{j=1}^{n}\frac{d^{2}\max_{a\in[k]}\|\hat{\Sigma}_{a}(z)- \hat{\Sigma}_{a}(z^{*})\|^{2}}{\Delta^{2}\ell(z,z^{*})}\] \[\preceq \frac{k^{2}d^{2}}{\Delta^{2}}\bigg{(}\frac{\tau}{n}+\frac{1}{ \Delta^{2}}+\frac{d^{2}}{n\Delta^{2}}\bigg{)}\,.\]

Finally for (31), by (43) and the similar argument as (32), we can get

\[\max_{a\in[k]}\left|\log|\hat{\Sigma}_{a}(z^{*})|-\log|\Sigma_{a}^{*}|\right| \preceq d\sqrt{\frac{k(d+\log n)}{n}}\]

which implies (31). We complete the proof. 

**Lemma B.4**.: _With the same conditions as Theorem 3.2, for any sequence \(\delta_{n}=o(1)\), we have_

\[\xi_{\text{ideal}}(\delta_{n})\leq n\exp\biggl{(}-(1+o(1))\frac{\text{SNR}^{ \prime 2}}{8}\biggr{)}\,.\]

_with probability at least \(1-n^{-C^{\prime}}-\exp(-\text{SNR}^{\prime})\)._Proof.: Under the conditions of Theorem 3.2, the inequalities (33)-(38) hold with probability at least \(1-n^{-C^{\prime}}\). In the remaining proof, we will work on the event these inequalities hold. Similar to the proof of Lemma A.5, we have a decomposition \(\xi_{\text{ideal}}\leq\sum_{i=1}^{6}M_{i}\) where

\[M_{1}=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\bigg{\{} \langle\epsilon_{j},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{ *})\rangle+\frac{1}{2}\langle\epsilon_{j},((\Sigma_{b}^{*})^{-1}-(\Sigma_{z_{j }^{*}}^{*})^{-1})\epsilon_{j}\rangle\] \[-\frac{1}{2}\log|\Sigma_{z_{j}^{*}}|+\frac{1}{2}\log|\Sigma_{b}^{ *}|\leq-\frac{1-\delta-\bar{\delta}}{2}\langle\theta_{z_{j}^{*}}^{*}-\theta_{ b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle \bigg{\}}\]

is the main term and

\[M_{2}=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\bigg{\{}\langle \epsilon_{j},((\hat{\Sigma}_{b}(z^{*}))^{-1}-(\Sigma_{b}^{*})^{-1})(\theta_{z _{j}^{*}}^{*}-\theta_{b}^{*})\rangle\leq-\frac{\bar{\delta}}{10}\langle\theta_ {z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}- \theta_{b}^{*})\rangle\bigg{\}}\]

\[M_{3}=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\bigg{\{}-\langle \epsilon_{j},(\hat{\Sigma}_{z_{j}^{*}}(z^{*}))^{-1}(\theta_{z_{j}^{*}}^{*}- \hat{\theta}_{z_{j}^{*}}(z^{*}))\rangle\leq-\frac{\bar{\delta}}{10}\langle \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}} ^{*}-\theta_{b}^{*})\rangle\bigg{\}}\]

\[M_{4}=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\left\{-\langle \epsilon_{j},(\hat{\Sigma}_{b}(z^{*}))^{-1}(\hat{\theta}_{b}(z^{*})-\theta_{b}^ {*})\rangle\leq-\frac{\bar{\delta}}{10}\langle\theta_{z_{j}^{*}}^{*}-\theta_{ b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}) \rangle\right\}\]

\[M_{5}=\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\left\{\frac{1}{2} \langle\epsilon_{j},((\hat{\Sigma}_{b}(z^{*}))^{-1}-(\Sigma_{b}^{*})^{-1}) \epsilon_{j}\rangle\leq-\frac{\bar{\delta}}{10}\langle\theta_{z_{j}^{*}}^{*}- \theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}) \rangle\right\}.\]

Using the same arguments as the proof of Lemma A.5, we can choose some \(\bar{\delta}=\bar{\delta}_{n}=o(1)\) which is slowly diverging to zero satisfying

\[\mathbb{E}M_{i}\leq n\exp\!\left(-(1+o(1))\frac{\text{SNR}^{{}^{ \prime}2}}{2}\right)\quad\text{for }i=2,3,4.\]

As for \(M_{5}\), by (43) we have

\[M_{5}\leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{I}\left\{C\bar{\delta }\left\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\leq\|w_{j}\|^{2} \sqrt{\frac{\log n}{n}}\right\},\]

where \(C\) is a constant and \(w_{j}\overset{iid}{\sim}\mathcal{N}(0,I_{d})\). Since there exists some constant \(C^{\prime}\) such that \(\text{SNR}^{\prime}\leq C^{\prime}\Delta\), we can choose appropriate \(\bar{\delta}=o(1)\) such that

\[\mathbb{E}M_{5} \leq\sum_{j=1}^{n}\sum_{b\in[k]\setminus\{z_{j}^{*}\}}\left\| \theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\mathbb{P}\left\{C\bar{\delta }\left\|\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*}\right\|^{2}\sqrt{\frac{n}{\log n }}\leq\|w_{j}\|^{2}\right\}\] \[\leq n\exp\!\left(-(1+o(1))\frac{\text{SNR}^{{}^{\prime}2}}{8} \right).\]\(M_{6}\) is essentially the same with \(M_{5}\) and can be proved similarly. Finally for \(M_{1}\), using Lemma C.10, we have

\[\mathbb{P}\bigg{(}\langle\epsilon_{j},(\Sigma_{b}^{*})^{-1}(\theta _{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle+\frac{1}{2}\langle\epsilon_{j},((\Sigma_ {b}^{*})^{-1}-(\Sigma_{z_{j}^{*}}^{*})^{-1})\epsilon_{j}\rangle\] \[\qquad-\frac{1}{2}\log|\Sigma_{z_{j}^{*}}^{*}|+\frac{1}{2}\log| \Sigma_{b}^{*}|\leq-\frac{1-\delta-\bar{\delta}}{2}\langle\theta_{z_{j}^{*}}^{ *}-\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*} )\rangle\bigg{)}\] \[= \mathbb{P}\bigg{(}\langle w_{j},(\Sigma_{z_{j}^{*}}^{*})^{\frac{1 }{2}}(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*})\rangle+ \frac{1}{2}\langle w_{j},((\Sigma_{z_{j}^{*}}^{*})^{\frac{1}{2}}(\Sigma_{b}^{ *})^{-1}(\Sigma_{z_{j}^{*}}^{*})^{\frac{1}{2}}-I_{d})w_{j}\rangle\] \[\qquad-\frac{1}{2}\log|\Sigma_{z_{j}^{*}}^{*}|+\frac{1}{2}\log| \Sigma_{b}^{*}|\leq-\frac{1-\delta-\bar{\delta}}{2}\langle\theta_{z_{j}^{*}}^ {*}-\theta_{b}^{*},(\Sigma_{b}^{*})^{-1}(\theta_{z_{j}^{*}}^{*}-\theta_{b}^{*} )\rangle\bigg{)}\] \[\leq\exp\!\left(-(1-o(1))\frac{\text{SNR}_{z_{j}^{*},b}^{\prime} }{8}\right).\]

Then we have

\[\mathbb{E}M_{1}\leq n\exp\!\left(-(1+o(1))\frac{\text{SNR}^{{}^{ \prime}2}}{8}\right).\]

Using the Markov's inequality we complete the proof of Lemma B.4. 

Proof of Theorem 3.2.: By Lemmas B.2-B.4, we can obtain the result by arguments used in the proof of Theorem 2.2 and hence the proof is omitted here. 

## Appendix C Technical Lemmas

In this section, we present and prove technical lemmas used in this paper. Lemmas C.1 and C.2 are about \(\chi^{2}\) distributions. Appendix C.1 gives various upper bounds needed in the proofs of Appendix B. Appendix C.2 is devoted to the calculation related to \(\text{SNR}^{\prime}\).

**Lemma C.1**.: _For any \(x>0\), we have_

\[\mathbb{P}(\chi_{d}^{2}\geq d+2\sqrt{dx}+2x) \leq e^{-x},\] \[\mathbb{P}(\chi_{d}^{2}\leq d-2\sqrt{dx}) \leq e^{-x}.\]

Proof.: These results are Lemma 1 of [12]. 

**Lemma C.2**.: _Let \(W_{i}\stackrel{{ iid}}{{\sim}}\chi_{d}^{2}\) for any \(i\in[n]\) where \(n,d\) are positive integers. Then we have_

\[\mathbb{P}\bigg{(}\sum_{i=1}^{n}W_{i}^{2}\geq 3nd^{2}\bigg{)}\leq\frac{4}{ nd}.\]

Proof.: We have \(\mathbb{E}\sum_{i=1}^{n}W_{i}^{2}=nd(d+2)\) and \(\mathbb{E}\sum_{i=1}^{n}W_{i}^{4}=nd(d+2)(d+4)(d+6)\). Then we have \(\text{Var}\big{(}\sum_{i=1}^{n}W_{i}^{2}\big{)}=8nd(d+2)(d+3)\). Then we obtain the desired result by Chebyshev's inequality. 

### With-High-Probability Bounds

**Lemma C.3**.: _For any \(z^{*}\in[k]^{n}\) and \(k\in[n]\), consider independent vectors \(\epsilon_{j}\sim\mathcal{N}(0,\Sigma_{z_{j}^{*}}^{*})\) for any \(j\in[n]\). Assume there exists a constant \(\lambda_{\max}>0\) such that \(\|\Sigma_{a}^{*}\|\leq\lambda_{\max}\) for any \(a\in[k]\). Then, for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(C^{\prime},\lambda_{\max}\) such that_

\[\max_{a\in[k]}\left\|\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\} \epsilon_{j}}{\sqrt{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}}\right\| \leq C\sqrt{d+\log n},\] (33) \[\max_{a\in[k]}\frac{1}{d+\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\} }\left\|\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\epsilon_{j}\epsilon_{j}^{T}\right\| \leq C,\] (34) \[\max_{T\subset[n]}\left\|\frac{1}{\sqrt{|T|}}\sum_{j\in T} \epsilon_{j}\right\| \leq C\sqrt{d+n},\] (35) \[\max_{a\in[k]}\max_{T\subset\{j:z_{j}^{*}=a\}}\left\|\frac{1}{ \sqrt{|T|(d+\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\})}}\sum_{j\in T}\epsilon_{j} \right\| \leq C,\] (36)

_with probability at least \(1-n^{-C^{\prime}}\). We have used the convention that \(0/0=0\)._

Proof.: Note that \(\epsilon_{j}\) is sub-Gaussian with parameter \(\lambda_{\max}\) which is a constant. The inequalities (33) and (35) are respectively Lemmas A.4, A.1 in [15]. The inequality (34) is a slight extension of Lemma A.2 in [15]. This extension follows from a standard union bound argument. The proof of (36) is identical to that of (35). 

**Lemma C.4**.: _Consider the same assumptions as in Lemma C.3. Assume additionally \(\min_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\geq\frac{\alpha n}{k}\) for some constant \(\alpha>0\) and \(\frac{k(d+\log n)}{n}=o(1)\). Then, for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(\alpha,C^{\prime},\lambda_{\max}\) such that_

\[\max_{a\in[k]}\left\|\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}\sum_{j= 1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\epsilon_{j}\epsilon_{j}^{T}-\Sigma_{a}^{*} \right\|\leq C\sqrt{\frac{k(d+\log n)}{n}},\] (37)

_with probability at least \(1-n^{-C^{\prime}}\)._

Proof.: Note that we have \(\epsilon_{j}=\Sigma_{z_{j}^{*}}^{\frac{1}{2}}\eta_{j}\) where \(\eta_{j}\stackrel{{ iid}}{{\sim}}\mathcal{N}(0,I_{d})\) for any \(j\in[n]\). Since \(\max_{a}\|\Sigma_{a}^{*}\|\leq\lambda_{\max}\), we have

\[\max_{a\in[k]}\left\|\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}\sum_{j= 1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\epsilon_{j}\epsilon_{j}^{T}-\Sigma_{a}^{*} \right\|\leq\lambda_{\max}\max_{a\in[k]}\left\|\frac{1}{\sum_{j=1}^{n} \mathbb{I}\{z_{j}^{*}=a\}}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\eta_{j} \eta_{j}^{T}-I_{d}\right\|.\]

Define

\[Q_{a}=\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}\sum_{j=1}^{n}\mathbb{ I}\{z_{j}^{*}=a\}\eta_{j}\eta_{j}^{T}-I_{d}.\]

Take \(S^{d-1}=\{y\in\mathbb{R}^{d}:\|y\|=1\}\) and \(N_{\epsilon}=\{v_{1},\cdots,v_{|N_{\epsilon}|}\}\) is an \(\epsilon\)-covering of \(S^{d-1}\). In particular, we pick \(\epsilon<\frac{1}{4}\), then \(|N_{\epsilon}|\leq 9^{d}\). By the definition of the \(\epsilon\)-covering, we have

\[\|Q_{a}\|\leq\frac{1}{1-2\epsilon}\max_{i=1,\cdots,|N_{\epsilon}|}|v_{i}^{T}Q_ {a}v_{i}|\leq 2\max_{i=1,\cdots,|N_{\epsilon}|}|v_{i}^{T}Q_{a}v_{i}|.\]

For any \(v\in N_{\epsilon}\),

\[v^{T}Q_{a}v=\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}\sum_{j=1}^{n} \mathbb{I}\{z_{j}^{*}=a\}(v^{T}\eta_{j}\eta_{j}^{T}v-1).\]Denote \(n_{a}=\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\). Then \(\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\). Then \(\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}v^{T}\eta_{j}\eta_{j}^{T}v\sim\chi_{n_{a}} ^{2}\). Using Lemma C.1, we have

\[\mathbb{P}(\max_{a\in[k]}\|Q_{a}\|\geq t) \leq\sum_{a=1}^{k}\mathbb{P}(\|Q_{a}\|\geq t)\] \[\leq\sum_{a=1}^{k}\sum_{i=1}^{|N_{\epsilon}|}\mathbb{P}(|v_{i}^{T} Q_{a}v_{i}|\geq t/2)\] \[\leq\sum_{a=1}^{k}2\exp\Biggl{\{}-\frac{n_{a}}{8}\min\{t,t^{2}\}+ d\log 9\Biggr{\}}.\]

Since \(\frac{k(d+\log n)}{n}=o(1)\) and \(n_{a}\geq\alpha n/k\) where \(\alpha\) is a constant, we can take \(t=C^{\prime\prime}\sqrt{\frac{k(d+\log n)}{n}}\) for some large constant \(C^{\prime\prime}\) and the proof is complete. 

**Lemma C.5**.: _Consider the same assumptions as in Lemma C.3. Then, for any \(s=o(n)\) and for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(C^{\prime},\lambda_{\max}\) such that_

\[\max_{T\subset[n]:|T|\leq s}\frac{1}{|T|\log\frac{n}{|T|}+\min\{1,\sqrt{|T|}\} d}\left\|\sum_{j\in T}\epsilon_{j}\epsilon_{j}^{T}\right\|\leq C,\] (38)

_with probability at least \(1-n^{-C^{\prime}}\). We have used the convention that \(0/0=0\)._

Proof.: Consider any \(a\in[s]\) and a fixed \(T\subset[n]\) such that \(|T|=a\). Similar to the proof of Lemma C.4, we can take \(S^{d-1}=\{y\in\mathbb{R}^{d}:\|y\|=1\}\) and its \(\epsilon\)-covering \(N_{\epsilon}\) with \(\epsilon<\frac{1}{4}\) and \(|N_{\epsilon}|\leq 9^{d}\). Then we have

\[\|\sum_{j\in T}\epsilon_{j}\epsilon_{j}^{T}\|=\sup_{\|w\|=1}\sum_{j\in T}(w^{T }\epsilon_{j})^{2}\leq 2\max_{w\in N_{*}}\sum_{j\in T}(w^{T}\epsilon_{j})^{2}.\]

Note that \(w^{T}\epsilon_{j}/\sqrt{\lambda_{\max}}\) is a sub-Gaussian random variable with parameter 1. By the tail probability result for quadratic forms of sub-Gaussian random vectors [10], for any fixed \(w\in N_{\epsilon}\), we have

\[\mathbb{P}\Biggl{(}\sum_{j\in T}(w^{T}\epsilon_{j})^{2}\geq\lambda_{\max} \Bigl{(}a+2\sqrt{at}+2t\Bigr{)}\Biggr{)}\leq\exp(-t)\,.\]

Since \(a=o(n)\), there exists a constant \(C_{0}\) such that \(2a\leq C_{0}a\log\frac{n}{a}\). We can take \(t=\tilde{C}(a\log\frac{n}{a}+d)\) with \(\tilde{C}=\frac{C}{16}-\frac{C_{0}}{4}\), then \(a+2\sqrt{at}+2t\leq\frac{C}{4}(a\log\frac{n}{a}+d)\). Thus,

\[\mathbb{P}\Biggl{(}\sum_{j\in T}(w^{T}\epsilon_{j})^{2}\geq\frac{C}{4}(a\log \frac{n}{a}+d)\Biggr{)}\leq\exp\bigg{(}-\tilde{C}(a\log\frac{n}{a}+d)\bigg{)}.\]

Hence, we have

\[\mathbb{P}\Biggl{(}\|\sum_{j\in T}\epsilon_{j}\epsilon_{j}^{T}\|\geq\frac{C}{ 2}(a\log\frac{n}{a}+d)\Biggr{)}\leq 9^{d}\exp\bigg{(}-\tilde{C}(a\log\frac{n}{a}+d) \bigg{)}.\]

As a result,

\[\mathbb{P}\bigg{\{}\max_{T\subset[n],1\leq|T|\leq s}\frac{1}{|T| \log\frac{n}{|T|}+d}\|\sum_{j\in T}\epsilon_{j}\epsilon_{j}^{T}\|\geq C\bigg{\}} \leq \sum_{a=1}^{s}\mathbb{P}\bigg{\{}\max_{|T|=a}\|\sum_{j\in T} \epsilon_{j}\epsilon_{j}^{T}\|\geq C(a\log\frac{n}{a}+d)\bigg{\}}\] \[\leq \sum_{a=1}^{s}\binom{n}{a}\max_{|T|=a}\mathbb{P}\bigg{\{}\|\sum_{j \in T}\epsilon_{j}\epsilon_{j}^{T}\|\geq C(a\log\frac{n}{a}+d)\bigg{\}}\] \[\leq \sum_{a=1}^{s}\binom{n}{a}9^{d}\exp\bigg{(}-\tilde{C}(a\log\frac{ n}{a}+d)\bigg{)}.\]Since \(a\log\frac{n}{a}\) is an increasing function when \(a\in[1,s]\) and \(a\log\frac{n}{a}\geq\log n\geq\log s\), a choice of \(\tilde{C}=3+C^{\prime}\), that is \(C=16C^{\prime}+4C_{0}+48\), can yield the desired result.

Finally, to allow \(|T|=0\), we note that \(d\leq\min\{1,\sqrt{|T|}\}d\). The proof is complete. 

**Lemma C.6**.: _For any \(z^{*}\in[k]^{n}\) and \(k\in[n]\), assume \(\min_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\geq\frac{\alpha n}{k}\) and \(\ell(z,z^{*})=o(\frac{n\Delta^{2}}{k})\), then_

\[\max_{a\in[k]}\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}{\sum_{j=1}^{n} \mathbb{I}\{z_{j}=a\}}\leq 2.\] (39)

Proof.: For any \(z\in[k]^{n}\) such that \(\ell(z,z^{*})=o(n)\) and any \(a\in[k]\), we have

\[\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\} \geq\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}-\sum_{j=1}^{n} \mathbb{I}\{z_{j}\neq z_{j}^{*}\}\] \[\geq\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}-\frac{\ell(z,z^{*})}{ \Delta^{2}}\] \[\geq\frac{\alpha n}{2k},\] (40)

which implies

\[\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}}{\sum_{j=1}^{n} \mathbb{I}\{z_{j}=a\}} \leq\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}+\sum_{j=1}^{n} \mathbb{I}\{z_{j}\neq z_{j}^{*}\}}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\] \[\leq 1+\frac{\alpha n/2k}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\] \[\leq 2.\]

Thus, we obtain (39). 

In the following lemma, we are going to analyze estimation errors of the centers and covariance matrices under the anisotropic GMMs. For any \(z\in[k]^{n}\) and for any \(z\in[k]\), recall the definitions

\[\hat{\theta}_{a}(z) =\frac{\sum_{j\in[n]}Y_{j}\mathbb{I}\left\{z_{j}=a\right\}}{\sum_ {j\in[n]}\mathbb{I}\left\{z_{j}=a\right\}},\forall a\in[k]\] \[\hat{\Sigma}_{a}(z) =\frac{\sum_{j\in[n]}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}-\hat{ \theta}_{a}(z))^{T}\mathbb{I}\left\{z_{j}=a\right\}}{\sum_{j\in[n]}\mathbb{I} \left\{z_{j}=a\right\}},\forall a\in[k].\]

**Lemma C.7**.: _For any \(z^{*}\in[k]^{n}\) and \(k\in[n]\), consider independent vectors \(Y_{j}=\theta_{z_{j}^{*}}^{*}+\epsilon_{j}\) where \(\epsilon_{j}\sim\mathcal{N}(0,\Sigma_{z^{*}_{j}}^{*})\) for any \(j\in[n]\). Assume there exist constants \(\lambda_{\min},\lambda_{\max}>0\) such that \(\lambda_{\min}\leq\lambda_{1}(\Sigma_{a}^{*})\leq\lambda_{d}(\Sigma_{a}^{*}) \leq\lambda_{\max}\) for any \(a\in[k]\), and a constant \(\alpha>0\) such that \(\min_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}\geq\frac{\alpha n}{k}\). Assume \(\frac{k(d+\log n)}{k}=o(1)\) and \(\frac{\Delta}{k}\rightarrow\infty\). Assume (33)-(38) hold. Then for any \(\tau=o(n)\) and for any constant \(C^{\prime}>0\), there exists some constant \(C>0\) only depending on \(\alpha,\lambda_{\max},C^{\prime}\) such that_

\[\max_{a\in[k]}\left\|\hat{\theta}_{a}(z^{*})-\theta_{a}^{*}\right\| \leq C\sqrt{\frac{k(d+\log n)}{n}},\] (41) \[\max_{a\in[k]}\left\|\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*})\right\| \leq C\left(\frac{k}{n\Delta}\ell(z,z^{*})+\frac{k\sqrt{d+n}}{n \Delta}\sqrt{\ell(z,z^{*})}\right),\] (42) \[\max_{a\in[k]}\left\|\hat{\Sigma}_{a}(z^{*})-\Sigma_{a}^{*}\right\| \leq C\sqrt{\frac{k(d+\log n)}{n}},\] (43) \[\max_{a\in[k]}\left\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\right\| \leq C\left(\frac{k}{n}\ell(z,z^{*})+\frac{k\sqrt{n\ell(z,z^{*})} }{n\Delta}+\frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}\right),\] (44)

_for all \(z\) such that \(\ell(z,z^{*})\leq\tau\)._Proof.: Using (33) we obtain (41). By the same argument of (118) in [8], we can obtain (42). By (33) and (37) and (41), we can obtain (43). In the remaining proof, we will establish (53).

Since \(\frac{k(d+\log n)}{n}=o(1)\), we have \(\|\hat{\Sigma}_{a}(z^{*})\|\leq 1\) for any \(a\in[k]\). The difference \(\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\) will be decomposed into several terms. We notice that

\[\left\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\right\|\leq S_{1}+S_{2},\] (45)

where

\[S_{1}=\bigg{\|}\frac{1}{\sum\mathbb{I}\{z_{j}=a\}}\sum_{j=1}^{n}\mathbb{I}\{z _{j}=a\}\bigg{(}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}-\hat{\theta}_{a}(z))^{T}-(Y_ {j}-\hat{\theta}_{a}(z^{*}))(Y_{j}-\hat{\theta}_{a}(z^{*}))^{T}\bigg{)}\bigg{\|},\]

and

\[S_{2}=\bigg{\|}\bigg{(}\frac{1}{\sum\mathbb{I}\{z_{j}=a\}}-\frac{1}{\sum \mathbb{I}\{z_{j}^{*}=a\}}\bigg{)}\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}(Y_ {j}-\hat{\theta}_{a}(z^{*}))(Y_{j}-\hat{\theta}_{a}(z^{*}))^{T}\bigg{\|}.\]

Also, we notice that

\[S_{1}\leq L_{1}+L_{2}+L_{3},\] (46)

where

\[L_{1} =\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1} ^{n}\mathbb{I}\{z_{j}=z_{j}^{*}=a\}\bigg{(}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}- \hat{\theta}_{a}(z))^{T}-(Y_{j}-\hat{\theta}_{a}(z^{*}))(Y_{j}-\hat{\theta}_{ a}(z^{*}))^{T}\bigg{)}\bigg{\|},\] \[L_{2} =\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1} ^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}(Y_{j}-\hat{\theta}_{a}(z))(Y_{j}- \hat{\theta}_{a}(z))^{T}\bigg{\|},\] \[L_{3} =\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1} ^{n}\mathbb{I}\{z_{j}\neq a,z_{j}^{*}=a\}(Y_{j}-\hat{\theta}_{a}(z^{*}))(Y_{j} -\hat{\theta}_{a}(z^{*}))^{T}\bigg{\|}.\]

For \(L_{1}\), we have

\[L_{1} \leq\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j =1}^{n}\mathbb{I}\{z_{j}=z_{j}^{*}=a\}(\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^ {*}))(\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}))^{T}\bigg{\|}\] \[\quad+2\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}} \sum_{j=1}^{n}\mathbb{I}\{z_{j}=z_{j}^{*}=a\}(Y_{j}-\hat{\theta}_{a}(z^{*}))( \hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}))^{T}\bigg{\|}\] \[\quad+\bigg{\|}\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*})\big{\|} \bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1}^{n}\mathbb{ I}\{z_{j}=z_{j}^{*}=a\}\epsilon_{j}\bigg{\|}.\] (47)

By (36), (39), (40), we have uniformly for any \(a\in[k]\),

\[\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1}^ {n}\mathbb{I}\{z_{j}=z_{j}^{*}=a\}\epsilon_{j}\bigg{\|} \leq\frac{\sqrt{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=z_{j}^{*}=a\}}}{ \sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sqrt{d+\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{ *}=a\}}\] \[\preceq 1.\] (48)

Since \(\max_{a\in[k]}\left\|\hat{\theta}_{a}(z^{*})-\theta_{a}^{*}\right\|=o(1)\), by (39), (42), (41), (47), and (48), we have uniformly for any \(a\in[k]\),

\[L_{1}\preceq\left\|\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*})\right\|\preceq \frac{k}{n\Delta}\ell(z,z^{*})+\frac{k\sqrt{d+n}}{n\Delta}\sqrt{\ell(z,z^{*})}.\] (49)To bound \(L_{2}\), we first give the following simple fact. For any positive integer \(m\) and any \(\{u_{j}\}_{j\in[m]},\{v_{j}\}_{j\in[m]}\in\mathbb{R}^{d}\), we have \(\|\sum_{j\in[m]}(u_{j}+v_{j})(u_{j}+v_{j})^{T}\|\leq 2\|\sum_{j\in[m]}u_{j}u_{j}^{T} \|+2\|\sum_{j\in[m]}v_{j}v_{j}^{T}\|\). Hence, for \(L_{2}\), we have the following decomposition

\[L_{2}\leq 2R_{1}+2R_{2},\] (50)

where

\[R_{1} =\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1} ^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}(Y_{j}-\theta_{a}^{*})(Y_{j}-\theta_ {a}^{*})^{T}\bigg{\|},\] \[R_{2} =\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1 }^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}(\theta_{a}^{*}-\hat{\theta}_{a}(z)) (\theta_{a}^{*}-\hat{\theta}_{a}(z))^{T}\bigg{\|}.\]

Since \(\max_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}\leq\frac{ \ell(z,z^{*})}{\Delta^{2}}\), we have

\[R_{2} \leq\bigg{\|}\theta_{a}^{*}-\hat{\theta}_{a}(z)\bigg{\|}^{2} \frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}}{\sum_{j=1}^{n} \mathbb{I}\{z_{j}=a\}}\] \[\preceq \bigg{(}\Big{\|}\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*}) \Big{\|}^{2}+\Big{\|}\hat{\theta}_{a}(z^{*})-\theta_{a}^{*}\Big{\|}^{2}\bigg{)} \,\frac{k\ell(z,z^{*})}{n\Delta^{2}}.\] (51)

By (38) and the fact that \(\max_{a\in[k]}\sum_{j=1}^{n}\mathbb{I}\{z_{i}=a,z_{i}^{*}\neq a\}\leq\frac{ \ell(z,z^{*})}{\Delta^{2}}\), we also have

\[R_{1} \leq 2\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_ {j=1}^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}(\theta_{z_{j}^{*}}^{*}-\theta_{z _{j}}^{*})(\theta_{z_{j}^{*}}^{*}-\theta_{z_{j}^{*}}^{*})^{T}\bigg{\|}\] \[\leq 2\frac{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}\| \theta_{z_{j}^{*}}^{*}-\theta_{z_{j}}^{*}\|^{2}}{\sum_{j=1}^{n}\mathbb{I}\{z_ {j}=a\}}+2\bigg{\|}\frac{1}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\sum_{j=1}^{n} \mathbb{I}\{z_{j}=a,z_{j}^{*}\neq a\}\epsilon_{j}\epsilon_{j}^{T}\bigg{\|}\] \[\preceq\frac{k\ell(z,z^{*})}{n}+\frac{\frac{\ell(z,z^{*})}{\Delta ^{2}}\log\frac{n\Delta^{2}}{\ell(z,z^{*})}+d\sqrt{\frac{\ell(z,z^{*})}{\Delta ^{2}}}}{n/k}.\]

We are going to simplify the above bounds for \(R_{1},R_{2}\). Under the assumption that \(\frac{k(d+\log n)}{n}=o(1)\), \(\Delta/k\to\infty\), and \(\ell(z,z^{*})\leq\tau=o(n)\), we have \(\max_{a\in[k]}\|\hat{\theta}_{a}(z)-\hat{\theta}_{a}(z^{*})\|=o(1)\), \(\max_{a\in[k]}\|\hat{\theta}_{a}(z^{*})-\theta_{a}^{*}\|=o(1)\), and \(\frac{k\ell(z,z^{*})}{n\Delta^{2}}=o(1)\). Hence \(R_{2}\preceq\frac{k\ell(z,z^{*})}{n\Delta^{2}}\). Also we have

\[\frac{k\ell(z,z^{*})}{n\Delta^{2}}\log\frac{n\Delta^{2}}{\ell(z,z^{*})}=\frac{ k\sqrt{\ell(z,z^{*})}}{n\Delta}\sqrt{\frac{\ell(z,z^{*})}{\Delta^{2}}} \bigg{(}\log\frac{n\Delta^{2}}{\ell(z,z^{*})}\bigg{)}^{2}\leq\frac{k\sqrt{n \ell(z,z^{*})}}{n\Delta}.\]

where in the last inequality, we use the fact that \(x(\log(n/x))^{2}\) is an increasing function of \(x\) when \(0<x=o(n)\). Then,

\[L_{2}\preceq\frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*})+ \frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\]

Since \(L_{3}\) is similar to \(L_{2}\), by (46) we have uniformly for any \(a\in[k]\)

\[S_{1}\preceq\frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*})+ \frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\] (52)

To bound \(S_{2}\), by (70) in [8], we have uniformly for any \(a\in[k]\),

\[S_{2}=\frac{\left|\sum_{j=1}^{n}\mathbb{I}\{z_{j}^{*}=a\}-\sum_{j=1}^{n} \mathbb{I}\{z_{j}=a\}\right|}{\sum_{j=1}^{n}\mathbb{I}\{z_{j}=a\}}\left\| \hat{\Sigma}_{a}(z^{*})\right\|^{2}\preceq\frac{k}{n}\frac{\ell(z,z^{*})}{ \Delta^{2}},\]where we use (43). Since \(\frac{k}{n}\frac{\ell(z,z^{*})}{\Delta^{2}}\preceq\frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}\), by (45) and the facts that \(\ell(z,z^{*})\leq\tau=o(n)\) we have

\[\max_{a\in[k]}\left\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*}) \right\|\preceq\frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*}) +\frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\]

**Lemma C.8**.: _Under the same assumption as in Lemma C.7, if additionally we assume \(kd=O(\sqrt{n})\) and \(\tau=o(n/k)\), there exists some constant \(C>0\) only depending on \(\alpha,\lambda_{\min},\lambda_{\max},C^{\prime}\) such that_

\[\max_{a\in[k]}\left\|(\hat{\Sigma}_{a}(z))^{-1}-(\hat{\Sigma}_{a}(z^{*}))^{-1} \right\|\leq C\!\left(\frac{k}{n}\ell(z,z^{*})+\frac{k\sqrt{n\ell(z,z^{*})}}{ n\Delta}+\frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}\right).\] (53)

Proof.: By (43) we have \(\max_{a\in[k]}\left\|\hat{\Sigma}_{a}(z^{*})\right\|,\max_{a\in[k]}\left\|( \hat{\Sigma}_{a}(z^{*}))^{-1}\right\|\preceq 1\). By (44) we also have \(\max_{a\in[k]}\left\|\hat{\Sigma}_{a}(z)\right\|,\max_{a\in[k]}\left\|(\hat{ \Sigma}_{a}(z))^{-1}\right\|\preceq 1\). Hence,

\[\max_{a\in[k]}\left\|(\hat{\Sigma}_{a}(z))^{-1}-(\hat{\Sigma}_{a}( z^{*}))^{-1}\right\|\leq \max_{a\in[k]}\left\|(\hat{\Sigma}_{a}(z^{*}))^{-1}\right\| \left\|\hat{\Sigma}_{a}(z)-\hat{\Sigma}_{a}(z^{*})\right\|\left\|(\hat{\Sigma }_{a}(z))^{-1}\right\|\] \[\preceq \frac{k\sqrt{n\ell(z,z^{*})}}{n\Delta}+\frac{k}{n}\ell(z,z^{*})+ \frac{kd}{n\Delta}\sqrt{\ell(z,z^{*})}.\] (54)

### Calculation Related to \(\text{SNR}^{\prime}\)

In the following lemmas, we study properties of \(\{\text{SNR}^{\prime}_{a,b}\}_{a\neq b}\). Consider any pair \(a\neq b\in[k]\). Let \(\eta\sim\mathcal{N}(0,I_{d})\) and \(\Xi_{a,b}=\theta_{a}^{*}-\theta_{b}^{*}\). Define

\[B_{a,b}(\delta)=\Bigg{\{}x\in\mathbb{R}^{d}:x^{T}\Sigma_{a}^{* \frac{1}{2}}(\Sigma_{b}^{*})^{-1}\Xi_{a,b}+\frac{1}{2}x^{T}\Big{(}\Sigma_{a}^ {*\frac{1}{2}}(\Sigma_{b}^{*})^{-1}\Sigma_{a}^{*\frac{1}{2}}-I_{d}\Big{)}\,x\] \[\qquad\qquad\leq-\frac{1-\delta}{2}\Xi_{a,b}^{T}(\Sigma_{b}^{*}) ^{-1}\Xi_{a,b}+\frac{1}{2}\log|\Sigma_{a}^{*}|-\frac{1}{2}\log|\Sigma_{b}^{*}|\, \Bigg{\}}\] \[=\Bigg{\{}x\in\mathbb{R}^{d}:\left\|x\right\|^{2}\geq \Big{(}x-(\Sigma_{a}^{*})^{-\frac{1}{2}}\Xi_{b,a}\Big{)}^{T}\Big{(}( \Sigma_{a}^{*})^{-\frac{1}{2}}\Sigma_{b}^{*}(\Sigma_{a}^{*})^{-\frac{1}{2}} \Big{)}^{-1}\Big{(}x-(\Sigma_{a}^{*})^{-\frac{1}{2}}\Xi_{b,a}\Big{)}\] \[\qquad+\log|(\Sigma_{a}^{*})^{-\frac{1}{2}}\Sigma_{b}^{*}(\Sigma_ {a}^{*})^{-\frac{1}{2}}|-\delta\Xi_{b,a}^{T}(\Sigma_{b}^{*})^{-1}\Xi_{b,a} \Bigg{\}},\]

for any \(\delta\in\mathbb{R}\). Then \(B_{a,b}(\delta)\subset B_{a,b}(\delta^{\prime})\) for any \(\delta^{\prime}\leq\delta\). In addition, we define

\[\text{SNR}^{\prime}_{a,b}(\delta)=\min_{x\in B_{a,b}(\delta)}2 \left\|x\right\|,\] \[\text{and }P_{a,b}(\delta)=\mathbb{P}(\eta\in B_{a,b}(\delta))\,.\]

Recall the definitions of \(B_{a,b}\) and \(\text{SNR}^{\prime}_{a,b}\) in Section 3. Then it is a special case of \(B_{a,b}(\delta)\) and \(\text{SNR}^{\prime}_{a,b}(\delta)\) with \(\delta=0\). That is, we have \(B_{a,b}=B_{a,b}(0)\) and \(\text{SNR}^{\prime}_{a,b}=\text{SNR}^{\prime}_{a,b}(0)\).

To understand these quantities, we first study a canonical setting that can be later applied to establish Lemma C.10.

**Lemma C.9**.: _Consider any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and any \(\Sigma\in\mathbb{R}^{d\times d}\) that is positive semi-definite. Let \(\lambda_{\max},\lambda_{\min}>0\) be the largest and smallest eigenvalue of \(\Sigma\), respectively. For any \(t\in\mathbb{R}\), define_

\[D(t)=\{x\in\mathbb{R}^{d}:\left\|x\right\|^{2}\geq(x-\theta)^{T}\Sigma^{-1}(x- \theta)+t\},\]

_and \(s(t)=\min_{x\in D(t)}\left\|x\right\|\). Then the following hold:_

* _Under the assumption that_ \(-\left\|\theta\right\|^{2}/(8\lambda_{\max})<t<\left\|\theta\right\|^{2}/8\)_, we have_ \[\left\|\theta\right\|/(\max\{2,2\sqrt{2\lambda_{\max}}\})<s(t)<(1-\min\{\sqrt{ \lambda_{\min}/8},1/2\})\left\|\theta\right\|.\]* _If_ \(t^{\prime}\) _also satisfies_ \(-\left\|\theta\right\|^{2}/\lambda_{\max}<t^{\prime}<\left\|\theta\right\|^{2}/8\)_, we have_ \[\left|s(t^{\prime})-s(t)\right|\leq\lambda_{\max}\frac{t^{\prime}-t}{2\min\{ \sqrt{\lambda_{\min}/8},1/2\}\left\|\theta\right\|}.\]
* _If_ \(\theta\) _is further assumed to satisfy_ \(\min\{\sqrt{\lambda_{\min}/8},1/2\}\left\|\theta\right\|\geq 2\lambda_{\max}\) _and_ \(\left\|\theta\right\|\geq\frac{\lambda_{\min}}{32}\min\{\sqrt{\lambda_{\min}/8},1/2\}\)_, there exists a_ \(d\)_-dimensional ball_ \(H(t)\in\mathbb{R}^{d}\) _with radius_ \((\lambda_{\min}/8)\min\{\sqrt{\lambda_{\min}/8},1/2\}\) _such that_ \(H(t)\subset D(t)\) _and_ \(\left\|x\right\|\leq(\lambda_{\min}/8)\min\{\sqrt{\lambda_{\min}/8},1/2\}+ \lambda_{\max}+s(t)\) _for all_ \(x\in H(t)\)_._

Proof.: First, we check whether each of the following two points is contained in \(D(t)\) or not, under the assumption \(-\left\|\theta\right\|^{2}/\lambda_{\max}<t<\left\|\theta\right\|^{2}\).

* When \(x=0\), we have \((x-\theta)^{T}\Sigma^{-1}(x-\theta)+t\geq\left\|\theta\right\|^{2}/\lambda_{ \max}+t>0\). Hence, \(0\notin D(t)\).
* When \(x=\theta\), we have \(\left\|\theta\right\|^{2}>t\). Hence, \(\theta\in D(t)\).

As a result, \(D(t)\) is non-empty, \(s(t)\) is well defined, and \(0<s(t)<\left\|\theta\right\|^{2}\). Next, we consider a few more points to sharpen upper and lower bounds on \(s(t)\) under the assumption \(-\left\|\theta\right\|^{2}/(8\lambda_{\max})<t<\left\|\theta\right\|^{2}/8\).

* For any \(x\) that satisfies \(\left\|x\right\|\leq\left\|\theta\right\|/(\max\{2,2\sqrt{2\lambda_{\max}}\})\), we have \(\left\|x-\theta\right\|\geq\left\|\theta\right\|/2\) and consequently, \((x-\theta)^{T}\Sigma^{-1}(x-\theta)+t\geq(\left\|\theta\right\|/2)^{2}/\lambda _{\max}+t=\left\|\theta\right\|^{2}/(4\lambda_{\max})+t\). Under the assumption that \(t>-\left\|\theta\right\|^{2}/(8\lambda_{\max})\), we can verify that \(\left\|\theta\right\|^{2}/(4\lambda_{\max})+t>\left\|\theta\right\|^{2}/(8 \lambda_{\max})\geq(\left\|\theta\right\|/\max\{2,2\sqrt{2\lambda_{\max}}\}) ^{2}\geq\left\|x\right\|^{2}\). Hence, such \(x\notin D(t)\).
* When \(x=(1-\min\{\sqrt{\lambda_{\min}/8},1/2\})\theta\), we have \(\left\|x\right\|\geq\left\|\theta\right\|/2\) and \((x-\theta)^{T}\Sigma^{-1}(x-\theta)+t\leq\left\|\theta\right\|^{2}(\min\{ \sqrt{\lambda_{\min}/8},1/2\})^{2}/\lambda_{\min}+t=\left\|\theta\right\|^{2}/ \max\{8,4\lambda_{\min}\}+t\). Under the assumption that \(t<\left\|\theta\right\|^{2}/8\), we have \(\left\|\theta\right\|^{2}/\max\{8,4\lambda_{\min}\}+t<\left\|\theta\right\|^{2 }/8+t\leq\left\|\theta\right\|^{2}/4\leq\left\|x\right\|^{2}\). Hence, such \(x\in D(t)\).

As a result, we have \(\left\|\theta\right\|/(\max\{2,2\sqrt{2\lambda_{\max}}\})<s(t)<(1-\min\{\sqrt{ \lambda_{\min}/8},1/2\})\left\|\theta\right\|\).

Define a ball \(\mathcal{S}(r)=\{x\in\mathbb{R}^{d}:\left\|x\right\|^{2}\leq r^{2}\}\) and define \(\mathcal{S}_{2}(r;t)=\{x\in\mathbb{R}^{d}:(x-\theta)^{T}\Sigma^{-1}(x-\theta) \leq r^{2}-t\}\) to be the part of \(\mathbb{R}^{d}\) that is inside the corresponding ellipsoid. Then we have \(s(t)=\min\{r\geq 0:\mathcal{S}_{1}(r)\cap\mathcal{S}_{2}(r;t)\neq\emptyset\}\). By the definition and bounds of \(s(t)\) and the convexity of \(\mathcal{S}_{1}(s(t))\) and \(\mathcal{S}_{2}(s(t);t)\), we must have \(\left|\mathcal{S}_{1}(s(t))\cap\mathcal{S}_{2}(s(t);t)\right|=1\), meaning that \(\mathcal{S}_{1}(s(t))\) and \(\mathcal{S}_{2}(s(t);t)\) touch each other externally at one point. This implies \(s(t)\) can be obtained by the following process: We let \(\mathcal{S}_{1}(r)\) and \(\mathcal{S}_{2}(r)\) grow by increasing \(r\), starting from \(0\). The first time they touch each other, we stop and the value of \(r\) is exactly \(s(t)\).

Denote \(y(t)\in\mathbb{R}\) such that \(\{y(t)\}=\mathcal{S}_{1}(s(t))\cap\mathcal{S}_{2}(s(t);t)\). Then we must have \(y(t)\in\mathcal{S}_{2}(s(t);t)\). By the first conclusion, we have

\[\left\|y(t)-\theta\right\|\geq\left\|\theta\right\|-\left\|y(t)\right\|= \left\|\theta\right\|-s(t)\geq\min\{\sqrt{\lambda_{\min}/8},1/2\}\left\|\theta\right\|\] \[\left\|y(t)-\theta\right\|\leq\left\|\theta\right\|+\left\|y(t) \right\|=\left\|\theta\right\|+s(t)\leq 2\left\|\theta\right\|.\]

In addition, we have

\[s^{2}(t)-t=(y(t)-\theta)^{T}\Sigma^{-1}(y(t)-\theta)\geq\left\|y(t)-\theta \right\|^{2}/\lambda_{\max}\geq(\min\{\sqrt{\lambda_{\min}/8},1/2\}\left\| \theta\right\|)^{2}/\lambda_{\max}.\]

Now we are going to prove the second conclusion of the lemma. Without loss of generality, assume \(t\leq t^{\prime}\). Then we have \(D(t)\supset D(t^{\prime})\) and \(s(t)\leq s(t^{\prime})\). We are going to establish a lower bound for \(s(t)\). First by definition of \(s(t^{\prime})\), we have \(\left|\mathcal{S}_{1}(s(t^{\prime}))\cap\mathcal{S}_{2}(s(t^{\prime});t^{ \prime})\right|=1\). Since \(t\leq t^{\prime}\), we have \(\mathcal{S}_{2}(s(t^{\prime});\(s(t)\leq s(t^{\prime})\). Now consider any \(x\in\bar{\mathcal{S}}_{2}(s(t^{\prime});t)\). It satisfies \((x-\theta)^{T}\Sigma^{-1}(x-\theta)=s^{2}(t^{\prime})-t\). Then we have

\[\left(\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}(t^{\prime})-t}}(x-\theta) \right)\Sigma^{-1}\left(\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}(t^{ \prime})-t}}(x-\theta)\right)=s^{2}(t^{\prime})-t^{\prime},\]

meaning that \(\theta+\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}(t^{\prime})-t}}(x- \theta)\in\bar{\mathcal{S}}_{2}(s(t^{\prime});t^{\prime})\). Hence,

\[\left\|x\right\| \geq\left\|\theta+\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}( t^{\prime})-t}}(x-\theta)\right\|-\left\|x-\left(\theta+\sqrt{\frac{s^{2}(t^{ \prime})-t^{\prime}}{s^{2}(t^{\prime})-t}}(x-\theta)\right)\right\|\] \[=\left\|\theta+\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}(t^ {\prime})-t}}(x-\theta)\right\|-\left(1-\sqrt{\frac{s^{2}(t^{\prime})-t^{ \prime}}{s^{2}(t^{\prime})-t}}\right)\left\|x-\theta\right\|\] \[\geq\min_{y\in\bar{\mathcal{S}}_{2}(s(t^{\prime});t^{\prime})} \left\|y\right\|-\left(1-\sqrt{\frac{s^{2}(t^{\prime})-t^{\prime}}{s^{2}(t^{ \prime})-t}}\right)\max_{y\in\bar{\mathcal{S}}_{2}(s(t^{\prime});t)}\left\|x- \theta\right\|.\]

Since \(\left|\mathcal{S}_{1}(s(t^{\prime}))\cap\bar{\mathcal{S}}_{2}(s(t^{\prime});t^ {\prime})\right|=1\), we have \(\min_{y\in\bar{\mathcal{S}}_{2}(s(t^{\prime});t^{\prime})}\left\|y\right\|=s(t ^{\prime})\). Since \((x-\theta)^{T}\Sigma^{-1}(x-\theta)\geq\lambda_{\max}^{-1}\left\|x-\theta \right\|^{2}\), we have \(\left\|x-\theta\right\|^{2}\leq\lambda_{\max}(s^{2}(t^{\prime})-t)\). Hence,

\[\left\|x\right\| \geq s(t^{\prime})-\left(1-\sqrt{\frac{s^{2}(t^{\prime})-t^{ \prime}}{s^{2}(t^{\prime})-t}}\right)\sqrt{\lambda_{\max}(s^{2}(t^{\prime})-t )}\] \[\geq s(t^{\prime})-\sqrt{\lambda_{\max}}\Big{(}\sqrt{s^{2}(t^{ \prime})-t}-\sqrt{s^{2}(t^{\prime})-t^{\prime}}\Big{)}\] \[=s(t^{\prime})-\sqrt{\lambda_{\max}}\frac{t^{\prime}-t}{\sqrt{s^ {2}(t^{\prime})-t}+\sqrt{s^{2}(t^{\prime})-t^{\prime}}}\] \[\geq s(t^{\prime})-\sqrt{\lambda_{\max}}\frac{t^{\prime}-t}{2 \sqrt{s^{2}(t^{\prime})-t^{\prime}}}.\]

As a result, for any \(r<s(t^{\prime})-\sqrt{\lambda_{\max}}\frac{t^{\prime}-t}{2\sqrt{s^{2}(t^{ \prime})-t^{\prime}}}\), we have \(\mathcal{S}_{1}(r)\cap\mathcal{S}_{2}(s(t^{\prime});t)=\emptyset\) and consequently \(\mathcal{S}_{1}(r)\cap\mathcal{S}_{2}(r;t)=\emptyset\). As a result, \(s(t)\geq s(t^{\prime})-\sqrt{\lambda_{\max}}\frac{t^{\prime}-t}{2\sqrt{s^{2}( t^{\prime})-t^{\prime}}}\). Since we have shown \(s^{2}(t^{\prime})-t^{\prime}\geq(\min\{\sqrt{\lambda_{\min}/8},1/2\}\left\| \theta\right\|)^{2}/\lambda_{\max}\), we have \(s(t)\geq s(t^{\prime})-\lambda_{\max}\frac{t^{\prime}-t}{2\min\{\sqrt{\lambda_{ \min}/8},1/2\}\left\|\theta\right\|}\).

For the third conclusion of the lemma, recall the definition of \(y(t)\). Under the assumption that \(\min\{\sqrt{\lambda_{\min}/8},1/2\}\left\|\theta\right\|\geq 2\lambda_{\max}\), we have \(\left\|y(t)-\theta\right\|-\lambda_{\max}>0\) and \(\lambda_{\max}/\left\|y(t)-\theta\right\|\leq 1/2\). Denote \(y^{\prime}(t)=\theta+\frac{y(t)-\theta}{\left\|y(t)-\theta\right\|}(\left\|y(t )-\theta\right\|-\lambda_{\max})\). Then

\[\left\|y^{\prime}(t)-\theta\right\| \leq\left\|y(t)-\theta\right\|\leq 2\left\|\theta\right\|,\] \[\left\|y^{\prime}(t)-\theta\right\| =\left(1-\lambda_{\max}/\left\|y(t)-\theta\right\|\right)\left\|y (t)-\theta\right\|,\] \[\left\|y(t)-y^{\prime}(t)\right\| =\left\|\frac{y(t)-\theta}{\left\|y(t)-\theta\right\|}\lambda_{ \max}\right\|=\lambda.\]

[MISSING_PAGE_EMPTY:37]

When \(\|\Xi_{b,a}\|\) is a constant, \(\|\theta\|\) is a constant, then \(\text{SNR}^{\prime}_{a,b}\) must be a constant as well. This is because \(D(t)\) is the set of points where one density is greater or equal to another. Then \(D(t)\) is non-empty as both densities have integral 1. Hence, \(s(t)\) must be finite, meaning \(\text{SNR}^{\prime}_{a,b}\) is a constant. In this case, we have \(P_{a,b}(0)=\mathbb{P}(\eta\in D(t))\) being a constant as well.

When \(\|\Xi_{b,a}\|\to\infty\), we have \(\|\theta\|\to\infty\). Since all the assumptions needed in Lemma C.9 are satisfied, by its first conclusion, we have \(s(t)\) being of the same order as \(\|\theta\|\), and consequently being of the same order as \(\|\Xi_{b,a}\|\). As a result, \(\text{SNR}^{\prime}_{a,b}\) and \(\|\Xi_{b,a}\|\) are of the same order. In addition, \(H(t)\) exists and its radius is some constant \(c_{1}>0\). Hence, its volume is \(c_{1}^{d}V_{d}\) where \(V_{d}\) is denoted as the volume of a \(d\)-dimensional unit ball. In addition, for any \(x\in H(t)\), we have \(\|x\|\leq s(t)+c_{2}=\text{SNR}^{\prime}_{a,b}/2+c_{2}\) for some constant \(c_{2}>0\). Recall \(\eta\sim N(0,I_{d})\). Then we have

\[P_{a,b}(0) \geq\mathbb{P}(\eta\in H(t))\geq c_{1}^{d}V_{d}\min_{x\in H(t)} \frac{1}{\sqrt{(2\pi)^{d}}}\exp\!\left(-\frac{1}{2}\left\|x\right\|^{2}\right)\] \[\geq c_{1}^{d}V_{d}\frac{1}{\sqrt{(2\pi)^{d}}}\exp\!\left(-\frac{ 1}{2}\left\|\text{SNR}^{\prime}_{a,b}/2+c_{2}\right\|^{2}\right)\] \[\geq\exp\!\left(-\frac{1+o(1)}{8}\text{SNR}^{{}^{\prime}2}_{a,b} \right).\]

Now let us consider \(B_{a,b}(\delta)\) where \(\delta=o(1)\). We can take \(\theta,\Sigma\) same as before, but let \(t^{\prime}=\log|(\Sigma_{a}^{*})^{-\frac{1}{2}}\Sigma_{b}^{*}(\Sigma_{a}^{*}) ^{-\frac{1}{2}}|-\delta\Xi_{b,a}^{T}(\Sigma_{b}^{*})^{-1}\Xi_{b,a}\). Then we have \(t^{\prime}=o(1)\left\|\theta\right\|^{2}\). Hence, by Lemma C.9, we have

\[\left|\text{SNR}^{\prime}_{a,b}-\text{SNR}^{\prime}_{a,b}(\delta)\right|=2 \left|s(t)-s(t^{\prime})\right|\preceq\frac{\left|t-t^{\prime}\right|}{\| \theta\|}=o(1)\left\|\theta\right\|=o(1)\text{SNR}^{\prime}_{a,b}.\]

Hence,

\[P_{a,b}(\delta) =\mathbb{P}(\eta\in B_{a,b}(\delta))\leq\max_{x\in B_{a,b}(\delta )}\frac{1}{\sqrt{(2\pi)^{d}}}\exp\!\left(-\frac{1}{2}\left\|x\right\|^{2}\right)\] \[=\frac{1}{\sqrt{(2\pi)^{d}}}\exp\!\left(-\frac{1}{2}\min_{x\in B_ {a,b}(\delta)}\left\|x\right\|^{2}\right)\] \[=\frac{1}{\sqrt{(2\pi)^{d}}}\exp\!\left(-\frac{1}{8}\text{SNR}^{{} ^{\prime}2}_{a,b}(\delta)\right)\] \[=\exp\!\left(-\frac{1-o(1)}{8}\text{SNR}^{{}^{\prime}2}_{a,b} \right).\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer:[Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification:

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No]

Justification:

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Experiment results were averaged. Error bars were inappropriate from a visual standpoint. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer:[No] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper carried out foundational and theoretical analysis on clustering which has no direct social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.