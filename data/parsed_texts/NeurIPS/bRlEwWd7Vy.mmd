# Distributionally Robust Bayesian Optimization

with \(\varphi\)-divergences

Hisham Husain

Amazon

hushisha@amazon.com

&Vu Nguyen

Amazon

vutngn@amazon.com

&Anton van den Hengel

Amazon

hengelah@amazon.com

###### Abstract

The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. [26], which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question _Can one devise a computationally tractable algorithm for solving this DRO-BO problem_? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in \(\varphi\)-divergences, which subsumes many popular choices, such as the \(\chi^{2}\)-divergence, Total Variation, and the extant Kullback-Leibler (KL) divergence. We show that the DRO-BO problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. We then show experimentally that our method surpasses existing methods, attesting to the theoretical results.

## 1 Introduction

Bayesian Optimization (BO) [29, 25, 52, 49, 34] allows us to model a black-box function that is expensive to evaluate, in the case where noisy observations are available. Many important applications of BO correspond to situations where the objective function depends on an additional context parameter [27, 57], for example in health-care, recommender systems can be used to model information about a certain type of medical domain. BO has naturally found success in a number of scientific domains [56, 20, 30, 18, 55] and also a staple in machine learning for the crucial problem of hyperparameter tuning [44, 36, 40, 41, 59].

As with all data-driven approaches, BO is prone to cases where the given data _shifts_ from the data of interest. While BO models this in the form of Gaussian noise for the inputs to the objective function, the context distribution is assumed to be consistent. This can be problematic, for example in healthcare where patient information shifts over time. This problem exists in the larger domain of operations research under the banner of _distributionally robust optimization_ (DRO) [46], where one is interested in being _robust_ against shifts in the distribution observed. In particular, for a given _distance_ between distributions \(\mathsf{D}\), DRO studies robustness against adversaries who are allowed to modify the observed distribution \(p\) to another distribution in the set:

\[\left\{q:\mathsf{D}(p,q)\leq\varepsilon\right\},\]

for some \(\varepsilon>0\). One can interpret this as a ball of radius \(\varepsilon\) for the given choice of \(\mathsf{D}\) and the adversary perturbs the observed distribution \(p\) to \(q\) where \(\varepsilon\) is a form of "budget".

Distributional shift is a topical problem in machine learning and the results of DRO have been specialized in the context of supervised learning [12; 13; 11; 10; 7; 16; 22], reinforcement learning [21] and Bayesian learning [51], as examples. One of the main challenges however is that the DRO is typically intractable since in the general setting of continuous contexts, involves an infinite dimensional constrained optimization problem. The choice of D is crucial here as various choices such as the Wasserstein distance [6; 7; 9; 48], Maximum Mean Discrepancy (MMD) [53] and \(\varphi\)-divergences 1[12; 13] allow for computationally tractable regimes. In particular, these specific choices of D have shown intimate links between regularization [22] which is a conceptually central topic of machine learning.

Footnote 1: as known as \(f\)-divergences in the literature

More recently however, DRO has been studied for the BO setting in Kirschner et al. [26], which as one would expect, leads to a complicated minimax problem, which causes a computational burden practically speaking. Kirschner et al. [26] makes the first step and casts the formal problem however develops an algorithm only in the case where D has been selected as the MMD. While, this work makes the first step and conceptualizes the problem of distributional shifts in context for BO, there are two main practical short-comings. Firstly, the algorithm is developed specifically to the MMD, which is easily computed, however cannot be replaced by another choice of D whose closed form is not readily accessible with samples such as the \(\varphi\)-divergence. Secondly, the algorithm is only tractable when the contexts are finite since at every iteration of BO, it requires solving an \(M\)-dimensional problem where \(M\) is the number of contexts.

The main question that remains is, _can we devise an algorithm that is computationally tractable for tackling the DRO-BO setting_? We answer this question to a large degree of generality by considering distributional shifts against \(\varphi\)-divergences - a large family of divergences consisting of the extant Kullback-Leibler (KL) divergence, Total Variation (TV) and \(\chi^{2}\)-divergence, among others. In particular, we exploit existing advances made in the large literature of DRO to show that the BO objective in this setting for any choice of \(\varphi\)-divergence yields a computationally tractable algorithm, even for the case of continuous contexts. We also present a robust regret analysis that illustrates a sublinear regret. Finally, we show, along with computational tractability, that our method is empirically superior on standard datasets against several baselines including that of Kirschner et al. [26]. In summary, our main contributions are

1. A theoretical result showing that the minimax distributionally robust BO objective with respect to \(\varphi\) divergences is equivalent to a single minimization problem.
2. An efficient algorithm, that works in the continuous context regime, for the specific cases of the \(\chi^{2}\)-divergence and TV distance, which admits a conceptually interesting relationship to regularization of BO.
3. A regret analysis that specifically informs how we can choose the DRO \(\varepsilon\)-budget to attain sublinear regret.

## 2 Related Work

Due to the multifaceted nature of our contribution, we discuss two streams of related literature, one relating to studies of robustness in Bayesian Optimization (BO) and one relating to advances in Distributionally Robust Optimization (DRO).

In terms of BO, the work most closest to ours is Kirschner et al. [26] which casts the distributionally robust optimization problem over contexts. In particular, the work shows how the DRO objective for any choice of divergence D can be cast, which is exactly what we build off. The main drawback of this method however is the limited practical setting due to the expensive inner optimization, which heavily relies on the MMD, and therefore cannot generalize easily to other divergences that are not available in closed forms. Our work in comparison, holds for a much more general class of divergences, and admits a practical algorithm that involves a finite dimensional optimization problem. In particular, we derive the result when D is chosen to be the \(\chi^{2}\)-divergence which we show performs the best empirically. This choice of divergence has been studied in the related problem of Bayesian quadrature [33], and similarly illustrated strong performance, complimenting our results. There also exists work of BO that aim to be robust by modelling adversaries through noise, point estimates or non-cooperative games [37; 32; 3; 39; 47]. The main difference between our work and theirs is that the notion of robustness we tackle is at the _distributional_ level. Another similar work to ours is that of Tay et al. [54] which considers approximating DRO-BO using Taylor expansions based on the sensitivity of the function. In some cases, the results coincide with ours however their result must account for an approximation error in general. Furthermore, an open problem as stated in their work is to solve the DRO-BO problem for continuous context domains, which is precisely one of the advantages of our work.

From the perspective of DRO, our work essentially is an extension of Duchi et al. [12; 13] which develops results that connect \(\varphi\)-divergence DRO to variance regularization. In particular, they assume \(\varphi\) admits a continuous second derivative, which allows them to connect the \(\varphi\)-divergence to the \(\chi^{2}\)-divergence and consequently forms a general connection to constrained variance. While the work is pioneering, this assumption leaves out important \(\varphi\)-divergences such as the Total Variation (TV) - a choice of divergence which we illustrate performs well in comparison to standard baselines in BO. At the technical level, our derivations are similar to Ahmadi-Javid [2] however our result, to the best of our knowledge, is the first such work that develops it in the context of BO. In particular, our results for the Total Variation and \(\chi^{2}\)-divergence show that variance is a key penalty in ensuring robustness which is a well-known phenomena existing in the realm of machine learning [12; 11; 10; 22; 1].

## 3 Preliminaries

Bayesian OptimizationWe consider optimizing a _black-box_ function, \(f:\mathcal{X}\rightarrow\mathbb{R}\) with respect to the _input_ space \(\mathcal{X}\subseteq\mathbb{R}^{d}\). As a black-box function, we do not have access to \(f\) directly however receive input in a sequential manner: at time step \(t\), the learner chooses some input \(\mathbf{x}_{t}\in\mathcal{X}\) and observes the _reward_\(y_{t}=f(\mathbf{x}_{t})+\eta_{t}\) where the noise \(\eta_{t}\sim\mathcal{N}(0,\sigma_{f}^{2})\) and \(\sigma_{f}^{2}\) is the output noise variance. Therefore, the goal is to optimize

\[\sup_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x}).\]

Additional to the input space \(\mathcal{X}\), we introduce the _context_ spaces \(\mathcal{C}\), which we assume to be compact. These spaces are assumed to be separable completely metrizable topological spaces.2 We have a reward function, \(f:\mathcal{X}\times\mathcal{C}\rightarrow\mathbb{R}\) which we are interested in optimizing with respect to \(\mathcal{X}\). Similar to sequential optimization, at time step \(t\) the learner chooses some input \(\mathbf{x}_{t}\in\mathcal{X}\) and receives a context \(c_{t}\in\mathcal{C}\) and \(f(\mathbf{x}_{t},c_{t})+\eta_{t}\). Here, the learner can not choose a context \(c_{t}\), but receive it from the environment. Given the context information, the objective function is written as

Footnote 2: We remark that this is an extremely mild condition, satisfied by the large majority of considered examples.

\[\sup_{\mathbf{x}\in\mathcal{X}}\mathbb{E}_{c\sim p}[f(\mathbf{x},c)],\]

where \(p\) is a probability distribution over contexts.

Gaussian ProcessesWe follow a popular choice in BO [49] to use GP as a surrogate model for optimizing \(f\). A GP [43] defines a probability distribution over functions \(f\) under the assumption that any subset of points \(\left\{\mathbf{x}_{i},f(\mathbf{x}_{i})\right\}\) is normally distributed. Formally, this is denoted as:

\[f(\mathbf{x})\sim\text{GP}\left(m(\mathbf{x}),k(\mathbf{x},\mathbf{x}^{\prime })\right),\]

where \(m\left(\mathbf{x}\right)\) and \(k\left(\mathbf{x},\mathbf{x}^{\prime}\right)\) are the mean and covariance functions, given by \(m(\mathbf{x})=\mathbb{E}\left[f(\mathbf{x})\right]\) and \(k(\mathbf{x},\mathbf{x}^{\prime})=\mathbb{E}\left[(f(\mathbf{x})-m(\mathbf{x}) )(f(\mathbf{x}^{\prime})-m(\mathbf{x}^{\prime}))^{T}\right]\). For predicting \(f_{*}=f\left(\mathbf{x}_{*}\right)\) at a new data point \(\mathbf{x}_{*}\), the conditional probability follows a univariate Gaussian distribution as \(p\big{(}f_{*}\mid\mathbf{x}_{*},[\mathbf{x}_{1}...\mathbf{x}_{N}],[y_{1},....y_{N}]\big{)}\sim\mathcal{N}\left(\mu\left(\mathbf{x}_{*}\right),\sigma^{2} \left(\mathbf{x}_{*}\right)\right)\). Its mean and variance are given by:

\[\mu\left(\mathbf{x}_{*}\right)= \mathbf{k}_{*,N}\mathbf{K}_{N,N}^{-1}\mathbf{y},\] (1) \[\sigma^{2}\left(\mathbf{x}_{*}\right)= k_{**}-\mathbf{k}_{*,N}\mathbf{K}_{N,N}^{-1}\mathbf{k}_{*,N}^{T}\] (2)

where \(k_{**}=k\left(\mathbf{x}_{*},\mathbf{x}_{*}\right)\), \(\mathbf{k}_{*,N}=[k\left(\mathbf{x}_{*},\mathbf{x}_{i}\right)]_{\forall i\leq N}\) and \(\boldsymbol{K}_{N,N}=\left[k\left(\mathbf{x}_{i},\mathbf{x}_{j}\right)\right]_{ \forall i,j\leq N}\). As GPs give full uncertainty information with any prediction, they provide a flexible nonparametric prior for Bayesian optimization. We refer to Rasmussen and Williams [43] for further details on GPs.

Distributional RobustnessLet \(\Delta(\mathcal{C})\) denote the set of probability distributions over \(\mathcal{C}\). A _divergence_ between distributions \(\mathsf{D}:\Delta(\mathcal{C})\times\Delta(\mathcal{C})\rightarrow\mathbb{R}\) is a dissimilarity measure that satisfies\(\Delta(p,q)\geq 0\) with equality if and only if \(p=q\) for \(p,q\in\Delta(\mathcal{C})\). For a function, \(h:\mathcal{C}\rightarrow\mathbb{R}\), base probability measure \(p\in\Delta(\mathcal{C})\), the central concern of Distributionally Robust Optimization (DRO) [4; 42; 5] is to compute

\[\sup_{q\in B_{\varepsilon,0}(p)}\mathbb{E}_{q(c)}[h(c)],\] (3)

where \(B_{\varepsilon,\mathsf{D}}(p)=\{q\in\Delta(\mathcal{C}):\mathsf{D}(p,q)\leq\varepsilon\}\), is ball of distributions \(q\) that are \(\varepsilon\) away from \(p\) with respect to the divergence \(\mathsf{D}\). The objective in Eq. (3) is intractable, especially in setting where \(\mathcal{C}\) is continuous as it amounts to a constrained infinite dimensional optimization problem. It is also clear that the choice of \(\mathsf{D}\) is crucial for both computational and conceptual purposes. The vast majority of choices typically include the Wasserstein due to the transportation-theoretic interpretation and with a large portion of existing literature finding connections to Lipschitz regularization [6; 7; 9; 48]. Other choices where they have been studied in the supervised learning setting include the Maximum Mean Discrepancy (MMD) [53] and \(\varphi\)-divergences [12; 13].

Distributionally Robust Bayesian OptimizationRecently, the notion of DRO has been applied to BO [26; 54], who consider robustness with respect to shifts in the context space and therefore are interested in solving

\[\sup_{\mathbf{x}\in\mathcal{X}}\inf_{q\in B_{\varepsilon,\mathsf{D}}(p)} \mathbb{E}_{c\sim q}[f(\mathbf{x},c)],\]

where \(p\) is the reference distribution. This objective becomes significantly more difficult to deal with since not only does it involve a constrained and possibly infinite dimensional optimization problem however also involves a minimax which can cause instability issues if solved iteratively.

Kirschner et al. [26] tackle these problems by letting \(\mathsf{D}\) be the kernel Maximum Mean Discrepancy (MMD), which is a popular choice of discrepancy motivated by kernel mean embeddings [19]. In particular, the MMD can be efficiently estimated in \(O(n^{2})\) where \(n\) is the number of samples. Naturally, this has two main drawbacks: The first is that it is still computationally expensive since one is required to solve two optimization problems, which can lead to instability and secondly, the resulting algorithm is limited to the scheme where the number of contexts is finite. In our work, we consider \(\mathsf{D}\) to be a \(\varphi\)-divergence, which includes the Total Variance, \(\chi^{2}\) and Kullback-Leibler (KL) divergence and furthermore show that minmax objective can be reduced to a single maximum optimization problem which resolves both the instability and finiteness assumption. In particular, we also present a similar analysis, showing that the robust regret decays sublinearly for the right choices of radii.

## 4 \(\varphi\)-Robust Bayesian Optimization

In this section, we present the main result on distributionally robustness when applied to BO using \(\varphi\)-divergence. Therefore, we begin by defining this key quantity.

**Definition 1** (\(\varphi\)-divergence): _Let \(\varphi:\mathbb{R}\rightarrow(-\infty,\infty]\) be a convex, lower semi-continuous function such that \(\varphi(1)=0\). The \(\varphi\)-divergence between \(p,q\in\Delta(\mathcal{C})\) is defined as_

\[\mathsf{D}_{\varphi}(p,q)=\mathbb{E}_{q(c)}\left[\varphi\left(\frac{dp}{dq}( c)\right)\right],\]

_where \(dp/dq\) is the Radon-Nikodym derivative if \(p\ll q\) and \(\mathsf{D}_{\varphi}(p,q)=+\infty\) otherwise._

Popular choices of the convex function \(\varphi\) include \(\varphi(u)=(u-1)^{2}\) which yields the \(\chi^{2}\) and, \(\varphi(u)=|u-1|\), \(\varphi(u)=u\log u\) which correspond to the \(\chi^{2}\) and KL divergences respectively. At any time step \(t\geq 1\), we consider distributional shifts with respect to an \(\varphi\)-divergence for any choice of \(\varphi\) and therefore relevantly define the DRO ball as

\[B_{\varphi}^{t}(p_{t}):=\left\{q\in\Delta(\mathcal{C}):\mathsf{D}_{\varphi}(q, p_{t})\leq\varepsilon_{t}\right\},\]

where \(p_{t}=\frac{1}{t}\sum_{s=1}^{t}\delta_{c_{s}}\) is the reference distribution and \(\varepsilon_{t}\) is the distributionally robust radius chosen at time \(t\). We remark that for our results, the choice of \(p_{t}\) is flexible and can be chosen based on the specific domain application. The \(\varphi\) divergence, as noted from the definition above, is only defined finitely when the measures \(p,q\) are absolutely continuous to each other and there is regarded as a _strong_ divergence in comparison to the Maximum Mean Discrepancy (MMD), which is utilized in Kirschner et al. [26]. The main consequence of this property is that the geometry of the ball \(B_{\varepsilon}^{t}\) would differ based on the choice of \(\varphi\)-divergence. The \(\varphi\)-divergence is a very popular choice for defining this ball in previous studies of DRO in the context of supervised learning due to the connections and links it has found to variance regularization [12, 13, 11].

We will exploit various properties of the \(\varphi\)-divergence to derive a result that reaps the benefits of this choice such as a reduced optimization problem - a development that does not currently exist for the MMD [26]. We first define the convex conjugate of \(\varphi\) as \(\varphi^{\star}(u)=\sup_{u^{\prime}\in\mathrm{dom}_{\varphi}}\left(u\cdot u^{ \prime}-\varphi(u^{\prime})\right)\), which we note is a standard function that is readily available in closed form for many choices of \(\varphi\).

**Theorem 1**: _Let \(\varphi:\mathbb{R}\to(-\infty,\infty]\) be a convex lower semicontinuous mapping such that \(\varphi(1)=0\). Let \(f\) be measurable and bounded. For any \(\varepsilon>0\), it holds that_

\[\sup_{\mathbf{x}\in\mathcal{X}}\inf_{q\in B_{\varphi}^{t}(p)}\mathbb{E}_{c \sim q}[f(\mathbf{x},c)]=\sup_{\mathbf{x}\in\mathcal{X},\lambda\geq 0,b\in \mathbb{R}}\left(b-\lambda\varepsilon_{t}-\lambda\mathbb{E}_{p_{t}(c)}\left[ \varphi^{\star}\left(\frac{b-f(\mathbf{x},c)}{\lambda}\right)\right]\right).\]

**Proof (Sketch)** The proof begins by rewriting the constraint over the \(\varphi\)-divergence constrained ball with the use of Lagrangian multipliers. Using existing identities for \(f\)-divergences, a minimax swap yields a two-dimensional optimization problem, over \(\lambda\geq 0\) and \(b\in\mathbb{R}\).

We remark that similar results exist for other areas such as supervised learning [50], robust optimization [4] and certifying robust radii [14]. However this is, to the best of our knowledge, the first development when applied to optimizing expensive black-box functions, the case of BO. The above Theorem is practically compelling for three main reasons. First, one can note that compared to the left-hand side, the result converts this into a single optimization (max) over three variables, where two of the variables are \(1\)-dimensional, reducing the computational burden significantly. Secondly, the notoriously difficult max-min problem becomes only a max, leaving behind instabilities one would encounter with the former objective. Finally, the result makes very mild assumptions on the context parameter space \(\mathcal{C}\), allowing infinite spaces to be chosen, which is one of the challenges for existing BO advancements. We show that for specific choices of \(\varphi\), the optimization over \(b\) and even \(\lambda\) can be expressed in closed form and thus simplified. All proofs for the following examples can be found in the Appendix Section 8.

**Example 2** (\(\chi^{2}\)-divergence): _Let \(\varphi(u)=(u-1)^{2}\), then for any measurable and bounded \(f\) we have for any choice of \(\varepsilon_{t}\)_

\[\sup_{\mathbf{x}\in\mathcal{X}}\inf_{q\in B_{\varphi}^{t}(p_{t})}\mathbb{E}_{c \sim q}[f(\mathbf{x},c)]=\sup_{\mathbf{x}\in\mathcal{X}}\left(\mathbb{E}_{p_{ t}(c)}[f(\mathbf{x},c)]-\sqrt{\varepsilon_{t}\cdot\mathrm{Var}_{p_{t}(c)}[f( \mathbf{x},c)]}\right).\]

The above example can be easily implemented as it involves the same optimization problem however now appended with a variance term. Furthermore, this objective admits a compelling conceptual insight which is that, by enforcing a penalty in the form of variance, one attains robustness. The idea that regularization provides guidance to robustness or generalization is well-founded in machine learning more generally for example in supervised learning [12, 13]. We remark that this penalty and its relationship to \(\chi^{2}\)-divergence has been developed in the similar yet related problem of Bayesian quadrature [33]. Moreover, it can be shown that if \(\varphi\) is twice differentiable then \(\mathsf{D}_{\varphi}\) can be approximated by the \(\chi^{2}\)-divergence via Taylor series, which makes \(\chi^{2}\)-divergence a centrally appealing choice for studying robustness. We now derive the result for a popular choice of \(\varphi\) that is not differentiable.

**Example 3** (Total Variation): _Let \(\varphi(u)=|u-1|\), then for any measurable and bounded \(f\) we have for any choice of \(\varepsilon_{t}\)_

\[\sup_{\mathbf{x}\in\mathcal{X}}\inf_{q\in B_{\varphi}^{t}(p_{t})}\mathbb{E}_ {c\sim q}[f(\mathbf{x},c)]=\sup_{\mathbf{x}\in\mathcal{X}}\left(\mathbb{E}_{p _{t}(c)}[f(\mathbf{x},c)]-\frac{\varepsilon_{t}}{2}\left(\sup_{c\in\mathcal{C }}f(\mathbf{x},c)-\inf_{c\in\mathcal{C}}f(\mathbf{x},c)\right)\right).\]

Similar to the \(\chi^{2}\)-case, the result here admits a variance-like term in the form of the difference between the maximal and minimal elements. We remark that such a result is conceptually interesting since both losses admit an objective that resembles a mean-variance which is a natural concept in ML, but advocates for it from the perspective of distributional robustness. This result exists for the supervised learning in Duchi and Namkoong [11] however is completely novel for BO and also holds for a choice of non-differentiable \(\varphi\), hinting at the deeper connection between \(\varphi\)-divergence DRO and variance regularization.

### Optimization with the GP Surrogate

To handle the distributional robustness, we have rewritten the objective function using \(\varphi\) divergences in Theorem 1. In DRBO setting, we sequentially select a next point \(\mathbf{x}_{t}\) for querying a black-box function. Given the observed context \(c_{t}\sim q\) coming from the environment, we evaluate the black-box function and observe the output as \(y_{t}=f(\mathbf{x}_{t},c_{t})+\eta_{t}\) where the noise \(\eta_{t}\sim\mathcal{N}(0,\sigma_{f}^{2})\) and \(\sigma_{f}^{2}\) is the noise variance.

As a common practice in BO, at the iteration \(t\), we model the GP surrogate model using the observed data \(\{\mathbf{x}_{i},y_{i}\}_{i=1}^{t-1}\) and make a decision by maximizing the acquisition function which is build on top of the GP surrogate:

\[\mathbf{x}_{t}=\arg\max_{\mathbf{x}\in\mathcal{X}}\alpha(\mathbf{x}).\]

While our method is not restricted to the form of the acquisition function, for convenience in the theoretical analysis, we follow the GP-UCB [52]. Given the GP predictive mean and variance from Eqs. (7,8), we have the acquisition function for the \(\chi^{2}\) in Example 2 as follows:

\[\alpha^{\chi^{2}}(\mathbf{x}):= \frac{1}{|C|}\sum_{c}\Big{[}\mu_{t}(\mathbf{x},c)+\sqrt{\beta_{t} }\sigma_{t}(\mathbf{x},c)\Big{]}-\sqrt{\frac{\varepsilon_{t}}{|C|}\sum_{c} \big{(}\mu_{t}(\mathbf{x},c)-\bar{\mu}_{t}\big{)}^{2}}\] (4)

where \(\beta_{t}\) is a explore-exploit hyperparameter defined in Srinivas et al. [52], \(\bar{\mu_{t}}=\frac{1}{|C|}\sum_{c}\mu_{t}(\mathbf{x},c)\) and \(c\sim q\) can be generated in a one dimensional space to approximate the expectation and the variance. In the experiment, we select \(q\) as the uniform distribution, but it is not restricted to. Similarly, an acquisition function for Total Variation in Example 3 is written as

\[\alpha^{TV}(\mathbf{x}):= \frac{1}{|C|}\sum_{c}\Big{[}\mu_{t}(\mathbf{x},c)+\sqrt{\beta_{t} }\sigma_{t}(\mathbf{x},c)\Big{]}-\frac{\varepsilon_{t}}{2}\big{(}\max\mu_{t}( \mathbf{x},c)-\min\mu_{t}(\mathbf{x},c)\big{)}.\] (5)

We summarize all computational steps in Algorithm 1.

Computational Efficiency against MMD.We make an important remark that since we do not require our context space to be finite, our implementation scales only linearly with the number of context samples \(|C|\) drawing from \(q\). This allows us to discretize our space and draw as many context samples as required while only paying a linear price. On the other hand, the MMD [26] at every iteration of \(t\) requires solving an \(|C|\)-dimensional constraint optimization problem that has no closed form solution. We refer to Section 5.2 for the empirical comparison.

```
1:Input: Max iteration \(T\), initial data \(D_{0}\), \(\eta\)
2:for\(t=1,\dots,T\)do
3: Fit and estimate GP hyperparameter given \(D_{t-1}\)
4: Select a next input \(\mathbf{x}_{t}=\arg\max\alpha(\mathbf{x})\)
5:\(\chi^{2}\)-divergence: \(\alpha(\mathbf{x}):=\alpha^{\chi^{2}}(\mathbf{x})\) from Eq. (4)
6: Total Variation: \(\alpha(\mathbf{x}):=\alpha^{TV}(\mathbf{x})\) from Eq. (5)
7: Observe a context \(c_{t}\sim q\)
8: Evaluate the black-box \(y_{t}=f(\mathbf{x}_{t},c_{t})+\eta_{t}\)
9: Augment \(D_{t}=D_{t-1}\cup(\mathbf{x}_{t},c_{t},y_{t})\)
10:endfor ```

**Algorithm 1** DRBO with \(\varphi\)-divergence

### Convergence Analysis

One of the main advantages of Kirschner et al. [26] is the choice of MMD makes the regret analysis simpler due to the nice structure and properties of MMD. In particular, the MMD is well-celebrated for a \(O(t^{-1/2})\) convergence where no such results exist for \(\varphi\)-divergences. However, using Theorem 1, we can show a regret bound for the Total Variation with a simple boundedness assumption and show how one can extend this result to other \(\varphi\)-divergences. We begin by defining the _robust regret_, \(R_{T}\), with \(\varphi\)-divergence balls:

\[R_{T}(\varphi)=\sum_{t=1}^{T}\inf_{q\in B_{\varphi}^{t}}\mathbb{E}_{q(c)}[f( \mathbf{x}_{t}^{*},c)]-\inf_{q\in B_{\varphi}^{t}}\mathbb{E}_{q(c)}[f(\mathbf{ x}_{t},c)],\] (6)where \(\mathbf{x}_{t}^{\star}=\operatorname*{arg\,max}_{\mathbf{x}\in\mathcal{X}}\inf_{q \in B_{\varepsilon,\varphi}^{t}}\mathbb{E}_{q(c)}[f(\mathbf{x},c)]\). We use \(\bm{K}_{t}\) to denote the generated kernel matrix from dataset \(D_{t}=\{(\mathbf{x}_{i},c_{i})\}_{i=1}^{t}\subset\mathcal{X}\times\mathcal{C}\). we now introduce a standard quantity in regret analysis in BO is the _maximum information gain_: \(\gamma_{t}=\max_{D\subset\mathcal{X}\times\mathcal{C}:|D|=t}\log\det\Big{(} \mathbf{I}_{t}+\sigma_{f}^{-2}\bm{K}_{t}\Big{)}\) where \(\bm{K}_{t}=\left[k\left([\mathbf{x}_{i},c_{i}],[\mathbf{x}_{j},c_{j}]\right) \right]_{\forall i,j\leq t}\) is the covariance matrix and \(\sigma_{f}^{2}\) is the output noise variance.

**Theorem 4** (\(\varphi\)-divergence Regret): _Suppose the target function is bounded, meaning that \(M=\sup_{(\mathbf{x},c)\in\mathcal{X}\times\mathcal{C}}|f(\mathbf{x},c)|<\infty\) and suppose \(f\) has bounded RKHS norm with respect to \(k\). For any lower semicontinuous convex \(\varphi:\mathbb{R}\to(-\infty,\infty]\) with \(\varphi(1)=0\), if there exists a monotonic invertible function \(\Gamma_{\varphi}:[0,\infty)\to\mathbb{R}\) such that \(\mathrm{TV}(p,q)\leq\Gamma_{\varphi}(\mathsf{D}_{\varphi}(p,q))\), the following holds_

\[R_{T}(\varphi)\leq\frac{\sqrt{8T\beta_{T}\gamma_{T}}}{\log(1+\sigma_{f}^{-2})} +\left(2M+\sqrt{\beta_{T}}\right)\sum_{t=1}^{T}\Gamma_{\varphi}(\varepsilon_{ t}),\]

_with probability \(1-\delta\), where \(\beta_{t}=2||f||_{k}^{2}+300\gamma_{t}\ln^{3}(t/\delta)\), \(\gamma_{t}\) is the maximum information gain as defined above, and \(\sigma_{f}\) is the standard deviation of the output noise._

The full proof can be found in the Appendix Section 8. We first remark that with regularity assumptions on \(f\), sublinear analytical bounds for \(\gamma_{T}\) are known for a range of kernels, e.g., given \(\mathcal{X}\times\mathcal{C}\subset\mathbb{R}^{d+1}\) we have for the RBF kernel, \(\gamma_{T}\leq\mathcal{O}\left(\log(T)^{d+2}\right)\) or for the Matern kernel with \(\nu>1\), \(\gamma_{T}\leq\mathcal{O}\left(T^{\frac{(d+1)(\delta+2)}{2\nu(d+1)(\delta+2)} }(\log T)\right)\). The second term in the bound is directly a consequence of DRO and by selecting \(\varepsilon_{t}=0\), it will vanish since any such \(\Gamma_{\varphi}\) will satisfy \(\Gamma_{\varphi}(0)=0\). To ensure sublinear regret, we can select \(\varepsilon_{t}=\Gamma_{\varphi}^{-1}\left(\frac{1}{\sqrt{t}+\sqrt{t+1}}\right)\), noting that the second term will reduce to \(\sum_{t=1}^{T}\varepsilon_{t}\leq\sqrt{T}\). Finally, we remark that the existence of \(\Gamma_{\varphi}\) is not so stringent since for a wide choices of \(\varphi\), one can find inequalities between the Total Variation and \(D_{\varphi}\), to which we refer the reader to Sason and Verdu [45]. For the examples discussed above, we can select \(\Gamma_{\varphi}(t)=t\) for the TV. For the \(\chi^{2}\) and \(\mathrm{KL}\) cases, one can choose \(\Gamma_{\chi^{2}}(b)=2\sqrt{\frac{b}{1+b}}\) and \(\Gamma_{\mathrm{KL}}(b)=1-\exp(-b)\).

Figure 1: Two settings in DRO when the stochastic solution and robust solution are different (_top_) and identical (_bottom_). _Left_: original function \(f(\mathbf{x},c)\). _Middle_: selection of input \(\mathbf{x}_{t}\) over iterations. _Right_: performance with different \(\varepsilon\).

## 5 Experiments

**Experimental setting.** The experiments are repeated using \(30\) independent runs. We set \(|C|=30\) which should be sufficient to draw \(c\stackrel{{\text{iid}}}{{\sim}}q\) in one-dimensional space to compute Eqs. (4,5). We optimize the GP hyperparameter (e.g., learning rate) by maximizing the GP log marginal likelihood [43]. We will release the Python implementation code in the final version.

**Baselines.** We consider the following baselines for comparisons. _Rand_: we randomly select \(\mathbf{x}_{t}\) irrespective of \(c_{t}\). _BO_: we follow the GP-UCB [52] to perform standard Bayesian optimization (ignoring the context \(c_{t}\)). The selection at each iteration is \(\mathbf{x}_{t}=\operatorname*{argmax}_{\mathbf{x}}\mu(\mathbf{x})+\beta_{t} \sigma(\mathbf{x})\). _Stable-Opt_: we consider the worst-case robust optimization presented in Bogunovic et al. [8]. The selection at each iteration \(\mathbf{x}_{t}=\operatorname*{argmax}_{\mathbf{x}}\operatorname*{argmin}_{c }\mu(\mathbf{x},c)+\beta_{t}\sigma(\mathbf{x},c)\). _DRBO MMD_[26]: Since there is no official implementation available, we have tried our best to re-implement the algorithm.

We consider the popular benchmark functions3 with different dimensions \(d\). To create a context variable \(c\), we pick the last dimension of these functions to be the context input while the remaining \(d-1\) dimension becomes the input \(\mathbf{x}\).

Footnote 3: https://www.sfu.ca/ ssurjano/optimization.html

### Ablation Studies

To gain understanding into how our framework works, we consider two popular settings below.

**DRBO solution is different from stochastic solution.** In Fig. 0(a), the vanilla BO tends to converge greedily toward the stochastic solution (non-distributionally robust) \(\operatorname*{argmax}_{\mathbf{x}}f(\mathbf{x},\cdot)\). Thus, BO keeps exploiting in the locality of \(\operatorname*{argmax}_{\mathbf{x}}f(\mathbf{x},\cdot)\) from iteration \(15\). On the other hand, all other DRBO methods will keep exploring to seek for the distributionally robust solutions. Using the high value of \(\varepsilon_{t}\in\{0.5,1\}\) will result in the best performance.

**DRBO solution is identical to stochastic solution.** When the stochastic and robust solutions coincide at the same input \(\mathbf{x}^{*}\), the solution of BO will be equivalent to the solution of DRBO methods. This is demonstrated by Fig. 0(b). Both stochastic and robust approaches will quickly identify the optimal solution (see the \(\mathbf{x}_{t}\) selection). We learn empirically that setting \(\varepsilon_{t}\to 0\) will lead to the best performance. This is because the DRBO setting will become the standard BO.

The best choice of \(\varepsilon\) depends on the property of the underlying function, e.g., the gap between the stochastic and DRBO solutions. In practice, we may not be able to identify these scenarios in advance. Therefore, we can use the adaptive value of \(\varepsilon_{t}\) presented in Section 4.2. Using this adaptive setting, the performance is stable, as illustrated in the figures.

### Computational efficiency

The key benefit of our framework is simplifying the existing intractable computation by providing the closed-form solution. Additional to improving the quality, we demonstrate this advantage in terms of computational complexity. Our main baseline for comparison is the MMD [26]. As shown in Fig. 2, our DRBO is consistently faster than the constraints linear programming approximation used for

Figure 3: Cumulative robust regret across algorithms. The results show that the proposed \(\chi^{2}\) and TV achieve the best performance across benchmark functions. Random and vanilla BO approaches perform poorly which do not take into account the robustness criteria. Best viewed in color.

MMD. This gap is substantial in higher dimensions. In particular, as compared to Kirschner et al. [26], our DRBO is \(5\)-times faster in _5d_ and \(10\)-times faster in _6d_.

### Optimization performance comparison

We compare the algorithms in Fig. 3 using the robust (cumulative) regret defined in Eq. (6) which is commonly used in DRO literature [33; 26]. The random approach does not make any intelligent information in making decision, thus performs the worst. While BO performs better than random, it is still inferior comparing to other distributionally robust optimization approaches. The reason is that BO does not take into account the context information in making the decision. The StableOpt [8] performs relatively well that considers the worst scenarios in the subset of predefined context. This predefined subset can not cover all possible cases as opposed to the distributional robustness setting.

The MMD approach [26] needs to solve the inner adversary problem using linear programming with convex constraints, additional to the main optimization step. As a result, the performance of MMD is not as strong as our TV and \(\chi^{2}\). Our proposed approach does not suffer this pathology and thus scale well in continuous and high dimensional settings of context input \(c\).

Real-world functions.We consider the deterministic version of the robot pushing objective from Wang and Jegelka [60]. The goal is to find a good pre-image for pushing an object to a target location. The 3-dimensional function takes as input the robot location \((r_{x},r_{y})\in[-5,5]^{2}\) and pushing duration \(r_{t}\in[1,30]\). We follow Bogunovic et al. [8] to twist this problem in which there is uncertainty regarding the precise target location, so one seeks a set of input parameters that is robust against a number of different potential pushing duration which is a context.

We perform an experiment on Wind Power dataset [8] and vary the context dimensions \(|C|\in\{30,100,500\}\) in Fig. 4. When \(|C|\) enlarges, our DRBO \(\chi^{2}\), TV and KL improves. However, the performances do not improve further when increasing \(|C|\) from \(100\) to \(500\). Similarly, MMD improves with \(|C|\), but it comes with the quadratic cost w.r.t. \(|C|\). Overall, our proposed DRBO still performs favourably in terms of optimization quality and computational cost than the MMD.

## 6 Conclusions, Limitations and Future works

In this work, we showed how one can study the DRBO formulation with respect to \(\varphi\)-divergences and derived a new algorithm that removes much of the computational burden, along with a sublinear regret bound. We compared the performance of our method against others, and showed that our results unveil a deeper connection between regularization and robustness, which serves useful conceptually.

Limitations and Future WorksOne of the limitations of our framework is in the choice of \(\varphi\), for which we provide no guidance. For different applications, different choices of \(\varphi\) would prove to be more useful, the study of which we leave for future work.

Figure 4: All divergences improve with larger \(|C|\). However, MMD comes with the quadratic cost.

Figure 2: We compare the computational cost across methods. Our proposed DRBO using \(\chi^{2}\) and TV take similar cost per iteration which is significantly lower than the DRBO MMD [26].

## Acknowledgements

We would like to thank the anonymous reviewers for providing feedback.

## References

* [1] Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic regression. In _Advances in Neural Information Processing Systems_, pages 1576-1584, 2015.
* [2] Amir Ahmadi-Javid. Entropic value-at-risk: A new coherent risk measure. _Journal of Optimization Theory and Applications_, 155(3):1105-1123, 2012.
* [3] Justin J Beland and Prasanth B Nair. Bayesian optimization under uncertainty. In _NIPS BayesOpt 2017 workshop_, 2017.
* [4] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. _Management Science_, 59(2):341-357, 2013.
* [5] M Bennouna and Bart PG Van Parys. Learning and decision-making with data: Optimal formulations and phase transitions. _arXiv preprint arXiv:2109.06911_, 2021.
* [6] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* [7] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.
* [8] Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher. Adversarially robust optimization with Gaussian processes. In _Conference on Neural Information Processing Systems (NIPS)_, number CONF, 2018.
* [9] Zac Cranko, Simon Kornblith, Zhan Shi, and Richard Nock. Lipschitz networks and distributional robustness. _arXiv preprint arXiv:1809.01129_, 2018.
* [10] Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised lipschitz regularisation equals distributional robustness. In _International Conference on Machine Learning_, pages 2178-2188. PMLR, 2021.
* [11] John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. _The Journal of Machine Learning Research_, 20(1):2450-2504, 2019.
* [12] John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax rates. In _FOCS_, 2013.
* [13] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. _Mathematics of Operations Research_, 46(3):946-969, 2021.
* [14] KD Dvijotham, J Hayes, B Balle, Z Kolter, C Qin, A Gyorgy, K Xiao, S Gowal, and P Kohli. A framework for robustness certification of smoothed classifiers using f-divergences. In _International Conference on Learning Representations_, 2020.
* [15] Ky Fan. Minimax theorems. _Proceedings of the National Academy of Sciences of the United States of America_, 39(1):42, 1953.
* [16] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributional robustness and regularization in statistical learning. _arXiv e-prints_, pages arXiv-1712, 2017.
* [17] Ziv Goldfeld, Kristjan Greenewald, Jonathan Niles-Weed, and Yury Polyanskiy. Convergence of smoothed empirical measures with applications to entropy estimation. _IEEE Transactions on Information Theory_, 66(7):4368-4391, 2020.

* [18] Shivapratap Gopakumar, Sunil Gupta, Santu Rana, Vu Nguyen, and Svetha Venkatesh. Algorithmic assurance: An active approach to algorithmic testing using bayesian optimisation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [19] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [20] Jose Miguel Hernandez-Lobato, James Requeima, Edward O Pyzer-Knapp, and Alan Aspuru-Guzik. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. _In International Conference on Machine Learning_, pages 1470-1479, 2017.
* [21] Linfang Hou, Liang Pang, Xin Hong, Yanyan Lan, Zhiming Ma, and Dawei Yin. Robust reinforcement learning with wasserstein constraint. _arXiv preprint arXiv:2006.00945_, 2020.
* [22] Higham Husain. Distributional robustness with ipms and links to regularization and gans. _Advances in Neural Information Processing Systems_, 33:11816-11827, 2020.
* [23] Higham Husain and Jeremias Knoblauch. Adversarial interpretation of bayesian inference. In _International Conference on Algorithmic Learning Theory_, pages 553-572. PMLR, 2022.
* [24] Higham Husain, Richard Nock, and Robert C Williamson. A primal-dual link between gans and autoencoders. In _Advances in Neural Information Processing Systems_, pages 413-422, 2019.
* [25] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. _Journal of Global Optimization_, 13(4):455-492, 1998.
* [26] Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause. Distributionally robust Bayesian optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 2174-2184. PMLR, 2020.
* [27] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In _Advances in Neural Information Processing Systems_, pages 2447-2455, 2011.
* [28] Solomon Kullback and Richard A Leibler. On information and sufficiency. _The Annals of Mathematical Statistics_, 22(1):79-86, 1951.
* [29] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. _Journal of Basic Engineering_, 86(1):97-106, 1964.
* [30] Cheng Li, Rana Santu, Sunil Gupta, Vu Nguyen, Svetha Venkatesh, Alessandra Sutti, David Rubin De Celis Leal, Teo Slezak, Murray Height, Mazher Mohammed, and Ian Gibson. Accelerating experimental design by incorporating experimenter hunches. In _International Conference on Data Mining_, pages 257-266, 2018.
* [31] Shuang Liu and Kamalika Chaudhuri. The inductive bias of restricted f-gans. _arXiv preprint arXiv:1809.04542_, 2018.
* [32] Ruben Martinez-Cantin, Kevin Tee, and Michael McCourt. Practical Bayesian optimization in the presence of outliers. In _International Conference on Artificial Intelligence and Statistics_, pages 1722-1731. PMLR, 2018.
* [33] Thanh Nguyen, Sunil Gupta, Huong Ha, Santu Rana, and Svetha Venkatesh. Distributionally robust Bayesian quadrature optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 1921-1931. PMLR, 2020.
* [34] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian optimization. In _International Conference on Machine Learning_, pages 7317-7326, 2020.
* [35] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In _Proceedings of The 9th Asian Conference on Machine Learning (ACML)_, pages 279-294, 2017.

* [36] Vu Nguyen, Vaden Masrani, Rob Brekelmans, Michael Osborne, and Frank Wood. Gaussian process bandit optimization of the thermodynamic variational objective. _Advances in Neural Information Processing Systems_, 33, 2020.
* [37] Jose Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino, and Lorenzo Jamone. Unscented Bayesian optimization for safe robot grasping. In _2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1967-1972. IEEE, 2016.
* [38] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In _Advances in Neural Information Processing Systems_, pages 271-279, 2016.
* [39] Rafael Oliveira, Lionel Ott, and Fabio Ramos. Bayesian optimisation under uncertain inputs. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1177-1184. PMLR, 2019.
* [40] Jack Parker-Holder, Vu Nguyen, and Stephen J Roberts. Provably efficient online hyperparameter optimization with population-based bandits. _Advances in neural information processing systems_, 33:17200-17211, 2020.
* [41] Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi, Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton, Jean Baptiste Faddoul, et al. Amazon sagemaker automatic model tuning: Scalable gradient-free optimization. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 3463-3471, 2021.
* [42] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. _arXiv preprint arXiv:1908.05659_, 2019.
* [43] Carl E Rasmussen and Christopher K I Williams. _Gaussian Processes for Machine Learning_. MIT Prees, 2006.
* [44] Binxin Ru, Ahsan Alvi, Vu Nguyen, Michael A Osborne, and Stephen Roberts. Bayesian optimisation over multiple continuous and categorical inputs. In _International Conference on Machine Learning_, pages 8276-8285. PMLR, 2020.
* [45] Igal Sason and Sergio Verdu. \(f\)-divergence inequalities. _IEEE Transactions on Information Theory_, 62(11):5973-6006, 2016.
* [46] Herbert E Scarf. A min-max solution of an inventory problem. Technical report, RAND CORP SANTA MONICA CALIF, 1957.
* [47] Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, and Andreas Krause. No-regret learning in unknown games with correlated payoffs. _Advances in Neural Information Processing Systems_, 32:13624-13633, 2019.
* [48] Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. _Journal of Machine Learning Research_, 20(103):1-68, 2019.
* [49] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. _Proceedings of the IEEE_, 104(1):148-175, 2016.
* [50] Alexander Shapiro. Distributionally robust stochastic programming. _SIAM Journal on Optimization_, 27(4):2258-2275, 2017.
* [51] Alexander Shapiro, Enlu Zhou, and Yifan Lin. Bayesian distributionally robust optimization. _SIAM Journal on Optimization_, 33(2):1279-1304, 2023.
* [52] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _International Conference on Machine Learning_, pages 1015-1022, 2010.

* [53] Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. In _Advances in Neural Information Processing Systems_, pages 9131-9141, 2019.
* [54] Sebastian Shenghong Tay, Chuan Sheng Foo, Urano Daisuke, Richalynn Leong, and Bryan Kian Hsiang Low. Efficient distributionally robust bayesian optimization with worst-case sensitivity. In _International Conference on Machine Learning_, pages 21180-21204. PMLR, 2022.
* [55] MC Tran, V Nguyen, Richard Bruce, DC Crockett, Federico Formenti, PA Phan, SJ Payne, and AD Farmery. Simulation-based optimisation to quantify heterogeneity of specific ventilation and perfusion in the lung by the inspired sinewave test. _Scientific reports_, 11(1):1-10, 2021.
* [56] Tsuyoshi Ueno, Trevor David Rhone, Zhufeng Hou, Teruyasu Mizoguchi, and Koji Tsuda. Combo: an efficient Bayesian optimization library for materials science. _Materials discovery_, 4:18-21, 2016.
* [57] Nienke ER van Bueren, Thomas L Reed, Vu Nguyen, James G Sheffield, Sanne HG van der Ven, Michael A Osborne, Evelyn H Kroesbergen, and Roi Cohen Kadosh. Personalized brain stimulation for effective neurointervention across participants. _PLOS Computational Biology_, 17(9):e1008886, 2021.
* [58] Cedric Villani. _Optimal transport: old and new_, volume 338. Springer Science & Business Media, 2008.
* [59] Xingchen Wan, Cong Lu, Jack Parker-Holder, Philip J Ball, Vu Nguyen, Binxin Ru, and Michael Osborne. Bayesian generational population-based training. In _International Conference on Automated Machine Learning_, pages 14-1. PMLR, 2022.
* [60] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. In _International Conference on Machine Learning_, pages 3627-3635, 2017.
* [61] Jingzhao Zhang, Aditya Krishna Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In _International Conference on Learning Representations 2021_.