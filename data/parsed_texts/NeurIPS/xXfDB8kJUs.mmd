# Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles

Benjamin S. Ruben\({}^{1}\), Cengiz Pehlevan\({}^{2,3,4}\)

\({}^{1}\)Biophysics Graduate Program

\({}^{2}\)Center for Brain Science, \({}^{3}\)John A. Paulson School of Engineering and Applied Sciences, \({}^{4}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138

benruben@g.harvard.edu, cpehlevan@seas.harvard.edu

###### Abstract

Feature bagging is a well-established ensembling method which aims to reduce prediction variance by combining predictions of many estimators trained on subsets or projections of features. Here, we develop a theory of feature-bagging in noisy least-squares ridge ensembles and simplify the resulting learning curves in the special case of equocrrelated data. Using analytical learning curves, we demonstrate that subsampling shifts the double-descent peak of a linear predictor. This leads us to introduce heterogeneous feature ensembling, with estimators built on varying numbers of feature dimensions, as a computationally efficient method to mitigate double-descent. Then, we compare the performance of a feature-subsampling ensemble to a single linear predictor, describing a trade-off between noise amplification due to subsampling and noise reduction due to ensembling. Our qualitative insights carry over to linear classifiers applied to image classification tasks with realistic datasets constructed using a state-of-the-art deep learning feature map.

## 1 Introduction

Ensembling methods are ubiquitous in machine learning practice [1]. A class of ensembling methods (known as attribute bagging [2] or the random subspace method [3]) is based on feature subsampling [2, 3, 4, 5, 6], where predictors are independently trained on subsets of the features, and their predictions are combined to achieve a stronger prediction. The random forest method is a popular example [3, 7].

While commonly used in practice, a theoretical understanding of ensembling via feature subsampling is not well developed. Here, we provide an analysis of this technique in the linear ridge regression setting. Using methods from statistical physics [8, 9, 10, 11, 12], we obtain analytical expressions for typical-case generalization error in linear ridge ensembles (proposition 1), and simplify these expressions in the special case of equocrrelated data with isotropic feature noise (proposition 2). The result provides a powerful tool to quickly probe the generalization error of ensembled regression under a rich set of conditions. In section 3, we study the behavior of a single feature-subsampling regression model. We observe that subsampling shifts the location of a predictor's sample-wise double-descent peak [13, 14, 15]. This motivates section 4, where we study ensembles built on predictors which are heterogeneous in the number of features they access, as a method to mitigate double-descent. We demonstrate this method's efficacy in a realistic image classification task. In section 5 we apply our theory to the trade-off between ensembling and subsampling in resource-constrained settings. Wecharacterize how a variety of factors influence the optimal ensembling strategy, finding a particular significance to the level of noise in the predictions made by ensemble members.

In summary, we make the following contributions:

* Using the replica trick from statistical physics [8, 11], we derive the generalization error of ensembled least-squares ridge regression in a general setting, and simplify the resulting expressions in the tractable special case where features are equicorrelated.
* We demonstrate benefits of heterogeneous ensembling as a robust and computationally efficient regularizer for mitigating double-descent with analytical theory and in a realistic image classification task.
* We describe the ensembling-subsampling trade-off in resource-constrained settings, and characterize the effect of label noise, feature noise, readout noise, regularization, sample size and task structure on the optimal ensembling strategy.

**Related works:** A substantial body of work has elucidated the behavior of linear predictors for a variety of feature maps [14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. Several recent works have extended this research to characterize the behavior of ensembled regression using solvable models [31, 32, 24, 33]. Additional recent works study the performance of ridge ensembles with example-wise subsampling [34, 35] and simultaneous subsampling of features and examples [32], finding that subsampling behaves as an implicit regularization. Methods from statistical physics have long been used for machine learning theory [10, 11, 12, 26, 27, 30, 36, 37]. Relevant work in this domain include [38] which studied ensembling by data-subsampling in linear regression.

## 2 Learning Curves for Ensembled Ridge Regression

We consider noisy ensembled ridge regression in the setting where ensemble members are trained independently on masked versions of the available features. We derive our main analytical formula for generalization error of ensembled linear regression, as well as analytical expressions for generalization error in the special case of equicorrelated features with isotropic noise.

### Problem Setup

Consider a training set \(\mathcal{D}=\{\bar{\bm{\psi}}^{\mu},y^{\mu}\}_{\mu=1}^{P}\) of size \(P\). The training examples \(\bar{\bm{\psi}}^{\mu}\in\mathbb{R}^{M}\) are drawn from a Gaussian distribution with Gaussian feature noise: \(\bar{\bm{\psi}}^{\mu}=\bm{\psi}^{\mu}+\bm{\sigma}^{\mu}\), where \(\bm{\psi}^{\mu}\sim\mathcal{N}(0,\bm{\Sigma}_{s})\) and \(\bm{\sigma}^{\mu}\sim\mathcal{N}(0,\bm{\Sigma}_{0})\). Data and noise are drawn i.i.d. so that \(\mathbb{E}\left[\bm{\psi}^{\mu}\bm{\psi}^{\nu\top}\right]=\delta_{\mu\nu}\bm{ \Sigma}_{s}\) and \(\mathbb{E}\left[\bm{\sigma}^{\mu}\bm{\sigma}^{\nu\top}\right]=\delta_{\mu\nu} \bm{\Sigma}_{0}\). Labels are generated from a noisy teacher function \(y^{\mu}=\frac{1}{\sqrt{M}}\bm{w}^{*\top}\bm{\psi}^{\mu}+\epsilon^{\mu}\) where \(\epsilon^{\mu}\sim\mathcal{N}(0,\zeta^{2})\). Label noises are drawn i.i.d. so that \(\mathbb{E}[\epsilon^{\mu}\epsilon^{\nu}]=\delta_{\mu\nu}\zeta^{2}\).

We seek to analyze the quality of predictions which are averaged over an ensemble of ridge regression models, each with access to a subset of the features. We consider \(k\) linear predictors with weights \(\hat{\bm{w}}_{r}\in\mathbb{R}^{N_{r}}\), \(r=1,\ldots,k\). Critically, we allow \(N_{r}\neq N_{r^{\prime}}\) for \(r\neq r^{\prime}\), which allows us to introduce _structural_ heterogeneity into the ensemble of predictors. A forward pass of the model is given as:

\[f(\bm{\psi})=\frac{1}{k}\sum_{r=1}^{k}f_{r}(\bm{\psi}),\qquad f_{r}(\bm{\psi}) =\frac{1}{\sqrt{N_{r}}}\hat{\bm{w}}_{r}^{\top}\bm{A}_{r}(\bm{\psi}+\bm{ \sigma})+\xi_{r}.\] (1)

The model's prediction \(f(\bm{\psi})\) is an average over \(k\) linear predictors. The "measurement matrices" \(\bm{A}_{r}\in\mathbb{R}^{N_{r}\times M}\) act as linear masks restricting the information about the features available to each member of the ensemble. Subsampling may be implemented by choosing the rows of each \(A_{r}\) to coincide with the rows of the identity matrix - the row indices corresponding to indices of the sampled features. The feature noise \(\bm{\sigma}\sim\mathcal{N}(0,\bm{\Sigma}_{0})\) and the readout noises \(\xi_{r}\sim\mathcal{N}(0,\eta_{r}^{2})\), are drawn independently at the execution of each forward pass of the model. Note that while the feature noise is shared across the ensemble, readout noise is drawn independently for each readout: \(\mathbb{E}[\xi_{r}\xi_{r^{\prime}}]=\delta_{rr^{\prime}}\eta_{r}^{2}\).

The weight vectors are trained separately in order to minimize an ordinary least-squares loss function with ridge regularization:

\[\hat{\bm{w}}_{r}=\operatorname*{arg\,min}_{\bm{w}_{r}\in\mathbb{R}^{N_{r}}}\left[ \sum_{\mu=1}^{P}\left(\frac{1}{\sqrt{N_{r}}}\bm{w}_{r}^{\top}\bm{A}_{r}\bm{\bar {\psi}}^{\mu}+\xi_{r}^{\mu}-y^{\mu}\right)^{2}+\lambda_{r}|\bm{w}_{r}^{2}|\right]\] (2)

Here \(\{\xi_{r}^{\mu}\}\) represents the readout noise which is present during training, and independently drawn: \(\xi_{r}^{\mu}\sim\mathcal{N}(0,\eta_{r}^{2})\), \(\mathbb{E}[\xi_{r}^{\mu}\xi_{r}^{\nu}]=\eta_{r}^{2}\delta_{\mu\nu}\). As a measure of model performance, we consider the generalization error, given by the mean-squared-error (MSE) on ensemble-averaged prediction:

\[E_{g}(\mathcal{D})=\mathbb{E}_{\psi,\sigma,\{\xi_{r}\}}\left[\left(f(\bm{\psi} )-\frac{1}{\sqrt{M}}\bm{w}^{*\top}\bm{\psi}\right)^{2}\right]\] (3)

Here, the expectation is over the data distribution and noise: \(\bm{\psi}\sim\mathcal{N}(0,\bm{\Sigma}_{s})\), \(\bm{\sigma}\sim\mathcal{N}(0,\bm{\Sigma}_{0})\), \(\xi_{r}\sim\mathcal{N}(0,\eta_{r}^{2})\). The generalization error depends on the particular realization of the dataset \(\mathcal{D}\) through the learned weights \(\{\hat{\bm{w}}^{*}\}\). We may decompose the generalization error as follows:

\[E_{g}(\mathcal{D}) =\frac{1}{k^{2}}\sum_{r,r^{\prime}=1}^{k}E_{rr^{\prime}}(\mathcal{ D})\] (4) \[E_{rr^{\prime}}(\mathcal{D}) \equiv\frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{A}_{r}^ {\top}\hat{\bm{w}}_{r}-\bm{w}^{*}\right)^{\top}\bm{\Sigma}_{s}\left(\frac{1}{ \sqrt{\nu_{rr^{\prime}r^{\prime}}}}\bm{A}_{r^{\prime}}^{\top}\hat{\bm{w}}_{r^{ \prime}}-\bm{w}^{*}\right)\right.\] (5) \[\qquad\qquad\left.+\frac{1}{\sqrt{\nu_{rr}\nu_{r^{\prime}r^{ \prime}}}}\hat{\bm{w}}_{r}^{\top}\bm{A}_{r}\bm{\Sigma}_{0}\bm{A}_{r^{\prime}}^ {\top}\hat{\bm{w}}_{r^{\prime}}+M\delta_{rr^{\prime}}\eta_{r}^{2}\right]\]

Computing the generalization error of the model is then a matter of calculating \(E_{rr^{\prime}}\) in the cases where \(r=r^{\prime}\) and \(r\neq r^{\prime}\). In the asymptotic limit we consider, we expect that the generalization error concentrates over randomly drawn datasets \(\mathcal{D}\).

### Main Result

We calculate the generalization error using the replica trick from statistical physics, and present the calculation in Appendix F. The result of our calculation is stated in proposition 1.

**Proposition 1**.: _Consider the ensembled ridge regression problem described in Section 2.1. Consider the asymptotic limit where \(M,P,\{N_{r}\}\to\infty\) while the ratios \(\alpha=\frac{P}{M}\) and \(\nu_{rr}=\frac{N_{r}}{M}\), \(r=1,\ldots,k\) remain fixed. Define the following quantities:_

\[\tilde{\bm{\Sigma}}_{rr^{\prime}} \equiv\frac{1}{\sqrt{\nu_{rr}\nu_{r^{\prime}r^{\prime}}}}\bm{A}_ {r}[\bm{\Sigma}_{s}+\bm{\Sigma}_{0}]\bm{A}_{r^{\prime}}^{\top}\] (6) \[\bm{G}_{r} \equiv\bm{I}_{N_{r}}+\hat{q}_{r}\tilde{\bm{\Sigma}}_{rr}\] (7) \[\gamma_{rr^{\prime}} \equiv\frac{\alpha}{M(\lambda_{r}+q_{r})(\lambda_{r^{\prime}}+q_ {r^{\prime}}))}\operatorname{tr}\left[\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}_{rr^{ \prime}}\bm{G}_{r^{\prime}}^{-1}\tilde{\bm{\Sigma}}_{r^{\prime}r}\right]\] (8)

_Then the terms of the average generalization error (eq. 5) may be written as:_

\[\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D}}= \frac{\gamma_{rr^{\prime}}\zeta^{2}+\delta_{rr^{\prime}}\eta_{r}^ {2}}{1-\gamma_{rr^{\prime}}}+\frac{1}{1-\gamma_{rr^{\prime}}}\left(\frac{1}{M} \bm{w}^{*\top}\bm{\Sigma}_{s}\bm{w}^{*}\right)\] (9) \[-\frac{1}{M(1-\gamma_{rr^{\prime}})}\bm{w}^{*\top}\bm{\Sigma}_{s} \left[\frac{1}{\nu_{rr}}\hat{q}_{r}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\bm{A}_{r}+ \frac{1}{\nu_{r^{\prime}r^{\prime}}}\hat{q}_{r^{\prime}}\bm{A}_{r^{\prime}}^{ \top}\bm{G}_{r^{\prime}}^{-1}\bm{A}_{r^{\prime}}\right]\bm{\Sigma}_{s}\bm{w}^ {*}\] \[+\frac{\hat{q}_{r}\hat{q}_{r^{\prime}}}{M(1-\gamma_{rr^{\prime}}) }\frac{1}{\sqrt{\nu_{rr}\nu_{r^{\prime}r^{\prime}}}}\bm{w}^{*\top}\bm{\Sigma}_{ s}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}_{rr^{\prime}}\bm{G}_{r^{ \prime}}^{-1}\bm{A}_{r^{\prime}}\bm{\Sigma}_{s}\bm{w}^{*}\]

_where the pairs of order parameters \(\{q_{r},\hat{q}_{r}\}\) for \(r=1,\ldots,K\), satisfy the following self-consistent saddle-point equations_

\[\hat{q}_{r}=\frac{\alpha}{\lambda_{r}+q_{r}},\qquad q_{r}=\frac{1}{M} \operatorname{tr}\left[\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}_{rr}\right].\] (10)Proof.: We calculate the terms in the generalization error using the replica trick, a standard but non-rigorous method from the statistical physics of disordered systems. The full derivation may be found in the Appendix F. When the matrices \(\bm{A}_{r}\left(\bm{\Sigma}_{s}+\bm{\Sigma}_{0}\right)\bm{A}_{r}^{\top}\), \(r=1,\ldots,k\) have bounded spectra, this result may be obtained by extending the results of [31] to include readout noise, as shown in Appendix G. 

We make the following remarks:

_Remark 1_.: Implicit in this theorem is the assumption that the relevant matrix contractions and traces which appear in the generalization error (eq. 9) and the surrounding definitions tend to a well-defined limit which remains \(\mathcal{O}(1)\) as \(M\to\infty\).

_Remark 2_.: This result applies for any (well-behaved) linear masks \(\{\bm{A}_{r}\}\). We will focus on the case where each \(\bm{A}_{r}\) implements subsampling of an extensive fraction \(\nu_{rr}\) of the features.

_Remark 3_.: When \(k=1\), our result reduces to the generalization error of a single ridge regression model, as studied in refs. [36, 39].

_Remark 4_.: We include "readout noise" which independently corrupts the predictions of each ensemble member. This models sources of variation between ensemble members not otherwise accounted for. For example, ensembles of deep networks will vary due to random initialization of parameters [24, 31, 36]. Readout noise is more directly present in physical neural networks, such as an analog neural networks [40] or biological neural circuits[41] due to their inherent stochasticity.

In Figure 0(a), we confirm the result of the general calculation by comparing with numerical experiments using a synthetic dataset with \(M=2000\) highly structured features (see caption for details). \(k=3\) readouts see random, fixed subsets of features. Theory curves are calculated by solving the fixed-point equations 10 numerically for the chosen \(\bm{\Sigma}_{s}\), \(\bm{\Sigma}_{0}\) and \(\{\bm{A}_{r}\}_{r=1}^{k}\) then evaluating eq. 9.

### Equicorrelated Data

Our general result allows the freedom to tune many important parameters of the learning problem: the correlation structure of the dataset, the number of ensemble members, the scales of noise, etc. However, the derived expressions are rather opaque. In order to better understand the phenomena captured by these expressions, we examine the following special case:

Figure 1: Comparison of numerical and theoretical learning curves for ensembled linear regression. Circles represent numerical results averaged over 100 trials; lines indicate theoretical predictions. Error bars represent the standard error of the mean but are often smaller than the markers. (a) Testing of proposition 1 with \(M=2000\), \(\left[\bm{\Sigma}_{s}\right]_{ij}=.8^{|i-j|}\), \(\left[\bm{\Sigma}_{0}\right]_{ij}=\frac{1}{10}(0.3)^{|i-j|}\), \(\zeta=0.1\), and all \(\eta_{r}=0.2\) and \(\lambda_{r}=\lambda\) (see legend). \(k=3\) linear predictors access fixed, randomly selected (with replacement) subsets of the features with fractional sizes \(\nu_{rr}=0.2,0.4,0.6\). Fixed ground-truth weights \(\bm{w}^{\star}\) are drawn from an isotropic Gaussian distribution. (b) Testing of proposition 2 with \(M=5000\), \(s=1\), \(c=0.6\), \(\omega^{2}=0.1\), \(\zeta=0.1\), all \(\eta_{r}=0.1\), and all \(\lambda_{r}=\lambda\) (see legend). Ground truth weights sampled as in eq. 11 with \(\rho=0.3\). Feature subsets accessed by each readout are mutually exclusive (inset) with fractional sizes \(\nu_{rr}=0.1,0.3,0.5\).

**Proposition 2**.: _In the setting of section 2.1 and proposition 1, consider the following special case:_

\[\bm{w}^{*} =\sqrt{1-\rho^{2}}\mathbb{P}_{\perp}\bm{w}_{0}^{*}+\rho\bm{1}_{M}\] (11) \[\bm{w}_{0}^{*} \sim\mathcal{N}(0,\bm{I}_{M})\] (12) \[\bm{\Sigma}_{s} =s\left[(1-c)\bm{I}_{M}+c\bm{1}_{M}\bm{1}_{M}^{\top}\right]\] (13) \[\bm{\Sigma}_{0} =\omega^{2}\bm{I}_{M}\] (14)

_with \(c\in[0,1],\rho\in[-1,1]\). Label and readout noises \(\zeta,\eta_{r}\geq 0\) are permitted. Here \(\mathbb{P}_{\perp}=\bm{I}_{M}-\frac{1}{M}\bm{1}_{M}\bm{1}_{M}^{\top}\) is a projection matrix which removes the component of \(\bm{w}_{0}^{*}\) which is parallel to \(\bm{1}_{M}\). The matrices \(\{\bm{A}_{r}\}_{r=1}^{k}\) have rows consisting of distinct one-hot vectors so that each of the \(k\) readouts has access to a subset of \(N_{r}=\nu_{rr}M\) features. For \(r\neq r^{\prime}\), denote by \(n_{rr^{\prime}}\) the number of neurons sampled by both \(\bm{A}_{r}\) and \(\bm{A}_{r^{\prime}}\) and let \(\nu_{rr^{\prime}}\equiv n_{rr^{\prime}}/M\) remain fixed as \(M\to\infty\)._

_Define the following quantities:_

\[a\equiv s(1-c)+\omega^{2}\qquad S_{r}\equiv\frac{\hat{q}_{r}}{\nu_{rr}+a\hat{ q}_{r}},\qquad\gamma_{rr^{\prime}}\equiv\frac{a^{2}\nu_{rr^{\prime}}S_{r}S_{r^{ \prime}}}{\alpha}\] (15)

_The terms of the decomposed generalization error may then be written:_

\[\langle E_{rr^{\prime}}\rangle_{\mathcal{D},\bm{w}_{0}^{*}}=\frac{1}{1-\gamma _{rr^{\prime}}}\left((1-\rho^{2})I_{rr^{\prime}}^{0}+\rho^{2}I_{rr^{\prime}}^{ 1}\right)+\frac{\gamma_{rr^{\prime}}\zeta^{2}+\delta_{rr^{\prime}}\eta_{r}^{2}} {1-\gamma_{rr^{\prime}}}\] (16)

_where we have defined_

\[I_{rr^{\prime}}^{0} \equiv s(1-c)\left(1-s(1-c)\nu_{rr}S_{r}-s(1-c)\nu_{r^{\prime}r^{ \prime}}S_{r^{\prime}}+as(1-c)\nu_{rr^{\prime}}S_{r}S_{r^{\prime}}\right)\] (17) \[I_{rr^{\prime}}^{1} \equiv\begin{cases}\frac{s(1-c)(\nu_{rr^{\prime}}-\nu_{rr^{ \prime}}\nu_{r^{\prime}r^{\prime}})+\omega^{2}\nu_{rr^{\prime}}}{\nu_{rr^{ \prime}}\nu_{r^{\prime}r^{\prime}}}&\text{if }0<c\leq 1\\ I_{rr^{\prime}}^{0}&\text{if }c=0\end{cases}\] (18)

_and where \(\{q_{r},\hat{q}_{r}\}\) may be obtained analytically as the solution (with \(q_{r}>0\)) to:_

\[q_{r}=\frac{a\nu_{rr}}{\nu_{rr}+a\hat{q}_{r}}\qquad,\qquad\hat{q}_{r}=\frac{ \alpha}{\lambda_{r}+q_{r}}\] (19)

_In the "ridgeless" limit where all \(\lambda_{r}\to 0\), we may make the following simplifications:_

\[S_{r}\to\frac{2\alpha}{a\left(\alpha+\nu_{rr}+|\alpha-\nu_{rr}|\right)},\quad \gamma_{rr^{\prime}}\quad\to\frac{4\alpha\nu_{rr^{\prime}}}{\left(\alpha+\nu _{rr}+|\alpha-\nu_{rr}|\right)\left(\alpha+\nu_{r^{\prime}r^{\prime}}+|\alpha- \nu_{r^{\prime}r^{\prime}}|\right)}\] (20)

Proof.: Simplifying the fixed-point equations and generalization error formulas in this special case is an exercise in linear algebra. The main tools used are the Sherman-Morrison formula [42] and the fact that the data distribution is isotropic in the features so that the form of \(\tilde{\bm{\Sigma}}_{rr}\) and \(\tilde{\bm{\Sigma}}_{rr^{\prime}}\) depend only on the subsampling and overlap fractions \(\nu_{rr},\nu_{r^{\prime}r^{\prime}},\nu_{rr^{\prime}}\). To aid in computing the necessary matrix contractions we developed a custom Mathematica package which handles block matrices of symbolic dimension, with blocks containing matrices of the form \(\bm{M}=c_{1}\bm{I}+c_{2}\bm{1}\bm{1}^{\top}\). This package and the Mathematica notebook used to derive these results are available online (see Appendix B) 

In this tractable special case, \(c\in[0,1]\) is a parameter which tunes the strength of correlations between features of the data. When \(c=0\), the features are independent, and when \(c=1\) the features are always equivalent. \(s\) sets the overall scale of the features and the "Data-Task alignment" \(\rho\) tunes the alignment of the ground truth weights with the special direction in the covariance matrix (analogous to "task-model" alignment [14, 27]). A table of parameters is provided in Appendix A. In Figure 0(b), we test these results by comparing the theoretical expressions for generalization error with the results of numerical experiments, finding perfect agreement.

With an analytical formula for the generalization error, we can compute the optimal regularization parameters \(\lambda_{r}\) which minimize the generalization error. These may, in general, depend on both \(r\) and the sample size \(\alpha\). Rather than minimizing the error of the ensemble, we may minimize the generalization error of predictions made by the ensemble members independently. We find that this "locally optimal" regularization, denoted \(\lambda^{*}\), is independent of \(\alpha\), generalizing results from [14, 43] to correlated data distributions (see Appendix H.3).

## 3 Subsampling shifts the double-descent peak of a linear predictor

Consider a single linear regressor (\(k=1\)) which connects to a subset of \(N=\nu M\) features in the equicorrelated data setting of proposition 2. Also setting \(c=0\), \(s=1\), and \(\eta_{r}=\omega=0\) and taking the limit \(\lambda\to 0\) the generalization error reads:

\[\langle E_{g}\rangle_{\mathcal{D},\bm{w}^{*}}=\left\{\begin{array}{ll}\frac{ \nu}{\nu-\alpha}\left[(1-\nu)+\frac{1}{\nu}(\alpha-\nu)^{2}\right]+\frac{ \alpha}{\nu-\alpha}\zeta^{2},&\text{if }\alpha<\nu\\ \frac{\alpha}{\alpha-\nu}\left[1-\nu\right]+\frac{\alpha}{\alpha-\nu}\zeta^{2 },&\text{if }\alpha>\nu\end{array}\right\}\] (21)

We thus see that double descent can arise from two possible sources of variance: explicit label noise (if \(\zeta>0\)) or implicit label noise induced by feature subsampling (\(\nu<1\)). As \(E_{g}\sim(\alpha-\nu)^{-1}\), generalization error diverges when sample size is equal to the number of sampled features. Intuitively, this occurs because subsampling changes the number of parameters of the regression model, and thus its interpolation threshold. To demonstrate this, we plot the learning curves for subsampled linear regression on equicorrelated data in Figure 2. At small finite ridge the test error no longer diverges when \(\alpha=\nu\), but still displays a distinctive peak.

## 4 Heterogeneous connectivity mitigates double-descent

Double-descent - over-fitting to noise in the training set near a model's interpolation threshold - poses a serious risk in practical machine-learning applications [22]. Cross-validating the regularization strength against the training set is the canonical approach to avoiding double-descent [17, 43], but in practice requires a computationally expensive parameter sweep and prior knowledge of the task. In situations where computational resources are limited or hyperparameters are fixed prior to specification of the task, it is natural to seek an alternative solution. Considering again the plots in Figure 2(b), we observe that at any value of \(\alpha\), the double-descent peak can be avoided with an acceptable choice of the subsampling fraction \(\nu\). This suggests another strategy to mitigate double descent: heterogeneous ensembling. Ensembling over predictors with a heterogeneous distribution of interpolation thresholds, we may expect that when one predictor fails due to over-fitting, the other members of the ensemble compensate with accurate predictions.

In Figure 3, we show that heterogeneous ensembling can guard against double-descent. We define two ensembling strategies: in homogeneous ensembling, each of \(k\) readouts connects a fraction \(\nu_{rr}=1/k\) features. In heterogeneous ensembling, the number of features connected by each of the \(k\) readouts are drawn from a Gamma distribution \(\Gamma_{k,\sigma}(\nu)\) with mean \(1/k\) and standard deviation \(\sigma\) (see Fig. 3b) then re-scaled to sum to 1 (see Appendix C for details). All feature subsets are mutually exclusive (\(\nu_{rr^{\prime}}=0\) for \(r\neq r^{\prime}\)). Homogeneous and heterogeneous ensembling are illustrated for \(k=10\) in Figs. 3 a.i and 3 a.ii respectively. We test this hypothesis using eq. 16 in 3c. At small regularization

Figure 2: Subsampling alters the location of the double-descent peak of a linear predictor. (a) Illustrations of subsampled linear predictors with varying subsampling fraction \(\nu\). (b) Comparison between experiment and theory for subsampling linear regression on equicorrelated datasets. We choose task parameters as in proposition 2 with \(c=\omega=\zeta=\eta=0\), \(s=1\), and (i) \(\lambda=0\), (ii) \(\lambda=10^{-4}\), (iii) \(\lambda=10^{-2}\). All learning curves are for a single linear predictor \(k=1\) with subsampling fraction \(\nu\) shown in legend. Circles show results of numerical experiment. Lines are analytical prediction.

(\(\lambda=.001\)), we find that heterogeneity of the distribution of subsampling fractions ( \(\sigma>0\)) lowers the double-descent peak of an ensemble of linear predictors, while at larger regularization (\(\lambda=0.1\)), there is little difference between homogeneous and heterogeneous learning curves. The asymptotic (\(\alpha\rightarrow\infty\)) error is unaffected by the presence of heterogeneity in the degrees of connectivity, which can be seen as the coincidence of the triangular markers in Fig. 3c, as well as from the \(\alpha\rightarrow\infty\) limit of eq. 16 (see Appendix H.5). Fig. 3c also shows the learning curve of a single linear predictor with no feature subsampling and optimal regularization. We see that the feature-subsampling ensemble appreciably outperforms the fully-connected model when \(c=0.8\) and \(\eta=0.5\), suggesting the important roles of data correlations and readout noise in determining the optimal readout strategy. These roles are further explored in section 5 and fig 4.

We also test the effect of heterogeneous ensembling in the a realistic classification task. Specifically, we train ensembles of linear classifiers to predict the labels of imagenet [44] images corresponding to 10 different dog breeds (the "Imagewoof" task [45]) from their top-hidden-layer representations in a pre-trained ResNext deep network [46] (see Appendix E for details). We characterize the statistics of the resulting \(M=2048\)-dimensional feature set in Fig. S1. This "ResNext-Imagewoof" classification task has multiple features which make it amenable to learning with a feature-subsampling ensemble. First, the ResNext features have a high degree of redundancy [47], allowing classification to be performed accurately using only a fraction of the available features (see Fig. 3d and S1c). Second, when classifications of multiple predictors are combined by a majority vote, there is a natural upper bound on the influence of a single erring ensemble member (unlike in regression where predictions can diverge). Calculating learning curves for the imagewoof classification task using homogeneous ensembles, we see sharp double-descent peaks in an ensemble of size \(k\) when \(P=M/k\) (Fig. 3e.i). Using a heterogeneous ensemble mitigates this catastrophic over-fitting, leading to monotonically decreasing error without regularization (Fig. 3e.ii). A single linear predictor with a tuned regularization of \(\lambda=0.1\) performs only marginally better than the heterogeneous feature-subsampling ensemble with \(k=16\) or \(k=32\). This suggests heterogeneous ensembling can be an effective alternative to regularization in real-world classification tasks using pre-trained deep learning feature maps.

Note that training a feature-subsampling ensemble also benefits from improved computational complexity. Training an estimator of dimension \(N_{r}\) involves, in the worst case, inverting an \(N_{r}\times N_{r}\) matrix, which requires \(\mathcal{O}(N_{r}^{3})\) operations. Setting \(N_{r}=M/k\), we see that the number of operations required to train an ensemble of \(k\) predictors scales as \(\mathcal{O}(k^{-2})\).

## 5 Correlations, Noise, and Task Structure Dictate the Ensembling-Subsampling Trade-off

In resource-constrained settings, one must decide between training a single large predictor or an ensemble of smaller predictors. When the number of weights is constrained, ensembling may benefit generalization by averaging over multiple predictions, but at the expense of each prediction incorporating fewer features. Intuitively, the presence of correlations between features limits the penalty incurred by subsampling, as measurements from a subset of features will also confer information about the unsampled features. The equicorrelated data model of proposition 2 permits a solvable toy model for these competing effects. We consider the special case of ensembling over \(k\) readouts, each connecting the same fraction \(\nu_{rr}=\nu=1/k\) of all features. For simplicity, we set \(\nu_{rr^{\prime}}=0\) for \(r\neq r^{\prime}\). We asses the learning curves of this toy model in both the ridgeless limit \(\lambda\to 0\) where double-descent has a large effect on test error, and at 'locally optimal' regularization \(\lambda=\lambda^{*}\) for which double-descent is eliminated. In these special cases, one can write the generalization error in the following forms (see Appendix H.4 for derivation):

\[E_{g}(k,s,c,\eta,\omega,\zeta,\rho,\alpha,\lambda=0)=s(1-c) \mathcal{E}(k,\rho,\alpha,H,W,Z)\] (22) \[E_{g}(k,s,c,\eta,\omega,\zeta,\rho,\alpha,\lambda=\lambda^{*})=s (1-c)\mathcal{E}^{*}(k,\rho,\alpha,H,W,Z)\] (23)

where we have defined the effective noise-to-signal ratios:

\[H\equiv\frac{\eta^{2}}{s(1-c)},\quad W=\frac{\omega^{2}}{s(1-c)},\quad Z= \frac{\zeta^{2}}{s(1-c)}\] (24)Therefore, given fixed parameters \(s,c,\rho,\alpha\), the value \(k^{*}\) which minimizes error depends on the noise scales, \(s\), and \(c\) only through the ratios \(H\), \(W\) and \(Z\):

\[k^{*}_{\lambda=0}(H,W,Z,\rho,\alpha)\equiv\operatorname*{arg\,min }_{k\in\mathbb{N}}E_{g}(k)=\operatorname*{arg\,min}_{k\in\mathbb{N}}\mathcal{E} (k,\rho,\alpha,H,W,Z)\] (25) \[k^{*}_{\lambda=\lambda^{*}}(H,W,Z,\rho,\alpha)\equiv\operatorname *{arg\,min}_{k\in\mathbb{N}}E_{g}(k)=\operatorname*{arg\,min}_{k\in\mathbb{N}} \mathcal{E}^{*}(k,\rho,\alpha,H,W,Z)\] (26)

In Fig. 4a, we plot these reduced errors curves \(\mathcal{E}\), \(\mathcal{E}^{*}\) as a function of \(\alpha\) for varying ensemble sizes \(k\) and reduced readout noise scales \(H\). At zero regularization learning curves diverge at their interpolation threshold. At locally optimal regularization \(\lambda=\lambda^{*}\), learning curves decrease monotonically with sample size. Increasing readout noise \(H\) raises generalization error more sharply for smaller \(k\). In Fig. 4b we plot the optimal \(k^{*}\) in various two-dimensional slices of parameter space in which \(\rho\) is fixed and \(W=Z=0\) while \(\alpha\) and \(H\) vary. The resulting phase diagrams may be divided into three regions. In the signal-dominated phase a single fully-connected readout is optimal (\(k^{*}=1\)). In an intermediate phase, \(1<k^{*}<\infty\) minimizes error. And in a noise-dominated phase \(k^{*}=\infty\). At zero regularization, we have determined an analytical expression for the boundary between the intermediate and noise-dominated phases (see Appendix H.4.1 and dotted lines in Figs 4.b,c,d). The signal-dominated, intermediate, and noise-dominated phases persist when \(\lambda=\lambda^{*}\), removing the effects of double descent. In all panels, an increase in H causes an increase in \(k^{*}\). This can occur because of a decrease in the signal-to-readout noise ratio \(s/\eta^{2}\), or through an increase in the correlation strength \(c\). An increase in \(\rho\) also leads to an increase in \(k^{*}\), indicating that ensembling is more effective for easier tasks. Figs 4c,d show analogous phase diagrams where \(W\) or \(Z\) are varied. Signal-dominated, intermediate, and noise-dominated regimes are visible in the resulting phase diagrams at zero regularization. However, when optimal regularization is used, \(k^{*}=1\) is always optimal. The presence of regions where \(k^{*}>1\) can thus be attributed to double-descent at sub-optimal regularization or to the presence of readout noise which is independent across predictors. We chart the parameter-space of the reduced errors and optimal ensemble size \(k^{*}\) extensively in Appendix I. We plot learning curves for the "ResNext-Imagewoof" ensembled linear classification task with varying strength of readout noise in Fig. 4e, and phase diagrams of optimal ensemble size \(k\) in Fig. 4f, finding similar behavior to the toy model. See Figs. S3, S4, S5 and Appendix E.4.3 for further discussion.

## 6 Conclusion

In this paper, we provided a theory of feature-subsampled linear ridge regression. We identified the special case in which features of the data are "equicorrelated" as a minimal toy model to explore the combined effects of subsampling, ensembling, and different types of noise on generalization error. The resulting learning curves displayed two potentially useful phenomena.

First, we demonstrated that heterogeneous ensembling can mitigate over-fitting, reducing or eliminating the double-descent peak of an under-regularized model. In most machine learning applications, the size of the dataset is known at the outset and suitable regularization may be determined to mitigate double descent, either by selecting a highly over-parameterized model [22] or by cross-validation techniques (see for example [17]). However, in contexts where a single network architecture is designed for an unknown task or a variety of tasks with varying structure and noise levels, heterogeneous ensembling may be used to smooth out the perils of double-descent.

Next, we described a trade-off between noise reduction due to ensembling and noise amplification due to subsampling in a resource-constrained setting where the total number of weights is fixed. Our analysis suggests that ensembling is particularly useful in neural networks with an inherent noise. Physical neural networks, such as analog neural networks[40] and biological neural circuits [41] present such a resource-constrained environments where intrinsic noise is a significant issue.

Much work remains to achieve a full understanding of the interactions between data correlations, readout noise, and ensembling. In this work, we have given a thorough treatment of the convenient special case where features are equicorrelated. Future work should analyze subsampling and ensembling for codes with realistic correlation structure, such as the power-law spectra (see Fig. S1) [27, 30, 48, 49] and sparse activation patterns [50].

Figure 3: Heterogeneous ensembling mitigates double-descent. (a) We compare (i) homogeneous ensembling, in which \(k\) readouts connect to the same fraction \(\nu=1/k\) of features, and (ii) heterogeneous ensembling (b) In heterogeneous ensembling subsampling fractions are drawn i.i.d. from \(\Gamma_{k,\sigma}(\nu)\), shown here for \(k=10\), then re-scaled to sum to 1. (c) Generalization Error Curves for Homogeneous and Heterogeneous ensembling with \(k=10\), \(\zeta=0\), \(\rho=0.3\) and indicated values of \(\lambda\), \(c\), and \(\eta\). Blue: homogeneous subsampling. Red, green, and purple show heterogeneous subsampling with \(\sigma=0.25/k,0.5/k,1/k\) respectively. Dashed lines show learning curves for 3 particular realizations of \(\{\nu_{11},\dots,\nu_{kk}\}\). Solid curves show the average over 100 realizations. Gray shows the learning curve for a single linear readout with \(\nu=1\) and optimal regularization (eq. 193). Triangular marks show the asymptotic generalization error (\(\alpha\rightarrow\infty\)), with downward-pointing gray triangles indicating an asymptotic error of zero. (d,e) Generalization error of linear classifiers applied to the imagewoof dataset with ResNext features averaged over 100 trials. (d) \(P=100\), \(k=1\) varying subsampling fraction \(\nu\) and regularization \(\lambda\) (legend). (e) Generalization error of (i) homogeneous and (ii) heterogeneous (with \(\sigma=0.75/k\)) ensembles of classifiers. Legend indicates \(k\) values. \(\lambda=0\) except for gray curves, where \(\lambda=0.1\)

Figure 4: Task parameters dictate the ensembling-subsampling trade-off: (a-d) In the setting of proposition 2 in the special case where all \(\nu_{rr^{\prime}}=\frac{1}{k}\delta_{rr^{\prime}}\) so that feature subsets are mutually exclusive and the total number of weights is conserved. (a) We plot the reduced generalization errors \(\mathcal{E}\) (for \(\lambda=0\), using eq. 22) and \(\mathcal{E}^{*}\) (for \(\lambda=\lambda^{*}\) using eq. 23) of linear ridge ensembles of varying size \(k\) with \(\rho=0\) and \(H=0,1\) (values indicated above plots). Grey lines indicate \(k=1\), dashed black lines \(k\to\infty\), and intermediate \(k\) values by the colorbar. (b) We plot optimal ensemble size \(k^{*}\) (eqs. 25, 26) in the parameter space of sample size \(\alpha\) and reduced readout noise scale \(H\) setting \(W=Z=0\). Grey indicates \(k^{*}=1\) and white indicates \(k^{*}=\infty\), with intermediate values given by the colorbar. Appended vertical bars show \(\alpha\to\infty\). Dotted black lines show the analytical boundary between the intermediate and noise-dominated phases given by eq. 214. (c) optimal readout \(k^{*}\) phase diagrams as in (b) but showing \(W\)-dependence with \(H=Z=0\). (d) optimal readout \(k^{*}\) phase diagrams as in (b) but showing \(Z\)-dependence with \(H=W=0\). (e) Learning curves for feature-subsampling ensembles of linear classifiers combined using a majority vote rule on the imagewoof classification task (see Appendix E). As in (a-d) we set \(\nu_{rr^{\prime}}=\frac{1}{k}\delta_{rr^{\prime}}\). Error is calculated as the probability of incorrectly classifying a test example. \(\lambda\) and \(\eta\) values are indicated in each panel. (f) Numerical phase diagrams showing the value of \(k\) which minimizes test error in the parameter space of sample size \(P\) and readout noise scale \(\eta\), with regularization (i) \(\lambda=0\) (pseudoinverse rule) (ii) \(\lambda=0.1\).

Acknowledgements

CP is supported by NSF Award DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. BSR was also supported by the National Institutes of Health Molecular Biophysics Training Grant NIH/ NIGMS T32 GM008313. We thank Jacob Zavatone-Veth and Blake Bordelon for thoughtful discussion and comments on this manuscript.

## References

* [1] Gautam Kunapuli. _Ensemble Methods for Machine Learning_. Simon and Schuster, 2023.
* [2] Robert Bryll, Ricardo Gutierrez-Osuna, and Francis Quek. Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. _Pattern recognition_, 36(6):1291-1302, 2003.
* [3] Tin Kam Ho. The random subspace method for constructing decision forests. _IEEE transactions on pattern analysis and machine intelligence_, 20(8):832-844, 1998.
* [4] Yali Amit and Donald Geman. Shape quantization and recognition with randomized trees. _Neural computation_, 9(7):1545-1588, 1997.
* [5] Gilles Louppe and Pierre Geurts. Ensembles on random patches. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part I 23_, pages 346-361. Springer, 2012.
* [6] Marina Skurichina and Robert P. W. Duin. Bagging, boosting and the random subspace method for linear classifiers. _Pattern Analysis & Applications_, 5(2):121-135, June 2002.
* [7] Tin Kam Ho. Random decision forests. In _Proceedings of 3rd international conference on document analysis and recognition_, volume 1, pages 278-282. IEEE, 1995.
* [8] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. _Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications_. World Scientific Publishing Company, 1987.
* [9] M Mezard, G Parisi, and M Virasoro. _Spin Glass Theory and Beyond_. WORLD SCIENTIFIC, November 1986.
* [10] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from examples. _Physical review A_, 45(8):6056, 1992.
* [11] Andreas Engel and Christian Van den Broeck. _Statistical mechanics of learning_. Cambridge University Press, 2001.
* [12] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. _Annual Review of Condensed Matter Physics_, 11:501-528, 2020.
* [13] Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. _arXiv preprint arXiv:1912.07242_, 2019.
* [14] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. _Nature communications_, 12(1):2914, 2021.
* [15] Fatih Furkan Yilmaz and Reinhard Heckel. Regularization-wise double descent: Why it occurs and how to eliminate it. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 426-431. IEEE, 2022.

- 986, 2022.
* Hastie et al. [2009] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* Rocks and Mehta [2022] Jason W. Rocks and Pankaj Mehta. Bias-variance decomposition of overparameterized regression with random linear features. _Physical Review E_, 106:025304, Aug 2022.
* Rocks and Mehta [2022] Jason W. Rocks and Pankaj Mehta. Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models. _Physical Review Research_, 4:013201, Mar 2022.
* Mei and Montanari [2019] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75, 2019.
* Bartlett et al. [2020] Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* Belkin et al. [2019] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019.
* Hu and Lu [2023] Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 69(3):1932-1964, 2023.
* D'Ascoli et al. [2020] Stephane D'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance(s) in the lazy regime. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 2280-2290. PMLR, 13-18 Jul 2020.
* Adlam and Pennington [2020] Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-variance decomposition. In _Advances in Neural Information Processing Systems_, volume 33, pages 11022-11032, 2020.
* Zavatone-Veth et al. [2022] Jacob A. Zavatone-Veth, William L. Tong, and Cengiz Pehlevan. Contrasting random and learned features in deep bayesian linear regression, 2022.
* Bordelon et al. [2020] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1024-1034. PMLR, 13-18 Jul 2020.
* Simon et al. [2022] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The Eigen-learning framework: A conservation law perspective on kernel regression and wide neural networks. _arXiv_, 2022.
* Bach [2023] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections, 2023.
* Zavatone-Veth and Pehlevan [2023] Jacob A. Zavatone-Veth and Cengiz Pehlevan. Learning curves for deep structured gaussian feature models, 2023.
* Loureiro et al. [2022] Bruno Loureiro, Cedric Gerbelot, Maria Refinetti, Gabriele Sicuro, and Florent Krzakala. Fluctuations, bias, variance & ensemble of learners: Exact asymptotics for convex losses in high-dimension. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14283-14314. PMLR, 17-23 Jul 2022.

* [32] Daniel LeJeune, Hamid Javadi, and Richard Baraniuk. The implicit regularization of ordinary least squares ensembles. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 3525-3535. PMLR, 26-28 Aug 2020.
* [33] Pratik Patil and Daniel LeJeune. Asymptotically free sketched ridge ensembles: Risks, cross-validation, and tuning, 2023.
* [34] Jin-Hong Du, Pratik Patil, and Arun Kumar Kuchibhotla. Subsample ridge ensembles: Equivalences and generalized cross-validation, 2023.
* [35] Pratik Patil, Jin-Hong Du, and Arun Kumar Kuchibhotla. Bagging in overparameterized learning: Risk characterization and risk monotonization, 2022.
* [36] Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. The onset of variance-limited behavior for networks in the lazy and rich regimes. In _The Eleventh International Conference on Learning Representations_, 2023.
* [37] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Out-of-distribution generalization in kernel regression. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 12600-12612. Curran Associates, Inc., 2021.
* [38] Peter Sollich and Anders Krogh. Learning with ensembles: How overfitting can be useful. _Advances in neural information processing systems_, 8, 1995.
* [39] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [40]Devon Janke and David V. Anderson. Analyzing the effects of noise and variation on the accuracy of analog neural networks. In _2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS)_. IEEE, August 2020.
* [41] A. Aldo Faisal, Luc P. J. Selen, and Daniel M. Wolpert. Noise in the nervous system. _Nature Reviews Neuroscience_, 9(4):292-303, April 2008.
* [42] Jack Sherman and Winfred J. Morrison. Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. _The Annals of Mathematical Statistics_, 21(1):124-127, March 1950.
* [43] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can mitigate double descent. _arXiv preprint arXiv:2003.01897_, 2020.
* [44] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [45] J Howard. Imagenette. Accessed: 25-10-2025.
* [46] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks, 2017.
* [47] Diego Doimo, Aldo Glielmo, Sebastian Goldt, and Alessandro Laio. Redundant representations help generalization in wide neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [48] Alexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws, 2022.
* [49] Blake Bordelon and Cengiz Pehlevan. Population codes enable learning from few examples by shaping inductive bias. _eLife_, 11, December 2022.

* [50] B OSLHAUSEN and D FIELD. Sparse coding of sensory inputs. _Current Opinion in Neurobiology_, 14(4):481-487, August 2004.
* [51] Luc Devroye. _Non-Uniform Random Variate Generation_. Springer New York, 1986.
* [52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* [53] John R. Silvester. Determinants of block matrices. _The Mathematical Gazette_, 84(501):460-467, November 2000.
* [54] Wolfram Research, Inc. Mathematica, Version 13.3. Champaign, IL, 2023.

## Appendix A Table of Parameters from Proposition 2 and Figures 2,3,4

\begin{tabular}{l|l} Data Scale & \(s\) \\ Correlation Strength & \(c\) \\ Data-Task Alignment & \(\rho\) \\ Readout Noise Scale & \(\eta\) \\ Feature noise scale & \(\omega\) \\ Label Noise Scale & \(\zeta\) \\ Ensemble Size & \(k\) \\ Subsampling Fractions & \(\nu_{rr}\quad r=1,\ldots,K\) \\ Subsampling Overlap Fractions & \(\nu_{rr^{\prime}}\quad r\neq r^{\prime}\) \\ Sample Size & \(P\rightarrow\infty\) \\ Data Dimensionality & \(M\rightarrow\infty\) \\ Sample Complexity & \(\alpha\equiv\frac{P}{M}\quad\text{(finite)}\) \\ Effective Noise-To-Signal Ratios & \(H\equiv\frac{\eta^{2}}{s(1-c)}\), \(W\equiv\frac{\omega^{2}}{s(1-c)}\), \(Z\equiv\frac{\zeta^{2}}{s(1-c)}\) \\ \end{tabular}

## Appendix B Code Availability and Compute

All Code used in this paper has been made available online (see https://github.com/benruben87/Learning-Curves-for-Heterogeneous-Feature-Subsampled-Ridge-Ensembles.git). This includes code used to perform numerical experiments, calculate theoretical learning curves, and produce plots as well as the custom Mathematica libraries used to simplify the generalization error in the special case of equicorrelated data. The compute time required to do all of the calculations in this paper is approximately 3 GPU days.

## Appendix C Homogeneous and Heterogeneous Subsampling

In this section, we describe the homoegeneous and heterogeneous subsampling strategies, as used in this work, in detail. This sampling method was used in calculating the theoretical loss curves for heterogeneous subsampling experiments seen in main text Fig. 3, and in the heterogeneous subsampling experiments applied to the ResNext-features-based image classification task in Figs. 2(e), S2.

In homogeneous ensembling, the subsampling fractions \(\nu_{rr}=N_{r}/M\) are chosen as \(\nu_{rr}=1/k\) for all \(r=1,\ldots,k\). In heterogeneous ensembling, the subsampling fractions \(\{\nu_{11},\ldots,\nu_{kk}\}\) are generated according to the following statistical process:

1. Each fraction \(\nu_{rr}\) is drawn independently as \(\nu_{rr}\sim\Gamma_{k,\sigma}\), where a \(\Gamma_{k,\sigma}\) represents a Gamma distribution with mean \(\frac{1}{k}\) and variance \(\sigma^{2}\).
2. The fractions are re-scaled in order to sum to 1: \(\nu_{rr}\rightarrow\nu_{rr}/(\nu_{1}+\cdots+\nu_{k})\)

Equivalently, the subsampling fractions are drawn from a Dirichlet distribution parameterized by the ensemble size \(k\) and a chosen variance \(\sigma\) as \((\nu_{11},\ldots,\nu_{kk})\sim\mathrm{Dir}((\sigma k)^{-2},\ldots,(\sigma k)^ {-2})\)[51].

In main text Fig. 2(c), we combine this sampling strategy with theoretical learning curves for the equicorrelated data model in a quasi-numerical experiment. At each trial of the experiment, we draw a particular realization of the subsampling fractions \(\{\nu_{11},\ldots,\nu_{kk}\}\), then use the analytical expression (eq. 16) to calculate the resulting learning curve. Dotted lines show the loss curves for 3 single trials, corresponding to three particular realizations of the subsampling fractions. The solid lines show the average over 100 trials.

Note that we have defined our own convention for the parameterization of the \(\Gamma\) distribution in which the inverse of the mean and the standard deviation are specified. In terms of the standard "shape" and "scale" parameters, we have:

\[\Gamma_{k,\sigma}\equiv\Gamma\left(\text{shape}\ =(k\sigma)^{-2},\text{ scale}\ =k\sigma^{2}\right)\] (27)Numerical Linear Regression with Synthetic Datasets

Numerical experiments were performed using the PyTorch library [52]. The code used to perform numerical experiments and generate plots has been made publicly available (see section B).

In numerical regression experiments, synthetic datasets with label noise are constructed as described in section 2.1, drawing data randomly from multivariate Gaussian distributions and adding label noise (see "DatasetMaker.py" in available code). Representing the training set in terms of a data matrix \(\bm{\Psi}\in\mathbb{R}^{M\times P}\) in which column \(\mu\) consist of the training point \(\bm{\psi}_{\mu}\), and the labels with a column vector \(\bm{y}\) such that \(\bm{y}_{\mu}=y_{\mu}\), the learned weights are calculated as:

\[\hat{\bm{w}}=\bm{\Psi}\left(\bm{\Psi}^{\top}\bm{\Psi}+\lambda\bm{I}_{p}\right)^ {-1}\bm{y}\] (28)

In the ridgeless case, a pseudoinverse is used:

\[\hat{\bm{w}}=\bm{\Psi}^{\dagger}\bm{y}\] (29)

## Appendix E Ensembled Linear Classification of Imagenet Images

In this section, we provide the details of numerical experiments which demonstrate that qualitative insights gained from our analysis of the linear regression task with Gaussian data carries over to a practical machine learning task. In particular, we apply ensembles of linear classifiers to datasets constructed using a pre-trained ResNext [46] a specific type of Convolutional Neural Network (CNN).

### Dataset Construction

To construct the dataset, we start with a set of \(n\) images \(\{\bm{x}^{\mu}\}_{\mu=1}^{n}\) from a subset of \(C=10\) classes of the imagenet dataset. For each image \(\bm{x}^{\mu}\), we obtain a corresponding feature vector \(\bm{\psi}^{\mu}\in\mathbb{R}^{M}\) as the last-hidden-layer activation of the ResNext [46], which has been pre-trained on the imagenet classification task [44]. The architecture we use produces \(M=2048\) features per image. These features will serve as the data input to the downstream linear classifier. The corresponding labels \(\bm{y}^{\mu}\in\mathbb{R}^{C}\) are one-hot vectors.

We construct two datasets using this method, using images from the "Imagenette" and "Imagewoof" datasets [45]. For the "Imagenette" task, we a construct a training set of size \(n_{tr}=9469\) and a test set of size \(n_{test}=3925\) containing features corresponding to images from 10 unrelated classes (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute). For the "Imagewoof" task, we a construct a training set of size \(n_{tr}=9025\) and a test set of size \(n_{test}=3929\) containing features corresponding to images of 10 different dog breeds (Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog). The imagewoof classification task is naturally more difficult. The statistics of the resulting datasets are described in Fig. S1, where we plot the data-data covariance matrix, feature-feature covariance matrix, and the eigenvalue spectrum for both the "Imagenette" and "Imagewoof" tasks.

### Model Training

At training time a dataset of \(P\) examples is constructed: \(\mathcal{D}=\{\bm{\psi}^{\mu},\bm{y}^{\mu}\}_{\mu=1}^{P}\). We represent the training set with a "design matrix" \(\bm{\Phi}\in\mathbb{R}^{P\times M}\) and a label matrix \(\bm{Y}\in\{0,1\}^{P\times C}\). The loss function for each ensemble member is generalized to multi-class regression. With ridge regularization, the objective for each ensemble member \(r\) becomes:

\[\hat{\bm{W}}_{r}=\operatorname*{arg\,min}_{\bm{W}_{r}\in\mathbb{R}^{N_{r} \times C}}\left[\sum_{\mu=1}^{P}\|\frac{1}{\sqrt{N_{r}}}\bm{A}_{r}\bar{\bm{\psi }}^{\mu}\bm{W}_{r}+\bm{\xi}_{r}^{\mu}-\bm{y}^{\mu}\|_{2}^{2}+\lambda_{r}\|\bm{ W}_{r}\|_{F}^{2}\right],\]Figure S1: In numerical experiments, we train linear classifiers to predict labels of imagenet images based on their last-hidden-layer representations in a pre-trained RexNext deep learning architecture [46]. Here, we show the structure of the datasets constructed using the ResNext feature map for the Imagenette task (left), which consists of categorizing images from 10 unrelated categories, and the Imagewoof task (right), which consists of categorizing images from 10 different dog breeds. (a) Gram matrix of the centered ResNext features defined as \(\frac{1}{P}\left(\mathbf{\Phi}-\bar{\mathbf{\Phi}}\right)^{\top}\left(\mathbf{ \Phi}-\bar{\mathbf{\Phi}}\right)\) for data matrix \(\Phi\in\mathbb{R}^{P\times M}\) where \(P\) is the total size of the dataset. Dataset is sorted by label and tick marks show the boundaries between classes. (b) The covariance eigenspectrum of the ResNext features is well described by a power law decay. (c) Generalization error of Linear classification with a single linear predictor with access to a fraction \(\nu=N/M\) of the ResNext features averaged over 100 trials (see discussion in section E.4.1)

where \(\|\cdot\|_{F}\) denotes the Frobenius norm and where the readout noise vector \(\bm{\xi}_{r}^{\mu}\sim\mathcal{N}(0,\eta^{2}\bm{I}_{C})\) is a \(C\) - dimensional readout noise which corrupts the model's prediction. The weights \(\hat{\bm{W}}_{r}\) can be determined in closed form as follows:

\[\hat{\bm{W}}_{r}=\frac{1}{\sqrt{N_{r}}}\left(\frac{1}{N_{r}}\bm{A}_{r}\bm{\Phi} ^{\top}\bm{\Phi}\bm{A}_{r}^{\top}+\lambda_{r}\bm{I}\right)^{-1}\left(\bm{A}_{r} \bm{\Phi}^{\top}(\bm{Y}-\bm{\Xi}_{r})\right).\]

where we have defined \([\bm{\Xi}_{r}]_{\mu c}=[\bm{\xi}_{r}^{\mu}]_{c}\). When \(\lambda=0\) we instead use a pseudoinverse rule. In all experiments presented here, we use measurement matrices \(\bm{A}_{r}\) which implement an ordinary subsampling of the features, so that the rows and columns of \(\bm{A}_{r}\) consist of one-hot vectors.

### Model Prediction

Once trained, the learned weights may be used to predict the label of a new example \(\bm{\psi}\) as follows. For \(r=1,\ldots,k\) we calculate the prediction of each ensemble member by first assigning each class a "score". The scores of predictor \(r\) are stored in a vector \(\bm{f}_{r}\in\mathbb{R}^{C}\):

\[\bm{f}_{r}(\bm{\psi})=\frac{1}{\sqrt{N_{r}}}\bm{A}_{r}\bm{\psi}\hat{\bm{W}}_{ r}+\bm{\xi}_{r}\]

Where \(\bm{\xi}_{r}\sim\mathcal{N}(0,\eta^{2}\bm{I}_{C})\) is drawn randomly at model evaluation. Each ensemble member's "vote" corresponds to the class with the largest score. The prediction of the ensemble is then calculated as a majority vote of the ensemble members. Generalization error is then calculated as the probability of misclassifying an example from the test set.

### Linear Classification Experiments

We apply the described majority-vote linear classifier ensembles to the ResNext-Imagenette and ResNext-Imagewoot tasks in three different experiments.

#### e.4.1 Reduncancy of ResNext Features

In the first experiment, we investigate the performance of a single linear classifier (\(k=1\)) as the fraction of features \(\nu\) which it has access to varies. We set \(P=1000\) and vary \(\nu\) over 50 values on a logarithmic scale from \(10^{-3}\) to \(1\). We also vary the regularization strength over \(0\) and a logarithmic scale from \(10^{-3}\) to \(10^{4}\). We average over 100 trials. At each trial the particular subset of \(P=1000\) training examples and the particular subset of \(\nu*M\) features is randomly re-sampled. We find that ResNext features are highly redundant - classification accuracy is very robust to subsampling of the features. For example, in the Imagewoot classification task with the best regularization tested (\(\lambda=0.1\)), test error increases from about 1% to about 2% as the subsampling fraction decreases from \(\nu=1\) to \(\nu=0.1\) (meaning 90% of the features are ignored) (see Fig. (d)d). Similarly, for the imagenette task, test error increases from about 0.1% to about 0.2% as the subsampling fraction decreases from \(\nu=1\) to \(\nu=0.1\) (see Fig. S1c). Code used to run these experiments may be found in the folder "DeepNet_Subsamp" in the GitHub repository.

#### e.4.2 Heterogeneous Ensembling Mitigates Double-Descent

In the second experiment, we compare learning curves for homogeneous ensembling and heterogeneous ensembling applied to the ResNext-Imagewoot classification task. In each trial, we train ensembles of \(k=\{1,4,8,16,32\}\) linear predictors whose subsampling fractions \(\{\nu_{rr},\ldots,\nu_{kk}\}\) are assigned either with Homogeneous ensembling or with Heterogeneous Ensembling with \(\sigma=0.75/k\) C. After subsampling fraction are assigned, the training set is randomly shuffled. We iterate over 50 sample sizes \(P\) logarithmically distributed from \(400\) to \(4000\), and then add the values \(M/k\) for each \(k\) to the list of \(P\) values. We repeat for 100 trials for both \(\lambda_{r}=0\) (pseudoinverse rule) and \(\lambda=0.1\)which was found to mitigate double-descent by the parameter sweep in Fig. S1c. In main text Fig. 3e, we plot the mean learning curves over 100 trials. In Fig. S2 we show standard deviation over the 100 trials as shaded error bars. When \(\lambda=0\), heterogeneous ensembling mitigates double-descent, leading to a monotonically decreasing learning curve for sufficiently high \(k\). When \(\lambda=0.1\), homogeneous and heterogeneous ensembles of size \(k\) perform similarly. Code used to run these experiments may be found in the folder "DeepNet_HomVHet" in the GitHub repository.

#### e.4.3 Readout Noise Encourages Ensembling

In the third experiment, we test the effect of a readout noise which is independent across the members of the ensemble on generalization error. We do this by sweeping over the readout noise scale \(\eta\) as defined in sections E.2, E.3. For \(\lambda=0,0.1\) and \(\nu=0,.1,\ldots,1\) we compute the learning curves of linear predictors with \(k=1,2,3,4,5,10,15,20,25,30,100,200\) and all \(\nu_{rr}=1/k\), averaged over 50 trials. These learning curves are shown in Fig. S3 for both the ResNext-Imagenette and ResNext-Imagewoof task. In Figs. S4a and S5a, we plot the value \(k^{*}\) which minimizes error as page diagrams in the parameter space of \(\alpha\) and \(\eta\), analogous to the phase diagrams in Fig. 4b. We see that the qualitative shape of these phase diagrams is similar to the equicorrelated model. The differences may be attributed to the nonlinear nature of the classification task. Furthermore, we find that, in general the optimal \(k^{*}\) tends to be higher with the Imagenette dataset than with the Imagewoof dataset, in agreement with our finding in the equicorrelated regression model that as \(\rho\) increases (making the classification task easier), optimal ensemble size tends to increase (Fig. 4b and S6, S7, S8). In Fig S3 we see that there are often a number of \(k\) values for which test error is at or near to its lowest. To quantify this, we also plot diagrams of the minimum and maximum values of \(k\) that are within an small margin \(\epsilon\) of the minimum measured error. For the ResNet-Imagenette task, we use \(\epsilon=0.001\) and for ResNext-Imagewoof \(\epsilon=0.01\). We see that there is a wide array of \(k\) values which bring error near-to-minimum in practice (Figs S4b, c, S5b,c). We also plot the minimum achieved error, and the difference between minimum errors at \(\lambda=0.1,0\) (Figs. S4d, S5d). Code used to run these experiments may be found in the folder "DeepNet_PD" in the GitHub repository.

Figure S3: Learning curves for ensembles of linear classifiers with homogeneous subsampling for \(\lambda=0,0.1\) and readout noise \(\eta\) values indicated in the figure. Results are averaged over 50 trials. Experiments are described in section E.4.3

Figure S4: Diagrams described in section E.4.3 for the ResNext-Imagenette experiment. Using learning curves in Fig. S3 (and for additional values of \(\eta\) not shown there), we plot (a) Optimal \(k^{*}\) in the parameter space of \(\alpha\) and \(\eta\), (b) Minimum value of \(k\) for which error is within a tolerance \(\epsilon=.001\) of its value for \(k^{*}\), (b) Maximum value of \(k\) for which error is within a tolerance \(\epsilon=.001\) of its value for \(k^{*}\), (d) the value of the minimum error \(E(k^{*})\), and (e) the difference between this optimal error for \(\lambda=0.1\) and \(\lambda=0\).

Figure S5: Diagrams described in section E.4.3 for the ResNext-Imagewoof experiment. Using learning curves in Fig. S3 (and for additional values of \(\eta\) not shown there), we plot (a) Optimal \(k^{*}\) in the parameter space of \(\alpha\) and \(\eta\), (b) Minimum value of \(k\) for which error is within a tolerance \(\epsilon=.001\) of its value for \(k^{*}\), (b) Maximum value of \(k\) for which error is within a tolerance \(\epsilon=.001\) of its value for \(k^{*}\), (d) the value of the minimum error \(E(k^{*})\), and (e) the difference between this optimal error for \(\lambda=0.1\) and \(\lambda=0\).

Generalization error of ensembled linear regression from the replica trick

Here we use the Replica Trick from statistical physics to derive analytical expressions for \(E_{rr^{\prime}}\). We treat the cases where \(r=r^{\prime}\) and \(r\neq r^{\prime}\) separately. Following a statistical mechanics approach, we calculate the average generalization error over a Gibbs measure with inverse temperature \(\beta\);

\[Z =\int\prod_{r}d\bm{w}\exp\left(-\frac{\beta}{2}\sum_{r}E_{t}^{r}- \frac{M\beta}{2}\sum_{r,r^{\prime}}J_{rr^{\prime}}E_{rr^{\prime}}(\bm{w_{r}}, \bm{w_{r^{\prime}}})\right)\] (30) \[E_{t}^{r} =\sum_{\mu=1}^{P}\left(\frac{1}{\sqrt{N_{r}}}\bm{w}_{r}^{\top} \bm{A}_{r}\bar{\bm{\psi}}_{\mu}+\xi_{r}-y_{\mu}\right)^{2}+\lambda|\bm{w}_{r}^ {2}|\] (31)

In the limit where \(\beta\to\infty\) the gibbs measure will concentrate around the weight vector \(\hat{\bm{w}}_{r}\) which minimizes the regularized loss function. The replica trick relies on the following identity:

\[\langle\log(Z[\mathcal{D}])\rangle_{\mathcal{D}}=\lim_{n\to 0}\frac{1}{n}\log \left(\langle Z^{n}\rangle_{\mathcal{D}}\right)\] (32)

where \(\langle\cdot\rangle_{\mathcal{D}}\) represents an average over all quenched disorder in the system. In this case, quenched disorder - the disorder which is fixed prior to and throughout training of the weights - consists of the selected training examples along with their feature noise and label noise: \(\mathcal{D}=\{\bm{\psi}_{\mu},\bm{\sigma}^{\mu},\epsilon^{\mu}\}_{\mu=1}^{P}\). The calculation proceeds by first computing the average of the replicated partition function assuming \(n\) is a positive integer. Then, in a non-rigorous but standard step, we analytically extend the result to \(n\to 0\).

### Diagonal Terms

We start by calculating \(E_{rr}\) for some fixed choice of \(r\). This derivation partially follows section D.3 from [36], with the addition of readout noise and label noise. Noting that the diagonal terms of the generalization error \(E_{rr}\) only depend on the learned weights \(\bm{w}_{r}\), and the loss function separates over the readouts, we may consider the Gibbs measure over only these weights:

\[Z=\int d\bm{w}_{r}\exp\left(-\frac{\beta}{2\lambda}E_{r}^{t}-\frac{JM\beta}{2 }E_{rr}(\bm{w}_{r})\right)\] (33)

\[\begin{split}\langle Z^{n}\rangle_{\mathcal{D}}=\int\prod_{a}d \bm{w}_{r}^{a}\mathbb{E}_{\{\bm{\psi}_{\mu},\bm{\sigma}^{\mu},\epsilon^{\mu}\} }\\ \exp\left(-\frac{\beta M}{2\lambda}\sum_{\mu,a}\frac{1}{M}\left[ \frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^{\top}\bm{A}_{r}\left(\bm{\psi}_{\mu}+\bm {\sigma}^{\mu}\right)-\bm{w}^{*\top}\bm{\psi}_{\mu}-\sqrt{M}(\epsilon^{\mu}- \xi_{r}^{\mu})\right]^{2}\\ -\frac{\beta}{2}\sum_{a}|\bm{w}_{r}^{a}|^{2}-\frac{JM\beta}{2} \sum_{a}E_{rr}(\bm{w}^{a})\end{split}\] (34)

Next we must perform the averages over quenched disorder. We first integrate over \(\{\bm{\psi}_{\mu},\bm{\sigma}^{\mu},\xi_{r}^{\mu},\epsilon^{\mu}\}_{\mu=1}^{P}\). Noting that the scalars

\[h_{\mu}^{ra}\equiv\frac{1}{\sqrt{M}}\left[\frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^ {a\top}\bm{A}_{r}\left(\bm{\psi}_{\mu}+\bm{\sigma}^{\mu}\right)-\bm{w}^{*\top }\bm{\psi}_{\mu}-\sqrt{M}(\epsilon^{\mu}-\xi_{r}^{\mu})\right]\]

are Gaussian random variables (when conditioned on \(A_{r}\)) with mean zero and covariance:\[\langle h_{\mu}^{ra}h_{\nu}^{rb}\rangle =\delta_{\mu\nu}Q_{ab}^{rr}\] (35) \[Q_{ab}^{rr} =\frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^{a\top} \bm{A}_{r}-\bm{w}^{*\top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_{rr}}} \bm{A}_{r}^{\top}\bm{w}_{r}^{b}-\bm{w}^{*}\right)\right.\] (36) \[\left.+\frac{1}{\nu_{rr}}\bm{w}_{r}^{a\top}\bm{A}_{r}\bm{\Sigma}_ {0}\bm{A}_{r}^{\top}\bm{w}_{r}^{b}\right.+M(\zeta^{2}+\eta_{r}^{2})\big{]}\]

To perform this integral we re-write in terms of \(\{\bm{H}_{\mu}^{r}\}_{\mu=1}^{P}\), where

\[\bm{H}_{\mu}^{r}=\begin{bmatrix}h_{\mu}^{r1}\\ h_{\mu}^{r2}\\ \vdots\\ h_{\mu}^{rn}\end{bmatrix}\in\mathbb{R}^{n}\] (37)

\[\langle Z^{n}\rangle_{\mathcal{D}}=\int\prod_{a}d\bm{w}_{r}^{a}\mathbb{E}_{\{ \psi_{\mu},\bm{\sigma}^{\mu},\bm{\epsilon}^{\mu}\}}\exp\left(-\frac{\beta}{2 \lambda}\sum_{\mu}\bm{H}_{\mu}^{r\top}\bm{H}_{\mu}^{r}-\frac{\beta}{2}\sum_{a }|\bm{w}_{r}^{a}|^{2}-\frac{JM\beta}{2}\sum_{a}E_{rr}(\bm{w}^{a})\right)\] (38)

Integrating over the \(\bm{H}_{\mu}^{r}\) we get:

\[\langle Z^{n}\rangle_{\mathcal{D}}=\int\prod_{a}d\bm{w}_{r}^{a}\exp\left(- \frac{P}{2}\log\det\left(\bm{I}_{n}+\frac{\beta}{\lambda}\bm{Q}^{rr}\right)- \frac{\beta}{2}\sum_{a}|\bm{w}_{r}^{a}|^{2}-\frac{JM\beta}{2}\sum_{a}E_{rr}( \bm{w}_{r})\right)\] (39)

Next we integrate over \(\bm{Q}^{r}\) and add constraints. We use the following identity:

\[\begin{split} 1=\prod_{ab^{\prime}}\int dQ_{ab}^{rr}\delta\left(Q_{ab}^{rr }-\frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^{a\top}\bm{A}_{r} -\bm{w}^{*\top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{A}_{r} ^{\top}\bm{w}_{r}^{b}-\bm{w}^{*}\right)\right.\right.\\ \left.\left.+\frac{1}{\nu_{rr}}\bm{w}_{r}^{a\top}\bm{A}_{r}\bm{ \Sigma}_{0}\bm{A}_{r}^{\top}\bm{w}_{r}^{b}\right.+M(\zeta^{2}+\eta_{r}^{2}) \big{]}\right)\end{split}\] (40)

Using the Fourier representation of the delta function, we get:

\[\begin{split} 1=\prod_{ab}\int\frac{1}{4\pi i/M}dQ_{ab}^{rr}d\dot{Q}_{ab}^{ rr}&\exp\left(\frac{M}{2}\dot{Q}_{ab}^{rr}\left(Q_{ab}^{rr}-\frac{1}{M} \left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^{a\top}\bm{A}_{r}-\bm{w}^{* \top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{A}_{r}^{\top}\bm {w}_{r}^{b}-\bm{w}^{*}\right)\right.\right.\\ \left.\left.+\frac{1}{\nu_{rr}}\bm{w}_{r}^{a\top}\bm{A}_{r}\bm{ \Sigma}_{0}\bm{A}_{r}^{\top}\bm{w}_{r}^{b}\right.+M(\zeta^{2}+\eta_{r}^{2}) \big{]}\right)\end{split}\] (41)

Inserting this identity into the replicated partition function and substituting \(E_{rr}(\bm{w}_{r}^{a})=Q_{aa}^{rr}-\zeta^{2}\) we find:\[\langle Z^{n}\rangle_{\mathcal{D}}\propto\] \[\int\prod_{ab}dQ^{rr}_{ab}d\hat{Q}^{r}_{ab}\exp\left(-\frac{P}{2} \log\det\left(\bm{I}_{n}+\frac{\beta}{\lambda}\bm{Q}^{rr}\right)+\frac{1}{2} \sum_{ab}M\hat{Q}^{rr}_{ab}Q^{rr}_{ab}-\frac{JM\beta}{2}\sum_{a}(Q^{rr}_{aa}- \zeta^{2})\right)\] \[\int\prod_{a}d\bm{w}^{a}_{r}\exp\left(-\frac{\beta}{2}\sum_{a}| \bm{w}^{a}_{r}|^{2}-\frac{1}{2}\sum_{ab}\hat{Q}^{rr}_{ab}\left[\left(\frac{1}{ \sqrt{\nu_{rr}}}\bm{w}^{a\top}_{r}\bm{A}_{r}-\bm{w}^{*\top}\right)\bm{\Sigma}_{ s}\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{A}^{\top}_{r}\bm{w}^{b}_{r}-\bm{w}^{*} \right)\right.\right.\] \[\left.\left.+\frac{1}{\nu_{rr}}\bm{w}^{a\top}_{r}\bm{A}_{r}\bm{ \Sigma}_{0}\bm{A}^{\top}_{r}\bm{w}^{b}_{r}+M(\zeta^{2}+\eta^{2}_{r})\right]\right)\] (42)

In order to perform the Gaussian integral over the \(\{\bm{w}^{a}_{r}\}\), we unfold over the replica index \(a\). We first define the following:

\[\bm{w}^{{}^{\prime}}_{r} \equiv\begin{bmatrix}\bm{w}^{1}_{r}\\ \vdots\\ \bm{w}^{n}_{r}\end{bmatrix}\] (43) \[T^{r} \equiv\beta\bm{I}_{n}\otimes\bm{I}_{N_{r}}+\hat{\bm{Q}}^{rr} \otimes\left(\frac{1}{\nu_{rr}}\bm{A}_{r}(\bm{\Sigma}_{s}+\bm{\Sigma}_{0})\bm{ A}^{\top}_{r}\right)\] (44) \[V^{r} \equiv(\hat{\bm{Q}}^{rr}\otimes\bm{I}_{N_{r}})(\bm{1}_{n}\otimes \frac{1}{\sqrt{\nu_{rr}}}\bm{A}_{r}\bm{\Sigma}_{s}\bm{w}^{*})\] (45)

We then have for the integral over \(w\)

\[\int d\bm{w}^{{}^{\prime}}_{r}\exp\left(-\frac{1}{2}\bm{w}^{{}^{ \top}}_{r}T^{r}\bm{w}^{{}^{\prime}}_{r^{\prime}}+V^{r\top}\bm{w}^{{}^{\prime} }_{r}\right)\] (46) \[= \exp\left(\frac{1}{2}V^{r\top}(T^{r})^{-1}V^{r}-\frac{1}{2}\log \det(T^{r})\right)\] (47)

We can finally write the replicated partition function as:

\[\langle Z^{n}\rangle_{\mathcal{D}}\propto\] \[\int\prod_{ab}dQ^{rr}_{ab}d\hat{Q}^{r}_{ab}\exp\left(-\frac{P}{2} \log\det\left(\bm{I}_{n}+\frac{\beta}{\lambda}\bm{Q}^{rr}\right)+\frac{1}{2} \sum_{ab}M\hat{Q}^{rr}_{ab}Q^{rr}_{ab}-\frac{JM\beta}{2}\sum_{a}(Q^{rr}_{aa}- \zeta^{2})\right)\] \[\exp\left(\frac{1}{2}V^{r\top}(T^{r})^{-1}V^{r}-\frac{1}{2}\log \det(T^{r})-\frac{1}{2}\sum_{ab}\hat{Q}^{rr}_{ab}(M(\zeta^{2}+\eta^{2}_{r})+ \bm{w}^{*\top}\Sigma_{s}\bm{w}^{*})\right)\]

We now make the following replica-symmetric ansatz:

\[Q^{rr}_{ab} =\beta^{-1}q\delta_{ab}+q_{0}\] (49) \[\hat{Q}^{rr}_{ab} =\beta\hat{q}\delta_{ab}+\beta^{2}\hat{q}_{0}\] (50)

which is well-motivated because the loss function is convex. We will verify that the chosen scalings are self-consistent in the zero-temperature limit where \(\beta\rightarrow\infty\). We may then rewrite the partition function as follows:

[MISSING_PAGE_FAIL:27]

### Off-Diagonal Terms

We now calculate \(E_{rr^{\prime}}\) for \(r\neq r^{\prime}\). We now must consider the joint Gibbs Measure over \(\bm{w}_{r}\) and \(\bm{w}_{r^{\prime}}\):

\[Z=\int d\bm{w}_{r}d\bm{w}_{r^{\prime}}\exp\left(-\frac{\beta}{2 \lambda}(E_{r}^{t}+E_{r^{\prime}}^{t})-\frac{JM\beta}{2}E_{rr^{\prime}}(\bm{w}_ {r},\bm{w}_{r^{\prime}})\right)\] (61)

\[\langle Z^{n}\rangle_{\mathcal{D}}=\int\prod_{a}d\bm{w}_{r}^{a}d \bm{w}_{r^{\prime}}^{a}\mathbb{E}_{\{\psi_{\mu},\bm{\sigma}^{\mu},\bm{\epsilon }^{\mu}\}}\] (63)

Where the \(h_{\mu}^{ra}\) are defined as before. Next we must perform the averages over quenched disorder. To do so, we note that the \(h_{\mu}^{ra}\) are Gaussian random variables with covariance structure:

\[\langle h_{\mu}^{ra}h_{\nu}^{r^{\prime}b}\rangle =\delta_{\mu\nu}Q_{ab}^{rr^{\prime}}\] (64) \[Q_{ab}^{rr^{\prime}} =\frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}_{r}^{a \top}\bm{A}_{r}-\bm{w}^{*\top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_ {r^{\prime}r^{\prime}}}}\bm{A}_{r^{\prime}}^{\top}\bm{w}_{r^{\prime}}^{b}- \bm{w}^{*}\right)\right.\] (65) \[\left.+\frac{1}{\sqrt{\nu_{rr}\nu_{r^{\prime}r^{\prime}}}}\bm{w }_{r}^{a\top}\bm{A}_{r}\bm{\Sigma}_{0}\bm{A}_{r^{\prime}}^{\top}\bm{w}_{r^{ \prime}}^{b}+M\zeta^{2}\right]\]

To perform this integral we re-write in terms of \(\{\bm{H}_{\mu}\}_{\mu=1}^{P}\), where

\[\bm{H}_{\mu}=\begin{bmatrix}\bm{H}_{\mu}^{r}\\ \bm{H}_{\mu}^{r}\end{bmatrix}\in\mathbb{R}^{2n}\] (66)

\[\langle Z^{n}\rangle_{\mathcal{D}} =\int\prod_{a}d\bm{w}_{r}^{a}d\bm{w}_{r^{\prime}}^{a}\mathbb{E}_{ \{\psi_{\mu},\bm{\sigma}^{\mu},\bm{\epsilon}^{\mu}\}}\] (67) \[\exp\left(-\frac{\beta}{2\lambda}\sum_{\mu}\bm{H}_{\mu}^{\top}\bm {H}_{\mu}-\frac{\beta}{2}\sum_{a}\left[|\bm{w}_{r}^{a}|^{2}+|\bm{w}_{r^{\prime }}^{a}|^{2}\right]-\frac{JM\beta}{2}\sum_{a}E_{rr^{\prime}}(\bm{w}_{r}^{a},\bm{ w}_{r^{\prime}}^{a})\right)\]

Integrating over \(\bm{H}_{\mu}\) we get:

\[\langle Z^{n}\rangle_{\mathcal{D}}=\int\prod_{a}d\bm{w}_{r}^{a}d \bm{w}_{r^{\prime}}^{a}\] (68) \[\exp\left(-\frac{P}{2}\log\det\left(\bm{I}_{2n}+\frac{\beta}{ \lambda}\bm{Q}\right)-\frac{\beta}{2}\sum_{a}\left[|\bm{w}_{r}^{a}|^{2}+|\bm{ w}_{r^{\prime}}^{a}|^{2}\right]-\frac{JM\beta}{2}\sum_{a}E_{rr}(\bm{w}_{r}^{a}, \bm{w}_{r^{\prime}}^{a})\right)\]

Where we have defined the matrix \(\bm{Q}\) so that:

\[\bm{Q}=\begin{bmatrix}\bm{Q}^{rr}&\bm{Q}^{rr^{\prime}}\\ \bm{Q}^{rr^{\prime}}&\bm{Q}^{r^{\prime}r^{\prime}}\end{bmatrix}\] (69)

Next we integrate over \(\bm{Q}\) and add constraints. We use the following identity:\[1 = \prod_{ab}\int dQ^{rr^{\prime}}_{ab}\delta\left(Q^{rr^{\prime}}_{ab}- \frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}^{a\,\top}_{r}\bm{A}_{r}- \bm{w}^{*\top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_{r^{\prime}r^{ \prime}}}}\bm{A}_{r^{\prime}}^{\top}\bm{w}^{b}_{r^{\prime}}-\bm{w}^{*}\right)\right.\] \[\left.+\frac{1}{\sqrt{\nu_{rr}}\nu_{r^{\prime}r^{\prime}}}\bm{w}^ {a\,\top}_{r}\bm{A}_{r}\bm{\Sigma}_{0}\bm{A}_{r^{\prime}}^{\top}\bm{w}^{b}_{r^ {\prime}}+M\zeta^{2}\right]\right)\]

Using the Fourier representation of the delta function, we get:

\[1 = \prod_{ab}\int\frac{1}{4\pi i/M}dQ^{rr^{\prime}}_{ab}d\hat{Q}^{ rr^{\prime}}_{ab}\exp\left(\frac{M}{2}\hat{Q}^{rr^{\prime}}_{ab}\left(Q^{rr^{ \prime}}_{ab}-\frac{1}{M}\left[\left(\frac{1}{\sqrt{\nu_{rr}}}\bm{w}^{a\, \top}_{r}\bm{A}_{r}-\bm{w}^{*\top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{ \nu_{r^{\prime}r^{\prime}}}}\bm{A}_{r^{\prime}}^{\top}\bm{w}^{b}_{r^{\prime}}- \bm{w}^{*}\right)\right.\right.\] \[\left.\left.+\frac{1}{\nu_{rr}}\bm{w}^{a\,\top}_{r}\bm{A}_{r}\bm {\Sigma}_{0}\bm{A}_{r^{\prime}}^{\top}\bm{w}^{b}_{r^{\prime}}+M\zeta^{2}\right] \right)\right)\]

Inserting this identity and the corresponding statements for \(Q^{rr^{\prime}}_{ab}\) and \(Q^{r^{\prime}r^{\prime}}_{ab}\) into the replicated partition function and substituting \(E_{rr^{\prime}}(\bm{w}^{a})=Q^{rr^{\prime}}_{aa}-\zeta^{2}\) we find:

\[\langle Z^{n}\rangle_{\mathcal{D}}\propto\int\prod_{abr_{1}r_{2}} dQ^{r_{1}r_{2}}_{ab}d\hat{Q}^{r_{1}r_{2}}_{ab}\] \[\exp\left(-\frac{P}{2}\log\det\left(\bm{I}_{2n}+\frac{\beta}{ \lambda}\bm{Q}\right)+\frac{1}{2}\sum_{abr_{1}r_{2}}M\hat{Q}^{r_{1}r_{2}}_{ab} Q^{r_{1}r_{2}}_{ab}-\frac{JM\beta}{2}\sum_{a}(Q^{rr^{\prime}}_{aa}-\zeta^{2})\right)\] \[\int\prod_{a}d\bm{w}^{a}_{r}d\bm{w}^{a}_{r^{\prime}}\exp\left(- \frac{\beta}{2}\sum_{a}\left[|\bm{w}^{a}_{r}|^{2}+|\bm{w}^{a}_{r^{\prime}}|^{2 }\right]-\frac{1}{2}\sum_{abr_{1}r_{2}}\hat{Q}^{r_{1}r_{2}}_{ab}\left[\left( \frac{1}{\sqrt{\nu_{r_{1}}}}\bm{w}^{a\,\top}_{r_{1}}\bm{A}_{r_{1}}-\bm{w}^{* \top}\right)\bm{\Sigma}_{s}\left(\frac{1}{\sqrt{\nu_{r_{2}}}}\bm{A}_{r_{2}}^ {\top}\bm{w}^{b}_{r_{2}}-\bm{w}^{*}\right)\right.\right.\] \[\left.\left.+\frac{1}{\sqrt{\nu_{r_{1}}\nu_{r_{2}}}}\bm{w}^{a\, \top}_{r_{1}}\bm{A}_{r_{1}}\bm{\Sigma}_{0}\bm{A}_{r_{2}}^{\top}\bm{w}^{b}_{r_ {2}}+M\zeta^{2}\right]\right)\] (72)

Where sums over \(r_{1}\) and \(r_{2}\) run over \(\{r,r^{\prime}\}\).

In order to perform the Gaussian integral over the \(\{\bm{w}^{a}_{r}\}\), we unfold in two steps. We first define the following:

\[\bm{w}^{{}^{\prime}}_{r} \equiv \begin{bmatrix}\bm{w}^{1}_{r}\\ \vdots\\ \bm{w}^{n}_{r}\end{bmatrix}\] (73) \[\left[\hat{\bm{Q}}^{rr^{\prime}}\right]_{ab} \equiv \hat{Q}^{rr^{\prime}}_{ab}\] (74) \[\tilde{\bm{\Sigma}}_{rr^{\prime}} \equiv \frac{1}{\sqrt{\nu_{rr}}\nu_{r^{\prime}r^{\prime}}}\bm{A}_{r}[ \bm{\Sigma}_{s}+\bm{\Sigma}_{0}]\bm{A}_{r^{\prime}}^{\top}\] (75) \[T^{rr^{\prime}} \equiv \beta\delta_{rr^{\prime}}\bm{I}_{n}\otimes\bm{I}_{N_{r}}+\hat{ \bm{Q}}^{rr^{\prime}}\otimes\tilde{\bm{\Sigma}}_{rr^{\prime}}\] (76)

Unfolding over the replica indices, we then get:\[\langle Z^{n}\rangle_{\mathcal{D}}\propto\int\prod_{abr_{1}r_{2}}dQ^{r_{1}r _{2}}_{ab}d\hat{Q}^{r_{1}r_{2}}_{ab}\] \[\exp\left(-\frac{P}{2}\log\det\left(\bm{I}_{2n}+\frac{\beta}{\lambda }\bm{Q}\right)+\frac{1}{2}\sum_{abr_{1}r_{2}}M\hat{Q}^{r_{1}r_{2}}_{ab}Q^{r_{1 }r_{2}}_{ab}-\frac{JM\beta}{2}\sum_{a}(Q^{rr^{\prime}}_{aa}-\zeta^{2})\right)\] \[\exp\left(-\frac{1}{2}\sum_{abr_{1}r_{2}}\hat{Q}^{r_{1}r_{2}}_{ab }\left(\bm{w^{*}}^{\top}\bm{\Sigma}_{s}\bm{w}^{*}+M\zeta^{2}\right)\right)\] \[\int d\bm{w}_{r}d\bm{w}_{r^{\prime}}\exp\left(-\frac{1}{2}\sum_{r _{1}r_{2}}\bm{w}_{r_{1}}^{\cdot\top}T^{r_{1}r_{2}}\bm{w}_{r_{2}}+\sum_{r_{1}r _{2}}\left[(\hat{\bm{Q}}^{r_{1}r_{2}}\otimes\bm{I}_{N_{r_{1}}})(\bm{1}_{n} \otimes\frac{1}{\sqrt{\nu_{r_{1}}}}\bm{A}_{r_{1}}\bm{\Sigma}_{s}\bm{w}^{*}) \right]^{\top}\bm{w}_{r_{1}}\right)\] (77)

Note that the dimensionality of \(T^{r_{1}r_{2}}\) varies for different choices of \(r_{1}\) and \(r_{2}\). Next, we unfold over the two readouts:

\[\bm{w} \equiv\begin{bmatrix}\bm{w}_{r}^{\cdot}\\ \bm{w}_{r^{\prime}}^{\cdot}\end{bmatrix}\] (78) \[T \equiv\begin{bmatrix}T^{rr^{\prime}}&T^{rr^{\prime}}\\ T^{r^{\prime}r}&T^{r^{\prime}r^{\prime}}\end{bmatrix}\] (79) \[V \equiv\begin{bmatrix}\left((\hat{\bm{Q}}^{rr}+\hat{\bm{Q}}^{rr^{ \prime}})\otimes\bm{I}_{N_{r}}\right)\left(\bm{1}_{n}\otimes\frac{1}{\sqrt{ \nu_{rr^{\prime}}}}\bm{A}_{r}\bm{\Sigma}_{s}\bm{w}^{*}\right)\\ \left((\hat{\bm{Q}}^{r^{\prime}r^{\prime}}+\hat{\bm{Q}}^{r^{\prime}r})\otimes \bm{I}_{N_{r^{\prime}}}\right)\left(\bm{1}_{n}\otimes\frac{1}{\sqrt{\nu_{rr^{ \prime}}r^{\prime}}}\bm{A}_{r^{\prime}}\bm{\Sigma}_{s}\bm{w}^{*}\right)\end{bmatrix}\] (80)

The integral over w then becomes:

\[\int d\bm{w}\exp\left(-\frac{1}{2}\bm{w}^{\top}T\bm{w}+V^{\top}\bm{w}\right) \propto\exp\left(\frac{1}{2}V^{\top}T^{-1}V-\frac{1}{2}\log\det T\right)\] (81)

We are now ready to make a replica-symmetric ansatz. The order parameter that we wish to constrain is \(Q^{rr^{\prime}}_{ab}\). Overlaps go between the weights from different replicas of the system as well as different readouts. The scale of the overlap between two measurements depends on their overlap with each other and with the principal components of the data distribution. An ansatz which is replica-symmetric but makes no assumptions about the overlaps between different measurements is as follows:

\[Q^{r_{1}r_{2}}_{ab} =\beta^{-1}q^{r_{1}r_{2}}\delta_{ab}+Q^{r_{1}r_{2}}\] (82) \[\hat{Q}^{r_{1}r_{2}}_{ab} =\beta\hat{q}^{r_{1}r_{2}}\delta_{ab}+\beta^{2}\hat{Q}^{r_{1}r_{2}}\] (83)

Next step is to plug the RS ansatz into the free energy and simplify. To make calculations more transparent, we re-label the paramters in the RS ansatz as follows:

\[\bm{Q}^{rr} =\beta^{-1}q\bm{I}+Q\bm{1}\bm{1}^{\top}\] (84) \[\bm{Q}^{r^{\prime}r^{\prime}} =\beta^{-1}r\bm{I}+R\bm{1}\bm{1}^{\top}\] (85) \[\bm{Q}^{rr^{\prime}} =\beta^{-1}c\bm{I}+C\bm{1}\bm{1}^{\top}\] (86) \[\hat{\bm{Q}}^{rr} =\beta\hat{q}\bm{I}+\beta^{2}\hat{Q}\bm{1}\bm{1}^{\top}\] (87) \[\hat{\bm{Q}}^{r^{\prime}r^{\prime}} =\beta\hat{r}\bm{I}+\beta^{2}\hat{R}\bm{1}\bm{1}^{\top}\] (88) \[\hat{\bm{Q}}^{rr^{\prime}} =\beta\hat{c}\bm{I}+\beta^{2}\hat{C}\bm{1}\bm{1}^{\top}\] (89)In order to simplify \(\log\det\left(\lambda\bm{I}_{2n}+\beta\bm{Q}\right)\), we note that this is a symmetric 2-by-2-block matrix, where each block commutes with all other blocks. We may then use [53]'s result to simplify.

\[\log\det\left(\lambda\bm{I}_{2n}+\beta\bm{Q}\right)=n\left[\log \left((\lambda+q)(\lambda+r)-c^{2}\right)+\beta\frac{(\lambda+q)R+(\lambda+r)Q -2cC}{(\lambda+q)(\lambda+r)-c^{2}}\right]+\mathcal{O}(n^{2})\] (90)

\[\sum_{abr_{1}r_{2}}\hat{\bm{Q}}_{ab}^{r_{1}r_{2}}\bm{Q}_{ab}^{r_{1}r_{2}}=n \left[\left(q\hat{q}+\beta\hat{q}Q+\beta q\hat{Q}\right)+\left(r\hat{r}+\beta \hat{r}R+\beta r\hat{R}\right)+2\left(c\hat{c}+\beta\hat{c}C+\beta c\hat{C} \right)\right]+\mathcal{O}(n^{2})\] (91)

\[\sum_{a}\left(\bm{Q}_{aa}^{rr^{\prime}}-\zeta^{2}\right)=n\left[ \frac{1}{\beta}c+C-\zeta^{2}\right]+\mathcal{O}(n^{2})\] (92)

\[\sum_{abr_{1}r_{2}}\hat{\bm{Q}}_{ab}^{r_{1}r_{2}}=\beta n\left[ \hat{q}+\hat{r}+2\hat{c}\right]+\mathcal{O}(n^{2})\] (93)

\[\log\det(T)=n\left[\log(\beta)+\log\det\begin{bmatrix}\bm{G}_{rr}&\bm{G}_{rr^{ \prime}}\\ \bm{G}_{r^{\prime}r}&\bm{G}_{r^{\prime}r^{\prime}}\end{bmatrix}+\beta\operatorname {tr}\left(\begin{bmatrix}\bm{G}_{rr}&\bm{G}_{rr^{\prime}}\\ \bm{G}_{r^{\prime}r}&\bm{G}_{r^{\prime}r^{\prime}}\end{bmatrix}^{-1}\begin{bmatrix} \hat{Q}\tilde{\bm{\Sigma}}_{rr}&\hat{C}\tilde{\bm{\Sigma}}_{rr^{\prime}}\\ \hat{C}\tilde{\bm{\Sigma}}_{r^{\prime}r}&\hat{R}\tilde{\bm{\Sigma}}_{r^{ \prime}r^{\prime}}\end{bmatrix}\right)\right]+\mathcal{O}(n^{2})\] (94)

where \(\bm{G}_{rr}=\bm{I}_{N_{r}}+\hat{q}\tilde{\bm{\Sigma}}_{rr}\)\(\bm{G}_{r^{\prime}r^{\prime}}=\bm{I}_{N_{r^{\prime}}}+\hat{r}\tilde{\bm{\Sigma}}_{r^{ \prime}r^{\prime}}\)\(\bm{G}_{rr^{\prime}}=\hat{c}\tilde{\bm{\Sigma}}_{rr^{\prime}}\)\(\bm{G}_{r^{\prime}r}=\hat{c}\tilde{\bm{\Sigma}}_{r^{\prime}r}\) (95)

\[V^{\top}T^{-1}V=n\beta\bm{w}^{*\top}\begin{bmatrix}\frac{1}{\sqrt{ \nu_{rr}^{\prime}}}(\hat{q}+\hat{c})\bm{A}_{r}\bm{\Sigma}_{s}\\ \frac{1}{\sqrt{\nu_{rr^{\prime}r^{\prime}}}}(\hat{r}+\hat{c})\bm{A}_{r^{\prime} }\bm{\Sigma}_{s}\end{bmatrix}^{\top}\begin{bmatrix}\bm{G}_{rr}&\bm{G}_{rr^{ \prime}}\\ \bm{G}_{r^{\prime}r}&\bm{G}_{r^{\prime}r^{\prime}}\end{bmatrix}^{-1}\begin{bmatrix} \frac{1}{\sqrt{\nu_{rr}}}(\hat{q}+\hat{c})\bm{A}_{r}\bm{\Sigma}_{s}\\ \frac{1}{\sqrt{\nu_{rr^{\prime}r^{\prime}}}}(\hat{r}+\hat{c})\bm{A}_{r^{\prime }}\bm{\Sigma}_{s}\end{bmatrix}\bm{w}^{*}+\mathcal{O}(n^{2})\] (96)

Collecting these terms, we may write the replicated partition function as follows:

\[\langle Z^{n}\rangle_{\mathcal{D}}=\exp\left(-\frac{nM}{2}\mathfrak{g}\left[q, Q,r,R,c,C,\hat{q},\hat{Q},\hat{r},\hat{R},\hat{c},\hat{C}\right]\right)\] (97)

Where the free energy is written:

\[\mathfrak{g}\left[q,Q,r,R,c,C,\hat{q},\hat{Q},\hat{r},\hat{R}, \hat{c},\hat{C}\right]=\] (98) \[\alpha\left[\log\left((\lambda+q)(\lambda+r)-c^{2}\right)+\beta \frac{(\lambda+q)R+(\lambda+r)Q-2cC}{(\lambda+q)(\lambda+r)-c^{2}}\right]\] (99) \[-\left[\left(q\hat{q}+\beta\hat{q}Q+\beta q\hat{Q}\right)+\left( r\hat{r}+\beta\hat{r}R+\beta r\hat{R}\right)+2\left(c\hat{c}+\beta\hat{c}C+\beta c \hat{C}\right)\right]\] (100) \[+J(c+\beta C-\beta\zeta^{2})\] (101) \[+\beta\left[\hat{q}+\hat{r}+2\hat{c}\right]\left(\frac{1}{M}\bm{ w}^{*\top}\Sigma\bm{w}^{*}+\zeta^{2}\right)\] (102) \[-\frac{1}{M}\beta\bm{w}^{*\top}\begin{bmatrix}\frac{1}{\sqrt{\nu _{rr}^{\prime}}}(\hat{q}+\hat{c})\bm{A}_{r}\bm{\Sigma}_{s}\\ \frac{1}{\sqrt{\nu_{rr^{\prime}r^{\prime}}}}(\hat{r}+\hat{c})\bm{A}_{r^{\prime }}\bm{\Sigma}_{s}\end{bmatrix}^{\top}\bm{G}^{-1}\begin{bmatrix}\frac{1}{\sqrt{ \nu_{rr}}}(\hat{q}+\hat{c})\bm{A}_{r}\bm{\Sigma}_{s}\\ \frac{1}{\sqrt{\nu_{rr^{\prime}r^{\prime}}}}(\hat{r}+\hat{c})\bm{A}_{r^{\prime }}\bm{\Sigma}_{s}\end{bmatrix}\bm{w}^{*}\] (103) \[+\frac{1}{M}\left[\log(\beta)+\log\det\bm{G}+\beta\operatorname{ tr}\left(\bm{G}^{-1}\begin{bmatrix}\hat{Q}\tilde{\bm{\Sigma}}_{rr}&\hat{C}\tilde{\bm{\Sigma}}_{ rr^{\prime}}\\ \hat{C}\tilde{\bm{\Sigma}}_{r^{\prime}r}&\hat{R}\tilde{\bm{\Sigma}}_{r^{\prime}r^{\prime}} \end{bmatrix}\right)\right]\] (104)where we have defined \(\bm{G}\equiv\begin{bmatrix}\bm{G}_{rr}&\bm{G}_{rr^{\prime}}\\ \bm{G}_{r^{\prime}r}&\bm{G}_{r^{\prime}r^{\prime}}\end{bmatrix}\)

The saddle-point equations for the replica-diagonal order parameters are:

\[\frac{\partial\mathfrak{g}}{\partial Q} =0=\beta\frac{\alpha(\lambda+r)}{(\lambda+q)(\lambda+r)-c^{2}}- \beta\hat{q}\] (105) \[\frac{\partial\mathfrak{g}}{\partial\hat{Q}} =0=-\beta q+\beta\frac{1}{M}\operatorname{tr}\left(\bm{G}^{-1} \begin{bmatrix}\tilde{\bm{\Sigma}}_{rr}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\right)\] (106) \[\frac{\partial\mathfrak{g}}{\partial R} =0=\beta\frac{\alpha(\lambda+q)}{(\lambda+q)(\lambda+r)-c^{2}}- \beta\hat{r}\] (107) \[\frac{\partial\mathfrak{g}}{\partial\hat{R}} =0=-\beta r+\beta\frac{1}{M}\operatorname{tr}\left(\bm{G}^{-1} \begin{bmatrix}\bm{0}&\bm{0}\\ \bm{0}&\tilde{\bm{\Sigma}}_{r^{\prime}r^{\prime}}\end{bmatrix}\right)\] (108) \[\frac{\partial\mathfrak{g}}{\partial\hat{C}} =0=-\beta\frac{2\alpha c}{(\lambda+q)(\lambda+r)-c^{2}}-2\beta \hat{c}+\beta J\] (109) \[\frac{\partial\mathfrak{g}}{\partial\hat{C}} =0=-2\beta c+\beta\frac{1}{M}\operatorname{tr}\left(\bm{G}^{-1} \begin{bmatrix}\bm{0}&\tilde{\bm{\Sigma}}_{rr^{\prime}}\\ \tilde{\bm{\Sigma}}_{r^{\prime}r}&\bm{0}\end{bmatrix}\right)\] (110)

Note that when \(J=0\), the saddle point equations 109, 110 are solved by setting \(c=\hat{c}=0\), and in this case the remaining saddle-point equations decouple over the readouts (as expected for independently trained ensemble members) giving: For readout \(r\):

\[0 =\frac{\alpha}{(\lambda+q)}-\hat{q}\] (111) \[0 =-q+\frac{1}{M}\operatorname{tr}\left(\bm{G}_{rr}^{-1}\tilde{\bm {\Sigma}}_{rr}\right)\] (112)

and for readout \(r^{\prime}\):

\[0 =\frac{\alpha}{(\lambda+r)}-\hat{r}\] (113) \[0 =-r+\frac{1}{M}\operatorname{tr}\left(\bm{G}_{rr^{\prime}r}^{-1} \tilde{\bm{\Sigma}}_{r^{\prime}r^{\prime}}\right)\] (114)

These are equivalent to the saddle-point equations for a single readout given in equation 53, 54 as expected for independently trained readouts. It is physically sensible that \(c=0\) when \(J=0\), because at zero source, there is no term in the replicated system energy function which would distinguish the overlap between two readouts from the same replica from the overlap between two readouts in separate replicas (we expect that the total overlap between readouts is non-zero, as we may still have \(C>0\)).

The saddle-point equations obtained by setting the derivatives \(\frac{\partial\mathfrak{g}}{\partial q}=\frac{\partial\mathfrak{g}}{\partial \hat{q}}=\frac{\partial\mathfrak{g}}{\partial r}=\frac{\partial\mathfrak{g}}{ \partial r}=0\) will similarly decouple to recover two copies of the diagonal case 57 55. We will not re-write the expressions here as they are not necessary to determine the off-diagonal error term \(E_{rr^{\prime}}\).

The remaining saddle-point equations are obtained by setting \(\frac{\partial\mathfrak{g}}{\partial c}=\frac{\partial\mathfrak{g}}{\partial \hat{c}}=0\)

\[\frac{\partial\mathfrak{g}}{\partial c}\bigg{|}_{c=\hat{c}=J=0}=-\frac{2\alpha \beta C}{(\lambda+q)(\lambda+r)}-2\beta\hat{C}\quad\Rightarrow\quad\hat{C}=- \frac{\alpha C}{(\lambda+q)(\lambda+r)}\] (115)\[\begin{split}\frac{\partial\mathfrak{g}}{\partial\hat{c}}\bigg{|}_{c =\hat{c}=J=0}=&-2\beta C+2\beta\left(\frac{1}{M}{\boldsymbol{w}^{ *}}^{\top}\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}+\zeta^{2}\right)\\ &-\frac{2\beta}{M}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{\Sigma} _{s}\left[\frac{1}{\nu_{rr}}\hat{q}\boldsymbol{A}_{r}^{\top}\boldsymbol{G}_{ rr}^{-1}\boldsymbol{A}_{r}+\frac{1}{\nu_{r^{\prime}r^{\prime}}}\hat{r}\boldsymbol{A}_{r^{ \prime}}^{\top}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1}\boldsymbol{A}_{r^{ \prime}}\right]\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\\ &+\frac{2\beta\hat{q}\hat{r}}{M}\frac{1}{\sqrt{\nu_{rr}\nu_{r^{ \prime}r^{\prime}}}}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{\Sigma}_{s} \boldsymbol{A}_{r}^{\top}\boldsymbol{G}_{rr}^{-1}\tilde{\boldsymbol{\Sigma}}_ {rr^{\prime}}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1}\boldsymbol{A}_{r^{ \prime}}\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\\ &-\frac{2\hat{C}\beta}{M}\operatorname{tr}\left[\boldsymbol{G}_ {rr}^{-1}\tilde{\boldsymbol{\Sigma}}_{rr^{\prime}}\boldsymbol{G}_{rr^{\prime }r^{\prime}}^{-1}\tilde{\boldsymbol{\Sigma}}_{r^{\prime}r}\right]\end{split}\] (116)

Solving equations 115 and 116 for \(C\), we obtain:

\[\begin{split} C&=\frac{1}{1-\gamma}\zeta^{2}+\frac{1 }{1-\gamma}\left(\frac{1}{M}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{\Sigma}_{s} \boldsymbol{w}^{*}\right)\\ &-\frac{1}{M(1-\gamma)}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{ \Sigma}_{s}\left[\frac{1}{\nu_{rr}}\hat{q}\boldsymbol{A}_{r}^{\top}\boldsymbol {G}_{rr}^{-1}\boldsymbol{A}_{r}+\frac{1}{\nu_{r^{\prime}r^{\prime}}}\hat{r} \boldsymbol{A}_{r^{\prime}}^{\top}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1} \boldsymbol{A}_{r^{\prime}}\right]\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\\ &+\frac{1}{M(1-\gamma)}\hat{q}^{\hat{r}}\frac{1}{\sqrt{\nu_{rr} \nu_{r^{\prime}r^{\prime}}}}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{\Sigma}_{s} \boldsymbol{A}_{r}^{\top}\boldsymbol{G}_{rr}^{-1}\tilde{\boldsymbol{\Sigma}}_ {rr^{\prime}}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1}\boldsymbol{A}_{r^{ \prime}}\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\\ \text{where}\quad\gamma\equiv\frac{\alpha}{(\lambda+q)(\lambda+r )}\operatorname{tr}\left[\boldsymbol{G}_{rr}^{-1}\tilde{\boldsymbol{\Sigma}}_ {rr^{\prime}}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1}\tilde{\boldsymbol{ \Sigma}}_{r^{\prime}r}\right]\end{split}\] (117)

We can obtain the generalization error as:

\[E_{rr^{\prime}}=\lim_{\beta\to\infty}\frac{1}{\beta}\frac{\partial}{\partial J }\mathfrak{g}\left[q,Q,r,R,c,C,\hat{q},\hat{Q},\hat{r},\hat{R},\hat{c},\hat{C} \right]=\lim_{\beta\to\infty}\frac{1}{\beta}(c+\beta C-\beta\zeta^{2})=C-\zeta ^{2}\] (118)

We may then simplify the expression for the \(E_{rr^{\prime}}\) error as follows:

\[\begin{split} E_{rr^{\prime}}&=\frac{\gamma}{1- \gamma}\zeta^{2}+\frac{1}{1-\gamma}\left(\frac{1}{M}{\boldsymbol{w}^{*}}^{ \top}\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\right)\\ &-\frac{1}{M(1-\gamma)}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{ \Sigma}_{s}\left[\frac{1}{\nu_{rr}}\hat{q}\boldsymbol{A}_{r}^{\top} \boldsymbol{G}_{rr}^{-1}\boldsymbol{A}_{r}+\frac{1}{\nu_{r^{\prime}r^{\prime}} }\hat{r}\boldsymbol{A}_{r^{\prime}}^{\top}\boldsymbol{G}_{rr^{\prime}r^{ \prime}}^{-1}\boldsymbol{A}_{r^{\prime}}\right]\boldsymbol{\Sigma}_{s} \boldsymbol{w}^{*}\\ &+\frac{1}{M(1-\gamma)}\hat{q}^{\hat{r}}\frac{1}{\sqrt{\nu_{rr} \nu_{r^{\prime}r^{\prime}}}}{\boldsymbol{w}^{*}}^{\top}\boldsymbol{\Sigma}_{s} \boldsymbol{A}_{r}^{\top}\boldsymbol{G}_{rr}^{-1}\tilde{\boldsymbol{\Sigma}}_ {rr^{\prime}}\boldsymbol{G}_{r^{\prime}r^{\prime}}^{-1}\boldsymbol{A}_{r^{ \prime}}\boldsymbol{\Sigma}_{s}\boldsymbol{w}^{*}\end{split}\] (120)

Re-labeling the order parameters: \(\hat{q}\to\hat{q}_{r}\), \(\hat{r}\to\hat{q}_{r^{\prime}}\), \(\gamma\to\gamma_{rr^{\prime}}\) and \(\boldsymbol{G}_{rr}\to\boldsymbol{G}_{r}\), we obtain the result given in the main text.

## Appendix G Derivation of Proposition 1 from [31]

In the case where the data and noise covariance matrices \(\boldsymbol{\Sigma}_{s}\) and \(\boldsymbol{\Sigma}_{0}\) have bounded spectra, our main result may be derived using Theorem 4.1 from Loureiro et. al. [31], with a few additional arguments to incorporate a readout noise which varies across ensemble members, and to allow for the presence of label noise in the training set but not at test time. Rather than reproducing their very lengthy statements here, we direct the reader to theorem 4.1 and corollary 4.2 in [31].

### Altered Expectations at Evaluation

In [31], labels are generated at both training and test time through the same statistical process \(y\sim P_{y}^{0}(y|\nu)\) where \(\nu=\frac{\langle\bm{x}|\bm{\theta}\rangle}{\sqrt{d}}\). However, their results may be easily extended to the case where data are generated through a different statistical process for the training set and at evaluation. This will allow the application of their results to the case where label noise is present during training but not at evaluation as studied in this work. We therefore introduce separate distributions of the labels \(y\) at training and evaluation. During training, we still have \(y\sim P_{y}^{0}(y|\nu)\). At evaluation, we put \(y\sim P_{y}^{g}(y|\nu)\). This leads to the updated formula:

\[\mathbb{E}_{(y,\bm{x})}\left[\varphi\left(y,\frac{\langle\langle\hat{\bm{W}} \mid\bm{U}\rangle\rangle}{\sqrt{p}}\right)\right]\overset{\mathrm{P}}{\to} \mathbb{E}_{(\nu,\bm{\mu})}\left[\int dyP_{y}^{g}(y|\nu)\varphi(y,\bm{\mu})\right]\] (121)

when the expectation is over data-label pairs \((y,\bm{x})\) at _evaluation_.

### Rigorous Proof of Proposition 1

We now restate the the problem setup of our main theorem using notation consistent with [31]. We study generalization error in an ensemble of estimators \(\{\hat{\bm{w}}_{k}\}\), \(k=1,\ldots,K\). We say \(\hat{\bm{w}}_{k}\in\mathbb{R}^{p}\) for all \(k=1,\ldots,K\). The weights are trained independently such that each minimizes a ridge loss function:

\[\hat{\bm{w}}_{k} =\operatorname*{arg\,min}_{\bm{w}}\sum_{\mu=1}^{n}\left(y^{\mu}- \frac{1}{\sqrt{N_{k}}}\bm{u}_{k}^{\top}\bm{w}-\xi_{k}^{\mu}\right)^{2}+\lambda _{k}|\bm{w}|^{2}\qquad k=1,\ldots,K\] (122) \[=\operatorname*{arg\,min}_{\bm{w}}\sum_{\mu=1}^{n}\left(y^{\mu}- \frac{1}{\sqrt{p}}\bm{u}_{k}^{\top}\left(\frac{1}{\sqrt{\nu_{kk}}}\bm{w} \right)-\xi_{k}^{\mu}\right)^{2}+\nu_{kk}\lambda_{k}|\frac{1}{\sqrt{\nu_{kk}} }\bm{w}|^{2}\]

So that our results will correspond to the results of [31] after re-scaling the regularizations \(\lambda_{k}\to\nu_{kk}\lambda_{k}\). The training labels are drawn as \(y^{\mu}\sim P_{y}^{0}(y|\frac{1}{\sqrt{p}}\bm{\theta}^{\top}\bm{x}^{\mu})\) where \(P_{y}^{0}(y|x)=\mathcal{N}(x,\zeta^{2})\), \(\xi_{k}^{\mu}\sim\mathcal{N}(0,\eta_{r}^{2})\) and \(\theta\) represent the ground truth weights. For \(k=1,\ldots,k\) we have a "measurement matrix" \(\bm{A}_{k}\in\mathbb{R}^{N_{k}\times d}\), and we may set \(d=p\geq\max_{k}N_{k}\) (so that \(\gamma=\frac{d}{p}=1\)). The feature vectors \(\bm{u}_{k}(x)\) are then drawn as

\[\bm{u}_{k}(\bm{x})=\begin{bmatrix}\bm{A}_{k}(\bm{x}+\bm{\sigma})\\ \bm{0}_{(p-N_{k})}\end{bmatrix}=\bar{\bm{A}}_{k}(\bm{x}+\bm{\sigma})\]

where \(\bm{x}\sim\mathcal{N}(0,\bm{\Sigma}_{s})\) and \(\bm{\sigma}\sim\mathcal{N}(0,\bm{\Sigma}_{0})\). For convenience, we have defined the auxilary matrices

\[\bar{\bm{A}}_{k}\equiv\begin{bmatrix}\bm{A}_{k}\\ \bm{0}_{(p-N_{k})\times p}\end{bmatrix}\in\mathbb{R}^{p\times p}\]

By constructing the feature vectors as p-dimensional vectors with only \(N_{k}\) non-zero components, we may apply the results of [31] while preserving structural heterogeneity (we may have \(N_{k}\neq N_{k^{\prime}}\) for \(k\neq k^{\prime}\)) as is present in our main result. Because \([\bm{u}_{k}]_{i}=0\) for all \(i>N_{k}\), these auxiliary dimensions will not affect model predictions or generalization error. We then have \(\mu_{k}=\frac{1}{\sqrt{p}}\bm{u}_{k}^{\top}\hat{\bm{w}}_{k}\), and labels are generated at evaluation according to :

\[y\sim P_{y}^{g}(y|\frac{1}{K}\sum_{k}\mu_{k})\text{ where }P_{y}^{g}(y|x)= \mathcal{N}(x,\frac{1}{K^{2}}\sum_{k}\eta_{k}^{2})\] (123)

The generalization error may then be decomposed as \(E_{g}=\frac{1}{K^{2}}\sum_{k,k^{\prime}}E_{kk^{\prime}}\) where \(E_{kk^{\prime}}=\mathbb{E}_{(y,\bm{x})}\left[(y-\mu_{k})(y-\mu_{k^{\prime}})\right]\). We will apply eq. 121 separately to calculate \(E_{rr^{\prime}}\) in the cases where \(r\neq r^{\prime}\) and \(r=r^{\prime}\).

#### g.2.1 Off-Diagonal Terms

Eq. 121 cannot be used to calculate \(E_{rr^{\prime}}\) directly in the case where \(r\neq r^{\prime}\) due to the presence of noises \(\xi_{k}^{\mu}\) which vary over elements of the ensemble (indexed by \(k\)) in the loss function (eq. 122). We may argue, however, that the presence of readout noise noise has no effect on the expected value of \(E_{rr^{\prime}}\) when \(r\neq r^{\prime}\), then apply eq. 121 with \(\eta_{k}=\eta_{k^{\prime}}=0\). To see this, we examine the analytical form of the minimizer of the loss function (eq, 122):

\[\hat{\bm{w}}_{k}=\bm{U}_{k}^{\top}\left(\bm{U}_{k}\bm{U}_{k}^{\top}+\lambda_{k }\bm{I}\right)^{-1}(\bm{y}-\bm{\xi}_{k})\] (124)

where we have defined the design matrices \([\bm{U}_{k}]_{\mu}=\frac{1}{\sqrt{p}}[\bm{u}_{k}(\bm{x}^{\mu})]_{i}\) and the vectors \([\bm{y}]_{\mu}=y^{\mu}\) and \([\bm{\xi}_{k}]_{\mu}=\xi_{k}^{\mu}\). We then have

\[E_{kk^{\prime}} =\left(y-\frac{1}{\sqrt{p}}\bm{y}^{\top}\left(\bm{U}_{k}\bm{U}_{k }^{\top}+\lambda_{k}\bm{I}\right)^{-1}\bm{U}_{k}\right)\left(y-\frac{1}{\sqrt{p }}\bm{y}^{\top}\left(\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{\top}+\lambda_{k ^{\prime}}\bm{I}\right)^{-1}\bm{U}_{k^{\prime}}\right)\] \[+\left(y-\frac{1}{\sqrt{p}}\bm{y}^{\top}\left(\bm{U}_{k}\bm{U}_{k }^{\top}+\lambda_{k}\bm{I}\right)^{-1}\bm{U}_{k}\right)\left(\frac{1}{\sqrt{p }}\bm{\xi}_{k^{\prime}}^{\top}\left(\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{ \top}+\lambda_{k^{\prime}}\bm{I}\right)^{-1}\bm{U}_{k^{\prime}}\right)\] \[+\left(y-\frac{1}{\sqrt{p}}\bm{y}^{\top}\left(\bm{U}_{k^{\prime}} \bm{U}_{k^{\prime}}^{\top}+\lambda_{k^{\prime}}\bm{I}\right)^{-1}\bm{U}_{k^{ \prime}}\right)\left(\frac{1}{\sqrt{p}}\bm{\xi}_{k}^{\top}\left(\bm{U}_{k}\bm{ U}_{k}^{\top}+\lambda_{k}\bm{I}\right)^{-1}\bm{U}_{k}\right)\] \[+\left(\frac{1}{\sqrt{p}}\bm{\xi}_{k}^{\top}\left(\bm{U}_{k}\bm{ U}_{k}^{\top}+\lambda_{k}\bm{I}\right)^{-1}\bm{U}_{k}\right)\left(\frac{1}{\sqrt{p }}\bm{\xi}_{k^{\prime}}^{\top}\left(\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{ \top}+\lambda_{k^{\prime}}\bm{I}\right)^{-1}\bm{U}_{k^{\prime}}\right)\] (125)

Taking the expectation value over the readout noise in the training set we get:

\[\mathbb{E}_{\{\bm{\xi}_{1},\ldots,\bm{\xi}_{K}\}}E_{kk^{\prime}}=\left(y- \frac{1}{\sqrt{p}}\bm{y}^{\top}\left(\bm{U}_{k}\bm{U}_{k}^{\top}+\lambda_{k} \bm{I}\right)^{-1}\bm{U}_{k}\right)\left(y-\frac{1}{\sqrt{p}}\bm{y}^{\top} \left(\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{\top}+\lambda_{k^{\prime}}\bm{ I}\right)^{-1}\bm{U}_{k^{\prime}}\right)\quad(k\neq k^{\prime})\] (126)

This is identical to \(E_{kk^{\prime}}\) when \(\bm{\xi}_{k}=0\) for all \(k\). We may therefore calculate the off-diagonal error terms by setting \(\bm{\xi}_{k}=0\) in eq. 122, which gives a problem compatible with the theorem 4 of [31]. To calculate \(E_{rr^{\prime}}\), we appeal to theorem 4.1 and corollary 4.2 of [31]. The following objects defined in 121 are given the following definitions:

\[r(\{\hat{\bm{w}}_{1},\ldots,\hat{\bm{w}}_{K}\}) =\frac{1}{2}\sum_{k=1}^{K}\nu_{kk}\lambda_{k}|\hat{\bm{w}}_{k}| ^{2}\] (127) \[\Delta(y,\hat{y}(\bm{x})) =(y-\hat{y}(\bm{x}))^{2}\] (128) \[\hat{\ell}(y,\bm{\mu}) =\frac{1}{2}|\bm{\mu}-y\bm{1}|^{2}\] (129)

\[\mathcal{Z}^{0}(y,\mu,\sigma):=\int\frac{P_{y}^{0}(y\mid x){\rm d }x}{\sqrt{2\pi\sigma}}{\rm e}^{-\frac{(x-\mu)^{2}}{2\sigma}}=\frac{1}{\sqrt{2 \pi(\zeta^{2}+\sigma)}}\exp\left(-\frac{(y-\mu)^{2}}{2(\zeta^{2}+\sigma)}\right)\] (130)

\[E_{kk^{\prime}} =\mathbb{E}_{(y,\bm{x})}\left[(y-\mu_{k})(y-\mu_{k^{\prime}}) \right]\rightarrow\mathbb{E}_{(\nu,\bm{x})}\left[\int dyP_{y}^{g}(y|\nu)(y-\mu_ {k})(y-\mu_{k^{\prime}})\right]\] (131) \[=\mathbb{E}_{(\nu,\bm{x})}\left[(\nu-\mu_{k})(\nu-\mu_{k^{\prime}} )\right]+\frac{1}{K^{2}}\sum_{k=1}^{K}\eta_{k}^{2}\] (132) \[=\rho-[\bm{m}]_{k}-[\bm{m}]_{k^{\prime}}+[\bm{Q}]_{kk^{\prime}}+ \frac{1}{K^{2}}\sum_{k=1}^{K}\eta_{k}^{2}\] (133)

Where \(\rho\), \(\bm{m}\) and \(\bm{Q}\) are defined as in [31]:

\[(\nu,\bm{\mu})\sim\mathcal{N}\left(\bm{0}_{1+K},\left(\begin{array}{cc}\rho& \bm{m}^{\top}\\ \bm{m}&\bm{Q}\end{array}\right)\right)\] (134)What remains is to determine the values of the order parameters \(\rho\), \(\bm{m}\) and \(\bm{Q}\). We next define the covariance matrices which characterize the feature maps. The feature-feature covariance is:

\[\bm{\Omega}_{kk^{\prime}}^{ij} =\mathbb{E}_{\bm{x}}\left[(\bm{u}_{k}(\bm{x}))_{i}(\bm{u}_{k^{ \prime}}(\bm{x}))_{j}\right]=\mathbbm{1}_{\{1\leq i\leq N_{k},1\leq j\leq N_{k^ {\prime}}\}}\left[\bm{A}_{k}\left(\bm{\Sigma}_{s}+\bm{\Sigma}_{0}\right)\bm{A}_ {k^{\prime}}^{\top}\right]_{ij}\] (135) \[=\sqrt{\nu_{kk}\nu_{k^{\prime}k^{\prime}}}\mathbbm{1}_{\{1\leq i \leq N_{k},1\leq j\leq N_{k^{\prime}}\}}\left[\tilde{\bm{\Sigma}}_{kk^{\prime} }\right]_{ij}=\left[\tilde{\bm{\Sigma}}_{kk^{\prime}}\right]_{ij}\] (136)

where we have defined \(\nu_{kk}=\frac{N_{k}}{p}\), \(\tilde{\bm{\Sigma}}_{kk^{\prime}}=\frac{1}{\sqrt{\nu_{kk}\nu_{k^{\prime}k^{ \prime}}}}\bm{A}_{k}\left(\bm{\Sigma}_{s}+\bm{\Sigma}_{0}\right)\bm{A}_{k^{ \prime}}^{\top}\), and \(\tilde{\bm{\Sigma}}_{kk^{\prime}}=\sqrt{\nu_{kk}\nu_{k^{\prime}k^{\prime}}} \begin{bmatrix}\tilde{\bm{\Sigma}}_{kk^{\prime}}&\bm{0}\\ \bm{0}&\bm{0}\end{bmatrix}\in\mathbb{R}^{p\times p}\). Note that while in [31], all \(\bm{\Omega}_{kk}\) must have strictly positive eigenvalues, their result can be easily extended to cover the case where some eigenvalues are zero by a continuity argument.

The feature-label covariance is given by:

\[\left[\bm{\hat{\Phi}}\right]_{k}^{i}=\mathbb{E}_{\bm{x}}\left[\bm{u}_{k}(\bm{ x})\bm{x}^{\top}\bm{\theta}\right]_{i}=\mathbbm{1}_{\{1\leq i\leq N_{k}\}} \left[\bm{A}_{k}\bm{\Sigma}_{s}\bm{\theta}\right]_{i}=\left[\tilde{\bm{A}}_{k }\bm{\Sigma}_{s}\bm{\theta}\right]_{i}\] (137)

\[\left[\bm{\Theta}\right]_{kk^{\prime}}^{ij}=\left[\bm{\hat{\Phi}}\right]_{k}^ {i}\left[\bm{\hat{\Phi}}\right]_{k^{\prime}}^{j}=\left[\tilde{\bm{A}}_{k}\bm{ \Sigma}_{s}\bm{\theta}\right]_{i}\left[\tilde{\bm{A}}_{k^{\prime}}\bm{\Sigma}_ {s}\bm{\theta}\right]_{j}\] (138)

Recalling \(\bm{\omega}:=\bm{Q}^{1/2}\bm{\xi}\), we can now obtain an explicit form for the proximal \(\bm{h}\):

\[\bm{h}:=\operatorname*{argmin}_{\bm{u}}\left[\frac{(\bm{u}-\bm{\omega})\bm{V}^ {-1}(\bm{u}-\bm{\omega})}{2}+\hat{\ell}(y,\bm{u})\right]=\bm{V}(\bm{I}+\bm{V}) ^{-1}(\bm{V}^{-1}\bm{Q}^{1/2}\bm{\xi}+y\bm{1})\] (139)

The proximal \(\bm{G}\) should not arise in this special case.

Next, we will simplify the saddle-point equations. Simplifying where possible, we may write:

\[\bm{f} =\bm{V}^{-1}(\bm{h}-\bm{w})=(\bm{I}+\bm{V})^{-1}\left(y\bm{1}- \bm{\omega}\right)\] (140) \[\partial_{\bm{\omega}}\bm{f} =-(\bm{I}+\bm{V})^{-1}\] (141) \[\rho =\mathbb{E}_{\bm{x}}\left[\left(\frac{1}{\sqrt{d}}\bm{\theta}^{ \top}\bm{x}\right)^{2}\right]=\frac{1}{d}\bm{\theta}^{\top}\bm{\Sigma}_{s}\bm{\theta}\] (142) \[\omega_{0} \equiv\bm{m}^{\top}\bm{Q}^{-1/2}\bm{\xi}\] (143) \[\sigma_{0} =\rho-\bm{m}^{\top}\bm{Q}^{-1}\bm{m}=\frac{1}{d}\bm{\theta}^{ \top}\bm{\Sigma}_{s}\bm{\theta}-\bm{m}^{\top}\bm{Q}^{-1}\bm{m}\] (144)

\[\hat{\bm{V}}=-\alpha\int dy\mathbb{E}_{\bm{\xi}}\left[Z^{0}(y, \omega_{0},\sigma_{0})(-(\bm{I}+\bm{V})^{-1})\right]=\alpha(\bm{I}+\bm{V})^{- 1}\mathbb{E}_{\bm{\xi}}\left[\underbrace{\int dyZ^{0}(y,\omega_{0},\sigma_{0 })}_{1}\right]\] (145) \[\Rightarrow\hat{\bm{V}}=\alpha(\bm{I}+\bm{V})^{-1}\] (146)

We can simplify the prior equation for \(\hat{\bm{V}}\) to the following set of equations:

\[\bm{V}_{kk}=\frac{1}{p}\operatorname{tr}\left[\tilde{\bm{\Sigma}} _{kk}\left[\nu_{kk}\lambda_{k}\bm{I}_{p}+\hat{\bm{V}}_{kk}\tilde{\bm{\Sigma}}_ {kk}\right]^{-1}\right] k=1,\ldots,K\] (147) \[\bm{V}_{kk^{\prime}}=\frac{1}{p}\operatorname{tr}\left[\tilde{\bm {\Sigma}}_{kk^{\prime}}\left[\hat{\bm{V}}_{k^{\prime}k}\tilde{\bm{\Sigma}}_{k^{ \prime}k}\right]^{-1}\right] k^{\prime}\neq k\] (148)Equations 146 and 148 are solved by setting \(\hat{\bm{V}}_{kk^{\prime}}=\bm{V}_{kk^{\prime}}=0\) for all \(k\neq k^{\prime}\) so that \(\hat{\bm{V}}_{kk^{\prime}}=\hat{V}_{k}\delta_{kk^{\prime}}\) and \(\bm{V}_{kk^{\prime}}=V_{k}\delta_{kk^{\prime}}\). The diagonal components then satisfy

\[\hat{V}_{k} =\frac{\alpha}{1+V_{k}}\] (149) \[V_{k} =\frac{1}{p}\operatorname{tr}\left[\bm{\bar{\Sigma}}_{kk}\left[ \nu_{kk}\lambda_{k}\bm{I}_{p}+\hat{V}_{k}\bm{\bar{\Sigma}}_{kk}\right]^{-1}\right]\] (150)

Which may be solved separately for each \(k=1,\ldots,K\). Simplifying the remaining channel equations we have:

\[\hat{\bm{Q}} =\alpha(\bm{I}+\bm{V})^{-1}\left[\left(\zeta^{2}+\rho\right)\bm{1 }\bm{1}^{\top}-\bm{1}\bm{m}^{\top}-\bm{m}\bm{1}^{\top}+\bm{Q}\right](\bm{I}+ \bm{V})^{-1}\] (151) \[\Rightarrow\hat{Q}_{kk^{\prime}} =\frac{1}{\alpha}\hat{V}_{k}\hat{V}_{k^{\prime}}\left[\zeta^{2}+ \rho-m_{k}-m_{k^{\prime}}+Q_{kk^{\prime}}\right]\] (152) \[\hat{\bm{m}} =\alpha(\bm{I}+\bm{V})^{-1}\bm{1}\quad\Rightarrow\quad\hat{m}_{ k}=\frac{\alpha}{(1+V_{k})}=\hat{V}_{k}\] (153)

Simplifying the prior equations we obtain (through some tedious but straightforward algebra):

\[Q_{kk^{\prime}} =\hat{Q}_{kk^{\prime}}J_{kk^{\prime}}+\hat{V}_{k}\hat{V}_{k^{ \prime}}\Lambda_{kk^{\prime}}\] (154) \[m_{k} =\hat{V}_{k}R_{k}\] (155)

Where we have defined:

\[\bm{G}_{k} \equiv\nu_{kk}\lambda_{k}\bm{I}_{p}+\hat{V}_{k}\bm{\Sigma}_{kk}\] (156) \[J_{kk^{\prime}} \equiv\frac{1}{p}\operatorname{tr}\left[\bm{\bar{\Sigma}}_{kk^{ \prime}}\bm{G}_{k^{\prime}}^{-1}\bm{\bar{\Sigma}}_{k^{\prime}k}\bm{G}_{k}^{-1}\right]\] (157) \[\Lambda_{kk^{\prime}} \equiv\frac{1}{p}\bm{\theta}^{\top}\bm{\Sigma}_{s}\bm{\bar{A}}_{ k}^{\top}\bm{G}_{k}^{-1}\bm{\bar{\Sigma}}_{kk^{\prime}}\bm{G}_{k^{\prime}}^{-1} \bm{\bar{A}}_{k^{\prime}}\bm{\Sigma}_{s}\bm{\theta}\] (158) \[R_{k} \equiv\bm{\theta}^{\top}\bm{\Sigma}_{s}\bm{\bar{A}}_{k}^{\top}\bm {G}_{k}^{-1}\bm{\bar{A}}_{k}\bm{\Sigma}_{s}\bm{\theta}\] (159)

Solving eq's 154,152 for \(\bm{Q}\), we obtain:

\[Q_{kk^{\prime}}=\frac{\gamma_{kk^{\prime}}}{1-\gamma_{kk^{\prime}}}\left( \zeta^{2}+\rho-m_{k}-m_{k^{\prime}}\right)+\frac{\hat{V}_{k}\hat{V}_{k^{\prime }}}{1-\gamma_{kk^{\prime}}}\Lambda_{kk^{\prime}}\qquad(k\neq k^{\prime})\] (160)

Combining these results, we arrive at a formula for \(E_{kk^{\prime}}\):

\[E_{kk^{\prime}} =\frac{1}{1-\gamma_{kk^{\prime}}}\left(\gamma_{kk^{\prime}}\zeta^ {2}+\rho-\hat{V}_{k}R_{k}-\hat{V}_{k^{\prime}}R_{k^{\prime}}+\hat{V}_{k}\hat{V} _{k^{\prime}}\Lambda_{kk^{\prime}}\right)+\frac{1}{K^{2}}\sum_{k}\eta_{k}^{2} \qquad(k\neq k^{\prime})\] (161)

where \(\gamma_{kk^{\prime}}\equiv\frac{1}{\alpha}\hat{V}_{k}\hat{V}_{k^{\prime}}J_{kk ^{\prime}}\) (162)

Where the order parameters \(\hat{V}_{k}\), \(k=1,\ldots,K\) satisfy the fixed-point equations given by eq's 149, 150.

#### g.2.2 Diagonal Terms

To calculate \(E_{rr}\), we cannot ignore the presence of readout noise in the loss function. However, as the loss function is separable over the readouts \(k\), we may calculate \(E_{rr}\) using the results of [31] in the special case where \(K=1\), incorporating the readout noise into the label noise. Concretely, we may calculate \(E_{rr}\) as the error of a single linear predictor under the same setup as the off-diagonal terms except that in the training set \(y^{\mu}\sim P_{y}^{0}(y|\frac{1}{\sqrt{\rho}}\bm{\theta}^{\top}\bm{x}^{\mu})\) where \(P_{y}^{0}(y|x)=\mathcal{N}(x,\zeta^{2}+\eta_{k}^{2})\). This may be recovered from the calculation for the off-diagonal terms by setting \(k=k^{\prime}\) and re-scaling \(\zeta^{2}\to\zeta^{2}+\eta_{k}^{2}\), giving:

\[E_{kk}=\frac{1}{1-\gamma_{kk}}\left(\gamma_{kk}(\zeta^{2}+\eta_{r}^{2})+\rho- 2\hat{V}_{k}R_{k}+\hat{V}_{k}^{2}\Lambda_{kk}\right)+\frac{1}{K^{2}}\sum_{k} \eta_{k}^{2}\hskip 28.452756pt(k\neq k^{\prime})\] (163)

where the definitions of \(\gamma_{kk}\), \(\rho\), \(R_{k}\), and \(\Lambda_{kk}\) can be inherited from the off-diagonal case, as well as the saddle-point equations 149, 150.

#### g.2.3 Full Error

The results obtained here are equivalent to the results of our main theorem, up to a reshuffling of additive constants \(\eta_{k}^{2}\) among the error terms, and a trivial re-scaling of the order parameters as follows: \(V_{k}\to\frac{1}{\lambda_{k}}V_{k}\), \(\hat{V}_{k}\to\lambda_{k}\hat{V}_{k}\)

## Appendix H Equicorrelated Data Model

To gain an intuition for the joint effects of correlated data, subsampling, ensembling, feature noise, and readout noise, we simplify the formulas for the generalization error in the following special case:

\[\bm{\Sigma}_{s} =s\left[(1-c)\bm{I}_{M}+c\bm{1}_{M}\bm{1}_{M}^{\top}\right]\] (164) \[\bm{\Sigma}_{0} =\omega^{2}\bm{I}_{M}\] (165)

Here \(s\) is a parameter which sets the overall scale of the data and \(c\in[0,1]\) tunes the correlation structure in the data and \(\omega^{2}\) sets the scale of an isotropic feature noise. We consider an ensemble of \(k\) readouts, each of which sees a subset of the features. Due to the isotropic nature of the equicorrelated data model and the pairwise decomposition of the generalization error, we expect that the generalization error will depend on the partition of features among the readout neurons through only:

* The number of features sampled by each readout: \(N_{r}\equiv\nu_{rr}M\), for \(r=1,\dots,k\)
* The number of features jointly sampled by each pair of readouts \(n_{rr^{\prime}}\equiv\nu_{rr^{\prime}}M\) for \(r,r^{\prime}\in\{1,\dots,k\}\)

Here, we have introduced the subsampling fractions \(\nu_{rr}=\frac{N_{r}}{M}\) and the overlap fractions \(\nu_{rr^{\prime}}=\frac{n_{rr^{\prime}}}{M}\)

We will average the generalization error over readout weights drawn randomly from the space perpendicular to \(\bm{1}_{M}\), with an added spike along the direction of \(\bm{1}_{M}\):

\[\bm{w}^{*} =\sqrt{1-\rho^{2}}\mathbb{P}_{\perp}\bm{w}_{0}^{*}+\rho\bm{1}_{M}\] (166) \[\bm{w}_{0}^{*} \sim\mathcal{N}(0,\bm{I}_{M})\] (167)

The projection matrix may be written \(\mathbb{P}_{\perp}=\bm{I}_{M}-\frac{1}{N}\bm{1}_{M}\bm{1}_{M}^{\top}\). The two components of the ground truth weights will yield independent contributions to the generalization error in the sense that

\[\langle E_{rr^{\prime}}\rangle=(1-\rho^{2})E_{rr^{\prime}}(\rho=0)+\rho^{2}E_ {rr^{\prime}}(\rho=1)\] (168)

Calculating \(E_{rr}\) and \(E_{rr^{\prime}}\) is an exercise in linear algebra which is straightforward but tedious. To assist with the tedious algebra, we wrote a Mathematica package which can handle multiplication, addition, and inversion of matrices of symbolic dimension of the specific form encountered in this problem. This form consists of block matrices, where the blocks may be written as \(a\delta_{MN}\bm{I}_{M}+b\bm{1}_{M}\bm{1}_{N}^{\top}\), where \(a,b\) are scalars and \(\delta_{MN}\) ensures that there is only a diagonal component for square blocks (when \(M=N\)). This package is included as supplemental material to this publication.

### Diagonal Terms and Saddle-Point Equations

Here, we solve for the dominant values of \(q_{r}\) and \(\hat{q}_{r}\) and simplify the expressions for \(E_{rr}\) in the case of equicorrelated features described above. In this isotropic setting, \(E_{rr},q_{r},\hat{q}_{r}\) will depend on the subsampling only through \(N_{r}=\nu_{rr}M\). We may then write, without loss of generality \(\bm{A}_{r}=(\bm{I}_{N_{r}}\quad\bm{0})\in\mathbb{R}^{N_{r}\times M}\) where \(\bm{0}\) denotes a matrix of all zeros, of the appropriate dimensionality.

We start by simplifying the saddle-point equations 53,54. Expanding \(\frac{1}{M}\operatorname{tr}\left(\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}_{rr}\right)\) and keeping only leading order terms, the saddle-point equations for \(q_{r}\) and \(\hat{q}_{r}\) reduce to:

\[q_{r} =\frac{\nu_{rr}\left(s(1-c)+\omega^{2}\right)}{\hat{q}_{r}(s(1-c) +\omega^{2})+\nu_{r}}\] (169) \[\hat{q}_{r} =\frac{\alpha}{\lambda+q_{r}}\] (170)

Defining \(a\equiv s(1-c)+\omega^{2}\) and solving this system of equations, we find:

\[q_{r} =\frac{\sqrt{a^{2}\alpha^{2}+2a\alpha(\lambda-a)\nu_{r}+(a+ \lambda)^{2}\nu_{r}^{2}}-a\alpha+(a-\lambda)\nu_{r}}{2\nu_{r}}\] (171) \[\hat{q}_{r} =\frac{\sqrt{a^{2}\alpha^{2}+2a\alpha(\lambda-a)\nu_{r}+(a+ \lambda)^{2}\nu_{r}^{2}}+a\alpha-(a+\lambda)\nu_{r}}{2a\lambda}\] (172)

We have selected the solution with \(q_{r}>0\) because self-overlaps must be at least as large as overlaps between different replicas. This solution to the saddle-point equatios can be applied to each of the \(k\) readouts.

Next, we calculate \(E_{rr}\). Expanding \(\gamma_{rr}\equiv\frac{\alpha}{M\kappa^{2}}\operatorname{tr}\left[(\bm{G}^{-1 }\tilde{\bm{\Sigma}})^{2}\right]\) to leading order in \(M\), we find:

\[\gamma_{rr}=\frac{a^{2}\alpha\nu_{r}}{\left(\lambda+q_{r}\right)^{2}\left(a \hat{q}_{r}+\nu_{r}\right)^{2}}\] (173)

\[\langle E_{rr}\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=0)= \frac{1}{1-\gamma_{rr}}\frac{1}{M}\operatorname{tr}\left[\mathbb{ P}_{\perp}\left(\bm{\Sigma}_{s}-\frac{2}{\nu_{rr}}\hat{q}_{r}\bm{\Sigma}_{s}\bm{A}_{r} ^{\top}\bm{G}_{r}^{-1}\bm{A}_{r}\bm{\Sigma}_{s}+\frac{1}{\nu_{rr}}\hat{q}_{r}^ {2}\bm{\Sigma}_{s}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}\bm{G}_{ r}^{-1}\bm{A}_{r}\bm{\Sigma}_{s}\right)\mathbb{P}_{\perp}\right]\] \[+\frac{\gamma_{rr}}{1-\gamma_{rr}}\zeta^{2}+\eta_{r}^{2},\] (174) \[\langle E_{rr}\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=1)= \frac{1}{1-\gamma_{rr}}\frac{1}{M}\mathbf{1}_{M}^{\top}\left[\bm{ \Sigma}_{s}-\frac{2}{\nu_{rr}}\hat{q}_{r}\bm{\Sigma}_{s}\bm{A}_{r}^{\top}\bm{G }_{r}^{-1}\bm{A}_{r}\bm{\Sigma}_{s}+\frac{1}{\nu_{rr}}\hat{q}_{r}^{2}\bm{ \Sigma}_{s}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}\bm{G}_{r}^{-1} \bm{A}_{r}\bm{\Sigma}_{s}\right]\bm{1}_{m}\] \[+\frac{\gamma_{rr}}{1-\gamma_{rr}}\zeta^{2}+\eta_{r}^{2},\] (175)

With the aid of our custom Mathematica package, we calculate the traces and contractions in these expressions and expand them to leading order in \(M\), finding:

\[\langle E_{rr}\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=0)= \frac{1}{1-\gamma_{rr}}\left(s(1-c)\left(1-\frac{(1-c)s\hat{q}_{r} \nu_{r}\left(\hat{q}_{r}(s(1-c)+\omega^{2})+2\nu_{r}\right)}{(\hat{q}_{r}(s(1- c)+\omega^{2})+\nu_{r})^{\,2}}\right)\right)+\frac{\gamma_{rr}\zeta^{2}+\eta_{r}^{2}}{1- \gamma_{rr}}\] (176) \[\langle E_{rr}\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=1)= \frac{1}{1-\gamma_{rr}}\left(\frac{s(1-c)(1-\nu_{rr})+\omega^{2}}{ \nu_{rr}}\right)+\frac{\gamma_{rr}\zeta^{2}+\eta_{r}^{2}}{1-\gamma_{rr}}\] (177)

In the "ridgeless" limit where \(\lambda\to 0\), we obtain:\[\gamma_{rr}=\frac{4\alpha\nu_{rr}}{(\alpha+\nu_{rr}+|\alpha-\nu_{rr}|)^{2}}\] (178)

\[\langle E_{rr}(\rho=0)\rangle_{\mathcal{D},\bm{w}^{*}}=\left\{\begin{array}{ll} \frac{s(1-c)\nu_{rr}}{\nu_{rr}-\alpha}\left(1+\frac{s\alpha(1-c)(\alpha-2\nu_{ rr})}{\nu_{rr}[s(1-c)+\omega^{2}]}\right)+\frac{\alpha\zeta^{2}+\nu_{rr}\eta_{r}^{2}}{ \nu_{rr}-\alpha},&\text{if }\alpha<\nu_{rr}\\ \frac{s(1-c)\alpha}{\alpha-\nu_{rr}}\left(1-\frac{s(1-c)\nu_{rr}}{s(1-c)+ \omega^{2}}\right)+\frac{\nu_{rr}\zeta^{2}+\alpha\eta_{r}^{2}}{\alpha-\nu_{rr }},&\text{if }\alpha>\nu_{rr}\end{array}\right\}\quad(\lambda\to 0)\] (179)

\[\langle E_{rr}(\rho=1)\rangle_{\mathcal{D},\bm{w}^{*}}=\left\{\begin{array}{ ll}\frac{\nu_{rr}}{\nu_{rr}-\alpha}\left(\frac{s(1-c)(1-\nu_{rr})+\omega^{2}}{\nu_{rr}} \right)+\frac{\alpha\zeta^{2}+\nu_{rr}\eta_{r}^{2}}{\nu_{rr}-\alpha},&\text{ if }\alpha<\nu_{rr}\\ \frac{\alpha}{\alpha-\nu_{rr}}\left(\frac{s(1-c)(1-\nu_{rr})+\omega^{2}}{\nu _{rr}}\right)+\frac{\nu_{rr}\zeta^{2}+\alpha\eta_{r}^{2}}{\alpha-\nu_{rr}},& \text{if }\alpha>\nu_{rr}\end{array}\right\}\quad(\lambda\to 0)\] (180)

### Off-Diagonal Terms

In this section, we calculate the off-diagonal error terms \(E_{rr^{\prime}}\) for \(r\neq r^{\prime}\), again making use of our custom Mathematica package to simplify contractions of block matrices of the prescribed form. By the isotropic nature of the equicorrelated data model, \(E_{rr^{\prime}}\) can only depend on the subsampling scheme through \(\nu_{rr}\), \(\nu_{r^{\prime}r^{\prime}}\), and \(\nu_{rr^{\prime}}\). We can thus, without loss of generality, write:

\[\bm{A}_{r}=\begin{pmatrix}\bm{1}_{n_{r}\times n_{r}}&\bm{0}_{n_{r}\times n_{r ^{\prime}}}&\bm{0}_{n_{r}\times n_{s}}&\bm{0}_{n_{r}\times l}\\ \bm{0}_{n_{s}\times n_{r}}&\bm{0}_{n_{s}\times n_{r^{\prime}}}&\bm{I}_{n_{s} \times n_{s}}&\bm{0}_{n_{s}\times l}\end{pmatrix}\in\mathbb{R}^{N_{r}\times M}\] (181)

\[\bm{A}_{r^{\prime}}=\begin{pmatrix}\bm{0}_{n_{r^{\prime}}\times n_{r}}&\bm{I}_ {n_{r^{\prime}}\times n_{r^{\prime}}}&\bm{0}_{n_{r}\times n_{s}}&\bm{0}_{n_{r^ {\prime}}\times l}\\ \bm{0}_{n_{s}\times n_{r}}&\bm{0}_{n_{s}\times n_{r^{\prime}}}&\bm{I}_{n_{s} \times n_{s}}&\bm{0}_{n_{s}\times l}\end{pmatrix}\in\mathbb{R}^{N_{r^{\prime}} \times M}\] (182)

where we have defined \(n_{s}\) to be the number of features shared between the readouts, \(n_{r}=N_{r}-n_{s}\) and \(n_{r^{\prime}}=N_{r^{\prime}}-n_{s}\) and the count of remaining features \(l=M-n_{r}-n_{r^{\prime}}-n_{s}\).

Then, to leading order in \(M\), we find:

\[\gamma_{rr^{\prime}}=\frac{\alpha\nu_{rr^{\prime}}(s(1-c)+\omega^{2})^{2}}{ \left(\lambda+q_{r}\right)\left(\lambda+q_{r^{\prime}}\right)\left(\nu_{rr}+(s (1-c)+\omega^{2})\hat{q}_{r}\right)\left(\nu_{r^{\prime}r^{\prime}}+(s(1-c)+ \omega^{2})\hat{q}_{r^{\prime}}\right)}\] (183)

Averaging \(E_{rr^{\prime}}\) over \(\bm{w}_{0}^{*}\sim\mathcal{N}(0,\bm{I}_{M})\), we get:

\[\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=0)= \frac{\gamma_{rr^{\prime}}}{1-\gamma_{rr^{\prime}}}\zeta^{2}+\frac{ 1}{1-\gamma_{rr^{\prime}}}\left(\frac{1}{M}\operatorname{tr}\left[\mathbb{P}_{ \perp}\bm{\Sigma}_{s}\mathbb{P}_{\perp}\right]\right)\] (184) \[-\frac{1}{M(1-\gamma_{rr^{\prime}})}\operatorname{tr}\left[ \mathbb{P}_{\perp}\bm{\Sigma}_{s}\left(\frac{1}{\nu_{rr}}\hat{q}_{r}\bm{A}_{r}^ {\top}\bm{G}_{r}^{-1}\bm{A}_{r}+\frac{1}{\nu_{r^{\prime}r^{\prime}}}\hat{q}_{ r^{\prime}}\bm{A}_{r^{\prime}}^{\top}\bm{G}_{r^{\prime}}^{-1}\bm{A}_{r^{\prime}} \right)\bm{\Sigma}_{s}\mathbb{P}_{\perp}\right]\] \[+\frac{\hat{q}_{r}\hat{q}_{r^{\prime}}}{M(1-\gamma_{rr^{\prime}})} \frac{1}{\sqrt{\nu_{rr}\nu_{rr^{\prime}r^{\prime}}}}\operatorname{tr}\left[ \mathbb{P}_{\perp}\bm{\Sigma}_{s}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\tilde{\bm{ \Sigma}}_{rr^{\prime}}\bm{G}_{r^{\prime}}^{-1}\bm{A}_{r^{\prime}}\bm{\Sigma}_{ s}\mathbb{P}_{\perp}\right],\]

\[\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D},\bm{w}^{*}}(\rho=1)= \frac{\gamma_{rr^{\prime}}}{1-\gamma_{rr^{\prime}}}\zeta^{2}+ \frac{1}{M(1-\gamma_{rr^{\prime}})}\left(\bm{1}_{M}^{\top}\bm{\Sigma}_{s}\bm{1}_ {M}\right)\] (185) \[-\frac{1}{M(1-\gamma_{rr^{\prime}})}\bm{1}_{M}^{\top}\bm{\Sigma}_{s} \left(\frac{1}{\nu_{rr}}\hat{q}_{r}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\bm{A}_{r}+ \frac{1}{\nu_{r^{\prime}r^{\prime}}}\hat{q}_{r^{\prime}}\bm{A}_{r^{\prime}}^{ \top}\bm{G}_{r^{\prime}}^{-1}\bm{A}_{r^{\prime}}\right)\bm{\Sigma}_{s}\bm{1}_{M}^{\top}\] \[+\frac{\hat{q}_{r}\hat{q}_{r^{\prime}}}{M(1-\gamma_{rr^{\prime}})} \frac{1}{\sqrt{\nu_{rr}\nu_{r^{\prime}r^{\prime}}}}\bm{1}_{M}^{\top}\bm{ \Sigma}_{s}\bm{A}_{r}^{\top}\bm{G}_{r}^{-1}\tilde{\bm{\Sigma}}_{rr^{\prime}}\bm{G} _{r^{\prime}}^{-1}\bm{A}_{r^{\prime}}\bm{\Sigma}_{s}\bm{1}_{M}\]Calculating these contractions and traces and expanding to leading order in \(M\), we get:

\[\begin{split}\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D}, \boldsymbol{w}^{*}}(\rho=0)=\frac{s(1-c)}{1-\gamma_{rr^{\prime}}}& \left(1-\frac{s(1-c)\nu_{rr}\hat{q}_{r}}{\nu_{rr}+(s(1-c)+\omega^{2})\hat{q}_ {r}}-\frac{s(1-c)\nu_{r^{\prime}r^{\prime}}\hat{q}_{r^{\prime}}}{\nu_{r^{ \prime}r^{\prime}}+(s(1-c)+\omega^{2})\hat{q}_{r^{\prime}}}\right.\\ &\left.+\frac{s(1-c)(s(1-c)+\omega^{2})\nu_{rr^{\prime}}\hat{q}_{r }\hat{q}_{r^{\prime}}}{(\nu_{rr}+(s(1-c)+\omega^{2})\hat{q}_{r})(\nu_{r^{ \prime}r^{\prime}}+(s(1-c)+\omega^{2})\hat{q}_{r^{\prime}})}\right)\\ &+\frac{\gamma_{rr^{\prime}}}{1-\gamma_{rr^{\prime}}}\zeta^{2} \end{split}\] (186)

\[\begin{split}\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{ \mathcal{D},\boldsymbol{w}^{*}}(\rho=1)=\frac{1}{1-\gamma_{rr^{\prime}}}\left( \frac{s(1-c)(\nu_{rr^{\prime}}-\nu_{rr}\nu_{r^{\prime}r^{\prime}})+\omega^{2} \nu_{rr^{\prime}}}{\nu_{rr}\nu_{r^{\prime}r^{\prime}}}\right)+\frac{\gamma_{ rr^{\prime}}}{1-\gamma_{rr^{\prime}}}\zeta^{2}\end{split}\] (187)

Taking \(\lambda\to 0\) we get the ridgeless limit:

\[\gamma_{rr^{\prime}}\rightarrow\frac{4\alpha\nu_{rr^{\prime}}}{(\alpha+\nu_{ rr}+|\alpha-\nu_{rr}|)(\alpha+\nu_{rr^{\prime}}+|\alpha-\nu_{r^{\prime}r^{\prime}}|)} \qquad(\lambda\to 0)\] (188)

\[\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D},\boldsymbol{w}^{*}} (\rho=0)=\frac{1}{1-\gamma_{rr^{\prime}}}F_{0}(\alpha)+\frac{\gamma_{rr^{ \prime}}}{1-\gamma_{rr^{\prime}}}\zeta^{2}\quad(r\neq r^{\prime})\] (189)

where

\[F_{0}(\alpha)\equiv\left\{\begin{array}{ll}\frac{(c-1)s\left(\nu_{r}\nu_{r^ {\prime}}((2\alpha-1)(c-1)s+\omega^{2})-\alpha^{2}(c-1)s\nu_{rr^{\prime}})}{ \nu_{r}(\left(c-1\right)s-\omega^{2})\nu_{r^{\prime}}\right)},&\text{if }\alpha \leq\nu_{rr}\leq\nu_{r^{\prime}r^{\prime}}\\ \frac{(c-1)s\left(\nu_{r^{\prime}}(\left(c-1\right)s\nu_{r^{\prime}}+(\alpha-1 )(c-1)s\nu^{2})-\alpha(c-1)s\nu_{rr^{\prime}}\right)}{(c-1)s-\omega^{2})\nu_{ r^{\prime}}},&\text{if }\nu_{rr}\leq\alpha\leq\nu_{r^{\prime}r^{\prime}}\\ \frac{(c-1)s\left(\left(c-1\right)s\nu_{r^{\prime}}-s\nu_{rr^{\prime}}+(c-1)s \nu_{rr^{\prime}}-cs+s\nu_{rr^{\prime}}+s+\omega^{2}\right)}{(c-1)s-\omega^{2} },&\text{if }\nu_{rr}\leq\nu_{r^{\prime}r^{\prime}}\leq\alpha\end{array}\right\}\] (190)

\[\langle E_{rr^{\prime}}(\mathcal{D})\rangle_{\mathcal{D},\boldsymbol{w}^{*}} (\rho=1)=\frac{1}{1-\gamma_{rr^{\prime}}}\left(\frac{s(1-c)(\nu_{rr^{\prime}}- \nu_{rr}\nu_{r^{\prime}r^{\prime}})+\omega^{2}\nu_{rr^{\prime}}}{\nu_{rr}\nu_ {r^{\prime}r^{\prime}}}\right)+\frac{\gamma_{rr^{\prime}}}{1-\gamma_{rr^{ \prime}}}\zeta^{2}\qquad(\lambda\to 0)\] (191)

### Optimal Regularization

Here, we derive the "locally" optimal regularization which minimizes the prediction error of the ensemble members independently. Equivalently we derive the optimal regularization for a single readout (\(k=1\)) or the regularization which minimizes the diagonal terms \(E_{rr}\) of the generalization errror. By differentiating \(E_{rr}\) with respect to the regularization \(\lambda\), one can show that for the optimal regularization, we will have

\[\frac{1}{(1-\gamma_{rr})}\frac{dS_{r}}{d\lambda}\left[\frac{a^{2}\nu_{rr}}{ \alpha(1-\gamma_{rr})}S_{r}\left((1-\rho^{2})I_{rr}^{0}+\rho^{2}I_{rr}^{1}+ \zeta^{2}+\eta_{r}^{2}\right)+(1-\rho^{2})s^{2}(1-c^{2})\nu_{rr}(aS_{r}-1) \right]=0\] (192)

It is easy to show that \(\frac{dS_{c}}{d\lambda}\) cannot be equal to \(0\). Setting the term in brackets to zero and solving for \(\lambda\), we find with the aid of the computer algebra system Mathematica [54]:

\[\lambda^{\star}=a\left(\frac{a\left(\nu_{rr}(\zeta^{2}+\eta^{2})+a\rho^{2}+s(1 -c)\nu_{rr}(1-2\rho^{2})\right)}{(1-c)^{2}\nu_{rr}^{2}\left(1-\rho^{2}\right)s^{ 2}}-1\right),\qquad(0<c\leq 1)\] (193)

In the limiting case of isotropic data (\(c=0\)), the optimal regularization reduces to:\[\lambda^{\star}=\frac{1}{\nu_{rr}}(\zeta^{2}+\eta^{2})+s\left(\frac{1}{\nu_{rr}}-1 \right),\qquad(c=0)\] (194)

We note that while generalization error is discontinuous at \(c=0\), this result can be quickly obtained from eq. 193 by noticing that the formula for the generalization error with \(0<c\leq 1\) reduces to the formula for generalization error with \(c=0\) when we set \(\rho=c=0\) (when \(c=0\), the generalization error does not depend on \(\rho\) because there is no special direction in the data).

### Homogeneous Ensembling with Resource Constraints

We make further simplifications in the special case where \(\lambda_{r}=\lambda\), \(\eta_{r}=\eta\), \(\nu_{rr}=\frac{1}{k}\) for all \(r=1,\ldots k\). For simplicity, we also set \(\nu_{rr^{\prime}}=0\) for all \(r\neq r^{\prime}\) so that ensemble members sample mutually exclusive subsets of the features. We will consider both the ridgeless limit \(\lambda\to 0\) and the case of "locally optimal" regularization \(\lambda=\lambda^{\star}\) (see eq. 193). Concretely, we show here that under these special cases, generalization error has the forms given in eqs. 25, 26.

We start by analyzing the saddle-point equations 111, 112. Note that in this special case the saddle point equations will be identical for all \(r=1,\ldots,k\). We will therefore suppress the \(r\) index. Solving this quadratic system of equations explicitly, we encounter the following radical, which we assign variable name \(x\):

\[x\equiv\sqrt{(a\alpha-a\nu+\lambda\nu)^{2}+4a\lambda\nu^{2}}\] (195)

In order to simplify this radical to begin extracting the factor of \(s(1-c)\) that appears in eqs. 25, 26, we define a reduced regularization:

\[\Lambda\equiv\frac{\lambda}{s(1-c)}\] (196)

And substitute \(\Lambda\), \(H\), \(W\), and \(Z\) (recall eq. 24) for \(\lambda\), \(\eta\), \(\omega\), and \(\zeta\), giving

\[x =s(1-c)\mathcal{X}(\alpha,\nu,W,\Lambda)\] (197) \[\text{where }\mathcal{X}(\alpha,\nu,W,\Lambda) \equiv\sqrt{(\alpha(W+1)+\nu(\Lambda-W-1))^{2}+4\Lambda\nu^{2}(W+1)}\] (198)

making the same substitutions, we can then write

\[\hat{q} =\left[s(1-c)\right]^{-1}\mathcal{Q}\] (199) \[\text{where }\quad\mathcal{Q} \equiv\frac{2\alpha\nu}{(-\alpha(W+1)+\nu(\Lambda+W+1)+X)}\] (200)

Continuing to make substitutions in this manner, we arrive at:

\[S =[s(1-c)]^{-1}\mathcal{S}\] (201) \[\text{where }\quad\mathcal{S} \equiv\frac{\mathcal{Q}}{\nu+\mathcal{Q}(1+W)}\] (202)

Finally, we may express the generalization error in a reasonably compact form. In the special case at hand, the generalization error is written:

\[E_{k}=\frac{1}{k}E_{rr}(\nu_{rr}=\frac{1}{k})+\frac{k-1}{k}E_{rr^{\prime}}(\nu _{rr^{\prime}}=\frac{1}{k}\delta_{rr^{\prime}})\] (203)

Substituting 201 for S in eq. 16, and making further substitutions as necessary, we arrive at:

\[E_{rr}(\nu_{rr}=\frac{1}{k})=s(1-c)\mathcal{E}_{rr}(k,\alpha,\rho,\Lambda,H,W,Z)\] (204)where

\[\mathcal{E}_{rr} =\frac{N_{1}+N_{2}}{D}\] (205) \[N_{1} =\alpha(H+1)k+\mathcal{S}(\mathcal{S}(W+1)(\alpha+WZ+Z)-2\alpha)\] (206) \[N_{2} =\alpha\rho^{2}(k-\mathcal{S})(W(k+\mathcal{S})+k+\mathcal{S}-2)\] (207) \[D =\alpha k-\mathcal{S}^{2}(W+1)^{2}\] (208)

We also obtain

\[E_{rr^{\prime}}(\nu_{rr^{\prime}}=\frac{1}{k}\delta_{rr^{\prime}})=s(1-c) \mathcal{E}_{rr^{\prime}}(k,\alpha,\rho,\Lambda,H,W,Z)\] (209)

where

\[\mathcal{E}_{rr^{\prime}}=\frac{2\left(\rho^{2}-1\right)\mathcal{S}}{k}-2\rho ^{2}+1\] (210)

Combining these, we obtain the error of the ensemble as:

\[E_{k}=s(1-c)\mathcal{E}(k,\alpha,\rho,\Lambda,H,W,Z)\] (211)

where:

\[\mathcal{E}=\frac{1}{k}\mathcal{E}_{rr}+\frac{k-1}{k}\mathcal{E}_{rr^{\prime}}\] (212)

These result are derived in the Mathematica notebook titled "EquiCorrParameterReduction.nb" included with the available code. It follows from eq. 211 that when \(\lambda=0\) error can be written in the form of eq. 22. It follows from equations 211, 209 that at "locally optimal" regularization \(\lambda^{*}\), error can be written in the form of eq. 23. To see this, note that the reduced regularization \(\Lambda^{*}\) which minimizes \(E_{rr}\) will only depend on the other arguments of \(\mathcal{E}_{rr}\). Full expresions for the generalization error in the case \(\lambda\to 0\) and \(\lambda=\lambda^{*}\) can be found in the mathematica notebooks "EquiCorPhaseDiagram_ZeroReg.nb" and "EquiCorrPhaseDiagram_LocalReg.nb" included with the available code. These equations are long and difficult to interpreter - nor are they directly used in our code - and so are omitted here.

#### h.4.1 The Intermediate to Noise-Dominated Transition

The transition between the intermediate regime where \(1<k^{*}<\infty\) and the noise-dominated regime where \(k^{*}=\infty\) can be studied analytically relatively painlessly in the ridgeless limit \(\lambda\to 0\). The strategy we employ to determine this phase boundary is to examine the large-\(k\) asymptotic expansion of \(E_{k}\) to determine whether the error approaches its asymptotic value from below or above. If \(E_{k}\) approaches \(E_{\infty}\) from below, then \(k^{*}\) must be finite. If \(E_{k}\) approaches \(E_{\infty}\) from above, then \(k^{*}\) may be infinite - however, there is still the possibility of \(k^{*}<\infty\) if \(E_{k}\) is non-monotonic in \(k\). In practice, we check the values of \(E_{k}\) for \(k=1,2,\ldots,100\) and \(k=\infty\).

Setting \(\Lambda=0\) and expanding \(E_{k}\) around \(k=\infty\), we find:

\[\frac{E_{k}}{s(1-c)}=1-\rho^{2}+\rho^{2}W+\frac{\left((1+W)^{2}\rho^{2}+ \alpha(-2+H+HW+2\rho^{2})\right)}{(1+W)\alpha k}+\mathcal{O}\left(\frac{1}{k^ {2}}\right)\] (213)

Setting the coefficient of \(k^{-1}\) to zero, we find the phase boundary as:

\[\alpha=\frac{(1+W)^{2}\rho^{2}}{2(1-\rho^{2})-H(1+W)}\] (214)

This equation explains the shapes of the boundaries between the intermediate and noise-dominated regions in the phase diagrams of fig. 4 with \(\lambda=0\) (see black dotted lines in panels b, c, d).

An analytical formula for the boundary between the intermediate and noise-dominated regimes at locally optimal regularization \(\lambda=\lambda^{*}\) cannot be easily obtained. To understand why, we can asses the large-\(k\) asymptotic expansion of the generalization error at locally optimal regularization:

\[E_{k}(\lambda=\lambda^{*})=s(1-c)\left[1-\rho^{2}+\rho^{2}W+\frac{H}{k}\right]+ \mathcal{O}\left(\frac{1}{k^{2}}\right)\] (215)

This shows that at large \(k\), when \(H>0\), error always approaches its asymptotic value from above (recall that when \(H=0\) we always have \(k^{*}=1\), so that there is no phase boundary unless \(H>0\)). Thus, determining the phase boundary requires determining the value of \(k\) which minimizes \(E_{k}\), which is not analytically tractable.

### Infinite Data Limit

In this section we consider the behavior of generalization error in the equicorrelated data model as \(\alpha\rightarrow\infty\) while keeping the \(\lambda\sim\mathcal{O}(1)\). For simplicity, we assume \(\nu_{rr^{\prime}}=0\) for \(r\neq r^{\prime}\), isotropic features (\(c=0\)), no feature noise (\(\omega=0\)) and uniform readout noise \(\eta_{r}=\eta\) as in main text Fig. 3. This limit corresponds to data-rich learning, where the number of training examples is large relative to the number of model parameters. In this case, the saddle point equations reduce to:

\[\hat{q}_{r} \rightarrow\frac{\alpha}{\lambda}\] (216) \[q_{r} \rightarrow\frac{\nu_{rr^{\prime}}\lambda}{\alpha}\] (217)

In this limit, we find that \(\gamma_{rr^{\prime}}\to 0\). Using this, we can simplify the generalization error as follows:

\[E_{g}=\frac{1}{k^{2}}\sum_{rr^{\prime}=1}^{k}E_{rr^{\prime}}=s\left[1-\left(2- \frac{1}{k}\right)\left(\frac{1}{k}\sum_{r=1}^{k}\nu_{rr}\right)\right]+\frac {\eta^{2}}{k}\] (218)

Interestingly, we find that the readout error in this case depends on the subsampling fractions \(\nu_{rr}\) only through their mean. Therefore, with infinite data, there will be no distinction between homogeneous and heterogeneous subsampling.

## Appendix I Theoretical Learning Curves and Optimal Subsampling Phase Diagrams

Here, we provide additional learning curves and phase diagrams of \(k^{*}\) such as those in Fig. 4a,b,c,d, exploring more parameter values for the task-model alignment \(\rho\) and the Reduced noises \(H\), \(W\), and \(Z\). Generalization errors are calculated for a homogeneous ensemble of \(k\) linear regressors, as described in sections 5, H.4. We also show diagrams of generalization error \(E_{k^{*}}\).

Figure S6: (a) Optimal ensemble size \(k^{*}\) (eqs. 25, 26) in the parameter space of sample size \(\alpha\) and reduced readout noise scale \(H\) setting \(W=Z=0\). Grey indicates \(k^{*}=1\) and white indicates \(k^{*}\to\infty\), with intermediate values given by the colorbar. Appended vertical bars show \(\alpha\to\infty\). \(\rho\) and \(\lambda\) indicated in panel titles. \(\lambda=\lambda^{*}\) denotes the 'locally optimal" regularization (see section H.3) (b) Optimal generalization error \(E_{k^{*}}\) for the same parameter values in (a).

Figure S7: (a) Optimal ensemble size \(k^{*}\) (eqs. 25, 26) in the parameter space of sample size \(\alpha\) and reduced feature noise scale \(W\) setting \(H=Z=0\). Grey indicates \(k^{*}=1\) and white indicates \(k^{*}\to\infty\), with intermediate values given by the colorbar. Appended vertical bars show \(\alpha\to\infty\). \(\rho\) and \(\lambda\) indicated in panel titles. \(\lambda=\lambda^{*}\) denotes the ‘locally optimal” regularization (see section H.3) (b) Optimal generalization error \(E_{k^{*}}\) for the same parameter values in (a).

[MISSING_PAGE_EMPTY:47]

Figure S9: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=0\) and \(W=Z=0\) (given by eq. 22) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(H\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\rightarrow\infty\), and intermediate \(k\) values by the colorbar.

Figure S10: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=\lambda^{*}\) and \(W=Z=0\) (given by eq. 23) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(H\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\rightarrow\infty\), and intermediate \(k\) values by the colorbar.

Figure S11: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=0\) and \(H=Z=0\) (given by eq. 22) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(W\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\rightarrow\infty\), and intermediate \(k\) values by the colorbar.

Figure S12: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=\lambda^{*}\) and \(H=Z=0\) (given by eq. 23) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(W\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\to\infty\), and intermediate \(k\) values by the colorbar.

Figure S13: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=0\) and \(H=W=0\) (given by eq. 22) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(Z\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\rightarrow\infty\), and intermediate \(k\) values by the colorbar

Figure S14: Reduced generalization errors \(E_{g}/s(1-c)\) with \(\lambda=\lambda^{*}\) and \(H=W=0\) (given by eq. 23) for linear ridge ensembles of varying size \(k\). \(\rho\) and \(Z\) values indicated above plots. Grey lines indicate \(k=1\), dashed black lines \(k\rightarrow\infty\), and intermediate \(k\) values by the colorbar.