# On Learning Latent Models with Multi-Instance Weak Supervision

 Kaifu Wang

University of Pennsylvania

kaifu@sas.upenn.edu

&Efthymia Tsamoura

Samsung AI

efi.tsamoura@samsung.com

&Dan Roth

University of Pennsylvania

danroth@seas.upenn.edu

###### Abstract

We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function \(\sigma\) of labels associated with multiple input instances. We formulate this problem as _multi-instance Partial Label Learning (multi-instance PLL)_. Our problem is an extension to the standard PLL problem and is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition \(\sigma\). Our main contributions are as follows. First, we propose a necessary and sufficient condition for the learnability of the problem. This condition nontrivially generalizes and relaxes the existing _small ambiguity degree_ in PLL literature since we allow the transition to be deterministic. Second, we derive Rademacher-style error bounds based on a top-\(k\) surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning under unknown transitions. The empirical results align with our theoretical findings, exposing also the issue of scalability in the weak supervision literature.

## 1 Introduction

Consider the scenario in Figure 1, where a learner aims to learn one or more classifiers \(f_{1},\ldots,f_{n}\), each of which maps an instance \(x\) to its corresponding label \(y\). The learner is given training examples, each of which consists of a _vector_ of \(M>1\) instances \(\bm{x}=(x_{1},\ldots,x_{M})\), and each \(x_{i}\) is processed by one of the \(f_{j}\)'s. Differently from supervised learning, the gold labels \(\bm{y}=(y_{1},\ldots,y_{M})\) of \(\bm{x}\) are _hidden_; instead, the learner is provided with a _weak_ label \(s\) which is produced by applying a _transition function_1\(\sigma\) to the gold labels \(\bm{y}\). The transition \(\sigma\) itself may be unknown to the learner.

Footnote 1: We use the term transition function (or transition in short) instead of simply function, due to the relationship between our setting and learning with partial labels literature [8, 9].

The above weak supervision setting has been a topic of active research in NLP [42, 35, 34, 32, 45, 52, 22]. Recently, it has received renewed attention as it is met in _neuro-symbolic_ frameworks [14], i.e., frameworks that integrate inference and learning over neural models with inference and learning over symbolic models, such as logical theories [30, 53, 13, 59, 43, 31, 24, 27]. The benefits of this setting over architectures that approximate the \(f_{i}\)'s and \(\sigma\) via an end-to-end neural model [48] are (i) the ability to reuse the latent models, something particularly useful in NLP [34, 32], (ii) higher accuracy in multiple tasks, e.g., NLP [55] and visual question answering [24], (iii) greater interpretability and (iv) the ability to encode prior knowledge via \(\sigma\). Below, we present an example of our learning setting in neuro-symbolic learning.

**Example 1** (Sum2).: _We aim to learn an MNIST classifier \(f\). Differently from supervised learning, we use training examples of the form \((x_{1},x_{2},s)\), where \(x_{1}\) and \(x_{2}\) are MNIST digits and \(s\) istheir sum. The above implies a transition function \(\sigma:\{0,\ldots,9\}\times\{0,\ldots,9\}\to\{0,\ldots,18\}\), s.t. \(\sigma(y,y^{\prime})=y+y^{\prime}\). The target sum \(s\) restricts the space of labels that can be assigned to the input pair of digits \((x_{1},x_{2})\), e.g., if \(s=2\), then the only combinations of labels that abide by the constraint that the sum is 2 are (2,0), (1,1) and (0,2). This problem is referred to as SUM2 in literature [24; 30]._

The _transition function_\(\sigma\) is not necessarily one-to-one. For instance, multiple combinations of digits can lead to the same target sum. The above leaves us with two questions to answer: _can we provide any learning guarantees for the classifiers \(f_{1},\ldots,f_{n}\)?_ and _what if \(\sigma\) is unknown?_ Despite that multiple techniques for neuro-symbolic and latent structural learning have been recently proposed, those theoretical questions remain open. Practically, for the single input case (\(M=1\)), our setting can reduce to that of _partial label learning_ (PLL), where each input instance is accompanied by a set of labels, including the gold one [25; 11; 5; 29; 39; 54; 57; 60]. However, a key PLL learnability assumption - that the probability a wrong label co-occurs with the gold one is always less than one [28]- is violated in our setting, rendering existing learnability results inapplicable. This is because \(\sigma\) is a deterministic but _not_ necessarily bijective function, as opposed to \(\sigma\) in PLL which is randomized.

Due to its relevance to PLL, we refer to our learning setting as _multi-instance PLL_. Differently from prior art, e.g., [42; 35], we do not restrict \(\sigma\) to specific types of functions, e.g., linear functions. Hence, we can express constraints in several formal languages, including systems of Boolean equations and Datalog [1], a language widely used in neuro-symbolic techniques [24]. Notice that many neuro-symbolic learning techniques are oblivious to the implementation of the symbolic component: abstracting the symbolic component as a transition \(\sigma\) has been actually proposed as the means to compositionally integrate neural with symbolic models [43]. Furthermore, although we primarily consider deterministic transitions motivated by the neuro-symbolic and NLP literature, our results can be extended to support randomized transitions when \(\sigma\) is known.

We aim to make _minimal_ assumptions on the data distributions by showing learnability even under the "toughest" distributions. In addition, we provide learning guarantees under the semantic loss [56], a widely-used surrogate loss for training classifiers subject to logical theories [30; 43; 24]. To our knowledge, we are the first to provide this theoretical analysis, closing a gap in the neuro-symbolic and latent structural learning literature.

**Contributions.** Our contributions can be summarized into the following:

* We propose necessary and sufficient learnability conditions of the multi-instance PLL problem assuming all the instances are classified by a single classifier \(f\) and the transition is known. We further provide a Rademacher-style error bound using a top-\(k\) approximation to the _semantic loss_[56], see Section 3. Our analysis confirms the intuition that a larger \(k\) decreases the empirical risk, but increases the number of samples required for learning.
* We extend the above results for the more general case where one learns multiple classifiers \(\bm{f}=(f_{1},\ldots,f_{n})\) that classify instances from different domains, see Section 4.
* We prove learnability with a single classifier and an unknown transition function, see Section 5, and assess the validity of our theoretical results using SOTA neuro-symbolic frameworks, see Section 6.

Proofs, additional backgrounds and details on our empirical analysis are in the appendix.

## 2 Preliminaries

We use \(\mathbb{P}(\cdot)\) to denote probability mass, \(\mathbb{E}[\cdot]\) to denote expectation and \(\mathbbm{1}\{\cdot\}\) to denote an indicator function. For a positive integer \(M\), we use \([M]\) as a short for \(\{1,\ldots,M\}\). For a vector of \(M\) elements \((x_{1},\ldots,x_{M})\), a _position_\(j\) is a number in \([M]\) used to identify the element \(x_{j}\). A vector is said to be _diagonal_ if its elements in all positions are equal. Suppose \(f\) is a function on a domain \(\mathcal{X}\) and \(\bm{x}\) is a vector of \(M\) elements in \(\mathcal{X}\), we use the symbol \(f(\bm{x})\) to denote the vector \((f(x_{1}),\ldots,f(x_{M}))\).

Figure 1: Multi-Instance PLL. We aim to learn the \(f_{i}\)’s given the \(x_{i}\)’s and \(s\). \(M\) may be different from \(n\) and \(\sigma\) may be unknown.

**Classifiers.** Let \(\mathcal{X}\) denote an instance space and \(\mathcal{Y}\) denote an output space with \(|\mathcal{Y}|=c\). Let also \(\mathcal{D}\) be the joint distribution of two random variables \((X,Y)\in\mathcal{X}\times\mathcal{Y}\) and \(\mathcal{D}_{X}\) be the marginal of \(X\). Throughout the text, we use \(\bm{x}\) for \((x_{1},\dots,x_{M})\) and \(\bm{y}\) for the corresponding vector of gold labels. We consider _seroring functions_ of the form \(f:\mathcal{X}\to\Delta_{c}\), where \(\Delta_{c}\) is the space of probability distributions on \(\mathcal{Y}\) (e.g., \(f\) outputs the softmax probabilities of a neural network). We use \(f^{j}(x)\) to denote the \(j\)-th output of \(f(x)\). A scoring function \(f\) induces a _classifier_ whose _prediction_ on \(x\)\([f](x)\) is defined by \([f](x):=\operatorname*{argmax}_{j\in[c]}f^{j}(x)\). We use \(\mathcal{F}\) and \([\mathcal{F}]\) to denote the space of scoring functions and the space of classifiers induced by \(\mathcal{F}\), respectively. We use the VC-dimension, the Natarajan dimension and Rademacher complexities to characterize the complexity of \(\mathcal{F}\)[40].

**Loss functions.** We define the problem of _learning_ a scoring function using a fixed set of training samples as the one of _choosing_ the scoring function from \(\mathcal{F}\) that better _fits_ the training samples. We give semantics to the term _fits_ by using the _risk_\(\mathcal{R}(f;\ell)\) of \(f\) subject to a sampling distribution and a given _loss function_\(\ell\): the lower \(\mathcal{R}(f;\ell)\) becomes, the better \(f\) fits the data. When there exists an \(f^{*}\in\mathcal{F}\), such that \(\mathcal{R}(f^{*};\ell)=0\), we say that the space \(\mathcal{F}\) is _realizable_ under \(\ell\). In _supervised learning_, we are provided with samples of the form \((x,y)\) drawn from \(\mathcal{D}\). We use \(\widehat{\mathcal{R}}\) to denote the _empirical risk_, i.e., the average risk computed over the training samples only. The _risk_2 of \(f\) subject to a loss function \(\ell:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}^{+}\) is given by \(\mathcal{R}(f;\ell):=\mathbb{E}_{(X,Y)\sim\mathcal{D}}[\ell([f](X),Y)]\).

Footnote 2: As stated above, the risk is defined subject to a sampling distribution. For brevity, we will omit that distribution when defining the risk and fix the sampling distribution in each case.

A commonly used loss function is the _zero-one_ loss defined by \(\ell^{01}(y,y^{\prime}):=\mathbbm{1}\{y^{\prime}\neq y\}\) for any pair \(y,y^{\prime}\in\mathcal{Y}\). We use \(\mathcal{R}^{01}(f)\) as a short for \(\mathcal{R}(f;\ell^{01})\) and refer to it as the _zero-one risk of \(f\)_. We will also consider _Semantic Loss_ (SL) [56], which has been adopted to train classifiers subject to Boolean formulas [30, 43, 24]. Let \(\varphi\) be a Boolean formula where each variable in \(\varphi\) is associated with a class from \(\mathcal{Y}\). Then, the SL of \(\varphi\) subject to \(f(x)\) is the negative logarithm of the _weighted model counting_ (WMC) [7] of \(\varphi\) under \(f(x)\), namely \(\mathrm{SL}(\varphi,f(x)):=-\log(\mathrm{WMC}(\varphi,f(x)))\), where \(\mathrm{WMC}(\varphi,f(x))\) takes values in \((0,1)\) and denotes the probability that \(\varphi\) is logically satisfied when its variables become true with probabilities specified by \(f(x)\).

## 3 Learning a Single Classifier Under a Known Transition

We begin with the setting where the goal is to learn a single classifier \(f\in\mathcal{F}\) under a known transition \(\sigma\). Firstly, we show ERM-learnability (Theorem 1). Then, inspired by neuro-symbolic learning, we provide a Rademacher-style error bound with a top-\(k\) surrogate loss based on WMC (Theorem 2). Below, we formally introduce the learning setting.

**Problem setting.** Let \(\sigma:\mathcal{Y}^{M}\to\mathcal{S}\) be a transition function, where \(\mathcal{S}=\{1,\dots,d\}\) with \(|\mathcal{S}|=d\geq 1\). Let also \(\mathcal{T}_{\mathsf{P}}\) be a set of \(m_{\mathsf{P}}\)_partially labeled_ samples of the form \((\bm{x},s)=(x_{1},\dots,x_{M},s)\). Each training sample is formed by, firstly drawing \(M\) i.i.d. samples \((x_{i},y_{i})\) from \(\mathcal{D}\) and then setting \(s=\sigma(y_{1},\dots,y_{M})\). We use \(\mathcal{D}_{\mathsf{P}}\) to denote the distribution followed by the training samples \((\bm{x},s)\). In analogy to the zero-one classification loss \(\ell^{01}\), we define the _zero-one partial loss_ subject to \(\sigma\) as \(\ell^{01}_{\sigma}(\bm{y},s):=\mathbbm{1}\{\sigma(\bm{y})\neq s\}\), for any \(\bm{y}\in\mathcal{Y}^{M}\) and \(s\in\mathcal{S}\). The learner aims to find the classifier \(f\) with the minimal _zero-one risk_\(\mathcal{R}^{01}(f)\). As the gold labels are hidden, the learner uses the dataset \(\mathcal{T}_{\mathsf{P}}\) to estimate and minimize the _zero-one partial risk_ of \(f\) subject to \(\sigma\) defined as \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma):=\mathbb{E}_{(X_{1},\dots,X_{M},S)\sim \mathcal{D}_{\mathsf{P}}}[\ell^{01}_{\sigma}(([f](X_{1}),\dots,[f](X_{M})),S)]\).

We demonstrate these notions via Example 1. There, \(f\) is an MNIST classifier, \(\mathcal{X}\) is the space of MNIST images and \(\mathcal{Y}=\{0,\dots,9\}\). The training samples are of the form \((x_{1},x_{2},s)\) where \(s=\sigma(y_{1},y_{2})=y_{1}+y_{2}\). The partial risk of \(f\) is then the probability of predicting the wrong sum.

**Learnability.** A _partial learning algorithm_\(\mathcal{A}\) takes a partially labeled dataset \(\mathcal{T}_{\mathsf{P}}\) with \(|\mathcal{T}_{\mathsf{P}}|=m_{\mathsf{P}}\) as input and outputs a function \(\mathcal{A}(\mathcal{T}_{\mathsf{P}})\in\mathcal{F}\). Similar to standard PAC learning (see, for example, [40]), we say a multi-instance problem is _learnable_, if there exists a partial learning algorithm \(\mathcal{A}\), such that for any data distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\) and any \(\delta,\epsilon\in(0,1)\), there is an integer \(m_{\epsilon,\delta}\), such that \(m_{\mathsf{P}}\geq m_{\epsilon,\delta}\) implies \(\mathcal{R}^{01}(\mathcal{A}(\mathcal{T}_{\mathsf{P}}))\leq\epsilon\) with probability at least \(1-\delta\) with respect to \(\mathcal{D}_{\mathsf{P}}\). We consider \(\mathcal{A}\) to be an Empirical Risk Minimizer (ERM) algorithm that finds the classifier \(f\in\mathcal{F}\) which minimizes the empirical zero-one partial risk \(\widehat{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}}):= \sum_{(\bm{x},s)\in\mathcal{T}_{\mathsf{P}}}\ell^{01}_{\sigma}(([f](x_{1}), \dots,[f](x_{M})),s)/m_{\mathsf{P}}\).

### ERM Learnability

To prove learnability under the partial zero-one loss, we aim to bound \(\mathcal{R}^{01}(f)\) with the partial risk \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\). Firstly, we propose a sufficient and necessary condition called _\(M\)-unambiguity_.

Recall that learnability requires handling all possible distributions of \((\bm{x},\bm{y})\). Therefore, to propose a _necessary_ condition to guarantee learnability, we consider a special case: the one in which \(\mathcal{D}\) concentrates its mass on a single item \(x_{0}\), i.e., \(\mathbb{P}_{\mathcal{D}_{X}}(x_{0})=1\). In that case, the only sample observed during learning includes only the instance \(x_{0}\). Now, let \(l_{0}\in\mathcal{Y}\) be the gold label of \(x_{0}\). If \(f\) mispredicts the class of \(x_{0}\), i.e., \(f(x_{0})\neq l_{0}\), and this misprediction leads to the same observed partial label, i.e., \(\sigma(f(x_{0}),\ldots,f(x_{0}))=\sigma(l_{0},\ldots,l_{0})\), then the problem will be not learnable, as we will never be able to correct this error. Motivated by this special case, we propose the following condition:

**Definition 1** (\(M\)-unambiguity).: _Transition \(\sigma\) is \(M\)-unambiguous if for any two diagonal label vectors \(\bm{y}\) and \(\bm{y}^{\prime}\in\mathcal{Y}^{M}\) such that \(\bm{y}\neq\bm{y}^{\prime}\), we have that \(\sigma(\bm{y}^{\prime})\neq\sigma(\bm{y})\)._

Recall that a vector is diagonal if all its elements are equal. Below, we provide examples of transitions that satisfy (or not) the \(M\)-unambiguity condition.

**Example 2**.: _Consider the following learning problems._

* _(SUM-\(M\)): Consider a variant of Example_ 1 _where_ \(s\) _is the sum of_ \(M\) _digits, i.e.,_ \(\sigma(y_{1},\ldots,y_{M})=\sum_{i=1}^{M}y_{i}\)_. The transition is_ \(M\)_-unambigous, since_ \(\sigma(y,\ldots,y)\neq\sigma(y^{\prime},\ldots,y^{\prime})\) _holds for any_ \(y\neq y^{\prime}\)_._
* _(PRODUCT-\(M\)): Consider a variant of Example_ 1 _where_ \(s\) _is given by_ \(\sigma^{*}(y_{1},\ldots,y_{M})=\prod_{i=1}^{M}y_{i}\)_. Then,_ \(\sigma^{*}\) _is_ \(M\)_-unambiguous, since_ \(\sigma^{*}(y,\ldots,y)=y^{M}\neq(y^{\prime})^{M}\) _holds for any_ \(y\neq y^{\prime}\)_._
* _(XOR): Consider Boolean labels. We take the XOR of the two digits as the partial label, i.e.,_ \(\sigma^{\text{XOR}}(y_{1},y_{2})=\mathbbm{1}\{y_{1}\neq y_{2}\}\)_. Then_ \(\sigma^{\text{XOR}}\) _is not_ \(M\)_-unambiguous, since_ \(\sigma^{\text{XOR}}(1,1)=\sigma^{\text{XOR}}(0,0)\)_._

Although \(M\)-unambiguity is proposed as a necessary condition under special types of data distributions, we now show that it is also _sufficient_ for proving ERM-learnability. Firstly, we prove that the classification risk can be bounded by the partial risk.

**Lemma 1**.: _If \(\sigma\) is \(M\)-unambigous, then we have:_

\[\mathcal{R}^{01}(f)\leq\mathcal{O}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)^{1 /M})\quad\text{as}\quad\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\to 0\] (1)

_Moreover, if \(\sigma\) is not \(M\)-unambiguous, then learning from partial labels is arbitrarily difficult, in the sense that a classifier \(f\) with partial risk \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)=0\) can have a risk of \(\mathcal{R}^{01}(f)=1\)._

To show learnability, we bound the partial risk with its empirical counterpart in the realizable case.

**Theorem 1** (ERM learnability under \(M\)-unambiguity).: _Suppose \(\mathcal{F}\) is realizable under \(\ell^{01}_{\mathsf{P}}\) and \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\). Then for any \(\epsilon,\delta\in(0,1)\), there exists a universal3 constant \(C_{0}>0\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\hat{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}})=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

Footnote 3: A constant is universal if it that does not depend on the parameters of the learning problem (e.g., \(M\) or \(c\)).

\[m_{\mathsf{P}}\geq C_{0}\frac{c^{2M-2}}{\epsilon^{M}}\left(d_{[\mathcal{F}]} \log(6cMd_{[\mathcal{F}]})\log\left(\frac{c^{2M-2}}{\epsilon^{M}}\right)+\log \left(\frac{1}{\delta}\right)\right)\] (2)

Beyond Lemma 1, Theorem 1 builds upon several non-trivial intermediate results: (i) A bound of the VC dimension for the partial label predictor (Lemma 3 in the Appendix) and (ii) Construction of counter-examples for arguing the necessity of \(M\)-unambiguity (see the proof of Theorem 1).

**Toward a faster convergence rate.** Theorem 1 presents a rather slow convergence rate of \((c^{2}/\epsilon)^{M}\). In the following, we show that a better convergence rate is achievable by forcing stricter ambiguity conditions. In the following, we introduce the concept of _1-unambiguity_, which requires the transition to be sensitive to 1-position perturbations:

**Definition 2** (1-unambiguity).: _Transition \(\sigma\) is 1-unambiguous, if there exists an index \(1\leq i\leq M\), such that flipping the \(i\)-th label of any label vector \(\bm{y}\in\mathcal{Y}^{M}\), results in a vector \(\bm{y}^{\prime}\) with \(\sigma(\bm{y}^{\prime})\neq\sigma(\bm{y})\)._

In the following, we provide examples of transitions that satisfy (or not) the \(1\)-unambiguity condition.

**Example 3**.: _Let us continue with Example 2. The transition \(\sigma\) from SUM-\(M\) is 1-unambiguous since \(\sigma(\bm{y})=\sum_{i=1}^{M}y_{i}\) always change when replacing any of the labels \(y_{i}\). However, the transition \(\sigma^{*}\) from PRODUCT-\(M\) is not 1-unambiguous, since for the label vector \(\bm{y}=(0,1,1)\) (with product zero), the flipped vector \(\bm{y}^{\prime}=(0,1,2)\) also leads to product zero._

We show that a better convergence rate can be achieved if \(\sigma\) is both 1- and \(M\)-unambigous.

**Proposition 1** (ERM learnability under 1- and \(M\)-unambiguity).: _If \(\sigma\) is both 1- and \(M\)-unambigous, then we have:_

\[\mathcal{R}^{01}(f)\leq\mathcal{O}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)) \quad\text{as}\quad\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\to 0\] (3)

_Furthermore, if \(\mathcal{F}\) is realizable under \(\ell^{01}_{\mathsf{P}}\) and \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\), then for any \(\delta\in(0,1)\) and \(\epsilon\in(0,1)\) that is sufficiently close to 0, there exists a universal constant \(C_{1}\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widehat{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

\[m_{\mathsf{P}}\geq C_{1}\frac{1}{\epsilon}\left(d_{[\mathcal{F}]}\log(6cMd_{[ \mathcal{F}]})\log\left(\frac{2}{\epsilon}\right)+\log\left(\frac{1}{\delta} \right)\right)\] (4)

**Remark 1**.: The supervision power of a partial label \(s\) depends on the "instability" of \(\sigma\): if \(\sigma\) returns different results under certain perturbations in \(I\) positions for an \(I\in[M]\), then the classification risk is bounded by \(\mathcal{R}^{01}(f)\leq\mathcal{O}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)^{1/ I})\). To formalize this intuition, in the appendix, we provide a generalized definition of both 1- and \(M\)-unambiguity, called \(I\)-unambiguity, and show learnability with intermediate convergence rates. Notice that \(M\)-unambiguity is closely related to the small ambiguity degree condition in the PLL literature [28]. Appendix E.3 provides a relevant discussion and an extension of our results for randomized transitions.

**Remark 2**.: We use the concentrated distribution described at the beginning of Section 3.1 as a pivot to design a necessary and sufficient learnability condition. However, the results in Lemma 1, Theorem 1 and Proposition 1 do not apply only to this distribution, but to any distribution \(\mathcal{D}_{\mathsf{P}}\) of partial data. The same applies to all learnability results that follow.

### Error Bounds with the Top-\(k\) Semantic Loss

We now study the error bounds with the semantic loss (see Section 2 and Appendix A) under top-\(k\) approximations (Theorem 2). We are interested in this loss for two reasons. Firstly, risk minimization under the zero-one loss is intractable even for linear classifiers [17] and hence in practice, learning is performed by minimizing differentiable surrogate losses such as the semantic loss. Secondly, computing a surrogate loss typically introduces another source of intractability: that of enumerating the entries of \(\sigma\)[24, 43, 31]. For example, in SUM-\(M\), we have to enumerate \(10^{M}\) digit combinations to populate \(\sigma\), which is not scalable.

A popular way to reduce the second source of inefficiency is to consider only the \(k\) most probable label combinations during training, where the probability of a label vector \(\bm{y}\) given \(\bm{x}\) and classifier \(f\) is defined by \(P_{f(\bm{x})}(\bm{y}):=\prod_{i=1}^{M}f^{y_{i}}(x_{i})\). Then, training can proceed by taking the semantic loss over the top-\(k\) label vectors [24]. The above is possible, as the top-\(k\) label vectors can be equivalently viewed as a formula that is true iff one or more of the top-\(k\) label vectors is the gold one.

**Example 4**.: _Let us return back to Example 1. Instead of considering all three combinations \((2,0)\), \((1,1)\) and \((0,2)\) assuming a target \(s=2\), we can consider only the first and the last combination, if the probabilities (predicted by \(f\)) of the two images being 1 are way lower than those of being 0 or 2. Then, the top-2 label vectors \((2,0)\), \((0,2)\) can be seen as the formula \((A_{1,2}\wedge A_{2,0})\vee(A_{1,0}\wedge A_{2,2})\), where \(A_{i,j}\) is a Boolean variable that is true iff the \(i\)-th input digit is assigned label \(j\)._

We use \(\bigvee_{i=1}^{k}\bm{y}_{j}\) to denote the Boolean formula computed out of the \(k\) label vectors \(\bm{y}_{1},\dots,\bm{y}_{k}\), where each \(\bm{y}_{i}\) is the conjunction of Boolean variables of the form \(A_{i,y}\), as in Example 4. Given a softmax score \(f(\bm{x})\), variable \(A_{i,y}\) is assigned probability \(f^{y}(x_{i})\). We now define the _top-\(k\) partial loss_:

**Definition 3** (Top-\(k\) partial loss).: _The top-\(k\) partial loss for an integer \(k\geq 1\) subject to a classifier \(f\), transition \(\sigma\) and sample \((\bm{x},s)\) is defined as_

\[\ell^{k}_{\sigma}(f(\bm{x}),s):=\operatorname{SL}\left(\bigvee_{i=1}^{k}\bm{y} ^{(i)},f(\bm{x})\right)\] (5)_where \(\bm{y}^{(1)},\ldots,\bm{y}^{(k)}\in\mathcal{Y}^{M}\) are the top-\(k\) maximizers of \(P_{f(\bm{x})}\) in the preimage of \(s\), i.e., \(\{\bm{y}\in\mathcal{Y}^{M}:\sigma(\bm{y})=s\}\). The top-\(k\) partial classification risk of \(f\) subject to \(\sigma\) is then given by_

\[\mathcal{R}^{k}_{\mathcal{B}}(f;\sigma):=\mathbb{E}_{(X_{1},\ldots,X_{M},S) \sim\mathcal{D}_{\mathsf{P}}}[\ell^{k}_{\sigma}(f(X_{1},\ldots,X_{M}),S)]\] (6)

**Remark**.: The WMC is upper bounded by the sum of probabilities, so we can approximate the SL as \(\ell^{k}_{\sigma}(f(\bm{x}),s)\geq-\log(\sum_{i=1}^{k}P_{f(\bm{x})}(\bm{y}^{(i )}))\). Furthermore, the special case where \(k=1\) reduces to the infimum loss [5] and minimal loss [29] in the PLL literature as we show in Appendix B.2.

Let \(\mathfrak{R}_{m}(\mathcal{F})\) denote the Rademacher complexity [4, 10] of \(\mathcal{F}\) with \(m\) samples as defined in Appendix A. We are now ready to bound the zero-one classification risk with the empirical top-\(k\) partial risk.

**Theorem 2** (Error bound under unambiguity).: _Let an integer \(k\geq 1\) and \(\delta\in(0,1)\). If \(\sigma\) is both 1- and \(M\)-unambiguous, then with probability at least \(1-\delta\), we have:_

\[\mathcal{R}^{01}(f)\leq\Phi\left((k+1)\left(\widehat{\mathcal{R}}^{k}_{ \mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}})+2\sqrt{k}M^{3/2}\mathfrak{R}_{ Mm_{\mathsf{P}}}(\mathcal{F})+\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}} \right)\right)\] (7)

_where \(\widehat{\mathcal{R}}^{k}_{\mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}})= \sum_{(\bm{x},s)\in\mathcal{T}_{\mathsf{P}}}\ell^{k}_{\sigma}(f(\bm{x}),s)/ m_{\mathsf{P}}\) is the empirical counterpart of (6) and \(\Phi\) is an increasing function that satisfies \(\lim_{t\to 0}\Phi(t)/t=1\)._

Proof sketch.: Theorem 2 builds upon several results. Firstly, we derived an inequality that bounds the top-\(k\) loss with the zero-one loss (Lemma 5), which requires the construction of an intermediate \(\ell^{1}\) loss (Definition 11). Secondly, we show the Lipschtness of the semantic loss (Lemma 7). This result is further combined with a contraction lemma that is proposed in [10] (Lemma 6) to bound the Rademacher complexity of the model. 

**Remark**.: Similarly to Theorem 1 and Proposition 1, Theorem 2 suggests that the learning difficulty increases as \(M\) increases. This is intuitive, since it gets harder to disambiguate the gold labels when \(M\) increases, e.g., for 2-SUM, there are \(10^{2}\) possible label vectors need to be considered, while for 4-SUM, there are \(10^{4}\) ones. Our bound also suggests a tradeoff with the choice of \(k\): a larger \(k\) decreases the risk, but tends to increase the complexity term.

## 4 Learning Multiple Classifiers Under a Known Transition

In this section, we extend the learning problem from Section 3 to jointly learn \(n\geq 1\) different classifiers. Similarly to Section 3, we aim to propose minimal assumptions for the data distribution to show both results on ERM-learnability (Theorem 3) and a Rademacher-style error bound with the top-\(k\) loss (Theorem 4). We formally define the learning setting below.

**Problem setting.** We aim to learn \(n\geq 1\) classifiers, each of which maps instances from a space \(\mathcal{X}_{i}\) to the corresponding label space \(\mathcal{Y}_{i}\) with \(|\mathcal{Y}_{i}|=c_{i}\). For each \(i\in[n]\), we define a scoring space \(\mathcal{F}_{i}\), which contains mappings of the form \(f_{i}:\mathcal{X}_{i}\rightarrow\Delta_{c_{i}}\). Each \(f_{i}\) is used to classify \(M_{i}\geq 1\) instances \((x_{i1},\ldots,x_{iM_{i}})=\bm{x}_{i}\in\mathcal{X}_{i}^{M_{i}}\) with (hidden) gold labels \(\bm{y}_{i}=(y_{i1},\ldots,y_{iM_{i}})\). We denote the vector of scoring functions as \(\bm{f}=(f_{1},\ldots,f_{n})\). Let \(\mathcal{D}_{i}\) be the joint distribution over elements from \(\mathcal{X}_{i}\times\mathcal{Y}_{i}\). Each training sample given to the learner \((\bm{x}_{1},\ldots,\bm{x}_{n},s)\) is formed by (i) drawing \(M_{i}\) i.i.d. samples \((\bm{x}_{i},\bm{y}_{i})\) from \(\mathcal{D}_{i}\), for each \(i\in[n]\) and then (ii) obtaining its partial label as \(s=\sigma(\bm{y}_{1},\ldots,\bm{y}_{n})\). We override the notation \(\mathcal{D}_{\mathsf{P}}\) to denote the distribution of the training samples in this learning setting. The partially labeled dataset \(\mathcal{T}_{\mathsf{P}}\) then contains \(m_{\mathsf{P}}\) i.i.d. samples drawn from \(\mathcal{D}_{\mathsf{P}}\).

Similarly to Section 3, the learner aims to find the classifiers \(f_{1},\ldots,f_{n}\) with the minimal _zero-one risk_ defined as \(\mathcal{R}^{01}(\bm{f}):=\sum_{i=1}^{n}\mathcal{R}^{01}(f_{i})\). As the gold labels are hidden, the learner uses the dataset \(\mathcal{T}_{\mathsf{P}}\) to estimate and minimize the _zero-one partial risk_ of \(f_{1},\ldots,f_{n}\), which is defined as \(\mathcal{R}_{\mathsf{P}}^{01}(f_{1},\ldots,f_{n};\sigma):=\mathbb{E}_{(\bm{X} _{1},\ldots,\bm{X}_{n},S)\sim\mathcal{D}_{\mathsf{P}}}[\ell^{01}_{\sigma}(([f_{ 1}](\bm{X}_{1}),\ldots,[f_{n}](\bm{X}_{n})),S)]\), where \([f_{i}](\bm{X}_{i})\) is a short for \(([f_{i}](X_{i1}),\ldots,[f_{i}](X_{iM_{i}}))\), for \(1\leq i\leq M_{i}\).

**Example 5** (Learning binary operators).: _Consider a setting where we aim to train a classifier \(f_{1}\) for recognizing MNIST digits and a classifier for recognizing images of addition and multiplication taken from some space \(\mathcal{X}_{2}\). The training samples are of the form \((x_{11},x_{12},x_{21},s)\), where \(x_{11}\) and \(x_{12}\) are non-zero MNIST digits with \(\mathcal{Y}_{1}=\{1,\ldots,9\}\), \(x_{21}\) is an image of an operator with \(\mathcal{Y}_{2}=\{+,\times\}\), and \(s\) is the result of applying the operator in \(x_{2}\) on the digits in \(x_{11}\) and \(x_{12}\)._

### ERM Learnability

Similarly to Section 3, to propose a necessary condition for learnability, we consider the most challenging distribution where each \(\mathcal{D}_{i}\) is concentrated on a single instance \(x_{i}^{*}\in\mathcal{X}_{i}\), for \(1\leq i\leq n\). Then, the only sample that will be observed during learning is \((\bm{x}_{1}^{*},\dots,\bm{x}_{n}^{*})\) where \(\bm{x}_{i}=(x_{i}^{*},\dots,x_{i}^{*})\), for \(1\leq i\leq n\). Assuming that \(l_{i}^{*}\) is the gold label of \(x_{i}^{*}\), detecting a classification error is possible only if a misclassification of any \(x_{i}^{*}\), i.e., \([f_{i}](x_{i}^{*})\neq l_{i}^{*}\), will lead to a prediction vector \(\bm{y}^{\prime}\) with a different image under \(\sigma\) from the gold label \(\bm{y}\), i.e., \(\sigma(\bm{y}^{\prime})\neq\sigma(\bm{y})\). The above intuition is captured via the notion of _multi-unambiguity_, which generalizes the notion of \(M\)-unambiguity. Below, we use \(\bm{y}_{i}^{M_{i}}\), for \(i\in[n]\), to denote the \(M_{i}\)-ary diagonal vector including the \(y_{i}\) element only.

**Definition 4** (Multi-unambiguity).: _Transition \(\sigma\) is multi-unambiguous if for any vector \(\bm{y}=(\bm{y}_{1}^{M_{1}},\dots,\bm{y}_{n}^{M_{n}})\), and any position \(i\in[n]\), such that the vector \(\bm{y}^{\prime}\) that results after flipping the labels in \(\bm{y}_{i}^{M_{i}}\) to some diagonal vector \((\bm{y}_{i}^{\prime})^{M_{i}}\neq\bm{y}_{i}^{M_{i}}\), has a different image under \(\sigma\), i.e., \(\sigma(\bm{y})\neq\sigma(\bm{y}^{\prime})\)._

Multi-unambiguity reduces to \(M\)-unambiguity for \(n=1\). An example of multi-unambiguity is below.

**Example 6** (Learning binary operator, cont'd).: _In Example 5, the multi-unambiguity condition is violated since \(2+2=2\times 2\). Namely, if a distribution assigns all its weight to the digit \(2\), then it is difficult for the model to distinguish the two operators. On the other hand, if instead, the label space is \(\mathcal{Y}_{1}=\{3,\dots,9\}\), then the multi-unambiguity condition is satisfied._

**Bounded risk assumption.** Differently from Section 3, multi-unambiguity alone is not sufficient to ensure learnability. To see this, consider a variant of SUM2 where the digits are classified by two independent classifiers, \(f_{1}\) and \(f_{2}\). If the distributions of the first and the second image are concentrated on 1 and 7 respectively, but \(f_{1}(x_{1})=7\) and \(f_{2}(x_{2})=1\), then these errors cannot be detected. Therefore, we assume there is a constant \(R<1\), such that \(\mathcal{R}^{01}(f_{i})\leq R\) holds for any \(i\in[n]\) and any \(f\in\mathcal{F}_{i}\). We refer to such \(f_{i}\)'s as _zero-one risk \(R\)-bounded_. This assumption is mild since it only requires the classifiers to be slightly better than being totally wrong. Also, when a small directly labeled dataset is available, one can bound the classification risks away from 1 with high probability in a uniform manner using standard learning theory. See Appendix C for more details. Now, we are ready to state the ERM-learnability result assuming \(\prod_{i=1}^{n}\mathcal{F}_{i}\) is realizable under \(\ell_{\mathsf{P}}^{01}\).

**Theorem 3** (ERM learnability under multi-unambiguity).: _Assume that there is a constant \(R<1\), such that for each \(i\in[n]\), each \(f\in\mathcal{F}_{i}\) is zero-one risk \(R\)-bounded. Assume also that there exist positive integers \(M^{*}\) and \(c^{*}\), such that \(M_{i}\leq M^{*}\) and \(c_{i}\leq c_{0}\) hold for any \(i\in[n]\). Then, if \(\sigma\) is multi-unambiguous, we have:_

\[\mathcal{R}^{01}(\bm{f})\leq\mathcal{O}((\mathcal{R}_{\mathsf{P}}^{01}(\bm{f} ;\sigma))^{1/M^{*}})\quad\text{as}\quad\mathcal{R}_{\mathsf{P}}^{01}(\bm{f}; \sigma)\to 0\] (8)

_Furthermore, for any \(\epsilon,\delta\in(0,1)\), there is a universal constant \(C_{3}\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widehat{\mathcal{R}}_{\mathsf{P}}^{01}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\) if_

\[m_{\mathsf{P}}\geq C_{3}\frac{nc_{0}^{2M^{*}-2}}{\epsilon^{M^{*}}(1-R)^{M}} \left(\sum_{i=1}^{n}d_{[\mathcal{F}_{i}]}\log(nc_{i}M_{i}d_{[\mathcal{F}_{i}] })\log\left(\frac{nc_{0}^{2M^{*}-2}}{\epsilon^{M^{*}}(1-R)^{M}}\right)+\log \left(\frac{1}{\delta}\right)\right)\] (9)

**Remark.** Theorem 3 suggests that the rate of convergence is controlled by \(M^{*}\). A smaller \(M^{*}\) leads to a more strict unambiguity condition. For example, the case \(M^{*}=1\) requires \(\sigma\) to be unstable to _any_ 1-position perturbations, while the case \(M^{*}=M\) reduces to \(M\)-unambiguity. This observation confirms the intuition that the supervision power of partial labels depends on the "instability" of \(\sigma\).

The top-\(k\) partial loss \(\ell_{\sigma}^{k}(\bm{f}(\bm{x}),s)\) for multiple classifiers subject to \(\bm{f}\in\mathcal{F}^{n}\), transition \(\sigma\) and \(s\in\mathcal{S}\) straightforwardly extends the top-\(k\) partial loss for the single classifier case, see Definition 3. Similarly, the _(empirical) top-\(k\) partial classification risk_ of \(\bm{f}\) subject to \(\sigma\) straightforwardly extends the one from Section 3.2. Below, we provide a Rademacher-style error bound with the top-\(k\) loss.

**Theorem 4** (Error bound under multi-unambiguity with multiple classifiers).: _Suppose \(\sigma\) is multi-unambiguous and each \(f_{i}\) is zero-one risk \(R\)-bounded, for \(R\in(0,1)\). Then, for any integer \(k\geq 1\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have:_

\[\mathcal{R}^{01}(\bm{f})\leq\left(\frac{nc_{0}^{2M^{*}-2}(k+1)}{(1-R)^{M}} \left(\widehat{\mathcal{R}}_{\mathsf{P}}^{k}(\bm{f};\sigma;\mathcal{T}_{\mathsf{ P}})+\sqrt{kM}\sum_{i=1}^{n}M_{i}\mathfrak{R}_{m_{\mathsf{P}}M_{i}}( \mathcal{F}_{i})+\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}}\right)\right)^{1 /M^{*}}\]Learning Under an Unknown Transition

We now explore another direction by dropping the assumption that \(\sigma\) is known. Instead, we assume the learner has access to a _transition space_\(\mathcal{G}\) that contains mappings of the form \(\mathcal{Y}^{M}\to\mathcal{S}\) including the "true" transition \(\sigma\). Notice that learning now becomes particularly challenging, since \(\mathcal{G}\) can be expressive enough to lead to correct predictions for \(s\) from many wrong classifications.

**Problem setting.** To illustrate the core idea, we consider a simple setting where we only learn a single scoring function \(f\in\mathcal{F}\) as in Section 3. However, \(\sigma\) is an unknown mapping in \(\mathcal{G}\). Given partially labeled samples, the learner aims to minimize the _zero-one partial risk of \(f\) subject to \(\mathcal{G}\)_\(\mathcal{R}^{01}_{\Theta}(f;\mathcal{G})\) that is defined as the _minimal_ partial risk to predict \(s\) with a candidate transition in \(\mathcal{G}\), i.e., \(\mathcal{R}^{01}_{\Theta}(f;\mathcal{G}):=\inf_{\sigma^{*}\in\mathcal{G}} \mathcal{R}^{01}_{\Theta}(f;\sigma^{*})\). The risk is empirically estimated with a partially labeled dataset \(\mathcal{P}_{\text{p}}\) as \(\inf_{\sigma^{*}\in\mathcal{G}}\widehat{\mathcal{R}}^{01}_{\Theta}(f;\sigma^{ *};\mathcal{P}_{\text{p}})\). We demonstrate this learning setting with the following example:

**Example 7**.: _Let \(\mathcal{G}=\{(y_{1},y_{2})\mapsto\alpha y_{1}+\beta y_{2}|(\alpha,\beta)\in \mathbb{R}^{2}-\{(0,0)\}\}\), namely all the weighted sums with at least one non-zero weight. We aim to learn an MNIST classifier \(f\), given training samples of the form \((x_{1},x_{2},s)\), where the \(x_{i}\)'s are MNIST digits and \(s\) is the result of applying some \(\sigma\) from \(\mathcal{G}\) on \((y_{1},y_{2})\), where \(y_{i}\) is the prediction of \(f\) on \(x_{i}\). The gold \(\sigma\), i.e., the exact \(\alpha\) and \(\beta\), is unknown._

### ERM Learnability

Similarly to the known transition case, learnability requires us to learn under any data distribution \(\mathcal{D}\). Slightly differently from Sections 3 and 4, we start with the adversarial distribution where the mass in \(\mathcal{D}\) is concentrated to a single label, i.e., all instances \(x_{i}\) have the same gold label \(l_{i}\), and hence, the training samples are all associated with the gold label vector \(\boldsymbol{y}=(l_{i},\dots,l_{i})\). Suppose there is a certain probability that \(f\) misclassifies \(l_{i}\) as \(l_{j}\). Then, the predicted label vectors will be in \(\{l_{i},l_{j}\}^{M}\). In this case, it will be impossible to detect the classification errors if there is a candidate transition \(\sigma^{\prime}\in\mathcal{G}\) which maps all the vectors in \(\{l_{i},l_{j}\}^{M}\) to the gold observation of \(s\). Since the actual \(\sigma\) is unknown, we require this does not happen for any pair of \(\sigma,\sigma^{\prime}\in\mathcal{G}\) to ensure learnability. Below, we formalize our learnability condition:

**Definition 5** (Unambiguous transition space).: _Transition space \(\mathcal{G}\) is unambigous, if for each \(\sigma^{\prime}\in\mathcal{G}\), each diagonal label vector \(\boldsymbol{y}=(l_{i},\dots,l_{i})\), where \(l_{i}\in\mathcal{Y}\), and each \(l_{j}\neq l_{i}\), where \(l_{j}\in\mathcal{Y}\), there exists a vector \(\boldsymbol{y}^{\prime}\in\{l_{i},l_{j}\}^{M}\), such that \(\sigma^{\prime}(\boldsymbol{y}^{\prime})\neq\sigma(\boldsymbol{y})\)._

In pratic, one can examine whether a transition space is unambiguous by checking if for each \(\sigma^{\prime}\in\mathcal{G}\) and any two different labels \(l_{i}\neq l_{j}\), set \(\{\sigma^{\prime}(\boldsymbol{y})|\boldsymbol{y}\in\{l_{i},l_{j}\}^{M}\}\) is not a singleton. The above ensures that when given a fixed diagonal label vector, and when the classifier makes mistakes, the predicted partial labels are not unique, and hence cannot all agree with the ground truth label.

Class \(\mathcal{G}\) in Example 7 is unambiguous: for each transition \((y_{1},y_{2})\mapsto\alpha^{\prime}y_{1}+\beta^{\prime}y_{2}\) and each two labels \(l^{\prime}\neq l\), the set \(\{\alpha^{\prime}y_{1}+\beta^{\prime}y_{2}|(y_{1},y_{2})\in\{l,l^{\prime}\}^{2}\}\) is not a singleton, so there must exist a label vector with a different weighted sum that is different than the true sum. A counterexample is below:

**Example 8**.: _Let \(\mathcal{G}=\{(y_{1},y_{2})\mapsto w_{1}y_{1}+w_{2}y_{2}^{2}+w_{3}y_{2}^{2}+w_ {4}y_{2}|w_{i}\neq 0\ \forall i\in[4]\}\). Consider the true transition \(\sigma:(y_{1},y_{2})\mapsto y_{1}-y_{1}^{2}+y_{2}-y_{2}^{2}\) from \(\mathcal{G}\) and a candidate transition \(\sigma^{\prime}:(y_{1},y_{2})\mapsto y_{1}-y_{1}^{2}-y_{2}+y_{2}^{2}\). Then, \(\mathcal{G}\) is not unambiguous, since \(\sigma^{\prime}(\boldsymbol{y})=\sigma(\boldsymbol{y})=0\) for any \(\boldsymbol{y}\in\{0,1\}^{2}\). Namely, the partially labeled data are not informative enough to distinguish the label \(0\) from \(1\)._

**Bounded risk assumption.** Similarly to Section 4, this unambiguity condition is necessary but not sufficient to show learnability. To show learnability, we additionally assume there is a constant \(r>0\) such that \(\mathbb{P}([f](X)=y|Y=y)\geq r\) for each \(y\in\mathcal{Y}\) and \(f\in\mathcal{F}\). We refer to such an \(f\) as \(r\)_-bounded_. This assumption is slightly stronger than that of Section 4 as it additionally requires the classifiers to be not fully wrong for _every_ possible label. The main learnability result of this section is below:

**Theorem 5**.: _If \(\mathcal{G}\) is unambiguous and any \(f\in\mathcal{F}\) is \(r\)-bounded, then we have:_

\[\mathcal{R}^{01}(f)\leq O(\mathcal{R}^{01}_{\Theta}(f;\mathcal{G})^{1/M})\quad \text{as}\quad\mathcal{R}^{01}_{\Theta}(f;\mathcal{G})\to 0\] (10)

_Furthermore, suppose \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\) and the function class \(\{(\boldsymbol{y},s)\mapsto 1\{\sigma^{\prime}(\boldsymbol{y})\neq s\}|\sigma^{ \prime}\in\mathcal{G}\}\) has a finite VC-dimension \(d_{\mathcal{G}}\). Then, for any \(\epsilon,\delta\in(0,1)\), there is a universal constant \(C_{4}\) such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widehat{\mathcal{R}}_{\mathbf{p}}^{01}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

\[m_{\mathbf{p}}\geq C_{4}\frac{c^{2M-2}}{r^{M}\epsilon^{M}}\left(\left((d_{[ \mathcal{F}]}+d_{\mathcal{G}})\log(6M(d_{[\mathcal{F}]}+d_{\mathcal{G}}))+d_{[ \mathcal{F}]}\log c\right)\log\left(\frac{c^{2M-2}}{r^{M}\epsilon^{M}}\right)+ \log\left(\frac{1}{\delta}\right)\right)\]

## 6 Experiments

We further explore the learning scenario in Section 5 with empirical evaluations. We aim to learn an MNIST classifier using the weighted sum of 2, 3 and 4 MNIST digits and assuming that the weights are unknown, as in Example 7. The weighted sum function is very expressive as it can model Boolean formulas including conjunction and disjunction [36]. Notice that we do not aim to exhaustively assess the weak supervision literature ([24; 43; 59; 13]), but to validate the results of our theoretical analysis.

**Baselines.** We considered state-of-the-art neuro-symbolic frameworks, namely DeepProbLog (DLog) [30], DeepProbLog with approximations (DLog-A) [31], NeuroLog (NLog) [43], NeurASP (NASP) [59], ABL [13], ENT [27] and Scallop [24]. Only the target sum is used during training under those frameworks. DLog and NLog employ the semantic loss without any approximations (see Section 2). NLog(\(k\)) denotes an NLog variant where only the top-\(k\) predictions are considered during training, while Scallop(\(k\)) denotes Scallop using the top-\(k\) semantic loss, see Section 3.2. The approximations in DLog-A are different from the ones in NLog and Scallop; however, both DLog-A and NLog employ the semantic loss on the chosen subset of proofs. As the above frameworks learn classifiers under fixed theories only, we considered weights in \(\{1,2,3,4,5\}\), and used an additional neural classifier to learn the unknown weights. Finally, we considered standard supervised learning (SL). To contrast learning under unknown to learning under known transitions, we also ran experiments assuming that the weights are known. Our results show that the classification accuracy exceeds the 98% even for \(M=10\). More details on this experiment are in the arXiv version of this paper [51].

**Results.** Tables 1 and 2 report the accuracy of the learned MNIST classifiers over ten runs. #0, #8 and #16 denote the number of directly labeled samples used for pretraining the MNIST classifiers: #0 means no pretraining; for #8 and #16, the accuracy of the pre-trained classifiers is 37% and 58%. Table 1 presents results for \(M=2\), while Table 2 presents results for \(M\in\{2,3,4\}\) for Scallop and the two best performing frameworks in Table 1, DLog and NLog- NASP could not scale for \(M\geq 3\). We used DLog-A for \(M\in\{3,4\}\), due to scalability reasons. Similarly, NLog without approximations does not scale for \(M=4\) (N/A indication). In Table 1, the number of weak samples is in parentheses, e.g., DLog(#4K). For ABL and ENT, we used 10K samples, as they rely on sampling, and used the hyperparameters suggested by the authors.

**Discussion.** The results confirm our theory: (i) we can learn classifiers under unknown transitions as per Theorem 5 (see the accuracy for DLog and NLog in Table 1); and (ii) learning gets harder when \(M\) increases as per Eq. (5) (see the accuracy for different \(M\)'s in Table 2). For \(M=2\), weak supervision leads to better accuracy than SL even without pre-training and with fewer training samples: DLog(#4K) and NLog(#4K) lead to mean accuracy 96% and 97%, while SL(#4K) leads to mean accuracy 93%, see Table 1. This result suggests that the \(r\)-bounded assumption in Theorem 5 could be potentially relaxed. Another observation is that the approximations in DLog-A are less effective than the top-\(k\) one of NLog. We leave the theoretical analysis of the latter approximation as part of future research. Table 2 also suggests that the top-\(k\) semantic loss constitutes the most scalable and effective approach to training: in contrast to DLog-A and NLog, the accuracy of the digit classifier reaches 78% for \(M=4\), when \(k=4\) for Scallop.

Our empirical analysis shows that a key issue is _scalability_. This is a known problem in neuro-symbolic learning and it is partially because the relevant problems in logic are intractable. For instance, semantic loss requires computing the models of Boolean formulas, which is #P-complete. Reasoning over logical theories can be also intractable [2], if not undecidable. Scalability straightforwardly affects accuracy, e.g., NLog obtains better accuracy over DLog-A for \(M=3\) (see Table 2) due to its ability to explore the whole search space. It is also worth stressing that Scallop could not scale beyond \(M=4\) for \(k>1\). The above challenges bring up new research directions [44; 41]. Closing this discussion, it is worth exploring why ABL and ENT led to low accuracy4.

Footnote 4: The authors from [43] also reported low accuracy for ABL for some scenarios.

## 7 Related Work

We provide an overview of the literature that closely relates to our work. A more comprehensive review of PLL, latent structural learning, and neuro-symbolic learning, is presented in Appendix F.

**Partial label learning.** Learnability of PLL is typically shown under the _small ambiguity_ assumption [11; 28; 5]. While it is technically possible to cast our problem to standard PLL by viewing a target \(s\) as a partial label for the hidden labels \(\bm{y}\), the small ambiguity condition is violated in our case since a distracting label can occur because \(\sigma\) is deterministic. Therefore, our proposed conditions relax and generalize the small ambiguity by allowing deterministic transitions, see Appendix E.3. Another line of work in weak supervision exploits the invertibility of _transition matrices_[46] to compute posterior class probabilities [8; 9; 61]. Our learnability condition, \(M\)-unambiguity, and the invertibility condition do not imply each other, see Appendix E.1 and E.2. There, we also show that small ambiguity and the invertibility condition do not imply each other, which might be of independent interest. Another related topic is _learning with label constraints_[21; 6; 49], where the label of each instance \(x\) (possibly structured) in the dataset is constrained in a subset \(C\subseteq\mathcal{Y}\). The difference is that the constraint mapping itself is known to the learner and hence can be encoded in the inference algorithm directly, for example, via the CCM [6].

**Neuro-symbolic and structured learning.** We were motivated by frameworks that employ logic for training neural models [30; 53; 13; 59; 43; 31; 24; 27; 23]. We prove learnability under (unknown) transitions that capture different languages and error bounds under the SL [56] and top-\(k\) approximations. Notice that [58] also proves the consistency of a top-\(k\) loss. However, they use a zero-one-style loss for standard supervised learning. Our work is closely related to [62], which studies a similar problem of weakly supervised learning with multiple instances. Our results are stronger in the sense that we propose sufficient and necessary conditions to recover the hidden labels, while [62] concerns the likelihood of the observed labels rather than the hidden ones. Our work is also relevant to _latent structural prediction_[42; 35; 34; 32]. To our knowledge, no formal learning guarantees have been given for that problem. Complementary to ours is the work in [19; 47; 33] that integrates combinatorial solvers into deep models.

## 8 Conclusions

We formulated multi-instance PLL and showed its connections with latent structural and neuro-symbolic learning. Our work exhibits a greater level of flexibility compared to (single-instance) PLL, as it allows for multiple instances and deterministic transitions. We introduced minimal assumptions that enabled us to establish ERM-learnability and derive error bounds with the top-\(k\) semantics loss. Our findings suggest that the interaction of multiple instances in forming the observed labels can help relax the learnability assumptions in weakly supervised learning. For future work, we will further relax the learning conditions by assuming a certain degree of smoothness for the data distributions. Another direction is to explore the setting where the hidden labels are non-i.i.d. or structured.

\begin{table}
\begin{tabular}{|l||c c||c c||c c c|} \hline  & \multicolumn{2}{c||}{\(M=2,mp=4\)K} & \multicolumn{2}{c||}{\(M=3,mp=4\)K} & \multicolumn{2}{c|}{\(M=4,mp=10\)K} \\ \hline  & \#0 & \#16 & \#0 & \#16 & \#0 & \#16 \\ \hline
**DLog-A** & \(34\%\pm 0.1\) & \(80\%\pm 0.06\) & \(24\%\pm 0.09\) & \(27\%\pm 0.13\) & \(18\%\pm 0.02\) & \(26\%\pm 0.25\) \\
**NLog** & \(97\%\pm 0.05\) & \(97\%\pm 0.01\) & \(61\%\pm 0.07\) & \(97\%\pm 0.01\) & N/A & N/A \\
**NLog(5)** & \(48\%\pm 0.003\) & \(87\%\pm 0.007\) & \(27\%\pm 0.06\) & \(84\%\pm 0.06\) & \(29\%\pm 0.1\) & \(46\%\pm 0.05\) \\
**Scallop(1)** & \(97\%\pm 0.05\) & \(97\%\pm 0.01\) & \(21\%\pm 3.5\) & \(97\%\pm 0.01\) & \(36\%\pm 0.2\) & \(54\%\pm 0.02\) \\
**Scallop(4)** & \(97\%\pm 0.05\) & \(97\%\pm 0.05\) & \(53\%\pm 0.15\) & \(97\%\pm 0.07\) & \(51\%\pm 0.09\) & \(78\%\pm 0.05\) \\ \hline \end{tabular}
\end{table}
Table 2: Classifier accuracy for WEIGHTED-SUM for \(M=2\). #0, #8 and #16 are # of directly labeled samples used for pre-training. Inside the parentheses are the # of (weak) training samples.

\begin{table}
\begin{tabular}{|l||c c||c c c c|} \hline  & **SL(4K)** & **SL(8K)** & **DLog(\#4K)** & **NLog(\#4K)** & **NASP(\#4K)** & **ABL(\#10K)** & **ENT(\#10K)** \\ \hline
40 & \(93\%\pm 0.01\) & \(96\%\pm 0.008\) & \(96\%\pm 0.004\) & \(97\%\pm 0.05\) & \(98\%\pm 0.01\) & \(9\%\pm 0.05\) & \(40\%\pm 15.2\) \\
**\#8** & \(93\%\pm 0.01\) & \(96\%\pm 0.008\) & \(96\%\pm 0.005\) & \(97\%\pm 0.005\) & \(98\%\pm 0.01\) & \(10\%\pm 0.01\) & \(43\%\pm 17.11\) \\
**\#16** & \(93\%\pm 0.01\) & \(96\%\pm 0.008\) & \(96\%\pm 0.001\) & \(97\%\pm 0.01\) & \(98\%\pm 0.01\) & \(11\%\pm 0.1\) & \(49\%\pm 16.4\) \\ \hline \end{tabular}
\end{table}
Table 1: Classifier accuracy for WEIGHTED-SUM for \(M=2\). #0, #8 and #16 are # of directly labeled samples used for pre-training. Inside the parentheses are the # of (weak) training samples.

## Acknowledgements

This work was partially supported by Contract FA8750-19-2-0201 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.

This work was also partially sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

This work was also supported by Contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.

This work was also partially funded by ONR Contract N00014-19-1-2620.

## References

* [1] S. Abiteboul, R. Hull, and V. Vianu. _Foundations of Databases_. Addison Wesley, 1995.
* [2] Jean-Francois Baget, Marie-Laure Mugnier, Sebastian Rudolph, and Michael Thomazo. Walking the complexity lines for generalized guarded existential rules. page 712-717, 2011.
* [3] Othman El Balghiti, Adam N. Elmachtoub, Paul Grigas, and Ambuj Tewari. Generalization bounds in the predict-then-optimize framework. In _NeurIPS_, pages 14389-14398, 2019.
* [4] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _The Journal of Machine Learning Research_, 3:463-482, 2003.
* [5] Vivien Cabannes, Alessandro Rudi, and Francis Bach. Structured prediction with partial labelling through the infimum loss. In _ICML_, page 1230-1239, 2020.
* [6] Ming-Wei Chang, Lev Ratinov, and Dan Roth. Structured learning with constrained conditional models. _Machine Learning_, 88(3):399-431, 2012.
* 799, 2008.
* [8] Jesus Cid-Sueiro. Proper losses for learning from partial labels. In _NeurIPS_, pages 1574-1582, 2012.
* [9] Jesus Cid-Sueiro, Dario Garcia-Garcia, and Raul Santos-Rodriguez. Consistency of losses for learning from weak labels. In _ECML PKDD_, pages 197-210, 2014.
* [10] Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Structured Prediction Theory Based on Factor Graph Complexity. In _NeurIPS_, 2016.
* [11] Timothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. _Journal of Machine Learning Research_, 12:1501-1536, 2011.
* [12] Wang-Zhou Dai and Stephen Muggleton. Abductive knowledge induction from raw data. In Zhi-Hua Zhou, editor, _IJCAI_, pages 1845-1851. AAAI, 2021.
* [13] Wang-Zhou Dai, Qiuling Xu, Yang Yu, and Zhi-Hua Zhou. Bridging Machine Learning and Logical Reasoning by Abductive Learning. In _NeurIPS_, pages 2815-2826, 2019.
* [14] Artur S. d'Avila Garcez, Krysia Broda, and Dov M. Gabbay. _Neural-symbolic learning systems: foundations and applications_. Perspectives in neural computing. Springer, 2002.

* [15] Adam N. Elmachtoub and Paul Grigas. Smart "predict, then optimize". _CoRR_, abs/1710.08005, 2017.
* [16] Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. _Journal of Artificial Intelligence Research_, 61(1):1-64, 2018.
* [17] Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of monomials by halfspaces is hard. _SIAM Journal on Computing_, 41(6):1558-1590, 2012.
* [18] Jonathan Feldstein, Dominic Phillips, and Efthymia Tsamoura. Principled and efficient motif finding for structure learning of lifted graphical models. _CoRR_, abs/2302.04599, 2023.
* [19] Aaron M. Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program as a layer. _CoRR_, abs/1907.05912, 2019.
* [20] Robert Fink, Jiewen Huang, and Dan Olteanu. Anytime approximation in probabilistic databases. _VLDB Journal_, 22(6):823-848, 2013.
* [21] Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and Ben Taskar. Posterior regularization for structured latent variable models. _Journal of Machine Learning Research_, 11:2001-2049, 2010.
* [22] Nitish Gupta, Sameer Singh, Matt Gardner, and Dan Roth. Paired examples as indirect supervision in latent decision models. In _EMNLP_, pages 5774-5785, 2021.
* [23] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. In _ACL_, pages 2410-2420, 2016.
* [24] Jiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, and Xujie Si. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. In _NeurIPS_, pages 25134-25145, 2021.
* [25] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In _NeurIPS_, pages 897-904, 2002.
* [26] Antonis C. Kakas. Abduction. In Claude Sammut and Geoffrey I. Webb, editors, _Encyclopedia of Machine Learning and Data Mining_, pages 1-8. Springer US, Boston, MA, 2017.
* [27] Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, and Jian Lu. Softened symbol grounding for neurosymbolic systems. In _ICLR_, 2023.
* [28] Li-Ping Liu and Thomas G. Dietterich. Learnability of the superset label learning problem. In _ICML_, pages 1629---1637, 2014.
* [29] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In _ICML_, page 6500-6510, 2020.
* [30] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In _NeurIPS_, pages 3749-3759, 2018.
* [31] Robin Manhaeve, Giuseppe Marra, and Luc De Raedt. Approximate Inference for Neural Probabilistic Logic Programming. In _KR_, pages 475-486, 2021.
* [32] Tsvetomila Mihaylova, Vlad Niculae, and Andre F. T. Martins. Understanding the mechanics of SPIGOT: Surrogate gradients for latent structure learning. In _EMNLP_, pages 2186-2202, 2020.
* [33] Anselm Paulus, Michal Rolinek, Vit Musil, Brandon Amos, and Georg Martius. Comboptnet: Fit the right np-hard problem by learning integer programming constraints, 2021.
* [34] Hao Peng, Sam Thomson, and Noah A. Smith. Backpropagating through structured argmax using a SPIGOT. In _ACL_, pages 1863-1873, 2018.
* [35] Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision with linear moments. In _ICML_, volume 48, pages 2568-2577, 2016.

* [36] Dan Roth and Wen-tau Yih. _Global Inference for Entity and Relation Identification via a Linear Programming Formulation_. MIT Press, Introduction to Statistical Relational Learning edition, 2007.
* [37] Rajhans Samdani, Ming-Wei Chang, and Dan Roth. Unified expectation maximization. In _ACL_, pages 688-698, 2012.
* [38] Prithviraj Sen, Breno W. S. R. de Carvalho, Ryan Riegel, and Alexander Gray. Neuro-symbolic inductive logic programming with logical neural networks. _AAAI_, 36(8):8212-8219, 2022.
* [39] Junghoon Seo and Joon Suk Huh. On the power of deep but naive partial label learning. In _ICASSP_, pages 3820-3824, 2021.
* [40] Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, USA, 2014.
* [41] Mate Soos, Divesh Aggarwal, Sourav Chakraborty, Kuldeep S. Meel, and Maciej Obremski. Engineering an efficient approximate dnf-counter. In _IJCAI_, pages 2031-2038, 2023.
* [42] Jacob Steinhardt and Percy S Liang. Learning with relaxed supervision. In _NeurIPS_, volume 28, 2015.
* [43] Efthymia Tsamoura, Timothy Hospedales, and Loizos Michael. Neural-symbolic integration: A compositional perspective. In _AAAI_, pages 5051-5060, 2021.
* [44] Efthymia Tsamoura, Jaehun Lee, and Jacopo Urbani. Probabilistic reasoning at scale: Trigger graphs to the rescue. _Proceedings of the ACM on Management of Data_, 1(1), 2023.
* [45] Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. Learning from explicit and implicit supervision jointly for algebra word problems. In _EMNLP_, pages 297-306, 2016.
* [46] Brendan van Rooyen and Robert C. Williamson. A theory of learning with corrupted labels. _Journal of Machine Learning Research_, 18(228):1-50, 2018.
* [47] Marin Vlastelica, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers. _CoRR_, abs/1912.02175, 2019.
* [48] Haoyu Peter Wang, Nan Wu, Hang Yang, Cong Hao, and Pan Li. Unsupervised learning for combinatorial optimization with principled objective relaxation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _NeurIPS_, volume 35, pages 31444-31458, 2022.
* [49] Kaifu Wang, Hangfeng He, Tin D. Nguyen, Piyush Kumar, and Dan Roth. On regularization and inference with label constraints. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 35740-35762. PMLR, 23-29 Jul 2023.
* [50] Kaifu Wang, Qiang Ning, and Dan Roth. Learnability with indirect supervision signals. In _NeurIPS_, volume 33, pages 9145-9155, 2020.
* [51] Kaifu Wang, Efi Tsamoura, and Dan Roth. On learning latent models with multi-instance weak supervision, 2023.
* [52] Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli Gao, Bing Tian Dai, and Heng Tao Shen. Template-based math word problem solvers with recursive neural networks. In _AAAI_, pages 7144-7151, 2019.
* [53] Po-Wei Wang, Priya L. Donti, Bryan Wilder, and J. Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In _ICML_, 2019.
* [54] Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. _CoRR_, abs/2106.05731, 2021.

* [55] Zhaofeng Wu. Learning with latent structures in natural language processing: A survey. _arXiv preprint arXiv:2201.00490_, 2022.
* [56] Jingyi Xu, Zulu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In _ICML_, pages 5502-5511, 2018.
* [57] Ning Xu, Congyu Qiao, Xin Geng, and Min-Ling Zhang. Instance-dependent partial label learning. In _NeurIPS_, volume 34, pages 27119-27130, 2021.
* [58] Forest Yang and Sanmi Koyejo. On the consistency of top-k surrogate losses. In _ICML_, volume 119, pages 10727-10735, 2020.
* [59] Zhun Yang, Adam Ishay, and Joohyung Lee. NeurASP: Embracing neural networks into answer set programming. In _IJCAI_, pages 1755-1762, 2020.
* [60] Peilin Yu, Tiffany Ding, and Stephen H. Bach. Learning from multiple noisy partial labelers. In _PMLR_, volume 151, pages 11072-11095, 2022.
* [61] Mingyuan Zhang, Jane Lee, and Shivani Agarwal. Learning from noisy labels with no change to the training process. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 12468-12478, 2021.
* [62] Yivan Zhang, Nontawat Charoenphakdee, Zheng Wu, and Masashi Sugiyama. Learning from aggregate observations. _ArXiv_, abs/2004.06316, 2020.

## Appendix

This appendix is organized as follows:

* In Appendix A, we provide detailed definitions for the loss functions and the complexity measures we use in our work.
* In Appendix B, C and D, we provide the proofs to all formal statements.
* In Appendix F, we provide a comprehensive review of the related work in PLL, latent structural learning, contrained learning, and neuro-symbolic learning.
* In Appendix E, we compare common distributional assumptions in the standard PLL literature with ours. In particular: 1. In Appendix E.1, we provide a transition matrix [8, 46] formulation of multi-instance PLL and show that \(M\)-unambiguity does not imply transition matrix invertibility for multi-instance PLL and vice versa. 2. In Appendix E.2, we show a result of independent interest: that transition matrix invertibility does not imply small ambiguity [28] and vice versa. 3. In Appendix E.3, we show that \(M\)-unambiguity, the unambiguity notion proposed in Definition 1, is an extension of the small ambiguity degree [28], by considering a generalized setting where \(\sigma\) is stochastic.
* In Appendix G, we provide implementation details of our experiments.

## Appendix A Preliminaries

In this section, we introduce in further detail some key notions used in the main body of our paper. We start with the definition of the semantic loss [56].

### Loss functions

_Semantic Loss_ (SL) [56] has been adopted to train classifiers subject to logical theories [30, 43, 24]. SL is the cross entropy of the _weighted model counting_ (WMC) [7] of a formula subject to a softmax output. Below, we provide the necessary notions. Let \(\mathcal{Z}\) denote a set of Boolean variables. An _interpretation_\(I\) of \(\mathcal{Z}\) is a mapping from each \(A\in\mathcal{Z}\) to true (\(\top\)) or false (\(\bot\)). Interpretation \(I\) is a _model_ of a Boolean formula \(\varphi\), if \(\varphi\) evaluates to true in \(I\). We use the notation \(A\in\varphi\) to denote that Boolean variable occurs in \(\varphi\). By treating each Boolean variable \(A\) occurring in \(\varphi\) as an independent Bernoulli random variable that becomes true with probability \(\omega(A)\) and false with probability \(1-\omega(A)\), the different interpretations of \(\mathcal{Z}\) induce a probability distribution, where the probability \(P_{\varphi}(I,\omega)\) of each interpretation \(I\) subject to \(\omega\) and \(\varphi\) is given by:

\[P_{\varphi}(I,\omega):=\begin{cases}\prod_{A\in\varphi|I(A)=\top}\omega(A) \cdot\prod_{A\in\varphi|I(A)=\bot}(1-\omega(A))&I\text{ is a model of }\varphi,\\ 0&\text{otherwise.}\end{cases}\] (11)

\begin{table}
\begin{tabular}{|c c c c c|} \hline \(A_{1,2}\) & \(A_{2,0}\) & \(A_{1,0}\) & \(A_{2,2}\) & \(P_{\varphi}(I,\omega)\) \\ \hline \(\bot\) & \(\bot\) & \(\bot\) & \(\bot\) & \(0\) \\ \(\bot\) & \(\bot\) & \(\bot\) & \(\top\) & \(0\) \\ \(\bot\) & \(\bot\) & \(\top\) & \(\bot\) & \(0\) \\ \(\bot\) & \(\bot\) & \(\top\) & \(\bot\) & \((1-\omega(A_{1,2}))\times(1-\omega(A_{2,0}))\times\omega(A_{1,0})\times \omega(A_{2,2})\) \\ \(\bot\) & \(\top\) & \(\bot\) & \(\bot\) & \(0\) \\ \(\bot\) & \(\top\) & \(\bot\) & \(\top\) & \(0\) \\ \(\bot\) & \(\top\) & \(\top\) & \(\bot\) & \(0\) \\ \(\bot\) & \(\top\) & \(\top\) & \(\top\) & \((1-\omega(A_{1,2}))\times\omega(A_{2,0})\times\omega(A_{1,0})\times\omega(A_ {2,2})\) \\ \(\top\) & \(\bot\) & \(\bot\) & \(\top\) & \(0\) \\ \(\top\) & \(\bot\) & \(\top\) & \(\omega(A_{1,2})\times(1-\omega(A_{2,0}))\times\omega(A_{1,0})\times\omega(A_ {2,2})\) \\ \(\top\) & \(\top\) & \(\bot\) & \(\bot\) & \(\omega(A_{1,2})\times\omega(A_{2,0})\times(1-\omega(A_{1,0}))\times(1- \omega(A_{2,2}))\) \\ \(\top\) & \(\top\) & \(\bot\) & \(\top\) & \(\omega(A_{1,2})\times\omega(A_{2,0})\times(1-\omega(A_{1,0}))\times(1- \omega(A_{2,2}))\) \\ \(\top\) & \(\top\) & \(\top\) & \(\bot\) & \(\omega(A_{1,2})\times\omega(A_{2,0})\times\omega(A_{1,0})\times(1-\omega(A_ {2,2}))\) \\ \(\top\) & \(\top\) & \(\top\) & \(\top\) & \(\omega(A_{1,2})\times\omega(A_{2,0})\times\omega(A_{1,0})\times\omega(A_ {2,2})\) \\ \hline \end{tabular}
\end{table}
Table 3: The \(2^{4}\) interpretations of \(\{A_{1,2},A_{2,0},A_{1,0},A_{2,2}\}\) and their corresponding truth probabilities subject to the \(\omega\) and \(\varphi\) from Example \(9\). Interpretations that are not models of \(\varphi\) have zero probability.

The WMC \(\mathrm{WMC}(\varphi,\omega)\) of \(\varphi\) under \(\omega\) then denotes _the probability of \(\varphi\) being satisfied_, i.e., it evaluates to true, under \(\omega\)5, and is defined as:

Footnote 5: Notice that WMC also applies to the case where \(\omega\) maps each Boolean variable to a non-negative real number. However, when restricting to \((0,1)\), the WMC of a Boolean formula equals to its probability [20].

\[\mathrm{WMC}(\varphi,\omega):=\sum_{\text{Interpretation $I$ of the variables in $\varphi$}}P_{\varphi}(I,\omega)\] (12)

Assuming that each variable in \(\varphi\) is associated with a class from \(\mathcal{Y}\), and treating \(f(x)\) as a mapping from the variables in \(\varphi\) to the \(f\)'s scores on \(x\), the semantic loss of \(\varphi\) under \(f(x)\) is defined as:

\[\mathrm{SL}(\varphi,f(x)):=-\log(\mathrm{WMC}(\varphi,f(x)))\] (13)

We demonstrate the above concepts via Example 4.

**Example 9** (Example 4, cont'd).: _Recall that the Boolean formula induced by the partial label \(s=2\) is \(\varphi=(A_{1,2}\wedge A_{2,0})\vee(A_{1,0}\wedge A_{2,2})\), where \(A_{i,j}\) is a Boolean variable that is true iff the \(i\)-th input digit has label \(j\). Given a probabilistic prediction \(f(\boldsymbol{x})\), variable \(A_{i,y}\) is assigned probability \(f^{y}(x_{i})\), inducing a probability mapping \(\omega:=\bigcup\limits_{i\in\{1,2\},y\in\{0,\ldots,9\}}\{A_{i,y}\mapsto f^{y}( x_{i})\}\). Table 3 shows all interpretations of \(\{A_{1,2},A_{2,0},A_{1,0},A_{2,2}\}\) and their corresponding probabilities subject to \(\omega\) and \(\varphi\). The WMC of \(\varphi\) under \(\omega\) is the sum of the probabilities of all the \(2^{4}\) interpretations._

To establish SL, each \((x_{i},y))\), where \(\boldsymbol{x}=(x_{1},\ldots,x_{M})\), \(i\in[M]\) and \(y\in\mathcal{Y}^{M}\), is associated with a distinct Boolean variable \(A_{i,y}\).

The nature of multi-instance PLL implies Boolean formulas in _disjunctive normal form_ (DNF), where a formula is in DNF form, if it is a disjunction over conjunctions of Boolean variables or their negations, see Example 4. In fact, each Boolean formula computed out of the labels associated with a partial label is a positive DNF formula, i.e., a DNF formula where no variable occurs negatively. Using the union bound of standard probability theory and the model-based semantics of formula probabilities [20], the following result directly follows:

**Lemma 2**.: _Let \(\Phi_{1},\ldots,\Phi_{K}\) be \(K\) positive conjunctive formulas over the set of Boolean variables \(\mathcal{Z}\) and \(\omega:\mathcal{Z}\mapsto(0,1)\). Assuming that each variable \(Z\in\mathcal{Z}\) is treated as an independent Bernoulli random variable that becomes true with probability \(\omega(Z)\) and false with probability \(1-\omega(Z)\), the following holds:_

\[\mathrm{WMC}(\bigvee_{i=1}^{K}\Phi_{i},\omega)\leq\sum_{i=1}^{K}\mathrm{WMC}( \Phi_{i},\omega)\] (14)

_where the equality holds when the \(\Phi_{i}\)'s are logically inconsistent, i.e., there is no interpretation of \(\mathcal{Z}\) in which two or more conjunctions are simultaneously true._

Let \(\Sigma\) denote exclusiveness constraints among the variables occurring in the \(\Phi_{i}\)'s. For instance, returning back to Example 9, to align with the semantics of the classifier, only of the Boolean variables \(A_{1,2}\) and \(A_{1,0}\) can be true in each interpretation6. Then, \(\mathrm{WMC}(\bigvee_{i=1}^{K}\Phi_{i}\wedge\Sigma,\omega)\leq\mathrm{WMC}( \bigvee_{i=1}^{K}\Phi_{i},\omega)\). This is because \(\Sigma\) reduces the number of models of \(\varphi\).

Footnote 6: The above can be captured via the integrity constraint \(\neg(A_{1,2}\wedge A_{1,0})\).

In addition, when the \(\Phi_{i}\)'s share no Boolean variables, the following holds [20]:

\[\mathrm{WMC}(\bigvee_{i=1}^{K}\Phi_{i},\omega)=1-\prod_{i=1}^{K}(1-\mathrm{ WMC}(\Phi_{i},\omega))\] (15)

Due to (15), the top-\(k\) partial loss becomes

\[\ell_{\sigma}^{k}(f(\boldsymbol{x}),s)=-\log(1-\prod_{i=1}^{k}(1-P_{f( \boldsymbol{x})}(\boldsymbol{y}^{(i)})))\] (16)

when the \(\boldsymbol{y}^{(i)}\)'s in Definition 3 share no Boolean variables.

[MISSING_PAGE_FAIL:17]

Proof.: Suppose that the VC dimension of \(\mathcal{H}_{\mathcal{F},\mathcal{G}}\) is \(d\). Let \(N\) be the maximum number of distinct ways to assign labels in \(\mathcal{Y}^{M}\) to \(d\) points in \(\mathcal{X}^{M}\times\mathcal{S}\) using \([\mathcal{F}]\). Each possible label assignment leads to a set of \(d\) elements in \(\mathcal{X}^{M}\times\mathcal{Y}^{M}\times\mathcal{S}\), and for any \(d\) elements, by Sauer-Shelah lemma (see, for example, [40]'s Lemma 6.10), there are at most

\[\left(\frac{\mathrm{e}Md}{d_{\mathcal{G}}}\right)^{d_{\mathcal{G}}}\]

ways for \(\mathcal{G}\) to decide if \(\mathbbm{1}\{\sigma^{\prime}(\boldsymbol{y})\neq s\}\), where \(\mathrm{e}\) is the base of the natural logarithm. Based on the above, we have:

\[2^{d}\leq N\left(\frac{\mathrm{e}Md}{d_{\mathcal{G}}}\right)^{d_{\mathcal{G}}}\]

Therefore, \(N\geq 2^{d}\left(d_{\mathcal{G}}/\mathrm{e}Md\right)^{d_{\mathcal{G}}}\).

By Natarajan's lemma of multiclass classification, we have:

\[N\leq(Md)^{d_{[\mathcal{F}]}}c^{2d_{[\mathcal{F}]}}\] (22)

Combining (22) with the above equations, it follows that

\[(Md)^{d_{[\mathcal{F}]}}c^{2d_{[\mathcal{F}]}}\geq N\geq 2^{d}\left(\frac{d_{ \mathcal{G}}}{\mathrm{e}Md}\right)^{d_{\mathcal{G}}}\]

Taking the logarithm on both sides, we have:

\[d_{[\mathcal{F}]}\log(Md)+2d_{[\mathcal{F}]}\log c\geq d\log 2+d_{\mathcal{G}}( \log d_{\mathcal{G}}-\log(Md)-1)\]

Rearranging the inequality yields:

\[d\log 2+d_{\mathcal{G}}(\log d_{\mathcal{G}}-1) \leq(d_{[\mathcal{F}]}+d_{\mathcal{G}})(\log d+\log M)+2d_{[ \mathcal{F}]}\log c\] \[\leq(d_{[\mathcal{F}]}+d_{\mathcal{G}})\left(\frac{d}{6(d_{[ \mathcal{F}]}+d_{\mathcal{G}})}+\log(6(d_{[\mathcal{F}]}+d_{\mathcal{G}}))-1\right)\] \[\qquad+(d_{[\mathcal{F}]}+d_{\mathcal{G}})\log M+2d_{[\mathcal{F }]}\log c\] \[=d/6+(d_{[\mathcal{F}]}+d_{\mathcal{G}})\left(\log(6M(d_{[ \mathcal{F}]}+d_{\mathcal{G}}))-1\right)+2d_{[\mathcal{F}]}\log c\] \[\leq d/6+(d_{[\mathcal{F}]}+d_{\mathcal{G}})\log(6M(d_{[ \mathcal{F}]}+d_{\mathcal{G}}))-d_{\mathcal{G}}+2d_{[\mathcal{F}]}\log c\]

where the second step follows from the first-order Taylor series expansion of the logarithm function at the point \(6(d_{[\mathcal{F}]}+d_{\mathcal{G}})\). Therefore,

\[d \leq\frac{(d_{[\mathcal{F}]}+d_{\mathcal{G}})\log(6M(d_{[ \mathcal{F}]}+d_{\mathcal{G}}))+2d_{[\mathcal{F}]}\log c-d_{\mathcal{G}}(\log (d_{\mathcal{G}}))}{\log 2-1/6}\] \[\leq 2\left((d_{[\mathcal{F}]}+d_{\mathcal{G}})\log(6M(d_{[ \mathcal{F}]}+d_{\mathcal{G}}))+2d_{[\mathcal{F}]}\log c\right)\]

where the last step follows from the fact that \(\log 2-1/6>1/2\). The above concludes the proof of Lemma 3. 

Notice that if \(\sigma\) is known to the learner (i.e., \(\mathcal{G}\) is a singleton) and hence, \(d_{\mathcal{G}}=0\)7, then (21) becomes

Footnote 7: Singleton hypothesis classes have VC-dimension equal to zero.

\[d\leq 2\left(d_{[\mathcal{F}]}\log(6Md_{[\mathcal{F}]})+2d_{[\mathcal{F}]}\log c\right)\] (23)

### Proofs for Section 3.1

**Lemma 1**.: _If \(\sigma\) is \(M\)-unambigous, then we have:_

\[\mathcal{R}^{01}(f)\leq\mathcal{O}(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)^{1/ M})\quad\text{as}\quad\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)\to 0\] (1)

_Moreover, if \(\sigma\) is not \(M\)-unambiguous, then learning from partial labels is arbitrarily difficult, in the sense that a classifier \(f\) with partial risk \(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)=0\) can have a risk of \(\mathcal{R}^{01}(f)=1\)._Proof.: Given (19), the zero-one risk of \(f\) can be rewritten as

\[\mathcal{R}^{01}(f)=\sum_{l_{i}\neq l_{j}}E_{l_{i},l_{j}}(f)\] (24)

Since \(\sigma\) is \(M\)-unambigous, we know that if all the \(M\) input instances have the same label and are wrongly classified as another label, then the predicted partial label will be wrong. Therefore, the zero-one partial risk is lower bounded by the probability of such events, namely the sum of the same type of classification mistake being repeated by \(M\) times. We have that:

\[\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma) \geq\sum_{l_{i}\neq l_{j}}E_{l_{i},l_{j}}(f)^{M}\] (25) \[\geq\frac{\left(\sum_{l_{i}\neq l_{j}}E_{l_{i},l_{j}}(f)\right)^{ M}}{(c(c-1))^{M-1}}\quad\text{(Power Mean inequality)}\] \[=\frac{\mathcal{R}^{01}(f)^{M}}{(c(c-1))^{M-1}}\]

where \(|\mathcal{Y}|=c\). Rearranging the inequality yields:

\[\mathcal{R}^{01}(f) \leq(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)(c(c-1))^{M-1})^{1/M}\] (26) \[\leq(c^{2M-2}\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M}\] \[=\mathcal{O}((\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M})\]

The above concludes the first part of the proof of Lemma 1.

We now focus on the second part of the proof. If \(\sigma\) is not \(M\)-unambiguous, then from Definition 1, we know that there exists a pair of labels \(y,y^{\prime}\in\mathcal{Y}\) with \(y^{\prime}\neq y\), such that \(\sigma(y,\ldots,y)\neq\sigma(y^{\prime},\ldots,y^{\prime})\) holds. Consider now a sample \((x_{0},y)\) drawn from \(\mathcal{D}\). If \(\mathcal{D}\) concentrates all its mass on \(x_{0}\in\mathcal{X}\), i.e., \(\mathbb{P}_{\mathcal{D}}(x_{0})=1\), and classifier \(f\) misclassifies \(x_{0}\) as \(y^{\prime}\neq y\), then \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)=0\) but \(\mathcal{R}^{01}(f)=1\). The above concludes the proof of Lemma 1. 

**Theorem 6** (ERM learnability under \(M\)-unambiguity).: _Suppose \(\mathcal{F}\) is realizable under \(\ell^{01}_{\mathsf{P}}\) and \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\). Then for any \(\epsilon,\delta\in(0,1)\), there exists a universal8 constant \(C_{0}>0\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widetilde{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}})=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

Footnote 8: A constant is universal if it that does not depend on the parameters of the learning problem (e.g., \(M\) or \(c\)).

\[m_{\mathsf{P}}\geq C_{0}\frac{c^{2M-2}}{\epsilon^{M}}\left(d_{[\mathcal{F}]} \log(6cMd_{[\mathcal{F}]})\log\left(\frac{c^{2M-2}}{\epsilon^{M}}\right)+\log \left(\frac{1}{\delta}\right)\right)\] (27)

Proof.: Let \(f\) be the empirical partial risk minimizer, such that \(\widetilde{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma;\mathcal{T}_{\mathsf{P}})=0\) holds. For any \(\epsilon\in(0,1)\), we know by the proof of Lemma 1 that \(\mathcal{R}^{01}(f)\leq(c^{2M-2}\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M}\) holds. Hence, by bounding \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\leq\epsilon^{M}/c^{2M-2}\), the following would follow: \(\mathcal{R}^{01}(f)\leq\epsilon\). By the standard bound for sample complexity based on VC-dimension (see, for example, Theorem 6.7 in [40]), we know that for any confidence parameter \(\delta\in(0,1)\), \(\mathcal{R}^{01}(f)\leq(c^{2M-2}\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M}\) holds with probability at least \(1-\delta\), if

\[m_{\mathsf{P}}\geq m(\mathcal{H}_{\mathcal{F}},\delta,\epsilon^{M}/c^{2M-2})\] (28)

where

\[m(\mathcal{H}_{\mathcal{F}},\delta,t):=\frac{C}{t}\left(\mathcal{H}_{\mathcal{ F}}\log\left(\frac{1}{t}\right)+\log\left(\frac{1}{\delta}\right)\right)\] (29)

where \(C\) is a universal constant. Combining (23) with (28) and (29) yields the desired result, concluding the proof of Theorem 6. 

Before proving Proposition 1, we will introduce a more general definition of unambiguity:

**Definition 10** (\(I\)-unambiguity).: _Mapping \(\sigma\) is \(I\)-unambiguous if there exists a set of distinct \(I\) indices \(\mathcal{I}=\{i_{1},\ldots,i_{I}\}\subseteq[M]\), such that for any \(\bm{y}\in\mathcal{Y}^{M}\) having the same label \(l\) in all positions in \(\mathcal{I}\), the vector \(\bm{y}^{\prime}\) that results after flipping \(l\) to \(l^{\prime}\neq l\) in those positions satisfies \(\sigma(\bm{y}^{\prime})\neq\sigma(\bm{y})\)._We now prove the following lemma based on \(I\)-unambiguity:

**Lemma 4**.: _If \(\sigma\) is both \(I\)- and \(M\)-unambigious, then we have:_

\[\mathcal{R}^{01}(f)\leq\mathcal{O}((\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1 /I})\] (30)

Proof.: Let \(\mathcal{I}\) be the disambiguation set of \(\sigma\). Then, for each pair of labels \(l_{i},l_{j}\in\mathcal{Y}\) with \(l_{i}\neq l_{j}\), the following holds with probability \(E_{l_{i},l_{j}}(f)^{I}\): the gold labels in \(\mathcal{I}\) are all \(l_{i}\), but they are all misclassified by \(f\) as \(j\). Since \(\sigma\) is \(I\)-unambiguous, this type of error implies a partial label error. Then, we have the following:

\[\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma) \geq\sum_{l_{i}\neq l_{j}}E_{l_{i},l_{j}}(f)^{I}\underbrace{(1- \mathcal{R}^{01}(f))^{M-I}}_{\text{Probability of correctly classifying the remaining $M-I$ labels}}\] (31) \[\geq\frac{\left(\sum_{l_{i}\neq l_{j}}E_{l_{i},l_{j}}(f)\right)^{ I}(1-\mathcal{R}^{01}(f))^{M-I}}{(c(c-1))^{I-1}}\quad\text{(Power-Mean Inequality)}\] \[=\frac{\mathcal{R}^{01}(f)^{I}(1-\mathcal{R}^{01}(f))^{M-I}}{(c( c-1))^{I-1}}\]

Since \(\sigma\) is also \(M\)-unambigous by assumption, we have from (26) that \(\mathcal{R}^{01}(f)\leq c^{2}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M}\). Hence, \(1-\mathcal{R}^{01}(f)\geq 1-c^{2}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M}\). Combining the above with (31), yields:

\[\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\geq\frac{\mathcal{R}^{01}(f)^{I}(1-(c ^{2M-2}\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M})^{M-I}}{(c(c-1))^{I-1}}\]

Therefore,

\[\mathcal{R}^{01}(f)\leq\left(\frac{\mathcal{R}^{01}(f)c^{2I-2}}{(1-(c^{2M-2} \mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))^{1/M})^{M-I}}\right)^{1/I}\]

Based on the above, we conclude that

\[\mathcal{R}^{01}(f)\leq\Phi_{I}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma))\] (32)

where

\[\Phi_{I}:t\mapsto\min\left\{\left(\frac{tc^{2I-2}}{(1-(c^{2M-2}t)^{\frac{1}{M }})^{M-I}}\right)^{1/I},t^{\frac{1}{M}}c^{2}\right\}\] (33)

as \(\mathcal{R}_{\mathsf{P}}(f;\ell^{01}_{\mathsf{P}})\to 0\). We can see that \(\Phi_{I}(t)=\mathcal{O}(t^{1/I})\) as \(t\to 0\). The above concludes the proof of Lemma 4. 

We are now ready to prove Proposition 1.

**Proposition 1** (ERM learnability under 1- and \(M\)-unambiguity).: _If \(\sigma\) is both 1- and \(M\)-unambigous, then we have:_

\[\mathcal{R}^{01}(f)\leq\mathcal{O}(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)) \quad\text{as}\quad\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\to 0\] (3)

_Furthermore, if \(\mathcal{F}\) is realizable under \(\ell^{01}_{\mathsf{P}}\) and \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\), then for any \(\delta\in(0,1)\) and \(\epsilon\in(0,1)\) that is sufficiently close to 0, there exists a universal constant \(C_{1}\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widetilde{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

\[m_{\mathsf{P}}\geq C_{1}\frac{1}{\epsilon}\left(d_{[\mathcal{F}]}\log(6 cM_{[\mathcal{F}]})\log\left(\frac{2}{\epsilon}\right)+\log\left(\frac{1}{ \delta}\right)\right)\] (4)

Proof.: The first claim can be derived from Lemma 4, by setting \(I=1\). For the second claim, using inequality (33) and letting \(I=1\), we know that

\[\mathcal{R}^{01}(f)\leq\frac{t}{(1-(c^{2M-2}t)^{1/M})^{M-1}}\]

Suppose \(\epsilon\) is small enough so that

\[\epsilon\leq\frac{1}{(2M)^{M}c^{2M-2}}\] (34)Now, if we could bound \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\leq\epsilon/2\), we will have that

\[\mathcal{R}^{01}(f) \leq\frac{\epsilon/2}{(1-(c^{2M-2}(\epsilon/2))^{\frac{1}{M}})^{M -1}}\] \[\leq\frac{\epsilon/2}{(1-(c^{2M-2}\epsilon)^{\frac{1}{M}})^{M}}\] \[\leq\frac{\epsilon/2}{1-M(c^{2M-2}\epsilon)^{\frac{1}{M}}}\] (Bernoulli's inequality) \[\leq\epsilon\] (Equation ( 34 ))

Now, our goal is to bound the sample complexity so that \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\leq\epsilon/2\). Similarly to the proof of Theorem 6, by the standard bound for sample complexity based on VC-dimension, for any \(\delta\in(0,1)\), \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\leq\epsilon/2\) holds with probability at least \(1-\delta\), if

\[m_{\mathsf{P}}\geq m(\mathcal{H}_{\mathcal{F},\mathcal{G}},\delta,\epsilon/2)\] (35)

where

\[m(\mathcal{H}_{\mathcal{F},\mathcal{G}},\delta,t):=\frac{C}{t}\left(\text{VC} (\mathcal{H}_{\mathcal{F},\mathcal{G}})\log\left(\frac{1}{t}\right)+\log \left(\frac{1}{\delta}\right)\right)\] (36)

where \(C\) is a universal constant. Combining (23) with (35) and (36) yields the desired result. The above concludes the proof of Proposition 1. 

### Proofs for Section 3.2

#### b.2.1 Relating the top-\(k\) semantic loss to the infimum/minimum loss

In this section, we show that the minimal loss can be viewed as a special case of the top-\(k\) partial loss from Definition 3.

We start by recapitulating the definition of the _minimal loss_[29]. Given a scoring function \(f:\mathcal{X}\rightarrow\mathbb{R}^{M}\), a loss function \(\ell\) and a partial labelset \(s\in 2^{\mathcal{Y}}\), the _minimal loss_[29] for standard PLL [28] is defined as:

\[\min_{y\in s}\ell(f(x),y)\] (37)

Below, we show that the minimal loss coincides with the top-\(1\) partial loss.

For multi-instance PLL, we can extend (37) in a straightforward fashion as follows:

\[\min_{\sigma(\bm{y})=s}\ell(f(\bm{x}),\bm{y})\] (38)

If \(\ell\) is the cross-entropy loss, then the minimal loss finds the most likely label vector in the preimage of \(s\). Now, let us focus on Definition 3. If we set \(k=1\) in \(\ell^{k}_{\sigma}\), then the only model for the formula \(\varphi=\bm{y}^{(1)}:=A_{1,y_{1}^{(1)}}\wedge\cdots\wedge A_{M,y_{M}^{(1)}}\) assigns to each Boolean variable occurring in \(\bm{y}^{(1)}\) value \(\top\). The WMC of \(\varphi\) is given by

\[\prod_{i=1}^{M}f^{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{ }}}}}} { { }}}}}}}}} {{{{{{{{{}}}}}}}}}}}{{{{{{{{{{{{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}(x_{)}}}=\max_{\limits_{ \sigma(\bm{y})=s}}P_{f(\bm{x})}(\bm{y}^{(1)})\] (39)

which is also the maximum likelihood that can be obtained in the preimage of \(s\). This implies that the top-\(1\) semantic loss finds the minimum negative log-likelihood. Therefore, the top-\(k\) partial loss equals to the minimal loss for \(k=1\).

Furthermore, for a partial label \(s\in 2^{\mathcal{Y}}\) and a label prediction \(z\), the _infimum loss_[5] is defined as

\[L(z,s):=\inf_{y\in s}\ell(z,y)\]

Treating the probabilistic prediction \(f(\bm{x})\) as a generalized label, the above definition becomes equivalent to that of the minimal loss (notice that \(\mathcal{Y}\) is finite, so infimum = minimum).

The above discussion shows that the minimal loss is a special case of the top-\(k\) partial loss.

#### b.2.2 Proofs

To prove the main result of this section, we will need the following definitions and lemmas.

**Definition 11** (Top-\(k\) partial \(\ell^{1}\) loss).: _For an integer \(k\geq 1\), the top-\(k\) partial \(\ell^{1}\) loss under scoring function \(f\), input \(\bm{x}\in\mathcal{X}^{M}\) and partial label \(s\in\mathcal{S}\) is given by_

\[\tilde{\ell}^{k}_{\mathsf{P}}(f(\bm{x}),s):=1-\operatorname{WMC}\left(\bigvee \limits_{i=1}^{k}\bm{y}^{(i)},f(\bm{x})\right)\] (40)

_where \(\bm{y}^{(1)},\ldots,\bm{y}^{(k)}\) are as in Definition 3._

We now prove the following lemma:

**Lemma 5**.: _For any classifier \(f\in\mathcal{F}\), \(\bm{x}\in\mathcal{X}^{M}\), \(s\in\mathcal{S}\) and integer \(k\geq 1\), we have:_

\[\ell^{01}_{\mathsf{P}}(f(\bm{x}),s)\leq(k+1)\tilde{\ell}^{k}_{\mathsf{P}}(f( \bm{x}),s)\leq(k+1)\ell^{k}_{\mathsf{P}}(f(\bm{x}),s)\] (41)

Proof.: Let us denote by \(\bm{y}_{f}=([f](x_{1}),\ldots,[f](x_{M}))\) the most likely label assignment to \(\bm{x}\) by \(f\). If \(\ell^{01}_{\mathsf{P}}(\bm{y}_{f},s)=1\) holds, then \(\sigma(\bm{y}_{f})\neq s\). Therefore \(\bm{y}_{f}\) is different from the top-\(k\)\(\bm{y}^{(i)}\) vectors. Hence, the probabilities of the \(\bm{y}^{(i)}\)'s, for \(i\in[k]\), and \(\bm{y}_{f}\) sum to at most 1, i.e.,

\[\begin{split} 1&\geq P_{f(\bm{x})}(\bm{y}_{f})+\sum \limits_{i=1}^{k}P_{f(\bm{x})}(\bm{y}^{(i)})\\ &\geq\left(\frac{1}{k}+1\right)\sum\limits_{i=1}^{k}P_{f(\bm{x}) }(\bm{y}^{(i)})\end{split}\] (42)

The second inequality holds because the assignment \(\bm{y}_{f}\) has a larger score than any other label assignments in the preimage of \(s\). Furthermore, by Lemma 2, we know that

\[\operatorname{WMC}\left(\bigvee\limits_{i=1}^{k}\bm{y}^{(i)},f(\bm{x})\right) \leq\sum\limits_{i=1}^{k}P_{f(\bm{x})}(\bm{y}^{(i)})\leq\frac{k}{k+1}\] (43)

Since the zero-one loss \(\ell^{01}_{\sigma}(\bm{y}_{f},s)\) is by definition either 0 or 1, we have:

\[\tilde{\ell}^{k}_{\mathsf{P}}(f(\bm{x}),s)\geq\frac{1}{k+1}\geq\frac{1}{k+1} \ell^{01}_{\mathsf{P}}(f(\bm{x}),s)\]

Finally, from the inequality \(-\log(1-t)\geq t\)\(\forall t>0\), we know that \(\tilde{\ell}^{k}_{\mathsf{P}}(f(\bm{x}),s)\) lower bounds the cross-entropy top-\(k\) loss:

\[\tilde{\ell}^{k}_{\mathsf{P}}(f(\bm{x}),s)\leq\ell^{k}_{\mathsf{P}}(f(\bm{x}),s)\] (44)

The above concludes the proof of Lemma 5. 

The following two lemmas concern the Lipschitzness of the top-\(k\) loss.

**Lemma 6** (Contraction lemma (Lemma 5 from [10])).: _Let \(N\) and \(m\) be two positive integers. Let also \(\mathcal{H}\) be a set of functions that map \(\mathcal{X}\) to \(\mathbbm{R}^{N}\). Suppose that for each \(i\in[m]\), function \(\Psi_{i}:\mathbbm{R}^{N}\to\mathbbm{R}\) is \(\mu_{i}\)-Lipschitz with the 2-norm, i.e.,_

\[|\Psi_{i}(v^{\prime})-\Psi_{i}(v)|\leq\mu_{i}\|v^{\prime}-v\|_{2}\quad\forall v,v^{\prime}\in\mathbbm{R}^{N}\] (45)

_Then, for any set of \(m\) points \(x_{1},\ldots,x_{m}\in\mathcal{X}\), the following holds:_

\[\frac{1}{m}\mathbbm{E}_{\sigma}\left[\sup_{h\in\mathcal{H}}\sum \limits_{i=1}^{m}\sigma_{i}\Phi_{i}(h(x_{i}))\right]\leq\frac{\sqrt{2}}{m} \mathbbm{E}_{\epsilon}\left[\sup_{h\in\mathcal{H}}\sum\limits_{i=1}^{m}\sum \limits_{j=1}^{N}\epsilon_{ij}\mu_{i}h_{j}(x_{i})\right]\] (46)

_where the \(\sigma_{i}\)'s and the \(\epsilon_{ij}\)'s are independent Rademacher variables uniformly distributed over \(\{-1,+1\}\)._

**Lemma 7** (Lipschitzness).: _For a given scoring function \(f:\mathcal{X}\times\mathcal{Y}\to\mathbbm{R}\) and a vector of instances \(\bm{x}\in\mathcal{X}^{M}\), let \(f(\bm{x})=[f^{y}(x_{i})]_{i\in[M],y\in\mathcal{Y}}\in\mathbbm{R}^{M\times| \mathcal{Y}|}\) denote the vector of scores for each label. Then, for any two scoring functions \(f,f^{\prime}\in\mathcal{F}\) and any sample \((\bm{x},s)\in\mathcal{D}_{\mathsf{P}}\), we have:_

\[|\tilde{\ell}^{k}_{\sigma}(f(\bm{x}),s)-\tilde{\ell}^{k}_{\sigma}(f^{\prime}( \bm{x}),s)|\leq\sqrt{Mk}\|f(\bm{x})-f^{\prime}(\bm{x})\|_{2}\] (47)Proof.: Given \((\bm{x},s)\), the derivative of the WMC with respect to the score \(f^{y}(x_{i})\) for a label \(y\in\mathcal{Y}\) is 0 if \(A_{i,y}\) does not appear in formula \(\varphi\). Otherwise,

\[\frac{\operatorname{WMC}(\bigvee_{i=1}^{k}\bm{y}^{(i)},f(\bm{x}))}{ \operatorname{d}(f^{y}(x_{i}))} =\sum_{\text{Model 1}\text{ of }\varphi}\prod_{A\in\varphi|I(A)= \top}\omega(A)\cdot\prod_{A\in\varphi|I(A)=\bot}(1-\omega(A))\] \[=\sum_{\text{Model 1}\text{ of }\varphi}\mathcal{I}_{I}(A_{i}) \prod_{A\in\varphi_{i,y}|I(A)=\top}\omega(A)\cdot\prod_{A\in\varphi_{i,y}|I(A)= \bot}(1-\omega(A))\] \[\leq\sum_{\text{Model 1}\text{ of }\varphi}\prod_{A\in\varphi_{i,y}|I(A)= \top}\omega(A)\cdot\prod_{A\in\varphi_{i,y}|I(A)=\bot}(1-\omega(A))\] \[=\frac{\operatorname{WMC}(\bigvee_{i=1}^{k}\bm{y}^{(i)},f(\bm{x} ))}{\operatorname{d}(f^{y}(x_{i}))}\] \[\leq 1\]

where

\[\mathcal{I}_{I}(A_{i,y})=\begin{cases}1&I(A_{i,y})=\top\\ -1&\text{otherwise}\end{cases}\] (48)

and \(\varphi_{i,y}\) is the logic formula that deletes all occurrences of \(A_{i,y}\) in \(\varphi\). Therefore, the 2-norm of the gradient of the mapping \(f(\bm{x})\mapsto\widehat{\ell}_{\mathsf{P}}^{k}(f(\bm{x}),s)\) is upper bounded by

\[\sqrt{\text{(number of Boolean variables in }\varphi)}\leq\sqrt{Mk}\] (49)

The boundedness of the gradient implies that the function \(f(\bm{x})\mapsto\operatorname{WMC}(\bigvee_{i=1}^{k}\bm{y}^{(i)},f(\bm{x}))\) is Lipschitz with a Lipschitz constant \(\sqrt{Mk}\), concluding the proof of Lemma 7. 

We are now ready to present the proof of the main theorem of this section.

**Theorem 2** (Error bound under unambiguity).: _Let an integer \(k\geq 1\) and \(\delta\in(0,1)\). If \(\sigma\) is both 1- and \(M\)-unambiguous, then with probability at least \(1-\delta\), we have:_

\[\mathcal{R}^{01}(f)\leq\Phi\left((k+1)\left(\widehat{\mathcal{R}}_{\mathsf{P} }^{k}(f;\sigma;\mathcal{T}_{\mathsf{P}})+2\sqrt{k}M^{3/2}\mathfrak{R}_{Mm_{ \mathsf{P}}}(\mathcal{F})+\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}}\right)\right)\] (7)

_where \(\widehat{\mathcal{R}}_{\mathsf{P}}^{k}(f;\sigma;\mathcal{T}_{\mathsf{P}})= \sum_{(\bm{x},s)\in\mathcal{T}_{\mathsf{P}}}\ell_{\sigma}^{k}(f(\bm{x}),s)/m_ {\mathsf{P}}\) is the empirical counterpart of (6) and \(\Phi\) is an increasing function that satisfies \(\lim_{t\to 0}\Phi(t)/t=1\)._

Proof.: _Bound with the partial risk._ From the proof of Lemma 4 and \(I=1\), we know that

\[\mathcal{R}^{01}(f)\leq\Phi(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma))\]

where

\[\Phi:t\mapsto\min\left\{\frac{t}{(1-(c^{2M-2}t)^{1/M})^{M-1}},t^{\frac{1}{M}} c^{2}\right\}\] (50)

We can see that \(\Phi\) is monotone increasing and \(\lim_{t\to 0}\Phi(t)/t=1\).

_Bound the zero-one partial risk with the top-\(k\) loss._ From Lemma 5, we know that \(\ell_{\mathsf{P}}^{01}(f(\bm{x}),s)\leq\overline{(k+1)\ell_{\mathsf{P}}^{k}(f( \bm{x}),s)}\). Taking expectation yields:

\[\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)\leq(k+1)\mathcal{R}_{\mathsf{P}}^{k}(f;\sigma)\]

_Bound with the empirical risk._ Given a partially labelled dataset \(\mathcal{T}_{\mathsf{P}}=\{(\bm{x}_{i},s_{i})\}_{i=1}^{m_{\mathsf{P}}}\), by the standard Rademacher complexity bounds, we know that for each \(\delta\in(0,1)\) and each \(f\in\mathcal{F}\), the following inequality holds with probability at least \(1-\delta\):

\[\mathcal{R}_{\mathsf{P}}^{k}(f;\sigma)\leq\widehat{\mathcal{R}}_{\mathsf{P}}(f ;\widehat{\ell}_{\mathsf{P}}^{k};\mathcal{T}_{\mathsf{P}})+2\mathfrak{R}_{m_{ \mathsf{P}}}(\mathcal{A}_{\mathcal{F},k})+\sqrt{\frac{\log(1/\delta)}{2m_{ \mathsf{P}}}}\] (51)where \(\mathcal{A}_{\mathcal{F},k}\) is a class of functions that take as inputs elements of the form \((\bm{x},s)\) with \(\bm{x}\in\mathcal{X}^{M}\) and \(s\in\mathcal{S}\), and is defined as follows:

\[\mathcal{A}_{\mathcal{F},k}:=\left\{(\bm{x},s)\mapsto\tilde{\ell}_{\mathsf{p}}^ {k}(f(\bm{x}),s):f\in\mathcal{F}\right\}\] (52)

By Lemma 6 with \(\Psi_{i}:f(\bm{x}_{i})\to\tilde{\ell}_{\mathsf{p}}^{k}(f(\bm{x}_{i}),s_{i})\), and Lipschitzness (Lemma 7) we have:

\[\begin{split}\mathfrak{R}_{m}(\mathcal{A}_{\mathcal{F},k})& =\frac{1}{m_{\mathsf{p}}}\mathbb{E}_{\mathcal{T}_{\mathsf{p}}} \mathbb{E}_{\sigma}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{m_{\mathsf{p}}} \sigma_{i}\tilde{\ell}_{\mathsf{p}}^{k}(f(\bm{x}_{i}),s_{i})\right]\\ &=\sqrt{kM}\frac{1}{m_{\mathsf{p}}}\mathbb{E}_{\mathcal{T}} \mathbb{E}_{\epsilon}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{m_{\mathsf{p}}} \sum_{j=1}^{M}\sum_{y\in\mathcal{Y}}\epsilon_{ijy}f^{y}(x_{ij})\right]\\ &=\sqrt{kM}M\underbrace{\frac{1}{Mm_{\mathsf{p}}}\mathbb{E}_{ \mathcal{T}}\mathbb{E}_{\epsilon}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{m_{ \mathsf{p}}}\sum_{j=1}^{M}\sum_{y\in\mathcal{Y}}\epsilon_{ijy}f^{y}(x_{ij}) \right]}_{\text{Rademacher complexity with $M\times m_{\mathsf{p}}$ instances}}\\ &=\sqrt{k}M^{3/2}\mathfrak{R}_{Mm_{\mathsf{p}}}(\mathcal{F})\end{split}\] (53)

_Bound with cross-entropy._ Finally, from the inequality \(-\log(1-t)\geq t\ \forall t>0\), we know that

\[\widehat{\mathcal{R}}_{\mathsf{p}}(f;\tilde{\ell}_{\mathsf{p}}^{k};\mathcal{T }_{\mathsf{p}})\leq\widehat{\mathcal{R}}_{\mathsf{p}}(f;\ell_{\mathsf{p}}^{k} ;\mathcal{T}_{\mathsf{p}})\] (54)

Putting the above inequalities together yields the proof of Theorem 2. 

## Appendix C Proofs for Section 4

### On the bounded risk assumption

In Section 4, we introduced the bounded risk assumption. Recall that the assumption requires a constant \(R<1\), such that for each \(i\in[n]\) and each \(f\in\mathcal{F}_{i}\), \(\mathcal{R}^{01}(f)\leq R\) holds. Below, we discuss how this assumption is achieved with a small amount of directly labeled data \(\mathcal{T}_{\mathsf{L}}=\left\{(x_{i},y_{i})\right\}_{i=1}^{m_{\mathsf{L}}}\) i.i.d. drawn from \(\mathcal{D}\).

Given \(\mathcal{T}_{\mathsf{L}}\) and a trade-off parameter \(\lambda>1\), consider the following combined learning objective:

\[\widehat{\mathcal{L}}(f;\mathcal{T}_{\mathsf{P}},\mathcal{T}_{\mathsf{L}}):= \frac{1}{m_{\mathsf{p}}}\sum_{(\bm{x},s)\in\mathcal{T}_{\mathsf{P}}}\ell_{ \sigma}^{01}(f(\bm{x}),s)+\frac{\lambda}{m_{\mathsf{L}}}\sum_{(x,y)\in \mathcal{T}_{\mathsf{L}}}\ell^{01}([f](x),y)\]

Suppose that there is a classifier \(f_{0}\in\mathcal{F}\) that can achieve a small empirical classification risk \(r_{0}\) given by \(\frac{1}{m_{\mathsf{L}}}\sum_{(x,y)\in\mathcal{T}_{\mathsf{L}}}\ell^{01}([f_{ 0}](x),y)\)9. Then, any classifier \(f_{1}\) with an empirical classification risk greater than \(r_{0}+1/\lambda\) will be rejected by this learning objective. This is because

Footnote 9: Under the realizable assumption, we have that \(r_{0}=0\).

\[\widehat{\mathcal{L}}(f_{1};\mathcal{T}_{\mathsf{P}},\mathcal{T}_{\mathsf{L}}) \geq\lambda\left(r_{0}+\frac{1}{\lambda}\right)=\lambda r_{0}+1\geq\widehat{ \mathcal{L}}(f_{0};\mathcal{T}_{\mathsf{P}},\mathcal{T}_{\mathsf{L}})\]

Therefore, all the possibly learned classifiers must have an empirical risk less than \(r_{0}+1/\lambda\). Furthermore, under standard learning theory assumptions (e.g., finite VC dimension or vanishing Rademacher complexity), for any \(\delta_{0}\in(0,1)\) we can typically bound the expected classification risk with probability at least \(1-\delta_{0}\) by

\[r_{0}+1/\lambda+B_{1}\sqrt{\frac{B_{2}+\log(1/\delta_{0})}{m_{\mathsf{L}}}}\] (55)

where \(B_{1},B_{2}\) is a constant depending on the learning problem. As long as we can have \(m_{\mathsf{L}}\) that is large enough to make (55) less than 1 (i.e., _non-vacuous_), we can guarantee that our assumption holds with probability at least \(1-\delta_{0}\). The above justified the bounded risk assumption.

### Proofs

Below, we introduce the analog of Lemma 3 for the multi-classifier case. Let

\[\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n}}:=\{(\bm{x},s)\mapsto \mathbbm{1}\{\sigma([\bm{f}](\bm{x}))\notin s\}|\bm{f}\in\mathcal{F}_{1}\times \cdots\times\mathcal{F}_{n}\}\]

Given a training sample \((\bm{x},s)\), a binary classifier from \(\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n}}\) predicts whether the labels predictions for \(\bm{x}\) are consistent with the partial label \(s\). Suppose \((\bm{x},s)\) are drawn from \(\mathcal{D}_{\mathsf{P}}\). Then, \(0\) is always the gold label of this binary classification problem. The zero-one binary classification error of \(\{(\bm{x},s)\mapsto\mathbbm{1}\{\sigma([\bm{f}](\bm{x}))\notin s\}\) equals the zero-one partial loss defined as \(\ell_{\sigma}^{01}(\bm{f}(\bm{x}),s)\).

**Lemma 8**.: _Let \(\operatorname{Nat}([\mathcal{F}_{i}])=d_{[\mathcal{F}_{i}]}<\infty\) be the Natarajan dimension of each \([\mathcal{F}_{i}]\). Then, we can bound the VC dimension of the function class \(\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n}}\) as_

\[d\leq 4\sum_{i=1}^{n}\left(d_{[\mathcal{F}_{i}]}\log(M_{i}\cdot n\cdot d_{[ \mathcal{F}_{i}]})+2d_{[\mathcal{F}_{i}]}\log c_{i}\right)\] (56)

Proof.: By Natarajan's lemma of multiclass classification, we have:

\[2^{d}\leq\prod_{i=1}^{n}(M_{i}d)^{d_{[\mathcal{F}_{i}]}}c_{i}^{2d_{[ \mathcal{F}_{i}]}}\] (57)

where \(c_{i}=|\mathcal{Y}_{i}|\). Taking the logarithm on both sides, we have

\[d\log 2\leq\sum_{i=1}^{n}d_{[\mathcal{F}_{i}]}\log(M_{i}d)+2d_{[ \mathcal{F}_{i}]}\log c_{i}\]

Rearranging the inequality yields:

\[d\log 2\leq \sum_{i=1}^{n}\left(d_{[\mathcal{F}_{i}]}\left(\log(nM_{i}d_{[ \mathcal{F}_{i}]})+\frac{d}{ned_{[\mathcal{F}_{i}]}}\right)+2d_{[\mathcal{F}_ {i}]}\log c_{i}\right)\] \[= \sum_{i=1}^{n}\left(d_{[\mathcal{F}_{i}]}\log(nM_{i}d_{[ \mathcal{F}_{i}]})+2d_{[\mathcal{F}_{i}]}\log c_{i}\right)+\frac{d}{\text{e}}\]

where the second step follows from the first-order Taylor series expansion of the logarithm function \(\log(M_{i}t)\) at the point \(t=ned_{[\mathcal{F}_{i}]}\). Using the fact that \((\log 2-1/\text{e})^{-1}<4\), yields (56), concluding the proof of Lemma 8. 

**Theorem 3** (ERM learnability under multi-unambiguity).: _Assume that there is a constant \(R<1\), such that for each \(i\in[n]\), each \(f\in\mathcal{F}_{i}\) is zero-one risk \(R\)-bounded. Assume also that there exist positive integers \(M^{*}\) and \(c^{*}\), such that \(M_{i}\leq M^{*}\) and \(c_{i}\leq c_{0}\) hold for any \(i\in[n]\). Then, if \(\sigma\) is multi-unambiguous, we have:_

\[\mathcal{R}^{01}(\bm{f})\leq\mathcal{O}((\mathcal{R}_{\mathsf{P}}^{01}(\bm{f} ;\sigma))^{1/M^{*}})\quad\text{as}\quad\mathcal{R}_{\mathsf{P}}^{01}(\bm{f}; \sigma)\to 0\] (8)

_Furthermore, for any \(\epsilon,\delta\in(0,1)\), there is a universal constant \(C_{3}\), such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widehat{\mathcal{R}}_{\mathsf{P}}^{01}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\) if_

\[m_{\mathsf{P}}\geq C_{3}\frac{nc_{0}^{2M^{*}-2}}{\epsilon^{M^{*}}(1-R)^{M}} \left(\sum_{i=1}^{n}d_{[\mathcal{F}_{i}]}\log(nc_{i}M_{i}d_{[\mathcal{F}_{i}] })\log\left(\frac{nc_{0}^{2M^{*}-2}}{\epsilon^{M^{*}}(1-R)^{M}}\right)+\log \left(\frac{1}{\delta}\right)\right)\] (9)Proof.: Recall that \(c_{i}=|\mathcal{Y}_{i}|\). By the multi-unambiguity assumption, we can lower bound the partial label risk as

\[\mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma) \geq\sum_{i=1}^{n}\underbrace{\frac{(\mathcal{R}^{01}(f_{i}))^{M_{ i}}}{(c_{i}(c_{i}-1))^{M_{i}-1}}\prod_{j\neq i}(1-\mathcal{R}^{01}(f_{j}))^{M_{j}}}_{ \text{Probability that each }x\in\bm{x}_{i}\text{ is misclassified but each other prediction is correct.}\] (58) \[\geq\sum_{i=1}^{n}\frac{(\mathcal{R}^{01}(f_{i}))^{M_{i}}}{(c_{i} (c_{i}-1))^{M^{*}-1}}\prod_{j\neq i}(1-R)^{M_{j}}\] \[=\sum_{i=1}^{n}\frac{(\mathcal{R}^{01}(f_{i}))^{M_{i}}}{(c_{i}(c_ {i}-1))^{M^{*}-1}}(1-R)^{M-M_{i}}\] \[\geq\sum_{i=1}^{n}\frac{(\mathcal{R}^{01}(f_{i}))^{M^{*}}}{(c_{0 }(c_{0}-1))^{M^{*}-1}}(1-R)^{M-M_{*}}\] \[\geq\frac{(1-R)^{M-M_{*}}}{(nc_{0}(c_{0}-1))^{M^{*}-1}}\left( \sum_{i=1}^{n}\mathcal{R}^{01}(f_{i})\right)^{M^{*}}\]

Therefore, we have:

\[\mathcal{R}^{01}(\bm{f}) \leq\left(\frac{(nc_{0}(c_{0}-1))^{M^{*}-1}}{(1-R)^{M-M_{*}}} \mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma)\right)^{1/M^{*}}\] (59) \[\leq\left(\frac{(nc^{2})^{M^{*}-1}}{(1-R)^{M-M_{*}}}\mathcal{R}^ {01}_{\mathsf{P}}(\bm{f};\sigma)\right)^{1/M^{*}}\] \[=\mathcal{O}((\mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma))^{1/M^ {*}})\]

The above concludes the first part of Theorem 3.

To show the second part, from (59), we know that if the following holds:

\[\mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma)\leq\frac{\epsilon^{M^{*}}(1-R)^ {M-M_{*}}}{(nc_{0}^{2})^{M^{*}-1}}\] (60)

then, \(\mathcal{R}^{01}(\bm{f})\leq\epsilon\) holds. By the standard bound for sample complexity based on VC-dimension, we know that for any confidence parameter \(\delta\in(0,1)\), inequality (60) holds with probability no less than \(1-\delta\), if

\[m_{\mathsf{P}}\geq m\left(\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n} },\delta,\frac{\epsilon^{M^{*}}(1-R)^{M-M_{*}}}{(nc_{0}^{2})^{M^{*}-1}}\right)\] (61)

where

\[m(\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n}},\delta,t):=\frac{C}{t} \left(\text{VC}(\mathcal{H}_{\mathcal{F}_{1},\ldots,\mathcal{F}_{n}})\log \left(\frac{1}{t}\right)+\log\left(\frac{1}{\delta}\right)\right)\] (62)

and \(C\) is a universal constant. Combining (56) with (61) and (62), concludes the second part of Theorem 3. 

**Theorem 4** (Error bound under multi-unambiguity with multiple classifiers).: _Suppose \(\sigma\) is multi-unambiguous and each \(f_{i}\) is zero-one risk \(R\)-bounded, for \(R\in(0,1)\). Then, for any integer \(k\geq 1\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have:_

\[\mathcal{R}^{01}(\bm{f})\leq\left(\frac{nc_{0}^{2M^{*}-2}(k+1)}{(1-R)^{M}} \left(\widehat{\mathcal{R}}^{k}_{\mathsf{P}}(\bm{f};\sigma;\mathcal{T}_{ \mathsf{P}})+\sqrt{kM}\sum_{i=1}^{n}M_{i}\mathfrak{R}_{mpM_{i}}(\mathcal{F}_{i })+\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}}\right)\right)^{1/M^{*}}\]

Proof.: Let

\[\bm{f}(\bm{x})=\left[f_{i}^{y}(x_{ij})\right]_{i\in[n],j\in[M_{i}],y\in \mathcal{Y}_{i}}\]

be the vector that encodes all the probabilities that \(\bm{f}\) assign to a pair \((x,y)\) where \(x\) is from \(\bm{x}\) and \(y\) is a possible label for \(x\). Similarly to the single-variable case, we define

\[\tilde{\ell}^{k}_{\mathsf{P}}(\bm{f}(\bm{x}),s):=1-\text{WMC}\left(\bigvee_{i= 1}^{k}\bm{y}^{(i)},\bm{f}(\bm{x})\right)\] (63)Bound with the partial risk._ From the proof of Theorem 3, we know that

\[\mathcal{R}^{01}(\bm{f})\leq\left(\frac{(nc^{2})^{M^{*}-1}}{(1-R)^{M-M_{*}}} \mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma)\right)^{1/M^{*}}\]

Bound with the top-\(k\) loss._ Similarly to the single-classifier case, it can be shown that

\[\mathcal{R}^{01}_{\mathsf{P}}(\bm{f};\sigma)\leq(k+1)\mathcal{R}_{\mathsf{P}} (\bm{f};\widetilde{\ell}^{k}_{\mathsf{P}})\]

Bound with the empirical risk._ By standard Rademacher complexity bounds, given a partially labeled dataset \(\mathcal{T}_{\mathsf{P}}\) of size \(m_{\mathsf{P}}\), for each \(\delta\in(0,1)\) and each \(f\in\mathcal{F}\), the following holds with probability at least \(1-\delta\):

\[\mathcal{R}^{k}_{\mathsf{P}}(\bm{f};\sigma)\leq\widehat{\mathcal{R}}_{\mathsf{ P}}(\bm{f};\widetilde{\ell}^{k}_{\mathsf{P}})+2\mathfrak{R}_{m_{\mathsf{P}}}( \mathcal{H}_{\mathcal{F}})+\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}}\] (64)

where \(\mathcal{H}_{\mathcal{F},k}\) is a class of functions that take as input elements of the form \((\bm{x},s)\), with \(\bm{x}\in\mathcal{X}^{M}\) and \(s\,\in\mathcal{S}\), and is defined as follows:

\[\mathcal{H}_{\mathcal{F},k}:=\left\{(\bm{x},s)\mapsto\widetilde{\ell}^{k}_{ \mathsf{P}}(\bm{f}(\bm{x}),s):f\in\mathcal{F}\right\}\] (65)

Denote \(\bm{\mathcal{F}}=\prod_{i=1}^{M}\mathcal{F}_{i}\). Using exactly the same argument in Lemma 7, we can show the mapping \(\bm{f}(\bm{x})\mapsto\widetilde{\ell}^{k}_{\mathsf{P}}(\bm{f}(\bm{x}),s)\) is Lipschitz with Lipschitz constant \(\sqrt{kM}\), By the contraction lemma (Lemma 6), we have that

\[\mathfrak{R}_{m}(\mathcal{H}_{\mathcal{F},k}) =\frac{1}{m_{\mathsf{P}}}\mathbb{E}_{\mathcal{T}_{\mathsf{P}}} \mathbb{E}_{\sigma}\left[\sup_{\bm{f}\in\mathcal{F}}\sum_{i=1}^{m_{\mathsf{P} }}\sigma_{i}\widetilde{\ell}^{k}_{\mathsf{P}}(\bm{f}(\bm{f}),s)\right]\] (66) \[=\sqrt{kM}\frac{1}{m_{\mathsf{P}}}\mathbb{E}_{\mathcal{T}} \mathbb{E}_{\epsilon}\left[\sup_{\bm{f}\in\bm{\mathcal{F}}}\sum_{i=1}^{m_{ \mathsf{P}}}\sum_{j=1}^{n}\sum_{y\in\mathcal{Y}_{i}}\epsilon_{ily}f^{y}_{l}(x_{ lj})\right]\] \[\leq\sqrt{kM}\sum_{l=1}^{n}M_{l}\frac{1}{m_{\mathsf{P}}M_{l}} \mathbb{E}_{\mathcal{T}}\mathbb{E}_{\epsilon}\left[\sup_{\bm{f}\in\bm{ \mathcal{F}}}\sum_{i=1}^{m_{\mathsf{P}}}\sum_{j=1}^{M_{l}}\sum_{y\in\mathcal{Y }_{i}}\epsilon_{ily}f^{y}_{l}(x_{lj})\right]\] \[=\sqrt{kM}\sum_{l=1}^{n}M_{l}\mathfrak{R}_{m_{\mathsf{P}}M_{l}}( \mathcal{F}_{l})\] \[=\sqrt{kM}\sum_{i=1}^{n}M_{i}\mathfrak{R}_{m_{\mathsf{P}}M_{i}}( \mathcal{F}_{i})\]

The above implies

\[\mathcal{R}_{\mathsf{P}}(\bm{f};\ell^{k}_{\mathsf{P},1})\leq\widehat{\mathcal{ R}}_{\mathsf{P}}(\bm{f};\widetilde{\ell}^{k}_{\mathsf{P}};\mathcal{T}_{\mathsf{P}})+2 \sqrt{kM}\sum_{i=1}^{n}M_{i}\mathfrak{R}_{m_{\mathsf{P}}M_{i}}(\mathcal{F}_{i}) +\sqrt{\frac{\log(1/\delta)}{2m_{\mathsf{P}}}}\] (67)

Bound with cross-entropy._ Finally, from the inequality \(-\log(1-t)\geq t\;\forall t>0\), we know that

\[\widehat{\mathcal{R}}_{\mathsf{P}}(\bm{f};\widetilde{\ell}^{k}_{\mathsf{P}}; \mathcal{T}_{\mathsf{P}})\leq\widehat{\mathcal{R}}_{\mathsf{P}}(\bm{f};\ell^{k }_{\mathsf{P}};\mathcal{T}_{\mathsf{P}})\] (68)

Putting the above inequalities together, the proof of Theorem 4 follows. 

## Appendix D Proofs for Section 5

**Theorem 5**.: _If \(\mathcal{G}\) is unambigous and any \(f\in\mathcal{F}\) is \(r\)-bounded, then we have:_

\[\mathcal{R}^{01}(f)\leq O(\mathcal{R}^{01}_{\mathsf{P}}(f;\mathcal{G})^{1/M}) \quad\text{as}\quad\mathcal{R}^{01}_{\mathsf{P}}(f;\mathcal{G})\to 0\] (10)

_Furthermore, suppose \([\mathcal{F}]\) has a finite Natarajan dimension \(d_{[\mathcal{F}]}\) and the function class \(\{(\bm{y},s)\mapsto\mathbbm{1}\{\sigma^{\prime}(\bm{y})\neq s\}|\sigma^{ \prime}\in\mathcal{G}\}\) has a finite VC-dimension \(d_{\mathcal{G}}\). Then, for any \(\epsilon,\delta\in(0,1)\), there is a universal constant \(C_{4}\) such that with probability at least \(1-\delta\), the empirical partial risk minimizer with \(\widehat{\mathcal{R}}^{01}_{\mathsf{P}}(f;\sigma)=0\) has a classification risk \(\mathcal{R}^{01}(f)<\epsilon\), if_

\[m_{\mathsf{P}}\geq C_{4}\frac{c^{2M-2}}{r^{M}\epsilon^{M}}\left(\left((d_{[ \mathcal{F}]}+d_{\mathcal{G}})\log(6M(d_{[\mathcal{F}]}+d_{\mathcal{G}}))+d_{[ \mathcal{F}]}\log c\right)\log\left(\frac{c^{2M-2}}{r^{M}\epsilon^{M}}\right)+ \log\left(\frac{1}{\delta}\right)\right)\]Proof.: Fix a classifier \(f\in\mathcal{F}\). Since \(\mathcal{G}\) is unambiguous, it follows that for each pair of labels \((l_{i},l_{j})\) such that \(E_{l_{i},l_{j}}(f)>0\) and each \(\sigma^{\prime}\in\mathcal{G}\), there exists a vector \(\boldsymbol{y}\in\{l_{i},l_{j}\}^{M}\), such that \(\sigma^{\prime}(\boldsymbol{y})\neq\sigma(l_{i},l_{i},\ldots,l_{i})\) holds. Therefore, we have:

\[\begin{split}\min_{\sigma^{\prime}\in\mathcal{G}}\mathcal{R}_{ \mathsf{P}}^{01}(f;\sigma^{\prime})&\geq\sum_{i\neq j}\min_{0 \leq k\leq M}E_{l_{i},l_{i}}(f)^{k}E_{l_{i},l_{j}}(f)^{M-k}\\ &\geq\sum_{l_{i}\neq l_{j}}r^{M}E_{l_{i},l_{j}}^{M}(f)\\ &\geq\frac{r^{M}}{c^{2M-2}}\left(\sum_{l_{i}\neq l_{j}}E_{l_{i}, l_{j}}(f)\right)^{M}\\ &=\frac{r^{M}}{c^{2M-2}}\mathcal{R}^{01}(f)^{M}\end{split}\] (69)

The above implies that \(\mathcal{R}^{01}(f)\leq O(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)^{1/M})\) holds.

To prove the second part of Theorem 5, from the above discussion we know that \(\mathcal{R}^{01}(f)\leq(c^{2M-2}\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)/r^{M} )^{1/M}\). Therefore, if we could control \(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)\leq\epsilon^{M}r^{M}/c^{2M-2}\), we would have that \(\mathcal{R}^{01}(f)\leq\epsilon\). By the standard bound for sample complexity based on VC-dimension, then for any \(\delta\in(0,1)\), \(\mathcal{R}_{\mathsf{P}}^{01}(f;\sigma)\leq\epsilon^{M}r^{M}/c^{2M-2}\) can happen with probability at least \(1-\delta\) if

\[m_{\mathsf{P}}\geq m(\mathcal{H}_{\mathcal{F},\mathcal{G}},\delta,\epsilon^{M }r^{M}/c^{2M-2})\] (70)

where

\[m(\mathcal{H}_{\mathcal{F},\mathcal{G}},\delta,t)=\frac{C}{t}\left(\text{VC}( \mathcal{H}_{\mathcal{F},\mathcal{G}})\log\left(\frac{1}{t}\right)+\log\left( \frac{1}{\delta}\right)\right)\] (71)

and \(C\) is a universal constant. Now, recall (21) from Lemma 3. Combining (21) with (70) and (71) yields the proof of the second part of Theorem 5. 

## Appendix E Results for standard PLL

### Counterexamples with transition matrices

**Definition 12** (Transition matrix [8, 46]).: _A transition matrix \(\mathbf{T}\) for a learning problem with hidden label \(Y\in\mathcal{Y}\) and observation \(S\in\mathcal{S}\) is a stochastic matrix of dimension \(|\mathcal{Y}|\times|\mathcal{S}|\), where the element in its \(i^{th}\) column and \(j^{th}\) row is the conditional probability \(\mathbb{P}\left(S=j|Y=i\right)\). Transition matrix \(\mathbf{T}\) is invertible if it is left invertible, i.e., there is a matrix \(\mathbf{P}\) of dimension \(|\mathcal{Y}|\times|\mathcal{S}|\) such that \(\mathbf{PT}=\mathbf{I}_{|\mathcal{Y}|}\)._

As shown in [8, 46], if the transition is invertible, then it is possible to construct an unbiased estimator for the classification loss using the samples of the partial label \(s\). Therefore, the invertibility of the transition is desirable since it implies one can minimize the classification risk with a partially labeled dataset alone.

**A transition matrix formulation for multi-instance PLL.** Consider the multi-instance PLL with a single classifier and a deterministic transition \(\sigma\). Consider a naive formulation of the transition matrix where we view \(\mathcal{Y}^{M}\) as the label space. Then, if there are two different label assignments \(\boldsymbol{y}\neq\boldsymbol{y}^{\prime}\) such that \(\sigma(\boldsymbol{y})\neq\sigma(\boldsymbol{y}^{\prime})\), their corresponding column vectors in \(\mathbf{T}\) will be the same, resulting in a non-invertible transition. For example, in the MNIST with SUM2 problem, the column vectors corresponding to the label pairs \((0,1)\) and \((1,0)\) will be:

\[\begin{array}{c}\boldsymbol{y}=(1,2)\hskip 14.226378pt\boldsymbol{y}=(2,1)\\ s=0\\ s=1\\ s=2\\ \vdots\\ s=18\end{array}\left[\begin{array}{ccc}0&0\\ 1&1\\ 0&0\\ \vdots&\vdots\\ 0&0\end{array}\right]\] (72)

This fact implies that the transition \(\sigma\) must be injective to ensure invertibility, which is too strong for the partial labels in practice.

To overcome this issue, we consider an alternative formulation, where we view \(s\) as a randomized partial label for each of the \(M\) instances, where the randomness comes from the distribution of the other instances. Suppose the marginal distribution of \(Y\) is known (or can be estimated in some ways). Then, we construct a transition matrix \(\mathbf{T}_{k}\) for each \(k\in[M]\) by defining its \(i^{\text{th}}\) column and \(j^{\text{th}}\) row to be \(\mathbb{P}(s=j|Y_{i}=i)=\mathbb{P}(\bm{y}|\sigma(\bm{y})=j,y_{k}=i)\). For example, the transition matrices for the MNIST with 2sum problem are

\[\begin{array}{ccccc}&0&1&\cdots&9\\ &s=0&\begin{bmatrix}\mathbb{P}(Y=0)&0&\cdots&0\\ \mathbb{P}(Y=1)&\mathbb{P}(Y=0)&\cdots&0\\ s=2&\mathbb{P}(Y=2)&\mathbb{P}(Y=1)&\cdots&0\\ &\vdots&\vdots&\ddots&\vdots\\ &s=9&\mathbb{P}(Y=9)&\mathbb{P}(Y=8)&\cdots&\mathbb{P}(Y=0)\\ s=10&0&\mathbb{P}(Y=9)&\cdots&\mathbb{P}(Y=1)\\ s=11&0&0&\cdots&\mathbb{P}(Y=2)\\ &\vdots&\vdots&\ddots&\vdots\\ &s=18&0&0&\cdots&\mathbb{P}(Y=9)\end{bmatrix}\end{array}\] (73)

We say the standard PLL problem is _invertible_ if there exists an \(k\in[M]\) such that \(\mathbf{T}_{k}\) is left invertible.

\(M\)-unambiguity \(\not\Rightarrow\) invertibility for multi-instance PLL.Consider the following example, where \(M\)-unambiguity holds but the transition matrix is not invertible for no input position. Consider \(\mathcal{Y}=[3]\) and \(M=3\). Assume \(\sigma(1,1,1)\neq\sigma(2,2,2)\neq\sigma(3,3,3)\) so that \(\sigma\) is \(M\)-unambiguous. Now, suppose

\[\sigma(1,1,1)=\sigma(1,2,3)=\sigma(1,3,2)\] \[=\sigma(2,1,2)=\sigma(2,2,1)=\sigma(2,3,3)\] (74) \[=\sigma(3,1,3)=\sigma(3,2,2)=\sigma(3,3,1)\]

and

\[\sigma(1,1,2)=\sigma(1,2,1)=\sigma(1,3,3)\] \[=\sigma(2,1,3)=\sigma(2,2,2)=\sigma(2,3,1)\] (75) \[=\sigma(3,1,1)=\sigma(3,2,3)=\sigma(3,3,2)\]

and

\[\sigma(1,1,3)=\sigma(1,2,2)=\sigma(1,3,1)\] \[=\sigma(2,1,1)=\sigma(2,2,3)=\sigma(2,3,2)\] (76) \[=\sigma(3,1,2)=\sigma(3,2,1)=\sigma(3,3,3)\]

so that \(\mathcal{S}=\{\sigma(1,1,1),\sigma(2,2,2),\sigma(3,3,3)\}\). Suppose that \(\mathbb{P}(Y=1)=\mathbb{P}(Y=2)=\mathbb{P}(Y=3)=1/3\). Then, it can be verified that

\[\mathbf{T}_{1}=\mathbf{T}_{2}=\mathbf{T}_{3}=\begin{array}{c}\sigma(1,1,1) \\ \sigma(2,2,2)\\ \sigma(3,3,3)\end{array}\begin{bmatrix}1/3&1/3&1/3\\ 1/3&1/3&1/3\\ 1/3&1/3&1/3\end{bmatrix}\] (77)

This means that all the transition matrices are non-invertible.

Invertibility \(\not\Rightarrow\)\(M\)-unambiguity for multi-instance PLL.Suppose \(\mathcal{Y}=[2]\) and \(\sigma(y_{1},y_{2})=\mathbbm{1}\{y_{1}=y_{2}\}\). Then \(M\)-unambiguity does not hold since \(\sigma(1,1)=\sigma(2,2)\). But if \(\mathbb{P}(Y=1)=0.1=1-\mathbb{P}(Y=2)\), then

\[\mathbf{T}_{1}=\begin{array}{cc}0&1&2\\ 1&\begin{bmatrix}0.9&0.1\\ 0.1&0.9\end{bmatrix}\end{array}\] (78)

is an invertible transition.

### Counterexamples for standard PLL

We show that for the standard PLL, the small ambiguity degree condition [28] and the invertibility of the transition matrix, see Definition 12, do not imply each other. Below, we recapitulate the small ambiguity degree condition.

**Definition 13** (Ambiguity degree [28]).: _The ambiguity degree for standard PLL is defined as_

\[\gamma:=\sup_{\mathcal{D}(x,y)>0,\boldsymbol{y}^{\prime}\neq y}\mathbb{P}_{(x,y)\sim\mathcal{D}}(y^{\prime}\in\sigma(y))\] (79)

_where \(\mathcal{D}(x,y)\) is the density function at \((x,y)\). We say that a PLL instance satisfies the small ambiguity degree condition, if \(\gamma<1\)._

**Small ambiguity \(\neq\) invertibility.** Consider the following transition matrix (where all blank elements are zero) with \(\mathcal{Y}=[5]\) and \(\mathcal{S}=2^{\mathcal{Y}}\):

\[\mathbf{T}=\begin{array}{ccccc}&1&2&3&4&5\\ \{1,2,3\}&\begin{bmatrix}1/2&1/2&1/2&&\\ 1/2&&1/2&1/2\\ &1/2&&1/2\\ &1/2&&1/2\\ \end{bmatrix}\\ \end{array}\] (80)

It can be verified that the small ambiguity degree is \(1/2\) but this matrix is of rank \(4<5\) and the invertibility condition does not hold.

**Invertibility \(\neq\) small ambiguity.** Consider the following transition matrix with \(\mathcal{Y}=[2]\) and \(\mathcal{S}=2^{\mathcal{Y}}\):

\[\mathbf{T}=\begin{array}{ccccc}&1&2\\ \{1\}&\begin{bmatrix}1&0\\ 0&1\end{bmatrix}\\ \end{array}\] (81)

We can see that the small ambiguity degree is \(1\) but the above matrix is of full rank.

### \(M\)-ambiguity and small ambiguity

We show that our proposed \(M\)-unambiguity, see Definition 1, is an extension of the small ambiguity degree, see Definition 13. To do this, we consider a generalized setting, where \(s\) is a randomized function of \(\boldsymbol{y}\). In particular, similarly to standard PLL (aka superset learning problem) [28; 8], we define \(\sigma\) to be a random function \(\mathcal{Y}^{M}\to 2^{\mathcal{Y}^{M}}\) so that \(s\in 2^{\mathcal{Y}^{M}}\) is a random set of vectors in \(\mathcal{Y}^{M}\). Below, let \(\mathcal{D}^{M}\) be a distribution followed by \(M\) i.i.d. random variables \((X_{i},Y_{i})\), where \((X_{i},Y_{i})\sim D\), for each \(i\in[M]\). We use \(\mathcal{D}^{M}(\boldsymbol{x},\boldsymbol{y})\) to denote the density at \((\boldsymbol{x},\boldsymbol{y})\), for \(\boldsymbol{x}\in\mathcal{X}^{M}\) and \(\boldsymbol{y}\in\mathcal{Y}^{M}\).

**Definition 14** (\(M\)-Ambiguity degree).: _The ambiguity degree for standard PLL subject to \(\mathcal{D}^{M}\) is given by_

\[\gamma:=\sup_{\mathcal{D}^{M}(\boldsymbol{x},\boldsymbol{y})>0,\boldsymbol{ y},\boldsymbol{y}^{\prime}\text{ are diagonal with }\boldsymbol{y}\neq\boldsymbol{y}^{\prime}}\mathbb{P}_{(\boldsymbol{x}, \boldsymbol{y})\sim\mathcal{D}^{M}}(\boldsymbol{y}^{\prime}\in s)\] (82)

_We say that a PLL instance satisfies the small \(M\)-ambiguity degree condition, if \(\gamma<1\)._

Firstly, it can be seen that the \(M\)-ambiguity degree, see Definition 14, reduces to the ambiguity degree from Definition 13 when \(M=1\), since all label vectors are diagonal in that case. Below, we show that the small \(M\)-ambiguity degree condition guarantees learnability.

**Proposition 2**.: _If the small \(M\)-ambiguity degree condition is satisfied, then we have:_

\[\mathcal{R}^{01}(f)\leq\frac{1}{1-\gamma}(c^{2M-2}\mathcal{R}^{01}_{\mathsf{ P}}(f;\sigma))^{1/M}\] (83)

Proof.: Let \(\mathcal{E}\) be the event that for an input vector \(\boldsymbol{x}\in\mathcal{X}^{M}\) with gold labels \(\boldsymbol{y}\), there exists a diagonal vector \(\boldsymbol{y}^{\prime}\in\mathcal{Y}^{M}\) with \(\boldsymbol{y}^{\prime}\neq\boldsymbol{y}\), such that \(\boldsymbol{y}^{\prime}\in s\). Then, by definition, we have \(\mathbb{P}(\mathcal{E})\leq\gamma\). Also, conditioned on \(\mathcal{E}\), from the proof of Lemma 1, we know that \(\mathcal{R}^{01}_{\mathsf{P}}(f)\geq\frac{\mathcal{R}^{01}(f)^{M}}{(c(c-1))^{M -1}}\). The above implies that

\[\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\geq(1-\mathbb{P}(\mathcal{E}))\frac{ \mathcal{R}^{01}(f)^{M}}{(c(c-1))^{M-1}}\] (84)Therefore, we have:

\[\mathcal{R}^{01}(f)\leq\frac{\left(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)c^{2} \right)^{1/M}}{1-\mathbb{P}(\mathcal{E})}\leq\frac{\left(\mathcal{R}^{01}_{ \mathsf{P}}(f;\sigma)c^{2}\right)^{1/M}}{1-\gamma}\]

concluding the proof of Proposition 2. 

The special case of Proposition 2, where \(M=1\) recovers the classical result for standard PLL (see the discussion at the end of Section 3 in [28]). The partial risk \(\mathcal{R}^{01}_{\mathsf{P}}(f;\sigma)\) can be further bounded by its empirical counterpart, assuming that the Natarajan dimension is finite, as we have previously shown.

## Appendix F Related Work

**Partial Label Learning.** Partial label learning (aka _superset learning_) assumes the learner receives the training data in the form of \((x,z)\), where \(z\in 2^{\mathcal{Y}}\) is a subset of the label space that contains the gold label. Technically, it is technically possible to cast our problem to standard PLL by identifying \(s\) as its preimages, namely the set of all labels that is consistent with the observation \(s\). Nevertheless, our work extends the standard PLL in two ways. Firstly, our framework allows multiple instances to appear in one training sample. Secondly, the learnability of PLL is typically shown under the _small ambiguity_ assumption, see, for example, [11, 28, 5]. This condition requires no distracting label to be contained in \(z\) with probability 1, which is violated in our case since a distracting label can _always_ occur because \(\sigma\) is deterministic. Therefore, our proposed conditions relax and generalize small ambiguity by allowing deterministic transitions, see Appendix E.3.

Another line of work in weak supervision exploits the _transition matrices_[46] to compute posterior class probabilities in different scenarios, e.g., PLL [8, 9] and noisy label learning [61]. The key assumption is that the transition matrix is _invertible_[8, 46]. Our learnability condition, \(M\)-unambiguity, and the invertibility condition do not imply each other, see Appendix E.1 and E.2 for a self-contained discussion. There, we also show that small ambiguity and the invertibility condition do not imply each other, which might be of independent interest.

**Latent structure models.** Our work also relates to _latent structural learning_, where the aim is to learn a latent representation using supervision on a deterministic transition \(\sigma\) over the latent variables [45, 52]. To reduce the cost of computing \(\sigma\), [42] proposes approximations for a restricted class of decomposable transitions. Both [42, 35] assume latent models in the exponential family. Our work is also related to SPIGOT [34, 32]. There, the aim is to learn both the transition and the latent model, proposing techniques for back-propagating through the argmax layer of the latent model. Complementary to the above research, we focus on rigorous theoretical analysis.

Our work is closely related to [62], which studies a similar problem of weakly supervised learning with multiple instances. However, our results are stronger and closer to the aims of weak supervision in comparison to the results in [62]. This is because our work proposes sufficient and necessary conditions to recover the hidden labels (i.e., \(Z_{1:K}\) in [62]), while consistency in [62] concerns the likelihood of the observed labels rather than the hidden ones, as defined in Definition 3 and 4 of [62], where two parameters are equivalent if they have the same likelihood on the observed partial label (rather than the hidden labels). Also, we use a different surrogate loss based on semantic loss that allows a top-\(k\) approximation. In general, semantic loss is designed to capture Boolean constraints over the hidden labels and is standard to use in neuro-symbolic learning. Furthermore, our work directly extends the theory of PLL, which is thoroughly discussed in Appendix. In particular, our learnability condition from Definition 1 directly extends the classical small ambiguity degree condition, as shown in Appendix E.3. In contrast, the connection to PLL is missing from [62].

**Constrained learning.** The problem of partial label learning is closely related to constrained learning, in the sense that the prediction for the label (vector) is subject to the constraint \(\sigma(\bm{y})=s\) at the training stage. Training classifiers under constraints has been well studied in NLP. The work in [36] proposes a formulation for training under linear constraints; [37] proposes a learning framework that unifies expectation maximization with integer linear programming. Posterior regularization has been proposed for training classifiers under graphical models [21]. The latter two lines of research were adopted by [27] and [23] for neurosymbolic learning.

Neurosymbolic learning.Our work was motivated by frameworks that employ logic for training neural models [30, 53, 13, 59, 43, 31, 24, 27]. We prove learnability under (unknown) transitions that capture different languages: systems of Boolean formulas, non-linear constraints and logical theories. Notice that the key to model logical theories, e.g., Datalog, via \(\sigma\) is through _abduction_[26], see [43, 13] for a discussion. We also prove error bounds under a common neurosymbolic loss, the SL [56], and top-\(k\) approximations. We are the first to provide rigorous results on neurosymbolic learning, closing a gap in the literature.

Notice that [58] also proves consistency of a top-\(k\) loss. However, they use a zero-one-style loss. Finally, [3] provides generalization bounds of the _smart predict-then-optimize_ (SPO) loss [15]; it assumes, though, that the gold labels of the inputs are given. Complementary to our research is the work in [19, 47, 33] that aims to integrate combinatorial solvers as differentiable layers in deep neural models. Finally, [48] proposes techniques for learning to solve combinatorial optimization problems using end-to-end neural models-loosing the ability to extract the latent models.

A subtle, yet substantial, difference between standard PLL and multi-instance PLL is that populating \(\sigma\) may incur a substantial computational overhead in the latter case. For instance, let us return back to Example 1. To populate the entries of \(\sigma\) for each target sum \(s\), we need to compute all solutions of the equation \(X+Y=s\), where \(X,Y\in\{0,\ldots,9\}\). In contrast, the set of labels associated with each input instance is part of the input [28, 11].

**Learning logical theories.** Another relevant strand of research is that of learning logical theories [12, 16, 38]. The work in [12] mixes abduction with induction to jointly learn a theory while training the neural classifiers, while [16] introduces a differentiable formulation of inductive logic programming that relies on the semantics of fuzzy logic for interpreting formulas. Finally, the authors in [38] propose learning rules using the notion of logical neural networks. The techniques in [12, 16, 38] rely on templates for learning logical theories. Instead of relying on pre-specified templates, the authors in [18] aim to learn those templates by mining patterns (aka motifs) from the data, following mathematically rigorous techniques and providing formal guarantees related to the mined motifs. Differently from our work, the above line of research provides no guarantees in terms of learnability or error bounds.

## Appendix G Experiment Details

Firstly, recall that in all neurosymbolic frameworks considered in Section 6, we considered weights in \(\{1,\ldots,5\}^{10}\) and used an additional neural classifier to learn the unknown weights. As the weights are fixed, the inputs to the weight classifiers in all frameworks are constant signals, i.e., in each iteration, we provided the classifier with the same signal \(W_{i}\) for the \(i\)-th weight. For each neurosymbolic framework, our implementation built upon the sources made available by the authors.

We now provide information about DeepProbLog, NeurASP and NeuroLog. Listings 1, 2 and 3 show the logic programs used to train the MNIST digit classifiers under the above three frameworks using the WEIGHTED-SUM problem, see Section 6.

DeepProbLog and NeurASP link the classifiers' predictions with the logic program via _neural predicates_. WEIGHTED-SUM requires two neural predicates: digit and weight. The first one classifies the input MNIST digits into \(\{0,\ldots,9\}\), while the second one classifies the input weights into \(\{1,\ldots,5\}\). In NeuroLog, the association between the classifiers' predictions and the logic program is maintained via _abducible_ predicates. In particular, the instances of those predicates associate the probabilistic predictions with the facts from the domain of the logic program. Listing 3 uses two abducible predicates: atdigit and atweight. Atom atdigit\((s,j)\) denotes that the \(j\)-th input to the digit classifier maps to value \(s\). The abducible atom atweight\((s,j)\) is analogously defined.

In terms of heuristics, in DeepProbLog, we used the Geometric Mean [31].

Following ENT's official implementation, we encoded WEIGHTED-SUM(2) using the Z3 SMT solver. In addition, similarly to the handwritten formula evaluation scenario in [27] (Section 5.1 from [27]), we used two projection operators. To generate the initial random labels, we projected out all the digit labels and kept only the weight labels. During random walks, we projected out the second input digit's labels, keeping the labels of the first digit and the labels of both input weights. The rest of the hyperparameters, as well as the two-stage training process, were set as in the hand-written formula evaluation scenario from [27].

**Neural architectures.** The layers of the MNIST digit classifier are as follows: _Conv2d(1, 6, 5), MaxPool2d(2, 2), ReLU(True), Conv2d(6, 16, 5), MaxPool2d(2, 2), ReLU(True), Linear(16 * 4 * 4, 120), ReLU(), Linear(120, 84), ReLU(), Linear(84, N), Softmax(1)._

The neural classifiers for all frameworks but ABL were built using PyTorch 2.0.0 and Python 3.9. For ABL, we aligned with the implementation provided by the authors and used Tensorflow 2.11.0 and Keras 2.11.0. For each framework, we used the hyperparameters proposed by the authors in analogous scenarios.

nn(mnist_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: digit(X,Y). nn(weight_net,[X],Y,[1,2,3,4,5]) :: weight(X,Y). weighted_sum2(I1, I2, W1, W2, S) :-  digit(I1,D1), weight(W1,A1),  digit(I2,D2), weight(W2,A2),  S is A1*D1+A2*D2. weighted_sum3(I1, I2, I3, W1, W2, W3, S) :-  digit(I1,D1), weight(W1,A1),  digit(I1,D1), weight(W1,A1),  digit(I2,D2), weight(W2,A2),  digit(I3,D3), weight(W3,A3),  S is A1*D1+A2*D2+A3+D3. weighted_sum4(I1, I2, I3, I4, W1, W2, W3, W4,S) :-  digit(I1,D1), weight(W1,A1),  digit(I2,D2), weight(W2,A2),  digit(I3,D3), weight(W3,A3),  digit(I4,D4), weight(W4,A4),  S is A1*D1+A2*D2+A3+D3+A4*D4.

nn(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X). nn(weight(1,X), [1,2,3,4,5]) :- wei(X). img(i1). img(i2). img(i3). img(i4). wei(0). wei(02). wei(03). wei(04). weighted_sum2(X1,X2,X3,X4,S) :-  digit(0,X1,N1), weight(0,X3,W1),  digit(0,X2,N2), weight(0,X4,W2),  S=N1*N1+W2*N2. weighted_sum3(X1,X2,X3,X4,X5,X6,S) :-  digit(0,X1,N1), weight(0,X4,W1),  digit(0,X2,N2), weight(0,X5,W2),  digit(0,X3,N3), weight(0,X6,W3),  S=N1*N1+W2*N2+W3*N3. weighted_sum4(X1,X2,X3,X4,X5,X6,X7,X8,S) :-  digit(0,X1,N1), weight(0,X5,W1),  digit(0,X2,N2), weight(0,X6,W2),  digit(0,X3,N3), weight(0,X7,W3),  digit(0,X4,N4), weight(0,X8,W4),  S=N1*N1+W2*N2+W3*N3+W4*N4.

Listing 2: NeurASP program for the WEIGHTED-SUM problem.

abducible(atdigit(_,_)). abducible(atweight(_,_)). digit(N) :- N in 0..9. weight(N) :- N in 1..5.

weighted_sum2(S) :- digit(D1), atdigit(D1,1),  digit(D2), atdigit(D2,2),  weight(S1), atweight(S1,1),  weight(S2), atweight(S2,2),  S #= (D1 * S1) + (D2 * S2).

weighted_sum3(S) :- digit(D1), atdigit(D1,1),  digit(D2), atdigit(D2,2),  digit(D3), atdigit(D3,3),  weight(S1), atweight(S1,1),  weight(S2), atweight(S2,2),  weight(S3), atweight(S3,3),  S #= (D1 * S1) + (D2 * S2) + (D3 * S3).

weighted_sum4(S) :- digit(D1), atdigit(D1,1),  digit(D2), atdigit(D2,2),  digit(D3), atdigit(D3,3),  digit(D4), atdigit(D4,4),  weight(S1), atweight(S1,1),  weight(S2), atweight(S2,2),  weight(S3), atweight(S3,3),  weight(S4), atweight(S4,4),  S #= (D1 * S1) + (D2 * S2) + (D3 * S3) + (D4 * S4).

Listing 3: NeuroLog program for the WEIGHTED-SUM problem.