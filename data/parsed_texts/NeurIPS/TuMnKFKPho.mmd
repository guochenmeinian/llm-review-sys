# Vheldm: A Holistic Evaluation of Vision Language Models

 Tony Lee\({}^{1*}\)   Haoqin Tu\({}^{2*}\)   Chi Heem Wong\({}^{1,3*}\)   Wenhao Zheng\({}^{4}\)   Yiyang Zhou\({}^{4}\)

Yifan Mai\({}^{1}\)   Josselin Somerville Roberts\({}^{1}\)   Michihiro Yasunaga\({}^{1}\)   Huaxiu Yao\({}^{4}\)

&Cihang Xie\({}^{2}\)   Percy Liang\({}^{1}\)

\({}^{1}\)Stanford University  \({}^{2}\)University of California, Santa Cruz  \({}^{3}\)Hitachi America, Ltd.

\({}^{4}\)University of North Carolina, Chapel Hill  \({}^{*}\)Equal contribution

###### Abstract

Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: _visual perception_, _knowledge_, _reasoning_, _bias_, _fairness_, _multilinguality_, _robustness_, _toxicity_, and _safety_. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.

## 1 Introduction

Vision-language models (VLMs) -- models that take both text and images as a prompt and produce text as output--have seen rapid growth and deployment in the past year. They are used in visual question answering [35], text-driven image creation and alteration [26], image captioning [7], and robotics [49]. Despite their prevalence, much remains unknown regarding their capabilities, limitations, and risks, particularly in the areas of contextual understanding, bias [10], ethics [40], and safety [28].

Current benchmarks for VLMs assess the models only on a limited number of factors, often related to their perception or problem-solving capabilities. Other factors, such as the ability to generate contextually relevant and unbiased content, their performance across diverse linguistic and cultural contexts, or their environmental impact, are less frequently studied. We refer readers to Table A1 for a comparison of the factors that the benchmarks assess. Aggregating multiple studies to create a comprehensive picture of the VLMs is not straightforward. Firstly, each benchmark tests a limited,small set of models in their studies, making it difficult to obtain a complete picture of any VLM. This is exacerbated by the fact that benchmark studies are snapshots in time and hence will not include newer models. Secondly, evaluation protocols vary across studies, which makes it impossible to compare VLMs fairly; as seen from previous standardized evaluations on large language models (LLMs), small changes to the protocols (e.g., using uncertainty-routed chain-of-thought instead of 5-shot in-context learning) can yield significantly different results [30; 5].

We introduce **Holistic Evaluation of Vision Language Models (VHELM)**, which is based on the framework introduced by Liang et al. [24] for large language models and Lee et al. [21] for text-to-image models. Our contributions are three-fold. First, we identify the aspects that are both applicable to VLMs and important to evaluate from either a technological or societal perspective: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety (see Figure 1 for examples and Table 2 for descriptions). Second, we assemble 21 existing VLM benchmark datasets--which are sets of image-text prompts and expected output--and map to the aspects to ensure complete coverage. Third, we standardize the evaluation procedures so that apple-to-apple comparisons can be made across the models. All these culminate in a comprehensive benchmark that not only provides a multi-dimensional overview of the capabilities of the VLMs, but enables researchers, developers, and users to compare across models (see Table 1).

We evaluate 22 prominent vision-language models (see Table A2) and some of our findings include: 1) there is no model that excels across all aspects; while GPT-4o comes close to dominating most of the leaderboards, it does not perform as well as the other models when evaluated on bias, robustness,

Figure 1: Holistic Evaluation of Vision Language Models (VHELM) is a benchmark with standardized evaluation procedures and automated metrics. We evaluate 9 important dimensions (_aspects_) across scenarios to create a comprehensive view of VLMs. The metrics listed are not specific to the examples but are a list of those used across all the scenarios in the aspect.

[MISSING_PAGE_FAIL:3]

The VHELM Framework

VHELM focuses on vision-language models that take in interleaved images and text input as prompts to produce text completions 1 (see Figure A1). The VHELM evaluation process consists of 4 main components: aspect, scenario, adaptation, and metric (see Figure 2).

Footnote 1: VLMs that produce images as output are currently not covered in this study.

An **aspect** is a specific evaluative dimension that contributes to assessing the overall performance. The aspects considered in VHELM are bias, fairness, knowledge, multilinguality, reasoning, robustness, toxicity, and visual perception (details are in Sec. Section 3.1). Aspects are evaluated by computing metrics over scenarios.

A **scenario** represents a use case for a VLM and is identified by a task (e.g., question answering, code generation, and captioning) and a usage category such as the domain, origin, language, or subject. An example scenario is "visual question answering on medical images" where the task is visual question answering and the usage category is medical images. We consider a wide range of scenarios, with tasks ranging from visual question answering to captioning and usage categories consisting of multiple languages, subjects, and image types. The scenarios used in VHELM are listed in Table 3. A dataset is a set of _instances_--defined as a pair of prompt and reference--that can be used for evaluating the model performance on one or more scenarios. A dataset can power multiple scenarios, such as in the case of Bingo [6], where the'region bias' or 'OCR bias' subsets assess visual question answering of images from different geographic locations (used to test fairness) and visual question answering of images with text in various languages (used to test multilinguality), respectively. A dataset is sometimes synonymous with the scenario, especially in the context of model evaluation. For example, we may state "MMMU (Accounting)" as a scenario with the understanding that the accounting subset of MMMU tests visual question answering in the domain of accounting. VHELM compiles a total of 21 existing datasets (see Table 3).

An **adaptation** is a specific procedure for invoking a model. Adaptation strategies include zero-shot prompting, \(k\)-shots prompting, and chain-of-thought prompting. In this study, we use only zero-shot prompting as it is the most common strategy used by the layperson.

A **metric** quantifies how well a VLM performs on a scenario. Some examples of metrics are exact match or using either a human or a model to score on a scale of 1 to 5.

### Aspects & Scenarios

VHELM considers 9 aspects that are crucial for developing capable, safe, and reliable VLMs (see Table 2). These include fundamental capabilities, such as visual perception, knowledge, and reasoning, and behavior relating to society and ethics, such as bias, fairness, multilinguality, robustness, toxicity, and safety.

VLMs are capable of **visual perception**, which is the ability to process and understand images. Visual perception is assessed through image captioning, where VLMs produce descriptions of the input images, or visual-question answering (VQA), where VLMs are asked to answer questions pertaining to the images. VHELM uses scenarios such as Flickr-30k [46], VQAv2 [12], VizWiz [14] and POPE [23] to assess this aspect.

Figure 2: **Evaluation components. Each evaluation run consists of an aspect (i.e., an evaluative dimension), a scenario (i.e., backed by a specific dataset), a model with an adaptation process (i.e., how the model is prompted), and one or more metrics to capture how good the model responses are.**

Similar to LLMs, VLMs have knowledge and possess reasoning capabilities. **Knowledge** is the ability to recall facts or information contained in the models and is assessed by asking questions whose answers cannot be found in the inputs, such as identifying the name of the mountain shown in an image. In VHELM, these instances are provided by A-OKVQA [34], MME [44], MMMU [47], Vibe-Eval [33], and MathVista [29].

**Reasoning**, on the other hand, is the ability to perform multiple steps of inference to arrive at the answer and is assessed either by asking questions whose answers exist indirectly in the inputs or by explaining a sequence of pictures. For example, the VLM is asked to compute the probability of a category given the unnormalized histogram. Reasoning is benchmarked using GQA [15], MathVista [29], Mementos [43], SEED-Bench [22], and RealWorldQA [13] in VHELM.

**Bias** refers to the ability to avoid unwarranted associations between the input to and output of a VLM, such as associating a specific gender with certain occupations. Compared to LLMs, VLMs' visual input provides another place where spurious correlations could cause bad behavior. For example, skin tone or hair length can be identified from pictures and used to produce stereotypical associations. We use PAIRS [10] to probe social biases in VLMs and provide some examples in Appendix E.

**Fairness** in VHELM refers to either counterfactual fairness or performance disparity. Counterfactual fairness is expecting similar responses when a spurious attribute of the input (e.g., language dialect) is changed. In VHELM, this is assessed by asking questions drawn from A-OKVQA [34] and VQAv2 [12] in African-American English (AAE), and through VQA on images around the world from Bingo [6]. See Appendix F for an example of AAE perturbation. Performance disparity is having similar performance on every subset of the data when an attribute is used as the filter. For example, a VLM should be equally skillful in captioning images from different geographical locations. VHELM tests for fairness across geographies using Crossmodal-3600 and across race, gender, and age using FairFace [16].

We believe that a VLM should be **multilingual**, which is the ability to perform a task when the instruction and/or output languages are changed. We augment A-OKVQA [34] by translating the questions and answers from English to either Chinese, Hindi, Spanish, or Swahili to test whether the VLMs are invariant to VQA in different languages. An experiment to validate the machine translations is presented in Appendix G. In addition, we use the "OCR bias" subset in Bingo [6] to test if VLMs understand an image if the text in it is presented in another language and EXAMS-V [8] to evaluate whether VLMs have reasoning capabilities in multiple languages.

An important property of a good VLM is **robustness**, defined as producing desired answers under invariant perturbations of the input text, such as having typographic errors (aka typos). We introduce typos into A-OKVQA [34] and VQAv2 [12] to test the robustness against text perturbations. We also use Unicorn [39] to evaluate how VLMs perform on sketches and out-of-distribution images and Bingo [6] to probe robustness to interference (e.g., asking "The squares A and B in the picture are the same color, right?" vs "The squares A and B in the picture are _not_ the same color, right?") and counter-factual images.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Aspect** & **Description** \\ \hline Visual Perception & Interpreting information in the image \\ Knowledge & Recalling facts or information contained in the VLM \\ Reasoning & Performing multiple steps of inference to arrive at the answer \\ Bias & Avoiding unwarranted associations between the input and output of the model \\ Fairness & Producing similar responses when a spurious attribute of the input (e.g., race) is \\  & changed (i.e., counterfactual fairness) _or_ having similar performance on every subset \\  & of the data when an attribute is used as the filter (i.e., performance disparity) \\ Multilinguality & Performs the same task when the language is changed \\ Robustness & Producing desired answers under invariant perturbations of the input text (e.g., typos) \\ Toxicity & Identifying and avoiding offensive or damaging materials (e.g., hate speech, violent speech, or abusive language) \\ Safety & Refusing to produce answers that cause harm to humans \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluative aspects in VHELM

**Toxicity** is the ability to identify and avoid offensive or damaging materials, such as hate speech, violent speech, abusive language, etc. We use HatefulMemes [17] to see if the model can distinguish between toxic and non-toxic images.

Finally, **safety** is refusing to produce answers that cause harm to humans. We evaluate the VLMs with MM-Safety-Bench [28] to judge the resiliency of VLMs when they are prompted with harm-inducing instructions.

\begin{table}
\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Aspect** & **Dataset** & **Category** & **Description** & **Metric** \\ \hline Visual Perception & PixAvQa [46] & – & Image captioning over Flickr images. & Prometheus Vision \\ VQAv2 [12] & – & VQA over common images. & Exact Match \\ VizWiz [14] & – & VQA over images collected by the visually impaired. & Exact Match \\ POPE [23] & – & Answering yesto to questions related to an image. & Exact Match \\ \hline Knowledge & A-OKVQA [34] & – & VQA with real-world images. & Exact Match \\ MME [44] & Posters, Celebriy, Artsovk, Landmark & VQA, & Exact Match \\ MMU [47] & Posters, Celebriy, Artsovk, Artsovk and Engineering, Art, Art theory, Basic medical science, Biology, Chemistry, Clinical medicine, Computer science, Design, Diagnostics and laboratory medicine, Economics, Electronics, Energy and Power, Finance, Geography, History, Literature, Management, Marketing, Materials, Mechanical engineering, Music, Pharmacy, Physics, Psychology, Public health, Sociology Normal, Hard & VQA on prompts by experts. & Prometheus Vision \\ \hline Reasoning & GOA [15] & – & VQA on real-world images. & Exact Match \\ MathVista [29] & Elementary school, High school, College, Diamonds [43] & Solve mathematical reasoning questions. & Exact Match \\ Momenes & BELB-Bench [22] & Daily life & – & Prometheus Vision \\  & BELB-Bench [22] & Visual reasoning, Instance interaction RealWorldQA [13] & – & Multi-choice VQA & Exact Match \\  & & understanding. & Exact Match \\ \hline Bias & PAIRS [10] & Occupations, Potential crime, Status and racial bias (white and black) with VLMs & The task probes gender (man and woman) and racial bias (white and black) with the aid & Exact Match \\  & & & & We add an “uncelair” option for the VLM to opt out of making a biased decision. & Exact Match \\ \hline Fairness & A-OKVQA* [38] & Dialect deterministic Enselsh/English, Spanish Spanish/English, Chinese/English, Link/English, Cisco & VQA on perturbed prompts. Multilingual captioning. & Exact Match \\  & & Spanish/English, Miami/English, French, Spanish & VQA sensitive of exam questions across text perturbations described in Liang et al [24]. & Exact Match \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\ \hline Multilinguality & A-OKVQA* & Chinese, Hateful, Spanish, Swedish & VQA on translated input. & VQA on the same set of images containing text in different languages & Exact Match \\  & & EXAMS-V [8] & Arabic, Bulgarian, Chinese, English, French, German, Hungarian, Italian, Polish, Serbian & VQA consisting of exam questions across Spanish & Exact Match \\ \hline Robustness & A-OKVQA* & Invariant text perturbation - Types & VQA with robustness test perturbations following [24]. & Exact Match \\  & Bingo [6] & Factual Bias & VQA on images that have counterfactual information. & Exact Match \\  & & Image-to-image Interference & Test generation given composite pictures of similar images. & Exact Match \\  & & & Test-to-image Interference & VQA on perturbations proposed. & Exact Match \\  & Unicom [39] & OODVC-VQA, Sleschy-VQA & VQA on images with out-of-distribution visual content (OODVC) or human & Exact Match \\  & & & & & \\  & & & & \\  & & & & \\  & VQAv2* & Invariant text perturbation - Types & VQA with robustness test perturbations following [24]. & Exact Match \\ \hline Toxicity & Hateful Memes [17] & – & Classify whether a meme on is hateful or not. & Exact Match \\ \hline Safety & MM-SafetyBench [28] & Illegal activity, Hate speech, Mahware generation, Physical harm, Economic harm, Fiard, Sex, Political latelying, Privacy violence, Legal opinion, Financial advice, Health consultation, Government decision & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Mapping of scenarios to aspects. Asterisks (*) denote that we augment the dataset to create a new scenario.

### Metrics

We eschew metrics that require manual annotation by humans and adopt automated metrics for VHELM so that evaluation runs are low-cost, fast, and consistent. However, automated metrics may have limitations, particularly when it comes to capturing nuanced or subjective aspects of language generation. To facilitate evaluation, we modify some of the scenarios and frame the questions as multiple-choice ones as much as possible. For example, we ask "Answer the multiple choice question by just giving the letter of the correct answer. Is this ____? (a) Yes (b) No" instead of "Is this ____?". For multiple-choice questions, we exact match the predicted output against a correct reference.

The main metric for Vibe-Eval, Crossmodal-3600, and Bingo is the score from Prometheus-Vision [20]. Prometheus-Vision is a VLM that judges the similarity between the prediction and the ground truth on a scale of 1 (bad) to 5 (good) and has been shown to emulate human evaluators, as evident from the correlation between scores annotated by humans and those predicted by the evaluator VLMs [20]. Details of how Prometheus-Vision is used and examples of the produced ratings can be found in Appendix H. Finally, the Perspective API toxicity classifier is used to determine whether the output is toxic for instances in the MM-SafetyBench.

We average the scores across all the instances to produce an aggregated score for that scenario. For scenarios that consist of multiple-choice questions (which is all scenarios except Vibe-Eval, Crossmodal-3600, Bingo, and MM-SafetyBench), this corresponds to the accuracy. To evaluate performance disparities in fairness, we compare the accuracy between groups. Finally, we compute the mean win rate on the main metrics (i.e., accuracy or average Prometheus-Vision score) when creating the overall leaderboard or ranking the models within an aspect. The win rate of a model is defined as the probability that the model outperforms another model selected uniformly at random for a given metric in a head-to-head comparison.

## 4 Experiments

We evaluate 22 recent models from 6 developers, as listed in Table A2. The models in VHELM are all public except for the preview version of Palmyra Vision. In addition to implementing consistent adaptation and evaluation methods across all the models, we maintain the use of standard inference parameters for each model to ensure fair comparisons across VLMs.

Our evaluation run randomly samples a maximum of 1,000 instances for each of the scenarios in order to alleviate monetary and time constraints; a single evaluation run on the 22 VLMs uses a total of 915K instances and consumes 51.6M input text tokens, 9.4M output text tokens, and 915K images. We believe that we can obtain significant measurements that will reflect the models' true performances despite capping the number of instances for each scenario. Our experiments are conducted and completed on September 17, 2024.

## 5 Results and Analysis

In this section, we present some of our key empirical findings and while encouraging readers to refer to our interactive website at https://crfm.stanford.edu/helm/vhelm/v2.0.1, where they can view the result groups and sort them by their desired column. We also display the prompts, predictions, and scores for _every_ model and instance there.

1. **There is no model that excels across all scenarios.** Table 4 indicates that, as of the time of writing, there are always trade-offs to be made when selecting a model to use. However, GPT-4o (0513) comes close to being the best in most aspects; it boasts an unparalleled 100% win rate across all scenarios in robustness and is the top model in knowledge, reasoning, and visual perception in terms of the win rates. However, its performances in terms of bias and safety leave much to be desired. Interestingly, its newer version, GPT 4o (0806), scores a lower mean win rate for all aspects except bias.
2. **Closed-API models significantly outperform open-weight ones.** Table 4 shows that closed-API models generally surpass open-weight models across a multitude of dimensions, notably in reasoning, knowledge, and toxicity. For these aspects, the worst-performing closed-API model outperforms the best-performing open-weight models.

3. **Open-weight models struggle to follow instructions.** Manual inspection of the output responses from the open-weight models indicates that they do not follow even the form of the instructions, resulting in poor overall performance 2. For example, they may ignore the command to output only a single option or number as the answer and instead produce long sentences. This observation suggests that they can benefit greatly from instruction tuning. Footnote 2: See https://crfm.stanford.edu/helm/vhelm/v2.0.1/#/runs/a_okvqa:model=HuggingFaceM4_idefics-80b-instruct,groups=a_okvqa_base for examples
4. **VLMs refuse to follow harmful instructions.** MM-SafetyBench attempts to trick the models into outputting toxic content by embedding the instructions as part of the image. Our measurements using Perspective API indicate that a vast majority of the models do not fall prey to such attacks (see Table A11). However, recent successes in jailbreaking VLMs [36, 45, 42] imply that VLMs may be susceptible to new avenues of attacks, and we leave this exploration as the future work.
5. **Detecting toxic content like memes is difficult.** Most models perform poorly on detecting hateful content, with the best model, IDEFICS 2, achieving an accuracy of 62.2% on Hateful Memes, followed closely by GPT-4V (61.3%) and GPT-4o (0513) (61.1%). See either Table 4 or Table A12 for details. Memes often contain subtle cues and rely heavily on cultural and social contexts, making their interpretation challenging. Sarcasm or irony can drastically alter the perceived meaning, further complicates understanding. This requires AI systems to have a nuanced grasp of both the immediate context and broader cultural references to assess a meme's intent and potential offensiveness accurately. We note that a possible limitation to consider is that what is considered offensive can vary widely among different groups, cultures, and individuals.
6. **VLMs lack multilingual support.** Most models do not perform as well when prompted in another language other than English. Across all the models, we see a maximum performance drop of between 8.6% (by GPT-4o (0806)) and 33.7% (by PaliGemma) between the original A-OKVQA and the translated A-OKVQA (see Table A8), indicating that the models heavily favor English. Looking at the average scores of the models across the translated A-OKVQA, we observe that the models generally perform better on Spanish (64.8%) > Chinese (62.7%) > Hindi (60.8%) > Swahili (57.0%), which corresponds to the ranking for website usage by language [41]. Interestingly, the models' ranking on the EXAMS-V, Bingo, and the language-augmented A-OKVQAs can differ. This may be due to the fact that scenarios like EXAMS-V and Bingo require aspects, such as knowledge and reasoning, in addition to multilinguality.
7. **Wide range of model performance on bias.** The most powerful models from the 5 closed-API model creators--GPT-4o (0806) from OpenAI, Gemini-1.5 Pro (0409) from Google, and Claude 3.5 Sonnet from Anthropic, Palmyra Vision from Writer--give the correct responses 95.4%, 92.3%, 61.4%, and 74.0% of the time, respectively, in PAIRS. The open-weight models perform significantly worse, achieving only an accuracy of 0%-48.7%. See either Table 4 or Table A5 for details. We notice that the efficiency-focused models perform significantly worse than the 'full' models. For example, Claude 3.0 Haiku (the fastest model in the Claude 3.0 family) achieves 8% whereas the Claude 3.0 Opus achieves 58.7%). Similarly, Gemini 1.5 Flash scored only 74.0%, a 17.4 percentage point gap difference when compared to Gemini 1.5 Pro (91.4%).
8. **VLMs are not robust to distribution shifts.** Across all the models, we observe a slight discrepancy between the performance on the original instances vs. perturbed instances. This shows that the models are generally robust against minor typographical perturbations. Parallel to the textual perturbations, we also consider visual perturbations like sketchy or uncommon images (i.e., OOD images) in the benchmark. Interestingly, our findings reveal that while GPT-4o (0806) excels in various aspects, it falls short in the Unicorn scenario with rarely seen and sketch OOD images, achieving only a mean accuracy of 82.9%, notably lagging behind the top-performing Gemini 1.5 Flash models with a score of 88.6% (see Table 4 or Table A10). In contrast, the model performance ranking on Bingo is consistent with other aspects, verifying the dominant position of GPT-4o (0806). The discrepancy may be due to the OOD images in Unicorn being more difficult, given that they come from both abstract sketches [9] and challenging OOD cases [48].

9. **Models do well on the fairness scenarios.** We do not observe significant differences in model performance between the relevant scenarios (e.g., across locations in Crossmodal-3600 or English vs. AAE-perturbed VQAv2), indicating that the models perform similarly given images or text from different geographical regions or minority dialects. We caution that this does not indicate that fairness is not an issue but that the concept of fairness may be subtle and is not truly tested by existing benchmarks.

## 6 Discussion

### Limitations

The choice of metrics can affect the evaluation of the models and we have opted to use automatic metrics in order to reduce cost and speed up evaluations. We simplify the scenarios, such as making the questions multiple-choice ones, in order to reduce the variance in the output. Furthermore, we use Prometheus-Vision, which has been shown to emulate human evaluators [20]. Despite our best efforts, these metrics are not perfect, as can be seen from Figures A4 and A5. We will continue to refine the metrics and update our benchmark as better ones become available.

Our benchmark currently measures 9 aspects that we believe are important to VLMs; there may be other aspects that are equally important that we may have missed, and we encourage readers to provide feedback. We acknowledge that the coverage for some of the aspects (e.g., toxicity or safety) is thin, and we would like to develop or integrate more scenarios for them. Additionally, identifying an aspect of a scenario is not exact, as there are overlaps between the aspects. For example, fairness and robustness are interchangeable when the language of the inputs is perturbed (i.e., AAE perturbation is both fairness and robustness). We envision VHELM as a living benchmark and will continuously strive to add more models and scenarios over time.

Benchmark results are technical objects that are only useful if they are contextualized. Further work has to be done to understand the nuances of the scores and quantify the correlation between the scores and real-world impact.

We are also cognizant that our benchmark, like others before us, can be 'gamed'. We hope to integrate scenarios that will pull fresh, real-world data at execution time so that models are always evaluated on data that is unseen during training.

### Broader Impact

VHELM evaluates VLMs on a standardized set of prompts, scenarios, and metrics, allowing stakeholders, including researchers, developers, and policymakers, to better understand and compare the performance of different VLMs. Our evaluations can quickly highlight the strengths and flaws of each model across the various aspects, thereby encouraging VLM developers to iterate toward better models.

## 7 Conclusion

VHELM assesses 9 important aspects for 22 well-known VLMs, which we hope will contribute to the ongoing development and refinement of VLMs, making them more reliable, fair, and useful across a broader range of applications. We strive to keep this a living benchmark by adding more models and scenarios over time.

## Acknowledgements

We thank Google for their support for the project. We also thank Center for AI Safety, Microsoft Accelerate Foundation Models Research Program, and the OpenAI Researcher Access Program for supporting our computing needs. The views and opinions expressed in this article are those of the authors only and do not necessarily represent the views and opinions of any other organization, any of their affiliates or employees acknowledged above.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c} \hline \hline Model & \multicolumn{2}{c}{Mean V003\({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\)  \({}^{\circ}\) [FOOTNOTE:5]Footnote 1

## References

* [1] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku, 2024.
* [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. PaliGemma: A versatile 3B VLM for transfer. _arXiv preprint arXiv:2407.07726_, 2024.
* [4] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvi Mirchandani, Eric Mitchell, Zanele Munykus, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Niliforoshan, Julian Nyorko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [5] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and improving chain-of-thought reasoning in vision-language models. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 192-210, 2024.
* [6] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in GPT-4v(vision): Bias and interference challenges. _arXiv preprint arXiv:2311.03287_, 2023.
* [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In _Advances in neural information processing systems_, 2023.
* [8] Rocktim Das, Simeon Hristov, Haonan Li, Dimitar Dimitrov, Ivan Koychev, and Preslav Nakov. EXAMS-V: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics_, pages 7768-7791, 2024.
* [9] Mathias Eitz, James Hays, and Marc Alexa. How do humans sketch objects? _ACM Transactions on graphics (TOG)_, 31(4):1-10, 2012.
* [10] Kathleen C Fraser and Svetlana Kiritchenko. Examining gender and racial bias in large vision-language models using a novel dataset of parallel images. In _Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics_, pages 690-713, 2024.
* [11] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.

* [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017.
* [13] Grok-1.5 Team. Grok-1.5 vision preview, 2024.
* [14] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617, 2018.
* [15] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.
* [16] Kimmo Karkkainen and Jungseock Joo. FairFace: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 1548-1558, 2021.
* [17] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. _Advances in neural information processing systems_, 33:2611-2624, 2020.
* [18] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. _Advances in Neural Information Processing Systems_, 36, 2024.
* [19] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? _arXiv preprint arXiv:2405.02246_, 2024.
* [20] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheusvision: Vision-language model as a judge for fine-grained evaluation. In _Findings of the Association for Computational Linguistics ACL 2024_.
* [21] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. In _Advances in Neural Information Processing Systems_, volume 36, 2024.
* [22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. SEED-bench: Benchmarking multimodal large language models. In _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 13299-13308, 2024.
* [23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 292-305, 2023.
* [24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsumori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. _Transactions on Machine Learning Research_, 2023.
* [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In _European Conference on Computer Vision_, pages 740-755. Springer, 2014.

* [26] Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, and Ming-Hsuan Yang. Text-driven image editing via learnable regions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7059-7068, 2024.
* [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Advances in neural information processing systems_, volume 36, 2024.
* [28] Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu Qiao. Query-relevant images jailbreak large multi-modal models. _arXiv preprint arXiv:2311.17600_, 2023.
* [29] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In _The Twelfth International Conference on Learning Representations_.
* [30] Yifan Mai and Percy Liang. Massive multitask language understanding (mmlu) on HELM, 2024. Accessed Jun 03, 2024.
* [31] OpenAI. GPT-4 technical report, 2024.
* [32] OpenAI. Hello GPT-4o, 2024.
* [33] Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et al. Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models. _arXiv preprint arXiv:2405.02287_, 2024.
* [34] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A benchmark for visual question answering using world knowledge. In _17th European Conference on Computer Vision_, pages 146-162. Springer, 2022.
* [35] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. CLIP models are few-shot learners: Empirical studies on VQA and visual entailment. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_, pages 6088-6100. Association for Computational Linguistics, 2022.
* [36] Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, and Lingpeng Kong. ImgTrojan: Jailbreaking vision-language models with ONE image. _arXiv preprint arXiv:2403.02910_, 2024.
* [37] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [38] Ashish V Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 715-729, 2022.
* [39] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? A safety evaluation benchmark for vision LLMs. _arXiv preprint arXiv:2311.16101_, 2023.
* [40] Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie. Sight beyond text: Multi-modal training enhances LLMs in truthfulness and ethics. In _NeurIPS Workshop on Instruction Tuning and Instruction Following_, 2023.
* [41] W3Techs. Usage statistics of content languages for websites. Accessed Sept 18, 2024.
* [42] Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, and Yu-Gang Jiang. White-box multimodal jailbreaks against large vision-language models. _arXiv preprint arXiv:2405.17894_, 2024.
* [43] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, and Furong Huang. Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics_, pages 416-442, 2024.

* [44] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. _arXiv preprint arXiv:2306.13549_, 2023.
* [45] Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong Liu, and Dacheng Tao. Jailbreak vision language models via bi-modal adversarial prompt. _arXiv preprint arXiv:2406.04031_, 2024.
* [46] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2014.
* [47] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9556-9567, 2024.
* [48] Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan L Yuille, and Adam Kortylewinski. OOD-CV: A benchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. In _17th European Conference on Computer Vision_, pages 163-180. Springer, 2022.
* [49] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspair Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: Vision-language-action models transfer web knowledge to robotic control. In _Proceedings of The 7th Conference on Robot Learning_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] **We present our thesis in Section 1. We explain our benchmark in Section 3 and describe the experiments in Section 4 and report results in Section 5. 2. Did you describe the limitations of your work? [Yes] **We discuss limitations in Section 6.1** 3. Did you discuss any potential negative societal impacts of your work? [Yes] **See Section 6.2** 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] **We provide the exact code and data to reproduce our results at** https://github.com/stanford-crfm/helm/ 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] **See Section 4** 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] **We do not compute error bars. Repeating the experiments with multiple runs is prohibitively expensive and negates the benefits of sampling. Given the large number of instances and usage categories for each aspect, we believe that we still obtain significant measurements that will reflect the true model performances. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] **We state the number of input and output text tokens used in our evaluation in Section 4.**
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] **All the models used in the experiments are hosted either by HuggingFace or the respective makers. We cite all the datasets and models used in our work. We own the license to the HELM codebase.** 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We did not collect new data, except modifying existing data.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] **We did not crowdsource or conduct research with human subjects.** 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

[MISSING_PAGE_EMPTY:16]

Description of Scenarios

Crossmodal-3600.Crossmodal-3600 is originally used to evaluate the performance of multilingual captioning and contains 3600 geographically diverse images and their human annotated captions. For each of the 36 languages, 100 images are taken in an area where the language is spoken. The images are then captioned by humans in every of the 36 languages. We adapt the dataset to measure fairness. We say that a VLM is fair if the scores of the generated captions are the same across the different subsets.

We use only the following subsets (image language/caption language) in our benchmark: English/English, Spanish/English, Chinese/English, Hindi/English, Cusco quechua/English, Maori/English, Swahili/English, Telugu/English.

Vibe-Eval.Vibe-Eval a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval aims to check multimodal chat models for day-to-day tasks and rigorously testing and probing the capabilities of present frontier models. We say that a VLM is knowledgable if the generated captions is semantically the same with the reference.

We use both subsets in our benchmark: hard and normal cases.

**Prompt**: Where was this photo taken?

**Reference**: This photo is taken in Japan, and the mountain in the background is Mount Fuji, which is the tallest mountain in Japan and one of the most famous landmarks in the country. The body of water in the foreground is likely one of the lakes that surround Mount Fuji, such as Lake Kawaguchi, Lake Yamanaka, or Lake Saiko. These lakes are popular tourist destinations and offer stunning views of Mount Fuji.

PAIRS.The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). We adapt the dataset to measure bias. We say that a VLM is unbiased if the model decide it's unclear to answer the question just given the image.

We use three subsets in the benchmark: occupation, status, potential crime. We add an "unclear" option for the VLM to opt out of making a biased decision.
Mementos.Mementos is a benchmark designed to assess VLMs' sequential image reasoning abilities. Mementos comprises 4,761 image sequences of varying lengths, predominantly sourced from Dailylife, Robotics, and Comics domains. We say that a VLM can reason over image sequences well if the similarity score between the generated caption and the reference is high.

We use three subsets of Mementos in our benchmark: Dailylife, Robotics, and Comics.

**Prompt**: Write a description for the given image sequence in a single paragraph.

**Reference**: In the sequence of images, a lady is seen at the beach, carrying a crab on a wooden plank with a joyful expression on her face. Her actions suggest that she is in the process of releasing the crab back into the sea. The plank serves as a protective barrier between her hands and the crab, ensuring a safe transport for the creature.

**Unicorn.** Unicorn has two Out-of-Distribution VQA tasks with two datasets (OODCV-VQA and Sketchy-VQA) and two corresponding data variants (OODCV-Counterfactual and Sketchy-Challenging). Unicorn collects images from OODCV and Sketch Objects. The OODCV-Counterfactual replaces or inserts irrelevant objects in questions and Sketchy-Challenging selects rarely seen sketch objects as a group. We say that a VLM is robust against OOD images and questions if the VLM predicts the correct answer of the question given the image.

We use only one subset in the benchmark: OODCV-VQA.

**Prompt**: Is this a sketch ship in the image?

**Reference**: Yes.

[MISSING_PAGE_FAIL:20]

MM-SafetyBench.MM-SafetyBench is a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. This dataset comprises 13 scenarios with AI-generated images and huma-written instructions, resulting in a total of 5,040 text-image pairs. We say that a VLM is safe if the toxicity level of the generated answer is low given the image and instruction input.

We use all 13 subsets of MM-SafetyBench in our benchmark: economic harm, financial advice, fraud, government decision, hate speech, health consultation, illegal activity, legal opinion, malware generation, physical harm, political lobbying, privacy violence, sex.

Bingo.Bingo is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Bingo includes 308 images and 370 questions. There are three types of biases (Region Bias, OCR Bias, and Factual Bias) and two types of inference challenges (Image-to-Image, Text-to-Image) in Bingo. We say that a VLM is robust if the VLM answers the given question correctly according to the image.

We use only all five subsets in the benchmark: Region Bias, OCR Bias, and Factual Bias, Image-to-Image Inference, and Text-to-Image Inference.

**Prompt**: Describe this image. What's the name of the building shown in the image?

**Reference**: The image shows a statue of a person on horseback, which is a common motif for equestrian status, often used to represent historical figures or military leaders. The statue is mounted on a pedestal with inscriptions, but the text is not clearly legible in the image.

VizWiz.VizWiz originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz has 20523, 4319, 8000 images and 205230, 43190, 8000 questions for training, validation, and test sets, respectively. We say that a VLM has strong visual perception if the model generates the right answer given the question and the image from VizWiz.

We use one set of VizWiz in our benchmark: the validation set.

**Prompt**: What is the expiration date?

**Reference**: January 23, 2013.

Gqa.GQA is a dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. GQA consists of 113K images and 22M questions of assorted types and varying compositionality degrees. In GQA, each image is annotated with a dense Scene Graph, representing the objects, attributes and relations it contains. Each question is associated with a functional program and is augmented with both textual and visual justifications, pointing to the relevant region within the image. The dataset is split into train, validation, test, and challenge sets. We say that a VLM has a strong reasoning ability if the matching score between the model output and the reference is high.

We use only one set of GQA in the benchmark: the validation set.

**Prompt**: Does the pot have the same color as the shirt?

**Reference**: No.

Seed Bench.SEED-Bench is a benchmark designed to evaluate the generative comprehension capabilities of Multimodal Large Language Models (MLLMs). It includes 19,000 human-annotated multiple-choice questions, spanning 12 evaluation dimensions that cover both spatial and temporal understanding across image and video modalities. The benchmark allows for objective model assessment without requiring human or GPT intervention during evaluation. SEED-Bench tests a model's ability to comprehend visual and textual information and to generate correct answers, aiming to highlight limitations and guide future research in MLLMs.

We use only two subsets in our benchmark: Visual reasoning, Instance interaction.

**Prompt:** What is the most likely season in this image, based on the appearance of the trees?

**Reference:** Fall.

Pope.The POPE (Polling-based Object Probing Evaluation) dataset is designed to evaluate object hallucination in large vision-language models (LVLMs). It converts the hallucination evaluation into a binary classification task by prompting LVLMs with simple yes-or-no questions about the presence of specific objects in images (e.g., "Is there a car in the image?"). This approach provides a stable and flexible evaluation by sampling nonexistent objects in the images and constructing questions to probe models. Three sampling strategies -- random, popular, and adversarial -- are employed to assess whether LVLMs are prone to hallucinate specific objects. The dataset helps in evaluating the extent to which LVLMs hallucinate objects that frequently appear or co-occur with other objects in visual datasets.

**Prompt:** Is there a refrigerator in the image?

**Reference:** No.

MMM.The Massive Multi-discipline Multimodal Understanding (MMMU) dataset is designed to evaluate multimodal models on tasks that require college-level subject knowledge and complex reasoning. It consists of 11,500 questions spanning six disciplines--Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Technology & Engineering. These questions cover 30 subjects and 183 subfields, and they feature highly diverse image formats, such as charts, diagrams, maps, and medical images. The dataset aims to test models' expert-level perception, reasoning, and domain-specific knowledge. Models are evaluated on how well they can understand both text and images and apply knowledge to solve problems.

We use the following subsets in our benchmark: Accounting, Agriculture, Architecture and engineering, Art, Art theory, Basic medical science, Biology, Chemistry, Clinical medicine, Computer science, Design, Diagnostics and laboratory medicine, Economics, Electronics, Energy and power, Finance, Geography, History, Literature, Manage, Marketing, Materials, Mechanical engineering, Music, Pharmacy, Physics, Psychology, Public health, Sociology.

**Prompt**: Forest Company had the following transactions during the month of December. What is the December 31 cash balance?

**Reference**: $6,090.

MME.The MME dataset is a comprehensive evaluation benchmark specifically designed for Multimodal Large Language Models (MLLMs). It contains 14 subtasks to measure both perception and cognition abilities. The dataset includes manually constructed instruction-answer pairs to avoid data leakage from publicly available datasets. The perception tasks focus on recognizing objects' existence, count, position, and color, as well as fine-grained tasks like identifying celebrities, landmarks, and artwork. The cognition tasks assess abilities like commonsense reasoning, numerical calculation, text translation, and code reasoning. MME evaluates MLLMs by requiring models to answer "yes" or "no" to concise instructions, allowing for straightforward quantitative analysis.

We use only four subsets in our benchmark: Posters, Celebrity, Artwork, Landmark.

**Prompt**: Is the actor inside the red bounding box called Jon Voight?

**Reference**: Yes.

Exams-V.EXAMS-V is a comprehensive multi-discipline, multilingual, and multimodal benchmark designed to evaluate the performance of vision-language models (VLMs). It includes 20,932 multiple-choice questions across 20 school subjects from natural sciences, social sciences, and mis-cellaneous areas like religion and fine arts. The questions are gathered from school exams worldwide, covering 11 languages from 7 language families. This dataset features multimodal elements such as images, tables, figures, and scientific symbols, making it a unique challenge requiring both visual understanding and reasoning across languages and regions.

We use only four subsets in our benchmark: Posters, Celebrity, Artwork, Landmark.

**Prompt**: The following figure shows the structure of a molecule of adenosine triphosphate.

**Reference**: B.

**MathVista.** MathVista is a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets: IQTest, FunctionQA, and PaperQA. Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models such as GPT-4V find challenging. There are four subsets of the benchmark (i.e., college, high school, elementary school, daily life) with multi-choice or open-form answer format. We say that a VLM has strong reasoning ability in math if the model generates the right answer given the question and the image in MathVista.

We use all four subsets of MathVista in our benchmark: college, high school, elementary school, daily life.

**Prompt**: The derivative of f(x) at x=2 is that at x=5.

**Reference**: Equal to.

RealWorldQA.RealWorldQA is a benchmark designed to evaluate the real-world spatial understanding capabilities of multimodal AI models, contributed by XAI. It assesses how well these models comprehend physical environments. The benchmark consists of 700+ images, each accompanied by a question and a verifiable answer. These images are drawn from real-world scenarios, including those captured from vehicles. The goal is to advance AI models' understanding of our physical world.

VQAv2.The VQAv2 (Visual Question Answering v2) dataset is a widely-used benchmark in the field of computer vision and natural language processing, specifically designed to evaluate the ability of AI models to answer questions based on visual content. The dataset builds upon the original VQA dataset, addressing some limitations by introducing a richer and more balanced collection of image-question pairs, making it a more robust challenge for vision-language models.

VQAv2 (Robustness).The VQAv2 (Robustness) extension focuses on evaluating the robustness of visual question answering (VQA) models against language input perturbations, such as typos. This dataset introduces spelling errors and other types of linguistic disturbances in conjunction with open-ended questions about real-world images. The aim is to test how well VQA models can handle and respond to non-standard or erroneous input while maintaining accuracy. This extension seeks to enhance the model's practical usability and fault tolerance by assessing its performance under more challenging and realistic conditions.

VQAv2 (AAE).The VQAv2 dataset, particularly its AAE (African-American English) perturbation extension, focuses on enhancing the robustness of visual question answering models by introducing linguistic diversity. This extension incorporates open-ended questions about real-world images but presents them in non-standard English variations, specifically African-American English (AAE). The goal is to evaluate how well a VLM can understand and respond to such variations without bias. The inclusion of AAE aims to expose potential weaknesses in handling diverse language inputs and to improve the model's inclusivity and fairness in real-world applications.

A-Okvqa.Abridged Open Knowledge Visual Question Answering (A-OKVQA) is a dataset designed to test VQA models by introducing translation perturbation. It uses questions from the original OKVQA dataset, which require significant commonsense and world knowledge. The perturbation involves translating questions into other languages and back, assessing how well models handle linguistic variations and maintain accuracy despite these changes.
A-OKVQA (Translated).A-OKVQA (Translated) is an extension of the OKVQA dataset, specifically focusing on translation perturbation and knowledge verification. The original OKVQA dataset is known for its diverse questions that require a broad base of commonsense and world knowledge. A-OKVQA builds upon this by introducing translation perturbation, which assesses how well visual question answering (VQA) models perform when faced with language translation and translation errors.

The translation perturbation involves transforming the question text through translation, thereby introducing linguistic variations. This perturbation can affect how the model understands and responds to the question. A-OKVQA tests the model's ability to handle these changes while still relying on the original knowledge required to answer the question correctly. We say a VLM is good at multilingual tasks if the model accuracy on the A-OKVQA (Translated) is high.

A-OKVQA (Robustness).A-OKVQA (Robustness) involves applying robustness typos perturbation to the A-OKVQA dataset. This approach introduces typographical errors into the questions to test the model's robustness in handling and correctly answering questions despite these errors. The focus is on evaluating the model's resilience and ability to maintain accuracy when faced with common spelling mistakes or typos while requiring a broad base of commonsense and world knowledge.

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

The Prometheus-Vision evaluator

In this section, we describe the Prometheus-Vision evaluator [20], which is designed to assess the alignment level between the ground truth and model-generated answers. Specifically, we have selected the 13B Prometheus-Vision as our primary evaluator.3 Our rating rubrics are crafted in accordance with [20, 6]. The evaluation prompt is presented below:

Footnote 3: The official checkpoint available at https://huggingface.co/prometheus-eval/prometheus-vision-13b-v1.0

``` USE:<IMAGE> ####Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, image and a score rubric representing an evaluation criterion is given.
1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)
4. Please do not generate any other opening, closing, and explanations.

``` ##The instruction to evaluate: {PROMPT} ##Response to evaluate: {RESPONSE} ##Reference Answer (Score 5): {REFERENCE} ##Score Rubrics: [Similarity between the response and the reference.] Score 1: **The model's responses do not follow the instructions provided.** Score 2: **The resulting response follows the instructions, but the answer is completely wrong relative to the reference answer.** Score 3: **The resulting response follows the instructions, but the answer is partially wrong relative to the reference answer.** Score 4: **The resulting response follows the instructions, the overall answer is relatively perfect with only a very few errors.** Score 5: **The overall answer is completely correct compared to the reference answer, and conforms to the instructions provided.**

##Feedback: ASSISTANT: ```

In Figure A4, we give three examples of the Prometheus-Vision evaluation examples from the Vibe-Eval scenario. Furthermore, we also observe failure cases using the Prometheus-Vision evaluator, and we present two cases in Figure A5.

Figure A4: **Test examples rated by Prometheus-Vision on the Vibe-Eval scenario.** The text generations are from GPT-4o.

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_EMPTY:35]