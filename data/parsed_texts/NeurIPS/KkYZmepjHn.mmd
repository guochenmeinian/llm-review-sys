# Fast Sampling via Discrete Non-Markov Diffusion

Models with Predetermined Transition Time

Zixiang Chen Huizhuo Yuan Yongqian Li Yiwen Kou Junkai Zhang Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{chenzx19,hzyuan,yongqianl,evankou,jkzhang,qgu}@cs.ucla.edu

###### Abstract

Discrete diffusion models have emerged as powerful tools for high-quality data generation. Despite their success in discrete spaces, such as text generation tasks, the acceleration of discrete diffusion models remains under-explored. In this paper, we propose discrete non-Markov diffusion models (DNDM), which naturally induce the predetermined transition time set. This enables a training-free sampling algorithm that significantly reduces the number of function evaluations (i.e., calls to the neural network), making the sampling process much faster. Furthermore, we study the transition from finite to infinite step sampling, offering new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models. Codes are available at https://github.com/uclaml/DNDM.

## 1 Introduction

Diffusion-based generative models, as first introduced by Sohl-Dickstein et al. (2015), have shown remarkable capabilities in generating high-quality samples across various domains, including images (Ho et al., 2020; Song and Ermon, 2020), audio (Chen et al., 2020; Kong et al., 2020), and videos (Ho et al., 2022). The diffusion model utilizes an innovative approach comprising a forward process that gradually transforms training data into pure noise and a reverse process that reconstructs clean data from the noise. Throughout the training phase, the model optimizes a neural network by minimizing an objective derived from maximum likelihood estimation. Once trained, the model can generate samples using various decoding strategies, including implicit dynamics (Song et al., 2020), analytical processes (Bao et al., 2022), or differential equation solvers (Song et al., 2020; Liu et al., 2022; Lu et al., 2022). In particular, Song et al. (2020) introduced the denoising diffusion implicit model (DDIM), providing a non-Markov and de-randomized version of the Denoising Diffusion Probabilistic Model (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020), which enables faster generation of high-quality samples.

Although diffusion models were initially introduced for both discrete and continuous-state spaces (Sohl-Dickstein et al., 2015), these studies have largely focused on Gaussian diffusion processes in continuous-state spaces. Recently, Discrete Denoising Diffusion Probabilistic Models (D3PMs) (Austin et al., 2021) working in discrete-state spaces have gained increasing interest due to their applications in diverse areas such as text generation (Hoogeboom et al., 2021), medical record generation (Ceritli et al., 2023), and protein design (Gruver et al., 2024). These models, which are distinct from their Gaussian counterparts, employ discrete noises, such as the multinomial distribution, for diffusion processes. Very recently, Zheng et al. (2023) introduced a reparameterized diffusionmodel (RDM) that can improve sampling speed and sample quality in text generation tasks. However, their proposed algorithm is a training-based approach. Compared with diffusion models using Gaussian noise, discrete diffusion models remain under-studied, especially regarding training-free sampling acceleration.

In this work, we introduce a training-free approach aiming at enhancing the sampling speed of discrete diffusion models. This approach stems from a unique characteristic of discrete diffusion models: unlike continuous diffusion models, which typically employ Gaussian noise for data corruption (Ho et al., 2020; Song and Ermon, 2020; Song et al., 2020, 2020), discrete diffusion models often use categorical white noises (Hoogeboom et al., 2021; Austin et al., 2021; Zheng et al., 2023).

By delving into this special property, we develop a discrete non-Markov diffusion model, together with a design of accelerated algorithm. Notably, this new sampling technique does not require any modifications to the training objective of diffusion models and is, therefore, training-free. Our contributions are summarized as follows:

* We propose discrete non-Markov diffusion models (DNDM), which naturally induces a set of latent variables \(\mathcal{T}\), termed as the _transition time set_. This key feature enables us to develop a training-free sampling algorithm that can accelerate a large family of discrete diffusion models. Importantly, DNDM preserves the essential properties of the original discrete diffusion model: for any diffusion trajectory \(\{\mathbf{x}_{t}\}\) starting from real data \(\mathbf{x}_{0}\), it provably maintains both the marginal distribution \(q(\mathbf{x}_{t})\) and the conditional distribution \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\). Our method can accelerate the two most widely used discrete diffusion models: multinomial diffusion (Hoogeboom et al., 2021) and absorbing diffusions (Austin et al., 2021). Similar to how DDIM introduces a de-randomized, faster sampling algorithm compared to DDPM in continuous space, DNDM achieves acceleration through a predetermined transition time set in discrete space (See Table 1).
* Based on the predetermined transition time set \(\mathcal{T}\) in DNDM, we design an accelerated sampling algorithm that reduces the required number of neural network function evaluations. In a standard \(T\) time-step discrete diffusion process, while D3PM, including Multinomial (Ho et al., 2020) and absorbing state discrete sampling (Austin et al., 2021), requires evaluating the neural network function \(T\) times, our approach only requires \(|\mathcal{T}|\) function evaluations, where \(|\mathcal{T}|\) is the cardinality of the transition set \(\mathcal{T}\). Moreover, \(|\mathcal{T}|\) is provably less than \(T\) and approaches \(O(1)\) as \(T\) goes to infinity. We provide both theoretical analysis and empirical experiments showing that the improvement in the number of function evaluations (NFE) is significant. Notably, our algorithm is about \(3\times\) faster than baselines for \(T=50\) and about \(30\times\) faster for \(T=1000\) while preserving the sample quality.
* To further illustrate the effectiveness of DNDM, we explore the limit as \(T\rightarrow\infty\) and introduce an infinite-step sampling algorithm. With a pretrained neural network, we can generate an initial noise \(\mathbf{x}_{T}\) and a transition time set \(\mathcal{T}\subseteq[0,1]\) with infinitesimal spacing, such that \(|\mathcal{T}|=O(1)\). This enables the generation of the real data distribution with only \(|\mathcal{T}|\) neural network evaluations. This study offers new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.

**Notation.** We use \(|\mathcal{T}|\) to denote the cardinality of the set \(\mathcal{T}\) (excluding repeated elements). We use lowercase letters to denote scalars, boldface lowercase letters to denote vectors, and boldface uppercase letters to denote matrices. The notation \(1:N\) indicates the sequence from \(1\) through \(N\). The symbol \(\mathbf{q}\) designates the real distribution in a diffusion process, while \(\mathbf{p}\) represents the distribution during sampling. With its success probability inside the parentheses, the Bernoulli distribution is denoted by \(\mathrm{Bernoulli}(\cdot)\). We further use \(\mathrm{Cat}(\mathbf{x};\mathbf{p})\) to denote a categorical distribution over a one-hot row vector \(\mathbf{x}\) with probabilities given by the row vector \(\mathbf{p}\).

## 2 Background

In this section, we provide the background of discrete diffusion models. We begin by introducing the discrete Markov diffusion model, designed for handling categorical random variables. Specifically,

\begin{table}
\begin{tabular}{c|c|c} \hline \hline  & **Continuous** & **Discrete** \\ \hline \multirow{2}{*}{**Markov**} & DDPM & D3PM \\  & (Sohl-Dickstein et al., 2015) & Austin et al. (2021) \\ \hline \multirow{2}{*}{**Non-Markov**} & DDIM & DNDM \\  & (Song et al., 2020) & (Ours) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Cross Comparison of Diffusion Models.

consider a diffusion model trying to generate distributions over a discrete random variable \(\mathbf{x}\in\mathbb{R}^{K}\) that is one-hot encoded with \(K\) categories, i.e., \(\mathbf{x}\) can be chosen as one of \(K\) categories, and for any \(k\in[K]\), \(\mathbf{x}\) is categorized as \(k\) if \(\mathbf{x}\) aligns with the standard basis vector \(\mathbf{e}_{k}\). The sequence \(\{\mathbf{x}_{t}\}_{t=0}^{T}\) represents how this random variable changes over time \(0\leq t\leq T\), starting from an \(\mathbf{x}_{0}\in\mathbb{R}^{K}\) drawn from the real distribution \(\mathbf{q}_{\mathrm{data}}\). In this paper, we focus on the two most widely used D3PMs: multinomial diffusion (Hoogeboom et al., 2021) and absorbing diffusions (Austin et al., 2021).

**Forward Process.** During the forward process, the real distribution \(\mathbf{q}_{\mathrm{data}}\) is gradually transformed into a noise distribution named \(\mathbf{q}_{\mathrm{noise}}\). The transformation occurs through \(T\) steps, with \(T\) intermediate latent variables \(\mathbf{x}_{1},\ldots\mathbf{x}_{T}\) and update rules given by:

\[\mathbf{x}_{t}=b_{t}\mathbf{x}_{t-1}+(1-b_{t})\mathbf{w}_{t},\qquad t=1, \ldots,T\] (1)

Here \(b_{t}\) is randomly drawn from a Bernoulli distribution with parameter \(\beta_{t}\), denoted by \(b_{t}\sim\mathrm{Bernoulli}(\beta_{t})\), and \(\mathbf{w}_{t}\) is randomly drawn from the noise distribution \(\mathbf{q}_{\mathrm{noise}}\), while for different \(t\) the samples are independent. In this work, we focus on cases where the noise \(\mathbf{q}_{\mathrm{noise}}\) can be either a uniform distribution over the vocabulary \(\{1,2,\ldots,K\}\)(Hoogeboom et al., 2021), or a point mass with all of the probability mass lying on an absorbing state (Austin et al., 2021). Following this notation, the process in (1) defines a Markov process characterized by the transition kernel

\[q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{ p}=\beta_{t}\mathbf{x}_{t-1}+(1-\beta_{t})\mathbf{q}_{\mathrm{noise}}\big{)}.\] (2)

Moreover, the Markov chain property allows us to get samples \(\mathbf{x}_{0:t}\) from \(\mathbf{x}_{0}\) by multiplying the transition probabilities at each step as \(p(\mathbf{x}_{1:t}|\mathbf{x}_{0})=\prod_{i=1}^{t}q(\mathbf{x}_{t}|\mathbf{x} _{t-1})\). It further leads to the following marginal distribution.

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{ p}=\alpha_{t}\mathbf{x}_{0}+(1-\alpha_{t})\mathbf{q}_{\mathrm{noise}}\big{)},\] (3)

where \(\alpha_{t}:=\prod_{s=1}^{t}\beta_{s}\) is determined by the sequence of \(\beta_{t}\) of our choice and decreases from \(1\) to \(0\).

**Reverse Process.** Given the forward Markov process, the reverse process can be derived by Bayes' rule (Hoogeboom et al., 2021; Austin et al., 2021; Zheng et al., 2023). The conditional probability \(q(\mathbf{x}_{t-1}|\mathbf{x}_{0},\mathbf{x}_{t})\) can be determined by \(q(\mathbf{x}_{t-1}|\mathbf{x}_{0},\mathbf{x}_{t})=q(\mathbf{x}_{t}|\mathbf{x} _{t-1})q(\mathbf{x}_{t-1}|\mathbf{x}_{0})/q(\mathbf{x}_{t}|\mathbf{x}_{0})\). The reverse process can be used for synthetic data generation by sampling from the noise distribution \(q_{\mathrm{noise}}\) and repeatedly applying a learned predictor (neural network) \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\) parameterized by \(\boldsymbol{\theta}\):

\[p_{\boldsymbol{\theta}}(\mathbf{x}_{T})=q_{\mathrm{noise}}(\mathbf{x}_{T}), \qquad q_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}|\mathbf{x}_{t})=\int_{\tilde{ \mathbf{x}}_{0}}q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\widehat{\mathbf{x}}_{0}) p_{\boldsymbol{\theta}}(\widehat{\mathbf{x}}_{0}|\mathbf{x}_{t})d\widehat{ \mathbf{x}}_{0}.\] (4)

We note that the reverse process \(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\widehat{\mathbf{x}}_{0})\) is stochastic and thus requires function evaluation at every step.

**Training the Neural Network.** The neural network \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\) that predicts \(\widehat{\mathbf{x}}_{0}\) is trained by maximizing the evidence lower bound (ELBO) (Sohl-Dickstein et al., 2015),

\[\log p_{\theta}(\mathbf{x}_{0}) \geq\mathbb{E}_{q(\mathbf{x}_{1:T}|\mathbf{x}_{0})}\Big{[}\log \frac{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_ {0})}\Big{]}d\mathbf{x}_{1:T}\] \[=\mathbb{E}_{q(\mathbf{x}_{1}|\mathbf{x}_{0})}[\log p_{ \boldsymbol{\theta}}(\mathbf{x}_{0}|\mathbf{x}_{1})]-\sum_{t=2}^{T}\mathbb{E}_ {q(\mathbf{x}_{t}|\mathbf{x}_{0})}[\mathrm{KL}(q(\mathbf{x}_{t-1}|\mathbf{x} _{t},\mathbf{x}_{0})\|p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}|\mathbf{x}_{t }))\] \[\qquad-\mathbb{E}_{q(\mathbf{x}_{T}|\mathbf{x}_{0})}\mathrm{KL}(q( \mathbf{x}_{T}|\mathbf{x}_{0})\|p_{\boldsymbol{\theta}}(\mathbf{x}_{T})),\] (5)

Here KL denotes Kullback-Liebler divergence and the last term \(\mathbb{E}_{q(\mathbf{x}_{T}|\mathbf{x}_{0})}\mathrm{KL}(q(\mathbf{x}_{T}| \mathbf{x}_{0})\|q_{\mathrm{noise}}(\mathbf{x}_{T}))\) equals zero. Building on this foundation, Austin et al. (2021) introduced an auxiliary denoising objective, which refines the data predictions \(\mathbf{x}_{0}\) at each time step. Since this paper primarily focuses on reverse sampling, we leave detailed discussions of these losses to Appendix B.

## 3 Discrete Non-Markov Diffusion Models (DNDM)

### Forward and Reverse Process

In this section, we introduce a non-Markov process such that the joint distribution of \((\mathbf{x}_{0},\mathbf{x}_{t})\) remains the same as the one defined with Markov process in Section 2. The new process aims to gradually transform input data \(\mathbf{q}_{\mathrm{data}}\) to the noise distribution \(\mathbf{q}_{\mathrm{noise}}\) through \(T\) intermediate latent variables \(\mathbf{x}_{1},\ldots\mathbf{x}_{T}\) with the following process:

\[\mathbf{x}_{t}=b_{t}\mathbf{x}_{t-1}+(1-b_{t})\mathbf{w},\] (6)

where \(b_{t}\) is independently drawn from the Bernoulli distribution \(\mathrm{Bernoulli}(\beta_{t})\) and \(\mathbf{w}\) is drawn from the noise distribution \(\mathbf{q}_{\mathrm{noise}}\). The only difference between (6) and (1) is that we replace \(\mathbf{w}_{t}\) in (1) by \(\mathbf{w}\), which is time-invariant during the diffusion. Therefore, the process in (6) becomes non-Markov since \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1},\ldots,\mathbf{x}_{0})\) doesn't necessarily equals \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})\). The following theorem shows that the conditional distribution \(q(\mathbf{x}_{t}|\mathbf{x}_{0})\) remains unchanged.

**Theorem 3.1**.: _For the non-Markov process in (6), we have_

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{p} =\alpha_{t}\mathbf{x}_{0}+(1-\alpha_{t})\mathbf{q}_{\mathrm{noise}}\big{)},\]

_where \(\alpha_{t}:=\Pi_{i=1}^{s}\beta_{s}\) is specified to decrease from \(1\) to \(0\)._

Using the Bayes' rule, we have \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\propto q(\mathbf{x}_{t}|\mathbf{x}_{0})q( \mathbf{x}_{0})\). Consequently, the condtional distribution \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) remains consistent with the one induced by the process process in (1). Therefore, neural network \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\) trained by the Markov process in (1), remains applicable to our non-Markov process (6) (see Appendix B for detail).

Based on the discrete non-Markov diffusion model, we can give a simple characterization of the reverse process by introducing the transition time.

**Definition 3.2**.: Transition time \(\tau\) is the time that the token \(\mathbf{x}_{t}\) transition from \(\mathbf{x}_{0}\) to noise, i.e., \(\tau:=\min_{t}\{t|b_{t}=0\}\).

_Remark 3.3_.: The concept of transition time has also been introduced in Hoogeboom et al. (2021a). However, Hoogeboom et al. (2021a) restricts the transition time to be the first time of entering the absorbing state, which is only applicable to absorbing diffusion. Our definition is more general and applicable to discrete diffusion with various noise including multinomial diffusion.

Given the transition time \(\tau\), the forward process reduces to:

\[\mathbf{x}_{t}=\mathds{1}(\tau>t)\mathbf{x}_{0}+\mathds{1}(\tau \leq t)\mathbf{w},\] (7)

which shows that the token will be a real token \(\mathbf{x}_{0}\) before the time \(\tau\) and will be the noise \(\mathbf{w}\) after the transition time. Since token only get changed at the transition time \(\tau\), we can derive a reverse process based on (7),

\[\mathbf{x}_{t-1}=\mathds{1}(\tau=t)\mathbf{x}_{0}+\mathds{1}( \tau\neq t)\mathbf{x}_{t}.\] (8)

Therefore, the process in (8) is de-randomized given transition time \(\tau\). Specifically, after independently sampled transition times \(\tau\), \(\mathbf{x}_{t-1}\) becomes deterministically known and fixed if we observe \(\mathbf{x}_{0}\) and \(\mathbf{x}_{t}\). It is also worth noting that given \(\mathbf{x}_{0}\) and \(\tau\), the exact reverse process (8) is Markovian, since \(\mathbf{x}_{t-1}\) solely depends on \(\mathbf{x}_{0},\tau,\mathbf{x}_{t}\). Plugging (8) into (4) gives the generation process. We can prove the ELBO of the DNDM is equivalent to the ELBO of the original process (5) up to some constant, which further supports the neural network \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\) trained by the Markov process in (1), remains applicable to DNDM. (See Appendix B.3 for details).

_Remark 3.4_.: (7) and (8) suggest that even though there are \(T\) distinct time steps, not every time in the range \(1:T\) is crucial for capturing the process. Therefore, our primary focus should be on the most significant time step, i.e., the transition time \(\tau\), enabling faster reverse sampling. We further note that although transition happens only at time \(\tau\), the transition time is random, differs across runs, and covers the full range from \(1\) to \(T\) on average.

_Remark 3.5_.: While Song et al. (2020a) proposed a non-Markov multinomial diffusion model in Appendix A, DDIM and DNDM are fundamentally different models when specialized to multinomial diffusion. DDIM's discrete process remains stochastic at every step, even with deterministic noise scheduling. In contrast, DNDM achieves full de-randomization by pre-determined transition time \(\tau\) (Equation 8 in our paper). By sampling these transition times upfront, DNDM establishes a predetermined transition time set that guides the sampling process, enabling deterministic evolution and faster sampling speed even under the same number of sampling steps, which is not reported under DDIM framework. For detailed technical comparison, see Appendix B.1.

### Accelerated Reverse Sampling

In this section, we demonstrate that sampling from DNDM can lead to accelerated reverse sampling. Although our algorithm is quite general, we focus on text generation in the presentation.

In Section 3.1, we only consider the case of a single token \(\mathbf{x}\in\mathbb{R}^{K}\) being one hot encoding of \(K\) categories. In real applications, we are interested in generating a sentence with multiple tokens. So, we extend the terminology in Section 3.1, and we denote the sequence of tokens at \(t\)-th time step to be \(\mathbf{x}_{t,1:N}=[\mathbf{x}_{t,1},\ldots,\mathbf{x}_{t,N}]\) where \(\mathbf{x}_{t,n}\) is the \(n\)-th token and \(N\) is the sequence length. The noise will be added to each token in a sequence independently. Therefore, each token will have its own transition time defined in Definition 3.2. We denote the transition time for each token \(\mathbf{x}_{n}\) to be \(\tau_{n}\) and further denote the transition time set \(\mathcal{T}:=\{\tau_{n}\}_{n=1}^{N}\). Given the transition times \(\tau_{n}\in\mathcal{T}\), our DNDM can now be extended to the sequence with multiple tokens

\[\mathbf{x}_{t-1,n}=\mathds{1}(\tau_{n}=t)\mathbf{x}_{0,n}+\mathds{1}(\tau_{n} \neq t)\mathbf{x}_{t,n},\forall n\in[N].\] (9)

**Learning the Reverse Process.** We first generate the transition times \(\tau_{n}\) for \(n\in[N]\), then we follow (9) to generate the learned reverse process. Since \(\mathbf{x}_{0,n}\) is unknown in the process, we use the neural network evaluation \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\) obtained in Section 3.1 to predict \(\mathbf{x}_{0,n}\). In detail, the noisy sequence \(\mathbf{x}_{t,1:N}\) is fed into \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t,1:N})\) and the prediction tokens \(\widehat{\mathbf{x}}_{0,1:N}\sim p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t,1 :N})\) are collected.

**Transition time.** Transition time, denoted by \(\tau\), is crucial in our reverse process. This is because the reverse sampling becomes deterministic upon using (9). Each instance of transition time \(\tau\) is a random variable within the set \(\{1,2,\ldots,T\}\). Let's assume it follows the distribution \(\mathcal{D}_{\tau}\). Given the schedule \(\{\alpha_{t}\}_{t=0}^{T}\), we can derive the distribution for \(\mathcal{D}_{\tau}\).

**Theorem 3.6**.: _Each specific transition time \(\tau_{n}\) in Definition 3.2 is independent. Furthermore, they collectively adhere to the distribution \(\mathcal{D}_{\tau}\), which obeys the rule \(\mathbb{P}(\tau_{n}=t)=\alpha_{t-1}-\alpha_{t}\)._

From Theorem 3.6, we discern that the nature of the diffusion model scheduler, \(\alpha_{t}\), clarifies the distribution of \(\tau\). Take the linear schedule as an example, as given by Austin et al. (2021), the relationship is \(\alpha_{t}=1-t/T\). This translates to \(\mathbb{P}(\tau_{n}=t)=1/T\) for every \(t\) in the range \(1\) to \(T\). As a result, transition time distributes uniformly across each moment in the set \(\{1,\ldots,T\}\). Generally, if we express \(\alpha_{t}\) as \(g(t/T)\), then we can simplify to \(\mathbb{P}(\tau_{n}=t)=g((t-1)/T)-g(t/T)\), which further refines to \((1/T)|g^{\prime}(t/T)|+o(1/T)\). This indicates that transitions are more likely where \(|g^{\prime}|\) is large.

In practice, we observed that the shape of the transition time does not need to exactly match the theoretically predicted schedule \(\mathcal{D}\tau\) in Theorem 3.6. Algorithm 1 works even if \(\mathcal{D}\tau\) is unknown. In particular, we can approximate the schedule with a Beta distribution by first sampling a time \(t\in[0,1]\) from a Beta distribution, then adjusting these samples to fit by multiplying by \(T\) and rounding the result to obtain an integer.

**Accelerated Sampling.** According to (9), a token \(\mathbf{x}_{t-1,n}\) is updated only if step \(t\) is the transition time for the \(n\)-th token. If step \(t\) is not the transition time for any token, the sentence from the previous step can be directly copied: \(\mathbf{x}_{t-1,1:N}=\mathbf{x}_{t,1:N}\). As a result, there is no need to do a function evaluation for the current step. Our attention, therefore, can be solely centered on the transition set \(\mathcal{T}\), necessitating function evaluations only for \(t\) within \(\mathcal{T}\). For our method, when \(N\) is fixed while \(T\rightarrow\infty\), the total NFE \(|\mathcal{T}|\) will reach \(N\). On the other hand, when \(T\) is fixed and \(N\rightarrow\infty\), the NFE \(\mathcal{T}\) will reach \(T\) (See Theorem D.1 for detail). It is worth noting that the auto-regressive diffusion model (ARDM) (Hoogeboom et al., 2021) can also achieve at most \(N\) NFE when \(T=\infty\). However, ARDM only focuses on infinite time steps, while our method here isable to accelerate sampling for finite time steps. More detailed discussion and theoretical analysis can be found in Section D, where additional experiments also demonstrate that our DNDM achieves an NFE that is less than half of the original Markov sampling method for discrete diffusion.

By incorporating the forward process with different noises, we can develop DNDM-Multi and DNDM-Absorb, which accelerate the Multinomial and Absorbing sampling methods respectively. Recent works have demonstrated that the quality of samples can be enhanced by utilizing supplementary information derived from the neural network, (Ghazvininejad et al., 2019; Savinov et al., 2021; Chang et al., 2022; He et al., 2022; Zheng et al., 2023). Our DNDM can also be improved using this idea. We call it a discrete non-Markov Diffusion Model with Top-k Transition Time (DNDM-\(k\)). Due to the limit of the pages, we leave the detailed Algorithm and discussion to Appendix E.

### Continous-time (Infinite Step) Reverse Sampling

In the context of continuous state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021; Zhang and Chen, 2022; Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020; Dockhorn et al., 2021). However, the application of continuous-time schemes to discrete-state spaces remains largely unexplored. Campbell et al. (2022) first developed a continuous framework for discrete-time diffusion for the Markovian process and randomized sampling, but not in our non-Markovian setting. In this section, we investigate the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.

**Continuous-time Forward and Backward process.** Recall that the forward process described in (6) can be sampled from \(\mathbf{x}_{0,n}\) through the following process:

\[\mathbf{x}_{t,n}=\alpha_{t}\mathbf{x}_{0,n}+(1-\alpha_{t})\mathbf{q}_{\text{ noise}},\quad\alpha_{t}=\prod_{i=1}^{t}\beta_{i}.\] (10)

In the previous section, we are constrained to discrete time steps, where we must define a maximum step, denoted by \(T\). The values of \(\mathbf{x}_{t}\) are computed only for \(t=1,\ldots,T\). As a result, during the training process, it is only possible to predict \(\mathbf{x}_{0}\) at these predetermined time steps. This constraint confines the computation of our reverse process exclusively to these fixed time stamps. To derive the continuous limit of (10), for each \(T\) we rescale (10) to a diffusion process on \([0,1]\), e.g., \(\mathbf{x}_{T,n}=\widehat{\mathbf{x}}_{1,n},\mathbf{x}_{0,n}=\widehat{\mathbf{ x}}_{0,n}\), and \(\mathbf{x}_{t,n}=\widehat{\mathbf{x}}_{t/T,n}\). Therefore, when \(T\rightarrow\infty\), \(\widehat{\mathbf{x}}_{t,n}\) represents the continuous process that has values at arbitrary \(t\in[0,1]\). If the choice of \(\alpha_{t}\) for each \(T\) is scale-invariant, we can define a continuous function \(\alpha(t)\) as the continuous \(\alpha\) schedule of the discrete counterpart1. More specifically, we obtain

Footnote 1: If we represent \(\alpha_{t}\) with maximum step \(T\) as \(\alpha_{t}(T)\), the scale-invariant property states that \(\alpha_{ct}(cT)=\alpha_{t}(T)\). The simplest example of such an \(\alpha_{t}\) schedule is \(\alpha_{t}(T)=1-t/T\), under which \(\alpha(t)=1-t\).

\[\widehat{\mathbf{x}}_{t,n}=\alpha(t)\widehat{\mathbf{x}}_{0,n}+(1-\alpha(t)) \mathbf{q}_{\text{noise}},\quad t\in[0,1].\] (11)

For the reverse-time process, we define the transition time set \(\mathcal{T}:=\{\tau_{n}\}_{n=1}^{N}\) consistent with Theorem 3.6 and sample it from \(\mathbb{P}(\tau_{n}=t)=-\alpha^{\prime}(t)\) (we always use decreasing \(\alpha(t)\)). With \(\mathcal{T}\) defined, the updates to \(\mathbf{x}_{t,n}\) only occur at \(\{\tau_{n}\}\). Consequently, we arrange \(\tau_{n}\) to obtain an ordered sequence \(\tau_{n_{k}}\), where \(\tau_{n_{1}}<\tau_{n_{2}}<\ldots<\tau_{n_{N}}\). When omitting the infinitely many time steps between \(\tau_{n_{k}}\) and \(\tau_{n_{k-1}}\), the resulting reverse process is then given by:

\[\mathbf{x}_{\tau_{n_{k-1}},n}=\mathds{1}(\tau_{n}=\tau_{n_{k-1}})\mathbf{x}_{ 0,n}+\mathds{1}(\tau_{n}\neq\tau_{n_{k-1}})\mathbf{x}_{\tau_{n_{k}},n},.\] (12)

for all \(n\in[N]\). The detailed algorithm named DNDM-C is shown in Algorithm 2.

```
0: Trained prediction function \(p_{\boldsymbol{\theta}}\), \(\mathbf{q}_{\text{noise}}\), \(\mathcal{D}_{\tau}\)
1:for\(n=1\ldots N\)do
2: Initiate each token \(\mathbf{x}_{T,n}\sim\mathbf{q}_{\text{noise}}\)
3: Initiate the transition time \(\tau_{n}\sim\mathcal{D}_{\tau}\) and order them as \(\tau_{n_{1}}<\ldots<\tau_{n_{N}}\)
4:endfor
5:for\(k=N\ldots 1\)do
6: Generate \(\widehat{\mathbf{x}}_{0,1:N}\) from \(p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{\tau_{n_{k}},1:N},\tau_{n_{k}})\)
7:for\(n=1\ldots N\)do
8: Update \(\mathbf{x}_{\tau_{n_{k-1}},n}\) based on condition of \(\tau_{n}\)
9:endfor
10:endfor
11:Return\(\mathbf{x}_{0,1:N}\) ```

**Algorithm 2** Sampling from DNDM-C

_Remark 3.7_.: Autoregressive Diffusion Model (ARDM) (Hoogeboom et al., 2021a) is a discrete diffusion model built upon the autoregressive nature of data. ARDM is shown to be equivalent to a continuous-time absorbing diffusion model and thus provides a unique perspective for discrete diffusion. For continuous-time (\(T=\infty\)) reverse sampling, both ARDM and our method achieve \(N\) NFEs. Unlike ARDM which is limited to absorbing-state transitions, our method provides a unified framework including both absorbing and multinomial diffusions, applicable to both finite time and continuous time diffusions. For infinite timesteps, Hoogeboom et al. (2021a) also proposed an advanced parallelizing technique that can reduce NFE according to the log-likelihood, which we have not considered in DNDM-C.

## 4 Experiments

In this section, we evaluate DNDM and demonstrate its superior performance on two types of tasks: conditional sequence-to-sequence text generation (i.e., machine translation) and unconditional text generation. For the fairness of comparison, all the experiments are conducted using a single NVIDIA RTX A6000 GPU with 48 GB memory. Additional experiment details are provided in Appendix F.

### Conditional Text Generation

We evaluate DNDM's effectiveness on conditional text generation through machine translation tasks. Following Zheng et al. (2023), we use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to create a shared vocabulary of words and subwords from both source and target languages. We implement our experiments using FairSeq (Ott et al., 2019), which employs an encoder-decoder architecture. The model uses bi-directional self-attention blocks without causal masking, allowing tokens to attend to both past and future positions during training and inference. The encoder processes the source text, while the decoder generates the target translation.

**Datasets.** We use the following three datasets to compare with the baselines for machine translation tasks: (1) IWSLT14 DE-EN (Cettolo et al., 2014), a dataset with German as the source language and English as the target language. It consists of \(174272\) examples (sentence pairs), and each of the validation set and the testing set accounts for \(7283\) and \(6750\) of the dataset; (2) WMT14 EN-DE (Bojar et al., 2014), which is an English-to-German translation dataset consisting of \(3967182\) examples. Each of the validation set and the testing set accounts for \(3000\) and \(3003\) of the dataset; and (3) WMT16 EN-RO (Bojar et al., 2016), which is an English-to-Russian translation dataset consisting of \(612317\) examples. Each of the validation sets and the testing set accounts for \(1999\) and \(1999\) of the dataset. The train-validation-test split is fixed across all experiments for all machine translation datasets to ensure fair comparison.

**Performance Metrics.** We use the BLEU score (Papineni et al., 2002) to evaluate the machine translation quality, where the BLEU score is calculated based on the similarity between the actual target sequence and the predicted target sequence. The sampling speed is measured by wall-clock time (in second).

**Baselines.** The main baselines we are comparing with are RDM and RDM-\(k\) from Zheng et al. (2023). Here, we use RDM-\(k\) and RDM to denote the sampling method proposed in their paper with and without the usage of top-\(k\) selection for the token generation technique (see Appendix E for more details), respectively. RDM and RDM-\(k\) are applied to two previously proposed state-of-the-art discrete diffusion models: Multinomial Diffusion (Hoogeboom et al., 2021b) and Absorbing Diffusion (Austin et al., 2021).

**Results and Discussion.** Tables 2 and 3 present the performance evaluations of our algorithms in machine translation tasks. Table 2 presents results for multinomial diffusion, while Table 3 displays results for absorbing diffusion. Our reported time and BLEU scores are averaged over 5 repeated experiments, except for the baseline RDM experiment2.

Footnote 2: Due to computational intensity, we did not repeat the 1000-step sampling for the RDM baseline. However, reproducing it was deemed unnecessary as the sampling time is largely stable across repeated experiments, and the precise averaged timing is not critical for demonstrating the speed improvement of DNDM.

From Tables 2 and 3, we observe that methods based on DNDM significantly accelerate the sampling process compared to baseline diffusion models. This acceleration allows for greater flexibility in increasing the number of steps (up to infinity) without imposing a significant computational burden.

In particular, more sampling steps lead to better generation quality (BLEU) at the expense of longer sampling time, as indicated in each column of Tables 2 and 3. For RDM-based methods, generation time increases linearly with the number of sampling steps. On the contrary, for our DNDM-based method, generation time only increases marginally (See Figure 4 in Section G). As a result of the difference in the growing speed of sampling time with respect to sampling steps, the more sampling steps, the more speedup DNDM can obtain.

Continuous-time results, as the ultimate limit of increasing sampling steps, are presented in the last row of each dataset with the tag \(\infty\). Given that the results with 1000 steps consistently outperform those with 50 steps, we compare \(\infty\) with 1000 steps in Table 2 and 3. For IWSLT14 and WMT16, where the generation BLEU score is relatively high, we observe a consistent performance improvement of up to \(0.3\) in BLEU score when utilizing the DNDM-C algorithm, with the exception of a single case in the absorbing diffusion setting for WMT16 without the use of top-\(k\) selection. The performance gain of the continuous-time method on WMT14 is less significant, with both drops and gains. However, WMT14 itself has not reached a high level of performance, with a BLEU score significantly lower than other datasets. In general, training WMT14 poses challenges across all diffusion models, including multinomial diffusion (Hoogeboom et al., 2021), absorbing diffusion (Austin et al., 2021), and RDM diffusion (Zheng et al., 2023), etc. We defer a more detailed discussion on WMT14 to Appendix F.1. Finally, when compared with the results obtained with 50 steps, the performance of DNDM-C demonstrates improvement consistently. Furthermore, we note that regardless of the dataset or the method (i.e., RDM or DNDM) employed, top-\(k\) token generation consistently outperforms vanilla methods. This approach enhances the BLEU score by approximately \(1\)-\(2\) points without introducing significant increases in sampling time.

Scaling Law in Sampling Speed.For illustrative purposes, we use the example of IWSLT14 to visualize how the sample quality scales regarding sampling speed for different methods. In Figure 1, we observe the trend of the BLEU score in relation to computational time. Each line in the legend represents a different sampling algorithm, and a steeper slope indicates a larger marginal gain when sampling for longer periods. Figure 1 demonstrates that our algorithm displays nearly linear growth in BLEU score over the log of time, which is remarkable in contrast with the flat curve of the baseline. Particularly, for multinomial diffusion, the BLEU score increases by 1 in less than 60 seconds of additional sampling time. For absorbing diffusion, DNDM outperforms RDM before RDM samples 50 steps. In Tables 7 and 8 in Appendix D, we further use the average number of function evaluations (NFE) to measure the improved speed within the specified number of sampling steps. Additionally, in Figure 2, we visualize how the BLEU score and the generated text change throughout the sampling process.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Steps**} & \multicolumn{2}{c|}{**RDM-Multi**} & \multicolumn{2}{c|}{**DNDM-Multi**} & \multicolumn{2}{c|}{**RDM-\(k\)-Multi**} & \multicolumn{2}{c}{**DNDM-\(k\)-Multi**} \\ \cline{3-10}  & & **BLEU** & **Time (s)** & **BLEU** & **Time (s)** & **BLEU** & **Time(s)** & **BLEU** & **Time (s)** \\ \hline \multirow{4}{*}{\begin{tabular}{c} IWSLT14 \\ (6.75k) \\ \end{tabular} } & 25 & **31.26** & 166.9 & 30.95 & **52.9** & **32.82** & 161.9 & 32.30 & **52.6** \\  & 50 & **31.50** & 328.6 & 31.45 & **83.9** & **32.82** & 321.2 & 32.80 & **93.2** \\  & 1000 & 31.69 & 6308.9 & **31.82** & **191.3** & 32.64 & 6321.3 & **33.15** & **191.5** \\  & \(\infty\) & - & - & **31.89** & **225.2** & - & - & **33.44** & **228.1** \\ \hline \multirow{4}{*}{\begin{tabular}{c} WMT14 \\ (3k) \\ \end{tabular} } & 25 & **25.25** & 237.3 & 25.01 & **90.7** & **26.03** & 230.9 & 25.98 & **90.5** \\  & 50 & **25.75** & 466.1 & 25.33 & **138.4** & 26.14 & 500.2 & **26.37** & **138.3** \\  & 1000 & 25.66 & 8996.7 & **25.71** & **265.4** & 25.82 & 8991.7 & **26.88** & **265.5** \\  & \(\infty\) & - & - & **24.79** & **307.5** & - & - & **26.39** & **307.3** \\ \hline \multirow{4}{*}{
\begin{tabular}{c} WMT16 \\ (2k) \\ \end{tabular} } & 25 & **32.29** & 145.2 & 31.97 & **36.4** & **33.12** & 143.5 & 32.94 & **36.4** \\  & 50 & **32.53** & 286.1 & 32.50 & **63.2** & **33.41** & 312.4 & 33.26 & **62.7** \\ \cline{1-1}  & 1000 & 32.63 & 5588.9 & **32.86** & **171.4** & 33.67 & 5601.0 & **33.79** & **171.2** \\ \cline{1-1}  & \(\infty\) & - & - & **32.91** & **196.4** & - & - & **33.86** & **196.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: BLEU score comparison of multinomial diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k).

### Unconditional Text Generation

For unconditional text generation, we evaluate our approach on language modeling tasks, where the model learns to generate text that matches the statistical patterns of the training data. Unlike conditional generation, this task involves directly learning \(q(\mathbf{x}_{0}|\mathbf{x}_{t})\) without conditioning on any input text. We conduct experiments on the text8 and enwik8 datasets using a decoder-only architecture similar to GPT models. Since unconditional generation does not require encoding input sequences, we employ a 12-layer Transformer decoder without an encoder component.

**Datasets.** The natural language generation task is evaluated on two language datasets following Hoogeboom et al. (2021): text8 and enwik8. Both datasets are from Wikipedia, but their contents are highly distinct. In text8, the plain text consists of English words (all the letters are in lower case) and spaces, and it is tokenized into 26 characters and one blank space, resulting in 27 categories. In contrast to the cleanness of text8, enwik8 preserves the original XML dump contents, and there exist various special symbols in its raw text, so its text is tokenized into 1 Byte, resulting in 256 categories. We utilize text8 dataset with sequence length 256 and enwik8 dataset with sequence length 320. The train/val/test splits are \(9\mathrm{e}7/5\mathrm{e}6/5\mathrm{e}5\) for both text8 and enwik8.

**Performance Metrics.** Our evaluation of text generation quality relies on the perplexity score. When generating text8 data, we calculate perplexity scores using the GPT2 model, while for enwik8 data generation, we employ the GPT2-large model. The sampling speed is measured in seconds.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Steps**} & \multicolumn{2}{c|}{**RDM-Absorb**} & \multicolumn{2}{c|}{**DNDM-Absorb**} & \multicolumn{2}{c|}{**RDM-\(k\)-Absorb**} & \multicolumn{2}{c}{**DNDM-\(k\)-Absorb**} \\ \cline{3-10}  & & **BLEU** & **Time (s)** & **BLEU** & **Time (s)** & **BLEU** & **Time(s)** & **BLEU** & **Time (s)** \\ \hline \multirow{4}{*}{IWSLT14} & 25 & 31.58 & 116.3 & **32.43** & **67.2** & **34.50** & 108.9 & 34.14 & **67.3** \\  & 50 & 31.80 & 227.2 & **32.63** & **95.9** & **34.58** & 213.9 & 34.34 & **96.2** \\  & 1000 & 31.91 & 4197.4 & **32.93** & **161.1** & **34.60** & 4205.9 & 34.56 & **162.3** \\  & \(\infty\) & - & - & **33.03** & **174.6** & - & - & **34.65** & **180.7** \\ \hline \multirow{4}{*}{WMT14 (3k)} & 25 & 24.97 & 116.4 & **25.79** & **68.1** & **27.50** & 107.5 & 27.18 & **68.0** \\  & 50 & 24.95 & 231.1 & **26.10** & **102.0** & **27.73** & 255.2 & 27.66 & **102.5** \\  & 1000 & 25.22 & 4169.4 & **26.43** & **178.3** & 27.75 & 4167.4 & **27.82** & **179.1** \\  & \(\infty\) & - & - & **26.50** & **180.1** & - & - & **27.50** & **181.2** \\ \hline \multirow{4}{*}{WMT16 (2k)} & 25 & 32.86 & 75.5 & **33.20** & **41.2** & 33.92 & 69.9 & **33.96** & **41.4** \\  & 50 & 32.93 & 148.4 & **33.30** & **62.5** & 34.10 & 166.1 & **34.20** & **62.7** \\ \cline{1-1}  & 1000 & 33.25 & 2951.7 & **33.60** & **121.3** & **34.44** & 2718.7 & 34.38 & **122.7** \\ \cline{1-1}  & \(\infty\) & - & - & **33.42** & **121.8** & - & - & **34.41** & **121.9** \\ \hline \hline \end{tabular}
\end{table}
Table 3: BLEU score comparison of absorbing diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k).

Figure 1: Generation quality to generation time comparison on IWSLT14. \(x\)-axis: computational time in seconds; \(y\)-axis: BLEU score.

**Baselines.** We compare our proposed DNDM on unconditional text generation task with the vanilla Multinomial Diffusion (Hoogeboom et al., 2021). **Results and Discussion.** Table 4 displays the performance of our algorithms in text generation tasks. We run the multinomial diffusion model on the text8 dataset for 1000 diffusion steps and on the enwik8 dataset for 4000 diffusion steps. Our DNDM-based algorithms outperform the vanilla sampling algorithm used in Hoogeboom et al. (2021) in terms of both sampling time and perplexity score. Specifically, for the text8 dataset, DNDM-based algorithms are \(5\) times faster than the vanilla algorithm. For the enwik8 dataset, DNDM-based algorithms are 14 times faster than the vanilla algorithm.

## 5 Conclusion and Future Work

This paper presents a novel discrete non-Markov diffusion model (DNDM) accompanied by an accelerated sampling algorithm designed to boost sampling speed in a discrete-state space. Our discrete diffusion model incorporates "transition time set" latent variables, establishing itself as an efficacious diffusion and data generation method. Thanks to our acceleration technique, we significantly decrease the number of neural network function evaluations without sacrificing sample quality. We also introduce an infinite-step sampling algorithm, DNDM-C, which provides new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. While this study focuses on text generation using non-autoregressive models, a promising direction for future exploration is applying our method to other tasks, such as audio and image generation.

## Acknowledgement

We thank the anonymous reviewers and area chair for their helpful comments. ZC, HY, YL, YK, JZ, and QG are supported in part by the National Science Foundation CAREER Award 1906169, IIS-2008981, and the Sloan Research Fellowship. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline  & & Vanilla & DNDM \\ \hline \multirow{2}{*}{text8} & Perplexity & 1,465.75 & **600.02** \\  & Time (s) & 135.9 & **31.1** \\ \hline \multirow{2}{*}{enwik8} & Perplexity & 801.78 & **556.78** \\  & Time (s) & 602.8 & **47.4** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of different sampling methods for unconditional text generation (multinomial diffusion) on text8 and enwik8 benchmarks. Sampling time is computed by generating a single text sample of length 256 for text8 and length 320 for enwik8, averaged over 10 runs. The blue background represents our algorithms, and the bold number indicates the optimal value.

Figure 2: We demonstrate the 100-step generation process of DNDM-\(k\)-Multi as an example, where the left is the change of the BLEU score along the generation process, and the right is the text at different time steps. As the time goes from 100 to 0, noise is gradually removed until the corresponding English text emerges. Since the transition time follows a Beta distribution as described in Section 3.2, the majority of transitions occur near the starting time.

## References

* Alain et al. (2016)Alain, G., Bengio, Y., Yao, L., Yosinski, J., Thibodeau-Laufer, E., Zhang, S. and Vincent, P. (2016). Gsns: generative stochastic networks. _Information and Inference: A Journal of the IMA_**5** 210-249.
* ALIAS PARTH GOYAL et al. (2017)ALIAS PARTH GOYAL, A. G., Ke, N. R., Ganguli, S. and Bengio, Y. (2017). Variational walkback: Learning a transition operator as a stochastic recurrent net. _Advances in Neural Information Processing Systems_**30**.
* Austin et al. (2021)Austin, J., Johnson, D. D., Ho, J., Tarlow, D. and Van Den Berg, R. (2021). Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_**34** 17981-17993.
* Bao et al. (2022)Bao, F., Li, C., Zhu, J. and Zhang, B. (2022). Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. _arXiv preprint arXiv:2201.06503_.
* Bengio et al. (2014)Bengio, Y., Laufer, E., Alain, G. and Yosinski, J. (2014). Deep generative stochastic networks trainable by backprop. In _International Conference on Machine Learning_. PMLR.
* Bojar et al. (2014)Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L. and Tamchyna, A. (2014). Findings of the 2014 workshop on statistical machine translation. In _Proceedings of the Ninth Workshop on Statistical Machine Translation_. Association for Computational Linguistics, Baltimore, Maryland, USA.
* Bojar et al. (2016)Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., Jimeno Yepes, A., Koehn, P., Logacheva, V., Monz, C., Negri, M., Neveol, A., Neves, M., Popel, M., Post, M., Rubino, R., Scarton, C., Specia, L., Turchi, M., Verspoor, K. and Zampieri, M. (2016). Findings of the 2016 conference on machine translation. In _Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers_. Association for Computational Linguistics, Berlin, Germany.
* Bordes et al. (2017)Bordes, F., Honari, S. and Vincent, P. (2017). Learning to generate samples from noise through infusion training. _arXiv preprint arXiv:1703.06975_.
* Campbell et al. (2022)Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G. and Doucet, A. (2022). A continuous time framework for discrete denoising models. _Advances in Neural Information Processing Systems_**35** 28266-28279.
* Ceritil et al. (2023)Ceritil, T., Ghosheh, G. O., Chauhan, V. K., Zhu, T., Creagh, A. P. and Clifton, D. A. (2023). Synthesizing mixed-type electronic health records using diffusion models. _arXiv preprint arXiv:2302.14679_.
* Cettolo et al. (2014)Cettolo, M., Niehues, J., Stuker, S., Bentivogli, L. and Federico, M. (2014). Report on the 11th IWSLT evaluation campaign. In _Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign_. Lake Tahoe, California.
* Chang et al. (2022)Chang, H., Zhang, H., Jiang, L., Liu, C. and Freeman, W. T. (2022). Maskgit: Masked generative image transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.
* Chen et al. (2020)Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M. and Chan, W. (2020). Wavegrad: Estimating gradients for waveform generation. _arXiv preprint arXiv:2009.00713_.
* Chung et al. (2022)Chung, H., Sim, B. and Ye, J. C. (2022). Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.
* Dockhorn et al. (2021)Dockhorn, T., Vahdat, A. and Kreis, K. (2021). Score-based generative modeling with critically-damped langevin diffusion. _arXiv preprint arXiv:2112.07068_.
* Dockhorn et al. (2022)Dockhorn, T., Vahdat, A. and Kreis, K. (2022). Genie: Higher-order denoising diffusion solvers. _Advances in Neural Information Processing Systems_**35** 30150-30166.
* Dockhorn et al. (2021)Ghazvininejad, M., Levy, O., Liu, Y. and Zettlemoyer, L. (2019). Mask-predict: Parallel decoding of conditional masked language models. _arXiv preprint arXiv:1904.09324_.
* Gruver et al. (2024)Gruver, N., Stanton, S., Frey, N., Rudner, T. G., Hotzel, I., Lafrance-Vanasse, J., Rajpal, A., Cho, K. and Wilson, A. G. (2024). Protein design with guided discrete diffusion. _Advances in Neural Information Processing Systems_**36**.
* He et al. (2022)He, Z., Sun, T., Wang, K., Huang, X. and Qiu, X. (2022). Diffusionbert: Improving generative masked language models with diffusion models. _arXiv preprint arXiv:2211.15029_.
* Ho et al. (2022)Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J. et al. (2022). Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_.
* Ho et al. (2020)Ho, J., Jain, A. and Abbeel, P. (2020). Denoising diffusion probabilistic models. _Advances in neural information processing systems_**33** 6840-6851.
* Hoogeboom et al. (2021)Hoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B., Berg, R. v. d. and Salimans, T. (2021a). Autoregressive diffusion models. _arXiv preprint arXiv:2110.02037_.
* Hoogeboom et al. (2021b)Hoogeboom, E., Nielsen, D., Jaini, P., Forre, P. and Welling, M. (2021b). Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_**34** 12454-12465.
* Jolicoeur-Martineau et al. (2021)Jolicoeur-Martineau, A., Li, K., Piche-Taillefer, R., Kachman, T. and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. _arXiv preprint arXiv:2105.14080_.
* Karras et al. (2022)Karras, T., Aittala, M., Aila, T. and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_**35** 26565-26577.
* Kong and Ping (2021)Kong, Z. and Ping, W. (2021). On fast sampling of diffusion probabilistic models. _arXiv preprint arXiv:2106.00132_.
* Kong et al. (2020)Kong, Z., Ping, W., Huang, J., Zhao, K. and Catanzaro, B. (2020). Diffwave: A versatile diffusion model for audio synthesis. _arXiv preprint arXiv:2009.09761_.
* Liu et al. (2022)Liu, L., Ren, Y., Lin, Z. and Zhao, Z. (2022). Pseudo numerical methods for diffusion models on manifolds. _arXiv preprint arXiv:2202.09778_.
* Lu et al. (2022)Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C. and Zhu, J. (2022). Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_**35** 5775-5787.
* Lyu (2012)Lyu, S. (2012). Interpretation and generalization of score matching. _arXiv preprint arXiv:1205.2629_.
* Movellan (2008)Movellan, J. R. (2008). Contrastive divergence in gaussian diffusions. _Neural Computation_**20** 2238-2252.
* Nachmani et al. (2021)Nachmani, E., Roman, R. S. and Wolf, L. (2021). Non gaussian denoising diffusion models. _arXiv preprint arXiv:2106.07582_.
* Nichol and Dhariwal (2021)Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_. PMLR.
* Ott et al. (2019)Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D. and Auli, M. (2019). fairseq: A fast, extensible toolkit for sequence modeling. _arXiv preprint arXiv:1904.01038_.
* Papineni et al. (2002)Papineni, K., Roukos, S., Ward, T. and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_.
* Pogogorian et al. (2019)Reid, M., Hellendoor, V. J. and Neubig, G. (2022). Diffuser: Discrete diffusion via edit-based reconstruction. _arXiv preprint arXiv:2210.16886_.
* Salimans and Ho (2022)Salimans, T. and Ho, J. (2022). Progressive distillation for fast sampling of diffusion models. _arXiv preprint arXiv:2202.00512_.
* San-Roman et al. (2021)San-Roman, R., Nachmani, E. and Wolf, L. (2021). Noise estimation for generative diffusion models. _arXiv preprint arXiv:2104.02600_.
* Savinov et al. (2021)Savinov, N., Chung, J., Binkowski, M., Elsen, E. and Oord, A. v. d. (2021). Step-unrolled denoising autoencoders for text generation. _arXiv preprint arXiv:2112.06749_.
* Sennrich et al. (2016)Sennrich, R., Haddow, B. and Birch, A. (2016). Neural machine translation of rare words with subword units.
* Sohl-Dickstein et al. (2009)Sohl-Dickstein, J., Battaglino, P. and DeWeese, M. R. (2009). Minimum probability flow learning. _arXiv preprint arXiv:0906.4779_.
* Sohl-Dickstein et al. (2015)Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N. and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_. PMLR.
* Song et al. (2020a)Song, J., Meng, C. and Ermon, S. (2020a). Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_.
* Song et al. (2023)Song, Y., Dhariwal, P., Chen, M. and Sutskever, I. (2023). Consistency models. _arXiv preprint arXiv:2303.01469_.
* Song and Ermon (2019)Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_**32**.
* Song and Ermon (2020)Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. _Advances in neural information processing systems_**33** 12438-12448.
* Song et al. (2020b)Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S. and Poole, B. (2020b). Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_.
* Sun et al. (2022)Sun, H., Yu, L., Dai, B., Schuurmans, D. and Dai, H. (2022). Score-based continuous-time discrete diffusion models. _arXiv preprint arXiv:2211.16750_.
* Vahdat et al. (2021)Vahdat, A., Kreis, K. and Kautz, J. (2021). Score-based generative modeling in latent space. _Advances in Neural Information Processing Systems_**34** 11287-11302.
* Watson et al. (2021)Watson, D., Ho, J., Norouzi, M. and Chan, W. (2021). Learning to efficiently sample from diffusion probabilistic models. _arXiv preprint arXiv:2106.03802_.
* Ye et al. (2023)Ye, J., Zheng, Z., Bao, Y., Qian, L. and Gu, Q. (2023). Diffusion language models can perform many tasks with scaling and instruction-finetuning. _arXiv preprint arXiv:2308.12219_.
* Zhang and Chen (2022)Zhang, Q. and Chen, Y. (2022). Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_.
* Zheng et al. (2023)Zheng, L., Yuan, J., Yu, L. and Kong, L. (2023). A reparameterized discrete diffusion model for text generation. _arXiv preprint arXiv:2302.05737_.

## Broader Impact

This paper presents work that aims to advance the field of diffusion models. We believe this work may enable future applications of synthetic data generation, which may lead to positive impacts. Our experiments demonstrate that the proposed method achieves state-of-the-art performance in the acceleration of the generative model. However, proper controls may be needed whenever applying our method to tasks that involve sensitive data data. There may be other potential societal consequences of our work, none of which we feel must be specifically highlighted here.

## Limitations

* The scope of the empirical claims is limited to the text domain with non-auto regressive setting. The applicability and performance of DNDM for other tasks like audio and image generation, as well as with other architectures like auto-regressive GPT models, are not explored and left as future work.
* While DNDM-C, the infinite-step sampling algorithm, offers new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models, the sample quality is not guaranteed to be superior to the accelerated algorithm with \(1000\) steps. Some intuitions here: the assumption that the neural network can be optimally trained is an ideal case and is often not realized in practice. There is an inherent estimation error associated with the training process. As the number of steps increases, these estimation errors can accumulate, potentially leading to a degradation in performance. This cumulative estimation error might explain why using an infinite number of steps does not necessarily yield better results than a finite number of steps like 1000 in the conditional generation experiments. How to further improve sample quality of infinite steps is interesting but beyond the scope of this paper.
* This paper focuses on the comparison with discrete Markov diffusion models since it aims to propose an accelerated algorithm for discrete diffusion with DNDM. Other text generation models, such as continuous diffusion models or auto-regressive models, are not considered in this paper.
* This paper focuses on acceleration while maintaining good sample quality. The hyper parameter regions with poor sample qualities are not explored in this paper.

By highlighting these limitations, this paper aims to clearly scope its contributions and spark future work on addressing these important challenges with discrete diffusion models for generative modeling.

## Appendix A Related Work

**Continous Diffusion Models.** Generative modeling via continuous-time stochastic process has been investigated thoroughly in a series of work (Movellan, 2008; Lyu, 2012; Sohl-Dickstein et al., 2009; Bengio et al., 2014; Alain et al., 2016; ALIAS PARTH GOYAL et al., 2017; Bordes et al., 2017). The two lines of probabilistic modeling, _denoising diffusion probabilistic model_(Sohl-Dickstein et al., 2015; Ho et al., 2020) and _score matching with Langevin dynamics_(Song and Ermon, 2019) are unified by Song et al. (2020) through introducing the SDE framework for SGM. Based on it, subsequent works (Dockhorn et al., 2021; Nachmani et al., 2021; Vahdat et al., 2021) introduced a more complex diffusion process to improve the generation speed and quality. On the other hand, the score-based sampling process is time-consuming and has attracted much attention for improvements in speed (San-Roman et al., 2021; Watson et al., 2021; Kong and Ping, 2021; Karras et al., 2022; Song et al., 2023). "Gotta go fast" (GGF), an SDE solver with adaptive step size tailored to SGM, is proposed in Jolicoeur-Martineau et al. (2021). Song et al. (2020) introduced a non-Markov diffusion process that corresponds to a deterministic sampling process, enabling the generation of high-quality samples more rapidly. Dockhorn et al. (2022); Liu et al. (2022) proposed a high-order SDE/ODE solver to achieve lower discretization error. Lu et al. (2022); Zhang and Chen (2022) leveraged the semi-linear structure of reverse ODE to reduce the discretization error and achieve state-of-the-art sampling speed.

**Discrete Diffusion Models.** Research on discrete diffusion models was initiated by Sohl-Dickstein et al. (2015), who investigated diffusion processes over binary random variables. The methodology was expanded upon by Ho et al. (2020), integrating categorical random variables through transition matrices with uniform probabilities. Though Song et al. (2020) suggested a similar extension intheir supplementary content, they abstained from experimenting with this model type. Later on, Austin et al. (2021) unveiled a more intricate framework for diffusion concerning categorical random variables, enhancing the discrete diffusion models by merging them with Masked language models (MLMs). Contemporary research has furthered this domain by introducing features like editing-based operations (Jolicoeour-Martineau et al., 2021; Reid et al., 2022), auto-regressive diffusion models (Hoogeboom et al., 2021; Ye et al., 2023), the evolution of a continuous-time structure (Campbell et al., 2022), and the exploration of neural network analogs for learning (Sun et al., 2022). Additionally, Zheng et al. (2023) introduced a re-parameterized loss and an associated sampling technique, attaining commendable outcomes in fewer iterations. Our contributions run parallel to these aforementioned studies.

## Appendix B Additional details of Discrete Diffusion

In our paper, we treat all the \(\mathbf{x}\), \(\mathbf{q}_{\mathrm{noise}}\) as a row vector and treat \(\mathds{1}\) as a column vector with all elements equal \(1\).

### Comparison between D3PM and DNDM

In Section 3.1, we introduced two different diffusion processes, the Markov process in (1) and the non-Markov process in (6). In this section, we explain why they are different but result in the same joint distribution of \((\mathbf{x}_{0},\mathbf{x}_{t})\) for every time step \(t\). Since \(\mathbf{q}(\mathbf{x}_{0})\) keeps the same, we only need to prove that the conditional distribution \(\mathbf{q}(\mathbf{x}_{t}|\mathbf{x}_{0})\) is the same for the two processes.

**Markov Process.** 1 is a Markov process since \(\mathbf{w}_{n}\) is independent with \(\mathbf{x}_{t-1},\ldots,\mathbf{x}_{0}\), so \(\mathbf{x}_{t}\) is independent of all the past states given the present state. This can also be inferred from the following distribution, which does not depend on \(\mathbf{x}_{0},\ldots,\mathbf{x}_{t-2}\),

\[q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{p} =\beta_{t}\mathbf{x}_{t-1}+(1-\beta_{t})\mathbf{q}_{\mathrm{noise}}\big{)}.\] (13)

Denote \(\mathbf{Q}_{t}:=\beta_{t}\mathds{1}+(1-\beta_{t})\,\mathds{1}\;\mathbf{q}_{ \mathrm{noise}}\), then we have that

\[\mathbf{x}_{t-1}\mathbf{Q}_{t}=\beta_{t}\mathbf{x}_{t-1}+(1-\beta_{t}) \mathbf{x}_{t-1}\,\mathds{1}\;\mathbf{q}_{\mathrm{noise}}=\beta_{t}\mathbf{x }_{t-1}+(1-\beta_{t})\mathbf{q}_{\mathrm{noise}},\]

where the last equality holds due to the fact that \(\mathbf{x}_{t-1}\) is a one hot vector and thus \(\mathbf{x}_{t-1}\,\mathds{1}=1\). Therefore, we can rewrite (13) as \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{ p}=\mathbf{x}_{t-1}\mathbf{Q}_{t}\big{)}\). Then, it is a Markov process with transition kernel \(\mathbf{Q}_{t}\). So \(q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{ p}=\mathbf{x}_{0}\mathbf{Q}_{0}\ldots\mathbf{Q}_{t}\big{)}\)(Austin et al., 2021). We can then have that

\[\mathbf{Q}_{0}\ldots\mathbf{Q}_{t} =\left[\beta_{0}\mathds{1}+(1-\beta_{0})\,\mathds{1}\;\mathbf{q}_ {\mathrm{noise}}\right]\ldots\left[\beta_{t}\mathds{1}+(1-\beta_{t})\, \mathds{1}\;\mathbf{q}_{\mathrm{noise}}\right]\] \[=\Pi_{s=0}^{t}\beta_{s}\mathds{1}+(1-\Pi_{s=0}^{t}\beta_{s})\, \mathds{1}\;\mathbf{q}_{\mathrm{noise}},\]

where the last equality holds since identity matrix \(\mathds{1}\) multiplying any vector equals the vector itself and \(\mathds{1}\;\mathbf{q}_{\mathrm{noise}}\,\mathds{1}\;\mathbf{q}_{\mathrm{noise }}=\mathds{1}(\mathbf{q}_{\mathrm{noise}}\,\mathds{1})\mathbf{q}_{\mathrm{noise }}=\mathds{1}\;\mathbf{q}_{\mathrm{noise}}\). Therefore, we have that

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{p} =\Pi_{s=0}^{t}\beta_{s}\mathbf{x}_{0}+(1-\Pi_{s=0}^{t}\beta_{s})\mathbf{q}_{ \mathrm{noise}}\big{)}=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{p}=\alpha_{t} \mathbf{x}_{0}+(1-\alpha_{t})\mathbf{q}_{\mathrm{noise}}\big{)},\]

where the last equality holds due to the definition \(\alpha_{t}=\Pi_{s=0}^{t}\beta_{s}\). This gives rise to why the Markov process (1) results in conditional distribution \(q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t};\mathbf{p}= \alpha_{t}\mathbf{x}_{0}+(1-\alpha_{t})\mathbf{q}_{\mathrm{noise}}\big{)}\).

**Non-Markov Process.** Recall that our DNDM is defined by

\[\mathbf{x}_{t}=b_{t}\mathbf{x}_{t-1}+(1-b_{t})\mathbf{w},\]

where \(\mathbf{w}\) is fixed for any time \(t\). Therefore, \(\mathbf{w}\) is no longer independent with \(\mathbf{x}_{0},\ldots,\mathbf{x}_{t-1}\). Therefore, we can't define the transition kernel and compute \(\mathbf{q}(\mathbf{x}_{t}|\mathbf{x}_{0})\) by using the property of Markov. Therefore, we need to advance the technique to calculate the conditional distribution.

Proof of Theorem 3.1.: By (6), we can derive the following explicit expression for a recursive sequence,

\[\mathbf{x}_{t} =b_{1}\ldots b_{t}\mathbf{x}_{0,n}+\sum_{s=1}^{t}(1-b_{s})b_{s+1} \ldots b_{t}\mathbf{w}\] \[=b_{1}\ldots b_{t}\mathbf{x}_{0}+(1-b_{1}\ldots b_{t})\mathbf{w}\]\[=a_{t}\mathbf{x}_{0}+(1-a_{t})\mathbf{w},\]

where second equality is by cancellation of terms, the last inequality holds by defining \(a_{t}=b_{1}\ldots b_{t}\). Since \(a_{t}\) either equals to \(1\) or \(0\). Besides, \(a_{t}\) equals \(1\) if and only if \(b_{1}=b_{2}=\ldots=b_{t}=1\), so we have that \(a_{t}\) follows Bernoulli distribution \(\mathrm{Bernoulli}(\beta_{1}\ldots\beta_{t})=\mathrm{Bernoulli}(\alpha_{t})\) where \(\alpha_{t}=\Pi_{i=1}^{t}\beta_{s}\). Therefore, we can conclude that \(\mathbf{q}(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathrm{Cat}\big{(}\mathbf{x}_{t}; \mathbf{p}=\alpha_{t}\mathbf{x}_{0}+(1-\alpha_{t})\mathbf{q}_{\mathrm{noise}} \big{)}\), which completes the proof. 

**Comparison between D3PM-Absorb and DNDM.** Recall the forward processes of D3PM and DNDM as follows:

\[\mathrm{D3PM}: \mathbf{x}_{t}=b_{t}\mathbf{x}_{t-1}+(1-b_{t})\mathbf{w}_{t}, \quad\forall t=1\ldots T,\] \[\mathrm{DNDM}: \mathbf{x}_{t}=b_{t}\mathbf{x}_{t-1}+(1-b_{t})\mathbf{w},\quad \forall t=1\ldots T.\]

For absorbing diffusion where \(\mathbf{w}=[\mathrm{Mask}]\), DNDM's forward process becomes equivalent to D3PM since \(\mathbf{w}_{t}=\mathbf{w}=[\mathrm{Mask}]\) in this special case. However, for multinomial diffusion or other diffusion processes where \(\mathbf{w}_{t}\neq\mathbf{w}\), these two processes exhibit different behaviors. In addition, even for absorbing diffusion, our proposed reverse sampling algorithm for DNDM is still different from that for D3PM.

To elucidate the key differences between the sampling algorithm in DNDM and that in D3PM for absorbing diffusion, let's directly compare the algorithms:

* For the D3PM-Absorb algorithm: We begin with an all \([\mathrm{Mask}]\) sequence. At each time step \(t\), we sample \(\mathbf{x}_{0}\sim p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_{t})\). If \(\mathbf{x}_{t}=[\mathrm{Mask}]\), \(\mathbf{x}_{t-1}\) transitions to \([\mathrm{Mask}]\) with probability \((1-\alpha_{t-1})/(1-\alpha_{t})\) and to \(\mathbf{x}_{0}\) with probability \((\alpha_{t-1}-\alpha_{t})/(1-\alpha_{t})\). If \(x_{t}\neq[\mathrm{Mask}]\), it remains unchanged.
* For the DNDM-Absorb algorithm: We also start with an all \([\mathrm{Mask}]\) sequence, but crucially, we first determine the transition time set. During sampling, if \(\mathbf{x}_{t}=[\mathrm{Mask}]\), the transition probabilities for \(\mathbf{x}_{t-1}\) are identical to D3PM. However, we only sample \(\mathbf{x}_{0}\sim p_{\theta}(\mathbf{x}_{0}|\mathbf{x}_{t})\) when at least one token needs to change, as determined by our pre-computed transition set. This selective sampling is the key to our algorithm's efficiency.

Therefore, you can see that DNDM will skip many steps during the sampling process to avoid function evaluation and save computational cost. Even though the forward process of DNDM is the same as that of D3PM for absorbing diffusion, our DNDM approach introduces an algorithm design in the sampling process by pre-computing the transition time set and selectively applying function evaluations. This distinguishes DNDM from D3PM algorithm, offering a more computationally efficient approach to inference in discrete diffusion.

**Comparison between DDIM and DNDM for Multinomial Diffusion.** While there are similarities between DNDM and DDIM (Appendix A), they are fundamentally different models, and DNDM is not a special case of DDIM. DNDM introduces a novel framework specifically designed for discrete spaces, while DDIM was originally developed for continuous diffusion models. The key differences for multinomial diffusion are as follows.

* DDIM: Following Song et al. (2020a) (eq. 19 in Appendix A), \(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})=\text{Cat}(\sigma_{t} \mathbf{x}_{t}+(\alpha_{t-1}-\sigma_{t}\alpha_{t})\mathbf{x}_{0}+((1-\alpha_{t -1})-(1-\alpha_{t})\sigma_{t})\mathbf{1}_{K})\). Even with \(\sigma_{t}=\frac{1-\alpha_{t-1}}{1-\alpha_{t}}\), the process remains stochastic: \(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})=\text{Cat}(\sigma_{t} \mathbf{x}_{t}+(1-\sigma_{t})\mathbf{x}_{0})\). This means at every step, there's a probability of choosing \(\mathbf{x}_{0}\), regardless of whether it has transitioned to \(\mathbf{x}_{0}\) or not. Unlike Absorbing discrete diffusion, no \([\mathrm{Mask}]\) exists in multinomial diffusion. Therefore, DDIM cannot distinguish whether \(\mathbf{x}_{t}\) already equals \(\mathbf{x}_{0}\) or not. In particular, although the sampling process becomes less stochastic in the DDIM setting, it will still be predicted \(\mathbf{x}_{0}\) with high probability \(1-\sigma_{t}=\frac{\alpha_{t-1}-\alpha_{t}}{1-\alpha_{t}}\).
* DNDM: Achieves full de-randomization using transition time \(\tau\), where: \[\mathbf{x}_{t-1}=\mathds{1}(\tau=t)\mathbf{x}_{0}+\mathds{1}(\tau\neq t) \mathbf{x}_{t},\quad\text{with }P(\tau=t)=\alpha_{t-1}-\alpha_{t}.\] (14) This crucial difference allows DNDM to achieve full de-randomization once \(\tau\) is sampled, leading to a deterministic evolution that DDIM cannot achieve.

While DNDM and DDIM are both non-Markov models for multinomial diffusion, their fundamental approaches to and achievements in de-randomization differ significantly in discrete spaces.

### Training Objective

Hoogeboom et al. (2021) utilized \(L_{t}\) derived from the negative variational bound. In detail,

\[L_{t}=\mathrm{KL}\big{(}\mathrm{Cat}(\mathbf{x};\mathbf{p}=\boldsymbol{\theta}_{ \mathrm{post}}(\mathbf{x}_{t},\mathbf{x}_{0})\big{|}\mathrm{Cat}(\mathbf{x}; \mathbf{p}=\boldsymbol{\theta}_{\mathrm{post}}(\mathbf{x}_{t},\widehat{\mathbf{ x}}_{0})\big{)},\] (15)

where \(\widehat{\mathbf{x}}_{0}\sim p_{\boldsymbol{\theta}}(\cdot|\mathbf{x}_{t})\), \(\boldsymbol{\theta}_{\mathrm{post}}=(\beta_{t}\mathbf{x}_{t}+(1-\beta_{t})/K \,\mathds{1}^{\top})\odot(\alpha_{t-1}\mathbf{x}_{0}+(1-\alpha_{t-1})/K\, \mathds{1}^{\top})\) and \(\boldsymbol{\theta}_{\mathrm{post}}=(\beta_{t}\mathbf{x}_{t}+(1-\beta_{t})/K \,\mathds{1}^{\top})\odot(\alpha_{t-1}\widehat{\mathbf{x}}_{0}+(1-\alpha_{t-1 })/K\,\mathds{1}^{\top})\). This loss evolves KL divergence between two categorical distributions.

Building on this foundation, Austin et al. (2021) introduced an auxiliary denoising objective to strengthen the data predictions \(\mathbf{x}_{0}\) at each time step. In detail, the auxiliary objective is as follows,

\[\mathbb{E}_{q(\mathbf{x}_{t},\mathbf{x}_{0})}\Big{[}-\log p_{\boldsymbol{ \theta}}(\mathbf{x}_{0}|\mathbf{x}_{t})\Big{]},\]

where the auxiliary loss term is minimized exactly when \(p_{\theta}(\cdot|\mathbf{x}_{t})\) has all its mass on the data point \(\mathbf{x}_{0}\).

Furthering the advancements, Zheng et al. (2023) put forth a reparametrized loss \(L_{t}\) that incorporates a re-weighted parameter \(\lambda_{t}\). The detailed loss is

\[\overline{L}_{t}=\lambda_{t-1}\mathbb{E}_{\mathbf{x}_{t-1},\mathbf{x}_{t}\sim q (\cdot|\mathbf{x}_{0})}\mathrm{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{ x}_{0})|p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t})).\]

This loss can be related to the standard multi-class cross-entropy loss function, which is also simple and powerful. That's why we consider Zheng et al. (2023) as the baseline model.

In Section 3.3, we consider the continuous-time forward and backward process. Based on that, we were motivated to analyze the infinite limit of the average loss \(\lim_{t\to\infty}\frac{1}{T}\sum_{t=1}^{T}L_{t}\). We find that the new loss can provide a better checkpoint than the loss averaged on the finite step on some tasks.

### Calculation of the Evidence Lower Bound

#### b.3.1 Finite Time DNDM

In this section, we derive the evidence lower bound (ELBO) for our model. The derivatives are inspired by the reasoning in DDIM (Song et al., 2020). Specifically, We denote the generative process as \(p_{\theta}(\mathbf{x}_{0:T}|\tau)=p_{\theta}^{(T)}(\mathbf{x}_{T}|\tau)\prod _{t=1}^{T}p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t},\tau)\). Here, \(p_{\theta}^{(T)}\) is the pure noise and \(p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t},\tau)=q(\mathbf{x}_{t-1}| \mathbf{x}_{t},\widehat{\mathbf{x}}_{0},\tau)\), where \(\widehat{\mathbf{x}}_{0}\) is given by a neural network \(p_{\boldsymbol{\theta}}\), i.e., \(\widehat{\mathbf{x}}_{0}=p_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)\). Notice that by Jensen's inequality,

\[\log p_{\theta}(\mathbf{x}_{0})=\log\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}[p _{\theta}(\mathbf{x}_{0}|\tau)]\geq\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}[ \log p_{\theta}(\mathbf{x}_{0}|\tau)].\] (16)

The evidence lower bound inequality gives

\[\log p_{\theta}(\mathbf{x}_{0}|\tau)\geq\mathbb{E}_{\mathbf{x}_{1:T}\sim q( \mathbf{x}_{1:T}|\mathbf{x}_{0},\tau)}\log\frac{p_{\theta}(\mathbf{x}_{0:T}| \tau)}{q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau)}.\] (17)

Plugging (17) into (16) gives the following ELBO,

\[\log p_{\theta}(\mathbf{x}_{0})\geq\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}} \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau)}\log \frac{p_{\theta}(\mathbf{x}_{0:T}|\tau)}{q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau )}:=\text{ELBO}.\]

We factorize the \(p_{\theta}\) and \(q\) by

\[p_{\theta}(\mathbf{x}_{0:T}|\tau) =p_{\theta}^{(T)}(\mathbf{x}_{T}|\tau)\prod_{t=1}^{T}p_{\theta}^{( t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t},\tau),\] \[q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau) =q(\mathbf{x}_{T}|\mathbf{x}_{0},\tau)\prod_{t=2}^{T}q(\mathbf{x }_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0},\tau).\]

Here \(q\) admits such a decomposition due to our definition of the diffusion process in (6), which introduce the following reverse process:

\[\mathbf{x}_{t-1}=\mathds{1}(\tau=t)\mathbf{x}_{0}+\mathds{1}(\tau\neq t) \mathbf{x}_{t}.\]Therefore, \(\mathbf{x}_{1:T}\) is Markovian when conditioned on \(\mathbf{x}_{0}\) and \(\tau\). Based on the factorization, we have

\[\text{ELBO} =\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}\mathbb{E}_{\mathbf{x}_{1:T \sim q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau)}}\Big{[}\log p_{\theta}^{(T)}( \mathbf{x}_{T}|\tau)+\sum_{t=1}^{T}\log p_{\theta}^{(t)}(\mathbf{x}_{t-1}| \mathbf{x}_{t},\tau)\] \[\qquad-\log q(\mathbf{x}_{T}|\mathbf{x}_{0},\tau)-\sum_{t=2}^{T} \log q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0},\tau)\Big{]}\] \[=\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}\mathbb{E}_{\mathbf{x}_{ 1:T\sim q(\mathbf{x}_{1:T}|\mathbf{x}_{0},\tau)}}\Big{[}\log p_{\theta}^{(1)}( \mathbf{x}_{0}|\mathbf{x}_{1},\tau)+\sum_{t=2}^{T}\log\frac{p_{\theta}^{(t)}( \mathbf{x}_{t-1}|\mathbf{x}_{t},\tau)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_{0},\tau)}\] \[\qquad+\log\frac{p_{\theta}^{(T)}(\mathbf{x}_{T}|\tau)}{q( \mathbf{x}_{T}|\mathbf{x}_{0},\tau)}\Big{]}\] \[=\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}\mathbb{E}_{\mathbf{x}_{ 1}\sim q(\cdot|\mathbf{x}_{0},\tau)}\log p_{\theta}^{(1)}(\mathbf{x}_{0}| \mathbf{x}_{1},\tau)\] \[\qquad+\sum_{t=2}^{T}\mathbb{E}_{\mathbf{x}_{t-1},\mathbf{x}_{t} \sim q(\cdot|\mathbf{x}_{0},\tau)}\log\frac{p_{\theta}^{(t)}(\mathbf{x}_{t-1} |\mathbf{x}_{t},\tau)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0},\tau)} +\text{const}\] \[=\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}}\underbrace{\mathbb{E}_{ \mathbf{x}_{1}\sim q(\cdot|\mathbf{x}_{0},\tau)}\log p_{\theta}^{(1)}(\mathbf{ x}_{0}|\mathbf{x}_{1},\tau)}_{\mathcal{L}_{1}}\] \[\qquad-\sum_{t=2}^{T}\mathbb{E}_{\tau\sim\mathcal{D}_{\tau}} \underbrace{\mathbb{E}_{\mathbf{x}_{t-1},\mathbf{x}_{t}\sim q(\cdot|\mathbf{x} _{0},\tau)}\text{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0},\tau)|p_ {\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t},\tau))}_{\widetilde{\mathcal{L }}_{t}}+\text{const}.\]

By a slight abuse of notations we use \(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0}),p_{\theta}^{(t)}(\mathbf{x} _{0}|\mathbf{x}_{1})\) to indicate the distribution of the diffusion process defined in Zheng et al. (2023), that is, the standard Markov discrete diffusion process. In particular, we have

\[\overline{\mathcal{L}}_{1} =\left\{\begin{array}{ll}\mathbb{E}_{\mathbf{x}_{1}\sim q(\cdot |\mathbf{x}_{0})}\log p_{\theta}^{(1)}(\mathbf{x}_{0}|\mathbf{x}_{1}),&\tau=1, \\ \text{const},&\tau\neq 1.\end{array}\right.\] \[\overline{\mathcal{L}}_{t} =\left\{\begin{array}{ll}\mathbb{E}_{\mathbf{x}_{t-1},\mathbf{x }_{t}\sim q(\cdot|\mathbf{x}_{0})}\text{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})|p_{\theta}^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_{t})),&\tau=t,\\ 0,&\tau\neq t.\end{array}\right.\]

Thus, we can obtain that

\[\text{ELBO}= \mathbb{P}(\tau=1)\cdot\underbrace{\mathbb{E}_{\mathbf{x}_{1} \sim q(\cdot|\mathbf{x}_{0})}\log p_{\theta}^{(1)}(\mathbf{x}_{0}|\mathbf{x}_{1 })}_{\mathcal{L}_{1}}\] \[-\sum_{t=2}^{T}\mathbb{P}(\tau=t)\cdot\underbrace{\mathbb{E}_{ \mathbf{x}_{t-1},\mathbf{x}_{t}\sim q(\cdot|\mathbf{x}_{0})}\text{KL}(q( \mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})|p_{\theta}^{(t)}(\mathbf{x}_{t -1}|\mathbf{x}_{t}))}_{\mathcal{L}_{t}}+\text{const}.\]

Here \(\mathcal{L}_{t}\) matches the loss terms in Zheng et al. (2023). In the practical training process, Zheng et al. (2023) samples \(t\) from \(\text{Unif}\{1,\cdots,T\}\) in each iteration and optimizes \(\lambda_{t}\cdot\mathcal{L}_{t}\), where \(\lambda_{t}\)'s are weights. Thus, when we sample \(\tau\) and optimize \(\mathcal{L}_{\tau}\), our ELBO indeed leads to the same training objective as Zheng et al. (2023) up to reweighting. Since Zheng et al. (2023) is a parametrization of existing works (Austin et al., 2021; Hoogeboom et al., 2021), our training objective indeed aligns with previous discrete diffusion models.

#### b.3.2 Continous Time DNDM

In Section B.3, we derived an ELBO for DNDM and its accelerated algorithm defined in Section 3.1 and 3.2. While for finite sampling steps, we can decompose the diffusion process via the sampling steps \(1,\ldots,T\) in (17), it becomes intractable for continuous Time DNDM (Infinite steps \(T\rightarrow\infty\)). Therefore, we can formulate the ELBO of continuous time DNDM by decomposing the transition times. The idea of decomposition of transition times follows Hoogeboom et al. (2021), but their proof is only applicable to absorbing discrete diffusion, while ours can deal with discrete diffusion with various noise \(q_{\rm noise}\) including multinomial diffusion.

In Section B.3, we only consider the case of a single token \(\mathbf{x}\in\mathbb{R}^{K}\) for simplicity as we decompose with the sampling steps \(T\). In this section, we decompose over the transition time \(\tau\). Therefore, we need to consider a sentence with multiple tokens \(\mathbf{x}_{t,1:N}=[\mathbf{x}_{t,1},\ldots,\mathbf{x}_{t,N}]\) where \(\mathbf{x}_{t,n}\) is the \(n\)-th token and \(N\) is the sequence length. Recall that we defined the transition time set \(\mathcal{T}=\{\tau_{n}\}_{n=1}^{N}\) in Section 3.2. We arrange \(\tau_{n}\) to obtain an ordered sequence \(\tau_{n_{k}}\), where \(0=\tau_{n_{0}}<\tau_{n_{1}}<\tau_{n_{2}}<\ldots<\tau_{n_{N}}=T\). Then conditioning on the transition time set \(\mathcal{T}=\{\tau_{1},\ldots,\tau_{N}\}\), we have that

\[p_{\theta}(\mathbf{x}_{0:T,1:N}|\mathcal{T})=p_{\theta}(\mathbf{x}_{\tau_{n_{ N}},1:N}|\mathcal{T})\prod_{s=N,\ldots,1}p_{\theta}(\mathbf{x}_{\tau_{n_{s-1}},1:N}|\mathbf{x}_{\tau_{n_{s}},1:N},\mathcal{T}),\]

where we omit the time superscript of \(p\) for simplicity. Then, the evidence lower bound inequality gives

\[\log p_{\theta}(\mathbf{x}_{0,1:N}|\mathcal{T})\geq\mathbb{E}_{\mathbf{x}_{ \tau_{n_{1}},T,1:N}\sim q(\mathbf{x}_{\tau_{n_{1}},T,1:N}|\mathbf{x}_{0,1:N}, \mathcal{T})}\log\frac{p_{\theta}(\mathbf{x}_{0:T,1:N}|\mathcal{T})}{q( \mathbf{x}_{\tau_{n_{1}},T,1:N}|\mathbf{x}_{0,1:N},\mathcal{T})}.\] (18)

By Jensen's inequality, we have

\[\log p_{\theta}(\mathbf{x}_{0,1:N})=\log\mathbb{E}_{\tau_{1},\ldots,\tau_{n} \sim\mathcal{D}_{\tau}}[p_{\theta}(\mathbf{x}_{0,1:N}|\mathcal{T})]\geq \mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim\mathcal{D}_{\tau}}[\log p_{\theta}( \mathbf{x}_{0}|\mathcal{T})].\] (19)

Plugging (18) into (19) gives the following ELBO,

\[\log p_{\theta}(\mathbf{x}_{0,1:N})\geq\mathbb{E}_{\tau_{1},\ldots,\tau_{n} \sim\mathcal{D}_{\tau}}\mathbb{E}_{\mathbf{x}_{\tau_{n_{1}},T}\sim q(\mathbf{x }_{\tau_{n_{1}},T}|\mathbf{x}_{0},\mathcal{T})}\log\frac{p_{\theta}(\mathbf{x }_{0:T}|\mathcal{T})}{q(\mathbf{x}_{\tau_{n_{1}}:T}|\mathbf{x}_{0},\mathcal{T} )}:=\text{ELBO}.\]

We factorize the \(p_{\theta}\) and \(q\) by

\[p_{\theta}(\mathbf{x}_{0:T,1:N}|\mathcal{T}) =p_{\theta}(\mathbf{x}_{T,1:N}|\mathcal{T})\prod_{s=N,\ldots,1}p_{ \theta}(\mathbf{x}_{\tau_{n_{s-1}},1:N}|\mathbf{x}_{\tau_{n_{s}},1:N}, \mathcal{T}),\] \[q(\mathbf{x}_{\tau_{n_{1}}:T,1:N}|\mathbf{x}_{0,1:N},\mathcal{T}) =q(\mathbf{x}_{T,1:N}|\mathbf{x}_{0},\mathcal{T})\prod_{s=N,\ldots,2}q(\mathbf{x}_{\tau_{n_{s-1}},1:N}|\mathbf{x}_{\tau_{n_{s}},1:N}, \mathbf{x}_{0,1:N},\mathcal{T}).\]

Therefore, we have

\[\text{ELBO}=\mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim\mathcal{D}_{ \tau}}\mathbb{E}_{\mathbf{x}_{\tau_{n_{1}},T,1:N}\sim q(\mathbf{x}_{\tau_{n_{1 }},T,1:N}|\mathbf{x}_{0,1:N},\mathcal{T})}\Big{[}\log p_{\theta}(\mathbf{x}_{T,1:N}|\mathcal{T})\] \[\qquad+\sum_{s=1}^{N}\log p_{\theta}(\mathbf{x}_{\tau_{n_{s-1}},1 :N}|\mathbf{x}_{\tau_{n_{s}},1:N},\mathcal{T})-\log q(\mathbf{x}_{T,1:N}| \mathbf{x}_{0,1:N},\mathcal{T})\] \[\qquad-\sum_{s=2}^{N}\log q(\mathbf{x}_{\tau_{n_{s-1}},1:N}| \mathbf{x}_{\tau_{n_{s}},1:N},\mathbf{x}_{0,1:N},\mathcal{T})\Big{]}\] \[=\mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim\mathcal{D}_{\tau}} \mathbb{E}_{\mathbf{x}_{\tau_{n_{1}},T,1:N}\sim q(\mathbf{x}_{\tau_{n_{1}},T,1:N }|\mathbf{x}_{0,1:N},\mathcal{T})}\Big{[}\log p_{\theta}(\mathbf{x}_{0,1:N}| \mathbf{x}_{1,1:N},\mathcal{T})\] \[\qquad+\sum_{s=2}^{N}\log\frac{p_{\theta}(\mathbf{x}_{\tau_{n_{s-1} },1:N}|\mathbf{x}_{\tau_{n_{s}},1:N},\mathcal{T})}{q(\mathbf{x}_{\tau_{n_{s-1}},1 :N}|\mathbf{x}_{\tau_{n_{s}},1:N},\mathcal{T})}+\log\frac{p_{\theta}(\mathbf{x}_{ T,1:N}|\mathcal{T})}{q(\mathbf{x}_{T,1:N}|\mathbf{x}_{0,1:N},\mathcal{T})}\Big{]}\] \[=\mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim\mathcal{D}_{\tau}}\mathbb{ E}_{\mathbf{x}_{1,1:N}\sim q(\cdot|\mathbf{x}_{0,1:N},\mathcal{T})}\log p_{\theta}( \mathbf{x}_{0,1:N}|\mathbf{x}_{1,1:N},\mathcal{T})\] \[\qquad+\sum_{s=2}^{N}\mathbb{E}_{\mathbf{x}_{\tau_{n_{s-1}},1:N}, \mathbf{x}_{\tau_{n_{s}},1:N}\sim q(\cdot|\mathbf{x}_{0,1:N},\mathcal{T})}\log \frac{p_{\theta}(\mathbf{x}_{\tau_{n_{s-1}},1:N}|\mathbf{x}_{\tau_{n_{s}},1:N}, \mathcal{T})}{q(\mathbf{x}_{\tau_{n_{s-1}},1:N}|\mathbf{x}_{\tau_{n_{s}},1:N}, \mathbf{x}_{0,1:N},\mathcal{T})}+\text{const}\] \[=\mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim\mathcal{D}_{\tau}}\mathbb{ E}_{\mathbf{x}_{1,1:N}\sim q(\cdot|\mathbf{x}_{0,1:N},\mathcal{T})}\log p_{\theta}( \mathbf{x}_{0,1:N}|\mathbf{x}_{1,1:N},\mathcal{T})\] \[\qquad-\sum_{s=2}^{N}\mathbb{E}_{\tau_{1},\ldots,\tau_{n}\sim \mathcal{D}_{\tau}}\mathbb{E}_{\mathbf{x}_{\tau_{n_{s-1}},1:N},\mathbf{x}_{ \tau_{n_{s}},1:N}\sim q(\cdot|\mathbf{x}_{0,1:N},\mathcal{T})}\] \[\qquad\text{KL}(q(\mathbf{x}_{\tau_{n_{s-1}},1:Choice of the Transition Time

Transition time \(\tau\) in Definition 3.2 plays an important role in DNDM. In this section, we provide a deeper discussion of the transition time. We first give a proof of the Theorem 3.6.

Proof of Theorem 3.6.: By the definition of \(\tau\), we know that \(\tau_{n}=t\) is equivalent to \(b_{0,n}=1,\dots,b_{t-1,n}=1\) and \(b_{t,n}=0\). Since \(\{b_{t,n}\}_{t=0}^{T}\) is independent for different \(n\) by definition, each \(\tau_{n}\) is also independent. Therefore, we drop the subscript \(n\) for simplicity. On the other hand if \(b_{0}=1,\dots,b_{t-1}=1\) and \(b_{t}=0\) we can also conclude that \(\tau=t\). Therefore, we have that

\[\mathbb{P}(\tau=t) =\mathbb{P}(b_{0}=1,\dots,b_{t-1}=1,b_{t}=0)\] \[=\left[\Pi_{s=1}^{t-1}\beta_{s}\right]\cdot(1-\beta_{t})\] \[=\Pi_{s=1}^{t-1}\beta_{s}-\Pi_{s=1}^{t}\beta_{s}\] \[=\alpha_{t-1}-\alpha_{t},\]

where the second equality is due to \(b_{s},s=1,2,\dots,t\) are independent random variable following \(\mathrm{Bernoulli}(\beta_{s})\) distribution and the last equality is by the definition of \(\alpha_{t}=\Pi_{s=1}^{t}\beta_{s}\). 

Notice that \(\alpha_{t}\) is a decreasing sequence in the \(0\) to \(1\) range. Therefore, \(\mathbb{P}(\tau=t)\in[0,1]\) for any \(t\in\{1,\dots,T\}\). Besides \(\sum\mathbb{P}(\tau=t)=\sum_{t=1}^{T}\left(\alpha_{t-1}-\alpha_{t}\right)= \alpha_{0}-\alpha_{T}=1\). Therefore, the derived distribution is valid as long as the \(\alpha_{t}\) is decreasing from \(1\) to \(0\).

From Theorem 3.6, we discern that the nature of the diffusion model scheduler, \(\alpha_{t}\), clarifies the distribution of \(\tau\).

**Linear \(\alpha\) schedule.** This is a schedule studied in Austin et al. (2021), where \(\alpha_{t}=1-t/T\). This will result in \(\mathbb{P}(\tau_{n}=t)=1/T\) for every \(t\) in the range \(1\) to \(T\). As a result, transition time distributes uniformly across each moment in the set \(\{1,\dots,T\}\). This can be verified in a) of Figure 3.

**Cosine \(\alpha\) schedule.** This is a schedule studied in Hoogeboom et al. (2021), where \(\alpha_{t}=\cos(\pi*t/2T)\). For numerical consideration of the noise, a small offset \(s\) is added, i.e., \(\alpha_{t}=f(t)/f(0)\)

Figure 3: Different distribution of transition time for \(T=50\). \(a),b),c)\) The transition time sampled 1K times under the different \(\alpha_{t}\) schedule. d) The approximated transition time for \(t=1,\dots,T\) using different hypter-parameters.

where \(f(t)=\cos((s+t/T)/(1+s)*\pi/2)\). As shown in b) of Figure 3, the transition time will concentrate more on the large \(T\).

**Cosine square \(\alpha\) schedule.** This is a schedule studied in Zheng et al. (2023), where \(\alpha_{t}=\cos^{2}(\pi*t/2T)\), which motivated by Nichol and Dhariwal (2021). Again, for numerical consideration of the noise, a small offset \(s\) is added, i.e., \(\alpha_{t}=f(t)/f(0)\) where \(f(t)=\cos^{(}(s+t/T)/(1+s)*\pi/2)\). As shown in c) of Figure 3, the transition time will concentrate more on the middle of the range.

Generally, if we express \(\alpha_{t}\) as \(g(t/T)\), then we can simplify to \(\mathbb{P}(\tau=t)=g((t-1)/T)-g(t/T)\), which further refines to \((1/T)[g^{\prime}(t/T)]+o(1/T)\). This indicates that transitions are more likely where \(|g^{\prime}|\) is large. Such a mathematical finding can match our observation in Figure 3.

In practice, we find that the shape of the transition time doesn't need to match the theoretical prediction schedule exactly. As we can see from d) in Figure 3. A reshaped Beta distribution can approximate all the transition time distributions in a fixed range. We first extract a time \(t\in[0,1]\) from a Beta distribution, then adjust these samples to fit by multiplying \(T\) and round them to acquire the integer. Our experiment finds that a properly chosen Beta distribution (tuned on the validation set) makes DNDM perform better on the translation tasks. Specifically, the chosen Beta distributions and the searching method are reported in Appendix F. The performance of the four transition time schedules mentioned above, including the reported Beta distributions for comparison, are listed in Table 5, where we find the other three schedules affect the performance, and most of their scores are lower than the scores of Beta distribution, but their scores are at least still close to the reported Beta distributions, especially for DNDM-k-absorb and DNDM-absorb. The efficiencies (measured by NFE) are also similar to one another.

Additionally, the ablation study on a reasonable range of different Beta distributions with 50 and 1000 sampling steps are shown in Tables 10 and 9, where the BLEU scores and NFE values on the test set of one of the three machine translation datasets, WMT16, are shown for demonstration. The range of Beta distributions covers our chosen Beta schedules based on validation sets and a variety of basic Beta distribution shapes. These results show that the different Beta distributions influence the performance, but most of these choices of parameters still achieve results close to the optimal. Since the Beta distributions of the reported results in Tables 2 and 3 are selected using the validation set, they do not always have the highest scores on the test set, but their scores still at least belong to the top tiers according to these tables.

**Another view of the transition time.** In Algorithm 1, we only need to call the neural network when \(t\in\mathcal{T}\), which can significantly speed up the sampling since we reduce the function call. Notice that after we get the \(\mathbf{x}_{0}\) prediction, we only update the \(\mathbf{x}_{t}\) for those tokens at the transition time. However, (7) implies that \(\mathbf{x}_{t}=\mathbf{x}_{0}\) as long as \(\tau>t\). Therefore, instead of only updating the \(\mathbf{x}_{t}\) for those tokens at the transition time, i.e., \(\tau=t\), we can also update those tokens with transition time \(\tau>=t\). This motivates us to consider a variation presented as Algorithm 3, which keeps almost the same sampling time but will update the tokens several times rather than just once. Since the tokens now get the chance to be corrected over time. The new Algorithm 3 will be more robust than Algorithm 1.

\begin{table}
\begin{tabular}{l|l|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Schedules} & \multicolumn{2}{c|}{DNDM-multi} & \multicolumn{2}{c|}{DNDM-absorb} & \multicolumn{2}{c|}{DNDM-k-multi} & \multicolumn{2}{c}{DNDM-k-absorb} \\ \cline{3-10}  & & BLEU & Avg NFE & BLEU & Avg NFE & BLEU & Avg NFE & BLEU & Avg NFE \\ \hline \multirow{4}{*}{IWSLT14} & Cosine & 31.72 & 31.71 & 32.71 & 31.21 & 32.91 & 31.71 & 34.50 & 31.21 \\  & Cosine\({}^{2}\) & 31.78 & 31.74 & \(\mathbf{32.93}\) & 31.21 & 32.78 & 31.74 & 34.53 & 31.21 \\  & Linear \(\alpha\) & 31.77 & 31.82 & 32.65 & 31.33 & 32.83 & 31.82 & 34.53 & 31.33 \\  & Beta (reported) & \(\mathbf{31.82}\) & \(\mathbf{30.33}\) & \(\mathbf{32.93}\) & \(\mathbf{31.08}\) & \(\mathbf{33.15}\) & \(\mathbf{30.33}\) & \(\mathbf{34.56}\) & \(\mathbf{31.08}\) \\ \hline \multirow{4}{*}{WMT14} & Cosine & \(\mathbf{25.80}\) & 39.61 & \(\mathbf{26.54}\) & 39.18 & 26.63 & 39.61 & 27.81 & 39.18 \\  & Cosine\({}^{2}\) & 25.52 & 39.48 & 26.53 & 39.18 & 25.01 & 39.48 & \(\mathbf{27.95}\) & 39.18 \\  & Linear \(\alpha\) & 25.58 & 39.97 & 26.33 & 39.82 & 25.47 & 39.97 & 27.63 & 39.82 \\  & Beta (reported) & 25.71 & \(\mathbf{38.94}\) & 26.43 & \(\mathbf{38.76}\) & \(\mathbf{26.88}\) & \(\mathbf{38.94}\) & 27.82 & \(\mathbf{38.76}\) \\ \hline \multirow{4}{*}{WMT16} & Cosine & 32.71 & 40.50 & 33.56 & 40.45 & 33.46 & 40.50 & 34.37 & 40.45 \\  & Cosine\({}^{2}\) & 32.73 & 40.50 & 33.51 & 40.45 & 33.44 & 40.50 & 34.24 & 40.45 \\ \cline{1-1}  & Linear \(\alpha\) & 32.85 & 40.36 & 33.46 & 40.36 & 33.47 & 40.36 & 33.88 & 40.36 \\ \cline{1-1}  & Beta (reported) & \(\mathbf{32.86}\) & \(\mathbf{38.46}\) & \(\mathbf{33.60}\) & \(\mathbf{38.27}\) & \(\mathbf{33.79}\) & \(\mathbf{38.45}\) & \(\mathbf{34.38}\) & \(\mathbf{38.27}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The BLEU scores and average number of function evaluations (NFE) values of different distributions of transition time for \(1000\) sampling steps with batch size 100. The parameters of the Beta distributions in this table are the same as in Tables 2 and 3 and are reported in Appendix F.

**Impact of Transition Order.** We further evaluate the impact of transition order. Building upon the results in Table 3, we investigate how the model performance will change if the transition time is influenced by the position of the tokens: from left to right and from right to left. In the left-to-right approach, tokens positioned on the left are transitioned to \(\mathbf{x}_{0}\) earlier, and vice versa for the right-to-left approach. Our experiments show that the left-to-right approach consistently outperforms the right-to-left approach across all datasets and step counts, as demonstrated in Table 6.

This result suggests that the order of token transitions significantly influences the model's performance, with earlier transitions of left-side tokens leading to better generation quality.

## Appendix D Discussion on the Number of Function Evaluations (NFE).

In this section, we discuss the number of function evaluations (NFE) in DNDM. According to (9), the update of a token \(\mathbf{x}_{t-1,n}\) occurs solely at its designated transition time. Meanwhile, if step \(t\) does not coincide with a transition time for any token, we maintain the sentence from the preceding step unchanged: \(\mathbf{x}_{t,1:N}=\mathbf{x}_{t-1,1:N}\). Therefore, our algorithm removes the need of function evaluation for steps outside the set of transition times. Given this structure, our analytical emphasis is on the transition set \(\mathcal{T}\) since function evaluations are required only at times \(t\) that are members of \(\mathcal{T}\). Consequently, the NFE is precisely the cardinality of the transition set, denoted by \(|\mathcal{T}|\). In our main paper, we propose a naive upper bound for \(|\mathcal{T}|\) as \(\min\{N,T\}\), which effectively demonstrates the speed of our method when \(T>N\). Next, we demonstrate that DNDM also reduces the NFE when \(T<N\), by providing a precise estimation of \(|\mathcal{T}|\).

**Theorem D.1**.: _Suppose transition time follows distribution \(\mathcal{D}_{\tau}\), and consider a sequence of length \(N\). Then, the cardinality of the transition set \(\mathcal{T}:=\{\tau_{1},\ldots,\tau_{N}\}\) satisfies:_

* \(1\leq|\mathcal{T}|\leq\min\{N,T\}\)_,_
* \(\mathbb{E}[|\mathcal{T}|]=[1-C_{T,N,\mathcal{D}_{\tau}}]\cdot T\)_, where_ \(C_{T,N,\mathcal{D}_{\tau}}\) _is a constant in the range_ \((0,1)\)_. Furthermore,_ \[C_{T,N,\mathcal{D}_{\tau}}=\Big{(}\sum_{i=1}^{T}(1-p_{i})^{N}\Big{)}/T\geq(1- 1/T)^{N},\] _where_ \(p_{i}=\mathbb{P}(\tau=i)\) _for_ \(\tau\sim\mathcal{D}_{\tau}\)_, and the equality holds if and only if_ \(\mathcal{D}_{\tau}\) _is a uniform distribution._

Proof.: The first statement is straightforward. For completeness, the proof is provided. Since there are only \(N\) transition times (possibly repeated): \(\tau_{1},\ldots,\tau_{N}\), the distinct transition times must satisfy \(|\mathcal{T}|\leq N\). Additionally, since \(\mathcal{T}\subseteq\{1,\ldots,T\}\), we also have \(|\mathcal{T}|\leq T\).

To prove the second statement, we decompose \(\mathcal{T}\) and use the property of expectation. Note that \(|\mathcal{T}|=\sum_{i=1}^{T}\mathds{1}\{i\in\mathcal{T}\}\). Thus,

\[\mathbb{E}[|\mathcal{T}|]=\mathbb{E}\bigg{[}\sum_{i=1}^{T}\mathds{1}\{i\in \mathcal{T}\}\bigg{]}=\sum_{i=1}^{T}\mathbb{P}(i\in\mathcal{T}).\] (21)

Assuming \(\mathbb{P}_{\mathcal{D}_{\tau}}(\tau=i)=p_{i}\), and that \(\tau_{n}\) are i.i.d. draws from \(\mathcal{D}_{\tau}\), we have

\[\mathbb{P}(i\in\mathcal{T})=1-\mathbb{P}(i\notin\mathcal{T})=1-(1-p_{i})^{N}.\] (22)

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Steps** & **Direction** & **IWSLT14** & **WMT14** & **WMT16** \\ \hline \multirow{2}{*}{25} & Left-to-right & 31.08 & 24.41 & 31.67 \\  & Right-to-left & 30.54 & 23.33 & 31.33 \\ \hline \multirow{2}{*}{50} & Left-to-right & 32.87 & 26.46 & 33.37 \\  & Right-to-left & 32.47 & 25.18 & 32.78 \\ \hline \multirow{2}{*}{1000} & Left-to-right & 34.45 & 27.93 & 34.43 \\  & Right-to-left & 34.04 & 27.02 & 34.15 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of left-to-right and right-to-left transition approaches across different datasets and step counts.

Substituting (22) into (21) yields

\[\mathbb{E}[|\mathcal{T}|]=\sum_{i=1}^{T}\Big{[}1-(1-p_{i})^{N}\Big{]}=\Big{[}1- \frac{\sum_{i=1}^{T}(1-p_{i})^{N}}{T}\Big{]}\cdot T=[1-C_{T,N,\mathcal{D}_{ \tau}}]\cdot T,\]

where \(C_{T,N,\mathcal{D}_{\tau}}=\Big{(}\sum_{i=1}^{T}(1-p_{i})^{N}\Big{)}/T\). An upper bound for \(C_{T,N,\mathcal{D}_{\tau}}\) is given as

\[C_{T,N,\mathcal{D}_{\tau}}=\Big{[}1-\frac{\sum_{i=1}^{T}(1-p_{i})^{N}}{T}\Big{]} \cdot T\leq\Big{[}1-\Big{(}1-\frac{1}{T}\Big{)}^{N}\Big{]}\cdot T,\]

where the inequality holds if and only if \(p_{i}=1/T\) for all \(i\in[T]\), i.e., \(\mathcal{D}_{\tau}\) is a uniform distribution. 

_Remark D.2_.: Theorem D.1 suggests that even when \(T\leq N\), our method still provides a significant improvement. Specifically, for \(T=N\geq 4\), we have \(C_{T,N,\mathcal{D}_{\tau}}=(1-1/N)^{N}\geq 0.3\). This implies that our model requires at most \(0.7T\) even in the worst case. Moreover, if we consider a special scenario where the number of \(p_{i}\) satisfying \(p_{i}<\epsilon\) is more than \(M\), then we have \(C_{T,N,\mathcal{D}_{\tau}}>M(1-\epsilon)^{N}/T\), indicating that with \(M\) sufficiently large and \(\epsilon\) sufficiently small, \(C_{T,N,\mathcal{D}_{\tau}}\) can be pretty close to \(1\).

_Remark D.3_.: In practical applications of our model, we employ a beta distribution for \(\mathcal{D}_{\tau}\), which typically exhibits a right-heavy tail. Therefore \(C_{T,N,\mathcal{D}_{\tau}}\) tends to be larger than that in the worst-case scenario. In Tables 7 and 8, we list the average NFE for each experiment we run in SS4. These results demonstrate a significant reduction in NFE compared to the original counts: for \(T=25\), the NFE is only about half of the original count; for \(T=50\), it is approximately one-third; and for \(T=1000\), it reduces to less than one-twentieth of the original count.

_Remark D.4_.: By Bernoulli's inequality, \((1-p)^{N}>1-N\cdot p\) for \(1>p>0\). Therefore, \(C_{T,N,\mathcal{D}_{\tau}}>1-N/T\), implying that \(\mathbb{E}[|\mathcal{T}|]<N\). As \(T\to\infty\), assuming the transition time does not concentrate at a single point, the probability that two transitions occur simultaneously is zero. Consequently, the generation process will sequentially go through each token. Thus, the expected number of function evaluations (NFE), \(\mathbb{E}[|\mathcal{T}|]\), will be \(N\). In contrast, when \(T\) is finite, there is a non-zero probability that multiple transitions happen at the same time. Hence, in this case, the NFE, \(|\mathcal{T}|\), is strictly less than \(N\)

## Appendix E Discrete Non-Markov Diffusion Model with Top-k Transition Time (DNDM-K).

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Steps**} & \multicolumn{2}{c|}{**RDM-Multi**} & \multicolumn{2}{c|}{**DNDM-Multi**} & \multicolumn{2}{c|}{**RDM-\(k\)-Multi**} & \multicolumn{2}{c}{**DNDM-\(k\)-Multi**} \\ \cline{3-10}  & & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** \\ \hline \multirow{4}{*}{IWSLT14} & 25 & **31.26** & 25 & 30.95 & **9.03** & **32.82** & 25 & 32.30 & **9.03** \\  & 50 & **31.50** & 50 & 31.45 & **14.07** & **32.82** & 50 & 32.80 & **14.07** \\  & 1000 & 31.69 & 1000 & **31.82** & **30.33** & 32.64 & 1000 & **33.15** & **30.33** \\  & \(\infty\) & - & - & **31.89** & **32.73** & - & - & **33.44** & **32.73** \\ \hline \multirow{4}{*}{WMT14} & 25 & **25.25** & 25 & 25.01 & **13.52** & **26.03** & 25 & 25.98 & **13.52** \\  & 50 & **25.75** & 50 & 25.33 & **20.58** & 26.14 & 50 & **26.37** & **20.58** \\  & 1000 & 25.66 & 1000 & **25.71** & **38.94** & 25.82 & 1000 & **26.88** & **38.94** \\  & \(\infty\) & - & **24.79** & **40.67** & - & - & **26.39** & **40.67** \\ \hline \multirow{4}{*}{WMT16} & 25 & **32.29** & 25 & 31.97 & **8.5** & **33.12** & 25 & 32.94 & **8.5** \\  & 50 & **32.53** & 50 & 32.50 & **14.73** & **33.41** & 50 & 33.26 & **14.73** \\ \cline{1-1}  & 1000 & 32.63 & 1000 & **32.86** & **38.45** & 33.67 & 1000 & **33.79** & **38.45** \\ \cline{1-1}  & \(\infty\) & - & - & **32.91** & **41.64** & - & - & **33.86** & **41.64** \\ \hline \hline \end{tabular}
\end{table}
Table 7: BLEU score and the average number of function evaluations (NFE) comparison of multinomial diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. The blue background highlights our algorithms. The average NFE values are calculated by dividing the number of times calling the denoising function (neural network) during generation by the number of batches, where the batch sizes of all experiments are 100.

Recent works have demonstrated that the quality of samples can be enhanced by utilizing supplementary information derived from the neural network (Ghazvininejad et al., 2019; Savinov et al., 2021; Chang et al., 2022; He et al., 2022). Very recently, Zheng et al. (2023) applied this idea in their RDM framework and can achieve significant performance improvement. Specifically, after decoding \(\widehat{\mathbf{x}}_{0,1:N}\) from transformer \(p_{\theta}(\cdot|\mathbf{x}_{t,1:N})\), the score corresponding to this decoded token from the transformer's last layer, is also recorded and denote as \(s_{t,n}\). Tokens with high scores are more likely to be selected for updates.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Steps**} & \multicolumn{2}{c|}{**RDM-Absorb**} & \multicolumn{2}{c|}{**DNDM-Absorb**} & \multicolumn{2}{c|}{**RDM-\(k\)-Absorb**} & \multicolumn{2}{c}{**DNDM-\(k\)-Absorb**} \\ \cline{3-10}  & & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** & **BLEU** & **Avg NFE** \\ \hline \multirow{3}{*}{IWSLT14} & 25 & 31.58 & 25 & **32.43** & **13.81** & **34.50** & 25 & 34.14 & **13.81** \\  & 50 & 31.80 & 50 & **32.63** & **19.24** & **34.58** & 50 & 34.34 & **19.24** \\  & 1000 & 31.91 & 1000 & **32.93** & **31.08** & **34.60** & 1000 & 34.56 & **31.08** \\  & \(\infty\) & - & - & **33.03** & **32.07** & - & - & **34.65** & **32.07** \\ \hline \multirow{3}{*}{WMT14} & 25 & 24.97 & 25 & **25.79** & **15.09** & **27.50** & 25 & 27.18 & **15.09** \\  & 50 & 24.95 & 50 & **26.10** & **22.45** & **27.73** & 50 & 27.66 & **22.45** \\  & 1000 & 25.22 & 1000 & **26.43** & **38.76** & 27.75 & 1000 & **27.82** & **38.76** \\  & \(\infty\) & - & - & **26.50** & **40.39** & - & - & **27.50** & **40.39** \\ \hline \multirow{3}{*}{WMT16} & 25 & 32.86 & 25 & **33.20** & **13.91** & 33.92 & 25 & **33.96** & **13.91** \\  & 50 & 32.93 & 50 & **33.30** & **20.95** & 34.10 & 50 & **34.20** & **20.95** \\ \cline{1-1}  & 1000 & 33.25 & 1000 & **33.60** & **38.27** & **34.44** & 1000 & 34.38 & **38.27** \\ \cline{1-1}  & \(\infty\) & - & - & **33.42** & **41.59** & - & - & **34.41** & **41.59** \\ \hline \hline \end{tabular}
\end{table}
Table 8: BLEU score and the average number of function evaluations (NFE) comparison of absorbing diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. The blue background highlights our algorithms. The average NFE values are calculated by dividing the number of times calling the denoising function (neural network) during generation by the number of batches, where the batch sizes of all experiments are 100.

Inspired by Zheng et al. (2023), we introduce the discrete non-Markov discrete diffusion Model with top-K transition time (DNDM-K). Instead of directly determining which token gets updated at step \(t\) by first drawing transition time \(\tau\sim\mathcal{D}_{\tau}\), we employ a two-step process.

1. We first compute \(K_{t}=\sum_{n=1}^{N}\mathds{1}(\tau_{n}\geq t)\). \(k_{t}\) represents how many tokens should be decoded at the current step.
2. Compare \(K_{t-1}\) and \(K_{t}\), if \(K_{t-1}=K_{t}\). There is no transition time at time \(t\), we just update \(\mathbf{x}_{t-1,1:N}=\mathbf{x}_{t,1:N}\). If \(K_{t-1}>K_{t}\), Then there exist transition time at time \(t\), we calculate and select the indexes with top-\(K_{t-1}\) scores. Then we update those tokens if it hasn't been updated yet.

Subsequently, we will only update those tokens with the highest \(K_{t}\) score that hasn't been changed yet. Since the function evaluation occurs only when \(K_{t}\) changes, DNDM-K can give an accelerated sampling algorithm. The details are presented in Algorithm 4.

## Appendix F Experiment details

### Conditional Text Generation

**Parameter choices.** In all experiments, the batch size is chosen to be \(100\). For RDM and RDM-\(k\), our hyperparameter settings follow the original paper (Zheng et al., 2023) except for the batch size. Before the sampling, we used the saved checkpoint of trained models provided by the authors for discrete sampling experiments, and we trained the corresponding models for continuous sampling experiments.

For finite-step DNDM, the transition times are determined by the schedule, and we approximate the schedule with a Beta distribution \(\text{Beta}(\alpha,\beta)\) (please refer to Section 3.2 for detailed explanation). The \(\alpha\) and \(\beta\) values are selected by applying grid search on the validation sets. Based on the BLEU scores on the validation sets, we have selected \(\text{Beta}(15,7)\) for Multinormal Diffusion on IWSLT14, \(\text{Beta}(3,3)\) for Absorbing Diffusion on both IWSLT14 and WMT14, \(\text{Beta}(5,3)\) for Multinormal Diffusion on WMT14 and Absorbing Diffusion on WMT16, and \(\text{Beta}(20,7)\) for Multinormal Diffusion on WMT16.

For infinite-steps (continuous-step) diffusion (DNDM-C), the transition timestamps are sampled from \(\text{Beta}(\alpha,\beta)\), where the choice of \((\alpha,\beta)\) are chosen from \((100.0,4.0)\) or \((17.0,4.0)\), based on the performance comparison on the validation set. In the end we choose \(\text{Beta}(17,4)\) for IWSLT14 and \(\text{Beta}(100,4)\) for WMT14 and WMT16.

We conduct a performance comparison based on varying configurations of the Beta and Alpha distributions. The results of these comparisons are presented in Tables 10 and 9. Furthermore, to evaluate the efficacy of discrete versus continuous step schemes, we also conduct an ablation study under the same set of parameters \((100,4)\) in Table 11.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Alpha} & \multicolumn{8}{c}{Beta} \\ \cline{3-10}  & & 3 & 5 & 7 & 9 & 11 & 13 & 15 & 17 & 19 & 21 \\ \hline \multirow{3}{*}{DNDM-k-Multi} & 3 & 33.47 & 33.67 & 33.62 & 33.77 & **33.87** & 33.64 & 33.73 & 33.60 & 33.68 & 33.56 \\  & 5 & 33.18 & 33.47 & 33.68 & 33.53 & 33.71 & 33.69 & 33.73 & 33.72 & 33.74 & 33.82 \\  & 7 & 32.99 & 33.20 & 33.49 & 33.56 & 33.58 & 33.61 & 33.67 & 33.72 & 33.78 & 33.83 \\ \hline \multirow{3}{*}{DNDM-Multi} & 3 & 32.73 & 32.66 & 32.74 & 32.82 & 32.77 & **32.92** & 32.80 & 32.81 & 32.76 & 32.86 \\  & 5 & 32.32 & 32.62 & 32.70 & 32.80 & 32.83 & 32.83 & 32.90 & 32.95 & 32.91 & 32.87 \\  & 7 & 32.35 & 32.35 & 32.53 & 32.67 & 32.75 & 32.78 & 32.86 & 32.80 & 32.86 & 32.88 \\ \hline \multirow{3}{*}{DNDM-k-Absorb} & 3 & 34.19 & 34.38 & 34.34 & 34.32 & 34.21 & 34.24 & 34.07 & 34.31 & **34.42** & 34.36 \\  & 5 & 32.15 & 33.99 & 34.29 & 34.30 & 34.29 & 34.40 & 34.40 & 34.24 & 34.30 & 34.22 \\  & 7 & 27.67 & 32.87 & 33.94 & 34.28 & 34.27 & 34.38 & 34.31 & 34.29 & 34.38 & 34.40 \\ \hline \multirow{3}{*}{DNDM-Absorb} & 3 & 33.53 & 33.60 & 33.67 & 33.71 & 33.71 & 33.70 & 33.58 & 33.63 & 33.53 & 33.54 \\  & 5 & 32.70 & 33.33 & 33.52 & 33.60 & 33.66 & 33.73 & 33.70 & **33.74** & 33.72 & **33.74** \\ \cline{1-1}  & 7 & 30.56 & 32.65 & 33.28 & 33.37 & 33.51 & 33.52 & 33.61 & 33.67 & 33.63 & 33.67 \\ \hline \hline \end{tabular}
\end{table}
Table 9: BLEU scores on dataset WMT16 from the ablation study of other different \(\text{Beta}(\alpha,\beta)\) distributions of the transition time with 1000 sampling steps.

Continuous time vs discrete time diffusions.To test our hypothesis that the continuous-time sampler will produce more accurate results in reverse sampling if our \(\mathbf{x}_{0}\) estimator consistently approximates the true \(\mathbf{x}_{0}\) over time, we conduct various sampling experiments using a shared pretrained neural network. For discrete-time sampling, we consider three cases: \(T=25,50,1000\). In each case, we rescale the interval \([0,T]\) to \([0,50]\) and divide it into \(T\) fractions. In contrast, for continuous-time sampling, we directly sample from a continuous distribution over the interval \([0,50]\) without any partitioning.

**Training approach.** In machine translation tasks, the neural network is designed to learn \(q(\mathbf{x}_{0}|\mathbf{x}_{t},\mathbf{z})\), where \(\mathbf{z}\) represents the embedding of the source text obtained using transformer encoder layers. For a fair comparison, we employ the same neural network structure as our baseline, with detailed architecture specifications available in Section E.2 of Zheng et al. (2023). Furthermore, given that the primary focus of this paper is the speed and effectiveness of our sampling algorithm, we omit the training procedure and instead use a state-of-the-art diffusion-based pretrained checkpoint from Zheng et al. (2023). In the Appendix, we present additional results of continuous sampling based on a continuously trained checkpoint. In this setting, we rescale our network input to the interval \([0,1]\) and uniformly sample from this interval. The rest of the architecture follows that of Zheng et al. (2023).

**Performance on WMT14.** Our work primarily focuses on the sampling process, and for the training, we utilized a pretrained checkpoint trained on 50 steps. In our sampling experiments we noticed that our method does not work ideally on WMT14, this could be possibly attributed to the fact that the training performance on WMT14 was not ideal. Specifically, when we performed sampling using 1000 steps, the network was trained with exposure to only 50 time steps, specifically at intervals of 20 (0, 20, 40,..., 980, 1000). As a result, when we apply our model to generation using 1000 steps, the checkpoint NN has only been explicitly trained on these intervals. While we generally assume that the network can still provide a good estimate for the untrained steps, this might not hold under some hard scenarios. Considering the longer training time and poorer performance of WMT14, it is likely that the training performance is insufficient for us to rely on those unseen steps. In a word, the model's trained checkpoint may not be robust enough to effectively handle unseen steps, especially for timesteps 1000 or infinite timesteps.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Alpha} & \multicolumn{8}{c}{Beta} \\ \cline{3-12}  & & 3 & 5 & 7 & 9 & 11 & 13 & 15 & 17 & 19 & 21 \\ \hline \multirow{3}{*}{DNDM-k-Multi} & 3 & 33.31 & 33.47 & 33.39 & 33.48 & 33.29 & 33.23 & 33.25 & 33.27 & 33.11 & 33.17 \\  & 5 & 32.93 & 33.28 & 33.29 & **33.58** & 33.45 & 33.21 & 33.40 & 33.49 & 33.16 & 33.19 \\  & 7 & 32.61 & 32.98 & 33.31 & 33.20 & 33.27 & 33.41 & 33.39 & 33.53 & 33.35 & 33.08 \\ \hline \multirow{3}{*}{DNDM-Multi} & 3 & 32.63 & 32.46 & 32.44 & 32.56 & 32.59 & 32.55 & 32.37 & 32.33 & 32.22 & 32.23 \\  & 5 & 32.31 & 32.43 & 32.66 & 32.64 & **32.68** & 32.55 & 32.55 & 32.44 & 32.35 & 32.30 \\  & 7 & 31.95 & 32.11 & 32.22 & 32.26 & 32.54 & 32.52 & 32.50 & 32.58 & 32.48 & 32.41 \\ \hline \multirow{3}{*}{DNDM-k-Absorb} & 3 & 34.05 & 34.2 & 34.31 & 34.37 & 34.15 & 34.05 & 34.06 & 33.77 & 33.81 & 33.84 \\  & 5 & 32.30 & 34.08 & 34.30 & **34.38** & 34.26 & 34.23 & 34.09 & 34.06 & 34.02 & 34.13 \\  & 7 & 27.39 & 32.64 & 33.71 & 34.18 & 34.02 & 34.33 & 34.31 & 34.17 & 34.12 & 34.19 \\ \hline \multirow{3}{*}{DNDM-Absorb} & 3 & 33.26 & 33.30 & 33.29 & 33.24 & 33.23 & 32.97 & 33.06 & 32.85 & 32.89 & 32.63 \\  & 5 & 32.47 & 33.08 & 33.31 & 33.22 & **33.41** & 33.25 & 33.15 & 33.27 & 33.04 & 32.98 \\ \cline{1-1}  & 7 & 30.34 & 32.27 & 33.27 & 33.03 & 33.16 & 33.14 & 33.27 & 33.11 & 33.11 & 33.07 \\ \hline \hline \end{tabular}
\end{table}
Table 10: BLEU scores on dataset WMT16 from the ablation study of other different Beta\((\alpha,\beta)\) distributions of the transition time with 50 sampling steps.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Steps & DNDM-k-multi & DNDM-k-absorb & DNDM-multi & DNDM-absorb \\ \hline
50 & 31.60 & 31.74 & 30.39 & 29.69 \\
1000 & 33.59 & 34.37 & 32.87 & 33.52 \\ \(\infty\) & 33.86 & 34.41 & 32.91 & 33.42 \\ \hline \end{tabular}
\end{table}
Table 11: The BLEU scores on dataset WMT16 with Beta(100,4) as the transition time schedule for discrete sampling or the distribution to sample transition timestamps for continuous sampling.

### Unconditional Text Generation

**Parameter choices.** We recover the checkpoints of the multinomial diffusion model employing the provided code by Hoogeboom et al. (2021). We train 12-layer Transformers for both text8 and enwik8 datasets for 500 epochs with the cosine schedule. For the text8 dataset, we utilize a training batch size of 256, while for the enwik8 dataset, we use a batch size of 128. During training, we employ a learning rate of 0.0001, a weight decay parameter of 0.99, and the Adam optimizer.

## Appendix G Additional Experiments

In this section, we present additional experimental results. We begin by plotting the relationship between computational time and the number of sampling steps, using the absorbing diffusion in IWSLT14 as an example. Figure 4 displays the growth of computational time for absorbing diffusion (yellow and orange lines), RDM-absorbing diffusion, and our model DNDM-Absorb and DNDM-T-Absorb (green and blue lines).

We see from Figure 4 that previous algorithms, including absorbing diffusion and RDM-absorbing diffusion all suffer from linear growth of computational time.

### Continuous Training

In Section 4.1, we introduce the DNDM-C algorithm, designed for continuous-time, over discrete-time algorithms. However, this algorithm assumes that we have learned a sufficiently accurate neural network at any timestamp \(t\in[0,1]\). Using the checkpoint trained with 50 discrete time partitions might not suffice for the purpose of continuous sampling. In this section, we investigate the performance of continuous sampling when training is also done continuously.

\begin{table}
\begin{tabular}{c|c|c|c c|c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Step scheme} & \multicolumn{2}{c|}{C-DNDM-Multi} & \multicolumn{2}{c}{C-DNDM-Absorb} \\ \cline{3-6}  & & Default & Top-k & Default & Top-k \\ \hline IWSLT14 & Continuous & **32.07** & **33.57** & 32.80 & 34.52 \\ \hline WMT16 & Continuous & **33.48** & 33.71 & **33.50** & 34.36 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Continuous Training + Continuous Sampling

Figure 4: The growth of computational time with the increase of the sampling stepsIn Table 12, we summarize the performance of DNDM-C based on a neural network estimated continuously during training time. This involves sampling time uniformly from \([0,1]\) during training, and the forward process follows (11) in Section 3.3. The training objective remains the same as in discrete-time training. In Table 12 we list the result of IWSLT14 and WMT16 with continuous training followed by continuous sampling. In addition, we compare the value with the corresponding value during discrete training and continuous sampling in Section 4.1 and mark every item that improves in bold. As demonstrated in Table 12, there is room for enhancement in the overall sampling scores by training the neural network in a complete space of timestamps.

### Comparison with more generative models

In our study, a key aspect of evaluating our fast discrete generative model involves comparisons with prior work known for speed in sampling with minimal steps. Specifically, we draw a direct comparison with the Mask-Predict (Ghazvininejad et al., 2019), which is notable for its ability to generate high-quality results within just 10 iterations. The results are shown in Table 13. All experiments were conducted on the same GPU and within the same machine setup.

### Samples from the multinomial text models

**Conditional Generation.** For DNDM-Multi trained on IWSLT14, we provide a full generation process with 100 steps in Figure 5. A token ending with @@ indicates it is an incomplete word; it will be concatenated with the following token to form a complete word. For example, "fel0@ lo@ ws" means "fellows". We can see that after \(t=39\), the generate sentence converges.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline \multicolumn{3}{|c|}{Mask-Predict} & \multicolumn{5}{|c|}{DNDM-Absorb} & \multicolumn{5}{|c|}{DNDM-k-Absorb} \\ \hline
**Steps** & **BLEU** & **Time** & **Steps** & **BLEU** & **Time** & **NFE** & **Steps** & **BLEU** & **Time** & **NFE** \\ \hline
10 & 33.08 & 49.25 & 25 & 33.20 & 41.2 & 13.91 & 25 & 33.96 & 41.4 & 13.91 \\ \hline
15 & 33.06 & 67.94 & 50 & 33.30 & 62.5 & 20.95 & 50 & 34.20 & 62.7 & 20.95 \\ \hline
25 & 33.16 & 111.89 & 1000 & 33.60 & 121.3 & 38.27 & 1000 & 34.38 & 122.7 & 38.27 \\ \hline
40 & 33.10 & 169.95 & \(\infty\) & 33.42 & 121.8 & 41.59 & \(\infty\) & 34.41 & 121.9 & 41.59 \\ \hline \end{tabular}
\end{table}
Table 13: The performance comparison on WMT16 of DNDM with Mask-Predict (Ghazvininejad et al., 2019). We align the number of sampling steps used in Mask-Predict with a similar number of function evaluations (NFE) in our DNDM algorithm. We see that our Algorithm runs faster, with better BLEU score.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions are summarized as three points at the end of the introduction. The scope is fast sampling via discrete non-Markov diffusion models, provided in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We add a limitation section in front of the Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: Theorems 3.1, 3.5, and D.1 are clearly stated, well-organized with consistent numbering, and supported by rigorous proofs that establish their validity.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides detailed information on the experimental setup, model architecture, and training procedures. The authors have submitted their training code along with the main paper, which enables reproducibility of the main results. The code and detailed instructions allow other researchers to replicate the key findings of the paper. Guidelines: In addition to experiment and implementation details on appendix, we submit our training and evaluation codes when submtting our main paper. * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All the datasets are public and can be open accessed. Our codebase will be available in public upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide these details on Appendix (D, E, F). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Confidence intervals are provided in the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided detailed information about the computation resources in Section 4: a single NVIDIA258 RTX A6000 GPU with 48 GB memory. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have checked NeurIPS Code of Ethics. Our submission satisfies all the requirement. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provide Broader Impacts Section in the beginning of Appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no related risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All used code, data and models in this project are properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

[MISSING_PAGE_EMPTY:36]