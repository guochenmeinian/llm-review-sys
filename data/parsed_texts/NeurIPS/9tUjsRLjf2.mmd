# Dynamic Regret of Adversarial Linear Mixture MDPs

Long-Fei Li, Peng Zhao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{lilf, zhaop, zhouzh}@lamda.nju.edu.cn

###### Abstract

We study reinforcement learning in episodic inhomogeneous MDPs with adversarial full-information rewards and the unknown transition kernel. We consider the linear mixture MDPs whose transition kernel is a linear mixture model and choose the _dynamic regret_ as the performance measure. Denote by \(d\) the dimension of the feature mapping, \(H\) the length of each episode, \(K\) the number of episodes, \(P_{T}\) the non-stationary measure, we propose a novel algorithm that enjoys an \(\widetilde{\mathcal{O}}\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})} \big{)}\) dynamic regret under the condition that \(P_{T}\) is known, which improves previously best-known dynamic regret for adversarial linear mixture MDP and adversarial tabular MDPs. We also establish an \(\Omega\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{HK(H+P_{T})}\big{)}\) lower bound, indicating our algorithm is _optimal_ in \(K\) and \(P_{T}\). Furthermore, when the non-stationary measure \(P_{T}\) is unknown, we design an online ensemble algorithm with a meta-base structure, which is proved to achieve an \(\widetilde{\mathcal{O}}\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T}) +H^{2}\mathcal{S}_{T}^{2}}\big{)}\) dynamic regret and here \(S_{T}\) is the expected switching number of the best base-learner. The result can be optimal under certain regimes.

## 1 Introduction

Reinforcement Learning (RL) aims to learn a policy that maximizes the cumulative reward through interacting with the environment, which has achieved tremendous successes in various fields, including games [1, 2], robotic control [3, 4], and dialogue generation [5, 6]. In reinforcement learning, the Markov Decision Process (MDP) [7] is the most widely used model to describe the environment.

Traditional MDPs assume that the reward functions are stochastic and the number of actions and states is small. However, in many real-world applications, the reward functions may be adversarially changing and the state and action spaces are large or even infinite. Previous work studies these two problems separately. To deal with the adversarial reward functions, Even-Dar et al. [8] first consider learning _adversarial_ MDPs with known transition and full-information reward feedback. They propose the MDP-E algorithm that enjoys \(\widetilde{\mathcal{O}}(\sqrt{\tau^{3}T})\) regret, where \(\tau\) is the mixing time and \(T\) is the number of steps. There is a line of subsequent work studying adversarial MDPs [9, 10, 11, 12, 13, 14, 15, 16], which studies various settings depending on whether the transition kernel is known, and whether the feedback is full-information or bandit. To overcome the large state and action space issue, a widely used method is _linear function approximation_, which reparameterizes the value function as a linear function over some feature mapping that maps the state and action to a low-dimensional space. Amongst these work, linear MDPs [17, 18, 19, 20, 21, 22, 23] and linear mixture MDPs [24, 25, 26, 27, 28, 29, 30, 31] are two of the most popular MDP models with linear function approximation. In particular, He et al. [23] and Zhou et al. [31] attain the minimax optimal \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\) regret for linear MDPs and linear mixture MDPs with stochastic rewards respectively.

Recent studies try to combine two lines of work to establish the theoretical foundation of adversarial MDPs with large state and action space. In particular, Cai et al. [32] study adversarial linear mixtureMDPs and propose the OPPO algorithm that enjoys an \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{4}K})\) regret. He et al. [33] improve the result to \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\) and show it is (nearly) optimal by presenting a matching lower bound. However, these studies choose _static regret_ as the performance measure, defined as the performance difference between the learner's policy and that of the best-fixed policy in hindsight, namely,

\[\mathrm{Regret}(K)=\max_{\pi\in\Pi}\sum_{k=1}^{K}V_{k,1}^{\pi}(s_{k,1})-\sum_{ k=1}^{K}V_{k,1}^{\pi_{k}}(s_{k,1}),\] (1)

where \(V\) is the value function and \(\Pi\) is the set of all stochastic policies. One caveat in (1) is that the best _fixed_ policy may behave poorly in non-stationary environments. To this end, we introduce _dynamic regret_, which benchmarks the learner's performance with changing policies, defined as

\[\mathrm{D-Regret}(K)=\sum_{k=1}^{K}V_{k,1}^{\pi_{k}^{c}}(s_{k,1})-\sum_{k=1}^ {K}V_{k,1}^{\pi_{k}}(s_{k,1}),\] (2)

where \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\in\Pi\) is compared policies. Define \(P_{T}=\sum_{k=2}^{K}d(\pi_{k}^{c},\pi_{k-1}^{c})\) with a certain distance measure \(d(\cdot,\cdot)\) as the non-stationary measure. A favorable dynamic regret should scale with \(P_{T}\).

Dynamic regret is a more appropriate measure in non-stationary environments, but it is more challenging to optimize such that few studies focus on it in the literature. Zhao et al. [34] investigate the dynamic regret of adversarial tabular MDPs with the _known_ transition kernel and present an algorithm with optimal dynamic regret. Importantly, their algorithm does not require the non-stationarity measure \(P_{T}\) as the algorithmic input ahead of time. For the unknown transition setting, Fei et al. [35] study adversarial tabular MDPs and propose an algorithm with dynamic regret guarantee. Zhong et al. [36] further extend the algorithm of Fei et al. [35] to accommodate non-stationary transition kernels with linear function approximation. Both algorithms of Fei et al. [35] and Zhong et al. [36] require the quantity of \(P_{T}\) as the input. Moreover, their dynamic regret bounds are suboptimal in terms of \(K\) and \(P_{T}\) as demonstrated by the lower bound established by our work (see Theorem 4).

This work investigates the dynamic regret of adversarial linear mixture MDPs, with a focus on the full-information feedback and the unknown transition. We first propose POWERS-FixShare algorithm when \(P_{T}\) is known, an algorithm combining optimistic policy optimization with a Bernstein bonus and fixed-share mechanism. We show it enjoys an \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})})\) dynamic regret, where \(d\) is the dimension of the feature mapping, \(H\) is the length of each episode, \(K\) is the number of episodes, \(P_{T}\) is the non-stationary measure. We also establish a dynamic regret lower bound of \(\Omega(\sqrt{d^{2}H^{3}K}+\sqrt{HK(H+P_{T})})\). We stress four remarks regarding our results:

1. Our result can recover the \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\) minimax optimal static regret in He et al. [33].
2. Our result improves upon the previously best-known \(\widetilde{\mathcal{O}}(dH^{7/4}K^{3/4}+H^{2}{K^{2/3}P_{T}}^{1/3})\) dynamic regret for the fixed transition of Zhong et al. [36, Theorem 4.6] in terms of \(H\), \(K\) and \(P_{T}\).
3. Our result can imply an \(\widetilde{\mathcal{O}}(\sqrt{H^{4}S^{2}AK}+H^{2}\sqrt{(K+P_{T})(1+P_{T})})\) dynamic regret for adversarial tabular MDPs, strictly improving upon the previously best-known \(\widetilde{\mathcal{O}}(\sqrt{H^{4}S^{2}AK}+H^{2}{K^{2/3}P_{T}}^{1/3})\) dynamic regret of Fei et al. [35, Theorem 1], in terms of both \(K\) and \(P_{T}\).
4. As the lower bound suggests, our result is the first optimal regarding the dependence on \(d\), \(K\) and \(P_{T}\) and can be optimal in terms of \(H\) under certain regimes (\(H\leq d\) and \(P_{T}\leq d^{2}/H\)).

Furthermore, we study the case when \(P_{T}\) is unknown and design a novel algorithm equipped with the dynamic regret guarantee by the meta-base two-layer structure. Our algorithm enjoys an \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})+H^{2 }S_{T}^{2}})\) dynamic regret, where \(S_{T}\) is the expected switching number of the best base-learner. Though \(S_{T}\) is a data-dependent quantity, it also reflects the degree of environmental non-stationarity to some extent. Moreover, under specific regimes, the magnitude of \(S_{T}\) may be relatively negligible, resulting in our results still being optimal. Indeed, given that \(S_{T}\) is a data-dependent quantity, its inclusion in the regret bound is not ideal. Deriving bounds that rely exclusively on problem-dependent quantities, like \(P_{T}\), remains an open challenge. We discuss the technical difficulty of removing \(S_{T}\) in Section 5 and take this issue for future work.

Finally, we also highlight the main technical challenges and our solutions as follows.

* We first show that the dynamic regret depends on the inverse of the minimal probability over the action space of our policies, which can be arbitrarily small. To this end, we propose a novel algorithm with the _fixed-share_ mechanism [37]. While this mechanism is proved to enjoy favorable dynamic regret in online convex optimization [38], it suffers an additional term that can be regarded as the weighted sum of the difference between the occupancy measure of the compared policies in online MDPs. To overcome the difficulty, we exploit the _multiplicative stability_ to bound this term, eliminating the need for a restart strategy to handle the environmental non-stationarity as in previous studies [35; 36] and allows us to attain the dynamic regret optimal in terms of \(K\) and \(P_{T}\).
* We show the dynamic regret of online MDPs can be written as the weighted average of "multi-armed bandits" problems over all states, where the weight for each state is the _unknown_ and _changing_ probability of being visited by \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\). For the unknown \(P_{T}\) case, we first show the standard two-layer structure used in non-stationary online learning studies [39; 40; 41] fails to achieve a favorable dynamic regret guarantee, which characterizes the unique difficulty of online MDPs. Then, we present an initial attempt to address this issue by a specifically designed two-layer structure. We prove our algorithm enjoys nice dynamic regret guarantees under certain regimes.

Notations.We denote by \(\Delta(\mathcal{A})\) the set of probability distributions on a set \(\mathcal{A}\) and denote the KL-divergence by \(D_{\mathrm{KL}}(p||p^{\prime})=\sum_{a\in\mathcal{A}}p(a)\log\frac{p(a)}{p(a)}\) for any \(p,p^{\prime}\in\Delta(\mathcal{A})\). We define \(\Delta(\mathcal{A}\mid\mathcal{S},H)=\{\{\pi_{h}(\cdot\mid\cdot)\}_{h=1}^{H} \mid\pi_{h}(\cdot\mid x)\in\Delta(\mathcal{A}),\forall s\in\mathcal{S},h\in[ H]\}\) for any set \(\mathcal{S}\) and \(H\in\mathbb{Z}_{+}\). Further, for any \(\pi,\pi^{\prime},\pi^{\prime\prime}\in\Delta(\mathcal{A}\mid\mathcal{S},H)\), we define \(\mathbb{E}_{\pi}[\widetilde{D}_{\mathrm{KL}}(\pi^{\prime}\|\pi^{\prime\prime} )]=\mathbb{E}_{\pi}[\sum_{h=1}^{H}D_{\mathrm{KL}}(\pi^{\prime}_{h}(\cdot\mid s _{h})\|\pi^{\prime\prime}_{h}(\cdot\mid s_{h}))]\). For any policy pair \(\pi_{h},\pi^{\prime}_{h}\), we define \(\|\pi_{h}-\pi^{\prime}_{h}\|_{1,\infty}=\max_{s\in\mathcal{S}}\|\pi_{h}(\cdot \mid s)-\pi^{\prime}_{h}(\cdot\mid s)\|_{1}\). For any \(a,b,x\in\mathbb{R}\) with \(a\leq b\), let \([x]_{[a,b]}\) denote \(\min\{\max(x,a),b\}\). \(\widetilde{\mathcal{O}}(\cdot)\) omits the logarithmic factors.

## 2 Related Work

RL with adversarial rewards.There are many studies on learning adversarial MDPs where the reward functions are adversarially chosen, yielding fruitful results that can be categorized into three lines [8; 9; 10; 11; 12; 13; 14; 15; 16]. In particular, the first line of work considers the infinite-horizon MDPs with uniform mixing assumption. In the known transition and full-information setting, the seminal work of Even-Dar et al. [8] proposes MDP-E algorithm that achieves the \(\widetilde{\mathcal{O}}(\sqrt{\tau^{3}T})\) regret, where \(\tau\) is the mixing time and \(T\) is the number of steps. Another concurrent work of Yu et al. [9] achieves \(\widetilde{\mathcal{O}}(T^{2/3})\) in the same setting. In the known transition and bandit-feedback setting, Neu et al. [10] propose MDP-EXP3 algorithm that attains \(\widetilde{\mathcal{O}}(T^{2/3})\) regret. The second line of work considers the episodic loop-free MDPs. Neu et al. [11] first study this problem under the known transition setting and propose algorithms that achieve \(\widetilde{\mathcal{O}}(\sqrt{T})\) and \(\widetilde{\mathcal{O}}(\sqrt{T/\alpha})\) for full-information and bandit-feedback respectively, where \(\alpha\) is the lower bound of the probability of all states under any policy. Zimin and Neu [12] propose O-REPS algorithm that enjoys \(\widetilde{\mathcal{O}}(\sqrt{T})\) regret in both full-information and bandit-feedback setting without any additional assumption. Rosenberg and Mansour [13] and Jin et al. [14] further consider the harder unknown transition and bandit-feedback setting. The last line of work studies the episodic Stochastic Shortest Path (SSP) setting [15; 16]. In this paper, we focus on episodic MDPs with the unknown transition and full-information setting.

RL with linear function approximation.To design RL algorithms in large state and action space scenarios, recent works focus on solving MDPs with linear function approximation. In general, these works can be divided into three lines based on the specific assumption of the underlying MDP. The first line of work considers the low Bellman-rank assumption [42; 43; 44; 45], which assumes the Bellman error matrix has a low-rank factorization. The second line of work is based on the linear MDP assumption [17; 18; 19; 20; 21; 22; 23], where both the transition kernel and reward functions can be parameterized as linear functions of given state-action feature mappings \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\). The last line of work studies the linear mixture MDP [24; 25; 26; 28; 29; 30; 31], where the transition kernel can be parameterized as a linear function of a feature mapping \(\phi:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}^{d}\) but without the linear reward functions assumption. Amongst these works, He et al. [23] and Zhou et al. [31] attain the minimax optimal \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\) regret for both episodic linear MDPs and linear mixture MDPs respectively. However, all the above studies consider the stochastic reward setting. In this paper, we study the episodic linear mixture MDP setting but with adversarial reward functions.

Non-stationary RL.Another related line of research is online non-stationary MDPs. In contrast to adversarial MDPs where the reward functions are generated in an adversarial manner, online non-stationary MDPs consider the setting where the reward functions are generated stochastically according to some distributions that may vary over time. Jaksch et al. [46] study the piecewise stationary setting where the transitions and rewards are allowed to change certain times and propose UCRL2 with restart technique to deal with the non-stationarity. Later, Ortner et al. [47] consider the generalized setting where the changes are allowed to take place every step. However, the above studies need prior knowledge about the magnitude of non-stationary. To address this issue, Cheung et al. [48] propose the Bandit-over-RL algorithm to remove this requirement. A recent breakthrough by Wei and Luo [49] introduces a black-box method that can convert any algorithm satisfying specific conditions and enjoying optimal static regret in stationary environments into another with optimal dynamic regret in non-stationary environments, without requiring prior knowledge about the degree of non-stationarity. However, this approach does not apply to the adversarial setting. Specifically, their reduction requires the base algorithm to satisfy a certain property enjoyed by typical UCB-type algorithms. When a new instance of the base algorithm surpasses the optimistic estimator, it can be inferred that the environment has changed, prompting a restart of the algorithm to disregard prior information. However, this approach of constructing an optimistic estimator by a UCB-type algorithm can only be applied to a _stochastic_ setting. In the _adversarial_ setting, where no model assumptions are made and comparators can be arbitrary, this approach encounters significant difficulties.

Dynamic Regret.Dynamic regret of RL with adversarial rewards is only recently studied in the literature [34; 35; 36]. Zhao et al. [34] investigate the dynamic regret of adversarial tabular MDPs with the _known_ transition kernel and present an algorithm with optimal dynamic regret. Importantly, their algorithm does not require the non-stationarity measure \(P_{T}\) as the algorithmic input ahead of time. For the unknown transition setting, Fei et al. [35] study adversarial tabular MDPs and propose an algorithm with dynamic regret guarantees. Zhong et al. [36] further extend the algorithm of Fei et al. [35] to accommodate non-stationary transition kernels with linear function approximation. Both algorithms [35; 36] require the quantity of \(P_{T}\) as the input. Moreover, their dynamic regret bounds are suboptimal in \(K\) and \(P_{T}\) as shown by the lower bound established in our work. In this work, we first design an _optimal_ algorithm in terms of \(K\) and \(P_{T}\) when \(P_{T}\) is known. Further, we develop the first algorithm to handle the unknown \(P_{T}\) issue in adversarial MDPs with unknown transition.

## 3 Problem Setup

We focus on episodic inhomogeneous MDPs with full-information reward functions and the unknown transition kernel. Denote by \(M=\{\mathcal{S},\mathcal{A},H,\{r_{k,h}\}_{k\in[K],h\in[H]},\{\mathbb{P}_{h}\} _{h\in[H]}\}\) an episodic inhomogeneous MDP, where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(K\) is the number of episodes, \(H\) is the horizon, \(r_{k,h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the reward function, \(\mathbb{P}_{h}(\cdot\mid\cdot,\cdot):\mathcal{S}\times\mathcal{A}\times \mathcal{S}\rightarrow[0,1]\) is the transition kernel. We assume the rewards are deterministic without loss of generality and extending our results to stochastic rewards is straightforward. Let \(S=|\mathcal{S}|\) and \(A=|\mathcal{A}|\).

The learner interacts with the MDP through \(K\) episodes without knowledge of transition kernel \(\{\mathbb{P}_{h}\}_{h\in[H]}\). In each episode \(k\), the environment chooses the reward function \(\{r_{k,h}\}_{h\in[H]}\) and decides the initial state \(s_{k,1}\), where the reward function may be chosen in an adversarial manner and depend on the history of the past \((k-1)\) episodes. Simultaneously, the learner decides a policy \(\pi_{k}=\{\pi_{k,h}\}_{h\in[H]}\) where each \(\pi_{k,h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) is a function that maps a state \(s\) to a distribution over action space \(\mathcal{A}\). In the \(h\) stage in episode \(k\), the learner observes current state \(s_{k,h}\), chooses an action \(a_{k,h}\sim\pi_{k,h}(\cdot\mid s_{k,h})\), and transits to the next state \(s_{k,h+1}\sim\mathbb{P}_{h}(\cdot\mid s_{k,h},a_{k,h})\). Then the learner obtains the reward \(r_{k,h}(s_{k,h},a_{k,h})\) and observes the reward function \(r_{k,h}\) as we consider the full-information setting. At stage \(H+1\), the learner observes the final state \(s_{k,H+1}\) but does not take any action, and the episode \(k\) terminates. Denote by \(T=KH\) the total steps throughout \(K\) episodes.

Linear Mixture MDPs.In this work, we focus on a special class of MDPs called _linear mixture MDPs_, a setting initiated by Ayoub et al. [24] and further studied in the subsequent work [32; 31; 33]. In this setup, the transition kernel can be parameterized as a linear function of a feature mapping \(\phi:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}^{d}\). The formal definition of linear mixture MDPs is as follows.

**Definition 1** (Linear Mixture MDPs).: An MDP \(M=\{\mathcal{S},\mathcal{A},H,\{r_{k,h}\}_{k\in[K],h\in[H]},\{\mathbb{P}_{h}\} _{h\in[H]}\}\) is called an inhomogeneous, episode \(B\)-bounded linear mixture MDP, if there exist a _known_ feature mapping \(\phi(s^{\prime}\mid s,a):\mathcal{S}\times\mathcal{A}\times\mathcal{S} \rightarrow\mathbb{R}^{d}\) and an _unknown_ vector \(\theta_{h}^{s}\in\mathbb{R}^{d}\) with \(\|\theta_{h}^{s}\|_{2}\leq B\), \(\forall h\in[H]\) such that (i) \(\mathbb{P}_{h}(s^{\prime}\mid s,a)=\phi(s^{\prime}\mid s,a)^{\top}\theta_{h}^{s}\) for all \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\) and \(h\in[H]\), (ii) \(\|\phi_{V}(s,a)\|_{2}\triangleq\|\sum_{s^{\prime}\in\mathcal{S}}\phi(s^{\prime} \mid s,a)V(s^{\prime})\|_{2}\leq 1\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and function \(V:\mathcal{S}\rightarrow[0,1]\).

**Dynamic Regret.** For any policy \(\pi=\{\pi_{h}\}_{h\in[H]}\) and any \((k,h,s,a)\in[K]\times[H]\times\mathcal{S}\times\mathcal{A}\), we define the action-value function \(Q_{k,h}^{\pi}\) and value function \(V_{k,h}^{\pi}\) as

\[Q_{k,h}^{\pi}(s,a)=\mathbb{E}_{\pi}\left[\sum_{b^{\prime}=h}^{H}r_{k,h^{\prime}}( s_{h^{\prime}},a_{h^{\prime}})\,\Big{|}\,s_{h}=s,a_{h}=a\right],V_{k,h}^{\pi}(s)= \mathbb{E}_{\pi}\left[\sum_{b^{\prime}=h}^{H}r_{k,h^{\prime}}(s_{h^{\prime}},a _{h^{\prime}})\,\Big{|}\,s_{h}=s\right].\]

The Bellman equation is given by \(Q_{k,h}^{\pi}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}^{\pi}\), and \(V_{k,h}^{\pi}(s)=\mathbb{E}_{a\sim\pi_{h}(\,\cdot\,|\,s)}[Q_{k,h}^{\pi}(s,a)]\) with \(V_{k,H+1}^{\pi}=0\). For simplicity, for any function \(V:\mathcal{S}\rightarrow\mathbb{R}\), we define the operator

\[[\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}(\,\cdot\,|\,s,a)}V(s^{\prime}),\quad[\mathbb{V}_{h}V](s,a)=[\mathbb{P}_{h}V^{2}](s,a)-([ \mathbb{P}_{h}V](s,a))^{2}.\] (3)

As stated in Section 1, dynamic regret is a more appropriate measure compared with static regret for the adversarial environments, which is defined in (2) and we rewrite it below for clarity:

\[\mathrm{D-Regret}(K)=\sum_{k=1}^{K}V_{k,1}^{\pi_{k}^{c}}(s_{k,1})-\sum_{k=1}^ {K}V_{k,1}^{\pi_{k}}(s_{k,1}),\] (4)

where \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\) is any sequence of compared policies. We define \(\pi_{0}^{c}=\pi_{1}^{c}\) to simplify the notation. The non-stationarity measure is defined as \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\lVert\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\rVert_{1,\infty}\).

## 4 Optimal Dynamic Regret with Known \(P_{t}\)

We present our proposed algorithm in Algorithm 1. Similar to previous studies, the algorithm consists of (i) policy improvement phase, and (ii) policy evaluation phase. We introduce the details below. In Sections 4.1, we first consider the case when the transition is _known_ to highlight the challenges even under the ideal setting. Then, we extend the results to the _unknown_ transition setting in Section 4.2.

### Policy Improvement Phase

In the policy improvement phase, the algorithm updates \(\pi_{k}\) based on \(\pi_{k-1}\) using the proximal policy optimization (PPO) method [50]. Specifically, at episode \(k\), we define the following linear function:

\[L_{k-1}(\pi)=V_{k,1}^{\pi_{k-1}}(s_{k,1})+\mathbb{E}_{\pi_{k-1}} \left[\sum_{h=1}^{H}\langle Q_{k-1,h}^{\pi_{k-1}},\pi_{h}(\cdot\mid s_{h})-\pi_ {k-1,h}(\cdot\mid s_{h})\rangle\ \Big{|}\ s_{1}=s_{k,1}\right],\]

which is the first-order Taylor approximation of \(V_{k-1,1}^{\pi}(s_{k,1})\) around \(\pi_{k-1}\). Then, we update \(\pi_{k}\) by

\[\pi_{k}=\operatorname*{arg\,max}_{\pi\in\Delta(\mathcal{A}\mid \mathcal{S},H)}L_{k-1}(\pi)-\frac{1}{\eta}\mathbb{E}_{\pi_{k-1}}\left[\sum_{h= 1}^{H}D_{\mathrm{KL}}\big{(}\pi_{h}(\cdot\mid s_{h})\|\pi_{k-1,h}(\cdot\mid s _{h})\big{)}\right],\] (5)

where \(\eta>0\) is the stepsize and the KL-divergence encourages \(\pi_{k}\) to be close to \(\pi_{k-1}\) so that \(L_{k-1}(\pi)\) is a good approximation of \(V_{k-1,1}^{\pi}(s_{k,1})\). The update rule in (5) takes the following closed form,

\[\pi_{k,h}(\cdot\mid s)\propto\pi_{k-1,h}(\cdot\mid s)\cdot\exp \big{(}\eta\cdot Q_{k-1,h}^{\pi_{k-1}}(s,\cdot)\big{)},\forall h\in[H],s\in \mathcal{S}.\] (6)

We show the update rule in (6) ensures the following guarantee and the proof is in Appendix C.1.

**Lemma 1**.: _The update rule in (6) ensures the following dynamic regret guarantee:_

\[\mathrm{D-Regret}(K)\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\sum _{k=1}^{K}\mathbb{E}_{\pi_{k}^{c}}\left[\widetilde{D}_{\mathrm{KL}}\left(\pi_ {k}^{c}\|\pi_{k}\right)-\widetilde{D}_{\mathrm{KL}}\left(\pi_{k}^{c}\|\pi_{k+ 1}\right)\right].\] (7)

Note that the expectation in the last term in (7) is taken over \(\pi_{k}^{c}\) which may _change_ over episode \(k\). For static regret, i.e., \(\pi_{1}^{c}=\ldots=\pi_{K}^{c}=\pi^{*}\), we can control this term by a standard telescoping argument, which is not viable for dynamic regret analysis. Fei et al. [35] propose a restart strategy to handle this term. Specifically, they restart the algorithm every certain number of steps and decompose the above expectation into \(\mathbb{E}_{\pi_{k}^{c}}[\cdot]=\mathbb{E}_{\pi_{k_{0}}^{c}}[\cdot]+\mathbb{E }_{\pi_{k}^{c}-\pi_{k_{0}}^{c}}[\cdot]\) where \(k_{0}<k\) is the episode in which restart takes place most recently before episode \(k\). For the first expectation \(\mathbb{E}_{\pi_{k_{0}}^{c}}[\cdot]\), they apply a customized telescoping argument to each period as the expectation is taken over the fixed policy. The second expectation \(\mathbb{E}_{\pi_{k}^{c}-\pi_{k_{0}}^{c}}[\cdot]\) involves the difference \(\pi_{k}^{c}-\pi_{k_{0}}^{c}\) and can be bounded by \(P_{T}\). However, as we will show in Theorem 4, their regret bound is suboptimal in terms of \(K\) and \(P_{T}\).

We introduce our approach below. Let us first consider taking expectations over any fixed policy \(\pi\). Denote by \(\delta\) the minimal probability over any action at any state for policies \(\pi_{1},\ldots,\pi_{K}\), i.e., \(\delta=\min_{k\in[K]}\pi_{k}(a\mid s),\forall a\in\mathcal{A},s\in\mathcal{S}\), the last term in (7) can be upper bounded by \(\sum_{k=1}^{K}\mathbb{E}_{\pi}[\widetilde{D}_{\mathrm{KL}}\left(\pi_{k}^{c}\| \pi_{k}\right)-\widetilde{D}_{\mathrm{KL}}\left(\pi_{k}^{c}\|\pi_{k+1}\right)] \leq H\log A+P_{T}\log\frac{1}{\delta}\), showing that we need to control the minimal value of \(\delta\) to obtain a favorable dynamic regret bound. To this end, we slightly modify the update rule in (6) and add a uniform distribution \(\pi^{u}(\cdot\mid s)=\frac{1}{A}\), \(\forall s\in\mathcal{S}\). That is, the policy \(\pi^{u}\) chooses each action with equal probability at any state. Thus, the update rule in (6) is modified as:

\[\pi_{k,h}^{\prime}(\cdot\mid s)\propto\pi_{k-1,h}(\cdot\mid s) \exp(\eta\cdot Q_{k-1,h}^{\pi_{k-1}}(s,\cdot)),\pi_{k,h}(\cdot\mid s)=(1- \gamma)\pi_{k,h}^{\prime}(\cdot\mid s)+\gamma\pi^{u}(\cdot\mid s)\] (8)

for any \(s\in\mathcal{S},h\in[H]\), where \(\gamma\geq 0\) is the exploration parameter. This update is called the _fixed-share_ mechanism in online learning literature [37]. While the fixed-share mechanism is standard to obtain dynamic regret in modern online learning [38], several important new challenges arise in online MDPs due to taking expectations over the policy sequence of _changing_ policies \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\). In particular, we prove that performing (8) ensures the following dynamic regret.

**Lemma 2**.: _Set \(\pi_{1}\) as uniform distribution on \(\mathcal{A}\) for any state \(s\in\mathcal{S}\). The update rule in (8) ensures_

\[\mathrm{D-Regret}(K)\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(P _{T}\log\frac{A}{\gamma}+KH\log\frac{1}{1-\gamma}\right)\] \[+\frac{1}{\eta}\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c} }\left[\sum_{a\in\mathcal{A}}\left(\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{ \pi_{k,h}^{\prime}(a\mid s_{h})}-\pi_{k,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k +1,h}^{\prime}(a\mid s_{h})}\right)\right]\] (9)

The proof can be found in Appendix C.2. In the dynamic regret analysis in online learning, the last term in (9) is usually canceled out through telescoping since we do not need to take expectations [38].

However, this is _not_ the case in online MDPs. Since the expectation is taken over the policy sequence of _changing_ policies \(\pi^{c}_{1},\ldots,\pi^{c}_{K}\), this term cannot be canceled out, which requires a more refined analysis. To address this issue, we decompose one step of the expectation in (9) as follows.1

Footnote 1: With a slight abuse of notations, we omit \((\cdot\mid s_{h})\) for simplicity.

\[\left(\mathbb{E}_{\pi^{c}_{k-1}}\left[\sum_{a\in\mathcal{A}}\pi^{c}_{k-1,h} \log\frac{1}{\tau^{\prime}_{k,h}}\right]-\mathbb{E}_{\pi^{c}_{k}}\left[\sum_{a \in\mathcal{A}}\pi^{c}_{k,h}\log\frac{1}{\pi^{\prime}_{k+1,h}}\right]\right)+ \mathbb{E}_{\pi^{c}_{k-n}\pi^{c}_{k-1}}\left[\sum_{a\in\mathcal{A}}\pi^{c}_{k-1, h}\log\frac{1}{\pi^{\prime}_{k,h}}\right].\]

With this decomposition, the first term can be canceled out through telescoping, yet it remains to control the second term -- the weighted difference between the state-action occupancy measures of policy \(\pi^{c}_{k}\) and \(\pi^{c}_{k-1}\) with weight \(-\pi^{c}_{k-1,h}(a\mid s_{h})\log\pi^{\prime}_{k,h}(a\mid s_{h})\) for state-action \((s_{h},a)\). To control it, we need to (i) ensure the weight is upper bounded by some universal constant, and (ii) bound the unweighted difference between the state-action occupancy measures, which are new challenges that arose in online MDPs compared with standard online learning.

For the first challenge, note that the weight \(-\pi^{c}_{k-1,h}(a\mid s_{h})\log\pi^{\prime}_{k,h}(a\mid s_{h})\) can be large or even infinite since \(\pi^{\prime}_{k,h}\) is the policy before uniform exploration and \(\pi^{\prime}_{k,h}(a\mid s_{h})\) can be arbitrarily small. Fortunately, \(\pi^{\prime}_{k,h}\) is obtained by one-step descent from \(\pi_{k-1,h}\), which is the policy after uniform exploration and can be lower bounded. We provide the following _multiplicative stability lemma_ for the one-step update, which shows \(\pi^{\prime}_{k,h}\) is not far from \(\pi_{k-1,h}\) and thus is also lower bounded.

**Lemma 3** (Multiplicative Stability).: _For any distributions \(p\in\Delta(\mathcal{A})\) with \(p(a)>0\), for all \(a\in A\), and any function \(Q:\mathcal{S}\times\mathcal{A}\rightarrow[0,H]\), it holds for \(p^{\prime}\in\Delta(\mathcal{A})\) with \(p^{\prime}(a)\propto p(a)\cdot\exp(\eta\cdot Q(s,a))\) and \(\eta\leq 1/H\) that \(p^{\prime}(a)\in[p(a)/4,4p(a)]\), for all \(a\in\mathcal{A}\)._

For the second challenge, we show the unweighted difference between the state-action occupancy measures can be bounded by the path length of policies. In particular, we have the following lemma.

**Lemma 4**.: _For any policy sequence \(\pi^{c}_{1},\ldots,\pi^{c}_{K}\), it holds that_

\[\sum_{k=1}^{K}\left(\mathbb{E}_{\pi^{c}_{k}}-\mathbb{E}_{\pi^{c}_{k-1}} \right)\left[\sum_{h=1}^{H}\mathds{1}(s_{h})\mid s_{1}=s_{k,1}\right]\leq\sum _{k=1}^{K}\sum_{h=1}^{H}\sum_{i=1}^{h}\lVert\pi^{c}_{k,i}-\pi^{c}_{k-1,i} \rVert_{1,\infty}=HP_{T}.\]

**Remark 1**.: We note that a similar argument is also used in Fei et al. [35, Appendix B.2.2]. However, they prove this lemma by imposing an additional smooth visitation measures assumption [35, Assumption 1], which is not required in our analysis.

The proofs for Lemmas 3 and 4 can be found in Appendices C.3 and C.4 respectively. Combining Lemmas 2, 3 and 4, we can prove the guarantee for update rule (8). The proof is in Appendix C.5.

**Theorem 1**.: _Set \(\pi_{1}\) as uniform distribution on \(\mathcal{A}\) for any state \(s\in\mathcal{S}\). The update rule in (8) ensures_

\[\text{D-}\mathrm{Regret}(K)\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(H \log A+(1+H)\log\frac{4A}{\gamma}P_{T}+KH\log\frac{1}{1-\gamma}\right).\]

**Remark 2**.: Considering the static regret where \(\pi^{c}_{1}=\ldots=\pi^{c}_{K}=\pi^{*}\), we can recover the \(\mathcal{O}(\sqrt{H^{4}K\log A})\) static regret in Cai et al. [32] under the stationary scenario by setting \(\gamma=0\), that is, without uniform exploration. However, when \(\gamma=0\), the dynamic regret is not bounded as there lacks an upper bound for \(-\log\gamma\), showing the necessity of the fixed-share mechanism.

### Policy Evaluation Phase

Sections 4.1 focus on the simplified scenario where the transition is known. In this subsection, we further consider the unknown transition setting such that it is necessary to evaluate the policy \(\pi_{k}\) based on the \((k-1)\) historical trajectories. To see how the model estimation error enters the dynamic regret, we decompose the dynamic regret in the following lemma.

**Lemma 5** (Fei et al. [35, Lemma 1]).: _Define the model prediction error as \(\iota_{k,h}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}-Q_{k,h}\), the dynamic regret \(\text{D-}\mathrm{Regret}(K)=\sum_{k=1}^{K}V_{k,1}^{\pi^{c}_{k}}(s_{k,1})-\sum _{k=1}^{K}V_{k,1}^{\pi_{k}}(s_{k,1})\) can be written as_

\[\sum_{k,h}\mathbb{E}_{\pi^{c}_{k}}\big{[}\langle Q_{k,h}(s_{h}, \cdot),\pi^{c}_{k,h}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\rangle\big{]}+ \mathcal{M}_{K,H}+\sum_{k,h}\big{(}\mathbb{E}_{\pi^{c}_{k}}[\iota_{k,h}(s_{h},a _{h})]-\iota_{k,h}(s_{k,h},a_{k,h})\big{)},\]

_where \(\mathcal{M}_{K,H}=\sum_{k=1}^{K}\sum_{h=1}^{H}M_{k,h}\) is a martingale that satisfies \(M_{k,h}\leq 4H\), \(\forall k\in[k],h\in[H]\)._

**Remark 3**.: Lemma 5 is independent of the structure of MDPs. The first term in Lemma 5 is the dynamic regret over the estimated action-value function \(Q_{k,h}\), which can be upper bounded by Theorem 5. The second term is a martingale, which can be bounded by Azuma-Hoeffding inequality. The third term is the model estimation error, which is the main focus of this section. Note the model prediction error \(t_{k,h}(s_{h},a_{h})\) can be large for the state-action pairs that are less visited or even unseen. The general approach is incorporating the bonus function into the estimated \(Q\)-function such that \(\iota_{k,h}(s_{h},a_{h})\leq 0\) for all \(s\in\mathcal{S},a\in\mathcal{A}\) (i.e., \(\mathbb{E}_{\pi_{k}^{c}}[\iota_{k,h}(s_{h},a_{h})]\leq 0\)) and we only need to control \(-\iota_{k,h}(s_{k,h},a_{k,h})\), which is the model estimation error at the visited state-action pair \((s_{k,h},a_{k,h})\).

When applied to linear mixture MDPs, the key idea is learning the unknown parameter \(\theta_{h}^{*}\) of the linear mixture MDP and using the learned parameter \(\theta_{k,h}\) to build an optimistic estimator \(Q_{k,h}(\cdot,\cdot)\) such that the model prediction error is non-positive, which is more or less standard. From the definition of linear mixture MDP, for the learned value function \(V_{k,h}(\cdot)\), we have \([\mathbb{P}_{h}V_{k,h+1}](s,a)=\langle\sum_{s^{\prime}}\phi(s^{\prime}\mid s, a)V_{k,h+1}(s^{\prime}),\theta_{h}^{*}\rangle=\langle\phi V_{k,h+1}(s,a), \theta_{h}^{*}\rangle\). Inspired by recent advances in policy evaluation for linear mixture MDPs [31], we adopt the _weighted ridge regression_ to estimate the parameter \(\theta_{h}^{*}\), that is, we construct the estimator \(\widehat{\theta}_{k,h}\) by solving the following weighted ridge regression problem:

\[\widehat{\theta}_{k,h}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{d}} \sum\nolimits_{j=1}^{k-1}\left[\left\langle\phi_{V_{j,h+1}}\left(s_{j,h},a_{j }\right),\theta\right\rangle-V_{j,h+1}\left(s_{j,h+1}\right)\right]^{2}/ \bar{\sigma}_{j,h}^{2}+\lambda\|\theta\|_{2}^{2}.\]

Here, \(\bar{\sigma}_{j,h}^{2}\) is the upper confidence bound of the variance \([\mathbb{V}_{h}V_{j,h+1}](s_{j,h},a_{j,h})\), and we set it as \(\bar{\sigma}_{k,h}=\sqrt{\max\{H^{2}/d,[\mathbb{V}_{k,h}V_{k,h+1}]\left(s_{k,h },a_{k,h}\right)+E_{k,h}\}}\), where \([\mathbb{V}_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})\) is a scalar-valued empirical estimate for the variance of the value function \(V_{k,h+1}\) under the transition probability \(\mathbb{P}_{h}(\cdot\mid s_{k},a_{k})\), and \(E_{k,h}\) is the bonus term to guarantee that the true variance \([\mathbb{V}_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})\) is upper bounded by \([\mathbb{V}_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})+E_{k,h}\) with high probability. Then, the confidence set \(\widehat{\mathcal{C}}_{k,h}\) is constructed as follows:

\[\widehat{\mathcal{C}}_{k,h}=\big{\{}\theta\mid\|\widehat{\Sigma}_{k,h}^{1/2}( \theta-\widehat{\theta}_{k,h})\|_{2}\leq\widehat{\beta}_{k}\big{\}}.\] (10)

where \(\widehat{\Sigma}_{k,h}\) is a covariance matrix based on the observed data, and \(\widehat{\beta}_{k}\) is a radius of the confidence set. Given \(\widehat{\mathcal{C}}_{k,h}\), we estimate the \(Q\)-function following the principle of "optimism in the face of uncertainty" [51] and set it as \(Q_{k,h}(\cdot,\cdot)=[r_{k,h}(\cdot,\cdot)+\max_{\theta\in\widehat{\mathcal{C }}_{k,h}}\langle\theta,\phi_{V_{k,h+1}}(\cdot,\cdot)\rangle]_{[0,H-h+1]}\).

It remains to estimate the variance \([\mathbb{V}_{h}V_{k,h+1}](s_{k,h},a_{k,h})\). By the definition of linear mixture MDPs, we have \([\mathbb{V}_{h}V_{k,h+1}](s_{k,h},a_{k,h})=\langle\phi_{V_{k,h+1}^{2}}(s_{k,h },a_{k,h}),\theta_{h}^{*}\rangle-[\langle\phi_{V_{k,h+1}}(s_{k,h},a_{k,h}), \theta_{h}^{*}\rangle]^{2}\). Therefore, we estimate \([\mathbb{V}_{k,h}V_{k,h+1}]\left(s_{k,h},a_{k,h}\right)\) by the expression below

\[\left[\left\langle\phi_{V_{k,h+1}^{2}}\left(s_{k,h},a_{k,h}\right),\widetilde{ \theta}_{k,h}\right\rangle\right]_{[0,H^{2}]}-\big{[}\big{\langle}\phi_{V_{k,h +1}}\left(s_{k,h},a_{k,h}\right),\widehat{\theta}_{k,h}\big{\rangle}\big{]}_{[ 0,H]}^{2},\] (11)

where \(\widetilde{\theta}_{k,h}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{d}} \sum_{j=1}^{k-1}[\langle\phi_{V_{j,h+1}^{2}}(s_{j,h},a_{j,h}),\theta\rangle-V_{ j,h+1}^{2}(s_{j,h+1})]^{2}+\lambda\|\theta\|_{2}^{2}\). The details are summarized in Lines 10-20 of Algorithm 1 and we provide the following guarantee.

**Theorem 2**.: _Set the parameters as in Lemma 8, with probability at least \(1-\delta\), we have_

\[\sum_{k=1}^{K}\left(V_{k,1}\left(s_{1}^{k}\right)-V_{k,1}^{\pi^{k}}\left(s_{1}^{ k}\right)\right)=\mathcal{M}_{K,H}-\sum_{k=1}^{K}\sum_{h=1}^{H}\iota_{k,h}(s_{k,h},a_{k,h}) \leq\widetilde{\mathcal{O}}\left(\sqrt{dH^{4}K+d^{2}H^{3}K}\right).\]

The proof is given in Appendix C.6. Theorem 2 shows the model estimation error can be bounded. Combining Theorems 1, 2 and Lemma 5, we present the dynamic regret bound in the next section.

### Regret Guarantee: Upper and Lower Bounds

In this section, we provide the regret bound for our algorithm and present a lower bound of the dynamic regret for any algorithm for adversarial linear mixture MDPs with the unknown transition.

**Theorem 3**.: _Set \(\eta=\min\{\sqrt{(P_{T}+\log A)/K},1\}/H\), \(\gamma=1/(KH)\) and \(\widehat{\beta}_{k}\) as in Lemma 8, then with probability at least \(1-\delta\), it holds_

\[\mathrm{D-Regret}(K)\leq\widetilde{\mathcal{O}}\Big{(}\sqrt{dH^{4}K+d^{2}H^{3}K }+\sqrt{H^{4}(K+P_{T})(1+P_{T})}\Big{)},\] (12)

_where \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\|\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\|_{1,\infty}\) is the path length of the compared policies._

**Remark 4** (recovering static regret).: Since static regret is a special case with \(\pi_{k}^{c}=\pi^{*},\forall k\), our result can recover the optimal \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\) static regret when \(H\leq d\), same as the result in He et al. [33].

**Remark 5** (improving linear mixture case).: Our result improves upon the previously best-known \(\widetilde{\mathcal{O}}(dH^{7/4}K^{3/4}+H^{2}K^{2/3}{P_{T}}^{1/3})\) dynamic regret for adversarial linear mixture MDPs of Zhong et al. [36, Theorem 4.6] in terms of the dependence on \(H\), \(K\) and \(P_{T}\).

**Remark 6** (improving tabular case).: For the adversarial tabular MDPs, our result implies an \(\widetilde{\mathcal{O}}(\sqrt{H^{4}S^{2}AK}+H^{2}\sqrt{(K+P_{T})(1+P_{T})})\) dynamic regret. This improves upon the best-known \(\widetilde{\mathcal{O}}(\sqrt{H^{4}S^{2}AK}+H^{2}K^{2/3}{P_{T}}^{1/3})\) result of Fei et al. [35, Theorem 1]. The details are in Appendix B.

We finally establish the lower bound of this problem. The proof can be found in Appendix C.8.

**Theorem 4**.: _Suppose \(B\geq 2,d\geq 4,H\geq 3,K\geq(d-1)^{2}H/2\), for any algorithm and any constant \(\Gamma\in[0,2KH]\), there exists an episodic \(B\)-bounded adversarial linear mixture MDP and compared policies \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\) such that \(P_{T}\leq\Gamma,\) and \(\operatorname{D-Regret}(K)\geq\Omega(\sqrt{d^{2}H^{3}K}+\sqrt{HK(H+\Gamma)})\)._

When \(H\leq d\), the upper bound is \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})})\). Combining it with Theorem 4, we discuss the optimality of our result. We consider the following three regimes.

* Small \(P_{T}\): when \(0\leq P_{T}\leq d^{2}/H\), the upper bound (12) can be simplified as \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\), and the lower bound is \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K})\), hence our result is optimal in terms of \(d\), \(H\) and \(K\).
* Moderate \(P_{T}\): when \(d^{2}/H\leq P_{T}\leq K\), the upper bound (12) can be simplified as \(\widetilde{\mathcal{O}}(\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}K(1+P_{T})})\), and it is minimax optimal in \(d\), \(K\) and \(P_{T}\) but looses a factor of \(H^{\frac{3}{2}}\).
* Large \(P_{T}\): when \(P_{T}\geq K\), any algorithm suffers at most \(\mathcal{O}(HK)\) dynamic regret, while the lower bound is \(\Omega(K\sqrt{H})\). So our result is minimax optimal in \(K\) but looses a factor of \(\sqrt{H}\).

## 5 Towards Optimal Dynamic Regret with Unknown \(P_{t}\)

This section further considers the case when the non-stationarity measure \(P_{T}\) is unknown. By Theorem 1, we need to tune the step size \(\eta\) optimally to balance the number of episodes \(K\) and \(P_{T}\) to achieve a favorable dynamic regret. To address the difficulty of not knowing \(P_{T}\) ahead of time, we develop an online ensemble method to handle this uncertainty, in which a two-layer meta-base structure is maintained. While the methodology can be standard in recent non-stationary online learning [52, 39, 40, 41], new challenges arise in online MDPs. We introduce the details below.

By the performance difference lemma in Cai et al. [32, Lemma 3.2] (as restate in Lemma 13), we can rewrite the dynamic regret as

\[\sum_{k=1}^{K}\left[V_{k,1}^{\pi_{k}^{c}}\left(s_{1}^{k}\right)-V_{k,1}^{\pi_ {k}}\left(s_{1}^{k}\right)\right]=\sum_{k=1}^{K}\mathbb{E}_{\pi_{k}^{c}}\left[ \sum_{h=1}^{H}\left\langle Q_{k,h}^{\pi_{k}}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot \mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\right\rangle\right],\]

where the expectation is taken over the randomness of the state trajectory sampled according to \(\pi_{k}^{c}\). The dynamic regret of online MDPs can be written as the weighted average of some "multi-armed bandits" problems over all states, where the weight for each state is the _unknown_ and _changing_ probability of being visited by \(\pi_{1}^{c},\ldots,\pi_{K}^{c}\). As the optimal step size depends on the unknown non-stationarity measure \(P_{T}\) as shown in Section 4, a natural idea is to the two-layer structure to learn the optimal step size as in recent online convex optimization literature [52, 39, 40, 41].

The general idea is constructing a step size pool \(\mathcal{H}=\{\eta_{1},\ldots,\eta_{N}\}\) to discretize the value range of the optimal step size; and then maintaining multiple base-learners \(\mathcal{B}_{1},\ldots,\mathcal{B}_{N}\), each of which works with a specific step size \(\eta_{i}\). Finally, a meta-algorithm is used to track the best base-learner and yield the final policy. Then, the dynamic regret can be decomposed as follows (omit \((\cdot\mid s_{h})\) for simplicity):

\[\sum_{k=1}^{K}\mathbb{E}_{\pi_{k}^{c}}\left[\sum_{h=1}^{H}\left\langle Q_{k,h}^ {\pi_{k}},\pi_{k,h}^{c}-\pi_{k,h}^{i}\right\rangle\right]+\sum_{k=1}^{K} \mathbb{E}_{\pi_{k}^{c}}\left[\sum_{h=1}^{H}\left\langle Q_{k,h}^{\pi_{k}},\pi_{k,h}^{i}-\pi_{k,h}\right\rangle\right].\]

Since the above decomposition holds for any index \(i\in[N]\), we can always choose the base-learner with optimal step size to analyze and the first term is easy to control. The challenge is to control thesecond term, which is the regret of the meta-algorithm. Different from the standard "Prediction with Expert Advice" problem, it involves an additional expectation over the randomness of states sampled according to \(\pi_{k}^{i}\). This poses a grand challenge compared to conventional online convex optimization where the expectation is not required. Although we can bound this term by \(P_{T}\) again, optimal tuning of the meta-algorithm is hindered as \(P_{T}\) is unknown. Consequently, we opt to upper bound it by the worst-case dynamic regret [53], that is, benchmarking the performance with the best choice of each round, which in turn introduces the dependence on the switching number of the best base-learner.

We introduce our approach as follows. We maintain multiple base-learners, each of which works with a specific step size \(\eta_{i}\). All base-learners update their policies according to the same action-value function \(Q_{k-1,h}(s_{h},\cdot)\) of the combined policy \(\pi_{k-1}\), that is, the base-learner \(\mathcal{B}_{i}\) updates policy by

\[\pi_{k,h}^{i,\prime}(\cdot\mid s)\propto\pi_{k-1,h}^{i}(\cdot\mid s)\exp(\eta _{i}Q_{k-1,h}(s,\cdot)),\pi_{k,h}^{i}(\cdot\mid s)=(1-\gamma)\pi_{k,h}^{i, \prime}(\cdot\mid s)+\gamma\pi^{u}(\cdot\mid s),\] (13)

Then, the meta-algorithm chooses the base-learner by measuring the quality of each base-learner. In our approach, we choose the best base-learner at the last episode, that is,

\[\pi_{k,h}(\cdot\mid s)=\pi_{k,h}^{i^{*}_{k-1,h}}(\cdot\mid s)\text{ with }i^{*}_{k-1,h}(s)=\arg\max_{i\in[N]}\langle Q_{k-1,h}(s,\cdot),\pi_{k-1,h}^{i}( \cdot\mid s)\rangle.\] (14)

The details are summarized in Algorithm 2 of Appendix A and the guarantee is as follows.

**Theorem 5**.: _Set \(\gamma=1/(KH)\), step size pool \(\mathcal{H}=\{\eta_{i}=(2^{i}/H)\sqrt{(\log A)/K}\mid i\in[N]\}\) with \(N=\lfloor\frac{1}{2}\log(\frac{K}{\log A})\rfloor\). Algorithm 2 ensures_

\[\mathrm{D-Regret}(K)\leq\widetilde{\mathcal{O}}\Big{(}\sqrt{dH^{4}K+d^{2}H^{3 }K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})+H^{2}S_{T}^{2}}\Big{)},\]

_where \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\lVert\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\rVert_{1,\infty}\) is the path length of the compared policies, \(S_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{i}}\mathds{1}[i^{*}_{k,h}(s_{h})\neq i^{*}_{k-1,h}(s_{h})]\) is the expected switching number of best base-learner._

Combining it with Theorem 3, we discuss the optimality of our result. We consider two regimes.

* Small \(S_{T}\): when \(S_{T}\leq\max\{d\sqrt{HK},H\sqrt{(K+P_{T})(1+P_{T})}\}\), the term \(S_{T}\) can be subsumed by other terms. In this case, the upper bound in Theorem 5 is entirely the _same_ as that in Theorem 3. This implies we maintain the same guarantees without \(P_{T}\) as algorithmic input.
* Large \(S_{T}\): when \(S_{T}>\max\{d\sqrt{HK},H\sqrt{(K+P_{T})(1+P_{T})}\}\), our result looses a factor of \(HS_{T}\) compared with the result in Theorem 3 for the known \(P_{T}\) setting.

By the above discussion, our result can be optimal in terms of \(K\) and \(P_{T}\) under certain regimes when \(P_{T}\) is unknown. In comparison, the regret bounds achieved via the restart mechanism [35; 36] remain sub-optimal across all regimes even \(P_{T}\) is known. Note that we introduce the notation \(S_{T}\) in the regret analysis, which also reflects the degree of environmental non-stationarity to some extent. Consider the following two examples: (i) in the stationary environment, \(S_{T}\) could be relatively small as the best base-learner would seldom change, and (ii) in the piecewise-stationary environment, \(S_{T}\) would align with the frequency of environmental changes. Indeed, given that \(S_{T}\) is a data-dependent quantity, its inclusion in the regret analysis is not ideal. Deriving bounds that rely exclusively on problem-dependent quantities, like \(P_{T}\), remains a significant open challenge.

## 6 Conclusion and Future Work

In this work, we study the dynamic regret of adversarial linear mixture MDPs with the unknown transition. For the case when \(P_{T}\) is known, we propose a novel policy optimization algorithm that incorporates a _fixed-share_ mechanism without the need for restarts. We show it enjoys a dynamic regret of \(\widetilde{\mathcal{O}}\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})} \big{)}\), strictly improving the previously best-known result of Zhong et al. [36] for the same setting and Fei et al. [35] when specialized to tabular MDPs. We also establish an \(\Omega\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{HK(H+P_{T})}\big{)}\) lower bound, indicating that our algorithm is optimal regarding \(d\), \(K\) and \(P_{T}\) and can be optimal in terms of \(H\) under certain regimes. Moreover, we explore the more complex scenario where \(P_{T}\) is unknown. We show this setting presents unique challenges that distinguish online MDPs from conventional online convex optimization. We introduce a novel two-layer algorithm and show its dynamic regret guarantee is attractive under certain regimes.

There are several important future works to investigate. First, how to remove the dependence on the switching number \(S_{T}\) is an important open question. Moreover, we focus on the full-information feedback in this work, it remains an open problem to extend the results to the bandit feedback.

## Acknowledgements

This research was supported by National Key R&D Program of China (2022ZD0114800) and NSFC (62206125, 61921006). Peng Zhao was supported in part by the Xiaomi Foundation.

## References

* [1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing Atari with deep reinforcement learning. _ArXiv preprint_, 1312.5602, 2013.
* [2] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. _Nature_, pages 484-489, 2016.
* [3] John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In _Proceedings of the 32nd International Conference on Machine Learning (ICML)_, pages 1889-1897, 2015.
* [4] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _Proceedings of the 33nd International Conference on Machine Learning (ICML)_, pages 1329-1338, 2016.
* [5] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1192-1202, 2016.
* [6] William Yang Wang, Jiwei Li, and Xiaodong He. Deep reinforcement learning for NLP. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)_, pages 19-21, 2018.
* [7] Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. Wiley, 1994.
* [8] Eyal Even-Dar, Sham. M. Kakade, and Yishay Mansour. Online Markov decision processes. _Mathematics of Operations Research_, pages 726-736, 2009.
* [9] Jia Yuan Yu, Shie Mannor, and Nahum Shimkin. Markov decision processes with arbitrary reward processes. _Mathematics of Operations Research_, pages 737-757, 2009.
* [10] Gergely Neu, Andras Gyorgy, Csaba Szepesvari, and Andras Antos. Online Markov decision processes under bandit feedback. In _Advances in Neural Information Processing Systems 24 (NIPS)_, pages 1804-1812, 2010.
* [11] Gergely Neu, Andras Gyorgy, and Csaba Szepesvari. The online loop-free stochastic shortest-path problem. In _Proceedings of 23rd Conference on Learning Theory (COLT)_, pages 231-243, 2010.
* [12] Alexander Zimin and Gergely Neu. Online learning in episodic Markovian decision processes by relative entropy policy search. In _Advances in Neural Information Processing Systems 26 (NIPS)_, pages 1583-1591, 2013.
* [13] Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial Markov decision processes. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, pages 5478-5486, 2019.
* [14] Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial Markov decision processes with bandit feedback and unknown transition. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 4860-4869, 2020.

* [15] Aviv Rosenberg and Yishay Mansour. Stochastic shortest path with adversarially changing costs. In _Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI)_, pages 2936-2942, 2021.
* [16] Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Minimax regret for stochastic shortest path with adversarial costs and known transition. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, pages 1180-1215, 2021.
* [17] Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, pages 6995-7004, 2019.
* [18] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. In _Proceedings of the 33rd Conference on Learning Theory (COLT)_, pages 2137-2143, 2020.
* [19] Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In _Proceedings of the 8th International Conference on Learning Representations (ICLR)_, 2020.
* [20] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1954-1964, 2020.
* [21] Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In _Proceedings of the 9th International Conference on Learning Representations (ICLR)_, 2021.
* [22] Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, pages 4171-4180, 2021.
* [23] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear Markov decision processes. _ArXiv preprint_, 2212.06132, 2022.
* [24] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 463-474, 2020.
* [25] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In _Proceedings of the 23th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 2010-2020, 2020.
* [26] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 10746-10756, 2020.
* [27] Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted MDPs with feature mapping. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, pages 12793-12802, 2021.
* [28] Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S. Du. Improved variance-aware confidence sets for linear bandits and linear mixture MDP. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 4342-4355, 2021.
* [29] Dongruo Zhou and Quanquan Gu. Computationally efficient horizon-free reinforcement learning for linear mixture MDPs. _ArXiv preprint_, 2205.11507, 2022.
* [30] Yue Wu, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal regret for learning infinite-horizon average-reward MDPs with linear function approximation. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 3883-3913, 2022.

* [31] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture Markov decision processes. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, pages 4532-4576, 2021.
* [32] Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 1283-1294, 2020.
* [33] Jiafan He, Dongruo Zhou, and Quanquan Gu. Near-optimal policy optimization algorithms for learning adversarial linear mixture MDPs. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 4259-4280, 2022.
* [34] Peng Zhao, Long-Fei Li, and Zhi-Hua Zhou. Dynamic regret of online Markov decision processes. In _Proceedings of the 39th International Conference on Machine Learning (ICML)_, pages 26865-26894, 2022.
* [35] Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization in non-stationary environments. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 6743-6754, 2020.
* [36] Han Zhong, Zhuoran Yang, Zhaoran Wang, and Csaba Szepesvari. Optimistic policy optimization is provably efficient in non-stationary MDPs. _ArXiv preprint_, arXiv:2110.08984, 2021.
* [37] Mark Herbster and Manfred K. Warmuth. Tracking the best expert. _Machine Learning_, 32(2):151-178, 1998.
* [38] Nicolo Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and feels no regret). In _Advances in Neural Information Processing Systems 25 (NIPS)_, pages 989-997, 2012.
* [39] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth functions. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 12510-12520, 2020.
* [40] Dheeraj Baby and Yu-Xiang Wang. Optimal dynamic regret in exp-concave online learning. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, pages 359-409, 2021.
* [41] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Adaptivity and non-stationarity: Problem-dependent dynamic regret for online convex optimization. _ArXiv preprint_, 2112.14368, 2021.
* [42] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In _Proceedings of the 34th International Conference on Machine Learning (ICML)_, pages 1704-1713, 2017.
* [43] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In _Proceedings of the 32nd Conference on Learning Theory (COLT)_, pages 2898-2933, 2019.
* [44] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with rich observations via latent state decoding. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, pages 1665-1674, 2019.
* [45] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 13406-13418, 2021.
* [46] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, pages 1563-1600, 2010.
* [47] Ronald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for reinforcement learning. In _Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 81-90, 2019.

* [48] Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationary Markov decision processes: The blessing of (more) optimism. _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 1843-1854, 2020.
* [49] Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. In _Proceedings of the 34th Conference on Learning Theory (COLT)_, pages 4300-4354, 2021.
* [50] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _ArXiv preprint_, 1707.06347, 2017.
* [51] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In _Advances in Neural Information Processing Systems 24 (NIPS)_, pages 2312-2320, 2011.
* [52] Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In _Advances in Neural Information Processing Systems 31 (NeurIPS)_, pages 1330-1340, 2018.
* [53] Peng Zhao and Lijun Zhang. Improved analysis for dynamic regret of strongly convex and smooth functions. In _Proceedings of the 3rd Conference on Learning for Dynamics and Control (L4DC)_, pages 48-59, 2021.
* [54] Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Impossible tuning made possible: A new expert algorithm and its applications. In _Proceedings of the 34th Conference On Learning Theory (COLT)_, pages 1216-1259, 2021.
* [55] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning provably efficient? In _Advances in Neural Information Processing Systems 31 (NeurIPS)_, pages 4868-4878, 2018.

Algorithm with Unknown \(P_{t}\)

In this part, we present the POWERS-FixShare-OnE algorithm in Algorithm 2 for the case when \(P_{T}\) is unknown. The algorithm is based on the POWERS-FixShare algorithm and we further employ an online ensemble structure to eliminate the algorithmic dependence on \(P_{T}\). In Line 5, each base-learner updates her policy with a specific step size \(\eta_{i}\) and the meta-learner selects the best policy among the base-learners in Line 6. In the policy evaluation phase (Line 11 - 21), we use the same estimator as in Algorithm 1 to estimate the parameter \(\theta_{h}^{*}\) and construct the confidence set.

```
1:step size pool \(\mathcal{H}\), exploration parameter \(\gamma\) and regularization parameter \(\lambda\).
2:Initialize \(\{\pi_{0,h}^{i}(\cdot\mid s)\}_{h=1}^{H},\forall i\in[N],s\in\mathcal{S}\) as uniform distribution on \(\mathcal{A}\), \(\{p_{0,h}(s)\}_{h=1}^{H},\forall s\in\mathcal{S}\) as uniform distribution on \(N\) and set \(\{Q_{0,h}(\cdot,\cdot)\}_{h=1}^{H}\) as zero function.
3:for\(k=1,2,\cdots,K\)do
4: Receive the initial state \(s_{k,1}\).
5:for\(h=1,2,\cdots,H\)do
6: For all \(h\in[H],s\in\mathcal{S}\), each base-learner \(\mathcal{B}_{i},\forall i\in[N]\) updates policy by \[\pi_{k,h}^{i,\prime}(\cdot\mid s)\propto\pi_{k-1,h}^{i}(\cdot\mid s)\exp \left(\eta_{i}Q_{k-1,h}(s,\cdot)\right),\pi_{k,h}^{i}(\cdot\mid s)=(1-\gamma) \pi_{k,h}^{i,\prime}(\cdot\mid s)+\gamma\pi^{u}(\cdot\mid s).\]
7: Set \(\pi_{k,h}(\cdot\mid s)=\pi_{k,h}^{i^{*}_{k-1,h}}(\cdot\mid s)\) with \(i^{*}_{k-1,h}(s)=\arg\max_{i\in[N]}\langle Q_{k-1,h}(s,\cdot),\pi_{k-1,h}^{i}( \cdot\mid s)\rangle\).
8: Take the action following \(a_{k,h}\sim\pi_{k,h}(\cdot\mid s_{k,h})\) and transit to the next state \(s_{k,h+1}\).
9: Obtain reward \(r_{k,h}(s_{k,h},a_{k,h})\) and observe the reward function \(r_{k,h}(\cdot,\cdot)\).
10:endfor
11: Initialize \(V_{k,H+1}(\cdot)\) as a zero function.
12:for\(h=H,H-1,\cdots,1\)do
13: Set \(Q_{k,h}(\cdot,\cdot)\leftarrow\big{[}r_{k,h}(\cdot,\cdot)+\langle\widehat{ \theta}_{k,h},\phi_{V_{k,h+1}}(\cdot,\cdot)\rangle+\widehat{\beta}_{k}\big{\|} \widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}(\cdot,\cdot)\big{\|}_{2}\big{]}_ {[0,H-h+1]}\).
14: Set \(V_{k,h}(\cdot)\leftarrow\mathbb{E}_{a\sim\pi_{k,h}(\cdot\mid\cdot)}\left[Q_{ k,h}(\cdot,\cdot)\right]\).
15: Set the estimated variance \(\big{[}\bar{\mathbb{V}}_{k,h}V_{k,h+1}\big{]}\left(s_{k,h},a_{k,h}\right)\) as in (11), bonus \(E_{k,h}\) as in (21).
16:\(\widehat{\Sigma}_{k+1,h}\leftarrow\widehat{\Sigma}_{k,h}+\bar{\sigma}_{k,h}^{- 2}\phi_{V_{k,h+1}}\left(s_{k,h},a_{k,h}\right)\phi_{V_{k,h+1}}\left(s_{k,h},a_ {k,h}\right)^{\top}\).
17:\(\widehat{b}_{k+1,h}\leftarrow\widehat{b}_{k,h}+\bar{\sigma}_{k,h}^{-2}\phi_{V_ {k,h+1}}\left(s_{k,h},a_{k,h}\right)V_{k,h+1}\left(s_{k+1}^{k}\right)\).
18:\(\widetilde{\Sigma}_{k+1,h}\leftarrow\widetilde{\Sigma}_{k,h}+\phi_{V_{k,h+1}^{2 }}\left(s_{k,h},a_{k,h}\right)\phi_{V_{k,h+1}^{2}}\left(s_{k,h},a_{k,h}\right) ^{\top}\).
19:\(\widetilde{b}_{k+1,h}\leftarrow\widetilde{b}_{k,h}+\phi_{V_{k,h+1}^{2}}\left(s _{k,h},a_{k,h}\right)V_{k,h+1}^{2}\left(s_{k+1}^{k}\right)\).
20:\(\widehat{\theta}_{k+1,h}\leftarrow\widetilde{\Sigma}_{k+1,h}^{-1}\widehat{b}_{k +1,h},\widehat{\theta}_{k+1,h}\leftarrow\widetilde{\Sigma}_{k+1,h}^{-1} \widetilde{b}_{k+1,h}\).
21:endfor
22:endfor ```

**Algorithm 2** POWERS-FixShare-OnE

## Appendix B Recovering Tabular Case

In this part, we show the result of our algorithm when specialized to the tabular case. Note in the linear case, we adopt the _weighted ridge regression_ to estimate the parameter \(\theta_{h}^{*}\), that is, we construct the estimator \(\widehat{\theta}_{k,h}\) by solving the following weighted ridge regression problem:

\[\widehat{\theta}_{k,h}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{d}} \sum\nolimits_{j=1}^{k-1}\left[\left\langle\phi_{V_{j,h+1}}\left(s_{j,h},a_{j,h} \right),\theta\right\rangle-V_{j,h+1}\left(s_{j,h+1}\right)\right]^{2}/\bar{ \sigma}_{j,h}^{2}+\lambda\|\theta\|_{2}^{2}.\]

In the tabular case, we simply set \(\bar{\sigma}_{j,h}=1\), and compute \(\phi_{V_{k,h+1}}\) by taking the sample mean of \(\{V_{k,h+1}(s_{j,h+1})\}_{j\in[k-1]}\). That is, we set it as

\[\phi_{V_{j,h+1}}\left(s_{j,h},a_{j,h}\right)=\sum_{s^{\prime}\in\mathcal{S}} \frac{N_{k}(s_{j,h},a_{j,h},s^{\prime})}{N_{k}(s_{j,h},a_{j,h})+\lambda}\cdot V_{ j,h+1}(s^{\prime}),\]of each \((s,a)\), where \(N_{k}\) counts the number of times each tuple \((s,a,s)\) or \((s,a)\) has been visited up to episode \(k\). Then, we construct the estimator \(\widehat{\theta}_{k,h}\) by solving the linear regression problem:

\[\widehat{\theta}_{k,h}=\operatorname*{arg\,min}_{\theta\in\mathbb{R}^{d}}\sum \nolimits_{j=1}^{k-1}\big{[}\big{\langle}\phi_{V_{j,h+1}}\left(s_{j,h},a_{j,h} \right),\theta\big{\rangle}-V_{j,h+1}\left(s_{j,h+1}\right)\big{]}^{2}+\lambda \|\theta\|_{2}^{2}.\]

Then, the confidence set \(\widehat{\mathcal{C}}_{k,h}\) is constructed as follows:

\[\widehat{\mathcal{C}}_{k,h}=\big{\{}\theta\mid\|\widehat{\Sigma}_{k,h}^{1/2}( \theta-\widehat{\theta}_{k,h})\|_{2}\leq\beta\big{\}}.\]

The remaining part of the algorithm is the same as the linear case. The following theorem shows the result of our algorithm in the tabular case.

**Theorem 6**.: _Set \(\gamma=1/(KH)\), step size pool \(\mathcal{H}=\{\eta_{i}=(2^{i}/H)\sqrt{(\log A)/K}\mid i\in[N]\}\) with \(N=\lfloor\frac{1}{2}\log(\frac{K}{\log A})\rfloor\) and \(\beta=H\sqrt{S\log(dKH/\delta)}\), with probability at least \(1-\delta\), it holds_

\[\mathrm{D-Regret}(K)\leq\widetilde{\mathcal{O}}\Big{(}\sqrt{H^{4}S^{2}AK}+ \sqrt{H^{4}(K+P_{T})(1+P_{T})}\Big{)},\]

_where \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\|\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\|_{1,\infty}\) is the path length of the compared policy sequence._

Proof.: By Lemma 5, we decompose the dynamic regret as follows.

\[\mathrm{D-Regret}(K)\leq \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}\big{[} \langle Q_{k,h}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot \mid s_{h})\rangle\big{]}+\mathcal{M}_{K,H}\] \[+\sum_{k=1}^{K}\sum_{h=1}^{H}\big{(}\mathbb{E}_{\pi_{k}^{c}}[l_{k,h}(s_{h},a_{h})]-\iota_{k,h}(s_{k,h},a_{k,h})\big{)},\]

Note that the policy evaluation phase is the same as the one in Fei et al. [35, Algorithm 3], by the estimator bound in Fei et al. [35, Lemmas 7 and 9], we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\iota_{k,h}(s_{h},a_{h})] \leq 0,\sum_{k=1}^{K}\sum_{h=1}^{H}-\iota_{k,h}(s_{k,h},a_{k,h})\leq \mathcal{O}\Big{(}\sqrt{H^{4}S^{2}AT\log^{2}(SAKH/\delta)}\Big{)}.\] (15)

The remaining proof is the same as the proof of Theorem 3 in Appendix C.7. 

## Appendix C Proofs

In this section, we provide the proof of the results in this paper.

### Proof of Lemma 1

Proof.: By the definition of dynamic regret, we have

\[\mathrm{D-Regret}(K)\] \[= \sum_{k=1}^{K}V_{k,1}^{\pi_{k}^{c}}(s_{k,1})-\sum_{k=1}^{K}V_{k, 1}^{\pi_{k}}(s_{k,1})\] \[= \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}\Big{[}\langle Q _{k,h}^{\pi_{k}}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot \mid s_{h})\rangle\Big{\mid}\ s_{1}=s_{k,1}\Big{]}\] \[\leq \frac{\eta KH^{3}}{2}+\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{ \pi_{k}^{c}}\bigg{[}D_{\mathrm{KL}}\left(\pi_{k,h}^{c}(\cdot\mid s_{h})\|\pi_ {k,h}(\cdot\mid s_{h})\right)-D_{\mathrm{KL}}\left(\pi_{k,h}^{c}(\cdot\mid s _{h})\|\pi_{k+1,h}(\cdot\mid s_{h})\right)\bigg{]},\]

where the equality holds by the performance difference lemma in Lemma 13 and the inequality holds by the one-step descent guarantee in Lemma 14. This finishes the proof.

### Proof of Lemma 2

Proof.: Note the update rule is

\[\pi^{\prime}_{k,h}(\cdot\mid s)\propto\pi_{k-1,h}(\cdot\mid s)\exp(\eta\cdot Q^{ \pi_{k-1,h}}_{k-1,h}(s,\cdot)),\pi_{k,h}(\cdot\mid s)=(1-\gamma)\pi^{\prime}_{k,h}(\cdot\mid s)+\gamma\pi^{u}(\cdot\mid s)\]

By Lemma 14, for any \(s_{h}\in\mathcal{S},h\in[H],k\in[K]\), we have

\[\langle Q^{\pi_{k}}_{k,h}(s_{h},\cdot),\pi^{c}_{k,h}(\cdot\mid s_ {h})-\pi_{k,h}(\cdot\mid s_{h})\rangle\] \[\leq\frac{\eta H^{2}}{2}+\frac{1}{\eta}\bigg{(}D_{\mathrm{KL}} \left(\pi^{c}_{k,h}(\cdot\mid s_{h})\|\pi_{k,h}(\cdot\mid s_{h})\right)-D_{ \mathrm{KL}}\left(\pi^{c}_{k,h}(\cdot\mid s_{h})\|\pi^{\prime}_{k+1,h}(\cdot \mid s_{h})\right)\bigg{)}\] \[=\frac{\eta H^{2}}{2}+\frac{1}{\eta}\sum_{a\in\mathcal{A}}\bigg{(} \pi^{c}_{k,h}(a\mid s_{h})\log\frac{1}{\pi_{k,h}(a\mid s_{h})}-\pi^{c}_{k,h}(a \mid s_{h})\log\frac{1}{\pi^{\prime}_{k+1,h}(a\mid s_{h})}\bigg{)},\] (16)

where the equality holds by the definition of KL divergence. We decompose the last term as follows:

\[\sum_{a\in\mathcal{A}}\bigg{(}\pi^{c}_{k,h}(a\mid s_{h})\log \frac{1}{\pi_{k,h}(a\mid s_{h})}-\pi^{c}_{k,h}(a\mid s_{h})\log\frac{1}{\pi^{ \prime}_{k+1,h}(a\mid s_{h})}\bigg{)}\] \[=\sum_{a\in\mathcal{A}}\bigg{(}\pi^{c}_{k,h}(a\mid s_{h})\log \frac{1}{\pi_{k,h}(a\mid s_{h})}-\pi^{c}_{k-1,h}(a\mid s_{h})\log\frac{1}{\pi^ {\prime}_{k,h}(a\mid s_{h})}\bigg{)}\] (17) \[\qquad\qquad\qquad\qquad+\sum_{a\in\mathcal{A}}\bigg{(}\pi^{c}_{k -1,h}(a\mid s_{h})\log\frac{1}{\pi^{\prime}_{k,h}(a\mid s_{h})}-\pi^{c}_{k,h}( a\mid s_{h})\log\frac{1}{\pi^{\prime}_{k+1,h}(a\mid s_{h})}\bigg{)}.\]

For the first term, we have

\[\sum_{a\in\mathcal{A}}\bigg{(}\pi^{c}_{k,h}(a\mid s_{h})\log \frac{1}{\pi_{k,h}(a\mid s_{h})}-\pi^{c}_{k-1,h}(a\mid s_{h})\log\frac{1}{\pi^ {\prime}_{k,h}(a\mid s_{h})}\bigg{)}\] \[=\sum_{a\in\mathcal{A}}\bigg{(}\left(\pi^{c}_{k,h}(a\mid s_{h})- \pi^{c}_{k-1,h}(a\mid s_{h})\right)\log\frac{1}{\pi_{k,h}(a\mid s_{h})}+\pi^{ c}_{k-1,h}(a\mid s_{h})\log\frac{\pi^{\prime}_{k,h}(a\mid s_{h})}{\pi_{k,h}(a\mid s _{h})}\bigg{)}.\]

By the update rule, we have \(1\leq 1/\pi_{k,h}(a\mid s_{h})\leq A/\gamma\) and \(\pi^{\prime}_{k,h}(a\mid s_{h})/\pi_{k,h}(a\mid s_{h})\leq 1/(1-\gamma)\). Therefore, we have

\[\sum_{a\in\mathcal{A}}\bigg{(}(\pi^{c}_{k,h}(a\mid s_{h})-\pi^{ c}_{k-1,h}(a\mid s_{h})\big{)}\log\frac{1}{\pi_{k,h}(a\mid s_{h})}+\pi^{c}_{k-1,h}(a \mid s_{h})\log\frac{\pi^{\prime}_{k,h}(a\mid s_{h})}{\pi_{k,h}(a\mid s_{h})} \bigg{)}\] \[\leq\|\pi^{c}_{k,h}(\cdot\mid s_{h})-\pi^{c}_{k-1,h}(\cdot\mid s_{ h})\|_{1}\cdot\log\frac{A}{\gamma}+\log\frac{1}{1-\gamma}.\] (18)

Then, the dynamic regret is bounded as follows:

\[\mathrm{D-Regret}(K)\] \[=\sum_{k=1}^{K}V^{\pi^{c}_{k}}_{k,1}(s_{k,1})-\sum_{k=1}^{K}V^{ \pi_{k}}_{k,1}(s_{k,1})\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi^{c}_{k}}\left[\langle Q ^{\pi_{k}}_{k,h}(s_{h},\cdot),\pi^{c}_{k,h}(\cdot\mid s_{h})-\pi_{k,h}(\cdot \mid s_{h})\rangle\,\Big{|}\,s_{1}=s_{k,1}\right]\] \[\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\sum_{k=1}^{K}\sum_{h=1}^{ H}\sum_{a\in\mathcal{A}}\mathbb{E}_{\pi^{c}_{k}}\left[\pi^{c}_{k,h}(a\mid s_{h}) \log\frac{1}{\pi_{k,h}(a\mid s_{h})}-\pi^{c}_{k,h}(a\mid s_{h})\log\frac{1}{ \pi^{\prime}_{k+1,h}(a\mid s_{h})}\right]\] \[\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\sum_{k=1}^{K}\sum_{h=1}^{ H}\mathbb{E}_{\pi^{c}_{k}}\left[\|\pi^{c}_{k,h}(\cdot\mid s_{h})-\pi^{c}_{k-1,h}( \cdot\mid s_{h})\|_{1}\cdot\log\frac{A}{\gamma}+\log\frac{1}{1-\gamma}\right]\] \[\quad+\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi^{c}_{k}}\left[ \sum_{a\in\mathcal{A}}\left(\pi^{c}_{k-1,h}(a\mid s_{h})\log\frac{1}{\pi^{\prime}_ {k,h}(a\mid s_{h})}-\pi^{c}_{k,h}(a\mid s_{h})\log\frac{1}{\pi^{\prime}_{k+1,h}( a\mid s_{h})}\right)\right]\]\[\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(P_{T}\log\frac{A}{ \gamma}+KH\log\frac{1}{1-\gamma}\right)\] \[\quad+\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[ \sum_{a\in\mathcal{A}}\left(\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k,h}^{ \prime}(a\mid s_{h})}-\pi_{k,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k+1,h}^{ \prime}(a\mid s_{h})}\right)\right],\]

where the first inequality holds by (16), the second inequality is due to (17) and (18), and the last inequality holds by the definition of \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\lVert\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\rVert_{1,\infty}\). It ends the proof. 

### Proof of Lemma 3

Proof.: Lemma 3 is a simplified version of Chen et al. (2016, Lemma 17) and we prove it in our notations below for self-containedness. It is easy to verify that the update rule \(p^{\prime}(a)\propto p(a)\cdot\exp(\eta\cdot Q(s,a))\) is equivalent to the update

\[p^{\prime}=\operatorname*{arg\,max}_{p^{\prime}\in\Delta(\mathcal{A})}\eta \langle p^{\prime},Q(s,\cdot)\rangle+D_{\mathrm{KL}}(p^{\prime}\lVert p).\]

Thus, by the KKT condition, we have for some \(\lambda\) and \(\mu(a)\geq 0\), such that

\[Q(s,a)-\frac{1}{\eta}\log\frac{p^{\prime}(a)}{p(a)}+\lambda+\mu(a)=0,\text{ and }\mu(a)p^{\prime}(a)=0,\forall a\in\mathcal{A}.\]

The above equations give the closed-form solution \(p^{\prime}(a)=p(a)\exp(\eta(Q(s,a)+\lambda+\mu(a)))\). First, we prove that for all \(a\in\mathcal{A}\) we have \(\mu(a)=0\). Indeed, when \(\mu(a)\neq 0\), by \(\mu(a)p^{\prime}(a)=0\), we have \(p^{\prime}(a)=p(a)\exp(\eta(Q(s,a)+\lambda+\mu(a)))=0\), which contradicts with \(p(a)>0\).

Then, we now separately discuss two cases.

Case 1: \(\min_{a\in\mathcal{A}}Q(s,a)\neq\max_{a\in\mathcal{A}}Q(s,a)\).In this case, we first show that \(\min_{a\in\mathcal{A}}-Q(s,a)\leq\lambda\leq\max_{a\in\mathcal{A}}-Q(s,a)\). We prove it by contradiction: If \(\lambda\geq\max_{a\in\mathcal{A}}-Q(s,a)\), then we have

\[\sum_{a\in\mathcal{A}}p^{\prime}(a)=\sum_{a\in\mathcal{A}}p(a)\exp(\eta(Q(s,a )+\lambda+\mu(a)))\geq\sum_{a\in\mathcal{A}}p(a)\geq 1.\]

contradicting with \(p^{\prime}\in\Delta(\mathcal{A})\). A similar argument holds for the case \(\lambda\leq\min_{a\in\mathcal{A}}(-Q(s,i))\). Thus, we have \(\lambda\in[\min_{a\in\mathcal{A}}(-Q(s,a)),\max_{a\in\mathcal{A}}(-Q(s,a))]\). Then, we have \(|Q(s,a)+\lambda+\mu(a)|\leq\max_{a\in\mathcal{A}}Q(s,a)-\min_{a\in\mathcal{A} }Q(s,a)\leq H\). By the condition on \(\eta H\leq 1\), we have \(p^{\prime}(a)\in[\exp(-1)p(a),\exp(1)p(a)]\in[1/(4p(a)),4p(a)]\).

Case 2: \(\min_{a\in\mathcal{A}}Q(s,a)=\max_{a\in\mathcal{A}}Q(s,a)\).In this case, it is clear that \(\lambda=-Q(s,a)\) must hold for all \(a\in\mathcal{A}\) to make \(p\) and \(p^{\prime}\) both discussions. Thus \(p^{\prime}(a)=p(a)\) for all \(a\in\mathcal{A}\).

Combining the above two cases finishes the proof. 

### Proof of Lemma 4

To prove Lemma 4, we first introduce the following two lemmas. Denote by \(\mathbb{P}_{h}^{\pi_{h}}(s^{\prime}\mid s)=\sum_{a\in\mathcal{A}}\mathbb{P}_{h} (s^{\prime}\mid s,a)\pi_{h}(a\mid s)\) the transition kernel of the MDP in step \(h\) under policy \(\pi_{h}\) and recall that \(\lVert\pi-\pi^{\prime}\rVert_{1,\infty}=\max_{s\in\mathcal{S}}\lVert\pi(\cdot \mid s)-\pi^{\prime}(\cdot\mid s)\rVert\). The first lemma shows the difference between the state distribution of two policies can be bounded by the path length of the policies.

**Lemma 6** (Zhao et al. (2016, Lemma 7)).: _For any state distribution \(d\), policy pair \(\pi\) and \(\pi^{\prime}\), and transition kernel \(\mathbb{P}\), we have_

\[\big{\lVert}d\mathbb{P}_{h}^{\pi_{h}}(\cdot,s)-d\mathbb{P}_{h}^{\pi_{h}^{\prime }}(\cdot,s)\big{\rVert}_{1}\leq\lVert\pi_{h}-\pi_{h}^{\prime}\rVert_{1,\infty}, \forall h\in[H].\]

Proof.: Consider the case when \(d\) is a delta function on \(s\). The difference in the next distributions is

\[\big{\lVert}\mathbb{P}_{h}^{\pi_{h}}(\cdot,s)-\mathbb{P}_{h}^{\pi_ {h}}(\cdot,s)\big{\rVert}_{1} =\sum_{s^{\prime}\in\mathcal{S}}\sum_{a\in\mathcal{A}}\lvert \mathbb{P}(s^{\prime}\mid s,a)(\pi_{h}(a\mid s)-\pi_{h}(a\mid s))\rvert\] \[\leq\sum_{s^{\prime}\in\mathcal{S}}\sum_{a\in\mathcal{A}}\mathbb{ P}(s^{\prime}\mid s,a)\lVert\pi_{h}(a\mid s)-\pi_{h}^{\prime}(a\mid s)\rVert_{1}\]\[\leq\sum_{a\in\mathcal{A}}\lvert\pi(a\mid s)-\pi^{\prime}(a\mid s) \rvert\leq\lVert\pi_{h}-\pi^{\prime}_{h}\rVert_{1,\infty}.\]

Linearity of expectation leads to the result for arbitrary distributions. This finishes the proof. 

The second lemma shows the difference between the state distribution of the policy starting from the different initial distributions can be bounded by the difference between the initial distributions.

**Lemma 7** (Zhao et al. [34, Lemma 8]).: _For any two initial distributions \(d\) and \(d^{\prime}\), transition kernel \(\mathbb{P}\) and policy \(\pi\), we have_

\[\left\lVert d\mathbb{P}_{h}^{\pi_{h}}-d^{\prime}\mathbb{P}_{h}^{\pi_{h}} \right\rVert_{1}\leq\lVert d-d^{\prime}\rVert_{1},\forall h\in[H].\]

Proof.: Note the relationship that \(d(s^{\prime})=\sum_{s\in\mathcal{S}}d(s)\mathbb{P}_{s,s^{\prime}}^{\pi_{h}}\), we have

\[\left\lVert d\mathbb{P}_{h}^{\pi_{h}}-d^{\prime}\mathbb{P}_{h}^{ \pi_{h}}\right\rVert_{1} =\sum_{s^{\prime}}\Big{\lvert}\sum_{s}d(s)\mathbb{P}_{h}^{\pi_{h} }(s^{\prime}\mid s)-d^{\prime}(s)\mathbb{P}_{h}^{\pi_{h}}(s^{\prime}\mid s) \Big{\rvert}\] \[\leq\sum_{s^{\prime}}\sum_{s}\lvert d(s)\mathbb{P}_{h}^{\pi_{h} }(s^{\prime}\mid s)-d^{\prime}(s)\mathbb{P}_{h}^{\pi_{h}}(s^{\prime}\mid s) \big{\rvert}\] \[=\sum_{s^{\prime}}\sum_{s}\lvert d(s)-d^{\prime}(s)\rvert\, \mathbb{P}_{h}^{\pi_{h}}(s^{\prime}\mid s)\] \[=\sum_{s}\lvert d(s)-d^{\prime}(s)\rvert\sum_{s^{\prime}}\mathbb{ P}_{h}^{\pi_{h}}(s^{\prime}\mid s)\] \[=\sum_{s}\lvert d(s)-d^{\prime}(s)\rvert=\lVert d-d^{\prime} \rVert_{1}.\]

This finishes the proof. 

Now, we are ready to prove Lemma 4.

Proof of Lemma 4.: For any policy \(\pi\) and \(\pi^{\prime}\) and any initial state \(s_{1}\), denote by \(d_{h}\) the distribution of the MDP in step \(h\) under policy \(\pi_{h}\) and \(d^{\prime}_{h}\) the distribution of the MDP in step \(h\) under policy \(\pi^{\prime}_{h}\), that is \(d_{h}(s_{h})=\mathbb{E}_{\pi}[\mathds{1}(s_{h})\mid s_{1}]\) and \(d^{\prime}_{h}(s_{h})=\mathbb{E}_{\pi^{\prime}}[\mathds{1}(s_{h})\mid s_{1}]\). We have

\[\left\lVert d_{h}-d^{\prime}_{h}\right\rVert_{1} =\lVert d_{h-1}\mathbb{P}_{h}^{\pi_{h}}-d^{\prime}_{h-1}\mathbb{P }_{h}^{\pi^{\prime}_{h}}\rVert_{1}\] \[\leq\left\lVert d_{h-1}\mathbb{P}_{h}^{\pi_{h}}-d^{\prime}_{h-1} \mathbb{P}_{h}^{\pi_{h}}\right\rVert_{1}+\left\lVert d^{\prime}_{h-1}\mathbb{ P}_{h}^{\pi_{h}}-d^{\prime}_{h-1}\mathbb{P}_{h}^{\pi^{\prime}_{h}}\right\rVert_{1}\] \[\leq\left\lVert d_{h-1}-d^{\prime}_{h-1}\right\rVert_{1}+\left\lVert \pi_{h}-\pi^{\prime}_{h}\right\rVert_{1,\infty}\] \[\leq\sum_{i\in[h]}\lVert\pi_{i}-\pi^{\prime}_{i}\rVert_{1,\infty},\]

where the second inequality holds by Lemmas 6 and 7 and the last inequality holds by a recursive calculation. Thus, we have

\[\sum_{k=1}^{K}\left(\mathbb{E}_{\pi^{c}_{k}}-\mathbb{E}_{\pi^{c}_{k-1}}\right) \left[\sum_{h=1}^{H}\mathds{1}(s_{h})\mid s_{1}=s_{k,1}\right]\leq\sum_{k=1} ^{K}\sum_{h=1}^{H}\sum_{i=1}^{h}\lVert\pi^{c}_{k,i}-\pi^{c}_{k-1,i}\rVert_{1, \infty}=HP_{T}.\]

This finishes the proof. 

### Proof of Theorem 1

Proof.: By Lemma 2, we have

\[\text{D-Regret}(K)\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(P _{T}\log\frac{4A}{\gamma}+KH\log\frac{1}{1-\gamma}\right)\] (19) \[+\frac{1}{\eta}\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi^{c}_{k }}\left[\sum_{a\in\mathcal{A}}\left(\pi^{c}_{k-1,h}(a\mid s_{h})\log\frac{1}{ \pi^{\prime}_{k,h}(a\mid s_{h})}-\pi^{c}_{k,h}(a\mid s_{h})\log\frac{1}{\pi^{ \prime}_{k+1,h}(a\mid s_{h})}\right)\right]\]Then, the last term can be bounded as follows:

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}\left[\sum_{a\in \mathcal{A}}\left(\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k,h}^{\prime}(a \mid s_{h})}-\pi_{k,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k+1,h}^{\prime}(a\mid s _{h})}\right)\right]\] \[\leq \sum_{k=1}^{K}\sum_{h=1}^{H}\left(\mathbb{E}_{\pi_{k-1}^{c}} \left[\sum_{a\in\mathcal{A}}\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k,h}^ {\prime}(a\mid s_{h})}\right]-\mathbb{E}_{\pi_{k}^{c}}\left[\sum_{a\in\mathcal{ A}}\pi_{k,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k+1,h}^{\prime}(a\mid s_{h})} \right]\right)\] \[+\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}-\pi_{k-1}^{c }}\left[\sum_{a\in\mathcal{A}}\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k, h}^{\prime}(a\mid s_{h})}\right]\] \[\leq \mathbb{E}_{\pi_{1}^{c}}\left[\sum_{a\in\mathcal{A}}\pi_{1,h}^{c }(a\mid s_{h})\log\frac{1}{\pi_{1,h}^{\prime}(a\mid s_{h})}\right]+\sum_{k=1}^ {K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}-\pi_{k-1}^{c}}\left[\sum_{a\in \mathcal{A}}\pi_{k-1,h}^{c}(a\mid s_{h})\log\frac{1}{\pi_{k,h}^{\prime}(a\mid s _{h})}\right]\] \[\leq H\log A+\log\frac{4A}{\gamma}\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{E}_{\pi_{k}^{c}-\pi_{k-1}^{c}}\left[\mathds{1}(s_{h})\mid s_{1}=s_{k, 1}\right]\] \[\leq H\log A+\log\frac{4A}{\gamma}HP_{T},\] (20)

where the second inequality holds by the telescoping sum and the third inequality holds because \(\pi_{1,h}^{\prime}=\pi_{1,h}\) is the uniform distribution and \(\pi_{k,h}^{\prime}(a\mid s)\geq\frac{\gamma}{4A}\) by the multiplicative stability lemma in Lemma 3 and the last inequality holds by Lemma 4.

Combining (19) and (20), we have

\[\text{D-Regret}(K)\leq\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(H\log A+(1+H) \log\frac{4A}{\gamma}P_{T}+KH\log\frac{1}{1-\gamma}\right),\]

which finishes the proof. 

### Proof of Theorem 2

To prove Theorem 2, we first introduce the following key lemma omitted in the main paper due to the space limit, which shows the guarantee of the confidence set.

**Lemma 8** (Zhou et al. [31, Lemma 5]).: _Let \(\widehat{\mathcal{C}}_{k,h}\) be defined in (10) and set \(\widehat{\beta}_{k}\) as_

\[\widehat{\beta}_{k}=8\sqrt{d\log(1+k/\lambda)\log\left(4k^{2}H/\delta\right)}+ 4\sqrt{d}\log\left(4k^{2}H/\delta\right)+\sqrt{\lambda}B.\]

_Then, with probability at least \(1-3\delta\), we have that simultaneously for all \(h\in[H]\) and \(k\in[K]\),_

\[\theta_{h}^{*}\in\left\{\theta\big{|}\big{\|}\widehat{\Sigma}_{k,h}^{1/2}( \theta-\widehat{\theta}_{k,h})\big{\|}_{2}\leq\widehat{\beta}_{k}\right\}, \big{|}\left[\bar{\psi}_{k,h}V_{k,h+1}\right](s_{k,h},a_{k,h})-\left[\forall_{h }V_{k,h+1}\right](s_{k,h},a_{k,h})\,\big{|}\leq E_{k,h},\]

_where \(E_{k,h}\) is defined as follows:_

\[\min\left\{H^{2},2H\hat{\beta}_{k}\left\|\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_ {k,h+1}}\left(s_{k,h},a_{k,h}\right)\right\|_{2}\right\}+\min\left\{H^{2}, \widetilde{\beta}_{k}\left\|\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}^{2}} \left(s_{k,h},a_{k,h}\right)\right\|_{2}\right\}\]

_with_

\[\hat{\beta}_{k} =8d\sqrt{\log(1+k/\lambda)\log\left(4k^{2}H/\delta\right)}+4\sqrt {d}\log\left(4k^{2}H/\delta\right)+\sqrt{\lambda}B,\] \[\widetilde{\beta}_{k} =8\sqrt{dH^{4}\log\left(1+kH^{4}/(d\lambda)\right)\log\left(4k^{2 }H/\delta\right)}+4H^{2}\log\left(4k^{2}H/\delta\right)+\sqrt{\lambda}B.\] (21)

We denote by \(\mathcal{E}\) the event when the result of Lemma 8 holds, we have \(\Pr(\mathcal{E})\geq 1-3\delta\).

Then we present the following lemma that shows the value function difference can be decomposed into two parts, one is a martingale sequence and the other is the estimation error.

**Lemma 9**.: _For all \(k\in[K],h\in[H]\), it holds that_

\[V_{k,h}(s_{k,h})-V_{k,h}^{\pi_{k}}(s_{k,h})=\sum_{h^{\prime}=h}^{H}\Big{(}M_{k,h ^{\prime},1}+M_{k,h^{\prime},2}-\iota_{k,h^{\prime}}(s_{k,h^{\prime}},a_{k,h^{ \prime}})\Big{)},\]_with_

\[M_{k,h,1} =\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})}[Q_{k,h}(s_{k,h},a)- Q_{k,h}^{\tau_{k}}(s_{k,h},a)]-(Q_{k,h}(s_{k,h},a_{k,h})-Q_{k,h}^{\tau_{k}}(s_{k,h},a_{ k,h})),\] \[M_{k,h,2} =[\mathbb{P}_{h}(V_{k,h+1}-V_{k,h+1}^{\tau_{k}})](s_{k,h},a_{k,h})- (V_{k,h+1}(s_{k,h+1})-V_{k,h+1}^{\tau_{k}}(s_{k,h+1})).\]

Proof.: By the definition \(V_{k,h}(s_{k,h})=\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})}[Q_{k,h}(s_{k,h },a)]\), we have

\[V_{k,h}(s_{k,h})-V_{k,h}^{\pi_{k}}(s_{k,h})\] \[=\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})}\big{[}Q_{k,h}(s _{k,h},a)-Q_{k,h}^{\tau_{k}}(s_{k,h},a)\big{]}\] \[=\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})}\big{[}Q_{k,h}(s _{k,h},a)-Q_{k,h}^{\tau_{k}}(s_{k,h},a)\big{]}-\big{(}Q_{k,h}(s_{k,h},a_{k,h}) -Q_{k,h}^{\tau_{k}}(s_{k,h},a_{k,h})\big{)}\] \[\quad+\big{(}Q_{k,h}(s_{k,h},a_{k,h})-Q_{k,h}^{\pi_{k}}(s_{k,h},a _{k,h})\big{)}\] \[=\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})}\big{[}Q_{k,h}(s _{k,h},a)-Q_{k,h}^{\tau_{k}}(s_{k,h},a)\big{]}-\big{(}Q_{k,h}(s_{k,h},a_{k,h}) -Q_{k,h}^{\pi_{k}}(s_{k,h},a_{k,h})\big{)}\] \[\quad+\big{[}\mathbb{P}_{h}(V_{k,h+1}-V_{k,h+1}^{\pi_{k}})](s_{k,h },a_{k,h})-\iota_{k,h}(s_{k,h},a_{k,h})\] \[=\underbrace{\mathbb{E}_{a\sim\tau_{k}(\,\cdot\mid\,s_{k,h})} \big{[}Q_{k,h}(s_{k,h},a)-Q_{k,h}^{\tau_{k}}(s_{k,h},a)\big{]}-\big{(}Q_{k,h}( s_{k,h},a_{k,h})-Q_{k,h}^{\pi_{k}}(s_{k,h},a_{k,h})\big{)}}_{M_{k,h,1}}\] \[\quad+\underbrace{[\mathbb{P}_{h}(V_{k,h+1}-V_{k,h+1}^{\pi_{k}})] (s_{k,h},a_{k,h})-\big{(}V_{k,h+1}(s_{k,h+1})-V_{k,h+1}^{\tau_{k}}(s_{k,h+1}) \big{)}}_{M_{k,h,2}}\] \[\quad+\big{(}V_{k,h+1}(s_{k,h+1})-V_{k,h+1}^{\pi_{k}}(s_{k,h+1}) \big{)}-\iota_{k,h}(s_{k,h},a_{k,h})\]

where the third equality holds by the fact \(Q_{k,h}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}-\iota_{k,h}\) and \(Q_{k,h}^{\tau_{k}}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}^{\tau_{k}}\). Summing up the above equation from \(h\) to \(H\) recursively finishes the proof. 

Note \(M_{k,h,1}\) is the noise from the stochastic policy and \(M_{k,h,2}\) is the noise from the state transition, Let \(M_{k,h}=M_{k,h,1}+M_{k,h,2},\forall k\in[K],h\in[H]\), we define two following high probability events:

\[\mathcal{E}_{1}=\bigg{\{}\forall h\in[H],\sum_{k=1}^{K}\sum_{h^{ \prime}=h}^{H}M_{k,h^{\prime}}\leq 4\sqrt{H^{3}K\log\frac{H}{\delta}}\bigg{\}}, \mathcal{E}_{2}=\bigg{\{}\sum_{k=1}^{K}\sum_{h=1}^{H}M_{k,h,2}\leq\sqrt{8H^{3} K\log\frac{1}{\delta}}\bigg{\}}.\]

According to the Azuma-Hoeffding inequality, we have \(\Pr(\mathcal{E}_{1})\geq 1-\delta\) and \(\Pr(\mathcal{E}_{2})\geq 1-\delta\).

Then, we show the model prediction error \(\iota_{k,h}\) can be upper and lower bounded both.

**Lemma 10**.: _Define prediction error \(\iota_{k,h}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}-Q_{k,h}\), on the event \(\mathcal{E}\), it holds that_

\[-2\widehat{\beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}( \cdot,\cdot)\big{\|}_{2}\leq\iota_{k,h}(\cdot,\cdot)\leq 0,\forall k\in[K],h\in[H].\]

Proof.: First, we prove the left-hand side inequality. By the definition of \(\iota_{k,h}\), we have

\[-\iota_{k,h}(s,a)\] \[=Q_{k,h}(s,a)-(r_{k,h}+\mathbb{P}_{h}V_{k,h+1})(s,a)\] \[\leq r_{k,h}(s,a)+\big{\langle}\widehat{\theta}_{k,h},\phi_{V_{k,h+1} }(s,a)\big{\rangle}+\widehat{\beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2} \phi_{V_{k,h+1}}(s,a)\big{\|}_{2}-(r_{k,h}+\mathbb{P}_{h}V_{k,h+1})(s,a)\] \[=\big{\langle}\widehat{\theta}_{k,h}-\theta_{k}^{*},\phi_{V_{k,h+1 }}(s,a)\big{\rangle}+\widehat{\beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2} \phi_{V_{k,h+1}}(s,a)\big{\|}_{2}\] \[\leq 2\widehat{\beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2} \phi_{V_{k,h+1}}(s,a)\big{\|}_{2},\]

where the first inequality holds by the configuration of \(Q_{k,h}\) in Algorithm 1, the second equality holds by the definition of linear mixture MDP such that \([\mathbb{P}_{h}V_{k,h+1}](s,a)=\langle\phi_{V_{k,h+1}}(s,a),\theta_{h}^{*}\rangle\) and the last inequality holds by the configuration of confidence set in Lemma 8.

Next, we prove the right-hand side inequality. By the definition of \(\iota_{k,h}\), we have

\[\iota_{k,h}(s,a)\] \[=(r_{k,h}+\mathbb{P}_{h}V_{k,h+1})(s,a)-Q_{k,h}(s,a)\] \[\leq(r_{k,h}+\mathbb{P}_{h}V_{k,h+1})(s,a)-\Big{[}r_{k,h}(s,a)+ \big{\langle}\widehat{\theta}_{k,h},\phi_{V_{k,h+1}}(s,a)\big{\rangle}+\widehat{ \beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}(s,a)\big{\|}_{2} \Big{]}_{[0,H-h+1]}\]\[=\max\Big{\{}\langle\widehat{\theta}_{k,h}-\theta_{h}^{*},\phi_{V_{k,h+1 }}(s,a)\rangle-\widehat{\beta}_{k}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{ k,h+1}}(s,a)\big{\|}_{2},(r_{k,h}+\mathbb{P}_{h}V_{k,h+1})(s,a)-(H-h+1)\Big{\}}\] \[\leq 0,\]

where the first inequality holds by the configuration of \(Q_{k,h}\) in Algorithm 1, the second equality holds by the definition of linear mixture MDP such that \([\mathbb{P}_{h}V_{k,h+1}](s,a)=\langle\phi_{V_{k,h+1}}(s,a),\theta_{h}^{*}\rangle\) and the last inequality holds by the configuration of confidence set in Lemma 8. 

Next, we show the prediction error can be bounded by the cumulative estimate variance.

**Lemma 11**.: _Define prediction error \(\iota_{k,h}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}-Q_{k,h}\), on the event \(\mathcal{E}\), it holds that_

\[-\sum_{k=1}^{K}\sum_{h=1}^{H}\iota_{k,h}(s_{k,h},a_{k,h})\leq 2\widehat{ \beta}_{K}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\bar{\sigma}_{k,h}^{2}}\sqrt{2Hd \log(1+K/\lambda)}.\]

Proof.: By Lemma 10 and the definition of \(\iota_{k,h}=r_{k,h}+\mathbb{P}_{h}V_{k,h+1}-Q_{k,h}\), we have

\[-\sum_{k=1}^{K}\sum_{h=1}^{H}\iota_{k,h}(s_{k,h},a_{k,h})\] \[\leq \sum_{k=1}^{K}\sum_{h=1}^{H}2\min\{\widehat{\beta}_{k}\big{\|} \widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}(s_{k,h},a_{k,h})\big{\|}_{2},H\}\] \[\leq \sum_{k=1}^{K}\sum_{h=1}^{H}2\widehat{\beta}_{k}\bar{\sigma}_{k, h}\min\big{\{}\big{\|}\widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}\left(s_{k,h},a_ {k,h}\right)/\bar{\sigma}_{k,h}\big{\|}_{2},1\big{\}}\] \[\leq 2\widehat{\beta}_{K}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\bar{ \sigma}_{k,h}^{2}}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\min\Big{\{}\Big{\|} \widehat{\Sigma}_{k,h}^{-1/2}\phi_{V_{k,h+1}}\left(s_{k,h},a_{k,h}\right)/\bar {\sigma}_{k,h}\Big{\|}_{2},1\Big{\}}}\] \[\leq 2\widehat{\beta}_{K}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\bar{ \sigma}_{k,h}^{2}}\sqrt{2Hd\log(1+K/\lambda)},\]

where the second inequality holds by \(2\widehat{\beta}_{k}\bar{\sigma}_{k,h}\geq\sqrt{d}H/\sqrt{d}=H\), the third inequality is by Cauchy-Schwarz inequality and the last inequality holds by the elliptical potential lemma in Lemma 16. 

Define the event

\[\mathcal{E}_{3}=\bigg{\{}\sum_{k=1}^{K}\sum_{h=1}^{H}[\mathbb{V}_{h}V_{h+1}^{ \pi_{k}^{k}}](s_{h}^{k},a_{h}^{k})\leq 3(HK+H^{3}\log(1/\delta))\bigg{\}},\]

by the law of total variance in Lemma 15, we have \(\Pr(\mathcal{E}_{3})\geq 1-\delta\). Then, we have the following lemma which bounds the estimated variance of the value function.

**Lemma 12** (He et al. [33, Lemma 6.5]).: _On the events \(\mathcal{E}\cap\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), it holds that_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\bar{\sigma}_{k,h}^{2} \leq 2H^{3}K/d+179H^{2}K+165d^{3}H^{4}\log^{2}\big{(}4K^{2}H/\delta \big{)}\log^{2}\big{(}1+KH^{4}/\lambda\big{)}\] \[\quad+2062d^{2}H^{5}\log^{2}\big{(}4K^{2}H/\delta\big{)}\log^{2}( 1+K/\lambda).\]

Now, we are ready to prove Theorem 2.

Proof of Theorem 2.: On the events \(\mathcal{E}\cap\mathcal{E}_{1}\cap\mathcal{E}_{2}\cap\mathcal{E}_{3}\), for any \(h\in[H]\), it holds that

\[\sum_{k=1}^{K}V_{k,h}(s_{k,h})-\sum_{k=1}^{K}V_{k,h}^{\pi_{k}}(s_{k,h})\]\[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}(M_{k,h,1}+M_{k,h,2}-\iota_{k,h}(s_{k,h },a_{k,h}))\] \[\leq 4\sqrt{H^{3}K\log(H/\delta)}+2\widehat{\beta}_{K}\sqrt{\sum_{k=1} ^{K}\sum_{h=1}^{H}\bar{\sigma}_{k,h}^{2}}\sqrt{2Hd\log(1+K/\lambda)}\] \[\leq\widetilde{\mathcal{O}}\big{(}\sqrt{dH^{4}K+d^{2}H^{3}K}\big{)},\]

where the first equality holds by Lemma 9, the second inequality holds by Lemma 11, and the last inequality holds by Lemma 12. This finishes the proof. 

### Proof of Theorem 3

Proof.: By Lemma 5, we can rewrite the dynamic regret as follows.

\[\mathrm{D-Regret}(K) = \sum_{k=1}^{K}V_{k,1}^{\pi_{k}^{c}}(s_{k,1})-\sum_{k=1}^{K}V_{k,1 }^{\pi_{k}}(s_{k,1})\] (22) \[= \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\langle Q_{k,h}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\rangle]\] \[+\sum_{k=1}^{K}\sum_{h=1}^{H}(\mathbb{E}_{\pi_{k}^{c}}[\iota_{k,h }(s_{h},a_{h})]-\iota_{k,h}(s_{k,h},a_{k,h}))+\mathcal{M}_{K,H}.\]

By Lemma 10, we have \(\iota_{k,h}(s,a)\leq 0\) for any \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\). Thus, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\iota_{k,h}(s_{h},a_{h})] \leq 0.\] (23)

By Theorem 2, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}(-\iota_{k,h}(s_{k,h},a_{k,h}))+\mathcal{M}_{K,H} \leq\widetilde{\mathcal{O}}\big{(}\sqrt{dH^{4}K+d^{2}H^{3}K}\big{)}.\] (24)

It remains to bound the first term. Note our algorithm is indeed updated based on the estimated action-value function \(Q_{k,h}\). By Theorem 1, set \(\gamma=1/KH\) and note that \(\log(1/(1-\gamma))\leq\gamma/(1-\gamma)\) for all \(\gamma>0\). Then, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\langle Q_{k,h}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\rangle]\] \[\leq \frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(H\log A+(1+H)\log\frac{ A}{\gamma}P_{T}+2\right)\]

It is clear that the optimal step size is \(\eta^{*}=\sqrt{(P_{T}+\log A)/(KH^{2})}\). Note our step size is set as \(\eta=\min\{\sqrt{(P_{T}+\log A)/K},1\}/H\) to ensure \(\eta\leq 1/H\). We consider the following two cases:

Case 1: \(\eta^{*}\leq 1/H\).In this case, our step size is set as \(\eta=\sqrt{(P_{T}+\log A)/(KH^{2})}\). We have

\[\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(H\log A+(1+H)\log\frac{A}{\gamma}P_ {T}+2\right)\leq\widetilde{\mathcal{O}}(\sqrt{KH^{4}(1+P_{T})}).\]

Case 2: \(\eta^{*}>1/H\).In this case, our step size is set as \(\eta=1/H\). Therefore, we have

\[\frac{\eta KH^{3}}{2}+\frac{1}{\eta}\left(H\log A+(1+H)\log\frac{A}{\gamma}P_ {T}+2\right)\leq\widetilde{\mathcal{O}}(H^{2}P_{T}),\]where the equality holds by \(P_{T}\geq K\) in this case. Combining these two cases, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\langle Q_{k,h}(s_{h},\cdot), \pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\rangle]\leq\widetilde{ \mathcal{O}}(\sqrt{H^{4}(K+P_{T})(1+P_{T})}).\] (25)

Combining (22), (23), (24) and (25), we obtain

\[\mathrm{D}\text{-}\mathrm{Regret}(K)\leq\widetilde{\mathcal{O}}\left(\sqrt{dH^ {4}K+d^{2}H^{3}K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})}\right).\]

This finishes the proof. 

### Proof of Theorem 4

Proof.: At a high level, we prove this lower bound by noting that optimizing the dynamic regret of linear mixture MDPs is harder than (i) optimizing the static regret of linear mixture MDPs with the unknown transition kernel, (ii) optimizing the dynamic regret of linear mixture MDPs with the known transition kernel, both. Thus, we can consider the lower bound of these two problems separately and combine them to obtain the lower bound of the dynamic regret of linear mixture MDPs with the unknown transition kernel. We present the details below.

First, we consider the lower bound of optimizing the static regret of adversarial linear mixture MDPs with the unknown transition kernel. From lower bound in He et al. [33, Theorem 5.3], we have the following lower bound in this case since the dynamic regret is no smaller than the static regret.

\[\mathrm{D}\text{-}\mathrm{Regret}(K)\geq\Omega(\sqrt{d^{2}H^{3}K}).\] (26)

Then, we consider the lower bound of optimizing the dynamic regret of adversarial linear mixture MDPs with the known transition kernel. We note that Zimin and Neu [12] show the lower bound of the static regret for adversarial episodic loop-free SSP with known transition kernel is \(\Omega(H\sqrt{K\log{(SA)}})\), we utilize this lower bound to establish our lower bound as the episodic loop-free SSP is a special case of linear mixture MDPs with \(d=S^{2}A\). We consider two cases:

Case 1: \(\Gamma\leq 2H\).In this case, we can directly utilize the established lower bound of static regret as a natural lower bound of dynamic regret,

\[\mathrm{D}\text{-}\mathrm{Regret}(K)\geq\Omega(H\sqrt{K\log{(SA)}}).\] (27)

Case 2: \(\Gamma>2H\).Without loss of generality, we assume \(L=\lceil\Gamma/2H\rceil\) divides \(K\) and split the whole episodes into \(L\) pieces equally. Next, we construct a special policy sequence such that the policy sequence is fixed within each piece and only changes in the split point. Since the sequence changes at most \(L-1\leq\Gamma/2H\) times and the path length of the policy sequence at each change point is at most \(2H\), the total path length in \(K\) episodes does not exceed \(\Gamma\). As a result, we have

\[\mathrm{D}\text{-}\mathrm{Regret}(K)\geq LH\sqrt{K/L\log{(SA)}}=H\sqrt{KL\log{ (SA)}}\geq\Omega(\sqrt{KH\Gamma\log{(SA)}}).\] (28)

Combining (27) and (28), we have the following lower bound for the dynamic regret of adversarial linear mixture MDPs with the known transition kernel,

\[\mathrm{D}\text{-}\mathrm{Regret}(K)\geq\Omega\big{(}\max\{H\sqrt{K\log{(SA) }},\sqrt{KH\Gamma\log{(SA)}}\}\big{)}\geq\Omega(\sqrt{KH(H+\Gamma)\log{(SA)}}),\] (29)

where the last inequality holds by \(\max\{a,b\}\geq(a+b)/2\).

Combining two lower bounds (26) and (29), we have the lower bound of the dynamic regret of adversarial linear mixture MDPs with the unknown transition kernel,

\[\mathrm{D}\text{-}\mathrm{Regret}(K) \geq\Omega\big{(}\max\{\sqrt{d^{2}H^{3}K},\sqrt{KH(H+\Gamma)\log {(SA)}}\}\big{)}\] \[\geq\Omega\big{(}\sqrt{d^{2}H^{3}K}+\sqrt{KH(H+\Gamma)\log{(SA)}} \big{)}.\]

This finishes the proof.

### Proof of Theorem 5

Proof.: By Lemma 5, we can rewrite the dynamic regret as follows.

\[\text{D-Regret}(K) = \sum_{k=1}^{K}V_{k,1}^{\pi_{k}^{c}}(s_{k,1})-\sum_{k=1}^{K}V_{k,1}^{ \pi_{k}}(s_{k,1})\] \[= \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\langle Q_{k, h}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h})\rangle]\] \[+\sum_{k=1}^{K}\sum_{h=1}^{H}(\mathbb{E}_{\pi_{k}^{c}}[\iota_{k, h}(s_{h},a_{h})]-\iota_{k,h}(s_{k,h},a_{k,h}))+\mathcal{M}_{K,H}.\]

By Lemma 10, we have \(\iota_{k,h}(s,a)\leq 0\) for any \(k\in[K],h\in[H],s\in\mathcal{S},a\in\mathcal{A}\). Thus, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}[\iota_{k,h}(s_{h},a_{h})] \leq 0.\]

By Theorem 2, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}(-\iota_{k,h}(s_{k,h},a_{k,h}))+\mathcal{M}_{K,H} \leq\widetilde{\mathcal{O}}\big{(}\sqrt{dH^{4}K+d^{2}H^{3}K}\big{)}.\]

It remains to bound the first term. We decompose this term as follows.

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}}\left[\langle Q _{k,h}^{\pi_{k}}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})-\pi_{k,h}(\cdot \mid s_{h})\rangle\right]\] \[= \underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}} \left[\langle Q_{k,h}^{\pi_{k}}(s_{h},\cdot),\pi_{k,h}^{c}(\cdot\mid s_{h})- \pi_{k,h}^{i}(\cdot\mid s_{h})\rangle\right]}_{\text{base-regret}}\] \[+\underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{c}} \left[\langle Q_{k,h}^{\pi_{k}}(s_{h},\cdot),\pi_{k,h}^{i}(\cdot\mid s_{h})- \pi_{k,h}(\cdot\mid s_{h})\rangle\right]}_{\text{meta-regret}},\] (30)

where the decomposition holds for any base-learner \(i\in N\). Next, we bound the two terms separately.

Upper bound of base regret.From Theorem 1, we have

\[\texttt{base-regret}\leq\frac{\eta_{i}KH^{3}}{2}+\frac{1}{\eta_{i}}\left(H \log A+(1+H)\log\frac{A}{\gamma}P_{T}+KH\log\frac{1}{1-\gamma}\right)\]

Set \(\gamma=1/KH\) and note that \(\log(1/(1-\gamma))\leq\gamma/(1-\gamma)\) for all \(\gamma>0\). Then, we have

\[\texttt{base-regret} \leq\frac{\eta_{i}KH^{3}}{2}+\frac{1}{\eta_{i}}\left(H\log A+(1+H )P_{T}\log(KHA)+2\right)\] \[\leq\frac{\eta_{i}KH^{3}}{2}+\frac{2H}{\eta_{i}}\left(\log A+P_{T} \log(KHA)\right).\]

It is clear that the optimal learning rate \(\eta^{*}=\sqrt{4(\log A+P_{T}\log(KHA))/(KH^{2})}\). By the definition \(P_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\lVert\pi_{k,h}^{c}-\pi_{k-1,h}^{c}\rVert_{1,\infty}\), it holds that \(0\leq P_{T}\leq 2KH\). Therefore, the range of the optimal learning rate is

\[\eta_{\min}=\sqrt{\frac{4\log A}{KH^{2}}},\text{ and }\eta_{\max}=\sqrt{\frac{4 \log A+8KH\log(KHA)}{KH^{2}}}.\]

From the construction of the step size pool \(\mathcal{H}=\{\eta_{i}=(2^{i}/H)\sqrt{(\log A)/K}\mid i\in[N]\}\) with \(N=\lfloor\frac{1}{2}\log(\frac{K}{\log A})\rfloor\), we know that the step size therein is monotonically increasing, in particular

\[\eta_{1}=\sqrt{\frac{4\log A}{KH^{2}}},\text{ and }\eta_{N}=\frac{1}{H}.\]

In the following, we consider two cases:Case 1: \(\eta^{*}\in[\eta_{1},\eta_{N}]\).In this case, we ensure there exists \(i^{*}\in N\) such that \(\eta_{i^{*}}\leq\eta^{*}\leq 2\eta_{i^{*}}\). Note the decomposition in (30) holds for any base-learner. Therefore, we choose the base-learner whose step size is \(\eta_{i^{*}}\) and have

\[\texttt{base-regret} \leq\frac{\eta_{i^{*}}KH^{3}}{2}+\frac{2H}{\eta_{i^{*}}}\left( \log A+P_{T}\log(KHA)\right)\] \[\leq\frac{\eta^{*}KH^{3}}{2}+\frac{4H}{\eta^{*}}\left(\log A+P_{T }\log(KHA)\right)\] \[=3\sqrt{KH^{4}\left(\log A+P_{T}\log(KHA)\right)},\]

where the second inequality holds by the condition that \(\eta_{i^{*}}\leq\eta^{*}\leq 2\eta_{i^{*}}\) and the last equality holds by substituting \(\eta^{*}=\sqrt{4(\log A+P_{T}\log(KHA))/(KH^{2})}\).

Case 2: \(\eta^{*}>\eta_{N}\).In this case, we know that \(4(\log A+P_{T}\log(KHA))>K\). Therefore, we choose the base-learner whose step size is \(\eta_{N}\) and have

\[\texttt{base-regret} \leq\frac{\eta_{N}KH^{3}}{2}+\frac{2H}{\eta_{N}}\left(\log A+P_{ T}\log(KHA)\right)\] \[=\frac{KH^{2}}{2}+2H^{2}\left(\log A+P_{T}\log(KHA)\right)\] \[\leq 4H^{2}\left(\log A+P_{T}\log(KHA)\right),\]

where the last inequality holds by the condition that \(4(\log A+P_{T}\log(KHA))>K\).

Summing over the two upper bounds yields

\[\texttt{base-regret} \leq 3\sqrt{KH^{4}\left(\log A+P_{T}\log(KHA)\right)}+4H^{2} \left(\log A+P_{T}\log(KHA)\right)\] \[\leq 4\sqrt{H^{4}(K+\log A+P_{T}\log(KHA))(\log A+P_{T}\log(KHA))}\] \[=\widetilde{\mathcal{O}}(\sqrt{H^{4}(K+P_{T})(1+P_{T})}),\] (31)

where the inequality holds by \(\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)},\forall a,b\geq 0\).

Upper bound of meta regret.For meta-regret, we have

\[\texttt{meta-regret}= \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[\langle Q _{k,h}(s_{h},\cdot),\pi_{k,h}^{i}(\cdot\mid s_{h})-\pi_{k,h}(\cdot\mid s_{h}) \rangle\right]\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[\langle e ^{i}(s_{h})-p_{k,h}^{i}(s_{h}),Q_{k,h}(s_{h},\cdot)\cdot\pi_{k,h}^{i}(\cdot \mid s_{h})\rangle\right]\] \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[ \langle e^{i_{k,h}^{*}}(s_{h})-p_{k,h}^{i}(s_{h}),Q_{k,h}(s_{h},\cdot)\cdot \pi_{k,h}^{i}(\cdot\mid s_{h})\rangle\right]\] \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[ \langle e^{i_{k,h}^{*}}(s_{h})-e^{i_{k-1,h}^{*}}(s_{h}),Q_{k,h}(s_{h},\cdot) \cdot\pi_{k,h}^{i}(\cdot\mid s_{h})\rangle\right]\] \[\leq H\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\left[ \|e^{i_{k,h}^{*}}(s_{h})-e^{i_{k-1,h}^{*}}(s_{h})\|_{1}\right]\] \[\leq 2HS_{T},\] (32)

where the first inequality holds by the definition that \(i_{k,h}^{*}=\arg\max_{i\in[N]}\langle Q_{k,h}(s_{h},\cdot),\pi_{k,h}^{i}(\cdot \mid s_{h})\rangle\), the second inequality holds due to \(p_{k,h}^{i}(s_{h})=e^{i_{k-1,h}^{*}}(s_{h})\), and the last equality holds by the definition \(S_{T}=\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}_{\pi_{k}^{*}}\mathds{1}[i_{k,h}^ {*}(s_{h})\neq i_{k-1,h}^{*}(s_{h})]\).

Combining (31) and (32), by \(\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)},\forall a,b\geq 0\), we have

\[\mathrm{D-Regret}(K)\leq\widetilde{\mathcal{O}}\Big{(}\sqrt{dH^{4}K+d^{2}H^{3} K}+\sqrt{H^{4}(K+P_{T})(1+P_{T})+H^{2}S_{T}^{2}}\Big{)}.\]

This finishes the proof.

Supporting Lemmas

In this section, we introduce the supporting lemmas used in the proofs.

First, we introduce the performance difference lemma which connects the difference between two policies to the difference between their expected total rewards through the Q-function.

**Lemma 13** (Cai et al. (2012, Lemma 3.2)).: _For any policies \(\pi,\pi^{\prime}\in\Delta(\mathcal{A}\,|\,\mathcal{S},H)\), it holds that_

\[V_{k,1}^{\pi^{\prime}}(s_{k,1})-V_{k,1}^{\pi}(s_{k,1})=\mathbb{E}_{\pi^{ \prime}}\left[\sum_{h=1}^{H}\langle Q_{k,h}^{\pi}(s_{h},\cdot),\pi_{h}^{\prime }(\cdot\,|\,s_{h})-\pi_{h}(\cdot\,|\,s_{h})\rangle\,\Big{|}\,s_{1}=s_{k,1} \right].\]

Then, we introduce the following lemmas which show the "one-step descent" guarantee.

**Lemma 14** (Cai et al. (2012, Lemma 3.3)).: _For any distributions \(p^{*},p\in\Delta(\mathcal{A})\), state \(s\in\mathcal{S}\), and function \(Q:\mathcal{S}\times\mathcal{A}\to[0,H]\), it holds for \(p^{\prime}\in\Delta(\mathcal{A})\) with \(p^{\prime}(\cdot)\propto p(\cdot)\cdot\exp(\eta\cdot Q(s,\cdot))\) that_

\[\langle Q(s,\cdot),p^{*}(\cdot)-p(\cdot)\rangle\leq\eta H^{2}/2+\eta^{-1}\cdot \left(D_{\mathrm{KL}}\left(p^{*}(\cdot)\|p(\cdot)\right)-D_{\mathrm{KL}}\left( p^{*}(\cdot)\|p^{\prime}(\cdot)\right)\right).\]

Next, we introduce the law of total variance, which bounds the variance of the value function.

**Lemma 15** (Jin et al. (2015, Lemma C.5)).: _With probability at least \(1-\delta\), it holds that_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\left[\mathbb{V}_{h}V_{h+1}^{\pi^{k}}\right]\left( s_{h}^{k},a_{h}^{k}\right)\leq 3\left(HK+H^{3}\log\frac{1}{\delta}\right).\]

Finally, we introduce the elliptical potential lemma, which is a key lemma in online linear regression.

**Lemma 16** (Abbasi-Yadkori et al. (2015, Lemma 11)).: _Let \(\left\{\mathbf{x}_{t}\right\}_{t=1}^{\infty}\) be a sequence in \(\mathbb{R}^{d}\) space, \(\mathbf{V}_{0}=\lambda\mathbf{I}\) and define \(\mathbf{V}_{t}=\mathbf{V}_{0}+\sum_{i=1}^{t}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\). If \(\left\|\mathbf{x}_{i}\right\|_{2}\leq L,\forall i\in\mathbb{Z}_{+}\), then for each \(t\in\mathbb{Z}_{+}\),_

\[\sum_{i=1}^{t}\min\left\{1,\left\|\mathbf{x}_{i}\right\|_{\mathbf{V}_{i-1}^{- 1}}\right\}\leq 2d\log\left(\frac{d\lambda+tL^{2}}{d\lambda}\right).\]