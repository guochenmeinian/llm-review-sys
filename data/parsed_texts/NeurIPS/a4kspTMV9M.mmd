# A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Kernel-based optimal transport (OT) estimation is an alternative to the standard plug-in OT estimation. Recent works suggested that kernel-based OT estimators are more statistically efficient than plug-in OT estimators when comparing probability measures in high-dimensions [59]. However, the computation of these estimators relies on the short-step interior-point method for which the required number of iterations is known to be _large_ in practice. In this paper, we propose a nonsmooth equation model for kernel-based OT estimation and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method. Indeed, by exploring the special problem structure, the per-iteration cost of performing one SSN step can be significantly reduced in practice. We also prove that our algorithm can achieve a global convergence rate of \(O(1/\sqrt{k})\) and a local quadratic convergence rate under some standard regularity conditions. Finally, we demonstrate the effectiveness of our algorithm by conducing the experiments on both synthetic and real datasets.

## 1 Introduction

Optimal transport (OT) theory [60] has provided a principled framework for comparing probability distributions. It has been extensively adopted in machine learning and related fields, with examples including generative modeling [2; 21; 51; 57], classification and clustering [20; 55; 25], and domain adaptation [9; 10; 49], see also the monograph [43]. It has also had an impact in applied areas such as neuroimaging [27] and cell trajectory prediction [53; 66].

**Curse of Dimensionality.** In many real application problems, the OT cost is computed for squared Euclidean distance on the sampled distributions with \(n\) observations (leading to the 2-Wasserstein distance). It is known that OT estimation suffers from the curse of dimensionality [16; 19; 62]: the standard plug-in estimator, which consists in computing the OT distance between the sampled distributions with \(n\) observations, converges to the OT distance between true distributions at a rate of \(O(n^{-1/d})\), which degrades exponentially in the dimension \(d\). This rate can be improved to \(O(n^{-1/2d})\) when true distributions are different [7] but it is still problematic in a high-dimensional regime. This issue can be a barrier to its adoption in machine learning since various application problems arising from image processing and bioengineering are high-dimensional. Practitioners have long been aware of such limitations and proposed efficient computational schemes that not only improve computational complexity but also carry out statistical regularization.

**Regularization.** In this context, two threads have been investigated to regularize the OT distance: entropic regularization [11; 12; 22; 36] or low-dimensional projection [48; 4; 41; 29; 39; 31; 32; 40].

For the former approach, the sample complexity of entropic OT is bounded by \(O(\eta^{-d/2}n^{-1/2})\) for a regularization parameter \(\eta>0\). For the latter approach, the sample complexity of projection OT is bounded by \(O(n^{-1/k})\) for an integer-valued projection dimension \(k\leq d\). Even though these boundsattain the dimension-free dependence on \(n\), they deteriorate when \(\eta\) is small or \(k\) is large, either of which is needed to study the sample complexity of OT [7], and which plays a role in real applications.

**Leveraging Smoothness.** A recent line of works have focused on the _wavelet-based OT estimators_ under a strong smoothness condition [63; 26; 15; 34]. Although these estimators are minimax optimal from a statistical viewpoint, they are algorithmically intractable [59]. In contrast, a specific entropic regularized OT estimator is computationally tractable but still suffers from the curse of dimensionality when the dimension is sufficiently large [44]. Recently, Vacher et al. [59] has closed this statistical-computational gap by designing a kernel-based estimator relying on kernel sums-of-squares (SoS) and showed that it can be computed by a short-step interior-point method with polynomial-time complexity guarantee. However, the short-step interior-point method is well known to be ineffective for large number of iterations required as the sample size increases, diminishing their value from both statistical and practical viewpoints1. In this context, Muzzlec et al. [38] proposed to use the relaxation model and solve it using gradient-based methods. However, the relaxation model may not be a good approximation for kernel-based OT estimator, thereby lacking any statistical guarantee.

Footnote 1: The short-step interior-point method proposed by Vacher et al. [59] is in fact a Newton barrier method and does not exploit the special structure of kernel-based OT estimation. The required number of iterations is large as shown by our experiments in the subsequent of this paper.

**Goal:** While there is an ongoing debate in the OT literature on the merits of computing the plug-in OT estimators v.s. kernel-based OT estimators, we adopt the perspective that Vacher et al. [59] does introduce a fairly novel approach and we believe that it is worth studying if the kernel-based OT estimation can provide leads for practical use. The goal of this paper is therefore to facilitate the computational aspect by designing new algorithms, and to figure out whether that estimator's theoretical claims is also supported by practical relevance. The statistical analysis of kernel-based OT estimation itself, e.g., the proper choice of penalty parameters, is beyond the scope of this paper.

**Contribution:** In this paper, we propose a nonsmooth equation model for computing kernel-based OT estimators and show that it has a special problem structure, allowing it to be solved in an efficient manner using semismooth Newton method [37; 47; 46; 58].

We first propose a nonsmooth equation model for computing the kernel-based OT estimator and define an approximate OT value, which allows us to carry out a finite-time analysis of the algorithm. Then, we propose a specialized semismooth Newton method for computing the kernel-based OT estimator and prove a global convergence rate of \(O(1/\sqrt{k})\) (Theorem 3.3) and a local quadratic convergence rate under standard regularity conditions (Theorem 3.4). Notably, we significantly reduce the per-iteration computational cost by exploiting the special problem structure. Finally, we conduct the experiments to evaluate our algorithm on both synthetic and real datasets. Experimental results demonstrate its efficiency for solving the kernel-based OT estimation.

**Organization.** The remainder of the paper is organized as follows. In Section 2, we present the nonsmooth equation model for computing the kernel-based OT estimators and define the optimality notion based on the residual map. In Section 3, we propose and analyze the specialized semismooth Newton (SSN) algorithm for computing the kernel-based OT estimators and prove that our algorithm achieves the convergence rate guarantee in both global and local sense. In Section 4, we conduct the experiments on both synthetic and real datasets, demonstrating that our algorithm can effectively compute the kernel-based OT estimators and is more efficient than short-step interior-point methods. In Section 5, we conclude this paper. In the supplementary material, we provide further background materials on SSN methods, additional experimental results, and missing proofs for key results.

## 2 Preliminaries and Technical Background

In this section, we present the basic setup for the kernel-based optimal transport (OT) estimation and propose a nonsmooth equation model for its computation.

### Kernel-based OT estimation

We formally define the OT distance and review the kernel-based OT estimation [59]. Indeed, the OT distance with strong smooth distributions can be estimated at a dimension-free statistical rate with high probability by solving a suitably defined optimization model.

Let \(X\) and \(Y\) be two bounded domains in \(\mathbb{R}^{d}\) and let \(\mathscr{P}(X)\) and \(\mathscr{P}(Y)\) be the set of Borel probability measures in \(X\) and \(Y\). Suppose that \(\mu\in\mathscr{P}(X)\), \(\nu\in\mathscr{P}(Y)\) and \(\Pi(\mu,\nu)\) is the set of couplings between \(\mu\) and \(\nu\), the OT distance [60] is given by

\[\text{OT}(\mu,\nu):=\tfrac{1}{2}\left(\inf_{\pi\in\Pi(\mu,\nu)}\int_{X\times Y }\|x-y\|^{2}\;d\pi(x,y)\right).\]

Its dual formulation is stated as follows,

\[\sup_{u,v\in C(\mathbb{R}^{d})}\int_{X}u(x)d\mu(x)+\int_{Y}v(y)d\nu(y),\quad \text{s.t.}\;\tfrac{1}{2}\|x-y\|^{2}\geq u(x)+v(y),\forall(x,y)\in X\times Y,\]

where \(C(\mathbb{R}^{d})\) is the space of continuous functions on \(\mathbb{R}^{d}\). Note that the supremum can be attained and the corresponding optimal dual functions \(u_{\star}\) and \(v_{\star}\) are referred to as the Kantorovich potentials [52]. This problem is delicate to solve since \(\tfrac{1}{2}\|x-y\|^{2}\geq u(x)+v(y)\) needs to be satisfied on a continuous set \(X\times Y\). A natural approach is to take \(n\) points \(\{(\tilde{x}_{1},\tilde{y}_{1}),\ldots,(\tilde{x}_{n},\tilde{y}_{n})\}\subseteq X \times Y\) and consider the constraints \(\tfrac{1}{2}\|\tilde{x}_{i}-\tilde{y}_{i}\|^{2}\geq u(\tilde{x}_{i})+v(\tilde{ y}_{i})\) for all \(1\leq i\leq n\). However, it can not leverage the smoothness of potentials [3], yielding an error of \(\Omega(n^{-1/d})\). Vacher et al. [59] has overcome this difficulty by replacing the inequality constraints with equality constraints that are equivalent and considering the equality constraints over \(n\) points. Following their works, we impose the following assumption on the support sets \(X,Y\) and the densities of \(\mu\) and \(\nu\).

**Assumption 2.1**: _Let \(d\geq 1\) be the dimension and let \(m>2d+2\) be the order of smoothness. Then, we assume that (i) the support sets \(X,Y\) are convex, bounded, and open with Lipschitz boundaries; (ii) the densities of \(\mu,\nu\) are finite, bounded away from zero and \(m\)-times differentiable._

Assumption 2.1 guarantees that the potentials \(u_{\star}\) and \(v_{\star}\) have a similar order of differentiability [14], leading to an effective way to represent \(u\) and \(v\) via a _reproducing Kernel Hilbert space_ (RKHS) [42]. In particular, we define \(H^{s}(Z):=\{f\in L^{2}(Z)\mid\|f\|_{H^{s}(Z)}:=\sum_{|o|\leq s}\|D^{\alpha}f\| _{L^{2}(Z)}<+\infty\}\) and remark that \(H^{s}(Z)\subseteq C^{k}(Z)\) for any \(s>\frac{d}{2}+k\), where \(k\geq 0\) is integer-valued. This implies that \(H^{m+1}(X)\), \(H^{m+1}(Y)\) and \(H^{m}(X\times Y)\) are RKHS under Assumption 2.1 and they are associated with three bounded continuous feature maps \(\phi_{X}:X\mapsto H^{m+1}(X)\), \(\phi_{Y}:Y\mapsto H^{m+1}(Y)\) and \(\phi_{XY}:X\times Y\mapsto H^{m}(X\times Y)\). For simplicity, we let \(H_{X}=H^{m+1}(X)\), \(H_{Y}=H^{m+1}(Y)\) and \(H_{XY}=H^{m}(X\times Y)\). Vacher et al. [59, Corollary 7] shows that (i) \(u_{\star}\in H_{X}\) and \(v_{\star}\in H_{Y}\) with

\[\int_{X}u(x)d\mu(x)=\langle u,w_{\mu}\rangle_{H_{X}},\;\int_{X}v(y)d\nu(y)= \langle v,w_{\nu}\rangle_{H_{Y}},\]

where \(w_{\mu}=\int_{X}\phi_{X}(x)d\mu(x)\) and \(w_{\nu}=\int_{Y}\phi_{Y}(y)d\nu(y)\) are _kernel mean embeddings_; (ii) \(A_{\star}\in\mathbb{S}^{+}(H_{XY})^{2}\) exists and satisfies the equality constraint as follows:

\[\tfrac{1}{2}\|x-y\|^{2}-u_{\star}(x)-v_{\star}(y)=\langle\phi_{XY}(x,y),A_{ \star}\phi_{XY}(x,y)\rangle_{H_{XY}}.\]

Putting these pieces yields a representation theorem for estimating the OT distance. Indeed, under Assumption 2.1, the dual OT problem is equivalent to the RKHS-based problem given by

\[\begin{array}{rl}\max_{u,v,A}&\langle u,w_{\mu}\rangle_{H_{X}}+\langle v,w_ {\nu}\rangle_{H_{Y}},\\ &\text{s.t.}&\tfrac{1}{2}\|x-y\|^{2}-u(x)-v(y)=\langle\phi_{XY}(x,y),A\phi_{ XY}(x,y)\rangle_{H_{XY}}.\end{array}\] (2.1)

The above equation offers two advantages: (i) The equality constraint can be well approximated under Assumption 2.1; (ii) RKHSs allow the kernel trick: computing parameters are expressed in terms of _kernel functions_ that correspond to

\[k_{X}(x,x^{\prime})=\langle\phi_{X}(x),\phi_{X}(x^{\prime})\rangle_{H_{X}}, \quad k_{Y}(y,y^{\prime})=\langle\phi_{Y}(y),\phi_{Y}(y^{\prime})\rangle_{H_{ Y}},\]

and

\[k_{XY}((x,y),(x^{\prime},y^{\prime}))=\langle\phi_{XY}(x,y),\phi_{XY}(x^{ \prime},y^{\prime})\rangle_{H_{XY}},\]

where the kernel functions are explicit and can be computed in \(O(d)\) given the samples. The final step is to approximate Eq. (2.1) using the data \(x_{1},\ldots,x_{n_{\text{tuple}}}\sim\mu\) and \(y_{1},\ldots,y_{n_{\text{tuple}}}\sim\nu\), and the filling points \(\{(\tilde{x}_{1},\tilde{y}_{1}),\ldots,(\tilde{x}_{n},\tilde{y}_{n})\}\subseteq X \times Y\). Indeed, we define \(\hat{\mu}=\frac{1}{n_{\text{tuple}}}\sum_{i=1}^{n_{\text{tuple}}}\delta_{x_{i}}\) and \(\hat{\nu}=\frac{1}{n_{\text{sample}}}\sum_{i=1}^{n_{\text{sample}}}\phi_{y_{i}}\), and use \(\langle u,w_{\hat{\mu}}\rangle_{H_{X}}+\langle v,w_{\hat{\nu}}\rangle_{H_{Y}}\) instead of \(\langle u,w_{\hat{\mu}}\rangle_{H_{X}}+\langle v,w_{\nu}\rangle_{H_{Y}}\) where \(w_{\hat{\mu}}=\frac{1}{n_{\text{sample}}}\sum_{i=1}^{n_{\text{sample}}}\phi_{X} (x_{i})\) and \(w_{\hat{\nu}}=\frac{1}{n_{\text{sample}}}\sum_{i=1}^{n_{\text{sample}}}\phi_{Y} (y_{i})\). We also impose _the penalization terms_ for \(u\), \(v\), and \(A\) to alleviate the error induced by sampling the corresponding equality constraints. Then, the resulting problem with regularization parameters \(\lambda_{1},\lambda_{2}>0\) is summarized as follows:

\[\begin{array}{rl}\max_{u,v,A}&\langle u,w_{\hat{\mu}}\rangle_{H_{X}}+\langle v,w_{\hat{\nu}}\rangle_{H_{Y}}-\lambda_{1}\text{Tr}(A)-\lambda_{2}(\|u\|_{H_{X }}^{2}+\|v\|_{H_{Y}}^{2}),\\ \text{s.t.}&\frac{1}{2}\|\tilde{x}_{i}-\tilde{y}_{i}\|^{2}-u(\tilde{x}_{i})-v (\tilde{y}_{i})=\langle\phi_{XY}(\tilde{x}_{i},\tilde{y}_{i}),A\phi_{XY}( \tilde{x}_{i},\tilde{y}_{i})\rangle_{H_{XY}}\,.\end{array}\] (2.2)

Focusing on the case \(n_{\text{sample}}=\Theta(n)\), we let \(\hat{u}_{\star}\) and \(\hat{v}_{\star}\) be the unique maximizers of Eq. (2.2). Then, the estimator for \(\text{OT}(\mu,\nu)\) we consider corresponds to

\[\widehat{\text{OT}}^{n}=\langle\hat{u}_{\star},w_{\hat{\mu}}\rangle_{H_{X}}+ \langle\hat{v}_{\star},w_{\hat{\nu}}\rangle_{H_{Y}}.\] (2.3)

**Remark 2.2**: _It follows from Vacher et al. [59, Corollary 3] that the norm of empirical potentials can be controlled using \(\lambda_{1}=\tilde{\Theta}(n^{-1/2})\) and \(\lambda_{2}=\tilde{\Theta}(n^{-1/2})\) in high probability sense, leading to the sample complexity bound: \(|\widehat{\text{OT}}^{n}-\text{OT}(\mu,\nu)|=\tilde{O}(n^{-1/2})\). In comparison with plug-in estimators, the kernel-based OT estimators are better when the sample size is small and the dimension is high._

Note that Eq. (2.2) is an infinite-dimensional optimization problem and is thus difficult to be solved. Thanks to Vacher et al. [59, Theorem 15], we have that the dual problem of Eq. (2.2) can be presented in a finite-dimensional space and the strong duality holds true. Indeed, we define \(Q\in\mathbb{R}^{n\times n}\) with \(Q_{ij}=k_{X}(\tilde{x}_{i},\tilde{x}_{j})+k_{Y}(\tilde{y}_{i},\tilde{y}_{j})\), and \(z\in\mathbb{R}^{n}\) with \(z_{i}=w_{\hat{\mu}}(\tilde{x}_{i})+w_{\hat{\nu}}(\tilde{y}_{i})-\lambda_{2}\| \tilde{x}_{i}-\tilde{y}_{i}\|^{2}\), and \(q^{2}=\|w_{\hat{\mu}}\|_{H_{X}}^{2}+\|w_{\hat{\nu}}\|_{H_{Y}}\), where we have

\[w_{\hat{\mu}}(\tilde{x}_{i})=\frac{1}{n_{\text{sample}}}\sum_{j=1}^{n_{\text{ sample}}}k_{X}(x_{j},\tilde{x}_{i}),\quad w_{\hat{\nu}}(\tilde{y}_{i})=\frac{1}{n_{ \text{sample}}}\sum_{j=1}^{n_{\text{sample}}}k_{Y}(y_{j},\tilde{y}_{i}),\]

and

\[\|w_{\hat{\mu}}\|_{H_{X}}^{2}=\frac{1}{n_{\text{sample}}^{2}}\sum_{1\leq i,j \leq n_{\text{sample}}}k_{X}(x_{i},x_{j}),\quad\|w_{\hat{\nu}}\|_{H_{Y}}^{2}= \frac{1}{n_{\text{sample}}^{2}}\sum_{1\leq i,j\leq n_{\text{sample}}}k_{Y}(y_{i },y_{j}).\]

We define \(K\in\mathbb{R}^{n\times n}\) with \(K_{ij}=k_{XY}((\tilde{x}_{i},\tilde{y}_{i}),(\tilde{x}_{j},\tilde{y}_{j}))\) and \(R\) as an upper triangular matrix for the Cholesky decomposition of \(K\). We let \(\Phi_{i}\) be the \(i^{\text{th}}\) column of \(R\). Then, the dual problem of Eq. (2.2) reads:

\[\min_{\gamma\in\mathbb{R}^{n}}\tfrac{1}{4\lambda_{2}}\gamma^{\top}Q\gamma- \tfrac{1}{2\lambda_{2}}\gamma^{\top}z+\tfrac{q^{2}}{4\lambda_{2}},\quad\text{ s.t. }\sum_{i=1}^{n}\gamma_{i}\Phi_{i}\Phi_{i}^{\top}+\lambda_{1}I\succeq 0.\] (2.4)

Suppose that \(\hat{\gamma}\) is one minimizer, we have

\[\widehat{W}^{n}=\tfrac{q^{2}}{2\lambda_{2}}-\tfrac{1}{2\lambda_{2}}\sum_{i=1}^ {n}\hat{\gamma}_{i}(w_{\hat{\mu}}(\tilde{x}_{i})+w_{\hat{\nu}}(\tilde{y}_{i})).\]

To our knowledge, the existing method proposed for solving Eq. (2.4) is a short-step interior-point method for which the required number of iterations is known to be large when \(n\) is large, which is necessary to guarantee small statistical error. To avoid this issue, Muzellec et al. [38] proposed solving an unconstrained relaxation model which allows for the application of gradient-based methods. However, the estimators obtained from solving such relaxation model lack any statistical guarantee.

### Nonsmooth equation model and optimality condition

For simplicity, we define the operator \(\Phi:\mathbb{R}^{n\times n}\mapsto\mathbb{R}^{n}\) and its adjoint \(\Phi^{\star}:\mathbb{R}^{n}\mapsto\mathbb{R}^{n\times n}\) by

\[\Phi(X)=\begin{pmatrix}\langle X,\Phi_{1}\Phi_{1}^{\top}\rangle\\ \vdots\\ \langle X,\Phi_{n}\Phi_{n}^{\top}\rangle\end{pmatrix},\quad\Phi^{\star}(\gamma)= \sum_{i=1}^{n}\gamma_{i}\Phi_{i}\Phi_{i}\Phi_{i}^{\top}.\]

We present the optimality notion for Eq. (2.4) as follows:

**Definition 2.1**: _A point \(\hat{\gamma}\in\mathbb{R}^{n}\) is an optimal solution of Eq. (2.4) if we have \(\Phi^{\star}(\hat{\gamma})+\lambda_{1}I\succeq 0\) and \(\frac{1}{4\lambda_{2}}\hat{\gamma}^{\top}Q\hat{\gamma}-\frac{1}{2\lambda_{2}} \hat{\gamma}^{\top}z+\frac{q^{2}}{4\lambda_{2}}\leq\frac{1}{4\lambda_{2}} \gamma^{\top}Q\gamma-\frac{1}{2\lambda_{2}}\gamma^{\top}z+\frac{q^{2}}{4 \lambda_{2}}\) for all \(\gamma\) satisfying that \(\Phi^{\star}(\gamma)+\lambda_{1}I\succeq 0\)._

Clearly, Eq. (2.4) can be reformulated as the following optimization problem given by

\[\min_{\gamma\in\mathbb{R}^{n}}\max_{X\succeq 0}\ \frac{1}{4\lambda_{2}} \gamma^{\top}Q\gamma-\frac{1}{2\lambda_{2}}\gamma^{\top}z+\frac{q^{2}}{4 \lambda_{2}}-\langle X,\Phi^{\star}(\gamma)+\lambda_{1}I\rangle.\] (2.5)

We denote \(w=(\gamma,X)\) as a vector-matrix pair and let \(R:\mathbb{R}^{n}\times\mathbb{R}^{n\times n}\to\mathbb{R}^{n}\times\mathbb{R} ^{n\times n}\) be given by

\[R(w)=\begin{pmatrix}\frac{1}{2\lambda_{2}}Q\gamma-\frac{1}{2\lambda_{2}}z- \Phi(X)\\ X-\text{proj}_{\mathcal{S}^{n}_{+}}(X-(\Phi^{\star}(\gamma)+\lambda_{1}I))\end{pmatrix}.\] (2.6)

where \(\mathcal{S}^{n}_{+}=\{X\in\mathbb{R}^{n\times n}:X\succeq 0\}\). Then, we can measure the optimality of \(w\) via appeal to the quantity \(\|R(w)\|\) and shows that the notion is the same as used in Definition 2.1.

**Proposition 2.3**: _A point \(\hat{\gamma}\) is an optimal solution of Eq. (2.4) if and only if \(\hat{w}=(\hat{\gamma},\hat{X})\) satisfies \(R(\hat{w})=0\) for some \(\hat{X}\succeq 0\)._

Proposition 2.3 shows that we can compute the kernel-based OT estimators by solving the nonsmooth equation model \(R(w)=0\). The optimality criterion based on the residual map \(R(\cdot)\) allows for a global convergence rate analysis for our specialized semismooth Newton method.

## 3 Algorithm and Convergence Analysis

In this section, we derive our algorithm and provide a convergence rate analysis. The key idea here is to apply the regularized semismooth Newton (SSN) method for solving \(R(w)=0\) and improve the computation of each SSN step by exploring the special structure of generalized Jacobian. We also safeguard the regularized SSN method by min-max method to achieve a global rate.

**Generalized Jacobian.** We first examine the special structure of the generalized Jacobian of \(R(w)\). Indeed, by using the definition of \(\mathcal{S}^{n}_{+}\), we have \(\text{proj}_{\mathcal{S}^{n}_{+}}(Z)=P_{\alpha}\Sigma_{\alpha}P^{\top}_{\alpha}\) where

\[Z=P\Sigma P^{\top}=(P_{\alpha}\quad P_{\bar{\alpha}})\begin{pmatrix}\Sigma_{ \alpha}&0\\ 0&\Sigma_{\bar{\alpha}}\end{pmatrix}\begin{pmatrix}P^{\top}_{\alpha}\\ P^{\top}_{\bar{\alpha}}\end{pmatrix},\] (3.1)

with \(\Sigma=\text{diag}(\sigma_{1},\dots,\sigma_{n})\) and the sets of the indices of positive and nonpositive eigenvalues of \(Z\) (we denote these sets by \(\alpha=\{i\mid\sigma_{i}>0\}\) and \(\bar{\alpha}=\{1,2,\dots,n\}\setminus\alpha\)). Moreover, we notice that \(R\) is Lipschitz continuous. Then, Rademacher's theorem can guarantee that \(R\) is almost everywhere differentiable. We introduce the concepts of generalized Jacobian [8].

**Definition 3.1**: _Suppose that \(R\) is Lipschitz continuous and \(D_{R}\) is the set of differentiable points of \(R\). The B-subdifferential of \(R\) at \(w\) is given by \(\partial_{B}R(w):=\{\lim_{k\to+\infty}\nabla F(w^{k})\mid w^{k}\in D_{R},w^{k }\to w\}\). The set \(\partial R(w)=\text{conv}(\partial_{B}R(w))\) is called generalized Jacobian where \(\text{conv}\) denotes the convex hull._

We define a generalized operator \(\mathcal{M}(Z)\in\partial\text{proj}_{\mathcal{S}^{n}_{+}}(Z)\) using its application to an \(n\times n\) matrix \(S\):

\[\mathcal{M}(Z)[S]=P(\Omega\circ(P^{\top}SP))P^{\top}\text{ for all }S\succeq 0,\]

where the \(\circ\) symbol denotes a Hadamard product and \(\Omega=\begin{pmatrix}E_{\alpha\alpha}&\eta_{\alpha\bar{\alpha}}\\ \eta_{\alpha\bar{\alpha}}^{\top}&0\end{pmatrix}\) with \(E_{\alpha\alpha}\) being a matrix of ones and \(\eta_{ij}=\frac{\sigma_{i}}{\sigma_{i}-\sigma_{j}}\) for all \((i,j)\in\alpha\times\bar{\alpha}\). Note that all entries of \(\Omega\) lie in the interval \((0,1]\). In general, it is nontrivial to characterize the generalized Jacobian \(\partial R(w)\) exactly but we can compute an element \(\mathcal{J}(w)\in\partial R(w)\) using \(\mathcal{M}(\cdot)\) as defined before.

We next introduce the definition of the (strong) semismoothness of an operator.

**Definition 3.2**: _Suppose that \(R\) is Lipschitz continuous. Then, \(R\) is (strongly) semismooth at \(w\) if (i) \(R\) is directionally differentiable at \(w\); and (ii) for any \(\Delta w\) and \(\mathcal{J}\in\partial R(w+\Delta w)\), we have_

\[\begin{array}{cc}\textbf{(semismooth)}&\frac{\|R(w+\Delta w)-R(w)-\mathcal{J }[\Delta w]\|}{\|\Delta w\|}\to 0,\\ \textbf{(strongly semismooth)}&\frac{\|R(w+\Delta w)-R(w)-\mathcal{J}[\Delta w]\|}{\| \Delta w\|^{2}}\leq C.\end{array},\quad\text{as }\Delta w\to 0.\]The following proposition characterizes the residual map given in Eq. (2.6) and its generalized Jacobian matrix. It also guarantees that the SSN method is suitable to solve \(R(w)=0\).

**Proposition 3.1**: _The residual map \(R\) given in Eq. (2.6) is strongly semismooth._

**Regularized SSN step.** We then discuss how to compute the Newton direction efficiently. In particular, at a given iterate \(w_{k}\), we compute a Newton direction \(\Delta w_{k}\) by solving the equation

\[(\mathcal{J}_{k}+\mu_{k}\mathcal{I})[\Delta w_{k}]=-r_{k},\] (3.2)

where \(\mathcal{J}_{k}\in\partial R(w_{k})\), \(r_{k}=R(w_{k})\) and \(\mathcal{I}\) is an identity operator. The regularization parameter is chosen as \(\mu_{k}=\theta_{k}\|r_{k}\|\) for stabilizing the semismooth Newton method in practice. From a computational point of view, it is not practical to solve the linear system in Eq. (3.2) exactly. Thus, we seek an approximation step \(\Delta w_{k}\) by solving Eq. (3.2) approximately such that

\[\|(\mathcal{J}_{k}+\mu_{k}\mathcal{I})[\Delta w_{k}]+r_{k}\|\leq\tau\min\{1, \kappa\|r_{k}\|\|\Delta w_{k}\|\},\] (3.3)

where \(0<\tau,\kappa<1\) are some positive constants and \(\|\cdot\|\) is defined for a vector-matrix pair \(w=(\gamma,X)\) (i.e., \(\|w\|=\|\gamma\|_{2}+\|X\|_{F}\) where \(\|\cdot\|_{2}\) is Euclidean norm and \(\|\cdot\|_{F}\) is Frobenius norm).

Since \(\mathcal{J}_{k}\) in Eq. (3.2) is nonsymmetric and its dimension is large, we consider applying the Schur complement trick to transform Eq. (3.2) into a smaller symmetric system. If we vectorize the vector-matrix pair \(\Delta w\)3, the operators \(\mathcal{M}(Z)\) and \(\Phi\) can be expressed as matrices:

Footnote 3: If \(w=(\gamma,X)\) is a vector-matrix pair, we define \(\text{vec}(w)=(\gamma;\text{vec}(X))\) as its vectorization.

\[M(Z)=\tilde{P}\Gamma\tilde{P}^{\top}\in\mathbb{R}^{n^{2}\times n^{2}},\quad A =\begin{pmatrix}\Phi_{1}^{\top}\otimes\Phi_{1}^{\top}\\ \vdots\\ \Phi_{n}^{\top}\otimes\Phi_{n}^{\top}\end{pmatrix}\in\mathbb{R}^{n\times n^{2}},\]

where \(\tilde{P}=P\otimes P\) and \(\Gamma=\text{diag}(\text{vec}(\Omega))\).

We next provide a key lemma on the matrix form of \(\mathcal{J}_{k}+\mu_{k}I\) at a given iterate \(w_{k}=(\gamma_{k},X_{k})\).

**Lemma 3.2**: _Given an iterate \(w_{k}=(\gamma_{k},X_{k})\), we compute \(Z_{k}=X_{k}-(\Phi^{\star}(\gamma_{k})+\lambda_{1}I)\) and use Eq. (3.1) to obtain \(P_{k}\), \(\Sigma_{k}\), \(\alpha_{k}\) and \(\bar{\alpha}_{k}\). We then obtain \(\Omega_{k}\), \(\tilde{P}_{k}=P_{k}\otimes P_{k}\) and \(\Gamma_{k}=\text{diag}(\text{vec}(\Omega_{k}))\). Then, the matrix form of \(\mathcal{J}_{k}+\mu_{k}I\) is given by_

\[(J_{k}+\mu_{k}I)^{-1}=C_{1}BC_{2},\]

_where_

\[C_{1}=\begin{pmatrix}I&0\\ -T_{k}A^{\top}&I\end{pmatrix},\quad C_{2}=\begin{pmatrix}I&\frac{1}{\mu_{k}+ 1}(A+AT_{k})\\ 0&I\end{pmatrix},\]

_and_

\[B=\begin{pmatrix}(\frac{1}{2\lambda_{2}}Q+\mu_{k}I+AT_{k}A^{\top})^{-1}&0\\ 0&\frac{1}{\mu_{k}+1}(I+T_{k})\end{pmatrix},\]

_with \(T_{k}=\tilde{P}_{k}L_{k}\tilde{P}_{k}^{\top}\) where \(L_{k}\) is a diagonal matrix with \((L_{k})_{ii}=\frac{(\Gamma_{k})ii}{\mu_{k}+1-(\Gamma_{k})_{ii}}\) and \((\Gamma_{k})_{ii}\in(0,1]\) is then denoted as the \(i^{\text{th}}\) diagonal entry of \(\Gamma_{k}\)._

As a consequence of Lemma 3.2, the solution of Eq. (3.2) can be obtained by solving one certain symmetric linear system with the matrix \(\frac{1}{2\lambda_{2}}Q+\mu_{k}I+AT_{k}A^{\top}\). We remark that this system is well-defined since both \(Q\) and \(AT_{k}A^{\top}\) are positive semidefinite and the coefficient \(\mu_{k}\) is chosen such that \(\frac{1}{2\lambda_{2}}Q+\mu_{k}I+AT_{k}A^{\top}\) is invertible. This also shows that Eq. (3.2) is well-defined.

We define \(\mathcal{T}_{k}\) and \(\mathcal{Q}\) as the operator form of \(T_{k}=\tilde{P}_{k}L_{k}\tilde{P}_{k}^{\top}\) and \(Q\) and write \(r_{k}=(r_{k}^{1},r_{k}^{2})\) explicitly where \(r_{k}^{1}\in\mathbb{R}^{n}\) and \(r_{k}^{2}\in\mathbb{R}^{n\times n}\). Then, we have

\[\text{vec}(a)=-\begin{pmatrix}I&\frac{1}{\mu_{k}+1}(A+AT)\\ 0&I\end{pmatrix}\text{vec}(r_{k})\Longrightarrow\left\{\begin{array}{ll}a^{ 1}=-r_{k}^{1}-\frac{1}{\mu_{k}+1}(\Phi(r_{k}^{2}+\mathcal{T}_{k}[r_{k}^{2}])),\\ a^{2}=-r_{k}^{2}.\end{array}\right.\]

The next step consists in solving a new symmetric linear system and is given by

\[\text{vec}(\tilde{a})=\begin{pmatrix}(\frac{1}{2\lambda_{2}}Q+\mu_{k}I+AT_{k} A^{\top})^{-1}&0\\ 0&\frac{1}{\mu_{k}+1}(I+T_{k})\end{pmatrix}\text{vec}(a),\]

which leads to

\[\left\{\begin{array}{ll}\tilde{a}^{1}=(\frac{1}{2\lambda_{2}}\mathcal{Q}+\mu _{k}\mathcal{I}+\Phi\mathcal{T}_{k}\Phi^{\star})^{-1}a^{1},\\ \tilde{a}^{2}=\frac{1}{\mu_{k}+1}(a^{2}+\mathcal{T}_{k}[a^{2}]).\end{array}\right.\]

Compared to Eq. (3.2) whose matrix form has size \((n^{2}+n)\times(n^{2}+n)\), we remark that the one in the step above is smaller with the size of \(n\times n\) and can be efficiently solved by conjugate gradient (CG) method or symmetric quasi-minimal residual (QMR) method [28, 50]. The final step is to compute the Newton direction \(\Delta w_{k}=(\Delta w_{k}^{1},\Delta w_{k}^{2})\) as follows,

\[\text{vec}(\Delta w_{k})=\begin{pmatrix}I&0\\ -TA^{\top}&I\end{pmatrix}\text{vec}(\tilde{a})\Longrightarrow\left\{\begin{array} []{ll}\Delta w_{k}^{1}=\tilde{a}^{1},\\ \Delta w_{k}^{2}=\tilde{a}^{2}-\mathcal{T}_{k}[\Phi^{\star}(\tilde{a}^{1})]. \end{array}\right.\]

It remains to provide an efficient manner to compute \(\mathcal{T}_{k}[\cdot]\). Since \(\mathcal{T}_{k}\) is defined as the operator form of \(T=\tilde{P}_{k}L_{k}\tilde{P}_{k}^{\top}\), we have

\[\mathcal{T}_{k}[S]=P_{k}(\Psi_{k}\circ(P_{k}^{\top}SP_{k}))P_{k}^{\top},\]

where \(\Psi_{k}\) is determined by \(\mu_{k}\) and \(\Omega_{k}\). Indeed, we have

\[\Omega_{k}=\begin{pmatrix}E_{\alpha_{k}\alpha_{k}}&\eta_{\alpha_{k}\bar{ \alpha}_{k}}\\ \eta_{\alpha_{k}\bar{\alpha}_{k}}^{\top}&0\end{pmatrix}\Longrightarrow\Psi_{ k}=\begin{pmatrix}\frac{1}{\mu_{k}}E_{\alpha_{k}\alpha_{k}}&\xi_{\alpha_{k}\bar{ \alpha}_{k}}\\ \xi_{\alpha_{k}\bar{\alpha}_{k}}^{\top}&0\end{pmatrix},\]

where \(\xi_{ij}=\frac{\eta_{i}}{\mu_{k}+1-\eta_{ij}}\) for all \((i,j)\in\alpha_{k}\times\bar{\alpha}_{k}\). Following Zhao et al. [68], we use the decomposition \(\mathcal{T}_{k}[S]=G+G^{\top}\) where \(U=P_{k}(:,\alpha_{k})^{\top}S\) and

\[G=P_{k}(:,\alpha_{k})(\frac{1}{2\mu_{k}}(UP_{k}(:,\alpha_{k}))P_{k}(:,\alpha_{ k})^{\top}+\xi_{\alpha_{k}\bar{\alpha}_{k}}\circ(UP_{k}(:,\bar{\alpha}_{k}))P_{k}(:, \bar{\alpha}_{k})^{\top}).\]

The number of flops required to compute \(\mathcal{T}_{k}[S]\) is \(8|\alpha_{k}|n^{2}\). For the case of \(|\alpha_{k}|>\bar{\alpha}_{k}\), we compute \(\mathcal{T}_{k}[S]\) via \(\mathcal{T}_{k}[S]=\frac{1}{\mu_{k}}S-P_{k}((\frac{1}{\mu_{k}}E-\Psi_{k}) \circ(P_{k}^{\top}SP_{k}))P_{k}^{\top}\) using \(8|\bar{\alpha}_{k}|n^{2}\) flops. This demonstrates that we can obtain an approximate solution of Eq. (3.2) efficiently whenever \(|\alpha_{k}|\) or \(|\bar{\alpha}_{k}|\) is small. We present the scheme for computing an approximate Newton direction in Algorithm 1.

```
1:Input:\(\tau,\kappa\), \(\alpha_{2}\geq\alpha_{1}>0\), \(\beta_{0}<1\), \(\beta_{1},\beta_{2}>1\) and \(\underline{\theta},\overline{\theta}>0\).
2:Initialization:\(v_{0}=w_{0}\in\mathbb{R}^{n}\times\mathcal{S}_{+}^{n}\) and \(\theta_{0}>0\). Set \(k=0\).
3:for\(k=0,1,2,\ldots\)do
4: Update \(v_{k+1}\) from \(v_{k}\) using one-step EG.
5: Select \(\mathcal{J}_{k}\in\partial R(w_{k})\).
6: Solve the linear system in Eq. (3.2) approximately such that \(\Delta w_{k}\) satisfies Eq. (3.3).
7: Compute \(\tilde{w}_{k+1}=w_{k}+\Delta w_{k}\).
8: Update \(\theta_{k+1}\) using Eq. (3.4) accordingly.
9: Set \(w_{k+1}=\tilde{w}_{k+1}\) if \(\|R(\tilde{w}_{k+1})\|\leq\|R(v_{k+1})\|\) is satisfied. Otherwise, set \(w_{k+1}=v_{k+1}\). ```

**Algorithm 2** A specialized SSN method with safeguarding

**Adaptive strategy.** We propose a rule for updating \(\theta_{k}\) where \(\mu_{k}=\theta_{k}\|r_{k}\|\) is defined in Eq. (3.2). Indeed, we compute \(\rho_{k}=-\langle R(w_{k}),\Delta w_{k}\rangle\) and use it to update \(\theta_{k+1}\). The update rule is summarized as follows:

\[\theta_{k+1}=\left\{\begin{array}{ll}\max\{\underline{\theta},\beta_{0}\theta_{ k}\},&\text{if }\rho_{k}\geq\alpha_{2}\|\underline{\Delta w_{k}}\|^{2},\\ \beta_{1}\theta_{k},&\text{if }\alpha_{1}\|\Delta w_{k}\|^{2}\leq\rho_{k}<\alpha_{2}\| \Delta w_{k}\|^{2},\\ \min\{\overline{\theta},\beta_{2}\theta_{k}\},&\text{otherwise}.\end{array}\right.\] (3.4)

where \(\beta_{0}<1,\beta_{1},\beta_{2}>1\) and \(\underline{\theta},\overline{\theta}>0\).

**Main scheme.** We summarize the complete scheme of our new algorithm in Algorithm 2. Indeed, we generate a sequence of iterates by alternating between extragradient (EG) method [17; 6] and the aforementioned regularized SSN method.

Note that we maintain one auxiliary sequence of iterates \(\{v_{k}\}_{k\geq 0}\). This sequence is directly generated by the EG method for solving the min-max optimization problem in Eq. (2.5) and is used to safeguard the regularized SSN method to achieve a global convergence rate. More specifically, we start with \(v_{0}=w_{0}\in\mathbb{R}^{n}\times\mathcal{S}_{n}^{n}\) and perform the \(k^{\text{th}}\) iteration as follows,

1. Update \(v_{k+1}\) from \(v_{k}\) using one-step EG.
2. Update \(\tilde{w}_{k+1}\) from \(w_{k}\) using one-step regularized SSN.
3. Set \(w_{k+1}=\tilde{w}_{k+1}\) if \(\|R(\tilde{w}_{k+1})\|\leq\|R(v_{k+1})\|\) and \(w_{k+1}=v_{k+1}\) otherwise.

In our experiment, we find that the main iterates are mostly generated by regularized SSN steps and the whole algorithm converges at a superlinear rate. This phenomenon is quite intuitive: if the initial point is sufficiently close to one nondegenerate optimal solution, the regularized SSN method can achieve the similar quadratic convergence rate (cf. Theorem 3.4) as shared by other SSN methods in the existing literature [35; 18; 1]. The detailed analysis will be provided in the appendix.

**Main results.** We establish the convergence guarantee of Algorithm 2 in the following theorems.

**Theorem 3.3**: _Suppose that \(\{w_{k}\}_{k\geq 0}\) is a sequence of iterates generated by Algorithm 2. Then, the residuals of \(\{w_{k}\}_{k\geq 0}\) converge to 0 at a rate of \(1/\sqrt{k}\), i.e., \(\|R(w_{k})\|=O(1/\sqrt{k})\)._

**Theorem 3.4**: _Suppose that \(\{w_{k}\}_{k\geq 0}\) is a sequence of iterates generated by Algorithm 2. Then, the residuals of \(\{w_{k}\}_{k\geq 0}\) converge to 0 at a quadratic rate if the initial point \(w_{0}\) is sufficiently close to \(w^{\star}\) with \(R(w^{\star})=0\) and every element of \(\partial R(w^{\star})\) is invertible._

**Remark 3.5**: _In the context of constrained convex-concave min-max optimization problem, Cai et al. [6] proved the \(O(1/\sqrt{k})\) last-iterate convergence rate of the EG, matching the lower bounds [24; 23]. Since the kernel-based OT estimation can be solved as a min-max problem, the global convergence rate in Theorem 3.3 demonstrates the efficiency of Algorithm 2. It remains unclear whether or not we can improve the convergence result by exploring special structure of Eq. (2.5)._

## 4 Experiments

We present the results of experiments that evaluate the kernel-based OT estimation with our algorithm. The baseline approach is the short-step interior-point method [59]; we exclude the gradient-based method [38] from our experiment since it only solves the relaxation model. All the experiments were conducted on a MacBook Pro with an Intel Core i9 2.4GHz and 16GB memory.

Following the setup in Vacher et al. [59], we draw \(n_{\text{sample}}\) samples from \(\mu\) and \(n_{\text{sample}}\) samples from \(\nu\), where \(\mu\) is a mixture of 3 \(d\)-dimensional Gaussian distributions and \(\nu\) is a mixture of 5 \(d\)-dimensional Gaussian distributions. Then, we sample \(n\) filling samples from a \(2d\) Sobol sequence. We also set the bandwidth \(\sigma^{2}=0.01\) and parameters \(\lambda_{1}=\frac{1}{n}\) and \(\lambda_{2}=\frac{1}{\sqrt{n_{\text{sample}}}}\). Focusing on the case of \(d=1\) (i.e., 1-dimensional setting), we report the visualization results in Figure 1 and 2 and find that the inferred OT map will be closer the the true OT map as the number of filling points and data samples increase.

Figure 1: Visualization of the OT map with \(n_{\text{sample}}=n\in\{50,100,200\}\).

By varying the dimension \(d\in\{2,5,10\}\), we also report the computation efficiency results in Figure 3. It indicates that the our new algorithm is more efficient than the IPM as the number of filling points increases, with smaller variance in computation time (seconds).

The experiments comparing kernel-based OT estimators with plug-in OT estimators on synthetic datasets have been conducted before [59; 38] and the results demonstrate that the kernel-based OT estimators behave better when the number of samples is small. Here, we repeat such experiment but using the real-world 4i datasets from Bunne et al. [5], which contains single-cell perturbed responses, and which include the unperturbed cells and cells subject to drug perturbations. Our experiments are conducted on 15 datasets with different drug perturbations.

Due to space limit, we defer the results to Appendix G (see Figure 4). We can see that the kernel-based OT estimators computed by our algorithm achieve satisfactory performance and behave better in most cases when the number of training samples is small; in particular, they better on 6 datasets, comparable on 5 datasets and worse on 4 datasets. Note that \(\tt{OTT}\) computes the entropic regularized plug-in OT estimators and is heavily optimized to effectively handle noisy data. Therefore, it would be no surprise that \(\tt{OTT}\) outperforms our algorithm when the number of training samples is sufficient. However, the kernel-based OT estimation still provides a fairly effective alternative when the number of training samples is small, which is consistent with the previous observations on synthetic data [59; 38]. Our results also validate the effectiveness of our algorithm for computing kernel-based OT estimators.

## 5 Concluding Remarks

In this paper, we propose a nonsmooth equation model for computing kernel-based OT estimators and show that it has a special problem structure, allowing it to be solved in an efficient manner using semismooth Newton method. In particular, we propose a specialized semismooth Newton method that achieves low per-iteration computational cost by exploiting the special problem structure, and prove a global sublinear convergence rate and a local quadratic convergence rate under standard regularity conditions. Preliminary experimental results on synthetic datasets show that our algorithm is more efficient than the short-step interior-point method [59], and the results on real data demonstrate the effectiveness of our algorithm. Future work includes the applications of kernel-based OT estimators to deep generative models and other real-world problems.

Figure 3: Comparisons of mean computation time of IPM and our algorithm on CPU time.

Figure 2: Visualization of the constraint with \(n_{\text{sample}}=n\in\{50,100\}\). The right one is ground truth.

## References

* [1] A. Ali, E. Wong, and J. Z. Kolter. A semismooth Newton method for fast, generic convex programming. In _ICML_, pages 70-79. PMLR, 2017.
* [2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _ICML_, pages 214-223, 2017.
* [3] P-C. Aubin-Frankowski and Z. Szabo. Hard shape-constrained kernel machines. In _NeurIPS_, pages 384-395, 2020.
* [4] N. Bonneel, J. Rabin, G. Peyre, and H. Pfister. Sliced and radon Wasserstein barycenters of measures. _Journal of Mathematical Imaging and Vision_, 51(1):22-45, 2015.
* [5] C. Bunne, S. G. Stark, G. Gut, J. S. del Castillo, K-V. Lehmann, L. Pelkmans, A. Krause, and G. Ratsch. Learning single-cell perturbation responses using neural optimal transport. _BioRxiv_, 2021.
* [6] Y. Cai, A. Oikonomou, and W. Zheng. Finite-time last-iterate convergence for learning in multi-player games. In _NeurIPS_, pages 33904-33919, 2022.
* [7] L. Chizat, P. Roussillon, F. Leger, F-X. Vialard, and G. Peyre. Faster Wasserstein distance estimation with the Sinkhorn divergence. In _NeurIPS_, pages 2257-2269, 2020.
* [8] F. H. Clarke. _Optimization and Nonsmooth Analysis_. SIAM, 1990.
* [9] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 39(9):1853-1865, 2016.
* [10] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In _NIPS_, pages 3733-3742, 2017.
* [11] M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In _NIPS_, pages 2292-2300, 2013.
* [12] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In _ICML_, pages 685-693. PMLR, 2014.
* [13] M. Cuturi, L. Meng-Papaxanthos, Y. Tian, C. Bunne, G. Davis, and O. Teboul. Optimal transport tools (OTT): A java toolbox for all things wasserstein. _ArXiv Preprint: 2201.12324_, 2022.
* [14] G. De Philippis and A. Figalli. The Monge-Ampere equation and its link to optimal transportation. _Bulletin of the American Mathematical Society_, 51(4):527-580, 2014.
* [15] N. Deb, P. Ghosal, and B. Sen. Rates of estimation of optimal transport maps using plug-in estimators via barycentric projections. In _NeurIPS_, pages 29736-29753, 2021.
* [16] R. M. Dudley. The speed of mean glivenko-cantelli convergence. _The Annals of Mathematical Statistics_, 40(1):40-50, 1969.
* [17] F. Facchinei and J-S. Pang. _Finite-Dimensional Variational Inequalities and Complementarity Problems_. Springer Science & Business Media, 2007.
* [18] F. Facchinei, A. Fischer, and C. Kanzow. Inexact Newton methods for semismooth equations with applications to variational inequality problems. In _Nonlinear Optimization and Applications_, pages 125-139. Springer, 1996.
* [19] N. Fournier and A. Guillin. On the rate of convergence in wasserstein distance of the empirical measure. _Probability Theory and Related Fields_, 162(3):707-738, 2015.
* [20] C. Frogner, C. Zhang, H. Mobahi, M. Araya-Polo, and T. Poggio. Learning with a Wasserstein loss. In _NIPS_, pages 2053-2061, 2015.

* Genevay et al. [2018] A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with Sinkhorn divergences. In _AISTATS_, pages 1608-1617, 2018.
* Genevay et al. [2019] A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyre. Sample complexity of Sinkhorn divergences. In _AISTATS_, pages 1574-1583. PMLR, 2019.
* Golowich et al. [2020] N. Golowich, S. Pattathil, and C. Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In _NeurIPS_, pages 20766-20778, 2020.
* Golowich et al. [2020] N. Golowich, S. Pattathil, C. Daskalakis, and A. Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In _COLT_, pages 1758-1784. PMLR, 2020.
* Ho et al. [2017] N. Ho, X. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel clustering via Wasserstein means. In _ICML_, pages 1501-1509. PMLR, 2017.
* Hutter and Rigollet [2021] J-C. Hutter and P. Rigollet. Minimax estimation of smooth optimal transport maps. _The Annals of Statistics_, 49(2):1166-1194, 2021.
* Janati et al. [2020] H. Janati, T. Bazeille, B. Thirion, M. Cuturi, and A. Gramfort. Multi-subject MEG/EEG source imaging with sparse multi-task regression. _NeuroImage_, 220:116847, 2020.
* Kelley [1995] C. T. Kelley. _Iterative Methods for Linear and Nonlinear Equations_. SIAM, 1995.
* Kolouri et al. [2019] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. K. Rohde. Generalized sliced Wasserstein distances. In _NIPS_, pages 261-272, 2019.
* Li et al. [2018] X. Li, D. Sun, and K-C. Toh. A highly efficient semismooth Newton augmented Lagrangian method for solving Lasso problems. _SIAM Journal on Optimization_, 28(1):433-458, 2018.
* Lin et al. [2020] T. Lin, C. Fan, N. Ho, M. Cuturi, and M. I. Jordan. Projection robust Wasserstein distance and Riemannian optimization. In _NeurIPS_, pages 9383-9397, 2020.
* Lin et al. [2021] T. Lin, Z. Zheng, E. Chen, M. Cuturi, and M. I. Jordan. On projection robust optimal transport: Sample complexity and model misspecification. In _AISTATS_, pages 262-270. PMLR, 2021.
* Liu et al. [2022] Y. Liu, Z. Wen, and W. Yin. A multiscale semismooth Newton method for optimal transport. _Journal of Scientific Computing_, 91(2):1-29, 2022.
* Manole et al. [2021] T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman. Plugin estimation of smooth optimal transport maps. _ArXiv Preprint: 2107.12364_, 2021.
* Martinez and Qi [1995] J. Martinez and L. Qi. Inexact Newton methods for solving nonsmooth equations. _Journal of Computational and Applied Mathematics_, 60(1-2):127-145, 1995.
* Mena and Niles-Weed [2019] G. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: Sample complexity and the central limit theorem. In _NIPS_, pages 4541-4551, 2019.
* Mifflin [1977] R. Mifflin. Semismooth and semiconvex functions in constrained optimization. _SIAM Journal on Control and Optimization_, 15(6):959-972, 1977.
* Muzellec et al. [2021] B. Muzellec, A. Vacher, F. Bach, F-X. Vialard, and A. Rudi. Near-optimal estimation of smooth transport maps with kernel sums-of-squares. _ArXiv Preprint: 2112.01907_, 2021.
* Nadjahi et al. [2020] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli. Statistical and topological properties of sliced probability divergences. In _NeurIPS_, pages 20802-20812, 2020.
* Niles-Weed and Rigollet [2022] J. Niles-Weed and P. Rigollet. Estimation of Wasserstein distances in the spiked transport model. _Bernoulli_, 28(4):2663-2688, 2022.

* [41] F-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In _ICML_, pages 5072-5081. PMLR, 2019.
* [42] V. I. Paulsen and M. Raghupathi. _An Introduction to The Theory of Reproducing Kernel Hilbert Spaces_, volume 152. Cambridge University Press, 2016.
* [43] G. Peyre and M. Cuturi. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [44] A-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. _ArXiv Preprint: 2109.12004_, 2021.
* [45] H. Qi and D. Sun. An augmented Lagrangian dual approach for the H-weighted nearest correlation matrix problem. _IMA Journal of Numerical Analysis_, 31(2):491-511, 2011.
* [46] L. Qi and D. Sun. A survey of some nonsmooth equations and smoothing Newton methods. In _Progress in Optimization_, pages 121-146. Springer, 1999.
* [47] L. Qi and J. Sun. A nonsmooth version of Newton's method. _Mathematical Programming_, 58(1):353-367, 1993.
* [48] J. Rabin, G. Peyre, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture mixing. In _International Conference on Scale Space and Variational Methods in Computer Vision_, pages 435-446. Springer, 2011.
* [49] I. Redko, N. Courty, R. Flamary, and D. Tuia. Optimal transport for multi-source domain adaptation under target shift. In _AISTATS_, pages 849-858. PMLR, 2019.
* [50] Y. Saad. _Iterative Methods for Sparse Linear Systems_. SIAM, 2003.
* [51] T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. In _ICLR_, 2018. URL https://openreview.net/forum?id=rkQkBnJAb.
* [52] F. Santambrogio. _Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling_, volume 87. Birkhauser, 2015.
* [53] G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, and P. Berube. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. _Cell_, 176(4):928-943, 2019.
* [54] M. V. Solodov and B. F. Svaiter. A globally convergent inexact Newton method for systems of monotone equations. _Reformulation: Nonsmooth, Piecewise Smooth, Semismooth and Smoothing Methods_, pages 355-369, 1999.
* [55] S. Srivastava, V. Cevher, Q. Dinh, and D. Dunson. WASP: Scalable Bayes via barycenters of subset posteriors. In _AISTATS_, pages 912-920. PMLR, 2015.
* [56] D. Sun and J. Sun. Semismooth matrix-valued functions. _Mathematics of Operations Research_, 27(1):150-169, 2002.
* [57] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In _ICLR_, 2018.
* [58] M. Ulbrich. _Semismooth Newton Methods for Variational Inequalities and Constrained Optimization Problems in Function Spaces_. SIAM, 2011.
* [59] A. Vacher, B. Muzellec, A. Rudi, F. Bach, and F-X. Vialard. A dimension-free computational upper-bound for smooth optimal transport estimation. In _COLT_, pages 4143-4173. PMLR, 2021.
* [60] C. Villani. _Optimal Transport: Old and New_, volume 338. Springer, 2009.
* [61] and 3.)* Wang et al. [2010] C. Wang, D. Sun, and K-C. Toh. Solving log-determinant optimization problems by a Newton-CG primal proximal point algorithm. _SIAM Journal on Optimization_, 20(6):2994-3013, 2010. (Cited on page 14.)
* Weed and Bach [2019] J. Weed and F. Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. _Bernoulli_, 25(4A):2620-2648, 2019.
* Weed and Berthet [2019] J. Weed and Q. Berthet. Estimation of smooth densities in Wasserstein distance. In _COLT_, pages 3118-3119. PMLR, 2019.
* Xiao et al. [2018] X. Xiao, Y. Li, Z. Wen, and L. Zhang. A regularized semismooth Newton method with projection steps for composite convex programs. _Journal of Scientific Computing_, 76(1):364-389, 2018. (Cited on page 14.)
* Yang et al. [2013] J. Yang, D. Sun, and K-C. Toh. A proximal point algorithm for log-determinant optimization with group Lasso regularization. _SIAM Journal on Optimization_, 23(2):857-893, 2013. (Cited on page 14.)
* Yang et al. [2020] K. D. Yang, K. Damodaran, S. Venkatachalapathy, A. C. Soylemezoglu, G. V. Shivashankar, and C. Uhler. Predicting cell lineages using autoencoders and optimal transport. _PLoS Computational Biology_, 16(4):e1007828, 2020.
* Yang et al. [2015] L. Yang, D. Sun, and K-C. Toh. SDPNAL++: a majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints. _Mathematical Programming Computation_, 7(3):331-366, 2015.
* Zhao et al. [2010] X-Y. Zhao, D. Sun, and K-C. Toh. A Newton-CG augmented Lagrangian method for semidefinite programming. _SIAM Journal on Optimization_, 20(4):1737-1765, 2010.
* Zhou and Toh [2005] G. Zhou and K-C. Toh. Superlinear convergence of a Newton-type algorithm for monotone equations. _Journal of Optimization Theory and Applications_, 125(1):205-221, 2005.