# Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training

Rie Johnson

RJ Research Consulting

New York, USA

riejohnson@gmail.com

&Tong Zhang

HKUST

Hong Kong

tozhang@tongzhang-ml.org

This work was done when the second author was jointly with Google Research.

###### Abstract

As deep neural networks are highly expressive, it is important to find solutions with small _generalization gap_ (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call _inconsistency_ and _instability_ of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the _sharpness_ of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.

## 1 Introduction

As deep neural networks are highly expressive, the _generalization gap_ (the difference between the performance on the training data and unseen data) can be a serious issue. There has been intensive effort to improve generalization, concerning, for example, network architectures [15; 11], the training objective [10], strong data augmentation and mixing [6; 41; 39]. In particular, there has been extensive effort to understand the connection between generalization and the _sharpness_ of the loss landscape surrounding the model [14; 20; 16; 18; 10]. [18] conducted large-scale experiments with a number of metrics and found that the sharpness-based metrics predicted the generalization gap best. [10] has shown that the sharpness measured by the maximum loss difference around the model (\(m\)_-sharpness_) correlates well to the generalization gap, which justifies their proposed method _sharpness-aware minimization (SAM)_, designed to reduce the \(m\)-sharpness.

This paper studies generalization gap from a different perspective. Noting that the standard procedure for neural network training is _stochastic_ so that a different instance leads to a different model, we first present a theoretical analysis in which the bound of generalization gap depends on _inconsistency_ and _instability of model outputs_, conceptually described as follows. Let \(P\) be a stochastic training procedure, e.g., minimization of the cross-entropy loss by stochastic gradient descent (SGD) starting from random initialization with a certain combination of hyperparameters. It can be regarded as a random function that maps a set of labeled training data as its input to model parameter as its output. We say procedure \(P\) has high _inconsistency_ of model outputs if the predictions made by the models trained with \(P\) using the _same training data_ are very different from one another in expectation over the underlying (but unknown) data distribution. We also say \(P\) has high _instability_ of model outputs if _different_ sampling of _training data_ changes the expected predictions a lot over the underlying data distribution. Although both quantities are discrepancies of model outputs, the sources of thediscrepancies are different. The term _stability_ comes from the related concept in the literature [4; 31]. Inconsistency is related to the _disagreement_ metric studied in [27; 17; 22], but there are crucial differences between them, as shown later. Both inconsistency and instability of model outputs can be estimated on _unlabeled data_.

Our high-level goal is to find the essential property of models that generalize well; such an insight would be useful for improving training algorithms. With this goal, we empirically studied the connection of inconsistency and instability with the generalization gap. As our bound also depends on a property of model parameter distributions (for which we have theoretical insight but which we cannot easily estimate), we first experimented to find the condition under which inconsistency and instability of model outputs are highly correlated to generalization gap. The found condition - low randomness in the final state - is consistent with the theory, and it covers practically useful settings. We also found that when this condition is met, inconsistency alone is almost as predictive as inconsistency and instability combined. This is a practical advantage since estimation of instability requires multiple training sets and estimation of inconsistency does not. We thus focused on inconsistency, which enabled the use of full-size training data, and studied inconsistency in comparison with sharpness in the context of algorithmic reduction of these two quantities. We observed that while both sharpness reduction and inconsistency reduction lead to better generalization, there are a number of cases where inconsistency and generalization gap are reduced and yet sharpness remains relatively high. We view it as an indication that inconsistency has a stronger (and perhaps more essential) connection with generalization gap than sharpness.

Our contributions are as follows.

* We develop a theory that relates generalization gap to instability and inconsistency of model outputs, which can be estimated on unlabeled data.
* Empirically, we show that instability and inconsistency are strongly predictive of generalization gap in various settings.
* We show that algorithmic encouragement of consistency reduces inconsistency and improves performance, which can lead to further improvement of the state of the art performance.
* Our results provide a theoretical basis for existing methods such as co-distillation and ensemble.

## 2 Theory

The theorem below quantifies generalization gap by three quantities: inconsistency of model outputs, instability of model outputs, and information-theoretic instability of model parameter distributions.

NotationLet \(f(\theta,x)\) be the output of model \(\theta\) on data point \(x\) in the form of probability estimates (e.g., obtained by applying softmax). We use the upright bold font for probability distributions. Let \(Z=(X,Y)\) be a random variable representing a labeled data point (data point \(X\) and label \(Y\)) with a given unknown distribution \(\mathbf{Z}\). Let \(Z_{n}=\{(X_{i},Y_{i}):i=1,\ldots,n\}\) be a random variable representing iid training data of size \(n\) drawn from \(\mathbf{Z}\). Let \(\mathbf{\Theta}_{P|Z_{n}}\) be the distribution of model parameters resulting from applying a (typically stochastic) training procedure \(P\) to training set \(Z_{n}\).

Inconsistency, instability, and information-theoretic instabilityInconsistency of model outputs \(\mathcal{C}_{P}\) ('\(\mathcal{C}\)' for consistency) of training procedure \(P\) represents _the discrepancy of outputs among the models trained on the same training data_:

\[\mathcal{C}_{P}=\mathbb{E}_{Z_{n}}\mathbb{E}_{\mathbf{\Theta},\mathbf{\Theta }^{\prime}\sim\mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{X}\mathrm{KL}(f(\mathbf{ \Theta},X)||f(\mathbf{\Theta}^{\prime},X))\qquad\text{ (Inconsistency of model outputs)}\]

The source of inconsistency could be the random initialization, sampling of mini-batches, randomized data augmentation, and so forth.

To define instability of model outputs, let \(\bar{f}_{P|Z_{n}}(x)\) be the expected outputs (for data point \(x\)) of the models trained by procedure \(P\) on training data \(Z_{n}\): \(\bar{f}_{P|Z_{n}}(x)=\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_{n}}} f(\mathbf{\Theta},x)\). Instability of model outputs \(\mathcal{S}_{P}\) ('\(\mathcal{S}\)' for stability) of procedure \(P\) represents _how much the expected prediction \(\bar{f}_{P|Z_{n}}(x)\) would change with change of training data_:

\[\mathcal{S}_{P}=\mathbb{E}_{Z_{n},Z_{n}^{\prime}}\mathbb{E}_{X}\mathrm{KL}( \bar{f}_{P|Z_{n}}(X)||\bar{f}_{P|Z_{n}^{\prime}}(X))\qquad\qquad\text{( Instability of model outputs)}\]Finally, the instability of model parameter distributions \(\mathcal{I}_{P}\) is the mutual information between the random variable \(\Theta_{P}\), which represents the model parameters produced by procedure \(P\), and the random variable \(Z_{n}\), which represents training data. \(\mathcal{I}_{P}\) quantifies the dependency of the model parameters on the training data, which is also called _information theoretic (in)stability_ in [38]:

\[\mathcal{I}_{P}=I(\Theta_{P};Z_{n})=\mathbb{E}_{Z_{n}}\mathrm{KL}(\mathbf{ \Theta}_{P|Z_{n}}||\mathbb{E}_{Z_{n}^{\prime}}\mathbf{\Theta}_{P|Z_{n}^{\prime }})\] (Instability of model parameter distributions)

The second equality follows from the well-known relation of the mutual information to the KL divergence. The rightmost expression might be more intuitive, which essentially represents _how much the model parameter distribution would change with change of training data_.

Note that inconsistency and instability of model outputs can be estimated on _unlabeled data_. \(\mathcal{I}_{P}\) cannot be easily estimated as it involves distributions over the entire model parameter space; however, we have theoretical insight from its definition. (More precisely, it is possible to estimate \(\mathcal{I}_{P}\) by sampling in theory, but practically, it is not quite possible to make a reasonably good estimate of \(\mathcal{I}_{P}\) for deep neural networks with a reasonably small amount of computation.)

**Theorem 2.1**.: _Using the definitions above, we consider a Lipschitz loss function \(\phi(f,y)\in[0,1]\) that satisfies \(|\phi(f,y)-\phi(f^{\prime},y)|\leq\frac{\gamma}{2}\|f-f^{\prime}\|_{1},\) where \(f\) and \(f^{\prime}\) are probability estimates and \(\gamma/2>0\) is the Lipschitz constant. Let \(\psi(\lambda)=\frac{e^{\lambda}-\lambda-1}{\lambda^{2}}\), which is an increasing function. Let \(\mathcal{D}_{P}=\mathcal{C}_{P}+\mathcal{S}_{P}\). Let \(\Phi_{\mathbf{Z}}(\theta)\) be test loss: \(\Phi_{\mathbf{Z}}(\theta)=\mathbb{E}_{Z=(X,Y)}\phi(f(\theta,X),Y)\). Let \(\Phi(\theta,Z_{n})\) be empirical loss on \(Z_{n}=\{(X_{i},Y_{i})|i=1,\cdots,n\}\): \(\Phi(\theta,Z_{n})=\frac{1}{n}\sum_{i=1}^{n}\phi(f(\theta,X_{i}),Y_{i})\). Then for a given training procedure \(P\), we have_

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta^{\sim}\mathbf{\Theta}_{P|Z_{n}}}\left[ \Phi_{\mathbf{Z}}(\Theta)-\Phi(\Theta,Z_{n})\right]\leq\inf_{\lambda>0}\left[ \gamma^{2}\psi(\lambda)\lambda\mathcal{D}_{P}+\frac{\mathcal{I}_{P}}{\lambda n }\right].\]

The theorem indicates that the upper bound of generalization gap depends on the three quantities, instability of two types and inconsistency, described above. As a sanity check, note that right after random initialization, generalization gap of the left-hand side is zero, and \(\mathcal{I}_{P}\) and \(\mathcal{S}_{P}\) in the right-hand side are also zero, which makes the right-hand side zero as \(\lambda\to 0\). Also note that this analysis is meant for stochastic training procedures. If \(P\) is a deterministic function of \(Z_{n}\) and not constant, the mutual information \(\mathcal{I}_{P}\) would become large while \(\mathcal{D}_{P}>0\), which would make the bound loose.

Intuitively, when training procedure \(P\) is more random, model parameter distributions are likely to be flatter (less concentrated) and less dependent on the sampling of training data \(Z_{n}\), and so \(\mathcal{I}_{P}\) is lower. However, high randomness would raise inconsistency of the model outputs \(\mathcal{C}_{P}\), which would raise \(\mathcal{D}_{P}\). Thus, the theorem indicates that there should be a trade-off with respect to the randomness of the training procedure.

The style of this general bound follows from the recent information theoretical generalization analyses of stochastic machine learning algorithms [38, 30, 28] that employ \(\mathcal{I}_{P}\) as a complexity measure, and such results generally hold in expectation. However, unlike the previous studies, we obtained a more refined Bernstein-style generalization result. Moreover, we explicitly incorporate inconsistency and instability of model outputs into our bound and show that smaller inconsistency and smaller instability lead to a smaller generalization bound on the right-hand side. In particular, if \(\mathcal{I}_{P}\) is relatively small so that we have \(\mathcal{I}_{P}\leq n\gamma^{2}\mathcal{D}_{P}\), then setting \(\lambda=\sqrt{\mathcal{I}_{P}/(n\gamma^{2}\mathcal{D}_{P})}\) and using \(\psi(\lambda)<1\) for \(\lambda\leq 1\), we obtain a simpler bound

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta^{\sim}\mathbf{\Theta}_{P|Z_{n}}}\left[ \Phi_{\mathbf{Z}}(\Theta)-\Phi(\Theta,Z_{n})\right]\leq 2\gamma\sqrt{\frac{ \mathcal{D}_{P}\mathcal{I}_{P}}{n}}.\]

Relation to _disagreement_It was empirically shown in [27, 17] that with models trained to zero training error, _disagreement_ (in terms of classification decision) of identically trained models is approximately equal to test error. When measured for the models that share training data, disagreement is closely related to inconsistency \(\mathcal{C}_{P}\) above, and it can be expressed as \(\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim\mathbf{\Theta}_{P|Z_{ n}}}\mathbb{E}_{X}\mathbb{I}\left[\left.c(\Theta,X)\neq c(\Theta^{\prime},X) \right.\right]\), where \(c(\theta,x)\) is classification decision \(c(\theta,x)=\arg\max_{i}f(\theta,x)[i]\) and \(\mathbb{I}\) is the indicator function \(\mathbb{I}[u]=1\) if \(u\) is true and 0 otherwise. In spite of the similarity, in fact, the behavior of disagreement and inconsistency \(\mathcal{C}_{P}\) can be quite different. Disagreement is equivalent to sharpening the prediction to a one-hot vector and then taking 1-norm of the difference: \(\mathbb{I}\left[\left.c(\theta,x)\neq c(\theta^{\prime},x)\right.\right]=\frac{1 }{2}\left|\mathrm{onehot}(f(\theta,x))-\mathrm{onehot}(f(\theta^{\prime},x)) \right\|_{1}\). This ultimate sharpening makes inconsistency and disagreement very different when the confidence-level of \(f(\theta,x)\) is low. This means that disagreement and inconsistency would behave more differently on more complex data (such as ImageNet) on which it is harder to train models to a state of high confidence. In Figure 10 (Appendix) we show an example of how different the behavior of disagreement and inconsistency can be, with ResNet-50 trained on 10% of ImageNet.

## 3 Empirical study

Our empirical study consists of three parts. As the bound depends not only \(\mathcal{D}_{P}(=\mathcal{C}_{P}+\mathcal{S}_{P})\) but also \(\mathcal{I}_{P}\), we first seek the condition under which \(\mathcal{D}_{P}\) is predictive of generalization gap (Section 3.1). Noting that, when the found condition is met, inconsistency \(\mathcal{C}_{P}\) alone is as predictive of generalization gap as \(\mathcal{D}_{P}\), Section 3.2 focuses on inconsistency and shows that inconsistency is predictive of generalization gap in a variety of realistic settings that use full-size training sets. Finally, Section 3.3 reports on the practical benefit of encouraging low inconsistency in algorithmic design. The details for reproduction are provided in Appendix C.

Remark: Predictiveness of \(\mathcal{D}_{P}\) is relativeNote that we are interested in how well the _change_ in \(\mathcal{D}_{P}\) matches the _change_ in generalization gap; thus, evaluation of the relation between \(\mathcal{D}_{P}\) and generalization gap always involves _multiple_ training procedures to observe the _changes/differences_. Given set \(S\) of training procedures, we informally say \(\mathcal{D}_{P}\)_is predictive of generalization gap for \(S\) if the relative smallness/largeness of \(\mathcal{D}_{P}\) of the procedures in \(S\) essentially coincides with the relative smallness/largeness of their generalization gap_.

On the predictiveness of \(\mathcal{D}_{P}\)= Inconsistency \(\mathcal{C}_{P}\)+ Instability \(\mathcal{S}_{P}\)

Inconsistency \(\mathcal{C}_{P}\) and instability \(\mathcal{S}_{P}\) were estimated as follows. For each training procedure \(P\) (identified by a combination of hyperparameters such as the learning rate and training length), we trained \(J\) models on each of \(K\) disjoint training sets. That is, \(K\times J\) models were trained with each procedure \(P\). The expectation values involved in \(\mathcal{C}_{P}\) and \(\mathcal{S}_{P}\) were estimated by taking the average; in particular, the expectation over data distribution \(\mathbf{Z}\) was estimated on the held-out unlabeled data disjoint from training data or test data. Disagreement was estimated similarly. \((K,J)\) was set to (4,8) for CIFAR-10/100 and (4,4) for ImageNet, and the size of each training set was set to 4K for CIFAR-10/100 and 120K (10%) for ImageNet.

#### 3.1.1 Results

We start with the experiments that varied the learning rate and the length of training fixing anything else to see whether the change of \(\mathcal{D}_{P}\) is predictive of the change of generalization gap caused by the change of these two hyperparameters. To avoid the complex effects of learning rate decay, we trained models with SGD with a constant learning rate (_constant SGD_ in short).

Constant SGD with iterate averaging (Fig 1,3)Although constant SGD could perform poorly by itself, constant SGD with _iterate averaging_ is known to be competitive [29; 16]. Figure 1 shows \(\mathcal{D}_{P}\) (\(x\)-axis) and generalization gap (\(y\)-axis) of the training procedures that performed constant SGD with iterate averaging. Iterate averaging was performed by taking the exponential moving average with momentum 0.999. Each point represents a procedure (identified by the combination of the learning rate and training length), and the procedures with the same learning rate are connected by a line in the increasing order of training length. A _positive correlation_ is observed between \(\mathcal{D}_{P}\) and generalization gap on all three datasets. Figure 3 plots generalization gap (left) and \(\mathcal{D}_{P}\) (right) on the \(y\)-axis and training loss on the \(x\)-axis for the same procedures as in Figure 1; observe that the up and down of \(\mathcal{D}_{P}\) is remarkably similar to the up and down of generalization gap. Also note that on CIFAR-10/100, sometimes generalization gap first goes up and then starts coming down towards the low-training-loss end of the lines (increase of training loss in some cases is due to the effects of weight decay). This '_turning around_' of generalization gap (from going up to going down) in Figure 3 shows up as the '_curling up_' of the upper-right end of the lines in Figure 1. It is interesting to see that \(\mathcal{D}_{P}\) and generalization gap are matching at such a detailed level.

near the end of training bounce around the local minimum, and the random noise in the gradient of the last mini-batch has a substantial influence on the final model parameter. The influence of this noise is amplified by larger learning rates. On the other hand, \(\mathcal{I}_{P}\) is likely to be lower with higher randomness since higher randomness should make model parameter distributions flatter (less concentrated) and less dependent on the sampling of training data \(Z_{n}\). This means that in this case (i.e., where the final randomness is high and varies a lot across the procedures), the interaction of \(\mathcal{I}_{P}\) and \(\mathcal{D}_{P}\) in the theoretical bound is complex, and \(\mathcal{D}_{P}\) alone could be substantially less predictive than what is indicated by the bound.

For \(\mathcal{D}_{P}\) to be predictive, final randomness should be equally low (Fig 5 (a)-(c))Therefore, we hypothesized and empirically confirmed that for \(\mathcal{D}_{P}\) to be predictive, the degrees of final randomness/uncertainty/noisiness should be equally low, which can be achieved with either a vanishing learning rate or iterate averaging. This means that, fortunately, \(\mathcal{D}_{P}\) is mostly predictive for high-performing procedures that matter in practice.

Figure 1: \(\mathcal{D}_{P}\) estimates (\(x\)-axis) and generalization gap (\(y\)-axis). SGD with a **constant learning rate** and **iterate averaging**. Only the learning rate and training length were varied. A positive correlation is observed.

Figure 4: Inconsistency and instability (\(x\)-axis) and generalization gap (\(y\)-axis). Same procedures and legend as in Fig 2. Instability is relatively unaffected by the learning rate. CIFAR-100. Similar results on CIFAR-10 and ImageNet (Figure 11 in the Appendix).

Figure 3: Up and down of generalization gap (\(y\)-axis; left) and \(\mathcal{D}_{P}\) (\(y\)-axis; right) as training proceeds. The \(x\)-axis is training loss. SGD with a constant learning rate with iterate averaging. The analyzed models and the legend are the same as in Fig 1. For each dataset, the left graph looks very similar to the right graph; up and down of \(\mathcal{D}_{P}\) as training proceeds is a good indicator of up and down of generalization gap.

Figure 2: \(\mathcal{D}_{P}\) estimates (\(x\)-axis) and generalization gap (\(y\)-axis). **Constant learning rates. No iterate averaging**. \(\mathcal{D}_{P}\) is predictive of generalization gap only for the procedures that share the learning rate.

in network architectures, mini-batch sizes, presence/absence of data augmentation, learning rate schedules (constant or vanishing), weight decay parameters, in addition to learning rates and training lengths. A positive correlation between \(\mathcal{D}_{P}\) and generalization gap is observed on all three datasets.

Inconsistency \(\mathcal{C}_{P}\) vs. \(\mathcal{D}_{P}\) (Fig 5 (d)-(f))Figure 5 (d)-(f) show that inconsistency \(\mathcal{C}_{P}\) is almost as predictive as \(\mathcal{D}_{P}\) when the condition of low randomness of the final states is satisfied. This is a practical advantage since unlike instability \(\mathcal{S}_{P}\), inconsistency \(\mathcal{C}_{P}\) does not require multiple training sets for estimation.

Disagreement (Fig 6,7)For the same procedures/models as in Figures 1 and 5, we show the relationship between disagreement and test error in Figures 6 and 7, respectively. We chose test error instead of generalization gap for the \(y\)-axis since the previous finding was 'Disagreement \(\approx\) Test error'. The straight lines are \(y=x\). The relation in Fig 6 (a) and Fig 7 (a) is close to equality, but the relation appears to be more complex in the others. The procedures plotted in Fig 7 are those which satisfy the condition for \(\mathcal{D}_{P}\) to be predictive of generalization gap; thus, the results indicate that the condition for disagreement to be predictive of test error is apparently different.

Note that while the previous empirical study of disagreement [27, 17] focused on the models with zero training error, we studied a wider range of models. Unlike CIFAR-10/100 (on which zero training error can be quickly achieved), a common practice for datasets like ImageNet is _budgeted training_[25] instead of training to zero error. Also note that zero training error does not necessarily lead to the best performance, which, for example, has been observed in the ImageNet experiments of this section.

### On the predictiveness of inconsistency \(\mathcal{C}_{P}\): from an algorithmic perspective

Based on the results above, this section focuses on inconsistency \(\mathcal{C}_{P}\), which enables experiments with larger training data, and studies the behavior of inconsistency in comparison with sharpness from an algorithmic perspective.

Figure 5: (a)–(c) \(\mathcal{D}_{P}\) (\(x\)-axis) and generalization gap (\(y\)-axis). (d)–(f) \(\mathcal{C}_{P}\) (\(x\)-axis) and generalization gap (\(y\)-axis). Both \(\mathcal{D}_{P}\) and \(\mathcal{C}_{P}\) are predictive of generalization gap for training procedures with iterate averaging or a vanishing learning rate (so that final randomness is low), irrespective of differences in the setting. In particular, the CIFAR-10 results include the training procedures that differ in network architectures, mini-batch sizes, data augmentation, weight decay parameters, learning rate schedules, learning rates and training lengths.

Training objectivesParallel to the fact that SAM seeks flatness of training loss landscape, a meta-algorithm _co-distillation_ (named by [1], and closely related to deep mutual learning [42], summarized in Algorithm 1 in the Appendix) encourages consistency of model outputs. It simultaneously trains two (or more) models with two (or more) different random sequences while penalizing the inconsistency between the predictions of the two models. This is typically described as 'teaching each other', but we take a different view of consistency encouragement. Note that the penalty term merely _'encourages'_ low inconsistency. Since inconsistency by definition depends on the unknown data distribution, it cannot be directly minimized by training. The situation is similar to minimizing loss on the training data with the hope that loss will be small on unseen data. We analyzed the models that were trained with one of the following training objectives (shown with the abbreviations used below):

* _'Standard'_: the standard cross-entropy loss.
* _'Consist.'_: the standard loss with encouragement of consistency.
* _'Flat.'_: the SAM objective, which encourages flatness.
* _'Consist+Flat'_: encouraging both consistency and flatness. Coupling two instances of SAM training with the inconsistency penalty term.

CasesWe experimented with ten combinations (_cases_) of dataset, network architecture, and training scenario. Within each case, we fixed the basic settings (e.g., weight decay, learning rate) to the ones known to perform well from the previous studies, and only varied the training objectives so that for each case we had at least four stochastic training procedures distinguished by the four training objectives (for some cases, we had more than four due to testing multiple values of \(\rho\) for SAM). We obtained four models (trained with four different random sequences) per training procedure. Tables 1 and 2 show the datasets and network architectures we used, respectively. Note that as a result of adopting high-performing settings, all the cases satisfy the condition of low final randomness.

#### 3.2.1 Results

As in the previous section, we quantify the generalization gap by the difference of test loss from training loss (note that the loss is the cross-entropy loss; the inconsistency penalty or anything else is not included); the inconsistency values were again measured on the held-out unseen _unlabeled_ data, disjoint from both training data and test data. Two types of sharpness values were measured, 1-sharpness (per-example loss difference in the adversarial direction) of [10] and the magnitude of the loss Hessian (measured by the largest eigenvalue), which represent the sharpness of training loss landscape and have been shown to correlate to generalization performance [10].

Inconsistency correlates to generalization gap (Figure 8)Figure 8 shows inconsistency or sharpness values (\(x\)-axis) and the generalization gap (\(y\)-axis). Each point in the figures represents a model; i.e., in this section, we show model-wise quantities (instead of procedure-wise), simulating the model selection setting. (Model-wise inconsistency for model \(\theta\) trained on \(Z_{n}\) with procedure \(P\) is \(\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P}|_{Z_{n}}}\mathbb{E}_{X}\mathrm{KL} (f(\Theta,X)||f(\theta,X))\).) All quantities are standardized so that the mean is zero and the standard deviation is 1. In the figures, we distinguish the four training objectives. In Figure 8 (b) and (c), we confirm, by comparing the 'Flat.' models (\(\times\)) with the baseline standard models (\(+\)), that encouragement of flatness (by SAM) indeed reduces sharpness (\(x\)-axis) as well as the generalization gap (\(y\)-axis). This serves as a sanity check of our setup.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline Name & \#class & \multicolumn{2}{c|}{\#train} & \multicolumn{1}{c|}{\#dev} & \multicolumn{1}{c|}{\#test} \\ \cline{3-5}  & \multicolumn{1}{c|}{provided} & \multicolumn{1}{c|}{used} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline ImageNet [7] & 1000 & 1.28M & 1.27M & 10K & 50K \\ Food101 [3] & 101 & 75750 & 70700 & 5050 & 25250 \\ Dogs [21] & 120 & 12000 & 10800 & 1200 & 8580 \\ Cars [23] & 196 & 8144 & 7144 & 1000 & 8041 \\ CIFAR-10 [24] & 10 & 50000 & 4000 & 5000 & 100000 \\ MNLI [36] & 3 & 392702 & 382902 & 9800 & 9815 \\ QNLI [36] & 2 & 104743 & 99243 & 5500 & 5463 \\ \hline \end{tabular} 
\begin{tabular}{|c|c|c|c|} \hline Network & \#param & Case \\ \hline ResNet-50 [12] & 24M & 1 \\ ViT-S/32 [9] & 23M & 2 \\ ViT-B/16 [9] & 86M & 3 \\ Mixer-B/16 [35] & 59M & 4 \\ WRN-2-2 [40] & 1.5M & 5 \\ EN-B0 [33] & 4.2M & 6,7 \\ roberta-base[26] & 124.6M & 8,9 \\ ResNet-18 [12] & 11M & 10 \\ \hline \end{tabular}
\end{table}
Table 1: Datasets. Mostly full-size natural images and 2 texts.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Name & \#class & \multicolumn{2}{c|}{\#train} & \multicolumn{1}{c|}{\#dev} & \multicolumn{1}{c|}{\#test} \\ \cline{3-5} Name & \multicolumn{1}{c|}{provided} & \multicolumn{1}{c|}{used} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline ImageNet [7] & 1000 & 1.28M & 1.27M & 10K & 50K \\ Food101 [3] & 101 & 75750 & 70700 & 5050 & 25250 \\ Dogs [21] & 120 & 120000 & 10800 & 1200 & 8580 \\ Cars [23] & 196 & 8144 & 7144 & 1000 & 8041 \\ CIFAR-10 [24] & 10 & 50000 & 4000 & 5000 & 100000 \\ MNLI [36] & 3 & 392702 & 382902 & 9800 & 9815 \\ QNLI [36] & 2 & 104743 & 99243 & 5500 & 5463 \\ \hline \end{tabular} 
\begin{tabular}{|c|c|c|c|} \hline Name & \#learn & Case \\ \hline ResNet-50 [12] & 24M & 1 \\ ViT-S/32 [9] & 23M & 2 \\ ViT-B/16 [9] & 86M & 3 \\ Mixer-B/16 [35] & 59M & 4 \\ WRN-2-2 [40] & 1.5M & 5 \\ EN-B0 [33] & 4.2M & 6,7 \\ roberta-base[26] & 124.6M & 8,9 \\ ResNet-18 [12] & 11M & 10 \\ \hline \end{tabular}
\end{table}
Table 2: Networks.

We observe in Figure 8 that inconsistency shows a clear positive correlation with the generalization gap, but the correlation of sharpness with generalization gap is less clear (e.g., in (b)-(c), the generalization gap of the 'Consist.' models (\(\triangle\)) is much smaller than that of the 'Flat.' models (\(\times\)), but their sharpness is larger). This is a general trend observed across multiple network architectures (ResNet, Transformers, MLP-Mixer, and so on), multiple datasets (images and texts), and multiple training scenarios (from scratch, fine-tuning, and distillation); the Appendix provides the figures for all 10 cases. Moreover, we found that while both sharpness reduction and inconsistency reduction lead to better generalization, in a number of cases inconsistency and generalization gap are reduced and yet sharpness remains relatively high. We view this phenomenon as an indication that _inconsistency has a stronger (and perhaps more essential) connection with generalization gap than sharpness_.

Inconsistency is more predictive of generalization gap than sharpness (Table 3)Motivated by the linearity observed in the figure above, we define a linear predictor of the generalization gap that takes the inconsistency or sharpness value as input. We measure the predictive power of the metrics through the prediction performance of this linear predictor trained with least square minimization. To make the inconsistency and sharpness values comparable, we standardize them and also generalization gap. For each case, we evaluated the metrics by performing the leave-one-out cross validation on the given set of models (i.e., perform least squares using all models but one and evaluate the obtained linear predictor on the left-out model; do this \(k\) times for \(k\) models and take the average). The results in Table 3 show that inconsistency outperforms sharpness, and the superiority still generally holds even when sharpness is given an 'unfair' advantage of additional _labeled_ data.

### Practical impact: algorithmic consequences

The results above suggest a strong connection of inconsistency of model outputs with generalization, which also suggests the importance of seeking consistency in the algorithmic design. To confirm this

\begin{table}
\begin{tabular}{|c|c c c c c c c c c|} \hline Case\# & I & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline Dataset & ImageNet & Food101 & C10 & Dog & Car & Mnli & Qli & Food \\ \hline Training scenario & \multicolumn{5}{c|}{From scratch} & \multicolumn{5}{c|}{Fine-tuning} & Distill. \\ \hline Inconsistency & _0.13_ & **0.10** & **0.17** & **0.27** & 0.15 & **0.09** & 0.19 & **0.15** & **0.18** & **0.19** \\
1-sharpness & 0.21 & 0.48 & 0.75 & 0.74 & 0.84 & 0.34 & 0.24 & 0.58 & 0.98 & 0.75 \\ Hessian & 0.50 & 1.00 & 0.77 & 0.72 & 0.78 & 0.70 & 0.68 & 0.59 & 0.87 & 0.59 \\ \hline \multicolumn{5}{c|}{Giving the sharpness metrics an ‘unfair’ advantage by providing additional _labels_:} \\ \hline
1-sharpness of dev. loss & 0.14 & 0.64 & 0.47 & 0.40 & 0.72 & 0.22 & _0.14_ & 0.36 & 0.37 & 0.62 \\ Hessian of dev. loss & 0.45 & 0.82 & 0.58 & 0.75 & _0.13_ & 0.27 & 0.15 & 0.91 & 0.81 & 0.89 \\ \hline \end{tabular}
\end{table}
Table 3: Generalization gap prediction error. The leave-one-out cross validation results, which average the generalization gap prediction residuals, are shown. See Table 2 for the networks used for each case. Smaller is better, and a value near 1.0 is very poor. Inconsistency: estimated on unlabeled data, 1-sharpness and Hessian: sharpness of training loss landscape. Inconsistency outperforms the sharpness metrics. Noting that inconsistency had access to additional data, even though _unlabeled_, we also show in the last 2 rows sharpness of test loss landscape estimated on the development data (held-out _labeled_ data), which gives the sharpness metrics an ‘unfair’ advantage by providing them with additional _labels_. With this advantage, the predictiveness of sharpness mostly improves; however, still, inconsistency is generally more predictive. The **bold** font indicates that the difference is statistically significant at 95% confidence level against all others including the last 2 rows. The _hatile_ font indicates the best but not statistically significant.

Figure 8: Inconsistency and sharpness (\(x\)-axis) and generalization gap (\(y\)-axis). Each point represents a model. Note that each graph plots 16–20 points, and some of them are overlapping.

point, we first experimented with two methods (ensemble and distillation) and found that in both, encouraging low inconsistency in every stage led to the best test error. Note that in this section we report test error instead of generalization gap in order to show the practical impact.

Ensemble (Fig 9 (a))Figure 9 (a) shows that, compared with non-ensemble models (\(+\)), ensemble models (averaging logits of two models) reduce both inconsistency (\(x\)-axis) and test error (\(y\)-axis). It is intuitive that averaging the logits would cancel out the dependency on (or particularities of) the individual random sequences inherent in each single model; thus, outputs of ensemble models are expected to become similar and so _consistent_ (as empirically observed). Moreover, we presume that reduction of inconsistency is the mechanism that enables ensemble to achieve better generalization. The best test error was achieved when the ensemble was made by two models trained with consistency encouragement (\(\bullet\) in Fig 9 (a)).

Distillation (Fig 9 (b))We experimented with distillation [13] with encouragement of consistency for the teacher model and/or the student model. Figure 9 (b) shows the test error (\(y\)-axis) and the inconsistency (\(x\)-axis). The standard model (\(+\)) serves as the baseline. The knowledge level of teachers increases from left to right, and within each graph, when/whether to encourage consistency differs from the point to point. The main finding here is that at all the knowledge levels of the teacher, both inconsistency and test error go down as the pursuit of consistency becomes more extensive; in each graph, the points for the _distilled_ models form a nearly straight line. (The line appears to shift away from the baseline (\(+\)) as the amount of external knowledge of the teacher increases. We conjecture this could be due to the change in \(\mathcal{I}_{P}\).) While distillation alone reduces inconsistency and test error, the best results are obtained by encouraging consistency at every stage of training (i.e., training both the teacher and student with consistency encouragement). The results underscore the importance of inconsistency reduction for better performance.

On the pursuit of the state of the art (Table 4,5)We show two examples of obtaining further improvement of state-of-the-art results by adding consistency encouragement. The first example is semi-supervised learning with CIFAR-10. Table 4 shows that the performance of a state-of-the-art semi-supervised method can be further improved by encouraging consistency between two training instances of this method on the unlabeled data (Algorithm 2 in the Appendix). The second example is transfer learning. We fine-tuned a public ImageNet-trained EfficientNet with a more focused dataset Food101. Table 5 shows that encouraging consistency between two instances of SAM training improved test error. These examples demonstrate the importance of consistency encouragement in the pursuit of the state of the art performance.

## 4 Limitations and discussion

Theorem 2.1 assumes a bounded loss \(\phi(f,y)\in[0,1]\) though the standard loss for classification is the cross-entropy loss, which is unbounded. This assumption can be removed by extending the theorem to a more complex analysis with other moderate assumptions. We, however, chose to present the simpler and so more intuitive analysis with a bounded loss. As noted above, this theorem is intended

Figure 9: Inconsistency and test error with ensemble and distillation. (a) Ensemble reduces inconsistency (\(x\)-axis) and test error (\(y\)-axis). \(+\) Non-ensemble (baseline), \(*\) Ensemble of standard models, \(\bullet\) Ensemble of the models trained with consistency encouragement. RN18 on Food101. (b) Test error (\(y\)-axis) and inconsistency of distilled models (\(x\)-axis). Without unlabeled data. \(+\) No distillation, \(\vartriangle\) Standard distillation, \(\blacktriangle\) Consistency encouragement for the student, \(\bullet\) Consistency encouragement for both the student and teacher. The best performance is obtained when consistency is encouraged for both the teacher and student. Food101. Student: RN18, Teacher: RN18 (left), RN50 (middle), and fine-tuning of ImageNet-trained ENB0 (right). In both (a) and (b), the average of 4 models is shown; see Appendix for the standard deviations.

[MISSING_PAGE_FAIL:10]

- mining discriminative components with random forests. In _European Conference on Computer Vision_, 2014.
* [4] Olivier Bousquet and Andre Elisseef. Stability and generalization. _Journal of Machine Learning Research_, 2:499-526, 2002.
* [5] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In _International Conference on Learning Representations (ICLR)_, 2022.
* [6] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. RandAugment: Practical automated data augmentation with a reduced search space. In _Advances in Neural Information Processing Systems 33 (NeurIPS 2020)_, 2020.
* [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2009.
* [8] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. _arXiv:1708.04552_, 2017.
* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations (ICLR)_, 2021.
* [10] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations (ICLR)_, 2021.
* [11] Xavier Gastaldi. Shake-shake regularization. _arXiv:1705.07485_, 2017.
* [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In _Proceedings of Deep Learning and Representation Learning Workshop: NIPS 2014_, 2014.
* [14] Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural Computation_, 9:1-42, 1997.
* [15] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2016.
* [16] Pavel Izmailov, Dmitri Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)_, 2018.
* [17] Yiding Jiang, Vaishnav Nagarajan, Christina Baek, and J. Zico Kolter. Assessing generalization of SGD via disagreement. In _International Conference on Learning Representations (ICLR)_, 2022.
* [18] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _International Conference on Learning Representations (ICLR)_, 2020.
* [19] Rie Johnson and Tong Zhang. Guided learning of nonconvex models through successive functional gradient optimization. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, 2020.
* [20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations (ICLR)_, 2017.
* [21] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In _First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011.
* [22] Andreas Kirsch and Yarin Gal. A note on "Assessing generalization of SGD via disagreement". _Transactions on Machine Learning Research_, 2022.

* [23] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In _4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)_, Sydney, Australia, 2013.
* [24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* [25] Mengtian Li, Ersin Yumer, and Deva Ramanan. Budgeted training: Rethinking deep neural network training under resource constraints. In _International Conference on Learning Representations (ICLR)_, 2020.
* [26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. _arXiv:1907.11692_, 2019.
* [27] Preetum Nakkiran and Yamini Bansal. Distributional generalization: A new kind of generalization. _arXiv:2009.08092_, 2020.
* [28] Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, and Daniel M Roy. Information-theoretic generalization bounds for stochastic gradient descent. In _Conference on Learning Theory_, pages 3526-3545. PMLR, 2021.
* [29] Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic gradient descent. In _Proceedings of Conference on Learning Theory (COLT)_, 2018.
* [30] Daniel Russo and James Zou. How much does your data exploration overfit? Controlling bias via information usage. _IEEE Transactions on Information Theory_, 66(1):302-323, 2019.
* [31] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. _Journal of Machine Learning Research_, 11:2635-2670, 2010.
* [32] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. FixMatch: Simplifying semi-supervised learning with consistency and confidence. In _Advances in Neural Information Processing Systems 33 (NeurIPS 2020)_, 2020.
* [33] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, 2019.
* [34] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In _Advances in Neural Information Processing Systems 30 (NIPS 2017)_, 2017.
* [35] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, and Alexey Dosovitskiy Mario Lucic. MLP-Mixer: An all-MLP architecture for vision. In _Advances in Neural Information Processing Systems 34 (NeurIPS 2021)_, 2021.
* [36] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _International Conference on Learning Representations (ICLR)_, 2019.
* [37] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training. In _International Conference on Learning Representations (ICLR)_, 2020.
* [38] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In _Advances in Neural Information Processing Systems 30 (NIPS 2017)_, 2017.
* [39] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2019.
* [40] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _Proceedings of the British Machine Vision Conference (BMVC)_, 2016.
* [41] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _International Conference on Learning Representations (ICLR)_, 2018.
* [42] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu. Deep mutual learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.

## Appendix A Proof of Theorem 2.1

As in the main paper, let \(Z=(X,Y)\) be a random variable representing a labeled data point (data point \(X\) and label \(Y\)) with distribution \(\mathbf{Z}\). Let \(Z_{n}=\{(X_{i},Y_{i}):i=1,2,\ldots,n\}\) be a random variable representing iid training data of size \(n\) drawn from \(\mathbf{Z}\).

We have the following lemma.

**Lemma A.1**.: _Given an arbitrary model parameter distribution \(\mathbf{\Theta}^{0}\), let_

\[\bar{f}_{P}(x)= \mathbb{E}_{Z_{n}^{\prime}}\bar{f}_{P|Z_{n}^{\prime}}(x)=\mathbb{ E}_{Z_{n}^{\prime}}\ \mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_{n}^{\prime}}}f(\varTheta,x),\] \[\ell_{\theta}(z)= \phi(f(\theta,x),y)-\phi(\bar{f}_{P}(x),y)\text{ where }z=(x,y)\]

_then_

\[-n\mathbb{E}_{Z_{n}}\ \mathbb{E}_{\mathbf{\Theta}\sim\mathbf{ \Theta}_{P|Z_{n}}}\ln\mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\leq\mathbb{ E}_{Z_{n}}\ \mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_{n}}}\sum_{i=1}^{n}\lambda \ell_{\Theta}(X_{i},Y_{i})+\mathbb{E}_{Z_{n}}\ \mathrm{KL}(\mathbf{\Theta}_{P|Z_{n}}|| \mathbf{\Theta}^{0}).\]

Proof.: Let \(\mathbf{\Theta}_{*}\) be a model parameter distribution such that

\[\mathbf{\Theta}_{*}\propto\mathbf{\Theta}^{0}\exp\left[\sum_{i=1}^{n}\left(- \lambda\ell_{\Theta}(X_{i},Y_{i})-\ln\mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}( Z))\right)\right].\]

We have

\[\mathbb{E}_{Z_{n}}\exp\left[\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{ \Theta}_{P|Z_{n}}}\sum_{i=1}^{n}\left(-\lambda\ell_{\Theta}(X_{i},Y_{i})-\ln \mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\right)-\mathrm{KL}(\mathbf{\Theta }_{P|Z_{n}}||\mathbf{\Theta}^{0})\right]\] \[\leq \mathbb{E}_{Z_{n}}\sup_{\mathbf{\Theta}}\exp\left[\mathbb{E}_{ \mathbf{\Theta}\sim\mathbf{\Theta}}\sum_{i=1}^{n}\left(-\lambda\ell_{\Theta}( X_{i},Y_{i})-\ln\mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\right)-\mathrm{KL}( \mathbf{\Theta}||\mathbf{\Theta}^{0})\right]\] \[= \mathbb{E}_{Z_{n}}\sup_{\mathbf{\Theta}}\mathbb{E}_{\mathbf{ \Theta}\sim\mathbf{\Theta}^{0}}\exp\left[\sum_{i=1}^{n}\left(-\lambda\ell_{ \Theta}(X_{i},Y_{i})-\ln\mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\right)- \mathrm{KL}(\mathbf{\Theta}||\mathbf{\Theta}_{*})\right]\] \[= \mathbb{E}_{Z_{n}}\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta} ^{0}}\exp\left[\sum_{i=1}^{n}\left(-\lambda\ell_{\Theta}(X_{i},Y_{i})-\ln \mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\right)\right]=1.\]

The first inequality takes \(\sup\) over all probability distributions of model parameters. The first equality can be verified using the definition of the KL divergence. The second equality follows from the fact that the supreme is attained by \(\mathbf{\Theta}=\mathbf{\Theta}_{*}\). The last equality uses the fact that \((X_{i},Y_{i})\) for \(i=1,\ldots,n\) are iid samples drawn from \(\mathbf{Z}\). The desired bound follows from Jensen's inequality and the convexity of \(\exp(\cdot)\). 

Proof of Theorem 2.1.: Using the notation of Lemma A.1, we have

\[\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_{n}}}\ln \mathbb{E}_{Z}\exp(-\lambda\ell_{\Theta}(Z))\leq\mathbb{E}_{\mathbf{\Theta} \sim\mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{Z}\left[\exp(-\lambda\ell_{\Theta}(Z )-1]\right.\] \[\leq-\lambda\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_ {n}}}\mathbb{E}_{Z}\ell_{\Theta}(Z)+\psi(\lambda)\lambda^{2}\mathbb{E}_{\mathbf{ \Theta}\sim\mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{Z}\ell_{\Theta}(Z)^{2}\] \[\leq-\lambda\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_ {n}}}\mathbb{E}_{Z}\ell_{\Theta}(Z)+\frac{\gamma^{2}}{4}\psi(\lambda)\lambda^{2 }\mathbb{E}_{\mathbf{\Theta}\sim\mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{X}\|f( \varTheta,X)-\bar{f}_{P}(X)\|_{1}^{2}.\] (1)

The first inequality uses \(\ln u\leq u-1\). The second inequality uses the fact that \(\psi(\lambda)\) is increasing in \(\lambda\) and \(-\lambda\ell_{\theta}(z)\leq\lambda\). The third inequality uses the Lipschitz assumption of the loss function.

We also have the following from the triangle inequality of norms, Jensen's inequality, the relationship between the 1-norm and the total variation distance of distributions, and Pinsker's inequality.

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P|Z_{n}}} \mathbb{E}_{X}\|f(\Theta,X)-\bar{f}_{P}(X)\|_{1}^{2}\] \[\leq 2\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P|Z_{n} }}\mathbb{E}_{X}\left[\|f(\Theta,X)-\bar{f}_{P|Z_{n}}(X)\|_{1}^{2}+\|\bar{f}_{ P|Z_{n}}(X)-\bar{f}_{P}(X)\|_{1}^{2}\right]\] \[\leq 2\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim \mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{X}\|f(\Theta,X)-f(\Theta^{\prime},X)\|_{ 1}^{2}+2\mathbb{E}_{Z_{n}}\mathbb{E}_{X}\|\bar{f}_{P|Z_{n}}(X)-\bar{f}_{P}(X) \|_{1}^{2}\] \[\leq 4\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim \mathbf{\Theta}_{P|Z_{n}}}\mathbb{E}_{X}\mathrm{KL}(f(\Theta,X)\|f(\Theta^{ \prime},X))+2\mathbb{E}_{Z_{n}}\mathbb{E}_{X}\|\bar{f}_{P|Z_{n}}(X)-\bar{f}_{P }(X)\|_{1}^{2}\] \[= 4\mathcal{C}_{P}+2\mathbb{E}_{Z_{n}}\mathbb{E}_{X}\|\bar{f}_{P|Z _{n}}(X)-\bar{f}_{P}(X)\|_{1}^{2}\leq 4\mathcal{C}_{P}+2\mathbb{E}_{Z_{n}} \mathbb{E}_{Z_{n}}\mathbb{E}_{X}\|\bar{f}_{P|Z_{n}}(X)-\bar{f}_{P|Z_{n}^{ \prime}}(X)\|_{1}^{2}\] \[\leq 4\mathcal{C}_{P}+4\mathbb{E}_{Z_{n}}\mathbb{E}_{Z_{n}^{\prime}} \mathbb{E}_{X}\mathrm{KL}(\bar{f}_{P|Z_{n}}(X)\|\bar{f}_{P|Z_{n}^{\prime}}(X) )=4\left(\mathcal{C}_{P}+\mathcal{S}_{P}\right)=4\mathcal{D}_{P}.\] (2)

Using (1), (2), and Lemma A.1, we obtain

\[\lambda\mathbb{E}_{Z_{n}}\,\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P|Z_{n}}} \mathbb{E}_{Z}\left[n\ell_{\Theta}(Z)-\sum_{i=1}^{n}\ell_{\Theta}(X_{i},Y_{i} )\right]\leq n\gamma^{2}\psi(\lambda)\lambda^{2}\,\mathcal{D}_{P}+\mathbb{E}_ {Z_{n}}\ \mathrm{KL}(\mathbf{\Theta}_{P|Z_{n}}\|\mathbf{\Theta}^{0}).\] (3)

Set \(\mathbf{\Theta}^{0}=\mathbb{E}_{Z_{n}^{\prime}}\mathbf{\Theta}_{P|Z_{n}^{ \prime}}\) so that we have

\[\mathcal{I}_{P}=\mathbb{E}_{Z_{n}}\ \mathrm{KL}(\mathbf{\Theta}_{P|Z_{n}}\| \mathbf{\Theta}^{0}).\] (4)

Also note that \(\bar{f}_{P}\) in \(\ell_{\Theta}\) cancels out since \(Z_{n}\) is iid samples of \(\mathbf{Z}\), and so using the notation for the test loss and empirical loss defined in Theorem 2.1, we have

\[\lambda\mathbb{E}_{Z_{n}}\,\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P|Z_{n}}} \mathbb{E}_{Z}\left[n\ell_{\Theta}(Z)-\sum_{i=1}^{n}\ell_{\Theta}(X_{i},Y_{i} )\right]=n\lambda\mathbb{E}_{Z_{n}}\,\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P| Z_{n}}}\left[\Phi_{\mathbf{Z}}(\Theta)-\Phi(\Theta,Z_{n})\right].\] (5)

(3), (4) and (5) imply the result. 

## Appendix B Additional figures

Figure 10 supplements 'Relation to _disagreement_' at the end of Section 2. It shows an example where the behavior of inconsistency is different from disagreement. Training was done on 10% of ImageNet with the _seed procedure_ (tuned to perform well) with training length variations described in Table 6 below. Essentially, in this example, inconsistency goes up like generalization gap, and disagreement goes down like test error and goes up in the end, as training becomes longer.

Figure 11 supplements Figure 4 in Section 3.1. It shows inconsistency and instability of model outputs of the models trained with SGD with constant learning rates without iterate averaging. In this setting of high final randomness, larger learning rates make inconsistency larger while instability is mostly unaffected. By contrast, Figure 12 shows that when final randomness is low (due to iterate averaging in this case), both inconsistency and instability are predictive of generalization gap both within and across the learning rates.

Figure 13-15 supplement Figure 8 in Section 3.2. These figures show the relation of inconsistency and sharpness to generalization gap. Note that each graph has at least 16 points, and some of them (typically for the models trained with the same procedure) are overlapping. Inconsistency shows a stronger correlation with generalization gap than sharpness does.

1. _Choose a network architecture and the size of training set_. First, for each dataset, we chose a network architecture and the size of the training set. The training set was required to be smaller than the official set so that disjoint training sets can be obtained for estimating instability. For the network architecture, we chose relatively small residual nets (WRN-28-2 for CIFAR-10/100 and ResNet-50 for ImageNet) to reduce the computational burden.
2. _Choose a seed procedure_. Next, for each dataset, we chose a procedure that performs reasonably well with the chosen network architecture and the size of training data, and we call this procedure a _seed procedure_. This was done by referring to the previous studies [32, 10] and performing some tuning on the development data considering that the training data is smaller than in [32, 10]. This step was for making sure to include high-performing (and so practically interesting) procedures in our empirical study.
3. _Make core procedures from the seed procedure_. For each dataset, we made _core procedures_ from the seed procedure by varying the learning rate, training length, and the presence/absence of iterate averaging. Table 6 shows the resulting core procedures.
4. _Diversify by changing an attribute_. To make the procedures more diverse, for each dataset, we generated additional procedures by changing _one_ attribute of the core procedure. This was done for all the pairs of the core procedures in Table 6 and the attributes in Table 7.

Figure 11: Inconsistency \(\mathcal{C}_{P}\) (left) and instability \(\mathcal{S}_{P}\) (right) (\(x\)-axis) and generalization gap (\(y\)-axis). Supplement to Figure 4 in Section 3.1. SGD with a constant learning rate. **No iterate averaging**, and therefore, high randomness in the final state. Same procedures (and models) as in Figure 2. A larger learning rate makes inconsistency larger, but instability is mostly unaffected.

Figure 12: Inconsistency \(\mathcal{C}_{P}\) (left) and instability \(\mathcal{S}_{P}\) (right) (\(x\)-axis) and generalization gap (\(y\)-axis). SGD with a constant learning rate **with iterate averaging**, and therefore, low randomness in the final state. Same procedures (and models) as in Figure 1. Both inconsistency and instability are predictive of generalization gap across the learning rates.

Figure 10: Inconsistency \(\mathcal{C}_{P}\) and disagreement (\(y\)-axis) in comparison with generalization gap and test error (\(y\)-axis). The \(x\)-axis is train loss. The arrows indicate the direction of training becoming longer. Each point is the average of 16 instances. Training was done on 10% of ImageNet with the _seed procedure_ (tuned to perform well) with training length variations; see Table 6. In this example, essentially, inconsistency goes up like generalization gap, and disagreement goes down like test error and goes up in the end, as training becomes longer.

Figure 14: Supplement to Figure 8 in Section 3.2. 1-sharpness (\(x\)-axis) and generalization gap (\(y\)-axis) for all the 10 cases. All values are standardized. Same legend as in Figure 13.

Figure 13: Supplement to Figure 8 in Section 3.2. Inconsistency (\(x\)-axis) and generalization gap (\(y\)-axis) for all the 10 cases. All values are standardized so that the average is 0 and the standard deviation is 1.

Figure 15: Supplement to Figure 8 in Section 3.2. Hessian (\(x\)-axis) and generalization gap (\(y\)-axis) for all the 10 cases. All values are standardized except that for the \(x\)-axis of Case#8 and 9, non-standardized values are shown in the log-scale for better readability. Same legend as in Figure 13.

Note that after training, a few procedures with very high training loss were excluded from the analysis (see Table 6 for the cut-off). Right after the model parameter initialization, inconsistency \(\mathcal{C}_{P}\) is obviously not predictive of generalization gap since it is non-zero merely reflecting the initial randomness while generalization gap is zero. Similar effects of initial randomness are expected in the initial phase of training; however, these near random models are not of practical interest. Therefore, we excluded from our analysis.

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & CIFAR-10/100 & ImageNet \\ \hline Network & WRN-28-2 & ResNet-50 \\ Training data size & 4K & 120K \\ Learning rate & \{0.005, 0.01, 0.025\({}^{*}\), 0.05\(\}\) & \{1/64, 1/32, 1/16\({}^{*}\), 1/8\(\}\) \\ Weight decay & 2e-3 & 1e-3 \\ Schedule & Constant & Constant \\ Iterate averaging & \{EMA\({}^{*}\), None\} & \{EMA\({}^{*}\), None\} \\ Epochs & \{ 25,..., 1000\({}^{*}\),..., 2000 \}\(\dagger\ddagger\) & \{10, 20,..., 200\({}^{*}\)\}\(\ddagger\) \\ Mini-batch size & 64 & 512 \\ Data augmentation & Standard+Cutout & Standard \\ Label smoothing & – & 0.1 \\ \hline \end{tabular}
\end{table}
Table 6: Core training procedures of the experiments in Section 3.1. The core training procedures consist of the exhaustive combinations of these attributes. The seed procedure attributes are indicated by \({}^{*}\) when there are multiple values. The optimizer was fixed to SGD with Resetver momentum 0.9.

\(\dagger\) More precisely, { 25, 50,..., 250, 300,..., 500, 600,..., 1000, 1200,..., 2000 } so that the interval gradually increased from 25 to 200. \(\ddagger\) After training, a few procedures with very high training loss were excluded from the analysis. The cut-off was 0.3 (CIFAR-10), 2.0 (CIFAR-100), and 3.0 (ImageNet), reflecting the number of classes (10, 100, and 1000).

The choice of the constant scheduling is for starting the empirical study with simpler cases by avoiding the complex effects of decaying learning rates, as mentioned in the main paper; also, we found that in these settings, constant learning rates rival the cosine scheduling as long as iterate averaging is performed.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & CIFAR-10 & CIFAR-100 & ImageNet \\ \hline Network & WRN-16-4 & – & – \\ Weight decay & 5e-4 & – & 1e-4 \\ Schedule & Cosine & Cosine & Cosine \\ Mini-batch size & 256 & 256 & – \\ Data augmentation & None & – & – \\ \hline \end{tabular}
\end{table}
Table 7: Attributes that were varied for making variations of core procedures. Only one of the attributes was varied at a time.

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & CIFAR-10 & CIFAR-100 & ImageNet \\ \hline Network & WRN-16-4 & – & – \\ Weight decay & 5e-4 & – & 1e-4 \\ Schedule & Cosine & Cosine & Cosine \\ Mini-batch size & 256 & 256 & – \\ Data augmentation & None & – & – \\ \hline \end{tabular}
\end{table}
Table 8: The values of the fixed attributes of the procedures shown in Figures 1–4 and 6 as well as 11–12. The training length and the learning rate were varied as shown in Table 6. The presence/absence of iterate averaging is indicated in each figure.

\begin{table}
\begin{tabular}{|c|c c|} \hline  & CIFAR-10 & CIFAR-100 & ImageNet \\ \hline Network & WRN-28-2 & ResNet-50 \\ Training data size & 4K & 120K \\ Weight decay & 2e-3 & 1e-3 \\ Schedule & Constant & Constant \\ Mini-batch size & 256 & 64 & 512 \\ Data augmentation & Standard+Cutout & Standard \\ Label smoothing & – & 0.1 \\ \hline \end{tabular}
\end{table}
Table 9: The values of the fixed attributes of the procedures shown in Figures 16–19. The training length and the learning rate were varied as shown in Table 6. The presence/absence of iterate averaging is indicated in each figure. The rest of the attributes are the same as in Table 8

#### c.1.1 SGD with constant learning rates (Figures 1-4 and 6)

In Section 3.1, we first focused on the effects of varying learning rates and training lengths while fixing anything else, with the training procedures that use a constant learning rate with or without iterate averaging. We analyzed all the subsets of the procedures that met this condition, and reported the common trend. That is, when the learning rate is constant, with iterate averaging, \(\mathcal{D}_{P}\) is predictive of generalization gap within and across the learning rates, and without iterate averaging, \(\mathcal{D}_{P}\) is predictive of generalization gap only for the procedures that share the learning rate; moreover, without iterate averaging, larger learning rates cause \(\mathcal{D}_{P}\) to overestimate generalization gap by larger amounts. Figures 1-4 and 6 show one particular subset for each dataset, and Table 8 shows the values of the attributes fixed in these subsets. To demonstrate the generality of the finding, we show the corresponding figures for one more subset for each dataset in Figures 16-19. The values of the fixed attributes in these subsets are shown in Table 9.

#### c.1.2 Procedures with low final randomness (Figures 5 and 7)

The procedures shown in Figures 5 and 7 are subsets (three subsets for three datasets) of all the procedures (the core procedures in Table 6 times the attribute changes in Table 7). The subsets consist of the procedures with either iterate averaging or a vanishing learning rate (i.e., going to zero) so

Figure 19: Disagreement (\(x\)-axis) and test error (\(y\)-axis). Same models and legend as in Fig 16.

Figure 17: \(\mathcal{D}_{P}\) (\(x\)-axis) and generalization gap (\(y\)-axis). **Constant learning rates. No iterate averaging.** Only the learning rate and training length were varied as in Figure 2 and the attributes were fixed to the values different from Figure 2; see Table 9 for the fixed values. As in Figure 2, \(\mathcal{D}_{P}\) is predictive of generalization gap for the procedures that share the learning rate, but not clear otherwise.

Figure 18: Inconsistency \(\mathcal{C}_{P}\) (left) and instability \(\mathcal{S}_{P}\) (right) (\(x\)-axis) and generalization gap (\(y\)-axis). Same procedures (and models) as in Figure 17 (**no iterate averaging**). As in Figures 4 and 11 (also no iterate averaging), a larger learning rate makes inconsistency larger, but instability is mostly unaffected.

that they meet the condition of low final randomness. These subsets include those with the cosine learning rate schedule. With the cosine schedule, we were interested in not only letting the learning rate go to zero but also stopping the training before the learning rate reaches zero and setting the final model to be the iterate averaging (EMA) at that point, which is well known to be useful (e.g., [32]). Therefore, we trained the models with the cosine schedule for { 250, 500, 1000, 2000 } epochs (CIFAR-10/100) or 200 epochs (ImageNet), and saved the iterate averaging of the models with the interval of one tenth (CIFAR-10/100) or one twentieth (ImageNet) of the entire epochs.

### Details of the experiments in Section 3.2

Section 3.2 studied inconsistency in comparison with sharpness in the settings where these two quantities are reduced by algorithms. The training algorithm with consistency encouragement (co-distillation) is summarized in Algorithm 1. These experiments were designed to study

* practical models trained on full-size training data, and
* diverse models resulting from diverse training settings,

in the situation where algorithms are compared after basic tuning is done, rather than the hyperparameter tuning-like situation in Section 3.1.

#### c.2.1 Training of the models

This section describes the experiments for producing the models used in Section 3.2.

Basic settingsWithin each of the 10 cases, we used the same basic setting for all, and these shared basic settings were adopted/adapted from the previous studies when possible. Tables 10 and 11

\begin{table}
\begin{tabular}{|l|c c|c|c|c|c|} \hline Training type & \multicolumn{4}{c|}{From scratch} & Fine-tuning & Distillation \\ \hline Dataset & \multicolumn{2}{c|}{ImageNet} & \multicolumn{2}{c|}{Food101} & CIFAR10 & Cars / Dogs & Food101 \\ Network & ResNet50 & ViT & ViT / Mixer & WRN28-2 & EN-B0 & ResNet-18 \\ \hline Batch size & 512 & 4096 & 512 & 64 & 256 & 512 \\ Epochs & 100 & 300 & 200 / 100 & – & – & 400 \\ Update steps & – & – & – & 500K & 4K / 2K & – \\ Warmup steps & 0 & 10K & 0 & 0 & 0 & 0 \\ Learning rate & 0.125 & 3e-3 & 3e-3 & 0.03 & 0.1 & 0.125 \\ Schedule & \multicolumn{2}{c|}{Cosine} & \multicolumn{2}{c|}{Linear/Cosine} & \multicolumn{2}{c|}{Cosine} & \multicolumn{2}{c|}{Constant} & \multicolumn{2}{c}{Cosine} \\ Optimizer & Momentum & AdamW & AdamW & Momentum & Momentum & Momentum \\ Weight decay & 1e-4 & 0.3 & 0.3 & 5e-4 & 1e-5 & 1e-3 \\ Label smooth & 0.1 & 0 & 0.1 & 0 & 0 & 0 \\ Iterate averaging & – & – & – & EMA & EMA & – \\ Gradient clipping & – & 1.0 & 1.0 & – & 20.0 & – \\ \hline Data augment & \multicolumn{4}{c|}{Standard} & Cutout & \multicolumn{2}{c|}{Standard} \\ \hline Reference & [10] & [5] & [5] & [10],[32] & [10],[33] & [10] \\ \hline Case\# & 1 & 2 & 3 / 4 & 5 & 6 / 7 & 10 \\ \hline \multicolumn{6}{l}{ ‘Momentum’: SGD with Nesterov momentum 0.9.} \\ \end{tabular}
\end{table}
Table 11: Basic settings shared by all the models for each case (Case#8–9; text). Hyperparameters for Case#8–9 (text) basically followed the RoBERTa paper [26]. The learning rate schedule was equivalent to early stopping of 10-epoch linear schedule after 4 epochs. Although it appears that [26] tuned when to stop for each run, we used the same number of epochs for all. Iterate averaging is our addition, which consistently improved performance.

\begin{table}
\begin{tabular}{|l|c|} \hline Initial learning rate \(\eta_{0}\) & 1e-5 \\ Learning rate schedule & Linear from \(\eta_{0}\) to \(0.6\eta_{0}\) \\ Epochs & 4 \\ Batch size & 32 \\ Optimizer & AdamW (\(\beta_{1}\)=0.9, \(\beta_{2}\)=0.98, \(\epsilon\)=1e-6) \\ Weight decay & 0.1 \\ Iterate averaging & EMA with momentum 0.999 \\ \hline \end{tabular}
\end{table}
Table 10: Basic settings shared by all the models for each case (Case#1–7,10; images)describe the basic settings and the previous studies that were referred to. Some changes to the previous settings were made for efficiency in our computing environment (no TPU); e.g., for Case#1, we changed the batch size from 4096 to 512 and accordingly the learning rate from 1 to 0.125. When adapting the previous settings to new datasets, minimal tuning was done for obtaining reasonable performance, e.g., for Cases#3 and 4, we changed batch size from 4096 to 512 and kept the learning rate without change as it performed better.

For CIFAR-10, following [32], we let the learning rate decay to \(0.2\eta_{0}\) instead of 0 and set the final model to the EMA of the models with momentum 0.999. For Cases#6-7 (fine-tuning), we used a constant learning rate and used the EMA of the models with momentum 0.999 as the final model, which we found produced reasonable performance with faster training. Cases#6-7 fine-tuned the publicly available EfficientNet-B02 pretrained with ImageNet by [33]. The dropout rate was set to 0.1 for Case#3, and the stochastic depth drop rate was set to 0.1 for Case#4. The teacher models for Case#10 (distillation) were ensembles of ResNet-18 trained with label smoothing 0.1 for 200 epochs with the same basic setting as the student models (Table 10) otherwise.

Footnote 2: https://github.com/google-research/sam

For CIFAR-10, the standard data augmentation (shift and horizontal flip) and Cutout [8] were applied. For the other image datasets, only the standard data augmentation (random crop with distortion and random horizontal flip) was applied; the resolution was 224\(\times\)224.

Hyperparameters for SAMThere are two values that affect the performance of SAM, \(m\) for \(m\)-sharpness and the diameter of the neighborhood \(\rho\). Their values are shown in Table 12. [10] found that smaller \(m\) performs better. However, a smaller \(m\) can be less efficient as it can reduce the degree of parallelism, depending on the hardware configuration. We made \(m\) no greater than the reference study in most cases, but for practical feasibility we made it larger for Case#2. \(\rho\) was either set according to the reference when possible or chosen on the development data otherwise, from {0.05, 0.1, 0.2} for images and from {0.002, 0.005, 0.01, 0.02, 0.05, 0.1} for texts. For some cases (typically those with less computational burden), we trained the models for one additional value of \(\rho\) to have more data points.

Hyperparameters for the inconsistency penalty termThe weight of the inconsistency penalty term for encouraging consistency was fixed to 1.

Number of the modelsFor each training procedure (identified by the training objective within each case), we obtained 4 models trained with 4 distinct random sequences. Cases#1-9 consisted of either 4 or 6 procedures depending on the number of the values chosen for \(\rho\) for SAM. Case#10 (distillation) consisted of 6 procedures3, resulting from combining the choice of the training objectives for the teacher and the choice for the student. In total, we had 52 procedures and 208 models.

Footnote 3: Although the number of all possible combinations is 16, considering the balance with other cases, we chose to experiment with the following: teacher { ‘Standard’, ‘Consist.’, ‘Consist+Flat’} \(\times\) student { ‘Standard’, ‘Consist.’}

#### c.2.2 Estimation of the model-wise inconsistency and sharpness in Section 3.2

When the expectation over the training set was estimated, for a large training set such as ImageNet, 20K data points were sampled for this purpose. As described above, we had 4 models for each of the training procedures. For the procedure _without_ encouragement of low inconsistency, the expectation of the divergence of each model was estimated by taking the average of the divergence from the three other models. As for the procedure _with_ encouragement of low inconsistency, the four models were obtained from two runs as each run produced two models, and so when averaging for estimating inconsistency, we excluded the divergence between the models from the same run due to their dependency.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline Case\# & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8.9 & 10 \\ \hline \(m\)-sharpness & 128 & 256 & 32 & 32 & 32 & 16 & 16 & 2 & 128 \\ \(\rho\) & 0.05,0.1 & 0.05 & 0.1 & 0.1 & 0.1,0.2 & 0.1,0.2 & 0.1 & 0.005,0.01 & 0.1 \\ \hline \end{tabular}
\end{table}
Table 12: Hyperparameters for SAM.

1-sharpness requires \(\rho\) (the diameter of the local region) as input. We set it to the best value for SAM.

### Details of the experiments in Sections 3.3

The optimizer was SGD with Nesterov momentum 0.9.

#### c.3.1 Ensemble experiments (Figure 9 (a))

The models used in the ensemble experiments were ResNet-18 trained for 200 epochs with label smoothing 0.1 with the basic setting of Case#10 in Table 10 otherwise. Table 13 shows the standard deviation of the values presented in Figure 9 (a). The ensembles also served as the teachers in the distillation experiments.

#### c.3.2 Distillation experiments (Figure 9 (b))

The student models were ResNet-18 trained for 200 epochs with the basic setting of Case#10 of Table 10 otherwise. The teachers for Figure 9(b) (left) were ResNet-18 ensemble models trained as described in C.3.1. The teachers for Figure 9(b) (middle) were ResNet-50 ensemble models trained similarly. The teachers for Figure 9(b) (right) were EfficientNet-B0 models obtained by fine-tuning the public ImageNet-trained model (footnote 2); fine-tuning was done with encouragement of consistency and flatness (with \(\rho\)=0.1) with batch size 512, weight decay 1e-5, the initial learning rate 0.1 with cospin scheduling, gradient clipping 20, and 20K updates. Table 14 shows the standard deviations of the values plotted in Figure 9 (b).

\begin{table}
\begin{tabular}{|c l|c|c|} \hline  & Training method & Test error(\%) & inconsistency \\ \hline \(+\) & Non-ensemble & 17.09\(\pm_{0.20}\) & 0.30\(\pm_{0.001}\) \\ \hline \(\star\) & Ensemble of standard models & 14.99\(\pm_{0.05}\) & 0.14\(\pm_{0.001}\) \\ \hline \(\bullet\) & Ensembles of Consist. models & 14.07\(\pm_{0.08}\) & 0.10\(\pm_{0.001}\) \\ \hline \end{tabular}
\end{table}
Table 13: Supplement to Figure 9 (a). Error rate (%) and inconsistency of ensembles in comparison with non-ensemble models (\(+\)). Food101, ResNet-18. The average and standard deviation of 4 are shown. Ensemble reduces test error and inconsistency.

\begin{table}
\begin{tabular}{|c|l|l|l|l|l|} \hline  & \multicolumn{3}{c|}{Test error (\%)} & \multicolumn{3}{c|}{inconsistency} \\ \cline{2-5}  & left & middle & right & left & middle & right \\ \hline \(+\) & \multicolumn{3}{c|}{17.09\(\pm_{0.20}\)} & \multicolumn{3}{c|}{0.30\(\pm_{0.001}\)} \\ \hline \(\triangle\) & 15.74\(\pm_{0.09}\) & 14.95\(\pm_{0.17}\) & 14.61\(\pm_{0.14}\) & 0.19\(\pm_{0.001}\) & 0.21\(\pm_{0.001}\) & 0.22\(\pm_{0.002}\) \\ \hline \(\blacktriangle\) & 14.99\(\pm_{0.07}\) & 14.28\(\pm_{0.11}\) & 13.89\(\pm_{0.08}\) & 0.14\(\pm_{0.001}\) & 0.15\(\pm_{0.001}\) & 0.14\(\pm_{0.002}\) \\ \hline \(\bullet\) & 14.41\(\pm_{0.05}\) & 13.63\(\pm_{0.09}\) & 13.06\(\pm_{0.06}\) & 0.10\(\pm_{0.001}\) & 0.12\(\pm_{0.001}\) & 0.08\(\pm_{0.001}\) \\ \hline \end{tabular}
\end{table}
Table 14: Supplement to Figure 9 (b). Test error (%) and inconsistency of distilled-models in comparison with standard models (\(+\)). The average and standard deviation of 4 are shown.

```
1Initialize \(\theta^{\text{a}}\), \(\theta^{\text{b}}\), \(\bar{\theta}^{\text{a}}\), and \(\bar{\theta}^{\text{b}}\).
2for\(t=1,\ldots,T\)do
3 Sample labeled mini-batches \(B^{\text{a}}\) and \(B^{\text{b}}\) from \(Z_{n}\), and sample an unlabeled mini-batch \(B_{U}\) from \(U\).
4 Let \(\psi(\theta,\bar{\theta},x)=\beta\mathbb{I}(\max_{i}f(\bar{\theta};x)[i]>\tau) \mathrm{KL}(f(\bar{\theta},x)||f(\theta,x))\) where \(\mathbb{I}\) is the indicator function.
5\(\theta^{\text{a}}\leftarrow\theta^{\text{a}}-\eta_{t}\nabla_{\theta^{\text{a}}} \left[\frac{1}{|B^{\text{a}}|}\sum_{(x,y)\in B^{\text{a}}}\phi(f(\theta^{ \text{a}},x),y)\ +\frac{1}{|B_{U}|}\sum_{x\in B_{U}}\psi(\theta^{\text{a}},\bar{ \theta}^{\text{b}},x)\right]\)
6\(\theta^{\text{b}}\leftarrow\theta^{\text{b}}-\eta_{t}\nabla_{\theta^{\text{b}}} \left[\frac{1}{|B^{\text{b}}|}\sum_{(x,y)\in B^{\text{b}}}\phi(f(\theta^{ \text{b}},x),y)+\frac{1}{|B_{U}|}\sum_{x\in B_{U}}\psi(\theta^{\text{b}},\bar {\theta}^{\text{a}},x)\right]\)
7\(\bar{\theta}^{\text{a}}\) and \(\bar{\theta}^{\text{b}}\) keep the EMA of \(\theta^{\text{a}}\) and \(\theta^{\text{b}}\), with momentum \(\mu\), respectively. ```

**Algorithm 2**Our semi-supervised variant of co-distillation.

#### c.3.3 Semi-supervised experiments reported in Table 4

The unlabeled data experiments reported in Table 4 used our modification of Algorithm 1, taylored for use of unlabeled data, and it is summarized in Algorithm 2. It differs from Algorithm 1 in two ways. First, to compute the inconsistency penalty term, the model output is compared with that of the exponential moving average (EMA) of the other model, reminiscent of Mean Teacher [34]. The output of the EMA model during the training typically has higher confidence (or lower entropy) than the model itself, and so taking the KL divergence against EMA of the other model serves the purpose of sharpening the pseudo labels and reducing the entropy on unlabeled data. Second, we adopted the masked divergence from _Unsupervised Data Augmentation (UDA)_[37], which masks (i.e., ignores) the unlabeled instances for which the confidence level of the other model is less than threshold. These changes are effective in the semi-supervised setting (but not in the supervised setting) for preventing the models from getting stuck in a high-entropy region.

In the experiments reported in Table 4, we penalized inconsistency between the model outputs of two training instances of _Unsupervised Data Augmentation (UDA)_[37], using Algorithm 2, by replacing loss \(\phi(\theta,x)\) with the UDA objective. The UDA objective penalizes discrepancies between the model outputs for two different data representations (a strongly augmented one and a weakly augmented one) on the unlabeled data (the _UDA penalty_). The inconsistency penalty term of Algorithm 2 also uses unlabeled data, and for this purpose, we used a strongly augmented unlabeled batch sampled independently of those for the UDA penalty. UDA'sharpens' the model output on the weakly augmented data by scaling the logits, which serves as pseudo labels for unlabeled data, and the degree of sharpening is a tuning parameter. However, we tested UDA without sharpening and obtained better performance on the development data (held-out 5K data points) than reported in [32], and so we decided to use UDA without sharpening for our UDA+'Consist.' experiments. We obtained test error 3.95% on the average of 5 independent runs, which is better than 4.33% of UDA alone and 4.25% of FixMatch. Note that each of the 5 runs used a different fold of 4K examples as was done in the previous studies.

Following [32], we used labeled batch size 64, weight decay 5e-4, and updated the weights 500K times with the cosine learning rate schedule decaying from 0.03 to 0.2\(\times\)0.03. We set the final model to the average of the last 5% iterates (i.e., the last 25K snapshots of model parameters). We used these same basic settings for all (UDA, FixMatch, and UDA+'Consist.'). The unlabeled batch size for testing UDA and FixMatch was 64\(\times\)7, as in [32]. For computing each of the two penalty terms for UDA+'Consist.', we set the unlabeled batch size to 64\(\times\)4, which is approximately one half of that for UDA and FixMatch. We made it one half so that the total number of unlabeled data points used by each model (64\(\times\)4 for the UDA penalty plus 64\(\times\)4 for the inconsistency penalty) becomes similar to that of UDA or FixMatch. RandAugment with the same modification as described in the FixMatch study was used as strong augmentation, and the standard data augmentation (shift and flip) was used as weak augmentation. The threshold for masking was set to 0.5 for both the UDA penalty and the inconsistency penalty and the weights of the both penalties were set to 1. Note that we fixed the weights of penalties to 1, and only tuned the threshold for masking by selecting from \(\{0,0.5,0.7\}\) on the development data (5K examples).

#### c.3.4 Fine-tuning experiments in Table 5

The EfficientNet-B4 (EN-B4) fine-tuning experiments reported in Table 5 were done with weight decay 1e-5 (following [10]), batch size 256 and number of updates 20K following the previous studies. We set the learning rate to 0.1 and performed gradient clipping with size 20 to deal with a sudden surge of the gradient size. The diameter of the local region \(\rho\) for SAM was chosen from \(\{0.05,0.1,0.2\}\) based on the performance of SAM on the development data, and the same chosen value 0.2 was used for both SAM and SAM+'Consist.' Following the EN-B7 experiments of [10], the value \(m\) for \(m\)-sharpness was set to 16. Since SAM+'Consist.' is approximately twice as expensive as SAM as a result of training two models, we also tested SAM with 40K updates (20K\(\times\)2) and found that it did not improve performance. We note that our baseline EN-B4 SAM performance is better than the EN-B7 SAM performance of [10]. This is due to the difference in the basic setting. In [10], EN-B7 was fine-tuned with a larger batch size 1024 with a smaller learning rate 0.016 while the batch normalization statistics was fixed to the pre-trained statistics. Our basic setting allowed a model to go farther away from the initial pre-trained model. Also note that we experimented with smaller EN-B4 instead of EN-B7 due to resource constraints.

## Appendix D Additional information

### Additional correlation analyses using the framework of Jiang et al. (2020)

This section reports on the additional correlation analysis using the rigorous framework of Jiang et al. (2020) [18] and shows that the results are consistent with the results in the main paper and the previous work. The analysis uses correlation metrics proposed by [18], which seek to mitigate the effect of what [18] calls _spurious correlations_ that do not reflect causal relationships with generalization. For completeness, we briefly describe below these metrics, and [18] should be referred to for more details and justification.

NotationIn this section, we write \(\pi\) for a training procedure (or equivalently, a combination of hyperparameters including the network architecture, the data augmentation method, and so forth). Let \(g(\pi)\) be the generalization gap of \(\pi\), and let \(\mu(\pi)\) be the quantity of interest such as inconsistency or disagreement.

Ranking-basedLet \(\mathcal{T}\) be the set of the corresponding pairs of generalization gap and the quantity of interest to be considered: \(\mathcal{T}:=\cup_{\pi}\left\{(\mu(\pi),g(\pi))\right\}\). Then the standard Kendall's ranking coefficient \(\tau\) of \(\mathcal{T}\) can be expressed as:

\[\tau(\mathcal{T}):=\frac{1}{|\mathcal{T}|(|\mathcal{T}|-1)}\sum_{(\mu_{1},g_{ 1})\in\mathcal{T}}\ \ (\mu_{2},g_{2})\in\mathcal{T}\backslash(\mu_{1},g_{1})\ \mathrm{sign}(\mu_{1}-\mu_{2})\mathrm{sign}(g_{1}-g_{2})\]

[18] defines _granulated Kendall's coefficient_\(\Psi\) as:

\[\Psi:=\frac{1}{n}\sum_{i=1}^{n}\psi_{i},\ \psi_{i}:=\frac{1}{m_{i}}\sum_{\pi_{1 }\in\Pi_{1}}\cdots\sum_{\pi_{i-1}\in\Pi_{i-1}}\sum_{\pi_{i+1}\in\Pi_{i+1}} \cdots\sum_{\pi_{n}\in\Pi_{n}}\tau\left(\cup_{\pi_{i}\in\Pi_{i}}\left(\mu( \pi),g(\pi)\right)\right)\] (6)

where \(\pi_{i}\) is the \(i\)-th hyperparameter so that \(\pi=(\pi_{1},\pi_{2},\cdots,\pi_{n})\), \(\Pi_{i}\) is the set of all possible values for the \(i\)-th hyperparameter, and \(m_{i}:=|\Pi_{1}\times\cdots\times\Pi_{i-1}\times\Pi_{i+1}\times\cdots\times \Pi_{n}|\).

Mutual information-basedDefine \(V_{g}(\pi,\pi^{\prime}):=\mathrm{sign}(g(\pi)-g(\pi^{\prime}))\), and similarly define \(V_{\mu}(\pi,\pi^{\prime}):=\mathrm{sign}(\mu(\pi)-\mu(\pi^{\prime}))\). Let \(U_{S}\) be a random variable representing the values of the hyperparameter types in \(S\) (e.g., \(S\) = { learning rate, batch size }). Then \(I(V_{\mu},V_{g}|U_{S})\), the conditional mutual information between \(\mu\) and \(g\) given the set \(S\) of hyperparameter types, and the 

\begin{table}

\end{table}
Table 15: Correlation scores of inconsistency and disagreement. For the training procedures with low final randomness (as in Figure 5), model-wise quantities (one model per procedure) were analyzed. (a)–(c) differ in the restriction on training loss; (b) and (c) exclude the models with high training loss while (a) does not. The average and standard deviation of 4 independent runs (that use 4 distinct subsamples of training sets as training data and distinct random seeds) are shown. **Correlation scores**: Two types of mutual information-based scores (‘\(\mathcal{K}\)’ as in (7) and ‘\(|S|\)=0’: \(\frac{I(V_{\theta_{i}},V_{\theta_{j}}|U_{S})}{H(V_{\theta_{j}}|U_{S})}\) with \(|S|\)=0) and two types of Kendall’s rank-correlation coefficient-based scores (\(\Psi\) as in (6) and overall \(\mathcal{K}\)). A larger number indicates a higher correlation. The highest numbers are highlighted. **Tested quantities**: ‘Inconcisti.’: 'Inconsistency, ’Disagree.’: Disagreement, ’Random’ (baseline): random numbers drawn from the normal distribution, ’Canonical’ (baseline): a slight extension of the canonical ordering in [18]; it heuristically determines the order of two procedures by preferring smaller batch size, larger weight decay, larger learning rate, and presence of data augmentation (which are considered to be associated with better generalization) by adding one point for each and breaking ties randomly. **Target quantities**: Generalization gap (test loss minus training loss) as defined in the main paper, test error, and test error minus training error. **Observation**: Inconsistency correlates well to generalization gap, and disagreement correlates well to test error. With more aggressive exclusion of the models with high training loss (going from (a) to (c)), the correlation of disagreement to generalization gap improves and approaches that of inconsistency.

conditional entropy \(H(V_{g}|U_{S})\) can be expressed as follows.

\[I(V_{\mu},V_{g}|U_{S})=\sum_{U_{S}}p(U_{S})\sum_{V_{\mu}\in\{\pm 1\}} \sum_{V_{g}\in\{\pm 1\}}p(V_{\mu},V_{g}|U_{S})\log\left(\frac{p(V_{\mu},V_{g}|U_{S})}{p(V_{ \mu}|U_{S})p(V_{g}|U_{S})}\right)\] \[H(V_{g}|U_{S})=-\sum_{U_{S}}p(U_{S})\sum_{V_{g}\in\{\pm 1\}}p(V_{g}|U _{S})\log(p(V_{g}|U_{S}))\]

Dividing \(I(V_{\mu},V_{g}|U_{S})\) by \(H(V_{g}|U_{S})\) for normalization and restricting the size of \(S\) for enabling computation, [18] defines the metric \(\mathcal{K}(\mu)\) as:

\[\mathcal{K}(\mu):=\min_{U_{S}\ \mathrm{s.t.}|S|\leq 2}\frac{I(V_{\mu},V_{g}|U_{S })}{H(V_{g}|U_{S})}.\] (7)

Results (Tables 15-16)In Table 15, we show the average and standard deviation of the correlation scores of model-wise quantities (for one model per training procedure), following [18]. The model-wise inconsistency for model \(\theta\) trained on \(Z_{n}\) with procedure \(P\) is \(\mathbb{E}_{\Theta\sim\Theta_{P|Z_{n}}}\mathbb{E}_{X}\mathrm{KL}(f(\Theta,X) \|f(\theta,X))\), and \(Z_{n}\) was fixed here; similarly, model-wise disagreement is \(\mathbb{E}_{\Theta\sim\Theta_{P|Z_{n}}}\mathbb{E}_{X}\mathbb{I}\left[\,c( \Theta,X)\neq c(\theta,X)\,\right]\) where \(c(\theta,x)\) is the classification decision of model \(\theta\) on data point \(x\). The average and standard deviation were computed over 4 independent runs that used 4 distinct subsamples of training sets as training data \(Z_{n}\) and distinct random seeds for model parameter initialization, data mini-barching, and so forth.

Table 15 compares inconsistency and disagreement in terms of their correlations with the generalization gap (test loss minus training loss as defined in the main paper), test error, and test error minus training error. The training procedures analyzed here are the procedures that achieve low final randomness by either a vanishing learning rate or iterate averaging as in Figure 5. Tables (a), (b), and (c) differ in the models included in the analysis. As noted in Appendix C.1, since near-random models in the initial phase of training are not of practical interest, procedures with very high training loss were excluded from the analysis in the main paper. Similarly, Table 15-(b) excludes the models with high training loss using the same cut-off values as used in the main paper, and (c) does this with smaller (and therefore more aggressive) cut-off values (one half of (b)), and (a) does not exclude any model. Consequently, the average training loss is the highest in (a) and lowest in (c).

Let us first review Table 15-(a). The results show that inconsistency correlates well to generalization gap (test loss minus training loss) as suggested by our theorem, and disagreement correlates well to test error as suggested by the theorem of the original disagreement study [17]. Regarding 'test error minus training error' (last 4 rows): on CIFAR-10, training error is relatively small and so it approaches test error, which explains why disagreement correlates well to it; on the other datasets, 'test error minus training error' is more related to 'test loss minus training loss', which explains why inconsistency correlates well to it. The standard deviations are relatively small, and so the results are solid. (The standard deviation of \(\Psi\) tends to be higher than the others for all quantities including the baseline 'Random', and this is due to the combination of the macro averaging-like nature of \(\Psi\) and the smallness of \(|\Pi_{i}|\) for some \(i\)'s, independent of the nature of inconsistency or disagreement.)

The overall trend of Table 15-(b) and (c) is similar to (a). That is, inconsistency correlates well to generalization gap (test loss minus training loss) while disagreement correlates well to test error, consistent with the results in the main paper and the original disagreement study. Comparing (a), (b), and (c), we also note that as we exclude the models with high training loss more aggressively (i.e., going from (a) to (c)), the correlation of disagreement to generalization gap (relative to that of inconsistency) improves and approaches that of inconsistency. For example, on CIFAR-100, the ratio of \(\mathcal{K}\) for (disagreement, generalization gap) with respect to \(\mathcal{K}\) for (inconsistency, generalization gap) improves from (a) 0.11/0.51=22% to (b) 0.26/0.47=55% to (c) 0.31/0.44=70%. With these models, empirically, high training loss roughly corresponds to the low confidence-level on unseen data, and so this observation is consistent with the theoretical insight that when the confidence-level on unseen data is high, disagreement should correlate to generalization gap as well as inconsistency, which is discussed in more detail in Appendix D.3.

Table 16 shows that the correlation of the estimate of \(\mathcal{C}_{P}\) (defined in Section 2) to the generalization gap is generally as good as the estimate of \(\mathcal{D}_{P}\) (defined in Theorem 2.1), which is consistent with the results in the main paper.

### Training error

Tables 17 and 18 show the training error values (the average, minimum, median, and the maximum) associated with the empirical results reported in Section 3.1 and 3.2, respectively.

### More on inconsistency and disagreement

Inconsistency takes how strongly the models disagree on each data point into account while disagreement ignores it. That is, the information disagreement receives on each data point is _binary_ (whether the classification decisions of two models agree or disagree) while the information inconsistency receives is continuous and _more complex_. On the one hand, this means that inconsistency could use information ignored by disagreement and thus it could behave quite differently from disagreement as seen in our empirical study. On the other hand, it should be useful also to consider the situation where inconsistency and disagreement are highly correlated since in this case our theoretical results can be regarded as providing a theoretical basis for the correlation of not only inconsistency but also disagreement with generalization gap though indirectly.

To simplify the discussion towards this end, let us introduce a slight variation of Theorem 2.1, which uses 1-norm instead of the KL-divergence since disagreement is related to 1-norm as noted in Section 2.

**Proposition D.1** (1-norm variant of Theorem 2.1).: _Using the notation of Section 2, define 1-norm inconsistency \(\mathcal{C}_{1,P}\) and 1-norm instability \(\mathcal{S}_{1,P}\) which use the squared 1-norm of the difference in place of the KL-divergence as follows._

\[\mathcal{C}_{1,P} =\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim\mathbf{ \Theta}_{P|Z_{n}}}\mathbb{E}_{X}\|f(\Theta,X)-f(\Theta^{\prime},X)\|_{1}^{2} \text{(1-norm inconsistency)}\] \[\mathcal{S}_{1,P} =\mathbb{E}_{Z_{n},Z_{n}^{\prime}}\mathbb{E}_{X}\|\tilde{f}_{P|Z _{n}}(X)-\tilde{f}_{P|Z_{n}^{\prime}}(X)\|_{1}^{2} \text{(1-norm instability)}\]

_Then with the same assumptions and definitions as in Theorem 2.1, we have_

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\mathbf{\Theta}_{P|Z_{n}}}\left[\Phi_{ \mathbf{Z}}(\Theta)-\Phi(\Theta,Z_{n})\right]\leq\inf_{\lambda>0}\left[\frac{ \gamma^{2}}{2}\psi(\lambda)\lambda\left(\mathcal{C}_{1,P}+\mathcal{S}_{1,P} \right)+\frac{\mathcal{I}_{P}}{\lambda n}\right].\]

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline  & \multicolumn{4}{c|}{CIFAR-10} & \multicolumn{4}{c|}{CIFAR-100} & \multicolumn{4}{c|}{ImageNet} \\ \cline{2-13}  & \multicolumn{2}{c|}{MI-based} & \multicolumn{2}{c|}{Ranking} & \multicolumn{2}{c|}{MI-based} & \multicolumn{2}{c|}{Ranking} & \multicolumn{2}{c|}{MI-based} & \multicolumn{2}{c|}{Ranking} \\ \cline{2-13}  & \(\mathcal{K}\) & \(|S|\)=0 & \(\Psi\) & \(\tau\) & \(\mathcal{K}\) & \(|S|\)=0 & \(\Psi\) & \(\tau\) & \(\mathcal{K}\) & \(|S|\)=0 & \(\Psi\) & \(\tau\) \\ \hline \(\mathcal{C}_{P}\) & 0.38 & 0.59 & 0.71 & 0.83 & 0.49 & 0.53 & 0.80 & 0.80 & 0.75 & 0.75 & 0.59 & 0.92 \\ \(\mathcal{D}_{P}\) & 0.38 & 0.59 & 0.80 & 0.84 & 0.42 & 0.47 & 0.75 & 0.76 & 0.81 & 0.81 & 0.60 & 0.94 \\ \hline \end{tabular}
\end{table}
Table 16: Correlation analyses of the estimates of \(\mathcal{C}_{P}\) and \(\mathcal{D}_{P}\) (as defined in Section 2) using the rank-based and mutual information-based metrics from [18]. Training procedures with low final randomness of Figure 5.

\begin{table}
\begin{tabular}{|c c|c c c|c c c|c c c|c c c|} \hline  & \multicolumn{4}{c|}{CIFAR-10} & \multicolumn{4}{c|}{CIFAR-100} & \multicolumn{4}{c|}{ImageNet} \\ \hline Figure 1,3,6 & 0.4 & 0.0 & 0.0 & 8.8 & 4.7 & 0.0 & 0.1 & 53.6 & 24.9 & 3.0 & 23.5 & 56.6 \\ Figure 2 & 3.3 & 0.0 & 2.6 & 10.9 & 18.2 & 0.2 & 13.6 & 56.3 & 44.5 & 16.8 & 45.6 & 65.3 \\ Figure 5,7 & 0.5 & 0.0 & 0.0 & 10.1 & 4.0 & 0.0 & 0.0 & 54.5 & 16.3 & 0.1 & 11.3 & 59.0 \\ \hline \end{tabular}
\end{table}
Table 17: Training error of the models analyzed in Section 3.1. The four numbers represent the average, minimum, median, and maximum values (%).

\begin{table}
\begin{tabular}{|c c c|c c|c c|c c c|c c|c c c|} \hline  & \multicolumn{4}{c|}{Case\#1} & \multicolumn{4}{c|}{Case\#2} & \multicolumn{4}{c|}{Case\#3} & \multicolumn{4}{c|}{Case\#4} & \multicolumn{4}{c|}{Case\#5} \\ \hline
10.0 & 8.8 & 10.1 & 11.6 & 4.5 & 2.1 & 4.5 & 6.7 & 0.06 & 0.02 & 0.04 & 0.17 & 0.04 & 0.01 & 0.03 & 0.11 & 0.0 & 0.0 & 0.0 & 0.0 \\ \hline  & \multicolumn{4}{c|}{Case\#6} & \multicolumn{4}{c|}{Case\#7} & \multicolumn{4}{c|}{Case\#8} & \multicolumn{4}{c|}{Case\#9} & \multicolumn{4}{c|}{Case\#10} \\ \hline
0.7 & 0.3 & 0.8 & 1.1 & 0.1 & 0.1 & 0.1 & 0.2 & 4.8 & 3.9 & 4.9 & 5.5 & 2.7 & 1.9 & 2.9 & 3.3 & 2.8 & 0.5 & 1.8 & 6.5 \\ \hline \end{tabular}
\end{table}
Table 18: Training error of the models analyzed in Section 3.2. The four numbers represent the average, minimum, median, and maximum values (%).

Sketch of proof.: Using the triangle inequality of norms and Jensen's inequality, replace inequality (2) of the proof of Theorem 2.1

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\bm{\Theta}_{P|Z_{n}}}\mathbb{E}_{X}\|f( \Theta,X)-\bar{f}_{P}(X)\|_{1}^{2}\leq 4\left(\mathcal{C}_{P}+\mathcal{S}_{P}\right)\] (2)

with the following,

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\bm{\Theta}_{P|Z_{n}}} \mathbb{E}_{X}\|f(\Theta,X)-\bar{f}_{P}(X)\|_{1}^{2}\] \[\leq 2\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta\sim\bm{\Theta}_{P|Z_{n}}} \mathbb{E}_{X}\left[\|f(\Theta,X)-\bar{f}_{P|Z_{n}}(X)\|_{1}^{2}+\|\bar{f}_{P| Z_{n}}(X)-\bar{f}_{P}(X)\|_{1}^{2}\right]\] \[\leq 2\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim\bm{ \Theta}_{P|Z_{n}}}\mathbb{E}_{X}\|f(\Theta,X)-f(\Theta^{\prime},X)\|_{1}^{2}+ 2\mathbb{E}_{Z_{n}}\mathbb{E}_{Z_{n}^{\prime}}\mathbb{E}_{X}\|\bar{f}_{P|Z_{n }}(X)-\bar{f}_{P|Z_{n}^{\prime}}(X)\|_{1}^{2}\] \[= 2\left(\mathcal{C}_{1,P}+\mathcal{S}_{1,P}\right).\]

Now suppose that with the models of interest, the confidence level of model outputs is always high so that the highest probability estimate is always near 1, i.e., for any model \(\theta\) of interest, we have \(1-\max_{i}f(x,\theta)[i]<\epsilon\) for a positive constant \(\epsilon\) such that \(\epsilon\approx 0\) on any unseen data point \(x\). Let \(c(\theta,x)\) be the classification decision of \(\theta\) on data point \(x\) as in Section 2: \(c(\theta,x)=\arg\max_{i}f(\theta,x)[i]\). Then it is easy to show that we have

\[\frac{1}{2}\|f(\theta,x)-f(\theta^{\prime},x)\|_{1}\approx\mathbb{I}\left[\;c (\theta,x)\neq c(\theta^{\prime},x)\;\right].\] (8)

Disagreement measured for shared training data can be expressed as

\[\mathbb{E}_{Z_{n}}\mathbb{E}_{\Theta,\Theta^{\prime}\sim\bm{\Theta}_{P|Z_{n} }}\mathbb{E}_{X}\mathbb{I}\left[\;c(\Theta,X)\neq c(\Theta^{\prime},X)\; \right].\] (9)

Comparing (9) with the definition of \(\mathcal{C}_{1,P}\) above and considering (8), it is clear that under this high-confidence condition, disagreement (9) and 1-norm inconsistency \(\mathcal{C}_{1,P}\) should be highly correlated; therefore, under this condition, Proposition D.1 suggests the relation of disagreement (measured for shared training data) to generalization gap indirectly through \(\mathcal{C}_{1,P}\).

While this paper focused on the KL-divergence-based inconsistency motivated by the use of the KL-divergence by the existing algorithm for encouraging consistency, the proposition above suggests that 1-norm-based inconsistency might also be useful. We have conducted limited experiments in this regard and observed mixed results. In the settings of Appendix D.1, the correlation scores of 1-norm inconsistency with respect to generalization gap are generally either similar or slightly better, which is promising. As for consistency encouragement during training, we have not seen a clear advantage of using 1-norm inconsistency penalty over using the KL-divergence inconsistency penalty as is done in this paper, and more experiments would be required to understand its advantage/disadvantage.

In our view, however, for the purpose of encouraging consistency during training, KL-divergence inconsistency studied in this paper is more desirable than 1-norm inconsistency in at least three ways. First, minimization of the KL-divergence inconsistency penalty is equivalent to minimization of the standard cross-entropy loss with soft labels provided by the other model; therefore, with the KL-divergence penalty, the training objective can be regarded as a weighted average of two cross-entropy loss terms, which are in the same range (while 1-norm inconsistency is not). This makes tuning of the weight for the penalty more intuitive and easier. Second, optimization of the standard cross-entropy loss term with the KL-divergence inconsistency penalty has an interpretation of functional gradient optimization, as shown in [19]. The last (but not least) point is that optimization may be easier with KL-divergence inconsistency, which is smooth, than with 1-norm inconsistency, which is not smooth.

Related to the last point, disagreement, which involves \(\arg\max\) in the definition, cannot be easily integrated into the training objective, and this is a crucial difference between inconsistency and disagreement from the algorithmic viewpoint.

Finally, we believe that for improving deep neural network training, it is useful to study the connection between generalization and discrepancies of model outputs in general including instability, inconsistency, and disagreement, and we hope that this work contributes to progress in this direction.