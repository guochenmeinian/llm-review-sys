# Tiny Time Mixers (TTMs): Fast Pre-trained Models

for Enhanced Zero/Few-shot Forecasting of Multivariate Time Series

 Vijay Ekambaram

IBM Research

Bangalore, India

vijayel2@in.ibm.com

&Arindam Jati

IBM Research

Bangalore, India

arindam.jati@ibm.com

&Pankaj Dayama

IBM Research

Bangalore, India

pankajdayama@in.ibm.com

&Sumanta Mukherjee

IBM Research

Bangalore, India

sumann03@in.ibm.com

&Nam H. Nguyen

IBM Research

Yorktown Heights, NY, USA

nnguyen@us.ibm.com

&Wesley M. Gifford

IBM Research

Yorktown Heights, NY, USA

wmgifford@us.ibm.com

&Chandra Reddy

IBM Research

Yorktown Heights, NY, USA

creddy@us.ibm.com

&Jayant Kalagnanam

IBM Research

Yorktown Heights, NY, USA

jayant@us.ibm.com

###### Abstract

Large pre-trained models excel in zero/few-shot learning for language and vision tasks but face challenges in multivariate time series (TS) forecasting due to diverse data characteristics. Consequently, recent research efforts have focused on developing pre-trained TS forecasting models. These models, whether built from scratch or adapted from large language models (LLMs), excel in zero/few-shot forecasting tasks. However, they are limited by slow performance, high computational demands, and neglect of cross-channel and exogenous correlations. To address this, we introduce Tiny Time Mixers (TTM), a compact model (starting from 1M parameters) with effective transfer learning capabilities, trained exclusively on public TS datasets. TTM, based on the light-weight TSMixer architecture, incorporates innovations like adaptive patching, diverse resolution sampling, and resolution prefix tuning to handle pre-training on varied dataset resolutions with minimal model capacity. Additionally, it employs multi-level modeling to capture channel correlations and infuse exogenous signals during fine-tuning. TTM outperforms existing popular benchmarks in zero/few-shot forecasting by (4-40%), while reducing computational requirements significantly. Moreover, TTMs are lightweight and can be executed even on CPU-only machines, enhancing usability and fostering wider adoption in resource-constrained environments. The model weights for reproducibility and research use are available here, while enterprise-use weights under the Apache license can be accessed as follows: the initial TTM\({}_{Q}\)variant here, and the latest variants (TTM\({}_{B}\), TTM\({}_{E}\), TTM\({}_{A}\)) weights are available here. The source code for the TTM model along with the usage scripts are available here.

Introduction

Multivariate time-series (TS) forecasting entails predicting future values for multiple interrelated time series based on their historical values. The channels1 being forecast are called target variables, while those influencing the forecasts are termed exogenous variables. This field has seen significant advancements through the application of statistical and machine learning (ML) methods across various domains such as weather, traffic, retail, and energy.

Footnote 1: “Channel” refers to an individual dimension in multivariate data (i.e., multivariate or multichannel).

**Related Work:** Recent advances in multivariate forecasting have been marked by the advent of Transformer-based [31] approaches, exemplified by models like PatchTST [22], Autoformer [38], and FEDFormer [45]. These models have demonstrated notable improvements over traditional statistical and ML methods. Furthermore, architectures based on MLP-Mixer [30], such as TSMixer [6] and TimeMixer [33], have emerged as efficient Transformer alternatives, boasting 2-3X reduced compute requirements with no accuracy compromise compared to their Transformer counterparts.

Recently, there has been substantial interest in the research community to build general pre-trained or foundation models (FMs) for TS forecasting that can successfully transfer the learning to unseen target TS dataset, similar to the successes in NLP and vision tasks. However, pre-training in the TS domain is particularly challenging due to the limited public availability of the datasets and the diverse nature across applications. Early in 2024, this interest culminated in the release of several "large" and "massive" TS pre-trained models for forecasting, generating considerable excitement in the research community. Among these releases were Moment[10]3, TimesFM [3]3, Chronos[2]3, Moirai[35]3, and Laglam[26]3 that successfully established strong benchmarks in zero-shot forecasting. In addition, there has been a trend towards leveraging pre-trained large language models (LLMs) for TS forecasting, treating the forecasting task as a form of cross-domain transfer learning. These universal cross-transfer approaches, exemplified by recent works such as LLMTime [11], Time-LLM [15], and GPT4TS [46], exhibit promising outcomes in zero/few-shot forecasting scenarios. However, most of these "large" TS pre-trained models demand extremely high computational resources, given their scale ranges from several hundred million to billions of parameters. Given the recent surge in popularity of "small" language models[1][29][39] that address practical resource and cost constraints in real-world industrial settings, this work considers the following question: _Can "tiny" pre-trained models succeed in the TS domain too? If so, can they outperform the zero/few-shot forecasting results of "large" TS pre-trained models demanding significant computational resources and runtime?_ Surprisingly, as we demonstrate in this work, the answer is _yes_.

Footnote 3: Work done concurrently with this research.

Footnote 3: Time-LLM and LLMTime are excluded here as we couldn’t run them on CPUs, but their accuracy is compared later in experiments.

Toward this, we propose **Multi-level Tiny Time Mixers (TTM)**, a significantly smaller pre-trained model (starting from 1 million (M) parameters) for effective zero/few-shot multivariate forecasting. In particular, TTM supports channel correlations and exogenous signals, which are critical and practical business requirements in the context of multivariate forecasting, features lacking in many existing TS pretrained models. TTM is based on the light-weight TSMixer architecture that uses MLPMixer blocks interleaved with simple gated attention as alternatives to the quadratic time-consuming self-attention blocks in Transformers, which makes TTM pre-training and fine-tuning extremely fast. TTM is pre-trained using multiple public datasets (\(\sim\)1 billion (B) samples) from the Monash and LibCity data repositories. Note that the datasets exhibit considerable diversity in characteristics, such as different domains, temporal resolutions4 (ranging from seconds to days),

Figure 1: **Size, time, and accuracy overview of TTM\({}_{B}\) vs. open-sourced pre-trained TS benchmarks2. We plot each model based on its model size and per batch CPU inference time. The X% mentioned for each baseline indicates that the baseline’s forecast is X% less accurate compared to TTM’s forecast. Full details in Tables [1, 2, 3, 4, 5].**

lengths, and numbers of channels. Pre-training on such heterogeneous datasets using extremely small models requires specialized architectural advancements. Hence, TTM proposes the following enhancements to the TSMixer architecture for resource-constrained pre-training/fine-tuning: (i) **adaptive patching (AP)** across layers, considering the varied suitability of patch lengths for different datasets, (ii) **diverse resolution sampling (DRS)** to augment the data for increasing coverage across different resolutions, (iii) **resolution prefix tuning (RPT)** to explicitly embed resolution information in the first patch, facilitating resolution-conditioned modeling while training on diverse datasets. Moreover, our approach leverages **multi-level modeling**, where TTMs are first pre-trained in a channel-independent way, and then fine-tuned with channel mixing to incorporate correlations across targets and exogenous channels in the target domain.

**Outline of TTM's key capabilities:** (1) Amidst the prevalence of "large" pre-trained models demanding significant compute and training time, our work is the first to demonstrate the power of transfer learning using "tiny" TS pre-trained models for zero/few-shot forecasting. (2) Pre-training tiny models on heterogeneous multi-resolution datasets with extremely limited model capacity is challenging. Towards this, we propose various **architectural and training enhancements**, such as **AP**, **DRS**, and **RPT** for robust and resource-constrained pre-training/fine-tuning workflows (as defined above). (3) TTM employs a **multi-level modeling strategy** to explicitly model channel correlations, and incorporate exogenous signals - a crucial capability lacking in most of the existing pre-trained models. (4) Through extensive evaluation of zero/few-shot forecasting on 11 datasets, we establish that TTM models, with sizes as small as 1M parameters, consistently outperform the forecasts of "large" TS pretrained models while offering significant computational benefits. Figure 1 highlights that TTM outperforms popular benchmarks in all three primary dimensions: size, runtime, and accuracy. (5) Given their compact size, zero-shot inference and fine-tuning of TTM models can be easily executed with just one GPU or in CPU-only environments. This greatly enhances the practical adoption and extended reach of our pre-trained models with ease of use.

## 2 TTM Components

Let \(\mathbf{X}\in\mathbb{R}^{c\times sl}\) be a multivariate time series of length \(sl\) and number of channels \(c\). The forecasting task can be formally defined as predicting the future values \(\mathbf{Y}\in\mathbb{R}^{c^{\prime}\times fl}\) given the history/context \(\mathbf{X}\). Here, \(fl\) denotes the forecast length/horizon, and \(c^{\prime}\) denotes number of forecast channels, where \(c^{\prime}\leq c\). The predictions from the model are denoted by \(\hat{\mathbf{Y}}\in\mathbb{R}^{c^{\prime}\times fl}\). In a general multivariate forecasting task, each channel falls into one of the following categories: (a) **Target variables (mandatory):** channels for which forecasts are required, (b) **Exogenous variables (optional):** channels influencing the targets, with known or estimated values throughout the forecast horizon.

### Multi-level Modeling:

TTM follows a multi-level architecture consisting of four key components (see Figure 2(a)): (1) The **TTM backbone** is assembled using building blocks derived from the efficient TSMixer architecture [6]. TSMixer is based on MLP blocks interleaved with gated

Figure 2: TTM overview (a) Refer to Sections 2 and 3, (b) Refer to Section 3.1, (c) Refer to Section 3.2

attention, that enable the mixing of features within patches, across patches and channels, surpassing existing Transformer-based TS approaches with minimal computational requirements. Since TSMixer was not designed to handle multi-resolution data with limited capacity, we introduce various novel enhancements to it as explained later. (2) **TTM decoder** follows the same backbone architecture but is considerably smaller in size, approximately 10-20% of the size of the backbone, (3) **Forecast head** consists of a linear head designed to produce the forecast output, and (4) Optional **Exogenous mixer** serves to fuse exogenous data into the forecasting process. The TTM decoder and forecast head together constitute the **TTM head**, whose weights get updated during the fine-tuning process. This multi-level model refactoring is required to dynamically change the working behavior of various components based on the workflow type, as explained in Section 3. In addition to the above primary components, there is also a preprocessing component as explained next.

2.2 Preprocessing:As shown in Figure 2(a) with colorless blocks, the historical time series \(\mathbf{X}\) is first **normalized** per instance to have zero mean and unit standard deviation for each channel, to tackle any possible distribution shifts [22, 6]. This process is reversed at the end before computing the loss. The normalized data \(\mathbf{\overline{X}}\) is subsequently **patched**, \(\mathbf{X}_{p}\in\mathbb{R}^{c\times n\times pl}\), into \(n\) non-overlapping windows, each of length \(pl\), and then passed to the TTM backbone. Patching, as introduced in [22], has proven to be highly valuable for forecasting. Its effectiveness lies in preserving local semantic information, accommodating longer history, and reducing computation.

## 3 TTM Methodology

3.1 Pre-training Workflow:TTM operates in two stages: pre-training and fine-tuning (Figure 2(a)). In the pre-training stage, we train the model on a large collection of diverse public datasets. Since the primary focus of TTM is forecasting, pre-training is modeled with a direct forecasting objective. TTM is first pre-trained in a univariate fashion with independent channels on all the existing datasets. Due to varied channel counts in pre-training datasets, modeling multivariate correlations is challenging here; it is addressed later during the fine-tuning stage. Multivariate pre-training datasets are initially transformed into independent univariate TS \((\mathbf{X}_{1},\cdots,\mathbf{X}_{N})\in\mathbb{R}^{c(=1)\times sl}\). These are pre-processed (Section 2.2), and subsequently fed into the TTM backbone for multi-resolution pre-training. The output of the backbone \(\mathbf{X}_{h}^{L}\in\mathbb{R}^{(c=1)\times n\times hf}\) is passed through the decoder and forecast head to produce the forecast \(\hat{\mathbf{Y}}\in\mathbb{R}^{(c=1)\times fl}\) which is then reverse-normalized to return to the original scale. We pre-train the TTM with mean squared error (MSE) loss calculated over the forecast horizon: \(\mathcal{L}=||\mathbf{Y}-\hat{\mathbf{Y}}||_{2}^{2}\). Thus for a given input context length \(sl\) and forecast length \(fl\), we get a pre-trained model capturing the common temporal forecasting dynamics and seasonal patterns as observed in the overall pre-training data.

3.1.1 Multi-Resolution Pre-training via TTM Backbone:In TTM, our goal is to create models that are extremely small yet capable of generalizing well across a wide range of diverse datasets with varying resolutions. This is a significant challenge because the models can easily under-fit due to their small size. To tackle these challenges of resource-constrained pre-training, we introduce the following enhancements to the TSMixer backbone.

**Adaptive patching (AP):** The TTM backbone is crafted with an adaptive patching architecture where different layers of the backbone operate at varying patch lengths and numbers of patches. Since each dataset in the pre-training corpora may perform optimally at a specific patch length, this approach greatly aids in generalization when the pretraining datasets with different resolutions are introduced. Moreover, it helps in scenarios when the availability of the pre-training data is limited as adaptive patching quickly generalizes the model across different granularities. As shown in Figure 2(b), the patched data \(\mathbf{X}_{p}\in\mathbb{R}^{c\times n\times pl}\) is passed through a embedding layer to project it to the patch hidden dimension, \(\mathbf{X}_{h}\in\mathbb{R}^{c\times n\times hf}\). If the resolution prefix tuning module is activated (as explained later), the resolution prefix is concatenated with \(\mathbf{X}_{h}\). For notational simplicity, we denote the concatenated tensor with \(\mathbf{X}_{h}\) as well. The TTM backbone consists of \(L\) levels, each comprising \(M\) TTM blocks with identical patch configurations. The first block in the first level receives \(\mathbf{X}_{h}\). The first TTM block in the \(i\)-th level, \(i=2,\ldots,L\), receives the processed data \(\mathbf{X}_{h}^{(i-1)}\in\mathbb{R}^{c\times n\times hf}\) from the previous block. Each TTM block is further comprised of a patch partition block, a vanilla TSMixer block, and a patch merging block. The patch partition block at level \(i\) increases the number of patches by a factor of \(K_{i}\) and reduces the patch dimension size by the same factor by reshaping \(\mathbf{X}_{h}^{(i-1)}\in\mathbb{R}^{c\times n\times hf}\)to \(\mathbf{X}_{h}^{i}\in\mathbb{R}^{c\times(n\cdot K_{i})\times(hf/K_{i})}\), where \(K_{i}=2^{(L-i)}\). Figure 2(b) shows the TTM backbone for \(L=3\) and \(M=2\). Note that, we set \(hf=m\cdot 2^{L-1}\) for some integer \(m\). Then, TSMixer is applied to the adapted data \(\mathbf{X}_{h}^{i}\). Finally, the output from TSMixer is again reshaped to its original shape (i.e., \(\mathbb{R}^{c\times n\times hf}\)) in the patch merging block. In subsequent layers, for each increment in level \(i\), the number of patches is halved and the patch dimension doubled. This enables better generalization for small models as we pre-train across multiple datasets. The idea of adaptive patching is popular and very successful in the vision domain (e.g., Swin Transformers [20]) and we successfully apply it to the TS domain to resolve multi-resolution issues in modelling diverse TS datasets. Note that adaptive patching is enabled only in the backbone and not in the decoder, which is designed to be very lightweight.

**Augmentation via diverse resolution sampling (DRS):** A significant challenge in TS pre-training datasets is the scarcity of public datasets with diverse resolutions. Generally, high-resolution datasets will account for a larger fraction of the samples given their finer sampling resolution. Without adjustment to the training strategy, this can lead to a model that is biased toward the finer resolution data. To overcome this, different strategies are applied to high-resolution datasets to balance the volume of samples at lower resolutions and lead to more uniform coverage. Strategies used include: 1) averaging \(k\) samples in sequential, non-overlapping windows to produce a lower resolution dataset; and 2) conventional decimation where only every \(k\)th sample is retained. In both cases, the integer \(k\) is chosen to achieve the desired resolution from the resolution of the base dataset. For example, from a 4-second resolution dataset, we derive multiple datasets at minutely (\(k=15\)) and hourly resolutions (\(k=900\)). Note that the original high-resolution dataset remains within the pool of pre-training datasets. This methodology increases the number of datasets for each resolution which greatly improves the model performance.

**Resolution prefix tuning (RPT):** This technique explicitly learns and incorporates a new patch embedding as a learnable prefix into the input data based on the input resolution (see Figure 2(b) and Table 8). Similar to the concept of prefix tuning [16], this approach provides an explicit signal to the model about the resolution for resolution-conditioned modeling. First, we map every resolution to a unique integer, which is then passed through an embedding layer to project it to the hidden dimension, \(hf\). Subsequently, we expand the embedding across all channels to have a representation of shape \(c\times 1\times hf\). This resolution-based learnable embedding is particularly beneficial in quickly modeling huge volumes of diverse resolution datasets with limited modelling capacity, as the model can easily decouple the data from different resolutions for resolution-conditioned modeling. In addition, RPT also helps in scenarios when the context length (\(sl\)) is short. In these scenarios, automatically detecting the resolution becomes a challenge for the model. Hence, by explicitly fusing the resolution information as a prefix, we can enhance the model's ability to learn effectively across resolutions without increasing its size.

### Fine-tuning Workflow:

In the fine-tuning workflow, we work with data from the _target_ domain that has no overlap with the pre-training datasets. We have three options here: (a) In **zero-shot** forecasting, we directly use the pre-trained model to evaluate on the _test_ part of the target data; (b) In **few-shot** forecasting, we utilize only a tiny portion (5-10%) of the _train_ part of the target data to quickly update the pre-trained weights of the _TTM head_, and subsequently evaluate it on the _test_ part; (c) In **full-shot** forecasting, we fine-tune the pre-trained weights of the _TTM head_ on the entire _train_ part of the target data, and then evaluate on the _test_ part.

The backbone is frozen during fine-tuning, and still operates in a channel-independent univariate fashion. However, the slim decoder in the TTM Head can be fine-tuned utilizing channel mixing or channel independence for multivariate or univariate target data, respectively. If pure multivariate modeling is needed, then the channel-mixer block in all the TSMixer components (see Figure 2(b)) in the decoder is enabled to explicitly capture the cross-channel correlations. The forecast head and reverse normalization perform similar operations as in the pre-training stage. The fine-tuning also optimizes the forecasting objective with MSE loss. This thoughtful multi-level design choice ensures that our backbone excels in channel-independent pre-training, enabling effective temporal correlation modeling across diverse datasets. Simultaneously, the decoder handles target-data-specific tasks like channel-correlation modeling and fine-tuning. In addition, if the target data has exogenous variables, then an exogenous mixer block is applied to the actual forecasts as explained next.

**Exogenous Mixer Block:** As described in Section 2, the future values of the exogenous channels are known in advance. Let the forecast from the forecast head be \(\hat{\mathbf{Y}}\in\mathbb{R}^{c\times fl}\). Let the channels \(\mathbf{x}_{0},\cdots,\mathbf{x}_{c^{\prime}}\) denote the target variables and \(\mathbf{x}_{c^{\prime}+1},\cdots,\mathbf{x}_{c}\) denote all exogenous variables with their future values known. First, we replace the forecast values for the exogenous channels with the _true_ future values (\(\mathbf{Y}\)) and transpose it: \(\hat{\mathbf{Y}}_{e}=[\hat{\mathbf{y}}_{0},\cdots,\hat{\mathbf{y}}_{c^{\prime}},\mathbf{y}_{c^ {\prime}+1},\cdots,\mathbf{y}_{c}]\in\mathbb{R}^{fl\times c}\). Next, to learn inter-channel _lagged_ correlations, we patch \(\hat{\mathbf{Y}}_{e}\) into a series of overlapped windows (i.e., patching with stride\(=1\)) to create a new tensor: \(\hat{\mathbf{Y}}_{e,p}\in\mathbb{R}^{fl\times\Delta\times c}\), where \(\Delta=2\cdot l+1\) with \(l\) being the context length to incorporate on either side of a time point5. Subsequently, we pass \(\hat{\mathbf{Y}}_{e,p}\) through a vanilla TSMixer block with channel mixing enabled. Thus, the lagged dependency of the forecasts for the target channels on the exogenous channels is seamlessly learned. Finally, we attach a linear head to produce the forecasts for the target channels which is then reshaped as \(\hat{\mathbf{Y}}\in\mathbb{R}^{c^{\prime}\times fl}\). Thus, TTM easily handles exogenous infusion which is a practical requirement in any industrial forecasting problem. Figure 2(c) depicts this procedure.

Footnote 5: This needs padding \(\hat{\mathbf{Y}}_{e}\) with zeros of length \(l\) on both sides.

## 4 Experiments and Results

### Datasets & Metrics :

Pre-training employs a subset of \(\sim\)1B samples from Monash [9] and Libcity [32] data collection. We specifically exclude a few datasets (like yearly, monthly) as they do not possess sufficient length for the long-term forecasting task. Moreover, we remove all the datasets that we utilize for evaluation (i.e., Weather, Electricity, and Traffic). For zero/few-shot evaluation we consider seven public datasets (**D1**): ETTH1, ETTH2, ETTM1, ETTM2, Weather, Electricity, and Traffic as popularly used in most prior state-of-the-art (SOTA) works [44, 22]. Since these datasets do _not_ contain any exogenous variables nor exhibit cross-channel correlation benefits, we incorporate four other datasets (**D2**) for separately validating the efficacy of the decoder channel mixing and exogenous mixer module: bike sharing (BS) [7], carbon capture plant (CC) [13], and 2 more datasets, Application (APP) and Service (SER), from Business and IT observability domain [27, 24]. For full details, refer to the Appendix C. We use mean squared error (MSE) as the standard error metric. In addition, we use the following relative improvement metrics: (i) forecast improvement percentage (_f-imp(%)_) which refers to the MSE (%) improvement of TTM over the considered baseline, averaged across all datasets, and (ii) size improvement metric (_s-imp(X)_) is calculated as the ratio of the baseline model size to the TTM model size (i.e., total parameters).

### SOTA Benchmarks:

We benchmark6 TTM with 24 of the latest open-sourced SOTA forecasting models categorized as follows: (a) **TS pre-trained models:** Lag-Llama [26], TimesFM [3], Moirai [35], Chronos [2] and Moment [10]. (b) **LLM-based TS pre-trained models:** GPT4TS [46], LLMTime [11], Time-LLM [15], UniTime [18] (c) **Self-supervised pre-trained models:** SimMTM [5],Ti-MAE [17], TST [42], LaST [34], TF-C [43], CoST [36] and Ts2Vec [40] (d) **Other architectures:** PatchTST [22], TSMixer [6], TimeMixer [33], iTransformer [19], DLin- ear [41] and TimesNet [37], FEDFormer [45] and Autoformer [38].

Footnote 6: For all tables, we highlight the best and second best models with **bold** and underline, respectively. We denote TTM’s improvement and degradation _w.r.t._ a baseline with \(\uparrow\) and \(\downarrow\) respectively.

### TTM Model Details:

We pre-train three primary variants of TTM as follows: (i) **TTM-Base (TTM\({}_{B}\))**: 1M parameter model trained with context length, \(sl=512\) and patch length, \(pl=64\), (ii) **TTM-Enhanced (TTM\({}_{E}\))**: 4M parameter model trained with \(sl=1024\) and \(pl=128\), (iii) **TTM-Advanced (TTM\({}_{A}\))**: 5M parameter model trained with \(sl=1536\) and \(pl=128\). These TTMs are pre-trained using the 1B pre-training dataset, which takes only 24-30 hours with 6 A100 GPUs, a notably faster time compared to existing counterparts which often take days to weeks. Additionally, for secondary studies, we utilize **Quick TTM (TTM\({}_{Q}\))**, a variant trained on a smaller subset of the Monash dataset (\(\sim\)250 million samples), requiring only 4-6 hours for pre-training.

Although, a TTM model needs to be pre-trained for a specific forecast length (FL), we provide two forecast length adaption (FLA) techniques (explained in Section 4.7) that enable a pre-trained TTM to work across different FLs. Users can either build a direct pre-trained model (from one of the above variants) targeting a specific FL, or use the FLA techniques to adapt an existing TTM model to their

[MISSING_PAGE_FAIL:7]

Table 4, TTM\({}_{B}\) surpasses GPT4TS by 15% and Time-LLM by 10% in the few-shot 5% setting, where only 5% of the train data is used for fine-tuning. In addition, we also report the Few-shot 5% results of several popular SOTA architectures in Table 4, where TTM demonstrates superior performance. This underscores the significance of TTM's pre-trained weights, which substantially contribute to its effectiveness in data-constrained scenarios. Likewise, TTM also excels in few-shot cross-transfer learning tasks outperforming popular SOTAs (including SimMTM [5]) as shown in the Appendix F.6.

Alternatively, if the train split of the complete target dataset is available, head probing using the entire dataset becomes feasible. This involves fine-tuning the model head using all available data while keeping the backbone weights unchanged. Recently, the Moment [10] model has achieved the SOTA results in head probing as compared to GPT4TS and Time-LLM. However, as indicated in Table 5, TTM further outperforms the results reported by Moment by 3-4%. In addition, TTM head probing results are very competitive as compared to the full end-to-end training of popular architectures as depicted in Appendix F.7. Hence, TTM, with its significantly reduced model size and the absence of compute-intensive components like self-attention, enables quick fine-tuning of models compared to the cumbersome process required by the massive Transformer models. Note that Moment is excluded from the comparison of zero/few-shot forecasting results because it does not report them.

### TTM's Effectiveness in Cross-channel and Exogenous Modeling:

Since the datasets (**D1**) used in previous experiments do not have exogenous variables, we evaluate the effectiveness of TTM on 4 other datasets (**D2**, as explained in Section 4.1) to quantify its benefits. Since these datasets are already very small, we used their full data for fine-tuning. Table 6 shows the performance of the pre-trained TTM\({}_{Q}\) model fine-tuned on the target data with exogenous mixer module and decoder channel-mixing enabled (TTM-CM). We compare TTM-CM with plain TTM finetuning and other primary SOTAs (PatchTST, TSMixer variants, and GPT4TS) trained from scratch. Specifically, we compare with TSMixer with channel-mixing enabled (TSMixer-CM) and TSMixer with cross-channel reconciliation head (TSMixer-CC) [6] as they are the latest SOTAs in channel-correlation modelling. From Table 6, we can see that TTM-CM outperforms all the competitive models with a significant margin (15-44%), thus, demonstrating the power of TTM in capturing inter-channel correlations.

### Ablation Studies:

The impacts of various techniques used in TTM are analyzed here.

**Pre-training data (Quality Vs Quantity):** Figure 3 demonstrates the vital role of both pretraining

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{} & \multicolumn{4}{c|}{**Pre-trained models fine-tuned on 5\% data**} & \multicolumn{4}{c|}{**Other popular architectures trained on 5\% data**} \\ \hline \multirow{2}{*}{Data} & **TTM\({}_{\textbf{TM}}\)** & **TTM\({}_{\textbf{TM}}\)** & **GPT4TS** & **Time-LLM** & \multicolumn{1}{c|}{PatchTST} & \multicolumn{1}{c|}{TSMixer} & \multicolumn{1}{c|}{TimeMixer} & \multicolumn{1}{c|}{ITransformer} & \multicolumn{1}{c|}{ITransNet} & \multicolumn{1}{c|}{Dlinear} \\  & (Ours) & (Ours) & (Ours) & (Nourts) & (Nourts) & (LCLR 23) & (KDD 23) & (KDD 24) & (ICLR 24) & (ICLR 23) & (AAAI 23) \\ \hline ETH1 & **0.383** & 0.385 & 0.386 & 0.682 & 0.627 & 0.695 & 0.635 & 1.088 & 0.756 & 0.926 & 0.75 \\ ETH2 & 0.324 & 0.318 & **0.314** & 0.401 & 0.382 & 0.439 & 0.385 & 0.508 & 0.437 & 0.464 & 0.828 \\ ETH1 & 0.376 & 0.378 & **0.361** & 0.472 & 0.425 & 0.526 & 0.479 & 0.578 & 0.568 & 0.717 & 0.401 \\ ETH2 & 0.272 & 0.268 & **0.253** & 0.308 & 0.274 & 0.314 & 0.297 & 0.34 & 0.309 & 0.344 & 0.399 \\ Weather & 0.234 & 0.24 & **0.229** & 0.263 & 0.261 & 0.27 & 0.268 & 0.317 & 0.297 & 0.298 & 0.264 \\ Electricity & 0.183 & 0.207 & 0.18 & 0.178 & 0.177 & **0.176** & 0.197 & 0.239 & 0.202 & 0.402 & 0.177 \\ Traffic & 0.433 & 0.437 & 0.49 & 0.434 & 0.423 & **0.418** & 0.435 & 0.503 & 0.452 & 0.867 & 0.451 \\ \hline
**Size** & **1M** & **4M** & **5M** & **84M** & **7B** & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \hline
**TTM\({}_{\textit{tmp}}\)**\(\{\data and diverse resolution sampling (DRS). Initially, the zero-shot results were unsatisfactory when pre-training TTM with the smaller Monash dataset (i.e., PT(M)). To improve performance, we introduced the DRS technique on the Monash data to increase diversity and coverage (250M PT samples). This significantly improved the results by 37%. In addition, extending the dataset size from 250M to 1B further improved the results by 6%. These experiments highlight that while the quantity of pre-training data is significant, the quality of the data, especially in terms of resolution diversity and coverage, is even more crucial for improving the model performance.

**Effect of Resolution Prefix Tuning (RPT) and Adaptive Patching(AP):** RPT enhances forecast performance, especially with large and diverse pretraining (PT) data. Adding a learnable resolution prefix token allows models to easily decouple weights across different resolutions, leading to a 3% improvement in 1B PT data setup (Table 7). RPT is also beneficial for very short context length scenarios, improving performance by 8% (Appendix F.9). On the other hand, AP generally improves the forecasting performance across all set-ups, but the impact is consistently high in less PT data settings (3% boost).

**Forecast Length Adaptations (FLA):** Given a _FL_, we can either pre-train a Direct TTM tailored for the specific _FL_ or adapt existing TTMs trained on different _FLs_. Two possible adaptations are: (i) **Pruning:** Take TTM trained on _FL_' where _FL_' >_FL_, and prune it to the required _FL_ (e.g., TTM (_FL_ = \(720\)) pruned to other reduced _FL_\(\in\{96,192,336\}\)). (ii) **Recursive:** Take TTM trained on _FL_', where _FL_' <_FL_ and do recursive prediction (of length _FL_') till we reach the required _FL_ (e.g., Extend TTM (_FL_ = \(96\)) recursively to greater _FL_\(\in\{192,336,720\}\)). Figure 4 compares these techniques. For shorter adaptation (96 to 192), recursive predictions yield the best performance and match the direct forecast results. However, for wider adaptations (336-96 or 720-96), the pruning approach gives more stable and closer results to the direct forecasts. Hence, using these approaches, TTM models can be easily adapted to various _FLs_ based on user requirements.

### TTM Model Insights & Explainability:

Figure 5 illustrates the TTM embeddings from various datasets (weather, traffic, and electricity) using PCA. From each dataset, three distant, non-overlapping, fixed-length time segments (**S-1**, **S-2**, **S-3**) are selected, each depicted with a unique marker shape. The visualization uses the first and second principal components of the TTM embeddings. The inset image focuses on the weather dataset alone, revealing a deeper structure learned by the TTM architecture. The cyclic orbits in the embeddings reflect the seasonal patterns in the data. Both

Figure 4: **FL adaptation: impact of adapting TTMs (_FL_\(720\)) and TTMs (_FL_\(96\)) to all other FLs. MSE averaged across all D1 datasets is reported for _FL_\(\in\{96,192,336,720\}\). Best viewed in data used. Average MSE of zero-shot results across _FL_\(96\), 192 reported.**

Figure 5: (a) **TTM embedding projections across 3 datasets and 3 segments within datasets. (b) Cross-channel attention based explanation.**

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & \multicolumn{2}{c|}{Less PT Data} & \multicolumn{2}{c|}{More PT Data} \\  & \multicolumn{2}{c|}{(250M)} & \multicolumn{2}{c|}{(1B)} \\ \hline Data & w/o & AP/w & AP/w & RPT/w & RPT \\ \hline ETH1 & 0.369 & **0.365** & 0.366 & **0.364** \\ ETH2 & **0.283** & 0.285 & 0.285 & **0.277** \\ ETH1 & 0.446 & **0.413** & 0.341 & **0.322** \\ ETTM2 & 0.191 & **0.187** & 0.18 & **0.171** \\ Weather & 0.159 & **0.154** & **0.153** & 0.158 \\ Electricity & 0.179 & **0.169** & 0.178 & **0.166** \\ Traffic & 0.521 & **0.518** & 0.528 & **0.514** \\ \hline
**TMIP**(\%) & **3\%** & & **3\%** \\ \hline \end{tabular}
\end{table}
Table 7: **Impact of AP and RPT: Impacts of adaptive patching (AP) in less pre-training (PT) data setting (i.e., TTM),a and resolution prefix tuning (RPT) in more pre-training (PT) data setting (i.e., TTM). Zero-shot results on FL 96 reported. [‘w’: with, ‘w/o’: ‘without’.]**hourly datasets (traffic and electricity) form concentric orbits due to similar seasonal patterns, while the weather data, with its distinct seasonal pattern, shows cyclic orbits in a different sub-dimension. In addition, the cross-channel attention from the fine-tuned model's channel mixing layers reveals feature importance across channels. As shown in Figure 5, the model focuses on channels like weather, season, holiday, and temperature to predict bike-rental counts. These attention model weights correlate with the general data characteristics where bike rental demands are heavily influenced by weather and holidays, providing explanation for the fine-tuned model predictions. More details are in Appendix G.

### Discussion on TTM Design choices:

In this section, we intuitively explain the important design choices of TTM that greatly enhance its forecasting accuracy and transfer learning capabilities despite its extremely small model capacity:

* All existing pre-trained models use a very high volume of pretraining data (for example, TimesFM used 300B and Moirai used 27B time-points), hence they naturally require massive model sizes. However, as shown in Figure.3, we observe that "limited" pretraining data with "high resolution diversity" greatly helps in time-series model generalization, as opposed to simply increasing the pretraining data size. This is an important observation and finding that resolution diversity in pretraining data is very crucial for time-series FMs. Based on these findings, we proceed with a well-reduced dataset (1B samples) with high resolution diversity which naturally reduces our model size compared to counterparts needing to pre-train with several hundred billion time-series. We introduce a high diversity in our data via Diverse Resolution Sampling technique (DRS) which our counterparts fail to do.
* Secondly, we opted for TSMixer-based models instead of transformer-based models, which further reduced the model size drastically. The TSMixer architecture has successfully established in the past that interleaving simple gated attentions with mixing components across patches, channels, and features greatly enhances forecasting accuracies with very limited model capacity, as the quadratic time-complexity of self-attentions can be entirely avoided. Following TSMixer, several other mixer architectures [33][24] have been published, re-iterating the power of these simple architectures. Thus, avoiding complex transformer architectures further reduced our model size significantly.
* In addition, we further increased the modeling power of TSMixer without drastically increasing its size by introducing several innovative components, such as adaptive patching, diverse resolution sampling, and resolution prefix tuning. These enhancements are crucial for effectively handling large pre-training across datasets with varying resolutions, all while keeping the model capacity very minimal.
* Finally, framing the pre-training objective as a direct forecasting task demonstrates improved zero-shot performance as compared to the traditional masking-based pre-training approaches. We hypothesize that this method enables the model to effectively learn complex non-linear mappings between the fixed context and forecast windows during pre-training that generalizes well to unseen datasets.

## 5 Conclusions and Future Work

We propose TTM, an extremely lightweight pre-trained model for multivariate time-series forecasting. Unlike existing large models, TTM is significantly smaller and faster, with efficient pre-training and fine-tuning workflows. Results show that TTM is highly effective in pre-training on heterogeneous datasets despite its limited model capacity. It achieves state-of-the-art results in zero/few-shot forecasting, offering significant computational efficiency while capturing cross-channel relationships and exogenous variables - critical features lacking in popular methods. Additionally, TTM supports both CPU and GPU deployments, greatly enhancing its adoption and ease of use. Moving forward, we plan to generalize our approach to support other downstream tasks beyond forecasting.

## References

* [1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical re port: A highly capable language model locally on your phone. _arXiv preprint arXiv:2404.14219_, 2024.
* [2] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, et al. Chronos: Learning the language of time series. _arXiv preprint arXiv:2403.07815_, 2024.
* [3] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. _International Conference on Machine Learning (ICML)_, 2023.
* [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _CoRR_, abs/1810.04805, 2018.
* [5] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm: A simple pre-training framework for masked time-series modeling. In _Advances in Neural Information Processing Systems_, 2023.
* [6] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '23, page 459-469, New York, NY, USA, 2023.
* [7] Hadi Fanaee-T. Bike Sharing Dataset. UCI Machine Learning Repository, 2013. DOI: [https://doi.org/10.24432/C5W894](https://doi.org/10.24432/C5W894).
* [8] Azul Garza and Max Mergenthaler-Canseco. Timegpt-1, 2023.
* [9] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. In _Neural Information Processing Systems Track on Datasets and Benchmarks_, 2021.
* [10] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: A family of open time-series foundation models. _International Conference on Machine Learning (ICML)_, 2024.
* [11] Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [12] R.J. Hyndman and G. Athanasopoulos, editors. _Forecasting: principles and practice_. OTexts: Melbourne, Australia, 2021. OTexts.com/fpp3.
* [13] Kevin Maik Jablonka, Charithea Charalambous, Eva Sanchez Fernandez, Georg Wiechers, Juliana Monteiro, Peter Moser, Berend Smit, and Susana Garcia. Machine learning for industrial processes: Forecasting amine emissions from a carbon capture plant. _Science Advances_, 9(1):eadc9576, 2023.
* [14] Arindam Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley M. Gifford, Pavithra Harsha, Stuart Siegel, Sumanta Mukherjee, and Chandra Narayanaswami. Hierarchical proxy modeling for improved hpo in time series forecasting. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '23, page 891-900, New York, NY, USA, 2023.
* [15] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [16] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597, Online, August 2021.

* [17] Zhe Li, Zhongwen Rao, Lujia Pan, Pengyun Wang, and Zenglin Xu. Ti-mae: Self-supervised masked time series autoencoders. _arXiv preprint arXiv:2301.08871_, 2023.
* [18] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann. Unitime: A language-empowered unified model for cross-domain time series forecasting. In _Proceedings of the ACM Web Conference 2024_, 2024.
* [19] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting, 2024.
* [20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.
* [21] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. M5 accuracy competition: Results, findings, and conclusions. _International Journal of Forecasting_, 2022. [https://doi.org/10.1016/j.ijforecast.2021.11.013](https://doi.org/10.1016/j.ijforecast.2021.11.013).
* [22] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _ICLR_, 2023.
* [23] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In _International Conference on Learning Representations_, 2020.
* [24] Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H Nguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, et al. Automixer for improved multivariate time-series forecasting on business and it observability data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 22962-22968, 2024.
* [25] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [26] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilos, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, et al. Lag-llama: Towards foundation models for time series forecasting. _arXiv preprint arXiv:2310.08278_, 2023.
* [27] BizITOps Dataset Repository. [https://github.com/BizITObs/BizITObservabilityData/tree/main/Complete/Time%20Series/RobotShop](https://github.com/BizITObs/BizITObservabilityData/tree/main/Complete/Time%20Series/RobotShop), 2023.
* [28] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. _International Journal of Forecasting_, 36(3):1181-1191, 2020.
* [29] Timo Schick and Hinrich Schutze. It's not just size that matters: Small language models are also few-shot learners. _arXiv preprint arXiv:2009.07118_, 2020.
* [30] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in Neural Information Processing Systems_, 34:24261-24272, 2021.
* [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [32] Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chao Li, and Wayne Xin Zhao. Libcity: An open library for traffic prediction. In _Proceedings of the 29th International Conference on Advances in Geographic Information Systems_, SIGSPATIAL '21, page 145-148, New York, NY, USA, 2021.

* [33] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In _International Conference on Learning Representations (ICLR)_, 2024.
* [34] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. _Advances in Neural Information Processing Systems_, 35:38775-38787, 2022.
* [35] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. _International Conference on Machine Learning (ICML)_, 2024.
* [36] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In _International Conference on Learning Representations_, 2022.
* [37] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _The Eleventh International Conference on Learning Representations_, 2022.
* [38] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. In _Advances in Neural Information Processing Systems_, 2021.
* [39] Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. _arXiv preprint arXiv:2312.16862_, 2023.
* [40] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 8980-8987, 2022.
* [41] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? _arXiv preprint arXiv:2205.13504_, 2022.
* [42] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2114-2124, 2021.
* [43] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. _Advances in Neural Information Processing Systems_, 35:3988-4003, 2022.
* [44] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _The Thirty-Fifth AAAI Conference on Artificial Intelligence_, volume 35, pages 11106-11115, 2021.
* [45] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _Proc. 39th International Conference on Machine Learning_, 2022.
* [46] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One Fits All: Power general time series analysis by pretrained lm. In _NeurIPS_, 2023.

## Appendix A TSMixer Background

We employed TSMixer [6] as a building block for the proposed TTM model due to its state-of-the-art performance, faster execution, and significantly lower memory usage. However, as explained in the main paper, vanilla TSMixer cannot be trained on multiple diverse datasets. Therefore, it necessitated the incorporation of the proposed novel components. In this section, we provide a high-level overview of the TSMixer model for a simpler and quicker understanding by the readers.

TSMixer is a lightweight alternative to transformer-based time series models, with no compromise on forecast accuracy. TSMixer adopts some well-established pre-processing steps from the literature, such as normalization and patching. Additionally, it offers the flexibility of enabling or disabling channel mixing. Channel mixing has been found to be beneficial in handling multivariate datasets with cross-channel correlations. For the main learning process, TSMixer employs a series of MLP-Mixer [30] blocks that perform inter-patch, intra-patch, and inter-channel mixing operations. A mixing operation in TSMixer ensures learning correlations across a specific dimension. For example, inter-channel mixing enables it to learn cross-channel correlations. In the experiments, we employed three different flavors of the TSMixer model: TSMixer vanilla (referred as TSMixer throughout the text), TSMixer with cross-channel mixing enabled (TSMixer-CM), and TSMixer with cross-channel reconciliation head (TSMixer-CC). We request the readers to refer to [6] for further details about these variants.

## Appendix B Literature Survey

### Multivariate Time Series Forecasting:

Statistical approaches for time series forecasting, such as SARIMAX and Exponential Smoothing, generally generate forecasts independently for each time series [12]. These methods are essentially univariate and do not build a single model by learning from multiple time series. On the other hand, more advanced models, built upon machine/deep learning techniques, including LightGBM-based models [21, 14], N-BEATS [23], and DeepAR [28], have the capability to learn from multiple time series. However, these models still follow univariate approaches, thus ignoring any potential cross-channel correlations.

Advanced multivariate forecasting models mostly involve deep neural networks, specifically the transformer [31] architecture. A series of transformer-based model have been proposed in the last few years including Informer [44], Autoformer [38], and FEDFormer [45]. Although these models outperformed all the prior arts, the DLinear [41] model showed that an embarrassingly simple linear model can beat these models by following a few empirically established steps like time series decomposition, normalization, and channel-independent modeling.

PatchTST [22] showed that transformers can be effective for forecasting if the input time series is patched or segregated in multiple windows, and subsequently, modeled by a transformer. The patching operation helps preserve local semantic information, accommodates a longer history, and reduces computation time. The PatchTST model outperformed all prior transformer-based models and the DLinear model.

Although PatchTST reinstated faith in transformers for time series modeling, transformer-based models are generally resource-intensive, with slow execution and a high memory footprint. The recently proposed TSMixer model [6] addresses these challenges effectively. TSMixer, built on the MLPMixer architecture [30], stands out for its exceptional speed and lightweight design. It has attained state-of-the-art (SOTA) performance on benchmark datasets, demonstrating a 2-3X reduction in both execution time and memory usage.

Recently, several new Transformer- and Mixer-based architectures have been proposed. The iTransformer model [19] applies attention and MLP modules to the inverted dimension. Instead of operating on the temporal tokens, these operations are applied to the variate tokens, resulting in "variate-unmixed representations". This approach is claimed to enhance generalization across different channels and improve the use of arbitrary context lengths. The TimeMixer model [33] leverages the observation that time series exhibit unique patterns at different sampling scales. By utilizingdifferent MLPMixer blocks, it aims to capture both microscopic and macroscopic information to produce more accurate forecasts. Similarly, the recent TimesNet model [37] disentangles the complex multi-periodicity in a time series into intra-period and inter-period variations. It then learns time series representations using an Inception block, enhancing the model's ability to capture intricate patterns in the data.

### Pre-trained Models for Time Series:

One major drawback of all the above models is that they need to be trained in-domain. Hence, none of these models can be transferred to out-of-domain data with zero or minimal training. This approach has been found to be extremely beneficial in the natural language processing (NLP) domain with the invention of BERT [4] and GPT [25] models.

However, this is an extremely challenging task in the time series domain because of the unavailability of a publicly accessible large pre-training corpora. There are multiple independent time series datasets, but, unlike in NLP, these datasets differ significantly in important characteristics such as the domain of the data (e.g., retail, sensor data, traffic, etc.), the number of channels, temporal resolution, and length. This makes it hard to train a single model on all the datasets together.

Hence, a few prior works have focused on experimenting with _same-dataset_ self-supervised learning for time series [17, 34, 36, 40]. These methods learn a time series representation from the _train_ split of a dataset, build a forecaster on top of the learned representation on the same data, and then evaluate it on the _test_ split of the same dataset. Although these approaches have demonstrated promising results, they do not provide evidence of the transfer capability of the model between datasets.

Subsequently, models such as SimMTM [5] and TF-C [43] have demonstrated the transfer capabilities of their models between pairs of datasets. These pairs are carefully chosen so that the _source_ (the dataset where the model is pre-trained) and _target_ (the dataset where the model is fine-tuned and tested) datasets share some matching properties. For instance, SimMTM showcased its few-shot capability by selecting ETTH2 as the source data and ETTH1 as the target data. Both ETTH1 and ETTH2 are collected from Electricity Transformers at two stations, denoting data from a similar domain. TF-C demonstrated the transferability of the model across four different (source, target) pairs, such as (ECG, EMG) and (FD-A, FD-B), where domain-similarity exists in both the source and target datasets.

To overcome this limitation, the time series research community is increasingly focused on developing General Pre-Trained (GPT) or Foundation Models (FM) for time-series forecasting, capable of effectively transferring knowledge to new target TS datasets. This growing interest led to the release of several "large" and "massive" pre-trained time-series models for forecasting in early 2024, generating significant excitement among researchers. Notable releases include Moment [10], TimesFM [3], Chronos [2], Moirai [35], and Lag-llama [26], all of which set strong benchmarks in zero-shot forecasting. The Moment [10] model pre-trains a Transformer encoder model in a univariate way on a collected set of diverse "Time Series Pile". Moment is pre-trained with mask reconstruction objective, and it can be fine-tuned on a downstream forecasting task. The TimesFM [3] pre-trains a decoder-style attention model (with causal self-attention) in univariate fashion on a large collection of real world and synthetic datasets. The Chronos [2] model tokenizes the input time series, and feed the tokens into a large language model (specifically the T5 model). Chronos is pre-trained in a univariate fashion. During inference, Chronos auto-regressively samples tokens and map them to the numerical values via dequantization. Chronos is trained on a large corpora of time series including synthetic data for better generalization. The Moirai [35] model pre-trains a Transformer encoder on a massive collection of "LOTSA" dataset (27B time points). Moirai masks the forecast horizon of each target channel and performs mask reconstruction. The flattening operation of all channels in a multivariate time series enables Moirai to pre-train on "any-variate" settings. The Lag-Llama [26] model pre-trains a decoder-only Transformer model that utilizes the time series lags as covariates. Lag-Llama is pre-trained on a large collection of diverse time series datasets in a univariate fashion. All the above models are open-sourced and used in our experiments for comparison. However, closed-source models such as TimeGPT [8] are not included due to their inaccessibility.

### Pre-trained LLMs for Time Series:

Parallel to the above trend of general pre-trained TS models, there has been a notable increase in the adoption of pre-trained large language models (LLMs) for time series tasks. These models are approached as cross-domain transfer learning problems. The LLMTime model [11] feeds the time series values as text representations and demonstrates promising performance in a zero-shot setting. The GPT4TS model [46] adopts a pre-trained LLM like GPT and fine-tunes only the input embedding layer, normalization layers, and output layer. Specifically, it does not alter the self-attention weights and feed-forward layers. The Time-LLM [15] model proposed a reprogramming framework, where they reuse existing LLMs for forecasting while keeping the LLM backbone intact. The overall approach to building a pre-trained model for time series from LLMs is promising, but it does not model cross-channel correlations observed in many multivariate time series datasets. Moreover, these LLMs are very large and exhibit slow execution and a large memory footprint.

## Appendix C Datasets

### List of Pre-training Datasets:

Pre-training employs a subset of \(\sim\)1B samples from Monash [9] and Libcity [32, 35] data collection, where Monash results in \(\sim\)250M samples and LibCity accounts for the rest. In this estimate, one sample denotes a pair of training windows: \(X\in\mathbb{R}^{1\times sl}\) and \(Y\in\mathbb{R}^{1\times fl}\). We employ a subset of the datasets available in the Monash forecasting data repository [9] available at [https://forecastingdata.org/](https://forecastingdata.org/). Since our primary focus in this study is long term forecasting with forecast length ranging from 96 to 720, it is not possible to use yearly, monthly, quarterly, or weekly datasets due to their short lengths. Hence, we skip a few datasets of short lengths. The Monash datasets used are available under a Creative Commons Attribution 4.0 International license. For LibCity, we employ all datasets released by the Moiriat authors [35], available at [https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main](https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main) (except the Rotterdam dataset which was not available during our experimentation). The LibCity datasets at the above link were released under an Apache 2.0 license. The final list of all pre-training datasets is shown in Table 8. Please note that, the last three datasets in the Libcity section have been excluded from the pre-training process for the model releases intended for enterprise-use.

#### c.1.1 Temporal cross-validation:

Temporal cross-validation is used to chronologically split all the time series into train and validation parts. During pre-training, moving windowing technique is used to create \((\mathbf{X},\mathbf{Y})\) pairs of lengths \(sl\) and \(fl\) respectively. Please note that, these pre-training datasets have no overlap with the evaluation datasets. Specifically, the aus

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Source** & \multicolumn{1}{c|}{**Dataset**} & \multicolumn{1}{c|}{**Resolution**} \\ \hline  & kaggle\_web\_trailing\_dataset\_without\_missing\_values & daily \\  & m\_5daily\_dataset\_without\_missing\_values & daily \\  & solar\_10\_minutes\_dataset\_+ **Downsample** & 10 mins, 30 mins, hourly \\  & australian\_electricity\_demand\_dataset\_+ **Downsample** & 30 mins, hourly, daily \\  & solar\_4\_seconds\_dataset\_+ **Downsample** & 4 seconds, 10 mins, 15 mins, 30 mins, hourly \\  & wind\_4\_seconds\_dataset\_+ **Downsample** & 4 seconds, 10 mins, 15 mins, 30 mins, hourly \\
**Monash** & us\_birth\_dataset & daily \\  & sangopathy\_dataset\_without\_missing\_values & daily \\  & sangopathy\_dataset\_without\_missing\_values & daily \\  & australian\_weather\_dataset\_dataset\_with\_missing\_values & hourly \\  & kdd\_cup\_2018\_dataset\_without\_missing\_values & daily \\  & bitcoin\_dataset\_without\_missing\_values & daily \\  & wind\_farms\_minutely\_dataset\_without\_missing\_values + **Downsample** & minutely, 10 mins, 15 mins, 30 mins, hourly \\  & london\_start\_meters\_dataset\_without\_missing\_values + **Downsample** & 30 mins, hourly, daily \\ \hline  & PEMSO3 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & PEMSO4 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & PEMSO7 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & PEMSO8 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & PEMS8 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & PEMS8 **+ Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & LOS **LOOP + Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & LOOP\_SEATTLE + Downsample** & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & LOOP\_SEATTLE + Downsample & 5 mins, 10 mins, 15 mins, 30 mins, hourly \\  & SZ\_TAXI **+ Downsample** & 15 mins, 30 mins \\  & Q-TRAFFIC + Downsample & 15 mins, 30 mins, hourly \\ \hline \end{tabular}
\end{table}
Table 8: List of pre-training datasets. A dataset with “+ **Downsample**” denotes that the proposed Diversity Resolution Sampling (DRS) has been applied on that dataset to generate new diverse datasets at frequencies lower than the original frequency of the data. Please note that, these pre-training datasets have no overlap with the evaluation datasets. Specifically, the australian\_electricity\_demand\_dataset and australian\_weather\_dataset used in pre-training are completely different (_w.r.t_ location, measured variables, type, resolution, length, etc.) from the standard Electricity (ECL) and Weather dataset used in the evaluation. Please note that, the last three datasets in the Libcity section have been excluded from the pre-training process for the model releases intended for enterprise-use.

tralian_electricity_demand_dataset and australian_weather_dataset used in pre-training are completely different (_w.r.t_ location, measured variables, type, resolution, length, etc.) from the standard Electricity (ECL) and Weather dataset used in the evaluation.

### List of Evaluation Datasets:

Table 9 illustrates various characteristics of the eleven evaluation datasets. Below, we present the details.

**Set D1:** For zero/few/full-shot evaluation, we utilize seven multivariate time series datasets that have consistently been employed in the literature. Below, we offer a brief overview of these datasets.

1. **ETT datasets:** The four ETT datasets [44] (ETTH1, ETTH2, ETTM1, ETTM2) contain multivariate time series data collected from electrical transformers at two stations. ETTH1 and ETTH2 are collected at an hourly interval, while ETTM1 and ETTM2 are collected every 15 minutes. All four datasets have 7 channels.
2. **Weather:** The weather dataset consists of 21 channels, which serve as weather indicators. It is collected at 10-minute intervals at the Max Planck Institute of Biogeochemistry weather station.
3. **Electricity (ECL):** The Electricity dataset, also known as the ECL dataset, comprises the hourly electricity consumption data of 321 clients.
4. **Traffic:** This dataset records the hourly rates of road occupancy on the San Francisco Freeways using 862 sensors.

We used the datasets provided in the repository of the Autoformer paper [38]7. For all the D1 datasets, we execute the same train/validation/test splitting as was performed in the literature [44, 38, 22, 6].

Footnote 7: Available at [https://github.com/thuml/Autoformer](https://github.com/thuml/Autoformer)

**Set D2:** To assess the effectiveness of the proposed TTM model in extracting information from exogenous channels, we conduct evaluations on four additional datasets that are known to contain exogenous or control variables.

1. **Bike Sharing (BS):** The Bike Sharing dataset [7] documents the hourly rental counts of bikes from the Capital Bikeshare system in Washington D.C., USA, spanning the years 2011 to 2012. Rental counts are typically associated with environmental and seasonal conditions. Consequently, this 14-channel dataset encompasses various weather-related features. Our goal was to forecast all three rental counts: "casual", "registered", and "cnt" (total count). As the remaining 11 features are consistently available at all future time points, they are treated as exogenous variables in our experiment.
2. **Carbon Capture Plant (CC):** The Carbon Capture Plant data [13] records the emission profiles of "2-amino-2-methyl-1-propanol" (AMP) and "piperazine" (Pz) collected at every 2 minutes interval. We utilize the 8-channel dataset made available in the official repository [13]. Among the remaining 6 channels, the following 5 serve as control variables:

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline Set & Dataset & Resolution & Length & Total & \#Target & \#Exog. & Source \\  & & & & \#Channels & variables & variables & Source \\ \hline \multirow{10}{*}{**D1**} & ETTH1 & 1 hour & 17420 & \multirow{4}{*}{7} & \multirow{4}{*}{7} & \multirow{4}{*}{Not Applicable} & \multirow{4}{*}{[38]} \\ \cline{2-3} \cline{5-8}  & ETTH2 & 1 hour & 17420 & & & & \\ \cline{1-1} \cline{3-8}  & ETTH1 & 15 minutes & 69680 & & & & \\ \cline{1-1} \cline{2-8}  & ETTM2 & 15 minutes & 69680 & & & & \\ \cline{1-1} \cline{2-8}  & Weather & 10 minutes & 52696 & 21 & 21 & & [38] \\ \cline{1-1} \cline{2-8}  & ECL & 1 hour & 26304 & 321 & 321 & & [38] \\ \cline{1-1} \cline{2-8}  & Traffic & 1 hour & 17544 & 862 & 862 & & [38] \\ \hline \multirow{10}{*}{**D2**} & BS & 1 hour & 17379 & 14 & 3 & 11 & [7] \\ \cline{1-1} \cline{2-8}  & CC & 2 minutes & 5409 & 8 & 2 & 5 & [13] \\ \cline{1-1} \cline{2-8}  & APP & 10 seconds & 8834 & 39 & 4 & 35 & [27] \\ \cline{1-1} \cline{2-8}  & SER & 10 seconds & 8835 & 107 & 72 & 35 & [27] \\ \hline \end{tabular}
\end{table}
Table 9: Details of the evaluation datasets.

["TI-19","FI-19", "TI-3", "FI-11", "FI-1213"]. The remaining 1 variable is treated as a conditional variable (as it is neither a target variable nor available during the forecast period to consider it as exogenous). For additional details, please refer to the supplementary materials of [13].
3. **Service (SER):** This dataset pertains to the cloud-based "Stan's Robot Shop" application, managed by Instana. It simulates a user's e-commerce experience, encompassing site access to shipping, utilizing a load generator. Intermittent fault injection introduces diverse IT events. The dataset provides business KPIs for services (e.g., payment, catalog) and IT events tracked by Instana. Sampling occurs every 10 seconds due to high traffic and event frequency. For our experiments, all business KPIs are treated as target variables and IT events are treated as exogenous variables and the goal of our forecasting is to predict the business KPIs given the IT events.
4. **Application (APP):** This dataset is similar to the SER data, but it captures KPIs for the entire application instead of capturing at the service level. Even in this case, all business KPIs are treated as target variables and IT events are treated as exogenous variables and the goal of our forecasting is to predict the business KPIs given the IT events.

## Appendix D TTM Model Hyper-parameters and Baselines

**D.1 Pretraining:** Pre-training is performed in a distributed fashion with 50 CPUs and 6 NVIDIA A100 GPUs. Standard model configurations are as follows: patch length \(pl\) = 64 (when \(sl\) is 512), 128 (when \(sl\) is 1024 or 1536) and 8 (when \(sl\) is 96); stride \(s\) = \(pl\) (i.e., non-overlapping patches), number of patches \(n=sl/pl\), number of levels in backbone \(L\) = 3, number of TTM blocks per level \(M\) = 2, number of decoder layers = 2, batch size \(b\) = 4500, number of epochs \(ep\) = 20, and dropout \(do\) = 0.4. PatchTSMixer-specific hyperparameters include feature scaler \(fs\) = 3, hidden feature size \(hf\) = \(fs*pl\), expansion feature size \(ef=hf*2\). Please note that \(hf\) and \(n\) will change across TTM blocks based on the adaptive patching strategy. Resolution prefix tuning is enabled by default on all variants other than TTM\({}_{Q}\). Decoder channel-mixing and exogenous mixer blocks are disabled during pre-training and enabled during fine-tuning based on the dataset requirement.

### Fine-tuning:

Most model parameters remain the same from pretraining except the following parameters. Head dropout is changed during finetuning based on the target dataset used (0.7 for smaller ETT datasets and 0.2 for other datasets). Likewise, the batch size is set to 8 for Traffic, 32 for Electricity, and 64 for all other datasets. Moreover, decoder channel-mixing and exogenous mixer block are enabled for datasets that need cross-channel modelling (i.e. D2 datasets). Unlike pre-training, fine-tuning is executed in just 1 A100 GPU as it is a fast process. All these hyper-parameters are selected based on the validation performance, and the final test results are reported in the paper.

**D.3 Computational Benefits of TTM over existing models - Setup details:** Table 3 compares the computational benefits of TTM over existing TS-pretrained models and reports the following metrics: (i) GPU Inference Time per batch (in milliseconds (ms)), (ii) CPU Inference Time per batch (in seconds (s)), (iii) Max GPU Memory used during inference (in GB), (iv) Params: Total parameters of the models (in Millions). Experiments are conducted using \(sl\) = 512, \(fl\) = 96, and batch size = 32 in one A100 80GB GPU, 16 cores with 256GB memory. GPU is not enabled while capturing the CPU time. Since many pre-trained models process data in a purely univariate fashion, while TTM processes data in a multi-variate fashion, we set the number of channels \(c\) to 1 for this experiment so that, the number of samples per batch remains the same across all models for a fair comparison. In addition, we used a small batch size of 32 for this experiment, as many pre-trained models (like Chronos\({}_{L}\)) were encountering out-of-memory (OOM) errors with high batch sizes. For this experiment, we set the number of probabilistic samples to 1 (i.e., num_samples = 1) for probabilistic algorithms (such as Chronos or Lag-Llama) to compute their fastest possible runtime. Note, that for forecast accuracy comparison, we set the number of samples to 100 for Lag-Llama and 20 for Chronos as suggested in their open-source code examples. All the baselines algorithms were evaluated using their open-sourced inference APIs as detailed in Section D.4. Please note that the computational benefits of TTM will further amplify if we use higher batch sizes or high number of channels as our models are extremely small and can process multiple channels at the same time using the channel-independence approach [22].

[MISSING_PAGE_FAIL:19]

Other zero-shot comparisons:LLMTime [11] reported the test performance only on the last windows of the test datasets (instead of sliding windows) for horizons 96 and 192 due to computational reasons. We recreate the same experimental setup for TTM, and depict the comparative results in Table 20. We observe 26-36% improvement across all three variants of TTM with tremendous (70,000 to 14,000) reduction in model sizes. Another zero-shot comparison with the UniTime [18] model is shown in Table 21. In this comparison, TTM outperforms UniTime by 29-31%.

### TTM Zero-shot _vs._ SOTA Full-shot (short context setting):

In Table 14 we compare the zero-shot results of TTM variants with full-shot end-2-end training of popular TS architectures like iTransformer, PatchTST _etc._. The full-shot SOTA algorithms were trained in short-context length setting (\(sl=96\)) on the train split of each target dataset, and these results are obtained from the Moirai paper [35] where the authors draw similar comparison. TTM was tested in zero-shot setting without any training on the target datasets. We also provide the zero-shot results of Moirai variants and TimesFM from Table 1 for reference purpose. We can see that the zero-shot performance of all variants of TTM outperforms the full-shot performance of SOTA models, even though the latter are trained on the target datasets. This underscores the strength of the pre-trained TTM model.

### Full table for 5% few-shot experiment:

Table 16 shows the 5% few-shot results for all forecast lengths across all D1 datasets.

### TTM _vs._ Cross-transfer models:

Table 22 draws a comparative analysis of TTM\({}_{Q}\) with SimMTM, Ti-MAE, TST, LaST, TF-C, CoST, and TS2Vec models in different few-shot settings (10% to 100% availability of training data) on ETH1 dataset. The baseline models are trained on ETH2 data, and tested on ETH1 data, thus demonstrating their transferability across datasets having similar characteristics. The baseline numbers are taken from [5]. TTM\({}_{Q}\) outperform all of them (including the recent SOTA SimMTM) by a significant margin. This highlights the usefulness of the pre-trained TTM weights and their ability to adapt to a target domain with few-shot fine-tuning.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & & FL & TTM\({}_{Q}\) & TTM\({}_{B}\) & TTM\({}_{E}\) & TTM\({}_{A}\) \\ \hline \multirow{8}{*}{TTM} & 96 & 0.365 & 0.364 & 0.363 & **0.359** \\  & 192 & 0.393 & **0.386** & 0.393 & 0.389 \\  & 336 & 0.415 & **0.404** & 0.406 & 0.405 \\  & 720 & 0.538 & **0.424** & 0.452 & 0.448 \\ \hline \multirow{8}{*}{TTM} & 96 & 0.285 & 0.277 & 0.271 & **0.264** \\  & 192 & 0.341 & 0.334 & 0.324 & **0.321** \\  & 336 & 0.383 & 0.362 & 0.357 & **0.351** \\  & 720 & 0.441 & 0.408 & **0.388** & 0.395 \\ \hline \multirow{8}{*}{TTM} & 96 & 0.413 & 0.322 & 0.327 & **0.318** \\  & 192 & 0.476 & 0.376 & 0.377 & **0.354** \\  & 336 & 0.553 & 0.407 & 0.395 & **0.376** \\  & 720 & 0.737 & 0.439 & 0.419 & **0.398** \\ \hline \multirow{8}{*}{TTM} & 96 & 0.187 & 0.171 & 0.178 & **0.169** \\  & 192 & 0.261 & 0.238 & 0.238 & **0.223** \\  & 336 & 0.323 & 0.304 & 0.29 & **0.276** \\  & 720 & 0.436 & 0.41 & 0.379 & **0.342** \\ \hline \multirow{8}{*}{TTM} & 96 & **0.154** & 0.158 & 0.166 & 0.159 \\  & 192 & **0.203** & 0.206 & 0.214 & **0.203** \\  & 336 & 0.256 & 0.256 & 0.254 & **0.247** \\  & 720 & 0.329 & 0.328 & 0.319 & **0.314** \\ \hline \multirow{8}{*}{TTM} & 96 & 0.169 & 0.166 & 0.157 & **0.152** \\  & 192 & 0.196 & 0.191 & **0.174** & 0.179 \\ \cline{1-1}  & 336 & 0.209 & 0.207 & 0.195 & **0.193** \\ \cline{1-1}  & 720 & 0.264 & 0.255 & 0.25 & **0.243** \\ \hline \multirow{8}{*}{TTM} & 96 & 0.518 & 0.514 & 0.476 & **0.462** \\ \cline{1-1}  & 192 & 0.548 & 0.544 & 0.5 & **0.491** \\ \cline{1-1}  & 336 & 0.55 & 0.575 & 0.51 & **0.509** \\ \cline{1-1}  & 720 & 0.605 & 0.622 & 0.571 & **0.547** \\ \hline \multirow{8}{*}{TTM} & **Model Size** & **1M** & **1M** & **4M** & **5M** \\ \cline{1-1} \cline{2-2}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \end{tabular}
\end{table}
Table 11: Zero-shot results of all TTM variants on D1 data benchmark across all sliding test windows (standard test protocol).

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

[MISSING_PAGE_FAIL:25]

Figure 6: Sample TTM Zero-shot Forecasts across datasets

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & FL & TTM\({}_{B}\) & TTM\({}_{E}\) & TTM\({}_{A}\) & LLMTime \\ \hline \hline  & 96 & 0.277 & 0.271 & **0.264** & 0.306 \\  & 192 & 0.334 & 0.324 & **0.321** & 0.389 \\  & 336 & 0.362 & 0.357 & **0.351** & 0.424 \\  & 720 & 0.408 & **0.388** & 0.395 & 0.434 \\ \hline  & 96 & **0.158** & 0.166 & 0.159 & 0.21 \\  & 192 & 0.206 & 0.214 & **0.203** & 0.264 \\  & 336 & 0.256 & 0.254 & **0.247** & 0.326 \\  & 720 & 0.328 & 0.319 & **0.314** & 0.402 \\ \hline  & 96 & 0.166 & 0.157 & **0.152** & 0.409 \\  & 192 & 0.191 & **0.174** & 0.179 & 0.41 \\  & 336 & 0.207 & 0.195 & **0.193** & 0.439 \\  & 720 & 0.255 & 0.25 & **0.243** & 0.487 \\ \hline  & \multicolumn{5}{c}{299} & \multicolumn{5}{c}{299} \\  & **TTM\({}_{E}\) **f-imp(\%)** & \multicolumn{5}{c}{30\%} \\ \cline{2-6}  & **TTM\({}_{B}\) **f-imp(\%)** & \multicolumn{5}{c}{31\%} \\ \cline{2-6}  & **TTM\({}_{E}\) **f-imp(\%)** & \multicolumn{5}{c}{299\%} \\ \cline{2-6}  & **TTM\({}_{E}\) **f-imp(\%)** & \multicolumn{5}{c}{30\%} \\ \cline{2-6}  & **TTM\({}_{A}\) **f-imp(\%)** & \multicolumn{5}{c}{31\%} \\ \hline \end{tabular}
\end{table}
Table 21: TTM vs UniTime MSE Improvement (_f-imp_) in zero-shot setting using full sliding-window test set. Results reported in UniTime [18] are used for this comparison.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & \multicolumn{2}{c|}{Less PT Data} & \multicolumn{2}{c|}{More PT Data} \\  & \multicolumn{2}{c|}{(250M samples)} & \multicolumn{2}{c|}{(1B samples)} \\ \hline \hline Data & w/o RPT & w/o RPT & w/o RPT & w/o RPT \\ \hline \hline ETH1 & **0.305** & 0.36 & 0.366 & **0.364** \\ ETH2 & 0.285 & **0.28** & 0.285 & **0.277** \\ ETTM1 & 0.413 & **0.384** & 0.341 & **0.322** \\ ETTM2 & **0.187** & 0.194 & 0.18 & **0.171** \\ Weather & **0.154** & 0.16 & **0.153** & 0

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & \multicolumn{2}{c|}{**[T\(\mathbf{M}_{Q}\)\(SL=\mathbf{96}\): \(FL=\mathbf{24}\)]**} \\ \hline  & **w/o RPT** & **w/o RPT** \\ \hline ETTH1 & 0.373 & **0.358** \\ ETTH2 & 0.180 & **0.179** \\ ETTM1 & 0.559 & **0.387** \\ ETTM2 & 0.127 & **0.108** \\ Weather & **0.103** & **0.103** \\ Electricity & 0.208 & **0.201** \\ Traffic & 0.754 & **0.740** \\ \hline
**NPT** (\%) & **8\%** \\ \hline \end{tabular}
\end{table}
Table 25: Impact of RPT in less context setting (\(SL=96\)). Zero-shot results on FL 24 reported. RPT helps in scenarios when the context length (\(sl\)) is short. In these scenarios, automatically detecting the resolution becomes a challenge for the model. Hence, by explicitly fusing the resolution information as a prefix, we can enhance the model’s ability to learn effectively across resolutions. [‘w/’: with, ‘w/o’: ‘without’.]

Figure 7: Data segments for TTM embedding analysis.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes. The main claims made in the abstract and introduction (Section 3) accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Appendix Section H explains the limitations of this work and future directions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: Not applicable, as this work is grounded more on large-scale experimentation and empirical analysis. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide detailed information on the full experimental setup, model hyperparameters and dataset details. We also provide information on how each result of every baseline is reported. Refer to Section D, C.2, C.1, D.4 for more details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes. The source code and model weight links for reproducibility are shared in the abstract. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The relevant details are provided in Section D Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The proposed approach and many of the associated baseline papers do not report error bars as our experiments fall under Foundation Models, which are computationally very expensive to pre-train for multiple seeds. However, TTM is compared with other state-of-the-art models across multiple settings (4 different forecast lengths and 3 different variants), wherein, TTM outperforms the baselines consistently in all these experiments, to give substantial evidence for our claims. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experimental section has all the relevant details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have reviewed the details and we conform to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Not Applicable, as our work does not directly relate to the societal impacts in the ecosystem.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable to our work, as no such misuse has been reported. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, we followed the licensing and terms of use very carefully while architecting our design. All assets used in this work will be credited properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. ** For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, all source code files and associated scripts are well documented and will further be improved before open-source release. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable to our work because no human participants were involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable to our work because no human participants were involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.