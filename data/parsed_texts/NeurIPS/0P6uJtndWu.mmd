# Efficient Diffusion Policies for Offline Reinforcement Learning

 Bingyi Kang Xiao Ma Chao Du Tianyu Pang Shuicheng Yan

Sea AI Lab

{bingykang,yusufma555,duchao0726}@gmail.com {tianyupang,yansc}@sea.com

equal contribution

###### Abstract

Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL [37] significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (_e.g._, policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose _efficient diffusion policy_ (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time **from 5 days to 5 hours** on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp.

## 1 Introduction

Offline reinforcement learning (RL) is much desired in real-world applications as it can extract knowledge from previous experiences, thus avoiding costly or risky online interactions. Extending online RL algorithms to the offline domain faces the distributional shift [9] problem. Existing methods mainly focus on addressing this issue by constraining a policy to stay close to the data-collecting policy [6, 39], making conservative updates for Q-networks [17, 15, 40], or combining these two strategies [21, 12]. However, Offline RL can also be viewed as a state-conditional generative modeling problem of actions, where the parameterization of the policy network is important but largely overlooked. Most offline RL works follow the convention of parameterizing the policy as a diagonal Gaussian distribution with the learned mean and variance. This scheme might become inferior when the data distribution is complex, especially when offline

Figure 1: Efficiency and Generality. D-QL is Diffusion-QL. _Left_: The training time on the locomotion tasks in D4RL. _Right_: the performance of EDP and previous SOTA on each domain in D4RL. EDP is trained with TD3 on locomotion and IQL on the other three domains. (Best viewed in color.)data are collected from various sources and present strong multi-modalities [32]. Therefore, more expressive models for the policy are strongly desired.

Recently, Diffusion-QL [37] made a successful attempt by replacing the diagonal Gaussian policy with a diffusion model, significantly boosting the performance of the TD3+BC [6] algorithm. Diffusion models [34; 10] have achieved the new state-of-the-art (SOTA) in image generation tasks [26; 5], demonstrating a superior ability to capture complex data distributions.

Despite the impressive improvement that Diffusion-QL has achieved, it has two critical drawbacks preventing it from practical applications. _First_, training a diffusion policy with offline RL is computationally inefficient. Consider a parameterized diffusion policy \(\pi_{\theta}(\bm{a}|\bm{s})\), Diffusion-QL optimizes it by maximizing the Q value \(Q(\bm{s},\bm{a}_{\theta})\) of a state \(\bm{s}\) given a policy-generated action \(\bm{a}_{\theta}\sim\pi_{\theta}(\bm{a}\mid\bm{s})\). However, sampling from a diffusion model relies on a long parameterized Markov chain (_e.g._, 1,000 steps), whose forward inference and gradient backpropagation are unaffordably expensive. _Second_, diffusion policy is not a generic policy class as it is restricted to TD3-style algorithms. As computing the sample likelihood \(\pi_{\theta}(a\mid s)\) is intractable in diffusion models [35], diffusion policy is incompatible with a large family of policy gradient algorithms (_e.g._, V-Trace [22], AWR [28], IQL [15]), which require a tractable and differentiable log-likelihood \(\log\pi_{\theta}(\bm{a}|\bm{s})\) for policy improvement.

In this work, we propose _efficient diffusion policy_ (EDP) to address the above two limitations of diffusion policies. Specifically, we base EDP on the denoising diffusion probabilistic model (DDPM) [10], which learns a noise-prediction network to predict the noise used to corrupt an example. In the forward diffusion process, a corrupted sample follows a predefined Gaussian distribution when the clean example and timestep are given. In turn, given a corrupted sample and predicted noise, we can approximate its clean version by leveraging the reparametrization trick. Based on this observation, to avoid the tedious sampling process, we propose _action approximation_ to build an action from a corrupted one, which can be easily constructed from the dataset. In this way, each training step only needs to pass through the noise-prediction network once, thus substantially reducing the training time. As experimented, by simply adding action approximation, we obtain **2x** speed-up without performance loss. Moreover, we apply DPM-Solver [20], a faster ODE-based sampler, to further accelerate both the training and sampling process. Finally, to support likelihood-based RL algorithms, we leverage the evidence lower bound for the likelihood developed in DDPM and approximate the policy likelihood from a constructed Gaussian distribution with variance fixed and mean obtained from action approximation.

We evaluate the efficiency and generality of our method on the popular D4RL benchmarking, as shown in Fig. 1. We first benchmark the efficiency of EDP on gym-locomotion tasks. By replacing the diffusion policy in Diffusion-QL with our EDP, the training time of Diffusion-QL is reduced substantially **from five days to five hours** (compared to their official code). Meanwhile, we observe slight to clear performance improvements on different tasks as the improved efficiency enables training DDPM with more timesteps than before. Moreover, we plug EDP into three different offline algorithms (including TD3+BC, CRR, and IQL), and the results justify its superiority over standard diagonal Gaussian policies. As a result, EDP set up new state-of-the-art on all four domains in D4RL.

## 2 Related Work

Offline RLDistributional shift between the learned and behavior policies is offline RL's biggest challenge. Existing research mitigates this problem by making modifications to policy evaluation [21; 17; 15; 24] or policy improvement [9; 39; 6; 33; 38; 42; 41]. For example, conservative Q-learning (CQL) [17] penalizes out-of-distribution actions for having higher Q-values, proving that this is equivalent to optimizing a lower bound of Q-values. Onestep RL [1] conducts policy evaluation on in-distribution data to avoid querying unseen actions. IQL [15] introduces expectile regression [14] to approximate dynamic programming with the Bellman optimality function. TD3+BC explicitly constrains the learned policy by adding a behavior cloning loss to mimic the behavior policy. Instead, CRR and AWR impose an implicit policy regularization by performing policy gradient-style policy updates. Despite their effectiveness, they ignore that the capacity of a policy representation plays a vital role in fitting the data distribution. This paper instead focuses on an orthogonal aspect (_i.e._, policy parameterization) that all the above methods can benefit. Another line of work tries to cast offline RL as a sequence-to-sequence model [3; 11], which is beyond the scope of this work.

Policy ParametrizationDifferent RL algorithms may pose different requirements for parameterizing a policy distribution. There are mainly two categories of requirements: 1) The sampling process is differentiable, such as the deterministic policy in DDPG [18] and TD3 [7]. 2) The log-likelihood of samples is tractable. For example, policy gradient methods [30; 31; 38; 28] optimize a policy based on maximum likelihood estimation (MLE). Therefore, most works represent policy with a diagonal Gaussian distribution with mean and variance parameterized with a multi-layer perceptron (MLP). On the other hand, BCQ [9] and BEAR [16] choose to model policy with a conditional variational autoencoder (CVAE). Recently, Diffusion-QL [37] introduced diffusion models into offline RL and demonstrated that diffusion models are superior at modeling complex action distributions than CVAE and diagonal Gaussian. However, it takes tens to hundreds more time to train a diffusion policy than a diagonal Gaussian one. Moreover, diffusion policy only satisfies the first requirement, which means many other offline RL algorithms can not use it, including the current SOTA IQL.

Our method is motivated to solve the about two limitations. We first propose a more efficient way to train diffusion policies, which reduces training time to the level of a Gaussian policy. Then, we generalize the diffusion policy to be compatible with MLE-based RL methods.

## 3 Preliminaries

### Offline Reinforcement Learning

A decision-making problem in reinforcement learning is usually represented by a Markov Decision Process (MDP): \(\mathcal{M}=\{\mathcal{S},\mathcal{A},P,R,\gamma\}\). \(\mathcal{S}\) and \(\mathcal{A}\) are the state and action spaces respectively, \(P(\boldsymbol{s}^{\prime}|\boldsymbol{s},\boldsymbol{a})\) measures the transition probability from state \(\boldsymbol{s}\) to state \(\boldsymbol{s}^{\prime}\) after taking action \(\boldsymbol{a}\) while \(R(\boldsymbol{s},\boldsymbol{a},\boldsymbol{s}^{\prime})\) gives the reward for the corresponding transition, \(\gamma\in[0,1)\) is the discount factor. A policy \(\pi(\boldsymbol{a}|\boldsymbol{s})^{2}\) describes how an agent interacts with the environment. The optimal policy \(\pi^{*}(\boldsymbol{a}|\boldsymbol{s})\) is the one achieves maximal cumulative discounted returns: \(\pi^{*}=\arg\max\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r( \boldsymbol{s}_{t},\boldsymbol{a}_{t})\right]\). Reinforcement learning algorithms frequently rely on the definition of value function \(V(\boldsymbol{s})=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r( \boldsymbol{s}_{t},\boldsymbol{a}_{t})|\boldsymbol{s}_{0}=\boldsymbol{s}\right]\), and action value (Q) function \(Q(\boldsymbol{s},\boldsymbol{a})=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^{t}r(\boldsymbol{s}_{t},\boldsymbol{a}_{t})|\boldsymbol{s}_{0}= \boldsymbol{s},\boldsymbol{a}_{0}=\boldsymbol{a}\right]\), which represents the expected cumulative discounted return of a policy \(\pi\) given the initial state \(\boldsymbol{s}\) or state-action pair \((\boldsymbol{s},\boldsymbol{a})\).

In the offline RL setting, instead of learning from interactions with the environment, agents focus on learning an optimal policy from a previously collected dataset of transitions: \(\mathcal{D}=\{(\boldsymbol{s}_{t},\boldsymbol{a}_{t},\boldsymbol{s}_{t+1},r_ {t})\}\). Offline RL algorithms for continuous control are usually based on an actor-critic framework that alternates between policy evaluation and policy improvement. During policy evaluation, a parameterized Q network \(Q_{\phi}(\boldsymbol{s},\boldsymbol{a})\) is optimized based on approximate dynamic programming to minimize the following temporal difference (TD) error \(L_{\text{TD}}(\phi)\): \(\mathbb{E}_{(\boldsymbol{s},\boldsymbol{a},\boldsymbol{s}^{\prime})\sim \mathcal{D}}\left[\left(r(\boldsymbol{s},\boldsymbol{a})+\gamma\max_{ \boldsymbol{a}^{\prime}}Q_{\hat{\phi}}(\boldsymbol{s}^{\prime},\boldsymbol{a}^ {\prime})-Q_{\phi}(\boldsymbol{s},\boldsymbol{a})\right)^{2}\right],\) where \(Q_{\hat{\phi}}(\boldsymbol{s},\boldsymbol{a})\) denotes a target network. Then at the policy improvement step, knowledge in the Q network is distilled into the policy network in various ways. Offline RL methods address the distributional shift [9] problem induced by the offline dataset \(\mathcal{D}\) by either modifying the policy evaluation step to regularize Q learning or constraining the policy improvement directly. In the following, we will show that our diffusion policy design is compatible with any offline algorithms and can speed up policy evaluation and improvement.

### Diffusion Models

Consider a real data distribution \(q(\boldsymbol{x})\) and a sample \(\boldsymbol{x}^{0}\sim q(\boldsymbol{x})\) drawn from it. The (forward) diffusion process fixed to a Markov chain gradually adds Gaussian noise to the sample in \(K\) steps, producing a sequence of noisy samples \(\boldsymbol{x}^{1},\ldots\boldsymbol{x}^{K}\). Note that we use superscript \(k\) to denote diffusion timestep to avoid conflicting with the RL timestep. The noise is controlled by a variance schedule \(\beta^{1},\ldots,\beta^{K}\):

\[q(\boldsymbol{x}^{k}|\boldsymbol{x}^{k-1})=\mathcal{N}(\boldsymbol{x}^{k}; \sqrt{1-\beta^{k}}\boldsymbol{x}^{k-1},\beta^{k}\boldsymbol{I}),\quad q( \boldsymbol{x}^{1:K}|\boldsymbol{x}^{0})=\prod_{k=1}^{K}q(\boldsymbol{x}^{k}| \boldsymbol{x}^{k-1}).\] (1)

When \(K\rightarrow\infty\), \(\boldsymbol{x}^{K}\) distributes as an isotropic Gaussian distribution. Diffusion models learn a conditional distribution \(p_{\theta}(\boldsymbol{x}^{t-1}|\boldsymbol{x}^{t})\) and generate new samples by reversing the above process:

\[p_{\theta}(\boldsymbol{x}^{0:K})=p(\boldsymbol{x}^{K})\prod_{k=1}^{K}p_{\theta} (\boldsymbol{x}^{k-1}|\boldsymbol{x}^{k}),\quad p_{\theta}(\boldsymbol{x}^{k-1}| \boldsymbol{x}^{k})=\mathcal{N}(\boldsymbol{x}^{k-1};\boldsymbol{\mu}_{\theta}( \boldsymbol{x}^{k},k),\boldsymbol{\Sigma}_{\theta}(\boldsymbol{x}^{k},k)),\] (2)where \(p(\bm{x}^{K})=\mathcal{N}(\bm{0},\bm{I})\) under the condition that \(\prod_{k=1}^{K}(1-\beta^{k})\approx 0\). The training is performed by maximizing the evidence lower bound (ELBO):\(\mathbb{E}_{\bm{x}_{0}}[\log p_{\theta}(\bm{x}^{0})]\geq\mathbb{E}_{q}\left[ \log\frac{p_{\theta}(\bm{x}^{0,K})}{q(\bm{x}^{1:K}|\bm{x}^{0})}\right]\).

## 4 Efficient Diffusion Policy

In this section, we detail the design of our efficient diffusion policy (EDP). First, we formulate an RL policy with a diffusion model. Second, we present a novel algorithm that can train a diffusion policy efficiently, termed Reinforcement-Guided Diffusion Policy Learning (RGDPL). Then, we generalize the diffusion policy to work with arbitrary offline RL algorithms and compare our EDP with Diffusion-QL to highlight its superiority in efficiency and generality. Finally, we discuss several methods to sample from the diffusion policy during evaluation.

### Diffusion Policy

Following [37], we use the reverse process of a conditional diffusion model as a parametric policy:

\[\pi_{\theta}(\bm{a}|\bm{s})=p_{\theta}(\bm{a}^{0:K}|\bm{s})=p(\bm{a}^{K}) \prod_{k=1}^{K}p_{\theta}(\bm{a}^{k-1}|\bm{a}^{k},\bm{s}),\] (3)

where \(\bm{a}^{K}\sim\mathcal{N}(\bm{0},\bm{I})\). We choose to parameterize \(\pi_{\theta}\) based on Denoising Diffusion Probabilistic Models (DDPM) [10], which sets \(\bm{\Sigma}_{\theta}(\bm{a}^{k},k;\bm{s})=\beta^{k}\bm{I}\) to fixed time-dependent constants, and constructs the mean \(\bm{\mu}_{\theta}\) from a noise prediction model as: \(\bm{\mu}_{\theta}(\bm{a}^{k},k;\bm{s})=\frac{1}{\sqrt{\alpha^{k}}}\left(\bm{ a}^{k}-\frac{\beta^{k}}{\sqrt{1-\bar{\alpha}^{k}}}\bm{\epsilon}_{\theta}( \bm{a}^{k},k;\bm{s})\right)\), where \(\alpha^{k}=1-\beta^{k}\), \(\bar{\alpha}^{k}=\prod_{s=1}^{k}\), and \(\bm{\epsilon}_{\theta}\) is a parametric model.

To obtain an action from DDPM, we need to draw samples from \(K\) different Gaussian distributions sequentially, as illustrated in Eqn. (2)-(3). The sampling process can be reformulated as

\[\bm{a}^{k-1}=\frac{1}{\sqrt{\alpha^{k}}}\left(\bm{a}^{k}-\frac{\beta^{k}}{ \sqrt{1-\bar{\alpha}^{k}}}\bm{\epsilon}_{\theta}(\bm{a}^{k},k;\bm{s})\right)+ \sqrt{\beta^{k}}\bm{\epsilon},\] (4)

with the reparametrization trick, where \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})\), \(k\) is the reverse timestep from \(K\) to \(0\).

Similar to DDPM, plugging in the conditional Gaussian distributions, the ELBO in Sec. 3.2 can be simplified to the following training objective \(L_{\text{diff}}(\theta)\):

\[\mathbb{E}_{k,\bm{\epsilon},(\bm{a}^{0},\bm{s})}\left[\left\|\bm{\epsilon}- \bm{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}^{k}}\bm{a}^{0}+\sqrt{1-\bar{ \alpha}^{k}}\bm{\epsilon},k;\bm{s}\right)\right\|^{2}\right],\] (5)

where \(k\) follows a uniform distribution over the discrete set \(\{1,\dots,K\}\). It means the expectation is taken over all diffusion steps from clean action to pure noise. Moreover, \(\bm{\epsilon}\in\mathcal{N}(\bm{0},\bm{I})\), and \((\bm{a}^{0},\bm{s})\in\mathcal{D}\) are state-action pairs drawn from the offline dataset. Given a dataset, we can easily and efficiently train a diffusion policy in a behavior-cloning manner as we only need to forward and backward through the network once each iteration. As shown in Diffusion-QL [37], diffusion policies can greatly boost the performance when trained with TD3-based Q learning. However, it still faces two main drawbacks that limit its real-world application: 1) It is inefficient in sampling and training; 2) It is not generalizable to other strong offline reinforcement learning algorithms.

### Reinforcement-Guided Diffusion Policy Learning

To understand how a parametric policy \(\pi_{\theta}\) is trained with offline RL algorithms, we start with a typical Q-learning actor-critic framework for continuous control, which iterates between policy evaluation and policy improvement. Policy evaluation learns a Q network by minimizing the TD error \(L_{\text{TD}}(\phi)\):

\[\mathbb{E}_{(\bm{s},\bm{a},\bm{s}^{\prime})\sim\mathcal{D}}\left[\left(r(\bm{s },\bm{a})+\gamma Q_{\hat{\phi}}(\bm{s}^{\prime},\bm{a}^{\prime})-Q_{\phi}(\bm{ s},\bm{a})\right)^{2}\right],\] (6)

where the next action \(\bm{a}^{\prime}\sim\pi_{\theta}(\cdot|\bm{s}^{\prime})\). The policy is optimized to maximize the expected Q values :

\[\max_{\theta}\mathbb{E}_{\bm{s}\sim\mathcal{D},\bm{a}\sim\pi_{\theta}(\bm{a}| \bm{s})}\left[Q_{\phi}(\bm{s},\bm{a})\right].\] (7)It is straightforward to optimize this objective when a Gaussian policy is used, but things get much more difficult when a diffusion policy is considered due to its complicated sampling process. Instead, we propose to view the offline RL problem from the perspective of generative modeling, where a diffusion policy can be easily learned in a supervised manner from a given dataset. However, unlike in computer vision, where the training data are usually perfect, offline RL datasets often contain suboptimal state-action pairs. Suppose we have a well-trained Q network \(Q_{\phi}\), the question becomes how we can efficiently use \(Q_{\phi}\) to guide diffusion policy training procedure. We now show that this can be achieved without sampling actions from diffusion policies.

Let's revisit the forward diffusion process in Eqn. 1. A notable property of it is that the distribution of noisy action \(\bm{a}^{k}\) at any step \(k\) can be written in closed form: \(q(\bm{a}^{k}|\bm{a}^{0})=\mathcal{N}(\bm{a}^{k};\sqrt{\bar{\alpha}^{k}}\bm{a}^ {0},(1-\bar{\alpha}^{k})\bm{I})\). Using the reparametrization trick, we are able to connect \(\bm{a}^{k}\), \(\bm{a}^{0}\) and \(\epsilon\) by:

\[\bm{a}^{k}=\sqrt{\bar{\alpha}^{k}}\bm{a}^{0}+\sqrt{1-\bar{\alpha}^{k}}\bm{ \epsilon},\quad\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I}).\] (8)

Recall that our diffusion policy is parameterized to predict \(\bm{\epsilon}\) with \(\bm{\epsilon}_{\theta}(\bm{a}^{k},k;\bm{s})\). By relaxing \(\bm{\epsilon}\) with \(\bm{\epsilon}_{\theta}(\bm{a}^{k},k;\bm{s})\) and rearranging Eqn. (8), we obtain the approximatied action:

\[\hat{\bm{a}}^{0}=\frac{1}{\sqrt{\bar{\alpha}^{k}}}\bm{a}^{k}-\frac{\sqrt{1- \bar{\alpha}^{k}}}{\sqrt{\bar{\alpha}^{k}}}\bm{\epsilon}_{\theta}(\bm{a}^{k}, k;\bm{s}).\] (9)

In this way, instead of running the reverse diffusion process to sample an action \(\bm{a}^{0}\), we can cheaply construct \(\hat{\bm{a}}^{0}\) from a state-action pair \((\bm{s},\bm{a})\) in the dataset by first corrupting the action \(\bm{a}\) to \(\bm{a}^{k}\) then performing one-step denoising to it. We will refer to this technique as _action approximation_ in the following. Accordingly, the policy improvement for diffusion policies is modified as follows:

\[L_{\pi}(\theta)=-\mathbb{E}_{\bm{s}\sim\mathcal{D},\hat{\bm{a}}^{0}}\left[Q_{ \phi}(\bm{s},\hat{\bm{a}}^{0})\right].\] (10)

To improve the efficiency of policy evaluation, we propose to replace the DDPM sampling in Eqn. (4) with DPM-Solver [20], which is an ODE-based sampler. The algorithm is defered to the appendix.

### Generalization to Various RL algorithms

There are mainly two types of approaches to realize the objective in Eqn. 7 for policy improvement.

_Direct policy optimization_: It maximizes Q values and directly backpropagate the gradients from Q network to policy network, _i.e._, \(\nabla_{\theta}L_{\pi}(\theta)=-\frac{\partial Q_{\phi}(\bm{s},\bm{a})}{ \partial\bm{a}}\frac{\partial\bm{a}}{\partial\theta}\). This is only applicable to cases where \(\frac{\partial\bm{a}}{\partial\theta}\) is tractable, _e.g._, when a deterministic policy \(\bm{a}=\pi_{\theta}(\bm{s})\) is used or when the sampling process can be reparameterized. Sample algorithms belonging to this category include TD3 [7], TD3+BC [6], and CQL [17]. One can easily verify that both the expensive DDPM sampling in Eqn. (4) and our efficient approximation in Eqn. (9) can be used for direct policy optimization.

_Likelihood-based policy optimization_: It tries to distill the knowledge from the Q network into the policy network indirectly by performing weighted regression or weighted maximum likelihood:

\[\max_{\theta}\quad\mathbb{E}_{(\bm{s},\bm{a})\sim\mathcal{D}}\left[f(Q_{\phi}( \bm{s},\bm{a}))\log\pi_{\theta}(\bm{a}|\bm{s})\right],\] (11)

where \(f(Q_{\phi}(\bm{s},\bm{a}))\) is a monotonically increasing function that assigns a weight to each state-action pair in the dataset. This objective requires the log-likelihood of the policy to be tractable and differentiable. AWR [28], CRR [38], and IQL [15] fall into this category but each has a unique design in terms of the weighting function \(f\). Since the likelihood of samples in Diffusion models is intractable, we propose the following two variants for realizing Eqn. 11.

First, instead of computing the likelihood, we turn to a lower bound for \(\log\pi_{\theta}(\bm{a}|\bm{s})\) introduced in DDPM [10]. By discarding the constant term that does not depend on \(\theta\), we can have the objective:

\[\mathbb{E}_{k,\bm{\epsilon},(\bm{a},\bm{s})}\left[\frac{\beta^{k}\cdot f(Q_{ \phi}(\bm{s},\bm{a}))}{2\alpha^{k}(1-\bar{\alpha}^{k-1})}\left\|\bm{\epsilon} -\bm{\epsilon}_{\theta}\left(\bm{a}^{k},k;\bm{s}\right)\right\|^{2}\right].\] (12)

Second, instead of directly optimizing \(\log\pi_{\theta}(\bm{a}|\bm{s})\), we propose to replace it with an approximated policy \(\hat{\pi}_{\theta}(\bm{a}|\bm{s})\triangleq\mathcal{N}(\hat{\bm{a}}^{0},\bm{ I})\), where \(\hat{\bm{a}}^{0}\) is from Eqn. (9). Then, we get the following objective:

\[\mathbb{E}_{k,\bm{\epsilon},(\bm{a},\bm{s})}\left[f(Q_{\phi}(\bm{s},\bm{a})) \left\|\bm{a}-\hat{\bm{a}}^{0}\right\|^{2}\right].\] (13)Empirically, we find these two choices perform similarly, but the latter is easier to implement. So we will report results mainly based on the second realization. In our experiments, we consider two offline RL algorithms under this category, _i.e._, CRR, and IQL. They use two weighting schemes: \(f_{\text{CRR}}=\exp\big{[}\big{(}Q_{\phi}(s,a)-\mathbb{E}_{a^{\prime}\sim\hat{n} (a|s)}Q(s,a^{\prime})\big{)}\big{/}\tau_{\text{CRR}}\big{]}\) and \(f_{\text{IQL}}=\exp\big{[}\big{(}Q_{\phi}(s,a)-V_{\psi}(s)\big{)}\big{/}\tau_{ \text{IQL}}\big{]}\), where \(\tau\) refers to the temperature parameter and \(V_{\psi}(s)\) is an additional value network parameterized by \(\psi\). We defer the details of these two algorithms to Appendix A.

### Comparison to Diffusion-QL

Now we are ready to compare our method and Diffusion-QL comprehensively. Though our EDP shares the same policy parametrization as Diffusion-QL, it differs from Diffusion-QL significantly in the training algorithm. As a result, the computational efficiency and generality of diffusion policies have been improved substantially.

**Efficiency** The diffusion policy affects both policy evaluation (Eqn. (6)) and policy improvement (Eqn. (7)). First, calculating \(L_{\text{TD}}(\phi)\) in policy evaluation requires drawing the next action from it. Diffusion-QL uses DDPM sampling while EDP employs a DPM-Solver, which can reduce the sampling steps from 1000 to 15, thus accelerating the training. Second, in policy improvement, Diffusion-QL again applies DDPM for sampling. Then, it calculates the loss function based on sampled actions and backpropagates through the sampling process for network update. This means it needs to forward and backward a neural network for \(K\) times each training iteration. As a result, Diffusion-QL can only work with small \(K\), _e.g._, \(5\sim 100\). In comparison, our training scheme only passes through the network once an iteration, no matter how big \(K\) is. This enables EDP to use a larger \(K\) (1000 in our experiments) to train diffusion policy on the more fine-grained scale. The results in Tab. 1 also show a larger \(K\) can give better performance.

**Generality** Diffusion-QL can only work with direct policy optimization, which contains only a small portion of algorithms. Moreover, thanks to their flexibility and high performance, the likelihood-based algorithms are preferred for some tasks (_e.g._, Antmaze). Our method successfully makes diffusion trainable with any RL algorithm.

### Controlled Sampling from Diffusion Policies

Traditionally, a continuous policy is represented with a state-conditional Gaussian distribution. During evaluation time, a policy executes deterministically to reduce variance by outputting the distribution mean as an action. However, with diffusion policies, we can only randomly draw a sample from the underlying distribution without access to its statistics. As a result, the sampling process is noisy, and the evaluation is of high variance. We consider the following method to reduce variance.

**Energy-based Action Selection (EAS)** Recall that the goal of (offline) RL is to learn a policy that can maximize the cumulative return or values. Though the policy \(\pi_{\theta}\) is stochastic, the learned \(Q_{\phi}\) provides a deterministic critic for action evaluation. We can sample a few actions randomly, then use \(Q_{\phi}\) for selection among them to eliminate randomness. EAS first samples \(N\) actions from \(\pi_{\theta}\) by using any samplers (_i.e._, DPM-Solver), then sample one of them with weights proportional to \(e^{Q(\bm{s},\bm{a})}\). This procedure can be understood as sampling from an improved policy \(p(\bm{a}|\bm{s})\propto e^{Q(\bm{s},\bm{a})}\pi_{\theta}(\bm{a}|\bm{s})\). All results will be reported based on EAS. See Appendix. C.4 for the other two methods.

## 5 Experiments

We conduct extensive experiments on the D4RL benchmark [2] to verify the following assumptions: 1) Our diffusion policy is much more efficient than the previous one regarding training and evaluation costs. 2) Our diffusion policy is a generic policy class that can be learned through direct and likelihood-based policy learning methods. We also provide various ablation studies on the critical components for better understanding.

BaselinesWe evaluate our method on four domains in D4RL, including Gym-locomotion, AntMaze, Adroit, and Kitchen. For each domain, we consider extensive baselines to provide a thorough evaluation. The simplest method is the classifier behavior cloning (BC) baseline and 10% BC that performs behavior cloning on the best 10% data. TD3+BC [6] combines off-policy reinforcement learning algorithms with BC. OneStepRL [1] first conducts policy evaluation to obtain the Q-value of the behavior policy from the offline dataset, then use it for policy improvement. AWAC [25], AWR [28], and CRR [38] improve policy improvement by adding advantage-based weights to policy loss functions. CQL [17] and IQL [15] constrain the policy evaluation process by making conservative Q updates or replacing the max operator with expectile regression. We also consider the Decision Transformer (DT) [4] baseline that maps offline RL as a sequence-to-sequence translation problem.

Experimental SetupWe keep the backbone network architecture the same for all tasks and algorithms, which is a 3-layer MLP (hidden size 256) with Mish [23] activation function following Diffusion-QL [37]. For the noise prediction network \(\bm{\epsilon}_{\theta}(\bm{a}^{k},k;\bm{s})\) in diffusion policy, we first encode timestep \(k\) with sinusoidal embedding [36], then concatenate it with the noisy action \(\bm{a}^{k}\) and the conditional state \(\bm{s}\). We use the Adam [13] to optimize both diffusion policy and the Q networks. The models are trained for 2000 epochs on Gym-locomotion and 1000 epochs on the other three domains. Each epoch consists of 1000 iterations of policy updates with batch size 256. For DPM-Solver [20], we use the third-order version and set the model call steps to 15. We reimplement DQL strictly following the official PyTorch code [27] for fair comparison and we refer to DQL (JAX) for all sample efficiency comparisons. We defer the complete list of all hyperparameters to the appendix due to space limits. Throughout this paper, the results are reported by averaging 5 random seeds.

Evaluation ProtocalWe consider two evaluation metrics in this paper. First, _online model selection_ (OMS), proposed by Diffusion-QL [37], selects the best-performing model throughout the whole training process. However, though OMS can reflect an algorithm's capacity, it is cheating, especially when the training procedure is volatile on some of the tasks. Therefore, we propose another metric to focus on the training stability and quality, which is _running average at training_ (RAT). RAT calculates the running average of evaluation performance for ten consecutive checkpoints during training and reports the last score as the final performance.

### Efficiency and Reproducibility

In this section, we focus on the training and evaluation efficiency of our efficient diffusion policy. We choose the OMS evaluation metric to make a fair comparison with the baseline method Diffusion-QL [37]. We consider four variants of EDP to understand how each component contributes to the high efficiency of our method. 1) EDP is the complete version of our method. It uses the action approximation technique in Eqn. (9) for policy training and uses DPM-Solver for sampling. 2) _EDP w/o DPM_ modifies EDP by replacing DPM-Solver with the original DDPM sampling method in Eqn. 4. 3) _EDP w/o AP_ removes the action approximation technique. 4) DQL (JAX) is our Jax implementation of Diffusion-QL.

We first benchmark and compare the training/evaluation speed of the above three variants and Diffusion-QL. We choose walker2d-medium-expert-v2 as the testbed. For training speed, we run each algorithm for 10,000 iterations of policy updates and calculate the corresponding iterations-per-second (IPS). Similarly, we sample 10,000 transitions by interacting with the environment and calculate the corresponding steps-per-second (SPS) for evaluation speed. Based on the visualization in Fig. 2, by taking DQL (JAX) as the baseline, we are able to attribute the performance boost to specific techniques proposed. Specifically, we can observe that action approximation makes 2.3x training and 3.3x sampling faster, while using the DPM-Solver adds an additional 2.3x training speedup. We can observe that DQL (JAX) is \(5\times\) faster than Diffusion-QL, which means our Jax implementation is more computationally efficient than Diffusion-QL's PyTorch code. This demonstrates that both the action approximation technique and DPM-Solver play a critical role in making the training of diffusion policy efficient. However, this technique does not affect the sampling procedure; thus, _EDP w/o DPM_ and DQL (JAX) are on par with each other regarding sampling speed.

Figure 2: Training and evaluation speed comparison. The training IPS are: \(4.66,22.30,50.94,38.4\), and \(116.21\). The sampling SPS are: \(18.67,123.70\), \(123.06\), \(411.0\), \(411.79\).

[MISSING_PAGE_FAIL:8]

Evaluation MetricsTo reveal that evaluation metric is important, we train EDP with TD3 algorithms on three selected environments: walker2d-medium-expert-v2, hopper-medium-expert-v2, and antmaze-medium-diverse-v0. We then compare the scores for OMS (best) and RAT (average) by plotting the training curves in Fig. 3. On walker2d, the training is stable; thus, both OMS and RAT scores steadily grow and result in close final scores. A similar trend can be observed on the hopper but with a more significant gap between these two metrics. However, these two metrics diverge significantly when the training succeeds and then crashes on antmaze. Therefore, OMS is misleading and can not give a reliable evaluation of algorithms, which explains the necessity of using RAT in Sec. 5.2.

Energy-Based Action SelectionWe notice that energy-based action selection (EAS) is a general method and can also be used for arbitrary policies. We apply EAS to normal TD3+BC and find no improvement, which shows EAS is only necessary for diffusion sampling. The results are deferred to the Appendix. Moreover, set the number of actions used in EAS from 1 to 200, and report the performance on gym-locomotions tasks in Fig. 4. It shows the normalized score monotonically grows as the number of actions increases on 8 out of 9 tasks. In our main experiments, we set the number of actions to 10 by trading off the performance and computation efficiency.

DPM-SolverWe are using DPM-Solver to speed up the sampling process of diffusion policies. The number of models in DPM-Solver is an important hyper-parameter that affects sampling efficiency and quality. We vary this number from 3 to 30 and compare the performance on gym-locomotion tasks in Fig. 5. We can observe that the performance increases as more steps of model calls are used. The performance gradually plateaus after 15 model calls. Therefore, we use 15 in our main experiments.

## 6 Conclusion

Diffusion policy has emerged as an expressive policy class for offline reinforcement learning. Despite its effectiveness, diffusion policy is limited by two drawbacks, hindering it from wider applications. First, training a diffusion policy requires to forward and backward through a long parameterized Markov chain, which is computationally expensive. Second, the diffusion policy is a restricted policy class that can not work with likelihood-based RL algorithms, which are preferred in many scenarios. We propose efficient diffusion policy (EDP) to address these limitations and make diffusion policies faster, better, and more general. EDP relies on an action approximation to construct actions from corrupted ones, thus avoiding running the Markov chain for action sampling at training. Our benchmarking shows that EDP achieves 25\(\times\) speedup over Diffusion-QL at training time on the gym-locomotion tasks in D4RL. We conducted extensive experiments by training EDP with various offline RL algorithms, including TD3, CRR, and IQL, the results clearly justify the superiority of diffusion policies over Gaussian policies. As a result, EDP set new state-of-the-art on all four domains in D4RL.

Figure 4: Performance of different number of actions used in EAS. The experiments are conducted on the nine locomotion tasks.

Figure 5: Performance of DPM-Solver with varying steps. The experiments are conducted on the nine locomotion tasks.

Figure 3: Training curves for EDP +TD3 on three representative environments. Average represents RAT, Best represents OMS.

## References

* [1] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. _Advances in Neural Information Processing Systems_, 34:4933-4946, 2021.
* [2] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. _Advances in Neural Information Processing Systems_, 34:4933-4946, 2021.
* [3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* [6] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [7] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [8] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [9] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, 2019.
* [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [11] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* [12] Bingyi Kang, Xiao Ma, Yirui Wang, Yang Yue, and Shuicheng Yan. Improving and benchmarking offline reinforcement learning algorithms. _arXiv preprint arXiv:2306.00972_, 2023.
* [13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [14] Roger Koenker and Kevin F Hallock. Quantile regression. _Journal of economic perspectives_, 15(4):143-156, 2001.
* [15] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* [16] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* [17] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* [18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [19] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.

* [20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.
* [21] Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, and Shuicheng Yan. Mutual information regularized offline reinforcement learning. _arXiv preprint arXiv:2210.07484_, 2022.
* [22] Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale offline reinforcement learning. In _Deep RL Workshop NeurIPS 2021_, 2021.
* [23] Diganta Misra. Mish: A self regularized non-monotonic neural activation function. _arXiv preprint arXiv:1908.08681_, 4(2):10-4850, 2019.
* [24] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. _Advances in neural information processing systems_, 2017.
* [25] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* [26] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* [28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [30] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* [31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [32] Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning \(k\) modes with one stone. _arXiv preprint arXiv:2206.11251_, 2022.
* [33] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In _International Conference on Learning Representations_, 2020.
* [34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [35] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Advances in Neural Information Processing Systems_, 34:1415-1428, 2021.
* [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [37] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. _arXiv preprint arXiv:2208.06193_, 2022.
* [38] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. _Advances in Neural Information Processing Systems_, 33:7768-7778, 2020.

* [39] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* [40] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in neural information processing systems_, 34:28954-28967, 2021.
* [41] Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, and Shuicheng Yan. Offline prioritized experience replay. _arXiv preprint arXiv:2306.05412_, 2023.
* [42] Yang Yue, Bingyi Kang, Xiao Ma, Zhongwen Xu, Gao Huang, and Shuicheng Yan. Boosting offline reinforcement learning via data rebalancing. _arXiv preprint arXiv:2210.09241_, 2022.

## Broader Impact

In this paper, we propose an efficient yet powerful policy class for offline reinforcement learning. We show that this method is superior to most existing methods on simulated robotic tasks. However, some war robots or weapon robots might employ our EDP to learn strategic agents considering the generalization ability of EDP. Depending on the specific application scenarios, it might be harmful to domestic privacy and safety.

## Appendix A Reinforcement Learning Algorithms

In this section, we introduce the details of the RL algorithms experimented.

### Td3+bc

TD3 [8] is a popular off-policy RL algorithm for continuous control. TD3 improves DDPG [19] by addressing the value overestimation issue. Specifically, TD3 adopts a double Q-learning paradigm that computes the TD-target \(\hat{Q}(\bm{s},\bm{a})\) as

\[\hat{Q}(\bm{s},\bm{a})=r(\bm{s},\bm{a})+\gamma\min(Q_{1}(\bm{s}^{\prime},\bm{a }^{\prime}),Q_{2}(\bm{s}^{\prime},\bm{a}^{\prime})),\quad\bm{a}^{\prime}=\pi( \bm{s}^{\prime}),\] (14)

where \(\pi(\bm{s}^{\prime})\) is a deterministic policy, \(Q_{1}\) and \(Q_{2}\) are two independent value networks. Specifically, TD3 takes **only**\(Q_{1}\) for policy improvement

\[\pi^{*}=\arg\max_{\pi}\mathbb{E}\left[Q_{1}(\bm{s},\hat{\bm{a}})\right],\quad \hat{a}=\pi(\bm{s}).\] (15)

Built on top of TD3, TD3+BC simply adds an additional behavior cloning term for its policy improvement

\[\pi^{*}=\arg\max_{\pi}\mathbb{E}_{s,a\sim\mathcal{D}}\left[Q(\bm{s},\hat{\bm{ a}})-\alpha(\hat{a}-a)^{2}\right],\quad\hat{\bm{a}}=\pi(\bm{s}),\] (16)

where \(\alpha\) is a hyper-parameter that balances these two terms. However, as the scale of Q-values are different from the behavior cloning loss, TD3+BC normalizes it with \(\frac{1}{N}\sum\limits_{i=1}^{N}|Q(\bm{s}_{i},\hat{\bm{a}}_{i})|\) for numerical stability.

In the context of EDP, we observe that 1) the behavior cloning term can be naturally achieved as a diffusion loss as defined in Eqn. 5; 2) the sampled action \(\hat{\bm{a}}\) is replaced by action approximation as in Eqn. 9 for efficient policy improvement. Different from the original TD3+BC that uses only \(Q_{1}\) for policy improvement, we sample from \(Q_{1}\) and \(Q_{2}\) with equal probability for each policy improvement step.

### Critic Regularized Regression

CRR follows Advantage Weighted Regression (AWR) [29], which is a simple yet effective off-policy RL method, for policy improvement. Specifically, AWR considers a constrained policy improvement step

\[\arg\max_{\pi}\int_{\bm{s}}d_{\mu}(\bm{s})\int_{\bm{a}}\pi(\bm{a}\mid\bm{s})A (\bm{s},\bm{a})d\bm{a}d\bm{s},\quad\mathrm{s.t.}\quad\int_{s}d_{\mu}(\bm{s})D_ {\text{KL}}\left[\pi(\cdot\mid\bm{s})\mid\mid\mu(\cdot\mid\bm{s})\right]\leq\epsilon,\] (17)

where \(A(\bm{s},\bm{a})\) is the advantage function, \(\mu\) is a behavior policy that is used to generate trajectories during off-policy RL, \(d_{\mu}(\bm{s})\) is the state distribution induced by \(\mu\), \(D_{\text{KL}}\) is the KL divergence, and \(\epsilon\) is a threshold parameter. This constrained optimization problem can be solved in closed form, which gives an optimal policy of

\[\pi^{*}(\bm{a}\mid\bm{s})=\frac{1}{Z(\bm{s})}\mu(\bm{a}\mid\bm{s})\exp\left( \frac{1}{\beta}A(\bm{s},\bm{a})\right),\] (18)

with \(Z(\bm{s})\) being the partition function and \(\beta\) is a hyper-parameter. For policy improvement, AWR simply distills the one-step improved optimal policy to the learning policy \(\pi\) by minimizing theKL-divergence

\[\arg\min_{\pi}\mathbb{E}_{\bm{s}\sim d_{\mu}(\bm{s})}\left[D_{\text{KL} }\left[\pi^{*}(\cdot\mid\bm{s})\parallel\pi(\cdot\mid\bm{s})\right]\right]\] \[= \arg\max_{\pi}\mathbb{E}_{\bm{s}\sim d_{\mu}(\bm{s}),\bm{a}\sim\mu (\cdot\mid\bm{s})}\left[\log\pi(\bm{a}\mid\bm{s})\exp(\frac{1}{\beta}A(\bm{s}, \bm{a}))\right]\] (19)

CRR performs policy improvement in the same way as AWR, which simply replaces the sampling distribution with a fixed dataset

\[\arg\max_{\pi}\mathbb{E}_{\bm{s},\bm{a}\sim\mathcal{D}}\left[\log\pi(\bm{a} \mid\bm{s})\exp(\frac{1}{\beta}A(\bm{s},\bm{a}))\right].\] (20)

As a result, this naturally imposes an implicit constraint on its policy improvement step.

However, as computing the log-likelihood \(\log\pi(a\mid s)\) is intractable in diffusion models, in practical implementations, we use Eqn. 13 instead. In addition, we compute the advantage by

\[A(\bm{s},\bm{a})=\min(Q_{1}(\bm{s},\bm{a})-Q_{2}(\bm{s},\bm{a}))-\frac{1}{N} \sum_{i=1}^{N}\min(Q_{1}(\bm{s},\hat{\bm{a}}_{i}),Q_{2}(\bm{s},\hat{\bm{a}}_{ i})),\] (21)

where \(\hat{a}_{i}\sim\mathcal{N}(\hat{\bm{a}}^{0},\bm{\sigma})\) is a sampled action with the mean of approximated action and an additional standard deviation. In our experiment, we found using a fixed standard deviation, an identity matrix of \(\bm{\sigma}=\mathbb{I}\), \(\beta=1\), and sample size \(N=10\) generally produce a good performance.

### Implicit Q Learning

Similar to CRR, IQL also adopts the AWR-style policy improvement that naturally imposes a constraint to encourage the learning policy to stay close to the behavior policy that generates the dataset. Different from CRR query novel and potentially out-of-distribution (OOD) actions when computing advantages, IQL aims to completely stay in-distribution with only dataset actions, while maintaining the ability to perform effective multi-step dynamic programming during policy evaluation. IQL achieves this by introducing an additional value function \(V(\bm{s})\) and performing expectile regression. Specifically, the policy evaluation in IQL is implemented as

\[\min_{V}\mathbb{E}_{\bm{s},\bm{a}\sim\mathcal{D}}\left[L_{2}^{ \tau}(Q(\bm{s},\bm{a}))-V(\bm{s})\right]\] \[\min_{Q}\mathbb{E}_{\bm{s},\bm{a},\bm{s}^{\prime}\sim\mathcal{D} }\left[(r(\bm{s},\bm{a})+\gamma V(\bm{s}^{\prime})-Q(\bm{s},\bm{a}))^{2} \right],\] (22)

where \(L_{2}^{\tau}\) is the expectile regression loss defined as \(L_{2}^{\tau}(x)=|\tau-\mathbb{1}(x<0)|x^{2}\) with hyper-paramter \(\tau\in(0,1)\). The intuition behind IQL is that with a larger \(\tau\), we will be able to better approximate the \(\max\) operator. As a result, IQL approximates the Bellman's optimality equation without querying OOD actions.

In practical implementations, we also adopt double Q learning for IQL, where we replace the \(Q(\bm{s},\bm{a})\) in Eqn. 22 with \(\min(Q_{1}(\bm{s},\bm{a}),Q_{2}(\bm{s},\bm{a}))\), and then use the updated value network to train both Q value networks. For IQL, we follow the policy improvement steps of CRR, as described in Eqn. 20 and Eqn. 21. The key difference is that instead of sampling actions to compute \(\frac{1}{N}\sum\limits_{i=1}^{N}\min(Q_{1}(\bm{s},\hat{\bm{a}}_{i}),Q_{2}( \bm{s},\hat{\bm{a}}_{i}))\), IQL replaces it directly with the learned \(V(\bm{s})\). As for hyper-parameters, we use a temperature of \(\beta=1\), a fixed standard deviation \(\bm{\sigma}=\mathbb{I}\), and for expectile ratio \(\tau\), we use \(\tau=0.9\) for antmaze-v0 environments and \(\tau=0.7\) for other tasks.

## Appendix B Reinforcement Guided Diffusion Policy Details

The overall algorithm for our Reinforcement Guided Diffusion Policy Learning is given in Alg. 1.

The detailed algorithm for energy-based action selection is given in Alg. 2.

## Appendix C Environmental Details

### Hyper-Parameters

We detail our hyperparameters in Tab. 3.

### More Results

We first expand Tab. 1 by providing detailed numbers for each of the tasks used in Tab. 4.

We report the performance of EDP trained with TD3, CRR, and IQL in Tab. 5, where we directly compare the scores of different evaluation metrics, _i.e._, OMS and RAT. We can observe that there are huge gaps between OMS and RAT for all domains and all algorithms. However, IQL and CRR are relatively more stable than \(TD3\). For example, on the antmaze domain, TD3 achieves a best score of 80.5, while the average score is just 29.8. In comparison, the best and average scores of IQL are 89.2 and 73.4, respectively.

### More Results on EAS

We compare normal TD3+BC, TD3+BC with EAS for evaluation and TD3 + EDP in Tab. 6.

### More Experiments on Controlled Sampling

As in Sec. 4.5, we discussed reducing variance with EAS. We now detail another two methods experimented as below.

**Policy scaling**  Instead of sampling from the policy directly, we can sample from a sharper policy \(\pi_{\theta}^{\tau}(\bm{a}|\bm{s})\), where \(\tau>1\). The scaled policy \(\pi_{\theta}^{\tau}\) shares the same distribution modes as \(\pi_{\theta}\) but with reduced variance. Since diffusion policies are modeling the scores \(\log\pi_{\theta}(\bm{a}_{t}|\bm{s})\), policy scaling can be easily achieved by scaling the output of noise-prediction network by the factor \(\tau\). We conduct experiments on the gym-locomotion tasks in D4RL by varying \(\tau\) from \(0.5\) to \(2.0\), as shown in Fig. 6, the results show the best performance is achieved when \(\tau=1.0\). It means sampling from a scaled policy does not work.

**Deterministic Sampling** This method is based on the observation that the sampling process of the DPM-Solver is deterministic except for the first step. The first step is to sample from an isotropic Gaussian distribution. We modify it to use the mean, thus avoiding stochasticity. Consider a initial noise \(\bm{a}^{K}\sim\mathcal{N}(\bm{0},\bm{I})\), we rescale \(\bm{a}^{K}\) by a noise scale factor. We show how this factor affects the final performance by varying it from 0.0 to 1.0. As illustrated in Fig. 7, the best performance is achieved at zero noise in most cases, and a normal \(\bm{a}^{K}\) performs worst. This means reducing the variance of the initial noise is able to improve the performance of a diffusion policy. However, the best performance achieved in this way still falls behind EAS.

### Computational Cost Comparison between EDP and Feed-Forward Policy networks

We benchmark the training speed of TD3+BC with EDP on walker2d-medium-expert-v2, by training each agent for 10,000 iterations of policy updates. The training speed for TD3+BC is 689 iterations-per-second (IPS), while EDP is 412 IPS. In other words, it takes around 3 hours to train an agent with the feed-forward network, while EDP needs around 5 hours. We also conducted experiments by double the network used in TD3+BC; unfortunately, there was no performance gain on the locomotion tasks (75.3 to 75.6). Moreover, both feed-forward policy and diffusion policy utilize a 3-layer MLP (hidden size 256) as the backbone network. Therefore, the network capacity should be comparable.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multicolumn{1}{c}{Task} & Learning rate & Gradient norm clipping & Loss weight & Epochs & Batch size \\ \hline Locomotion & 0.0003 & 5 & 1.0 & 2000 & 256 \\ Antmaze & 0.0003 & 5 & 1.0 & 1000 & 256 \\ Adroit & 0.00003 & 5 & 0.1 & 1000 & 256 \\ Kitchen & 0.0003 & 5 & 0.005 & 1000 & 256 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The performance of Diffusion-QL with efficient diffusion policy. The results for Diffusion-QL are directly quoted from [37]. EDP is our method. DQL (JAX) is a variant that uses the exact same configurations as Diffusion-QL. All results are reported based on the OMS metric.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{1}{c}{Dataset} & Diffusion-QL & DQL (JAX) & EDP \\ \hline halfcheetah-medium-v2 & \(51.5\) & \(52.3\) & \(52.8\) \\ hopper-medium-v2 & \(96.6\) & \(95.3\) & \(98.6\) \\ walker2d-medium-v2 & \(87.3\) & \(86.9\) & \(89.6\) \\ halfcheetah-medium-replay-v2 & \(48.3\) & \(50.3\) & \(50.4\) \\ hopper-medium-replay-v2 & \(102.0\) & \(101.8\) & \(102.7\) \\ walker2d-medium-replay-v2 & \(98.0\) & \(96.3\) & \(97.7\) \\ halfcheetah-medium-expert-v2 & \(97.2\) & \(97.3\) & \(97.1\) \\ hopper-medium-expert-v2 & \(112.3\) & \(113.1\) & \(112.0\) \\ walker2d-medium-expert-v2 & \(111.2\) & \(111.5\) & \(112.0\) \\ average & \(89.4\) & \(89.4\) & \(90.3\) \\ \hline antmaze-v2-v0 & \(96.0\) & \(93.4\) & \(93.4\) \\ antmaze-v2-three-v0 & \(84.0\) & \(74.0\) & \(66.0\) \\ antmaze-medium-play-v0 & \(79.8\) & \(96.0\) & \(88.0\) \\ antmaze-medium-diverse-v0 & \(82.0\) & \(82.0\) & \(96.0\) \\ antmaze-large-play-v0 & \(49.0\) & \(66.0\) & \(60.0\) \\ antmaze-large-diverse-v0 & \(61.7\) & \(60.0\) & \(64.0\) \\ \hline average & \(75.4\) & \(78.6\) & \(77.9\) \\ \hline pen-human-v1 & \(75.7\) & \(74.0\) & \(98.3\) \\ pen-cloned-v1 & \(60.8\) & \(59.1\) & \(79.9\) \\ average & \(68.3\) & \(66.5\) & \(89.1\) \\ \hline kitchen complete+v0 & \(84.5\) & \(100.0\) & \(97.0\) \\ kitchen-partial-v0 & \(63.7\) & \(73.0\) & \(71.5\) \\ kitchen-mixed-v0 & \(66.6\) & \(76.0\) & \(73.0\) \\ average & \(71.6\) & \(83.0\) & \(80.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters used by EDP.

Figure 6: Performance of EDP + TD3 on gym-locomotion tasks with varying \(\tau\).

### The effect of action approximation

We compare with and without action approximation on the following three environments by using OMS metric (Tab. 4). In Tab. 7, the DDPM column will forward and backward a policy network 100 times at training time, but action approximation only needs once. We can observe that action approximation will slightly harm the performance, when the same number of diffusion steps is used. However, it supports training diffusion policies with larger K (e.g. 1000), while Diffusion-QL does

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & TD3+Dift & TD3+EAS & TD3 \\ \hline halfcheetah-medium-v2 & 52.1 & 48.7 & 47.8 \\ hopper-medium-v2 & 81.9 & 50.8 & 54.0 \\ walker2d-medium-v2 & 86.9 & 77.7 & 53.4 \\ halfcheetah-medium-replay-v2 & 49.4 & 44.4 & 44.1 \\ hopper-medium-replay-v2 & 101.0 & 59.8 & 59.1 \\ walker2d-medium-replay-v2 & 94.9 & 74.8 & 71.6 \\ halfcheetah-medium-repert-v2 & 95.5 & 85.9 & 92.3 \\ hopper-medium-repert-v2 & 97.4 & 71.9 & 95.1 \\ walker2d-medium-expert-v2 & 110.2 & 108.4 & 110.0 \\ gym-locomotion-v2 (avg) & 85.5 & 69.2 & 69.7 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Energy-based Action Selection + Normal TD3

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{2}{c}{EDP + TD3} & \multicolumn{2}{c}{EDP + CRR} & \multicolumn{2}{c}{EDP + IQL} \\ \cline{2-7}  & Average & Best & Average & Best & Average & Best \\ \hline halfcheetah-medium-v2 & \(52.1\) & \(52.8\) & \(49.2\) & \(50.2\) & \(48.1\) & \(48.7\) \\ hopper-medium-v2 & \(81.9\) & \(98.6\) & \(78.7\) & \(95.0\) & \(63.1\) & \(97.3\) \\ walker2d-medium-v2-v2 & \(86.9\) & \(89.6\) & \(82.5\) & \(85.8\) & \(85.4\) & \(88.7\) \\ halfcheetah-medium-replay-v2 & \(49.4\) & \(50.4\) & \(43.5\) & \(47.8\) & \(43.8\) & \(45.5\) \\ hopper-medium-replay-v2 & \(101.0\) & \(102.7\) & \(99.0\) & \(101.7\) & \(99.1\) & \(100.9\) \\ walker2d-medium-replay-v2 & \(94.9\) & \(97.7\) & \(63.3\) & \(89.8\) & \(84.0\) & \(93.4\) \\ halfcheetah-medium-expert-v2 & \(95.5\) & \(97.1\) & \(85.6\) & \(93.5\) & \(86.7\) & \(80.9\) \\ hopper-medium-expert-v2 & \(97.4\) & \(112.0\) & \(92.9\) & \(109.4\) & \(99.6\) & \(95.7\) \\ walker2d-medium-expert-v2 & \(110.2\) & \(110.1\) & \(112.3\) & \(109.0\) & \(111.5\) \\ \hline average & 85.5 & \(90.3\) & \(78.3\) & \(87.3\) & \(79.9\) & \(84.7\) \\ \hline kitchen-complete-v0 & 61.5 & \(93.4\) & \(73.9\) & \(95.8\) & \(75.5\) & \(95.0\) \\ kitchen-partial-v0 & 52.8 & \(66.0\) & \(40.0\) & \(56.7\) & \(46.3\) & \(72.5\) \\ kitchen-mixed-v0 & 60.8 & \(88.0\) & \(46.1\) & \(59.2\) & \(56.5\) & \(70.0\) \\ average & \(58.4\) & \(96.0\) & \(53.3\) & \(70.6\) & \(59.4\) & \(79.2\) \\ \hline pen-human-v0 & 48.2 & \(60.0\) & \(70.2\) & \(127.8\) & \(72.7\) & \(130.3\) \\ pen-cloned-v0 & 15.9 & \(64.0\) & \(54.0\) & \(106.0\) & \(70.0\) & \(138.2\) \\ average & 32.1 & \(77.9\) & \(62.1\) & \(116.9\) & \(71.3\) & \(134.3\) \\ \hline antmaze-umaze-v0 & \(96.6\) & \(98.3\) & \(95.9\) & \(98.0\) & \(94.2\) & \(98.0\) \\ antmaze-umaze-diverse-v0 & \(69.5\) & \(79.9\) & \(15.9\) & \(80.0\) & \(79.0\) & \(90.0\) \\ antmaze-medium-play-v0 & \(0.0\) & \(89.1\) & \(33.5\) & \(82.0\) & \(81.8\) & \(89.0\) \\ antmaze-medium-diverse-v0 & \(6.4\) & \(97.0\) & \(32.7\) & \(72.0\) & \(82.3\) & \(88.0\) \\ antmaze-large-play-v0 & \(1.6\) & \(71.5\) & \(26.0\) & \(57.0\) & \(42.3\) & \(52.0\) \\ antmaze-large-dverse-v0 & \(4.4\) & \(73.0\) & \(58.5\) & \(71.0\) & \(60.6\) & \(68.0\) \\ average & \(29.8\) & \(80.5\) & \(43.8\) & \(76.7\) & \(73.4\) & \(89.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average normalized score on the D4RL benchmark of EDP trained with different algorithms. “Best” represents the online model selection metric, while “Average” is our running average at training metric.

Figure 7: Performance of EDP + TD3 on gym-locomotion tasks with varying initial noise scale.

not. Increasing K is able to avoid performance drop as evidenced by the last column of Tab. 2 and Tab. 4.

## Appendix D Negative Societal Impacts

In this paper, we propose an efficient yet powerful policy class for offline reinforcement learning. We show that this method is superior to most existing methods on simulated robotic tasks. However, some war robots or weapon robots might employ our EDP to learn strategic agents considering the generalization ability of EDP. Depending on the specific application scenarios, it might be harmful to domestic privacy and safety.

\begin{table}
\begin{tabular}{c c c} \hline \hline OMS K=100 & DDPM & Action Approx \\ \hline walker2d-medium-v2 & 86.9 & 85.5 \\ walker2d-medium-reply-v2 & 96.3 & 93.3 \\ walker2d-medium-expert-v2 & 111.5 & 111.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: The effect of action approximation.