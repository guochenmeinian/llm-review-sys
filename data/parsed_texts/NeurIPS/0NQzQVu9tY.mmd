# DMC-VB: A Benchmark for Representation Learning

for Control with Visual Distractors

 Joseph Ortiz, Antoine Dedieu, Wolfgang Lehrach, J. Swaroop Guntupalli,

Carter Wendelken, Ahmad Humayun, Guangyao Zhou, Sivaramakrishnan Swaminathan,

Miguel Lazaro-Gredilla, Kevin Murphy

Google DeepMind

{joeortiz,adedieu}@google.com

Equal contribution

###### Abstract

Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint. In this paper, we present the _DeepMind Control Vision Benchmark_ (DMC-VB), a dataset collected in the _DeepMind Control Suite_ to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.

## 1 Introduction

Reinforcement learning (RL) [46] provides a framework for learning behaviors for control, represented by policies, that maximize rewards collected in an environment. Online RL algorithms iteratively take actions--collecting observations and rewards from the environment--then update their policy using the latest experience. This online learning process is however fundamentally slow. Recently it has become clear that learning behaviors from large previously collected datasets, via behavioral cloning or other forms of offline RL [29], is an effective alternative way to build scalable generalist agents--see e.g. [40, 38].

Despite these advances, recent research indicates that agents trained on offline visual data often exhibit poor generalization to novel visual domains, and can fail under minor visual variations in the background or camera viewpoint [9, 41]. This poor generalization can be understood by formalizing environments as Markov Decision Processes (MDPs) with a factorized state consisting of control relevant and irrelevant variables [13]--see Appendix B for further discussion. In such MDPs, inorder to generalize to novel visual domains, an agent must learn latent representations that capture the information sufficient for control, yet that are minimal and therefore robust to irrelevant variations in the input--see [27, 44]. We call such representations "_control-sufficient_".

Several works derive latent representations for control with generative models of the input [49, 48]. By construction, such representations are not minimal, and may not be robust. Therefore various non-generative methods have been proposed as alternatives. These methods include contrastive learning [11, 28], latent forward models [21, 39], and inverse dynamics models [26, 25, 7]. While there exist several datasets to evaluate the generalization of these (and other) representation learning techniques to various visual perturbations [9, 41, 30, 50], these datasets do not provide the essential properties that are needed to thoroughly evaluate these representation learning methods for control.

To fill this gap, we introduce the _DeepMind Control Vision Benchmark_ (DMC-VB)--a dataset collected using the DeepMind Control Suite [47] and various extensions thereof. DMC-VB is carefully designed to enable systematic and rigorous evaluation of representation learning methods for control in the presence of visual variations, by satisfying six key desiderata. (a) It contains a diversity of tasks--including tasks where state-of-the-art algorithms struggle--to drive the development of novel algorithms. (b) It contains different types of visual distractors (e.g. changing background, moving camera) to study robustness to various realistic visual variations. (c) It includes demonstrations of differing quality to investigate whether effective policies can be derived from suboptimal demonstrations. (d) It includes both pixel observations and states, where states are relevant proprioceptive and exteroceptive measurements. Policies trained on states can then provide an upper bound to quantify the "_representation gap_" of policies trained on pixels. (e) It is larger than previous datasets. (f) It includes tasks where the goal cannot be determined from visual observations, for which recent work [7] suggests that pretraining representations is critical. As DMC-VB is the first dataset to satisfy these six desiderata (see Table 1) it is well placed to advance research in representation learning for control.

Accompanying the dataset, we propose three benchmarks that leverage the carefully designed properties of DMC-VB to evaluate representation learning methods for control. **(B1)** evaluates the degradation of policy learning in the presence of visual distractors, and, in doing so, quantifies the representation gap between agents trained on states and on pixel observations. We find that the simple behavior cloning (BC) baseline is the best overall method, and that recently proposed representation learning methods, such as inverse dynamics (ID) [25], do not show benefits on our benchmark. **(B2)** investigates a practical setting with access to a large dataset of mixed quality data and a few expert demonstrations. We find that leveraging mixed data for pretraining a visual representation improves policy learning. Finally, **(B3)** studies representation learning on demonstrations with random hidden goals, as may be the case when an agent collects data in a new environment via goal-free exploration. We find that representations pretrained on this data help few-shot policy learning on new tasks.

Figure 1: DeepMind Control Vision Benchmark.

## 2 Related work

**Environments for control:** Many environments have been developed to study the performance of RL agents. The popular _Arcade Learning Environment (ALE)_[4] proposes an interface to hundreds of Atari 2600 games, and has been used in groundbreaking works in deep RL [36; 22]. The _OpenAI Gym_[8] extends _ALE_ to board games, robotics, and continuous control tasks; the _DM Control suite_[47] also proposes a similar suite of environments. Several works [45; 23; 53; 2] extend these control environments to visual tasks with various types of distractors, including agent color, background, and camera pose. However, these environments evaluate the robustness of online RL agents to visual distractors and do not provide pre-collected trajectories for offline learning.

**Offline datasets for control:** To foster research in offline RL, _D4RL_[15] proposed a suite of datasets collected in the _OpenAI Gym_ which cover diverse tasks with properties such as sparse rewards, partial observability, multitasking, etc. However, as _D4RL_ only gives access to states, _VD4RL_[33] extends _D4RL_ to complex visual scenes. _VD4RL_ considers three locomotion tasks from the _DM Control Suite_, and collects datasets containing observations with visual distractors for each task by deploying five behavioral policies, ranging from random to expert. However, _VD4RL_ does not include states, which are useful to measure the representation gap between policies trained on images and on states. Second, the _VD4RL_ datasets released only cover a small fraction of the combinations of embodiments, policies and distractors. Finally, each dataset only contains \(100\)k steps which, as we show in Appendix I, can be too small for RL algorithms. The _RL unplugged_[20] datasets also collect trajectories on different environments from _ALE_ and the _DM Control Suite_. Recently, _Libero_ proposes several datasets to evaluate lifelong learning for robotics manipulation. Finally, _Colosseum_[41] studies generalization on \(20\) manipulation tasks, which can be systematically modified across \(14\) axes of variation. However, _RL unplugged_, _Libero_, and _Colosseum_ datasets lack visual distractor diversity in the data and do not include different behavioral policies.

**Representation learning for control:** Representation learning methods map a high-dimensional input (an image) into a compact low-dimensional space. Several studies show that pretrained representations can improve RL policy learning [51; 7]. Early works [49; 48] learn representations with auto-encoders. As these models reconstruct the observations, they fail to discard visual distractors and to generalize under visual distribution shifts [9]. To learn control-sufficient representations, several works have explored non-generative losses. Lamb et al. [26] showed that multi-step inverse dynamics (ID) can, theoretically and empirically, learn a "minimal" world model, which can be used for exploration and navigation. ID has also proven to be beneficial to train offline RL agents in the presence of visual distractors [25], and when the goal is a hidden variable [7]. Another approach, latent forward models (LFD), predict future latent representations from current ones [21]. Contrastive methods maximize the similarity between augmentations of the same data [11; 28; 23; 34] or temporally related images [39; 37]. Other works maximize the mutual information between the latents and control relevant variables such as the reward or actions [6; 14] or use masked auto-encoding [35; 42]. Our experiments (Sec. 5) benchmark simple variants of many of these methods on DMC-VB.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Dataset & Task diversity & Distractor diversity & Different policies & States and obs. & Large & Hidden goal \\ \hline \hline _D4RL_[15] & ✓ & ✗ & (✓) not for each task & ✗ & ✓ & ✓ \\ \hline _VD4RL_[33] & ✗ & (✓) not released & (✓) not released & ✗ & ✗ & ✗ \\ \hline _RL unplugged_[20] & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ \\ \hline _Libero_[31] & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\ \hline _Colosseum_[41] & ✓ & (✗) only for eval. & ✗ & ✓ & ✓ & ✗ \\ \hline
**DMC-VB** (ours) & ✓ **(B1)** & ✓ **(B1)** & ✓ **(B1-2)** & ✓ **(B1)** & ✓ **(B1-2-3)** & ✓ **(B3)** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Among existing offline RL datasets for continuous control tasks, DMC-VB is the only one to satisfy the six desiderata we have identified. We highlight the specific benchmark(s) within Sec.5 that leverage each of these properties. This table includes five relevant datasets and is not an exhaustive list. We exclude related environments which do not provide datasets for offline learning.

DeepMind Control Vision Benchmark

In this section, we describe our dataset, DMC-VB, that enables systematic evaluation of representation learning methods for control with visual distractors. A visual overview is presented in Fig.1.

### Dataset Characteristics / Desiderata

As highlighted in Table 1, DMC-VB is the first offline RL dataset to satisfy these six desiderata:

**Task diversity:** DMC-VB contains both simpler locomotion tasks and harder \(3\)D navigation tasks. We use the same three locomotion tasks as [33]: walker-walk, cheetah-run and humanoid-walk. The walking humanoid task is significantly harder due to its high degree of freedom action space. For each task, the maximum reward is attained by reaching a target velocity. In addition, we create a suite of seven navigation tasks, corresponding to seven maze layouts with three levels of complexity: empty, single-obstacle(\(\times 3\)) and multi-obstacle (\(\times 3\)). For each task, a quadruped ant is randomly placed in the maze and has to reach a randomly placed visible goal (a green sphere). We define a dense reward as the negative normalized shortest path length between the ant and the goal, respecting the maze layout--see Appendix C.3. This ensures the reward falls in the range \([-1,0]\).

**Distractor diversity:** For locomotion tasks, we leverage the _Distracting Control Suite_[45] to add visual variation to the agent color, camera viewpoint, and background. We use three different distractor settings: _none_ has no variations; _static_ has an image background, random static camera viewpoint and variable agent color; _dynamic_ has a video background, randomly moving camera and variable agent color. For backgrounds, we use synthetic videos or images of multiple objects falling from the Kubric dataset [18]. For the navigation tasks, visual distractors consist of different textures and colors applied to floors and walls of the maze environment.

**Demonstration quality diversity:** To produce datasets with diversity in demonstration quality, DMC-VB generates trajectories using behavioral policies of different skill levels, ranging from random to expert. We select checkpoints for each behavioral policy level as follows: _expert_ is the first checkpoint that achieves 95% of the reward achieved at convergence, _medium_ is the checkpoint that achieves 50% of the reward at convergence, _mixed_ uses eight evenly spaced checkpoints that achieve between 0-50% of the reward at convergence, _random_ samples actions randomly from the action space. We use the 95% reward checkpoint as the expert policy because we find that behavior diversity diminishes with more training steps. The reward distributions for our dataset are shown in Fig.2.

**State and visual observations:** Few works compare agents trained on pixel observations versus on states. In contrast, DMC-VB systematically collects both states and images which allows training agents on both to systematically quantify the representation gap.

**Large size:** Each dataset in DMC-VB contains \(1\)M steps, which is equivalent to \(2\)k episodes for the locomotion tasks and more than \(2\)k variable-length episodes for the ant maze tasks. This makes DMC-VB one order of magnitude larger than _VD4RL_[33], the most similar prior dataset. We motivate this choice in Appendix I through experiments which find that limiting expert data significantly harms BC agents, particularly for harder tasks and with visual distractors.

**Hidden goal:**[7] shows the benefits of inverse dynamics pretraining for tasks in which the goal is not determinable from the visual observations. Inspired by this finding, we include a variant of our navigation dataset in which the target sphere is not visible, which we study in Sec. 5.3.

### Dataset Generation

To generate a dataset of DMC-VB, for each task, we first train an online MPO agent [1] using states for \(2\)M steps. The states and rewards for each task are described in Appendices C.2 and C.3 respectively. Relevant training checkpoints are then selected for the behavioral policies. We generate demonstrations by rolling out the behavioral policy in the environment, collecting rewards, actions, states and visual observations. As in [52; 33], we use an action repeat of two in all environments to increase the change between successive frames and ease learning from pixels.

For navigation tasks, we collect three observations for each timestep: (a) a top-down view of the maze, (b) an egocentric view, (c) a follow-up view2--which is useful to infer the agent's state--see Fig.1. We additionally collect these same observations with the goal sphere hidden.

Footnote 2: We also collect an overhead view, as we see in Fig.1, but do not use it for training.

## 4 Agents and Visual Representation Learning for Control on DMC-VB

Given a DMC-VB dataset \(D=\{\ \tau_{i}\ \}_{i=1:N}\) of \(N\) trajectories pre-collected in an environment, an agent must learn a policy that maximizes the expected sum of the rewards when deployed in the same environment. Each trajectory contains observations, actions and rewards3: \(\tau=(\mathbf{o}_{1},r_{1},\mathbf{a}_{1},\mathbf{o}_{2},r_{2},\mathbf{a}_{2},...)\). Given an observation \(\mathbf{o}_{t}\), an agent selects an action \(\hat{\mathbf{a}}_{t}=\pi(\phi(\mathbf{o}_{t}))\), where \(\phi\) is a visual encoder network, and \(\pi\) is a policy network.

Footnote 3: We also collect states, i.e. \(\tau=(\mathbf{o}_{1},\mathbf{s}_{1},r_{1},\mathbf{a}_{1}...)\), although they are not used for policies trained on images.

The agent is trained in a two-stage procedure. The pretraining stage first learns the visual encoder network \(\phi\) to minimize a representation learning loss:

\[\phi^{*}\in\operatorname*{argmin}_{\phi}\ \mathbb{E}_{\tau\sim D}\left[\ \mathcal{L}_{\text{pretrain}}(\phi,\tau)\ \right]\.\] (1)

The second stage learns the policy network \(\pi\) by minimizing a policy learning objective with the visual encoder network \(\phi^{*}\) held constant:

\[\pi^{*}\in\operatorname*{argmin}_{\pi}\ \mathbb{E}_{\tau\sim D}\left[\ \mathcal{L}_{\text{policy}}(\pi,\phi^{*},\tau)\ \right]\.\] (2)

Architectures:We standardize \(\phi\) as a convolutional neural network (CNN) followed by a linear projection, and \(\pi\) as a multi-layer perceptron (MLP)--see Appendix D for details.

Frame stacking:As in [33], we use frame stacking of \(\ell=3\) and select actions as \(\hat{\mathbf{a}}_{t}=\pi(\phi(\tilde{\mathbf{o}}_{t}))\) where \(\tilde{\mathbf{o}}_{t}=(\mathbf{o}_{t-\ell+1},\dots,\mathbf{o}_{t})\). We drop the notation \(\tilde{\mathbf{o}}_{t}\) in this section for simplicity. Appendix H verifies, through an ablation study, that frame stacking is crucial for policy learning.

### Policy learning

As the primary focus is to benchmark visual representation learning for control, we limit the study to two simple objectives for learning our policy \(\pi\): behavioral cloning (BC) and TD3-BC [16].

Behavioral cloning (BC) learns a policy via supervised learning by minimizing the objective:

\[\mathcal{L}_{\text{BC}}(\pi,\phi,\tau)=\mathbb{E}_{(\mathbf{o}_{t},\mathbf{a }_{t})\sim\tau}\left\|\pi(\phi(\mathbf{o}_{t}))-\ \mathbf{a}_{t}\right\|_{2}^{2}.\] (3)

TD3-BC is a model-free offline RL algorithm that trains an actor and two critic networks [16]. TD3-BC uses the same critic objective as TD3 [17], and adds a BC term to the actor (policy) objective, to regularise the learned policy towards actions in the dataset \(D\) (see Appendix D for details):

\[\mathcal{L}_{\text{TD3-BC, Actor}}(\pi,\phi,\tau)=\mathbb{E}_{(\mathbf{o}_{t}, \mathbf{a}_{t})\sim\tau}\left[\lambda\ Q_{1}(\phi(\mathbf{o}_{t}),\pi(\phi( \mathbf{o}_{t}))-\left\|\pi(\phi(\mathbf{o}_{t}))-\ \mathbf{a}_{t}\right\|_{2}^{2} \right]\.\] (4)

Figure 2: Reward distribution for different behavioral policy levels in DMC-VB. Note the log scale on the vertical axis. Statistics of these distributions are summarized in Appendix C.1.

### Visual representation learning

We explore several representation learning methods to pretrain the encoder \(\phi\), before policy learning.

**Inverse Dynamics (ID):** An inverse dynamics model estimates the next action from the current observation and a future observation \(k\) steps ahead via the objective:

\[\mathcal{L}_{\text{ID}}(\phi,\tau)=\mathbb{E}_{(\mathbf{o}_{t},\mathbf{o}_{t+k}, \mathbf{a}_{t})\sim\tau}\left\|\mathbf{a}_{t}-f(\phi(\mathbf{o}_{t}),\;\phi( \mathbf{o}_{t+k}),\;k)\right\|_{2}^{2}.\] (5)

In practice, we replace \(\mathbf{o}_{t},\mathbf{o}_{t+k}\) with \(\tilde{\mathbf{o}}_{t},\tilde{\mathbf{o}}_{t+k}\) to combine ID with frame stacking and set \(k=1\).

**Latent Forward Dynamics (LFD):** The latent forward dynamics objective predicts the next latent observation from the current observation and action:

\[\mathcal{L}_{\text{LFD}}(\phi,\tau)=\mathbb{E}_{(\mathbf{o}_{t},\mathbf{a}_{t}, \mathbf{o}_{t+1})\sim\tau}\left\|\phi^{\text{EMA}}(\mathbf{o}_{t+1})-g(\phi( \mathbf{o}_{t}),\;\mathbf{a}_{t})\right\|_{2}^{2}.\] (6)

To avoid latent collapse [19], we encode \(\mathbf{o}_{t+1}\) using a target network \(\phi^{\text{EMA}}\), whose weights are an exponential moving average of the weights of \(\phi\), with decay rate \(0.99\).

**AutoEncoder (AE):** An autoencoder [52] jointly trains the encoder \(\phi\) with a decoder \(\psi\) to minimize the pixel reconstruction loss: \(\mathcal{L}_{\text{AE}}(\phi,\tau)=\mathbb{E}_{\mathbf{o}_{t}\sim\tau}\left\| \psi(\phi(\mathbf{o}_{t}))-\mathbf{o}_{t}\right\|_{2}^{2}\).

**Pretrained DINO encoder:** Lastly, we consider a pretrained DINO encoder [10]. Note that we need to pad the \(64\times 64\) observations in the DMC-VB dataset to the \(224\times 224\) size required by DINO.

### Baselines using privileged states

The presence of both visual observations and privileged states in DMC-VB enables the evaluation of realistic upper bounds on representation learning methods. We include two state-based baselines.

**BC (state):** This BC variant has access to the states at both training and evaluation time. The actions are predicted as \(\hat{\mathbf{a}}_{t}=\pi(\tilde{\phi}(\mathbf{s}_{t}))\), where \(\tilde{\phi}\) is a MLP.

**State prediction pretraining:** Representations are pretrained to predict states (only accessible during training) via the objective: \(\mathcal{L}_{\text{state}}(\phi,\tau)=\mathbb{E}_{(\mathbf{s}_{t},\mathbf{o}_ {t})\sim\tau}\left\|\mathbf{s}_{t}-h(\phi(\mathbf{o}_{t}))\right\|_{2}^{2}\), where \(h\) is a linear layer.

## 5 Benchmark Experiments

Accompanying DMC-VB, we propose three benchmark evaluations that examine the utility of pretrained visual representations for policy learning in the presence of visual variations. Each benchmark leverages unique properties of DMC-VB--see Table 1--and selects different data subsets to investigate the following questions on visual representation learning for control:

\(\bullet\) **(B1)** studies whether visual representation learning makes policies robust to distractors. (Sec. 5.1)

\(\bullet\) **(B2)** investigates whether visual representations pretrained on mixed quality data improve policy learning with limited expert data. (Sec. 5.2)

\(\bullet\) **(B3)** explores whether visual representations pretrained on tasks with stochastic hidden goals improve policy learning on a new task with fixed hidden goal and limited expert data. (Sec. 5.3)

**Notation:** Following Sec.4, we denote \(\mathcal{M}_{1}+\mathcal{M}_{2}\) the agent for which the method \(\mathcal{M}_{1}\) is used to pretrain the encoder \(\phi\), and \(\mathcal{M}_{2}\) is used to learn the policy \(\pi\). For instance, ID + BC refers to first pretraining the encoder with ID, followed by learning the policy with BC (with a frozen encoder). We denote NULL + BC the agent for which \(\phi\circ\pi\) is trained end-to-end with BC. In Secs. 5.2 and 5.3, when we pretrain on a dataset \(D_{1}\), then learn the policy on another dataset \(D_{2}\), we name the agent \(\mathcal{M}_{1}(D_{1})+\mathcal{M}_{2}(D_{2})\). In addition, NULL + BC (states) trains end-to-end \(\phi\circ\pi\) on states with BC. Lastly, Data refers to the average reward on the training dataset.

**Preprocessing details:** We first normalize actions in \([-1,1]\) and observations in \([-0.5,0.5]\). Second, as in [25], we pad each \(64\times 64\) image by \(4\) pixels on each side, and then apply a random cropping to return a randomly shifted \(64\times 64\) image. Finally, we apply a frame stacking of three consecutive observations. For models which use state, we center and normalize each state dimension.

**Training:** For both representation pretraining and policy learning, we use for \(400\)k Adam iterations on a single NVIDIA A100 GPU with batch size \(256\) and learning rate \(0.001\). Appendix D details all the architectures and hyperparameters used.

**Online evaluation:** Every \(20\)k training steps, we evaluate the agent online over \(30\) online rollouts in the evaluation environment. When visual distractors are present, the evaluation environment contains unseen visual distractors from the training distractor distribution. In the figures displayed in this section, we report the best (online) evaluation scores obtained through training.

### Policy learning with visual distractors

**Benchmark 1** evaluates the reward collected by different agents on the full DMC-VB expert and medium datasets in Fig.3. For the locomotion tasks, we find that the baseline NULL+BC is the top performing method. ID+BC matches (but does not exceed) this baseline, and the other pretraining methods all perform worse. This indicates that pretrained representations do not seem to help, at least in these benchmark experiments. With distractors, on walker-expert, walker-medium, and cheetah-medium, NULL+BC and ID+BC are the only methods that maintain high rewards suggesting that they learn representations that are robust to visual variations. Lastly, offline RL (NULL+TD3-BC) is outperformed by NULL+BC most of the time, and never exceeds it.

For the DMC-VB navigation tasks, NULL+BC is the best method and pretrained visual representations only harm performance. We observe similar results when plotting the fraction of successes in reaching the goal and the average velocity of the agent toward the goal in Appendix E.6. We observe that for many tasks, performance degrades more strongly with visual distractors for expert than for medium datasets; we expect this is due to the medium dataset having higher coverage (see Fig. 2), reducing the chances of failure by entering a highly out of distribution state.

**Inspecting the visual representations:** To provide insights into the learned visual representations, we freeze the encoder and train two decoders to reconstruct (a) observations, and (b) states (leveraging the states in DMC-VB). Fig. 4 shows that representations that achieve low state reconstruction error capture minimal control-relevant features and lead to better downstream policy performance (ID, BC). Meanwhile, representations that attain low observation reconstruction error are not robust to visual distractors and result in worse downstream policies (LFD, AE, DINO). For locomotion tasks

Figure 3: Online evaluation scores on the locomotion tasks [three top rows] and ant maze navigation tasks [bottom row] of DMC-VB, averaged over \(30\) trajectories, with standard errors. Higher reward is better. For locomotion task rows, results are grouped by distractor type and demonstration data quality. For the ant maze row, results are grouped by maze difficulty and demonstration data quality. NULL + BC is the best overall method. Pretrained representations offer no advantage with or without visual distractors. LFD + BC performs poorly, AE + BC and DINO + BC learn moderate policies, and ID + BC is comparable to NULL + BC. Full scores are in Appendix E.1 and the temporal evolution of rewards through training is plotted in Appendix E.2.

with visual distractors, both ID and BC achieve the highest observation reconstruction errors and the lowest state reconstruction errors, also discarding control-irrelevant information such as background and agent color. As expected the AE achieves the lowest observation reconstruction errors, and similar to DINO fails to learn good policies due to high state reconstruction errors. Finally, LFD has the highest state reconstruction errors, which explains its bad performance.

**State-based upper bounds:** Policies trained on states are unaffected by the presence of visual distractors, and provide an upper bound for representation learning methods. Fig. 3 shows that the representation gap between policies trained on states versus on observations is minimal in the absence of distractors, but widens significantly in the presence of dynamic distractors. Lastly, we found no advantage in pretraining the encoder to predict the state, which we attribute to the fact that some state components (e.g. velocities) are hard to predict--see Appendix E.5 for details.

### Pretraining representations on mixed data helps few-shot policy learning

**Benchmark 2** leverages DMC-VB datasets collected with different behavioral policy levels to investigate whether pretraining a representation on sub-optimal (mixed) data helps policy learning when expert data is scarce. Specifically, for each locomotion task, we first train two vision encoders, using ID and BC on the full _mixed_ quality DMC-VB datasets. Second, we freeze the encoders and train the policy networks using only \(1\%\) of the _expert_ dataset (\(20\) trajectories). Fig. 5 compares these two-stage training agents (in green and red) to a BC agent trained on \(1\%\) of the expert dataset (blue), and a BC agent trained on the combined \(1\%\) expert dataset and full mixed dataset (orange). The results suggest that (a) limiting expert data impacts performance, (b) merely combining mixed and limited expert data leads to a drop in performance for BC, and (c) pretraining a representation on mixed data provides a performance boost, especially in the presence of visual distractors.

### Pretraining on tasks with stochastic hidden goals helps few-shot policy learning

Figure 4: [Left] Least-squared test error for reconstructing the observations [top] and states [bottom], averaged over the different DMC-VB locomotion tasks and policies. Lower is better. As results are averaged over \(150\)k samples, the standard errors are too small to be visible. See Appendix E.3, for a detailed breakdown per task and distractor. [Right] Observation reconstruction examples. See Appendix E.4, for additional image reconstructions. BC and ID both (a) discard visual distractors (background object and agent color), and (b) reach the lowest state reconstruction errors.

Figure 5: Pretraining encoders on mixed data improves performance when a BC policy is trained on a small expert dataset. BC and ID pretraining perform similarly. For each task, performance is reported as the proportion of reward obtained by BC on full expert data without distractors (higher is better). We include full results including _cheetah_, and LFD/AE pretraining in Appendix F.

**Benchmark 3** considers a variant of the ant maze datasets of DMC-VB, in which image observations are rendered without the goal (green sphere). This setting mimics scenarios in which data is generated by an agent exploring a new environment in an open-ended manner without explicit goals. For each maze, we are given a large _expert_ dataset of \(1\)M steps. In each trajectory, the agent moves from a random initial position to a random final position--note the goal cannot be derived from visual observations. We denote this pretraining dataset: _stochastic goals_ in Fig. 6. Additionally, for each maze, we have access to five small datasets of ten _expert_ trajectories each. All trajectories in each dataset go from a fixed start to a fixed goal location, with noise added to the agent's body initialization. We denote this dataset: _fixed goal_. As in **(B2)**, Fig. 6 compares BC agents trained directly on the small dataset, to those with two-stage training, for which representations are pre-trained on the large _stochastic goals_ dataset. Our findings indicate that pretraining a representation on a collection of tasks with stochastic hidden goals aids policy learning on new tasks with fixed hidden goals. Despite prior findings that ID pretraining on tasks with stochastic hidden goals performs better than BC [7], we find that BC pretraining outperforms ID pretraining on this benchmark.

## 6 Conclusions

We propose DMC-VB, a dataset designed for systematic evaluation of representation learning methods for control tasks in environments with visual distractors. DMC-VB is the first dataset to satisfy six key desiderata, enabling detailed and diverse evaluations. Alongside our dataset, we propose three benchmarks on DMC-VB which compare several established representation methods.

Our results suggest, rather surprisingly, that current pretraining methods (with or without generative losses) do not help BC policy learning on DMC-VB. We leave a detailed investigation into the reason for these results to future work. We also expose a large representation gap between the performance of (a) BC with and without visual distractors, and (b) BC on pixels vs. BC on states, which highlights the need for better representation learning methods. In addition, our findings reveal the potential of leveraging suboptimal datasets, and tasks with stochastic hidden goals, to pretrain representations and learn better policies on DMC-VB with limited expert data.

DMC-VB's unique features present researchers with the tools to investigate fundamental questions in representation learning for control; and to systematically benchmark the performance of future representation learning methods, ultimately contributing to the creation of robust, generalist agents. Our work has two primary limitations. First, DMC-VB could be extended to a broader range of environments, including sparse rewards, multiple agents, complex manipulation tasks, or stochastic dynamics. Second, the synthetic nature of our visual distractors may raise questions about the generalization of our findings to real-world tasks. While our findings may be applicable to robotics tasks, incorporating more diverse and realistic distractors would strengthen our findings.

## References

* Abdolmaleki et al. [2018] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=S1ANxQW0b.
* Almuzairee et al. [2024] Abdulaziz Almuzairee, Nicklas Hansen, and Henrik I. Christensen. A recipe for unbounded data augmentation in visual reinforcement learning, 2024.
* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.

Figure 6: Pretraining representations on tasks with stochastic hidden goals improves performance when subsequently training a policy on tasks with a fixed hidden goal and limited expert data. Higher reward is better. Fraction of successful episodes and average velocity metrics are reported in Appendix G.

* [4] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* [5] Richard Bellman. A markovian decision process. _Journal of mathematics and mechanics_, pages 679-684, 1957.
* [6] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based rl. _arXiv preprint arXiv:2204.08585_, 2022.
* [7] David Brandfonbrener, Ofir Nachum, and Joan Bruna. Inverse dynamics pretraining learns good representations for multitask imitation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* [9] Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Yu, Chelsea Finn, and Karol Hausman. What makes pre-trained visual representations successful for robust manipulation? _arXiv preprint arXiv:2312.12444_, 2023.
* [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [12] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient rl with rich observations via latent state decoding. In _International Conference on Machine Learning_, pages 1665-1674. PMLR, 2019.
* [13] Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In _International Conference on Learning Representations_, 2021.
* [14] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Robust predictable control. _Advances in Neural Information Processing Systems_, 34:27813-27825, 2021.
* [15] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [16] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [17] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3749-3761, 2022.
* [19] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [20] Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gomez, Konrad Zolna, Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged: A suite of benchmarks for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:7248-7259, 2020.

* [21] Zhaohan Guo, Shantanu Thakoor, Miruna Pislar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. _Advances in neural information processing systems_, 35:31855-31870, 2022.
* [22] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* [23] Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. In _International Conference on Robotics and Automation_, 2021.
* [24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* [25] Riashat Islam, Manan Tomar, Alex Lamb, Yonathan Efroni, Hongyu Zang, Aniket Didolkar, Dipendra Misra, Xin Li, Harm Van Seijen, Remi Tachet des Combes, et al. Agent-controller representations: Principled offline rl with rich exogenous information. _International Conference on Machine Learning_, 2023.
* [26] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Didolkar, Dipendra Misra, Dylan Foster, Lekan Molu, Rajan Chari, Akshay Krishnamurthy, and John Langford. Guaranteed discovery of control-endogenous latent states with multi-step inverse models. _arXiv preprint arXiv:2207.08229_, 2022.
* [27] J. Langford and A. Lamb. Discovering agent-centric latent dynamics in theory and in practice, 2023. ICML tutorial.
* [28] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In _International conference on machine learning_, pages 5639-5650. PMLR, 2020.
* [29] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* [30] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. _arXiv preprint arXiv:2306.03310_, 2023.
* [31] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [32] Yuren Liu, Biwei Huang, Zhengmao Zhu, Honglong Tian, Mingming Gong, Yang Yu, and Kun Zhang. Learning world models with identifiable factorization. _Advances in Neural Information Processing Systems_, 36, 2024.
* [33] Cong Lu, Philip J Ball, Tim GJ Rudner, Jack Parker-Holder, Michael A Osborne, and Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations. _arXiv preprint arXiv:2206.04779_, 2022.
* [34] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: Towards universal visual reward and representation via value-implicit pre-training. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=YJ7o2wetJ2.
* [35] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? _Advances in Neural Information Processing Systems_, 36, 2024.
* [36] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.

* Nair et al. [2022] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. _arXiv preprint arXiv:2203.12601_, 2022.
* Team et al. [2024] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In _Proceedings of Robotics: Science and Systems_, Delft, Netherlands, 2024.
* van den Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* Padalkar et al. [2023] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.
* Pumacay et al. [2024] Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, and Dieter Fox. The colosseum: A benchmark for evaluating generalization for robotic manipulation. _arXiv preprint arXiv:2402.08191_, 2024.
* Radosavovic et al. [2022] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In _6th Annual Conference on Robot Learning_, 2022. URL https://openreview.net/forum?id=KWCZfuqshd.
* Sethian [1996] J.A. Sethian. A fast marching level set method for monotonically advancing fronts. _Proceedings of the National Academy of Sciences_, 1996.
* Soatto [2011] Stefano Soatto. Steps towards a theory of visual information: Active perception, signal-to-symbol conversion and the interplay between sensing and control. _arXiv preprint arXiv:1110.2053_, 2011.
* Stone et al. [2021] Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite-a challenging benchmark for reinforcement learning from pixels. _arXiv preprint arXiv:2101.02722_, 2021.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Tassa et al. [2018] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* Wahlstrom et al. [2015] Niklas Wahlstrom, Thomas B Schon, and Marc Peter Deisenroth. From pixels to torques: Policy learning with deep dynamical models. _arXiv preprint arXiv:1502.02251_, 2015.
* Watter et al. [2015] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. _Advances in neural information processing systems_, 28, 2015.
* Xing et al. [2021] Eliot Xing, Abhinav Gupta, Sam Powers*, and Victoria Dean*. Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts. In _NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2021. URL https://openreview.net/forum?id=DdglKo8hBq0.
* Yang and Nachum [2021] Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential decision making. In _International Conference on Machine Learning_, pages 11784-11794. PMLR, 2021.
* Yarats et al. [2021] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10674-10681, 2021.
* Yuan et al. [2024] Zhecheng Yuan, Sizhe Yang, Pu Hua, Can Chang, Kaizhe Hu, and Huazhe Xu. Rl-vigen: A reinforcement learning benchmark for visual generalization. _Advances in Neural Information Processing Systems_, 36, 2024.

Dataset Documentation

The overall characteristics, properties and intended use of the dataset are laid out in the main paper, predominantly in Section 3. Appendix C also provides further details on the environments and dataset reward distributions.

**License.** The DMC-VB dataset is released under the CC-BY license. The software code to reproduce the benchmark experiments on DMC-VB is released under the Apache 2.0 license.

**Hosting and Maintenance.** The dataset is hosted in a Google Cloud Storage Bucket at: https://console.cloud.google.com/storage/browser/dmc_vision_benchmark. The code is hosted on GitHub at: https://github.com/google-deepmind/dmc_vision_benchmark. The GitHub page contains instructions to download the dataset using the Google Cloud CLI. Metadata for the dataset is automatically provided via the Google Cloud Storage Bucket. Examples of the dataset and an overview of the dataset structure are provided in the GitHub README.md. We, as the dataset owners, will carry out any necessary maintenance to the dataset and software code in order to ensure that it is preserved and accessible in the long term.

**Reading the data.** Detailed instructions for downloading the data are provided in our GitHub page. The GitHub code contains examples of loading the data for our benchmark experiments. The data is loaded using Tensorflow Datasets and the code to load the data can easily be re-used for other experiments.

**Reproducibility.** We have followed the Machine Learning Reproducibility Checklist in order to ensure that all results are easily reproducible. For benchmark experiments, we detail all model architectures and hyperparameters in Appendix D, and the evaluation procedure in the main paper. Scripts to reproduce all experiments in the main paper are provided in our GitHub page. The code release contains all relevant models and evaluation procedures, as well as instructions for accessing the dataset.

**Ethics and Responsible Use.** In terms of the ethical implications of our dataset, AI agents that can learn from offline visual data will eventually be capable of interacting with humans in many domains both physical and virtual, and alignment will be crucial. Our dataset captures simple locomotion and navigation problems with visual variations. Consequently, AI agents trained on this data will have limited capabilities and thus currently minimal ethical considerations.

**Privacy.** Our dataset does not contain any personally identifiable information, and so we do not consider there to be any privacy concerns with our dataset.

**Responsibility Statement.** We, as the dataset owners, confirm that we bear responsibility in case of violation of rights.

## Appendix B Deterministic Exogenous Block Markov Decision Process

A Markov decision process (MDP) [5] is the tuple \(\text{MDP}=(\mathcal{S},\ \mathcal{A},\ \mathcal{O},\ T,\ r,\ q(\mathbf{o}|\mathbf{s}))\) where \(\mathcal{S}\) is the set of states \(\mathbf{s}\in\mathcal{S}\), \(\mathcal{A}\) is the set of actions \(\mathbf{a}\in\mathcal{S}\), \(\mathcal{O}\) is the set of observations \(\mathbf{o}\in\mathcal{O}\), \(T(\mathbf{s}^{\prime}|\mathbf{s},\mathbf{a})\) is dynamics function, \(r(\mathbf{s},\mathbf{a})\) is the reward function, \(q(\mathbf{o}|\mathbf{s})\) is the emission function.

DMC-VB is generated in environments that can represented by Deterministic Exogenous Block MDPs (EX-BMDP) [13]. In this MDP family, the state is factorized into an endogenous latent and an exogenous latent, as shown in Fig.7. The endogenous latent (\(\mathbf{s}_{1}\)) captures all factors controllable by the agent while the exogenous latent (\(\mathbf{s}_{2}\)) captures all other factors that do not affect the agent or the reward. Additionally, the dynamics are factorized as \(T(\mathbf{s}^{\prime}|\ \mathbf{s},\mathbf{a})=T_{1}(\mathbf{s}^{\prime}_{1}\ | \ \mathbf{s}_{1},\mathbf{a})\ T_{2}(\mathbf{s}^{\prime}_{2}\ |\ \mathbf{s}_{2})\), where \(T_{1}\) is deterministic and \(T_{2}\) can be stochastic. The block assumption preserves the full observability of the MDP by ensuring that it is possible to recover \(\mathbf{s}_{1}\) and \(\mathbf{s}_{2}\) from the observation [12, 13]. As the exogenous latent cannot influence the reward, the dashed line in Fig.7 indicates that the optimal action depends only on the endogenous latent.

The EX-BMDP framework represents a realistic family of MDPs that appear in many scenarios involving control from visual observations. In the _DM Control Suite_ environments considered in our dataset, the endogenous variables are the agent's state (including joint positions, velocities, goal location, etc.), while the exogenous variables are the camera viewpoint, background and agent color. Note that, in our experiments, we apply a frame stacking of three consecutive observations to preserve full observability as the agent's velocity is not computable from a single frame. This guarantees that the block assumption is satisfied.

In robotic tasks, the endogenous variables capture the robot gripper and objects it can interact with while the background and other control-irrelevant objects are exogenous variables. Lastly, in autonomous driving, endogenous variables include all vehicles, agents and obstacles, while background scenery is an exogenous variable.

Despite their simplicity, EX-BMDPs are quite versatile, and can themselves be sub-divided by further factorization of the endogenous state [32].

Figure 7: EX-BMDP.

Dataset Characteristics

### Reward Distribution

Table 2 presents statistics of the reward distributions for the DMC-VB datasets.

### State spaces

**Locomotion tasks**. The state space is unmodified from the original DM Control Suite environments and contains only proprioceptive measurements. The state is formed by concatenating the following measurements for each task.

* walker-walk: orientations, height, joint velocities (total 24 dimensional).
* cheetah-run: joint positions, joint velocities (total 17 dimensional).
* humanoid-walk: center of mass velocity, extremity positions, head height, joint angles, torso velocity, joint velocities (total 67 dimensional).

**Ant maze task**. The state space is formed by concatenating the proprioceptive measurements from the DM Control Suite ant maze environment and additional exteroceptive measurements of the ant and goal positions. The original proprioceptive states are: appendage positions, body positions, body quaternions, egocentric target vector, end effector positions, joint positions, joint velocities, sensor accelerometer, sensor gyroscope, and touch sensor. We added the exteroceptive states: ant position, goal position, world-frame vector from the ant to the goal, egocentric vector from the ant to the goal, and shortest path distance from the ant to the goal. The total state space is 167 dimensional.

### Rewards

**Locomotion tasks**. We use the default reward from the DM Control Suite environment which is 1 when the walker attains a velocity greater than the target velocity and is zero otherwise.

**Ant maze**. We define our own dense reward as:

\[r=-d/d_{max}\] (7)

where \(d\) is the current shortest path distance from the ant to the target, and \(d_{max}\) is the maximum shortest path distance to the target from any point in the maze. We use the Fast Marching Method

\begin{table}
\begin{tabular}{l l r r r r r r r r r} \hline \hline  & Task & Policy level & Steps & Mean & Std. Dev. & Min. & P25 & Median & P75 & Max. \\ \hline \multirow{8}{*}{\begin{tabular}{} \end{tabular} } & \multirow{4}{*}{walker} & expert & 1M & 957.8 & 20.4 & 831.9 & 946.8 & 960.1 & 970.9 & 993.0 \\  & & medium & 1M & 513.8 & 108.5 & 9.7 & 503.9 & 534.1 & 563.5 & 694.3 \\  & & mixed & 1M & 177.5 & 179.0 & 10.3 & 48.8 & 86.9 & 255.2 & 622.2 \\  & & random & 1M & 40.1 & 8.2 & 26.6 & 33.7 & 37.9 & 44.9 & 80.9 \\ \cline{2-11}  & \multirow{4}{*}{cheetah} & expert & 1M & 890.5 & 18.5 & 688.8 & 882.3 & 895.1 & 903.3 & 917.8 \\  & & medium & 1M & 454.8 & 26.3 & 86.6 & 446.4 & 457.1 & 468.0 & 495.1 \\  & & mixed & 1M & 284.3 & 132.0 & 33.9 & 151.9 & 336.9 & 391.1 & 465.3 \\  & & random & 1M & 6.8 & 2.6 & 1.0 & 4.9 & 6.5 & 8.4 & 22.5 \\ \cline{2-11}  & \multirow{4}{*}{humanoid} & expert & 1M & 766.0 & 21.3 & 651.4 & 753.7 & 770.7 & 782.5 & 810.8 \\  & & medium & 1M & 410.2 & 15.9 & 10.2 & 402.1 & 410.8 & 420.0 & 449.2 \\  & & mixed & 1M & 139.9 & 137.2 & 0.1 & 5.0 & 60.3 & 272.1 & 372.6 \\  & & random & 1M & 1.1 & 0.8 & 0.0 & 0.5 & 1.0 & 1.6 & 8.2 \\ \hline \multirow{8}{*}{
\begin{tabular}{} \end{tabular} } & \multirow{4}{*}{empty} & expert & 1M & -26.6 & 20.4 & -160.9 & -39.7 & -22.8 & -10.8 & -0.9 \\  & & medium & 1M & -172.2 & 138.7 & -937.3 & -247.8 & -131.5 & -70.8 & -2.1 \\ \cline{1-1} \cline{2-11}  & \multirow{2}{*}{single-} & expert & 1M & -65.5 & 63.6 & -603.4 & -98.7 & -46.1 & -14.3 & -0.7 \\ \cline{1-1}  & & medium & 1M & -131.1 & 185.5 & -1156.9 & -146.3 & -61.0 & -20.2 & -0.7 \\ \cline{1-1} \cline{2-11}  & \multirow{4}{*}{obstacle} & expert & 1M & -76.6 & 68.2 & -512.1 & -115.6 & -60.7 & -19.1 & -0.9 \\ \cline{1-1}  & & medium & 1M & -176.0 & 181.2 & -1036.7 & -240.6 & -118.7 & -46.8 & -0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Reward statistics for episodes in the DMC-VB dataset. Episodes in the locomotion environments are all 500 timesteps while ant maze episodes are variable length as they terminate when the agent reaches the goal. Note rewards are positive in locomotion tasks and negative in ant maze tasks, with higher rewards being always better.

algorithm [43] to compute the shortest path lengths in the maze. Episodes terminate if and when the ant makes contact with the target.

### Further details for ant maze dataset

Behavioral policy training:We train one behavioral policy per maze layout (per task) which learns to navigate from arbitrary start to goal positions. The behavioral policy takes as input the proprioceptive states, the ant position, the target position, the world-frame vector from ant to target, and the egocentric vector from ant to target. The maze layout is not part of the state space and the agent must learn the maze layout from the reward. Ant and target positions are initialized randomly at the start of each episode.

Observations:During generation, we collect four observations for each timestep: (a) a top-down view of the maze, (b) an egocentric view, (c) a follow-up view-which is useful to infer the proprioceptive states--(d) an overhead view. See Fig.1. We additionally collect these same observations by hiding the goal. Note that we do not use the overhead view for training.

Architecture and hyperparameters

Table 3 summarizes all the different architectures and hyperparameters used in our experiments. We detail some choices below.

### Encoders

We use a simple observation encoder architecture, similar to Brandfonbrener et al. [7]. The only difference we make is that similar to [33], we use a frame stacking of \(3\). The stacked RGBs input, of size \(64\times 64\times 9\) (for locomotion tasks) or \(64\times 64\times 27\) (for ant-maze tasks, since we have three images at each timestep) go through a convolutional neural network (CNN) with four layers. The CNN layers use filters of size \(3\times 3\), strides of \((2,2,1,1)\), gaussian error linear units (gelu) [24] activations, and an increasing number of \((32,64,128,256)\) channels. The CNN output, of size \(16\times 16\times 256\) is flattened into a \(65,536\)-dimensional vector before being mapped to a low dimensional vector of size \(64\) via a linear _trunk_ layer (which is the layer with the largest number of parameters of the model). Finally, the output goes through a normalization layer [3] and a hyperbolic tangent activation function.

For TD3-BC, we use separate actor trunk layer and critic trunk layers. For good performance, we found it important to use the actor loss to update (a) the CNN shared by the actor and critic encoder, (b) the actor trunk layer, and (c) the critic trunk layer.

For the BC agent trained on states, we do not use frame stacking or the CNN. The state encoder consists of the linear trunk layer, followed by the normalization layer and a tanh activation.

### Default MLP module

The default MLP modules in our experiments consist of a two hidden layers MLP, each of hidden size \(256\), with rectified linear unit (relu) activation functions. Since all the actions in DMC-VB are normalized to be between \(-1\) and \(1\), we apply a hyperbolic tangent activation to the output.

### Decoders

Our decoder network draws inspiration from [52] and reverts the visual encoder. First, we map the \(64\)-dimensional observation encoding back to a \(65,536\)-dimensional vector, which is then unflattened into a \(16\times 16\times 256\) tensor. This tensor goes through a deconvolutional neural network (DeCNN), with filters of size \(3\times 3\) filters, strides of \((1,1,2,2)\), gaussian error linear units (gelu) [24], and a decreasing number of channels of \((128,64,32,9)\). We do not use activation for the last layer. This last output, of dimension \(64\times 64\times 9\), is then unflattened into three images (for locomotion tasks) or nine images (for ant-maze tasks), each image being of size \(64\times 64\times 3\).

The state reconstruction module used in **(B1)** is a default MLP. However, we do not use activation for the last layer, which predicts the state.

### Agent details

This section details the different agents used.

BcFor BC, the action prediction network is the default MLP above.

IdWith the notations of Equation 5, \(f\) concatenates the embeddings \(\phi(\mathbf{o}_{t})\), \(\phi(\mathbf{o}_{t+k})\), and (if present) the embedding of \(k\) and passes them the inverse dynamics action prediction network, which is the default MLP above. Our code supports setting the number \(k\) of ID steps to a fixed value, or sampling uniformly in the range \(\{1,\dots,k_{\text{max}}\}\) at each iteration. For the latter, we use a linear layer to map \(k\) to a \(64\)-dimensional embedding.

LfWith the notations of Equation 6, \(g\) concatenates \(\phi(\mathbf{o}_{t})\) and an embedding of \(\mathbf{a}_{t}\), before passing them through a latent forward predictor network, The inverse dynamics action prediction network, which is the default MLP above. As detailed in Equation 6, we encode the future observation using a target network \(\phi^{\text{EMA}}\), which has the same architecture as the encoder network \(\phi\) and whose 

\begin{table}
\begin{tabular}{l l l} \hline Module & Hyperparameter & Value \\ \hline \hline Observation encoder & Frame stacking & \(3\) \\  & CNN number of channels & \((32,64,128,256)\) \\  & CNN kernel sizes & \((3,3,3,3)\) \\  & CNN activation & gelu \\  & CNN kernel strides & \((2,2,1,1)\) \\  & Output activation & tanh \\  & Output dimension & \(64\) \\ \hline State encoder & Frame stacking & \(1\) \\  & Output activation & tanh \\  & Output dimension & \(64\) \\ \hline State predictor layer & Output dimension & Equal to state dimension \\ \hline Action predictor & Hidden layer sizes & \((256,256)\) \\ ID action predictor & Activation function & relu \\  & Output activation & tanh \\  & Output dimension & Equal to action dimension \\ \hline LFD predictor & Hidden layer sizes & \((256,256)\) \\ Action encoder & Activation function & relu \\  & Output activation & tanh \\  & Output dimension & \(64\) \\  & LFD target network decay rate & \(0.99\) \\ \hline ID step embedding & Output dimension & \(64\) \\  & Number of ID steps & \(1\) (default) \\  & Sample ID steps & False (default) \\ \hline Actor, Critic 1 and Critic 2 & Hidden layer sizes & \((256,256)\) \\  & Activation function & relu \\  & Last layer activation & tanh \\  & Output dimension & 64 \\ \hline TD3-BC & Trade-off \(\alpha\) & \(2.5\) \\  & Discount factor & \(0.99\) \\  & Policy noise & \(0.2\) \\  & Policy noise clipping & \(0.5\) \\  & Policy update frequency & 2 \\  & Target network decay rate & \(0.99\) \\  & Loss for shared CNN & actor loss \\  & Loss for actor/critic trunks & actor loss \\ \hline Observation decoder & Linear layer dimension & \(16\times 16\times 256\) \\  & DeCNN number of channels & \((128,64,32,9)\) \\  & DeCNN kernel sizes & \((3,3,3,3)\) \\  & DeCNN activation & gelu \\  & DeCNN kernel strides & \((1,1,2,2)\) \\  & Output activation & Not used \\  & Output shape & Equal to observation shape \\ \hline State decoder & Hidden layer sizes & \((256,256)\) \\  & Activation function & relu \\  & Output activation & Not used \\  & Output dimension & Equal to state dimension \\ \hline \hline Learning & Number of iterations & \(400,000\) \\  & Batch size & \(256\) \\  & Optimizer & Adam \\  & Learning rate & \(0.001\) \\  & Online evaluation frequency & \(20,000\) \\ \hline Online evaluation & Number of runs & \(30\) \\  & Action repeat & \(2\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters and architecture used for the different agents evaluated on DMC-VBweights \(\mu^{\text{EMA}}\) are an exponential moving average of the weights of the teacher network \(\mu\). At each gradient update, we set \(\mu^{\text{EMA}}\leftarrow\beta\mu^{\text{EMA}}+(1-\beta)\mu\), where \(\beta\) is the decay rate, which we fix to \(0.99\).

AeWe use the decoder described above.

Td3-BcWith the notations of Equation 4, on each mini-batch \(\mathcal{B}\), in order to be agnostic to the scalar of the reward, we set \(\lambda=\frac{\alpha}{|\mathcal{B}|}\sum_{\omega\in\mathcal{B}}|Q_{1}(\phi( \mathbf{o}),\pi(\phi(\mathbf{o}))|\) with \(\beta=2.5\). Here, TD3-BC takes as input (a stack) of images. The actor and the two critic networks are the default MLP modules above. We use a decay rate of \(0.99\) for the target actor and critic networks--which are used to compute the target for both critics. Note that, as in [16], the actor network and the target networks are updated twice less frequently than the critics.

For good performance, we found it important to use (a) a separate linear trunk layer for the actor and the critic, and (b) the actor loss to update the shared CNN and the actor and critic trunk layers. This guarantees that for \(\alpha=0\), we recover BC.

[MISSING_PAGE_FAIL:20]

### Time series of rewards

As indicated in the main text, we evaluate each method online every \(20\)k training steps. Fig.8 displays the temporal evolution of the average rewards, when trained on the full expert datasets for locomotion tasks. As expected, we find that training on state information converges very fast and that humanoid training can take a large number of steps due to the difficulty of the task.

Figure 8: Time-series of rewards scores on the locomotion tasks of DMC-VB, averaged over \(30\) trajectories, with standard errors represented as the shaded area. These are all trained on expert trajectories. Higher reward is better.

[MISSING_PAGE_EMPTY:22]

### Observations reconstructions examples

Fig. 11 presents some observations reconstructed by the different representation methods, on the walker-walk tasks, with dynamic distractors.

Figure 11: Observation reconstruction from a decoder with frozen encoder, for the different representation learning methods.

### State prediction on locomotion datasets

For the method State + BC, during pretraining, a visual encoder is trained to directly regress the state from the corresponding observation. In Fig.12, we show the \(\ell_{1}\) state prediction error on held out evaluation data for the locomotion tasks. The error is broken down into errors for each part of the state space. Note that, as we have centered and normalized each state dimension during preprocessing, the errors for the different parts are comparable.

Despite frame stacking, we find that velocity is the most challenging to predict. Additionally, we see that the state for humanoid is much harder to predict than for walker and cheetah due to the high degree of freedom body.

### Additional metrics on ant maze environments

For ant-maze environments in **(B1)**, we use the total reward per episode to compare the different agents in Sec.5.1. Here we report two additional metrics: (a) the fraction of success in reaching the goal over the \(30\) online evaluation rollouts (Fig. 13), and (b) for trajectories reaching the goal, the average velocity towards the goal (Fig. 14). These are useful measures of an agent's behavior in the environment that are independent of the initial distance between the agent and the goal, unlike the reward.

These metrics highlight the gap between the LFD + BC agent and other agents. The LFD + BC agent never manages to reach the goal, despite often attaining reasonable scores in the reward.

Figure 14: Average velocity of the agent towards the goal during online evaluation of different benchmark models in Ant maze. Velocity is zero if the agent does not succeed in reaching the goal.

Figure 12: Absolute error evaluation loss for state prediction on the locomotion datasets, when regressed directly from the corresponding image observation.

Additional results for Benchmark 2

In addition to the inverse dynamics (ID) and the behavior cloning (BC) methods presented in Fig. 5, we evaluate latent forward dynamics (LFD) and autoencoder (AE) for representation learning on the mixed dataset. We present extended results including those additional methods in Fig. 15.

## Appendix G Additional metrics for Benchmark 3

Similar to Benchmark 1 above, we present the fraction of success and the average velocity metrics for ant maze for Benchmark 3. Fig. 16 shows the fraction of success and average agent velocity metrics respectively for Benchmark 3. These metrics are consistent with the reward reported in Fig. 6, in showing that BC pretraining on stochastic hidden goal tasks helps few-shot policy learning.

Figure 16: Fraction of success and average velocity of the agent towards the goal during online evaluation of Benchmark 3. Average velocity is only computed for trajectories that reach the goal and is zero otherwise.

Figure 15: Pretraining encoders on mixed data using different representation learning objectives improve performance when BC policy is trained on small expert data. Performance using different pretraining objectives is shown as the proportion of reward obtained by BC on full expert data without distractors for each task.

Frame stacking ablation on locomotion datasets

Fig. 17 compares BC with frame stacking of three consecutive frames versus without frame stacking on DMC-VB expert datasets for locomotion tasks. We see that frame stacking is crucial for learning, likely due to the fact that the agent's velocity is not inferable from a single observation. As a consequence of this result, we use frame stacking in all experiments.

## Appendix I Dataset size study

To validate the choice of making DMC-VB a large dataset with 1M steps per data subset, we evaluate the performance of behavioral cloning on smaller percentages of the dataset. We see in Fig.18 that for harder tasks, and in the presence of visual distractors, BC agents benefit from large training datasets. This suggests that _VD4RL_[33], which only contains \(100\)k steps per dataset, is not sufficiently large to evaluate visual representation learning for control, and this motivates our decision to make DMC-VB \(10\times\) larger.

Figure 17: Frame stacking significantly boosts BC performance on DMC-VB.

Figure 18: Performance of behavioral cloning without pretraining (NULL+BC) on different percentages of the expert dataset for the DMC-VB locomotion tasks. Rewards are plotted as a proportion of the reward when using 100% of the distractor-free (_none_) dataset. Scores significantly drop when using smaller proportions of the dataset.

Multistep Inverse Dynamics Pretraining

In experiments in the main paper, we found that single-step inverse dynamics pretraining is consistently on par or worse than the simple behavioral cloning baseline. Here we evaluate multistep inverse dynamics pretraining corresponding to different values of \(k\) in Equation 5. Figure 19 shows that as the value of \(k\) increases, the performance of ID pretraining converges towards the performance of behavioural cloning. For larger \(k\) the future frame becomes less useful for predicting the current action and the ID objective becomes equivalent to behavioral cloning. The figure also includes the BC + BC baseline, in which pretraining and policy learning share the same objective and data. This is equivalent to NULL + BC and ID + BC as \(k\) becomes large.

Figure 19: k-step inverse dynamics pretraining is denoted ID-k. Performance improves for larger values of k and tends towards the performance of BC pretraining.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Appendix A.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Our claims in the abstract and introduction are justified through our experiments. 2. Did you describe the limitations of your work? Limitations are detailed in the Conclusions (Section 6) 3. Did you discuss any potential negative societal impacts of your work? We do not foresee any potential negative societal impacts of our work. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Code and instructions to reproduce experimental results can be found on our github page (linked in the abstract): https://github.com/google-deepmind/dmc_vision_benchmark. This page also includes a link to the data and instructions for downloading it. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix D. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We reported error bars for the standard error on all plots where applicable. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Appendix D.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We leverage DeepMind Control Suite environments [47] and Kubric dataset videos [18], both of which are cited in the main paper. 2. Did you mention the license of the assets? The licences of the code and data that we release are described in Appendix A. 3. Did you include any new assets either in the supplemental material or as a URL? The dataset and code are included in the paper via our github url: https://github.com/google-deepmind/dmc_vision_benchmark. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] DeepMind Control Suite environments [47] and Kubric dataset are released under the Apache 2.0 licence and so we did not need to seek consent to use their code as part of our data generation process.

5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] Our data does not contain any personally identifiable information or offensive content. It contains purely synthetic images.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]