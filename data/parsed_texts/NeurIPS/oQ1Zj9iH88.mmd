# Penalty-based Methods for Simple Bilevel Optimization under Holderian Error Bounds

 Pengyu Chen

School of Data Science

Fudan University

pychen22@m.fudan.edu.cn

&Xu Shi

School of Data Science

Fudan University

xshi22@m.fudan.edu.cn

&Rujun Jiang

School of Data Science

Fudan University

rjjiang@fudan.edu.cn

Equal contribution

Corresponding author

Jiulin Wang

School of Data Science

Fudan University

wangjiulin@fudan.edu.cn

###### Abstract

This paper investigates simple bilevel optimization problems where we minimize an upper-level objective over the optimal solution set of a convex lower-level objective. Existing methods for such problems either only guarantee asymptotic convergence, have slow sublinear rates, or require strong assumptions. To address these challenges, we propose a penalization framework that delineates the relationship between approximate solutions of the original problem and its reformulated counterparts. This framework accommodates varying assumptions regarding smoothness and convexity, enabling the application of specific methods with different complexity results. Specifically, when both upper- and lower-level objectives are composite convex functions, under an \(\alpha\)-Holderian error bound condition and certain mild assumptions, our algorithm attains an \((\epsilon,\epsilon^{\beta})\)-optimal solution of the original problem for any \(\beta>0\) within \(\mathcal{O}\left(\sqrt{1/\epsilon^{\max\{\alpha,\beta\}}}\right)\) iterations. The result can be improved further if the smooth part of the upper-level objective is strongly convex. We also establish complexity results when the upper- and lower-level objectives are general nonsmooth functions. Numerical experiments demonstrate the effectiveness of our algorithms.

## 1 Introduction

Bilevel optimization involves embedding one optimization problem within another, creating a hierarchical structure where the upper-level problem's feasible set is influenced by the lower-level problem. This framework frequently occurs in various real-world scenarios, such as meta-learning (Bertinetto et al., 2018; Rajeswaran et al., 2019), hyper-parameter optimization (Chen et al., 2024; Franceschi et al., 2018; Shaban et al., 2019), reinforcement learning (Mingyi et al., 2020) and adversarial learning (Bishop et al., 2020; Wang et al., 2021, 2022). In this paper, we concentrate on a subset of bilevel optimization known as simple bilevel optimization (SBO), which has garnered significant interest in the machine learning community due to its relevance in dictionary learning (Beck and Sabach, 2014; Jiang et al., 2023), lexicographic optimization (Kissel et al., 2020; Gong et al., 2021), lifelong learning (Malitsky, 2017; Jiang et al., 2023); see more details in Appendix A.

SBO aims to find an optimal solution that minimizes the upper-level objective over the solution set of the lower-level problem. In other words, we are interested in solving the following problem:

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}F(\mathbf{x})\quad\text{s.t.}\ \ \mathbf{x}\in \operatorname*{arg\,min}_{\mathbf{z}\in\mathbb{R}^{n}}G(\mathbf{z}).\] (P)

Here \(F,G:\mathbb{R}^{n}\to\mathbb{R}\bigcup\{\infty\}\) are proper, convex, and lower semi-continuous functions. We also assume that the optimal solution set of the lower-level problem, denoted as \(X_{\text{opt}}\), is nonempty. Moreover, since \(G\) is convex and lower semi-continuous, it holds that \(X_{\text{opt}}\) is closed and convex (Bertsekas et al., 2003, Proposition 1.2.2 and Page 49).

In this paper, we first reformulate problem (P) into the constrained form:

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}F(\mathbf{x})\quad\text{s.t.}\ \ G(\mathbf{x})-G^{*}\leq 0,\] (P \[{}_{\text{Val}}\] )

where \(G^{*}\) represents the optimal value of the unconstrained lower-level problem.

Based on this reformulation, we consider the following penalization of (P\({}_{\text{Val}}\)),

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\Phi_{\gamma}(\mathbf{x})=F(\mathbf{x})+ \gamma p(\mathbf{x}),\] (P \[{}_{\gamma}\] )

where \(p(\mathbf{x}):=G(\mathbf{x})-G^{*}\) is the so-called residual function and \(\gamma>0\) is the penalized parameter. Obviously, we have \(p(\mathbf{x})\geq 0\), and \(p(\mathbf{x})=0\) if and only if \(\mathbf{x}\in X_{\text{opt}}\).

Denote \(F^{*}\) and \(G^{*}\) as the optimal values of problem (P) and the lower-level problem \(\min_{\mathbf{x}\in\mathbb{R}^{n}}G(\mathbf{x})\), respectively. We aim to find an \((\epsilon_{F},\epsilon_{G})\)-optimal solution \(\tilde{\mathbf{x}}^{*}\) of problem (P), which satisfies

\[F(\tilde{\mathbf{x}}^{*})-F^{*}\leq\epsilon_{F},\quad G(\tilde{\mathbf{x}}^{*} )-G^{*}\leq\epsilon_{G}.\] (1)

Moreover, a point \(\tilde{\mathbf{x}}^{*}_{\gamma}\) is said to be an \(\epsilon\)-optimal solution of problem (P\({}_{\gamma}\)) if

\[\Phi_{\gamma}(\tilde{\mathbf{x}}^{*}_{\gamma})-\Phi^{*}_{\gamma}\leq\epsilon,\]

where \(\Phi^{*}_{\gamma}\) is the optimal value of problem (P\({}_{\gamma}\)).

### Related work

Various approaches have been developed to solve problem (P) (Cabot, 2005; Solodov, 2007; Sabach and Shtern, 2017; Dutta and Pandit, 2020; Gong et al., 2021). Among those, one category that is the most related to penalization formulation (P\({}_{\gamma}\)) is the regularization method, which integrates the upper- and lower-level objectives through Tikhonov regularization (Tikhonov and Arsenin, 1977)

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\eta(\mathbf{x}):=\sigma F(\mathbf{x})+G( \mathbf{x}),\] (P \[{}_{\text{Reg}}\] )

where \(\sigma\) is the so-called regularization parameter. When \(F\) is strongly convex and its domain is compact, Amini and Yousefian (2019) extended the IR-PG method from Solodov (2007), which achieved a asymptotic convergence rate for the upper-level problem and a convergence rate of \(\mathcal{O}\left(1/K^{0.5-b}\right)\) for the lower-level problem, where \(b\in(0,0.5)\). Malitsky (2017) studied a version of Tseng's accelerated gradient method and showed a convergence rate of \(\mathcal{O}\left(1/K\right)\) for the lower-level problem, while the convergence rate for the upper-level objective is not explicitly provided. Kaushik and Yousefian (2021) proposed an iteratively regularized gradient (a-IRG) method which obtains a complexity of \(\mathcal{O}\left(1/K^{0.5-b}\right)\) and \(\mathcal{O}\left(1/K^{b}\right)\) for the upper- and lower-level objective, respectively, where \(b\in(0,0.5)\). Inspired by this research, and under a quasi-Lipschitz assumption for \(F\), Merchav and Sabach (2023) introduced a bi-subgradient (Bi-SG) method. This method demonstrates convergence rates of \(\mathcal{O}(1/K^{b})\) and \(\mathcal{O}(1/K^{1-b})\) for the lower- and upper-level objectives, respectively, where \(b\in(0.5,1)\). In their framework, the convergence rate of the upper-level objective can be improved to be linear when \(F\) is strongly convex. Recently, under the weak-sharp minima assumption of the lower-level problem, Samadi et al. (2023) proposed a regularized accelerated proximal method (R-APM), showing a convergence rate of \(\mathcal{O}(1/K^{2})\) for both upper- and lower-level objectives. When the domain is compact and \(F,G\) are both smooth, Giang-Tran et al. (2023) proposed an iteratively regularized conditional gradient (IR-CG) method, which ensures convergence rates of \(\mathcal{O}(1/K^{p})\) and \(\mathcal{O}(1/K^{1-p})\) for upper- and lower-level objectives, respectively, where \(p\in(0,1)\).

Despite the abundance of existing methodologies yielding non-asymptotic convergence outcomes, their efficacy is frequently contingent upon additional assumptions. Denote \(L_{f_{1}}\) and \(L_{g_{1}}\) as the Lipschitz constants for the gradients of the smooth components in the upper- and lower-level objectives,respectively. Specifically, when \(F\) is strongly convex and \(G\) is smooth, Beck and Sabach (2014) presented the Minimal Norm Gradient (MNG) method and provided the asymptotic convergence to the optimal solution set and a convergence rate of \(\mathcal{O}\left(L_{g_{1}}^{2}/\epsilon^{2}\right)\) for the lower-level problem. When \(F\) is assumed to be smooth, Jiang et al. (2023) introduced a conditional gradient-based bilevel optimization (CG-BiO) method, which invokes at most \(\mathcal{O}\left(\max\{L_{f_{1}}/\epsilon_{F},L_{g_{1}}/\epsilon_{G}\}\right)\) of linear optimization oracles to achieve an \((\epsilon_{F},\epsilon_{G})\)-optimal solution. Shen et al. (2023) combined an online framework with the mirror descent algorithm and established a convergence rate of \(\mathcal{O}(1/\epsilon^{3})\) for both upper- and lower-level objectives, assuming a compact domain and boundedness of the functions and gradients at both levels. Furthermore, they showed that the convergence rate can be improved to \(\mathcal{O}(1/\epsilon^{2})\) under additional structural assumptions. For a concise overview of overall methodologies, including their assumptions and convergence outcomes, refer to Table 1 in Appendix B.

For general bilevel optimization problems, there have been recent results on convergent guarantees (Shen and Chen, 2023; Sow et al., 2022; Chen et al., 2023; Huang, 2023). Among those, the one that is the most related to ours is (Shen and Chen, 2023). It investigates the case when the upper-level objective is nonconvex and gives convergence results under additional assumptions (Shen and Chen, 2023, Theorem 3 and 4). However, as the general bilevel optimization problem is nonconvex, the algorithms in the literature often converge to weak stationary points, while our method for SBO converges to global optimal solution.

### Our approach

Our approach is straightforward. Firstly, we introduce a penalization framework delineating the connection between approximate solutions of problems (P) and (P\({}_{\gamma}\)). This framework enables the attainment of an \((\epsilon_{F},\epsilon_{G})\)-optimal solution by solving problem (P\({}_{\gamma}\)) approximately. Subsequently, our focus shifts solely to resolving the unconstrained problem (P\({}_{\gamma}\)). Depending on varying assumptions regarding smoothness and convexity, we can employ different methods such as the accelerated proximal gradient (APG) methods (Beck and Teboulle, 2009; Nesterov, 2013; Lin and Xiao, 2014) to solve problem (P\({}_{\gamma}\)). We summarize our main contributions as follows.

* We propose a framework that explicitly examines the relationship between an \(\epsilon\)-optimal solution of penalty formulation (P\({}_{\gamma}\)) and an \((\epsilon_{F},\epsilon_{G})\)-optimal solution of problem (P). We also provide a lower bound for the metric \(F(\mathbf{x})-F^{*}\).
* When \(F\) and \(G\) are both composite convex functions, we provide a penalty-based APG algorithm that attains an \((\epsilon,e^{\beta})\)-optimal solution of problem (P) within \(\mathcal{O}(\sqrt{1/\epsilon^{\max\{\alpha,\beta\}}})\) iterations. If the upper-level objective is strongly convex, the complexity can be improved to \(\mathcal{O}(\sqrt{1/\epsilon^{\max\{\alpha-1,\beta-1\}}}\log\frac{1}{\epsilon})\). We also apply our method for the scenario where both the upper- and lower-level objectives are generalized nonsmooth convex functions.
* We present adaptive versions of PB-APG and PB-APG-sc with warm-start, which dynamically adjust the penalty parameters, and solve the associated penalized problem with adaptive accuracy. The adaptive ones have similar complexity results as their primal counterparts but can achieve superior performance in some experiments.

Utilizing the penalization method to address the original SBO problem is a novel approach. While Tikhonov regularization may seem similar to our framework, its principles differ. Implementing Tikhonov regularization necessitates the "slow condition" (\(\lim_{k\rightarrow\infty}\sigma_{k}=0,\sum_{k=0}^{\infty}\sigma_{k}=+\infty\)), which requires iterative solutions for each iteration. In contrast, our method simply involves solving a single optimization problem (P\({}_{\gamma}\)) for a given \(\gamma\). Furthermore, we establish a relationship between the approximate solutions of the original bilevel problem and those of the reformulated single-level problem (P\({}_{\gamma}\)) for a specific \(\gamma\). This is the first theoretical result connecting the original bilevel problem to the penalization problem, accompanied by an optimal non-asymptotic complexity result.

## 2 The penalization framework

We begin by outlining specific assumptions for \(F\) and \(G\), as detailed below.

**Assumption 2.1**.: The set \(S:=\bigcup_{\mathbf{x}\in X_{\text{opt}}}\partial F(\mathbf{x})\) is bounded with a diameter \(l_{F}:=\max_{\xi\in S}\|\xi\|\).

Note that the type of subdifferential \(\partial F\) used here is the most general form for a convex function, as detailed in (Bertsekas et al., 2003, Section 4.2). When the upper-level objective \(F\) is non-convex,we replace the assumption with the condition that the upper-level objective is Lipschitz continuous (cf. Theorems 2.7 and 2.8).

**Assumption 2.2** (Holderian error bound).: The function \(p(\mathbf{x}):=G(\mathbf{x})-G^{*}\) satisfies the Holderian error bound with exponent \(\alpha\geq 1\) and \(\rho>0\). Namely,

\[\operatorname{dist}(\mathbf{x},X_{\text{opt}})^{\alpha}\leq\rho p(\mathbf{x} ),\forall\mathbf{x}\in\operatorname{dom}(G),\]

where \(\operatorname{dist}(\mathbf{x},X_{\text{opt}}):=\inf_{\mathbf{y}\in X_{ \text{opt}}}\|\mathbf{x}-\mathbf{y}\|\).

We remark that Holderian error bounds are satisfied by many practical problems and widely used in optimization literature (Pang, 1997; Bolte et al., 2017; Zhou and So, 2017; Roulet and d'Aspremont, 2020; Jiang and Li, 2022). There are two notable special cases: (i) when \(\alpha=1\), we often refer to \(X_{\text{opt}}\) as a set of weak sharp minima of \(G\)(Burke and Ferris, 1993; Studniarski and Ward, 1999; Burke and Deng, 2005; Samadi et al., 2023); (ii) when \(\alpha=2\), Assumption 2.2 is known as the quadratic growth condition (Drusvyatskiy and Lewis, 2018a). Additional examples of functions exhibiting Holderian error bound, along with their corresponding parameters, are presented in Appendix C.

We are now ready to establish the connection between approximate solutions of problems (P) and (P\({}_{\gamma}\)). The subsequent two lemmas build upon the work of Shen and Chen (2023) for (general) bilevel optimization. Compared with their work, we generalize the exponent \(\alpha\) from \(2\) to \(\alpha\geq 1\), providing a more general result. Furthermore, we also derive a lower bound for the penalized parameter for all \(\alpha\geq 1\) and present a theoretical framework for these scenarios.

**Lemma 2.3**.: _Suppose that Assumptions 2.1 and 2.2 hold with \(\alpha>1\). Then, for any \(\epsilon>0\), an optimal solution of problem (P) is an \(\epsilon\)-optimal solution of problem (P\({}_{\gamma}\)) when \(\gamma\geq\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-\alpha}\epsilon^{1-\alpha}\)._

Lemma 2.3 establishes the relationship between an optimal solution of problem (P) and an \(\epsilon\)-optimal solution of problem (P\({}_{\gamma}\)) when \(\alpha>1\). It also provides a lower bound for \(\gamma\), which plays a pivotal role in the complexity results. The proofs of this paper are deferred to Appendix E.

The lemma presented below yields a more favorable outcome when \(\alpha=1\), which is referred to as exact penalization. Notably, this specific result is not discussed in Shen and Chen (2023).

**Lemma 2.4**.: _Suppose that Assumptions 2.1 and 2.2 hold with \(\alpha=1\). Then an optimal solution of problem (P) is also an optimal solution of problem (P\({}_{\gamma}\)) if \(\gamma\geq\rho l_{F}\), and vice versa if \(\gamma>\rho l_{F}\). In this case, we say that there is an exact penalization between problems (P) and (P\({}_{\gamma}\))._

For simplicity, we define

\[\gamma^{*}=\left\{\begin{array}{rl}&\rho l_{F}^{\alpha}(\alpha-1)^{\alpha- \alpha}\epsilon^{1-\alpha}&\text{if }\alpha>1\\ &\rho l_{F}&\text{if }\alpha=1\end{array}\right..\] (2)

Based on Lemmas 2.3 and 2.4, we give an overall relationship of approximate solutions between problems (P\({}_{\gamma}\)) and (P).

**Theorem 2.5**.: _Suppose that Assumptions 2.1 and 2.2 hold. For any given \(\epsilon>0\) and \(\beta>0\), let_

\[\gamma=\gamma^{*}+\left\{\begin{array}{rl}&2l_{F}^{\beta}\epsilon^{1-\beta} &\text{if }\alpha>1,\\ &l_{F}^{\beta}\epsilon^{1-\beta}&\text{if }\alpha=1,\end{array}\right.\]

_with \(\gamma^{*}\) defined in (2). If \(\tilde{\mathbf{x}}_{\gamma}^{*}\) is an \(\epsilon\)-optimal solution of problem (P\({}_{\gamma}\)), then \(\tilde{\mathbf{x}}_{\gamma}^{*}\) is an \((\epsilon,l_{F}^{-\beta}\epsilon^{\beta})\)-optimal solution of problem (P)._

Particularly, we are also able to establish a lower bound for \(F(\tilde{\mathbf{z}}_{\gamma}^{*})-F^{*}\) under the same conditions outlined in Theorem 2.5.

**Theorem 2.6**.: _Suppose that the conditions in Theorem 2.5 hold. Then, \(\tilde{\mathbf{x}}_{\gamma}^{*}\) satisfies the following suboptimality lower bound,_

\[F(\tilde{\mathbf{x}}_{\gamma}^{*})-F^{*}\geq-l_{F}(\rho l_{F}^{-\beta} \epsilon^{\beta})^{\frac{1}{\alpha}}.\]

By setting \(\beta=\alpha\), we obtain \(F(\tilde{\mathbf{x}}_{\gamma}^{*})-F^{*}\geq-\rho^{\frac{1}{\alpha}}\epsilon\). which along with Theorem 2.5 gives

\[|F(\tilde{\mathbf{x}}_{\gamma}^{*})-F^{*}|\leq\max\{\epsilon,\rho^{\frac{1}{ \alpha}}\epsilon\}.\]

We emphasize that the lower bound established in Theorem 2.6 is an intrinsic property of problem (P) under Assumptions 2.1 and 2.2. This property is independent of the algorithms we present.

### The upper-level function is non-convex

Note that the upper-level objective \(F\) is required to be convex in the above context (cf. Theorem 2.5). This raises a question: while Theorem 2.5 establishes the relationship between approximate solutions of problems (P) and (P\({}_{\gamma}\)), the distinction between the global or local optimal solutions of problem (P) and (P\({}_{\gamma}\)) remains unclear when \(F\) is non-convex.

We first establish the relationship between global optimal solutions of problems (P) and (P\({}_{\gamma}\)) when \(F\) is non-convex, which is similar to Theorem 2.5.

**Theorem 2.7**.: _Suppose that Assumption 2.2 holds, \(G\) is convex, and \(F\) is \(l\)-Lipschitz continuous on \(\mathrm{dom}(F)\). For any given \(\epsilon>0\) and \(\beta>0\), let_

\[\gamma=\gamma^{*}+\left\{\begin{array}{rl}&2l^{\beta}\epsilon^{1-\beta}& \text{if }\alpha>1,\\ &l^{\beta}\epsilon^{1-\beta}&\text{if }\alpha=1,\end{array}\right.\] (3)

_where \(\gamma^{*}\) is given by (2). If \(\tilde{\mathbf{x}}_{\gamma}^{*}\) is an \(\epsilon\)-global optimal solution of problem (P\({}_{\gamma}\)), then \(\tilde{\mathbf{x}}_{\gamma}^{*}\) is an \((\epsilon,l^{-\beta}\epsilon^{\beta})\)-global optimal solution of problem (P)._

Theorem 2.7 provides the relationship between the global optimal solutions of problems (P\({}_{\gamma}\)) and (P). However, the relationship between local optimal solutions of these problems is more intricate than those of the global ones (Shen and Chen, 2023). Given \(r>0\) and \(\mathbf{z}\in\mathbb{R}^{n}\), define \(\mathcal{B}(\mathbf{z},r):=\{\mathbf{x}\in\mathbb{R}^{n}:\|\mathbf{x}-\mathbf{ z}\|\leq r\}\). We present the following theorem, which demonstrates that local optimal solutions of problem (P\({}_{\gamma}\)) can serve as approximate local optimal solutions of problem (P).

**Theorem 2.8**.: _Suppose that Assumption 2.2 holds and \(G\) is convex. Let \(\mathbf{x}_{\gamma}^{*}\) be a local optimal solution of problem (P\({}_{\gamma}\)) on \(\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\). Assume \(F\) is \(l\)-Lipschitz continuous on \(\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\). Then \(\mathbf{x}_{\gamma}^{*}\) is an approximate local optimal solution of problem (P) that satisfies \(F(\mathbf{x}_{\gamma}^{*})-F_{\mathcal{B}}^{*}\leq 0\) and \(G(\mathbf{x}_{\gamma}^{*})-G^{*}\leq\epsilon\) when \(\alpha>1\) and \(\gamma\geq(\frac{\rho l^{\alpha}}{\epsilon^{\alpha-1}})^{\frac{1}{\alpha}}\), where \(F_{\mathcal{B}}^{*}\) is the optimal value of problem (P) on \(\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\bigcap X_{\mathrm{opt}}\). Furthermore, \(\mathbf{x}_{\gamma}^{*}\) is a local optimal solution of problem (P) when \(\alpha=1\) and \(\gamma>\rho l\)._

Indeed, the relationship between approximate local optimal solutions of problems (P\({}_{\gamma}\)) and (P) is more intricate than the connection among global solutions presented in Theorem 2.5. These interactions will be the focus of our future work. The proofs of Theorems 2.7 and 2.8 are presented in Appendixes E.5 and E.6.

## 3 Main algorithms

In this section, we concentrate on addressing problem (P), making various assumptions, and offering distinct convergence outcomes.

### Both objectives are convex composite functions

In this scenario, we address problem (P) where \(F\) and \(G\) are both composite functions, i.e., \(F=f_{1}+f_{2}\) and \(G=g_{1}+g_{2}\).

**Assumption 3.1**.: \(F\) and \(G\) satisfy the following assumptions.

(1) The gradient of \(f_{1}(\mathbf{x})\), denoted as \(\nabla f_{1}\), is \(L_{f_{1}}\)-Lipschitz continuous on \(\mathrm{dom}(F)\);

(2) The gradient of \(g_{1}(\mathbf{x})\), denoted as \(\nabla g_{1}\), is \(L_{g_{1}}\)-Lipschitz continuous on \(\mathrm{dom}(G)\);

(3) \(f_{2}\) and \(g_{2}\) are proper, convex, lower semicontinuous, and possibly non-smooth.

We remark that Assumption 3.1(1)(3) is more general than many existing papers in the literature. Specifically, while previous works such as Beck and Sabach (2014), Amini and Yousefian (2019), Jiang et al. (2023), Giang-Tran et al. (2023) require the upper-level objective to be smooth or strongly convex, we simply assume that \(F\) is a composite function composed of a smooth convex function and a possibly non-smooth convex function. For the lower-level objective, previous works such as Beck and Sabach (2014), Amini and Yousefian (2019), Jiang et al. (2023), Giang-Tran et al. (2023) impose smoothness assumptions and, in some cases, convexity and compactness constraints on the domain; while our approach does not require these additional constraints, allowing for more flexibility and generality as presented in Assumption 3.1(2)(3).

We are now prepared to introduce two algorithms: the penalty-based accelerated proximal gradient (PB-APG) algorithm and its adaptive counterpart, the aPB-APG to solve problem (P\({}_{\gamma}\)) and, subsequently, to obtain an \((\epsilon_{F},\epsilon_{G})\)-optimal solution of problem (P).

To simplify notations, we omit the constant term \(-\gamma G^{*}\), and rewrite problem (P\({}_{\gamma}\)) as follows,

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\Phi_{\gamma}(\mathbf{x}):=\phi_{\gamma}( \mathbf{x})+\psi_{\gamma}(\mathbf{x}),\] (P \[{}_{\Phi}\] )

where \(\phi_{\gamma}(\mathbf{x})=f_{1}(\mathbf{x})+\gamma g_{1}(\mathbf{x})\) and \(\psi_{\gamma}(\mathbf{x})=f_{2}(\mathbf{x})+\gamma g_{2}(\mathbf{x})\) represent the smooth and nonsmooth parts, respectively. Then, it follows that the gradient of \(\phi_{\gamma}(\mathbf{x})\) is \(L_{\gamma}\)-Lipschitz continuous with \(L_{\gamma}=L_{f_{1}}+\gamma L_{g_{1}}\).

To implement the APG methods, we need another assumption concerning \(\psi_{\gamma}(\mathbf{x})\).

**Assumption 3.2**.: For any \(\gamma>0\), the function \(\psi_{\gamma}(\mathbf{x})\) is prox-friendly, i.e., the proximal mapping

\[\text{prox}_{t\psi_{\gamma}}(\mathbf{y}):=\underset{\mathbf{x}\in\mathbb{R}^ {n}}{\arg\min}\{\psi_{\gamma}(\mathbf{x})+\frac{1}{2t}\|\mathbf{x}-\mathbf{y} \|^{2}\},\]

is easy to compute for any \(t>0\).

The function \(\psi_{\gamma}(\mathbf{x})\) represents the sum of two non-smooth functions, and proximal mapping for such function sums is widely studied and used in the literature (Yu, 2013; Pustelnik and Condat, 2017; Adly et al., 2019; Boob et al., 2023; Latafat et al., 2023). This assumption is also a more general requirement compared to many existing algorithms (Sabach and Shtern, 2017; Giang-Tran et al., 2023). It is important to note that our assumption is more general than existing literature. In the simple bilevel literature, when employing proximal mappings, researchers often consider the scenario where only one level contains a nonsmooth term (see, e.g., (Jiang et al., 2023; Doron and Shtern, 2023; Samadi et al., 2023; Merchav and Sabach, 2023)). In this case, the proximal mapping of the sum \(f_{2}+\gamma g_{2}\) is then reduced to the proximal mapping of either \(f_{2}\) or \(g_{2}\), which is a more easily satisfied condition.

#### 3.1.1 Accelerated proximal gradient-based algorithm

We apply the APG algorithm (Beck and Teboulle, 2009; Lin and Xiao, 2014; Nesterov, 2013) to solve problem (P\({}_{\Phi}\)), as outlined in Algorithm 1. Moreover, if the Lipschitz constant \(L_{\gamma}\) is unknown or computationally infeasible, line search (Beck and Teboulle, 2009) can be adopted and will yield almost the same complexity bound. For brevity, we denote Algorithm 1 as \(\hat{\mathbf{x}}=\text{PB-APG}(\phi_{\gamma},\psi_{\gamma},L_{f_{1}},L_{g_{1}}, \mathbf{x}_{0},\epsilon)\), where \(\hat{\mathbf{x}}\) represents an \(\epsilon\)-optimal solution of (P\({}_{\Phi}\)).

```
1:Input:\(\gamma,L_{\gamma}=L_{f_{1}}+\gamma L_{g_{1}},\mathbf{x}_{-1}=\mathbf{x}_{0}\in \mathbb{R}^{n},R>0\), \(t_{-1}=t_{0}=1,k=0,\epsilon>0\) and \(\{t_{k}\}\).
2:for\(k\geq 0\)do
3:\(\mathbf{y}_{k}=\mathbf{x}_{k}+t_{k}\left(t_{k-1}^{-1}-1\right)(\mathbf{x}_{k} -\mathbf{x}_{k-1})\)
4:\(\mathbf{x}_{k+1}=\text{prox}_{L_{\gamma}^{-1}\psi_{\gamma}}(\mathbf{y}_{k}-L_ {\gamma}^{-1}\nabla\phi_{\gamma}(\mathbf{y}_{k}))\)
5:endfor ```

**Algorithm 1** Penalty-based APG (PB-APG)

In Algorithm 1, we stop the loop of Line. 3 - 4 if the number of iterations satisfies that:

\[\frac{2(L_{f}+\gamma L_{g})R^{2}}{(k+1)^{2}}\leq\epsilon,\]

where \(R\) is a constant that satisfies \(\|\mathbf{x}_{0}-\mathbf{x}^{*}\|\leq R\).

Combining Theorem 2.5 and (Tseng, 2008, Corollary 2), we establish the following complexity result for problem (P).

**Theorem 3.3**.: _Suppose that Assumptions 2.1, 2.2, 3.1 and 3.2 hold and the sequence \(\{t_{k}\}\) in Algorithm 1 satisfies \(\frac{1-t_{k+1}}{t_{k+1}^{2}}\leq\frac{1}{t_{k}^{2}}\). Let \(\gamma\) be given as in Theorem 2.5. Algorithm 1 generates an \((\epsilon,l_{F}^{-\beta}\epsilon^{\beta})\)-optimal solution of problem (P) after at most \(K\) iterations, where_

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+\sqrt{\frac{l_{F}^{\max( \alpha,\beta)}L_{g_{1}}}{\epsilon^{\max(\alpha,\beta)}}}\right).\]Note that Theorem 3.3 encompasses all possible relationships between the magnitudes of \(\epsilon_{F}\) and \(\epsilon_{G}\) in (1), as \(\alpha\geq 1\) and \(\beta>0\) are arbitrary. Specially, if \(\alpha=1\) and \(\beta\leq\alpha\), the number of iterations is \(K=\mathcal{O}\left(\sqrt{(L_{f_{1}}+l_{F}L_{g_{1}})/\epsilon}\right)\). This result matches the lower bound complexity for unconstrained smooth or convex composite optimization (Nemirovsky and Yudin, 1983; Woodworth and Srebro, 2016). Additionally, if \(g_{1}\equiv 0\), the number of iterations for obtaining an \((\epsilon,\epsilon^{\beta})\)-optimal solution of problems (P) is independent of \(\gamma\), which can be improved to \(K=\mathcal{O}(\sqrt{L_{f_{1}}/\epsilon})\).

**Remark 3.4**.: It is noteworthy that Theorem 1 in a previous paper Samadi et al. (2023) provides the first method that needs \(\mathcal{O}(\sqrt{(L_{g_{1}}+l_{F}L_{g_{1}})/\epsilon})\) iterations to achieve an \((\epsilon,\epsilon)\) solution if \(\alpha=1\) and \(F\) is smooth. Nevertheless, our methodology diverges in various respects. First, our approach is rooted in the penalization formulation of problem (P\({}_{\text{val}}\)), while the approach proposed by Samadi et al. (2023) is based on the Tikhonov regularization (Tikhonov and Arsenin, 1977). Second, we provide a theoretical framework that clearly delineates the relationship between approximate solutions of problems (P) and (P\({}_{\gamma}\)) for all cases of \(\alpha\geq 1\) and \(F\) is non-convex, as indicated in Lemmas 2.3, 2.4 and Theorems 2.5, 2.7, 2.8. Therefore, we can first shift our focus from (P) to (P\({}_{\gamma}\)) based on the penalization framework and then use various methods to solve (P\({}_{\gamma}\)), not limited to using the APG methods. Besides, the association between approximate solutions of problem (P) and (P\({}_{\gamma}\)) differs significantly based on whether \(\alpha>1\) or \(\alpha=1\). For the case of \(\alpha>1\), the lower bound comprehensively integrates the accuracy parameter \(\epsilon\), which results in a more sophisticated analysis of the convergence result, while Samadi et al. (2023) did not consider the situation when \(\alpha>1\). Third, our method applies to the case that \(F\) is composite, while Samadi et al. (2023) requires \(F\) to be smooth. Finally, we also propose an adaptive version of our algorithm (see Algorithm 2) that does not require an estimate of \(\gamma\).

#### 3.1.2 Adaptive version with warm-start mechanism

In practice, the penalty parameter \(\gamma\) might be difficult to determine. This motivates us to propose Algorithm 2, which adaptively updates \(\gamma\) and invokes PB-APG with dynamic \(\gamma\) and solution accuracies.

```
1:Input:\(\mathbf{x}_{0}\in\mathbb{R}^{n}\), \(\gamma_{0}=\gamma_{1}>0\), \(L_{f_{1}},L_{g_{1}}\), \(\nu>1,\eta>1\), \(\epsilon_{0}>0\).
2:for\(k\geq 0\)do
3:\(\phi_{k}(\mathbf{x})=f_{1}(\mathbf{x})+\gamma_{k}g_{1}(\mathbf{x})\)
4:\(\psi_{k}(\mathbf{x})=f_{2}(\mathbf{x})+\gamma_{k}g_{2}(\mathbf{x})\)
5: Invoke \(\mathbf{x}_{k}=\text{PB-APG}(\phi_{k},\psi_{k},L_{f_{1}},L_{g_{1}},\mathbf{x}_ {k-1},\epsilon_{k})\)
6:\(\epsilon_{k+1}=\epsilon_{k}/\eta\)
7:\(\gamma_{k+1}=\nu\gamma_{k}\)
8:endfor ```

**Algorithm 2** Adaptive PB-APG method (aPB-APG)

In Algorithm 2, we adaptively update the penalty parameter \(\gamma_{k}\), and invoke the PB-APG to generate an approximate solution for (P\({}_{\gamma}\)) with accuracy \(\epsilon=\epsilon_{k}\). Meanwhile, a warm-start mechanism is employed, meaning that the initial point for each subproblem is the output of the preceding subproblem. The convergence result of Algorithm 2 is as follows.

**Theorem 3.5**.: _Suppose that Assumptions 2.1, 2.2, 3.1, and 3.2 hold. Also assume that for every outcome of inner loop in Algorithm 2, \(\|\mathbf{x}_{k}-\mathbf{x}_{k}^{*}\|\leq R\). Let \(\epsilon_{0}>0\) be given._

* _When_ \(\alpha>1\)_, set_ \(\nu>\eta^{\alpha-1}\)_, and define_ \(N:=\lceil\log_{\eta^{1-\alpha}\nu}(\rho L_{F}^{\alpha}(\alpha-1)^{\alpha-1} \alpha^{-\alpha}\epsilon_{0}^{1-\alpha}/\gamma_{0})\rceil_{+}\) _and_ \(\gamma_{k}^{*}:=\rho L_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha} \epsilon_{0}^{1-\alpha}\eta^{k(\alpha-1)}\)_._
* _When_ \(\alpha=1\)_, set_ \(\nu>1\)_, and define_ \(N:=\lceil\log_{\nu}(\rho L_{F}/\gamma_{0})\rceil_{+}\) _and_ \(\gamma_{k}^{*}:=\rho L_{F}\)_._

_Then, for any \(k\geq N\), Algorithm 2 generates an \((\frac{\epsilon_{0}}{\eta^{k}},\frac{2\epsilon_{0}}{\eta^{k}(\gamma_{0}\nu^{k} -\gamma_{k}^{*})})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}\eta^{k}}{\epsilon_{0}}}+\sqrt{\frac{L_ {g_{1}}\gamma_{0}(\eta\nu)^{k}}{\epsilon_{0}}}\right).\]

Theorem 3.5 shows that for any given initial accuracy \(\epsilon_{0}>0\), Algorithm 2 can produce an approximate solution of problem (P) with the desired accuracy.

**Remark 3.6**.: From Theorem 3.5, one can obtain an \((\epsilon,\frac{\epsilon}{\gamma_{0}\nu^{k}-\gamma_{k}^{*}})\)-optimal solution of problem (P) within \(\mathcal{O}(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+\sqrt{\frac{L_{g_{1}}}{\epsilon ^{\alpha}}})\) iterations when \(\epsilon/\eta\leq\epsilon_{0}/\eta^{k}\leq\epsilon\), which is similar to the complexity results in Theorem 3.3.

#### 3.1.3 The upper-level objective is strongly convex

We investigate the convergence outcomes when the smooth part of the upper-level objective exhibits strong convexity.

**Assumption 3.7**.: \(f_{1}(\mathbf{x})\) is \(\mu\)-strongly convex on \(\operatorname{dom}(F)\) with \(\mu>0\).

Assumption 3.7 is another widely adopted setting in the existing SBO literature (Beck and Sabach, 2014; Sabach and Shtern, 2017; Amini and Yousefian, 2019; Merchav and Sabach, 2023). Here, we propose a variant of PB-APG that can provide better complexity results than existing methods. Our main integration is an APG-based algorithm, which has been studied in the existing literature (Nesterov, 2013; Lin and Xiao, 2014; Xu, 2022). In this paper, we adopt the algorithm proposed in Lin and Xiao (2014) and modify it with a constant step-size for simplicity as in Algorithm 3. Similar to Algorithm 1, we denote Algorithm 3 by \(\hat{\mathbf{x}}=\text{PB-APG-sc}(\phi_{\gamma},\psi_{\gamma},\mu,L_{f_{1}},L_ {g_{1}},\mathbf{y}_{0},\epsilon)\).

```
1:Input:\(\mu\), \(\gamma\), \(L_{\gamma}=L_{f_{1}}+\gamma L_{g_{1}}\), \(\mathbf{x}_{-1},\mathbf{y}_{0}\in\mathbb{R}^{n}\).
2:\(\tilde{\mathbf{y}}=\mathbf{y}_{0}-L_{\gamma}^{-1}\nabla\phi_{\gamma}( \mathbf{x}_{-1})\)
3:\(\tilde{\mathbf{x}}=\text{prox}_{L_{1}^{-1}\psi_{\gamma}}(\tilde{\mathbf{y}}- L_{\gamma}^{-1}\nabla\phi_{\gamma}(\tilde{\mathbf{y}}))\)
4:Initialization: Let \(\mathbf{x}_{-1}=\mathbf{x}_{0}=\tilde{\mathbf{x}}\), \(k=0\)
5:for\(k\geq 0\)do
6:\(\mathbf{y}_{k}=\mathbf{x}_{k}+\frac{\sqrt{L_{\gamma}-\sqrt{\mu}}}{\sqrt{L_{ \gamma}+\sqrt{\mu}}}(\mathbf{x}_{k}-\mathbf{x}_{k-1})\)
7:\(\mathbf{x}_{k+1}=\text{prox}_{L_{1}^{-1}\psi_{\gamma}}(\mathbf{y}_{k}-L_{ \gamma}^{-1}\nabla\phi_{\gamma}(\mathbf{y}_{k}))\)
8:endfor ```

**Algorithm 3** PB-APG method for Strong Convexity Case (PB-APG-sc)

The convergence analysis of Algorithm 3 is in the existing literature (Nesterov, 2013; Lin and Xiao, 2014). Combining (Lin and Xiao, 2014, Theorem 1) and Theorem 2.5, we have the following complexity result.

**Theorem 3.8**.: _Suppose that Assumptions 2.1, 2.2, 3.1, 3.2, and 3.7 hold. Algorithm 3 can produce an \((\epsilon,l_{F}^{-\beta}\epsilon^{\beta})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{1}{\epsilon}+\sqrt{ \frac{l_{F}^{\max\{\alpha,\beta\}}L_{g_{1}}}{\epsilon^{\max\{\alpha-1,\beta-1 \}}}}\log\frac{1}{\epsilon}\right).\]

Theorem 3.8 improves the complexity results of Theorem 3.3 significantly. Specifically, when \(0<\beta\leq\alpha=1\), the convergence rate can be improved to be linear, i.e., \(K=\mathcal{O}(\sqrt{L_{f_{1}}/\mu}\log\frac{1}{\epsilon})\).

Additionally, we present an adaptive variant of PB-APG-sc, termed aPB-APG-sc, which adaptively executes \(\mathbf{x}_{k}=\text{PB-APG-sc}(\phi_{k},\psi_{k},\mu,L_{f_{1}},L_{g_{1}}, \mathbf{x}_{k-1},\epsilon_{k})\) and enjoys the similar complexity results of Algorithm 3, as delineated in Algorithm 4 within Appendix D.1.

### Both objectives are non-smooth

In this section, we focus on the scenario where both the upper- and lower-level objectives are non-smooth, namely, \(f_{1}=g_{1}\equiv 0\). Additionally, we assume that there is a point \(x\in C\) in the lower level problem, where \(C\) is either \(\mathbb{R}^{n}\) (the unconstrained case) or a nonempty closed and convex set satisfying \(C\subseteq\operatorname{int}\left(\operatorname{dom}(F)\bigcap\operatorname{ dom}(G)\right)\).

It is worth noting that in the case where both \(F\) and \(G\) are non-smooth, the convergence result may not be as favorable as those in the previous scenarios. This is primarily due to the limited availability of information and unfavorable properties concerning \(F\) and \(G\). In this case, we employ a subgradient method to solve problem (P\({}_{\gamma}\)), which has been extensively studied in the existing literature (Shor, 2012; Bubeck et al., 2015; Beck, 2017; Nesterov, 2018). Specifically, we update

\[\mathbf{x}_{k+1}=\operatorname{Proj}_{C}(\mathbf{x}_{k}-\eta_{k}\xi_{k}),\] (4)

where \(\xi_{k}\in\partial\Phi_{\gamma}(\mathbf{x}_{k})\) is an subgradient of \(\Phi_{\gamma}(\mathbf{x}_{k})\), and \(\operatorname{Proj}_{C}(\mathbf{x})\) is the projection of \(\mathbf{x}\) onto \(C\).

Let \(\mathbf{x}_{\gamma}^{*}\) be an optimal solution of problem (P\({}_{\gamma}\)) and suppose that there exists a constant \(R\) such that \(\|\mathbf{x}_{0}-\mathbf{x}_{\gamma}^{*}\|\leq R\). Motivated by Theorem 8.28 in Beck (2017), we establish the subsequent complexity result for problem (P).

**Theorem 3.9**.: _Suppose that Assumption 3.1(3) holds, \(f_{2}\) and \(g_{2}\) are \(l_{f_{2}}\)- and \(l_{g_{2}}\)-Lipschitz continuous, respectively. Set step-size \(\eta_{k}=\frac{R}{l_{\gamma}\sqrt{k+1}}\) in (4). Then, the subgradient method produces an \((\epsilon,l_{f_{2}}^{-\beta}\epsilon^{\beta})\)-optimalsolution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+\frac{l_{f_{2}}^{\max\{2 \alpha,2\beta\}}l_{g_{2}}^{2}}{\epsilon^{\max\{2\alpha,2\beta\}}}\right).\]

For non-smooth SBO problems, our method has lower complexity compared to existing approaches. Specifically, under a bounded domain assumption, Helou and Simoes (2017) simply proposed an \(\epsilon\)-subgradient method with an asymptotic rate towards the optimal solution set. The a-IRG method in Kaushik and Yousefian (2021) achieved convergence rates of \(\mathcal{O}(1/\epsilon^{\frac{1}{0.5-b}})\) and \(\mathcal{O}(1/\epsilon^{\frac{1}{b}})\) for the upper- and lower-level objectives, respectively, where \(b\in(0,0.5)\). Setting \(b=0.25\) yields the convergence rates of \(\mathcal{O}(1/\epsilon^{4})\) for both upper- and lower-level objectives, which indicates that our complexity is more efficient than theirs when \(\alpha<2\) and \(\beta\leq\alpha\). Furthermore, the online framework proposed in Shen et al. (2023) performed a complexity of \(\mathcal{O}(1/\epsilon^{4})\) for both upper- and lower-level objectives. Similarly, our approach prevails over theirs when \(\alpha<1.5\) and \(\beta\leq\alpha\).

Strongly convex upper-level objective.Based on Theorem 8.31 in Beck (2017), we next explore the improved complexity result for problem (P) when \(f_{2}\) is additionally strongly convex.

**Theorem 3.10**.: _Suppose that Assumption 3.1(3) holds, \(C\subseteq\operatorname{int}\left(\operatorname{dom}(F)\bigcap\operatorname{ dom}(G)\right)\). \(f_{2}\) is \(l_{f_{2}}\)-Lipschitz continuous and \(\mu_{f_{2}}\)-strongly convex3, and \(g_{2}\) is \(l_{g_{2}}\)-Lipschitz continuous. Choose step-size \(\eta_{k}=\frac{2}{\mu_{f_{2}}(k+1)}\) in (4). Then, the subgradient method produces an \((\epsilon,l_{f_{2}}^{-\beta}\epsilon^{\beta})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

Footnote 3: In this case, we must have \(C\) bounded, as \(f_{2}\) is both strongly convex and Lipschitz continuous.

\[K=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{ \max\{2\alpha,2\beta\}}l_{g_{2}}^{2}}{\mu_{f_{2}}\epsilon^{\max\{2\alpha-1,2 \beta-1\}}}\right).\]

To our knowledge, within the context of Theorem 3.10, current findings fail to exploit strong convexity to enhance results. However, our approach capitalizes on distinct structural characteristics that yield superior complexity outcomes relative to Theorem 3.9 in cases where \(\alpha<2\) and \(\beta\leq\alpha\).

## 4 Numerical experiments

We apply our Algorithms 1, 2, 3 and 4 to two simple bilevel optimization problems from the motivating examples in Appendix A. The performances of our methods are compared with several existing methods: MNG (Beck and Sabach, 2014), BiG-SAM (Sabach and Shtern, 2017), DBGD (Gong et al., 2021), a-IRG (Kaushik and Yousefian, 2021), CG-BiO (Jiang et al., 2023), Bi-SG (Merchav and Sabach, 2023) and R-APM (Samadi et al., 2023). For practical efficiency, we use the Greedy FISTA algorithm proposed in Liang et al. (2022) as the APG method in our approach. Detailed settings and additional experimental results are presented in Appendix F.

### Logistic regression problem (LRP)

The LRP reads

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\frac{1}{2}\|\mathbf{x}\|^{2} \quad\mathrm{s.t.}\ \ \mathbf{x}\in\operatorname*{arg\,min}_{\mathbf{z}\in\mathbb{R}^{n}}\frac{1}{m} \sum_{i=1}^{m}\log(1+\exp(-\mathbf{a}_{i}^{\mathrm{T}}\mathbf{z}b_{i}))+I_{C} (\mathbf{z}),\] (5)

where \(I_{C}(\mathbf{x})\) is the indicator function of the set \(C=\{\mathbf{x}\in\mathbb{R}^{n}:\|\mathbf{x}\|_{1}\leq\theta\}\) with \(\theta=10\). Our goal is to find a solution to the lower-level problem with the smallest Euclidean norm. The upper-level objective only consists of the smooth part, which is \(1\)-strongly convex and \(1\)-smooth; meanwhile, the lower-level objective is a composite function, where the smooth part is \(\frac{1}{4m}\lambda_{\max}(A^{\mathrm{T}}A)\)-smooth, and the nonsmooth part is prox-friendly (Duchi et al., 2008).

In this experiment, we compare our methods with MNG, BiG-SAM, DBGD, a-IRG, CG-BiO, and Bi-SG. We plot the values of residuals of the lower-level objective \(G(\mathbf{x}_{k})-G^{*}\) and the upper-level objective over time in Figure 1.

As shown in Figure 1, the PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc algorithms exhibit significantly faster convergence performance than the other methods for both lower- and upper-level objectives, although R-APM attains similar outcomes, our PB-APG and PB-APG-sc ensure a more rapid decline than it, as shown in the first subfigure of Figure 1. This is because our methods achieve lower optimal gaps and desired function values of the lower- and upper-level objectives with less execution time. This observation confirms the improved complexity results stated in the theorems above. Although the high exactness of our methods for the lower-level problem leads to larger upper-level objectives, Table 3 in Appendix F.1 shows that our methods are much closer to the optimal value. This is reasonable because the other methods exhibit lower accuracy at the lower-level problem, resulting in larger feasible sets compared to the lower-level optimal solution set \(X_{\text{opt}}\). In addition, Figure 1 demonstrates that aPB-APG and aPB-APG-sc outperform PB-APG and PB-APG-sc in terms of convergence rate. This improvement can be attributed to the adaptiveness incorporated in Algorithms 2 and 4.

### Least squares regression problem (LSRP)

The LSRP has the following form:

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\frac{\tau}{2}\|\mathbf{x}\|^{2}+\|\mathbf{ x}\|_{1}\quad\mathrm{s.t.}\ \mathbf{x}\in\operatorname*{arg\,min}_{\mathbf{x}\in\mathbb{R}^{n}}\frac{1}{2 m}\left\|A\mathbf{z}-b\right\|^{2},\] (6)

where \(\tau=0.02\) regulates the trade-off between \(\ell_{1}\) and \(\ell_{2}\) norms. We aim to find a sparse solution for the lower-level problem. The upper-level objective is formulated as a composite function, which consists of a \(\tau\)-strongly convex and \(\tau\)-smooth component, along with a proximal-friendly non-smooth component (Beck, 2017). The lower-level objective is a smooth function with a smoothness parameter of \(\frac{1}{m}\lambda_{\max}(A^{\mathrm{T}}A)\).

In this experiment, we compare the performances of our methods with a-IRG, BiG-SAM, and Bi-SG. We plot the values of residuals of lower-level objective \(G(\mathbf{x}_{k})-G^{*}\) and the upper-level objective over time in Figure 2.

Figure 2 shows that the proposed PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc converge faster than the compared methods for both the lower- and upper-level objectives, as well. For the upper-level objective, our methods achieve larger function values than other methods, except BiG-SAM (\(\delta=0.01\)). This is because our methods attain higher accuracy for the lower-level objective than other methods. We have similar observations in Section 4.1. Furthermore, Figure 1 also demonstrates that the adaptive mechanism produces staircase-shaped curves for aPB-APG and aPB-APG-sc, which might prevent undesirable fluctuations in PB-APG and PB-APG-sc.

## 5 Conclusion

This paper proposes a penalization framework that effectively addresses the challenges inherent in simple bilevel optimization problems. By delineating the relationship between approximate solutions of the original problem and its penalized reformulation, we enable the application of specific methods under varying assumptions for the original problem. Under the Holderian error bound condition, our methods achieve superior complexity results compared to the existing methods. The performance is further improved when the smooth component of the upper-level objective is strongly convex. Additionally, we extend our framework to scenarios involving general nonsmooth objectives. Numerical experiments also validate the effectiveness of our algorithms.

## Acknowledgements

This work is partly supported by the National Key R&D Program of China under grant 2023YFA1009300, National Natural Science Foundation of China under grants 12171100 and the Major Program of NFSC (72394360,72394364).

## References

* Adly et al. (2019) Samir Adly, Loic Bourdin, and Fabien Caubet. On a decomposition formula for the proximal operator of the sum of two convex functions. _Journal of Convex Analysis_, 26(2):699-718, 2019.
* Adly et al. (2019)Mostafa Amini and Farzad Yousefian. An iterative regularized incremental projected subgradient method for a class of bilevel optimization problems. In _2019 American Control Conference (ACC)_, pages 4069-4074. IEEE, 2019.
* Beck (2017) Amir Beck. _First-order methods in optimization_. SIAM, 2017.
* Beck and Sabach (2014) Amir Beck and Shoham Sabach. A first order method for finding minimal norm-like solutions of convex optimization problems. _Mathematical Programming_, 147(1-2):25-46, 2014.
* Beck and Teboulle (2009) Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. _SIAM Journal on Imaging Sciences_, 2(1):183-202, 2009.
* Bertinetto et al. (2018) Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. _arXiv preprint arXiv:1805.08136_, 2018.
* Bertsekas et al. (2003) Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. _Convex analysis and optimization_, volume 1. Athena Scientific, 2003.
* Bishop et al. (2020) Nicholas Bishop, Long Tran-Thanh, and Enrico Gerding. Optimal learning from verified training data. _Advances in Neural Information Processing Systems_, 33:9520-9529, 2020.
* Bolte et al. (2017) Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W Suter. From error bounds to the complexity of first-order descent methods for convex functions. _Mathematical Programming_, 165:471-507, 2017.
* Boob et al. (2023) Digvijay Boob, Qi Deng, and Guanghui Lan. Stochastic first-order methods for convex and nonconvex functional constrained optimization. _Mathematical Programming_, 197(1):215-279, 2023.
* Bubeck et al. (2015) Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Burke and Deng (2005) James V. Burke and Sien Deng. Weak sharp minima revisited, part ii: application to linear regularity and error bounds. _Mathematical Programming_, 104(2-3):235-261, 2005.
* Burke and Ferris (1993) James V Burke and Michael C Ferris. Weak sharp minima in mathematical programming. _SIAM Journal on Control and Optimization_, 31(5):1340-1359, 1993.
* Cabot (2005) Alexandre Cabot. Proximal point algorithm controlled by a slowly vanishing term: applications to hierarchical minimization. _SIAM Journal on Optimization_, 15(2):555-572, 2005.
* Chen et al. (2024) H. Chen, H. Xu, R. Jiang, et al. Lower-level duality based reformulation and majorization minimization algorithm for hyperparameter optimization. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, pages 784-792. PMLR, 2024.
* Chen et al. (2023) L. Chen, J. Xu, and J. Zhang. Bilevel optimization without lower-level strong convexity from the hyper-objective perspective. _arXiv preprint arXiv:2301.00712_, 2023.
* Davis and Drusvyatskiy (2019) Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. _SIAM Journal on Optimization_, 29(1):207-239, 2019.
* Dempe et al. (2021) Stephan Dempe, Nguyen Dinh, Joydeep Dutta, and Tanushree Pandit. Simple bilevel programming and extensions. _Mathematical Programming_, 188:227-253, 2021.
* Doron and Shtern (2023) Lior Doron and Shimrit Shtern. Methodology and first-order algorithms for solving nonsmooth and non-strongly convex bilevel optimization problems. _Mathematical Programming_, 201:521-558, 2023.
* Drusvyatskiy and Lewis (2018) Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. _Mathematics of operations research_, 43(3):919-948, 2018a.
* Drusvyatskiy and Lewis (2018) Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. _Mathematics of Operations Research_, 43(3):919-948, 2018b.
* Duchi et al. (2008) John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In _Proceedings of the 25th international conference on Machine learning_, pages 272-279, 2008.
* Dutta and Pandit (2020) Joydeep Dutta and Tanushree Pandit. Algorithms for simple bilevel programming. _Bilevel Optimization: Advances and Next Challenges_, pages 253-291, 2020.
* Dutta et al. (2018)* Franceschi et al. [2018] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International conference on machine learning_, pages 1568-1577. PMLR, 2018.
* Friedlander and Tseng [2008] Michael P Friedlander and Paul Tseng. Exact regularization of convex programs. _SIAM Journal on Optimization_, 18(4):1326-1350, 2008.
* Giang-Tran et al. [2023] Khanh-Hung Giang-Tran, Nam Ho-Nguyen, and Dabeen Lee. Projection-free methods for solving convex bilevel optimization problems. _arXiv preprint arXiv:2311.09738_, 2023.
* Gong et al. [2021] Chengyue Gong, Xingchao Liu, and Qiang Liu. Bi-objective trade-off with dynamic barrier gradient descent. In _International Conference on Neural Information Processing Systems_, pages 29630-29642, 2021.
* Helou and Simoes [2017] Elias S Helou and Lucas EA Simoes. \(\epsilon\)-subgradient algorithms for bilevel convex optimization. _Inverse Problems_, 33(5):055020, 2017.
* Huang [2023] F. Huang. On momentum-based gradient methods for bilevel optimization with nonconvex lower-level. _arXiv preprint arXiv:2303.03944_, 2023.
* Jiang et al. [2023] Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. A conditional gradient-based method for simple bilevel optimization with convex lower-level problem. In _International Conference on Artificial Intelligence and Statistics_, pages 10305-10323. PMLR, 2023.
* Jiang and Li [2022] Rujun Jiang and Xudong Li. Holderian error bounds and kurdyka-lojasiewicz inequality for the trust region subproblem. _Mathematics of Operations Research_, 47(4):3025-3050, 2022.
* Kaushik and Yousefian [2021] Harshal D. Kaushik and Farzad Yousefian. A method with convergence rates for optimization problems with variational inequality constraints. _SIAM Journal on Optimization_, 31(3):2171-2198, 2021.
* Kissel et al. [2020] Matthias Kissel, Martin Gottwald, and Klaus Diepold. Neural network training with safe regularization in the null space of batch activations. In _Artificial Neural Networks and Machine Learning-ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15-18, 2020, Proceedings, Part II 29_, pages 217-228. Springer, 2020.
* Latafat et al. [2023] Puya Latafat, Andreas Themelis, Silvia Villa, and Panagiotis Patrinos. Adabim: An adaptive proximal gradient method for structured convex bilevel optimization. _arXiv preprint arXiv:2305.03559_, 2023.
* Liang et al. [2022] Jingwei Liang, Tao Luo, and Carola-Bibiane Schonlieb. Improving fast iterative shrinkage-thresholding algorithm: Faster, smarter, and greedier. _SIAM Journal on Scientific Computing_, 44(3):A1069-A1091, 2022.
* Lin and Xiao [2014] Qihang Lin and Lin Xiao. An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization. In _International Conference on Machine Learning_, pages 73-81. PMLR, 2014.
* Luo et al. [1996] Zhi-Quan Luo, Jong-Shi Pang, Daniel Ralph, and Shi-Quan Wu. Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. _Mathematical Programming_, 75(1):19-76, 1996.
* Malitsky [2017] Yura Malitsky. Chambolle-Pock and Tsengs methods: relationship and extension to the bilevel optimization. _arXiv preprint arXiv:1706.02602_, 2017.
* Merchav and Sabach [2023] Roey Merchav and Shoham Sabach. Convex bi-level optimization problems with nonsmooth outer objective function. _SIAM Journal on Optimization_, 33(4):3114-3142, 2023.
* Mingyi et al. [2020] Hong Mingyi, Wai Hoi-To, Wang Zhaoran, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. _arXiv preprint arXiv:2007.05170_, 2020.
* Nemirovsky and Yudin [1983] Arkadij Semenovic Nemirovsky and David Borisovich Yudin. _Problem complexity and method efficiency in optimization_. Wiley, 1983.
* Nesterov [2013] Yurii Nesterov. Gradient methods for minimizing composite functions. _Mathematical programming_, 140(1):125-161, 2013.
* Nesterov [2018] Yurii Nesterov. _Lectures on convex optimization_, volume 137. Springer, 2018.
* Pang [1997] Jong Shi Pang. Error bounds in mathematical programming. _Mathematical Programming_, 79(1-3):299-332, 1997.
* Pustelnik and Condat [2017] Nelly Pustelnik and Laurent Condat. Proximity operator of a sum of functions; application to depth map estimation. _IEEE Signal Processing Letters_, 24(12):1827-1831, 2017.
* Pustelnik et al. [2018]Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. _Advances in neural information processing systems_, 32, 2019.
* Roulet and d'Aspremont [2020] Vincent Roulet and Alexandre d'Aspremont. Sharpness, restart and acceleration. _SIAM Journal on Optimization_, 30(1):262-289, 2020.
* Sabach and Shtern [2017] Shoham Sabach and Shimrit Shtern. A first order method for solving convex bilevel optimization problems. _SIAM Journal on Optimization_, 27(2):640-660, 2017.
* Samadi et al. [2023] Sepideh Samadi, Daniel Burbano, and Farzad Yousefian. Achieving optimal complexity guarantees for a class of bilevel convex optimization problems. _arXiv preprint arXiv:2310.12247_, 2023.
* Shaban et al. [2019] Amireza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1723-1732. PMLR, 2019.
* Shen and Chen [2023] Han Shen and Tianyi Chen. On penalty-based bilevel gradient descent method. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 30992-31015. PMLR, 23-29 Jul 2023.
* Shen et al. [2023] Lingqing Shen, Nam Ho-Nguyen, and Fatma Kilinc-Karzan. An online convex optimization-based framework for convex bilevel optimization. _Mathematical Programming_, 198(2):1519-1582, 2023.
* Shor [2012] Naum Zuselevich Shor. _Minimization methods for non-differentiable functions_, volume 3. Springer Science & Business Media, 2012.
* Solodov [2007] Mikhail Solodov. An explicit descent method for bilevel convex optimization. _Journal of Convex Analysis_, 14(2):227-237, 2007.
* Sow et al. [2022] D. Sow, K. Ji, Z. Guan, and et al. A primal-dual approach to bilevel optimization with multiple inner minima. _arXiv preprint arXiv:2203.01123_, 2022.
* Studniarski and Ward [1999] Marcin Studniarski and Doug E Ward. Weak sharp minima: characterizations and sufficient conditions. _SIAM Journal on Control and Optimization_, 38(1):219-236, 1999.
* Tikhonov and Arsenin [1977] Andre Nikolaevich Tikhonov and V. I. A. K. Arsenin. _Solutions of ill-posed problems_. Wiley, 1977.
* Tseng [2008] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. _unpublished manuscript_, 2008.
* Wang et al. [2021] Jiali Wang, He Chen, Rujun Jiang, Xudong Li, and Zihao Li. Fast algorithms for stackelberg prediction game with least squares loss. In _International Conference on Machine Learning_, pages 10708-10716. PMLR, 2021.
* Wang et al. [2022] Jiali Wang, Wen Huang, Rujun Jiang, Xudong Li, and Alex L Wang. Solving stackelberg prediction game with least squares loss via spherically constrained least squares reformulation. In _International Conference on Machine Learning_, pages 22665-22679. PMLR, 2022.
* Woodworth and Srebro [2016] Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. _Advances in neural information processing systems_, 29, 2016.
* Xu [2022] Yangyang Xu. First-order methods for problems with o (1) functional constraints can have almost the same convergence rate as for unconstrained problems. _SIAM Journal on Optimization_, 32(3):1759-1790, 2022.
* Yu [2013] Yao-Liang Yu. On decomposing the proximal map. _Advances in neural information processing systems_, 26, 2013.
* Zeng et al. [2019] Jinshan Zeng, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao. Global convergence of block coordinate descent in deep learning. In _International conference on machine learning_, pages 7313-7323. PMLR, 2019.
* Zhou and So [2017] Zirui Zhou and Anthony Man-Cho So. A unified approach to error bounds for structured convex optimization problems. _Mathematical Programming_, 165:689-728, 2017.

Motivating examples

Many machine learning applications involve a primary objective \(G\), which usually represents the training loss, and a secondary objective \(F\), which can be a regularization term or an auxiliary loss. A common approach for such problems is to optimize \(G\) fully and then use \(F\) to select the optimal solutions from the ones obtained for \(G\). This is called lexicographic optimization (Kissel et al., 2020; Gong et al., 2021). Two classes of lexicographic optimization problems are the regularized problem, also known as the ill-posed optimization problem (Amini and Yousefian, 2019; Jiang et al., 2023), and the over-parameterized regression (Jiang et al., 2023), where the upper-level objectives are the regularization terms or loss functions, and the lower-level objectives are the loss functions and the constraint terms. We present some examples of these classes of problems as follows.

**Example A.1** (Linear Inverse Problems).: Linear inverse problems aim to reconstruct a vector \(\mathbf{x}\in\mathbb{R}^{n}\) from measurements \(b\in\mathbb{R}^{m}\) that satisfy \(b=A\mathbf{x}+\rho\varepsilon\), where \(A:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\) is a linear mapping, \(\varepsilon\in\mathbb{R}^{m}\) is unknown noise, and \(\rho>0\) is its magnitude. Various optimization techniques can address these problems. We focus on the bileved formulation, widely adopted in the literature (Beck and Sabach, 2014; Sabach and Shtern, 2017; Dempe et al., 2021; Latafat et al., 2023; Merchav and Sabach, 2023).

The lower-level objective in the bilevel formulation is given by

\[G(\mathbf{x})=\frac{1}{2m}\left\|A\mathbf{x}-b\right\|^{2}+I_{C}(\mathbf{x}),\] (7)

where \(I_{C}(\mathbf{x})\) is the indicator function of a set \(C\) that satifies \(I_{C}(\mathbf{x})=0\) if \(\mathbf{x}\in C\), and \(I_{C}(\mathbf{x})=+\infty\) if \(\mathbf{x}\notin C\). The set \(C\) is a closed, convex set that can be chosen as \(C=\mathbb{R}^{n}\), \(C=\{\mathbf{x}\in\mathbb{R}^{n}:\mathbf{x}\geq 0\}\), or \(C=\{\mathbf{x}\in\mathbb{R}^{n}:\|\mathbf{x}\|_{1}\leq\theta\}\) for some \(\theta>0\).

This problem may have multiple minimizer solutions. Hence, a reasonable option is to consider the minimal norm solution problem, i.e., find the optimal solution with the smallest Euclidean norm (Beck and Sabach, 2014; Sabach and Shtern, 2017; Latafat et al., 2023):

\[F(\mathbf{x})=\frac{1}{2}\left\|\mathbf{x}\right\|^{2}.\]

We need to solve the simple bilevel optimization problem:

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\frac{1}{2}\|\mathbf{x}\|^{2}\quad\mathrm{ s.t.}\ \ \mathbf{x}\in\operatorname*{arg\,min}_{\mathbf{z}\in\mathbb{R}^{n}}\frac{1}{2m} \left\|A\mathbf{z}-b\right\|^{2}+I_{C}(\mathbf{z}).\]

**Example A.2** (Sparse Solution of Linear Inverse Problems).: Consider the same setting as in Example A.1, but with the additional goal of finding a sparse solution among all the minimizers of the linear inverse problem (7). This can simplify the model and improve computational efficiency. To achieve sparsity, we can use any function that encourages it. One such function is the well-known elastic net regularization (Friedlander and Tseng, 2008; Amini and Yousefian, 2019; Merchav and Sabach, 2023), which is defined as

\[F(\mathbf{x})=\left\|\mathbf{x}\right\|_{1}+\frac{\tau}{2}\left\|\mathbf{x} \right\|^{2},\]

where \(\tau>0\) regulates the trade-off between \(\ell_{1}\) and \(\ell_{2}\) norms.

This example corresponds to our second experiment in Section 4.2.

**Example A.3** (Logistic Regression Problem).: The logistic regression problem aims to map the feature vectors \(\mathbf{a}_{i}\) to the target labels \(b_{i}\). A standard machine learning technique for this problem is to minimize the logistic loss function over the given dataset (Amini and Yousefian, 2019; Gong et al., 2021; Jiang et al., 2023; Latafat et al., 2023; Merchav and Sabach, 2023). We assume that the dataset consists of a feature matrix \(A\in\mathbb{R}^{m\times n}\) and a label vector \(b\in\mathbb{R}^{m}\), with \(b_{i}\in\{-1,1\}\) for each \(i\). The logistic loss function is defined as

\[g_{1}(\mathbf{x})=\frac{1}{m}\sum_{i=1}^{m}\log(1+\exp(-\mathbf{a}_{i}^{ \mathrm{T}}\mathbf{x}b_{i})).\] (8)

Over-fitting is a common issue when the number of features is large compared to the number of instances \(m\). A possible approach is to regularize the logistic objective function with a specific function or a constraint (Jiang et al., 2023; Merchav and Sabach, 2023). For instance, we can use \(g_{2}(\mathbf{x})=I_{C}(\mathbf{x})\), where \(I_{C}(\mathbf{x})\) is the indicator of the set \(C=\{\mathbf{x}\in\mathbb{R}^{n}:\|\mathbf{x}\|_{1}\leq\theta\}\), as in Example A.1.

This problem may also have multiple optimal solutions. Hence, a natural extension is to consider the minimal norm solution problem (Gong et al., 2021; Jiang et al., 2023; Latafat et al., 2023), as in Example A.1. This requires solving the following problem:

\[\min_{\mathbf{x}\in\mathbb{R}^{n}}\frac{1}{2}\|\mathbf{x}\|^{2}\quad\mathrm{ s.t.}\ \ \mathbf{x}\in\operatorname*{arg\,min}_{\mathbf{z}\in\mathbb{R}^{n}}\frac{1}{m} \sum_{i=1}^{m}\log(1+\exp(-\mathbf{a}_{i}^{\mathrm{T}}\mathbf{z}b_{i}))+I_{C} (\mathbf{z}).\]

[MISSING_PAGE_FAIL:15]

## Appendix D Supplementary results

### Adaptive version of PB-APG method with strong convexity assumption

```
1:Input:\(\mathbf{x}_{-1}=\mathbf{x}_{0}\in\mathbb{R}^{n}\), \(\gamma_{0}=\gamma_{1}>0\), \(L_{f_{1}},L_{g_{1}},\nu>1,\eta>1\), \(\epsilon_{0}>0\).
2:for\(k\geq 0\)do
3:\(\phi_{k}(\mathbf{x})=f_{1}(\mathbf{x})+\gamma_{k}g_{1}(\mathbf{x})\)
4:\(\psi_{k}(\mathbf{x})=f_{2}(\mathbf{x})+\gamma_{k}g_{2}(\mathbf{x})\)
5: Invoke \(\mathbf{x}_{k}=\) PB-APG-sc\((\phi_{k},\psi_{k},\mu,L_{f_{1}},L_{g_{1}},\mathbf{x}_{k-1},\epsilon_{k})\)
6:\(\epsilon_{k+1}=\frac{1}{\eta}\epsilon_{k}\)
7:\(\gamma_{k+1}=\nu\gamma_{k}\)
8:endfor ```

**Algorithm 4** Adaptive PB-APG-sc method (aPB-APG-sc)

Similar to Algorithm 2, we have the following convergence results of Algorithm 4.

**Theorem D.1**.: _Suppose that Assumptions 2.1, 2.2, 3.1, 3.2, and 3.7 hold. Let \(\epsilon_{0}>0\) be given._

* _When_ \(\alpha>1\)_, set_ \(\nu>\eta^{\alpha-1}\)_,_ \(N=\lceil\log_{\eta^{1-\alpha}\nu}(\rho L_{F}^{\alpha}(\alpha-1)^{\alpha-1} \alpha^{-\alpha}\epsilon_{0}^{1-\alpha}/\gamma_{0})\rceil_{+}\) _and_ \(\gamma_{k}^{*}=\rho L_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha} \epsilon_{0}^{1-\alpha}\eta^{k(\alpha-1)}\)_;_
* _When_ \(\alpha=1\)_, set_ \(\nu>1\)_,_ \(N=\lceil\log_{\nu}(\rho l_{F}/\gamma_{0})\rceil_{+}\) _and_ \(\gamma_{k}^{*}=\rho L_{F}\)_._

_Then, for any \(k\geq N\), Algorithm 2 generates an \((\frac{\epsilon_{0}}{\eta^{k}},\frac{2\epsilon_{0}}{\eta^{k}(\gamma_{0}\eta^{ k}-\gamma_{k}^{*})})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{\eta^{k}}{\epsilon_{ 0}}+\sqrt{\frac{\nu^{k}l_{F}^{\max\{\alpha,\beta\}}L_{g_{1}}}{\epsilon^{\max \{\alpha-1,\beta-1\}}}}\log\frac{\eta^{k}}{\epsilon_{0}}\right).\]

The proof is similar to the proof of Theorem 3.5 in Appendix E.8. So we omit it here.

## Appendix E Proofs of main results

In this section, we propose the proofs of our main convergence results in this paper.

### Proof of Lemma 2.3

Proof.: Since \(X_{\text{opt}}\) is closed and convex (Beck and Sabach, 2014), the projection of any \(\mathbf{x}\in\mathbb{R}^{n}\) onto \(X_{\text{opt}}\), denoted as \(\tilde{\mathbf{x}}\), exists and is unique. Furthermore, it holds that \(\text{dist}(\mathbf{x},X_{\text{opt}})=\|\mathbf{x}-\tilde{\mathbf{x}}\|\).

Then, by Assumption 2.1, we have

\[F(\mathbf{x})-F(\tilde{\mathbf{x}})\geq-\xi^{\top}(\mathbf{x}-\tilde{\mathbf{ x}})\geq-\|\xi\|\|\mathbf{x}-\tilde{\mathbf{x}}\|\geq-l_{F}\|\mathbf{x}-\tilde{ \mathbf{x}}\|,\;\forall\xi\in\partial F(\tilde{\mathbf{x}}).\] (10)

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(G(\mathbf{x})\) & Remarks & Name & \(\alpha\) \\ \hline \(\max_{i\in[m]}\{(\mathbf{a}_{i},\mathbf{x})-b_{i}\}\) & \(\mathbf{a}_{i}\in\mathbb{R}^{n},i\in[m],b\in\mathbb{R}^{m}\) & piece-wise maximum & 1 \\ \(\|\mathbf{x}-\mathbf{x}_{0}\|_{Q}=\sqrt{(\mathbf{x}-\mathbf{x}_{0})^{\text{T}}Q( \mathbf{x}-\mathbf{x}_{0})}\) & \(Q\in\mathbb{S}^{n},Q\sim 0,\mathbf{x}_{0}\in\mathbb{R}^{n}\) & \(Q\)-norm & 1 \\ \(\|\mathbf{x}-\mathbf{x}_{0}\|_{p}\) & \(\mathbf{x}_{0}\in\mathbb{R}^{n},p\geq 1\) & \(\ell_{p}\)-norm & 1 \\ \(\|x\|_{1}\dddot{x}\|_{1}\dddot{x}\|_{2}^{2}\) & \(\tau>0\) & Elastic net & 1 or 24 \\ \(\dddot{\mathbf{x}}-\|_{2}^{2}\) & \(A\in\mathbb{R}^{m\times n},b\in\mathbb{R}^{m}\) & Least squares & 2 \\ \(\frac{1}{m}\sum_{i=1}^{m}\log(1+\exp(-\mathbf{a}_{i}^{\top}\mathbf{x}b_{i}))\) & \(\mathbf{a}_{i}\in\mathbb{R}^{n},i\in[m],b\in\mathbb{R}^{m},A\in\mathbb{R}^{m \times n}\) & Logistic loss & 2 \\ \(\eta(\mathbf{x})+\frac{\sigma}{2}\|\mathbf{x}\|^{2}\) & \(\eta\) convex, \(\sigma>0\) & Strongly-convex & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of some functions satisfying Hlderian error bound with corresponding exponents.

[MISSING_PAGE_EMPTY:17]

### Proof of Theorem 2.5

Proof.: Denote \(\mathbf{x}^{*}\), \(\mathbf{x}^{*}_{\gamma}\) as optimal solutions of problem (P) and (P\({}_{\gamma}\)), respectively.

* **Case of \(\alpha>1\).** Since \(\tilde{\mathbf{x}}^{*}_{\gamma}\) is an \(\epsilon\)-optimal solution of (P\({}_{\gamma}\)), we have \[F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma p(\tilde{\mathbf{x}}^{*}_{\gamma}) \leq F(\mathbf{x})+\gamma p(\mathbf{x})+\epsilon,\quad\forall\mathbf{x}\in \mathbb{R}^{n}.\] (16) Note that the arguments in the proof of Lemma 2.3 still hold. Substituting \(\mathbf{x}=\mathbf{x}^{*}\) into (16) and utilizing \(p(\mathbf{x}^{*})=0\), we have \[F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma p(\tilde{\mathbf{x}}^{*}_{\gamma}) \leq F(\mathbf{x}^{*})+\epsilon=F(\mathbf{x}^{*})+\gamma^{*}p(\mathbf{x}^{*}) +\epsilon\leq F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma^{*}p(\tilde{\mathbf{ x}}^{*}_{\gamma})+2\epsilon,\] where the last inequality follows from setting \(\mathbf{x}=\tilde{\mathbf{x}}^{*}_{\gamma}\) in (13). Then, it holds that \[p(\tilde{\mathbf{x}}^{*}_{\gamma})\leq\frac{2\epsilon}{\gamma-\gamma^{*}}= \frac{2\epsilon}{2l_{F}^{\beta}\epsilon^{1-\beta}}=l_{F}^{-\beta}\epsilon^{ \beta}.\] (17) By setting \(\mathbf{x}=\mathbf{x}^{*}\) in (16), we have \[F(\tilde{\mathbf{x}}^{*}_{\gamma})-F(\mathbf{x}^{*})\leq\gamma(p(\mathbf{x}^{* })-p(\tilde{\mathbf{x}}^{*}_{\gamma}))+\epsilon.\] Using the fact that \(p(\mathbf{x}^{*})=0\leq p(\tilde{\mathbf{x}}^{*}_{\gamma})\), we have \[F(\tilde{\mathbf{x}}^{*}_{\gamma})-F(\mathbf{x}^{*})\leq\epsilon.\] (18) Combing (18) with (17), we conclude that \(\tilde{\mathbf{x}}^{*}_{\gamma}\) is an \((\epsilon,l_{F}^{-\beta}\epsilon^{\beta})\)-optimal solution of (P).
* **Case of \(\alpha=1\).** Since \(\tilde{\mathbf{x}}^{*}_{\gamma}\) is an \(\epsilon\)-optimal solution of (P\({}_{\gamma}\)), we have \[F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma p(\tilde{\mathbf{x}}^{*}_{\gamma}) \leq F(\mathbf{x}^{*}_{\gamma})+\gamma p(\mathbf{x}^{*}_{\gamma})+\epsilon.\] (19) On the one hand, as \(\gamma=\gamma^{*}+l_{F}^{\beta}\epsilon^{1-\beta}>\gamma^{*}\), by Lemma 2.4, \(\mathbf{x}^{*}_{\gamma}\) is an optimal solution of (P). On the other hand, since \(\gamma\geq\gamma^{*}\), according to Lemma 2.4, \(\mathbf{x}^{*}\) is also an optimal solution of (P\({}_{\gamma}\)). Therefore, \(p(\mathbf{x}^{*})=0\) and \(p(\mathbf{x}^{*}_{\gamma})=0\), it holds that \[F(\mathbf{x}^{*}) \leq F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma p(\tilde{\mathbf{ x}}^{*}_{\gamma})\] \[\overset{\eqref{eq:p(\mathbf{x}^{*}_{\gamma})}}{\leq}F(\mathbf{ x}^{*}_{\gamma})+\gamma p(\mathbf{x}^{*}_{\gamma})+\epsilon\] \[=F(\mathbf{x}^{*}_{\gamma})+\gamma^{*}p(\mathbf{x}^{*}_{\gamma})+\epsilon\] (20) \[=F(\mathbf{x}^{*})+\gamma^{*}p(\mathbf{x}^{*})+\epsilon\] \[\leq F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma^{*}p(\tilde{ \mathbf{x}}^{*}_{\gamma})+\epsilon,\] where the first inequality follows from the fact that \(\mathbf{x}^{*}\) is an optimal solution of (P\({}_{\gamma}\)), and the last inequality follows from the optimality of \(\mathbf{x}^{*}\) to (P\({}_{\gamma}\)) when \(\gamma\geq\gamma^{*}\). The second inequality of (20) and \(p(\tilde{\mathbf{x}}^{*}_{\gamma})\geq 0\) imply that \[F(\tilde{\mathbf{x}}^{*}_{\gamma})\leq F(\mathbf{x}^{*}_{\gamma})+\gamma p( \mathbf{x}^{*}_{\gamma})+\epsilon=F(\mathbf{x}^{*})+\gamma p(\mathbf{x}^{*})+ \epsilon\leq F(\mathbf{x}^{*})+\epsilon.\] That is, it holds that \[F(\tilde{\mathbf{x}}^{*}_{\gamma})\leq F(\mathbf{x}^{*})+\epsilon.\] (21) In addition, from (20), we have \(F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma p(\tilde{\mathbf{x}}^{*}_{\gamma}) \leq F(\tilde{\mathbf{x}}^{*}_{\gamma})+\gamma^{*}p(\tilde{\mathbf{x}}^{*}_{ \gamma})+\epsilon\), which implies that \[p(\tilde{\mathbf{x}}^{*}_{\gamma})\leq\frac{\epsilon}{\gamma-\gamma^{*}}= \frac{\epsilon}{l_{F}^{\beta}\epsilon^{1-\beta}}=l_{F}^{-\beta}\epsilon^{\beta}.\] (22) This result along with (21) demonstrate that \(\tilde{\mathbf{x}}^{*}_{\gamma}\) is an \((\epsilon,l_{F}^{-\beta}\epsilon^{\beta})\)-optimal solution of (P).

### Proof of Theorem 2.6

Proof.: Let \(\tilde{\mathbf{x}}^{*}_{\gamma}\) be the projection of \(\tilde{\mathbf{x}}^{*}_{\gamma}\) on \(X_{\text{opt}}\), we have \(\|\tilde{\mathbf{x}}^{*}_{\gamma}-\tilde{\mathbf{x}}^{*}_{\gamma}\|=\mathrm{ dist}(\tilde{\mathbf{x}}^{*}_{\gamma},X_{\text{opt}})\). By Assumption 2.2, the following inequality holds,

\[\|\tilde{\mathbf{x}}^{*}_{\gamma}-\hat{\mathbf{x}}^{*}_{\gamma}\|^{\alpha}\leq \rho p(\tilde{\mathbf{x}}^{*}_{\gamma})\overset{\eqref{eq:p(\mathbf{x}^{*}_{ \gamma})}}{\leq}\rho l_{F}^{-\beta}\epsilon^{\beta}\implies\|\tilde{\mathbf{x}}^{* }_{\gamma}-\tilde{\mathbf{x}}^{*}_{\gamma}\|\leq\left(\rho l_{F}^{-\beta} \epsilon^{\beta}\right)^{\frac{1}{\alpha}},\] (23)

where \((a)\) follows from (17) when \(\alpha>1\) or from (22) when \(\alpha=1\).

By Assumption 2.1, we have

\[F(\tilde{\mathbf{x}}^{*}_{\gamma})-F^{*}\geq F(\tilde{\mathbf{x}}^{*}_{\gamma})-F( \tilde{\mathbf{x}}^{*}_{\gamma})\overset{\eqref{eq:p(\mathbf{x}^{*}_{\gamma})}}{ \geq}-l_{F}\|\tilde{\mathbf{x}}^{*}_{\gamma}-\hat{\mathbf{x}}^{*}_{\gamma}\| \geq-l_{F}\left(\rho l_{F}^{-\beta}\epsilon^{\beta}\right)^{\frac{1}{\alpha}},\]

where the first inequality follows from \(F(\hat{\mathbf{x}}^{*}_{\gamma})\geq F^{*}\) and \(\hat{\mathbf{x}}^{*}_{\gamma}\in X_{\text{opt}}\).

### Proof of Theorem 2.7

Proof.: For any \(\mathbf{x}\in\mathrm{dom}(F)\), let \(\bar{\mathbf{x}}\) be the projection of \(\mathbf{x}\) onto \(X_{\text{opt}}\), where the existence and uniqueness of \(\bar{\mathbf{x}}\) follows from that \(X_{\text{opt}}\) is closed and convex. Since \(F\) is \(l\)-Lipschitz continuous, similar to (10), we have

\[F(\mathbf{x})-F(\bar{\mathbf{x}})\geq-l\|\mathbf{x}-\bar{\mathbf{x}}\|,\; \forall\xi\in\partial F(\bar{\mathbf{x}}).\] (24)

Therefore, all the requirements of (10) in equations (11), (14) and (15) can be replaced by (24). This implies that Lemmas 2.3 and 2.4 also hold for the global solutions of problems (P) and (P\({}_{\gamma}\)) when \(F\) is non-convex. Then, the final result follows a similar pattern to Theorem 2.5. Here we omit it. 

### Proof of Theorem 2.8

Proof.: Let \(\bar{\mathbf{x}}_{\gamma}^{*}\) be the projection of \(\mathbf{x}_{\gamma}^{*}\) onto \(X_{\text{opt}}\) and \(\hat{\mathbf{x}}_{\gamma}^{*}=c\mathbf{x}_{\gamma}^{*}+(1-c)\bar{\mathbf{x}}_ {\gamma}^{*}\) with \(c=\min\{1,1-\frac{r}{\|\mathbf{x}_{\gamma}^{*}-\bar{\mathbf{x}}_{\gamma}^{*} \|}\}\), which implies that \(\hat{\mathbf{x}}_{\gamma}^{*}\in\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\). Then, we have

\[F(\mathbf{x}_{\gamma}^{*})+\gamma p(\mathbf{x}_{\gamma}^{*})\leq F(\hat{ \mathbf{x}}_{\gamma}^{*})+\gamma p(\hat{\mathbf{x}}_{\gamma}^{*})\overset{(i)} {\leq}F(\hat{\mathbf{x}}_{\gamma}^{*})+\gamma(cp(\mathbf{x}_{\gamma}^{*})+(1- c)p(\bar{\mathbf{x}}_{\gamma}^{*}))=F(\hat{\mathbf{x}}_{\gamma}^{*})+\gamma cp (\mathbf{x}_{\gamma}^{*}),\] (25)

where inequality \((i)\) follows from the convexity of \(p(\mathbf{x})\).

Inequality (25) demonstrates that

\[\gamma(1-c)p(\mathbf{x}_{\gamma}^{*})\leq F(\hat{\mathbf{x}}_{\gamma}^{*})-F (\mathbf{x}_{\gamma}^{*})\leq l\|\hat{\mathbf{x}}_{\gamma}^{*}-\mathbf{x}_{ \gamma}^{*}\|=l(1-c)\|\mathbf{x}_{\gamma}^{*}-\bar{\mathbf{x}}_{\gamma}^{*}\| \leq l(1-c)(\rho p(\mathbf{x}_{\gamma}^{*}))^{\frac{1}{\alpha}},\]

where the second inequality follows from the \(l\)-Lipschitz continuity of \(F\) on \(\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\). Therefore, it holds that

\[\gamma p(\mathbf{x}_{\gamma}^{*})\leq l(\rho p(\mathbf{x}_{\gamma}^{*}))^{ \frac{1}{\alpha}}.\] (26)

* **Case of \(\alpha>1\).** By (26), we have \(p(\mathbf{x}_{\gamma}^{*})\leq\big{(}\frac{\rho l^{\alpha}}{\gamma^{\alpha}} \big{)}^{\frac{1}{\alpha-1}}\), which demonstrates that \(p(\mathbf{x}_{\gamma}^{*})\leq\epsilon\) if \(\gamma\geq(\frac{\rho l^{\alpha}}{\varepsilon^{\alpha-1}})^{\frac{1}{\alpha}}\). Then, for any \(\mathbf{x}_{\gamma}\in\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\) that also satisfies \(p(\mathbf{x}_{\gamma})\leq p(\mathbf{x}_{\gamma}^{*})\leq\epsilon\), we have \[F(\mathbf{x}_{\gamma}^{*})+\gamma p(\mathbf{x}_{\gamma}^{*})\leq F(\mathbf{x} _{\gamma})+\gamma p(\mathbf{x}_{\gamma}),\] (27) which implies that \(F(\mathbf{x}_{\gamma}^{*})-F(\mathbf{x}_{\gamma})\leq\gamma(p(\mathbf{x}_{ \gamma})-p(\mathbf{x}_{\gamma}^{*}))\leq 0\). The desired result follows.
* **Case of \(\alpha=1\).** By (26), we have \(p(\mathbf{x}_{\gamma}^{*})=0\) if \(\gamma>\rho l\). Therefore, for any \(\mathbf{x}_{\gamma}\in\mathcal{B}(\mathbf{x}_{\gamma}^{*},r)\bigcap X_{\text{ opt}}\), by the definition of \(\mathbf{x}_{\gamma}^{*}\), it holds that \[F(\mathbf{x}_{\gamma}^{*})+\gamma p(\mathbf{x}_{\gamma}^{*})\leq F(\mathbf{x} _{\gamma})+\gamma p(\mathbf{x}_{\gamma}),\] which demonstrates that \(F(\mathbf{x}_{\gamma}^{*})\leq F(\mathbf{x}_{\gamma})\). The desired result follows.

### Proof of Theorem 3.3

Proof.: From [1, Theorem 10.34], the objective value after \(K\) iterations can be bounded by

\[\Phi_{\gamma}(\mathbf{x}_{K})-\Phi_{\gamma}^{*}\leq\frac{2L_{\gamma}\|\mathbf{ x}_{0}-\mathbf{x}^{*}\|^{2}}{(K+1)^{2}},\]

where \(L_{\gamma}=L_{f_{1}}+\gamma L_{g_{1}}\).

Combining this with our stopping criterion, we find that after \(K\) iterations,

\[\Phi_{\gamma}(\mathbf{x}_{K})-\Phi_{\gamma}^{*}\leq\epsilon.\]

This indicates that we obtain an \(\epsilon\)-optimal solution to problem (P\({}_{\gamma}\)). The value of \(K\) satisfies:

\[K=\sqrt{\frac{2(L_{f_{1}}+\gamma L_{g_{1}})}{\epsilon}}R-1.\]

Specifically, we analyze the value of \(K\) in various scenarios in the form of \(\mathcal{O}(\cdot)\).

* **Case of \(\alpha>1\).** In this case, \(\gamma=\gamma^{*}+2l_{F}^{\beta}\epsilon^{1-\beta}\) comprises two components: \(\gamma^{*}\) and \(2l_{F}^{\beta}\epsilon^{1-\beta}\). Therefore, it is natural to discuss which of these two components plays the dominant role in the complexity results. First, we write \(K\) in the form: \[K=\sqrt{\frac{2(L_{f_{1}}+(\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{- \alpha}\epsilon^{1-\alpha}+2l_{F}^{\beta}\epsilon^{1-\beta})L_{g_{1}})}{ \epsilon}}R-1.\]If \(\beta<\alpha\), the dominating term in \(\gamma\) is \(\gamma^{*}=\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha}\epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\alpha}\epsilon^{1-\alpha}L_{g_{ 1}}}{\epsilon}}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+ \sqrt{\frac{l_{F}^{\alpha}L_{g_{1}}}{\epsilon^{\alpha}}}\right).\] If \(\beta=\alpha\), we have \(\gamma=\left(\rho(\alpha-1)^{\alpha-1}\alpha^{-\alpha}+2\right)l_{F}^{\alpha} \epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\alpha}\epsilon^{1-\alpha}L_{g_ {1}}}{\epsilon}}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+ \sqrt{\frac{l_{F}^{\alpha}L_{g_{1}}}{\epsilon^{\alpha}}}\right).\] If \(\beta>\alpha\), the dominating term in \(\gamma\) is \(2l_{F}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+2l_{F}^{\beta}\epsilon^{1-\beta}L_{ g_{1}}}{\epsilon}}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+ \sqrt{\frac{l_{F}^{\beta}L_{g_{1}}}{\epsilon}}\right).\]
* **Case of \(\alpha=1\)**. In this case, \(\gamma=\gamma^{*}+l_{F}^{\beta}\epsilon^{1-\beta}\), where \(\gamma^{*}=\rho l_{F}\). Similarly, we explore which of these two elements plays a more significant role. If \(\beta<1\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\rho l_{F}L_{g_{1}}}{\epsilon}}\right) =\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+\sqrt{\frac{l_{F}L_{g_{1}} }{\epsilon}}\right).\] If \(\beta=1\), we have \(\gamma=(\rho+1)l_{F}\epsilon^{1-\alpha}\). Then the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+(\rho+1)l_{F}L_{g_{1}}}{\epsilon}} \right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+\sqrt{\frac{l_{F} L_{g_{1}}}{\epsilon}}\right).\] If \(\beta>1\), the dominating term in \(\gamma\) is \(l_{F}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\beta}\epsilon^{1-\beta}L_{g_ {1}}}{\epsilon}}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+ \sqrt{\frac{l_{F}^{\beta}L_{g_{1}}}{\epsilon^{\beta}}}\right).\]

Combining the above results, we conclude that

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\epsilon}}+\sqrt{\frac{l_{F}^{\max \{\alpha,\beta\}}L_{g_{1}}}{\epsilon^{\max\{\alpha,\beta\}}}}\right).\]

### Proof of Theorem 3.5

Proof.: In this proof, we denote \(\Phi_{k}^{*}\) as the optimal value of problem (P\({}_{\gamma}\)) when \(\gamma=\gamma_{k}\), and \(\mathbf{x}_{k}\) as the output of PB-APG (Algorithm 1) in the \(k\)-th iteration.

* **Case of \(\alpha>1\).** Suppose that \(N\) is the smallest nonnegative integer such that \(\gamma_{N}\geq\gamma_{N}^{*}:=\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{- \alpha}\epsilon_{N}^{1-\alpha}\). In this case, we have \[\gamma_{N}=\gamma_{0}\nu^{N}\geq\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha ^{-\alpha}\epsilon_{N}^{1-\alpha}=\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1} \alpha^{-\alpha}\epsilon_{0}^{1-\alpha}(1/\eta)^{(1-\alpha)N},\] (28) which is equivalent to \[\gamma_{0}\left(\nu\eta^{1-\alpha}\right)^{N}\geq\rho l_{F}^{\alpha}(\alpha-1 )^{\alpha-1}\alpha^{-\alpha}\epsilon_{0}^{1-\alpha}.\] (29) From (29), after at most \(N:=\lceil\log_{\eta^{1-\alpha}\nu}\left(\frac{\rho l_{F}^{\alpha}(\alpha-1)^{ \alpha-1}\alpha^{-\alpha}\epsilon_{0}^{1-\alpha}}{\gamma_{0}}\right)\rceil_{+}\) iterations, (28) holds. Since \(x_{N}=\text{PB-APG}(\phi_{N},\psi_{N},L_{f_{1}},L_{g_{1}},\mathbf{x}_{N-1}, \epsilon_{N})\), we have \[\Phi_{N}(\mathbf{x}_{N})-\Phi_{N}^{*}\leq\epsilon_{N},\quad\gamma_{N}\geq \gamma_{N}^{*},\]which shows that \(\mathbf{x}_{N}\) is an \(\epsilon_{N}\)-optimal solution of (P\({}_{\gamma}\)) with \(\gamma=\gamma_{N}\). From the proof in Theorem 2.5 (see inequalities (17) and (18) in Appendix E.3), \(\mathbf{x}_{N}\) is also an \((\frac{L_{0}}{\eta\eta},\frac{2\epsilon_{0}}{\eta^{N}(\gamma_{0}\eta^{N}- \gamma_{N}^{*})})\)-optimal solution of problem (P). Furthermore, note that for any iteration \(k\geq N\), inequality (29) always holds, which means that the following statement holds for any \(k\geq N\): \[\Phi_{k}(\mathbf{x}_{k})-\Phi_{k}^{*}\leq\epsilon_{k},\quad\gamma_{k}\geq \gamma_{k}^{*}.\] (30) Let \(I_{k}\) be the number of iterations of PB-APG required to satisfy (30) at the \(k\)-th iteration of aPB-APG. Then, for any \(k\geq N\), the total number of iterations is \[K=I_{0}+I_{1}+\cdots+I_{k}.\] From (Beck, 2017, Theorem 10.34), the number of iterations in \(i\)-th inner loop satisfies: \[I_{i}=\sqrt{\frac{2(L_{f_{1}}+\gamma_{i}L_{g_{1}})}{\epsilon_{i}}}\|\mathbf{x} _{i-1}-\mathbf{x}_{i}^{*}\|-1,\] where \(\mathbf{x}_{i}^{*}\) is the optimal solution in \(i\)-th inner loop. Then we have that \[K =\sum_{i=0}^{k}\sqrt{\frac{2(L_{f_{1}}+\gamma_{i}L_{g_{1}})}{ \epsilon_{i}}}\|\mathbf{x}_{i-1}-\mathbf{x}_{i}^{*}\|-k\] \[\leq\sum_{i=0}^{k}\sqrt{\frac{2(L_{f_{1}}+\gamma_{k}L_{g_{1}})}{ \epsilon_{i}}}R-k\] \[=\frac{\eta^{\frac{k}{2}}-1}{\eta^{\frac{1}{2}}-1}\sqrt{\frac{2(L _{f_{1}}+\gamma_{0}\nu^{k}L_{g_{1}})}{\epsilon_{0}}}-k.\] For simplicity, we can also use \(\mathcal{O}(\cdot)\) to show the value of \(K\). \[K =\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{0}L_{g_{1}}}{ \epsilon_{0}}}\right)+\cdots+\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k }L_{g_{1}}}{\epsilon_{k}}}\right)\] \[\leq\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k}L_{g_{1}}}{ \epsilon_{0}}}\right)+\cdots+\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k }L_{g_{1}}}{\epsilon_{k}}}\right)\] \[=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k}L_{g_{1}}}{ \epsilon_{k}}}\left(1+\sqrt{1/\eta}+\sqrt{1/\eta^{2}}+\cdots+\sqrt{1/\eta^{k}} \right)\right)\] \[=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k}L_{g_{1}}}{ \epsilon_{k}}}\right)\] \[=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}\eta^{k}}{\epsilon_{0}}}+ \sqrt{\frac{L_{g_{1}}\gamma_{0}(\eta\nu)^{k}}{\epsilon_{0}}}\right).\]
* **Case of \(\alpha=1\).** Suppose that after \(N\) updates, we have \(\gamma_{N}\geq\rho l_{F}\), i.e., \[\gamma_{0}\nu^{N}\geq\rho l_{F}.\] (31) This demonstrates that after for all \(k\geq N:=\log_{\nu}\left(\frac{\rho l_{F}}{\gamma_{0}}\right)\), (31) always holds. Similar to the case of \(\alpha>1\), the total iteration number is: \[K =\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{0}L_{g_{1}}}{ \epsilon_{0}}}\right)+\cdots+\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k }L_{g_{1}}}{\epsilon_{k}}}\right)\] \[=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\gamma_{k}L_{g_{1}}}{ \epsilon_{k}}}\right)\] \[=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}\eta^{k}}{\epsilon_{0}}}+ \sqrt{\frac{L_{g_{1}}\gamma_{0}(\eta\nu)^{k}}{\epsilon_{0}}}\right).\]

### Proof of Theorem 3.8

Before proving Theorem 3.8, we need the following lemma that is modified from Theorem 1 in Lin and Xiao (2014), we state it in the subsequent lemma for completeness.

**Lemma E.1**.: _Suppose that Assumptions 2.1, 3.1, 3.2, and 3.7 hold. Let \(\mathbf{x}_{\gamma}^{*}\) be an optimal solution of problem (\(\mathsf{P}_{\gamma}\)) and suppose that there exists a constant \(R\) such that \(\max\{\|\mathbf{y}_{0}-\mathbf{x}_{\gamma}^{*}\|,\|\tilde{\mathbf{x}}- \mathbf{x}_{\gamma}^{*}\|\}\leq R\). Then, the sequence \(\{\mathbf{x}_{k}\}\) generated by Algorithm 3 satisfy_

\[\Phi_{\gamma}(\mathbf{x}_{k})-\Phi_{\gamma}(\mathbf{x}_{\gamma}^{*})\leq\left( \frac{L_{\gamma}+\mu}{2}R^{2}\right)\left(1-\sqrt{\frac{\mu}{L_{\gamma}}} \right)^{k}.\] (32)

Proof.: Denote \(L_{\gamma}=L_{f_{1}}+\gamma L_{g_{1}}\). By Theorem 3.1 in Beck and Teboulle (2009), we have

\[\Phi_{\gamma}(\tilde{\mathbf{x}})-\Phi_{\gamma}(\mathbf{x}_{\gamma}^{*})\leq \frac{L_{\gamma}}{2}\|\mathbf{y}_{0}-\mathbf{x}_{\gamma}^{*}\|^{2}.\] (33)

Utilize Theorem 1 in Lin and Xiao (2014), we have

\[\Phi_{\gamma}(\mathbf{x}_{k})-\Phi_{\gamma}(\mathbf{x}_{\gamma}^ {*}) \leq\left(\Phi_{\gamma}(\tilde{\mathbf{x}})-\Phi_{\gamma}(\mathbf{ x}_{\gamma}^{*})+\frac{\mu}{2}\|\tilde{\mathbf{x}}-\mathbf{x}_{\gamma}^{*}\|^{2} \right)\left(1-\sqrt{\frac{\mu}{L_{\gamma}}}\right)^{k}\] (34) \[\overset{\eqref{eq:L_1}}{\leq}\left(\frac{L_{\gamma}}{2}\| \mathbf{y}_{0}-\mathbf{x}_{\gamma}^{*}\|^{2}+\frac{\mu}{2}\|\tilde{\mathbf{x}} -\mathbf{x}_{\gamma}^{*}\|^{2}\right)\left(1-\sqrt{\frac{\mu}{L_{\gamma}}} \right)^{k}\] \[\leq\left(\frac{L_{\gamma}+\mu}{2}R^{2}\right)\left(1-\sqrt{\frac {\mu}{L_{\gamma}}}\right)^{k}.\]

By Lemma E.1, we are now prepared to prove Theorem 3.8.

Proof.: By Lemma E.1, the number of iterations required to achieve an \(\epsilon\)-optimal solution for problem (\(\mathsf{P}_{\gamma}\)) is

\[K=\mathcal{O}\left(\sqrt{\frac{L_{\gamma}}{\mu}}\log\left(\frac{L_{\gamma}+ \mu}{2\epsilon}R^{2}\right)\right)=\mathcal{O}\left(\sqrt{\frac{L_{\gamma}}{ \mu}}\log\frac{1}{\epsilon}\right).\]

* **Case of \(\alpha>1\).** In this case, \(\gamma=\gamma^{*}+2l_{F}^{\beta}\epsilon^{1-\beta}\), where \(\gamma^{*}=\rho l_{F}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha}\epsilon^{1 -\alpha}\). If \(\beta<\alpha\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\alpha}\epsilon^{1-\alpha}L_{ g_{1}}}{\mu}}\log\frac{1}{\epsilon}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{ \mu}}\log\frac{1}{\epsilon}+\sqrt{\frac{l_{F}^{\alpha}L_{g_{1}}}{\epsilon^{ \alpha-1}}}\log\frac{1}{\epsilon}\right).\] If \(\beta=\alpha\), we have \(\gamma=\left(\rho(\alpha-1)^{\alpha-1}\alpha^{-\alpha}+2\right)l_{F}^{\alpha} \epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\alpha}\epsilon^{1-\alpha}L_{ g_{1}}}{\mu}}\log\frac{1}{\epsilon}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{ \mu}}\log\frac{1}{\epsilon}+\sqrt{\frac{l_{F}^{\alpha}L_{g_{1}}}{\epsilon^{ \alpha-1}}}\log\frac{1}{\epsilon}\right).\] If \(\beta>\alpha\), the dominating term in \(\gamma\) is \(2l_{F}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+2l_{F}^{\beta}\epsilon^{1-\beta}L_{ g_{1}}}{\mu}}\log\frac{1}{\epsilon}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{ \mu}}\log\frac{1}{\epsilon}+\sqrt{\frac{l_{F}^{\beta}L_{g_{1}}}{\epsilon^{ \alpha-1}}}\log\frac{1}{\epsilon}\right).\]
* **Case of \(\alpha=1\).** When \(\alpha=1\), \(\gamma\) can be written as \(\gamma=\gamma^{*}+l_{F}^{\beta}\epsilon^{1-\beta}\), where \(\gamma^{*}=\rho l_{F}\). If \(\beta<1\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{1}{\epsilon}\right)= \mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{1}{\epsilon}+\sqrt{ \frac{l_{F}L_{g_{1}}}{\epsilon^{\alpha-1}}}\log\frac{1}{\epsilon}\right).\] If \(\beta=1\), we have \(\gamma=(\rho+1)l_{F}\epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+\rho l_{F}L_{g_{1}}}{\mu}}\log\frac{1}{ \epsilon}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{1}{ \epsilon}+\sqrt{\frac{l_{F}L_{g_{1}}}{\epsilon^{\alpha-1}}}\log\frac{1}{ \epsilon}\right).\]If \(\beta>1\), the dominating term in \(\gamma\) is \(l_{F}^{\beta}\epsilon^{1-\beta}\). Then, we have

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}+l_{F}^{\beta}\epsilon^{1-\beta}L_{g_{1}} }{\mu}}\log\frac{1}{\epsilon}\right)=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{ \mu}}\log\frac{1}{\epsilon}+\sqrt{\frac{l_{F}^{\beta}L_{g_{1}}}{\epsilon^{ \beta-1}}}\log\frac{1}{\epsilon}\right).\]

Combining the above results, we conclude that

\[K=\mathcal{O}\left(\sqrt{\frac{L_{f_{1}}}{\mu}}\log\frac{1}{\epsilon}+\sqrt{ \frac{l_{F}^{\max\left(\alpha,\beta\right)}L_{g_{1}}}{\epsilon^{\max\left( \alpha-1,\beta-1\right)}}}\log\frac{1}{\epsilon}\right).\]

### Proof of Theorem 3.9

Proof.: Denote \(l_{\gamma}=l_{f_{2}}+\gamma l_{g_{2}}\). Define \(\Phi_{\gamma,best}^{K}=\min\limits_{i=0,\ldots,K}\Phi_{\gamma}(\mathbf{x}_{i})\) and \(\hat{\Phi}_{\gamma,best}^{K,j}=\min\limits_{i=j,\ldots,K}\Phi_{\gamma}( \mathbf{x}_{i})\) for all \(0\leq j\leq K\). We claim that the sequence generated by the subgradient method satisfies

\[\Phi_{\gamma,best}^{K}-\Phi_{\gamma}^{*}\leq\frac{l_{\gamma}}{4}\frac{R^{2}+2 \log 2}{\sqrt{K+2}}.\] (35)

Specifically, from Lemma 8.24 in Beck (2017), for all \(0\leq j\leq K\), we have

\[\hat{\Phi}_{\gamma,best}^{K,j}-\Phi_{\gamma}^{*}\leq\frac{1}{2}\frac{R^{2}+ \sum_{k=1}^{K}\eta_{k}^{2}\|\xi_{k}\|^{2}}{\sum_{k=j}^{K}\eta_{k}}.\] (36)

Define \(\lfloor\cdot\rfloor\) and \(\lceil\cdot\rceil\) as rounding up and rounding down, respectively. Let \(j=\lfloor\frac{K}{2}\rfloor\) in (36), by the definition of step-size \(\eta_{k}=\frac{R}{l_{\gamma}\sqrt{k+1}}\), we have

\[\hat{\Phi}_{\gamma,best}^{K,j}-\Phi_{\gamma}^{*}\leq\frac{l_{\gamma}}{2}\frac {R^{2}+\sum_{k=1}^{K}\frac{K}{2}\lfloor\frac{1}{K+1}\over\sqrt{k+1}}{\sum_{k= 1}^{K}\lfloor\frac{1}{K}\rfloor}\leq\frac{l_{\gamma}}{4}\frac{R^{2}+2\log 2 }{\sqrt{K+2}},\] (37)

where the second inequality follows from that \(\sum_{k=1}^{K}\lfloor\frac{1}{K}\rfloor\frac{1}{k+1}\leq\int_{\lceil\frac{K}{2 }\rceil-1}^{K}\frac{1}{s+1}ds\leq 2\log 2\) and \(\sum_{k=1}^{K}\lfloor\frac{K}{2}\rfloor\frac{1}{\sqrt{k+1}}\geq\int_{\lceil \frac{K}{2}\rceil}^{K+1}\frac{1}{\sqrt{s+1}}ds\geq\frac{1}{2}\sqrt{K+2}\).

From the fact that \(\Phi_{\gamma,best}^{K}\leq\hat{\Phi}_{\gamma,best}^{K,j}\), The desired result of (35) follows.

Then, inequality (35) demonstrates that the number of iterations to obtain an \(\epsilon\)-optimal solution for problem (P\({}_{\gamma}\)) is

\[K=\mathcal{O}\left(\frac{l_{f_{2}}+\gamma l_{g_{2}}}{\epsilon}\right)^{2}.\]

* **Case of \(\alpha>1\).** we have \(\gamma=\gamma^{*}+2l_{f_{2}}^{\beta}\epsilon^{1-\beta}\) and \(\gamma^{*}=\rho l_{f_{2}}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha} \epsilon^{1-\alpha}\). If \(\beta<\alpha\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+l_{f_{2}}^{\alpha}\epsilon^{1-\alpha}l_{g_{ 2}}}{\epsilon}\right)^{2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+ \frac{l_{f_{2}}^{2\alpha}l_{g_{2}}^{2}}{\epsilon^{2\alpha}}\right).\] If \(\beta=\alpha\), we have \(\gamma=\left(\rho(\alpha-1)^{\alpha-1}\alpha^{-\alpha}+2\right)l_{F}^{\alpha} \epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+l_{f_{2}}^{\alpha}\epsilon^{1-\alpha}l_{g_{ 2}}}{\epsilon}\right)^{2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+ \frac{l_{f_{2}}^{2\alpha}l_{g_{2}}^{2}}{\epsilon^{2\alpha}}\right).\] If \(\beta>\alpha\), the dominating term in \(\gamma\) is \(2l_{F}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+2l_{f_{2}}^{\beta}\epsilon^{1-\beta}l_{g_{ 2}}}{\epsilon}\right)^{2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+ \frac{l_{f_{2}}^{2\beta}l_{g_{2}}^{2}}{\epsilon^{2\beta}}\right).\]* **Case of \(\alpha=1\).** we have \(\gamma=\gamma^{*}+l_{f_{2}}^{\beta}\epsilon^{1-\beta}\) and \(\gamma^{*}=\rho l_{f_{2}}\). If \(\beta<1\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+\rho l_{f_{2}}l_{g_{2}}}{\epsilon}\right)^{ 2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+\frac{l_{f_{2}}^{2}l_{g _{2}}^{2}}{\epsilon^{2}}\right).\] If \(\beta=1\), we have \(\gamma=(\rho+1)l_{F}\epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+\rho l_{f_{2}}l_{g_{2}}}{\epsilon}\right)^{ 2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+\frac{l_{f_{2}}^{2}l_{g _{2}}^{2}}{\epsilon^{2}}\right).\] If \(\beta>1\), the dominating term in \(\gamma\) is \(l_{f_{2}}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{l_{f_{2}}+l_{f_{2}}^{\beta}l_{g_{2}}\epsilon^{1- \beta}}{\epsilon}\right)^{2}=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{ 2}}+\frac{l_{f_{2}}^{2\beta}l_{g_{2}}^{2}}{\epsilon^{2\beta}}\right).\] Combining the above results, we conclude that \[K=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\epsilon^{2}}+\frac{l_{f_{2}}^{\max\{2 \alpha,2\beta\}}l_{g_{2}}^{2}}{\epsilon^{\max\{2\alpha,2\beta\}}}\right).\]

### Proof of Theorem 3.10

Proof.: Denote \(l_{\gamma}=l_{f_{2}}+\gamma l_{g_{2}}\), define \(\Phi_{\gamma,best}^{K}=\min\limits_{i=0,\ldots,K}\Phi_{\gamma}(\mathbf{x}_{i})\). From Theorem 8.31 in Beck [2017], the sequence generated by the subgradient method satisfies

\[\Phi_{\gamma,best}^{K}-\Phi_{\gamma}^{*}\leq\frac{2l_{\gamma}^{2}}{\mu_{f_{2} }(K+1)}.\]

This demonstrates that the number of iterations to obtain an \(\epsilon\)-optimal solution for problem (P\({}_{\gamma}\)) is

\[K=\mathcal{O}\left(\frac{(l_{f_{2}}+\gamma l_{g_{2}})^{2}}{\mu_{f_{2}}\epsilon }\right).\]

* **Case of \(\alpha>1\).** we have \(\gamma=\gamma^{*}+2l_{f_{2}}^{\beta}\epsilon^{1-\beta}\) and \(\gamma^{*}=\rho l_{f_{2}}^{\alpha}(\alpha-1)^{\alpha-1}\alpha^{-\alpha} \epsilon^{1-\alpha}\). If \(\beta<\alpha\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{(l_{f_{2}}+l_{f_{2}}^{\alpha}\epsilon^{1-\alpha}l_{g _{2}})^{2}}{\mu_{f_{2}}\epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{ \mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{2\alpha}l_{g_{2}}^{2}}{\mu_{f_{2}} \epsilon^{2\alpha-1}}\right).\] If \(\beta=\alpha\), we have \(\gamma=\left(\rho(\alpha-1)^{\alpha-1}\alpha^{-\alpha}+2\right)l_{g}^{\alpha} \epsilon^{1-\alpha}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{(l_{f_{2}}+l_{f_{2}}^{\alpha}\epsilon^{1-\alpha}l_{g _{2}})^{2}}{\mu_{f_{2}}\epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{ \mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{2\alpha}l_{g_{2}}^{2}}{\mu_{f_{2}} \epsilon^{2\alpha-1}}\right).\] If \(\beta>\alpha\), the dominating term in \(\gamma\) is \(2l_{F}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{(l_{f_{2}}+2l_{f_{2}}^{\beta}\epsilon^{1-\beta}l_{g_ {2}})^{2}}{\mu_{f_{2}}\epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{ \mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{2\beta}l_{g_{2}}^{2}}{\mu_{f_{2}} \epsilon^{2\beta-1}}\right).\]
* **Case of \(\alpha>1\).** we have \(\gamma=\gamma^{*}+l_{g_{2}}^{\beta}\epsilon^{1-\beta}\) and \(\gamma^{*}=\rho l_{f_{2}}\). If \(\beta<1\), the dominating term in \(\gamma\) is \(\gamma^{*}\). Then, the number of iterations is \[K=\mathcal{O}\left(\frac{(l_{f_{2}}+\rho l_{f_{2}}l_{g_{2}})^{2}}{\mu_{f_{2}} \epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\mu_{f_{2}}\epsilon}+ \frac{l_{f_{2}}^{2}l_{g_{2}}^{2}}{\mu_{f_{2}}\epsilon}\right).\]If \(\beta=1\), we have \(\gamma=(\rho+1)l_{F}\epsilon^{1-\alpha}\). Then, the number of iterations is

\[K=\mathcal{O}\left(\frac{(l_{f_{2}}+\rho l_{f_{2}}l_{g_{2}})^{2}}{\mu_{f_{2}} \epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\mu_{f_{2}}\epsilon}+ \frac{l_{f_{2}}^{2}l_{g_{2}}^{2}}{\mu_{f_{2}}\epsilon}\right).\]

If \(\beta>1\), the dominating term in \(\gamma\) is \(l_{f_{2}}^{\beta}\epsilon^{1-\beta}\). Then, the number of iterations is

\[K=\mathcal{O}\left(\frac{(l_{f_{2}}+l_{f_{2}}^{\beta}l_{g_{2}}\epsilon^{1- \beta})^{2}}{\mu_{f_{2}}\epsilon}\right)=\mathcal{O}\left(\frac{l_{f_{2}}^{2} }{\mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{2\beta}l_{g_{2}}^{2}}{\mu_{f_{2}} \epsilon^{\beta-1}}\right).\]

Combining the above results, we conclude that

\[K=\mathcal{O}\left(\frac{l_{f_{2}}^{2}}{\mu_{f_{2}}\epsilon}+\frac{l_{f_{2}}^{ \max\{2\alpha,2\beta\}}l_{g_{2}}^{2}}{\mu_{f_{2}}\epsilon^{\max\{2\alpha-1,2 \beta-1\}}}\right).\]

## Appendix F Implementation details

In this section, we provide supplementary experiment settings and results. Specifically, in Appendix F.1, we present the detailed experimental settings, and in Appendix F.2, we provide the detailed experimental results. Additionally, in Appendix F.3 and F.4, we conduct experiments with different values of penalty parameter \(\gamma\) and solution accuracy \(\epsilon\), respectively.

### Experiment setting

All simulations are implemented using MATLAB R2023a on a PC running Windows 11 with an AMD (R) Ryzen (TM) R7-7840H CPU (3.80GHz) and 16GB RAM.

#### f.1.1 Experiment setting of Section 4.1

We conduct the first experiment using the aia.t data from LIBSVM datasets5. This data consists of \(30,956\) instances, each with \(n=123\) features. For this experiment, a sample of \(1,000\) instances is taken from the data, denoted as \(A\). The corresponding labels for these instances are denoted as \(b\), where each label \(b_{i}\) is either \(-1\) or \(1\), corresponding to the \(i\)-th instance \(\mathbf{a}_{i}\).

Footnote 5: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/aia.t

The Greedy FISTA algorithm (Liang et al., 2022) is used as a benchmark to compute \(G^{*}\). To compute the proximal mapping of \(f_{2}(\mathbf{x})+\gamma g_{2}(\mathbf{x})\) in problem (P\({}_{\gamma}\)), i.e, projection onto a \(1\)-norm ball, we utilize the method proposed in Duchi et al. (2008), which performs exact projection in \(\mathcal{O}(n)\) expected time, where n is the dimension of \(\mathbf{x}\).

For the PB-APG and PB-APG-sc algorithms, we set the value of \(\gamma=10^{5}\), and we terminate the algorithms when \(\|\mathbf{x}_{k+1}-\mathbf{x}_{k}\|\leq 10^{-10}\). For the aPB-APG and aPB-APG-sc algorithms, we set \(\gamma_{0}=\frac{1}{2^{\gamma}}\), \(\nu=20\), \(\eta=10\), and \(\epsilon_{0}=10^{-6}\). The iterations of these two algorithms continue until \(\epsilon_{k}\) reaches \(10^{-10}\) (meanwhile, \(\gamma=10^{5}\)).

We compare our methods with MNG, BiG-SAM, DBGD, a-IRG, CG-BiO, Bi-SG, and R-APM in this experiment. Specifically, for R-APM (Samadi et al., 2023), the regularization parameter \(\eta\) is set to \(\eta=1/\gamma\), reflecting the equivalence of the penalty formulation (P\({}_{\gamma}\)) to (P\({}_{\text{Reg}}\)), with \(\sigma=1/\gamma\), as previously discussed.

We note that the termination criterion \(\|\mathbf{x}_{k+1}-\mathbf{x}_{k}\|\leq 10^{-10}\) used in our experiments is different from the one proposed in our algorithms since the parameters required for the latter are not easily measurable. Nevertheless, this termination criterion is also widely used in the literature, as it corresponds to a gradient mapping (Beck, 2017, Nesterov, 2018, Davis and Drusvyatskiy, 2019). Furthermore, Theorem 3.5 of Drusvyatskiy and Lewis (2018) implies that \(\|\mathbf{x}_{k+1}-\mathbf{x}_{k}\|\) also measures the distance to the optimal solution set.

#### f.1.2 Experiment setting of Section 4.2

In the second experiment, we address the problem of least squares regression using the YearPredictionMSD data from the UCI Machine Learning Repository6. This data consists of \(515,345\) songs with release years ranging from \(1992\) to \(2011\). Each song has \(90\) features, and the corresponding release year is used as the label.

For this experiment, a sample of \(m=1,000\) songs is taken from the data, and the feature matrix and release years vector are denoted as \(A\) and \(b\), respectively.

Following Section 5.2 in Merchav and Sabach (2023), we apply the min-max scaling technique to normalize the feature matrix \(A\). Additionally, we add an intercept term and \(90\) collinear features to \(A\) such that the resulting matrix \(A^{T}A\) becomes positive semi-definite, which implies that the feasible set \(X_{\text{opt}}\) is not a singleton.

We compare our methods with a-IRG, BiG-SAM, and Bi-SG in this experiment. Specifically, for BiG-SAM (Sabach and Shtern, 2017), we consider the accuracy parameter \(\delta\) for the Moreau envelope with two values, namely \(\delta=1\) and \(\delta=0.01\).

To benchmark the performance, we utilize the MATLAB function lsqminnorm to compute \(G^{*}\). Moreover, we follow the parameter settings outlined in Section 4.1.

### Detailed results of experiments

To approximate the optimal value \(F^{*}\), we use the MATLAB function fmincon to solve a relaxed version of the function-value-based reformulations in equation (P\({}_{\text{val}}\)). In this relaxed version, we replace the constraint in (P\({}_{\text{val}}\)) with \(G(\mathbf{x})-G^{*}\leq\varepsilon\), where \(\varepsilon=10^{-10}\). This allows us to obtain an approximation of the optimal value while allowing for a small deviation from the true optimal value \(G^{*}\).

We gather the total number of iterations for our methods, as well as the lower- and upper-level objective values and the optimal gaps for all the methods, in Table 3. Subsequently, we compare the optimal gaps of all methods, which are defined as \(G(\mathbf{x})-G^{*}\) and \(F(\mathbf{x})-F^{*}\) for the lower- and upper-level optimal gaps, respectively.

Table 3 reveals that for the logistic regression problem (5), our PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc exhibit almost identical function values for both objectives, surpassing other methods in terms of optimal gaps for the lower- and upper-level objectives (measured by the numerical value of the upper-level objective). In the case of the least squares regression problem (6), aPB-APG achieves the smallest optimal gaps for both objectives, followed by PB-APG and PB-APG-sc. These results demonstrate that our methods, despite yielding larger upper-level function values, generate solutions that are significantly closer to the optimal solution, as depicted in Figure 1. Additionally, for the problem in (5), both aPB-APG and aPB-APG-sc require fewer iterations than PB-APG and PB-APG-sc, respectively. This can be attributed to the warm-start mechanism employed in aPB-APG and aPB-APG-sc. Moreover, for the problem in (6), both aPB-APG and aPB-APG-sc require more iterations than PB-APG and PB-APG-sc, respectively. However, they exhibit staircase-shaped curves, which avoid the unwanted oscillations in PB-APG and PB-APG-sc, we have a similar observation in Figure 2.

### Supplementary experiments for different penalty parameters

In this section, we investigate the impact of different values of penalty parameter \(\gamma\) on the experimental results of problems (5) and (6). We set \(\gamma\) to be either \(2\times 10^{4}\) or \(5\times 10^{5}\) for PB-APG and PB-APG-sc, and choose the

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{4}{c}{Logistic Regression Problem (5)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 1470 & 3.2794e-01 & **1.7630e-08** & 4.9382e+00 & **-3.3998e-03** \\ aPB-APG & 1010 & 3.2794e-01 & **1.7630e-08** & 4.9382e+00 & **-3.3998e-03** \\ PB-APG-sc & 2278 & 3.2794e-01 & **1.7630e-08** & 4.9382e+00 & **-3.3998e-03** \\ aPB-APG-sc & 1046 & 3.2794e-01 & **1.7630e-08** & 4.9382e+00 & **-3.3998e-03** \\ MNG & / & 3.4540e-01 & 1.7459e-02 & 1.7469e+00 & -3.1947e+00 \\ BiG-SAM & / & 3.3878e-01 & 1.0840e-02 & 2.2873e+00 & -2.6543e+00 \\ DBGD & / & 5.2681e-01 & 1.9887e-01 & 8.8408e-02 & -4.8532e+00 \\ a+IRG & / & 3.3765e-01 & 9.7121e-03 & 2.5401e+00 & -2.4016e+00 \\ CG-BiO & / & 4.3040e-01 & 1.0246e-01 & 3.7684e-01 & -4.5648e+00 \\ Bi-SG & / & 3.2806e-01 & 1.1530e-04 & 4.6873e+00 & -2.5432e-01 \\ R-APM & / & 3.2794e-01 & 1.7645e-08 & 4.9382e+00 & -3.4013e-03 \\ \hline \hline  & \multicolumn{4}{c}{Least Squares Regression Problem (6)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 39314 & 7.3922e-03 & 6.0034e-07 & 4.7236e+00 & -1.1888e-01 \\ aPB-APG & 40784 & 7.3922e-03 & **6.0030e-07** & 4.7236e+00 & -**1.1888e-01** \\ PB-APG-sc & 46446 & 7.3922e-03 & 6.0034e-07 & 4.7236e+00 & -1.1888e-01 \\ aPB-APG-sc & 61777 & 7.3922e-03 & 6.0035e-07 & 4.7236e+00 & -1.1888e-01 \\ BiG-SAM (\(\delta=1\)) & / & 7.5189e-03 & 1.2733e-04 & 3.5081e+00 & -1.3344e+00 \\ BiG-SAM (\(\delta=0.01\)) & / & 7.3958e-03 & 4.2281e-06 & 5.8510e+01 & 5.3668e+01 \\ a-IRG & / & 1.6224e-02 & 8.8328e-03 & 4.7745e-01 & -4.3651e+00 \\ Bi-SG & / & 8.5782e-03 & 1.1866e-03 & 1.3832e+00 & -3.4593e+00 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Methods comparison: lower- and upper-level objectives and optimal gaps

[MISSING_PAGE_EMPTY:27]

### Supplementary experiments for different solution accuracies

In this section, we investigate the impact of different solution accuracies on the experimental results of problems (5) and (6). We set \(\epsilon\) to be either \(10^{-4}\) or \(10^{-7}\) and terminate the algorithms for PB-APG and PB-APG-sc when \(\|\mathbf{x}_{k+1}-\mathbf{x}_{k}\|\leq\epsilon\). For aPB-APG and aPB-APG-sc, we choose the corresponding \(\epsilon_{0}\) values as 1 or \(10^{-3}\). The remaining settings are the same as in Section 4.

We also plot the values of the residuals of the lower-level objective \(G(\mathbf{x}_{k})-G^{\star}\) and the upper-level objective over time in Figures 5 and 6. Additionally, we also collect the total number of iterations, the lower- and upper-level objective values, and the optimal gaps of our methods in Table 6 for problems (5) and (6) with different solution accuracies.

From Figures 5 and 6, it is evident that in most cases, our methods outperform the other methods in terms of both the lower- and upper-level objectives. However, there is an exception in the case of the upper-level objective for problem (6) when \(\epsilon=10^{-4}\). As illustrated in the second subfigure in Figure 6, our methods exhibit larger function values for the upper-level objective compared to the other methods (except BiG-SAM (\(\delta=0.01\))), despite still achieving smaller optimal gaps for the lower-level objective. This discrepancy actually indicates that our methods have not yet achieved the desired accuracy when \(\epsilon=10^{-4}\), and it is important to note that \(\|\mathbf{x}_{k+1}-\mathbf{x}_{k}\|\leq\epsilon\) is not the termination criterion in our proposed algorithms, as explained in Appendix F.1. Therefore, the larger optimality gaps for the upper-level objective in this case may be attributed to the termination criterion.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{6}{c}{\(\gamma=2\times 10^{4}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 17153 & 7.4052e-03 & 1.3619e-05 & 4.2843e+00 & -5.5818e-01 \\ aPB-APG & 20877 & 7.4052e-03 & 1.3619e-05 & 4.2843e+00 & -5.5818e-01 \\ PB-APG-sc & 27501 & 7.4052e-03 & 1.3619e-05 & 4.2843e+00 & -5.5818e-01 \\ aPB-APG-sc & 40077 & 7.4052e-03 & 1.3619e-05 & 4.2843e+00 & -5.5818e-01 \\ \hline \hline \multicolumn{6}{c}{\(\gamma=5\times 10^{5}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 85511 & 7.3916e-03 & 2.4094e-08 & 4.8198e+00 & -2.2752e-02 \\ aPB-APG & 85502 & 7.3916e-03 & 2.4093e-08 & 4.8198e+00 & -2.2752e-02 \\ PB-APG-sc & 173731 & 7.3916e-03 & 2.4071e-08 & 4.8198e+00 & -2.2740e-02 \\ aPB-APG-sc & 166324 & 7.3916e-03 & 2.4091e-08 & 4.8198e+00 & -2.2751e-02 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Lower- and upper-level objectives and optimal gaps with different penalty parameters for problem (6).

Figure 5: LRP (5) with \(\epsilon=10^{-4}\) (left two subfigures) and \(\epsilon=10^{-7}\) (right two subfigures).

Figure 6: LSRP (6) with \(\epsilon=10^{-4}\) (left two subfigures) and \(\epsilon=10^{-7}\) (right two subfigures).

Tables 3, 6, and 7 demonstrate that the number of iterations for our methods also increases with the solution accuracy, while the optimal gaps of the lower- and upper-level objectives decrease correspondingly. This finding confirms that the number of iterations and the optimal gaps are influenced by the solution accuracy, as illustrated in the expressions for the number of iterations provided by Theorem 3.3 and other related theorems.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{6}{c}{\(\epsilon=10^{-4}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 124 & 3.2794e-01 & 2.8671e-07 & 4.9483e+00 & 6.7024e-03 \\ aPB-APG & 148 & 3.2794e-01 & 2.3660e-07 & 4.9419e+00 & 2.9831e-04 \\ PB-APG-sc & 100 & 3.2794e-01 & 5.6474e-07 & 4.9287e+00 & -1.2956e-02 \\ aPB-APG-sc & 149 & 3.2794e-01 & 7.9015e-07 & 4.9302e+00 & -1.1404e-02 \\ \hline \hline \multicolumn{6}{c}{\(\epsilon=10^{-7}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 841 & 3.2794e-01 & 1.7631e-08 & 4.9382e+00 & -3.3999e-03 \\ aPB-APG & 551 & 3.2794e-01 & 1.7707e-08 & 4.9382e+00 & -3.4075e-03 \\ PB-APG-sc & 225 & 3.2794e-01 & 1.7493e-08 & 4.9383e+00 & -3.3691e-03 \\ aPB-APG-sc & 614 & 3.2794e-01 & 1.7507e-08 & 4.9382e+00 & -3.3874e-03 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Lower- and upper-level objectives and optimal gaps with different solution accuracies for problem (5).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{6}{c}{\(\epsilon=10^{-4}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 426 & 7.6950e-03 & 3.0342e-04 & 6.0249e+01 & 5.5407e+01 \\ aPB-APG & 432 & 7.8018e-03 & 4.1016e-04 & 4.8967e+01 & 4.4125e+01 \\ PB-APG-sc & 437 & 7.6456e-03 & 2.5400e-04 & 6.0196e+01 & 5.5354e+01 \\ aPB-APG-sc & 517 & 7.6143e-03 & 2.2274e-04 & 4.9292e+01 & 4.4449e+01 \\ \hline \hline \multicolumn{6}{c}{\(\epsilon=10^{-7}\)} \\ \hline Method & Total iterations & Lower-level value & Lower-level gap & Upper-level value & Upper-level gap \\ \hline PB-APG & 13707 & 7.3922e-03 & 5.9756e-07 & 4.7279e+00 & -1.1460e-01 \\ aPB-APG & 7803 & 7.3923e-03 & 6.5025e-07 & 4.7300e+00 & -1.1248e-01 \\ PB-APG-sc & 12724 & 7.3922e-03 & 5.7840e-07 & 4.7354e+00 & -1.0714e-01 \\ aPB-APG-sc & 7429 & 7.3922e-03 & 6.3816e-07 & 4.7326e+00 & -1.0992e-01 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Lower- and upper-level objectives and optimal gaps with different solution accuracies for problem (6).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to Abstract and Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to the assumptions adopted in this paper (e.g. Assumptions 2.1, 2.2, and 3.2), our study is based on these assumptions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Please refer to the theorems and the proofs of them in this paper, please refer to Appendix E. For example, Theorem 3.3 and its proof in Appendix E.7. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Please refer to Section 4 and Appendix F and the supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The error bars are not applicable in this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Please refer to Section 4 and Appendix F. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the creators or original owners of assets are properly credited, and the license and terms of use are explicitly mentioned and properly respected, please refer to Section 4 and Appendix F. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Please refer to the supplemental materials and the 'README.m' file. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.