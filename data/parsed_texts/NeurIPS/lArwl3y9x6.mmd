# Normalization Layers Are All That Sharpness-Aware Minimization Needs

 Maximilian Muller

University of Tubingen

and Tubingen AI Center

maximilian.mueller@wisi.uni-tuebingen.de &Tiffany Vlaar

McGill University and

Mila - Quebec AI Institute

tiffany.vlaar@mila.quebec &David Rolnick

McGill University and

Mila - Quebec AI Institute

drolnick@cs.mcgill.ca &Matthias Hein

University of Tubingen

and Tubingen AI Center

matthias.hein@uni-tuebingen.de

###### Abstract

Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (typically comprising 0.1% of the total parameters) in the adversarial step of SAM can outperform perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness.

+
Footnote â€ : Code is provided at https://github.com/mueller-mp/SAM-ON.

## 1 Introduction

Numerous works have been dedicated to studying the potential connection between flatness of minima and generalization performance of deep neural networks [34, 18, 21, 45, 4]. Several aspects of training are thought to affect sharpness, but how these interact with each other remains an ongoing area of research. Recently, sharpness-aware minimization (SAM) has become a popular approach to actively try to find minima with low sharpness using a min-max type algorithm [23]. SAM was found to be remarkably effective in enhancing generalization performance for various settings [23, 14, 7, 1, 33].

Several variants of SAM have been proposed, focusing both on enhanced performance [37, 35] and reduced computational cost [11, 20]. In particular, with the original aim of making SAM more efficient Mi et al. [42] propose a'sparse' SAM approach, which applies SAM only to a select number of parameters. Although they do not actually succeed in reducing wall-clock time, they make the surprising observation that using 50% (in some settings even up to 95%) sparse perturbations can maintain or even enhance performance compared to applying SAM to all parameters. They thus hypothesize that "complete perturbation on all parameters will result in suboptimal minima'.

Similar to the effect of SAM [23, 14], normalization layers are thought to reduce sharpness [41, 46]. Frankle et al. [24] found for ResNets that the trainable affine parameters of the normalization layershave remarkable representation capacity in their own right, whereas disabling them can reduce performance. Inspired by this we focus on the interplay between SAM and normalization layers and show that for various settings perturbing exclusively the normalization layers of a network (often less than 0.1% of the total parameters) outperforms perturbing all parameters. We find that this can not be solely attributed to possible benefits of sparse perturbation approaches and highlight the unique role played by the normalization layer affine parameters. As our main contributions we show that:

* Applying SAM only to the normalization layers of a network (_SAM-ON_, short for _SAM-OnlyNorm_) enhances performance on CIFAR data compared to applying SAM to the full network (_SAM-all_) and also performs competitively on ImageNet. We corroborate the remarkable generalization performance of _SAM-ON_ for ResNet and Vision Transformer architectures, across different SAM variants, and for different batch sizes. _(Section 4)_
* Alternative sparse perturbation approaches do not result in similar performance as _SAM-ON_, especially not at the extreme sparsity levels of our method. _(Section 5.1)_
* Similar to _SAM-all_, _SAM-ON_ yields non-trivial adversarial robustness (Section 4.2). It also reduces the feature-rank, but the sharpness-reducing qualities of _SAM-all_ are not fully preserved (Section 5.2).

## 2 Related Work

**Normalization layers.** Batch Normalization (BatchNorm) [31] and Layer Normalization (LayerNorm) [6] form an essential component of most convolutional [25; 29] and Transformer [51; 19] architectures, respectively. Across various works these normalization layers were shown to accelerate and stabilize training, reducing sensitivity to initialization and learning rate [13; 5; 60; 36]. But despite their widespread adoption and illustrated effectiveness, a conclusive explanation for their success is still elusive. The original motivation for BatchNorm as reducing internal covariance shift [31] has been disputed [46]. The hypothesis that normalization layers enhance smoothness is supported through both empirical and theoretical analyses [46; 10; 41], though also not completely undisputed [57]. Unlike LayerNorm, BatchNorm is sensitive to the choice of batch size [38; 49]. Ghost BatchNorm, where BatchNorm statistics from disjoint subsets of the batch are used, is found to regularize and generally enhance generalization [28; 49] even though it reduces smoothness [17].

**Affine parameters.** There are relatively few papers that study the role of the trainable affine parameters of the normalization layers. Frankle et al. [24] were able to obtain surprisingly high performance on vision data by only training the BatchNorm layers, illustrating the expressive power of the affine parameters, which potentially achieve this by sparsifying activations. For BatchNorm in ResNets, disabling the affine parameters was shown empirically to reduce generalization performance [24], but for LayerNorm in Transformers to not affect or even improve performance [56]. For few-shot transfer tasks disabling the BatchNorm affine parameters during pretraining was found to enhance performance [58]. Further, many other aspects of training will have a non-trivial effect, e.g. applying weight decay to the BatchNorm affine parameters was found to increase performance for ResNets but harm performance in other settings [49].

Figure 1: **The interplay of normalization layers with SAM: Perturbing _only_ normalization layers (OnlyNorm, dashed) improves generalization performance, while omitting them in the perturbation (no-norm, dotted) can harm training. WideResNet-28-10 trained with different SAM-variants on CIFAR-100. Best seen in color.**

**Sharpness-aware minimization.** SAM was developed to try to actively seek out minima with low sharpness [23]. Training with SAM may lead to increased sparsity of active neurons [14] and models which are more compressible [44]. SAM has been shown to be effective in enhancing generalization performance in various settings, but also increases the computational overhead compared to base optimizers [23; 14; 7]. Hence there have been several approaches to try to reduce the computational cost of SAM, such as ESAM which utilizes sharpness-sensitive data selection and perturbs only a randomly selected fraction of parameters [20]. Related work shows that "only employing 20% of the batch to compute the gradients for the ascent step,... [can] result in equivalent performance" [11] and that only applying SAM to part of the parameters (SSAM) using e.g. a Fisher-information mask can lead to enhanced performance [42]. A common variant of SAM utilizes \(m\)-sharpness [23], which uses subbatches of size \(m\) and benefits performance [20; 2; 43] though nuances in its implementation vary [8]. Andriushchenko and Flammarion [2] argue that its success is not unique to settings with BatchNorm and hence cannot be attributed to the Ghost BatchNorm effect. Andriushchenko et al. [3] further show that SAM leads to low-rank features. We discuss different SAM variants in Section 3.2.

## 3 Background: SAM and Normalization Layers

In this paper, we focus on the interplay between two popular aspects of neural network training, both of which we recapitulate here: In Sec. 3.1 we provide an overview of normalization layers, in particular BatchNorm and LayerNorm, and in Sec. 3.2 of Sharpness-Aware Minimization variants.

### BatchNorm and LayerNorm

Modern neural network architectures typically incorporate normalization layers. In this work we will focus on Batch Normalization (BatchNorm) [31] and Layer Normalization (LayerNorm) [6], which are an essential building block of most convolutional [25; 29] and transformer [51; 19] architectures, respectively. Normalization layers transform an input \(\mathbf{x}\) according to

\[N(\mathbf{x})=\gamma\times\frac{\mathbf{x}-\mu}{\sigma}+\beta\] (1)

where \(\mu\) and \(\sigma^{2}\) are the mean and variance, which are computed over the batch dimension in the case of BatchNorm, or over the embedding dimension, in the case of LayerNorm. BatchNorm is therefore sensitive to the choice of batch size [38; 49]. For BatchNorm, \(\mu\) and \(\sigma\) are computed from the current batch-statistics during training, and running estimates are used at test time. In our experiments, we focus on the trainable parameters \(\gamma\) and \(\beta\), which perform an affine transformation of the normalized input.

### SAM and its variants

We recapitulate SAM [23], ASAM [37] and Fisher-SAM [35] with their respective perturbation models. To this end, we consider a neural network \(f_{\mathbf{w}}:\mathbb{R}^{d}\longrightarrow\mathbb{R}^{k}\) which is parameterized by a vector \(\mathbf{w}\) as our model. The training dataset \(S^{train}=\{(\mathbf{x}_{1},\mathbf{y}_{1}),...(\mathbf{x}_{n},\mathbf{y}_{n})\}\) consists of input-output pairs which are drawn from the data distribution \(D\) and we write the loss function as \(l:\mathbb{R}^{k}\times\mathbb{R}^{k}\longrightarrow\mathbb{R}_{+}\). The goal is to learn a model \(f_{\mathbf{w}}\) with good generalization performance, i.e. low expected loss \(L_{D}(\mathbf{w})=\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim D}[l(\mathbf{y},f_{ \mathbf{w}}(\mathbf{x}))]\) on the distribution \(D\). The training loss can be written as \(L(\mathbf{w})=\frac{1}{n}\sum_{i=1}^{n}l(\mathbf{y}_{i},f_{\mathbf{w}}(\mathbf{ x}_{i}))\). Conventional SGD-like optimization methods minimize (a regularized version of) \(L\) by stochastic gradient descent. SAM aims at additionally minimizing the worst-case sharpness of the training loss in a neighborhood defined by an \(\ell_{p}\) ball around \(\mathbf{w}\), i.e. \(\max_{||\epsilon||_{p}\leq\rho}L(\mathbf{w}+\epsilon)-L(\mathbf{w})\). This leads to the overall objective

\[\min_{\mathbf{w}}\max_{||\epsilon||_{p}\leq\rho}L(\mathbf{w}+\epsilon).\] (2)

In practice, SAM uses \(p=2\) and approximates the inner maximization by a single gradient step, yielding \(\epsilon=\rho\nabla L(\mathbf{w})/||\nabla L(\mathbf{w})||_{2}\) and requiring an additional forward-backward pass compared to SGD. The gradient is then re-evaluated at the perturbed point \(\mathbf{w}+\epsilon\), giving the actual weight update

\[\mathbf{w}\longleftarrow\mathbf{w}-\alpha\nabla L(\mathbf{w}+\epsilon)\] (3)

with learning rate \(\alpha\). Computing \(\epsilon\) separately for the batch of each GPU in multi-GPU settings and then averaging the resulting perturbed gradients for the update step in Eq. (3) has been shown toincrease SAM's performance [23]. This method is called \(m\)-sharpness, with \(m\) being the number of samples on each GPU. Since the perturbation model in Eq. (2) is not invariant with respect to a rescaling of the weights that leaves \(f_{\mathbf{w}}\) invariant [18], ASAM [37], a partly scale-invariant version of SAM, was proposed, with the objective

\[\min_{\mathbf{w}}\max_{||T_{w}^{i-1}||_{2}\leq\rho}L(\mathbf{w}+\epsilon)\] (4)

where \(T_{w}\) is a normalization operator, making the perturbation adaptive to the scale of the network parameters. Kwon et al. [37] choose \(T_{w}\) to be diagonal with entries \(T_{w}^{i}=|w_{i}|+\eta\) for weight parameters and \(T_{w}^{i}=1\) for bias parameters, called _elementwise_ normalization. \(\eta\) is typically set to \(0.01\). As with SAM, the inner maximization is solved by a single gradient step:

\[\epsilon_{2}=\rho\frac{T_{w}^{2}\nabla L(\mathbf{w})}{||T_{w}\nabla L( \mathbf{w})||_{2}}\text{ for }p=2,\qquad\quad\epsilon_{\infty}=\rho T_{w}\text{ sign}\big{(}\nabla L(\mathbf{w})\big{)}\text{ for }p=\infty.\] (5)

We note that for \(T_{w}\) equal to the identity matrix and \(p=2\), this is equivalent to the original SAM formulation. Recently, Kim et al. [35] proposed to use a distance metric induced by the Fisher information instead of a Euclidean distance measure between parameters. The approach can also be framed as a variant of ASAM, with \(T_{w}\) being diagonal with entries \(T_{w}^{i}=1/\sqrt{1+\eta f_{i}}\) and \(f_{i}\) approximating the \(i^{th}\) diagonal entry of the Fisher-matrix by the squared average batch-gradient, \(f_{i}=(\partial_{w_{i}}L_{Batch}(\mathbf{w}))^{2}\). For our experiments, we additionally employ layerwise normalization. This is, we set the diagonal entries of \(T_{w}^{i}=||\mathbf{W}_{\text{layer}[i]}||_{2}\), which corresponds to a normalization with respect to the \(\ell_{2}\)-norm of a layer, similar to [39].

## 4 SAM-ON: Perturbing Only the Normalization Layers

We study the effect of applying SAM (and its variants) solely to the normalization layers of a considered model. We will refer to this approach as _SAM-ON_ (SAM-OnlyNorm) throughout this paper and provide a convergence analysis for _SAM-ON_ in Appendix C. We find that _SAM-ON_ obtains enhanced generalization performance compared to conventional SAM (denoted as _SAM-all_) for ResNet architectures with BatchNorm (Section 4.1) and Vision Transformers with LayerNorm (Section 4.2) on CIFAR data and performs competitively on ImageNet. For comparison, we also study the reverse of _SAM-ON_, i.e. we exclude the affine normalization parameters from the adversarial SAM-step, which we shall refer to as _no-norm_.

**Training set-up.** We use SGD with momentum, weight decay, and cosine learning rate decay as our base optimizer for ResNet architectures and employ label smoothing to adopt similar settings as in the literature [37]. For Vision Transformers we employ AdamW [40] as our base optimizer on CIFAR and for ImageNet we additionally use Lion [15]. We use both basic augmentations (random cropping and flipping) and strong augmentations (basic+AutoAugment, denoted as +AA). We consider a range of SAM-variants which differ either in the perturbation model (\(\ell_{2}\) or \(\ell_{\infty}\)) or in the definition of the normalization operator. We train models for 200 epochs, and do not employ \(m\)-sharpness unless indicated otherwise. Complete training details are described in Appendix A.

### BatchNorm and ResNet

**CIFAR.** We showcase the effect of _SAM-ON_, i.e. only applying SAM to the BatchNorm parameters, for a WideResNet-28-10 (WRN-28) on CIFAR-100 in Figure 1. We observe that _SAM-ON_ obtains higher accuracy than conventional SAM (_SAM-all_) for all SAM variants considered (more SAM-variants are shown in Figure 6 in the Appendix). In contrast, excluding the affine BatchNorm parameters from the adversarial step (_no-norm_) either significantly decreases performance (for elementwise-\(\ell_{2}\) variants) or maintains similar performance as _SAM-all_ (for all other variants). For variants which do not experience a performance drop for _no-norm_, the ideal SAM perturbation radius \(\rho\) shifts towards larger values, indicating that the perturbation model cannot perturb the BatchNorm parameters enough when _all_ parameters are used. To study if the benefits of only perturbing the normalization layers extends to other settings, we train more ResNet-like models on CIFAR-10 and CIFAR-100. For each SAM-variant and dataset, we probe a set of pre-defined \(\rho\)-values (shown in Table 7 in the Appendix) with a ResNet-56 (RN-56) and fix the best-performing \(\rho\) for the other models to compare _SAM-ON_ to _SAM-all_. We report mean accuracy and standard deviation over 3 seeds for CIFAR-100 in Table 1. On average, _SAM-ON_ outperforms _SAM-all_ for all considered SAM-variants. Of these, layerwise-\(\ell_{2}\) achieves the highest performance for most settings. We obtain similar results for CIFAR-10 (App. Table 10) and for more network architectures (App. Table 11).

**ImageNet.** For ImageNet, we adopt the timm training script [53]. We train a ResNet-50 for 100 epochs on eight 2080-Ti GPUs with \(m=64\), leading to an overall batch-size of 512. Apart from \(\rho\), all hyperparameters are shared for all SAM-variants and can be found in the Appendix in Table 8. We select the most promising _SAM-ON_ variants and compare them against the established methods (SGD, SAM, ASAM elementwise \(\ell_{2}\)). The results are shown in Table 2. We observe that for layerwise \(\ell_{2}\), the _all_ variant achieves higher accuracy, whereas the _SAM-ON_ models outperform their _all_ counterparts for elementwise \(\ell_{2}\) and elementwise \(\ell_{\infty}\). All _SAM-ON_ variants outperform the previously established methods (SGD, SAM, ASAM). For reference, we also show the values reported for ESAM [20] and GSAM [61], two SAM-variants we did not include in our study.

**Varying the batchsize.** In Figure 4 (right) we report the performance of a WRN-28 on CIFAR-100 with _SAM-ON_ and _SAM-all_ for a range of batch-sizes and values of \(m\), where \(m\) is the batch-size per accelerator, as discussed in Section 3.2. Similar to the findings in [23, 2], we confirm that lower values of \(m\) lead to better performance within each batch-size. Importantly, _SAM-ON_ outperforms _SAM-all_ for all combinations of batch-size and \(m\), illustrating that Ghost BatchNorm [28, 49] (see discussion in Section 2) does not play a role in the success of _SAM-ON_.

### LayerNorm and Vision Transformer

**CIFAR.** To study the effectiveness of _SAM-ON_ beyond ResNet architectures and BatchNorm, we train ViTs from scratch on CIFAR data with AdamW as the base optimizer (Figure 2). Although ResNet architectures are known to outperform Vision Transformers when trained from scratch on small-scale datasets like CIFAR, our aim here is not to outperform state-of-the-art, but rather to study if the benefits of _SAM-ON_ extend to substantially different training settings. Remarkably, we find that the same phenomena occur: The _SAM-ON_ variants outperform their conventional counterparts _SAM-all_ by a clear margin. For the elementwise-\(\ell_{2}\) variants there is a strong drop in accuracy for _no-norm_, whereas for the other SAM variants the optimal perturbation radius \(\rho\) shifts towards larger values. We show that this extends to a ViT-T and CIFAR-10 as well (Table 3).

\begin{table}
\begin{tabular}{|c|l|c|c|c c|c c|} \hline  & & \multicolumn{2}{c|}{RN-56 [25]} & \multicolumn{2}{c|}{RNkT [55]} & \multicolumn{2}{c|}{WRN-28 [59]} \\  & variant & all & ON & all & ON & all & ON \\ \hline \multirow{6}{*}{**ResNet-100**} & SGD & \(72.82^{\pm 0.3}\) & \multicolumn{2}{c|}{\(80.16^{\pm 0.3}\)} & \multicolumn{2}{c|}{\(80.71^{\pm 0.2}\)} \\  & SAM & \(75.07^{\pm 0.6}\) & **75.58\({}^{\pm 0.4}\)** & \(81.79^{\pm 0.4}\) & **82.22\({}^{\pm 0.2}\)** & \(83.11^{\pm 0.3}\) & **84.19\({}^{\pm 0.2}\)** \\  & el. \(\ell_{2}\) & \(75.05^{\pm 0.1}\) & **76.25\({}^{\pm 0.0}\)** & \(81.26^{\pm 0.2}\) & **82.30\({}^{\pm 0.3}\)** & \(82.38^{\pm 0.2}\) & **83.67\({}^{\pm 0.3}\)** \\  & el. \(\ell_{\infty}\) & orig. & \(75.54^{\pm 0.7}\) & **76.07\({}^{\pm 0.2}\)** & **82.15\({}^{\pm 0.3}\)** & \(81.90^{\pm 0.4}\) & **83.67\({}^{\pm 0.1}\)** & 83.53\({}^{\pm 0.2}\) \\  & el. \(\ell_{\infty}\) & \(75.36^{\pm 0.1}\) & **76.10\({}^{\pm 0.2}\)** & \(81.02^{\pm 0.6}\) & **82.38\({}^{\pm 0.3}\)** & \(83.25^{\pm 0.2}\) & **84.14\({}^{\pm 0.2}\)** \\  & Fisher & \(75.01^{\pm 0.4}\) & **75.65\({}^{\pm 0.1}\)** & \(81.55^{\pm 0.2}\) & **82.21\({}^{\pm 0.2}\)** & \(83.37^{\pm 0.1}\) & **84.01\({}^{\pm 0.1}\)** \\  & layer. \(\ell_{2}\) & \(74.63^{\pm 0.1}\) & **76.03\({}^{\pm 0.3}\)** & \(81.66^{\pm 0.2}\) & **82.52\({}^{\pm 0.2}\)** & \(83.23^{\pm 0.2}\) & **84.05\({}^{\pm 0.2}\)** \\ \hline \multirow{6}{*}{**ResNet-200**} & SGD & \(75.26^{\pm 0.2}\) & \multicolumn{2}{c|}{} & \(80.31^{\pm 0.3}\)} & \multicolumn{2}{c|}{} & \(83.62^{\pm 0.1}\) \\  & SAM & **76.33\({}^{\pm 0.3}\)** & \(76.02^{\pm 0.3}\) & \(82.33^{\pm 0.5}\) & **83.19\({}^{\pm 0.2}\)** & \(85.30^{\pm 0.1}\) & **85.42\({}^{\pm 0.1}\)** \\  & el. \(\ell_{2}\) & **76.51\({}^{\pm 0.1}\)** & \(76.04^{\pm 0.3}\) & \(82.00^{\pm 0.3}\) & **83.20\({}^{\pm 0.1}\)** & \(84.80^{\pm 0.3}\) & **85.43\({}^{\pm 0.3}\)** \\  & el. \(\ell_{2}\), orig. & \(76.49^{\pm 0.2}\) & **76.58\({}^{\pm 0.4}\)** & \(82.78^{\pm 0.1}\) & **82.87\({}^{\pm 0.3}\)** & \(85.25^{\pm 0.4}\) & **85.41\({}^{\pm 0.1}\)** \\  & el. \(\ell_{\infty}\) & \(74.89^{\pm 0.4}\) & **76.19\({}^{\pm 0.4}\)** & \(82.33^{\pm 0.1}\) & **83.11\({}^{\pm 0.2}\)** & \(85.28^{\pm 0.1}\) & **85.46\({}^{\pm 0.1}\)** \\  & Fisher & **76.67\({}^{\pm 0.1}\)** & \(76.25^{\pm 0.2}\) & \(82.56^{\pm 0.3}\) & **83.28\({}^{\pm 0.4}\)** & \(85.09^{\pm 0.3}\) & **85.35\({}^{\pm 0.1}\)** \\  & layer. \(\ell_{2}\) & \(76.23^{\pm 0.5}\) & **76.93\({}^{\pm 0.4}\)** & \(82.61^{\pm 0.3}\) & **83.32\({}^{\pm 0.2}\)** & \(85.32^{\pm 0.3}\) & **85.95\({}^{\pm 0.1}\)** \\ \hline \end{tabular}
\end{table}
Table 1: _SAM-ON improves over _SAM-all_ for BatchNorm and ResNets_: Test accuracy for ResNet-like models on CIFAR-100. Bold values mark the better performance between _SAM-ON_ and _SAM-all_ within a SAM-variant, and underline highlights the overall best method per model and augmentation.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline SGD & SAM & ESAMI & GSAM & elem. \(\ell_{2}\) & elem. \(\ell_{\infty}\) & layer \(\ell_{2}\) \\  & all & all & all & ON & all & ON & all & ON \\ \hline \(77.03^{\pm 0.13}\) & \(77.65^{\pm 0.11}\) & \(77.05\) & \(77.20\) & \(77.65^{\pm 0.05}\) & \(\textbf{77.82}^{\pm 0.14}\) & \(77.45^{\pm 0.04}\) & \(\textbf{77.82}^{\pm 0.01}\) & \(\textbf{78.14}^{\pm 0.05}\) & \(77.87^{\pm 0.07}\) \\ \hline \end{tabular}
\end{table}
Table 2: ImageNet top-1 accuracy for a ResNet-50. ESAM [20] and GSAM [61] values are taken from the respective papers.

[MISSING_PAGE_FAIL:6]

### Computational savings

In Figure 3 we report the wall-clock time of training a WRN-28 (left) and a ResNeXt (right) with batchsize 128 on a single A100 with PyTorch. Since the normalization parameters at the earlier layers of the network require a gradient, potentially a full backpropagation pass has to be computed for the ascent-step of _SAM-ON_, even though only a tiny fraction of all parameters is perturbed. However, as discussed in [12] for a related setting, the gradients of the intermediate (no-norm) layers do not need to be stored or used for updating. This leads to computational gains of _SAM-ON_ over _SAM-all_ as reported in Figure 3.

In contrast, although future development of hardware for sparse operation may allow for acceleration, Mi et al. [42] report that their sparse perturbation approach does not at present lead to reduced wall-clock time. Their approach also suffers from additional computational cost associated with selecting the mask, and hence is outshined by _SAM-ON_ both in computational cost and generalization performance (as discussed in Section 5.1).

We also show results for perturbing only the normalization layers of selected blocks (Block 1-3) of the network, and interestingly the main benefits of _SAM-ON_ seem to arise from the later normalization layers, allowing for further computational savings. When perturbing only the normalization layers from the last block (B3), the biggest computational gains can be achieved (reducing the additional cost of SAM by more than 50%), without much loss in test accuracy. When perturbing the normalization layers from Block 2 and 3 (B2+B3), the test accuracy even slightly improves over _SAM-ON_ for both models, while the runtime is still significantly lower. We have not investigated this variant of _SAM-ON_ thoroughly, i.e. in combination with other ASAM perturbation models and more network architectures, but think that this is an interesting research direction for future work.

## 5 Towards Understanding SAM-ON

To gain a better understanding of _SAM-ON_, we study different hypotheses for the method's success. First, we investigate the role of sparsity by comparing _SAM-ON_ to different sparsified perturbation approaches (Section 5.1), concluding that sparsity alone is not enough to explain its success. Then, we highlight that _SAM-ON_ might in fact find _sharper_ minima, while generalizing better than _SAM-all_ (Section 5.2). We also show that _SAM-ON_ - similar to _SAM-all_ - reduces the feature-rank compared to vanilla optimizers (Section 5.2). Further, we showcase that depending on the perturbation method, _SAM-ON_ can induce a significant shift in the distribution of the normalization parameters (Section 5.3) and relate this to the _no-norm_ results from Section 4. Unless stated otherwise, we use a WRN-28 with BatchNorm and the setting described in Section 4 for the ablation studies. Further ablation studies are presented in Appendix B.

Figure 3: **Computational gains of SAM-ON over SAM-all:** Test accuracy vs. normalized wall-clock runtime for SAM and different variations of SAM-ON for a WRN-28 (left) and a ResNeXt (right) on CIFAR-100. Only perturbing selected normalization parameters (e.g. those from block 3, or those from block 2 _and_ 3) can lead to further computational gains. Reported values are averaged over three random seeds.

### The effect of sparsified perturbations

Mi et al. [42] propose a sparsified SAM (_SSAM_) approach which only considers part of the parameters for the SAM perturbation step. They find that using 50% perturbation sparsity _SSAM_ can outperform _SAM-all_, and still perform competitively with up to 99% sparsity in certain settings. This raises the question whether the enhanced performance of _SAM-ON_ over _SAM-all_ is simply due to perturbing fewer parameters. We therefore compare _SAM-ON_ to other sparse perturbation approaches.

In order to determine the parameters which should be perturbed, one solution Mi et al. [42] propose is to compute a binary mask via an approximation of the Fisher matrix. We call this approach _SSAM-F_ to avoid confusion with Fisher-SAM introduced in Section 3.2. Mi et al. [42] also propose a dynamic mask sparse perturbation approach (_SSAM-D_), but do not clearly favour either method. Since they consider _SSAM-F_ "relatively more stable but a bit time-consuming, while _[SSAM-D]_ is more efficient", we will for fair comparison focus on _SSAM-F_ here and provide results for _SSAM-D_ in Appendix B.2. According to Mi et al. [42] neither approach improves wall-clock time in practice, while we find that _SAM-ON_ does (see Section 4.3).

We provide a comparison between _SAM-ON_, _SSAM-F_, and a random sparsity mask of the same sparsity level as _SAM-ON_ for _a)_ ResNet-18 on CIFAR-10 (main setting considered in [42]) in Figure 4 (left) and _b)_ WRN-28 on CIFAR-100 in Table 5. We find that although _SSAM-F_ can indeed perform on par or even outperform _SAM-all_ at the medium to high sparsity levels recommended in [42] it is less successful than _SAM-ON_. Moreover, for the sparsity levels of _SAM-ON_, which are above \(99.9\%\) in both settings, a random mask performs poorly. These results suggest that sparsity is not the sole cause for _SAM-ON_'s success.

### Sharpness and feature-rank of SAM-ON

The improved generalization performance of SAM-trained models is often attributed to finding flatter minima [27; 34] - indeed this was the initial motivation behind SAM in [23]. Andriushchenko and Flammarion [2] however cast doubt on this explanation, and argue that the benefits of SAM might stem from a favorable implicit bias induced by the method. A recent study furthermore found that sharpness often does not correlate well with a model's generalization performance [4]. In this section we therefore compare the sharpness of _SAM-all_ to _SAM-ON_ models (following the setup in [4], more details provided in Appendix B.10).

Figure 4: **Left:** _SAM-ON_ outperforms _SSAM-F_[42] (with different sparsity levels) and random mask _SAM-rand_ (same sparsity level 99.93% as _SAM-ON_) sparse perturbation approaches on CIFAR-10 for ResNet-18. **Right:**_SAM-ON_ improves over SAM for various batch-sizes (bs) and values of \(m\), where smaller values of \(m\) tend to improve performance. WRN-28, CIFAR-100.

\begin{table}
\begin{tabular}{l|l|l|l|l}  & SAM & SAM-ON & Random Mask & \multicolumn{1}{c}{SSAM-F} \\ Sparsity & 0\% & 99.95\% & 99.95\% & 50\% & 99.95\% \\ \hline Test Accuracy (\%) & 83.11\(\pm\)0.3 & **84.19\(\pm\)**0.2 & 80.97\(\pm\)0.2 & 83.94\(\pm\)0.1 & 83.14\(\pm\)0.1 \\ \end{tabular}
\end{table}
Table 5: **SAM-ON outperforms other sparse perturbation approaches:** Although _SSAM-F_[42] with different sparsity levels can outperform _SAM-all_ on CIFAR-100 with WRN-28, it is less effective than _SAM-ON_, especially when probed at very high sparsity levels.

[MISSING_PAGE_FAIL:9]

## 6 Discussion and Conclusion

In recent years the method of sharpness-aware minimization (SAM) [23] has risen to prominence due to its demonstrated effectiveness and the community's long-standing interest in flat minima. In this work we show that only applying SAM to the normalization layers (_SAM-ON_) - typically less than \(0.1\%\) of the total parameters - can significantly enhance performance compared to regular SAM (_SAM-all_). We show results on CIFAR and ImageNet for ResNet and Vision Transformers with BatchNorm and Layernorm, respectively. Although the use of sparsified perturbations was recently shown to benefit generalization [42], we show that the success of _SAM-ON_ cannot be attributed to sparsity alone: targeting the normalization layers clearly improves over other masked sparsity approaches.

We find that while _SAM-ON_ outperforms _SAM-all_ in almost all settings, the optimal SAM variant to use varies. We do not see a consistent benefit of using reparameterization-invariant perturbation models compared to variants with fewer invariances. In particular, we find that layerwise \(\ell_{2}\) in combination with _SAM-ON_ reaches the highest accuracy for many settings.

While _SAM-ON_ improves generalization, it loses some of SAM's sharpness-reducing qualities. Although perhaps surprising given SAM's original motivation, this finding relates naturally to the literature. [4] find that sharpness does not always correlate well with generalization performance. Further, [2] question if sharpness is the sole driving factor behind SAM's success in enhancing generalization. We lend support to this question, showing that SAM-like methods can generalize better, without significant sharpness reduction.

In summary, we demonstrate benefits of SAM beyond reducing sharpness and highlight the special role played by the normalization layers. More investigation into the interplay of SAM and other aspects of training are needed to fully understand where the methods gains come from.

**Limitations.** Similar to the main inspirations for this work [23; 24; 42] we focus on vision data. We provide a simple experiment in the language domain in Appendix B.5 indicating that the efficacy of _SAM-ON_ might be preserved for language tasks, but more extensive benchmarking, as done by [7] is required. Further, more work is required to try to leverage the perturbation sparsity for reduced computational cost beyond the gains obtained in this work, and to fully explore the benefits of perturbing subsets of the normalization layers building on the findings in Section 4.3.

## Acknowledgements

We acknowledge support from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy (EXC number 2064/1, Project number 390727645), as well as from the Carl Zeiss Foundation in the project "Certification and Foundations of Safe Machine Learning Systems in Healthcare". We also thank the European Laboratory for Learning and Intelligent Systems (ELLIS) for supporting Maximilian Muller. We are grateful for support from the Canada CIFAR AI Chairs Program and US National Science Foundation award tel:1910864. In addition, we acknowledge material support from NVIDIA and Intel in the form of computational resources and are grateful for technical support from the Mila IDT team in maintaining the Mila Compute Cluster.

Figure 5: _SAM-ON_ changes the weight distribution of the normalization layer weights of a WRN-28 towards larger values. More SAM-variants are shown in Figure 9 in the appendix.

## References

* [1]M. Abbas, Q. Xiao, L. Chen, P. Chen, and T. Chen (2022) Sharp-MAML: sharpness-aware model-agnostic meta learning. ICML. Cited by: SS1.
* [2]M. Andriushchenko, D. Bahri, H. Mobahi, and N. Flammarion (2023) Sharpness-aware minimization leads to low-rank features. NeurIPS. Cited by: SS1.
* [3]M. Andriushchenko, F. Croce, M. Muller, M. Hein, and N. Flammarion (2023) A modern look at the relationship between sharpness and generalization. preprint arXiv:2302.07011. Cited by: SS1.
* [4]S. Arora, Z. Li, and K. Lyu (2019) Theoretical analysis of auto rate-tuning by Batch Normalization. ICLR. Cited by: SS1.
* [5]J. L. Ba, J. R. Kiros, and G. E. Hinton (2016) Layer Normalization. Advances in NeurIPS Deep Learning Symposium. Cited by: SS1.
* [6]D. Bahri, H. Mobahi, and Y. Tay (2022) Sharpness-aware minimization improves language model generalization. ACL. Cited by: SS1.
* [7]K. Behdin, Q. Song, A. Gupta, A. Acharya, D. Durfee, B. Ocejo, S. Keerthi, and R. Mazumder (2023) mSAM: micro-batch-averaged sharpness-aware minimization. preprint arXiv:2302.09693. Cited by: SS1.
* [8]P. Benz, C. Zhang, and I. S. Kweon (2021) Batch normalization increases adversarial vulnerability and decreases adversarial transferability: a non-robust feature perspective. ICCV. Cited by: SS1.
* [9]J. Bjorck, C. Gomes, B. Selman, and K. Q. Weinberger (2018) Understanding batch normalization. NeurIPS. Cited by: SS1.
* [10]A. Brock, S. De, S. L. Smith, and K. Simonyan (2021) High-performance large-scale image recognition without normalization. ICML. Cited by: SS1.
* [11]B. Chen, P. Li, B. Li, C. Lin, C. Li, M. Sun, J. Yan, and W. Ouyang (2021) BN-NAS: neural architecture search with Batch Normalization. ICCV. Cited by: SS1.
* [12]M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L. Jones, M. Schuster, N. Shazeer, N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, Z. Chen, Y. Wu, and M. Hughes (2018) The best of both worlds: combining recent advances in neural machine translation. ACL. Cited by: SS1.
* [13]X. Chen, C. Hsieh, and B. Gong (2022) When Vision Transformers outperform ResNets without pre-training or strong data augmentations. ICLR. Cited by: SS1.
* [14]X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong, C. Hsieh, Y. Lu, and Q. V. Le (2023) Symbolic discovery of optimization algorithms. arXiv:2302.06675. Cited by: SS1.
* [15]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML. Cited by: SS1.
* [16]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML. Cited by: SS1.
* [17]N. Dimitriou and O. Arandjelovic (2020) A new look at ghost normalization. preprint arXiv: 2007.08554. Cited by: SS1.
* [18]L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio (2017) Sharp minima can generalize for deep nets. ICML. Cited by: SS1.
* [19]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [20]J. Du, H. Yan, J. Feng, J. T. Zhou, L. Zhen, R. S. M. Goh, and V. Tan (2022) Efficient sharpness-aware minimization for improved training of neural networks. ICLR. Cited by: SS1.
* [21]G. K. Dziugaite and D. M. Roy (2017) Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. Uncertainty in AI. Cited by: SS1.
* [22]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [23]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [24]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [25]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [26]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [27]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [28]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [29]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [30]G. K. Dziugaite, A. Drouin, B. Neal, N. Rajkumar, E. Caballero, L. Wang, I. Mitliagkas, and D. M. Roy (2020) In search of robust measures of generalization. NeurIPS. Cited by: SS1.
* [31]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [32]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [33]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [34]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [35]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [36]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [37]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [38]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [39]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [40]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [41]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [42]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
* [43]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. ICLR. Cited by: SS1.
** [23] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. _ICLR_, 2021.
* [24] J. Frankle, D. J. Schwab, and A. S. Morcos. Training BatchNorm and only BatchNorm: On the expressive power of random features in CNNs. _ICLR_, 2021.
* [25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. _CVPR_, 2016.
* [26] S. Hochreiter and J. Schmidhuber. Simplifying neural nets by discovering flat minima. _Advances in Neural Information Processing Systems_, 1994.
* [27] S. Hochreiter and J. Schmidhuber. Flat minima. _Neural Computation_, 9(1):1-42, 1997.
* [28] E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. _NeurIPS_, 2017.
* [29] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. _CVPR_, 2017.
* [30] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* [31] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating deep network training by reducing internal covariate shift. _ICML_, 2015.
* [32] Y. Jiang*, B. Neyshabur*, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. _ICLR_, 2020.
* [33] J. Kaddour, L. Liu, R. Silva, and M. J. Kusner. When do flat minima optimizers work? _NeurIPS_, 2022.
* [34] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. _ICLR_, 2017.
* [35] M. Kim, D. Li, S. X. Hu, and T. Hospedales. Fisher SAM: Information geometry and sharpness aware minimisation. _ICML_, 2022.
* [36] J. Kohler, H. Daneshmand, A. Lucchi, M. Zhou, K. Neymeyr, and T. Hofmann. Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization. _AISTATS_, 2019.
* [37] J. Kwon, J. Kim, H. Park, and I. K. Choi. ASAM: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. _ICML_, 2021.
* [38] X. Lian and J. Liu. Revisit Batch Normalization: New understanding and refinement via composition optimization. _AISTATS_, 2019.
* [39] Y. Liu, S. Mai, X. Chen, C.-J. Hsieh, and Y. You. Towards efficient and scalable sharpness-aware minimization. _CVPR_, 2022.
* [40] I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.
* [41] K. Lyu, Z. Li, and S. Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. _NeurIPS_, 2022.
* [42] P. Mi, L. Shen, T. Ren, Y. Zhou, X. Sun, R. Ji, and D. Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. _NeurIPS_, 2022.
* [43] T. Mollenhoff and M. E. Khan. SAM as an optimal relaxation of Bayes. _ICLR_, 2023.
* [44] C. Na, S. V. Mehta, and E. Strubell. Train flat, then compress: Sharpness-aware minimization learns more compressible models. _EMNLP_, 2022.
* [45] H. Petzka, M. Kamp, L. Adilova, C. Sminchisescu, and M. Boley. Relative flatness and generalization. _NeurIPS_, 2021.
* [46] S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How does batch normalization help optimization? _NeurIPS_, 2018.
* [47] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition, 2015.
* [48] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L. Beyer. How to train your ViT? Data, augmentation, and regularization in Vision Transformers. _TMLR_, 2022.

* [49] C. Summers and M. J. Dinneen. Four things everyone should know to improve batch normalization. _ICLR_, 2020.
* [50] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. _ICLR_, 2014.
* [51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _NeurIPS_, 2017.
* [52] Z. Wei, J. Zhu, and Y. Zhang. Sharpness-aware minimization alone can improve adversarial robustness. _ICML AdvML-Frontiers Workshop, arXiv:2305.05392_, 2023.
* [53] R. Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.
* [54] C. Xie and A. Yuille. Intriguing properties of adversarial training at scale. _ICLR_, 2020.
* [55] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. _CVPR_, 2017.
* [56] J. Xu, X. Sun, Z. Zhang, G. Zhao, and J. Lin. Understanding and improving layer normalization. _NeurIPS_, 2019.
* [57] Z. Yao, A. Gholami, K. Keutzer, and M. Mahoney. PyHessian: Neural networks through the lens of the Hessian. _IEEE BigData, arXiv:1912.07145_, 2020.
* [58] M. Yazdanpanah, A. A. Rahman, M. Chaudhary, C. Desrosiers, M. Havaei, E. Belilovsky, and S. E. Kahou. Revisiting learnable affines for Batch Norm in few-shot transfer learning. _CVPR_, 2022.
* [59] S. Zagoruyko and N. Komodakis. Wide residual networks. In _BMVC_, 2016.
* [60] H. Zhang, Y. N. Dauphin, and T. Ma. Fixup initialization: Residual learning without normalization. _ICLR_, 2019.
* [61] J. Zhuang, B. Gong, L. Yuan, Y. Cui, H. Adam, N. Dvornek, S. Tatikonda, J. Duncan, and T. Liu. Surrogate gap minimization improves sharpness-aware training. _ICLR_, 2022.

## Appendix

This appendix is structured as follows:

* In Appendix A we provide more training details. In particular, we report the hyperparameters used for the CIFAR experiments in A.1 and for the ImageNet experiments in A.2. In A.3 we provide more details and a formal definition of the SAM-variants used throughout this paper.
* In Appendix B we show additional experimental results for: CIFAR in B.1, ImageNet in B.3, and a machine translation task in B.5. In B.2 we provide additional ablation studies for sparse perturbation _SSAM_ approaches and in B.4 we extend the discussion on adversarial robustness. To gain a better understanding of SAM-ON, we further investigate: the weight distribution shift induced by _SAM-ON_ (B.6), the effect of SAM when fixing the normalization parameters during training (B.7), SAM's performance when only training the normalization layers (B.8), and ablations on weight decay and dropout (B.9). Finally, we provide an extended discussion on the sharpness evaluation and more ablations in B.10.
* In Appendix C we provide a convergence analysis for _SAM-ON_.

## Appendix A Training Details

### CIFAR training details

For our CIFAR experiments, we consider a range of SAM-variants which differ either in the norm (\(p\in\{2,\infty\}\)) or in the definition of the normalization operator. We use SGD, the original SAM with no normalization and \(p=2\), Fisher-SAM and the following ASAM-variants: elementwise-\(\ell_{\infty}\), layerwise-\(\ell_{2}\), and elementwise-\(\ell_{2}\). For the ViT-experiments, we use AdamW instead of SGD. For each of the ASAM-variants, we normalize both bias and weight parameters and set \(\eta=0\). Additionally, we employ the original ASAM-algorithm, where the bias parameters are not normalized and \(\eta=0.01\). We train all models on a single GPU for 200 epochs, and m-sharpness is not employed (unless indicated otherwise). For ResNets, we follow [37] and adopt a learning rate of 0.1, momentum of 0.9, weight decay of \(0.0005\) and use label smoothing with a factor of 0.1. We use both basic augmentations (random cropping and flipping) and strong augmentations (basic+AutoAugment). For ViTs we use AdamW with learning rate \(0.0001\), batchsize \(64\) and only strong augmentations, the other settings remain unchanged. The ResNet results were computed on 2080ti-GPUs and the ViT results on A100s. The values of \(\rho\) we considered for each method can be found in Table 7. The ResNet-networks we considered for the CIFAR-experiments in the main paper are ResNet56 (RN56) [25], ResNeXt-29-32x4d (RNxT) [55], and WideResNet-28-10 (WRN) [59]. We adopted the ViTs to CIFAR by setting the image-size to 32 and patch-size to 4.

\begin{table}
\begin{tabular}{l l|c|c|c}  & \multicolumn{2}{c|}{CIFAR-10 RN} & \multicolumn{2}{c|}{CIFAR-100 RN} & \multicolumn{2}{c}{CIFAR-10/100 ViT} \\ \hline SAM & all & 0.05, **0.1**, 0.25 & 0.05, **0.1**, 0.5, 1. & 0.025, 0.05, **0.1**, 0.25, 0.5 \\ SAM & ON & 0.1, **0.5**, 1 & 0.1, 0.5, 1, 5. & 1, 2.5, 5, **10, 25** \\ el. \(l_{2}\) & all & 0.5, 1, **2**, 3, 5 & 0.5, **1, 2.5**, 5, 10. & 0.5, 1, **2.5**, 5, 10 \\ el. \(l_{2}\) & ON & 0.5, 1, 2, 3, 5 & 0.5, 1, **2.5**, 5, 10. & 1, 2.5, 5, **10, 25** \\ el. \(l_{2}\), orig. & all & 0.1, **0.5**, 1, 5, 10 & 0.5, **1**, 2.5, 5 & 0.5, 1, **2.5**, 5, 10 \\ el. \(l_{2}\), orig. & ON & 0.1, **0.5**, 1, 5, 10 & 0.5, **1**, 2.5, 5 & 1, 2.5, 5, **10** \\ el. \(l_{\infty}\) & all & 0.001, **0.005**, 0.01, 0.05 & 0.001, 0.05 & 0.0005, 0.001, **0.0025**, 0.005, 0.01 \\ el. \(l_{\infty}\) & ON & 0.01, **0.025**, 0.05, 0.1 & 0.01, **0.05**, 0.1, 0.5 & 0.025, 0.05, **0.1**, 0.25, 0.5 \\ layer \(l_{2}\) & all & 0.005, 0.01, **0.025**, 0.05, 0.1 & 0.001, **0.01**, 0.05, 0.1 & 0.001, 0.0025, **0.005**, 0.01, 0.025 \\ layer \(l_{2}\) & ON & 0.05, 0.1, **0.25**, 0.5, 1 & 0.1, **0.2**, 0.5, 1. & 0.05, 0.1, 0.25, **0.5**, 1. \\ Fisher & all & 0.05, **0.1**, 0.5, 1, 5 & 0.05, **0.1**, 0.5, 1 & 0.05, **0.1**, 0.5, 1, 5 \\ Fisher & ON & 0.1, **0.5**, 1, 5, 10 & 0.1, 0.5, **1**, 5, 10 & 0.1, 0.5, 1, 5, **10** \\ \end{tabular}
\end{table}
Table 7: Search-space for \(\rho\). The values used for the the experiments in Tables 1,3 and 10 are marked in bold.

### ImageNet training details

Table 8 shows the hyperparameters for all variants used for ImageNet training. For the ResNet-50 with SGD, SAM and elementwise-\(\ell_{2}\) we used the hyperparameters from [23] and [37]. For the layerwise \(\ell_{2}\) and elementwise-\(\ell_{\infty}\) we tried two \(\rho\)-values per configuration and report the results of the better one (named \(\rho\) (reported) in the table). \(\rho\) (discarded) refers to the \(\rho\) value we probed, but found to perform worse than the other one. For the ViT-S (additional fine-tuning experiments in Appendix B.3), we tried at least three values of \(\rho\) per SAM-configuration and reported the best one.

\begin{table}
\begin{tabular}{|c c c c c c c c|} \hline param & SGD & SAM & elem. \(\ell_{2}\) & ResNet-50 & elem. \(\ell_{\infty}\) & layer \(\ell_{2}\) \\  & all & all & onlyNorm & & all & onlyNorm & all & onlyNorm \\ \hline train epochs & & & & & 90 & & & \\ warm-up epochs & & & & 3 & & & & \\ cool-down epochs & & & & 10 & & & & \\ batch-size & & & & 512 & & & & \\ augmentation & & & & & inception-style & & & \\ lr decay & & & & & 0.2 & & & \\ lr decay & & & & & Cosine & & & \\ weight decay & & & & 0.0001 & & & & \\ \(\rho\) (reported) & 0.05 & 1 & 1 & & 0.001 & 0.005 & 0.005 & 0.05 \\ \(\rho\) (discarded) & & & & & 0.01 & 0.05 & 0.05 & 0.5 \\ Input Resolution & & & & & \(224\times 224\) & & & \\  & & & & & \(64\) & & & \\ GPU Type & & & & & \(8\times\)2080-ii & & & \\ \hline \hline param & AdamW & AdamW+SAM & ViT-S scratch & Lion & Lion+SAM & & & \\  & all & all & onlyNorm & & all & all & onlyNorm \\ \hline train epochs & & & & & 300 & & & \\ warm-up epochs & & & & 10 & & & \\ cool-down epochs & & & & & 0 & & \\ batch-size & & & & & 128 & & & \\ augmentation & & & & & inception-style & & & \\ lr & & & & & 0.001 & & & \\ lr decay & & & & & & Cosine & & \\ weight decay & & & & & 0.1 & & & \\ \(\rho\) (reported) & â€“ & 1 & 15 & & & â€“ & 1 & 10 \\ \(\rho\) (discarded) & â€“ & 0.05,0.1,0.5,2 & 10,20 & & & â€“ & 0.5,2 & 5,20 \\ Input Resolution & & & & & \(224\times 224\) & & & \\  & & & & & 128 & & & \\ GPU Type & & & & & \(1\times\)A100 & & & \\ \hline \end{tabular}
\end{table}
Table 8: Hyperparameters for training on ImageNet. Top: ResNet-50 from scratch, center: ViT-S from scratch, bottom: finetuning the ViT-S.

### SAM variants

Here, we provide a more comprehensive overview of the SAM-variants used throughout the experiments. To this end, we first recall the definition of the (A)SAM-perturbation (Eq. (5) in the main paper):

\[\epsilon_{2}=\rho\frac{T_{w}^{2}\nabla L(\mathbf{w})}{||T_{w}\nabla L(\mathbf{w} )||_{2}}\text{ for }p=2,\qquad\qquad\epsilon_{\infty}=\rho T_{w}\text{sign}\big{(} \nabla L(\mathbf{w})\big{)}\text{ for }p=\infty.\]

with the normalization operator \(T_{w}^{i}\), which is diagonal for all variants. We note that _SAM-ON_ can be formally defined as using the conventional (A)SAM-algorithm but setting all entries \(T_{w}^{i}=0\) if \(w_{i}\) is not a normalization parameter. This leads to a change of the perturbation \(\epsilon\) according to Eq. (5). Importantly, the magnitude of \(\epsilon\) is still \(\rho\), since both the nominator and the denominator of Eq. (5) change. We provide an overview over all (A)SAM-variants and their respective perturbation models in Table 9.

## Appendix B Further Experimental Results

### SAM-ON on CIFAR

We omitted the results for ResNet-like models on CIFAR-10 in the main paper. Those are thus reported in Table 10. Due to the already very high accuracies, the differences between _SAM-ON_ and _SAM-all_ are smaller, yet on average _SAM-ON_ is still clearly the better method. We further plot all considered SAM-variants for different values of \(\rho\) in Figure 6 for a WRN-28 and in Figure 7 for a ViT-S on CIFAR-100. We show results for various VGG-models [47] and DenseNet-100 [30] for CIFAR-10/100 in Table 11 and observe that _SAM-ON_ consistently improves over _SAM-all_.

### Additional ablation studies for sparse SAM

In this section we provide additional ablation studies for sparsified perturbation approaches as discussed in Section 5.1. Mi et al. [42] proposed two sparsified SAM (_SSAM_) approaches: Fisher SSAM (_SSAM-F_) and Dynamic SSAM (_SSAM-D_). As an extension to Figure 4 for ResNet-18 on CIFAR-10 data in the main paper we provide an accompanying Figure 8 which includes error bars

\begin{table}
\begin{tabular}{l l|l l l} variant & & \(T_{w}^{i}\) & \(p\) & \(\eta\) \\ \hline SAM & all & 1 & 2 & 0 \\  & ON & \(\left\{\begin{array}{ll}1\\ 0\end{array}\right.\) & if \(w_{i}\) is a normalization parameter & 2 & 0 \\ \hline el. \(\ell_{2}\) & all & \(|w_{i}|\) & 2 & 0 \\  & ON & \(\left\{\begin{array}{ll}|w_{i}|\\ 0\end{array}\right.\) & if \(w_{i}\) is a normalization parameter & 2 & 0 \\ \hline el. \(\ell_{2}\), orig. & all & \(\left\{\begin{array}{ll}|w_{i}|+\eta\\ 1+\eta\end{array}\right.\) & if \(w_{i}\) is a weight parameter & 2 & 0.01 \\  & ON & \(\left\{\begin{array}{ll}|w_{i}|+\eta\\ 1+\eta\end{array}\right.\) & if \(w_{i}\) is a normalization weight & & \\  & ON & \(\left\{\begin{array}{ll}|w_{i}|+\eta\\ 1+\eta\end{array}\right.\) & if \(w_{i}\) is a normalization bias & 2 & 0.01 \\  & 0 & else & & \\ \hline el. \(\ell_{\infty}\) & all & \(|w_{i}|\) & \(\infty\) & 0 \\  & ON & \(\left\{\begin{array}{ll}|w_{i}|\\ 0\end{array}\right.\) & if \(w_{i}\) is a normalization parameter & \(\infty\) & 0 \\ \hline layer \(\ell_{2}\) & all & \(\|\mathbf{W_{layer(i)}}\|_{2}\) & 2 & 0 \\  & ON & \(\left\{\begin{array}{ll}\|\mathbf{W_{layer(i)}}\|_{2}\\ 0\end{array}\right.\) & if \(w_{i}\) is a normalization parameter & 2 & 0 \\ \hline Fisher & all & \(\left(1+\eta\left(\partial_{w_{i}}L_{Batch}(\mathbf{w})\right)^{2}\right)^{-0. 5}\) & 2 & 1 \\  & ON & \(\left\{\begin{array}{ll}\left(1+\eta\left(\partial_{w_{i}}L_{Batch}(\mathbf{ w})\right)^{2}\right)^{-0.5}&\text{ if }w_{i}\text{ is a normalization parameter }\\ 0&\text{ else }\end{array}\right.\) & 2 & 1 \\ \end{tabular}
\end{table}
Table 9: The definition of \(T_{w}^{i}\) for the considered SAM-variants.

and comparisons with the dynamic sparse perturbation approach (_SSAM-D_) [42]. We also provide additional results for _SSAM-D_ for a WideResNet-28 on CIFAR-100 data in Table 12. We found optimal performance for _SSAM_ for 50% sparsity and \(\rho=0.1\) on CIFAR-10 and \(\rho=0.2\) on CIFAR-100 (as also observed in [42] for slightly different training settings). We find that although both _SSAM_ approaches can perform on par or even outperform regular _SAM_, they are less effective than our _SAM-ON_ approach. The generalization gap increases even further when considering the same high sparsity levels as for _SAM-ON_.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|} \hline  & SAM variant & \multicolumn{2}{c|}{RN-56} & \multicolumn{2}{c|}{RNxT} & \multicolumn{2}{c|}{WRN-28} \\  & & all & onlyNorm & all & onlyNorm & all & onlyNorm \\ \hline \multirow{9}{*}{**SSAM-D**} & SGD & \(94.28^{\pm 0.2}\) & \(95.37^{\pm 0.1}\) & & \(96.20^{\pm 0.1}\) & \\  & SAM & \(94.94^{\pm 0.1}\) & \(\textbf{95.18}^{\pm 0.1}\) & \(96.35^{\pm 0.2}\) & \(\textbf{96.48}^{\pm 0.1}\) & \(97.08^{\pm 0.1}\) & \(\textbf{97.10}^{\pm 0.0}\) \\  & elem. \(\ell_{2}\) & \(\textbf{94.96}^{\pm 0.1}\) & \(94.94^{\pm 0.2}\) & \(96.41^{\pm 0.1}\) & \(\textbf{96.53}^{\pm 0.1}\) & \(96.98^{\pm 0.2}\) & \(\textbf{97.06}^{\pm 0.0}\) \\  & elem. \(\ell_{2}\), orig. & \(95.14^{\pm 0.1}\) & \(\textbf{95.21}^{\pm 0.1}\) & \(96.40^{\pm 0.1}\) & \(\textbf{96.41}^{\pm 0.1}\) & \(\textbf{97.10}^{\pm 0.1}\) & \(97.07^{\pm 0.1}\) \\  & elem. \(\ell_{\infty}\) & \(94.93^{\pm 0.1}\) & \(\textbf{94.96}^{\pm 0.0}\) & \(96.06^{\pm 0.2}\) & \(\textbf{96.22}^{\pm 0.1}\) & \(96.95^{\pm 0.2}\) & \(\textbf{97.00}^{\pm 0.1}\) \\  & Fisher & \(95.01^{\pm 0.1}\) & \(\textbf{95.03}^{\pm 0.1}\) & \(96.31^{\pm 0.0}\) & \(\textbf{96.55}^{\pm 0.0}\) & \(96.95^{\pm 0.0}\) & \(\textbf{97.13}^{\pm 0.1}\) \\  & layer. \(\ell_{2}\) & \(94.95^{\pm 0.2}\) & \(\textbf{95.07}^{\pm 0.1}\) & \(96.07^{\pm 0.3}\) & \(\textbf{96.46}^{\pm 0.1}\) & \(\textbf{97.02}^{\pm 0.0}\) & \(96.96^{\pm 0.1}\) \\ \hline \multirow{9}{*}{**SSAM-ON**} & SGD & \(94.70^{\pm 0.1}\) & \(96.19^{\pm 0.2}\) & & \(97.01^{\pm 0.0}\) & \\  & SAM & \(95.25^{\pm 0.1}\) & \(\textbf{95.40}^{\pm 0.1}\) & \(96.98^{\pm 0.1}\) & \(\textbf{97.22}^{\pm 0.3}\) & \(97.57^{\pm 0.1}\) & \(\textbf{97.58}^{\pm 0.0}\) \\  & elem. \(\ell_{2}\) & \(\textbf{95.12}^{\pm 0.0}\) & \(94.82^{\pm 0.2}\) & \(97.01^{\pm 0.0}\) & \(\textbf{97.21}^{\pm 0.1}\) & \(97.61^{\pm 0.0}\) & \(\textbf{97.69}^{\pm 0.0}\) \\  & elem. \(\ell_{2}\), orig. & \(95.39^{\pm 0.1}\) & \(\textbf{95.60}^{\pm 0.1}\) & \(97.24^{\pm 0.0}\) & \(\textbf{97.33}^{\pm 0.1}\) & \(\textbf{97.60}^{\pm 0.0}\) & \(97.56^{\pm 0.0}\) \\  & elem. \(\ell_{\infty}\) & \(95.12^{\pm 0.1}\) & \(\textbf{95.48}^{\pm 0.3}\) & \(96.70^{\pm 0.2}\) & \(\textbf{96.91}^{\pm 0.2}\) & \(97.52^{\pm 0.1}\) & \(\textbf{97.62}^{\pm 0.1}\) \\  & Fisher & \(95.19^{\pm 0.0}\) & \(\textbf{95.38}^{\pm 0.1}\) & \(96.77^{\pm 0.0}\) & \(\textbf{97.24}^{\pm 0.1}\) & \(97.53^{\pm 0.0}\) & \(\textbf{97.65}^{\pm 0.1}\) \\  & layer. \(\ell_{2}\) & \(\textbf{95.43}^{\pm 0.3}\) & \(95.28^{\pm 0.1}\) & \(96.80^{\pm 0.1}\) & \(\textbf{96.88}^{\pm 0.1}\) & \(\textbf{97.60}^{\pm 0.0}\) & \(97.48^{\pm 0.1}\) \\ \hline \end{tabular}
\end{table}
Table 10: _SAM-ON improves over SAM-all_ for BatchNorm and freeNets on CIFAR-10: Test accuracy for ResNet-like models on CIFAR-10. Bold values mark the better performance between SAM-ON and SAM-all within a SAM-variant, and underline highlights the overall best method per model and augmentation_

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|l|} \hline  & SAM variant & \multicolumn{2}{c|}{VGG-13} & \multicolumn{2}{c|}{VGG-16} & \multicolumn{2}{c|}{VGG-19} & \multicolumn{2}{c|}{DenseNet-100} \\  & SAM variant & all & onlyNorm & all & onlyNorm & all & onlyNorm & all & onlyNorm \\ \hline \multirow{9}{*}{**SSAM-D**} & SGD & \(75.44^{\pm 0.2}\) & \(\textbf{77.57}^{\pm 0.1}\) & \(75.81^{\pm 0.2}\) & \(\textbf{76.86}^{\pm 0.1}\) & \(74.08^{\pm 0.6}\) & \(\textbf{75.60}^{\pm 0.1}\) & \(79.42^{\pm 0.6}\) & \(\textbf{79.90}^{\pm 0.3}\) \\  & elem. \(\ell_{2}\) & \(76.65^{\pm 0.1}\) & \(\textbf{77.49}^{\pm 0.1}\) & \(75.95^{\pm 0.2}\) & \(\textbf{76.45}^{\pm 0.2}\) & \(74.72^{\pm 0.2}\) & \(\textbf{75.12}^{\pm 0.1}\) & \(78.90^{\pm 0.2}\) & \(\textbf{79.83}^{\pm 0.3}\) \\  & elem. \(\ell_{2}\), \(\eta=0.01\) & \(77.27^{\pm 0.2}\) & \(\textbf{77.73}^{\pm 0.2}\) & \(76.65^{\pm 0.1}\) & \(\textbf{76.66}^{\pm 0.0}\) & \(75.00^{\pm 0.5}\) & \(\textbf{75.44}^{\pm 0.2}\) & \(79.94^{\pm 0.4}\) & \(\textbf{80.14}^{\pm 0.1}\) \\  & elem. \(\ell_{\infty}\) & \(76.82^{\pm 0.2}\) & \(\textbf{77.62}^{\pm 0.2}\) & \(75.43^{\pm 0.4}\) & \(\textbf{76.68}^{\pm 0.1}\) & \(72.74^{\pm 0.2}\) & \(\textbf{74.50}^{\pm 0.4}\) & \(\textbf{79.74}^{\pm 0.3}\) & \(\textbf{79.64}^{\pm 0.2}\) \\  & Fisher, \(\eta=1\), & \(76.76^{\pm 0.2}\) & \(\textbf{77.68}^{\pm 0.4}\) & \(75.85^{\pm 0.2}\) & \(\textbf{76.99}^{\pm 0.1}\) & \(74.03^{\pm 0.2}\) & \(\textbf{74.96}^{\pm 0.3}\) & \(79.68^{\pm 0.2}\) & \(

### Finetuning from ImageNet-21k

Since ViTs are commonly trained on large-scale datasets and then fine-tuned, we investigate this scenario for _SAM-ON_. In particular, we consider a ViT-S pretrained on ImageNet-21k from [48]. We fine-tune for 9 epochs with SGD for a range of SAM-variants with their respective _SAM-ON_ counterpart. For each setup, we probe three values of \(\rho\) and report the best result in Table 13. We find in this setting that _SAM-ON_ performs on par with _SAM-all_ although there are small differences across SAM variants: for layerwise-\(\ell_{2}\)_SAM-ON_ performs slightly worse, whereas for all other variants _SAM-ON_ performs equally well or slightly better than _SAM-all_. In all cases _SAM-ON_ outperforms plain SGD.

Figure 8: _SAM-ON_ outperforms _SSAM-F_ and _SSAM-D_[42] (with different sparsity levels) and random mask _SAM-rand_ (same sparsity level 99.93% as _SAM-ON_) sparse perturbation approaches on CIFAR-10 for ResNet-18.

Figure 6: All considered SAM-variants and their _SAM-ON_ counterpart for a WRN-28 on CIFAR-100.

Figure 7: All considered SAM-variants and their _SAM-ON_ counterpart for a ViT-S on CIFAR-100.

### Adversarial robustness

Here, we provide additional results and extend the discussion on adversarial robustness from Section 4.2. In a study by Wei et al. [52] SAM-trained models showed non-trivial robustness to small adversarial perturbations [50]. Since there are several works highlighting the role of normalization layers for adversarial robustness [9; 54], it is interesting to investigate whether the robustness properties of SAM can be preserved when training with SAM-ON instead of SAM-all. In Table 15 we report the adversarial robustness of the ViT-S trained from scratch on ImageNet (as reported in Section 4.2 evaluated with the two white-box attacks from APGD, but for more radii). The SAM-ON models are not only better than the base optimizer, but consistently outperform the SAM-all models by a small margin. For a WRN-28-10 on CIFAR-100 the differences are less pronounced and often within the standard deviation (reported over 3 seeds in Table 14). SAM-ON also improves over SAM-all, but for the ASAM-elementwise-\(\ell_{\infty}\) the all-variant is slightly better than the ON-variant. Overall, we find that in order to get SAM-like improvements for adversarial robustness (as shown in [52]) it is enough to only perturb the normalization layers in SAM, illustrating again their special role.

### Machine translation task

To probe the effectiveness of _SAM-ON_ outside the vision domain, we apply it to the IWSLT'14 DE-EN machine translation task, following the setup of Kwon et al. [37]. We report the resulting Bleu scores in Table 16: _SAM-all_ and _SAM-ON_ perform similar (within standard deviations reported over 3 random seeds), both improving over the vanilla optimizer. While being very limited in its scope, this experiment is a first hint that _SAM-ON_ might also be effective outside the vision domain. Proper evaluations, as for instance done in [7], are required to confirm this for large-scale settings.

\begin{table}
\begin{tabular}{c c c c} vanilla & SAM-all & SAM-ON \\ \hline \(34.56^{\pm 0.11}\) & \(34.83^{\pm 0.10}\) & \(34.95^{\pm 0.16}\) \\ \end{tabular}
\end{table}
Table 16: IWSLT-DE-EN Bleu scores. Reported over 3 random seeds.

\begin{table}
\begin{tabular}{c c c c c c}  & & SGD & SAM & \multicolumn{2}{c}{ASAM-el.-\(l_{\infty}\)} \\ threat model & \(\epsilon\) & & all & ON & all & ON \\ \hline \(\ell_{2}\) & \(0.10\) & \(18.14^{\pm 0.11}\) & \(28.14^{\pm 1.09}\)**31.28**\({}^{\pm 0.50}\) & \(30.33^{\pm 0.80}\) & \(30.16^{\pm 0.26}\) \\ \(\ell_{2}\) & \(0.20\) & \(2.33^{\pm 0.11}\) & \(5.39^{\pm 0.34}\) & **6.62**\({}^{\pm 0.07}\) & **6.63**\({}^{\pm 0.12}\) & \(6.10^{\pm 0.18}\) \\ \hline \(\ell_{\infty}\) & \(1/255\) & \(10.29^{\pm 0.04}\) & \(17.96^{\pm 1.08}\)**19.56**\({}^{\pm 0.33}\) & **20.69**\({}^{\pm 0.81}\) & \(18.63^{\pm 0.30}\) \\ \(\ell_{\infty}\) & \(2/255\) & \(0.67^{\pm 0.01}\) & \(1.96^{\pm 0.17}\) & **2.16**\({}^{\pm 0.07}\) & **2.62**\({}^{\pm 0.01}\) & \(2.05^{\pm 0.17}\) \\ \hline Clean acc. & & \(80.7^{\pm 0.2}\) & \(83.1^{\pm 0.3}\) & **84.2**\({}^{\pm 0.2}\) & \(83.3^{\pm 0.2}\) & **84.1**\({}^{\pm 0.2}\) \\ \end{tabular}
\end{table}
Table 14: **Adversarial robustness CIFAR-100:** Reported is robust accuracy (in %) for a WRN-28 trained from scratch on CIFAR-100. Adversarial robustness is evaluated with the two whitebox APGD attacks from autoattack [16].

\begin{table}
\begin{tabular}{c c c c c c c c}  & & \multicolumn{4}{c}{AdamW} & \multicolumn{2}{c}{Lion} \\  & \(\epsilon\) & vanilla & SAM-all & SAM-ON & vanilla & SAM-all & SAM-ON \\ \hline \(\ell_{2}\) & \(0.25\) & \(19.67^{\pm 0.47}\) & \(37.53^{\pm 0.69}\) & **41.16**\({}^{\pm 0.24}\) & \(22.01^{\pm 0.78}\) & \(38.52^{\pm 0.66}\) & **43.12**\({}^{\pm 0.97}\) \\ \(\ell_{2}\) & \(0.50\) & \(5.47^{\pm 0.18}\) & \(17.71^{\pm 0.61}\) & **22.72**\({}^{\pm 0.25}\) & \(6.63^{\pm 0.46}\) & \(19.03^{\pm 0.92}\) & **24.27**\({}^{\pm 1.34}\) \\ \(\ell_{2}\) & \(1.00\) & \(0.43^{\pm 0.09}\) & \(3.34^{\pm 0.36}\) & **5.58**\({}^{\pm 0.19}\) & \(0.57^{\pm 0.07}\) & \(3.98^{\pm 0.28}\) & **6.64**\({}^{\pm 0.69}\) \\ \(\ell_{\infty}\) & \(0.25/255\) & \(33.45^{\pm 0.80}\) & \(48.08^{\pm 0.14}\) & **49.34**\({}^{\pm 0.08}\) & \(35.31^{\pm 0.08}\) & \(49.57^{\pm 0.60}\) & **51.37**\({}^{\pm 0.99}\) \\ \(\ell_{\infty}\) & \(0.5/255\) & \(14.98^{\pm 0.18}\) & \(29.68^{\pm 0.09}\) & **32.46**\({}^{\pm 0.15}\) & \(15.86^{\pm 0.13}\) & \(31.68^{\pm 0.62}\) & **34.23**\({}^{\pm 1.73}\) \\ \(\ell_{\infty}\) & \(1/255\) & \(2.61^{\pm 0.16}\) & \(8.64^{\pm 0.01}\) & **10.82**\({}^{\pm 0.56}\) & \(2.93^{\pm 0.29}\) & \(10.02^{\pm 0.56}\) & **12.03**\({}^{\pm 1.30}\) \\ \hline Clean acc. & & \(66.89^{\pm 0.04}\) & \(71.47^{\pm 0.12}\) & \(71.37^{\pm 0.026}\) & \(68.20^{\pm 0.02}\) & \(71.90^{\pm 0.19}\) & **72.64**\({}^{\pm 0.14}\) \\ \end{tabular}
\end{table}
Table 15: **Adversarial robustness ImageNet:** Reported is robust accuracy (in %) for a ViT-S trained from scratch on ImageNet, as reported in Table 4. Adversarial robustness is evaluated with the two whitebox APGD attacks from autoattack [16].

### Weight distribution after training

In order to get a better understanding of the impact of _SAM-ON_ on \(\gamma\) and \(\beta\) (as defined in Eq. 1), we train a WideResNet-28-10 with different SAM-variants and both _SAM-ON_ and _all_. We show the distribution of \(|w_{i}|\), i.e. the parameter magnitudes, at the end of training for different layer types in Figure 9. Different to the discussion in Section 5.3, we show the \(y\)-axis on log-scale, in order to inspect more nuanced differences. For elementwise \(\ell_{2}\) there is no strong change in the distribution of the BatchNorm parameters between _all_ and _SAM-ON_. For elementwise \(\ell_{\infty}\), layerwise \(\ell_{2}\) and SAM, however, the magnitude of the BatchNorm parameters shifts clearly towards larger values, especially for the weight parameters. We note that this resembles a pattern we observed when comparing the optimal \(\rho\)-value for _all_ and _SAM-ON_ in Table 7: The optimal \(\rho\) of elementwise \(\ell_{2}\) did not change much for ResNet architectures, whereas for the other considered methods, it shifted towards larger values for _SAM-ON_. Additionally and in contrast to the other methods, the elementwise \(\ell_{2}\) variant showed a strong performance decrease in _no-norm_ (Figure 1), indicating that it implicitly focuses on perturbing the BatchNorm layers already. We note that larger BatchNorm parameters do not necessarily indicate a functionally different network, since there are many reparameterization invariances in ReLU networks, some of which ASAM tries to leverage in its perturbation definition Eq. (4). Nevertheless, the scale of the network still has an impact on the training dynamics, since other methods like e.g. weight decay depend on it. We discuss the impact of weight decay further in Appendix B.9.

### Removing the affine parameters

Frankle et al. [24] found for SGD that fixing the normalization parameters typically decreases the generalization performance of networks. As an ablation, we therefore study the effect of SAM when the normalization weights are non-trainable. This is, we set \(\gamma=1\) and \(\beta=0\) and train the remaining parameters with SAM. The results are shown for a WRN-28 in Figure 10, where it can be seen that fixing the normalization parameters (_fix-norm_) does _not_ lead to a decrease in the performance of SAM. We thus hypothesize that in certain settings, SAM might not leverage the expressive power of the normalization layers, which might contribute to the improved performance of _SAM-ON_.

### Training BatchNorm and only BatchNorm

The affine parameters of the normalization layers are relatively understudied in the literature. Recently, Frankle et al. [24] were able to obtain surprisingly high performance for ResNet architectures by only training the BatchNorm layers (freezing all other parameters), illustrating their expressive power. We study the effect of SAM in this setting (i.e. when all parameters except for the BatchNorm layers are frozen) for a ResNet-101 and a WRN-28 on CIFAR-10 and find that SAM still aids generalization in this setting (Table 17).

### Weight decay and dropout

Here, we explore potential connections of _SAM-ON_ with weight decay and dropout. Since weight decay is sometimes applied to all network parameters, and sometimes normalization layers are omitted, it is worth investigating if the benefits of _SAM-ON_ can be attributed to its interaction with weight decay. To this end, we train a WRN-28 with SGD, _SAM-all_ and _SAM-ON_, and apply weight decay to either all parameters, all _except_ the normalization layers, or not at all (Figure 11, right). For each setting _SAM-ON_ outperforms SAM, outlining that its success should not be attributed to the interaction with weight decay.

We further test if _SAM-ON_-like performance can be achieved by simply applying stronger regularization and stochasticity to the normalization parameters. To this end, we apply dropout solely on the normalization layers (Figure 11, left) and find that this is not the case.

\begin{table}
\begin{tabular}{l|c|c c} Model & SGD & SAM \(\rho=0.01\) & SAM \(\rho=0.05\) \\ \hline ResNet-101 & 78.75 & 78.63 & **79.27** \\ WRN-28 & 63.49 & **64.48** & 62.70 \\ \end{tabular}
\end{table}
Table 17: Effect of SAM when training _only_ BatchNorm layers, for networks trained on CIFAR-10.

Figure 10: When training with SAM, fixing \(\gamma=1,\beta=0\) (_fix-norm_) barely changes the performance of the network. WRN-28, CIFAR-100.

Figure 9: SAM-ON leads to a shift in the distribution of \(\gamma\).

### Details on sharpness evaluation

For the following discussion, we note that the term _generalization_ is sometimes used as the difference between train and test error, while in other cases people use it as a synonym for test error. Since in CIFAR settings the models achieve train error close to zero, the two definitions become equivalent.

Many studies have attempted to better understand the possible connection between the generalization of deep neural networks and the flatness of the loss-surface [26, 32, 22, 34, 18]. Recently, Andriushchenko et al. [4] conducted a large-scale study for a range of models, datasets, and sharpness-definitions, finding that "while there definitely exist restricted settings where correlation between sharpness and generalization is significantly positive (e.g., for ResNets on CIFAR-10 with a specific combination of augmentations and mixup) it is not true anymore when we compare all models jointly" and concluding "that one should avoid blanket statements like _flat minima generalize better_". In order to evaluate sharpness, we therefore adopt their setup and choose the best-performing sharpness measure for CIFAR from their study, which is logit-normalized elementwise-adaptive worst-case-\(\ell_{\infty}\)-\(m\)-sharpness. This is, \(m\)-sharpness \(s_{w}^{m}\) is defined as the largest possible change in loss within the adaptive perturbation model defined in 4,

\[s_{w}^{m}=\mathbb{E}_{x,y\sim D_{m}}\max_{||T_{w}^{-1}\epsilon||_{p}\leq\rho} L(\mathbf{w}+\epsilon)-L(\mathbf{w})\] (6)

where \(T_{w}^{i}=|w_{i}|\), \(p=\infty\) and \(D_{m}\) returns data batches of size \(m\). \(\rho\) here denotes the size of the ball over which sharpness is evaluated and is not to be confused with the \(\rho\) from the SAM-algorithm. Like for ASAM [37], the motivation behind adaptive sharpness measures is to make them invariant to reparameterizations of the network. Further, the logit-outputs of the network are normalized with respect to their \(\ell_{2}\)-norm in order to mitigate the scale-sensitivity of classification losses. In practice, Andriushchenko et al. [4] compute \(s_{w}^{m}\) over a subset of the train set of size \(1024\) and use \(m=128\), i.e. average 8 batches. We use a subset of size \(2048\) in order to obtain more reliable sharpness estimates, and adopt \(m=128\). The maximization in (6) is performed with AutoPGD [16], a hyperparameter-free method designed for accurate estimation of adversarial robustness. It is to note that except for the logit-normalization, the sharpness definition reported in Table 6 corresponds exactly to the perturbation model that ASAM elementwise \(\ell_{\infty}\) uses, and hence the 1-step sharpness reported should be fairly close the the objective that ASAM elementwise \(\ell_{\infty}\) actually minimizes during training. While ASAM elementwise \(\ell_{\infty}\) yields slightly smaller sharpness values than the conventional SAM algorithm, the differences are rather small when compared to the significantly sharper _SAM-ON_ models. For the results in Table 6 in the main paper we tuned the sharpness radius \(\rho\) such that we obtain sharpness values similar to those reported to yield the highest correlation in Andriushchenko et al. [4]. In Table 18 we report sharpness values for a ResNeXt-model, in addition to the WRN-28 from the main paper. In all cases the _SAM-ON_ models are sharper than the _SAM-all_ models yet generalize better. In Table 19 we further report other sharpness measures without logit-normalization for a WRN-28. SAM-ON is sharper than SAM-all with respect to most metrics, although there exist some exceptions. It should however be stressed that many of those metrics did not show good correlation with generalization in the study by Andriushchenko et al. [4].

Figure 11: **Left: Applying dropout only to the normalization layers (blue/red) performs worse than _SAM-ON_. **Right:**_SAM-ON_ improves over _SAM-all_ irrespective of whether weight decay is applied to all parameters (green), all _except_ the normalization layers (blue) or not at all (yellow).

[MISSING_PAGE_FAIL:23]

iteration for \(w_{N}\) is:

\[w_{N}^{t+1/2} =w_{N}^{t}+\rho\frac{g_{N,x_{i}}(w^{t})}{\|g_{N,x_{i}}(w^{t})\|}\] \[w_{N}^{t+1} =w_{N}^{t}-h\:g_{N,x_{i}}\left(w^{t+1/2}\right)\] (10)

and for \(w_{A}\) is:

\[w_{A}^{t+1/2} =w_{A}^{t}\] \[w_{A}^{t+1} =w_{A}^{t}-h\:g_{A,x_{i}}\left(w^{t+1/2}\right).\] (11)

**Theorem C.5**.: _Assuming C.1 and C.2, \(h\leq 1/L\), we obtain:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\|\nabla f(w^{t})\|^{2}\right]\leq \frac{2(f(w^{0})-f(w^{*}))}{hT}+2LhM+L^{2}\rho^{2}(1+Lh),\] (12)

_with \(w^{*}\) the optimal solution to \(f(w)\)._

Proof.: From Assumption C.1 and thus Remark C.3 it follows that:

\[f(w^{t+1}) \leq f(w^{t})+\nabla f(w^{t})\cdot(w^{t+1}-w^{t})+\frac{L}{2}\|w ^{t+1}-w^{t}\|^{2}\] (13) \[\leq f(w^{t})-h\nabla f(w^{t})\cdot g_{x_{i}}\left(w^{t+1/2} \right)+\frac{h^{2}L}{2}\left\|g_{x_{i}}\left(w^{t+1/2}\right)\right\|^{2}\] (14) \[=f(w^{t})-h\nabla f(w^{t})\cdot g_{x_{i}}\left(w^{t+1/2}\right)\] \[\quad+\frac{h^{2}L}{2}\left(\|\nabla f(w^{t})-g_{x_{i}}(w^{t+1/2} )\|^{2}-\|\nabla f(w^{t})\|^{2}+2\left(\nabla f(w^{t})\cdot g_{x_{i}}(w^{t+1/2 })\right)\right)\] \[=f(w^{t})-\frac{Lh^{2}}{2}\|\nabla f(w^{t})\|^{2}+\frac{Lh^{2}}{2 }\|\nabla f(w^{t})-g_{x_{i}}(w^{t+1/2})\|^{2}\] \[\quad-(1-Lh)h\left(\nabla f(w^{t})\cdot g_{x_{i}}(w^{t+1/2})\right)\] (15) \[\leq f(w^{t})-\frac{Lh^{2}}{2}\|\nabla f(w^{t})\|^{2}+Lh^{2}\| \nabla f(w^{t})-g_{x_{i}}(w^{t})\|^{2}\] \[\quad+Lh^{2}\|g_{x_{i}}(w^{t})-g_{x_{i}}(w^{t+1/2})\|^{2}-(1-Lh)h \left(\nabla f(w^{t})\cdot g_{x_{i}}(w^{t+1/2})\right).\] (16)

Taking the double expectation gives (because unbiased gradient and Assumption C.2 and Remark C.4):

\[\mathbb{E}[f(w^{t+1})] \leq\mathbb{E}[f(w^{t})]-\frac{Lh^{2}}{2}\mathbb{E}\|\nabla f(w^{ t})\|^{2}+Lh^{2}M\] \[\quad+\underbrace{Lh^{2}\|g(w^{t})-g(w^{t+1/2})\|^{2}}_{\mathcal{ A}}-(1-Lh)h\underbrace{\mathbb{E}\left[\nabla f(w^{t})\cdot g(w^{t+1/2}) \right]}_{\mathcal{B}}.\] (17)

For term \(\mathcal{A}\) we obtain using Assumption C.1:

\[\mathcal{A}\leq L^{3}h^{2}\|w^{t}-w^{t+1/2}\|^{2}=L^{3}h^{2}\rho^{2}.\] (18)

For term \(\mathcal{B}\) we obtain:

\[\mathcal{B} =\mathbb{E}\left[\{\nabla f_{N}(w^{t}),\nabla f_{A}(w^{t})\} \cdot\{g_{N}(w^{t+1/2}),g_{A}(w^{t+1/2})\}\right]\] (19) \[=\mathbb{E}[\nabla f_{A}(w^{t})\cdot(g_{A}(w^{t+1/2})-g_{A}(w^{t} )+g_{A}(w^{t}))]\] \[\quad+\mathbb{E}[\nabla f_{N}(w^{t})\cdot(g_{N}(w^{t+1/2})-g_{N}( w^{t})+g_{N}(w^{t}))]\] (20) \[=\mathbb{E}\left[\|\nabla f(w^{t})\|^{2}\right]\] \[\quad+\underbrace{\mathbb{E}[\nabla f_{A}(w^{t})\cdot(g_{A}(w^{t+ 1/2})-g_{A}(w^{t}))]+\:\mathbb{E}[\nabla f_{N}(w^{t})\cdot(g_{N}(w^{t+1/2})-g_{ N}(w^{t}))]}_{\mathcal{C}}.\] (21)Using \(xy\leq\frac{1}{2}\|x\|_{2}^{2}+\frac{1}{2}\|y\|_{2}^{2}\) and Assumption C.1 we get for \(\mathcal{C}\):

\[|\mathcal{C}|\leq\frac{1}{2}\mathbb{E}\left[\|\nabla f(w^{t})\|^{2}\right]+ \frac{L^{2}}{2}\|w^{t+1/2}-w^{t}\|^{2}=\frac{1}{2}\mathbb{E}\left[\|\nabla f(w ^{t})\|^{2}\right]+\frac{L^{2}\rho^{2}}{2}.\] (22)

Plugging this into (17) gives:

\[\mathbb{E}[f(w^{t+1})] \leq\mathbb{E}[f(w^{t})]-\frac{Lh^{2}}{2}\mathbb{E}\|\nabla f(w^{t })\|^{2}+Lh^{2}M+L^{3}h^{2}\rho^{2}-(1-Lh)h\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[+(1-Lh)h\left(\frac{1}{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}+\frac{ L^{2}\rho^{2}}{2}\right)\] (23) \[\leq\mathbb{E}[f(w^{t})]-\frac{h}{2}\mathbb{E}\|\nabla f(w^{t})\| ^{2}+Lh^{2}M+\frac{1}{2}hL^{2}\rho^{2}(1+Lh).\] (24)

In \(T\) iterations we obtain using a telescoping sum:

\[f(w^{*})-f(w^{0}) \leq\mathbb{E}[f(w^{T})]-f(w^{0})\] \[\leq-\frac{h}{2}\sum_{t=0}^{T-1}\mathbb{E}\left[\|\nabla f(w^{t} )\|^{2}\right]+Lh^{2}MT+\frac{1}{2}hL^{2}\rho^{2}(1+Lh)T.\] (25)

This gives Theorem C.5.