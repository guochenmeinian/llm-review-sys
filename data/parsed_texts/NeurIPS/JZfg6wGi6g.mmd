Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time

Zichang Liu

Department of Computer Science

Rice University

zichangliu@rice.edu

&Aditya Desai

Department of Computer Science

Rice University

Aditya.P.Desai@rice.edu

&Fangshuo Liao

Department of Computer Science

Rice University

Fangshuo.Liao@rice.edu

&Weitao Wang

Department of Computer Science

Rice University

wtwang@rice.edu

&Victor Xie

Department of Computer Science

Rice University

vyx2@rice.edu

&Zhaozhuo Xu

Department of Computer Science

Stevens Institute of Technology

zxu79@stevens.edu

&Anastasios Kyrillidis

Department of Computer Science

Rice University

anastasios@rice.edu

&Anshumali Shrivastava

Department of Computer Science

Rice University & ThirdAI Corp.

anshumali@rice.edu

###### Abstract

Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize _the persistence of importance_: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of KV cache under a fixed budget without finetuning the model. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5\(\times\) without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization for further compression Introduction

Large language models(LLMs), trained on immense amounts of text data, have demonstrated an incredible ability to generate text that is both logically connected and contextually relevant [1, 2, 3, 4, 5]. LLM inference follows an autoregressive fashion, generating one token at each step conditioned on the previous steps. At each step, the key-value embedding in attention is stored in memory to avoid repetitive key-value projection computation at future steps. Unfortunately, the memory of the key-value cache( KV cache), including prompts and previously generated tokens, can be surprisingly large. Using OPT-175B as an example, the impressive 175 billion parameters consume around 325 GB of memory. At the same time, at batch size 128 and sequence length 2048, the KV cache requires around 950 GB of memory, three times larger than the model weights. Considering that 8 Nvidia A100-80GB offers 640GB GPU memory, the memory usage of the KV cache is truly concerning.

LLMs are typically deployed on fixed memory hardware, and the size of model weights is also fixed once deployed. Apart from a small memory buffer typically reserved for communication and computation, the rest of the available memory is for the KV cache. The size of the KV cache depends on batch size, sequence length, and model dimension. Thus, at a given inference sequence length, compression in the KV cache memory translates almost linearly into an increase in the batch size. And any increase in batch size is significant for high-throughput inference systems [6, 7].

Quantization and sparsity approaches [8, 9, 10, 11, 12, 13, 14] have been studied in LLMs to reduce the model sizes. However, compressing the KV cache remains an open but challenging problem. First, training models at the scale of hundreds of billions of parameters on a large amount of data is prohibitively expensive. Thus, an ideal compression algorithm should be applicable without training. Second, emerging applications such as dialogue systems require an extremely long context window. The maximum sequence length of LLMs is growing to over 32K [15]. The size of the KV cache also grows linearly with sequence length. For scalability, an ideal compression algorithm should reduce the memory from the sequence length dimension. At last, compression should preserve LLMs' quality and in-context learning ability.

We go beyond the traditional model compression techniques to achieve such demanding requirements. We envision that not all tokens must be stored in memory for LLM to understand the context. Just like humans can skim through an article and grasp the main idea, LLMs may also be able to skim and comprehend. It is commonly observed that the attention score from one token follows a strong power law distribution [16, 17, 18, 19, 20], meaning that one token will only heavily attend to a small number of tokens. More importantly, we observe **Repetitive Attention Pattern** from different tokens in the sequence in a trained LLM( Figure 1). Certain tokens are more important throughout the paragraph. Specifically, for two different tokens, there are similarities between which tokens they are heavily attending to and similarities between which tokens they are ignoring.

Inspired by the above observation, we articulate the **Persistence of Importance Hypothesis:**_Only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step._ This hypothesis, if true, suggests that it is possible to foresee which token is likely to be important for future generations. Fortunately, we empirically verify that later tokens in the

Figure 1: **Repetitive Attention Pattern. We plot the attention map at three token positions in a sentence. Only five attention heads are plotted for a clearer presentation. We discretize the attention score such that the high score is dark green, and the low score is light green. In Figure 1(a), the token at position 178 pays heavy attention to positions 27, 63, 98, etc. This pattern is also present in the attention maps of position 228 and position 278.**

sentence mostly only attend to tokens that were heavily attended from the early tokens in a sentence. And the overlapping ratio is surprisingly high, over 90% in most of the transformer layers (Figure 2).

Based on the above two findings, we present Scissorhands that exploits the _persistence of importance hypothesis_ to realize LLM inference with a compressed KV cache. In Section 4, we present an efficient algorithm such that the size of KV cache is always less than a predetermined budget. A theoretical guarantee justifies that such a compressed KV cache can approximate the attention output. In Section 5, we empirically evaluate Scissorhands and show that Scissorhands reduces the memory usage of KV cache \(2-5\times\) without compromising model quality. Reduction in the KV cache can directly result in a larger batch size. Further, we adopt quantization and show its compatibility with Scissorhands.

## 2 Problem Description and Related Work

This paper considers the LLM inference workflow, specifically focusing on the memory usage for storing the keys and values in attention. Let \(d\) be the hidden dimension of the model, \(b\) be the batch size, and \(p\) be the length of prompt sentences. We are given the trained model weights, \(W^{i}_{K}\in\mathbb{R}^{d\times d}\), \(W^{i}_{V}\in\mathbb{R}^{d\times d}\) for the key and value projection matrix at the \(i^{th}\) transformer layer.

The standard LLM inference consists of two stages: prompting and token generation. In the prompt stage, the model takes the prompt sentences as the input, and the key/value embedding in attention is stored as a cache to reduce repetitive computation. Denote \(x^{i}_{\text{prompt}}=[x^{i}_{1},...,x^{i}_{p}],x^{i}_{\text{prompt}}\in \mathbb{R}^{b\times p\times d}\) as the input to attention at the \(i^{th}\) transformer layer. Denote the key cache and value cache at layer \(i\) as \(\mathcal{K}^{i},\mathcal{V}^{i}\in\mathbb{R}^{b\times p\times d}\), \(\mathcal{K}^{i}_{0}=x^{i}_{\text{prompt}}W^{i}_{K},\mathcal{V}^{i}_{0}=x^{i} _{\text{prompt}}W^{i}_{V}\).

In the generation stage, the model starts with the stored KV cache in the prompting stage and generates one token at each step. At each step, the KV cache gets updated. Given the input to attention at step \(t\) in the \(i^{th}\) transformer layer \(x^{i}_{t}\in\mathbb{R}^{b\times 1\times d}\). \(\mathcal{K}^{i}_{t+1}=[\mathcal{K}^{i}_{t},x^{i}_{t}W^{i}_{K}],\mathcal{V}^{i} _{t+1}=[\mathcal{V}^{i}_{t},x^{i}_{t}W^{i}_{V}]\).

### LLM Inference Memory Breakdown

In this section, we provide the memory consumption breakdown of LLMs. The memory footprint consists of three parts: model weights, KV cache, and activation buffer. The size of model weights depends on model configuration, such as the number of transformer layers and hidden size. The size of the KV cache depends on model configurations, sequence length, and batch size. The size of the activation buffer depends on parallelism strategy, model configurations, and implementation. The size of the activation buffer is considerably smaller than the previous two. As shown in Table 1, the size of the KV cache, 2.5\(\times\)-5\(\times\) larger than model weights, can quickly become the bottleneck in memory consumption. At the same time, much research has been spent on extending the length of the context window. GPT-4-32K can process up to 32,768 tokens [15]. Longer sequence length would make the KV cache memory problem even more severe.

Assuming LLM generates until its maximum sequence length, we summarize the maximum batch size before going out of GPU memory on a box of 8 A100 80GB GPU in Table 2. At the GPT-3 scale with a maximum sequence length of 2048, batch size cannot exceed 35 without offloading. Small batch size limits the model inference throughput.

### Efficient Attention

Computing the attention matrix necessitates a time complexity of \(O(n^{2})\), where \(n\) is the sequence length. As a result, a line of work has been proposed to mitigate the computation burden of the attention mechanism [16; 17; 18; 19; 20]. These approaches exploit low-rank or sparsification to approximate the attention output. Besides, [21] realized exact efficient attention with wall-clock speed by optimizing

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Model & \# of Layer & Hidden Size & Weights (GB) & KV cache (GB) \\ \hline OPT-175B & 96 & 12288 & 325 & 1152 \\ \hline LLaMA-65B & 80 & 8192 & 130 & 640 \\ \hline BLOOM & 70 & 14336 & 352 & 950 \\ \hline \end{tabular}
\end{table}
Table 1: The memory consumption of model weights and KV cache for three different LLMs at batch size 128 and sequence length 2048 shows that the KV cache dominates the memory consumption.

the number of memory reads and writes. However, these approaches were evaluated mostly for training, focused on computation complexity, and did not address the KV-Cache memory usage introduced by auto-regressive language models.

Recently, there is active research attempting to apply quantization or pruning in LLM [8, 9, 10, 11, 12, 13, 14]. However, they mostly focus on reducing the size of model weights. Flexgen [7] applies quantization and sparsification to the KV cache; however, the memory of the KV cache is not reduced regarding sequence lengths. It stores the quantized KV cache for all tokens in CPU memory and loads all attention keys from CPU memory to compute attention scores. At the same time, methods such as Multi-Query-Attention(MQA) [22] change the attention design such that keys and values are shared across all attention heads. MQA requires training the model from scratch, while our works focus entirely on the inference stage.

## 3 The Persistence of Importance Hypothesis

We first present one interesting observation upon which the _persistence of importance hypothesis_ is derived in Section 3.1. In Section 3.2, we discuss the hypothesis in detail with empirical verification. Then, in Section 3.3, we provide theoretical intuition on the reason behind such model behaviors.

### Repetitive Attention Pattern.

**Observation.** We are interested in the attention score from the position \(t\) over all the words that come before it in the sentence. In Figure 1, we provide three attention maps of a sentence randomly drawn from the Colossal Clean Crawled Corpus (C4) [23] using OPT-6B. Each attention map is a discretized attention score calculated at a random position. We consider a score larger than \(\frac{1}{t}\) as significant as \(\frac{1}{t}\) indicates an averaging mixing score. High attention scores are marked with dark green.

**Result.** High attention scores are observed at the same set of tokens from various positions in the sentence. In all three plots, we see dark green at sequence positions 27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention at all three positions. We observe similar model behavior at different transformer layers with different text inputs. More plots are in Appendix A.

**Implication.** Even though small differences exist, repetitive attention patterns are evident in the attention maps. There exist specific tokens that keep receiving high attention. Meanwhile, these attention maps show sparsity: only a few tokens have high attention scores.

### The Persistence of Importance Hypothesis

The repetitive attention pattern suggests that specific tokens are influential throughout the sequence. A stricter claim is that these tokens are the only ones that could be significant for a future step. Thus, we articulate the _persistence of importance hypothesis_.

**The Persistence of Importance Hypothesis.**_With a trained autoregressive language model, only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step._

If true, this hypothesis indicates the possibility of foreseeing what information in the previous sequences could be vital for future steps. This hypothesis is trivial when pivotal tokens include all tokens in the entire sentences. However, a much more interesting case is when pivotal tokens are a subset of previous words. This would enable us to reduce the size of the KV cache by throwing away the embedding of non-important tokens.

**Pivotal Token.** One natural indication of a token's influence is the attention score. We consider a token pivotal for position \(t\) if this token receives an attention score larger than threshold \(\alpha\) from the token at position \(t\). Let \(S_{t}\) denote the set of pivotal tokens for position \(t\). \(S_{a\to b}\) denote the set of

\begin{table}
\begin{tabular}{c|c|c|c} \hline Model & OPT-175B & LLaMA-65B & BLOOM \\ \hline Maximum Batch Size & 34 & 102 & 36 \\ \hline \end{tabular}
\end{table}
Table 2: Maximum batch size before hitting out of memory on a box of 8 A100 80GB GPU when models are deployed with its maximum sequence length.

pivotal tokens for every position from \(a\) to \(b\).

\[S_{a\to b}=\cup_{t=a}^{t=b}S_{t}\]

**Verification.** We measure _persistence ratio_ as an empirical test the hypothesis. _Persistence ratio_ measures how many tokens in the pivotal token sets of the later part of the sentence are also in the pivotal token sets of the initial part of the sentence. Let \(l\) denote the length of the sentence. We record \(S_{1\to t}\in\{x_{1},...x_{t}\}\), tokens in \(\{x_{1},...,x_{t}\}\) who received high attention from every position until \(t\). Then, we record \(S_{t+1\to l}\in\{x_{1},...x_{t}\}\), tokens in \(\{x_{1},...,x_{t}\}\) who received high attention from position after \(t\). The persistence ratio is the intersection divided by the size of \(S_{t+1\to l}\). Formally,

\[\text{\emph{Persistence Ratio}}=\frac{|S_{t+1\to l}\cap S_{0\to l}|}{|\{x|x \in S_{t+1\to l},x\in\{x_{1},...,x_{t}\}\}|}\]

At the same time, we measure \(\frac{|S_{0\to t}|}{t}\). \(|S_{0\to t}|=t\) indicates that every token substantially impacted at least one position, which is the trivial case of _persistence of importance hypothesis_. Our test is performed with OPT models [24] with different datasets such as OpenBookQA [25] and WikiText [26]. In our verification, we set \(t=\frac{l}{2}\), which measures the overlapping between the first and later half of the sentences. Same as in Section 3.1, we set \(\alpha=\frac{1}{t}\), which suggests an average score.

**Result.** We present our main results in Figure 2. First, given the current criterion of pivotal token and \(t\) value, the size of \(S_{0\to t}\) is considerably smaller than half of the sentence length. This verifies that we are not considering the trivial case of our hypothesis. Second, the persistence ratio is generally over 95%, with dips in the later transformer layers. The pivotal token set of the later half sentences is mostly included in the set of the first half sentences. Combining these two pieces of empirical evidence, we see positive evidence for our hypothesis test.

**Implication.** The hypothesis provides insights for understanding the behavior of LLMs and opens up new opportunities for reducing the KV cache memory. The hypothesis suggests the possibility of predicting the potentially influential tokens for future steps. The non-influential tokens are unnecessary to store in the memory, as they are unlikely to have high attention scores. This reduces the number of tokens stored in the KV cache and the computation required at the attention.

### Attention Weights Decides the Pivotal Tokens

In the previous section, we verified that the significant tokens would continue to be significant. In this section, we try to understand the reasons for such phenomena. We consider the token generation process of a simplified model: a single-layer transformer model with single-head attention.

\[x_{t+1}=\mathcal{F}\left(a_{t}\right),\text{ where }a_{t}=\texttt{softmax} \left(\nicefrac{{1}}{{t}}\cdot x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top}\right)X_{t -1}W_{V}W_{O} \tag{1}\]

\(x_{t}\in\mathbb{R}^{1\times d}\) is a row vector. \(X_{t-1}\in\mathbb{R}^{(t-1)\times d}\) denotes the aggregation of \(x_{1},\dots,x_{t-1}\), where the \(j\)th row is \(x_{j}\). \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{d\times d}\) and \(W_{O}\in\mathbb{R}^{d\times d}\) are the attention weights. Lastly, \(\mathcal{F}:\mathbb{R}^{1\times d}\rightarrow\mathbb{R}^{1\times d}\) denotes the MLP block following attention block, a two-layer MLP with skip connections, given by

\[\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x) \tag{2}\]

Figure 2: Persistence ratio and the corresponding size of the pivotal token set. The persistence ratio is over 95% in most layers, with decreases at the later layers. Meanwhile, the number of pivotal tokens is considerably smaller than the sequence length. This suggests that the pivotal tokens of later half sentences are almost all included in the set of first halves.

```
Input: Memory Budget \(B\), Maximum Sequence Length \(T_{\text{max}}\) Key Cache \(\bar{\mathcal{K}}\in R^{n\times d}\), Value Cache \(\bar{\mathcal{V}}\in R^{n\times d}\), where \(n=0\) while\(t<T_{\text{max}}\)do  Model update \(\bar{\mathcal{K}},\bar{\mathcal{V}}\) such that \(n\gets n+1\) if\(n>B\)then  Compress KV cache using Algorithm 2 such that \(n\leq B\). endif \(t\gets t+1\) endwhile
```

**Algorithm 1** Inference with Budget KV cache

We are interested in the attention scores \(\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot x_{t}W_{Q}W_{K}^{\top}X_ {t-1}^{\top})\). Notice that \(\alpha_{t,j}\) scales with \(x_{t}W_{Q}W_{K}^{\top}x_{j}^{\top}\). The following theorem characterizes the behavior of \(x_{t}W_{Q}W_{K}^{\top}x_{j}^{\top}\)

**Theorem 3.1**.: _Let \(A=W_{V}W_{O}W_{Q}W_{K}^{\top}\) and let \(\lambda_{K},\lambda_{Q},\lambda_{V},\lambda_{O}\) denote the largest singular values of \(W_{K},W_{Q},W_{V},W_{O}\), respectively. Consider the transformer in (1) with normalized inputs \(\left\lVert x_{t}\right\rVert_{2}=1\) for all \(t\). Let \(c,\epsilon>0\) be constants. Assume that \(a_{t}x_{t+1}^{\top}\geq(1-\delta)\left\lVert a_{t}\right\rVert_{2}\) with \(\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O} }\right)^{2}\). Then for all \(x_{\ell}\) satisfying \(x_{\ell}Ax_{\ell}^{\top}\geq c\) and \(x_{\ell}Ax_{\ell}\geq\epsilon^{-1}\max_{j\in[t],j\neq\ell}x_{j}Ax_{\ell}^{\top}\), it holds that_

\[\frac{x_{\ell}Ax_{\ell}^{\top}}{\left\lVert a_{t}\right\rVert_{2}}(\alpha_{t, \ell}-3\epsilon)\leq x_{t+1}W_{Q}W_{K}^{\top}x_{j}^{\top}\leq\frac{x_{\ell}Ax _{\ell}^{\top}}{\left\lVert a_{t}\right\rVert_{2}}(\alpha_{t,\ell}+3\epsilon) \tag{3}\]

The proof is provided in Appendix B. Theorem 3.1 shows that under an assumption on the MLP in (2), for all \(x_{\ell}\) such that \(x_{\ell}Ax_{\ell}^{\top}\) is large enough, \(x_{t+1}W_{Q}W_{K}^{\top}x_{j}^{\top}\) satisfies Equation (3). The assumption on the MLP \(a_{t}x_{t+1}^{\top}\geq(1-\delta)\left\lVert a_{t}\right\rVert_{2}\) essentially requires a large cosine similarity between the input and output of \(\mathcal{F}\). This behavior can be empirically verified in Appendix A. Essentially, skip connection dominates the output because \(\left\lVert x\right\rVert_{2}\gg\left\lVert W_{2}\texttt{relu}(W_{1}x)\right\rVert _{2}\), resulting in a cosine similarity close to one between input and output. Equation (3) shows that despite a factor of \(\frac{x_{\ell}Ax_{\ell}^{\top}}{\left\lVert a_{t}\right\rVert_{2}}\), \(x_{t+1}W_{Q}W_{K}^{\top}x_{j}^{\top}\) almost scales with \(\alpha_{t,\ell}\). Since \(x_{t+1}W_{Q}W_{K}^{\top}x_{j}^{\top}\) directly affects \(\alpha_{t+1,\ell}\), this property shows that a larger \(\alpha_{t,\ell}\) will potentially imply a large \(\alpha_{t+1,\ell}\).

Our theorem shows that the property in Equation (3) property only holds for \(x_{\ell}\) such that \(x_{\ell}Ax_{\ell}^{\top}\) is large. \(A\) are trained attention weights. This condition may suggest that the trained weights \(A\) selects \(x_{\ell}\) as a pivotal token. Each attention is learned to identify some subspace. Only those tokens embedded inside these regions are pivotal for this attention. This would explain why only some specific tokens are always relevant.

## 4 Sequential Token Generation Under budget

In this section, we present Scissorhands, which reduces the KV cache memory from the sequence length dimension without fine-tuning the model. In Section 4.1, we describe how Scissorhands maintains the KV cache under a given budget. Section 4.2 provides a theoretical analysis of the algorithm and the approximation error.

### Budget KV Cache for Single Attention Head

In this section, for the sake of the discussion, we drop the layer number notation \(i\) and batch size dimension. \(\mathcal{K}_{t},\mathcal{V}_{t}\in R^{t\times d}\) denote for the KV cache until step \(t\). \(x_{t}\in\mathbb{R}^{1\times d}\) is a row vector that denotes the input to attention at step \(t\). The output of an attention head at step \(t\) can be written as,

\[a_{t}=\sum_{i=1}^{t}\alpha_{t,i}\mathcal{V}[i]_{t}\text{, where }\alpha_{t,i}= \frac{\exp(\langle x_{t}W_{Q},\mathcal{K}_{t}[i]\rangle)}{\sum_{i=1}^{t}\exp( \langle x_{t}W_{Q},\mathcal{K}_{t}[i]\rangle)}\]

**Intuition.** As shown in Section 3, the attention scores \(\alpha_{t,i}\) follow a strong power-law distribution. For the autoregressive generation process, if there exists an oracle such that we can identify the heavy score tokens before the future generation step, then the memory of the KV cache can be significantly reduced by only storing the heavy score tokens. Fortunately, the _persistence of importance hypothesis_ provides us with such an oracle. It states that only historical tokens with significant contributions toward previous generated tokens will have significant contributions toward future tokens.

**Challenges.** LLMs are deployed on hardware with a fixed memory. The algorithm should maintain the cache under fixed memory to meet the hard requirement. Further, LLMs are already computationally intensive. The algorithm should avoid introducing much extra burden on computation.

A fixed memory budget for one attention head is \(B\) tokens. In other words, we can store key and value embedding for \(B\) previous tokens. We describe the problem as follows,

**Definition 4.1** (Sequential generation at an attention head under budget \(B\)).: _Given a stream of token embedding, including prompt and previously generated tokens, denotes their input to the head as \(\{x_{1},\dots,x_{t},\dots\}\). The problem of sequential generation at an attention head under budget \(B\) is maintaining a key cache \(\tilde{\mathcal{K}}_{t}\) and value cache \(\tilde{\mathcal{V}}_{t}\) such that \(\tilde{\mathcal{K}}_{t},\tilde{\mathcal{V}}_{t}\in R^{n\times d}\) and \(n<B\)._

**Approach.** Inspired by the textbook solution of reservoir sampling and the Least Recent Usage cache replacement algorithm, Scissorhands reserves a fixed memory buffer for the KV cache. When the buffer is full, Scissorhands drops stored but non-influential tokens from the cache. We present the main algorithm in Algorithm 1 and Algorithm 2.

When the KV cache size exceeds the budget, Scissorhands drops tokens from the KV cache according to Algorithm 2. The importance record is a counter that indicates how many times a token is deemed non-important. We choose attention scores as the importance indicators, following our methodology in Section 3.2. The importance record is collected over a history window \(w\) to reduce variance. A higher counter suggests dropping from the cache. Recent tokens are always kept because of the lack of information on their importance by setting the counter for all tokens in the recent window \(r\) to 0.

With a sampled KV cache, attention output can be computed by the following estimator

\[\hat{a}_{t}=\sum_{i=1}^{n}\hat{\alpha}_{t,i}\tilde{\mathcal{V}}_{t}[i]\text{, where }\hat{\alpha}_{t,i}=\frac{\exp(\langle x_{t}W_{Q},\tilde{\mathcal{K}}_{t}[i]\rangle)}{\sum_{i=1}^{n}\exp( \langle x_{t}W_{Q},\tilde{\mathcal{K}}_{t}[i]\rangle)}\]

**Overhead Tradeoff** At the compression step, an extra attention computation is introduced to collect the importance measurements over a history window. However, such compression is not required at every generation step. \(m\) controls the frequency, and we use \(m=0.5B\) in our experiment. Further, steps after the compression have reduced attention computation because of the reduction in the KV cache. On the other hand, one can trade a tiny amount of memory to avoid the overhead by maintaining the importance record during generation steps in Algorithm 1.

**Allocating Budgets Across Attention Heads.** An LLM typically consists of \(L\) transformer layers where each layer has \(H\) heads. A total memory budget has to be distributed over layers and heads. Within each transformer layer, the budget is distributed evenly across heads. Within the entire model, we distributed the budget according to Figure 2. The rule of thumb is to allocate more budget to later layers to compensate for the lower persistence ratio.

### Theoretical Analysis.

We study how much the tokens generated by the compressed KV cache deviate from the tokens generated by the original transformer using our simplified model in (1). Let \(\{\tilde{x}_{t}\}_{t=0}^{T}\) denote the tokens generated by the transformer with budget KV cache as in Algorithm 2 with \(m=1\):

\[\tilde{x}_{t+1}=\mathcal{F}\left(\tilde{a}_{t}\right)\text{, where }\tilde{a}_{t}= \texttt{softmax}\left(\nicefrac{{1}}{{t}}\cdot\tilde{x}_{t}W_{Q}\tilde{\mathcal{ K}}_{t}^{\top}\right)\tilde{\mathcal{V}}_{t}^{\top}W_{O}\]

Notice that when \(m=1\), i.e., in each iteration, we drop one token with the lowest score, the cache will always maintain \(B\) tokens. If the ranking of the attention scores does not change in each iteration, Algorithm 2 will always drop tokens with the smallest attention scores.

For reference purposes, let \(\{x_{t}\}_{t=0}^{T}\) denote the tokens generated by a vanilla transformer defined in (1). We will bound the difference \(\left\|x_{t}-\tilde{x}_{t}\right\|_{2}\).

**Theorem 4.1**.: _Let \(\lambda_{1},\lambda_{2}\) denote the largest singular values of \(W_{1}\) and \(W_{2}\) in (2). Let_

\[\beta_{t,j}=\frac{\exp\left(\nicefrac{{1}}{{t}}\cdot\tilde{x}_{t}W_{Q}W_{K}^{ \top}\tilde{x}_{j}^{\top}\right)}{\sum_{i=1}^{t-1}\exp\left(\nicefrac{{1}}{{t} }\cdot\tilde{x}_{t}W_{Q}W_{K}^{\top}\tilde{x}_{i}^{\top}\right)}\]

_and assume that each \(\beta_{t,j}=cv_{t,j}\), where \(v_{t,j}\) are sampled from a power-law distribution with pdf \(f(x)=c(x+b)^{-k}\). Suppose that \(\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq \frac{1}{2}\). Let \(T_{\min}\) and \(T_{\max}\) denote the starting and maximum sequence lengths, respectively, and let \(B\leq T_{\max}\) denote the budget as in Algorithm 2. If for all \(t\in[T_{\min},T_{\max}]\), \(S_{t}\) contains only tokens with at most the largest \(B\) values of \(\beta_{t,j}\), that is, \(\left|S_{t}\right|=B\) and \(\min_{j\in S_{t}}\beta_{t,j}\geq\max_{j\notin S_{t}}\beta_{t,j}\), then for all \(\epsilon\in(0,1)\), with probability at least \(1-T_{\max}\exp\left(-\frac{e^{2}\beta_{t}(T_{\min}-1)}{(k-2)^{2}(u+b)^{2}} \right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-B/T_{\max})^{2}}{(1-\epsilon )^{2}}\right)\), the following error bound must hold for all \(t\in[T_{\min},T_{\max}]\)_

\[\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|_{2}\right]\leq\frac{2.1(1- \nicefrac{{B}}{{T_{\max}}})}{(1-\epsilon)^{2}}\left(k-(k-1)\left(\frac{1- \epsilon}{\nicefrac{{B}}{{T_{\max}}}-\epsilon}\right)^{\nicefrac{{1}}{{(k-1) }}}\right) \tag{4}\]

The definition of \(\beta_{t,j}\) means the attention scores computed on the tokens generated by the compressed approach. Our theorem assumes that dropping the tokens depends on the attention score of the current iteration. (4) provided a bound on the expected difference between the tokens generated in the budget and the original approach. The upper bound scales with \(1-\nicefrac{{B}}{{T_{\max}}}\). When \(B=T_{\max}\), meaning that we are keeping all of the tokens, the error becomes zero. The term \(k-(k-1)\left(\frac{1-\epsilon}{\nicefrac{{1}}{{B-\epsilon}}}\right)\) depends on the distribution that the attention scores are fitted to and is always less than one. With a strong power-law distribution, this term provides a further decrease to the error bound in (4).

## 5 Empirical Evaluation

In this section, we present the results that demonstrate Scissorhands achieves up to 5\(\times\) reduction in the KV cache memory compared to the standard model with no accuracy loss. We also show that Scissorhands is compatible with 4-bit quantization.

**Experiment Setting.** We compare the accuracy of Scissorhands-OPT against the original OPT on one language model datasets C4 [23] and a number of few-shot downstream tasks: Hellaswag [27], MathQA [28], PIQA [29], Winogrande [30]. We use lm-eval-harness [31] to evaluate few-shot tasks. Our experiments are conducted on NVIDIA 4 A100 40GB GPU servers.

**No Accuracy Drop until 5\(\times\).** In Figure 3, we present Scissorhands's accuracy trend where 1\(\times\) denotes the original OPT. In the language modeling setting, perplexity is the lower the better. For OPT-6B, perplexity is maintained until 50% of the original KV cache size for OPT-13B. For OPT-66B, perplexity is maintained until 75% of the original KV cache. We observe a flatter accuracy trend as the model size grows, which is exceptionally encouraging. This suggests that Scissorhands can scale with the model size. Downstream tasks are usually less sensitive to perturbation and bear more variance in terms of accuracy. We evaluate the 5-shot setting and 1\(\times\) denotes the original OPT model. For Winogrande and MathQA, accuracy is maintained even after 5\(\times\) compression for OPT-66B. Similar to the language modeling setting, Scissorhands performs better at larger models. Generally, accuracy is maintained with 15% - 30% of the original KV cache size.

#### Ablation on the Importance of Pivotal Tokens

We divide C4 into three subsets depending on the sequence length. C4-[256-512] contains data sequences that are longer than 256 tokens but less than 512 tokens. C4-[512-1024] contains data sequences longer than 512 tokens but less than 1024 tokens. C4-[1024-2048] contains data sequences that are longer than 1024 tokens but less than 2048. Results are summarized in Table 3. Local Windows refers to only keeping tokens in the recent window, while Scissorhandskeeps both recent tokens and pivotal tokens. We observe the perplexity of the full model degrades slightly with the growing sequence length. At all sequence lengths, Scissorhands's performance is comparable against the full cache model, while Local Window incurs a significant quality loss. This demonstrates that keeping the pivotal tokens is important to reserve model performance. It is also interesting to note that at longer sequence lengths, the local window has higher accuracy. This also shows at longer sequence length, the attention mechanism in current architecture tends to focus on recent context.

#### Compatible with 4-bit Quantization

We test the compatibility of quantization and Scissorhands at \(2\times\) compression. We adopt 4-bit quantization following [7]. Even Hellaswag is most sensitive based on Figure 3, adding quantization doesn't introduce compounded errors.

#### Ablation on Attention Score Error.

We present the change ratio in attention score between original OPT-13B and Scissorhands OPT-13B at \(3\times\) compression on C4 in Figure 4.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & [256 - 512] & [512 - 1024] & [1024- 2048] \\ \hline OPT-13B & 8.7968 & 9.1017 & 9.3005 \\ \hline OPT-13B + Local Window & 81.8297 & 29.3823 & 15.5883 \\ \hline OPT-13B + Scissorhands & 8.7972 & 9.1011 & 9.3009 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Perplexity on C4 with different sequence lengths.

Figure 3: Accuracy trend of Scissorhands on language modeling dataset and downstream tasks with different KV cache compression. In general, Scissorhands incurs no accuracy drop until \(5\times\) compression on OPT-66B.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline  & [256 - 512] & [512- 1024] & [1024- 2048] \\ \hline OPT-13B & 8.7968 & 9.1017 & 9.3005 \\ \hline OPT-13B + Local Window & 81.8297 & 29.3823 & 15.5883 \\ \hline OPT-13B + Scissorhands & 8.7972 & 9.1011 & 9.3009 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Applying 4-bit quantization on top of Scissorhands on Hellaswag.

We observe the attention score generated from Scissorhands is almost the same as the original KV cache, which also echoes Theorem 4.1. The change ratio is calculated as \(\frac{\alpha_{s}-\alpha_{a}}{\alpha_{o}}\) where \(\alpha_{s}\) is the Scissorhands attention score and \(\alpha_{o}\) is the original score. From Figure 4, we observe that the change ratio is centered around 0. -1 indicating that \(\alpha_{s}\) is significantly smaller compared to the original, suggesting that a small portion of the important tokens are dropped in the cache. To explain the above observation of Scissorhands, we denote the \(n\) number of tokens with the highest score as \(\{x_{t}^{top_{-}n}\}_{t=0}^{T}\). Then, for any other sets of tokens \(\{x_{t}^{\prime}\}_{t=0}^{T}\) that has no greater than \(n\) tokens, we can easily prove that \(similarity\left(x_{t}^{topB},x_{t}\right)\leq(x_{t}^{\prime},x_{t})\). Thus, Scissorhands gives the most similar output as the original model at all layers.

## 6 Discussion, Limitation, and Future Work

We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section 3.1. As shown in Figure 5, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problems in language generation, such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.

Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we foresee no negative impacts.

## 7 Conclusion

Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed Scissorhands to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of \(5\times\) in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of Scissorhands with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.

## 8 Acknowledgement

We would like to thank the anonymous reviewers for their helpful discussions and feedback. This work is supported by NSF-CCS-2211815, ONR-DURIP and NSF-BIGDATA-1838177.

Figure 4: Score between OPT and Scissorhands.

Figure 5: We plot the attention map corresponding to Section 3.1 but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.

## References

* [1]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.
* [2]P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. (2022) Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. Cited by: SS1.
* [3]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [4]S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer (2022) Rethinking the role of demonstrations: what makes in-context learning work?. arXiv preprint arXiv:2202.12837. Cited by: SS1.
* [5]S. CY Chan, A. Santoro, A. K. Lampinen, J. X. Wang, A. K. Singh, P. H. Richemond, J. McClelland, and F. Hill (2022) Data distributional properties drive emergent in-context learning in transformers. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [6]R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean (2022) Efficiently scaling transformer inference. Cited by: SS1.
* [7]Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, D. Y. Fu, Z. Xie, B. Chen, C. Barrett, J. E. Gonzalez, P. Liang, C. Re, I. Stoica, and C. Zhang (2023) High-throughput generative inference of large language models with a single gpu. Cited by: SS1.
* [8]Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He (2022) ZeroQAT: efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861. Cited by: SS1.
* [9]G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee (2022) nuqmm: quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557. Cited by: SS1.
* [10]T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer (2022) LLM. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339. Cited by: SS1.
* [11]E. Frantar and D. Alistarh (2023) Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774. Cited by: SS1.
* [12]E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh (2022) Gptq: accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323. Cited by: SS1.
* [13]H. Bansal, K. Gopalakrishnan, S. Dingliwal, S. Bodapati, K. Kirchhoff, and D. Roth (2022) Rethinking the role of scale for in-context learning: an interpretability-based case study at 66 billion scale. arXiv preprint arXiv:2212.09095. Cited by: SS1.
* [14]G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han (2022) Smoothquant: accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438. Cited by: SS1.
* [15]OpenAI. Gpt-4 technical report (2023). Cited by: SS1.
* [16]N. Kitaev, L. Kaiser, and A. Levskaya (2020) Reformer: the efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, External Links: Link Cited by: SS1.
* [17]S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma (2020) Linformer: self-attention with linear complexity. arXiv preprint arXiv:2006.04768. Cited by: SS1.

* [18] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for efficient neural network training. In _International Conference on Learning Representations_, 2021.
* [19] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. _Advances in Neural Information Processing Systems_, 34:17413-17426, 2021.
* [20] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [21] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.
* [22] Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019.
* [23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.
* [24] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
* [25] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_, 2018.
* [26] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.
* [27] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019.
* [28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [29] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piga: Reasoning about physical commonsense in natural language. In _Thirty-Fourth AAAI Conference on Artificial Intelligence_, 2020.
* [30] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. Winogrande: An adversarial winograd schema challenge at scale. In _Communications of the ACM_, 2019.
* [31] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation. In _Version v0. 0.1. Sept._ Zenodo, September 2021.

[MISSING_PAGE_EMPTY:13]

### Cross Layer Cosine Similarity

In Section 3.3, our analysis assumes a large cosine similarity between the input and output of \(\mathcal{F}\). Here, we provide empirical evidence to support such an assumption in Figure 10. Because of the residual connection in \(\mathcal{F}\) and the domination of \(x\), the cosine similarity between \(x\) and \(\mathcal{F}(x)\) is extremely high.

### Generated examples with Scissor

Figure 8: Attention Map at Layer 15

Figure 9: Attention Map at Layer 20

## Appendix B Proofs

### Proof of Theorem 3.1

We consider the token generation process of a simplified model: a single-layer transformer model with single-head attention.

\[x_{t+1}=\mathcal{F}\left(a_{t}\right)\text{, where }a_{t}=\texttt{softmax}\left( \nicefrac{{1}}{{t}}\cdot x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top}\right)X_{t-1}W_{ V}W_{O} \tag{5}\]

\(x_{t}\in\mathbb{R}^{1\times d}\) is a row vector. \(X_{t-1}\in\mathbb{R}^{(t-1)\times d}\) denotes the aggregation of \(x_{1},\dots,x_{t-1}\), where the \(j\)th row is \(x_{j}\). \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{d\times p}\) and \(W_{O}\in\mathbb{R}^{p\times d}\) are the attention weights. Lastly, \(\mathcal{F}:\mathbb{R}^{1\times d}\rightarrow\mathbb{R}^{1\times d}\) denotes the MLP block following attention block, a two-layer MLP with skip connections, given by

\[\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x) \tag{6}\]

We are interested in the attention scores \(\alpha_{t}=\texttt{softmax}(1/t\cdot x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})\). Notice that \(\alpha_{t,j}\) scales with \(x_{t}W_{Q}W_{K}^{\top}x_{j}^{\top}\). We first re-state the Theorem 3.1 below.

Figure 10: \(x\) and \(\mathcal{F}(x)\) is high in cosine similarity

\begin{table}
\begin{tabular}{l|l} \hline \hline  & Hi. Does Asus ever plan to update the intel and nvidia display drivers on the UL30VT laptop? \\  & The only downloadable drivers from the Asus website are at least 3 years behind and new features for both graphics chips have been released since then. I sent a message to Asus but no response was given. Intel and Nvidia will not allow the user to download the new drivers directly. problem is the drivers won’t install. states something about your notebook needs custom drivers from the manufacturer due to customized this art or the other. \\ \hline OPT-13B & I have the same problem. I have a laptop with an Intel HD Graphics 4000 and an Nvidia GeForce GT 740M. I can’t install the drivers from the Asus website. I have to use the drivers from the Nvidia website. \\ \hline
3x Compression & I have the same problem. I have a laptop with an Intel HD Graphics 4000 and an Nvidia GeForce GT 740M. I can’t install the drivers from the Asus website. I have to use the drivers from the Nvidia website. \\ \hline
6x Compression & I have the same problem. I have a custom laptop and I can’t install the drivers. \\ \hline \hline \end{tabular}
\end{table}
Table 5: Generated examples using OPT-13B with full cache and Scissorhandsat different compression ratio.

**Theorem B.1**.: _Let \(A=W_{V}W_{O}W_{Q}W_{K}^{+}\) and let \(\lambda_{K},\lambda_{Q},\lambda_{V},\lambda_{O}\) denote the largest singular values of \(W_{K},W_{Q},W_{V},W_{O}\), respectively. Consider the transformer in (5) with normalized inputs \(\left\|x_{t}\right\|_{2}=1\) for all \(t\). Let \(c,\epsilon>0\) be constants. Assume that \(a_{t}x_{t+1}^{\top}\geq(1-\delta)\left\|a_{t}\right\|_{2}\) with \(\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}} \right)^{2}\). Then for all \(x_{\ell}\) satisfying \(x_{\ell}Ax_{\ell}^{\top}\geq c\) and \(x_{\ell}Ax_{\ell}\geq\epsilon^{-1}\max_{j\in[t],j\neq\ell}x_{j}Ax_{\ell}^{\top}\), it holds that_

\[\frac{x_{\ell}Ax_{\ell}^{\top}}{\left\|a_{t}\right\|_{2}}(\alpha_{t,\ell}-3 \epsilon)\leq x_{t+1}W_{Q}W_{K}^{\top}x_{j}^{\top}\leq\frac{x_{\ell}Ax_{\ell}^ {\top}}{\left\|a_{t}\right\|_{2}}(\alpha_{t,\ell}+3\epsilon) \tag{7}\]

As a preparation of the proof, we first show two lemmas.

**Lemma B.1**.: _Let \(x_{1},x_{2}\in\mathbb{R}^{1\times m}\) satisfies \(\left\|x_{1}\right\|_{2}=\left\|x_{2}\right\|_{2}=1\) and \(x_{1}x_{2}^{\top}\geq 1-\delta\) for some \(\delta\in(0,1)\). Then for all \(y\in\mathbb{R}^{1\times m}\) we have_

\[\left|x_{1}y^{\top}-x_{2}y^{\top}\right|\leq\sqrt{2\delta}\left\|y\right\|_{2}\]

Proof.: Let \(x_{2}=x_{2}^{\parallel}+x_{2}^{\perp}\) where

\[x_{2}^{\parallel}=x_{1}x_{2}^{\top}\cdot x_{1};\quad x_{2}^{\perp}=x_{2}-x_{2} ^{\parallel}\]

Then it is easy to see that \(x_{2}^{\perp}x_{1}^{\top}=0\). By the Pythagorean Theorem, we have

\[\left\|x_{2}^{\perp}\right\|_{2}^{2}=\left\|x_{2}\right\|_{2}^{2}-\left\|x_{2} ^{\parallel}\right\|_{2}^{2}=\delta(2-\delta)\]

Therefore, we have

\[\left\|x_{1}-x_{2}\right\|_{2}^{2} =\left\|(x_{1}-x_{2}^{\parallel})-x_{2}^{\perp}\right\|_{2}^{2}\] \[=\left\|\left(1-x_{1}x_{2}^{\top}\right)x_{1}-x_{2}^{\perp}\right\| _{2}^{2}\] \[=\left(1-x_{1}x_{2}^{\top}\right)^{2}+\left\|x_{2}^{\perp}\right\| _{2}^{2}\] \[=2\delta\]

Thus, the Cauchy-Schwarz inequality implies

\[\left|x_{1}y^{\top}-x_{2}y^{\top}\right|\leq\left\|x_{1}-x_{2}\right\|_{2} \cdot\left\|y\right\|_{2}=\sqrt{2\delta}\left\|y\right\|_{2}\]

**Lemma B.2**.: _Let \(\ell\in[t]\) be given. Suppose that \(x_{\ell}Ax_{\ell}^{\top}>\epsilon^{-1}\left|x_{j}Ax_{\ell}^{\top}\right|\) for all \(j\neq\ell\). Then we have \(\left(\mathcal{S}(t)_{\ell}-\epsilon\right)x_{\ell}^{\top}ax_{\ell}\leq x_{ \ell}^{\top}W_{K}^{\top}W_{Q}a_{t}\leq\left(\mathcal{S}(t)_{\ell}+\epsilon \right)x_{\ell}^{\top}ax_{\ell}\)_

Proof.: Notice that

\[a_{t}=\alpha_{t}X_{t-1}W_{V}W_{O}=\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j} \right)W_{V}W_{O}\]

Thus, we have

\[a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}=\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j} \right)W_{V}W_{O}W_{Q}W_{K}^{\top}x_{\ell}^{\top}=\sum_{j=1}^{t-1}\alpha_{t,j} x_{j}Ax_{\ell}^{\top}\]

Therefore

\[\left|a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\alpha_{t,\ell}x_{ \ell}Ax_{\ell}^{\top}\right| =\left|\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}x_{j}Ax_{\ell}^{ \top}\right|\] \[\leq\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}\left|x_{j}Ax_{\ell}^{ \top}\right|\] \[\leq\epsilon x_{\ell}Ax_{\ell}^{\top}\sum_{j=1,j\neq\ell}^{t-1} \alpha_{t,j}\] \[\leq\epsilon x_{\ell}Ax_{\ell}^{\top}\]where in the second inequality we use \(\epsilon^{-1}\left|x_{j}Ax_{\ell}^{\top}\right|\leq x_{\ell}Ax_{\ell}^{\top}\) and in the third inequality we use \(\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}\leq\sum_{j=1}^{t-1}\alpha_{t,j}=1\). This implies that

\[\left(\alpha_{t,\ell}-\epsilon\right)x_{\ell}Ax_{\ell}^{\top}\leq a_{t}W_{Q}W_{ K}^{\top}x_{\ell}^{\top}\leq\left(\alpha_{t,\ell}+\epsilon\right)x_{\ell}Ax_{ \ell}^{\top}\]

Now we proceed to the main body of the proof. Assume that \(\left\|x_{\ell}\right\|_{2}=1\) for all \(\ell\). Using Lemma (B.1), if \(a_{t}x_{t+1}^{\top}\geq\left(1-\delta\right)\left\|a_{t}\right\|_{2}\), then we have

\[\left|\left\|a_{t}\right\|_{2}^{-1}a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-x_{t+ 1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}\right|\leq\sqrt{2\delta}\left\|W_{Q}W_{K}^{ \top}x_{\ell}^{\top}\right\|_{2}\]

Recall that \(\lambda_{Q},\lambda_{K}\) are the maximum singular values of \(W_{Q}\) and \(W_{K}\), respectively. Then it holds that \(\left\|W_{Q}W_{K}^{\top}x_{\ell}^{\top}\right\|_{2}\leq\lambda_{Q}\lambda_{K} \left\|x_{\ell}\right\|_{2}\). Using \(\left\|x_{\ell}\right\|_{2}=1\), we have

\[\left|\left\|a_{t}\right\|_{2}^{-1}a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-x_{t+ 1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}\right|\leq\sqrt{2\delta}\lambda_{Q}\lambda_{K}\]

Notice that

\[\left\|a_{t}\right\|_{2} =\left\|\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W_{Q}\right\|\] \[\leq\lambda_{O}\lambda_{V}\left\|\sum_{j=1}^{t-1}\alpha_{t,j}x_{j }\right\|_{2}\] \[\leq\lambda_{O}\lambda_{V}\sum_{j=1}^{t-1}\alpha_{t,j}\left\|x_{j }\right\|_{2}\] \[=\lambda_{O}\lambda_{V}\]

Then since \(\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}} \right)^{2}\), we have

\[\left|\left\|a_{t}\right\|_{2}^{-1}a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-x_{t+ 1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}\right|\leq\frac{2c\epsilon}{\lambda_{V} \lambda_{O}}\leq\frac{2c\epsilon}{\left\|a_{t}\right\|_{2}}\]

Since by Lemma (B.2), we have

\[\left|a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\alpha_{t,\ell}x_{\ell}Ax_{\ell}^{ \top}\right|\leq\epsilon x_{\ell}^{\top}ax_{\ell}\]

It must hold that

\[\left|x_{t+1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\left\|a_{t+1}\right\|_{2}^{-1} \alpha_{t,\ell}x_{\ell}Ax_{\ell}^{\top}\right|\leq\frac{\epsilon}{\left\|a_{t }\right\|_{2}}x_{\ell}^{\top}ax_{\ell}+\frac{2c\epsilon}{\left\|a_{t}\right\| _{2}}\]

Since \(x_{\ell}^{\top}ax_{\ell}\geq c\), it holds that

\[\frac{2c\epsilon}{\left\|a_{t}\right\|_{2}}\leq\frac{2\epsilon}{\left\|a_{t} \right\|_{2}}x_{\ell}^{\top}ax_{\ell}\]

which implies that

\[\left|x_{t+1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\left\|a_{t}\right\|_{2}^{-1} \alpha_{t,\ell}x_{\ell}Ax_{\ell}^{\top}\right|\leq\frac{3\epsilon}{\left\|a_ {t}\right\|_{2}}x_{\ell}^{\top}ax_{\ell}\]

Therefore

\[\frac{x_{\ell}Ax_{\ell}^{\top}}{\left\|a_{t}\right\|_{2}}(\alpha_{t,\ell}-3 \epsilon)\leq x_{t+1}W_{Q}W_{K}^{\top}x_{\ell}^{\top}\leq\frac{x_{\ell}Ax_{ \ell}^{\top}}{\left\|a_{t}\right\|_{2}}(\alpha_{t,\ell}+3\epsilon)\]

### Proof of Theorem 4.1

Let \(\{\tilde{x}_{t}\}_{t=0}^{T}\) denote the tokens generated by the transformer with budget KV cache as in Algorithm 2 with \(m=1\):

\[\tilde{x}_{t+1}=\mathcal{F}\left(\tilde{a}_{t}\right)\text{, where }\tilde{a}_{t}= \texttt{softmax}\left(\nicefrac{{1}}{{t}}\cdot\tilde{x}_{t}W_{Q}\tilde{\mathcal{ K}}_{t}^{\top}\right)\tilde{\mathcal{V}}_{t}^{\top}W_{O}\]

Notice that when \(m=1\), i.e., in each iteration, we drop one token with the lowest score, the cache will always maintain \(B\) tokens. If the ranking of the attention scores does not change in each iteration, Algorithm 2 will always drop tokens with the smallest attention scores.

For reference purposes, let \(\{x_{t}\}_{t=0}^{T}\) denote the tokens generated by a vanilla transformer defined in (5). We re-state Theorem 4.1 below, which bounds the difference \(\left\lVert x_{t}-\tilde{x}_{t}\right\rVert_{2}\).

**Theorem B.2**.: _Let \(\lambda_{1},\lambda_{2}\) denote the largest singular values of \(W_{1}\) and \(W_{2}\) in (6). Let_

\[\beta_{t,j}=\frac{\exp\left(\nicefrac{{1}}{{t}}\cdot\tilde{x}_{t}W_{Q}W_{K}^{ \top}\tilde{x}_{t}^{\top}\right)}{\sum_{i=1}^{t-1}\exp\left(\nicefrac{{1}}{{t }}\cdot\tilde{x}_{t}W_{Q}W_{K}^{\top}\tilde{x}_{i}^{\top}\right)}\]

_and assume that each \(\beta_{t,j}=cv_{t,j}\), where \(v_{t,j}\) are sampled from a power-law distribution with pdf \(f(x)=c(x+b)^{-k}\). Suppose that \(\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq \frac{1}{2}\). Let \(T_{\min}\) and \(T_{\max}\) denote the starting and maximum sequence lengths, respectively, and let \(B\leq T_{\max}\) denote the budget as in Algorithm 2. If for all \(t\in[T_{\min},T_{\max}]\), \(S_{t}\) contains only tokes with at most the largest \(B\) values of \(\beta_{t,j}\), that is, \(|S_{t}|=B\) and \(\min_{j\in S_{t}}\beta_{t,j}\geq\max_{j\notin S_{t}}\beta_{t,j}\), then for all \(\epsilon\in(0,1)\), with probability at least \(1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}} \right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-B/T_{\max})^{2}}{(1-\epsilon )^{2}}\right)\), the following error bound must hold for all \(t\in[T_{\min},T_{\max}]\)

\[\mathbb{E}\left[\left\lVert x_{t}-\tilde{x}_{t}\right\rVert_{2}\right]\leq\frac {2.1(1-\nicefrac{{B}}{{T_{\max}}})}{(1-\epsilon)^{2}}\left(k-(k-1)\left(\frac{ 1-\epsilon}{\nicefrac{{B}}{{T_{\max}}}-\epsilon}\right)^{\nicefrac{{1}}{{(k -1)}}}\right)\]

Define \(m_{k,j}=\mathbb{I}\left\{j\in S_{t}\right\}\). With the definition of \(m_{k,j}\), \(\tilde{a}_{t}\) can be written as

\[\tilde{a}_{t}=\left(\sum_{j=1}^{t-1}\tilde{\alpha}_{t,j}\tilde{x}_{j}\right)W_ {V}W_{O};\quad\tilde{\alpha}_{t,j}=\frac{m_{k,j}\exp\left(\nicefrac{{1}}{{t}} \cdot\tilde{x}_{t}W_{Q}W_{K}^{\top}\tilde{x}_{j}^{\top}\right)}{\sum_{i=1}^{t -1}m_{k,j}\exp\left(\nicefrac{{1}}{{t}}\cdot\tilde{x}_{t}W_{Q}W_{K}^{\top} \tilde{x}_{i}^{\top}\right)} \tag{8}\]

Our first lemma shows the Lipschitzness of the attention module.

**Lemma B.3**.: _Consider two sequences of tokens \(\{x_{i}\}_{i=1}^{t}\) and \(\{y_{i}\}_{i=1}^{t}\) where \(\left\lVert x_{i}\right\rVert_{2}=\left\lVert y_{i}\right\rVert_{2}=1\) for all \(i\in[t]\). Define \(X_{t-1},Y_{t-1}\in\mathbb{R}^{(t-1)\times d}\) as the matrices whose \(i\)th row are \(x_{i}\) and \(y_{i}\), respectively. Let \(\Delta_{t}=\left\lVert x_{t}-y_{t}\right\rVert_{2}\). Then we have_

\[\left\lVert\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{ \top}\right)_{2}-\texttt{softmax}\left(\frac{1}{t}y_{t}W_{Q}W_{K}^{\top}Y_{t- 1}^{\top}\right)\right\rVert_{2}\leq 2\frac{\sqrt{t-1}}{t}\lambda_{Q}\lambda_{K} \Delta_{t}\]

Proof.: We can decompose the difference as

\[\left\lVert\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{ \top}X_{t-1}^{\top}\right)-\texttt{softmax}\left(\frac{1}{t}y_{t}W_{Q}W_{K}^{ \top}Y_{t-1}^{\top}\right)\right\rVert_{2}\] \[\qquad\leq\left\lVert\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W _{K}^{\top}X_{t-1}^{\top}\right)-\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K }^{\top}Y_{t-1}^{\top}\right)\right\rVert_{2}\] \[\qquad\qquad+\left\lVert\texttt{softmax}\left(\frac{1}{t}x_{t}W_{ Q}W_{K}^{\top}Y_{t-1}^{\top}\right)-\texttt{softmax}\left(\frac{1}{t}y_{t}W_{Q}W_{K}^{ \top}Y_{t-1}^{\top}\right)\right\rVert_{2}\]

By the Lipschitzness of softmax, we have

\[\left\lVert\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{ \top}X_{t-1}^{\top}\right)-\texttt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{ \top}Y_{t-1}^{\top}\right)\right\rVert_{2}\] \[\qquad\leq\frac{1}{t}\left\lVert x_{t}W_{Q}W_{K}^{\top}\left(X_{t -1}-Y_{t-1}\right)^{\top}\right\rVert_{2}\] \[\qquad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\lVert x_{t} \right\rVert_{2}\left\lVert X_{t-1}-Y_{t-1}\right\rVert_{2}\]Since \(\left\|x_{t}\right\|_{2}=1\) and \(\left\|X_{t-1}-Y_{t-1}\right\|_{2}=\left(\sum_{j=1}^{t-1}\left\|x_{j}-y_{j}\right\| _{2}\right)^{\frac{1}{2}}\leq\sqrt{t-1}\Delta_{t}\), we have

\[\left\|\mathtt{softmax}\left(x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top}\right)- \mathtt{softmax}\left(x_{t}W_{Q}W_{K}^{\top}Y_{t-1}^{\top}\right)\right\|_{2} \leq\frac{\sqrt{t-1}}{t}\lambda_{Q}\lambda_{K}\Delta_{t}\]

Similarly,

\[\left\|\mathtt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{\top}Y_{t -1}^{\top}\right)-\mathtt{softmax}\left(\frac{1}{t}y_{t}W_{Q}W_{K}^{\top}Y_{t-1 }^{\top}\right)\right\|_{2}\] \[\qquad\leq\frac{1}{t}\left\|(x_{t}-y_{t})W_{Q}W_{K}^{\top}Y_{t-1} ^{\top}\right\|_{2}\] \[\qquad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\|Y_{t-1}\right\| _{F}\left\|x_{t}-y_{t}\right\|_{2}\]

Since \(\left\|x_{t}-y_{t}\right\|_{2}=\Delta_{t}\) and \(\left\|Y_{t-1}\right\|_{2}=\sqrt{t-1}\), we have

\[\left\|\mathtt{softmax}\left(\frac{1}{t}x_{t}W_{Q}W_{K}^{\top}Y_{t -1}^{\top}\right)-\mathtt{softmax}\left(\frac{1}{t}y_{t}W_{Q}W_{K}^{\top}Y_{t-1 }^{\top}\right)\right\|_{2}\leq\frac{\sqrt{t-1}}{t}\lambda_{Q}\lambda_{K} \Delta_{t}\]

Combining the two bounds gives

\[\left\|\mathtt{softmax}\left(\frac{1}{\sqrt{t}}x_{t}W_{Q}W_{K}^{ \top}X_{t-1}^{\top}\right)-\mathtt{softmax}\left(\frac{1}{\sqrt{t}}y_{t}W_{Q}W_ {K}^{\top}Y_{t-1}^{\top}\right)\right\|_{2}\leq 2\frac{\sqrt{t-1}}{t} \lambda_{Q}\lambda_{K}\Delta_{t}\]

Our second lemma shows the difference between the output of the sampled and vanilla transformer when the input is the same.

**Lemma B.4**.: _Let \(\tilde{a}_{t}\) be defined as in (8). Define \(b_{t}\) as_

\[b_{t}=\left(\sum_{j=1}^{t-1}\beta_{t,j}\tilde{x}_{j}\right)W_{V}W_{O};\quad \beta_{t,j}=\frac{\exp\left(1/\imath\cdot\tilde{x}_{t}W_{Q}W_{K}^{\top}\tilde{ x}_{j}^{\top}\right)}{\sum_{i=1}^{t-1}\exp\left(1/\imath\cdot\tilde{x}_{t}W_{Q}W_ {K}^{\top}\tilde{x}_{i}^{\top}\right)} \tag{9}\]

_Assume that \(\left\|x_{j}\right\|_{2}=1\) for all \(j\in[t]\). Then we have_

\[\left\|\tilde{a}_{t}-b_{t}\right\|_{2}\leq\lambda_{V}\lambda_{O}\sum_{j\notin \mathcal{S}_{t}}\beta_{t,j}\]

Proof.: A direction computation yields

\[\tilde{a}_{t}-b_{t}=\left(\sum_{j=1}^{t-1}\left(\tilde{\alpha}_{t,j}-\beta_{t, j}\right)\tilde{x}_{j}\right)W_{V}W_{O}\]

Thus, \(\left\|\tilde{a}_{t}-b_{t}\right\|_{2}\) can be bounded as

\[\left\|\tilde{a}_{t}-b_{t}\right\|_{2}\leq\lambda_{V}\lambda_{O}\sum_{j=1}^{t -1}\left(\tilde{\alpha}_{t,j}-\beta_{t,j}\right)\left\|\tilde{x}_{j}\right\|_{ 2}=\lambda_{V}\lambda_{O}\sum_{j=1}^{t-1}\left(\tilde{\alpha}_{t,j}-\beta_{t,j}\right)\]

since \(\left\|\tilde{x}_{j}\right\|_{2}=1\) for all \(j\in[t]\). Now we analyze \(\tilde{\alpha}_{t,j}-\beta_{t,j}\). Let \(\hat{S}_{t}=S_{t}\setminus\{t\}\). Then \(m_{k,j}=1\) if and only if \(j\in\hat{S}_{t}\). For convenience, let \(r_{t,j}=\sfrac{1}{\imath}\prime\cdot\tilde{x}_{t}W_{Q}W_{K}^{\top}\tilde{x}_{j }^{\top}\). Thus, \(\beta\) can be written as

\[\beta_{t,j}=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp\left(r_{ t,i}\right)+\sum_{i\notin\hat{S}_{t}}\exp\left(r_{t,i}\right)}\]

Furthermore, for all \(j\notin\hat{S}_{t}\), we have \(\tilde{\alpha}_{t,j}=0\). For all \(j\in\hat{S}_{t}\), we have

\[\tilde{\alpha}_{t,j}=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp \left(r_{t,i}\right)}\]Therefore, for all \(j\in\hat{S}_{t}\), we have

\[\beta_{t,j}-\tilde{\alpha}_{t,j} =\exp\left(r_{t,j}\right)\cdot\frac{\sum_{i\notin\hat{S}_{t}}\exp \left(r_{t,i}\right)}{\left(\sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right) \right)\left(\sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right)+\sum_{i\notin\hat{S }_{t}}\exp\left(r_{t,i}\right)\right)}\] \[=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp\left( r_{t,i}\right)}\cdot\frac{\sum_{i\notin\hat{S}_{t}}\exp\left(r_{t,i}\right)}{ \sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right)+\sum_{i\notin\hat{S}_{t}}\exp \left(r_{t,i}\right)}\] \[=\tilde{\alpha}_{t,j}\sum_{i\notin\hat{S}_{t}}\beta_{t,j}\]

Therefore, the bound of \(\left\|\tilde{a}_{t}-b_{t}\right\|_{2}\) can be written as

\[\left\|\tilde{a}_{t}-b_{t}\right\|_{2}\leq\lambda_{V}\lambda_{O}\left(\sum_{j \in\hat{S}_{t}}^{t-1}\tilde{\alpha}_{t,j}\sum_{i\notin\hat{S}_{t}}\beta_{t,j} -\sum_{j\notin\hat{S}_{t}}\beta_{t,j}\right)=2\lambda_{V}\lambda_{O}\sum_{j \notin\hat{S}_{t}}\beta_{t,j}\]

where the last equality follows from \(\sum_{j\in\hat{S}_{t}}\tilde{\alpha}_{t,j}=1\). 

Our last lemma shows the Lipschitzness of the MLP in (6).

**Lemma B.5**.: _Let \(\lambda_{1},\lambda_{2}\) denote the largest singular values of \(W_{1},W_{2}\) in (6). For all \(x_{1},x_{2}\in\mathbb{R}^{d}\), we have_

\[\left\|\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\|\leq\left(1+\lambda_{1} \lambda_{2}\right)\left\|x_{1}-x_{2}\right\|_{2}\]

Proof.: Direct computation yields

\[\left\|\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\| =\left\|(x_{1}+W_{2}\mathtt{relu}\left(W_{1}x_{1}\right))-(x_{2} +W_{2}\mathtt{relu}\left(W_{1}x_{2}\right))\right\|\] \[\leq\left\|x_{1}-x_{2}\right\|_{2}+\lambda_{2}\left\|\mathtt{relu }\left(W_{1}x_{1}\right)-\mathtt{relu}\left(W_{1}x_{2}\right)\right\|\] \[\leq\left\|x_{1}-x_{2}\right\|_{2}+\lambda_{2}\left\|W_{1}\left(x _{1}-x_{2}\right)\right\|_{2}\] \[\leq\left\|x_{1}-x_{2}\right\|_{2}+\lambda_{1}\lambda_{1}\left\| x_{1}-x_{2}\right\|_{2}\] \[=\left(1+\lambda_{1}\lambda_{2}\right)\left\|x_{1}-x_{2}\right\| _{2}\]

where in the third inequality we use the fact that \(\mathtt{relu}(\cdot)\) is \(1\)-Lipschitz. 

Now we turn to the proof of our main theorem. Combining all of the results, we have

\[a_{t}-\tilde{a}_{t} =\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W_{O}-\left( \sum_{j=1}^{t-1}\tilde{\alpha}_{t,j}\tilde{x}_{j}\right)W_{V}W_{O}\] \[=\underbrace{\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W _{O}-\left(\sum_{j=1}^{t-1}\alpha_{t,j}\tilde{x}_{j}\right)W_{V}W_{O}}_{ \mathcal{T}_{1}}\] \[\qquad\quad+\underbrace{\left(\sum_{j=1}^{t-1}\alpha_{t,j} \tilde{x}_{j}\right)W_{V}W_{O}-\left(\sum_{j=1}^{t-1}\tilde{\alpha}_{t,j} \tilde{x}_{j}\right)W_{V}W_{O}}_{\mathcal{T}_{2}}\] \[\qquad\quad+\underbrace{\left(\sum_{j=1}^{t-1}\beta_{t,j}\tilde{ x}_{j}\right)W_{V}W_{O}-\left(\sum_{j=1}^{t-1}\tilde{\alpha}_{t,j}\tilde{x}_{j} \right)W_{V}W_{O}}_{\mathcal{T}_{3}}\]

Therefore, by triangle inequality, we have

\[\left\|a_{t}-\tilde{a}_{t}\right\|_{2}\leq\left\|\mathcal{T}_{1}\right\|_{2}+ \left\|\mathcal{T}_{2}\right\|_{2}+\left\|\mathcal{T}_{3}\right\|_{2} \tag{10}\]To start, the magnitude of \(\mathcal{T}_{1}\) can be bounded as

\[\left\|\mathcal{T}_{1}\right\|_{2} =\left\|\left(\sum_{j=1}^{t-1}\alpha_{t,j}(x_{t,j}-\tilde{x}_{t,j}) \right)W_{V}W_{O}\right\|_{2}\] \[\leq\lambda_{V}\lambda_{O}\left\|\sum_{j=1}^{t-1}\alpha_{t,j}(x_{t,j}-\tilde{x}_{t,j})\right\|\] \[\leq\lambda_{V}\lambda_{O}\sum_{j=1}^{t-1}\alpha_{t,j}\left\|x_{t,j}-\tilde{x}_{t,j}\right\|_{2}\] \[\leq\lambda_{V}\lambda_{O}\Delta_{t}\sum_{j=1}^{t-1}\alpha_{t,j}\] \[=\lambda_{V}\lambda_{O}\Delta_{t}\]

where in the third inequality we use \(\left\|x_{t,j}-\tilde{x}_{t,j}\right\|_{2}=\Delta_{t}\) and in the last equality we use \(\sum_{j=1}^{t-1}\alpha_{t,j}=1\). To bound the magnitude of \(\mathcal{T}_{2}\), we apply Lemma B.3, which shows that \(\left\|\alpha_{t}-\beta_{t}\right\|\leq 2\frac{\sqrt{t-1}}{t}\lambda_{Q} \lambda_{K}\Delta_{t}\) to get that

\[\left\|\mathcal{T}_{2}\right\|_{2} =\left\|\left(\sum_{j=0}^{t-1}(\alpha_{t,j}-\beta_{t,j})\tilde{x}_ {j}\right)W_{V}W_{O}\right\|_{2}\] \[\leq\lambda_{V}\lambda_{O}\left\|\left(\sum_{j=0}^{t-1}(\alpha_{t,j}-\beta_{t,j})\tilde{x}_{j}\right)\right\|_{2}\] \[\leq\lambda_{V}\lambda_{O}\sum_{j=0}^{t-1}\left|\alpha_{t,j}- \beta_{t,j}\right|\left\|\tilde{x}_{j}\right\|_{2}\] \[\leq\lambda_{V}\lambda_{O}\left\|\alpha_{t}-\beta_{t}\right\|_{1}\] \[\leq\sqrt{t-1}\lambda_{V}\lambda_{O}\left\|\alpha_{t}-\beta_{t} \right\|_{2}\] \[\leq 2\left(1-\frac{1}{t}\right)\lambda_{Q}\lambda_{K}\lambda_{V} \lambda_{O}\Delta_{t}\]

Lastly, to bound the magnitude of \(\mathcal{T}_{3}\), we use Lemma B.4 to get that

\[\left\|\mathcal{T}_{3}\right\|_{2}\leq 2\lambda_{V}\lambda_{O}\sum_{j\notin \mathcal{S}_{t}}\beta_{t,j}\]

Putting things together for (10), we have

\[\left\|a_{t}-\tilde{a}_{t}\right\|_{2}\leq\lambda_{V}\lambda_{O}\left(2\sum_{ j\notin\mathcal{S}_{t}}\beta_{t,j}+\left(2\lambda_{Q}\lambda_{K}+1\right) \Delta_{t}\right)\]

By Lemma B.5 we can further show that

\[\left\|x_{t+1}-\tilde{x}_{t+1}\right\|_{2}\leq(1+\lambda_{1}\lambda_{2}) \lambda_{V}\lambda_{O}\left(2\sum_{j\notin\mathcal{S}_{t}}\beta_{t,j}+\left(2 \lambda_{Q}\lambda_{K}+1\right)\Delta_{t}\right)\]

By Theorem B.3, we have that with probability at least \(1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}} \right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-B/T_{\max})^{2}}{(1-\epsilon )^{2}}\right)\), it holds for all \(t\in[T_{\min},T_{\max}]\) that

\[\mathbb{E}\left[\sum_{j\notin\mathcal{S}_{t}}\beta_{t,j}\right]\leq\frac{(1-B /T_{\max})}{0.98(1-\epsilon)^{2}}\left(k-(k-1)\left(\frac{1-\epsilon}{B/T_{ \max}}-\epsilon\right)^{\frac{1}{k-1}}\right):=\Delta_{\max}\]Given that \(\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|\right]\leq 2\Delta_{\max}\), we have

\[\mathbb{E}\left[\left\|x_{t+1}-\tilde{x}_{t+1}\right\|_{2}\right] \leq(1+\lambda_{1}\lambda_{2})\lambda_{V}\lambda_{O}\left(2\Delta_ {\max}+2\left(2\lambda_{Q}\lambda_{K}+1\right)\Delta_{\max}\right)\] \[\leq 4\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_ {Q}\lambda_{K})\Delta_{\max}\]

Thus, as long as \(\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq \frac{1}{2}\), we can guarantee that

\[\mathbb{E}\left[\left\|x_{t+1}-\tilde{x}_{t+1}\right\|_{2}\right]\leq 2\Delta_{ \max}\]

Thus, for all \(t\in[T_{\min},T_{\max}]\), we have that

\[\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|_{2}\right]\leq\frac{2.1(1- \sfrac{B}{T_{\max}}-1)}{(1-\epsilon)^{2}}\left(k-(k-1)\left(\frac{1-\epsilon} {\sfrac{B}{T_{\max}}-\epsilon}\right)^{\frac{1}{k-1}}\right)\]

### Budgeted Cache

**Theorem B.3**.: _Let \(\beta_{t,j}\) be sampled from some power-law distribution \(f(x)=c(x+b)^{-\gamma}\) with support on \([0,u-b)\) for some \(k>2\) and \(u\geq 5b\). Let \(S_{t}\) be defined in Theorem B.2, and define \(\hat{S}_{t}=S_{t}\setminus\{t\}\). Then with probability at least \(1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}} \right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-B)^{2}}{(1-\epsilon)^{2}}\right)\) it holds for all \(t\in T\) that_

\[\mathbb{E}\left[\sum_{i\notin S_{t}}\beta_{t,j}\right]\leq\frac{(1-\sfrac{B}{T _{\max}})}{0.98(1-\epsilon)^{2}}\left(k-(k-1)\left(\frac{1-\epsilon}{\sfrac{B}{ T_{\max}}-\epsilon}\right)^{\frac{1}{k-1}}\right) \tag{11}\]

We consider the case of maintaining a budget of \(B\) by dropping the smallest \(\beta_{t,j}\)'s. Assume that \(v_{j}\) has pdf \(f(x)=c(x+b)^{-k}\) with support on \([0,u-b)\). To make things precise, we first compute \(c\)

\[c=\left(\int_{0}^{u-b}(x+b)^{-k}dx\right)^{-1}=\frac{k-1}{b^{1-k}-u^{1-k}}\]

To start, we notice that

\[\int x(x+b)^{-k}=-\frac{(x+b)^{1-k}((k-1)x+b)}{(k-1)(k-2)}:=g(x)\]

Let \(C=\sum_{j=1}^{t-1}v_{j}\), then the expectation of \(C\) is

\[\mathbb{E}\left[C\right]=(t-1)\mathbb{E}\left[v_{1}\right] =(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}\int_{0}^{\infty}x(x+b)^{-k}dx\] \[=(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}(g(u)-g(0))\] \[=(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}\left(\frac{b^{2-k}}{(k-1)(k-2) }-\frac{u^{1-k}((k-1)u-(k-2)b)}{(k-1)(k-2)}\right)\] \[=\frac{t-1}{k-2}\cdot\frac{b^{2-k}-(k-1)u^{2-k}+(k-2)bu^{1-k}}{b^ {1-k}-u^{1-k}}\]

Let \(\Delta=\frac{b^{2-k}-(k-1)u^{2-k}+(k-2)bu^{1-k}}{b^{1-k}-u^{1-k}}\). By Hoeffding's inequality, we have that

\[\mathbb{P}\left(C\leq(1-\epsilon)\mathbb{E}\left[C\right]\right)\leq\exp\left( -\frac{2\epsilon^{2}\mathbb{E}\left[C\right]^{2}}{(t-1)(u-b)^{2}}\right)\]

This implies that with probability at least \(1-\exp\left(-\frac{2\epsilon^{2}\Delta^{2}(t-1)}{(k-2)^{2}(u-b)^{2}}\right)\) we have

\[C\geq(1-\epsilon)\Delta\frac{t-1}{k-2}\]

[MISSING_PAGE_FAIL:23]