# Learning Cuts via Enumeration Oracles

 Daniel Thuerck

Quantagonia

Bad Homburg, Germany

daniel.thuerck@quantagonia.com

&Boro Sofranac

Quantagonia

Bad Homburg, Germany

boro.sofranac@quantagonia.com

&Marc E. Pfetsch

Department of Mathematics, TU Darmstadt

Darmstadt

pfetsch@mathematik.tu-darmstadt.de

&Sebastian Pokutta

Zuse Institute Berlin and TU Berlin

Berlin, Germany

pokutta@zib.de

###### Abstract

Cutting-planes are one of the most important building blocks for solving large-scale integer programming (IP) problems to (near) optimality. The majority of cutting plane approaches rely on explicit rules to derive valid inequalities that can separate the target point from the feasible set. _Local cuts_, on the other hand, seek to directly derive the facets of the underlying polyhedron and use them as cutting planes. However, current approaches rely on solving Linear Programming (LP) problems in order to derive such a hyperplane. In this paper, we present a novel generic approach for learning the facets of the underlying polyhedron by accessing it implicitly via an enumeration oracle in a reduced dimension. This is achieved by embedding the oracle in a variant of the Frank-Wolfe algorithm which is capable of generating strong cutting planes, effectively turning the enumeration oracle into a separation oracle. We demonstrate the effectiveness of our approach with a case study targeting the multidimensional knapsack problem (MKP).

## 1 Introduction

In this paper, we deal with _integer programs_ (IP)

\[\max\,\{\langle c,x\rangle:Ax\leq b,\ x\in\mathds{Z}^{n}\},\] (IP)

where \(A\in\mathds{Q}^{m\times n}\), \(b\in\mathds{Q}^{m}\), and \(c\in\mathds{Q}^{n}\). Let \(P\coloneqq\{x\in\mathds{R}^{n}:Ax\leq b\}\) be the underlying polyhedron and its integer hull \(P_{I}\coloneqq\operatorname{conv}(P\cap\mathds{Z}^{n})\). We restrict attention to the case in which all variables are required to be integral, as the methods we will propose are more readily applicable to this case, but the general idea works for mixed-integer programs (MIP) as well.

Solving IPs is \(\mathcal{NP}\)-hard in general, however, surprisingly fast algorithms exist in practice [1, 50]. The most successful approach to solving IPs is based on the _branch-and-bound_ algorithm and its extensions. This algorithm involves breaking down the original problem into smaller subproblems that are easier to solve through a process known as branching. By repeatedly branching on subproblems, a search tree is obtained. The bounding step involves computing upper bounds for subproblems and pruning suboptimal nodes of the tree in order to avoid enumerating exponentially many subproblems. Upper bounds are generally computed with the help of Linear Programming (LP) relaxations

\[\max\,\{\langle c,x\rangle:x\in P\}.\] (LP)

Because the integrality constraints are relaxed, optimal solutions of (LP) provide an upper bound for the original problem (IP).

Alternatively, _cutting plane_ procedures iteratively solve LP-relaxations as long as the solution \(x^{*}\) is not integral (and thus \(x^{*}\notin P_{I}\)). To remove these solutions \(x^{*}\) from the relaxation's polyhedron, one adds cutting planes (or _cuts_) \(\left\langle\alpha,x\right\rangle\leq\beta\) with \(a\in\mathbb{Q}^{n}\), \(\beta\in\mathbb{Q}\), and \(\left\langle\alpha,x^{*}\right\rangle>\beta\). The search for such cutting planes or to determine that none exists is called the _separation problem_. The strongest cuts are those that define a _facet_, i.e., the face \(P_{I}\cap\left\{x:\left\langle\alpha,x\right\rangle=\beta\right\}\) has co-dimension 1 with respect to \(P_{I}\). When the cutting plane method is combined with branch-and-bound, the resulting algorithm is often called _branch-and-cut_. Gomory conducted foundational work in this field, demonstrating that pure cutting plane approaches can solve integer programs with rational data in a finite number of steps without the need for branching [32, 33, 34].

Gomory's initial approach to cutting planes suffered from numerical difficulties at that time, preventing pure cutting plane methods from being effective in practical applications. However, his proposed (Gomory) mixed integer (GMI) cuts are very efficient if combined with branch-and-bound (see the computational study in [11]) and still are one of the most important types of cutting planes used by contemporary solvers. As more GMI cuts are added to a problem, their incremental value tends to diminish. To address this issue, modern MIP and IP solvers use a range of techniques to generate cuts, e.g., mixed-integer-rounding (MIR) inequalities [55], knapsack covers [26, 38], flow covers [39], lift-and-project cuts [5], {0, \(\frac{1}{2}\)}-Chvatal-Gomory cuts [22], and others.

Most cutting plane separation algorithms rely on fixed formulas to derive valid inequalities that separate the target point \(x^{*}\) from the polyhedron \(P_{I}\). An alternative approach is to directly seek to derive the facets of \(P_{I}\) that separate the point \(x^{*}\). Notice that while the facets of the polyhedron \(P\) are explicitly known from the problem definition, the facets of its integer hull \(P_{I}\) are unknown in general. While the facet-defining inequalities are intuitively the strongest cuts, they can be relatively expensive to explicitly compute, limiting their applicability in practice. _Local cuts_, a type of cutting planes that try to derive facets of \(P_{I}\), approach this problem by deriving facets of \(P_{I}\) in a reduced dimension, and then _lifting_ those cuts to obtain facets in the original dimension. In this paper, we will propose a new variant of the _Frank-Wolfe_ algorithm with the goal of _learning_ the (unknown) facets of \(P_{I}\) (or at least valid inequalities) in a reduced dimension, which can then be lifted to the original dimension and be used as strong cutting planes. In our learning approach, the underlying polyhedron will only be accessed via an algorithmically simple linear optimization oracle, in contrast to existing approaches, which also need to solve LPs.

### Related Work

Local cuts have first been introduced as "Fenchel cuts" in Boyd [13, 14], who developed an algorithm to exactly separate inequalities for the knapsack polytope via the equivalence of separation and optimization. They were subsequently investigated extensively by Applegate et al. [3] for solving the traveling salesman problem (TSP). Buchheim et al. [20, 21] and Althaus et al. [2] adopted local cuts into their approaches for solving constrained quadratic 0-1 optimization problems and Steiner-tree problems, respectively. In [25], Chvatal et al. generalize the local cuts method to general MIP problems.

In the context of knapsack problems, after the aforementioned work of Boyd [13, 14], Boccia [12] introduced an approach based on local cuts, as stated by Kaparis and Letchford [47]1, who further refined the algorithm. Vasilyev presented an alternative approach with application to the generalized assignment problem in [63], see also the comprehensive computational study conducted by Avella et al. in [4]. In [64], Vasilyev et al. propose a new implementation of this approach, with the goal of making it more efficient. In [37], Gu presents an extension of the algorithm of Vasilyev et al. [64].

Footnote 1: We could not independently verify the claim as we could not access Bocciaâ€™s paper online.

Existing works on the application of learning methods in solving IP (and more generally MIP) problems can in general be divided into two categories: learning decision strategies within the solvers, and learning heurisitcs to obtain feasible (primal) solutions. Examples of the former would be learning to select branching variables [6, 31, 48], learning to select branching nodes [41], learning to select cutting planes [61], learning to optimize the usage of primal heuristics [23, 42, 49]. A typical example of the latter case would be learning methods to develop _large neighborhood search_ (LNS) heuristics [28, 60, 59]. Additionally, a number of works in the literature have focused on learning algorithms for solving specific IP problems [8, 44, 27, 51, 54, 56]. For a more detailed overview of using learning methods in IP, we refer the interested reader to [9].

### Contribution

The contributions of this paper can be summarized as follows:

1. We present an efficient, LP-free separation framework that aims to learn local cuts for IPs through the solution of subproblems. We propose to use a variant of the Frank-Wolfe [29] algorithm to solve the associated separation problem. The resulting framework is general and - given the availability of a suitable lifting method - applicable to any IP.
2. We propose a new, dynamic stopping criterion for the application of Frank-Wolfe to the separation problem at hand. This new criterion, derived by exploiting duality information, directly evaluates the strength of the resulting cut and thus dramatically decreases the number of iterations.
3. We illustrate the benefit of our approach in a case study for the multidimensional knapsack (MKP) problem, demonstrating its effectiveness. Our computational results show that embedding our method in the academic solver SCIP leads to 31% faster solving times on the instances solved to optimality, on average.

The rest of this paper is organized as follows: In Section 2, the fundamental framework of local cuts and required notation are introduced. Section 3 presents our approach for the LP-less generation procedure for local cuts. Section 4 demonstrates how the aforementioned framework can be applied to solve the multidimensional knapsack problem. Computational experiments are presented in Section 5. Finally, Section 6 summarizes conclusions and future work.

## 2 Local Cuts

To describe the idea of local cuts, assume that \(P\subset\mathds{R}^{n},n>0\), is a polytope, i.e., bounded, and full-dimensional. Then one considers a small subproblem with underlying polytope \(\tilde{P}\), which is usually an orthogonal projection of \(P\) onto a lower-dimensional space. The polytope \(\tilde{P}\) is restricted to being non-empty and its dimension \(0<k\leq n\) is chosen small enough such that integer optimization problems over \(\tilde{P}\) can be solved efficiently in practice, for example, by enumeration. Consider a projection \(\tilde{x}\) of the point to be separated \(x^{*}\) on \(\mathds{R}^{k}\). The procedure tries to generate a valid cut \(\langle\tilde{\alpha},x\rangle\leq\tilde{\beta}\) with \(\tilde{\alpha}\in\mathds{Q}^{k}\), \(\tilde{\beta}\in\mathds{Q}\), such that \(\langle\tilde{\alpha},\tilde{x}\rangle>\tilde{\beta}\), i.e., it cuts off \(\tilde{x}\) from \(\tilde{P}\). This cut can be "lifted" to the original space, which yields a cut \(\langle\alpha,x\rangle\leq\beta\) that hopefully cuts off \(x^{*}\).2

Footnote 2: The idea of local cuts is often confused with _lift-and-project_ cuts, as the two methods share some high-level ideas, such as exploring solutions in a different space and using projection. However, they are quite different. Lift-and- project methods first lift, then generate a cut and project it back, while local cuts first project and then lift the cut. Moreover, the subproblems to generate cuts are signficiantly different.

The approach to generate \(\langle\tilde{\alpha},x\rangle\leq\tilde{\beta}\), in the literature mentioned above, relies on the equivalence between optimization and separation [36] and can be very briefly explained as follows. By the Minkowski-Weyl Theorem, we can express \(\tilde{P}\) as the convex hull of its vertex set \(V\). Let \(\tilde{x}^{0}\in\tilde{P}\) be an interior point. Then consider the LP

\[\min_{\lambda,\gamma}\ \{\gamma:\sum_{v\in V}v\,\lambda_{v}+(\tilde{x}-\tilde{x }^{0})\gamma=\tilde{x},\ \sum_{v\in V}\lambda_{v}=1,\ \lambda\geq 0\}.\]

The dual problem \((D)\) is

\[\max_{\alpha,\beta}\ \{\langle\tilde{x},\alpha\rangle-\beta:\langle v,\alpha \rangle\leq\beta\ \forall v\in V,\ \langle\tilde{x}-\tilde{x}^{0},\alpha\rangle\leq 1\}.\]

Let \(\tilde{\alpha}\), \(\tilde{\beta}\) be an optimal solution of \((D)\). Then \(\langle\tilde{\alpha},x\rangle\leq\tilde{\beta}\) is a valid inequality for \(\tilde{P}\), since by construction \(\langle v,\tilde{\alpha}\rangle\leq\tilde{\beta}\) holds for all \(v\in V\) and thus by convexity for all points in \(\tilde{P}\). The objective enforces that this cut is maximally violated by \(\tilde{x}\) if the optimal value is positive.

Since \(\tilde{P}\) may have an exponential number of vertices, problem \((D)\) can be solved by a column generation algorithm (or cutting plane algorithm in the primal). In each iteration, one needs to solve the following pricing problem for the current point \((\hat{\alpha},\hat{\beta})\): Decide whether there exists \(v\in V\) with \(\langle v,\hat{\alpha}\rangle>\hat{\beta}\). This can be done by maximizing \(\hat{\alpha}\) over the subproblem \(\tilde{P}\), i.e., one can use a linear optimization oracle for the subproblem. This subproblem can contain integrality constraints, thereby requiring, again, IP techniques. Note that the most interesting case is where we operate on integerhulls, i.e. \(\tilde{P}=\tilde{P}_{l}\) to generate cuts for \(P=P_{I}\). In this way, local cuts can help solving an integer optimization problem over \(P\). Hence, in the following sections, any reference to \(P\), \(\tilde{P}\) holds for the integer case as well and our case study illustrates exactly that.

As mentioned above, the strongest cutting planes are those that define facets. The tilting method by Applegate et al. [3] produces such a facet. Buchheim et al. [20] introduced a different formulation that automatically produces a facet. Chvatal et al. [25] developed a formulation for general MIPs using linear optimization oracles. All three approaches use a sequence LPs at their heart; either for tilting a plane or through a column-generation procedure.

## 3 Learning Strong Cuts from Enumeration

The local cuts framework, applicable to general IPs, relies on a sequence of three operators: SEP, FACET and LIFT. SEP refers to a separation oracle separating the projected point \(\tilde{x}\) from \(\tilde{P}\) that returns a separating cut \(\langle\tilde{\alpha},x\rangle\leq\tilde{\beta}\) (or certifies that \(\tilde{x}\in\tilde{P}\)). FACET further refines the cut until it represents a facet of \(\tilde{P}\) and lastly, LIFT transforms the resulting facet into the space of \(P\) such that it separates \(x^{*}\) from \(P\) with high probability. In some variants of local cuts, SEP and FACET may be combined into one step similar to [20], whereas in [25], the _tilting_ process is a separate, concrete embodiment of FACET. Note that that facets of the subproblem, when lifted, result in the strongest cuts. In practice, it is often sufficient to find good valid inequalities of \(\tilde{P}\). As mentioned before, the original approach for local cuts through duality requires an expensive column-generation method which is based on LPs. In this section, we derive an alternative and LP-less approach.

The general idea of our new approach is sketched in Figure 1: Given a point \(\tilde{x}\in\mathds{R}^{n}\) that we intend to separate from \(P\), we solve the following optimization problem:

\[y^{*}=\operatorname*{argmin}_{y\in\tilde{P}}f(y),\] (Separation)

with \(f(y)\coloneqq\frac{1}{2}\|y-\tilde{x}\|^{2}\). Observe that this is effectively the projection of \(\tilde{x}\) onto \(\tilde{P}\) under the \(\ell_{2}\)-norm and that \(\nabla f(y)=(y-\tilde{x})\).

We solve (Separation) with a suitable variant of the Frank-Wolfe algorithm. The _Frank-Wolfe_ algorithm [29] (also called: _Conditional Gradients_[53]) is a method to minimize a smooth convex function \(f\) over a compact convex domain \(P\) by only relying on a _First-order Oracle (FO)_ for \(f\), i.e., given a point \(x\) the oracle returns \(\nabla f(x)\) (and potentially \(f(x)\)) as well as a _Linear Minimization Oracle (LMO)_ ("oracle" for the remainder of this paper), i.e., given an objective vector \(c\), the oracle returns \(v\in\operatorname*{argmin}_{x\in\tilde{P}}\langle c,x\rangle\). The original Frank-Wolfe algorithm, provided with step sizes \(\gamma_{t}>0\), iteratively calls the LMO to determine \(v_{t}\leftarrow\operatorname*{argmin}_{v\in c}\langle\nabla f(y_{t}),v\rangle\) and updates the iterate to \(y_{t+1}\gets y_{t}+\gamma_{t}(v_{t}-y_{t})\). There are various step-size strategies for \(\gamma_{t}\), but the actual choice is irrelevant for the discussion here; a common choice is \(\gamma_{t}=\frac{2}{t+2}\).

The main advantages of using Frank-Wolfe are (1) if there is a LP-less oracle, valid inequalities can be generated without solving LPs, (2) the computational overhead of the Frank-Wolfe steps compared to calls to the LMO are very light and, finally, (3) as we will show, for the case of (Separation), we can derive a new dynamic stopping criterion that can dramatically reduce the number of iterations. Note that we are not guaranteed to end up with facets, especially when the method is stopped early, however, valid inequalities that are "close" to being a facet can still serve as strong cutting planes.

For our problem minimizing \(f\), the Frank-Wolfe algorithm iteratively calls the oracle and updates its current iterate through a convex combination of the previous iterate and oracle's solution vertex. Step by step, the solution is thus expressed through a convex combination of vertices in \(\tilde{P}\) as shown in Figures 0(a) - 0(c). At convergence, the hyperplane \(\langle\nabla f(y^{*}),x\rangle\geq\langle\nabla f(y^{*}),y^{*}\rangle\) forms the desired cut.

### Separation via Conditional Gradients

Let \(y^{*}\in\tilde{P}\) be an optimal solution to (Separation) and let \(x\in\tilde{P}\) be arbitrary. By convexity, it follows that \(0\leq f(x)-f(y^{*})\leq\langle\nabla f(x),x-y^{*}\rangle\leq\max_{v\in\tilde{P }}\langle\nabla f(x),x-v\rangle\) and the last quantity is referred to as _Frank-Wolfe gap (at \(x\))_. Moreover, the following lemma holds, which is a direct consequence of the first-order optimality condition.

**Lemma 1** (First-order Optimality Condition).: _Let \(y^{*}\in\tilde{P}\). Then \(y^{*}\) is an optimal solution to \(\min_{y\in\tilde{P}}f(y)\) if and only if \(\langle\nabla f(y^{*}),y^{*}-v\rangle\leq 0\) for all \(v\in\tilde{P}\) (and in particular \(\max_{v\in\tilde{P}}\langle\nabla f(y^{*}),y^{*}-v\rangle=0\))._

Note that in the constrained case, it does not necessarily hold that \(\nabla f(y^{*})=0\), if \(y^{*}\) is an optimal solution. In fact, if the \(\tilde{x}\) that we want to separate is not contained in \(\tilde{P}\), then \(f(y^{*})>0\) and \(\nabla f(y^{*})\neq 0\) since \(y^{*}\) will lie on the boundary of \(\tilde{P}\).

It turns out that we can naturally use an optimal solution \(y^{*}\in\tilde{P}\) to (Separation) to derive a separating hyperplane. By Lemma 1:

\[\langle\nabla f(y^{*}),y^{*}\rangle\leq\langle\nabla f(y^{*}),v\rangle,\] (Cut)

which holds for all \(v\in\tilde{P}\). Moreover, if \(\tilde{x}\not\in\tilde{P}\), then (Cut) is violated by \(\tilde{x}\), i.e., \(\langle\nabla f(y^{*}),y^{*}\rangle>\langle\nabla f(y^{*}),\tilde{x}\rangle\), since \(\langle\nabla f(y^{*}),y^{*}-\tilde{x}\rangle\geq f(y^{*})-f(\tilde{x})=f(y^{* })>0\).

Usually, however, we do not solve Problem (Separation) exactly, but rather up to some accuracy. In fact, the Frank-Wolfe algorithm often uses the Frank-Wolfe gap as a stopping criterion, minimizing the function until for some iterate \(y_{t}\) it holds \(\max_{v\in\tilde{P}}\langle\nabla f(y_{t}),y_{t}-v\rangle\leq\varepsilon\) for some target accuracy \(\varepsilon\); note that the Frank-Wolfe gap converges with the same rate (up to small constant factors) as the primal gap (see e.g., [46]). Given an accuracy \(\varepsilon>0\), we obtain the valid inequality

\[\langle\nabla f(y_{t}),y_{t}\rangle-\varepsilon\leq\langle\nabla f(y_{t}),v\rangle,\] (approxCut)

for all \(v\in\tilde{P}\), which also separates \(\tilde{x}\) from \(\tilde{P}\) if it is \(\sqrt{\varepsilon}\)-far from \(\tilde{P}\), i.e., \(\|y^{*}-\tilde{x}\|>\sqrt{\varepsilon}\):

\[\langle\nabla f(y_{t}),y_{t}-\tilde{x}\rangle-\varepsilon\geq f(y_{t})-f( \tilde{x})-\varepsilon\geq f(y^{*})-\varepsilon>0.\]

The accuracy \(\varepsilon\) is chosen depending on the application; see also [17] for a sensitivity analysis for conditional gradients.

#### 3.1.1 A dynamic stopping criterion

It turns out, however, that in our case of interest, the above can be significantly improved by exploiting duality information. This allows us not only to stop the algorithm much earlier, but we also obtain a separating inequality directly from the associated stopping criterion and duality information.

The stopping criterion is derived from a few simple observations, which provide a new characterization of a point \(\tilde{x}\) that can be separated from \(\tilde{P}\). Our starting point is the following standard expansion. Let \(v\in\tilde{P}\) be arbitrary and let \(y_{t}\) be an iterate from above. Then,

\[\|\tilde{x}-v\|^{2}=\|\tilde{x}-y_{t}\|^{2}+\|y_{t}-v\|^{2}-2\langle y_{t}- \tilde{x},y_{t}-v\rangle,\]

Figure 1: We propose the following approach to separate a fractional point \(\tilde{x}\) from a full-dimensional polytope \(\tilde{P}\): We solve \(\min_{y\in\tilde{P}}f(y):=\frac{1}{2}\|y-\tilde{x}\|^{2}\), i.e., the \(L_{2}\) projection of \(\tilde{x}\) onto \(\tilde{P}\), through a variant of the Frank-Wolfe algorithm. Starting from a random vertex (a), the algorithm iteratively computes the gradient of \(f\) at the current iterate \(y_{k}\) and uses an _oracle_ to solve a linear integer optimization problem over \(\tilde{P}\), building up an _active set_ of vertices that form iterates through a convex combination (b). At convergence (c) the optimal solution \(y^{*}=y_{k+i}\) together with its gradient forms a cut that induces a facet of \(\tilde{P}\): \(\nabla f(y_{k+i})^{\top}x\geq f(y_{k+i})^{\top}y_{k+i}\) (except for degenerate cases).

which is equivalent to

\[\langle y_{t}-\tilde{x},y_{t}-v\rangle=\tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2}+\tfrac{ 1}{2}\|y_{t}-v\|^{2}-\tfrac{1}{2}\|\tilde{x}-v\|^{2}.\] (1)

Observe that the left hand-side is the Frank-Wolfe gap expression at iterate \(y_{t}\) (except for the maximization over \(v\in\tilde{P}\)) since \(\nabla f(y_{t})=y_{t}-\tilde{x}\).

Necessary Condition.Let us first assume \(\|y_{t}-v\|<\|\tilde{x}-v\|\) for all vertices \(v\in\tilde{P}\) in some iteration \(t\). Then (1) yields

\[\langle y_{t}-\tilde{x},y_{t}-v\rangle<\tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2}.\] (altTest)

If \(v_{t}\) is the Frank-Wolfe vertex in iteration \(t\), we obtain:

\[\tfrac{1}{2}\|y_{t}-\tilde{x}\|^{2} -\tfrac{1}{2}\|y^{*}-\tilde{x}\|^{2}=f(y_{t})-f(y^{*})\] \[\leq\max_{v\in\tilde{P}}\langle\nabla f(y_{t}),y_{t}-v\rangle= \langle\nabla f(y_{t}),y_{t}-v_{t}\rangle=\langle y_{t}-\tilde{x},y_{t}-v_{t} \rangle<\tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2}.\]

Subtracting \(\tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2}\) on both sides and re-arranging yields: \(0<\tfrac{1}{2}\|y^{*}-\tilde{x}\|^{2}\), which proves that \(\tilde{x}\not\in\tilde{P}\). Moreover, (1) also immediately provides a separating hyperplane: observe that (altTest) is actually a linear inequality in \(v\) and it holds for all \(v\in\tilde{P}\) since the maximum is achieved at a vertex. However, for the choice \(v=\tilde{x}\) the inequality is violated.

Sufficient Condition.Suppose that in each iteration \(t\) there exists a vertex \(\bar{v}_{t}\in\tilde{P}\) (not to be confused with the Frank-Wolfe vertex), so that \(\|y_{t}-\bar{v}_{t}\|\geq\|\tilde{x}-\bar{v}_{t}\|\). In this case (1) ensures:

\[\langle y_{t}-\tilde{x},y_{t}-\bar{v}_{t}\rangle=\tfrac{1}{2}\|\tilde{x}-y_{t }\|^{2}+\tfrac{1}{2}\|y_{t}-\bar{v}_{t}\|^{2}-\tfrac{1}{2}\|\tilde{x}-\bar{v }_{t}\|^{2}\geq\ \tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2}.\]

Thus, the Frank-Wolfe gap satisfies in each iteration \(t\) that

\[\max_{v\in\tilde{P}}\langle\nabla f(y_{t}),y_{t}-v\rangle\geq\langle y_{t}- \tilde{x},y_{t}-\bar{v}_{t}\rangle\geq\tfrac{1}{2}\|\tilde{x}-y_{t}\|^{2},\]

i.e., the Frank-Wolfe gap upper bounds the distance between the current iterate \(y_{t}\) and point \(\tilde{x}\) in each iteration. Now, the Frank-Wolfe gap converges to \(0\) as the algorithm progresses, with iterates \(y_{t}\in\tilde{P}\), so that with the usual arguments (compactness and limits etc.) it follows that \(\tilde{x}\in\tilde{P}\). In total, we obtain the following result.

_Characterization 2_.: The following are equivalent:

1. (Non-Membership) \(\tilde{x}\not\in\tilde{P}\).
2. (Distance) There exists an iteration \(t\), so that \(\|y_{t}-v\|<\|\tilde{x}-v\|\) for all vertices \(v\in\tilde{P}\).
3. (FW Gap) For some iteration \(t\), \(\max_{v\in\tilde{P}}\langle y_{t}-\tilde{x},y_{t}-v\rangle<\tfrac{1}{2}\| \tilde{x}-y_{t}\|^{2}\).

In particular, Characterization 2.3 can be easily tested within the algorithm, since the Frank-Wolfe gap is computed anyways. Using this criterion significantly improves the performance of the algorithm. Moreover, the characterization above can also be combined with standard convergence guarantees to estimate the number of iterations required to either certify non-membership or membership (up to an \(\varepsilon\)-error): If we use the vanilla Frank-Wolfe algorithm, then by standard guarantees (see e.g., [16]) it is known that the Frank-Wolfe gap \(g_{t}=\max_{v\in\tilde{P}}\langle y_{t}-\tilde{x},y_{t}-v\rangle\) satisfies \(\min_{0\leq\tau\leq t}g_{\tau}\leq\tfrac{4LD^{2}}{t+3}\) for appropriate positive constants \(L\) and \(D\). Suppose that \(\max_{v\in\tilde{P}}\langle y_{t}-\tilde{x},y_{t}-v\rangle\geq\tfrac{1}{2}\| \tilde{x}-y_{t}\|^{2}\) holds for all iterations \(0\leq t\leq T\). We want to estimate how long this can hold. If \(\tilde{x}\not\in\tilde{P}\), then using the convergence guarantee yields:

\[0<\tfrac{1}{2}\operatorname{dist}(\tilde{x},\tilde{P})^{2}\leq\min_{0\leq\tau \leq t}\tfrac{1}{2}\|\tilde{x}-y_{\tau}\|^{2}\leq\frac{4LD^{2}}{t+3}.\]

Using \(L=1\) as \(f(y)=\tfrac{1}{2}\|\tilde{x}-y\|^{2}\) and rearranging we obtain

\[t\leq T\coloneqq\frac{8D^{2}}{\operatorname{dist}(\tilde{x},\tilde{P})^{2}}-3,\]

i.e., after at most \(T\) iterations we have certified that \(\tilde{x}\) is not in \(\tilde{P}\). Guarantees for more advanced Frank-Wolfe variants can be obtained similarly.

### Computational Aspects

A common trait of the local cuts framework is that \(\tilde{P}\) is accessed implicitly via an oracle returning vertices. By far the simplest black-box oracle for any bounded IP is _enumeration_, which simply evaluates all possible solutions \(x\) and picks the best one. If the IP is unbounded, then pure enumeration does not suffice any more and the oracle needs to take the unboundedness into account. For some problems, we can find _problem-specific_ algorithms that only enumerate over feasible solutions or otherwise exploit the structure of the problem at hand to reduce the complexity of enumeration. Examples are the _dynamic programming_ approach for knapsack problems, see Section 4.1, or directly enumerating \(n!\) possible permutations of \(n\) items for the _linear ordering problem_ (LOP).

Similarly to the enumeration oracle, the _lifting_ routine can also avail of problem-specific structure in some cases. In the case of LOPs, the so-called _trivial lifting_ lemma holds, that is, facet-defining inequalities of the LOP polytope in dimension \(n\) also define facets in dimension \(r>n\)[35], meaning that no lifting is needed at all in this case. For knapsack problems, we can again use a dynamic programming approach, see Section 4.2.

In general, to apply our method to a given class of IP problems, one needs three components: i) a projection \(P\to\tilde{P}\). ii) An oracle solving linear optimization problems over \(\tilde{P}\) to optimality; in order to be practical, the selected \(\tilde{P}\) should be such that the oracle runs reasonably fast. iii) A lifting method to lift cuts from \(\tilde{P}\) up to \(P\).

In our implementation, we use the _Lazy Away-Step Frank-Wolfe algorithm_ of [18; 19], which converges linearly for (Separation). We integrate the novel termination criterion from Characterization 2.3, leading to Algorithm 1. This algorithm should be thought of as a more advanced version of the vanilla Frank-Wolfe algorithm. This variant is motivated by the fact that Frank-Wolfe trends towards sparse solutions and hence the oracle will often return previously-seen vertices. Hence, instead of querying the expensive oracle, one stores all previous vertices in a _active set_ whose size is controlled through so-called away steps. It provides superior convergence speed both in iterations andwall-clock time, exploiting the strong convexity of our objective function of the separation problem; we refer the interested reader to [46; 52; 16] for an overview. Lazification, to be thought of as an advanced caching technique, further reduces the per-iteration cost by reusing previously computed LMO solutions. Lastly, we note that in our setting and case study presented in Section 4, the LMO always returns a vertex. Even though this is not a theoretical requirement for the results presented in this paper, we do not consider the alternative case for brevity.

## 4 Case Study: The Multidimensional Knapsack Problem

The _multidimensional knapsack problem_ (MKP) is a well-known problem in combinatorial optimization and is strongly \(\mathcal{N}\mathcal{P}\)-hard. It has been used to address various practical resource allocation problems [30]. The problem involves maximizing the total profits of selected items, taking into account \(m\) resource capacity (knapsack) constraints. There are \(n\) items that contribute profits given by \(c\in\mathds{Z}^{n}\). The resource consumption of item \(j\) for the \(i\)th knapsack is given by \(a_{ij}\in\mathds{Z}_{+}\); this defines a matrix \(A=(a_{ij})\in\mathds{Z}_{+}^{m\times n}\). The capacities of the knapsacks are given by \(b\in\mathds{Z}^{m}\). We define binary variables \(x\in\{0,1\}^{n}\) such that \(x_{j}\) is equal to 1 if item \(j\) is selected and 0 otherwise. Then MKP can be expressed as an IP:

\[\max\{\langle c,x\rangle:Ax\leq b,\ x\in\{0,1\}^{n}\}.\] (MKP)

There exists abundant literature on the knapsack problems; we refer the interested reader to the recent survey by Hojny et al. [43].

We will also test our approach on the instances of the _generalized assignment problem_ (GAP), see Section 5. GAP is a variant of MKP with applications in scheduling [43]. In addition to the constraints from the MKP problem, it is required that each of the \(n\) items be assigned to exactly one knapsack. The interested reader can find a survey, more details, and a comprehensive reference list in [4; 58].

In order for our approach to work, we need to provide two things: the _oracle_, presented in Section 4.1, and the _lifting_ routine, presented in Section 4.2, cf. Section 2 and Section 3.

We consider each knapsack problem in turn and try to generate inequalities that are valid for each individual knapsack. This has the advantage that there are practically efficient oracles and more importantly efficient lifting processes. The disadvantage is that the cuts might be weaker, since they are valid for all integer solutions for all knapsack constraint instead of their intersection. An alternative would be to consider optimization oracles for the complete set of knapsack constraints as done by Gu [37]. However, then either lifting becomes more computationally demanding or one cannot use lifting.

### The Linear Minimization Oracle

The process begins with a solution \(x^{*}\) of the LP relaxation of the MKP. We create a reduced knapsack problem of dimension \(k\in\mathds{Z}_{+},k\leq n\) by removing variables of each knapsack that have integral values (0/1) in the LP relaxation. The oracles now solve the knapsack problems (KP) for each constraint of the form \(\max\,\{\langle c,x\rangle:\langle w,x\rangle\leq C,\ x\in\{0,1\}^{n}\}\), with \(c\in\mathds{R}^{k}\), \(w\in\mathds{Z}_{+}^{k}\), \(C\in\mathds{Z}_{+}\). In practice, the dimension \(k\) is rather small (in our test sets, see Section 5, we observe an average \(k\) value of 9.6 with a maximal size of 26), allowing for efficient solution approaches. In our implementation, we use a LMO based on _dynamic programming_. We note that we also apply the preprocessing improvements described by Vasilyev et al. [64], before we run the oracle on the reduced problem.

Dynamic programming, as presented by Bellman in 1957 [7], was one of the earliest exact algorithms for solving KPs. Toth [62] presents additional improvements to the algorithm. More recently, Boyer et al. present massively-parallel implementations running on GPUs [15]. The space and time complexity of the dynamic programming algorithm for KP is \(\mathcal{O}(kC)\)[15], where \(k\) is the number of items and \(C\) is the knapsack capacity. For this work, we reuse the single-threaded, CPU-based implementation of dynamic programming available in the open-source solver SCIP [10]. As we rely on existing implementations, we refer the interested reader to the above references for more details on this algorithm.

### The Lifting Routine

Lifting knapsack constraints has been extensively studied in literature, see [43] for a short survey with comprehensive references. Therefore, we only briefly summarize the implemented methods here and refer the interested reader to [43] and references therein for more details.

Let \(\{x\in\{0,1\}^{n}:\sum_{j=1}^{n}a_{j}x_{j}\leq a_{0}\}\) be one of the original knapsack constraints (corresponding to a single row in \(Ax\leq b\) in (MKP)). Define \([n]\coloneqq\{1,\ldots,n\}\), \(F_{0}\coloneqq\{j\in[n]:x_{j}^{*}=0\}\), and \(F_{1}\coloneqq\{j\in[n]:x_{j}^{*}=1\}\). Then \(S\coloneqq[n]\setminus(F_{0}\cup F_{1})\) are the variable indices in the reduced knapsack. The lifting procedure then lifts a given inequality \(\sum_{j\in S}\alpha_{j}x_{j}\leq\alpha_{0}\) valid for the reduced knapsack polytope \(\{x\in\{0,1\}^{S}:\sum_{j\in S}a_{j}x_{j}\leq a_{0}-\sum_{j\in F_{1}}a_{j}\}\) to a valid inequality for the original knapsack by computing new coefficients \(\beta_{j}\), \(j\in F_{0}\cup F_{1}\):

\[\sum_{j\in S}\alpha_{j}x_{j}+\sum_{j\in F_{1}}\beta_{j}x_{j}+\sum_{j\in F_{0} }\beta_{j}x_{j}\leq\alpha_{0}+\sum_{j\in F_{1}}\beta_{j}.\] (2)

We implemented algorithms known as _sequential up-lifting_ and _sequential down-lifting_, respectively. The implementation is based on dynamic programming as described by Vasilyev et al. [64].

## 5 Computational Experiments

We implemented the described methods in C/C++, using a developer version of SCIP 8.0.4 (github 3dbcb38) and CPLEX 12.10 as LP-solver. All tests were performed on a Linux cluster with 3.5 GHz Intel Xeon E5-1620 Quad-Core CPUs, having 32 GB main memory and 10 MB cache. All computations were run single-threaded and with a time limit of one hour. To concentrate on the improvement of local cuts on the dual bound, we initialize all runs with the optimal value. We used \(\epsilon=1\times 10^{-9}\) in Algorithm 1.

To demonstrate the advantage of using local cuts with the Frank-Wolfe approach, we run our implementation on the generalized assignment instances from the OR-Library available at http://people.brunel.ac.uk/~mastjjb/jeb/orlib/gapinfo.html. These instances have also been used by Avella et al. [4].

The results are presented in Table 1. Here, "default" are the default, factory settings of SCIP. The other settings are lcX-nc-Y, where \(X=0\) means that we only separate local cuts in the root node and \(X=1\) means that we separate local cuts in the whole tree; \(Y\) refers to whether we perform down lifting (\(Y=\text{downlift}\)) or up- and down lifting (\(Y=\text{lifting}\)). Note that for these settings, where local cuts are enabled, we turn the generation of all other cuts off, because this (somewhat surprisingly) showed better performance. The CPU time in seconds ("time") and separation time ("sep. time") as well as number of nodes ("#nodes") are given as shifted geometric means3. The numbers of generated cuts ("#cuts") are arithmetic means. Note that the iteration limit for the Frank-Wolfe algorithm is \(10\,000\) in the root node and \(1000\) in the subtree. We also reduce the effect of cut filtering allowing for more cuts to enter the main LP. Moreover, we initialize the runs with the best know solution values as in [4].

Footnote 3: The shifted geometric mean of values \(t_{1},\ldots,t_{r}\) is defined as \(\big{(}\prod(t_{i}+s)\big{)}^{1/r}-s\) with shift \(s=1\) for time and \(s=100\) for nodes in order to decrease the influence of easy instances in the mean values.

The results show that the best version is lc1-nc-lifting, i.e., it helps to separate local cuts in every node and perform up- and down lifting. This version is roughly 31% faster than the default settings on the instances solved to optimality. Using only down lifting performs badly. In any case, on these

\begin{table}
\begin{tabular}{l r r r r} \hline variant & \# solved & time & sep. time & \#cuts \\ default & 29 & 98.2 & â€“ & â€“ \\ lc-nc-downlift & 21 & 411.7 & 54.0 & 10858.6 \\ lc-nc-lifting & 26 & 113.7 & 9.8 & 4933.6 \\ lc1-nc-lifting & 29 & 86.8 & 31.8 & 160244.4 \\ \hline \end{tabular}
\end{table}
Table 1: Statistics for a branch-and-cut run with separation of local cuts for 45 generalized assignment problem instances (left) and 21 instances that were solved to optimization by all variants (right).

instances, using our local cuts method is a big advantage. Additional results are given in the appendix. In addition, we also ran an experiment in which we applied complemented mixed-integer rounding (CMIR) on the produced cut, which turned out to not be helpful and is therefore not reported in detail.

Some additional observations over all 45 instances in the test set are as follows: Variant lc1-nc-lifting called local cuts separation \(16\,870.6\) times on average. The total time for Frank-Wolfe separation is about one third of the total time. The time spent in the oracle is \(17.3\) seconds on average compared to a total of \(87.9\) seconds for the complete Frank-Wolfe algorithm. On average \(69\,968.8\) calls ended running into the iteration limit, \(81.2\) detected optimality with a zero gradient, \(8652.9\) stopped because the primal gap is small enough, and \(143\,155.9\) stopped because of the termination criterion of Section 3.1.1. This demonstrates the effect of this criterion.

## 6 Conclusion and Future Work

In this paper, we presented a novel method to learn local cuts without relying on solutions of LPs in the process. To show the effectiveness of our approach, we selected the multidimensional knapsack problem as a case study and presented computational results to support our claims.

Solving LPs has proved to be notoriously hard to parallelize, with only minor performance improvements reported in literature to date [40; 45]. Thus, existing methods for deriving local cuts, which rely on solving LPs, typically run single-threaded, on CPUs. Our approach is quite fast, as demonstrated in the computational experiments for our target problem class, but also paves the way for exploring highly parallel implementations on heterogeneous hardware and compute accelerators. This is made possible by eliminating the dependence on LPs and instead relying on the Frank-Wolfe algorithm. One such option we would like to explore in the future is to derive a _vectorized_ version of our Frank-Wolfe algorithm that could work on multiple separation problems at the same time, increasing the computational density of the operations performed and availing of massively parallel compute accelerators like _GPUs_ in the process.

The presented method is generic and can be applied to any (M)IP. We have chosen one important problem class in this paper to demonstrate the method. A natural extension of this work would be to consider other important problem classes and evaluate the benefits of using our method on those problems - especially those with beneficial properties as outlined in Section 3.2.

## References

* [1] T. Achterberg and R. Wunderling. Mixed integer programming: Analyzing 12 years of progress. In M. Junger and G. Reinelt, editors, _Facets of Combinatorial Optimization: Festschrift for Martin Grotschel_, pages 449-481. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
* [2] E. Althaus, T. Polzin, and S. V. Daneshmand. Improving linear programming approaches for the Steiner tree problem. In K. Jansen, M. Margraf, M. Mastrolilli, and J. D. P. Rolim, editors, _Experimental and Efficient Algorithms_, pages 1-14, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.
* [3] D. Applegate, R. Bixby, V. Chvatal, and W. Cook. TSP Cuts Which Do Not Conform to the Template Paradigm. In M. Junger and D. Naddef, editors, _Computational Combinatorial Optimization_, volume 2241 of _Lecture Notes in Computer Science_, pages 261-303. Springer Berlin Heidelberg, 2001.
* [4] P. Avella, M. Boccia, and I. Vasilyev. A computational study of exact knapsack separation for the generalized assignment problem. _Computational Optimization and Applications_, 45(3):543-555, 2010.
* [5] E. Balas, S. Ceria, and G. Cornuejols. A lift-and-project cutting plane algorithm for mixed 0-1 programs. _Mathematical Programming_, 58(1):295-324, 1993.
* [6] M.-F. Balcan, T. Dick, T. Sandholm, and E. Vitercik. Learning to branch. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 344-353. PMLR, 10-15 Jul 2018.
* [7] R. Bellman. _Dynamic Programming_. Princeton University Press, Princeton, NJ, USA, 1957.

* [8] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with reinforcement learning, 2017.
* [9] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: A methodological tour d'horizon. _European Journal of Operational Research_, 290(2):405-421, Apr. 2021.
* [10] K. Bestuzheva, M. Besancon, W.-K. Chen, A. Chmiela, T. Donkiewicz, J. van Doormmalen, L. Eifler, O. Gaul, G. Gamrath, A. Gleixner, L. Gottwald, C. Graczyk, K. Halbig, A. Hoen, C. Hoiny, R. van der Hulst, T. Koch, M. Lubbecke, S. Maher, F. Matter, E. Muhmer, B. Muller, M. E. Pfetsch, D. Rehfeldt, S. Schlein, F. Schlosser, F. Serrano, Y. Shinano, B. Sofranac, M. Turner, S. Vigerske, F. Wegscheider, P. Wellner, D. Weninger, and J. Witzig. Enabling research through the SCIP Optimization Suite 8.0. _ACM Transactions on Mathematical Software_, 2023. accepted for publication.
* [11] R. Bixby and E. Rothberg. Progress in computational mixed integer programming--a look back from the other side of the tipping point. _Annals of Operations Research_, 149(1):37-41, 2007.
* [12] M. Boccia. Using exact knapsack separation for the single-source capacitated facility location problem. _tech. rep., Department of Engineering, University of Sannio_, 2006.
* [13] A. E. Boyd. A pseudopolynomial network flow formulation for exact knapsack separation. _Networks_, 22(5):503-514, 1992.
* [14] E. A. Boyd. Generating Fenchel cutting planes for knapsack polyhedra. _SIAM Journal on Optimization_, 3(4):734-750, 1993.
* [15] V. Boyer, D. El Baz, and M. Elkihel. Solving knapsack problems on GPU. _Computers & Operations Research_, 39(1):42-47, 2012.
* [16] G. Braun, A. Carderera, C. W. Combettes, H. Hassani, A. Karbasi, A. Mokthari, and S. Pokutta. Conditional gradient methods. Preprint, arXiv, 2022. https://arxiv.org/abs/2211.14103.
* [17] G. Braun and S. Pokutta. Dual Prices for Frank-Wolfe Algorithms. Preprint, arXiv, 2021. https://arxiv.org/abs/2101.02087.
* [18] G. Braun, S. Pokutta, and D. Zink. Lazifying conditional gradient algorithms. In _Proceedings of the 34th International Conference on Machine Learning_, pages 566-575, 2017.
* [19] G. Braun, S. Pokutta, and D. Zink. Lazifying conditional gradient algorithms. _Journal of Machine Learning Research (JMLR)_, 20(71):1-42, 2019.
* [20] C. Buchheim, F. Liers, and M. Oswald. Local cuts revisited. _Operations Research Letters_, 36(4):430-433, 2008.
* [21] C. Buchheim, F. Liers, and M. Oswald. Speeding up IP-based algorithms for constrained quadratic 0-1 optimization. _Mathematical Programming_, 124(1):513-535, 2010.
* [22] A. Caprara and M. Fischetti. {0, 1/2}-Chvatal-Gomory cuts. _Mathematical Programming_, 74(3):221-235, 1996.
* [23] A. Chmiela, E. B. Khalil, A. Gleixner, A. Lodi, and S. Pokutta. Learning to schedule heuristics in branch and bound. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [24] P. Chu and J. E. Beasley. A genetic algorithm for the multidimensional knapsack problem. _Journal of Heuristics_, 4:63-86, 1998.
* [25] V. Chvatal, W. Cook, and D. Espinoza. Local cuts for mixed-integer programming. _Mathematical Programming Computation_, 5(2):171-200, 2013.
* [26] H. P. Crowder, E. L. Johnson, and M. W. Padberg. Solving large-scale zero-one linear programming problems. _Oper. Res._, 31:803-834, 1983.

* [27] H. Dai, E. B. Khalil, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization algorithms over graphs. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 6351-6361, Red Hook, NY, USA, 2017. Curran Associates Inc.
* [28] J.-Y. Ding, C. Zhang, L. Shen, S. Li, B. Wang, Y. Xu, and L. Song. Accelerating primal solution findings for mixed integer programs based on solution prediction. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(02):1452-1459, Apr. 2020.
* [29] M. Frank and P. Wolfe. An algorithm for quadratic programming. _Naval Research Logistics Quarterly_, 3(1-2):95-110, 1956.
* [30] A. Freville. The multidimensional 0-1 knapsack problem: An overview. _European Journal of Operational Research_, 155(1):1-21, 2004.
* [31] M. Gasse, D. Chetelat, N. Ferroni, L. Charlin, and A. Lodi. _Exact Combinatorial Optimization with Graph Convolutional Neural Networks_. Curran Associates Inc., Red Hook, NY, USA, 2019.
* [32] R. Gomory. Outline of an algorithm for integer solutions to linear programs. _Bulletin of the American Society_, 64:275-278, 1958.
* [33] R. Gomory. An algorithm for the mixed integer problem. Technical Report RM-2597, The RAND Cooperation, 1960.
* [34] R. Gomory. Solving linear programming problems in integers. _Combinatorial Analysis, R. Bellman and J. M. Hall, eds., Symposia in Applied Mathematics X, Providence, RI, 1960, American Mathematical Society_, 10:211--215, 1960.
* [35] M. Grotschel, M. Junger, and G. Reinelt. Facets of the linear ordering polytope. _Mathematical Programming_, 33:43-60, 1985.
* [36] M. Grotschel, L. Lovasz, and A. Schrijver. _Geometric Algorithms and Combinatorial Optimization_. Springer, 1988.
* [37] H. Gu. Local cuts for 0-1 multidimensional knapsack problems. In R. Sarker, H. A. Abbass, S. Dunstall, P. Kilby, R. Davis, and L. Young, editors, _Data and Decision Sciences in Action_, pages 81-89, Cham, 2018. Springer.
* [38] Z. Gu, G. L. Nemhauser, and M. W. Savelsbergh. Sequence Independent Lifting in Mixed Integer Programming. _Journal of Combinatorial Optimization_, 4(1):109-129, 2000.
* [39] Z. Gu, G. L. Nemhauser, and M. W. P. Savelsbergh. Lifted flow cover inequalities for mixed 0-1 integer programs. _Mathematical Programming_, 85(3):439-467, 1999.
* [40] J. A. J. Hall. Towards a practical parallelisation of the simplex method. _Computational Management Science_, 7(2):139-170, 2010.
* [41] H. He, H. Daume III, and J. M. Eisner. Learning to search in branch and bound algorithms. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* [42] G. Hendel. Adaptive large neighborhood search for mixed integer programming. _Mathematical Programming Computation_, 14(2):185-221, Nov. 2021.
* [43] C. Hojny, T. Gally, O. Habeck, H. Luthen, F. Matter, M. E. Pfetsch, and A. Schmitt. Knapsack polytopes: a survey. _Annals of Operations Research_, 292(1):469-517, 2020.
* [44] A. Hottung and K. Tierney. Neural large neighborhood search for routing problems. _Artificial Intelligence_, 313:103786, 2022.
* [45] Q. Huangfu and J. A. J. Hall. Parallelizing the dual revised simplex method. _Mathematical Programming Computation_, 10(1):119-142, 2018.

* [46] M. Jaggi. Revisiting Frank-Wolfe: projection-free sparse convex optimization. In _Proceedings of the 30th International Conference on Machine Learning_, pages 427-435, 2013.
* [47] K. Kaparis and A. N. Letchford. Separation algorithms for 0-1 knapsack polytopes. _Mathematical Programming_, 124(1):69-91, 2010.
* [48] E. Khalil, P. L. Bodic, L. Song, G. Nemhauser, and B. Dilkina. Learning to branch in mixed integer programming. _Proceedings of the AAAI Conference on Artificial Intelligence_, 30(1), Feb. 2016.
* [49] E. B. Khalil, B. Dilkina, G. L. Nemhauser, S. Ahmed, and Y. Shao. Learning to run heuristics in tree search. In _Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence_. International Joint Conferences on Artificial Intelligence Organization, Aug. 2017.
* [50] T. Koch, A. Martin, and M. E. Pfetsch. Progress in academic computational integer programming. In M. Junger and G. Reinelt, editors, _Facets of Combinatorial Optimization: Festschrift for Martin Grotschel_, pages 483-506. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
* [51] W. Kool, H. van Hoof, and M. Welling. Attention, learn to solve routing problems! In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [52] S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimization variants. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems 28_, pages 496-504. Curran Associates, 2015.
* [53] E. S. Levitin and B. T. Polyak. Constrained minimization methods. _USSR Computational Mathematics and Mathematical Physics_, 6(5):1-50, 1966.
* [54] D. Liu, A. Lodi, and M. Tanneau. Learning chordal extensions. _Journal of Global Optimization_, 81(1):3-22, Jan. 2021.
* [55] H. Marchand and L. A. Wolsey. Aggregation and Mixed Integer Rounding to Solve MIPs. _Operations Research_, 49(3):363-371, June 2001.
* [56] M. Nazari, A. Oroojlooy, L. Snyder, and M. Takac. Reinforcement learning for solving the vehicle routing problem. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [57] M. E. Pfetsch, G. Rinaldi, and P. Ventura. Optimal patchings for consecutive ones matrices. _Mathematical Programming Computation_, 14(1):43-84, 2022.
* [58] M. Posta, J. A. Ferland, and P. Michelon. An exact method with variable fixing for solving the generalized assignment problem. _Computational Optimization and Applications_, 52(3):629-644, 2012.
* [59] J. Song, r. lanka, Y. Yue, and B. Dilkina. A general large neighborhood search framework for solving integer linear programs. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 20012-20023. Curran Associates, Inc., 2020.
* [60] N. Sonnerat, P. Wang, I. Ktena, S. Bartunov, and V. Nair. Learning a large neighborhood search algorithm for mixed integer programs, 2021.
* [61] Y. Tang, S. Agrawal, and Y. Faenza. Reinforcement learning for integer programming: Learning to cut. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9367-9376. PMLR, 13-18 Jul 2020.
* [62] P. Toth. Dynamic programming algorithms for the zero-one knapsack problem. _Computing_, 25(1):29-45, 1980.

* [63] I. L. Vasil'ev. A cutting plane method for knapsack polytope. _Journal of Computer and Systems Sciences International_, 48(1):70-77, 2009.
* [64] I. Vasilyev, M. Boccia, and S. Hanafi. An implementation of exact knapsack separation. _Journal of Global Optimization_, 66(1):127-150, 2016.

[MISSING_PAGE_FAIL:15]

there is no clear advantage of the Frank-Wolfe approach, but for larger sizes, it solves more instances and is faster.

\begin{table}
\begin{tabular}{r r r r r r r r r r r r} \hline \hline  & & & \multicolumn{3}{c}{default} & \multicolumn{3}{c}{lc-ac-lifting} & \multicolumn{3}{c}{lc-1-ac-uplfit} \\ \cline{3-13} \(n\) & \(m\) & \(\alpha\) & \#solved & time & sep time & \#solved & time & sep time & \#solved & time & sep time \\ \hline

[MISSING_PAGE_POST]

 \hline \hline \end{tabular}
\end{table}
Table 3: Detailed statistics for a branch-and-cut run with three different algorithm variants for the multi-dimensional knapsack instances; each line represents shifted geometric means over 10 instances.