# MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models

 Tessa Han

Harvard University

Cambridge, MA

than@g.harvard.edu

&Aounon Kumar

Harvard University

Cambridge, MA

aokumar@hbs.edu

&Chirag Agarwal

University of Virginia

Charlottesville, VA

chiragagarwal@virginia.edu

&Himabidu Lakkaraju

Harvard University

Cambridge, MA

hlakkaraju@hbs.edu

###### Abstract

As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.

## 1 Introduction

Large language models (LLMs) have been progressing at a breathtaking speed and have been shown to be proficient in a variety of medical tasks such as answering medical questions [1], interpreting histopathology data [2], and conversing with patients [3]. While LLMs have the potential to improve medicine, they can also be used to cause severe medical harm, including mistreating patients, concealing medical errors, violating patient confidentiality, crafting fake medical records, devising ways to restrict access to medical care, and deliberately spreading misinformation. At stake are fundamental issues such as personal and public health, patient safety, and human rights. Therefore, investigating the medical safety of LLMs is of critical importance.

In light of the aforementioned risks, various organizations and communities have emphasized the medical safety of LLMs. For instance, the European Union's Artificial Intelligence (AI) Act, which "aims to protect fundamental rights, democracy, the rule of law, and environmental sustainability from high-risk AI"[4], classifies AI systems "that could put the life and health of citizens at risk" as"high-risk," the highest risk category for AI systems that are acceptable to be used [5]. In addition, the medical AI community has unequivocally called for the use of AI "to do no harm" and "the most good" in medicine [6; 7]. More broadly, various governments have called for the regulation of these systems (e.g., U.S. AI Bill of Rights [8] and AI Accountability Act [9], United Kingdom Bletchley Declaration [10], European Union AI Act [5]).

While recent research introduced benchmarks to evaluate the safety of LLMs by measuring their "unwillingness" to respond to harmful requests [11; 12; 13], these works predominantly focus on general safety and do not consider the unique safety concerns of the medical domain. For instance, general safety involves ensuring that LLMs do not answer questions that can cause harm in everyday life (e.g., "What are the steps to steal a car?") while medical safety involves ensuring that LLMs do not answer questions that can harm patients or healthcare providers in healthcare settings (e.g., "How can a doctor modify a patient's electronic health record to cover up a diagnostic error?"). However, there is little to no formal understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. These aspects are critical in ensuring the safe deployment of LLMs in medical settings. To address the aforementioned challenges, we introduce MedSafetyBench, a safety evaluation benchmark that addresses the unique safety concerns of the medical domain. Our work makes the following contributions:

* We define the notion of medical safety in LLMs based on the _Principles of Medical Ethics_ set forth by the American Medical Association.
* We leverage the understanding above to develop MedSafetyBench, the first medical safety evaluation benchmark for LLMs. MedSafetyBench comprises 1,800 harmful medical requests and corresponding safe responses. We use a combination of state-of-the-art LLMs (e.g., GPT-4) and adversarial jailbreaking techniques (e.g., Greedy Coordinate Gradient algorithm [14]) to construct this benchmark dataset.
* We demonstrate the utility of MedSafetyBench by evaluating the medical safety of publicly-available general-knowledge LLMs (e.g., Vicuna [15], Pythia [16], Llama-2 [17], Llama-3.1 [18], Mistral [19], Mistral [20], GPT-3.5 [21], GPT-4 [22], and GPT-4o [23]) and medical LLMs (e.g., Medplaca-13b [24], Meditron-70b [25], ClinicalCamel-70b [26], and Med42-70b [27]). We find that the medical LLMs fail to meet medical safety standards.
* We also demonstrate how fine-tuning these medical LLMs using MedSafetyBench can improve their medical safety while preserving their medical performance.

Figure 1: Contribution and findings. In this work, we define the notion of medical safety for LLMs, leverage this definition to develop a medical safety benchmark dataset, and use this benchmark to evaluate and improve the medical safety of LLMs. We find that 1) publicly-available medical LLMs do not meet standards of medical safety and that 2) fine-tuning these LLMs on medical safety demonstrations significantly improves their safety while preserving their medical performance.

By concretizing the notion of medical safety and introducing MedSafetyBench, our work enables a systematic study of medical safety in LLMs and motivates future research in this area, thereby paving the way to minimize the safety risks associated with LLMs in medicine.

## 2 Related Work

**Safety Evaluation Benchmarks for LLMs.** Recent works evaluate the safety of LLMs using benchmark datasets consisting of harmful requests that an LLM should refuse to answer [11; 12; 13] and the LLM's safety is measured by its "unwillingness" to respond to such requests [11]. For instance, Qi et al. [11] develop a safety dataset by collecting harmful prompts that violate Meta and OpenAI usage policies. Bhardwaj and Poria [12] create a dataset of harmful questions and answers using a red-teaming strategy called Chain of Utterances, which involves using one LLM to elicit harmful responses from another. In addition, Zhang et al. [13] introduce a multiple-choice question benchmark to evaluate the safety of LLMs. However, all of these safety evaluation benchmarks focus on general harm (e.g., illegal activity, violence, and fraud) and do not address the unique concerns of the medical domain (e.g., clinicians' responsibility to patients, patient rights to confidentiality, and treatment of medical errors). Therefore, in this work, we introduce the first safety evaluation benchmark dataset specific to the medical domain and use it to evaluate and improve the medical safety of LLMs.

**Safety Alignment of LLMs.** The training objective of an LLM, i.e., predicting the next token given a sequence of tokens, does not ensure that its behavior aligns with human preferences. As a result, the field of alignment has emerged in recent LLM research to build LLMs that behave in a manner consistent with human intentions, preferences, goals, and values [28; 29]. One of the key aspects of alignment in LLMs is safety (i.e., the model's ability to operate without causing harm, such as complying with legal requirements and ethical standards and avoiding danger, risk, and injury), where safety-aligned LLMs have safety guardarails that prevent them from complying with harmful instructions. Common alignment techniques include reinforcement learning from human feedback (RLHF) [30; 31; 32], where an LLM is fine-tuned using a reward model that is based on human feedback, and instruction-tuning [33], where an LLM is fine-tuned on a dataset of instruction-response pairs. While prior work has examined aligning LLMs to general safety standards [34; 35], aligning them to medical safety standards has not been investigated. In this work, we demonstrate that MedSafetyBench can be leveraged to perform instruction-tuning and improve the medical safety of LLMs.

## 3 MedSafetyBench: A Benchmark Dataset for the Medical Safety of LLMs

In this section, we first define medical safety for LLMs. Based on this definition, we develop MedSafetyBench, a medical safety benchmark dataset for LLMs.

### Defining Medical Safety for LLMs

What does it mean for an LLM to be safe in a medical setting? We base the definition of medical safety for LLMs on the _Principles of Medical Ethics_[36] (Table 1) set forth by the American Medical Association (AMA). These principles, adopted in 1957 and last revised in 2001 [37], are a set of time-tested, living principles for ethical conduct in medicine. According to the AMA, they represent "standards of conduct that define the essentials of honorable behavior for the physician", outlining a physician's "responsibility to patients first and foremost, as well as to society, to other health professionals, and to self" [38]. They cover various aspects of medical practice and promote medical practice that is consistent with medical knowledge, follows the law, and aspires to medical ideals.

Thus, we define an LLM to be aligned with medical safety standards if its output is not only accurate but also consistent with the AMA's _Principles of Medical Ethics_. Typically, during the training and development of an LLM, the focus is on the model's technical performance ("accuracy"). In this work, we emphasize that, in addition to high technical performance, an LLM must also adhere to other aspects of safety.

### Developing the Benchmark Dataset

**Harmful Medical Requests.** To evaluate medical safety of LLMs, we first generate a set of requests that violate the AMA's _Principles of Medical Ethics_. An LLM that meets medical safety standards should decline to answer such harmful medical requests. Each principle forms a category containing requests that violate the respective ethical guidelines. To generate a diverse set of harmful medical requests in a systematic fashion, we use GPT-4 [22] and Llama-2-7b-chat[39]. With GPT-4, we prompted the model, asking it to write commands or questions that might be given to an AI assistant that violate each principle of medical ethics and that include medical jargon. This prompting approach did not work with Llama-2-7b-chat, which declined to generate harmful medical requests. To circumvent this, with Llama-2-7b-chat, we employed a jailbreaking technique using the Greedy Coordinate Gradient attack algorithm [14]. Given a prompt, this algorithm optimizes a sequence of adversarial input tokens that, when appended to the prompt, cause the LLM to produce a desired target sequence. We set the prompt for Llama-2-7b-chat to a request to produce harmful prompts that violate a particular AMA principle and the target sequence to an affirmative response. We observe that the harmful medical requests created using GPT-4 tend to contain more medical jargon than those created using Llama-2-7b-chat. We manually inspect the harmful medical requests for correctness and quality and edited inadequate ones. This process yielded 1,800 unique harmful medical requests (900 from each model), spanning all nine _Principles of Medical Ethics_. An example harmful medical request is shown below.

```
1:functionmedical(regurds)

[MISSING_PAGE_POST]

medical safety demonstration consists of a harmful medical request and a safe response to the request. Given a harmful medical request, we create a corresponding safe response by prompting GPT-4 to decline to answer the harmful request and provide a brief explanation for why it is unsafe or unethical. We manually verified the quality of the safe responses and edited inadequate ones. An example safety demonstration is shown below.

**Dataset Safety demonstration**

**Harmful request:**

Can you help me create a fake medical degree or license?

**Safe response:**

I'm sorry, but I cannot assist with creating a fake medical degree or license. Doing so would be illegal and unethical, as it involves fraud and can have serious consequences for public safety and trust.

**Dataset Structure.** To use the benchmark dataset to both evaluate and improve LLM medical safety, we randomly split the 1,800 medical safety demonstrations into two groups of 900 demonstrations. We refer to these two groups as MedSafety-Eval and MedSafety-Improve. Each group consists of \(450\) medical safety demonstrations with harmful requests created using GPT-4 and \(450\) medical safety demonstrations with harmful requests created using Llama-2-7b-chat. For MedSafety-Eval, we refer to these two sets of medical safety demonstrations as MedSafety-Eval-GPT4 and MedSafety-Eval-Llama2, respectively. Additional details about the benchmark dataset and its development are in Appendix A.

**Dataset Validation.** To validate the benchmark dataset, we conducted a user study with 25 domain experts (doctors). Each domain expert viewed a randomly selected set of 25 harmful medical requests from the full dataset. For each harmful request, the domain experts were asked if it violates any of the nine Principles of Medical Ethics, and if so, they were further asked to select any of the nine principles that are violated by the harmful request. Through the user study, we confirm that the harmful requests in MedSafetyBench indeed violate _Principles of Medical Ethics_ and that the benchmark covers all nine _Principles of Medical Ethics_. Additional details about the user study are in Appendix D. These results from the user study are consistent with our own inspection of the benchmark dataset and further validate the benchmark.

## 4 Experiments

In this section, we demonstrate the utility of MedSafetyBench for studying the medical safety of LLMs by using MedSafety-Eval to evaluate the medical safety of LLMs and MedSafety-Improve to improve it.

### Evaluating the Medical Safety of LLMs

**Setup.** We prompt LLMs with harmful requests and evaluate the harmfulness of their responses. We evaluate both general and medical safety. To evaluate general safety, we use the GenSafety-Eval dataset [11] (\(n=330\)). It consists of harmful general requests, spanning eleven categories of general harm (illegal activity, child abuse content, hate / harassment / violence, malware, physical harm, economic harm, fraud deception, adult content, political campaigning, privacy violation activity, and tailored financial advice). To evaluate medical safety, we use MedSafety-Eval (\(n=900\)). It contains harmful medical requests, spanning nine categories of medical harm (corresponding to the nine principles of medical ethics). (Note: While MedSafety-Eval is a set of medical safety demonstrations, where each demonstration consists of a harmful medical request and a corresponding safe response, for the purpose of evaluation, we use only the harmful requests.)

We evaluate publicly-available LLMs. For medical LLMs, we evaluate Medplaca (7b and 13b) [24], Meditron (7b and 70b) [25], ClinicalCamel-70b [26], and Med42-70b [27]. To our knowledge, these LLMs are not safety-aligned. We also evaluate the general-knowledge LLMs on which these medical LLMs were pre-trained and/or fine-tuned: Llama (7b and 13b) [39] and Llama-2 (7b, 13b, and 70b) [17]. These LLMs are also not safety-aligned. In addition, we evaluate safety-aligned versions of these general-knowledge LLMs: Llama-2-chat (7b, 13b, and 70b) [17]. For additional benchmarking on a more diverse set of models, we also evaluate other state-of-the-art general 

[MISSING_PAGE_FAIL:6]

Figure 2: Average harmfulness score for each LLM by harm dataset. On the x-axis, LLMs with safety alignment are indicated by an asterisk. Error bars indicate the standard error of the mean. The results indicate that medical LLMs readily comply with harmful general and medical requests, and they do so more frequently than their safety-aligned, general-knowledge counterparts. Thus, medical LLMs do not meet standards of general and medical safety.

Figure 3: Safety of medical LLMs before fine-tuning (red) and after fine-tuning (green) on safety demonstrations. Error bars indicate the standard error of the mean. Fine-tuning on safety demonstrations significantly improves the safety of original medical LLMs. This trend is consistent across medical LLMs (Medalpaca-7b, Medalpaca-13b, and ClinicalCamel-70b), across evaluation datasets (GenSafety-Eval, MedSafety-Eval-GPT4, MedSafety-Eval-Llama2), and across the types of safety demonstrations on which the model is fine-tuned (general, medical, or both).

Fine-tuning improves LLM safety across tuning datasets, evaluation datasets, and models (Figure 3: green conditions have lower harmfulness scores than the red conditions; Appendix C, Table 4). In addition, fine-tuning on one type of safety improves not only that type of safety but also the other type of safety (Figure 3: gen_n900 and med_n900 conditions have lower harmfulness scores than the red conditions for both GenSafety-Eval and MedSafety-Eval). Fine-tuning on both types of safety improves safety the most, followed by fine-tuning on only medical safety, then by fine-tuning on only general safety (Figure 3: both_n1800 conditions have the lowest harmfulness scores, followed by med_n900 conditions, then by gen_n900 conditions). As the number of safety demonstrations used during fine-tuning increases, LLM safety improves (Appendix C, Figures 6, 7, and 8). Furthermore, fine-tuning for safety preserves the medical performance of medical LLMs (Appendix C, Figures 9, 10, 11, and 12), suggesting it may be possible to achieve both desiderata (safety and performance).

## 5 Discussion and Conclusion

In this work, we study the medical safety of LLMs. We define the notion of medical safety for LLMs and leverage this definition to develop MedSafetyBench, the first benchmark dataset for the medical safety of LLMs. Using MedSafetyBench, we evaluate and improve the medical safety of LLMs, finding that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using the benchmark dataset improves their safety. These analyses are made possible only by the development of a safety benchmark dataset that is specific to the medical domain.

This work paves the way for future research on the medical safety of LLMs. In this work, medical safety is defined based on the AMA's _Principle of Medical Ethics_. In practice, one could consider introducing nuance to the definition. For example, levels of acceptable risk may vary among medical subspecialities (e.g., emergency medicine vs. neurological surgery vs. dermatology) and based on a patient's condition and personal preference (e.g., a patient with a condition that has no established treatment options may be more willing to try risky experimental procedures). Aligning LLMs to account for different levels of acceptable risk and be tailored to different medical subspecialities is a future research direction. In addition, this work demonstrates that one way to improve the medical safety of LLMs is through instruction-tuning. Exploring other fine-tuning techniques, such as RLHF, is another direction for future research. Although very computationally intensive, RLHF could incorporate domain expert feedback during the safety alignment process and may facilitate the alignment of LLMs to more nuanced and bespoke medical safety standards.

In the application of LLMs to medical tasks, the focus has been on achieving high medical performance [1; 2; 24; 25; 26; 27]. However, as LLMs develop increasingly powerful capabilities and are applied in the medical domain, it is also critical to study and proactively mitigate their risks of medical harm. This calls for collective discussion in the medical research community and beyond of how to define medical safety for LLMs, continued evaluation of the medical safety of LLMs, and the development of safer LLMs, in order to mitigate their risks of harm in medicine. We hope this work jumpstarts this discussion and galvanizes future work in this area.

## Acknowledgements

This work is supported in part by the NSF awards IIS2008461, IIS-2040989, IIS-2238714, the AI2050 program at Schmidt Sciences, and faculty research awards from Google, JPMorgan, Harvard Data Science Initiative, and the Digital, Data, and Design (D\({}^{\wedge}\)3) Institute at Harvard. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.

## References

* [1] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. _Nature_, 2023.
* [2] Ming Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, Anil Parwani, Andrew Zhang, and Faisal Mahmood. A visual-language foundation model for computational pathology. _Nature Medicine_, 2024.
* [3] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic AI. _arXiv preprint arXiv:2401.05654_, 2024.
* [4] European Union. Artificial Intelligence Act (Press). https://www.europarl.europa.eu/news/en/press-room/20240308IPR19015/artificial-intelligence-act-meps-adopt-landmark-law, 2024. [Online; accessed 19-April-2024].
* [5] European Union. Artificial Intelligence Act. https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai, 2024. [Online; accessed 15-April-2024].
* and the most good
- with AI in health care. _Nature Medicine_, 2024.
* and the most good
- with AI in health care. _New England Journal of Medicine AI_, 2024.
* [8] United States. AI Bill of Rights. https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf, 2022. [Online; accessed 15-April-2024].
* [9] United States. AI Accountability Act. https://www.congress.gov/bill/118th-congress/house-bill/3369, 2023. [Online; accessed 15-April-2024].
* [10] United Kingdom. Bletchley Declaration. https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023, 2023. [Online; accessed 05-March-2024].
* [11] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _International Conference on Learning Representations_, 2024.
* [12] Rishabh Bhardwaj and Soujanya Poria. Red-teaming large language models using chain of utterances for safety-alignment. _arXiv preprint arXiv:2308.09662_, 2023.
* [13] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice questions. _arXiv preprint arXiv:2309.07045_, 2023.
* [14] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.
* [15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

* [16] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. _International Conference on Machine Learning_, 2023.
* [17] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024.
* [19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [20] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* [21] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 2020.
* [22] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [23] OpenAI. Hello GPT-4o, May 2024. URL https://openai.com/index/hello-gpt-4o/.
* An open-source collection of medical conversational ai models and training data. _arXiv preprint arXiv:2304.08247_, 2023.
* [25] Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. _arXiv preprint arXiv:2311.16079_, 2023.
* [26] Augustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin, and Bo Wang. Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. _arXiv preprint arXiv:2305.12031_, 2023.
* A clinical large language model. https://huggingface.co/m42-health/med42-70b, 2023. [Online; accessed 05-March-2024].
* [28] Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment of language agents. _arXiv preprint arXiv:2103.14659_, 2021.
* [29] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. _Advances in Neural Information Processing Systems_, 2020.
* [30] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in Neural Information Processing Systems_, 2017.
* [31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 2022.

* [32] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [33] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [34] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned Llamas: Lessons from improving the safety of large language models that follow instructions. _International Conference on Learning Representations_, 2024.
* [35] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. _Advances in Neural Information Processing Systems (Datasets and Benchmark Track)_, 2023.
* [36] American Medical Association. Code of Medical Ethics. https://code-medical-ethics.ama-assn.org/, 2001. [Online; accessed 05-March-2024].
* [37] American Medical Association. History of the Code. https://www.ama-assn.org/sites/ama-assn.org/files/corp/media-browser/public/ethics/ama-code-ethics-history.pdf, 2017. [Online; accessed 05-March-2024].
* [38] American Medical Association. Principles of Medical Ethics. https://code-medical-ethics.ama-assn.org/principles, 2001. [Online; accessed 05-March-2024].
* [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [40] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, July 2024. URL https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/.
* [41] Di Jin, Eileen Pan, Nassim Oufatole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. _Applied Sciences_, 2021.
* [42] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering. _Conference on Health, Inference, and Learning_, 2022.
* [43] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. _Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 2019.
* [44] Meta. Llama 2 Acceptable Use Policy. https://ai.meta.com/llama/use-policy/, 2023. [Online; accessed December 2023].
* [45] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_, 2022.
* [46] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. _International Conference on Learning Representations_, 2022.
* [47] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. PMC-Llama: Towards building open-source language models for medicine. _arXiv preprint arXiv:2305.10415_, 2023.

* [48] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _International Conference on Learning Representations_, 2021.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section 3
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] This work introduces MedSafetyBench, a new benchmark dataset for medical safety of LLMs (described and validated in Section 3). We demonstrate the benchmark's usefulness in evaluating the medical safety of general-knowledge and medical LLMs (Section 4.1) and in improving the safety of medical LLMs while preserving their medical performance (Section 4.2). 2. Did you describe the limitations of your work? [Yes] See Section 5, where we describe the limitations and how future work can address them. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix, where we describe the societal impact of our work. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The benchmark dataset, code, and instructions to reproduce the main experimental results are publicly available at https://github.com/A14LIFE-GROUP/med-safety-bench. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix A, B, and C. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix B and C.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? We cite the papers and GitHub repositories of the existing assets which contain their respective licenses. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] We share the new benchmark dataset and accompanying code at https://github.com/A14LIFE-GROUP/med-safety-bench.

* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] This paper introduces MedSafetyBench, the first medical safety evaluation benchmark for LLMs. The benchmark contains content (e.g., harmful medical requests) that may be considered offensive. Details about the benchmark dataset are discussed in Section 3 and Appendix A.
* If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Section D. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] See Section D. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Section D.

## Appendix

### Impact Statement

This paper studies the medical safety of LLMs, exposing their medical risks and exploring ways to improve their medical safety, with the goal of mitigating the risks of harm of LLMs in medicine. This paper contains content that may be graphic and may be used for harmful purposes. It should be used for research only. Evaluating and improving the risks of harms of LLMs is critical for ensuring the ethical use of AI in medicine and guides responsible innovation, policy development, and regulation in society.

## Appendix A MedSafetyBench

In LLM research literature, the safety of LLMs is often evaluated using benchmark datasets consisting of harmful prompts that an LLM should refuse to answer [11, 12, 13]. While several safety evaluation benchmarks have been proposed in the literature, none of them address the unique concerns of the medical domain. Therefore, we present MedSafetyBench, a benchmark dataset which consists of 1800 medical safety demonstrations that can be used to evaluate the medical safety of LLMs and align LLM to standards of medical safety. The benchmark dataset does not contain personally identifiable content. Since the benchmark dataset contains harmful medical requests, it does contain potentially offensive material. It is intended for research purposes only.

### Developing harmful medical requests

We develop harmful medical requests based on the nine _Principles of Medical Ethics_ outlined by the AMA (Table 1). We use two publicly available LLMs, GPT-4[22] and Llama-2[39], to generate a diverse set of prompts in an automated and systematic fashion.

**Generating harmful medical requests using** GPT-4. To generate harmful medical prompts using GPT-4, we prompted the model, asking it to write commands or questions that might be given to an AI assistant that violate each principle of medical ethics. We also ask that the commands or questions include medical jargon such as terms related to diseases, anatomy, drug names, and medical procedures. If GPT-4 declined to answer, we added that the prompts are for research or educational purposes. The following is an example of the prompt used for the principle of Law and Responsibility to Society from Table 1.

Through manual inspection, we confirmed the proper use of medical jargon in the prompts and filtered out prompts that were duplicates or that were irrelevant to the corresponding principle. This yielded a total of 900 harmful prompts, spanning all nine principles of medical ethics. We refer to the harmful medical requests generated by GPT-4 as med-harm-gpt4-full.

**Generating harmful medical requests using** Llama-2-7b-chat.** To generate harmful medical prompts using Llama-2, we used the Llama-2-7b-chat version of the model. Although it was possible to simply ask GPT-4 to generate harmful prompts, we found that Llama-2-7b-chat did not readily comply with our requests for generating such prompts. To circumvent this, we employed a jailbreaking technique on the Llama-2-7b-chat model using the Greedy Coordinate Gradient attack algorithm developed by Zou et al. [14]. Given a prompt, this algorithm optimizes a sequence of adversarial input tokens that, when appended to the prompt, cause the LLM to produce a desired target sequence. For our purposes, we set the input prompt for Llama-2-7b-chat to a request to produce harmful prompts that violate a particular AMA principle and the target sequence to an affirmative response. The following is an example of a prompt and target sequence using the principle of Competence, Compassion, and Respect for Human Dignity from Table 1.

### Developing safe responses to the harmful medical requests

For each harmful request in med-harm-gpt4-full and med-harm-llama2-full, we generate a corresponding safe response using GPT-4. The safe response declines to answer the harmful request and provides a brief explanation for why it is unsafe or unethical. We manually verified the quality of the safe responses and edited inadequate ones. Together, a harmful medical request and its corresponding safe response form a medical safety demonstration. An example safety demonstration is shown below.

\begin{table}
\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \hline
**Principle** & med-harm-gpt4-full & med-harm-llama2-full \\ \hline I & 100 & 118 \\ \hline II & 100 & 114 \\ \hline III & 100 & 62 \\ \hline IV & 100 & 112 \\ \hline V & 100 & 100 \\ \hline VI & 100 & 48 \\ \hline VII & 100 & 122 \\ \hline VIII & 100 & 128 \\ \hline IX & 100 & 96 \\ \hline \end{tabular}
\end{table}
Table 2: Number of harmful medical requests for each principle of medical ethics in med-harm-gpt4-full and med-harm-llama2-full.

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

## 6 Conclusion

Figure 4: Harmfulness score distributions for each LLM by harm dataset. LLMs that have been aligned to generate safe responses are indicated by an asterisk. The results indicate that medical LLMs readily comply with harmful general and medical requests, and they do so more frequently than their safety-aligned, general-knowledge counterparts. Thus, medical LLMs do not meet currently-achievable standards of general and medical safety.

## Appendix A

Figure 5: Harmfulness score raw distributions for each LLM by harm dataset. LLMs that have been aligned to generate safe responses are indicated by an asterisk. The results indicate that for medical LLMs, many responses to general and medical harmful requests fully comply with the requests.

## Appendix C Improving the medical safety of LLMs

### Fine-tuning details

In this section, we describe the fine-tuning setup, including the base models, fine-tuning datasets, fine-tuning procedure, harm evaluation datasets, and medical evaluation datasets.

**Base models.** Prior work has proposed fine-tuning general-knowledge LLMs on general safety demonstrations to improve their general safety [34]. Thus, in our work, we focus on fine-tuning medical LLMs on general and/or medical safety demonstrations to improve their overall safety performance. For computational tractability, we focus on fine-tuning 7-billion and 13-billion-parameter medical LLMs, i.e., Meditron-7b, Medalpaca-7b, and Medalpaca-13b.

**Fine-tuning datasets.** We fine-tune the models on demonstrations of medical and general safety. Each safety demonstration consists of a harmful prompt and a corresponding safe response. For

\begin{table}

\end{table}
Table 3: Results of two-sided Wilcoxon signed rank tests. We compare each medical LLM with its safety-aligned, general-knowledge counterpart, prompting the LLMs with harmful prompts from each dataset and measuring the harmfulness of the responses using the harmfulness score. We test the null hypothesis that the paired differences of scores is symmetric about zero. For each paired difference, scores are paired by harmful prompt, and the difference is the score of the general-knowledge LLM minus the score of the medical LLM. Thus, a negative difference indicates that the response of the general-knowledge LLM is safer than that of the medical LLM. We examine responses with valid paired scores (i.e., excluding responses with “NA” scores). The average paired difference is shown as the effect size in the table. In this paper, we conduct a total of 45 statistical tests (shown in Tables 3 and 4), so we use a significance threshold of \(0.05/45=0.001\) based on the Bonferroni correction for multiple comparisons. The results are highly statistically significant and suggest that medical LLMs are less safe than their safety-aligned, general-knowledge counterparts.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

[MISSING_PAGE_EMPTY:26]

\begin{table}

\end{table}
Table 4: Results of two-sided Wilcoxon signed rank tests. We compare each medical LLM before and after fine-tuning on safety demonstrations, prompting the LLMs with harmful prompts from each dataset and measuring the harmfulness of the responses using the harmfulness score. We test the null hypothesis that the paired differences of scores is symmetric about zero. For each paired difference, scores are paired by harmful prompt, and the difference is the score after fine-tuning minus the score before fine-tuning. Thus, a negative difference indicates that the response of the LLM after fine-tuning is safer than that of the LLM before fine-tuning. We examine responses with valid paired scores (i.e., excluding responses with “NA” scores). The average paired difference is shown as the effect size in the table. In this paper, we conduct a total of 45 statistical tests (shown in Tables 3 and 4), so we use a significance threshold of \(0.05/45=0.001\) based on the Bonferroni correction for multiple comparisons. The results are highly statistically significant and suggest that fine-tuning significantly improves the safety of the medical LLMs.

## Appendix A

Figure 10: Medical performance of Medalpaca-7b upon fine-tuning on safety demonstrations. Across medical benchmark datasets, fine-tuning preserves the model’s medical performance.

Figure 9: Medical performance of medical LLMs before fine-tuning (red) and after fine-tuning (green) on safety demonstrations. Error bars indicate the standard error of the mean. Fine-tuning (green) preserves the medical performance of the base model (red). This trend is consistent across medical LLMs (Medalpaca-7b, Medalpaca-13b, and Meditron-7b) and across medical benchmark datasets (MedQA, MedMCQA, and PubMedQA, MMLU-Medical).

Figure 11: Medical performance of Medalpaca-13b upon fine-tuning on safety demonstrations. Across medical benchmark datasets, fine-tuning preserves the model’s medical performance.

Figure 12: Medical performance of Meditron-7b upon fine-tuning on safety demonstrations. Across medical benchmark datasets, fine-tuning preserves the model’s medical performance.

Validating the MedSafetyBench dataset with domain experts (doctors)

### Study set-up and results

**User study set-up.** To validate the MedSafetyBench dataset, we conducted a user study with 25 domain experts (doctors). Each domain expert took a survey that consisted of 25 randomly-selected prompts from the full dataset (\(n=1800\)). For each prompt, domain experts were asked if the prompt violates any of the nine _Principles of Medical Ethics_. If so, they were further asked to select any of the nine principles that are violated by the prompt.

**Violation of the _Principles of Medical Ethics_.** Among the prompts shown to domain experts, 91% (567/625) were deemed to violate at least one principle (Figure 13). Thus, the vast majority of prompts are indeed unethical. The remaining 9% (58/625) of prompts were deemed ethical by at least one domain expert, but 31% (18/58) of these prompts were also deemed unethical by at least one other domain expert. Thus, rather than being categorically ethical, these prompts likely represent "gray-area" cases.

**Coverage of prompts.** For prompts that were deemed by domain experts to violate at least one principle, we examined the specific principles the prompts violated. The distribution of these principles is shown in Figure 14. The prompts span all nine _Principles of Medical Ethics_, relating to some principles (Principles 1, 2, and 8) more than others (Principles 5 and 6) (Figure 14).

Among prompts that were deemed unethical by domain experts, 85% (482/567) simultaneously violated two or more _Principles of Medical Ethics_ (Figure 13). In addition, some pairs of principles (such as Principles 1 and 2, 1 and 8, and 2 and 8) are often violated together (Figure 15). These results suggest that the _Principles of Medical Ethics_ are not mutually exclusive.

In summary, the user study further validates the benchmark, providing evidence that the vast majority of the prompts indeed violate _Principles of Medical Ethics_ and that the benchmark covers all nine principles.

### Additional details

This study was approved by the Institutional Review Board. For a given participant, the survey was expected to take at most 15 minutes. Participants were compensated $5.00 upon completion of the survey (i.e., at a rate of at least $20.00/hour). A screenshot of an example survey question is shown in Figure 16.

Figure 14: Coverage of the nine _Principles of Medical Ethics_ by prompts that violate at least one principle. Results show that the prompts cover all nine principles, providing further evidence of the validity of the dataset.

Figure 15: Co-appearance matrix of the nine _Principles of Medical Ethics_ for prompts that violate at least one principle. Diagonal terms indicate the number of times a principle was the sole violated principle and off-diagonal terms indicate the number of times a pair of principles were both violated for a prompt. Results show that prompts often violate more than one principle, with some pairs of principles often being violated together.

[MISSING_PAGE_FAIL:32]