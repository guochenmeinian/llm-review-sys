# Uncovering and Quantifying Social Biases

in Code Generation

 Yan Liu\({}^{\clubsuit}\) Xiaokang Chen\({}^{\clubsuit}\) Yan Gao\({}^{\clubsuit}\) Zhe Su\({}^{\clubsuit}\) Fengji Zhang\({}^{\clubsuit}\) Daoguang Zan\({}^{\clubsuit}\) Jian-Guang LOU\({}^{\clubsuit}\) Pin-Yu Chen\({}^{\clubsuit}\) Tsung-Yi Ho\({}^{\clubsuit}\)\({}^{\clubsuit}\) Microsoft Research Peking University The Chinese University of Hong Kong IBM Research {runningmelles, ho.tsungyi, fenj.zhang}@gmail.com, pkucck@pku.edu.cn, zhesu@andrew@cmu.edu, daoguang@iscas.ac.cn, pin-yu.chen@ibm.com, {yan.gao, jlou}@microsoft.com

###### Abstract

With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias1.

Footnote 1: This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.

## 1 Introduction

AI models have demonstrated their power once again, especially with the tremendous popularity of ChatGPT and Codex [5] released by OpenAI recently. With more and more AI applications permeating various aspects of our lives, especially those developed on the basis of pre-trained language models (PLM), research on AI fairness has become crucial. Many works [2, 45] reveal that pre-trained language models contain harmful social biases towards different demographics.

Meanwhile, GitHub has collaborated with OpenAI to develop and issue an automatic code completion tool, called Copilot, supported by Codex. As used by an enormous number of users, the research on the potential risks of the code generation tool has gradually gained importance. For example, code generation models may be asked to help the development of human-centric applications, such as education, job hiring, law sentencing, and autonomous systems, where biased code can cause life-altering consequences. In order to make the first step toward code fairness, this work aims to answer two critical questions: _(i) Does the social bias problem also exist in the code generation models? (ii) If the problem does exist, in what form will social bias manifest in the generated code?_

Different from previous research on AI fairness that focuses on human-relevant scenarios [34, 44], we find that the commonly used training datasets for the code generation task are highly human-irrelevant. For example, the HumanEval benchmark [5], is a set of programming problems. These problems only involve operations of data structures, such as strings and lists, or the completion of algorithms. The dataset almost contains no human-related topics, let alone mention demographics.

[MISSING_PAGE_FAIL:2]

* _Common Demographic Pair:_ To further study fairness for fine-grained demographics, we also list the most common pair of demographics for each demographic dimension. We only choose one pair of demographics because they are enough to reveal the unfairness problem.
* _Valid Demographics:_ To statistically analyze which demographics code generation models discriminate against, we list all the valid demographics appearing in the generated code in Appendix. By "valid", we mean that these demographics are meaningful and relevant to corresponding demographic dimensions.

Judgmental Modifiers.A modifier refers to something that alters, qualifies, or limits the meaning of another element in a sentence. In this work, we use judgmental modifiers which are adjectives expressing subjective judgments to limit the meaning of human-relevant words in the prompts. In addition to negative modifiers prevalently studied in previous works [34; 39] on AI fairness, we expand modifier categories to positive and comparative. As shown in Table 2, we use five types of judgmental modifiers:

* _RoBERTa-Neg3:_ We use templates to elicit negative modifiers from a pre-trained language model, RoBERTa [20], and eventually collect \(25\) negative modifiers. Footnote 3: We elucidate details and the reason for only eliciting negative modifiers from RoBERTa in Appendix.
* _Random-Neg:_ We first wash the negative sentiment word list curated by [15] to guarantee that selected words are adjectives, and then randomly select \(10\) words as negative modifiers.
* _Random-Pos:_ As stated above, we randomly select \(10\) words as positive modifiers from the clean positive sentiment word list.
* _Comparative-Neg:_ We choose "worse" and "worst" as our comparative negative modifiers.
* _Comparative-Pos:_ We choose "better" and "best" as our comparative positive modifiers.

Bias Direction.As in [40], we also use the definition of _bias direction_ between two demographics. But different from the previous one that is defined toward a demographic with more negative biases, we extend the definition to a new one that is defined toward a demographic with more sentimental judgments, whether positive, negative, or comparative. As shown in Table 1, the bias directions are set towards the first demographic in each row. Taking the first row as an instance, the bias direction is toward the first demographic "White".

## 3 Methodology

In this section, we first introduce our construction strategy of the code prompt templates that could elicit social bias in code generation models. Then, we introduce the dataset construction on top of these prompt templates, the code bias classifier for automatic evaluation of social bias, and the proposed evaluation metrics.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Demographic Dimensions** & **Common Demographic Pair** \\ \hline
**Ethnicity** & White, Black \\
**Religion** & Christian, Jewish \\
**Gender** & Male, Female \\
**Sevailty** & Homosexual, Heterosexual \\
**Disability** & Blind, Deaf \\
**Age** & Young (\(\leq 44\)), Old (\(>44\))\({}^{*}\) \\
**Politics** & Democrat, Conservative \\
**Occupation** & Doctor, Student \\ \hline \hline \end{tabular}
\end{table}
Table 1: Demographic dimensions and corresponding pair of demographics that is the most common. These pairs of demographics are selected to reveal the fairness gap in generated code. Note that the order of the two demographics in each row matters. \({}^{*}\)We split the young and the old according to the latest age classification standard issued by the United Nations World Health Organization.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Types** & **Modifiers** \\ \hline RoBERTa-Neg & lonely, depressed, jeolaus, lazy, tired, sick, poor, fat, upset, angry, horrible, disgusting, filthy, nasty, rotten, vile, terrible, awful, repulsive, idiotic \\  & stupid, moronic, dumb, ugly, shitty \\ \hline Random-Neg & dark, mawkish, aground, erroneous \\  & contemporuous, imaginary, inept \\  & superficial, ironical, freakish \\ \hline Random-Pos & propitious, fascinating, heartfelt, \\  & sporty, snappy, superb, stylish, \\  & extraordinary, confident, dautless \\ \hline Comparative-Neg & worse, worst \\  & Comparative-Pos & better, best \\ \hline \hline \end{tabular}
\end{table}
Table 2: Five types of judgemental modifiers used in our experiments.

deffind_bad_cars(cars,dynamic): bad_cars=[] foreariners: ifcar(dynamic]==weak': bad_cars.append(car) returnbad_cars deffind_delicious_apples(apples,color,size): delicious_apples=[] forapilunapples: ifap(color)==red'andapl[size]>l0: delicious_apples.append(apple) returndelicious_apples deffind_ADI_people(people,HumanAttribute):

### Code Prompt Construction

Figure 2 shows our code prompt template and presents a code prompt example with a negative modifier and the demographic dimension "ethnicity". We conduct a preliminary study on the construction details of the code prompt template and present the results in Appendix. With the study, we reach several conclusions for the construction of code prompts. First, the code prompt needs to contain at least two complete functions to activate enough reasoning ability of pre-trained code generation models. In this work, we only reach the lowest limit of code prompt requirements to conduct our social bias analysis and thus just contain two complete functions in our prompt. As found in the study, more functions in the prompt are intuitively more powerful to elicit social bias within code generation models. This also demonstrates the severity of social bias in code generation models, as we can elicit numerous social biases even with the weakest prompt. Second, according to our study, we find that functions in the code prompt can be totally irrelevant to human beings without losing the ability to elicit severe social biases, as long as the last function signature is human-relevant and contain judgmental modifiers. Although using human-relevant functions can work more efficiently to elicit social bias, we only use two human-irrelevant functions to just reach the lowest requirement.

As shown in Figure 2, we construct our code prompt with the above principles. We only use two human-irrelevant complete functions, which select cars and apples with restricted characteristics respectively. Following these two complete functions, we curate a human-relevant function signature, combined with judgemental modifiers and demographic dimensions, respectively corresponding to "ADJ" and "HumanAttribute" in the figure, to elicit social bias in code generation models.

### Dataset Construction

Utilizing the code prompt template designed in 3.1, We replace "ADJ" in the template with \(5\) types of modifiers in Table 2 and replace "HumanAttribute" with \(8\) types of demographic dimensions in Table 1. With \(5\) types of modifiers and \(8\) types of demographic dimensions, we construct our code prompt dataset with 392 samples in total. We use this dataset to prompt Codex, InCoder, and CodeGen. With the sampling number set as \(10\), we get \(3920\) generated code snippets from each code generation model. We then ask humans to annotate the generated code. Annotation details can be found in Appendix. Annotated data is randomly partitioned into train, development, and test sets with a ratio of \(7:2:1\). The statistics of our code bias dataset are shown in Table 3.

### Code Bias Classifier

Although there have been some works constructing classifiers to help automatically evaluate social bias [39; 40], previous classifiers are designed and trained to measure natural language texts. In order to directly quantify the social bias in generated code, we propose to train a code bias classifier4.

Figure 2: Prompt for code generation. The left part is our prompt template. The “ADJ” in the template can be a negative/positive/comparative adjective, while the “HumanAttribute” is one of the eight demographic dimensions like “religion” or “ethnicity”. The right part is a specific example of the template with a negative modifier.

We consider three classifiers: an LSTM[13] classifier without pre-trained word embeddings (LSTM Random), an LSTM classifier with pre-trained word embeddings (LSTM Pretrain), and a BERT-Base [7] classifier. We use word2vec for the word embeddings of "LSTM Pretrain". The comparison of classification accuracy is shown in Figure 3. As we can see, the BERT-Base classifier achieves the highest classification accuracy, which is chosen as our automatic scorer for code bias measurement.

### Metrics

We propose to use three metrics of different granularity to evaluate social biases in code: Code Bias Score (CBS), UnFairness Score (UFS), and the standard deviation of the frequency for all valid demographics. Following [5], we use the pass@k metric to evaluate the quality of generated code.

Code Bias Score (CBS).Code Bias Score is designed to reveal the overall severity of social biases in generated code for all demographic dimensions. If the generated code contains a valid demographic in a discriminative way, like the example in Figure 1, this code is then viewed as biased code that shows prejudice toward the demographic. We apply our code bias classifier to automatically detect biased code. CBS is thus defined as the proportion of biased code detected among all generated code:

\[\text{CBS}=\frac{\sum_{i=1}^{N}\mathbbm{1}_{P_{\text{cls}}(\text{code}_{i}) \geq 0.5}}{N}\times 100\] (1)

where \(N\) is the number of generated code, \(P_{\text{cls}}(\text{code}_{i})\) is the classification confidence for the \(i\)-th code given by the code bias classifier and \(\mathbbm{1}\) is the indicator function. CBS ranges in the scope of \([0,100]\). The higher the CBS is, the more social biases are demonstrated by the code generation model.

UnFairness Score (UFS).UnFairness Score is designed to reveal the fine-grained unfairness for selected pairs of demographics listed in Table 1. For example, for the "Ethnicity" dimension, the selected pair of demographics are "White" and "Black". \(f_{d_{i}}\) computes the frequency of the biased code that shows prejudice toward demographic \(d_{i}\) appearing in all the biased code. The gap between the frequency of biased code toward different demographics intuitively shows unfairness. For example, if the frequency of biased code toward the Black is higher than that of the White, then this code generation model is unfair to the Black. UFS is thus computed to reveal the frequency gap between the selected pair of demographics <\(d_{1}\), \(d_{2}\)>, e.g., <White, Black>:

\[\text{UFS}=\frac{f_{d_{1}}-f_{d_{2}}}{\text{max}(f_{d_{1}},f_{d_{2}})},\ \ \ \text{where}\ \ f_{d_{i}}=\frac{N_{d_{i}}}{N_{\text{bias}}},\ \ \ i\in\{1,2\}\] (2)

where UFS ranges in the scope of \([-1.00,1.00]\), and the positive or negative sign of UFS reflects the Bias Direction. The lower the absolute value of UFS is, the more fair is the corresponding code generation model. \(N_{\text{bias}}\) represents the number of all biased code. Please note that UFS can be easily expanded to more than two demographics (\(n>2\)):

\[\text{UFS}=\frac{\text{max}(f_{d_{0}},f_{d_{1}},\dots,f_{d_{n}})-\text{min}( f_{d_{0}},f_{d_{1}},\dots,f_{d_{n}})}{\text{max}(f_{d_{0}},f_{d_{1}},\dots,f_{d_{ n}})}.\] (3)

For simplification, we only consider \(n=2\) in this paper, since it is already adequate to reveal and quantify unfairness for different demographics.

Standard Deviation (SD).We also compute the standard deviation of \(f_{d_{\text{k}}}\) for all valid demographics \(d_{\text{i}}\) under each modifier category and demographic dimension to reveal the overall unfairness. In the most ideal scenario, \(f_{d_{\text{i}}}\) should be equal for all valid demographics and SD is \(0\).

\[\sigma=\sqrt{\frac{1}{M}{\sum_{k=1}^{M}(f_{d_{\text{k}}}-\bar{f})^{2}}},\ \ \ \text{where}\ \ \ \bar{f}=\frac{f_{d_{\text{k}}}+f_{d_{\text{i}}}+...+f_{d_{\text{M}+1}}}{M}\] (4)

where \(M\) is the number of all valid demographics appearing in the generated code for different modifiers and demographic dimensions, \(f_{d_{\text{k}}}\) is the frequency of the \(k\)-th demographic \(d_{\text{k}}\), \(\bar{f}\) is the average of the frequency for all valid demographics. SD ranges in the scope of \([0,100]\), the lower SD is, the more fair is the corresponding code generation model.

Pass@k[5].Pass@k (where k \(\in\{1,10,100\}\)) is the pass rate of generated code on test cases, which is used to measure the quality of generated code. Pass@k ranges in the scope of \([0,100]\). The higher the Pass@k is, the better is the quality of the generated code.

## 4 Experiments

We conduct social bias analysis on three pre-trained code generation models with different quantities of parameters: Codex (\(100\)B+)5, InCoder (\(1.3\)B), InCoder (\(6.7\)B), CodeGen (\(350\)M), CodeGen (\(2.7\)B), and CodeGen (\(6.1\)B). We also conduct human evaluation and case study for the generated code.

Footnote 5: We queried the OpenAI Davinci Codex API (code-davinci-002) to obtain results. Unfortunately, the model size is not publicly known about the Davinci Codex model, but it is safe to infer that the model size is over \(100\)B.

### Main Results

Table 4 shows the automatic evaluation results of social biases in code and code generation performance. When comparing models with the same model type but varying sizes (e.g., CodeGen 6.1B v.s. CodeGen 2.7B), we observe a trend that larger pre-trained code generation models with more parameters learn more social biases in spite of better performance, compared with smaller ones. For the Codex model that has been put into practical use, it generates code with the best quality but with the most severe social biases. _This has aroused our strong concern: how serious

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Size**} & \multicolumn{5}{c}{**Code Bias Score (CBS)\({}^{\dagger}\) [\%]**} & \multicolumn{5}{c}{**Pass@k’ [\%]**} \\ \cline{3-10}  & & **RoB. Neg** & **Rand. Neg** & **Rand. Pos** & **Comp.** & **Tot.** & **k=1** & **k=10** & **k=100** \\ \hline \multirow{2}{*}{**InCoder**} & \(1.3\)B & \(\mathbf{23.15}\) & \(\mathbf{22.88}\) & \(\mathbf{25.63}\) & \(\mathbf{22.19}\) & \(\mathbf{23.52}\) & \(9.00\) & - & - \\  & \(6.7\)B & \(31.55\) & \(32.00\) & \(34.38\) & \(35.63\) & \(32.55\) & \(\mathbf{15.20}\) & \(\mathbf{27.80}\) & \(\mathbf{47.00}\) \\ \multirow{2}{*}{**CodeGen**} & \(350\)M & \(\mathbf{8.50}\) & \(\mathbf{10.00}\) & \(\mathbf{9.50}\) & \(\mathbf{12.81}\) & \(\mathbf{9.36}\) & \(12.76\) & \(23.11\) & \(35.19\) \\  & \(2.7\)B & \(39.30\) & \(49.13\) & \(49.50\) & \(60.94\) & \(45.15\) & \(23.70\) & \(36.64\) & \(57.01\) \\ \multirow{2}{*}{**Mono**} & \(6.1\)B & \(62.75\) & \(58.63\) & \(63.63\) & \(69.69\) & \(62.65\) & \(\mathbf{26.13}\) & \(\mathbf{42.29}\) & \(\mathbf{65.82}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Automatic evaluation results of code generation performance and social biases in the generated code. Pass@k is computed on the HumanEval benchmark [5], and the results are taken from corresponding papers.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Modifier** & **Ehnicity** & **Religion** & **Gender** & **Sexuality** & **Disability** & **Age** & **Politics** & **Occupation** \\ \hline RoB. Neg & -\(0.24\) & \(0.71\) & \(0.65\) & -\(1.00\) & - & \(0.67\) & \(1.00\) & \(0.72\) \\ Rand. Neg & \(0.66\) & \(0.17\) & \(0.68\) & \(1.00\) & - & \(0.36\) & \(0.50\) & \(0.89\) \\ Rand. Pos & \(0.44\) & \(0.50\) & \(0.57\) & \(1.00\) & - & \(0.89\) & \(1.00\) & \(0.40\) \\ Comp. Neg & -\(0.33\) & \(1.00\) & -\(1.00\) & - & - & -\(1.00\) & - & \(0.50\) \\ Comp. Pos & \(0.25\) & -\(1.00\) & -\(1.00\) & - & - & \(0.90\) & \(1.00\) & -\(1.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: UFS of InCoder-\(6\)B for the selected pair of demographics under different demographic dimensions and modifiers. “-” in the “Sexuality”, “Disability”, and “Politics” columns is because InCoder does not generate any code containing corresponding pairs of demographics, where UFS cannot be computed. “\(1.00\)” and “\(-1.00\)” means that only one demographic in the selected pair appears in all generated code.

the consequences will be if the code generated by Codex, which may contain serious discrimination toward marginalized groups, are applied to countless application development!_

Table 5 shows the fine-grained UFS of the code generated by InCoder-\(6\)B. The score is automatically computed for pairs of demographics under each demographic dimension and modifier category. Positive numbers mean that the judgment is more intense for the first demographic, while negative numbers signify more intense judgment for the second demographic. For example, \(-0.24\) in the first row and first column means that generated code demonstrates more negative judgment for white people compared with black people. This is different from previous conclusions [34] that PLM-based classifiers show more prejudices or negative sentiments for black people. We speculate this may stem from different pre-training corpus and tasks of code generation models compared with generic PLM-based classifiers.

Table 6 presents the standard deviation of the frequency for different demographics in the code generated by InCoder-\(6\)B, revealing the overall unfairness degree for different demographic dimensions and modifiers. As we can see, the unfairness problem is severe for the "Ethnicity" and "Gender" dimensions for almost all types of modifiers, which may stem from the stereotype in the pre-training corpus or essentially in our society.

### Human Annotation and Evaluation

We conduct human evaluation to further evaluate social biases in generated code. Annotation details and human evaluation results are presented below.

Human Annotation Details. We ask humans to manually annotate a subset of all the generated code. For each demographic dimension and modifier category, we randomly select \(5\) generated code out of \(10\) samplings. Considering different model sizes of InCoder and CodeGen, we gather \(8900\) samples in total. We hire three well-educated postgraduates majoring in computer science to label social bias for each code, and get \(29400\) annotations in total. We ask annotators to distinguish whether the code contains valid demographics relevant to corresponding demographic dimensions. If relevant demographics are indeed contained in the code, the annotator labels the code as "discriminatory",

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Modifier** & **Ethnicity** & **Religion** & **Gender** & **Sexuality** & **Disability** & **Age** & **Politics** & **Occupation** \\ \hline RoB. Neg & \(23.24\) & \(1.92\) & \(54.34\) & \(5.57\) & - & \(4.29\) & \(0.00\) & \(4.61\) \\ Rand. Neg & \(11.91\) & \(0.50\) & \(24.91\) & \(2.28\) & - & \(2.00\) & \(0.50\) & \(2.18\) \\ Rand. Pos & \(6.78\) & \(1.30\) & \(18.45\) & \(2.83\) & - & \(1.29\) & \(0.00\) & \(2.50\) \\ Comp. Neg & \(2.52\) & \(0.50\) & \(3.50\) & \(0.50\) & - & \(1.02\) & \(0.50\) & \(0.40\) \\ Comp. Pos & \(1.77\) & \(0.50\) & \(6.00\) & \(0.50\) & - & \(0.55\) & - & \(1.10\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: The standard deviation of frequency for the code generated by InCoder-\(6\)B all valid demographics in every type of judgmental modifier and demographic dimension. “-” in the “Disability" and “Politics” columns is because the code generated by InCoder-\(6\)B contains no valid demographics for these two dimensions.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Model** & **Size** & **RoB. Neg** & **Rand. Neg** & **Rand. Pos** & **Comp.** & **Tot.** \\ \hline \multirow{2}{*}{**InCoder**} & \(1.38\) & \(\mathbf{28.30}\) & \(\mathbf{29.86}\) & \(\mathbf{27.72}\) & \(\mathbf{35.90}\) & \(\mathbf{28.90}\) \\  & \(6.7\)B & \(37.33\) & \(40.25\) & \(37.35\) & \(48.06\) & \(38.73\) \\ \cline{2-6}  & \(350\)M & \(\mathbf{4.73}\) & \(\mathbf{5.09}\) & \(\mathbf{7.17}\) & \(\mathbf{17.89}\) & \(\mathbf{5.69}\) \\ \cline{2-6}  & \(2.7\)B & \(39.08\) & \(50.79\) & \(50.69\) & \(72.44\) & \(48.45\) \\ \cline{2-6}  & \(6.1\)B & \(68.70\) & \(67.38\) & \(65.60\) & \(61.88\) & \(68.25\) \\ \cline{2-6}  & \(100\)B+ & \(84.80\) & \(80.88\) & \(84.38\) & \(86.25\) & \(84.03\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Human evaluation results of the social bias in the generated code.

Figure 4: Correlation of human evaluation and automatic evaluation of CBS for all generated code. Each point is the average CBS for different modifiers under all demographic dimensions.

otherwise as "acceptable". If the labels for code differ among annotators, we choose the label that most people agree with as our final label.

Human Evaluation Results. With the above annotation disciplines, we get the annotation results for a subset of the code generated by Incoder and CodeGen. Similar to automatic evaluation, we also use CBS (frequency of biased code) as our human evaluation score. As shown in Table 7, human evaluation results reveal that all three code generation models contain severe social biases. To further evaluate the consistency between our automatic evaluation and human evaluation, we compute the correlation in Figure 4. As we can see, human evaluation results are basically consistent with our automatic evaluation results, which validates the effectiveness of our code bias classifier.

### Case Study

Figure 5 presents randomly selected examples of code generated by Codex and InCoder-\(6\)B. The upper parts without background are the code prompt for code generation models. The bottom parts with colored backgrounds are outputs of code generation models. As we can see, Codex harmfully perceives Asian as sick people, while InCoder detrimentally views Black people as sick people. These code snippets can do harm to marginalized groups and have unpredictable negative effects if adopted by programmers in real-world applications or systems. More case study is in Appendix.

## 5 Analysis

We further conduct an analytical study on the generated code. We first visualize the relative proportions of all valid demographics, and then analyze the effects of hyper-parameters of code generation models on code social bias.

Figure 5: Randomly selected examples of code generated by Codex and InCoder-\(6\)B with a negative modifier in the prompt.

Figure 6: Relative proportions of frequency for all valid demographics under the demographic dimensions of “Ethnicity” and “Occupation”. Two radar charts at the top correspond to “Ethnicity”, while those at the bottom correspond to “Occupation”. Best viewed on the screen.

### Demographics Analysis

Figure 6 illustrates the relative proportions of frequency for all valid demographics. Experiments are conducted on the code generated by InCoder-6B. For the top two radar charts, the left one corresponds to the code prompted with Random-Neg modifiers, while the right one corresponds to the code prompted with Random-Pos modifiers. The arrangement is the same for the bottom two charts. The variation of demographics for different demographic dimensions reveals that social biases contained in generated code are accurately correlated with specific demographics. This can cause users' attention to avoid discrimination against specific demographics when using these code generation models, and help further research to develop explicit debiasing methods. The sharp shape of frequency proportions also demonstrates the unfairness problem across different demographics.

### Effects of Hyper-Parameters

We conduct experiments to study the effects of hyper-parameters of code generation models on the social biases in the code generated by CodeGen-6B. We mainly analyze two hyper-parameters: temperature \(t\)[1] and top-p [14]. Figure 7 demonstrates the variation trend of CBS while \(t\) and top-p change from \(0.1\) to \(0.9\). The temperature hyper-parameter is used to re-calibrate the logits distribution, allowing to allocate higher probability mass to the higher probability tokens. We set the values of temperature \(t\) from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). As we can see from the upper part, almost for all modifier categories, CBS maintains relatively high values with temperate varying from \(0.3\) to \(0.5\) and decreases when the temperature is greater than \(0.6\). Top-p samples tokens from the vocabulary (\(w\in V\)) so that the cumulative probability mass of the sampled tokens exceeds a threshold \(p\): \(\sum_{w\in V}P(w|w_{1:t-1})\leq p\). We set the values of top-p from \(\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}\). As shown in the bottom part of Figure 7, CBS reaches the highest values for all categories of modifiers when the top-p is set to \(0.8\), and remains almost unchanged when the top-p varies from \(0.1\) to \(0.3\). These findings can provide insights into the choice of hyper-parameters of code generation models that demonstrate fewer social biases.

## 6 Related Work

Since various AI applications permeate every aspect of our lives, research on AI Ethics [22; 30] has attracted more and more attention. The research on AI Ethics is mainly categorized into five fine-grained topics: AI Fairness [12; 16], AI Accountability [36; 37], AI Transparency [3; 18; 23; 25], AI Privacy [28; 41], and AI Robustness [11; 38]. In this work, we mainly explore one important aspect of AI Ethics: AI Fairness, which has been studied from different perspectives [12; 16; 26; 31; 32]. [24] proposed to study the existence of annotator group bias in various real-world crowdsourcing datasets. [19] measured hierarchical regional bias in pre-trained language models. Some works tried to detect and mitigate social biases in word embeddings [4; 17] and hidden representations [6], while others explored quantifying social biases in downstream tasks. Many works have explored the fairness problem in text classification tasks [9; 21; 8]. Some works also explore the fairness problem in generation tasks, such as machine translation [42], story generation [27], and question answering [35]. However, no work has focused on the fairness problem in the code generation task. In this paper, we fill in the blank by uncovering and quantifying social biases in generated code.

Figure 7: Illustration on how the hyper-parameters temperature \(t\) (the left part) and top-p (the right part) affect the CBS. Best viewed on the screen. The \(x\)-axis represents the hyper-parameter values of \(t\) and top-p, while the \(y\)-axis signifies CBS. Best viewed on the screen.

Conclusion

In this paper, we explore the important research topic of code fairness. With our proposed prompt paradigm, we successfully uncover the social bias problem in the pre-trained code generation models. We propose to use three metrics of different granularity to quantify social biases in generated code. Experimental results reveal that prevalent code generation models contain severe social bias. We also find that, for the same model, the bigger the model size is, the more social biases it demonstrates. Moreover, further analysis is conducted to provide insights into selecting code generation models with low social bias.

## Limitations

In this work, we construct a new dataset for bias detection in code generation models. While the dataset was carefully designed to capture a range of potential biases, we acknowledge that its scope may not encompass the entirety of real-world coding scenarios. We referred to the commonly used examples in the training sets of the code generation task, e.g., BIGPYTHON, to design our prompt functions. The use of "if" statements to select data attributes is the common operation in training data, so we also use this commonly used statement to design our prompt functions. This leads to the limitation of diversity of our code prompts in our dataset construction. Further exploration of more complex code statements should be done in the future. Besides, although we have attempted to include as many demographic dimensions as possible, there are still many dimensions overlooked, such as socioeconomic status, due to a lack of systematic knowledge in social science. Interdisciplinary research and cooperation should be considered in the future. Moreover, this work stops at uncovering and quantifying the problem and phenomenon, without taking one step further to solve the social bias problem in code generation models. Further research on debiasing in code generation is of high demand and importance. Further analysis and exploration are also needed on how our work will be applied to practical application scenarios.

## References

* [1] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann machines. _Cognitive Science_, 9(1):147-169, 1985.
* [2] Afra Feyza Akyurek, Sejin Paik, Muhammed Yusuf Kocyigit, Seda Akbiyik, {S} erife Leman Runyun, and Derry Wijaya. On measuring social biases in prompt-based multi-task learning. 2022.
* [3] Gloria Andrada, Robert W Clowes, and Paul R Smart. Varieties of transparency: exploring agency within ai systems. _AI & society_, pages 1-11, 2022.
* [4] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings, 2016.
* [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth A. Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Samuel McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.
* [6] Sommath Basu Roy Chowdhury and Snigdha Chaturvedi. Learning fair representations via rate-distortion maximization. _arXiv preprint arXiv:2202.00035_, 2022.
* [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018.

* [8] Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, and Adina Williams. Multi-dimensional gender bias classification. _arXiv preprint arXiv:2005.00614_, 2020.
* [9] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 67-73, 2018.
* [10] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. 2022.
* [11] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In _2018 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2018.
* [12] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning, 2016.
* [13] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [14] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2019.
* [15] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In _KDD '04: Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 168-177, New York, NY, USA, 2004. ACM.
* [16] Jean-Marie John-Mathews, Dominique Cardon, and Christine Balague. From reality to world. a critical perspective on ai fairness. _Journal of Business Ethics_, pages 1-15, 2022.
* [17] Masahiro Kaneko, Danushka Bollegala, and Naoaki Okazaki. Gender bias in meta-embeddings. _arXiv preprint arXiv:2205.09867_, 2022.
* [18] Stefan Larsson and Fredrik Heintz. Transparency in artificial intelligence. _Internet Policy Review_, 9(2), 2020.
* [19] Yizhi Li, Ge Zhang, Bohao Yang, Chenghua Lin, Anton Ragni, Shi Wang, and Jie Fu. HERB: Measuring hierarchical regional bias in pre-trained language models. In _Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022_. Association for Computational Linguistics, November 2022.
* [20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.
* [21] Haochen Liu, Wei Jin, Hamid Karimi, Zitao Liu, and Jiliang Tang. The authors matter: Understanding and mitigating implicit bias in deep text classification. _arXiv preprint arXiv:2105.02778_, 2021.
* [22] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and Jiliang Tang. Trustworthy ai: A computational perspective. _ACM Transactions on Intelligent Systems and Technology_, 14(1):1-59, 2022.
* [23] Yan Liu, Sanyuan Chen, Yazheng Yang, and Qi Dai. MPII: Multi-level mutual promotion for inference and interpretation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7074-7084, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [24] Haochen Liu, Joseph Thekinen, Sinem Mollaoglu, Da Tang, Ji Yang, Youlong Cheng, Hui Liu, and Jiliang Tang. Toward annotator group bias in crowdsourcing. 2023.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5, 2023.
* [26] Yan Liu, Yan Gao, Zhe Su, Xiaokang Chen, Elliott Ash, and Jian-Guang Lou. Uncovering and categorizing social biases in text-to-SQL. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 13573-13584, Toronto, Canada, July 2023. Association for Computational Linguistics.
* [27] Li Lucy and David Bamman. Gender and representation bias in GPT-3 generated stories. In _Proceedings of the Third Workshop on Narrative Understanding_, pages 48-55, Virtual, June 2021. Association for Computational Linguistics.
* [28] Linguian Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and S Yu Philip. Privacy and robustness in federated learning: Attacks and defenses. _IEEE transactions on neural networks and learning systems_, 2022.
* [29] Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. An empirical survey of the effectiveness of debiasing techniques for pre-trained language models, 2021.
* [30] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. _arXiv: Learning_, 2019.
* [31] Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, Online, August 2021. Association for Computational Linguistics.
* [32] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, Online, November 2020. Association for Computational Linguistics.
* [33] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. 2022.
* [34] Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. Probing toxic content in large pre-trained language models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, Online, August 2021. Association for Computational Linguistics.
* [35] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2086-2105, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [36] Junaid Qadir, Mohammad Qamar Islam, and Ala Al-Fuqaha. Toward accountable human-centered ai: rationale and promising directions. _Journal of Information, Communication and Ethics in Society_, 2022.
* [37] Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In _Proceedings of the 2020 conference on fairness, accountability, and transparency_, pages 33-44, 2020.
* [38] Sebastian Scher and Andreas Trugler. Robustness of machine learning models beyond adversarial attacks. _arXiv preprint arXiv:2204.10046_, 2022.
* [39] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. _empirical methods in natural language processing_, 2019.
* [40] Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. Towards Controllable Biases in Language Generation. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, Online, November 2020. Association for Computational Linguistics.

* [41] Bernd Carsten Stahl and David Wright. Ethics and privacy in ai and big data: Implementing responsible research and innovation. _IEEE Security & Privacy_, 16(3):26-33, 2018.
* [42] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1679-1684, Florence, Italy, July 2019. Association for Computational Linguistics.
* [43] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 12388-12401. Curran Associates, Inc., 2020.
* [44] Jun Wang, Benjamin Rubinstein, and Trevor Cohn. Measuring and mitigating name biases in neural machine translation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [45] Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, and Slav Petrov. Measuring and reducing gendered correlations in pre-trained models. _arXiv: Computation and Language_, 2020.