# RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs

 Yue Yu

Georgia Tech

&Wei Ping

NVIDIA

&Zihan Liu

NVIDIA

&Boxin Wang

NVIDIA

&Jiaxuan You

NVIDIA

&Chao Zhang

Georgia Tech

&Mohammad Shoeybi

NVIDIA

&Bryan Catanzaro

NVIDIA

Yue Yu did this work during an internship at NVIDIA. Correspondence to: Yue Yu <yueyu@gatech.edu>, Wei Ping <wping@nvidia.com>.

###### Abstract

Large language models (LLMs) typically utilize the top-\(k\) contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.

## 1 Introduction

Retrieval-augmented generation (RAG) (Lewis et al., 2020; Izacard and Grave, 2021; Lin et al., 2024; Wang et al., 2024) is a widely used technique for customizing large language models (LLMs) to handle long-tail knowledge (Mallen et al., 2023; Asai et al., 2024), provide up-to-date information (Kasai et al., 2023), and adapt to specific domains and tasks (Xiong et al., 2024) without modifying the model weights. In general, a dense embedding-based retriever (Karpukhin et al., 2020; Lin et al., 2023; Wang et al., 2022) first retrieves top-\(k\) chunked contexts from a collection of documents or external database for a given question. Then, LLM reads the top-\(k\) contexts to generate the answer.

However, the current RAG pipeline has the following limitations: _i)_ LLMs are not good at reading too many chunked contexts (e.g., top-100) even with the long-context window, not only due to efficiency reasons, but also because a shorter list of top-\(k\) (e.g., 5, 10) contexts usually leads to higher accuracy of generation (e.g., see Table 5 in Xu et al., 2024). _ii)_ Given a small \(k\), one needs a mechanism to ensure the _high recall_ of relevant contents. Relying solely on a retrieval model may be inadequate due to challenges in learning effective local alignments across the entire embedding space to support accurate matching (Luan et al., 2021). In practice, a separate ranking model (Nogueira et al., 2020; Glass et al., 2022; Ma et al., 2023) that cross-encodes question and candidate context can work better than a dense embedding-based retriever for obtaining the most relevant top-\(k\) contexts from top-\(N\)candidates (\(N\gg k\)). _iii)_ However, the zero-shot generalization capability of the expert ranking model can be relatively limited compared to the versatile LLM itself.

Based on the above considerations, our goal is to design an RAG instruction tuning pipeline that uses a single language model to achieve both high-recall context extraction and high-quality content generation. In previous study, instruction-tuned LLMs demonstrate a strong ability to extract answers from relevant context for a given question (e.g., OpenAI, 2023; Liu et al., 2024; Lin et al., 2024). This capability can be viewed as the "dual capability" of determining whether a chunk of context is relevant to the question thus is useful for generating the answer. We hypothesize that these capabilities mutually enhance each other. Motivated by this insight, we propose RankRAG, which instruction-tunes a single LLM for both context ranking and answer generation in the RAG framework. Furthermore, RankRAG expands upon existing instruction-tuning data by incorporating context-rich QA, retrieval-augmented QA and ranking datasets, enhancing the LLM's ability to filter out irrelevant contexts during both the retrieval and generation phases of RAG.

Our contribution can be summarized as follows:

* We propose RankRAG, a novel framework that enhances LLM's RAG capability through simultaneously instructing the LLM on context ranking and answer generation. During training, we design a specialized task focused on identifying relevant contexts or passages for a given question. This task is structured for ranking and framed as regular question answering with instruction, aligning more effectively with retrieval-augmented generation tasks. At inference, the LLM first reranks the retrieved contexts, then generates answer based on the refined top-\(k\) (e.g., 5). This framework is readily applicable to diverse knowledge-intensive NLP tasks.
* Remarkably, we observe that integrating a small fraction of ranking data into the instruction tuning blend of LLM works surprisingly well on the evaluations of ranking associated with the RAG tasks, even surpassing the LLMs fine-tuned with \(10\times\) more ranking data. We attribute this success to the transferable design of RankRAG training.
* We extensively compare the proposed RankRAG method with several strong baselines, including the open-sourced ChatQA-1.5. On nine general-domain and five biomedical knowledge-intensive benchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B by a margin, respectively.

In the remainder of the paper, we discuss related work in SS 2. We introduce problem setup in SS 3 and RankRAG method in SS 4. We present the experimental setup in SS 5, and conclude the paper in SS 6.

## 2 Related Work

Retrieval-augmented generation (RAG) has been established for knowledge-intensive NLP tasks (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023; Izacard and Grave, 2021). In the standard process, a standalone dense-embedding-based retriever (e.g., Karpukhin et al., 2020) first retrieves relevant information from an external corpus, which the LLM then utilizes in the generation process. To improve this pipeline, recent research has focused on aligning retrievers to the needs of LLMs for generation (Shi et al., 2024; Lin et al., 2024), designing multi-step retrieval processes (Trivedi et al., 2023; Jiang et al., 2023; Jeong et al., 2024; Shao et al., 2023), or filtering irrelevant contexts (Wang et al., 2023; Yoran et al., 2024; Xu et al., 2024). To improve generation, several studies have designed instruction-tuning methods dedicated to enhancing the search (Ma et al., 2023; Zhu et al., 2024; Muennighoff et al., 2024) and RAG capability of LLMs (Liu et al., 2024; Lin et al., 2024; Luo et al., 2023; Asai et al., 2024; Wang et al., 2024).

Although strong retrievers have been introduced (e.g., Lin et al., 2023; Yu et al., 2022; Wang et al., 2022, 2023; Lee et al., 2024), one potential approach to improve retriever is optimizing it along with LLM in an end-to-end manner (e.g., Guu et al., 2020; Shi et al., 2024; Sachan et al., 2021; Izacard et al., 2023). However, this requires surrogate loss for optimization and complicates the training pipeline, especially when the embedding database needs to be re-indexed frequently due to the update of the embedding model (i.e., retriever).

Ranking serves as an intermediate step to improve the quality of information retrieval (Mitra et al., 2018), and has been applied to RAG pipeline for improving generation quality (Glass et al., 2022; Ram et al., 2023). However, these methods still rely on an additional moderate-sized model (e.g. BERT, T5) for ranking, which is often insufficient to capture the relevance between query and contextsand may lack the zero-shot generalization capability. Although recent studies have demonstrated the strong ability of LLMs at ranking tasks (Khalifa et al., 2023; Qin et al., 2024; Sun et al., 2023), how to harvest this ability for the RAG pipeline remains underexplored.

## 3 Preliminaries

In this section, we first introduce the preliminaries of retrieval-augmented generation as well as the problem setup. Then we present the limitations in the current RAG pipeline, which motivates the proposed RankRAG method.

### Problem Setup

In retrieval-augmented generation, a collection of documents or contexts (e.g. Wikipedia) is given, providing the grounded knowledge. Given a question \(q\), the retriever \(\mathcal{R}\) (e.g., a parameterized embedding model) first retrieves top-\(k\) contexts \(\mathcal{C}=\{c_{1},\cdots,c_{k}\}\) that are most relevant to the question. Subsequently, the language model produces the final answer where the answer can either be a short phrase or a long sentence, depending on the type of the target task. Our focus is on autoregressive language models (OpenAI, 2022, 2023; Meta-AI, 2024), which is the most common architectures for LLMs.

### Limitation of Current RAG Pipelines

Before formally introducing RankRAG, we would like to first pinpoint several limitations of the current "retrieve-then-generate" pipeline with large language models.

**Limited Capacity of Retriever.** Current RAG systems usually employ sparse retrieval (e.g. BM25 (Robertson et al., 2004)) or moderate-size (e.g. BERT-based) embedding models (Karpukhin et al., 2020; Lin et al., 2023; Wang et al., 2022) as the retriever \(\mathcal{R}\), mainly due to efficiency consideration as there are often millions of, if not more, documents need to be indexed. These models encode questions and documents _independently_ and calculate the similarity between question and documents using vector similarity metrics. However, the _limited capacity of embedding models_ and _independent processing of query and documents_ constrain the ability to estimate textual relevance between question \(q\) and documents \(d\), reducing their effectiveness in new tasks or domains, verified by both theoretical (Menon et al., 2022) and empirical (Luan et al., 2021; Thakur et al., 2021) studies.

**Trade-off of Picking Top-\(k\) Contexts.** Although the state-of-the-art long-context LLM can take many retrieved contexts as input for answer generation, the performance quickly saturates with increased \(k\) in practice. For example, Xu et al. (2024) finds the optimal number of chunked context \(k\) is around \(10\) for long document QA tasks. As illustrated in Figure 1, we perform evaluation on ChatQA-1.5 (Liu et al., 2024), one of the strongest RAG models with open weights, and find the saturation of accuracy when \(k=10\). In general, a smaller \(k\) often fails to capture all relevant information, compromising the _recall_, given the limited expressibility of retriever. In contrast, a larger \(k\) improves _recall_ but at the cost of introducing irrelevant content that hampers the LLM's ability to generate accurate answers (Yoran et al., 2024; Yu et al., 2023b).

Figure 1: Performance of ChatQA-1.5, one of the strongest RAG models, on different context size \(k\). We observe a trade-off of selecting top-\(k\) contexts: a smaller \(k\) compromises the recall, while a larger \(k\) could introduce irrelevant or noisy context and mislead the LLM generation.

## 4 RankRAG

To address the limitations mentioned in the previous section, we propose the RankRAG method to enhance the LLM's ability for retrieval-augmented generation. Specifically, we instruction-tune the LLM to simultaneously _capture the relevance between the question and context_ and _utilize the retrieved context for answer generation_. The details are introduced as follows.

### Stage-I: Supervised Fine-Tuning (SFT)

It is observed that general instruction-tuning or supervised fine-tuning (SFT) often significantly improves the ability of LLMs to follow instructions, thus improving zero-shot results on various downstream tasks (Wei et al., 2022; Ouyang et al., 2022). As such, we follow existing works (Chung et al., 2024; Wang et al., 2024; Liu et al., 2024) to first leverage SFT on a blend of high quality instruction following datasets, including: _i)_ a _private crowd-sourced conversational dataset_ and _public conversation datasets_: OpenAssistant (Kopf et al., 2023), Dolly (Conover et al., 2023), and SODA (Kim et al., 2023), _ii)_ a long-form QA dataset_ ELI5 that requires elaborate answers (Fan et al., 2019), _iii)_ LLM-generated instructions_: Self-Instruct (Wang et al., 2023b) and Unnatural Instructions (Honovich et al., 2023), _iv)_ FLAN and Chain-of-thought datasets_(Chung et al., 2024).

There are overall 128K SFT examples in total. We make sure that there is _no overlap_ between SFT data and data from evaluation tasks. For each sample in the instruction-following dataset, we take the multi-turn conversational format, use the previous turns of conversation between the user and the assistant as the context, and compute the loss only at the last response from the assistant.

### Stage-II: Unified Instruction-Tuning for Ranking and Generation

The Stage-I SFT empowers the LLMs with basic instruction-following capabilities; however, their performance on RAG tasks often remains suboptimal, as the LLMs are not optimized for extracting answers from retrieved context for a given question. Although recent studies (Lin et al., 2024; Liu et al., 2024; Zhang et al., 2024) enhance the RAG capability of LLM by instruction tuning it on context-rich generation tasks, these approaches can still be ineffective with poor initial retrieval results. RankRAG instruction tunes the LLM for both retrieval-augmented generation and context ranking. In particular, the context ranking capability is crucial to obtain more relevant top-\(k\) context with imperfect retriever.

To achieve this goal, the instruction tuning blend of Stage-II consists the following five parts:

1) **SFT data from Stage-I.** This part is included to maintain LLM's instruction-following capability.

2) **Context-rich QA data.** We first follow Liu et al. (2024) to leverage multiple context-rich QA tasks to enhance the LLM's capability of using context for generation. The training blend we use consists of: _i)_ standard QA and reading comprehension datasets: DROP (Dua et al., 2019), NarrativeQA (Kocisky et al., 2018), Quroef (Dasigi et al., 2019), ROPES (Lin et al., 2019), NewsQA (Trischler et al., 2017), TAT-QA (Zhu et al., 2021), which contains a question, **a golden context** and an answer. _ii)_ conversational QA datasets: HumanAnnotatedConvQA and SyntheticConvQA open-sourced by Liu et al. (2024), which contains a conversation between user and assistant, as well as one background document. The model needs to generate an answer given the conversation history and document.

3) **Retrieval-augmented QA data.** In addition to the above QA datasets used in Liu et al. (2024), we add two datasets with not only gold context but also the top-retrieved context using BM25. Note that it is crucial to improve LLM's robustness over irrelevant context at generation. Being aware of

Figure 2: Two-stage instruction tuning framework for RankRAG.

this, we consider two QA tasks, namely SQuAD (Rajpurkar et al., 2016) and WebQuestions (Berant et al., 2013). For each question with the answer, we combine the gold context with the top-retrieved contexts using BM25, ensuring a total of five contexts. Note that some retrieved contexts may not contain the answer, and could be the "hard-negative" contexts.

4) **Context ranking data.** To empower LLMs with ranking capabilities, we use the popular MS MARCO passage (context) ranking dataset (Bajaj et al., 2016). We treat the gold query-passage pairs \((q,c^{+})\) as relevant while using hard negative passages \((q,c^{-})\) mined via BM25 as irrelevant pairs. The LLM needs to generate "True" or "False" given the corresponding query-passage pair, where the question along with the task-specific instruction is "For the question {question}, access whether the passage is relevant to the question.".

We want to handle ranking in conversational scenarios as well. While MS MARCO spans various topics, the questions are only single-turn short sentences. However, ranking data is only available, if any, at a small amount for conversation QA. To overcome this limitation, we repurpose the conversational QA pairs to generate pseudo relevance pairs. As each conversation is only associated with _one_ document \(d\), we cut each document into 150-word chunks \((c_{1},c_{2},\dots,c_{n})\). We compute the 4-gram recall score between each chunk \(c_{i}\) and the ground-truth answer \(a\), considering segments with a recall score above 0.5 as relevant and those below 0.1 as irrelevant for the corresponding conversation. Note that, each sample contains one question-context pair for this ranking dataset. In total, there are around 50k ranking pairs from MS MARCO ranking and synthetic conversations for instruction finetuning.

5) **Retrieval-augmented ranking data.** We aim to train the LLM with the capability of determining the relevance of multiple contexts simultaneously given a question, which is closer to the test-time behavior of RAG with top-\(k\) contexts. As before, we use two QA datasets, SQuAD (Rajpurkar et al., 2016) and WebQuestions (Berant et al., 2013). We combine the gold context with the top-retrieved contexts using BM25, ensuring a total of five contexts. The contexts containing the answer are considered relevant, and the LLM is trained to _explicitly_ identify all relevant contexts for the question.

**Unifying RAG and ranking with instruction tuning.** It is worth noting that, despite the variety of datasets and tasks described, they can all be cast into a standardized QA format \((x,c,y)\), where \(x\) is the question, \(c\) is the corresponding context, and \(y\) is the target output answer. For example, for the retrieval-augmented ranking data, the question is "_For the question <question>_, _find all the passages from the context that are relevant to the question._" Table 1 exhibits how to cast different tasks into a unified format. Despite its simplicity, this approach has the following advantages: _i_) It empowers the LLM with the ranking capability by adding a relatively small amount of ranking data. _ii_) By standardizing these tasks into a unified format, they can _mutually enhance_ each other. After that, we obtain the final RankRAG model that can be applied to various knowledge-intensive NLP tasks.

### RankRAG Inference: Retrieve-Rerank-Generate Pipeline

As RankRAG incorporates an additional reranking step, the inference pipeline for each question is modified as a _retrieve-rerank-generate_ pipeline, described as follows: (1) the retriever \(\mathcal{R}\) first retrieves top-\(N\) contexts from the corpus. (2) the RankRAG model calculates the relevance score between the question and retrieved \(N\) contexts as the probability of generating the answer as True using the prompt in Table 1, then reranks contexts to only retain top-\(k\) (\(k\ll N\)) contexts, which are then used as the input for the generation step. (3) The top-\(k\) contexts, along with the question, are concatenated and fed back into the RankRAG model to generate the final answer.

\begin{table}
\begin{tabular}{l|l l l} \hline \hline
**Task** & **Question \(x\)** & **Context \(c\)** & **Answer \(y\)** \\ \hline \multirow{2}{*}{Context-rich QA} & \multirow{2}{*}{Answer the following question from context. {question}} & \multirow{2}{*}{Passage: (Passage) (1 Pig.)} & A phrase/sentence \\ \cline{3-3} \cline{5-5}  & & & Passage 1: (Passage 3) \\ \hline \multirow{2}{*}{Retrieval-augmented QA} & \multirow{2}{*}{Answer the following question from context. {question}} & \multirow{2}{*}{Passage: (Passage 5) (Sp. tgt. total)} & A phrase/sentence \\ \cline{3-3} \cline{5-5}  & & & Passage : (Passage) (1 Pig.) \\ \hline \multirow{2}{*}{Context ranking} & \multirow{2}{*}{For the question (question), access whether the passage is relevant to the question.} & \multirow{2}{*}{Passage: (Passage) (1 Pig.)} & True/False \\ \cline{3-3} \cline{5-5}  & & & Passage 1: (Passage 1). \\ \hline \multirow{2}{*}{Retrieval-augmented ranking} & \multirow{2}{*}{For the question (question), find all passages from the context that are relevant to the question.} & \multirow{2}{*}{Passage 1: (Passage 5) (Sp. tgt. total)} & \multirow{2}{*}{Passage Indices} \\ \cline{3-3} \cline{5-5}  & & & Passage 1: (Passage 5) (Sp. tgt. total) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The instruction template for Stage-II. It is worth noting that all the tasks can be unified in the \((x,c,y)\) format, which is able to facilitate effective knowledge transfer across tasks.

**Efficiency Discussion.** We are aware that the addition of a reranking step introduces extra processing time. In practice, for each question, denote the time for indexing and retrieval as \(t_{1}\), the time for using LLM to calculate the relevance score as \(t_{2}\) and the time for generation as \(t_{3}\), then the ratio of added time overhead is \(\frac{N*t_{2}}{t_{1}+t_{3}}\). In practice, calculating relevance typically requires generating just one token and involves much shorter inputs compared to the generation step with top-\(k\) contexts. We provide efficiency study in SS5.5.

## 5 Experiments

In this section, we conduct comprehensive experiments on a variety of knowledge-intensive NLP tasks to demonstrate the zero-shot capabilities of RankRAG.

### Experiment Setup

Tasks and Datasets.We consider 3 types of tasks in experiments: (1) _Open-domain QA_ (OpenQA), which includes NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2023), HotpotQA (Yang et al., 2018) and 2WikimQA (Ho et al., 2020). The first three are single-hop QA tasks, while the last two are multi-hop QA datasets. For NQ, TriviaQA, and HotpotQA, we use the split from KILT benchmark (Petroni et al., 2021)2. (2) _Fact verification_, where we use FEVER (Thorne et al., 2018) from KILT benchmark. (3) _Conversational QA_ (ConvQA), we consider three datasets including Doc2Dial (Feng et al., 2020), TopiOCQA (Adlakha et al., 2022) and INSCIT (Wu et al., 2023), which have long documents that cannot be fitted directly into LLMs thus necessitates retrieval and ranking. The detailed dataset information is in Appendix A.1.

Footnote 2: The results of NQ and TriviaQA using the split from DPR (Karpukhin et al., 2020) are in Appendix F.

**Baselines.** We consider the following baselines: (1) _Baseline LLMs without RAG_, where we consider LLMs trained with proprietary data including InstructGPT (Ouyang et al., 2022), PaLM 2 (Anil et al., 2023), FLAN-LaMDA (Longpre et al., 2023), GLaM (Du et al., 2022), Claude 2 (Anthropic, 2023), Mixtral-8x22B-Instruct (Mistral, 2024), DeepSeek-V2 Chat (DeepSeek, 2024) and only use the official reported results. We also consider two ChatGPT-series models, namely GPT-3.5-turbo (gpt-3.5-turbo-0613) (OpenAI, 2022) and GPT-4 (gpt-4-0613) (OpenAI, 2023). (2) _Baselines with retrieval_, we evaluate models augmented with retrieval. Specifically, we include Atlas (Izacard et al., 2023) and Raven (Huang et al., 2023), two RAG models based on encoder-decoder LMs. For decoder-only models, we consider Self-RAG (Asai et al., 2024a), RECOMP (Xu et al., 2024a), InvnetRetro (Wang et al., 2024), RePlug (Shi et al., 2024), RA-DIT (Lin et al., 2024), Llama-3-instruct (Meta-AI, 2024) and ChatQA-1.5 (Liu et al., 2024). We also list the result of RAG pipelines using InstructGPT (175B parameters) as the backbone including GenRead (Yu et al., 2023a), Retrieve-read (Lazaridou et al., 2022) and ReFeed (Yu et al., 2024), but mainly for reference. Other reported numbers are directly comparable if they follow the standard zero-shot settings.

**Evaluation Metrics.** For OpenQA datasets, we use _Exact Match (EM)_ as the main metric but also report Accuracy for TriviaQA and PopQA and F1 score for HotpotQA and 2WikimQA as it is used in several studies (Asai et al., 2024a; Mallen et al., 2023). For FEVER, we use accuracy as the metric. For ConvQA datasets, we follow (Liu et al., 2024; Wang et al., 2024) to use F1 score as the metric.

**Implementation Details.** We use Llama3 8B and 70B (Meta-AI, 2024) as the backbone in our main experiments. For the two-stage instruction tuning, we set the batch size to 128 and train the model for 1000 steps with learning rate 5e-6 in Stage-I. Then, we reduce the learning rate to 3e-7 for 8B and 2e-7 for 70B model, set the batch size to 64, and train the model for 3300 steps (around 1 epoch). We use the Adam optimizer (Kingma & Ba, 2014) with \(\beta_{1}=0.9\) and \(\beta_{2}=0.98\). During the inference stage, we use the December 2018 Wikidump as the corpus index for NQ, TQA, HotpotQA, 2WikimQA, and use the December 2020 Wikidump for PopQA, following (Asai et al., 2024a). By default, we follow (Wang et al., 2024; Lin et al., 2024; Liu et al., 2024) to use the Dragon retriever (Lin et al., 2023) as default and retrieve top-\(N\) (\(100\) for 8B and \(30\) for 70B) documents for ranking, but RankRAG can be adapted to various retrievers and different \(N\) (see SS 5.3 and 5.5). To ensure a fair comparison, we test the performance of \(k\in\{5,10,20\}\) and report _the best performance_ for baselines. For generation, we keep temperature \(T=0\) and set the maximum number of generated token to be 32 for OpenQA, 128 for ConvQA and 8 for others. Training RankRAG-8B uses 32 NVIDIA A100 GPUs for 10 hours (4 hours for Stage-I and 6 hours for Stage-II finetuning), while training RankRAG-70B uses 128 NVIDIA A100 GPUs for 16 hours (4 hours for Stage-I and 12 hours for Stage-II Finetuning).

[MISSING_PAGE_FAIL:7]

contribute to the final performance. Removing context ranking hurts performance on all tasks, justifying its efficacy in selecting the most relevant contexts for the target question. Besides, the retrieval-augmented QA (RQA) and retrieval-augmented ranking (RAR) designed for instruction fine-tuning improve outcomes on most tasks by helping the model explicitly pinpoint relevant contexts. On the contrary, the RAFT method used in (Lin et al., 2024) treats each retrieved context separately during instruction finetuning, which yields suboptimal results when compared to RankRAG with the same training data.

**Performance with Different LLMs.** Table 4 reports the performance of RankRAG and the most recent baseline ChatQA using Llama2 with backbone having varying amounts of parameters. Notably, there exist consistent gains in terms of the average performance (7.8%/6.4%/6.3% on 7B/13B/70B variants respectively), justifying the advantage of RankRAG across different LLM types and scales.

**Performance with Different Retrievers.** Figure 3 exhibits the performance of RankRAG and ChatQA-1.5 with different dense retrievers on three representative tasks, where we consider DPR (Karpukhin et al., 2020) and Contriever-MS MARCO (Izacard et al., 2022) as two variants. We note that although the initial retrieved result is not good enough, RankRAG still surpasses ChatQA-1.5 by more than 10% for both retrievers on average. To summarize, RankRAG is robust to the choice of retrievers.

### Experiment on Domain-specific RAG Benchmarks

To demonstrate that RankRAG can adapt to specialized domains, we conduct experiments on Min-rage (Xiong et al., 2024), a recently introduced RAG benchmark for the biomedical field. We follow Xiong et al. (2024) to employ MedCPT (Jin et al., 2023) as the retriever \(\mathcal{R}\) with MedCorp3 as the corpus \(\mathcal{D}\).

Footnote 3: Link: https://huggingface.co/MedRAG. Detailed dataset information is in Appendix A.2.

The experiment results of RankRAG and baselines are shown in Table 5. From the table, we observe that RankRAG, even without fine-tuning on the biomedical domain, excels at medical QA tasks.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c} \hline \hline
**Task (Zero-Shot)** & **NQ** & **TrkivQA** & **PwpQA** & **HotpotQA** & **2WkimQA** & **FEVER** & **DocDial** & **TopicOCQA** & **Inseti** & **Avg.** \\ \hline
**Metric** & EM & EM / Acc. & EM / Acc. & EM / F1 & EM / F1 & Acc. & F1 & F1 & – \\ \hline Llama2-70B (Touwenh et al.) & 25.3 & 82.4\({}^{\prime}\) & \(-\)/\(-\)/\(-\) & \(-\)/\(-\) & \(-\)/\(-\) & – & – & – & – & – \\ \hline Llama2-CudpA-1 0.79 (Jin et al.) & 41.7 & 78.56 & 46.75 & 55.0 & 28.94 / 40.3 & 32.67 / 27.5 & 85.9 & 37.9 & 45.5 & 31.0 & 46.6 \\ Llama2-CudpA-1 0.136 (Jin et al.) & 41.7 & 89.5 & 87.6 & 51.8 / 36.2 & 32.73 / 43.2 & 27.6 / 31.1 & 57.6 & 38.1 & 48.9 & 30.8 & 49.6 \\ Llama2-CudpA-1 0.136 (Jin et al.) & 49.5 & 82.8 / 97.7 & 52.1 / 56.6 & 30.9 / 49.4 & 28.9 / 34.1 & 91.7 & 38.9 & 51.0 & 31.9 & 51.8 \\ \hline Llama2-RankRAG 78 & 46.9 & 840.9 / 89.6 & 58.9 / 61.3 & 32.27 / 43.2 & 28.65 / 30.7 & 86.6 & 38.6 & 49.2 & 32.3 & 50.3 \\ Llama2-RankRAG 138 & 50.5 & 84.5 / 91.0 & 58.0 / 63.3 & 36.4 / 47.3 & 29.5 / 34.2 & 91.7 & 39.5 & 49.2 & 33.4 & 52.5 \\ Llama2-RankRAG 708 & 53.2 & 85.8 / 92.1 & 38.7 / 64.5 & 41.8 / 53.1 & 33.8 / 38.8 & 91.9 & 41.2 & 52.9 & 35.8 & 55.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Zero-shot evaluation using Llama2 (Touwenh et al., 2023) model as the backbone.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline
**Task (Zero-Shot)** & **NQ** & **TrkivQA** & **PwpQA** & **HotpotQA** & **2WkimQA** & **FEVER** & **DocDial** & **TopicOCQA** & **Inseti** & **Avg.** \\ \hline
**Metric** & EM & EM / Acc. & EM / Acc. & EM / F1 & EM / F1 & Acc. & F1 & F1 & – \\ \hline RankRAG 8B & **50.2** & **82.9**/89.5** & **57.6**/**64.1** & **35.3**/**46.7** & **31.4**/**36.9** & **92.0** & **40.4** & **50.4** & 33.3 & **52.6** \\ \hline w/o reading & 48.0 & 80.3 / 86.8 & 49.7 / 59.0 & 31.3 / 41.6 & 26.4 / 30.5 & 91.1 & 39.7 & 49.4 & 30.9 & 49.8 \\ w/o ROQA & 49.4 & 82.0 / 88.9 & 55.1/ 62.9 & **35.6**/**45.9** & **31.8**/**37.5** & **92.1** & 39.4 & 46.8 & 32.4 & 51.6 \\ w/o RAR & 46.6 & 82.2 / 89.1 & 50.6 / 62.6 & 31.9 / 45.2 & 31.2 / 35.7 & 91.4 & 39.6 & 48.6 & **33.5** & 51.8 \\ \hline w/ RAFT (Lin et al.) & 43.3 & 80.8 / 87.6 & 48.9 / 56.3 & 30.5 / 41.8 & 25.2 / 29.6 & 91.2 & 36.8 & 46.4 & 30.1 & 48.1 \\ w/ Stage-1\(\,\)SFT Only & 38.3 & 63.7 / 76.6 & 49.8 / 54.6 & 26.5 / 40.3 & 18.0 / 25.9 & 85.7 & 33.3 & 33.7 & 30.5 & 42.2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study of RankRAG. We use Llama3-8B as the backbone. Where ‘RQA’ and ‘RAR’ stands for retrieval-augmented QA and retrieval-augmented ranking data, respectively. For ‘w/o Terranking’, we do not perform ranking in the inference stage.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Metric** & **MNLE-Net** & **PububomQA** & **HuaSGD** & **NdoQA** & **MedCorp3** & **Avg.** \\ \hline GPT+4653 (OpenA) & 87.24 & 70.60 & 92.96 & 82.80 & 66.65 & 79.97 \\ GPT+3.5 (OpenA) & 75.48 & 67.40 & 90.29 & 66.65 & 58.80 & 71.75 \\ Mnih et al. (2019) (Jin et al.) & 75.85 & 67.60 & 87.54 & 60.02 & 56.42 & 69.49 \\ Llama2-TurkQA (Jin et al.) & 54.55 & 30.40 & 73.95 & 44.93 & 43.08 & 53.38 \\ MedCorp (To Chen et al., 2020) & 65.38 & 56.40 & 78.66 & 95.57 & 52.67 & 60.18 \\ PMC-Llama (Bwu et al.) & 52.33 & 25.45 & 48.29 & 56.00 & 62.51 & 52.22 \\ Llama2-CudpA-1 5.8 (Bwu et al.) & 41.00 & 66.0 & 62.69 & 42.36 & 46.97 & 59.96 \\ Llama2-CudpA-15.7 (Bw et al.) & 80.51 & 73.40 & 83.17 & 68.39 & 25.44 & 73.38 \\ \hline Llama2-RankRAG (Wang et al., 2020) & 64.55 & 66.00 & 84.44 & 48.86 & 56.90 & 58.95 \\ Llama2-RankRAG (Wang et al., 2020) & 81.44 & 79.80 & 50.76 & 69.21 & 69.11 & 78.06 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The performance of RankRAG on Mirage, a zero-shot biomedical RAG benchmark. RankRAG and baselines use retrieval by default. Most of numbers are from (Xiong et al., 2024).

Figure 3: Performance with different retrievers. The performance of Recall is in Appendix E.1.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et al. Ms marco: A human generated machine reading comprehension dataset. _arXiv preprint arXiv:1611.09268_, 2016.
* Berant et al. (2013) Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on freebase from question-answer pairs. In _EMNLP_, 2013.
* Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In _ICML_. PMLR, 2022.
* Chen et al. (2023a) Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023a.
* Chen et al. (2023b) Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., Kopf, A., Mohtashami, A., et al. Meditron-70b: Scaling medical pretraining for large language models. _arXiv preprint arXiv:2311.16079_, 2023b.
* Chung et al. (2024) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. _JMLR_, 25(70), 2024.
* Conover et al. (2023) Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free Dolly: Introducing the world's first truly open instruction-tuned llm, 2023.
* Dasigi et al. (2019) Dasigi, P., Liu, N. F., Marasovic, A., Smith, N. A., and Gardner, M. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In _EMNLP_, 2019.
* DeepSeek (2024) DeepSeek. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.
* Du et al. (2022) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of-experts. In _ICML_, 2022.
* Dua et al. (2019) Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In _NAACL_, 2019.
* Fan et al. (2019) Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M. Eli5: Long form question answering. In _ACL_, 2019.
* Feng et al. (2020) Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L. doc2dial: A goal-oriented document-grounded dialogue dataset. In _EMNLP_, 2020.
* Glass et al. (2022) Glass, M., Rossiello, G., Chowdhury, M. F. M., Naik, A., Cai, P., and Gliozzo, A. Re2G: Retrieve, rerank, generate. In _NAACL_, 2022.
* Guu et al. (2020) Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In _ICML_, 2020.
* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In _ICLR_, 2021.
* Ho et al. (2020) Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In _COLING_, 2020.
* Honovich et al. (2023) Honovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. In _ACL_, 2023.
* Huang et al. (2023) Huang, J., Ping, W., Xu, P., Shoeybi, M., Chang, K. C.-C., and Catanzaro, B. Raven: In-context learning with retrieval augmented encoder-decoder language models. _arXiv preprint arXiv:2308.07922_, 2023.
* Izacard and Grave (2021) Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In _EACL_, 2021.
* Imai et al. (2020)Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. _TMLR_, 2022.
* Izacard et al. (2023) Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented language models. _JMLR_, 24(251):1-43, 2023.
* Jeong et al. (2024) Jeong, S., Baek, J., Cho, S., Hwang, S. J., and Park, J. C. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In _NAACL_, 2024.
* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* Jiang et al. (2023) Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G. Active retrieval augmented generation. In _EMNLP_, 2023.
* Jin et al. (2021) Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_, 11(14):6421, 2021.
* Jin et al. (2019) Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X. Pubmedqa: A dataset for biomedical research question answering. In _EMNLP_, 2019.
* Jin et al. (2023) Jin, Q., Kim, W., Chen, Q., Comeau, D. C., Yeganova, L., Wilbur, W. J., and Lu, Z. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. _Bioinformatics_, 39(11), 2023.
* Joshi et al. (2017) Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _ACL_, 2017.
* Karpukhin et al. (2020) Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In _EMNLP_, 2020.
* Kasai et al. (2023) Kasai, J., Sakaguchi, K., yoichi takahashi, Bras, R. L., Asai, A., Yu, X. V., Radev, D., Smith, N. A., Choi, Y., and Inui, K. Realtime QA: What's the answer right now? In _NeurIPS_, 2023.
* Khalifa et al. (2023) Khalifa, M., Logeswaran, L., Lee, M., Lee, H., and Wang, L. Few-shot reranking for multi-hop QA via language model prompting. In _ACL_, 2023.
* Khattab et al. (2022) Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. _arXiv preprint arXiv:2212.14024_, 2022.
* Kim et al. (2023) Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R. L., Alikhani, M., Kim, G., Sap, M., et al. Soda: Million-scale dialogue distillation with social commonsense contextualization. In _EMNLP_, 2023.
* Kingma and Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kocisky et al. (2018) Kocisky, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. _TACL_, 2018.
* Kwiatkowski et al. (2019) Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering research. _TACL_, 2019.
* democratizing large language model alignment. _arXiv preprint arXiv: 2304.07327_, 2023.
* Lazaridou et al. (2022) Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented language models through few-shot prompting for open-domain question answering. _arXiv preprint arXiv:2203.05115_, 2022.
* Kwiatkowski et al. (2023)Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models. _arXiv preprint arXiv:2405.17428_, 2024.
* Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _NeurIPS_, 33, 2020.
* Lin et al. (2019) Lin, K., Tafjord, O., Clark, P., and Gardner, M. Reasoning over paragraph effects in situations. In _Workshop on Machine Reading for Question Answering_, 2019.
* Lin et al. (2023) Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. In _Findings of EMNLP_, 2023.
* Lin et al. (2024) Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., Zettlemoyer, L., and tau Yih, W. RA-DIT: Retrieval-augmented dual instruction tuning. In _ICLR_, 2024.
* Liu et al. (2024) Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B. Chatqa: Surpassing gpt-4 on conversational qa and rag. _arXiv preprint arXiv:2401.10225_, 2024.
* Longpre et al. (2023) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., et al. The flan collection: Designing data and methods for effective instruction tuning. In _ICML_, 2023.
* Luan et al. (2021) Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. Sparse, dense, and attentional representations for text retrieval. _TACL_, 2021.
* Luo et al. (2023) Luo, H., Chuang, Y.-S., Gong, Y., Zhang, T., Kim, Y., Wu, X., Fox, D., Meng, H., and Glass, J. Sail: Search-augmented instruction learning. _arXiv preprint arXiv:2305.15225_, 2023.
* Ma et al. (2023) Ma, X., Wang, L., Yang, N., Wei, F., and Lin, J. Fine-tuning llama for multi-stage text retrieval. _arXiv preprint arXiv:2310.08319_, 2023.
* Mallen et al. (2023) Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In _ACL_, 2023.
* Menon et al. (2022) Menon, A., Jayasumana, S., Rawat, A. S., Kim, S., Reddi, S., and Kumar, S. In defense of dual-encoders for neural ranking. In _ICML_, 2022.
* Meta-AI (2024) Meta-AI. Llama 3 model card. 2024.
* Mistral (2020) Mistral. Mistral 8x22b. 2024. URL https://mistral.ai/news/mistral-8x22b/.
* Mitra et al. (2018) Mitra, B., Craswell, N., et al. An introduction to neural information retrieval. _Foundations and Trends(r) in Information Retrieval_, 2018.
* Muennighoff et al. (2024) Muennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D. Generative representational instruction tuning. _arXiv preprint arXiv:2402.09906_, 2024.
* Nogueira et al. (2020) Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J. Document ranking with a pretrained sequence-to-sequence model. In _Findings of EMNLP_, 2020.
* OpenAI (2022) OpenAI. Introducing ChatGPT, 2022.
* OpenAI (2023) OpenAI. GPT-4, 2023.
* Oren et al. (2024) Oren, Y., Meister, N., Chatterji, N. S., Ladhak, F., and Hashimoto, T. Proving test set contamination in black-box language models. In _ICLR_, 2024.
* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _NeurIPS_, 35, 2022.
* Pal et al. (2022) Pal, A., Umapathi, L. K., and Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In _CHIL_, 2022.
* Pal et al. (2022)Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rocktaschel, T., and Riedel, S. KILT: a benchmark for knowledge intensive language tasks. In _NAACL_, 2021.
* Qin et al. (2024) Qin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen, J., Liu, T., Liu, J., Metzler, D., Wang, X., et al. Large language models are effective text rankers with pairwise ranking prompting. In _Findings of NAACL_, 2024.
* Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. In _EMNLP_, 2016.
* Ram et al. (2023) Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y. In-context retrieval-augmented language models. _TACL_, 2023.
* Robertson et al. (2004) Robertson, S., Zaragoza, H., and Taylor, M. Simple bm25 extension to multiple weighted fields. In _CIKM_, 2004.
* Sachan et al. (2021) Sachan, D. S., Reddy, S., Hamilton, W. L., Dyer, C., and Yogatama, D. End-to-end training of multi-document reader and retriever for open-domain question answering. In _NeurIPS_, 2021.
* Shao et al. (2023) Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In _Findings of EMNLP_, 2023.
* Shi et al. (2024) Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language models. In _NAACL_, 2024.
* Sun et al. (2023) Sun, W., Yan, L., Ma, X., Wang, S., Ren, P., Chen, Z., Yin, D., and Ren, Z. Is ChatGPT good at search? investigating large language models as re-ranking agents. In _EMNLP_, 2023.
* Thakur et al. (2021) Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _NeurIPS_, 2021.
* Thorne et al. (2018) Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. Fever: A large-scale dataset for fact extraction and verification. In _NAACL_, 2018.
* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Trischler et al. (2017) Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. Newsqa: A machine comprehension dataset. In _RepL4NLP Workshop at ACL_, 2017.
* Trivedi et al. (2023) Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In _ACL_, 2023.
* Tsatsaronis et al. (2015) Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Alvers, M. R., Weissenborn, D., Krithara, A., Petridis, S., Polychronopoulos, D., et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. _BMC bioinformatics_, 2015.
* Wang et al. (2024) Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro: Instruction tuning post retrieval-augmented pretraining. In _ICML_, 2024.
* Wang et al. (2022) Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. _arXiv preprint arXiv:2212.03533_, 2022.
* Wang et al. (2023a) Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. Improving text embeddings with large language models. _arXiv preprint arXiv:2401.00368_, 2023a.
* Wang et al. (2023b) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In _ACL_, 2023b.
* Wang et al. (2023c) Wang, Z., Araki, J., Jiang, Z., Parvez, M. R., and Neubig, G. Learning to filter context for retrieval-augmented generation. _arXiv preprint arXiv:2311.08377_, 2023c.

Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In _ICLR_, 2022.
* Wu et al. (2024) Wu, C., Lin, W., Zhang, X., Zhang, Y., Xie, W., and Wang, Y. Pmc-llama: toward building open-source language models for medicine. _JAMIA_, 2024.
* Wu et al. (2023) Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H. Inscit: Information-seeking conversations with mixed-initiative interactions. _TACL_, 2023.
* Xiong et al. (2024) Xiong, G., Jin, Q., Lu, Z., and Zhang, A. Benchmarking retrieval-augmented generation for medicine. _arXiv preprint arXiv:2402.13178_, 2024.
* Xu et al. (2024a) Xu, F., Shi, W., and Choi, E. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In _ICLR_, 2024a.
* Xu et al. (2024b) Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. In _ICLR_, 2024b.
* Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _EMNLP_, 2018.
* Yoran et al. (2024) Yoran, O., Wolfson, T., Ram, O., and Berant, J. Making retrieval-augmented language models robust to irrelevant context. In _ICLR_, 2024.
* Yu et al. (2023a) Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate rather than retrieve: Large language models are strong context generators. In _ICLR_, 2023a.
* Yu et al. (2023b) Yu, W., Zhang, H., Pan, X., Ma, K., Wang, H., and Yu, D. Chain-of-note: Enhancing robustness in retrieval-augmented language models. _arXiv preprint arXiv:2311.09210_, 2023b.
* Yu et al. (2024) Yu, W., Zhang, Z., Liang, Z., Jiang, M., and Sabharwal, A. Improving language models via plug-and-play retrieval feedback, 2024.
* Yu et al. (2022) Yu, Y., Xiong, C., Sun, S., Zhang, C., and Overwijk, A. Coco-dr: Combating distribution shift in zero-shot dense retrieval with contrastive and distributionally robust learning. In _EMNLP_, 2022.
* Zhang et al. (2024) Zhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., and Gonzalez, J. E. Raft: Adapting language model to domain specific rag. _arXiv preprint arXiv:2403.10131_, 2024.
* Zhu et al. (2021) Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In _ACL_, 2021.
* Zhu et al. (2024) Zhu, Y., Zhang, P., Zhang, C., Chen, Y., Xie, B., Dou, Z., Liu, Z., and Wen, J.-R. Inters: Unlocking the power of large language models in search with instruction tuning. In _ACL_, 2024.

Dataset Description

The information for 14 datasets used in RankRAG is listed as follows.

### Main Experiments

* **NQ**(Kwiatkowski et al., 2019) is a widely used question-answering dataset constructed with Wikipedia. The questions are constructed from the Google search engine, and the answers are identified as text spans in the Wikipedia article.
* **TriviaQA**(Joshi et al., 2017) is a challenging QA dataset containing question-answer pairs from trivia enthusiasts and independently gathered evidence documents.
* **PopQA**(Mallen et al., 2023) is an entity-centric QA dataset concentrated on long-tail entities. For PopQA, we follow (Asai et al., 2024) to use the long-tail subset, consisting of questions on 1399 rare entities whose monthly Wikipedia page views are less than 100.
* **HotpotQA**(Yang et al., 2018) is a multi-hop QA dataset, where the goal is to answer complex questions that require understanding and linking information from multiple documents.
* **2WikimQA**(Ho et al., 2020) is also a multi-hop QA designed to test machine understanding across two different Wikipedia entities, evaluating the ability of systems to handle cross-lingual and cross-cultural retrieval and question answering.
* **FEVER**(Thorne et al., 2018) is a fact verification dataset aimed at supporting research into the automatic verification of factual claims. It consists of claims that are manually verified against evidence from Wikipedia, providing a benchmark for fact-checking systems.
* **Doc2Dial**(Feng et al., 2020) is a document-grounded conversational QA dataset covering four domains: DMV, SSA, VA, and Student Aid. Each sample comprises a dialogue where a user poses queries regarding the document, and an agent responds those questions. The average document length is around 101K words.
* **TopiOCQA**(Adlakha et al., 2022) is grounded on the whole Wikipedia. It incorporates topic switching and requires the agent to search the entire Wikipedia for answers to user questions.
* **INSCIT**(Wu et al., 2023) is also grounded on the whole Wikipedia. It studies the case where user questions are under-specified and require clarification.

### Biomedical Benchmarks

* **MMLU-med**(Hendrycks et al., 2021) is a subset of six tasks related to biomedicine, including anatomy, clinical knowledge, professional medicine, human genetics, college medicine, and college biology. It contains 1089 questions in total.
* **MedQA**(Jin et al., 2021) is collected from the US Medical Licensing Examination, contaiing 1273 four-option multiple-choice questions focused on real-world scenarios from professional medical board exams.
* **MedMCQA**(Pal et al., 2022) includes multiple-choice questions derived from Indian medical entrance exams, covering 2400 healthcare topics across 21 medical subjects. We use the 4,183-question development set from MedMCQA, as the test set lacks provided ground truths.
* **PubmedQA**(Jin et al., 2019) is a biomedical research QA dataset consisting of 1000 manually annotated questions based on PubMed abstracts. Answers in PubMedQA are structured as yes/no/maybe to reflect the validity of the questions.
* **BioASQ**(Tsatsaronis et al., 2015) includes 618 questions constructed from biomedical literature without providing the ground truth snippets, challenging RAG systems to infer answers independently.

## Appendix B Data Blending Details for Ranking-enhanced Instruction Finetuning

The dataset blending ratio for Stage-II is as follows:

* Drop: 0.069* narrativeqa: 0.09
* quoref: 0.026
* ropes: 0.026
* Squad (Retrieval-augmented QA): 0.09
* Squad (Retrieval-augmented Ranking): 0.02
* WebQuestions (Retrieval-augmented QA): 0.09
* WebQuestions (Retrieval-augmented Ranking): 0.02
* newsqa: 0.09
* tatqa-arithmetic: 0.15
* tatqa-others: 0.08
* ConvQA: 0.2
* MS MARCO ranking: 0.15
* ConvQA ranking: 0.03
* SFT: 0.2

The ratio for each dataset is further normalized to ensure the total ratio equals to 1.

## Appendix C Prompt Formats of Instruction Tuning

### Stage I: Supervised Fine-tuning

The format template of LLM inputs in stage-I is as follows:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

User: {Question 1} Assistant: {Answer 1}

... User: {Latest Question}

Assistant:

### Stage-II: Unified Instruction-Tuning for Ranking and Generation

The format template of LLM inputs in stage-II are as follows:

1) Context-rich QA data

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage: {(Gold) Passage containing relevant context for QA}

User: {Question 1}

Assistant: {Answer 1}

### User: {Latest Question}

Assistant:

We tailor specific user instructions for various dataset types. For instance:

For datasets requiring short answers (such as DROP, NarrativeQA, Quoref, ROPES, SQuAD1.1, SQuAD2.0, NewsQA), we use: "Answer the following question with a short span."

For datasets that necessitate long answers (such as Synthetic_ConvQA), we instruct: "Please give a full and complete answer for the question."

For datasets involving arithmetic calculations or number extraction from the context (such as TAT-QA), we specify: "Answer the following question with a number from the context or through math arithmetic."

For datasets that may require both short and long answers (such as TAT-QA-Others), we direct: "Answer the following question with a short span, or a full and complete answer."

2) Retrieval-augmented QA data

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage 1: {(Shuffled) Passage 1}

Passage 2: {(Shuffled) Passage 2}

Passage 3: {(Shuffled) Passage 3}

Passage 4: {(Shuffled) Passage 4}

Passage 5: {(Shuffled) Passage 5}

...

User: {Question}

Assistant:

3) Context ranking data

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage: {Passage 1}

User: {For the question <question>, access whether the passage is relevant to the question. Return True if relevant, otherwise False. }

Assistant:

4) Retrieval-augmented ranking data

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot befound in the context.

Passage 1: {(Shuffled) Passage 1}

Passage 2: {(Shuffled) Passage 2}

Passage 3: {(Shuffled) Passage 3}

Passage 4: {(Shuffled) Passage 4}

Passage 5: {(Shuffled) Passage 5}

User: {For the question <question>, access whether the above passages are relevant to the question. Return all the relevant passage id. }

Assistant:

## Appendix D Prompt Formats of Target Tasks

### Context Ranking

NQ/TriviaQA/HotpotQA/PopQA:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage: {Passage}

User: {For the question <question>, access whether the passage is relevant to the question. Return True if relevant, otherwise False. }

Assistant:

FEVER:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage: {Passage}

User: {For the claim <claim>, access whether the passage is relevant to the claim. Return True if relevant, otherwise False. }

Assistant:

Doc2dial, Inscit, TopiocQA:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage: {Passage}

User: {Question 1}

Assistant: {Answer 1}

...

User: {For the question <latest question>, access whether the passage is relevant to the question. Return True if relevant, otherwise False. }

Assistant:

### Rag

NQ/TriviaQA/HotpotQA/PopQA:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage 1: {Rerank Top Passage 1}

Passage 2: {Rerank Top Passage 2}

Passage 3: {Rerank Top Passage 3}

Passage 4: {Rerank Top Passage 4}

Passage 5: {Rerank Top Passage 5}

...

User: {Question}. Answer the above question with a short phrase.

Assistant:

Fever:

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage 1: {Rerank Top Passage 1}

Passage 2: {Rerank Top Passage 2}

Passage 3: {Rerank Top Passage 3}

Passage 4: {Rerank Top Passage 4}

Passage 5: {Rerank Top Passage 5}

...

User: Answer the following question with True or False. Is the claim '<claim>' correct? Assistant:

Doc2dial, Inscit, TopiOCQA:

System: This is a chat between a user and an artificial intelligence assistant.

The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

Passage 1: {Rerank Top Passage 1}

Passage 2: {Rerank Top Passage 2}

Passage 3: {Rerank Top Passage 3}

Passage 4: {Rerank Top Passage 4}

Passage 5: {Rerank Top Passage 5}

User: {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:

## Appendix E Additional Experiment Results

### Ranking Performance Using DPR and Contriever as Retrievers \(\mathcal{R}\)

Table 8 shows the ranking performance of RankRAG-8B using DPR (Karpukhin et al., 2020) and Contriever (Izacard et al., 2022) on three datasets. There are consistent performance gains for all tasks, indicating that RankRAG can apply to many popular retrieval models to improve the quality of retrieved contents.

### RAG Performance with Different \(k\)

We also show the performance of RankRAG with different context size \(k\) in figure 6. From the result, we observe that different from the trend of vanilla RAG approaches (without ranking), \(k=5\) already works well for most datasets. This effectiveness stems from the reranking step, which prioritizes the most relevant contexts at the top, reducing the necessity to include additional contexts.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{**NQ**} & \multicolumn{3}{c}{**DPR**} & \multicolumn{3}{c}{**Contriever**} \\ \cline{2-7}  & R@5 & R@10 & R@20 & R@5 & R@10 & R@20 \\ \hline Before Ranking & 69.50\% & 76.20\% & 81.00\% & 67.60\% & 75.24\% & 80.67\% \\ w/ RankRAG & 77.95\% & 81.70\% & 84.56\% & 75.32\% & 80.18\% & 84.70\% \\ \hline \multirow{2}{*}{**TriviaQA**} & \multicolumn{3}{c}{**DPR**} & \multicolumn{3}{c}{**Contriever**} \\ \cline{2-7}  & R@5 & R@10 & R@20 & R@5 & R@10 & R@20 \\ \hline Before Ranking & 67.80\% & 74.20\% & 80.30\% & 81.95\% & 86.76\% & 90.08\% \\ w/ RankRAG & 77.73\% & 79.40\% & 84.74\% & 88.71\% & 90.05\% & 92.59\% \\ \hline \multirow{2}{*}{**PopQA**} & \multicolumn{3}{c}{**DPR**} & \multicolumn{3}{c}{**Contriever**} \\ \cline{2-7}  & R@5 & R@10 & R@20 & R@5 & R@10 & R@20 \\ \hline Before Ranking & 43.60\% & 48.90\% & 54.25\% & 60.61\% & 65.54\% & 69.90\% \\ w/ RankRAG & 50.32\% & 53.75\% & 57.76\% & 65.11\% & 68.41\% & 71.77\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: Answer Recall Comparison Before and After Ranking on 3 Representative Datasets.

## Appendix F Performance of NQ and Trivia QA on DPR Splits

We observe that the NQ and TriviaQA datasets exist in two versions: one used by the DPR (Karpukhin et al., 2020) and FiD (Izacard and Grave, 2021) papers, which include 3610 and 11316 questions for NQ and TriviaQA, respectively. In contrast, the KILT benchmark (Petroni et al., 2021) utilizes only subsets of these, comprising 2837 and 5355 examples for NQ and TriviaQA, respectively. It is noteworthy that many recent studies report performance metrics on these datasets without clarifying which version was employed for evaluation.

To facilitate an _honest_ and _fair_ comparison, we present the performance of RankRAG on both datasets using the DPR splits in Table 9. Notably, regardless of the subset used, RankRAG consistently outperforms both ChatQA and Llama-3-instruct, our direct competitors, as well as other methods utilizing InstructGPT as backbones. We aim for these results to assist the community in making accurate comparisons when referring to the performance of RankRAG.

## Appendix G Additional Case Studies

Tables 10 and 11 provide additional examples from the PopQA and HotpotQA datasets, which focus on long-tailed and multi-hop QA. These tasks are particularly challenging for retrievers, making it difficult to obtain relevant context from the corpus. Consequently, ChatQA-1.5 often struggles to produce the correct answers. However, the reranking step in RankRAG helps counteract poor initial

\begin{table}
\begin{tabular}{l l c c} \hline \hline
**Model** & **Model Configuration** & **NQ EM (\%)** & **TriviaQA EM / Acc. (\%)** \\ \hline \multicolumn{4}{l}{_Representative Baselines_} \\ \hline \multirow{7}{*}{OpenAI GPT} & GPT-3.5-0613 & 35.2 & 70.1 / 81.3 \\  & GPT-3.5-0613 RAG & 42.3 & 65.8 / 76.7 \\ \cline{1-1}  & GPT-4-0613 & 37.2 & **72.6 / 85.1** \\ \cline{1-1}  & GPT-4-0613 RAG & 36.2 & 61.2 / 75.9 \\ \cline{1-1}  & GPT-4-turbo-2024-0409 & 38.3 & 68.0 / 84.5 \\ \cline{1-1}  & GPT-4-turbo-2024-0409 RAG & 36.3 & 57.6 / 79.2 \\ \hline \multicolumn{4}{l}{_Using Llama-2 (Touvron et al., 2023) as the backbone LLM_} \\ \hline Llama-2-Chat & Llama-2 RAG 70B & 37.7 & 65.6 / – \\ \hline \multirow{2}{*}{ChatQA-1.0} & Llama-2 7B & 37.0 & 62.4 / 74.3 \\  & Llama-2 13B & 43.9 & 66.6 / 76.9 \\ \cline{1-1}  & Llama-2 70B & 45.0 & 69.8 / 80.2 \\ \hline \multirow{2}{*}{RankRAG} & Llama-2 7B & 42.4 & 68.3 / 78.9 \\ \cline{1-1}  & Llama-2 13B & 46.2 & 69.5 / 80.0 \\ \cline{1-1}  & Llama-2 70B & 48.7 & 72.3 / 82.6 \\ \hline \multicolumn{4}{l}{_Using Llama-3 (Meta-AI, 2024) as the backbone LLM_} \\ \hline \multirow{2}{*}{Llama-3-Instruct} & Llama-3-Instruct RAG 8B & 27.6 & 57.1 / 74.6 \\ \cline{1-1}  & Llama-3-Instruct RAG 70B & 37.3 & 67.6 / 79.6 \\ \hline \multirow{2}{*}{ChatQA-1.5} & Llama-3 8B & 44.1 & 65.4 / 75.8 \\ \cline{1-1}  & Llama-3 70B & 46.0 & 69.0 / 80.4 \\ \hline \multirow{2}{*}{RankRAG} & Llama-3 8B & 46.1 & 68.8 / 79.9 \\ \cline{1-1}  & Llama-3 70B & **50.0** & **72.6 / 82.9** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance Across Models.

Figure 6: Performance of RankRAG on different context size \(k\).

retrieval by finding more pertinent evidence. Coupled with RAG-oriented finetuning, RankRAG effectively filters out distracting entities and pinpoints the correct answers.

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline
**Q**: Who is the author of The Universe Around Us? **A**: James Hepwood Jeans / James Jeans / Sir James Jeans / Sir James Hopwood Jeans \\ \hline
**Passage 1**: Heoper is the author of two books published by Smithsonian Books/HarperCollins. The first, Dark Cosmos: In Search of our Universe /s Missing Mans and Energy (2006) was named a notable book by Sted Magazine. His second book, Nature 3 Blueprint: Supersymmetry and the Search for a United Theory of Matter and Force (2008), was called “essential reading” by ”New Scientist.. **Passage 2**: Frakens is the author or co-author of it 1d books in the field of astronomy. He was the lead author of Weygaes through the Universe, an introductory college attorney textbook published by Brooks-Cole, which went through three editions. In the 1980s, he co-edited with Bryan Preis two collections of science articles and science fiction stories, “The Universe” and “The Planets.” With Sydney Wolf, Franklin founded and was co-circ of the first on-line journal devoted to astronomy education, “Astronomy Education Review”. **CharQA-15**: **Passage 3**: The Universe” and “The Planets." With Sydney Wolf, Frakens founded and was co-efitted for the first on-line journal devoted to astronomy education, “Astronomy Education Review”. He edited two collections of resources for K-12 teachers, The Universe at Your Fingerprints and Three Universe at Your Fingerprints published through the Astronomical Society of: **Passage 4**: Lincoln Kinnear Barret (1996-1979) was an editor and author, most notably at Life Magazine for many years. Lincoln Barret wrote a number of books, including “The Universe and Doctor Einstein” **Passage 5**: The Universe Master is a science fiction novelty? An current author. E.van Vogt, published in 1953 by Ace Books as an Ace Double with the World of Null-A. It is based on the authors” The Shadow Mea” (Starting Stokes, 1950), Set 400 years into the future, the main character is Morton Cagli, u. C. Army editor who served in the Korean War.. **Prediction**: Lincoln Barret (**✗**)** \\ \hline
**Passage 1**: The Universe Around Us is a science book written by English astrophysicist Sir James Jeans, first published in 1929 by the Syrides of the Cambridge University Press.. **Passage 2**: These books made Jeans fairly well known as an expositor of the revolutionary scientific discoveries of his day, especially in relativity and physical cosmology. In 1939, the Journal of the British Astronomical Association reported that Jeans was going to stand as a candidate for **Passage 3**: James Jeans books for the lay public, including “The Stars in Their Course” (1931), “The Universe Around Us.”, “Through Space and Time” (1934), “The New Background of Science”, “1933, “and The Mysterious Universe. **Passage 4**: The Universe Medal Us go special scientific knowledge. Parts of the book cover the same ground as various lectures I have recently delivered to University and other audiences, including a course of wireless talks jerk autumn. It has been found necessary to rewrite these almost in their entirety, so that very few sentences remain in their original form, but those who have asked me to publish my lectures and wireless talks will find the author of them in the present book. **Passage 5**: Lincoln Barret Lincoln Kramer Barrett (1996-1979) was an editor and author, most notably at Life Magazine for many years. Lincoln Barret wrote a number of books, including “The Universe and Doctor Einstein”, The World We Live In’, and “The Treasure of Our Tongue”. The Universe and Doctor Einstein is a layman introduction to the theory of relativity. In clouds a forwardby Albert Einstein, and has been reprinted several times... **Prediction**: Sir James Keans (✗) \\ \hline \hline \end{tabular}
\end{table}
Table 11: A case study on the top-retrieved context and predictions on HotpotQA dataset, illustrating the effectiveness of RankRAG-8B over ChatQA-1.5-SB. Red text denotes distractors, while green stands for evidences.

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline
**Q**: Who song did Eminen and Rhinama collaborate on after their other collaboration song in studio album ‘Unapologicie?" **A**: The Monster \\ \hline
**Passage 1**: Unapologicie is the seventh studio album by graduation singer Rhinama. It was released on November 19, 2012, by Def Jam Recorings and SRP Records. It was recorded between June and November 2012, during promotion of her sixth album, “Thk Flat Talk” (2011). As executive producer, Rhinama tested previously collaborators The Dream, David Guetta, Chase \& Status, and StarGate to work alongside new collaborators such as Parker Ignile, Mike Willad-Ito, and Labarith. **Passage 2**: Def Jam France announced via Twitter that Rhinama would release a new single the upcoming week while her seventh studio album as scheduled to be released November 2012. On October 1, 2012, in one of her tweets revealed that the title of her new album is "Unapologicie" alongside with its cover. ”What Now was written by British singer-asprowriter Lviv Texas Piogrevelt with Rhinama, Parker light and Nathan Casells, while the production of the song was done by the latter two. Iblight and Casells. **Passage 3**: Justin then went to work on a “Suity with Milky Ekko and recorded by Barbana singer Rhinama for her seventh studio album, “Unapologicie” (2012). It features great vocals by Mikly Ekko, and was released as the second single from the album on 7 January 2013. The song reached the top for twenty-four countries worldwide including number four in the UK and number three on the US Billboard Hot (100, bottom Rhinama's twenty-fourth up ten on the latter chart. **Passage 4**: Via her official Twitter account, Rhamn posted series of "teasing" tweets smoking he seventh studio album. On October 1, 2012, in one of her tweets revealed that the title of her new album is "Unapologicie" alongside with its cover. “Jump” is the overall seventh and final single of Unapologicie. It was written by Kevin Casson and M. B. Williams together with its producers StarGate (Makel's Eins and Teklemen's and Chase \& Status (Sand Milton) **Passage 5**: copies of the song were sold in the UK, making "Love the Way You Lie (our they "Le out the country's biggest-using song of 2010. The same year, a sequel to the single, title!" Love the Way You Lie (Part IV” was raised as part of Rhinama's fifth audio album "Loud", it mainly views matters from the female protagonist perspective. In November 2012, Eminen and Rhinama collaborated again on "Namb", which “In the Monster (**✗**) **Pedication**: Love the Way You Lie (Part IV)” was released as part of Rhinama’s fifth audio album "Loud", it mainly views matters from the female protagonist perspective. In _Nember 2012, Eminen and Rhinama collaborated again on “Namb", it mainly views matters from the female protagonist perspective. In November 2012, Eminen and Rhinama collaborated again on “Namb", it was included on Rhinama’s seventh album "Unapologicie?"_. **Passage 2**: **Namb’ is a song by Barbana singer Rhinama from her seventh studio album "Unapologicie?"_. (2012). It features great vocals by “One may familiar, making the pair’s third column since the two total revisions of “To Love the Way You Lie’. Following the album's release, “Namb” started on multiple charts worldwide including in Canada, the United Kingdom and the United States. “Namb” lasts for a duration of **Passage 3**: Emmenia who awarded to experiment with “retro, strange” sounds such as heateas and scratches, and be left that Rubin could help him “take it to another level”. Rhinama, with whom Eminen previously collaborated on “Love the Way but "Lok" formenheimer’s condition editor. However,” (2010), was featured on the song “The Monster”. On September 11, 2013, she hinted at the **Passage 4** together with Jay-Z. Bono and The Edge for the same campaign to alleviate the 2010 Hait earthquake. In summer 2010, Rhinama collaborated with larger Eminen on “Love the Way You Lie”, which was a major worldwide success, reaching No. 1 in over 20 countries. Raching number 2, the song became the biggest-selling song of 2010 in the UK and the first of Rhinama’s singles to sell over a million copies in the country. In October 2010, Rhinama’s withdrawal numbers **Passage 5**: Emmenia asked for more tracks and subsequently heard “Love the Way You Lie”. He chose it and told his manager Paul Rosenberg he wanted to collaborate with the Barbana singer Rhinama. Eminen told skyrock, It’s one of those tracks that I felt like only she could pull it off.” Rosenberg sent the track to Rhinama, who accepted Emmien’s request “at the last moment.” Eminen then wrote the raped vvess. **Prediction**: The Monster (✗) \\ \hline \hline \end{tabular}
\end{table}
Table 10: A case study on the top-retrieved context and predictions on PopQA dataset, illustrating the effectiveness of RankRAG-8B over ChatQA-1.5-SB. Red text denotes distractors, while green stands for evidences.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have comprehensive experimental results in SS 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of RankRAG in Appendix **??**. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This work does not propose any theory assumptions and does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the details of the data collection for training and evaluations, which can be found in section 4, 5. We will open-source model weights and scripts for reproducing our results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We open-source model weights and scripts for reproducing our results. For training data, they are all public available data with open access. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental details can be found in section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: All results are zero-shot and deterministic; we use greedy search for generations. Retrieval and re-ranking scores are also deterministic. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the compute resources information in section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential societal impacts and negative impacts in Appendix 7. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all data, models used in the paper properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We did not introduce any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.