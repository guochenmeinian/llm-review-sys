# When to Sense and Control?

A Time-adaptive Approach for Continuous-Time RL

 Lenart Treven, Bhavya Sukhija, Yarden As, Florian Dorfler, Andreas Krause

ETH Zurich, Switzerland

Correspondence to lenart.treven@inf.ethz.ch

###### Abstract

Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system. In this work, we formalize an RL framework, _Time-adaptive_**C**_ontrol & Sensing_ (**TaCoS**), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve. We demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency. Finally, we propose OTaCoS, an efficient model-based algorithm for our setting. We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.

## 1 Introduction

Nearly all state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2018; Lillicrap et al., 2015; Schulman et al., 2015) were developed for discrete-time MDPs. Nevertheless, continuous-time systems are ubiquitous in nature, ranging from robotics, biology, medicine, environment and sustainability etc. (cf. Spong et al., 2006; Jones et al., 2009; Lenhart and Workman, 2007; Panetta and Fister, 2003; Turchetta et al., 2022). Such systems can be naturally modeled with stochastic differential equations (SDEs), but computational approaches necessitate discretization. Furthermore, in many applications, obtaining measurements and switching actions is expensive. For instance, consider a greenhouse of fruits or medical treatment recommendations. In both cases, each measurement (crop inspection, medical exam) or switching of actions (climate control, treatment adjustment) typically involves costly human intervention. Hence, minimizing such interactions with the underlying system is desirable. This underlying challenge is rarely addressed in the RL literature.

In practice, a time-equidistant discretization frequency is set, often manually, adjusted to the underlying system's characteristic time scale. This is challenging, however, especially for unknown/uncertain systems, and systems with multiple dominant time scales (Engquist et al., 2007). Therefore, for many real-world applications having a global frequency of control is inadequate and wasteful. For example, in medicine, patient monitoring often requires higher frequency interaction during the onset of illness and lower frequency interactions as the patient recovers (Kaandorp and Koole, 2007).

In this work, we address this limitation of standard RL methods and propose a novel RL framework, **T**ime-**a**daptive **C**ontrol & **S**ensing (**TaCoS**). TaCoS reduces a general continuous-time RL problem with underlying SDE dynamics to an equivalent discrete-time MDP, that can be solved with anyRL algorithm, including standard policy gradient methods like PPO and SAC (Schulman et al., 2017; Haarnoja et al., 2018). We summarize our contributions below.

**Contributions**

1. We reformulate the problem of time-adaptive continuous time RL to an equaivalent discrete-time MDP that can be solved with standard RL algorithms.
2. Using our formulation, we extend standard policy gradient techniques (Haarnoja et al. (2018) and Schulman et al. (2017)) to the time-adaptive setting. Our empirical results on standard RL benchmarks (Freeman et al., 2021) show that TACoS outperforms its discrete-time counterpart in terms of policy performance, computational cost, and sample efficiency.
3. To further improve sample efficiency, we propose a model-based RL algorithm, OTAcoS. OTACoS uses well-calibrated probabilistic models to capture epistemic uncertainty and, similar to Curi et al. (2020) and Treven et al. (2023), leverages the principle of optimism in the face of uncertainty to guide exploration during learning. We theoretically prove that OTACoS suffers no regret and empirically demonstrate its sample efficiency.

## 2 Problem statement

We consider a general nonlinear continuous time dynamical system with continuous state \(\mathcal{X}\subset\mathbb{R}^{d_{\bm{u}}}\) and action \(\mathcal{U}\subset\mathbb{R}^{d_{\bm{u}}}\) space. The underlying dynamics are governed by a (controllable) SDE:

\[d\bm{x}_{t}=\bm{f}^{*}(\bm{x}_{t},\bm{u}_{t})dt+\bm{g}^{*}(\bm{x}_{t},\bm{u}_ {t})dB_{t}.\] (1)

Here \(\bm{x}_{t}\in\mathcal{X}\) is the state at time \(t\), \(\bm{u}_{t}\in\mathcal{U}\) the control input, \(\bm{f}^{*},\bm{g}^{*}\) are unknown drift and diffusion functions and \(\bm{B}_{t}\) is a standard Brownian motion in \(\mathbb{R}^{d_{\bm{B}}}\). Our goal is to find a control policy \(\bm{\pi}_{\mathcal{U}}:\mathcal{X}\times\mathcal{T}\to\mathcal{U}\) which maximizes an unknown reward \(b^{*}(\bm{x}_{t},\bm{u}_{t})\) over a fixed horizon \(\mathcal{T}\stackrel{{\text{def}}}{{=}}[0,T]\), i.e.,

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}\left[\int_{t\in\mathcal{T}}b^{*}(\bm{x}_{t}, \bm{\pi}_{\mathcal{U}}(\bm{x}_{t},t))dt\right],\]

where the expectation is taken w.r.t. the policy and stochastic dynamics and \(\Pi\) is the class of policies2 over which we search.

Footnote 2: We assume that \(\Pi\) is the set of \(L_{\bm{\pi}}\)-Lipschitz policies

In practice, we can only measure the system state and execute control policies in discrete points in time. In this work, we focus on problems where state measurement and control are synchronized in time. We refer to these synchronized time points as _interactions_ in the following parts of this paper. Synchronizing state measurement and control contrasts standard time-adaptive approaches such as event-triggered control (Heemels et al., 2021), where the state is measured arbitrarily high frequency and control inputs are changed only so often to ensure stability. It is also in contrast to the complementary setting, where control inputs are changing at an arbitrarily high frequency but measurements are collected adaptively in time (Treven et al., 2023). An adaptive control approach as Heemels et al. (2021) is very important for many real-world applications but similarly, an adaptive measurement strategy is crucial for efficient learning in RL (Treven et al., 2023). Our approach treats both of these requirements jointly.

We consider two different scenarios for continuous-time control: (_i_) Penalizing interactions with some cost, (_ii_) bounded number of interactions, i.e., hard constraint on control/measurement steps.

**Interaction cost** We consider the setting where every interaction we take has an inherent cost \(c(\bm{x}_{t},\bm{u}_{t})>0\). Note that we consider this cost structure for its simplicity and TACoS works for more general cost functions that depend on the duration of application for the action \(\bm{u}_{t}\) or the previous action \(\bm{u}_{t-1}\) and thus captures many practical real-world settings. We define this task more formally below

\[\max_{\bm{\pi}\in\Pi,\pi_{\mathcal{T}}}\mathbb{E}\left[\sum_{i=0 }^{K-1}\int_{t_{i-1}}^{t_{i}}b^{*}(\bm{x}_{t},\bm{\pi}_{\mathcal{U}}(\bm{x}_ {t_{i-1}},t_{i-1}))dt-c(\bm{x}_{t_{i-1}},\bm{\pi}_{\mathcal{U}}(\bm{x}_{t_{i- 1}},t_{i-1}))\right],\] (2) \[t_{i}=\pi_{\mathcal{T}}(\bm{x}_{t_{i-1}},t_{i-1})+t_{i-1},\ t_{0 }=0,t_{K}=T,\ \forall(\bm{x},t)\in\mathcal{X}\times\mathcal{T};\pi_{\mathcal{T}}(\bm{x},t) \in[t_{\min},t_{\max}].\]

Here \(t_{\min}>0\) is the minimal duration for which we have to apply the control, \(t_{\max}\in[t_{\min},T]\) the maximum duration, and \(\pi_{\mathcal{T}}\) is a policy that predicts the duration of applying the action.

[MISSING_PAGE_FAIL:3]

### Reformulation of Bounded Number of Interactions to Discrete-time MDPs

The second setting is similar to the one studied by Ni and Jang (2022). In this case, we consider the following class of policies:

\[\Pi_{BI}=\{\bm{\pi}:\mathcal{X}\times\mathcal{T}\times\mathbb{N}\to \mathcal{U}\times\mathcal{T}\mid\forall k\in[K]:\bm{\pi}(\cdot,\cdot,k)\text{ is }L_{\bm{\pi}}-\text{Lipschitz}\}\.\]

For an augmented state \(\bm{s}=(\bm{x},b,t,k)\), our policies map states \(\bm{x}\), time-to-go \(t\), number of past interactions \(k\) to a controller \(\bm{u}\) and the time duration \(\tau\) for applying the action. Here the optimal control problem reads

\[\max_{\bm{\pi}\in\Pi_{BI}}V_{\bm{\pi},\bm{\Phi}^{*}}(\bm{x}_{0},T )=\max_{\bm{\pi}\in\Pi_{BI}}\ \mathbb{E}\left[\sum_{k=0}^{K-1}r(\bm{s}_{k},\bm{\pi}(\bm{s}_{k}))\right]\] (7) \[\text{s.t.}\quad\bm{s}_{k+1}=\bm{\Psi}_{\bm{\Phi}^{*}}(\bm{s}_{k},\bm{\pi}(\bm{s}_{k}),\bm{w}_{k}),\ \bm{s}_{0}=(\bm{x}_{0},0,T,0),\]

where,

\[\bm{\Psi}_{\bm{\Phi}^{*}}(\bm{s}_{k},\bm{\pi}(\bm{s}_{k}),\bm{w}_ {k}) =(\bm{\Phi}^{*}(\bm{x}_{k},\bm{\pi}(\bm{x}_{k},t_{k},k))+\bm{w}_{k},t_{k}-\pi_{\mathcal{T}}(\bm{x}_{k},t_{k},k),k+1)\] \[r(\bm{s}_{k},\bm{\pi}(\bm{s}_{k})) =\Xi_{b^{*}}(\bm{x}_{k},\bm{\pi}(\bm{x}_{k},t_{k},k)).\]

In the following, we provide a simple proposition which shows that our reformulated problem is equivalent to its continuous-time counterpart from Section 2.

**Proposition 1**.: _The problem in Equation (2) and 3 are equivalent to Equation (6) and 7, respectively._

Figure 1 depicts the influence of interaction cost and \(K\) on the controller's performance for the pendulum environment.

## 4 TaCoS with Model-free RL Algorithms

We now illustrate the performance of TaCoS on several well-studied robotic RL tasks. We consider the RC car (Kabzan et al., 2020), Greenhouse (Tap, 2000), Pendulum, Reacher, Halfcheetah and Humanoid environments from Brax (Freeman et al., 2021). Thus our experiments range from environments necessitating time-adaptive control like the Greenhouse, a realistic and highly dynamic race car simulation, and a very high dimensional RL task like the Humanoid.3

Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.

We investigate both the bounded number of interactions and interaction cost settings in our experiments. In particular, we study how the bound \(K\) affects the performance of TaCoS and compare it to the standard equidistant baseline. We further study the interplay between the stochasticity of the environments (magnitude of \(\bm{g}^{*}\)) and interaction costs and the influence of \(t_{\text{min}}\)on TaCoS. For all experiments in this section, we combine SAC with TaCoS (SAC-TaCoS).

How does the bound on the number of interactions \(K\) affect TaCoS?We analyze the bounded number of interactions setting (cf. Section 3.2) of TaCoS, studying the relationship between the number of interactions and the achieved episode reward. We compare our algorithm with the standard equidistant time discretization approach which splits the whole horizon \(T\) into \(T/K\) discrete time steps at which an interaction takes place. We evaluate the two methods in the greenhouse and pendulum environments. For the pendulum, we consider the swing-up and swing-down tasks. The results are reported in Figure 2. The time-adaptive approach performs significantly better than the standard equidistant time discretization. This is particularly the case for the greenhouse and pendulum swing-down tasks. Both tasks involve driving the system to a stable equilibrium and thus, while high-frequency interaction might be necessary at the initial stages, a fairly low interaction frequency can be maintained when the system has reached the equilibrium state. This demonstrates the practical benefits of time-adaptive control.

How does the interaction cost magnitude influence TaCoS?We investigate the setting from Section 3.1 with interaction costs. In our experiments, we always pick a constant cost, i.e., \(c(\bm{x},\bm{u})=C\). We study the influence of \(C\) on the episode reward and on the number of interactions that the policy has with the system within an episode. We again evaluate this on the greenhouse and pendulum environment. For the pendulum, we consider the swing-up task. The results are presented in the first row of Figure 3. Noticeably, increasing \(C\) reduces the number of interactions. The decrease is

Figure 3: Effect of interaction cost (first row) and environment stochasticity (second row) on the number of interactions and episode reward for the Pendulum and Greenhouse tasks.

Figure 2: We study the effects of the bound on interactions \(K\) on the performance of the agent. TaCoS performs significantly better than equidistant discretization, especially for small values of \(K\).

d drastic for the greenhouse environment since it can be controlled with considerably fewer interactions without having any effect on the performance. Generally, we observe that decreasing the number of interactions, that is, increasing \(C\), also results in a slight decline in episode reward.

How does environment stochasticity influence the number of interactions?We analyze the influence of the environment's stochasticity, i.e., the magnitude of the diffusion term \(\bm{g}^{*}\), on the episode reward and number of interactions on TaCoS. Intuitively, the more stochastic the environment, the more interactions we would require to stabilize the system. We again evaluate our method on the greenhouse and pendulum swing-up tasks. The results are reported in the second row of Figure 3. The results verify our intuition that more stochasticity in the environment generally leads to more interactions. However, we observe that the policy is still able to achieve high rewards for a wide range of magnitude of \(\bm{g}^{*}\). This showcases the robustness and adaptability of TaCoS to stochastic environments.

How does \(t_{\min}\) influence TaCoS?As highlighted in Section 1, picking the right discretization for interactions is a challenging task. We show that TaCoS can naturally alleviate this issue and adaptively pick the frequency of interaction while also being more computationally and data-efficient. Moreover, we show that TaCoS is robust to the choice of \(t_{\min}\), which represents the minimal duration an action has to be applied, i.e., its inverse is the highest frequency at which we can control the system. In this experiment, we consider SAC-TaCoS and compare it to the standard SAC algorithm. TaCoS adaptively picks the number of interactions and therefore during an episode of time \(T\), it effectively collects less data than the standard discrete-time RL algorithm.4 This makes comparison to the discrete-time setting challenging since environment interactions and physical time on the environment are not linearly related for TaCoS as opposed to the standard discrete-time setting. Nevertheless, to be fair to the discrete-time method, we give SAC more physical time on the system for all environments, effectively resulting in the collection of more data for learning. Since the standard SAC algorithm updates the policy relative to the data amount, we consider a version of SAC, SAC-MC (SAC more compute), which leverages the additional data it collects to perform more gradient updates. This version essentially performs more policy updates than SAC-TaCoS and thus is computationally more expensive. Furthermore, to demonstrate the generality of our framework, we also combine TaCoS with PPO (PPO-TaCoS).

Footnote 4: A standard RL algorithm would collect \(\nicefrac{{T}}{{t_{\min}}}\) data points per episode.

We report the performance after convergence across different \(t_{\min}\) in the first row of Figure 4. From our experiment, we conclude that SAC-TaCoS and PPO-TaCoS are robust to the choice of \(t_{\min}\)

Figure 4: We compare the performance of TaCoS in combination with SAC and PPO with the standard SAC algorithm and SAC with more compute (SAC-MC) over a range of values for \(t_{\min}\) (first row). In the second row, we plot the episode reward versus the physical time in seconds spent in the environment for SAC-TaCoS, SAC, and SAC-MC for a specific evaluation frequency \(\nicefrac{{1}}{{t_{\text{eval}}}}\). We exclude PPO-TaCoS in this plot as it, being on-policy, requires significantly more samples than the off-policy methods. While all methods perform equally well for standard discretization (denoted with \(1/t^{*}\)), our method is robust to interaction frequency and does not suffer a performance drop when we decrease \(t_{min}\).

and perform equally well when \(t_{\min}\) is decreased, i.e., frequency is increased. This is in contrast to the standard RL methods, which have a significant drop in performance at high frequencies. This observation is also made in prior work (Hafner et al., 2019). Crucially, this highlights the sensitivity of the standard RL methods to the frequency of interaction. In the second row of Figure 4 we show the learning curve of the methods for a specific frequency \(\nicefrac{{1}}{{t_{\text{eval}}}}\). From the curve, we conclude that SAC-TaCoS achieves higher rewards with significantly less physical time on the environment. We believe this is because our method explores more efficiently (akin to Dabney et al., 2020; Eberhard et al., 2022), and also learns a much stronger/continuous-time representation of the underlying MDP.

Interestingly, at the default frequency used in the benchmarks \(\nicefrac{{1}}{{t^{*}}}\), all methods perform similarly. However, slightly decreasing the frequency already leads to a drastic drop in performance for all methods. Intuitively, decreasing the frequency prevents us from performing the necessary fine-grained control and obtaining the highest performance.

While we have access to the optimal frequency \(\nicefrac{{1}}{{t^{*}}}\) for these benchmarks, for a general and unknown system it is very difficult to estimate this frequency. Furthermore, as we observe in our experiments, picking a very high frequency is also not an option when using standard RL algorithms. We believe this is where TaCoS excels as it adaptively picks the frequency of interaction, thereby relieving the problem designer of this decision.

## 5 Efficient Exploration for TaCoS via Model-Based RL

In this section, we propose a novel model-based RL algorithm for TaCoS called **O**ptimistic **TaCoS** (OTaCoS). We analyze the episodic setting, where we interact with the system in episodes \(n=1,\ldots,N\). In episode \(n\), we execute the policy \(\bm{\pi}_{n}\), collect measurements and integrated rewards \((\bm{x}_{n,0},b_{n,0}),\ldots,(\bm{x}_{n,k_{n}},b_{n,k_{n}})\), and prepare the data \(\mathcal{D}_{n}=\{(\bm{x}_{n,1},\bm{y}_{n,1}),\ldots,(\bm{z}_{n,k_{n}},\bm{y}_ {n,k_{n}})\}\), where \(\bm{z}_{n,i}=(\bm{x}_{n,i-1},\bm{u}_{n,i-1},t_{n,i-1})\) and \(\bm{y}_{n,i}=(\bm{x}_{n,i},b_{n,i})\). From the dataset \(\mathcal{D}_{1:n}\stackrel{{\text{def}}}{{=}}\cup_{i\leq n} \mathcal{D}_{i}\) we build a model \(\mathcal{M}_{n}\) for the unknown function \(\bm{\Phi}^{*}\) such that it is well-calibrated in the sense of the following definition.

**Definition 1** (Well-calibrated statistical model of \(\bm{\Phi}^{*}\), Rothfuss et al. (2023)).: _Let \(\mathcal{Z}\stackrel{{\text{def}}}{{=}}\mathcal{X}\times\mathcal{ U}\times\mathcal{T}\). We assume \(\bm{\Phi}^{*}\in\bigcap_{n\geq 0}\mathcal{M}_{n}\) with probability at least \(1-\delta\), where statistical model \(\mathcal{M}_{n}\) is defined as_

\[\mathcal{M}_{n}\stackrel{{\text{def}}}{{=}}\left\{\bm{f}: \mathcal{Z}\rightarrow\mathbb{R}^{d_{x}+1}\mid\forall\bm{z}\in\mathcal{Z}, \forall j\in\{1,\ldots,d_{x}+1\}:|\mu_{n,j}(\bm{z})-f_{j}(\bm{z})|\leq\beta_{n} (\delta)\sigma_{n,j}(\bm{z})\right\},\]

_Here, \(\mu_{n,j}\) and \(\sigma_{n,j}\) denote the \(j\)-th element in the vector-valued mean and standard deviation functions \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) respectively, and \(\beta_{n}(\delta)\in\mathbb{R}_{\geq 0}\) is a scalar function that depends on the confidence level \(\delta\in(0,1]\) and which is monotonically increasing in \(n\)._

Similar to model-based RL algorithms for the discrete-time setting (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024), we follow the principle of optimism in the face of uncertainty and select the policy \(\bm{\pi}_{n}\) for both settings of TaCoS (cf. Sections 3.1 and 3.2) by solving:

\[\bm{\pi}_{n}\stackrel{{\text{def}}}{{=}}\operatorname*{argmax}_{ \bm{\pi}\in\Pi_{\Box}}\max_{\bm{\Phi}\in\mathcal{M}_{n-1}}V_{\bm{\pi},\bm{ \Phi}}(\bm{x}_{0},T),\] (8)

where \(\Box\in\{IC,BI\}\) is the appropriate policy class from Section 3. Running OTaCoS for \(N\) episodes, we measure the performance via the _regret_:

\[R_{N}=\sum_{n=1}^{N}\bigl{(}V_{\bm{\pi}^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{\bm {\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)\bigr{)}.\]

Here \(\bm{\pi}^{*}\) is the optimal policy from the class of policies we optimize over. Any kind of regret bound requires certain assumptions on the regularity of the underlying dynamics (1).

**Assumption 1** (Dynamics model).: _Given any norm \(\|\cdot\|\), we assume that the drift \(\bm{f}^{*}\), and diffusion \(\bm{g}^{*}\) are \(L_{\bm{f}^{*}}\) and \(L_{\bm{g}^{*}}\)-Lipschitz continuous, respectively, with respect to the induced metric. We further assume \(\sup_{\bm{z}\in\mathcal{Z}}\|\bm{g}^{*}(\bm{z})\|_{F}\leq A\)._

Assumption 1 ensures the existence of the SDE (1) solution under policy \(\bm{\pi}_{n}\). To provide bounds on the performance of OTaCoS for settings Sections 3.1 and 3.2 we also need some assumptions on the noise and reward model.

**Assumption 2** (Reward and noise model for Section 3.1 Setting).: _Given any norm \(\left\lVert\cdot\right\rVert\), we assume that running reward \(b\) is \(L_{b}\)-Lipschitz continuous, with respect to the induced metric. We further assume boundedness of the reward \(0\leq b^{*}(\bm{x},\bm{u})\leq B\), and interaction cost \(0\leq c(\bm{x},\bm{u})\leq C\). The dynamics noise is independent and follows: \(\bm{w}_{k}^{2}\sim\mathcal{N}\left(0,\sigma^{2}(\bm{x}_{k},\bm{u}_{k},t_{k})I_{ d_{x}}\right)\)._

**Assumption 3** (Reward and noise model for Section 3.2 Setting).: _Given any norm \(\left\lVert\cdot\right\rVert\), we assume that the running reward \(b\) is \(L_{b}\)-Lipschitz continuous, w.r.t. to the induced metric._

Finally, we assume that we learn a well-calibrated model of the unknown flow \(\bm{\Phi}^{*}\).

**Assumption 4** (Well calibration assumption).: _Our learned model is an all-time-calibrated statistical model of \(\bm{\Phi}^{*}\), i.e., there exists an increasing sequence of \(\left(\beta_{n}(\delta)\right)_{n\geq 0}\) such that our model satisfies the well-calibration condition, cf., Definition 1._

Analogous assumptions are made for model-based RL algorithms in the discrete-time setting (Curi et al., 2020; Sukhija et al., 2024). This calibration assumption is satisfied if \(\bm{\Phi}^{*}\) can be represented with Gaussian Process (GP) (Williams and Rasmussen, 2006; Kirschner and Krause, 2018) models.

**Theorem 2**.: _Consider the setting from Section 3.1 and let Assumption 1, 2, and Assumption 4 hold. Then we have with probability at least \(1-\delta\):_

\[R_{N}\leq\mathcal{O}\left(\beta_{N-1}T^{3/2}\sqrt{N\mathcal{I}_{N}}\right)\]

_Now consider, the setting with a bounded number of switches \(K\), and let Assumption 1, 3, and Assumption 4 hold. Then, we get with probability at least \(1-\delta\)_

\[R_{N}\leq\mathcal{O}\left(\beta_{N-1}^{K}Ke^{D(L_{\bm{f}^{*}}+L_{\bm{g}^{*}}^{ 2})(1+L_{\bm{\pi}})TK}\sqrt{N\mathcal{I}_{N}}\right),\]

_where \(D\) is a constant. Here, with \(\mathcal{I}_{N}\) we denote the model-complexity after observing \(N\) points (Curi et al., 2020), which quantifies the difficulty of learning \(\bm{\Phi}^{*}\). For GPs, it behaves similar to the maximum information gain \(\gamma_{N}\)(Srinivas et al., 2009), i.e., implying sublinear regret for several common kernels (Vakili et al., 2021)._

As a proof of concept, we evaluate OTaCoS on the pendulum and RC car environment for the interaction cost setting. 5 As baselines, we adapt common model-based RL methods such as PETS (Chua et al., 2018) and planning with the mean to TaCoS. We call them PETS-TaCoS and Mean-TaCoS, respectively. The result is reported in Figure 5. From the figure, we conclude that OTaCoS is more sample efficient than other model-based baselines and SAC-TaCoS (SAC-TaCoS requires circa \(6000\) episodes for the pendulum and \(2000\) for the RC car).

Footnote 5: The code is available at https://github.com/lasgroup/model-based-rl.

## 6 Related Work

Similar to this work, Holt et al. (2023); Ni and Jang (2022); Karimi (2023) consider continuous-time deterministic dynamical systems where the measurements or control input changes can only

Figure 5: We run OTaCoS on the pendulum and RC car environment. We report the achieved reward averaged over five different seeds with one standard error.

happen at discrete time steps. Moreover, Holt et al. (2023) proposes a similar problem as ours from Section 3.1, where they specify a cost on the number of interactions. However, their solution is based on a heuristic, where a measurement is taken when the variance of the potential reward surpasses a prespecified threshold. On the contrary, we directly tackle this problem at hand and propose a general framework for time-adaptive control that does not rely on any heuristics. Karimi (2023) adapt SAC (Haarnoja et al., 2018) to include a regularization term, which effectively adds a cost for every discrete interaction. Ni and Jang (2022) induce a soft-constraint on the duration \(\tau\) of each action in the environment. However, all the aforementioned works propose heuristic techniques to minimize interactions, whereas we formalize the problem systematically for the more general case of SDEs and show that it has an underlying MDP structure that any RL algorithm can leverage. In addition, we propose a no-regret model-based RL algorithm for this setting and analyze its sample complexity.

Temporal abstractions are considered also in the framework of options (Sutton et al., 1999; Mankowitz et al., 2014; Mann and Mannor, 2014; Harb et al., 2018). However, a key difference to TaCoS is that in the options framework, the agent measures the state even between the controller switches.

Learning to repeat actionsSeveral works observe that repeating actions in the discrete-time MDPs problems such as Atari (Mnih et al., 2013; Braylan et al., 2015) or Cartpole (Hafner et al., 2019) significantly increase the speed of learning. However, the action repeat is fixed through the entire rollout and treated as a hyperparameter. Durugkar et al. (2016); Vezhnevets et al. (2016); Srinivas et al. (2017); Sharma et al. (2017); Lee et al. (2020); Grigsby et al. (2021); Chen et al. (2021); Nam et al. (2021); Yu et al. (2021); Biedenkapp et al. (2021); Krale et al. (2023) automate the selection of action repeat, and show superior performance over the fixed number setting. Dabney et al. (2020) empirically show that repeating the actions helps with the exploration, effectively having a similar effect that colored noise exploration has over the standard white noise exploration (Eberhard et al., 2022).

Continuous-time RLFollowing the seminal work of Doya (2000) and the advances in Neural ODEs of Chen et al. (2018), continuous-time RL has regained interest (Cranmer et al., 2020; Greydanus et al., 2019; Yildiz et al., 2021; Lutter et al., 2021). Moreover, modeling in continuous-time is found to be particularly useful when learning from different data sources where each source is collected at a different frequency (Burns et al., 2023; Zheng et al., 2023). An important line of work exists for modeling continuous dynamics for the case when states and actions are discrete, called Markov Jump Processes (Kallianpur and Sundar, 2014; Berger, 1993; Huang et al., 2019; Seifner and Sanchez, 2023). Another line of work that is close to ours is event and self-Triggered Control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012, 2021), where they model continuous-time control systems by implementing changes to the input only when stability is at risk, ensuring efficient and timely interventions. Treven et al. (2023) propose a no-regret continuous-time model-based RL algorithm, which akin to OTAcoS, performs optimistic exploration. They study the problem where controls can be executed continuously in time and propose adaptive measurement selection strategies. Similarly, we propose a novel model-based RL algorithm, OTAcOS, based on the principle of optimism in the face of uncertainty. We show that OTAcoS has no regret for sufficiently smooth dynamics and has considerable sample-efficiency gains over its model-free counterpart.

## 7 Conclusion and discussion

We study the problem of time-adaptive RL for continuous-time systems with continuous state and action spaces. We investigate two practical settings where each interaction has an inherent cost and where we have a hard constraint on the number of interactions. We propose a novel RL framework, TACoS, and show that both of these settings result in extended MDPs which can be solved with standard RL algorithms. In our experiments, we show that combining standard RL algorithms with TaCoS results in a significant reduction in the number of interactions without having any effect on the performance for the interaction cost setting. Furthermore, for the second setting, TaCoS achieves considerably better control performance despite having a small budget for the number of interactions. Moreover, we show that TaCoS improves robustness to a large range of interaction frequencies, and generally improves sample complexity of learning. Finally, we propose, OTAcoS, a no-regret model-based RL algorithm for TaCoS and show that it has further sample efficiency gains.

## Acknowledgments and Disclosure of Funding

This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, the Microsoft Swiss Joint Research Center, grant of the Hasler foundation (grant no. 21039) and the SNSF Postdoc Mobility Fellowship 211086.

## References

* A. Anta and P. Tabuada (2010)To sample or not to sample: Self-triggered control for nonlinear systems. IEEE Transactions on automatic control55 (9), pp. 2030-2042. Cited by: SS1.
* K. J. Astrom and B. M. Bernhardsson (2002)Comparison of riemann and lebesgue sampling for first order stochastic systems. In Proceedings of the 41st IEEE Conference on Decision and Control, 2002., Vol. 2, pp. 2011-2016. Cited by: SS1.
* M. A. Berger (1993)Markov jump processes. Vol., Springer New York, New York, NY. Cited by: SS1.
* A. Biedenkapp, R. Rajan, F. Hutter, and M. Lindauer (2021)Temporal: learning when to act. In International Conference on Machine Learning, pp. 914-924. Cited by: SS1.
* S. G. Bobkov and F. Gotze (1999)Exponential integrability and transportation cost related to logarithmic sobolev inequalities. Journal of Functional Analysis163 (1), pp. 1-28. Cited by: SS1.
* A. Braylan, M. Hollenbeck, E. Meyerson, and R. Miikkulainen (2015)Frame skip is a powerful parameter for learning to play atari. In Workshops at the twenty-ninth AAAI conference on artificial intelligence, Cited by: SS1.
* K. Burns, T. Yu, C. Finn, and K. Hausman (2023)Offline reinforcement learning at multiple frequencies. In Conference on Robot Learning, pp. 2041-2051. Cited by: SS1.
* C. Chen, H. Tang, J. Hao, W. Liu, and Z. Meng (2021)Addressing action oscillations through learning policy inertia. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, pp. 7020-7027. Cited by: SS1.
* R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud (2018)Neural ordinary differential equations. Advances in neural information processing systems31, pp.. Cited by: SS1.
* K. Chua, R. Calandra, R. McAllister, and S. Levine (2018)Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In NeurIPS, Cited by: SS1.
* M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho (2020)Lagrangian neural networks. arXiv preprint arXiv:2003.04630. Cited by: SS1.
* S. Curi, F. Berkenkamp, and A. Krause (2020)Efficient model-based reinforcement learning through optimistic policy search and planning. Advances in Neural Information Processing Systems33, pp. 14156-14170. Cited by: SS1.
* W. Dabney, G. Ostrovski, and A. Barreto (2020)Temporally-extended \(\{\backslash\text{epsilon}\}\)-greedy exploration. arXiv preprint arXiv:2006.01782. Cited by: SS1.
* H. Djellout, A. Guillin, and L. Wu (2004)Transportation cost-information inequalities and applications to random dynamical systems and diffusions. The Annals of Probability32 (3), pp. 2702-2732. Cited by: SS1.
* K. Doya (2000)Reinforcement learning in continuous time and space. Neural computation12 (1), pp. 219-245. Cited by: SS1.
* I. P. Durugkar, C. Rosenbaum, S. Dernbach, and S. Mahadevan (2016)Deep reinforcement learning with macro-actions. arXiv preprint arXiv:1606.04615. Cited by: SS1.
* O. Eberhard, J. Hollenstein, C. Pinneri, and G. Martius (2022)Pink noise is all you need: colored noise exploration in deep reinforcement learning. In The Eleventh International Conference on Learning Representations, Cited by: SS1.

Enguist, B., Li, X., Ren, W., Vanden-Eijnden, E., et al. (2007). Heterogeneous multiscale methods: a review. _Communications in Computational Physics_, 2(3):367-450.
* a differentiable physics engine for large scale rigid body simulation.
* Greydanus et al. (2019) Greydanus, S., Dzamba, M., and Yosinski, J. (2019). Hamiltonian neural networks. _Advances in neural information processing systems_, 32.
* Grigsby et al. (2021) Grigsby, J., Yoo, J. Y., and Qi, Y. (2021). Towards automatic actor-critic solutions to continuous control. _arXiv preprint arXiv:2106.08918_.
* Haarnoja et al. (2018) Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR.
* Hafner et al. (2019) Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019). Dream to control: Learning behaviors by latent imagination. _arXiv preprint arXiv:1912.01603_.
* Harb et al. (2018) Harb, J., Bacon, P.-L., Klassarov, M., and Precup, D. (2018). When waiting is not an option: Learning options with a deliberation cost. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32.
* Heemels et al. (2021) Heemels, W., Johansson, K. H., and Tabuada, P. (2021). Event-triggered and self-triggered control. In _Encyclopedia of Systems and Control_, pages 724-730. Springer.
* Heemels et al. (2012) Heemels, W. P., Johansson, K. H., and Tabuada, P. (2012). An introduction to event-triggered and self-triggered control. In _2012 ieee 51st ieee conference on decision and control (cdc)_, pages 3270-3285. IEEE.
* Holt et al. (2023) Holt, S., Huyuk, A., and van der Schaar, M. (2023). Active observing in continuous-time control. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Huang et al. (2019) Huang, Y., Kavitha, V., and Zhu, Q. (2019). Continuous-time markov decision processes with controlled observations. In _2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 32-39. IEEE.
* Jones et al. (2009) Jones, D. S., Plank, M., and Sleeman, B. D. (2009). _Differential equations and mathematical biology_. CRC press.
* Kaandorp and Koole (2007) Kaandorp, G. C. and Koole, G. (2007). Optimal outpatient appointment scheduling. _Health care management science_, 10:217-229.
* Kabzan et al. (2020) Kabzan, J., Valls, M. I., Reijgwart, V. J., Hendrikx, H. F., Ehmke, C., Prajapat, M., Buhler, A., Gosala, N., Gupta, M., Sivanesan, R., et al. (2020). Amz driverless: The full autonomous racing system. _Journal of Field Robotics_, 37(7):1267-1294.
* Kakade et al. (2020) Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. (2020). Information theoretic regret bounds for online nonlinear control. _NeurIPS_, 33:15312-15325.
* Kallianpur and Sundar (2014) Kallianpur, G. and Sundar, P. (2014). 266Jump Markov Processes. In _Stochastic Analysis and Diffusion Processes_. Oxford University Press.
* Karimi (2023) Karimi, A. (2023). Decision frequency adaptation in reinforcement learning using continuous options with open-loop policies.
* Kirschner and Krause (2018) Kirschner, J. and Krause, A. (2018). Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_, pages 358-384. PMLR.
* Krale et al. (2023) Krale, M., Simao, T. D., and Jansen, N. (2023). Act-then-measure: reinforcement learning for partially observable environments with active measuring. In _Proceedings of the International Conference on Automated Planning and Scheduling_, volume 33, pages 212-220.
* Lee et al. (2020) Lee, J., Lee, B.-J., and Kim, K.-E. (2020). Reinforcement learning for control with multiple frequencies. _Advances in Neural Information Processing Systems_, 33:3254-3264.
* Lee et al. (2020)* Lenhart and Workman (2007) Lenhart, S. and Workman, J. T. (2007). _Optimal control applied to biological models_. CRC press.
* Lillicrap et al. (2015) Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_.
* Lutter et al. (2021) Lutter, M., Mannor, S., Peters, J., Fox, D., and Garg, A. (2021). Value iteration in continuous actions, states and time. _arXiv preprint arXiv:2105.04682_.
* Mankowitz et al. (2014) Mankowitz, D. J., Mann, T. A., and Mannor, S. (2014). Time regularized interrupting options. In _Internation Conference on Machine Learning_.
* Mann and Mannor (2014) Mann, T. and Mannor, S. (2014). Scaling up approximate value iteration with options: Better policies with fewer iterations. In _International conference on machine learning_, pages 127-135. PMLR.
* Mnih et al. (2013) Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_.
* Nam et al. (2021) Nam, H. A., Fleming, S., and Brunskill, E. (2021). Reinforcement learning with state observation costs in action-contingent noiselessly observable markov decision processes. _Advances in Neural Information Processing Systems_, 34:15650-15666.
* Ni and Jang (2022) Ni, T. and Jang, E. (2022). Continuous control on time. In _ICLR 2022 Workshop on Generalizable Policy Learning in Physical World_.
* Panetta and Fister (2003) Panetta, J. C. and Fister, K. R. (2003). Optimal control applied to competing chemotherapeutic cell-kill strategies. _SIAM Journal on Applied Mathematics_, 63(6):1954-1971.
* Rothfuss et al. (2023) Rothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. (2023). Hallucinated adversarial control for conservative offline policy evaluation. In _Uncertainty in Artificial Intelligence_, pages 1774-1784. PMLR.
* Schulman et al. (2015) Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR.
* Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.
* Seifner and Sanchez (2023) Seifner, P. and Sanchez, R. J. (2023). Neural markov jump processes. _arXiv preprint arXiv:2305.19744_.
* Sharma et al. (2017) Sharma, S., Srinivas, A., and Ravindran, B. (2017). Learning to repeat: Fine grained action repetition for deep reinforcement learning. _arXiv preprint arXiv:1702.06054_.
* Spong et al. (2006) Spong, M. W., Hutchinson, S., Vidyasagar, M., et al. (2006). _Robot modeling and control_, volume 3. Wiley New York.
* Srinivas et al. (2017) Srinivas, A., Sharma, S., and Ravindran, B. (2017). Dynamic action repetition for deep reinforcement learning. In _Proc. AAAI_.
* Srinivas et al. (2009) Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. (2009). Gaussian process optimization in the bandit setting: No regret and experimental design. _arXiv preprint arXiv:0912.3995_.
* Sukhija et al. (2024) Sukhija, B., Treven, L., Sancaktar, C., Blaes, S., Coros, S., and Krause, A. (2024). Optimistic active exploration of dynamical systems. _NeurIPS_.
* Sutton et al. (1999) Sutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Artificial intelligence_, 112(1-2):181-211.
* Tap (2000) Tap, F. (2000). _Economics-based optimal control of greenhouse tomato crop production_. Wageningen University and Research.
* Treven et al. (2023) Treven, L., Hubotter, J., Sukhija, B., Dorfler, F., and Krause, A. (2023). Efficient exploration in continuous-time model-based reinforcement learning.
* Treven et al. (2023)Turchetta, M., Corinzia, L., Sussex, S., Burton, A., Herrera, J., Athanasiadis, I., Buhmann, J. M., and Krause, A. (2022). Learning long-term crop management strategies with cyclesgym. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, _Advances in Neural Information Processing Systems_, volume 35, pages 11396-11409. Curran Associates, Inc.
* Vakili et al. (2021) Vakili, S., Khezeli, K., and Picheny, V. (2021). On information gain and regret bounds in gaussian process bandits. In _AISTATS_.
* Vezhnevets et al. (2016) Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J., and kavukcuoglu, k. (2016). Strategic attentive writer for learning macro-actions. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc.
* Williams and Rasmussen (2006) Williams, C. K. and Rasmussen, C. E. (2006). _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA.
* Yildiz et al. (2021) Yildiz, C., Heinonen, M., and Lahdesmaki, H. (2021). Continuous-time model-based reinforcement learning. In _International Conference on Machine Learning_, pages 12009-12018. PMLR.
* Yu et al. (2021) Yu, H., Xu, W., and Zhang, H. (2021). Taac: Temporally abstract actor-critic for continuous control. _Advances in Neural Information Processing Systems_, 34:29021-29033.
* Zheng et al. (2023) Zheng, Q., Henaff, M., Amos, B., and Grover, A. (2023). Semi-supervised offline reinforcement learning with action-free trajectories. In _International conference on machine learning_, pages 42339-42362. PMLR.

###### Contents of Appendix

* A Extended Theory
* A.1 Transition Cost setting
* A.2 Bounded number of transition
* B Additional Experiments
Extended Theory

In this section, we prove Theorem 2 for OTaCoS. We separate the section into two parts; proof for the transaction cost setting (Appendix A.1) and the proof for the bounded number of switches setting (Appendix A.2).

We start with the definitions of model complexity and sub-Gaussian random vector that we will use extensively in this section.

**Definition 2** (Model Complexity).: _We define the model complexity as is defined by [20],_

\[\mathcal{I}_{N}:=\max_{\mathcal{D}_{1},\dots,\mathcal{D}_{N}}\sum_{n=1}^{N} \sum_{(\bm{x},\bm{u},t)\in\mathcal{D}_{n}}\left\|\bm{\sigma}_{n}(\bm{x},\bm{u},t)\right\|_{2}^{2}.\] (9)

**Definition 3**.: _A random variable \(x\in\mathbb{R}\) is said to be sub-Gaussian with variance proxy \(\sigma^{2}\) if \(\mathbb{E}[x]=0\) and we have:_

\[\mathbb{E}[e^{tx}]\leq e^{\frac{\sigma^{2}t^{2}}{2}},\quad\forall t\in\mathbb{ R}\]

_A random vector \(\bm{x}\in\mathbb{R}^{d}\) is said to be sub Gaussian with variance proxy \(\sigma^{2}\) if for any \(\bm{e}\in\mathbb{R}^{d},\left\|\bm{e}\right\|_{2}=1\) the random variable \(\bm{x}^{\top}\bm{e}\) is \(\sigma^{2}\) sub Gaussian. We write \(\bm{x}\sim\text{subG}\left(\sigma^{2}\right)\)._

In the following, we will be distinguishing between the state of the augmented MDP \(\bm{s}\) and the true state of the dynamical system \(\bm{x}\). The augmented state at time step \(k\) includes the true state of the system, \(\bm{x}_{k}\), the integrated reward \(b_{k}\) between \(k-1\) and \(k\), and the time to left to go \(t_{k}\), i.e., \(\bm{s}_{k}=[\bm{x}_{k}^{\top},b_{k},t_{k}]^{\top}\).

### Transition Cost setting

We prove our regret bound for the transition cost case in the following. We start with the difference lemma which adapts [24, Lemma 2] to our setting.

**Lemma 3** (Difference lemma).: _Define \(V_{\bm{\pi}_{n},\bm{\Phi}}(\bm{x},\tau)\) as_

\[\mathbb{E}_{\bm{\pi},\bm{\Phi}}\left[\sum_{k\geq 0}^{K(\tau)-1}r(\bm{s}_{k}, \bm{\pi}(\bm{s}_{k}))\Big{|}\bm{x}_{0}=\bm{x}\right];\text{ where }\sum_{k=0}^{K(\tau)-1}\pi_{\mathcal{T}}(\bm{x}_{k},t_{k})=\tau\]

_that is the total reward starting with time to go \(\tau\) and state \(\bm{x}\) for the policy \(\bm{\pi}\) and dynamics \(\bm{\Phi}\). Here the expectation w.r.t. \(\bm{\pi},\bm{\Phi}\) represents the expectation of the underlying trajectory induced by the policy \(\bm{\pi}\) on the dynamics \(\bm{\Phi}\). Then we have for all \(\bm{\pi}\), \(\bm{\Phi}^{\prime}\), \(\bm{\Phi}^{*}\), \(\bm{x}_{0}\), \(T<0\);_

\[V_{\bm{\pi},\bm{\Phi}^{\prime}}(\bm{x}_{0},T)-V_{\bm{\pi},\bm{\Phi}^{*}}(\bm{x }_{0},T)=\mathbb{E}_{\bm{\pi},\bm{\Phi}^{*}}\left[\sum_{k\geq 0}V_{\bm{\pi},\bm{ \Phi}^{\prime}}(\widehat{\bm{x}}_{k+1},t_{k+1})-V_{\bm{\pi},\bm{\Phi}^{\prime}} (\bm{x}_{k+1},t_{k+1})\right],\] (10)

_where \(\widehat{\bm{x}}_{k+1}\) is the state of \(\widehat{\bm{s}}_{k+1}=\bm{\Psi}_{\bm{\Phi}^{\prime}}(\bm{s}_{k},\bm{\pi}(\bm {s}_{k}),\bm{w}_{k})\) and \(\bm{x}_{k+1}\) is the state of \(\bm{s}_{k+1}=\bm{\Psi}_{\bm{\Phi}^{*}}(\bm{s}_{k},\bm{\pi}(\bm{s}_{k}),\bm{w}_{k})\)._

Proof.: \[V_{\bm{\pi},\bm{\Phi}^{*}}(\bm{x}_{0},T) =\mathbb{E}_{\bm{\pi},\bm{\Phi}^{*}}\left[\sum_{k\geq 0}r(\bm{s}_{k}, \bm{\pi}(\bm{s}_{k}))\right]\] \[=\mathbb{E}_{\bm{\pi},\bm{\Phi}^{*}}\left[r(\bm{s}_{0},\bm{\pi}( \bm{s}_{0}))+\sum_{k\geq 1}r(\bm{s}_{k},\bm{\pi}(\bm{s}_{k}))\right]\] \[=\mathbb{E}_{\bm{\pi},\bm{\Phi}^{*}}\left[r(\bm{s}_{k},\bm{\pi}( \bm{s}_{0}))+V_{\bm{\pi},\bm{\Phi}^{*}}(\widehat{\bm{x}}_{1},t_{1})\right]\] \[=\mathbb{E}_{\bm{\pi},\bm{\Phi}^{*}}\left[V_{\bm{\pi},\bm{\Phi}}( \bm{x}_{0},T)-V_{\bm{\pi},\bm{\Phi}^{\prime}}(\widehat{\bm{x}}_{1},t_{1})+V_{ \bm{\pi},\bm{\Phi}^{*}}(\bm{x}_{1},t_{1})\right]\] \[=V_{\bm{\pi},\bm{\Phi}^{*}}(\bm{x}_{0},T)+\mathbb{E}_{\bm{\pi}, \bm{\Phi}^{*}}\left[V_{\bm{\pi},\bm{\Phi}}(\bm{x}_{1},t_{1})-V_{\bm{\pi},\bm{ \Phi}^{*}}(\widehat{\bm{x}}_{1},t_{1})\right]\]\[\|\bm{x}_{n,k+1}-\widehat{\bm{x}}_{n,k+1}\| \leq 2\sqrt{d_{\bm{x}}}\beta_{n-1}\left\|\bm{\sigma}_{n-1}(\bm{x}_ {n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))\right\|\]

Proof.: \[\|\bm{x}_{n,k+1}-\widehat{\bm{x}}_{n,k+1}\| =\|\bm{\Phi}^{*}(\bm{x}_{k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))+ \bm{w}_{n,k}-(\bm{\Phi}_{n}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k})+\bm {w}_{n,k})\|\] \[=\|\bm{\Phi}^{*}(\bm{x}_{k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))- \bm{\Phi}_{n}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k})\|\] \[\leq 2\sqrt{d_{\bm{x}}}\beta_{n-1}\left\|\bm{\sigma}_{n-1}(\bm{x} _{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))\right\|,\]

where the last inequality follows from the fact that \(\bm{\Phi}^{*},\bm{\Phi}_{n}\in\mathcal{M}_{n-1}\)Next, we relate the regret at each episode to the model epistemic uncertainty using Lemma 3 and Lemma 7.

**Corollary 8**.: _Let Assumption 1 - 2 and Assumption 4 hold, then we have for all \(n\geq 0\) with probability at least \(1-\delta\)._

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{\bm{\pi}^{*},\bm{ \Phi}^{*}}(\bm{x}_{0},T)\leq\frac{2\sqrt{d_{\bm{x}}}\beta_{n-1}T}{\sigma}\left(B +\frac{C}{t_{\min}}\right)\mathbb{E}\left[\sum_{k\geq 0}\|\bm{\sigma}_{n-1}(\bm{x}_ {n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))\|\right]\] (12)

Proof.: From Lemma 4 we have:

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{\bm{\pi}^{*},\bm{ \Phi}^{*}}(\bm{x}_{0},T)\leq\mathbb{E}\left[\sum_{k\geq 0}V_{\bm{\pi}_{n},\bm{ \Phi}_{n}}(\bm{x}_{n,k+1},t_{n,k+1})-V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\widehat{ \bm{x}}_{n,k+1},t_{n,k+1})\right].\]

Lemma 6 can be applied to positive function \(g\). We hence make a transformation and apply it to \(g(\cdot)=V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\cdot,t_{n,k+1})+\frac{C}{t_{\min}}T\), which is positive due to Lemma 5. Moreover, \(\forall\bm{x}\in\mathcal{X}\);

\[g(\cdot)=V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\cdot,t_{n,k+1})+\frac{C}{t_{\min}}T \leq Bt_{n,k+1}+\frac{C}{t_{\min}}T\leq T(B+\frac{C}{t_{\min}}).\]

Applying Lemma 6 we obtain:

\[V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,k+1},t_{n,k+1})-V_{\bm{ \pi}_{n},\bm{\Phi}_{n}}(\widehat{\bm{x}}_{n,k+1},t_{n,k+1})\leq\frac{T}{\sigma} \left(B+\frac{C}{t_{\min}}\right)\mathbb{E}\left[\|\bm{x}_{n,k+1}-\widehat{ \bm{x}}_{n,k+1}\|\right]\]

Finally, applying Lemma 7 we arrive at:

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{\bm{\pi}^{*}, \bm{\Phi}^{*}}(\bm{x}_{0},T)\leq\frac{2\sqrt{d_{\bm{x}}}\beta_{n-1}T}{\sigma} \left(B+\frac{C}{t_{\min}}\right)\mathbb{E}\left[\sum_{k\geq 0}\|\bm{ \sigma}_{n-1}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))\|\right]\]

Now we can prove our regret bound for the transition cost case.

**Theorem 9**.: _Let Assumption 1 - 2 and Assumption 4 hold, then we have for all \(n\geq 0\) with probability at least \(1-\delta\)._

\[R_{N} =\sum_{n=1}^{N}V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{ \bm{\pi}^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T)\] \[\leq\frac{2\sqrt{d_{\bm{x}}}\beta_{N-1}T^{3/2}}{\sigma^{2}t_{\min }}\left(B+\frac{C}{t_{\min}}\right)\sqrt{N\mathcal{I}_{N}}\]

Proof.: We compute:

\[R_{N} =\sum_{n=1}^{N}V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T)-V_{ \bm{\pi}^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T)\] \[\leq\frac{2\sqrt{d_{\bm{x}}}T}{\sigma}\left(B+\frac{C}{t_{\min}} \right)\sum_{n=1}^{N}\beta_{n-1}\mathbb{E}\left[\sum_{k\geq 0}\|\bm{\sigma}_{n-1}( \bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k}))\|\right]\] \[\leq\frac{2\sqrt{d_{\bm{x}}}\beta_{N-1}T}{\sigma}\left(B+\frac{C} {t_{\min}}\right)\sqrt{\frac{TN}{t_{\min}}}\mathbb{E}\left[\sqrt{\sum_{n=1}^{N }\sum_{k\geq 0}\|\bm{\sigma}_{n-1}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k} ))\|^{2}}\right]\]\[\leq\frac{2\sqrt{d_{\pi}}\beta_{N-1}T^{3/2}}{\sigma\sqrt{t_{\min}}} \left(B+\frac{C}{t_{\min}}\right)\sqrt{N\mathcal{I}_{N}}\]

Here the first inequality follows because of Corollary 8, the second inequality follows due to the monotonicity of sequence \((\beta_{n})_{n\geq 0}\), the third inequality follows by Cauchy-Schwarz and the last one by maximizing the term in expectation. 

Our regret \(R_{N}\) is sublinear if \(\beta_{N-1}\sqrt{N\mathcal{I}_{N}}\) is sublinear. For general well-calibrated models this is tough to verify. However, for Gaussian process dynamics, \(\mathcal{I}_{N}\) is equal to (up to constant factors) the maximum information gain \(\gamma_{N}\)(Srinivas et al., 2009)(c.f., Curi et al. (2020, Lemma 17)). The maximum information gain is sublinear for a rich class of kernels (Vakili et al., 2021), i.e., yielding sublinear regret for OTaCoS (see Sukhija et al. (2024, Theorem 2) for more detail).

### Bounded number of transition

We overload the notation in this section and add number of switches to the value function, such that we have \(V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,0)=V_{\bm{\pi}_{n},\bm{\Phi}^{*} }(\bm{x}_{0},T)\)

**Lemma 10** (Per episode regret bound).: _We have:_

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,0) -V_{\bm{\pi}^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T,0)\leq\] \[\leq\mathbb{E}\left[\sum_{k=0}^{K-1}V_{\bm{\pi}_{n},\bm{\Phi}_{n} }(\bm{x}_{n,k+1},t_{n,k+1},k+1)-V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\widehat{\bm{x} }_{n,k+1},t_{n,k+1},k+1)\right],\]

_where \(\widehat{\bm{x}}_{n,k+1}\) is the state of one step hallucinated component \(\widehat{\bm{s}}_{n,k+1}=\bm{\Psi}_{\bm{\Phi}_{n}}(\bm{s}_{n,k},\bm{\pi}_{n}( \bm{s}_{n,k}),\bm{w}_{n,k})\) and \(\bm{x}_{n,k+1}\) is the state of \(\bm{s}_{n,k+1}=\bm{\Psi}_{\bm{\Phi}^{*}}(\bm{s}_{n,k},\bm{\pi}_{n}(\bm{s}_{n,k }),\bm{w}_{n,k})\)._

Proof.: \[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,0) =\mathbb{E}\left[\sum_{k\geq 0}r(\bm{s}_{n,k},\bm{\pi}_{n}( \bm{s}_{n,k}))\right]=\mathbb{E}\left[r(\bm{s}_{n,0},\bm{\pi}_{n}(\bm{s}_{n,0} ))+\sum_{k\geq 1}r(\bm{s}_{n,k},\bm{\pi}_{n}(\bm{s}_{n,k}))\right]\] \[=\mathbb{E}\left[r(\bm{s}_{n,k},\bm{\pi}_{n}(\bm{s}_{n,0}))+V_{ \bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{n,1},t_{n,1},1)\right]\] \[=\mathbb{E}\left[r(\bm{s}_{n,k},\bm{\pi}_{n}(\bm{s}_{n,0}))+V_{ \bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)-V_{\bm{\pi}_{n},\bm{\Phi} _{n}}(\bm{x}_{0},T,0)\right]+\] \[+\mathbb{E}\left[V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{0},T,0)-V _{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)+V_{\bm{\pi}_{n},\bm{\Phi }^{*}}(\bm{x}_{n,1},t_{n,1},1)\right]\] \[=V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{0},T,0)+\mathbb{E}\left[V _{\bm{\pi}_{n},\bm{\Phi}_{n}}(\widehat{\bm{x}}_{n,1},t_{n,1},1)-V_{\bm{\pi}_{n}, \bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)\right]\] \[+\mathbb{E}\left[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{n,1},t_{ n,1},1)-V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)\right]\]

Hence we have:

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,0)-V_{\bm{\pi}_{n}, \bm{\Phi}_{n}}(\bm{x}_{0},T,0)=\] \[=\mathbb{E}\left[V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\widehat{\bm{x}} _{n,1},t_{n,1},1)-V_{\bm{\pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)\right] +\mathbb{E}\left[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{n,1},t_{n,1},1)-V_{\bm{ \pi}_{n},\bm{\Phi}_{n}}(\bm{x}_{n,1},t_{n,1},1)\right]\]

Repeating the step inductively the result follows and using \(V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{n,K},t_{n,K},K)=0\) we prove the lemma. 

#### a.2.1 Subgaussianity of the noise

In principle, we could assume that the noise \(\bm{w}_{k}\) is Gaussian and then with the same analysis obtain the regret bound. However, stochastic flows are in many cases not exactly Gaussian but only sub-Gaussian. For such noise we need can not apply Lemma 6 and need to escort to different analysis. First we show that under mild assumptions on the SDE dynamics functions \(\bm{f}^{*}\) and \(\bm{g}^{*}\) the resulting noise \(\bm{w}_{k}\) is sub-Gaussian.

To derive this result we will follow the work of Djellout et al. (2004). We present the results in quite informal way, for more rigorous statements we refer the reader to Djellout et al. (2004).

**Definition 4** (Wasserstein distance).: _Let \((\mathcal{E},d_{\mathcal{E}})\) be a metric space and let \(\mu,\nu\) be two probability measures on \(\mathcal{E}\). We define:_

\[W_{p}(\mu,\nu)=\inf_{\gamma\in\Gamma(\mu,\nu)}\mathbb{E}_{(x,y)\sim\gamma}\left[d (x,y)^{p}\right]^{\frac{1}{p}}\]

**Definition 5** (Kullback-Leibler divergence).: _Let \((\mathcal{E},d_{\mathcal{E}})\) be a metric space and let \(\mu,\nu\) be two probability measures on \(\mathcal{E}\). We define:_

\[H(\nu||\mu)=\begin{cases}\mathbb{E}_{x\sim\nu}\left[\log\left(\frac{d\nu(x)}{d \mu(x)}\right)\right],&\text{if }\nu\ll\mu\\ +\infty,&\text{else}\end{cases}\]

**Definition 6** (\(L^{p}\)-transportation cost information inequality).: _Let \((\mathcal{E},d_{\mathcal{E}})\) be a metric space and let \(\mu\) be a probability measure on \(\mathcal{E}\). We say that \(\mu\) satisfy the \(L^{p}\)-transportation cost information inequality, and for short write \(\mu\in T_{p}(C)\), if there exists a constant \(C\) such that for any measure \(\nu\) on \(\mathcal{E}\) we have:_

\[W_{p}(\mu,\nu)\leq\sqrt{2CH(\nu||\mu)}.\]

We now state an important theroem of Bobkov and Gotze (1999) that we will use later.

**Theorem 11** (From Bobkov and Gotze (1999)).: _Let \((\mathcal{E},d_{\mathcal{E}})\) be a metric space and let \(\mu\) be a probability measure on \(\mathcal{E}\). We have that \(\mu\in T_{1}(C)\) if and only if for any \(\mu\)-integrable and \(L_{F}\)-Lipschitz function \(F:(\mathcal{E},d_{\mathcal{E}})\to\mathbb{R}\) and for any \(\lambda\in\mathbb{R}\) we have:_

\[\mathbb{E}_{x\sim\mu}\left[e^{\lambda(F(x)-\mathbb{E}_{x\sim\mu}[F(x)])}\right] \leq e^{\frac{\lambda^{2}}{2}CL_{F}^{2}}\]

Next, we provide a condition under which \(\bm{\Xi}(\bm{x},\bm{u},t)\) is sub-Gaussian random variable for any \(t\in\mathcal{T}\).

**Corollary 12** (Adjusted Corollary 4.1 of Djellout et al. (2004)).: _Assume_

\[\sup_{\begin{subarray}{c}\bm{x}\in\mathbb{R}^{d_{x}}\\ \bm{u}\in\mathbb{R}^{d_{u}}\end{subarray}}\left\|\bm{g}^{*}(\bm{x},\bm{u}) \right\|_{F}\leq A,\quad\left\|\bm{f}^{*}(\bm{x},\bm{u})-\bm{f}^{*}(\widehat{ \bm{x}},\widehat{\bm{u}})\right\|\leq L_{\bm{f}^{*}}\left\|(\bm{x},\bm{u})-( \widehat{\bm{x}},\widehat{\bm{u}})\right\|,\]

_and denote the law of \((\bm{\Xi}(\bm{x},\bm{u},t))_{t\in\mathcal{T}}\) on the space \(C(\mathcal{T},\mathbb{R}^{d_{\bm{x}}})\) (space of continuous functions from \(\mathcal{T}\) to \(\mathbb{R}^{d_{\bm{x}}}\)) by \(\mathbb{P}_{\bm{x}}\). Then, there exist a constant \(C=C(A,L_{\bm{f}^{*}},T)\) such that \(\mathbb{P}_{\bm{x}}\in T_{1}(C)\) on the space \(C(\mathcal{T},\mathbb{R}^{d_{\bm{x}}})\) equipped with the metric:_

\[d(\gamma_{1},\gamma_{2})=\sup_{t\in[0,T]}\left\|\gamma_{1}(t)-\gamma_{2}(t)\right\|\]

Lets \(\bm{e}\) be a(ny) unit vector in \(\mathbb{R}^{d_{x}}\) and define:

\[F_{\bm{e},t}:C(\mathcal{T},\mathbb{R}^{d_{\bm{x}}})\to \mathbb{R}\] \[F_{\bm{e},t}:\gamma\mapsto\gamma(t)^{\top}\bm{e}\]

We have:

\[\left|F_{\bm{e},t}(\gamma_{1})-F_{\bm{e},t}(\gamma_{2})\right| =\left|(\gamma_{1}(t)-\gamma_{2}(t))^{\top}\bm{e}\right|\] \[\leq\left\|\gamma_{1}(t)-\gamma_{2}(t)\right\|\left\|e\right\|= \left\|\gamma_{1}(t)-\gamma_{2}(t)\right\|\] \[\leq\sup_{t\in\mathcal{T}}\left\|\gamma_{1}(t)-\gamma_{2}(t)\right\| =d(\gamma_{1},\gamma_{2})\]

Therefore for any \(\bm{e},t\) the function \(F_{\bm{e},t}\) is \(1\)-Lipschitz. Since we have

\[\mathbb{E}[\left|F_{\bm{e},t}(\gamma)\right|]=\int_{C(\mathcal{T},\mathbb{R}^ {d_{\bm{x}}})}\left|\gamma(t)\right|d\mathbb{P}_{\bm{x}}(\gamma)=\mathbb{E}[ \left|\bm{\Xi}(\bm{x},\bm{u},t)^{\top}\bm{e}\right|]<\infty\]

the function \(F_{\bm{e},t}\) is also \(\mathbb{P}_{\bm{x}}\)-integrable. Combining the latter observation with the Theorem 11 we obtain that for any \(\bm{e}\in\mathbb{R}^{d_{\bm{x}}}\) and any \(t\in\mathcal{T}\) we have:

\[\mathbb{E}_{\bm{\Xi}(\bm{x},\bm{u},t)}\left[e^{\lambda(\bm{\Xi}(\bm{x},\bm{u}, t)^{\top}\bm{e}-\mathbb{E}[\bm{\Xi}(\bm{x},\bm{u},t)^{\top}\bm{e}])}\right]= \mathbb{E}_{\gamma\sim\mathbb{P}_{\bm{x}}}\left[e^{\lambda(F_{\bm{e},t}(\gamma) -\mathbb{E}_{\gamma\sim\mathbb{P}_{\bm{x}}}[F_{\bm{e},t}(\gamma)])}\right]\leq e ^{\frac{\lambda^{2}}{2}C}\]

Hence under the assumption of Theorem 2 for Bounded number of switches setting we have that for any \(t\in\mathcal{T}\) the random variable \(\bm{\Xi}(\bm{x},\bm{u},t)-\mathbb{E}\left[\bm{\Xi}(\bm{x},\bm{u},t)\right]\sim \text{subG}\left(C\right)\). The variance proxy \(C\) depends on \(A,L_{\bm{f}^{*}},T\).

#### a.2.2 Lipschitzness of the expected flow \(\Phi^{*}\)

To apply analysis for the case when noise \(\bm{w}_{k}\) is any sub-Gaussian we also need to show that the dynamics function \(\bm{\Phi}^{*}\) is Lipschitz. We first start with some general results.

**Lemma 13**.: _Let \(\bm{f}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\), \(A\subset[n]\) and denote \(B=A^{C}\). If we have:_

* \(\left\|\bm{f}(\bm{x}_{A},\bm{x}_{B})-\bm{f}(\widehat{\bm{x}}_{A},\bm{x}_{B}) \right\|_{2}\leq L_{A}\left\|\bm{x}_{A}-\widehat{\bm{x}}_{A}\right\|_{2}\)_,_
* \(\left\|\bm{f}(\bm{x}_{A},\bm{x}_{B})-\bm{f}(\bm{x}_{A},\widehat{\bm{x}}_{B}) \right\|_{2}\leq L_{B}\left\|\bm{x}_{B}-\widehat{\bm{x}}_{B}\right\|_{2}\)_,_

_then \(\bm{f}\) is \(2(L_{A}+L_{B})\) Lipschitz._

Proof.: We have:

\[\left\|\bm{f}(\bm{x})-\bm{f}(\widehat{\bm{x}})\right\|_{2} =\left\|\bm{f}(\bm{x}_{A},\bm{x}_{B})-\bm{f}(\widehat{\bm{x}}_{A},\widehat{\bm{x}}_{B})\right\|_{2}\] \[=\left\|\bm{f}(\bm{x}_{A},\bm{x}_{B})-\bm{f}(\widehat{\bm{x}}_{A },\bm{x}_{B})+\bm{f}(\widehat{\bm{x}}_{A},\bm{x}_{B})-\bm{f}(\widehat{\bm{x}}_ {A},\widehat{\bm{x}}_{B})\right\|_{2}\] \[\leq L_{A}\left\|\bm{x}_{A}-\widehat{\bm{x}}_{A}\right\|_{2}+L_{ B}\left\|\bm{x}_{B}-\widehat{\bm{x}}_{B}\right\|_{2}\] \[\leq(L_{A}+L_{B})\left(\left\|\bm{x}_{A}-\widehat{\bm{x}}_{A} \right\|_{2}+\left\|\bm{x}_{B}-\widehat{\bm{x}}_{B}\right\|_{2})\] \[\leq 2(L_{A}+L_{B})\left\|\left(\begin{matrix}\bm{x}_{A}-\widehat{ \bm{x}}_{A}\\ \bm{x}_{B}-\widehat{\bm{x}}_{B}\end{matrix}\right)\right\|_{2}\] \[=2(L_{A}+L_{B})\left\|\bm{x}-\widehat{\bm{x}}\right\|_{2}\]

**Lemma 14** (Lipschitzness of \(\bm{\Phi_{f^{*}}}\)).: _There exists a positive constant \(L_{\bm{\Phi_{f}}}\) such that the flow \(\bm{\Phi_{f^{*}}}\) is \(L_{\bm{\Phi_{f}}}\)-Lipschitz._

Proof.: We will first prove coordinate-wise Lipschitzness. We observe:

1. Lipschitzness in time: \[\left\|\bm{\Phi_{f^{*}}}(\bm{x},\bm{u},t)-\bm{\Phi_{f^{*}}}(\bm{x },\bm{u},\widehat{t})\right\| =\left\|\int_{0}^{t}\mathbb{E}[\bm{f}^{*}(\bm{x}_{s},\bm{u})]ds- \int_{0}^{\widehat{t}}\mathbb{E}[\bm{f}^{*}(\bm{x}_{s},\bm{u})]ds\right\|\] \[\leq\int_{\widehat{t}}^{t}\mathbb{E}\left[\left\|\bm{f}^{*}(\bm{ x}_{s},\bm{u})\right\|\right]ds\leq F\left|t-\widehat{t}\right|\]
2. Lipschitness in state \(\bm{x}\): To prove this, consider the \(\delta\bm{x}_{t}=\bm{\Xi}(\bm{x},\bm{u},t)-\bm{\Xi}(\widehat{\bm{x}},\bm{u},t)\), then we have \[d\delta\bm{x}_{t} =(\bm{f}^{*}(\bm{x}_{t},\bm{u})-\bm{f}^{*}(\widehat{\bm{x}}_{t}, \bm{u}))dt+(\bm{g}^{*}(\bm{x}_{t},\bm{u})-\bm{f}^{*}(\widehat{\bm{x}}_{t}, \bm{u}))d\bm{B}_{t}\] \[=\delta\bm{f}_{t}^{*}dt+\delta\bm{g}_{t}^{*}d\bm{B}_{t}.\] Note that \(\left\|\delta\bm{f}_{t}^{*}\right\|\leq L_{\bm{f}^{*}}\left\|\delta\bm{x}_{t}\right\|\) and \(\|\delta\bm{g}_{t}^{*}\|\leq L_{\bm{g}^{*}}\left\|\delta\bm{x}_{t}\right\|\) since both functions are Lipschitz. Define \(\bm{y}_{t}=\delta\bm{x}_{t}^{\top}\delta\bm{x}_{t}\) and use Ito's Lemma to get \[d\bm{y}_{t}=2\delta\bm{x}_{t}^{\top}(\delta\bm{f}_{t}^{*}dt+\delta\bm{g}_{t}^{ *}d\bm{B}_{t})+\text{tr}(\delta\bm{g}_{t}^{*}(\delta\bm{g}_{t}^{*})^{\top})dt\] Moreover, \[\mathbb{E}[\bm{y}_{t}] =\int_{0}^{t}2\mathbb{E}[\delta\bm{x}_{s}^{\top}\delta\bm{f}_{s}^ {*}]+\mathbb{E}[\text{tr}(\delta\bm{g}_{s}^{*}(\delta\bm{g}_{s}^{*})^{\top})]ds\] \[\leq\int_{0}^{t}2\mathbb{E}\left[\|\delta\bm{x}_{s}\|\left\| \delta\bm{f}_{s}^{*}\right\|\right]+\mathbb{E}[\left\|\delta\bm{g}_{s}^{*}\right\| ^{2}]ds\] \[\leq\int_{0}^{t}(2L_{\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})\mathbb{E}[ \left\|\delta\bm{x}_{s}\right\|^{2}]ds\] Note that \(\bm{y}_{t}=\left\|\delta\bm{x}_{t}\right\|^{2}\), so we can apply Gronwall's inequality to get \[\mathbb{E}\left[\left\|\delta\bm{x}_{t}\right\|^{2}\right]\leq\left\|\delta \bm{x}_{0}\right\|^{2}e^{(2L_{\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})t}.\]Moreover, \[\|\mathbb{E}[\delta\bm{x}_{t}]\|\leq\sqrt{\mathbb{E}\left[\left\|\delta\bm{x}_{t} \right\|^{2}\right]}\leq\left\|\delta\bm{x}_{0}\right\|e^{\frac{2L_{\bm{f}^{ \star}}+L_{\bm{g}^{\star}}^{2}}{2}t}\leq\left\|\delta\bm{x}_{0}\right\|e^{\frac {2L_{\bm{f}^{\star}}+L_{\bm{g}^{\star}}^{2}}{2}T}.\] Hence we have: \[\left\|\bm{\Phi}_{\bm{f}^{\star}}(\bm{x},\bm{u},t)-\bm{\Phi}_{\bm{f}^{\star}}( \widehat{\bm{x}},\bm{u},t)\right\|\leq\left\|\bm{x}-\widehat{\bm{x}}\right\|e^ {\frac{2L_{\bm{f}^{\star}}+L_{\bm{g}^{\star}}^{2}}{2}T}.\]
3. Lipschitness in action \(\bm{u}\): We denote \(\delta\bm{x}_{t}=\bm{\Xi}(\bm{x},\bm{u},t)-\bm{\Xi}(\bm{x},\widehat{\bm{u}},t)\) and \(\delta\bm{u}=\bm{u}-\widehat{\bm{u}}\) Following the same steps as in the proof of Lipschitzness in state we arrive at: \[d\bm{y}_{t}=2\delta\bm{x}_{t}^{\top}(\delta\bm{f}_{t}^{\star}dt+\delta\bm{g}_{ t}^{\star}d\bm{B}_{t})+\text{tr}(\delta\bm{g}_{t}^{\star}(\delta\bm{g}_{t}^{ \star})^{\top})dt\] Integration yields: \[\mathbb{E}[\bm{y}_{t}] =\int_{0}^{t}2\mathbb{E}[\delta\bm{x}_{s}^{\top}\delta\bm{f}_{s}^ {\star}]+\mathbb{E}[\text{tr}(\delta\bm{g}_{s}^{\star}(\delta\bm{g}_{s}^{ \star})^{\top})]ds\] \[\leq\int_{0}^{t}2\mathbb{E}\left[\left\|\delta\bm{x}_{s}\right\| \left\|\delta\bm{f}_{s}^{\star}\right\|\right]+\mathbb{E}[\left\|\delta\bm{g} _{s}^{\star}\right\|^{2}]ds\] \[\leq\int_{0}^{t}3L_{\bm{f}^{\star}}+2L_{\bm{g}^{\star}}^{2}) \mathbb{E}\left[\bm{y}_{s}\right]+(L_{\bm{f}^{\star}}+2L_{\bm{g}^{\star}}^{2}) \left\|\delta\bm{u}\right\|ds,\] where we used \((a+b)^{2}\leq 2a^{2}+2b^{2}\) and \(ab\leq\frac{1}{2}(a^{2}+b^{2})\). Applying Gronwall's inequality results in: \[\mathbb{E}\left[\left\|\delta\bm{x}_{t}\right\|^{2}\right] \leq\left\|\delta\bm{u}\right\|^{2}(L_{\bm{f}^{\star}}+2L_{\bm{g}^ {\star}}^{2})e^{(3L_{\bm{f}^{\star}}+2L_{\bm{g}^{\star}}^{2})t}\] \[\leq\left\|\delta\bm{u}\right\|^{2}(L_{\bm{f}^{\star}}+2L_{\bm{g} ^{\star}}^{2})e^{(3L_{\bm{f}^{\star}}+2L_{\bm{g}^{\star}}^{2})T}\] Applying Lemma 13 on 2. and 3. we have that \(\bm{\Phi}_{\bm{f}^{\star}}(\cdot,\cdot,t)\) is \(2\left(e^{\frac{2L_{\bm{f}^{\star}}+L_{\bm{g}^{\star}}^{2}}{2}T}+\sqrt{L_{\bm {f}^{\star}}+2L_{\bm{g}^{\star}}^{2}},e^{\frac{3L_{\bm{f}^{\star}}+2L_{\bm{g}^ {\star}}^{2}}{2}T}\right)\)-Lipschitz. Applying Lemma 13 on 1. and \(\bm{\Phi}_{\bm{f}^{\star}}(\cdot,\cdot,t)\) and bounding \(2\leq 4\) we finally obtain that \(\bm{\Phi}_{\bm{f}^{\star}}\) is \(4\left(e^{\frac{2L_{\bm{f}^{\star}}+L_{\bm{g}^{\star}}^{2}}{2}T}+\sqrt{L_{\bm {f}^{\star}}+2L_{\bm{g}^{\star}}^{2}},e^{\frac{3L_{\bm{f}^{\star}}+2L_{\bm{g}^ {\star}}^{2}}{2}T}+F\right)\)-Lipschitz.

**Corollary 15** (Lipschitzness of the \(\Phi_{b^{\star}}\)).: _The cost flow \(\Phi_{b^{\star}}\) is \(\mathcal{O}\left(e^{C_{1}(L_{\bm{f}^{\star}}+L_{\bm{g}^{\star}}^{2})T}\right)\)-Lipschitz, where \(C_{1}\) is a constant._

Proof.: Same as in the proof of Lemma 14 we first show coordinate-wise Lipschitzness.

1. We first show Lipschitness in time: \[\left|\Phi_{b^{\star}}(\bm{x},\bm{u},t)-\Phi_{b^{\star}}(\bm{x}, \bm{u},\widehat{t})\right| =\left|\mathbb{E}\left[\int_{\widehat{t}}^{t}b^{\ast}(\bm{x}_{s}, \bm{u})ds\right]\right|\] \[\leq\mathbb{E}\left[\int_{\widehat{t}}^{t}\left|b^{\ast}(\bm{x} _{s},\bm{u})\right|ds\right]\] \[\leq\mathbb{E}\left[B(t-\widehat{t})\right]=B(t-\widehat{t}).\]
2. To obtain Lipschitzness in state observe: \[\left|\Phi_{b^{\star}}(\bm{x},\bm{u},t)-\Phi_{b^{\star}}(\widehat{\bm{x}}, \bm{u},t)\right|=\left|\mathbb{E}\left[\int_{0}^{t}b^{\ast}(\bm{\Xi}(\bm{x},\bm{ u},s),\bm{u})-b^{\ast}(\bm{\Xi}(\widehat{\bm{x}},\bm{u},s),\bm{u})ds \right]\right|\]\[\leq L_{b^{*}}\left(t+\frac{2\sqrt{L_{\bm{f}^{*}}+2L_{\bm{g}^{*}}^{2}} }{3L_{\bm{f}^{*}}+2L_{\bm{g}^{*}}^{2}}\left(e^{\frac{3L_{\bm{f}^{*}}+2L_{\bm{g}^ {*}}^{2}}{2}}-1\right)\right)\left\|\bm{u}-\widehat{\bm{u}}\right\|\] \[\leq L_{b^{*}}\left(T+\frac{2\sqrt{L_{\bm{f}^{*}}+2L_{\bm{g}^{*}} ^{2}}}{3L_{\bm{f}^{*}}+2L_{\bm{g}^{*}}^{2}}\left(e^{\frac{3L_{\bm{f}^{*}}+2L_{ \bm{g}^{*}}^{2}}{2}}T-1\right)\right)\left\|\bm{u}-\widehat{\bm{u}}\right\|\] \[\leq L_{b^{*}}\left(T+\frac{2\sqrt{L_{\bm{f}^{*}}+2L_{\bm{g}^{*}} ^{2}}}{3L_{\bm{f}^{*}}+2L_{\bm{g}^{*}}^{2}}\left(e^{\frac{3L_{\bm{f}^{*}}+2L_{ \bm{g}^{*}}^{2}}{2}}T-1\right)\right)\left\|\bm{u}-\widehat{\bm{u}}\right\|\]

Applying Lemma 13 result follows. 

**Corollary 16** (Lipschitzness of \(\bm{\Phi}^{*}\)).: _The unknown function \(\bm{\Phi}^{*}\) is \(L_{\bm{\Phi}}=L_{\bm{\Phi}_{f}}+L_{\bm{\Phi}_{b}}=\mathcal{O}\left(e^{D(L_{\bm {f}^{*}}+L_{\bm{g}^{*}}^{2})T}\right)\)-Lipschitz, where \(D\) is constant._

#### a.2.3 Regret bound

**Lemma 17** (Per episode regret bound (general sub-Gaussian noise)).: _Consider the setting with a bounded number of switches \(K\), and let Assumption 1, 3, and Assumption 4 hold. Then, we get with probability at least \(1-\delta\):_

\[V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,K)-V_{\bm{\pi}^{*}, \bm{\Phi}^{*}}(\bm{x}_{0},T,K)\leq\] \[\leq\mathcal{O}\left(L_{\bm{\sigma}}^{K-1}\beta_{n-1}^{K}e^{C(L_ {\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})(1+L_{\bm{\pi}})TK}\mathbb{E}\left[\sum_{k=0}^ {K}\left\|\bm{\sigma}_{n-1}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k},t_{n,k},k) )\right\|_{2}\right]\right)\]

Proof.: Applying Lemma 5 of Curi et al. (2020) the result follows. 

**Theorem 18**.: _Consider the setting with a bounded number of switches \(K\), and let Assumption 1, 3, and Assumption 4 hold. Then, we get with probability at least \(1-\delta\):_

\[R_{N}=\sum_{n=1}^{N}V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,K)-V_{\bm{\pi} ^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T,K)\]\[\leq\mathcal{O}\left(L_{\bm{\sigma}}^{K-1}\beta_{N-1}^{K}\sqrt{K}e^{C(L_{ \bm{f}^{*}}+L_{\bm{g}^{*}}^{2})(1+L_{\bm{\pi}})TK}\sqrt{N\mathcal{I}_{N}}\right)\]

Proof.: We apply Lemma 17 and Cauchy-Schwarz:

\[R_{N} =\sum_{n=1}^{N}V_{\bm{\pi}_{n},\bm{\Phi}^{*}}(\bm{x}_{0},T,K)-V_{ \bm{\pi}^{*},\bm{\Phi}^{*}}(\bm{x}_{0},T,K)\] \[\leq\sum_{n=1}^{N}\mathcal{O}\left(L_{\bm{\sigma}}^{K-1}\beta_{n- 1}^{K}e^{C(L_{\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})(1+L_{\bm{\pi}})TK}\mathbb{E} \left[\sum_{k=0}^{K}\|\bm{\sigma}_{n-1}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{n,k}, t_{n,k},k))\|_{2}\right]\right)\] \[\leq\mathcal{O}\left(L_{\bm{\sigma}}^{K-1}\beta_{N-1}^{K}e^{C(L_ {\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})(1+L_{\bm{\pi}})TK}\right)\mathbb{E}\left[\sum _{n=1}^{N}\sum_{k=0}^{K}\|\bm{\sigma}_{n-1}(\bm{x}_{n,k},\bm{\pi}_{n}(\bm{x}_{ n,k},t_{n,k},k))\|_{2}\right]\] \[\leq\mathcal{O}\left(L_{\bm{\sigma}}^{K-1}\beta_{N-1}^{K}e^{C(L_ {\bm{f}^{*}}+L_{\bm{g}^{*}}^{2})(1+L_{\bm{\pi}})TK}\right)\sqrt{K}\sqrt{N \mathcal{I}_{N}}\]

Here we first applied Lemma 17. Then we used the monotonicity of \((\beta_{n})_{n\geq 0}\) sequence. In the last step we first applied maximum over the collected data, then Cauchy-Schwarz inequality and finally the definition of model complexity.

[MISSING_PAGE_EMPTY:24]

Figure 7: When stochasticity of the environments increases, we need more interactions at the unstable equilibrium (Pendulum on top). The stochasticity scale goes from 0.1 to 0.5 to 1.0 from top to bottom row respectively.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? * Answer: [Yes] Justification: We highlight the problem setting, algorithm, and theoretical and empirical results in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 5 we highlight the assumptions of our work, which also correspond to the limitations of our theoretical analysis and also the setting for which our algorithm yields theoretical guarantees. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All theoretical results are accompanied by the relevant assumptions that are listed in Section 5 and we provide all proofs in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the code with all hyperparameters we used to run the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code with all the hyperparameters to run the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the experimental details either, minor part in the main paper, and the major part in the accompanying code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments are run with 5 seeds and mean performance with standard error is reported in all our plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the details of compute in the readme.txt file as part of the enclosed code. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms, in every respect, to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper proposes a method to improve exploration in reinforcement learning in the nonepisodic setting, and is not tied to specific applications. As such, it shares the many potential societal consequences that are associated with reinforcement learning and automation as a whole, spanning from environmental impact to concerns on ethics and alignment. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release high-risk data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all creators whose code we used in our experiments in Section 4. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is documented where applicable. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.