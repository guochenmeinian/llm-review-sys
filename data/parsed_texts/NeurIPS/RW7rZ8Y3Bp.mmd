# Federated Spectral Clustering via Secure Similarity Reconstruction

 Dong Qiao\({}^{1,2}\)  Chris Ding\({}^{1}\)  Jicong Fan \({}^{1,2}\)

\({}^{1}\)The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\)Shenzhen Research Institute of Big Data, Shenzhen, China

dongqiao@link.cuhk.edu.cn {chrisding,fanjicong}@cuhk.edu.cn

Corresponding author

###### Abstract

Federated learning has a significant advantage in protecting data and information privacy. Many scholars proposed various secure learning methods within the framework of federated learning but the study on secure federated unsupervised learning especially clustering is limited. We in this work propose a secure kernelized factorization method for federated spectral clustering on distributed data. The method is non-trivial because the kernel or similarity matrix for spectral clustering is computed by data pairs, which violates the principle of privacy protection. Our method implicitly constructs an approximation for the kernel matrix on distributed data such that we can perform spectral clustering under the constraint of privacy protection. We provide a convergence guarantee of the optimization algorithm, reconstruction error bounds of the Gaussian kernel matrix, and the sufficient condition of correct clustering of our method. We also present guarantees of differential privacy. Numerical results on synthetic and real datasets demonstrate that the proposed method is efficient and accurate in comparison to baselines.

## 1 Introduction

In the era of big data, human beings can analyze massive data in various fields due to the improvement of storage and computational capabilities of computing devices [11]. Some popular fields such as artificial intelligence, machine learning, internet of things (IoT), and cloud computing have seen explosive development over the past few years. Nevertheless, a side effect of this trend is that individuals and organizations have more and more concerns about potential violation of privacy [17]. As a result, it has become a challenge to mine valuable information from user data but not to directly access it.

Federated learning [17, 18] can train a global model without retrieving dispersed data [20]. This advantage has made it so popular that many scholars have put much effort into the study of federated learning. For example, Yang _et al._[21] presented the definitions of horizontal federated learning, vertical federated learning, and federated transfer learning. Some privacy-preserving machine learning models were also presented. For instance, He _et al._[20] developed a federated group knowledge transfer algorithm to train small CNNs on edge devices. Chen _et al._[2018] proposed a protocol to conduct privacy-preserving ridge regression over high-dimensional data. Besides, Kim _et al._[2018] proposed a block-chained federated learning architecture that enables on-device learning without any central coordination.

Regardless of the great progress of federated learning, it can be found that most of the existing studies are for supervised learning [11, 12]. Note that collecting labeled data may deserve very high cost in real situations [11] while unlabeled data are abundant. Thus, itis necessary and important to study federated learning for unsupervised learning (Zhang et al., 2020; Tzinis et al., 2021; Zhuang et al., 2021; Dennis et al., 2021) such as clustering (Ng et al., 2001; Fan and Chow, 2017; Fan et al., 2018, 2021; Fan, 2021; Cai et al., 2022; Fan et al., 2022). For example, Li et al. (2021) proposed a federated matrix factorization with a privacy guarantee for recommendation systems. Wang and Chang (2022) proposed two federated matrix factorization algorithms that can be used for federated clustering. Besides, there are some studies on federated spectral clustering. For instance, Wang et al. (2020) presented a federated multi-view spectral clustering method under the assumption that the data of each view are in one client. Hernandez-Pereira et al. (2021) developed a cooperative spectral clustering model to deal with distributed data but the model is linear. However, the study on federated spectral clustering is still very limited and deserves more attention and effort.

In this paper, we propose a federated kernelized factorization method to reconstruct a similarity matrix for secure spectral clustering on distributed data. Our contributions are as follows.

* We propose a federated spectral clustering algorithm and provide convergence guarantee for the optimization.
* We further propose to add noise to the data or the learned factors to enhance the security of clustering and provide guarantees of differential privacy.
* We provide upper bounds for the reconstruction error of the true similarity matrix and theoretical guarantees for correct clustering.

We test our method on both synthetic data and real datasets in comparison to baselines, which verify the effectiveness of our method.

NotationsWe use \(y\), \(\bm{y}\), and \(\bm{Y}\) to denote scalar, vector, and matrix, respectively. The element of \(\bm{Y}\) at row \(i\) and column \(j\) is denoted by \(y_{ij}\). We use \(\|\cdot\|_{2}\) to denote the \(\ell_{2}\) norm of a vector and use \(\text{Tr}(\cdot),\|\cdot\|_{F}\), and \(\|\cdot\|_{sp}\) to denote the trace, Frobenius norm, and spectral norm of a matrix respectively. The \(\ell_{\infty}\) norm and \(\ell_{2,\infty}\) norm of a matrix \(\bm{Y}\) are defined as \(\|\bm{Y}\|_{\infty}=\max_{ij}|y_{ij}|\) and \(\|\bm{Y}\|_{2,\infty}=\max_{j}\sqrt{\sum_{i}y_{ij}^{2}}\) respectively. \(\bm{K}\), \(\mathcal{K}\), \(\mathcal{K}\), and \(k\) denote the kernel matrix, kernel function, number of clusters, and the number \(k\) in KNN, respectively. \(\phi\) denotes the feature map induced by \(\mathcal{K}\).

## 2 Federated Spectral Clustering (FedSC)

Suppose we have \(n\) data points of dimension \(m\) distributing in \(P\) clients. For convenience, we denote by \(\bm{X}\in\mathbb{R}^{m\times n}\) the matrix composed of all the \(n\) data points and denote by \(\bm{X}_{p}\in\mathbb{R}^{m\times N_{p}}\) the matrix composed of the \(N_{p}\) data points in client \(c_{p}\), where \(N_{p}\geq 1\), \(p=1,\ldots,P\), and \(\sum_{p=1}^{P}N_{p}=n\). Without loss of generality, we let \(\bm{X}=[\bm{X}_{1},\bm{X}_{2},\ldots,\bm{X}_{P}]\), which means \(\{\bm{X}_{p}\}_{p=1}^{P}\) are submatrices of \(\bm{X}\). Our goal is to perform spectral clustering on these data to partition them into \(\mathcal{K}\) groups, under the constraint that the data in each client cannot leave the client itself and the privacy of the data should be protected as much as possible, though there could be a central server conducting clustering.

The aforementioned task is non-trivial because in spectral clustering, the first step is constructing an adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\), which has to evaluate the similarity between every data pair \((\bm{x}_{i},\bm{x}_{j})\) using a metric function \(\mathcal{M}(\cdot,\cdot)\) and hence violates the privacy constraint in the task. To solve the problem, we present a federated spectral clustering model in this section.

### Similarity Reconstruction via Feature Space Factorization

In spectral clustering, for \(\mathcal{M}(\cdot,\cdot)\), there are many choices such as \(k\) nearest neighbor similarity and various kernel functions. Let \(\mathcal{K}(\cdot,\cdot)\) be a kernel function and we have

\[\mathcal{K}(\bm{x}_{i},\bm{x}_{j})=\phi(\bm{x}_{i})^{T}\phi(\bm{x}_{j}),\] (1)

where \(\phi:\mathbb{R}^{m}\rightarrow\mathbb{R}^{m^{\prime}}\) is a feature map induced by the kernel function2 and does not need to be carried out explicitly. When it comes to federated spectral clustering, the central server has no access to the raw data distributed in clients and hence cannot compute \(\mathcal{K}(\bm{x}_{i},\bm{x}_{j})\) using (1). However, if the central server can learn an effective approximation (denoted by \(\widehat{\phi(\bm{x}_{i})}\)) for each \(\phi(\bm{x}_{i})\) without accessing \(\bm{x}_{i}\), \(\mathcal{K}(\bm{x}_{i},\bm{x}_{j})\) can be estimated, i.e.,

\[\mathcal{K}(\bm{x}_{i},\bm{x}_{j})\simeq\widehat{\phi(\bm{x}_{i})}^{T} \widehat{\phi(\bm{x}_{j})}.\] (2)

Thus, inspired by [19, 19], we propose to approximate each \(\phi(\bm{x}_{i})\) by

\[\widehat{\phi(\bm{x}_{i})}=\phi(\bm{Z})\bm{c}_{i},\] (3)

where \(\bm{Z}=[\bm{z}_{1},\bm{z}_{2},\ldots,\bm{z}_{d}]\in\mathbb{R}^{m\times d}\), \(\phi(\bm{Z})=[\phi(\bm{z}_{1}),\phi(\bm{z}_{2})\ldots,\phi(\bm{z}_{d})]\), and \(\bm{c}_{i}\in\mathbb{R}^{d}\). Both \(\bm{Z}\) and \(\bm{c}_{i}\) are learned from individual columns of \(\bm{X}\) and they can be regarded as intermediate variables avoiding the direct access of central server to \(\bm{x}_{i}\) (details of the learning will be introduced later). It follows from (2) and (3) that

\[\mathcal{K}(\bm{x}_{i},\bm{x}_{j})\simeq\bm{c}_{i}^{T}\phi(\bm{Z})^{\top}\phi (\bm{Z})\bm{c}_{j}.\] (4)

Thus we can reconstruct the similarity between \(\bm{x}_{i}\) and \(\bm{x}_{j}\) via (4). For convenience, let \(\bm{K}_{xx}=\mathcal{K}(\bm{X},\bm{X})=\phi(\bm{X})^{\top}\phi(\bm{X})\), \(\bm{K}_{zz}=\mathcal{K}(\bm{Z},\bm{Z})=\phi(\bm{Z})^{\top}\phi(\bm{Z})\in \mathbb{R}^{d\times d}\), and \(\bm{C}=[\bm{c}_{1},\bm{c}_{2},\ldots,\bm{c}_{n}]\in\mathbb{R}^{d\times n}\). Then we have

\[\bm{K}_{xx}\simeq(\phi(\bm{Z})\bm{C})^{T}(\phi(\bm{Z})\bm{C})=\bm{C}^{T}\bm{K} _{zz}\bm{C}\triangleq\hat{\bm{K}}_{xx}.\] (5)

Now we use \(\hat{\bm{K}}_{xx}\) as a reconstructed similarity matrix for spectral clustering.

In the form of federated learning, we expand (3) to

\[\phi(\bm{X}_{p})\simeq\phi(\bm{Z})\bm{C}_{p},\quad p=1,\ldots,P.\] (6)

It indicates that \(\bm{Z}\) is shared for all \(P\) clients and \(\bm{C}_{p}\) is private for client \(c_{p}\). Note that (6) is a matrix factorization problem in the feature space induced by a kernel on the data in client \(c_{p}\), \(p=1,\ldots,P\). Letting \(\bm{C}=[\bm{C}_{1},\ldots,\bm{C}_{P}]\), we solve the following distributed optimization problem3

Footnote 3: Note that we do not show the data \(\bm{X}_{p}\) in the objective explicitly since it is absorbed into \(f_{p}\).

\[\underset{\bm{Z},\,\,\bm{C}}{\text{minimize}}\,\,\,F(\bm{Z},\bm{C})\triangleq \sum_{p=1}^{P}\omega_{p}f_{p}(\bm{Z},\bm{C}_{p}).\] (7)

In (7), \(f_{p}\) is a local objective function for client \(c_{p}\) and \(\omega_{1},\ldots,\omega_{P}\) are nonnegative weights for the clients. In this work, we let

\[\begin{split} f_{p}(\bm{Z},\bm{C}_{p})=&\frac{1}{2} \left\|\phi(\bm{X}_{p})-\phi(\bm{Z})\bm{C}_{p}\right\|_{F}^{2}+\frac{\lambda}{2 }\left\|\bm{C}_{p}\right\|_{F}^{2}\\ =&\frac{1}{2}\text{Tr}(\mathcal{K}(\bm{X}_{p},\bm{X} _{p}))-\text{Tr}(\bm{C}_{p}^{T}\mathcal{K}(\bm{Z},\bm{X}_{p}))+\frac{1}{2} \text{Tr}(\bm{C}_{p}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}_{p})+\frac{\lambda}{ 2}\left\|\bm{C}_{p}\right\|_{F}^{2},\end{split}\] (8)

where \(\lambda\geq 0\) is a penalty parameter. To guarantee the privacy of information, problem (7) shall be solved in the framework of federated learning.

### FedSC by Similarity Reconstruction and Model Averaging

In this section, we develop a FedSC algorithm by similarity reconstruction and model averaging. As a classic and popular framework, FederatedAveraging (or FedAvg) is first introduced in [18] for federated learning. In our work, the proposed FedSC is, therefore, built up based on the backbone of FedAvg as in Figure 1. FedSC consists of two stages. The first stage, shown by the left plot of Figure 1, is federated similarity reconstruction, which constructs a similarity matrix in the manner of federated learning. The second stage, shown by the left plot of Figure 1, is using the reconstructed similarity matrix to implement spectral clustering.

**Stage I Federated Similarity Reconstruction**

Step I: As the startup settings for our algorithm, the shared variable \(\bm{Z}\) (_i.e._, the dictionary matrix \(\bm{Z}\)) and each local coefficient matrix \(\bm{C}_{p}\) for \(p=1,2,\cdots,P\) are initialized randomly in the central server and each client, respectively.

Step 2: For each round \(s\), where \(1\leq s\leq S\), the previous shared variable \(\bm{Z}\) will firstly be broadcast to each participated client. After that, every client uses this received dictionary matrix \(\bm{Z}\) to run its own iterative updates of local variables in the Local Update Module (LUM) as:

\[\bm{C}_{p}^{s}= \operatorname*{arg\,min}_{\bm{C}_{p}}f_{p}(\bm{Z}^{s-1},\bm{C}_{p })=\operatorname*{arg\,min}_{\bm{C}_{p}}\frac{1}{2}\left\|\phi(\bm{X}_{p})- \phi(\bm{Z}^{s-1})\bm{C}_{p}\right\|_{F}^{2}+\frac{\lambda}{2}\left\|\bm{C}_{p }\right\|_{F}^{2}\] (9) \[\bm{Z}_{p}^{s}= \operatorname*{arg\,min}_{\bm{Z}_{p}}f_{p}(\bm{Z}_{p},\bm{C}_{p }^{s})=\operatorname*{arg\,min}_{\bm{Z}_{p}}\frac{1}{2}\left\|\phi(\bm{X}_{p})- \phi(\bm{Z}_{p})\bm{C}_{p}^{s}\right\|_{F}^{2}+\frac{\lambda}{2}\left\|\bm{C}_ {p}^{s}\right\|_{F}^{2}\] (10)

Step 3: Each client sends back its own dictionary matrix \(\bm{Z}_{p}^{s}\), \(p=1,2,\ldots,P\), to the central server.

Step 4: The central server collects all (or a subset \(\mathcal{A}^{s-1}\) of) these uploaded matrices \(\{\bm{Z}_{p}^{s}\}_{p=1}^{P}\) and averages them into one new matrix \(\bm{Z}^{s}\) in Aggregation Module (AM), i.e.,

\[\bm{Z}^{s}=\frac{1}{\left|\mathcal{A}^{s-1}\right|}\sum_{p\in\mathcal{A}^{s-1} }\bm{Z}_{p}^{s}\] (11)

where \(\left|\mathcal{A}^{s-1}\right|\) is the number of participated clients. In our study, we fix the number of participating clients for each round \(s\). Therefore, we use the notation \(\bar{P}\) instead of \(\left|\mathcal{A}^{s-1}\right|\) in the sequel. This aggregated dictionary matrix \(\bm{Z}^{s}\) will then be used to push the next round of federated iteration until the tolerance condition is broken.

Step 5: When Stage I comes to an end, the spectral clustering will start.

**Stage II Spectral Clustering**

Step 6: Each client sends \((\bm{Z}_{p}^{S},\bm{C}_{p}^{S})\) back to the central server for the final aggregation of information.

Step 7: The required similarity matrix is then constructed based on the obtained dictionary matrix \(\bm{Z}^{S}\) and coefficient matrix \(\bm{C}^{S}\) in Spectral Clustering Module (SCM). Based on this approximated similarity matrix, the standard spectral clustering is implemented as usual:

\[\mathbf{y}=\text{SpectralClustering}(\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C },\mathcal{K}).\] (12)

Step 8: The central server broadcasts its clustering results to every corresponding client.

Figure 1: Diagram of the proposed FedSC. Stage I (left plot): Federated Similarity Reconstruction (Steps 1-5). Stage II (right plot): Spectral Clustering (Steps 6-8).

### Optimization Algorithm for Federated Similarity Reconstruction

As described in the above section, alternate updating of local variables is a key to solving the proposed FedSC problem. In the following two parts, we discuss the optimization for \(\bm{Z}\) and \(\bm{C}\), respectively.

For a client \(c_{p}\), consider the corresponding local optimization problem

\[\underset{\bm{Z},\bm{C}}{\text{minimize}}\ f_{p}(\bm{Z},\bm{C})\] (13)

where \(f_{p}(\bm{Z},\bm{C})=\frac{1}{2}\left\|\phi(\bm{X}_{p})-\phi(\bm{Z})\bm{C} \right\|_{F}^{2}+\frac{\lambda}{2}\left\|\bm{C}\right\|_{F}^{2}=\frac{1}{2} \text{Tr}(\mathcal{K}(\bm{X}_{p},\bm{X}_{p}))-\text{Tr}(\bm{C}^{T}\mathcal{K}( \bm{Z},\bm{X}_{p}))+\frac{1}{2}\text{Tr}(\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z}) \bm{C})+\frac{\lambda}{2}\left\|\bm{C}\right\|_{F}^{2}\). Let the derivative of \(f_{p}(\bm{Z},\bm{C})\) w.r.t. \(\bm{C}\) be zero, we get the following one-step update for \(\bm{C}\):

\[\bm{C}_{p}^{s}=(\mathcal{K}(\bm{Z}^{s-1},\bm{Z}^{s-1})+\lambda\bm{I}_{d})^{-1 }\mathcal{K}(\bm{Z}^{s-1},\bm{X}_{p}),\quad p=1,2,\ldots,P.\] (14)

The derivative of \(f_{p}(\bm{Z},\bm{C})\) w.r.t. \(\bm{Z}\) is

\[\frac{\partial\mathcal{L}}{\partial\bm{Z}}=\frac{1}{\sigma^{2}}(\bm{X}_{p}\bm {W}_{Z}-\bm{Z}\bar{\bm{W}}_{Z})+\frac{2}{\sigma^{2}}(\bm{Z}\bm{Q}_{Z}-\bm{Z} \bar{\bm{Q}}_{Z}),\] (15)

where the intermediate variables are detailed as

\[\begin{array}{ll}\bm{W}_{Z}=-\bm{C}^{T}\odot\mathcal{K}(\bm{X}_{p},\bm{Z})& \bar{\bm{W}}_{Z}=\text{diag}(\bm{1}_{1}^{T}\bm{W}_{Z})\\ \bm{Q}_{Z}=(0.5\bm{C}\bm{C}^{T})\odot\mathcal{K}(\bm{Z},\bm{Z})&\bar{\bm{Q}}_ {Z}=\text{diag}(\bm{1}_{d}^{T}\bm{Q}_{Z}).\end{array}\]

Here \(\bm{1}_{n}\) and \(\bm{1}_{d}\) are the column vectors with all elements of \(1\). Because of the kernel function, \(\bm{Z}\) cannot be updated like \(\bm{C}_{p}\). Here, we use the gradient method to update it. At local iteration \(t\), by setting \(\bm{Z}_{p}^{s,0}=\bm{Z}^{s-1}\), the update scheme of \(\bm{Z}\) is

\[\bm{Z}_{p}^{s,t}=\bm{Z}_{p}^{s,t-1}-\eta_{t}\frac{\partial f_{p}}{\partial\bm {Z}}(\bm{Z}_{p}^{s,t-1}).\] (16)

where \(\eta_{t}\) is the step size and can be set as the reverse of the Lipschitz constant of gradient if possible. We summarize the optimization details in Algorithm 1 (shown in Appendix A).

### Convergence Analysis of The Proposed Algorithm

First of all, it is obvious that all local objective functions \(f_{p}(\cdot,\cdot)\) for \(p=1,\ldots,P\) are lower bounded. To analyze the convergence of Algorithm 1, we make two assumptions. The first one is the Lipschitz continuity of the gradient of the local objective functions.

**Assumption 2.1**.: The gradients of all local objective functions \(f_{p}(\cdot,\cdot)\) for \(p=1,\ldots,P\) are \(L_{Z_{p}}^{s}\)-Lipschitz continuous in \(\bm{Z}\), that is

\[\left\|\nabla_{Z}f_{p}(\bm{Z}^{s,t},\bm{C}_{p}^{s})-\nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}\leq L_{Z_{p}}^{s}\left\|\bm{Z}^{s,t}-\bm{Z}^ {s,t-1}\right\|_{F}.\] (17)

In addition, there exist some lower and upper bounds for \(L_{Z_{p}}^{s}\), _i.e._, \(0<\underline{L}_{Z}\leq L_{Z_{p}}^{s}\leq\overline{L}_{Z}\) hold for all \(p=1,\ldots,P\) and \(s=1,\ldots,S\).

The second assumption, similar to (Li et al., 2019; Lian et al., 2017), is as follows.

**Assumption 2.2**.: The difference between the local gradient and the global gradient is bounded as

\[\left\|\nabla_{Z}f_{p}(\bm{Z},\bm{C}_{p})-\nabla_{Z}F(\bm{Z},\bm{C})\right\|_{F }\leq\zeta,\quad\forall\;p=1,\ldots,P.\] (18)

To build the convergence condition, we define the following iterative terms of \(\bm{Z}^{s,t}\) and \(\bm{C}^{s}\) for all \(t=1,\ldots,Q\) and \(s=1,\ldots,S\):

\[\begin{array}{l}T_{C}(\bm{Z}^{s,0},\bm{C}^{s})=\sum_{p=1}^{P}\omega_{p} \left\|\bm{C}_{p}^{s}-\bm{C}_{p}^{s-1}\right\|_{F}^{2}\\ T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})=\left\|\bm{Z}^{s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2} \end{array}\] (19)

where the instantaneous average \(\bm{Z}^{s,t}\) is defined as

\[\bm{Z}^{s,t}=\frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}}\bm{Z}_{p}^{s,t}.\] (20)

Based on the above assumptions, we provide the following convergence guarantee for Algorithm 1.

**Theorem 2.3** (Convergence of Algorithm 1).: _Suppose Assumption 2.1 and Assumption 2.2 hold. Let \(T=S(1+Q)\) be the total number of global and local rounds. Then the sequence \(\{(\bm{Z}^{s,t},\bm{C}^{s})\}\) generated by Algorithm 1 with stepsize \(1/L_{Z_{p}}^{s}\) and \(\omega_{p}=\frac{N_{p}}{n}\) satisfies_

\[\frac{1}{T}\left[\sum_{s=1}^{S}T_{C}(\bm{Z}^{s,0},\bm{C}^{s})+\sum_{s=1}^{S} \sum_{t=1}^{Q}T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})\right]\leq\frac{D}{T}[F(\bm{Z}^{1,0},\bm{C}^{0})-\underline{f}]+\frac{16\zeta^{2}\psi D}{\bar{P}\underline{L}_{Z}}\] (21)

_where \(\psi=1+\frac{(\bar{P}+8)(Q-1)(2Q-1)}{\bar{P}-4(Q-1)^{2}(1+\overline{L}_{Z}^{2} /L_{Z}^{2})}\) and \(D=\frac{2}{\sum_{min}+\lambda}+\frac{4}{\underline{L}_{Z}}\)._

The proof can be found in Appendix E. We see that when \(T\to\infty\), the algorithm converges to a finite value, which is small if \(\zeta\) is small and \(\overline{L}_{Z}\) is close to \(\underline{L}_{Z}\).

## 3 Security-Enhanced FedSC

In order to enhance the security of FedSC, we present two noise-augmented variants of the proposed algorithm in this section.

### FedSC with Perturbed Data

We add random noise to the data in each client and then perform Algorithm 1 to reconstruct a similarity matrix, which further improves the privacy of data. Specifically, the data \(\bm{X}\in\mathbb{R}^{m\times n}\) is perturbed by a noise matrix \(\bm{E}\in\mathbb{R}^{m\times n}\) to form the noisy data matrix

\[\tilde{\bm{X}}=\bm{X}+\bm{E},\] (22)

where \(E_{ij}\sim\mathcal{N}(0,\sigma^{2})\). We then perform Algorithm 1 with a Gaussian kernel of parameter \(r\) on \(\tilde{\bm{X}}\) and obtain \(\bm{Z}\), \(\bm{C}=[\bm{C}_{1},\bm{C}_{2},\ldots,\bm{C}_{P}]\), and

\[\hat{\bm{K}}_{\tilde{x}\tilde{x}}=\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}.\] (23)

We have the following reconstruction (for the true similarity matrix \(\bm{K}_{xx}=\mathcal{K}(\bm{X},\bm{X})\)) error bound4.

Footnote 4: We defer the proof for all theoretical results to the supplementary material.

**Theorem 3.1** (Error bound of similarity matrix reconstruction).: _Suppose \(\left\|\bm{X}\right\|_{2,\infty}=\theta\), \(\left\|\bm{C}\right\|_{2,\infty}=\tau_{C}\), and \(\left\|\phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{X}})\right\|_{2,\infty}\leq\gamma\), where \(\theta\), \(\tau_{C}\), and \(\gamma\) are some nonnegative constants. Then with the probability at least \(1-n(n-1)e^{-t}\), the reconstructed similarity matrix \(\hat{\bm{K}}_{\tilde{x}\tilde{x}}\) satisfies_

\[\left\|\hat{\bm{K}}_{\tilde{x}\tilde{x}}-\bm{K}_{xx}\right\|_{\infty}\leq\frac {1}{r^{2}}\left[(\sigma\xi+\sqrt{2}\theta)^{2}-2\theta^{2}\right]+(\sqrt{d} \tau_{C}+1)\gamma\] (24)

_where \(\xi=\sqrt{(m+2\sqrt{mt}+2t)}\)._

Note that \(r\) is the hyperparameter of the Gaussian kernel. In our experiment, \(r\) was automatically estimated as the mean of all pairwise distances between data points, _i.e._, \(r=\frac{1}{n^{2}}\sum_{i,j}\|\bm{x}_{i}-\bm{x}_{j}\|_{2}\). Assume \(|x_{ik}-x_{jk}|=\mathcal{O}(\varepsilon)\) for all \(i,j\in[n],k\in[m]\), then \(\|\bm{x}_{i}-\bm{x}_{j}\|=\mathcal{O}(\sqrt{m}\varepsilon)\), which means \(r^{2}\) is linear with \(m\varepsilon^{2}\). Thus, the reconstruction error \(\left\|\hat{\bm{K}}_{\tilde{x}\tilde{x}}-\bm{K}_{xx}\right\|_{\infty}\) is upper bounded by \(\mathcal{O}(\sigma^{2}/\varepsilon^{2}+\sqrt{d}\gamma\tau_{C})\), where \(\epsilon^{2}/\sigma^{2}\) can be regarded as a signal-noise ratio. Therefore, the bound is useful. In general, Theorem 3.1 indicates that when the added noise is small and the optimization makes \(\gamma\) small, the reconstruction error for the true similarity matrix is less than a small constant with high probability. This verified the effectiveness of our similarity reconstruction method.

It should be pointed out that \(\hat{\bm{K}}_{\tilde{x}\tilde{x}}\) is not guaranteed to be a sparse matrix and hence the corresponding graph may not contain multiple connected components. We therefore use an extra KNN-based operation to get a sparse similarity matrix, which may also reduce the computational cost of eigenvalue decomposition when \(n\) is very large. Specifically, we let

\[\hat{\bm{K}}_{\tilde{x}\tilde{x}}=\text{getSparseMatrixbyKNN}(\hat{\bm{K}}_{ \tilde{x}\tilde{x}},k)\] (25)which only keeps the largest \(k\) connections from each point to other points. Finally, we perform spectral clustering using \(\hat{\bm{K}}_{\hat{x}\hat{x}}\). The central server broadcasts the clustering results to each participating client.

As mentioned before, one can choose to inject some noise into its raw data to avoid privacy leakage. However, a question is how much noise we can add to the data to the largest extent for the guarantee of correct clustering. We first present the following definitions.

**Definition 3.2** (Local neighbor set).: Suppose \(\bm{x}_{i}\) and \(\bm{x}_{j}\) are data points of \(\bm{X}\in\mathbb{R}^{m\times n}\) with class labels \(L_{i}\) and \(L_{j}\) respectively, and let \(\text{KNN}(\bm{x}_{i})\) be the set of the k-nearest neighbors of \(\bm{x}_{i}\). We define

\[\mathcal{N}_{i}^{k,intra}:=\{\bm{x}_{j}\in\bm{X}|L_{i}=L_{j}\text{ and }\bm{x}_{j} \in\text{KNN}(\bm{x}_{i})\}.\] (26)

**Definition 3.3** (Global neighbor set).: Suppose \(\bm{x}_{i}\) and \(\bm{x}_{j}\) are data points of \(\bm{X}\in\mathbb{R}^{m\times n}\) with class labels \(L_{i}\) and \(L_{j}\) respectively, and let \(\text{KNN}(\bm{x}_{i})\) be the point set of k-nearest neighbors of \(\bm{x}_{i}\). We define

\[\mathcal{N}_{i}^{k,global}:=\{\bm{x}_{j}\in\bm{X}|\bm{x}_{j}\in\text{KNN}(\bm {x}_{i})\}.\] (27)

If we call the local neighbor of data point \(\bm{x}_{i}\) the _intra-class neighbor_ of \(\bm{x}_{i}\), another definition called _inter-class neighbor_ of \(\bm{x}_{i}\) can be further introduced as follows.

**Definition 3.4** (Inter-class neighbor set).: Suppose \(\bm{x}_{i}\) and \(\bm{x}_{j}\) are data points of data matrix \(\bm{X}\in\mathbb{R}^{m\times n}\) with class labels, \(L_{i}\) and \(L_{j}\), respectively, and let \(\text{KNN}(\bm{x}_{i})\) be the point set of k-nearest neighbors of \(\bm{x}_{i}\). We define

\[\mathcal{N}_{i}^{k,inter}:=\{x_{j}\in\bm{X}|L_{i}\neq L_{j}\text{ and }\bm{x}_{j}\in\text{KNN}(\bm{x}_{i})\}.\] (28)

Based on the above definitions, the following definition is presented to determine whether a data point can be correctly clustered or not.

**Definition 3.5** (Correct clustering).: Suppose \(\bm{x}_{i}\in\mathbb{R}^{m}\) and \(\bm{x}_{j}\in\mathbb{R}^{m}\) are data points of data matrix \(\bm{X}\in\mathbb{R}^{m\times n}\), \(\bm{x}_{i}\) is said to be correctly clustered with a tolerance of \(\epsilon\) if

1. \(\hat{\bm{K}}_{ij}\geq\max_{k}(\hat{\bm{K}}_{ik}^{inter})-\epsilon\) for any of \(\bm{x}_{j}\in\mathcal{N}_{i}^{k,intra}\);
2. \(\hat{\bm{K}}_{ij}\leq\min_{k}(\hat{\bm{K}}_{ik}^{intra})+\epsilon\) for any of \(\bm{x}_{j}\in\mathcal{N}_{i}^{k,inter}\).

Based on Definition 3.5, the following theorem gives the guarantee of our security-enhanced FedSC.

**Theorem 3.6** (Guarantee of noisy spectral clustering).: _Let \(B(\sigma)=\frac{1}{r^{2}}\left[(\sigma\xi+\sqrt{2}\theta)^{2}-2\theta^{2} \right]+(\sqrt{d}\tau_{C}+1)\gamma\). Then with the probability of at least \(1-n(n-1)e^{-t}\), performing spectral clustering using \(\hat{\bm{K}}_{\hat{x}\hat{x}}\) yields correct clustering results if_

\[B(\sigma)\leq\frac{\epsilon}{2}-\max_{i}\frac{1}{4}\left[\max_{k}(\bm{K}_{ik}^ {inter})-\min_{k}(\bm{K}_{ik}^{intra})\right]\] (29)

where \(\bm{K}_{ik}^{inter}=(\bm{K}_{xx})_{ik}^{inter}\) and \(\bm{K}_{ik}^{intra}=(\bm{K}_{xx})_{ik}^{intra}\).

Based on Theorem 3.1 and Theorem 3.6, we can get a bound on the variance of noise for FedSC with perturbed data:

\[\sigma\leq\frac{1}{\xi}\left[\sqrt{r^{2}(B_{1}-B_{2})+2\theta^{2}}-\sqrt{2} \theta\right]\] (30)

where \(B_{1}=\frac{\epsilon}{2}-\max_{i}\frac{1}{4}\left[\max_{k}(\bm{K}_{ik}^{inter}) -\min_{k}(\bm{K}_{ik}^{intra})\right]\) and \(B_{2}=(\sqrt{d}\tau_{C}+1)\gamma\). This bound indicates that the intensity of noise should not be too strong otherwise it may seriously affect the performance of federated spectral clustering. But, at least under this bound, one can choose to inject as much noise as possible into the raw data to ensure data security and privacy.

Using Theorem 3.22 in [Dwork _et al._, 2014] and the post-processing property of differential privacy, we have the following privacy guarantee for this enhanced FedSC algorithm.

**Proposition 3.7**.: _FedSC with perturbed data given by (22) is \((\varepsilon,\delta)\)-differentially private if \(\sigma\geq 2cr_{\mathcal{X}}/\varepsilon\), where \(c^{2}>2\ln(1.25/\delta)\)._Based on this proposition and (30), we obtain the following privacy-utility trade-off:

\[2\sqrt{2\ln 1.25/\delta}\tau_{X}/\varepsilon<\sigma\leq\frac{1}{\xi}\left[ \sqrt{r^{2}(B_{1}-B_{2})+2\theta^{2}}-\sqrt{2}\theta\right].\] (31)

This ensures both clustering performance and \((\varepsilon,\delta)\)-differential privacy. In particular, if we substitute \(\sigma\) with the upper bound, we can get a strong level of privacy but the worst utility. By the way, \(B_{1}-B_{2}\) is related to the property of the data. A larger \(B_{1}-B_{2}\) means a better property for clustering, which further provides a larger upper bound for the noise level \(\sigma\), yielding a stronger privacy guarantee.

### FedSC with Perturbed Factors

In FedSC with perturbed factor, we added Gaussian noise to \(\bm{Z}\) in every round of the optimization but added Gaussian noise to \(\bm{C}\) in the last round of the optimization. To be more specific, \(\tilde{\bm{C}}=\bm{C}+\bm{E}_{C}\), and \(\tilde{\bm{Z}}=\bm{Z}+\bm{E}_{Z}\), where the entries of \(\bm{E}_{C}\) and \(\bm{E}_{Z}\) are drawn from \(\mathcal{N}(0,\sigma_{C}^{2})\) and \(\mathcal{N}(0,\sigma_{Z}^{2})\) respectively. Then we perform spectral clustering using the following reconstructed kernel matrix:

\[\tilde{\bm{K}}_{xx}=\tilde{\bm{C}}^{T}\mathcal{K}(\tilde{\bm{Z}},\tilde{\bm{ Z}})\tilde{\bm{C}}.\] (32)

The following theorem shows the reconstruction error bound for the ground truth kernel matrix \(\bm{K}_{xx}\).

**Theorem 3.8**.: _Assume \(\left\|\phi(\bm{Z})\bm{C}-\phi(\bm{X})\right\|_{2,\infty}\leq\gamma\), \(\left\|\bm{C}\right\|_{2,\infty}\leq\tau_{C}\). Then with probability at least \(1-(n+d)e^{-t}\), it holds that_

\[\left\|\tilde{\bm{K}}_{xx}-\bm{K}_{xx}\right\|_{\infty}\leq\gamma_{zc}(\gamma _{zc}+2)\] (33)

_where \(\gamma_{zc}=\gamma+\sqrt{d}\left(\sigma_{C}\xi_{d}+\tau_{C}\sqrt{2\left(1- \exp\left(-\frac{\sigma_{C}^{2}\xi_{d}^{2}}{2r^{2}}\right)\right)}\right)\) and \(\xi_{d}^{2}=d+2\sqrt{dt}+2t\)._

We see that, given a fixed \(\gamma\), the reconstruction error becomes smaller if \(\sigma_{Z}\) and \(\sigma_{C}\) are smaller. Based on Theorem 3.8 and Definitions 3.2-3.5, we can obtain a bound similar to that in Theorem 3.6 to guarantee correct clustering, which will not be detailed here.

**Theorem 3.9**.: _In FedSC, assume \(\max_{(p,j)}\{\left\|\bm{x}_{p}\right\|,\left\|\bm{x}^{\prime}_{p}\right\|\} \leq\tau_{X}\), \(\max_{(i,j)}\left\|\bm{z}_{i}-\bm{x}_{j}\right\|_{\infty}=\Upsilon\), \(\left\|\bm{Z}^{s}_{p}\right\|_{sp}\leq\tau_{Z}\)\(\forall s\), and \(\left\|\bm{C}^{S}\right\|_{2,\infty}\leq\tau_{C}\), we perturb \(\{\bm{Z}^{s}_{p}\}_{p=1}^{P}\), \(\forall s=1,2,\ldots,S\) with noise drawn from \(\mathcal{N}(0,\sigma_{Z}^{2})\) with the parameter \(\sigma_{Z}\geq\sqrt{(8S\Delta^{2}(g_{Z})\log(e+(\varepsilon_{Z}/\delta_{Z}))/ \varepsilon_{Z}^{2})}\) where \(\Delta(g_{Z})=\frac{2\sqrt{d}\tau_{C}\tau_{X}\tau_{X}\eta_{k}}{r^{2}}\left\{1+ (\tau_{X}+\tau_{Z})\frac{(\tau_{X}+\Upsilon)}{r^{2}}\right\}\) and perturb \(\{\bm{C}^{S}_{p}\}_{p=1}^{P}\) with noise drawn from \(\mathcal{N}(0,\sigma_{C}^{2})\) with the parameter \(\sigma_{C}\geq 2c\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+\Upsilon)/(r^{2} \varepsilon_{C})\) for \(c^{2}>2\ln(1.25/\delta_{C})\). Then, FedSC is \((\varepsilon_{C}+\varepsilon_{Z},\delta_{C}+\delta_{Z})\)-differentially private._

Theorem 3.9 shows that our FedSC with perturbed factors can protect the data privacy provided that the noises added to \(\bm{C}\) and \(\bm{Z}\) are sufficiently large. Similarly to (31), we can also get a privacy-utility trade-off using Theorem 3.8 and Theorem 3.9, which is detailed in Appendix B.

## 4 Related Work

It should be pointed out that the study on federated spectral clustering in literature is very limited. Besides our work, the only work that aims to address the problem is [Hernandez-Pereira _et al._, 2021]. More introduction and discussion about the related work (federated matrix factorization/clustering [Yang _et al._, 2021; Ghosh _et al._, 2020; Dennis _et al._, 2021; Wang and Chang, 2022] and spectral clustering [Von Luxburg, 2007; Hernandez-Pereira _et al._, 2021]) are in the supplementary material.

## 5 Experiments

### Performance on similarity reconstruction

Taking the COIL20 dataset [Nene _et al._, 1996] as an example, we first obtain the similarity matrix from vanilla spectral clustering based on the same kernel function. Then, we use the proposed method to derive the estimated similarity matrix \(\hat{\bm{K}}_{\tilde{x}\tilde{x}}\) which is actually an approximation of ground truth. Tomake it clearer, we also give the corresponding sparse similarity matrices by KNN sparsification (25). Figure 2 shows the similarity matrices constructed by different methods. We see that the proposed method can be able to successfully reconstruct the similarity matrix in the federated scenarios. The reconstruction errors on synthetic data, iris, banknote authentication, and COIL20 are in the supplementary material.

### Clustering performance of FedSC

In this subsection, we check the clustering performance of the proposed security-enhanced FedSC method on both synthetic and real-world datasets. The synthetic dataset is generated from concentric circles. The details are in the supplementary. This synthetic dataset is visualized in Figure 3(a). Here, we continue to adopt the aforementioned COIL20 as an example of real-world datasets.

Taking the synthetic dataset as an example, the first group of cases helps illustrate the effectiveness of the proposed FedSC method. we first apply the vanilla spectral clustering method to the clean data. The predictive result is shown in Figure 3(b). It is clear that the vanilla spectral clustering method correctly clusters the data points lying in concentric circles. We then use the proposed FedSC method to cluster the data. One can find in Figure 3(c) that almost all of the data points also have been grouped correctly. However, when we inject some volume of noise into the raw data, things may change a lot. Figure 3(d) is actually the ground truth Figure 3(a) adding some Gaussian noise. When focusing on this noisy data of concentric circles, we see from Figure 3(e) that the vanilla SC failed to cluster the data points while the proposed FedSC method is still able to correctly cluster the data to some extent as in Figure 3(f). As we know, the similarity graph directly constructed from raw data could be very sensitive to each data point. When we add too much noise, the similarity graph may fail to model the local neighborhood relationships which may be the reason why data points in Figure 3(e) are not separable for vanilla SC. Instead, FedSC is based on matrix factorization in the high-dimensional feature space and has a potential denoising effect. Therefore, it is possible for our method to achieve a better performance.The visualization of COIL20 can be found in the supplementary material.

Figure 3: FedSC on concentric circles: (a) ground truth; (b) cluster assignment generated by vanilla SC; (c) cluster assignment generated by FedSC; (d) Noisy ground truth; (e) cluster assignment generated by vanilla SC on noisy data; (f) cluster assignment generated by FedSC on noisy data.

Figure 2: Visualization of similarity matrices: (a) similarity matrix of vanilla spectral clustering; (b) approximated similarity matrix of the proposed method; (c)(d) the corresponding sparse similarity matrices generated by KNN sparsification.

### Comparison with baselines

We compare our method with the clustering method DSC proposed by [1]. Because the existing literature on federated spectral clustering is rare, we here select both classic K-means and spectral clustering as the baselines. Two metrics including accuracy and NMI are adopted to evaluate the clustering results on four datasets including iris [10], COIL20 [11], banknote authentication [10], and USPS [12]. The details are in the supplementary material. Besides the clean data, we also consider adding noise to them to test the performance of methods under the condition of privacy protection. We directly inject Gaussian noise with zero mean and variance \(\sigma^{2}\) to the raw matrix \(\bm{X}\in\mathbb{R}^{m\times n}\) as \(\tilde{\bm{X}}=\bm{X}+\bm{E}\) where \(\bm{E}\) is a Gaussian noise matrix, each element \(\bm{E}_{i,j}\) of which is i.i.d. with \(\mathcal{N}(0,\sigma^{2})\).

Table 1 shows the clustering accuracy. Our FedSC almost always achieves comparable clustering results to vanilla SC. It even outperformed vanilla SC in some cases and K-means in most cases since FedSC has a potential denoising effect by approximating a similarity matrix. More importantly, FedSC significantly outperformed DSC in almost all cases. The reason is that DSC performs spectral clustering on each local dataset, which may lead to very unstable and inaccurate results.

### More numerical result

The tSNE visualization, clustering results in terms of NMI, the performance of FedSC with perturbed factors, etc, are in the supplementary material.

## 6 Conclusion

This paper has proposed a secure kernelized factorization method for federated spectral clustering on distributed data. We provide theoretical guarantees for optimization convergence, correct clustering, and differential privacy. The numerical experiments on synthetic and real image datasets verified the effectiveness of our method. To the best knowledge of the authors, this is the work that successfully addresses the problem of federated spectral clustering. One limitation of this work is that we haven't tested our FedSC on very large datasets, though the moderate-size datasets are sufficient to justify the effectiveness of our FedSC. Note that for large-scale datasets, the bottleneck of clustering is the eigenvalue decomposition of the Laplacian matrix, not our FedSC algorithm.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline  & & Kmeans & SC & DSC & FedSC \\ \hline \multirow{4}{*}{\(\bm{X}\)} & Iris & \(0.8933\pm 0.0000\) & \(0.9000\pm 0.0000\) & \(0.5480\pm 0.0679\) & \(0.9000\pm 0.0031\) \\  & COIL20 & \(0.6113\pm 0.0534\) & \(0.8025\pm 0.0009\) & \(0.1009\pm 0.0100\) & \(0.7828\pm 0.0231\) \\  & Bank & \(0.6122\pm 0.0000\) & \(0.5918\pm 0.0000\) & \(0.5582\pm 0.0045\) & \(0.7672\pm 0.1457\) \\  & USPS & \(0.6704\pm 0.0047\) & \(0.6635\pm 0.0000\) & \(0.1686\pm 0.0014\) & \(0.6596\pm 0.0021\) \\  & ORL & \(0.6325\pm 0.0270\) & \(0.7865\pm 0.0106\) & \(0.1653\pm 0.0073\) & \(0.7235\pm 0.0170\) \\ \hline \multirow{4}{*}{\(\tilde{\bm{X}}\) with \(0.1\sigma\)} & Iris & \(0.8940\pm 0.0152\) & \(0.9120\pm 0.0332\) & \(0.4533\pm 0.0658\) & \(0.8993\pm 0.0299\) \\  & COIL20 & \(0.6283\pm 0.0484\) & \(0.8024\pm 0.0020\) & \(0.0995\pm 0.0122\) & \(0.7790\pm 0.0213\) \\  & Bank & \(0.6067\pm 0.0022\) & \(0.6067\pm 0.0944\) & \(0.5558\pm 0.0023\) & \(0.7168\pm 0.1068\) \\  & USPS & \(0.6732\pm 0.0035\) & \(0.6647\pm 0.0016\) & \(0.1690\pm 0.0007\) & \(0.6643\pm 0.0022\) \\  & ORL & \(0.6195\pm 0.0329\) & \(0.7810\pm 0.0065\) & \(0.1600\pm 0.0089\) & \(0.7323\pm 0.0250\) \\ \hline \multirow{4}{*}{\(\tilde{\bm{X}}\) with \(0.3\sigma\)} & Iris & \(0.8420\pm 0.0274\) & \(0.8327\pm 0.0267\) & \(0.4533\pm 0.0674\) & \(0.8427\pm 0.0404\) \\  & COIL20 & \(0.6422\pm 0.0366\) & \(0.7997\pm 0.0029\) & \(0.0981\pm 0.0084\) & \(0.7793\pm 0.0240\) \\  & Bank & \(0.6020\pm 0.0038\) & \(0.5859\pm 0.0105\) & \(0.5588\pm 0.0074\) & \(0.6046\pm 0.0064\) \\  & USPS & \(0.6704\pm 0.0063\) & \(0.6720\pm 0.0044\) & \(0.1673\pm 0.0019\) & \(0.6884\pm 0.0509\) \\  & ORL & \(0.6098\pm 0.0167\) & \(0.7885\pm 0.0047\) & \(0.1665\pm 0.0057\) & \(0.7417\pm 0.0280\) \\ \hline \multirow{4}{*}{\(\tilde{\bm{X}}\) with \(0.5\sigma\)} & Iris & \(0.7410\pm 0.0252\) & \(0.7313\pm 0.0494\) & \(0.3833\pm 0.0204\) & \(0.7540\pm 0.0336\) \\  & COIL20 & \(0.6389\pm 0.0296\) & \(0.7950\pm 0.0080\) & \(0.1033\pm 0.0167\) & \(0.7403\pm 0.0294\) \\ \cline{1-1}  & Bank & \(0.6051\pm 0.0076\) & \(0.5923\pm 0.0094\) & \(0.5566\pm 0.0030\) & \(0.6086\pm 0.0073\) \\ \cline{1-1}  & USPS & \(0.6699\pm 0.0031\) & \(0.7843\pm 0.0030\) & \(0.1683\pm 0.0017\) & \(0.7778\pm 0.0062\) \\ \cline{1-1}  & ORL & \(0.5983\pm 0.0295\) & \(0.7930\pm 0.0172\) & \(0.1615\pm 0.0096\) & \(0.7107\pm 0.0345\) \\ \hline \multirow{4}{*}{\(\tilde{\bm{X}}\) with \(0.7\sigma\)} & Iris & \(0.6500\pm 0.0420\) & \(0.6087\pm 0.0468\) & \(0.3927\pm 0.0349\) & \(0.6120\pm 0.0455\) \\ \cline{1-1}  & COIL20 & \(0.6220\pm 0.0627\) & \(0.7662\pm 0.0172\) & \(0.0893\pm 0.0055\) & \(0.6803\pm 0.0198\) \\ \cline{1-1}  & Bank & \(0.6100\pm 0.0112\) & \(0.6046\pm 0.0144\) & \(0.5566\pm 0.0035\) & \(0.6106\pm 0.0107\) \\ \cline{1-1}  & USPS & \(0.6638\pm 0.0044\) & \(0.7747\pm 0.0040\) & \(0.1675\pm 0.0004\) & \(0.7587\pm 0.0117\) \\ \cline{1-1}  & ORL & \(0.5723\pm 0.0360\) & \(0.7860\pm 0.0093\) & \(0.1613\pm 0.0066\) & \(0.6910\pm 0.0232\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of clustering accuracy (\(\bm{X}\) and \(\tilde{\bm{X}}\) denote the raw data and corrupted data respectively). The results of NMI are in Section D.4.

## Acknowledgments

This work was partially supported by the Youth program 62106211 of the National Natural Science Foundation of China, the General Program JCYJ20210324130208022 of Shenzhen Fundamental Research, the research funding T00120210002 of Shenzhen Research Institute of Big Data, the Guangdong Key Lab of Mathematical Foundations for Artificial Intelligence, and the funding UDP01001770 of The Chinese University of Hong Kong, Shenzhen.

## References

* Cai et al. (2022) Jinyu Cai, Jicong Fan, Wenzhong Guo, Shiping Wang, Yunhe Zhang, and Zhao Zhang. Efficient deep embedded subspace clustering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1-10, June 2022.
* Chen et al. (2018) Yi-Ruei Chen, Amir Rezapour, and Wen-Guey Tzeng. Privacy-preserving ridge regression on distributed data. _Information Sciences_, 451:34-49, 2018.
* Dennis et al. (2021) Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated clustering. In _International Conference on Machine Learning_, pages 2611-2620. PMLR, 2021.
* Dua and Graff (2017) Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* Dwork et al. (2014) Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Fan and Chow (2017) Jicong Fan and Tommy W.S. Chow. Sparse subspace clustering for data with missing entries and high-rank matrix completion. _Neural Networks_, 93:36-44, 2017.
* Fan and Udell (2019) Jicong Fan and Madeleine Udell. Online high rank matrix completion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* Fan et al. (2018) Jicong Fan, Zhaoyang Tian, Mingbo Zhao, and Tommy W.S. Chow. Accelerated low-rank representation for subspace clustering and semi-supervised classification on large-scale data. _Neural Networks_, 100:39-48, 2018.
* Fan et al. (2021) Jicong Fan, Chengrun Yang, and Madeleine Udell. Robust non-linear matrix factorization for dictionary learning, denoising, and clustering. _IEEE Transactions on Signal Processing_, 69:1755-1770, 2021.
* Fan et al. (2022) Jicong Fan, Yiheng Tu, Zhao Zhang, Mingbo Zhao, and Haijun Zhang. A simple approach to automated spectral clustering. In _Advances in Neural Information Processing Systems_, volume 35, pages 9907-9921. Curran Associates, Inc., 2022.
* Fan (2021) Jicong Fan. Large-scale subspace clustering via k-factorization. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, KDD '21, page 342-352, New York, NY, USA, 2021. Association for Computing Machinery.
* Ghosh et al. (2020) Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. _Advances in Neural Information Processing Systems_, 33:19586-19597, 2020.
* He et al. (2020) Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated learning of large cnns at the edge. _Advances in Neural Information Processing Systems_, 33:14068-14080, 2020.
* Hernandez-Pereira et al. (2021) Elena Hernandez-Pereira, Oscar Fontenla-Romero, Bertha Guijarro-Berdinas, and Beatriz Perez-Sanchez. Federated learning approach for spectral clustering. In _ESANN_, 2021.
* Hull (1994) Jonathan J. Hull. A database for handwritten text recognition research. _IEEE Transactions on pattern analysis and machine intelligence_, 16(5):550-554, 1994.
* Kairouz et al. (2015) Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In _International conference on machine learning_, pages 1376-1385. PMLR, 2015.
* Liu et al. (2018)Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Kim et al. (2018) Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. On-device federated learning via blockchain and its latency analysis. _arXiv preprint arXiv:1808.03949_, 2018.
* Laurent and Massart (2000) Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. _Annals of Statistics_, pages 1302-1338, 2000.
* Li et al. (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. _arXiv preprint arXiv:1907.02189_, 2019.
* Li et al. (2020) Li Li, Yuxi Fan, Mike Tse, and Kuo-Yi Lin. A review of applications in federated learning. _Computers & Industrial Engineering_, 149:106854, 2020.
* Li et al. (2020) Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* Li et al. (2021) Zitao Li, Bolin Ding, Ce Zhang, Ninghui Li, and Jingren Zhou. Federated matrix factorization with privacy guarantee. _Proceedings of the VLDB Endowment_, 15(4):900-913, 2021.
* Lian et al. (2017) Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* Nene et al. (1996) SA Nene, SK Nayar, and H Murase. Columbia university image library (coil-20). _Technical Report CUCS-005-96_, 1996.
* Ng et al. (2001) Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In _Advances in Neural Information Processing Systems_, volume 14. MIT Press, 2001.
* Tzinis et al. (2021) Efthymios Tzinis, Jonah Casebeer, Zhepei Wang, and Paris Smaragdis. Separate but together: Unsupervised federated learning for speech enhancement from non-iid data. In _2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)_, pages 46-50. IEEE, 2021.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Luxburg (2007) Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17(4):395-416, 2007.
* Wang and Chang (2022) Shuai Wang and Tsung-Hui Chang. Federated matrix factorization: Algorithm design and application to data clustering. _IEEE Transactions on Signal Processing_, 70:1625-1640, 2022.
* Wang et al. (2020) Hongtao Wang, Ang Li, Bolin Shen, Yuyan Sun, and Hongmei Wang. Federated multi-view spectral clustering. _IEEE Access_, 8:202249-202259, 2020.
* Yang et al. (2018) Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query suggestions. _arXiv preprint arXiv:1812.02903_, 2018.
* Yang et al. (2019) Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 10(2):1-19, 2019.
* Yang et al. (2021) Enyue Yang, Yunfeng Huang, Feng Liang, Weike Pan, and Zhong Ming. Fcmf: Federated collective matrix factorization for heterogeneous collaborative filtering. _Knowledge-Based Systems_, 220:106946, 2021.
* Yang et al. (2019)* Zhang et al. (2020) Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang, and Xiaolin Li. Federated unsupervised representation learning. _arXiv preprint arXiv:2010.08982_, 2020.
* Zhuang et al. (2021) Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual representation learning from decentralized data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4912-4921, 2021.

Framework of the Proposed Algorithm

In the beginning, \(\bm{Z}\) and \(\bm{C}_{p}\) for \(p=1,2,\ldots,P\) are initialized randomly in the central server and clients, respectively. Then, \(\bm{Z}\) is broadcast to every participating client and helps do the alternate updating of \(\bm{Z}_{p}\) and \(\bm{C}_{p}\) based on the update schemes (16) and (14). Afterward, the obtained matrix \(\bm{Z}_{p}\) is sent back to the central server and aggregated for the next round of training. When the tolerance condition is broken, both \(\bm{Z}_{p}^{S}\) and \(\bm{C}_{p}^{S}\) are sent back to the central server for the subsequent clustering task.

```
0: Distributed data \(\{\bm{X}_{p},:\,p\in\mathcal{P}:=\{1,2,\ldots,P\}\}\), clients weights \(\{\omega_{p}:p\in\mathcal{P}\}\).
1: Initialize \(\bm{Z}^{0}\) at server side and \(\{\bm{C}_{p}^{0}\}_{p=1}^{P}\) at client sides.
2: Randomly choose \(\mathcal{A}^{0}\subseteq\mathcal{P}\) with \(|\mathcal{A}^{s}|=\bar{P}\).
3:for round \(s=1\) to \(S\)do
4: Server side: compute \(\bm{Z}^{s-1}=\frac{1}{P}\sum_{p\in\mathcal{A}^{s-1}}\bm{Z}_{p}^{s-1}\).
5: Broadcast \(\bm{Z}^{s-1}\) to clients \(c_{p}\), \(p\in\mathcal{A}^{s}\).
6: Client side:
7:for client \(p=1\) to \(\bar{P}\) in parallel do
8: set \(\bm{Z}_{p}^{s,0}=\bm{Z}^{s-1}\)
9: update local variable \(\bm{C}_{p}^{s}\):
10:\(\bm{C}_{p}^{s}=(\mathcal{K}(\bm{Z}^{s,0},\bm{Z}^{s,0})+\lambda\bm{I}_{d})^{-1} \mathcal{K}(\bm{Z}^{s,0},\bm{X}_{p})\)
11: update local variable \(\bm{Z}_{p}^{s}\):
12:for\(t=1\) to \(Q\)do
13:\(\bm{Z}_{p}^{s,t}=\bm{Z}_{p}^{s,t-1}-\eta_{s}\nabla_{\bm{Z}}f_{p}(\bm{Z}_{p}^{s, t-1})\)
14:endfor
15: denote \(\bm{Z}_{p}^{s}=\bm{Z}_{p}^{s,Q}\)
16:if client \(p\in\mathcal{A}^{s-1}\)then
17: upload \(\bm{Z}_{p}^{s}\) to the server.
18:endif
19:endfor
20: Randomly choose \(\mathcal{A}^{s}\subseteq\mathcal{P}\) with \(|\mathcal{A}^{s}|=\bar{P}\).
21:endfor
22:\(\bm{Z},\bm{C}_{p}\), \(p=1,2,\ldots,P\) ```

**Algorithm 1** Proposed Federated Similarity Reconstruction

## Appendix B More theoretical results about FedSC with perturbed factors

Based on Theorem 3.8 and Theorem 3.6, we can get a bound on the variance of noise for FedSC with perturbed factors:

\[\gamma_{zc}(\sigma_{Z},\sigma_{C})\leq-1+2\sqrt{1+B_{1}}\] (34)

where \(B_{1}=\frac{\epsilon}{2}-\max_{i}\frac{1}{4}\left[\max_{k}(\bm{K}_{ik}^{inter })-\min_{k}(\bm{K}_{ik}^{intra})\right]\) and \(\gamma_{zc}(\sigma_{Z},\sigma_{C})\) is exactly the \(\gamma_{zc}\) in Theorem 3.8 and is clearly a non-decreasing function with respect to \(\sigma_{Z}\) and \(\sigma_{C}\), respectively. Therefore, it is a valid upper bound on both \(\sigma_{Z}\) and \(\sigma_{C}\).

Proof.: By Theorems 3.8 and 3.6, we have

\[\gamma_{zc}(\gamma_{zc}+2)\leq B_{1}\] (35)

That is, we need to solve a quadratic equation \(\gamma_{zc}^{2}+2\gamma_{zc}-B_{1}=0\) with both \(\gamma_{zc}\geq 0\) and \(B_{1}\geq 0\). Since the discriminant \(\Delta=4+4B_{1}\geq 0\), we have two roots \(-1\pm 2\sqrt{1+B_{1}}\) for this equation. Thus, it has \(0\leq\gamma_{zc}\leq-1+2\sqrt{1+B_{1}}\) as desired. 

This bound indicates that the intensity of noise should not be too strong otherwise it may seriously affect the performance of federated spectral clustering. But, at least under this bound, one can choose to inject as much noise as possible into the raw data to ensure data security and privacy.

Based on Theorem 3.9 and (34), we obtain the following privacy-utility trade-off:

\[\begin{cases}\gamma_{zc}(\sigma_{Z},\sigma_{C})\leq-1+2\sqrt{1+B_{1}}\\ \sigma_{Z}\geq\sqrt{(8S\Delta^{2}(g_{Z})\log(e+(\varepsilon_{Z}/\delta_{Z}))/ \varepsilon_{Z}^{2})}\\ \sigma_{C}\geq 2c\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+\Upsilon)/(r^{2} \varepsilon_{C})\end{cases}\] (36)

This ensures both clustering performance and \((\varepsilon,\delta)\)-differential privacy. In particular, if we increase the intensity of either \(\sigma_{Z}\) or \(\sigma_{C}\) to reach the upper bound of \(\gamma_{zc}\), we can get a strong level of privacy but the worst utility. By the way, \(B_{1}\) is related to the property of the data. A larger \(B_{1}\) means a better property for clustering, which further provides a larger upper bound for the noise level, yielding a stronger privacy guarantee.

## Appendix C More discussion on the privacy-utility trade-off of FedSC

Although theoretical results are presented in our study to ensure the security of FedSC, we still provide here some insight into methods of using Secure Aggregation or other cryptographic techniques to handle pair-wise client functions (_i.e._, kernel function in our study). Even though the communication cost will be high, it might be an alternative to reduce the DP noise levels. Specifically, it can be performed as follows.

* Step 1: \(\bm{Z}_{p}^{S}\) is posted to the central server;
* Step 2: Central server aggregates \(\bm{Z}_{p}^{S}\) to get the global \(\bm{Z}^{S}\);
* Step 3: Central server computes \(\mathcal{K}_{ZZ}=\mathcal{K}(\bm{Z}^{S},\bm{Z}^{S})\) and broadcast it to clients;
* Step 4: For client \(p\) and \(\bm{c}_{i}\in\bm{C}_{p}\), if \(\bm{c}_{j}\in\bm{C}_{p}\), then client \(p\) directly calculate \(\hat{\mathcal{K}}_{ij}=\bm{c}_{i}\mathcal{K}_{ZZ}\bm{c}_{j}\); if \(\bm{c}_{j}\in\bm{C}_{p^{\prime}}\), then client \(p\) firstly encrypts and transfers its \(\bm{c}_{i}\) to client \(p^{\prime}\), and then client \(p^{\prime}\) also encrypts its \(\bm{c}_{j}\) and use the cipher text to compute \(enc(\hat{\mathcal{K}}_{ij})=enc(\bm{c}_{i}^{T})enc(\mathcal{K}_{ZZ})enc(\bm{c} _{j})\); Client \(p^{\prime}\) transfers the result \(enc(\hat{\mathcal{K}}_{ij})\) back to client \(p\); Client \(p\) decrypts \(enc(\hat{\mathcal{K}}_{ij})\) to get \(\hat{\mathcal{K}}_{ij}\).
* Step 5: Each client \(p\) sends its estimated results \(\hat{\mathcal{K}}_{ij}\) back to the central server without sending its own \(\bm{C}_{p}\);
* Step 6: Central Server checks whether these posted results from clients are compatible with each other in case of injection attacks and then performs spectral clustering.

This alternative does not send clients' \(\bm{C}\) to the central server and gives an extra cross-validation process in the central server which may be useful to enhance security.

## Appendix D More details and results of the experiments

It should be pointed out that in Table 2 of the main paper, the signal-noise ratio is as high as 12dB, which means the noise is tiny. The parameter \(d\) in our FedSC was automatically determined and has a much larger value than that in the noiseless case. That is why the performance of FedSC in the noisy case is even better than that in the noiseless case of some datasets. In this appendix, we increase the noise level (\(\sigma_{e}=\beta\sigma\), where \(\sigma\) denotes the standard deviation of the clean data) and consider one more real dataset.

### Dataset description

Synthetic dataThe synthetic data is generated from concentric circles. For \(\theta_{i}\in[0,2\pi],\ i=1,2,\ldots,1258\), all the points of this synthetic dataset \(\bm{X}\in\mathbb{R}^{2\times 1258}\) are generated by

\[\bm{x}_{i}=(x_{i1},x_{i2}):\begin{cases}x_{i1}=r\cos(\theta_{i})+e_{i1}\\ x_{i2}=r\sin(\theta_{i})+e_{i2}\end{cases}\] (37)

where \(\theta_{0}=0\), \(\theta_{1258}=2\pi\), and the remaining \(\theta_{i}\) are evenly spaced points between \(\theta_{0}\) and \(\theta_{1258}\). The additive noise \(e_{i1}\) and \(e_{i2}\) are drawn from \(\mathcal{N}(0,\sigma_{e}^{2})\). We let \(\sigma_{e}=0.1\sigma_{x}\), where \(\sigma_{x}\) denotes the standard deviation of the data without the additive noise \(\bm{e}\). In our experiment, we set the hyperparameter \(r\) in (37) to \(2\) and \(4\).

Real-world dataBoth iris and banknote authentication are from the UCI machine learning library. COIL20 is an image dataset from Columbia Imaging and Vision Laboratory. USPS is a dataset for handwritten text recognition research. The details of the mentioned datasets are shown in Table 2.

The visualizations (by t-SNE (Van der Maaten and Hinton, 2008)) of three real-world datasets are shown in Figure 5.

### Evaluation metrics

We use two metrics to evaluate the clustering performance of our method: accuracy and normalized mutual information (NMI). Between them, accuracy is affected by the misclassification rate of cluster assignment. The smaller the misclassification rate, the greater the accuracy. In this study, given data matrix \(\bm{X}\in\mathbb{R}^{m\times n}\) with \(n\) sample points of \(m\) features, let \(L_{i}\) and \(Lr_{i}\) for \(i=1,2,\ldots,n\) be the true labels and predictive labels, respectively, the accuracy of predictive cluster assignment \(Lr\) is computed by

\[\text{accuracy}=\frac{1}{n}\text{card}(\{i|L_{i}=Lr_{i},\;i=1,\ldots,n\}).\] (38)

NMI is a normalized version of mutual information that measures the agreement of two cluster assignments without considering their permutations.

\[\text{NMI}=\frac{2I(L;Lr)}{H(L)+H(Lr)}\] (39)

\begin{table}
\begin{tabular}{l|c|c|c} \hline  & \# of clusters & \# of attributes & \# of instances \\ \hline Iris & 3 & 4 & 150 \\ COIL20 & 20 & 20\(\times\)20 & 1440 \\ Bank & 2 & 5 & 1372 \\ USPS & 10 & 16\(\times\)16 & 9298 \\ ORL & 40 & 92\(\times\)112 & 400 \\ \hline \end{tabular}
\end{table}
Table 2: Summary of four real-world datasets

Figure 4: Synthetic dataset of \(2\) concentric circles: (a) ground truth; (b) noisy data perturbed by Gaussian noise with mean zero and standard deviation \(0.2\text{std}(\bm{X})\).

Figure 5: t-SNE visualization of some real-world datasets

where \(I(L;Lr)\) is the mutual information between \(L\) and \(Lr\), and \(H(L)\) and \(H(Lr)\) are the entropy of \(L\) and \(Lr\), respectively.

### Parameter settings

In our experiments, we set some hyperparameters including \(\lambda_{C},\ d,\ r\), and \(k\) for implementing the proposed FedSC. Among them, \(\lambda_{C}\) as the penalty parameter of the regularization term is set to \(1e-2\). \(k\) is the hyperparameter of the KNN-based operation on the similarity matrix. We set \(k\) to \(\max(\text{ceil}(\log(n)),1)\) based on [20]. The details of methods to set the remaining two parameters are as follows.

Setting of hyperparameter \(r\)The hyperparameter \(r\) controls the smoothness of Gaussian kernel function. Due to the distinct characteristics of the datasets, we adopt the following adaptive method to determine the value of \(r\):

\[r=c*\mathbf{Mean}(\mathbf{Re}(\sqrt{\bm{D}}))\] (40)

where \(c=1\) and \(\bm{D}\) is the distance matrix of data points, of which its element

\[\bm{D}_{ij}=\|\bm{x}_{i}-\bm{x}_{j}\|_{2}^{2},\quad\text{ for }i,j=1,2,\dots,n.\] (41)

In reality, the global \(\bm{D}\) cannot be obtained because FedSC does not allow data transmission. Then we compute \(\bm{D}_{p}\) for each client \(c_{p}\), \(p=1,\dots,P\) and then determine \(r\) as follows

\[r=\frac{c}{P}\sum_{p=1}^{P}\mathbf{Mean}(\mathbf{Re}(\sqrt{\bm{D}_{p}})),\] (42)

where \(c\) can be tuned.

Setting of hyperparameter \(d\)The hyperparameter \(d\) controls the complexity of the approximation \(\phi(\bm{X})\simeq\phi(\bm{Z})\bm{C}\). We adaptively determine the value of \(d\) for each dataset:

\[d=\inf_{k}\left\{k\in\mathbb{N}|\sum_{i=1}^{k}s_{i}\geq\text{tol for }k\leq n\right\}\] (43)

where \(s_{i}\) denotes the \(i\)-th largest eigenvalue of the kernel matrix \(\bm{K}_{xx}\), \(i=1,\dots,n\). In our experiment, we set the threshold tol to \(0.99\).

It is worth noting that we use the same \(r\) in the vanilla SC and our FedSC for a fair comparison, though \(\bm{D}\) cannot be obtained in our FedSC. In addition, the setting of \(d\) relies on \(\bm{K}_{xx}\) that is also not obtainable in our FedSC. We use this setting for convenience and in reality we need to determine \(d\) by other methods such as letting \(d=\gamma m\), where \(\gamma\) is a hyperparameter.

### Clustering results

**NMI results** The average results of 10 repeated trials are reported in Table 1 (ACC) and Table 3 (NMI). Note that for the USPS dataset, only 5 trials are performed to save time due to its large number of sample points. We see that our FedSC outperformed Kmeans and DSC significantly in almost all cases and has at least comparable performance as SC in most cases.

**Influence of \(d\)** The clustering accuracies with different \(d\) are reported in Table 4.

**Results of FedSC with perturbed factors** The results on COIL20 are reported in Table 5, where \(\sigma_{Z}=\alpha_{z}\text{std}(\bm{Z}_{p})\) and \(\sigma_{C}=\alpha_{c}\text{std}(\bm{C})\). Through this experiment, it can be observed that perturbing factors can have a more significant impact on the accuracy of clustering results than perturbing raw data. That is, perturbing factors are more sensitive and can achieve a specified level of differential privacy with weaker noise. Furthermore, it can be seen from Table 5 that the clustering performance is more sensitive to \(\sigma_{C}\) than \(\sigma_{Z}\).

**Malicious attack on \(\bm{C}\)** Due to the kernel trick, the optimization problem is nonlinear and nonconvex. Hence, it is very difficult for potential attackers to recover the data \(\bm{X}\) from the uploaded factors \(\bm{Z},\bm{C}\), especially when \(\bm{X}\) or \(\bm{Z},\bm{C}\) are perturbed by noise. Nevertheless, the attacker may perform K-means on the \(\{\bm{C}_{p}\}_{p=1}^{P}\) to obtain clustering results. However, we find that the clustering accuracy on \(\bm{C}\) is lower than those of Kmeans, SC, and FedSC reported in Table 1. For instance, the clustering accuracy on Iris is reported in Table.

**Influence of \(P\)** Table 7 compares the clustering performance between using a single client (\(P=1\)) and using multiple ones (P = 8). It is clear that the operation of splitting data across multiple clients may lead to an accuracy loss of clustering, which implies that our method is valid.

**More results on MNIST and CIFAR10** We have already included datasets of high-dimensional images in Table 1 like USPS and COIL20 with sizes \(16\times 16\) and \(20\times 20\), respectively. Nevertheless, to further improve the experiment, we added the results of MNIST (\(28\times 28\)) and CIFAR10(\(32\times 32\)) in Table 8.

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline  & d & 7K & 8K & 9K & 194 (SVD) & 10K & 11K & 12K \\ \hline \multirow{4}{*}{Clean data} & Trial 1 & 0.7806 & 0.7882 & 0.7951 & 0.7944 & 0.7778 & 0.7979 & 0.7694 \\  & Trial 2 & 0.8035 & 0.7431 & 0.7819 & 0.8007 & 0.7812 & 0.8049 & 0.7792 \\  & Trial 3 & 0.7562 & 0.7896 & 0.7569 & 0.8097 & 0.7708 & 0.7569 & 0.7764 \\  & Trial 4 & 0.7444 & 0.7771 & 0.7771 & 0.7889 & 0.7854 & 0.7903 & 0.7958 \\  & Trial 5 & 0.7965 & 0.7361 & 0.8014 & 0.7833 & 0.7632 & 0.7264 & 0.7694 \\ \hline \multirow{4}{*}{Noisy data} & Mean & 0.7762 & 0.7668 & 0.7825 & 0.7954 & 0.7757 & 0.7753 & 0.7781 \\ \cline{2-8}  & d & 10K & 20K & 30K & 749 (SVD) & 40K & 50K & 60K \\ \hline \multirow{4}{*}{Noisy data} & Trial 1 & 0.7708 & 0.7722 & 0.7764 & 0.8201 & 0.7000 & 0.7306 & 0.8139 \\  & Trial 2 & 0.7743 & 0.7889 & 0.7646 & 0.7882 & 0.7694 & 0.7451 & 0.7299 \\ \cline{1-1}  & Trial 3 & 0.7375 & 0.7743 & 0.7167 & 0.7493 & 0.7792 & 0.7965 & 0.7299 \\ \cline{1-1}  & Trial 4 & 0.7965 & 0.7688 & 0.8014 & 0.7736 & 0.7903 & 0.7382 & 0.7674 \\ \cline{1-1}  & Trial 5 & 0.7493 & 0.7507 & 0.7576 & 0.7000 & 0.7792 & 0.8076 & 0.8194 \\ \hline \multirow{4}{*}{Noisy data} & Mean & 0.7657 & 0.7710 & 0.7633 & 0.7662 & 0.7636 & 0.7636 & 0.7721 \\ \hline \end{tabular}
\end{table}
Table 4: Accuracy of the proposed algorithm on COIL20 (K = 20)

\begin{table}
\begin{tabular}{c|c|c c c c} \hline  & & \multicolumn{4}{c}{NMI} \\ \hline  & & Kmeans & SC & DSC & FedSC \\ \hline \multirow{4}{*}{\(\bm{X}_{0}\)} & Iris & \(0.6356\pm 0.0000\) & \(0.6647\pm 0.0000\) & \(0.2516\pm 0.0707\) & \(0.6708\pm 0.0139\) \\  & COIL20 & \(0.7658\pm 0.0143\) & \(0.8809\pm 0.0006\) & \(0.1259\pm 0.0225\) & \(0.8613\pm 0.0144\) \\  & Bank & \(0.1480\pm 0.0000\) & \(0.1228\pm 0.0000\) & \(0.0122\pm 0.0160\) & \(0.4901\pm 0.3050\) \\  & USPS & \(0.6125\pm 0.0026\) & \(0.8083\pm 0.0000\) & \(0.0064\pm 0.0016\) & \(0.7972\pm 0.0121\) \\  & ORL & \(0.8343\pm 0.0136\) & \(0.8980\pm 0.0045\) & \(0.3970\pm 0.0045\) & \(0.8525\pm 0.0097\) \\ \hline \multirow{4}{*}{\(\bm{X}_{n}\) with \(0.1\sigma\)} & Iris & \(0.6267\pm 0.0305\) & \(0.6878\pm 0.0764\) & \(0.1458\pm 0.0794\) & \(0.6666\pm 0.0600\) \\  & COIL20 & \(0.7721\pm 0.0122\) & \(0.8805\pm 0.0012\) & \(0.1214\pm 0.0254\) & \(0.8529\pm 0.0209\) \\  & Bank & \(0.1383\pm 0.0039\) & \(0.1314\pm 0.2010\) & \(0.0039\pm 0.0081\) & \(0.3853\pm 0.2390\) \\  & USPS & \(0.6147\pm 0.0022\) & \(0.8097\pm 0.0166\) & \(0.0076\pm 0.0010\) & \(0.7920\pm 0.0133\) \\  & ORL & \(0.8280\pm 0.0153\) & \(0.8948\pm 0.0019\) & \(0.3944\pm 0.0074\) & \(0.8554\pm 0.0104\) \\ \hline \multirow{4}{*}{\(\bm{X}_{n}\) with \(0.3\sigma\)} & Iris & \(0.5292\pm 0.0528\) & \(0.5072\pm 0.0442\) & \(0.1403\pm 0.0792\) & \(0.5385\pm 0.0594\) \\  & COIL20 & \(0.7745\pm 0.0185\) & \(0.8767\pm 0.0014\) & \(0.1174\pm 0.0224\) & \(0.8477\pm 0.0188\) \\  & Bank & \(0.1327\pm 0.0062\) & \(0.1031\pm 0.0211\) & \(0.0135\pm 0.0253\) & \(0.1489\pm 0.0151\) \\  & USPS & \(0.6112\pm 0.0052\) & \(0.7927\pm 0.0123\) & \(0.0070\pm 0.0016\) & \(0.7837\pm 0.0091\) \\  & ORL & \(0.8136\pm 0.0067\) & \(0.8974\pm 0.0047\) & \(0.3987\pm 0.0069\) & \(0.8578\pm 0.0147\) \\ \hline \multirow{4}{*}{\(\bm{X}_{n}\) with \(0.5\sigma\)} & Iris & \(0.4316\pm 0.0310\) & \(0.3912\pm 0.0426\) & \(0.0710\pm 0.0421\) & \(0.4046\pm 0.0388\) \\  & COIL20 & \(0.7676\pm 0.0148\) & \(0.8721\pm 0.0061\) & \(0.1238\pm 0.0286\) & \(0.8293\pm 0.0168\) \\  & Bank & \(0.1408\pm 0.0133\) & \(0.1189\pm 0.0175\) & \(0.0059\pm 0.010\) & \(0.1521\pm 0.0113\) \\  & USPS & \(0.6088\pm 0.0036\) & \(0.8049\pm 0.0164\) & \(0.0059\pm 0.0028\) & \(0.7951\pm 0.0127\) \\  & ORL & \(0.8058\pm 0.0150\) & \(0.8970\pm 0.0063\) & \(0.3977\pm 0.0095\) & \(0.8381\pm 0.0133\) \\ \hline \multirow{4}{*}{\(\bm{X}_{n}\) with \(0.7\sigma\)} & Iris & \(0.3025\pm 0.0407\) & \(0.2755\pm 0.0381\) & \(0.0715\pm 0.0459\) & \(0.2773\pm 0.0408\) \\  & COIL20

## Appendix E Proof for theorem on the convergence of FedSC algorithm

We derive the objective descent with respect to \(\bm{C}^{s}\) and \(\bm{Z}^{s,t}\), respectively.

Objective descent with w.r.t. \(\bm{C}\):Based on the update scheme (14) of \(\bm{C}\), we have

\[\nabla_{C}f_{p}(\bm{Z}^{s,0},\bm{C}_{p}^{s})=(\mathcal{K}(\bm{Z}^{s,0},\bm{Z}^{ s,0})+\lambda\bm{I}_{d})\bm{C}_{p}^{s}-\mathcal{K}(\bm{Z}^{s,0},\bm{X}_{p})=0\] (44)

where \(\bm{Z}^{s,0}=\bm{Z}^{s-1}=\frac{1}{P}\sum_{p\in\mathcal{A}^{s-1}}\bm{Z}_{p}^{s -1,Q}\).

According to the proposed FedSC problem 7, we have

\[\begin{split}& f_{p}(\bm{Z}^{s,0},\bm{C}_{p}^{s})-f_{p}(\bm{Z}^{s, 0},\bm{C}_{p}^{s-1})\\ =&\left[\frac{1}{2}\left\|\phi(\bm{X}_{p})-\phi(\bm{ Z}^{s,0})\bm{C}_{p}^{s}\right\|_{F}^{2}+\frac{\lambda}{2}\left\|\bm{C}_{p}^{s} \right\|_{F}^{2}\right]\\ &\qquad-\left[\frac{1}{2}\left\|\phi(\bm{X}_{p})-\phi(\bm{Z}^{s,0})\bm{C}_{p}^{s-1}\right\|_{F}^{2}+\frac{\lambda}{2}\left\|\bm{C}_{p}^{s-1} \right\|_{F}^{2}\right]\\ =&-\text{Tr}((\bm{C}_{p}^{s}-\bm{C}_{p}^{s-1})^{T} \mathcal{K}(\bm{Z}^{s,0},\bm{X}_{p}))+\frac{\lambda}{2}\text{Tr}(\bm{C}_{p}^{ s}(\bm{C}_{p}^{s})^{T}-\bm{C}_{p}^{s-1}(\bm{C}_{p}^{s-1})^{T})\\ &\qquad+\frac{1}{2}\text{Tr}(\left[\bm{C}_{p}^{s}(\bm{C}_{p}^{s })^{T}-\bm{C}_{p}^{s-1}(\bm{C}_{p}^{s-1})^{T}\right]\mathcal{K}(\bm{Z}^{s,0}, \bm{Z}^{s,0}))\\ =&-\text{Tr}((\bm{C}_{p}^{s}-\bm{C}_{p}^{s-1})^{T}( \mathcal{K}(\bm{Z}^{s,0},\bm{Z}^{s,0})+\lambda\bm{I}_{d})\bm{C}_{p}^{s})\\ &\qquad+\frac{1}{2}\text{Tr}(\left[\bm{C}_{p}^{s}(\bm{C}_{p}^{s })^{T}-\bm{C}_{p}^{s-1}(\bm{C}_{p}^{s-1})^{T}\right]\left[\mathcal{K}(\bm{Z}^{ s,0},\bm{Z}^{s,0})+\lambda\bm{I}_{d}\right])\\ =&-\frac{1}{2}\text{Tr}([\bm{C}_{p}^{s}-\bm{C}_{p}^{ s-1}]^{T}[\mathcal{K}(\bm{Z}^{s,0},\bm{Z}^{s,0})+\lambda\bm{I}_{d}][\bm{C}_{p}^{s }-\bm{C}_{p}^{s-1}])\\ \leq&-\frac{1}{2}(\gamma_{min}^{s}+\lambda)\left\| \bm{C}_{p}^{s}-\bm{C}_{p}^{s-1}\right\|_{F}^{2}\end{split}\] (45)

where \(\gamma_{min}^{s}=\gamma_{min}(\mathcal{K}(\bm{Z}^{s,0},\bm{Z}^{s,0}))\).

Summing it up from \(p=1\) to \(P\), we have

\[F(\bm{Z}^{s,0},\bm{C}^{s})-F(\bm{Z}^{s,0},\bm{C}^{s-1})\leq-\frac{1}{2}(\gamma _{min}^{s}+\lambda)\sum_{p=1}^{P}\omega_{p}\left\|\bm{C}_{p}^{s}-\bm{C}_{p}^{s -1}\right\|_{F}^{2}\] (46)

Objective descent with w.r.t. \(\bm{Z}\):According to Assumption 2.1, it implies

\[\begin{split} F(\bm{Z}^{s,t},\bm{C}^{s})-F(\bm{Z}^{s,t-1},\bm{C}^ {s})&\leq\underbrace{\langle\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^ {s}),\bm{Z}^{s,t}-\bm{Z}^{s,t-1}\rangle}_{\text{T.2}}\\ &\quad+\frac{L_{Z}^{s}}{2}\left\|\bm{Z}^{s,t}-\bm{Z}^{s,t-1} \right\|_{F}^{2}\end{split}\] (47)

\begin{table}
\begin{tabular}{c|c|c c|c c|c} \hline  & \multicolumn{2}{c}{Kmeans} & \multicolumn{2}{c}{SC} & \multicolumn{2}{c}{DSC} & \multicolumn{2}{c}{FedSC} & \multicolumn{2}{c}{Attack on \(\bm{C}\)} \\ \hline Iris & \(0.8920\pm 0.0028\) & \(0.9000\pm 0.0000\) & \(0.5493\pm 0.1263\) & \(0.9027\pm 0.0064\) & \(0.6360\pm 0.1557\) \\ \hline \end{tabular}
\end{table}
Table 6: Clustering accuracy among different ways

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline  & \multicolumn{2}{c}{Kmeans} & \multicolumn{2}{c}{SC} & \multicolumn{2}{c}{DSC} & \multicolumn{2}{c}{FedSC} & \multicolumn{2}{c}{Attack on \(\bm{C}\)} \\ \hline Iris & \(0.8920\pm 0.0028\) & \(0.9000\pm 0.0000\) & \(0.5493\pm 0.1263\) & \(0.9027\pm 0.0064\) & \(0.6360\pm 0.1557\) \\ \hline \end{tabular}
\end{table}
Table 5: Clustering accuracy (average over 10 trials) of FedSC with perturbed factors on COIL20.

[MISSING_PAGE_FAIL:20]

where we used the fact that \(\left\langle\bm{x},\bm{y}\right\rangle\leq\left\|\bm{x}\right\|_{2}^{2}+\left\|\bm {y}\right\|_{2}^{2},\forall c>0\).

Now, we give a bound on T.3.

**Lemma E.2**.: _For any \(s\) and \(t\), it holds that_

\[\begin{split}&\left\|\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^{s})- \frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1}, \bm{C}_{p}^{s})\right\|_{F}^{2}\\ \leq&\frac{16\zeta^{2}}{\bar{P}}+2(1+\frac{8}{\bar{P }})\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^ {s,t-1}\right\|_{F}^{2}\end{split}\] (52)

Proof.: For any \(s\) and \(t\), it holds that

\[\begin{split}&\left\|\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^{s})- \frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1}, \bm{C}_{p}^{s})\right\|_{F}^{2}\\ =&\left\|\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^{s})- \sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s}) \right.\\ &\qquad+\left.\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^ {s,t-1},\bm{C}_{p}^{s})-\frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z} f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}^{2}\\ \leq&\underbrace{\left\|\nabla_{Z}F(\bm{Z}^{s,t-1}, \bm{C}^{s})-\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_ {p}^{s})\right\|_{F}^{2}}_{\text{T.5}}\\ &\qquad+2\underbrace{\left\|\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{ p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})-\frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}} \nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}^{2}}_{\text{T.5 }}\end{split}\] (53)

where we used the fact that \(\left\|\sum_{i=1}^{n}a_{i}\right\|_{2}^{2}\leq n\sum_{i=1}^{n}\left\|a_{i} \right\|_{2}^{2}\).

Firstly, we give a bound on T.4:

\[\begin{split}&\left\|\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^{s})- \sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s}) \right\|_{F}^{2}\\ =&\left\|\sum_{p=1}^{P}\omega_{p}\left[\nabla_{Z}f( \bm{Z}^{s,t-1},\bm{C}_{p}^{s})-\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s })\right]\right\|_{F}^{2}\\ \leq&\sum_{p=1}^{P}\omega_{p}\left\|\nabla_{Z}F(\bm{ Z}^{s,t-1},\bm{C}_{p}^{s})-\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s })\right\|_{F}^{2}\\ \leq&\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2} \left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\end{split}\] (54)

where we used the fact that \(\nabla_{Z}f_{p}(\cdot,\bm{C}_{p}^{s})\) is \(L_{Z_{p}}^{s}\)-Lipschitz continuous.

Secondly, we give a bound on T.5:

\[\left\|\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1}, \bm{C}_{p}^{s})-\frac{1}{\bar{P}}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z} _{p}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}^{2}\] \[= \frac{1}{\bar{P}^{2}}\left\|\sum_{p^{\prime}\in\mathcal{A}^{s}} \left[\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s} )-\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,t-1},\bm{C}_{p^{\prime}}^{s}) \right]\right\|_{F}^{2}\] \[\leq \frac{1}{\bar{P}}\sum_{p^{\prime}\in\mathcal{A}^{s}}\left\|\sum_{ p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})-\nabla_{Z}f_{p^{ \prime}}(\bm{Z}_{p^{\prime}}^{s,t-1},\bm{C}_{p^{\prime}}^{s})\right\|_{F}^{2}\] (55) \[= \frac{1}{\bar{P}}\sum_{p^{\prime}=1}^{P}\omega_{p^{\prime}} \left\|\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{ s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,t-1},\bm{C}_{p^{\prime}}^{s}) \right\|_{F}^{2}\] \[\leq \frac{1}{\bar{P}}\sum_{p^{\prime}=1}^{P}\omega_{p^{\prime}} \sum_{p=1}^{P}\omega_{p}\underbrace{\left\|\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1}, \bm{C}_{p}^{s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,t-1},\bm{C}_{p ^{\prime}}^{s})\right\|_{F}^{2}}_{\text{T.6}}\]

Thirdly, we give a bound on T.6:

\[\left\|\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})-\nabla _{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,t-1},\bm{C}_{p^{\prime}}^{s})\right\| _{F}^{2}\] \[= \left\|\left[\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})- \nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s})+\nabla_{Z}f_{p}(\bm{Z}^{s,t-1}, \bm{C}_{p}^{s})\right.\right.\] \[\left.\left.-\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,t-1}, \bm{C}_{p^{\prime}}^{s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{ \prime}}^{s})+\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}}^{s})\right.\right.\] \[\left.\left.\left.-\nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}^{s})+ \nabla_{Z}f(\bm{Z}^{s,t-1},\bm{C}^{s})\right]\right\|_{F}^{2}\] \[= \left\|\left[\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})- \nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s})\right]\right.\] \[\left.\left.+\left[\nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s})- \nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}}^{s})\right]\right.\right.\] (56) \[\left.\left.+\left[\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C} _{p^{\prime}}^{s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}} ^{s})\right]\right.\right.\] \[\left.\left.+\left[\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C} _{p^{\prime}}^{s})-\nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}}^{s}) \right]\right.\right]\right.\] \[\leq 4\left\|\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})- \nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}^{2}\] \[\left.\left.+4\left\|\nabla_{Z}f_{p}(\bm{Z}^{s,t-1},\bm{C}_{p}^{s} )-\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}}^{s})\right\|_{F}^ {2}\right.\right.\] \[\left.\left.+4\left\|\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C }_{p^{\prime}}^{s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}} ^{s})\right\|_{F}^{2}\right.\right.\] \[\left.\left.+4\left\|\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C }_{p^{\prime}}^{s})-\nabla_{Z}f_{p^{\prime}}(\bm{Z}^{s,t-1},\bm{C}_{p^{\prime}} ^{s})\right\|_{F}^{2}\right.\right.\] \[\leq 4(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1} \right\|_{F}^{2}+4(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p^{ \prime}}^{s,t-1}\right\|_{F}^{2}+8\zeta^{2}\]

Here, it is the second time that we use the facts that \(\left\|\sum_{i=1}^{n}a_{i}\right\|_{2}^{2}\leq n\sum_{i=1}^{n}\left\|a_{i} \right\|_{2}^{2}\) and that \(\nabla_{Z}f_{p}(\cdot,\bm{C}_{p}^{s})\) is \(L_{Z_{p}}^{s}\)-Lipschitz continuous.

Based on the bound of T.6, we derive the bound on \(T.5\).

\[\begin{split}&\left\|\sum_{p=1}^{P}\omega_{p}\nabla_{Z}f_{p}(\bm{Z}_{p}^ {s,t-1},\bm{C}_{p}^{s})-\frac{1}{P}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}( \bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})\right\|_{F}^{2}\\ \leq&\frac{1}{P}\sum_{p^{\prime}=1}^{P}\omega_{p^{ \prime}}\sum_{p=1}^{P}\omega_{p}\underbrace{\left[4(L_{Z_{p}}^{s})^{2}\left\| \bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}+4(L_{Z_{p^{\prime}}}^{s})^{ 2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p^{\prime}}^{s,t-1}\right\|_{F}^{2}+8\zeta^{2} \right]}_{Bound\,\,of\,\,T.5}\\ \leq&\frac{8\zeta^{2}}{P}+\frac{8}{P}\sum_{p=1}^{P} \omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_ {F}^{2}\end{split}\] (57)

Fourthly, based on the bounds of \(T.4\) and \(T.5\), we continue to derive the final required bound.

\[\begin{split}&\left\|\nabla_{Z}F(\bm{Z}^{s,t-1},\bm{C}^{s})- \frac{1}{P}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_ {p}^{s})\right\|_{F}^{2}\\ \leq& 2\underbrace{\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s })^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}}_{Bound\,\,of\, \,T.3}+2\underbrace{\left[\frac{8\zeta^{2}}{P}+\frac{8}{P}\sum_{p=1}^{P}\omega _{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2 }\right]}_{Bound\,\,of\,\,T.4}\\ =&\frac{16\zeta^{2}}{P}+2(1+\frac{8}{P})\sum_{p=1}^{ P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2 }\end{split}\] (58)

Based on Lemma 2, we continue to do the following derivation.

\[\begin{split}&\quad F(\bm{Z}^{s,t},\bm{C}^{s})-F(\bm{Z}^{s,t-1}, \bm{C}^{s})\\ \leq&-\frac{L_{Z}^{s}}{4}\left\|\bm{Z}^{s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}\\ &\quad+\frac{1}{L_{Z}^{s}}\underbrace{\left[\frac{16\zeta^{2}}{P} +2(1+\frac{8}{P})\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t -1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\right]}_{Bound\,\,of\,\,T.3}\\ =&-\frac{L_{Z}^{s}}{4}\left\|\bm{Z}^{s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}+\frac{16\zeta^{2}}{\bar{P}L_{Z}^{s}}\\ &\quad+\frac{2}{L_{Z}^{s}}(1+\frac{8}{P})\sum_{p=1}^{P}\omega_{p }(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2} \end{split}\] (59)

Then, summing it up from \(t=1\) to \(Q\) yields

\[\begin{split}&\quad F(\bm{Z}^{s,Q},\bm{C}^{s})-F(\bm{Z}^{s,0},\bm{C}^ {s})\\ \leq&-\frac{L_{Z}^{s}}{4}\sum_{t=1}^{Q}\left\|\bm{Z}^ {s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}+\frac{16Q\zeta^{2}}{\bar{P}L_{Z}^{s}}\\ &\quad+\frac{2}{L_{Z}^{s}}(1+\frac{8}{P})\underbrace{\sum_{t=1}^{ Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1} \right\|_{F}^{2}}_{\text{T.7}}\end{split}\] (60)

Now, we give a bound on T.7.

**Lemma E.3**.: _For any \(s\), it holds that_

\[\begin{split}&\left\|\bm{Z}^{s,t}-\bm{Z}_{p}^{s,t}\right\|_{F}^{2} \\ \leq&\frac{4t}{(L_{Z}^{s})^{2}\bar{P}}\sum_{j=0}^{t-1} (L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+\frac{ 8t^{2}\zeta^{2}}{(L_{Z}^{s})^{2}\bar{P}}\\ &\quad+\frac{4t}{(L_{Z}^{s})^{2}\bar{P}}\sum_{j=0}^{t-1}\sum_{p^ {\prime}=1}^{P}\omega_{p^{\prime}}(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s,j}-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}\end{split}\] (61)

Proof.: Based on the update schemes of \(\bm{C}\) and \(\bm{Z}\), we have

\[\begin{split}&\bm{Z}_{p}^{s,t}=\bm{Z}_{p}^{s,t-1}-\frac{1}{L_{Z}^ {s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,t-1},\bm{C}_{p}^{s})\\ \Longleftrightarrow&\bm{Z}^{s,t}=\bm{Z}^{s,t-1}- \frac{1}{\bar{P}L_{Z}^{s}}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p }^{s,t-1},\bm{C}_{p}^{s})\end{split}\] (62)

Consequently, we have

\[\begin{split}&\bm{Z}_{p}^{s,t}=\bm{Z}_{p}^{s,0}-\frac{1}{L_{Z}^ {s}}\sum_{j=0}^{t-1}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s})\\ \Longleftrightarrow&\bm{Z}^{s,t}=\bm{Z}^{s,0}-\frac{1}{ \bar{P}L_{Z}^{s}}\sum_{j=0}^{t-1}\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm {Z}_{p}^{s,j},\bm{C}_{p}^{s})\end{split}\] (63)

where \(\bm{Z}_{p}^{s,0}=\bm{Z}^{s,0}=\bm{Z}^{s-1}\).

Based on the above identities,

\[\left\|\bm{Z}^{s,t}-\bm{Z}_{p}^{s,t}\right\|_{F}^{2}\] (64) \[= \left\|\left[\bm{Z}^{s,0}-\frac{1}{\bar{P}L_{Z}^{s}}\sum_{j=0}^{t-1 }\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s}) \right]-\left[\bm{Z}_{p}^{s,0}-\frac{1}{L_{Z}^{s}}\sum_{j=0}^{t-1}\nabla_{Z}f_{ p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s})\right]\right\|_{F}^{2}\] \[= \frac{1}{(L_{Z}^{s})^{2}}\left\|\frac{1}{\bar{P}}\sum_{j=0}^{t-1 }\sum_{p\in\mathcal{A}^{s}}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s})- \sum_{j=0}^{t-1}\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s})\right\|_{F}^{2}\] \[\leq \frac{t}{(L_{Z}^{s})^{2}}\sum_{j=0}^{t-1}\sum_{p^{\prime}\in \mathcal{A}^{s}}\left\|\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,j},\bm {C}_{p^{\prime}}^{s})-\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s})\right\| _{F}^{2}\] \[= \frac{t}{(L_{Z}^{s})^{2}}\bar{P}^{2}\sum_{j=0}^{t-1}\left\|\sum_{ p^{\prime}\in\mathcal{A}^{s}}\left[\nabla_{Z}f_{p^{\prime}}(\bm{Z}_{p^{\prime}}^{s,j}, \bm{C}_{p^{\prime}}^{s})-\nabla_{Z}f_{p}(\bm{Z}_{p}^{s,j},\bm{C}_{p}^{s}) \right]\right\|_{F}^{2}\] \[\leq \frac{t}{(L_{Z}^{s})^{2}}\bar{P}\sum_{j=0}^{t-1}\sum_{p^{\prime} =1}^{P}\omega_{p^{\prime}}\underbrace{\left[4(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^ {s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+4(L_{Z_{p^{\prime}}}^{s})^{2}\left\| \bm{Z}^{s,j}-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}+8\zeta^{2}\right]}_{Bound \ of\ T.6}\] \[= \frac{4t}{(L_{Z}^{s})^{2}}\bar{P}\sum_{j=0}^{t-1}(L_{Z_{p}}^{s})^ {2}\left\|\bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+\frac{8t^{2}\zeta^{2} }{(L_{Z}^{s})^{2}m}\] \[\quad+\frac{4t}{(L_{Z}^{s})^{2}}\bar{P}\sum_{j=0}^{t-1}\sum_{p^{ \prime}=1}^{P}\omega_{p^{\prime}}(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s, j}-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}\]

Based on Lemma 3, we give a bound on T.7.

**Lemma E.4**.: _For any \(s\), it holds that_

\[\sum_{t=1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1 }-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\leq\frac{8Q(Q-1)(2Q-1)\zeta^{2}}{\bar{P}- 4(Q-1)^{2}(1+\overline{L}_{Z}^{2}/L_{Z}^{2})}\] (65)Proof.: Based on Lemma 3, we have

\[\sum_{t=1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z }^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\] \[\leq \sum_{t=1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left[ \frac{4(t-1)}{(L_{Z}^{s})^{2}\bar{P}}\sum_{j=0}^{t-2}(L_{Z_{p}}^{s})^{2}\right\| \bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\left\|_{F}^{2}+\frac{8(t-1)^{2}\zeta^{2}}{(L_{Z}^ {s})^{2}\bar{P}}\right.\] \[\left.+\frac{4(t-1)}{(L_{Z}^{s})^{2}\bar{P}}\sum_{j=0}^{t-2}\sum_{ p^{\prime}=1}^{P}\omega_{p^{\prime}}(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s,j }-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}\right]\] \[= \sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}\sum_{p=1}^{P}\omega_{p}(L_{Z _{p}}^{s})^{2}\left(\frac{L_{Z_{p}}^{s}}{L_{Z}^{s}}\right)^{2}\sum_{j=0}^{t-2} \left\|\bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+\sum_{t=1}^{Q}\frac{8(t-1) ^{2}\zeta^{2}}{\bar{P}}\] \[\left.+\sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}\sum_{j=0}^{t-2}\sum_{ p^{\prime}=1}^{P}\omega_{p^{\prime}}(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s,j }-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}\right.\] (66) \[\leq \sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}\sum_{p=1}^{P}\omega_{p}(L_{Z _{p}}^{s})^{2}\left(\frac{\bar{L}_{Z}}{\bar{L}_{Z}}\right)^{2}\sum_{j=0}^{t-2} \left\|\bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+\sum_{t=1}^{Q}\frac{8(t-1) ^{2}\zeta^{2}}{\bar{P}}\] \[\left.+\sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}\sum_{j=0}^{t-2}\sum_{ p^{\prime}=1}^{P}\omega_{p^{\prime}}(L_{Z_{p^{\prime}}}^{s})^{2}\left\|\bm{Z}^{s,j }-\bm{Z}_{p^{\prime}}^{s,j}\right\|_{F}^{2}\right.\] \[= \sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}(1+\frac{\overline{L}_{Z}^{2} }{\underline{L}_{Z}^{2}})\sum_{j=0}^{t-2}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s })^{2}\left\|\bm{Z}^{s,j}-\bm{Z}_{p}^{s,j}\right\|_{F}^{2}+\frac{8Q(Q-1)(2Q-1) \zeta^{2}}{\bar{P}}\] \[\leq \frac{4(Q-1)^{2}}{\bar{P}}(1+\frac{\overline{L}_{Z}^{2}}{ \underline{L}_{Z}^{2}})\sum_{t=1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{ 2}\left\|\bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}+\frac{8Q(Q-1)(2Q-1) \zeta^{2}}{\bar{P}}\]

Here, we used the inequality as

\[\sum_{t=1}^{Q}\frac{4(t-1)}{\bar{P}}\sum_{j=0}^{t-2}\left\|\bm{Z}^{s,j}-\bm{Z} _{p}^{s,j}\right\|_{F}^{2}\leq\frac{4(Q-1)^{2}}{\bar{P}}\sum_{t=1}^{Q}\left\| \bm{Z}^{s,t-1}-\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\] (67)

Rearranging the above inequality, we have

\[\sum_{t=1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1} -\bm{Z}_{p}^{s,t-1}\right\|_{F}^{2}\leq\frac{8Q(Q-1)(2Q-1)\zeta^{2}}{\bar{P}-4 (Q-1)^{2}(1+\overline{L}_{Z}^{2}/\underline{L}_{Z}^{2})}\] (68)Based on Lemma 4, we continue to do the following derivation:

\[\begin{split}& F(\bm{Z}^{s,Q},\bm{C}^{s})-F(\bm{Z}^{s,0},\bm{C}^{s}) \\ \leq&-\frac{L_{Z}^{s}}{4}\sum_{t=1}^{Q}\left\|\bm{Z}^{s, t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}+\frac{16Q\zeta^{2}}{\bar{P}L_{Z}^{s}}\\ &\qquad+\frac{2}{L_{Z}^{s}}(1+\frac{8}{P})\underbrace{\sum_{t= 1}^{Q}\sum_{p=1}^{P}\omega_{p}(L_{Z_{p}}^{s})^{2}\left\|\bm{Z}^{s,t-1}-\bm{Z}^{ s,t-1}_{p}\right\|_{F}^{2}}_{T:\overline{\gamma}}\\ \leq&-\frac{L_{Z}^{s}}{4}\sum_{t=1}^{Q}\left\|\bm{Z}^ {s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}+\frac{16Q\zeta^{2}\psi}{\bar{P}L_{Z}^{s}} \\ &\qquad+\frac{2}{L_{Z}^{s}}(1+\frac{8}{P})\underbrace{\frac{8Q(Q- 1)(2Q-1)\zeta^{2}}{\bar{P}-4(Q-1)^{2}(1+\bar{L}_{Z}^{2}/L_{Z}^{2})}}_{Bound \ of\ T.\overline{\gamma}}\\ \leq&-\frac{L_{Z}^{s}}{4}\sum_{t=1}^{Q}\left\|\bm{Z} ^{s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}+\frac{16Q\zeta^{2}\psi}{\bar{P}L_{Z}^{ s}}\end{split}\] (69)

where \(\psi=1+\frac{(\bar{P}+8)(Q-1)(2Q-1)}{\bar{P}-4(Q-1)^{2}(1+\bar{L}_{Z}^{2}/L_{Z} ^{2})}\).

Then, combining (46) and (69) yields

\[\begin{split}&\frac{1}{2}(\gamma_{min}^{s}+\lambda)\sum_{p=1}^{ P}\omega_{p}\left\|\bm{C}_{p}^{s}-\bm{C}_{p}^{s-1}\right\|_{F}^{2}+\frac{L_{Z}^{ s}}{4}\sum_{t=1}^{Q}\left\|\bm{Z}^{s,t}-\bm{Z}^{s,t-1}\right\|_{F}^{2}\\ \leq&[F(\bm{Z}^{s,0},\bm{C}^{s-1})-F(\bm{Z}^{s,Q}, \bm{C}^{s})]+\frac{16Q\zeta^{2}\psi}{\bar{P}L_{Z}^{s}}\end{split}\] (70)

Derivation of the main result:Based on (70), we derive the convergence in terms of the iterative terms, \(T_{C}(\bm{Z}^{s,0},\bm{C}^{s})\) and \(T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})\) for \(s=1,2,\ldots,S\).

\[T_{C}(\bm{Z}^{s,0},\bm{C}^{s})\leq\frac{2}{\gamma_{min}^{s}+\lambda}[F(\bm{Z}^ {s,0},\bm{C}^{s-1})-F(\bm{Z}^{s,Q},\bm{C}^{s})]+\frac{32Q\zeta^{2}\psi}{(\gamma _{min}^{s}+\lambda)\bar{P}L_{Z}^{s}}\] (71)

Then, summing it up from \(s=1\) to \(S\) yields

\[\sum_{s=1}^{S}T_{C}(\bm{Z}^{s,0},\bm{C}^{s})\leq\frac{2}{\underline{\gamma}_{ min}+\lambda}[F(\bm{Z}^{1,0},\bm{C}^{0})-\underline{f}]+\frac{32SQ\zeta^{2} \psi}{(\underline{\gamma}_{min}+\lambda)\bar{P}L_{Z}}\] (72)

Similarly, we have

\[\sum_{t=1}^{Q}T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})\leq\frac{4}{L_{Z}^{s}}[F(\bm{Z}^{ s,0},\bm{C}^{s-1})-F(\bm{Z}^{s,Q},\bm{C}^{s})]+\frac{64Q\zeta^{2}\psi}{\bar{P}(L_{ Z}^{s})^{2}}\] (73)

Then, summing it up from \(s=1\) to \(S\) yields

\[\sum_{s=1}^{S}\sum_{t=1}^{Q}T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})\leq\frac{4}{L_{Z}}[ F(\bm{Z}^{1,0},\bm{C}^{0})-\underline{f}]+\frac{64SQ\zeta^{2}\psi}{\bar{P}L_{Z}^{ 2}}\] (74)

Lastly, combining (72) and (74) and dividing two sides of it by \(T=S(1+Q)\) yields

\[\frac{1}{T}\left[\sum_{s=1}^{S}T_{C}(\bm{Z}^{s,0},\bm{C}^{s})+\sum_{s=1}^{S} \sum_{t=1}^{Q}T_{Z}(\bm{Z}^{s,t},\bm{C}^{s})\right]\leq\frac{D}{T}[F(\bm{Z}^{1,0},\bm{C}^{0})-\underline{f}]+\frac{16\zeta^{2}\psi D}{\bar{P}L_{Z}}\] (75)

where \(D=\frac{2}{\underline{\gamma}_{min}+\lambda}+\frac{4}{L_{Z}}\).

Proof for theorem on error bound on noisy similarity matrix

Proof.: It follows from the triangle inequality of matrix norm that

\[\begin{split}\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{xx} \right\|_{\infty}&=\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{ \hat{x}\hat{x}}+\bm{K}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{\infty}\\ &\leq\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{\hat{x}\hat{x}} \right\|_{\infty}+\left\|\bm{K}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{\infty}. \end{split}\] (76)

Since \(\tilde{\bm{X}}=\bm{X}+\bm{E}\) and \(\phi(\tilde{\bm{X}})=\phi(\bm{Z})\bm{C}\), we have

\[\begin{cases}\hat{\bm{K}}_{\hat{x}\hat{x}}=(\phi(\bm{Z})\bm{C})^{T}(\phi(\bm{ Z})\bm{C})=\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}\\ \bm{K}_{\hat{x}\hat{x}}=\phi(\tilde{\bm{X}})^{T}\phi(\tilde{\bm{X}})= \mathcal{K}(\tilde{\bm{X}},\tilde{\bm{X}})\\ \bm{K}_{xx}=\phi(\bm{X})^{T}\phi(\bm{X})=\mathcal{K}(\bm{X},\bm{X})\end{cases}\] (77)

where \(\bm{E}\) is a Gaussian noise matrix, of which \(\bm{E}_{i,j}\sim\mathcal{N}(0,\sigma^{2})\). Hence, we obtain

\[\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{\infty}\leq\left\| \bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}-\mathcal{K}(\tilde{\bm{X}},\tilde{ \bm{X}})\right\|_{\infty}+\left\|\mathcal{K}(\tilde{\bm{X}},\tilde{\bm{X}})- \mathcal{K}(\bm{X},\bm{X})\right\|_{\infty}.\] (78)

Denote by \(\bm{x}_{i}\) (or \(\bm{e}_{i}\)) the \(i\)-th column of \(\bm{X}\in\mathbb{R}^{m\times n}\) (or \(\bm{E}\in\mathbb{R}^{m\times n}\)), \(i=1,\ldots,n\), we first derive the upper bound of the second term of the RHS of (78) as follows:

\[\begin{split}\left\|\bm{K}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{ \infty}&=\left\|\mathcal{K}(\tilde{\bm{X}},\tilde{\bm{X}})- \mathcal{K}(\bm{X},\bm{X})\right\|_{\infty}\\ &=\max_{i,j}|\mathcal{K}(\tilde{\bm{x}}_{i},\tilde{\bm{x}}_{j})- \mathcal{K}(\bm{x}_{i},\bm{x}_{j})|\\ &=\max_{i,j}\left|\exp\left(-\frac{\left\|(\bm{x}_{i}+\bm{e}_{i}) -(\bm{x}_{j}+\bm{e}_{j})\right\|_{2}^{2}}{2r^{2}}\right)-\exp\left(-\frac{ \left\|\bm{x}_{i}-\bm{x}_{j}\right\|_{2}^{2}}{2r^{2}}\right)\right|\\ &\leq\max_{i,j}\frac{1}{2r^{2}}\left|-\left\|(\bm{x}_{i}+\bm{e}_{i })-(\bm{x}_{j}+\bm{e}_{j})\right\|_{2}^{2}+\left\|\bm{x}_{i}-\bm{x}_{j}\right\| _{2}^{2}\right|\\ &=\max_{i,j}\frac{1}{2r^{2}}\left|\left\|(\bm{x}_{i}-\bm{x}_{j})+ (\bm{e}_{i}-\bm{e}_{j})\right\|_{2}^{2}-\left\|\bm{x}_{i}-\bm{x}_{j}\right\| _{2}^{2}\right|\\ &=\max_{i,j}\frac{1}{2r^{2}}\left|\left\|\bm{e}_{i}-\bm{e}_{j} \right\|_{2}^{2}+2\langle\bm{x}_{i}-\bm{x}_{j},\bm{e}_{i}-\bm{e}_{j}\rangle \right|\\ &\leq\max_{i,j}\frac{1}{2r^{2}}\left(\left\|\bm{e}_{i}-\bm{e}_{j} \right\|_{2}^{2}+2\left|\langle\bm{x}_{i}-\bm{x}_{j},\bm{e}_{i}-\bm{e}_{j} \rangle\right|\right)\\ &\leq\max_{i,j}\frac{1}{2r^{2}}\left(\left\|\bm{e}_{i}-\bm{e}_{j} \right\|_{2}^{2}+2\left\|\bm{x}_{i}-\bm{x}_{j}\right\|_{2}\left\|\bm{e}_{i}- \bm{e}_{j}\right\|_{2}\right),\end{split}\] (79)

where for the first inequality we have used the fact that the exponential function is locally Lipschitz continuous, i.e.,

\[\left|e^{x}-e^{y}\right|<\left|x-y\right|\text{ for }x,y<0.\]

Now, let us figure out the upper bound of \(\left\|\bm{e}_{i}-\bm{e}_{j}\right\|_{2}^{2}\). Note that

\[\left\|\bm{e}_{i}-\bm{e}_{j}\right\|_{2}^{2}=\sum_{l=1}^{m}(e_{li}-e_{lj})^{2}=2 \sigma^{2}\sum_{l=1}^{m}\left(\frac{e_{li}-e_{lj}}{\sqrt{2}\sigma}\right)^{2}\] (80)

where \(e_{li}\) represents the \(l\)-th element of the column vector \(\bm{e}_{i}\), \(k=1,\ldots,m\). It is clear that for \(l=1,\ldots,m\),

\[\mathbb{E}[e_{li}-e_{lj}]=0,\] (81) \[\text{var}[e_{li}-e_{lj}]=2\sigma^{2}.\]

Hence, \(\frac{e_{li}-e_{lj}}{\sqrt{2}\sigma}\) is a standard Gaussian random variable drawn from \(\mathcal{N}(0,1)\). Based on this, we can define a random variable as

\[Q=\sum_{l=1}^{m}\left(\frac{e_{li}-e_{lj}}{\sqrt{2}\sigma}\right)^{2},\] (82)which is distributed according to the Chi-squared distribution with \(m\) degrees of freedom.

From Laurent and Massart (2000), we know that for any positive \(t\), the Chi-squared variable \(Q\) with \(m\) degrees of freedom satisfies

\[\mathbb{P}(Q>m+2\sqrt{mt}+2t)\leq 1-e^{-t}.\] (83)

Hence, a bound on \(\left\|\bm{e}_{i}-\bm{e}_{j}\right\|_{2}^{2}\) with probability \(1-e^{-t}\) is

\[\left\|\bm{e}_{i}-\bm{e}_{j}\right\|_{2}^{2}=2\sigma^{2}Q\leq 2\sigma^{2}(m+2 \sqrt{mt}+2t).\] (84)

Using union bound for (84), we have

\[\max_{i,j}\left\|\bm{e}_{i}-\bm{e}_{j}\right\|_{2}^{2}=2\sigma^{2}Q\leq 2\sigma ^{2}(m+2\sqrt{mt}+2t).\] (85)

holds with probability at least \(1-n(n-1)e^{-t}\). Assume \(\left\|\bm{x}_{i}\right\|_{2}\leq\theta\), then \(\left\|\bm{x}_{i}-\bm{x}_{i}\right\|_{2}\leq 2\theta\). For convenience, let \(\xi=\sqrt{m+2\sqrt{mt}+2t}\). It follows from (79) and (85) that, with probability at least \(1-n(n-1)e^{-t}\),

\[\left\|\bm{K}_{\tilde{x}\tilde{x}}-\bm{K}_{xx}\right\|_{\infty} \leq\frac{1}{2r^{2}}\left[2\sigma^{2}\xi^{2}+2\left\|\bm{x}_{i}- \bm{x}_{j}\right\|_{2}\sqrt{2}\sigma\xi\right]\] (86) \[\leq\frac{1}{r^{2}}\left[\sigma^{2}\xi^{2}+2\sqrt{2}\sigma\xi\theta\right]\] \[=\frac{1}{r^{2}}\left[(\sigma\xi+\sqrt{2}\theta)^{2}-2\theta^{2} \right].\]

Now, we figure out the upper bound of the first term of the RHS of (78). We have

\[\left\|\hat{\bm{K}}_{\tilde{x}\tilde{x}}-\bm{K}_{\tilde{x}\tilde{ x}}\right\|_{\infty} =\left\|\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}-\mathcal{K}( \tilde{\bm{X}},\tilde{\bm{X}})\right\|_{\infty}\] (87) \[=\left\|(\phi(\bm{Z})\bm{C})^{T}(\phi(\bm{Z})\bm{C})-(\phi(\bm{Z} )\bm{C})^{T}\phi(\tilde{\bm{X}})+(\phi(\bm{Z})\bm{C})^{T}\phi(\tilde{\bm{X}})- \phi(\tilde{\bm{X}})^{T}\phi(\tilde{\bm{X}})\right\|_{\infty}\] \[=\left\|(\phi(\bm{Z})\bm{C})^{T}(\phi(\bm{Z})\bm{C}-\phi(\tilde{ \bm{X}}))+(\phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{X}}))^{T}\phi(\tilde{\bm{X}}) \right\|_{\infty}\] \[\leq\left\|(\phi(\bm{Z})\bm{C})^{T}(\phi(\bm{Z})\bm{C}-\phi( \tilde{\bm{X}}))\right\|_{\infty}+\left\|(\phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{ X}}))^{T}\phi(\tilde{\bm{X}})\right\|_{\infty}\] \[\leq\left\|(\phi(\bm{Z})\bm{C})\right\|_{2,\infty}\cdot\left\| \phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{X}})\right\|_{2,\infty}+\left\|\phi(\bm{Z} )\bm{C}-\phi(\tilde{\bm{X}})\right\|_{2,\infty}\cdot\left\|\phi(\tilde{\bm{X}} )\right\|_{2,\infty}\] \[=(\left\|(\phi(\bm{Z})\bm{C})\right\|_{2,\infty}+\left\|\phi( \tilde{\bm{X}})\right\|_{2,\infty})\left\|\phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{ X}})\right\|_{2,\infty}\]

where \(\left\|\cdot\right\|_{2,\infty}\) is a norm such that \(\left\|\bm{X}\right\|_{2,\infty}=\max_{i}\left\|\bm{X}_{:,i}\right\|_{2}\) for a real matrix \(\bm{X}\). Here we can just assume that \(\left\|\phi(\bm{Z})\bm{C}-\phi(\tilde{\bm{X}})\right\|_{2,\infty}\leq\gamma\), \(\gamma\) is some constant; this relies on the optimization.

Moreover, assume \(\left\|\bm{C}\right\|_{2}\leq\tau_{C}\), we have

\[\left\|\phi(\tilde{\bm{X}})\right\|_{2,\infty}=1\] (88) \[\left\|\phi(\bm{Z})\bm{C}\right\|_{2,\infty}\leq\left\|\phi(\bm{ Z})\right\|_{F}\max_{j}\left\|\bm{C}_{:,j}\right\|\leq\sqrt{d}\tau_{C}\]

Hence, we can continue to do the derivation of the preceding inequality.

\[\left\|\hat{\bm{K}}_{\tilde{x}\tilde{x}}-\bm{K}_{\tilde{x}\tilde{ x}}\right\|_{\infty} \leq\left(\left\|\phi(\bm{Z})\bm{C}\right\|_{2,\infty}+\left\|\phi( \tilde{\bm{X}})\right\|_{2,\infty}\right)\left\|\phi(\bm{Z})\bm{C}-\phi(\tilde{ \bm{X}})\right\|_{2,\infty}\] (89) \[\leq(\sqrt{d}\tau_{C}+1)\gamma\]As a result, the overall bound is

\[\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{\infty} \leq\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{\hat{x}\hat{x}} \right\|_{\infty}+\left\|\bm{K}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|_{\infty}\] (90) \[\leq\frac{1}{r^{2}}\left[(\sigma\xi+\sqrt{2}\theta)^{2}-2\theta^ {2}\right]+(\sqrt{d}\tau_{C}+1)\gamma\]

where \(\xi=\sqrt{(m+2\sqrt{mt}+2t)}\).

## Appendix G Proof for Theorem 3.6

Proof.: For a data matrix \(\bm{X}\in\mathbb{R}^{m\times n}\), it is perturbed by a Gaussian noise matrix \(\bm{E}\in\mathbb{R}^{m\times n}\) with \(\mathcal{N}(0,\sigma^{2})\) to form the noisy data matrix \(\tilde{\bm{X}}=\bm{X}+\bm{E}\). Let \(\bm{K}_{xx}=\mathcal{K}(\bm{X},\bm{X})\) be the ground truth and \(\hat{\bm{K}}_{\hat{x}\hat{x}}=\bm{C}^{T}\mathcal{K}(\bm{Z},\bm{Z})\bm{C}\) be the approximated similarity matrix by FedKMF algorithm 1. Then, consider any two data points in \(\bm{X}\), \(\bm{x}_{i}\) and \(\bm{x}_{j}\), with the identical label, we know that

\[\bm{K}_{iu}\leq\bm{K}_{ij}\] (91)

where \(\bm{K}_{iu}=\max_{k}((\bm{K}_{xx})_{ik}^{inter})\) and \(\bm{K}_{ij}=(\bm{K}_{xx})_{ij}\).

After running FedKMF (Algorithm 1) on the noisy matrix \(\tilde{\bm{X}}\), if the approximated similarity matrix satisfies \(\left\|\hat{\bm{K}}_{\hat{x}\hat{x}}-\bm{K}_{xx}\right\|<B(\sigma)\), then consider two points in \(\tilde{\bm{X}}\), \(\tilde{\bm{x}}_{i}\) and \(\tilde{\bm{x}}_{j}\), that are actually \(\bm{x}_{i}\) and \(\bm{x}_{j}\) perturbed by some noise, we have

\[\hat{\bm{K}}_{iu}-\hat{\bm{K}}_{ij} =\hat{\bm{K}}_{iu}-\bm{K}_{iu}+\bm{K}_{iu}-\hat{\bm{K}}_{ij}+\bm {K}_{ij}-\bm{K}_{ij}\] (92) \[=(\hat{\bm{K}}_{iu}-\bm{K}_{iu})+(\bm{K}_{ij}-\hat{\bm{K}}_{ij})+ (\bm{K}_{iu}-\bm{K}_{ij})\] \[\leq|\hat{\bm{K}}_{iu}-\bm{K}_{iu}|+|\bm{K}_{ij}-\hat{\bm{K}}_{ ij}|+(\bm{K}_{iu}-\bm{K}_{ij})\] \[\leq B(\sigma)+B(\sigma)+(\bm{K}_{iu}-\bm{K}_{ij})\] \[=2B(\sigma)+(\bm{K}_{iu}-\bm{K}_{ij})\]

where \(\hat{\bm{K}}_{iu}=\max_{k}((\hat{\bm{K}}_{\hat{x}\hat{x}})_{ik}^{inter})\) and \(\hat{\bm{K}}_{ij}=(\hat{\bm{K}}_{\hat{x}\hat{x}})_{ij}\).

Based on Definition 3.5, \(\tilde{\bm{x}}_{i}\) and \(\tilde{\bm{x}}_{j}\) can be correctly clustered only if the following inequality holds with some tolerance \(\epsilon>0\).

\[\hat{\bm{K}}_{iu}-\hat{\bm{K}}_{ij}\leq\epsilon\] (93)

Thus, combining inequalities (92) and (93), the bound function \(B(\sigma)\) satisfies

\[B(\sigma)\leq\frac{1}{2}[\epsilon-(\bm{K}_{iu}-\bm{K}_{ij})]\] (94)

Similarly, if we consider any two data points in \(\bm{X}\), \(\bm{x}_{i}\) and \(\bm{x}_{j}\), with different labels, we know that

\[\bm{K}_{ij}\leq\bm{K}_{iv}\] (95)

where \(\bm{K}_{iv}=\min_{k}((\bm{K}_{xx})_{ik}^{intra})\).

After running FedKMF algorithm 1 on the noisy matrix \(\tilde{\bm{X}}\), we have

\[\hat{\bm{K}}_{ij}-\hat{\bm{K}}_{iv} =\hat{\bm{K}}_{ij}-\bm{K}_{ij}+\bm{K}_{ij}-\hat{\bm{K}}_{iv}+\bm{K }_{iv}-\bm{K}_{iv}\] (96) \[=(\hat{\bm{K}}_{ij}-\bm{K}_{ij})+(\bm{K}_{iv}-\hat{\bm{K}}_{iv})+ (K_{ij}-K_{iv})\] \[\leq|\hat{\bm{K}}_{ij}-\bm{K}_{ij}|+|\bm{K}_{iv}-\hat{\bm{K}}_{iv}| +(\bm{K}_{ij}-\bm{K}_{iv})\] \[\leq B(\sigma)+B(\sigma)+(\bm{K}_{ij}-\bm{K}_{iv})\] \[=2B(\sigma)+(\bm{K}_{ij}-\bm{K}_{iv})\]

where \(\hat{\bm{K}}_{iu}=\max_{k}((\hat{\bm{K}}_{\hat{x}\hat{x}})_{ik}^{intra})\).

Based on Definition 3.5, \(\tilde{\bm{x}}_{i}\) and \(\tilde{\bm{x}}_{j}\) can be correctly clustered only if the following inequality holds with some tolerance \(\epsilon>0\).

\[\hat{\bm{K}}_{ij}-\hat{\bm{K}}_{iv}\leq\epsilon\] (97)

Thus, combining inequalities (96) and (97), the bound function \(B(\sigma)\) satisfies

\[B(\sigma)\leq\frac{1}{2}[\epsilon-(\bm{K}_{ij}-\bm{K}_{iv})]\] (98)

Then, with two upper bounds on \(B(\sigma)\), (94) and (98), we have

\[B(\sigma)\leq\frac{1}{2}\min_{i}\{\epsilon-(\bm{K}_{iu}-\bm{K}_{ij}),\epsilon- (\bm{K}_{ij}-\bm{K}_{iv})\}.\] (99)

where \(\epsilon\) is the parameter of tolerance.

Alternatively, a slightly looser version is like

\[\begin{split} B(\sigma)&\leq\min_{i}\frac{1}{4}[2 \epsilon-(\bm{K}_{iu}-\bm{K}_{iv})]\\ \text{or}& B(\sigma)&\leq\frac{\epsilon }{2}-\max_{i}\frac{1}{4}(\bm{K}_{iu}-\bm{K}_{iv}).\end{split}\] (100)

where \(\bm{K}_{iu}=\max_{k}((\bm{K}_{xx})_{ik}^{inter})\) and \(\bm{K}_{iv}=\min_{k}((\bm{K}_{xx})_{ik}^{intra})\).

## Appendix H Proof for Proposition 3.7

Proof.: The \(\ell_{2}\)-sensitivity [Dwork _et al._, 2014] of a function \(f:\mathbb{N}^{|\mathcal{X}|}\to\mathbb{R}^{k}\) is:

\[\Delta_{2}(f)=\max_{x\sim y}\|f(x)-f(y)\|_{2},\]

where \(x\sim y\) denotes that \(x\) and \(y\) are neighboring datasets. In our case, the function is \(f(x)=x\). Then

\[\|f(x)-f(y)\|_{2}=\|x-y\|_{2}\leq 2\tau_{X}.\]

It means \(\Delta_{2}(f)\leq 2\tau_{X}\). Now using Theorem 3.22 in [Dwork _et al._, 2014] and Lemma H.1, we get the desired result.

**Lemma H.1** (Post-Processing [Dwork _et al._, 2014]).: _Let \(\mathcal{M}:\mathbb{N}^{|\mathcal{X}|}\to R\) be a randomized algorithm that is \((\varepsilon,\delta)\)-differentially private. Let \(h:R\to R^{\prime}\) be an arbitrary randomized mapping. Then \(h\circ\mathcal{M}:\mathbb{N}^{|\mathcal{X}|}\to R^{\prime}\) is \((\varepsilon,\delta)-\) differentially private._

## Appendix I Proof for Theorem 3.8

Proof.: Based on the assumptions \(\|\bm{C}\|_{2,\infty}\leq\tau_{C}\), \(\left\|\phi(\bm{Z})\bm{C}-\phi(\bm{X})\right\|_{2,\infty}\leq\gamma\), \(\tilde{\bm{C}}=\bm{C}+\bm{E}_{C}\) for the entry \((\bm{E}_{C})_{ij}\sim\mathcal{N}(0,\sigma_{C}^{2})\), and \(\tilde{\bm{Z}}=\bm{Z}+\bm{E}_{Z}\) for the entry \((\bm{E}_{Z})_{ij}\sim\mathcal{N}(0,\sigma_{Z}^{2})\), we obtain

\[\begin{split}\left\|\tilde{\bm{K}}_{xx}-\bm{K}_{xx}\right\|_{ \infty}&=\left\|\tilde{\bm{C}}^{T}\mathcal{K}(\tilde{\bm{Z}}, \tilde{\bm{Z}})\tilde{\bm{C}}-\mathcal{K}(\bm{X},\bm{X})\right\|_{\infty}\\ &=\left\|(\phi(\tilde{\bm{Z}})\tilde{\bm{C}})^{T}(\phi(\tilde{\bm {Z}})\tilde{\bm{C}})-\phi^{T}(\bm{X})\phi(\bm{X})\right\|_{\infty}\\ &=\left\|(\phi(\tilde{\bm{Z}})\tilde{\bm{C}})^{T}(\phi(\tilde{\bm {Z}})\tilde{\bm{C}})-(\phi(\tilde{\bm{Z}})\tilde{\bm{C}})^{T}\phi(\bm{X})+( \phi(\tilde{\bm{Z}})\tilde{\bm{C}})^{T}\phi(\bm{X})-\phi^{T}(\bm{X})\phi(\bm{X })\right\|_{\infty}\\ &=\left\|(\phi(\tilde{\bm{Z}})\tilde{\bm{C}})^{T}(\phi(\tilde{\bm {Z}})\tilde{\bm{C}})-\phi(\bm{X})\right)+(\phi(\tilde{\bm{Z}})\tilde{\bm{C}}- \phi(\bm{X}))^{T}\phi(\bm{X})\right\|_{\infty}\\ &\leq\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}\right\|_{2,\infty} \left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}+ \left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\left\| \phi(\bm{X})\right\|_{2,\infty}\\ &\leq\left(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}\right\|_{2, \infty}+\left\|\phi(\bm{X})\right\|_{2,\infty}\right)\underbrace{\left\|\phi( \tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}}_{\text{T.1}}\\ \end{split}\] (101)where \(\left\|\phi(\bm{X})\right\|_{2,\infty}=1\). An upper bound on \(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}\right\|_{2,\infty}\) can be obtained only if we derive the upper bound on \(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\). That is, if \(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\leq \gamma_{zc}\), it implies that

\[\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}\right\|_{2,\infty}\leq \left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}+ \left\|\phi(\bm{X})\right\|_{2,\infty}=\gamma_{zc}+1\] (102)

which consequently gives

\[\left\|\tilde{\bm{K}}_{xx}-\bm{K}_{xx}\right\|_{\infty}\leq\gamma_{zc}(\gamma _{zc}+2)\] (103)

Hence, we derive the upper bound on \(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\) for the remaining proof.

\[\begin{split}\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm {X})\right\|_{2,\infty}&=\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{ C}}-\phi(\bm{Z})\bm{C}+\phi(\bm{Z})\bm{C}-\phi(\bm{X})\right\|_{2,\infty}\\ &\leq\underbrace{\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi( \bm{Z})\bm{C}\right\|_{2,\infty}}_{\text{T.2}}+\left\|\phi(\bm{Z})\bm{C}-\phi( \bm{X})\right\|_{2,\infty}\end{split}\] (104)

where the second term \(\left\|\phi(\bm{Z})\bm{C}-\phi(\bm{X})\right\|_{2,\infty}\leq\gamma\) is the assumption. Next, we derive the upper bound on \(T.2\).

\[\begin{split}\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm {Z})\bm{C}\right\|_{2,\infty}&=\left\|\phi(\tilde{\bm{Z}}) \tilde{\bm{C}}-\phi(\tilde{\bm{Z}})\bm{C}+\phi(\tilde{\bm{Z}})\bm{C}-\phi(\bm {Z})\bm{C}\right\|_{2,\infty}\\ &=\left\|\phi(\tilde{\bm{Z}})(\tilde{\bm{C}}-\bm{C})+(\phi( \tilde{\bm{Z}})-\phi(\bm{Z}))\bm{C}\right\|_{2,\infty}\\ &\leq\left\|\phi(\tilde{\bm{Z}})(\tilde{\bm{C}}-\bm{C})\right\|_{ 2,\infty}+\left\|(\phi(\tilde{\bm{Z}})-\phi(\bm{Z}))\bm{C}\right\|_{2,\infty} \\ &\leq\sqrt{d}\left\|\bm{E}_{C}\right\|_{2,\infty}+\left\|\phi( \tilde{\bm{Z}})-\phi(\bm{Z})\right\|_{F}\left\|\bm{C}\right\|_{2,\infty}\\ &\leq\sigma_{C}\xi_{d}\sqrt{d}+\tau_{C}\underbrace{\left\|\phi( \tilde{\bm{Z}})-\phi(\bm{Z})\right\|_{F}}_{\text{T.3}}\end{split}\] (105)

where we used \(\frac{1}{\sigma_{C}^{2}}\left\|\bm{E}_{C}\right\|_{2,\infty}^{2}\leq\xi_{d}^ {2}=d+2\sqrt{dt}+2t\) with probability at least \(1-ne^{-t}\) [Laurent and Massart, 2000] since it is the fact that \(\frac{1}{\sigma_{C}^{2}}\left\|\bm{E}_{C}\right\|_{2,\infty}^{2}\sim\chi_{d}^ {2}\) where the entry \((\bm{E}_{C})_{ij}\sim\mathcal{N}(0,\sigma_{C}^{2})\). For \(T.3\), we have

\[\begin{split}\left\|\phi(\tilde{\bm{Z}})-\phi(\bm{Z})\right\|_{F} ^{2}&=\text{Tr}((\phi(\tilde{\bm{Z}})-\phi(\bm{Z}))^{T}(\phi( \tilde{\bm{Z}})-\phi(\bm{Z})))\\ &=\text{Tr}(\phi^{T}(\tilde{\bm{Z}})\phi(\tilde{\bm{Z}})-\phi^{T} (\tilde{\bm{Z}})\phi(\bm{Z})-\phi^{T}(\bm{Z})\phi(\tilde{\bm{Z}})+\phi^{T}( \bm{Z})\phi(\bm{Z}))\\ &=2d-2\underbrace{\text{Tr}(\phi^{T}(\tilde{\bm{Z}})\phi(\bm{Z})) }_{\text{T.4}}\end{split}\] (106)

For \(T.4\), we can obtain

\[\begin{split}\text{Tr}(\phi^{T}(\tilde{\bm{Z}})\phi(\bm{Z}))& =\sum_{j=1}^{d}\phi^{T}(\tilde{\bm{z}}_{j})\phi(\bm{z}_{j})= \sum_{j=1}^{d}\exp\left(-\frac{\left\|\bm{z}_{j}+(\bm{E}_{Z})_{:,j}-\bm{z}_{j} \right\|_{2}^{2}}{2r^{2}}\right)\\ &=\sum_{j=1}^{d}\exp\left(-\frac{\left\|(\bm{E}_{Z})_{:,j}\right\|_ {2}^{2}}{2r^{2}}\right)\\ &\geq d\exp\left(-\frac{\sigma_{Z}^{2}\xi_{d}^{2}}{2r^{2}}\right) \end{split}\] (107)

where we use \(\frac{1}{\sigma_{Z}^{2}}\left\|(\bm{E}_{Z})_{:,j}\right\|_{2}^{2}\leq\xi_{d}^ {2}=d+2\sqrt{dt}+2t\) with probability at least \(1-de^{-t}\) [Laurent and Massart, 2000] since it is the fact that \(\frac{1}{\sigma_{Z}^{2}}\left\|(\bm{E}_{Z})_{:,j}\right\|_{2}^{2}\sim\chi_{d}^ {2}\) where the entry \((\bm{E}_{Z})_{ij}\sim\mathcal{N}(0,\sigma_{Z}^{2})\).

Hence, we can go back to give an upper bound on \(\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\):

\[\left\|\phi(\tilde{\bm{Z}})\tilde{\bm{C}}-\phi(\bm{X})\right\|_{2,\infty}\leq \gamma+\sigma_{C}\xi_{d}\sqrt{d}+\tau_{C}\sqrt{2d\left(1-\exp\left(-\frac{\sigma _{2}^{2}\xi_{d}^{2}}{2r^{2}}\right)\right)}\] (108)

Finally, we have

\[\left\|\tilde{\bm{K}}_{xx}-\bm{K}_{xx}\right\|_{\infty}\leq\gamma_{zc}(\gamma_ {zc}+2)\] (109)

where

\[\gamma_{zc}=\gamma+\sqrt{d}\left(\sigma_{C}\xi_{d}+\tau_{C}\sqrt{2\left(1- \exp\left(-\frac{\sigma_{Z}^{2}\xi_{d}^{2}}{2r^{2}}\right)\right)}\right).\] (110)

## Appendix J Proof for Theorem 3.9

**Lemma J.1**.: _Assume \(\Upsilon=\max_{\bm{z}_{j},\bm{x}^{\prime}}\|\bm{z}_{j}-\bm{x}^{\prime}\|_{2}\) and \(\|\bm{x}-\bm{x}^{\prime}\|_{2}\leq 2\tau_{X}\), let \(\{\bm{C}_{p}^{S}\}_{p=1}^{P}\) be perturbed by noise drawn from \(\mathcal{N}(0,\sigma^{2})\) with the parameter \(\sigma\geq\frac{2c\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+\Upsilon)}{r^{2}\varepsilon }\) for \(c^{2}>2\ln(1.25/\delta)\). Then, the Gaussian Mechanism that adds noise to \(\{\bm{C}_{p}^{S}\}_{p=1}^{P}\) is \((\varepsilon,\delta)\)-differentially private._

**Lemma J.2**.: _Assume \(\max_{(p,j)}\{\|\bm{x}_{p_{j}}\|_{2},\|\bm{x}_{p_{j}^{\prime}}\|_{2}\leq\tau_{ X},\ \max_{(i,j)}\|\bm{z}_{i}-\bm{x}_{j}\|_{\infty}=\Upsilon,\)\(\|\bm{Z}_{p}^{s}\|_{sp}\leq\tau_{Z}\ \forall s\), and \(\|\bm{C}^{S}\|_{2,\infty}\leq\tau_{C}\), let \(\{\bm{Z}_{p}^{s}\}_{p=1}^{P}\) for \(s=1,\cdots,S\) be perturbed by noise drawn from \(\mathcal{N}(0,\sigma^{2})\) with variance \((8S\Delta^{2}(g_{Z})\log(e+(\varepsilon/\delta))/\varepsilon^{2})\) where \(\Delta(g_{Z})=\frac{2\sqrt{d}\tau_{C}\tau_{X}\eta_{h}}{r^{2}}\left\{1+(\tau_{ X}+\tau_{Z})\frac{(\tau_{X}+\Upsilon)}{r^{2}}\right\}\). Then, the Gaussian Mechanism that adds noise to \(\{\bm{Z}_{p}^{s}\}_{p=1}^{P}\) for \(s=1,\cdots,S\) is \((\varepsilon,\delta)\)-differentially private._

By Lemma J.2, the mechanism that adds Gaussian noise to \(\bm{Z}_{p}^{s}\) for \(s=1,\cdots,S\) with variance \((8S\Delta^{2}(g_{Z})\log(e+(\varepsilon_{Z}/\delta_{Z}))/\varepsilon_{Z}^{2})\) satisfies \((\varepsilon_{Z},\delta_{Z})\)-differential privacy under \(S\)-fold adaptive composition for any \(\varepsilon_{Z}>0\) and \(\delta_{Z}\in(0,1]\). By Lemma J.1, the Gaussian Mechanism that injects noise to \(\bm{C}_{p}^{S}\) with parameter \(\sigma\geq 2c\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+\Upsilon)/(r^{2}\varepsilon _{C})\) is \((\varepsilon_{C},\delta_{C})\)-differentially private. Therefore, by Theorem 3.16 of [Dwork _et al._, 2014], the proposed algorithm that adds Gaussian noise to \(\bm{Z}_{p}^{s}\) for \(s=1,\cdots,S\) and \(\mathcal{C}_{p}^{S}\) is \((\varepsilon_{C}+\varepsilon_{Z},\delta_{C}+\delta_{Z})\)-differentially private. This finished the proof.

## Appendix K Proof for Lemma J.1

Proof.: In our FedSC, for each column of \(\bm{C}\), we have

\[\bm{c}=g_{C}(\bm{x})=\bm{G}\mathcal{K}(\bm{Z},\bm{x})=\bm{G}\begin{bmatrix} \exp\left(-\frac{\|\bm{z}_{1}-\bm{x}\|_{2}^{2}}{2r^{2}}\right)\\ \vdots\\ \exp\left(-\frac{\|\bm{z}_{d}-\bm{x}\|_{2}^{2}}{2r^{2}}\right)\end{bmatrix},\] (111)

where \(\bm{G}=(\mathcal{K}(\bm{Z},\bm{Z})+\lambda\bm{I}_{d})^{-1}\). We have

\[\|g_{C}(\bm{x})-g_{C}(\bm{x}^{\prime})\|_{2}\leq\|\bm{G}\|_{sp}\|\mathcal{K}( \bm{Z},\bm{x})-\mathcal{K}(\bm{Z},\bm{x}^{\prime})\|_{2},\] (112)

where \(\|\cdot\|_{sp}\) denotes the spectral norm of matrix. Since \(\exp(z)\) is locally Lipschitz continuous when \(z<0\), we have

\[\begin{split}&\left(\exp\left(-\frac{\|\bm{z}_{j}-\bm{x}\|_{2}^ {2}}{2r^{2}}\right)-\exp\left(-\frac{\|\bm{z}_{j}-\bm{x}\|_{2}^{2}}{2r^{2}} \right)\right)^{2}\\ \leq&\left|\frac{1}{2r^{2}}\left(\|\bm{z}_{j}-\bm{x}\|^{2}- \|\bm{z}_{j}-\bm{x}^{\prime}\|_{2}^{2}\right)\right|^{2}\\ =&\left|\frac{1}{2r^{2}}\left(\|\bm{x}-\bm{x}^{\prime}\|_{2 }^{2}+2\left\langle\bm{z}_{j}-\bm{x}^{\prime},\bm{x}^{\prime}-\bm{x}^{\prime} \right\rangle\right)\right|^{2}\\ \leq&\left|\frac{1}{2r^{2}}\left(\|\bm{x}-\bm{x}^{\prime}\|_{2 }^{2}+2\|\bm{z}_{j}-\bm{x}^{\prime}\|_{2}\|\bm{x}-\bm{x}^{\prime}\|_{2}\right) \right|^{2}.\end{split}\] (113)Let \(\Upsilon=\max_{\bm{z}_{j},\bm{x}^{\prime}}\|\bm{z}_{j}-\bm{x}^{\prime}\|_{2}\) and \(\|\bm{x}-\bm{x}^{\prime}\|_{2}\leq 2\tau_{X}\). Then the \(\ell_{2}\)-sensitivity of \(g\) is

\[\begin{split}\triangle_{2}(g_{C})\leq&\sup_{\bm{x},\bm{x}^{\prime}}\|\bm{G}\|_{\sigma}\sqrt{\sum_{j=1}^{d}\left|\frac{1}{2r^{2}} \left(\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2}+2\|\bm{z}_{j}-\bm{x}^{\prime}\|_{2} \|\bm{x}^{\prime}-\bm{x}^{\prime}\|_{2}\right)\right|^{2}}\\ \leq&\|\bm{G}\|_{\sigma}\sqrt{\sum_{j=1}^{d}\left| \frac{1}{2r^{2}}\left(4\tau_{X}^{2}+4\Upsilon\tau_{X}\right)\right|^{2}}\\ =&\|\bm{G}\|_{\sigma}\frac{2\sqrt{d}\tau_{X}(\tau_ {X}+\Upsilon)}{r^{2}}\\ \leq&\frac{2\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+ \Upsilon)}{r^{2}}.\end{split}\] (114)

Then according to Theorem 3.22 in [Dwork _et al._, 2014], for \(c^{2}>2\ln(1.25/\delta)\) the Gaussian Mechanism with parameter \(\sigma\geq\frac{2c\lambda^{-1}\sqrt{d}\tau_{X}(\tau_{X}+\Upsilon)}{r^{2}\varepsilon}\) is \((\varepsilon,\delta)\)-differentially private. This finished the proof. 

## Appendix L Proof for Lemma J.2

Proof.: **Proof Sketch** The \((\epsilon,\delta)\)-differential privacy of the proposed algorithm can be achieved by injecting noise into \(\bm{Z}\) for each local update and into \(\bm{C}\) at the final round. To prove this, we first compute the sensitivity of \(\bm{Z}\) and \(\bm{C}\) for determining the differential privacy of them. Then, we use the adaptive composition [Kairouz _et al._, 2015] to get the superposition of them which will give the final theoretical result.

Now, the formal proof is as follows.

In our FedSC, consider one-step update of \(\bm{Z}\) at client of \(p\)

\[\bm{Z}_{p}^{s,t}=\bm{Z}_{p}^{s,t-1}-\eta_{t}\frac{\partial f}{\partial\bm{Z}} (\bm{Z}_{p}^{s,t-1})\] (115)

where the derivative is given by

\[\frac{\partial f}{\partial\bm{Z}}(\bm{Z}_{p}^{s,t-1})=\frac{1}{r^{2}}(\bm{X} _{p}\bm{W}_{Z}-\bm{Z}\bar{\bm{W}}_{Z})+\frac{2}{r^{2}}(\bm{Z}\bm{Q}_{Z}-\bm{Z} \bar{\bm{Q}}_{Z})\] (116)

and \(\bm{X}_{p}=[\bm{x}_{p_{1}},\cdots,\bm{x}_{p_{j}-1},\bm{x}_{p_{j}},\bm{x}_{p_{j }+1},\cdots,\bm{x}_{p_{N_{p}}}]\).

For the simplicity of the proof, we omit the pair of iteration parameters \((s,t)\) and instead denote two adjacent local updates by \(k\) and \(k-1\) for nonnegative \(k\geq 1\). Thus, we have the equivalent version of a one-step update of \(\bm{Z}_{p}\) at client \(p\)

\[\bm{Z}_{p}^{k}=\bm{Z}_{p}^{k-1}-\eta_{k}\left\{\frac{1}{r^{2}}(\bm{X}_{p}\bm{W }_{Z}-\bm{Z}_{p}^{k-1}\bar{\bm{W}}_{Z})+\frac{2}{r^{2}}(\bm{Z}_{p}^{k-1}\bm{Q} _{Z}-\bm{Z}_{p}^{k-1}\bar{\bm{Q}}_{Z})\right\}\] (117)

where \(\bm{X}_{p}=[\bm{x}_{p_{1}},\cdots,\bm{x}_{p_{j}-1},\bm{x}_{p_{j}},\bm{x}_{p_{j }+1},\cdots,\bm{x}_{p_{N_{p}}}]\).

To compute the sensitivity of \(\bm{Z}_{p}\), we give the counterpart of the above update as

\[(\bm{Z}_{p}^{k})^{\prime}=\bm{Z}_{p}^{k-1}-\eta_{k}\left\{\frac{1}{r^{2}}(\bm{ X}_{p}^{\prime}\bm{W}_{Z}^{\prime}-\bm{Z}_{p}^{k-1}\bar{\bm{W}}_{Z}^{\prime})+ \frac{2}{r^{2}}(\bm{Z}_{p}^{k-1}\bm{Q}_{Z}-\bm{Z}_{p}^{k-1}\bar{\bm{Q}}_{Z})\right\}\] (118)

where \(\bm{X}_{p}^{\prime}=[\bm{x}_{p_{1}},\cdots,\bm{x}_{p_{j}-1},\bm{x}_{p_{j}}^{ \prime},\bm{x}_{p_{j}+1},\cdots,\bm{x}_{p_{N_{p}}}]\).

Next, let's start to derive the upper bound on \(\|\bm{Z}_{p}^{k}-(\bm{Z}_{p}^{k})^{\prime}\|_{F}\) term by term.

\[\begin{split}\left\|\bm{Z}_{p}^{k}-(\bm{Z}_{p}^{k})^{\prime}\right\| _{F}&=\left\|-\frac{\eta_{k}}{r^{2}}\left\{\left(\bm{X}_{p}\bm{W} _{Z}-\bm{Z}_{p}^{k-1}\bar{\bm{W}}_{Z}\right)-\left(\bm{X}_{p}^{\prime}\bm{W}_{Z} ^{\prime}-\bm{Z}_{p}^{k-1}\bar{\bm{W}}_{Z}^{\prime}\right)\right\}\right\|_{F} \\ &=\frac{\eta_{k}}{r^{2}}\left\|\left(\bm{X}_{p}\bm{W}_{Z}-\bm{X}_{ p}^{\prime}\bm{W}_{Z}^{\prime}\right)-\bm{Z}_{p}^{k-1}\left(\bar{\bm{W}}_{Z}-\bar{\bm{W}}_{Z}^{ \prime}\right)\right\|_{F}\\ &\leq\frac{\eta_{k}}{r^{2}}\left\{\underbrace{\left\|\bm{X}_{p} \bm{W}_{Z}-\bm{X}_{p}^{\prime}\bm{W}_{Z}\right\|_{F}}_{\Upsilon,1}+\underbrace{ \left\|\bm{Z}_{p}^{k-1}\left(\bar{\bm{W}}_{Z}-\bar{\bm{W}}_{Z}^{\prime} \right)\right\|_{F}}_{\Upsilon,2}\right\}\\ \end{split}\] (119)

[MISSING_PAGE_EMPTY:35]

Assume \(\|\bm{Z}_{p}^{k}\|_{sp}\leq\tau_{2}\ \forall k\), we have for \(T.2\)

\[\begin{split}\left\|\bm{Z}_{p}^{k-1}\left(\bar{\bm{W}}_{Z}-\bar{ \bm{W}}_{Z}^{\prime}\right)\right\|_{F}&=\left\|\bm{Z}_{p}^{k-1} \left(\text{diag}(\bm{1}_{n}^{T}\bm{W}_{Z})-\text{diag}(\bm{1}_{n}^{T}\bm{W}_{Z }^{\prime})\right)\right\|_{F}\\ &=\left\|\bm{Z}_{p}^{k-1}\text{diag}(\bm{1}_{n}^{T}(\bm{W}_{Z}- \bm{W}_{Z}^{\prime}))\right\|_{F}\\ &\leq\left\|\bm{Z}_{p}^{k-1}\right\|_{sp}\left\|\text{diag}(\bm{ 1}_{n}^{T}(\bm{W}_{Z}-\bm{W}_{Z}^{\prime}))\right\|_{F}\\ &=\left\|\bm{Z}_{p}^{k-1}\right\|_{sp}\left\|\bm{1}_{n}^{T}(\bm{ W}_{Z}-\bm{W}_{Z}^{\prime})\right\|_{2}\\ &\leq\left\|\bm{Z}_{p}^{k-1}\right\|_{sp}\left\|\bm{c}_{j}^{T} \odot\left(\mathcal{K}(\bm{x}_{p_{j}},\bm{Z}_{p}^{k-1})-\mathcal{K}(\bm{x}_{ p_{j}}^{\prime},\bm{Z}_{p}^{k-1})\right)\right\|_{2}\\ &\leq\left\|\bm{Z}_{p}^{k-1}\right\|_{sp}\left\|\bm{C}\right\|_{ 2,\infty}\left\|\triangle_{\bm{Z}_{p}^{k-1},\bm{x}_{p_{j}}}\right\|_{2}\\ &\leq\frac{2\sqrt{d}\tau_{Z}\tau_{C}\tau_{X}(\tau_{X}+\Upsilon)}{ r^{2}}\end{split}\] (124)

Thus, we get the upper bounds on \(T.1\) and \(T.2\), respectively, and finally give an upper bound on \(\left\|\bm{Z}_{p}^{k}-(\bm{Z}_{p}^{k})^{\prime}\right\|_{F}\).

\[\begin{split}\left\|\bm{Z}_{p}^{k}-(\bm{Z}_{p}^{k})^{\prime} \right\|_{F}&\leq\frac{\eta_{k}}{r^{2}}\left\{\underbrace{\left\| \bm{X}_{p}\bm{W}_{Z}-\bm{X}_{p}^{\prime}\bm{W}_{Z}^{\prime}\right\|_{F}}_{T.1}+ \underbrace{\left\|\bm{Z}_{p}^{k-1}\left(\bar{\bm{W}}_{Z}-\bar{\bm{W}}_{Z}^{ \prime}\right)\right\|_{F}}_{T.2}\right\}\\ &\leq\frac{\eta_{k}}{r^{2}}\left\{2\sqrt{d}\tau_{C}\tau_{X}\left( 1+\frac{\tau_{X}(\tau_{X}+\Upsilon)}{r^{2}}\right)+\frac{2\sqrt{d}\tau_{Z}\tau _{C}\tau_{X}(\tau_{X}+\Upsilon)}{r^{2}}\right\}\\ &=\frac{2\sqrt{d}\tau_{C}\tau_{X}\eta_{k}}{r^{2}}\left\{1+\left( \tau_{X}+\tau_{Z}\right)\frac{\left(\tau_{X}+\Upsilon\right)}{r^{2}}\right\} \end{split}\] (125)

Therefore, if we define \(\bm{Z}_{p}=g_{Z}(\bm{X}_{p})\), the \(\ell_{2}\)-sensitivity of \(g_{Z}\) is

\[\begin{split}\triangle_{2}(g_{Z})&=\sup_{\bm{X}_{p},\bm{X}_{p}^{\prime}}\|g_{Z}(\bm{X}_{p})-g_{Z}(\bm{X}_{p}^{\prime})\|_{2}\\ &=\sup_{\bm{X}_{p},\bm{X}_{p}^{\prime}}\left\|\bm{Z}_{p}^{k}-(\bm {Z}_{p}^{k})^{\prime}\right\|_{F}\\ &\leq\frac{2\sqrt{d}\tau_{C}\tau_{X}\eta_{k}}{r^{2}}\left\{1+\left( \tau_{X}+\tau_{Z}\right)\frac{\left(\tau_{X}+\Upsilon\right)}{r^{2}}\right\} \end{split}\] (126)

By Theorem 4.3 of [10], the mechanism that adds Gaussian noise to \(\bm{Z}_{p}^{s}\) for \(s=1,\cdots,S\) with variance \((8S\Delta^{2}(g_{Z})\log(e+(\varepsilon_{Z}/\delta_{Z}))/\varepsilon_{Z}^{2})\) satisfies \((\varepsilon_{Z},\delta_{Z})\)-differential privacy under \(S\)-fold adaptive composition for any \(\varepsilon_{Z}>0\) and \(\delta_{Z}\in(0,1]\). This finished the proof.