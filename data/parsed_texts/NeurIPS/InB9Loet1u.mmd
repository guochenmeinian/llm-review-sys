[MISSING_PAGE_FAIL:1]

necessarily proportional to the value of the good itself--nor should it be. We can characterize a set of bounded and monotonically decreasing utility functions that are implied by certain axioms. These are essentially the von Neumann-Morgenstern axioms [20] from classical decision theory, plus two novel runtime-specific axioms that assert a (weak) preference for faster runs over slower ones, and a (strict) preference for algorithm runs that complete over ones that time out. Preferences that follow these axioms are described by a utility function that is monotonically decreasing in runtime from 1 to 0. This utility function can incorporate factors like the benefit an end user attains from completing a run, the cost they pay for cloud computing resources, and any uncertainty they may have about when an answer will stop being useful to them. This utilitarian perspective opens a new and exciting direction for algorithm configuration.

While it is reassuring to know that optimizing utility instead of runtime is the right thing to do, it also offers significant algorithmic benefits. Because utilities are bounded and monotonically decreasing, exceedingly long runs contribute negligibly to estimates of an algorithm's expected utility. This means that there will always be some captime that allows us to accurately estimate true expected utility from capped samples. We can use tools from the multi-armed bandit literature for best arm identification, and the algorithms we describe will draw on the elimination and racing algorithms described in [20]; [17] and elsewhere. In contrast, runtime-based configuration procedures like Structured Procrastination [14], Structured Procrastination with Confidence [14], LeapsAndBounds [21], CapsAndRuns [15], and ImpatientCapsAndRuns [16] need to do many runs at large captimes in order to make theoretical guarantees. Furthermore, in order to be able to make any provable guarantees at all, they have to introduce an additional parameter specifying how much of the runtime CDF can be ignored; even if an algorithm has always finished very quickly on every instance seen so far, it could always take so long on the next instance that its expected runtime is arbitrarily large. In this setting, no optimality guarantees can be made without broadening the definition of optimality.

Other theoretically-motivated methods such as [18]; [19] offer performance guarantees based on different measures of complexity and guarantee notions of PAC optimality akin to those we present here. These papers do not focus on runtime, instead studying traditional sample complexity, with each sample (i.e., algorithm run) contributing the same "sampling cost." In our algorithm configuration setting, the cost of a sample is the amount of (capped) runtime spent acquiring it, with longer captimes potentially leading to more expensive (but also more informative) samples. Because of this difference, these two lines of work propose quite different types of learning algorithms. However, [19] do assume the existence of a bounded utility function which measures algorithm performance, and others [17] have argued that utility functions of this form are better objectives to optimize than runtime when choosing algorithms. Some methods have been specifically designed to exploit the parallel nature of the algorithm configuration problem; AC-Band [19] is a bandit-based procedure inspired by the Hyperband algorithm [14] that runs multiple configurations simultaneously while ruling out poor-performing ones along the way and offering theoretical guarantees with respect to the set of configurations considered.

Inspired by the mantra of procrastination that has found success in previous work, and by the benefits that come with the use of utility functions, this paper presents a procedure we dub Utilitarian Procrastination (UP), so-named because it performs as many low-captime runs as possible before proceeding to higher-captime runs. We offer theoretical guarantees about its performance, showing that it will return a good configuration and proving that its worst-case upper bound is similar to the theoretical lower bound that any procedure must require. We also present experimental results showing how this procedure performs in practice and how it compares to a more naive baseline.

## 2 Setup

We assume there is a monotonically decreasing runtime utility function \(u:\mathbb{R}_{\geq 0}\rightarrow[0,1]\) with \(u(0)=1\) and \(\lim_{t\rightarrow\infty}u(t)=0\). The existence of \(u\) follows from simple axioms [18]. The value \(u(t)\) describes the (expected) well-being of an individual who uses an algorithm to solve a problem instance (e.g., an integer program). During the configuration process, the goal will be to choose an algorithm that maximizes \(u(t)\), in expectation over \(t\).

Given a set of algorithms indexed by \(i=1,...,n\), our goal is to find an approximately optimal algorithm using capped runtime samples. We assume we have access to a stream of input instances indexed by \(j=1,2,...\) drawn from some distribution \(\mathcal{D}_{J}\). We will use \(t_{ij}\) to denote the true uncapped runtime of algorithm \(i\) on input \(j\), and \(t_{ij}(\kappa)=\min(t_{ij},\kappa)\) to be the \(\kappa\)-capped runtime of \(i\) on \(j\). When we do a run, we observe \(t_{ij}(\kappa)\), not \(t_{ij}\). Of course these coincide for any run that completes. The instance distribution \(\mathcal{D}_{J}\), along with any randomness of the algorithm or execution environment will together induce a runtime distribution for each algorithm \(i\). We will use \(\mathcal{D}_{i}\) to denote this runtime distribution, and \(F_{i}\) to denote its CDF. For each algorithm \(i\), the true uncapped expected utility is

\[U_{i}=\operatorname*{\mathbb{E}}_{t\sim D_{i}}\big{[}u(t)\big{]}.\]

The capped expected utility is

\[U_{i}(\kappa)=\operatorname*{\mathbb{E}}_{t\sim D_{i}}\big{[}u\big{(}\min(t, \kappa)\big{)}\big{]}.\]

And given any capped runtime samples \(t_{i1}(\kappa),...,t_{im}(\kappa)\), the capped empirical average utility is

\[\widehat{U}_{im}(\kappa)=\frac{1}{m}\sum_{j=1}^{m}u\big{(}t_{ij}(\kappa)\big{)}.\]

**Definition 1** (\(\epsilon\)-optimal).: _An algorithm \(i^{*}\) is called \(\epsilon\)-optimal if \(U_{i^{*}}\geq\max_{i}U_{i}-\epsilon\)._

Our goal will be to find \(\epsilon\)-optimal algorithms with probability at least \(1-\delta\), and to do so as quickly as possible. Our first lemma deterministically bounds an algorithm's uncapped expected utility in terms of its capped expected utility and CDF.

**Lemma 1**.: _For any \(i\) and \(\kappa\) we deterministically have \(U_{i}(\kappa)-u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}\leq U_{i}\leq U_{i}( \kappa)\,.\)_

Proof.: This follows from the law of total expectation and the fact that \(u\) is non-increasing, and so \(u(\kappa)\geq u(t)\) for all \(t\geq\kappa\). 

Intuitively, Lemma1 is true because the capped expected utility counts runs that cap as having just completed at the capture, when really they would have taken longer if given the chance. This makes the capped expected utility look more favourable than the uncapped expected utility. Our second lemma shows that if we do runs at capture \(\kappa\), then we can accurately estimate the algorithm's runtime CDF at \(\kappa\).

**Lemma 2**.: _For any \(i\), \(m\), \(\kappa\) and \(\delta\), let \(\widehat{F}_{im}(\kappa)\) be the fraction of the \(m\) runs that \(A\) completes within capture \(\kappa\). Then \(\widehat{F}_{im}(\kappa)-\sqrt{\frac{\ln(1/\delta)}{2m}}\leq F_{i}(\kappa) \leq\widehat{F}_{im}(\kappa)+\sqrt{\frac{\ln(1/\delta)}{2m}}\) with probability at least \(1-2\delta\)._

Proof.: For each \(j\), let \(X_{j}=1\) if \(t_{ij}<\kappa\) and \(X_{j}=0\) otherwise. The proof then follows from a straightforward application of Hoeffding's inequality. 

Lemma2 simply says that if we take enough samples at capture \(\kappa\), the fraction of those runs that complete will be close to the true likelihood of a run completing before \(\kappa\). Our third lemma shows that expected capped utility can be estimated accurately using capped runtime samples.

**Lemma 3**.: _For any \(i,\,m\), \(\kappa\) and \(\delta\) we have \(\widehat{U}_{im}(\kappa)-\big{(}1-u(\kappa)\big{)}\sqrt{\frac{\ln(1/\delta)} {2m}}\leq U_{i}(\kappa)\leq\widehat{U}_{im}(\kappa)+\big{(}1-u(\kappa)\big{)} \sqrt{\frac{\ln(1/\delta)}{2m}}\) with probability at least \(1-2\delta\)._

Proof.: Since \(u\big{(}t_{ij}(\kappa)\big{)}\in[u(\kappa),1]\) for all \(j\), the proof follows immediately from Hoeffding's inequality. 

Lemma3 simply says that if we take enough samples, then the empirical capped average utility will be close to the true expected capped utility. Together, these lemmas imply empirical confidence bounds on the true expected utility. Define the upper and lower confidence bounds

\[UCB_{im}(\kappa) =\widehat{U}_{im}(\kappa)+\big{(}1-u(\kappa)\big{)}\sqrt{\frac{\ln (4n/\delta)}{2m}}\] \[LCB_{im}(\kappa) =\widehat{U}_{im}(\kappa)-\sqrt{\frac{\ln(4n/\delta)}{2m}}-u( \kappa)(1-\widehat{F}_{im}(\kappa)).\]The next lemma shows that these are good confidence bounds if both the captime \(\kappa\) and the number of samples \(m\) are sufficiently large. With probability at least \(1-\delta\), each algorithm's true expected utility will fall within these confidence bounds, and the width of each confidence range will not be too large.

**Lemma 4**.: _If we do \(m\) runs of each algorithm at captime \(\kappa\), then with probability at least \(1-\delta\) we will have_

\[LCB_{im}\leq U_{i}\leq UCB_{im}\]

_and_

\[UCB_{im}-LCB_{im}\leq 2\sqrt{\frac{\ln(4n/\delta)}{2m}}+u(\kappa)\big{(}1-F_{ i}(\kappa)\big{)}\]

_for all \(i\) simultaneously._

See AppendixA for a full proof. The idea is that the definition of the bounds together with Lemmas2 and3 mean the confidence bounds hold and are accurate.

We can see in Lemma4 that the error in our estimate of an algorithm's expected utility comes from two sources: sampling and capping. The term \(2\sqrt{\frac{\ln(4n/\delta)}{2m}}\) represents the error due to sampling, while the term \(u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}\) represents the error due to capping. To make good guarantees, configuration procedures will need to ensure that both of these terms are sufficiently small.

## 3 Configuration Procedures

We first describe two hypothetical procedures that give us lower bounds on the number of samples and the captime that any configuration procedure will need to use. The bound on the number of samples (Section3.1) is a classic result. In Section3.2 we use a novel "prover-skeptic" argument to show the lower bound on captime. We then describe in Section3.3 a simple usable procedure that returns an \(\epsilon\)-optimal algorithm with probability at least \(1-\delta\), but which suffers from two major drawbacks. First, it requires that we specify an accuracy parameter \(\epsilon\) and a captime \(\kappa\) as input ahead of time. Making poor choices for these parameters can have a large impact on total configuration time (see Section4 for an illustration). Indeed, many choices of \(\epsilon\), and \(\kappa\) are mutually incompatible, giving rise to meaningless bounds. To avoid this, we hope to design procedures that are _anytime_: they continue to improve the accuracy guarantees they can make as they spend more time running. By taking gradually more and more samples, and by gradually taking them at higher and higher captimes, an anytime procedure continues to shrink the \(\epsilon\) that it can guarantee, rather than trying to target a fixed \(\epsilon\) it is given as input. Second, the naive procedure is not input-adaptive in any way. It does the same number of samples at the same captime for every algorithm. Both of these drawbacks are fixed by our UP procedure in Section3.4 UP is an anytime procedure, meaning it requires neither an \(\epsilon\) nor a \(\kappa\) as input, but instead gradually reduces the \(\epsilon\) it can guarantee by increasing both the number of samples and the captime.

### Hypothetical Runtime Oracle Procedure

The unique characteristic of our setting is the fact that we observe capped rather than true runtime samples, and that the cost we pay for each sample is equal to the time we spend collecting it. If this were not the case, then our problem would already be solved. If we had some oracle that we could simply query for the true runtime of an algorithm run on a given instance, then all we would need to do is collect sufficiently many samples from each algorithm. In this case, the optimal procedure simply takes an increasing number of samples of each algorithm and rules out each suboptimal one was soon as it can. The Runtime Oracle Procedure (Algorithm1) is an instantiation of the Successive Elimination algorithm of Even-Dar et al. (2006) (their Algorithm3). With probability at least \(1-\delta\) it will eventually eliminate all algorithms except the optimal one.

**Theorem 1**.: _With probability at least \(1-\delta\) the Runtime Oracle procedure will eventually return the optimal algorithm, and it will return an \(\epsilon\)-optimal algorithm if it is run until \(m\) is large enough that \(2\sqrt{\frac{\ln(4nm^{2}/\delta)}{2m}}\leq\epsilon\). At that point it will have taken \(m\) uncapped samples of each \(\epsilon\)-optimal algorithm,and \(m_{i}\leq m\) uncapped samples of each \(\epsilon\)-suboptimal algorithm \(i\), where \(2\sqrt{\frac{\ln(4nm_{i}^{2}/\delta)}{2m_{i}}}\leq\Delta_{i},\) and where \(\Delta_{i}\) is the difference between the expected utility of an optimal algorithm and of algorithm \(i\)._

See Even-Dar et al. (2006), Theorem 8 and Remark 9 for proofs and details. The Runtime Oracle procedure has two desirable qualities that we would like to preserve in the configuration procedures we design. Firstly, we do not need to supply the parameter \(\epsilon\) ahead of time. Instead, the procedure maintains an internal \(\epsilon\) that it can guarantee, continually shrinking this toward 0. Secondly, the number of samples needed to eliminate \(\epsilon\)-suboptimal algorithms grows with the square of the suboptimality gap \(\Delta_{i}=\max_{i^{\prime}}U_{i^{\prime}}-U_{i}\) rather than the square of \(\epsilon\). Theorem 1 gives an input-dependent guarantee for the Runtime Oracle procedure that we would like to preserve in the usable procedures we design. If some algorithm \(i\) is very suboptimal, with \(\Delta_{i}\gg\epsilon\), then we will be able to eliminate it with many fewer samples than an algorithm that is almost \(\epsilon\)-optimal.

The Runtime Oracle procedure gives us a baseline against which to compare. Its sample complexity guarantee is essentially optimal in the following data-dependent sense.

**Theorem 2** (Theorem 5 of Mannor and Tsitsiklis (2004)).: _There exists some set of input distributions for which every oracle procedure that returns an \(\epsilon\)-optimal algorithm with probability at least \(1-\delta\) must take \(\Omega\big{(}\frac{n-|N(\epsilon)|}{\epsilon^{2}}\log\frac{1}{\delta}+\sum_{i \in N(\epsilon)}\frac{1}{\Delta_{i}^{2}}\log\frac{1}{\delta}\big{)}\) samples, where \(N(\epsilon)\) is the set of \(\epsilon\)-suboptimal algorithms._

### Hypothetical Captime Verification Procedure

We now consider a lower bound on the capture we will be required to use, even in a world where we do not need to take samples. We imagine there are a _prover_ and a _skeptic_. The prover has access to the runtime CDF of each algorithm \(i\). The skeptic will get to see each runtime CDF only up to some \(\kappa_{i}\) that the prover will choose. The prover then recommends an algorithm \(i^{*}\) and the skeptic should be convinced that \(i^{*}\) is \(\epsilon\)-optimal, based only on the truncated CDFs. As the \(\kappa_{i}\)'s tend to infinity, the skeptic will see more and more of the true CDFs and therefore eventually be convinced of the prover's claim (assuming it is true). The goal is to convince the skeptic that \(i^{*}\) is \(\epsilon\)-optimal using \(\kappa_{i}\)'s that are as small as possible. The skeptic will be able to compute the following deterministic boundson an algorithm's expected utility

\[UB_{i} =\int_{0}^{\kappa_{i}}u(t)dF_{i}(t)+u(\kappa_{i})\big{(}1-F_{i}( \kappa_{i})\big{)}\] \[LB_{i} =\int_{0}^{\kappa_{i}}u(t)dF_{i}(t).\]

The skeptic knows that \(LB_{i}\leq U_{i}\leq UB_{i}\), but also that the runtime distributions could be such as to make \(U_{i}\) fall anywhere in this range. So whatever algorithm \(i^{*}\) the prover returns to the skeptic, the skeptic will be convinced that \(i^{*}\) is \(\epsilon\)-optimal if, and only if, they observe \(LB_{i^{*}}\geq UB_{i}-\epsilon\) for all \(i\neq i^{*}\). The following lemma justifies this condition.

**Lemma 5**.: _If \(LB_{i^{*}}\geq UB_{i}-\epsilon\) for all \(i\neq i^{*}\), then \(i^{*}\) is \(\epsilon\)-optimal. If there exists an \(i\neq i^{*}\) with \(LB_{i^{*}}<UB_{i}-\epsilon\), then regardless of the values of the CDFs \(F_{i}\) up to the captures \(\kappa_{i}\), there are some input distributions for which \(i^{*}\) is not \(\epsilon\)-optimal._

The proof works by constructing counter-example CDFs, but takes the CDFs as given up \(\kappa_{i}\), so does not rely on specifying any information available to the skeptic. No matter what the CDF of each \(i\) looks like up to \(\kappa_{i}\), there will be some inputs with \(LB_{i^{*}}<UB_{i}-\epsilon\) for which \(i^{*}\) is not \(\epsilon\)-optimal.

**Lemma 6**.: _The skeptic will be convinced that both \(i^{opt}\) and \(i^{*}=\operatorname*{arg\,max}_{i}LB_{i}\) are \(\epsilon\)-optimal if the captimes \(\kappa_{i}\) are large enough that \(u(\kappa_{i})\big{(}1-F_{i}(\kappa_{i})\big{)}\leq\Delta_{i}+\frac{\epsilon}{2}\) for all algorithms \(i\)._

Algorithm2 describes the prover-skeptic interaction. The next lemma shows that using these captures is essentially optimal. It gives us a baseline minimum captime against which to compare.

**Lemma 7**.: _There are some inputs for which any verification procedure must use captimes \(\kappa_{i}\) large enough that \(u(\kappa_{i})\big{(}1-F_{i}(\kappa_{i})\big{)}\leq\Delta_{i}+\epsilon\) for all algorithms \(i\), or the skeptic will not be convinced that the returned algorithm is \(\epsilon\)-optimal._

The proof simply constructs a counterexample for which doing runs at a smaller captime will lead any configuration procedure to make a wrong conclusion.

### Naive Procedure

We now turn to the setting we are actually interested in, where runs cost time and long ones must eventually be terminated. The simplest thing we could do is just pick a captime (or perhaps we somehow "know" the right captime) and do the necessary number of runs. With a fixed captime, the scenario is not much different from the runtime oracle scenario above. The main difference is that the "resolution" at which we can understand an algorithm's expected utility will be limited by the size of the captime we use to take our samples. We will never know what the runtime CDF looks like beyond the captime, no matter how many samples we take. Given any \(\kappa\), the confidence bounds described in Section2, tell us we could simply choose the smallest \(m\) satisfying \(2\sqrt{\frac{\ln(2n/\delta)}{2m}}+u(\kappa)\leq\epsilon\), do \(m\) runs of each algorithm at captime \(\kappa\), then return the one with largest empirical average utility. We can think of \(2\sqrt{\frac{\ln(2n/\delta)}{2m}}\) as the error due to sampling, and \(u(\kappa)\) as the error due to capping. This is the Naive procedure.

**Theorem 3**.: _With probability at least \(1-\delta\) the Naive procedure returns an \(\epsilon\)-optimal algorithm. It takes enough \(\kappa\)-capped runtime samples of each algorithm to ensure that \(2\sqrt{\frac{\ln(2n/\delta)}{2m}}+u(\kappa)\leq\epsilon\)._

The proof of this and of subsequent theorems are deferred to AppendixA Each essentially follows from the fact that the upper and lower confidence bounds are specified to satisfy Hoeffding's inequality and the union bound. Comparing Theorem3 to Theorem4 we can see the reason for the increased number of samples required when we observe capped instead of uncapped runs: if \(u(\kappa)\) is relatively large, it will take a much larger \(m\) to shrink the term \(2\sqrt{\frac{\ln(2n/\delta)}{2m}}\) enough to satisfy the inequality in Theorem3.

The Naive procedure has some undesirable qualities. Choosing the right \(\kappa\) may not be easy and in Section4 we will show what a difference this choice can make for the total runtime of the Naive procedure. We are also required to specify an \(\epsilon\) beforehand; it may hard to know the "right" \(\epsilon\), and the best \(m\) and \(\kappa\) for one \(\epsilon\) may be quite different from the best for a slightly different \(\epsilon\). What's more, this procedure ignores information about an algorithm's runtime distribution that could be learned by observing runs along the way; it essentially assumes that every algorithm always times out at the given captime. As a result, it takes more samples of \(\epsilon\)-suboptimal algorithms than is necessary. The UP procedure corrects these defects.

### Utilitarian Procrastination

Our Utilitarian Procrastination (UP) procedure starts by doing runs at the smallest captime possible, trying to rule out whatever configurations we can, and only starts doing runs at a larger captime when necessary.

**Theorem 4**.: _With probability at least \(1-\delta\) UP eventually returns the optimal algorithm and it returns an \(\epsilon\)-optimal algorithm if it is run until \(m\) is large enough that \(2\sqrt{\frac{\ln(11nm^{2}(\log\kappa_{opt}+1)^{2}/\delta)}{2m}}+u(\kappa_{i^{opt }})\big{(}1-F_{i^{opt}}(\kappa_{i^{opt}})\big{)}\leq\epsilon\). For any suboptimal \(i\), if \(m,\kappa_{i},\kappa_{i^{opt}}\) are ever large enough that \(2\sqrt{\frac{\ln(11nm^{2}(\log\kappa_{i}+1)^{2}/\delta)}{2m}}+2\sqrt{\frac{ \ln(11nm^{2}(\log\kappa_{i^{opt}}+1)^{2}/\delta)}{2m}}+u(\kappa_{i})\big{(}1-F _{i^{opt}}(\kappa_{i^{opt}})\big{)}+\Delta_{i}\) then \(i\) will be eliminated._

In Theorem4 we can clearly see the analog of both Theorem1 and Theorem3. UP is input dependent in two senses. The condition for ruling out a suboptimal \(i\) depends on \(\Delta_{i}\) and also on the runtime CDFs \(F_{i}\) and \(F_{i^{opt}}\). Just as in the pure sample complexity case of Theorem1 if \(i\) is very suboptimal and \(\Delta_{i}\) is large, it will be easier to satisfy the condition in Theorem4 and thus rule \(i\) out. Additionally, the condition will be easier to satisfy if the inputs are such that either \(i\) or \(i^{opt}\) is able to complete a lot of runs at their respective captimes, so that their CDFs are close to 1. Theorem4 also implies the following theorem and corollary, which together constitute our main theoretical result.

**Theorem 5**.: _For any \(m\), let \(\epsilon=3\sqrt{\frac{\ln(11nm^{4}/\delta)}{2m}}\). Then with probability at least \(1-\delta\), UP returns an \(\epsilon\)-optimal algorithm once it takes \(m\) samples. At that point, if \(\kappa_{i}^{*}\) is the largest captime algorithm \(i\) has been run with then \(\kappa_{i}^{*}\leq 2\inf\left\{\kappa~{}:~{}u(\kappa)\big{(}1-F_{i}(\kappa) \big{)}<\frac{\epsilon}{3\sqrt{2}}\right\}\)._

The proof follows from Theorem4 and from UP's specific choice of captime doubling condition. We note that there is room for improvement, but that this captime bound is comparable to the worst-case lower bound captime needed by any configuration procedure as presented in Section3.2 The smallest captime that satisfies \(u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}<\frac{\epsilon}{3\sqrt{2}}\) needs to be larger than the smallest captime that satisfies \(u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}<\Delta_{i}+\epsilon\), which is the best we can hope to do.

**Corollary 1**.: _With probability at least \(1-\delta\), UP returns an \(\epsilon\)-optimal algorithm after taking a number of samples that at most a logarithmic factor more than is optimally required by any procedure, and at a captime that is a constant larger than \(\inf\left\{\kappa~{}:~{}u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}<\frac{\epsilon} {3\sqrt{2}}\right\}\)._

Proof.: The number of samples follows immediately from the definition of \(\epsilon\) in Theorem5. Because the captimes are always doubled, the total time spent by UP on any single instance is at most a constant times larger than the time spent running that instance the final time it was run. Since the captime at that point was at most \(\kappa_{i}^{*}\), the total time spent running any instance is at most a constant times \(\inf\left\{\kappa~{}:~{}u(\kappa)\big{(}1-F_{i}(\kappa)\big{)}<\frac{\epsilon} {3\sqrt{2}}\right\}\). 

## 4 Experiments

We now illustrate the runtime costs of utilitarian algorithm configuration and the impacts of the adaptive improvements offered by UP over Naive. We would have liked to go further, and in particular to offer comparisons against baselines other than Naive. However, we saw no straightforward way of doing so: there is simply no previous work on offering algorithm configuration in the utility-maximizing setting. Our paper focuses on algorithm configuration with theoretical guarantees. Theredo exist many methods with guarantees in the runtime minimizing setting; we could have run existing procedures like Structured Procrastination, LeapsAndBounds, or CapsAndRuns and then evaluated them in terms of a utilitarian objective. However, such an apples-to-oranges comparison would likely have yielded very poor performance and regardless would have dispensed with the guarantees that motivate these methods. Second, we could have compared to heuristic methods like SMAC, ParamILS and GGA. Again, however, they optimize a different objective function. It is possible to imagine modifying one of these algorithms to optimize utility, but doing so would require fundamental algorithmic changes (e.g., to their so-called adaptive capping heuristics) and nontrivial software engineering effort. Even if we could have made such changes in a non-controversial way, comparing heuristic methods to methods offering guarantees would again be an apples-to-oranges comparison. Perhaps for these reasons, we note that no paper of which we are aware has yet compared any heuristic algorithm configuration method to any offering theoretical guarantees. We do intend to pursue such an investigation ourselves in future work, but anticipate that a careful study of this question will require an entire research paper.

Experimental Setup.We leverage three datasets from [20]. The first is a set of runtimes for the minist SAT solver on data generated by the CNFuzzdd instance generator. The others are sets of runtimes for the CPLEX integer program solver on the combinatorial auction winner determination instances (regions) and on woodpecker conservation problems (rcw); see Appendix D of [20] for details. We only used the first seed for the CPLEX datasets1.

Footnote 1: Code to reproduce all plots can be found at https://github.com/drgrhm/utilitarian-ac

Choosing the Right Captime Matters.In Fig.1 we plot the total configuration time of the Naive procedure with different input captures \(\kappa\) on three datasets from [20] using a log-Laplace utility function from [20]: \(u(t)=u_{LL}(t;60,1)=1-\frac{t}{2}\frac{t}{60}\) if \(t\leq 60\) and \(u(t)=\frac{1}{2}\frac{60}{t}\) otherwise. We used \(\epsilon\) values of \(0.1\), \(0.15\) and \(0.2\) and set \(\delta=0.1\). The first observation we make is that choosing a bad captime can have a large effect on total configuration time, especially for smaller \(\epsilon\) values. The second observation is that, for any fixed \(\kappa\), different values of \(\epsilon\) can also have huge effects on total runtime of the configuration procedure. Both of these points help to emphasize the need for a procedure like UP that starts out with a small \(\kappa\), only increasing it as needed, and refines the \(\epsilon\) it can guarantee "on the fly", based on the runs it has observed.

Anytime Speedups Matter.In Fig.2 we plot the total configuration time of UP and Naive as a function of \(\epsilon\). We used the same log-Laplace utility function as above and set \(\delta=0.1\) (within reasonable ranges, the value of \(\delta\) has relatively little effect on total runtime). UP can drastically outperform Naive, especially for smaller values of \(\epsilon\).

Unsurprisingly, being anytime in \(\kappa\) helps most when we do not know how to provide a good \(\kappa\) as input beforehand. If we somehow guess or know the right captime to use, then it can be hard to beat Naive. But if we use the wrong captime, UP can be much faster than Naive. In Fig.3 we use a uniform utility function from [20], with \(u(t)=u_{unif}(t;60)=1-\frac{t}{60}\) for \(t<60\) and \(0\) otherwise. In the top row, we have used a captime of \(\kappa=60\) for Naive, which is appropriate for this utility function since it is linear up to that point and then \(0\) thereafter. In the bottom row, we have set the captime poorly at \(\kappa=600\), meaning that we are not terminating some runs even though

Figure 1: Runtime of the Naive procedure for different values of input captime \(\kappa\) on three datasets using a log-Laplace utility function. Choosing a bad captime can have a large effect on total configuration time, especially for smaller \(\epsilon\) values.

they are so long they give us \(0\) utility. We can see that if we get the captime right, then Naive will perform quite well. But if we get it wrong, Naive can drastically underperform compared to UP.

## 5 Conclusion

We have presented Utilitarian Procrastination (UP), a utility-based approach to automated algorithm configuration. This is the first procedure that we know of to incorporate utility functions into the algorithm configuration framework. UP is anytime, meaning it requires minimal input from the user and continues to refine its guarantees as it is run. In simple experiments, we show that by freeing the user from having to provide either an accuracy parameter \(\epsilon\) or a captime \(\kappa\) UP can help avoid the excessively long runtimes that come when these inputs are chosen poorly. Of course we would have liked to have shown that UP never uses a captime larger than \(\kappa_{i}^{*}\leq 2\inf\left\{\kappa\ :\ u(\kappa)\big{(}1-F_{i}(\kappa) \big{)}<\Delta_{i}+\frac{\epsilon}{2}\right\}\), matching the hypothetical procedure. The bound we do show is a result of the condition UP checks when deciding whether to double the the captime \(\kappa_{i}\). UP uses a simple condition that balances the error due to sampling with the error due to capping. Other, more intelligent choices may be possible, but improving this will have to wait for future work.

Figure 3: Runtime of the different configuration procedures for different values of \(\epsilon\) on three different datasets using a uniform utility function. The top row shows a scenario where the Naive procedure has been given an appropriate captime, the bottom row shows a scenario where it has not.

Figure 2: Runtime of the different configuration procedures for different values of \(\epsilon\) on three different datasets using a log-Laplace utility function. UP easily outperforms Naive.

## 6 Acknowledgements

Graham and Leyton-Brown were funded by an NSERC Discovery Grant, a DND/NSERC Discovery Grant Supplement, a CIFAR Canada AI Research Chair (Alberta Machine Intelligence Institute), a Compute Canada RAC Allocation, and DARPA award FA8750-19-2-0222, CFDA #12.910 (Air Force Research Laboratory). Roughgarden's research at Columbia University is supported in part by NSF awards CCF-2006737 and CNS-2212745.

## References

* Ansotegui et al. (2009) Ansotegui, C., Sellmann, M., and Tierney, K. (2009). A gender-based genetic algorithm for the automatic configuration of algorithms. In _International Conference on Principles and Practice of Constraint Programming_, pages 142-157. Springer.
* Balcan et al. (2021) Balcan, M.-F., DeBlasio, D., Dick, T., Kingsford, C., Sandholm, T., and Vitercik, E. (2021). How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 919-932.
* Balcan et al. (2017) Balcan, M.-F., Nagarajan, V., Vitercik, E., and White, C. (2017). Learning-theoretic foundations of algorithm configuration for combinatorial partitioning problems. In _Conference on Learning Theory_, pages 213-274. PMLR.
* Birattari et al. (2002) Birattari, M., Stutzle, T., Paquete, L., and Varrentrapp, K. (2002). A racing algorithm for configuring metaheuristics. In _Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation_, page 11-18. Morgan Kaufmann Publishers Inc.
* Brandt et al. (2023) Brandt, J., Schede, E., Haddenhorst, B., Benggs, V., Hullermeier, E., and Tierney, K. (2023). Ac-band: A combinatorial bandit-based approach to algorithm configuration. _Proceedings of the AAAI Conference on Artificial Intelligence_, 37(10):12355-12363.
* Even-Dar et al. (2006) Even-Dar, E., Mannor, S., Mansour, Y., and Mahadevan, S. (2006). Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6).
* Graham et al. (2023) Graham, D. R., Leyton-Brown, K., and Roughgarden, T. (2023). Formalizing preferences over runtime distributions. In _International Conference on Machine Learning_, pages 11659-11682. PMLR.
* Gupta and Roughgarden (2017) Gupta, R. and Roughgarden, T. (2017). A pac approach to application-specific algorithm selection. _SIAM Journal on Computing_, 46(3):992-1017.
* Hoos and Stutzle (2004) Hoos, H. H. and Stutzle, T. (2004). _Stochastic local search: Foundations and applications_. Elsevier.
* Hutter et al. (2011) Hutter, F., H. Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization for general algorithm configuration. In _International Conference on Learning and Intelligent Optimization_, pages 507-523. Springer.
* Hutter et al. (2009) Hutter, F., Hoos, H. H., Leyton-Brown, K., and Stutzle, T. (2009). Paramils: an automatic algorithm configuration framework. _Journal of Artificial Intelligence Research_, 36:267-306.
* Kleinberg et al. (2017) Kleinberg, R., Leyton-Brown, K., and Lucier, B. (2017). Efficiency through procrastination: Approximately optimal algorithm configuration with runtime guarantees. In _IJCAI_, volume 3, page 1.
* Kleinberg et al. (2019) Kleinberg, R., Leyton-Brown, K., Lucier, B., and Graham, D. (2019). Procrastinating with confidence: Near-optimal, anytime, adaptive algorithm configuration. _arXiv preprint arXiv:1902.05454_.
* Li et al. (2017) Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2017). Hyperband: A novel bandit-based approach to hyperparameter optimization. _The journal of machine learning research_, 18(1):6765-6816.
* Lopez-Ibanez et al. (2016) Lopez-Ibanez, M., Dubois-Lacoste, J., Caceres, L. P., Birattari, M., and Stutzle, T. (2016). The irace package: Iterated racing for automatic algorithm configuration. _Operations Research Perspectives_, 3:43-58.
* Lopez-Ibanez et al. (2017)Mannor, S. and Tsitsiklis, J. N. (2004). The sample complexity of exploration in the multi-armed bandit problem. _Journal of Machine Learning Research_, 5(Jun):623-648.
* Tornede et al. (2020) Tornede, A., Wever, M., Werner, S., Mohr, F., and Hullermeier, E. (2020). Run2survive: a decision-theoretic approach to algorithm selection based on survival analysis. In _Asian Conference on Machine Learning_, pages 737-752. PMLR.
* Von Neumann and Morgenstern (1944) Von Neumann, J. and Morgenstern, O. (1944). _Theory of games and economic behavior_. Princeton university press.
* Weisz et al. (2020) Weisz, G., Gyorgy, A., Lin, W.-I., Graham, D., Leyton-Brown, K., Szepesvari, C., and Lucier, B. (2020). Impatientcapsandruns: Approximately optimal algorithm configuration from an infinite pool. _Advances in Neural Information Processing Systems_, 33:17478-17488.
* Weisz et al. (2018) Weisz, G., Gyorgy, A., and Szepesvari, C. (2018). Leapsandbounds: A method for approximately optimal algorithm configuration. In _International Conference on Machine Learning_, pages 5257-5265. PMLR.
* Weisz et al. (2019) Weisz, G., Gyorgy, A., and Szepesvari, C. (2019). Capsandruns: An improved method for approximately optimal algorithm configuration. In _International Conference on Machine Learning_, pages 6707-6715. PMLR.