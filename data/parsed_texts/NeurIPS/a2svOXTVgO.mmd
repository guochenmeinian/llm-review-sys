# On the Convergence of CART under Sufficient

Impurity Decrease Condition

 Rahul Mazumder

Sloan School of Management

Massachusetts Institute of Technology

rahulmaz@mit.edu

&Haoyue Wang

Operations Research Center

Massachusetts Institute of Technology

haoyuew@mit.edu

This work is supported by Office of Naval Research (N00014-21-1-2841).

###### Abstract

The decision tree is a flexible machine learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we investigate the convergence rate of CART under a regression setting. First, we establish an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition [10] - our result improves upon the known result by [10] under a similar assumption. Furthermore, we provide examples that demonstrate the error bound cannot be further improved by more than a constant or a logarithmic factor. Second, we introduce a set of easily verifiable sufficient conditions for the SID condition. Specifically, we demonstrate that the SID condition can be satisfied in the case of an additive model, provided that the component functions adhere to a "locally reverse Poincare inequality". We discuss several well-known function classes in non-parametric estimation to illustrate the practical utility of this concept.

## 1 Introduction

The decision tree [5] is one of the most fundamental and popular methods in the machine learning toolbox. It utilizes a flowchart-like structure to recursively partition the data and allows users to derive interpretable decisions. It is usually constructed in a data-dependent greedy manner, with an algorithm called CART [5]. Thanks to its computational efficiency and ability to capture nonlinear structures, decision trees have served as the foundation for various influential algorithms for ensemble learning, including bagging [6], random forest [7] and gradient boosting [11]. These algorithms are among the best-known nonparametric models for supervised learning with tabular data [14].

Despite its remarkable empirical success in various applications, our theoretical understanding of decision trees remains somewhat limited. The data-dependent splits employed in CART pose challenges for rigorous theoretical analysis. While some easy-to-analyze variants of CART have been widely studied [18; 4; 3], rigorous theoretical analysis of the original version of CART has been absent until recently. In particular, a recent line of work [19; 24; 10; 16] has established the consistency and prediction error bounds for CART and random forest. See Section 1.1 for a comprehensive review of related literature.

In this paper, we study the decision tree under a nonparametric regression model:

\[y_{i}=f^{*}(x_{i})+\epsilon_{i}\quad i\in[n]\] (1)

with \(x_{i}=(x_{i}^{(1)},...,x_{i}^{(p)})\in\mathbb{R}^{p}\), \(y_{i}\in\mathbb{R}\); \(\epsilon_{i}\) is the noise with zero-mean, and \(f^{*}(x)\) is the signal function. We consider a random design where \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. from some underlying distribution \(\mu\) supported on \([0,1]^{p}\), and \(\{\epsilon_{i}\}_{i=1}^{n}\) are i.i.d. independent of \(\{x_{i}\}_{i=1}^{n}\). For an estimator \(\hat{f}\) of \(f^{*}\), we usethe \(L^{2}\) prediction error \(\left\|\hat{f}-f^{*}\right\|_{L^{2}(\mu)}^{2}\) as a measure of the estimator \(\hat{f}\). Consider a regression tree algorithm that produces a heuristic (not necessarily exact) solution \(\hat{f}\) of

\[\min_{f\in\mathcal{T}_{d}}\ \sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}\] (2)

where \(\mathcal{T}_{d}\) is the space of binary (i.e. each splitting node has two children) axis-aligned regression trees with maximum depth \(d\). The statistical performance of the estimator \(\hat{f}\), therefore, is affected by the following three factors: (i) Approximation error: how accurately can \(f^{*}\) be approximated by a regression tree in \(\mathcal{T}_{d}\). (ii) Estimation error: the variance of the estimator, which is affected by the complexity of the function space \(\mathcal{T}_{d}\). A larger space \(\mathcal{T}_{d}\) (i.e. larger \(d\)) can reduce the approximation error, but it may also increase the estimation error and lead to overfitting. (iii) Optimization error: the discrepancy between \(\hat{f}\) and the exact optimal solution of the optimization problem (2). Indeed, if \(\hat{f}\) is the exact optimal solution of (2) (i.e. optimal regression tree), it can approximate a broad class of \(f^{*}\)[9], and the prediction error can be analyzed via the classical theory of least-squares estimators [22; 13]. However, since solving (2) to optimality is computationally challenging for large-scale data, heuristics like CART are typically employed in practice. To understand the statistical performance of CART, the major difficulty lies in the analysis of the optimization error of CART.

As a greedy algorithm, CART examines only one split in each iteration (see Section 1.3 for details). Although decision trees are intended to capture multivariate structures across different features, the greedy nature of CART poses challenges in approximating functions \(f^{*}\) that have complex multivariate structures. For example, consider the two-dimensional "XOR gate" function \(f^{*}(u):=1_{\{u\in[0,1/2)^{2}\}}+1_{\{u\in[1/2,1)^{2}\}}\)[14], and assume that \(x_{i}\) has a uniform distribution on \([0,1]^{2}\). Note that \(f^{*}\) itself is a depth-2 decision tree, with (e.g.) the root node split with the first coordinate at \(1/2\), and the left and right children split with the second coordinate at \(1/2\). However, due to the symmetry structure of \(f^{*}\), when the sample size \(n\) is large, the root split of CART is completely fitting to noises and is likely to generate a split near the boundary [5; 8]. Consequently, the resulting estimator \(\hat{f}\) fails to capture the true tree structure encoded in \(f^{*}\). The major difficulty in fitting the XOR gate function with CART lies in the complicated interaction between the two coordinates.

The discussions above naturally lead to the following two questions about CART:

* What conditions on \(f^{*}\) are needed such that CART can approximate \(f^{*}\) and yield a consistent estimator?
* If \(f^{*}\) satisfies such conditions, what is the convergence rate of the prediction error of CART?

In this paper, we aim to address these fundamental questions by providing sufficient conditions on \(f^{*}\) such that one can establish convergence rates of CART. These sufficient conditions are quite flexible to include a broad class of nonparametric models. In the following, we first discuss some related literature and the basics of CART. In Section 2, we prove a general error bound for CART under a _sufficient impurity decrease (SID)_ condition [10] of \(f^{*}\). This error bound is an improvement over the existing results under similar conditions [10]. In Section 3, we introduce a sufficient condition for the SID condition on \(f^{*}\), and use it to show that the SID condition can be satisfied by a broad class of \(f^{*}\).

### Related literature

The inception and analysis of decision trees and CART algorithm can be traced back to the seminal work of Breiman [5]. Breiman's study marked the first significant step towards establishing the consistency of CART, albeit under stringent assumptions concerning tree construction. The original formulation of CART, characterized by data-dependent splits, poses challenges when it comes to rigorous mathematical analysis. To address this issue, subsequent research efforts [18; 4; 3] have introduced alternative variants of CART that lend themselves to more tractable theoretical examinations. For instance, certain studies [12; 4; 3; 2] assume random selection of splitting features in the tree. Other approaches incorporate random selections of splitting thresholds [12; 2], while some restrict splits to occur exclusively at the midpoint of the corresponding cell in the chosen coordinate [3]. These simplifications ensure the independence between the splitting rules of the tree and the training data - the tree depends on the data only via the estimation in each cell. Such independence simplifies the analysis of the resultant trees. In a similar vein, alternative strategies have been proposed, such as the employment of a two-sample approach [23] to compute splitting rules and cell estimations separately, giving rise to what is known as an "honest tree". This methodology also ensures independence between various aspects of the tree construction and estimation process.

Since the first consistency result in [5], the strict theoretical analysis of the original CART algorithm has remained elusive until recent advancements. A significant breakthrough came with the seminal work of Scornet et al. [19], which demonstrated the consistency of CART under the assumption of an additive model [15] with continuous component functions. It makes a technical contribution on the decrease of optimization error of CART. This technical argument strongly relies on the additive model. Subsequent works [17, 16] further extended the theoretical analysis by providing non-asymptotic error bounds for CART under a similar additive model. However, the rates derived in [17, 16] is a slow rate of \(O((\log(n))^{-1})\), which seems not easy to be improved under the same assumption [8]. Although the additive model is a well-studied model in statistical learning, it is not immediately evident why such a modeling assumption is specifically required for CART to function effectively.

Another line of work [24, 10] adopts more intuitive assumptions on the signal function \(f^{*}\) to ensure the progress of CART. In particular, Chi et al. [10] introduced a sufficient impurity decrease (SID) condition. In essence, the SID condition posits that for every rectangular region \(B\) within the feature space, there exists an axis-aligned split capable of reducing the population variance of \(f^{*}\) (conditional on \(B\)) by a fixed ratio. Note that such a split cannot be directly identified by CART, as it necessitates knowledge of the population distribution of \(x_{i}\). Instead, CART employs empirical variance of \(y_{i=1}^{n}\) to guide its cell splitting decisions. Nonetheless, the SID condition is a strong assumption on the approximation power of tree splits, which can ensure the consistency of CART via an empirical process argument [10]. A similar condition has also appeared implicitly in [24]. Under the SID condition, Chi et al. [10] studied the consistency and convergence rates of CART, demonstrating a polynomial convergence \(O(n^{-\gamma})\) of the prediction error. A possible limitation of the SID condition is the difficulty of verification: it necessitates the decreasing condition to hold true for all rectangles. Although a few examples are provided in [10], the general procedure for verifying whether a specific \(f^{*}\) satisfies this condition remains unclear. Notably, its relationship with the additive model assumption in [19, 17] remains to be fully understood.

Lastly, it is worth mentioning that several recent studies [20, 8] have investigated the performance lower bounds of CART, shedding light on its limitations. Specifically, Tan et al. [20] demonstrated that even when assuming an additive regression function \(f^{*}\), any single-tree estimator still suffers from the curse of dimensionality. Consequently, these estimators can only achieve convergence rates considerably slower than the oracle rates established for additive models [21]. Another study by Cattaneo et al. [8] delved into the splitting behavior of CART and provided theoretical support for empirical observations [5] indicating that CART tends to generate splits near boundaries in cases where the signal is weak. These lower bounds support the importance of using trees in ensemble models to reduce the prediction error. Although we focus on analyzing CART for a single tree in this paper, the result can be adapted to the analysis of ensembles of trees for the fitting of more complex function classes.

### Contributions

The first contribution of our paper is a refined analysis of CART under the SID condition. Although the convergence of CART has been studied in [10] under SID condition, the analysis of [10] seems not tight when the noises have light-tails (see discussions in the Appendix C). We establish an improved analysis of CART under this setting, and show by examples that our error bound is tight up to log factors.

The second contribution of this paper is the decoding of the mystery of the SID condition. We discuss a sufficient condition under which the SID condition holds true. In particular, we introduce a class of univariate functions that satisfy an _locally reverse Poincare inequality_, and show that additive functions with each univariate component being in this class satisfy the SID condition. This builds a connection between the two types of assumptions in the literature: additive model and SID condition. We discuss a few examples that how the locally reverse Poincare inequality can be verified.

### Basics of CART

We review the methodology of CART to build the tree. The primary objective of decision trees is to find optimal partitions of feature space that produce a minimal variance of response variables. CART constructs the tree and minimizes the variance using a top-down greedy approach. It starts with a root node that represents the whole feature space \(\mathbb{R}^{p}\) - this can be viewed as an initial tree with depth \(0\). At each iteration, CART splits all the leave nodes in the current tree into a left child and a right child; the depth of the tree increases by \(1\), and the number of leave nodes in the tree doubles. At each leave node, it takes an _axis-aligned splits_ with maximum variance (impurity) decrease. Let \(\mathcal{E}\) be the set of all intervals (which can be open, closed, or open on one side and closed on another side). Define the set of rectangles in \([0,1]^{p}\) as:

\[\mathcal{A}:=\Big{\{}\prod_{j=1}^{p}E_{j}\;\Big{|}\;E_{j}\in\mathcal{E}\;\; \forall\;j\in[p]\Big{\}}\] (3)

For each \(A\in\mathcal{A}\), define \(\mathcal{I}_{A}:=\{i\in[n]\;|\;x_{i}\in A\}\). For a leave node that represents a rectangle \(A\in\mathcal{A}\), the impurity decrease of a split in feature \(j\in[p]\) with threshold \(b\in\mathbb{R}\) is given by:

\[\widehat{\Delta}(A,j,b):=\frac{1}{n}\sum_{i\in\mathcal{I}_{A}}(y_{i}-\bar{y}_ {\mathcal{I}_{A}})^{2}-\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{L}}}(y_{i}-\bar{y }_{\mathcal{I}_{A_{L}}})^{2}-\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{R}}}(y_{i}- \bar{y}_{\mathcal{I}_{A_{R}}})^{2}\] (4)

where \(y_{\mathcal{I}}:=\frac{1}{|\mathcal{I}|}\sum_{i\in\mathcal{I}}y_{i}\) for any \(\mathcal{I}\subseteq[n]\), and

\[\begin{split} A_{L}&=A_{L}(j,b):=A\cap\{v\in[0,1]^{ p}\;|\;v_{j}\leq b\},\\ A_{R}&=A_{R}(j,b):=A\cap\{v\in[0,1]^{p}\;|\;v_{j}>b\}.\end{split}\] (5)

To split on \(A\), CART takes \(\hat{j}\) and \(\hat{b}\) that maximize the impurity decrease, i.e.,

\[\hat{j},\hat{b}\in\operatorname*{argmax}_{j\in[p],b\in[0,1]}\widehat{\Delta} (A,j,b)\] (6)

This splitting procedure is repeated until the tree has grown to a given maximum depth \(d\).

### Notations

For \(a,b>0\), we write \(a=O(b)\) if there exists a universal constant \(C>0\) such that \(a\leq Cb\); we write \(b=\Omega(a)\) if \(a=O(b)\), and write \(a=\Theta(b)\) if both \(a=O(b)\) and \(a=\Omega(b)\) are true. For values \(a_{n,p},b_{n,p}\) that may depend on \(n\) and \(p\), we write \(a_{n,p}=\widetilde{O}(b_{n,p})\) if \(a_{n,p}=O(b_{n,p}\log^{\gamma}(np))\) for some fixed constant \(\gamma\geq 0\).

## 2 Error bound of CART under SID property

We study the prediction error bound of CART for the regression problem (1). We focus on a random design as in the following assumption.

**Assumption 2.1**: _(i) (Random design) Suppose \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. random variables with a distribution \(\mu\) supported on \([0,1]^{p}\). Suppose \(\mu\) has a density \(p_{X}\) (w.r.t. Lebesgue measure) on \([0,1]^{p}\) satisfying \(0<\underline{\theta}\leq p_{X}(u)\leq\overline{\theta}<\infty\) for all \(u\in[0,1]^{p}\) for some constants \(\underline{\theta}\leq 1\) and \(\overline{\theta}\geq 1\)._

_(ii) (Error distribution) Suppose \(\{\epsilon_{i}\}_{i=1}^{n}\) are i.i.d. zero-mean bounded random variables with \(|\epsilon_{i}|\leq m<\infty\) for some \(m>0\). Suppose \(\{\epsilon_{i}\}_{i=1}^{n}\) are independent to \(\{x_{i}\}_{i=1}^{n}\)._

_(iii) (Bounded signal function) Suppose \(\sup_{u\in[0,1]^{p}}|f^{*}(u)|\leq M<\infty\) for some constant \(M\)._

In Assumption 2.1 (i), it is assumed that the features \(x_{i}\) are supported on \([0,1]^{p}\) for convenience. The same results (different by at most a constant) can be proved if we replace \([0,1]^{p}\) with an arbitrary bounded rectangle. The second assumption (Assumption 2.1 (ii)) requires the error terms \(\{\epsilon_{i}\}_{i=1}^{n}\) to be i.i.d. bounded - this is the key difference of our paper and the assumptions made in [10]. In particular, [10] made a milder assumption on the error terms that allows them to have heavy tails. Their result, however, when specifying to the case of bounded noises, is not tight (seediscussions in Appendix C for a comparison). Note that for convenience we have assumed that \(\epsilon_{i}\) is bounded. If instead, we assume the noises are sub-Gaussian, similar conclusions can also be proved via a truncation argument (the bound may be different by a log factor). The third assumption (Assumption 2.1 (iii)) is a standard assumption for nonparametric regression models [22, 13].

Under the random design setting in Assumption 2.1, we are ready to introduce the sufficient impurity decrease condition on the underlying function \(f^{*}\). Let \(X\) be a random variable in \(\mathbb{R}^{p}\) that has the same distribution as \(x_{1}\) and is independent to \(\{x_{1},...,x_{n}\}\). For any \(A\in\mathcal{A}\), \(j\in[p]\) and \(b\in\mathbb{R}\), define:

\[\Delta(A,j,b):=\mathbb{P}(X\in A)\mathrm{Var}(f^{*}(X)|X\in A)- \mathbb{P}(X\in A_{L})\mathrm{Var}(f^{*}(X)|X\in A_{L})\] (7) \[\qquad\qquad-\mathbb{P}(X\in A_{R})\mathrm{Var}(f^{*}(X)|X\in A _{R})\]

where \(A_{L}\) and \(A_{R}\) are defined in (5), and \(\mathrm{Var}(f^{*}(X)|X\in A)\) means the conditional variance defined as

\[\mathrm{Var}(f^{*}(X)|X\in A):=\mathbb{E}\Big{(}\big{(}f^{*}(X)-\mathbb{E}(f^ {*}(X)|X\in A)\big{)}^{2}\Big{|}X\in A\Big{)}\] (8)

It is not hard to see the similarity between \(\Delta(A,j,b)\) and \(\widehat{\Delta}(A,j,b)\) (defined in (4)), where the former can be viewed as a population variant of the latter. Intuitively, for a split with feature \(j\) and threshold \(b\), \(\Delta(A,j,b)\) measures variance decrease of \(f^{*}\) after this split. The SID condition below assumes that there is a split with a large variance decrease.

**Assumption 2.2**: _(Sufficient Impurity Decrease) There exists a constant \(\lambda\in(0,1]\) such that for all \(A\in\mathcal{A}\),_

\[\sup_{j\in[p],b\in\mathbb{R}}\Delta(A,j,b)\;\geq\;\lambda\cdot\mathbb{P}(X\in A )\mathrm{Var}(f^{*}(X)|X\in A)\] (9)

Note that Assumption 2.2 is a condition depending on both the function \(f^{*}\) and the underlying distribution \(X\sim\mu\). Briefly speaking, the SID condition requires that the best split must decrease the population variance by a constant factor (note that \(\lambda\) is required to be bounded away from \(0\)). Intuitively, the condition (9) can be satisfied if \(f^{*}\) has a significant "trend" along some axis in \(A\), but may be hard to be satisfied if \(f^{*}\) is relatively "flat" in \(A\). Since Assumption 2.2 requires (9) being satisfied for all cells \(A\in\mathcal{A}\), it essentially requires that \(f^{*}\) is not "too flat" on any local rectangle \(A\). In Section 3, we develop a technical argument to translate this intuition into a rigorous statement, and use it to check Assumption 2.2 for a wide class of functions.

With Assumptions 2.1 and 2.2 at hand, we are ready to present the main result in this section. For any function \(g:\mathbb{R}^{p}\to\mathbb{R}\), let \(\|g\|_{L^{2}(\mu)}\) denote the \(L^{2}\)-norm of \(g\) with respect to the measure \(\mu\).

**Theorem 2.3**: _Suppose Assumptions 2.1 and 2.2 hold true. Let \(\widehat{f}^{(d)}(\cdot)\) be the tree estimated by CART with depth \(d\). Suppose \(n\) is large enough such that \(\frac{2\tilde{\theta}e^{2}d}{n}\vee\frac{\log(72p^{d}(n+1)^{2d}/\delta)}{n^{ \phi}(\lambda)}<3/4\). Then there exists a universal constant \(C>0\) such that with probability at least \(1-\delta\), it holds that for any \(\alpha>0\),_

\[\|\widehat{f}^{(d)}-f^{*}\|_{L^{2}(X)}^{2}\leq 2\mathrm{Var}(f^{*}(X))\cdot \Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}^{d}+C\frac{2^{d}(d\log(np)+\log (1/\delta))}{\alpha n}U^{2}\] (10)

_where \(U:=M+m\). In particular, taking \(\alpha=1/d\) and \(d=\lceil\log_{2}(n)/(1-\log_{2}(1-\lambda))\rceil\), it holds_

\[\|\widehat{f}^{(d)}-f^{*}\|_{L^{2}(X)}^{2}\leq C_{\lambda,U}\frac{\log^{2}(n) \log(np)+\log(n)\log(1/\delta)}{n^{\phi(\lambda)}}\] (11)

_where \(\phi(\lambda):=\frac{-\log_{2}(1-\lambda)}{1-\log_{2}(1-\lambda)}\), and \(C_{\lambda,U}\) is a constant that only depends on \(\lambda\) and \(U\)._

The error bound in (10) consists of two terms. The first term corresponds to the bias of the approximating \(f^{*}\) using CART. It decreases geometrically as \(d\) increases, which is suggested by the SID condition. The second term is \(O(2^{d}d\log(np)/n)\), which corresponds to the estimation variance of CART. Here the term \(2^{d}\) represents the model complexity - a fully-grown depth \(d\) tree has \(2^{d}\) leaves. The proof of Theorem 2.3 is presented in Appendix A. It is based on a careful technical argument that controls the difference between the population impurity decrease \(\Delta(A,j,b)\) and the empirical impurity decrease \(\widehat{\Delta}(A,j,b)\). In particular, our analysis is different from that of [10]. Note that [10] makes a different assumption on the error distribution, and their technical argument is not sufficient for the proof of Theorem 2.3. Particularly, the error bound in (11) is an improvement of the results proved in Theorem 1 of [10]--see Section C for a detailed comparison.

Note that the convergence rate in (11) has a crucial dependence on the coefficient \(\lambda\). For a large value of \(\lambda\), the exponent \(\phi(\lambda)\) is also larger, leading to a faster convergence rate in (11). On the other end, if \(\lambda\) is small, as \(\lambda\to 0\), we have \(\phi(\lambda)\to 0\), and the convergence rate in (11) can be arbitrarily slow. Nonetheless, as long as the SID condition is satisfied with \(\lambda\) bounded away from \(0\), (11) shows a polynomial rate, which is better than the logarithmic rate in [17, 16] without the SID condition.

To explore the tightness of the error bounds in Theorem 2.3, we consider a simple example with \(f^{*}\) being a univariate linear function.

**Example 2.1**: _(Univariate linear function) Suppose \(p=1\), and \(x_{i}\) are i.i.d. from the uniform distribution on \([0,1]\). Suppose \(f^{*}\) is a univariate linear function: \(f^{*}(t)=t\). Then the SID condition (Assumption 2.2) is satisfied with coefficient \(\lambda=3/4\). As a result, it holds \(\phi(\lambda)=2/3\), and the error bound in the RHS of (11) is \(\widetilde{O}(n^{-2/3})\)._

_Proof._ For any interval \(A=[\ell,r]\), we have

\[\mathbb{P}(X\in A)\mathrm{Var}(f^{*}(X)|X\in A)=\int_{\ell}^{r}\left(t-\frac{ \ell+r}{2}\right)^{2}\mathrm{d}t=\frac{(r-\ell)^{3}}{12}\] (12)

Take \(b:=(\ell+r)/2\), then we have

\[\begin{split}\Delta(A,1,b)&=\int_{\ell}^{r}\left(t -\frac{\ell+r}{2}\right)^{2}\mathrm{d}t-\int_{\ell}^{b}\left(t-\frac{\ell+b}{ 2}\right)^{2}\mathrm{d}t-\int_{b}^{r}\left(t-\frac{b+r}{2}\right)^{2}\mathrm{ d}t\\ &=\frac{1}{12}(r-\ell)^{3}-\frac{1}{12}(b-\ell)^{3}-\frac{1}{12}( r-b)^{3}=\frac{3}{48}(r-\ell)^{3}\end{split}\] (13)

Combining (12) and (13) we know that Assumption 2.2 is satisfied with \(\lambda=3/4\). \(\square\)

As shown by Example 2.1, when \(f^{*}\) is a univariate linear function and \(\{x_{i}\}_{i=1}^{n}\) are from a uniform distribution, the error bound in (11) reduces to \(\widetilde{O}(n^{-2/3})\). This rate matches the lower bound for any partition-based estimator (see e.g. [13] Chapter 4), 2 hence showing that the upper bound in (11) cannot be improved (by more than a log factor) via any refined analysis. To verify the rate \(\widetilde{O}(n^{-2/3})\), we conduct a simulation shown by Figure 1. In particular, Figure 1 presents the log-log plot of the prediction error of CART versus different sample sizes, where the depth \(d\) is taken as stated above (11). A reference line of slope \(-2/3\) is also shown. It can be seen that the slope of the line for CART is also roughly \(-2/3\), which is consistent with the rate \(\widetilde{O}(n^{-2/3})\) derived in Example 2.1.

Footnote 2: There is a subtle difference between the lower bound in [13] Chapter 4 and the upper bound in Theorem 2.3. The lower bound in [13] requires that the splitting points of the partition are independent of the training data, while for CART this condition is not satisfied.

Figure 1: Prediction error of CART versus sample size \(n\) for a univariate linear model.

Function classes satisfying SID condition

In this section, we introduce a class of sufficient conditions that facilitate the easy verification of the SID condition for a broad range of functions. Specifically, we focus on the case when \(f^{*}\) has an additive structure:

\[f^{*}(u)=f^{*}_{1}(u_{1})+f^{*}_{2}(u_{2})+\cdots+f^{*}_{p}(u_{p})\] (14)

where \(f^{*}_{j}\)'s are univariate functions. Note that additive model has also been assumed in prior works such as [19, 17, 16] for the study of CART. If we only assume that each component function \(f^{*}_{j}\) has bounded total variation, then only a slow rate \(O(\log^{-1}(n))\) is known [17]. In the following, we discuss a few sufficient conditions concerning the component functions \(f^{*}_{j}\) under which \(f^{*}\) satisfies the SID condition, consequently guaranteeing a faster convergence rate as indicated by Theorem 2.3.

First, we introduce a key class of univariate functions that satisfy a special integral inequality.

**Definition 3.1**: _(Locally Reverse Poincare Class) For a univariate differentiable function \(g\) on an interval \(Q\subset\mathbb{R}\), we say it belongs to the Locally Reverse Poincare (LRP) class on \(Q\) with parameter \(\tau\) if for any subinterval \([a,b]\subseteq Q\),_

\[\Big{(}\int_{a}^{b}|g^{\prime}(t)|\;\mathrm{d}t\Big{)}^{2}\leq\frac{\tau^{2}} {b-a}\inf_{w\in\mathbb{R}}\int_{a}^{b}|g(t)-w|^{2}\;\mathrm{d}t\] (15)

_We use the notation \(LRP(Q,\tau)\) to denote the class of all such functions._

It is worth noting that for any given univariate differentiable function \(g\) and a fixed interval \([a,b]\), there exists a constant \(\tau\) such that the reverse of inequality (15) holds true, as established by the Poincare inequality. The condition (15) requires a uniform constant \(\tau>0\) such that the reverse of the Poincare inequality holds true. Note that the LHS of (15) is equivalent to the square of the total variation of \(g\) on \([a,b]\), and the RHS is a constant multiple of the conditional variance of \(g\) on \([a,b]\) (under the uniform distribution). Intuitively, this condition can be satisfied when the function \(g\) exhibits a clear "trend" within any interval \([a,b]\) rather than being fuzzy fluctuations. We give a few examples below.

**Example 3.1**: _(Strongly increasing function) Let \(g\) be a strictly increasing function on \([0,1]\) with \(c_{2}\geq g^{\prime}(t)\geq c_{1}>0\) for all \(t\in[0,1]\). Then \(g\in LRP([0,1],2\sqrt{3}c_{2}/c_{1})\)._

Example 3.1 provides a direct generalization of the simple linear function showcased in Example 2.1. Note that this example was also discussed in [10]. In Example 3.1, since \(g^{\prime}(t)\) is consistently bounded away from zero, verifying the inequality 15 becomes straightforward. Below we present a more nuanced example where \(g^{\prime}(t)\) may equal zero at certain point \(t\in[0,1]\).

**Example 3.2**: _(Smooth and strongly convex function) Let \(g\) be a \(L\)-smooth and \(\sigma\)-strongly-convex function on \([0,1]\) for some \(L>\sigma>0\), i.e.,_

\[\frac{\sigma}{2}(t-s)^{2}\leq g(t)-g(s)-g^{\prime}(s)(t-s)\leq\frac{L}{2}(t-s )^{2}\] (16)

_for all \(t,s\in[0,1]\). Then \(g\in LRP([0,1],110(L/\sigma))\)._

Note that for a smooth and strongly convex function \(g\), the gradient \(g^{\prime}(t)\) can be zero at some \(t\in[0,1]\). But the strong convexity condition guarantees that there is at most one point \(t\) where \(g^{\prime}(t)=0\), and the inequality (15) can still be satisfied. Additional details can be found in Appendix B.

**Example 3.3**: _(Polynomial with degree \(r\)) There exists a constant \(C_{r}\) that only depends on \(r\) such that for any univariate polynomial \(g\) of degree at most \(r\), \(g\in LRP(\mathbb{R},C_{r})\)._

Although the set of polynomials (with a degree at most \(r\)) appears to be complicated, it is a finite-dimensional linear space. Therefore the differentiation is a bounded linear operator between finite dimensional normed linear spaces. As a result, the conclusion of Example 3.3 can be proved via a variable transformation argument. See Appendix B for details.

Examples 3.1 - 3.3 demonstrate that the LRP condition can be readily verified for various well-known function classes, but the relationship between the LRP class and the SID condition remains unclear. Below we present the first main result in this section, which establishes a connection between the LRP class and the SID condition under the additive model.

**Proposition 3.1**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has an additive structure as in (14). If \(f^{*}_{k}\in LRP([0,1],\tau)\) for all \(k\in[p]\), then Assumption 2.2 is satisfied with \(\lambda=4\theta/(p\tau^{2}\bar{\theta})\)._

As shown by Proposition 3.1, for the additive model (14), when the basic conditions in Assumption 2.1 are satisfied, the SID condition holds true for \(f^{*}\) as long as each component function is in an LRP class. Particularly, the SID condition is satisfied for component functions from Examples 3.1 - 3.3. Note that the coefficient \(\lambda\) in Proposition 3.1 is proportional to \(1/p\), which implies that it can be very small when \(p\) is large. In particular, with this \(\lambda=\Theta(1/p)\) (with \(\theta,\bar{\theta}\) and \(\tau\) being constants), by Theorem 2.3, the prediction error bound in the RHS of (11) is \(O(n^{-c/p})\) for some constant \(c>0\)3. Since the exponent is dependent on \(p\), the curse of dimensionality becomes evident. This seems to indicate that the analysis of Proposition 3.1 could potentially be refined. However, given the negative results demonstrated by [20; 8], it is likely that for tree-based estimators, even with the additive model, the curse of dimensionality cannot be circumvented.

Footnote 3: The lower bound is based on general additive models which may not satisfy the SID condition. When the SID condition is satisfied, the lower bound may be better.

Proposition 3.1 requires each component of the additive function to lie in an LRP class. Actually, this condition can be further relaxed as in the following proposition.

**Proposition 3.2**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has an additive structure as in (14). Given integer \(r\geq 1\) and constants \(\alpha>0,\beta\geq 1\). Suppose for each \(k\in[p]\) there exist \(0=t_{0}^{(k)}<t_{1}^{(k)}<\cdots<t_{r-1}^{(k)}<t_{r}^{(k)}=1\) with \(t_{j}^{(k)}-t_{j-1}^{(k)}\geq\alpha/r\) and \(f_{k}^{*}\in LRP([t_{j-1},t_{j}],\beta)\) for all \(j\in[r]\). Then Assumption 2.2 is satisfied with parameter \(\lambda=\theta/(p\bar{\theta}\tau^{2})\) where \(\tau^{2}=\max\left\{\frac{2r\bar{\theta}}{\theta},\frac{r^{2}}{2\alpha}\right\} \max\{9\beta^{2},32+\beta^{2}\}\)._

By Proposition 3.2, an additive function \(f^{*}\) satisfies the SID condition if each component function is a piecewise function, with each piece belonging to an LRP class. Notably, continuity at the joining points of consecutive pieces is not necessary. However, for technical reasons, it is required that each piece has a length that is not excessively small, as determined by the parameter \(\alpha\). See Appendix B for the proofs of Proposition 3.1 and 3.2.

## 4 Proof sketch of Theorem 2.3

We provide a sketch of the proof of Theorem 2.3. For \(p\geq 1\) and \(d\geq 1\), define

\[\mathcal{A}_{p,d}:=\Big{\{}\prod_{j=1}^{p}[\ell_{j},u_{j}]\in\mathcal{A}\, \Big{|}\,\#\{j\in[p]\mid[\ell_{j},u_{j}]\neq[0,1]\}\leq d\Big{\}}\] (17)

That is, each rectangle in \(\mathcal{A}_{p,d}\) has at most \(d\) dimensions that are not the full interval \([0,1]\). When \(p\geq d\), \(\mathcal{A}_{p,d}\) contains all the rectangles in \([0,1]^{p}\), i.e. \(\mathcal{A}_{p,d}=\mathcal{A}\). However, when \(d\) is much smaller than \(p\) (in a high-dimensional setting), \(\mathcal{A}_{p,d}\) contains much fewer rectangles than \(\mathcal{A}\). Note that for a decision tree with depth \(d\), each leaf node represents a rectangle in \(\mathcal{A}_{p,d}\).

The proof of Theorem 2.3 builds upon a few technical lemmas which provide uniform bounds between the empirical and populational quantities. These results may hold their own significance in the study of other partition-based algorithms. For \(\delta\in(0,1)\), define values

\[\bar{t}_{1}(\delta) =\bar{t}_{1}(\delta,n,d):=\ \frac{4}{n}\log(2p^{d}(n+1)^{2d}/\delta)\] (18) \[\bar{t}_{2}(\delta) =\bar{t}_{2}(\delta,n,d):=\ \frac{2\bar{\theta}e^{2}d}{n}\vee\frac{ \log(p^{d}(n+1)^{2d}/\delta)}{n}\] \[\bar{t}(\delta) =\bar{t}(\delta,n,d):=\ \bar{t}_{1}(\delta,n,d)\vee\bar{t}_{2}( \delta,n,d)\]

where \(\bar{\theta}\) is the constant in Assumption 2.1\((i)\). Note that we have \(\bar{t}(\delta)\leq O(\frac{d\log(np)+\log(1/\delta)}{n})\). We have the following uniform bounds on empirical and populational mean on every rectangle in \(\mathcal{A}_{p,d}\).

**Lemma 4.1**: _Suppose Assumption 2.1 holds true. Suppose \(\bar{t}_{2}(\delta/12)<3/4\). Then with probability at least \(1-\delta\), it holds_

\[\sup_{A\in\mathcal{A}_{p,d}}\sqrt{\mathbb{P}(X\in A)}\Big{|}\mathbb{E}(f^{*}(X )|X\in A)-\bar{y}_{\mathcal{I}_{A}}\Big{|}\ \leq\ 20U\sqrt{\bar{t}(\delta/12)}\] (19)For a tree-based estimator in practice, the prediction at each leaf node is usually given by the sample average of \(y_{i}\) for all the samples \(i\) routed to that leaf, i.e., the value \(\bar{y}_{A}\) for a leaf with rectangle \(A\). Therefore, Lemma 4.1 provides a uniform bound on the error if we replace the prediction at each leaf by the computational conditional mean \(\mathbb{E}(f^{*}(X)|X\in A)\). It is worth noting that the gap between \(\bar{y}_{A}\) and \(\mathbb{E}(f^{*}(X)|X\in A)\) is rescaled by the quantity \(\sqrt{\mathbb{P}(X\in A)}\). Intuitively, when the rectangle \(A\) is small and hence \(\mathbb{P}(X\in A)\) is small, few points are routed to the rectangle \(A\). As a result, the corresponding gap between \(\bar{y}_{A}\) and \(\mathbb{E}(f^{*}(X)|X\in A)\) can be large - which leads to a large discrepancy of the estimate and the signal locally in this cell \(A\). However, in another perspective, since the cell \(A\) is small, a large discrepancy in \(A\) only makes a small contribution to the overall error \(\|\widehat{f}^{(d)}-f^{*}\|^{2}_{L^{2}(X)}\). Therefore, the factor \(\sqrt{\mathbb{P}(X\in A)}\) in the LHS of (19) is a proper balance of these two aspects. This is the key technical argument that differs from the analysis in [10]. It helps establish the tighter bounds in Theorem 2.3.

**Lemma 4.2**: _Suppose Assumption 2.1 holds true. Given a constant \(\alpha>0\). Given any \(\delta\in(0,1)\), suppose \(\bar{t}_{2}(\delta/36)<3/4\). Then with probability at least \(1-\delta\), it holds_

\[\Delta(A,j,b) \leq(1+\alpha)\widehat{\Delta}(A,j,b)+(1+1/\alpha)\cdot C^{\prime }U^{2}\bar{t}(\delta/36)\quad\forall\;A\in\mathcal{A}_{p,d-1},\;j\in[p],\;b\in \mathbb{R}\] \[\widehat{\Delta}(A,j,b) \leq(1+\alpha)\Delta(A,j,b)+(1+1/\alpha)\cdot C^{\prime}U^{2}\bar {t}(\delta/36)\quad\forall\;A\in\mathcal{A}_{p,d-1},\;j\in[p],\;b\in\mathbb{R}\]

_for some universal constant \(C^{\prime}>0\)._

Lemma 4.2 establishes upper bounds on the discrepancy between empirical and populational impurity decrease. It builds an avenue to translate the SID condition into an empirical counterpart, which is further used to derive the decrease of objective value.

With Lemmas 4.1 and 4.2 at hand, we are ready to wrap up the proof of Theorem 2.3. First, we have

\[\|\widehat{f}^{(k)}-f^{*}\|^{2}_{L^{2}(X)}\;\leq\;2\|f^{*}-\widetilde{f}^{(k) }\|^{2}_{L^{2}(X)}+2\|\widetilde{f}^{(k)}-\widehat{f}^{(k)}\|^{2}_{L^{2}(X)} \;:=\;2J_{1}(k)+2J_{2}(k)\] (20)

where \(\widetilde{f}^{(k)}\) is a tree with the same splitting rule as \(\widehat{f}^{(k)}\) but leaf predictions replaced by the populational means (in that leaf). The term \(J_{2}(k)\) can be bounded as

\[J_{2}(k)\;=\;\sum_{t\in\mathcal{L}^{(k)}}\mathbb{P}(X\in A_{t})\big{(}\mathbb{ E}(f^{*}(X)|X\in A_{t},\mathcal{X}_{1}^{n})-\bar{y}_{\mathcal{I}_{A_{t}}} \big{)}^{2}\leq CU^{2}2^{k}\cdot\frac{k\log(np)+\log(1/\delta)}{n}\]

where \(\mathcal{L}^{(k)}\) is the set of leaves of \(\widehat{f}^{(k)}\) at depth \(k\); \(A_{t}\) is the corresponding rectangle for a leaf \(t\); and \(C\) is a universal constant. The last inequality in (21) is by Lemma 4.1. The term \(J_{1}(k)\) can be written as

\[J_{1}(k)\;=\;\sum_{t\in\mathcal{L}^{(k)}}\mathbb{P}(X\in A_{t}|\mathcal{X}_{1} ^{n})\cdot\mathrm{Var}(f^{*}(X)|X\in A_{t},\mathcal{X}_{1}^{n})\]

Making use of Lemma 4.2 and by some algebra (see Appendix A for details), a recursive inequality can be established:

\[J_{1}(k+1)\leq\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}J_{1}(k)+2^{k} \cdot\frac{CU^{2}(k\log(np)+\log(1/\delta))}{n}\]

The proof of Theorem 2.3 is complete by telescoping the inequality above, and combining the upper bounds for \(J_{1}(k)\) and \(J_{2}(k)\) with (20).

## 5 Conclusion and discussions

We have proved an error bound on the prediction error of CART for a regression problem when \(f^{*}\) satisfies the SID condition. This error bound cannot be improved by more than a log factor. We have also discussed a few sufficient conditions under which we can show that an additive model satisfies the SID condition.

One possible limitation of this work is that: it seems that the SID coefficients \(\lambda\) derived in Section 3 are not tight. Since \(\lambda\) appeared in the exponent of the rate in Theorem 2.3, a tighter estimate of \(\lambda\) leads to an improved convergence rate in (11). We leave it as a future work for a more precise analysis of the SID condition.

## References

* [1] Abhineet Agarwal, Yan Shuo Tan, Omer Ronen, Chandan Singh, and Bin Yu. Hierarchical shrinkage: Improving the accuracy and interpretability of tree-based models. In _International Conference on Machine Learning_, pages 111-135. PMLR, 2022.
* [2] Sylvain Arlot and Robin Genuer. Analysis of purely random forests bias. _arXiv preprint arXiv:1407.3939_, 2014.
* [3] Gerard Biau. Analysis of a random forests model. _The Journal of Machine Learning Research_, 13(1):1063-1095, 2012.
* [4] Gerard Biau, Luc Devroye, and Gabor Lugosi. Consistency of random forests and other averaging classifiers. _Journal of Machine Learning Research_, 9(9), 2008.
* [5] L Breiman, JH Friedman, R Olshen, and CJ Stone. Classification and regression trees. 1984.
* [6] Leo Breiman. Bagging predictors. _Machine learning_, 24(2):123-140, 1996.
* [7] Leo Breiman. Random forests. _Machine learning_, 45(1):5-32, 2001.
* [8] Mattias D Cattaneo, Jason M Klusowski, and Peter M Tian. On the pointwise behavior of recursive partitioning and its implications for heterogeneous causal effect estimation. _arXiv preprint arXiv:2211.10805_, 2022.
* [9] Sabyasachi Chatterjee and Subhajit Goswami. Adaptive estimation of multivariate piecewise polynomials and bounded variation functions by optimal decision trees. _The Annals of Statistics_, 49(5):2531-2551, 2021.
* [10] Chien-Ming Chi, Patrick Vossler, Yingying Fan, and Jinchi Lv. Asymptotic properties of high-dimensional random forests. _The Annals of Statistics_, 50(6):3415-3438, 2022.
* [11] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. _Annals of statistics_, pages 1189-1232, 2001.
* [12] Robin Genuer. Variance reduction in purely random forests. _Journal of Nonparametric Statistics_, 24(3):543-562, 2012.
* [13] Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, Harro Walk, et al. _A distribution-free theory of nonparametric regression_, volume 1. Springer, 2002.
* [14] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* [15] Trevor J Hastie. Generalized additive models. In _Statistical models in S_, pages 249-307. Routledge, 2017.
* [16] J Klusowski and P Tian. Large scale prediction with decision trees. _Journal of the American Statistical Association_, 2022.
* [17] Jason M Klusowski. Universal consistency of decision trees in high dimensions. _arXiv preprint arXiv:2104.13881_, 2021.
* [18] Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. _Journal of the American Statistical Association_, 101(474):578-590, 2006.
* [19] Erwan Scornet, Gerard Biau, and Jean-Philippe Vert. Consistency of random forests. _The Annals of Statistics_, 43(4):1716-1741, 2015.
* [20] Yan Shuo Tan, Abhineet Agarwal, and Bin Yu. A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. In _International Conference on Artificial Intelligence and Statistics_, pages 9663-9685. PMLR, 2022.
* [21] Zhiqiang Tan and Cun-Hui Zhang. Doubly penalized estimation in additive regression with high-dimensional data. 2019.
* [22] Alexandre B Tsybakov. Nonparametric estimators. _Introduction to Nonparametric Estimation_, 2009.
* [23] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. _Journal of the American Statistical Association_, 113(523):1228-1242, 2018.
* [24] Stefan Wager and Guenther Walther. Adaptive concentration of regression trees, with application to random forests. _arXiv preprint arXiv:1503.06388_, 2015.

Proof of Theorem 2.3

### Notations

We first define some notations in the context of the model (1). For \(p\geq 1\) and \(d\geq 1\), define

\[\mathcal{A}_{p,d}:=\Big{\{}\prod_{j=1}^{p}[\ell_{j},u_{j}]\in\mathcal{A}\;\Big{|} \;\#\{j\in[p]\;|\;[\ell_{j},u_{j}]\neq[0,1]\}\leq d\Big{\}}\] (21)

That is, each rectangle in \(\mathcal{A}_{p,d}\) has at most \(d\) dimensions that are not the full interval \([0,1]\). Note that for a decision tree with depth \(d\), each leave node represents a rectangle in \(\mathcal{A}_{p,d}\). Furthermore, for \(\delta\in(0,1)\), define values

\[\bar{t}_{1}(\delta)=\bar{t}_{1}(\delta,n,d):=\;\frac{4}{n}\log(2p^{d}(n+1)^{2 d}/\delta)\]

\[\bar{t}_{2}(\delta)=\bar{t}_{2}(\delta,n,d):=\;\frac{2\bar{\theta}\bar{e}^{2}d }{n}\vee\frac{\log(p^{d}(n+1)^{2d}/\delta)}{n}\] (22)

\[\bar{t}(\delta)=\bar{t}(\delta,n,d):=\;\bar{t}_{1}(\delta,n,d)\vee\bar{t}_{2 }(\delta,n,d)\]

where \(\bar{\theta}\) is the constant in Assumption 2.1\((i)\). Note that we have \(\bar{t}(\delta)\leq O(d\log(np/\delta)/n)\).

For two values \(a,b>0\), we write \(a\lesssim b\) if there is a universal constant \(C>0\) such that \(a\leq Cb\). We write \(a\lesssim_{r}b\) if there is a constant \(C_{r}\) that only depends on \(r\) such that \(a\leq C_{r}b\).

### Technical lemmas

Now we can introduce the major technical results to establish the error bound.

**Lemma A.1**: _Suppose Assumption 2.1 holds true. Suppose \(\bar{t}_{2}(\delta/12)<3/4\). Then with probability at least \(1-\delta\), it holds_

\[\sup_{A\in\mathcal{A}_{p,d}}\sqrt{\mathbb{P}(X\in A)}\Big{|}\mathbb{E}(f^{*} (X)|X\in A)-\bar{y}_{\mathcal{I}_{A}}\Big{|}\;\leq\;20U\sqrt{\bar{t}(\delta/1 2)}\] (23)

The proof of Lemma A.1 is presented in Section A.4. Note that Lemma A.1 provides a uniform bound on the gap between the populational mean \(\mathbb{E}(f^{*}(X)|X\in A)\) and the sample mean \(\bar{y}_{\mathcal{I}_{A}}\). This is used to derive the geometric decrease of the bias, using the SID assumption.

**Lemma A.2**: _Suppose Assumption 2.1 holds true. Given any \(\delta\in(0,1)\), suppose \(\bar{t}_{2}(\delta/4)<3/4\). Then with probability at least \(1-\delta\) it holds_

\[\sup_{A\in\mathcal{A}_{p,d}}\Big{|}\sqrt{\mathbb{P}(X\in A)}-\sqrt{| \mathcal{I}_{A}|/n}\;\Big{|}\leq 5\sqrt{\bar{t}(\delta/4)}\] (24)

The proof of Lemma A.2 is presented in Section A.5. Lemma A.2 provides a uniform deviation gap between the square root of probability and sample frequency over all sets in \(\mathcal{A}_{p,d}\). Note that this uniform bound is stronger than a result without a square root (which can be obtained easily via Hoeffding's inequality and a union bound), and is useful to prove the final error bound in Theorem 2.3.

For any rectangle \(A\in\mathcal{A}\), \(j\in[p]\) and \(b\in\mathbb{R}\), define

\[\Delta_{L}(A,j,b) := \mathbb{P}(X\in A_{L})\Big{(}\mathbb{E}(f^{*}(X)|X\in A)-\mathbb{ E}(f^{*}(X)|X\in A_{L})\Big{)}^{2}\] \[\Delta_{R}(A,j,b) := \mathbb{P}(X\in A_{R})\Big{(}\mathbb{E}(f^{*}(X)|X\in A)-\mathbb{ E}(f^{*}(X)|X\in A_{R})\Big{)}^{2}\] \[\widehat{\Delta}_{L}(A,j,b) := \frac{|\mathcal{I}_{A_{L}}|}{n}(\bar{y}_{\mathcal{I}_{A_{L}}}- \bar{y}_{\mathcal{I}_{A}})^{2}\] \[\widehat{\Delta}_{R}(A,j,b) := \frac{|\mathcal{I}_{A_{R}}|}{n}(\bar{y}_{\mathcal{I}_{A_{R}}}- \bar{y}_{\mathcal{I}_{A}})^{2}\]

We have the following identity regarding the impurity decrease of each split.

**Lemma A.3**: _For any rectangle \(A\in\mathcal{A}\), \(j\in[p]\) and \(b\in\mathbb{R}\), it holds_

\[\Delta(A,j,b)=\;\Delta_{L}(A,j,b)+\Delta_{R}(A,j,b)\] (25) \[\widehat{\Delta}(A,j,b)=\;\widehat{\Delta}_{L}(A,j,b)+\widehat{ \Delta}_{R}(A,j,b)\]Proof.: We just present the proof of the second equality. The proof of the first equality can be proved similarly. Note that

\[\begin{split}\widehat{\Delta}(A,j,b)&=\frac{1}{n}\sum _{i\in\mathcal{I}_{A}}(y_{i}-\bar{y}_{\mathcal{I}_{A}})^{2}-\frac{1}{n}\sum_{i \in\mathcal{I}_{A_{L}}}(y_{i}-\bar{y}_{\mathcal{I}_{A_{L}}})^{2}-\frac{1}{n} \sum_{i\in\mathcal{I}_{A_{R}}}(y_{i}-\bar{y}_{\mathcal{I}_{A_{R}}})^{2}\\ &=\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{L}}}\left[(y_{i}-\bar{y}_{ \mathcal{I}_{A}})^{2}-(y_{i}-\bar{y}_{\mathcal{I}_{A_{L}}})^{2}\right]+\frac{1 }{n}\sum_{i\in\mathcal{I}_{A_{R}}}\left[(y_{i}-\bar{y}_{\mathcal{I}_{A}})^{2}- (y_{i}-\bar{y}_{\mathcal{I}_{A_{R}}})^{2}\right]\end{split}\] (26)

For the first term, we have

\[\begin{split}&\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{L}}}\left[(y_{i}- \bar{y}_{\mathcal{I}_{A}})^{2}-(y_{i}-\bar{y}_{\mathcal{I}_{A_{L}}})^{2} \right]\\ &=\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{L}}}\left[(y_{i}-\bar{y}_ {\mathcal{I}_{A}})^{2}-(y_{i}-\bar{y}_{\mathcal{I}_{A}})^{2}-2(y_{i}-\bar{y}_{ \mathcal{I}_{A}})(\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}})-( \bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}})^{2}\right]\\ &=\frac{|\mathcal{I}_{A_{L}}|}{n}(\bar{y}_{\mathcal{I}_{A}}-\bar{ y}_{\mathcal{I}_{A_{L}}})^{2}\;=\;\widehat{\Delta}_{L}(A,j,b)\end{split}\] (27)

Similarly, we have

\[\frac{1}{n}\sum_{i\in\mathcal{I}_{A_{R}}}\left[(y_{i}-\bar{y}_{\mathcal{I}_{A }})^{2}-(y_{i}-\bar{y}_{\mathcal{I}_{A_{R}}})^{2}\right]\;=\;\widehat{\Delta}_ {R}(A,j,b)\] (28)

The proof is complete by combining (26), (27) and (28). 

**Lemma A.4**: _Suppose Assumption 2.1 holds true. Given a constant \(\alpha>0\). Given any \(\delta\in(0,1)\), suppose \(\bar{t}_{2}(\delta/36)<3/4\). Then with probability at least \(1-\delta\), it holds_

\[\Delta(A,j,b)\leq(1+\alpha)\widehat{\Delta}(A,j,b)+(1+1/\alpha)\cdot 5000U^{2} \bar{t}(\delta/36)\quad\forall\;A\in\mathcal{A}_{p,d-1},\;j\in[p],\;b\in\mathbb{R}\] (29)

_and_

\[\widehat{\Delta}(A,j,b)\leq(1+\alpha)\Delta(A,j,b)+(1+1/\alpha)\cdot 5000U^{2} \bar{t}(\delta/36)\quad\forall\;A\in\mathcal{A}_{p,d-1},\;j\in[p],\;b\in \mathbb{R}\] (30)

Proof.: For \(A\in\mathcal{A}_{p,d-1}\), \(j\in[p]\) and \(a\in\mathbb{R}\), by Lemma A.3 we have

\[\begin{split}\Delta(A,j,b)&=\;\Delta_{L}(A,j,b)+ \Delta_{R}(A,j,b)\\ \widehat{\Delta}(A,j,b)&=\;\widehat{\Delta}_{L}(A,j,b)+ \widehat{\Delta}_{R}(A,j,b)\end{split}\] (31)

Define the events \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\):

\[\begin{split}\mathcal{E}_{1}&:=\left\{\sup_{A\in \mathcal{A}_{p,d}}\sqrt{\mathbb{P}(X\in A)}\Big{|}\mathbb{E}(f^{*}(X)|X\in A)- \bar{y}_{\mathcal{I}_{A}}\Big{|}\;\leq\;20U\sqrt{\bar{t}(\delta/36)}\right\} \\ \mathcal{E}_{2}&:=\left\{\sup_{A\in\mathcal{A}_{p,d}} \Big{|}\sqrt{\mathbb{P}(X\in A)}-\sqrt{|\mathcal{I}_{A}|/n}\;\Big{|}\leq 5 \sqrt{\bar{t}(\delta/12)}\right\}\end{split}\] (32)

Then by Lemmas A.1 and A.2, we have \(\mathbb{P}(\mathcal{E}_{i})\geq 1-\delta/3\) for \(i=1,2\), so we have \(\mathbb{P}(\cap_{i=1}^{2}\mathcal{E}_{i})\geq 1-\delta\). Below we prove (29) and (30) conditioned on the events \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\).

Note that

\[\begin{split}\sqrt{\Delta_{L}(A,j,a)}&=\sqrt{\mathbb{ P}(X\in A_{L})}\Big{|}\mathbb{E}(f^{*}(X)|X\in A)-\mathbb{E}(f^{*}(X)|X\in A_{L}) \Big{|}\\ &\leq\sqrt{\mathbb{P}(X\in A_{L})}\Big{|}\mathbb{E}(f^{*}(X)|X \in A)-\bar{y}_{\mathcal{I}_{A}}\Big{|}+\sqrt{\mathbb{P}(X\in A_{L})}\Big{|} \bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}}\Big{|}\\ &\quad+\sqrt{\mathbb{P}(X\in A_{L})}\Big{|}\bar{y}_{\mathcal{I}_{ A_{L}}}-\mathbb{E}(f^{*}(X)|X\in A_{L})\Big{|}\\ &:=J_{1}+J_{2}+J_{3}\end{split}\] (33)

To bound \(J_{1},\) we have

\[J_{1}\leq\sqrt{\mathbb{P}(X\in A)}\Big{|}\mathbb{E}(f^{*}(X)|X\in A)-\bar{y}_ {\mathcal{I}_{A}}\Big{|}\leq 20U\sqrt{\bar{t}(\delta/36)}\] (34)

where the second inequality is by event \(\mathcal{E}_{1}\). Similarly, to bound \(J_{3}\), we have

\[J_{3}=\sqrt{\mathbb{P}(X\in A_{L})}\Big{|}\bar{y}_{\mathcal{I}_{A_{L}}}-\mathbb{E }(f^{*}(X)|X\in A_{L})\Big{|}\leq 20U\sqrt{\bar{t}(\delta/36)}\] (35)To bound \(J_{2}\), note that

\[J_{2} \leq\Big{|}\sqrt{\mathbb{P}(X\in A_{L})}-\sqrt{|\mathcal{I}_{A_{L}} |/n}\Big{|}\cdot|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}}|+\sqrt {|\mathcal{I}_{A_{L}}|/n}\cdot|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_ {A_{L}}}|\] (35) \[\leq 5\sqrt{t(\delta/12)}\cdot 2U+\sqrt{|\mathcal{I}_{A_{L}}|/n} \cdot|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}}|\]

where the second inequality made use of the event \(\mathcal{E}_{2}\). Combining (32) - (35), we have

\[\sqrt{\Delta_{L}(A,j,b)} \leq 40U\sqrt{t(\delta/36)}+10U\sqrt{t(\delta/12)}+\sqrt{|\mathcal{I }_{A_{L}}|/n}\cdot|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}}|\] \[\leq 50U\sqrt{t(\delta/36)}+\sqrt{|\mathcal{I}_{A_{L}}|/n}\cdot| \bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I}_{A_{L}}}|\]

which implies (by Young's inequality)

\[\Delta_{L}(A,j,a) \leq\ (1+1/\alpha)\cdot 2500U^{2}\bar{t}(\delta/36)+(1+\alpha) \frac{|\mathcal{I}_{A_{L}}|}{n}|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I} _{A_{L}}}|^{2}\] (36)

By a similar argument, we have

\[\Delta_{R}(A,j,a) \leq\ (1+1/\alpha)\cdot 2500U^{2}\bar{t}(\delta/36)+(1+\alpha) \frac{|\mathcal{I}_{A_{R}}|}{n}|\bar{y}_{\mathcal{I}_{A}}-\bar{y}_{\mathcal{I} _{A_{R}}}|^{2}\] (37)

Summing up (36) and (37), and by (31), we have

\[\Delta(A,j,a) \leq\ (1+1/\alpha)\cdot 5000U^{2}\bar{t}(\delta/36)+(1+\alpha) \widehat{\Delta}(A,j,a)\]

This completes the proof of (29). The proof of (30) is by a similar argument.

Lemma A.4 provides upper bounds between \(\Delta(A,j,b)\) and \(\widehat{\Delta}(A,j,b)\), which serves as a link to translate the population impurity decrease to sample impurity decrease. With all these technical lemmas at hand, we are ready to present the proof Theorem 2.3, as shown in the next subsection.

### Completing the proof of Theorem 2.3

Define events

\[\mathcal{E}_{1} :=\Big{\{}\sup_{A\in\mathcal{A}_{p,d}}\sqrt{\mathbb{P}(X\in A)} \Big{|}\mathbb{E}(f^{*}(X)|X\in A)-\bar{y}_{\mathcal{I}_{A}}\Big{|}\ \leq\ 20U\sqrt{t(\delta/24)}\Big{\}}\] \[\mathcal{E}_{2} :=\Big{\{}\Delta(A,j,a)\leq(1+\alpha)\widehat{\Delta}(A,j,a)+(1+ 1/\alpha)\cdot 5000U^{2}\bar{t}(\delta/72)\quad\forall\ A\in\mathcal{A}_{p,d-1}, \ j\in[p],\ a\in\mathbb{R}\Big{\}}\] \[\mathcal{E}_{3} :=\Big{\{}\widehat{\Delta}(A,j,a)\leq(1+\alpha)\Delta(A,j,a)+(1+ 1/\alpha)\cdot 5000U^{2}\bar{t}(\delta/72)\quad\forall\ A\in\mathcal{A}_{p,d-1}, \ j\in[p],\ a\in\mathbb{R}\Big{\}}\]

Then by Lemmas A.1 and A.4, and note that from the statement of Theorem 2.3, \(\bar{t}_{2}(\delta/72)<3/4\), so we have \(\mathbb{P}(\mathcal{E}_{1})\geq 1-\delta/2\) and \(\mathbb{P}(\mathcal{E}_{2}\cup\mathcal{E}_{3})\geq 1-\delta/2\), which implies \(\mathbb{P}(\cup_{i=1}^{3}\mathcal{E}_{i})\geq 1-\delta\). In the following, we prove (10) using a deterministic argument conditioned on \(\cup_{i=1}^{3}\mathcal{E}_{i}\).

For any \(k\in[d]\) and any leave node \(t\) of \(\widehat{f}^{(k)}\) (recall that \(\widehat{f}^{(k)}\) is the decision tree by CART with depth \(k\)), let \(A_{t}^{(k)}\) be the corresponding cube, that is, for any \(x\in\mathbb{R}^{p}\), \(x\in A_{t}^{(k)}\) if and only if \(x\) is routed to \(t\) in \(\widehat{f}^{(k)}\). Let \(\mathcal{L}^{(k)}\) be the set of all leave nodes of \(\widehat{f}^{(k)}\). Then we have

\[\widehat{f}^{(k)}(x)=\sum_{t\in\mathcal{L}^{(k)}}\bar{y}_{\mathcal{I}_{A_{t}^{( k)}}}1_{\{x\in A_{t}^{(k)}\}}\] (38)

Define a function

\[\widehat{f}^{(k)}(x):=\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}\Big{(}f^{*}(X) \Big{|}X\in A_{t}^{(k)},\mathcal{X}_{1}^{n}\Big{)}\cdot 1_{\{x\in A_{t}^{(k)}\}}\] (39)

where \(\mathcal{X}_{1}^{n}\) is the set of iid random variables \(\{x_{1},...,x_{n}\}\), and \(X\) is a random variable having the same distribution as \(x_{1}\) but independent of \(\mathcal{X}_{1}^{n}\). In other words, \(\widehat{f}^{(k)}\) is a tree with the same splitting structure as \(\widehat{f}^{(k)}\) and replaces the prediction value of each leave node as the populational conditional mean of \(f^{*}(\cdot)\).

First, using Cauchy-Schwarz inequality, we have

\[\|\widehat{f}^{(k)}-f^{*}\|_{L^{2}(X)}^{2}\ \leq\ 2\|f^{*}-\widehat{f}^{(k)}\|_{L^{2}(X)}^{2}+2\| \widehat{f}^{(k)}-\widehat{f}^{(k)}\|_{L^{2}(X)}^{2}\ :=\ 2J_{1}(k)+2J_{2}(k)\] (40)

To bound \(J_{1}(d)\), we derive recursive inequalities between \(J_{1}(k)\) and \(J_{1}(k+1)\) for all \(0\leq k\leq d-1\). Note that

\[J_{1}(k) =\mathbb{E}\left((f^{*}(X)-\widehat{f}^{(k)}(X))^{2}\Big{|} \mathcal{X}_{1}^{n}\right)\] (41) \[=\sum_{t\in\mathcal{L}^{(k)}}\mathbb{P}(X\in A_{t}|\mathcal{X}_{1} ^{n})\cdot\mathrm{Var}(f^{*}(X)|X\in A_{t},\mathcal{X}_{1}^{n})\]For each \(t\in\mathcal{L}^{(k)}\), let \(t_{L}\) and \(t_{R}\) be the two children of \(t\), then we have

\[\begin{split}&\mathbb{P}(X\in A_{t}|\mathcal{X}_{1}^{n})\cdot \mathrm{Var}(f^{*}(X)|X\in A_{t},\mathcal{X}_{1}^{n})\\ =&\mathbb{P}(X\in A_{t_{L}}|\mathcal{X}_{1}^{n}) \cdot\mathrm{Var}(f^{*}(X)|X\in A_{t_{L}},\mathcal{X}_{1}^{n})\\ &+\mathbb{P}(X\in A_{t_{R}}|\mathcal{X}_{1}^{n})\cdot\mathrm{Var} (f^{*}(X)|X\in A_{t_{R}},\mathcal{X}_{1}^{n})+\Delta(A_{t},\hat{j}_{t},\hat{b} _{t})\end{split}\] (42)

where

\[(\hat{j}_{t},\hat{b}_{t})\in\underset{j\in[p],b\in\mathbb{R}}{\mathrm{argmax}} \,\widehat{\Delta}(A_{t},j,b)\]

Let us define

\[(j_{t},b_{t})\in\underset{j\in[p],b\in\mathbb{R}}{\mathrm{argmax}}\,\Delta(A_ {t},j,b)\]

Then we have

\[\begin{split}\Delta(A_{t},\hat{j}_{t},\hat{b}_{t})& \geq\frac{1}{1+\alpha}\widehat{\Delta}(A_{t},\hat{j}_{t},\hat{b}_{t })-(5000/\alpha)U^{2}\bar{t}(\delta/72)\\ &\geq\frac{1}{1+\alpha}\widehat{\Delta}(A_{t},j_{t},b_{t})-(5000 /\alpha)U^{2}\bar{t}(\delta/72)\\ &\geq\frac{1}{(1+\alpha)^{2}}\Delta(A_{t},j_{t},b_{t})-\frac{2+ \alpha}{\alpha(1+\alpha)}5000U^{2}\bar{t}(\delta/72)\end{split}\] (43)

where the first inequality is by event \(\mathcal{E}_{3}\), the second inequality is by the definition of \((\hat{j}_{t},\hat{b}_{t})\), and the third inequality is because of event \(\mathcal{E}_{2}\). By Assumption 2.2, we have

\[\Delta(A_{t},j_{t},b_{t}) =\sup_{j\in[p],b\in\mathbb{R}}\Delta(A_{t},j,b)\;\geq\;\lambda \cdot\mathbb{P}(X\in A_{t}|\mathcal{X}_{1}^{n})\mathrm{Var}(f^{*}(X)|X\in A_ {t},\mathcal{X}_{1}^{n})\] (44)

Combining (42), (43) and (44), we have

\[\begin{split}&\mathbb{P}(X\in A_{t_{L}}|\mathcal{X}_{1}^{n}) \cdot\mathrm{Var}(f^{*}(X)|X\in A_{t_{L}},\mathcal{X}_{1}^{n})+\mathbb{P}(X \in A_{t_{R}}|\mathcal{X}_{1}^{n})\cdot\mathrm{Var}(f^{*}(X)|X\in A_{t_{R}}, \mathcal{X}_{1}^{n})\\ &\leq\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}\mathbb{P}(X \in A_{t}|\mathcal{X}_{1}^{n})\cdot\mathrm{Var}(f^{*}(X)|X\in A_{t},\mathcal{ X}_{1}^{n})+\frac{2+\alpha}{\alpha(1+\alpha)}5000U^{2}\bar{t}(\delta/72)\end{split}\]

Summing up the inequality above for all \(t\in\mathcal{L}^{(k)}\), we have

\[J_{1}(k+1)\leq\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}J_{1}(k)+2^{k} \cdot\frac{2+\alpha}{\alpha(1+\alpha)}5000U^{2}\bar{t}(\delta/72)\]

Using the inequality above recursively for \(k=0,1,....,d-1\), we have

\[\begin{split} J_{1}(d)&\leq\Big{(}1-\frac{\lambda}{( 1+\alpha)^{2}}\Big{)}^{d}J_{1}(0)+\frac{2+\alpha}{\alpha(1+\alpha)}5000U^{2} \bar{t}(\delta/72)\sum_{k=1}^{d}2^{k-1}\\ &\leq\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}^{d}\mathrm{ Var}(f^{*}(X))+2^{d}\cdot\frac{2+\alpha}{\alpha(1+\alpha)}5000U^{2}\bar{t}( \delta/72)\end{split}\] (45)

To bound \(J_{2}(d)\), we have

\[\begin{split} J_{2}(d)&=\sum_{t\in\mathcal{L}^{(d)}} \mathbb{P}(X\in A_{t})\Big{(}\mathbb{E}(f^{*}(X)|X\in A_{t},\mathcal{X}_{1}^{n} )-\bar{y}_{\mathcal{I}_{A_{t}}}\Big{)}^{2}\\ &\leq 2^{d}\cdot 400U^{2}\bar{t}(\delta/24)\end{split}\] (46)

where the inequality made use of event \(\mathcal{E}_{1}\).

Using (45) and (46), and recalling (40), we have

\[\begin{split}\|\widehat{f}^{(k)}-f^{*}\|_{L^{2}(X)}^{2}& \leq 2\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}^{d}\mathrm{ Var}(f^{*}(X))+2^{d+1}\cdot\frac{2+\alpha}{\alpha(1+\alpha)}5000U^{2}\bar{t}( \delta/72)\\ &\quad+2^{d+1}\cdot 400U^{2}\bar{t}(\delta/24)\\ &\lesssim\mathrm{Var}(f^{*}(X))\cdot(1-\lambda/(1+\alpha)^{2})^{ d}+\frac{2+\alpha}{\alpha(1+\alpha)}\frac{2^{d}(d\log(np)+\log(1/\delta))}{n}U^{2}\\ &\lesssim\mathrm{Var}(f^{*}(X))\cdot(1-\lambda/(1+\alpha)^{2})^{ d}+\frac{2^{d}(d\log(np)+\log(1/\delta))}{\alpha n}U^{2}\end{split}\] (47)

This completes the proof of (10). To prove (11), by taking \(\alpha=1/d\) and \(d=\lceil\log_{2}(n)/(1-\log_{2}(1-\lambda))\rceil\), we have

\[\begin{split}\Big{(}1-\frac{\lambda}{(1+\alpha)^{2}}\Big{)}^{d}& =(1-\lambda)^{d}\Big{(}1+\frac{\lambda}{1-\lambda}(1-(1+\alpha)^{-2}) \Big{)}^{d}\\ &=(1-\lambda)^{d}\Big{(}1+\frac{\lambda}{1-\lambda}\frac{2/d+1/d^ {2}}{(1+1/d)^{2}}\Big{)}^{d}\;\lesssim_{\lambda}\;(1-\lambda)^{d}\end{split}\] (48)Note that for \(s=\log_{2}(n)/(1-\log_{2}(1-\lambda))\) we have \((1-\lambda)^{s}=2^{s}/n\), hence by taking \(d=\lceil\log_{2}(n)/(1-\log_{2}(1-\lambda))\rceil\), we have

\[(1-\lambda)^{d}\leq\frac{2^{d}}{n}\leq 2n^{-1+\frac{1}{1-\log_{2}(1-\lambda)}} =2n^{-\phi(\lambda)}.\] (49)

Combining (47), (48) and (49) and note that \(\operatorname{Var}(f^{*}(X))\leq M<U\), we have

\[\|\tilde{f}^{(k)}-f^{*}\|_{L^{2}(X)}^{2} \lesssim_{\lambda,U}n^{-\phi(\lambda)}(d^{2}\log(np)+d\log(1/ \delta))\] \[\lesssim_{\lambda,U}n^{-\phi(\lambda)}(\log^{2}(n)\log(np)+\log( n)\log(1/\delta))\]

this completes the proof of (11).

### Proof of Lemma a.1

The main idea of proving Lemma A.1 is to find a proper finite net of the set \(\mathcal{A}_{p,d}\), control the gap on this net, and finally prove the result for all \(A\in\mathcal{A}_{p,d}\) based on the approximation gap of the net. We need a few auxiliary results. Let \(\mathcal{S}:=\{0,1/n,2/n,...,(n-1)/n,1\}\), and define

\[\widetilde{\mathcal{A}}_{p,d}:=\left\{\prod_{j=1}^{p}[\ell_{j},u_{j}]\in \mathcal{A}_{p,d}\ \middle|\ \ell_{j},u_{j}\in\mathcal{S}\text{ for all }j\in[p]\right\}\]

For any \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\in\mathcal{A}_{p,d}\), define

\[A^{\prime}=\prod_{j=1}^{p}[\ell_{j}^{\prime},u_{j}^{\prime}]\]

where \(\ell_{j}^{\prime}:=\max\left\{s\in\mathcal{S}\ |\ s\leq\ell_{j}\right\}\), and \(u_{j}^{\prime}:=\min\left\{s\in\mathcal{S}\ |\ s\geq u_{j}\right\}\). Roughly speaking, \(A^{\prime}\) is the smallest box with all edges in \(\mathcal{S}\) that contains \(A\). For any \(\widetilde{A}=\prod_{j=1}^{p}[\tilde{\ell}_{j},\tilde{u}_{j}]\in\widetilde{ \mathcal{A}}_{p,d}\) with \(\tilde{u}_{j}-\tilde{\ell}_{j}\geq 2/n\) for all \(j\in[p]\), define

\[B(\widetilde{A}):=\widetilde{A}\ \setminus\ \prod_{j=1}^{p}\Big{[}\tilde{\ell}_{j}+(1/n) \cdot 1_{(\tilde{\ell}_{j}\neq 0)}\,\ \tilde{u}_{j}-(1/n)\cdot 1_{\{\tilde{u}_{j}\neq 1 \}}\Big{]}.\]

and define \(\mathcal{B}_{p,d}\) to be the set of all such sets, that is

\[\mathcal{B}_{p,d}:=\left\{B(\widetilde{A})\ \middle|\ \widetilde{A}=\prod_{j=1}^{p}[ \tilde{\ell}_{j},\tilde{u}_{j}]\in\widetilde{\mathcal{A}}_{p,d}\text{ with }\tilde{u}_{j}-\tilde{\ell}_{j}\geq 2/n\right\}\]

The following lemma can be easily verified from the definitions of \(\widetilde{\mathcal{A}}_{p,d}\) and \(\mathcal{B}_{p,d}\).

**Lemma A.5**: _(1) For any \(A\in\mathcal{A}_{p,d}\), there exists \(B\in\mathcal{B}_{p,d}\) such that \(A^{\prime}\setminus A\subseteq B\). (2) \(\mathbb{P}(X\in B)\leq 2\bar{\theta}d/n\) for all \(B\in\mathcal{B}_{p,d}\). (3) The cardinality_

\[|\mathcal{B}_{p,d}|\leq|\widetilde{\mathcal{A}}_{p,d}|\leq\binom{p}{d}(n+1)^{ 2d}\leq p^{d}(n+1)^{2d}\]

Finally, for any \(t\geq 0\), we define

\[\mathcal{A}_{p,d}(t):=\Big{\{}A\in\mathcal{A}_{p,d}\ \Big{|}\ \mathbb{P}(X\in A)\leq t \Big{\}},\quad\text{and}\quad\widetilde{\mathcal{A}}_{p,d}(t):=\Big{\{}A\in \widetilde{\mathcal{A}}_{p,d}\ \Big{|}\ \mathbb{P}(X\in A)\leq t\Big{\}}\]

**Lemma A.6**: _Suppose Assumption 2.1 holds true. Let \(z_{1},...,z_{n}\) be i.i.d. bounded random variables with \(|z_{1}|\leq V<\infty\) almost surely. Assume that for each \(i\in[n]\), \(z_{i}\) is independent of \(\{x_{j}\}_{j\neq i}\), but may be dependent on \(x_{i}\). Given any \(\delta\in(0,1)\), with probability at least \(1-\delta\), it holds_

\[\max_{A\in\widetilde{\mathcal{A}}_{p,d},\widetilde{\mathcal{A}}_{p,d}(\bar{ \ell}_{1}(\delta))}\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{1}{n}\sum_{i =1}^{n}z_{i}1_{\{x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A\}})\Big{|}\ \leq\ 2V \sqrt{\ell_{1}(\delta)}\]

where \(U=M+m\).

_Proof._ For each fixed \(A\in\widetilde{\mathcal{A}}_{p,d}\setminus\widetilde{\mathcal{A}}_{p,d}(\bar{ \ell}_{1}(\delta))\), note that

\[\Big{|}\mathbb{E}\Big{(}(z_{1}1_{\{x_{1}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A \}}))^{k}\Big{)}\Big{|}\leq(2V)^{k}\mathbb{P}(X\in A)\quad\forall\ k\geq 2\]so by Lemma D.1 with \(t=2V\sqrt{\mathbb{P}(X\in A)}\sqrt{t_{1}(\delta)}\), \(\gamma^{2}=(2V)^{2}\mathbb{P}(X\in A)\) and \(b=2V\), we have

\[\mathbb{P}\left(\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{1} {n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{i}\in A\}}) \Big{|}>2V\sqrt{\bar{t}_{1}(\delta)}\right)\] \[\leq 2\exp\left(-\frac{n}{4}\left(\frac{4V^{2}\mathbb{P}(X\in A)\bar{ t}_{1}(\delta)}{4V^{2}\mathbb{P}(X\in A)}\wedge\frac{2V\sqrt{\mathbb{P}(X\in A) \bar{t}_{1}(\delta)}}{2V}\right)\right)\] \[\stackrel{{(i)}}{{=}} 2\exp\left(-\frac{n}{4}\bar{t}_{1}(\delta)\right)=\delta/(p^{d}( n+1)^{2d})\]

where \((i)\) is because \(\mathbb{P}(X\in A)\geq\bar{t}_{1}(\delta)\) (since \(A\in\widetilde{\mathcal{A}}_{p,d}\setminus\widetilde{\mathcal{A}}_{p,d}(\bar{ t}_{1}(\delta))\)). As a result, we have

\[\mathbb{P}\left(\max_{A\in\widetilde{\mathcal{A}}_{p,d}\setminus \widetilde{\mathcal{A}}_{p,d}(\bar{t}_{1}(\delta))}\frac{1}{\sqrt{\mathbb{P}( X\in A)}}\Big{|}\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{ i}\in A\}})\Big{|}>2V\sqrt{\bar{t}_{1}(\delta)}\right)\] \[\leq \sum_{A\in\widetilde{\mathcal{A}}_{p,d}\setminus\widetilde{ \mathcal{A}}_{p,d}(\bar{t}_{1}(\delta))}\mathbb{P}\left(\frac{1}{\sqrt{\mathbb{ P}(X\in A)}}\Big{|}\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A\}}- \mathbb{E}(z_{1}1_{\{x_{1}\in A\}})\Big{|}>2V\sqrt{\bar{t}_{1}(\delta)}\right)\] \[\leq |\widetilde{\mathcal{A}}_{p,d}\setminus\widetilde{\mathcal{A}}_{ p,d}(\bar{t}_{1}(\delta))|\cdot\delta/(p^{d}(n+1)^{2d})\leq\delta\]

where the last inequality makes use of Lemma A.5 (3).

**Lemma A.7**: _Let \(\mathcal{D}\) be a finite collection of measurable subsets of \([0,1]^{p}\) satisfying \(\mathbb{P}(X\in D)\leq\bar{\alpha}\) for all \(D\in\mathcal{D}\) (for some constant \(\bar{\alpha}\in(0,1)\)). Given any \(\delta\in(0,1)\), if_

\[w(\bar{\alpha},\delta):=(e^{2}\bar{\alpha})\vee\frac{\log(|\mathcal{D}|/\delta )}{n}\leq 3/4\]

_then with probability at least \(1-\delta\) it holds_

\[\max_{D\in\mathcal{D}}\left\{\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in D\}} \right\}\;\leq\;w(\bar{\alpha},\delta)\]

_Proof._ For any fixed \(D\in\mathcal{D}\), denote \(\alpha=\mathbb{P}(X\in D)\), then by Lemma D.2, for any \(t\in(0,3/4]\), we have

\[\mathbb{P}\Big{(}\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in D\}}>t \Big{)} \leq\] \[\leq\] \[\leq \exp\left(-n\left(t\log(t/\alpha)+(1-t)(-t^{2})\right)\right)\] \[= \exp\left(-n\left(t\left(\log(t/\alpha)-1\right)+t^{3}\right)\right)\] \[\leq \exp\left(-nt\left(\log(t/\alpha)-1\right)\right)\]

where the third inequality makes use of Lemma D.3 and the assumption \(t\leq 3/4\). Take \(t=w(\bar{\alpha},\delta)\), and note that

\[\log(w(\bar{\alpha},\delta)/\alpha)-1\geq\log(w(\bar{\alpha},\delta)/\bar{ \alpha})-1\geq\log(e^{2})-1\geq 1\]

we have

\[\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in B\}}>w(\bar{\alpha}, \delta)\right)\leq\exp\left(-nw(\bar{\alpha},\delta)\right)\leq\delta/| \mathcal{D}|\]

where the last inequality is because of the definition of \(w(\bar{\alpha},\delta)\). Taking the union bound we have

\[\mathbb{P}\left(\max_{D\in\mathcal{D}}\left\{\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{ i}\in D\}}\right\}>w(\bar{\alpha},\delta)\right)\leq|\mathcal{D}|\cdot\delta/| \mathcal{D}|=\delta\]

**Corollary A.8**: _Under Assumption 2.1 and given \(\delta\in(0,1)\), suppose \(\bar{t}_{2}(\delta)<3/4\), then with probability at least \(1-\delta\), it holds_

\[\max_{B\in\mathcal{B}_{p,d}}\left\{\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in B\}} \right\}\;\leq\;\bar{t}_{2}(\delta)\]Proof.: Apply Lemma A.7 with \(\mathcal{D}=\mathcal{B}_{p,d}\) and \(\bar{\alpha}=2\bar{\theta}d/n\), and note that \(|\mathcal{B}_{p,d}|\leq(n+1)^{2d}p^{d}\) (by Lemma A.5 (3)) and the definition \(\bar{t}_{2}(\delta)=\frac{2\bar{\theta}\bar{\theta}^{2}d}{n}\vee\frac{\log(p^{ d}(n+1)^{2d}/\delta)}{n}\). 

**Lemma A.9**: _Suppose Assumption 2.1 holds true. Let \(z_{1},...,z_{n}\) be i.i.d. bounded random variables with \(|Z|\leq V<\infty\) almost surely. Assume that for each \(i\in[n]\), \(z_{i}\) is independent of \(\{x_{j}\}_{j\neq i}\), but may be dependent on \(x_{i}\). Given any \(\delta\in(0,1)\), suppose \(\bar{t}_{2}(\delta/2)<3/4\), then with probability at least \(1-\delta\), it holds_

\[\sup_{A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(\bar{t}(\delta/2))} \frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{ x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A\}})\Big{|}\leq 5V\sqrt{\bar{t}(\delta/2)}.\] (50)

Proof.: Define events \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\):

\[\mathcal{E}_{1} :=\left\{\max_{B\in\mathcal{B}_{p,d}}\Big{\{}\frac{1}{n}\sum_{i= 1}^{n}1_{\{x_{i}\in B\}}\Big{\}}\leq\bar{t}_{2}(\delta/2)\right\}\] \[\mathcal{E}_{2} :=\left\{\max_{A\in\tilde{\mathcal{A}}_{p,d}\setminus\mathcal{A} _{p,d}(\bar{t}_{1}(\delta/2))}\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{ 1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A\}} )\Big{|}\leq\,V\sqrt{\bar{t}_{1}(\delta/2)}\right\}\]

Then by Lemma A.6 and Corollary A.8, we have \(\mathbb{P}(\mathcal{E}_{1})\geq 1-\delta/2\) and \(\mathbb{P}(\mathcal{E}_{2})\geq 1-\delta/2\), hence \(\mathbb{P}(\mathcal{E}_{1}\cap\mathcal{E}_{2})\geq 1-\delta\). Below we prove that when \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\) hold true, inequality (50) holds true.

Note that for any \(A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(\bar{t}(\delta/2))\),

\[\begin{split}&\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{ 1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A\}} )\Big{|}\\ &\leq\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{1}{n}\sum_{ i=1}^{n}z_{i}1_{\{x_{i}\in A\}}-\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A ^{\prime}\}}\Big{|}\\ &\quad+\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\frac{1}{n}\sum_ {i=1}^{n}z_{i}1_{\{x_{i}\in A^{\prime}\}}-\mathbb{E}(z_{1}1_{\{x_{1}\in A^{ \prime}\}})\Big{|}\\ &\quad+\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\mathbb{E}(z_{1 }1_{\{x_{1}\in A^{\prime}\}})-\mathbb{E}(z_{1}1_{\{x_{1}\in A\}})\Big{|}\\ &\quad:=T_{1}+T_{2}+T_{3}\end{split}\] (51)

To bound \(T_{1}\), we have

\[\begin{split} T_{1}&=\ \frac{1}{\sqrt{\mathbb{P}(X\in A)}} \Big{|}\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A^{\prime}\setminus A\}} \Big{|}\,\leq\ \frac{V}{\sqrt{\mathbb{P}(X\in A)}}\Big{(}\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in A ^{\prime}\setminus A\}}\Big{)}\\ &\leq\ \frac{V}{\sqrt{\mathbb{P}(X\in A)}}\max_{B\in\mathcal{B}_{p,d}} \Big{\{}\frac{1}{n}\sum_{i=1}^{n}1_{\{x_{i}\in B\}}\Big{\}}\,\leq\,\frac{V}{ \sqrt{\mathbb{P}(X\in A)}}\bar{t}_{2}(\delta/2)\ \leq\ V\sqrt{\bar{t}_{2}(\delta/2)}\end{split}\] (52)

where the second inequality makes use of Lemma A.5 (1), and the third inequality is by \(\mathcal{E}_{1}\).

To bound \(T_{2}\), note that

\[\begin{split} T_{2}&=\sqrt{\frac{\mathbb{P}(X\in A ^{\prime})}{\mathbb{P}(X\in A)}}\frac{1}{\sqrt{\mathbb{P}(X\in A^{\prime})}} \Big{|}\frac{1}{n}\sum_{i=1}^{n}z_{i}1_{\{x_{i}\in A^{\prime}\}}-\mathbb{E}(z_{ 1}1_{\{x_{1}\in A^{\prime}\}})\Big{|}\\ &\leq\sqrt{\frac{\mathbb{P}(X\in A^{\prime})}{\mathbb{P}(X\in A)} }2V\sqrt{\bar{t}_{1}(\delta/2)}\end{split}\] (53)

where the inequality is by event \(\mathcal{E}_{2}\) and because \(A^{\prime}\in\tilde{\mathcal{A}}_{p,d}\) and \(\mathbb{P}(X\in A^{\prime})\geq\mathbb{P}(X\in A)\geq\bar{t}(\delta/2)\geq\bar {t}_{1}(\delta/2)\). Note that

\[\mathbb{P}(X\in A^{\prime}\setminus A)\leq\frac{2\bar{\theta}d}{n}\leq\bar{t}_ {2}(\delta/2)\leq\mathbb{P}(X\in A)\] (54)

where the first inequality is by Lemma A.5 (2); the second inequality is by the definition of \(\hat{t}_{2}(\delta/2)\) in (22); the third inequality is because \(A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(\bar{t}(\delta/2))\). As a result of (53) and (54), we have

\[T_{2}\leq\sqrt{\frac{\mathbb{P}(X\in A^{\prime}\setminus A)+\mathbb{P}(X\in A)} {\mathbb{P}(X\in A)}}2V\sqrt{\bar{t}_{1}(\delta/2)}\leq 2\sqrt{2}V\sqrt{\bar{t}_{1}(\delta/2)}\] (55)

[MISSING_PAGE_FAIL:18]

### Proof of Lemma a.2

Define \(a:=\bar{t}(\delta/4)\) and \(b:=a+\frac{2\bar{\delta}d}{n}\). Define events \(\mathcal{E}_{1}\) and \(\mathcal{E}_{2}\):

\[\mathcal{E}_{1} :=\left\{\max_{A\in\mathcal{A}_{p,d}(b)}\frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{\{x_{i}\in A\}}\leq(e^{2}b)\vee\frac{\log(2(n+1)^{2d}p^{d}/\delta )}{n}\right\}\] \[\mathcal{E}_{2} :=\left\{\sup_{A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(a )}\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\mathbb{P}(X\in A)-\frac{1}{n} \sum_{i=1}^{n}\mathbbm{1}_{\{x_{i}\in A\}}\Big{|}\leq 5\sqrt{\bar{t}(\delta/4)}\right\}\]

Then by Lemmas A.7 and A.9, we know that \(\mathbb{P}(\mathcal{E}_{1})\geq 1-\delta/2\) and \(\mathbb{P}(\mathcal{E}_{2})\geq 1-\delta/2\), so \(\mathbb{P}(\mathcal{E}_{1}\cap\mathcal{E}_{2})\geq 1-\delta\). Below we prove (24) when \(\mathcal{E}_{1}\cap\mathcal{E}_{2}\) holds.

For \(A\in\mathcal{A}_{p,d}\), if \(\mathbb{P}(X\in A)\leq a\), then \(\mathbb{P}(A)\leq a+\frac{2\bar{\delta}d}{n}=b\). So we have

\[\begin{split}\sup_{A\in\mathcal{A}_{p,d}(a)}\frac{1}{n}\sum_{i=1 }^{n}\mathbbm{1}_{\{x_{i}\in A\}}&\leq\sup_{A\in\mathcal{A}_{p,d }(a)}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{x_{i}\in A^{\prime}\}}\leq\sup_{A \in\bar{\mathcal{A}}_{p,d}(b)}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{x_{i} \in\bar{A}\}}\\ &\leq\Big{(}e^{2}\bar{t}(\delta/4)+2e^{2}\bar{\theta}d/n\Big{)} \vee\frac{\log(2(n+1)^{2d}p^{d}/\delta)}{n}\\ &\leq(e^{2}+1)\bar{t}(\delta/4)\;\leq\;25\bar{t}(\delta/4)\end{split}\] (62)

where the third inequality is by event \(\mathcal{E}_{1}\) and the definition of \(b\); the fourth inequality is because

\[\bar{t}(\delta/4)\geq\bar{t}_{1}(\delta/4)\geq\frac{1}{n}\log(2p^{d}(n+1)^{2d }/\delta)\quad\text{and}\quad\bar{t}(\delta/4)\geq\bar{t}_{2}(\delta/4)\geq 2 \mathrm{e}^{2}\bar{\theta}d/n\;.\]

As a result, we have

\[\begin{split}&\sup_{A\in\mathcal{A}_{p,d}(a)}\left|\sqrt{ \mathbb{P}(X\in A)}-\sqrt{\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{x_{i}\in A\} }}\ \right|\\ &\leq\sup_{A\in\mathcal{A}_{p,d}(a)}\max\left\{\sqrt{\mathbb{P}(X \in A)},\sqrt{\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{x_{i}\in A\}}}\right\} \leq 5\sqrt{\bar{t}(\delta/4)}\end{split}\] (63)

where the second inequality made use of (62).

On the other hand,

\[\begin{split}&\sup_{A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(a)}\left|\sqrt{\mathbb{P}(X\in A)}-\sqrt{\frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{\{x_{i}\in A\}}}\ \right|\\ &=\sup_{A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(a)}\frac {\left|\mathbb{P}(X\in A)-\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{x_{i}\in A\} }\right|}{\sqrt{\mathbb{P}(X\in A)}+\sqrt{\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1 }_{\{x_{i}\in A\}}}}\\ &\leq\sup_{A\in\mathcal{A}_{p,d}\setminus\mathcal{A}_{p,d}(a)} \frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\mathbb{P}(X\in A)-\frac{1}{n}\sum_ {i=1}^{n}\mathbbm{1}_{\{x_{i}\in A\}}\Big{|}\;\leq\;5\sqrt{\bar{t}(\delta/4)} \end{split}\] (64)

where the last inequality is by event \(\mathcal{E}_{2}\).

Combining (63) and (64) the proof is complete.

## Appendix B Proofs in Section 3

For any interval \(E\in\mathcal{E}\) and any univariate function \(g\) on \([0,1]\), let \(V_{g}(E)\) be the total variation of \(g\) on \(E\). For the additive model (14) and a rectangle \(A=\prod_{j=1}^{p}E_{j}\in\mathcal{A}\), define \(V_{f^{*}}(A)=\sum_{j=1}^{p}V_{f^{*}_{j}}(E_{j})\). Recall that \(X\) is a random variable with the same distribution as \(x_{i}\), and \(X^{(j)}\) is the \(j\)-th coordinate of \(X\).

### Technical lemmas

**Lemma B.1**: _For any rectangle \(A\subseteq[0,1]^{p}\), any \(j\in[p]\) and any \(b\in\mathbb{R}\), it holds_

\[\Delta(A,j,b)=\Big{(}\mathbb{E}(f^{*}(X)\mathbbm{1}_{\{X\in A_{R}\}})-\mathbb{ E}(f^{*}(X)|X\in A)\mathbb{P}(X\in A_{R})\Big{)}^{2}\frac{\mathbb{P}(X\in A)}{ \mathbb{P}(X\in A_{L})\mathbb{P}(X\in A_{R})}\]

_where \(A_{L}=A_{L}(j,b)\) and \(A_{R}=A_{R}(j,b)\)._Proof.: We use the notations \(\nu:=\mathbb{E}(f^{*}(X)|X\in A)\), \(\nu_{L}:=\mathbb{E}(f^{*}(X)|X\in A_{L})\) and \(\nu_{R}:=\mathbb{E}(f^{*}(X)|X\in A_{R})\). First, note that

\[\mathbb{E}\left((f^{*}(X)-\nu)^{2}1_{\{X\in A_{L}\}}\right)=\ \mathbb{E}\left((f^{*}(X)-\nu_{L}+\nu_{L}-\nu)^{2}1_{\{X\in A_{L}\}}\right)\] (65) \[=\mathbb{E}\left((f^{*}(X)-\nu_{L})^{2}1_{\{X\in A_{L}\}}\right)+( \nu_{L}-\nu)^{2}\mathbb{P}(X\in A_{L})\]

Similarly, we have

\[\mathbb{E}\left((f^{*}(X)-\nu)^{2}1_{\{X\in A_{R}\}}\right)=\ \mathbb{E}\left((f^{*}(X)-\nu_{R})^{2}1_{\{X\in A_{R}\}}\right)+(\nu_{R}-\nu) ^{2}\mathbb{P}(X\in A_{R})\] (66)

Summing up (65) and (66) we have

\[(\nu_{L}-\nu)^{2}\mathbb{P}(X\in A_{L})+(\nu_{R}-\nu)^{2}\mathbb{P}(X\in A_{R})\]

\[=\mathbb{E}\left((f^{*}(X)-\nu)^{2}1_{\{X\in A\}}\right)-\mathbb{E}\left((f^{* }(X)-\nu_{L})^{2}1_{\{X\in A_{L}\}}\right)-\mathbb{E}\left((f^{*}(X)-\nu_{R})^ {2}1_{\{X\in A_{R}\}}\right)\] (67)

Note that

\[(\nu_{L}-\nu)^{2}\mathbb{P}(X\in A_{L})=\ \Big{(}\mathbb{E}(f^{*}(X)1_{\{X \in A_{L}\}})-\nu\mathbb{P}(X\in A_{L})\Big{)}^{2}(\mathbb{P}(X\in A_{L}))^{-1}\] (68) \[=\Big{(}\nu\mathbb{P}(X\in A)-\mathbb{E}(f^{*}(X)1_{\{X\in A_{R} \}})-\nu\mathbb{P}(X\in A_{L})\Big{)}^{2}(\mathbb{P}(X\in A_{L}))^{-1}\] \[=\Big{(}\nu\mathbb{P}(X\in A_{R})-\mathbb{E}(f^{*}(X)1_{\{X\in A _{R}\}})\Big{)}^{2}(\mathbb{P}(X\in A_{L}))^{-1}\] \[=(\nu_{R}-\nu)^{2}\frac{(\mathbb{P}(X\in A_{R}))^{2}}{\mathbb{P} (X\in A_{L})}\]

Combining (67) and (68) we have

\[\Delta(A,j,b) =(\nu_{R}-\nu)^{2}\frac{(\mathbb{P}(X\in A_{R}))^{2}}{\mathbb{P} (X\in A_{L})}+(\nu_{R}-\nu)^{2}\mathbb{P}(X\in A_{R})=\ (\nu_{R}-\nu)^{2}\frac{\mathbb{P}(X\in A_{R}) \mathbb{P}(X\in A)}{\mathbb{P}(X\in A_{L})}\] \[=\Big{(}\mathbb{E}(f^{*}(X)1_{\{X\in A_{R}\}})-\nu\mathbb{P}(X \in A_{R})\Big{)}^{2}\frac{\mathbb{P}(X\in A)}{\mathbb{P}(X\in A_{L}) \mathbb{P}(X\in A_{R})}\]

**Lemma B.2**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has the additive structure in (14). Then for any \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\), it holds_

\[\max_{j\in[p],\phi\in\mathbb{R}}\sqrt{\Delta(A,j,b)}\geq\frac{\sqrt{\mathbb{ P}(X\in A)}\mathrm{Var}(f^{*}(X)|X\in A)}{\sum_{k=1}^{p}\int_{\ell_{k}}^{u_{k}} \sqrt{q_{A}^{(k)}(t)(1-q_{A}^{(k)}(t))}dV_{f_{k}^{*}}([\ell_{j},t])}\]

_where \(q_{A}^{(k)}(t):=\mathbb{P}(X^{(k)}\leq t|x_{1}\in A)\)._

Proof.: For a fixed \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\), without loss of generality, assume \(\mathbb{E}(f^{*}(X)|X\in A)=0\). Note that for any \(j\in[p]\),

\[\max_{b\in\mathbb{K}}\sqrt{\Delta(A,j,b)}\geq \frac{\int_{\ell_{j}}^{u_{j}}\sqrt{q_{A}^{(j)}(s)(1-q_{A}^{(j)}(s) )}\sqrt{\Delta(A,j,s)}\ dV_{f_{j}^{*}}([\ell_{j},s])}{\int_{\ell_{j}}^{u_{j}} \sqrt{q_{A}^{(j)}(s)(1-q_{A}^{(j)}(s))}\ dV_{f_{j}^{*}}([\ell_{j},s])}\] (69)

where \(s\) the integration variable. Because \(q_{A}^{(j)}(s)=\mathbb{P}(X\in A_{L}(j,s))/\mathbb{P}(X\in A)\), using Lemma B.1 and recall that we have assumed \(\mathbb{E}(f^{*}(X)|X\in A)=0\), we have

\[\int_{\ell_{j}}^{u_{j}}\sqrt{q_{A}^{(j)}(s)(1-q_{A}^{(j)}(s))} \sqrt{\Delta(A,j,s)}\ dV_{f_{j}^{*}}([\ell_{j},s])\] \[=\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\int_{\ell_{j}}^{u_{j}}\Big{|} \mathbb{E}(f^{*}(X)1_{\{X\in A_{R}(j,s)\}})\Big{|}\ dV_{f_{j}^{*}}([\ell_{j},s])\] \[=\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\int_{\ell_{j}}^{u_{j}}\Big{|} \mathbb{E}(f^{*}(X)1_{\{X\in A\}}1_{\{X^{(j)>s}\}})\Big{|}\ dV_{f_{j}^{*}}([\ell_{j},s])\] \[\geq\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\int_{\ell_{j}}^{u_{ j}}\mathbb{E}(f^{*}(X)1_{\{X\in A\}}1_{\{X^{(j)>s}\}})\ dJ_{j}^{*}(s)\Big{|}\] \[=\frac{1}{\sqrt{\mathbb{P}(X\in A)}}\Big{|}\mathbb{E}(f^{*}(X)(f _{j}^{*}(X^{(j)})-f_{j}^{*}(\ell_{j}))1_{\{X\in A\}})\Big{|}\]where the last equality makes use of the assumption that \(\mathbb{E}(f^{*}(X)|X\in A)=0\). Combining the inequality above with (69), we have

\[\max_{b\in\mathbb{R}}\sqrt{\Delta(A,j,b)}\geq \frac{1}{\sqrt{\mathbb{P}(X\in A)}}\frac{\left|\mathbb{E}(f^{*}(X) f_{j}^{*}(X^{(j)})1_{\{X\in A\}})\right|}{\int_{\ell_{j}}^{u_{j}}\sqrt{q_{A}^{(j)} (s)(1-q_{A}^{(j)}(s))}\;dV_{J_{j}^{*}}([\ell_{j},s])}\]

As a result, we have

\[\max_{j\in[p],b\in\mathbb{R}}\sqrt{\Delta(A,j,b)}\geq\ \frac{1}{\sqrt{\mathbb{P}(X\in A )}}\frac{\sum_{j=1}^{p}\left|\mathbb{E}(f^{*}(X)f_{j}^{*}(X^{(j)})1_{\{X\in A\} })\right|}{\sum_{j=1}^{p}\int_{\ell_{j}}^{u_{j}}\sqrt{q_{A}^{(j)}(s)(1-q_{A}^ {(j)}(s))}\;dV_{J_{j}^{*}}([\ell_{j},s])}\] (70)

By the additive structure (14) we have

\[\sum_{j=1}^{p}\left|\mathbb{E}(f^{*}(X)f_{j}^{*}(X^{(j)})1_{\{X\in A\}}) \right|\geq\mathbb{E}((f^{*}(X))^{2}1_{\{X\in A\}})=\mathbb{P}(X\in A)\mathrm{ Var}(f^{*}(X)|X\in A)\] (71)

Combining (70) and (71), the proof is complete. 

**Lemma B.3**: _Suppose Assumption 2.1 holds true, and \(f^{*}\) has the additive structure in (14). If for any \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\) and any \(k\in[p]\) it holds_

\[\Big{(}\int_{\ell_{k}}^{u_{k}}\sqrt{q_{A}^{(k)}(t)(1-q_{A}^{(k)}(t))}\;dV_{J_{ k}^{*}}([\ell_{k},t])\Big{)}^{2}\leq\frac{\tau^{2}}{u_{k}-\ell_{k}}\inf_{w\in \mathbb{R}}\int_{\ell_{k}}^{u_{k}}|f_{k}^{*}(t)-w|^{2}\;dt\] (72)

_Then Assumption 2.2 is satisfied with \(\lambda=\theta/(p\tau^{2}\bar{\theta})\)._

_Proof._ Given \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\), without loss of generality, assume \(\mathbb{E}(f^{*}(X)|X\in A)=0\). Let \(p_{X}(\cdot)\) be the density of \(X\) on \([0,1]^{p}\). Then we have

\[\mathrm{Var}(f^{*}(X)|X\in A)=\frac{1}{\mathbb{P}(X\in A)}\int_{A}(f^{*}(z))^{ 2}p_{X}(z)\;dz\geq\frac{\theta}{\mathbb{P}(X\in A)}\int_{A}(f^{*}(z))^{2}\;dz\] (73)

where the second inequality made use of Assumption 2.1\((i)\). Denote \(c_{j}:=\frac{1}{u_{j}-\ell_{j}}\int_{\ell_{j}}^{u_{j}}f_{j}^{*}(t)dt\) and \(c:=\sum_{j=1}^{p}c_{j}\), then we have

\[\int_{A}(f^{*}(z))^{2}\;dz =\int_{A}\Big{(}c+\sum_{j=1}^{p}f_{j}^{*}(z_{j})-c_{j}\Big{)}^{2 }\,dz_{1}dz_{2}\cdots dz_{p}\] \[=\int_{A}c^{2}+\sum_{j=1}^{p}(f_{j}^{*}(z_{j})-c_{j})^{2}\,dz_{1} dz_{2}\cdots dz_{p}\] \[\geq\sum_{j=1}^{p}\frac{\prod_{k=1}^{p}(u_{k}-\ell_{k})}{u_{j}- \ell_{j}}\int_{\ell_{j}}^{u_{j}}(f_{j}^{*}(t)-c_{j})^{2}\;dt\] \[\geq\frac{\mathbb{P}(X\in A)}{\bar{\theta}}\sum_{j=1}^{p}\frac{1 }{u_{j}-\ell_{j}}\int_{\ell_{j}}^{u_{j}}(f_{j}^{*}(t)-c_{j})^{2}\;dt\]

Combining the inequality above with (73) we have

\[\mathrm{Var}(f^{*}(X)|X\in A)\geq\frac{\theta}{\bar{\theta}}\sum_{j=1}^{p} \frac{1}{u_{j}-\ell_{j}}\int_{\ell_{j}}^{u_{j}}(f_{j}^{*}(t)-c_{j})^{2}\;dt\] (74)

We use \(H_{k}^{2}\) to denote the LHS of (72), then (72) implies

\[\frac{1}{u_{j}-\ell_{j}}\int_{\ell_{j}}^{u_{j}}|f_{j}^{*}(t)-c_{j}|^{2}\;dt \geq\frac{1}{\tau^{2}}H_{j}^{2}\] (75)

As a result of (74) and (75), we have

\[\mathrm{Var}(f^{*}(X)|X\in A)\geq\frac{\theta}{\bar{\theta}\tau^{2}}\sum_{j=1} ^{p}H_{k}^{2}\] (76)By Lemma B.2 we have

\[\max_{j\in[p],b\in\mathbb{R}}\Delta(A,j,b) \geq\frac{\mathbb{P}(X\in A)\mathrm{Var}(f^{*}(X)|X\in A)^{2}}{( \sum_{k=1}^{p}H_{k})^{2}}\] \[\geq\frac{\theta}{\theta\tau^{2}}\frac{\sum_{j=1}^{p}H_{k}^{2}}{( \sum_{k=1}^{p}H_{k})^{2}}\mathbb{P}(X\in A)\mathrm{Var}(f^{*}(X)|X\in A)\] \[\geq\frac{\theta}{p\bar{\theta}\tau^{2}}\mathbb{P}(X\in A)\mathrm{ Var}(f^{*}(X)|X\in A)\]

where the second inequality is by (76), and the last inequality made use of the Cauchy-Schwarz inequality.

\(\Box\)

### Proof of Proposition 3.1

For any \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\) and any \(k\in[p]\) it holds

\[\Big{(}\int_{\ell_{k}}^{u_{k}}\sqrt{q_{A}^{(k)}(t)(1-q_{A}^{(k)} (t))}\;dV_{f_{k}^{*}}([\ell_{k},t])\Big{)}^{2}\leq\frac{1}{4}\Big{(}\int_{ \ell_{k}}^{u_{k}}|(f_{k}^{*})^{\prime}(t)|\;dt\Big{)}^{2}\] \[\leq\frac{\tau^{2}/4}{u_{k}-\ell_{k}}\inf_{w\in\mathbb{R}}\int_{ \ell_{k}}^{u_{k}}|f_{k}^{*}(t)-w|^{2}\;\mathrm{d}t\]

where the first inequality is by Cauchy-Schwarz inequality, and the second is because \(f_{k}^{*}\in LRP([0,1],\tau)\). Using Lemma B.3, the proof of complete.

### Proof of Proposition 3.2

For any \(A=\prod_{j=1}^{p}[\ell_{j},u_{j}]\subseteq[0,1]^{p}\) and any \(k\in[p]\), we prove that

\[\Big{(}\int_{\ell_{k}}^{u_{k}}\sqrt{q_{A}^{(k)}(t)(1-q_{A}^{(k)} (t))}\;dV_{f_{k}^{*}}([\ell_{k},t])\Big{)}^{2}\leq\max\Big{\{}\frac{2r\bar{ \theta}}{\bar{\theta}},\frac{r^{2}}{2\alpha}\Big{\}}\frac{\max\{9\beta^{2},32+ \beta^{2}\}}{u_{k}-\ell_{k}}\inf_{w\in\mathbb{R}}\int_{\ell_{k}}^{u_{k}}|f_{k}^ {*}(t)-w|^{2}\;dt\] (77)

Then the conclusion follows Lemma B.3.

For fixed \(A\) and \(k\in[p]\), to simplify the notation, we denote \(g:=f_{k}^{*}\), \(a:=\ell_{k}\), \(b:=u_{k}\), \(q(t):=q_{A}^{(k)}(t)\) for all \(t\in[\ell_{k},u_{k}]\), and \(t_{j}:=t_{j}^{(k)}\) for \(j=0,1,...,r\). Then (77) can be written as

\[\Big{(}\int_{a}^{b}\sqrt{q(t)(1-q(t))}\;dV_{g}([a,t])\Big{)}^{2}\leq 2r\max\Big{\{}\frac{\bar{ \theta}}{\bar{\theta}},\frac{r}{4\alpha}\Big{\}}\frac{\max\{9\beta^{2},32+ \beta^{2}\}}{b-a}\inf_{w\in\mathbb{R}}\int_{a}^{b}(g(t)-w)^{2}\;\mathrm{d}t\] (78)

For any \(s\in(0,1)\), define \(\Delta g(s):=\lim_{t\to s+}g(t)-\lim_{t\to s-}g(t)\). Let \(j^{\prime},j^{\prime\prime}\in[r]\) such that \(t_{j^{\prime}-1}\leq a<t_{j^{\prime}}\) and \(t_{j^{\prime\prime}-1}<b\leq t_{j^{\prime\prime}}\), and define \(r^{\prime}=j^{\prime\prime}-j^{\prime}+1\), and

\[z_{0}=a,\;z_{1}=t_{j^{\prime}},z_{2}=t_{j^{\prime}+1},\;...,z_{r^{\prime}-1}= t_{j^{\prime\prime}-1},\;z_{r^{\prime}}=b.\]

Then we have

\[\Big{(}\int_{a}^{b}\sqrt{q(t)(1-q(t))}\;\mathrm{d}V_{g}([a,t]) \Big{)}^{2}\] (79) \[= \Big{(}\sum_{j=1}^{r^{\prime}}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t)(1- q(t))}|g^{\prime}(t)|\;\mathrm{d}t+\sum_{j=1}^{r^{\prime}-1}\sqrt{q(z_{j})(1-q(z_{j}))} \Delta g(z_{j})\Big{)}^{2}\] \[\leq 2r^{\prime}\sum_{j=1}^{r^{\prime}}\Big{(}\int_{z_{j-1}}^{z_{j}} \sqrt{q(t)(1-q(t))}|g^{\prime}(t)|\;\mathrm{d}t\Big{)}^{2}+2(r^{\prime}-1) \sum_{j=1}^{r^{\prime}-1}q(z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}\]

We have the following 4 claims bounding the terms in the last line of the display above.

**Claim B.4**: _For \(j\in\{1,r^{\prime}\}\), it holds_

\[\Big{(}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t)(1-q(t))}|g^{\prime}(t)|\;\mathrm{d}t \Big{)}^{2}\leq\frac{\bar{\theta}\beta^{2}}{\bar{\theta}(b-a)}\inf_{w\in \mathbb{R}}\int_{z_{j-1}}^{z_{j}}(g(t)-w)^{2}\mathrm{d}t\]Proof of Claim B.4:.: We just prove the claim for \(j=1\). The proof for \(j=r^{\prime}\) follows a similar argument. To prove the claim for \(j=1\), we discuss two cases.

(Case 1) \(q(z_{1})\leq 1/2\). Then we have \(\sqrt{q(t)(1-q(t))}\leq\sqrt{q(z_{1})(1-q(z_{1}))}\), hence

\[\Big{(}\int_{a}^{z_{1}}\sqrt{q(t)(1-q(t))}|g^{\prime}(t)|\,\mathrm{ d}t\Big{)}^{2} \leq q(z_{1})(1-q(z_{1}))\Big{(}\int_{a}^{z_{1}}|g^{\prime}(t)|\, \mathrm{d}t\Big{)}^{2}\] \[\leq q(z_{1})(1-q(z_{1}))\frac{\beta^{2}}{z_{1}-a}\inf_{w\in \mathbb{R}}\int_{a}^{z_{1}}(g(t)-w)^{2}\,\mathrm{d}t\] (80) \[\leq\frac{\bar{\theta}\beta^{2}}{\bar{\theta}(b-a)}\inf_{w\in \mathbb{R}}\int_{a}^{z_{1}}(g(t)-w)^{2}\mathrm{d}t\]

where the second inequality is because \(g\in LRP((a,z_{1}),\beta)\); and the last inequality makes use of the fact \(q(z_{1})\leq\bar{\theta}(z_{1}-a)/(\bar{\theta}(b-a))\).

(Case 2) \(q(z_{1})>1/2\). Then we have

\[z_{1}-a\geq\frac{\bar{\theta}(b-a)}{\bar{\theta}}q(z_{1})\geq\frac{\bar{ \theta}(b-a)}{2\bar{\theta}}\] (81)

As a result,

\[\Big{(}\int_{a}^{z_{1}}\sqrt{q(t)(1-q(t))}|g^{\prime}(t)|\, \mathrm{d}t\Big{)}^{2} \leq\frac{1}{4}\Big{(}\int_{a}^{z_{1}}|g^{\prime}(t)|\,\mathrm{d}t \Big{)}^{2}\leq\frac{\beta^{2}}{4(z_{1}-a)}\inf_{w\in\mathbb{R}}\int_{a}^{z_{1 }}(g(t)-w)^{2}\,\mathrm{d}t\] \[\leq\frac{\bar{\theta}\beta^{2}}{2\bar{\theta}(b-a)}\inf_{w\in \mathbb{R}}\int_{a}^{z_{1}}(g(t)-w)^{2}\,\mathrm{d}t\]

where the first inequality is by Cauchy-Schwarz inequality; the second inequality is because \(g\in LRP((a,z_{1}),\beta)\); the third inequality is by (81).

Combining (Cases 1) and (Case 2), the proof of Claim B.4 is complete.

**Claim B.5**: _For \(j\in\{1,r^{\prime}-1\}\), it holds_

\[q(z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}\leq\max\Big{\{}\frac{4\bar{\theta}} {\bar{\theta}},\frac{r}{\alpha}\Big{\}}\frac{\max\{\beta^{2},4\}}{b-a}\inf_{w \in\mathbb{R}}\int_{z_{0}}^{z_{2}}(g(t)-w)^{2}\,\mathrm{d}t\]

Proof of Claim B.5:.: We just prove the claim for \(j=1\). The proof for \(j=r^{\prime}-1\) follows a similar argument. To prove the claim for \(j=1\), we discuss two cases.

(Case 1) \(|\Delta g(z_{1})|>4\max\{\int_{z_{0}}^{z_{1}}|g^{\prime}(t)|\,\mathrm{d}t\,\int_{z_{1}}^{z_{2}}|g^{\prime}(t)|\,\mathrm{d}t\}\). Then by Lemma D.6 we have

\[\inf_{w\in\mathbb{R}}\int_{z_{0}}^{z_{2}}(g(t)-w)^{2}\,\mathrm{d}t\ \geq\min\big{\{}z_{1}-z_{0},z_{2}-z_{1}\big{\}}\cdot\frac{(\Delta g(z_{1}))^{2 }}{16}\] (82)

Note that

\[\min\big{\{}z_{1}-z_{0},z_{2}-z_{1}\big{\}}\geq\min\Big{\{}\frac{\bar{\theta}q (z_{1})}{\bar{\theta}},\frac{\alpha}{r}(b-a)\Big{\}}\geq\min\Big{\{}\frac{\bar {\theta}q(z_{1})}{\bar{\theta}},\frac{\alpha}{r}\Big{\}}(b-a)\] (83)

So by (82) and (83) we have

\[|\Delta g(z_{1})|^{2}\leq\max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}q(z_{1})},\frac{r}{\alpha}\Big{\}}\frac{16}{b-a}\inf_{w\in\mathbb{R}}\int_{z_{0}}^{z_{2} }(g(t)-w)^{2}\,\mathrm{d}t\]

As a result,

\[q(z_{1})(1-q(z_{1}))|\Delta g(z_{1})|^{2}\] \[\leq\max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}}(1-q(z_{1})), \frac{r}{\alpha}q(z_{1})(1-q(z_{1}))\Big{\}}\frac{16}{b-a}\inf_{w\in\mathbb{R }}\int_{z_{0}}^{z_{2}}(g(t)-w)^{2}\,\mathrm{d}t\] \[\leq\max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}},\frac{r}{4\alpha }\Big{\}}\frac{16}{b-a}\inf_{w\in\mathbb{R}}\int_{z_{0}}^{z_{2}}(g(t)-w)^{2}\, \mathrm{d}t\]

where the second inequality is by Cauchy-Schwarz inequality.

(Case 2) \(|\Delta g(z_{1})|\leq 4\max\{\int_{z_{0}}^{z_{1}}|g^{\prime}(t)|\,\mathrm{d}t\,\int_{z_{1}}^{z_{2}}|g^{\prime}(t)|\,\mathrm{d}t\}\). Then we have

\[q(z_{1})(1-q(z_{1}))|\Delta g(z_{1})|^{2}\leq 4q(z_{1})(1-q(z_{1}))\max\Big{\{} \int_{z_{0}}^{z_{1}}|g^{\prime}(t)|\,\mathrm{d}t\,\int_{z_{1}}^{z_{2}}|g^{\prime}(t)|\,\mathrm{d}t\Big{\}}^{2}\] (84)By the same argument in (80), we have

\[4q(z_{1})(1-q(z_{1}))\Big{(}\int_{z_{0}}^{z_{1}}|g^{\prime}(t)|\,{\rm d}t\Big{)}^{2 }\leq\ \frac{4\bar{\theta}\beta^{2}}{\bar{\theta}(b-a)}\inf_{w\in\mathbb{R}}\int_{z_{ 0}}^{z_{1}}(g(t)-w)^{2}{\rm d}t\] (85)

On the other hand,

\[\begin{split}& 4q(z_{1})(1-q(z_{1}))\Big{(}\int_{z_{1}}^{z_{ 2}}|g^{\prime}(t)|\,{\rm d}t\Big{)}^{2}\leq\ \Big{(}\int_{z_{1}}^{z_{2}}|g^{\prime}(t)|\,{\rm d}t\Big{)}^{2}\\ &\leq\frac{\beta^{2}}{z_{2}-z_{1}}\inf_{w\in\mathbb{R}}\int_{z_{ 1}}^{z_{2}}(g(t)-w)^{2}\,{\rm d}t\leq\frac{r\beta^{2}}{\alpha(b-a)}\inf_{w\in \mathbb{R}}\int_{z_{1}}^{z_{2}}(g(t)-w)^{2}{\rm d}t\end{split}\] (86)

where the second inequality is because \(g\in LRP((z_{1},z_{2}),\beta)\); the last inequality is because \(z_{2}-z_{1}\geq\alpha/r\geq(\alpha/r)(b-a)\). By (84), (85) and (86), we have

\[q(z_{1})(1-q(z_{1}))|\Delta g(z_{1})|^{2}\leq\max\Big{\{}\frac{4\bar{\theta}} {\bar{\theta}},\frac{r}{\alpha}\Big{\}}\frac{\beta^{2}}{b-a}\inf_{w\in\mathbb{ R}}\int_{z_{0}}^{z_{2}}(g(t)-w)^{2}\,{\rm d}t\]

Combining (Case 1) and (Case 2), the proof of Claim B.5 is complete.

\(\Box\)

**Claim B.6**: _For \(2\leq j\leq r^{\prime}-1\), it holds_

\[\Big{(}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t)(1-q(t))}|g^{\prime}(t)|\,{\rm d}t \Big{)}^{2}\leq\frac{r\beta^{2}}{4\alpha}\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^ {z_{j}}(g(t)-w)^{2}{\rm d}t\]

_Proof of Claim B.6_: Note that

\[\begin{split}&\ \ \ \Big{(}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t)(1-q(t))}|g^{ \prime}(t)|\,{\rm d}t\Big{)}^{2}\\ &\leq\frac{1}{4}\Big{(}\int_{z_{j-1}}^{z_{j}}|g^{\prime}(t)|\,{ \rm d}t\Big{)}^{2}\leq\frac{\beta^{2}}{4(z_{j}-z_{j-1})}\inf_{w\in\mathbb{R}} \int_{z_{j-1}}^{z_{j}}(g(t)-w)^{2}{\rm d}t\\ &\leq\frac{r\beta^{2}}{4\alpha(b-a)}\inf_{w\in\mathbb{R}}\int_{z_ {j-1}}^{z_{j}}(g(t)-w)^{2}{\rm d}t\end{split}\]

where the first inequality is by Cauchy-Schwarz inequality; the second inequality is because \(g\in LRP((z_{j-1},z_{j}),\beta)\); the last inequality is by the assumption that \(t_{j}-t_{j-1}\geq\alpha/r\).

\(\Box\)

**Claim B.7**: _For \(2\leq j\leq r^{\prime}-2\), it holds_

\[q(z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}\leq\frac{r\max\{\beta^{2},4\}}{ \alpha}\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^{z_{j+1}}(g(t)-w)^{2}\,{\rm d}t\]

_Proof of Claim B.7_: We discuss two cases.

\(\underline{\mbox{(Case 1)}}\ |\Delta g(z_{j})|>4\max\{\int_{z_{j-1}}^{z_{j}}|g^{ \prime}(t)|\,{\rm d}t\,\int_{z_{j}}^{z_{j+1}}|g^{\prime}(t)|\,{\rm d}t\}\). Then by Lemma D.6 we have

\[\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^{z_{j+1}}(g(t)-w)^{2}\,{\rm d}t\ \geq\min\big{\{}z_{j}-z_{j-1},z_{j+1}-z_{j}\big{\}}\cdot\frac{(\Delta g(z_{j}) )^{2}}{16}\geq\ \frac{\alpha}{r}\frac{(\Delta g(z_{j}))^{2}}{16}\]

As a result,

\[\begin{split} q(z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}& \leq\frac{16r}{\alpha}q(z_{j})(1-q(z_{j}))\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^ {z_{j+1}}(g(t)-w)^{2}\,{\rm d}t\\ &\leq\frac{4r}{\alpha}\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^{z_{j+1 }}(g(t)-w)^{2}\,{\rm d}t\end{split}\](Case 2) \(|\Delta g(z_{j})|\leq 4\max\{\int_{z_{j-1}}^{z_{j}}|g^{\prime}(t)|\;\mathrm{d}t\;, \int_{z_{j}}^{z_{j+1}}|g^{\prime}(t)|\;\mathrm{d}t\}\). Then we have

\[q(z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}\] \[\leq 4q(z_{j})(1-q(z_{j}))\max\Big{\{}\int_{z_{j-1}}^{z_{j}}|g^{ \prime}(t)|\;\mathrm{d}t\;,\int_{z_{j}}^{z_{j+1}}|g^{\prime}(t)|\;\mathrm{d}t \Big{\}}^{2}\] \[\leq \max\Big{\{}\int_{z_{j-1}}^{z_{j}}|g^{\prime}(t)|\;\mathrm{d}t\;, \int_{z_{j}}^{z_{j+1}}|g^{\prime}(t)|\;\mathrm{d}t\Big{\}}^{2}\] \[\leq \max\Big{\{}\frac{\beta^{2}}{z_{j}-z_{j-1}}\inf_{w\in\mathbb{R}} \int_{z_{j-1}}^{z_{j}}(g(t)-w)^{2}\;\mathrm{d}t,\;\frac{\beta^{2}}{z_{j+1}-z_{ j}}\inf_{w\in\mathbb{R}}\int_{z_{j}}^{z_{j+1}}(g(t)-w)^{2}\;\mathrm{d}t\Big{\}}\] \[\leq \frac{\beta^{2}r}{\alpha}\max\Big{\{}\inf_{w\in\mathbb{R}}\int_{z _{j-1}}^{z_{j}}(g(t)-w)^{2}\;\mathrm{d}t,\;\inf_{w\in\mathbb{R}}\int_{z_{j}}^{ z_{j+1}}(g(t)-w)^{2}\;\mathrm{d}t\Big{\}}\] \[\leq \frac{\beta^{2}r}{\alpha}\inf_{w\in\mathbb{R}}\int_{z_{j-1}}^{z_{ j+1}}(g(t)-w)^{2}\;\mathrm{d}t\]

where the second inequality is by Cauchy-Schwarz inequality; the third inequality is because \(g\in LRP((z_{j-1},z_{j}),\beta)\) and \(g\in LRP((z_{j},z_{j+1}),\beta)\).

Combining (Case 1) and (Case 2), and note that \(b-a\leq 1\), the proof of Claim B.7 is complete.

### Completing the proof of Proposition 3.2

By (79) and note that \(r^{\prime}\leq r\), we have

\[\Big{(}\int_{a}^{b}\sqrt{q(t)(1-q(t))}\,\mathrm{d}V_{g}([a,t]) \Big{)}^{2}\] (87) \[\leq 2r\sum_{j=1}^{r^{\prime}}\Big{(}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t )(1-q(t))}|g^{\prime}(t)|\;\mathrm{d}t\Big{)}^{2}+2r\sum_{j=1}^{r^{\prime}-1}q (z_{j})(1-q(z_{j}))|\Delta g(z_{j})|^{2}\]

By Claims B.4 and B.6, we have

\[\sum_{j=1}^{r^{\prime}}\Big{(}\int_{z_{j-1}}^{z_{j}}\sqrt{q(t)(1- q(t))}|g^{\prime}(t)|\;\mathrm{d}t\Big{)}^{2}\] (88) \[\leq \max\Big{\{}\frac{\bar{\theta}\beta^{2}}{\bar{\theta}(b-a)},\frac {r\beta^{2}}{4\alpha}\Big{\}}\sum_{j=1}^{r^{\prime}}\inf_{w\in\mathbb{R}}\int _{z_{j-1}}^{z_{j}}(g(t)-w)^{2}\mathrm{d}t\] \[\leq \max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}},\frac{r}{4\alpha} \Big{\}}\frac{\beta^{2}}{b-a}\inf_{w\in\mathbb{R}}\int_{a}^{b}(g(t)-w)^{2} \mathrm{d}t\] \[= \max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}},\frac{r}{4\alpha} \Big{\}}\frac{\max\{8\beta^{2},32\}}{b-a}\inf_{w\in\mathbb{R}}\int_{a}^{b}(g(t )-w)^{2}\;\mathrm{d}t\]

By (87), (88) and (89) we have

\[\Big{(}\int_{a}^{b}\sqrt{q(t)(1-q(t))}\;\mathrm{d}V_{g}([a,t]) \Big{)}^{2}\] \[\leq 2r\max\Big{\{}\frac{\bar{\theta}}{\bar{\theta}},\frac{r}{4 \alpha}\Big{\}}\frac{\max\{9\beta^{2},32+\beta^{2}\}}{b-a}\inf_{w\in\mathbb{R} }\int_{a}^{b}(g(t)-w)^{2}\;\mathrm{d}t\]

Hence (78) is true, and the proof of Proposition 3.2 is complete.

### Proof of Example 3.1

Given \([a,b]\subseteq[0,1]\), without loss of generality, assume \(\int_{a}^{b}g(t)=0\) (because the infimum in \(w\) is achieved at \(w=\int_{a}^{b}g(t)\)). Let \(t_{0}\in[a,b]\) be the point with \(g(t_{0})=0\). Since \(g^{\prime}(t)\geq c_{1}>0\), we have

\[\int_{t_{0}}^{b}(g(t))^{2}\,\mathrm{d}t\geq\int_{t_{0}}^{b}(c_{1}(t-t_{0}))^{2 }\,\mathrm{d}t=\frac{c_{1}^{2}}{3}(b-t_{0})^{3}\]

Similarly,

\[\int_{a}^{t_{0}}(g(t))^{2}\,\mathrm{d}t\geq\int_{a}^{t_{0}}(c_{1}(t_{0}-t))^{2 }\,\mathrm{d}t=\frac{c_{1}^{2}}{3}(t_{0}-a)^{3}\]

As a result, we have

\[\int_{a}^{b}(g(t))^{2}\,\mathrm{d}t\geq\frac{c_{1}^{2}}{3}\Big{(}(b-t_{0})^{3 }+(t_{0}-a)^{3}\Big{)}\geq\frac{2c_{1}^{2}}{3}\Big{(}\frac{b-a}{2}\Big{)}^{3}= \frac{c_{1}^{2}}{12}(b-a)^{3}\] (90)

On the other hand, since \(|g^{\prime}(t)|\leq c_{2}\), we have

\[\Big{(}\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\Big{)}^{2}\leq c_{2}^{2}(b-a) ^{2}\] (91)

Combining (90) and (91), we have

\[\Big{(}\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\Big{)}^{2}\leq\frac{12c_{2}^{ 2}}{c_{1}^{2}(b-a)}\int_{a}^{b}(g(t))^{2}\,\mathrm{d}t\]

### Proof of Example 3.3

It suffices to prove that there exists a constant \(C_{r}\) such that for any univariate polynomial with a degree at most \(r\) and for any \(a<b\),

\[\Big{(}\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\Big{)}^{2}\leq\frac{C_{r}}{b -a}\int_{a}^{b}|g(t)|^{2}\,\mathrm{d}t\] (92)

We first prove the conclusion when \(a=0\) and \(b=1\). Let \(\mathcal{P}(r)\) be the set of all univariate polynomials with degree at most \(r\). Note that \(\mathcal{P}(r)\) is a finite-dimensional linear space, and the differential operator \(\Phi:g\mapsto g^{\prime}\) is a linear mapping on \(\mathcal{P}(r)\). As a result, there exists \(C_{r}\) such that

\[\int_{0}^{1}|g^{\prime}(t)|\,\mathrm{d}t\leq\sqrt{C_{r}}\int_{0}^{1}|g(t)|\, \mathrm{d}t\]

for all \(g\in\mathcal{P}(r)\).

For general \(a<b\), given \(g\in\mathcal{P}(r)\), define \(h(s):=g(a+(b-a)s)\), then \(h\in\mathcal{P}(r)\). So we have

\[\int_{0}^{1}|h^{\prime}(s)|\,\mathrm{d}s\leq\sqrt{C_{r}}\int_{0}^{1}|h(s)|\, \mathrm{d}s\] (93)

Note that

\[\int_{0}^{1}|h^{\prime}(s)|\,\mathrm{d}s=(b-a)\int_{0}^{1}|g^{\prime}(a+(b-a)s )|\,\mathrm{d}s=\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\] (94)

and

\[\int_{0}^{1}|h(s)|\,\mathrm{d}s=\int_{0}^{1}|g(a+(b-a)s)|\,\mathrm{d}s=\frac{1 }{b-a}\int_{a}^{b}|g(t)|\,\mathrm{d}t\] (95)

Combining (93), (94) and (95), we know that

\[\Big{(}\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\Big{)}^{2}\leq\Big{(}\frac{ \sqrt{C_{r}}}{b-a}\int_{a}^{b}|g(t)|\,\mathrm{d}t\Big{)}^{2}\leq\frac{C_{r}}{ b-a}\int_{a}^{b}|g(t)|^{2}\,\mathrm{d}t\]

where the last step is by Cauchy-Schwarz inequality.

### Proof of Example 3.2

It suffices to prove that for any \(a,b\in[0,1]\) with \(a<b\), it holds

\[\int_{a}^{b}|g^{\prime}(t)|\,\mathrm{d}t\leq\frac{110(L/\sigma)}{b-a}\int_{a} ^{b}|g(t)|\,\mathrm{d}t\] (96)

Once (96) is proved, the conclusion is true via Jensen's inequality.

[MISSING_PAGE_FAIL:27]

Comparison of Theorem 2.3 and Theorem 1 of [10]

We first restate Theorem 1 of [10] in the setting of fitting a single tree (note that [10] discussed random forest).

**Proposition C.1**: _(Theorem 1 of [10]) Suppose Assumptions 2.2 and 2.1 hold true. Let \(\widehat{f}^{(d)}(\cdot)\) be the tree estimated by CART with depth \(d\). Fixed constants \(\alpha_{2}>1\), \(0<\eta<1/8\), \(0<c<1/4\) and \(\delta>0\) with \(2\eta<\delta<1/4\). Then there exists constant \(C>0\) such that for all \(n\) and \(d\) satisfying \(1\leq d\leq c\log_{2}(n)\), it holds_

\[\mathbb{E}(\|\widehat{f}^{(d)}-f^{*}\|_{L^{2}(\mu)}^{2})\leq C\Big{(}n^{-\eta} +(1-\alpha_{2}^{-1}\lambda)^{d}+n^{-\delta+c}\Big{)}\] (105)

_In particular, the RHS of (105) is lower bounded by_

\[\Omega(n^{-\eta}+n^{-\delta+c}+n^{c\log_{2}(1-\lambda)})\] (106)

Note that (106) follows (105) by the fact \(1\leq d\leq c\log_{2}(n)\) and \(\alpha_{2}>1\). In the original Assumptions of Theorem 1 in [10], it was assumed that the noises can be heavy-tailed, which is a weaker assumption than Assumption 2.1. However, the parameter controlling the tails of the noises did not explicitly enter the error bound (105), and it seems that their proof techniques cannot improve the error bound even under the assumption that noises are bounded. In addition, the dependence on \(p\) was not explicitly stated in the bound (105), which seems to be hidden in the constant \(C\).

To compare our error bound with the error bound in (106), since the \(\|\widehat{f}^{(d)}-f^{*}\|_{L^{2}(\mu)}^{2}\) is bounded almost surely, it is not hard to transform the high-probability bound in (11) to an bound in expectation, and we have

\[\mathbb{E}(\|\widehat{f}^{(d)}-f^{*}\|_{L^{2}(\mu)}^{2})\leq O(n^{-\phi( \lambda)}\log(np)\log^{2}(n))\] (107)

Below we discuss two different cases.

* (Case 1) \(\lambda\geq 1/2\). Then it holds \[\phi(\lambda)=\frac{-\log_{2}(1-\lambda)}{1-\log_{2}(1-\lambda)}\geq\frac{- \log_{2}(1/2)}{1-\log_{2}(1/2)}=1/2\] (108) So our convergence rate in (107) is \(O(n^{-1/2}\log(np)\log^{2}(n))\), but the rate in (106) is \[\Omega(n^{-\eta}+n^{-\delta+c}+n^{c\log_{2}(1-\lambda)})\geq\Omega(n^{-\eta} )\geq\Omega(n^{-1/8})\] (109)
* (Case 2) \(0<\lambda\leq 1/2\). Then it holds \[1-\log_{2}(1-\lambda)\leq 1-\log_{2}(1/2)=2\] (110) and hence \(\phi(\lambda)\geq-\log_{2}(1-\lambda)/2\). So our rate in (107) is \(O(n^{\log_{2}(1-\lambda)/2}\log(np)\log^{2}(n))\), but the rate in (106) is \[\Omega(n^{-\eta}+n^{-\delta+c}+n^{c\log_{2}(1-\lambda)})\geq\Omega(n^{c\log_{ 2}(1-\lambda)})\geq\Omega(n^{\frac{1}{4}\log_{2}(1-\lambda)})\] (111)

## Appendix D Auxiliary results

**Lemma D.1**: _(Bernstein's inequality) Let \(Z_{1},....,Z_{n}\) be i.i.d. random variables satisfying \(|\mathbb{E}((Z_{1}-\mathbb{E}(Z_{1}))^{k})|\leq(1/2)k!\gamma^{2}b^{k-2}\) for some constants \(\gamma,b>0\) and for all \(k\geq 2\). Then for any \(t>0\),_

\[\mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^{n}Z_{i}-\mathbb{E}(Z_{i})\right| >t\right)\leq 2\exp\left(-\frac{n}{4}\Big{(}\frac{t^{2}}{\gamma^{2}}\wedge \frac{t}{b}\Big{)}\right)\]

**Lemma D.2**: _(Binomial tail bound) Let \(Z_{1},...,Z_{n}\) be i.i.d. random variables with \(\mathbb{P}(Z_{i}=1)=\alpha\) and \(\mathbb{P}(Z_{i}=0)=1-\alpha\). Then for any \(t\in(0,1)\),_

\[\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}Z_{i}>t\right)\ \leq\ \exp\left(-n\left[t\log\Big{(}\frac{t}{\alpha}\Big{)}+(1-t)\log\Big{(}\frac{1 -t}{1-\alpha}\Big{)}\right]\right)\]

**Lemma D.3**: _For any \(t\in(0,3/4)\), \(\log(1-t)>-t-t^{2}\)._

_Proof._ For \(t\in(0,3/4)\),

\[\log(1-t)+t+t^{2}\ =\ \frac{t^{2}}{2}-\sum_{k=3}^{\infty}\frac{t^{k}}{k!}\ \geq\ \frac{t^{2}}{2}-\frac{1}{6}\sum_{k=3}^{\infty}t^{k}\ =\ \frac{t^{2}}{2}-\frac{t^{3}}{6(1-t

**Lemma D.4**: _Suppose \(Z\) is a random variable satisfying \(\mathbb{E}(e^{\lambda Z})\leq e^{\lambda^{2}\sigma^{2}/2}\) for all \(\lambda\in\mathbb{R}\), where \(\sigma>0\) is a constant; then_

\[\mathbb{E}(|Z|^{k})\leq 9\sigma^{k}k!\]

_Proof._ By Chernoff inequality it holds \(\mathbb{P}(|Z|>t)\leq 2\exp(-t^{2}/(2\sigma^{2}))\) for all \(t>0\). As a result,

\[\mathbb{E}(|Z^{k}|/(k!\sigma^{k})) \leq\mathbb{E}(e^{|Z|/\sigma})=\int_{0}^{\infty}e^{t}\mathbb{P}(| Z|/\sigma>t)\;dt\] \[\leq\int_{0}^{\infty}2\exp\left(t-\frac{t^{2}}{2}\right)\,dt\;= \;2\sqrt{e}\int_{0}^{\infty}\exp(-(t-1)^{2}/2)\;dt\] \[\leq 2\sqrt{e}\int_{-\infty}^{\infty}\exp(-t^{2}/2)\;dt\;=\;2 \sqrt{2\pi e}\leq 9\]

\(\Box\)

**Lemma D.5**: _For any integer \(k\geq 2\) it holds \(\frac{1}{k^{2}}-\frac{4}{(k+1)^{3}}\leq\frac{1}{(k+1)^{2}}\)._

_Proof._ For any \(k\geq 2\) it holds

\[\frac{(2k+1)(k+1)}{2k^{2}}=(1+\frac{1}{2k})(1+\frac{1}{k})\leq(1+\frac{1}{4})( 1+\frac{1}{2})<2\]

Multiplying \(2/(k+1)^{3}\) in the display above, we have

\[\frac{2k+1}{k^{2}(k+1)^{2}}<\frac{4}{(k+1)^{3}}\]

The proof is complete by noting that \(\frac{2k+1}{k^{2}(k+1)^{2}}=\frac{1}{k^{2}}-\frac{1}{(k+1)^{2}}\). \(\Box\)

**Lemma D.6**: _Let \([a,b]\) be a sub-interval of \([0,1]\), and \(c\in(a,b)\). Let \(h\) be a function on \([a,b]\) such that \(h\) is differentiable on \((a,c)\) and \((c,b)\), but can be discontinuous at \(c\). Denote \(\Delta h(c):=\lim_{t\to c+}h(t)-\lim_{t\to c-}h(t)\). Suppose_

\[\Delta h(c)>4\max\left\{\int_{a}^{c}|h^{\prime}(t)|\;\mathrm{d}t,\;\int_{c}^{ b}|h^{\prime}(t)|\;\mathrm{d}t\right\}\] (112)

_Then it holds_

\[\inf_{w\in\mathbb{R}}\int_{a}^{b}(h(t)-w)^{2}\;\mathrm{d}t\;\geq\;\min\{c-a, b-c\}(\Delta h(c))^{2}/16\]

_Proof._ We assume that \(h\) is not continuous at \(c\), since otherwise, the conclusion holds true trivially. We use the notation \(h(c+):=\lim_{t\to c+}h(t)\) and \(h(c-):=\lim_{t\to c-}h(t)\). Without loss of generality, assume \(h(c+)>h(c-)\).

For \(w\geq(1/2)(h(c+)+h(c-))\), it holds \(w-h(c-)\geq(1/2)\Delta h(c)\). By (112), we know that for any \(t\in(a,c)\),

\[|h(t)-h(c-)|\leq\int_{a}^{c}|h^{\prime}(\tau)|\;\mathrm{d}\tau\leq\frac{1}{4} \Delta h(c)\]

Hence for all \(t\in(a,c)\),

\[w-h(t)=w-h(c-)+h(c-)-h(t)\geq\frac{1}{2}\Delta h(c)-\frac{1}{4}\Delta h(c)= \frac{1}{4}\Delta h(c)\]

As a result,

\[\int_{a}^{b}(h(t)-w)^{2}\;\mathrm{d}t\geq\int_{a}^{c}(h(t)-w)^{2}\;\mathrm{d}t \geq(c-a)(\Delta h(c))^{2}/16\] (113)

For \(w<(1/2)(h(c+)+h(c-))\), similarly, we can prove

\[\int_{a}^{b}(h(t)-w)^{2}\;\mathrm{d}t\geq(b-c)(\Delta h(c))^{2}/16\] (114)

The proof is complete by combining (113) and (114).