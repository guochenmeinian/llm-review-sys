# Physics-Informed Variational State-Space

Gaussian Processes

 Oliver Hamelijnck

University of Warwick

oliver.hamelijnck@warwick.ac.uk

Arno Solin

Aalto University

arno.solin@aalto.fi

&Theodoros Damoulas

University of Warwick

t.damoulas@warwick.ac.uk

###### Abstract

Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.

## 1 Introduction

Physical modelling is integral in modern science and engineering with applications from climate modelling [62] to options pricing [6]. Here, the key formalism to inject mechanistic physical knowledge are differential equations (DEs), which given initial and/or boundary values, are typically solved numerically [8]. In contrast machine learning is data-driven, and aims to learn latent functions from observations. However the increasing availability of data has spurred interest in combining these traditional mechanistic models with data-driven methods through physics-informed machine learning. These hybrids approaches aim to improve predictive accuracy, computational efficiency by leveraging both physical inductive biases with observations [30, 44].

A principled way to incorporate prior physical knowledge is through Gaussian processes (gps). gps are stochastic processes and are a data-centric approach that facilitates the quantification of uncertainty. Recently autoip was proposed in order to integrate non-linear physics into gps[40], where solutions to ordinary and partial differential equations (ODEs, PDEs) are observed at a finite set of collocation points. This is an extension of the probabilistic meshless method (pmm, [12]) to the variational setting such that non linear equations can be incorporated. Similarly, [4] introduced helmholtz-gp, that constructs gp priors that adhere to curl and divergence-free constraints. Such properties are required for the

Figure 1: The state-space formalism allows for linear-time inference in the temporal dimension.

successful modelling of electromagnetic fields [61] and ocean currents through the Helmholtz decomposition [4]. These approaches enable the incorporation of physics but incur a cubic computational complexity from needlessly computing full covariance matrices, as illustrated in Fig. 1. For ODEs (time-series setting), extended Kalman smoothers incorporate non-linear physics (eks) [65; 34] and recover popular ODE solvers whilst achieving linear-in-time complexity through state-space gps [58; 25].

In this work we propose a unified physics informed state-space gp (physs-gp) that is a probabilistic models where mechanistic/physics knowledge is incorporated as an inductive bias. We can handle both linear and non-linear PDEs and ODEs whilst maintaining linear-in-time computational efficiency. We additionally derive a state-space variational inference algorithm that further reduces the computational cost in the spatial dimension. We recover eks, pmm, and helmholtz-gp as special cases, and outperform autoip in terms of computational efficiency and predictive performance. In summary:

1. We derive a state-space gp that can handle spatio-temporal derivatives with a computational complexity that is linear in the temporal dimension.
2. With this we derive a unifying state-space variational inference framework that allows the incorporation of both linear and non-linear PDEs whilst achieving a linear-in-time complexity and recovering state-of-the-art methods such as eks, pmm and helmholtz-gp.
3. We further explore three approximations, namely a structured variational posterior, spatial sparsity, and spatial minibatching, that reduce the cubic spatial computational costs to linear.
4. We showcase our methods on a variety of synthetic and real-world experiments and outperform the current state-of-the-art methods autoip and helmholtz-gp both in terms computational and predictive performance.

Code to reproduce experiments is available at https://github.com/ohamelijnck/physs_gp.

## 2 Background on Gaussian Processes

Gaussian processesA GP is a distribution on an infinite collection of random variables such that any finite subset is jointly Gaussian [50]. Given observations \(\mathbf{X}\in\mathbb{R}^{N\times F}\) and \(\mathbf{y}\in\mathbb{R}^{N}\) then

\[p(\mathbf{y},f\,|\,\boldsymbol{\theta})=\prod_{n}^{N}p(y_{n}\,|\,f(\mathbf{x} _{n}),\boldsymbol{\theta})\,p(f\,|\,\boldsymbol{\theta})\] (1)

is a joint model where \(p(f\,|\,\boldsymbol{\theta})\) is a zero mean GP prior with kernel \(\mathbf{K}(\cdot,\cdot)\), \(f(\mathbf{X})\sim p(f(\mathbf{X})\,|\,0,\mathbf{K}(\mathbf{X},\mathbf{X}))\), and \(\boldsymbol{\theta}\) are (hyper) parameters. We are primarily concerned with the spatio-temporal setting where we observe \(N_{\mathrm{t}}\) temporal and \(N_{\mathrm{s}}\) spatial observations \(x_{\mathrm{t},s}\in\mathbb{R}\), \(y_{\mathrm{t},s}\in\mathbb{R}\) on a spatio-temporal grid. Under a Gaussian likelihood, all quantities for inference and training are available analytically and, naively, carry a dominant computational cost of \(\mathcal{O}((N_{\mathrm{t}}\,N_{\mathrm{s}})^{3})\). For time series data, an efficient way to construct a GP over \(f\) (and its time derivatives) is through the state-space representation of GPs. Given a Markov kernel, the temporal GP prior can be written as the solution of a discretised linear time-invariant stochastic differential equation (lti-sde), which at time \(k\) is

\[\bar{\mathbf{f}}_{k+1}=\mathbf{A}\,\bar{\mathbf{f}}_{k}+q_{k}\quad\text{and} \quad y_{k}\,|\,\bar{\mathbf{f}}_{k}\sim p(y_{k}\,|\,\bar{\mathbf{H}}\,\bar{ \mathbf{f}}_{k}),\] (2)

where \(\mathbf{A}\) is a transition matrix, \(q_{k}\) is Gaussian noise, \(\mathbf{H}\) is an observation matrix, and \(\bar{\mathbf{f}}\) is a \(d\)-dimensional vector of temporal derivatives \(\bar{f}=[f(\cdot),\frac{\partial f(\cdot)}{\partial x},\frac{\partial^{2}f( \cdot)}{\partial x^{2}},\cdots]^{\top}\). With appropriately designed states, matrices and densities, SDEs of this form represent a large class of gp models, and Kalman smoothing enables inference in \(\mathcal{O}(N_{\mathrm{t}}\,d^{3})\), see [56]. In the spatio-temporal setting, when the kernel matrix decomposes as a Kronecker product \(\mathbf{K}=\mathbf{K}_{t}\otimes\mathbf{K}_{s}\), then with a Markov time kernel, a state space form is admitted. This takes a particularly convenient form where the state is \(\bar{\mathbf{f}}_{t}=[\bar{f}((\mathbf{X}_{s})_{1},t),\cdots,\bar{f}((\mathbf{ X}_{s})_{Ns},t)]^{\top}\), and inference requires \(\mathcal{O}(N_{\mathrm{t}}(N_{\mathrm{s}}\,d)^{3})\), see [60].

Derivative Gaussian processesOne main appeal of gps is that they are closed under linear operators. Let \(\mathcal{D}\left[\cdot\right]=\mathcal{D}_{\mathrm{t}}\,\mathcal{D}_{\mathrm{ s}}\left[\cdot\right]\) be linear functional that computes \(D=d_{\mathrm{t}}\,d_{s}\) space-time derivatives with \(\mathcal{D}_{\mathrm{t}}\left[\cdot\right]=\left[\cdot,\frac{\partial}{ \partial t},\frac{\partial^{2}}{\partial t^{2}},\cdots\right]\) and \(\mathcal{D}_{\mathrm{s}}\left[\cdot\right]=\left[\cdot,\frac{\partial}{ \partial s},\frac{\partial^{2}}{\partial s^{2}},\cdots\right]\), then at a finite set of index points, the joint prior between \(\mathbf{f}\) and its time and spatial derivatives is

\[p(\bar{f}(\mathbf{X}))=\mathrm{N}\left(\,\mathcal{D}\,\mathbf{f}\,\,|\,\, \mathbf{0},\,\mathcal{D}\,\mathbf{K}(\mathbf{X},\mathbf{X})\,\mathcal{D}^{*}\,\right)\] (3)where \(\bar{f}(\mathbf{X})=\mathcal{D}\,f(\mathbf{X})\) and \(\mathcal{D}^{*}\) is the adjoint of \(\mathcal{D}\), meaning it operates on the second argument of the kernel [54]. When jointly modelling a single time and space derivative (\(d_{t}=d_{s}=1\)) the latent functions are \(\bar{\mathbf{f}}=[\mathbf{f},\frac{\partial\bar{\mathbf{f}}}{\partial s},\frac{ \partial\bar{\mathbf{f}}}{\partial t},\frac{\partial^{2}\mathbf{f}}{\partial t }\frac{\partial^{2}\mathbf{f}}{\partial s}]^{\top}\) and the kernel is

\[\bar{\mathbf{K}}=\mathcal{D}\,\mathbf{K}(\mathbf{X},\mathbf{X})\,\mathcal{D}^ {*}=\left[\begin{array}{cccc}\mathbf{K}&-&-&-\\ \frac{\partial}{\partial s}\,\mathbf{K}&\frac{\partial}{\partial s}\,\mathbf{K }\,\frac{\partial}{\partial s}^{\top}&-&-\\ \frac{\partial}{\partial t}\,\mathbf{K}&\frac{\partial}{\partial t}\,\mathbf{ K}\,\frac{\partial}{\partial s}^{\top}&\frac{\partial}{\partial t}\,\mathbf{K}\,\frac{ \partial}{\partial t}&-\\ \frac{\partial^{2}}{\partial t\,\partial s}\,\mathbf{K}&\frac{\partial^{2}}{ \partial t\,\partial s}\,\mathbf{K}\,\frac{\partial}{\partial s}^{\top}& \frac{\partial^{2}}{\partial t\,\partial s}\,\mathbf{K}\,\frac{\partial}{ \partial t}&\frac{\partial^{2}}{\partial t\,\partial s}\,\mathbf{K}\frac{ \partial^{2}}{\partial t\,\partial s}^{\top}\end{array}\right].\]

This is a multi-output prior whose samples are paths of \(f\) with its corresponding derivatives. This prior is commonly known as a derivative gp and has found applications in monotonic GPs [51], input-dependent noise [41; 67] and explicitly modelling derivatives [59; 17; 43]. State-space gps can be employed in the temporal setting since the underlying state computes \(f(\mathbf{x})\) with its corresponding time derivatives. In Sec. 3.1, we extend this to the spatio-temporal setting.

## 3 Physics-Informed State-Space Gaussian Processes (physs-gp)

We now propose a flexible generative model for incorporating information from both data observations and (non-linear) physical mechanics. We consider general non-linear evolution equations of the form

\[g(\mathcal{N}_{\theta}\,f)=\frac{\partial f}{\partial t}-\mathcal{N}_{\theta }\,f=0\] (4)

with appropriate boundary conditions, where \(f:\mathbb{R}^{F}\rightarrow\mathbb{R}\) is the latent quantity of interest and \(\mathcal{N}_{\theta}\) is a non-linear differential operator [49]. We assume that \(g:\mathbb{R}^{P\cdot D}\rightarrow\mathbb{R}\) is measurable, and is well-defined such that there are sensible solutions to the differential equation [25]. We wish to place a gp prior over \(f\) and update our beliefs after 'observing' that it should follow the solution of the differential equation. In general this is intractable and can only be handled approximately. By viewing Eqn. (4) as a loss function that measures the residual between \(\frac{\partial f}{\partial t}\) and the operator \(\mathcal{N}_{\theta}\,f\) then the right hand side (\(0\)) are virtual observations. The PDE can now be observed at a finite set of locations known as collocation points. This is a soft constraint (_i.e._\(\mathbf{f}\) is not guaranteed to follow the differential equation), but it can handle non-linear and linear mechanisms. However, there are special cases, namely curl and divergence-free constraints, that can be solved exactly. This follows from properties of vectors fields, where \(f\) defines a potential function where linear combinations of its partial derivatives define vector fields that enforce these properties. To handle both of these situations we propose the following generative model

\[\underbrace{\mathbf{F}_{n}=\mathbf{W}\cdot\big{[}\,\bar{f}_{q}( \mathbf{X}_{n})\,\big{]}^{\top}}_{\text{Linear Mixing}},\,\,\,\bar{f}_{q} \sim\mathcal{GP}(\mathbf{0},\bar{\mathbf{K}}_{q}),\] (5) \[\underbrace{\mathbf{y}_{n}^{(\mathcal{O})}=\mathbf{H}_{\mathcal{O }}\,\mathbf{F}_{n}+\epsilon_{\mathcal{O}}}_{\text{Data}},\,\,\underbrace{ \mathbf{0}_{n}^{(\mathcal{C})}=g(\mathbf{F}_{n})}_{\text{Collocation Points}},\,\,\, \underbrace{\mathbf{y}_{n}^{(\mathcal{B})}=\mathbf{H}_{\mathcal{B}}\,\mathbf{F }_{n}+\epsilon_{\mathcal{B}}}_{\text{Boundary Values}},\] (6)

where \(\bar{f}_{q}\) are derivative gps (see Eqn. (3)) that are linearly mixed by \(\mathbf{W}\in\mathbb{R}^{(\text{P}\,D)\times(\text{Q}\,D)}\), and \(\mathbf{Y}^{(\mathcal{O})},\mathbf{0}^{(\mathcal{C})}\in\mathbb{R}^{\text{N} \times\text{P}}\) are observations and collocation points over the P outputs and \(\mathbf{Y}^{(\mathcal{B})}\in\mathbb{R}^{\text{N}\times(\text{P}\,D)}\) are boundary values over the derivatives of each output. The observation matrices \(\mathbf{H}_{\mathcal{O}},\mathbf{H}_{\mathcal{B}}\) simply select the relevant parts of \(\mathbf{F}_{n}\). For further details on notation see App. A. In many case we want to observe the solution of the differential equation exactly, however in some cases it may be required to add observation noise \(\epsilon_{\mathcal{C}}\) to the collocation points, whether for numerical reasons or to model inexact mechanics. This is a flexible generative model where different assumptions and approximations will lead to various physics informed methods such as autoip, eks, pmm, and helmholtz-gp that we will develop state space algorithms for. Additionally it is possible to learn missing physics by parameterising unknown terms in Eqn. (4) through the gp priors in Eqn. (6) (see App. B.2).

**Example 3.1** (eks Prior and pmm).: We recover eks style generative models (see Hennig et al. [25]) when the mixing weight is identity \(\mathbf{W}=\mathbf{I}\), and \(\epsilon_{\mathcal{C}},\epsilon_{\mathcal{B}}\to 0\), and the non-linear transform \(g\) is linearised. Let the prior be Markov \(p(\bar{\mathbf{f}})=\prod_{k}^{N_{\text{t}}}p(\bar{\mathbf{f}}_{k}\,|\,\bar{ \mathbf{f}}_{k-1})\) with marginals \(p(\bar{\mathbf{f}}_{k})=\mathrm{N}\left(\,\bar{\mathbf{f}}_{k}\,|\,\,\mathbf{m}_ {k}^{-},\,\,\mathbf{P}_{k}^{-}\,\,\right)\). By taking a first-order Taylor linearisation \(g(\bar{\mathbf{f}}_{k})\simeq g(\mathbf{m}_{k}^{-})+\frac{\partial g(\mathbf{m}_ {k}^{-})}{\partial\mathbf{m}_{k}^{-}}\,\delta\bar{\mathbf{f}}_{k}\) with \(\delta\mathbf{\bar{f}}_{k}\sim\mathrm{N}\left(\,\mathbf{0},\,\mathbf{P}_{k}^{-}\,\right)\) the joint is

\[p(\begin{bmatrix}\mathbf{\bar{f}}_{k}\\ \mathbf{g}_{k}\end{bmatrix})\simeq\mathrm{N}\left(\begin{bmatrix}\mathbf{\bar{ f}}_{k}\\ \mathbf{g}_{k}\end{bmatrix}\,\,|\,\,\begin{bmatrix}\mathbf{m}_{k}^{-}\\ g_{k}(\mathbf{m}_{k}^{-})\end{bmatrix},\,\,\begin{bmatrix}\mathbf{I}\\ \frac{\partial g(\mathbf{m}_{k}^{-})}{\partial\mathbf{m}_{k}^{-}}\end{bmatrix} \,\,\mathbf{P}_{k}^{-}\,\,\begin{bmatrix}\mathbf{I}\\ \frac{\partial g(\mathbf{m}_{k}^{-})}{\partial\mathbf{m}_{k}^{-}}\end{bmatrix} \right)^{\top}.\] (7)

This is now a form that can directly be implemented into an extended Kalman smoothing algorithm [63]. When \(\mathrm{Q}>1\) the state \(\mathbf{\bar{f}}\) is constructed by stacking the individual states of each latent [56]. With linear ODEs eks coincides with pmm.

**Example 3.2** (helmholtz-gp and Curl and Divergence-Free Vector Fields in \(2\)D).: Let \(\mathbf{v}=[v_{t},v_{s_{1}},v_{s_{2}}]\) denote a 3D-vector field, then curl indicates the tendency of a vector field to rotate and divergence at a specific point indicates the tendency of the field to spread out. Curl and divergence-free fields follow

\[\nabla\times\mathbf{v}=0\,\,\,(\text{curl free}),\,\,\,\nabla\cdot\mathbf{v} \,\,\,\,\,\,=0\,\,\,(\text{div. free})\] (8)

where \(\nabla=[\frac{\partial}{\partial t},\frac{\partial}{\partial s_{1}},\frac{ \partial}{\partial s_{2}}]\). Two basic properties of vector fields state that the divergence of a curl field and the curl of a derivative field are zero [3]. Let \([f_{1},f_{2}]\) be scalar potential functions then

\[\mathbf{v}_{\text{curl}}=\nabla f_{1}\,\,\,(\text{curl free}),\,\,\,\mathbf{v}_{ \text{div}}=\nabla\times\nabla\,f_{2}\,\,\,(\text{div. free})\] (9)

define curl and divergence-free fields. In \(2\)D this simplifies to using the _grad_ and _rot_ operators over \(\mathbf{v}=[v_{s_{1}},v_{s_{2}}]\) (see [4]). Placing gp priors over \(f_{q}\) we incorporate this into Eqn. (6) by defining

\[\mathbf{W}_{\text{grad}}=\begin{bmatrix}1&0\\ 0&1\end{bmatrix}\,\,\mathbf{H},\,\,\,\mathbf{W}_{\text{rot}}=\begin{bmatrix} 0&1\\ -1&0\end{bmatrix}\,\,\mathbf{H}\,\,\,\text{where}\,\,\,\mathbf{H}\,\,\,\text{ selects}\,\,\,\left[\frac{\partial f}{\partial s_{1}},\frac{\partial f}{ \partial s_{2}}\right].\] (10)

helmholtz-gp is defined as the sum of gp priors over \(2\)D curl and divergence-free fields [4].

### A Spatio-Temporal State-Space Prior

The generative model in Eqn. (6) contains two complications: i) it includes potential non-lineararities, and ii) the independent priors are defined over latent functions with their partial derivatives which substantially increases the computational complexity. We wish to tackle both issues through state-space algorithms that are linear-in-time. We begin by deriving a state-space model that observes derivatives across space and time (see App. A.3 for the simpler time-series setting). In Sec. 3.2 we further derive a state-space variational lower bound that will enable computational speeds up in the spatial dimension.

First, we show how Kronecker structure in the kernel allows us to rewrite the model as the solution to an lti-sde. From the definition of \(\mathcal{D}\), the separable covariance matrix has a repetitive structure that can be represented through a Kronecker product. The gram matrix is

\[\mathcal{D}\,\mathbf{K}(\mathbf{x},\mathbf{x})\,\mathcal{D}^{*}=\mathbf{K}_{ \mathrm{t}}^{\mathcal{D}}(\mathbf{x}_{\mathrm{t}},\mathbf{x}_{\mathrm{t}})\, \otimes\,\,\mathbf{K}_{\mathrm{s}}^{\mathcal{D}}(\mathbf{x}_{\mathrm{s}}, \mathbf{x}_{\mathrm{s}})\] (11)

where \(\mathbf{K}_{\cdot}^{\mathcal{D}}\,=\begin{bmatrix}\mathbf{K}_{\cdot}&\mathbf{K }_{\cdot}\tilde{\mathcal{D}}^{*}\\ \tilde{\mathcal{D}}\,\mathbf{K}_{\cdot}&\tilde{\mathcal{D}}\,\mathbf{K}_{\cdot} \tilde{\mathcal{D}}^{*}\end{bmatrix}\) and \(\tilde{\mathcal{D}}[\cdot]=(\mathcal{D}[\cdot])_{1:}\) excludes the underlying latent function.

To find a Kronecker form of the gram matrix over \(\mathbf{X}\), we will exploit the fact that \(\mathbf{X}\) is on a spatio-temporal grid and that the kernel is separable. Due to the separable structure a derivative over either the spatio (or temporal) dimension only affects the corresponding kernel, and so when considering \(\mathbf{X}\), the gram matrix is still Kronecker structured:

\[\frac{\partial}{\partial s}\,\mathbf{K}(\mathbf{x},\mathbf{x})=\mathbf{K}_{t} (\mathbf{x},\mathbf{x})\cdot\frac{\partial}{\partial s}\,\mathbf{K}_{s}( \mathbf{x},\mathbf{x})\Rightarrow\frac{\partial}{\partial s}\,\mathbf{K}( \mathbf{X},\mathbf{X})=\mathbf{K}_{t}(\mathbf{X}_{t},\mathbf{X}_{t})\otimes \frac{\partial}{\partial s}\,\mathbf{K}_{s}(\mathbf{X}_{s},\mathbf{X}_{s}).\] (12)

The full prior over (a permuted) \(\mathbf{X}\) is now given as

\[p(\tilde{f}(\mathbf{X}))\,\tilde{=}\,\mathrm{N}\left(\,\mathbf{0},\,\mathbf{K}_ {\cdot}^{\mathcal{D}}(\mathbf{X}_{t},\mathbf{X}_{t})\otimes\mathbf{K}_{s}^{ \mathcal{D}}(\mathbf{X}_{s},\mathbf{X}_{s})\,\right).\]

This is the form of a spatio-temporal Gaussian process with derivative kernels that can be immediately cast into a state-space form as in Eqn. (2) where \(\mathbf{H}=\mathbf{I}\), as we want to observe the whole state, not just \(\mathbf{f}\). The marginal likelihood and the gp posterior can now be computed using standard Kalman filtering and smoothing algorithms with a computational time of \(O(N_{\mathrm{t}}\cdot(N_{\mathrm{s}}\cdot d_{s}\cdot d)^{3})\). Inference in physs-gp now follows Ex. 3.1 by recognising that the filtering state consists of the spatial points with there spatio-temporal derivatives. The eks prior in Ex. 3.1 can now be simply extended to the PDE setting by placing colocation points on a spatio-temporal grid [35].

### A State-Space Variational Lower Bound (physs-vgp and physs-eks)

We now derive a variational lower bound for physs-gp that maintains the computational benefits of state-space gps. This acts as an alternative way of handling the non-linearity of \(g\) in Eqn. (6), and will also enable the reduction of the cubic spatial computation complexity in Sec. 4. We start by focusing on the single latent function setting \((\mathrm{Q}=1)\) and collect all terms that relate to observations in Eqn. (6) with \(p(\mathbf{Y}\,|\,\bar{\mathbf{f}})=\prod_{n}^{N}p(\mathbf{y}_{n}^{(\mathcal{O} )}|\mathbf{H}_{\mathcal{O}}\,\mathbf{F}_{n})\,p(\mathbf{0}_{n}^{(\mathcal{O} )}|g(\mathbf{F}_{n}))\,p(\mathbf{y}_{n}^{(\mathcal{B})}|\mathbf{H}_{\mathcal{B }}\,\mathbf{F}_{n})\). vi frames inference as the minimisation of the Kullback-Leibler divergence between the true posterior and an approximate posterior, which leads the optimisation of the elbo[28]:

\[\operatorname*{arg\,max}_{q(\bar{\mathbf{f}}\,|\,\bm{\xi})}\,\mathcal{L}= \mathbb{E}_{\,q(\bar{\mathbf{f}})}\,\bigg{[}\log\frac{p(\mathbf{Y}\,|\,\bar{ \mathbf{f}})\,p(\bar{\mathbf{f}})}{q(\bar{\mathbf{f}})}\,\bigg{]}\] (13)

where we define the approximate posterior \(q(\mathbf{f}\,|\,\bm{\xi})\triangleq\mathrm{N}\left(\,\mathbf{f}\,|\,\, \mathbf{m},\,\mathbf{S}\,\right)\) as a free-form Gaussian with \(\bm{\xi}=(\mathbf{m},\mathbf{S})\) and \(\mathbf{m}\in\mathbb{R}^{D\,N\times 1}\), \(\mathbf{S}\in\mathbb{R}^{D\,N\times D\,N}\). The aim is to represent the approximate posterior as a state-space gp posterior, which will enable efficient computation of the whole evidence lower bound (elbo). We will achieve this through the use of natural gradients. The natural gradient preconditions the standard gradient with the inverse Fisher matrix, meaning the information geometry of the parameter space is taken into account, leading to faster convergence and superior performance [2, 31, 27]. For Gaussian approximate posteriors the natural gradient has a simple form [26]

\[\bm{\lambda}_{k}=\lambda_{k-1}+\beta\,\frac{\partial\mathcal{L}}{\partial \bm{\mu}_{k}}=(1-\beta)\,\widetilde{\bm{\lambda}}_{k-1}+\beta\,\frac{\partial \textsc{ELL}}{\partial\bm{\mu}_{k}}+\bm{\eta}=\widetilde{\bm{\lambda}}+\bm{\eta}\] (14)

where \(\bm{\lambda}=(\mathbf{S}^{-1}\mathbf{m},\nicefrac{{1}}{{2}}\,\mathbf{S}^{-1})\) and \(\bm{\mu}=(\mathbf{m},\mathbf{m}\,\mathbf{m}^{\top}+\mathbf{S})\) are the natural and expectation parameterisations. This is known as conjugate variational inference (cvi) as \(\widetilde{\bm{\lambda}}\) represent the natural parameters for the conjugate prior \(\bm{\eta}\)[31, 10, 20, 72]. For now, we will assume that the likelihood is conjugate to ensure that \([\bm{\lambda}_{k}]_{2}\) is _p.s.d_, this will be relaxed in Sec. 5. The derivative of the ell is

\[\frac{\partial\textsc{ELL}}{\partial[\bm{\mu}]_{2}}=\sum_{\mathrm{t,s}}^{N_{t },N_{s}}\frac{\partial}{\partial[\bm{\mu}]_{2}}\,\mathbb{E}_{\,q}\,\big{[}\log p (\mathbf{Y}_{(\mathrm{t,s})}\,|\,\bar{\mathbf{f}}_{(\mathrm{t,s})})\,\big{]}\,,\] (15)

where the expectation is under \(q(\bar{\mathbf{f}}_{(\mathrm{t,s})})\), a \(D\) dimensional Gaussian over the spatio-temporal derivatives at location \(\mathbf{x}_{\mathrm{t,s}}\). Within the sum, the only elements of \([\bm{\mu}]_{2}\) whose gradient will propagate through the expectation are the \(D\times D\) elements corresponding to these locations. These points are unique and so \(\frac{\partial\textsc{ELL}}{\partial[\bm{\mu}]_{2}}\) has some (permutated) block-diagonal structure, hence Eqn. (14) can be written as

\[q(\bar{\mathbf{f}})\propto\prod_{t}^{Nt}\left[\mathrm{N}(\widetilde{\mathbf{Y} }_{t}\,|\,\bar{\mathbf{f}}_{t},\widetilde{\mathbf{V}}_{t})\right]\,p(\bar{ \mathbf{f}})\] (16)

where \(\widetilde{\mathbf{Y}}_{t}\) is \(D\)-dimensional. The natural gradient update, _i.e._\(q(\bar{\mathbf{f}}_{t})\) in moment parameterisation, can now be computed using Kalman smoothing in \(\mathcal{O}(N_{t}\cdot(N_{s}\cdot d_{s}\cdot d)^{3})\). Collecting \(\widetilde{\mathbf{Y}}=\text{vec}\left(\,[\widetilde{\mathbf{Y}}_{t}]\, \right),\widetilde{\mathbf{V}}=\text{blkdiag}\left(\,[\widetilde{\mathbf{V}}_{ t}]\,\right)\), then the elbo can also be computed efficiently by substituting this form of \(q(\bar{\mathbf{f}}_{t})\) in

\[\mathcal{L}=\sum_{\mathrm{t,s}}^{N_{t},N_{s}}\,\mathbb{E}_{\,q(\bar{\mathbf{f}} _{(\mathrm{t,s})})}\,\big{[}\log p(\mathbf{Y}_{(\mathrm{t,s})}\,|\,\bar{ \mathbf{f}}_{(\mathrm{t,s})})\,\big{]}-\sum_{t}^{Nt}\mathbb{E}_{\,q(\bar{ \mathbf{f}}_{t})}\,\Big{[}\log\mathrm{N}(\widetilde{\mathbf{Y}}_{t}\,|\,\bar{ \mathbf{f}}_{t},\widetilde{\mathbf{V}}_{t})\,\Big{]}+\log p(\widetilde{\mathbf{ Y}}\,|\,\widetilde{\mathbf{V}})\] (17)

where the first two terms only depend on \(q(\mathcal{D}\,\mathbf{f}_{t})\) and the final term is simply a by-product of running the Kalman filter, leading to a dominant computational complexity of \(\mathcal{O}(N\cdot(N_{\mathrm{s}}\cdot d_{s}\cdot d)^{3})\). This cost is linear in the datapoints (\(N\)) because the expected log likelihood above decomposes across all spatio-temporal locations. In summary we have shown that natural gradient is equivalent updating a block-diagonal likelihood that decomposes across time; hence the approximate posterior is computable via Kalman smoothing algorithms. Extending to multiple latent functions (\(\mathrm{Q}>1\)) we define a full Gaussian approximate posterior that captures all correlations between the latent functions \(q(\bar{\mathbf{f}}_{1},\cdots,\bar{\mathbf{f}}_{\mathrm{Q}})\triangleq\mathrm{N} \left(\,\bar{\mathbf{f}}_{1},\cdots,\bar{\mathbf{f}}_{\mathrm{Q}}\,|\,\,\mathbf{m},\,\mathbf{S}\,\right)\) where \(\mathbf{m}\in\mathbb{R}^{(N\times Q)\times 1},\mathbf{S}\in\mathbb{R}^{(N\times Q) \times(N\times Q)}\). All the observation models in Eqn. (6) decompose across data points, hence Eqn. (16) is still block-diagonal and decomposes across time, except now each component is of dimension \(\mathrm{Q}\times N_{\mathrm{t}}\) as it encodes the correlations of spatial points and their spatio-temporal derivatives across the latent functions. We denote this model as physs-vgp and physs-eks when using a eks prior (see Ex. 3.1).

**Theorem 3.1**.: _Let the approximate posterior be (full) Gaussian \(q(\bar{\mathbf{f}}_{1},\cdots,\bar{\mathbf{f}}_{\mathcal{Q}})\triangleq\mathrm{N} \left(\,\bar{\mathbf{f}}_{1},\cdots,\bar{\mathbf{f}}_{\mathcal{Q}}\mid\mathbf{ m},\,\mathbf{S}\,\right)\) where \(\mathbf{m}\in\mathbb{R}^{(N\times Q)\times 1},\mathbf{S}\in\mathbb{R}^{(N\times Q) \times(N\times Q)}\). When \(g\) is linear a single natural gradient step with \(\beta=1\) recovers the optimal solution \(p(\bar{\mathbf{f}}_{1},\cdots,\bar{\mathbf{f}}_{\mathcal{Q}}\mid\mathbf{Y})\)._

We prove this in App. A.5.4. This result not only demonstrates the optimality of our proposed inference scheme in the linear Gaussian setting, but confirms that we recover batch models like pmm and helmholtz-gp, as well as eks (see Ex. 3.1).

## 4 Reducing the Spatial Computational Complexity

We now propose three approaches that reduce the cubic computational complexity in the number of spatial derivatives and locations. The first augments the process with inducing points that alleviate cubic costs associated with \(N_{\mathrm{s}}\). The second is a structured variational approximation that defines the approximate posterior only over the temporal prior and alleviates cubic costs associated with \(d_{s}\). Finally, we introduce spatial mini-batching that alleviates linear \(N_{\mathrm{s}}\) costs. When used in conjunction, the dominant computation cost is \(\mathcal{O}\left(N_{\mathrm{t}}\cdot d_{s}\cdot(M_{s}\cdot d_{t})^{3}\right)\). These approximations are not only useful for the state-space setting and can readily be applied to reduce the computational complexity for batch variational models (such as autoip). See App. B.1 for more details.

Spatio-Temporal Inducing Points (physs-svgp)In this first approximation, denoted by physs-svgp, we augment the full prior \(p(\bar{\mathbf{f}})\) with inducing points. By defining these inducing points on a spatio-temporal grid, we will show that we can still exploit Markov conjugate operations through natural gradients. Let \(\bar{\mathbf{u}}=\mathcal{D}\,\mathbf{u}\in\mathbb{R}^{M\times D}\) be inducing points at locations \(\mathbf{Z}\in\mathbb{R}^{M\times F}\). From the standard svgp formulation [27], the elbo is

\[\mathcal{L}=\mathbb{E}_{\,q(\bar{\mathbf{f}},\bar{\mathbf{u}})}\left[\,\log \frac{p(\mathbf{Y}\,|\,\bar{\mathbf{f}})\,p(\bar{\mathbf{u}})}{q(\bar{ \mathbf{u}})}\,\right]\] (18)

where \(q(\bar{\mathbf{f}},\bar{\mathbf{u}})\triangleq p(\bar{\mathbf{f}}\,|\,\bar{ \mathbf{u}})\,q(\bar{\mathbf{u}})\). By defining the inducing points on a spatio-temporal grid at temporal locations \(\mathbf{X}_{\mathrm{t}}\in\mathbb{R}_{\mathrm{t}}^{N}\) and spatial \(\mathbf{Z}_{\mathrm{s}}\in\mathbb{R}^{M_{s}\times(F-1)}\) then the marginal \(p(\bar{\mathbf{f}}\,|\,\bar{\mathbf{u}})\) is Gaussian with mean

\[\mu_{\mathbf{F}\,|\,\mathbf{U}}=\left[\,\mathbf{I}\otimes\mathbf{K}_{s}^{ \mathcal{D}}(\mathbf{X}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,(\mathbf{K}_{s }^{\mathcal{D}}(\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}}))^{-1}\, \right]\,\bar{\mathbf{u}}\] (19)

and variance given in Eqn. (41). This Kronecker structure allows us to again 'decouple' space and time, leading to natural gradient updates with block size \(M_{s}\times D\), reducing the computational complexity to \(\mathcal{O}(N\,(M_{s}\cdot d_{s}\cdot d)^{3})\). For full details, see App. A.5.1.

Structured Variational Inference (physs-svgp\({}_{\text{\sc{th}}}\))This second approximation, denoted as physs-svgp\({}_{\text{\sc{th}}}\), defines the inducing points _only_ over the temporal derivatives. This is a useful approximation as it can drastically reduce the size of the filter state, making it more computationally and memory efficient. We begin by defining the joint prior as

\[p(\mathbf{F},\mathcal{D}_{\mathrm{t}}\,\mathbf{f})=p(\mathbf{F}\,|\,\mathcal{D }_{\mathrm{t}}\,\mathbf{f})\,p(\mathcal{D}_{\mathrm{t}}\,\mathbf{f})\]

where \(p(\mathbf{F}\,|\,\mathcal{D}_{\mathrm{t}}\,\mathbf{f})\) is a Gaussian conditional with mean

\[\mathbb{E}\left[\mathbf{F}\,|\,\mathcal{D}_{\mathrm{t}}\,\mathbf{f}\right]= \left[\,\mathbf{I}\otimes\widetilde{\mathbf{K}}_{s}^{\mathcal{D}}(\mathbf{X}_{ \mathrm{s}},\mathbf{X}_{\mathrm{s}})\,\mathbf{K}_{s}(\mathbf{Z}_{s},\mathbf{Z} _{\mathrm{s}})^{-1}\,\right]\mathcal{D}_{\mathrm{t}}\,\mathbf{f},\,\,\,\text{ with }\,\,\widetilde{\mathbf{K}}_{s}^{\mathcal{D}}(\mathbf{X}_{\mathrm{s}},\mathbf{X}_{ \mathrm{s}})=\begin{bmatrix}\mathbf{K}_{s}(\mathbf{X}_{\mathrm{s}},\mathbf{Z} _{\mathrm{s}})\\ \mathcal{D}_{\mathrm{s}}\,\mathbf{K}_{s}(\mathbf{X}_{\mathrm{s}},\mathbf{Z}_{ \mathrm{s}})\end{bmatrix}.\]

We then define a structured variational posterior

\[q(\bar{\mathbf{f}},\mathcal{D}_{\mathrm{t}}\,\mathbf{f})\triangleq p(\mathbf{F} \,|\,\mathcal{D}_{\mathrm{t}}\,\mathbf{f})\,q(\mathcal{D}_{\mathrm{t}}\, \mathbf{f}).\]

Substituting this into the elbo we see that all the terms with the prior spatial derivatives cancel

\[\mathbb{E}_{\,q}\left[\,\log\frac{p(\mathbf{Y}\,|\,\bar{\mathbf{f}})\,p(\bar{ \mathbf{f}}|\mathcal{P}_{\mathrm{t}}\bar{\mathbf{f}})\,p(\mathcal{D}_{\mathrm{t }}\,\mathbf{f})}{p(\mathbf{F}|\,\!+\!\mathcal{D}_{\mathrm{t}}\,\!\overline{ \mathbf{f}})\,q(\mathcal{D}_{\mathrm{t}}\,\mathbf{f})}\,\right]\]

Again, the marginal \(q(\mathcal{D}_{\mathrm{t}}\,\mathbf{f})\) maintains Kronecker structure, enabling Markov conjugate operations, leading to a computational cost of \(\mathcal{O}(N\cdot d_{s}\cdot(N_{\mathrm{s}}\cdot d)^{3})\), see App. A.5.2. These variational approximations can simply be applied to non-state-space variational approximation, see App. B.1.

Spatial Mini-BatchingA standard approach for handling big data is through mini-batching where the ell is approximated using only a data subsample [27]. Directly appling mini-batching would be of little computation benefit because computation of the elbo requires running a Kalman smoother that iterates through all time points. Instead, we mini-batch by subsampling \(B_{\mathrm{s}}\) spatial points

\[\textsc{{ell}}\approx\sum_{\mathrm{t}}^{N_{\mathrm{t}}}\frac{N_{\mathrm{s}}}{B_ {\mathrm{s}}}\sum_{i}^{B_{\mathrm{s}}}\mathbb{E}_{q}\left[\,\log p(\mathbf{Y}_ {\mathrm{t},\mathrm{s}}\,|\,\bar{\mathbf{f}}_{i,i})\,\right]\] (20)

where \(i\) is uniformly sampled. We used in conjunction with physs-svgp and physs-svgp\({}_{\mathrm{H}}\), this results in dominant costs of \(\mathcal{O}(N_{\mathrm{t}}\,(M_{s}\cdot d_{s}\cdot d)^{3})\) and \(\mathcal{O}\left(N_{\mathrm{t}}\cdot d_{s}\cdot(M_{s}\cdot d)^{3}\right)\) when \(B_{s}\ll N_{\mathrm{s}}\).

## 5 Handling the PSD Constraint

As discussed in Sec. 3.2 when the differential equation is non-linear, the model is no longer conjugate and the resulting natural gradients are not guaranteed to result in _p.s.d_ updates. This issue has received some attention in the literature [53; 64; 39], but these approaches do not maintain an efficient conjugate representation. One distinction is [72], which uses the Gauss-Newton approximation to maintain conjugate operations. We now extend this to support spatial inducing points and non-linear transformations. Due to space we focus on physs-svgp, but see App. A.5.3 for further details. The troublesome term for the natural gradient update in Eqn. (14) is the Jacobian of the ell_w.r.t._ to the second expectation parameter; which is not guaranteed to be _p.s.d_ unless the ell is log convex [39]. Focusing at a single location \(n=(t,s)\):

\[\frac{\partial\textsc{{ell}}}{\partial[\bm{\mu}_{k}]_{\bar{z}}}=\frac{\partial }{\partial\mathbf{S}_{u}}\mathbb{E}_{q(\bar{\mathbf{u}}_{i})}\left[\mathbb{E}_ {p(\bar{\mathbf{f}}_{n}|\bar{\mathbf{u}}_{i})}\left[\,\log p(\mathbf{Y}_{n}\,| \,\bar{\mathbf{f}}_{n})\,\right]\right]\]

we apply the Bonnet's and Price's theorem [38] to bring the differential inside the expectation and make a Gauss-Newton [19] approximation ensuring that the Jacobian is _p.s.d_

\[\frac{\partial\textsc{{ell}}}{\partial[\bm{\mu}_{t}]_{\bar{z}}}\approx\sum_{n, p}^{N}\mathbb{E}_{q(\bar{\mathbf{u}}_{i})}\left[\,\mathbf{J}_{n,p}^{\top}\, \mathbf{H}_{n,p}\,\mathbf{J}_{n,p}\,\right],\text{ where }\,\mathbf{J}_{n,p}=\frac{\partial g_{n}(\mu_{n})}{ \partial\bar{\mathbf{u}}_{t}},\,\,\,\mathbf{H}_{n,p}=\frac{\mathrm{d}^{2}\! \log p(\mathbf{Y}_{n}\,|\,g_{n})}{\mathrm{d}^{2}g_{n}},\] (21)

and \(g_{n,p}=g(\bar{\mathbf{u}}_{n})\) (Eqn. (4)) and \(\mu_{n}\) is the mean of \(p(\bar{\mathbf{f}}_{n}\,|\,\bar{\mathbf{u}}_{t})\) (Eqn. (19)). When using spatial mini-batching Eqn. (21) is also subsampled.

## 6 Related Work

From the optimality of natural gradients, in the conjugate setting, we exactly recover batch gp based models such as [68; 29; 4]. Our inference scheme also applies to models that do not require derivative information i.e. in \(d_{t}=d_{s}=1\). As a special case, we recover [20], but we have extended the inference scheme to support spatial mini-batching, allowing big spatial datasets to be used. The linear weighting matrix can be used to define a linear model of coregionalisation and its variants [7; 77; 42; 66] and through appropriately designed functionals also non-linear variants [73].

In Alvarez et al. [76] gp priors over the solution of differential equations are obtained through a stochastic forcing term but they only consider situations where the Greens function is available. In [22; 23; 57; 33], efficient state-space algorithms are derived but are limited to the temporal setting only. Similarly, Heinonen et al. [24], learn a 'free-form ODE'. In the spatio-temporal setting Kramer et al. [35] and Duffin et al. [14] (which builds [18]) derive extended Kalman filter algorithms. Additionally there are approaches to constraining gps by linear differential equations [37; 1; 5]. More generally than [4] in [21] gp priors over the solutions to linear PDEs with constant coefficients are derived.

Beyond gp based models, physics informed neural networks (PINNs) incorporate physics by constructing a loss function between the network and the differential equation at a finite set of collocation points [48]. This amounts to a highly complex optimisation problem [36] bringing difficulties for training [70; 71] and uncertainty quantification (UQ) [16]. Current approaches to quantifying uncertainty in PINNs are based on dropout [75] and conformal predictions [47]. In recent years UQ and deep learning has received much attention however is limited by its computational cost [45].

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

points and a spatial mini-batch size of \(10\), and plot results in Fig. 4. Our predictions are in excellent agreement with the test data, achieving an rmse of \(0.14\), nlpd of \(-0.52\), crps of \(0.078\), and an average run-time of \(1.86(s)\) per epoch.

## 8 Conclusion

We introduced a physics-informed state-space gp that integrates observational data with physical knowledge. Within the variational inference framework, we derived a computationally efficient algorithm that uses Kalman smoothing to achieve linear-in-time costs. To gain further computational speed-ups, we proposed three approximations with inducing points, spatial mini batching and structured variational posteriors. When used in conjunction, they allow us to handle large-scale spatiotemporal problems. The bottleneck is always the state size, where nearest neighbours gps [13; 74] could be explored. For highly non-linear problems, future directions could explore deep approaches [52] or more flexible kernel families [69]. One limitation is the use of the collocation method which is only enforcing the differential equation point wise, whilst future work could look at the more general methods of weighted residuals [46].

## Acknowledgments and Disclosure of Funding

OH acknowledges funding from The Alan Turing Institute PhD fellowship programme and the UKRI Turing AI Fellowship (EP/V02678X/1). AS acknowledges support from the Research Council of Finland (339730). TD acknowledges support from UKRI Turing AI Acceleration Fellowship (EP/V02678X/1) and a Turing Impact Award from the Alan Turing Institute. The authors acknowledges the University of Warwick Research Technology Platform (aquifer) for assistance in the research described in this paper. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC-BY) license to any Author Accepted Manuscript version arising from this submission.

## References

* [1] C. G. Albert. Gaussian processes for data fulfilling linear differential equations. In _Proceedings_, volume 33, page 5. MDPI, 2019.
* [2] S.-i. Amari. Natural gradient works efficiently in learning. _Neural Computation_, 10(2):251-276, 1998.
* [3] G. B. Arfken, H. J. Weber, and F. E. Harris. _Mathematical Methods for Physicists: A Comprehensive Guide_. Academic Press, 2011.
* [4] R. Berlinghieri, B. L. Trippe, D. R. Burt, R. Giordano, K. Srinivasan, T. Ozgokmen, J. Xia, and T. Broderick. Gaussian processes at the Helm (holtz): A more fluid model for ocean currents. _arXiv preprint arXiv:2302.10364_, 2023.

Figure 4: Predicted ocean currents by physs-svgp\({}_{\text{H}}\). True observations are in grey, and predictions in green. The thickness of the line represents uncertainty and is computed by the L2 norm of the standard deviations across both outputs.

* [5] A. Besginow and M. Lange-Hegermann. Constraining Gaussian processes to systems of linear ordinary differential equations. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, pages 29386-29399. Curran Associates, Inc., 2022.
* [6] F. Black and M. Scholes. The pricing of options and corporate liabilities. _Journal of Political Economy_, 81(3):637-654, 1973.
* [7] E. V. Bonilla, K. Chai, and C. Williams. Multi-task Gaussian process prediction. In _Advances in Neural Information Processing Systems 20 (NIPS)_. Curran Associates, Inc., 2008.
* [8] D. Borthwick. _Introduction to Partial Differential Equations_. Universitext. Springer International Publishing, 2017.
* [9] J. Butcher. _Numerical Methods for Ordinary Differential Equations_. Wiley, 2016.
* [10] P. E. Chang, W. J. Wilkinson, M. E. Khan, and A. Solin. Fast variational learning in state-space Gaussian process models. In _30th IEEE International Workshop on Machine Learning for Signal Processing (MLSP)_, pages 1-6. IEEE, 2020.
* [11] T. Chow. _Introduction to Electromagnetic Theory: A Modern Perspective_. Jones and Bartlett Publishers, 2006.
* [12] J. Cockayne, C. Oates, T. Sullivan, and M. Girolami. Probabilistic numerical methods for PDE-constrained Bayesian inverse problems. _AIP Conference Proceedings_, 1853(1), 06 2017.
* [13] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets. _Journal of the American Statistical Association_, 111(514):800-812, 2016.
* [14] C. Duffin, E. Cripps, T. Stemler, and M. Girolami. Low-rank statistical finite elements for scalable model-data synthesis. _Journal of Computational Physics_, 463:111261, 2022.
* [15] E. D'Asaro, C. Guigand, A. Haza, H. Huntley, G. Novelli, T. Ozgokmen, and E. Ryan. Lagrangian submesoscale experiment (LASER) surface drifters, interpolated to 15-minute intervals, 2017. URL https://data.gulfresearchinitiative.org/data/R4.x265.237:0001.
* [16] C. Edwards. Neural networks learn to speed up simulations. _Communications of the ACM_, 65:27-29, 04 2022.
* [17] D. Eriksson, K. Dong, E. Lee, D. Bindel, and A. G. Wilson. Scaling Gaussian process regression with derivatives. In _Advances in Neural Information Processing Systems 31 (NeurIPS)_. Curran Associates, Inc., 2018.
* [18] M. Girolami, E. Febrianto, G. Yin, and F. Cirak. The statistical finite element method (statFEM) for coherent synthesis of observation data and model predictions. _Computer Methods in Applied Mechanics and Engineering_, 375:113533, 2021.
* [19] G. H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate. _SIAM Journal on Numerical Analysis_, 10(2):413-432, 1973.
* [20] O. Hamelijnck, W. J. Wilkinson, N. A. Loppi, A. Solin, and T. Damoulas. Spatio-temporal variational Gaussian processes. In _Advances in Neural Information Processing Systems (NeurIPS)_. Curran Associates, Inc., 2021.
* [21] M. Harkonen, M. Lange-Hegermann, and B. Raita. Gaussian process priors for systems of linear partial differential equations with constant coefficients. In _International Conference on Machine Learning_, pages 12587-12615. PMLR, 2023.
* [22] J. Hartikainen and S. Sarkka. Sequential inference for latent force models. In _Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence_, UAI'11, page 311-318, Arlington, Virginia, USA, 2011. AUAI Press.

* [23] J. Hartikainen, M. Seppanen, and S. Sarkka. State-space inference for non-linear latent force models with application to satellite orbit prediction. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, ICML'12, page 723-730. Omnipress, 2012.
* [24] M. Heinonen, Cagatay Yildiz, H. Mannerstrom, J. Intosalmi, and H. Lahdesmaki. Learning unknown ODE models with Gaussian processes. In _International Conference on Machine Learning (ICML)_, 2018.
* [25] P. Hennig, M. Osborne, and H. Kersting. _Probabilistic Numerics: Computation as Machine Learning_. Cambridge University Press, 2022.
* [26] J. Hensman, M. Rattray, and N. D. Lawrence. Fast variational inference in the conjugate exponential family. In _Advances in Neural Information Processing Systems (NIPS)_, pages 2888-2896. Curran Associates Inc., 2012.
* [27] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 282-290. AUAI Press, 2013.
* [28] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. _Journal of Machine Learning Research_, 14, 2013.
* [29] C. Jidling, N. Wahlstrom, A. Wills, and T. B. Schon. Linearly constrained Gaussian processes. In _Advances in Neural Information Processing Systems 30 (NeurIPS)_. Curran Associates, Inc., 2017.
* [30] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* [31] M. E. Khan and W. Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2017.
* [32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] N. Kramer and P. Hennig. Linear-time probabilistic solution of boundary value problems. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 11160-11171. Curran Associates, Inc., 2021.
* [34] N. Kramer and P. Hennig. Stable implementation of probabilistic ODE solvers. _Journal of Machine Learning Research_, 25(111):1-29, 2024.
* [35] N. Kramer, J. Schmidt, and P. Hennig. Probabilistic numerical method of lines for time-dependent partial differential equations. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 625-639. PMLR, 2022.
* [36] A. S. Krishnapriyan, A. Gholami, S. Zhe, R. Kirby, and M. W. Mahoney. Characterizing possible failure modes in physics-informed neural networks. _Advances in Neural Information Processing Systems 34 (NeurIPS)_, 2021.
* [37] M. Lange-Hegermann. Algorithmic linearly constrained Gaussian processes. In _Advances in Neural Information Processing Systems 31 (NeurIPS)_, pages 2137-2148. Curran Associates, Inc., 2018.
* [38] W. Lin, M. E. Khan, and M. Schmidt. Stein's lemma for the reparameterization trick with exponential family mixtures. _arXiv preprint arXiv:1910.13398_, 2019.
* [39] W. Lin, M. Schmidt, and M. E. Khan. Handling the positive-definite constraint in the Bayesian learning rule. In _Proceedings of the 37th International Conference on Machine Learning_, Proceedings of Machine Learning Research. PMLR, 2020.

* [40] D. Long, Z. Wang, A. Krishnapriyan, R. Kirby, S. Zhe, and M. Mahoney. AutoIP: A united framework to integrate physics into Gaussian processes. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_. PMLR, 17-23 Jul 2022.
* [41] A. Mchutchon and C. Rasmussen. Gaussian process training with input noise. In _Advances in Neural Information Processing Systems 25 (NeurIPS)_. Curran Associates, Inc., 2011.
* [42] P. Moreno-Munoz, A. Artes, and M. Alvarez. Heterogeneous multi-output Gaussian process prediction. In _Advances in Neural Information Processing Systems 31 (NeurIPS)_. Curran Associates, Inc., 2018.
* [43] M. Paddar, X. Zhu, L. Huang, J. R. Gardner, and D. S. Bindel. Scaling Gaussian processes with derivative information using variational inference. In _Advances in Neural Information Processing Systems (NeurIPS)_. Curran Associates, Inc., 2021.
* [44] I. Pan, L. R. Mason, and O. K. Matar. Data-centric engineering: integrating simulation, machine learning and statistics. challenges and opportunities. _Chemical Engineering Science_, 249:117271, 2022.
* [45] T. Papamarkou, M. Skoularidou, K. Palla, L. Aitchison, J. Arbel, D. Dunson, M. Filippone, V. Fortuin, P. Hennig, J. M. Hernandez-Lobato, A. Hubin, A. Immer, T. Karaletsos, M. E. Khan, A. Kristiadi, Y. Li, S. Mandt, C. Nemeth, M. A. Osborne, T. G. J. Rudner, D. Rugamer, Y. W. Teh, M. Welling, A. G. Wilson, and R. Zhang. Position: Bayesian deep learning is needed in the age of large-scale AI. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 39556-39586. PMLR, 2024.
* [46] M. Pfortner, I. Steinwart, P. Hennig, and J. Wenger. Physics-informed Gaussian process regression generalizes linear PDE solvers. _arXiv preprint arXiv:2212.12474_, 2022.
* [47] L. Podina, M. T. Rad, and M. Kohandel. Conformalized physics-informed neural networks. In _ICLR 2024 Workshop on AI4Differential Equations in Science_, 2024. URL https://openreview.net/forum?id=ZoFwS818qG.
* [48] M. Raissi and G. E. Karniadakis. Hidden physics models: Machine learning of nonlinear partial differential equations. _Journal of Computational Physics_, 2017.
* [49] M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational Physics_, 378:686-707, 2019.
* [50] C. Rasmussen and C. Williams. _Gaussian Processes for Machine Learning_. MIT Press, Cambridge, MA, USA, 2006.
* [51] J. Riihimaki and A. Vehtari. Gaussian processes with monotonicity information. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 9 of _Proceedings of Machine Learning Research_. PMLR, 2010.
* [52] H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In _Advances in Neural Information Processing Systems 30 (NeurIPS)_, pages 4588-4599. Curran Associates, Inc., 2017.
* [53] H. Salimbeni, S. Eleftheriadis, and J. Hensman. Natural gradients in practice: Non-conjugate variational inference in gaussian process models. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2018.
* Volume Part II_, ICANN'11, page 151-158. Springer-Verlag, 2011.
* [55] S. Sarkka and A. F. Garcia-Fernandez. Temporal parallelization of bayesian smoothers. _IEEE Transactions on Automatic Control_, 2021.

* [56] S. Sarkka and A. Solin. _Applied Stochastic Differential Equations_. Cambridge University Press, 2019.
* [57] J. Schmidt, N. Kramer, and P. Hennig. A probabilistic state space model for joint inference from differential equations and data. In _Advances in Neural Information Processing Systems 34 (NeurIPS)_, pages 12374-12385. Curran Associates, Inc., 2021.
* [58] M. Schober, D. K. Duvenaud, and P. Hennig. Probabilistic ODE solvers with Runge-Kutta means. In _Advances in Neural Information Processing Systems 27 (NeurIPS)_. Curran Associates, Inc., 2014.
* [59] E. Solak, R. Murray-smith, W. Leithead, D. Leith, and C. Rasmussen. Derivative observations in Gaussian process models of dynamic systems. In _Advances in Neural Information Processing Systems 15 (NIPS)_. MIT Press, 2002.
* [60] A. Solin. _Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression_. Doctoral thesis, Aalto University, 2016.
* [61] A. Solin, M. Kok, N. Wahlstrom, T. B. Schon, and S. Sarkka. Modeling and interpolation of the ambient magnetic field by Gaussian processes. _Transactions on Robotics_, 34(4):1112-1127, 2018.
* [62] T. Stocker. _Introduction to Climate Modelling_. Springer Science & Business Media, 2011.
* [63] S. Sarkka. _Bayesian Filtering and Smoothing_. Institute of Mathematical Statistics Textbooks. Cambridge University Press, 2013.
* [64] M.-N. Tran, D. H. Nguyen, and D. Nguyen. Variational Bayes on manifolds. _Statistics and Computing_, 31:1-17, 2021.
* [65] F. Tronarp, H. Kersting, S. Sarkka, and P. Hennig. Probabilistic solutions to ordinary differential equations as nonlinear Bayesian filtering: a new perspective. _Statistics and Computing_, 29:1297-1315, 2018.
* 447, 2020.
* [67] C. Villacampa-Calvo, B. Zaldivar, E. C. Garrido-Merchan, and D. Hernandez-Lobato. Multi-class Gaussian process classification with noisy inputs. _Journal of Machine Learning Research_, 22(1), 2021.
* [68] N. Wahlstrom, M. Kok, T. B. Schon, and F. Gustafsson. Modeling magnetic fields using Gaussian processes. In _IEEE International Conference on Acoustics, Speech and Signal Processing_, pages 3522-3526, 2013.
* [69] K. Wang, O. Hamelijnck, T. Damoulas, and M. Steel. Non-separable non-stationary random fields. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 9887-9897. PMLR, 2020.
* [70] S. Wang, X. Yu, and P. Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. _Journal of Computational Physics_, 449:110768, 2022.
* [71] S. Wang, S. Sankaran, and P. Perdikaris. Respecting causality for training physics-informed neural networks. _Computer Methods in Applied Mechanics and Engineering_, 421:116813, 2024.
* [72] W. J. Wilkinson, S. Sarkka, and A. Solin. Bayes-Newton methods for approximate Bayesian inference with PSD guarantees. _Journal of Machine Learning Research_, 24(83):1-50, 2023.
* [73] A. G. Wilson, D. A. Knowles, and Z. Ghahramani. Gaussian process regression networks. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, page 1139-1146, 2012.

* [74] L. Wu, G. Pleiss, and J. P. Cunningham. Variational nearest neighbor Gaussian process. In _Proceedings of the 39th International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 24114-24130. PMLR, 2022.
* [75] D. Zhang, L. Lu, L. Guo, and G. E. Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems. _Journal of Computational Physics_, 397:108850, 2019.
* [76] M. Alvarez, D. Luengo, and N. D. Lawrence. Latent force models. In _Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 5 of _Proceedings of Machine Learning Research_, pages 9-16. PMLR, 2009.
* [77] M. A. Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: A review. _Foundations and Trends in Machine Learning_, 4(3):195-266, 2012.

## Appendix A Variational Approximation Derivation

### Overview of Notation

### Layout of Vectors and Matrices

We use a numerator layout for derivatives. Let \(Q\) denote the number of independent latent functions and \(D\) the number of derivatives computed, and let \(f_{q,d}\) denote the latent gp for \(d\)'th derivative of the \(q\)'th latent function. We will need to keep track of the permutation of our data _w.r.t._ to space, time, and latent functions. Inspired by 'row-major' and 'column-major' layouts, we will use the following terminology that describes the ordering of the data across latent functions and time and space:

* **latent-data:**\(\mathbf{F}=\mathbf{F}_{\text{ld}}=[\mathbf{F}_{1}(\mathbf{X}),\cdots,\mathbf{F}_{Q} (\mathbf{X})]\) with \(\mathbf{F}_{q}(\mathbf{X})=[\mathbf{f}_{q,1}(\mathbf{X}),\cdots,\mathbf{f}_{q,D}(\mathbf{X})]\) which is ordered by stacking each of the latent functions on top of each other.
* **data-latent:**\(\mathbf{F}_{\text{dl}}=[\mathbf{F}_{1}(\mathbf{X}_{n}),\cdots,\mathbf{F}_{Q} (\mathbf{X}_{n})]_{n}^{N}\) which is ordered by stacking the latent functions evaluated at each data point across all data points.
* **time-space:**\([\mathbf{f}(\mathbf{X}_{\text{t}}^{(\text{st})})]_{\text{t}}^{N_{\text{t}}}\) which is ordered by stacking each of the input points at each time point on top of each other. This is only applicable when there is a single latent function.
* **latent-time-space:**\([\mathbf{F}_{1}(\mathbf{X}_{1}^{(\text{st})}),\cdots,\mathbf{F}_{1}( \mathbf{X}_{N_{\text{t}}}^{(\text{st})}),\cdots,\mathbf{F}_{Q}(\mathbf{X}_{ 1}^{(\text{st})}),\cdots,\mathbf{F}_{Q}(\mathbf{X}_{N_{\text{t}}}^{(\text{st} )})]\)

\begin{table}
\begin{tabular}{c c c} \hline \hline Symbol & Size & Description \\ \hline \(N\) &  & Number of observations. \\ \(Q\) &  & Number of latent functions. \\ \(P\) &  & Number of latent outputs. \\ \(F\) &  & Number of input features. \\ \(N_{\text{s}}\) &  & Number of spatial points. \\ \(N_{\text{t}}\) &  & Number of temporal points. \\ \(d_{s}\) &  & Number of spatial derivatives. \\ \(d_{t}\) &  & Number of temporal derivatives. \\ \(D=d_{s}\cdot d_{t}\) &  & Total number of spatio-temporal derivatives. \\ \(d\) &  & State dimension. \\ \(\mathbf{B}_{s}\) &  & Spatial batch size. \\ \(M_{s}\) &  & Number of spatial inducing points. \\ \(\mathbf{X}\) & \(N\times F\) & Input data matrix. \\ \(\mathbf{X}_{\text{s}}\) & \(N_{\text{s}}\times F\) & Spatial Locations of training data. \\ \(\mathbf{X}_{\text{t}}\) & \(N_{\text{t}}\) & Temporal locations of training data. \\ \(\mathbf{x},\mathbf{X}_{n},\mathbf{X}_{t,z}\) & \(F\) & Single training input. \\ \(\mathbf{x}_{t}\) &  & Temporal axis of a single training input location \(\mathbf{x}\). \\ \(\mathbf{x}_{s}\) & \(F-1\) & Spatial axes of a single training input location \(\mathbf{x}_{..}\) \\ \(\mathbf{Y}\) & \(N\times P\) & Output data matrix. \\ \(\mathbf{Y}_{n_{i}},\mathbf{Y}_{t,z}\) & \(P\) & Single training output. \\ \(\bar{\mathbf{f}}\) & \(N_{\text{s}}\times d\) & Filtering state. \\ \(\mathbf{W}\) & \((P\times D)\times(Q\times D)\) & Mixing matrix between \(Q\) latent gps \\ \(\bar{f}_{q}(\mathbf{X}_{n})\) & \(D\) & Random vector of the \(D\) derivatives at location \(\mathbf{X}_{n}\) \\ \(\mathbf{F}_{n}\) & \((P\times D)\) & Output of linearly mixed gps. \\ \(g:\mathbb{R}^{P\cdot D}\rightarrow\mathbb{R}\) &  & Differential equation defined using \(D\) spatio-temporal derivatives and \(P\) outputs/states. \\ \(\mathbf{Z}_{s}\) & \(\mathbb{R}^{M_{\text{s}}\times(F-1)}\) & Spatial Inducing Points. \\ \(\mathbf{K}_{s}(\mathbf{X}_{\text{s}},\mathbf{X}_{\text{s}})\) & \(N_{\text{s}}\times N_{\text{s}}\) & Spatial Kernel. \\ \(\mathbf{K}_{t}(\mathbf{X}_{t},\mathbf{X}_{t})\) & \(N_{\text{t}}\times N_{\text{t}}\) & Temporal Kernel. \\ \(\mathbf{K}(\mathbf{X},\mathbf{X})\) & \(\mathrm{N}\times\mathrm{N}\) & Spatio-Temporal Kernel. \\ \(\bar{\mathbf{K}}=\mathcal{D}\,\mathbf{K}(\mathbf{X},\mathbf{X})\,\mathcal{D}^{*}\) & \((\mathrm{N}\cdot D)\times(\mathrm{N}\cdot D)\) & Spatio-temporal kernel over all N locations and \(D\) derivatives. \\ \(\mathbf{K}_{t}^{\text{D}}(\mathbf{X}_{t},\mathbf{X}_{t})\) & \((N_{\text{t}}\times d_{t})\times(N_{\text{t}}\times d_{t})\) & Gram matrix over temporal derivatives. \\ \(\mathbf{K}_{s}^{\text{D}}(\mathbf{X}_{t},\mathbf{X}_{t})\) & \((N_{\text{s}}\times d_{s})\times(N_{\text{s}}\times d_{s})\) & Gram matrix over spatial derivatives. \\ \hline \hline \end{tabular}
\end{table}
Table 4: Table of Notation* **time-latent-space:**\([\mathbf{F}_{1}(\mathbf{X}_{\mathbf{t}}^{(\text{st})}),\cdots,\mathbf{F}_{Q}( \mathbf{X}_{\mathbf{t}}^{(\text{st})})]_{\mathbf{t}}^{N_{\mathbf{t}}}\)

The default order will be latent-data (and latent-time-space for spatio-temporal problems). Since all of these are just simple permutations of each other, there exists a permutation matrix that permutes between any two of the layouts above. We use the function \(\pi\) to denote a function that performs this permutation such that:

\[\mathbf{F}_{\text{dl}} =\pi_{\text{ld}\rightarrow\text{dl}}(\mathbf{F}_{\text{ld}})\] (24) \[\mathbf{F}_{\text{ld}} =\pi_{\text{dl}\rightarrow\text{ld}}(\mathbf{F}_{\text{dl}})\]

### Timeseries Setting - Single Latent Function

Let \(\mathbf{X}\in\mathbb{R}^{N\times D},\mathbf{Y}\in\mathbb{R}^{N\times P}\) be input-output observations across \(P\) outputs, where \(N=N_{\mathbf{t}}\). For now, we only consider the case where \(Q=1\). We assume that \(f\) has a state-space representation, and we denote its state with its \(D\) time derivatives as \(\mathbf{F}(\mathbf{X})=[\mathbf{f}(\mathbf{X}),\frac{\partial\mathbf{f}( \mathbf{X})}{\partial\mathbf{X}},\frac{\partial^{2}\mathbf{f}(\mathbf{X})}{ \partial\mathbf{X}^{2}},\cdots]\) in latent-data format. The vector \(\mathbf{F}(\mathbf{X})\) is of dimension \((N\times D)\). We also use the notation \(\mathbf{F}_{n}=\mathbf{F}(\mathbf{X}_{n})\), which is a \(D\)-dimensional vector of the derivatives at location \(\mathbf{X}_{n}\). The joint model is

\[\left[\;\prod_{n}^{N}p(\mathbf{Y}_{n}\,|\,\mathbf{F}_{n},\text{DE})\;\right]p (\mathbf{F}).\] (25)

At this point, we place no particular restriction on the form of the likelihood, aside from decomposing across \(N\). The prior \(p(\mathbf{F})\) is a multivariate gp of dimension \(N\times D\)

\[p(\mathbf{F})=\text{N}\,(\,\mathbf{F}\;|\;\mathbf{0},\,\mathcal{D}\,\mathbf{ K}(\mathbf{x},\mathbf{x})\,\mathcal{D}^{*}\,)\] (26)

Let \(q(\mathbf{F})\) be a free-form multivariate Gaussian of the same dimension as \(p(\mathbf{F})\) then the corresponding elbo is:

\[\mathcal{L} =\mathbb{E}_{\,q(\mathbf{F})}\left[\,\log\frac{p(\mathbf{Y}\,|\, \mathbf{F},\text{DE})\,p(\mathbf{F})}{q(\mathbf{F})}\;\right],\] (27) \[=\mathbb{E}_{\,q(\mathbf{F})}\left[\,\log p(\mathbf{Y}\,|\, \mathbf{F},\text{DE})\,\right]-\mathcal{KL}\left[\,q(\mathbf{F})\,||\,p( \mathbf{F})\,\right],\] \[=\underbrace{\sum_{n}^{N}\mathbb{E}_{\,q(\mathbf{F}_{n})}\left[\, \log p(\mathbf{Y}_{n}\,|\,\mathbf{F}_{n},\text{DE})\,\right]}_{\text{ELL}}- \underbrace{\mathcal{KL}\left[\,q(\mathbf{F})\,||\,p(\mathbf{F})\,\right]}_{ \text{KL}},\]

and the marginal \(q(\mathbf{F}_{n})\) is a \(D\)-dimensional Gaussian corresponding to the \(n\)'th observation. The natural gradients are

\[\widetilde{\boldsymbol{\lambda}} \leftarrow(1-\beta)\,\widetilde{\boldsymbol{\lambda}}+\beta\, \frac{\partial\text{ELL}}{\partial\boldsymbol{\mu}}\Big{\}} \text{Surrogate likelihood update}\] (28) \[\boldsymbol{\lambda} \leftarrow\widetilde{\boldsymbol{\lambda}}+\boldsymbol{\eta} \Big{\}} \text{Surrogate model update}\] (29)

where \(\widetilde{\boldsymbol{\lambda}}=\left[\widetilde{\boldsymbol{\lambda}}_{1}, \big{[}\widetilde{\boldsymbol{\lambda}}_{2}\big{]}^{\top}\text{ and }[\widetilde{\boldsymbol{\lambda}}_{1}\text{ is an }(N\times D)\text{ vector and }[\widetilde{\boldsymbol{\lambda}}]_{2}\text{ an }(N\times D)\times(N\times D)\text{ matrix.}\right.\) Eqn. (29) is a sum of natural parameters, and so is the conjugate Bayesian update. Naively computing this would yield no computation speed up as the computation cost would be cubic \(\mathcal{O}(N^{3})\). However, the natural parameters of the likelihood (\(\widetilde{\boldsymbol{\lambda}}\)) are guaranteed to be block diagonal, one block per data point (if \(\widetilde{\boldsymbol{\lambda}}_{0}\) is initialised as so). This immediately implies that Eqn. (29) can be computed using efficient Kalman filter and smoothing algorithms. The structure of \(\widetilde{\boldsymbol{\lambda}}\) depends on the gradient of the expected log-likelihood \(\frac{\partial\text{ELL}}{\partial\boldsymbol{\mu}}\). Expanding this out

\[\frac{\partial\text{ELL}}{\partial[\boldsymbol{\mu}]_{2}}=\sum_{n}^{N}\frac{ \partial\mathbb{E}_{\,q(\mathbf{F}_{n})}\left[\,\log p(\mathbf{Y}_{n}\,|\, \mathbf{F}_{n},\text{DE})\,\right]}{\partial[\boldsymbol{\mu}]_{2}}=\sum_{n}^ {N}\widetilde{\boldsymbol{\mu}}_{n}\] (30)

where each component \(\widetilde{\boldsymbol{\mu}}_{n}\) is a \((N\times D)\times(N\times D)\) matrix that only has \(D\times D\) non-zero entries; as these are the only elements that directly affect \(\mathbf{F}_{n}\). Collecting all these submatrices into a block diagonal matrix, we have a matrix in data-latent format, however, \(\frac{\partial\text{ELL}}{\partial\boldsymbol{\mu}}\) is in latent-data, and so all we need to do is permute by \(\mathbf{P}\):

\[\frac{\partial\text{ELL}}{\partial\boldsymbol{\mu}}=\pi_{\text{dl} \rightarrow\text{ld}}\left(\,\text{blkdiag}[\,\widetilde{\boldsymbol{\mu}}_{1}, \cdots,\widetilde{\boldsymbol{\mu}}_{N}\,]\,\right).\] (31)Converting from natural to moment paramiseration the surrogate update is:

\[q(\mathbf{F}) \propto\mathrm{N}\left(\,\widetilde{\mathbf{Y}}\,\mid\mathbf{F},\, \widetilde{\mathbf{V}}\,\right)\,p(\mathbf{F})\] (32) \[=\left[\,\prod_{n}^{N}\,\mathrm{N}\left(\,\widetilde{\mathbf{Y}}_ {n}\,\mid\,\mathbf{F}_{n},\,\widetilde{\mathbf{V}}_{n}\,\right)\,\,\right]p( \mathbf{F})\]

where \(\widetilde{\mathbf{Y}}_{n}\) is a \(D\)-dimensional vector, and \(\widetilde{\mathbf{V}}_{n}\) is a \(D\times D\) matrix, and efficient Kalman filtering and smoothing algorithms can be used to compute the surrogate model update. Substituting \(q(\mathbf{F})\) back into the elbo it further simplifies:

\[\mathcal{L} =\mathbb{E}_{\,q(\mathbf{F}_{n})}\left[\,\log p(\mathbf{Y}_{n}\, \mid\mathbf{F}_{n},\text{DE})\,\right]-\sum_{n}^{N}\mathbb{E}_{\,q(\mathbf{F} _{n})}\left[\,\log\mathrm{N}\left(\,\widetilde{\mathbf{Y}}_{n}\,\mid\, \mathbf{F}_{n},\,\widetilde{\mathbf{V}}_{n}\,\right)\,\right]+\log p( \widetilde{\mathbf{Y}}\,\mid\widetilde{\mathbf{V}})\] (33)

each term can be computed efficiently as the by-product of the Kalman filtering and smoothing algorithm used to compute \(q(\mathbf{F})\).

### Timeseries Setting - Multiple Latent Functions

We now generalise the previous section to handle multiple independent latent functions, _i.e._\(\mathrm{Q}>0\). The model prior now has the form

\[p(\mathbf{F})=\prod_{q}^{\mathrm{Q}}p(\mathbf{F}_{q})\] (34)

where \(p(\mathbf{F}_{q})\) is a prior over \(\mathbf{f}_{q,1}\) and its \(D\) partial derivatives. We consider two approaches: a mean-field approximate posterior and a full Gaussian.

The first approach defined mean-field approximate posterior \(q(\mathbf{F})\triangleq\prod_{q}^{\mathrm{Q}}q(\mathbf{F}_{q})\) where each \(q(\mathbf{F}_{q})\) is a free-form Gaussian of dimension \((N\times D)\). The natural gradient updates are now simply applied to each component \(q(\mathbf{F}_{q})\) separately, and we essentially follow the update set out in App. A.3.

The second approach is a full-Gaussian approximate posterior where \(q(\mathbf{F})\) is a \((\mathrm{Q}\times D\times N)\)-dimensional free-form Gaussian. In this case the ell is

\[\text{\sc ell}=\sum_{n}^{N}\mathbb{E}_{\,q(\mathbf{F}_{n})}\left[\,\log p( \mathbf{Y}_{n}\,\mid\mathbf{F}_{n},\text{DE})\,\right]\] (35)

where \(q(\mathbf{F}_{n})\) is of dimension \((\mathrm{Q}\times D)\). This implies that the gradient of the ell\(\frac{\partial\text{\sc ell}}{\partial[\boldsymbol{\mu}]_{s}}=\sum_{n}^{N} \widetilde{\boldsymbol{\mu}}_{n}\) where \(\widetilde{\boldsymbol{\mu}}_{n}\) now has \((\mathrm{Q}\times D)\times(\mathrm{Q}\times D)\) non-zero entries. Switching to moment parameterisation

\[q(\mathbf{F})\propto\left[\,\prod_{n}^{N}\,\mathrm{N}\left(\,\widetilde{ \mathbf{Y}}_{n}\,\mid\,\mathbf{F}_{n},\,\widetilde{\mathbf{V}}_{n}\,\right)\, \,\right]p(\mathbf{F})\] (36)

where \(\widetilde{\mathbf{Y}}_{n}\) is of dimension \((\mathrm{Q}\times D)\) and \(\widetilde{\mathbf{V}}_{n}\) is \((\mathrm{Q}\times D)\times(\mathrm{Q}\times D)\). We can still use state-space algorithms by simply stacking the states corresponding to each \(\mathbf{F}_{q}\)[56].

### Spatio-temporal Data - Single Latent Function

We now turn to the spatio-temporal setting where \(\mathbf{X}\), \(\mathbf{Y}\) are spatio-temporal observations on a spatio-temporal grid ordered in time-space format. We now derive the conjugate variational algorithm for physs-svgp and physs-svgp\({}_{\text{\sc ff}}\). The algorithms for physs-gp and physs-vgp\({}_{\text{\sc ff}}\) are recovered as special cases when \(\mathbf{Z}=\mathbf{X}_{\text{s}}\).

#### a.5.1 Spatial Derivative Inducing Points

We follow the standard sparse variational GP procedure and augment that prior with inducing points \(\mathbf{U}=\mathcal{D}\,\mathbf{u}\) at locations \(\mathbf{Z}\). We require that the inducing points are defined on a spatial-temporal grid at the same temporal points as the data \(\mathbf{X}\), such that \(\mathbf{Z}=[\mathbf{Z}_{t}]_{t}^{N_{t}}\). This is required to ensure Kronecker structure between the inducing points and the data. The joint model is

\[p(\mathbf{Y}\,|\,\mathbf{F})\,p(\mathbf{F}\,|\,\mathbf{U})\,p(\mathbf{U})\] (37)

where

\[p(\mathbf{U})=\mathrm{N}\left(\,\mathbf{U}\,\mid\,\mathbf{0},\,\mathbf{K}_{t }^{\mathcal{D}}(\mathbf{X}_{\mathrm{t}},\mathbf{X}_{\mathrm{t}})\otimes \mathbf{K}_{s}^{\mathcal{D}}(\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}}) \,\right),\] (38)

and the conditional mean and covariance are given by

\[\mu_{\mathbf{F}\,|\,\mathbf{U}} =\left[\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{\mathrm{t}}, \mathbf{X}_{\mathrm{t}})\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X}_{ \mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,\right]\left[\,\mathbf{K}_{t}^{\mathcal{ D}}(\mathbf{X}_{\mathrm{t}},\mathbf{X}_{\mathrm{t}})\otimes\mathbf{K}_{s}^{ \mathcal{D}}(\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,\right]^{-1} \mathbf{U}\] (39) \[=\left[\,\mathbf{I}\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X} _{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,(\mathbf{K}_{s}^{\mathcal{D}}( \mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}}))^{-1}\,\right]\mathbf{U},\]

and

\[\Sigma_{\mathbf{F}\,|\,\mathbf{U}}= \left[\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{\mathrm{t}}, \mathbf{X}_{\mathrm{t}})\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X}_{ \mathrm{s}},\mathbf{X}_{\mathrm{s}})\,\right]\] (40) \[-\left[\,\mathbf{K}_{\mathbf{X},\mathbf{Z}}^{\otimes}\,\right] \left[\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{\mathrm{t}},\mathbf{X}_{ \mathrm{t}})\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{Z}_{\mathrm{s}}, \mathbf{Z}_{\mathrm{s}})\,\right]^{-1}\left[\,\mathbf{K}_{\mathbf{X},\mathbf{Z }}^{\otimes}\,\right]^{\top}\]

where \(\mathbf{K}_{\mathbf{X},\mathbf{Z}}^{\otimes}=\left[\,\mathbf{K}_{t}^{\mathcal{ D}}(\mathbf{X}_{\mathrm{t}},\mathbf{X}_{\mathrm{t}})\otimes\mathbf{K}_{s}^{ \mathcal{D}}(\mathbf{X}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,\right]\) which simplifies to

\[\Sigma_{\mathbf{F}\,|\,\mathbf{U}}=\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{ \mathrm{t}},\mathbf{X}_{\mathrm{t}})\otimes\left[\,\mathbf{K}_{s}^{\mathcal{ D}}(\mathbf{X}_{\mathrm{s}},\mathbf{X}_{\mathrm{s}})-\mathbf{K}_{s}^{\mathcal{D}}( \mathbf{X}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,\mathbf{K}_{s}^{\mathcal{D}}( \mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})^{-1}\,\mathbf{K}_{s}^{ \mathcal{D}}(\mathbf{Z}_{\mathrm{s}},\mathbf{X}_{\mathrm{s}})\,\right]\] (41)

Due to the Kronecker structure, the marginal at time \(t\) only depends on the inducing points in that time slice so we can still get a CVI-style update that can be computed using a state-space model. To see why we again look at the Jacobian of the ell: \(\frac{\partial\text{\sc{ELL}}}{\partial[\bm{\mu}]_{\rho}}=\sum_{n}^{N}\widetilde {\bm{\mu}}_{n}\) where \(\widetilde{\bm{\mu}}_{n}\) now has \((D\times M)\times(D\times M)\) non-zero entries, which corresponding to needed all \(M\) spatial inducing points with there derivatives to predict at a single time point. This is similar to the time series setting, except we have now predicted in space to compute marginals of \(q(\mathbf{F})\). To be complete, we write that the marginal \(q(\mathbf{U})\) is

\[q(\mathbf{U})\propto\left[\,\prod_{t}^{N_{t}}\,\mathrm{N}\left(\,\widetilde{ \mathbf{Y}}_{t}\,\mid\,\mathbf{F}_{t},\,\widetilde{\mathbf{V}}_{t}\,\right)\, \right]p(\mathbf{F})\] (42)

where \(\widetilde{\mathbf{Y}}_{t}\) and \(\mathbf{F}_{t}\) are vectors of dimension \((D\times M)\), and \(\widetilde{\mathbf{V}}_{t}\) is a matrix of dimension \((D\times M)\times(D\times M)\). The marginals \(q(\mathbf{U}_{t})\), and the corresponding marginal likelihood \(p(\widetilde{\mathbf{Y}}\,|\,\widetilde{\mathbf{V}})\) can be computed by running a Kalman filter and smoother in \(\mathcal{O}(N_{t}\cdot(M_{s}\cdot d_{s}\cdot d_{s}\cdot d_{s})^{3})\). The marginal \(q(\mathbf{F})=\mathrm{N}\left(\,\mathbf{F}\,\mid\,\mu_{\mathbf{F}},\, \Sigma_{\mathbf{F}}\,\right)\) where

\[\mu_{\mathbf{F}}=\left[\,\mathbf{I}\otimes\mathbf{K}_{s}^{\mathcal{D}}( \mathbf{X}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,(\mathbf{K}_{s}^{\mathcal{D}} (\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}}))^{-1}\,\right]\mathbf{m}\] (43)

and

\[\Sigma_{\mathbf{F}}=\Sigma_{\mathbf{F}\,|\,\mathbf{U}}+ \left[\mathbf{I}\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X}_{ \mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,(\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{Z}_ {\mathrm{s}},\mathbf{Z}_{\mathrm{s}}))^{-1}\right]\mathbf{S}\,\left[\mathbf{I} \otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X}_{\mathrm{s}},\mathbf{Z}_{ \mathrm{s}})\,(\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_ {\mathrm{s}}))^{-1}\right]^{\top}\] (44)

#### a.5.2 Structured Approximate Posterior With Spatial Inducing Points

We now derive the algorithm for the case of the structured approximate posterior with spatial inducing points. The key is to define the free-form approximate posterior over the inducing points and their temporal derivatives and then use the model conditional to compute the spatial derivatives. The model is

\[p(\mathbf{Y}\,|\,\mathbf{F})\,p(\mathbf{F}\,|\,\mathcal{D}_{\mathrm{t}}\, \mathbf{u})\,p(\mathcal{D}_{\mathrm{t}}\,\mathbf{u}).\] (45)

Each term is

\[p(\mathcal{D}_{\mathrm{t}}\,\mathbf{u})=\mathrm{N}\left(\,\mathcal{D}_{\mathrm{t}} \,\mathbf{u}\mid\,\mathbf{0},\,\mathcal{D}_{\mathrm{t}}\,\mathbf{K}(\mathbf{Z}, \mathbf{Z})\,\mathcal{D}_{\mathrm{t}}^{*}\,\right),\] (46)

where

\[\mu_{\mathbf{F}\,|\,\mathbf{U}_{t}} =\left[\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{\mathrm{t}}, \mathbf{X}_{\mathrm{t}})\otimes\,\widetilde{\mathbf{K}}_{s}^{\mathcal{D}}( \mathbf{X}_{\mathrm{s}},\mathbf{X}_{\mathrm{s}})\,\right]\left[\,\mathbf{K}_{t}^{ \mathcal{D}}(\mathbf{X}_{\mathrm{t}},\mathbf{X}_{\mathrm{t}})\otimes\, \mathbf{K}_{s}(\mathbf{Z}_{\mathrm{s}},\mathbf{Z}_{\mathrm{s}})\,\right]^{-1} \mathcal{D}_{\mathrm{t}}\,\mathbf{u}\] (47)\[\Sigma_{\mathbf{F}\,|\,\mathbf{U}_{t}}= \big{[}\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{t},\mathbf{X}_{t })\otimes\mathbf{K}_{s}^{\mathcal{D}}(\mathbf{X}_{s},\mathbf{X}_{s})\,\big{]}\] (48) \[-\big{[}\,\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{Z}_{s}}^{ \otimes}\,\big{]}\,\big{[}\,\mathbf{K}_{t}^{\mathcal{D}}(\mathbf{X}_{t}, \mathbf{X}_{t})\otimes\mathbf{K}_{s}(\mathbf{Z}_{s},\mathbf{Z}_{s})\,\big{]}^ {-1}\,\big{[}\,\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{Z}_{s}}^{\otimes}\, \big{]}^{\top}\]

where \(\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{Z}_{s}}^{\otimes}=\mathbf{K}_{t}^{ \mathcal{D}}(\mathbf{X}_{t},\mathbf{X}_{t})\otimes\,\widetilde{\mathbf{K}}_{ s}^{\mathcal{D}}(\mathbf{X}_{s},\mathbf{Z}_{ss})\) and

\[\widetilde{\mathbf{K}}_{s}^{\mathcal{D}}(\mathbf{X}_{s},\mathbf{X}_{s})= \begin{bmatrix}\mathbf{K}_{s}(\mathbf{X}_{s},\mathbf{Z}_{s})\\ \mathcal{D}_{\mathbf{s}}\,\mathbf{K}_{s}(\mathbf{X}_{s},\mathbf{Z}_{s})\end{bmatrix}.\] (49)

The approximate posterior is defined as

\[q(\mathbf{F},\mathcal{D}_{t}\,\mathbf{u})=p(\mathbf{F}\,|\,\mathcal{D}_{t}\, \mathbf{u})\,q(\mathcal{D}_{t}\,\mathbf{u})\] (50)

where \(q(\mathcal{D}_{t}\,\mathbf{u}\) is a free-form Gaussian of dimension \((Nd\times N_{t}\times M_{s})\). The rest of the derivation simply follows App. A.5.1 by simple substituting \(\widetilde{\mathbf{K}}_{s}^{\mathcal{D}}(\mathbf{X}_{s},\mathbf{Z}_{ss})\) into the corresponding conditionals. The final result is that the approximate posterior decomposes as

\[q(\mathbf{U})\propto\bigg{[}\prod_{t}^{N_{t}}\,\mathrm{N}\,\Big{(}\, \widetilde{\mathbf{Y}}_{t}\,\,|\,\,\mathbf{F}_{t},\,\widetilde{\mathbf{V}}_{t }\,\,\Big{)}\,\,\bigg{]}\,p(\mathbf{F})\] (51)

where \(\widetilde{\mathbf{Y}}_{t}\) and \(\mathbf{F}_{t}\) are vectors of dimension \((d_{t}\times M_{s})\), and \(\widetilde{\mathbf{V}}_{t}\) is a matrix of dimension \((d_{t}\times M_{s})\times(d_{t}\times M_{s})\). The marginals \(q(\mathbf{U}_{t})\), and the corresponding marginal likelihood \(p(\widetilde{\mathbf{Y}}\,|\,\widetilde{\mathbf{V}})\) can be computed by running a Kalman filter and smoother in \(\mathcal{O}(N_{t}\cdot(M_{s}\cdot d)^{3})\), which compared to App. A.5.1 is _not_ cubic in the number of spatial derivatives.

#### a.5.3 Gauss-Newton Natural Gradient Approximation

We now provide the full derivation of the Gauss-Newton approximation of the natural gradient used to ensure _p.s.d_ updates. We will make use of the following identities, known as the Bonnet and Price theorems (see, [38]),

\[\frac{\partial}{\partial\mu}\,\mathbb{E}_{\,q(\mathbf{f}\,|\,\mu, \Sigma)}\,[\,\ell(\mathbf{f})\,] =\mathbb{E}_{\,q(\mathbf{f}\,|\,\mu,\Sigma)}\,\bigg{[}\,\frac{ \partial}{\partial\mathbf{f}}\,\ell(\mathbf{f})\,\bigg{]}\] (52) \[\frac{\partial}{\partial\Sigma}\,\mathbb{E}_{\,q(\mathbf{f}\,|\, \mu,\Sigma)}\,[\,\ell(\mathbf{f})\,] =\frac{1}{2}\,\mathbb{E}_{\,q(\mathbf{f}\,|\,\mu,\Sigma)}\, \bigg{[}\,\frac{\partial^{2}}{\partial\mathbf{f}\,\partial\mathbf{f}^{\top}} \,\ell(\mathbf{f})\,\bigg{]}\] (53)

which describes how to bring derivatives inside expectations. To ease notations, we work with a more general description of the model presented in the main paper, where we have multiple independent latent functions and use \(T_{p}\) to denote likelihood-specific functions which, for example, can be used to represent DE or as the identity of standard Gaussian likelihoods. The model is

\[p(\mathbf{u}_{q}) =\mathrm{N}\,(\,\mathbf{u}_{q}\,\,|\,\,0,\,\mathbf{K}_{q}\,)\] (54) \[p(\mathbf{f}_{q}\,|\,\mathbf{u}_{q}) =\mathrm{N}\,(\,\mathbf{f}_{q}\,\,|\,\,\mathbf{f}_{q_{1}\,|\, \mathbf{u}_{q}},\,\Sigma_{\mathbf{f}_{q}\,|\,\mathbf{u}_{q}}\,)\] \[\mathbf{Y}_{n,q} =p(\mathbf{Y}_{n,q}\,|\,T_{p}(\mathbf{f}_{n,1},\dots,\mathbf{f}_{ n,Q}))\]

where the shapes are \(\mathbf{u}_{q}\in\mathbb{R}^{M}\), \(\mathbf{f}_{q}\in\mathbb{R}^{N}\), \(T_{p}:\mathbb{R}^{Q}\to\mathbb{R}^{P}\), \(\mathbf{Y}\in\mathbb{R}^{N\times P}\), and \(\mathbf{Y}_{n,p}\in\mathbb{R}\). The variational approximation is

\[q(\mathbf{U})=\mathrm{N}\,(\,\mathbf{U}\,\,|\,\,\mathbf{m},\,\,\mathbf{S}\,)\] (55)

where \(\mathbf{U}=[\mathbf{u}_{1},\cdots,\mathbf{u}_{Q}]\), \(\mathbf{m}\in\mathbb{R}^{QM\times 1}\) and \(\mathbf{S}\in^{QM\times QM}\). Let \(\mathbf{F}=[\mathbf{f}_{1},\dots,\mathbf{f}_{Q}]\). The expected log-likelihood of the variational approximation is

\[\text{\sc ell} =\mathbb{E}_{\,q(\mathbf{U})}\,\bigg{[}\,\mathbb{E}_{\,p(\mathbf{ F}\,|\,\mathbf{U})}\,\bigg{[}\,\sum_{n,p}\log p(\mathbf{Y}_{n,p}\,|\,T_{p}( \mathbf{F}_{n,p}))\,\bigg{]}\,\bigg{]}\] (56) \[=\sum_{n,p}\mathbb{E}_{\,q(\mathbf{U}_{k})}\,\big{[}\,\mathbb{E}_{ \,p(\mathbf{F}_{n}\,|\,\mathbf{U}_{k})}\,[\,\log p(\mathbf{Y}_{n,p}\,|\,T_{p}( \mathbf{F}_{n,p}))\,]\,\big{]}\]

[MISSING_PAGE_EMPTY:21]

The covariance matrix can be simplified by invoking Woodbury's identity twice

\[\begin{split}\Sigma_{\mathbf{F}\,|\,\mathbf{Y}}&=\left[ \widehat{\mathbf{W}}\,\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}\,\widehat{ \mathbf{W}}^{\top}\right]-\left[\widehat{\mathbf{W}}\,\widetilde{\mathbf{K}}_{ \mathbf{X},\mathbf{X}}\,\widehat{\mathbf{W}}^{\top}\right]\,\left[\Phi+ \left[\widehat{\mathbf{W}}\,\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}\, \widehat{\mathbf{W}}^{\top}\right]\right]^{-1}\left[\widehat{\mathbf{W}}\, \widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}\,\widetilde{\mathbf{W}}^{\top} \right]\\ &=\widehat{\mathbf{W}}\,\left[\widetilde{\mathbf{K}}_{\mathbf{X}, \mathbf{X}}-\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}\,\widehat{\mathbf{ W}}^{\top}\left[\Phi+\widehat{\mathbf{W}}\,\widetilde{\mathbf{K}}_{ \mathbf{X},\mathbf{X}}\,\widehat{\mathbf{W}}^{\top}\right]^{-1}\,\widehat{ \mathbf{W}}\,\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}\right]\,\widehat{ \mathbf{W}}^{\top}\\ &=\widehat{\mathbf{W}}\,\left[\widetilde{\mathbf{W}}\,\Phi^{-1} \,\widehat{\mathbf{W}}^{\top}+\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}^ {-1}\right]^{-1}\,\widetilde{\mathbf{W}}^{\top}\end{split}\] (65)

and the mean can be expressed as

\[\begin{split}\mu_{\mathbf{F}\,|\,\mathbf{Y}}&=\Sigma _{\mathbf{F}\,|\,\mathbf{Y}}\,\Phi^{-1}\mathbf{Y}\\ &=\widehat{\mathbf{W}}\,\left[\widehat{\mathbf{W}}\,\Phi^{-1}\, \widehat{\mathbf{W}}^{\top}+\widetilde{\mathbf{K}}_{\mathbf{X},\mathbf{X}}^ {-1}\right]^{-1}\,\widehat{\mathbf{W}}^{\top}\,\Phi^{-1}\mathbf{Y}.\end{split}\] (66)

Now we can immediately read off the posterior \(p(\widetilde{\mathbf{f}}\,|\,\mathbf{Y})\) as \(p(\mathbf{F}(\mathbf{X})\,|\,\mathbf{Y})=p(\widehat{\mathbf{W}}\,\widetilde{ \mathbf{f}}\,|\,\mathbf{Y})\) is simply a transformed version

(67)

whose natural parameters are

\[\boldsymbol{\lambda}_{\widetilde{\mathbf{f}}\,|\,\mathbf{Y}}=\left[\widehat{ \mathbf{W}}^{\top}\,\Phi^{-1}\mathbf{Y},-\frac{1}{2}\,\widetilde{\mathbf{W}}\, \Phi^{-1}\,\widetilde{\mathbf{W}}^{\top}-\frac{1}{2}\,\widetilde{\mathbf{K}}_ {\mathbf{X},\mathbf{X}}^{-1}\right]^{\top}.\] (68)

We now derive the closed form expression of the natural gradient update with a learning rate of \(1\), and show that it recovers \(\boldsymbol{\lambda}_{\widetilde{\mathbf{f}}\,|\,\mathbf{Y}}\). The expected log likelihood (ELL) is

\[\begin{split}\textsc{ELL}&=\mathbb{E}_{q(\widetilde{ \mathbf{f}})}\left[\,\log\mathrm{N}\left(\,\mathbf{Y}\,\,|\,\,\widetilde{ \mathbf{W}}\,\widetilde{\mathbf{f}},\,\Phi\,\right)\,\right]\\ &=\log\mathrm{N}\left(\,\mathbf{Y}\,\,|\,\,\widetilde{\mathbf{W}} \,\widetilde{\mathbf{f}},\,\Phi\,\right)-\frac{1}{2}\,\text{Tr}\left[\,\Phi^{ -1}\,\widetilde{\mathbf{W}}\,\mathbf{S}\,\widetilde{\mathbf{W}}^{\top}\, \right].\end{split}\] (69)

The required derivatives are

\[\begin{split}\frac{\partial\textsc{ELL}}{\partial\mathbf{m}}& =-\frac{1}{2}\,\frac{\partial}{\partial\mathbf{m}}\left[(\mathbf{Y}- \widetilde{\mathbf{W}}\,\mathbf{m})^{\top}\,\Phi^{-1}\,(\mathbf{Y}- \widetilde{\mathbf{W}}\,\mathbf{m})\right]\\ &=\widetilde{\mathbf{W}}^{\top}\,\Phi^{-1}\,(\mathbf{Y}- \widetilde{\mathbf{W}}\,\mathbf{m})\end{split}\] (70)

where the last follows because \(\Phi\) is symmetric and

\[\begin{split}\frac{\partial\textsc{ELL}}{\partial\mathbf{S}}& =-\frac{1}{2}\,\frac{\partial}{\partial\mathbf{S}}\left[\text{Tr} \left[\,\Phi^{-1}\,\widetilde{\mathbf{W}}\,\mathbf{S}\,\widetilde{\mathbf{W}}^{ \top}\,\right]\right]\\ &=-\frac{1}{2}\,\widetilde{\mathbf{W}}\,\Phi^{-1}\,\widetilde{ \mathbf{W}}^{\top}.\end{split}\] (71)

The natural gradient is now given as

\[\begin{split}\frac{\partial\textsc{ELL}}{\partial\boldsymbol{ \mu}_{\widetilde{\mathbf{f}}\,|\,\mathbf{Y}}^{\star}}&=\left[ \begin{matrix}\frac{\partial\textsc{ELL}}{\partial\mathbf{m}}-\frac{2}{2} \frac{\partial\textsc{ELL}^{\top}}{\partial\mathbf{S}}^{\top}\,\mathbf{m} \\ \frac{\partial\textsc{ELL}}{\partial\textsc{ELL}}& \quad-\frac{1}{2}\,\widetilde{\mathbf{W}}\,\Phi^{-1}\,\widetilde{\mathbf{W}}^{ \top}\end{matrix}\,\mathbf{m}\right]\\ &=\left[\begin{matrix}\widetilde{\mathbf{W}}^{\top}\,\Phi^{-1}\, \mathbf{Y}\\ -\frac{1}{2}\,\widetilde{\mathbf{W}}\,\Phi^{-1}\,\widetilde{\mathbf{W}}^{\top} \end{matrix}\,\right].\end{split}\] (72)

The natural gradient update with a learning rate of \(1\) is

\[\boldsymbol{\lambda}_{q(\widetilde{\mathbf{f}})}=\frac{\partial\textsc{ELL}}{ \partial\boldsymbol{\mu}_{\widetilde{\mathbf{f}}\,|\,\mathbf{Y}}^{\star}}+ \boldsymbol{\lambda}_{p(\widetilde{\mathbf{f}})}\] (73)

where \(\boldsymbol{\lambda}_{p(\widetilde{\mathbf{f}})}=\left[\mathbf{0},-\frac{1}{2} \mathbf{K}^{-1}\right]^{\top}\) are the natural parameters of the prior \(p(\widetilde{\mathbf{f}})\), hence after the update the natural parameters are

\[\boldsymbol{\lambda}_{q(\widetilde{\mathbf{f}})}=\left[\begin{matrix} \widetilde{\mathbf{W}}^{\top}\,\Phi^{-1}\,\mathbf{Y}\\ -\frac{1}{2}\,\widetilde{\mathbf{W}}\,\Phi^{-1}\,\widetilde{\mathbf{W}}^{\top} -\frac{1}{2}\mathbf{K}^{-1}\end{matrix}\right].\] (74)

which recover those of \(p(\widetilde{\mathbf{f}}\,|\,\mathbf{Y})\), and hence we recover the optimal posterior.

Further Experimental Details and Results

eks methods were run on CPUs. State-space methods running on GPU used the parallel form of the Kalman smoother (see [55, 20]).

### An extension of autoip

If one drops the requirement for state-space representations then the approximations proposed in Sec. 4 directly define approximations to the variational gp defined by Eqn. (13), and hence directly extend autoip. For example on the non-linear damped pendulum in Sec. 7 we run this extension of autoip with whitening and inducing points for \(C=1000\) and achieve an RMSE of \(0.06\pm 0.001\) and running time of \(158.16\pm 0.34\), clearly improving the running time against autoip. However the benefit of our methods is that physs-gp remains linear in temporal dimensions which is vital for applications that are highly structured in time [20].

### Modelling Unknown Physics

Modelling of missing physics can be handled by parameterising unknown terms with gps. For example take a simple non-linear pendulum

\[\frac{d^{2}\theta}{dt^{2}}+\sin(\theta)=0.\] (75)

Now consider that the the \(\sin(\theta)\) is unknown and we would like to learn it. If we define the our differential equation in Eqn. (4) as

\[g=\frac{d^{2}\,f_{1}}{dt^{2}}+f_{2}(t)=0\] (76)

where both \(f_{1}(\cdot),f_{2}(\cdot)\) are latent gps that we wish to learn. We now construct 300 observations for training from the solution of Eqn. (75) across the range \([0,30]\) and \(1000\) for testing. We run physs-gp and compare the similarity of the learnt latent gp\(f_{2}(\cdot)\) to the true function at the test locations and achieve an rmse of \(0.068\) indicating we have recovered the latent force/unknown physics well.

### Monotonic Timeseries

This first example showcases the effectiveness of physs-gp in learning monotonic functions. Monotonicity information is expressed by regularising the first derivative to be positive at a set of collocation points [51]:

\[p(\mathbf{Y}\,|\,\mathbf{f})=\mathrm{N}\left(\,\mathbf{Y}\,\,|\,\mathbf{f},\, \sigma_{y}^{2}\,\right),\,\,p(\mathbf{0}\,|\,\frac{\partial\mathbf{f}}{ \partial t})\,\,\,\,=\Phi(\frac{\partial\mathbf{f}}{\partial t}\cdot\frac{1}{ v})\]

where \(\Phi(\cdot)\) is a Gaussian cumulative distribution function, and \(v=1e-1\) is a tuning parameter that controls the steepness of the step function. We plot predictive distributions of (batch) gp and physs-gp in Fig. 5. The gp fits data and does not learn a monotonic function. However, using \(300\) collocation points, physs-gp is able to include the additional information and learn a monotonic function whilst running \(1.5\) times faster.

### Non-linear Damped Pendulum

All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. All were optimised for \(1000\) epochs using Adam [32] with a learning rate of \(0.01\). Both the gp and autoip had an RBF kernel (following Long et al. [40]) and physs-gp used a Matern-\(\gamma_{2}\); all with a lengthscale of \(1.0\). The observation noise was initialised to \(0.01\) and the collocation \(0.001\). Both were fixed for the first \(40\%\) of training and then released. Predictive distribution of physs-gp and autoip are plotted in Fig. 6.

### Curl-free Magnetic Field Strength

All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. All models are run for \(5000\) epochs using Adam with a learning rate of \(0.01\), and use a Matern-\(\gamma_{2}\) kernel on time,with ARD RBF kernels on the spatial dimensions, with a lengthscale of \(0.1\) across all. The Gaussian likelihood is initialised with a variance of \(0.01\) and held for \(40\%\) of training. All our methods used a natural gradient learning rate of \(1.0\) as this is the conjugate setting.

### Diffusion-Reaction System

We use data provided by [49] under an MIT license. All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. Our method physs-svgp and physs-svgp\({}_{\text{H}}\) use a Matern 72 kernel on time and an RBF of space, both initialised with a lengthscale of \(0.1\). We place the collocation points on a regular grid of size \(20\times 10\) and use \(M_{s}=20\) spatial inducing points. We pretrain for \(100\) iterations using a natural gradient learning rate of \(0.01\) and after use a learning rate \(0.1\) for the remaining \(19000\) iterations. autoip uses a RBF kernel on both time and space with a lengthscale of \(0.1\). We place the collocation points on a regular grid of size \(10\times 10\). All models use Adam with a learning rate \(0.001\) and train for a total of \(20000\) iterations.

Figure 5: Predictive distributions of gp and physs-gp on the monotonic function in App. B. The gp cannot incorporate monotonicity information and fits the data.

Figure 6: Predictive distributions on the Damped Pendulum.

### Ocean Currents

Our method phys-svgp\({}_{\text{\tiny H}}\) was run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. We ran for \(10000\) iterations, using Adam with a learning rate of \(0.01\). For natural gradients with used a learning rate of \(0.1\). We used a Matern-\(\mathcal{V}_{2}\) kernel on time and RBF kernels on both spatial dimensions with lengthscales \([24.0,1.0,1.0]\). We used \(100\) spatial inducing points and a spatial mini-batch size of \(10\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes],[No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have both theoretically (Theorem 3.1), and empirically demonstrated our claims (Sec. 7). We provide computational complexities for all proposed models showing a linear in the temporal dimension performance. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: A discussion of two main limitations (collocation method, and spatial scaling) and future work is provided in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions of Theorem 3.1 are provided in Sec. 3 with full proof (correct to the best of our knowledge) provided in the appendix App. A.5.4. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experimental details, sufficient to reproduce the results, are provided both in the main paper Sec. 7 and the appendix App. B. Code reproducing all methods and results is provided in the supplementary material.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Ycs] Justification: Code for data downloading and processing for train test splits, as well as code reproducing all methods and results is provided in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All required details are provided in App. B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We followed established published results for experimental setups, which did not include error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Specific details on the CPUs and GPUs used for experiments are provided in App. B. Guidelines:* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and we conform with this in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the benefits of physics informed machine learning in our introduction. We do not envisage any negative societal implications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All existing methods and datasets used are open and cited throughout. Any licenses are explicitly stated in the appendix. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We release our code under CC-BY 4.0. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.