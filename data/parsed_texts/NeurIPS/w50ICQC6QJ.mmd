# Discovery of the Hidden World with

Large Language Models

 Chenxi Liu\({}^{1}\), Yongqiang Chen\({}^{2,3,4}\), Tongliang Liu\({}^{5,2}\)

\({}^{1}\)TMLR Group, Hong Kong Baptist University

\({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence

{cscxliu,bhanml}@comp.hkbu.edu.hk {yqchen,jcheng}@cse.cuhk.eduhkMingming Gong\({}^{6,2}\), James Cheng\({}^{4}\), Bo Han\({}^{1}\), Kun Zhang\({}^{2,3}\)

\({}^{3}\)Carnegie Mellon University \({}^{4}\)The Chinese University of Hong Kong

\({}^{5}\)Sydney AI Centre, The University of Sydney \({}^{6}\)The University of Melbourne

tongliang.liu@sydney.edu.aumingming.gong@unimelb.edu.au kunz1@cmu.edu

###### Abstract

Revealing the underlying causal mechanisms in the real world is the key to the development of science. Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. The _lack of well-defined high-level variables in many real-world applications_ has already been a longstanding roadblock to a broader application of CDs. To this end, this paper presents **C**ausal representati**O**n **A**ssistan**T** (COAT) that introduces large language models (LLMs) to bridge the gap. LLMs are trained on massive observations of the world and have demonstrated great capability in extracting key information from unstructured data. Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements. Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors. We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal. We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT. Extensive empirical results confirm the effectiveness and reliability of COAT with significant improvements.

## 1 Introduction

Science originates along with identifying important variables and revealing their causal relations [1, 2]. Despite the progress in the past decades, existing causal discovery approaches (CDs) mainly rely on high-quality measured variables, which are usually given by human experts [3, 4, 5]. However, the desired variables and their measurements are usually unavailable in a wide range of real-world applications. For example, Amazon sellers who want to analyze the factors related to user ratings only have raw user reviews, which are written according to the underlying user preferences for certain product characteristics. Therefore, _the lack of high-quality high-level variables_ has been a longstanding impediment to broader real-world applications of CDs or causality-inspired methods [6].

The recently emerged Large Language Models (LLMs) [7; 8; 9; 10] offer a new opportunity to mitigate the gap [11]. Trained from massive observations of the world, LLMs demonstrate impressive capabilities in _comprehending unstructured inputs_, and leveraging the learned rich knowledge to resolve a variety of general tasks [12]. A surge of early tests demonstrates promising results that LLMs can effectively leverage the learned knowledge to answer commonsense causal questions [11; 13; 14]. Nevertheless, existing approaches mainly focus on incorporating LLMs as a _straightforward reasoner_ with respect to the given _causal variables_. The reliability of LLMs in directly reasoning the true causal structure behind any specific data-generating process remains a debate [13; 15; 16; 17] due to a series of drawbacks of LLMs [18; 19; 20]. In addition, all of the existing combinations of LLMs and causal discovery have surprisingly overlooked the identifiability of causal structure, which plays an important role in classic causal discovery literature [3; 4; 5]. Hence, a challenging question comes:

_How can LLMs reliably assist in revealing the causal mechanisms behind the real world?_

In this work, we answer the question with a focus on local causal discovery with respect to a target variable that poses high value such as customer ratings and medical diagnosis, and introduce **C**ausal represent**i**O**n**Assistan**T** (COAT). Specifically, given the target variable \(Y\), COAT aims to identify a Markov blanket to \(Y\) from raw observation and also produce the theoretical-guaranteed causal structure. To achieve the goal, as illustrated in Fig. 1, COAT employs two mutually beneficial components: LLMs and CDs. Iteratively, at step (a), COAT leverages LLMs to look into a set of unstructured observations (e.g., customer comments) and propose potentially useful high-level factors. The proposed factors contain both the definition and the annotation criteria. Therefore, at step (b), another LLM is employed to give concrete values following the criteria. Then in step (c), CDs is used to reveal the structure among the identified factors. To ensure the reliability of the factor identification, COAT constructs feedback from the intermediate causal discovery results from step (c) to further inspire LLMs to improve further factors. The feedback includes sampling important observations that can not be well explained by the existing identified results. We show that the feedback provably helps with identifying the desired Markov Blanket and the structure (Proposition 2.2). We present a comprehensive analysis of COAT on both synthetic simulations and

Figure 1: Illustration of COAT framework to analyze the rating scores of AppleGastronome. COAT aims to uncover the underlying Markov Blanket with respect to the given ratings of the apples (i.e., factors that fit the preferences of gastronomes). COAT first (a) adopts an LLM to read, comprehend, and relate the rich knowledge about tasting the apples. The LLM needs to propose a series of candidate factors such as apple sizes and smells, along with some meta-information such as annotation guidelines. Based on the candidate factors, COAT then (b) prompts another LLM to annotate the unstructured review into structured data. (c) The CD algorithm then finds causal relations among the factors, and constructs feedback based on samples where the ratings can not be well explained by the existing factors. By looking into the new samples, the LLM is expected to associate more related knowledge to uncover more desired causal factors.

real-world case studies, ranging from analysis of human reviews to diagnosis of neuropathic and brain tumors (Sec. 4). Our contributions are summarized as follows:

* To the best of our knowledge, we are the first to leverage LLMs to propose high-level variables, thereby extending the scope of CDs to unstructured data (Sec. 2.1).
* We establish the first benchmarks with real-world data including AppleGastronome and Neuropathic to examine the unstructured causal discovery (Sec. 4).
* We propose the first framework COAT that combines the best of LLMs and CDs to find theoretically grounded causal results (Sec. 2.2), which are verified with extensive empirical studies (Sec. 4).
* Additionally, the analysis of COAT also derives the first metrics that measure the causal representation learning capabilities of various LLMs (Definition 2.3).

## 2 Representation Assistant for Causal Discovery

In this section, we present the formulation of leveraging a language model to serve as a representation assistant for causal discovery on unstructured data. The representation assistant needs to extract useful factors that capture sufficient information for an interested target variable.

### Problem Definition

DataTo begin with, we are given a _target variable_ of interest \(Y\), e.g., stars rated by a customer or the tumor type of a patient. We treat \(Y\) as a scalar random variable without loss of generality. The _unstructured data_, or raw observations, e.g., customer review of a certain product or images of tumors, are denoted as \(\bm{X}\). The dataset \(\mathcal{D}\) consists of \(n\) paired samples \(\left\{\left(\bm{x}_{i},y_{i}\right)\right\}_{i=1}^{n}\) that are independently drawn from the distribution over \(\left(\bm{X},Y\right)\). Note that the target variable \(Y\) serves as a guider, and no prior assumption on the relations between \(\bm{X}\) and \(Y\) is assumed.

ObjectiveWe seek a mapping \(h:\bm{X}\mapsto\bm{Z}\) which elicits the _structured_ representation \(\bm{Z}=h(\bm{X})\) such that \(Y\perp\bm{X}\mid\bm{Z}\). In other words, \(\bm{Z}\) serves as a Markov Blanket of \(Y\) for the unstructured raw observations. Built upon the structure representation, then, downstream causal discovery methods can be applied on \(\bm{Z}\cup\left\{Y\right\}\). The revealed causal structure can provide insights about the target variable \(Y\)[21; 22], such as what factors of the product would be most satisfactory to customers. Furthermore, the framework can be easily extended to discover a complete causal graph by shifting the target variable to the other identified factors or the other additionally available variables. Formal assumptions are discussed in Sec. 2.4.

LLM as a representation assistantWe aim to make the most use of the rich knowledge of LLMs to assist in extracting the relevant information from the raw observations \(\bm{X}\). To this end, the mapping \(h\) is decomposed as a collection of factors \(\mathcal{W}=\left\{\bm{w}_{1},\bm{w}_{2},...,\bm{w}_{k}\right\}\), each of which is a function \(\bm{w}_{i}:\bm{X}\mapsto\mathcal{C}\) that maps the raw observation \(\bm{x}\) to a predefined value space \(\mathcal{C}\). In other words, the structured representation is composed of multiple factors: \(h(\bm{X})=\left(\bm{w}_{1}\left(\bm{X}\right),\bm{w}_{2}\left(\bm{X}\right), \cdots,\bm{w}_{k}\left(\bm{X}\right)\right)\). Throughout this work, for the notation of factors, we use the symbol \(\bm{w}_{i}\) to denote the factor itself like _sweetness_, _size_, or _scent_, and \(\bm{w}_{i}(\cdot)\) to denote the function that maps from raw observation space \(\mathcal{X}\) to the predefined value space \(\mathcal{C}\).

Descriptions of the factorsWithout loss of generality, in this work, we consider the descriptions of the factors in natural language, which can be divided into two categories: i) Implicit factors, which need to be discovered and elaborated by LLMs. To obtain the values of the implicit factors, one could feed the factor descriptions and the unstructured input \(\bm{x}_{i}\) to a suitable LLM for value annotation; For example, given a customer review on an apple \(\bm{x}_{i}\), a discovered implicit factor \(\mathcal{C}=\left\{-1,0,1\right\}\): \(\bm{w}_{1}(\bm{x}_{i})=1\) could mean that the customer appreciates the sweetness of the apple; \(\bm{w}_{1}(\bm{x})=-1\) means that the customer is disappointed about the sweetness; \(\bm{w}_{1}(\bm{x})=0\) means the sweetness has not been mentioned. ii) Explicit factors such as heart rate, whose descriptions are already available. The measure of the explicit factors usually requires some external tools.

### The COAT Framework

We approach the aforementioned problem via a new framework called **C**ausal representati**O**n **A**ssistan**T** (COAT) (Algorithm 1). COAT aims to extract useful factors through multiple rounds of iteration. We use the superscript \(t\) to refer to the input and output of LLM at the \(t\)-th round. We also denote the union of results from the first \(t\) rounds using the superscript "\(\leq t\)".

Factor proposalTo induce useful high-level factors from the rich world knowledge of LLMs, COAT employs a prompt \(\bm{p}\) and a few samples \(\widehat{\mathcal{D}}\subseteq\mathcal{D}\).

Specifically, in Fig. 2, \(\bm{p}\) contains three components: _samples_, _instructions_, and _format control_. To encourage LLM to focus on the information related to the target variable \(Y\), samples are grouped by the value of \(Y\). The instruction requires \(\Psi\) to give each proposed factor \(\bm{w}_{i}\) a concrete description of the mapping \(\bm{w}_{i}(\cdot)\), like how to decide the factor values. In addition, the metadata about the task such as the task description and context can also be incorporated if available. The prompt \(\bm{p}\) essentially imitates human experts [23] in selecting and defining high-level variables. The set of resulting factors in the \(t\)-th round, defined with natural language by the LLM \(\Psi\), is denoted as \(\mathcal{W}^{t}=\Psi(\bm{p}^{t},\widehat{\mathcal{D}}^{t})\). We merge them with factors proposed in the previous rounds to update the set of all factors \(\mathcal{W}^{\leq t}=\mathcal{W}^{1}\cup\cdots\cup\mathcal{W}^{t}\).

Factor parsingOnce we obtain the candidate factors, we then collect the values of the factors from the unstructured observations. In prior works, they are usually collected from human experts according to the given factors [3]. To do so, another LLM \(\Psi_{s}\) is instructed to read the annotated guidelines of each variable \(\bm{w}_{i}\) and parse the unstructured observations into structured or tabular data:

\[\bm{z}_{i}:=\big{(}\bm{w}_{i}(\bm{x}_{1}),\cdots,\bm{w}_{i}(\bm{x}_{n})\big{)},\quad\bm{w}_{i}(\bm{x}):=\Psi_{s}(\bm{x},\bm{w}_{i},\bm{p}_{p}),\] (1)

where \(\bm{p}_{p}\) refers to the additional instruction to prompt \(\Psi_{s}\) to parse the observed data, and \(\bm{z}_{i}\) refers to the parsed values for the corresponding factor \(\widehat{\bm{w}}_{i}\). We define \(\bm{Z}^{\leq t}:=\text{Concat}\big{(}\{\bm{z}_{i}\text{ for }\bm{w}_{i}\in\mathcal{W}^{\leq t}\}\big{)}\).

When the data curation of the proposed factors requires additional domain-specific knowledge/skills (e.g., intervening on the external environments) that the LLMs do not acquire, we could fetch \(\bm{z}_{i}\) through some external process [24; 25]. For example, studying the causes of a disease requires annotating relevant symptoms from diagnosis records and conducting additional medical checks [26]. Our experiments show that COAT can effectively extract the hidden factors under both schemes.

Figure 2: Illustration of the prompt template for factor proposal in COAT.

Causal discoveryWith the given values \(\bm{Z}^{\leq t}\) associated with the candidate factors \(\mathcal{W}^{\leq t}\), a CD algorithm \(\mathcal{A}\) (e.g., FCI [27]) is used to reason about the causal structure based on the parsed data:

\[\mathcal{G}^{t}=\mathcal{A}(\bm{Z}^{\leq t}\cup\{Y\}),\] (2)

where \(\mathcal{G}^{t}\) is the discovered causal structure. In general, the inputs in each round to \(\mathcal{A}\) may contain noises as well as latent confounders, any CDs with suitable theoretical assumptions could be used for \(\mathcal{A}\). The noises injected through LLM-based parsing may be of independent interest to the literature of causal discovery [5; 28]. In this work, we demonstrate the idea of COAT via the FCI algorithm [4] as it is flexible with respect to different functional classes of the underlying generation process, allows for the existence of latent confounders [27], which aligns well with our objective.

Improving factor proposal with causal feedbackLLMs require proper prompts to fully unlock their capabilities [29; 30; 31]. When it comes to factor proposing, it is also hard for LLMs to propose all factors at once. Nevertheless, from the causal discovery results, we could find useful information and thus provide feedback to further improve the factor proposal:

\[(\widehat{\mathcal{D}}^{t+1},\bm{p}^{t+1})=\mathcal{F}(\mathcal{G}^{t}, \mathcal{D},\bm{p}^{t}),\] (3)

where \(\mathcal{F}\) samples specific examples from \(\mathcal{D}\) and constructs new prompts according to the results of \(\mathcal{A}\) for the next round of factor proposal. For example, FCI is able to imply the existence of latent confounders, from which we could refine \(\bm{p}\) to prompt \(\Psi\) to focus on the corresponding factors.

### Causal Feedback

Let \(X\) be the high-dimensional random variable for the raw data. After \(t\) rounds, COAT identifies \(h_{\leq t}(X)\) as the Markov Blanket for \(Y\) w.r.t. to \(\big{\{}\bm{w}_{i}(X)\mid\bm{w}_{i}\in\mathcal{W}^{\leq t}\big{\}}\). If \(Y\not\perp X\mid h_{\leq t}(X)\), which means it cannot serve as a Markov Blanket [32] for \(Y\) w.r.t. \(X\), then there exists a potential factor \(\widehat{\bm{w}}:X\mapsto\mathcal{C}\) such that:

\[H(Y|h_{\leq t}(X))>H(Y|h_{\leq t}(X),\widehat{\bm{w}}(X)),\] (4)

where \(H(\cdot)\) refers to the entropy. If the LLM \(\Psi\) can not find the desired \(\widehat{\bm{w}}\), it means that \(h_{\leq t}(X)\) is already sufficient to separate \(Y\) from \(X\). Therefore, for the next \((t+1)\)-th iteration with sample \(\widehat{\mathcal{D}}^{t+1}\), \(\Psi\) is expected to propose new factor \(\widehat{\bm{w}}\) that also satisfies the similar property:

\[H_{\widehat{\mathcal{D}}^{t+1}}(Y|h_{\leq t}(X))-H_{\widehat{\mathcal{D}}^{t +1}}(Y|h_{\leq t}(X),\widehat{\bm{w}}(X))>0,\] (5)

where \(H_{\widehat{\mathcal{D}}^{t}}(\cdot|\cdot)\) refers to the conditional entropy measured on \(\widehat{\mathcal{D}}^{t}\). As shown in Fig. 3, finding factors satisfying Eq. 4 progressively expands the discovered factors and pushes \(h_{\leq t}(X)\) to a valid Markov Blanket. Also, it implies conditioning on the identified factors would further strengthen the correlation between \(\widehat{w}\) and \(Y\). Therefore, to find the desired factor, we are motivated to select suitable \(\widehat{\mathcal{D}}^{t+1}\) for the next iteration such that \(\widehat{\mathcal{D}}^{t+1}=\arg\max_{\widehat{\mathcal{D}}\subset\mathcal{D} }H_{\widehat{\mathcal{D}}}(Y|h_{\leq t}(X)),\) where \(h_{\leq t}(X)\) can not well explain \(Y\). This problem can be converted into a classification problem in which \(\widehat{\mathcal{D}}^{*}\) are the samples that the fitted classifier yields a large prediction error. In our experiments, we implement the classification via clustering with respect to \(h_{\leq t}(X)\). The clustering elicits \(C\) groups \(\widehat{\mathcal{D}}_{c}:=\big{\{}\bm{x}_{i}\text{ for }i\in\mathcal{I}_{c} \big{\}}\): \(\mathcal{I}_{1},\cdots,\mathcal{I}_{C}=\text{K-Means}\big{(}h_{\leq t}(X)\big{)}\). We then take the group of samples with the largest conditional entropy to construct the feedback.

In practice, many factors, such as the LLM capabilities, data faithfulness, and prompt templates, could affect the satisfaction of Eq 5. Therefore, in the next section, we will establish a theoretical framework to discuss the influence of the factors above to the satisfaction of Eq 5.

### Theoretical Analysis

We then theoretically analyze some critical steps in COAT, including the feedback and identifiability.

Figure 3: Illustration of variables that could be discovered with COAT. Let \(W\in h_{\leq t}(X)\) be an identified variable, and assume there exists a latent variable \(\widehat{\bm{w}}\) to be discovered. When \(\widehat{\bm{w}}\) is the direct parent or child of \(Y\), finding hard-to-explain samples can help uncover it. When \(\widehat{\bm{w}}\) is the direct parent and also a child of \(W\), or the spouse of \(Y\) with \(W\) as the common child of \(Y\) and \(\widehat{\bm{w}}\), conditioning on \(W\) facilitates the discovery of \(\widehat{\bm{w}}\).

Feedback analysisGiven a new factor \(\bm{w}_{k+1}\), with the current representation as \(h_{[k]}(X)=\left(\bm{w}_{1}\left(X\right),\bm{w}_{2}\left(X\right),\cdots,\bm{w} _{k}\left(X\right)\right)\), and COAT tests:

\[Y\not\perp\bm{w}_{k+1}(X)\mid h_{[k]}(X).\] (6)

COAT also requires the following usual condition about distribution and causal graph:

**Assumption 2.1** (Faithful and Markov conditions, adjusted from [33]).: _For any disjoint non-empty subsets \(A,B,C\subset\mathcal{W}^{\leq t}\cup\{Y\}\), \(A\) and \(B\) are d-separated by \(C\) on the causal graph iff \(A\perp B\mid C\) on the factors' distribution. All conditional independencies are preserved after factor parsing._

The annotation from a poor model could introduce an additional "error term" on the true factor values, disturbing the true distribution, as one can observe in Fig. 4(a) and 4(b). If Assumption 2.1 holds, the conditional mutual information between \(Y\) and \(X\) given the desired factors decreases:

**Proposition 2.2**.: _Under assumption 2.1, if condition 6 holds, then for Markov Blanket \(\mathcal{S}\subseteq[k+1]\) of \(Y\), i.e., \(Y\perp h_{[k+1]\setminus\mathcal{S}}(X)\mid h_{\mathcal{S}}(X)\), we have the following about conditional mutual information:_

\[I\left(Y;X\mid h_{S}(X)\right)=I\left(Y;X\mid h_{[k+1]}(X)\right)<I\left(Y;X \mid h_{[k]}(X)\right)\] (7)

Factor identificationWe provide an initial exploration under what conditions LLMs can identify target-related factors and how the ability of an LLM influences this procedure.

**Definition 2.3** (Ability of LLMs).: _Given a suitable prompt about current factors and data, the LLM \(\Psi\) has non-zero probability \(p_{\Psi}>0\) to propose a new factor \(\bm{w}_{k+1}\) that satisfies condition 6 and_

\[\frac{I\left(Y;X\mid h_{[k+1]}(X)\right)}{I\left(Y;X\mid h_{[k]}(X)\right)}<1 -C_{\Psi},\] (8)

_for some positive constant \(C_{\Psi}\) whenever \(I\left(Y;X\mid h_{[k]}(X)\right)>0\). Note that \(h_{[0]}(X)=\phi\), hence we also use \(I\left(Y;X\mid h_{[0]}(X)\right)\) to refer \(I\left(Y;X\right)\). We use \(p\) instead of \(p_{\Psi}\) when the context is clear._

We further explain the intuition behind Def 2.3: the _Perception Score_\(p\) captures the LLM's responsiveness to the given prompts and the feedback; the _Capacity Score_\(C_{\Psi}\) captures the quality of the factors proposed by the LLM. Empirically, the two scores are used to estimate the abilities of the predominant LLMs (Sec. 3.2). Theoretically, we use them to characterize the influence of prompt templates, the LLM responsiveness, and the quality of factors on the performance of COAT:

**Proposition 2.4** (Characterization for Factor Identification Process).: _With assumption 2.1, for any small number \(\epsilon,\delta\in(0,\frac{1}{2})\), perception score \(p>0\), capacity score \(C_{\Psi}>0\), with \(t\) COAT rounds that_

\[\sqrt{t}>\frac{|z_{\delta}|\sqrt{1-p}}{2\sqrt{p}}\left(1+\sqrt{1+\frac{4\log \epsilon}{z_{\delta}^{2}(1-p)\log\left(1-C_{\Psi}\right)}}\right),\] (9)

_where \(z_{\delta}\) is the \(\delta\)-quantile of the standard normal distribution, we have_

\[\Pr\left(\frac{I\left(Y;X\mid h_{\leq t}(X)\right)}{I\left(Y;X\right)}< \epsilon\right)\geq 1-\delta.\] (10)

The proof is given in Appendix D.2. Prop. 2.4 gives a guarantee on identifying a Markov Blanket. Intuitively, Prop. 2.4 also characterizes the influence of prompt templates, the LLM responsiveness, and the quality of factors on the performance of COAT via the two proposed measures: \(p\) and \(C_{\Psi}\). When both of them are positive, COAT would converge exponentially:

**Proposition 2.5** (Rate of Convergence).: _With assumption 2.1, for any small number \(\epsilon,\delta\in(0,\frac{1}{2})\), perception score \(p>0\), capacity score \(C_{\Psi}>0\), after \(t\) COAT rounds, the following inequality holds with probability at least \(1-\delta\):_

\[\frac{I(Y;X\mid h_{\leq t}(X))}{I(Y;X)}\leq\left(\frac{1}{1-C_{\Psi}}\right)^{ -tp-z_{\delta}\sqrt{tp(1-p)}}\] (11)

Causal structure identificationIt is clear that LLMs are not involved in the causal discovery process, which is mainly executed by causal discovery methods such as FCI. Therefore, the CD guarantees the identifiability of the final causal graph over the LLM-proposed factors. The concrete assumptions required for identifiability depend on the specific CD used in COAT. For instance, the FCI algorithm requires faithfulness of the data distribution with respect to the true causal graph [27]. In our experiments, we also verify that the structured data annotated by LLMs has a high accuracy and little noise, which is friendly to the CD assumption. In general, one could switch to another CD in COAT, while using different CDs may require different assumptions. For example, the LiNGAM algorithm requires the relations among variables to be linear and non-Gaussian models. Empirically, we find that COAT with LiNGAM works very well (Appendix F).

### Practical Discussions

Prompt templateThe instruction following capability and the context window of LLMs may affect the satisfaction to the constraints of the prompt template. Including more data samples or background knowledge may improve the \(p\) and \(C_{\Psi}\), but it is more challenging for the LLM.

Modern causal discoveryWe use FCI in this paper in order to illustrate the idea. To attain identifiability better than the Markov equivalent class, one can choose more advanced Causal Discovery methods under different assumptions, see Appendix C if interested.

We also discuss some cases where we need to handle them properly in practice with LLMs.

Factor filteringLLMs may output several factors with similar semantics or exhibit multicollinearity in the annotated data, which will hinder the causal discovery process. To mitigate the issue, one could do factor filtering that adopts PCA or early conditional independence tests given the currently discovered variables in the Markov Blanket to detect and eliminate these variables.

Factor poolLLMs may discover useful factors in early rounds while being discarded. For example, the underlying spouse variables of the target label \(y\) may be independent with \(y\) without conditioning on their common children variables. To resolve the issue, we could introduce a factor pool that stores the candidate variables proposed in the past, and replay the variables that have not been passed by conditional independence tests with existing variables in the Markov Blanket for a double check.

## 3 Empirical Analysis of COAT

We evaluate whether COAT can propose and identify a set of high-level factors belonging to the Markov Blanket of the target variable \(Y\). We construct the first benchmark, called AppleGastronome, to verify the effectiveness of COAT in finding useful causal information, and compare COAT to previous methods [11] that merely leverage LLMs to perform causal discovery. Specifically, we use AppleGastronome to examine the capabilities of \(10\) predominant LLMs including GPT 4o [34], Claude-3-Opus [35], LLMA3-70b [36], and Mistral-Large [37] in realizing COAT. Due to the space limit, we report only the results of the popular LLMs and present full results in Appendix E.4.

### Experimental Setup

In AppleGastronome benchmark, we consider the target variable as a rating score of the apple by astronomers. We prepare different high-level factors: 3 parents of Y, one child of Y, and one spouse of Y. These factors form a Markov blanket of Y. In addition, we also prepared one disturbing factor

Figure 4: Quantitative evaluation of the causal capabilities of LLMs in COAT.

related to Y but not a part of this blanket. A good method is expected to propose the five high-level factors (up to semantical meanings) and exclude the disturbing factor.

Benchmark constructionEach apple has its own attributes, including size, smell, and taste (or sweetness). Each gastronomy pays unique attention to a subset of the above three attributes. They will write a review according to their preference and give the rating score. We generate the review using GPT 4 by feeding GPT 4 the preferences and the apple attributes. We generated 200 samples for LLMs' analysis and annotation. More details are left in Appendix E.1.

BaselinesFor factor proposal, we mainly employ two different uses of LLMs as the baselines: **META** is the zero-shot factor proposal given only the context to LLMs; and **DATA** additionally gives some samples of raw observations, which is an ablation of COAT without the _feedback_ module, i.e., only one COAT round. For causal relation inference, we follow Kiciman et al. [11] that prompt LLMs to reason for the causal direction of each pair of the discovered variables by **DATA**.

MetricsWe evaluate the ability on factor proposal based on three metrics: _MB_, _NMB_, and _OT_. _MB_ means the desired factor forming the Markov Blanket of Y. _NMB_ means the undesired factor relevant to data but not in _MB_. _OT_ means the unexpected factors irrelevant to data. We also present the corresponding recall, precision, and F1 with respect to \(\text{MB}(Y)\).

### Analysis with AppleGastronome Benchmark

Key findingsEmpirically, LLMs with CoT can be aware of high-level factors behind data (lower _OT_ than _META_) but still struggle to distinguish the desired factors in Markov Blanket (higher _NMB_ than COAT). COAT is more resistant to the "disturbing" factor, which is supported by the lower _NMB_ column. COAT filters out irrelevant factors from LLMs' prior knowledge that are not reflected by the data, which is supported by the lower _OT_ column. COAT robustly encourages LLM to find more expected factors through the feedback, which is supported by the higher _MB_ column.

Can LLMs be an effective factor proposer?As discussed in Sec. 2.4, there are two crucial abilities for LLMs in identifying potential high-level factors. The first one is to be aware of the existence of potential factors, and the second is to synthesize and describe these factors. Inspired

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline \multirow{2}{*}{LLM} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{Factor proposal} \\  & & MB & NMB & OT & Recall & Precision & F1 \\ \hline \multirow{3}{*}{GPT 4} & META & 2.67\(\pm\)0.04 & 0.67\(\pm\)0.27 & 2.33\(\pm\)0.0 & 0.53\(\pm\)0.0 & 0.46\(\pm\)0.04 & 0.49\(\pm\)0.27 \\  & DATA & 3.00\(\pm\)0.33 & 0.32\(\pm\)0.07 & 0.00\(\pm\)0.0 & 0.50\(\pm\)0.00 & 0.92\(\pm\)0.22 & 0.72\(\pm\)0.04 \\  & DATA+COT & 4.33\(\pm\)0.08 & 0.83\(\pm\)0.27 & 0.17\(\pm\)0.29 & 0.87\(\pm\)0.21 & 0.81\(\pm\)0.04 & 0.84\(\pm\)0.08 \\  & COAT & 4.00\(\pm\)0.02 & 0.33\(\pm\)0.07 & 0.00\(\pm\)0.00 & 0.80\(\pm\)0.16 & 0.93\(\pm\)0.00 & 0.85\(\pm\)0.11 \\ \hline \multirow{3}{*}{GPT 3.5} & META & 3.33\(\pm\)0.23 & 0.33\(\pm\)0.37 & 0.43\(\pm\)1.25 & 0.67\(\pm\)0.25 & 0.42\(\pm\)0.12 & 0.51\(\pm\)0.17 \\  & DATA & 2.67\(\pm\)0.47 & 0.67\(\pm\)0.07 & 0.00\(\pm\)0.00 & 0.53\(\pm\)0.00 & 0.81\(\pm\)0.14 & 0.64\(\pm\)0.09 \\  & DATA+COT & 5.00\(\pm\)0.00 & 0.10\(\pm\)0.00 & 1.33\(\pm\)0.08 & 0.10\(\pm\)0.00 & 0.84\(\pm\)0.08 & 0.81\(\pm\)0.04 \\  & COAT & 3.67\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.73\(\pm\)0.00 & 1.00\(\pm\)0.00 & 0.84\(\pm\)0.07 \\ \hline \multirow{3}{*}{LLMAn2} & META & 2.33\(\pm\)0.47 & 0.67\(\pm\)0.47 & 4.67\(\pm\)0.47 & 0.72\(\pm\)0.32 & 0.32\(\pm\)0.07 & 0.37\(\pm\)0.09 \\  & DATA & 2.33\(\pm\)0.49 & 0.67\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.47\(\pm\)0.19 & 0.75\(\pm\)0.52 & 0.57\(\pm\)0.29 \\ \cline{1-1}  & DATA+COT & 3.00\(\pm\)0.17 & 0.67\(\pm\)0.55 & 0.33\(\pm\)0.59 & 0.60\(\pm\)0.37 & 0.71\(\pm\)0.44 & 0.65\(\pm\)0.37 \\ \cline{1-1}  & COAT & 3.00\(\pm\)0.00 & 0.67\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.80\(\pm\)0.20 & 0.53\(\pm\)0.27 & 0.69\(\pm\)0.04 \\ \hline \multirow{3}{*}{MSTRAL} & META & 3.00\(\pm\)0.00 & 0.67\(\pm\)0.47 & 1.67\(\pm\)1.25 & 0.60\(\pm\)0.50 & 0.59\(\pm\)0.13 & 0.59\(\pm\)0.07 \\ \cline{1-1}  & DATA & 3.00\(\pm\)0.00 & 0.67\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.60\(\pm\)0.00 & 0.53\(\pm\)0.27 & 0.69\(\pm\)0.04 \\ \cline{1-1}  & DATA+COT & 4.33\(\pm\)0.08 & 1.00\(\pm\)0.00 & 0.57\(\pm\)0.53 & 0.87\(\pm\)0.27 & 0.75\(\pm\)0.07 & 0.79\(\pm\)0.04 \\ \cline{1-1}  & COAT & 4.67\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.53\(\pm\)0.00 & 1.00\(\pm\)0.00 & 0.96\(\pm\)0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Factor proposal results in AppleGastronome benchmark (Full results in Appendix E.4).

Figure 5: The discovered causal graphs in AppleGastronome. Compared to the ground truth results, directly adopting LLMs to reason the causal relations can easily elicit many false positive edges. In contrast, the relations recovered by COAT have a high precision and recall. The directed edge between “taste” and “juiciness” can not be recovered by COAT because of the limitations of FCI.

by this observation, we propose two novel metrics to quantify LLMs' causal ability: a) Perception that quantifies the ratio of _valid_ factors (satisfying Prop. 2.2) proposed by LLMs in each round; b) Capacity that measures the effective mutual information drop in Assumption 2.3. As shown in Fig. 4(c), LLMs differ largely on the perception score while comparably on the capacity score.

**Can LLMs be an effective factor annotator?** As shown in Fig. 4, both GPT 3.5 and GPT 4 annotate subjective attributes well. Regarding objective human preferences, the performances are still relatively high. Empirically, LLMs will not introduce new confounders, see Appendix E.3.

**Can COAT reliably recover the causal relationships?** We present quantitative and qualitative results in Table 3 (in Appendix E.1) and Fig. 5 (on page 8), respectively. Compared to directly adopting LLMs to reason the causal relations, COAT significantly boosts the causal relation recovery. Meanwhile, COAT maintains high performances based on various LLMs, which further demonstrates the effectiveness of the causal feedback in COAT to improve the robustness of this system. In fact, the causal feedback focuses on making maximal use of the rich knowledge of LLMs, and reducing the reliance on the reasoning capabilities of different LLMs, to assist with causal discovery. We provide the full results of \(10\) LLMs in Appendix E.3.

## 4 Empirical Study with Realistic Benchmarks

After examining the capabilities of COAT in AppleGastronome, we are further motivated to challenge COAT in a more complex setting from neuropathic panic diagnosis [26], brain tumor detection with MRI images [38], three-years news summary about one stock from the New York Times [39], and climatic reanalysis data with fine-grained time and space coverage [40]. We refer to Appendix H for a complete summary of all five benchmarks.

### Experimental Setup

**Benchmark construction** In the Neuropathic benchmark, we convert the dataset into a clinical diagnosis task. In the original dataset, there are three levels of causal variables, including the symptom level, radioactivity level, and the pathophysiology level. In the experiments, we mainly consider the target variable of right shoulder impingement. When generating the clinical diagnosis notes as \(\bm{x}\) using GPT 4, we avoid any mentioning of variables other than symptoms. We generated 100 samples for LLMs' analysis; since the number of possible factors is finite, we generate 1000 tabular data for CI tests.

As we intend to leverage the Neuropathic benchmark to simulate the real-world diagnosis, after the factor proposal stage, we directly incorporate external tools to measure the values of the candidate factors. More details about the construction of the Neuropathic are given in Appendix E.6.

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline \multirow{2}{*}{LLM} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{Factor Proposal} \\  & & PA & AN & OT & Acc & F1 \\ \hline \multirow{3}{*}{GPT 4} & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & Data & 2 & 2 & 0.95 & 0.50 \\  & DATA+COT & 3 & 4 & 13 & 0.81 & 0.35 \\  & COAT & 3 & 6 & 3 & 0.96 & 0.80 \\ \hline \multirow{3}{*}{GPT 3.5} & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & DATA & 3 & 5 & 4 & 0.94 & 0.67 \\  & DATA+COT & 2 & 2 & 3 & 0.91 & 0.36 \\  & COAT & 3 & 5 & 2 & 0.96 & 0.77 \\ \hline \multirow{3}{*}{LLMAA2} & Meta & 2 & 4 & 5 & 0.91 & 0.53 \\  & DATA & 3 & 3 & 1 & 0.95 & 0.60 \\  & DATA+COT & 2 & 4 & 7 & 0.88 & 0.47 \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\ \hline \multirow{3}{*}{LLMAA2} & Meta & 1 & 3 & 6 & 0.88 & 0.40 \\  & DATA & 3 & 6 & 4 & 0.95 & 0.75 \\  & DATA+COT & 0 & 1 & 10 & 0.81 & 0.12 \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\ \hline \multirow{3}{*}{LLMAA2} & Meta & 1 & 1 & 17 & 0.72 & 0.08 \\  & DATA & 3 & 6 & 3 & 0.96 & 0.80 \\  & DATA+COT & 0 & 0 & 10 & 0.79 & – \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\ \hline \multirow{3}{*}{Mistral-Medrum} & Meta & 3 & 6 & 3 & 0.96 & 0.80 \\  & DATA & 3 & 3 & 2 & 0.94 & 0.66 \\ \cline{1-1}  & DATA+COT & 3 & 5 & 8 & 0.88 & 0.53 \\ \cline{1-1}  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\ \hline \end{tabular}
\end{table}
Table 2: Factor proposal results in Neuropathic. PA, AN, and OT refer to the parents, ancestors, and others, respectively. Accuracy and F1 measure the recovery of the causal ancestors.

Figure 6: The discovered causal graphs in Neuropathic. (c) shows the result based on directly prompting LLM to reason for the causal relations among all factors. Disconnected ones are dropped.

**Evaluation and baselines** In Neuropathic, we adopt a similar evaluation protocol and the baselines as in AppleGastronome. Nevertheless, due to the faithfulness issue of the original dataset [26], for the evaluation of causal relation discovery, we mainly conduct a qualitative comparison between the ground truth that is faithful to the data, against the baselines and COAT.

### Empirical Results on Neuropathic Benchmark

Factor proposalThe quantitative results on Neuropathic benchmark are given in Table 2. Similarly, we can find that COAT consistently outperforms all of the baselines regardless of which LLMs are incorporated. In particular, even with the weakest backbone model, i.e.,LLaMA2-7b, COAT can still effectively leverage the intrinsic rich knowledge and beat the baselines with more powerful LLMs.

**Causal relation recovery** Fig. 6(a) shows the causal graph obtained by FCI running on the original data, where we can find that several causal relations cannot hold on the data. As shown in Fig. 6, when using LLMs to perform the reasoning, LLMs cannot identify the faithfulness issues. In contrast, COAT can imply faithful causal insights.

### More Real-world Results

**El Nino-Southern Oscillation (ENSO) case study** El Nino-Southern Oscillation (ENSO) is a climatic phenomenon in the Pacific Ocean that influences global weather patterns profoundly. To understand its mechanism, we apply COAT on NOAA dataset [40]. There are 13 factors identified by COAT, and their instantaneous causal relations are visualized in Fig 7. The target variable is the future change in monthly SST in the Nino3 region, which could be an important indicator of ENSO events. Each factor is a time series about a certain climate measurement above a specific level averaged over a specific region. The paths about _Sea level Pressure_, _Momentum Flux_, and _Cloud Cover_ matches the existing understanding from literature [41, 42, 43, 44]. It also suggests several inserting hypotheses that are less explored in literature, like the path from Soil Temperature in South American Coastal Region. We refer details in Appendix K.

**More real-world empirical studies** We also report concrete results on real-world problems involving MRI, time series, and NetCDF data in Appendix I, J, and K.

## 5 Conclusions

In this paper, we proposed a new paradigm COAT to incorporate the rich knowledge of LLMs into the CD pipeline. We found that COAT effectively extends the scope of CDs to unstructured data by identifying useful high-level variables from raw observations for CD methods. COAT suggests a new pathway towards building a causal foundation model for discovery. We leave more detailed discussions about future studies in Appendix B.

Figure 7: The final causal graph found by COAT in the ENSO case study

## Acknowledgments and Disclosure of Funding

We thank the reviewers for their valuable comments. This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. CXL and BH were supported by NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. MMG was partially supported by the following Australian Research Council projects: DE210101624 and DP240102088. JC was supported by CUHK direct grant 4055146.

## References

* [1] Norwood Russell Hanson. _Patterns of discovery : an inquiry into the conceptual foundations of science_. Cambridge University Press, 1958.
* [2] Thomas S. Kuhn and David Hawkins. The structure of scientific revolutions. _American Journal of Physics_, 31:554-555, 1963.
* [3] Peter Spirtes, Clark Glymour, and Richard Scheines. _Causation, Prediction, and Search, Second Edition_. Adaptive computation and machine learning. MIT Press, 2000. ISBN 978-0-262-19440-2.
* [4] Peter Spirtes, Clark Glymour, Richard Scheines, and Robert Tillman. _Automated Search for Causal Relations: Theory and Practice_, 2018.
* [5] Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D'ya like dags? a survey on structure learning and causal discovery. _ACM Computing Survey_, 55(4), 2022. ISSN 0360-0300.
* [6] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Towards causal representation learning. _arXiv preprint_, arXiv:2102.11107, 2021.
* [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, 2020.
* [8] OpenAI. Chatgpt. https://chat.openai.com/chat/, 2022.
* [9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint_, arXiv:2302.13971, 2023.
* [10] OpenAI. Gpt-4 technical report, 2023.
* [11] Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new frontier for causality. _arXiv preprint_, arXiv:2305.00050, 2023.
* [12] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. _arXiv preprint_, arXiv:2303.12712, 2023.
** [13] Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, and James Vaughan. Understanding causality with large language models: Feasibility and opportunities. _arXiv preprint_, arXiv:2304.05524, 2023.
* [14] Ahmed Abdulaal, adamos hadjivasiliou, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, and Daniel C. Alexander. Causal modelling agents: Causal graph discovery through synergising metadata- and data-driven reasoning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [15] Matej Zecevic, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* [16] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng LYU, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Scholkopf. CLadder: A benchmark to assess causal reasoning capabilities of language models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [17] Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, and Bernhard Scholkopf. Can large language models infer causation from correlation? _arXiv preprint_, arXiv:2306.05836, 2023.
* [18] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the AI ocean: A survey on hallucination in large language models. _arXiv preprint_, arXiv:2309.01219, 2023.
* [19] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v(ision): Bias and interference challenges. _arXiv preprint_, arXiv:2311.03287, 2023.
* [20] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on "a is b" fail to learn "b is a". _arXiv preprint_, arXiv:2309.12288, 2023.
* [21] Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D. Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for classification part i: Algorithms and empirical evaluation. _Journal of Machine Learning Research_, 11(7):171-234, 2010.
* [22] Shantanu Gupta, David Childers, and Zachary Chase Lipton. Local causal discovery for estimating causal effects. In _Conference on Causal Learning and Reasoning_, volume 213, pages 408-447, 2023.
* [23] Judea Pearl and Dana Mackenzie. _The Book of Why: The New Science of Cause and Effect_. Basic Books, Inc., USA, 1st edition, 2018. ISBN 046509760X.
* [24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint_, arXiv:2302.04761, 2023.
* [25] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. _arXiv preprint_, arXiv:2309.07864, 2023.
* [26] Ruibo Tu, Kun Zhang, Bo C. Bertilson, Hedvig Kjellstrom, and Cheng Zhang. Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation. In _Advances in Neural Information Processing Systems_, pages 12773-12784, 2019.
** [27] Peter Spirtes, Christopher Meek, and Thomas Richardson. Causal inference in the presence of latent variables and selection bias. In _Uncertainty in Artificial Intelligence_, page 499-506, 1995.
* [28] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. _Frontiers in Genetics_, 10, 2019.
* [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [30] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In _Conference on Empirical Methods in Natural Language Processing_, pages 1051-1068, 2023.
* [31] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. Reasoning with language model prompting: A survey. In _Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, July 2023.
* networks of plausible inference. In _Morgan Kaufmann series in representation and reasoning_, 1991.
* [33] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of Causal Inference: Foundations and Learning Algorithms_. The MIT Press, 2017. ISBN 0262037319.
* [34] OpenAI. Hello, gpt-4o! https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2024-05-20.
* [35] Anthropic. Claude 3 family. https://www.anthropic.com/news/claude-3-family, 2024. Accessed: 2024-05-20.
* [36] Meta AI. Meta llama 3. https://ai.meta.com/blog/meta-llama-3/, 2024. Accessed: 2024-05-20.
* [37] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mixtral of experts. _arXiv preprint_, arXiv:2401.04088, 2024.
* [38] Sartaj Bhuvaji, Ankita Kadam, Prajakta Bhumkar, and Sameer Dedge. Brain tumor classification using deep learning algorithms. https://github.com/SartajBhuvaji/Brain-Tumor-Classification-Using-Deep-Learning-Algorithms, 2024. Accessed: 2024-05-19.
* [39] Bidec Innovations. Stock price and news related to it, 2023. URL https://www.kaggle.com/datasets/Bidec Innovations/stock-price-and-news-realled-to-it. Accessed: 2023-10-01.
* [40] Gilbert P Compo, Jeffrey S Whitaker, Prashant D Sardeshmukh, Nobuki Matsui, Robert J Allan, Xungang Yin, Byron E Gleason, Russell S Vose, Glenn Rutledge, Pierre Bessemoulin, et al. The twentieth century reanalysis project. _Quarterly Journal of the Royal Meteorological Society_, 137(654):1-28, 2011.
* [41] Jakob Bjerknes. Atmospheric teleconnections from the equatorial pacific. _Monthly weather review_, 97(3):163-172, 1969.

* Wang [2004] Chunzai Wang. Enso, atlantic climate variability, and the walker and hadley circulations. In _The Hadley circulation: Present, past and future_, pages 173-202. Springer, 2004.
* Liu et al. [2016] Yinge Liu, Ninglian Wang, Lingang Wang, Zhongming Guo, and Xiaobo Wu. Variation of cloud amount over china and the relationship with enso from 1951 to 2014. _International Journal of Climatology_, 36(8):2931-2941, 2016.
* Mishra [2019] Anoop Kumar Mishra. Investigating changes in cloud cover using the long-term record of precipitation extremes. _Meteorological Applications_, 26(1):108-116, 2019.
* Lee et al. [2023] Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. _New England Journal of Medicine_, 388(13):1233-1239, 2023.
* Tu et al. [2023] Ruibo Tu, Chao Ma, and Cheng Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. _arXiv preprint_, arXiv:2301.13819, 2023.
* Pearl and Robins [1999] Judea Pearl and James M. Robins. Causal diagrams for epidemiologic research. _Epidemiology_, 10 1:37-48, 1999.
* Shimizu et al. [2006] Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7(10), 2006.
* Zhang and Hyvarinen [2012] Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. _arXiv preprint arXiv:1205.2599_, 2012.
* Hoyer et al. [2008] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. _Advances in neural information processing systems_, 2008.
* Huang et al. [2020] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Scholkopf. Causal discovery from heterogeneous/nonstationary data. _Journal of Machine Learning Research_, 21(89):1-53, 2020.
* Yang et al. [2018] Karren Yang, Abigail Katcoff, and Caroline Uhler. Characterizing and learning equivalence classes of causal DAGs under interventions. In _International Conference on Machine Learning_, pages 5541-5550, 2018.
* Brouillard et al. [2020] Philippe Brouillard, Sebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. In _Advances in Neural Information Processing Systems_, pages 21865-21877, 2020.
* Mooij et al. [2020] Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. _Journal of Machine Learning Research_, 21(99):1-108, 2020.
* Perry et al. [2022] Ronan Perry, Julius von Kugelgen, and Bernhard Scholkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. In _Advances in Neural Information Processing Systems_, pages 10904-10917, 2022.
* Malinsky and Spirtes [2019] Daniel Malinsky and Peter Spirtes. Learning the structure of a nonstationary vector autoregression. In _International Conference on Artificial Intelligence and Statistics_, pages 2986-2994, 2019.
* Huang et al. [2019] Biwei Huang, Kun Zhang, Mingming Gong, and Clark Glymour. Causal discovery and forecasting in nonstationary environments with state-space models. In _International Conference on Machine Learning_, pages 2901-2910, 2019.
* Liu and Kuang [2023] Chenxi Liu and Kun Kuang. Causal structure learning for latent intervened non-stationary data. In _International Conference on Machine Learning_, pages 21756-21777, 2023.

* Chen et al. [2022] Zhengming Chen, Feng Xie, Jie Qiao, Zhifeng Hao, Kun Zhang, and Ruichu Cai. Identification of linear latent variable model with arbitrary distribution. In _AAAI Conference on Artificial Intelligence_, pages 6350-6357, 2022.
* Wu et al. [2024] Anpeng Wu, Haoxuan Li, Kun Kuang, Zhang Keli, and Fei Wu. Learning causal relations from subsampled time series with two time-slices. In _International Conference on Machine Learning_, 2024.
* Huang et al. [2022] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark Glymour, and Kun Zhang. Latent hierarchical causal structure discovery with rank constraints. _Advances in neural information processing systems_, pages 5549-5561, 2022.
* Dong et al. [2023] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. A versatile causal discovery framework to allow causally-related hidden variables. _arXiv preprint_, arXiv:2312.11001, 2023.
* Jiang and Aragam [2023] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interventions. In _Advances in Neural Information Processing Systems_, volume 36, pages 60468-60513, 2023.
* Li et al. [2024] Xiu-Chuan Li, Kun Zhang, and Tongliang Liu. Causal structure recovery with latent variables under milder distributional and graphical assumptions. In _International Conference on Learning Representations_, 2024.
* Hyvarinen et al. [2019] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In _International Conference on Artificial Intelligence and Statistics_, pages 859-868, 2019.
* Khemakhem et al. [2020] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217, 2020.
* Locatello et al. [2020] Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359, 2020.
* Klindt et al. [2021] David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding, 2021.
* Lachapelle et al. [2022] Sebastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Remi LE PRIOL, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _Conference on Causal Learning and Reasoning_, pages 428-484, 2022.
* Ahuja et al. [2022] Kartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. In _Advances in Neural Information Processing Systems_, pages 15516-15528, 2022.
* Ahuja et al. [2023] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In _International conference on machine learning_, pages 372-407, 2023.
* Zhang et al. [2024] Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. _Advances in Neural Information Processing Systems_, 2024.
* Kugelgen et al. [2023] Julius von Kugelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Kekic, Elias Bareinboim, David Blei, and Bernhard Scholkopf. Nonparametric identifiability of causal representations from unknown interventions. In _Advances in Neural Information Processing Systems_, pages 48603-48638, 2023.

* [74] Chandler Squires, Anna Seigal, Salil S Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In _International Conference on Machine Learning_, pages 32540-32560, 2023.
* [75] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. _Advances in Neural Information Processing Systems_, 2024.
* [76] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _Advances in Neural Information Processing Systems_, 2022.
* [77] Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Does invariant graph learning via environment augmentation learn invariance? In _Advances in Neural Information Processing Systems_, 2023.
* [78] Yongqiang Chen, Yatao Bian, Bo Han, and James Cheng. Interpretable and generalizable graph learning via subgraph multilinear extension. In _ICLR 2024 Workshop on Machine Learning for Genomics Explorations_, 2024.
* [79] Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Kaili Ma, Yonggang Zhang, Han Yang, Bo Han, and James Cheng. Pareto invariant risk minimization. _arXiv preprint_, arXiv:2206.07766, 2022.
* [80] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In _Advances in Neural Information Processing Systems_, 2023.
* [81] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* [82] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _arXiv preprint_, arXiv:2110.14168, 2021.
* [83] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [84] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In _The Eleventh International Conference on Learning Representations_, 2023.
* [85] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypontize large language model to be jailbreaker, 2024. URL https://arxiv.org/abs/2311.03191.
* [86] Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, and Bo Han. Can large language models reason robustly with noisy rationales? In _ICLR 2024 Workshop on Reliable and Responsible Foundation Models_, 2024.
* 400, 2022. (Cited on page 22)
* Cui et al. [2024] Shaobo Cui, Zhijing Jin, Bernhard Scholkopf, and Boi Faltings. The odyssey of common-sense causality: From foundational benchmarks to cutting-edge reasoning. _arXiv preprint arXiv:2406.19307_, 2024.
* Hosseini et al. [2021] Pedram Hosseini, David A Broniatowski, and Mona Diab. Predicting directionality in causal relations in text. _arXiv preprint arXiv:2103.13606_, 2021.
* Gao et al. [2019] Lei Gao, Prafulla Kumar Choubey, and Ruihong Huang. Modeling document-level causal structures for event causal relation identification. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1808-1817, 2019.
* Weber et al. [2020] Noah Weber, Rachel Rudinger, and Benjamin Van Durme. Causal inference of script knowledge. _arXiv preprint arXiv:2004.01174_, 2020.
* Liu et al. [2021] Xiao Liu, Da Yin, Yansong Feng, Yuting Wu, and Dongyan Zhao. Everything has a cause: Leveraging causal inference in legal text analysis. _arXiv preprint arXiv:2104.09420_, 2021.
* Lampinen et al. [2023] Andrew Kyle Lampinen, Stephanie C.Y. Chan, Ishita Dasgupta, Andrew Joo Hun Nam, and Jane X Wang. Passive learning of active causal strategies in agents and language models. In _Advances in Neural Information Processing Systems_, 2023.
* Choi et al. [2022] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained language models as task-specific priors. _arXiv preprint_, arXiv:2210.12530, 2022.
* Long et al. [2023] Stephanie Long, Alexandre Piche, Valentina Zantedeschi, Tibor Schuster, and Alexandre Drouin. Causal discovery with language models as imperfect experts. _arXiv preprint_, arXiv:2307.02390, 2023.
* Ban et al. [2023] Taiyu Ban, Lyuzhou Chen, Xiangyu Wang, and Huanhuan Chen. From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. _arXiv preprint_, arXiv:2306.16902, 2023.
* Willig et al. [2022] Moritz Willig, Matej Zecevic, Devendra Singh Dhami, and Kristian Kersting. Can foundation models talk causality? In _UAI 2022 Workshop on Causal Representation Learning_, 2022.
* Long et al. [2023] Stephanie Long, Tibor Schuster, and Alexandre Piche. Can large language models build causal graphs? _arXiv preprint_, arXiv:2303.05279, 2023.
* LYU et al. [2022] Zhiheng LYU, Zhijing Jin, Rada Mihalcea, Mrinmaya Sachan, and Bernhard Scholkopf. Can large language models distinguish cause from effect? In _UAI 2022 Workshop on Causal Representation Learning_, 2022.
* Zhang et al. [2023] Yanming Zhang, Brette Fitzgibbon, Dino Garofolo, Akshith Kota, Eric Papenhausen, and Klaus Mueller. An explainable AI approach to large language model assisted causal model auditing and development. _arXiv preprint_, arXiv:2312.16211, 2023.
* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36:46595-46623, 2023.
* Lyndon et al. [2019] Daniel Lyndon, Joseph A Lansley, Jane Evanson, and Anant S Krishnan. Dural masses: meningiomas and their mimics. _Insights into imaging_, 10(1):11, 2019.
* Haydar et al. [2022] Nisreen Haydar, Khatoun Alyousef, Usama Alanan, Rana Issa, Fawaz Baddour, Zuheir Al-Shehabi, and Moatasem Hussein Al-Janabi. Role of magnetic resonance imaging (mri) in grading gliomas comparable with pathology: A cross-sectional study from syria. _Annals of Medicine and Surgery_, 82:104679, 2022.

* [104] Birkan Tunc, David A Hormuth II, George Biros, and Thomas E Yankeelov. Modeling of glioma growth with mass effect by longitudinal magnetic resonance imaging. _IEEE Transactions on Biomedical Engineering_, 68(12):3713-3724, 2021.
* [105] J Watts, G Box, A Galvin, P Brotchie, N Trost, and T Sutherland. Magnetic resonance imaging of meningiomas: a pictorial review. _Insights into imaging_, 5:113-122, 2014.
* [106] John Markoff. Competing as software goes to web. _The New York Times_, 2007. URL https://www.nytimes.com/2007/06/05/technology/05compute.html. Accessed: 2024-03-14.
* [107] Fama Eugene. Efficient capital markets: A review of theory and empirical work. _Journal of finance_, 25:383-417, 1970.
* [108] Jessica A Wachter. Can time-varying risk of rare disasters explain aggregate stock market volatility? _The Journal of Finance_, 68(3):987-1035, 2013.
* [109] Leland Bybee, Bryan T Kelly, Asaf Manela, and Dacheng Xiu. Business news and business cycles. _Journal of Finance, Forthcoming_, 2023.
* [110] Michael J McPhaden. Genesis and evolution of the 1997-98 el nino. _Science_, 283(5404):950-954, 1999.
* [111] Chester F Ropelewski and Michael S Halpert. Global and regional scale precipitation patterns associated with the el nino/southern oscillation. _Monthly weather review_, 115(8):1606-1626, 1987.
* [112] Wenju Cai, Simon Borlace, Matthieu Lengaigne, Peter Van Rensch, Mat Collins, Gabriel Vecchi, Axel Timmermann, Agus Santoso, Michael J McPhaden, Lixin Wu, et al. Increasing frequency of extreme el nino events due to greenhouse warming. _Nature climate change_, 4(2):111-116, 2014.
* [113] Chunzai Wang, Clara Deser, Jin-Yi Yu, Pedro DiNezio, and Amy Clement. El nino and southern oscillation (enso): a review. _Coral reefs of the eastern tropical Pacific: Persistence and loss in a dynamic environment_, pages 85-106, 2017.
* [114] Joris M Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. _Journal of Machine Learning Research_, 21(99):1-108, 2020.
* [115] Yi Ge Zhang, Mark Pagani, Jorijntje Henderiks, and Haojia Ren. A long history of equatorial deep-water upwelling in the pacific ocean. _Earth and Planetary Science Letters_, 467:1-9, 2017. ISSN 0012-821X.
* [116] J. Tarazona and W. Arntz. _The Peruvian Coastal Upwelling System_, pages 229-244. Springer Berlin Heidelberg, 2001.

**Appendix of COAT**

###### Contents

* A Table of Notations
* B Limitation and Future Opportunities
* C Related Work
* D Proofs for Theoretical Results
* D.1 Proof for Proposition 2.2
* D.2 Proof for Proposition 2.4
* E More Details about Experiments
* E.1 More Details on Constructing AppleGastronome
* E.2 More Details on Prompts for AppleGastronome
* E.3 More Details of Results on AppleGastronome
* E.4 The Full Result on the Apple Gastronome Benchmark
* E.5 Implementation of the FCI algorithm
* E.6 More Details on Constructing Neuropathic
* E.7 More Details of Results on Neuropathic
* E.8 Discussion on the time complexity
* E.9 Resources
* F COAT with Different Causal Discovery Algorithm
* G Ablation Study
* H Summary of Benchmark Data
* I Case Study on Brain Tumor
	* I.1 The Brain Tumor Dataset
	* I.2 Result and Discussion
* J Case Study on Stock News
* J.1 The Stock News Dataset
* J.2 Result and Discussion
* K Case Study on El Nino-Southern Oscillation (ENSO)
* K.1 Setting and Data processing
* K.2 Causal Graph and Discussion
* L Broader Impacts
* L

## Appendix A Table of Notations

\begin{tabular}{c|l} \hline \hline
**Notation** & **Description** \\ \hline \(\Psi\) & the LLM for factor proposal. \\ \hline \(\bm{p}^{t}\) & Prompt used at the \(t\)-th round for factor proposal. \\ \hline \(\Psi_{p}\) & the LLM for factor parsing. \\ \hline \(\bm{p}_{p}\) & Prompt used for factor parsing. \\ \hline \(\mathcal{A}\) & Algorithm for causal discovery. In this paper we mainly use FCI. \\ \hline \(\mathcal{F}\) & Abstraction of the COAT's Feedback procedure. \\ \hline \(\mathcal{D}\) & Full sample set. \\ \hline \(\widehat{\mathcal{D}}\) & A subset of sample set \\ \hline \(\widehat{\mathcal{D}}^{t}\) & The subset of sample used at the \(t\)-th round. \\ \hline \(\mathcal{C}\) & The predefined value space for LLM-proposed factors, e.g., set \(\mathcal{C}\) as \(\{-1,0,1\}\) \\ \hline \(\bm{x}\) & An unstructured sample of raw observation drawn from \(\mathcal{X}\) \\ \hline \(y\) & A structured sample of target variable drawn from \(\mathcal{Y}\) \\ \hline \(Y\) & target variable treated as a random variable. \\ \hline \(X\) & raw observation treated as a random variable. \\ \hline \(\bm{w}_{i}\) & The \(i\)-th factor proposed by LLMs, like _sweetness_, _size_, or _scent_ \\ \hline \(\bm{w}_{i}(\cdot)\) & Factor's corresponding mapping from raw observation space \(\mathcal{X}\) to the predefined value space \(\mathcal{C}\). \\ \hline \(\mathcal{W}^{t}\) & The set of factors proposed by LLMs at the \(t\)-th round. \\ \hline \(\mathcal{W}^{\leq t}\) & The set of factors proposed by LLMs at the first \(t\) rounds. \\ \hline \(\mathcal{S}\) & Index set of selected factors in \(\mathcal{W}\), e.g., \(\mathcal{S}=\{1,3\}\) means only select the first and third factors. \\ \hline \(h_{\mathcal{S}}(\cdot)\) & LLM-induced representation over selection \(\mathcal{S}\), e.g., \(h_{\{1,3\}}(\cdot)=(\bm{w}_{1}(\cdot),\bm{w}_{3}(\cdot))\). \(h(\cdot):=h_{[k]}(\cdot)\) \\ \hline \(\bm{Z}\) & Matrix of values of the structured high-level factors proposed by LLMs, i.e., \(\widehat{\bm{Z}}\in\mathcal{C}^{n\times k}\) \\ \hline \(\bm{Z}^{t}\) & Matrix of values of the structured high-level factors proposed at the \(t\)-th COAT round. \\ \hline \(\mathcal{Z}^{\leq t}\) & Matrix of values of the structured high-level factors proposed at the first \(t\) COAT rounds. \\ \hline \(\mathcal{G}^{t}\) & Causal graph at the \(t\)-th COAT round over \(\widehat{\bm{Z}}^{\leq t}\cup\{Y\}\) \\ \hline \(k\) & Number of factors \\ \hline \(n\) & Number of samples \\ \hline \(C\) & Number of clusters \\ \hline \(C_{\Psi}\) & Capacity Score of LLM \(\Psi\) \\ \hline \(p_{\Psi}\) or \(p\) & Perception Score of LLM \(\Psi\) \\ \hline \hline \end{tabular}

## Appendix B Limitation and Future Opportunities

Future OpportunitiesDespite the progress in the past decades, existing causal discovery algorithms mainly rely on high-quality measured variables given by human experts [3; 4; 5]. However, the causal variables and their measurements are usually available in a wide range of real-world applications. For example, Amazon sellers who want to analyze the factors related to user ratings only have user reviews, generated by the underlying user preferences for certain product characteristics. Therefore, the lack of measured high-quality causal variables has been a major impediment to broader real-world applications of causal or causality-inspired methods [6].

This work establishes and demonstrates a preliminary implementation of the system **C**ausal representati**O**n **A**ssistan**T** (COAT). We present comprehensive evaluations of COAT and find plentiful evidence that the recent emergence of Large Language Models (LLMs) [7; 8; 9; 10] has a great potential to mitigate the gap. In particular, as shown in Fig. 8, we envision an entangled system towards causal foundation models, which consists of two _mutually beneficial_ components: LLMs and causal discovery methods (CDs). On the one hand, LLMs that learn rich world knowledge about the world can assist with discovering high-level hidden variables from low-level observational data [12], or even certain commonsense causal knowledge [11; 13; 14]. On the other hand, it has also been found that the reliability of LLMs in the reasoning of causality remains a debate [13; 15; 16; 17], due to a series of drawbacks of LLMs [18; 19; 20], as well as the ethical considerations [45; 46]. Nevertheless, CDs can uncover the underlying causal relations between the discovered high-level variables with guarantees. More importantly, CDs can also provide feedback to improve the identification of the variables. The combination of LLMs and CDs iteratively improves the discovery of the hidden causal world, and hence opens up a broader adoption of various causal methods.

LimitationsWe would like to discuss some technical limitations of the current version of COAT.

* **Multicollinearity.** Sometimes, LLM may output factors with shared similar semantic meanings or overlapped factor-value criteria. The consequence of this phenomenon is that multicollinearity could occur in the resulting structured data, or the data matrix (each sample per row) would not be full column rank. This would hinder the conditional independent tests. In experiments, we adopt numerical methods like PCA to drop similar factors. More sophisticated methods could be investigated in the future.
* **Toward complete causal graph.** In this work, we mainly focus on one single target variable and identify a set of factors to serve as a Markov blanket of this target variable. Indeed, one can explore a more complete causal graph by applying COAT procedures on those identified factors to acquire a more comprehensive causal graph. One may also introduce multiple target variables or let LLM define a suitable target variable (and also parse it out) from pure raw observations. These extensions are also important, and we leave it to future work.

Figure 8: COAT combines both strengths of LLMs that learn the rich knowledge of the world, and causal discovery methods to uncover the hidden causal world. With the uncovered causal knowledge, COAT can empower broader applications of causal methods.

Meanwhile, identifiable causal discovery also requires certain assumptions about the data:

* **Faithfulness.** The empirical distribution of the data reflects the actual data-generating process.
* **No selection bias.** Otherwise, the condition of faithfulness would be violated.
* **Enough sample size.** Our method involves statistical tests; the more, the better.

## Appendix C Related Work

**Causal discovery** aims to discover the _unknown_ causal relations from the observational data [3; 4], which is critical to both real-world applications and scientific discoveries [23; 47]. We use FCI [27] in this paper just to illustrate the idea. To attain identifiability better than the Markov equivalent class, one can choose more advanced Causal Discovery methods under different assumptions, like constrained functional [48; 49; 50], multiple domain data [51; 52; 53; 54; 55], and non-stationary data [51; 56; 57; 58]. There are also some works aiming to address realistic challenges, like structure models with arbitrary distribution [59], subsampled time series [60], and latent hierarchical causal structure [61; 62; 63; 64]. Despite recent theoretical and empirical improvements [5; 28], most existing causal discovery approaches rely on well-structured data with human-crafted factors as inputs. They can easily suffer from the low quality of annotated data (e.g., latent confounders).

**Causal representation learning** aims to establish provable identifiability for latent high-level variables (like location and color of an object) from low-level observations such as images [6]. Such identification can be feasible with certain conditions, like auxiliary information [65; 66; 67], sparsity [68; 69; 70], or interventional data [71; 72; 73; 74; 75]. Recent works also generalize the causal representation learning to graph-structured data [76; 77; 78], and discuss the optimization challenges in realizing causal representation learning [79; 80]. In this work, we show that incorporating LLMs that learn world knowledge from massive training data could effectively relieve the need for artificial causal factor annotation. Meanwhile, COAT also opens up a new line to learn causal representations with rich pre-trained knowledge as well as feedback from causal discovery algorithms.

**Reasoning with LLMs** has achieved remarkable performance across a variety of tasks with few demonstrations of the samples [7; 8; 10; 81]. The strong capabilities of LLMs show that it is evident that LLMs could acquire and understand commonsense knowledge about the world [12]. The power of LLMs can be further unlocked with suitable context as inputs [29]. Nevertheless, LLMs have also been found to make mistakes in basic algorithmic reasoning [82; 83], easy to hallucinate nonfactual results [18], tend to learn shortcuts or dataset biases [19; 20; 84], vulnerable to adversarial jailbreaks or noisy interruptions [85; 86], and fall short in complex planning and reasoning tasks [12]. The drawbacks of LLMs render it _risky_ to rely on the direct LLM reasoning results to derive any rigorous results. Therefore, we do not directly derive the results from LLMs. Rather, we merely leverage the learned world knowledge in LLMs to find useful causal factors by constructing proper instructions based on causal feedback.

**Text mining of causal relations** aims to analyze the causal relations implied by the semantics given a text [87; 88; 89]. In particular, some works focus on the identification of causal relations among events specified in documents [90; 91; 92]. Different from these tasks, where factors are entities or events described by text, in this work, COAT emphasizes crafting high-level factors that go beyond the unstructured data like text description and also establish identifiability on the Markov blanket for a given target variable. Note that the target variable is allowed to be not mentioned in the text or other unstructured data. Therefore, this work brings new scope and opportunities for these text-understanding tasks.

**Causal learning with LLMs** has received muchattention results from the community [11; 13]. Kiciman et al. [11] find that LLMs can recover the pairwise causal relations very well. Lampinen et al. [93] show that transformer-based agents can learn causal strategies passively if intervention is allowed during tests. Abdulal et al. [14], Choi et al. [94], Long et al. [95], Ban et al. [96] propose to incorporate the causal discovery results by LLMs as a prior or constraint to improve the performance of data-driven causal discovery algorithms. However, Zecevic et al. [15], Willig et al. [97] find that LLMs can not understand causality but simply retell the causal knowledge contained in the training data. Zhang et al. [13], Jin et al. [16; 17] find that LLMs can hardly provide satisfactory answers for discovering new knowledge or decision-making tasks. Although Long et al. [98] find that LLMs can build causal graphs with 3-4 nodes, Tu et al. [46] find that the performance of LLMs in more complex causal discovery remains limited as LLMs can hardly understand new concepts and knowledge. The aforementioned debate implies the limitations in directly adopting the causal discovery results by LLMs, which motivates us to incorporate the existing causal discovery algorithms with rigorous guarantees instead of LLMs to learn the causal relations.

LYU et al. [99] find that it is crucial to incorporate prompts aligned with the underlying causal story for LLMs to do pairwise causal relation inference. Zhang et al. [100] propose to leverage LLMs to audit causal discovery results in an explainable way.

The closest works to ours are Abdulaal et al. [14], Choi et al. [94], Long et al. [95], Ban et al. [96] which also incorporates LLMs into the pipeline of causal discovery. Nevertheless, all of the existing combinations of LLMs and causal discovery still focus on artificially curated structured data and rely on the capability of LLMs to infer causal relations, therefore, limited in both the reliability and the utility of LLMs in causal learning.

## Appendix D Proofs for Theoretical Results

### Proof for Proposition 2.2

**Proposition D.1** (Restatement of Proposition 2.2).: _Given the current representation as \(h_{[k]}(X)=\left(\bm{w}_{1}\left(X\right),\bm{w}_{2}\left(X\right),\cdots,\bm {w}_{k}\left(X\right)\right)\), and a new factor \(\bm{w}_{k+1}\) satisfying_

\[Y\not\perp\bm{w}_{k+1}(X)\mid h_{[k]}(X),\] (12)

_for Markov Blanket \(\mathcal{S}\subseteq[k+1]\) of \(Y\), i.e.,_

\[Y\perp h_{[k+1]\setminus\mathcal{S}}(X)\mid h_{\mathcal{S}}(X),\] (13)

_we have the following result about conditional mutual information:_

\[I\left(Y;X\mid h_{\mathcal{S}}(X)\right)=I\left(Y;X\mid h_{[k+1]}(X)\right)<I \left(Y;X\mid h_{[k]}(X)\right)\] (14)

Proof for Prop. 2.2.: From Eq. 12, we have

\[H(y\mid h_{[k]}(\bm{x});\bm{w}_{k+1}(\bm{x}))<H(y\mid h_{[k]}(\bm{x}));\] (15)

From Eq. 13, we have

\[H(y\mid h_{\mathcal{S}}(\bm{x});h_{[k+1]\setminus\mathcal{S}}(\bm{x}))=H(y \mid h_{\mathcal{S}}(\bm{x})).\] (16)

Therefore:

\[H(y\mid h_{\mathcal{S}}(\bm{x})) =H(y\mid h_{\mathcal{S}}(\bm{x}),h_{[k+1]\setminus\mathcal{S}}( \bm{x}))\] (17) \[=H(y\mid h_{[k+1]}(\bm{x}))\] \[=H(y\mid h_{[k]}(\bm{x}),\bm{w}_{k+1}(\bm{x}))\] \[<H(y\mid h_{[k]}(\bm{x}))\]

Also note that

\[H(y\mid h_{\mathcal{S}}(\bm{x}),\bm{x})=H(y\mid\bm{x})=H(y\mid h_{[k]}(\bm{x} ),\bm{x}),\] (18)

therefore:

\[H(y\mid h_{\mathcal{S}}(\bm{x}))-H(y\mid h_{\mathcal{S}}(\bm{x}),\bm{x})<H(y \mid h_{[k]}(\bm{x}))-H(y\mid h_{[k]}(\bm{x}),\bm{x}),\] (19)

which is Eq. 14. 

### Proof for Proposition 2.4

**Proposition D.2** (Restatement of Proposition 2.4).: _Given Assumption 2.3, for any small number \(\epsilon,\delta\in(0,\frac{1}{2})\), with sufficiently \(t\) rounds of COAT, i.e.,_

\[\sqrt{t}>\frac{|z_{\delta}|\sqrt{1-p}}{2\sqrt{p}}\left(1+\sqrt{1+\frac{4 \log\epsilon}{z_{\delta}^{2}(1-p)\log 1-C_{\Psi}}}\right),\] (20)

_where \(z_{\delta}\) is the \(\delta\)-quantile of the standard normal distribution, we have_

\[\Pr\left(\frac{I\left(Y;X\mid h_{\leq t}(X)\right)}{I\left(Y;X\right)}< \epsilon\right)\geq 1-\delta.\] (21)Proof for Prop. 2.4.: Let \(n_{s}\) be the number of rounds that LLM proposed at least one usable factor satisfying Assumption 2.3. Since \(t\) is large, its Binomial distribution \(\text{Binom}(t,p)\) can be approximated by Gaussian distribution \(\mathcal{N}\left(tp,tp(1-p)\right)\). To enforce

\[\Pr\left(\frac{I\left(y;\boldsymbol{x}\mid h_{\leq t}(\boldsymbol{x}) \right)}{I\left(y;\boldsymbol{x}\right)}<\epsilon\right) \geq\Pr\Big{(}\left(1-C_{\Psi}\right)^{n_{s}}<\epsilon\Big{)}\] (22) \[=\Pr\Big{(}n_{s}>\frac{\log\epsilon}{\log\left(1-C_{\Psi}\right)} \Big{)}\] \[=\Pr\Big{(}\frac{n_{s}-tp}{\sqrt{tp(1-p)}}>\frac{1}{\sqrt{tp(1-p) }}\left(\frac{\log\epsilon}{\log\left(1-C_{\Psi}\right)}-tp\right)\Big{)}\] \[\geq 1-\delta,\]

we have

\[\frac{1}{\sqrt{tp(1-p)}}\left(\frac{\log\epsilon}{\log\left(1-C_{\Psi}\right)} -tp\right)<z_{\delta}=-|z_{\delta}|\] (23)

with Gaussian distribution approximation.

Isolating \(\sqrt{t}>0\) from the above inequality, we would have the desired result.

## Appendix E More Details about Experiments

### More Details on Constructing AppleGastronome

In the AppleGastronome benchmark, we consider the target variable as a rating score of the apple by several gastronomes. Each apple has its own attributes, including size, smell, and taste (or sweetness). Each gastronome has a unique preference for some attributes of the apple. They will give and rating as well as write a review according to the matchness of the apple with respect to their preference. We generate the review using GPT 4 by fetching GPT 4 the preferences and the apple attributes.

The prompts for generating the unstructured inputs are given in Fig. 9. The additional results on Relation Extraction are given in Table. 3.

Examples of AppleGastronome are given in Fig. 10.

### More Details on Prompts for AppleGastronome

The prompts for factor proposal are given in Fig. 11.

The prompt for factor annotation is given in Fig. 12.

\begin{table}
\begin{tabular}{l l c c} \hline \hline LLM & Test Object & T & p-value \\ \hline \multirow{2}{*}{GPT 4} & Feature & 0.2828 & 0.9997 \\  & Noise & 0.0327 & 0.9962 \\ \hline \multirow{2}{*}{GPT 3.5} & Feature & 0.4803 & 0.0325 \\  & Noise & 0.0446 & 0.9962 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Independence tests of the annotation noises with annotated features and other noises AppleGastronome.

\begin{table}
\begin{tabular}{l l|c c c c c} \hline \hline LLM & Method & SHD & SID & Recall & Precision & F1 \\ \hline GPT 4o & pairwise & 5.33\(\pm\)_2.62_ & 0.00\(\pm\)_0.00_ & 1.00\(\pm\)_0.00_ & 0.45\(\pm\)_0.07_ & 0.62\(\pm\)_0.07_ \\  & COAT & 1.00\(\pm\)_0.00_ & 0.00\(\pm\)_0.00_ & 1.00\(\pm\)_0.00_ & 0.79\(\pm\)_0.03_ & 0.89\(\pm\)_0.02_ \\ \hline GPT 4 & pairwise & 8.67\(\pm\)_2.05_ & 1.33\(\pm\)_1.89_ & 0.93\(\pm\)_0.09_ & 0.35\(\pm\)_0.05_ & 0.51\(\pm\)_0.06_ \\  & COAT & 2.33\(\pm\)_0.47_ & 1.00\(\pm\)_1.41_ & 0.93\(\pm\)_0.09_ & 0.70\(\pm\)_0.09_ & 0.79\(\pm\)_0.04_ \\ \hline GPT 3.5 & pairwise & 5.67\(\pm\)_0.03_ & 3.00\(\pm\)_2.94_ & 0.67\(\pm\)_0.24_ & 0.49\(\pm\)_0.36_ & 0.55\(\pm\)_0.32_ \\  & COAT & 2.00\(\pm\)_0.82_ & 2.67\(\pm\)_2.05_ & 0.75\(\pm\)_0.20_ & 0.63\(\pm\)_0.21_ & 0.68\(\pm\)_0.21_ \\ \hline \multirow{2}{*}{Mistral-Large} & pairwise & 7.33\(\pm\)_1.89_ & 0.00\(\pm\)_0.00_ & 1.00\(\pm\)_0.00_ & 0.38\(\pm\)_0.03_ & 0.55\(\pm\)_0.03_ \\  & COAT & 2.00\(\pm\)_0.00_ & 1.33\(\pm\)_1.25_ & 0.85\(\pm\)_0.11_ & 0.74\(\pm\)_0.05_ & 0.78\(\pm\)_0.02_ \\ \hline \multirow{2}{*}{Mistral-Medium} & pairwise & 6.00\(\pm\)_1.63_ & 2.00\(\pm\)_0.00_ & 0.51\(\pm\)_0.13_ & 0.37\(\pm\)_0.04_ & 0.42\(\pm\)_0.07_ \\  & COAT & 0.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 1.00\(\pm\)_0.00_ & 0.94\(\pm\)_0.08_ & 0.97\(\pm\)_0.04_ \\ \hline \multirow{2}{*}{llama-3-70b} & pairwise & 4.00\(\pm\)_1.41_ & 0.33\(\pm\)_0.47_ & 0.92\(\pm\)_0.12_ & 0.51\(\pm\)_0.09_ & 0.65\(\pm\)_0.11_ \\  & COAT & 2.33\(\pm\)_0.47_ & 3.67\(\pm\)_1.25_ & 0.75\(\pm\)_0.00_ & 0.70\(\pm\)_0.07_ & 0.72\(\pm\)_0.04_ \\ \hline

[MISSING_PAGE_EMPTY:26]

Figure 11: Illustration of the prompt for factor proposal.

Figure 12: Illustration of the prompt for factor annotation.

- <groups of samples> _
# Tasks: Factor Abstraction.

**What are the high-level factors behind the text that contribute to the allocation of groups?**

 - Propose your candidate factors based on your observation on given data. - The proposed factors should be diverse and different semantically. Their overlapping should be minimized.

 # About Output Your output should contain parts described as follows.

**Part 1**: Consideration.

In this part, feel free to write down the process of your considerations.

Hint: You need to abstract, identify, and design suitable factors with corresponding criteria, and each factor is only allowed to take value from [-1, 0, 1].

**Part 2**: Factor filtration. You should decide whether to use each of the proposed factors by following criteria:

 - Each new factor should be helpful to distinguish the groups. - Each Factor should focus on one concrete aspect and try to avoid overlapping. - Avoid overlapping with those existing factors: - size - atoms

Figure 13: Illustration of the prompt for feedback.

### The Full Result on the Apple Gastronome Benchmark

Full Results on the Apple Gastronome Benchmark are shown in Table 5.

Full Results of Causal Metrics each Round of each LLM on Apple Gastronome Benchmark are shown in Table 6.

### Implementation of the FCI algorithm

We use a third-party open-sourced Python library to perform the FCI algorithm: https://causal-learn.readthedocs.io/en/latest/

We set \(\alpha=0.05\), and independence_test_method="fisherz" throughout all experiments. Other parameters are kept as the default.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline LLMs & Method & MB & NMB & OT & Recall & Precision & F1 \\ \hline GPT 4o & META & 3.00\(\pm\)_0.82_ & 1.00\(\pm\)_0.00_ & 4.00\(\pm\)_0.82_ & 0.60\(\pm\)_0.16_ & 0.37\(\pm\)_0.09_ & 0.46\(\pm\)_0.12_ \\  & DATA & 3.00\(\pm\)_0.00_ & 0.67\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.60\(\pm\)_0.00_ & 0.83\(\pm\)_0.12_ & 0.69\(\pm\)_0.04_ \\  & DATA+Cot & 4.67\(\pm\)_0.88_ & 1.00\(\pm\)_0.00_ & 0.33\(\pm\)_0.58_ & 0.93\(\pm\)_0.12_ & 0.78\(\pm\)_0.10_ & 0.85\(\pm\)_0.10_ \\  & COAT & 4.00\(\pm\)_0.82_ & 0.00\(\pm\)_0.00_ & 0.00\(\pm\)_0.00_ & 0.80\(\pm\)_0.16_ & 1.00\(\pm\)_0.00_ & 0.88\(\pm\)_0.10_ \\ \hline GPT 4 & META & 2.67\(\pm\)_0.94_ & 0.67\(\pm\)_0.47_ & 2.33\(\pm\)_0.47_ & 0.53\(\pm\)_0.19_ & 0.46\(\pm\)_0.08_ & 0.49\(\pm\)_0.13_ \\  & DATA & 3.00\(\pm\)_0.00_ & 0.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.60\(\pm\)_0.00_ & 0.92\(\pm\)_0.12_ & 0.27\(\pm\)_0.04_ \\  & DATA+Cot & 4.33\(\pm\)_0.58_ & 0.83\(\pm\)_0.29_ & 0.17\(\pm\)_0.29_ & 0.87\(\pm\)_0.12_ & 0.81\(\pm\)_0.02_ & 0.84\(\pm\)_0.06_ \\  & COAT & 4.00\(\pm\)_0.82_ & 0.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.80\(\pm\)_0.16_ & 0.93\(\pm\)_0.09_ & 0.85\(\pm\)_0.11_ \\ \hline GPT 3.5 & META & 3.33\(\pm\)_1.25_ & 0.33\(\pm\)_0.47_ & 4.33\(\pm\)_1.25_ & 0.67\(\pm\)_0.25_ & 0.42\(\pm\)_0.12_ & 0.51\(\pm\)_0.15_ \\  & DATA & 2.67\(\pm\)_0.74_ & 0.67\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.53\(\pm\)_0.09_ & 0.81\(\pm\)_0.14_ & 0.64\(\pm\)_0.10_ \\  & DATA+Cot & 5.00\(\pm\)_0.00_ & 1.00\(\pm\)_0.00_ & 1.33\(\pm\)_0.58_ & 1.00\(\pm\)_0.00_ & 0.68\(\pm\)_0.05_ & 0.81\(\pm\)_0.04_ \\  & COAT & 3.67\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.00\(\pm\)_0.00_ & 0.73\(\pm\)_0.09_ & 1.00\(\pm\)_0.00_ & 0.84\(\pm\)_0.07_ \\ \hline \multirow{2}{*}{Mistral-Large} & META & 2.00\(\pm\)_0.82_ & 0.67\(\pm\)_0.47_ & 2.67\(\pm\)_0.94_ & 0.40\(\pm\)_0.16_ & 0.37\(\pm\)_0.09_ & 0.38\(\pm\)_0.12_ \\  & DATA & 3.00\(\pm\)_0.00_ & 0.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.60\(\pm\)_0.00_ & 0.92\(\pm\)_0.12_ & 0.72\(\pm\)_0.04_ \\  & DATA+Cot & 4.33\(\pm\)_0.58_ & 1.00\(\pm\)_0.00_ & 0.67\(\pm\)_0.58_ & 0.87\(\pm\)_0.12_ & 0.73\(\pm\)_0.07_ & 0.79\(\pm\)_0.05_ \\  & COAT & 4.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.00\(\pm\)_0.00_ & 0.87\(\pm\)_0.09_ & 1.00\(\pm\)_0.00_ & 0.93\(\pm\)_0.05_ \\ \hline \multirow{2}{*}{Mistral-Medium} & META & 3.00\(\pm\)_0.00_ & 0.67\(\pm\)_0.47_ & 1.67\(\pm\)_1.25_ & 0.60\(\pm\)_0.00_ & 0.59\(\pm\)_0.13_ & 0.59\(\pm\)_0.07_ \\  & DATA & 3.00\(\pm\)_0.00_ & 0.67\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.60\(\pm\)_0.00_ & 0.83\(\pm\)_0.12_ & 0.69\(\pm\)_0.04_ \\  & DATA+Cot & 4.33\(\pm\)_0.58_ & 1.00\(\pm\)_0.00_ & 0.67\(\pm\)_0.58_ & 0.87\(\pm\)_0.12_ & 0.73\(\pm\)_0.07_ & 0.79\(\pm\)_0.05_ \\  & COAT & 4.67\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.00\(\pm\)_0.00_ & 0.93\(\pm\)_0.09_ & 1.00\(\pm\)_0.00_ & 0.96\(\pm\)_0.05_ \\ \hline \multirow{2}{*}{ILMA-3-700} & META & 2.67\(\pm\)_0.47_ & 0.33\(\pm\)_0.47_ & 4.67\(\pm\)_0.47_ & 0.53\(\pm\)_0.09_ & 0.35\(\pm\)_0.05_ & 0.42\(\pm\)_0.07_ \\  & DATA & 2.67\(\pm\)_1.25_ & 0.33\(\pm\)_0.47_ & 0.00\(\pm\)_0.00_ & 0.53\(\pm\)_0.25_ & 0.93\(\pm\)_0.0

## 6 Conclusion

\begin{table}
\begin{tabular}{l c c c c} \hline LLMs & COAT Round & Perception Score & Capacity Score & \(I(y;x|h_{S})\) \\ \hline GPT 4o & iter 1 & 0.82\(\pm\)_0.02_ & 0.24\(\pm\)_0.01_ & 0.28\(\pm\)_0.02_ \\  & iter 2 & 0.44\(\pm\)_0.08_ & 0.26\(\pm\)_0.02_ & 0.21\(\pm\)_0.03_ \\  & iter 3 & 0.38\(\pm\)_0.12_ & 0.19\(\pm\)_0.09_ & 0.24\(\pm\)_0.05_ \\ \hline GPT 4 & iter 1 & 0.78\(\pm\)_0.02_ & 0.18\(\pm\)_0.02_ & 0.45\(\pm\)_0.04_ \\  & iter 2 & 0.61\(\pm\)_0.08_ & 0.26\(\pm\)_0.05_ & 0.28\(\pm\)_0.05_ \\  & iter 3 & 0.39\(\pm\)_0.28_ & 0.22\(\pm\)_0.00_ & 0.29\(\pm\)_0.02_ \\ \hline GPT 3.5 & iter 1 & 0.81\(\pm\)_0.02_ & 0.21\(\pm\)_0.01_ & 0.42\(\pm\)_0.05_ \\  & iter 2 & 0.52\(\pm\)_0.11_ & 0.23\(\pm\)_0.02_ & 0.31\(\pm\)_0.04_ \\  & iter 3 & 0.38\(\pm\)_0.10_ & 0.21\(\pm\)_0.03_ & 0.33\(\pm\)_0.11_ \\ \hline \multicolumn{2}{l}{Mistral-Large} & iter 1 & 0.78\(\pm\)_0.02_ & 0.20\(\pm\)_0.01_ & 0.41\(\pm\)_0.04_ \\  & iter 2 & 0.61\(\pm\)_0.08_ & 0.20\(\pm\)_0.03_ & 0.29\(\pm\)_0.02_ \\  & iter 3 & 0.33\(\pm\)_0.33_ & 0.32\(\pm\)_0.00_ & 0.25\(\pm\)_0.05_ \\ \hline \multicolumn{2}{l}{Mistral-Medium} & iter 1 & 0.78\(\pm\)_0.02_ & 0.20\(\pm\)_0.02_ & 0.39\(\pm\)_0.05_ \\  & iter 2 & 0.39\(\pm\)_0.15_ & 0.19\(\pm\)_0.10_ & 0.32\(\pm\)_0.04_ \\  & iter 3 & 0.36\(\pm\)_0.10_ & 0.37\(\pm\)_0.14_ & 0.25\(\pm\)_0.08_ \\ \hline \multicolumn{2}{l}{l}{llama-3-70b} & iter 1 & 0.75\(\pm\)_0.00_ & 0.19\(\pm\)_0.02_ & 0.47\(\pm\)_0.08_ \\  & iter 2 & 0.36\(\pm\)_0.31_ & 0.19\(\pm\)_0.00_ & 0.38\(\pm\)_0.02_ \\  & iter 3 & 0.27\(\pm\)_0.38_ & 0.18\(\pm\)_0.00_ & 0.36\(\pm\)_0.03_ \\ \hline \multicolumn{2}{l}{llama-2-70b} & iter 1 & 0.77\(\pm\)_0.02_ & 0.21\(\pm\)_0.05_ & 0.43\(\pm\)_0.13_ \\  & iter 2 & 0.08\(\pm\)_0.12_ & 0.35\(\pm\)_0.00_ & 0.35\(\pm\)_0.03_ \\  & iter 3 & 0.28\(\pm\)_0.04_ & 0.14\(\pm\)_0.07_ & 0.33\(\pm\)_0.03_ \\ \hline \multicolumn{2}{l}{Qwen-1.5-110b} & iter 1 & 0.77\(\pm\)_0.02_ & 0.17\(\pm\)_0.01_ & 0.45\(\pm\)_0.09_ \\  & iter 2 & 0.56\(\pm\)_0.16_ & 0.21\(\pm\)_0.12_ & 0.32\(\pm\)_0.08_ \\  & iter 3 & 0.28\(\pm\)_0.21_ & 0.18\(\pm\)_0.02_ & 0.33\(\pm\)_0.11_ \\ \hline \multicolumn{2}{l}{DeepSeek-V2} & iter 1 & 0.89\(\pm\)_0.08_ & 0.17\(\pm\)_0.02_ & 0.42\(\pm\)_0.05_ \\  & iter 2 & 0.62\(\pm\)_0.10_ & 0.20\(\pm\)_0.02_ & 0.34\(\pm\)_0.02_ \\  & iter 3 & 0.67\(\pm\)_0.24_ & 0.40\(\pm\)_0.12_ & 0.29\(\pm\)_0.06_ \\ \hline \multicolumn{2}{l}{Claude-3-Opus} & iter 1 & 0.77\(\pm\)_0.02_ & 0.18\(\pm\)_0.00_ & 0.43\(\pm\)_0.06_ \\  & iter 2 & 0.56\(\pm\)_0.08_ & 0.39\(\pm\)_0.11_ & 0.24\(\pm\)_0.06_ \\  & iter 3 & 0.75\(\pm\)_0.25_ & 0.15\(\pm\)_0.00_ & 0.22\(\pm\)_0.05_ \\ \hline \end{tabular}
\end{table}
Table 6: Full Result of Causal Metrics each Round each LLM on Apple Gastronome Benchmark

Figure 14: Ground truth and faithful (via FCI algorithm) causal graphs in AppleGastronome.

Figure 15: Causal graphs with GPT 4 in AppleGastronome.

Figure 16: Causal graphs with GPT 3.5 in AppleGastronome.

Figure 17: Causal graphs with Llama-2 in AppleGastronome.

Figure 19: Causal graphs with Claude-3-Opus in AppleGastronome.

Figure 18: Causal graphs with mistral Medium in AppleGastronome.

Figure 21: Causal graphs with Llama-3-70b in AppleGastronome.

Figure 20: Causal graphs with DeepSeek-V2 in AppleGastronome.

Figure 23: Causal graphs with qwen-1.5-110B in AppleGastronome.

Figure 22: Causal graphs with mistral-Large in AppleGastronome.

Figure 24: Causal graphs with GPT-4o in AppleGastronome.

### More Details on Constructing Neuropathic

In the Neuropathic benchmark, we convert the dataset into a clinical diagnosis task. In the original dataset, there are three levels of causal variables, including the symptom-level, radiculopathy-level and the pathophysiology-level. In experiments, we mainly consider the target variable of right shoulder impingement. When generating the clinical diagnosis notes as \(\bm{x}\) using GPT 4, we will avoid any mentioning of variables other than symptoms.

As we intend to leverage the Neuropathic benchmark to simulate the real-life diagnosis, after the factor proposal stage, we directly incorporate external experts that measure the values of the candidate factors. The prompts to generate the diagnosis records are given in Fig. 25.

Examples of Neuropathic are given in Fig. 26.

### More Details of Results on Neuropathic

The detailed causal graph results are given from Fig. 27 to Fig. 31.

### Discussion on the time complexity

Assume there are m samples with n possible factors.

Figure 25: Illustration of prompts for generating Neuropathic.

For Factor Proposal:

META only needs to interact once with LLM, so it is O(1). DATA runs one single COAT round, so it is O(1). COAT interacts with LLMs multiple rounds. In each round, at least one new factor should be proposed; otherwise, the loop will stop. So it is O(n) For Factor Annotation:

META: Not applicable. DATA: At most n factors would be proposed in a single round. And each of them needs m times annotations by LLMs for all samples. So it is O(nm). COAT: At most n factors would be proposed during all rounds. And each of them needs m times annotations by LLMs for all samples. So it is O(nm). For Causal Discovery:

Pair-wise reasoning by LLMs: O(\(n^{2}\)) COAT: LLM is not involved. The computational cost of it depends on the numeric methods. The FCI algorithm used by COAT has a time complexity that goes exponentially with n. Learning the causal graph over a large number of nodes effectively is still an open problem in causal discovery literature.

### Resources

We utilized a system comprising two Intel Xeon E5-2630v4 processors with 2.2GHz, two NVIDIA Tesla P40 GPUs, and 256 GB of memory. For conversations with large language models (LLMs), we leveraged the poe.com platform, while annotations were facilitated using the OpenAI API and the Mistral API.

Figure 27: Ground truth and faithful (via FCI algorithm) causal graphs in Neuropathic.

Figure 26: Illustration of examples in Neuropathic.

Figure 31: Causal graphs with Mistral-Medium in Neuropathic.

Figure 28: Causal graphs with GPT 4 in Neuropathic.

Figure 30: Causal graphs with LLaMA-2-70b in Neuropathic.

Figure 29: Causal graphs with GPT 3.5 in Neuropathic.

COAT with Different Causal Discovery Algorithm

## Appendix G Ablation Study

**Group Size in Prompt.** In the COAT prompt, several samples are given and grouped by the values of the target variable. The samples in each group are randomly selected to a fixed number (like 3 samples per group). Empirically, we keep it to be 3 throughout all experiments (sometimes smaller than 3 if samples are not enough). In practice, it is mainly constrained by the LLM's context length.

**The Number of Clusters.** When constructing feedback, we first use clustering to separate the dataset and then find the cluster where the target variable is not explained well by current factors (This is a heuristic for the problem in line 191). Empirically, we set the number of clusters to be one plus the number of current factors.

We conduct ablation studies of COAT with GPT-4 using different hyperparameters:

As shown in Table 8, one can observe that COAT is not sensitive to these hyperparameters and performs robustly well than the baselines under different hyperparameter setups.

**Prompt Template.** We conduct an ablation study with a different prompt template following [101]:

\begin{table}
\begin{tabular}{l l|c c c c c} \hline \hline \multirow{2}{*}{LLM} & \multirow{2}{*}{Method} & \multicolumn{5}{c}{Factor Proposal} \\  & & PA & AN & OT & Acc & F1 \\ \hline \multirow{3}{*}{GPT 4} & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & Data & 2 & 2 & 0 & 0.95 & 0.50 \\  & COAT & 3 & 6 & 3 & 0.96 & 0.80 \\  & Data (LiNGAM) & 3 & 3 & 0 & 0.96 & 0.67 \\  & COAT (LiNGAM) & 3 & 6 & 4 & 0.95 & 0.75 \\ \hline \multirow{3}{*}{GPT 3.5} & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & Data & 3 & 5 & 4 & 0.94 & 0.67 \\  & COAT & 3 & 5 & 2 & 0.96 & 0.77 \\  & Data (LiNGAM) & 2 & 3 & 4 & 0.91 & 0.46 \\  & COAT (LiNGAM) & 3 & 5 & 4 & 0.94 & 0.67 \\ \hline \multirow{3}{*}{LLAMA2-70b} & Meta & 2 & 4 & 5 & 0.91 & 0.53 \\  & Data & 3 & 3 & 1 & 0.95 & 0.60 \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\  & Data (LiNGAM) & 3 & 4 & 4 & 0.92 & 0.57 \\  & COAT (LiNGAM) & 3 & 6 & 4 & 0.95 & 0.75 \\ \hline \multirow{3}{*}{Mistral-Med} & Meta & 3 & 6 & 3 & 0.96 & 0.80 \\  & Data & 3 & 3 & 2 & 0.94 & 0.66 \\ \cline{1-1}  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\ \cline{1-1}  & Data (LiNGAM) & 3 & 3 & 3 & 0.92 & 0.50 \\ \cline{1-1}  & COAT (LiNGAM) & 3 & 6 & 3 & 0.96 & 0.80 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Causal discovery results in Neuropathic. PA, AN, and OT refer to the parents, ancestors, and others, respectively. Accuracy and F1 measure the recovery of the causal ancestors.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Method & Cluster Size & Group Size & MB & NMB & OT & Recall & Precision & F1 \\ \hline META & - & - & 2.67\(\pm\)0.94 & 0.67\(\pm\)0.47 & 2.33\(\pm\)0.47 & 0.53\(\pm\)0.19 & 0.46\(\pm\)0.08 & 0.49\(\pm\)0.13 \\ DATA & - & 3 & 3.00\(\pm\)0.00 & 0.33\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.60\(\pm\)0.00 & 0.92\(\pm\)0.12 & 0.72\(\pm\)0.04 \\ COAT & len(factor)+1 & 3 & 4.00\(\pm\)0.82 & 0.33\(\pm\)0.47 & 0.00\(\pm\)0.00 & 0.80\(\pm\)0.16 & 0.93\(\pm\)0.09 & 0.85\(\pm\)0.11 \\ COAT & len(factor)+1 & 1 & 4.67\(\pm\)0.58 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.93\(\pm\)0.12 & 1.00\(\pm\)0.00 & 0.96\(\pm\)0.06 \\ COAT & 2 & 3 & 3.67\(\pm\)1.53 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.73\(\pm\)0.31 & 1.00\(\pm\)0.00 & 0.82\(\pm\)0.22 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results about Ablation Study on Hyperparameters.

* We put the task description (also the format instructions) in the beginning [System] part, and we put samples in the last [Data] part of the prompt.
* The markdown grammar is replaced by blankets to represent headings, like [System].

[Data], and [Groups with Y=1]...
* 3 COAT iterations are performed, which is aligned with the original experimental setup.

\begin{tabular}{l c c c c c c} \hline \hline Model & MB & NMB & OT & Recall & Precision & F1 \\ \hline GPT-4 & 4 & 0 & 0 & 0.80 & 1.00 & 0.89 \\ GPT-3.5-Turbo & 4 & 0 & 0 & 0.80 & 1.00 & 0.89 \\ Mistral-Medium & 3 & 0 & 0 & 0.60 & 1.00 & 0.75 \\ \hline \hline \end{tabular}

In Table 9, we observe that COAT is robust to the choice of templates, rejects unexpected factors (zero NMB and OT), and keeps a high precision.

\begin{table} \end{table}
Table 9: COAT with changed prompt template 

[MISSING_PAGE_FAIL:43]

EvaluationThe ground truth is not directly available to this dataset. Therefore, each factor in the final result will be evaluated by searching the related literature and will be explained by human interpretation.

### Result and Discussion

Result interpretationAs shown in Fig. 35, there are two visual factors that appeared in the final causal graph: _contrast enhancement_ and _mass effect_. The first factor (contrast enhancement) is about whether the bright part is uniform or heterogeneous in the sample image. Another factor (mass effect) is about whether the tumor tissue has caused significant displacement of normal brain structures. One may refer to Fig. 36 to see detailed descriptions of these two factors.

Justification for the final factorsWe verify the proposed factors by the medical literature [102, 103, 104, 105], as shown Fig. 37. Note that these papers are searched according to keywords in factor descriptions and, therefore, can exclude some relevant studies. Therefore, the conclusions of this preliminary exploration should be considered a reference point for further in-depth validation by domain experts.

The factor description for _contrast enhancement_ directly matches with the related paper (the first two papers displayed in Fig. 37). In addition, this can be visually checked in Fig. 33. Therefore, we believe it is a good factor.

The second factor, _mass effect_, may not directly match the papers. It is pointed out in the last two papers displayed in Fig. 37 that the two tumors have different axial locations and thus influence the magnitude of the displacement of tissues. Both the keywords "axial location" and "displacement" have occurred in its factor description.

Figure 33: The initial input to COAT in the Brain Tumor case study. Each row contains 5 samples randomly selected from one category (top-down: glioma, meningioma, and no tumor).

## Appendix J Case Study on Stock News

In this section, we utilize COAT to explore time series data with text.

### The Stock News Dataset

DatasetThe dataset consists of the stock value of Microsoft (MSFT) from 2006 to 2009 and its news summary (only include news in _the New York Times_). This is a subset of an open Kaggle dataset (kaggle/stock-price-and-news-realted-to-it). Each sample is one trading day with the company's close stock price and news. The target value is the future return rate, and we are curious about factors in the related news. We fed data during the first 200 trading days to COAT, and we used the following 400 trading days for evaluation.

Data processingThe future return rate is calculated by close prices:

\[r_{t}=\frac{\text{close}_{t+4}}{\text{close}_{t}}-1.\] (24)

The target value is a binary variable according to the return rate

\[Y_{t}=\mathbf{1}_{\boldsymbol{\tau}_{t}>0},\] (25)

where \(\mathbf{1}_{A}\) is the indicator function. News fed to COAT are grouped according to the value of \(Y_{t}\). To keep the whole prompt within the context limit, only 3 samples would be randomly included in each group.

Factor processingGiven one proposed factor, denote its annotated value at the \(t\)-th trading day as \(a_{t}\in\{-1,0,1\}\). Each factor is rolling averaged over the past \(M\) days:

\[S_{t}=\frac{1}{M}\sum_{t-M\geq i\geq t}a_{i}.\] (26)

Figure 34: Illustration of the prompt for the Brain Tumor case study. It contains instructions about understanding the combined input picture.

Figure 35: Final causal graph by COAT in the Brain Tumor case study.

Each variable, including return rate, is normalized by the rolling standard deviation over the past \(M\) days:

\[F_{t}=\frac{1}{\text{Sd}\left(\left\{S_{i}\right\}_{t-M\geq i\geq t}\right)+1}S_{t}.\] (27)

An example of a processed factor can be seen in Fig. 38.

### Result and Discussion

Result Causal GraphAs shown in Fig. 39, four factors are identified in the final causal graph to form a Markov blanket of return rate: _Product Focus_, _Legal and Regulatory Issues_, _Market Strategy_, and _Innovation and Technology Focus_. One can refer to Fig. 40 for more detailed descriptions of these factors. One factor (_Innovation and technology focus_) is identified as a possible cause of the return rate; this matches the nature of the company's type and reflects people's expectation of the company to keep creating innovative computer software. It is also interesting to see that COAT captures the structure between factors and _market strategy_, where _product focus_ and _legal and regulatory issues_ are identified as potential causes. It also implies the existence of a latent confounder between _market strategy_ and _return rate_ that may not be significantly reflected in news text.

Figure 37: Medical literature about the brain tumor. Searched according to keywords in factor descriptions.

Figure 36: Detailed descriptions of these two final factors in the Brain Tumor case study. These descriptions were also directly fed to LLMs for factor annotation on images.

Evaluation by Trading StrategiesFor each factor \(\{F_{t}\}\) processed in Eq. 27, we establish a trading strategy on it to see the performance in the out-of-sample trading days. At each trading day \(t\), we fit the following model

\[r_{t}=\alpha+\beta F_{t},\] (28)

using samples \(\{r_{i},F_{i}\}_{t-200\leq i<t}\). Note that \(r_{t}\) is the future return rate defined in Eq. 24 and is not available on this day. After the estimation, we make decision based on \(\hat{r}_{t}:=\hat{\alpha}+\hat{\beta}F_{t}\). If \(\hat{r}_{t}>0\), we _go long_ with 1 unit capital, i.e., purchase that amount of stock, the gain would be gain would be \(r\times 1\) units capital. If \(\hat{r}_{t}<0\), we _go short_ with 1 unit capital, i.e., borrow and sell that amount of stocks immediately and buy them back next time, the gain would be \(-r\times 1\) units capital.

To align with the definition in Eq. 24, we make trading decisions every \(4\) trading day. We introduce an additional _Buy and Hold_ baseline to always go long one unit capital. The trading evaluation is after the \(200\)-th day and thus has no overlapping with data fed to COAT.

The cumulative return plot is shown in Fig. 41, and the metrics commonly used in economic literature [107, 108, 109] are shown in Table 11. One important metric to see a trading strategy's effectiveness is the sharp ratio: a measure of risk-adjusted return, showing the excess return (over the risk-free rate, we set it to be \(2\%\)) per unit of standard deviation. A higher Sharpe ratio indicates better performance per unit of risk. We see that the _Innovation and Technology Focus_ factor yields the highest sharp ratio and outperforms other non-causal factors.

Figure 39: The final causal graph by COAT in the Stock News case study

Figure 38: Example of one processed factors _Innovation and Technology Focus_ during the first 200 trading days. The red star marks the highest value. In June 2007, there was a discussion about future competition in desktop operating systems and the trend towards web-based services [106].

[MISSING_PAGE_EMPTY:48]

[MISSING_PAGE_FAIL:49]

, level, region) to help define factors. For example, in Lst. 1, we define the target variable with the help of this function. The _measurement_ specifies the climate variables like precipitation rate or temperature; the _level_ specifies the vertical location, like surface or specific pressure level; the region is a rectangle about the area concerned. This function would output a single time series about the measurement on the specified level averaged within the specified region. Note that the target variable is the only factor defined by humans, all other factors are defined by code from LLMs.

Feedback ConstructionThe dataset is in a special format; we can only draw a tabular subset of data from it in this setting. Therefore, it is difficult to make groups on samples like what we did in the previous cases. Instead, we construct feedback with some summary information. As shown in Fig. 43, two types of information are included in the feedback: _intermediate causal graph_ and _OLS regression result_ of factors in the current estimation of a Markov blanket on the target variable. These two types of information can be drawn naturally from the COAT's intermediate results and are not complex, so LLMs can always access them and easily understand them. Intuitively, these factors would help LLMs better understand existing factors and propose further possible factors based on the provided dataset and its knowledge.

### Causal Graph and Discussion

Result InterpretationIn this case study, we are treating non-stationary climate data, while assuming the causal structure is invariant. Under this assumption, the non-stationarity is actually helpful for causal structure learning [51, 114]. To this end, we utilize the CD-NOD algorithm [51] to fully utilize the changing causal modules for better identifiability. CD-NOD would first identify the non-stationary nodes, whose conditional distribution given the causal parents are changing with time, and then use them for better causal structure recovery. Four factors are identified to be non-stationary, as shown in Fig. 44.

We have adjusted the nodes' names, shapes, and colors for better visualization. There are 14 nodes in total, with 13 factors identified by COAT. Each factor is a time series about a certain climate measurement above a specific level averaged over a specific region. For simplicity, we only considered instantaneous causal relations among those time series.

There are three regions identified to be relevant to the ENSO phenomenon:

* **Equatorial Pacific Region** (Orange Nodes). This region (5N-5S, 120W-280W) is one of the most active places about ENSO. It becomes significantly warm during the El Nino phase and becomes significantly cold during the La Nina phase [115].
* **Nino3 Region** (Blue Nodes). This region (5N-5S, 150W-90W) is one of the most classical regions to monitor the El Nino events by scientists. It is also used by humans to construct the target variables [110].

* **South American Coastal Region**\((\text{Green Nodes})\)**.** This region (0N-20S, 80W-60W) includes the Peruvian coastal up-welling system, and is noticed to be relevant to the ENSO cycle [116].

Some insights are delivered by paths in the output causal graph:

* Sea level Pressure. This factor is about the sea-level pressure on the equatorial Pacific region. The pressure gradient would influence the movement of warm water, and thus influence the sea surface temperature (SST) change [41]. In addition, pressure can influence the water evaporation and thus regulate through the water circulation. This also matches another indirect path Sea level Pressure \(\rightarrow\) Sensible Heat Net Flux \(\rightarrow\) Volumetric Soil Moisture \(\rightarrow\) Cloud Cover \(\rightarrow\) SST Change.
* Momentum Flux, V-compoent. This factor is about the vertical movement of air. It is crucial in driving atmospheric convection [41], and it is related to the Walker Circulation, which is an important component in the ENSO system[42]. Also, it could influence the change in sea level pressure and indirectly influence the SST change.
* Cloud Cover. The factor is the fraction of the sky covered by clouds in the NINO3 region. It could influence the SST Change through solar radiation as well as water circulation. It is confirmed to have a significant correlation [43] with ENSO events and plays an important role in the atmospheric circulation and hydrological cycle [44].
* Soil Temperature. This might be a novel hypothesis proposed by COAT, since we found no sufficient research to confirm this point to the best of our knowledge. This causal graph has also suggested two possible indirect mechanisms: (1) through _Volumetric Soil Moisture_

Figure 43: The prompt with feedback based on previous loop in the ENSO case study

and _Cloud Cover_; and (2) through _Convective Precipitation Rate_ and _Sea Level Pressure_. Therefore, this finding encourages more serious investigations of these hypotheses.

Discussion on Additional assumptionThe constructed feedback contains the causal structure among existing factors and their OLS regression analysis on the target variable. By doing so, some additional assumptions are implicitly made. For example, it assumes a linear relation between factors and the target variables. More detailed theoretical analysis is out of the scope of this case study, and we left it for future work.

Discussion on HallucinationAlthough we have clarified how to use tools in prompts, LLM can sometimes propose unsupported data requests. For example, only certain pressure levels are supported in the dataset. To this end, we include a Python code in the prompt, as listed in Lst. 3. This function can check whether the LLM's request is supported. We ask LLM to run this function in its code interpreter to make sure the proposed factors are valid.

Figure 44: The final causal graph found by COAT in the ENSO case study

* [1]importpandas
* [2]defcheck_choice(measurement,level,detailed_level=None):
* [3]meta=pd.read_csv('Measurement_level_meta.csv')
* [4]measurement_set=set(meta.Measurement.values)
* [5]measurement_level_map=meta.groupby(['Measurement']).agg(("Level":lambda:set(x)))
* [6]detailed_level_range={
* [7]Pressure Levels':[1000,975,950,925,900,850,800,750,700,650,600,550,500,450,400,350,300,250,200,150,100,70,50,30,20,10,5,1], 'Multiple Subsurface Levels':[0,10,40,100],
* [8]Fixed Levels AbovetheSurface':[12,20,30,50,80,100,150,200,250,300,500],
* [9]'Isentropic Levels':[300,330,350]
* [10]}
* [11]ifmeasurementnotinmeasurement_set:returnFalse,f"measurementerror"
* [12]iflevelnotinmeasurement_level_map.loc[measurement,'Level']
* [13]returnFalse,f"levelerror,onlyallowed:{measurement_level_map.loc[measurement,'Level']}"
* [14]iflevelindetailed_level_range.keys():ifdetailed_levelnotindetailed_level_range[level]:returnFalse,f"needdetailedlevelin{
* [15]detailed_level_range[level]}"
* [16]returnTrue,None

Listing 3: LLM is required to use this function to overcome hallucination

## Appendix L Broader Impacts

This work focuses on fully leveraging the rich knowledge learned by LLMs during pre-training to facilitate causal discovery from unstructured data, with the hope of empowering broader applications and social benefits. Besides, this paper does not raise any ethical concerns. This study does not involve any human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See the end of the introduction part. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Sec. 2.5 and Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: see Appendix D.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: https://causalcoat.github.io/Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Sec. 3.1, Sec. 4.1 and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Standard Deviations are included. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix E.9. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Authors follow the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See the Appendix L. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No sensitive or risky content is included in this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix H. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [Yes] Justification: See Appendix H. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.