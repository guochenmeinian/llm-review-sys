# LoRA-GA: Low-Rank Adaptation with Gradient Approximation

Shaowen Wang

wangsw23@mails.tsinghua.edu.cn &Linxi Yu

yulx23@mails.tsinghua.edu.cn

&Jian Li

lijian83@mail.tsinghua.edu.cn

Tsinghua University

Beijing, China

Corresponding author

###### Abstract

Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (**Low Rank** **A**daptation with **G**radient **A**pproximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at code.

## 1 Introduction

Fine-tuning large language models (LLMs) is essential for enabling advanced techniques such as instruction fine-tuning [1], reinforcement learning from human feedback (RLHF) [2], and adapting models to specific downstream applications. However, the computational and storage costs associated with full fine-tuning are prohibitively high, particularly as model sizes continue to grow. To address these challenges, methods of Parameter-Efficient Fine-Tuning (PEFT) (see e.g., [3]), such as Low-Rank Adaptation (LoRA) [4], have emerged and gained significant attention.

Instead of updating the parameters of the model directly, LoRA incorporates auxilary low-rank matrices \(B\) and \(A\) into the linear layers of models (such as the \(Q,K,V\), and \(O\) matrices in a self-attention block [5]), while keeping the original layer weights \(W\) fixed. The modified layer is represented as \(y=(W+\eta BA)x\), where \(x\) is the input of that layer, \(y\) is the output, and \(\eta\) is the scaling factor. This approach significantly reduces the number of parameters that need to be fine-tuned, thereby lowering the computational and memory costs at each step.

Despite these benefits, extensive empirical evidence (see e.g., [6; 7; 8; 9]) shows that LoRA converges significantly slower compared to full finetune. This slower convergence often increases overall computational costs (measured in Floating Point Operations) and can sometimes lead to worse test performance. In our experiments, we typically observe that LoRA requires 5-6x more iterations and FLOPs to reach the same performance as full fine-tuning under the same learning rate, as shown in Figure 1.

To study the cause of slow convergence, we perform an in-depth investigation of the initialization strategy of LoRA's adapter weights. It is known that fine-tuning pretrained models using the same objective (e.g., language modeling) often converges faster than re-initializing new parameters (e.g., a classification head) [10]. This observation leads us to question whether the slow convergence of vanilla LoRA might be attributed to the default random initialization of adapter weights (LoRA initializes \(A\) using Kaiming initialization [11] and sets \(B\) to zero [4]). In our experiments, we find that different initialization strategies for LoRA can significantly impact the results, and its default initialization is suboptimal.

In pursuit of a convergence rate comparable to full fine-tuning, we aim for initialization so that the update of \(BA\) matches the update of \(W\) closely. Previous work suggests that gradient descent operates in a low-dimensional subspace [12; 13]. If we can closely approximate the gradients of the full model at the initial step, subsequent steps can also be approximated, potentially accelerating the convergence of LoRA.

To this end, we introduce a novel initialization method, LoRA-GA (**Low Rank Gradient Approximation**). By initializing \(A_{\text{init}}\) and \(B_{\text{init}}\) with the eigenvectors of the full gradient matrix, the gradient of the low-rank product \(BA\) aligns with the direction of the gradient of the full weight matrix \(W\). Mathematically, we aim to ensure that:

\[\Delta(BA)\approx\zeta\Delta W,\quad\text{for some non-zero positive constant }\zeta.\]

**Our contributions can be summarized as follows:**

**1.**: We propose LoRA-GA, a novel initialization method for LoRA that accelerates convergence by approximating the gradients of the low-rank matrices with ones of the full weight matrix.

Figure 1: **(Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LoRA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.**

**2.** We identify the scaling factor under non-zero initialization, which ensures the variance of adapter outputs is invariant to the rank of the adapter and the dimension of the input.

**3.** We validate LoRA-GA through extensive experiments, demonstrating significant performance improvements and faster convergence compared to vanilla LoRA. Specifically, LoRA-GA outperforms LoRA by 5.69% on the GLUE [14] subset with T5-Base [15], and by 0.34, 11.52%, and 5.05% on MT-bench [16], GSM8K [17], and HumanEval [18] with Llama 2-7B [19], respectively, while achieving up to 2-4 times faster convergence.

## 2 Related Work

### Initialization

The significance of maintaining variance stability during initialization has been widely acknowledged to prevent the occurrence of diminishing or exploding phenomena. Xavier initialization [20] ensures stability in both the forward and backward passes of a network under a linear activation function. He initialization [11] extends this solution to networks using ReLU activation. Distinct from these, LSUV initialization [21] selects a mini-batch of data, performing a forward pass to determine the output variance, and subsequently normalizing it to ensure stability. Tensor program (see e.g., [22]) has emerged as a powerful technique for tuning various hyperparameters, including the initialization, for large models.

### Parameter-Efficient Fine-Tuning (PEFT)

To fine-tune increasingly large language models within limited hardware resources, researchers have developed various Parameter-Efficient Fine-Tuning (PEFT) methods. Adapter-based methods [23; 24; 25; 26] incorporate new layers into existing model layers. While fine-tuning only these inserted layers significantly reduces resource consumption and requires much fewer parameters, this approach introduces additional latency during both forward and backward passes. Soft Prompt-based methods [10; 27; 28; 29; 30] prepend learnable soft tokens to the model's input to adapt the model to specific tasks. This approach effectively leverages the pre-trained model's capabilities, requiring only appropriate prompts for task adaptation, though it incurs computational overhead during inference. More broadly, GaLore [31] applies low-rank gradients to parameter updates for memory efficiency during training. While this approach is highly expressive and performant, it requires storing complete model checkpoints, consuming more storage than other PEFT methods.

### LoRA's Variants

LoRA is one of the most popular PEFT methods that introduces the product of low-rank matrices alongside existing layers to approximate weight changes during fine-tuning. Several methods have been proposed to improve the structure of LoRA. AdaLoRA [32] dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget. DoRA [8] enhances the model's expressiveness by adding learnable magnitudes to the direction adjustments made by low-rank matrix products. Additionally, LoHA [33] and LoKr [34] employ Hamiltonian and Kronecker products, respectively.

Despite these advancements, vanilla LoRA remains the most popular method due to its robust library and hardware support. Therefore, improving LoRA without altering its structure and at a low cost is crucial. Several recent methods focus on this aspect. ReLoRA [35] suggests periodically merging learned adapters into the weight matrices to enhance LoRA's expressibility. LoRA+ [36] proposes using different learning rates for the two matrices in LoRA to improve convergence. rsLoRA [37] introduces a new scaling factor to make the scale of the output invariant to rank. Although our stable scale approach appears similar to rsLoRA, rsLoRA assumes \(BA=0\) initialization, making \(r\) invariant to the update \(\Delta BA\). In contrast, our stable scale ensures that non-zero initialized \(BA\) remains invariant to both rank and input dimension from the start.

Recently, PiSSA [38] proposes to initializing \(A\) and \(B\) to approximate the original matrix \(W\), by performing SVD on \(W\). Our method, however, is based on a very different idea, that is to approximate the gradient of \(W\), which involves performing SVD on sampled gradients and properly scaling the initialized matrices, as detailed in Section E.

Methods

In this section, we analyze the initialization of LoRA and introduce our method, LoRA-GA. LoRA-GA consists of two key components: (i) approximating the direction of the gradient of full finetune and (ii) ensuring rank and scale stability in the initialization process. We examine each component and subsequently present their integration within LoRA-GA.

### Review of Vanilla LoRA

Structure of LoRABased on the hypothesis that the updates of fine-tuning are low-rank [13], LoRA [4] proposes to use the product of two low-rank matrices to represent the incremental part of the original matrix \(W\). Here, \(W\) is the weight matrix of a linear layer in the model. For example, in transformers, it could be the \(Q,K,V\), or \(O\) matrices of the self-attention layer or the weight matrix in the MLP layer. Specifically, LoRA has the following mathematical form:

\[W^{\prime}=W_{0}+\Delta W=W_{0}+\frac{\alpha}{r}BA:=W_{0}+\eta BA\]

where \(W^{\prime},W_{0}\in\mathbb{R}^{m\times n}\), \(B\in\mathbb{R}^{m\times r}\), and \(A\in\mathbb{R}^{r\times n}\), with \(r\ll\min(m,n)\). \(W_{0}\) is the pre-trained weight matrix, remains frozen during the fine-tuning process, while \(A\) and \(B\) are trainable.

Initialization of LoRAUnder LoRA's default initialization scheme [4; 39], matrix \(A\) is initialized using Kaiming uniform [11], while matrix \(B\) is initialized with all zeros. Consequently, \(BA=0\) and \(W_{0}^{\prime}=W_{0}\), ensuring that the initial parameters are unchanged.

If the additional term \(\Delta W=\eta BA\) is initially non-zero (e.g., [38]), the frozen parameter can be adjusted to ensure the initial parameters unchanged. This can be expressed as:

\[W^{\prime}=(W_{0}-\eta B_{\mathrm{init}}A_{\mathrm{init}})+\eta BA:=W_{ \mathrm{frozen}}+\eta BA\]

where \(W_{\mathrm{frozen}}=W_{0}-\eta B_{\mathrm{init}}A_{\mathrm{init}}\) is frozen, and \(B\) and \(A\) are trainable in this case.

### Gradient Approximation

Our goal is to ensure that the first-step update \(\Delta(\eta BA)\) approximate the direction of the weight update \(\Delta W\), i.e., \(\Delta(\eta BA)\approx\zeta\Delta W\) for some non-zero positive constant \(\zeta\). We will discuss how to choose \(\zeta\) in Section 3.3 and one can treat \(\zeta\) as a fixed constant for now.

Consider a gradient descent step with learning rate \(\lambda\), the updates for \(A\) and \(B\) are \(\Delta A=\lambda\nabla_{A}\mathcal{L}\left(A_{\mathrm{init}}\right)\) and \(\Delta B=\lambda\nabla_{B}\mathcal{L}\left(B_{\mathrm{init}}\right)\), respectively. Assuming learning rate \(\lambda\) is small, the update of \(\eta BA\) at the first step can be expressed as:

\[\eta(\Delta BA_{\mathrm{init}}+B_{\mathrm{init}}\Delta A)=\eta\lambda[\nabla_{ B}\mathcal{L}\left(B_{\mathrm{init}}\right)A_{\mathrm{init}}+B_{\mathrm{init}} \nabla_{A}\mathcal{L}\left(A_{\mathrm{init}}\right)]\]

To measure its approximation quality of scaled the update of the weights in full finetune \(\zeta\Delta W=\zeta\lambda\nabla_{W}\mathcal{L}\left(W_{0}\right)\), we use the Frobenius norm of the difference between these two updates as a criterion:

\[\left\|\eta(\Delta BA_{\mathrm{init}}+B_{\mathrm{init}}\Delta A)- \zeta\lambda\nabla_{W}\mathcal{L}\left(W_{0}\right)\right\|_{F}\] (1) \[= \lambda\left\|\eta\nabla_{B}\mathcal{L}\left(B_{\mathrm{init}} \right)A_{\mathrm{init}}+\eta B_{\mathrm{init}}\nabla_{A}\mathcal{L}\left(A_{ \mathrm{init}}\right)-\zeta\nabla_{W}\mathcal{L}\left(W_{0}\right)\right\|_{F}\]

**Lemma 3.1**.: _Suppose the loss function is \(\mathcal{L}\) and \(y=W^{\prime}x=(W_{0}+\eta BA)x\), where \(y\) is the output of a layer and \(x\) is the input, the gradients of \(A\) and \(B\) are linear mappings of the gradient of \(W^{\prime}\):_

\[\nabla_{A}\mathcal{L}=B^{T}\nabla_{W^{\prime}}\mathcal{L},\quad\nabla_{B} \mathcal{L}=\left(\nabla_{W^{\prime}}\mathcal{L}\right)A^{T}\]

_Remarkably, \(\nabla_{W^{\prime}}\mathcal{L}\) in LoRA and \(\nabla_{W}\mathcal{L}\) in full fine-tuning are equal at the beginning of the training._

By substituting the gradients in Lemma 3.1 into Equation 1, we can rewrite the criterion as follows:

\[\lambda\left\|\eta^{2}\nabla_{W^{\prime}}\mathcal{L}\left(W_{0}\right)\cdot A_ {\mathrm{init}}^{T}A_{\mathrm{init}}+\eta^{2}B_{\mathrm{init}}B_{\mathrm{init} }^{T}\cdot\nabla_{W}\mathcal{L}\left(W_{0}\right)-\zeta\nabla_{W}\mathcal{L} \left(W_{0}\right)\right\|_{F}\] (2)

This criterion evaluates how well the adapter's gradient approximates the direction of the gradient of full fine-tuning, and minimizing it brings the gradient of LoRA closer to that of full fine-tuning with a scaling factor \(\zeta\):

\[\min_{A_{\mathrm{init}},B_{\mathrm{init}}}\left\|\eta^{2}\nabla_{W}\mathcal{L} \cdot A_{\mathrm{init}}^{T}A_{\mathrm{init}}+\eta^{2}B_{\mathrm{init}}B_{ \mathrm{init}}^{T}\cdot\nabla_{W}\mathcal{L}-\zeta\nabla_{W}\mathcal{L}\right\| _{F}\] (3)

**Theorem 3.1**.: _For the optimization problem in Equation 3 with given \(\zeta\), if the Singular Value Decomposition (SVD) of \(\nabla_{W}\mathcal{L}\) is \(\nabla_{W}\mathcal{L}=USV^{T}\), the solution is:_

\[B_{\mathrm{init}}=\frac{\sqrt{\zeta}}{\eta}U_{I_{A}},\quad A_{\mathrm{init}}= \frac{\sqrt{\zeta}}{\eta}V_{I_{B}}^{T},\text{ such that }|I_{A}|=|I_{B}|=r,\,I_{A}\cup I_{B}=\{i\mid 1 \leq i\leq 2r,i\in\mathbb{N}\}\]

_where \(I_{A}\) and \(I_{B}\) are index sets._

Theorem 3.1 provides an appropriate initialization scheme for \(A_{\mathrm{init}}\) and \(B_{\mathrm{init}}\) given a specific \(\zeta\). The selection of \(\zeta\), which influences the scaling of the update \(\eta BA\), will be discussed in the following section.

### Scale Stability

Inspired by rsLoRA citekalajdzievski2023rank and the Kaiming initialization [11], we define the following notions of stability:

**Definition 3.1**.: _When \(d_{out},d_{in},r\rightarrow\infty\), an adapter \(\eta BA\) exhibits two distinct types of scale stabilities:_

_1. **Forward stability**: If the inputs to the adapter are independently and identically distributed (i.i.d.) with 2nd moment \(\Theta_{r,d_{out},d_{in}}\left(1\right)\), then the 2nd moment of the outputs remains \(\Theta_{r,d_{out},d_{in}}\left(1\right)\)._

_2. **Backward stability**: If the gradient of the loss with respect to the adapter outputs is \(\Theta_{r,d_{out},d_{in}}\left(1\right)\), then the gradient with respect to the inputs remains \(\Theta_{r,d_{out},d_{in}}\left(1\right)\)._

**Theorem 3.2**.: _Given the initialization proposed in Theorem 3.1, assume that the orthogonal vectors in \(A_{\mathrm{init}}\) and \(B_{\mathrm{init}}\) are randomly selected from the unit spheres in \(\mathbb{R}^{d_{in}}\) and \(\mathbb{R}^{d_{out}}\) with the constraint that the vectors are orthogonal to each other, and \(\eta=\Theta_{r,d_{out},d_{in}}\left(1/\sqrt{r}\right)\) as suggested by rsLoRA [37]. Under these conditions, the adapters are forward scale-stable if \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{d_{out}/r^{2}}\right)\) and backward scale-stable if \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{d_{in}/r^{2}}\right)\)._

Similar to the results obtained from Kaiming Initialization [11], we observe that either \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{d_{out}/r^{2}}\right)\) or \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{d_{in}/r^{2}}\right)\) work well independently. For all models presented in this paper, either form ensures convergence. Consequently, for all subsequent experiments, we adopt \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{d_{out}/r^{2}}\right)\).

**Remark**.: _We would like to remark that the scaling factor proposed in this subsection proves to be beneficial primarily when one adopts the learning rate typically used in full-finetuning (e.g., \(1e-5\)), since as LoRA-GA attempts to approximate the updates of full-finetuning. However, recent research [9] suggests that LoRA with default initialization performs much better with larger learning rates. Furthermore, tensor program analysis [40; 22] indicates that higher learning rates should be paired with smaller initialization magnitudes. Therefore, we recommend decreasing or omitting the scaling factor when training using larger learning rates (e.g., \(>1e-4\))._

### LoRA-GA Initialization

Combining the gradient approximation and stable scale components, we propose the LoRA-GA initialization method. First, we initialize \(A_{\mathrm{init}}\) and \(B_{\mathrm{init}}\) using the solution from Theorem 3.1. Then, we determine the scaling factor \(\zeta\) according to Theorem 3.2 to ensure rank and scale stability. Thus, based on Theorems 3.1 and 3.2, we propose a novel initialization method, LoRA-GA.

LoRA-GA :We adopt \(\eta=\frac{\alpha}{\sqrt{r}}\) and \(\zeta=\frac{\alpha^{2}}{\gamma^{2}}\sqrt{\frac{d_{out}}{r^{2}}}\), where \(\gamma\) is a hyperparameter. We define the index sets \(I_{A}=\{i\mid 1\leq i\leq r,i\in\mathbb{N}\}\) and \(I_{B}=\{i\mid r+1\leq i\leq 2r,i\in\mathbb{N}\}\). Denote the singular value decomposition (SVD) of \(\nabla_{W}\mathcal{L}\) as \(\nabla_{W}\mathcal{L}=USV^{T}\). The initializations are as follows:

\[A_{\mathrm{init}}=\frac{\sqrt[4]{d_{out}}}{\gamma}V_{[1:r]}^{T},\quad B_{ \mathrm{init}}=\frac{\sqrt[4]{d_{out}}}{\gamma}U_{[r+1:2r]},\quad W_{\mathrm{ init}}=W_{0}-\eta B_{\mathrm{init}}A_{\mathrm{init}}\]To save GPU memory during LoRA-GA initialization, we utilized a technique similar to [41]. By hooking into PyTorch's backward process, we compute the gradient for one layer at a time and discard the computed gradients immediately. This ensures that our memory usage remains at \(O(1)\) instead of \(O(L)\), where \(L\) is the number of layers. This approach allows the memory consumption during the initialization phase to be less than that during the subsequent LoRA finetuning phase. Our algorithm is shown in Algorithm 1. If the sampled batch size is large, we can also use gradient accumulation to save memory further, as shown in Algorithm 2.

```
0: Model \(f(\cdot)\) with \(L\) layers, parameters \(W\), sampled batch \(B=\{x,y\}\), LoRA rank \(r\), LoRA alpha \(\alpha\), loss function \(\mathcal{L}\), scale factor \(\gamma\)
0: Initialized parameters \(W\), \(\eta\), \(A\), \(B\)
1:\(\hat{y}\gets f(x,W)\)\(\triangleright\) Forward pass
2:\(\ell\leftarrow\mathcal{L}(y,\hat{y})\)
3:\(\eta\leftarrow\frac{\alpha}{\sqrt{r}}\)
4:for\(l=L,\ldots,1\)do
5: Compute \(\nabla_{W_{l}}\ell\)\(\triangleright\) Backward for one layer
6:\(d_{out},d_{in}\leftarrow\text{size}(W_{l})\)
7:\(U,S,V\leftarrow\text{svd}(\nabla_{W_{l}}\ell)\)
8:\(A_{l}\gets V_{[1:r]}\cdot\sqrt{d_{out}/\gamma}\)
9:\(B_{l}\gets U_{[r+1:2r]}\cdot\sqrt{d_{out}/\gamma}\)
10:\(W_{l}\gets W_{l}-\eta B_{l}A_{l}\)
11: Clear \(\nabla_{W_{l}}\ell\)\(\triangleright\) Gradient for this layer is not needed anymore
12:endfor
13:return\(W\), \(\eta\), \(A\), \(B\) ```

**Algorithm 1** LoRA-GA Initialization

## 4 Experiments

In this section, we evaluate the performance of LoRA-GA on various benchmark datasets. Initially, we assess Natural Language Understanding (NLU) capabilities using a subset of the GLUE dataset [14] with the T5-Base model [15]. Subsequently, we evaluate dialogue [16; 42], mathematical reasoning [17; 43], and coding abilities [18; 44] using the Llama 2-7B model [19]. Finally, we do the ablation study to prove the effectiveness of our method.

BaselinesWe compare LoRA-GA with several baselines to demonstrate its effectiveness:

**1.**_Full-Finetune_: Fine-tuning the model with all parameters, which requires the most resources.

**2.**_Vanilla LoRA_[4]: Fine-tuning the model by inserting a low-rank matrix product \(BA\) into linear layers. \(A\) is initialized using Kaiming initialization, while \(B\) is initialized to zero.

**3.**_LoRA Variants with Original Structure_: This includes several methods that retain the original LoRA structure:

- _rsLoRA_[37] introduces a new scaling factor to stabilize the scale of LoRA.

- _LoRA+_[36] updates the two matrices in LoRA with different learning rates.

- _PiSSA_[38] proposes performing SVD on the weight matrix \(W\) at the beginning of training and initializing \(A\) and \(B\) based on the components with larger singular values.

**4.**_LoRA Variants with Modified Structure_: This includes methods that modify the original LoRA structure:

- _DoRA_[8] enhances the model's expressiveness by adding learnable magnitudes.

- _AdaLoRA_[32] dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget.

### Experiments on Natural Language Understanding

Models and DatasetsWe fine-tune the T5-Base model on several datasets from the GLUE benchmark, including MNLI, SST-2, CoLA, QNLI, and MRPC. Performance is evaluated on the development set using accuracy as the primary metric.

Implementation DetailsWe utilize prompt tuning to fine-tune the T5-Base model on the GLUE benchmark. This involves converting labels into tokens (e.g., "positive" or "negative") and using the normalized probability of these tokens as the predicted label probability for classification. We provide the hyperparameters in Appendix D.1. Each experiment is conducted with 3 different random seeds, and the average performance is reported.

ResultsAs shown in Table 1, LoRA-GA consistently outperforms the original LoRA and other baseline methods, achieving performance comparable to full fine-tuning. Notably, LoRA-GA excels on smaller datasets such as CoLA and MRPC, demonstrating its ability to converge faster and effectively utilize limited training data.

### Experiment on Large Language Model

Models and DatasetsTo evaluate the scalability of LoRA-GA, we train Llama 2-7B on three tasks: _chat_, _math_, and _code_.

**1.**_Chat_: We train our model on a 52k subset of WizardLM [42], filtering out responses that begin with "As an AI" or "Sorry". We test our model on the MT-Bench dataset [16], which consists of 80 multi-turn questions designed to assess LLMs on multiple aspects. The quality of the responses is judged by GPT-4, and we report the first turn score.

**2.**_Math_: We train our model on a 100k subset of MetaMathQA [43], a dataset bootstrapped from other math instruction tuning datasets like GSM8K[17] and MATH [45], with higher complexity and diversity. We select data bootstrapped from the GSM8K training set and apply filtering. Accuracy is reported on the GSM8K evaluation set.

**3.**_Code_: We train our model on a 100k subset of Code-Feedback [44], a high-quality code instruction dataset, removing explanations after code blocks. The model is tested on HumanEval [18], which consists of 180 Python tasks, and we report the PASS@1 metric.

Implementation DetailsOur model is trained using standard supervised learning for language modelling. The loss for the input prompt is set to zero. Detailed hyperparameters can be found in Appendix D.2. Each experiment uses 3 different random seeds, and the average performance across these runs is reported.

ResultOur results, as summarized in Table 2, indicate that LoRA-GA outperforms or is comparable to other methods, including full-finetuning. Specifically, LoRA-GA achieves superior performance on both the GSM8K and Human-eval datasets, underscoring its effectiveness in handling tasks with higher complexity and diversity. On MT-Bench, LoRA-GA also demonstrates competitive performance, although it slightly trails behind DoRA. Nevertheless, LoRA-GA achieves this with fewer parameters and approximately 70% of the training time required by DoRA. Additionally, as illustrated in Figure 2 (Left), our method exhibits a significantly faster convergence rate compared to Vanilla LoRA, with convergence rates comparable to those of full-finetuning.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & **MNLI** & **SST-2** & **CoLA** & **QNLI** & **MRPC** & **Average** \\ Size & 393k & 67k & 8.5k & 105k & 3.7k & \\ \hline Full & \(86.33_{\pm 0.00}\) & \(94.75_{\pm 0.21}\) & \(80.70_{\pm 0.24}\) & \(93.19_{\pm 0.22}\) & \(84.56_{\pm 0.73}\) & \(87.91\) \\ LoRA & \(85.30_{\pm 0.04}\) & \(94.04_{\pm 0.11}\) & \(69.35_{\pm 0.05}\) & \(92.96_{\pm 0.09}\) & \(68.38_{\pm 0.01}\) & \(82.08\) \\ \hline PiSSA & \(85.75_{\pm 0.07}\) & \(94.07_{\pm 0.06}\) & \(74.27_{\pm 0.39}\) & \(93.15_{\pm 0.14}\) & \(76.31_{\pm 0.51}\) & \(84.71\) \\ rsLoRA & \(85.73_{\pm 0.10}\) & \(\mathbf{94.19}_{\pm 0.23}\) & \(72.32_{\pm 1.12}\) & \(93.12_{\pm 0.09}\) & \(52.86_{\pm 2.27}\) & \(79.64\) \\ LoRA+ & \(\mathbf{85.81}_{\pm 0.09}\) & \(93.85_{\pm 0.24}\) & \(77.53_{\pm 0.20}\) & \(93.14_{\pm 0.03}\) & \(74.43_{\pm 1.39}\) & \(84.95\) \\ \hline DoRA & \(85.67_{\pm 0.09}\) & \(94.04_{\pm 0.53}\) & \(72.04_{\pm 0.94}\) & \(93.04_{\pm 0.06}\) & \(68.08_{\pm 0.51}\) & \(82.57\) \\ AdaLoRA & \(85.45_{\pm 0.11}\) & \(93.69_{\pm 0.20}\) & \(69.16_{\pm 0.24}\) & \(91.66_{\pm 0.05}\) & \(68.14_{\pm 0.28}\) & \(81.62\) \\ \hline LoRA-GA & \(85.70_{\pm 0.09}\) & \(94.11_{\pm 0.18}\) & \(\mathbf{80.57}_{\pm 0.20}\) & \(\mathbf{93.18}_{\pm 0.06}\) & \(\mathbf{85.29}_{\pm 0.24}\) & \(\mathbf{87.77}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.

Effect of RankWe attribute the performance discrepancies on the GSM8K and Human-eval datasets, when compared to full-finetuning, primarily to the representational limitations imposed by the low-rank approximation. To address this, we experimented with higher ranks, specifically rank=32 and rank=128. Our findings reveal that LoRA-GA maintains stability across different rank settings and, in some cases, even surpasses full-finetuning performance. As shown in Figure 2 (Left), higher ranks with our initialization also result in loss curves that closely resemble those of full-finetuning.

### Ablation Study

We conducted ablation studies to evaluate the contributions of non-zero initialization, stable output, and gradient approximation in LoRA-GA using five distinct experimental settings. Details of each setting are provided in Table 3.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **MT-Bench** & **GSM8K** & **Human-eval** & **Average of GLUE** \\ \hline Full & \(5.56_{\pm 0.09}\) & \(54.20_{\pm 0.42}\) & \(19.87_{\pm 0.57}\) & \(87.91\) \\ LoRA & \(5.61_{\pm 0.10}\) & \(42.08_{\pm 0.04}\) & \(14.76_{\pm 0.17}\) & \(82.08\) \\ \hline Gaussian & \(5.62_{\pm 0.11}\) & \(38.21_{\pm 0.06}\) & \(14.76_{\pm 0.68}\) & \(81.88\) \\ + SO & \(5.72_{\pm 0.04}\) & \(42.81_{\pm 1.14}\) & \(15.55_{\pm 0.78}\) & \(82.28\) \\ + GA & \(5.48_{\pm 0.02}\) & \(46.65_{\pm 1.17}\) & \(16.15_{\pm 0.78}\) & \(82.54\) \\ LoRA-GA & \(5.95_{\pm 0.16}\) & \(53.60_{\pm 0.30}\) & \(19.81_{\pm 1.46}\) & \(87.77\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Performance of different settings in the ablation study. Results are shown for MT-Bench, GSM8K, and Human-eval on Llama 2 7b, as well as the average performance on a subset of GLUE on T5-Base. Detailed results can be found in Table 9.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & \(A\) Initialization & \(B\) Initialization & \(\eta\) \\ \hline LoRA & \(U\left(-\sqrt{\frac{3}{d_{in}}},\sqrt{\frac{3}{d_{in}}}\right)\) & 0 & \(\alpha/r\) \\ Gaussian & \(N(0,\frac{1}{d_{out}})\) & \(N(0,\frac{1}{d_{in}})\) & \(\alpha/r\) \\ +SO & \(\sqrt[4]{d_{out}}/\sqrt{\gamma}\cdot N(0,\frac{1}{d_{out}})\) & \(\sqrt[4]{d_{out}}/\sqrt{\gamma}\cdot N(0,\frac{1}{d_{in}})\) & \(\alpha/\sqrt{r}\) \\ +GA & \(V_{[1:r]}\) & \(U_{[r+1:2r]}\) & \(\alpha/r\) \\ LoRA-GA & \(V_{[1:r]}\cdot\sqrt[4]{d_{out}}/\sqrt{\gamma}\) & \(U_{[r+1:2r]}\cdot\sqrt[4]{d_{out}}/\sqrt{\gamma}\) & \(\alpha/\sqrt{r}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Initialization Methods and Corresponding Settings for Ablation Study. The table compares different initialization methods for LoRA and their settings for \(A\), \(B\), and \(\eta\). ”+SO” denotes stable output, scaling parameters appropriately to ensure stability. ”+GA” refers to gradient approximation, where \(A\) and \(B\) are initialized using orthogonal matrices derived from singular value decomposition.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **MT-Bench** & **GSM8K** & **Human-eval** \\ \hline Full & \(5.56_{\pm 0.09}\) & \(54.20_{\pm 0.42}\) & \(19.87_{\pm 0.57}\) \\ LoRA & \(5.61_{\pm 0.10}\) & \(42.08_{\pm 0.04}\) & \(14.76_{\pm 0.17}\) \\ \hline PiSSA & \(5.30_{\pm 0.02}\) & \(44.54_{\pm 0.27}\) & \(16.02_{\pm 0.78}\) \\ rsLoRA & \(5.25_{\pm 0.03}\) & \(45.62_{\pm 0.10}\) & \(16.01_{\pm 0.79}\) \\ LoRA+ & \(5.71_{\pm 0.08}\) & \(52.11_{\pm 0.62}\) & \(18.17_{\pm 0.52}\) \\ \hline DoRA & \(\mathbf{5.97}_{\pm 0.02}\) & \(53.07_{\pm 0.75}\) & \(19.75_{\pm 0.41}\) \\ AdaLoRA & \(5.57_{\pm 0.05}\) & \(50.72_{\pm 1.39}\) & \(17.80_{\pm 0.44}\) \\ \hline LoRA-GA & \(5.95_{\pm 0.16}\) & \(\mathbf{53.60}_{\pm 0.30}\) & \(\mathbf{19.81}_{\pm 1.46}\) \\ LoRA-GA (Rank=32) & \(5.79_{\pm 0.09}\) & \(55.12_{\pm 0.30}\) & \(20.18_{\pm 0.19}\) \\ LoRA-GA (Rank=128) & \(6.13_{\pm 0.07}\) & \(55.07_{\pm 0.18}\) & \(23.05_{\pm 0.37}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of fine-tuning Llama 2-7b using Full-FT and various LoRA variants, tested on MT-Bench, GSM8K, and Human-eval. LoRA-GA significantly outperforms Vanilla LoRA and approaches the performance of Full Finetune. Unless otherwise specified, the LoRA rank is set to 8.

Ablation ResultThe results are presented in Tables 4 and 9. For both small and large models, we observe that simply changing LoRA's initialization to Gaussian does not yield any performance gains and may result in a slight performance decline. However, when combined with either "+SO" (Stable Output) or "+GA" (Gradient Approximation), performance improves upon that of LoRA. LoRA-GA, which integrates both techniques, outperforms other methods. As shown in Figure 2 (Left) and Figure 4, +SO and +GA also enhance convergence speed, and when both are combined, the training loss curve is even closer to that of full-finetuning. This indicates that both output stability and gradient approximation contribute to the improvement of LoRA, each addressing different aspects of the model's performance.

### Memory Costs and Running Time

We benchmark LoRA-GA on a single RTX 3090 24GB GPU, a 128-core CPU, and 256GB of RAM. As shown in Table 5, the memory consumption of our new method does not exceed that used for training with LoRA, indicating no extra memory is needed. Additionally, the time cost of this operation is relatively negligible compared to the subsequent fine-tuning process. For instance, in the Code-Feedback task, the training process took approximately 10 hours, while the initialization required only about 1 minute, which is insignificant.

### Performance with Different Index Set Schemas

Theorem 3.1 establishes multiple optimal initialization schemes through different choices of index sets \(I_{A}\) and \(I_{B}\). While our primary experiments employed \(I_{A}=1,\ldots,r\) and \(I_{B}=r+1,\ldots,2r\), we conducted additional experiments to validate this choice by comparing three schemes:

* **ArB2r**: \(I_{A}=\{1,\ldots,r\},I_{B}=\{r+1,\ldots,2r\}\)
* **A2rBr**: \(I_{A}=\{r+1,\ldots,2r\},I_{B}=\{1,\ldots,r\}\)

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Parameters & Time(LoRA-GA) & Memory(LoRA-GA) & LoRA & Full-FT \\ \hline T5-Base & 220M & 2.8s & 1.69G & 2.71G & 3.87G \\ Llama 2-7B & 6738M & 74.7s & 18.77G & 23.18G & 63.92G \\ \hline \hline \end{tabular}
\end{table}
Table 5: Memory and Time Costs for Initialization and Fine-Tuning. ”Parameters” indicates the number of parameters in the model, ”Time(LoRA-GA)” represents the time required for initialization, ”Memory(LoRA-GA)” shows the memory usage during initialization, ”LoRA” and ”Full-FT” display the memory usage during LoRA and full fine-tuning, respectively.

Figure 2: **(Left)** Training loss curves of LoRA-GA with different ranks on the MetaMathQA dataset. Higher ranks result in faster loss reduction, approaching the performance of full fine-tuning. **(Right)** Training loss curves from the ablation study with different settings on the MetaMATHQA dataset. Compared to Vanilla LoRA, both components of LoRA-GA, +SO (stable output) and +GA (gradient approximation), improve convergence speed. LoRA-GA achieves the fastest convergence, closely matching that of full fine-tuning.

* **Random**: Random assignment of first \(2r\) indices into two groups

As shown in Table 6, ArB2r slightly outperforms the alternatives. While Theorem 3.1 proves these schemas are equivalent in the first step, their behaviors diverge afterward. The gradient of matrix \(B\) (\(\nabla_{B}\mathcal{L}=(\nabla_{W}\mathcal{L})A^{T}\)) becomes larger than that of \(A\) (\(\nabla_{A}\mathcal{L}=B^{T}\nabla_{W}\mathcal{L}\)), effectively increasing \(B\)'s learning rate. This aligns with findings from LoRA+[36], where larger learning rates for \(B\) proved beneficial, potentially explaining ArB2r's superior performance.

### Impact of Sampled Batch Size

The gradient approximation in LoRA-GA uses sampled batches, with smaller batches resembling Stochastic Gradient Descent (SGD) and larger ones approximating full Gradient Descent (GD). While theoretical work [46] suggests SGD's slower convergence may offer better generalization than GD, we conduct experiments to empirically evaluate different batch sizes.

We assess gradient approximation quality by comparing gradients from various batch sizes against a reference batch size of 2048 which serves as a proxy for the full dataset gradient using two metrics:

* **Sign Similarity**: The proportion of parameters sharing the same gradient sign.
* **Magnitude Similarity**: The proportion of parameters within the same order of magnitude (where one's absolute value is not more than 10 times the other).

As shown in Table 7, both similarity metrics consistently improve with larger batch sizes, indicating better approximation of the full gradient. The results also demonstrate that while larger batch sizes tend to yield marginally better performance, however, the differences are relatively small. Based on these findings, we recommend using a moderately large batch size (e.g., 64) when computational resources permit.

## 5 Conclusions

In this paper, we present a novel initialization scheme for low-rank adaptation (LoRA), with the goal of accelerating its convergence. By examining the initialization methods and update processes of LoRA, we develop a new initialization method, LoRA-GA, which approximates the gradients of the low-rank matrix product with those of full fine-tuning from the very first step.

Through extensive experiments, we have demonstrated that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning while delivering similar or even superior performance. Since LoRA-GA solely modifies the initialization of LoRA without altering the architecture or training algorithms, it offers an efficient and effective approach that is easy to implement. Furthermore, it can also be incorporated with other LoRA variants. For example, ReLoRA [35] periodically merges the adapters into frozen weights \(W\), which may allow LoRA-GA to demonstrate its advantages over more steps. We leave it as an interesting future direction.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & ArB2r & A2rBr & Random \\ \hline Performance & 52.79 & 52.38 & 52.01 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance comparison of initialization schemes on GSM8k using models trained on MetaMathQA subset.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Batch Size & 8 & 16 & 32 & 64 & 128 & 256 \\ \hline Sign Similarity & 0.743 & 0.790 & 0.838 & 0.875 & 0.903 & 0.925 \\ Magnitude Similarity & 0.878 & 0.908 & 0.933 & 0.950 & 0.962 & 0.971 \\ Performance & 52.79 & 52.99 & 52.91 & 53.56 & 52.57 & 53.22 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Gradient similarity metrics (vs. batch size 2048) and model performance on GSM8k using models trained on MetaMathQA subset.

## Acknowledgments and Disclosure of Funding

The authors are supported in part by the National Natural Science Foundation of China Grant 62161146004.

## References

* [1] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. _arXiv preprint arXiv:2308.10792_, 2023.
* [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [3] Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-efficient fine-tuning for large models: A comprehensive survey. _arXiv preprint arXiv:2403.14608_, 2024.
* [4] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
* [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [6] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. _Nature Machine Intelligence_, 5(3):220-235, 2023.
* [7] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient large language model fine-tuning, 2024.
* [8] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.
* [9] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. _arXiv preprint arXiv:2405.09673_, 2024.
* [10] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.
* [12] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. _arXiv preprint arXiv:1812.04754_, 2018.
* [13] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.
* [14] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* [16] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-jadge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.
* [17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.

* [18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [20] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
* [21] Dmytro Mishkin and Jiri Matas. All you need is a good init. _arXiv preprint arXiv:1511.06422_, 2015.
* [22] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. _arXiv preprint arXiv:2203.03466_, 2022.
* [23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* [24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning, 2022.
* [25] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptions for parameter-efficient model tuning. _arXiv preprint arXiv:2205.12410_, 2022.
* [26] Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. _arXiv preprint arXiv:2005.00247_, 2020.
* [27] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.
* [28] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Jimmy Ba, and Amjad Almahairi. Residual prompt tuning: Improving prompt tuning with residual reparameterization, 2023.
* [29] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. _AI Open_, 2023.
* [30] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_, 2021.
* [31] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.
* [32] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023.
* [33] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard product for communication-efficient federated learning. _arXiv preprint arXiv:2108.06098_, 2021.
* [34] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. _arXiv preprint arXiv:2212.10650_, 2022.
* [35] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Relora: High-rank training through low-rank updates, 2023.
* [36] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models, 2024.
* [37] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora, 2023.
* [38] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models, 2024.
* [39] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Pft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/pft, 2022.

* [40] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. _arXiv preprint arXiv:2011.14522_, 2020.
* [41] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-tuning for large language models with limited resources. _arXiv preprint arXiv:2:2306.09782_, 2023.
* [42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardIm: Empowering large language models to follow complex instructions, 2023.
* [43] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2024.
* [44] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement, 2024.
* [45] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.
* [46] Idan Amir, Tomer Koren, and Roi Livni. Sgd generalizes better than gd (and regularization doesn't help). In _Conference on Learning Theory_, pages 63-92. PMLR, 2021.
* [47] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1(3):211-218, 1936.
* [48] Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. _The quarterly journal of mathematics_, 11(1):50-59, 1960.
* [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.

Proofs of Theorems

### Proof of Theorem 3.1

**Lemma 3.1**.: _Suppose the loss function is \(\mathcal{L}\) and \(y=W^{\prime}x=(W_{0}+\eta BA)x\), where \(y\) is the output of a layer and \(x\) is the input, the gradients of adapters \(A\) and \(B\) are linear mappings of the gradient of \(W^{\prime}\):_

\[\nabla_{A}\mathcal{L}=B^{T}\nabla_{W^{\prime}}\mathcal{L},\quad\nabla_{B} \mathcal{L}=(\nabla_{W^{\prime}}\mathcal{L})\,A^{T}\]

_Remarkably, the gradient of \(W^{\prime}\) in LoRA and the gradient of \(W\) in full fine-tuning are equal at the beginning of the training._

Proof.: For the gradients in LoRA,

\[\nabla_{W^{\prime}}\mathcal{L} =\frac{\partial\mathcal{L}}{\partial W^{\prime}}=\frac{\partial \mathcal{L}}{\partial y}\frac{\partial y}{\partial W^{\prime}}=\frac{ \partial\mathcal{L}}{\partial y}x^{T}\] \[\nabla_{A}\mathcal{L} =\frac{\partial\mathcal{L}}{\partial A}=\frac{\partial W^{ \prime}}{\partial A}\cdot\frac{\partial\mathcal{L}}{\partial y}\frac{\partial y }{\partial W^{\prime}}=B^{T}\cdot\frac{\partial\mathcal{L}}{\partial y}x^{T} =B^{T}\nabla_{W^{\prime}}\mathcal{L}\] \[\nabla_{B}\mathcal{L} =\frac{\partial\mathcal{L}}{\partial B}=\frac{\partial\mathcal{ L}}{\partial y}\frac{\partial y}{\partial W^{\prime}}\cdot\frac{\partial W^{ \prime}}{\partial B}=\frac{\partial\mathcal{L}}{\partial y}x^{T}A^{T}=(\nabla _{W^{\prime}}\mathcal{L})\,A^{T}\]

At the beginning of training, both LoRA and full fine-tuning have \(y^{\prime}=y\) and identical \(x\), therefore,

\[\nabla_{W}\mathcal{L}=\nabla_{W^{\prime}}\mathcal{L}=\frac{\partial\mathcal{L }}{\partial y}(y)x^{T}\]

**Theorem 3.1**.: _Consider the following optimization problem:_

\[\min_{A_{\rm init}:B_{\rm init}}\left\|\eta^{2}\nabla_{W}\mathcal{L}\cdot A_{ \rm init}^{T}A_{\rm init}+\eta^{2}B_{\rm init}B_{\rm init}^{T}\cdot\nabla_{W} \mathcal{L}-\zeta\nabla_{W}\mathcal{L}\right\|_{F}\]

_If the Singular Value Decomposition (SVD) of \(\nabla_{W}\mathcal{L}\) is \(\nabla_{W}\mathcal{L}=USV^{T}\), the solution to this optimization problem is:_

\[B_{\rm init}=\frac{\sqrt{\zeta}}{\eta}U_{I_{A}},\quad A_{\rm init}=\frac{\sqrt {\zeta}}{\eta}V_{I_{B}}^{T}\quad s.t.\left|I_{A}\right|=\left|I_{B}\right|=r, \;I_{A}\cup I_{B}=\left\{i\;|\;1\leq i\leq 2r,i\in\mathbb{N}\right\}\]

_where \(I_{A},I_{B}\) are index sets._

Proof.: Since that \(rank(A_{\rm init})=rank(B_{\rm init})=r\) and \(2r<\min(m,n)\), we can assert that the matrix \(W^{\prime}=\eta^{2}\nabla_{W}\mathcal{L}A_{init}^{T}A_{init}+\eta^{2}B_{init}B _{init}^{T}\nabla_{W}\mathcal{L}\) has \(rank(W^{\prime})\leq 2r\).

Under this given solution,

\[W^{\prime} =\eta^{2}\nabla_{W}\mathcal{L}A_{\rm init}^{T}A_{\rm init}+\eta^{ 2}B_{\rm init}B_{\rm init}^{T}\nabla_{W}\mathcal{L}=\zeta USV^{T}(V_{I_{A}}V_{I _{A}}^{T})+\zeta(U_{I_{B}}U_{I_{B}}^{T})USV^{T}\] \[=\zeta\sum_{i\in I_{A}}\sigma_{i}u_{i}v_{i}^{T}+\zeta\sum_{j\in I _{B}}\sigma_{j}u_{j}u_{j}^{T}=\zeta\sum_{i=1}^{2r}\sigma_{i}u_{i}v_{i}^{T}\]

By the classic Eckart-Young Theorem (see e.g., [47, 48]), the optimal low-rank approximation with respect to Frobenius norm is:

\[W^{\prime*}=\arg\min_{rank(W^{\prime*})=2r}\left\|W^{\prime*}-\zeta\nabla_{W} \mathcal{L}\right\|_{F}=\zeta\sum_{i=1}^{2r}\sigma_{i}u_{i}v_{i}^{T}\]

This is identical to what we have got. Therefore, this is the optimal solution. 

### Proof of Theorem 3.2

**Lemma A.1**.: _In \(\mathbb{R}^{n}\), if we randomly pick a vector \(x\) that \(\sum_{i=1}^{n}x_{i}^{2}=1\), we have:_

1. \(\mathbb{E}\left(x_{i}\right)=0\)_,_ \(\mathbb{E}\left(x_{i}^{2}\right)=\frac{1}{n}\) _and_ \(\mathbb{E}\left(x_{i}^{4}\right)=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{n^{2}}\right)\)_;_
2. \(\mathbb{E}\left(x_{i}x_{j}\right)=0\)_;_
3. \(\mathbb{E}\left(x_{i}^{2}x_{j}^{2}\right)=\Theta_{r,d_{out},d_{in}}\left(\frac {1}{n^{2}}\right)\)_._
4. \(\mathbb{E}\left(x_{i}^{2}x_{j}x_{k}\right)=0\)_;_

Proof.: It is equivalent to sampling a random point uniformly from a unit sphere in \(\mathbb{R}^{n}\).

For property 1, \(\mathbb{E}\left(x_{i}\right)=0\) holds obvious by symmetry. Since \(\sum_{i=1}^{n}x_{i}^{2}=1\) and uniformly distributed, each entry has identical expectation, \(\mathbb{E}\left(\sum_{i=1}^{n}x_{i}^{2}\right)=n\mathbb{E}\left(x_{i}^{2} \right)=1\), \(\mathbb{E}\left(x_{i}^{2}\right)=\frac{1}{n}\). \(\mathbb{E}\left(x_{i}^{4}\right)=\mathbb{E}\left(x_{i}^{2}\cdot x_{i}^{2} \right)=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{n}\right)\Theta_{r,d_{out},d_{ in}}\left(\frac{1}{n^{2}}\right)=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{n^{2}}\right)\).

For property 2, it can also be proved by symmetry: we can always find vector that contains \(\left(x_{i},-x_{j}\right)\) also lies on the sphere. Therefore, \(\mathbb{E}\left(x_{i}x_{j}\right)=0\).

For property 3, \(\mathbb{E}\left(x_{i}^{2}x_{j}^{2}\right)=\mathbb{E}\left(x_{i}^{2}\cdot x_{ j}^{2}\right)=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{n}\right)\Theta_{r,d_{out},d_{ in}}\left(\frac{1}{n}\right)=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{n^{2}}\right)\).

For property 4, again it can be proved by symmetry: we can always find vector that contains \(\left(x_{i},x_{j},-x_{k}\right)\) also lies on the sphere. Therefore, \(\mathbb{E}\left(x_{i}^{2}x_{j}x_{k}\right)=0\). 

**Lemma A.2**.: _For a randomly selected orthogonal matrix \(A\in\mathbb{R}^{n\times n}\), and we randomly pick two different column vectors \(x\) and \(y\) from it. For these two vectors, we have the following:_

1. \(\mathbb{E}\left(x_{i}y_{i}\right)=0\)_;_
2. \(\mathbb{E}\left(x_{i}y_{j}\right)=0\)_;_

Proof.: It is equivalent to first selecting a random vector \(x\) from a unit sphere in \(\mathbb{R}^{n}\) uniformly, and then selecting the other one \(y\) that is orthogonal to \(x\).

For property 1, \(\sum_{i=1}^{n}x_{i}y_{i}=0\Rightarrow\mathbb{E}\left(\sum_{i=1}^{n}x_{i}y_{i} \right)=\sum_{i=1}^{n}\mathbb{E}\left(()\,x_{i}y_{i}\right)=0\Rightarrow \mathbb{E}\left(x_{i}y_{i}\right)=0\).

For property 2, consider that \(\mathbb{E}\left(\sum_{i=1}^{n}x_{i}\right)=\mathbb{E}\left(\sum_{i=1}^{n}y_{i} \right)=0\), and given \(x\), we can always find \(-y\) is also an orthogonal vector. Therefore, \(\mathbb{E}\left(\sum_{i=1}^{n}x_{i}\sum_{i=1}^{n}y_{i}\right)=0\Rightarrow E (x_{i}y_{i})=0\). 

**Theorem 3.2**.: _Given the initialization proposed in Theorem 3.1, assume that the orthogonal vectors in \(A_{\mathrm{init}}\) and \(B_{\mathrm{init}}\) are randomly selected from \(\mathbb{R}^{d_{in}}\) and \(\mathbb{R}^{d_{out}}\), and set \(\eta=\Theta_{r,d_{out},d_{in}}\left(\frac{1}{\sqrt{v}}\right)\) as suggested by rsLoRA [37]. Under these conditions, the adapters are forward scale-stable if \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{\frac{d_{out}}{r^{2}}}\right)\) and backward scale-stable if \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\sqrt{\frac{d_{in}}{r^{2}}}\right)\)._

Proof.: In LoRA, \(h=(W^{\prime}+\eta BA)x\), since that \(W^{\prime}\) is not considered here, therefore, denote \(y=\eta BAx\). When backward propagation, it's like \(\frac{\partial\mathcal{L}}{\partial x}=\eta A^{T}B^{T}\frac{\partial\mathcal{ L}}{\partial h}\). Represente \(\frac{\partial\mathcal{L}}{\partial h}\) as \(v\) and \(\frac{\partial\mathcal{L}}{\partial x}\) as \(g\). Therefore,

\[\begin{split} y_{i}&=\eta\sum_{j=1}^{r}\sum_{k=1}^{d_ {in}}B_{ij}A_{jk}x_{k},\;1\leq i\leq d_{out}\quad\text{(Forward)}\\ g_{i}&=\eta\sum_{j=1}^{r}\sum_{k=1}^{d_{out}}A_{ji}B _{kj}v_{k},\;1\leq i\leq d_{in}\quad\text{(Backward)}\end{split}\] (4)

Since that the output of each layer in model always passes a softmax function, so that the vector \(\frac{\partial\mathcal{L}}{\partial h}=v\) is \(\Theta_{r,d_{out},d_{in}}\left(1\right)\). Further, since that input \(x_{i}\)'s are _i.i.d._, without loss of generality, assume that \(E(x_{i})=0\) and \(E(x_{i}^{2})=1\).

For the adapter, as Equation 4 shows, and by the expectations we have proved in Lemma A.1 and A.2, we can calculate the scale of forward and backward process.

The scale of forward process is:

\[\begin{split}\mathbb{E}\left(y_{i}^{2}\right)&=\eta^{2} \sum_{j_{1}=1}^{r}\sum_{j_{2}=1}^{r}\sum_{k_{1}=1}^{d_{in}}\sum_{k_{2}=1}^{d_{ in}}\mathbb{E}\left(B_{ij_{1}}A_{j_{1}k_{1}}B_{ij_{2}}A_{j_{2}k_{2}}x_{k_{1}}x_{k_{2}} \right)\\ &=\eta^{2}\sum_{j_{1}=1}^{r}\sum_{j_{2}=1}^{r}\sum_{k_{1}=1}^{d_{ in}}\sum_{k_{2}=1}^{d_{in}}\mathbb{E}\left(B_{ij_{1}}B_{ij_{2}}\right)\mathbb{E} \left(A_{j_{1}k_{1}}A_{j_{2}k_{2}}\right)\mathbb{E}\left(x_{k_{1}}x_{k_{2}} \right)\\ &=\eta^{2}\sum_{j_{1}=1}^{r}\sum_{j_{2}=1}^{r}\sum_{k=1}^{d_{in}} \mathbb{E}\left(B_{ij_{1}}B_{ij_{2}}\right)\mathbb{E}\left(A_{j_{1}k}A_{j_{2}k }\right)=\eta^{2}\sum_{j=1}^{r}\sum_{k=1}^{d_{in}}\mathbb{E}\left(B_{ij}^{2} \right)\mathbb{E}\left(A_{jk}^{2}\right)\\ &=\eta^{2}\sum_{j=1}^{r}\sum_{k=1}^{d_{in}}\frac{\zeta^{2}}{\eta ^{4}}\frac{1}{d_{out}}\frac{1}{d_{in}}=\frac{1}{\alpha^{2}}\cdot\zeta^{2}\cdot \frac{r^{2}}{d_{out}}\end{split}\] (5)

The scale of the backward process is:

\[\begin{split}\mathbb{E}\left(g_{i}^{2}\right)&=\eta^{2 }\sum_{j_{1}=1}^{r}\sum_{j_{2}=1}^{r}\sum_{k_{1}=1}^{d_{out}}\sum_{k_{2}=1}^{d_ {out}}\mathbb{E}\left(A_{j_{1}i}B_{k_{1}j_{1}}A_{j_{2}i}B_{k_{2}j_{2}}v_{k_{1} }v_{k_{2}}\right)\\ &=\eta^{2}\sum_{j_{1}=1}^{r}\sum_{j_{2}=1}^{r}\sum_{k_{1}=1}^{d_ {out}}\sum_{k_{2}=1}^{m}v_{k_{1}}v_{k_{2}}\mathbb{E}\left(A_{j_{1}i}A_{j_{2}i} \right)\mathbb{E}\left(B_{k_{1}j_{1}}B_{k_{2}j_{2}}\right)\\ &=\eta^{2}\sum_{j=1}^{r}\sum_{k=1}^{d_{out}}v_{k}^{2}\mathbb{E} \left(A_{ji}^{2}\right)\mathbb{E}\left(B_{kj}^{2}\right)=\eta^{2}\sum_{j=1}^{r }\sum_{k=1}^{d_{out}}v_{k}^{2}\frac{\zeta^{2}}{\eta^{4}}\frac{1}{d_{in}}\frac {1}{d_{out}}=\frac{1}{\alpha^{2}}\cdot\zeta^{2}r^{2}\Theta_{r,d_{out},d_{in}} \left(\frac{1}{d_{in}}\right)\end{split}\] (6)

From the results derived by Equation 5 and 6, one can see that we cannot find a proper \(\zeta\) to make both scales \(\Theta_{r,d_{out},d_{in}}\left(1\right)\) unless \(\frac{d_{out}}{d_{in}}=\Theta_{r,d_{out},d_{in}}\left(1\right)\). We can also see that the forward scale is stable if adopting \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\frac{d_{out}}{r^{2}}\right)\) and the backward is stable if \(\zeta=\Theta_{r,d_{out},d_{in}}\left(\frac{d_{in}}{r^{2}}\right)\). 

## Appendix B Additional Experimental Results

### Convergence Speed

As Figure 3 and 4 shown, the convergence of LoRA-GA is significantly faster than vanilla LoRA and other ablation models, almost close to that of full fine-tuning, which support our claim about the speed of convergence.

Figure 3: Training Loss curves of Full Fine-tuning, LoRA and LoRA-GA on different datasets.

### Evaluating the Rank of the Gradient Matrix

Theorem 3.1 suggests that the closer the rank of the gradient matrix is to \(2r\), the better the gradient approximated, thereby enhancing the theoretical effectiveness of our initialization. Figure 5 illustrates the low-rank nature of gradient matrices. The left panel depicts a grid-like pattern in the gradients of a weight matrix, indicating a low-rank structure. The middle panel shows a steeply declining curve of singular values, reflecting the highly low-rank nature of the gradient matrix. The right panel presents the cumulative curve of squared singular values, demonstrating that a few ranks account for nearly all the singular values of the gradient matrix. Specifically, the coverage in the right panel is defined as

\[\text{Coverage}=\frac{\sum_{i=0}^{2r}\sigma_{i}^{2}}{\sum_{i=0}^{n}\sigma_{i}^{ 2}},\]

where \(r\) is the LoRA rank used in LoRA-GA, indicating how much of the low-rank matrix can be approximated by this rank.

We further validate this observation on larger models by analyzing LLaMA 2-7B during MetaMathQA training. Table 8 presents the coverage across different layers with varying LoRA ranks. Even with a relatively small rank of 8, we achieve a mean coverage of 92.9% across all layers, with the minimum coverage being 85.1%. Increasing the rank to 128 yields an impressive mean coverage of 99.3%, with the minimum coverage reaching 97.5%. These results demonstrate that even for large models with weight matrices of dimension 4096, a modest LoRA rank is sufficient to capture the majority of the gradient information.

Figure 4: Training Loss curves of different LoRA-GA ablations on different datasets.

Figure 5: (**Left**) A gradient matrix of T5-Base during fine-tuning on CoLA. (**Middle**) The decreasing curve of singular values of the gradient matrix. (**Right**) The cumulative curve showing the coverage of squared singular values.

[MISSING_PAGE_FAIL:18]

### Experiments on the Full MetaMathQA Dataset

Following [9], we conducted additional experiments by training on the complete MetaMathQA dataset for multiple epochs, whereas our main results in the previous section were based on fine-tuning for one epoch on the 100k subset of MetaMathQA. Due to computational constraints, we limited these extended experiments to three methods: LoRA[4], LoRA+[36], and LoRA-GA.

Table 12 presents the performance across four epochs, averaged over two random seeds.

The results show that LoRA-GA consistently achieves better performance than vanilla LoRA and outperforms LoRA+ in most cases across multiple epochs of training.

## Appendix C LoRA-GA Initialization With Gradient Accumulation

```
0: Model \(f(\cdot)\) with \(L\) layers, parameters \(W\), sampled batch \(B=\{x,y\}\), LoRA rank \(r\) with \(n\) samples, LoRA alpha \(\alpha\), loss function \(\mathcal{L}\), scale factor \(\gamma\), micro-batch size \(b\)
0: Initialized parameters \(W\), \(\eta\), \(A\), \(B\)
1:\(\hat{y}\gets f(x,W)\)\(\triangleright\) Forward pass
2:\(\ell\leftarrow\mathcal{L}(y,\hat{y})\)
3:\(\eta\leftarrow\frac{\alpha}{\sqrt{r}}\)
4:for\(l=1,\ldots,L\)do
5:\(\nabla_{W_{l}}^{\text{avg}}\ell\gets 0\)\(\triangleright\) Initialize average gradient for each layer on CPU
6:endfor
7:for each micro-batch \(B_{i}\) in \(B\)do
8:\(\hat{y}_{i}\gets f(x_{i},W)\)\(\triangleright\) Forward pass for micro-batch
9:\(\ell_{i}\leftarrow\mathcal{L}(y_{i},\hat{y}_{i})\)
10:for\(l=L,\ldots,1\)do
11: Compute \(\nabla_{W_{l}}\ell_{i}\)\(\triangleright\) Backward pass for one layer
12:\(\nabla_{W_{l}}^{\text{avg}}\ell\leftarrow\nabla_{W_{l}}^{\text{avg}}\ell+ \nabla_{W_{l}}\ell_{i}\cdot\frac{b}{n}\)\(\triangleright\) Move to CPU
13: Clear \(\nabla_{W_{l}}\ell_{i}\)\(\triangleright\) Gradient for this layer is not needed anymore
14:endfor
15:endfor
16:for\(l=L,\ldots,1\)do
17:\(d_{out},d_{in}\leftarrow\text{size}(W_{l})\)
18:\(U,S,V\leftarrow\text{svd}(\nabla_{W_{l}}^{\text{avg}}\ell)\)
19:\(A_{l}\gets V_{[1:r]}\cdot\sqrt[d]{d_{out}}/\gamma\)
20:\(B_{l}\gets U_{[r+1:2r]}\cdot\sqrt[d]{d_{out}}/\gamma\)
21:\(W_{l}\gets W_{l}-\eta B_{l}A_{l}\)
22:endfor
23:return\(W\), \(\eta\), \(A\), \(B\) ```

**Algorithm 2** LoRA-GA Initialization With Gradient Accumulation

## Appendix D Hyperparameter

### Experiments on Natural Language Understanding

We use the following hyperparameters with T5-Base.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **Epoch 1** & **Epoch 2** & **Epoch 3** & **Epoch 4** \\ \hline LoRA (Rank=8) & \(55.19\) & \(58.37\) & \(59.28\) & \(58.90\) \\ LoRA+ (Rank=8) & \(56.37\) & \(\mathbf{59.21}\) & \(59.93\) & \(59.97\) \\ LoRA-GA (Rank=8) & \(\mathbf{56.48}\) & \(58.64\) & \(\mathbf{60.16}\) & \(\mathbf{60.88}\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Performance comparison of different methods on full MetaMathQA dataset training for multiple epochs.

* Training Algorithm: AdamW [49] with \(\beta_{1}=0.9\), \(\beta_{2}=0.999\), \(\epsilon=1e-8\) and weight decay of 0. For full finetuning, LoRA, and its variants, a learning rate of \(1e-4\), a warmup ratio of 0.03, and cosine decay are employed. For DoRA [8], a learning rate of \(2e-4\) is used, while for Adalora, a learning rate of \(5e-4\) is applied, both with the same warmup ratio and cosine decay adhering to their respective papers.
* LoRA Hyperparameters: LoRA rank \(r=8\), \(\alpha=16\). LoRA target is all linear modules except embedding layer, layer norm and language model head.
* LoRA-GA Hyperparameter: \(\gamma=16\), sampled batch size \(sbs=8\)
* Other Hyperparameters: Sequence Length \(T=128\), train batch size \(bs=32\), number of train epochs \(E=1\). Precision FP32

### Experiment on Large Language Model

We use the following hyperparameters with Llama 2-7B.

* Training Algorithm: AdamW [49] with with \(\beta_{1}=0.9\), \(\beta_{2}=0.999\), \(\epsilon=1e-8\) and weight decay of 0. For full finetuning, LoRA, and its variants, a learning rate of \(2e-5\)[38], a warmup ratio of 0.03, and cosine decay are employed. For DoRA [8], a learning rate of \(2e-4\) is used, while for Adalora, a learning rate of \(5e-4\) is applied, both with the same warmup ratio and cosine decay adhering to their respective papers.
* Precision: The backbone model uses bf16 precision, while during training, LoRA's \(B\) and \(A\) matrices use fp32 precision, following the implementation of PEFT [39].
* LoRA-GA Hyperparameter: \(\gamma=64\), micro sampled batch size \(sbs=1\) with gradient accumulation of 32.
* LoRA Hyperparameters: LoRA rank \(r=8\) and \(\alpha=16\) for all experiments.
* Generation Hyperparameters: All generation is performed with \(top\_p=0.95\) and temperature \(T=0.8\).
* Other Hyperparameters: Number of train epochs \(E=1\), train micro batch size \(mbs=1\) with gradient accumulation of 32. Sequence Length \(T=1024\)

## Appendix E Comparison between LoRA-GA and PiSSA

Both LoRA-GA and PiSSA [38] concentrate on the initialization of LoRA, and utilizing SVD on pre-trained models. While they may appear similar superficially, significant differences exist between them.

Firstly, the motivations behind LoRA-GA and PiSSA are fundamentally different. As discussed in Section 3.2, LoRA-GA is motivated by the approximation of the LoRA update and full fine-tuning. We employ SVD on gradients solely because the optimal solution to the gradient approximation problem is precisely obtained (as stated in Theorem 3.1). Conversely, PiSSA adopts SVD under the assumption that pre-trained weights possess a low intrinsic rank, and thus, the SVD of weights can provide an accurate representation of original weights. In essence, LoRA-GA emphasizes on gradients and decomposes them, whereas PiSSA concentrates on weights and decomposes them.

Secondly, LoRA-GA and PiSSA employ different scales of initialization. In Section 3.3, LoRA-GA derives an appropriate scaling factor by considering the forward and backward stability of our initialization scheme. On the other hand, PiSSA uses the largest \(r\) singular values as the magnitude of orthogonal matrices directly.

## Appendix F Limitations

In this paper, we have demonstrated that LoRA-GA can achieve performance comparable to full fine-tuning on the T5-Base (220M) and Llama 2-7B models, while significantly reducing the number of parameters and associated costs. However, due to computational resource constraints, we have not validated LoRA-GA on larger pre-trained models (e.g., Llama 2-70B).

In LoRA-GA, we proposed that a scaling factor is necessary. But in some experiments with large learning rates, we observed potential numerical instability due to the effect of scaling factor and learning rate. This limitation suggests a need for careful tuning of the scaling factor and learning rate to maintain stability.

Another limitation pertains to our evaluation scope. While we provide evaluations on MTBench, GSM8K, and Human-eval, we did not assess our method on other datasets. Consequently, we cannot fully guarantee that our findings are universally consistent across all benchmarks.

Additionally, we did not implement our method on other LoRA variants that are orthogonal to our improvements (e.g., ReLoRA [35]). Therefore, we cannot ascertain whether LoRA-GA would perform equally well with other LoRA architectures/improvements.

Finally, compared to the original LoRA, LoRA-GA requires double the checkpoint storage, as it necessitates storing both the initial adapter checkpoints (\(A_{init}\) and \(B_{init}\)) and the final adapter checkpoints (\(A\) and \(B\)).

## Appendix G Compute Resources

In this paper, we utilized two types of GPUs: the RTX 3090 24GB GPU, supported by a 128-core CPU and 256GB of RAM (hereinafter referred to as "the RTX 3090"), and the A100 80GB GPU (hereinafter referred to as "the A100").

For the experiments on T5-Base using the GLUE dataset, reported in Section 4.1, all computations were performed on a single RTX 3090. For the Llama 2-7B experiments, reported in Section 4.2, full fine-tuning and DoRA scenarios were conducted on a single A100, while all other LoRA variants and LoRA-GA were executed on a single RTX 3090. Additionally, all ablation studies presented in Section 4.3 were carried out on a single RTX 3090.

## Appendix H Broader Impacts

In this paper, we identify some limitations of vanilla LoRA and propose a more efficient and effective method for LoRA initialization, LoRA-GA. LoRA-GA converges faster than vanilla LoRA and consistently achieves better evaluation results.

We believe that this work will have a positive social impact. The primary reasons are as follows: The high cost of training and fine-tuning large models is a significant challenge today. LoRA-GA offers a way to fine-tune with fewer parameters and lower computational costs while still achieving comparable performance. This will reduce the cost of fine-tuning models and, in turn, decrease energy consumption, such as electricity, contributing to the goal of a low-carbon environment. Furthermore, as the size of large language models (LLM) continues to grow, it becomes increasingly difficult for individuals or small organizations to develop their own LLMs. However, with the help of LoRA-GA and open-source large models, the hardware barrier to entry in this area is greatly reduced. This will promote democratization in the field of large models, preventing monopolies and dictatorships by a few companies.

On the other hand, our method could potentially make it easier to train language models that generate fake news or misleading information. This underscores the necessity for designing effective detectors to identify content generated by large language models (LLMs). Ensuring the responsible use of this technology is crucial to mitigating the risks associated with the misuse of advanced language models.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We included 3 contributions at the end of introduction. The method LoRA-GA and stable scale are proposed in Section 3. The extensive experiments and results are in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The last section (Appendix F) of this paper is entirely discuss the limitation of our works. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: There are 1 lemma (Lemma 3.1) and 2 theorems (Theorem 3.1 and 3.2) proposed in our paper. All them are properly proved in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Hyperparameters are disclosed in Appendix D. Other implementation information are disclosed in Section 4, in paragraphs began with "Implementation details". Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All codes of our experiments are uploaded to anonymous github. All datasets used in our experiments are all open source, which has been declared and cited in Section 1 (Introduction). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details that are important to understand and evaluate experimental results are shown in our paper or appendix like Table 3. Other details can be found in our codes. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report standard deviations of our evaluation results like footnotes in Table 1, 2 4, 9, 10 and 11. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Discussed in Appendix G. Guidelines:* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: I have read it. I'm sure that our research conforms NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed this in Appendix H. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our research is purely a foundamental reseach about LoRA, which cannot pose such risks. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All models and datasets used in our paper are all properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The way to run our codes work is attached in our codes. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects or participants involved in our research. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: No human subjects or participants involved in our research.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.