# Remove that Square Root: A New Efficient

Scale-Invariant Version of AdaGrad

Sayantan Choudhury

MBZUAI & Johns Hopkins University

&Nazarii Tupitsa

MBZUAI & Innopolis University

&Nicolas Loizou

Johns Hopkins University

&Samuel Horvath

MBZUAI

&Martin Takac

MBZUAI

&Eduard Gorbunov

MBZUAI

Part of this work was done when S. Choudhury was an intern at MBZUAI, UAE.

###### Abstract

Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named \(\mathsf{KATE}\), which presents a scale-invariant adaptation of the well-known \(\mathsf{AdGrad}\) algorithm. We prove the scale-invariance of \(\mathsf{KATE}\) for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of \(\mathcal{O}(\nicefrac{{\log T}}{{\sqrt{T}}})\) for \(\mathsf{KATE}\), matching the best-known ones for \(\mathsf{AdaGrad}\) and \(\mathsf{Adam}\). We also compare \(\mathsf{KATE}\) to other state-of-the-art adaptive algorithms \(\mathsf{Adam}\) and \(\mathsf{AdaGrad}\) in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that \(\mathsf{KATE}\) consistently outperforms \(\mathsf{AdaGrad}\) and matches/surpasses the performance of \(\mathsf{Adam}\) in all considered scenarios.

## 1 Introduction

In this work, we consider the following unconstrained optimization problem:

\[\min_{w\in\mathbb{R}^{d}}f(w),\] (1)

where \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a \(L\)-smooth and generally non-convex function. In particular, we are interested in the situations when the objective has either expectation \(f(w)=\mathbb{E}_{\xi\sim D}[f_{\xi}(w)]\) or finite-sum \(f(w)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(w)\) form. Such minimization problems are crucial in machine learning, where \(w\) corresponds to the model parameters. Solving these problems with stochastic gradient-based optimizers has gained much interest owing to their wider applicability and low computational cost. Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) and similar algorithms require the knowledge of parameters like \(L\) for convergence and are very sensitive to the choice of the stepsize in general. Therefore, SGD requires hyperparameter tuning, which can be computationally expensive. To address these issues, it is common practice to use adaptive variants of stochastic gradient-based methods that can converge without knowing the function's structure.

There exist many adaptive algorithms such as \(\mathsf{AdaGrad}\)(Duchi et al., 2011), \(\mathsf{Adam}\)(Kingma and Ba, 2014), \(\mathsf{AMSGrad}\)(Reddi et al., 2019), \(\mathsf{D}\)-Adaptation(Defazio and Mishchenko, 2023), \(\mathsf{Prodigy}\)(Mishchenko and Defazio, 2023), \(\mathsf{AI}\)-\(\mathsf{SARAH}\)(Shi et al., 2023) and their variants. These adaptive techniques are capable of updating their step sizes on the fly. For instance, the \(\mathsf{AdaGrad}\) method determines its step sizes using a cumulative sum of the coordinate-wise squared (stochastic) gradientof all the previous iterates:

\[\mathsf{AdaGrad}\text{: }w_{t+1}=w_{t}-\frac{\beta g_{t}}{\sqrt{\text{ diag}\left(\Delta I+\sum\limits_{\tau=1}^{t}g_{\tau}g_{\tau}^{\tau}\right)}},\] (2)

where \(g_{t}\) represents an unbiased estimator of \(\nabla f(w_{t})\), i.e., \(\mathbb{E}\left[g_{t}\mid w_{t}\right]=\nabla f(w_{t})\), \(\text{diag}(M)\in\mathbb{R}^{d}\) is a vector of diagonal elements of matrix \(M\in\mathbb{R}^{d\times d}\), \(\Delta>0\), and the division by vector is done component-wise. Ward et al. (2020) has shown that this method achieves a convergence rate of \(\mathcal{O}\left(\nicefrac{{\log T}}{{\sqrt{T}}}\right)\) for smooth functions, similar to SGD, without prior knowledge of the functions' parameters. However, the performance of AdaGrad deteriorates when applied to data that may exhibit poor scaling or ill-conditioning. In this work, we propose a novel algorithm, KATE, to address the issues of poor data scaling. KATE is also a stochastic adaptive algorithm that can achieve a convergence rate of \(\mathcal{O}\left(\nicefrac{{\log T}}{{\sqrt{T}}}\right)\) for smooth non-convex functions in terms of \(\min_{t\in[T]}\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|\right]^{2}\).

### Related Work

A significant amount of research has been done on adaptive methods over the years, including AdaGrad(Duchi et al., 2011; McMahan and Streeter, 2010), AMSGrad(Reddi et al., 2019), RMSProp(Tieleman and Hinton, 2012), AI-SARAH(Shi et al., 2023), and Adam(Kingma and Ba, 2014). However, all these works assume that the optimization problem is contained in a bounded set. To address this issue, Li and Orabona (2019) proposes a variant of the AdaGrad algorithm, which does not use the gradient of the last iterate (this makes the step sizes of \(t\)-th iteration conditionally independent of \(g_{t}\)) for computing the step sizes and proves convergence for the unbounded domain.

Each of these works considers a vector of step sizes for each coefficient. Duchi et al. (2011) and McMahan and Streeter (2010) simultaneously proposed the original AdaGrad algorithm. However, McMahan and Streeter (2010) was the first to consider the vanilla scalar form of AdaGrad, known as

\[\mathsf{AdaGradNorm}\text{: }w_{t+1}=w_{t}-\frac{\beta g_{t}}{\sqrt{\Delta+ \sum_{\tau=0}^{t}\left\|g_{\tau}\right\|^{2}}}.\] (3)

Later, Ward et al. (2020) analyzed AdaGradNorm for minimizing smooth non-convex functions. In a follow-up study, Xie et al. (2020) proves a linear convergence of AdaGradNorm for strongly convex functions. Recently, Liu et al. (2022) analyzed AdaGradNorm for solving smooth convex functions without the bounded domain assumption. Moreover, Liu et al. (2022) extends the convergence guarantees of AdaGradNorm to quasar-convex functions 2 using the function value gap. Orabona et al. (2015) introduce the notion of scale-invariance, which is a special case of affine invariance (Nesterov and Nemirovskii, 1994; Nesterov, 2018; d'Aspremont et al., 2018), propose a scale-invariant version of AdaGrad for online convex optimization for generalized linear models, and prove \(\mathcal{O}(\sqrt{T})\) regret bounds in this setup.

Footnote 2: \(f\) satisfy \(f^{*}\geq f(w)+\frac{1}{\zeta}\left\langle f(w),w^{*}-w\right\rangle\) for some \(\zeta\in(0,1]\) where \(w^{*}\in\text{argmin}_{w}f(w)\).

Recently, Defazio and Mishchenko (2023) introduced the \(\mathsf{D}\)-Adaptation method, which has gathered considerable attention due to its promising empirical performances. In order to choose the adaptive step size optimally, one requires knowledge of the initial distance from the solution, i.e., \(D:=\left\|w_{0}-w_{*}\right\|\) where \(w_{*}\in\text{argmin}_{w\in\mathbb{R}^{d}}f(w)\). The \(\mathsf{D}\)-Adaptation method works by maintaining an estimate of \(D\) and the stepsize choice in this case is \(\nicefrac{{d_{t}}}{{\sqrt{\sum_{\tau=0}^{t}\left\|g_{\tau}\right\|^{2}}}}\) for the \(t\)-th iteration (here \(d_{t}\) is an estimate of \(D\)). Mishchenko and Defazio (2023) further modifies the algorithm in a follow-up work and introduces \(\mathsf{Prodigy}\) (with stepsize choice \(\nicefrac{{d_{t}^{2}}}{{\sqrt{\sum_{\tau=0}^{t}d_{\tau}^{2}\left\|g_{\tau} \right\|^{2}}}}\)) to improve the convergence speed.

Another exciting line of work on adaptive methods is Polyak stepsizes. Polyak (1969) first proposed Polyak stepsizes for subgradient methods, and recently, the stochastic version (also known as SPS) was introduced by Oberman and Prazeres (2019); Loizou et al. (2021); Abdukhakiimov et al. (2024, 2023); Li et al. (2023) and Gower et al. (2021). For a finite sum problem \(\min_{w\in\mathbb{R}^{d}}f(w):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(w)\), Loizou et al. (2021) uses \(\frac{f_{i}(w_{t})-f_{i}^{*}}{c\left\|\nabla f_{i}(w_{t})\right\|^{2}}\) as their stepsize choices (here \(f_{i}^{*}:=\min_{w\in\mathbb{R}^{d}}f_{i}(w)\)), while Oberman and Prazeres (2019) uses \(\frac{2(f(w_{t})-f^{*})}{\mathbb{E}\left[\left\|\nabla f_{i}(w_{t})\right\|^{2} \right]}\) for \(k\)-th iteration. However, these methods are impractical when \(f^{*}\) or \(f_{i}^{*}\) is unknown. Following its introduction,several variants of the SPS algorithm emerged (Li et al., 2023; D'Orazio et al., 2021). Lately, Orvieto et al. (2022) tackled the issues with unknown \(f_{i}^{*}\) and developed a truly adaptive variant. In practice, the SPS method shows excellent empirical performance on overparameterized deep learning models (which satisfy the interpolation condition i.e. \(f_{i}^{*}=0,\ \forall i\in[n]\)) (Loizou et al., 2021).

### Main Contribution

Our main contributions are summarized below.

\(\bullet\)KATE: new scale-invariant version of AdaGrad.We propose a new method called KATE that can be seen as a version of AdaGrad, which does not use a square root in the denominator of the stepsize. To compensate for this change, we introduce a new sequence defining the numerator of the stepsize. We prove that KATE is scale-invariant for generalized linear models: if the starting point is zero, then the loss values (and training and test accuracies in the case of classification) at points generated by KATE are independent of the data scaling (Proposition 2.1), meaning that the speed of convergence of KATE is the same as for the best scaling of the data.

\(\bullet\)**Convergence for smooth non-convex problems.** We prove that for smooth non-convex problems with noise having bounded variance KATE has \(\mathcal{O}(\log(T)/\sqrt{T})\) convergence rate (Theorem 3.4), matching the best-known rates for AdaGrad and Adam(Defossez et al., 2020).

\(\bullet\)**Numerical experiments.** We empirically illustrate the scale-invariance of KATE on the logistic regression task and test its performance on logistic regression (see Section 4.1), image classification, and text classification problems (see Section 4.2). In all the considered scenarios, KATE outperforms AdaGrad and works either better or comparable to Adam.

### Notation

We denote the set \(\{1,2,\cdots,n\}\) as \([n]\). For a vector \(a\in\mathbb{R}^{d}\), \(a[k]\) is the \(k\)-th coordinate of \(a\) and \(a^{2}\) represents the element-wise suqare of \(a\), i.e., \(a^{2}[k]=(a[k])^{2}\). For two vectors \(a\) and \(b\), \(\frac{a}{b}\) stands for element-wise division of \(a\) and \(b\), i.e., \(k\)-th coordinate of \(\frac{a}{b}\) is \(\frac{a[k]}{b[k]}\). Given a function \(h:\mathbb{R}^{d}\rightarrow\mathbb{R}\), we use \(\nabla h\in\mathbb{R}^{d}\) to denote its gradient and \(\nabla_{k}h\) to indicate the \(k\)-th component of \(\nabla h\). Throughout the paper \(\|\cdot\|\) represents the \(\ell_{2}\)-norm and \(f_{*}=\inf_{w\in\mathbb{R}^{d}}f(w)\). Moreover, we use \(\left\|w\right\|_{A}\) for a positive-definite matrix \(A\) to define \(\left\|w\right\|_{A}\coloneqq\sqrt{w^{\top}\,Aw}\). Furthermore, \(\mathbb{E}\left[\cdot\right]\) denotes the total expectation while \(\mathbb{E}_{t}\left[\cdot\right]\) denotes the conditional expectation conditioned on all iterates up to step \(t\) i.e. \(w_{0},w_{1},\ldots,w_{t}\).

## 2 Motivation and Algorithm Design

We focus on solving the minimization problem (1) using a variant of AdaGrad. We aim to design an algorithm that performs well, irrespective of how poorly the data is scaled.

\begin{table}
\begin{tabular}{c c c} Algorithm & Convergence rate & Scale invariance \\ \hline AdaGradNorm (Ward et al., 2020) & \(\mathcal{O}\left(\log T/\sqrt{T}\right)\) & ✗ \\ \hline AdaGrad (Defossez et al., 2020) & \(\mathcal{O}\left(\log T/\sqrt{T}\right)\) & ✗ \\ \hline Adam (Defossez et al., 2020) & \(\mathcal{O}\left(\log T/\sqrt{T}\right)\) & ✗ \\ \hline KATE (this work) & \(\mathcal{O}\left(\log T/\sqrt{T}\right)\) & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of convergence guarantees for closely-related adaptive algorithms to solve _smooth non-convex stochastic_ optimization problems. Convergence rates are given in terms of \(\min_{t\in[T]}\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|\right]^{2}\). We highlight KATE’s _scale-invariance_ property for problems of type (4).

Generalized linear models.Here, we consider the parameter estimation problem in generalized linear models (GLMs) (Nelder and Wedderburn, 1972; Agresti, 2015) using maximum likelihood estimation. GLMs are an extension of linear models and encompass several other valuable models, such as logistic (Hosmer Jr et al., 2013) and Poisson regression (Frome, 1983), as special cases. The parameter estimation to fit GLM on dataset \(\{x_{i},y_{i}\}_{i=1}^{n}\) (where \(x_{i}\in\mathbb{R}^{d}\) are feature vectors and \(y_{i}\) are response variables) can be reformulated as

\[\min_{w\in\mathbb{R}^{d}}f(w)\coloneqq\frac{1}{n}\sum_{i=1}^{n} \varphi_{i}\left(x_{i}^{\top}w\right)\] (4)

for differentiable functions \(\varphi_{i}:\mathbb{R}\rightarrow\mathbb{R}\)(Shalev-Shwartz and Ben-David, 2014; Nguyen et al., 2017; Takac et al., 2013; He et al., 2018; Chezhegov et al., 2024). For example, the linear regression on data \(\{x_{i},y_{i}\}_{i=1}^{n}\) is equivalent to solving (4) with \(\varphi_{i}(z)=(z-y_{i})^{2}\). Next, the choice of \(\varphi_{i}\) for logistic regression is \(\varphi_{i}(z)=\log\left(1+\exp\left(-y_{i}z\right)\right)\).

Scale-invariance.Now consider the instances of fitting GLMs on two datasets \(\{x_{i},y_{i}\}_{i=1}^{n}\) and \(\{Vx_{i},y_{i}\}_{i=1}^{n}\), where \(V\in\mathbb{R}^{d\times d}\) is a diagonal matrix with positive entries. Note that the second dataset is a scaled version of the first one where the \(k\)-th component of feature vectors \(x_{i}\) are multiplied by a scalar \(V_{kk}\). Then, the minimization problems corresponding to datasets \(\{x_{i},y_{i}\}_{i=1}^{n}\) and \(\{Vx_{i},y_{i}\}_{i=1}^{n}\) are (4) and

\[\min_{w\in\mathbb{R}^{d}}f^{V}(w) \coloneqq \frac{1}{n}{\sum}_{i=1}^{n}\varphi_{i}\left(x_{i}^{\top}Vw \right),\] (5)

respectively, for functions \(\varphi_{i}\). In this work, _we want to design an algorithm with equivalent performance for the problems (4) and (5)_. If we can do that, the new algorithm's performance will not deteriorate for poorly scaled data, i.e., the method will be scale-invariant (Orabona et al., 2015), which is a special case of affine-invariance, see (Nesterov and Nemirovskii, 1994; Nesterov, 2018; d'Aspremont et al., 2018). To develop such an algorithm, we replace the denominator of AdaGrad step size with its square (remove the square root from the denominator), i.e., \(\forall k\in[d]\)

\[w_{t+1}[k] = w_{t}[k]-\frac{\beta m_{t}[k]}{\sum_{\tau=0}^{t}g_{\tau}^{2}[k]} g_{t}[k]\] (6)

for some \(m_{t}\in\mathbb{R}^{d}\).3 The following proposition shows that this method (6) satisfies a scale-invariance property with respect to functional value.

Footnote 3: Sequence \(\{m_{t}\}_{t\geq 0}\) can depend on the problem but is assumed to be scale-invariant.

**Proposition 2.1** (Scale invariance).: Suppose we solve problems (4) and (5) using algorithm (6). Then, the iterates \(\hat{w}_{t}\) and \(\hat{w}_{t}^{V}\) corresponding to (4) and (5) follow: \(\forall k\in[d]\)

\[\hat{w}_{t+1}[k] = \hat{w}_{t}[k]-\frac{\beta m_{t}[k]}{\sum_{\tau=0}^{t}g_{\tau}^{2 }[k]}g_{t}[k],\] (7) \[\hat{w}_{t+1}^{V}[k] = \hat{w}_{t}^{V}[k]-\frac{\beta m_{t}[k]}{\sum_{\tau=0}^{t}\left(g _{\tau}^{V}[k]\right)^{2}}g_{t}^{V}[k]\] (8)

with \(g_{\tau}=\varphi_{i_{\tau}}^{\prime}(x_{i_{\tau}}^{\top}\hat{w}_{\tau})x_{i_{ \tau}}\) and \(g_{\tau}^{V}=\varphi_{i_{\tau}}^{\prime}(x_{i_{\tau}}^{\top}V\hat{w}_{\tau})Vx_{ i_{\tau}}\) for \(i_{\tau}\) chosen uniformly from \([n]\), \(\tau=0,1,\ldots,t\), \(t\geq 0\). Moreover, updates (7) and (8) satisfy

\[\hat{w}_{t}=V\hat{w}_{t}^{V},\quad Vg_{t}=g_{t}^{V},\quad f\left( \hat{w}_{t}\right)=f^{V}\left(\hat{w}_{t}^{V}\right)\] (9)

for all \(t\geq 0\) when \(\hat{w}_{0}=\hat{w}_{0}^{V}=0\in\mathbb{R}^{d}\). Furthermore we have

\[\left\|g_{t}^{V}\right\|_{V^{-2}}^{2} = \left\|g_{t}\right\|^{2}.\] (10)

The Proposition 2.1 highlights that the update rule of the form (6) satisfies a scale-invariance property for GLMs. In contrast, AdaGrad does not satisfy (9) and (10). In Appendix C, we illustrate numerically the scale-invariance of KATE and the lack of the scale-invariance of AdaGrad. We also emphasize that AdaGrad with \(\Delta=0\) is known to be a scale-free method4.

Footnote 4: The algorithm is called scale-free if for any \(c>0\), it generates the same sequence of points for functions \(f\) and \(cf\) given the same initialization and hyperparameters. To the best of our knowledge, this definition is Footnote 4: Note that, for \(m_{t}=b_{i}\forall t\) we get the AdaGrad algorithm.

```
0: Initial point \(w_{0}\in\mathbb{R}^{d}\), step size \(\beta>0,\eta\in\mathbb{R}^{d}_{+}\) and \(b_{-1},m_{-1}=0\).
1:for\(t=0,1,...,T\)do
2: Compute \(g_{t}\in\mathbb{R}^{d}\) such that \(\mathbb{E}\left[g_{t}\right]=\nabla f(w_{t})\).
3:\(b_{t}^{2}=b_{t-1}^{2}+g_{t}^{2}\)
4:\(\left|m_{t}^{2}=m_{t-1}^{2}+\eta g_{t}^{2}+\frac{g_{t}^{2}}{b_{t}^{2}}\right|\)
5:\(\left.w_{t+1}=w_{t}-\frac{\beta m_{t}}{b_{t}^{2}}g_{t}\right.\) ```

**Algorithm 1**KATE

Design of Kate.In order to construct an algorithm following the update rule (6), one may choose \(m_{t}[k]=1\)\(\forall k\in[d]\). However, the step size from (6) in this case may decrease very fast, and the resulting method does not necessarily converge. Therefore, we need a more aggressive choice of \(m_{t}\), which grows with \(t\). It motivates the construction of our algorithm KATE (Algorithm 1),5 where we choose \(m_{t}[k]=\sqrt{\eta[k]b_{t}^{2}[k]+\sum_{\tau=0}^{t}\frac{g_{t}^{2}[k]}{b_{ \tau}^{2}[k]}}\). Note that the term \(\sum_{\tau=0}^{t}\frac{g_{t}^{2}[k]}{b_{\tau}^{2}[k]}\) is scale-invariant for GLMs (follows from Proposition 2.1). To make \(m_{t}\) scale-invariant, we choose \(\eta\in\mathbb{R}^{d}\) in the following way:

Footnote 5: Note that, for \(m_{t}=b_{i}\forall t\) we get the AdaGrad algorithm.

* \(\eta\to 0\): When \(\eta\) is very small, \(m_{t}\) is also approximately scale-invariant for GLMs.
* \(\eta=\nicefrac{{1}}{{(\nabla f(w_{0}))^{2}}}\): In this case \(\eta b_{t}^{2}=\nicefrac{{b_{t}^{2}}}{{(\nabla f(w_{0}))^{2}}}\) is scale-invariant for GLMs (follows from Proposition 2.1) as well as \(m_{t}\).

Kate can be rewritten in the following coordinate form

\[w_{t+1}[k]=w_{t}[k]-\nu_{t}[k]g_{t}[k],\qquad\forall k\in[d],\] (11)

where \(g_{t}\) is an unbiased estimator of \(\nabla f(w_{t})\) and the per-coefficient step size \(\nu_{t}[k]\) is defined as

\[\nu_{t}[k]\coloneqq\frac{\beta\sqrt{\eta[k]b_{t}^{2}[k]+\sum_{\tau=0}^{t} \frac{g_{t}^{2}[k]}{b_{\tau}^{2}[k]}}}{b_{t}^{2}[k]}.\] (12)

Note that the numerator of the steps \(\nu_{t}[k]\) is increasing with iterations \(t\). However, one of the crucial properties of this step size choice is that the steps always decrease with \(t\), which we rely on in our convergence analysis.

[Decreasing step size] For \(\nu_{t}[k]\) defined in (11) we have

\[\nu_{t+1}[k]\leq\nu_{t}[k],\qquad\forall k\in[d].\] (13)

Comparison with the scale-invariant version of AdaGrad by Orabona et al. (2015).In the special case of GLMs, Orabona et al. (2015) propose a different version of AdaGrad. The method is proposed for the case of online convex optimization, and in the case of standard optimization with GLMs (4), it has the following form

\[w_{0}:=0,\quad w_{t+1}:=-\beta\frac{\sum_{\tau=0}^{t}\nabla f_{i_{\tau}}(w_{ \tau})}{a_{t}^{2}\sqrt{d}\sqrt{\gamma^{2}+\sum_{\tau=0}^{t}\left(\nabla f_{i_ {\tau}}(w_{\tau})/a_{\tau}\right)^{2}}},\quad a_{t}:=\max_{\tau=0,...,t}|x_{i_{ \tau}}|,\] (14)

where \(\{i_{\tau}\}_{\tau=0}^{t}\) are arbitrary indices from \([n]\) (e.g., selected uniformly at random), functions \(f_{i}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) are defined as \(f_{i}(w):=\varphi_{i}(x_{i}^{\top}w)\) for \(i\in[n]\), and \(\gamma\) is such that \(f_{i}(w)\) is \(\gamma\)-Lipschitz for \(i\in[n]\). In this setup, the update rule of KATE with \(w_{0}=0\) can be written as follows:

\[w_{t+1}:=-\beta\sum_{\tau=0}^{t}\frac{m_{\tau}}{b_{\tau}^{2}}\nabla f_{i_{\tau} }(w_{\tau}),\quad m_{t}:=\sqrt{\eta\sum_{\tau=0}^{t}(\nabla f_{i_{\tau}}(w_{ \tau}))^{2}+\sum_{\tau=0}^{t}\left(\nabla f_{i_{\tau}}(w_{\tau})\right)^{2}/b_ {\tau}^{2}},\]

[MISSING_PAGE_FAIL:6]

**Theorem 3.3**.: Suppose \(f\) satisfy Assumption 3.1 and \(g_{t}=\nabla f(w_{t})\). Moreover, \(\beta>0\) and \(\eta[k]>0\) are chosen such that \(\nu_{0}[k]\leq\frac{1}{L}\) for all \(k\in[d]\). Then the iterates of \(\mathsf{KATE}\) satisfies

\[\min_{t\leq T}\|\nabla f(w_{t})\|^{2} \leq \frac{\left(\frac{2(f(w_{0})-f_{*})}{\sqrt{\eta_{0}}\beta}+\sum_ {k=1}^{d}b_{0}[k]\right)^{2}}{T+1},\]

where \(\eta_{0}\coloneqq\min_{k\in[d]}\eta[k]\).

Discussion on Theorem 3.3.Theorem 3.3 establishes an \(\mathcal{O}\left(\nicefrac{{1}}{{T}}\right)\) convergence rate for \(\mathsf{KATE}\), which is optimal for finding a first-order stationary point of a non-convex problem (Carmon et al., 2020). However, this result is not parameter-free. To prove the convergence, we assume that \(\nu_{0}[k]\leq\frac{1}{L},\ \forall k\in[d]\) in Theorem 3.3, which is equivalent to \(\beta\sqrt{1+\eta_{0}\left(\nabla_{k}f(w_{0})\right)^{2}}\leq\nicefrac{{( \nabla_{k}f(w_{0}))^{2}}}{L},\ \forall k\in[d]\). Note that the later condition holds for sufficiently small (dependent on \(L\)) values of \(\beta,\eta_{0}>0\).

However, it is possible to derive a parameter-free version of Theorem 3.3. Indeed, Lemma 2.2 implies that the step sizes are decreasing. Therefore, we can break down the analysis of \(\mathsf{KATE}\) into two phases: Phase I when \(\nu_{0}[k]>\nicefrac{{1}}{{L}}\) and Phase II when \(\nu_{0}[k]\leq\nicefrac{{1}}{{L}}\), when the current analysis works, and then follow the proof techniques of Ward et al. (2020) and Xie et al. (2020). We leave this extension as a possible future direction of our work.

Stochastic setting.Next, we present the convergence guarantees for \(\mathsf{KATE}\) in the stochastic case, when we can access an unbiased gradient estimate \(g_{t}\) with non-zero noise.

**Theorem 3.4**.: Suppose \(f\) satisfy Assumption 3.1 and \(g_{t}\) is an unbiased estimator of \(\nabla f(w_{t})\) such that \(\mathsf{BV}\) holds. Moreover, we assume \(\|\nabla f(w_{t})\|^{2}\leq\gamma^{2}\) for all \(t\). Then the iterates of \(\mathsf{KATE}\) satisfy

\[\min_{t\leq T}\mathbb{E}\left[\|\nabla f(w_{t})\|\right]\leq\left(\frac{\|g_{ 0}\|}{T}+\frac{2(\gamma+\sigma)}{\sqrt{T}}\right)^{\nicefrac{{1}}{{2}}}\sqrt {\frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}},\]

where \(\eta_{0}\coloneqq\min_{k\in[d]}\eta[k]\) and

\[\mathcal{C}_{f} \coloneqq f(w_{0})-f_{*}+2\beta\sigma\sum_{k=1}^{d}\sqrt{\eta[k]}\log \left(\frac{e(\sigma^{2}+\gamma^{2})T}{g_{0}^{2}[k]}\right)\] \[+\!\!\sum_{k=1}^{d}\left(\frac{\beta^{2}\eta[k]L}{2}+\frac{\beta^ {2}L}{2g_{0}^{2}[k]}\right)\log\left(\frac{e(\sigma^{2}+\gamma^{2})T}{g_{0}^{ 2}[k]}\right).\]

Comparison with prior work.Theorem 3.4 shows an \(\mathcal{O}(\nicefrac{{\log^{1/2}T}}{T}\nicefrac{{1}}{{4}})\) convergence rate for \(\mathsf{KATE}\) with respect to the metric \(\min_{t\leq T}\mathbb{E}\left[\|\nabla f(w_{t})\|\right]\) for the stochastic setting. Note that, in the stochastic setting, \(\mathsf{KATE}\) achieves a slower rate than Theorem 3.3 due to noise accumulation. Up to the logarithmic factor, this rate is optimal (Arieyani et al., 2023). Similar rates for the same metric follow from the results6 of (Defossez et al., 2020) for \(\mathsf{AdaGrad}\) and \(\mathsf{Adam}\).

Footnote 6: Defossez et al. (2020) derive \(\mathcal{O}(\nicefrac{{\log T}}{{\sqrt{T}}})\) convergence rates for \(\mathsf{AdaGrad}\) and \(\mathsf{Adam}\) in terms of \(\min_{t\leq T}\mathbb{E}\left[\|\nabla f(w_{t})\|^{2}\right]\) which is not smaller than \(\min_{t\leq T}\left(\mathbb{E}\left[\|\nabla f(w_{t})\|\right]\right)^{2}\).

Finally, Li and Orabona (2019) considers a variant of \(\mathsf{AdaGrad}\) closely related to \(\mathsf{KATE}\):

\[w_{t+1}=w_{t}-\frac{\beta g_{t}}{\left(\operatorname{diag}\left(\Delta I+\sum_ {\tau=1}^{t-1}\!g_{\tau}g_{\tau}^{\top}\right)\right)^{\frac{1}{2}+\varepsilon}},\] (16)

for some \(\varepsilon\in[0,\nicefrac{{1}}{{2}})\) and \(\Delta>0\). It differs from \(\mathsf{AdaGrad}\) in two key aspects: the denominator of the stepsize does not contain the last stochastic gradient, and also, instead of the square root of the sum of squared gradients, this sum is taken in the power of \(\nicefrac{{1}}{{2}}+\varepsilon\). However, the results from Li and Orabona (2019) do not imply convergence for the case of \(\varepsilon=\nicefrac{{1}}{{2}}\), which is expected since, in this case, the stepsize converges to zero too quickly in general. To compensate for such a rapid decrease, in \(\mathsf{KATE}\), we introduce an increasing sequence \(m_{t}\) in the numerator of the stepsize.

Proof technique.Compared to the AdaGrad, KATE uses more aggressive steps (the larger numerator of KATE due to the extra term \(\sum_{\tau=0}^{t}g_{t}^{\sigma_{t}^{2}[k]/b_{\tau}^{2}[k]}\)). Therefore, we expect KATE to have better empirical performance. However, introducing \(\sum_{\tau=0}^{t}g_{t}^{\sigma_{t}^{2}[k]/b_{\tau}^{2}[k]}\) in the numerator raises additional technical difficulties in the proof technique. Fortunately, as we rigorously show, the KATE steps \(\nu_{t}[k]\) retain some of the critical properties of AdaGrad steps. For instance, they (i) are lower bounded by AdaGrad steps up to a constant, (ii) decrease with iteration \(t\) (Lemma 2.2), and (iii) have closed-form upper bounds for \(\sum_{t=0}^{T}\nu_{t}^{2}[k]g_{t}^{2}[k]\). These are indeed the primary building blocks of our proof technique.

## 4 Numerical Experiments

In this section, we implement KATE in several machine learning tasks to evaluate its performance. To ensure transparency and facilitate reproducibility, we provide an access to the source code for all of our experiments at https://github.com/naraya/KATE.

### Logistic Regression

In this section, we consider the logistic regression model

\[\min_{w\in\mathbb{R}^{d}}f(w)=\frac{1}{n}\sum_{i=1}^{n}\log\left(1+\exp\left(- y_{i}x_{i}^{\top}w\right)\right),\] (17)

to elaborate on the scale-invariance and robustness of KATE for various initializations. For the experiments of this Section 4.1, we used Mac mini (M1, 2020), RAM 8 GB and storage 256 GB. Each of these plots took about 20 minutes to run.

#### 4.1.1 Robustness of Kate

To conduct this experiment, we set the total number of samples to \(1000\) (i.e. \(n=1000\)). Here, we simulate the independent vectors \(x_{i}\in\mathbb{R}^{20}\) such that each entry is from \(\mathcal{N}(0,1)\). Moreover, we generate a diagonal matrix \(V\in\mathbb{R}^{20\times 20}\) such that \(\log V_{kk}\stackrel{{\mathrm{iid}}}{{\sim}}\text{Unif}(-10,10), \ \forall k\in[20]\). Similarly, we generate \(w^{*}\in\mathbb{R}^{20}\) with each component from \(\mathcal{N}(0,1)\) and set the labels

\[y_{i}=\begin{cases}1,&x_{i}^{\top}Vw^{*}\geq 0,\\ -1,&x_{i}^{\top}Vw^{*}<0,\end{cases}\quad\forall i\in[n].\]

We compare KATE's performance with four other algorithms: AdaGrad, AdaGradNorm, SGD-decay and SGD-constant, similar to the section 5.1 of Ward et al. (2020). For each algorithm, we initialize with \(w_{0}=0\in\mathbb{R}^{20}\) and independently draw a sample of mini-batch size \(10\) to update the weight vector \(w_{t}\). We compare the algorithms \(\bullet\) AdaGrad with stepsize \(\frac{\beta}{\sqrt{\Delta+\sum_{\tau=0}^{t}g_{t}^{2}}}\), \(\bullet\) AdaGradNorm with step size \(\frac{\beta}{\sqrt{\Delta+\sum_{\tau=0}^{t}\|g_{\tau}\|^{2}}}\), \(\bullet\) SGD-decay with stepsize \(\nicefrac{{\beta}}{{\Delta\sqrt{t+1}}}\), and \(\bullet\) SGD-constant with step size \(\nicefrac{{\beta}}{{\Delta}}\). Similarly, for KATE we use stepsize \(\frac{\beta m_{t}}{b_{t}^{2}}\) where \(m_{t}^{2}=\eta b_{t}^{2}+\sum_{\tau=0}^{t}g_{\tau}^{2}/b_{\tau}^{2}\) and \(b_{t}^{2}=\Delta+\sum_{\tau=0}^{t}g_{\tau}^{2}\). Here, we choose \(\beta=f(w_{0})-f(w^{*})\) and vary \(\Delta\) in \(\{10^{-8},10^{-6},10^{-4},10^{-2},1,10^{2},10^{4},10^{6},10^{8}\}\).

In Figures 0(a), 0(b), and 0(c), we plot the functional value \(f(w_{t})\) (on the \(y\)-axis) after \(10^{4},5\times 10^{4}\), and \(10^{5}\) iterations, respectively. In theory, the convergence of SGD requires the knowledge of smoothness constant \(L\). Therefore, when the \(\Delta\) is small (hence the stepsize is large), SGD-decay and SGD-constant diverge. However, the adaptive algorithms KATE, AdaGrad, and AdaGradNorm can auto-tune themselves and converge for a wide range of \(\Delta\)s (even when the \(\Delta\) is too small). As we observe in Figure 1, when the \(\Delta\) is small, KATE outperforms all other algorithms. For instance, when \(\Delta=10^{-8}\), KATE achieves a functional value of \(10^{-3}\) after only \(10^{4}\) iterations (see Figure 0(a)), while other algorithms fail to achieve this even after \(10^{5}\) iterations (see Figure 0(c)). Furthermore, KATE performs as well as AdaGrad and better than other algorithms when the \(\Delta\) is large. _In particular, this experiment highlights that_ KATE_ is robust to initialization \(\Delta\).

#### 4.1.2 Performance of Kate on Real Data

In this section, we examine KATE's performance on real data. We test KATE on three datasets: heart, australian, and splice from the LIBSVM library (Chang and Lin, 2011). The response variables \(y_{i}\) of each of these datasets contain two classes, and we use them for binary classification tasks using a logistic regression model (17). We take \(\eta=\nicefrac{{1}}{{(\nabla f_{(w_{0})})^{2}}}\) for \(\mathsf{KATE}\) and tune \(\beta\) in all the experiments. For tuning \(\beta\), we do a grid search on the list \(\{10^{-10},10^{-8},10^{-6},10^{-4},10^{-2},1\}\). Similarly, we tune stepsizes for other algorithms. We take \(5\) trials for each of these algorithms and plot the mean of their trajectories.

We plot the functional value \(f(w_{t})\) (i.e. loss function) in Figures 1(a), 1(b) and 1(c), whereas Figures 1(d), 1(e) and 1(f) plot the corresponding accuracy of the weight vector \(w_{t}\) on the \(y\)-axis for \(5,000\) iterations. We observe that \(\mathsf{KATE}\) performs superior to all other algorithms, even on real datasets.

### Training of Neural Networks

In this section, we compare the performance of \(\mathsf{KATE}\), \(\mathsf{AdaGrad}\) and \(\mathsf{Adam}\) on two tasks, i.e. training ResNet18 (He et al., 2016) on the CIFAR10 dataset (Krizhevsky and Hinton, 2009) and BERT (Devlin et al., 2018) fine-tuning on the emotions dataset (Saravia et al., 2018) from the Hugging Face Hub. We use internal cluster with the following hardware: AMD EPYC 7552 48-Core Processor, 512GiB RAM, NVIDIA A100 40GB GPU, 200gb user storage space.

General comparison.We choose standard parameters for \(\mathsf{Adam}\) (\(\beta_{1}=0.9\) and \(\beta_{2}=0.999\)) that are default values in PyTorch and select the learning rate of \(10^{-5}\) for all considered methods. We run \(\mathsf{KATE}\) with different values of \(\eta\in\{0,10^{-1},10^{-2}\}\). For the image classification task, we normalize the images (similar to Horvath and Richtarik (2020)) and use a mini-batch size of 500. For the BERT fine-tuning, we use a mini-batch size 160 for all methods.

Figures 3-8 report the evolution of top-1 accuracy and cross-entropy loss (on the \(y\)-axis) calculated on the test data. For the image classification task, we observe that \(\mathsf{KATE}\) with different choices of

Figure 1: Comparison of \(\mathsf{KATE}\) with \(\mathsf{AdaGrad}\), \(\mathsf{AdaGradNorm}\), SGD-decay and SGD-constant for different values of \(\Delta\) (on \(x\)-axis for logistic regression model. Figure 0(a), 0(b) and 0(c) plots the functional value \(f(w_{t})\) (on \(y\)-axis) after \(10^{4},5\times 10^{4}\), and \(10^{5}\) iterations respectively.

Figure 2: Comparison of \(\mathsf{KATE}\) with \(\mathsf{AdaGrad}\), \(\mathsf{AdaGradNorm}\), SGD-decay and SGD-constant on datasets heart, australian, and splice from LIBSVM. Figures 1(a), 1(b) and 1(c) plot the functional value \(f(w_{t})\), while 1(d), 1(e) and 1(f) plot the accuracy on \(y\)-axis for \(5,000\) iterations.

\(\eta\) outperforms Adam and AdaGrad. Finally, we also observe that KATE performs comparably to Adam on the BERT fine-tuning task and is better than AdaGrad. These preliminary results highlight the potential of KATE to be applied for training neural networks for different tasks. For BERT each run takes about 35 minutes, and 25 minutes for ResNet.

Hyper-parameters tuning.Next, we compare baselines presented in Saravia et al. (2018) for emotions classification and Zhang et al. (2019) for image classification. These papers provide efficient setups for learning rates and learning rate schedulers that are reasonable to compare with. Saravia et al. (2018) performs a search of efficient learning rate and uses a linear learning rate scheduler with warmup for Adam optimizer. A different learning rate (1e-5), \(\Delta\)=1e-5 and the same scheduler applied for KATE lead to the same performance, see Figure 9. We would like to point out that it is challenging to find a reference for hyper-parameters for a certain setup. Thus, to fairly compare with Saravia et al. (2018) we use distiloberta-base model. Zhang et al. (2019) did a grid search for an efficient learning rate and used a multi-step scheduler for Adam optimizer, decaying the learning rate by a factor of 5 at the 60th, 120th, and 160th epochs. Zhang et al. (2019) refers to DeVries and Taylor (2017) for the code implementing special techniques, namely data augmentation and cutout to achieve higher accuracy. A different learning rate (1e-3), the same scheduler and \(\Delta\)=1e-3 applied for KATE demonstrates comparable performance, see Figure 10. For BERT each run takes about 20 minutes, while 100 minutes for ResNet.

Figure 10: Emotion: \(\eta=0.001\)

## Acknowledgments and Disclosure of Funding

We thank Francesco Orabona and Dmitry Kamzolov for the pointers to the related works that we missed while preparing the first version of this paper. We also thank anonymous reviewers for their useful feedback and suggestions.

## References

* Abdukhakimov et al. (2023) Abdukhakimov, F., Xiang, C., Kamzolov, D., Gower, R., and Takac, M. (2023). Sania: Polyak-type optimization framework leads to scale invariant stochastic algorithms. _arXiv preprint arXiv:2312.17369_.
* Abdukhakimov et al. (2024) Abdukhakimov, F., Xiang, C., Kamzolov, D., and Takac, M. (2024). Stochastic gradient descent with preconditioned polyak step-size. _Computational Mathematics and Mathematical Physics_, 64(4):621-634.
* Agresti (2015) Agresti, A. (2015). _Foundations of linear and generalized linear models_. John Wiley & Sons.
* Arjevani et al. (2023) Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2023). Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, 199(1-2):165-214.
* Beznosikov and Takac (2021) Beznosikov, A. and Takac, M. (2021). Random-reshuffled sarah does not need a full gradient computations. In _Optimization for Machine Learning Workshop @ NeurIPS 2021_.
* Carmon et al. (2020) Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points i. _Mathematical Programming_, 184(1-2):71-120.
* Cesa-Bianchi et al. (2005) Cesa-Bianchi, N., Mansour, Y., and Stoltz, G. (2005). Improved second-order bounds for prediction with expert advice. In _International Conference on Computational Learning Theory_, pages 217-232. Springer.
* Cesa-Bianchi et al. (2007) Cesa-Bianchi, N., Mansour, Y., and Stoltz, G. (2007). Improved second-order bounds for prediction with expert advice. _Machine Learning_, 66:321-352.
* Chang and Lin (2011) Chang, C.-C. and Lin, C.-J. (2011). Libsvm: a library for support vector machines. _ACM transactions on intelligent systems and technology (TIST)_, 2(3):1-27.
* Chezhegov et al. (2024) Chezhegov, S., Skorik, S., Khachaturov, N., Shalagin, D., Avetisyan, A., Beznosikov, A., Takac, M., Kholodov, Y., and Gasnikov, A. (2024). Local methods with adaptivity via scaling. _arXiv preprint arXiv:2406.00846_.
* d'Aspremont et al. (2018) d'Aspremont, A., Guzman, C., and Jaggi, M. (2018). Optimal affine-invariant smooth minimization algorithms. _SIAM Journal on Optimization_, 28(3):2384-2405.
* Defazio and Mishchenko (2023) Defazio, A. and Mishchenko, K. (2023). Learning-rate-free learning by D-adaptation. _arXiv preprint arXiv:2301.07733_.
* Defossez et al. (2020) Defossez, A., Bottou, L., Bach, F., and Usunier, N. (2020). A simple convergence proof of adam and adagrad. _arXiv preprint arXiv:2003.02395_.
* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.
* DeVries and Taylor (2017) DeVries, T. and Taylor, G. W. (2017). Improved regularization of convolutional neural networks with cutout. _arXiv preprint arXiv:1708.04552_.
* D'Orazio et al. (2021) D'Orazio, R., Loizou, N., Laradji, I., and Mitliagkas, I. (2021). Stochastic mirror descent: Convergence analysis and adaptive variants via the mirror stochastic polyak stepsize. _arXiv preprint arXiv:2110.15412_.
* Duchi et al. (2011) Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7).
* Faw et al. (2022) Faw, M., Tziotis, I., Caramanis, C., Mokhtari, A., Shakkottai, S., and Ward, R. (2022). The power of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance. In _Conference on Learning Theory_, pages 313-355. PMLR.
* Frome (1983) Frome, E. L. (1983). The analysis of rates using poisson regression models. _Biometrics_, pages 665-674.
* Gower et al. (2021) Gower, R. M., Defazio, A., and Rabbat, M. (2021). Stochastic polyak stepsize with a moving target. _arXiv preprint arXiv:2106.11851_.
* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778.
* He et al. (2017)He, X., Tappenden, R., and Takac, M. (2018). Dual free adaptive minibatch sdca for empirical risk minimization. _Frontiers in Applied Mathematics and Statistics_, 4:33.
* Horvath and Richtarik (2020) Horvath, S. and Richtarik, P. (2020). A better alternative to error feedback for communication-efficient distributed learning. _arXiv preprint arXiv:2006.11077_.
* Hosmer Jr et al. (2013) Hosmer Jr, D. W., Lemeshow, S., and Sturdivant, R. X. (2013). _Applied logistic regression_, volume 398. John Wiley & Sons.
* Kingma and Ba (2014) Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.
* Krizhevsky and Hinton (2009) Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.
* Li et al. (2023) Li, S., Swartworth, W. J., Takac, M., Needell, D., and Gower, R. M. (2023). Sp2: A second order stochastic polyak method. _ICLR_.
* Li and Orabona (2019) Li, X. and Orabona, F. (2019). On the convergence of stochastic gradient descent with adaptive stepsizes. In _The 22nd international conference on artificial intelligence and statistics_, pages 983-992. PMLR.
* Liu et al. (2022) Liu, Z., Nguyen, T. D., Ene, A., and Nguyen, H. L. (2022). On the convergence of adagrad on \(\mathbb{R}^{d}\): Beyond convexity, non-asymptotic rate and acceleration. _arXiv preprint arXiv:2209.14827_.
* Loizou et al. (2021) Loizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien, S. (2021). Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In _International Conference on Artificial Intelligence and Statistics_, pages 1306-1314. PMLR.
* McMahan and Streeter (2010) McMahan, H. B. and Streeter, M. (2010). Adaptive bound optimization for online convex optimization. _arXiv preprint arXiv:1002.4908_.
* Mishchenko and Defazio (2023) Mishchenko, K. and Defazio, A. (2023). Prodigy: An expeditiously adaptive parameter-free learner. _arXiv preprint arXiv:2306.06101_.
* Nelder and Wedderburn (1972) Nelder, J. A. and Wedderburn, R. W. (1972). Generalized linear models. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 135(3):370-384.
* Nesterov (2018) Nesterov, Y. (2018). _Lectures on convex optimization_, volume 137. Springer.
* Nesterov and Nemirovskii (1994) Nesterov, Y. and Nemirovskii, A. (1994). _Interior-point polynomial algorithms in convex programming_. SIAM.
* Nguyen et al. (2017a) Nguyen, L., Liu, J., Scheinberg, K., and Takac, M. (2017a). Sarah: A novel method for machine learning problems using stochastic recursive gradient. In _In 34th International Conference on Machine Learning, ICML 2017_.
* Nguyen et al. (2017b) Nguyen, L. M., Liu, J., Scheinberg, K., and Takac, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. _arXiv preprint arXiv:1705.07261_.
* Nguyen et al. (2018) Nguyen, L. M., Nguyen, P. H., van Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). Sgd and hogwild! convergence without the bounded gradients assumption. In _In 34th International Conference on Machine Learning, ICML 2018_.
* Nguyen et al. (2021) Nguyen, L. M., Scheinberg, K., and Takac, M. (2021). Inexact sarah algorithm for stochastic optimization. _Optimization Methods and Software_, 36(1):237-258.
* Oberman and Prazeres (2019) Oberman, A. M. and Prazeres, M. (2019). Stochastic gradient descent with polyak's learning rate. _arXiv preprint arXiv:1903.08688_.
* Orabona et al. (2015) Orabona, F., Crammer, K., and Cesa-Bianchi, N. (2015). A generalized online mirror descent with applications to classification and regression. _Machine Learning_, 99:411-435.
* Orabona and Pal (2015) Orabona, F. and Pal, D. (2015). Scale-free algorithms for online linear optimization. In _International Conference on Algorithmic Learning Theory_, pages 287-301. Springer.
* Orabona and Pal (2018) Orabona, F. and Pal, D. (2018). Scale-free online learning. _Theoretical Computer Science_, 716:50-69.
* Orvieto et al. (2022) Orvieto, A., Lacoste-Julien, S., and Loizou, N. (2022). Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. _Advances in Neural Information Processing Systems_, 35:26943-26954.
* Polyak (1969) Polyak, B. T. (1969). Minimization of unsmooth functionals. _USSR Computational Mathematics and Mathematical Physics_, 9(3):14-29.
* Polyak (1969)Reddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence of adam and beyond. _arXiv preprint arXiv:1904.09237_.
* Robbins and Monro (1951) Robbins, H. and Monro, S. (1951). A stochastic approximation method. _The annals of mathematical statistics_, pages 400-407.
* Saravia et al. (2018) Saravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y.-S. (2018). CARER: Contextualized affect representations for emotion recognition. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3687-3697, Brussels, Belgium. Association for Computational Linguistics.
* Shalev-Shwartz and Ben-David (2014) Shalev-Shwartz, S. and Ben-David, S. (2014). _Understanding machine learning: From theory to algorithms_. Cambridge university press.
* Shi et al. (2023) Shi, Z., Sadiev, A., Loizou, N., Richtarik, P., and Takac, M. (2023). AI-SARAH: Adaptive and implicit stochastic recursive gradient methods. _Transactions on Machine Learning Research_.
* Takac et al. (2013) Takac, M., Bijral, A., Richtarik, P., and Srebro, N. (2013). Mini-batch primal and dual methods for svms. In _In 30th International Conference on Machine Learning, ICML 2013_.
* Tieleman and Hinton (2012) Tieleman, T. and Hinton, G. (2012). Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. _COURSERA Neural Networks Mach. Learn_, 17.
* Ward et al. (2020) Ward, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex landscapes. _The Journal of Machine Learning Research_, 21(1):9047-9076.
* Xie et al. (2020) Xie, Y., Wu, X., and Ward, R. (2020). Linear convergence of adaptive stochastic gradient descent. In _International conference on artificial intelligence and statistics_, pages 1475-1485. PMLR.
* Zhang et al. (2019) Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. (2019). Lookahead optimizer: k steps forward, 1 step back. _Advances in neural information processing systems_, 32.

## Supplementary Material

###### Contents

* 1 Introduction
	* 1.1 Related Work
	* 1.2 Main Contribution
	* 1.3 Notation
* 2 Motivation and Algorithm Design
* 3 Convergence Analysis
	* 3.1 Assumptions
	* 3.2 Main Results
* 4 Numerical Experiments
	* 4.1 Logistic Regression
		* 4.1.1 Robustness of KATE
		* 4.1.2 Performance of KATE on Real Data
	* 4.2 Training of Neural Networks
* A Technical Lemmas
* B Proof of Main Results
* B.1 Proof of Proposition 2.1
* B.2 Proof of Lemma 2.2
* B.3 Proof of Theorem 3.3
* B.4 Proof of Theorem 3.4
* C Additional Experiments: Scale-Invariance Verification

## Appendix A Technical Lemmas

**Lemma A.1** (Am-Gm).: For \(\lambda>0\) we have

\[ab\leq\frac{\lambda}{2}a^{2}+\frac{1}{2\lambda}b^{2}.\] (18)

**Lemma A.2** (Cauchy-Schwarz Inequality).: For \(a_{1},\cdots,a_{n},b_{1},\cdots,b_{n}\in\mathbb{R}\) we have

\[\left(\sum_{i=1}^{n}a_{i}^{2}\right)\left(\sum_{i=1}^{n}b_{i}^{2}\right) \geq \left(\sum_{i=1}^{n}a_{i}b_{i}\right)^{2}.\] (19)

**Lemma A.3** (Holder's Inequality).: Suppose \(X,Y\) are two random variables and \(p,q>1\) satisfy \(\frac{1}{p}+\frac{1}{q}=1\). Then

\[\mathbb{E}\left(|XY|\right)\leq\left(\mathbb{E}\left(|X|^{p}\right)\right)^{ \frac{1}{p}}\left(\mathbb{E}\left(|Y|^{q}\right)\right)^{\frac{1}{q}}.\] (20)

**Lemma A.4** (Jensen's Inequality).: For a convex function \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and a random variable \(X\) such that \(\mathbb{E}(\Psi(X))\) and \(\Psi\) (\(\mathbb{E}(X)\)) are finite, we have

\[\Psi\left(\mathbb{E}(X)\right)\leq\mathbb{E}(\Psi(X)).\] (21)

**Lemma A.5**.: For \(a_{1},a_{2},\cdots,a_{n}\geq 0\) and \(b_{1},b_{2},\cdots,b_{n}>0\) we have

\[\sum_{i=1}^{n}\frac{a_{i}}{\sqrt{b_{i}}}\geq\frac{\sum_{i=1}^{n}a_{i}}{\sqrt{ \sum_{i=1}^{n}b_{i}}}.\] (22)

Proof.: Expanding the LHS of (22) we get

\[\left(\sum_{i=1}^{n}\frac{a_{i}}{\sqrt{b_{i}}}\right)^{2} = \sum_{i=1}^{n}\frac{a_{i}^{2}}{b_{i}}+2\sum_{i\neq j}\frac{a_{i}a _{j}}{\sqrt{b_{i}b_{j}}}\] (23) \[\geq \sum_{i=1}^{n}\frac{a_{i}^{2}}{b_{i}}.\]

The last inequality follows from \(\frac{a_{i}}{\sqrt{b_{i}}}\geq 0\) for all \(i\in[n]\). Now, using Cauchy-Schwarz Inequality (19), we have

\[\left(\sum_{i=1}^{n}\frac{a_{i}^{2}}{b_{i}}\right)\left(\sum_{i=1}^{n}b_{i} \right) \geq \left(\sum_{i=1}^{n}a_{i}\right)^{2}.\] (24)

Then combining (23) and (24), we get

\[\left(\sum_{i=1}^{n}\frac{a_{i}}{\sqrt{b_{i}}}\right)^{2}\left( \sum_{i=1}^{n}b_{i}\right) \geq \left(\sum_{i=1}^{n}a_{i}\right)^{2}.\]

Finally dividing both sides by \(\sum_{i=1}^{n}b_{i}\) and taking square root we get the desired result.

[MISSING_PAGE_EMPTY:16]

Then from (26) we have

\[\mathbb{E}_{t}\left[\left(\frac{\beta\sqrt{\eta[k]}}{\sqrt{b_{t-1}^{2} [k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}-\nu_{t}[k]\right)\nabla_{k} f(w_{t})g_{t}[k]\right]\] (27) \[\leq \underbrace{\beta\sqrt{\eta[k]}\mathbb{E}_{t}\left[\frac{\left|g_{ t}[k]-\nabla_{k}f(w_{t})\right||\nabla_{k}f(w_{t})||g_{t}[k]|}{b_{t}[k]\sqrt{b_{t-1}^{2} [k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\right]}_{\text{term I}}\] \[+\underbrace{\beta\sqrt{\eta[k]}\mathbb{E}_{t}\left[\frac{\sigma \left|\nabla_{k}f(w_{t})\right||g_{t}[k]|}{b_{t}[k]\sqrt{b_{t-1}^{2}[k]+\left( \nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\right]}_{\text{term II}}.\]

For term I in (27), we use Lemma A.1 with

\[\lambda = \frac{2\sigma^{2}}{\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t}) \right)^{2}+\sigma^{2}}},\] \[a = \frac{\left|g_{t}[k]\right|}{b_{t}[k]},\] \[b = \frac{\left|g_{t}[k]-\nabla_{k}f(w_{t})\right||\nabla_{k}f(w_{t}) \right|}{\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}},\]

to get

\[\beta\sqrt{\eta[k]}\mathbb{E}_{t}\left[\frac{\left|g_{t}[k]-\nabla _{k}f(w_{t})\right||\nabla_{k}f(w_{t})||g_{t}[k]|}{b_{t}[k]\sqrt{b_{t-1}^{2}[k ]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\right]\] (28) \[\leq \frac{\beta\sqrt{\eta[k]}\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w _{t})\right)^{2}+\sigma^{2}}}{4\sigma^{2}}\frac{\left(\nabla_{k}f(w_{t}) \right)^{2}\mathbb{E}_{t}\left[g_{t}[k]-\nabla_{k}f(w_{t})\right]^{2}}{b_{t-1} ^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}\] \[+\frac{\beta\sqrt{\eta[k]}\sigma^{2}}{\sqrt{b_{t-1}^{2}[k]+\left( \nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\mathbb{E}_{t}\left[\frac{g_{t}^{2}[ k]}{b_{t}^{2}[k]}\right]\] \[\leq \frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{4 \sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}+\beta \sqrt{\eta[k]}\sigma\mathbb{E}_{t}\left[\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]} \right].\]

The last inequality follows from BV. Similarly, we again use Lemma A.1 with

\[\lambda = \frac{2}{\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2} +\sigma^{2}}},\] \[a = \frac{\sigma\left|g_{t}[k]\right|}{b_{t}[k]},\] \[b = \frac{\left|\nabla_{k}f(w_{t})\right|}{\sqrt{b_{t}^{2}[k]+\left( \nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\]

and \(\sqrt{b_{t}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}\geq\sigma\) to get

\[\beta\sqrt{\eta[k]}\mathbb{E}_{t}\left[\frac{\sigma\left|\nabla _{k}f(w_{t})\right||g_{t}[k]|}{b_{t}[k]\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f( w_{t})\right)^{2}+\sigma^{2}}}\right] \leq \beta\sqrt{\eta[k]}\sigma\mathbb{E}_{t}\left[\frac{g_{t}^{2}[k]}{ b_{t}^{2}[k]}\right]\] (29) \[+\frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{4 \sqrt{b_{t-1}^{2}[k]+\left(\nabla f(w_{t})\right)^{2}+\sigma^{2}}}.\]Therefore using (28) and (29) in (28) we get

\[\mathbb{E}_{t}\left[\left(\frac{\beta\sqrt{\eta[k]}}{\sqrt{b_{t-1}^{ 2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}-\nu_{t}[k]\right)\nabla_{ k}f(w_{t})g_{t}[k]\right] \leq 2\beta\sqrt{\eta[k]}\sigma\mathbb{E}_{t}\left[\frac{g_{t}^{2}[k]}{ b_{t}^{2}[k]}\right]\] \[+\frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{2 \sqrt{b_{t-1}^{2}[k]+\left(\nabla f(w_{t})\right)^{2}+\sigma^{2}}}.\]

This completes the proof of this Lemma. 

**Lemma A.7**.: \[\sum_{t=0}^{T}\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\leq\log\left(\frac{b_{T}^{2}[k ]}{b_{0}^{2}[k]}\right)+1\] (30)

Proof.: Using \(b_{t}^{2}[k]=\sum_{\tau=0}^{t}g_{\tau}^{2}[k]\) we have

\[\sum_{t=0}^{T}\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]} = 1+\sum_{t=1}^{T}\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\] \[= 1+\sum_{t=1}^{T}\frac{b_{t}^{2}[k]-b_{t-1}^{2}[k]}{b_{t}^{2}[k]}\] \[= 1+\sum_{t=1}^{T}\frac{1}{b_{t}^{2}[k]}\int_{b_{t-1}^{2}[k]}^{b_{t }^{2}[k]}dz\] \[\leq 1+\sum_{t=1}^{T}\int_{b_{t-1}^{2}[k]}^{b_{t}^{2}[k]}\frac{dz}{z}\] \[= 1+\int_{b_{0}^{2}[k]}^{b_{T}^{2}[k]}\frac{dz}{z}\] \[= 1+\log\left(\frac{b_{T}^{2}[k]}{b_{0}^{2}[k]}\right).\]

The inequality follows from the fact \(\frac{1}{b_{t}^{2}[k]}\leq\frac{1}{z}\) when \(b_{t-1}^{2}[k]\leq z\leq b_{t}^{2}[k]\). This completes the proof of the Lemma.

Proof of Main Results

### Proof of Proposition 2.1

**Proposition B.1** (Scale invariance).: Suppose we solve problems (4) and (5) using algorithm (6). Then, the iterates \(\hat{w}_{t}\) and \(\hat{w}_{t}^{V}\) corresponding to (4) and (5) follow: \(\forall k\in[d]\)

\[\hat{w}_{t+1}[k] = \hat{w}_{t}[k]-\frac{\delta m_{t}[k]}{\sum_{r=0}^{t}g_{r}^{T}[k]}g _{t}[k],\] (31) \[\hat{w}_{t+1}^{V}[k] = \hat{w}_{t}^{V}[k]-\frac{\delta m_{t}[k]}{\sum_{r=0}^{t}\left(g_{ r}^{V}[k]\right)^{2}}g_{t}^{V}[k]\] (32)

with \(g_{r}=\varphi_{i_{r}}^{\prime}(x_{i_{r}}^{\top}\hat{w}_{r})x_{i_{r}}\) and \(g_{r}^{V}=\varphi_{i_{r}}^{\prime}(x_{i_{r}}^{\top}V\hat{w}_{r})Vx_{i_{r}}\), for \(i_{r}\) chosen uniformly from \([n]\), \(\tau=0,1,\ldots,t\), \(t\geq 0\). Moreover, updates (31) and (32) satisfy

\[\hat{w}_{t}=V\hat{w}_{t}^{V},\quad Vg_{t}=g_{t}^{V},\quad f\left(\hat{w}_{t} \right)=f^{V}\left(\hat{w}_{t}^{V}\right)\]

for all \(t\geq 0\) when \(\hat{w}_{0}=\hat{w}_{0}^{V}=0\in\mathbb{R}^{d}\). Furthermore we have

\[\left\|g_{t}^{V}\right\|_{V^{-2}}^{2} = \left\|g_{t}\right\|^{2}.\] (33)

Proof.: First, we will show \(\hat{w}_{t}=V\hat{w}_{t}^{V}\) and \(Vg_{t}=g_{t}^{V}\) using induction. Note that for \(\tau=1\) and \(k\in[d]\), we get

\[\hat{w}_{1}[k] = \frac{-\beta m_{0}[k]\left|\varphi_{i_{0}}^{\prime}(0)x_{i_{0}} [k]\right|}{\left(\varphi_{i_{0}}^{\prime}(0)x_{i_{0}}[k]\right)^{2}}=\frac{- \beta m_{0}[k]}{\varphi_{i_{0}}^{\prime}(0)x_{i_{0}}[k]},\] \[\hat{w}_{1}^{V}[k] = \frac{-\beta m_{0}[k]\varphi_{i_{0}}^{\prime}(0)V_{kk}x_{i_{0}}[ k]}{\left(\varphi_{i_{0}}^{\prime}(0)V_{kk}x_{i_{0}}[k]\right)^{2}}=\frac{- \beta m_{0}[k]}{\varphi_{i_{0}}^{\prime}(0)V_{kk}x_{i_{0}}[k]}.\]

as \(\hat{w}_{0}=\hat{w}_{0}^{V}=0\). Therefore, we have \(\forall k\in[d],\hat{w}_{1}[k]=V_{kk}\hat{w}_{1}^{V}[k]\). This can be equivalently written as \(\hat{w}_{1}=V\hat{w}_{1}^{V}\), as \(V\) is a diagonal matrix. Then it is easy to check

\[Vg_{1}=\varphi_{i_{1}}^{\prime}\left(x_{i_{1}}^{\top}\hat{w}_{1}\right)Vx_{i_ {1}}=\varphi_{i_{1}}^{\prime}\left(x_{i_{1}}^{\top}V\hat{w}_{1}^{V}\right)Vx_{ i_{1}}=g_{1}^{V},\] (34)

where the second equality follows from \(\hat{w}_{1}=V\hat{w}_{1}^{V}\). Now, we assume the proposition holds for \(\tau=1,\cdots,t\). Then, we need to prove this proposition for \(\tau=t+1\). Note that, from (7) we have

\[\hat{w}_{t+1}[k]=\hat{w}_{t}[k]-\frac{\beta m_{t}[k]}{\sum_{r=0}^{t}\theta_{ \overline{[k]}}^{2}}g_{t}[k]=V_{kk}\hat{w}_{t}^{V}[k]-\frac{\beta m_{t}[k]V_{ kk}^{2}}{\sum_{t=0}^{t}\left(g_{r}^{V}[k]\right)^{2}}\frac{g_{r}^{V}[k]}{V_{kk}}=V_{kk }\hat{w}_{t+1}^{V}[k].\]

Here, the second last equality follows from \(\hat{w}_{r}=V\hat{w}_{r}^{V}\) and \(Vg_{r}=g_{r}^{V}\quad\forall\tau\in[t]\), while the last equality holds due to (32). Therefore, we have \(\hat{w}_{t+1}=V\hat{w}_{t+1}^{V}\). Then similar to (34) we get \(Vg_{t+1}=g_{t+1}^{V}\) using \(\hat{w}_{t+1}=V\hat{w}_{t+1}^{V}\). Again, using \(\hat{w}_{t}=V\hat{w}_{t}^{V}\), we can rewrite \(f(\hat{w}_{t})\) as follow

\[f(\hat{w}_{t})=\frac{1}{n}\sum_{i=1}^{n}\varphi_{i}\left(x_{i}^{\top}\hat{w}_ {t}\right)=\frac{1}{n}\sum_{i=1}^{n}\varphi_{i}\left(x_{i}^{\top}V\hat{w}_{t}^ {V}\right)=f^{V}\left(\hat{w}_{t}^{V}\right).\]

The last equality follows from (5). This proves \(f(\hat{w}_{t})=f^{V}\left(\hat{w}_{t}^{V}\right)\). Finally using \(Vg_{t}=g_{t}^{V}\) we get

\[\left\|g_{t}^{V}\right\|_{V^{-2}}^{2}=\left(g_{t}^{V}\right)^{\top}V^{-2}g_{t }^{V}=g_{t}^{\top}VV^{-2}Vg_{t}=\left\|g_{t}\right\|^{2}.\]

This completes the proof of Proposition 2.1.

### Proof of Lemma 2.2

**Lemma B.2** (Decreasing step size).: For \(\nu_{t}[k]\) defined in (11) we have

\[\nu_{t+1}[k]\leq\nu_{t}[k]\qquad\forall k\in[d].\]

Proof.: We want to show that \(\nu_{t+1}[k]\leq\nu_{t}[k]\). Taking square and rearranging the terms (13) is equivalent to proving

\[b_{t}^{4}[k]m_{t+1}^{2}[k]\leq b_{t+1}^{4}[k]m_{t}^{2}[k].\] (35)

Using the expansion of \(m_{t+1}^{2}[k],b_{t+1}^{2}[k]\), LHS of (35) can be expanded as follow

\[b_{t}^{4}[k]m_{t+1}^{2}[k] = b_{t}^{4}[k]\left(m_{t}^{2}[k]+\eta[k]g_{t+1}^{2}[k]+\frac{g_{t+ 1}^{2}[k]}{b_{t}^{2}[k]+g_{t+1}^{2}[k]}\right).\] (36)

Similarly, the RHS of (35) can be expanded to

\[b_{t+1}^{4}[k]m_{t}^{2}[k] = m_{t}^{2}[k]\left(b_{t}^{2}[k]+g_{t+1}^{2}[k]\right)^{2}\] (37) \[= m_{t}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[k]+2m_{t}^{2}[k ]g_{t+1}^{2}[k]b_{t}^{2}[k].\]

Therefore using (36) and (37), inequality (35) is equivalent to

\[b_{t}^{4}[k]\left(m_{t}^{2}[k]+\eta[k]g_{t+1}^{2}[k]+\frac{g_{t+ 1}^{2}[k]}{b_{t}^{2}[k]+g_{t+1}^{2}[k]}\right) \leq m_{t}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[k]\] (38) \[+2m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{2}[k].\]

Now subtracting \(m_{t}^{2}[k]b_{t}^{4}[k]\) from both sides of (38) and then multiplying both sides by \(b_{t}^{2}[k]+g_{t+1}^{2}[k]\), (38) is equivalent to

\[\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[ k]+g_{t+1}^{2}[k]b_{t}^{4}[k] \leq m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]+2m_{t}^{2}[k]g_{t+1}^{2}[k ]b_{t}^{4}[k]\] (39) \[+m_{t}^{2}[k]g_{t+1}^{6}[k]+2m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[ k].\]

Therefore, proving (13) is equivalent to proving (39). Note that, from the expansion \(m_{t}^{2}[k]=\eta[k]b_{t}^{2}[k]+\sum_{\tau=0}^{t}\frac{g_{t}^{2}[k]}{b_{t}^{2} [k]}\), we have \(m_{t}^{2}[k]\geq\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}=1\) and \(m_{t}^{2}[k]\geq\eta[k]b_{t}^{2}[k]\). Then using \(m_{t}^{2}[k]\geq 1\) we get

\[g_{t+1}^{4}[k]b_{t}^{2}[k]\leq m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k].\] (40)

Again, using \(m_{t}^{2}[k]\geq\eta[k]b_{t}^{2}[k]\), we have

\[\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[ k] \leq m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[ k]b_{t}^{2}[k].\] (41)

Then adding (40) and (41) we get

\[\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[ k]+g_{t+1}^{2}[k]b_{t}^{4}[k] \leq m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]+2m_{t}^{2}[k]g_{t+1}^{2}[k ]b_{t}^{4}[k].\] (42)

Therefore, (39) is true due to (42) and \(m_{t}^{2}[k]g_{t+1}^{6}[k]+2m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]\geq 0\). This completes the proof of the Lemma.

### Proof of Theorem 3.3

**Theorem B.3**.: Suppose \(f\) is \(L\)-smooth, \(g_{t}=\nabla f(w_{t})\) and \(\eta,\beta\) are chosen such that \(\nu_{0}[k]\leq\frac{1}{L}\) for all \(k\in[d]\). Then for (11) we have

\[\min_{t\leq T}\|\nabla f(w_{t})\|^{2}\leq\frac{1}{T+1}\left(\sum_{k=1}^{d}b_{0} [k]+\frac{2(f(w_{0})-f_{*})}{\sqrt{\eta}\beta}\right)^{2}.\]

Proof.: Suppose \(g_{t}=\nabla f(w_{t})\). Then using the smoothness of \(f\) we get

\[f(w_{T+1}) \leq f(w_{T})+\langle g_{T},w_{T+1}-w_{T}\rangle+\frac{L}{2}\left\|w _{T+1}-w_{T}\right\|^{2}\] \[= f(w_{T})+\sum_{k=1}^{d}g_{T}[k]\left(w_{T+1}[k]-w_{T}[k]\right) +\frac{L}{2}\sum_{k=1}^{d}\left(w_{T+1}[k]-w_{T}[k]\right)^{2}\] \[= f(w_{T})-\sum_{k=1}^{d}\nu_{T}[k]g_{T}^{2}[k]+\frac{L}{2}\sum_{ k=1}^{d}\nu_{T}^{2}[k]g_{T}^{2}[k]\] \[= f(w_{T})-\sum_{k=1}^{d}\nu_{T}[k]\left(1-\nu_{T}[k]\frac{L}{2} \right)g_{T}^{2}[k].\]

Then using this bound recursively we get

\[f(w_{T+1}) \leq f(w_{0})-\sum_{t=0}^{T}\sum_{k=1}^{d}\nu_{t}[k]\left(1-\nu_{t}[k] \frac{L}{2}\right)g_{t}^{2}[k].\]

Note that, we initialized KATE such that \(\nu_{0}[k]\leq\frac{1}{L}\forall k\in[d]\). Therefore using Lemma 2.2 we have \(\nu_{t}[k]\leq\frac{1}{L}\), which is equivalent to \(1-\nu_{t}[k]\frac{L}{2}\geq\frac{1}{2}\) for all \(k\in[d]\). Hence from (43) we have

\[f(w_{T+1}) \leq f(w_{0})-\sum_{t=0}^{T}\sum_{k=1}^{d}\frac{\nu_{t}[k]}{2}g_{t}^{2 }[k].\]

Then rearranging the terms and using \(f(w_{T+1})\geq f_{*}\) we get

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\frac{\nu_{t}[k]}{2}g_{t}^{2}[k]\leq f(w_{0})-f_{*}.\] (43)

Then from (43) and \(m_{t}[k]\geq\sqrt{\eta_{0}}b_{t}[k]\) we get

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\frac{g_{t}^{2}[k]}{b_{t}[k]}\leq\frac{2(f(w_{0})- f_{*})}{\sqrt{\eta_{0}}\beta}.\] (44)

Now from the definition of \(b_{t}^{2}[k]\), we have \(b_{t}^{2}[k]=b_{t-1}^{2}[k]+g_{t}^{2}[k]\). This can be rearranged to get

\[b_{T}[k] = b_{T-1}[k]+\frac{g_{T}^{2}[k]}{b_{T}[k]+b_{T-1}[k]}\] (45) \[\leq b_{T-1}[k]+\frac{g_{T}^{2}[k]}{b_{T}[k]}\] \[\leq b_{0}[k]+\sum_{t=0}^{T}\frac{g_{t}^{2}[k]}{b_{t}[k]}.\] (46)

Here the last inequality (46) follows from recursive use of (45). Then, taking squares on both sides and summing over \(k\in[d]\) we get

\[\sum_{k=1}^{d}b_{T}^{2}[k] \leq \sum_{k=1}^{d}\left(b_{0}[k]+\sum_{t=0}^{T}\frac{g_{t}^{2}[k]}{b _{t}[k]}\right)^{2}\] (47) \[\leq \left(\sum_{k=1}^{d}b_{0}[k]+\sum_{t=0}^{T}\sum_{k=1}^{d}\frac{g _{t}^{2}[k]}{b_{t}[k]}\right)^{2}\] \[\leq \left(\sum_{k=1}^{d}b_{0}[k]+\frac{2(f(w_{0})-f_{*})}{\sqrt{\eta_{ 0}}\beta}\right)^{2}.\]The second inequality follows from \(b_{0}[k]+\sum_{t=0}^{T}\frac{g_{t}^{2}[k]}{b_{t}[k]}\geq 0\) for all \(k\in[d]\) and the last inequality from (44). Now note that \(\sum_{t=0}^{T}\|g_{t}\|^{2}=\sum_{t=0}^{T}\sum_{k=1}^{d}g_{t}^{2}[k]=\sum_{k=1}^ {d}b_{t}^{2}[k]\). Therefore dividing both sides of (47) by \(T+1\), we get

\[\min_{t\leq T}\|\nabla f(w_{t})\|^{2}\leq\frac{1}{T+1}\left(\sum_{k=1}^{d}b_{0 }[k]+\frac{2(f(w_{0})-f_{*})}{\sqrt{\eta_{0}}\beta}\right)^{2}.\]

This completes the proof of the theorem.

### Proof of Theorem 3.4

**Theorem B.4**.: Suppose \(f\) is a \(L\)-smooth function and \(g_{t}\) is an unbiased estimator of \(\nabla f(w_{t})\) such that \(\mathsf{BV}\) holds. Moreover, we assume \(\|\nabla f(w_{t})\|^{2}\leq\gamma^{2}\) for all \(t\). Then \(\mathsf{KATE}\) satisfies

\[\min_{t\leq T}\mathbb{E}\|\nabla f(w_{t})\| \leq \left(\frac{\|g_{0}\|}{T}+\frac{2(\gamma+\sigma)}{\sqrt{T}} \right)^{1/2}\sqrt{\frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}}\]

where

\[\mathcal{C}_{f}=f(w_{0})-f_{\star}+\sum_{k=1}^{d}\left(2\beta\sqrt{\eta[k]} \sigma+\frac{\beta^{2}\eta[k]L}{2}+\frac{\beta^{2}L}{2g_{0}^{2}[k]}\right) \left(\log\left(\frac{(\sigma^{2}+\gamma^{2})T}{g_{0}^{2}[k]}\right)+1\right).\]

Proof.: Using smoothness, we have

\[f(w_{t+1}) \leq f(w_{t})+\langle\nabla f(w_{t}),w_{t+1}-w_{t}\rangle+\frac{L}{2 }\left\|w_{t+1}-w_{t}\right\|^{2}\] \[= f(w_{t})+\sum_{k=1}^{d}\nabla_{k}f(w_{t})\left(w_{t+1}[k]-w_{t} [k]\right)+\frac{L}{2}\sum_{k=1}^{d}\left(w_{t+1}[k]-w_{t}[k]\right)^{2}\] \[= f(w_{t})-\sum_{k=1}^{d}\nu_{t}[k]\nabla_{k}f(w_{t})g_{t}[k]+\frac {L}{2}\sum_{k=1}^{d}\nu_{t}^{2}[k]g_{t}^{2}[k].\]

Then, taking the expectation conditioned on \(w_{t}\), we have

\[\mathbb{E}_{t}\left[f(w_{t+1})\right] \leq f(w_{t})-\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}[k]\nabla_{k} f(w_{t})g_{t}[k]\right]+\frac{L}{2}\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}^{2}[ k]g_{t}^{2}[k]\right]\] \[= f(w_{t})-\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}[k]\nabla_{k} f(w_{t})g_{t}[k]\right]+\frac{L}{2}\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}^{2}[ k]g_{t}^{2}[k]\right]\] \[-\sum_{k=1}^{d}\frac{\beta\sqrt{\eta[k]}}{\sqrt{b_{t-1}^{2}[k]+ \left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\mathbb{E}_{t}\left[\nabla_{k} f(w_{t})\left(\nabla_{k}f(w_{t})-g_{t}[k]\right)\right]\] \[= f(w_{t})+\sum_{k=1}^{d}\mathbb{E}_{t}\left[\left(\frac{\beta \sqrt{\eta[k]}}{\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+ \sigma^{2}}}-\nu_{t}[k]\right)\nabla_{k}f(w_{t})g_{t}[k]\right]\] \[+\frac{L}{2}\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}^{2}[k]g_{t }^{2}[k]\right]-\sum_{k=1}^{d}\frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t} )\right)^{2}}{\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+ \sigma^{2}}}.\]

The second last equality follows from \(\mathbb{E}_{t}\left[\nabla_{k}f(w_{t})\left(\nabla_{k}f(w_{t})-g_{t}[k]\right)\right] =\nabla_{k}f(w_{t})\left(\nabla_{k}f(w_{t})-\mathbb{E}_{t}\left[g_{t}[k]\right] \right)=0\). Now we use (25) to get

\[\mathbb{E}_{t}\left[f(w_{t+1})\right] \leq f(w_{t})+\sum_{k=1}^{d}2\beta\sqrt{\eta[k]}\sigma\mathbb{E}_{t} \left[\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\right]+\frac{L}{2}\sum_{k=1}^{d} \mathbb{E}_{t}\left[\nu_{t}^{2}[k]g_{t}^{2}[k]\right]\] \[-\sum_{k=1}^{d}\frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t} )\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+ \sigma^{2}}}.\]

Then rearranging the terms we have

\[\sum_{k=1}^{d}\frac{\beta\sqrt{\eta[k]}\left(\nabla_{k}f(w_{t}) \right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma ^{2}}} \leq f(w_{t})-\mathbb{E}_{t}\left[f(w_{t+1})\right]+\sum_{k=1}^{d}2 \beta\sqrt{\eta[k]}\sigma\mathbb{E}_{t}\left[\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\right]\] \[+\frac{L}{2}\sum_{k=1}^{d}\mathbb{E}_{t}\left[\nu_{t}^{2}[k]g_{t }^{2}[k]\right].\]Now we take the total expectations to derive

\[\sum_{k=1}^{d}\mathbb{E}\left[\frac{\beta\sqrt{\eta[k]}\left(\nabla_ {k}f(w_{t})\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{ 2}+\sigma^{2}}}\right] \leq \mathbb{E}\left[f(w_{t})\right]-\mathbb{E}\left[f(w_{t+1}) \right]+\sum_{k=1}^{d}2\beta\sqrt{\eta[k]}\sigma\mathbb{E}\left[\frac{g_{t}^{2 }[k]}{b_{t}^{2}[k]}\right]\] \[+\frac{L}{2}\sum_{k=1}^{d}\mathbb{E}\left[\nu_{t}^{2}[k]g_{t}^{2 }[k]\right].\]

The above inequality holds for any \(t\). Therefore summing up from \(t=0\) to \(t=T\) and using \(f(w_{T+1})\geq f_{*}\) we get

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\mathbb{E}\left[\frac{\beta\sqrt{\eta[ k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t} )\right)^{2}+\sigma^{2}}}\right] \leq f(w_{0})-f_{*}+\sum_{t=0}^{T}\sum_{k=1}^{d}2\beta\sqrt{\eta[k]} \sigma\mathbb{E}\left[\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\right]\] (48) \[+\frac{L}{2}\sum_{t=0}^{T}\sum_{k=1}^{d}\mathbb{E}\left[\nu_{t}^{ 2}[k]g_{t}^{2}[k]\right].\]

Note that, using the expansion of \(\nu_{t}^{2}[k]\) we have

\[\nu_{t}^{2}[k] = \frac{\beta^{2}\eta[k]b_{t}^{2}[k]+\beta^{2}\sum_{j=0}^{t}\frac{g _{j}^{2}[k]}{b_{j}^{2}[k]}}{b_{t}^{4}[k]}\] (49) \[= \frac{\beta^{2}\eta[k]}{b_{t}^{2}[k]}+\frac{\beta^{2}}{b_{t}^{4}[ k]}\sum_{j=0}^{t}\frac{g_{j}^{2}[k]}{b_{j}^{2}[k]}\] \[\leq \frac{\beta^{2}\eta[k]}{b_{t}^{4}[k]}+\frac{\beta^{2}}{b_{t}^{4}[ k]b_{0}^{2}[k]}\sum_{j=0}^{t}g_{j}^{2}[k]\] \[= \frac{\beta^{2}\eta[k]}{b_{t}^{2}[k]}+\frac{\beta^{2}}{b_{t}^{2}[ k]g_{0}^{2}[k]}.\] (50)

Here (49) follows from \(b_{t}^{2}[k]\geq b_{0}^{2}[k]\) and (50) from \(b_{t}^{2}[k]=\sum_{j=0}^{t}g_{j}^{2}[k]\). Then using (50) in (48) we derive

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\mathbb{E}\left[\frac{\beta\sqrt{\eta [k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k }f(w_{t})\right)^{2}+\sigma^{2}}}\right] \leq f(w_{0})-f_{*}+\sum_{t=0}^{T}\sum_{k=1}^{d}\left(2\beta\sqrt{ \eta[k]}\sigma+\frac{\beta^{2}\eta[k]L}{2}+\frac{\beta^{2}L}{2g_{0}^{2}[k]} \right)\mathbb{E}\left[\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\right]\] \[\leq f(w_{0})-f_{*}\] \[+\sum_{k=1}^{d}\left(2\beta\sqrt{\eta[k]}\sigma+\frac{\beta^{2} \eta[k]L}{2}+\frac{\beta^{2}L}{2g_{0}^{2}[k]}\right)\mathbb{E}\left[\log\left( \frac{b_{T}^{2}[k]}{b_{0}^{2}[k]}\right)+1\right].\]

Here the last inequality follows from (30). Now using Jensen's Inequality (21) with \(\Psi(z)=\log(z)\) we have

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\mathbb{E}\left[\frac{\beta\sqrt{\eta [k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k }f(w_{t})\right)^{2}+\sigma^{2}}}\right] \leq f(w_{0})-f_{*}\] \[+\sum_{k=1}^{d}\left(2\beta\sqrt{\eta[k]}\sigma+\frac{\beta^{2} \eta[k]L}{2}+\frac{\beta^{2}L}{2g_{0}^{2}[k]}\right)\left(\log\left(\frac{ \mathbb{E}\left[b_{T}^{2}[k]\right]}{b_{0}^{2}[k]}\right)+1\right).\]

Now note that \(\mathbb{E}\left[b_{T}^{2}[k]\right]=\sum_{t=0}^{T}\mathbb{E}\left[g_{t}^{2}[k] \right]=\sum_{t=0}^{T}\mathbb{E}\left[g_{t}[k]-\nabla_{k}f(w_{t})\right]^{2}+ \left(\nabla_{k}f(w_{t})\right)^{2}\leq(\sigma^{2}+\gamma^{2})T\).

Therefore, we have the bound

\[\sum_{t=0}^{T}\sum_{k=1}^{d}\mathbb{E}\left[\frac{\beta\sqrt{\eta [k]}\left(\nabla_{k}f(w_{t})\right)^{2}}{2\sqrt{b_{t-1}^{2}[k]+\left(\nabla_{k }f(w_{t})\right)^{2}+\sigma^{2}}}\right] \leq f(w_{0})-f_{*}+2\beta\sigma\sum_{k=1}^{d}\sqrt{\eta[k]}\log\left( \frac{\epsilon(\sigma^{2}+\gamma^{2})T}{b_{0}^{2}[k]}\right)\] \[+\sum_{k=1}^{d}\left(\frac{\beta^{2}\eta[k]L}{2}+\frac{\beta^{2}L}{ 2g_{0}^{2}[k]}\right)\log\left(\frac{\epsilon(\sigma^{2}+\gamma^{2})T}{b_{0}^{2 }[k]}\right).\]Here the RHS is exactly \(\mathcal{C}_{f}\). Using (22) we have

\[\sum_{k=1}^{d}\frac{(\nabla_{k}f(w_{t}))^{2}}{\sqrt{b_{t-1}^{2}[k]+ \left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}} \geq \frac{\sum_{k=1}^{d}\left(\nabla_{k}f(w_{t})\right)^{2}}{\sqrt{ \sum_{k=1}^{d}b_{t-1}^{2}[k]+\left(\nabla_{k}f(w_{t})\right)^{2}+\sigma^{2}}}\] (52) \[= \frac{\left\|\nabla f(w_{t})\right\|^{2}}{\sqrt{\left\|b_{t-1} \right\|^{2}+\left\|\nabla f(w_{t})\right\|^{2}+d\sigma^{2}}}.\]

Therefore using (52) in (51) we arrive at

\[\sum_{t=0}^{T}\mathbb{E}\left[\frac{\left\|\nabla f(w_{t})\right\| ^{2}}{\sqrt{\left\|b_{t-1}\right\|^{2}+\left\|\nabla f(w_{t})\right\|^{2}+d \sigma^{2}}}\right] \leq \frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}.\] (53)

Now we use Holder's Inequality (20) \(\frac{\mathbb{E}(XY)}{\left(\mathbb{E}\left|Y\right|^{3}\right)^{\frac{1}{3}}} \leq\left(\mathbb{E}\left|X\right|^{\frac{2}{3}}\right)^{\frac{2}{3}}\) with

\[X=\left(\frac{\left\|\nabla f(w_{t})\right\|^{2}}{\sqrt{\left\|b_{t-1}\right\| ^{2}+\left\|\nabla f(w_{t})\right\|^{2}+d\sigma^{2}}}\right)^{\frac{2}{3}} \quad\text{and}\quad Y=\left(\sqrt{\left\|b_{t-1}\right\|^{2}+\left\|\nabla f( w_{t})\right\|^{2}+d\sigma^{2}}\right)^{\frac{2}{3}}\]

to get a lower bound on LHS of (53):

\[\mathbb{E}\left[\frac{\left\|\nabla f(w_{t})\right\|^{2}}{\sqrt{ \left\|b_{t-1}\right\|^{2}+\left\|\nabla f(w_{t})\right\|^{2}+d\sigma^{2}}}\right] \geq \frac{\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|^{\frac{1}{3} }\right]^{\frac{2}{3}}}{\sqrt{\mathbb{E}\left(\left\|b_{t-1}\right\|^{2}+ \left\|\nabla f(w_{t})\right\|^{2}+d\sigma^{2}\right)}}\] (54) \[\geq \frac{\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|^{\frac{1}{3} }\right]^{\frac{2}{3}}}{\sqrt{\left\|b_{0}\right\|^{2}+2t(\gamma^{2}+d\sigma^ {2})}}.\]

Therefore from (53) and (54) we get

\[\frac{T}{\sqrt{\left\|b_{0}\right\|^{2}+2T(\gamma^{2}+d\sigma^{2})}}\min_{t \leq T}\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|^{\frac{4}{3}}\right]^{ \frac{3}{2}} \leq \frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}.\]

Then multiplying both sides by \(\frac{\left\|b_{0}\right\|+\sqrt{2T}(\gamma+\sqrt{d}\sigma)}{T}\) we have

\[\min_{t\leq T}\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|^{ \frac{1}{3}}\right]^{\frac{3}{2}} \leq \left(\frac{\left\|b_{0}\right\|}{T}+\frac{2(\gamma+\sigma)}{ \sqrt{T}}\right)\frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}.\]

Here we use \(\mathbb{E}\left[\left\|\nabla f(w_{t})\right\|\right]^{\frac{3}{3}}\leq \mathbb{E}\left[\left\|\nabla f(w_{t})\right\|\right]^{\frac{3}{3}}\) (follows from Jensen's Inequality (21) with \(\Psi(z)=z^{\nicefrac{{4}}{{3}}}\)) in the above equation to get

\[\min_{t\leq T}\mathbb{E}\left[\left\|\nabla f(w_{t})\right\| \right]^{2} \leq \left(\frac{\left\|b_{0}\right\|}{T}+\frac{2(\gamma+\sigma)}{ \sqrt{T}}\right)\frac{2\mathcal{C}_{f}}{\beta\sqrt{\eta_{0}}}.\]

This completes the proof of the Theorem.

[MISSING_PAGE_FAIL:26]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: the new method and its scale invariance property are introduced in Section 2, main convergence results are provided in Section 3, and the numerical results are provided in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see Section 3. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: see Section 3 and the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: see Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: see Section 4. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: see Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: the results are consistent for different runs. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have added all the details in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the paper follows NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: the paper is mostly theoretical and does not have a direct societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: we do not release data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

1. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: see Section 4. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: not applicable. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: not applicable. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.