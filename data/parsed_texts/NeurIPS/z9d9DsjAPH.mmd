# CycleNet: Rethinking Cycle Consistency in

Text-Guided Diffusion for Image Manipulation

 Sihan Xu\({}^{1}\) Ziqiao Ma\({}^{1}\) Yidong Huang\({}^{1}\) Honglak Lee\({}^{1,2}\) Joyce Chai\({}^{1}\)

\({}^{1}\)University of Michigan, \({}^{2}\)LG AI Research

{sihanxu,marstin,owenhji,honglak,chaijy}@umich.edu

Equal contribution.

###### Abstract

Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces CycleNet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate CycleNet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that CycleNet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. CycleNet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train.

## 1 Introduction

Recently, pre-trained diffusion models (DMs) [38, 37, 39] have enabled an unprecedented breakthrough in image synthesis tasks. Compared to GANs [9] and VAEs [22], DMs exhibit superior stability and quality in image generation, as well as the capability to scale up to open-world multi-modal data. As such, pre-trained DMs have been applied to image-to-image (I2I) translation, which is to acquire a mapping between images from two distinct domains, e.g., different scenes, different

Figure 1: A high-resolution example of CycleNet for diffusion-based image-to-image translation compared to other diffusion-based methods. CycleNet produces high-quality translations with satisfactory consistency. The areas in the boxes are enlarged for detailed comparisons.

objects, and different object states. For such translations, text-guided diffusion models typically require mask layers [32; 2; 7; 1] or attention control [10; 30; 25; 35]. However, the quality of masks and attention maps can be unpredictable in complex scenes, leading to semantic and structural changes that are undesirable. Recently, researchers have explored using additional image-conditioning to perform paired I2I translations with the help of a side network [51] or an adapter [31]. Still, it remains an open challenge to adapt pre-trained DMs in _unpaired_ I2I translation with a _consistency_ guarantee.

We emphasize that _consistency_, a desirable property in image manipulation, is particularly important in unpaired I2I scenarios where there is no guaranteed correspondence between images in the source and target domains. Various applications of DMs, including video prediction and infilling [14], imagination-augmented language understanding [49], robotic manipulation [18; 8] and world models [46], would rely on strong consistency across the source and generated images.

To enable unpaired I2I translation using pre-trained DMs with satisfactory consistency, this paper introduces CycleNet, which allows DMs to translate a source image by conditioning on the input image and text prompts. More specifically, we adopt ControlNet [51] with pre-trained Stable Diffusion (SD) [38] as the latent DM backbone. Motivated by cycle consistency in GAN-based methods [55], CycleNet leverages consistency regularization over the image translation cycle. As illustrated in Figure 2, the image translation cycle includes a forward translation from \(x_{0}\) to \(\bar{y}_{0}\) and a backward translation to \(\bar{x}_{0}\). The key idea of our method is to ensure that when conditioned on an image \(c_{\mathrm{img}}\) that falls into the target domain specified by \(c_{\mathrm{text}}\), the DM should be able to reproduce this image condition through the reverse process.

We validate CycleNet on I2I translation tasks of different granularities. Besides the scene and object level tasks introduced by Zhu et al. [55], we additionally contribute ManiCups, a multi-domain I2I translation dataset for manipulating physical state changes of objects. ManiCups contains 6k images of empty cups and cups of coffee, juice, milk, and water, collected from human-annotated bounding boxes. The empirical results demonstrate that compared to previous approaches, CycleNet is superior in translation faithfulness, cycle consistency, and image quality. Our approach is also computationally friendly, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Further analysis shows that CycleNet is a robust zero-shot I2I translator, which can generate faithful and high-quality images for out-of-domain distributions with a simple change of the textual prompt. This opens up possibilities to develop consistent diffusion-based image manipulation models with image conditioning and free-form language instructions.

## 2 Preliminaries

We start by introducing a set of notations to characterize image-to-image translation with DMs.

Diffusion ModelsDiffusion models progressively add Gaussian noise to a source image \(z_{0}\sim q(z_{0})\) through a forward diffusion process and subsequently reverse the process to restore the original image. Given a variance schedule \(\beta_{1},\dots,\beta_{T}\), the forward process is constrained to a Markov chain \(q(z_{t}|z_{t-1}):=\mathcal{N}(z_{t};\sqrt{1-\beta_{t}}z_{t-1},\beta_{t}\mathbf{ I})\), in which \(z_{1:T}\) are latent variables with dimensions matching \(z_{0}\). The reverse process \(p_{\theta}(z_{0:T})\) is as well Markovian, with learned Gaussian transitions that begin at \(z_{T}\sim\mathcal{N}(0,\mathbf{I})\). Ho et al. [13] noted that the forward process allows the sampling of \(z_{t}\) at any time step \(t\) using a closed-form sampling function (Eq. 1).

\[z_{t}=S(z_{0},\varepsilon,t):=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{ \alpha}_{t}}\varepsilon,\;\varepsilon\sim\mathcal{N}(0,\mathbf{I})\text{ and }t\sim[1,T]\] (1)

in which \(\alpha_{t}:=1-\beta_{t}\) and \(\bar{\alpha}_{t}:=\prod_{s=1}^{t}\alpha_{s}\). Thus, the reverse process can be carried out with a UNet-based network \(\varepsilon_{\theta}\) that predicts the noise \(\varepsilon\). By dropping time-dependent variances, the model can be trained according to the objective in Eq. 2.

\[\min_{\theta}\mathbb{E}_{z_{0},\varepsilon,t}\;||\varepsilon-\varepsilon_{ \theta}(z_{t},t)||_{2}^{2}\] (2)

Eq. 2 implies that in principle, one could estimate the original source image \(z_{0}\) given a noised latent \(z_{t}\) at any time \(t\). The reconstructed \(\bar{z}_{0}\) can be calculated with the generation function:

\[\bar{z}_{0}=G(z_{t},t):=\big{[}z_{t}-\sqrt{1-\bar{\alpha}_{t}}\varepsilon_{ \theta}(z_{t},t)\big{]}/\sqrt{\bar{\alpha}_{t}}\] (3)

For simplicity, we drop the temporal conditioning \(t\) in the following paragraphs.

Conditioning in Latent Diffusion ModelsLatent diffusion models (LDMs) like Stable Diffusion [38] can model conditional distributions \(p_{\theta}(z_{0}|c)\) over condition \(c\), e.g., by augmenting the UNet backbone with a condition-specific encoder using cross-attention mechanism [45]. Using textual prompts is the most common approach for enabling conditional image manipulation with LDMs. With a textual prompt \(c_{z}\) as conditioning, LDMs strive to learn a mapping from a latent noised sample \(z_{t}\) to an output image \(z_{0}\), which falls into a domain \(\mathcal{Z}\) that is specified by the conditioning prompt. To enable more flexible and robust conditioning in diffusion-based image manipulation, especially a mixture of text and image conditioning, recent work obtained further control over the reverse process with a side network [51] or an adapter [31]. We denote such conditional denoising autoencoder as \(\varepsilon_{\theta}(z_{t},c_{\text{text}},c_{\text{img}})\), where \(c_{\text{img}}\) is the image condition and the text condition \(c_{\text{text}}\). Eq. 3 can thus be rewritten as:

\[\bar{z}_{0}=G(z_{t},c_{\text{text}},c_{\text{img}}):=\big{[}z_{t}-\sqrt{1- \bar{\alpha}_{t}}\varepsilon_{\theta}(z_{t},c_{\text{text}},c_{\text{img}}) \big{]}/\sqrt{\bar{\alpha}_{t}}\] (4)

The text condition \(c_{\text{text}}\) contains a pair of conditional and unconditional prompts \(\{c^{+},c^{-}\}\). A conditional prompt \(c^{+}\) guides the diffusion process towards the images that are associated with it, whereas a negative prompt \(c^{-}\) drives the diffusion process away from those images.

Consistency Regularization for Unpaired Image-to-Image TranslationThe goal of unpaired image-to-image (I2I) translation is to learn a mapping between two domains \(\mathcal{X}\subset\mathbb{R}^{d}\) and \(\mathcal{Y}\subset\mathbb{R}^{d}\) with unpaired training samples \(\{x_{i}\}\) for \(i=1,\dots,N\), where \(x_{i}\in\mathcal{X}\) belongs to \(X\), and \(\{y_{j}\}\) for \(j=1,\dots,M\), where \(y_{j}\in\mathcal{Y}\). In traditional GAN-based translation frameworks, the task typically requires two mappings \(G:\mathcal{X}\rightarrow\mathcal{Y}\) and \(F:\mathcal{Y}\rightarrow\mathcal{X}\). **Cycle consistency** enforces transitivity between forward and backward translation functions by regularizing pairs of samples, which is crucial in I2I translation, particularly in unpaired settings where no explicit correspondence between images in source and target domains is guaranteed [55; 27; 50; 44]. To ensure cycle consistency, CycleGAN [55] explicitly regularizes the translation cycle, bringing \(F(G(x))\) back to the original image \(x\), and vice versa for \(y\). Motivated by consistency regularization, we seek to enable consistent unpaired I2I translation with LDMs. Without introducing domain-specific generative models, we use one single denoising network \(\varepsilon_{\theta}\) for translation by conditioning it on text and image prompts.

## 3 Method

In the following, we discuss only the translation from domain \(\mathcal{X}\) to \(\mathcal{Y}\) due to the symmetry of the backward translation. Our goal, at inference time, is to enable LDMs to translate a source image \(x_{0}\) by using it as the image condition \(c_{\text{img}}=x_{0}\), and then denoise the noised latent \(y_{t}\) to \(y_{t-1}\) with text prompts \(c_{\text{text}}=c_{y}\). To learn such a translation model \(\varepsilon_{\theta}(y_{t},c_{y},x_{0})\), we consider two types of training objectives. In the following sections, we describe the **cycle consistency regularization** to ensure cycle consistency so that the structures and unrelated semantics are preserved in the generated images, and the **self regularization** to match the distribution of generated images with the target

Figure 2: The image translation cycle includes a forward translation from \(x_{0}\) to \(\bar{y}_{0}\) and a backward translation to \(\bar{x}_{0}\). The key idea of our method is to ensure that when conditioned on an image \(c_{\text{img}}\) that falls into the target domain specified by \(c_{\text{text}}\), the LDM should reproduce this image condition through the reverse process. The dashed lines indicate the regularization in the loss functions.

domain, As illustrated in Figure 2, the image translation cycle includes a forward translation from a source image \(x_{0}\) to \(\bar{y}_{0}\), followed by a backward translation to the reconstructed source image \(\bar{x}_{0}\).

### Cycle Consistency Regularization

We assume a likelihood function \(P(z_{0},c_{\mathrm{text}})\) that the image \(z_{0}\) falls into the data distribution specified by the text condition \(c_{\mathrm{text}}\). We consider a generalized case of cycle consistency given the conditioning mechanism in LDMs. If \(P(c_{\mathrm{img}},c_{\mathrm{text}})\) is close to 1, i.e., the image condition \(c_{\mathrm{img}}\) falls exactly into the data distribution described by the text condition \(c_{\mathrm{text}}\), we should expect that \(G(z_{t},c_{\mathrm{text}},c_{\mathrm{text}},c_{\mathrm{img}})=c_{\mathrm{img}}\) for any noised latent \(z_{t}\). With the translation cycle in Figure 2, the goal is to optimize (1) \(\mathcal{L}_{x\to x}=\mathbb{E}_{x_{0},\varepsilon_{x}}\)\(||x_{0}-G(x_{t},c_{x},x_{0})||_{2}^{2}\); (2) \(\mathcal{L}_{y\to y}=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\)\(||\bar{y}_{0}-G(y_{t},c_{y},\bar{y}_{0})||_{2}^{2}\); (3) \(\mathcal{L}_{x\to y\to x}=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\)\(||x_{0}-G(y_{t},c_{x},x_{0})||_{2}^{2}\); and (4) \(\mathcal{L}_{x\to y\to y}=\mathbb{E}_{x_{0},\varepsilon_{x}}\)\(||\bar{y}_{0}-G(x_{t},c_{y},\bar{y}_{0})||_{2}^{2}\).

**Proposition 1** (Cycle Consistency Regularization).: _With the translation cycle in Figure 2, a set of consistency losses is given by dropping time-dependent variances:_

\[\mathcal{L}_{x\to x} =\mathbb{E}_{x_{0},\varepsilon_{x}}\)\(||\varepsilon_{\theta}(x_{t},c_{x},x_{0})-\varepsilon_{x}||_{2}^{2}\) (5) \[\mathcal{L}_{y\to y} =\mathbb{E}_{x_{0},x_{\varepsilon},\varepsilon_{y}}\)\(||\varepsilon_{\theta}(y_{t},c_{y},\bar{y}_{0})-\varepsilon_{y}||_{2}^{2}\) (6) \[\mathcal{L}_{x\to y\to x} =\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\)\(||\varepsilon_{\theta}(y_{t},c_{x},x_{0})+\varepsilon_{\theta}(x_{t},c_{y},x_{0})- \varepsilon_{x}-\varepsilon_{y}||_{2}^{2}\) (7) \[\mathcal{L}_{x\to y\to y} =\mathbb{E}_{x_{0},\varepsilon_{x}}\)\(||\varepsilon_{\theta}(x_{t},c_{y},x_{0})-\varepsilon_{\theta}(x_{t},c_{y},\bar{y}_{0}) ||_{2}^{2}\) (8)

We leave the proof in Section A.2. Proposition 1 states that pixel-level consistency can be acquired by regularizing the conditional denoising autoencoder \(\varepsilon_{\theta}\). Specifically, the **reconstruction loss**\(\mathcal{L}_{x\to x}\) and \(\mathcal{L}_{y\to y}\) ensures that CycleNet can function as a LDM to reverse an image similar to Eq. 2. The cycle **consistency loss**\(\mathcal{L}_{x\to y\to x}\) serves as the transitivity regularization, which ensures that the forward and backward translations can reconstruct the original image \(x_{0}\). The **invariance loss**\(\mathcal{L}_{x\to y\to y}\) requires that the target image domain stays invariant under forward translation, i.e., given a forward translation from \(x_{t}\) to \(\bar{y}_{0}\) conditioned on \(x_{0}\), repeating the translation conditioned on \(\bar{y}_{0}\) would reproduce \(\bar{y}_{0}\).

### Self Regularization

In the previous section, while \(x_{0}\) is naturally sampled from domain \(\mathcal{X}\), we need to ensure that the generated images fall in the target domain \(\mathcal{Y}\), i.e., the translation leads to \(G(x_{t},c_{y},x_{0})\in\mathcal{Y}\). Our goal is therefore to maximize \(P(\bar{y}_{0},c_{y})\), or equivalently to minimize

\[\mathcal{L}_{\mathrm{LDM}}=-\mathbb{E}_{x_{0},\varepsilon_{x}}P\big{[}G\big{(} S(x_{0},\varepsilon),c_{y},x_{0}\big{)},c_{y}\big{]}\] (9)

**Assumption 1** (Domain Smoothness).: _For any text condition, \(P(\cdot,c_{\mathrm{text}})\) is \(L\)-Lipschitz._

\[\exists\ L<\infty,\ |P(z_{0}^{1},c_{\mathrm{text}})-P(z_{0}^{2},c_{\mathrm{text}} )|\leq L||z_{0}^{1}-z_{0}^{2}||_{2}\] (10)

**Proposition 2** (Self Regularization).: _Let \(\varepsilon_{\theta}^{*}\) denote the denoising autoencoder of the pre-trained text-guided LDM backbone. Let \(x_{t}=S(x_{0},\varepsilon_{x})\) be a noised latent. A self-supervised upper bound of \(\mathcal{L}_{\mathrm{LDM}}\) is given by:_

\[\mathcal{L}_{\mathrm{self}}=\mathbb{E}_{x_{0},\varepsilon_{x}}\left[L\sqrt{ \frac{1-\bar{\alpha}_{t}}{\bar{\alpha}_{t}}}||\varepsilon_{\theta}(x_{t},c_{y},x_{0})-\varepsilon_{\theta}^{*}(x_{t},c_{y})||_{2}\right]+\mathrm{const}\] (11)

Lipschitz assumptions have been widely adopted in diffusion methods [53; 48]. Assumption 1 hypothesizes that similar images share similar domain distributions. A self-supervised upper bound \(\mathcal{L}_{\mathrm{self}}\) can be obtained in Proposition 2, which intuitively states that if the output of the conditional translation model does not deviate far from the pre-trained LDM backbone, the outcome image should still fall in the same domain specified by the textual prompt. We leave the proof in Section A.3.

### CycleNet

In practice, \(\mathcal{L}_{\mathrm{self}}\) can be minimized from the beginning of training by using a ControlNet [51] with pre-trained Stable Diffusion (SD) [38] as the LDM backbone, which is confirmed through preliminary experiments. As shown in Figure 2, the model keeps the SD encoder frozen and makes a trainable copy in the side network. Additional zero convolution layers are introduced to encode the image condition and control the SD decoder. These zero convolution layers are 1D convolutions whose initial weights and biases vanish and can gradually acquire the optimized parameters from zero. Since the zero convolution layers keep the SD encoder features untouched, \(\mathcal{L}_{\mathrm{self}}\) is minimal at the beginning of the training, and the training process is essentially fine-tuning a pre-trained LDM with a side network.

The text condition \(c_{\mathrm{text}}=\{c^{+},c^{-}\}\) contains a pair of conditional and unconditional prompts. We keep the conditional prompt in the frozen SD encoder and the unconditional prompt in the ControlNet, so that the LDM backbone focuses on the translation and the side network looks for the semantics that needs modification. For example, to translate an image of summer to winter, we rely on a conditional prompt \(l_{x}=\text{``summer''}\) and unconditional prompt \(l_{y}=\text{``winter''}\). Specifically, we use CLIP [36] encoder to encode the language prompts \(l_{x}\) and \(l_{y}\) such that \(c_{x}=\{\mathrm{CLIP}(l_{x}),\mathrm{CLIP}(l_{y})\}\) and \(c_{y}=\{\mathrm{CLIP}(l_{y}),\mathrm{CLIP}(l_{x})\}\).

We also note that \(\mathcal{L}_{y\to y}\) can be omitted, as \(\mathcal{L}_{x\to x}\) can serve the same purpose in the symmetry of the translation cycle from \(\mathcal{Y}\) to \(\mathcal{X}\), and early experiments confirmed that dropping this term lead to significantly faster convergence. The simplified objective is thus given by:

\[\mathcal{L}_{x}=\lambda_{1}\mathcal{L}_{x\to x}+\lambda_{2}\mathcal{L}_{x \to y\to y}+\lambda_{3}\mathcal{L}_{x\to y\to x}\] (12)

Consider both translation cycle from \(\mathcal{X}\leftrightarrow\mathcal{Y}\), the complete training objective of CycleNet is:

\[\mathcal{L}_{\mathrm{CycleNet}}=\mathcal{L}_{x}+\mathcal{L}_{y}\] (13)

The pseudocode for training is given in Algo. 1.

### FastCycleNet

Similar to previous cycle-consistent GAN-based models for unpaired I2I translation, there is a trade-off between the image translation quality and cycle consistency. Also, the cycle consistency loss \(\mathcal{L}_{x\to y\to x}\) requires deeper gradient descent, and therefore more computation expenses during training (Table 6). In order to speed up the training process in this situation, one may consider further removing \(\mathcal{L}_{x\to y\to x}\) from the training objective, and name this variation FastCycleNet. Through experiments, FastCycleNet can achieve satisfying consistency and competitive translation quality, as shown in Table 1. Different variations of models can be chosen depending on the practical needs.

## 4 Experiments

### Benchmarks

Scene/Object-Level ManipulationWe validate CycleNet on I2I translation tasks of different granularities. We first consider the benchmarks used in CycleGAN by Zhu et al. [55], which contains:

* [leftmargin=*]
* (Scene Level) Yosemite summer\(\leftrightarrow\)winter: We use around 2k images of summer and winter Yosemite, with default prompts "summer" and "winter";
* (Object Level) horse\(\leftrightarrow\)zebra: We use around 2.5k images of horses and zebras from the dataset with default prompts "horse" and "zebra";
* (Object Level) apple\(\leftrightarrow\)orange: We use around 2k apple and orange images with default prompts of "apple" and "orange".

State Level ManipulationAdditionally, we introduce ManiCups1, a dataset of state-level image manipulation that tasks models to manipulate cups by filling or emptying liquid to/from containers, formulated as a multi-domain I2I translation dataset for object state changes:

Footnote 1: Our data is available at https://huggingface.co/datasets/sled-umich/ManiCups.

* (State Level) ManiCups: We use around 5.7k images of empty cups and cups of coffee, juice, milk, and water for training. The default prompts are set as "empty cup" and "cup of <liquid>". The task is to either empty a full cup or fill an empty cup with liquid as prompted.

ManiCups is curated from human-annotated bounding boxes in publicly available datasets and Bing Image Search (under Share license for training and Modify for test set). We describe our three-stage data collection pipeline. In the **image collection** stage, we gather raw images of interest from MSCOCO [26], Open Images [23], as well as Bing Image Search API. In the **image extraction** stage, we extract regions of interest from the candidate images and resize them to a standardized size. Specifically, for subsets obtained from MSCOCO and Open Images, we extract the bounding boxes with labels of interest. All bounding boxes with an initial size less than 128\(\times\)128 are discarded, and the remaining boxes are extended to squares and resized to a standardized size of 512\(\times\)512 pixels. After this step, we obtained approximately 20k extracted and resized candidate images. We then control the data quality through a **filtering and labeling** stage. Our filtering process first discards replicated images using the L2 distance metric and remove images containing human faces, as well as cups with a front-facing perspective with a CLIP processor. Our labeling process starts with an automatic annotation with a CLIP classifier. To ensure the accuracy of the dataset, three human annotators thoroughly review the collected images, verifying that the images portray a top-down view of a container and assigning the appropriate labels to the respective domains. The resulting ManiCups dataset contains 5 domains, including 3 abundant domains (empty, coffee, juice) with more than 1K images in each category and 2 low-resource domains (water, milk) with less than 1K images to facilitate research and analysis in data-efficient learning.

To our knowledge, ManiCups is one of the first datasets targeted to the physical state changes of objects, other than stylistic transfers or type changes of objects. The ability to generate consistent state changes based on manipulation is fundamental for future coherent video prediction [14] as well as understanding and planning for physical agents [49; 18; 8; 46]. For additional details on data collection, processing, and statistics, please refer to Appendix B.

### Experiment Setup

BaselinesWe compare our proposed models FastCycleNet and CycleNet to state-of-the-art methods for unpaired or zero-shot image-to-image translation.

* GAN-based methods: CycleGAN [55] and CUT [34];
* Mask-based diffusion methods: Direct inpainting with CLIPSeg [28] and Text2LIVE [2];
* Mask-free diffusion methods: ControlNet with Canny Edge [51], ILVR [5], EGSDE [53], SDEdit [29], Pix2Pix-Zero [35], MasaCtrl [3], CycleDiffusion [48], and Prompt2Prompt [10] with null-text inversion [30].

TrainingWe train our model with a batch size of 4 on only one single A40 GPU.2 Additional details on the implementations are available in Appendix C.

Footnote 2: Our code is available at https://github.com/sled-group/CycleNet.

SamplingAs shown in Figure 3, CycleNet has a good efficiency at inference time and more sampling steps lead to better translation quality. We initialize the sampling process with the latent noised input image \(z_{t}\), collected using Equation 1. Following [29], a standard 50-step sampling is applied at inference time with \(t=100\) for fair comparison.

### Qualitative Evaluation

We present qualitative results comparing various image translation models. Due to the space limit, additional examples will be available in Appendix E. In Figure 4, we present the two unpaired translation tasks: summer\(\rightarrow\)winter, and horse\(\rightarrow\)zebra. To demonstrate the image quality, translation quality, and consistency compared to the original images, we provide a full image for each test case

Figure 3: Step skipping during sampling. The source image is from MSCOCO [26].

and enlarge the boxed areas for detailed comparisons. As presented with the qualitative examples, our methods are able to perform image manipulation with high quality like the other diffusion-based methods, while preserving the structures and unrelated semantics.

In Figure 5, we present qualitative results for filling and emptying a cup: coffee\(\leftrightarrow\)empty and empty\(\leftrightarrow\)jucie. As demonstrated, image editing tasks that require physical state changes pose a significant challenge to baselines, which struggle with the translation itself and/or maintaining strong consistency. CycleNet, again, is able to generate faithful and realistic images that reflect the physical state changes.

### Quantitative Evaluation

We further use three types of evaluation metrics respectively to assess the quality of the generated image, the quality of translation, and the consistency of the images. For a detailed explanation of these evaluation metrics, we refer to Appendix C.4.

* **Image Quality**. To evaluate the quality of images, we employ two metrics: The naive Frechet Inception Distance (FID) [12] and FID\({}_{\mathrm{clip}}\)[24] with CLIP [36];

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline
**Tasks** & \multicolumn{4}{c}{**summer\(\rightarrow\)winter (Scene level, 26\(\times\)26)**} & \multicolumn{4}{c}{**horse\(\rightarrow\)zebra (Object level, 26\(\times\)26)**} \\ \hline
**Metrics** & **FID\({}_{\mathrm{clip}}\)** & **FID\({}_{\mathrm{clip}}\)** & **CLIP\({}_{\mathrm{p}}\)** & **PSNR\({}^{\dagger}\)** & **SSMM\({}^{\dagger}\)** & **L2\({}^{\mathrm{clip}\dagger}\)** & **FID\({}_{\mathrm{clip}}\)** & **FID\({}_{\mathrm{clip}}\)** & **CLIP\({}^{\dagger}\)** & **LIP\({}_{\mathrm{p}}\)** & **PSNR\({}^{\dagger}\)** & **SSMM\({}^{\dagger}\)** & **L2\({}^{\mathrm{clip}\dagger}\)** \\ \hline \hline \multicolumn{12}{c}{**CyclicGAN**} & 133.16 & 18.55 & 22.07 & 0.20 & 16.27 & 0.39 & 36.71 & 71.88 & 27.69 & 28.07 & 0.25 & 18.53 & 0.67 & 1.39 \\ \multicolumn{12}{c}{**CUT**} & 180.09 & 23.45 & 24.21 & 0.19 & 20.05 & 0.71 & 1.15 & 45.50 & 21.00 & 29.15 & 0.46 & 13.71 & 0.35 & 2.44 \\ \multicolumn{12}{c}{**Mackhoff DIMATION**} \\ \hline \hline \multicolumn{12}{c}{**Initial**} & 246.55 & 79.70 & 21.85 & 0.27 & 12.61 & 0.19 & 28.38 & 18.763 & 40.03 & 26.52 & 0.30 & 15.45 & 0.41 & 2.31 \\ \multicolumn{12}{c}{**TencLIP\({}_{\mathrm{clip}}\)**} & 100.63 & 25.29 & 26.03 & 0.22 & 16.51 & 0.67 & 17.42 & 28.21 & 24.26 & 0.51 & 0.14 & 21.05 & 0.81 & 1.03 \\ \multicolumn{12}{c}{**Mackhoff DIMATION**} \\ \hline \hline \multicolumn{12}{c}{**CyclicGAN**} & 338.24 & 83.26 & 21.77 & 0.59 & 6.65 & 0.09 & 11.30 & 39.71 & 77.16 & 23.88 & 0.66 & 7.37 & 0.07 & 3.89 \\ \multicolumn{12}{c}{**LIVE**} & 105.93 & 372.24 & 22.91 & 0.59 & 10.66 & 0.16 & 3.62 & 148.45 & 40.80 & 25.95 & 0.57 & 10.24 & 0.17 & 3.57 \\ \multicolumn{12}{c}{**Bicycle**} & 113.00 & 28.74 & 22.96 & 0.44 & 17.68 & 0.27 & 15.93 & 79.64 & 27.79 & 27.31 & 0.41 & 18.05 & 0.29 & 1.44 \\ \multicolumn{12}{c}{**SPMR**} & 330.98 & 79.70 & 21.88 & 0.57 & 12.63 & 0.19 & 2.83 & 98.96 & 83.32 & 24.17 & 0.66 & 9.75 & 0.11 & 4.01 \\ \multicolumn{12}{c}{**P-S**rome} & 310.13 & 81.54 & 22.03 & 0.57 & 14.31 & 0.52 & 5.08 & 37.74 & 86.21 & 26.32 & 0.67 & 11.88 & 0.19 & 3.85 \\ \multicolumn{12}{c}{**MSCM**} & 106.91 & 52.38 & 20.79 & 0.36 & 16.22 & 0.36 & 0.37 & 37.31 & 68.31 & 23.15 & 0.40 & 16.31 & 0.37 & 1.83 \\ \multicolumn{12}{c}{**P-S**affect} & 460.00 & 41.22 & 23.31 & 0.37 & 16.84 & 0.39 & 1.73 & 25.04 & 48.93 & 25.91 & 0.36 & 17.28 & 0.41 & 1.68 \\ \multicolumn{12}{c}{**CyodeDiffusion**} & 243.98 & 62.96 & 23.26 & **0.44** & **15.06** & 0.31 & 2.20 & 347.27 & **66.80** & 25.04 & 0.57 & 11.51 & 0.21 & 3.46 \\ \multicolumn{12}{c}{**FactV**-**

[MISSING_PAGE_FAIL:8]

### Zero-shot Generalization to Out-of-Distribution Domains

CycleNet performs image manipulation with text and image conditioning, making it potentially generalizable to out-of-distribution (OOD) domains with a simple change of the textual prompt. As illustrated in Figure 6, we demonstrate that CycleNet has a remarkable capability to generate faithful and high-quality images for unseen domains. These results highlight the robustness and adaptability of CycleNet to make the most out of the pre-trained LDM backbone to handle unseen scenarios. This underscores the potential to apply CycleNet for various real-world applications and paves the way for future research in zero-shot learning and OOD generalization.

### Translation Diversity

Diversity is an important feature of image translation models. As shown in Figure 6, we demonstrate that CycleNet can generate a variety of images that accurately satisfy the specified translation task in the text prompts, while maintaining consistency.

### Limitations: Trade-off between consistency and translation

There have been concerns that cycle consistency could be too restrictive for some translation task [54]. As shown in Figure 7, while CycleNet maintains a strong consistency over the input image, the quartered apple failed to be translated into its orange equivalence. In GAN-based methods, local discriminators have been proposed to address this issue [56], yet it remains challenging to keep global consistency while making faithful local edits for LDM-based approaches.

## 6 Related Work

### Conditional Image Manipulation with Diffusion Models

Building upon Diffusion Probabilistic Models [42; 13], pre-trained Diffusion Models (DMs) [38; 37; 39] have achieved state-of-the-art performance in image generation tasks. Text prompts are the most common protocol to enable conditional image manipulation with DMs, which can be done by fine-tuning a pre-trained DM [19; 20]. Mask-based methods have also been proposed with the help of user-prompted/automatically generated masks [32; 7; 1] or augmentation layers [2]. To refrain from employing additional masks, recent work has explored attention-based alternatives [10; 30; 25; 35].

Figure 6: Examples of output diversity and zero-shot generalization to out-of-domain distributions.

Figure 7: The trade-off between consistency and translation.

They first invert the source images to obtain the cross-attention maps and then perform image editing with attention control. The promising performance of these methods is largely dependent on the quality of attention maps, which cannot be guaranteed in images with complicated scenes and object relationships, leading to undesirable changes. Very recently, additional image-conditioning has been explored to perform paired image-to-image translation, using a side network [51] or an adapter [31]. In this work, we follow this line of research and seek to enable unpaired image-to-image translation with pre-trained DMs while maintaining a satisfactory level of consistency.

### Unpaired Image-to-Image Translation

Image-to-image translation (I2I) is a fundamental task in computer vision, which is concerned with learning a mapping across images of different domains. Traditional GAN-based methods [16] require instance-level paired data, which are difficult to collect in many domains. To address this limitation, the unpaired I2I setting [55; 27] was introduced to transform an image from the source domain \(\mathcal{X}\) into one that belongs to the target domain \(\mathcal{Y}\), given only unpaired images from each domain. Several GAN-based methods [55; 50; 27] were proposed to address this problem. In recent years, DPMs have demonstrated their superior ability to synthesize high-quality images, with several applications in I2I translation [40; 5]. With the availability of pre-trained DMs, SDEdit [29] changes the starting point of generation by using a noisy source image that preserves the overall structure. EGSDE [53] combines the merit of ILVR and SDEdit by introducing a pre-trained energy function on both domains to guide the denoising process. While these methods result in leading performance on multiple benchmarks, it remains an open challenge to incorporate pre-trained DMs for high-quality image generation, and at the same time, to ensure translation consistency.

### Cycle Consistency in Image Translation

The idea of cycle consistency is to regularize pairs of samples by ensuring transitivity between the forward and backward translation functions [41; 33; 17]. In unpaired I2I translation where explicit correspondence between source and target domain images is not guaranteed, cycle consistency plays a crucial role [55; 21; 27]. Several efforts were made to ensure cycle consistency in diffusion-based I2I translation. UNIT-DDPM [40] made an initial attempt in the unpaired I2I setting, training two DPMs and two translation functions from scratch. Cycle consistency losses are introduced in the translation functions during training to regularize the reverse processes. At inference time, the image generation does not depend on the translation functions, but only on the two DPMs in an iterative manner, leading to sub-optimal performance. Su et al. [43] proposed the DDIB framework that exact cycle consistency is possible assuming zero discretization error, which does not enforce any cycle consistency constraint itself. Cycle Diffusion [48] proposes a zero-shot approach for image translation based on Su et al. [43]'s observation that a certain level of consistency could emerge from DMs, and there is no explicit treatment to encourage cycle consistency. To the best of our knowledge, CycleNet is the first to guarantee cycle consistency in unpaired image-to-image translation using pre-trained diffusion models, with a simple trainable network and competitive performance.

## 7 Conclusion

The paper introduces CycleNet that incorporates the concept of cycle consistency into text-guided latent diffusion models to regularize the image translation tasks. CycleNet is a practical framework for low-resource applications where only limited data and computational power are available. Through extensive experiments on unpaired I2I translation tasks at scene, object, and state levels, our empirical studies show that CycleNet is promising in consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt.

Future WorkThis paper is primarily concerned with the unpaired I2I setting, which utilizes images from unpaired domains during training for domain-specific applications. Although CycleNet demonstrates robust out-of-domain generalization, enabling strong zero-shot I2I translation capabilities is not our focus here. We leave it to our future work to explore diffusion-based image manipulation with image conditioning and free-form language instructions, particularly in zero-shot settings.

## Acknowledgements

This work was supported in part by NSF IIS-1949634, NSF SES-2128623, LG AI Research, and by the Automotive Research Center at the University of Michigan. The authors would like to thank Yinpei Dai and Yuexi Du for their valuable feedback, and extend their special appreciation to Michael de la Paz for granting us permission to use his photograph of summer Yosemite3 in the teaser.

Footnote 3: A hyperlink to the original photo on Flickr. All rights reserved by the artist.

## References

* [1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV_, pages 707-723. Springer, 2022.
* [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 22560-22570, October 2023.
* [4] Luen C Chan and Peter Whiteman. Hardware-constrained hybrid coding of video imagery. _IEEE Transactions on Aerospace and Electronic Systems_, (1):71-84, 1983.
* [5] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. llvm: Conditioning method for denoising diffusion probabilistic models. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14347-14356. IEEE Computer Society, 2021.
* [6] Jooyoung Choi, Jungboom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11472-11481, 2022.
* [7] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In _The Eleventh International Conference on Learning Representations_, 2023.
* [8] Xiaolin Fang, Caelan Reed Garrett, Clemens Eppner, Tomas Lozano-Perez, Leslie Pack Kaelbling, and Dieter Fox. Dimsam: Diffusion models as samplers for task and motion planning under partial observability. _arXiv preprint arXiv:2306.13196_, 2023.
* [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In _Neural Information Processing Systems_, 2014.
* [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In _The Eleventh International Conference on Learning Representations_, 2022.
* [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7514-7528, 2021.
* [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.

* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hoppe et al. [2022] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* Huang et al. [2018] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In _Proceedings of the European conference on computer vision (ECCV)_, pages 172-189, 2018.
* Isola et al. [2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1125-1134, 2017.
* Kalal et al. [2010] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Forward-backward error: Automatic detection of tracking failures. In _2010 20th international conference on pattern recognition_, pages 2756-2759. IEEE, 2010.
* Kapelyukh et al. [2023] Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale diffusion models to robotics. _IEEE Robotics and Automation Letters_, 2023.
* Kawar et al. [2023] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.
* Kim et al. [2022] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2426-2435, 2022.
* Kim et al. [2017] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In _International conference on machine learning_, pages 1857-1865. PMLR, 2017.
* Kingma and Welling [2014] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _International Conference on Learning Representations_, 2014.
* Kuznetsova et al. [2020] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International Journal of Computer Vision_, 128(7):1956-1981, 2020.
* Kynkaanniemi et al. [2023] Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. In _The Eleventh International Conference on Learning Representations_, 2023.
* Li et al. [2023] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. _arXiv preprint arXiv:2303.15649_, 2023.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* Liu et al. [2017] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. _Advances in neural information processing systems_, 30, 2017.
* Luddecke and Ecker [2022] Timo Luddecke and Alexander Ecker. Image segmentation using text and image prompts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7086-7096, 2022.

* Meng et al. [2021] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* Mokady et al. [2023] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.
* Mou et al. [2023] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.
* Nichol et al. [2022] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pages 16784-16804. PMLR, 2022.
* Pan et al. [2009] Pan Pan, Fatih Porikli, and Dan Schonfeld. Recurrent tracking using multifold consistency. In _Proceedings of the Eleventh IEEE International Workshop on Performance Evaluation of Tracking and Surveillance_, volume 3, 2009.
* Park et al. [2020] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pages 319-345. Springer, 2020.
* Parmar et al. [2023] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In _ACM SIGGRAPH 2023 Conference Proceedings_, pages 1-11, 2023.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_, 2022.
* Sasaki et al. [2021] Hiroshi Sasaki, Chris G Willcocks, and Toby P Breckon. Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models. _arXiv preprint arXiv:2104.05358_, 2021.
* Sethi and Jain [1987] Ishwar K. Sethi and Ramesh Jain. Finding trajectories of feature points in a monocular image sequence. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-9(1):56-73, 1987. doi: 10.1109/TPAMI.1987.4767872.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Su et al. [2022] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. In _International Conference on Learning Representations_, 2022.
* Torbunov et al. [2023] Dmitriri Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shinjae Yoo, Meifeng Lin, Brett Viren, and Yihui Ren. Uvccgan: Unet vision transformer cycle-consistent gan for unpaired image-to-image translation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 702-712, 2023.

* [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [46] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Divedreamer: Towards real-world-driven world models for autonomous driving. _arXiv preprint arXiv:2309.09777_, 2023.
* [47] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
* [48] Chen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7378-7387, 2023.
* [49] Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Z-LaVI: Zero-shot language solver fueled by visual imagination. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 1186-1203, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [50] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image translation. In _Proceedings of the IEEE international conference on computer vision_, pages 2849-2857, 2017.
* [51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.
* [52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [53] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. _Advances in Neural Information Processing Systems_, 35:3609-3623, 2022.
* [54] Yihao Zhao, Ruihai Wu, and Hao Dong. Unpaired image-to-image translation using adversarial consistency loss. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pages 800-815. Springer, 2020.
* [55] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2223-2232, 2017.
* [56] Xianhui Zong, Zhehan Chen, and Dadong Wang. Local-cyclegan: a general end-to-end network for visual enhancement in complex deep-water environment. _Applied Intelligence_, 51:1947-1958, 2021.

[MISSING_PAGE_FAIL:15]

Cycle Consistency LossDropping the variances, \(\mathcal{L}_{x\to y\to x}=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\)\(||\varepsilon_{\theta}(y_{t},c_{x},x_{0})+\varepsilon_{\theta}(x_{t},c_{y},x_{0})- \varepsilon_{x}-\varepsilon_{y}||_{2}^{2}\).

Proof.: \[\mathcal{L}_{x\to y\to x} =\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ ||x_{0}-G(y_{t},c_{x},x_{0})||_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ ||x_{0}-\big{[} \sqrt{\bar{\alpha}_{t}}\bar{y}_{0}+\sqrt{1-\bar{\alpha}_{t}}\varepsilon_{y}- \sqrt{1-\bar{\alpha}_{t}}\varepsilon_{\theta}(y_{t},c_{x},x_{0})\big{]}/\sqrt{ \bar{\alpha}_{t}}||_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \Big{|}\Big{|} \Big{|}x_{0}-\bar{y}_{0}+\sqrt{\frac{1-\bar{\alpha}_{t}}{\bar{\alpha}_{t}}} \big{[}\varepsilon_{\theta}(y_{t},c_{x},x_{0})-\varepsilon_{y}\big{]}\Big{|} \Big{|}_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \Big{|} \Big{|} \Big{|}x_{0}-G(x_{t},c_{y},x_{0})+\sqrt{\frac{1-\bar{\alpha}_{t}}{\bar{ \alpha}_{t}}}\big{[}\varepsilon_{\theta}(y_{t},c_{x},x_{0})-\varepsilon_{y} \big{]}\Big{|}\Big{|}_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \Big{|} \Big{|} \Big{|}x_{0}-x_{t}/\sqrt{\bar{\alpha}_{t}}+\sqrt{\frac{1-\bar{\alpha}_{t}}{ \bar{\alpha}_{t}}}\big{[}\varepsilon_{\theta}(x_{t},c_{y},x_{0})+\varepsilon_{ \theta}(y_{t},c_{x},x_{0})-\varepsilon_{y}\big{]}\Big{|}\Big{|}_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \Big{|} \Big{|} \Big{|}x_{0}-(\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{\frac{1-\bar{\alpha}_{t}}{ \bar{\alpha}_{t}}}\varepsilon_{x})/\sqrt{\bar{\alpha}_{t}}+\sqrt{\frac{1-\bar {\alpha}_{t}}{\bar{\alpha}_{t}}}\big{[}\varepsilon_{\theta}(x_{t},c_{y},x_{0} )+\varepsilon_{\theta}(y_{t},c_{x},x_{0})-\varepsilon_{y}\big{]}\Big{|}\Big{|} _{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \Big{|} \Big{|} \Big{|}\varepsilon_{\theta}(y_{t},c_{x},x_{0})+\varepsilon_{\theta}(x_{t},c_{ y},x_{0})-\varepsilon_{x}-\varepsilon_{y}\big{]}\Big{|}\Big{|}_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x},\varepsilon_{y}}\ \sqrt{\frac{1-\bar{\alpha}_{t}}{\bar{\alpha}_{t}}}\ || \varepsilon_{\theta}(y_{t},c_{x},x_{0})+\varepsilon_{\theta}(x_{t},c_{y},x_{0})- \varepsilon_{x}-\varepsilon_{y}||_{2}^{2}\]

Invariance LossDropping the variances, \(\mathcal{L}_{x\to y\to y}=\mathbb{E}_{x_{0},\varepsilon_{x}}\ || \varepsilon_{\theta}(x_{t},c_{y},x_{0})-\varepsilon_{\theta}(x_{t},c_{y},\bar{y }_{0})||_{2}^{2}\).

Proof.: \[\mathcal{L}_{x\to y\to y} =\mathbb{E}_{x_{0},\varepsilon_{x}}\ ||\bar{y}_{0}-G(x_{t},c_{y},\bar{y }_{0})||_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\ ||G(x_{t},c_{y},x_{0})-G(x_{t},c_{y}, \bar{y}_{0})||_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\ ||\big{[}\mathscr{H}-\sqrt{1-\bar{ \alpha}_{t}}\varepsilon_{\theta}(x_{t},c_{y},x_{0})\big{]}/\sqrt{\bar{ \alpha}_{t}}-\big{[}\mathscr{H}-\sqrt{1-\bar{\alpha}_{t}}\varepsilon_{\theta}( x_{t},c_{y},\bar{y}_{0})\big{]}/\sqrt{\bar{\alpha}_{t}}||_{2}^{2}\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\ \sqrt{\frac{1-\bar{\alpha}_{t}}{\bar{ \alpha}_{t}}}\ ||\varepsilon_{\theta}(x_{t},c_{y},x_{0})-\varepsilon_{\theta}(x_{t},c_{y}, \bar{y}_{0})||_{2}^{2}\]

### Proof of Self Regularization

Let \(\varepsilon_{\theta}^{*}\) denote the denoising autoencoder of the pre-trained text-guided LDM backbone with a generation function \(G^{*}\). Note that \(x_{t}=S(x_{0},\varepsilon_{x})\).

Proof.: \[\mathcal{L}_{\mathrm{LDM}} =1-\mathbb{E}_{x_{0},\varepsilon_{x}}P\big{[}G(x_{t},c_{y},x_{0}),c_{y}\big{]}-1\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\ \Big{|}1-P\big{[}G(x_{t},c_{y},x_{0}),c_{y} \big{]}\Big{|}-1\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\ \Big{|}1-P\big{[}G^{*}(x_{t},c_{y}),c_{y} \big{]}+P\big{[}G^{*}(x_{t},c_{y}),c_{y}\big{]}-P\big{[}G(x_{t},c_{y},x_{0}),c_{y} \big{]}\Big{|}-1\] \[\leq\mathbb{E}_{x_{0},\varepsilon_{x}}\ \Big{|}1-P\big{[}G^{*}(x_{t},c_{y}),c_{y} \big{]}\Big{|}+\mathbb{E}_{x_{0},\varepsilon_{x}}\ \Big{|}P\big{[}G^{*}(x_{t},c_{y}),c_{y}\big{]}-P\big{[}G(x_{t},c_{y},x_{0}),c_{y} \big{]}\Big{|}-1\]Note that \(\mathbb{E}_{x_{0},\varepsilon_{x}}\left|1-P\big{[}G^{*}(x_{t},c_{y}),c_{y}\big{]} \right|:=\mathbb{E}_{\mathrm{LDM}}^{*}\) is the translation error likelihood of the pre-trained LDM backbone. According to the domain smoothness in Assumption 1, we have:

\[\mathcal{L}_{\mathrm{LDM}} \leq\mathbb{E}_{x_{0},\varepsilon_{x}}\left|P\big{[}G^{*}(x_{t}, c_{y}),c_{y}\big{]}-P\big{[}G(x_{t},c_{y},x_{0}),c_{y}\big{]}\right|+\mathbb{E}_{ \mathrm{LDM}}^{*}-1\] \[\leq\mathbb{E}_{x_{0},\varepsilon_{x}}\left.L\right|\Big{|}G^{*}( x_{t},c_{y})-G(x_{t},c_{y},x_{0})\Big{|}\Big{|}+\mathbb{E}_{\mathrm{LDM}}^{*}-1\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\left.L\right|\Big{|}\Big{[} \mathscr{P}\!\ell-\sqrt{1-\bar{\alpha}_{t}}\varepsilon_{\theta}^{*}(x_{t},c_{ y})\big{]}/\sqrt{\bar{\alpha}_{t}}-\big{[}\mathscr{P}\!\ell-\sqrt{1-\bar{\alpha}_{t}} \varepsilon_{\theta}(x_{t},c_{y},\bar{y}_{0})\big{]}/\sqrt{\bar{\alpha}_{t}} \Big{|}\Big{|}+\mathbb{E}_{\mathrm{LDM}}^{*}-1\] \[=\mathbb{E}_{x_{0},\varepsilon_{x}}\left.L\sqrt{\frac{1-\bar{ \alpha}_{t}}{\bar{\alpha}_{t}}}||\varepsilon_{\theta}(x_{t},c_{y},x_{0})- \varepsilon_{\theta}^{*}(x_{t},c_{y})||_{2}+\mathbb{E}_{\mathrm{LDM}}^{*}-1\right.\]

## Appendix B Scientific Artifacts and Licenses

### The ManiCups Dataset

We introduce ManiCups, a dataset that tasks image editing models to manipulate cups by filling or emptying liquid to/from containers, curated from human-annotated bounding boxes in publicly available datasets and Bing Image Search (under Share license for training and Modify for test set). The dataset consists of a total of 5714 images of empty cups and cups of coffee, juice, milk, and water. We describe our three-stage data collection pipeline in the following paragraphs.

In the **Image Collection** stage, we gather raw images of interest from MSCOCO [26], Open Images [23], as well as Bing Image Search API. For the MSCOCO 2017 dataset, we specifically searched for images containing at least one object from the categories of "bottle," "cup," and "wine glass." Regarding the Open Images v7 dataset, our search focused on images with at least one object falling under the categories of "coffee," "coffee cup," "mug," "juice," "milk," and "wine glass." To conduct the Bing Image Search, we utilized API v7 and employed queries with the formats "empty <container>" and "<container> of <liquid>." The <container> category encompasses cups, glasses, and mugs, while the <liquid> category includes coffee, juice, water, and milk. We obtained approximately 30,000 image candidates after this step.

During the **Image Extraction** stage, we extract regions of interest from the candidate images and resize them to a standardized size of 512\(\times\)512 pixels. Specifically, for subsets obtained from MSCOCO and Open Images, we extract the bounding boxes with labels of interest. To ensure a comprehensive representation, each bounding box is extended by 5% on each side, and the width is adjusted to create a square window. In cases where the square window exceeds the image boundaries, we shift the window inwards. If, due to resizing or shifting, a square window cannot fit within the image, we utilize the cv2.BORDER_REPLICATE function to replicate the edge pixels. All bounding boxes with an initial size less than 128\(\times\)128 are discarded, and the remaining images are resized to a standardized size of 512\(\times\)512 pixels. The same approach is applied to images obtained from the Bing Search API, utilizing cv2.BORDER_REPLICATE for edge pixel replication. Following this step, we obtained approximately 20,000 extracted and resized images.

In the **Filtering and Labeling** stage, our focus is on controlling the quality of images and assigning appropriate labels. Our filtering process begins by identifying and excluding replicated images using the L2 distance metric. Subsequently, we leverage the power of CLIP to detect and remove images containing human faces, as well as cups with a front-facing perspective. Additionally, we use CLIP to classify the remaining images into their respective domains. To ensure the accuracy of the dataset, three human annotators thoroughly review the collected images, verifying that the images portray a top-down view of a container and assigning the appropriate labels to the respective domains. The resulting ManiCups dataset contains 5 domains, including 3 abundant domains (empty, coffee, juice) with more than 1K images in each category and 2 low-resource domains (water, milk) with less than 1K images to facilitate research and analysis in data-efficient learning.

To our knowledge, ManiCups is one of the first datasets targeted to the physical state changes of objects, other than stylistic transfers or type changes of objects. The ability to generate consistent state changes based on manipulation is fundamental for future coherent video prediction [14] as well as understanding and planning for physical agents [49]. We believe that ManiCups is a valuable resource to the community.

### Dataset Statistics

We present the statistics of the ManiCups in Table 3, and other scene/object level datasets in Table 4.

### Licenses of Scientific Artifacts

We present a complete list of references and licenses in Table 5 for all the scientific artifacts we used in this work, including data processing tools, datasets, software source code, and pre-trained weights.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Domains** & **Train** & **Test** & **Total** \\ \hline Empty Cup & 1256 & 160 & 1416 \\ Cup of Coffee & 1550 & 100 & 1650 \\ Cup of Juice & 1754 & 100 & 1854 \\ \hline Cup of Water & 801 & 50 & 851 \\ Cup of Milk & 353 & 50 & 403 \\ \hline Total & 5714 & 460 & 6174 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The statistics of the ManiCups dataset, with 3 abundant domains and 2 low-resource domains.

\begin{table}
\begin{tabular}{l l l} \hline \hline Data Sources & URL & License \\ \hline FiftyOne (Tool) & Link & Apache v2.0 \\ Summer2Winter Yosemite & Link & ImageNet, See Link \\ Horse2Zebra & Link & ImageNet, See Link \\ Apple2Orange & Link & ImageNet, See Link \\ MSCOCO 2017 & Link & CC BY 4.0 \\ Open Images v7 & Link & Apache v2.0 \\ Bing Image Search API v7 & Link & Share (training) \& Modify (test) \\ \hline Software Code & URL & License \\ \hline Stable Diffusion v1 & Link & CreativeML Open RAIL-M \\ Stable Diffusion v2 & Link & CreativeML Open RAIL++-M \\ ControlNet & Link & Apache v2.0 \\ CUT & Link & Mixed, See Link \\ Prompt2Prompt + NullText & Link & Apache v2.0 \\ Stable Diffusion Inpainting & Link & MIT license \\ Text2LIVE & Link & MIT license \\ Stable Diffusion SDEdit & Link & Apache v2.0 \\ Cycle diffusion & Link & Apache v2.0 \\ ILVR & Link & MIT license \\ EGSDE & Link & N/A \\ P2-weighting & Link & MIT license \\ Pix2pix-zero & Link & MIT license \\ Masactrl & Link & Apache v2.0 \\ \hline Metric Implementations & URL & License \\ \hline FID & Link & Apache v2.0 \\ FCD & Link & MIT license \\ CLIP Score & Link & Apache v2.0 \\ PSNR \& SSIM & Link & BSD-3-Clause \\ L2 Distance & Link & BSD-3-Clause \\ \hline \hline \end{tabular}
\end{table}
Table 4: The statistics of the Yosemite summer\(\leftrightarrow\)winter, horse\(\leftrightarrow\)zebra, and apple\(\leftrightarrow\)orange datasets.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Domains** & **Train** & **Test** & **Total** \\ \hline Summer & 1231 & 309 & 1540 \\ Winter & 962 & 238 & 1200 \\ \hline Horse & 1067 & 118 & 1194 \\ Zebra & 1334 & 140 & 1474 \\ \hline Apple & 995 & 256 & 1251 \\ Orange & 1019 & 248 & 1267 \\ \hline \hline \end{tabular}
\end{table}
Table 5: License information for the scientific artifacts used.

Experiment Details

### Computational Resources

Table 6 illustrates the training performance of CycleNet and FastCycleNet on a single NVIDIA A40 GPU under \(256\times 256\) with batch size of 4. The table provides information on the training speed in seconds per iteration and the memory usage in gigabytes for both models. FastCycleNet exhibits a faster training speed of 1.1 seconds per iteration while consuming 24.5 GB of memory. On the other hand, CycleNet demonstrates a slightly slower training speed of 1.8 seconds per iteration, and it requires 27.9 GB of memory.

### Hyper-parameter Decisions

We include the major hyper-parameter tuning decisions for reproducibility purposes. In the training of CycleNet, the weights of our three loss functions are respectively set as \(\lambda_{1}=1\), \(\lambda_{2}=0.1\), and \(\lambda_{3}=0.01\). We train the model for 50k steps. Following [29], we initialize the sampling process with the latent noised input image \(z_{t}\), collected using Equation 1. A standard 50-step sampling is applied at inference time with \(t=100\). Our configuration is as follows:

For more details, please refer to the supplementary codes.

### Baseline Implementations

* **CycleGAN**: We used some of the results provided in [34].
* **CUT**: We used the official code4 provided by the authors. Footnote 4: https://github.com/taesungp/contrastive-unpaired-translation
* **Inpainting + ClipSeg**: we modified from the gradio5 provided by the community. Footnote 5: https://huggingface.co/spaces/multimodalart/stable-diffusion-inpainting
* **Text2LIVE**: We used the official code6 provided by the authors. Footnote 6: https://github.com/omerbt/Text2LIVE
* **ILVR**: We first pre-train the diffusion model using P2-weighting [6], and then generated output using the official code7 provided by the authors.

\begin{table}
\begin{tabular}{c c c} \hline \hline Training & Train Speed (sec/iteration) & Mem Use (GB) \\ \hline FastCycleNet & **1.1** & **24.5** \\ CycleNet & 1.8 & 27.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Speed of CycleNet and FastCycleNet* **EGSDE**: We first pre-train the diffusion model using P2-weighting [6], and then generated output using the official code8 provided by the authors. Footnote 8: https://github.com/ML-GSAI/EGSDE
* **SDEdit**: We used the community implementation9 of SDEdit based on stable diffusion. Footnote 9: https://huggingface.co/docs/diffusers
* **CycleDiffusion**: We modified from the official gradio10 provided by the authors. Footnote 10: https://huggingface.co/spaces/ChenWu98/Stable-CycleDiffusion
* **Prompt2Prompt + NullText**: We used the official code11 provided by the authors. Footnote 11: https://github.com/google/prompt-to-prompt
* **Masactrl**: We used the official code12 provided by the authors. Footnote 12: https://github.com/TencentARC/MasActrl
* **Pix2pix-zero**: We used the official code13 provided by the authors, and we generated the summer2winter direction assets following the scripts provided by the authors. Footnote 13: https://github.com/pix2pixzero/pix2pix-zero

### Evaluation Metrics Explained

Image QualityTo evaluate the quality of images, we employ two metrics.

* **Frechet Inception Distance (FID) [12]** is a widely used metric in image generation tasks. A lower FID score indicates better image quality and more realistic samples.
* **FID\({}_{\mathrm{clip}}\)[24]** combines the FID metric with features extracted with a CLIP [36] encoder, providing better assessment of image quality. Similar to FID, a lower FID\({}_{\mathrm{clip}}\) score represents better image quality.

Translation QualityTo measure to what extent is the translation successful, we use the **CLIP Score**[11], i.e., the CLIP model [36] to obtain the latent representations of images and prompts, and then calculate the cosine similarity between them. A higher CLIP score indicates a stronger similarity between the generated image and the text prompt, thus better translation.

Translation ConsistencyWe measure translation consistency using four different metrics.

* **L2 Distance** is a measure of the Euclidean distance between two images. A lower L2 distance indicates higher similarity and better translation consistency.
* **Peak Signal-to-Noise Ratio (PSNR) [4]** measures the ratio between the maximum possible power of a signal and the power of corrupting noise. A higher PSNR score indicates better translation consistency.
* **Structural Similarity Index Measure (SSIM) [47]** is a metric used to compare the structural similarity between two images. A higher SSIM score suggests higher similarity and better translation consistency.
* **Learned Perceptual Image Patch Similarity (LPIPS) [52]** is a comprehensive evaluation metric for the perceptual similarity between two images. A lower LPIPS score indicates higher perceptual similarity.

## Appendix D Broader Impact

While CycleNet holds great promise, it is essential to address potential broader impacts, including ethical, legal, and societal considerations. One significant concern is copyright infringement. As an image translation model, CycleNet can potentially be used to create derived works from artists' original images, raising the potential for copyright violations. To safeguard the rights of content creators and uphold the integrity of the creative economy, it is imperative to prioritize careful measures and diligently adhere to licensing requirements. Another critical aspect to consider is the potential for fabricated images to contribute to deception and security threats. If misused or accessed by malicious actors, the ability to generate realistic fake images could facilitate misinformation campaigns, fraud, and even identity theft. This underscores the need for responsible deployment and robust security measures to mitigate such risks. CycleNet leverages pre-trained latent diffusion models, which may encode biases that lead to fairness issues. It is worth noting that the proposed method is currently purely algorithmic, devoid of pre-training on web-scale datasets itself. By acknowledging and actively addressing these broader impacts, we can work towards harnessing the potential of CycleNet while prioritizing ethics, legality, and societal well-being.

## Appendix E Addendum to Results

### Choice of Pre-trained LDM Backbone

For all experiments in the main paper, we use Stable Diffusion 2.114 as the pre-trained LDM backbone. We additionally attach a quantitative comparison of CycleNet using Stable Diffusion 1.5,15 which indicates marginal differences in practice.

Footnote 114: https://huggingface.co/stabilityai/stable-diffusion-2-1

Footnote 15: https://huggingface.co/runwayml/stable-diffusion-v1-5

### Image Resolution

We notice that the Stable Diffusion [38] backbone is pre-trained in multiple stages, initially at a resolution of 256\(\times\)256 and followed by another stage at a resolution of 512\(\times\)512 or beyond. This could potentially lead to the under-performance of zero-shot diffusion-based methods on summer\(\rightarrow\)winter and horse\(\rightarrow\)zebra, which are at a resolution of 256\(\times\)256. We repeat the experiment on these two tasks at a generation resolution of 512\(\times\)512 and report the results in Table 8. It's important to acknowledge that this particular setting presents an unfair comparison with considerable challenges for our methods, primarily because the training images are set at a resolution of 256\(\times\)256, yet our model is expected to adapt to a higher resolution. Still, we observe a competitive performance of our models, especially in summer\(\rightarrow\)winter. The diminished effectiveness of our method in the horse\(\rightarrow\)zebra task can be attributed to the fact that the zebra patterns, initially acquired at a resolution of 256\(\times\)256, lose realism and become overly dense when scaled up to 512\(\times\)512 (see Example 3, Figure 11). This limitation can potentially be addressed by scaling to images with multiple resolutions.

### Sudden Convergence

As shown in Figure 8, CycleNet can translate an input image of summer to winter at the beginning of training with no consistency observed. Similar to ControlNet [51], CycleNet also demonstrates the sudden convergence phenomenon, which usually happens around 3k to 8k iterations of training.

### Additional Quantitative Results

In Table 9, we present the complete numerical performance of the state-changing tasks on ManiCups. In general, we found that emptying a cup is a more challenging image editing task for most of

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline Tasks & \multicolumn{6}{c}{**ssumer-winter (512 \(\times\) 512)**} & \multicolumn{6}{c}{horse+zebra (512 \(\times\) 512)} \\ \hline Metrics & FID\({}_{1}\) & FID\({}_{\text{obj}}\) & CLIP \(\uparrow\) & LPIPSPS & PSNR \(\uparrow\) & SSIM \(\uparrow\) & L2\({}^{\times 10^{7}}\) & FID\({}_{1}\) & FID\({}_{\text{obj}}\) & CLIP \(\uparrow\) & LPIPS\({}_{1}\) & PSNR \(\uparrow\) & SSIM\(\uparrow\) & L2\({}^{\times 10^{7}}\) \\ \hline \multicolumn{11}{c}{Masks} & \multicolumn{6}{c}{Masks} & \multicolumn{6}{c}{Masks} & \multicolumn{6}{c}{Masks} & \multicolumn{6}{c}{Masks} & \multicolumn{6}{c}{Masks} & \multicolumn{6}{c}{Masks} \\ \hline Input + ClipSeg & 163.43 & 26.46 & 26.57 & 0.65 & 0.901 & 0.13 & 4.07 & 79.14 & 26.05 & 28.71 & 0.29 & 16.39 & 0.40 & 1.95 \\ TextXLVI & 86.12 & 18.30 & 25.59 & 0.27 & 16.35 & 0.68 & 1.07 & 10.13 & 14.27 & 27.15 & 31

[MISSING_PAGE_FAIL:22]

Figure 9: Additional qualitative results using CycleNet on Manicups dataset.

Figure 10: Additional qualitative comparison of our method with other baselines.

Figure 11: Additional high-resolution examples from Yosemite summer\(\leftrightarrow\)winter HD [15] and MSCOCO [26].

Figure 12: Additional examples of output diversity in the target domains and zero-shot generalization to out-of-domain distributions.