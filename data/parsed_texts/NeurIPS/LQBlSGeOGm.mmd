# How Molecules Impact Cells:

Unlocking Contrastive PhenoMolecular Retrieval

 Philip Fradkin\({}^{1,2,}\), Puria Azadi\({}^{1,3,}\),

**Karush Suri\({}^{1}\)**, Frederik Wenkel\({}^{1}\)**, Ali Bashashi\({}^{3}\)**,

**Maciej Sypetkowski\({}^{1}\)\({}^{1}\)**, Dominique Beain\({}^{1,4,}\)\({}^{1}\)**

\({}^{1}\) Valence Labs, \({}^{2}\) University of Toronto, Vector Institute,

\({}^{3}\) University of British Columbia, \({}^{4}\) Universite de Montreal, Mila- Quebec AI Institute

dominique@valencelabs.com

These two authors contributed equally (ordered alphabetically) and reserve the right to swap their order.Equal advising.

###### Abstract

Predicting molecular impact on cellular function is a core challenge in therapeutic design. Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. In this work, we learn a joint latent space between molecular structures and microscopy phenomena experiments, aligning paired samples with contrastive learning. Specifically, we study the problem of _Contrastive PhenoMolecular Retrieval_, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration. We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration. Following this recipe, we propose _MoIPhenix_, a molecular _phenomics_ model. MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. In particular, we demonstrate an 8.1\(\times\) improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications.

## 1 Introduction

Quantifying cellular responses elicited by genetic and molecular perturbations represents a core challenge in medicinal research [4, 57]. Out of an approximate \(10^{60}\) druglike molecule designs, a small number are able to alter cellular properties to reverse the course of diseases [5, 27]. In recent years, microscopy-based cell morphology screening techniques, demonstrated potential for quantitative understanding of a molecule's biological effects. Experimental techniques such as cell-painting are used to capture cellular morphology, which correspond to physical and structural properties of the cell [6, 7]. Cells treated with molecular perturbations can change morphology, which is captured by staining and high throughput microscopy techniques. Perturbations with similar cellular impact induce analogous morphological changes, allowing to capture underlying biologicaleffects in phenomic experiments. Identifying such perturbations with similar morphological changes can aid in discovery of novel therapeutic drug candidates [50; 29; 22].

Determining molecular impact on the cell can be formulated as a multi-modal learning problem, allowing us to build on a rich family of methods [43; 62; 53]. Similar to text-image models, paired data is collected from phenomic experiments along with molecules used to perturb the cells. Contrastive objectives have been used as an effective approach in aligning paired samples from different modalities [43; 32]. A model that has learned a cross-modal joint latent space must be able to retrieve a molecular perturbant conditioned on the phenomic experiment. We identify this problem as _contrastive phenomolecular retrieval_ (see Figure 2). Addressing this problem can allow for identification of molecular impact on cellular function, however, this comes with its own set of challenges. [18; 2; 65].

**(1)** Firstly, multi-modal paired phenomics molecular data suffers from lower overall dataset sizes and is subject to batch effects. Challenges with uniform processing and prohibitive costs associated with acquisition of paired data, leads to an order of magnitude fewer data points compared to text-image datasets [49; 11]. Furthermore, data is subject to random batch effects that capture non-biologically meaningful variation [33; 55]. **(2)** Paired phenomic-molecular data contains inactive perturbations that do not have a biological effect or do not perturb cellular morphology. It is difficult to infer a priori whether a molecule has a cellular effect, leading to the collection of paired molecular structures with unperturbed cells. These data-points are challenging to filter out without an effective phenomic embedding, as morphological effects are rarely discernible. These samples can be interpreted as misannotated, under the assumption of all collected pairs having biologically meaningful interactions. **(3)** Finally, a complete solution for capturing molecular effects on cells must capture molecular concentration. The same molecule can have drastically different effects along its dose response curve, thus making concentration an essential component for learning molecular impact.

In this work, we explore the problem of contrastive phenomolecular retrieval by addressing the above challenges circumvented in prior works. Our key contributions are as follows:

* We demonstrate significantly higher phenomolecular retrieval rates by utilizing a pretrained unimodal phenomic encoder. Thus alleviating the data availability challenge, reducing the impact of batch effects, and identifying molecular activity levels.
* We propose a novel soft-weighted sigmoid locked loss (S2L) that addresses the effects of inactive molecules. This is done by leveraging distances computed in the phenomic embedding space to learn inter-sample similarities.
* We explore _explicit_ and _implicit_ methods to encode molecular concentration, assessing the model's ability to perform retrieval in an inter-concentration setting and generalize to unseen concentrations.

Figure 1: Illustration of proposed guidelines when incorporated in our _MolPhenix_ contrastive phenomolecular retrieval framework. We address challenges by utilizing uni-modal pretrained MAE & MPNN models, inter-sample weighting with a dosage aware S2L loss, undersampling inactive molecules, and encoding molecular concentration.

[MISSING_PAGE_EMPTY:3]

(such as molecules) which are obtained at a specific dosage concentration \(\mathbf{c}_{i}\in\mathbf{C}\), while \(\psi\in\mathbf{\Psi}\) denotes molecular activity threshold.

Figure 2 describes the problem of contrastive phenomolecular retrieval, where for a single image \(\mathbf{x}_{i}\), the challenge consists of identifying the matching perturbation, \(\mathbf{m}_{i}\), and concentration, \(\mathbf{c}_{i}\), used to induce morphological effects. This can be accomplished in a zero-shot way by generating embeddings for \((\mathbf{m}_{1},\mathbf{c}_{1}),...(\mathbf{m}_{j},\mathbf{c}_{j})\) and \(\mathbf{x}_{i}\) using functions \(f_{\theta_{m}}(\mathbf{m},\mathbf{c})\), \(f_{\theta_{x}}(\mathbf{x})\) which map samples into \(\mathbb{R}^{d}\). Then, by defining a similarity metric between generated embeddings \(\mathbf{z}_{x_{i}}\) and \(\mathbf{z}_{m_{i}}\), \(f_{sim}\), we can rank \((\mathbf{m}_{1},\mathbf{c}_{1})...(\mathbf{m}_{j},\mathbf{c}_{j})\) based on computed similarities. An effective solution to the contrastive phenomolecular retrieval problem would learn \(f_{\theta_{m}}(\mathbf{m},\mathbf{c})\) and \(f_{\theta_{x}}(\mathbf{x})\) that results in consistently high retrieval rates of \((\mathbf{m}_{i},\mathbf{c}_{i})\) used to perturb \(\mathbf{x}_{i}\).

In practice, the image embeddings are generated using a phenomics microscopy foundation MAE model [28; 20]. We use phenomic embeddings to marginalize batch effects, infer inter-sample similarities, and undersample inactive molecules. Activity is determined using consistency of replicate measurements for a given perturbation. For each sample, a \(p\) value cutoff \(\psi\in\mathbf{\Psi}\) is used to quantify molecular activity. Only molecules below the \(p\) value cutoff \(\psi\) are considered active.

Prior methods in multi-modal contrastive learning utilize the InfoNCE loss, and variants thereof [38] to maximize the joint likelihood of \(\mathbf{x}_{i}\) and \(\mathbf{m}_{i}\). Given a set of \(N\times N\) random samples \((\mathbf{x}_{1},\mathbf{m}_{1},\mathbf{c}_{1}),\cdots,(\mathbf{x}_{N}, \mathbf{m}_{N},\mathbf{c}_{N})\) containing \(N\) positive samples at \(k^{\text{th}}\) index and \((N-1)\times N\) negative samples, optimizing Equation 1 maximizes the likelihood of positive pairs while minimizing the likelihood of negative pairs:

\[\mathcal{L}_{\text{InfoNCE}}=-\frac{1}{N}\sum_{i=1}^{N}\left[\log\frac{\exp( \left\langle\mathbf{z}_{x_{i}},\mathbf{z}_{m_{i}}\right\rangle/\left.\tau \right\rangle}{\sum_{k=1}^{N}\exp(\left\langle\mathbf{z}_{x_{i}},\mathbf{z}_{ m_{k}}\right\rangle/\tau)}+\log\frac{\exp(\left\langle\mathbf{z}_{x_{i}}, \mathbf{z}_{m_{i}}\right\rangle/\left.\tau\right\rangle}{\sum_{k=1}^{N}\exp( \left\langle\mathbf{z}_{m_{i}},\mathbf{z}_{x_{k}}\right\rangle/\tau)}\right].\] (1)

Where \(\mathbf{z}_{x}\), \(\mathbf{z}_{m}\) correspond to phenomics and molecular embeddings respectively, \(\tau\) is softmax temperature, and \(\left\langle\cdot\right\rangle\) corresponds to cosine similarity.

#### Challenge 1: Phenomic Pretraining and Generalization

We find that using a phenomics foundation model to embed microscopy images allows for mitigation of batch effects, reduces the required number of paired data points, and improves generalization in the process. While CLIP, a hallmark model in the field of text-image multi-modality, was trained on 400 million curated paired data points, there is an order of magnitude fewer paired molecular-phenomic molecule samples [43]. Cost and systematic pre-processing of data make large scale data generation efforts challenging, and resulting data is affected by experimental batch effects. **Batch effects** induce noise in the latent space as a result of random perturbations in the experimental process, while biologically meaningful variation remains unchanged [39; 52]. Limited dataset sizes and batch effects make it challenging for contrastive learners to capture molecular features affecting cell morphology, yielding low retrieval rates [48].

We address data availability and generalization challenges by utilizing representations from a large **unimodal pre-trained phenomic model**, \(\theta_{\text{ph}}\), trained to capture representations of cellular morphology. \(\theta_{\text{Ph}}\) is pretrained on microscopy images using a Fourier modified MAE objective, utilizing the ViT-L/8 architecture with methodology similar to Kraus et al. (2024) [20; 14; 28]. For simplicity in future sections, we refer to this model as _Phenom1_. This pretrained model allows a drastic

Figure 2: Illustration of the contrastive phenomolecular retrieval challenge. Image \(\mathbf{x}_{i}\) and a set of molecules and corresponding concentrations \((\mathbf{m}_{k},\mathbf{c}_{k})\) get mapped into a \(\mathbb{R}^{d}\) latent space. Their similarities get computed with \(f_{sim}\) and ranked to evaluate whether the paired perturbation appears in the top K%.

reduction in the required number of paired multi-modal samples [63]. In addition, using phenomic representations alleviates the challenge of batch effects by averaging samples, \(\mathbf{z}_{x}\), generated with the same perturbation \(\mathbf{m}_{i}\) over multiple lab experiments \(\epsilon_{i}\). Averaging model representations \(\frac{1}{N}\Sigma_{i\in N}^{1}\mathbf{z}_{x_{i}}\) allows marginalizing batch effect induced by individual experiments.

**Guideline 1**: _Utilizing pre-trained uni-modal encoder, \(\theta_{\text{ph}}\), can be used to reduce the number of paired data-points compared to training \(\theta\) without prior optimization. In addition, averaging phenomic embeddings \(\mathbf{z}_{x}\) from matched perturbations can alleviate batch effects._

To reason over molecular structures, we make use of features learned from GNNs trained on molecular property prediction [34]. We utilize a pretrained MPNN foundational model up to the order of 1B parameters for extracting molecular representations following a similar procedure to Sypetkowski et al. (2024) [56]. We refer to this model as _MolGPS_.

**Challenge 2: Inactive Molecular Perturbations**

The phenomics-molecular data collection process can result in pairing of molecular structures with unperturbed cells in cases where the molecule has no effect on cell morphology (Figure 3)

Since the morphological effects observed in cell \(\mathbf{x}_{i}\) is conditioned on the perturbation, in the absence of a molecular effect \(P(\mathbf{x}_{i}|\mathbf{x}_{i}^{0},\mathbf{c}_{i},\mathbf{m}_{i})\sim P( \mathbf{x}_{i}|\mathbf{x}_{i}^{0})\). In these samples, phenomic data will be independent, from paired molecular data, which results in misannotation under the assumption of data-pairs having an underlying biological relationship. We demonstrate how utilizing Phenom1 to undersample inactive molecules and learn continuous similarities between samples can alleviate this challenge.

To **undersample inactive molecules**, we extract the embeddings from Phenom1 and calculate the relative activity of each perturbation \((\mathbf{m}_{i},\mathbf{c}_{i})\in(\mathbf{M},\mathbf{C})\). This is done using the rank of cosine similarities between technical replicates produced for a molecular perturbation against a null distribution. The null distribution is established by calculating cosine similarities from random pairs of Phenom1 embeddings generated with perturbation \((\mathbf{m}_{j},\mathbf{c}_{j}),(\mathbf{m}_{k},\mathbf{c}_{k})\). Hence, we can compute a p-value and filter out samples likely to belong to the null distribution with an arbitrary threshold \(\psi\).

In addition, by utilizing an inter-sample aware S2L **training objective**, the model can learn similarities between inactive molecules. S2L is grounded in previous work which demonstrates improved robustness to label noise (SigLip) and learnable inter-sample associations (CWCL) [62, 53]. Continuous Weighted Contrastive Loss (CWCL) provides better multi-modal alignment using a uni-modal pretrained model to suggest sample distances, relaxing the negative equidistant assumption present in InfoNCE [53]:

\[\mathcal{L}_{\text{CWCL, }\mathcal{M}\rightarrow\mathcal{X}}=-\frac{1}{N} \sum_{i=1}^{N}\left[\frac{1}{\sum_{j=1}^{N}\mathbf{w}_{i,j}^{\mathcal{X}}} \sum_{j=1}^{N}\mathbf{w}_{i,j}^{\mathcal{X}}\log\frac{\exp\left(\left\langle \mathbf{z}_{x_{i}},\mathbf{z}_{m_{j}}\right\rangle/\tau\right)}{\sum_{k=1}^{N} \exp\left(\left\langle\mathbf{z}_{x_{i}},\mathbf{z}_{m_{k}}\right\rangle/\tau \right)}\right].\] (2)

CWCL weights logits with a continuous measure of similarity \(\mathbf{w}^{\mathcal{X}}\), resulting in better alignment of embeddings \(\mathbf{z}_{\mathbf{x}_{i}}\) and \(\mathbf{z}_{\mathbf{m}_{j}}\) across modalities. In equation 2, \(\mathbf{w}^{\mathcal{X}}\) is computed using a within modality similarity function such as \(\mathbf{w}_{i,j}^{\mathcal{X}}=\left\langle z_{\mathbf{x}_{i}},z_{\mathbf{x}_{ j}}\right\rangle/2+0.5\). Note, the above formula is used only for mapping samples from modality \(\mathcal{M}\) to \(\mathcal{X}\) for which a pre-trained model \(\theta_{\text{ph}}\)is available.

Another work, SigLIP, demonstrates robustness to label noise and reduces computational requirements during contrastive training [62]. It does so by avoiding computation of a softmax over the entire set of in-batch samples, instead relying on element-wise sigmoid operation:

\[\mathcal{L}_{\text{SigLIP}}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N}\left[\log \frac{1}{1+\exp\left(\mathbf{y}_{i,j}(-\alpha\left\langle\mathbf{z}_{\mathbf{x }_{i}},\mathbf{z}_{\mathbf{m}_{j}}\right\rangle+b)\right)}\right].\] (3)

Figure 3: Data generation process of a phenomic experiment on cells \(\mathbf{x_{i}}\) with molecular perturbations \(\mathbf{m_{i}}\) and concentrations \(\mathbf{c_{i}}\).

In equation 3, \(\alpha\) and \(b\) are learned, calibrating the model confidence conditioned on the ratio of positive to negative pairs. \(\mathbf{y}_{i,j}\) is set to 1 if \(i=j\) and -1 otherwise.

Inspired by prior works, we introduce S2L for molecular representation learning, which leverages inter-sample similarities and robustness to label noise to mitigate weak or inactive perturbations.

\[\mathcal{L}_{\text{S2L}}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N}\log\left[ \frac{\mathbf{w}_{i,j}^{\mathcal{X}}}{1+\exp\left(-\alpha\langle\mathbf{z}_{ \mathbf{x}_{i}},\mathbf{z}_{\mathbf{m}_{j}}\rangle+b\right)\right)}+\frac{(1- \mathbf{w}_{i,j}^{\mathcal{X}})}{1+\exp\left(\alpha\langle\mathbf{z}_{\mathbf{ x}_{i}},\mathbf{z}_{\mathbf{m}_{j}}\rangle+b\rangle\right)}\right].\] (4)

In the equation above, \(\mathbf{z}_{\mathbf{x}_{i}}\) and \(\mathbf{z}_{\mathbf{m}_{j}}\) correspond to latent representations of images and molecules, respectively. \(\alpha\) and \(b\) correspond to learnable temperature and bias parameters for the calibrated sigmoid function. \(\mathbf{w}_{ij}^{\mathcal{X}}\) is an inter-sample similarity function computed from images using the pretrained model \(\theta_{\mathbf{p}_{\mathbf{n}}}\). To compute \(\mathbf{w}_{i,j}^{\mathcal{X}}\), we use the arctangent of L2 distance instead of cosine similarity, as was the case for Equation 2 (more details in Appendix D.3). Intuitively, S2L can be thought of as shifting from a multi-class classification to a soft multi-label problem. In our problem setting, the labels are continuous and determined by sample similarity in the phenomics space.

```
1:# mol_emb : molecule model embedding [n, dim]
2:# phn_emb : phenomics model embedding [n, dim]
3:# t_prime, b : learnable temperature and bias
4:# n : mini-batch size
5:# \(\langle\cdot\rangle\) : custom similarity function
6:# \(\gamma,\zeta\) : similarity dampening parameters
7:
8:t = exp(t_prime)
9:zmol = 12_normalize(mol_emb)
10:zphn = 12_normalize(phn_emb)
11:logits = dot(zmol, zphn.T) * t + b
12:sim_matrix = \(\langle\)zphn, zphn.T\(\rangle\) # [n, n] sample similarity matrix
13:pos = log_sigmoid(logits)
14:neg = log_sigmoid(- logits)
15:l = sim_matrix * pos + (\(\gamma\) - \(\zeta\) sim_matrix) * neg
16:l = - sum(l) / n ```

**Algorithm 1** S2L loss pseudo-implementation.

#### Challenge 3: Variable Concentrations

Perturbation effect on a cell is determined by both molecular structure and corresponding concentration [58]. A model capturing molecular impact on cell morphology must be able to generalize across different doses, since variable concentrations can correspond to different data distributions.

We note that providing concentrations \(\mathbf{c}_{i}\) as input to the model would benefit performance, as this would indicate the magnitude of molecular impact. However, we find that simply concatenating concentrations does not result in effective training due to its compressed dynamic range. To that end, we add concentration information in two separate ways: _implicit_ and _explicit_ formulations.

We add **implicit concentration** as molecular perturbation classes by using the S2L loss (Equation 4) to treat perturbation \(\mathbf{m}_{i}\) with concentrations \(\mathbf{c}_{i}\) and \(\mathbf{c}_{j}\) as distinct classes. This pushes samples apart in the latent space proportionally to similarities between phenomic experiments.

We add **explicit concentration**\(c_{i}\) by passing it to the molecular encoder. We explore different formulation for dosage concentrations, \(\mathbf{f}^{\prime}(c_{i})\), where \(\mathbf{f}^{\prime}\) maps \(\mathbf{c_{i}}\rightarrow\mathbb{R}\). Encoded representations \(\mathbf{f}^{\prime}(c_{i})\)are concatenated at the initial layer of the model. We find simple functional encodings \(\mathbf{f}^{\prime}\) (such as one-hot and logarithm) to work well in practice.

**Guideline 3**: _When training a molecular-phenomic model, conditioning on an (implicit and explicit) representation of concentration \(\mathbf{f}^{\prime}(\mathbf{c}_{i})\) aids in capturing molecular impacts on cell morphology and improves generalization to previously unseen molecules and concentrations._

## 4 Experimental Setup

In this section, we describe evaluation datasets used, and descriptions of the underlying data modalities. To assess phenomolecular retrieval, we use 1% recall metric unless stated otherwise, as it allows direct comparison between datasets with different number of samples. Additional implementation and evaluation details can be found in Appendix D.

Datasets:Our training dataset consists of fluorescent microscopy images paired with molecular structures and concentrations, which are used as perturbants. We assess models' phenomolecular retrieval capabilities on three datasets of escalating generalization complexity. First dataset, consisting of unseen microscopy images and molecules present in the training dataset. Second, a dataset consisting of previously unseen phenomics experiments and molecules split by the corresponding molecular scaffold. Finally, we evaluate on an open source dataset with a different data generating distribution [16]. In the case of the latter two datasets, the model is required to perform zero-shot classification, as it has no access to those molecules in the training data. This requires the model to reason over molecular graphs to identify structures inducing corresponding cellular morphology changes. Using methodology described in guideline 2 we report retrieval results for all molecules as well as on an active subset. Finally, all datasets are comprised of molecular structures at multiple concentrations (\(.01,.1,1.0,10\), etc.) Additional details regarding the datasets can be found in Appendix C.

Modality Representations:In our evaluations, we consider different representations for molecular perturbations and phenomic experiments and quantitatively evaluate their impact.

* Images: Image encoders utilize 6-channel fluorescent microscopy images of cells representing phenomic experiments. Images are 2048 \(\times\) 2048 pixels, capturing cellular morphology changes post molecular perturbation. We downscale each image to 256 \(\times\) 256 using block mean downsampling.
* Phenom1: We characterize phenomic experiments by embedding high resolution microscopy images in the latent space of a phenomics model \(\theta_{Ph}\) as described in guideline 1.
* Fingerprints: Molecular fingerprints utilize RDKIT [31], MACCS [30] and MORGAN3 [46] bit coding, which represent binary presence of molecular substructures. Additional information such as atomic identity, atomic radius and torsional angles are included in the fingerprint representations.
* MolGPS: We generate molecular representations from a large pretrained GNN. Specifically, we obtain molecular embeddings from a 1B parameter MPNN [34].

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline  & & \multicolumn{3}{c}{**Active Molecules**} & \multicolumn{3}{c}{**All Molecules**} \\ \cline{3-10} Method & & & Unseen & Unseen & Unseen & Unseen & Unseen & Unseen \\  & & Im. & Im. + Mol. & Dataset & Im. & Im. + Mol. & Dataset \\ \hline CLOOME & Images \& Multi-FPS & \(0.756\pm 0.042\) & \(0.787\pm 0.005\) & \(0.528\pm 0.0057\) & \(0.547\pm 0.0028\) & \(0.0661\pm 0.0020\) & \(0.2223\pm 0.0014\) \\ CLOOME & Phenom1 \& Multi-FPS & \(4.659\pm 0.042\) & \(5057\pm 0.0014\) & \(2065\pm 0.146\) & \(3009\pm 0.0053\) & \(2.474\pm 0.013\) & \(1.337\pm 0.0045\) \\ MolPhenix & Phenom1 \& Multi-FPS & \(\mathbf{7.807}\pm\mathbf{0.0025}\) & \(6.365\pm 0.004\) & \(3.354\pm 0.0097\) & \(\mathbf{5.253}\pm\mathbf{0.0029}\) & \(\mathbf{3.655}\pm\mathbf{0.0017}\) & \(2163\pm 0.0021\) \\ MolPhenix & Phenom1 \& MolGPS & \(7.646\pm 0.0014\) & \(\mathbf{63.887}\pm\mathbf{0.0056}\) & \(\mathbf{4160}\pm\mathbf{0.0016}\) & \(5.0012\pm 0.0002\) & \(3511\pm 0.0041\) & \(\mathbf{2508}\pm\mathbf{0.0026}\) \\ \hline MolPhenix* & Phenom1 \& MolGPS & \(\mathbf{9.688}\pm\mathbf{0.017}\) & \(\mathbf{7.733}\pm\mathbf{0.036}\) & \(\mathbf{5860}\pm\mathbf{0.0082}\) & \(\mathbf{5.558}\pm\mathbf{0.007}\) & \(\mathbf{3.824}\pm\mathbf{0.016}\) & \(\mathbf{2.809}\pm\mathbf{0.0060}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an \(\mathbf{8.1}\times\) improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenixâ€™s main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.

[MISSING_PAGE_EMPTY:8]

retrieval performance gap, throughout training, with a **1.26**\(\times\) final improvement (Figure 4, Table 1). Compared to CLOOME [48] trained directly on images, MolPhenix achieves an average improvement of \(\mathbf{8.78\times}\) on active molecules on the unseen dataset. These results verify the effectiveness of Guideline 1 in accelerating training, and the importance of Guidelines 2 and 3 in recall improvements over CLOOME.

We evaluate the impact of different loss objectives on the proposed MolPhenix training framework. Table 2 presents top-1% retrieval accuracy across different contrastive losses utilized to train molecular-phenomics encoders on cumulative concentrations. Compared to prior methods, the proposed S2L loss demonstrates improved retrieval rates in cumulative concentration setting. Label noise and inter-sample similarity aware losses such as CWCL and SigLip also demonstrate improved performance. The effectiveness of S2L can be attributed to smoothed inter-sample similarities and implicit concentration information.

Finally, in Table 3, we observe recall improvements when considering both molecular structures and concentration. We note the importance of the addition of implicit concentration, further confirming the importance of considering molecular effects at different concentrations as different classes. Explicitly encoding molecular concentration with one-hot, logarithm and sigmoid yields improved recall performance, where one-hot performs the best in a cumulative concentration setting. These findings verify the efficacy of implicit and explicit concentration encoding outlined in Guideline 3.

Results are averaged across experiments for each dropped concentration, and across three seeds. Recall is reported for active molecules, while the results for all molecules can be found in Table 13.

### Evaluation on held-out concentrations:

Next, we evaluate recall on held-out concentrations to obtain a measure of generalization performance. This evaluation allows us to capture the utility of our models for prediction of unseen concentrations, hence resembling _in-silico_ testing. We omit concentrations from the training set and evaluate recall at the excluded data, where we observe a drop in retrieval performance for unseen concentrations. Similar to cumulative concentration results, we find that using S2L improves recall over other losses and outperforms CLOOME by up to 126% (Table 4). While one-hot encoding exhibits significant improvements in cumulative concentrations, its expressivity on unseen concentrations is limited (Table 5) and sigmoid encoding provides a sufficient representation of concentration.

### Ablation Studies

We assess the importance of our design decisions by conducting an ablation study over our proposed guidelines. Figure 5 presents the variation of top-1% recall accuracy across key components such as cutoff \(p\) value, fingerprint type, and embedding averaging. We observe that employing a lower cutoff \(p\) value yields improved generalization for unseen dataset, while employing a higher cutoff appears to be optimal for unseen images + unseen molecules. For molecular structure representations, we find that using embeddings from the large pretrained MPNN graph based model (e.g., MolGPS) surpasses traditional fingerprints. Finally, utilization of embedding averaging demonstrates improved recall.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Loss & \begin{tabular}{c} Unseen \\ Im. \\ \end{tabular} & \begin{tabular}{c} Unseen \\ Im. + Mol. \\ \end{tabular} & \begin{tabular}{c} Unseen \\ Dataset \\ \end{tabular} \\ \hline \begin{tabular}{l} CLIP \\ Hopfield-CLIP \\ InfoLOOB \\ \end{tabular} &.2122 &.2496 &.1519 \\ \begin{tabular}{l} CLOOME \\ \end{tabular} &.2164 &.2461 &.1479 \\ \begin{tabular}{l} PCL \\ \end{tabular} &.4717 &.4027 &.2841 \\ \begin{tabular}{l} CVCL \\ SigLip \\ \end{tabular} &.5731 &.4403 &.3232 \\ 
\begin{tabular}{l} S2L (ours) \\ \end{tabular} & **.8334** & **.4615** & **.3792** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Top-1% recall accuracy of different loss objectives while using the proposed MolPhenix guidelines, such as Phenom1 and embedding averaging.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Loss} & \begin{tabular}{c} Unseen \\ Im. \\ \end{tabular} & \begin{tabular}{c} Unseen \\ Im. + Mol. \\ \end{tabular} & \begin{tabular}{c} Unseen \\ Dataset \\ \end{tabular} \\ \hline \begin{tabular}{l} CLIP \\ Hopfield-CLIP \\ InfoLOOB \\ \end{tabular} &.2122 &.2496 &.1501 \\ \begin{tabular}{l} CLOOME \\ \end{tabular} &.2164 &.2461 &.1479 \\ \begin{tabular}{l} PCL \\ \end{tabular} &.4717 &.4027 &.2841 \\ \begin{tabular}{l} CVCL \\ SigLip \\ \end{tabular} &.5731 &.4403 &.3232 \\ \begin{tabular}{l} SigLip \\ \end{tabular} &.5718 &.4217 &.3021 \\ 
\begin{tabular}{l} S2L (ours) \\ \end{tabular} & **.8334** & **.4615** & **.3792** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Top-1% recall accuracy across different concentration encoding choices while using the proposed MolPhenix guidelines, such as Phenom1 and embedding averaging.

## 6 Conclusion

In this work, we investigate the problem of _contrastive phenomolecular retrieval_ by constructing a joint multi-modal embedding of phenomic experiments and molecular structures. We identify a set of challenges afflicting molecular-phenomic training and proposed a set of guidelines for improving retrieval and generalization. Empirically, we observed that contrastive learners demonstrate higher retrieval rates when using representations from a high-capacity uni-modal pretrained model. Use of inter-sample similarities with a label noise resistant loss such as S2L allows us to tackle the challenge of inactive molecules. Finally, adding implicit and explicit concentrations allows models to generalize to previously unseen concentrations. MolPhenix demonstrates an **8.1\(\times\)** improvement in zero shot retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. In addition, we conduct a preliminary investigation on MolPhenix's ability to uncover biologically meaningful properties (activity prediction, zero-shot biological perturbation matching, and molecular property prediction in Appendix E.1, E.2, and E.3, respectively.). We expect a wide range of applications for MolPhenix, particularly in drug discovery. While there's a remote chance of misuse for developing chemical weapons, such harm is unlikely, with our primary focus remaining on healthcare improvement.

**Limitations and Future Work:** While our study covers challenges in phenomolecular recall, we leave three research directions for future work. (1) Future investigations could consider studying additional modalities such as text, genetic perturbations and chemical multi-compound interventions. (2) While we propose and evaluate our guidelines on previously conducted phenomic experiments, we note that a rigorous evaluation would evaluate model predictions in a wet-lab setting. (3) In addition, our work makes the assumption that the initial unperturbed cell state \(x_{i}^{0}\) can be marginalized by utilizing a single cell line with an unperturbed genetic background. Future works can relax this assumption, aiming to capture innate intercellular variation.

## Acknowledgments and Disclosure of Funding

We thank the broader Valence Labs team and Recursion Pharmaceuticals for support on the project. We thank Berton Earnshaw, Jason Hartford, Emmanuel Bengio, Oren Kraus, and Emmanuel Noutahi for providing valuable feedback on the manuscript.

Figure 5: Ablations of top-1 % recall accuracy with **(top-left)** the size of embedding dimension, **(top-center)** number of parameters, **(top-right)** batch size, **(bottom-left)** cutoff \(p\) value, **(bottom-center)** fingerprint type, and **(bottom-right)** random batch averaging. Compact embedding sizes from pretrained models, larger number of parameters, larger batch sizes, lower cutoff p-values, pretrained MolGPS fingerprints and presence of random batch averaging improving retrieval of our MolPhenix framework.

## References

* [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning, 2022.
* [2] S. Albelwi. Survey on self-supervised learning: auxiliary pretext tasks and contrastive learning methods in imaging. _Entropy_, 24(4):551, 2022.
* [3] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A cookbook of self-supervised learning, 2023.
* [4] C. Bock, P. Datlinger, F. Chardon, M. A. Coelho, M. B. Dong, K. A. Lawson, T. Lu, L. Maroc, T. M. Norman, B. Song, G. Stanley, S. Chen, M. Garnett, W. Li, J. Moffat, L. S. Qi, R. S. Shapiro, J. Shendure, J. S. Weissman, and X. Zhuang. High-content crispr screening. _Nature Reviews Methods Primers_, 2(1), Feb. 2022.
* [5] R. S. Bohacek, C. McMartin, and W. C. Guida. The art and practice of structure-based drug design: A molecular modeling perspective. _Medicinal Research Reviews_, 16(1):3-50, Jan. 1996.
* [6] M. Boutros, F. Heigwer, and C. Laufer. Microscopy-based high-content screening. _Cell_, 163(6):1314-1325, 2015.
* [7] M.-A. Bray, S. Singh, H. Han, C. T. Davis, B. Borgeson, C. Hartland, M. Kost-Alimova, S. M. Gustafsdottir, C. C. Gibson, and A. E. Carpenter. Cell painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes. _Nature protocols_, 11(9):1757-1774, 2016.
* [8] M.-A. Bray, S. Singh, H. Han, C. T. Davis, B. Borgeson, C. Hartland, M. Kost-Alimova, S. M. Gustafsdottir, C. C. Gibson, and A. E. Carpenter. Cell painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes. _Nature protocols_, 11(9):1757-1774, 2016.
* [9] J. C. Caicedo, S. Cooper, F. Heigwer, S. Warchal, P. Qiu, C. Molnar, A. S. Vasilevich, J. D. Barry, H. S. Bansal, O. Kraus, et al. Data-analysis strategies for image-based cell profiling. _Nature methods_, 14(9):849-863, 2017.
* [10] S. Cao, P. Xu, and D. A. Clifton. How to understand masked autoencoders. _arXiv preprint arXiv:2202.03670_, 2022.
* [11] S. N. Chandrasekaran, J. Ackerman, E. Alix, D. M. Ando, J. Arevalo, M. Bennion, N. Boisseau, A. Borowa, J. D. Boyd, L. Brino, P. J. Byrne, H. Ceulemans, C. Ch'ng, B. A. Cimini, D.-A. Clevert, N. Deflaux, J. G. Doench, T. Dorval, R. Doyonnas, V. Dragone, O. Engkvist, P. W. Faloon, B. Fritchman, F. Fuchs, S. Garg, T. J. Gilbert, D. Glazer, D. Gnutt, A. Goodale, J. Grignard, J. Guenther, Y. Han, Z. Hanifelhou, S. Hariharan, D. Hernandez, S. R. Horman, G. Hormel, M. Huntley, I. Icke, M. Iida, C. B. Jacob, S. Jaensch, J. Khetan, M. Kost-Alimova, T. Krawiec, D. Kuhn, C.-H. Lardeau, A. Lembke, F. Lin, K. D. Little, K. R. Lofstrom, S. Lotfi, D. J. Logan, Y. Luo, F. Madoux, P. A. Marin Zapata, B. A. Marion, G. Martin, N. J. McCarthy, L. Mervin, L. Miller, H. Mohamed, T. Monteverde, E. Mouchet, B. Nicke, A. Ogier, A.-L. Ong, M. Osterland, M. Otrocka, P. J. Peeters, J. Pilling, S. Prechtl, C. Qian, K. Rataj, D. E. Root, S. K. Sakata, S. Scrace, H. Shimizu, D. Simon, P. Sommer, C. Spruiell, I. Sumia, S. E. Swalley, H. Terauchi, A. Thibaudeau, A. Unruh, J. Van de Waeter, M. Van Dyck, C. van Staden, M. Warchol, E. Weisbart, A. Weiss, N. Wiest-Daessle, G. Williams, S. Yu, B. Zapiec, M. Zyla, S. Singh, and A. E. Carpenter. Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations. _bioRxiv_, 2023.
* [12] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton. Big self-supervised models are strong semi-supervised learners. _Advances in neural information processing systems_, 33:22243-22255, 2020.

* [13] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In _International Conference on Machine Learning_, pages 7480-7512. PMLR, 2023.
* [14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [15] K. Dunn, A. Aotaki-Keen, F. Putkey, and L. Hjelmeland. Arpe-19, a human retinal pigment epithelial cell line with differentiated properties. _Experimental eye research_, 62(2):155-170, 1996.
* [16] M. M. Fay, O. Kraus, M. Victors, L. Arumugam, K. Vuggumudi, J. Urbanik, K. Hansen, S. Celik, N. Cernek, G. Jagannathan, et al. Rxrx3: Phenomics map of biology. _Biorxiv_, pages 2023-02, 2023.
* [17] C. Feichtenhofer, Y. Li, K. He, et al. Masked autoencoders as spatiotemporal learners. _Advances in neural information processing systems_, 35:35946-35958, 2022.
* [18] A. Furst, E. Rumetshofer, J. Lehner, V. T. Tran, F. Tang, H. Ramsauer, D. Kreil, M. Kopp, G. Klambauer, A. Bitto, et al. Cloob: Modern hopfield networks with infoloob outperform clip. _Advances in neural information processing systems_, 35:20450-20468, 2022.
* [19] J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new approach to self-supervised learning, 2020.
* [20] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, 2022.
* [21] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning, 2020.
* [22] M. Hofmarcher, E. Rumetshofer, D.-A. Clevert, S. Hochreiter, and G. Klambauer. Accurate prediction of biological assays with high-throughput microscopy images and convolutional networks. _Journal of chemical information and modeling_, 59(3):1163-1171, 2019.
* [23] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei. Language is not all you need: Aligning perception with language models, 2023.
* [24] W. E. Johnson, C. Li, and A. Rabinovic. Adjusting batch effects in microarray expression data using empirical bayes methods. _Biostatistics_, 8(1):118-127, 2007.
* [25] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [26] P. Kemmeren, K. Sameith, L. A. Van De Pasch, J. J. Benschop, T. L. Lenstra, T. Margaritis, E. O'Duibhir, E. Apweiler, S. van Wageningen, C. W. Ko, et al. Large-scale genetic perturbations reveal regulatory networks and an abundance of gene-specific repressors. _Cell_, 157(3):740-752, 2014.
* [27] C. Knox, M. Wilson, C. M. Klinger, M. Franklin, E. Oler, A. Wilson, A. Pon, J. Cox, N. E. L. Chin, S. A. Strawbridge, M. Garcia-Patino, R. Kruger, A. Sivakumaran, S. Sanford, R. Doshi, N. Khetarpal, O. Fatokun, D. Doucet, A. Zubkowski, D. Y. Rayat, H. Jackson, K. Harford, A. Anjum, M. Zakir, F. Wang, S. Tian, B. Lee, J. Liigand, H. Peters, R. Q. R. Wang, T. Nguyen, D. So, M. Sharp, R. da Silva, C. Gabriel, J. Scantlebury, M. Jasinski, D. Ackerman, T. Jewison, T. Sajed, V. Gautam, and D. S. Wishart. Drugbank 6.0: the drugbank knowledgebase for 2024. _Nucleic Acids Research_, 52(D1):D1265-D1275, Nov. 2023.

* [28] O. Kraus, K. Kenyon-Dean, S. Saberian, M. Fallah, P. McLean, J. Leung, V. Sharma, A. Khan, J. Balakrishnan, S. Celik, et al. Masked autoencoders for microscopy are scalable learners of cellular biology. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11757-11768, 2024.
* [29] O. Z. Kraus, B. T. Grays, J. Ba, Y. Chong, B. J. Frey, C. Boone, and B. J. Andrews. Automated analysis of high-content microscopy data with deep learning. _Molecular systems biology_, 13(4):924, 2017.
* [30] H. Kuwahara and X. Gao. Analysis of the effects of related fingerprints on molecular similarity using an eigenvalue entropy approach. _Journal of Cheminformatics_, 13:1-12, 2021.
* [31] G. Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. _Greg Landrum_, 8(31.10):5281, 2013.
* [32] F. Lanusse, L. Parker, S. Golkar, M. Cranmer, A. Bietti, M. Eickenberg, G. Krawezik, M. McCabe, R. Ohana, M. Pettee, B. R.-S. Blancard, T. Tesileanu, K. Cho, and S. Ho. Astroclip: Cross-modal pre-training for astronomical foundation models, 2023.
* [33] J. T. Leek, R. B. Scharpf, H. C. Bravo, D. Simcha, B. Langmead, W. E. Johnson, D. Geman, K. Baggerly, and R. A. Irizarry. Tackling the widespread and critical impact of batch effects in high-throughput data. _Nature Reviews Genetics_, 11(10):733-739, 2010.
* [34] D. Masters, J. Dean, K. Klaser, Z. Li, S. Maddrell-Mander, A. Sanders, H. Helal, D. Beker, A. Fitzgibbon, S. Huang, et al. Gps++: Reviving the art of message passing for molecular property prediction. _arXiv preprint arXiv:2302.02947_, 2023.
* [35] D. Mendez, A. Gaulton, A. P. Bento, J. Chambers, M. De Veij, E. Felix, M. P. Magarinos, J. F. Mosquera, P. Mutowo, M. Nowotka, et al. Chembl: towards direct deposition of bioassay data. _Nucleic acids research_, 47(D1):D930-D940, 2019.
* [36] O. Mendez-Lucio, C. Nicolaou, and B. Earnshaw. Mole: a molecular foundation model for drug discovery, 2022.
* [37] C. Q. Nguyen, D. Pertusi, and K. M. Branson. Molecule-morphology contrastive pretraining for transferable molecular representation. _bioRxiv_, pages 2023-05, 2023.
* [38] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [39] H. S. Parker and J. T. Leek. The practical effect of batch on genomic prediction. _Statistical applications in genetics and molecular biology_, 11(3), 2012.
* [40] Z. Pincus and J. Theriot. Comparison of quantitative methods for cell-shape analysis. _Journal of microscopy_, 227(2):140-156, 2007.
* [41] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual information. In _International Conference on Machine Learning_, pages 5171-5180. PMLR, 2019.
* [42] D. L. Purich. _Enzyme kinetics: catalysis and control: a reference of theory and best-practice methods_. Elsevier, 2010.
* [43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [44] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019.
* [45] H. Ramsauer, B. Schafl, J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlovic, G. K. Sandve, et al. Hopfield networks is all you need. _arXiv preprint arXiv:2008.02217_, 2020.

* [46] D. Rogers and M. Hahn. Extended-connectivity fingerprints. _Journal of chemical information and modeling_, 50(5):742-754, 2010.
* [47] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang. Self-supervised graph transformer on large-scale molecular data, 2020.
* [48] A. Sanchez-Fernandez, E. Rumetshofer, S. Hochreiter, and G. Klambauer. Cloome: contrastive learning unlocks bioimaging databases for queries with chemical structures. _Nature_, 2023.
* [49] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* [50] J. Simm, G. Klambauer, A. Arany, M. Steijaert, J. K. Wegner, E. Gustin, V. Chupakhin, Y. T. Chong, J. Vialard, P. Buijnsters, et al. Repurposing high-throughput image assays enables biological activity prediction for drug discovery. _Cell chemical biology_, 25(5):611-618, 2018.
* [51] K. Sohn. Improved deep metric learning with multi-class n-pair loss objective. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS'16, page 1857-1865, Red Hook, NY, USA, 2016. Curran Associates Inc.
* [52] C. Soneson, S. Gerster, and M. Delorenzi. Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation. _PloS one_, 9(6):e100335, 2014.
* [53] R. S. Srinivasa, J. Cho, C. Yang, Y. M. Saidutta, C.-H. Lee, Y. Shen, and H. Jin. Cwcl: Cross-modal transfer with continuously weighted contrastive loss. _Advances in Neural Information Processing Systems_, 36, 2023.
* [54] D. R. Stirling, M. J. Swain-Bowden, A. M. Lucas, A. E. Carpenter, B. A. Cimini, and A. Goodman. Cellprofiler 4: improvements in speed, utility and usability. _BMC bioinformatics_, 22:1-11, 2021.
* [55] M. Sypetkowski, M. Rezanejad, S. Saberian, O. Kraus, J. Urbanik, J. Taylor, B. Mabey, M. Victors, J. Yosinski, A. R. Sereshkeh, et al. Rxx1: A dataset for evaluating experimental batch correction methods. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4284-4293, 2023.
* [56] M. Sypetkowski, F. Wenkel,, F. Poursafaei, N. Dickson, K. Suri, P. Fradkin, and D. Beaini. On the scalability of foundational models for molecular graphs. _arxiv_, 2024.
* [57] F. Vincent, A. Nueda, J. Lee, M. Schenone, M. Prunotto, and M. Mercola. Phenotypic drug discovery: recent successes, lessons learned and new directions. _Nature Reviews Drug Discovery_, 21(12):899-914, 2022.
* [58] R. M. Walmsley and N. Billinton. How accurate is in vitro prediction of carcinogenicity? _British Journal of Pharmacology_, 162(6):1250-1258, Feb. 2011.
* [59] C. Wang, S. Gupta, C. Uhler, and T. Jaakkola. Removing biases from molecular representations via information maximization, 2023.
* [60] R. Xie, K. Pang, G. D. Bader, and B. Wang. Maester: Masked autoencoder guided segmentation at pixel resolution for accurate, self-supervised subcellular structure recognition. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, June 2023.
* [61] S. Zaidi, M. Schaarschmidt, J. Martens, H. Kim, Y. W. Teh, A. Sanchez-Gonzalez, P. Battaglia, R. Pascanu, and J. Godwin. Pre-training via denoising for molecular property prediction, 2022.
* [62] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11975-11986, 2023.
* [63] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. Lit: Zero-shot transfer with locked-image text tuning, 2022.

* [64] S. Zheng, J. Rao, J. Zhang, L. Zhou, J. Xie, E. Cohen, W. Lu, C. Li, and Y. Yang. Cross-modal graph contrastive learning with cellular images. _Advanced Science_, 11(32), June 2024.
* [65] Y. Zhong, H. Tang, J. Chen, J. Peng, and Y.-X. Wang. Is self-supervised learning more robust than supervised learning? _arXiv preprint arXiv:2206.05259_, 2022.
* [66] G. Zhou, Z. Gao, Q. Ding, H. Zheng, H. Xu, Z. Wei, L. Zhang, and G. Ke. Uni-mol: A universal 3d molecular representation learning framework. In _The Eleventh International Conference on Learning Representations_, 2023.

## 7 NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract, we claim that we build a multi-modal molecular-phenomics model and demonstrate improvements over prior works. This is done by taking using a uni-modal pre-trained phenomics model, tackling inactive molecules by undersampling and learning inter-sample similarities. In addition, we take into account concentration in our model training. We demonstrate comprehensive results supporting these claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the conclusion, we have a limitations subsection discussing future research directions and assumptions in our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Our work does not contain proofs or theorems. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our work documents our design decisions in detail and has comprehensive details about the underlying dataset. We document all our hyperparameter choices and model architectural decisions. Our evaluation is performed on a publicly accessible dataset RXRX3, allowing for benchmarking of other methods. To reproduce the pre-trained phenomics model, we base our architecture on the work from [28], for which they have also provided access to a snakker model, namely Phenom-Beta via a web platform hosted on the BioNeMo platform https://www.rxrx.ai/phenom. To reproduce the pre-trained molecular model, we based our architecture on [56], for which the authors provide all the code and data needed to reproduce it. We further note that the molecular model can be replaced by simple molecular fingerprints with only a slight drop in performance. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: As part of the submission, we are unable to provide code to reproduce model training due to use of its proprietary nature. The training dataset is also an asset of a private institution, meaning that we are unable to be made publicly accessible. The unseen dataset RXRX3 is, however, open source and can be used by the community to evaluate public phenomics models. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide details regarding our hyperparameter choices in the Appendix C. In addition we document the use of scaffold splitting for Unseen Molecules & Images dataset. Unseen Dataset RXRX3 is publicly accessible. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our reported results are averaged over 3 random seeds used to initialize the model and dictating stochasticity during model training. We report most standard deviations in the main text, and the remaining ones are all present in the Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on compute time for each experiment in Appendix D.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research described does not violate the NeurIPS Code of Ethics. Our experiments do not include human subjects, we follow fair use of data, privacy, and do not release model weights for mitigating impact measures. Guidelines:* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our work discusses the potential in which MolPhenix can have positive societal impact and we touch on the externalities in our concluding statements. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: In our work we do not release model weights or the underlying code. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Assetts used are referenced and licenses checked or otherwise not released publicly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing not human subject research. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing not human subject research. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

Glossary

**Cell morphology:** The examination and characterization of cellular structure, including shape, size, and organization, which can provide insights into cellular function and health [40].

**Cell line (ARPE-19):** A specific immortalized human retinal pigment epithelial cell line, widely utilized in ophthalmic research due to its differentiated properties and stability [15].

**Molecular concentration:** The quantitative measure of a specific molecule's abundance within a defined volume, typically expressed in molar units [42].

**Cell staining:** A laboratory technique involving the application of dyes or fluorescent markers to enhance the visibility and differentiation of cellular components under microscopic examination [9].

**Molecular perturbations:** Induced alterations in cellular molecular systems, often used to study cellular responses and regulatory networks [26].

**Inactive molecule perturbations:** Cellular system alterations caused by molecules lacking significant biological activity.

**Batch effects:** Systematic non-biological variation between groups of samples in an experiment, resulting from technical or experimental factors rather than true biological differences. This phenomenon is commonly observed in high-throughput molecular biology experiments, such as microarray studies, mass spectrometry, and single-cell RNA sequencing [24].

**Molecular fingerprints:** Distinctive patterns of molecular features that characterize specific cellular states or responses, often used for comparative analyses and classification [8].

## Appendix B Assumption of the Initial Cell State

There is an important distinction between genomics - molecule and text - image contrastive training although there are initial similarities. In the text - image domain the two modalities are directly generated by the same latent variable which is the underlying semantic class. Whereas in phenomics - molecule, the observed phenomics variable is actually conditioned on molecular structure and the initial state. There are two important conclusions from this: (1) This indicates that if molecular structure has no effect on the initial cell state, there will not be a positive pairing between the molecular structure and morphological patterns captured by phenomics, making it indistinguishable from a control image. (2) There is an underlying assumption that the initial cell state \(x_{i}^{0}\) is constant. In accordance with this assumption we utilize experiments with a fixed cell line, _HUVEC-19_, and a constant genetic background. Future works can relax this assumption by taking into account phenomics experiments of the cells prior to the perturbation. This can allow the models to generalize beyond a single cell line and to diverse genetic backgrounds.

## Appendix C Dataset

Models have been trained using our in house training set and we have conducted our evaluation on two novel datasets and an open-source molecule dataset [16]:

* Training Set: Our training dataset comprises 1,316,283 pairs of molecules and concentration concentration combinations, complemented by fluorescent microscopy images generated through over 2,150,000 phenomic experiments.
* Unseen Images + Seen Molecules: The first set consists of unseen images and seen molecules. Unseen microscopy images are associated with 15,058 pairs of molecules and concentrations from the training set and selected randomly.
* Unseen Images + Unseen Molecules: The second set includes previously unseen molecules, and images (consisting of 45,771 molecule and concentration pairs). Predicting molecular identities of previously unseen molecular perturbations corresponds to zero-shot prediction. Scaffold splitting was used to split this validation dataset from training ensuring minimal information leakage.
* Unseen Dataset: Finally, we utilize the RXRX3 dataset [16], an open-source out of distribution (OOD) dataset consisting of 6,549 novel molecule and concentration
pairs associated with phenomic experiments. The distribution of molecular structures differs from previous datasets, making this a challenging zero-shot prediction task.

### Concentration Details

Additional details regarding the number of molecules at significant concentrations of each evaluation set are available in Table 6.

## Appendix D Implementation Details

In our experiments we report the top 1% recall metric as it is agnostic to the size of the dataset used. Across different datasets, top 1 metric can correspond to varying levels of difficulty due to the number of negatives evaluated. Top 1% can be used to compare models with different batch sizes, datasets, and evaluations with different number of concentrations.

### Hyperparameters

Our design choices and utilized hyperparameters for is presented in Table 7. We set batch size to 512 through experiments presented in top section of Table 1 and Figure 4 since training CLOOME model on images is not efficient compared to using pretrained models. In addition, results presented at bottom section of Table 1 are based on the best parameters found through described ablation studies (section E.5).

### Resource Computation

We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately \(\sim\)4.75 hours each. For loss comparison experiments, we run each model using 3 different seeds and 8 different losses, resulting in a total of 114 hours of GPU processing time. For concentration experiments we train 7 runs, one for each concentration, with 3 seeds each

totaling 21 runs per set of parameters. With 25 sets of parameters evaluated (13), that amounts to 2,500 A100 compute hours. Moreover, we employed 8 NVIDIA A100 GPUs to train CLOOME model on phenomics images, with an average of 40 hour usage per run. Across three seeds, that amounts to \(\sim\) 1000 hours of A100 GPU usage (8 GPUs for 40 hours 3 times).

Note that, without accounting for the time to train Phenom1, MolPhenix is 8.4 \(\times\) faster than the CLOOME baseline.

### S2L Distance function

To calculate inter sample distances, we utilize arctangent of l2 distances between Phenom1 embeddings. More specifically, we calculate distances with

\[\arctan(\|z_{\mathbf{x}_{i}}-z_{\mathbf{x}_{j}}\|_{2}^{2}/c)*\frac{4}{\pi}-1,\] (5)

where \(c\) is a constant indicating the median l2 distance between a null set of embeddings. Empirically, we've found that setting similarities below a threshold \(k\) to 0 improves model performance: \(\lceil w\rceil^{k}\).

Usage of arctan-l2 distances is motivated by an observation that cosine similarities do not effectively separate inactive molecules from other molecular pairs (Figure 6). To alleviate inactive molecule challenge, we require significant separation of CDF curves of inactive perturbations (p value >.9) and active molecules (p <.01). We observe that in both the plots using arctangent and cosine similarities achieves this purpose. However, if we compare high p-value curves with high-low, we find that in the case of cosine similarities they are almost identical. This indicates that the distribution of cosine similarities between active - inactive molecules is almost identical to that of inactive - inactive molecules. In contrast, when using arctangent similarities, we observe that the two CDF curves are well separated.

Figure 6: Plotted are cumulative densities of distance metrics for cosine similarity and arctangent of l2 distances between embeddings. Random mol corresponds to Phenom1 distances between random molecules, high pval corresponds to distances between molecules with high p-values, low pval corresponds to distances between active molecules with low p-values, finally high-low corresponds to distances between active and inactive molecules.

This property of l2 distances can inform our model training to identify inactive-inactive molecules. These results informed our decision to utilize arctangent of l2 distances to specify sample similarities for the S2L loss.

### Intuition for S2L Loss

In this section we aim to provide some additional intuition for the S2L loss and further relate it to previous works. We will first assess the conceptual similarities between InfoNCE and CWCL loss and then justify a similar extrapolation for the relationship between S2L and SigLIP losses.

InfoNCE can be considered a special case of the CWCL loss where \(w_{ij}\) is set to 0 for all pairs of \(i\) and \(j\) unless \(i=j\). Conceptually this is equivalent to stating that all the negative pairs are equally distant from the reference sample. We will consider a uni directional loss CWCL, for identifying \(\mathcal{X}\) from \(\mathcal{M}\):

\[\mathcal{L}_{\text{CWCL},:\mathcal{M}\rightarrow\mathcal{X}}=-\frac{1}{N}\sum _{i=1}^{N}\left[\frac{1}{\sum_{j=1}^{N}\mathbf{w}_{i,j}^{\mathcal{X}}}\sum_{j =1}^{N}\mathbf{w}_{i,j}^{\mathcal{X}}\log\frac{\exp\left(\langle\mathbf{z}_{x_ {i}},\mathbf{z}_{m_{j}}\rangle/\tau\right)}{\sum_{k=1}^{N}\exp\left(\langle \mathbf{z}_{x_{j}},\mathbf{z}_{m_{k}}\rangle/\tau\right)}\right].\]

If we set \(w_{ij}=0\) when \(i\neq j\) and \(1\) otherwise then the term \(\Sigma_{j=1}^{N}w_{i,j}^{\mathcal{X}}\) evaluates to 1 and the above expression simplifies to:

\[\mathcal{L}_{\text{InfoNCE}}=-\frac{1}{N}\sum_{i=1}^{N}\left[\log\frac{\exp( \langle\mathbf{z}_{x_{i}},\mathbf{z}_{m_{i}}\rangle/\tau)}{\sum_{k=1}^{N}\exp( \langle\mathbf{z}_{x_{i}},\mathbf{z}_{m_{k}}\rangle/\tau)}\right].\]

In the case of CWCL, a non 0 \(w_{i,j}\) determined by a within modality similarity function informed by a pre-trained model, allows for an additional inductive bias. It is especially beneficial in a training setting with a limited dataset-size and in the presence of inactive negative molecules.

Similarly SigLip can be considered a special case of S2L when \(w_{i,j}^{\mathcal{X}}=0\) when \(i\neq j\) and \(w_{i,j}^{\mathcal{X}}=1\) in the case \(i=j\). This is the formulation of S2L

\[\mathcal{L}_{\text{S2L}}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N}\log\left[ \frac{\mathbf{w}_{i,j}^{\mathcal{X}}}{1+\exp\left(-\alpha\langle\mathbf{z}_{ \mathbf{x}_{i}}\mathbf{z}_{m_{j}}\rangle+b\right)}+\frac{(1-\mathbf{w}_{i,j}^ {\mathcal{X}})}{1+\exp\left(\alpha\langle\mathbf{z}_{\mathbf{x}_{i}}\mathbf{ z}_{m_{j}}\rangle+b\right)}\right].\]

It can be simplified to SigLip by setting \(w_{i,j}^{\mathcal{X}}\) to \(1\) when \(y_{i,j}=1\) thus setting the term \(\frac{(1-\mathbf{w}_{i,j}^{\mathcal{X}})}{1+\exp\left(\alpha\mathbf{z}_{ \mathbf{x}_{i}}\cdot\mathbf{z}_{m_{j}}+b\right)}\), corresponding to \(i\neq j\), we set \(w_{i,j}^{\mathcal{X}}\) to 0 thus negating the first part of the \(\mathcal{L}_{S2L}\) loss, evaluating to:

\[\mathcal{L}_{\text{SigLIP}}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N}\left[\log \frac{1}{1+\exp\left(\mathbf{y}_{i,j}(-\alpha\langle\mathbf{z}_{\mathbf{x}_{i} },\mathbf{z}_{m_{j}}\rangle+b)\right)}\right].\]

Having a \(0\leq w_{i,j}^{\mathcal{X}}\leq 1\) allows us to inform the training by going between discrete negative labels to continuous informed by some prior information. This information is given by a pre-trained encoder \(\theta_{Phi}\), in our case but can be informed by any pre-trained model.

There are a lot of domain specific choices that can be made to inform the choice for setting \(w_{i,j}^{\mathcal{X}}\) which we discuss in the appendix. Briefly, we identify that a modified \(l2\) loss is most effective for identifying inactive molecules.

## Appendix E Additional Results

### Predicting molecular activity

Given the significance of identifying active molecules, we evaluate the ability of the chemical encoder to predict molecular activity. To do so, we assessed whether embeddings generated from the chemical encoder can be used to predict calculated p-values for unseen molecules. We fit a logistic regression on molecular embeddings from the training set, classifying whether a molecular perturbation and concentration have a p-value below.01. We find that the trained logistic regression is capable of predicting molecular activity on two downstream datasets with a non-overlapping set of molecules, Figure 8. In addition, we provide a u-map of molecular embedding for the unseen dataset RXRX3, colored by p-value. We qualitatively observe a clustering of active molecules using a U-map (Figure 7). It demonstrates that predicting compounds activity is possible using MolPhenix chemical encoder as molecules representations are distinct, independent of the experimented dosage concentration.

Figure 7: U-map demonstrating dimensionality reduction of the chemical embeddings of unseen dataset RXRX3. First two dimensions are visualized and points are colored corresponding to their activity measured in phenomics experiments. Activity is evaluated using p-values calculated using technical replicability of Phenom1 embeddings. Top plot shows the u-map figure of all chemical embeddings, and bottom figure contains u-map figure of representations at specific concentrations.

### Zero Shot Biological Validation

We conduct a preliminary investigation into whether MolPhenix can be used to identify biological relationships without the need for conducting the underlying experiments. To this end, we evaluate on a subset of ChEMBL with curated pairs of gene knockouts and molecular perturbants [35]. These pairs of perturbations were curated due to the similarity of their effects on cells, although these might not always be captured through phonomic experiments. Thus, there is maximum performance that can be reached through just phonomic data, which we assume to be achieved by experimental data embedded using Phenom1.

To evaluate MolPhenix's ability to identify previously known biological associations directly from data, we embed phenomics experiments from gene knockouts using the vision encoder. To perform in-silico screening, we then embed the molecular structures associated with positive pairs using the chemical encoder. Generating molecular embeddings and the corresponding concentrations does not utilize any experimental data. We then calculate cosine similarities between embeddings of phenomics experiments evaluating gene knockouts, and representations of the chemical representations along with encoded concentrations. Using the computed cosine similarities we are then able to assess whether MolPhenix is capable of identifying known associations between gene knockouts and molecular structures. Since there is no information on molecular concentration at which the cells must be treated with, we repeat the experiment across 4 concentrations. To get a null distribution of cosine similarities we take pairs of genes knockouts and molecules for which there are no annotated relationships. We calculate a cut-off for a low and high percentiles, and then evaluate what percentage of pairs of genes and molecules with known relationships exceed the set thresholds.

Figure 9 demonstrates that in-silico screening using MolPhenix Molecular encoder is capable of recovering a significant portion of known interactions. This is performed without the use of experimental data on the molecular encoder. It is difficult to estimate an upper bound on the expected performance due to uncertainty in the quality of curation of known pairs, presence of unknown associations between genes and molecules, and uncertainty regarding molecular concentration. There is a clear trend however that MolPhenix molecular encoder is capable of recovering a meaningful fraction of these interactions.

Figure 8: **Top left:** ROC AUC of logistic regression predicting molecular activity on new dataset. **Top right:** ROC AUC of logistic regression predicting molecular activity on validation dataset with new molecules and new images.

[MISSING_PAGE_FAIL:30]

### Addressing Challenges in Contrastive Phenomic Retrieval

Table 10 and 12 show the complete Top 1% and 5% results of evaluation on cumulative concentrations on only active and all molecules, respectively. Similarly, Table 11 and 13 presents the full retrieval results of held-out concentrations experiments. In comparison to prior loss functions, S2L loss objective demonstrates consistent high retrieval rate in all tasks and molecular groups (i.e. all or active molecules), while using the same modality (Phenom1) and with or without explicit concentration information.

### Ablation Studies

Figure 10 and Table 15, 16, 17, 18 and 19 present top-1% recall accuracy across for the full ablation study on the variation of MolPhenix key components. We note that compact embedding sizes from pretrained models stabilize training. This indicates that embeddings are expressive and accurately capture intricate aspects of molecules. Larger batch sizes result in a greater number of negative samples, hence improving performance. This is in line with prior contrastive learning methods continuing to improve by increasing the batch size [12]. Increasing the number of parameters leads to more expressive models thereby enhancing retrieval performance. This result is in accordance with

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}{c} Enplest \\ Concentration (ours) \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Methods \\ **Uncertainties** \\ \end{tabular} } & \multicolumn{4}{c}{**top-15**} & \multicolumn{4}{c}{**top-16**} \\ \cline{3-10}  & & & & & & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Cliff \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} Enplest \\ **F** \\ \end{tabular} } & \multirow{2}{*

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Loss} & \multirow{2}{*}{\begin{tabular}{c} Explicit \\ Concentration \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} Mobility \\ \end{tabular} } & \multicolumn{2}{c}{Usern Images} & \multicolumn{2}{c}{Unseen Images} & \multicolumn{2}{c}{Unseen Images} & \multicolumn{2}{c}{Unseen Dataset} & \multicolumn{2}{c}{Avg.} & \multicolumn{2}{c}{Usern Images} & \multicolumn{2}{c}{Unseen Images} & \multicolumn{2}{c}{Unseen Images} & \multicolumn{2}{c}{Avg.} \\  & & & Unseen Molecules & (0-shot) & & & Unseen Molecules & (0-shot) & & Unseen Molecules & (0-shot) \\ \hline \hline CLIP & âœ— & Precons1 &.1531 &.1709 &.6673 &.3104 &.3667 &.3746 &.2068 &.3170 \\ \hline Inf.LOOB & âœ— & Precons1 &.1568 &.2005 &.0911 &.1628 &.3938 &.3888 &.2231 &.383 \\ CLODME & sigmoid & Precons1 &.3875 &.2922 &.1415 &.2627 &.5662 &.4601 &.2940 &.461 \\ CLODME & length & Precons1 & **.4088** &.3046 & **.3503** & **.2579** &.5730 &.5166 &.3058 &.450 \\ CLODME & one-shot & Precons1 &.680 &.3123 &.1496 &.2000 & **.5801** & **.5306** & **.4720** \\ \hline DCL & âœ— &.3277 &.2562 &.1364 &.2001 &.4485 &.4170 &.2768 &.3931 \\ DCL & sigmoid & Precons1 &.4881 &.3880 &.2009 &.3422 &.6222 &.5116 &.3381 &.4930 \\ DCL & length & Precons1 &.3683 &.3615 &.2122 &.3873 &.6311 &.5581 &.3587 &.5160 \\ DCL & one-shot & Precons1 & **.3226** & **.3790** & **.2288** & **.3768** & **.6791** & **.3570** & **.3968** & **.5843** \\ \hline CVCL & âœ— & Precons1 &.3653 &.2696 &.1586 &.2619 &.5122 &.4226 &.2933 &.4107 \\ CVCL & sigmoid & Precons1 &.5070 &.3457 &.2010 &.3422 &.6178 &.5272 &.3642 &.5037 \\ CVCL & length & Precons1 &.5146 &.3725 &.2246 &.3706 &.6437 &.5733 &.3660 &.5277 \\ CVCL & one-shot & Precons1 & **.5041** & **.3880** & **.2336** & **.3628** & **.6881** & **.6981** & **.4001** & **.5625** \\ \hline Sai-Up & âœ— & Precons1 &.3725 &.2544 &.1470 &.2581 &.5200 &.4179 &.2638 &.4072 \\ Sai-Up & sigmoid & Precons1 &.3521 &.3275 &.2002 &.3456 &.6300 &.3211 &.3430 &.3007 \\ Sai-Up & sigmoid & Precons1 &.5156 &.3636 &.2233 &.3675 &.6452 &.5689 &.3643 &.5365 \\ Sai-Up & one-shot & Precons1 & **.5534** & **.3745** & **.3748** & **.3805** & **.6888** & **.3792** & **.3945** & **.5577** \\ \hline Sai-Up & âœ— & Precons1 &.4688 &.2823 &.1838 &.3126 &.5970 &.4519 &.3171 &.4554 \\ Sai-Up & sigmoid & Precons1 &.3641 &.3700 &.2275 &.3720 &.6422 &.5315 &.3554 &.5099 \\ Sai-Up & sigmoid & Precons1 &.3533 &.3700 &.2275 &.3720 &.6492 &.5650 &.3756 &.5300 \\ Sai-Up & one-shot & Precons1 & **.3843** & **.3819** & **.2384** & **.3879** & **.4054** & **.5895** & **.4030** & **.5636** \\ \hline Sai-Up & âœ— & Precons1 &.4688 &.2729 &.2001 &.3139 &.5956 &.4374 &.3430 &.4587 \\ Sai-Up & sigmoid & Precons1 &.4683 &.3230 &.2397 &.3533 &.6433 &.5015 &.3790 &.5056 \\ Sai-Up & sigmoid & Precons1 &.5101 &.3589 &.2535 &.3742 &.6938 &.5660 &.3992 &.5350 \\ Sai-Up & one-shot & Precons1 & **.5370** & **.3720** & **.2462** & **.5022** & **.6870** & **.5888** & **.4336** & **.5895** \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Evaluation on cumulative concentrations for active and inactive perturbations** Average Top-1% and Top-5% Recall accuracy of methods utilizing different contrastive learning methods. Best performing methods are highlighted in **bold**.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Loss} & \multirow{2}{*}{\begin{tabular}{c} Explicit \\ Concentration \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Mobility \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} User Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Dataset \\ (0-shot) \\ \end{tabular} } & \multirow{2}{*}{Avg.} & \multirow{2}{*}{Unseen Images} & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Images \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Unseen Dataset \\ (0-shot) \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} Avg.} \\ \\ \end{tabular} \\ \hline CLIP & âœ— & Precons1 &.2109 &.3425 &.4519 &.2018 & **.4458** & **.4088** & **.3091** & **.4539** \\ \hline Inf.LOOB & âœ— & Precons1 &.4881 & **.2054** & **.3178** & **.1604** & **.3783** & **.4413** & **.3068** & **.3740** \\ \hline Inf.LOOB & âœ— & Precons1 &.2122 &.2068 &.4591 &.2008 & **.4443** & **.5088** & **.3055** & **.4309** \\ \hline C.LOOOOO & âœ— & Precons1 &.2164 &.2461 &.1479 &.2005 &.4590 &.4856 &.3562 &.4588 \\ C.LOOOOO & sigmoid & Precons1 &.3968 &.2048 &.3615 &.2068 &.5991 &.4512 &.3563 &.4598 \\ C.LOOOOOO & sigmoid & Precons1 &.3975 &.3948 &.1607 &.2068 &.5997 &.4346 &.3675 &.4633 \\ \hline DCLrecent advances in language modelling and scaling laws across different data and compute budgets [25].

### Investigating Other Pre-trained Phenolic Encoders

To investigate the impact of pre-trained encoders, we perform additional experiments evaluating a supervised phenomenic image encoder (Table 20). Instead of Phenom1, we trained Molphenix framework using AdaBN, a CNN-based supervised phenomenic encoder, with an analogous implementation discussed in [55]. We find that the general trends between Phenom1 and AdaBN are consistent with a slight decrease in overall performance. These findings provide additional support to the generality of the proposed guidelines.

### Integrating MolGPS Embeddings With Other Fingerprints

Molphenix architecture is flexible, allowing that the proposed components be replaced by other phenomic or molecular pretrained models. We leveraged from MolGPS, which is a MPNN based

GNN model with 1B parameters which allows us to maximize architecture expressivity while minimizing the risk of overfitting [34, 56]. For additional investigation, we combine MolGPS molecular embeddings with RDKIT, MACCS, and Morgan fingerprints and show that they can provide Molphenix with richer molecular information and yields overall higher performance of MolPhenix in both cumulative and held-out concentration scenarios. Results for active and all molecules retrieval of Molphenix trained on the discussed combinational molecular embeddings are available in table 21 and 22.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Dim size & Unseen images & Unseen images + & Unseen dataset \\  & & Unseen molecules & (0-shot) \\ \hline
256 &.9452 &.7510 &.4929 \\
512 &.9430 &.7490 &.4850 \\
1024 &.9392 &.7288 &.4710 \\ \hline \end{tabular}
\end{table}
Table 16: Ablation across different embedding dimensions. Compact embedding sizes capture more molecular information.

Figure 10: Ablations of top-1 % recall accuracy with **(top-left)** the size of embedding dimension, **(top-center)** number of parameters, **(top-right)** batch size, (bottom-left)** cutoff \(p\) value, **(bottom-center)** fingerprint type, and **(bottom-right)** random batch averaging. Compact embedding sizes from pretrained models, larger number of parameters, larger batch sizes, lower cutoff p-values, pretrained MolGPS fingerprints and presence of random batch averagin improving retrieval of our MolPhenix framework.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Epistic} & \multirow{2}{*}{Mobility} & \multirow{2}{*}{Unseen Images} & \multirow{2}{*}{Unseen Images +} & Unseen Dataset & Avg. \\  & & & Unseen Molecules & (0-shot) & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & Unseen Molecules & (0-shot) & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \hline \end{tabular}
\end{table}
Table 22: Evaluation on **holdout concentrations** while combining MolGPS, RDKIT, MACCS, and **Morgan fingerprints**.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Explicit} & \multirow{2}{*}{Mobility} & \multirow{2}{*}{Unseen Images} & Unseen Images + \\  & & Unseen molecules & (0-shot) \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & Unseen Molecules & (0-shot) \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & \\ \cline{3-5}  & & & & \\ \hline \end{tabular}
\end{table}
Table 17: Ablation across different p-value cutoff thresholds. p values <.1 benefit retrieval of active molecules.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Explicit} & \multirow{2}{*}{Mobility} & \multirow{2}{*}{Unseen Images} & Unseen Images + \\  & & Unseen molecules & (0-shot) & & Unseen Images + \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & Unseen Molecules & (0-shot) & & Unseen Molecules & (0-shot) \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & & \\ \cline{3-5}  & & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & & \\ \cline{3-5}  & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & & \\ \hline \multirow{3}{*}{MatPhenix} & - & \multirow{3}{*}{Freenent \& MoIKPS \& 3.9 & 5155 \\  & & & & & & \\ \cline{3-5}  & & & & & & \\ \cline{3-5}  & & & & & & \\ \cline{3-5}  & & & & & \\ \cline{3-5}  & & & & & & \\ \cline{3-5}  & & & & & \\ \hline \end{tabular}
\end{table}
Table 18: Ablation across different fingerprint types. A combination of embeddings bootstrapped from Phenom1 and MolGPS significantly benefit retrieval.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Explicit} & \multirow{2}{*}{Mobility} & \multirow{2}{*}