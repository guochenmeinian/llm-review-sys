# Restart Sampling for Improving Generative Processes

 Yilun Xu

MIT

ylxu@mit.edu

&Mingyang Deng

MIT

dengm@mit.edu

&Xiang Cheng1

MIT

chengx@mit.edu

&Yonglong Tian

Google Research

yonglong@google.com

&Ziming Liu

MIT

zmliu@mit.edu

&Tommi Jaakkola

MIT

tommi@csail.mit.edu

Equal Contribution.

###### Abstract

Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called _Restart_ in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet \(64\times 64\). In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION \(512\times 512\). Code is available at https://github.com/Newbeeer/diffusion_restart_sampling

## 1 Introduction

Deep generative models based on differential equations, such as diffusion models and Poission flow generative models, have emerged as powerful tools for modeling high-dimensional data, from image synthesis [23, 9, 13, 27, 28] to biological data [10, 26]. These models use iterative backward processes that gradually transform a simple distribution (_e_.\(g\)., Gaussian in diffusion models) into a complex data distribution by solving a differential equations. The associated vector fields (or drifts) driving the evolution of the differential equations are predicted by neural networks. The resulting sample quality can be often improved by enhanced simulation techniques but at the cost of longer sampling times.

Prior samplers for simulating these backward processes can be categorized into two groups: ODE-samplers whose evolution beyond the initial randomization is deterministic, and SDE-samplers where the generation trajectories are stochastic. Several works [23, 12, 13] show that these samplers demonstrate their advantages in different regimes, as depicted in Fig. 1(b). ODE solvers [22, 16, 13] result in smaller discretization errors, allowing for decent sample quality even with larger step sizes (_i_.\(e\)., fewer number of function evaluations (NFE)). However, their generation quality plateaus rapidly. In contrast, SDE achieves better quality in the large NFE regime, albeit at the expense of increased sampling time. To better understand these differences, we theoretically analyze SDE performance: thestochasticity in SDE contracts accumulated error, which consists of both the discretization error along the trajectories as well as the approximation error of the learned neural network relative to the ground truth drift (_e.g._, score function in diffusion model [23]). The approximation error dominates when NFE is large (small discretization steps), explaining the SDE advantage in this regime. Intuitively, the stochastic nature of SDE helps "forget" accumulated errors from previous time steps.

Inspired by these findings, we propose a novel sampling algorithm called _Restart_, which combines the advantages of ODE and SDE. As illustrated in Fig. 1, the Restart sampling algorithm involves \(K\) repetitions of two subroutines in a pre-defined time interval: a _Restart forward process_ that adds a substantial amount of noise, akin to "restarting" the original backward process, and a _Restart backward process_ that runs the backward ODE. The Restart algorithm separates the stochasticity from the drifts, and the amount of added noise in the Restart forward process is significantly larger than the small single-step noise interleaving with drifts in previous SDEs such as [23, 13], thus amplifying the contraction effect on accumulated errors. By repeating the forward-backward cycle \(K\) times, the contraction effect introduced in each Restart iteration is further strengthened. The deterministic backward processes allow Restart to reduce discretization errors, thereby enabling step sizes comparable to ODE. To maximize the contraction effects in practice, we typically position the Restart interval towards the end of the simulation, where the accumulated error is larger. Additionally, we apply multiple Restart intervals to further reduce the initial errors in more challenging tasks.

Experimentally, Restart consistently surpasses previous ODE and SDE solvers in both quality and speed over a range of NFEs, datasets, and pre-trained models. Specifically, Restart accelerates the previous best-performing SDEs by \(10\times\) fewer steps for the same FID score on CIFAR-10 using VP [23] (\(2\times\) fewer steps on ImageNet \(64\times 64\) with EDM [13]), and outperforms ODE solvers even in the small NFE regime. When integrated into previous state-of-the-art pre-trained models, Restart further improves performance, achieving FID scores of 1.88 on unconditional CIFAR-10 with PFGM++ [28], and 1.36 on class-conditional ImageNet \(64\times 64\) with EDM. To the best of our knowledge, these are the best FID scores obtained on commonly used UNet architectures for diffusion models without additional training. We also apply Restart to the practical application of text-to-image Stable Diffusion model [19] pre-trained on LAION \(512\times 512\). Restart more effectively balances text-image alignment/visual quality (measured by CLIP/Aesthetic scores) and diversity (measured by FID score) with a varying classifier-free guidance strength, compared to previous samplers.

Our contributions can be summarized as follows: **(1)** We investigate ODE and SDE solvers and theoretically demonstrate the contraction effect of stochasticity via an upper bound on the Wasserstein distance between generated and data distributions (Sec 3); **(2)** We introduce the Restart sampling, which better harnesses the contraction effect of stochasticity while allowing for fast sampling. The sampler results in a smaller Wasserstein upper bound (Sec 4); **(3)** Our experiments are consistent with the theoretical bounds and highlight Restart's superior performance compared to previous samplers on standard benchmarks in terms of both quality and speed. Additionally, Restart improves the trade-off between key metrics on the Stable Diffusion model (Sec 5).

Figure 1: **(a) Illustration of the implementation of drift and noise terms in ODE, SDE, and Restart. **(b)** Sample quality versus number of function evaluations (NFE) for different approaches. ODE (Green) provides fast speeds but attains only mediocre quality, even with a large NFE. SDE (Yellow) obtains good sample quality but necessitates substantial sampling time. In contrast to ODE and SDE, which have their own winning regions, Restart (Red) achieves the best quality across all NFEs.

## 2 Background on Generative Models with Differential Equations

Many recent successful generative models have their origin in physical processes, including diffusion models [9; 23; 13] and Poisson flow generative models [27; 28]. These models involve a forward process that transforms the data distribution into a chosen smooth distribution, and a backward process that iteratively reverses the forward process. For instance, in diffusion models, the forward process is the diffusion process with no learned parameters:

\[\mathrm{d}x=\sqrt{2\hat{\sigma}(t)\sigma(t)}\mathrm{d}W_{t},\]

where \(\sigma(t)\) is a predefined noise schedule increasing with \(t\), and \(W_{t}\in\mathbb{R}^{d}\) is the standard Wiener process. For simplicity, we omit an additional scaling function for other variants of diffusion models as in EDM [13]. Under this notation, the marginal distribution at time \(t\) is the convolution of data distribution \(p_{0}=p_{\text{data}}\) and a Gaussian kernel, _i.e._, \(p_{t}=p_{0}*\mathcal{N}(\mathbf{0},\sigma^{2}(t)\bm{I}_{d\times d})\). The prior distribution is set to \(\mathcal{N}(\mathbf{0},\sigma^{2}(T)\bm{I}_{d\times d})\) since \(p_{T}\) is approximately Gaussian with a sufficiently large \(T\). Sampling of diffusion models is done via a reverse-time SDE [1] or a marginally-equivalent ODE [23]:

\[\text{(SDE)}\] (1) \[\text{(ODE)}\] (2)

where \(\nabla_{x}\log p_{t}(x)\) in the drift term is the score of intermediate distribution at time \(t\). W.l.o.g we set \(\sigma(t)=t\) in the remaining text, as in [13]. Both processes progressively recover \(p_{0}\) from the prior distribution \(p_{T}\) while sharing the same time-dependent distribution \(p_{t}\). In practice, we train a neural network \(s_{\theta}(x,t)\) to estimate the score field \(\nabla_{x}\log p_{t}(x)\) by minimizing the denoising score-matching loss [25]. We then substitute the score \(\nabla_{x}\log p_{t}(x)\) with \(s_{\theta}(x,t)\) in the drift term of above backward SDE (Eq. (1))/ODE (Eq. (2)) for sampling.

Recent work inspired by electrostatics has not only challenged but also integrated diffusion models, notably PFGM/PFGM++, enhances performance in both image and antibody generation [27; 28; 10]. They interpret data as electric charges in an augmented space, and the generative processes involve the simulations of differential equations defined by electric field lines. Similar to diffusion models, PFGMs train a neural network to approximate the electric field in the augmented space.

## 3 Explaining SDE and ODE performance regimes

To sample from the aforementioned generative models, a prevalent approach employs general-purpose numerical solvers to simulate the corresponding differential equations. This includes Euler and Heun's 2nd method [2] for ODEs (e.g., Eq. (2)), and Euler-Maruyama for SDEs (e.g., Eq. (1)). Sampling algorithms typically balance two critical metrics: (1) the quality and diversity of generated samples, often assessed via the Frechet Inception Distance (FID) between generated distribution and data distribution [7] (lower is better), and (2) the sampling time, measured by the number of function evaluations (NFE). Generally, as the NFE decreases, the FID score tends to deteriorate across all samplers. This is attributed to the increased discretization error caused by using a larger step size in numerical solvers.

However, as illustrated in Fig. 1(b) and observed in previous works on diffusion models [23; 22; 13], the typical pattern of the quality vs time curves behaves differently between the two groups of samplers, ODE and SDE. When employing standard numerical solvers, ODE samplers attain a decent quality with limited NFEs, whereas SDE samplers struggle in the same small NFE regime. However, the performance of ODE samplers quickly reaches a plateau and fails to improve with an increase in NFE, whereas SDE samplers can achieve noticeably better sample quality in the high NFE regime. This dilemma raises an intriguing question: _Why do ODE samplers outperform SDE samplers in the small NFE regime, yet fall short in the large NFE regime?_

The first part of the question is relatively straightforward to address: given the same order of numerical solvers, simulation of ODE has significantly smaller discretization error compared to the SDE. For example, the first-order Euler method for ODE results in a local error of \(O(\delta^{2})\), whereas the first-order Euler-Maruyama method for SDEs yeilds a local error of \(O(\delta^{\frac{3}{2}})\) (see _e.g._, Theorem 1 of [4]), where \(\delta\) denotes the step size. As \(O(\delta^{\frac{3}{2}})\gg O(\delta^{2})\), ODE simulations exhibit lower sampling errors than SDEs, likely causing the better sample quality with larger step sizes in the small NFE regime.

In the large NFE regime the step size \(\delta\) shrinks and discretization errors become less significant for both ODEs and SDEs. In this regime it is the _approximation error_ -- error arising from aninaccurate estimation of the ground-truth vector field by the neural network \(s_{\theta}\) -- starts to dominate the sampling error. We denote the discretized ODE and SDE using the learned field \(s_{\theta}\) as ODE\({}_{\theta}\) and SDE\({}_{\theta}\), respectively. In the following theorem, we evaluate the total errors from simulating ODE\({}_{\theta}\) and SDE\({}_{\theta}\) within the time interval \([t_{\text{min}},t_{\text{max}}]\subset[0,T]\). This is done via an upper bound on the Wasserstein-1 distance between the generated and data distributions at time \(t_{\text{min}}\). We characterize the accumulated initial sampling errors up until \(t_{\text{max}}\) by total variation distances. Below we show that the inherent stochasticity of SDEs aids in contracting these initial errors at the cost of larger additional sampling error in \([t_{\text{min}},t_{\text{max}}]\). Consequently, SDE results in a smaller upper bound as the step size \(\delta\) nears \(0\) (pertaining to the high NFE regime).

**Theorem 1** (Informal).: _Let \(t_{\text{max}}\) be the initial noise level and \(p_{t}\) denote the true distribution at noise level \(t\). Let \(p_{t}^{\text{ODE}_{\theta}},p_{t}^{\text{SDE}_{\theta}}\) denote the distributions of simulating ODE\({}_{\theta}\), SDE\({}_{\theta}\) respectively. Assume that \(\forall t\in[t_{\text{min}},t_{\text{max}}]\), \(\|x_{t}\|<B/2\) for any \(x_{t}\) in the support of \(p_{t}\), \(p_{t}^{\text{ODE}_{\theta}}\) or \(p_{t}^{\text{SDE}_{\theta}}\). Then_

\[W_{1}(p_{t_{\text{max}}}^{\text{ODE}_{\theta}},p_{t_{\text{max}}}) \leq B\cdot TV\left(p_{t_{\text{max}}}^{\text{ODE}_{\theta}},p_{t_{ \text{max}}}\right)+O(\delta+\epsilon_{approx})\cdot\left(t_{\text{max}}-t_{ \text{min}}\right)\] \[\underbrace{W_{1}(p_{t_{\text{max}}}^{\text{SDE}_{\theta}},p_{t_ {\text{min}}})}_{\text{total error}} \leq\underbrace{\left(1-\lambda e^{-U}\right)B\cdot TV(p_{t_{\text{max}}}^{ \text{SDE}_{\theta}},p_{t_{\text{max}}})}_{\text{upper bound on estimated error}}+ \underbrace{O(\sqrt{\delta t_{\text{max}}}+\epsilon_{approx})\left(t_{\text{ max}}-t_{\text{min}}\right)}_{\text{upper bound on estimated sampling error}}\]

_In the above, \(U=BL_{1}/t_{\text{min}}+L_{1}^{2}t_{\text{max}}^{2}/t_{\text{min}}^{2}\), \(\lambda<1\) is a contraction factor, \(L_{1}\) and \(\epsilon_{approx}\) are uniform bounds on \(\|t_{\text{S}\theta}(x_{t},t)\|\) and the approximation error \(\|t\nabla_{x}\log p_{t}(x)-ts_{\theta}(x,t)\|\) for all \(x_{t},t\), respectively. \(O()\) hides polynomial dependency on various Lipschitz constants and dimension._

We defer the formal version and proof of Theorem 1 to Appendix A.1. As shown in the theorem, the upper bound on the total error can be decomposed into upper bounds on the _contracted error_ and _additional sampling error_. \(TV(p_{t_{\text{max}}}^{\text{ODE}_{\theta}},p_{t_{\text{max}}})\) and \(TV(p_{t_{\text{max}}}^{\text{SDE}_{\theta}},p_{t_{\text{max}}})\) correspond to the initial errors accumulated from both approximation and discretization errors during the simulation of the backward process, up until time \(t_{\text{max}}\). In the context of SDE, this accumulated error undergoes contraction by a factor of \(1-\lambda e^{-BL_{1}/t_{\text{min}}-L_{1}^{2}t_{\text{max}}^{2}/t_{\text{min}} ^{2}}\) within \([t_{\text{min}},t_{\text{max}}]\), due to the effect of adding noise. Essentially, the minor additive Gaussian noise in each step can drive the generated distribution and the true distribution towards each other, thereby neutralizing a portion of the initial accumulated error.

The other term related to additional sampling error includes the accumulation of discretization and approximation errors in \([t_{\text{min}},t_{\text{max}}]\). Despite the fact that SDE incurs a higher discretization error than ODE (\(O(\sqrt{\delta})\) versus \(O(\delta)\)), the contraction effect on the initial error is the dominant factor impacting the upper bound in the large NFE regime where \(\delta\) is small. Consequently, the upper bound for SDE is significantly lower. This provides insight into why SDE outperforms ODE in the large NFE regime, where the influence of discretization errors diminishes and the contraction effect dominates. In light of the distinct advantages of SDE and ODE, it is natural to ask whether we can combine their strengths. Specifically, can we devise a sampling algorithm that maintains a comparable level of discretization error as ODE, while also benefiting from, or even amplifying, the contraction effects induced by the stochasticity of SDE? In the next section, we introduce a novel algorithm, termed _Restart_, designed to achieve these two goals simultaneously.

## 4 Harnessing stochasticity with Restart

In this section, we present the Restart sampling algorithm, which incorporates stochasticity during sampling while enabling fast generation. We introduce the algorithm in Sec 4.1, followed by a theoretical analysis in Sec 4.2. Our analysis shows that Restart achieves a better Wasserstein upper bound compared to those of SDE and ODE in Theorem 1 due to greater contraction effects.

### Method

In the Restart algorithm, simulation performs a few repeated back-and-forth steps within a pre-defined time interval \([t_{\text{min}},t_{\text{max}}]\subset[0,T]\), as depicted in Figure 1(a). This interval is embedded into the simulation of the original backward ODE referred to as the _main backward process_, which runs from \(T\) to \(0\). In addition, we refer to the backward process within the Restart interval \([t_{\text{min}},t_{\text{max}}]\) as the _Restart backward process_, to distinguish it from the main backward process.

Starting with samples at time \(t_{\text{min}}\), which are generated by following the main backward process, the Restart algorithm adds a large noise to transit the samples from \(t_{\text{min}}\) to \(t_{\text{max}}\) with the help of the forward process. The forward process does not require any evaluation of the neural network \(s_{\theta}(x,t)\), as it is generally defined by an analytical perturbation kernel capable of transporting distributions from \(t_{\text{min}}\) to \(t_{\text{max}}\). For instance, in the case of diffusion models, the perturbation kernel is \(\mathcal{N}(\mathbf{0},(\sigma(t_{\text{max}})^{2}-\sigma(t_{\text{min}})^{2} )\bm{I}_{d\times d})\). The added noise in this step induces a more significant contraction compared to the small, interleaved noise in SDE. The step acts as if partially restarting the main backward process by increasing the time. Following this step, Restart simulates the backward ODE from \(t_{\text{max}}\) back to \(t_{\text{min}}\) using the neural network predictions as in regular ODE. We repeat these forward-backward steps within \([t_{\text{min}},t_{\text{max}}]\) interval \(K\) times in order to further derive the benefit from contraction. Specifically, the forward and backward processes in the \(i^{\text{th}}\) iteration (\(i\in\{0,\dots,K-1\}\)) proceed as follows:

\[(\text{Restart forward process}) x_{t_{\text{max}}}^{i+1}=x_{t_{\text{min}}}^{i}+ \varepsilon_{t_{\text{max}}\to t_{\text{max}}}\] (3) \[(\text{Restart backward process}) x_{t_{\text{min}}}^{i+1}=\text{ODE}_{\theta}(x_{t_{\text{max}}}^{i+1},t_{ \text{max}}\to t_{\text{min}})\] (4)

where the initial \(x_{t_{\text{min}}}^{0}\) is obtained by simulating the ODE until \(t_{\text{min}}\): \(x_{t_{\text{min}}}^{0}=\text{ODE}_{\theta}(x_{T},T\to t_{\text{min}})\), and the noise \(\varepsilon_{t_{\text{min}}\to t_{\text{max}}}\) is sampled from the corresponding perturbation kernel from \(t_{\text{min}}\) to \(t_{\text{max}}\). The Restart algorithm not only adds substantial noise in the Restart forward process (Eq. (3)), but also separates the stochasticity from the ODE, leading to a greater contraction effect, which we will demonstrate theoretically in the next subsection. For example, we set \([t_{\text{min}},t_{\text{max}}]=[0.05,0.3]\) for the VP model [13] on CIFAR-10. Repetitive use of the forward noise effectively mitigates errors accumulated from the preceding simulation up until \(t_{\text{max}}\). Furthermore, the Restart algorithm does not suffer from large discretization errors as it is mainly built from following the ODE in the Restart backward process (Eq. (4)). The effect is that the Restart algorithm is able to reduce the total sampling errors even in the small NFE regime. Detailed pseudocode for the Restart sampling process can be found in Algorithm 2, Appendix B.2.

### Analysis

We provide a theoretical analysis of the Restart algorithm under the same setting as Theorem 1. In particular, we prove the following theorem, which shows that Restart achieves a much smaller contracted error in the Wasserstein upper bound than SDE (Theorem 1), thanks to the separation of the noise from the drift, as well as the large added noise in the Restart forward process (Eq. (3)). The repetition of the Restart cycle \(K\) times further leads to a enhanced reduction in the initial accumulated error. We denote the intermediate distribution in the \(i^{\text{th}}\) Restart iteration, following the discretized trajectories and the learned field \(s_{\theta}\), as \(p_{t\in[t_{\text{min}},t_{\text{max}}]}^{\text{Resut}_{\theta}(i)}\).

**Theorem 2** (Informal).: _Under the same setting of Theorem 1, assume \(K\leq\frac{C}{L_{2}(t_{\text{max}}-t_{\text{max}})}\) for some universal constant \(C\). Then_

\[\underbrace{W_{1}(p_{t_{\text{max}}}^{\text{Resut}_{\theta}(K)},p_{t_{\text{ min}}})}_{\text{total error}}\leq\underbrace{B\cdot(1-\lambda)^{K}\,TV(p_{t_{\text{max}}}^{\text{Resut}_{ \theta}(0)},p_{t_{\text{max}}})}_{\text{upper bound on contracted error}}+ \underbrace{(K+1)\cdot O\left(\delta+\epsilon_{approx}\right)(t_{\text{max}}- t_{\text{min}})}_{\text{upper bound on additional sampling error}}\]

_where \(\lambda<1\) is the same contraction factor as Theorem 1. \(O()\) hides polynomial dependency on various Lipschitz constants, dimension._

Proof sketch.: To bound the total error, we introduce an auxiliary process \(q_{t\in[t_{\text{min}},t_{\text{max}}]}^{\text{Restut}_{\theta}(i)}\), which initiates from true distribution \(p_{t_{\text{max}}}\) and performs the Restart iterations. This process differs from \(p_{t\in[t_{\text{min}},t_{\text{max}}]}^{\text{Resut}_{\theta}(i)}\) only in its initial distribution at \(t_{\text{max}}\) (\(p_{t_{\text{max}}}\) versus \(p_{t_{\text{max}}}^{\text{Resut}_{\theta}(0)}\)). We bound the total error by the following triangular inequality:

\[\underbrace{W_{1}(p_{t_{\text{min}}}^{\text{Restut}_{\theta}(K)},p_{t_{\text{ min}}})}_{\text{total error}}\leq\underbrace{W_{1}(p_{t_{\text{min}}}^{\text{Restut}_{\theta}(K)},q_{t_{ \text{min}}}^{\text{Restut}_{\theta}(K)})}_{\text{contracted error}}+\underbrace{W_{ 1}(q_{t_{\text{min}}}^{\text{Restut}_{\theta}(K)},p_{t_{\text{min}}})}_{\text{ additional sampling error}}\]

To bound the contracted error, we construct a careful coupling process between two individual trajectories sampled from \(p_{t_{\text{min}}}^{\text{Restut}_{\theta}(i)}\) and \(q_{t_{\text{min}}}^{\text{Restut}_{\theta}(i)},i=0,\dots,K-1\). Before these two trajectories converge, the Gaussian noise added in each Restart iteration is chosen to maximize the probability of the two trajectories mapping to an identical point, thereby maximizing the mixing rate in TV. After converging, the two processes evolve under the same Gaussian noise, and will stay converged as their drifts are the same. Lastly, we convert the TV bound to \(W_{1}\) bound by multiplying \(B\). The bound on the additional sampling error echoes the ODE analysis in Theorem 1: since the noise-injection and ODE-simulation stages are separate, we do not incur the higher discretization error of SDE.

We defer the formal version and proof of Theorem 2 to Appendix A.1. The first term in RHS bounds the contraction on the initial error at time \(t_{\text{max}}\) and the second term reflects the additional sampling error of ODE accumulated across repeated Restart iterations. Comparing the Wasserstein upper bound of SDE and ODE in Theorem 1, we make the following three observations: _(1)_ Each Restart iteration has a smaller contraction factor \(1-\lambda\) compared to the one in SDE, since Restart separates the large additive noise (Eq. (3)) from the ODE (Eq. (4)). _(2)_ Restart backward process (Eq. (4)) has the same order of discretization error \(O(\delta)\) as the ODE, compared to \(O(\sqrt{\delta})\) in SDE. Hence, the Restart allows for small NFE due to ODE-level discretization error. _(3)_ The contracted error further diminishes exponentially with the number of repetitions \(K\) though the additional error increases linearly with \(K\). It suggests that there is a sweet spot of \(K\) that strikes a balance between reducing the initial error and increasing additional sampling error. Ideally, one should pick a larger \(K\) when the initial error at time \(t_{\text{max}}\) greatly outweigh the incurred error in the repetitive backward process from \(t_{\text{max}}\) to \(t_{\text{min}}\). We provide empirical evidences in Sec 5.2.

While Theorem 1 and Theorem 2 compare the upper bounds on errors of different methods, we provide empirical validation in Section 5.1 by directly calculating these errors, showing that the Restart algorithm indeed yields a smaller total error due to its superior contraction effects. The main goal of Theorem 1 and Theorem 2 is to study how the already accumulated error changes using different samples, and to understand their ability to self-correct the error by stochasticity. In essence, these theorems differentiate samplers based on their performance post-error accumulation. For example, by tracking the change of accumulated error, Theorem 1 shed light on the distinct "winning regions" of ODE and SDE: ODE samplers have smaller discretization error and hence excel at the small NFE regime. In contrast, SDE performs better in large NFE regime where the discretization error is negligible and its capacity to contract accumulated errors comes to the fore.

### Practical considerations

The Restart algorithm offers several degrees of freedom, including the time interval \([t_{\text{min}},t_{\text{max}}]\) and the number of restart iterations \(K\). Here we provide a general recipe of parameter selection for practitioners, taking into account factors such as the complexity of the generative modeling tasks and the capacity of the network. Additionally, we discuss a stratified, multi-level Restart approach that further aids in reducing simulation errors along the whole trajectories for more challenging tasks.

**Where to Restart?** Theorem 2 shows that the Restart algorithm effectively reduces the accumulated error at time \(t_{\text{max}}\) by a contraction factor in the Wasserstein upper bound. These theoretical findings inspire us to position the Restart interval \([t_{\text{min}},t_{\text{max}}]\) towards the end of the main backward process, where the accumulated error is more substantial. In addition, our empirical observations suggest that a larger time interval \(t_{\text{max}}-t_{\text{min}}\) is more beneficial for weaker/smaller architectures or more challenging datasets. Even though a larger time interval increases the additional sampling error, the benefits of the contraction significantly outweighs the downside, consistent with our theoretical predictions. We leave the development of principled approaches for optimal time interval selection for future works.

**Multi-level Restart** For challenging tasks that yield significant approximation errors, the backward trajectories may diverge substantially from the ground truth even at early stage. To prevent the ODE simulation from quickly deviating from the true trajectory, we propose implementing multiple Restart intervals in the backward process, alongside the interval placed towards the end. Empirically, we observe that a \(1\)-level Restart is sufficient for CIFAR-10, while for more challenging datasets such as ImageNet [5], a multi-level Restart results in enhanced performance [5].

## 5 Experiments

In Sec 5.1, we first empirically verify the theoretical analysis relating to the Wasserstein upper bounds. We then evaluate the performance of different sampling algorithms on standard image generation benchmarks, including CIFAR-10 [14] and ImageNet \(64\times 64\)[5] in Sec 5.2. Lastly, we employ Restart on text-to-image generation, using Stable Diffusion model [19] pre-trained on LAION-5B [21] with resolution \(512\times 512\), in Sec 5.3.

### Additional sampling error versus contracted error

Our proposed Restart sampling algorithm demonstrates a higher contraction effect and smaller addition sampling error compared to SDE, according to Theorem 1 and Theorem 2. Although our theoretical analysis compares the upper bounds of the total, contracted and additional sampling errors, we further verify their relative values through a synthetic experiment.

**Setup** We construct a \(20\)-dimensional dataset with 2000 points sampled from a Gaussian mixture, and train a four-layer MLP to approximate the score field \(\nabla_{x}\log p_{t}\). We implement the ODE, SDE, and Restart methods within a predefined time range of \([t_{\text{min}},t_{\text{max}}]=[1.0,1.5]\), where the process outside this range is conducted via the first-order ODE. To compute various error types, we define the distributions generated by three methods as outlined in the proof of Theorem 2 and directly gauge the errors at end of simulation \(t=0\) instead of \(t=t_{\text{min}}\): (1) the generated distribution as \(p_{0}^{\text{Sampler}}\), where \(\text{Sampler}\in\{\text{ODE}_{\theta},\text{SEB}_{\theta},\text{Restart}_{ \theta}(K)\}\); (2) an auxiliary distribution \(q_{0}^{\text{Sampler}}\) initiating from true distribution \(p_{t_{\text{max}}}\) at time \(t_{\text{max}}\). The only difference between \(p_{0}^{\text{Sampler}}\) and \(q_{0}^{\text{Sampler}}\) is their initial distribution at \(t_{\text{max}}\) (\(p_{t_{\text{max}}}^{\text{ODE}_{\theta}}\) versus \(p_{t_{\text{max}}}\)); and (3) the true data distribution \(p_{0}\). In line with Theorem 2, we use Wasserstein-1 distance \(W_{1}(p_{0}^{\text{Sampler}},q_{0}^{\text{Sampler}})\) / \(W_{1}(q_{0}^{\text{Sampler}},p_{0})\) to measure the contracted error / additional sampling error, respectively. Ultimately, the total error corresponds to \(W_{1}(p_{0}^{\text{Sampler}},p_{0})\). Detailed information about dataset, metric and model can be found in the Appendix C.5.

**Results** In our experiment, we adjust the parameters for all three processes and calculate the total, contracted, and additional sampling errors across all parameter settings. Figure 2(a) depicts the Pareto frontier of additional sampling error versus contracted error. We can see that Restart consistently achieves lower contracted error for a given level of additional sampling error, compared to both the ODE and SDE methods, as predicted by theory. In Figure 2(b), we observe that the Restart method obtains a smaller total error within the additional sampling error range of \([0.8,0.85]\). During this range, Restart also displays a strictly reduced contracted error, as illustrated in Figure 2(a). This aligns with our theoretical analysis, suggesting that the Restart method offers a smaller total error due to its enhanced contraction effects. From Figure 2(c), Restart also strikes an better balance between efficiency and quality, as it achieves a lower total error at a given NFE.

### Experiments on standard benchmarks

To evaluate the sample quality and inference speed, we report the FID score [7] (lower is better) on 50K samplers and the number of function evaluations (NFE). We borrow the pretra

Figure 3: FID versus NFE on **(a)** unconditional generation on CIFAR-10 with VP; **(b)** class-conditional generation on ImageNet with EDM.

Figure 2: Additional sampling error versus **(a)** contracted error, where the Pareto frontier is plotted and **(b)** total error, where the scatter plot is provided. **(c)** Pareto frontier of NFE versus total error.

[MISSING_PAGE_FAIL:8]

lower than any previous numbers (even lower than the SDE sampler with an NFE greater than 1000 in [23]), and make VP model on par with the performance with more advanced models (such as EDM).

Theorem 4 shows that each Restart iteration reduces the contracted errors while increasing the additional sampling errors in the backward process. In Fig. 5, we explore the choice of the number of Restart iterations \(K\) on CIFAR-10. We find that FID score initially improves and later worsens with increasing iterations \(K\), with a smaller turning point for stronger EDM model. This supports the theoretical analysis that sampling errors will eventually outweigh the contraction benefits as \(K\) increases, and EDM only permits fewer Restart iterations due to smaller accumulated errors. It also suggests that, as a rule of thumb, we should apply greater Restart strength (_e.g_., larger \(K\)) for weaker or smaller architectures and vice versa.

### Experiments on large-scale text-to-image model

We further apply Restart to the text-to-image Stable Diffusion v1.5 2 pre-trained on LAION-5B [21] at a resolution of \(512\times 512\). We employ the commonly used classifier-free guidance [8, 20] for sampling, wherein each sampling step entails two function evaluations - the conditional and unconditional predictions. Following [18, 20], we use the COCO [15] validation set for evaluation. We assess text-image alignment using the CLIP score [6] with the open-sourced ViT-g/14 [11], and measure diversity via the FID score. We also evaluate visual quality through the Aesthetic score, as rated by the LAION-Aesthetics Predictor V2 [24]. Following [17], we compute all evaluation metrics using 5K captions randomly sampled from the validation set and plot the trade-off curves between CLIP/Aesthetic scores and FID score, with the classifier-free guidance weight \(w\) in \(\{2,3,5,8\}\).

Footnote 2: https://huggingface.co/runwayml/stable-diffusion-v1-5

We compare with commonly used ODE sampler DDIM [22] and the stochastic sampler DDPM [9]. For Restart, we adopt the DDIM solver with 30 steps in the main backward process, and Heun in the Restart backward process, as we empirically find that Heun performs better than DDIM in the Restart. In addition, we select different sets of the hyperparameters for each guidance weight. For instance, when \(w=8\), we use \([t_{\text{min}},t_{\text{max}}]{=}[0.1,2],K{=}2\) and \(10\) steps in Restart backward process. We defer the detailed Restart configuration to Appendix C.2, and the results of Heun to Appendix D.1.

As illustrated in Fig. 6(a) and Fig. 6(b), Restart achieves better FID scores in most cases, given the same CLIP/Aesthetic scores, using only 132 function evaluations (_i.e_., 66 sampling steps). Remarkably, Restart achieves substantially lower FID scores than other samplers when CLIP/Aesthetic scores are high (_i.e_., with larger \(w\) values). Conversely, Restart generally obtains a better text-image alignment/visual quality given the same FID. We also observe that DDPM generally obtains comparable performance with Restart in FID score when CLIP/Aesthetic scores are low, with Restart being more time-efficient. These findings suggest that Restart balances diversity (FID score) against text-image alignment (CLIP score) or visual quality (Aesthetic score) more effectively than previous samplers.

In Fig. 7, we visualize the images generated by Restart, DDIM and DDPM with \(w=8\). Compared to DDIM, the Restart generates images with superior details (_e.g_., the rendition of duck legs by DDIM is less accurate) and visual quality. Compared to DDPM, Restart yields more photo-realistic images (_e.g_., the astronaut). We provide extended of text-to-image generated samples in Appendix E.

Figure 6: FID score versus **(a)** CLIP ViT-g/14 score and **(b)** Aesthetic score for text-to-image generation at \(512\times 512\) resolution, using Stable Diffusion v1.5 with a varying classifier-free guidance weight \(w=2,3,5,8\).

## 6 Conclusion and future direction

In this paper, we introduce the Restart sampling for generative processes involving differential equations, such as diffusion models and PFGMs. By interweaving a forward process that adds a significant amount of noise with a corresponding backward ODE, Restart harnesses and even enhances the individual advantages of both ODE and SDE. Theoretically, Restart provides greater contraction effects of stochasticity while maintaining ODE-level discretization error. Empirically, Restart achieves a superior balance between quality and time, and improves the text-image alignment/visual quality and diversity trade-off in the text-to-image Stable Diffusion models.

A current limitation of the Restart algorithm is the absence of a principled way for hyperparameters selection, including the number of iterations \(K\) and the time interval \([t_{\text{min}},t_{\text{max}}]\). At present, we adjust these parameters based on the heuristic that weaker/smaller models, or more challenging tasks, necessitate a stronger Restart strength. In the future direction, we anticipate developing a more principled approach to automating the selection of optimal hyperparameters for Restart based on the error analysis of models, in order to fully unleash the potential of the Restart framework.

## Acknowledgements

YX and TJ acknowledge support from MIT-DSTA Singapore collaboration, from NSF Expeditions grant (award 1918839) "Understanding the World Through Code", and from MIT-IBM Grand Challenge project. Xiang Cheng acknowledges support from NSF CCF-2112665 (TILOS AI Research Institute).

Figure 7: Visualization of generated images with classifier-free guidance weight \(w=8\), using four text prompts (“A photo of an astronaut riding a horse on mars.”, ”A raccoon playing table tennis”, ”Intricate origami of a fox in a snowy forest” and ”A transparent sculpture of a duck made out of glass”) and the **same** random seeds.

## References

* [1] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [2] Uri M. Ascher and Linda R. Petzold. Computer methods for ordinary differential equations and differential-algebraic equations. In _SIAM_, 1998.
* [3] Andrei N Borodin and Paavo Salminen. _Handbook of Brownian motion-facts and formulae_. Springer Science & Business Media, 2015.
* [4] Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. _Stochastic Processes and their Applications_, 129(12):5278-5311, 2019.
* [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* [6] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Joseph Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In _Conference on Empirical Methods in Natural Language Processing_, 2021.
* [7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _NIPS_, 2017.
* [8] Jonathan Ho. Classifier-free diffusion guidance. _ArXiv_, abs/2207.12598, 2022.
* [9] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. _ArXiv_, abs/2006.11239, 2020.
* [10] Chutian Huang, Zijing Liu, Shengyuan Bai, Linwei Zhang, Chencheng Xu, Zhe Wang, Yang Xiang, and Yuanpeng Xiong. Pf-abgen: A reliable and efficient antibody generator via poisson flow. _Machine Learning for Drug Discovery Workshop, International Conference on Learning Representations_, 2023.
* [11] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. _Zenodo_, 2021.
* [12] Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. _ArXiv_, abs/2105.14080, 2021.
* [13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _ArXiv_, abs/2206.00364, 2022.
* [14] Alex Krizhevsky. Learning multiple layers of features from tiny images. _Citeseer_, 2009.
* [15] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision_, 2014.
* [16] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.
* [17] Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. _ArXiv_, abs/2210.03142, 2022.
* [18] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, 2021.

* [19] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10674-10685, 2021.
* [20] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _ArXiv_, abs/2205.11487, 2022.
* [21] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. _ArXiv_, abs/2210.08402, 2022.
* [22] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _ArXiv_, abs/2010.02502, 2020.
* [23] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _ArXiv_, abs/2011.13456, 2020.
* [24] LAION-AI Team. Laion-aesthetics predictor v2. https://github.com/christophschuhmann/improved-aesthetic-predictor, 2022.
* [25] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23:1661-1674, 2011.
* [26] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vazquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Regina Barzilay, T. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models. _bioRxiv_, 2022.
* [27] Yilun Xu, Ziming Liu, Max Tegmark, and T. Jaakkola. Poisson flow generative models. _ArXiv_, abs/2209.11178, 2022.
* [28] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, and T. Jaakkola. Pfgm++: Unlocking the potential of physics-inspired generative models. _ArXiv_, abs/2302.04265, 2023.

## Appendix A Proofs of Main Theoretical Results

In this section, we provide proofs of our main results. We define below some crucial notations which we will use throughout. We use ODE\((\dots)\) to denote the backwards ODE under exact score \(\nabla\log p_{t}(x)\). More specifically, given any \(x\in\mathbb{R}^{d}\) and \(s>r>0\), let \(x_{t}\) denote the solution to the following ODE:

\[dx_{t}=-t\nabla\log p_{t}(x_{t})dt.\] (5)

ODE\((x,s\to r)\) is defined as "the value of \(x_{r}\) when initialized at \(x_{s}=x\)". It will also be useful to consider a "time-discretized ODE with drift \(ts_{\theta}(x,t)\)": let \(\delta\) denote the discretization step size and let \(k\) denote any integer. Let \(\delta\) denote a step size, let \(\overline{x}_{t}\) denote the solution to

\[d\overline{x}_{t}=-ts_{\theta}(x_{k\delta},k\delta)dt,\] (6)

where for any \(t\), \(k\) is the unique integer such that \(t\in((k-1)\delta,k\delta]\). We verify that the dynamics of Eq. (6) is equivalent to the following discrete-time dynamics for \(t=k\delta,k\in\mathbb{Z}\):

\[\overline{x}_{(k-1)\delta}=\overline{x}_{k\delta}-\frac{1}{2}\left(\left((k- 1)\delta\right)^{2}-(k\delta)^{2}\right)s_{\theta}(x_{k\delta},k\delta).\]

We similarly denote the value of \(\overline{x}_{r}\) when initialized at \(\overline{x}_{s}=x\) as ODE\({}_{\theta}(x,s\to r)\). Analogously, we let SDE\((x,s\to r)\) and SDE\({}_{\theta}(x,s\to r)\) denote solutions to

\[dy_{t} =-2t\nabla\log p_{t}(y_{t})dt+\sqrt{2}tdB_{t}\] \[d\overline{y}_{t} =-2ts_{\theta}(\overline{y}_{t},t)dt+\sqrt{2}tdB_{t}\]

respectively. Finally, we will define the Restart\({}_{\theta}\) process as follows:

\[(\text{Restart}_{\theta}\text{ forward process}) x_{t_{\text{max}}}^{i+1}=x_{t_{\text{min}}}^{i}+\varepsilon_{t_{\text{max}} \to t_{\text{max}}}^{i}\] \[(\text{Restart}_{\theta}\text{ backward process}) x_{t_{\text{min}}}^{i+1}=\text{ODE}_{\theta}(x_{t_{\text{max}}}^{i+1},t_{\text{max}}\to t_{ \text{min}}),\] (7)

where \(\varepsilon_{t_{\text{min}}\to t_{\text{max}}}^{i}\sim\mathcal{N}\left( \mathbf{0},\left(t_{\text{max}}^{2}-t_{\text{min}}^{2}\right)\boldsymbol{I}\right)\). We use Restart\({}_{\theta}(x,K)\) to denote \(x_{t_{\text{min}}}^{K}\) in the above processes, initialized at \(x_{t_{\text{min}}}^{0}=x\). In various theorems, we will refer to a function \(Q(r):\mathbb{R}^{+}\to[0,1/2)\), defined as the Gaussian tail probability \(Q(r)=Pr(a\geq r)\) for \(a\sim\mathcal{N}(0,1)\).

### Main Result

**Theorem 3**.: _[Formal version of Theorem 1] Let \(t_{\text{max}}\) be the initial noise level. Let the initial random variables \(\overline{x}_{t_{\text{max}}}=\overline{y}_{t_{\text{max}}}\), and_

\[\overline{x}_{t_{\text{min}}} =\text{ODE}_{\theta}(\overline{x}_{t_{\text{max}}},t_{\text{max} }\to t_{\text{min}})\] \[\overline{y}_{t_{\text{min}}} =\text{SDE}_{\theta}(\overline{y}_{t_{\text{max}}},t_{\text{max} }\to t_{\text{min}}),\]

_Let \(p_{t}\) denote the true population distribution at noise level \(t\). Let \(p_{t}^{\text{ODE}_{\theta}}\), \(p_{t}^{\text{SDE}_{\theta}}\) denote the distributions for \(x_{t},y_{t}\) respectively. Assume that for all \(x,y,s,t\), \(s_{\theta}(x,t)\) satisfies \(\left\|t_{\theta}(x,t)-ts_{\theta}(x,s)\right\|\leq L_{0}|s-t|\), \(\left\|t_{\theta}(x,t)\right\|\leq L_{1}\), \(\left\|ts_{\theta}(x,t)-ts_{\theta}(y,t)\right\|\leq L_{2}\left\|x-y\right\|\), and the approximation error \(\left\|ts_{\theta}(x,t)-t\nabla\log p_{t}(x)\right\|\leq\epsilon_{approx}\). Assume in addition that \(\forall t\in[t_{\text{min}},t_{\text{max}}]\), \(\left\|x_{t}\right\|<B/2\) for any \(x_{t}\) in the support of \(p_{t}\), \(p_{t}^{\text{ODE}_{\theta}}\) or \(p_{t}^{\text{SDE}_{\theta}}\), and \(K\leq\frac{C}{L_{2}(t_{\text{min}}-t_{\text{min}})}\) for some universal constant \(C\). Then_

\[W_{1}(p_{t_{\text{min}}}^{\text{ODE}_{\theta}},p_{t_{\text{min} }}) \leq B\cdot TV\left(p_{t_{\text{max}}}^{\text{ODE}_{\theta}},p_{t_{ \text{max}}}\right)\] \[+e^{L_{2}(t_{\text{max}}-t_{\text{min}})}\cdot\left(\delta(L_{2} L_{1}+L_{0})+\epsilon_{approx}\right)(t_{\text{max}}-t_{\text{min}})\] (8) \[W_{1}(p_{t_{\text{min}}}^{\text{SDE}_{\theta}},p_{t_{\text{min} }}) \leq B\cdot\left(1-\lambda e^{-BL_{1}/t_{\text{min}}-L_{1}^{2}t_{ \text{max}}^{2}/t_{\text{min}}^{2}}\right)TV(p_{t_{\text{max}}}^{\text{SDE}_{ \theta}},p_{t_{\text{max}}})\] \[+e^{2L_{2}(t_{\text{max}}-t_{\text{max}})}\left(\epsilon_{approx}+ \delta L_{0}+L_{2}\left(\delta L_{1}+\sqrt{2\delta dt_{\text{max}}}\right) \right)(t_{\text{max}}-t_{\text{min}})\] (9)

_where \(\lambda:=2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}}\right)\)._Proof.: Let us define \(x_{t_{\text{max}}}\sim p_{t_{\text{max}}}\), and let \(x_{t_{\text{min}}}=\text{ODE}(x_{t_{\text{max}}},t_{\text{max}}\to t_{\text{min}})\). We verify that \(x_{t_{\text{min}}}\) has density \(p_{t_{\text{min}}}\). Let us also define \(\hat{x}_{t_{\text{min}}}=\text{ODE}_{\theta}(x_{t_{\text{max}}},t_{\text{max}} \to t_{\text{min}})\). We would like to bound the Wasserstein distance between \(\bar{x}_{t_{\text{min}}}\) and \(x_{t_{\text{min}}}\) (_i.e._, \(p_{t_{\text{min}}}^{\text{ODE}_{\theta}}\) and \(p_{t_{\text{min}}}\)), by the following triangular inequality:

\[W_{1}(\bar{x}_{t_{\text{min}}},x_{t_{\text{min}}})\leq W_{1}(\bar{x}_{t_{\text {min}}},\hat{x}_{t_{\text{min}}})+W_{1}(\hat{x}_{t_{\text{min}}},x_{t_{\text{ min}}})\] (10)

By Lemma 2, we know that

\[\|\hat{x}_{t_{\text{min}}}-x_{t_{\text{min}}}\|\leq e^{(t_{\text{max}}-t_{\text{min}})L_{2}}\left( \delta(L_{2}L_{1}+L_{0})+\epsilon_{approx}\right)\left(t_{\text{max}}-t_{\text {min}}\right),\]

where we use the fact that \(\|\hat{x}_{t_{\text{max}}}-x_{t_{\text{max}}}\|=0\). Thus we immediately have

\[W_{1}(\hat{x}_{t_{\text{min}}},x_{t_{\text{min}}})\leq e^{(t_{\text{max}}-t_{\text{min}})L_{2}}\left(\delta(L_{2}L_{1}+L_{0})+ \epsilon_{approx}\right)\left(t_{\text{max}}-t_{\text{min}}\right)\] (11)

On the other hand,

\[W_{1}(\hat{x}_{t_{\text{min}}},\overline{x}_{t_{\text{min}}})\leq B\cdot TV\left(\hat{x}_{t_{\text{min}}},\overline{x}_{t_{\text{min}}}\right)\] \[\leq B\cdot TV\left(\hat{x}_{t_{\text{max}}},\overline{x}_{t_{\text{ min}}}\right)\] (12)

where the last equality is due to the data-processing inequality. Combining Eq. (11), Eq. (12) and the triangular inequality Eq. (10), we arrive at the upper bound for ODE (Eq. (8)). The upper bound for SDE (Eq. (9)) shares a similar proof approach. First, let \(y_{t_{\text{max}}}\sim p_{t_{\text{max}}}\). Let \(\hat{y}_{t_{\text{min}}}=\text{SDE}_{\theta}(y_{t_{\text{max}}},t_{\text{max} }\to t_{\text{min}})\). By Lemma 5,

\[TV\left(\hat{y}_{t_{\text{min}}},\overline{y}_{t_{\text{min}}}\right)\leq \left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}} \right)\cdot e^{-BL_{1}/t_{\text{min}}-L_{1}^{2}t_{\text{max}}^{2}/t_{\text{ min}}^{2}}\right)\cdot TV\left(\hat{y}_{t_{\text{max}}},\overline{y}_{t_{\text{max}}}\right)\]

On the other hand, by Lemma 4,

\[\mathbb{E}\left[\|\hat{y}_{t_{\text{min}}}-y_{t_{\text{min}}}\|\right]\leq e^{2L_{2}(t_{\text{max}}-t_{\text{min}})}\left(\epsilon_{approx}+ \delta L_{0}+L_{2}\left(\delta L_{1}+\sqrt{2\delta dt_{\text{max}}}\right) \right)\left(t_{\text{max}}-t_{\text{min}}\right).\]

The SDE triangular upper bound on \(W_{1}(\bar{y}_{t_{\text{min}}},y_{t_{\text{min}}})\) follows by multiplying the first inequality by \(B\) (to bound \(W_{1}(\bar{y}_{t_{\text{min}}},\hat{y}_{t_{\text{min}}})\)) and then adding the second inequality (to bound \(W_{1}(y_{t_{\text{min}}},\hat{y}_{t_{\text{min}}})\)). Notice that by definition, \(TV\left(\hat{y}_{t_{\text{max}}},\overline{y}_{t_{\text{max}}}\right)=TV\left(y _{t_{\text{min}}},\overline{y}_{t_{\text{max}}}\right)\). Finally, because of the assumption that \(K\leq\frac{C}{L_{2}(t_{\text{max}}-t_{\text{min}})}\) for some universal constant, we summarize the second term in the Eq. (8) and Eq. (9) into the big \(O\) in the informal version Theorem 1. 

**Theorem 4**.: _[Formal version of Theorem 2] Consider the same setting as Theorem 3. Let \(p_{t_{\text{max}}}^{Restar_{\theta},i}\) denote the distributions after \(i^{\text{th}}\) Restart iteration, i.e., the distribution of \(\overline{x}_{t_{\text{min}}}^{i}=\text{Restart}_{0}(\overline{x}_{t_{\text{ min}}}^{0},i)\). Given initial \(\overline{x}_{t_{\text{max}}}^{0}\sim p_{t_{\text{max}}}^{Restart,0}\), let \(\overline{x}_{t_{\text{max}}}^{0}=\text{ODE}_{\theta}(\overline{x}_{t_{\text{ max}}}^{0},t_{\text{max}}\to t_{\text{min}})\). Then_

\[W_{1}(p_{t_{\text{min}}}^{Restart_{\theta},K},p_{t_{\text{max}}}) \leq \underbrace{B\cdot\left(1-\lambda\right)^{K}TV(p_{t_{\text{max}}} ^{Restart,0},p_{t_{\text{max}}})}_{\text{upper bound on contracted error}}\] \[+\underbrace{e^{(K+1)L_{2}(t_{\text{max}}-t_{\text{min}})}(K+1) \left(\delta(L_{2}L_{1}+L_{0})+\epsilon_{approx}\right)\left(t_{\text{max}}-t_{ \text{min}}\right)}_{\text{upper bound on additional sampling error}}\]

_where \(\lambda=2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{max}}^{2}}}\right)\)._

Proof.: Let \(x_{t_{\text{max}}}^{0}\sim p_{t_{\text{max}}}\). Let \(x_{t_{\text{min}}}^{K}=\text{Restart}(x_{t_{\text{min}}}^{0},K)\). We verify that \(x_{t_{\text{min}}}^{K}\) has density \(p_{t_{\text{min}}}\). Let us also define \(\hat{x}_{t_{\text{min}}}^{0}=\text{ODE}_{\theta}(x_{t_{\text{max}}}^{0},t_{ \text{max}}\to t_{\text{min}})\) and \(\hat{x}_{t_{\text{min}}}^{K}=\text{Restart}_{\theta}(\hat{x}_{t_{\text{min}}}^{0},K)\).

By Lemma 1,

\[TV\left(\overline{x}_{t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^{ K}\right)\leq \left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}} \right)\right)^{K}TV\left(\overline{x}_{t_{\text{min}}}^{0},\hat{x}_{t_{\text{min}}}^{0}\right)\] \[\leq \left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}} }\right)\right)^{K}TV\left(\overline{x}_{t_{\text{min}}}^{0},\hat{x}_{t_{\text{ min}}}^{0}\right)\] \[= \left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}} }\right)\right)^{K}TV\left(\overline{x}_{t_{\text{min}}}^{0},x_{t_{\text{ min}}}^{0}\right)\]The second inequality holds by data processing inequality. The above can be used to bound the 1-Wasserstein distance as follows:

\[W_{1}\left(\overline{x}_{t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^{K}\right) \leq B\cdot TV\left(\overline{x}_{t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^ {K}\right)\leq\left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min} }^{2}}}\right)\right)^{K}TV\left(\overline{x}_{t_{\text{max}}}^{0},x_{t_{\text {max}}}^{0}\right)\] (14)

On the other hand, using Lemma 3,

\[W_{1}\left(x_{t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^{K}\right) \leq\left\|x_{t_{\text{min}}}^{K}-\hat{x}_{t_{\text{min}}}^{K}\right\|\] \[\leq e^{\left(K+1\right)L_{2}\left(t_{\text{max}}-t_{\text{min}} \right)}(K+1)\left(\delta(L_{2}L_{1}+L_{0})+\epsilon_{approx}\right)\left(t_{ \text{max}}-t_{\text{min}}\right)\] (15)

We arrive at the result by combining the two bounds above (Eq. (14), Eq. (15)) with the following triangular inequality,

\[W_{1}(\bar{x}_{t_{\text{min}}}^{K},x_{t_{\text{min}}}^{K})\leq W_{1}(\bar{x}_ {t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^{K})+W_{1}(\hat{x}_{t_{\text{min }}}^{K},x_{t_{\text{min}}}^{K})\]

### Mixing under Restart with exact ODE

**Lemma 1**.: _Consider the same setup as Theorem 4. Consider the Restart\({}_{\theta}\) process defined in equation 7. Let_

\[x_{t_{\text{min}}}^{i} =\text{Restart}_{\theta}(x_{t_{\text{min}}}^{0},i)\] \[y_{t_{\text{min}}}^{i} =\text{Restart}_{\theta}(y_{t_{\text{min}}}^{0},i).\]

_Let \(p_{t}^{\text{Restart}_{\theta}(i)}\) and \(q_{t}^{\text{Restart}_{\theta}(i)}\) denote the densities of \(x_{t}^{i}\) and \(y_{t}^{i}\) respectively. Then_

\[TV\left(p_{t_{\text{min}}}^{\text{Restart}_{\theta}(K)},q_{t_{\text{min}}}^{ \text{Restart}_{\theta}(K)}\right)\leq\left(1-\lambda\right)^{K}TV\left(p_{t_ {\text{min}}}^{\text{Restart}_{\theta}(0)},q_{t_{\text{min}}}^{\text{Restart} _{\theta}(0)}\right),\]

_where \(\lambda=2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}}\right)\)._

Proof.: Conditioned on \(x_{t_{\text{min}}}^{i},y_{t_{\text{min}}}^{i}\), let \(x_{t_{\text{max}}}^{i+1}=x_{t_{\text{min}}}^{i}+\sqrt{t_{\text{max}}^{2}-t_{ \text{min}}^{2}}\xi_{i}^{x}\) and \(y_{t_{\text{max}}}^{i+1}=y_{t_{\text{min}}}^{i}+\sqrt{t_{\text{max}}^{2}-t_{ \text{min}}^{2}}\xi_{i}^{y}\). We now define a coupling between \(x_{t_{\text{min}}}^{i+1}\) and \(y_{t_{\text{min}}}^{i+1}\) by specifying the joint distribution over \(\xi_{i}^{x}\) and \(\xi_{i}^{y}\).

If \(x_{t_{\text{min}}}^{i}=y_{t_{\text{min}}}^{i}\), let \(\xi_{i}^{x}=\xi_{i}^{y}\), so that \(x_{t_{\text{min}}}^{i+1}=y_{t_{\text{min}}}^{i+1}\). On the other hand, if \(x_{t_{\text{min}}}^{i}\neq y_{t_{\text{min}}}^{i}\), let \(x_{t_{\text{min}}}^{i+1}\) and \(y_{t_{\text{max}}}^{i+1}\) be coupled as described in the proof of Lemma 7, with \(x^{\prime}=x_{t_{\text{min}}}^{i+1},y^{\prime}=y_{t_{\text{max}}}^{i+1},\sigma= \sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}\). Under this coupling, we verify that,

\[\mathbb{E}\left[\mathbbm{1}\left\{x_{t_{\text{min}}}^{i+1}\neq y _{t_{\text{min}}}^{i+1}\right\}\right]\] \[\leq \mathbb{E}\left[\mathbbm{1}\left\{x_{t_{\text{min}}}^{i+1}\neq y _{t_{\text{min}}}^{i+1}\right\}\right]\] \[\leq \mathbb{E}\left[\left(1-2Q\left(\frac{\left\|x_{t_{\text{min}}}^ {i}-y_{t_{\text{min}}}^{i}\right\|}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2 }}}\right)\right)\mathbbm{1}\left\{x_{t_{\text{min}}}^{i}\neq y_{t_{\text{min}}}^ {i}\right\}\right]\] \[\leq \left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min} }^{2}}}\right)\right)\mathbb{E}\left[\mathbbm{1}\left\{x_{t_{\text{min}}}^{i} \neq y_{t_{\text{min}}}^{i}\right\}\right].\]

Applying the above recursively,

\[\mathbb{E}\left[\mathbbm{1}\left\{x_{t_{\text{min}}}^{K}\neq y_{t_{\text{min}}}^ {K}\right\}\right]\leq\left(1-2Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{ \text{min}}^{2}}}\right)\right)^{K}\mathbb{E}\left[\mathbbm{1}\left\{x_{t_{ \text{min}}}^{0}\neq y_{t_{\text{min}}}^{0}\right\}\right].\]

The conclusion follows by noticing that \(TV\left(p_{t_{\text{min}}}^{\text{Restart}_{\theta}(K)},q_{t_{\text{min}}}^{ \text{Restart}_{\theta}(K)}\right)\leq Pr\left(x_{t_{\text{min}}}^{K}\neq y_{t_{ \text{min}}}^{K}\right)=\mathbb{E}\left[\mathbbm{1}\left\{x_{t_{\text{min}}}^{K} \neq y_{t_{\text{min}}}^{K}\right\}\right]\), and by selecting the initial coupling so that \(Pr\left(x_{t_{\text{min}}}^{0}\neq y_{t_{\text{min}}}^{0}\right)=TV\left(p_{t_{ \text{min}}}^{\text{Restart}_{\theta}(0)},q_{t_{\text{min}}}^{\text{Restart}_{ \theta}(0)}\right)\).

### \(W_{1}\) discretization bound

**Lemma 2** (Discretization bound for ODE).: _Let \(x_{t_{\text{max}}}=\text{ODE}\left(x_{t_{\text{max}}},t_{\text{max}}\to t_{\text{ min}}\right)\) and let \(\overline{x}_{t_{\text{max}}}=\text{ODE}_{\theta}\left(\overline{x}_{t_{\text{max }}},t_{\text{max}}\to t_{\text{min}}\right)\). Assume that for all \(x,y,s,t\), \(s_{\theta}(x,t)\) satisfies \(\left\|t_{s\theta}(x,t)-ts_{\theta}(x,s)\right\|\leq L_{0}|s-t|\), \(\left\|ts_{\theta}(x,t)\right\|\leq L_{1}\) and \(\left\|t_{s\theta}(x,t)-ts_{\theta}(y,t)\right\|\leq L_{2}\left\|x-y\right\|\). Then_

\[\left\|x_{t_{\text{max}}}-\overline{x}_{t_{\text{min}}}\right\|\leq e^{(t_{ \text{max}}-t_{\text{max}})L_{2}}\left(\left\|x_{t_{\text{max}}}-\overline{x} _{t_{\text{max}}}\right\|+\left(\delta(L_{2}L_{1}+L_{0})+\epsilon_{approx} \right)(t_{\text{max}}-t_{\text{min}})\right)\]

Proof.: Consider some fixed arbitrary \(k\), and recall that \(\delta\) is the step size. Recall that by definition of ODE and ODE\({}_{\theta}\), for \(t\in((k-1)\delta,k\delta]\),

\[dx_{t} =-t\nabla\log p_{t}(x_{t})dt\] \[d\overline{x}_{t} =-ts_{\theta}(\overline{x}_{k\delta},k\delta)dt.\]

For \(t\in[t_{\text{min}},t_{\text{max}}]\), let us define a time-reversed process \(x_{t}^{\leftarrow}:=x_{-t}\). Let \(v(x,t):=\nabla\log p_{-t}(x)\). Then for \(t\in[-t_{\text{max}},-t_{\text{min}}]\)

\[dx_{t}^{\leftarrow}=tv(x_{t}^{\leftarrow},t)ds.\]

Similarly, define \(\overline{x}_{t}^{\leftarrow}:=\overline{x}_{-t}\) and \(\overline{v}(x,t):=s_{\theta}\left(x,-t\right)\). It follows that

\[d\overline{x}_{t}^{\leftarrow}=t\overline{v}(\overline{x}_{k\delta}^{ \leftarrow},k\delta)ds,\]

where \(k\) is the unique (negative) integer satisfying \(t\in[k\delta,(k+1)\delta)\). Following these definitions,

\[\frac{d}{dt}\left\|x_{t}^{\leftarrow}-\overline{x}_{t}^{\leftarrow}\right\|\] \[\leq \left\|tv(x_{t}^{\leftarrow},t)-t\overline{v}(x_{t}^{\leftarrow}, t)\right\|\] \[\quad+\left\|t\overline{v}(\overline{x}_{t}^{\leftarrow},t)-t \overline{v}(\overline{x}_{t}^{\leftarrow},t)\right\|\] \[\quad+\left\|t\overline{v}(\overline{x}_{t}^{\leftarrow},t)-t \overline{v}(\overline{x}_{t}^{\leftarrow},k\delta)\right\|\] \[\quad+\left\|t\overline{v}(\overline{x}_{t}^{\leftarrow},k\delta) -t\overline{v}(\overline{x}_{k\delta}^{\leftarrow},k\delta)\right\|\] \[\leq \epsilon_{approx}+L_{2}\left\|x_{t}^{\leftarrow}-\overline{x}_{t}^ {\leftarrow}\right\|+\delta L_{0}+L_{2}\left\|\overline{x}_{t}^{\leftarrow}- \overline{x}_{k\delta}^{\leftarrow}\right\|\] \[\leq \epsilon_{approx}+L_{2}\left\|x_{t}^{\leftarrow}-\overline{x}_{t} ^{\leftarrow}\right\|+\delta L_{0}+\delta L_{2}L_{1}.\]

Applying Gronwall's Lemma over the interval \(t\in[-t_{\text{max}},-t_{\text{min}}]\),

\[\left\|x_{t_{\text{min}}}-\overline{x}_{t_{\text{min}}}\right\|\] \[= \left\|x_{-t_{\text{min}}}^{\leftarrow}-\overline{x}_{-t_{\text{ min}}}^{\leftarrow}\right\|\] \[\leq e^{L_{2}\left(t_{\text{max}}-t_{\text{min}}\right)}\left(\left\|x_ {-t_{\text{max}}}^{\leftarrow}-\overline{x}_{-t_{\text{max}}}^{\leftarrow} \right\|+\left(\epsilon_{approx}+\delta L_{0}+\delta L_{2}L_{1}\right)(t_{ \text{max}}-t_{\text{min}})\right)\] \[= e^{L_{2}\left(t_{\text{max}}-t_{\text{min}}\right)}\left(\left\|x _{t_{\text{max}}}-\overline{x}_{t_{\text{min}}}\right\|+\left(\epsilon_{approx }+\delta L_{0}+\delta L_{2}L_{1}\right)(t_{\text{max}}-t_{\text{min}})\right).\]

**Lemma 3**.: _Given initial \(x_{t_{\text{max}}}^{0}\), let \(x_{t_{\text{min}}}^{0}=\text{ODE}\left(x_{t_{\text{max}}}^{0},t_{\text{max}} \to t_{\text{min}}\right)\), and let \(\hat{x}_{t_{\text{min}}}^{0}=\text{ODE}_{\theta}\left(x_{t_{\text{max}}}^{0},t_ {\text{max}}\to t_{\text{min}}\right)\). We further denote the variables after \(K\) Restart iterations as \(x_{t_{\text{max}}}^{K}=\text{Restart}(x_{t_{\text{min}}}^{0},K)\) and \(\hat{x}_{t_{\text{min}}}^{K}=\text{Restart}_{\theta}(\hat{x}_{t_{\text{max}}}^{0},K)\), with true field and learned field respectively. Then there exists a coupling between \(x_{t_{\text{min}}}^{K}\) and \(\hat{x}_{t_{\text{min}}}^{K}\) such that_

\[\left\|x_{t_{\text{min}}}^{K}-\hat{x}_{t_{\text{min}}}^{K}\right\|\leq e^{(K+1)L_{2} \left(t_{\text{min}}-t_{\text{min}}\right)}(K+1)\left(\delta(L_{2}L_{1}+L_{0})+ \epsilon_{approx}\right)\left(t_{\text{max}}-t_{\text{min}}\right).\]

Proof.: We will couple \(x_{t_{\text{min}}}^{i}\) and \(\hat{x}_{t_{\text{min}}}^{i}\) by using the same noise \(\varepsilon_{t_{\text{min}}\to t_{\text{min}}}^{i}\) in the Restart forward process for \(i=0\ldots K-1\) (see Eq. (7)). For any \(i\), let us also define \(y_{t_{\text{min}}}^{i,j}:=\text{Restart}_{\theta}\left(x_{t_{\text{min}}}^{i},j-i\right)\), and this process uses the same noise \(\varepsilon_{t_{\text{min}}\to t_{\text{max}}}^{i}\) as previous ones. From this definition, \(y_{t_{\text{min}}}^{K,K}=x_{t_{\text{min}}}^{K}\). We can thus bound

\[\left\|x_{t_{\text{min}}}^{K},\hat{x}_{t_{\text{min}}}^{K}\right\|\leq\left\|y_{t_ {\text{min}}}^{0,K}-\hat{x}_{t_{\text{min}}}^{K}\right\|+\sum_{i=0}^{K-1}\left\|y_{t_ {\text{min}}}^{i,K}-y_{t_{\text{min}}}^{i+1,K}\right\|\] (16)Using the assumption that \(ts_{\theta}(\cdot,t)\) is \(L_{2}\) Lipschitz,

\[\left\|y_{t_{\text{min}}}^{0,i+1}-\hat{x}_{t_{\text{min}}}^{i+1}\right\|\] \[= \left\|\text{ODE}_{\theta}(y_{t_{\text{max}}}^{0,i},t_{\text{max} }\to t_{\text{min}})-\text{ODE}_{\theta}(\hat{x}_{t_{\text{max}}}^{i},t_{ \text{max}}\to t_{\text{min}})\right\|\] \[\leq e^{L_{2}(t_{\text{max}}-t_{\text{min}})}\left\|y_{t_{\text{max}}} ^{0,i}-\hat{x}_{t_{\text{max}}}^{i}\right\|\] \[= e^{L_{2}(t_{\text{max}}-t_{\text{min}})}\left\|y_{t_{\text{min}}} ^{0,i}-\hat{x}_{t_{\text{min}}}^{i}\right\|,\]

where the last equality is because we add the same additive Gaussian noise \(\varepsilon_{t_{\text{min}}\to t_{\text{max}}}^{i}\) to \(y_{t_{\text{min}}}^{0,i}\) and \(\hat{x}_{t_{\text{min}}}^{i}\) in the Restart forward process. Applying the above recursively, we get

\[\left\|y_{t_{\text{min}}}^{0,K}-\hat{x}_{t_{\text{min}}}^{K}\right\|\leq e^{KL_{2}(t_{\text{max}}-t_{\text{min}})}\left\|y_{t_{\text{min}}}^{0,0}- \hat{x}_{t_{\text{min}}}^{0}\right\|\] \[\leq e^{KL_{2}(t_{\text{max}}-t_{\text{min}})}\left\|x_{t_{\text{min}} }^{0}-\hat{x}_{t_{\text{min}}}^{0}\right\|\] \[\leq e^{(K+1)L_{2}(t_{\text{max}}-t_{\text{min}})}\left(\delta(L_{2} L_{1}+L_{0})+\epsilon_{approx}\right)(t_{\text{max}}-t_{\text{min}})\,,\] (17)

where the last line follows by Lemma 2 when setting \(x_{t_{\text{max}}}=\bar{x}_{t_{\text{max}}}\). We will now bound \(\left\|y_{t_{\text{min}}}^{i,K}-y_{t_{\text{min}}}^{i+1,K}\right\|\) for some \(i\leq K\). It follows from definition that

\[y_{t_{\text{min}}}^{i,i+1}=\text{ODE}_{\theta}\left(x_{t_{\text {max}}}^{i},t_{\text{max}}\to t_{\text{min}}\right)\] \[y_{t_{\text{min}}}^{i+1,i+1}=x_{t_{\text{min}}}^{i+1}=\text{ODE }\left(x_{t_{\text{max}}}^{i},t_{\text{max}}\to t_{\text{min}}\right).\]

By Lemma 2,

\[\left\|y_{t_{\text{min}}}^{i,i+1}-y_{t_{\text{min}}}^{i+1,i+1}\right\|\leq e^{L _{2}(t_{\text{max}}-t_{\text{min}})}\left(\delta(L_{2}L_{1}+L_{0})+\epsilon_{ approx}\right)(t_{\text{max}}-t_{\text{min}})\]

For the remaining steps from \(i+2\ldots K\), both \(y^{i,\cdot}\) and \(y^{i+1,\cdot}\) evolve with ODE\({}_{\theta}\) in each step. Again using the assumption that \(ts_{\theta}(\cdot,t)\) is \(L_{2}\) Lipschitz,

\[\left\|y_{t_{\text{min}}}^{i,K}-y_{t_{\text{min}}}^{i+1,K}\right\|\leq e^{(K-i )L_{2}(t_{\text{max}}-t_{\text{min}})}\left(\delta(L_{2}L_{1}+L_{0})+\epsilon _{approx}\right)(t_{\text{max}}-t_{\text{min}})\]

Summing the above for \(i=0...K-1\), and combining with Eq. (16) and Eq. (17) gives

\[\left\|x_{t_{\text{min}}}^{K}-\hat{x}_{t_{\text{min}}}^{K}\right\|\leq e^{(K+1 )L_{2}(t_{\text{max}}-t_{\text{min}})}(K+1)\left(\delta(L_{2}L_{1}+L_{0})+ \epsilon_{approx}\right)(t_{\text{max}}-t_{\text{min}})\,.\]

**Lemma 4**.: _Consider the same setup as Theorem 3. Let \(x_{t_{\text{min}}}=\text{SDE}\left(x_{t_{\text{max}}},t_{\text{max}}\to t_{ \text{min}}\right)\) and let \(\overline{x}_{t_{\text{min}}}=\text{SDE}\left(\overline{x}_{t_{\text{max}}},t_ {\text{max}}\to t_{\text{min}}\right)\). Then there exists a coupling between \(x_{t}\) and \(\overline{x}_{t}\) such that_

\[\mathbb{E}\left[\left\|x_{t_{\text{min}}}-\overline{x}_{t_{\text{ min}}}\right\|\right]\leq e^{2L_{2}(t_{\text{max}}-t_{\text{min}})}\mathbb{E} \left[\left\|x_{t_{\text{max}}}-\overline{x}_{t_{\text{max}}}\right\|\right]\\ +e^{2L_{2}(t_{\text{max}}-t_{\text{min}})}\left(\epsilon_{approx}+ \delta L_{0}+L_{2}\left(\delta L_{1}+\sqrt{2\delta dt_{\text{max}}}\right) \right)(t_{\text{max}}-t_{\text{min}})\]

Proof.: Consider some fixed arbitrary \(k\), and recall that \(\delta\) is the stepsize. By definition of SDE and SDE\({}_{\theta}\), for \(t\in((k-1)\delta,k\delta]\),

\[dx_{t} =-2t\nabla\log p_{t}(x_{t})dt+\sqrt{2t}dB_{t}\] \[d\overline{x}_{t} =-2ts_{\theta}(\overline{x}_{k\delta},k\delta)dt+\sqrt{2t}dB_{t}.\]

Let us define a coupling between \(x_{t}\) and \(\overline{x}_{t}\) by identifying their respective Brownian motions. It will be convenient to define the time-reversed processes \(x_{t}^{\leftarrow}:=x_{-t}\), and \(\overline{x}_{t}^{\leftarrow}:=\overline{x}_{-t}\), along with \(v(x,t):=\nabla\log p_{-t}(x)\) and \(\overline{v}(x,t):=s_{\theta}(x,-t)\). Then there exists a Brownian motion \(B_{t}^{\leftarrow}\), such that for \(t\in[-t_{\text{max}},-t_{\text{min}}]\),

\[dx_{t}^{\leftarrow}=-2tv(x_{t}^{\leftarrow},t)dt+\sqrt{-2t}dB_{t}^{\leftarrow}\] \[d\overline{x}_{t}^{\leftarrow}=-2t\overline{v}(\overline{x}_{k \delta}^{\leftarrow},k\delta)dt+\sqrt{-2t}dB_{t}^{\leftarrow}\] \[\Rightarrow d(x_{t}^{\leftarrow}-\overline{x}_{t}^{\leftarrow})=-2t\left(v(x_{t}^{ \leftarrow},t)-\overline{v}(\overline{x}_{k\delta}^{\leftarrow},k\delta)\right)dt,\]where \(k\) is the unique negative integer such that \(t\in[k\delta,(k+1)\delta)\). Thus

\[\frac{d}{dt}\mathbb{E}\left[\left\|x_{t}^{\leftarrow}-\overline{x} _{t}^{\leftarrow}\right\|\right]\] \[\leq 2\left(\mathbb{E}\left[\left\|tv(x_{t}^{\leftarrow},t)-t \overline{v}(x_{t}^{\leftarrow},t)\right\|\right]+\mathbb{E}\left[\left\|t \overline{v}(x_{t}^{\leftarrow},t)-t\overline{v}(\overline{x}_{t}^{ \leftarrow},t)\right\|\right)\right.\] \[\qquad+2\left(\mathbb{E}\left[\left\|t\overline{v}(\overline{x}_ {t}^{\leftarrow},t)-t\overline{v}(\overline{x}_{t}^{\leftarrow},k\delta) \right\|\right]+\mathbb{E}\left[\left\|t\overline{v}(\overline{x}_{t}^{ \leftarrow},k\delta)-t\overline{v}(\overline{x}_{k\delta}^{\leftarrow},k \delta)\right\|\right]\right)\] \[\leq 2\left(\epsilon_{approx}+L_{2}\mathbb{E}\left[\left\|x_{t}^{ \leftarrow}-\overline{x}_{t}^{\leftarrow}\right\|\right]+\delta L_{0}+L_{2} \mathbb{E}\left[\left\|\overline{x}_{t}^{\leftarrow}-\overline{x}_{k\delta}^ {\leftarrow}\right\|\right)\] \[\leq 2\left(\epsilon_{approx}+L_{2}\mathbb{E}\left[\left\|x_{t}^{ \leftarrow}-\overline{x}_{t}^{\leftarrow}\right\|\right]+\delta L_{0}+L_{2} \left(\delta L_{1}+\sqrt{2\delta dt_{\text{max}}}\right)\right).\]

By Gronwall's Lemma,

\[\mathbb{E}\left[\left\|x_{t_{\text{min}}}-\overline{x}_{t_{\text {min}}}\right\|\right]\] \[= \mathbb{E}\left[\left\|x_{t_{\text{min}}}^{\leftarrow}-\overline{x }_{t_{\text{min}}}^{\leftarrow}\right\|\right]\] \[\leq e^{2L_{2}\left(t_{\text{max}}-t_{\text{min}}\right)}\left( \mathbb{E}\left[\left\|x_{-t_{\text{max}}}^{\leftarrow}-\overline{x}_{-t_{ \text{min}}}^{\leftarrow}\right\|\right]+\left(\epsilon_{approx}+\delta L_{0}+L _{2}\left(\delta L_{1}+\sqrt{2\delta dt_{\text{max}}}\right)\right)\left(t_{ \text{max}}-t_{\text{min}}\right)\right)\] \[= e^{2L_{2}\left(t_{\text{max}}-t_{\text{min}}\right)}\left( \mathbb{E}\left[\left\|x_{t_{\text{max}}}-\overline{x}_{t_{\text{min}}} \right\|\right]+\left(\epsilon_{approx}+\delta L_{0}+L_{2}\left(\delta L_{1}+ \sqrt{2\delta dt_{\text{max}}}\right)\right)\left(t_{\text{max}}-t_{\text{ min}}\right)\right)\]

### Mixing Bounds

**Lemma 5**.: _Consider the same setup as Theorem 3. Assume that \(\delta\leq t_{\text{min}}\). Let_

\[x_{t_{\text{min}}}=\text{SDE}_{\theta}\left(x_{t_{\text{max}}},t_ {\text{max}}\to t_{\text{min}}\right)\] \[y_{t_{\text{min}}}=\text{SDE}_{\theta}\left(y_{t_{\text{max}}},t_ {\text{max}}\to t_{\text{min}}\right).\]

_Then there exists a coupling between \(x_{s}\) and \(y_{s}\) such that_

\[TV\left(x_{t_{\text{min}}},y_{t_{\text{min}}}\right)\leq\left(1-2Q\left(\frac {B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}}\right)\cdot e^{-BL_{1}/t_{ \text{min}}-L_{1}^{2}t_{\text{max}}^{2}/t_{\text{min}}^{2}}\right)TV\left(x_{ t_{\text{min}}},y_{t_{\text{max}}}\right)\]

Proof.: We will construct a coupling between \(x_{t}\) and \(y_{t}\). First, let \((x_{t_{\text{max}}},y_{t_{\text{max}}})\) be sampled from the optimal TV coupling, _i.e._, \(Pr(x_{t_{\text{max}}}\neq y_{t_{\text{max}}})=TV(x_{t_{\text{max}}},y_{t_{ \text{max}}})\). Recall that by definition of \(\text{SDE}_{\theta}\), for \(t\in((k-1)\delta,k\delta]\),

\[dx_{t}=-2ts_{\theta}(x_{k\delta},k\delta)dt+\sqrt{2t}dB_{t}.\]

Let us define a time-rescaled version of \(x_{t}\): \(\overline{x}_{t}:=x_{t^{2}}\). We verify that

\[d\overline{x}_{t}=-s_{\theta}(\overline{x}_{(k\delta)^{2}},k\delta)dt+dB_{t},\]

where \(k\) is the unique integer satisfying \(t\in[((k-1)\delta)^{2},k^{2}\delta^{2})\). Next, we define the time-reversed process \(\overline{x}_{t}^{\leftarrow}:=\overline{x}_{-t}\), and let \(v(x,t):=s_{\theta}(x,-t)\). We verify that there exists a Brownian motion \(B_{t}^{x}\) such that, for \(t\in[-t_{\text{max}}^{2},-t_{\text{min}}^{2}]\),

\[d\overline{x}_{t}^{\leftarrow}=v_{t}^{x}dt+dB_{t}^{x},\]

where \(v_{t}^{x}=s_{\theta}(\overline{x}_{-(k\delta)^{2}}^{\leftarrow},-k\delta)\), where \(k\) is the unique positive integer satisfying \(-t\in(((k-1)\delta)^{2},(k\delta)^{2}]\). Let \(d\overline{y}_{t}^{\leftarrow}=v_{t}^{y}dt+dB_{t}^{y},\) be defined analogously. For any positive integer \(k\) and for any \(t\in[-(k\delta)^{2},-((k-1)\delta)^{2})\), let us define

\[z_{t}=\overline{x}_{-k^{2}\delta^{2}}^{\leftarrow}-\overline{y}_{-k^{2} \delta^{2}}^{\leftarrow}+(2k-1)\delta^{2}\left(v_{-(k\delta)^{2}}^{x}-v_{-(k \delta)^{2}}^{y}\right)+\left(B_{t}^{x}-B_{-(k\delta)^{2}}^{x}\right)-\left(B_{t }^{y}-B_{-(k\delta)^{2}}^{y}\right).\]

Let \(\gamma_{t}:=\frac{z_{t}}{\left\|z_{t}\right\|}\). We will now define a coupling between \(dB_{t}^{x}\) and \(dB_{t}^{y}\) as

\[dB_{t}^{y}=\left(I-2\mathbb{1}\left\{t\leq\tau\right\}\gamma_{t}\gamma_{t}^{T} \right)dB_{t}^{x},\]where \(\mathbbm{1}\left\{\right\}\) denotes the indicator function, i.e. \(1\left\{t\leq\tau\right\}=1\) if \(t\leq\tau\), and \(\tau\) is a stopping time given by the first hitting time of \(z_{t}=0\). Let \(r_{t}:=\left\|z_{t}\right\|\). Consider some \(t\in\left(-i^{2}\delta^{2},-(i-1)^{2}\delta^{2}\right)\), and Let \(j:=\frac{t_{\text{max}}}{\delta}\) (assume w.l.o.g that this is an integer), then

\[r_{t}-r_{-t_{\text{max}}^{2}}\leq \sum_{k=i}^{j}(2k-1)\delta^{2}\left\|(v_{-(k\delta)^{2}}^{x}-v_{- (k\delta)^{2}}^{y})\right\|+\int_{-t_{\text{max}}^{2}}^{t}\mathbbm{1}\left\{t \leq\tau\right\}2dB_{s}^{1}\] \[\leq \sum_{k=i}^{j}\left(k^{2}-(k-1)^{2}\right)\delta^{2}2L_{1}/ \left(t_{\text{min}}\right)+\int_{-t_{\text{max}}^{2}}^{t}\mathbbm{1}\left\{t \leq\tau\right\}2dB_{t}^{1}\] \[= \int_{-t_{\text{max}}^{2}}^{-(i-1)\delta^{2}}\frac{2L_{1}}{t_{ \text{min}}}ds+\int_{-t_{\text{max}}^{2}}^{t}\mathbbm{1}\left\{t\leq\tau \right\}2dB_{s}^{1},\]

where \(dB_{s}^{1}=\left\langle\gamma_{t},dB_{s}^{x}-dB_{s}^{y}\right\rangle\) is a 1-dimensional Brownian motion. We also verify that

\[r_{-t_{\text{max}}^{2}}= \left\|z_{-t_{\text{max}}^{2}}\right\|\] \[= \left\|\overline{x}_{-t_{\text{max}}^{2}}^{\leftarrow}-\overline{ y}_{-t_{\text{max}}^{2}}^{\leftarrow}+(2j-1)\delta^{2}\left(v_{-t_{\text{max}}^{2}}^{x}-v_ {-t_{\text{max}}^{2}}^{y}\right)+\left(B_{t}^{x}-B_{-t_{\text{max}}^{2}}^{x} \right)-\left(B_{t}^{y}-B_{-t_{\text{max}}^{2}}^{y}\right)\right\|\] \[\leq \left\|\overline{x}_{-t_{\text{max}}^{2}}^{\leftarrow}+(2j-1) \delta^{2}v_{-t_{\text{max}}^{2}}^{x}+\left(B_{-(j-1)^{2}\delta^{2}}^{x}-B_{-t _{\text{max}}^{2}}^{x}\right)\right\|\] \[\quad+\left\|\overline{y}_{-t_{\text{max}}^{2}}^{\leftarrow}+(2j -1)\delta^{2}v_{-t_{\text{max}}^{2}}^{y}+\left(B_{-(j-1)^{2}\delta^{2}}^{x}-B_ {t}^{x}+B_{t}^{y}-B_{-t_{\text{min}}^{2}}^{y}\right)\right\|\leq B\]

where the third relation is by adding and subtracting \(B_{-(j-1)^{2}\delta^{2}}^{x}-B_{t}^{x}\) and using triangle inequality. The fourth relation is by noticing that \(\overline{x}_{-t_{\text{max}}^{2}}^{\leftarrow}+(2j-1)\delta^{2}v_{-t_{\text {max}}^{2}}^{x}+\left(B_{-(j-1)^{2}\delta^{2}}^{x}-B_{-t_{\text{max}}^{2}}^{x }\right)=\overline{x}_{-(j-1)^{2}\delta^{2}}^{+}\) and that \(\overline{y}_{-t_{\text{max}}^{2}}^{\leftarrow}+(2j-1)\delta^{2}v_{-t_{\text {min}}^{2}}^{y}+\left(B_{-(j-1)^{2}\delta^{2}}^{x}-B_{t}^{x}+B_{t}^{y}-B_{-t_{ \text{min}}^{2}}^{y}\right)\overset{d}{=}\overline{y}_{-(j-1)^{2}\delta^{2}}^ {\leftarrow}\), and then using our assumption in the theorem statement that all processes are supported on a ball of radius \(B/2\).

We now define a process \(s_{t}\) defined by \(ds_{t}=2L_{1}/t_{\text{min}}dt+2dB_{t}^{1}\), initialized at \(s_{-t_{\text{min}}^{2}}=B\geq r_{-t_{\text{min}}^{2}}\). We can verify that, up to time \(\tau\), \(r_{t}\leq s_{t}\) with probability 1. Let \(\tau^{\prime}\) denote the first-hitting time of \(s_{t}\) to \(0\), then \(\tau\leq\tau^{\prime}\) with probability 1. Thus

\[Pr(\tau\leq-t_{\text{min}}^{2})\geq Pr(\tau^{\prime}\leq-t_{\text{min}}^{2})\geq 2 Q\left(\frac{B}{2\sqrt{t_{\text{max}}^{2}-t_{\text{min}}^{2}}}\right)\cdot e^{- BL_{1}/t_{\text{min}}-L_{1}^{2}t_{\text{max}}^{2}/t_{\text{min}}^{2}}\]

where we apply Lemma 6. The proof follows by noticing that, if \(\tau\leq-t_{\text{min}}^{2}\), then \(x_{t_{\text{min}}}=y_{t_{\text{min}}}\). This is because if \(\tau\in[-k^{2}\delta^{2},-(k-1)^{2}\delta^{2}]\), then \(\overline{x}_{-(k-1)^{2}\delta^{2}}^{\leftarrow}=\overline{y}_{-(k-1)^{2} \delta^{2}}^{\leftarrow}\), and thus \(\overline{x}_{t}^{\leftarrow}=\overline{y}_{t}^{\leftarrow}\) for all \(t\geq-(k-1)^{2}\delta^{2}\), in particular, at \(t=-t_{\text{min}}^{2}\).

**Lemma 6**.: _Consider the stochastic process_

\[dr_{t}=dB_{t}^{1}+cdt.\]

_Assume that \(r_{0}\leq B/2\). Let \(\tau\) denote the hitting time for \(r_{t}=0\). Then for any \(T\in\mathbb{R}^{+}\),_

\[Pr(\tau\leq T)\geq 2Q\left(\frac{B}{2\sqrt{T}}\right)\cdot e^{-ac-\frac{c^{2}T}{ 2}},\]

_where \(Q\) is the tail probability of a standard Gaussian defined in Definition 1._

Proof.: We will use he following facts in our proof:

1. For \(x\sim\mathcal{N}(0,\sigma^{2})\), \(Pr(x>r)=\frac{1}{2}\left(1-erf\left(\frac{r}{\sqrt{2\sigma}}\right)\right)= \frac{1}{2}erfc\left(\frac{r}{\sqrt{2\sigma}}\right)\).
2. \(\int_{0}^{T}\frac{a\exp\left(-\frac{a^{2}}{2T}\right)}{\sqrt{2\pi t^{3}}}dt=erfc \left(\frac{a}{\sqrt{2T}}\right)=2Pr\left(\mathcal{N}(0,T)>a\right)=2Q\left( \frac{a}{\sqrt{T}}\right)\) by definition of \(Q\).

Let \(dr_{t}=dB_{t}^{1}+cdt\), with \(r_{0}=a\). The density of the hitting time \(\tau\) is given by

\[p(\tau=t)=f(a,c,t)=\frac{a\exp\left(-\frac{\left(a+ct\right)^{2}}{2t}\right)}{ \sqrt{2\pi t^{3}}}.\] (18)

(see e.g. [3]). From item 2 above,

\[\int_{0}^{T}f(a,0,t)dt=2Q\left(\frac{a}{\sqrt{T}}\right).\]

In the case of a general \(c\neq 0\), we can bound \(\frac{\left(a+ct\right)^{2}}{2t}=\frac{a^{2}}{2t}+ac+\frac{c^{2}t}{2}\). Consequently,

\[f(a,c,t)\geq f(a,0,t)\cdot e^{-ac-\frac{c^{2}t}{2}}.\]

Therefore,

\[Pr(\tau\leq T)=\int_{0}^{T}f(a,c,t)dt\geq\int_{0}^{T}f(a,0,t)dte^{-c}=2Q\left( \frac{B}{2\sqrt{T}}\right)\cdot e^{-ac-\frac{c^{2}T}{2}}.\]

### TV Overlap

**Definition 1**.: _Let \(x\) be sampled from standard normal distribution \(\mathcal{N}(0,1)\). We define the Gaussian tail probability \(Q(a):=Pr(x\geq a)\)._

**Lemma 7**.: _We verify that for any two random vectors \(\xi_{x}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})\) and \(\xi_{y}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})\), each belonging to \(\mathbb{R}^{d}\), the total variation distance between \(x^{\prime}=x+\xi_{x}\) and \(y^{\prime}=y+\xi_{y}\) is given by_

\[TV(x^{\prime},y^{\prime})=1-2Q\left(r\right)\leq 1-\frac{2r}{r^{2}+1}\frac{1}{ \sqrt{2\pi}}e^{-r^{2}/2},\]

_where \(r=\frac{\left\|x-y\right\|}{2\sigma}\), and \(Q(r)=Pr(\xi\geq r)\), when \(\xi\sim\mathcal{N}(0,1)\)._

Proof.: Let \(\gamma:=\frac{x-y}{\left\|x-y\right\|}\). We decompose \(x^{\prime},y^{\prime}\) into the subspace/orthogonal space defined by \(\gamma\):

\[x^{\prime} =x^{\perp}+\xi_{x}^{\perp}+x^{\parallel}+\xi_{x}^{\parallel}\] \[y^{\prime} =y^{\perp}+\xi_{y}^{\perp}+y^{\parallel}+\xi_{y}^{\parallel}\]

where we define

\[x^{\parallel} :=\gamma\gamma^{T}x\qquad x^{\perp}:=x-x^{\parallel}\] \[y^{\parallel} :=\gamma\gamma^{T}y\qquad y^{\perp}:=y-y^{\parallel}\] \[\xi_{\parallel}^{\parallel} :=\gamma\gamma^{T}\xi_{x}\qquad\xi_{x}^{\perp}:=\xi_{x}-\xi_{x}^ {\parallel}\] \[\xi_{y}^{\parallel} :=\gamma\gamma^{T}\xi_{y}\qquad\xi_{y}^{\perp}:=\xi_{y}-\xi_{y}^ {\parallel}\]

We verify the independence \(\xi_{x}^{\perp}\perp\!\!\!\perp\xi_{x}^{\parallel}\) and \(\xi_{y}^{\perp}\perp\!\!\!\perp\xi_{y}^{\parallel}\) as they are orthogonal decompositions of the standard Gaussian. We will define a coupling between \(x^{\prime}\) and \(y^{\prime}\) by setting \(\xi_{x}^{\perp}=\xi_{y}^{\perp}\). Under this coupling, we verify that

\[\left(x^{\perp}+\xi_{x}^{\perp}\right)-\left(y^{\perp}+\xi_{y}^{\perp}\right) =x-y-\gamma\gamma^{T}(x-y)=0\]

Therefore, \(x^{\prime}=y^{\prime}\) if and only if \(x^{\parallel}+\xi_{x}^{\parallel}=y^{\parallel}+\xi_{y}^{\parallel}\). Next, we draw \((a,b)\) from the optimal coupling between \(\mathcal{N}(0,1)\) and \(\mathcal{N}(\frac{\left\|x-y\right\|}{\sigma},1)\). We verify that \(x^{\parallel}+\xi_{x}^{\parallel}\) and \(y^{\parallel}+\xi_{y}^{\parallel}\) both lie in the span of \(\gamma\). Thus it suffices to compare \(\left\langle\gamma,x^{\parallel}+\xi_{x}^{\parallel}\right\rangle\) and \(\left\langle\gamma,y^{\parallel}+\xi_{y}^{\parallel}\right\rangle\). We verify that \(\left\langle\gamma,y^{\parallel}\right\rangle+\left\langle\gamma,x^{\parallel}-y^{ \parallel}\right\rangle+\left\langle\gamma,\xi_{x}^{\parallel}\right\rangle \sim\mathcal{N}(\left\langle\gamma,y^{\parallel}\right\rangle+\left\|x-y \right\|,\sigma^{2})\overset{d}{=}\left\langle\gamma,y^{\parallel}\right\rangle +\sigma b\). We similarly verify that \(\left\langle\gamma,y^{\parallel}+\xi_{y}^{\parallel}\right\rangle=\left\langle \gamma,y^{\parallel}\right\rangle+\left\langle\gamma,\xi_{y}^{\parallel} \right\rangle\sim\mathcal{N}(\left\langle\gamma,y^{\parallel}\right\rangle, \sigma^{2})\overset{d}{=}\left\langle\gamma,y^{\parallel}\right\rangle+\sigma a\).

Thus \(TV(x^{\prime},y^{\prime})=TV(\sigma a,\sigma b)=1-2Q\left(\frac{\left\|x-y \right\|}{2\sigma}\right)\). The last inequality follows from

\[Pr(\mathcal{N}(0,1)\geq r)\geq\frac{r}{r^{2}+1}\frac{1}{\sqrt{2\pi}}e^{-r^{2} /2}\]

## Appendix B More on Restart Algorithm

### EDM Discretization Scheme

[13] proposes a discretization scheme for ODE given the starting \(t_{\text{max}}\) and end time \(t_{\text{min}}\). Denote the number of steps as \(N\), then the EDM discretization scheme is:

\[t_{i<N}=\left(t_{\text{max}}^{\frac{1}{\rho}}+\frac{i}{N-1}(t_{\text{min}}^{ \frac{1}{\rho}}-t_{\text{max}}^{\frac{1}{\rho}})\right)^{\rho}\]

with \(t_{0}=t_{\text{max}}\) and \(t_{N-1}=t_{\text{min}}\). \(\rho\) is a hyperparameter that determines the extent to which steps near \(t_{\text{min}}\) are shortened. We adopt the value \(\rho=7\) suggested by [13] in all of our experiments. We apply the EDM scheme to creates a time discretization in each Restart interval \([t_{\text{max}},t_{\text{min}}]\) in the Restart backward process, as well as the main backward process between \([0,T]\) (by additionally setting \(t_{\text{min}}=0.002\) and \(t_{N}=0\) as in [13]). It is important to note that \(t_{\text{min}}\) should be included within the list of time steps in the main backward process to seamlessly incorporate the Restart interval into the main backward process. We summarize the scheme as a function in Algorithm 1.

```
1:return\(\left\{(t_{\text{max}}^{\frac{1}{\rho}}+\frac{i}{N-1}(t_{\text{min}}^{\frac{1}{ \rho}}-t_{\text{max}}^{\frac{1}{\rho}}))^{\rho}\right\}_{i=0}^{N-1}\) ```

**Algorithm 1** EDM_Scheme(\(t_{\text{min}},t_{\text{max}},N,\rho=7\))

### Restart Algorithm

We present the pseudocode for the Restart algorithm in Algorithm 2. In this pseudocode, we describe a more general case that applies \(l\)-level Restarting strategy. For each Restart segment, the include the number of steps in the Restart backward process \(N_{\text{Restart}}\), the Restart interval \([t_{\text{min}},t_{\text{max}}]\) and the number of Restart iteration \(K\). We further denote the number of steps in the main backward process as \(N_{\text{main}}\). We use the EDM discretization scheme (Algorithm 1) to construct time steps for the main backward process (\(t_{0}=T,t_{N_{\text{main}}}=0\)) as well as the Restart backward process, when given the starting/end time and the number of steps.

Although Heun's 2nd order method [2] (Algorithm 3) is the default ODE solver in the pseudocode, it can be substituted with other ODE solvers, such as Euler's method or the DPM solver [16].

The provided pseudocode in Algorithm 2 is tailored specifically for diffusion models [13]. To adapt Restart for other generative models like PFGM++ [28], we only need to modify the Gaussian perturbation kernel in the Restart forward process (line 10 in Algorithm 2) to the one used in PFGM++.

## Appendix C Experimental Details

In this section, we discuss the configurations for different samplers in details. All the experiments are conducted on eight NVIDIA A100 GPUs.

```
1:Input: Score network \(s_{\theta}\), time steps in main backward process \(t_{i\in\{0,N_{\text{sum}}\}}\), Restart parameters \(\{(N_{\text{Restart},j},K_{j},t_{\text{min},j},t_{\text{max},j})\}_{j=1}^{l}\)
2:Round \(t_{\text{min},j\in\{1,l\}}\) to its nearest neighbor in \(t_{i\in\{0,N_{\text{sum}}\}}\)
3:Sample \(x_{0}\sim\mathcal{N}(0,T^{2}\bm{I})\)
4:for\(i=0\)...\(N_{\text{main}}-1\)do\(\triangleright\) Main backward process
5:\(x_{t_{i+1}}=\text{OneStep\_Heun}(s_{\theta},t_{i},t_{i+1})\)\(\triangleright\) Running single step ODE
6:if\(\exists j\in\{1,\ldots,l\},t_{i+1}=t_{\text{min},j}\)then
7:\(t_{\text{min}}=t_{\text{min},j},t_{\text{max}}=t_{\text{max},j},K=K_{j},N_{ \text{Restart}}=N_{\text{Restart},j}\)
8:\(x_{t_{\text{min}}}^{0}=x_{t_{i+1}}\)
9:for\(k=0\ldots K-1\)do\(\triangleright\) Restart for \(K\) iterations
10:\(\varepsilon_{t_{\text{min}}\to t_{\text{max}}}\sim\mathcal{N}(\mathbf{0},(t_{ \text{max}}^{2}-t_{\text{min}}^{2})\bm{I})\)
11:\(x_{t_{i}}^{k+1}=x_{t_{\text{min}}}^{k}+\varepsilon_{t_{\text{min}}\to t_{ \text{max}}}\)\(\triangleright\) Restart forward process
12:\(\{\bar{t}_{m}\}_{m=1}^{N_{\text{max}}-1}=\text{EDM\_Scheme}(t_{\text{min}},t_{ \text{max}},N_{\text{Restart}})\)
13:for\(m=0\ldots N_{\text{Restart}}-1\)do\(\triangleright\) Restart backward process
14:\(x_{t_{m+1}}^{k+1}=\text{OneStep\_Heun}(s_{\theta},\bar{t}_{m},\bar{t}_{m+1})\)
15:endfor
16:endfor
17:endif
18:endfor
19:return\(x_{t_{N_{\text{min}}}}\) ```

**Algorithm 2** Restart sampling

### Configurations for Baselines

We select **Vanilla SDE**[23], **Improved SDE**[13], **Gonna Go Fast**[12] as SDE baselines and the **Heun**'s 2nd order method [2] (Alg 3) as ODE baseline on standard benchmarks CIFAR-10 and ImageNet \(64\times 64\). We choose **DDIM**[22], **Heun**'s 2nd order method, and **DDPM**[9] for comparison on Stable Diffusion model.

Vanilla SDE denotes the reverse-time SDE sampler in [23]. For Improved SDE, we use the recommended dataset-specific hyperparameters (_e.g._, \(S_{\text{max}},S_{\text{min}},S_{\text{churn}}\)) in Table 5 of the EDM paper [13]. They obtained these hyperparameters by grid search. Gonna Go Fast [12] applied an adaptive step size technique based on Vanilla SDE and we directly report the FID scores listed in [12] for Gonna Go Fast on CIFAR-10 (VP). For fair comparison, we use the EDM discretization scheme [13] for Vanilla SDE, Improved SDE, Heun as well as Restart.

We borrow the hyperparameters such as discretization scheme or initial noise scale on Stable Diffusion models in the diffuser 3 code repository. We directly use the DDIM and DDPM samplers implemented in the repo. We apply the same set of hyperparameters to Heun and Restart.

Footnote 3: https://github.com/huggingface/diffusers

### Configurations for Restart

We report the configurations for Restart for different models and NFE on standard benchmarks CIFAR-10 and ImageNet \(64\times 64\). The hyperparameters of Restart include the number of steps in the main backward process \(N_{\text{main}}\), the number of steps in the Restart backward process \(N_{\text{Restart}}\), the Restart interval \([t_{\text{min}},t_{\text{max}}]\) and the number of Restart iteration \(K\). In Table 3 (CIFAR-10, VP)we provide the quintuplet \((N_{\text{main}},N_{\text{Restart}},t_{\text{min}},t_{\text{max}},K)\) for each experiment. Since we apply the multi-level Restart strategy for ImageNet \(64\times 64\), we provide \(N_{\text{main}}\) as well as a list of quadruple \(\{(N_{\text{Restart},i},K_{i},t_{\text{min},i},t_{\text{max},i})\}_{i=1}^{l}\) (\(l\) is the number of Restart interval depending on experiments) in Table 5. In order to integrate the Restart time interval to the main backward process, we round \(t_{\text{min},i}\) to its nearest neighbor in the time steps of main backward process, as shown in line 2 of Algorithm 2. We apply Heun method for both main/backward process. The formula for NFE calculation is \(\text{NFE}=\underbrace{2\cdot N_{\text{main}}-1}_{\text{main backward process}}+\sum_{i=1}^{l} \underbrace{K_{i}}_{\text{number of repetitions}}\cdot\underbrace{(2\cdot(N_{\text{ Restart},i}-1))}_{\text{per iteration in $i$th Restart interval}}\) in this case. Inspired by [13], we inflate the additive noise in the Restart forward process by multiplying \(S_{\text{noise}}=1.003\) on ImageNet \(64\times 64\), to counteract the over-denoising tendency of neural networks. We also observe that setting \(\gamma=0.05\) in Algorithm 2 of EDM [13] would sligtly boost the Restart performance on ImageNet\(64\times 64\) when \(t\in[0.01,1]\).

We further include the configurations for Restart on Stable Diffusion models in Table 10, with a varying guidance weight \(w\). Similar to ImageNet \(64\times 64\), we use multi-level Restart with a fixed number of steps \(N_{\text{main}}=30\) in the main backward process. We utilize the Euler method for the main backward process and the Heun method for the Restart backward process, as our empirical observations indicate that the Heun method doesn't yield significant improvements over the Euler method, yet necessitates double the steps. The number of steps equals to \(N_{\text{main}}+\sum_{i=1}^{l}K_{i}\cdot(2\cdot(N_{\text{Restart},i}-1))\) in this case. We set the total number of steps to \(66\), including main backward process and Restart backward process.

Given the prohibitively large search space for each Restart quadruple, a comprehensive enumeration of all possibilities is impractical due to computational limitations. Instead, we adjust the configuration manually, guided by the heuristic that weaker/smaller models or more challenging tasks necessitate a stronger Restart strength (e.g., larger \(K\), wider Restart interval, etc). On average, we select the best configuration from \(5\) sets for each experiment; these few trials have empirically outperformed previous SDE/ODE samplers. We believe that developing a systematic approach for determining Restart configurations could be of significant value in the future.

### Pre-trained Models

For CIFAR-10 dataset, we use the pre-trained VP and EDM models from the EDM repository 4, and PFGM++ (\(D=2048\)) model from the PFGM++ repository 5. For ImageNet \(64\times 64\), we borrow the pre-trained EDM model from EDM repository as well.

Footnote 4: https://github.com/NVlabs/edm

Footnote 5: https://github.com/Newbeeer/pfgmpp

### Classifier-free Guidance

We follow the convention in [20], where each step in classifier-free guidance is as follows:

\[\tilde{s}_{\theta}(x,c,t)=ws_{\theta}(x,c,t)+(1-w)s_{\theta}(x,t)\]

where \(c\) is the conditions, and \(s_{\theta}(x,c,t)/s_{\theta}(x,t)\) is the conditional/unconditional models, sharing parameters. Increasing \(w\) would strength the effect of guidance, usually leading to a better text-image alignment [20].

### More on the Synthetic Experiment

#### c.5.1 Discrete Dataset

We generate the underlying discrete dataset \(S\) with \(|S|=2000\) as follows. Firstly, we sample 2000 points, denoted as \(S_{1}\), from a mixture of two Gaussians in \(\mathbb{R}^{4}\). Next, we project these points onto \(\mathbb{R}^{20}\). To ensure a variance of 1 on each dimension, we scale the coordinates accordingly. This setup aims to simulate data points that primarily reside on a lower-dimensional manifold with multiple modes.

The specific details are as follows: \(S_{1}\sim 0.3N(a,s^{2}I)+0.7(-a,s^{2}I)\), where \(a=(3,3,3,3)\subset\mathbb{R}^{4}\) and \(s=1\). Then, we randomly select a projection matrix \(P\in\mathbb{R}^{20\times 4}\), where each entry is drawn from \(N(0,1)\), and compute \(S_{2}=PS_{1}\). Finally, we scale each coordinate by a constant factor to ensure a variance of 1.

#### c.5.2 Model Architecture

We employ a common MLP architecture with a latent size of 64 to learn the score function. The training method is adapted from [13], which includes the preconditioning technique and denoising score-matching objective [25].

#### c.5.3 Varying Hyperparameters

To achieve the best trade-off between contracted error and additional sampling error, and optimize the NFE versus FID (Frechet Inception Distance) performance, we explore various hyperparameters. [13] shows that the Vanilla SDE can be endowed with additional flexibility by varying the coefficient \(\beta(t)\) (Eq.(6) in [13]). Hence, regarding SDE, we consider NFE values from \(\{20,40,80,160,320\}\), and multiply the original \(\beta(t)=\dot{\sigma}(t)/\sigma(t)\)[13] with values from \(\{0,0.25,0.5,1,1.5,2,4,8\}\). It is important to note that larger NFE values do not lead to further performance improvements. For restarts, we tried the following two settings: first we set the number of steps in Restart backward process to 40 and vary the number of Restart iterations \(K\) in the range \(\{0,5,10,15,20,25,30,35\}\). We also conduct a grid search with the number of Restart iterations \(K\) ranging from 5 to 25 and the number of steps in Restart backward process varying from 2 to 7. For ODE, we experiment with the number of steps set to \(\{20,40,80,160,320,640\}\).

Additionally, we conduct an experiment for Improved SDE in EDM. We try different values of \(S_{\text{churn}}\) in the range of \(\{0,1,2,4,8,16,32,48,64\}\). We also perform a grid search where the number of steps ranged from \(20\) to \(320\) and \(S_{\text{churn}}\) takes values of \([0.2\times\text{steps},0.5\times\text{steps},20,60]\). The plot combines the results from SDE and is displayed in Figure 8.

To mitigate the impact of randomness, we collect the data by averaging the results from five runs with the same hyperparameters. To compute the Wasserstein distance between two discrete distributions, we use minimum weight matching.

#### c.5.4 Plotting the Pareto frontier

We generate the Pareto frontier plots as follows. For the additional sampling error versus contracted error plot, we first sort all the data points based on their additional sampling error and then connect the data points that represent prefix minimums of the contracted error. Similarly, for the NFE versus FID plot, we sort the data points based on their NFE values and connect the points where the FID is a prefix minimum.

## Appendix D Extra Experimental Results

### Numerical Results

In this section, we provide the corresponding numerical reuslts of Fig. 3(a) and Fig. 3(b), in Table 2, 3 (CIFAR-10 VP, EDM, PFGM++) and Table 4, 5 (ImageNet \(64\times 64\) EDM), respectively. We also include the performance of Vanilla SDE in those tables. For the evaluation, we compute the Frechet distance between 50000 generated samples and the pre-computed statistics of CIFAR-10 and ImageNet \(64\times 64\). We follow the evaluation protocol in EDM [13] that calculates each FID scores three times with different seeds and report the minimum.

Figure 8: Comparison of additional sampling error versus **(a)** contracted error (plotting the Pareto frontier) and **(b)** total error (using a scatter plot). **(c)** Pareto frontier of NFE versus total error.

[MISSING_PAGE_FAIL:25]

\begin{table}
\begin{tabular}{c c c} \hline \hline NFE & FID (50k) & Configuration \\  & & \(N_{\text{main}}\), \(\{(N_{\text{Restart},i},K_{i},t_{\text{min},i},t_{\text{max},i})\}_{i=1}^{l}\) \\ \hline \multirow{4}{*}{623} & \multirow{4}{*}{1.36} & 36, \{((10, 3, 19.35, 40.79),(10, 3, 1.09, 1.92), \\  & & (7, 6, 0.59, 1.09), (7, 6, 0.30, 0.59), \\  & & (7, 25, 0.06, 0.30)\} \\ \multirow{4}{*}{535} & \multirow{4}{*}{1.39} & 36, \{(6, 1, 19.35, 40.79),(6, 1, 1.09, 1.92), \\  & & (7, 6, 0.59, 1.09), (7, 6, 0.30, 0.59), \\  & & (7, 25, 0.06, 0.30)\} \\ \multirow{4}{*}{385} & \multirow{4}{*}{1.41} & 36, \{(3, 1, 19.35, 40.79),(6, 1, 1.09, 1.92), \\  & & (6, 5, 0.59, 1.09), (6, 5, 0.30, 0.59), \\  & & (6, 20, 0.06, 0.30)\} \\ \multirow{4}{*}{203} & \multirow{4}{*}{1.46} & 36, \{(4, 1, 19.35, 40.79),(4, 1, 1.09, 1.92), \\  & & (4, 5, 0.59, 1.09), (4, 5, 0.30, 0.59), \\  & & (6, 6, 0.06, 0.30)\} \\ \multirow{4}{*}{165} & \multirow{4}{*}{1.51} & 18, \{(3, 1, 19.35, 40.79),(4, 5, 0.30, 0.59), \\  & & (4, 10, 0.06, 0.30)\} \\ \multirow{4}{*}{99} & \multirow{4}{*}{1.71} & 18, \{(3, 1, 19.35, 40.79),(4, 1, 1.09, 1.92), \\  & & (4, 4, 0.59, 1.09), (4, 1, 0.30, 0.59), \\  & & (4, 4, 0.06, 0.30)\} \\ \multirow{4}{*}{67} & \multirow{4}{*}{1.95} & 18, \{(5, 1, 19.35, 40.79),(5, 1, 1.09, 1.92), \\  & & (5, 1, 0.59, 1.09), (5, 1, 0.06, 0.30)\} \\ \multirow{4}{*}{39} & \multirow{4}{*}{2.38} & 14, \{(3, 1, 19.35, 40.79),(3, 1, 1.06, 0.30)\} \\  & & (3, 1, 1.09, 1.92), (3, 1, 0.06, 0.30)\} \\ \hline \hline \end{tabular}
\end{table}
Table 4: ImageNet \(64\times 64\) sample quality (FID score) and number of function evaluations (NFE) on EDM [13] for baselines

\begin{table}
\begin{tabular}{c c c} \hline \hline  & NFE & FID (50k) \\ \hline _ODE (Heun)_[13] & 1023 & 2.24 \\  & 511 & 2.24 \\  & 255 & 2.24 \\  & 127 & 2.25 \\  & 63 & 2.30 \\  & 35 & 2.46 \\ \hline _Vanilla SDE_[23] & 1024 & 1.89 \\  & 512 & 3.38 \\  & 256 & 11.91 \\  & 128 & 59.71 \\ \hline _Improved SDE_[13] & 1023 & 1.40 \\  & 511 & 1.45 \\  & 255 & 1.50 \\  & 127 & 1.75 \\  & 63 & 2.24 \\  & 35 & 2.97 \\ \hline \hline \end{tabular}
\end{table}
Table 5: ImageNet \(64\times 64\) sample quality (FID score), number of function evaluations (NFE) and Restart configurations on EDM [13]

### Sensitivity Analysis of Hyper-parameters

We also investigate the impact of varying \(t_{\text{min}}\) when \(t_{\text{max}}=t_{\text{min}}+0.3\), and the length the restart interval when \(t_{\text{min}}=0.06\). Fig. 10(a) reveals that FID scores achieve a minimum at a \(t_{\text{min}}\) close to \(0\)

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Steps & FID (5k) \(\downarrow\) & CLIP score \(\uparrow\) & Aesthetic score \(\uparrow\) \\ \hline _DDIM_[22] & 50 & 16.08 & 0.2905 & 5.13 \\  & 100 & 15.35 & 0.2920 & 5.15 \\ \hline _Heun_ & 51 & 18.80 & 0.2865 & 5.14 \\  & 101 & 18.21 & 0.2871 & 5.15 \\ \hline _DDPM_[9] & 100 & 13.53 & 0.3012 & 5.20 \\  & 200 & 13.22 & 0.2999 & 5.19 \\ \hline _Restart_ & 66 & 13.16 & 0.2987 & 5.19 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight \(w=2\)

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Steps & FID (5k) \(\downarrow\) & CLIP score \(\uparrow\) & Aesthetic score \(\uparrow\) \\ \hline _DDIM_[22] & 50 & 14.28 & 0.3056 & 5.22 \\  & 100 & 14.30 & 0.3056 & 5.22 \\ \hline _Heun_ & 51 & 15.63 & 0.3022 & 5.20 \\  & 101 & 15.40 & 0.3026 & 5.21 \\ \hline _DDPM_[9] & 100 & 15.72 & 0.3129 & 5.28 \\  & 200 & 15.13 & 0.3131 & 5.28 \\ \hline _Restart_ & 66 & 14.48 & 0.3079 & 5.25 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight \(w=3\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Steps & FID (5k) \(\downarrow\) & CLIP score \(\uparrow\) & Aesthetic score \(\uparrow\) \\ \hline _DDIM_[22] & 50 & 16.60 & 0.3154 & 5.31 \\  & 100 & 16.80 & 0.3157 & 5.31 \\ \hline _Heun_ & 51 & 16.26 & 0.3135 & 5.28 \\  & 101 & 16.38 & 0.3136 & 5.29 \\ \hline _DDPM_[9] & 100 & 19.62 & 0.3197 & 5.36 \\  & 200 & 18.88 & 0.3200 & 5.35 \\ \hline _Restart_ & 66 & 16.21 & 0.3179 & 5.33 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight \(w=5\)

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Steps & FID (5k) \(\downarrow\) & CLIP score \(\uparrow\) & Aesthetic score \(\uparrow\) \\ \hline _DDIM_[22] & 50 & 19.83 & 0.3206 & 5.37 \\  & 100 & 19.82 & 0.3200 & 5.37 \\ \hline _Heun_ & 51 & 18.44 & 0.3186 & 5.35 \\  & 101 & 18.72 & 0.3185 & 5.36 \\ \hline _DDPM_[9] & 100 & 22.58 & 0.3223 & 5.39 \\  & 200 & 21.67 & 0.3212 & 5.38 \\ \hline _Restart_ & 47 & 18.40 & 0.3228 & 5.41 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Numerical results on Stable Diffusion v1.5 with a classifier-free guidance weight \(w=8\)on VP, indicating higher accumulated errors at the end of sampling and poor neural estimations at small \(t\). Note that the Restart interval \(0.3\) is about twice the length of the one in Table 1 and Restart does not outperform the ODE baseline on EDM. This suggests that, as a rule of thumb, we should apply greater Restart strength (_e.g._, larger \(K\), \(t_{\text{max}}-t_{\text{min}}\)) for weaker or smaller architectures and vice versa.

In theory, a longer interval enhances contraction but may add more additional sampling errors. Again, the balance between these factors results in a V-shaped trend in our plots (Fig. 10(b)). In practice, selecting \(t_{\text{max}}\) close to the dataset's radius usually ensures effective mixing when \(t_{\text{min}}\) is small.

\begin{table}
\begin{tabular}{c c} \hline \hline \(w\) & Configuration \\  & \(N_{\text{main}}\), \{((N_{\text{Restart},i},K_{i},t_{\text{min},i},t_{\text{max},i})\}_{i=1}^{l}\) \\ \hline
2 & 30, \{(5,2,1,9), (5,2,5,10)\} \\
3 & 30, \{(10,2,0.1,3)\} \\
5 & 30, \{(10,20.1,2)\} \\
8 & 30, \{(10,2,0.1,2)\} \\ \hline \hline \end{tabular}
\end{table}
Table 10: Restart (Steps=66) configurations on Stable Diffusion v1.5

Figure 10: (a): Adjusting \(t_{\text{min}}\) in Restart on VP/EDM; (b): Adjusting the Restart interval length when \(t_{\text{min}}=0.06\).

Figure 9: FID score versus **(a)** CLIP ViT-g/14 score and **(b)** Aesthetic score for text-to-image generation at \(512\times 512\) resolution, using Stable Diffusion v1.5 with varying classifier-free guidance weight \(w=2,3,5,8\).

Extended Generated Images

In this section, we provide extended generated images by Restart, DDIM, Heun and DDPM on text-to-image Stable Diffusion v1.5 model [19]. We showcase the samples of four sets of text prompts in Fig. 11, Fig. 12, Fig. 13, Fig. 14, with a classifier-guidance weight \(w=8\).

## Appendix F Heun's method is DPM-Solver-2 (with \(r_{2}=1\))

The first order ODE in DPM-Solver [16] (DPM-Solver-1) is in the form of:

\[\hat{x}_{t_{i-1}}=\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}}\hat{x}_{t_{i-1}}-( \hat{\sigma}_{t_{i-1}}\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}}-\hat{\sigma}_{t_ {i}})\hat{\sigma}_{t_{i}}\nabla_{x}\log p_{\hat{\sigma}_{t_{i}}}(\hat{x}_{t_{i}})\] (19)

The first order ODE in EDM is in the form of

\[x_{t_{i-1}}=x_{t_{i}}-(\sigma_{t_{i-1}}-\sigma_{t_{i}})\sigma_{t_{i}}\nabla_{x }\log p_{\sigma_{t_{i}}}(x_{t_{i}})\] (20)

Figure 11: Generated images with text prompt=“A photo of an astronaut riding a horse on mars” and \(w=8\).

When \(x_{t}=\frac{\hat{x}_{t}}{\alpha_{t}},\hat{\sigma}_{t}=\sigma_{t}\alpha_{t}\), we can rewrite the DPM-Solver-1 (Eq. (19)) as:

\[x_{t_{i-1}} =x_{t_{i}}-(\sigma_{t_{i-1}}-\sigma_{t_{i}})\hat{\sigma}_{t_{i}} \nabla_{x}\log p_{\hat{\sigma}_{t_{i}}}(\hat{x}_{t_{i}})\] \[=x_{t_{i}}-(\sigma_{t_{i-1}}-\sigma_{t_{i}})\hat{\sigma}_{t_{i}} \nabla_{x}\log p_{\sigma_{t_{i}}}(x_{t_{i}})\frac{1}{\alpha_{t_{i}}}\qquad \text{(change-of-variable)}\] \[=x_{t_{i}}-(\sigma_{t_{i-1}}-\sigma_{t_{i}})\sigma_{t_{i}} \nabla_{x}\log p_{\sigma_{t_{i}}}(x_{t_{i}})\]

where the expression is exact the same as the ODE in EDM [13]. It indicates that the sampling trajectory in DPM-Solver-1 is equivalent to the one in EDM, up to a time-dependent scaling (\(\alpha_{t}\)). As \(\lim_{t\to 0}\alpha_{t}=1\), the two solvers will leads to the same final points when using the same time discretization. Note that the DPM-Solver-1 is also equivalent to DDIM (_c.f._ Section 4.1 in [16]), as also used in this paper.

With that, we can further verify that the Heun's method used in this paper corresponds to the DPM-Solver-2 when setting \(r_{1}=1\).

Figure 12: Generated images with text prompt=“A raccoon playing table tennis” and \(w=8\).

Figure 13: Generated images with text prompt="Intricate origami of a fox in a snowy forest” and \(w=8\).

Figure 14: Generated images with text prompt="A transparent sculpture of a duck made out of glass" and \(w=8\).

Broader Impact

The field of deep generative models incorporating differential equations is rapidly evolving and holds significant potential to shape our society. Nowadays, a multitude of photo-realistic images generated by text-to-image Stable Diffusion models populate the internet. Our work introduces Restart, a novel sampling algorithm that outperforms previous samplers for diffusion models and PFGM++. With applications extending across diverse areas, the Restart sampling algorithm is especially suitable for generation tasks demanding high quality and rapid speed. Yet, it is crucial to recognize that the utilization of such algorithms can yield both positive and negative repercussions, contingent on their specific applications. On the one hand, Restart sampling can facilitate the generation of highly realistic images and audio samples, potentially advancing sectors such as entertainment, advertising, and education. On the other hand, it could also be misused in _deepfake_ technology, potentially leading to social scams and misinformation. In light of these potential risks, further research is required to develop robustness guarantees for generative models, ensuring their use aligns with ethical guidelines and societal interests.