# The Crucial Role of Normalization in Sharpness-Aware Minimization

 Yan Dai

IIIS, Tsinghua University

yan-dai20@mails.tsinghua.edu.cn

&Kwangjun Ahn

EECS, MIT

kjahn@mit.edu

&Suvrit Sra

TU Munich / MIT

suvrit@mit.edu

The first two authors contribute equally. Work done while Yan Dai was visiting MIT.

###### Abstract

Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding _the role played by normalization_, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima - a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.

## 1 Introduction

We study the recently proposed gradient-based optimization algorithm _Sharpness-Aware Minimization (SAM)_(Foret et al., 2021) that has shown impressive performance in training deep neural networks to generalize well (Foret et al., 2021; Bahri et al., 2022; Mi et al., 2022; Zhong et al., 2022). SAM updates involve an ostensibly small but key modification to Gradient Descent (GD). Specifically, for a loss function \(\mathcal{L}\) and each iteration \(t\geq 0\), instead of updating the parameter \(w_{t}\) as \(w_{t+1}=w_{t}-\eta\nabla\mathcal{L}(w_{t})\) (where \(\eta\) is called the _learning rate_), SAM performs the following update:1

Footnote 1: In principle, the normalization in Equation 1 may make SAM ill-defined. However, Wen et al. (2023, Appendix B) showed that except for countably many learning rates, SAM (with any \(\rho\)) is always well-defined for almost all initialization. Hence, throughout the paper, we assume that the SAM iterates are always well-defined.

\[w_{t+1}=w_{t}-\eta\nabla\mathcal{L}\bigg{(}w_{t}+\rho\frac{\nabla\mathcal{L}( w_{t})}{\|\nabla\mathcal{L}(w_{t})\|}\bigg{)}\,,\] (1)

where \(\rho\) is an additional hyper-parameter that we call the _perturbation radius_. Foret et al. (2021) motivate SAM as an algorithm minimizing the robust loss \(\max_{\|t\|\leq\rho}\mathcal{L}(w+\epsilon)\), which is roughly the loss at \(w\) (i.e., \(\mathcal{L}(w)\)) plus the "sharpness" of the loss landscape around \(w\), hence its name.

The empirical success of SAM has driven a recent surge of interest in characterizing its dynamics and theoretical properties (Bartlett et al., 2022; Wen et al., 2023; Ahn et al., 2023). However, a major component of SAM remains unexplained in prior work: the role and impact of the normalization factor \(\frac{1}{\|\nabla\mathcal{L}(w_{t})\|}\) used by SAM. In fact, quite a few recent works drop the normalization factor for simplicity when analyzing SAM (Andriushchenko and Flammarion, 2022; Behdin and Mazumder, 2023; Agarwala and Dauphin, 2023; Kim et al., 2023; Compagnoni et al., 2023). Instead of the SAM update (1), these works consider the following update that we call _Un-normalized SAM (USAAM)_:

\[w_{t+1}=w_{t}-\eta\nabla\mathcal{L}(w_{t}+\rho\nabla\mathcal{L}(w_{t}))\,.\] (2)Apart from experimental justifications in (Andriushchenko and Flammarion, 2022), the effect of this simplification has not yet been carefully investigated, although it is already widely adopted in the community. Thus, is it really the case that such normalization can be omitted "for simplification" when theoretically analyzing SAM? These observations raise our main question:

_What is the role of the normalization factor \(\frac{1}{\|\nabla\mathcal{L}(w_{t})\|}\) in the SAM update (1)?_

### Motivating Experiments and Our Contributions

We present our main findings through two motivating experiments. For the setting, we choose the well-known over-parameterized matrix sensing problem (Li et al., 2018); see Appendix A for details.

1. **Normalization helps with stability.** We first pick a learning rate \(\eta\) that allows GD to converge, and we gradually increase \(\rho\) from \(0.001\) to \(0.1\). Considering the early stage of training shown in Figure 1. One finds that _SAM has very similar behavior to GD_, whereas _USAM diverges even with a small \(\rho\)_ - it seems that normalization helps stabilize the algorithm.
2. **Normalization permits moving along minima.** We reduce the step size by \(10\) times and consider their performance of reducing test losses in the long run. One may regard Figure 2 as the behavior of SAM, USAM, and GD when close to a "manifold" of minima (which exists since the problem is over-parametrized) as the training losses are close to zero. The first plot compares SAM and USAM with the same \(\rho=0.1\) (the largest \(\rho\) for which USAM doesn't diverge): notice that USAM and GD both converge to a minimum and do not move further; on the other hand, SAM keeps decreasing the test loss, showing its ability to drift along the manifold. We also vary \(\rho\) and compare their behaviors (shown on the right): _the ability of SAM to travel along the manifold of minimizers seems to be robust_ to the size of \(\rho\), while _USAM easily gets stuck at a minimum._

**Overview of our contributions.** In this work, as motivated by Figure 1 and Figure 2, we identify and theoretically explain the two roles of normalization in SAM. The paper is organized as follows.

1. In Section 2, we study the role of normalization in the algorithm's stability and show that normalization helps stabilize. In particular, we demonstrate that normalization ensures that GD's convergence implies SAM's non-divergence, whereas USAM can start diverging much earlier.
2. In Section 3, we study the role of normalization near a manifold of minimizers and show that the normalization factor allows iterates to keep drifting along this manifold - giving better performance in many cases. Without normalization, the algorithm easily gets stuck and no longer progresses.

Figure 1: Role of normalization for stabilizing algorithms (\(\eta=0.05\)).

Figure 2: Role of normalization when close to a manifold of minimizers (\(\eta=0.005\)).

3. In Section 4, to illustrate our main findings, we adopt the sparse coding example of Ahn et al. (2023). Their result implies a dilemma in hyper-parameter tuning for GD: a small \(\eta\) gives worse performance, but a large \(\eta\) results in divergence. We show that this dilemma extends to USAM - but not SAM. In other words, SAM easily solves the problem where GD and USAM often fail.

These findings also shed new light on why SAM is practical and successful, as we highlight below.

**Practical importance of our results.** The main findings in this work explain and underscore several practical aspects of SAM that are mainly due to the normalization step. One practical feature of SAM is the way the hyper-parameter \(\rho\) is tuned: Foret et al. (2021) suggest that \(\rho\) can be tuned independently after tuning the parameters of base optimizers (including learning rate \(\eta\), momentum \(\beta\), and so on). In particular, this feature makes SAM a perfect "add-on" to existing gradient-based optimizers. Our findings precisely support this practical aspect of SAM. Our results suggest that _the stability of SAM is less sensitive to the choice of \(\rho\), thanks to the normalization factor._

The same principle applies to the behavior of the algorithm near the minima: Recent theoretical works (Bartlett et al., 2022; Wen et al., 2023; Ahn et al., 2023) have shown that the drift along the manifold of minimizers is a main feature that enables SAM to reduce the sharpness of the solution (which is believed to boost generalization ability in practice) - our results indicate that _the ability of SAM to keep drifting along the manifold is independent of the choice of \(\rho\), again owing to normalization_. Hence, our work suggests that the normalization factor plays an important role towards SAM's empirical success.

### Related Work

**Sharpness-Aware Optimizers.** Inspired by the empirical and theoretical observation that the generalization effect of a deep neural network is correlated with the "sharpness" of the loss landscape (see (Keskar et al., 2017; Jastrzekbski et al., 2017; Jiang et al., 2020) for empirical observations and (Dziugaite and Roy, 2017; Neyshabur et al., 2017) for theoretical justifications), several recent papers (Foret et al., 2021; Zheng et al., 2021; Wu et al., 2020) propose optimizers that penalize the sharpness for the sake of better generalization. Subsequent efforts were made on making such optimizers scale-invariant (Kwon et al., 2021), more efficient (Liu et al., 2022; Du et al., 2022), and generalize better (Zhuang et al., 2022). This paper focuses on the vanilla version proposed by Foret et al. (2021).

**Theoretical Advances on SAM.** Despite the success of SAM in practice, theoretical understanding of SAM was absent until two recent works: Bartlett et al. (2022) analyze SAM on locally quadratic losses and identify a component reducing the sharpness \(\lambda_{\max}(\nabla^{2}\mathcal{L}(w_{t}))\), while Wen et al. (2023) characterize SAM near the manifold \(\Gamma\) of minimizers and show that SAM follows a Riemannian gradient flow reducing \(\lambda_{\max}(\nabla^{2}\mathcal{L}(w))\) when i) initialized near \(\Gamma\), and ii) \(\eta\) is "small enough". Note that while the results of Wen et al. (2023) apply to more general loss functions, our result in Theorem 16 applies to i) any initialization far from the origin, and ii) any \(\eta=o(1)\) and \(\rho=O(1)\). A recent work by Ahn et al. (2023) formulates the notion of \(\varepsilon\)-approximate flat minima and analyzed the iteration complexity of practical algorithms like SAM to find such approximate flat minima. A concurrent work by Si and Yun (2023) also analyzes the original version of SAM with the normalization in (1), and makes a case that practical SAM does not converge all the way to optima.

**Unnormalized SAM (USAM).** USAM was first proposed by Andriushchenko and Flammario (2022) who observed a similar performance between USAM and SAM when training ResNet over CIFAR-10. This simplification is further accepted by Behdin and Mazumder (2023) who study the regularization effect of USAM over a linear regression model, by Agarwala and Dauphin (2023) who study the initial and final dynamics of USAM over a quadratic regression model, and by Kim et al. (2023) who study the convergence instability of USAM near saddle points. To our knowledge, (Compagnoni et al., 2023) is the only work comparing SAM and USAM dynamics. More preciously, they consider the continuous-time behavior of SGD, SAM, and USAM and find different behaviors of SAM and USAM: USAM attracts local minima while SAM aims at global ones. Still, we remark that as they are considering continuous-time variants of algorithms while we consider discrete (original) versions, our results directly apply to the SAM deployed in practice and the USAM studied in theory.

**Edge-of-Stability.** In the optimization theory literature, Gradient Descent (GD) was only shown to find minima if the learning rate \(\eta\) is smaller than an "Edge-of-Stability" threshold, which is related to the sharpness of the nearest minimum. However, people recently observe that when training neural networks, GD with a \(\eta\) much larger than that threshold often finds good minima as well (see (Cohenet al., 2021) and references therein). Aside from convergence, GD with large \(\eta\) is also shown to find _flatter_ minima (Arora et al., 2022; Ahn et al., 2022; Wang et al., 2022; Damian et al., 2023).

## 2 Role of Normalization for Stability

In this section, we discuss the role of normalization in the stability of the algorithm. We begin by recalling a well-known fact about the stability of GD: for a convex quadratic cost with the largest eigenvalue of Hessian being \(\beta\) (i.e., \(\beta\)-smooth), GD converges to a minimum iff \(\eta<\nicefrac{{2}}{{\beta}}\). Given this folklore fact, we ask: how do the ascent steps in SAM (1) and USAM (2) affect their stability?

### Strongly Convex and Smooth Losses

Consider an \(\alpha\)-strongly-convex and \(\beta\)-smooth loss function \(\mathcal{L}\) where GD is guaranteed to converge once \(\eta<\nicefrac{{2}}{{\beta}}\). We characterize the stability of SAM and USAM in the following result.

**Theorem 1** (Strongly Convex and Smooth Losses).: _For any \(\alpha\)-strongly-convex and \(\beta\)-smooth loss function \(\mathcal{L}\), for any learning rate \(\eta<\nicefrac{{2}}{{\beta}}\) and perturbation radius \(\rho\geq 0\), the following holds:_

1. _SAM. The iterate_ \(w_{t}\) _converges to a local neighborhood around the minimizer_ \(w^{\star}\)_. Formally,_ \[\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq\big{(}1-\alpha\eta(2-\eta\beta )\big{)}^{t}(\mathcal{L}(w_{0})-\mathcal{L}(w^{\star}))+\frac{\eta\beta^{3} \rho^{2}}{2\alpha(2-\eta\beta)},\quad\forall t.\] (3)
2. _USAM. In contrast, there exists some_ \(\alpha\)_-strongly-convex and_ \(\beta\)_-smooth loss_ \(\mathcal{L}\) _such that the USAM with_ \(\eta\in\nicefrac{{2}}{{(\beta+\rho\beta^{2})}},\nicefrac{{2}}{{\beta}}\) _diverges for all except measure zero initialization_ \(w_{0}\)_._

As we discussed, it is well-known that GD converges iff \(\eta<\nicefrac{{2}}{{\beta}}\), and Theorem 1 shows that SAM also does not diverge and stays within an \(\mathcal{O}(\sqrt{\eta}\rho)\)-neighborhood around the minimum as long as \(\eta<\nicefrac{{2}}{{\beta}}\). However, USAM diverges with an even lower learning rate: \(\eta>\nicefrac{{2}}{{(\beta+\rho\beta^{2})}}\) can already make USAM diverge. Intuitively, the larger the value of \(\rho\), the easier it is for USAM to diverge.

One may notice that Equation 3, compared to the standard convergence rate of GD, exhibits an additive bias term of order \(\mathcal{O}(\eta\rho^{2})\). This term arises from the unstable nature of SAM: the perturbation in (1) (which always has norm \(\rho\)) prevents SAM from decreasing the loss monotonically. Thus, SAM can only approach a minimum up to a neighborhood. For this reason, in this paper whenever we say SAM "finds" a minimum, we mean its iterates approach and stay within a neighborhood of that minimum.

Due to space limitations, the full proof is postponed to Appendix C and we only outline it here.

Proof Sketch.: For SAM, we show an analog to the descent lemma of GD as follows (see Lemma 9):

\[\mathcal{L}(w_{t+1})\leq\mathcal{L}(w_{t})-\frac{1}{2}\eta(2-\eta\beta)\| \nabla\mathcal{L}(w_{t})\|^{2}+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\,.\] (4)

By invoking the strong convexity that gives \(\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq\frac{1}{2\alpha}\|\nabla f(w_{t })\|^{2}\), we obtain

\[\mathcal{L}(w_{t+1})-\mathcal{L}(w^{\star})\leq\big{(}1-\alpha\eta(2-\eta\beta )\big{)}(\mathcal{L}(w_{t})-\mathcal{L}(w^{\star}))+\frac{\eta^{2}\beta^{3} \rho^{2}}{2}\,.\]

Recursively applying this relation gives the first conclusion. For USAM, we consider the quadratic loss function same as (Bartlett et al., 2022). Formally, suppose that \(\mathcal{L}(w)=\frac{1}{2}w^{\top}\Lambda w\) where \(\Lambda=\operatorname{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{d})\) is a PSD matrix such that \(\lambda_{1}>\lambda_{2}\geq\cdots\lambda_{d}>0\). Let the eigenvectors corresponding to \(\lambda_{1},\lambda_{2},\ldots,\lambda_{d}\) be \(e_{1},e_{2},\ldots,e_{d}\), respectively. Then we show the following in Theorem 10: for any \(\eta(\lambda_{1}+\rho\lambda_{1}^{2})>2\) and \(\langle w_{0},e_{1}\rangle\neq 0\), USAM must diverge. As \(\mathcal{L}(w)=\frac{1}{2}w^{\top}\Lambda w\) is \(\lambda_{1}\)-smooth and \(\lambda_{d}\)-strongly-convex, the second conclusion also follows. 

Intuitively, the difference in stability can be interpreted as follows: during the early stage of training, \(w_{t}\) and \(\nabla\mathcal{L}(w_{t})\) often have large norms. The normalization in SAM then makes the ascent step \(w_{t}+\rho\frac{\nabla\mathcal{L}(w_{t})}{\|\nabla\mathcal{L}(w_{t})\|}\) not too far away from \(w_{t}\). Hence, if GD does not diverge for this \(\eta\), SAM does not either (unless the \(\rho\)-perturbation is non-negligible, i.e., \(\|w_{t}\|\gg\rho\) no longer holds). This is not true for USAM: since the ascent step is un-normalized, it leads to a point far away from \(w_{t}\), making the size of USAM updates much larger. In other words, the removal of normalization leads to much more aggressive steps, resulting in a different behavior than GD and also an easier divergence.

### Generalizing to Non-Convex Cases: Scalar Factorization Problem

Now let us move on to non-convex losses. We consider a _scalar version_ of the matrix factorization problem \(\min_{U,V}\frac{1}{2}\|UV^{T}-A\|_{2}^{2}\), whose loss function is defined as \(\mathcal{L}(x,y)=\frac{1}{2}(xy)^{2}\). Denote the initialization by \((x_{0},y_{0})\), then \(\mathcal{L}(x,y)\) is \(\beta\triangleq(x_{0}^{2}+y_{0}^{2})\)-smooth inside the region \(\{(x,y):x^{2}+y^{2}\leq\beta\}\). Hence, a learning rate \(\eta<\nicefrac{{2}}{{\beta}}\) again allows GD to converge due to the well-known descent lemma. The following result compares the behavior of SAM and USAM under this setup.

**Theorem 2** (Scalar Factorization Problem; Informal).: _For the loss function \(\mathcal{L}(x,y)=\frac{1}{2}(xy)^{2}\) restricted to a \(\beta\)-smooth region, if we set \(\eta=\nicefrac{{1}}{{\beta}}<\nicefrac{{2}}{{\beta}}\) (so GD finds a minimum), then_

1. _SAM. SAM never diverges and approaches a minimum within an_ \(O(\rho)\)_-neighborhood (in fact, SAM with distinct_ \(\rho\)_'s always find the same minimum_ \((0,0)\)_)._
2. _USAM. On the other hand, USAM diverges once_ \(\rho\geq 15\eta\) _- which could be much smaller than 1._

Thus, our observation in Theorem 1 is not limited to convex losses - for our non-convex scalar-factorization problem, the stability of SAM remains robust to the choice of \(\rho\), while USAM is provably unstable. One may refer to Appendix D for the formal statement and proof of Theorem 2.

### Experiment: Early-Stage Behaviors when Training Neural Networks

As advertised, our result holds not only for convex or toy loss functions but also for practical neural networks. In Figure 3, we plot the early-stage behavior of GD, SAM, and USAM with different \(\rho\) values (while fixing \(\eta\)). We pick two neural networks: a convolutional neural network (CNN) with tanh activation and a fully-connected network (FCN) with ReLU activation. We train them over the CIFAR10 dataset and report the early-stage training losses. Similar to Figure 1, Theorem 1 and Theorem 2, _the stability of SAM is not sensitive to the choice of \(\rho\), while USAM diverges easily_.

## 3 Role of Normalization for Drifting Near Minima

Now, we explain the second role of normalization: enabling the algorithm to drift near minima. To convince why this is beneficial, we adopt a loss function recently considered by Ahn et al. (2023) when understanding the behavior of GD with large learning rates. Their result suggests that GD needs a "large enough" \(\eta\) for enhanced performance, but this threshold can never be known a-priori in practice. To verify our observations from Figure 2, we study the dynamics of SAM and USAM over the same loss function and find that: i) _no careful tuning is needed for SAM_; instead, SAM with any configuration finds the same minimum (which is the "best" one according to Ahn et al. (2023)); and ii) _such property is only enjoyed by SAM_ - for USAM, careful tuning remains essential.

### Toy Model: Single-Neuron Linear Network Model

To theoretically study the role of normalization near minima, we consider the simple two-dimensional non-convex loss \(\mathcal{L}(x,y)\) defined over all \((x,y)\in\mathbb{R}^{2}\) as

\[\mathcal{L}\colon(x,y)\mapsto\ell(x\times y)\,,\qquad\text{where $\ell$ is convex, even, and 1-Lipschitz}.\] (5)

Figure 3: Behaviors of different algorithms when training neural networks (\(\eta=0.025\)).

This \(\mathcal{L}\) was recently studied in (Ahn et al., 2023a) to understand the behavior of GD with large \(\eta\)'s. By direct calculation, the gradient and Hessian of \(\mathcal{L}\) at a given \((x,y)\) can be written as:

\[\nabla\mathcal{L}(x,y)=\ell^{\prime}(xy)\begin{bmatrix}y\\ x\end{bmatrix},\quad\nabla^{2}\mathcal{L}(x,y)=\ell^{\prime\prime}(xy)\begin{bmatrix} y\\ x\end{bmatrix}^{\otimes 2}+\ell^{\prime}(xy)\begin{bmatrix}0&1\\ 1&0\end{bmatrix}.\] (6)

Without loss of generality, one may assume \(\ell\) is minimized at \(0\) (see Appendix E for more details regarding \(\ell\)). Then, \(\mathcal{L}\) achieves minimum at the entire \(x\)- and \(y\)-axes, making it a good toy model for studying the behavior of algorithms near a continuum of minima. Finally, note that the parametrization \(x\times y\) can be interpreted as a single-neuron linear network model - hence its name.

Before moving on to SAM and USAM we first briefly introduce the behavior of GD on such loss functions characterized in (Ahn et al., 2023a). Since \(\ell\) is even, without loss of generality, we always assume that the initialization \(w_{0}=(x_{0},y_{0})\) satisfies \(y_{0}\geq x_{0}>0\).

**Theorem 3** (Theorems 5 and 6 of (Ahn et al., 2023a); Informal).: _For any \(\eta=\gamma/(y_{0}^{2}-x_{0}^{2})\), the GD trajectory over the loss function \(\mathcal{L}(x,y)=\ell(xy)\) has two possible limiting points:_

1. _If_ \(\gamma<2\)_, then the iterates converge to_ \((0,y_{\infty})\) _where_ \(y_{\infty}^{2}\in[\nicefrac{{\gamma}}{{\eta}}-\mathcal{O}(\gamma)-\mathcal{O} (\nicefrac{{\eta}}{{\gamma}}),\nicefrac{{\gamma}}{{\eta}}+\mathcal{O}( \nicefrac{{\eta}}{{\gamma}})]\)_._
2. _If_ \(\gamma>2\)_, then the iterates converge to_ \((0,y_{\infty})\) _where_ \(y_{\infty}^{2}\in[\nicefrac{{2}}{{\eta}}-\mathcal{O}(\eta),\nicefrac{{2}}{{ \eta}}]\)_._

Intuitively, the limiting point of GD (denoted by \((0,y_{\infty})\)) satisfies \(y_{\infty}^{2}\approx\min\{y_{0}^{2}-x_{0}^{2},\nicefrac{{2}}{{\eta}}\}\). For simplicity, we denote \(\eta_{\text{GD}}\approx\nicefrac{{2}}{{y_{0}^{2}-x_{0}^{2}}}\) as the threshold of \(\eta\) that distinguishes these two cases.

**Interpretation of Ahn et al. (2023a).** Fixing the initialization \((x_{0},y_{0})\), it turns out this model has a nice connection to the sparse coding problem, wherein it's desirable to get a smaller \(y_{\infty}^{2}\) (which we will briefly discuss in Section 4). According to Theorem 3, to get a smaller \(y_{\infty}^{2}\), one must increase the learning rate \(\eta\) beyond \(\eta_{\text{GD}}\). Hence we mainly focus on the case where \(\eta>\eta_{\text{GD}}\) - in which case we abbreviate \(y_{\infty}^{2}\approx\nicefrac{{2}}{{\eta}}\) (see Table 1). However, GD diverges once \(\eta\) is too large - in their language, \(\gamma\) cannot be much larger than \(2\). This dilemma of tuning \(\eta\), as we shall illustrate in Section 4 in more detail, makes GD a brittle choice for obtaining a better \(y_{\infty}^{2}\).

On the other hand, from the numerical illustrations in Figure 4, one can see that _SAM keeps moving along the manifold of minimizers_ (i.e., the \(y\)-axis) until the origin. This phenomenon is characterized in Theorem 4; in short, any moderate choice of \(\eta\) and \(\rho\) suffices to drive SAM toward the origin - no difficult tuning needed anymore!

In contrast, USAM does not keep moving along the axis. Instead, a lower bound on \(y_{\infty}^{2}\) also presents - although smaller than the GD version. As we will justify in Theorem 5, _USAM does get trapped_ at some non-zero \(y_{\infty}^{2}\) Thus, a dilemma similar to that of GD shows up: for enhanced performance, an aggressive \((\eta,\rho)\) is needed; however, as we saw from Section 2, this easily results in a divergence.

**Assumptions.** To directly compare with (Ahn et al., 2023a), we focus on the cases where \(y_{0}^{2}-x_{0}^{2}=\gamma/\eta\) and \(\gamma\in[\frac{1}{2},2]\) is a constant of moderate size; hence, \(\eta\) is not too different from the \(\eta_{\text{GD}}\) defined in Theorem 3. In contrast to most prior works which assume a tiny \(\rho\) (e.g., (Wen et al., 2023)), we allow \(\rho\) to be as large as a constant (i.e., we only require \(\rho=\mathcal{O}(1)\) in Theorem 4 and Theorem 5).

#### 3.1.1 SAM Keeps Drifting Toward the Origin

We characterize the trajectory of SAM when applied to the loss defined in Equation 5 as follows:

**Theorem 4** (SAM over Single-Neuron Networks; Informal).: _For any \(\eta\in[\frac{1}{2}\eta_{\text{GD}},2\eta_{\text{GD}}]\) and \(\rho=\mathcal{O}(1)\), the SAM trajectory over the loss function \(\mathcal{L}(x,y)=\ell(xy)\) can be divided into three phases:_

1. _Initial Phase._ \(x_{t}\) _drops so rapidly that_ \(|x_{t}|=\mathcal{O}(\sqrt{\eta})\) _in_ \(\mathcal{O}(\nicefrac{{1}}{{\eta}})\) _steps. Meanwhile,_ \(y_{t}\) _remains large: specifically,_ \(y_{t}=\Omega(\sqrt{\nicefrac{{1}}{{\eta}}})\)_. Thus, SAM approaches the_ \(y\)_-axis (the set of global minima)._

Figure 4: Trajectories of different algorithms for the \(\ell(xy)\) loss (\(\eta=0.4\) and \(\rho=0.1\); initialization \((x_{0},y_{0})=(2,\sqrt{40})\) is marked by a black dot).

2. **Middle Phase.**\(x_{t}\) _oscillates closely to the axis such that_ \(|x_{t}|=\mathcal{O}(\sqrt{\eta})\) _always holds. Meanwhile,_ \(y_{t}\) _decreases fast until_ \(y_{t}\leq|x_{t}|^{2}\) _- that is,_ \(|x_{t}|\) _remains small and SAM approaches the origin._
3. **Final Phase.**\(w_{t}=(x_{t},y_{t})\) _gets close to the origin such that_ \(|x_{t}|,|y_{t}|=\mathcal{O}(\sqrt{\eta}+\eta\rho)\)_. We then show that_ \(w_{t}\) _remains in this region for the subsequent iterates._

Informally, SAM first approaches the minimizers on \(y\)-axis (which form a manifold) and then keeps moving until a specific minimum. Moreover, SAM always approaches this minimum for almost all \((\eta,\rho)\)'s. This matches our motivating experiment in Figure 2: No matter what hyper-parameters are chosen, SAM _always_ drift along the set of minima, in contrast to the behavior of GD. This property allows SAM always to approach the origin \((0,0)\) and remains in its neighborhood, while GD converges to \((0,\sqrt{\nicefrac{{2}}{{\eta}}})\) (see Table 1). The formal version of Theorem 4 is in Appendix F.

#### 3.1.2 USAAM Gets Trapped at Different Minima

We move on to characterize the dynamics of USAAM near the minima. Like GD or SAM, the first few iterations of USAAM drive iterates to the \(y\)-axis. However, unlike SAM, USAAM does not keep drifting along the \(y\)-axis and stops at some threshold - in the result below, we prove a lower bound on \(y_{t}^{2}\) that depends on both \(\eta\) and \(\rho\). In other words, the lack of normalization factor leads to diminishing drift.

**Theorem 5** (USAAM over Single-Neuron Networks; Informal).: _For any \(\eta\in[\frac{1}{2}\eta_{\text{GD}},2\eta_{\text{GD}}]\) and \(\rho=\mathcal{O}(1)\), the USAAM trajectory over the loss function \(\mathcal{L}(x,y)=\ell(xy)\) have the following properties:_

1. **Initial Phase.** _Similar to Initial Phase of Theorem_ 4_,_ \(|x_{t}|=\mathcal{O}(\sqrt{\eta})\) _and_ \(y_{t}=\Omega(\sqrt{\nicefrac{{1}}{{\eta}}})\) _hold for the first_ \(\mathcal{O}(\nicefrac{{1}}{{\eta}})\) _steps. That is, USAAM also approaches_ \(y\)_-axis, the set of global minima._
2. **Final Phase.** _However, for USAAM, once the following condition holds for some round_ \(\mathfrak{t}\)_:_3 Footnote 3: The \(\tilde{y}_{\text{USAAM}}^{2}\) in Equation 7 is defined as the solution to the equation \((1+\rho y^{2})y^{2}=2\). \[(1+\rho y_{t}^{2})y_{t}^{2}<\frac{2}{\eta},\quad\text{i.e.},\;\;y_{t}^{2}< \tilde{y}_{\text{USAAM}}^{2}\triangleq\left.\left(\sqrt{8\frac{\rho}{\eta}+1} -1\right)\middle/2\rho\right.,\] (7) \[\left.\left|x_{t}\right|\text{decays exponentially fast, which in turn ensures }y_{\infty}^{2}\triangleq\liminf_{t\to\infty}y_{t}^{2}\gtrsim(1-\eta^{2}-\rho^{2 })y_{t}^{2}.\]

**Remark.** Note that USAAM becomes GD as we send \(\rho\to 0+\), and our characterized threshold \(y_{\text{USAAM}}^{2}\) indeed recovers that of GD (i.e., \(\nicefrac{{2}}{{\eta}}\) from Theorem 3) because \(\lim_{\rho\to 0+}(\sqrt{8\nicefrac{{\rho}}{{\eta}}+1}-1)/2\rho=\nicefrac{{2}}{{ \eta}}\). Compared with SAM, the main difference occurs when close to minima, i.e., \(|x_{t}|=\mathcal{O}(\sqrt{\eta})\). Consistent with our motivating experiment (Figure 2), the removal of normalization leads to diminishing drift along the minima. Thus, USAAM is more like an improved version of GD rather than a simplification of SAM, and the comparison between Theorem 3 and Theorem 5 reveals that USAAM only improves over GD if \(\rho\) is large enough - in which case USAAM is prone to diverges as we discussed in Section 2.

See Appendix G for a formal version of Theorem 5 together with its proof.

#### 3.1.3 Technical Distinctions Between GD, SAM, and USAAM

Before moving on, we present a more technical comparison between the results stated in Theorem 3 versus Theorem 4 and Theorem 5. We start with an intuitive explanation of why GD and USAAM get stuck near the manifold of minima but SAM does not: when the iterates approach the set of minima, both \(w_{t}\) and \(\nabla\mathcal{L}(w_{t})\) become small. Hence the normalization plays an important role: as \(\nabla\mathcal{L}(w_{t})\) are small, \(w_{t}\) and \(w_{t}+\rho\nabla\mathcal{L}(w_{t})\) become nearly identical, which leads to a diminishing updates of GD and USAAM near the minima. On the other hand, having the normalization term, the SAM update doesn't diminish, which prevents SAM from converging to a minimum and keeps drifting along the manifold.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Algorithm & GD & SAM & USAAM \\ \hline Limiting Point \((0,y_{\infty})\) & \(y_{\infty}^{2}\approx\nicefrac{{2}}{{\eta}}\) & \(y_{\infty}^{2}\approx 0\) & \((1+\rho y_{\infty}^{2})y_{\infty}^{2}\approx\nicefrac{{2}}{{\eta}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Limiting points of GD, SAM, and USAAM for the \(\ell(xy)\) loss (assuming \(\eta>\eta_{\text{GD}}\)).

This high-level idea is supported by the following calculation: recall Equation 6 that \(\nabla\mathcal{L}(x_{t},y_{t})=\ell^{\prime}(x_{t}y_{t})\cdot[y_{t}\quad x_{t}]^{\top}\). Hence, when \(|x_{t}|\ll y_{t}\) in Final Phase, the "ascent gradient" direction \(\nabla\mathcal{L}(x_{t},y_{t})\) (i.e., the ascent steps in Equation 1 and Equation 2) is almost perpendicular to the \(y\)-axis. We thus rewrite the update direction (i.e., the difference between \(w_{t+1}\) and \(w_{t}\)) for each algorithm as follows.

1. For SAM, after normalization, \(\frac{\nabla\mathcal{L}(w_{t})}{\|\nabla\mathcal{L}(w_{t})\|}\) is roughly a unit vector along the \(x\)-axis. Hence, the update direction is the gradient at \(w_{t+\nicefrac{{1}}{{2}}}\approx[\rho\quad y_{t}]^{\top}\). Once \(y_{t}\) is large (making \(w_{t+\nicefrac{{1}}{{2}}}\) far from minima), \(\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\) thus have a large component along \(y_{t}\), which leads to drifting near minima.
2. For GD, by approximating \(\ell^{\prime}(u)\approx u\), we derive \(\nabla\mathcal{L}(x_{t},y_{t})\approx[x_{t}y_{t}^{2}\quad x_{t}^{2}y_{t}]^{\top}\). When \(\nicefrac{{2}}{{\eta}}>y_{t}^{2}\), the magnitude of \(x_{t}\) is updated as \(|x_{t+1}|\approx|x_{t}-\eta x_{t}y_{t}^{2}|=|(1-\eta y_{t}^{2})x_{t}|\), which allows an exponential decay. Thus, GD converges to a minimum and stop moving soon after \(\nicefrac{{2}}{{\eta}}>y_{t}^{2}\).
3. For USAAM, the descent gradient is taken at \(w_{t}+\rho\nabla\mathcal{L}(w_{t})\approx[(1+\rho y_{t}^{2})x_{t}\quad(1+ \rho x_{t}^{2})y_{t}]^{\top}\). Thus, \(\nabla\mathcal{L}(w_{t}+\rho\nabla\mathcal{L}(w_{t}))\approx[(1+\rho y_{t}^{2 })(1+\rho x_{t}^{2})^{2}x_{t}y_{t}^{2}\quad(1+\rho y_{t}^{2})^{2}(1+\rho x_{t }^{2})^{2}x_{t}y_{t}^{\top}]^{\top}\) by writing \(\ell^{\prime}(u)\approx u\). This makes USAAM deviate away from SAM and behave like GD: by the similar argument as GD, USAAM stops at a minimum soon after \(\nicefrac{{2}}{{\eta}}>(1+\rho y_{t}^{2})(1+\rho x_{t}^{2})^{2}y_{t}^{2} \approx(1+\rho y_{t}^{2})y_{t}^{2}\)!

Hence, the normalization factor in the ascent gradient helps maintain a non-diminishing component along the minima, leading SAM to keep drifting. This distinguishes SAM from GD and USAAM.

### USAAM Gets Trapped Once Close to Minima

In this section, we extend our arguments to nonconvex costs satisfying Polyak-Lojasiewicz (PL) functions (see, e.g., (Karimi et al., 2016)). Recall that \(f\) satisfies the \(\mu\)-PL condition if \(\frac{1}{2}\|\nabla\mathcal{L}(w)\|^{2}\geq\mu(\mathcal{L}(w)-\min_{w}\mathcal{ L}(w))\) for all \(w\). Building upon the analysis of Andriushchenko and Flammarion (2022), we show the following result when applying USAAM to \(\beta\)-smooth and \(\mu\)-PL losses.

**Theorem 6** (USAAM over PL Losses; Informal).: _For \(\beta\)-smooth and \(\mu\)-PL loss \(\mathcal{L}\), for any \(\eta<\nicefrac{{1}}{{\beta}}\) and \(\rho<\nicefrac{{1}}{{\beta}}\), and for any initialization \(w_{0}\), \(\|w_{t}-w_{0}\|\leq\text{poly}(\eta,\rho,\beta,\mu)\cdot\sqrt{\mathcal{L}(w_{0} )-\min_{w}\mathcal{L}(w)}\), \(\forall t\)._

This theorem has the following consequence: Suppose that USAAM encounters a point \(w_{0}\) that is close to some minimum (i.e., \(\mathcal{L}(w_{0})\approx\min_{w}\mathcal{L}(w)\)) during training. Then Theorem 6 implies that _the total distance traveled by USAAM from \(w_{0}\) is bounded_ - in other words, the distance USAAM moves along the manifold of minimizers can only be of order \(\mathcal{O}(\sqrt{\mathcal{L}(w_{0})-\min_{w}\mathcal{L}(w)})\).

As a remark, we compare Theorem 6 with the recent result by Wen et al. (2023): their result essentially implies that, for small enough \(\eta\) and \(\rho\), SAM iterates initialized close to a manifold of the minimizers approximately track some continuous dynamics (more precisely, a Riemannian gradient flow induced by a "sharpness" measure they find) and keep drifting along the manifold. This property is indeed in sharp contrast with USAAM whose total travel distance is bounded.

The formal statement and proof of Theorem 6 are contained in Appendix H.

### Experiments for Practical Neural Networking Training

We close this section by verifying our claims in practical neural network training. We train a ResNet18 on the CIFAR-10 dataset, initialized from a poor global minimum gener

Figure 5: Training ResNet18 on CIFAR-10 from a bad global minimum (\(\eta=0.001\), batch size \(=128\)).

(we used the "adversarial checkpoint" released by Damian et al. (2021)). This initialization has \(100\%\) training accuracy but only \(48\%\) test accuracy - which lets us observe a more pronounced algorithmic behavior near the minima via tracking the test accuracy. From Figure 5, we observe:

1. GD gets stuck at this adversarial minimum, in the sense that the test accuracy stays at \(48\%\).
2. SAM keeps drifting while staying close to the manifold of minimizers (because the training accuracy remains \(100\%\)), which results in better solutions (i.e., the test accuracy keeps increasing).
3. USAAM with small \(\rho\) gets stuck like GD, while USAAM with larger \(\rho\)'s deviate from this manifold.

Hence, USAAM faces the dilemma that we describe in Subsection 3.1: a conservative hyper-parameter configuration does not lead to much drift along the minima, while a more aggressive choice easily leads to divergence. However, the stability of SAM is quite robust to the choice of hyper-parameter and they all seem to lead to consistent drift along the minima.

**Remark.** Apart from the "adversarial checkpoint" which is unrealistic but can help highlight different algorithms' behavior when they are close to a bad minimum, we also conduct the same experiments but instead initialized from a "full-batch checkpoint" (Damian et al., 2021), which is the 100% training accuracy point reached by running full-batch GD on the training loss function. The result is plotted as Figure 8 in Subsection B.1. One can observe that USAAM still gets stuck at the "full-batch checkpoint", while SAM keeps increasing its test accuracy via drifting along the minima manifold.

## 4 Case Study: Learning Threshold Neurons for Sparse Coding Problem

To incorporate our two findings into a single example, we consider training one-hidden-layer ReLU networks for the sparse coding problem, a setup considered in (Ahn et al., 2023a) to study the role of \(\eta\) in GD. Without going into details, the crux of their experiment is to understand how GD with large \(\eta\) finds desired structures of the network - in this specific case, the desired structure is the negative bias in ReLU unit (also widely known as "thresholding unit/neuron"). In this section, we evaluate SAM and USAAM under the same setup, illustrating the importance of normalization.

**Main observation of Ahn et al. (2023a).** Given this background, the main observation of Ahn et al. (2023a) is that i) when training the ReLU network with GD, different learning rates induce very different trajectories; and ii) the desired structure, namely a _negative bias in ReLU, only arises with large "unstable" learning rates_ for which GD exhibits unstable behaviors. We reproduce their results in Figure 6, plotting the test accuracy on the left and the bias of ReLU unit on the right. As they claimed, GD with larger \(\eta\) learns more negative bias, which leads to better test accuracy.

Their inspiring observation is however a bit discouraging for practitioners. According to their theoretical results, such learning rates have to be quite large - large to the point where GD shows very unstable behavior (a la Edge-of-Stability (Cohen et al., 2021)). In practice, without knowledge of the problem, this requires a careful hyper-parameter search to figure out the correct learning rate. More importantly, such large and unstable learning rates may cause GD to diverge or lead to worse performance. More discussions can be found in the recent paper by Kaur et al. (2022).

In contrast, as we will justify shortly, _SAM does not suffer from such a "dilemma of tuning"_ - matching with our results in Theorem 4. Moreover, _the removal of normalization no longer attains such a property_, as we demonstrated in Theorem 5. In particular, for USAAM, one also needs to carefully tune \(\eta\) and \(\rho\) for better performance - as we inspired in Theorem 5 and Theorem 6, small \((\eta,\rho)\) makes the iterates get stuck early; on the other hand, as we presented in Section 2, an aggressive choice causes USAAM to diverge. The following experiments illustrate these claims in more detail.

In Figure 7, we plot the performance of SAM, USAAM, and GD with different \(\eta\)'s (while fixing \(\rho\)) - gray lines for GD, solid lines for SAM, and dashed lines for USAAM. From the plot, USAAM behaves more similarly to GD than SAM: the bias does not decrease sufficiently when the learning rate is not large enough, which consequently to leads poor test accuracy. On the other hand, no matter what

Figure 6: Behavior of GD for sparse coding problem.

\(\eta\) is chosen for SAM, bias is negative enough and ensures better generalization. Hence, Figure 7 illustrates that compared to SAM, USAM is less robust to the tuning of \(\eta\).

In Figure 9 (deferred to Subsection B.2), we also compare these three algorithms when varying \(\rho\) and fixing \(\eta\). In addition to what we observe in Figure 7, we show that normalization also helps stability - USAM quickly diverges as we increase \(\rho\), while SAM remains robust to the choice of \(\rho\). Thus, USAM is also less robust to the tuning of \(\rho\). In other words, our observation in Figure 7 extends to \(\rho\).

Hence, putting Figure 7 and Figure 9 together, we conclude that _SAM is robust to different configurations of \((\eta,\rho)\) while USAM is robust to neither of them_. Hence, the normalization of SAM eases hyper-parameter tuning, which is typically a tough problem for GD and many other algorithms - normalization boosts the success of SAM in practice.

## 5 Conclusion

In this paper, we investigate the role played by normalization in SAM. By theoretically characterizing the behavior of SAM and USAM on both convex and non-convex losses and empirically verifying our conclusions via real-world neural networks, we found that normalization i) helps stabilize the algorithm iterates, and ii) enables the algorithm to keep moving along the manifold of minimizers, leading to better performance in many cases. Moreover, as we demonstrate via various experiments, these two properties make SAM require less hyper-parameter tuning, supporting its practicality.

In this work, we follow a recent research paradigm of "physics-style" approaches to understanding deep neural networks based on simplified models (c.f. (Zhang et al., 2022; Garg et al., 2022; von Oswald et al., 2023; Abernethy et al., 2023; Allen-Zhu and Li, 2023; Liu et al., 2023; Li et al., 2023; Ahn et al., 2023, 2023, 2023)). We found such physics-style approaches quite helpful, especially for complex modern neural networks. We hope that our work builds stepping stones for future works on understanding working mechanisms of modern deep neural networks.

## Acknowledgments

Kwangjun Ahn was supported by the ONR grant (N00014-20-1-2394) and MIT-IBM Watson as well as a Vannevar Bush fellowship from Office of the Secretary of Defense. Suvrit Sra acknowledges support from an NSF CAREER grant (1846088), and NSF CCF-2112665 (TILOS AI Research Institute). Kwangjun Ahn also acknowledges support from the Kwanjeong Educational Foundation. We thank Kaiyue Wen, Zhiyuan Li, and Hadi Daneshmand for their insightful discussions.

Figure 7: Behaviors of different algorithms for sparse coding problem (\(\rho=0.01\)). The gray curves (corresponding to GD with different learning rates) are taken from Figure 6 with the same set of \(\eta\)â€™s.

## References

* Abernethy et al. (2023) Jacob Abernethy, Alekh Agarwal, Teodor V Marinov, and Manfred K Warmuth. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. _arXiv preprint arXiv:2305.17040_, 2023.
* Agarwala and Dauphin (2023) Atish Agarwala and Yann N Dauphin. Sam operates far from home: eigenvalue regularization as a dynamical phenomenon. _arXiv preprint arXiv:2302.08692_, 2023.
* Ahn et al. (2022) Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra. Understanding the unstable convergence of gradient descent. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 247-257. PMLR, 2022.
* Ahn et al. (2023a) Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the "edge of stability". _NeurIPS 2023 (arXiv:2212.07469)_, 2023a.
* Ahn et al. (2023b) Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. _NeurIPS 2023 (arXiv:2306.00297)_, 2023b.
* Ahn et al. (2023c) Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). _arXiv 2310.01082_, 2023c.
* Ahn et al. (2023d) Kwangjun Ahn, Ali Jadbabaie, and Suvrit Sra. How to escape sharp minima. _arXiv preprint arXiv:2305.15659_, 2023d.
* Allen-Zhu and Li (2023) Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. _arXiv preprint arXiv:2305.13673_, 2023.
* Andriushchenko and Flammarion (2022) Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In _International Conference on Machine Learning_, pages 639-668. PMLR, 2022.
* Arora et al. (2022) Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In _International Conference on Machine Learning_, pages 948-1024. PMLR, 2022.
* Bahri et al. (2022) Dara Bahri, Hossein Mobahi, and Yi Tay. Sharpness-aware minimization improves language model generalization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7360-7371, 2022.
* Bartlett et al. (2022) Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. _arXiv preprint arXiv:2210.01513_, 2022.
* Behdin and Mazumder (2023) Kayhan Behdin and Rahul Mazumder. Sharpness-aware minimization: An implicit regularization perspective. _arXiv preprint arXiv:2302.11836_, 2023.
* Blanc et al. (2020) Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In _Conference on learning theory_, pages 483-513. PMLR, 2020.
* Cohen et al. (2021) Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2021.
* Compagnoni et al. (2023) Enea Monzio Compagnoni, Antonio Orvieto, Luca Biggio, Hans Kersting, Frank Norbert Proske, and Aurelien Lucchi. An sde for modeling sam: Theory and insights. _arXiv preprint arXiv:2301.08203_, 2023.
* Damian et al. (2021) Alex Damian, Tengyu Ma, and Jason D. Lee. Label noise SGD provably prefers flat global minimizers. In _Advances in Neural Information Processing Systems_, 2021.
* Damian et al. (2023) Alex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _International Conference on Learning Representations_, 2023.
* Damian et al. (2021)Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Tan. Efficient sharpness-aware minimization for improved training of neural networks. In _International Conference on Learning Representations_, 2022.
* Dziugaite and Roy [2017] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In _Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence_, 2017.
* Foret et al. [2021] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021.
* Garg et al. [2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Jastrzkebski et al. [2017] Stanislaw Jastrzkebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. _arXiv preprint arXiv:1711.04623_, 2017.
* Jiang et al. [2020] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _8th International Conference on Learning Representations_, 2020.
* Karimi et al. [2016] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-tojasiewicz condition. In _Machine Learning and Knowledge Discovery in Databases: European Conference_, pages 795-811. Springer, 2016.
* Kaur et al. [2022] Simran Kaur, Jeremy Cohen, and Zachary C Lipton. On the maximum hessian eigenvalue and generalization. _arXiv preprint arXiv:2206.10654_, 2022.
* Keskar et al. [2017] Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017.
* Kim et al. [2023] Hoki Kim, Jinseong Park, Yujin Choi, and Jaewook Lee. Stability analysis of sharpness-aware minimization. _arXiv preprint arXiv:2301.06308_, 2023.
* Kwon et al. [2021] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* Li et al. [2018] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In _Conference On Learning Theory_, pages 2-47. PMLR, 2018.
* Li et al. [2023] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. _International Conference on Machine Learning (ICML) (arXiv:2303.04245)_, 2023.
* Liu et al. [2023] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _ICLR (arXiv:2210.10749)_, 2023.
* Liu et al. [2020] Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas. Bad global minima exist and sgd can reach them. _Advances in Neural Information Processing Systems_, 33:8543-8552, 2020.
* Liu et al. [2022] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12360-12370, 2022.
* Mi et al. [2022] Peng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. In _Advances in Neural Information Processing Systems_, 2022.
* Mi et al. [2020]Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. _Advances in neural information processing systems_, 30, 2017.
* Si and Yun (2023) Dongkuk Si and Chulhee Yun. Practical sharpness-aware minimization cannot converge all the way to optima. _NeurIPS 2023 (arXiv:2306.09850)_, 2023.
* Oswald et al. (2023) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Wang et al. (2022) Zixuan Wang, Zhouzi Li, and Jian Li. Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability. _Advances in Neural Information Processing Systems_, 35:9983-9994, 2022.
* Wen et al. (2023) Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize sharpness? In _International Conference on Learning Representations_, 2023.
* Wu et al. (2020) Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. _Advances in Neural Information Processing Systems_, 33:2958-2969, 2020.
* Zhang et al. (2022) Yi Zhang, Arturs Backurs, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. _arXiv preprint arXiv:2206.04301_, 2022.
* Zheng et al. (2021) Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8156-8165, 2021.
* Zhong et al. (2022) Qihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua Liu, Bo Du, and Dacheng Tao. Improving sharpness-aware minimization with fisher mask for better generalization on language models. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 4064-4085, 2022.
* Zhuang et al. (2022) Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, James s Duncan, Ting Liu, et al. Surrogate gap minimization improves sharpness-aware training. In _International Conference on Learning Representations_, 2022.

**Appendix**

## Appendix A Setup of the Motivating Experiment

### Additional Experimental Results

* Running SAM and USAM from Other Initializations
* Varying \(\rho\) While Fixing \(\eta\) in Sparse Coding Example
* **Omitted Proof for Smooth and Strongly Convex Losses*
* SAM Allows a Descent Lemma Like GD
* USAM Diverges on Quadratic Losses
* **Omitted Proof for Scalar Factorization Problems*
* SAM Always Converges on Scalar Factorization Problems
* USAM Diverges with Small \(\rho\)
* **Assumptions in the Single-Neuron Linear Network Model**
* **Omitted Proof of SAM Over Single-Neuron Linear Networks*
* Basic Properties and Notations
* Initial Phase: \(x_{t}\) Decreases Fast while \(y_{t}\) Remains Large
* Middle Phase: \(y\) Keeps Decreasing Until Smaller Than \(|x_{t}|\)
* Final Phase: Both \(x_{t}\) and \(y_{t}\) Oscillates Around the Origin
* **Omitted Proof of USAM Over Single-Neuron Linear Networks*
* Basic Properties and Notations
* Initial Phase: \(x_{t}\) Decreases Fast while \(y_{t}\) Remains Large
* Final Phase: \(y_{t}\) Gets Trapped Above the Origin
* **Omitted Proof of USAM Over General PL functions**Setup of the Motivating Experiment

In the motivating experiments (Figure 1 and Figure 2), we follow the over-parameterized matrix sensing setup as Li et al. (2018) and Blanc et al. (2020). Specifically, we do the following:

1. Generate the true matrix by sampling each entry of \(U^{\star}\in\mathbb{R}^{d\times r}\) independently from a standard Gaussian distribution and let \(X^{\star}=U^{\star}(U^{\star})^{\top}\).
2. Normalize each column of \(U^{\star}\) to unit norm so that the spectral norm of \(U^{\star}\) is close to one.
3. For every sensing matrix \(A_{i}\) (\(i=1,2,\ldots,m\)), sample the entries of \(A_{i}\) independently from a standard Gaussian distribution. Then observe \(b_{i}=\langle A_{i},X^{\star}\rangle\).

In particular, for the experiments, we chose \(r=5\), \(d=100\), and \(m=5dr\).

## Appendix B Additional Experimental Results

### Running SAM and USAM from Other Initializations

### Varying \(\rho\) While Fixing \(\eta\) in Sparse Coding Example

Aside from Figure 7 which varies \(\eta\) while fixing \(\rho\), we perform the same experiment when varying \(\rho\) and fixing \(\eta\). The main observation for SAM is similar to that of Figure 7: different hyper-parameters all keep decreasing the bias and give better test accuracy - even with the tiniest choice \(\rho=0.01\).

Figure 8: Training ResNet18 on CIFAR-10 from a â€œfull-batch checkpointâ€ of Damian et al. (2021) (\(\eta=0.001\), batch size \(=128\)).

Figure 9: Behaviors of different algorithms for sparse coding problem (\(\eta=0.025\)). Note that USAM with \(\rho=0.5\) (the dashed violet curve) diverges and becomes invisible except for the very beginning.

However, for USAM, there are three different types of \(\rho\)'s as shown in Figure 9:

1. For tiny \(\rho=0.01\), the bias doesn't decrease much. Consequently, the performance of USAM nearly degenerates to that of GD - while SAM with \(\rho=0.01\) still gives outstanding performance.
2. For moderate \(\rho=0.1\) and \(\rho=0.3\), USAM manages to decrease the bias and improve its accuracy, though with a slower speed than SAM with the same choices of \(\rho\).
3. For large \(\rho=0.5\) (where SAM still works well; see the solid curve in blue), USAM diverges.

Thus, the dilemma described in Section 4 indeed applies to not only \(\eta\) but also \(\rho\) - matching our main conclusion that normalization helps make hyper-parameter tuning much more manageable.

## Appendix C Omitted Proof for Smooth and Strongly Convex Losses

We shall first restate Theorem 1 here for the ease of presentation.

**Theorem 7** (Restatement of Theorem 1).: _For any \(\alpha\)-strongly-convex and \(\beta\)-smooth loss function \(\mathcal{L}\), for any learning rate \(\eta\leq\nicefrac{{2}}{{\beta}}\) and perturbation radius \(\rho\geq 0\), the following holds:_

1. _SAM. The iterate_ \(w_{t}\) _converges to a local neighborhood around the minimizer_ \(w^{\star}\)_. Formally,_ \[\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq\big{(}1-\alpha\eta(2-\eta\beta )\big{)}^{t}(\mathcal{L}(w_{0})-\mathcal{L}(w^{\star}))+\frac{\eta\beta^{3} \rho^{2}}{2\alpha(2-\eta\beta)},\quad\forall t.\]
2. _USAM. In contrast, there exists some_ \(\alpha\)_-strongly-convex and_ \(\beta\)_-smooth loss_ \(\mathcal{L}\) _such that the USAM with_ \(\eta\in\nicefrac{{2}}{{(\beta+\rho\beta^{2})}},\nicefrac{{2}}{{\beta}}\) _diverges for all except measure zero initialization_ \(w_{0}\)_._

We will show these two conclusions separately. The proof directly follows from Theorem 8 and Theorem 10.

### SAM Allows a Descent Lemma Like GD

**Theorem 8** (SAM over Strongly Convex and Smooth Losses).: _For any \(\alpha\)-strongly-convex and \(\beta\)-smooth loss function \(\mathcal{L}\), for any learning rate \(\eta\leq\nicefrac{{2}}{{\beta}}\) and perturbation radius \(\rho\geq 0\), the following holds for SAM:_

\[\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq(1-\alpha\eta(2-\eta\beta))^{t}( \mathcal{L}(w_{0})-\mathcal{L}(w^{\star}))+\frac{\eta\beta^{3}\rho^{2}}{2 \alpha(2-\eta\beta)}\,.\]

Proof.: We first claim the following analog of descent lemma, which we state as Lemma 9:

\[\mathcal{L}(w_{t+1})\leq\mathcal{L}(w_{t})-\frac{1}{2}\eta(2-\eta\beta)\| \nabla\mathcal{L}(w_{t})\|^{2}+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\,.\]

By definition of strong convexity, we have

\[\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq\langle\nabla\mathcal{L}(w_{t}),w_{t}-w^{\star}\rangle-\frac{\alpha}{2}\|w_{t}-w^{\star}\|^{2}\leq\frac{1}{2 \alpha}\|\nabla f(w_{t})\|^{2}\,,\]

where the last inequality uses the fact that \(\langle a,b\rangle-\frac{1}{2}\|b\|^{2}\leq\frac{1}{2}\|a\|^{2}\). Thus, combining the two inequalities above, we obtain

\[\mathcal{L}(w_{t+1})\leq\mathcal{L}(w_{t})-\alpha\eta(2-\eta\beta)(\mathcal{L }(w_{t})-\mathcal{L}(w^{\star}))+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\,,\]

which after rearrangement becomes

\[\mathcal{L}(w_{t+1})-\mathcal{L}(w^{\star})\leq(1-\alpha\eta(2-\eta\beta))( \mathcal{L}(w_{t})-\mathcal{L}(w^{\star}))+\frac{\eta^{2}\beta^{3}\rho^{2}}{2 }\,.\]

Unrolling this recursion, we obtain

\[\mathcal{L}(w_{t})-\mathcal{L}(w^{\star})\leq(1-\alpha\eta(2-\eta\beta))^{t} (\mathcal{L}(w_{0})-\mathcal{L}(w^{\star}))+\frac{\eta^{2}\beta^{3}\rho^{2}}{2 }\sum_{k=0}^{t-1}(1-\alpha\eta(2-\eta\beta))^{k}\]\[\leq(1-\alpha\eta(2-\eta\beta))^{t}(\mathcal{L}(w_{0})-\mathcal{L}(w^{ \star}))+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\sum_{k=0}^{\infty}(1-\alpha\eta(2- \eta\beta))^{k}\] \[=(1-\alpha\eta(2-\eta\beta))^{t}(\mathcal{L}(w_{0})-\mathcal{L}(w^ {\star}))+\frac{\eta^{2}\beta^{3}\rho^{2}}{2\alpha\eta(2-\eta\beta)}\,.\]

This completes the proof. 

**Lemma 9** (SAM Descent lemma).: _For a convex loss \(\mathcal{L}\) that is \(\beta\)-smooth, SAM iterates \(w_{t}\) satisfy the following when the learning rate \(\eta<\frac{2}{\beta}\) and \(\rho\geq 0\):_

\[\mathcal{L}(w_{t+1})\leq\mathcal{L}(w_{t})-\frac{1}{2}\eta(2-\eta\beta)|| \nabla\mathcal{L}(w_{t})\|^{2}+\frac{\eta^{2}\beta^{3}\rho^{2}}{2},\quad\forall t.\]

Proof.: Let \(v_{t}\triangleq\nabla\mathcal{L}(w_{t})/\|\nabla\mathcal{L}(w_{t})\|\) and \(w_{t+\nicefrac{{1}}{{2}}}\triangleq w_{t}+\rho v_{t}\) so we have \(w_{t+1}=w_{t}-\eta\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\). Then the \(\beta\)-smoothness of \(\mathcal{L}\) yields

\[\mathcal{L}(w_{t+1})\leq\mathcal{L}(w_{t})-\eta\langle\nabla\mathcal{L}(w_{t} ),\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\rangle+\frac{\eta^{2}\beta}{2} \|\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\|^{2}.\]

We start with upper bounding the norm of \(\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\):

\[\|\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\|^{2} =\|\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})-\nabla\mathcal{L }(w_{t})\|^{2}-\|\nabla\mathcal{L}(w_{t})\|^{2}+2\langle\nabla\mathcal{L}(w_{ t+\nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w_{t})\rangle\] \[\leq\beta^{2}\rho^{2}-\|\nabla\mathcal{L}(w_{t})\|^{2}+2\langle \nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w_{t})\rangle,\]

Hence, as long as \(\eta<\frac{2}{\beta},\) we have the following upper bound on \(\mathcal{L}(w_{t+1})\):

\[\mathcal{L}(w_{t+1}) \leq\mathcal{L}(w_{t})-\eta\langle\nabla\mathcal{L}(w_{t+ \nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w_{t})\rangle+\frac{\eta^{2}\beta}{2} \|\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}})\|^{2}\] \[\leq\mathcal{L}(w_{t})-\eta\langle\nabla\mathcal{L}(w_{t+ \nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w_{t})\rangle+\frac{\eta^{2}\beta}{2} \big{(}\beta\rho^{2}-\|\nabla\mathcal{L}(w_{t})\|^{2}+2\langle\nabla\mathcal{ L}(w_{t+\nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w_{t})\rangle\big{)}\] \[=\mathcal{L}(w_{t})-\frac{\eta^{2}\beta}{2}\|\nabla\mathcal{L}(w_ {t})\|^{2}-(\eta-\eta^{2}\beta)\langle\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2} }}),\nabla\mathcal{L}(w_{t})\rangle+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}.\]

Now we lower bound \(\big{\langle}\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}}),\nabla\mathcal{L}(w _{t})\big{\rangle}\). Note that

\[\langle\nabla\mathcal{L}(w_{t}+\rho v_{t}),\nabla\mathcal{L}(w_{t })\rangle =\langle\nabla\mathcal{L}(w_{t}+\rho v_{t})-\nabla\mathcal{L}(w_{t }),\nabla\mathcal{L}(w_{t})\rangle+\|\nabla\mathcal{L}(w_{t})\|^{2}\] \[=\frac{\|\nabla\mathcal{L}(w_{t})\|}{\rho}\langle\nabla\mathcal{L} (w_{t}+\rho v_{t})-\nabla\mathcal{L}(w_{t}),\rho v_{t}\rangle+\|\nabla \mathcal{L}(w_{t})\|^{2}\geq\|\nabla\mathcal{L}(w_{t})\|^{2}\,,\]

where the last inequality uses the following standard fact about convex functions: for any \(w_{1},w_{2}\), \(\langle\nabla\mathcal{L}(w_{1})-\nabla\mathcal{L}(w_{2}),w_{1}-w_{2}\rangle\geq 0\). Hence, we arrive at

\[\mathcal{L}(w_{t+1}) \leq\mathcal{L}(w_{t})-\frac{\eta^{2}\beta}{2}\|\nabla\mathcal{L}( w_{t})\|^{2}-(\eta-\eta^{2}\beta)\langle\nabla\mathcal{L}(w_{t+\nicefrac{{1}}{{2}}}), \nabla\mathcal{L}(w_{t})\rangle+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\] \[\leq\mathcal{L}(w_{t})-\frac{\eta^{2}\beta}{2}\|\nabla\mathcal{L}( w_{t})\|^{2}-(\eta-\eta^{2}\beta)\|\nabla\mathcal{L}(w_{t})\|^{2}+\frac{\eta^{2} \beta^{3}\rho^{2}}{2}\] \[\leq\mathcal{L}(w_{t})-\frac{1}{2}\eta(2-\eta\beta)\|\nabla \mathcal{L}(w_{t})\|^{2}+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}\,,\]

and thus finishing the proof. 

### USAAM Diverges on Quadratic Losses

**Theorem 10** (USAM over Quadratic Losses).: _Following (Bartlett et al., 2022), consider the quadratic loss \(\mathcal{L}\) induced by a PSD matrix \(\Lambda\). Without loss of generality, let \(\mathcal{L}\) minimize at the origin. Formally,_

\[\mathcal{L}(w)=\frac{1}{2}w^{\top}\Lambda w\,,\quad\text{where }\Lambda=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{d})\,,\] (8)

_where \(\lambda_{\max}=\lambda_{1}>\cdots\geq\lambda_{d}>0\) and \(v_{\max}=e_{1},e_{2},\ldots,e_{d}\) are the eigenvectors corresponding to \(\lambda_{1},\lambda_{2},\ldots,\lambda_{d}\), respectively. Then the iterates of USAAM Equation 2 applied to Equation 8 satisfy:_1. _If_ \(\eta(\lambda_{\max}+\rho\lambda_{\max}^{2})<2\)_, then the iterates converges to the global minima exponentially fast._
2. _If_ \(\eta(\lambda_{\max}+\rho\lambda_{\max}^{2})>2\) _and if_ \(\langle w_{0},v_{\max}\rangle\neq 0\)_, then the iterates diverge._

_Moreover, such a loss function \(\mathcal{L}\) is \(\lambda_{1}\)-smooth and \(\lambda_{d}\)-strongly-convex._

Proof.: As \(\nabla\ell(w)=\Lambda w\), the USAM update Equation 2 reads

\[w_{t+1}=w_{t}-\eta\nabla\mathcal{L}(w_{t}+\rho\nabla\mathcal{L}(w_{t}))= \big{(}I-\eta\Lambda-\eta\rho\Lambda^{2}\big{)}w_{t}.\]

Hence, if \(\eta(\lambda_{\max}+\rho\lambda_{\max}^{2})<2\), then since \(\big{\|}I-\eta\Lambda-\eta\rho\Lambda^{2}\big{\|}<1\), we have that \(\|w_{t}\|\to 0\) exponentially fast. On the other hand, if \(\eta(\lambda_{\max}+\rho\lambda_{\max}^{2})>2\), then \(|1-\eta\lambda_{1}-\eta\lambda_{1}^{2}|>1\). Since \(\|w_{t}\|\geq|1-\eta\lambda_{1}-\eta\lambda_{1}^{2}|^{t}|\langle w_{0},e_{1}|\rangle\), it follows that the iterates diverge as long as \(\langle w_{0},v_{\max}\rangle\neq 0\). Finally, as \(\nabla^{2}\mathcal{L}(w)=\Lambda\), we directly know that \(\mathcal{L}\) is \(\lambda_{1}\)-smooth and \(\lambda_{d}\)-strongly-convex. 

## Appendix D Omitted Proof for Scalar Factorization Problems

**Theorem 11** (Formal Version of Theorem 2).: _Consider the scalar factorization loss \(\mathcal{L}(x,y)=\frac{1}{2}(xy)^{2}\). If \((x_{0},y_{0})\) satisfies \(2x_{0}>y_{0}>x_{0}\gg 0\) and \(\eta=(x_{0}^{2}+y_{0}^{2})^{-1}\), then i) SAM finds a neighborhood of the origin with radius \(\mathcal{O}(\rho)\) for all \(\rho\), and ii) USAM diverges as long as \(\rho\geq 15\eta\)._

This theorem is a combination of Theorem 12 and Theorem 15 that we will show shortly.

### SAM Always Converges on Scalar Factorization Problems

Let \(w_{t}=[x_{t}\quad y_{t}]^{\top}\) be the \(t\)-th iterate. Denote \(\nabla_{t}=\nabla\mathcal{L}(w_{t})=[x_{t}y_{t}^{2}\quad x_{t}^{2}y_{t}]^{\top}\). For each step \(t\), the actual update of SAM is therefore the gradient taken at

\[\tilde{w}_{t}=w_{t}+\rho\frac{\nabla_{t}}{\|\nabla_{t}\|}=\begin{bmatrix}x_{ t}+\rho y_{t}z_{t}^{-1}\\ y_{t}+\rho x_{t}z_{t}^{-1}\end{bmatrix},\quad\text{where }z_{t}^{-1}=\frac{ \operatorname{sign}(x_{t}y_{t})}{\sqrt{x_{t}^{2}+y_{t}^{2}}}.\]

By denoting \(\tilde{\nabla}_{t}=\nabla\mathcal{L}(\tilde{w}_{t})\), the update rule of SAM is

\[\begin{bmatrix}x_{t+1}\\ y_{t+1}\end{bmatrix}=w_{t+1}=w_{t}-\eta\tilde{\nabla}_{t}=\begin{bmatrix}x_{ t}-\eta\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}\big{(}y_{t}+\rho x_{t}z_{t}^{-1} \big{)}^{2}\\ y_{t}-\eta\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}^{2}\big{(}y_{t}+\rho x_{t}z_{t }^{-1}\big{)}\end{bmatrix}.\]

We make the following claim:

**Theorem 12** (SAM over Scalar Factorization Problems).: _Under the setting of Theorem 11, there exists some threshold \(T\) for SAM such that \(|x_{t}|,|y_{t}|\leq 5\rho\) for all \(t\geq T\)._

Proof.: We first observe that SAM over \(\mathcal{L}(x,y)\) always pushes the iterate towards the minima (which are all the points on the \(x\)- and \(y\)- axes). Formally:

* If \(x_{t}\geq 0\), then \(x_{t+1}\leq x_{t}\). If \(x_{t}\leq 0\), then \(x_{t+1}\geq x_{t}\).
* If \(y_{t}\geq 0\), then \(y_{t+1}\leq y_{t}\). If \(y_{t}\leq 0\), then \(y_{t+1}\geq y_{t}\).

This observation can be verified by noticing \(\operatorname{sign}(\rho y_{t}z_{t}^{-1})=\operatorname{sign}(y_{t}) \operatorname{sign}(x_{t}y_{t})=\operatorname{sign}(x_{t})\) and similarly \(\operatorname{sign}(\rho x_{t}z_{t}^{-1})=\operatorname{sign}(y_{t})\). In other words, \(\operatorname{sign}(x_{t}+\rho y_{t}z_{t}^{-1})=\operatorname{sign}(x_{t})\) and thus always pushes \(x_{t}\) towards the \(y\)-axis. The same also holds for \(y_{t}\) by symmetry.

In analog to the descent lemma for GD, we can show the following lemma:

**Lemma 13**.: _When picking \(\eta=(x_{0}^{2}+y_{0}^{2})^{-1}\), we have \(\mathcal{L}(w_{t+1})-\mathcal{L}(w_{t})<0\) as long as \(x_{t},y_{t}\geq 5\rho\)._

Proof.: As \(\nabla^{2}\mathcal{L}(x,y)=\begin{bmatrix}y^{2}&2xy\\ 2xy&x^{2}\end{bmatrix}\), we know \(\mathcal{L}\) is \(\beta\triangleq(x_{0}^{2}+y_{0}^{2})\)-smooth inside the region \(\{(x,y):x^{2}+y^{2}\leq\beta\}\). Then we have (recall that \(\eta=(x_{0}^{2}+y_{0}^{2})^{-1}=\nicefrac{{1}}{{\beta}}\))

\[\mathcal{L}(w_{t+1})-\mathcal{L}(w_{t})\leq\langle\nabla\mathcal{L}(w_{t}),w_{t+ 1}-w_{t}\rangle+\frac{\beta}{2}\|w_{t+1}-w_{t}\|^{2}\]\[=-\eta\langle\nabla_{t},\tilde{\nabla}_{t}\rangle+\frac{\beta}{2}\eta^{2} \|\tilde{\nabla}_{t}\|^{2}=-\eta\Big{(}\langle\nabla_{t}-\tfrac{1}{2}\tilde{ \nabla}_{t},\tilde{\nabla}_{t}\rangle\Big{)}.\]

To make sure that it's negative, we simply want

\[0\leq\langle\nabla_{t}-\tfrac{1}{2}\tilde{\nabla}_{t},\tilde{ \nabla}_{t}\rangle =\bigg{(}x_{t}y_{t}^{2}-\frac{1}{2}\big{(}x_{t}+\rho y_{t}z_{t}^{-1 }\big{)}\big{(}y_{t}+\rho x_{t}z_{t}^{-1}\big{)}^{2}\bigg{)}\big{(}x_{t}+\rho y _{t}z_{t}^{-1}\big{)}\big{(}y_{t}+\rho x_{t}z_{t}^{-1}\big{)}^{2}+\] \[\bigg{(}x_{t}^{2}y_{t}-\frac{1}{2}\big{(}x_{t}+\rho y_{t}z_{t}^{- 1}\big{)}^{2}\big{(}y_{t}+\rho x_{t}z_{t}^{-1}\big{)}\bigg{)}\big{(}x_{t}+\rho y _{t}z_{t}^{-1}\big{)}^{2}\big{(}y_{t}+\rho x_{t}z_{t}^{-1}\big{)},\]

which can be ensured once

\[\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}\big{(}y_{t}+\rho x_{t}z_{t}^{-1} \big{)}^{2}\leq 2x_{t}y_{t}^{2},\quad\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}^{2} \big{(}y_{t}+\rho x_{t}z_{t}^{-1}\big{)}\leq 2x_{t}^{2}y_{t}.\]

If we have \(x_{t},y_{t}\geq 5\rho\), then as \(z_{t}^{-1}\leq\min\{x_{t}^{-1},y_{t}^{-1}\}\), we have

\[\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}\big{(}y_{t}+\rho x_{t}z_{t}^{-1} \big{)}^{2}\leq(x_{t}+\rho)(y_{t}+\rho)^{2}\leq 1.2^{3}x_{t}y_{t}^{2}<2x_{t}y_{t} ^{2},\]

which shows the first inequality. The second one follows from symmetry. 

Therefore, SAM will progress until \(x_{t}\leq 5\rho\) or \(y_{t}\leq 5\rho\). Without loss of generality, assume that \(x_{t}\leq 5\rho\); we then claim that \(y_{t}\) will soon decrease to \(\mathcal{O}(\rho)\).

**Lemma 14**.: _Suppose that \(|x_{t}|\leq 5\rho\) but \(y_{t}\geq 5\rho\). Then \(|x_{t+1}|\leq 5\rho\) but \(y_{t+1}\leq(1-\tfrac{1}{2}\eta\rho^{2})y_{t}\)._

Proof.: First, show that \(|x_{t}|\) remains bounded by \(5\rho\). Assume \(x_{t}\geq 0\) without loss of generality:

\[x_{t+1} =x_{t}-\eta\big{(}x_{t}+\rho y_{t}z_{t}^{-1}\big{)}\big{(}y_{t}+ \rho x_{t}z_{t}^{-1}\big{)}^{2}\] \[\geq x_{t}-\eta(5\rho+\rho)(y_{t}+\rho)^{2}\] \[\geq-\eta\xi\rho\Big{(}\frac{4}{5\eta}+\rho^{2}+2\rho\sqrt{\frac{ 4}{5\eta}}\Big{)}\geq-5\rho,\] (9)

where the second last line uses \(y_{t}^{2}\leq y_{0}^{2}=\frac{y_{t}^{2}}{x_{0}^{2}+y_{0}^{2}}\eta^{-1}\leq \frac{4}{5}\eta^{-1}\) and the last one uses \(\eta=(x_{0}^{2}+y_{0}^{2})^{-1}\ll 1\). Meanwhile, we see that \(y_{t}\) decreases exponentially fast by observing the following:

\[y_{t+1} =y_{t}-\eta(y_{t}+\rho x_{t}z_{t}^{-1})(x_{t}+\rho y_{t}z_{t}^{-1} )^{2}\] \[\leq y_{t}-\eta y_{t}(\rho y_{t}z_{t}^{-1})^{2}\] \[\leq\bigg{(}1-\frac{1}{2}\eta\rho^{2}\bigg{)}y_{t},\]

where the last line uses \(z_{t}^{-1}=(x_{t}^{2}+y_{t}^{2})^{-1/2}\geq(2y_{t}^{2})^{-1/2}=2^{-1/2}y_{t}^ {-1}\) as \(y_{t}\geq 5\rho\geq|x_{t}|\). 

So eventually we have \(|x_{t}|,|y_{t}|\leq 5\rho\). Recall that Equation 2.1.1 infers \(|x_{t+1}|\leq 5\rho\) from \(|x_{t}|\leq 5\rho\). Hence, by symmetry, we conclude that \(|x_{t+1}|,|y_{t+1}|\leq 5\rho\) hold as well. Therefore, SAM always finds an \(\mathcal{O}(\rho)\)-neighborhood of the origin, i.e., it is guaranteed to converge regardless of \(\rho\). 

### USAM Diverges with Small \(\rho\)

For USAM, the dynamics can be written as

\[\begin{bmatrix}x_{t+1}\\ y_{t+1}\end{bmatrix}=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\nabla\mathcal{L}\begin{pmatrix}x_{t}+\rho x_{t}y_{t}^{ 2}\\ y_{t}+\rho x_{t}^{2}y_{t}\end{pmatrix}=\begin{bmatrix}x_{t}-\eta(x_{t}+\rho x_{t }y_{t}^{2})(y_{t}+\rho x_{t}^{2}y_{t})^{2}\\ y_{t}-\eta(x_{t}+\rho x_{t}y_{t}^{2})^{2}(y_{t}+\rho x_{t}^{2}y_{t})\end{bmatrix}.\] (10)

We make the following claim which is similar to Theorem 10:

**Theorem 15** (USAM over Scalar Factorization Problems).: _Under the setup of Theorem 11, for any \(\rho\geq 15\eta\), \(|x_{t+1}|\geq 2|x_{t}|\) and \(|y_{t+1}|\geq 2|y_{t}|\) for all \(t\geq 1\); in other words, USAM diverges exponentially fast._Proof.: Prove by induction. From Equation 10, our conclusion follows once

\[\eta\big{|}(x_{t}+\rho x_{t}y_{t}^{2})(y_{t}+\rho x_{t}^{2}y_{t})^{2}\big{|}\geq 3 |x_{t}|,\quad\eta\big{|}(x_{t}+\rho x_{t}y_{t}^{2})^{2}(y_{t}+\rho x_{t}^{2}y_{t} )\big{|}\geq 3|y_{t}|.\]

According to our setup that \(\eta=(x_{0}^{2}+y_{0}^{2})^{-1}\), \(y_{0}\leq 2x_{0}\), and the induction statement that, we have \(\eta\geq(5x_{t}^{2})^{-1}\). The second inequality then holds as long as

\[\big{|}(\rho x_{t}y_{t}^{2})^{2}(\rho x_{t}^{2}y_{t})\big{|}\geq 15x_{t}^{2}|y_{t }|,\quad\text{i.e., }\rho^{3}x_{t}^{2}y_{t}^{4}\geq 15,\]

which is true as \(x_{t}^{2}\geq x_{0}^{2}\), \(y_{t}^{4}\geq y_{0}^{4}\), \(y_{0}\geq x_{0}\), and \(\rho\geq 15\eta\geq 3x_{0}^{-2}\). Note that the bounds on \(\rho\) are very loose, and we made no effort to optimize it; instead, we only aimed to show that USAM starts to diverge from a \(\rho=\Theta(\eta)\ll 1\). 

## Appendix E Assumptions in the Single-Neuron Linear Network Model

Assumptions on \(\ell\). Following Ahn et al. (2023), we make the following assumptions about \(\ell\):

1. \(\ell\) is a convex, even, \(1\)-Lipschitz function that is minimized at \(0\).
2. \(\ell\) is twice continuously differentiable near the origin with \(\ell^{\prime\prime}(0)=1\), which infers the existence of a constant \(c>0\) such that \(|\ell^{\prime}(s)|\leq|s|\) for all \(|s|\leq c\).
3. We further assume a "linear tail" away from the minima, i.e., \(|\ell^{\prime}(s)|\geq c/2\) for all \(|s|\geq c\) and \(|\ell^{\prime}(s)|\geq|s|/2\) for \(|s|\leq c\).

Some concrete example of loss functions satisfying the above assumption include a symmetrized logistic loss \(\frac{1}{2}\log(1+\exp(-2s))+\frac{1}{2}\log(1+\exp(2s))\) and a square root loss \(\sqrt{1+s^{2}}\) One may refer to their paper for more details.

## Appendix F Omitted Proof of SAM Over Single-Neuron Linear Networks

**Theorem 16** (Formal Version of Theorem 4).: _For a loss \(\mathcal{L}\) over \((x,y)\in\mathbb{R}^{2}\) defined as \(\mathcal{L}(x,y)=\ell(xy)\) where \(\ell\) satisfies Assumptions (A1), (A2), and (A3), if the initialization \((x_{0},y_{0})\) satisfies:_

\[y_{0}\geq x_{0}\gg 0,\quad y_{0}^{2}-x_{0}^{2}=\frac{\gamma}{\eta},\quad y_{0}^ {2}=C\frac{\gamma}{\eta},\]

_where \(\gamma\in[\frac{1}{2},2]\) and \(C\geq 1\) is constant, then for all hyper-parameter configurations \((\eta,\rho)\) such that4_

Footnote 4: As \(y_{0}\gg 0\), we must have \(\eta\ll 1\); thus, these conditions automatically hold when \(\eta=o(1)\) and \(\rho=\mathcal{O}(1)\).

\[\eta\rho+\sqrt{C\gamma\eta}\leq\min\biggl{\{}\frac{1}{2},C\biggr{\}}\sqrt{ \frac{\gamma}{\eta}},\quad\frac{4\sqrt{C}}{c}\eta^{-1}=\mathcal{O}(\min\{\eta^ {-1.5}\rho^{-1}\gamma^{1/2},\eta^{-2}\}),\]

_we can decompose the trajectory of SAM (defined in Equation 1) into three phases, whose main conclusions are stated separately in three theorems and are informally summarized here:_

1. _(**Theorem 20**) Until_ \(x_{t}=\mathcal{O}(\sqrt{\gamma\eta})\)_, we must have_ \(y_{t}=\Omega(\sqrt{\gamma/\eta})\)_, and_ \(x_{t+1}\leq x_{t}-\Omega\big{(}\sqrt{\gamma\eta})\)_._
2. _(**Theorem 26**) After Initial Phase and until_ \(y_{t}\leq|x_{t}|\)_,_ \(|x_{t}|=\mathcal{O}(\eta\rho+\sqrt{\eta})\) _still holds. Meanwhile,_ \(y_{t+1}\leq y_{t}-\min\{\Omega(\eta\rho^{2})y_{t},\Omega(\eta\rho)\}\) _(i.e.,_ \(y_{t}\) _either drops by_ \(\Omega(\eta\rho)\) _or decays by_ \(\Omega(\eta\rho^{2})\)_)._
3. _(**Theorem 32**) After Middle Phase, we always have_ \(|x_{t}|,|y_{t}|=\mathcal{O}(\eta\rho+\sqrt{\eta})\)_._

### Basic Properties and Notations

Recall the update rule of SAM in Equation 1: \(w_{t+1}\gets w_{t}-\eta\nabla\mathcal{L}(w_{t}+\rho\frac{\nabla\mathcal{L }(w_{t})}{\|\nabla\mathcal{L}(w_{t})\|})\). By writing \(w_{t}\) as \(\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}\), substituting \(\mathcal{L}(x,y)=\ell(xy)\), and utilizing the expressions of \(\nabla\mathcal{L}(x,y)\) in Equation 6, we have:

\[w_{t}+\rho\frac{\nabla\mathcal{L}(w_{t})}{\|\nabla\mathcal{L}(w_{t})\|}= \begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}+\rho\frac{\ell^{\prime}(x_{t}y_{t})}{|\ell^{\prime}(x_{t}y_{t })|\sqrt{x_{t}^{2}+y_{t}^{2}}}\begin{bmatrix}y_{t}\\ x_{t}\end{bmatrix}=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}+\rho\frac{\operatorname{sign}(x_{t}y_{t})}{\sqrt{x_{t}^{2}+y_ {t}^{2}}}\begin{bmatrix}y_{t}\\ x_{t}\end{bmatrix},\] (11)where the second step uses the fact that \(\ell\) is a even function so \(\ell^{\prime}(t)\) has the same sign with \(t\). Define \(z_{t}=\frac{\sqrt{x_{t}^{2}+y_{t}^{2}}}{\operatorname{sign}(x_{t}y_{t})}\). Then \(w_{t}+\rho\frac{\nabla\mathcal{L}(w_{t})}{\|\nabla\mathcal{L}(w_{t})\|}= \begin{bmatrix}x_{t}+\rho y_{t}z_{t}^{-1}\\ y_{t}+\rho x_{t}z_{t}^{-1}\end{bmatrix}\). Further denoting \(\ell^{\prime}\big{(}(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+\rho x_{t}z_{t}^{-1}) \big{)}\) by \(\ell^{\prime}_{t}\), one can simplify the update rule Equation 1 as follows:

\[w_{t+1} =\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\ell^{\prime}\big{(}(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+ \rho x_{t}z_{t}^{-1})\big{)}\begin{bmatrix}y_{t}+\rho x_{t}z_{t}^{-1}\\ x_{t}+\rho y_{t}z_{t}^{-1}\end{bmatrix}\] \[=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\ell^{\prime}_{t}\begin{bmatrix}y_{t}+\rho x_{t}z_{t}^ {-1}\\ x_{t}+\rho y_{t}z_{t}^{-1}\end{bmatrix}.\] (12)

**Assumption**.: Following footnote 1, we assume that \(x_{t},y_{t}\neq 0\) for all \(t\).

We are ready to give some basic properties of Equation 12. First, we claim that the sign of \(\ell^{\prime}_{t}\) is the same as the product \(x_{t}y_{t}\). Formally, we have the following lemma:

**Lemma 17** (Sign of Gradient in SAM).: _If \(x_{t}\neq 0,y_{t}\neq 0\), then \(\operatorname{sign}(x_{t})=\operatorname{sign}(x_{t}+\rho y_{t}z_{t}^{-1})\) and \(\operatorname{sign}(y_{t})=\operatorname{sign}(y_{t}+\rho x_{t}z_{t}^{-1})\). In particular, if \(x_{t}\neq 0,y_{t}\neq 0\), we must have \(\operatorname{sign}(\ell^{\prime}_{t})=\operatorname{sign}(x_{t}y_{t})\)._

Proof.: Note that \(\operatorname{sign}(z_{t})=\operatorname{sign}(x_{t}y_{t})\), so \(\operatorname{sign}(y_{t}z_{t}^{-1})=\operatorname{sign}(x_{t})\). Similarly, \(\operatorname{sign}(x_{t}z_{t}^{-1})=\operatorname{sign}(y_{t})\). Thus, we have \(\operatorname{sign}(x_{t})=\operatorname{sign}(x_{t}+\rho y_{t}z_{t}^{-1})\) and \(\operatorname{sign}(y_{t})=\operatorname{sign}(y_{t}+\rho x_{t}z_{t}^{-1})\), which in turn shows that \(\operatorname{sign}(\ell^{\prime}_{t})=\operatorname{sign}\bigl{(} \operatorname{sign}(x_{t}+\rho y_{t}z_{t}^{-1})\operatorname{sign}(y_{t}+\rho x _{t}z_{t}^{-1})\bigr{)}=\operatorname{sign}(x_{t}y_{t})\). 

Using Lemma 17, the SAM update can be equivalently written as

\[\begin{bmatrix}x_{t+1}\\ y_{t+1}\end{bmatrix}=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\ell^{\prime}_{t}\begin{bmatrix}y_{t}\\ x_{t}\end{bmatrix}-\eta|\ell^{\prime}_{t}|\begin{bmatrix}\rho x_{t}(x_{t}^{2}+y _{t}^{2})^{-\nicefrac{{1}}{{2}}}\\ \rho y_{t}(x_{t}^{2}+y_{t}^{2})^{-\nicefrac{{1}}{{2}}}\end{bmatrix}.\] (13)

We then claim the following property, which can be intuitively interpolated as SAM always goes towards the minimizes (i.e., both axes) for each step, although sometimes it may overshoot.

**Lemma 18**.: _Suppose that \(x_{t},y_{t}\neq 0\). Then, the following basic properties hold:_

* _If_ \(x_{t}>0\)_, then_ \(x_{t+1}<x_{t}\)_. If_ \(x_{t}<0\)_, then_ \(x_{t+1}>x_{t}\)_._
* _If_ \(y_{t}>0\)_, then_ \(y_{t+1}<y_{t}\)_. If_ \(y_{t}<0\)_, then_ \(y_{t+1}>y_{t}\)_._

Proof.: We only show the case that \(x_{t}>0\) as the other cases are similar. Recall the dynamics of \(x_{t}\):

\[x_{t+1}=x_{t}-\eta\ell^{\prime}_{t}\cdot\rho z_{t}^{-1}x_{t}-\eta\ell^{\prime} _{t}\cdot y_{t}\,.\]

Since \(x_{t}\neq 0\), Lemma 17 implies that \(\operatorname{sign}(\ell^{\prime}_{t})=\operatorname{sign}(x_{t}y_{t})\), and so

\[x_{t+1} =x_{t}-\eta\ell^{\prime}_{t}\cdot\rho z_{t}^{-1}x_{t}-\eta\ell^{ \prime}_{t}\cdot y_{t}\] \[=x_{t}-\eta\operatorname{sign}(x_{t}y_{t})|\ell^{\prime}_{t}| \cdot\rho\operatorname{sign}(x_{t}y_{t})(x_{t}^{2}+y_{t}^{2})^{-\nicefrac{{1}} {{2}}}x_{t}-\eta\operatorname{sign}(x_{t}y_{t})|\ell^{\prime}_{t}|\cdot \operatorname{sign}(y_{t})|y_{t}|\] \[=x_{t}-\eta|\ell^{\prime}_{t}|\cdot\rho(x_{t}^{2}+y_{t}^{2})^{- \nicefrac{{1}}{{2}}}x_{t}-\eta|\ell^{\prime}_{t}|\cdot|y_{t}|\leq x_{t},\]

where the last line uses the assumption that \(x_{t}>0\). 

### Initial Phase: \(x_{t}\) Decreases Fast while \(y_{t}\) Remains Large

As advertised in Theorem 16, we group all rounds until \(x_{t}\leq\frac{c}{4}\sqrt{\gamma\eta}\) as Initial Phase. Formally:

**Definition 19** (Initial Phase).: _Let \(t_{1}\) be the largest time such that \(x_{t}>\frac{c}{4}\sqrt{\gamma\eta}\) for all \(t\leq t_{1}\). We call the iterations \([0,t_{1}]\)**Initial Phase**._

**Theorem 20** (Main Conclusion of Initial Phase; SAM Case).: _For \(t_{0}=\Theta(\min\{\eta^{-1.5}\rho^{-1}\gamma^{\nicefrac{{1}}{{2}}},\eta^{-2}\})\), we have \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for all \(t\leq\min\{t_{0},t_{1}\}\) under the conditions of Theorem 16. Moreover, we have \(x_{t+1}\leq x_{t}-\frac{c}{4}\sqrt{\gamma\eta}\) for all \(t\leq\min\{t_{0},t_{1}\}\), which consequently infers \(t_{1}\leq t_{0}\) under the conditions of Theorem 16, i.e., \(\min\{t_{0},t_{1}\}\) is just \(t_{1}\). This shows the first claim of Theorem 16._

**Remark**.: This theorem can be intuitively explained as follows: At initialization, \(x_{0},y_{0}\) are both \(\Theta(\sqrt{1/\eta})\). However, after \(t_{1}\) iterations, we have \(|x_{t_{1}+1}|=\mathcal{O}(\sqrt{\eta})\); meanwhile, \(y_{t}\) is still of order \(\Omega(\sqrt{1/\eta})\) (much larger than \(\mathcal{O}(\sqrt{\eta})\)). Hence, \(|x_{t}|\) and \(|y_{t}|\) get widely separated in Initial Phase.

Proof.: This theorem is a direct consequence of Lemma 21 and Lemma 24. 

**Lemma 21**.: _There exists \(t_{0}=\Theta(\min\{\eta^{-1.5}\rho^{-1}\gamma^{\nicefrac{{1}}{{2}}},\eta^{-2}\})\) such that \(|y_{t}|\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for \(t\leq t_{0}.\) If \(\eta\) is sufficiently small s.t. \(\eta\rho+\sqrt{C\gamma\eta}\leq\frac{1}{2}\sqrt{\gamma/\eta}\), then \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for \(t\leq\min\{t_{1},t_{0}\}.\)_

Proof.: Let \(t_{0}\) be defined as follows:

\[t_{0}\triangleq\max\!\left\{t\ :\ \left(1-\frac{8}{\sqrt{C}}\eta^{1.5}\gamma^{ -\nicefrac{{1}}{{2}}}\rho-\eta^{2}\right)^{t}\geq\frac{1}{4}\right\}.\] (14)

Then, it follows that \(t_{0}=\Theta(\min\{\eta^{-1.5}\rho^{-1}\gamma^{\nicefrac{{1}}{{2}}},\eta^{- 2}\})\). We first prove the following claim.

**Claim 22**.: For all \(t\leq t_{0}\), \(y_{t}^{2}\geq\frac{1}{4}\gamma/\eta\).

Proof.: We prove this claim by induction. We trivially have \(y_{0}^{2}=C\gamma/\eta\geq\frac{1}{4}C\gamma/\eta\). Now suppose that the conclusion holds up to some \(t<t_{0}\), i.e., \(y_{t^{\prime}}^{2}\geq\frac{1}{4}\gamma/\eta\) for all \(t^{\prime}\leq t\). We consider \(y_{t+1}^{2}-x_{t+1}^{2}\):

\[y_{t+1}^{2}-x_{t+1}^{2} =(1-\eta\ell_{t}^{\prime}\cdot\rho z_{t}^{-1})^{2}(y_{t}^{2}-x_{t }^{2})-(\eta\ell_{t}^{\prime})^{2}(y_{t}^{2}-x_{t}^{2})\] \[=(1-2\eta\ell_{t}^{\prime}\cdot\rho z_{t}^{-1}+\eta^{2}(\ell_{t}^ {\prime}\cdot\rho z_{t}^{-1})^{2}-\eta^{2}(\ell_{t}^{\prime})^{2})\cdot(y_{t}^ {2}-x_{t}^{2})\] \[\stackrel{{(a)}}{{\geq}}(1-2\eta\rho z_{t}^{-1}-\eta ^{2})\cdot(y_{t}^{2}-x_{t}^{2})\] \[\geq(1-2\eta\rho y_{t}^{-1}-\eta^{2})\cdot(y_{t}^{2}-x_{t}^{2})\] \[\stackrel{{(b)}}{{\geq}}(1-\frac{8}{\sqrt{C}}\eta^{1. 5}\gamma^{-\nicefrac{{1}}{{2}}}\rho-\eta^{2})\cdot(y_{t}^{2}-x_{t}^{2}),\]

where (a) used \(\ell_{t}^{\prime}\leq 1\) (thanks to Assumption (A1)) and (b) used the induction hypothesis \(y_{t}^{2}\geq\frac{1}{4}C\gamma/\eta\). This further implies that

\[y_{t+1}^{2}-x_{t+1}^{2}\geq\left(1-\frac{8}{\sqrt{C}}\eta^{1.5}\gamma^{- \nicefrac{{1}}{{2}}}\rho-\eta^{2}\right)^{t+1}(y_{0}^{2}-x_{0}^{2})=\left(1- \frac{8}{\sqrt{C}}\eta^{1.5}\gamma^{-\nicefrac{{1}}{{2}}}\rho-\eta^{2}\right) ^{t+1}\frac{\gamma}{\eta}.\]

Thus, by the definition of \(t_{0}\), we must have \(y_{t+1}^{2}\geq y_{t+1}^{2}-x_{t+1}^{2}\geq\frac{1}{4}\gamma/\eta\), which proves the claim. 

Next, we prove the second conclusion.

**Claim 23**.: If \(\eta\rho+\sqrt{C\gamma\eta}\leq\frac{1}{2}\sqrt{\gamma/\eta}\), then \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for \(t\leq\min\{t_{0},t_{1}\}\).

Proof.: Still show by induction. Let \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}>0\) for some \(t<\min\{t_{0},t_{1}\}\). Consider \(y_{t+1}\).

By Definition 19, \(x_{t}\) is positive for all \(t\leq t_{1}\). Thus, using Lemma 18, we have \(x_{t}\leq x_{0}\leq y_{0}=\sqrt{C\gamma/\eta}\). Since \(x_{t},y_{t}>0\), we have \(\operatorname{sign}(\ell_{t}^{\prime})=\operatorname{sign}(x_{t}y_{t})= \operatorname{sign}(x_{t})>0\) and hence (13) gives

\[y_{t+1} =y_{t}-\eta|\ell_{t}^{\prime}|\rho(x_{t}^{2}+y_{t}^{2})^{- \nicefrac{{1}}{{2}}}|y_{t}|-\eta|\ell_{t}^{\prime}||x_{t}|\] \[\geq y_{t}-\eta\rho-\eta|x_{t}|,\]

where we used the facts that \(|\ell_{t}^{\prime}|\leq 1\) and \((x_{t}^{2}+y_{t}^{2})^{-\nicefrac{{1}}{{2}}}\leq\min\{|x_{t}|^{-1},|y_{t}|^{- 1}\}\). Hence, we get

\[y_{t+1}\geq y_{t}-\eta\rho-\sqrt{C\gamma\eta}\geq 0\]

since \(\eta\rho+\sqrt{C\gamma\eta}\leq\frac{1}{2}\sqrt{\gamma/\eta}\leq y_{t}\). By Claim 22, \(y_{t+1}\geq 0\) implies \(y_{t+1}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) as well. 

Combining Claim 22 and Claim 23 finishes the proof of Lemma 21. 

**Lemma 24**.: _For any \(t\leq\min\{t_{0},t_{1}\}\), we have_

\[x_{t+1}\leq x_{t}-\frac{c}{4}\sqrt{\gamma\eta}\,.\]

_In particular, if \(\eta\) is sufficiently small s.t. \(\frac{4\sqrt{C}}{c}\eta^{-1}\leq t_{0}\), then we must have \(t_{1}\leq\frac{4}{c}\eta^{-1}-1<t_{0}\)._Proof.: Since \(x_{t}>0\) for all \(t\leq t_{1}\), we have \(\operatorname{sign}(\ell_{t}^{\prime})=\operatorname{sign}(x_{t}y_{t})= \operatorname{sign}(y_{t})\) from Lemma 17 and Lemma 21, so the SAM update (13) becomes

\[x_{t+1}=x_{t}-\eta|\ell_{t}^{\prime}|\cdot\rho(x_{t}^{2}+y_{t}^{2})^{-\nicefrac{{ 1}}{{2}}}x_{t}-\eta|\ell_{t}^{\prime}|\cdot|y_{t}|.\]

Since \(x_{t}>2c\sqrt{\frac{\eta}{C\gamma}}\), we have

\[(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+\rho x_{t}z_{t}^{-1})>x_{t}y_{t}\geq 2c\sqrt{ \frac{\eta}{\gamma}}\cdot\frac{1}{2}\sqrt{\frac{\gamma}{\eta}}\geq c\,,\]

which implies that \(\ell_{t}^{\prime}\geq c/2\) from Assumption (A3). Together with Lemma 21, we have

\[x_{t+1} =x_{t}-\eta|\ell_{t}^{\prime}|\cdot\rho(x_{t}^{2}+y_{t}^{2})^{- \nicefrac{{1}}{{2}}}z_{t}^{-1}x_{t}-\eta|\ell_{t}^{\prime}|\cdot|y_{t}|\] \[\leq x_{t}-\eta|\ell_{t}^{\prime}|\cdot|y_{t}|\leq x_{t}-\eta \frac{c}{2}\frac{1}{2}\sqrt{\frac{\gamma}{\eta}}=x_{t}-\frac{c}{4}\sqrt{\gamma \eta}.\]

Let \(t_{1}^{\prime}\triangleq\frac{\sqrt{C\gamma/\eta}}{\frac{c}{4}\sqrt{\gamma \eta}}=\frac{4\sqrt{C}}{c}\eta^{-1}\). Since \(x_{0}\leq\sqrt{C\gamma/\eta}\), we have \(x_{t_{1}^{\prime}}<\frac{c}{4}\sqrt{\gamma\eta}\) as long as \(t_{1}\leq t_{0}\). Thus, it follows that \(t_{1}\leq t_{1}^{\prime}-1<t_{0}\), as desired. 

### Middle Phase: \(y\) Keeps Decreasing Until Smaller Than \(|x_{t}|\)

Then we move on to the second claim of Theorem 16. We define all rounds until \(y_{t}<|x_{t}|\) that are after Initial Phase as Middle Phase. Formally, we have the following definition.

**Definition 25** (Middle Phase).: _Let \(t_{2}\) be the first time that \(y_{t}<|x_{t}|\). We call \((t_{1},t_{2}]\)**Middle Phase**._

Before presenting the main conclusion of Middle Phase, we first define a threshold \(B\) that we measure whether a variable is "small enough", which we will use throughout this section. Formally,

\[B\triangleq\max\biggl{\{}\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\biggr{\}}.\]

**Theorem 26** (Main Conclusion of Middle Phase; SAM Case).: _Under the conditions of Theorem 16, \(|y_{t}|\leq\sqrt{C\gamma/\eta}\) and \(|x_{t}|\leq B\) throughout Middle Phase. Under the same conditions, we further have \(y_{t+1}\leq y_{t}-\min\{\frac{1}{2}\eta\rho^{2}y_{t},\frac{c}{2\sqrt{2}}\eta\rho\}\) for all \(t_{1}\leq t\leq t_{2}\) - showing the second claim of Theorem 16._

**Remark**.: This theorem can be understood as follows. Upon entering Middle Phase, \(|x_{t}|\) is bounded by \(\mathcal{O}(\eta\rho+\sqrt{\eta})\). This then gets preserved throughout Middle Phase. Meanwhile, in \(t_{2}\) iterations, \(y_{t}\) drops rapidly such that \(y_{t_{2}+1}\leq|x_{t_{2}+1}|=\mathcal{O}(\eta\rho+\sqrt{\eta})\). In other words, \(x_{t}\) and \(y_{t}\) are both "close enough" to 0 after Middle Phase; thus, SAM finds the flattest minimum (which is the origin).

Proof.: The claims are shown in Lemma 27, Lemma 28 and Lemma 30, respectively. 

**Lemma 27**.: _If \(\eta\rho+\sqrt{C\gamma\eta}<\sqrt{C\gamma/\eta}\), then during the Middle Phase, \(|y_{t}|\leq y_{0}\leq\sqrt{C\gamma/\eta}\)._

Proof.: When \(\operatorname{sign}(y_{t+1})=\operatorname{sign}(y_{t})\), then Lemma 18 implies that \(|y_{t+1}|\leq|y_{t}|\). Now consider the case \(\operatorname{sign}(y_{t+1})=-\operatorname{sign}(y_{t})\). For simplicity, say \(y_{t}>0\) and \(y_{t+1}<0\). Since \(y_{t}>0\), we have \(\operatorname{sign}(\ell_{t}^{\prime})=\operatorname{sign}(x_{t}y_{t})= \operatorname{sign}(x_{t})\) and hence Equation 13 gives

\[y_{t+1} =y_{t}-\eta|\ell_{t}^{\prime}|\rho(x_{t}^{2}+y_{t}^{2})^{- \nicefrac{{1}}{{2}}}|y_{t}|-\eta|\ell_{t}^{\prime}||x_{t}|\] \[\geq-\eta\rho-\eta|x_{t}|\geq-\eta\rho-\eta y_{t}\] \[\geq-\eta\rho-\eta\sqrt{\frac{C\gamma}{\eta}}=-\eta\rho-\sqrt{C \gamma\eta}\,,\]

which shows that \(|y_{t+1}|\leq\eta\rho+\sqrt{C\gamma\eta}\). This proves the statement. 

**Lemma 28**.: _Suppose that \(|x_{t}|\leq B=\max\Bigl{\{}\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\Bigr{\}}\). Then we have \(|x_{t+1}|\leq B\)._Proof.: By symmetry, we only consider the case where \(x_{t}>0\). If \(x_{t+1}\geq 0\), then we have \(0\leq x_{t+1}\leq x_{t}\) due to Lemma 24. If \(x_{t+1}<0\), then since \(x_{t}>0\), it follows that

\[x_{t+1} =\mathrm{sign}(x_{t})x_{t+1}=|x_{t}|-\eta|\ell_{t}^{\prime}|\rho(x _{t}^{2}+y_{t}^{2})^{\nicefrac{{-1}}{{2}}}|x_{t}|-\eta|\ell_{t}^{\prime}||y_{t}|\] \[\geq|x_{t}|-\eta\rho-\eta\sqrt{C\gamma/\eta}\geq-\eta\rho-\sqrt{ C\eta\gamma}\,,\] (15)

where the last inequality is because \(|y_{t}|\leq y_{0}\leq\sqrt{C\gamma/\eta}\). This concludes the proof. 

**Corollary 29**.: _Note that, by definition of Initial Phase in Definition 19, we already have \(|x_{t_{1}+1}|\leq B\). Hence, this lemma essentially says \(|x_{t}|\leq B=\max\Bigl{\{}\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\Bigr{\}}\) for all \(t_{1}<t\leq t_{2}\)._

**Lemma 30**.: _If \(y_{t}\geq|x_{t}|\), then we must have_

\[y_{t+1}\leq y_{t}-\min\biggl{\{}\frac{1}{2}\eta\rho^{2}y_{t},\ \ \frac{c}{2\sqrt{2}}\eta\rho\biggr{\}}\,.\]

Proof.: Since \(y_{t}\geq|x_{t}|\), we have \((x_{t}^{2}+y_{t}^{2})^{\nicefrac{{-1}}{{2}}}\geq(2y_{t}^{2})^{\nicefrac{{-1 }}{{2}}}=(\sqrt{2}y_{t})^{-1}\). Consider two cases:

1. if \(|(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+\rho x_{t}z_{t}^{-1})|\geq c\), then \(|\ell_{t}^{\prime}|\in[\frac{c}{2},1]\) according to Assumption (A3). Moreover, since \(\mathrm{sign}(\ell_{t}^{\prime})=\mathrm{sign}(x_{t}y_{t})=\mathrm{sign}(x_{t})\), it follows that \[y_{t+1} =y_{t}-\eta|\ell_{t}^{\prime}|\cdot\rho(x_{t}^{2}+y_{t}^{2})^{ \nicefrac{{-1}}{{2}}}y_{t}-\eta\ell_{t}^{\prime}\cdot x_{t}\] \[=\Bigl{(}1-\eta|\ell_{t}^{\prime}|\rho(x_{t}^{2}+y_{t}^{2})^{ \nicefrac{{-1}}{{2}}}\Bigr{)}y_{t}-\eta|\ell_{t}^{\prime}||x_{t}|\] \[\leq\Bigl{(}1-\eta c_{t}\rho(\sqrt{2}y_{t})^{-1}\Bigr{)}y_{t}\] \[\leq y_{t}-\eta\rho\frac{c}{2}\frac{1}{\sqrt{2}}=y_{t}-\Omega( \eta\rho).\] (16)
2. otherwise, it follows from (A3) that \[|\ell_{t}^{\prime}|\geq\frac{1}{2}|(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+\rho x _{t}z_{t}^{-1})|\,.\] As \(\mathrm{sign}(x_{t})=\mathrm{sign}(\rho y_{t}z_{t}^{-1})\) and \(\mathrm{sign}(y_{t})=\mathrm{sign}(\rho x_{t}z_{t}^{-1})\), it follows that \[|\ell_{t}^{\prime}|\cdot\rho z_{t}^{-1}y_{t} \geq\frac{1}{2}|(x_{t}+\rho y_{t}z_{t}^{-1})(y_{t}+\rho x_{t}z_{t }^{-1})|\cdot\rho z_{t}^{-1}y_{t}\] \[\geq\frac{1}{2}(\rho y_{t}z_{t}^{-1})^{2}y_{t}\] we must have \[y_{t+1} \leq y_{t}-\frac{1}{2}\eta(\rho y_{t}z_{t}^{-1})^{2}(y_{t})\leq y _{t}-\frac{1}{2}\eta\rho^{2}y_{t}=(1-\Omega(\eta\rho^{2}))y_{t},\] (17) where we used \(z_{t}^{-1}\geq(\sqrt{2}y_{t})^{-1}\) and thus \(y_{t}z_{t}^{-1}\geq 1/\sqrt{2}\).

Combining item 16 and item 17, we obtain the desired conclusion. 

### Final Phase: Both \(x_{t}\) and \(y_{t}\) Oscillates Around the Origin

It only remains to show that the iteration never escapes the origin.

**Definition 31** (Final Phase).: _We denote by "Final Phase" all iterations after \(t_{2}\)._

**Theorem 32** (Main Conclusion of Final Phase; SAM Case).: _For all \(t>t_{2}\), we have \(|x_{t}|,|y_{t}|\leq B\) where \(B=\max\Bigl{\{}\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\Bigr{\}}\)._

**Remark.** As we will see shortly, when entering Final Phase, both \(|x_{t}|\) and \(|y_{t}|\) are bounded by \(\mathcal{O}(\eta\rho+\sqrt{\eta})\). This theorem essentially says that they can never exceed this bound in Final Phase. In other words, in Final Phase, both \(x_{t}\) and \(y_{t}\) are oscillating around \(0\).

Proof.: We first show the following lemma, ruling out the possibility of \(|y_{t}|>B\) after Middle Phase:

**Lemma 33**.: _If \(\eta\rho+\eta B\leq B\),5 then \(|y_{t_{2}+1}|\leq B=\max\Bigl{\{}\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\Bigr{\}}\)._

Footnote 5: Recall that \(B=\max\{\frac{2}{c}\sqrt{\frac{\eta}{C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\}\), we only need to ensure that \(\eta B\leq\sqrt{C\eta\gamma}\), which can be done by \(\eta\leq\min\{cC\gamma,\frac{1}{4}C\gamma\rho^{-2},\frac{1}{2}\}\). As the RHS is of order \(\Omega(1)\), \(\eta=\mathcal{O}(1)\) again suffices.

Proof.: The proof will be very similar to Lemma 28. Recall that by Definition 25, we must have \(y_{t_{2}}>0\). If \(y_{t_{2}+1}>0\) as well, then \(|y_{t_{2}+1}|=y_{t_{2}+1}\leq|x_{t_{2}+1}|\leq B\). Otherwise,

\[y_{t_{2}+1}=\operatorname{sign}(y_{t_{2}})y_{t_{2}+1}=|y_{t_{2}}|-\eta|\ell^ {\prime}_{t_{2}}|\rho(x_{t_{2}}^{2}+y_{t_{2}}^{2})^{-\nicefrac{{1}}{{2}}}|y_ {t_{2}}|-\eta|\ell^{\prime}_{t_{2}}||x_{t_{2}}|\geq-\eta\rho-\eta B,\]

where we used \(|x_{t_{2}}|\leq B\). We proved our claim as \(\eta\rho+\eta B\leq B\). 

According to Lemma 28 Lemma 33, we have

\[|x_{t_{2}+1}|,|y_{t_{2}+1}|\leq B=\max\biggl{\{}\frac{2}{c}\sqrt{\frac{\eta}{ C\gamma}},\ \eta\rho+\sqrt{C\eta\gamma}\biggr{\}}.\] (18)

Hence, it only remains to do a reduction, stated as follows.

**Lemma 34**.: _If \(\eta\rho+\eta B\leq B\), then \(|x_{t+1}|,|y_{t+1}|\leq B\) as long as \(|x_{t}|,|y_{t}|\leq B\)._

Proof.: Without loss of generality, let \(x_{t}>0\) for some \(t>t_{2}\) and show that \(|x_{t+1}|\leq B\) - which is identical to the proof of Lemma 27. By symmetry, the same also holds for \(|x_{t}|\). 

Our conclusion thus follows from an induction based on Equation 18 and Lemma 34. 

## Appendix G Omitted Proof of USAM Over Single-Neuron Linear Networks

**Theorem 35** (Formal Version of Theorem 5).: _For a loss \(\mathcal{L}\) over \((x,y)\in\mathbb{R}^{2}\) defined as \(\mathcal{L}(x,y)=\ell(xy)\) where \(\ell\) satisfies Assumptions (A1), (A2), and (A3), if the initialization \((x_{0},y_{0})\) satisfies:_

\[y_{0}\geq x_{0}\gg 0,\quad y_{0}^{2}-x_{0}^{2}=\frac{\gamma}{\eta},\quad y_{0}^ {2}=C\frac{\gamma}{\eta},\]

_where \(\gamma\in[\frac{1}{2},2]\) and \(C\geq 1\) is constant, then for all hyper-parameter configurations \((\eta,\rho)\) such that6_

Footnote 6: Similar to Theorem 16, these conditions are satisfied once \(\eta=o(1)\) and \(\rho=\mathcal{O}(1)\).

\[\eta\leq\frac{1}{2},\quad\eta\rho\leq\min\biggl{\{}\frac{1}{2},C^{-1}\gamma^{- 1}\biggr{\}},\quad 2\sqrt{C}\eta^{-1}=\mathcal{O}(\min\{\eta^{-1}\rho^{-1},\eta^{-2}\}), \quad C\gamma\biggl{(}1+\rho C\frac{\gamma}{\eta}\biggr{)}\geq 16,\]

_we can characterize the initial and final phases of the trajectory of USAM (defined in Equation 2) by the following two theorems, whose main conclusions are informally summarized below:_

1. **(Theorem 39)** _Until_ \(x_{t}=\mathcal{O}(\sqrt{\gamma\eta})\)_, we must have_ \(y_{t}=\Omega(\sqrt{\nicefrac{{\gamma}}{{\eta}}})\)_, and_ \(x_{t+1}\leq x_{t}-\Omega(\sqrt{\gamma\eta})\)_._
2. **(Theorem 44)** _Once \((1+\rho y_{t}^{2})y_{t}^{2}\lesssim\nicefrac{{2}}{{\eta}}\), \(|x_{t}|\) decays exponentially and thus USAM gets stuck._

Different from Theorem 16, there is no characterization of Middle Phase here, which means the iterates can also stop above the threshold \(\tilde{y}_{\text{USAM}}^{2}\) defined in Equation 7 (the technical reason is sketched in subsubsection 3.1.3, i.e., SAM gradients non non-negligible when \(y_{t}\) is large, while USAM gradients vanish once \(|x_{t}|\) is small). However, we remark that the main takeaway of Theorem 35 is to contrast SAM (which always attains \(y_{\infty}^{2}\approx 0\)) with USAM (which must stop once \((1+\rho y_{t}^{2})y_{t}^{2}\lesssim\nicefrac{{2}}{{\eta}}\)).

### Basic Properties and Notations

Recall the update rule of USAM in Equation 2: \(w_{t+1}\gets w_{t}-\eta\nabla\mathcal{L}(w_{t}+\rho\nabla\mathcal{L}(w_{t}))\). Still writing \(w_{t}\) as \([x_{t}\quad y_{t}]^{\top}\), we have

\[w_{t+1}=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\nabla\mathcal{L}\biggl{(}\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}+\rho\ell^{\prime}(x_{t}y_{t})\begin{bmatrix}y_{t}\\ x_{t}\end{bmatrix}\biggr{)}\]\[=\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}-\eta\ell^{\prime}\big{(}(x_{t}+\rho\ell^{\prime}(x_{t}y_{t})y_{t })(y_{t}+\rho\ell^{\prime}(x_{t}y_{t})x_{t})\big{)}\begin{bmatrix}y_{t}+\rho\ell ^{\prime}(x_{t}y_{t})x_{t}\\ x_{t}+\rho\ell^{\prime}(x_{t}y_{t})y_{t}\end{bmatrix}.\] (19)

Due to the removal of normalization, \(\ell^{\prime}\) are taken twice at different points in Equation 19. For simplicity, we denote \(\tilde{\ell}^{\prime}_{t}=\ell^{\prime}(x_{t}y_{t})\) and \(\ell^{\prime}_{t}=\ell^{\prime}((x_{t}+\rho\tilde{\ell}^{\prime}_{t}y_{t})(y_ {t}+\rho\tilde{\ell}^{\prime}_{t}x_{t}))\). The update rule can be rewritten as:

\[x_{t+1}=x_{t}-\eta\ell^{\prime}_{t}\cdot(y_{t}+\rho\tilde{\ell}^{\prime}_{t}x_ {t}),\quad y_{t+1}=y_{t}-\eta\ell^{\prime}_{t}\cdot(x_{t}+\rho\tilde{\ell}^{ \prime}_{t}y_{t}).\] (20)

Similar to the SAM case, we shall approximate \(\tilde{\ell}^{\prime}_{t}\) and \(\ell^{\prime}_{t}\) according to the magnitude of \(x_{t}y_{t}\) and \((x_{t}+\rho\tilde{\ell}^{\prime}_{t}y_{t})(y_{t}+\rho\tilde{\ell}^{\prime}_{t} x_{t})\). We first have the following lemma similar to Lemma 17:

**Lemma 36** (Sign of Gradient in USAM).: _If \(x_{t}\neq 0,y_{t}\neq 0\), then \(\operatorname{sign}(x_{t})=\operatorname{sign}(x_{t}+\rho\tilde{\ell}^{ \prime}_{t}y_{t})\) and \(\operatorname{sign}(y_{t})=\operatorname{sign}(y_{t}+\rho\tilde{\ell}^{ \prime}_{t}x_{t})\). In particular, if \(x_{t}\neq 0,y_{t}\neq 0\), then \(\operatorname{sign}(\ell^{\prime}_{t})=\operatorname{sign}(\tilde{\ell}^{ \prime}_{t})=\operatorname{sign}(x_{t}y_{t})\)._

Proof.: First of all, \(\operatorname{sign}(\tilde{\ell}^{\prime}_{t})=\operatorname{sign}(x_{t}y_{t})\) according to Assumption (A1). Therefore,

\[\operatorname{sign}(x_{t}+\rho\tilde{\ell}^{\prime}_{t}y_{t}) =\operatorname{sign}(x_{t}+\rho\operatorname{sign}(x_{t}y_{t})y_{t })=\operatorname{sign}(x_{t}),\] \[\operatorname{sign}(y_{t}+\rho\tilde{\ell}^{\prime}_{t}x_{t}) =\operatorname{sign}(y_{t}+\rho\operatorname{sign}(x_{t}y_{t})x_{t })=\operatorname{sign}(y_{t}),\]

giving our first two claims. The last conclusion follows by definition. 

Moreover, we also have the following lemma analog to Lemma 18:

**Lemma 37**.: _Suppose that \(x_{t},y_{t}\geq 0\). Then, the following basic properties hold:_

* _If_ \(x_{t}>0\)_, then_ \(x_{t+1}<x_{t}\)_. If_ \(x_{t}<0\)_, then_ \(x_{t+1}>x_{t}\)_._
* _If_ \(y_{t}>0\)_, then_ \(y_{t+1}<y_{t}\)_. If_ \(y_{t}<0\)_, then_ \(y_{t+1}>y_{t}\)_._

Proof.: We only prove the statement for \(x_{t}>0\) as the proof is similar for other cases. Recall the dynamics of \(x_{t}\):

\[x_{t+1}=x_{t}-\eta\ell^{\prime}_{t}\cdot(y_{t}+\rho\tilde{\ell}^{\prime}_{t}x_ {t}).\]

Since \(x_{t}\neq 0\), Lemma 36 implies that \(\operatorname{sign}(\ell^{\prime}_{t})=\operatorname{sign}(\tilde{\ell}^{ \prime}_{t})=\operatorname{sign}(x_{t}y_{t})\), and so

\[x_{t+1} =x_{t}-\eta\ell^{\prime}_{t}\cdot(y_{t}+\rho\tilde{\ell}^{\prime}_ {t}x_{t})\] \[=x_{t}-\eta\operatorname{sign}(x_{t}y_{t})|\ell^{\prime}_{t}| \cdot(y_{t}+\rho\operatorname{sign}(x_{t}y_{t})|\tilde{\ell}^{\prime}_{t}|x_{t})\] \[=x_{t}-\eta|\ell^{\prime}_{t}|\cdot(|y_{t}|+\rho|\tilde{\ell}^{ \prime}_{t}|x_{t})\leq x_{t},\]

where the last line uses the assumption that \(x_{t}>0\). 

### Initial Phase: \(x_{t}\) Decreases Fast while \(y_{t}\) Remains Large

The definition of Initial Phase is very similar to the one of SAM - and the conclusion is also analogue, although the proofs are slightly different because of the different update rule.

**Definition 38** (Initial Phase).: _Let \(t_{1}\) be the largest time such that \(x_{t}>\frac{1}{2}\sqrt{\gamma\eta}\) for all \(t\leq t_{1}\). We denote by "Initial Phase" the iterations \([0,t_{1}]\)._

**Theorem 39** (Main Conclusion of Initial Phase; USAM Case).: _For \(t_{0}=\Theta(\min\{\eta^{-1}\rho^{-1},\eta^{-2}\})\), we have \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for all \(t\leq\min\{t_{0},t_{1}\}\) under the conditions of Theorem 35. Moreover, we have \(x_{t+1}\leq x_{t}-\frac{1}{2}\sqrt{C\gamma\eta}\) for all \(t\leq\min\{t_{0},t_{1}\}\), which consequently infers \(t_{1}\leq t_{0}\) under the conditions of Theorem 35, i.e., \(\min\{t_{0},t_{1}\}\) is just \(t_{1}\). This shows the first claim of Theorem 35._

Proof.: This theorem is a combination of Lemma 40 and Lemma 43. 

Similar to Lemma 21, \(y_{t}\) in USAM also cannot be too small in the first few iterations:

**Lemma 40**.: _There exists \(t_{0}=\Theta(\min\{\eta^{-1}\rho^{-1},\eta^{-2}\})\) such that for \(t\leq t_{0}\), we have \(|y_{t}|\geq\frac{1}{2}\sqrt{\gamma/\eta}\). Assuming \(\eta\leq\frac{1}{2}\), then \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for \(t\leq\min\{t_{1},t_{0}\}\)._Proof.: The proof idea follows from Lemma 21. Let \(t_{0}\) be defined as follows:

\[t_{0}\triangleq\max\biggl{\{}t\ :\ \bigl{(}1-\eta^{2}-2\eta\rho\bigr{)}^{t}\geq \frac{1}{4}\biggr{\}}\,.\] (21)

Then, it follows that \(t_{0}=\Theta(\min\{\eta^{-1}\rho^{-1},\eta^{-2}\})\). We first show that \(y_{t}^{2}\) cannot be too small until \(t_{0}\):

**Claim 41**.: For all \(t\leq t_{0}\), \(y_{t}^{2}\geq\frac{1}{4}\gamma/\eta\).

Proof.: Prove by induction. Initially, \(y_{0}^{2}=C\gamma/\eta\geq\frac{1}{4}\gamma/\eta\). Then consider some \(t<t_{0}\) such that \(y_{t^{\prime}}^{2}\geq\frac{1}{4}\gamma/\eta\) for all \(t^{\prime}\leq t\). By Equation 20, we have

\[y_{t+1}^{2}-x_{t+1}^{2} =\Bigl{(}y_{t}-\eta\ell_{t}^{\prime}\cdot(x_{t}+\rho\tilde{\ell }_{t}^{\prime}y_{t})\Bigr{)}^{2}-\Bigl{(}x_{t}-\eta\ell_{t}^{\prime}\cdot(y_{t }+\rho\tilde{\ell}_{t}^{\prime}x_{t})\Bigr{)}^{2}\] \[=(y_{t}^{2}-x_{t}^{2})+(\eta\ell_{t}^{\prime})^{2}\Bigl{(}(x_{t}+ \rho\tilde{\ell}_{t}^{\prime}y_{t})^{2}-(y_{t}+\rho\tilde{\ell}_{t}^{\prime}x _{t})^{2}\Bigr{)}+\] \[\quad 2x_{t}\cdot\eta\ell_{t}^{\prime}\cdot(y_{t}+\rho\tilde{\ell }_{t}^{\prime}x_{t})-2y_{t}\cdot\eta\ell_{t}^{\prime}\cdot(x_{t}+\rho\tilde{ \ell}_{t}^{\prime}y_{t})\] \[=(y_{t}^{2}-x_{t}^{2})+(\eta\ell_{t}^{\prime})^{2}(x_{t}^{2}-y_{t }^{2})+(\eta\ell_{t}^{\prime})^{2}(\rho\tilde{\ell}_{t}^{\prime})^{2}(y_{t}^{2 }-x_{t}^{2})+2\eta\ell_{t}^{\prime}\cdot\rho\tilde{\ell}_{t}^{\prime}(x_{t}^{2 }-y_{t}^{2})\] \[=\Bigl{(}1-(\eta\ell_{t}^{\prime})^{2}+(\eta\ell_{t}^{\prime})^{2 }(\rho\tilde{\ell}_{t}^{\prime})^{2}-2\eta\ell_{t}^{\prime}\cdot\rho\tilde{ \ell}_{t}^{\prime}\Bigr{)}(y_{t}^{2}-x_{t}^{2}).\]

Using Assumption (A1), we know that \(|\ell_{t}^{\prime}|\leq 1\) and \(|\tilde{\ell}_{t}^{\prime}|\leq 1\), giving

\[y_{t+1}^{2}-x_{t+1}^{2}\geq\bigl{(}1-\eta^{2}-2\eta\rho\bigr{)}(y_{t}^{2}-x_{t }^{2})\geq\cdots\geq\bigl{(}1-\eta^{2}-2\eta\rho\bigr{)}^{t+1}(y_{0}^{2}-x_{0} ^{2}).\]

By definition of \(t_{0}\) and condition that \(t<t_{0}\), we have \(y_{t+1}^{2}\geq y_{t+1}^{2}-x_{t+1}^{2}\geq\frac{1}{4}\gamma/\eta\). 

**Claim 42**.: Suppose that \(\sqrt{C\gamma\eta}\leq\frac{1}{2}\sqrt{\gamma/\eta}\), i.e., \(\eta\leq\frac{1}{2}\sqrt{C}\). Then \(y_{t}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for all \(t\leq t_{0}\).

Proof.: We still consider the maximum single-step difference in \(y_{t}\). Suppose that for some \(t^{\prime}<\min\{t_{0},t_{1}\}\), \(y_{t^{\prime}}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) for all \(t^{\prime}\leq t\). According to Equation 20 and Assumption (A1),

\[y_{t+1}=y_{t}-\eta\ell_{t}^{\prime}\cdot(x_{t}+\rho\tilde{\ell}_{t}^{\prime}y_ {t})\geq y_{t}-\eta(x_{t}+\rho y_{t})\geq(1-\eta\rho)y_{t}-\eta\sqrt{C\gamma/ \eta},\]

where the last inequality used Lemma 37 to conclude that \(x_{t}\leq x_{0}\leq y_{0}=\sqrt{C\gamma/\eta}\). Hence, as the first term is non-negative and \(\sqrt{C\gamma\eta}\leq\frac{1}{2}\sqrt{\gamma/\eta}\), we must have \(y_{t+1}\geq 0\), which implies \(y_{t+1}\geq\frac{1}{2}\sqrt{\gamma/\eta}\) according to Claim 41. 

Putting these two claims together gives our conclusion. Note that the condition \(\eta\leq\frac{1}{2}\sqrt{C}\) in Claim 42 can be inferred from the assumptions \(\eta\leq\frac{1}{2}\) and \(C\geq 1\) made in Theorem 35. 

After showing that \(y_{t}\) never becomes too small, we are ready to show that \(x_{t}\) decreases fast enough.

**Lemma 43**.: _For \(t\leq\min\{t_{0},t_{1}\}\), we have_

\[x_{t+1}\leq x_{t}-\frac{1}{2}\sqrt{\gamma\eta}\,.\]

_In particular, if \(\eta\) is sufficiently small s.t. \(2\sqrt{C}\eta^{-1}\leq t_{0}\), then we must have \(t_{1}\leq 2\sqrt{C}\eta^{-1}-1<t_{0}\)._

Proof.: Since \(x_{t}>0\) for all \(t\leq t_{1}\), we have \(\operatorname{sign}(\ell_{t}^{\prime})=\operatorname{sign}(x_{t}y_{t})= \operatorname{sign}(y_{t})\) from Lemma 36 Lemma 40, so the USAM update (20) simplifies as follows according to Assumption (A1) and Lemma 40

\[x_{t+1}=x_{t}-\eta|\ell_{t}^{\prime}|\cdot(|y_{t}|+\rho|\ell_{t}^{\prime}||x_{t }|)\leq x_{t}-\eta|y_{t}|\leq x_{t}-\frac{1}{2}\sqrt{\gamma\eta}.\]

Let \(t_{1}^{\prime}=\frac{\sqrt{C\gamma/\eta}}{\frac{1}{2}\sqrt{\gamma\eta}}=2\sqrt{C }\eta^{-1}\), then \(t_{1}^{\prime}-1<t_{0}\) and thus \(x_{t_{1}^{\prime}}<\frac{1}{2}\sqrt{\gamma\eta}\), i.e., \(t_{1}<t_{0}\) must holds.

### Final Phase: \(y_{t}\) Gets Trapped Above the Origin

Now, we are going to consider the final-stage behavior of USAM.

**Theorem 44** (Main Conclusion of Final Phase; USAM Case).: _After Initial Phase, we always have \(|x_{t}|\leq\sqrt{C\gamma\eta}\) and \(|y_{t}|\leq\sqrt{C\gamma/\eta}\). Moreover, once we have \(\eta(1+\rho y_{t}^{2})y_{t}^{2}=2-\epsilon\) for some \(\mathfrak{t}\geq t_{1}\) and \(\epsilon>0\), we must have \(|x_{t+1}|\leq\exp(-\Omega(\epsilon))|x_{t}|\) for all \(t\geq\mathfrak{t}\). This consequently infers that \(y_{\infty}^{2}\), which is defined as \(\liminf_{t\to\infty}\frac{y_{t}^{2}}{\eta_{t}^{2}}\), satisfies \(y_{\infty}^{2}\geq(1-4C\gamma(\eta+\rho C\gamma)^{2}\epsilon^{-1})y_{t}^{2}\). As \(\epsilon\) is a constant independent of \(\eta\) and \(\rho\) and can be arbitrarily close to \(0\), this shows the second claim of Theorem 35._

Proof.: The first conclusion is an analog of Lemma 28 and Lemma 27 (which allows a simpler analysis thanks to the removal of normalization), as we will show in Lemma 45. The second part requires a similar (but much more sophisticated) analysis to Lemma 15 of Ahn et al. (2023), which we will cover in Lemma 47. 

**Lemma 45**.: _Suppose that \(|x_{t}|\leq\sqrt{C\gamma\eta}\), \(|y_{t}|\leq\sqrt{C\gamma/\eta}\). Assuming \(\eta\rho\leq 1\), then \(|x_{t+1}|\leq\sqrt{C\gamma\eta}\) and \(|y_{t+1}|\leq\sqrt{C\gamma/\eta}\) as well. Furthermore, \(|x_{t}|\leq\sqrt{C\gamma\eta}\) and \(|y_{t}|\leq\sqrt{C\gamma/\eta}\) hold for all \(t>t_{1}\)._

Proof.: Suppose that \(x_{t}\geq 0\) without loss of generality. The case where \(x_{t+1}\geq 0\) is trivial by Lemma 37. Otherwise, using Lemma 36 and Equation 20, we can write

\[x_{t+1}=x_{t}-\eta\ell_{t}^{\prime}\cdot(y_{t}+\rho\tilde{\ell}_{t}^{\prime}x_ {t})=x_{t}-\eta|\ell_{t}^{\prime}|\cdot(|y_{t}|+\rho|\tilde{\ell}_{t}^{\prime}| x_{t}).\]

By Assumption (A1), \(|\ell_{t}^{\prime}|\) and \(|\tilde{\ell}_{t}^{\prime}|\) are bounded by \(1\). By the condition that \(\eta\rho\leq 1\),

\[x_{t+1}\geq(1-\eta\rho)x_{t}-\eta|y_{t}|\geq-\sqrt{C\gamma\eta},\]

where we used \(|y_{t}|\leq sqrtC\gamma/\eta\). Similarly, for \(y_{t+1}\), only considering the case where \(y_{t}\geq 0\) and \(y_{t+1}\leq 0\) suffices. We have the following by symmetry

\[y_{t+1}=y_{t}-\eta\ell_{t}^{\prime}\cdot(x_{t}+\rho\tilde{\ell}_{t}^{\prime}y _{t})\geq y_{t}-\eta(|x_{t}|+\rho y_{t})=(1-\eta\rho)y_{t}-\eta|x_{t}|\geq- \eta^{1.5}\sqrt{C\gamma}\geq-\sqrt{C\gamma/\eta},\]

where we used \(|x_{t}|\leq\sqrt{C\gamma\eta}\).

The second part of the conclusion is done by induction. According to the definition of Initial Phase, we have \(|x_{t_{1}+1}|\leq\frac{1}{2}\sqrt{\gamma\eta}\). As \(C\geq 1\geq\frac{1}{4}\), we consequently have \(|x_{t}|\leq\sqrt{C\gamma\eta}\) for all \(t>t_{1}\) from the first part of the conclusion. Regarding \(|y_{t}|\), recall Lemma 40 infers \(y_{t}>0\) for all \(t\leq t_{1}+1\) and Lemma 37 infers the monotonicity of \(y_{t}\), we have \(y_{t_{1}+1}\leq y_{0}=\sqrt{C\gamma/\eta}\). Hence, \(|y_{t}|\leq\sqrt{C\gamma/\eta}\) for all \(t>t_{1}\) as well. 

Before showing the ultimate conclusion Lemma 47, we first show the following single-step lemma:

**Lemma 46**.: _Suppose that \(\eta(1+\rho y_{t}^{2})y_{t}^{2}<2\) and \(|x_{t}|\leq\sqrt{C\gamma\eta}\) for some \(t\). Define \(\epsilon_{t}=2-\eta(1+\rho y_{t}^{2})y_{t}^{2}\) (then we must have \(\epsilon_{t}\in(0,2)\)). Then we have_

\[|x_{t+1}|\leq|x_{t}|\exp\biggl{(}-\min\biggl{\{}(1+\rho C\gamma\eta)\epsilon_{t}, \frac{2-\epsilon_{t}}{8}\biggr{\}}\biggr{)}.\] (22)

Proof.: Without loss of generality, assume that \(x_{t}>0\). From Equation 20, we can write:

\[x_{t+1} =x_{t}-\eta\ell_{t}^{\prime}\cdot(y_{t}+\rho\tilde{\ell}_{t}^{ \prime}x_{t})=x_{t}-\eta|\ell_{t}^{\prime}|\cdot(|y_{t}|+\rho|\tilde{\ell}_{t}^ {\prime}|x_{t}),\]

where we used \(\mathrm{sign}(\ell_{t}^{\prime})=\mathrm{sign}(\tilde{\ell}_{t}^{\prime})= \mathrm{sign}(x_{t}y_{t})=\mathrm{sign}(y_{t})\) (Lemma 36). By Assumption (A2),

\[x_{t+1} \geq x_{t}-\eta\Big{|}\bigl{(}x_{t}+\rho\tilde{\ell}_{t}^{\prime} y_{t}\bigr{)}\bigl{(}y_{t}+\rho\tilde{\ell}_{t}^{\prime}x_{t}\bigr{)}\Big{|} \bigl{(}|y_{t}|+\rho|x_{t}y_{t}|x_{t}\bigr{)}\] \[=x_{t}-\eta\bigl{(}x_{t}+\rho|\tilde{\ell}_{t}^{\prime}||y_{t}| \bigr{)}\bigl{(}|y_{t}|+\rho|\tilde{\ell}_{t}^{\prime}|x_{t}\bigr{)}\bigl{(}|y_{t }|+\rho x_{t}^{2}|y_{t}|\bigr{)}\] \[\geq x_{t}-\eta(x_{t}+\rho x_{t}y_{t}^{2})(|y_{t}|+\rho x_{t}^{2}| y_{t}|)\bigl{(}|y_{t}|+\rho x_{t}^{2}|y_{t}|\bigr{)}\] \[=\bigl{(}1-\eta(1+\rho y_{t}^{2})(1+\rho x_{t}^{2})^{2}y_{t}^{2} \bigr{)}x_{t}\] \[\geq\bigl{(}1-(1+\rho C\gamma\eta)\eta(1+\rho y_{t}^{2})y_{t}^{2} \bigr{)}x_{t}\] \[\geq-(1-(1+\rho C\gamma\eta)\epsilon_{t})x_{t},\] (23)where we used \(|x_{t}|\leq\sqrt{C\gamma\eta}\). For the other direction, we have the following by Assumption (A3):

\[x_{t+1} \leq x_{t}-\eta\frac{|(x_{t}+\rho\tilde{\ell}_{t}^{\prime}y_{t})(y_{ t}+\rho\tilde{\ell}_{t}^{\prime}x_{t})|}{2}\bigg{(}|y_{t}|+\rho\frac{|x_{t}y_{t}| }{2}x_{t}\bigg{)}\] \[=x_{t}-\eta\frac{(x_{t}+\rho|\tilde{\ell}_{t}^{\prime}|y_{t})(|y_ {t}|+\rho|\tilde{\ell}_{t}^{\prime}|x_{t})}{2}\bigg{(}|y_{t}|+\rho\frac{x_{t}^{ 2}}{2}|y_{t}|\bigg{)}\] \[\leq x_{t}-\eta\frac{(x_{t}+\rho x_{t}y_{t}^{2})(|y_{t}|+\rho|y_{ t}|x_{t}^{2})}{8}\bigg{(}|y_{t}|+\rho\frac{x_{t}^{2}}{2}|y_{t}|\bigg{)}\] \[=\bigg{(}1-\frac{\eta}{8}(1+\rho y_{t}^{2})(1+\rho x_{t}^{2}) \bigg{(}1+\rho\frac{x_{t}^{2}}{2}\bigg{)}y_{t}^{2}\bigg{)}x_{t}\] \[\leq\Big{(}1-\frac{\eta}{8}(1+\rho y_{t}^{2})y_{t}^{2}\Big{)}x_{t }=\bigg{(}1-\frac{2-\epsilon}{8}\bigg{)}x_{t},\] (24)

where we used \((1+\rho x_{t}^{2})\geq 1\) and \((1+\rho\frac{x_{t}^{2}}{2})\geq 1\). Equation 22 follows from Equation 23 and Equation 24. 

**Lemma 47**.: _Let \(\mathfrak{t}\) be such that i) \(\eta(1+\rho y_{t}^{2})y_{t}^{2}=2-\epsilon\) where \(\epsilon\in(0,\frac{2}{9})\) is a constant, and ii) \(|x_{t}|\leq\sqrt{C\gamma\eta}\). Then we have the following conclusion on \(\liminf_{t\to\infty}y_{t}^{2}\), denoted by \(y_{\infty}^{2}\) in short:_

\[y_{\infty}^{2}=\liminf_{t\to\infty}y_{t}^{2}\geq\big{(}1-4C\gamma(\eta+\rho C \gamma)^{2}\epsilon^{-1}\big{)}y_{t}^{2}.\]

While the \(\epsilon^{-1}\) looks enormous, it is a constant independent of \(\eta\) and \(\rho\); in other words, we are allowed to set \(\epsilon\) as close to zero as we want. As we only consider the dependency on \(\eta\) and \(\rho\), we can abbreviate this conclusion as \(y_{\infty}^{2}\geq(1-\mathcal{O}(\eta^{2}+\rho^{2}))y_{t}^{2}\), as we claim in the main text.

Proof.: In analog to Equation 23, we derive the following for \(y_{t}\):

\[y_{t+1}^{2}\geq\bigg{(}1-\eta\big{(}1+\rho y_{t}^{2}\big{)}^{2}(1+\rho x_{t}^ {2})x_{t}^{2}\bigg{)}^{2}y_{t}^{2}\geq\bigg{(}1-2\eta(1+\rho C\gamma/\eta)^{2} (1+\rho C\gamma\eta)x_{t}^{2}\bigg{)}y_{t}^{2},\]

where the second inequality uses Lemma 45. Let \(d_{t}=y_{t}^{2}-y_{t}^{2}\), then we have

\[d_{t+1}\leq d_{t}+2\eta(1+\rho C\gamma/\eta)^{2}(1+\rho C\gamma\eta)x_{t}^{2}y _{t}^{2}\leq d_{t}+4\eta^{-1}(\eta+\rho C\gamma)^{2}y_{t}^{2}x_{t}^{2},\]

where we used the assumption that \(\rho C\gamma\eta\leq 1\) and the fact that \(y_{t}^{2}\) is monotonic (thus \(y_{t}^{2}\leq y_{t}^{2}\)). According to Equation 22, we have \(|x_{t+1}|\leq|x_{t}|\exp(-\Omega(\epsilon))\) for all \(t\geq t_{2}\). Hence, we have

\[d_{\infty}=\limsup_{t\to\infty}d_{t}\leq d_{\mathfrak{t}}+\sum_{t=\mathfrak{t }}^{\infty}4\eta^{-1}(\eta+\rho C\gamma)^{2}y_{\mathfrak{t}}^{2}x_{t}^{2}=4\eta ^{-1}(\eta+\rho C\gamma)^{2}y_{\mathfrak{t}}^{2}\cdot\epsilon^{-1}x_{\mathfrak{ t}}^{2},\]

where we used \(d_{\mathfrak{t}}=0\) (by definition) and the sum of geometric series. Plugging back \(x_{\mathfrak{t}}^{2}\leq C\gamma\eta\),

\[y_{\infty}^{2}=\liminf_{t\to\infty}y_{t}^{2}\geq y_{\mathfrak{t}}^{2}-4C\gamma (\eta+\rho C\gamma)^{2}y_{\mathfrak{t}}^{2},\]

as claimed. 

## Appendix H Omitted Proof of USAM Over General PL functions

**Theorem 48** (Formal Version of Theorem 6).: _For any \(\mu\)-PL and \(\beta\)-smooth loss function \(\mathcal{L}\), for any learning rate \(\eta<\nicefrac{{1}}{{\beta}}\) and \(\rho<\nicefrac{{1}}{{\beta}}\), for any initialization \(w_{0}\), the following holds for USAM:_

\[\|w_{t}-w_{0}\|\leq\eta(1+\beta\rho)\sqrt{\frac{2\beta^{2}}{\mu}}\bigg{(}1-2 \mu\eta(1-\rho\beta)\bigg{(}\frac{\eta\beta}{2}(1-\rho\beta)\bigg{)}\bigg{)}^{ \nicefrac{{-1}}{{2}}}\sqrt{\mathcal{L}(w_{0})-\mathcal{L}^{*}},\quad\forall t \geq 0,\]

_where \(\mathcal{L}^{*}\) is the short-hand notation for \(\min_{w}\mathcal{L}(w)\)._

We first state the following useful result by Andriushchenko and Flammarion (2022, Theorem 10).

**Lemma 49** (Descent Lemma of USAM over Smooth and PL Losses).: _For any \(\beta\)-smooth and \(\mu\)-PL loss function \(\mathcal{L}\), for any learning rate \(\eta<\nicefrac{{1}}{{\beta}}\) and \(\rho<\nicefrac{{1}}{{\beta}}\), the following holds for USAM:_

\[\mathcal{L}(w_{t})-\mathcal{L}^{\star}\leq\bigg{(}1-2\mu\eta(1- \rho\beta)\bigg{(}1-\frac{\eta\beta}{2}(1-\rho\beta)\bigg{)}\bigg{)}^{t}( \mathcal{L}(w_{0})-\mathcal{L}^{\star}),\quad\forall t\geq 0.\]

Proof of Theorem 48.: We follow the convention in Karimi et al. (2016): Let \(\mathcal{X}^{\star}\) be the set of global minima and \(x_{p}\) be the projection of \(x\) onto the solution set \(\mathcal{X}^{\star}\). From \(\beta\)-smoothness, it follows that

\[\|\nabla\mathcal{L}(x)\|=\|\nabla\mathcal{L}(x)-\nabla\mathcal{L} (x_{p})\|\leq\beta\|x-x_{p}\|,\quad\forall x.\]

Now since \(\mathcal{L}\) is \(\beta\)-smooth and \(\mu\)-PL, Theorem 2 from (Karimi et al., 2016) implies that the quadratic growth condition holds, i.e.,

\[\frac{2}{\mu}(\mathcal{L}(x)-\mathcal{L}^{\star})\geq\|x-x_{p}\|^ {2},\quad\forall x.\]

Thus, it follows that

\[\|\nabla\mathcal{L}(x)\|^{2}\leq\frac{2\beta^{2}}{\mu}(\mathcal{ L}(x)-\mathcal{L}^{\star}),\quad\forall x.\]

Moreover, from \(\beta\)-smoothness, we have

\[\|\nabla\mathcal{L}(x+\rho\nabla\mathcal{L}(x))\|\leq\|\nabla \mathcal{L}(x)\|+\beta\|\rho\nabla\mathcal{L}(x)\|=(1+\beta\rho)\|\nabla \mathcal{L}(x)\|,\quad\forall x.\]

Thus, by the update rule of USAM (2), it follows that

\[\|w_{t}-w_{0}\| \leq\eta\sum_{i=0}^{t-1}\|\nabla\mathcal{L}(w_{i}+\rho\nabla \mathcal{L}(w_{i}))\|\] \[\leq\eta(1+\beta\rho)\sum_{i=0}^{t-1}\|\nabla\mathcal{L}(w_{i})\|\] \[\leq\eta(1+\beta\rho)\sum_{i=0}^{t-1}\sqrt{\frac{2\beta^{2}}{\mu} (\mathcal{L}(w_{i})-\mathcal{L}^{\star})}\] \[=\eta(1+\beta\rho)\sqrt{\frac{2\beta^{2}}{\mu}}\sum_{i=0}^{t-1} \sqrt{\mathcal{L}(w_{i})-\mathcal{L}^{\star}}.\]

Now we only to invoke the USAM descent lemma stated before, i.e., Lemma 49, giving

\[\sum_{i=0}^{t-1}\sqrt{\mathcal{L}(w_{i})-\mathcal{L}^{\star}} \leq\sum_{i=0}^{t-1}\bigg{(}1-2\mu\eta(1-\rho\beta)\bigg{(}1-\frac {\eta\beta}{2}(1-\rho\beta)\bigg{)}\bigg{)}^{\nicefrac{{1}}{{2}}}\sqrt{ \mathcal{L}(w_{0})-\mathcal{L}^{\star}}\] \[\leq\bigg{(}1-2\mu\eta(1-\rho\beta)\bigg{(}\frac{\eta\beta}{2}(1- \rho\beta)\bigg{)}\bigg{)}^{-\nicefrac{{1}}{{2}}}\sqrt{\mathcal{L}(w_{0})- \mathcal{L}^{\star}}.\]

Putting the last two inequalities together then give our conclusion.