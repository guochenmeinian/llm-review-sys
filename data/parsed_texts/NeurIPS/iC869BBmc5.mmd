# ProEdit: Simple Progression is All You Need

for High-Quality 3D Scene Editing

 Jun-Kun Chen Yu-Xiong Wang

University of Illinois Urbana-Champaign

{junkun3, yxw}@illinois.edu

immortalco.github.io/ProEdit

###### Abstract

This paper proposes ProEdit - a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel _progressive_ manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model's large _feasible output space_ (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high quality and efficiency in performing each subtask. Extensive evaluation shows that our ProEdit achieves state-of-the-art results in various scenes and challenging editing tasks, _all_ through a simple framework _without_ any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, ProEdit also provides a new way to control, preview, and select the "aggressivity" of editing operation during the editing process.

## 1 Introduction

The emergence and advancement of modern scene representation models, exemplified by neural radiance fields (NeRFs) [1] and 3D Gaussian splatting (3DGS) [2], have significantly reduced the difficulty associated with high-quality reconstruction and rendering of large-scale scenes. In addition to reconstructing known scenes, there is growing interest in editing existing scenes to create new ones.

Figure 1: By decomposing a difficult task into easy subtasks and then progressively performing them (upper part), our ProEdit achieves high-quality 3D editing results with bright colors and detailed textures along with introducing new controllability of the editing aggressivity (lower part). **More results are provided on our project page.**Among the various editing operations, the _instruction-guided scene editing_ (IGSE) stands out as one of the most free-form tasks, supporting editing based on simple text descriptions. Due to the lack of 3D supervision data to train editing models in 3D, current state-of-the-art methods tackle IGSE using _2D diffusion distillation_, which involves distilling editing signals from a pre-trained 2D diffusion model [3, 4]. These methods leverage the 2D diffusion model to edit rendered images of scenes from multiple viewpoints, and then reconstruct the edited scene from these edited images using specific distillation losses.

However, a substantial challenge faced by such distillation-based approaches in achieving high-quality scene editing lies in ensuring that the scene representation converges on the edited multi-view images. Failure to achieve so results in gloomy colors, blurred textures, and noisy geometries (_e.g._, the failure cases from [5]). We argue that **this challenge is rooted in the diffusion model's large _feasible output space_ (FOS) for the same instruction** - since a text instruction can be interpreted in different yet plausible ways. For example, "make the person wear a hat" could be implemented with a hat of any style, shape, size, position, _etc_.

Therefore, large FOS is the underlying cause of _multi-view inconsistency_ in 2D editing results, making the scene representation - originally designed for reconstructing from consistent images - hard to converge. Previous work, often unaware of this fundamental issue, deals with multi-view inconsistency by introducing inconsistency-robust distillation losses [6, 7] to tolerant inconsistency, or proposing additional components and training procedures [8, 9] to select consistent images from the FOS. While adding costs and complexities, these methods frequently fail to converge to a high-quality scene when the FOS is considerably large, especially for operations that change the scene's geometry.

In overcoming this challenge posed by the large FOS, _our key insight_ is to control the FOS size through _editing task decomposition_, as illustrated in Fig. 1. Building on this insight, we propose _ProEdit_, a simple, novel framework to achieve high-quality IGSE, by decomposing the original, large-FOS task into multiple _subtasks_ with significantly smaller FOS, and then _progressively_ performing high-quality editing for each of these tasks. With each subtask's FOS effectively controlled, they can be solved under a simple solution _without_ the need for additional distillation losses, components, or complex training procedures. Progressively solving all these subtasks naturally leads to a high-quality edited scene that meets the requirements of the original task.

To perform subtask decomposition, we introduce an intuitive formulation of "subtasks" with text encoding interpolation. Based on this formulation, we propose a _subtask scheduler_ to determine the subtask decomposition and guide the editing process. This decomposition consists of a sequence of subtasks, where each subtask is applied to the edited scene from the previous one. We adaptively assign subtasks according to the estimated FOS size, so that each subtask has comparable FOS sizes and difficulty levels and can thus be solved relatively easily with high quality and efficiency.

Guided by the subtask scheduler, we progressively iterate on the subtasks to apply editing. Though their FOS size and difficulty are controlled, it still remains non-trivial to make the scene representation converge in precise geometry. Failing to achieve this will accumulate errors across subtasks, leading to unreasonable geometry in the final results. To this end, we choose 3D Gaussian splatting (3DGS) [2] as our scene representation for its high training efficiency. We design a novel _adaptive_ Gaussian creation strategy in training to maintain and refine the geometric structure in each subtask, by controlling the size of the splitting and duplication operations. This strategy allows the geometry to be adjusted toward the goal of each subtask, while preventing and removing floc, floating noise, and multi-face structures.

With these key designs, our ProEdit achieves high-quality instruction-guided scene editing in various scenes and editing tasks with precise geometry and detailed textures, as shown in Fig. 1. Notably, ProEdit does not rely on complicated or expensive add-ons, such as specialized distillation losses, additional 3D attention or convolution components, or extended training procedures on the diffusion model. Moreover, as each subtask represents a partial completion of the overall task, our method enables users to _control, preview, and select_ the intermediate stages of editing, which we refer to as "aggressivity" of editing operation during the editing process. This can be simply achieved by taking the edited scene from a subtask either during or after the editing process. Thus, in contrast to previous methods such as classifier-free guidance [10] and SDEdit [11], our ProEdit provides a novel way to monitor and manage the editing process. Users can _preview_ different versions of editing with the intermediate outcomes, adjust the subtasks _on the fly_ accordingly to achieve improved final results, and finally _select_ the most satisfactory editing result from all the intermediate ones.

**Our contributions** are three-fold. (1) We offer a novel insight into subtask decomposition and progressive editing, tailored to address the core challenge of large feasible output space in 3D scene editing. (2) We propose a simple yet effective framework, ProEdit, that generates high-quality edited scenes by progressively solving each subtask, without requiring any complicated or expensive add-ons to the diffusion model, while also supporting control, training-time preview, and selection of editing task aggressivity. (3) We consistently achieve high-quality editing results in various scenes and challenging tasks, establishing state-of-the-art performance.

## 2 Related Work

**Learning-Based 3D Scene Representation.** Our framework necessitates a learnable 3D representation to depict the scene being edited. Traditional methods model the 3D geometric structure of a scene with implicit [12; 13; 14] or explicit [15; 16; 17] representations. However, these methods require more information or pre-processing beyond multi-view camera images. In 2020, the neural radiance field (NeRF) [1] emerges as the first neural network-based scene representation, enabling direct scene reconstruction from multi-view images captured at known camera locations, inspiring numerous follow-up work [18; 19; 20; 21; 22; 23; 24] that explores different aspects including quality, efficiency, and visual effects. Later, 3D Gaussian splatting (3DGS) [2] becomes a new trend, outperforming NeRF and its variants in rendering quality and efficiency. 3DGS also leads to several follow-up variants, aiming to improve geometry [25; 26] and visual effects [27], as well as extending to dynamic 3D scenes [28; 29; 30].

**3D Scene Editing.** Various scene editing tasks have been investigated, each aiming to achieve different editing objectives for a given scene across a range of scene representations. These tasks cover different aspects of a scene, including the location, shape, and color of objects [20; 31; 32; 33], physical effects [34], lighting conditions [27; 35; 36], and the overall appearance [37; 38; 5; 7; 39].

**Instruction-Guided Scene Editing.** Instruction-guided scene editing is a highly free-form yet challenging task, characterized by a straightforward task descriptor - either an editing operation (_e.g._, "Give the person a hat") or a description of the desired scene (_e.g._, "A person wearing a hat"). This task has attracted much attention in the computer vision community. Due to the lack of large-scale 3D datasets to train editing models directly in 3D, current state-of-the-art methods [38; 39; 40; 5; 6; 7; 30; 41] achieve scene editing by distilling knowledge from a pre-trained 2D diffusion model [3; 5] using score distillation sampling (SDS) [42] and its variants. Instruct-NeRF2NeRF (IN2N) [5] and its variants [37; 39] apply SDS-equivalent iterative dataset updates to generate edited multi-view images and train the scene representation on them. One direction of follow-up work [6; 7] proposes novel distillation methods to better utilize the 2D editing capability, while another [9; 41] introduces additional components and training procedures to improve the consistency of generation. However, these approaches are unaware of the core challenge posed by large feasible output space (FOS), mitigating it with add-ons that may still fail when the FOS becomes considerably large. In contrast, our ProEdit is tailored for this challenge by proposing subtask decomposition to explicitly control the size of FOS, thereby extending the capability boundary of instruction-guided scene editing.

## 3 ProEdit: Methodology

The key insight of our ProEdit is to decompose a full editing task, described by a text instruction, into a sequence of simpler subtasks with smaller feasible output space (FOS), and apply each of them progressively on the scene. Our framework consists of three major components: (1) an interpolation-based subtask formulation that defines, obtains, and interprets each subtask; (2) a difficulty-aware subtask decomposition scheduler that breaks down the full editing task into several subtasks of comparable difficulty; and (3) an adaptive 3D Gaussian splatting (3DGS)-based [2] geometry-precise scene editing method that ensures high-quality editing for each subtask, ultimately leading to successful completion of the full task. Our framework is visualized in Fig. 2.

### Interpolation-Based Subtask Formulation

In order to decompose a text-described task into subtasks, we first need to clearly define "task" and "subtasks." We define an editing task \(T(s,e=E(p))\) as an operation that applies a prompt (instruction) \(p\) on the original scene \(s\), where \(e=E(p)\) denotes the text encoding of \(p\) calculated by a frozen text encoder \(E(\cdot)\) as part of a 2D diffusion model. The notation \(T(s,e)\) represents the editedscene resulting from this task, and we also use \(T(\cdot,e)\) to indicate the mapping from the original scene to the edited scene within this context. Additionally, we define \(\varnothing\) as the empty prompt, indicating that the editing task with this prompt retains the original scene, or \(T(s,E(\varnothing))=s\).

Next, we define subtasks as \(S(s,r)=T(s,e(r))\) with a ratio \(r\in[0,1]\), where \(e(r)=r\cdot E(p)+(1-r)\cdot E(\varnothing)\). This represents a task characterized by an instruction \(p(r)=E^{-1}(r\cdot E(p)+(1-r)\cdot E(\varnothing))\), whose embedding is a ratio-\(r\) interpolation between \(E(p)\) and \(E(\varnothing)\). Assuming that the neural network \(E(\cdot)\) is continuous, \(S(s,r)\) will also be continuous w.r.t. \(r\). Therefore, this formulation provides a continuous space of subtasks or intermediate tasks between the original task \(T(\cdot,E(p))\) and the identity mapping \(T(\cdot,E(\varnothing))\).

### Difficulty-Aware Subtask Scheduler

**Feasible Output Space (FOS) and Task Difficulty.** Inspired by the derivation of SDS [42], we introduce the concept of _feasible output space_ (FOS) for an editing task \(T(s,E(p))\) as follows: the set of scenes \(s^{\prime}\) such that, when \(s^{\prime}\) is rendered from any view \(v\), the resulting image resembles the edited image (based on instruction \(p\)) from the corresponding view \(v\) of the original scene \(s\), _i.e._, the set of all possible scenes that can be regarded as valid edited result for the given task. A larger FOS indicates greater diversity in how the editing task can be executed; however, this variability can cause multi-view inconsistency, if different views are edited differently. Therefore, an editing task with a larger FOS is inherently more difficult to accomplish.

**Formulation of Subtask Decomposition.** Our goal is to decompose the original editing task \(T(\cdot,E(p))\) into a sequence of subtasks, such that applying each subtask progressively or iteratively on the current scene leads to the final editing result. Formally, the decomposition of a task \(T(\cdot,E(p))\) is a monotonically increasing sequence \(r_{0},r_{1},\cdots,r_{n}\), where \(r_{0}=0,r_{n}=1\). We then define \(s_{i}\) as the edited scene resulting from the \(i\)-th subtask. We have

\[s_{i}=\left\{\begin{array}{ll}s,&(\text{original scene}),\\ S(s_{i-1},r_{i}),&(\text{apply subtask }r_{i}\text{ on previously edited scene}), \end{array}\right. i=0,\] (1)

In other words, the \(i\)-th subtask is \(S(s_{i-1},r_{i})\), which is applied on the edited scene \(s_{i-1}\) from the previous \((i-1)\)-th subtask. The outcome of the \(i\)-th subtask is \(s_{i}\).

**Subtask Difficulty Measurement and Approximation.** The difficulty of each subtask \(S(s_{i-1},r_{i})\) is measured as being proportional to the size of FOS (a continuous space), which is difficult to compute or even rigorously define. Therefore, we approximate this difficulty by evaluating the difference between the original and edited images of the 2D diffusion model. Intuitively, an editing task that brings a significant change typically has more degrees of freedom, leading to a larger FOS. Additionally, each subtask \(r_{i}\) is applied on the scene \(s_{i-1}\), which cannot be determined until all prior subtasks \(r_{1},r_{2},\cdots,r_{i-1}\) are completed. So, we make another approximation based on the assumption that the image of a view in \(s_{i}\) will closely resemble the corresponding view of \(s\) edited by the 2D diffusion model following the instruction of the \(i\)-th subtask. In other words,

\[v_{k}(s_{i})\approx T_{\text{2D}}(v_{k}(s),e(r_{i})),\forall k\in V,\] (2)

Figure 2: **Our ProEdit framework** features three major designs: an interpolation-based subtask formulation (Sec. 3.1), a difficulty-aware subtask scheduler for subtask decomposition (Sec. 3.2), and an adaptive 3DGS tailored for progressive scene editing through a dual-GPU pipeline (Sec. 3.3). For an editing task, we first decompose it into interpolation-based subtasks to schedule the editing process with the subtask scheduler, and then progressively perform the subtasks with adaptive 3DGS.

where \(v_{k}(s)\) is the rendered image at the \(k\)-th view of scene \(s\), and \(T_{\mathrm{2D}}(v,e)\) is the output of a 2D editing task applied on image \(v\) with instruction embedding \(e\), generated by the 2D diffusion model. By applying such an approximation to both subtasks and using Learned Perceptual Image Patch Similarity (LPIPS) to measure the perceptual difference between images, we can then define the difficulty metric as

\[\mathrm{d}(r_{i},r_{j})\stackrel{{\text{Def}}}{{=}}\sum_{k\in V} L_{\mathrm{LPIPS}}(v_{k}(s_{i}),v_{k}(s_{j}))\approx\sum_{k\in V}L_{\mathrm{ LPIPS}}(T_{\mathrm{2D}}(v_{k}(s),e(r_{i})),T_{\mathrm{2D}}(v_{k}(s),e(r_{j}))).\] (3)

Observing that \(\mathrm{d}(r_{i},r_{j})\)'s approximation is only related to the rendered image \(v_{k}(s)\) of the original scene \(s\) and is independent of that of the edited scene (namely, \(v_{k}(s_{i})\)), we can then allow \(\mathrm{d}(r_{a},r_{b})\) to take any two arbitrary subtasks \(r_{a}\) and \(r_{b}\). Our goal is to find the subtask decomposition \(r_{0},\cdots,r_{n}\) with similar \(\{\mathrm{d}(r_{i-1},r_{i})\}\) for each \(i\).

**Difficulty-Aware Adaptive Subtask Decomposition.** The approximation of \(\mathrm{d}(r_{i},r_{j})\) disentangles its computation from the edited scene of task \(T(\cdot,e(r_{i}))\), by substituting it with \(T_{\mathrm{2D}}(\cdot,e(r_{i}))\). This enables us to decompose the subtasks from a more _global_ perspective. Therefore, we propose an adaptive method to obtain the set of subtask ratios \(R=r_{0},\cdots,r_{n}\). The algorithm operates recursively over an interval \([r_{a},r_{b}]\) with a difficulty threshold \(\mathrm{d}_{\mathrm{threshold}}\), starting with the interval \([0,1]\). In each recursion, the algorithm first includes both \(r_{a}\) and \(r_{b}\) in the set \(R\), and stops the recursion if \(\mathrm{d}(r_{a},r_{b})\leq\mathrm{d}_{\mathrm{threshold}}\). Otherwise, it selects the middle point \(r_{m}=(r_{a}+r_{b})/2\), and recurses on the intervals \([r_{a},r_{m}]\) and \([r_{m},r_{b}]\). Once the recursion is complete, we obtain the sequence of subtasks \(r_{0},\cdots,r_{n}\) by sorting the set \(R\), ensuring that \(\mathrm{d}(r_{i-1},r_{i})\leq\mathrm{d}_{\mathrm{threshold}}\) for all subtasks.

To simplify the subtask decomposition, we check if there exists a subtask \(r_{i}\) such that \(\mathrm{d}(r_{i-1},r_{i+1})\leq\mathrm{d}_{\mathrm{threshold}}\). If so, we could safely remove the subtask \(r_{i}\) while still maintaining \(\mathrm{d}(r_{i-1},r_{i+1})\leq\mathrm{d}_{\mathrm{threshold}}\). This iterative check continues until no further subtasks can be pruned.

Notably, an interpolated subtask can be regarded as a partial completion of the editing instruction. For example, the instruction "Make him smile" with an interpolation ratio of \(r=0.5\) can be interpreted as "Make him half-smile," indicating a lower _aggressivity_ of the editing operation. In this context, high aggressivity indicates more significant changes towards the editing operation, whereas low aggressivity reflects greater similarity between the edited scene and the original one. Therefore, our subtask decomposition not only lays the foundation for our editing process but also categorizes task aggressivity, where each subtask corresponds to a specific level of aggressivity. Consequently, beyond performing editing, our ProEdit enables users to control, preview, and select the aggressivity of the editing operation _during or after the editing process_, by utilizing the edited scene of a subtask throughout the progressive editing workflow. Such a capability is absent in previous work.

**Subtask Scheduling.** The subtask scheduler also determines when the current subtask is complete, allowing us to proceed to the next one. Designing an image-based criterion to assess whether the images in the current subtask have been sufficiently edited is challenging. Therefore, we propose a criterion based on the scene representation training procedure. Specifically, when the running mean of the training loss no longer decreases over a specified number of iterations, we regard the scene representation to be converging to the edited scene, indicating that the current editing subtask is complete. Moreover, apart from the subtasks \(r_{0},r_{1},\cdots,r_{n}\), we prepend an additional subtask \(r_{0}\) to refine the initial scene representation using diffusion-reconstructed original images, and append another subtask \(r_{n}\) to consolidate the editing results, as detailed in Appendix B.

### Adaptive 3DGS Tailored for Progression

We choose 3DGS [2] as our scene representation for its high efficiency and rendering quality. However, 3DGS is primarily designed for reconstruction from multi-view consistent images. Directly training on edited images with 3DGS results in a continuously increasing number of Gaussians that overfit the inconsistent views, ending up with an out-of-memory error. Therefore, we propose a novel Adaptive 3DGS specifically tailored for progressive scene editing.

**Basic Workflow for Each Subtask.** As each subtask has a reduced FOS and lower difficulty, we can use a straightforward approach to perform the subtask editing. Consistent with Instruct-NeRF2NeRF (IN2N) [5], we apply a simple iterative dataset update (Iterative DU) that iteratively generates edited views using the diffusion model and employs them to train the scene representation. Unlike NeRFs [1], our 3DGS-based scene representation accepts full images as supervision instead of rays, allowingus to directly train on the edited images without the need to replace rays. This enables a simpler yet more effective workflow.

**Adaptive Gaussian Creation Strategy.** While the decomposition of subtasks controls the size of FOS and reduces potential inconsistencies, making 3DGS converge on the edited multi-view images remains challenging. Designed only for reconstruction from multi-view consistent images, 3DGS is not robust enough to deal with all inconsistencies. This can lead to overfitting on the inconsistent edited images with view-dependent colors, floating or floc noises, and multi-face structures.

Therefore, we propose an adaptive Gaussian creation strategy to refine the geometry of 3DGS, enabling it to converge on the edited images with reasonable and potentially high-quality geometric structures. As introduced in [2], the original 3DGS maintains Gaussian-represented geometry by periodically culling unnecessary Gaussians based on an opacity threshold, and by creating new Guassians (through splitting or duplicating) to expand model capability according to a training gradient threshold. Our strategy builds on this geometry maintenance schedule by _adaptively_ controlling both thresholds. (1) At the beginning of each subtask, we set the opacity of all Gaussians to the threshold and perform several iterations of training without geometry maintenance. This training procedure implicitly identifies the Gaussians that correctly lie on the object surface by making them learn higher opacity, which allows them to be preserved in the scene representation. Conversely, Gaussians with incorrect geometry learn lower opacity and are subsequently culled during the next maintenance phase. (2) To prevent the training process from creating too many noisy Gaussians in a single iteration when operating with edited images, we also control the gradient threshold for Gaussian creation to achieve a smooth increase in the number of Gaussians. We schedule the number of created Gaussians based on the existing Gaussians in the scene and the number previously culled, selecting the threshold according to this scheduled number, as detailed in Appendix D. With these strategies, our 3DGS is able to converge to the edited scenes with clear texture and reasonable, even precise geometry.

**Dual-GPU Training to Decouple Diffusion and 3DGS.** Given the significant difference in iteration speeds - around 2 seconds per generation for the diffusion model inference and less than 0.02 seconds per iteration for the 3DGS training procedure - it is challenging to achieve an effective trade-off on a single GPU during Iterative DU. Inspired by [9; 43], we employ a dual-GPU training schedule to decouple them. The first GPU iteratively generates newly edited images using the diffusion model and stores them in a buffer as the updated dataset. Meanwhile, the second GPU iteratively trains 3DGS with the edited images in the buffer and raises a signal to indicate when the current subtask is complete. This approach enables a highly efficient training procedure within our ProEdit framework.

## 4 Experiments

### Experimental Settings

**Scene Representation and Diffusion Model.** As mentioned in Sec. 3.3, our ProEdit leverages 3DGS-based scene representation for high quality and efficiency. We use the Splatfacto model from the NeRFStudio [44] library as our backbone. For the diffusion model, consistent with previous work [5; 6; 37; 45], we use a pre-trained Instruct-Pix2Pix (IP2P) [4] model from HuggingFace.

**Scenes and Editing Instructions.** According to Sec. 3.1, each editing task \(T(s,E(p))\) is characterized by a scene \(s\) and an instruction \(p\), and the desired output is the edited scene. We evaluate our ProEdit on the following scene datasets: (1) The IN2N dataset introduced by Instruct-NeRF2NeRF (IN2N) [5], which is available for free use and is the most widely used dataset in prior work. (2) The ScanNet++ dataset of indoor scenes, released under the ScanNet++ Terms of Use, which is introduced for instruction-guided scene editing in [9]. We use instructions either from previous methods for comparisons or from tasks that require highly noticeable geometric changes in the scene - one of the most challenging editing tasks that previous methods have struggled to perform well.

**Subtask Scheduling.** We determine the number of subtasks to balance editing quality, controllability, and efficiency. For texture-focused instructions (_e.g_., style transfer), we decompose each task into approximately 4 subtasks using an appropriate threshold \(\mathrm{d_{threshold}}\); for geometry-related instructions with much higher FOS, we break each task down into around 8 subtasks with a proper \(\mathrm{d_{threshold}}\).

**Baselines.** We compare our ProEdit with recent state-of-the-art instruction-guided scene editing methods, including Instruct-NeRF2NeRF (IN2N) [5] (along with its 3DGS-based implementation [45]), ViCA-NeRF [41], ConsistDreamer [9], CSD [6], PDS [7], Efficient-NeRF2NeRF (EN2N) [37], DreamEditor [46], _etc_. As different methods use different tasks for visualization in their papers, and some do not provide publicly available code or pre-trained models, our primary comparisons focus on common editing tasks, leveraging the visualizations presented in their papers. Also, we include comparisons for some additional tasks with results generated from available code or re-implementations. As our ProEdit specifically targets the instruction-guided scene editing task, we do not include comparisons with methods designed for other scene editing or generation tasks.

**Implementation Details.** We follow the default hyperparameter settings of the Splatfacto method, and set the classifier-free guidance (CFG) [10] as \(7.5\times 1.5\) for all instructions in the diffusion model. During the editing process for each subtask, consistent with IN2N [5], we use SDEdit's [11] method to control similarity with denoising timesteps between 450 and 850. We also apply HiFA's [47] annealing strategy to gradually decrease denoising timesteps in this process. Utilizing a dual-GPU training workflow (Sec. 3.3), the editing tasks are conducted on two NVIDIA A6000 or A100 GPUs, with each subtask taking 10 to 20 minutes to complete depending on its difficulty and convergence.

**Metrics.** We present the quantitative assessment under the following metrics: User Study of Overall Quality (USO), User Study of 3D Consistency (US3D), GPT Evaluation Score (GPT), CLIP [48]

Figure 3: **In the comparative experiments on the Fangzhou and Face scenes**, our ProEdit achieves high-quality editing, with strong instruction fidelity, clear textures, and precise shapes across both levels of aggressivity controlled by subtask scheduling. The “medium aggressivity” editing results are obtained from an intermediate subtask. The editing results of the baselines are sourced from visualizations in their respective papers.

Text-Image Direction Similarity (CTIDS), and CLIP Direction Consistency (CDC). The user study was conducted with 26 participants. The GPT score is detailed in Appendix E. The CLIP-based scores are consistent with those reported in IN2N [5].

### Experimental Results and Analysis

**Qualitative Results.** Fig. 3 shows the comparisons in the Fangzhou scene and the IN2N's Face scene. Our ProEdit demonstrates results on two levels of editing aggressivity: high aggressivity results are obtained by executing all subtasks, while medium aggressivity results are derived from completing only the first 40% subtasks. Overall, our ProEdit produces high-quality editing results characterized by clear textures, bright colors, reasonable and precise geometry, and high instruction fidelity. Compared with the baselines, our ProEdit shows enhanced geometry editing capabilities, particularly in the "Tolkien Elf" editing which features a thinner face, and the "Lord Voldemort" editing which incorporates more wrinkles in the Fangzhou scene. By contrast, the baselines tend to maintain geometry more similar to the original scene. Notably, for the editing task "Give him a plaid jacket," our ProEdit generates much clearer and more noticeable plaid patterns than all baselines.

The experimental results on the ScanNet++ dataset are shown in Fig. 4. With subtask decomposition and progressive editing, our ProEdit achieves high-quality results that are comparable to and even outperform the baseline ConsistDreamer [9], which incorporates three complicated add-ons for ensuring 3D consistency. This shows that our simple progression is more effective in reducing inconsistency - through reducing the size of FOS - than explicit 3D consistency-enforcing components.

Figure 4: **In the comparative experiments on the ScanNet++ scenes, our simple ProEdit also achieves high-quality editing that is comparable to, and in some cases even outperforms, the sophisticated baseline ConsistDreamer [9]. All visualizations are sourced from ConsistDreamer’s paper.**

Figure 5: **In the comparative experiments across various outdoor scenes, our ProEdit not only achieves high-quality editing that surpasses the baselines, but also enables aggressivity controls for a range of scenes and tasks.**

[MISSING_PAGE_FAIL:9]

**Ablation Study.** To validate the necessity of our subtask decomposition, we conduct experiments on a variant of ProEdit using only one subtask (\(n=1\), \(r_{0}=0\), \(r_{1}=1\)), effectively disabling decomposition (referred to as "ND"). Qualitative results are shown in Fig. 6. Without subtask decomposition, the variant generates unrealistically long cheeks to accommodate inconsistencies in cheek decorations across views, resulting in blurred cheek textures in the rendered output due to the large FOS of the editing task. In contrast, our full ProEdit achieves bright, clear results with precise and realistic geometry. Quantitative results are shown in Table 2. For this comparison, we conducted a new user study involving 41 participants, including an additional User Study of Shape Plausibility ("USP") metric: we provide participants with the modeled depth maps, similar to those in Fig. 6, along with the rendered RGB images. We then ask them to evaluate whether the shapes are realistic and match the rendered images. The "ND" variant performs significantly worse than our full method on all user study metrics, further underscoring the effectiveness of our subtask decomposition. These results collectively demonstrate that reducing FOS through subtask decomposition is crucial to our high-quality results.

## 5 Discussion

**3D Consistency Add-Ons.** Different from our subtask decomposition strategy, 3D consistency add-ons, such as distillation losses, consistency-inducing components, and specific training procedures, offer an alternative way to control and reduce FOS. Although our framework achieves high-quality editing without them, combining it with these 3D consistency add-ons can leverage the strengths of both approaches, potentially reducing the number of required subtasks and enhancing editing quality.

**Limitations.** Our ProEdit is a distillation-guided framework from 2D diffusion, similar to all baselines. Therefore, its editing capability is constrained by the underlying diffusion model. If the diffusion model does not support applying a specific editing instruction on most views of a scene, our ProEdit will also be unable to do so. Additionally, ProEdit relies on 3DGS for efficient training, which NeRF-based representations do not support; consequently, it inherits certain limitations of 3DGS, including limited suitability for unbounded outdoor scenes. Finally, ProEdit may still encounter the multi-face or Janus problems, as the 2D diffusion model lacks 3D awareness.

**Future Directions.** There are many promising directions to explore in subtask decomposition beyond the interpolation-based strategy introduced in this paper. One potential way is to explicitly construct intermediate subtasks using semantic guidance. For example, applying "Turn him into a bald person" before "Make him wear a hat" could lead to a more free-form hat independent of the hair, with such intermediate instructions generated by large language models. Another alternative avenue involves leveraging video generation models to "animate" the transition from the original scene to the edited scene, treating this animation process as a series of subtasks. Doing so will enable ProEdit to function as a 3D scene animator, generating high-quality 4D (dynamic 3D) scenes. Additionally, the progressive framework of ProEdit can be potentially applied to scene generation.

**Potential Societal Impacts.** The positive societal impacts of our ProEdit include (1) the development of consumer-grade 3D scene editing products and applications, facilitated by advancements in 3D structured-light scanners for mobile phones and virtual reality (VR) and augmented reality (AR); and (2) the transformation of high-quality 3D and 4D (dynamic 3D) scene creation through the editing of existing high-resolution scenes. On the other hand, as our framework is based on generative models, it is crucial to address potential ethical and safety concerns, including risks of producing biased results and the possibility of misuse for illegal activities.

## 6 Conclusion

This paper proposes ProEdit, a novel 3D scene editing framework that decomposes the editing task into subtasks and performs them progressively. Our method targets the fundamental cause of inconsistency - the large feasible output space of the diffusion model with respect to an editing task. Extensive experiments show that our ProEdit produces high-quality editing results characterized by bright colors, sharp and detailed textures, and precise geometric structures across various scenes and editing tasks. Our method further enables a novel controllability over the aggressivity of the editing task, by allowing users to select which subtasks to execute. We hope that our ProEdit will inspire exciting applications and new research directions in 3D scene editing and generation.

## Acknowledgments

This work was supported in part by NSF Grant 2106825, NIFA Award 2020-67021-32799, the IBM-Illinois Discovery Accelerator Institute, the Toyota Research Institute, and the Jump ARCHES endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation. This work used computational resources on NCSA Delta through allocations CIS220014 and CIS230012 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, and on TACC Frontera through the National Artificial Intelligence Research Resource (NAIRR) Pilot.

## References

* [1] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.
* [2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. _TOG_, 42(4):139-1, 2023.
* [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
* [4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Learning to follow image editing instructions. In _CVPR_, 2023.
* [5] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3D scenes with instructions. In _ICCV_, 2023.
* [6] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn2, and Jinwoo Shin1. Collaborative score distillation for consistent visual editing. In _NeurIPS_, 2023.
* [7] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. In _CVPR_, 2024.
* [8] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. ConsistNet: Enforcing 3D consistency for multi-view images diffusion. In _CVPR_, 2024.
* [9] Jun-Kun Chen, Samuel Rota Bulo, Norman Muller, Lorenzo Porzi, Peter Kontschieder, and Yu-Xiong Wang. ConsistDreamer: 3D-consistent 2D diffusion for high-fidelity scene editing. In _CVPR_, 2024.
* [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.
* [11] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.
* [12] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In _CVPR_, 2019.
* [13] Marc Levoy and Pat Hanrahan. Light field rendering. In _SIGGRAPH_, 1996.
* [14] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In _CVPR_, 2019.
* [15] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2Mesh: Generating 3D mesh models from single RGB images. In _ECCV_, 2018.
* [16] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. SurfaceNet: An end-to-end 3D neural network for multiview stereopsis. In _ICCV_, 2017.
* [17] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. _TPAMI_, 38(10):2024-2039, 2015.
* [18] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. MVS-NeRF: Fast generalizable radiance field reconstruction from multi-view stereo. In _ICCV_, 2021.
* [19] Liwen Wu, Jae Yong Lee, Anand Bhattad, Yu-Xiong Wang, and David Forsyth. DIVeR: Real-time and accurate neural radiance fields with deterministic integration for volume rendering. In _CVPR_, 2022.

* [20] Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. NeuralEditor: Editing neural radiance fields via manipulating point clouds. In _CVPR_, 2023.
* [21] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In _NeurIPS_, 2020.
* [22] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _CVPR_, 2022.
* [23] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In _ICCV_, 2021.
* [24] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-Hai Zhang. NeRFReN: Neural radiance fields with reflections. In _CVPR_, 2022.
* [25] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. DN-Splatter: Depth and normal priors for Gaussian splatting and meshing, 2024.
* [26] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian splatting for geometrically accurate radiance fields. In _SIGGRAPH_, 2024.
* [27] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3DGSR: Implicit surface reconstruction with 3D Gaussian splatting, 2024.
* [28] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4D Gaussian splatting for real-time dynamic scene rendering. In _CVPR_, 2024.
* [29] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4D reconstruction with dynamic 3D Gaussian particle. In _CVPR_, 2024.
* [30] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3D Gaussians for high-fidelity monocular dynamic scene reconstruction. In _CVPR_, 2024.
* [31] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Junyan Zhu, and Bryan C. Russell. Editing conditional radiance fields. In _ICCV_, 2021.
* [32] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In _ICCV_, 2021.
* [33] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. In _NeurIPS_, 2022.
* [34] Yi-Ling Qiao, Alexander Gao, and Ming C. Lin. NeuPhysics: Editable neural geometry and physics from monocular videos. In _NeurIPS_, 2022.
* [35] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. NeRF for outdoor scene relighting. In _ECCV_, 2022.
* [36] Yingyan Xu, Gaspard Zoss, Prashanth Chandran, Markus Gross, Derek Bradley, and Paulo Gotardo. ReNeRF: Relightable neural radiance fields with nearfield lighting. In _ICCV_, 2023.
* [37] Liangchen Song, Liangliang Cao, Jiatao Gu, Yifan Jiang, Junsong Yuan, and Hao Tang. Efficient-NeRF2NeRF: Streamlining text-driven 3D editing with multiview correspondence-enhanced diffusion models, 2023.
* [38] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. GaussianEditor: Swift and controllable 3D editing with Gaussian splatting. In _CVPR_, 2024.
* [39] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, and Takuya Narihira. Instruct 3D-to-3D: Text instruction guided 3D-to-3D conversion. _arXiv preprint arXiv:2303.15780_, 2023.
* [40] Lu Yu, Wei Xiang, and Kang Han. Edit-DiffNeRF: Editing 3D neural radiance fields using 2D diffusion model. _arXiv preprint arXiv:2306.09551_, 2023.
* [41] Jiahua Dong and Yu-Xiong Wang. ViCA-NeRF: View-consistency-aware 3D editing of neural radiance fields. In _NeurIPS_, 2023.

* [42] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In _ICLR_, 2023.
* [43] Linzhan Mou, Jun-Kun Chen, and Yu-Xiong Wang. Instruct 4D-to-4D: Editing 4D scenes as pseudo-3D scenes using 2D diffusion. In _CVPR_, 2024.
* [44] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: A modular framework for neural radiance field development. In _SIGGRAPH_, 2023.
* [45] Cyrus Vachha and Ayan Haque. Instruct-GS2GS: Editing 3D Gaussian splats with instructions, 2024.
* [46] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. DreamEditor: Text-driven 3D scene editing with neural fields. In _SIGGRAPH Asia_, 2023.
* [47] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D with advanced diffusion guidance. In _ICLR_, 2024.
* [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [49] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcon, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belpun, Irwan Bello, Jake Berdine, Gabriel Bernadet-Shapiro, Christopher Berner, Lemy Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damian Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eletti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizina, Shannat Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jorn, Heewoo Jun, Tomer Kafan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosci, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lantody Lee, Jan Keinke, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfa circuit, Sam Manning, Todor Markov, Yaniv Markovski, Biana Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeynowo Noh, Long Ouyang, Cullen O'Keefe, Jakab Pachocki, Alex Paino, Joele Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parpariat, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Aletena Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnur, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherakov, Jessica Shieh, Sarah Shoker, Pranay Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alwin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Velhianta, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [50] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Ajako Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In _ECCV_, 2024.

## Appendix A Editing Difficulty w.r.t. Subtask Ratio \(r\)

We provide a visualization of per-view edited results (_i.e_., each image is edited _individually_ with IP2P [4]) with respect to different \(r\)'s, as shown in Fig. A. The multi-view inconsistency situations are as follows:

* \(r_{0}\): All views remain identical to the original view, resulting in perfect consistency.
* \(r_{1}\): The face begins to turn white, with the only inconsistency being the different degrees of color change.
* \(r_{2}\): Some areas of the face become red, introducing a new inconsistency in different locations of the red parts.
* \(r_{3}\): Additional areas of the face change color, and the nose alters shape, leading to increased inconsistencies in color distribution and nose shape.
* \(r_{4}\): The final edited results exhibit various inconsistencies across all parts, even including changes to hair color.

This visualization shows that the editing inconsistency and difficulty increase as \(r\) rises.

## Appendix B Additional Subtasks \(r_{0}\) and \(r_{n}\)

Our ProEdit framework is designed to accept _any_ scene representation for input and output, including NeRFs [1] and conventional 3DGS [2]. However, our editing process requires the scene representation to be our adaptive 3DGS (Sec. 3.3), which is optimized for progressive editing.

Therefore, the additional subtasks \(r_{0}\) and \(r_{n}\) represent the input and output states where the scene is in other representations. The corresponding subtasks \(s_{0}=S(s_{\mathrm{input}},r_{0})\) and \(s_{\mathrm{output}}=S(s_{n},r_{n})\) are for the conversion between these other scene representations and our adaptive 3DGS (_i.e_., reconstructions). Within such re-reconstructions, the diffusion model acts as a simple refiner for the per-view images, which preserves most appearances while refining defects or abnormalities and compensating for minor inadequacies in the edited areas.

Figure A: The visualization of per-view edited results (where each view is edited separately with IP2P [4]) shows that with the increment of the subtask ratio \(r\), the multi-view inconsistency also increases, leading to greater editing difficulty.

We present a visualization of the results before and after the refinement from the additional subtask \(r_{n}\) in Fig. B. The depth maps, modeled by 3DGS, are segmented to emphasize the foreground. We can observe that the two images exhibit similar appearances, but the refined version demonstrates more precise geometry and detail near the ear. This shows that while the additional subtask \(r_{n}\) does not lead to significant changes or improvements in overall appearance, it provides minor refinements to the edited results.

## Appendix C Consistency and Non-Linearity of Subtasks

In our method, we employ adaptive task decomposition (Sec. 3.2) to reduce the difficulty and inconsistency of each subtask. We approximate the difficulty by measuring the difference between the original and edited images, and design an adaptive subtask decomposition upon this difference. Even if the instruction encoder \(E(\cdot)\) is non-linear, this method enables us to achieve subtask decomposition with reduced difficulty, ensuring that the difficulty of each subtask does not exceed \(\mathrm{d}_{\mathrm{threshold}}\), a preset threshold for subtask difficulty.

As our method decomposes one editing task into multiple subtasks, we need to solve more editing (sub-)tasks in total. While completing all these subtasks may require a longer total running time, each subtask is simpler and therefore faster to achieve than performing the entire editing task in one step. This trade-off allows us to significantly improve performance while gaining control over editing aggressivity. Notably, our ProEdit is considerably more efficient than the current state-of-the-art, ConsistDreamer, as detailed in Table 1.

## Appendix D Gaussian Creation Strategy

Our Gaussian creation strategy regulates the growth speed of the Gaussians. Specifically, if we culled \(n\) Gaussians in the previous step, we will only allow \(t(n)\) Gaussians to be created at the current step, where \(t(n)\) represents a threshold function based on \(n\) and the total number of Gaussians.

This control strategy for Gaussian creation (1) prevents the generation of excessive Gaussians for minimally inconsistent multi-view images, and (2) concentrates Gaussian generation in high-frequency regions of the scene, improving the overall results.

## Appendix E GPT Evaluation Score

For the "GPT evaluation score" metric, we provide GPT-4o [49] with the original video, the editing prompt, and the videos generated by three methods all together with random names and in random

Figure B: The additional subtask \(r_{n}\) does not significantly change the overall appearance, but it brings slight improvements on geometric structure.

order to ensure unbiased scoring. GPT-4o then evaluates each video on a scale of 1 to 100, considering (1) editing completeness and accuracy, (2) preservation of the original content, (3) 3D consistency, and (4) visual appearance. The scores for all baselines are returned as a JSON array. We repeat this process multiple times and report the average score.

Our GPT score functions as a Monte-Carlo implementation of the recently proposed VQAScore [50], a metric that leverages vision-language model evaluation of generated images and has been shown to outperform CLIP-based [48] scores. Given the advanced capabilities of vision-language models to evaluate various relevant aspects, this VQA-based metric, along with our GPT score, offers a more comprehensive automated quantitative measurement than CLIP-based scores.

## Appendix F Semantic Meaning of Subtasks and Alignment with IP2P Editing

In our method, the semantic meaning can be interpreted as "how IP2P [4] behaves with the interpolated embedding." Although it is difficult to explicitly define text instructions for the interpolated embedding, we can still visualize them with IP2P's editing results corresponding to the interpolated embeddings.

We provide a visualization illustrating the alignment of the edited scene in Fig. C. From the first row, we can roughly interpret each subtask's goal. For example, \(r_{2}\) suggests "give him blue eyes and pointy ears; make the background slightly green," while \(r_{4}\) indicates "make his eyes completely blue, his hair red, and his face slightly thinner; make the background dark green." Comparing the two rows, we can observe that the edited scene roughly matches the appearance and effect of IP2P image editing, particularly in hair color. This reveals the semantic alignment for each subtask.

Although our task decomposition uses a linear interpolation of embeddings, our method is agnostic to their exact semantic meaning. Our key insight is to decompose a difficult task into several easier tasks to reduce the inconsistency during distillation (Sec. 3.2). Instead of focusing on the semantic meaning of the interpolated embeddings, we emphasize whether a selected interpolation point effectively decreases difficulty and inconsistency. Therefore, our difficulty metric for adaptive subtask decomposition is designed based on the approximated task difficulty, rather than semantic differences.

Figure C: The edited 3D scene at each decomposed subtask aligns well with the corresponding interpolated tasks edited by IP2P [4] for the image.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have made the claims of contributions in the abstract and introduction (Sec. 1, the last paragraph, "Our contributions"). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: They are discussed in Sec. 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided comprehensive implementation details in Sec. 4. Our method is also straightforward enough to facilitate reproduction. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide our code on our project page. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided all necessary details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Reporting error bars is computationally intensive, as it necessitates generating multiple distinct editing results for a single editing task. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We mention this in the implementation details in Sec. 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: They are discussed in Sec. 5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The method we propose in this paper is a framework to utilize existing generative models instead of proposing a new one, and threfore does not introduce additional risks of misuse beyond the generative models themselves. The safeguards within such generative models are sufficient to prevent misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have credited the papers and licenses of datasets we use (IN2N and ScanNet++). Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.