# Distributionally Robust Skeleton Learning of Discrete Bayesian Networks

 Yeshu Li*

Alibaba Group

liyeshu.lys@alibaba-inc.com &Brian D. Ziebart

Department of Computer Science

University of Illinois at Chicago

bziebart@uic.edu

Work done when Yeshu was a PhD student at UIC.

###### Abstract

We consider the problem of learning the exact skeleton of general discrete Bayesian networks from potentially corrupted data. Building on distributionally robust optimization and a regression approach, we propose to optimize the most adverse risk over a family of distributions within bounded Wasserstein distance or KL divergence to the empirical distribution. The worst-case risk accounts for the effect of outliers. The proposed approach applies for general categorical random variables without assuming faithfulness, an ordinal relationship or a specific form of conditional distribution. We present efficient algorithms and show the proposed methods are closely related to the standard regularized regression approach. Under mild assumptions, we derive non-asymptotic guarantees for successful structure learning with logarithmic sample complexities for bounded-degree graphs. Numerical study on synthetic and real datasets validates the effectiveness of our method.

## 1 Introduction

A Bayesian network is a prominent class of probabilistic graphical models that encodes the conditional dependencies among variables with a directed acyclic graph (DAG). It provides a mathematical framework for formally understanding the interaction among variables of interest, together with computationally attractive factorization for modeling multivariate distributions. If we impose causal relationships on the edges between variables, the model becomes a causal Bayesian network that encodes the more informative causation. Without such interpretation, a Bayesian network serves as a dependency graph for factorization of a multivariate distribution. We focus on discrete Bayesian networks with purely categorical random variables that are not ordinal, but will discuss related work on both discrete and continuous Bayesian networks for completeness.

The DAG structure of a Bayesian network is typically unknown in practice (Natori et al., 2017; Kitson et al., 2023). Structure learning is therefore an important task that infers the structure from data. The _score-based_ approach defines a scoring function that measures the goodness-of-fit of each structure and aims to find an optimal DAG that maximizes the score. Unfortunately, the resulting combinatorial optimization problem is known to be NP-hard (Chickering et al., 2004) without distributional assumptions. Representative approaches include those based on heuristic search (Chickering, 2002), dynamic programming (Silander and Myllymaki, 2006), integer linear programming (Jaakkola et al., 2010) or continuous optimization (Zheng et al., 2018), which either yields an approximate solution or an exact solution in worst-case exponential time. The _constraint-based_ approach (Spirtes and Glymour, 1991; Spirtes et al., 1999; Colombo et al., 2014) performs conditional independence tests to determine the existence and directionality of edges. The time complexity is, however, exponential with the maximum in-degree. Furthermore, the independence test results may be unreliable or inconsistent with the true distribution because of finite samples oreven corrupted samples. In general, without interventional data or assumptions on the underlying distribution, we can only identify a Markov equivalence class (MEC) the true DAG belongs to from observational data where DAGs in the MEC are Markov equivalent, that is, encoding the same set of conditional independencies.

A super-structure is an undirected graph that contains the skeleton as a subgraph which removes directionality from the true DAG. It has been shown that a given super-structure possibly reduces the search space or the number of independence tests to be performed. For example, exact structure learning of Bayesian networks may be (fixed-parameter) tractable (Downey and Fellows, 1995) if the super-structure satisfies certain graph-theoretic properties such as bounded tree-width (Korhonen and Parviainen, 2013; Loh and Buhlmann, 2014), bounded maximum degree (Ordyniak and Szeider, 2013) and the feedback edge number (Ganian and Korchemma, 2021). An incomplete super-structure with missing edges also helps improve the learned DAG with a post-processing hill-climbing method (Tsamardinos et al., 2006; Perrier et al., 2008). Furthermore, a combination of a skeleton and a variable ordering determines a unique DAG structure. Learning the exact skeleton rather than a rough super-structure is desirable in Bayesian network structure learning.

Spirtes and Glymour (1991), Tsamardinos et al. (2006) make use of independence tests to estimate the skeleton. Loh and Buhlmann (2014) learn a super-structure called moralized graph via graphical lasso (Friedman et al., 2008). Shojaie and Michailidis (2010) learn the skeleton assuming an ordering of variables. Bank and Honorio (2020) leverage linear regression for skeleton recovery in polynomial time. These methods either rely on independence test results, which are unstable, or a regularized empirical risk minimization problem, where regularization is usually heuristically chosen to combat overfitting. In practice, the observational data is commonly contaminated by sensor failure, transmission error or adversarial perturbation (Lorch et al., 2022; Sankararaman et al., 2022; Kitson et al., 2023). Sometimes only a small amount of data is available for learning. As a result, the existing algorithms are vulnerable to such distributional uncertainty and may produce false or missing edges in the estimated skeleton.

In this paper, we propose a distributionally robust optimization (DRO) method (Rahimian and Mehrotra, 2019) that solves a node-wise multivariate regression problem (Bank and Honorio, 2020) for skeleton learning of general discrete Bayesian networks to overcome the above limitations. We do not assume any specific form of conditional distributions. We take into account the settings with a small sample size and potential perturbations, which makes the true data generating distribution highly uncertain. Our method explicitly models the uncertainty by constructing an ambiguity set of distributions characterized by certain a priori properties of the true distribution. The optimal parameter is learned by minimizing the worst-case expected loss over all the distributions within the ambiguity set so that it performs uniformly well on all the considered distributions. The ambiguity set is usually defined in such a way that it includes all the distributions close to the empirical distribution in terms of some divergence. With an appropriately chosen divergence measure, the set contains the true distribution with high probability. Hence the worst-case risk can be interpreted as an upper confidence bound of the true risk. The fact that a discrete Bayesian network encompasses an exponential number of states may pose a challenge to solve the DRO problem. We develop efficient algorithms for problems with ambiguity sets defined by Wasserstein distances and Kullback-Leibler (KL) divergences. We show that a group regularized regression method is a special case of our approach. We study statistical guarantees of the proposed estimators such as sample complexities. Experimental results on synthetic and real-world datasets contaminated by various perturbations validate the superior performance of the proposed methods.

### Related Work

Bayesian networks have been widely adopted in a number of applications such as gene regulatory networks (Werhli et al., 2006), medical decision making (Kyrimi et al., 2020) and spam filtering (Manjusha and Kumar, 2010).

In addition to the score-based structure learning methods and constraint-based methods discussed in the introduction section, there are a third class of hybrid algorithms leveraging constraint-based methods to restrict the search space of a score-based method (Tsamardinos et al., 2006; Gasse et al., 2014; Nandy et al., 2018). There is also a flurry of work on score-based methods based on neural networks and continuous optimization (Zheng et al., 2018; Wei et al., 2020; Ng et al., 2020; Yu et al., 2021; Ng et al., 2022; Gao et al., 2022), motivated by differentiable characterization of acyclicitywithout rigorous theoretical guarantees. We refer the interested readers to survey papers (Dron and Maathuis, 2017; Heinze-Deml et al., 2018; Constantinou et al., 2021) for a more thorough introduction of DAG structure learning and causal discovery methods.

Recently, there is an emerging line of work proposing polynomial-time algorithms for DAG learning (Park and Raskutti, 2017; Ghoshal and Honorio, 2017, 2018; Chen et al., 2019; Bank and Honorio, 2020; Gao et al., 2020; Rajendran et al., 2021), among which Bank and Honorio (2020) particularly focuses on general discrete Bayesian networks without resorting to independence tests.

Learning a super-structure can be done by independence tests, graphical lasso or regression, as discussed in introduction. Given a super-structure, how to determine the orientation has been studied by Perrier et al. (2008); Ordyniak and Szeider (2013); Korhonen and Parviainen (2013); Loh and Buhlmann (2014); Ng et al. (2021); Ganian and Korchemna (2021).

DRO is a powerful framework emerging from operations research (Delage and Ye, 2010; Blanchet and Murthy, 2019; Shafieezadeh-Abadeh et al., 2019; Duchi and Namkoong, 2019) and has seen effective applications in many graph learning problems such as inverse covariance estimation (Nguyen et al., 2022), graphical lasso learning (Cisneros-Velarde et al., 2020), graph Laplacian learning (Wang et al., 2021), Markov random field (MRF) parameter learning (Fathony et al., 2018), MRF structure learning (Li et al., 2022) and causal inference (Bertsimas et al., 2022).

## 2 Preliminaries

We introduce necessary background and a baseline method for skeleton learning of Bayesian networks.

### Notations

We refer to \([n]\) as the index set \(\{1,2,\ldots,n\}\). For a vector \(\bm{x}\in\mathbb{R}^{n}\), we use \(x_{i}\) for its \(i\)-th element and \(\bm{x}_{\mathcal{S}}\) for the subset of elements indexed by \(\mathcal{S}\subseteq[n]\) with \(\bar{i}\preceq[n]\backslash\{i\}\). For a matrix \(\bm{A}\in\mathbb{R}^{n\times m}\), we use \(A_{ij}\), \(\bm{A}_{i:}\) and \(\bm{A}_{:j}\) to denote its \((i,j)\)-th entry, \(i\)-th row and \(j\)-th column respectively. \(\bm{A}_{\mathcal{S}\mathcal{T}}\) represents the submatrix of \(\bm{A}\) with rows restricted to \(\mathcal{S}\) and columns restricted to \(\mathcal{T}\subseteq[m]\). We define a row-partitioned block matrix as \(\bm{A}\triangleq[\bm{A}_{1}\bm{A}_{2}\cdots\bm{A}_{k}]^{\intercal}\in\mathbb{ R}^{\sum_{i}n_{i}\times m}\) where \(\bm{A}_{i}\in\mathbb{R}^{n_{i}\times m}\). The \(\ell_{p}\)-norm of a vector \(\bm{x}\) is defined as \(\|\bm{x}\|_{p}\coloneqq(\sum_{i}|x_{i}|^{p})^{1/q}\) with \(|\cdot|\) being the absolute value function. The \(\ell_{p,q}\) norm of a matrix \(\bm{A}\) is defined as \(\|\bm{A}\|_{p,q}\coloneqq(\sum_{j}\|\bm{A}_{:j}\|_{p}^{q})^{1/q}\). When \(p=q=2\), it becomes the Frobenius norm \(\|\cdot\|_{F}\). The operator norm is written as \(\|\bm{A}\|_{p,q}\coloneqq\sup_{\|\bm{x}\|_{p}=1}\|\bm{A}\bm{v}\|_{q}\). The block matrix norm is defined as \(\|\bm{A}\|_{B,p,q}\coloneqq(\sum_{i=1}^{k}\|\bm{A}_{i}\|_{p}^{q})^{1/q}\). The inner product of two matrices is designated by \(\langle\bm{A},\bm{B}\rangle\triangleq\text{Tr}\big{[}\bm{A}^{\intercal}\bm{B} \big{]}\) where \(\bm{A}^{\intercal}\) is the transpose of \(\bm{A}\). Denote by \(\otimes\) the tensor product operation. With a slight abuse of notation, \(|\mathcal{S}|\) stands for the cardinality of a set \(\mathcal{S}\). We denote by \(\bm{1}\) (\(\bm{0}\)) a vector or matrix of all ones (zeros). Given a distribution \(\mathbb{P}\) on \(\Xi\), we denote by \(\mathbb{E}_{\mathbb{P}}\) the expectation under \(\mathbb{P}\). The least \(c\)-Lipschitz constant of a function \(f:\Xi\rightarrow\mathbb{R}\) with a metric \(c:\Xi\times\Xi\rightarrow\mathbb{R}\) is written as \(\text{lip}_{c}(f)\coloneqq\inf\Lambda_{c}(f)\) where \(\Lambda_{c}(f)\coloneqq\{\lambda>0:\forall\xi_{1},\xi_{2}\in\Xi\quad|f(\xi_{1} )-f(\xi_{2})|\leq\lambda c(\xi_{1},\xi_{2})\}\).

### Bayesian Network Skeleton Learning

Let \(\mathbb{P}\) be a discrete joint probability distribution on \(n\) categorical random variables \(\mathcal{V}:=\{X_{1},X_{2},\ldots,X_{n}\}\). Let \(\mathcal{G}\coloneqq(\mathcal{V},\mathcal{E}_{\text{true}})\) be a DAG with edge set \(\mathcal{E}_{\text{true}}\). We use \(X_{i}\) to represent the \(i\)-th random variable or node interchangeably. We call \((\mathcal{G},\mathbb{P})\) a Bayesian network if it satisfies the Markov condition, i.e., each variable \(X_{r}\) is independent of any subset of its non-descendants conditioned on its parents \(\mathbf{Pa}_{r}\). We denote the children of \(X_{r}\) by \(\mathbf{Ch}_{r}\), its neighbors by \(\mathbf{Ne}_{r}\coloneqq\mathbf{Pa}_{r}\cup\mathbf{Ch}_{r}\) and the complement by \(\mathbf{Co}_{r}\coloneqq[n]-\mathbf{Ne}_{r}-\{r\}\). The joint probability distribution can thus be factorized in terms of local conditional distributions:

\[\mathbb{P}(\bm{X})=\mathbb{P}(X_{1},X_{2},\ldots,X_{n})\triangleq\prod_{i=1}^{ n}\mathbb{P}(X_{i}|\mathbf{Pa}_{i}).\]

Let \(\mathcal{G}_{\text{skel}}\coloneqq(\mathcal{V},\mathcal{E}_{\text{skel}})\) be the undirected graph that removes directionality from \(\mathcal{G}\). Given \(m\) samples \(\{\bm{x}^{(i)}\}_{i=1}^{m}\) drawn i.i.d. from \(\mathbb{P}\), the goal of skeleton learning is to estimate \(\mathcal{G}_{\text{skel}}\) from the samples.

We do not assume faithfulness (Spirtes et al., 2000) or any specific parametric form for the conditional distributions. The distribution is faithful to a graph if all (conditional) independencies that hold true in the distribution are entailed by the graph, which is commonly violated in practice (Uhler et al., 2013; Mabrouk et al., 2014). The unavailability of a true model entails a substitute model. Bank and Honorio (2020) propose such a model based on encoding schemes and surrogate parameters.

Assume that each variable \(X_{r}\) takes values from a finite set \(\mathcal{C}_{r}\) with cardinality \(|\mathcal{C}_{r}|>1\). For an indexing set \(\mathcal{S}\subseteq[n]\), define \(\rho_{\mathcal{S}}:=\sum_{i\in\mathcal{S}}|\mathcal{C}_{i}|-1\) and \(\rho_{\mathcal{S}}^{+}:=\sum_{i\in\mathcal{S}}|\mathcal{C}_{i}|\). The maximum cardinality minus one is defined as \(\rho_{\text{max}}:=\max_{i\in[n]}|\mathcal{C}_{i}|-1\). Let \(\mathcal{S}_{r}:=\bigcup_{i\in\mathbf{N}_{\mathbf{e}_{r}}}\{\rho_{[i-1]}+1, \ldots,\rho_{[i]}\}\) be indices for \(\mathbf{N}_{\mathbf{e}_{r}}\) in \(\rho_{[n]}\) and its complement by \(\mathcal{S}_{r}^{c}:=[\rho_{[n]}]-\mathcal{S}_{r}-\{\rho_{[r-1]}+1,\ldots,\rho _{[r]}\}\). Let \(\mathcal{E}:\mathcal{C}_{r}\rightarrow\mathcal{B}^{\rho_{r}}\) be an encoding mapping with a bounded and countable set \(\mathcal{B}\subset\mathbb{R}\). We adopt encoding schemes with \(\mathcal{B}=\{-1,0,1\}\) such as dummy encoding and unweighted effects encoding2 which satisfy a linear independence condition. With a little abuse of notation, we reuse \(\mathcal{E}\) for encoding any \(X_{r}\) and denote by \(\mathcal{E}(\bm{X}_{\mathcal{S}})\in\mathcal{B}^{\rho_{\mathcal{S}}}\) the concatenation of the encoded vectors \(\{\mathcal{E}(X_{i})\}_{i\in\mathcal{S}}\). Consider a linear structural equation model for each \(X_{r}\): \(\mathcal{E}(X_{r})=\bm{W}^{\ast}\mathcal{E}(\bm{X}_{F})+\bm{e}\), where \(\bm{W}^{\ast}\triangleq\big{[}\bm{W}_{1}^{\ast}\cdots\bm{W}_{r-1}^{\ast}\bm{W }_{r+1}^{\ast}\cdots\bm{W}_{n}^{\ast}\big{]}^{\mathsf{T}}\in\mathbb{R}^{\rho_{ r}\times\rho_{r}}\) with \(\bm{W}_{i}^{\ast}\in\mathbb{R}^{\rho_{i}\times\rho_{r}}\) is a surrogate parameter matrix and \(\bm{e}\in\mathbb{R}^{\rho_{r}}\) is a vector of errors not necessarily independent of other quantities. A natural choice of a fixed \(\bm{W}^{\ast}\) is the solution to the following problem given knowledge of the true Bayesian network:

Footnote 2: If there are four variables, dummy encoding may adopt \(\{(1,0,0),(0,1,0),(0,0,1),(0,0,0)\}\) whereas unweighted effects encoding may adopt \(\{(1,0,0),(0,1,0),(0,0,1),(-1,-1,-1)\}\) as encoding vectors.

\[\bm{W}^{\ast}\in\arg\inf_{\bm{W}}\frac{1}{2}\mathbb{E}_{p}\| \mathcal{E}(X_{r})-\bm{W}^{\mathsf{T}}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2} \quad\text{s.t.}\quad\bm{W}_{i}=\bm{0}\quad\forall i\in\mathbf{Co}_{r}.\] (1)

Therefore \(\bm{W}^{\ast}=(\bm{W}_{\mathcal{S}_{r}}^{\ast};\bm{0})\) with \(\bm{W}_{\mathcal{S}_{r}}^{\ast}=\mathbb{E}_{p}[\mathcal{E}(\bm{X}_{F})_{ \mathcal{S}_{r}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}^{\mathsf{T}}]^ {-1}\mathbb{E}_{\mathbb{P}}[\mathcal{E}(\bm{X}_{F})_{\mathcal{S}_{r}}\mathcal{ E}(X_{r})^{\mathsf{T}}]\) is the optimal solution by the first-order optimality condition assuming that \(\mathbb{E}_{p}[\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}\mathcal{E}(\bm{X} _{\bar{r}})_{\mathcal{S}_{r}}^{\mathsf{T}}]\) is invertible. The expression of \(\bm{W}_{\mathcal{S}_{r}}^{\ast}\) captures the intuitions that neighbor nodes should be highly related to the current node \(r\) while the interaction among neighbor nodes should be weak for them to be distinguishable. We further assume that the errors are bounded:

**Assumption 1** (Bounded error).: For the error vector, \(\|\bm{e}\|_{\infty}\leq\sigma\) and \(\|\mathbb{E}_{p}[|\bm{e}|]\|_{\infty}\leq\mu\).

Note that the true distribution does not have to follow a linear structural equation model. Equation (1) only serves as a surrogate model to find technical conditions for successful skeleton learning, which will be discussed in a moment.

The surrogate model under the true distribution indicates that \(\|\bm{W}_{i}^{\ast}\|_{2,2}>0\implies X_{i}\in\mathbf{N}_{\mathbf{e}_{r}}\). This suggests a regularized empirical risk minimization (ERM) problem to estimate \(\bm{W}^{\ast}\):

\[\tilde{\bm{W}}\in\arg\inf_{\bm{W}}\tilde{L}(\bm{W}):=\frac{1}{ 2}\mathbb{E}_{\tilde{\mathbb{P}}_{m}}\|\mathcal{E}(X_{r})-\bm{W}^{\mathsf{T}} \mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}+\tilde{\lambda}\|\bm{W}\|_{B,2,1},\] (2)

where \(\tilde{\lambda}>0\) is a regularization coefficient, the block \(\ell_{2,1}\) norm is adopted to induce sparsity and \(\tilde{\mathbb{P}}_{m}:=\frac{1}{m}\sum_{i=1}^{m}\delta_{\bm{x}^{(i)}}\) stands for the empirical distribution with \(\delta_{\bm{x}^{(i)}}\) being the Dirac point measure at \(\bm{x}^{(i)}\). This approach is expected to succeed as long as only neighbor nodes have a non-trivial impact on the current node, namely, \(\|\bm{W}_{i}^{\ast}\|_{2,2}>0\iff X_{i}\in\mathbf{N}_{\mathbf{e}_{r}}\).

Define the risk of some \(\bm{W}\) under a distribution \(\tilde{\mathbb{P}}\) as

\[R^{\tilde{\mathbb{P}}}(\bm{W}):=\mathbb{E}_{\tilde{\mathbb{P}}} \ell_{\bm{W}}(\bm{X}):=\mathbb{E}_{\tilde{\mathbb{P}}}\frac{1}{2}\|\mathcal{E} (X_{r})-\bm{W}^{\mathsf{T}}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2},\]

where \(\ell_{\bm{W}}(\cdot)\) is the squared loss function. The Hessian of the empirical risk \(R^{\tilde{\mathbb{P}}_{m}}(\bm{W})\) is a block diagonal matrix \(\nabla^{2}R^{\tilde{\mathbb{P}}_{m}}(\bm{W})\triangleq\tilde{\bm{H}}\otimes \bm{I}_{\rho_{r}}\in\mathbb{R}^{\rho_{r}\times\rho_{r}\rho_{r}\rho_{r}}\), where \(\tilde{\bm{H}}:=\mathbb{E}_{\tilde{\mathbb{P}}_{m}}[\mathcal{E}(\bm{X}_{\bar{r}}) \mathcal{E}(\bm{X}_{\bar{r}})^{\mathsf{T}}]\in\mathbb{R}^{\rho_{r}\times\rho_{ r}}\) and \(\bm{I}_{\rho_{r}}\in\mathbb{R}^{\rho_{r}\times\rho_{r}}\) is the identity matrix of dimension \(\rho_{r}\). Similarly under the true distribution, \(\bm{H}:=\mathbb{E}_{\mathbb{P}}[\mathcal{E}(\bm{X}_{\bar{r}})\mathcal{E}(\bm{X} _{\bar{r}})\mathcal{E}(\bm{X}_{\bar{r}})^{\mathsf{T}}]\). As a result, \(\bm{H}\) is independent of the surrogate parameters \(\bm{W}^{\ast}\) thus conditions on the Hessian translate to conditions on a matrix of cross-moments of encodings, which only depend on the encoding function \(\mathcal{E}\) and \(\mathbb{P}\).

In order for this baseline method to work, we make the following assumptions.

**Assumption 2** (Minimum weight).: For each node \(r\), the minimum norm of the true weight matrix \(\bm{W}^{\ast}\) for neighbor nodes is lower bounded: \(\min_{i\in\mathbf{N}_{\mathbf{e}_{r}}}\|\bm{W}_{i}\|_{\bar{r}}\geq\beta>0\).

**Assumption 3** (Positive definiteness of the Hessian).: For each node \(r\), \(\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}}>0\), or equivalently, \(\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}})\geq\Lambda>0\) where \(\Lambda_{\text{min}}(\cdot)\) denotes the minimum eigenvalue.

**Assumption 4** (Mutual incoherence).: For each node \(r\), \(\|\bm{H}_{\tilde{\mathcal{S}}^{c}\mathcal{S}_{r}}\bm{H}_{\mathcal{S}_{r}^{-1} \mathcal{S}_{r}}^{-1}\|_{B,1,\infty}\leqslant 1-\alpha\) for some \(0<\alpha\leqslant 1\).

Assumption 2 guarantees that the influence of neighbor nodes is significant in terms of a non-zero value bounded away from zero, otherwise they will be indistinguishable from those with zero weight. Assumption 3 ensures that Equation (2) yields a unique solution. Assumption 4 is a widely adopted assumption that controls the impact of non-neighbor nodes on \(r\)(Wainwright, 2009; Ravikumar et al., 2010; Daneshmand et al., 2014). One interpretation is that the rows of \(\bm{H}_{\mathcal{S}^{c}\mathcal{S}_{r}}\) should be nearly orthogonal to the rows of \(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}\). Bank and Honorio (2020) show that these assumptions hold for common encoding schemes and finite-sample settings with high probability under mild conditions. They also show that incoherence is more commonly satisfied for the neighbors than the Markov blanket, which justifies the significance of skeleton learning.

Finally, we take the union of all the learned neighbor nodes for each \(r\in[n]\) by solving Equation (2) to get the estimated skeleton \(\tilde{\mathcal{G}}:=(\mathcal{V},\tilde{\mathcal{E}}_{\text{skel}})\).

## 3 Method

As noted in Bank and Honorio (2020), due to model misspecification, even in the infinite sample setting, there is possible discrepancy between the ERM minimizer \(\tilde{\bm{W}}\) and the true solution \(\bm{W}^{*}\), resulting in false or missing edges. In the high-dimensional setting (\(m<n\)) or the adversarial setting, this issue becomes more serious due to limited knowledge about the data-generating mechanism \(\mathbb{P}\).

In this section, we attempt to leverage a DRO framework to incorporate distributional uncertainty into the estimation process. We present efficient algorithms and study the theoretical guarantees of our methods. All technical proofs are deferred to the supplementary materials.

### Basic Formulation

Let \(\mathcal{X}\) be a measurable space of all states of the Bayesian network \((\mathcal{G},\mathbb{P})\), i.e., \(\bm{X}\in\mathcal{X}\). Let \(\mathcal{P}(\mathcal{X})\) be the space of all Borel probability measures on \(\mathcal{X}\). Denote by \(\mathcal{X}^{\varepsilon}:=\{\mathcal{E}(\bm{X}):\forall\bm{X}\in\mathcal{X}\}\) the space of all the allowed encodings.

Instead of minimizing the empirical risk and relying on regularization, we seek a distributionally robust estimator that optimizes the worst-case risk over an ambiguity set of distributions:

\[\hat{\bm{W}}\in\arg\inf_{\bm{W}}\sup_{\mathbb{Q}\in\mathcal{A}}\frac{1}{2} \mathbb{E}_{\mathbb{Q}}\|\mathcal{E}(X_{r})-\bm{W}^{\intercal}\mathcal{E}( \bm{X}_{\bar{r}})\|_{2}^{2},\] (3)

where \(\mathcal{A}\subseteq\mathcal{P}(\mathcal{X})\) is an ambiguity set typically defined by a nominal probability measure \(\tilde{\mathbb{P}}\) equipped with a discrepancy measure \(\text{div}(\cdot,\cdot)\) for two distributions \(\mathcal{A}_{\varepsilon}^{\text{div}}(\tilde{\mathbb{P}}):=\{\mathbb{Q}\in \mathcal{P}(\mathcal{X}):\text{div}(\mathbb{Q},\tilde{\mathbb{P}})\leqslant\varepsilon\}\), where \(\varepsilon\) is known as the ambiguity radius or size. This way of uncertainty quantification can be interpreted as an adversary that captures out-of-sample effect by making perturbations on samples within some budget \(\varepsilon\). Some common statistical distances satisfy \(\text{div}(\mathbb{Q},\mathbb{P})=0\iff\mathbb{Q}=\mathbb{P}\). In this case, if \(\varepsilon\) is set to zero, Equation (3) reduces to Equation (2) without regularization. We will show that the DRO estimator \(\hat{\bm{W}}\) can be found efficiently and encompasses attractive statistical properties with a judicious choice of \(\mathcal{A}\).

### Wasserstein DRO

Wasserstein distances or Kantorovich-Rubinstein metric in optimal transport theory can be interpreted as the cost of the optimal transport plan to move the mass from \(\mathbb{P}\) to \(\mathbb{Q}\) with unit transport cost \(c:\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{+}\). Denote by \(\mathcal{P}_{p}(\mathcal{X})\) the space of all \(\mathbb{P}\in\mathcal{P}(\mathcal{X})\) with finite \(p\)-th moments for \(p\geqslant 1\). Let \(\mathcal{M}(\mathcal{X}^{2})\) be the set of probability measures on the product space \(\mathcal{X}\times\mathcal{X}\). The \(p\)-Wasserstein distance between two distributions \(\mathbb{P},\mathbb{Q}\in\mathcal{P}_{p}(\mathcal{X})\) is defined as \(W_{p}(\mathbb{P},\mathbb{Q}):=\inf_{\Pi\in\mathcal{M}(\mathcal{X}^{2})}\Bigg{\{} \Big{[}\int_{\mathcal{X}^{2}}c^{p}(\bm{x},\bm{x}^{\prime})\Pi(\mathrm{d}\bm{x}, \mathrm{d}\bm{x}^{\prime})\Big{]}^{\frac{1}{p}}:\Pi(\mathrm{d}\bm{x},\mathcal{ X})=\mathbb{P}(\mathrm{d}\bm{x}),\Pi(\mathcal{X},\mathrm{d}\bm{x}^{\prime})= \mathbb{Q}(\mathrm{d}\bm{x}^{\prime})\Bigg{\}}.\)

We adopt the Wasserstein distance of order \(p=1\) as the discrepancy measure, the empirical distribution as the nominal distribution, and cost function \(c(\bm{x},\bm{x}^{\prime})=\|\mathcal{E}(\bm{x})-\mathcal{E}(\bm{x}^{\prime})\|\) for some norm\(\|\cdot\|\). The primal DRO formulation becomes

\[\hat{\bm{W}}\in\arg\inf_{\bm{W}}\sup_{\mathbb{Q}\in\mathcal{A}_{\bm{v}}^{n_{p}}( \mathbb{P}_{m})}\frac{1}{2}\mathbb{E}_{\mathbb{Q}}\|\mathcal{E}(X_{r})-\bm{W}^{ \intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}.\] (4)

According to Blanchet and Murthy (2019), the dual problem of Equation (4) can be written as

\[\inf_{\bm{W},\gamma\geq 0}\gamma\varepsilon+\frac{1}{m}\sum_{i=1}^{m}\sup_{ \bm{x}\in\mathcal{X}}\frac{1}{2}\|\mathcal{E}(x_{r})-\bm{W}^{\intercal} \mathcal{E}(\bm{x}_{\bar{r}})\|_{2}^{2}-\gamma\|\mathcal{E}(\bm{x})-\mathcal{E }(\bm{x}^{(i)})\|.\] (5)

Strong duality holds according to Theorem 1 in Gao and Kleywegt (2022). The inner supremum problems can be solved independently for each \(\bm{x}^{(i)}\). Henceforth, we focus on solving it for some \(i\in[m]\):

\[\sup_{\bm{x}\in\mathcal{X}}\frac{1}{2}\|\mathcal{E}(x_{r})-\bm{W}^{\intercal }\mathcal{E}(\bm{x}_{\bar{r}})\|_{2}^{2}-\gamma\|\mathcal{E}(\bm{x})- \mathcal{E}(\bm{x}^{(i)})\|.\] (6)

Equation (6) is a supremum of \(|\mathcal{X}|\) convex functions of \(\bm{W}\), thus convex. Since \(\mathcal{X}^{\mathcal{E}}\) is a discrete set consisting of a factorial number of points (\(\Pi_{i\in[n]}\rho_{i}\)), unlike the regression problem with continuous random variables in Chen and Paschalidis (2018), we may not simplify Equation (6) into a regularization form by leveraging convex conjugate functions because \(\mathcal{X}^{\mathcal{E}}\) is non-convex and not equal to \(\mathbb{R}^{\rho_{[n]}}\). Moreover, since changing the value of \(x_{j}\) for some \(j\in\bar{r}\) is equivalent to changing \(\bm{W}^{\intercal}\mathcal{E}(\bm{x}_{\bar{r}})\) by a vector, unlike Li et al. (2022) where only a set of discrete labels rather than encodings are dealt with, there may not be a greedy algorithm based on sufficient statistics to find the optimal solution to Equation (6). In fact, let the norm be the \(\ell_{1}\) norm, we can rewrite Equation (6) by fixing the values of \(\|\mathcal{E}(\bm{x})-\mathcal{E}(\bm{x}^{(i)})\|_{1}\):

\[\sup_{\bm{x}\in\mathcal{X},0\leq k\leq\rho_{[n]}^{\intercal},\|\mathcal{E}( \bm{x})-\mathcal{E}(\bm{x}^{(i)})\|_{1}=k}\frac{1}{2}\|\mathcal{E}(x_{r})-\bm {W}^{\intercal}\mathcal{E}(\bm{x}_{\bar{r}})\|_{2}^{2}-\gamma k.\] (7)

If we fix \(k\), Equation (7) is a generalization of the 0-1 quadratic programming problem, which can be transformed into a maximizing quadratic programming (MAXQP) problem. As a result, Equation (6) is an NP-hard problem with proof presented in Proposition 11 in appendix. Charikar and Wirth (2004) develop an algorithm to find an \(\Omega(1/\log n)\) solution based on semi-definite programming (SDP) and sampling for the MAXQP problem. Instead of adopting a similar SDP algorithm with quadratic constraints, we propose a random and greedy algorithm to approximate the optimal solution, which is illustrated in Algorithm 1 in appendix, whose per-iteration time complexity is \(\Theta(n^{2}m\rho_{\text{max}})\). It follows a simple idea that for a random node order \(\bm{\pi}\), we select a partial optimal solution sequentially from \(\pi_{1}\) to \(\pi_{n}\). We enumerate the possible states of the first node to reduce uncertainty. In practice, we find that this algorithm always finds the exact solution that is NP-hard to find for random data with \(n\leq 12\) and \(\rho_{\text{max}}\leq 5\) in most cases.

Since \(\mathcal{X}^{\mathcal{E}}\) is non-convex and not equal to \(\mathbb{R}^{\rho_{[n]}}\), using convex conjugate functions will not yield exact equivalence between Equation (5) and a regularized ERM problem. However, we can draw such a connection by imposing constraints on the dual variables as shown by the following proposition:

**Proposition 5** (Regularization Equivalence).: _Let \(\hat{\bm{W}}:=[\bm{W};-\bm{I}_{\rho_{r}}]^{\intercal}\in\mathbb{R}^{\rho_{[n] }\times\rho_{r}}\) with \(\bm{W}_{r}=-\bm{I}_{\rho_{r}}\). If \(\gamma\geq\rho_{[n]}\|\tilde{\bm{W}}\|_{F}^{2}\), the Wasserstein DRO problem in Equation (5) is equivalent to_

\[\inf_{\bm{W}}\mathbb{E}_{\bar{\bm{W}}_{m}}\frac{1}{2}\|\mathcal{E}(X_{r})-\bm{ W}^{\intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}+\varepsilon\rho_{[n]} \|\tilde{\bm{W}}\|_{F}^{2},\]

_which subsumes a linear regression approach regularized by the Frobenius norm as a special case._

This suggests that minimizing a regularized empirical risk may not be enough to achieve distributional robustness. Note that exact equivalence between DRO and regularized ERM in Chen and Paschalidis (2018) requires \(\mathcal{X}^{\mathcal{E}}=\mathbb{R}^{d}\).

Now we perform non-asymptotic analysis on the proposed DRO estimator \(\hat{\bm{W}}\). First, we would like to show that the solution to the Wasserstein DRO estimator in Equation (4) is unique so that we refer to an estimator unambiguously. Note that Equation (4) is a convex optimization problem but not necessarily strictly convex, and actually never convex in the high-dimensional setting. However, given a sufficient number of samples, the problem becomes strictly convex and yields a unique solution with high probability. Second, we show that the correct skeleton \(\mathcal{E}_{\text{skel}}\) can be recovered with high probability. This is achieved by showing that, for each node \(X_{r}\), the estimator has zero weights for non-neighbor nodes \(\mathbf{C}\mathbf{o}_{r}\) and has non-zero weights for its neighbors \(\mathbf{N}\mathbf{e}_{r}\) with high confidence. Before presenting the main results, we note that they are based on several important lemmas.

**Lemma 6**.: _Suppose \(\Xi\) is separable Banach space and fix \(\mathbb{P}_{0}\in\mathcal{P}(\Xi^{\prime})\) for some \(\Xi^{\prime}\subseteq\Xi\). Suppose \(c:\Xi\to\mathbb{R}_{\geq 0}\) is closed convex, \(k\)-positively homogeneous. Suppose \(f:\Xi\to\mathcal{Y}\) is a mapping in the Lebesgue space of functions with finite first-order moment under \(\mathbb{P}_{0}\) and upper semi-continuous with finite Lipschitz constant \(\text{lip}_{c}(f)\). Then for all \(\varepsilon\geq 0\), the following inequality holds with probability \(1\): \(\sup_{\mathbb{Q}\in\mathcal{A}_{r}^{W_{p}}(\mathbb{P}_{0}),\mathbb{Q}\in \mathcal{P}(\Xi^{\prime})}\int f(\xi^{\prime})\mathbb{Q}(\mathrm{d}\xi^{ \prime})\leq\text{clip}_{c}(f)+\int f(\xi^{\prime})\mathbb{P}_{0}(\mathrm{d} \xi^{\prime})\)._

Lemma 6 follows directly from Cranko et al. (2021) and allows us to obtain an upper bound between the worst-case risk and empirical risk. It is crucial for the following finite-sample guarantees.

**Lemma 7**.: _If Assumption 3 holds, for any \(\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}(\tilde{\mathbb{P}}_{m})\), with high probability, \(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}}\) is positive definite._

**Lemma 8**.: _If Assumption 3 and Assumption 4 hold, for any \(\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}(\tilde{\mathbb{P}}_{m})\) and \(\alpha\in(0,1]\), with high probability,_

\[\|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}}(\bm{H}_{\mathcal{S}_{r }\mathcal{S}_{r}}^{\mathbb{Q}})^{-1}\|_{B,1,\infty}\leq 1-\frac{\alpha}{2}.\]

The above two lemmas illustrate that Assumption 3 and Assumption 4 hold in the finite-sample setting. Let the estimated skeleton, neighbor nodes and the complement be \(\hat{\mathcal{G}}\coloneqq(\mathcal{V},\hat{\mathcal{E}}_{\text{skel}})\), \(\hat{\mathbf{N}}\mathbf{e}_{r}\) and \(\hat{\mathbf{C}}\mathbf{o}_{r}\) respectively. We derive the following guarantees for the proposed Wasserstein DRO estimator.

**Theorem 9**.: _Given a Bayesian network \((\mathcal{G},\mathbb{P})\) of \(n\) categorical random variables and its skeleton \(\mathcal{G}_{\text{skel}}\coloneqq(\mathcal{V},\mathcal{E}_{\text{skel}})\). Assume that the condition \(\|\bm{W}^{*}\|_{B,2,1}\leq\bar{B}\) holds for some \(\bar{B}>0\) associated with an optimal Lagrange multiplier \(\lambda_{B}^{*}>0\) for \(\bm{W}^{*}\) defined in Equation (1). Suppose that \(\hat{\bm{W}}\) is a DRO risk minimizer of Equation (4) with a Wasserstein distance of order \(1\) and an ambiguity radius \(\varepsilon=\varepsilon_{0}/m\) where \(m\) is the number of samples drawn i.i.d. from \(\mathbb{P}\). Under Assumptions 1, 2, 3, 4, if the number of samples satisfies_

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log{(n/\delta)}+\log{\rho_{[n]}}) \sigma^{2}\rho_{\text{max}}^{4}\rho_{[n]}^{3}}{\min(\mu^{2},1)}),\]

_where \(C\) only depends on \(\alpha\), \(\Lambda\), and if the Lagrange multiplier satisfies_

\[\frac{32\mu\rho_{\text{max}}}{\alpha}<\lambda_{B}^{*}<\frac{\beta}{(\alpha/(4 -2\alpha)+2)\rho_{\text{max}}\sqrt{\rho_{[n]}}}\sqrt{\frac{\Lambda}{4}},\]

_then for any \(\delta\in(0,1]\), \(r\in[n]\), with probability at least \(1-\delta\), the following properties hold:_

* _The optimal estimator_ \(\hat{\bm{W}}\) _is unique._
* _All the non-neighbor nodes are excluded:_ \(\bm{C}\mathbf{o}_{r}\subseteq\hat{\bm{C}}\mathbf{o}_{r}\)_._
* _All the neighbor nodes are identified:_ \(\bm{N}\mathbf{e}_{r}\subseteq\hat{\bm{N}}\mathbf{e}_{r}\)_._
* _The true skeleton is successfully reconstructed:_ \(\mathcal{G}_{\text{skel}}=\hat{\mathcal{G}}_{\text{skel}}\)_._

Proof sketch.: The main idea in the proof follows that in the lasso estimator (Wainwright, 2009). Based on a primal-dual witness construction method and Lemma 8, it can be shown that if we control \(\lambda_{B}^{*}\), a solution constrained to have zero weight for all the non-neighbor nodes is indeed optimal. Furthermore, Lemma 7 implies that there is a unique solution given information about the true neighbors. The uniqueness of the aforementioned optimal solution without knowing the true skeleton is then verified via convexity and a conjugate formulation of the block \(\ell_{2,1}\) norm. Hereby we have shown that the optimal solution to Equation (4) is unique and excluding all the non-neighbor nodes. Next, we derive conditions on \(\lambda_{B}^{*}\) for the estimation bias \(\|\hat{\bm{W}}-\bm{W}^{*}\|_{B,2,\infty}<\beta/2\) to hold, which allows us to recover all the neighbor nodes. In such manner, applying the union bound over all the nodes \(r\in[n]\) leads to successful exact skeleton discovery with high probability.

The results in Theorem 9 encompass some intuitive interpretations. Compared to Theorem 1 in Bank and Honorio (2020), we make more explicit the relationship among \(m\), \(\lambda_{B}^{*}\) and \(\delta\). On one hand, the lower bound of \(\lambda_{B}^{*}\) ensures that a sparse solution excluding non-neighbor nodes is obtained. A large error magnitude expectation \(\mu\) therefore elicits stronger regularization. On the other hand, the upper bound \(\lambda_{B}^{*}\) is imposed to guarantee that all the neighbor nodes are identified with less restriction on \(\bm{W}\). There is naturally a trade-off when choosing \(\bar{B}\) in order to learn the exact skeleton. The sample complexity depends on cardinalities \(\rho_{[n]}\), confidence level \(\delta\), the number of nodes \(n\), the ambiguity level \(\varepsilon_{0}\) and assumptions on errors. The dependence on \(\sigma\) indicates that higher uncertainty caused by larger error norms demands more samples whereas the dependence on \(\mu^{-2}\) results from the lower bound condition on \(\lambda_{B}^{*}\) with respect to \(\mu\). The ambiguity level is set to \(\varepsilon_{0}/m\) based on the observation that obtaining more samples reduces ambiguity of the true distribution. In practice, we find that \(\varepsilon_{0}\) is usually small thus negligible. Note that the sample complexity is polynomial in \(n\). Furthermore, if we assume that the true graph has a bounded degree of \(d\), we find that \(m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log\left(n/\delta\right)+\log n+\log \rho_{\text{max}})\sigma^{2}\rho_{\text{max}}^{2}d^{3}}{\min(\mu^{2},1)})\) is logarithmic with respect to \(n\), consistent with the results in Wainwright (2009).

We introduce constants \(\bar{B}\) and \(\lambda_{B}^{*}\) in order to find a condition for the statements in Theorem 9 to hold. If there exists a \(\bm{W}\) incurring a finite loss, we can always find a solution \(\hat{\bm{W}}\) and let \(\bar{B}\triangleq\max_{\hat{\bm{W}}}\lVert\hat{\bm{W}}\rVert_{B,2,1}\) be the maximum norm of all solutions. Imposing \(\lVert\bm{W}\rVert_{B,2,1}\leq\bar{B}\) is equivalent to the original problem. By Lagrange duality and similar argument for the lasso estimator, there exists a \(\lambda_{B}^{*}\) that finds all the solutions with \(\lVert\hat{\bm{W}}\rVert_{B,2,1}=\bar{B}\). Therefore we have a mapping between \(\varepsilon\) and \(\lambda_{B}^{*}\).

### Kullback-Leibler DRO

In addition to optimal transport, \(\phi\)-divergence is also widely used to construct an ambiguity set for DRO problems. We consider the following definition of a special \(\phi\)-divergence called the KL divergence: \(D(\mathbb{Q}\parallel\mathbb{F})\coloneqq\int_{\mathcal{X}}\ln\frac{\mathbb{Q }(\mathrm{d}\bm{x})}{\mathbb{P}(\mathrm{d}\bm{x})}\mathbb{Q}(\mathrm{d}\bm{x})\), where \(\mathbb{Q}\in\mathcal{P}(\mathcal{X})\) is absolutely continuous with respect to \(\mathbb{P}\in\mathcal{P}(\mathcal{X})\) and \(\frac{\mathbb{Q}(\mathrm{d}\bm{x})}{\mathbb{P}(\mathrm{d}\bm{x})}\) denotes the Radon-Nikodym derivative. A noteworthy property of ambiguity sets based on the KL divergence is absolute continuity of all the candidate distributions with respect to the empirical distribution. It implies that if the true distribution is an absolutely continuous probability distribution, the ambiguity set will never include it. In fact, any other point outside the support of the nominal distribution remains to have zero probability. Unlike the Wasserstein metric, the KL divergence does not measure some closeness between two points, nor does it have some measure concentration results. However, we argue that adopting the KL divergence may bring advantages over the Wasserstein distance since the Bayesian network distribution we study is a discrete distribution over purely categorical random variables. Moreover, as illustrated below, adopting the KL divergence leads to better computational efficiency.

Let \(\mathcal{A}\in\mathcal{A}_{\varepsilon}^{D}(\bar{\mathbb{P}}_{m})\) be the ambiguity set, the dual formulation of Equation (3) follows directly from Theorem 4 in Hu and Hong (2013):

\[\inf_{\bm{W},\gamma>0}\gamma\ln\big{[}\frac{1}{m}\sum_{i\in[m]}e^{\frac{1}{2} \lVert\mathcal{E}(x_{r}^{(i)})-\bm{W}^{\intercal}\mathcal{E}(\bm{x}_{r}^{(i)} )\rVert_{2}^{2}/\gamma}\big{]}+\gamma\varepsilon,\]

which directly minimizes a convex objective. In contrast to the approximate Wasserstein estimator, this KL DRO estimator finds the exact solution to the primal problem by strong duality.

The worst-case risk over a KL divergence ball can be bounded by variance (Lam, 2019), similar to Lipschitz regularization in Lemma 6. Based on this observation, we derive the following results:

**Theorem 10**.: _Suppose that \(\hat{\bm{W}}\) is a DRO risk minimizer of Equation (4) with the KL divergence and an ambiguity radius \(\varepsilon=\varepsilon_{0}/m\). Given the same definitions of \((\mathcal{G},\mathbb{P})\), \(\mathcal{G}_{\text{skel}}\), \(\bar{B}\), \(\lambda_{B}^{*}\), \(m\) in Theorem 9. Under Assumptions 1, 2, 3, 4, if the number of samples satisfies_

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log\left(n/\delta\right)+\log\rho_{[n]} )\sigma^{2}\rho_{\text{max}}^{4}\rho_{\text{\min}}^{3}}{\min(\mu^{2},1)}).\]

_where \(C\) depends on \(\alpha\), \(\Lambda\) while independent of \(n\), and if the Lagrange multiplier satisfies the same condition as in Theorem 9, then for any \(\delta\in(0,1]\), \(r\in[n]\), with probability at least \(1-\delta\), the properties (a)-(d) in Theorem 9 hold._The sample complexities in Theorem 9 and Theorem 10 differ in the constant \(C\) due to the difference between the two probability metrics. Note that \(C\) is independent of \(n\) in both methods. The dependency on \(1/(\lambda_{B}^{\bullet})^{2}\) is absorbed in the denominator because we require that \(\lambda_{B}^{\bullet}-16\mu\rho_{\text{max}}/\alpha>0\). The sample complexities provide a perspective of our confidence on upper bounding the true risk in terms of the ambiguity radius. \(\varepsilon_{0}\) serves as our initial guess on distributional uncertainty and increases the sample complexity only slightly because it is usually dominated by other terms in practice: \(\varepsilon\ll\log(n/\delta)\). Even though the samples are drawn from an adversarial distribution with a proportion of noises, the proposed methods may still succeed as long as the true distribution can be made close to an upper confidence bound.

## 4 Experiments

We conduct experiments3 on benchmark datasets (Scutari, 2010) and real-world datasets (Malone et al., 2015) perturbed by the following contamination models:

Footnote 3: Our code is publicly available at https://github.com/DanielLeee/drslbn.

* **Noisefree model.** This is the baseline model without any noises.
* **Huber's contamination model.** In this model, each sample has a fixed probability of \(\zeta\) to be replaced by a sample drawn from an arbitrary distribution.
* **Independent failure model.** Each entry of a sample is independently corrupted with probability \(\zeta\).

We conduct all experiments on a laptop with an Intel Core i7 2.7 GHz processor. We adopt the proposed approaches based on Wasserstein DRO and KL DRO, the group norm regularization method (Bank and Honorio, 2020), the MMPC algorithm (Tsamardinos et al., 2006) and the GRaSP algorithm (Lam et al., 2022) for skeleton learning. Based on the learned skeletons, we infer a DAG with the hill-climbing (HC) algorithm (Tsamardinos et al., 2006). For the Wasserstein-based method, we leverage Adam (Kingma and Ba, 2014) to optimize the overall objective with \(\beta_{1}=0.9\), \(\beta_{2}=0.990\), a learning rate of \(1.0\), a batch size of \(500\), a maximum of \(200\) iterations for optimization and \(10\) iterations for approximating the worst-case distribution. For the KL-based and standard regularization methods, we use the L-BFGS-B (Byrd et al., 1995) optimization method with default parameters. We set the cardinality of the maximum conditional set to \(3\) in MMPC. The Bayesian information criterion (BIC) (Neath and Cavanaugh, 2012) score is adopted in the HC algorithm. A random mixture of \(20\) random Bayesian networks serves as the adversarial distribution for both contamination models. All hyper-parameters are chosen based on the best performance on random Bayesian networks with the same size as the input one. Each experimental result is taken as an average over \(10\) independent runs. When dealing with real-world datasets, we randomly split the data into two halves for training and testing.

We use the F1-score, or the Dice coefficient (regarding the label of each edge indicating its presence as a binary random variable and considering all possible edges), to evaluate performance on benchmark datasets and BIC for real-world datasets. The results are reported in Table 1 and more results can be found in Table 2 in appendix. We observe that in most cases the proposed DRO methods are comparable to MMPC and MMHC, which are generally the best-performing methods in Bank and Honorio (2020). We illustrate in Figure 1 the results on earthquake by varying the number of samples, corruption level and ambiguity radius or regularization coefficient. Figure 1 (a) suggests that all the methods perfectly recover the true skeleton given more than \(2,000\) samples. The results in Figure 1 (b-c) indicate that, in highly uncertain settings, Wasserstein DRO as well as KL DRO is superior to other approaches. Meanwhile, Table 1 and Figure 1 (a) suggest that the DRO methods and the regularized ERM approach are comparable to MMPC and GRaSP when clean data is given. The sensitivity analysis (Figure 1 (d)) suggests a trade-off between robustness and target performance (F1-score in our case). All the approaches have similar execution time except that Wasserstein DRO is several times slower due to the combinatorial sub-problem of computing the worst-case distribution.

## 5 Discussion and Conclusion

In this paper, we put forward a distributionally robust optimization method to recover the skeleton of a general discrete Bayesian network. We discussed two specific probability metrics, developed

[MISSING_PAGE_FAIL:10]

## References

* Aragam et al. (2015) Bryon Aragam, Arash A Amini, and Qing Zhou. Learning directed acyclic graphs with penalized neighbourhood regression. _arXiv preprint arXiv:1511.08963_, 2015.
* Bank and Honorio (2020) Adarsh Bank and Jean Honorio. Provable efficient skeleton learning of encodable discrete bayes nets in poly-time and sample complexity. In _2020 IEEE International Symposium on Information Theory (ISIT)_, pages 2486-2491. IEEE, 2020.
* Bertsimas et al. (2022) Dimitris Bertsimas, Kosuke Imai, and Michael Lingzhi Li. Distributionally robust causal inference with observational data. _arXiv preprint arXiv:2210.08326_, 2022.
* Blanchet and Murthy (2019) Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* Byrd et al. (1995) Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. _SIAM Journal on scientific computing_, 16(5):1190-1208, 1995.
* Charikar and Wirth (2004) Moses Charikar and Anthony Wirth. Maximizing quadratic programs: Extending grothendieck's inequality. In _45th Annual IEEE Symposium on Foundations of Computer Science_, pages 54-60. IEEE, 2004.
* Chen and Paschalidis (2018) Ruidi Chen and Ioannis C Paschalidis. A robust learning approach for regression models based on distributionally robust optimization. _Journal of Machine Learning Research_, 19(13), 2018.
* Chen et al. (2019) Wenyu Chen, Mathias Drton, and Y Samuel Wang. On causal discovery with an equal-variance assumption. _Biometrika_, 106(4):973-980, 2019.
* Chickering (2002) David Maxwell Chickering. Optimal structure identification with greedy search. _Journal of machine learning research_, 3(Nov):507-554, 2002.
* Chickering et al. (2004) Max Chickering, David Heckerman, and Chris Meek. Large-sample learning of bayesian networks is np-hard. _Journal of Machine Learning Research_, 5:1287-1330, 2004.
* Cisneros-Velarde et al. (2020) Pedro Cisneros-Velarde, Alexander Petersen, and Sang-Yun Oh. Distributionally robust formulation and model selection for the graphical lasso. In _International Conference on Artificial Intelligence and Statistics_, pages 756-765. PMLR, 2020.
* Colombo et al. (2014) Diego Colombo, Marloes H Maathuis, et al. Order-independent constraint-based causal structure learning. _J. Mach. Learn. Res._, 15(1):3741-3782, 2014.
* Constantinou et al. (2021) Anthony C Constantinou, Yang Liu, Kiattikun Chobtham, Zhigao Guo, and Neville K Kitson. Large-scale empirical validation of bayesian network structure learning algorithms with noisy data. _International Journal of Approximate Reasoning_, 131:151-188, 2021.
* Cranko et al. (2021) Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised lipschitz regularisation equals distributional robustness. In _International Conference on Machine Learning_, pages 2178-2188. PMLR, 2021.
* Daneshmand et al. (2014) Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, and Bernhard Schoelkopf. Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm. In _International conference on machine learning_, pages 793-801. PMLR, 2014.
* Delage and Ye (2010) Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with application to data-driven problems. _Operations research_, 58(3):595-612, 2010.
* Downey and Fellows (1995) Rod G Downey and Michael R Fellows. Fixed-parameter tractability and completeness i: Basic results. _SIAM Journal on computing_, 24(4):873-921, 1995.
* Drton and Maathuis (2017) Mathias Drton and Marloes H Maathuis. Structure learning in graphical modeling. _Annual Review of Statistics and Its Application_, 4:365-393, 2017.
* Duchi and Namkoong (2019) John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. _The Journal of Machine Learning Research_, 20(1):2450-2504, 2019.
* Duchi et al. (2019)Rizal Fathony, Ashkan Rezaei, Mohammad Ali Bashiri, Xinhua Zhang, and Brian Ziebart. Distributionally robust graphical models. _Advances in Neural Information Processing Systems_, 31, 2018.
* Friedman et al. (2008) Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. _Biostatistics_, 9(3):432-441, 2008.
* Ganian and Korchemna (2021) Robert Ganian and Viktoriia Korchemna. The complexity of bayesian network learning: Revisiting the superstructure. _Advances in Neural Information Processing Systems_, 34:430-442, 2021.
* Gao et al. (2020) Ming Gao, Yi Ding, and Bryon Aragam. A polynomial-time algorithm for learning nonparametric causal graphs. _Advances in Neural Information Processing Systems_, 33:11599-11611, 2020.
* Gao and Kleywegt (2022) Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. _Mathematics of Operations Research_, 2022.
* Gao et al. (2022) Tian Gao, Debarun Bhattacharjya, Elliot Nelson, Miao Liu, and Yue Yu. Idyno: Learning nonparametric dags from interventional dynamic data. In _International Conference on Machine Learning_, pages 6988-7001. PMLR, 2022.
* Gasse et al. (2014) Maxime Gasse, Alex Aussem, and Haytham Elghazel. A hybrid algorithm for bayesian network structure learning with application to multi-label learning. _Expert Systems with Applications_, 41(15):6755-6772, 2014.
* Ghoshal and Honorio (2017) Asish Ghoshal and Jean Honorio. Learning identifiable gaussian bayesian networks in polynomial time and sample complexity. _Advances in Neural Information Processing Systems_, 30, 2017.
* Ghoshal and Honorio (2018) Asish Ghoshal and Jean Honorio. Learning linear structural equation models in polynomial time and sample complexity. In _International Conference on Artificial Intelligence and Statistics_, pages 1466-1475. PMLR, 2018.
* Hastie et al. (2015) Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity. _Monographs on statistics and applied probability_, 143:143, 2015.
* Heinze-Deml et al. (2018) Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. _Annual Review of Statistics and Its Application_, 5:371-391, 2018.
* Hu and Hong (2013) Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization. _Available at Optimization Online_, 1(2):9, 2013.
* Jaakkola et al. (2010) Tommi Jaakkola, David Sontag, Amir Globerson, and Marina Meila. Learning bayesian network structure using lp relaxations. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_, pages 358-365. JMLR Workshop and Conference Proceedings, 2010.
* Kantorovich and Rubinshtein (1958) Leonid Vasilevich Kantorovich and SG Rubinshtein. On a space of totally additive functions. _Vestnik of the St. Petersburg University: Mathematics_, 13(7):52-59, 1958.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kitson et al. (2023) Neville Kenneth Kitson, Anthony C Constantinou, Zhigao Guo, Yang Liu, and Kiattikun Chobtham. A survey of bayesian network structure learning. _Artificial Intelligence Review_, pages 1-94, 2023.
* Korhonen and Parviainen (2013) Janne Korhonen and Pekka Parviainen. Exact learning of bounded tree-width bayesian networks. In _Artificial Intelligence and Statistics_, pages 370-378. PMLR, 2013.
* Kyrimi et al. (2020) Evangelia Kyrimi, Mariana Raniere Neves, Scott McLachlan, Martin Neil, William Marsh, and Norman Fenton. Medical idioms for clinical bayesian network development. _Journal of Biomedical Informatics_, 108:103495, 2020.
* Lam (2019) Henry Lam. Recovering best statistical guarantees via the empirical divergence-based distributionally robust optimization. _Operations Research_, 67(4):1090-1105, 2019.
* Lam and Kessler (2019)Wai-Yin Lam, Bryan Andrews, and Joseph Ramsey. Greedy relaxations of the sparsest permutation algorithm. In _Uncertainty in Artificial Intelligence_, pages 1052-1062. PMLR, 2022.
* Li et al. (2022) Yeshu Li, Zhan Shi, Xinhua Zhang, and Brian Ziebart. Distributionally robust structure learning for discrete pairwise markov networks. In _International Conference on Artificial Intelligence and Statistics_, pages 8997-9016. PMLR, 2022.
* Loh and Buhlmann (2014) Po-Ling Loh and Peter Buhlmann. High-dimensional learning of linear causal networks via inverse covariance estimation. _The Journal of Machine Learning Research_, 15(1):3065-3105, 2014.
* Lorch et al. (2022) Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard Scholkopf. Amortized inference for causal structure learning. _Advances in Neural Information Processing Systems_, 35:13104-13118, 2022.
* Mabrouk et al. (2014) Ahmed Mabrouk, Christophe Gonzales, Karine Jabet-Chevalier, and Eric Chojnacki. An efficient bayesian network structure learning algorithm in the presence of deterministic relations. In _ECAI 2014_, pages 567-572. IOS Press, 2014.
* Malone et al. (2015) Brandon M Malone, Matti Jarvisalo, and Petri Myllymaki. Impact of learning strategies on the quality of bayesian networks: An empirical evaluation. In _UAI_, pages 562-571. Citeseer, 2015.
* Manjusha and Kumar (2010) K Manjusha and Rakesh Kumar. Spam mail classification using combined approach of bayesian and neural network. In _2010 International Conference on Computational Intelligence and Communication Networks_, pages 145-149. IEEE, 2010.
* Nandy et al. (2018) Preetam Nandy, Alain Hauser, and Marloes H Maathuis. High-dimensional consistency in score-based and hybrid structure learning. _The Annals of Statistics_, 46(6A):3151-3183, 2018.
* Natori et al. (2017) Kazuki Natori, Masaki Uto, and Maomi Ueno. Consistent learning bayesian networks with thousands of variables. In _Advanced Methodologies for Bayesian Networks_, pages 57-68. PMLR, 2017.
* Neath and Cavanaugh (2012) Andrew A Neath and Joseph E Cavanaugh. The bayesian information criterion: background, derivation, and applications. _Wiley Interdisciplinary Reviews: Computational Statistics_, 4(2):199-203, 2012.
* Ng et al. (2020) Ignavier Ng, AmirEmad Ghassami, and Kun Zhang. On the role of sparsity and dag constraints for learning linear dags. _Advances in Neural Information Processing Systems_, 33:17943-17954, 2020.
* Ng et al. (2021) Ignavier Ng, Yujia Zheng, Jiji Zhang, and Kun Zhang. Reliable causal discovery with improved exact search and weaker assumptions. _Advances in Neural Information Processing Systems_, 34:20308-20320, 2021.
* Ng et al. (2022) Ignavier Ng, Shengyu Zhu, Zhuangyan Fang, Haoyang Li, Zhitang Chen, and Jun Wang. Masked gradient-based causal structure learning. In _Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)_, pages 424-432. SIAM, 2022.
* Nguyen et al. (2022) Viet Anh Nguyen, Daniel Kuhn, and Peyman Mohajerin Esfahani. Distributionally robust inverse covariance estimation: The wasserstein shrinkage estimator. _Operations Research_, 70(1):490-515, 2022.
* Ordyniak and Szeider (2013) Sebastian Ordyniak and Stefan Szeider. Parameterized complexity results for exact bayesian network structure learning. _Journal of Artificial Intelligence Research_, 46:263-302, 2013.
* Park and Raskutti (2017) Gunwoong Park and Garvesh Raskutti. Learning quadratic variance function (qvf) dag models via overdispersion scoring (ods). _J. Mach. Learn. Res._, 18:224-1, 2017.
* Perrier et al. (2008) Eric Perrier, Seiya Imoto, and Satoru Miyano. Finding optimal bayesian network given a superstructure. _Journal of Machine Learning Research_, 9(10), 2008.
* Rahimian and Mehrotra (2019) Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. _arXiv preprint arXiv:1908.05659_, 2019.
* Rajendran et al. (2021) Goutham Rajendran, Bohdan Kivva, Ming Gao, and Bryon Aragam. Structure learning in polynomial time: Greedy algorithms, bregman information, and exponential families. _Advances in Neural Information Processing Systems_, 34:18660-18672, 2021.
* Rajendran et al. (2020)Pradeep Ravikumar, Martin J Wainwright, and John D Lafferty. High-dimensional ising model selection using \(\ell_{1}\)-regularized logistic regression. _The Annals of Statistics_, 38(3):1287-1319, 2010.
* Sankararaman et al. (2022) Karthik A Sankararaman, Anand Louis, and Navin Goyal. Robust identifiability in linear structural equation models of causal inference. In _Uncertainty in Artificial Intelligence_, pages 1728-1737. PMLR, 2022.
* Scutari (2010) Marco Scutari. Learning bayesian networks with the bnlearn r package. _Journal of Statistical Software_, 35:1-22, 2010.
* Shafieezadeh-Abadeh et al. (2019) Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. _Journal of Machine Learning Research_, 20(103):1-68, 2019.
* Shojaie and Michailidis (2010) Ali Shojaie and George Michailidis. Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs. _Biometrika_, 97(3):519-538, 2010.
* Silander and Myllymaki (2006) Tomi Silander and Petri Myllymaki. A simple approach for finding the globally optimal bayesian network structure. In _Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence_, pages 445-452, 2006.
* Spirtes and Glymour (1991) Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. _Social science computer review_, 9(1):62-72, 1991.
* Spirtes et al. (1999) Peter Spirtes, Christopher Meek, and Thomas Richardson. An algorithm for causal inference in the presence of latent variables and selection bias. _Computation, causation, and discovery_, 21:211-252, 1999.
* Spirtes et al. (2000) Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. _Causation, prediction, and search_. MIT press, 2000.
* Tsamardinos et al. (2006) Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. _Machine learning_, 65(1):31-78, 2006.
* Uhler et al. (2013) Caroline Uhler, Garvesh Raskutti, Peter Buhlmann, and Bin Yu. Geometry of the faithfulness assumption in causal inference. _The Annals of Statistics_, pages 436-463, 2013.
* Wainwright (2009) Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using \(\ell_{1}\)-constrained quadratic programming (lasso). _IEEE transactions on information theory_, 55(5):2183-2202, 2009.
* Wang et al. (2021) Xiaolu Wang, Yuen-Man Pun, and Anthony Man-Cho So. Distributionally robust graph learning from smooth signals under moment uncertainty. _arXiv e-prints_, pages arXiv-2105, 2021.
* Wei et al. (2020) Dennis Wei, Tian Gao, and Yue Yu. Dags with no fears: A closer look at continuous optimization for learning bayesian networks. _Advances in Neural Information Processing Systems_, 33:3895-3906, 2020.
* Werhli et al. (2006) Adriano V Werhli, Marco Grzegorczyk, and Dirk Husmeier. Comparative evaluation of reverse engineering gene regulatory networks with relevance networks, graphical gaussian models and bayesian networks. _Bioinformatics_, 22(20):2523-2531, 2006.
* Yu et al. (2021) Yue Yu, Tian Gao, Naiyu Yin, and Qiang Ji. Dags with no curl: An efficient dag structure learning approach. In _International Conference on Machine Learning_, pages 12156-12166. PMLR, 2021.
* Zheng et al. (2018) Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. _Advances in Neural Information Processing Systems_, 31, 2018.

Algorithms

The pseudo-code of the greedy algorithm for solving Equation (6) in Wasserstein DRO is illustrated in Algorithm 1.

``` Input:\(\bm{W}\), \(\gamma\), \(\bm{x}^{(i)}\) Output: a solution \(\hat{\bm{x}}\) to Equation (6)  Initialize \(\hat{\bm{x}}=\bm{x}^{(i)}\) for all\((j,x_{j}^{t})\in[n]\times\mathcal{C}_{j}\)do  Get a random permutation \(\bm{\pi}\) over \([n]\) with \(\pi_{1}=j\) for\(k:=2\) to \(n\)do \(x_{\pi_{j}}^{t}\leftarrow\arg\sup_{x_{\pi_{k}}^{t}}\ell_{\bm{W}}(\bm{x}_{\pi_{ [k]}}^{t})-\gamma\|\mathcal{E}(\bm{x}_{\pi_{[k]}}^{t})-\mathcal{E}(\bm{x}_{\pi_ {[k]}}^{(i)})\|\) endfor if\(\bm{x}^{t}\) yields a greater objective than \(\hat{\bm{x}}\)then \(\hat{\bm{x}}\leftarrow\bm{x}^{t}\) endif endfor ```

**Algorithm 1** Greedy Algorithm for the Wasserstein Worst-case Risk

## Appendix B Optimization Details

Define

\[\ell_{\bm{W}}(\bm{X}):=\frac{1}{2}\|\mathcal{E}(X_{r})-\bm{W}^{ \intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}.\]

The Lagrangian dual problem of the Wasserstein DRO problem is

\[\inf_{\bm{W},\gamma\geq 0}f(\bm{W},\gamma):=\gamma\varepsilon+\frac{1}{m} \sum_{i=1}^{m}\sup_{\bm{x}\in\mathcal{X}}\ell_{\bm{W}}(\bm{x})-\gamma\| \mathcal{E}(\bm{x})-\mathcal{E}(\bm{x}^{(i)})\|_{1}.\]

One of its sub-gradients can be computed as

\[\frac{1}{m}\sum_{i=1}^{m}\mathcal{E}(\hat{\bm{x}}_{\bar{r}}^{(i)} )\mathcal{E}(\hat{\bm{x}}_{\bar{r}}^{(i)})^{\intercal}\bm{W}-\mathcal{E}(\hat {\bm{x}}_{\bar{r}}^{(i)})\mathcal{E}(\hat{\bm{x}}_{r}^{(i)})^{\intercal}\in \frac{\partial}{\partial\bm{W}}f\] \[\varepsilon-\frac{1}{m}\sum_{i=1}^{m}\|\mathcal{E}(\hat{\bm{x}}^ {(i)})-\mathcal{E}(\bm{x}^{(i)})\|_{1}\in\frac{\partial}{\partial\gamma}f.\]

For the DRO problem based on the KL divergence:

\[\inf_{\bm{W},\gamma>0}f(\bm{W},\gamma):=\gamma\ln\left[\frac{1}{m}\sum_{i \in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/\gamma}\right]+\gamma\varepsilon,\]

a sub-gradient of which can be computed as

\[\frac{\sum_{i\in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/\gamma}( \mathcal{E}(\bm{x}_{\bar{r}}^{(i)})\mathcal{E}(\bm{x}_{\bar{r}}^{(i)})^{ \intercal}\bm{W}-\mathcal{E}(\bm{x}_{\bar{r}}^{(i)})\mathcal{E}(\bm{x}_{r}^{ (i)})^{\intercal})}{\sum_{i\in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/\gamma}} \in\frac{\partial}{\partial\bm{W}}f\] \[\ln(\frac{1}{m}\sum_{i\in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/ \gamma})-\frac{\sum_{i\in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/\gamma}\cdot\ell_ {\bm{W}}(\bm{x}^{(i)})}{\gamma\sum_{i\in[m]}e^{\ell_{\bm{W}}(\bm{x}^{(i)})/ \gamma}}+\epsilon\in\frac{\partial}{\partial\gamma}f.\]

## Appendix C Technical Proofs

**Proposition 11** (NP-hardness of Wasserstein DRO Supremum).: _The problem in Equation (6) is NP-hard._Proof.: Recall the MAXQP problem:

\[\sum_{i,j=1}^{n}a_{ij}x_{i}x_{j},\quad\text{s.t. }x_{i}\in\{-1,1\}\ \forall i.\]

In Equation (6), let \(\gamma=0\), \(\mathcal{E}(x_{r})=0\), \(\bm{x}_{\bar{r}}\) correspond to \(n\) binary variables taking values in {-1, 1} and \(\mathcal{E}(\bm{x}_{\bar{r}})=\bm{x}_{\bar{r}}\). Let \(\bm{W}\in\mathbb{R}^{n\times n^{2}}\). For all \(i,j\in[n]\), let \(k=(i-1)n+j\). The \(k\)-the column of \(\bm{W}\) satisfies \(W_{ik}=1\), \(W_{jk}=a_{ij}/2\) and \(0\) for the other elements. We have obtained a polynomial-time reduction from an NP-hard problem to Equation (6). 

**Proposition 5** (Regularization Equivalence).: _Let \(\tilde{\bm{W}}:=[\bm{W};-\bm{I}_{\rho_{r}}]^{\intercal}\in\mathbb{R}^{\rho_{[n ]}\times\rho_{r}}\) with \(\bm{W}_{r}=-\bm{I}_{\rho_{r}}\). If \(\gamma\geqslant\rho_{[n]}\|\tilde{\bm{W}}\|_{F}^{2}\), the Wasserstein distributionally robust regression problem in Equation (5) is equivalent to_

\[\inf_{\bm{W}}\mathbb{E}_{\mathbb{P}_{m}}\frac{1}{2}\|\mathcal{E}(X_{r})-\bm{W }^{\intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}+\varepsilon\rho_{[n]}\| \tilde{\bm{W}}\|_{F}^{2},\]

_which subsumes a linear regression approach regularized by the Frobenius norm as a special case._

Proof.: Recapitulating on Equation (6):

\[\sup_{\bm{x}\in\mathcal{X}}\frac{1}{2}\|\mathcal{E}(x_{r})-\bm{W }^{\intercal}\mathcal{E}(\bm{x}_{\bar{r}})\|_{2}^{2}-\gamma\|\mathcal{E}(\bm {x})-\mathcal{E}(\bm{x}^{(i)})\|_{1}.\]

Observe that

\[\|\mathcal{E}(x_{r})-\bm{W}^{\intercal}\mathcal{E}(\bm{x}_{\bar{r }})\|_{2}^{2}\triangleq \|\tilde{\bm{W}}^{\intercal}\mathcal{E}(\bm{x}_{[n]})\|_{2}^{2}\] \[\leqslant \|\tilde{\bm{W}}^{\intercal}\|_{2,0}^{2}\] \[\leqslant \|\tilde{\bm{W}}\|_{1,2}^{2}\] \[\leqslant \rho_{[n]}\|\tilde{\bm{W}}\|_{F}^{2}\] \[\leqslant \gamma.\]

Therefore, for any \(\bm{x}\neq\bm{x}^{(i)}\),

\[\frac{1}{2}\|\mathcal{E}(x_{r})-\bm{W}^{\intercal}\mathcal{E}( \bm{x}_{\bar{r}})\|_{2}^{2}-\gamma\|\mathcal{E}(\bm{x})-\mathcal{E}(\bm{x}^{(i )})\|_{1}-(\frac{1}{2}\|\mathcal{E}(x_{r}^{(i)})-\bm{W}^{\intercal}\mathcal{E} (\bm{x}_{\bar{r}}^{(i)})\|_{2}^{2}-\gamma\|\mathcal{E}(\bm{x}^{(i)})-\mathcal{ E}(\bm{x}^{(i)})\|_{1})\] \[\leqslant \frac{1}{2}(\|\mathcal{E}(x_{r})-\bm{W}^{\intercal}\mathcal{E}( \bm{x}_{\bar{r}})\|_{2}^{2}-\|\mathcal{E}(x_{r}^{(i)})-\bm{W}^{\intercal} \mathcal{E}(\bm{x}_{\bar{r}}^{(i)})\|_{2}^{2})-\gamma\|\mathcal{E}(\bm{x})- \mathcal{E}(\bm{x}^{(i)})\|_{1}\] \[\leqslant \frac{1}{2}(2\gamma)-\gamma\|\mathcal{E}(\bm{x})-\mathcal{E}(\bm {x}^{(i)})\|_{1}\] \[\leqslant \gamma-\gamma\] \[= 0,\]

which implies that the supremum can always be achieved at \(\bm{x}=\bm{x}^{(i)}\). Minimizing over \(\gamma\) leads to

\[\inf_{\bm{W}}\mathbb{E}_{\mathbb{P}_{m}}\frac{1}{2}\|\mathcal{E}(X_{r})-\bm{W }^{\intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}+\varepsilon\rho_{[n]}\| \tilde{\bm{W}}\|_{F}^{2}.\]

**Lemma 6**.: _Suppose \(\Xi\) is separable Banach space and fix \(\mathbb{P}_{0}\in\mathcal{P}(\Xi^{\prime})\) for some \(\Xi^{\prime}\subseteq\Xi\). Suppose \(c:\Xi\to\mathbb{R}_{\geqslant 0}\) is closed convex, \(k\)-positively homogeneous. Suppose \(f:\Xi\to\mathcal{Y}\) is a mapping in the Lebesgue space of functions with finite first-order moment under \(\mathbb{P}_{0}\) and upper semi-continuous with finite Lipschitz constant \(\text{lip}_{c}(f)\). Then for all \(\varepsilon\geqslant 0\), the following inequality holds with probability \(1\):_

\[\sup_{\mathbb{Q}\in\mathcal{A}_{c}^{W_{\mathbb{P}_{0}}}(\mathbb{P}_{0}),\mathbb{ Q}\in\mathcal{P}(\Xi^{\prime})}\int f(\xi^{\prime})\mathbb{Q}(\mathrm{d}\xi^{ \prime})\leqslant\varepsilon\text{lip}_{c}(f)+\int f(\xi^{\prime})\mathbb{P}_{ 0}(\mathrm{d}\xi^{\prime}).\]Proof.: The result follows directly from Theorem 1 in Cranko et al. [2021]:

\[\sup_{\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}(\mathbb{P}_{0}),\mathbb{Q} \in\mathcal{P}(\Xi)}\int f(\xi)\mathbb{Q}(\mathrm{d}\xi)\leq\varepsilon\mathrm{ lip}_{c}(f)+\int f(\xi^{\prime})\mathbb{P}_{0}(\mathrm{d}\xi^{\prime}).\]

Since \(\Xi^{\prime}\subseteq\Xi\), observe

\[\sup_{\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}(\mathbb{P}_{0}), \mathbb{Q}\in\mathcal{P}(\Xi^{\prime})}\int f(\xi^{\prime})\mathbb{Q}(\mathrm{ d}\xi^{\prime})\leq\sup_{\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}( \mathbb{P}_{0}),\mathbb{Q}\in\mathcal{P}(\Xi)}\int f(\xi)\mathbb{Q}(\mathrm{ d}\xi).\]

**Lemma 7**.: _If Assumption 3 holds, for any \(\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{p}}(\tilde{\mathbb{P}}_{m})\), with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m\varepsilon^{2}}{2|\mathcal{S}_{r}|^{2} })}\), we have_

\[\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}}) \geq\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})-4\varepsilon |\mathcal{S}_{r}|^{\frac{1}{2}}-t.\]

Proof.: The minimum eigenvalue of the true covariance matrix \(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}\) satisfies:

\[\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}) \triangleq\min_{\|\bm{v}\|_{2}=1}\bm{v}^{\intercal}\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}\bm{v}\] \[=\min_{\|\bm{v}\|_{2}=1}\bm{v}^{\intercal}\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}^{\mathbb{Q}}\bm{v}+\bm{v}^{\intercal}(\tilde{\bm{H}}_{ \mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{ \mathbb{Q}})\bm{v}+\bm{v}^{\intercal}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}} -\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}})\bm{v}\] \[\leq \Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{ \mathbb{Q}})+\bm{u}^{\intercal}(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r} }-\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\bm{u}+\bm{u}^{ \intercal}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\tilde{\bm{H}}_{\mathcal{S }_{r}\mathcal{S}_{r}})\bm{u},\]

where \(\|\bm{u}\|_{2}=1\) is an eigenvector of \(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}}\) with minimum eigenvalue.

Therefore, \(\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\) can be lower bounded as follows:

\[\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{ \mathbb{Q}})\geq \Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})-\bm {u}^{\intercal}(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\bm{u}-\bm{u}^{\intercal}(\bm{H} _{\mathcal{S}_{r}\mathcal{S}_{r}}-\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r }})\bm{u}\] \[\geq \Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})-| \bm{u}^{\intercal}(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\bm{u}|-\|(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}-\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}})\|_{F},\]

due to the fact that

\[\bm{u}^{\intercal}\bm{H}\bm{u}\leq\Lambda_{\text{max}}(\bm{H})\leq\sqrt{\sum_ {i}(\Lambda_{i}(\bm{H}))^{2}}\leq\|\bm{H}\|_{2,2}.\]

We can obtain an upper bound on \(|\bm{u}^{\intercal}(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\bm{u}|\) based on Lemma 6:

\[|\bm{u}^{\intercal}(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{Q}})\bm{u}|\leq 4|\mathcal{S}_{r}|^{\frac{1}{2}}\varepsilon,\]

because for function \(g(\mathcal{E}(\bm{x}))\coloneqq\bm{u}^{\intercal}\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}\bm{u}\), it can be shown that for any \(\|\mathcal{E}(\bm{x})-\mathcal{E}(\bm{x}^{\prime})\|_{1}=k\) and some \(|\mathcal{S}|=k\),

\[|g(\mathcal{E}(\bm{x}))-g(\mathcal{E}(\bm{x}^{\prime}))|\leq\sum_{k\in \mathcal{S}}\sum_{i\in\mathcal{S}_{r}}|H_{ik}-H_{ik}^{\prime}|u_{i}u_{k}+|H_{ ki}-H_{ki}^{\prime}|u_{k}u_{i}\leq 4k|\mathcal{S}_{r}|^{\frac{1}{2}}.\]

Recall that we assume that the encoding schemes take values in \(\mathcal{B}=\{-1,0,1\}\). Therefore \(\text{lip}_{c}(g)=4|\mathcal{S}_{r}|^{\frac{1}{2}}\).

We derive an upper bound of \(\|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\tilde{\bm{H}}_{\mathcal{S}_{r} \mathcal{S}_{r}})\|_{F}\) as follows. Consider a random variable and its expectation

\[Z_{ij}:=(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}})_{ij}= \frac{1}{m}\sum_{l=1}^{m}\mathcal{E}(\bm{x}_{i}^{(l)})_{i}\mathcal{E}(\bm{x}_{i }^{(l)})_{j}\in[-1/m,1/m]\] \[\mathbb{E}_{p}Z_{ij}=(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})_{ij}.\]

By Hoeffding's inequality, we observe

\[\text{Prob}(|(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}})_{ij}-(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}})_{ij}|\geq t)\leq 2\exp{(-\frac{mt^{2}}{2})},\]for \(t>0\). Setting \(t=\frac{t}{|\mathcal{S}_{r}|}\) for all \(i,j\in\mathcal{S}_{r}\) and applying the union bound,

\[\text{Prob}(\|(\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}})-(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}})\|_{F}\geq t)\leq 2|\mathcal{S}_{r}|^{2}\exp {(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2}})}.\] (8)

To conclude, with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2}})}\), we have

\[\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q})\geq\Lambda_{ \text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})-4\varepsilon|\mathcal{S}_ {r}|^{\frac{1}{2}}-t.\]

**Lemma 8**.: _If Assumption 3 and Assumption 4 hold, for any \(\mathbb{Q}\in\mathcal{A}_{\varepsilon}^{W_{r}}(\tilde{\mathbb{P}}_{m})\) and \(\alpha\in(0,1]\), with probability at least \(1-\mathcal{O}(\exp{(-\frac{Cm}{\rho_{\text{max}}^{2}|\mathcal{S}_{r}|^{3}}+ \log|\mathcal{S}_{r}^{c}|+\log|\mathcal{S}_{r}|))}\) and \(\varepsilon\leq\frac{C}{\rho_{\text{max}}^{2}|\mathcal{S}_{r}|^{3/2}}\),_

\[\|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}(\bm{H}_{\mathcal{S}_{r}\mathcal{ S}_{r}}^{Q})^{-1}\|_{B,1,\infty}\leq 1-\frac{\alpha}{2},\]

_where \(C\) only depends on \(\alpha\), \(\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})\)._

Proof.: We would like to obtain an upper bound for \(\|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}(\bm{H}_{\mathcal{S}_{r}\mathcal{ S}_{r}}^{Q})^{-1}\|_{B,1,\infty}\). We may write

\[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}(\bm{H}_{\mathcal{S}_{ r}\mathcal{S}_{r}}^{Q})^{-1}= \bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}[(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}^{Q})^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}]\] \[+[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}-\bm{H}_{\mathcal{S }_{r}\mathcal{S}_{r}}][(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q})^{-1}-( \bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}]\] \[+\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}})^{-1}\] \[\implies\] \[\|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}(\bm{H}_{\mathcal{S }_{r}\mathcal{S}_{r}}^{Q})^{-1}\|_{B,1,\infty}\leq \|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}\big{[}(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{Q})^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S} _{r}})^{-1}\big{]}\|_{B,1,\infty}\] \[+\|[\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{Q}-\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}][(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q}) ^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}]\|_{B,1,\infty}\] \[+\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}(\bm{H}_{\mathcal{ S}_{r}\mathcal{S}_{r}})^{-1}\|_{B,1,\infty}.\]

By Hoeffding's inequality,

\[\text{Prob}(|(\tilde{\bm{H}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}})_{ij}-(\bm{ H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}})_{ij}|\geq t)\leq 2\exp{(-\frac{mt^{2}}{2})},\]

for \(t>0\). Taking \(t=\frac{t}{\rho_{i}|\mathcal{S}_{r}|}\) and applying the union bound over \(i\in\textbf{Co}_{r}\), we observe that

\[\text{Prob}(\|\tilde{\bm{H}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}} -\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}\|_{B,1,\infty}\geq t)\leq \sum_{i\in\textbf{Co}_{r}}2\rho_{i}|\mathcal{S}_{r}|\exp{(-\frac{mt^{2}}{2 \rho_{i}^{2}|\mathcal{S}_{r}|^{2}})}\] \[\leq 2|\mathcal{S}_{r}^{c}||\mathcal{S}_{r}|\exp{(-\frac{mt^{2}}{2 \rho_{\text{max}}^{2}|\mathcal{S}_{r}|^{2}})}.\]

Similarly, taking \(t=\frac{t}{|\mathcal{S}_{r}|}\),

\[\text{Prob}(\|\tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm {H}_{\mathcal{S}_{r}\mathcal{S}_{r}}\|_{\infty,\infty}\geq t)\leq \sum_{i\in\mathcal{S}_{r}}\sum_{j\in\mathcal{S}_{r}}2\exp{(-\frac{mt^ {2}}{2|\mathcal{S}_{r}|^{2}})}\] \[= 2|\mathcal{S}_{r}|^{2}\exp{(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2} })}.\]

In order to bound \(\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{Q}-\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}\|_{B,1,\infty}\), for \(\mathbb{Q}\neq\tilde{\mathbb{P}}\), consider

\[\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{Q}-\tilde{\bm{H}}_{ \mathcal{S}_{r}^{c}\mathcal{S}_{r}}\|_{B,1,\infty}\leq \|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{Q}\|_{B,1,\infty}+\| \tilde{\bm{H}}_{\mathcal{S}_{r}\mathcal{S}_{r}}\|_{B,1,\infty}\] \[\leq \mathbb{E}_{Q}\|\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{ c}}\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{c}}^{\mathsf{T}}\|_{B,1,\infty}+ \mathbb{E}_{\mathbb{P}_{m}}\|\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{c}} \mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{c}}^{\mathsf{T}}\|_{B,1,\infty}\] \[= \sup_{\mathbb{P}_{m},\mathbb{Q}^{c}\in\bm{A}_{\varepsilon}^{W_{r} }(\mathbb{P}_{m}^{c})}\|\mathbb{E}_{\mathbb{Q}^{c}}\xi_{1}\|\mathcal{E}(\bm{X}_{ \tilde{r}})_{\mathcal{S}_{r}^{c}}\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r} }^{\mathsf{T}}\|_{B,1,\infty}-\mathbb{E}_{\mathbb{P}_{m}}\xi_{2}\|\mathcal{E}(\bm{X}_{ \tilde{r}})_{\mathcal{S}_{r}^{c}}\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r} }^{\mathsf{T}}\|_{B,1,\infty}|,\]where \(\mathbb{Q}^{\prime}\) and \(\tilde{\mathbb{P}}^{\prime}_{m}\) are probability measures on \(\mathcal{X}\times\Xi\) with \(\Xi=\{-1,+1\}\) and identical marginals as \(\mathbb{Q}\) and \(\tilde{\mathbb{P}}_{m}\) respectively. We assume that \(\mathbb{Q}\neq\tilde{\mathbb{P}}\) because otherwise \(\|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\tilde{\bm{H}}_{ \mathcal{S}_{r}\mathcal{S}_{r}}\|_{B,1,\infty}=0\) holds trivially. In this way, the equality is always achieved by some \(\mathbb{Q}^{\prime},\tilde{\mathbb{P}}^{\prime}_{m}\), i.e., setting \(\mathbb{Q}^{\prime}(\mathcal{X},\xi=1)=1\) and \(\tilde{\mathbb{P}}^{\prime}_{m}(\mathcal{X},\xi=-1)=1\).

Define the transport cost function in the ambiguity set \(\mathcal{A}^{W_{p}}_{e}(\tilde{\mathbb{P}}^{\prime}_{m})\) to be \(c^{\prime}((\bm{X}_{1},\xi_{1}),(\bm{X}_{2},\xi_{2})):=\|\mathcal{E}(\bm{X}_{ 1})-\mathcal{E}(\bm{X}_{2})\|_{1}\) with zero cost for \(\xi\). Let \(g(\bm{X},\xi):=\xi_{1}\|\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{c} }\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{S}_{r}^{c}}^{\intercal}\|_{B,1,\infty}\). Consider the Lipschitz constants of \(g\):

\[\text{lip}_{c^{\prime}}(g) \leqslant\sup_{\bm{X},\xi,\mathcal{X}^{\prime},\xi^{\prime}}\frac {|g(\bm{X},\xi)-g(\bm{X}^{\prime},\xi^{\prime})|}{c^{\prime}((\bm{X},\xi),(\bm {X}^{\prime},\xi^{\prime}))}\] \[\leqslant\sup_{\bm{X},\bm{X}^{\prime}}\frac{\|\mathcal{E}(\bm{X} _{\tilde{r}})_{\mathcal{S}_{r}^{c}}\mathcal{E}(\bm{X}_{\tilde{r}})_{\mathcal{ S}_{r}^{c}}^{\intercal}\|_{B,1,\infty}+\|\mathcal{E}(\bm{X}_{\tilde{r}}^{\prime})_{ \mathcal{S}_{r}^{c}}\mathcal{E}(\bm{X}_{\tilde{r}}^{\prime})_{\mathcal{S}_{r }}^{\intercal}\|_{B,1,\infty}}{\|\mathcal{E}(\bm{X})-\mathcal{E}(\bm{X}^{ \prime})\|_{1}}\] \[\leqslant 2\rho_{\text{max}}|\mathcal{S}_{r}|.\] (9)

Therefore, by the Kantorovich-Rubinstein theorem (Kantorovich and Rubinshtein, 1958),

\[\|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}-\tilde {\bm{H}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}\|_{B,1,\infty} \leqslant\sup_{\tilde{\mathbb{P}}^{\prime}_{m},\mathbb{Q}\in \mathcal{A}^{W_{p}}_{e}(\tilde{\mathbb{P}}^{\prime}_{m})}\|\mathbb{E}_{ \mathbb{Q}^{\prime}}g(\bm{X},\xi)-\mathbb{E}_{\tilde{\mathbb{P}}^{\prime}_{m} }g(\bm{X},\xi)|\] \[\leqslant\sup_{\tilde{\mathbb{P}}^{\prime}_{m},\mathbb{Q}\in \mathcal{A}^{W_{p}}_{e}(\tilde{\mathbb{P}}^{\prime}_{m})}\text{lip}_{c^{ \prime}}(g)|\mathbb{E}_{\mathbb{Q}^{\prime}}g(\bm{X},\xi)/\text{lip}_{c^{ \prime}}(g)-\mathbb{E}_{\tilde{\mathbb{P}}^{\prime}_{m}}g(\bm{X},\xi)/\text{lip }_{c^{\prime}}(g)|\] \[\leqslant \sup_{\tilde{\mathbb{P}}^{\prime}_{m},\mathbb{Q}\in\mathcal{A}^{ W_{p}}_{e}(\tilde{\mathbb{P}}^{\prime}_{m})}\text{lip}_{c^{\prime}}(g)W_{1}( \mathbb{Q}^{\prime},\tilde{\mathbb{P}}^{\prime}_{m})\] \[\leqslant \text{lip}_{c^{\prime}}(g)\varepsilon\] \[\leqslant 2\varepsilon\rho_{\text{max}}|\mathcal{S}_{r}|.\]

Similarly,

\[\|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\tilde{\bm{H}}_{ \mathcal{S}_{r}\mathcal{S}_{r}}\|_{\infty,\infty}\leqslant 2\varepsilon|\mathcal{S}_{r}|.\]

Based on the above two inequalities, we find that

\[\|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}-\bm{H} _{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}\|_{B,1,\infty}\leqslant \|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}-\tilde{ \bm{H}}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}\|_{B,1,\infty}+\|\tilde{\bm{H}}_{ \mathcal{S}_{r}^{c}\mathcal{S}_{r}}-\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r }}\|_{B,1,\infty}\] \[\leqslant 2\varepsilon\rho_{\text{max}}|\mathcal{S}_{r}|+t,\] (10)

with probability at least \(1-2|\mathcal{S}_{r}^{c}||\mathcal{S}_{r}|\exp{(-\frac{mt^{2}}{2\rho_{\text{max} }^{2}|\mathcal{S}_{r}|^{2}})}\), and

\[\|\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}\|_{\infty,\infty}\leqslant 2\varepsilon|\mathcal{S}_{r}|+t,\] (11)

with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2}})}\).

Based on Equation (8), we also have

\[\|[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r} \mathcal{S}_{r}}]\|_{F}\leqslant 2\varepsilon|\mathcal{S}_{r}|+t,\] (12)

with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2}})}\).

Next we look at the upper bound on the difference between the inverses of \(\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}\) and \(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}\). Observe that

\[\|(\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}-(\bm{H} _{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}\|_{\infty,\infty}= \|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}[\bm{H}_{\mathcal{S} _{r}\mathcal{S}_{r}}-\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}](\bm{H}^{ \mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}\|_{\infty,\infty}\] \[\leqslant \sqrt{|\mathcal{S}_{r}|}\|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}) ^{-1}[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r} \mathcal{S}_{r}}](\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}\|_{2,2}\] \[\leqslant \sqrt{|\mathcal{S}_{r}|}\|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}) ^{-1}\|_{2,2}\|[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}^{\mathbb{Q}}_{ \mathcal{S}_{r}\mathcal{S}_{r}}]\|_{2,2}\|(\bm{H}^{\mathbb{Q}}_{\mathcal{S}_{r} \mathcal{S}_{r}})^{-1}\|_{2,2}\] \[\leqslant \sqrt{\frac{|\mathcal{S}_{r}|}{\Lambda_{\text{min}}(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}})}}\|[\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}-\bm{H}^{ \mathbb{Q}}_{\mathcal{S}_{r}\mathcal{S}_{r}}]\|_{2,2}\|(\bm{H}^{\mathbb{Q}}_{ \mathcal{S}_{r}\mathcal{S}_{r}})^{-1}\|_{2,2}.\]

[MISSING_PAGE_EMPTY:20]

Using Equation (10) by setting \(t=\frac{\alpha}{12}\sqrt{\frac{\lambda_{\min}(\bm{H}_{S_{r}},\mathcal{S}_{r})}{| \mathcal{S}_{r}|}}\) and \(\varepsilon\leqslant\frac{\alpha}{24\rho_{\max}|\mathcal{S}_{r}|}\sqrt{\frac{ \lambda_{\min}(\bm{H}_{S_{r}},\mathcal{S}_{r})}{|\mathcal{S}_{r}|}}\), we have, with probability at least \(1-2|\mathcal{S}_{r}^{c}|\mathcal{S}_{r}|\exp{(-\frac{m\alpha^{2}\Lambda_{\min} (\bm{H}_{S_{r}},\mathcal{S}_{r})}{28\mathcal{S}_{r}^{2}\max|\mathcal{S}_{r}|^{ 3}})}\) and \(\varepsilon\leqslant\frac{\alpha}{24\rho_{\max}|\mathcal{S}_{r}|}\sqrt{\frac{ \lambda_{\min}(\bm{H}_{S_{r}},\mathcal{S}_{r})}{|\mathcal{S}_{r}|}}\),

\[\|[\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}-\bm{H}_{\mathcal{S }_{r}^{c}\mathcal{S}_{r}}](\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}})^{-1}\|_{ B,1,\infty}\leqslant\frac{\alpha}{6}.\]

For the third term, we obtain the upper bound

\[\|[\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}-\bm{H }_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}][(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{ r}}^{\mathbb{Q}})^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}]\|_{B,1,\infty}\] \[\leqslant \|[\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}-\bm{H }_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}]\|_{B,1,\infty}\|[(\bm{H}_{\mathcal{S}_ {r}\mathcal{S}_{r}}^{\mathbb{Q}})^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{ r}})^{-1}]\|_{\infty,\infty}.\]

Taking \(t=\sqrt{\frac{\alpha}{6}}\) in Equation (14). Taking \(t=\frac{1}{2}\sqrt{\frac{\alpha}{6}}\) and \(2\varepsilon\rho_{\max}|\mathcal{S}_{r}|\leqslant\frac{1}{2}\sqrt{\frac{\alpha }{6}}\) in Equation (10). We establish the upper bound that, with probability at least \(1-2|\mathcal{S}_{r}^{c}||\mathcal{S}_{r}|\exp{(-\frac{m\alpha}{48\rho_{\max} |\mathcal{S}_{r}|^{2}})}-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m\alpha(\Lambda_{ \min}(\bm{H}_{\mathcal{S}_{r}},\mathcal{S}_{r}))^{2}}{192|\mathcal{S}_{r}|^{ 3}})}-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m(\Lambda_{\min}(\bm{H}_{\mathcal{S}_ {r}},\mathcal{S}_{r}))^{2}}{8|\mathcal{S}_{r}|^{2}})}\) and \(\varepsilon\leqslant\min{(\frac{1}{4\rho_{\max}|\mathcal{S}_{r}|}\sqrt{\frac{ \alpha}{6}},\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{8| \mathcal{S}_{r}|}\sqrt{\frac{\alpha}{6}},\frac{\Lambda_{\min}(\bm{H}_{\mathcal{ S}_{r}\mathcal{S}_{r}})}{16|\mathcal{S}_{r}|^{\frac{1}{2}}})}\),

\[\|[\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}-\bm{H}_{\mathcal{S }_{r}^{c}\mathcal{S}_{r}}][(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\mathbb{ Q}})^{-1}-(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{-1}]\|_{B,1,\infty}\leqslant\frac{ \alpha}{6}.\]

For the fourth term, in accordance with Assumption 4,

\[\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}(\bm{H}_{\mathcal{S}_{r}\mathcal{ S}_{r}})^{-1}\|_{B,1,\infty}\leqslant 1-\alpha.\]

In conclusion, we have shown that, with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m\alpha^{2}\Lambda_{\min}(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}})^{2}}{1152(1-\alpha)^{2}\mathcal{S}_{r}|^{ 3}})}-2|\mathcal{S}_{r}^{c}||\mathcal{S}_{r}|\exp{(-\frac{m\alpha^{2}\Lambda_{ \min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{28\mathcal{S}_{r}^{2}\max| \mathcal{S}_{r}|^{3}})}-2|\mathcal{S}_{r}^{c}||\mathcal{S}_{r}||\exp{(-\frac{ m\alpha}{48\rho_{\max}^{2}|\mathcal{S}_{r}|^{2}})}-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m \alpha}{48\rho_{\max}^{2}|\mathcal{S}_{r}|^{2}})}-2|\mathcal{S}_{r}|^{2}\exp{(- \frac{m\alpha}{192|\mathcal{S}_{r}|^{3}}\sqrt{\frac{\alpha}{6}},\frac{\Lambda_ {\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})^{2}}{8|\mathcal{S}_{r}|^{2}})}\) and

\[\varepsilon\leqslant\min(\frac{\alpha}{48(1-\alpha)|\mathcal{S}_ {r}|}\sqrt{\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{| \mathcal{S}_{r}|}},\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{ 16|\mathcal{S}_{r}|^{\frac{1}{2}}},\frac{\alpha}{24\rho_{\max}|\mathcal{S}_{r}|} \sqrt{\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{|\mathcal{S}_{r }|}},\] \[\frac{1}{4\rho_{\max}|\mathcal{S}_{r}|}\sqrt{\frac{\alpha}{6}},\frac{ \Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}{8|\mathcal{S}_{r}|} \sqrt{\frac{\alpha}{6|\mathcal{S}_{r}|}},\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}})}{16|\mathcal{S}_{r}|^{\frac{1}{2}}}),\]

the mutual incoherence condition holds for any worst-case distributions:

\[\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}^{\mathbb{Q}})^{-1}\|_{B,1,\infty}\leqslant 1-\frac{\alpha}{2}.\]

Simplifying the above expressions, with probability at least \(1-\mathcal{O}(\exp{(-\frac{Cm}{\rho_{\max}^{2}|\mathcal{S}_{r}|^{3}}+\log| \mathcal{S}_{r}^{c}|+\log|\mathcal{S}_{r}|)})\) and \(\varepsilon\leqslant\frac{C}{\rho_{\max}|\mathcal{S}_{r}|^{3/2}}\),

\[\|\bm{H}_{\mathcal{S}_{r}^{c}\mathcal{S}_{r}}^{\mathbb{Q}}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}^{\mathbb{Q}})^{-1}\|_{B,1,\infty}\leqslant 1-\frac{\alpha}{2},\]

where \(C\) only depends on \(\alpha\), \(\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})\). 

**Lemma 12**.: _If Assumption 1 holds, then for any \(\mathbb{Q}\in\mathcal{A}_{\epsilon}^{W_{p}}(\bar{\mathbb{P}}_{m})\) and \(\alpha\in(0,1]\), with probability at least \(1-|\mathcal{S}_{r}|\rho_{r}\exp{(-\frac{m\mu^{2}}{2\sigma^{2}})}\), \(\varepsilon\leqslant\frac{\mu}{\sigma}\) and \(\lambda_{B}^{\star}>\frac{32\mu\sqrt{\rho_{r}}(1-\alpha/2)}{\alpha}\), we have_

\[\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}\bm{e}^{ \intercal}\|_{2,\infty}\leqslant\frac{\lambda_{B}^{\star}\alpha}{8(1- \alpha/2)}.\]

_With probability at least \(1-|\bm{C}\bm{o}_{r}|\rho_{r}\exp{(-\frac{m\mu^{2}}{2\sigma^{2}})}\), \(\varepsilon\leqslant\frac{\mu}{\sigma}\) and \(\lambda_{B}^{\star}>\frac{32\mu\sqrt{\rho_Proof.: We start with \(\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{S_{r}}\bm{e}^{\intercal}\|_ {2,\infty}\). After some algeriaic manipulation, we find that

\[\begin{split}\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}} )_{S_{r}}\bm{e}^{\intercal}\|_{2,\infty}&\leq\max_{i\in\mathcal{ S}_{r}}\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{i}\bm{e}\|_{2}\\ &\leq\max_{i\in\mathcal{S}_{r}}\sqrt{\rho_{r}}\max_{j\in\rho_{r} }|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{i}e_{j}|\\ &\leq\max_{i\in\mathcal{S}_{r}}\sqrt{\rho_{r}}\max_{j\in\rho_{r} }\mathbb{E}_{\mathbb{Q}}|\mathcal{E}(\bm{X}_{\bar{r}})_{i}e_{j}|\\ &\leq\max_{i\in\mathcal{S}_{r}}\sqrt{\rho_{r}}\max_{j\in\rho_{r} }\mathbb{E}_{\mathbb{Q}}|e_{j}|.\end{split}\]

Since \(|e_{j}|\) is a bounded random variable according to Assumption 1, we apply Hoeffding's inequality to get

\[\text{Prob}(\mathbb{E}_{\bar{P}_{m}}|e_{j}|\geq\mu+t)\leq\exp{(- \frac{mt^{2}}{2\sigma^{2}})}.\]

Base on a similar argument as Equation (9), we can derive

\[\mathbb{E}_{\mathbb{Q}}|e_{j}|-\mathbb{E}_{\bar{P}_{m}}|e_{j}|\leq 2\varepsilon\sigma,\]

which leads to

\[\text{Prob}(\mathbb{E}_{\mathbb{Q}}|e_{j}|\geq 2 \varepsilon\sigma+\mu+t)\leq\exp{(-\frac{mt^{2}}{2\sigma^{2}})}.\]

Taking the union bound over all \(i\in\mathcal{S}_{r}\) and \(j\in\rho_{r}\), we find that

\[\text{Prob}(\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r} })_{\mathcal{S}_{r}}\bm{e}^{\intercal}\|_{2,\infty}\geq\sqrt{\rho_{r}}(2 \varepsilon\sigma+\mu+t))\leq|\mathcal{S}_{r}|\rho_{r}\exp{(-\frac{mt^{2}}{2 \sigma^{2}})}.\]

Setting \(t=\mu\) and \(\varepsilon\leq\frac{\mu}{\sigma}\) while requiring \(\lambda_{B}^{*}>\frac{32\mu\sqrt{\rho_{r}}(1-\alpha/2)}{\alpha}\). With probability at least \(1-|\mathcal{S}_{r}|\rho_{r}\exp{(-\frac{mt^{2}}{2\sigma^{2}})}\), we have

\[\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{ S}_{r}}\bm{e}^{\intercal}\|_{2,\infty}\leq\frac{\lambda_{B}^{*}\alpha}{8(1- \alpha/2)}.\] (15)

Then we consider \(\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}^{c}} \bm{e}^{\intercal}\|_{B,2,\infty}\):

\[\begin{split}\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r} })_{\mathcal{S}_{r}^{c}}\bm{e}^{\intercal}\|_{B,2,\infty}\leq&\max _{i\in\mathbf{C}_{\bm{e}_{r}}}\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(X_{i})\bm{ e}^{\intercal}\|_{2,2}\\ \leq&\max_{i\in\mathbf{C}_{\bm{e}_{r}}}\sqrt{\rho_{i }\rho_{r}}\max_{j\in\rho_{i},k\in\rho_{r}}|\mathbb{E}_{\mathbb{Q}}\mathcal{E} (X_{i})_{j}e_{k}|\\ \leq&\max_{i\in\mathbf{C}_{\bm{e}_{r}}}\sqrt{\rho_{i }\rho_{r}}\max_{k\in\rho_{r}}\mathbb{E}_{\mathbb{Q}}|e_{k}|.\end{split}\]

Similarly, applying Hoeffding's inequality and the Kantorovich-Rubinstein theorem gives us

\[\text{Prob}(\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r} })_{\mathcal{S}_{r}^{c}}\bm{e}^{\intercal}\|_{B,2,\infty}\geq\sqrt{\rho_{\text {max}}\rho_{r}}(2\varepsilon\sigma+\mu+t))\leq|\mathbf{C}_{\bm{e}_{r}}|\rho_{r} \exp{(-\frac{mt^{2}}{2\sigma^{2}})}.\]

Let \(t=\mu\), \(\varepsilon\leq\frac{\mu}{\sigma}\) and \(\lambda_{B}^{*}>\frac{32\mu\sqrt{\rho_{\text{max}}\rho_{r}}}{\alpha}\) hold, we have, with probability at least \(1-|\mathbf{C}_{\bm{e}_{r}}|\rho_{r}\exp{(-\frac{mt^{2}}{2\sigma^{2}})}\),

\[\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{ S}_{r}^{c}}\bm{e}^{\intercal}\|_{B,2,\infty}\leq\frac{\lambda_{B}^{*}\alpha}{8}.\]

**Theorem 9**.: _Given a Bayesian network \((\mathcal{G},\mathbb{P})\) of \(n\) categorical random variables and its skeleton \(\mathcal{G}_{\text{skel}}\coloneqq(\mathcal{V},\mathcal{E}_{\text{skel}})\). Assume that the condition \(\|\bm{W}^{*}\|_{B,2,1}\leq\bar{B}\) holds for some \(\bar{B}>0\) associated with an optimal Lagrange multiplier \(\lambda_{B}^{*}>0\) for \(\bm{W}^{*}\) defined in Equation (1). Suppose that \(\hat{\bm{W}}\) is a DRO risk minimizer of Equation (4) with a Wasserstein distance of order \(1\) and an ambiguity radius \(\varepsilon=\varepsilon_{0}/m\) where \(m\) is the number of samples drawn i.i.d. from \(\mathbb{P}\). Under Assumptions 1, 2, 3, 4, if the number of samples satisfies_

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log{(n/\delta)}+\log{\rho_{[n]}}) \sigma^{2}\rho_{\text{max}}^{4}\rho_{[n]}^{3}}{\min(\mu^{2},1)}),\]

_where \(C\) only depends on \(\alpha\), \(\Lambda\), and if the Lagrange multiplier satisfies_

\[\frac{32\mu\rho_{\text{max}}}{\alpha}<\lambda_{B}^{\text{*}}<\frac{\beta}{( \alpha/(4-2\alpha)+2)\rho_{\text{max}}\sqrt{\rho_{[n]}}}\sqrt{\frac{\Lambda}{ 4}},\]

_then for any \(\delta\in(0,1]\), \(r\in[n]\), with probability at least \(1-\delta\), the following properties hold:_

* _The optimal estimator_ \(\hat{\bm{W}}\) _is unique._
* _All the non-neighbor nodes are excluded:_ \(\bm{Co}_{r}\subseteq\dot{\bm{Co}}_{r}\)_._
* _All the neighbor nodes are identified:_ \(\bm{Ne}_{r}\subseteq\hat{\bm{N}}\bm{e}_{r}\)_._
* _The true skeleton is successfully reconstructed:_ \(\mathcal{G}_{\text{skel}}=\hat{\mathcal{G}}_{\text{skel}}\)_._

Proof.: We prove the statements in this theorem in several steps. In order to prove (a) and (b), we will show that the DRO problem is strictly convex if true non-neighbors are known so that there is an optimal solution. Next we would like to demonstrate that this solution with a non-neighbor constraint is indeed unique for all the solutions without constraints. The proof for uniqueness comes with a conclusion that we do not accidentally include any edge between the current node and its non-neighbors. Next, to prove (c), we present a generalization bound for the DRO estimator in terms of its true risk, which leads to a \(\ell_{\infty}\) bound of the difference between the estimator \(\hat{\bm{W}}\) and the true weight matrix \(\bm{W}^{\text{*}}\). Combined with the assumption on the minimum weight, it implies that we include all the neighbor nodes successfully. Finally, by taking a union bound for all the nodes, we could conclude that the correct skeleton is recovered with high probability, which proves (d).

**(i) Given the true non-neighbors, there is a unique solution.**

We start with the Wasserstein DRO problem, which we recapitulate here for convenience:

\[\hat{\bm{W}}\in\arg\inf_{\bm{W}}\sup_{\mathbb{Q}\in\mathcal{A}_{c}^{\bm{W}_{p }}(\tilde{\mathbb{P}}_{m})}\frac{1}{2}\mathbb{E}_{\mathbb{Q}}\|\mathcal{E}(X_ {r})-\bm{W}^{\intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}.\]

The objective is convex because it is a supremum of convex functions.

For now, we assume that the non-neighbor nodes \(\bm{Co}_{r}\) are given. We can then explicitly restrict \(\bm{W}_{i}=\bm{0}\) for all \(i\in\bm{Co}_{r}\). The Hessian of \(\bm{W}_{\mathcal{S}_{r}}\) is a block diagonal matrix reads

\[\nabla^{2}R^{\mathbb{Q}}(\bm{W}_{\mathcal{S}_{r},\cdot})=\begin{bmatrix}\bm{H }_{\mathcal{S}_{r},\mathcal{S}_{r}}^{\mathbb{Q}}&\bm{0}&\cdots&\bm{0}\\ \bm{0}&\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}}^{\mathbb{Q}}&\cdots&\bm{0}\\ \vdots&\vdots&\ddots&\vdots\\ \bm{0}&\bm{0}&\cdots&\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}}^{\mathbb{Q}} \end{bmatrix}\in\mathbb{R}^{\rho_{r}\rho_{\bm{W}_{r}}\times\rho_{r}\rho_{\bm{ W}_{r}}},\]

where

\[\bm{H}^{\mathbb{Q}}:=\mathbb{E}_{\mathbb{Q}}[\mathcal{E}(\bm{X}_{\bar{r}}) \mathcal{E}(\bm{X}_{\bar{r}})^{\intercal}]\in\mathbb{R}^{\rho_{r}\times\rho_{ r}}\]

is the covariance matrix of encodings of \(\bm{X}_{\bar{r}}\) under some distribution \(\mathbb{Q}\in\mathcal{A}_{c}^{W_{p}}(\tilde{\mathbb{P}}_{m})\).

Since \(\bm{W}_{\mathcal{S}_{r},\cdot}\) is fixed to be zero and \(\nabla^{2}R^{\mathbb{Q}}(\bm{W}_{\mathcal{S}_{r},\cdot})\) is a block diagonal matrix, we focus on showing that \(\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}}^{\mathbb{Q}}\succ\bm{0}\).

We apply Lemma 7 to get the bound

\[\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}}^{\mathbb{Q}}) \geq\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r},\mathcal{S}_{r}})-4\varepsilon |\mathcal{S}_{r}|^{\frac{1}{2}}-t,\]with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{mt^{2}}{2|\mathcal{S}_{r}|^{2}})}\). \(\Lambda_{\text{min}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})-4\varepsilon| \mathcal{S}_{r}|^{\frac{1}{2}}-t>0\) will guarantee that the DRO problem in Equation (4) has a unique solution when the \(\bm{W}_{i}=\bm{0}\) is satisfied for non-neighbor nodes.

**(ii) Given the true non-neighbors, the solution is optimal.**

We would like to show that the solution to Equation (4) with true non-neighbor constraints is optimal. In this way, we do not recover any non-neighbor nodes in the skeleton. We adopt the primal-dual witness (PDW) (Wainwright, 2009) method to show optimality for the constrained unique solution.

Recall that we assume \(\|\bm{W}\|_{B,2,1}\leq\bar{B}\). To begin with, we write the dual problem as

\[\hat{\bm{W}}\in\arg\inf_{\bm{W}}\sup_{\mathbb{Q}\in\bm{A}_{r}^{ \mathsf{N}_{F}}(\mathbb{P}_{m}),\|\bm{Z}\|_{B,2,\infty}\leq 1,\lambda_{B} \geq 0}\frac{1}{2}\mathbb{E}_{\mathbb{Q}}\|\mathcal{E}(X_{r})-\bm{W}^{ \intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}+\lambda_{B}(\langle\bm{Z}, \bm{W}\rangle-\bar{B})\] (16)

s.t. \(\forall i\in\text{{Co}}_{r}\quad\bm{W}_{i}=\bm{0}\),

where \(\lambda_{B}\) is the Lagrange multiplier for the norm constraint on \(\bm{W}\).

\(\hat{\bm{W}}\) is optimal if and only if there exists \((\mathbb{Q}^{*},\bm{Z}^{*},\lambda_{B}^{*})\) that satisfies the KKT condition:

\[\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})\mathcal{ E}(\bm{X}_{\bar{r}})^{\intercal}\hat{\bm{W}}-\mathbb{E}_{\mathbb{Q}^{*}} \mathcal{E}(\bm{X}_{\bar{r}})\mathcal{E}(\bm{X}_{r})^{\intercal}+\lambda_{B}^ {*}\bm{Z}^{*}=\bm{0}\] \[\mathbb{Q}^{*}\in\mathcal{A}_{\bar{r}}^{\mathsf{N}_{F}}(\mathbb{P }_{m}),\|\bm{Z}^{*}\|_{B,2,\infty}\leq 1,\lambda_{B}^{*}\geq 0,\|\hat{\bm{W}}\|_{B, 2,1}\leq\bar{B}\] \[\langle\bm{Z}^{*},\hat{\bm{W}}\rangle=\|\hat{\bm{W}}\|_{B,2,1}, \lambda_{B}^{*}(\|\hat{\bm{W}}\|_{B,2,1}-\bar{B})=0.\]

Note that we assume that the constraint \(\|\bm{W}\|_{B,2,1}\leq\bar{B}\) is active such that \(\lambda_{B}^{*}>0\). This assumption is only for convenience of theoretical analysis and not restrictive. If it is not active, we have \(\|\hat{\bm{W}}\|_{B,2,1}=\check{B}<\bar{B}\) for some \(\check{B}\) and \(\lambda_{B}^{*}=0\), which leads to an unconstrained problem similar to the ordinary least square problem, which is known to suffer from overfitting. Instead, we are usually interested in solutions that have finite norms so we can always find \(\bar{B}=\check{B}-\epsilon<\check{B}\) for some small positive constant \(\epsilon>0\) to make the constraint active and thus \(\lambda_{B}^{*}>0\).

Substituting \(\mathcal{E}(X_{r})=\bm{W}^{*\intercal}\mathcal{E}(\bm{X}_{\bar{r}})+\bm{e}\) into the first-order optimality condition yields

\[\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})\mathcal{ E}(\bm{X}_{\bar{r}})^{\intercal}(\hat{\bm{W}}-\bm{W}^{*})-\mathbb{E}_{\mathbb{Q}^{*}} \mathcal{E}(\bm{X}_{\bar{r}})\bm{e}^{\intercal}+\lambda_{B}^{*}\bm{Z}^{*}=\bm{ 0}\] \[\Longleftrightarrow\begin{bmatrix}\bm{H}_{\mathcal{S}_{r}\mathcal{ S}_{r}}^{\otimes\bm{*}}&\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}}\\ \bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S}_{r}}^{\otimes\bm{*}}&\bm{H}_{\mathcal{ S}_{r}^{\circ}\mathcal{S}_{r}^{\circ}}^{\otimes\bm{*}}\end{bmatrix}\begin{bmatrix} \hat{\bm{W}}_{\mathcal{S}_{r}^{\circ}}-\bm{W}_{\mathcal{S}_{r}}^{*}\\ \bm{0}\end{bmatrix}-\begin{bmatrix}\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X }_{\bar{r}})_{\mathcal{S}_{r}}\bm{e}^{\intercal}\\ \mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}^{ \circ}}\bm{e}^{\intercal}\end{bmatrix}+\lambda_{B}^{*}\begin{bmatrix}\bm{Z}_{ \mathcal{S}_{r}^{\circ}}^{*}\\ \bm{Z}_{\mathcal{S}_{r}^{\circ}}^{\otimes\bm{*}}\end{bmatrix}=\begin{bmatrix} \bm{0}\\ \bm{0}\end{bmatrix}.\] (17)

Solving for \(\bm{Z}_{\mathcal{S}_{r}^{\circ}}^{*}\), we find that

\[\lambda_{B}^{*}\bm{Z}_{\mathcal{S}_{r}^{\circ}}^{*}=\lambda_{B}^{* }\bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S}_{r}}^{\otimes\bm{*}}(\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1}\bm{Z}_{\mathcal{S}_{r}}^{ \otimes\bm{*}}-\bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S}_{r}}^{\otimes\bm{*}}( \bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1}\mathbb{E}_{ \mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}\bm{e}^{\intercal }+\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}^{ \circ}}\bm{e}^{\intercal},\]

which can be bounded such that

\[\lambda_{B}^{*}\|\bm{Z}_{\mathcal{S}_{r}^{\circ}}^{*}\|_{B,2,\infty}\] \[= \|\lambda_{B}^{*}\bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S}_{r}}^{ \otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1} \bm{Z}_{\mathcal{S}_{r}}^{\otimes\bm{*}}-\bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S} _{r}}^{\otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1 }\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}\bm{e}^{ \intercal}\|_{B,2,\infty}\] \[\leq \lambda_{B}^{*}\|\bm{H}_{\mathcal{S}_{r}^{\circ}\mathcal{S}_{r}}^{ \otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1} \bm{Z}_{\mathcal{S}_{r}^{\circ}\|_{B,2,\infty}}+\|\bm{H}_{\mathcal{S}_{r}^{ \circ}\mathcal{S}_{r}}^{\otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{ \otimes\bm{*}})^{-1}\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{ \mathcal{S}_{r}}\bm{e}^{\intercal}\|_{B,2,\infty}+\|\mathbb{E}_{\mathbb{Q}^{*}} \mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}^{\circ}}\bm{e}^{\intercal}\|_{B,2,\infty}\] \[\leq \lambda_{B}^{*}\|\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{ \otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}})^{-1} \|_{B,1,\infty}\|\bm{Z}_{\mathcal{S}_{r}^{\circ}}^{*}\|_{2,\infty}+\|\bm{H}_{ \mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes\bm{*}}(\bm{H}_{\mathcal{S}_{r}\mathcal{S} _{r}}^{\otimes\bm{*}})^{-1}\|_{B,1,\infty}\|\mathbb{E}_{\mathbb{Q}^{*}} \mathcal{E}(\bm{X}_{\bar{r}})_{\mathcal{S}_{r}}\bm{e}^{\intercal}\|_{2,\infty}\] \[+\|\mathbb{E}_{\mathbb{Q}^{*}}\mathcal{E}(\bm{X}_{\bar{r}})_{ \mathcal{S}_{r}^{\circ}

Recall that \(0<\alpha\leq 1\) in Assumption 4. Based on Lemma 8 and Lemma 12, we may write

\[\lambda_{B}^{\ast}\|\bm{Z}_{S^{c}_{r}}^{\ast}\|_{B,2,\infty}\] \[\leq \lambda_{B}^{\ast}\|\bm{H}_{S^{c}_{r}}^{\ast}(\bm{H}_{S^{c}_{r}}^{ \ast})^{-1}\|_{B,1,\infty}\|\bm{Z}_{S^{c}_{r}}^{\ast}\|_{2,\infty}+\|\bm{H}_{S^ {c}_{r}}^{\ast}(\bm{H}_{S^{c}_{r}}^{\ast})^{-1}\|_{B,1,\infty}\|\mathbb{E}_{ \mathbb{Q}^{\ast}}\mathcal{E}(\bm{X}_{\bar{r}})_{S_{r}}\bm{e}^{\intercal}\|_{2,\infty}\] \[+\|\mathbb{E}_{\mathbb{Q}^{\ast}}\mathcal{E}(\bm{X}_{\bar{r}})_{S^ {c}_{r}}\bm{e}^{\intercal}\|_{B,2,\infty}\] \[\leq \lambda_{B}^{\ast}(1-\frac{\alpha}{2})+(1-\frac{\alpha}{2})(\frac {\lambda_{B}^{\ast}\alpha}{8(1-\alpha/2)})+\frac{\lambda_{B}^{\ast}\alpha}{8}\] \[\leq \lambda_{B}^{\ast}(1-\frac{\alpha}{4})\] \[< \lambda_{B}^{\ast},\]

with high probability and certain conditions on \(\lambda_{B}^{\ast}\) and \(\varepsilon\).

Henceforth, \(\|\bm{Z}_{S^{c}_{r}}^{\ast}\|_{B,2,\infty}<1\) satisfies strict dual feasibility and we must have \(\|\hat{\bm{W}}_{S^{c}_{r}}\|_{B,2,1}=0\) according to complementary slackness: \(\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle=\|\hat{\bm{W}}\|_{B,2,1}\). In other words, we have

\[\forall i\in\mathbf{Co}_{r}\quad\hat{\bm{W}}_{i}=\bm{0},\]

with high probability. This guarantees that we do not recover any node that is not a neighbor of \(r\) with high probability.

**(iii) Without information about the true skeleton, we have a unique and optimal solution.**

We follow the proof of Lemma 11.2 in Hastie et al. (2015).

We have shown that \(\hat{\bm{W}}\) satisfying \(\hat{\bm{W}}_{i}=\bm{0}\quad\forall i\in\mathbf{Co}_{r}\) is an optimal solution with optimal dual variables \(\|\bm{Z}_{S^{c}_{r}}^{\ast}\|_{B,2,\infty}<1\).

To avoid clutter of notations, we define

\[L^{\text{DRO}}(\bm{W}):=\sup_{\mathbb{Q}\in\mathcal{A}_{c}^{\text{W}_{p}}( \bar{\mathbb{P}}_{m})}\frac{1}{2}\mathbb{E}_{\mathbb{Q}}\|\mathcal{E}(X_{r})- \bm{W}^{\intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}.\]

Let \((\hat{\bm{W}},\hat{\lambda})\) be any other optimal solution to \(\inf_{\bm{W}}\sup_{\lambda}L^{\text{DRO}}(\bm{W})+\lambda(\|\bm{W}\|_{B,2,1}- \bar{B})\). By definition,

\[L^{\text{DRO}}(\hat{\bm{W}})+\tilde{\lambda}(\|\hat{\bm{W}}\|_{ B,2,1}-\bar{B})=L^{\text{DRO}}(\hat{\bm{W}})+\lambda_{B}^{\ast}(\langle\bm{Z}^{ \ast},\hat{\bm{W}}\rangle-\bar{B})\] \[\iff L^{\text{DRO}}(\hat{\bm{W}})+\tilde{\lambda}(\|\hat{\bm{W}}\|_{ B,2,1}-\bar{B})-\lambda_{B}^{\ast}\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle=L^{ \text{DRO}}(\hat{\bm{W}})+\lambda_{B}^{\ast}(\langle\bm{Z}^{\ast},\hat{\bm{W} }-\hat{\bm{W}}\rangle-\bar{B}).\]

The first-order optimality condition for \(\hat{\bm{W}}\) says

\[\nabla L^{\text{DRO}}(\hat{\bm{W}})+\lambda_{B}^{\ast}\bm{Z}^{\ast}=\bm{0},\]

which implies

\[\tilde{\lambda}(\|\hat{\bm{W}}\|_{B,2,1}-\bar{B})+\lambda_{B}^{\ast}(\bar{B}- \langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle)=L^{\text{DRO}}(\hat{\bm{W}})+\langle \nabla L^{\text{DRO}}(\hat{\bm{W}}),\check{\bm{W}}-\hat{\bm{W}}\rangle-L^{ \text{DRO}}(\check{\bm{W}}).\]

By definition, \(\|\hat{\bm{W}}\|_{B,2,1}-\bar{B}=0\) and \(\lambda_{B}^{\ast}>0\). Since \(L^{\text{DRO}}(\cdot)\) is convex, the RHS of the above equation should be non-positive, or equivalently,

\[\|\hat{\bm{W}}\|_{B,2,1}\leq\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle.\]

On the other hand,

\[\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle\leq\|\bm{Z}^{\ast}\|_{B,2,\infty}\| \hat{\bm{W}}\|_{B,2,1}\leq\|\check{\bm{W}}\|_{B,2,1}.\]

Therefore, the equality holds for the above inequalities, which leads to

\[\|\check{\bm{W}}\|_{B,2,1}=\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle.\]

Recall that \(\|\bm{Z}_{S^{c}_{r}}^{\ast}\|_{B,2,\infty}<1\). In order for \(\|\hat{\bm{W}}\|_{B,2,1}=\langle\bm{Z}^{\ast},\hat{\bm{W}}\rangle\) to hold, we must have

\[\hat{\bm{W}}_{S^{c}_{r}}=\bm{0}.\]In that wise, all the optimal solutions \(\hat{\bm{W}}\) have

\[\hat{\bm{W}}_{i}=\bm{0}\quad\forall i\in\mathbf{Co}_{r}.\]

This implies that we have a unique solution that excludes all the non-neighbor nodes without information about the true skeleton. Until now, we have proven properties (a) and (b).

**(iv) The set of correct neighbors is recovered.**

Consider again the first-order optimality condition in Equation (17),

\[\hat{\bm{W}}_{\mathcal{S}_{r}}\!-\bm{W}_{\mathcal{S}_{r}}^{*}= (\bm{H}_{\mathcal{S}_{r}}^{\otimes*})^{-1}(\mathbb{E}_{\mathbb{Q} *}\bm{\mathcal{E}}(\bm{X}_{\mathcal{F}})_{\mathcal{S}_{r}}\bm{e}^{\intercal}- \lambda_{B}^{*}\bm{Z}_{\mathcal{S}_{r}}^{*})\] \[\implies\|\hat{\bm{W}}_{\mathcal{S}_{r}}\!-\bm{W}_{\mathcal{S}_{r} }^{*}\|_{B,2,\infty}= \|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes*})^{-1}( \mathbb{E}_{\mathbb{Q}*}\bm{\mathcal{E}}(\bm{X}_{\mathcal{F}})_{\mathcal{S}_{ r}}\bm{e}^{\intercal}-\lambda_{B}^{*}\bm{Z}_{\mathcal{S}_{r}}^{*})\|_{B,2,\infty}\] \[\leqslant \|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes*})^{-1}\|_{B, 1,\infty}\|\mathbb{E}_{\mathbb{Q}*}\bm{\mathcal{E}}(\bm{X}_{\mathcal{F}})_{ \mathcal{S}_{r}}\bm{e}^{\intercal}-\lambda_{B}^{*}\bm{Z}_{\mathcal{S}_{r}}^{*} \|_{2,\infty}\] \[\leqslant \|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes*})^{-1}\|_{B, 1,\infty}(\|\mathbb{E}_{\mathbb{Q}*}\bm{\mathcal{E}}(\bm{X}_{\mathcal{F}})_{ \mathcal{S}_{r}}\bm{e}^{\intercal}\|_{2,\infty}+\|\lambda_{B}^{*}\bm{Z}_{ \mathcal{S}_{r}}^{*}\|_{2,\infty})\] \[\leqslant \rho_{\max}\|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{\otimes*}) ^{-1}\|_{\infty,\infty}(\|\mathbb{E}_{\mathbb{Q}*}\bm{\mathcal{E}}(\bm{X}_{ \mathcal{F}})_{\mathcal{S}_{r}}\bm{e}^{\intercal}\|_{2,\infty}+\lambda_{B}^{*})\] \[\leqslant \rho_{\max}\sqrt{|\mathcal{S}_{r}|}\|(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}^{\otimes*})^{-1}\|_{2,2}(\|\mathbb{E}_{\mathbb{Q}*}\bm{ \mathcal{E}}(\bm{X}_{\mathcal{F}})_{\mathcal{S}_{r}}\bm{e}^{\intercal}\|_{2, \infty}+\lambda_{B}^{*}).\]

According to Equation (13), with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m(\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}))^{2}}{8|\mathcal{S}_{r}|^{2}})}\) and \(\varepsilon\leqslant\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r }})}{16|\mathcal{S}_{r}|^{2}}\),

\[\|(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}}^{Q})^{-1}\|_{2,2}\leqslant\sqrt{ \frac{4}{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}}.\]

According to Equation (15), with probability at least \(1-|\mathcal{S}_{r}|\rho_{r}\exp{(-\frac{m\mu^{2}}{2\sigma^{2}})}\), \(\varepsilon\leqslant\frac{\mu}{\sigma}\) and \(\lambda_{B}^{*}>\frac{32\mu\sqrt{\rho_{r}}(1-\alpha/2)}{\alpha}\), we have

\[\|\mathbb{E}_{\mathbb{Q}}\mathcal{E}(\bm{X}_{\mathcal{F}})_{\mathcal{S}_{r}} \bm{e}^{\intercal}\|_{2,\infty}\leqslant\frac{\lambda_{B}^{*}\alpha}{8(1- \alpha/2)}.\]

On that account, with probability at least \(1-2|\mathcal{S}_{r}|^{2}\exp{(-\frac{m(\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}}))^{2}}{8|\mathcal{S}_{r}|^{2}})}-|\mathcal{S}_{r}|\rho_{r}\exp {(-\frac{m\mu^{2}}{2\sigma^{2}})}\) and \(\varepsilon\leqslant\min{(\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}})}{16|\mathcal{S}_{r}|^{2}},\frac{\mu}{\sigma})}\) while requiring \(\lambda_{B}^{*}>\frac{32\mu\sqrt{\rho_{r}}(1-\alpha/2)}{\alpha}\),

\[\|\hat{\bm{W}}_{\mathcal{S}_{r}}\!-\bm{W}_{\mathcal{S}_{r}}^{*}\|_{B,2,\infty} \leqslant\rho_{\max}\sqrt{|\mathcal{S}_{r}|}\sqrt{\frac{4}{\Lambda_{\min}(\bm {H}_{\mathcal{S}_{r}\mathcal{S}_{r}})}}\lambda_{B}^{*}(\frac{\alpha}{8(1- \alpha/2)}+1).\]

By Assumption 2, if the condition \(\lambda_{B}^{*}<\frac{\beta}{2(\frac{\alpha}{8(1-\alpha/2)}+1)\rho_{\max}\sqrt{| \mathcal{S}_{r}|}}\sqrt{\frac{\Lambda_{\min}(\bm{H}_{\mathcal{S}_{r} \mathcal{S}_{r}})}{4}}\) is satisfied, the following inequality holds:

\[\|\hat{\bm{W}}_{\mathcal{S}_{r}}\!-\bm{W}_{\mathcal{S}_{r}}^{*}\|_{B,2,\infty}< \beta/2.\]

In this way, we are able to recover all the neighbor nodes with a threshold \(\beta/2\). This proves (c).

**(v) The true skeleton is recovered with high probability.**

The above arguments tell us that with high probability and certain conditions for \(\varepsilon\) and \(\lambda_{B}^{*}\) satisfied, for each node \(r\), we do not recover any non-neighbor and we do recover all the neighbor nodes. The correct \(\mathbf{Ne}_{r}\) and \(\mathbf{Co}_{r}\) are thus identified. Now we are ready to prove (d).

Putting everything together and taking the the union bound for all nodes \(r\in[n]\), with probability at least \(1-\mathcal{O}(n\exp{(-\frac{Cm^{2}}{\sigma^{2}\rho_{\max}^{2}\rho_{\max}^{3/2}}+2 \log\rho_{[n]})})\), \(\varepsilon\leqslant\frac{C\mu}{\sigma_{\rho_{\max}^{3/2}}}\) and \(\frac{32\mu\rho_{\max}}{\alpha}<\lambda_{B}^{*}<\frac{\beta}{2(\frac{\alpha}{8(1- \alpha/2)}+1)\rho_{\max}\sqrt{\rho_{[n]}}}\sqrt{\frac{\Lambda}{4}}\), where \(C\) only depends on \(\alpha\), \(\Lambda\), we have

\[\hat{\mathcal{G}}_{\text{skel}}=\mathcal{G}_{\text{skel}}.\]Setting \(\varepsilon=\frac{\varepsilon_{0}}{m}\) and making the dependence on the sample size more explicit. We draw the conclusion that, if the number of samples satisfies

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log{(n/\delta)}+\log{\rho_{[n]}})\sigma^{ 2}\rho_{\text{max}}^{4}\rho_{[n]}^{3}}{\min(\mu^{2},1)}),\]

where \(C\) only depends on \(\alpha\), \(\Lambda\), and if \(\lambda_{B}^{*}\) satisfies

\[\frac{32\mu\rho_{\text{max}}}{\alpha}<\lambda_{B}^{*}<\frac{\beta}{(\alpha/(4 -2\alpha)+2)\rho_{\text{max}}\sqrt{\rho_{[n]}}}\sqrt{\frac{\Lambda}{4}},\]

then with probability at least \(1-\delta\) for \(\delta\in(0,1]\):

\[\hat{\mathcal{G}}_{\text{skel}}=\mathcal{G}_{\text{skel}}.\]

Moreover, if we assume that the target graph has a bounded degree of \(d\), the sample complexity becomes logarithmic in \(n\):

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log{(n/\delta)}+\log{n}+\log{\rho_{ \text{max}}})\sigma^{2}\rho_{\text{max}}^{7}d^{3}}{\min(\mu^{2},1)}).\]

**Theorem 10**.: _Suppose that \(\hat{\bm{W}}\) is a DRO risk minimizer of Equation (4) with the KL divergence and an ambiguity radius \(\varepsilon=\varepsilon_{0}/m\). Given the same definitions of \((\mathcal{G},\mathbb{P})\), \(\mathcal{G}_{\text{skel}}\), \(\bar{B}\), \(\lambda_{B}^{*}\), \(m\) in Theorem 9. Under Assumptions 1, 2, 3, 4, if the number of samples satisfies_

\[m=\mathcal{O}(\frac{C(\varepsilon_{0}+\log{(n/\delta)}+\log{\rho_{[n]}}) \sigma^{2}\rho_{\text{max}}^{4}\rho_{[n]}^{3}}{\min(\mu^{2},1)}).\]

_where \(C\) depends on \(\alpha\), \(\Lambda\) while independent of \(n\), and if the Lagrange multiplier satisfies the same condition as in Theorem 9, then for any \(\delta\in(0,1]\), \(r\in[n]\), with probability at least \(1-\delta\), the properties (a)-(d) in Theorem 9 hold._

Proof.: Define

\[\ell_{\bm{W}}(\bm{X}):=\frac{1}{2}\|\mathcal{E}(X_{r})-\bm{W}^{ \intercal}\mathcal{E}(\bm{X}_{\bar{r}})\|_{2}^{2}.\]

According to Theorem 7 in Lam (2019), the worst-case risk with a KL divergence ambiguity set can be bounded as follows:

\[\sup_{\mathbb{Q}\in\mathcal{A}_{\bar{\bm{V}}}^{p}(\bar{\mathcal{ G}}_{m})}\mathbb{E}_{\mathbb{Q}}\ell_{\bm{W}}(\bm{X})\leq \mathbb{E}_{\bar{\mathbb{P}}_{m}}\ell_{\bm{W}}(\bm{X})+\sqrt{ \varepsilon}\sqrt{\frac{1}{m}\sum_{i\in[m]}(\ell_{\bm{W}}(\bm{x}^{(i)})-\ell_ {\bm{W}}^{-})^{2}}+C\varepsilon\frac{\sum_{i\in[m]}|\ell_{\bm{W}}(\bm{x}^{(i)} )-\ell_{\bm{W}}^{-}|^{3}}{\sum_{i\in[m]}(\ell_{\bm{W}}(\bm{x}^{(i)})-\ell_{ \bm{W}}^{-})^{2}}\] \[\leq \mathbb{E}_{\bar{\mathbb{P}}_{m}}\ell_{\bm{W}}(\bm{X})+\sqrt{ \varepsilon}\max_{i\in[m]}|\ell_{\bm{W}}(\bm{x}^{(i)})-\ell_{\bm{W}}^{-}|+C \varepsilon\max_{i\in[m]}|\ell_{\bm{W}}(\bm{x}^{(i)})-\ell_{\bm{W}}^{-}|,\]

where \(\ell_{\bm{W}}^{-}=\frac{1}{m}\sum_{i\in[m]}\ell_{\bm{W}}(\bm{x}^{(i)})\) and \(C>0\) is constant independent of \(n\).

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]