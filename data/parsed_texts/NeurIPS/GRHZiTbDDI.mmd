[MISSING_PAGE_FAIL:1]

Introduction

The emergence of intelligent agents, autonomous systems, and robots demands a profound understanding of real-world environments [1; 2; 3; 4; 5; 6]. This understanding involves more than just recognizing individual objects - it requires an intricate understanding of the relationships between these objects. In this context, research on Scene Graph Generation (SGG) [7], has sought to provide a more detailed, relational perspective on scene understanding. In this approach, scene graphs represent objects as nodes and their relationships as edges, offering a more comprehensive and structured understanding of the scene [8; 7; 9; 10; 11]. Panoptic Scene Graph Generation (PSG) [12] expands the scope of SGG to encompass pixel-level precise object localization and comprehensive scene understanding, including background elements. Then PSG has been further extended to the domain of videos [13] with the inspiration from Video Scene Graph Generation (VidSGG) [14; 15].

The utility of scene graphs also extends into the realm of 3D perception, introducing the concept of 3D Scene Graphs (3DSG) [16; 17]. 3DSGs offer a precise representation of object locations and inter-object relationships within three-dimensional scenes [18; 19]. Despite these developments, the existing approaches have not fully integrated dynamic, spatio-temporal relationships, particularly those involving human-object and human-human interactions. Consider Figure 1 as an illustrative example. Traditional 3D scene graph methods may recognize the static elements of this scene, such as identifying a booth situated on the ground. However, a more ideal, advanced, and dynamic perception is required for real-world scenarios. For instance, a system should be capable of identifying a dynamic event like a person who has fallen off their bike, so that it could then comprehend the necessity to offer assistance, like helping the person stand up and stabilize their bike.

Therefore, our work takes a significant step towards a more comprehensive approach to sensing and understanding the world. We introduce a new task, the 4D Panoptic Scene Graph (PSG-4D), aiming to bridge the gap between raw visual inputs in a dynamic 4D world and high-level visual understanding. PSG-4D comprises two main elements: nodes, representing entities with accurate location and status information, and edges, denoting temporal relations. This task encapsulates both spatial and temporal dimensions, bringing us closer to a true understanding of the dynamic world.

To facilitate research on this new task, we contribute an extensively annotated PSG-4D dataset that is composed of 2 sub-sets, PSG4D-GTA and PSG4D-HOI. The PSG4D-GTA subset consists of 67 RGB-D videos with a total of 28K frames, selected from the SAIL-VOS 3D dataset [20] collected from the video game Grand Theft Auto V (GTA-V) [21]. The PSG4D-HOI subset is a collection of 3K egocentric real-world videos sampled from the HO14D dataset [22]. All frames in either of the subset are labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs. We believe this dataset will serve as a valuable resource for researchers in the field.

To tackle this novel task, we propose a unified framework called PSG4DFormer. This unified structure encapsulates two primary components: a 4D Panoptic Segmentation model and a Relation model. The 4D Panoptic Segmentation model is designed to accommodate both RGB-D and point cloud data inputs, yielding a 4D panoptic segmentation. This output comprises 3D object masks, which are continuously tracked across temporal dimensions. Then, the Relation model accepts these 3D mask tubes and utilizes a spatial-temporal transformer architecture to delineate long-term dependencies and intricate inter-entity relationships, subsequently yielding a relational scene graph. Through extensive experiments, we demonstrate the effectiveness of the proposed PSG-4D task and the PSG4DFormer model. Our work constitutes a pivotal step towards a comprehensive understanding of dynamic environments, setting the stage for future research in this exciting and crucial area of study.

In summary, we make the following contributions to the community:

* **A New Task**: We propose a novel scene graph generation task focusing on the prediction of 4D panoptic scene graphs from RGB-D or point cloud video sequences.
* **A New Dataset**: We provide a PSG-4D dataset, which covers diverse viewpoints: (i) a third-view synthetic subset (PSG4D-GTA) and (ii) an egocentric real-world subset (PSG4D-HOI).
* **A Unified Framework**: We propose a unified two-stage model composed of a feature extractor and a relation learner. In addition, we offer demo support for both synthetic and real-world scenarios to facilitate future research and real-world applications.
* **Open-Source Codebase**: We open-source our codebase to facilitate future PSG-4D research.

## 2 Related Work

**Scene Graph Generation (SGG)** SGG transforms an image into a graph, where nodes represent objects and edges represent relationships [7]. Several datasets [23] and methods, including two-stage [8; 7; 9; 10; 11] and one-stage models [24; 12; 25], have been developed for SGG. Video scene graph generation (VidSGG) extends SGG to videos with notable datasets [14; 15; 26]. Despite progress, limitations remain in SGG and VidSGG due to noisy grounding annotations caused by coarse bounding box annotations and trivial relation definitions. Recent work on panoptic scene graph generation (PSG) [12; 27; 28; 29; 30; 31] has attempted to overcome these issues, and PVSG [13; 32] further extends it into the video domain. This paper presents an extension of PSG into a 4D dynamic world, meeting the needs of active agents for precise location and comprehensive scene understanding.

**3D Scene Graph Generation** 3D Scene Graphs (3DSGs) offer a precise 3D representation of object locations and inter-object relationships, making them a vital tool for intelligent agents operating in real-world environments [16; 17]. 3DSGs can be categorized into flat and hierarchical structures [33]. The former represents objects and relationships as a simple graph [18; 19], while the latter layers the structures of 3D scenes [34; 35]. Recent 3DSG techniques [19] employ PointNet [36] with 3D object detectors on point clouds or RGBD scans, generating 3D graphs via graph neural networks [18]. Some settings, such as Kimera [37], emphasize pairwise spatiotemporal status to facilitate task planning, while incremental 3DSG necessitates agents to progressively explore environments [38]. However, these graphs largely represent positional relations, lacking dynamic spatiotemporal relations like human-object interactions and human-human relations.

**4D Perception** Research on 4D perception can be divided by the specific data format they use. The first one is RGB-D video, which can be easily obtained using cheap sensors, _e.g._ Kinect, and iPhone. With the additional depth data, more geometric and spatial information can be used for reliable and robust detection [39; 40; 41] and segmentation [42; 43; 44; 45]. For RGB-D video, the depth input is usually treated like images. But for point clouds video, 3D or higher dimension convolutions [46; 47; 48; 49] are more commonly used, especially on LiDAR point cloud videos for autonomous driving perception system. In this work, beyond the 4D panoptic segmentation, we focus on more daily scenes and pursue a more high-level and structured understanding of 4D scenes by building 4D scene graphs.

## 3 The PSG-4D Problem

The PSG-4D task is aimed at generating a dynamic scene graph, which describes a given 4D environment. In this context, each node corresponds to an object, while each edge represents a spatial-temporal relation. The PSG-4D model ingests either an RGB-D video sequence or a point cloud video sequence, subsequently outputting a PSG-4D scene graph \(\mathbf{G}\). This graph is composed of 4D object binary mask tubes \(\mathbf{M}\), object labels \(\mathbf{O}\), and relations \(\mathbf{R}\).

The object binary mask tubes, \(\mathbf{m}_{i}\in\{0,1\}^{T\times H\times W\times 4}\), express the 3D location and extent of the tracked object \(i\) over time (\(T\)) in the case of an RGB-D sequence input, while \(\mathbf{m}_{i}\in\{0,1\}^{T\times M\times 6}\) is used for point cloud video inputs. Here, 4 denotes RGB-D values, and 6 represents XYZ plus RGB values. M stands for the number of point clouds of interest. The object label, \(o_{i}\in\mathbb{C}^{O}\), designates the category of the object. The relation \(r_{i}\in\mathbb{C}^{R}\) represents a subject and an object linked by a predicate class and a time period. \(\mathbb{C}^{O}\) and \(\mathbb{C}^{R}\) refer to the object and predicate classes, respectively. The PSG-4D task can be mathematically formulated as:

\[\Pr\left(\mathbf{G}\mid\mathbf{I}\right)=\Pr\left(\mathbf{M},\mathbf{O}, \mathbf{R}\mid\mathbf{I}\right),\] (1)

where \(\mathbf{I}\) represents the input RGB-D video sequence or point cloud representation.

**Evaluation Metrics** For evaluating the performance of the PSG-4D model, we employ the R@K and mR@K metrics, traditionally used in the scene graph generation tasks. R@K calculates the triplet recall, while mR@K computes the mean recall, both considering the top K triplets from the PSG-4D model. A successful recall of a ground-truth triplet must meet the following criteria: 1) correct category labels for the subject, object, and predicate; 2) a volume Intersection over Union (vIOU) greater than 0.5 between the predicted mask tubes and the ground-truth tubes. When these criteria are satisfied, a soft recall score is recorded, representing the time vIOU between the predicted and the ground-truth time periods.

## 4 The PSG-4D Dataset

This section outlines the development of the PSG-4D dataset. We begin by exploring existing datasets that inspired the creation of PSG-4D, followed by a presentation of its statistics, and finally a brief overview of the steps involved in its construction.

### Leveraging Existing Datasets for PSG-4D

Rather than constructing the PSG-4D dataset from the ground up, we sought to evaluate whether currently available datasets could either directly support or be adapted for the PSG-4D task. As shown in Table 1, our initial exploration focused on 3D datasets, including 3D scene graph datasets like 3DSGG [18] and Rel3D [50], along with more conventional 3D datasets such as ScanNet [51] and Matterport 3D [52]. However, while these datasets can be used to reconstruct entire scenes and can generate 3D videos accordingly, the resulting scenes remain static and lack dynamic elements.

We then shifted our focus to video datasets containing 3D information. Autonomous driving datasets such as Nuscenes [53] and WAYMO [54] incorporate point cloud videos, particularly bird's-eye view footage. Nevertheless, the vehicles within these scenes are only captured in 2D video. While this technically constitutes a dynamic 4D scene, it does not align well with the objectives of this study. The dynamic relations in traffic scenarios are relatively limited, and our goal is to develop a visual understanding model for embodied AI [58; 59; 60; 61] that captures 3D scenes from the agent's perspective, not a bird's-eye view.

Another category of 3D videos uses RGB-D sequences as input, which can be easily converted into point clouds. This data format aligns perfectly with the operation of intelligent agents, mimicking human perception, which captures continuous RGB images with depth. Thankfully, recent datasets like SAIL-VOS 3D [55], HOI4D [56], and EgoBody [57] have adopted this approach. While SAIL-VOS 3D uses synthetic data from the GTA game [21], the HOI4D dataset captures egocentric RGB-D videos of simple tasks, such as tool picking. On the other hand, the EgoBody dataset [57] records office activities like conversations, but lacks segmentation annotation and is primarily intended for human pose reconstruction. Despite its wealth of videos, the object interaction in EgoBody is limited. In the medical domain, 4D-OR [60] excels in providing detailed depictions of surgical scenes, showcasing its specialized utility. To cater to a broader spectrum of research applications, we formulated the PSG-4D dataset, integrating the versatile strengths of the SAIL-VOS 3D [55] and HOI4D [56] datasets.

### Dataset Statistics

Figure 2 presents a selection of four video frames, drawn from both the PSG4D-GTA and PSG4D-HOI datasets. Each frame is an RGB-D video with corresponding panoptic segmentation annotations. Underneath each scene, we depict the associated scene graph and statistical word clouds. Annotators constructed these scene graphs as triplets, complete with frame duration. The PSG4D-GTA dataset is

\begin{table}
\begin{tabular}{l l l l l l l l} \hline Dataset & Type & Scale & View & \#O/PCs & \#P/cells & Annotation & Year \\ \hline DSSG [18] & 3DSG & 36\& RGB-D image, 1482 scene, 478 scenes, & TPV & 534 & 40 & 3D model, 3D graph & 200\\\ RoSD [50] & 3DSG & 278 KGB-D images, 9900 3D scenes & TPV & 67 & 50 & 3D model & 200\\\ \hline ScanNet [51] & 30 Images & 25,84 KGB-D images, 1513 indoor scenes & TPV & 20 & - & 58, 3D model & 2017 \\ Matteport 3D [52] & 30 Images & 194,400 KGB-D images, 1513 indoor scenes & TPV & 40 & - & 58, 3D model & 2017 \\ \hline Nuscenes [53] & 3D Images & 194,400 KGB-D images, 1513 indoor scenes & TPV & 40 & - & 58, 3D model & 2017 \\ \hline Nuovo [53] & 20 Video+PC & 1K video (vog 202, 134 137 optical methods) & Vehicle & 23 & - & 3D BR & 200\(\times\) \\ WAVMO [54] & 2D Video+PC & 1.2K video (vog 202, 134 137 optical methods) & Vehicle & 20 & - & 20 B B, 3D BR & 200\(\times\) \\ \hline Sni-VOS [50] & 3D Video & 484 video, 328 KGB-D images, 6070 clips & openstreve & 178 & - & 58, 3D model & 201\(\times\) \\ HOI4D [56] & 3D Video & 484 video, 224 KGB-D images, 610 indoor scenes & openstreve & 16 & 11 & 78, 78, 202\(\times\) \\ EgoBody [57] & 3D Video & 125 videos, 197 KGB-D images, 15 indoor scenes & openstreve, TPV & 36 & 13 & 3D model, 8F & 2022\(\times\) \\ \hline
**PSG4D-GTA** & PSG4D & 67 videos (vog 84, 288, RG-D images, 283 88 pointclouds) & TPV & 35 & 43 & PS, 4DSG & 2003\(\times\) \\
**PSG4D-HOI** & PSG4D & 2973 videos (vog 202, 891K RGB-D images, 282 indoor scenes & openstreve & 46 & 15 & PS, 4DSG & 2003\(\times\) \\ \hline \end{tabular}
\end{table}
Table 1: **Illustration of the PSG-4D dataset and related datasets.** Unlike the static 3D indoor scenes usually found in 3DSG datasets, the PSG-4D dataset introduces dynamic 3D videos, each annotated with panoptic segmentation. Various 3D video datasets were evaluated as potential sources for PSG-4D, resulting in the creation of two subsets: PSG4D-GTA and PSG4D-HOI. Regarding annotations, PS represents Panoptic Segmentation, BB represents Bounding Box, SS represents Semantic Segmentation, KP represents key points, and PC represents point clouds. TPV represents third-person-view.

particularly noteworthy for its composition: it contains 67 videos with an average length of 84 seconds, amounting to 27,700 RGB-D images, 28.3 billion point clouds, and comprises 35 object categories, and 43 relationship categories. This synthetic dataset was captured from a third-person perspective. In contrast, the PSG4D-HOI dataset is compiled from an egocentric perspective, providing a different context for analysis. It includes 2,973 videos with an average duration of 20 seconds, equating to 891,000 RGB-D images across 282 indoor scenes. This dataset includes 46 object categories and 15 object-object relationship categories, offering a diverse range of real-world data for the study. The combination of these two datasets offers a comprehensive understanding of 4D environments due to their complementary nature. A statistical overview of both datasets can be found in the final two rows of Table 1.

### Dataset Construction Pipeline

As outlined in Section 4.1, the PSG4D-GTA is built upon the SAIL-VOS 3D dataset, while the PSG4D-HOI is derived from the HOI4D dataset. To adapt the SAIL-VOS 3D dataset for our purpose, we commenced with a comprehensive review of all 178 GTA videos within the dataset. This stage involved a meticulous elimination process to exclude videos containing NSFW content, resulting in a refined pool of 67 videos. The SAIL-VOS 3D dataset, which is equipped with 3D instance segmentation, required additional annotation for background elements to integrate panoptic segmentation. Leveraging the PVSG annotation pipeline, we employed an event detection method [62] to isolate the key frames. The background elements within these key frames were subsequently annotated using the pre-annotation provided by the SAM model [63]. Upon completion of key frame annotations, the AOT method [64] was utilized to propagate the segmentation across the entire video sequence. The final step involved overlaying the instance segmentation on the stuff segmentation, thereby completing the process. The HOI-4D dataset, devoid of NSFW content, already provides a 4D panoptic segmentation. Consequently, we included all videos from the HOI-4D dataset in the PSG4D-HOI dataset without further modifications.

Upon completion of 4D panoptic segmentation annotation, we proceed to annotate the dynamic scene graph according to the masks. Although HOI4D includes action annotation concerning the person, it doesn't account for interactions between objects. Nevertheless, certain actions such as "pick up" are appropriately considered predicates, and we automatically position the key object in the video to form a subject-verb-object triplet. Once the automatic-annotated dataset is prepared, we ask annotators to review and revise the pre-annotations to ensure accuracy. As SAIL-VOS 3D lacks all kinds of relational annotation, we commence scene graph annotation from scratch. The entire annotation process is diligently executed by the authors around the clock.

## 5 Methodology

This section details a unified pipeline PSG4DFormer for addressing the PSG-4D problem. As shown in Figure 3, our approach comprises two stages. The initial 4D panoptic segmentation stage aims to segment all 4D entities, including objects and background elements in Figure 3 (a), with the accurate

Figure 2: **The Examples and Word Clouds of PSG-4D dataset. The PSG-4D dataset contains 2 subsets, including (a) PSG4D-GTA selected from the SAIL-VOS 3D [20] dataset, and (b) PSG4D-HOI from HOI4D [22] dataset. We selected 4 frames of an example video from each subset. Each frame has aligned RGB and depth with panoptic segmentation annotation. The scene graph is annotated in the form of triplets. The word cloud for object and relation categories in each dataset is also represented.**

temporal association in Figure 3 (b). We extract features for each object and obtain feature tubes according to tracking results for subsequent relation modeling in Figure 3 (c).

### 4D Panoptic Segmentation Modeling

As specified in Section 3, given a 3D video clip input, such as an RGB-D sequence of \(\mathbf{I}\in\mathbb{R}^{T\times H\times W\times 4}\) or a point cloud sequence of \(\mathbf{I}\in\mathbb{R}^{T\times M\times 6}\), the initial stage's goal is to segment and track each pixel non-overlappings. The model predicts a set of video clips with the output of \(\left(\mathbf{m}_{i},\mathbf{q}_{i},p_{i}\right)_{i=1}^{N}\), where \(\mathbf{m}_{i}\) denotes the tracked object mask tube, \(\mathbf{q}_{i}\) denotes the tracked feature tube, and \(p_{i}\) represents the probability of the object belonging to each category. \(N\) is the number of entities, encompassing things and stuff classes.

Frame-Level Panoptic Segmentation with RGB-D SequenceGiven the dual input of RGB and depth images, we adopt a separation-and-aggregation gate (SA-Gate) [65] to efficiently blend information from both modalities. This combined feature set, enriched with data from both inputs, is then fed into a robust Mask2Former [4] for frame-level panoptic segmentation. In the inference stage, at the frame \(t\), given an RGB-D image \(\mathbf{I}\), the Mask2Former with SA-Gate directly outputs a set of object query features \(q_{i}^{t}\in\mathbb{R}^{d},i=1,\ldots,N\), each \(q_{i}^{t}\) representing one entity at the frame \(t\).

Frame-Level Panoptic Segmentation with Point Cloud SequenceApart from perceiving point cloud sequences directly, 3D point cloud coordinates can be calculated and converted from RGB-D data. This conversion involves computing the Normalized Device Coordinates (NDC) using the depth map and projecting the NDC to world coordinates using the transformation matrix provided. We retain only points with a depth below a defined threshold \(\lambda\), discarding distant, less relevant elements like far-off mountains. To leverage texture information from the image, point cloud coordinates can be augmented with corresponding RGB values, creating a colorful point cloud representation \(\mathbf{P}\in\mathbb{R}^{M\times 6}\), where \(M\) is the total number of points in a frame.

We employ DKNet [66], a state-of-the-art indoor segmentation method, as our point cloud segmentation network. It processes input point clouds with a 3D UNet-like [67] backbone and uses sparse convolutions [68] for feature extraction. DKNet localizes instance centroids based on a candidate mining branch and encodes each instance's information into an instance kernel \(k_{i}\in\mathbb{R}^{d}\). These instance kernels \(\{k_{i}\}_{i=1}^{N}\) are used as the weights of a few convolution layers to obtain the final instance masks.

Figure 3: **Illustration of the PSG4DFormer pipeline. This unified pipeline supports both RGB-D and point cloud video inputs and is composed of two main components: 4D panoptic segmentation modeling and relation modeling. The first stage seeks to obtain the 4D panoptic segmentation mask for each object, along with its corresponding feature tube spanning the video length. This is accomplished with the aid of (a) frame-level panoptic segmentation and (b) a tracking model. The subsequent stage (c) employs a spatial-temporal transformer to predict pairwise relations based on all feature tubes derived from the first stage.**

**Tracking** After frame-level panoptic segmentation, we link each frame via using UniTrack [69] for tracking to obtain the final tracked video cubes for each clip for either modality input. Specifically, instead of incorporating an additional appearance model for tracking embedding extraction, we directly utilize the instance kernels \(\{k_{i}\}_{i=1}^{N}\) from the segmentation step of DKNet, or object query features \(\{q_{i}\}_{i=1}^{N}\) from Mask2Former as the tracking embeddings for the association. We find that the instance kernels exhibit sufficient distinctiveness for tracking purposes, even when dealing with different objects belonging to the same semantic class. This is primarily because each instance kernel is designed to maximize the response for a specific instance while suppressing the responses of all other instances, including those with the same semantic class. For a video sequence with the length \(T\), the obtained 4D feature tubes are noted as \(Q_{i}=\{q_{i}^{t}\}_{t=1}^{T}\).

### Relation Modeling: 4D Scene Graph Generation

The object query tubes \(Q_{i}\) and mask tubes \(\mathbf{m}_{i}\) form a bridge between the first and second stages. These feature tubes first pass through a spatial-temporal transformer encoder, which augments them with both global context information from the overall image and global temporal space.

**Spatial-Temporal Transformer Encoder** To infuse the feature tubes with additional temporal dimension information and characteristics from other objects in the scene, we draw inspiration from the Spatial-Temporal Transformer [70]. A spatial encoder is initially employed. For all objects co-occurring at the same time \(t\), a two-layer transformer encoder is applied to the input, comprising all object features specific to time frame \(t\). The spatially-encoded feature tube updates the object feature tube into \(\{\tilde{q}_{i}^{t}\}_{i=1}^{N}\). Subsequently, a temporal transformer encoder updates each object feature tube along the temporal dimension \(T\). By leveraging both the spatial and temporal encoders, we obtain the final feature tube \(\{\hat{q}_{i}^{t}\}_{i=1}^{N}\), ready for relation training.

**Relation Classification Training** To train the relation model based on the updated query tube, a training set for relation training must be constructed. It is worth noting that the relation annotation in the training set is in the form of "object-1 relation object-2", with the mask tube of both objects annotated. To start, we associate the updated query tube with ground truth objects. For each ground truth tube, we find the most suitable updated query tube by calculating the video Intersection over Union (vIOU) between ground truth mask tubes, and assign the query feature tube to the respective objects. A frame-level predicate classification is conducted with the assistance of a lightweight fully-connected layer. The inference of the relation classification component simply computes the relation probability between pairs of \(\hat{q}_{i}^{t}\) and \(\hat{q}_{j}^{t}\).

## 6 Experiments

Table 2 presents the results of experiments conducted on the PSG-4D dataset. For RGB-D sequences, an ImageNet-pretrained ResNet-101 serves as both the RGB and depth encoder. We set the training duration to 12 epochs. The DKNet, trained from scratch, requires a longer training period of 200 epochs. In the second stage, both spatial and temporal transformer encoders span two layers, and training continues for an additional 100 epochs. Besides the standard PSG4DFormer, we also examine variants with the temporal encoder removed (denoted as "/t") and the depth branch removed (denoted as "/d"). As a baseline, we use the 3DSGG model [18], which employs a GNN model to encode frame-level object and relation information, without considering temporal data.

**RGB-D _vs._ Point Cloud Input** Table 2 is divided into two sections. The upper part (#1-#3) reports results from point cloud input, while the latter part (#4-#7) details results from the RGB-D sequence. It appears that the RGB-D sequence generally yields better results than the point cloud sequence, particularly for the PSG4D-GTA dataset. This could potentially be attributed to the ResNet-101 backbone used for the RGB-D data, which being pretrained on ImageNet, exhibits robust performance on complex datasets like PSG4D-GTA. Meanwhile, the PSG4D-HOI dataset seems to offer a more consistent scenario with abundant training data, thus narrowing the performance gap between the point cloud and RGB-D methods.

**Significance of Depth** The results in Table 2 also allow us to evaluate the importance of depth in the RGB-D method. Specifically, we designed a variant of PSG4DFormer (marked as "/d") thatdoesn't utilize the depth branch. In other words, both the RGB encoder and the SA-Gate are removed, turning the pipeline into a video scene graph generation pipeline. The performance of this variant is inferior compared to the original, which highlights the significance of depth information in the scene graph generation task.

Necessity of Temporal Attention Table 2 includes two methods that do not utilize temporal attention. Specifically, the 3DSGG baseline learns interactions between static object features using a graph convolutional network, while PSG4DFormer\({}^{/t}\) removes the temporal transformer encoders. The results demonstrate that ignoring the temporal component could lead to sub-optimal outcomes, emphasizing the importance of temporal attention in 4D scene graph generation.

## 7 Real-World Application

This section illustrates the deployment of the PSG-4D model in a real-world application, specifically within a service robot. It extends beyond theoretical concepts and computational models, delving into the practical integration and execution of this cutting-edge technology. As shown in Figure 4, the focus here is to demonstrate how the robot leverages the PSG-4D model (pretrained from PSG4D-HOI, RGB-D input) to interpret and respond to its surroundings effectively.

Interaction with Large Language Models The recent advancements in large language models (LLMs) have displayed their exceptional capabilities in reasoning and planning [71]. LLMs have been utilized as planners in numerous recent studies to bridge different modalities, paving the way for more intuitive and efficient human-machine interaction [72]. In this work, we employ GPT-4 [71], as the primary planner. Designed to align with human instruction, GPT-4 communicates with the robot by translating the raw scene graph representations into comprehensible human language. Therefore, the interaction begins with the prompt, "I am a service robot. For every 30 seconds, I will give you what I have seen in the last 30 seconds. Please suggest me what I could serve." Subsequently, every 30 seconds, the robot engages with GPT-4, providing an update: "In the past 30s, what I captured is: <from start_time to end_time, object-1 relation object-2>, <...>, <...>." This enables GPT-4 to analyze the situation and provide appropriate feedback.

Post-Processing for Execution The effective deployment of the PSG-4D model necessitates a robust set of predefined actions that the robot can execute. Currently, the action list includes tasks such as picking up litter and engaging in conversation with individuals. After GPT-4 provides its suggestions, it is further prompted to select a suitable action from this predefined list for the robot to execute. However, the flexibility of this system allows for the expansion of this action list, paving the way for more complex and varied tasks in the future. To encourage community involvement and the development of fascinating applications, we also release the robot deployment module alongside the PSG4D codebase. The demo robot is priced at approximately $1.2K and comes equipped with an RGB-D sensor, microphone, speakers, and a robotic arm.

\begin{table}
\begin{tabular}{c c c c c|c c c} \hline \hline Input & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{PSG4D-GTA} & \multicolumn{3}{c}{PSG4D-HOI} \\ \cline{3-8} Type & & R/mR@20 & R/mR@50 & R/mR@100 & R/mR@20 & R/mR@50 & R/mR@100 \\ \hline Point & \#1 3DSGG [18] & 1.48 / 0.73 & 2.16 / 0.79 & 2.92 / 0.85 & 3.46 / 2.19 & 3.15 / 2.47 & 4.96 / 2.84 \\ Cloud & \#2 PSG4DFormer\({}^{/t}\) & 2.25 / 1.03 & 2.67 / 1.72 & 3.14 / 2.05 & 3.26 / 2.04 & 3.16 / 2.35 & 4.18 / 2.64 \\ Sequence & \#3 PSG4DFormer & 4.33 / 2.10 & 4.83 / 2.93 & 5.22 / 3.13 & 5.36 / 3.10 & 5.61 / 3.95 & 6.76 / 4.17 \\ \hline \multirow{3}{*}{RGB-D} & \#4 3DSGG [18] & 2.29 / 0.92 & 2.46 / 1.01 & 3.81 / 1.45 & 4.23 / 2.19 & 4.47 / 2.31 & 4.86 / 2.41 \\  & \#5 PSG4DFormer\({}^{/t}\) & 4.43 / 1.34 & 4.89 / 2.42 & 5.26 / 2.83 & 4.44 / 2.37 & 4.83 / 2.43 & 5.21 / 2.84 \\ Sequence & \#6 PSG4DFormer\({}^{/t}\) & 4.40 / 1.42 & 4.91 / 1.93 & 5.49 / 2.27 & 5.49 / 3.42 & 5.97 / 3.92 & 6.43 / 4.21 \\  & \#7 PS4DFormer & 6.68 / 3.31 & 7.17 / 3.85 & 7.22 / 4.02 & 5.62 / 3.65 & 6.16 / 4.16 & 6.28 / 4.97 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Main Results on PSG4D. Experimental results are reported on both the PSG4D-GTA and PSG4D-HOI datasets. In addition to comparing with traditional 3DSGG methods, we conduct experiments to compare the PSG4DFormer and its variants. This includes a version with the temporal encoder removed (denoted as “/t”) and one with the depth branch removed (denoted as “/d”).**

## 8 Conclusion, Challenges, and Outlook

This paper presents a novel and demanding extension to the traditional scene graph generation, the 4D Panoptic Scene Graph Generation, which incorporates the spatio-temporal domain into the framework. We introduce a comprehensive framework, the PSG4DFormer, capable of processing both RGB-D and point cloud sequences. The successful deployment of this pipeline in a practical service robot scenario underscores its potential in real-world applications. However, these achievements also highlight the nascent state of this field, emphasizing the necessity for continued advancements to fully exploit the potential of 4D Panoptic Scene Graph Generation.

**Challenges** Despite encouraging results, we have also revealed several persistent challenges in the realm of 4D Panoptic Scene Graph Generation. Through our demonstration, we found that current models, whether derived from PSG4D-GTA or PSG4D-HOI, can handle only simple scenes and falter when faced with more complex real-world environments. Notably, there exist robust models trained in the 2D world. Finding effective and efficient strategies to adapt these models to the 4D domain presents a compelling direction for future exploration.

**Outlook** Future work in this field presents several intriguing trajectories. There is a pressing need for more efficient algorithms for 4D Panoptic Scene Graph Generation, which can handle larger and more diverse environments. Equally important is the creation of comprehensive and diverse datasets that would allow more rigorous evaluation and foster advancements in model development. Particularly noteworthy is a recent Digital Twin dataset [73], which promises a high level of accuracy and photorealism, aligning seamlessly with the objectives of PSG4D. This dataset will be incorporated as the third subset of the PSG4D dataset, readily accessible from our codebase. In addition to robotics, as demonstrated by the practical application of PSG4DFormer, we are also exploring its potential as an autonomous player in the GTA game. Actually, our recent endeavor Octopus [58] strives to complete GTA missions by employing a visual-language programmer to generate executable action code. In contrast to the previously passive task completion, the application in this paper actively perceives and understands the environment, showcasing a shift towards autonomy in robotics. Furthermore, Octopus [58] utilizes a 4D scene graph structure to capture environmental information during the visual-language programmer training, exemplifying a practical application of the PSG4D modality.

Figure 4: **Demonstration of a Robot Deployed with the PSG-4D Model. The service robot interprets the RGB-D sequence shown in (a), where a man is seen drinking coffee and subsequently dropping the empty bottle on the ground. The robot processes this sequence, translating it into a 4D scene graph depicted in (b). This graph comprises a set of temporally stamped triplets, with each object associated with a panoptic mask, accurately grounding it in 3D space. The robot regularly updates its PSG4D to GPT-4, awaiting feedback and instructions. In this scenario, GPT-4 advises the robot to clean up the discarded bottle and remind the man about his action. This directive is translated into robot action, as visualized in (d).**We eagerly anticipate the future progress in the field of 4D Panoptic Scene Graph Generation and its potential to revolutionize our understanding of real-world dynamics.

Potential Negative Societal ImpactsThis work releases a dataset containing human behaviors, posing possible gender and social biases inherently from data. Potential users are encouraged to consider the risks of oversighting ethical issues in imbalanced data, especially in underrepresented minority classes. Nevertheless, all the NSFW content is removed from the dataset.

## Acknowledgement

This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, the National Key R&D Program of China under grant number 2022ZD0161501, as well as cash and in-kind contribution from the industry partner(s).

## References

* [1] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. _arXiv preprint arXiv:2210.07474_, 2022.
* [2] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* [3] Sonia Raychaudhuri, Tommaso Campari, Unnat Jain, Manolis Savva, and Angel X Chang. Reduce, reuse, recycle: Modular multi-object navigation. _arXiv preprint arXiv:2304.03696_, 2023.
* [4] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.
* [5] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, and Chen Change Loy. Tube-link: A flexible cross tube baseline for universal video segmentation. In _ICCV_, 2023.
* [6] Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan, Guangliang Cheng, Pang Jiangmiao, Kai Chen, Ziwei Liu, and Chen Change Loy. Transformer-based visual segmentation: A survey. _arXiv pre-print_, 2023.
* [7] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In _CVPR_, 2017.
* [8] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In _CVPR_, 2019.
* [9] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In _CVPR_, 2018.
* [10] Mohammed Suhail, Abhay Mittal, Behjat Siddiqiue, Chris Broaddus, Jayan Eledath, Gerard Medioni, and Leonid Sigal. Energy-based learning for scene graph generation. In _CVPR_, 2021.
* [11] Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, and Yin Li. Learning to generate scene graph from natural language supervision. In _ICCV_, 2021.
* [12] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In _European Conference on Computer Vision_, pages 178-196. Springer, 2022.
* [13] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, and Ziwei Liu. Panoptic video scene graph generation. In _CVPR_, 2023.
* [14] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang, and Tat-Seng Chua. Video visual relation detection. In _ACM MM_, 2017.
* [15] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in user-generated videos. In _ICMR_, 2019.
* [16] Matthew Fisher, Manolis Savva, and Pat Hanrahan. Characterizing structural relationships in scenes using graph kernels. In _ACM SIGGRAPH 2011 papers_, pages 1-12. 2011.

* [17] Robert F Tobler. Separating semantics from rendering: a scene graph based architecture for graphics applications. _The Visual Computer_, 27(6-8):687-695, 2011.
* [18] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3961-3970, 2020.
* [19] Shoulong Zhang, Aimin Hao, Hong Qin, et al. Knowledge-inspired 3d scene graph prediction in point cloud. _Advances in Neural Information Processing Systems_, 34:18620-18632, 2021.
* [20] Y.-T. Hu, J. Wang, R. A. Yeh, and A. G. Schwing. SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data. In _Proc. CVPR_, 2021.
* [21] Grand theft auto v, 2014.
* [22] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 21013-21022, June 2022.
* [23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 2017.
* [24] Rongjie Li, Songyang Zhang, and Xuming He. Sqrt: End-to-end scene graph generation with transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19486-19496, 2022.
* [25] Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn. Reltr: Relation transformer for scene graph generation. _arXiv preprint arXiv:2201.11460_, 2022.
* [26] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In _CVPR_, 2020.
* [27] Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, and Ziwei Liu. Pair then relation: Pair-net for panoptic scene graph generation. _arXiv preprint arXiv:2307.08699_, 2023.
* [28] Chengyang Zhao, Yikang Shen, Zhenfang Chen, Mingyu Ding, and Chuang Gan. Textpsg: Panoptic scene graph generation from textual descriptions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2839-2850, 2023.
* [29] Zijian Zhou, Miaojing Shi, and Holger Caesar. Hilo: Exploiting high low frequency relations for unbiased panoptic scene graph generation. _arXiv preprint arXiv:2303.15994_, 2023.
* [30] Julian Lorenz, Florian Barthel, Daniel Kienzle, and Rainer Lienhart. Haystack: A panoptic scene graph dataset to evaluate rare predicate classes. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 62-70, 2023.
* [31] Jingkang Yang, Zheng Ma, Qixun Wang, Xiaofeng Guo, Haofan Wang, Ziwei Liu, Wayne Zhang, Xing Xu, and Hai Zhang. The psg challenge: towards comprehensive scene understanding. _National Science Review_, 10(6):nwad126, 2023.
* [32] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen, Guangliang Cheng, Yunhai Tong, and Chen Change Loy. Video k-net: A simple, strong, and unified baseline for video segmentation. In _CVPR_, 2022.
* [33] Jaewon Bae, Dongmin Shin, Kangbeen Ko, Juchan Lee, and Ue-Hwan Kim. A survey on 3d scene graphs: Definition, generation and application. In _Robot Intelligence Technology and Applications 7: Results from the 10th International Conference on Robot Intelligence Technology and Applications_, pages 136-147. Springer, 2023.
* [34] Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song, and Jong-Hwan Kim. 3-d scene graph: A sparse and semantic representation of physical environments for intelligent agents. _IEEE transactions on cybernetics_, 50(12):4921-4933, 2019.
* [35] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: A structure for unified semantics, 3d space, and camera. In _ICCV_, 2019.

* [36] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _CVPR_, 2017.
* [37] Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca Carlone. Kimera: From slam to spatial perception with 3d dynamic scene graphs. _The International Journal of Robotics Research_, 40(12-14):1510-1546, 2021.
* [38] Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7515-7525, 2021.
* [39] Hema Koppula and Ashutosh Saxena. Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation. In _International conference on machine learning_, pages 792-800. PMLR, 2013.
* [40] Qian Xie, Oussama Remil, Yanwen Guo, Meng Wang, Mingqiang Wei, and Jun Wang. Object detection and tracking under occlusion for object-level rgb-d video segmentation. _IEEE Transactions on Multimedia_, 20(3):580-592, 2017.
* [41] Guyue Zhang, Jun Liu, Hengduo Li, Yan Qiu Chen, and Larry S Davis. Joint human detection and head pose estimation via multistream networks for rgb-d videos. _IEEE Signal Processing Letters_, 24(11):1666-1670, 2017.
* [42] David Weikersdorfer, Alexander Schick, and Daniel Cremers. Depth-adaptive supervoxels for rgb-d video segmentation. In _2013 IEEE International Conference on Image Processing_, pages 2708-2712. IEEE, 2013.
* [43] Huazhu Fu, Dong Xu, and Stephen Lin. Object-based multiple foreground segmentation in rgbd video. _IEEE Transactions on Image Processing_, 26(3):1418-1427, 2017.
* [44] Steven Hickson, Stan Birchfield, Irfan Essa, and Henrik Christensen. Efficient hierarchical graph-based segmentation of rgbd videos. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 344-351, 2014.
* [45] Numair Khan, Qian Zhang, Lucas Kasser, Henry Stone, Min H Kim, and James Tompkin. View-consistent 4d light field superpixel segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7811-7819, 2019.
* [46] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3075-3084, 2019.
* [47] Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, and Chen Change Loy. Robust multi-modality multi-object tracking. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2365-2374, 2019.
* [48] Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris M Kitani. Gnn3dmot: Graph neural network for 3d multi-object tracking with 2d-3d multi-feature learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6499-6508, 2020.
* [49] Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani. 3d multi-object tracking: A baseline and new evaluation metrics. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 10359-10366. IEEE, 2020.
* [50] Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. Rel3d: A minimally contrastive benchmark for grounding spatial relations in 3d. _Advances in Neural Information Processing Systems_, 33:10514-10525, 2020.
* [51] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5828-5839, 2017.
* [52] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. _arXiv preprint arXiv:1709.06158_, 2017.
* [53] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11621-11631, 2020.

* [54] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2446-2454, 2020.
* [55] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexander G Schwing. Sail-vos 3d: A synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1418-1428, 2021.
* [56] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21013-21022, 2022.
* [57] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022. Proceedings, Part VI_, pages 180-200. Springer, 2022.
* [58] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiam Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. _arXiv preprint arXiv:2310.08588_, 2023.
* [59] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. _arXiv preprint arXiv:2307.06135_, 2023.
* [60] Ege Ozsoy, Evin Pinar Ornek, Ulrich Eck, Tobias Czempiel, Federico Tombari, and Nassir Navab. 4d-or: Semantic scene graphs for or domain modeling. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 475-485. Springer, 2022.
* [61] Saeid Amiri, Kishan Chandan, and Shiqi Zhang. Reasoning with scene graphs for robot planning under partial observability. _IEEE Robotics and Automation Letters_, 7(2):5560-5567, 2022.
* [62] Kiyotaka Otsuji and Yoshinobu Tonomura. Projection detecting filter for video cut detection. In _Proceedings of the first ACM international conference on Multimedia_, pages 251-257, 1993.
* [63] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [64] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. In _NeurIPS_, 2021.
* [65] Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, and Gang Zeng. Bi-directional cross-modality feature propagation with separation-and-aggregation gate for rgb-d semantic segmentation. In _European Conference on Computer Vision (ECCV)_, 2020.
* [66] Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao, and Weicai Zhong. 3d instances as 1d kernels. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIX_, pages 235-252. Springer, 2022.
* [67] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [68] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9224-9232, 2018.
* [69] Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip HS Torr, and Luca Bertinetto. Do different tracking tasks require different appearance models? _NeurIPS_, 2021.
* [70] Yuren Cong, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn, and Michael Ying Yang. Spatial-temporal transformer for dynamic scene graph generation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 16372-16382, 2021.
* [71] OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023.

* [72] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. _ArXiv_, abs/2201.07207, 2022.
* [73] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: A new benchmark dataset for egocentric 3d machine perception. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 20133-20143, 2023.