# Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training

 Hitesh Sapkota Dingrong Wang Zhiqiang Tao Qi Yu

Rochester Institute of Technology

{hxs1943, dw7445, zhiqiang.tao, qi.yu}@rit.edu

Corresponding author

###### Abstract

The recently developed sparse network training methods, such as Lottery Ticket Hypothesis (LTH) and its variants, have shown impressive learning capacity by finding sparse sub-networks from a dense one. While these methods could largely sparsify deep networks, they generally focus more on realizing comparable accuracy to dense counterparts yet neglect network calibration. However, how to achieve calibrated network predictions lies at the core of improving model reliability, especially when it comes to addressing the overconfident issue and out-of-distribution cases. In this study, we propose a novel Distributionally Robust Optimization (DRO) framework to achieve an ensemble of lottery tickets towards calibrated network sparsification. Specifically, the proposed DRO ensemble aims to learn multiple diverse and complementary sparse sub-networks (tickets) with the guidance of uncertainty sets, which encourage tickets to gradually capture different data distributions from easy to hard and naturally complement each other. We theoretically justify the strong calibration performance by showing how the proposed robust training process guarantees to lower the confidence of incorrect predictions. Extensive experimental results on several benchmarks show that our proposed lottery ticket ensemble leads to a clear calibration improvement without sacrificing accuracy and burdening inference costs. Furthermore, experiments on OOD datasets demonstrate the robustness of our approach in the open-set environment.

## 1 Introduction

While there is remarkable progress in developing deep neural networks with densely connected layers, most of these dense networks have poor calibration performance [9], limiting their applicability in safety-critical domains like self-driving cars [4] and medical diagnosis [11]. The poor calibration is mainly due to the fact that there exists a good number of wrongly classified data samples (_i.e.,_ low accuracy) with high confidence resulting from the memorization effect introduced by an over-parameterized architecture [27]. Recent sparse network training methods, such as Lottery Ticket Hypothesis (LTH) [6] and its variants [3; 2; 38; 18; 16; 35] generally assume that there exists a sparse sub-network (_i.e.,_ lottery ticket) in a randomly initialized dense network, which could be trained in isolation and also match the performance of its dense counterpart network in terms of accuracy. While these methods may, to some extent, alleviate the overconfident issue, most of them require pre-training of a dense network followed by multi-step iterative pruning, making the overall training process highly costly, especially for large dense networks. Even for techniques that do not rely on pre-training and iterative pruning (_e.g.,_ Edge Popup or EP [25]), their learning goal focuses on pushing the accuracy up to the original dense networks and hence may still exhibit a severely over-fitting behavior, leading to a poor calibration performance as demonstrated in Figure 1 (b).

Inspired by the recent success of using ensembles to estimate uncertainties [13; 34], a potential solution to realize well-calibrated predictions would be training multiple sparse sub-networks andbuilding an ensemble from them. As such, by leveraging accurate uncertainty quantification, the ensemble is expected to achieve better calibration. However, existing ensemble models of sparse networks rely on pre-training and iterative fine-tuning for learning each sub-network [18, 35], leading to a significant overhead for building the entire ensemble. Furthermore, an ensemble of independently trained sparse sub-networks does not necessarily improve the calibration performance. Since these networks are trained in a similar fashion from the same training data distribution, they could be strongly correlated such that the ensemble model will potentially inherit the overfitting behavior of each sub-network as shown in Figure 1(c). Therefore, the calibration capacity of sparse sub-network ensemble can be compromised as shown empirically in Figure 1 (d).

To further enhance the calibration of the ensemble, it is critical to ensure sufficient diversity among sparse sub-networks so that they are able to complement each other. One natural way to achieve diversity is to allow each sparse sub-network (ticket) to primarily focus on a specific part of training data distribution. This inspires us to leverage the AdaBoost [28] framework that sequentially finds tickets by manipulating training data distribution based on errors. By this means, the AdaBoost facilitates the training for a sequence of complementary sparse sub-networks. However, the empirical analysis (see Table 1) reveals that in the AdaBoost ensemble, most sub-networks (except for the first one) severely under-fit data leading to poor generalization ability. This is mainly because of the overfitting behavior of the first sub-network, which assigns very low training losses to the majority of data samples, making the subsequent sub-networks concentrate on very rare difficult samples that are likely to be outliers or noises. Hence, directly learning from these difficult samples without having global knowledge of the entire training distribution will result in the failure of subsequent training tickets and also hurt the overall calibration.

To this end, we need a more robust learning process for proper training of complementary sparse sub-networks, each of which can be learned in an efficient way to ensure the cost-effective construction of the entire ensemble. We propose a Distributionally Robust Optimization (DRO) framework to schedule learning an ensemble of lottery tickets (sparse sub-networks) with complimentary calibration behaviors that contribute to an overall well-calibrated ensemble as shown in Figure 1 (e-h). Our technique directly searches sparse sub-networks in a randomly initialized dense network without pre-training or iterative pruning. Unlike the AdaBoost ensemble, the proposed ensemble ticket method starts from the original training distribution and eventually allows learning each sub-network from different parts of the training distribution to enrich diversity. This is also fundamentally different from existing sparse ensemble models [18, 35], which attempt to obtain diverse sub-networks in a heuristic way by relying on different learning rates. As a result, these models offer no guaranteed complementary behavior among sparse sub-networks to cover a different part of training data, which is essential to alleviate the overfitting behavior of the learned sparse sub-networks. In contrast, we realize a principled scheduling process by changing the uncertainty set of DRO, where a small set pushes sub-networks learning with easy data samples and a large set focuses on the difficult ones (see Figure 2). By this means, the ticket ensemble governed by our DRO framework could work complementary and lead to much better calibration ability as demonstrated in Figure 1(h). On the one hand, we hypothesize that the ticket found with easy data samples will tend to be learned and

Figure 1: Calibration performance by expected calibration error (ECE) on Cifar100 dataset with ResNet101 architecture with density \(\mathcal{K}=\)15%. EP refers to the Edge Popup algorithm [25].

overfitted easily, resulting in overconfident predictions (Figure 1(e)). On the other hand, the ticket focused on more difficult data samples will be less likely to overfit and may become conservative and give under-confident predictions. Thus, it is natural to form an ensemble of such lottery tickets to complement each other in making calibrated predictions. As demonstrated in Figure 1 (h), owing to the diversity in the sparse sub-networks (e-g), the DRO ensemble exhibits better calibration ability. It is also worth noting that under the DRO framework, our sparse sub-networks already improve the calibration ability as shown in Figure 1 (f-g), which is further confirmed by our theoretical results.

Experiments conducted on three benchmark datasets demonstrate the effectiveness of our proposed technique compared to sparse counterparts and dense networks. Furthermore, we show through the experimentation that because of the better calibration, our model is being able to perform well on the distributionally shifted datasets [6] (CIFAR10-C and CIFAR100-C). The experiments also demonstrate that our proposed DRO ensemble framework can better detect open-set samples on varying confidence thresholds. The contribution of this work can be summarized as follows:

* a new sparse ensemble framework that combines multiple sparse sub-networks to achieve better calibration performance without dense network training and iterative pruning.
* a distributionally robust optimization framework that schedules the learning of an ensemble complementary sub-networks (tickets),
* theoretical justification of the strong calibration performance by showing how the proposed robust training process guarantees to lower the confidence of incorrect predictions in Theorem 2,
* extensive empirical evidence on the effectiveness of the proposed lottery ticket ensemble in terms of competitive classification accuracy and improved open-set detection performance.

## 2 Related Work

**Sparse networks training.** Sparse network training has received increasing attention in recent years. Representative techniques include lottery ticket hypothesis (LTH) [6] and its variants [5; 32]. To avoid training a dense network, supermasks have been used to find the winning ticket in the dense network without training network weights [38]. Edge-Popup (EP) extends this idea by leveraging training scores associated with the neural network weights and only weights with top scores are used for predictions. There are two key limitations to most existing LTH techniques. First, most of them require pre-training of a dense network followed by multi-step iterative pruning making the overall training process expensive. Second, their learning objective remains as improving the accuracy up to the original dense networks and may still suffer from over-fitting (as shown in Figure 1).

**Sparse network ensemble.** There are recent advancements in building ensembles from sparse networks. A pruning and regrowing strategy has been developed in a model, called CigL [16], where dropout serves as an implicit ensemble to improve the calibration performance. CigL requires weight updates and performs pruning and growing for multiple rounds, leading to a high training cost. Additionally, dropping many weights may lead to a performance decrease, which prevents building highly sparse networks. This idea has been further extended by using different learning rates to generate different typologies of the network structure for each sparse network [18; 35]. While diversity among sparse networks can be achieved, there is no guarantee that this can improve the calibration performance of the final ensemble. In fact, different networks may still learn from the training data in a similar way. Hence, the learned networks may exhibit similar overfitting behavior with a high correlation, making it difficult to generate a well-calibrated ensemble. In contrast, the proposed DRO ensemble schedules different sparse networks to learn from complementary parts of the training distribution, leading to improved calibration with theoretical guarantees.

**Model calibration.** Various attempts have been proposed to make the deep models more reliable either through calibration [9; 24; 32] or uncertainty quantification [7; 30]. Post-calibration techniques have been commonly used, including temperature scaling [24; 9], using regularization to penalize overconfident predictions [23]. Recent studies show that post-hoc calibration falls short of providing reliable predictions [22]. Most existing techniques require additional post-processing steps and an additional validation dataset. In our setting, we aim to improve the calibration ability of sparse networks without introducing additional post-calibration steps or validation dataset.

## 3 Methodology

Let \(\mathcal{D}_{\mathcal{N}}=\{\mathbf{X},\mathbf{Y}\}=\{(\mathbf{x}_{1},y_{1}),..,(\mathbf{x}_{N},y_{N})\}\) be a set of training samples where each \(\mathbf{x}_{n}\in\mathbb{R}^{D}\) is a D-dimensional feature vector and \(y_{n}\in[1,C]\) be associated label with \(C\) total classes. Let \(M\) be the total number of base learners used in the given ensemble technique. Further, consider \(\mathcal{K}\) to be the density ratio in the given network, which denotes the percentage of weights we keep during the training process. The major notations are summarized in the Appendix.

### Preliminaries

**Edge-Popup (EP)**[25]. EP finds a lottery ticket (sparse sub-network) from a randomly initialized dense network based on the score values learned from training data. Specifically, to find the sub-network with density \(\mathcal{K}\), the algorithm optimizes the scores associated with each weight in the dense network. During the forward pass, the top-\(\mathcal{K}\) weights in each layer are selected based on their scores. During the backward pass, scores associated with all weights are updated, which allows potentially useful weights that are ignored in previous forward passes to be re-considered.

**Expected calibration error.** Expected Calibration Error (ECE) measures the correspondence between predicted probability and empirical accuracy [20]. Specifically, mis-calibration is computed based on the difference in expectation between confidence and accuracy: \(\mathbb{E}_{\hat{p}}\left[|\mathbb{P}(\hat{y}=y|\hat{p}=p)-p|\right]\). In practice, we approximate the expectation by partitioning confidences into \(T\) bins (equally spaced) and take the weighted average on the absolute difference between each bins' accuracy and confidence. Let \(B_{t}\) denote the \(t\)-th beam and we have \(\text{ECE}=\sum_{t=1}^{T}\frac{|B_{t}|}{N}|acc(B_{t})-conf(B_{t})|\).

### Distributionally Robust Ensemble (DRE)

As motivated in the introduction, to further enhance the calibration of a deep ensemble, it is instrumental to introduce sufficient diversity among the component sparse sub-networks so that they can complement each other when forming the ensemble. One way to achieve diversity is to allow each sparse sub-network to primarily focus on a specific part of the training data distribution. Figure 2 provides an illustration of this idea, where the training data can be imagined to follow a multivariate Gaussian distribution with the red dot representing its mean. In this case, the first sub-network will learn the most common patterns by focusing on the training data close to the mean. The subsequent sub-networks will then learn relatively rare patterns by focusing on other parts of the training data (_e.g.,_ two or three standard deviations from the mean).

**AdaBoost ensemble.** The above idea inspires us to leverage the AdaBoost framework [28] to manipulate the training distribution that allows us to train a sequence of complementary sparse sub-networks. In particular, we train the first sparse sub-network from the original training distribution, where each data sample has an equal probability to be sampled. In this way, the first sparse sub-network can learn the common patterns from the most representative training samples. Starting from the second sub-network, the training distribution is changed according to the losses suffered from the previous sub-network during the last round of training. This allows the later sub-networks to focus on the difficult data samples by following the spirit of AdaBoost.

However, our empirical results reveal that in the AdaBoost ensemble, most sub-networks (except for the first one) severely underfit the training data, leading to a rather poor generalization capability. This is caused by the overfitting behavior of the first sparse sub-network, which assigns very small training losses to a majority of data samples. As a result, the subsequent sub-networks can only focus on a limited number of training samples that correspond to relatively rare patterns (or even outliers and noises) in the training data. Directly learning from these difficult data samples without a general knowledge of the entire training distribution will result in the failure of training the sub-networks.

Figure 2: Robust ensemble where \(\eta\) defines the size of an uncertainty set with \(\eta_{1}\leq\eta_{2}\leq\eta_{3}\).

**Distributionally robust ensemble (DRE).** To tackle the challenge as outlined above, we need a more robust learning process to ensure proper training of complementary sparse sub-networks. Different from the AdaBoost ensemble, the training of all sub-networks starts from the original training distribution in the DRO framework. Meanwhile, it also allows each sub-network to eventually focus on learning from different parts of the training distribution to ensure the desired diverse and complementary behavior. Let \(l(\mathbf{x}_{n},\Theta)\) denote the loss associated with the \(n^{th}\) data sample with \(\Theta\) being the parameters in the sparse sub-network. Then, the total loss is given by

\[\mathcal{L}^{\text{Robust}}(\Theta)=\max_{\mathbf{z}\in\mathcal{U}^{\text{ Robust}}}\sum_{n=1}^{N}z_{n}l(\mathbf{x}_{n},\Theta)\] (1)

The uncertainty set defined to assign weights \(\mathbf{z}\) is given as

\[\mathcal{U}^{\text{Robust}}:=\left\{\mathbf{z}\in\mathbb{R}^{N}:\mathbf{z}^{ \top}\mathbf{1}=1,\mathbf{z}\geq 0,D_{f}(\mathbf{z}\|\frac{\mathbf{1}}{N}) \leq\eta\right\}\] (2)

where \(D_{f}(\mathbf{z}\|\mathbf{q})\) is \(f\)-divergence between two distributions \(\mathbf{z}\) and \(\mathbf{q}\) and \(\eta\) controls the size of the uncertainty set and \(\mathbf{1}\in 1^{N}\) is \(N\)-dimensional unit vector. Depending on the \(\eta\) value, the above robust framework instantiates different sub-networks. For example, by making \(\eta\rightarrow\infty\), we have \(\mathcal{U}^{\text{Robust}}=\left\{\mathbf{z}\in\mathbb{R}^{N}:\mathbf{z}^{ \top}\mathbf{1}=1,\mathbf{z}\geq 0,D_{f}(\mathbf{z}\|\frac{\mathbf{1}}{N}) \leq\infty\right\}\). In this case, we train a sub-network by only using the most difficult sample in the training set. On the other extreme with \(\eta\to 0\), we have \(\mathcal{U}^{\text{Robust}}=\left\{\mathbf{z}\in\mathbb{R}^{N}:\mathbf{z}^{ \top}\mathbf{1}=1,\mathbf{z}\geq 0,D_{f}(\mathbf{z}\|\frac{\mathbf{1}}{N}) \leq 0\right\}\), which assigns equal weights to all data samples. So, the sub-network learns from the original training distribution.

To fully leverage the key properties of the robust loss function as described above, we propose to perform distributionally robust ensembling learning to generate a diverse set of sparse sub-networks with well-controlled overfitting behavior that can collectively achieve superior calibration performance. The training process starts with a relatively small \(\eta\) value to ensure that the initially generated sub-networks can adequately capture the general patterns from the most representative data samples in the original training distribution. The training proceeds by gradually increasing the \(\eta\) value, which allows the subsequent sub-networks to focus on relatively rare and more difficult data samples. As a result, the later generated sub-networks tend to produce less confident predictions that complement the sub-networks generated in the earlier phase of the training process. This diverse and complementary behavior among different sparse sub-networks is clearly illustrated in Figure 1 (e)-(g). During the ensemble phase, we combine the predictions of different sub-networks in the logit space by taking the mean and then performing the softmax. In this way, the sparse sub-networks with high \(\eta\) values help to lower the overall confidence score, especially those wrongly predicted data samples. Furthermore, the sub-networks with lower \(\eta\) values help to bring up the confidence score of correctly predicted data samples. Thus, the overall confidence score will be well compensated, resulting in a better calibrated ensemble.

### Theoretical Analysis

In this section, we theoretically justify why the proposed DRE framework improves the calibration performance by extending the recently developed theoretical framework on multi-view learning [1]. In particular, we will show how it can effectively lower the model's false confidence on its wrong predictions resulting from spurious correlations. To this end, we first define the problem setup that includes some key concepts used in our theoretical analysis. We then formally show that DRO helps to tackle the spurious correlations by learning from less frequent features that characterize difficult data samples in a training dataset. This important property further guarantees better calibration performance of DRO as we show in the main theorem. It is worth to note that our theoretical analysis is primarily from the spurious correlation perspective. This is only one of the potential sources that can lead to over-confidence, resulting in poor-calibration in neural networks. The goal is offer deeper insights on why the proposed approach is able to improve the calibration performance resulting from spurious correlations by effectively lowering the model's false confidence on its wrong predictions.

**Problem setup.** Assume that each data sample \(\mathbf{x}_{n}\in\mathbb{R}^{D}\) is divided into \(P\) total patches, where each patch is a \(d\)-dimensional vector. For the sake of simplicity, let us assume each class \(c\in[1,C]\) has two characterizing (major) features \(\mathbf{v}_{c}=\{\mathbf{v}_{c,i}\}_{t=1}^{L}\) with \(L=2\). For example, the features for Cars could be Headlights and Tires. Let \(\mathcal{D}_{N}^{S}\) and \(\mathcal{D}_{N}^{M}\) denote the set of _single-view_ and _multi-view_ datasamples, respectively, which are formally defined as

\[\begin{cases}\{\mathbf{x}_{n},y_{n}\}\in\mathcal{D}_{N}^{S}\text{ if one of }\mathbf{v}_{c,1}\text{ or }\mathbf{v}_{c,2}\text{ appears along with some noise features}\\ \{\mathbf{x}_{n},y_{n}\}\in\mathcal{D}_{N}^{M}\text{ if both }\mathbf{v}_{c,1} \text{ and }\mathbf{v}_{c,2}\text{ appears along with some noise features}\end{cases}\] (3)

The noise features (also called minor features) refer to those that do not characterize (or differentiate) a given class \(c\) (_e.g.,_ being part of the background). In important applications like computer vision, images supporting such a "multi-view" structure is very common [1]. For example, for most car images, we can observe all main features, such as Wheels, Tires, and Headlights so they belong to \(\mathcal{D}_{N}^{M}\). Meanwhile, there may also be images, where multiple features are missing. For example, if the car image is taken from the front, the tire and wheel features may not be captured. In most real-world datasets, such single-view data samples are usually much limited as compared to their multi-view counterparts. The Appendix provides concrete examples of both single and multi-view images. Let us consider \((\mathbf{x},y)\in\mathcal{D}_{N}^{S}\) with the major feature \(\mathbf{v}_{c,l}\) where \(y=c\). Then each patch \(\mathbf{x}^{p}\in\mathbb{R}^{d}\) can be expressed as

\[\mathbf{x}^{p}=a^{p}\mathbf{v}_{c,l}+\sum_{\mathbf{v}^{\prime}\in\cup\backslash \mathbf{v}_{c}}\alpha^{p,\mathbf{v}^{\prime}}\mathbf{v}^{\prime}+\epsilon^{p}\] (4)

where \(\cup=\{\mathbf{v}_{c,1},\mathbf{v}_{c,2}\}_{c=1}^{C}\) is collection of all features, \(a^{p}>0\) is the weight allocated to feature \(\mathbf{v}_{c,l}\), \(\alpha^{p,\mathbf{v}^{\prime}}\in[0,\gamma]\) is the weight allocated to the noisy feature \(\mathbf{v}^{\prime}\) that is not present in feature set \(\mathbf{v}_{c}\)_i.e.,_\(\mathbf{v}^{\prime}\in\cup\backslash\mathbf{v}_{c}\), and \(\boldsymbol{\epsilon}^{p}\sim\mathcal{N}(0,(\sigma^{p})^{2}\mathbb{I})\) is a random Gaussian noise. In (4), a patch \(\mathbf{x}^{p}\) in a single-view sample \(\mathbf{x}\) also contains set of minor (noise) features presented from other classes _i.e.,_\(\mathbf{v}^{\prime}\in\cup\backslash\mathbf{v}_{c}\) in addition to the main feature \(\mathbf{v}_{c,l}\). Since \(\mathbf{v}_{c,l}\) characterizes class \(c\), we have \(a^{p}>\alpha^{p,\mathbf{v}^{\prime}};\forall\mathbf{v}^{\prime}\in\cup \backslash\mathbf{v}_{c}\). However, since the single-view data samples are usually sparse in the training data, it may prevent the model from accumulating a large \(a^{p}\) for \(\mathbf{v}_{c,l}\) as shown Lemma 1 below. In contrast, some noise \(\mathbf{v}^{\prime}\) may be selected as the dominant feature (due to spurious correlations) to minimize the errors of specific training samples, leading to potential overfitting of the model.

We further assume that the network contains \(H\) convolutional layers, which outputs \(F(\mathbf{x};\Theta)=(F_{1}(\mathbf{x}),...F_{C}(\mathbf{x}))\in\mathbb{R}^{C}\). The logistic output for the \(c^{th}\) class can be represented as

\[F_{c}(\mathbf{x})=\sum_{h\in[H]}\sum_{p\in[P]}\text{ReLU}[\langle\Theta_{c,h}, \mathbf{x}^{p}\rangle]\] (5)

where \(\Theta_{c,h}\) denote the \(h^{th}\) convolution layer (feature map) associated with class \(c\). Under the above data and network setting, we propose the following lemma.

**Lemma 1**.: _Let \(\mathbf{v}_{c,l}\) be the main feature vector present in the single-view data \(\mathcal{D}_{N}^{S}\). Assume that number of single-view data samples containing feature \(\mathbf{v}_{c,l}\) is limited as compared with the rest, i.e., \(N_{\mathbf{v}_{c,l}}\ll N_{\cup\backslash\mathbf{v}_{c,l}}\). Then, at any iteration \(t>0\), we have_

\[\langle\Theta_{c,h}^{t+1},\mathbf{v}_{c,l}\rangle=\langle\Theta_{c,h}^{t}, \mathbf{v}_{c,l}\rangle+\beta\max_{\mathbf{z}\in\mathcal{U}}\sum_{n=1}^{N}z_{ n}\left[\mathbb{1}_{y_{j}=c}(V_{c,h,l}(\mathbf{x}_{n})+\kappa)(1-\texttt{SOFT}_{c}(F( \mathbf{x}_{n})))\right]\] (6)

_where \(\kappa\) is a dataset specific constant, \(\beta\) is the learning rate, \(\texttt{SOFT}_{c}\) is the softmax output for class \(c\), and \(V_{c,h,l}(\mathbf{x}_{j})=\sum_{p\in\mathcal{P}_{\mathbf{v}_{c,l}}(\mathbf{x} _{j})}\text{ReLU}(\langle\Theta_{c,h},\mathbf{x}_{j}^{p}\rangle a^{p})\) with \(\mathcal{P}_{\mathbf{v}_{c,l}}(\mathbf{x}_{j})\) being the collection of patches containing feature \(\mathbf{v}_{c,l}\) in \(\mathbf{x}_{j}\). The set \(\mathcal{U}\) is an uncertainty set that assigns a weight to each data sample based on it loss. In particular, the uncertainty set under DRO is given as in (2) and we further define the uncertainty set under ERM: \(\mathcal{U}^{\text{ERM}}:=\big{\{}\mathbf{z}\in\mathbb{R}^{N}:z_{n}=\frac{1}{N} ;\forall n\in[1,N]\big{\}}\). Learning via the robust loss in (1) leads to a stronger correlation between the network weights \(\Theta_{c,h}\) and the single-view data feature \(\mathbf{v}_{c,l}\):_

\[\{\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l}\rangle\}_{Robust}>\{\langle\Theta_{c, h}^{t},\mathbf{v}_{c,l}\rangle\}_{ERM};\forall t>0\] (7)

**Remark.** The robust loss \(\mathcal{L}^{\text{Robust}}\) forces the model to learn from the single-view samples (according to the loss) by assigning a higher weight. As a result, the network weights will be adjusted to increase the correlation with the single-view data features \(\mathbf{v}_{c,l}\) due to Lemma 1. In contrast, for standard ERM, weight is uniformly assigned to all samples. Due to the sparse single-view data features (which also makes them more difficult to learn from, leading to a larger loss), the model does not grow sufficient correlation with \(\mathbf{v}_{c,l}\). In this case, the ERM model instead learns to memorize some noisy feature \(\mathbf{v}^{\prime}\) introduced through certain spurious correlations. For a testing data sample, the ERM model may confidently assign it to an incorrect class \(k\) according to the noise feature \(\mathbf{v}^{\prime}\). In the theorem below, we show how the robust training process can effectively lower the confidence of incorrect predictions, leading to an improved calibration performance.

**Theorem 2**.: _Given a new testing sample \(\mathbf{x}\in\mathcal{D}_{S}^{N}\) containing \(\mathbf{v}_{c,l}\) as the main feature and a dominant noise feature \(\mathbf{v}^{\prime}\) that is learned due to memorization, we have_

\[\{\texttt{SOFT}_{k}(\mathbf{x})\}_{Robust}<\{\texttt{SOFT}_{k}(\mathbf{x})\} _{ERM}\] (8)

_where \(\mathbf{v}^{\prime}\) is assumed to be a main feature characterizing class \(k\)._

**Remark.** For ERM, due to the impact of the dominate noisy feature \(\mathbf{v}^{\prime}\), it assigns a large probability to class \(k\) since \(\mathbf{v}^{\prime}\) is one of its major features, leading to high confidence for an incorrect prediction. In contrast, the robust learning process allows the model to learn a stronger correlation with the main feature \(\mathbf{v}_{c,l}\) as shown in Lemma 1. Thus, the model is less impacted by the noise feature \(\mathbf{v}^{\prime}\), resulting in reduced confidence in predicting the wrong class \(k\). Such a key property guarantees an improved calibration performance, which is clearly verified by our empirical evaluation. It is also worth noting that Theorem 2 does not necessarily lead to better classification accuracy. This is because (8) only ensures that that the false confidence is lower than an ERM model, but there is no guarantee that \(\{\texttt{SOFT}_{k}(\mathbf{x})\}_{Robust}<\{\texttt{SOFT}_{c}(\mathbf{x})\} _{Robust}\). It should be noted that our DRE framework ensures diverse sparse sub-network focusing on different single-view data samples from different classes. As such, an ensemble of those diverse sparse subnetworks provides maximum coverage of all features (even the weaker one) and therefore can ultimately improve the calibration performance. The detailed proofs are provided in the Appendix.

## 4 Experiments

We perform extensive experimentation to evaluate the distributionally robust ensemble of sparse sub-networks. Specifically, we test the ability of our proposed technique in terms of calibration and classification accuracy. For this, we consider three settings: (a) general classification, (b) out-of-distribution setting where we have in-domain data but with different distributions, and (c) open-set detection, where we have unknown samples from new domains.

### Experimental Settings

**Dataset description.** For the general classification setting, we consider three real-world datasets: Cifar10, Cifar100 [12], and TinyImageNet [14]. For the out-of-distribution setting, we consider the corrupted version of the Cifar10 and Cifar100 datasets which are named Cifar10-C and Cifar100-C [10]. It should be noted that in this setting, we train all models in clean dataset and perform testing in the corrupted datasets. For open-set detection, we use the SVHN dataset [21] as the open-set dataset and Cifar10 and Cifar100 as the close-set data. A more detailed description of each dataset is presented in the Appendix.

**Evaluation metrics.** To assess the model performance in the first two settings, we report the classification accuracy (\(\mathcal{ACC}\)) along with the Expected Calibration Error (\(\mathcal{ECE}\)). In the case of open-set detection, we report open-set detection for different confidence thresholds.

**Implementation details.** In all experiments, we use a family of ResNet architectures with two density levels: \(9\%\) and \(15\%\). To construct an ensemble, we learn 3 sparse sub-networks each with a density of \(3\%\) for the total of \(9\%\) density and that of \(5\%\) density for the total of density \(15\%\). All experiments are conducted with the 200 total epochs with an initial learning rate of 0.1 and a cosine scheduler function to decay the learning rate over time. The last-epoch model is taken for all analyses. For the training loss, we use the EP-loss in our DRO ensemble that optimizes the scores for each weight and finally selects the sub-network from the initialized dense network for the final prediction. The selection is performed based on the optimized scores. More detailed information about the training process and hyperparameter settings can be found in the Appendix.

### Performance Comparison

In our comparison study, we include baselines that are relevant to our technique and therefore we primarily focus on the LTH-based techniques. Specifically, we include the initial lottery ticket hypothesis (LTH) [6] that iteratively performs pruning from a dense network until the randomly initialized sub-network with a given density is reached. Once the sub-network is found, the model trains the sub-network using the training dataset. Similarly, we also include L1 pruning [17]. We also include three approaches CigL [16], Sup-ticket [35], DST Ensemble [18] which are based on the pruning and regrowing sparse network training strategies. From Venkatesh et al. [32] we consider MixUp strategy as a comparison baseline as it does not require multi-step forward passes. A dense network is also included as a reference (denoted as _Dense_1). Furthermore, we report the performance obtained using the EP algorithm [25] on a single model with a given density. Finally, we also include the deep ensemble technique (_i.e.,_ Sparse Network Ensemble (SNE), where each base model is randomly initialized and independently trained. The approaches that require pre-training of a dense network are categorized under the _Dense Pre-training_ category. Those performing sparse network training but actually updating the network parameters are grouped as _Sparse Training_. It should be noted that sparse training techniques still require iterative pruning and regrowing. Finally, techniques that attempt to search the best initialized sparse sub-network through mask update (_e.g.,_ EP) are grouped as _Mask Training_.

\begin{table}
\begin{tabular}{l l c c c c c c c c} \hline \hline \multirow{2}{*}{Training Type} & \multirow{2}{*}{Approach} & \multicolumn{5}{c}{Cifar10} & \multicolumn{5}{c}{Cifar100} \\ \cline{3-10}  & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\ \cline{3-10}  & & & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline \hline  & _Dense_1 & 94.82 & 5.87 & 95.12 & 5.99 & 76.40 & 16.89 & 77.97 & 16.73 \\ \hline \hline Dense Pre-training & _L1 Pruning [17]_ & 93.45 & 5.31 & 93.67 & 6.14 & 75.11 & 15.89 & 75.12 & 16.24 \\  & _L1H [6]_ & 92.65 & 3.68 & 92.87 & 6.02 & 74.09 & 15.45 & 74.41 & 16.12 \\  & _DLTH [3]_ & 93.27 & 5.87 & 95.12 & 7.09 & 77.29 & 16.64 & 77.86 & 17.26 \\  & _Mixup [32]_ & 92.86 & 3.68 & 93.06 & 6.01 & 74.15 & 5.41 & 74.28 & 16.05 \\ \hline Sparse Training & _CigL [16]_ & 92.39 & 5.06 & 93.41 & 4.60 & 76.40 & 9.30 & 76.46 & 9.91 \\  & _DST Ensemble [18]_ & 88.87 & 2.02 & 84.93 & 0.8 & 63.57 & 7.23 & 63.22 & 6.18 \\  & Sup-ticket [35] & 94.52 & 3.30 & 95.04 & 3.10 & 78.28 & 10.20 & 78.60 & 10.50 \\ \hline Mask Training & _AdaBoost_ & 93.12 & 5.13 & 94.15 & 5.46 & 75.15 & 22.96 & 75.89 & 24.54 \\  & _EP [25]_ & 94.20 & 3.97 & 94.35 & 4.03 & 75.05 & 14.62 & 75.68 & 14.41 \\  & _SNE_ & 94.70 & 2.51 & 94.48 & 3.51 & 75.69 & 9.02 & 75.22 & 10.89 \\  & **DRE (Ours)** & 94.60 & **0.7** & 94.28 & **0.7** & 74.68 & **1.20** & 74.37 & **2.09** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy and ECE performance with \(9\%\) density for Cifar10 and Cifar100.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline \multirow{2}{*}{Training Type} & \multirow{2}{*}{Approach} & \multicolumn{5}{c}{Cifar10} & \multicolumn{5}{c}{Cifar100} \\ \cline{3-10}  & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\ \cline{3-10}  & & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline \hline  & Dense1 & 79.65 & 19.63 & 79.65 & 19.63 & 54.75 & 35.32 & 54.75 & 35.32 \\ \hline \hline Dense Pre-training & _L1 Pruning [17]_ & 77.34 & 17.95 & 76.39 & 17.89 & 52.06 & 31.45 & 51.67 & 30.98 \\  & _L1H [6]_ & 75.85 & 17.88 & 76.15 & 17.62 & 50.79 & 31.23 & 51.35 & 30.56 \\  & _DLTH [3]_ & 79.67 & 21.74 & 80.12 & 20.31 & 54.82 & 37.55 & 55.12 & 35.74 \\  & _Mixup [32]_ & 76.35 & 17.74 & 76.88 & 17.55 & 51.36 & 31.12 & 51.92 & 30.35 \\ \hline Sparse Training & _CigL [16]_ & 70.80 & 21.04 & 69.84 & 21.42 & 49.42 & 25.86 & 51.49 & 24.13 \\  & Sup-ticket [35]_ & 72.89 & 17.80 & 73.01 & 18.82 & 48.80 & 24.99 & 48.81 & 25.62 \\ \hline Mask Training & _AdaBoost_ & 75.94 & 22.96 & 74.55 & 21.46 & 51.36 & 38.45 & 51.25 & 38.34 \\  & _EP [25]_ & 77.58 & 17.82 & 77.73 & 17.46 & 52.18 & 30.60 & 52.14 & 29.48 \\  & _SNE_ & 78.93 & 15.73 & 78.61 & 15.56 & 54.74 & 24.22 & 54.00 & 20.54 \\  & **DRE (Ours)** & 78.57 & **10.92** & 78.00 & **10.19** & 54.11 & **14.28** & 53.21 & **8.13** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy and ECE performance on out-of-distribution datasets.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline \multirow{2}{*}{Training Type} & \multirow{2}{*}{Approach} & \multicolumn{5}{c}{Cifar10} & \multicolumn{5}{c}{Cifar100} \\ \cline{3-10}  & & & ResNet50 & ResNet101 & ResNet101 & ResNet152 \\ \cline{3-10}  & & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline \hline  & Dense1 & 79.65 & 19.63 & 79.65 & 19.63 & 54.75 & 35.32 & 54.75 & 35.32 \\ \hline \hline Dense Pre-training & _L1 Pruning [17]_ & 77.34 & 17.95 & 76.39 & 17.89 & 52.06 & 31.45 & 51.67 & 30.98 \\  & _L1H [6]_ & 75.85 & 17.88 & 76.15 & 17.62 & 50.79 & 31.23 & 51.35 & 30.56 \\  & _DLTH [3]_ & 79.67 & 21.74 & 80.12 & 20.31 & 54.82 & 37.55 & 55.12 & 35.74 \\  & _Mixup [32]_ & 76.35 & 17.74 & 76.88 & 17.55 & 51.36 & 31.12 & 51.92 & 30.35 \\ \hline Sparse Training & _CigL [16]_ & 70.80 & 21.04 & 69.84 & 21.42 & 49.42 & 25.86 & 51.49 & 24.13 \\  & Sup-ticket [35]_ & 72.89 & 17.80 & 73.01 & 18.82 & 48.80 & 24.99 & 48.81 & 25.62 \\ \hline Mask Training & _AdaBoost_ & 75.94 & 22.96 & 74.55 & 21.46 & 51.36 & 38.45 & 5

**General classification setting.** In this setting, we consider clean Cifar10, Cifar100, and TinyImageNet datasets. Tables 1, 2, and 6 (in the Appendix) show the accuracy and calibration error for different models with density \(9\%\) and \(15\%\). It should be noted that for the TinyImageNet dataset, we could not run the Sparse Training techniques due to the computation issue (_i.e.,_ memory overflow). This may be because sparse training techniques require maintaining additional parameters for the pruning and regrowing strategy. In the Appendix, we have made a comparison of the proposed DRE with those baselines on a lower architecture size. There are three key observations we can infer from the experimental results. First, sparse networks are able to maintain or improve the generalization performance (in terms of accuracy) with better calibration, which can be seen by comparing dense network performance with the edge-popup algorithm. Second, the ensemble in general helps to further lower the calibration error (lower the better). For example, in all datasets, standard ensemble (SNE) consistently improves the EP model. Finally, the proposed DRE significantly improves the calibration performance by diversifying base learners and allow each sparse sub-network to focus on different parts of the training data. The strong calibration performance provides clear empirical evidence to justify our theoretical results.

**Out-of-distribution classification setting.** In this setting, we assess the effectiveness of the proposed techniques on out-of-distribution samples. Specifically, [10] provide the Cifar10-C and Cifar100-C validation datasets which are different than that of the original clean datasets. They apply different corruptions (such as blurring noise, and compression) to shift the distribution of the datasets. We assess those corrupted datasets using the models trained using the clean dataset. Table 3 shows the performance using different architectures. In this setting, we have not included DST Ensemble, because: (a) its accuracy is far below the SOTA performance, and (b) same training mechanism as that of the Sup-ticket, whose performance is reported. As shown, the proposed DRE provides much better calibration performance even with the out of distribution datasets.

**Open-set detection setting.** In this setting, we demonstrate the ability of our proposed DRO ensemble in detecting open-set samples. For this, we use the SVHN dataset as an open-set dataset. Specifically, if we have a better calibration, we would be able to better differentiate the open-set samples based on the confidence threshold. For this, we randomly consider \(20\%\) of the total testing in-distribution dataset as the open-set samples from the SVHN dataset. The reason for only choosing a subset of the dataset is to imitate the practical scenario where we have very few open-set samples compared to the close-set samples. We treat the open-set samples as the positive and in-distribution (close-set) ones as the negative. Since this is a binary detection problem, we compute the F-score [8] at various thresholds, which considers both precision and recall. Figure 3 shows the performance for the proposed technique along with comparative baselines. As shown, our proposed DRE (refereed as DRO Ensemble) always stays on the top for various confidence thresholds which demonstrates that strong calibration performance can benefit DRE for open-set detection as compared to other baselines.

### Ablation Study

In this section, we investigate the impact of the backbone architecture along with the size of the ensemble. Additional ablation studies are presented in Appendix D.9.

**Performance analysis of different backbones.** Table 4 (a) reports the performance of Cifar10 from both DRE and EP using different backbone architectures. In case of WideResNet28-10, the calibration error is low without sacrificing the accuracy. It also demonstrates that the superior performance of DRE is not limited to a specific backbone. In case of ViT, DRE still achieves a much lower calibration error than EP. However, using ViT as a backbone, the accuracy from both EP and DRE is lower and ECE is higher than other backbones. Existing studies show that without pretraining, the lack of useful

Figure 3: Open-set detection performance on different confidence thresholds.

inductive biases for ViT can cause performance drop [1]. Since no pretraining is conducted in both EP and DRE, it causes a lower accuracy (and a higher ECE).

**Impact of number of sparse-sub-networks.** In this analysis, we study the impact of number of sparse sub-networks. It should be noted that our work is not limited only for \(M=3\). We can instead increase the \(M\) value. For example, Table 4 (b) shows the performance for an ensemble model with \(M=5\), where each sub-network is trained with \(\mathcal{K}=3\%\), leading to a total \(\mathcal{K}=15\%\). We also show the performance with \(M=3\), where each sub-network is trained with \(\mathcal{K}=5\%\). As can be seen, if there is a sufficient learning capacity for each sub-network, the ECE score can further improve with the increase of \(M\).

### Qualitative Analysis

In this section, we provide illustrative examples to further justify why the proposed DRE is better calibrated as compared to existing baselines. Figure 4 (a)-(d) show the confidence values for the wrongly classified samples by different sparse sub-networks in the DRE ensemble. As can be seen, each sparse sub-network provides confidence values in different ranges, where the sub-network in (a) is learned from most representative samples and the one in (c) is from the most difficult ones. As these sub-networks are complementary with each other, the DRE has a much better confidence distribution for incorrect samples than the baselines. In contrast, as shown in Figure 8 of Appendix D.12, all the baselines, including the dense network, EP sparse networks, and the SNE, tend to allocate much higher confidence values to wrongly classified examples, leading to poor calibration. In case of correctly classified samples, the proposed DRE generates confident predictions and thereby not compromising the calibration performance, which is further discussed in Appendix D.12.

## 5 Conclusion

In this paper, we proposed a novel DRO framework, called DRE, that achieves an ensemble of lottery tickets towards calibrated network sparsification. Specifically, with the guidance of uncertainty sets under the DRO framework, the proposed DRE aims to learn multiple diverse and complementary sparse sub-networks (tickets) where uncertainty sets encourage tickets to gradually capture different data distributions from easy to hard and naturally complement each other. We have theoretically justified the strong calibration performance by demonstrating how the proposed robust training process guarantees to lower the confidence of incorrect predictions. The extensive evaluation shows that the proposed DRE leads to significant calibration improvement without sacrificing the accuracy and burdening inference cost. Furthermore, experiments on OOD and open-set datasets show its effectiveness in terms of generalization and novelty detection capability, respectively.

\begin{table}

\end{table}
Table 4: ACC and ECE with different backbones and number of subnetworks

Figure 4: Confidence scores of incorrectly classified samples in CIFAR100 with ResNet101.

## Acknowledgement

This research was supported in part by an NSF IIS award IIS-1814450 and an ONR award N00014-18-1-2875. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.

## References

* [1] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [2] Yue Bai, Huan Wang, Xu Ma, Yitian Zhang, Zhiqiang Tao, and Yun Fu. Parameter-efficient masking networks. In _Advances in Neural Information Processing Systems_, 2022.
* [3] Yue Bai, Huan Wang, Zhiqiang Tao, Kunpeng Li, and Yun Fu. Dual lottery ticket hypothesis. In _International Conference on Learning Representations_, 2022.
* [4] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving cars, 2016.
* [5] Tianlong Chen, Zhenyu Zhang, Jun Wu, Randy Huang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Can you win everything with a lottery ticket? _Transactions on Machine Learning Research_, 2022.
* [6] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2018.
* [7] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning, 2015.
* [8] Cyril Goutte and Eric Gaussier. A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. In David E. Losada and Juan M. Fernandez-Luna, editors, _Advances in Information Retrieval_, pages 345-359, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.
* Volume 70_, ICML'17, page 1321-1330. JMLR.org, 2017.
* [10] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _Proceedings of the International Conference on Learning Representations_, 2019.
* 274, 2011.
* [12] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
* [13] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 6405-6416, Red Hook, NY, USA, 2017. Curran Associates Inc.
* [14] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015.
* [15] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. SNIP: single-shot network pruning based on connection sensitivity. _CoRR_, abs/1810.02340, 2018.
* [16] Bowen Lei, Ruqi Zhang, Dongkuan Xu, and Bani Mallick. Calibrating the rigged lottery: Making all tickets reliable. In _The Eleventh International Conference on Learning Representations_, 2023.

* [17] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets, 2016.
* [18] Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity, 2022.
* [19] Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. _ArXiv_, abs/1902.05967, 2019.
* [20] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In _Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence_, AAAI'15, page 2901-2907. AAAI Press, 2015.
* [21] Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* [22] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift, 2019.
* [23] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions, 2017.
* [24] John Platt and Nikos Karampatziakis. Probabilistic outputs for svms and comparisons to regularized likelihood methods. 2007.
* [25] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What's hidden in a randomly weighted neural network?, 2019.
* [26] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _CoRR_, abs/1911.08731, 2019.
* [27] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations, 2020.
* [28] Robert E Schapire. Explaining adaboost. In _Empirical inference_, pages 37-52. Springer, 2013.
* [29] Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. _CoRR_, abs/1610.02391, 2016.
* [30] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty, 2018.
* [31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision, 2015.
* [32] Bindya Venkatesh, Jayaraman J. Thiagarajan, Kowshik Thopalli, and Prasanna Sattigeri. Calibrate and prune: Improving reliability of lottery tickets through prediction calibration, 2020.
* [33] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. 2020.
* [34] Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 4697-4708, 2020.
* [35] Lu Yin, Vlado Menkovski, Meng Fang, Tianjin Huang, Yulong Pei, Mykola Pechenizkiy, Decebal Constantin Mocanu, and Shiwei Liu. Superposing many tickets into one: A performance booster for sparse neural network training, 2022.

* [36] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, Siyue Wang, Minghai Qin, Bin Ren, Yanzhi Wang, Sijia Liu, and Xue Lin. MEST: accurate and fast memory-economic sparse training framework on the edge. _CoRR_, abs/2110.14032, 2021.
* [37] Jize Zhang, Bhavya Kailkhura, and T. Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning, 2020.
* [38] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask, 2019.

Supplementary Materials

## Appendix

Table of Contents

* A Summary of Notations
* B Robust Loss Optimization in DRO
* B.1 Robust Loss Optimization
* B.2 Hyperparameter settings
* C Proofs of Main Theoretical Results
* C.1 Proof of Lemma 1
* C.2 Proof of Theorem 2
* D Experimental Details and Additional Results
* D.1 Detailed Dataset Description
* D.2 Hardware Details for Experimentation
* D.3 Single-view and Multi-view Examples
* D.4 Additional Result on Cifar10 and Cifar100
* D.5 Additional Baseline Results on Sparse Training Methods
* D.6 Additional Baseline Results on TinyImageNet
* D.7 Performance from Ensemble Members
* D.8 Comparison with Common Calibration Techniques
* D.9 Ablation Study
* D.10 Parameter Size and Inference Speed
* D.11 Diversity on Sparse Sub-networks
* D.12 Qualitative Analysis
* E Broader Impact, Limitations, and Future Work
* E.1 Broader Impact
* E.2 Limitations and Future Works
* F Source Code

## Organization of Appendix

In this appendix, we first present a table summarizing the major notations used by the main paper in Appendix A. Next, we provide detailed information about the training process and hyperaparameters setting B. We provide the detailed proof of Lemma 1 and Theorem 2 in Section C. After that, we provide additional experimental details and results in Appendix D. Finally, we discuss the broader impacts, limitations, and future work of our DRE technique in Appendix E. The link to the source code can be found in the end of the Appendix.

## Appendix A Summary of Notations

Table 5 below shows the major notations used in the main paper. We further assign each notation into one of four major categories: dataset, DRO formulation, sparse training, and theoretical results.

## Appendix B Robust Loss Optimization in DRO

In this section, we first provide a detailed description on how we optimize the robust loss function in (1). We then explain how to set the uncertainty set by choosing a proper hyperparameter.

### Robust Loss Optimization

The optimization problem specified in (1) involves an inequality constraint so directly solving it may incur a higher computational overhead. Therefore, we consider a regularized version of the robust loss to train each base learner by using the following loss:

\[\mathcal{L}^{Robust}=\max_{\mathbf{z}\geq 0,\mathbf{z}^{\top}\mathbf{z}=1} \sum_{n=1}^{N}z_{n}l_{n}(\Theta)-\lambda D_{f}\left(\mathbf{z}||\frac{1}{N}\right)\] (9)

where \(l_{n}(\Theta)=l(\mathbf{x}_{n},\Theta)\). Solving the above maximization problem leads to a closed-form solution for \(\mathbf{z}^{*}\) as shown by the following lemma:

\begin{table}
\begin{tabular}{|c||c||c||} \hline
**Symbol Group** & **Notation** & **Description** \\ \hline \multirow{6}{*}{Dataset} & \(\mathbf{X}\) & Set of training images \\  & \(\mathbf{Y}\) & Set of training class labels \\  & \(\mathbf{C}\) & Total classes \\  & \(\hat{y}\) & Predicted class label \\  & \(N\) & Total number of training samples \\  & \(D\) & Dimensionality of each data sample \\ \hline \multirow{3}{*}{DRO} & \(D_{f}\) & \(f\)-divergence \\  & \(\eta\) & Parameter controlling size of uncertainty set in DRO framework \\  & \(z_{n}\) & Weight associated with \(n^{th}\) data sample \\ \hline \multirow{6}{*}{Sparse Training} & \(M\) & Number of sparse sub-networks \\  & \(\mathcal{K}\) & Density of the given network \\  & \(\Theta\) & Parameter associated with given neural network \\  & \(\hat{p}\) & Confidence associated with predicted class \\  & \(l(\mathbf{x}_{n},\Theta)\) & Loss associated with \(n^{th}\) data sample \\ \hline \multirow{6}{*}{Theoretical Results} & \(\beta\) & Learning rate of the given network \\  & \(P\) & Total number of patches in each data sample \\  & \(d\) & Dimensionality of each patch \\  & \(\mathbf{v}_{c,l}\) & Major \(l^{th}\) feature associated with class c \\  & \(L\) & Total number of features in each class class \\  & \(D_{N}^{S}\) & Collection of single-view data samples \\  & \(D_{N}^{N}\) & Collection of multi-view data samples \\  & \(\cup\) & Collection of features \\  & \(H\) & Number of convolution layers \\  & \(F_{c}(\mathbf{x})\) & Logistic output for the \(c^{th}\) class for the data sample \(\mathbf{x}\) \\  & \(\mathcal{P}_{\mathbf{v}_{c,l}}\) & Collection of patches containing feature \(\mathbf{v}_{c,l}\) in sample \(\mathbf{x}_{j}\) \\  & SOFT\({}_{c}\) & Softmax output for class \(c\) \\ \hline \end{tabular}
\end{table}
Table 5: Symbols with Descriptions.

**Lemma 3**.: _Assuming that \(D_{f}\) is the KL divergence, then solving (9) leads to the following solution_

\[\mathcal{L}^{\text{Robust}}=\sum_{n=1}^{N}z_{n}^{*}l_{n}(\Theta)\] (10)

_where \(z_{n}^{*}\) is given by_

\[z_{n}^{*}=\frac{\exp\left(\frac{l_{n}(\Theta)}{\lambda}\right)}{\sum_{j=1}^{N} \exp\left(\frac{l_{j}(\Theta)}{\lambda}\right)}\] (11)

It can be verified that there is a one-to-one correspondence between \(\eta\) in (2) and \(\lambda\) in (9). Given their roles in the corresponding equations, a large \(\eta\) implies a small \(\lambda\) and a small \(\eta\) implies a large \(\lambda\).

### Hyperparameter settings

The hyperparameter in the regularization term is chosen based on the difficulty of a dataset. Specifically, for DRE, we always consider the \(\lambda\rightarrow\infty\) for the first sparse sub-network which is equivalent to Expected Risk Minimization (ERM). For the second and third sub-networks, we choose this hyperparameter based on the difficulty of data samples. It should be noted that we need to set higher \(\lambda\) values for more difficult datasets as difficult samples are more common on those datasets. Using this notion, for Cifar10, we choose small \(\lambda\) values so that the model can focus on the difficult samples that are few. For this, we choose \(\lambda=10\) for the second sparse sub-network and \(\lambda=500\) for the third sparse sub-network. Considering Cifar100 is more difficult, we would have more difficult samples and therefore higher \(\lambda\) value is preferred. For this, we choose \(\lambda=50\) for the second sparse sub-network and \(\lambda=500\) for the third one. In the case of TinyImageNet, we have many difficult samples and therefore we choose relatively large \(\lambda\) values. Specifically, we choose \(\lambda=100\) for the second sparse sub-network and \(\lambda=1,000,000\) for the third sparse sub-network.

## Appendix C Proofs of Main Theoretical Results

In this section, we provide detailed proofs of the theoretical results presented in the main paper.

### Proof of Lemma 1

Proof.: For \(y_{n}=c\), with respect to data sample \(\{\mathbf{x}_{n},y_{n}\}\), the gradient can be evaluated as

\[-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{x}_{n},y_{n})=[1-\texttt{SOFT}_{c}(F( \mathbf{x}_{n}))]\sum_{p\in[P]}\texttt{ReLU}[\langle\Theta_{c,h},\mathbf{x}_{ n}^{p}\rangle]\mathbf{x}_{n}^{p}\] (12)

Assume that the given sample has a major feature \(\mathbf{v}_{c,l}\), taking dot product with respect to \(\mathbf{v}_{c,l}\) on both side of (12) leads

\[\langle-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{x}_{n},y_{n}),\mathbf{v}_{c,l} \rangle=[1-\texttt{SOFT}_{c}(F(\mathbf{x}_{n}))]\sum_{p\in[P]}\langle\texttt{ ReLU}[\langle\Theta_{c,h},\mathbf{x}_{n}^{p}\rangle]\mathbf{x}_{n}^{p},\mathbf{v}_{c,l}\rangle\] (13)

Let's further assume that the feature set is orthonormal: \(\forall c,c^{\prime},\forall l\in[L]\), \(||\mathbf{v}_{c,l}||_{2}=1\) and \(\mathbf{v}_{c,l}\perp\mathbf{v}_{c^{\prime},l^{\prime}}\) when \((c,l)\neq(c^{\prime},l^{\prime})\). Using \(\mathbf{x}^{p}=a^{p}\mathbf{v}_{c,l}+\sum_{\mathbf{v}^{\prime}\in\cup\mathbf{ v}_{c}}\alpha^{p,\mathbf{v}^{\prime}}\mathbf{v}^{\prime}+e^{p}\) given in (4), we have

\[\langle-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{x}_{n},y_{n}),\mathbf{v}_{c,l} \rangle=[1-\texttt{SOFT}_{c}(F(\mathbf{x}_{n}))]\left(\sum_{p\in\mathcal{P}_{ v,l}(\mathbf{x}_{n})}\texttt{ReLU}[\langle\Theta_{c,h},\mathbf{x}_{n}^{p} \rangle a^{p}]+\sum_{p\in[P]}\langle e^{p},\mathbf{v}_{c,l}\rangle\right)\] (14)

It should be noted that the term _i.e.,_\(\sum_{v^{\prime}\in\cup\setminus\mathbf{v}_{c}}\alpha^{p,v^{\prime}}\langle \mathbf{v}^{\prime},\mathbf{v}_{c,l}\rangle\) becomes zero due to the orthogonal properties of the feature set. Let us represent the second term by \(\kappa\): \(\sum_{p\in[P]}\langle e^{p},\mathbf{v}_{c,l}\rangle=\kappa\). Then, we have

\[\langle-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{x}_{n},y_{n}),\mathbf{v}_{c,l} \rangle=(1-\texttt{SOFT}_{c}(F(\mathbf{x}_{n})))\left(\sum_{p\in\mathcal{P}_{ v,l}(\mathbf{x}_{n})}\texttt{ReLU}[\langle\Theta_{c,h},\mathbf{x}_{n}^{p} \rangle a^{p}]+\kappa\right)\] (15)Furthermore, let us define \(V_{c,h,l}(\mathbf{x}_{j})=\sum_{p\in\mathcal{P}_{\mathbf{v}_{c,l}}(\mathbf{x}_{j})} \mathsf{ReLU}(\langle\Theta_{c,h},\mathbf{x}_{j}^{p}\rangle a^{p})\) then above equation further reduces to following

\[\langle-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{x}_{n},y_{n}),\mathbf{v}_{c,l} \rangle=(1-\mathsf{SOFT}_{c}(F(\mathbf{x}_{n})))(V_{c,h,l}(\mathbf{x}_{n})+\kappa)\] (16)

Recall the above equation is the gradient with respect to the \(n^{th}\) data sample. Considering the gradient with respect to all data samples with \(y_{n}=c\), and let us consider the total loss, where the weight \(z_{n}\) of each loss is assigned according to a distribution specified by the uncertainty set \(\mathcal{U}\). Then, the total gradient is

\[\langle-\nabla_{\Theta_{c,h}}l(\Theta;\mathbf{X},\mathbf{Y}),\mathbf{v}_{c,l} \rangle=\max_{\mathbf{z}\in\mathcal{U}}\sum_{n=1}^{N}z_{n}\left[\mathbb{1}_{ y_{j}=c}(V_{c,h,l}(\mathbf{x}_{n})+\kappa)(1-\mathsf{SOFT}_{c}(F(\mathbf{x}_{n})))\right]\] (17)

Now using the standard gradient update rule with \(\beta\) being the learning rate, we have

\[\langle\Theta_{c,h}^{t+1},\mathbf{v}_{c,l}\rangle=\langle\Theta_{c,h}^{t}, \mathbf{v}_{c,l}\rangle+\beta\max_{\mathbf{z}\in\mathcal{U}}\sum_{n=1}^{N}z_{ n}\left[\mathbb{1}_{y_{j}=c}(V_{c,h,l}(\mathbf{x}_{n})+\kappa)(1-\mathsf{SOFT}_{c}(F( \mathbf{x}_{n})))\right]\] (18)

Let \(\mathbf{x}_{k}\in\mathcal{D}_{N}^{S}\) be the most difficult sample having \(\mathbf{v}_{c,l}\) as the main feature. Also, consider \(\mathbf{x}_{n}\in\mathcal{D}_{N}^{M}\) to be the easy sample with \(y_{n}=c,y_{k}=c\). Then, we have

\[[1-\mathsf{SOFT}_{c}(F(\mathbf{x}_{k}))]\geq[(1-\mathsf{SOFT}_{c}(F(\mathbf{x }_{n}))],\ \forall n\in[1,N],n\neq k,y_{n}=c\] (19)

Using above property, we can write the following using (18)

\[\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l}\rangle+\beta\max_{\mathbf{z}\in \mathcal{U}}\sum_{n=1}^{N}z_{n}\left[\mathbb{1}_{y_{j}=c}(V_{c,h,l}(\mathbf{x }_{n})+\kappa)(1-\mathsf{SOFT}_{c}(F(\mathbf{x}_{n})))\right]\]

\[\leq\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l}\rangle+\beta Nz_{k}(1-\mathsf{ SOFT}_{c}(F(\mathbf{x}_{k})))\] (20)

On the r.h.s., we have \(z_{n}=\frac{1}{N}\) for ERM, which assigns equal weights to all samples. Under the assumption of \(N_{\mathbf{v}_{c,l}}\ll N_{\cup\mathbf{v}_{c,l}}\), the contribution of the \(N_{\mathbf{v}_{c,l}}\) on overall gradient will be negligible. In contrast, for the DRO framework, using (11), we have

\[z_{k}=\frac{1}{\sum_{j=1,j\neq k}^{N}\exp\left(\frac{l_{j}(\Theta)-l_{k}( \Theta)}{\lambda}\right)+1}\] (21)

Since \(l_{k}(\Theta)>l_{j}(\Theta),\forall\lambda>0,\lambda\neq\infty\), we have \(z_{k}>\frac{1}{N}\). Using r.h.s. of (20) and incorporating \(z_{k}=\frac{1}{N}\) for ERM and \(z_{k}>\frac{1}{N}\), we have

\[\{\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l}\rangle+\beta(1-\mathsf{SOFT}_{c}(F (\mathbf{x}_{k})))\}_{ERM}\leq\{\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l} \rangle+\beta(1-\mathsf{SOFT}_{c}(F(\mathbf{x}_{k})))\}_{Robust}\] (22)

This subsequently leads to the following:

\[\{\langle\Theta_{c,h}^{t},\mathbf{v}_{c,l}\rangle\}_{Robust}>\{\langle\Theta_{ c,h}^{t},\mathbf{v}_{c,l}\rangle\}_{ERM};\forall t>0\] (23)

which completes the proof of Lemma 1. 

### Proof of Theorem 2

Let \(\mathbf{x}\in\mathcal{D}_{S}^{N}\) from class \(\mathbf{c}\) with \(\mathbf{v}_{c,l}\) as the main feature and \(\mathbf{v}^{\prime}\) as the dominant feature learned through the memorization. Also consider \(\mathbf{v}^{\prime}\) to be the main feature characterizing class \(k\). Then for any class \(c^{\prime}\), we can define the following

\[\mathsf{SOFT}_{c^{\prime}}(\mathbf{x})=\frac{\exp(F_{c^{\prime}}(\mathbf{x}))}{ \sum_{j\in[C]}\exp(F_{j}(\mathbf{x}))}\] (24)

In the above equation, \(F_{c^{\prime}}(\mathbf{x})\) can be written as

\[F_{c^{\prime}}(\mathbf{x})=\sum_{h\in[H]}\sum_{p\in[P]}\mathsf{ReLU}[\langle \Theta_{c^{\prime},h},\mathbf{x}^{p}\rangle]\] (25)Substituting \(\mathbf{x}^{p}\) from (4), we have

\[F_{c^{\prime}}(\mathbf{x})=\sum_{h\in[H]}\sum_{p\in[P]}\mathtt{ReLU}\left[a^{p} \langle\Theta_{\sigma,h},\mathbf{v}_{c,l}\rangle+\sum_{\mathbf{v}^{\prime}\in \cup\setminus\mathbf{v}_{c}}\alpha^{p,\mathbf{v}^{\prime}}\langle\Theta_{\sigma,h},\mathbf{v}^{\prime}\rangle+\langle\Theta_{\sigma,h},\epsilon^{p}\rangle\right]\] (26)

Substituting \(c^{\prime}\) by \(k\), we have

\[F_{k}(\mathbf{x})=\sum_{h\in[H]}\sum_{p\in[P]}\mathtt{ReLU}\left[a^{p}\langle \Theta_{k,h},\mathbf{v}_{c,l}\rangle+\sum_{\mathbf{v}^{\prime}in\cup\setminus \mathbf{v}_{c}}\alpha^{p,\mathbf{v}^{\prime}}\langle\Theta_{k,h},\mathbf{v}^{ \prime}\rangle+\langle\Theta_{k,h},\epsilon^{p}\rangle\right]\] (27)

In case of ERM, the \(\mathbf{v}_{c,l}\) signal is fairly weak during the training process due to \(N_{\mathbf{v}_{c,l}}\ll N_{\cup\setminus\mathbf{v}_{c,l}}\). Therefore, the term \(\langle\Theta_{k,h},\mathbf{v}_{c,l}\rangle\) is negligible. Also, the last term \(\langle\Theta_{k,h},\epsilon^{p}\rangle\) is also small as this corresponds to the Gaussian noise. For the second term \(\exists\mathbf{v}^{\prime}\) for which \(\langle\Theta_{k,h},\mathbf{v}^{\prime}\rangle\) is very high because of the spurious correlation. In contrast, for the robust loss, using Lemma 1, the model learns a stronger correlation with the true class parameter and therefore \(\langle\Theta_{c,h},\mathbf{v}_{c,l}\rangle\) is high. As such, both terms \(\langle\Theta_{k,h},\mathbf{v}_{c,l}\rangle\) as well as \(\langle\Theta_{k,h},\mathbf{v}^{\prime}\rangle,\forall v\) becomes low. As a result, we have

\[\{F_{k}(\mathbf{x})\}_{ERM}>\{F_{k}(\mathbf{x})\}_{Robust}\] (28)

Substituting this inequality to (24), we have

\[\{\mathtt{SOFT}_{k}(\mathbf{x})\}_{Robust}<\{\mathtt{SOFT}_{k}(\mathbf{x})\}_ {ERM}\] (29)

This completes the proof of Theorem 2.

## Appendix D Experimental Details and Additional Results

In this section, we first provide a detailed description of datasets used in our experimentation followed by hardware description of our experimentation. We then provide examples of single-view and multi-view data samples. Next, we provide additional experimental results and baselines on Cifar10 and Cifar100 datasets. After that, we provide additional baseline results TinyImageNet. We also compare individual ensemble members' performance followed by our model performance with different calibration techniques commonly used in dense networks. Then, we perform an in-depth ablation study. Parameter size and inference speed are discussed in the subsequent subsection. We also further investigate the diversity of the sparse subnetworks. Finally, we provide detailed qualitative analysis to support our proposed claim.

### Detailed Dataset Description

For general classification setting, we consider Cifar10, Cifar100 [12], and TinyImageNet [14] datasets. For the out of distribution setting, we consider corrupted version of Cifar10 and Cifar100, which are named as Cifar10-C and Cifar100-C [10], respectively. Finally, for open-set detection, we leverage SVHN [21] as the open-set dataset. The detailed description of each dataset is given below:

* _Cifar10_ consists of total 10 classes, each consisting of 5,000 training samples and 1,000 testing (evaluation) samples. Each image is a colored image with size \(32\times 32\).
* _Cifar100_ consists of 20 super classes where each super-class consists of 5 classes resulting into total 100 classes. Each class consists of 500 training samples and 100 testing samples. Each image is a colored image with size \(32\times 32\).
* _TinyImageNet_ consists of 200 classes with 1,000,000 samples where each class has 500 training images, 50 validation images, and 50 test images. Each image is a colored image with size \(64\times 64\).
* _Cifar10-C_ consists of fifteen different types of corruptions applied on the Cifar10 clean testing dataset where each corruption has 5 severity levels, ranging from 1 to 5 with 1 being least severe and 5 being most severe. The corruptions include Gaussian noise, shot noise, impulse noise, defocus blur, forsted glass blur, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic, pixelate, and JPEG.
* _Cifar10-C_ consists of fifteen different corruptions applied on the Cifar100 clean testing dataset.
* _SVHN_ consists of 10 classes with digit 1 as class 1, digit 9 as class 9 and digit 0 as class 10. These are original, variable-resolution, colored house-number images with character level bounding boxes. We use this dataset as the open-set dataset in our experimentation.

### Hardware Details for Experimentation

All experimentations are conducted using NVIDIA RTX A6000 GPU with 48GB memory requiring 300 Watt power. For GPU, CUDA Version: 11.6, Driver Version: 510.108.03, and NVIDIA-SMI: 510.108.03 is used. In terms of CPU, our experimentation uses an Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz with a 64-bit system and an x86_64 architecture.

### Single-view and Multi-view Examples

Figure 5 show the three example images, where the first image is a representative single-view data sample whereas the last two are multi-view samples. In this example, we consider three major features for cars: _i.e.,_\(\mathtt{Tire}\), Headlight, and \(\mathtt{Door}\) handle. As only headlight feature is present in the first image, it belongs to the single-view category. For the second and third images, multiple features are presented and therefore we regard those images as multi-view data samples.

### Additional Result on Cifar10 and Cifar100

Table 6 shows the experimental result on Cifar10 and Cifar100 datasets with a \(15\%\) density. As shown, the proposed technique has a far superior performance in terms of the ECE score compared to the competitive baselines. This is consistent with the results with a 9% density as presented in the main paper, which further justifies the effectiveness of our proposed technique.

\begin{table}
\begin{tabular}{l l c c c c c c c} \hline \hline \multirow{2}{*}{Training Type} & \multirow{2}{*}{Approach} & \multicolumn{4}{c}{Cifar10} & \multicolumn{4}{c}{Cifar100} \\ \cline{3-10}  & & \multicolumn{2}{c}{ResNet50} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet152} \\ \cline{3-10}  & & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline \multirow{3}{*}{Dense Training} & Dense\({}^{\dagger}\) & 94.82 & 5.87 & 95.12 & 5.99 & 76.40 & 16.89 & 77.97 & 16.73 \\ \cline{2-10}  & L1 Pruning & 93.88 & 5.69 & 94.23 & 5.88 & 75.53 & 15.52 & 75.83 & 15.78 \\  & LTH & 92.97 & 4.03 & 93.15 & 5.69 & 74.36 & 15.13 & 74.77 & 15.22 \\  & DLTH & 95.15 & 6.21 & 95.65 & 6.96 & 77.98 & 16.24 & 78.23 & 16.54 \\  & Mixup & 93.22 & 4.02 & 93.38 & 5.68 & 74.48 & 15.10 & 74.68 & 15.16 \\ \hline \multirow{3}{*}{Sparse Training} & CigL & 92.25 & 4.67 & 93.34 & 4.59 & 77.88 & 10.16 & 77.27 & 10.62 \\  & DST Ensemble & 89.57 & 2.10 & 88.64 & 1.34 & 64.57 & 9.76 & 64.75 & 9.27 \\  & Sup-ticket & 94.65 & 3.20 & 94.95 & 3.09 & 78.68 & 10.16 & 78.95 & 10.32 \\ \hline \multirow{3}{*}{Mask Training} & AdaBoost & 94.07 & 5.65 & 94.76 & 5.14 & 75.98 & 23.55 & 76.28 & 24.27 \\  & EP & 94.41 & 3.90 & 94.42 & 4.07 & 75.66 & 14.79 & 76.05 & 14.79 \\ \cline{1-1}  & SNE & 94.85 & 3.05 & 94.96 & 3.18 & 76.82 & 11.12 & 77.23 & 11.63 \\ \cline{1-1}  & _DRE_ & 94.87 & **1.71** & 94.74 & **1.34** & 75.86 & **4.90** & 76.46 & **5.81** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Accuracy and ECE performance with \(15\%\) density for Cifar10 and Cifar100 dataset.

Figure 5: Examples of single-view and multi-view samples.

### Additional Baseline Results on Sparse Training Methods

Apart from the baselines included in the main paper, there are other sparse training methods without the need for iterative pruning/growing [15; 19; 33; 36]. However, as all these methods primarily focus on pushing the accuracy up to the original dense networks, they still suffer from a severely overfitting behavior, leading to a poor calibration performance as shown in Table 7.

### Additional Baseline Results on TinyImageNet

As mentioned in the main paper, the computational issue (_i.e.,_ memory overflow) makes it impossible to run sparse learning techniques _i.e.,_ CigL [16], DST Ensemble [18], and Sup-ticket [35] on the ResNet101 and WideResNet101 architectures to make a fair comparison. Therefore, in this section, we pick a lower capacity model (ResNet50) and compare the performance. Even for the ResNet50 architecture, CigL still runs into the memory overflow issue with a batch size of \(128\). Furthermore, lowering the batch size (_e.g.,_ 16) makes the training process extremely slow even using a \(48Gb\) GPU, where each training epoch takes more than half an hour, making model training extremely difficult. Therefore, we did not report the performance of CigL. It should be noted that CigL can be trained on Cifar10 and Cifar100 because of lower dimension of the input images and we have already reported its performance in the main paper. Table 8 shows the performance of DRE along with those from DST Ensemble and Sup-ticket on ResNet50. It is clear that DRE achieves better performance compared to these baselines.

### Performance from Ensemble Members

We investigate how performance varies in different sparse sub-networks. We use Cifar100 as an example and Table 9 report the individual sub-network performance on both accuracy and ECE. While each sparse sub-network is a relatively weaker learner (which is expected), they contribute to the final ensemble model in a complementary way, leading to a better ECE score as well as accuracy.

### Comparison with Common Calibration Techniques

In this section, we investigate whether existing calibration techniques designed for training dense networks can be leveraged to further improve the calibration performance of sparse networks. However, most of these techniques (_e.g.,_ temperature scaling and mix-n-match) are post hoc techniques, which require a separate validation set to fine-tune the parameters. This means we need to further divide the training data into training and validation sets, which may negatively impact the generalization capability of the trained model (due to less training data). To make a comparison, we pick Temperature Scaling (TS) [9], Label Smoothing (LS) [31], and a few other techniques proposed in

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Approach} & \multicolumn{4}{c}{Cifar10} & \multicolumn{4}{c}{Cifar100} \\ \cline{2-9}  & \multicolumn{2}{c}{\(9\%\)} & \multicolumn{2}{c}{\(15\%\)} & \multicolumn{2}{c}{\(9\%\)} & \multicolumn{2}{c}{\(15\%\)} \\ \cline{2-9}  & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline SNIP [15] & 93.45 & 4.10 & 94.12 & 3.86 & 52.99 & 10.97 & 54.40 & 10.56 \\ Dynamic Sparse [19] & 91.11 & 5.73 & 92.09 & 5.33 & 70.06 & 15.56 & 71.23 & 14.29 \\ GraSP [33] & 92.73 & 4.78 & 93.16 & 4.54 & 72.60 & 17.07 & 73.07 & 15.95 \\ MEST [36] & 92.94 & 4.67 & 93.50 & 4.40 & 72.19 & 16.47 & 73.49 & 15.81 \\ EP & 94.20 & 3.97 & 94.41 & 3.90 & 75.07 & 14.62 & 75.66 & 14.79 \\ \hline \hline
**DRE** & 94.60 & **0.7** & 94.87 & **1.71** & 74.68 & **1.20** & 75.86 & **4.90** \\ \hline \hline \end{tabular}
\end{table}
Table 7: ECE and Accuracy for other sparse baselines.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{Subnetworks} & ResNet101 & ResNet152 \\ \cline{2-5}  & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline Subnetwork \(I\) (3\%) & 68.22 & 14.35 & 69.65 & 13.31 \\ \hline Subnetwork \(Z\) (3\%) & 69.03 & 1.39 & 70.00 & 3.39 \\ \hline Subnetwork \(3\) (3\%) & 72.86 & 11.96 & 70.24 & 14.78 \\ DRE & 74.68 & **1.20** & 74.37 & 2.09 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Additional baseline results on TinyImageNet using ResNet50 with \(\mathcal{K}=15\%\).

[37], including Ensemble Temperature Scaling (ETS) and Isotonoic Regression One vs All combined with Temperature Scaling (IROvA-TS). We apply these calibration techniques on the top of the EP algorithm. Specifically, as LS does not require a separate validation set, we train it on the full training dataset using the LS loss (with \(\epsilon=0.1\)). Other calibration techniques require a separate validation set and therefore we divide training data into training and validation with a 80:20 ratio. EP (No Validation) uses the full training dataset whereas EP (Validation) is trained using \(80\%\) of the training data. Once the model is trained with \(80\%\) of training data using EP, we further calibrate it using the aforementioned calibration techniques. Table 10 shows the results. There are two key observations: (i) the classification accuracy decreases for all calibration techniques at the expense of improving calibration performance as they require a separate validation set, and (ii) DRE achieves the best ECE in all cases, which further justifies its strong calibration performance.

### Ablation Study

In this section, we first show the impact of \(\lambda\) values on the prediction and calibration performance. We then compare the performance of proposed DRE with respect to dense ensemble techniques. Finally, we show how proposed DRE avoids learning from noisy features by considering the dataset containing explicit spurious features.

**Impact of the uncertainty set size.** For simplicity, we always keep one sparse sub-network in our framework to be with \(\lambda_{1}\rightarrow\infty\). The ECE performance with respect to different sets of \(\lambda\) value for the remaining sub-networks is shown using the heatmap given in Figure 6 (a-b). As can be seen, it is important to choose \(\lambda_{2}\) and \(\lambda_{3}\) with very distinct values to achieve a low calibration error.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Approach} & \multicolumn{4}{c}{Cifar10} & \multicolumn{4}{c}{Cifar100} \\ \cline{2-9}  & \multicolumn{2}{c}{ResNet50} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet152} \\ \cline{2-9}  & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) \\ \hline \hline
**TS** & 93.42 & 0.96 & 93.42 & 1.37 & 73.06 & 1.72 & 73.40 & 2.45 \\ ETS & 93.42 & 0.97 & 93.42 & 1.37 & 73.06 & 1.76 & 73.40 & 2.40 \\ IROvA-TS & 89.90 & 1.45 & 88.69 & 0.89 & 60.87 & 1.56 & 60.77 & 2.86 \\ LS & 94.06 & 7.56 & 94.21 & 7.41 & 75.96 & 9.36 & 76.40 & 7.71 \\ \hline \hline EP (No Validation) & 94.20 & 3.97 & 94.35 & 4.03 & 75.05 & 14.62 & 75.68 & 14.41 \\ EP (Validation) & 93.42 & 4.46 & 93.42 & 4.83 & 73.06 & 15.56 & 73.40 & 15.88 \\ \hline \hline
**DRE** & 94.60 & **0.7** & 94.28 & **0.7** & 74.68 & **1.20** & 74.37 & **2.09** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Different calibration techniques on the top of EP Algorithm with \(\mathcal{K}=9\%\).

Figure 6: (a-b) Impact of \(\lambda\) on ECE using ResNet101 architecture on Cifar100 dataset.

[MISSING_PAGE_FAIL:22]

of non-spurious only, DRE achieves better performance both in terms of accuracy and ECE, which justifies that our model indeed learns from important features instead of spurious correlations. In the original test set, because of the large number of samples holding spurious correlations, we do not see a clear advantage in terms of accuracy. However, DRE still achieves a clearly better calibration performance compared to SNE.

We have further visualized the the heatmap of convolution 4 layer using the Grad-Cam technique. As shown in the Figure 7 (a), the sparse sub-network in the SNE focuses on the water background instead of focusing on the actual landbird object. This is because, during training process, the sparse sub-network in SNE is likely to learn to associate the spurious background feature with the true label. Specifically, the model learns to predict landbird whenever there is a land background and waterbird whenever there is a water background. In contrast, using the DRE technique, as demonstrated through the heatmap, the sparse sub-network focuses on the actual object instead of the background. It is worth mentioning that, because of the overfitting phenomenon and lack of a systematic way for diversification, each sparse sub-network in the SNE behaves in a similar way by focusing on the spurious feature instead of the actual object. In contrast, in the case of DRE, each sparse sub-network is controlled by the \(\eta\) parameter in Eq. (2). Specifically, we use a low \(\eta\) value (_i.e._, \(\eta\to 0\)) for one of the sparse sub-networks, which will be similar to that of the sparse sub-network obtained using the SNE. However, for the higher \(\eta\) value, it will focus on learning from more difficult samples, including those not holding the spurious correlations. As such, the sparse sub-network is forced to learn from the actual object instead of through the background. Therefore, the model focuses mostly on actual objects as demonstrated in Figure 7 (b). When these diverse sparse sub-networks are combined in the DRE, it achieves a better calibration without being confidently wrong like in the SNE.

### Parameter Size and Inference Speed

We compare parameter size and inference speed of different types of sparse networks. Table 14 shows the FLOPS along with number of parameters associated with each technique. As can be seen, the proposed DRE has a comparable parameter size as that of the sparse network ensemble. In terms of computational times, our approach is comparable to the sparse network ensemble. Compared to a dense network, our technique has a much smaller parameter size with less FLOPS.

### Diversity on Sparse Sub-networks

To justify our claim that our technique ensures the diverse sparse sub-networks, we adapt the disagreement metric (\(d_{dist}\)) from [18]. This metric measures the disagreement among sub-networks in terms of class label prediction. Table 15 below shows the results for Cifar10 and Cifar100 datasets. As shown, compared to Sparse Network Ensemble, DRE achieves higher disagreement which implies that the sparse sub-networks are more diverse.

### Qualitative Analysis

As demonstrated in the qualitative analysis section of the main paper, our approach is able to generate the less confident prediction in the case of incorrectly samples and thereby making model better calibrated. Figure 8 (a)-(d) show the confidence values of the wrongly classified samples from

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Approach & \multicolumn{2}{c}{ResNet50} & \multicolumn{2}{c}{ResNet101} \\ \cline{2-5}  & Params & Flops (\(\times 10^{9}\)) & Params & Flops (\(\times 10^{9}\)) \\ \hline Dense\({}^{\dagger}\) & 23.6M & 4.14 & 42.5M & 7.88 \\ \hline SNE & 3.5M & 1.31 & 6.3M & 2.53 \\ DRE & 3.5M & 1.31 & 6.3M & 2.53 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Parameter size and inference speed.

Figure 7: Visualization of the layer 4 convolution using the Grad-Cam technique [29] on a landbird data sample: (a) SNE focuses on the water background, (b) DRE focuses on the actual object.

the baseline models, which are concentrated on the higher end as compared with DRE. We further show that DRE behavior in the correctly classified samples in Figure 8 (e)-(l). As can be seen, the confidence score of correctly classified data samples from the CIFAR100 dataset with different techniques. As shown, our DRE technique remains confident on the correct data samples while being not confident on the incorrect data samples. This result shows our approach is well calibrated and trustworthy compared with the competitive baselines. In summary, our proposed technique remains uncertain for incorrect samples while being confident on the correct samples resulting in a much improved calibration.

## Appendix E Broader Impact, Limitations, and Future Work

In this section, we first describe the potential broader impacts of our work. We then discuss the limitations and identify some possible future directions.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Approach} & \multicolumn{6}{c}{Cifar10} & \multicolumn{6}{c}{Cifar100} \\ \cline{2-11}  & \multicolumn{2}{c}{ResNet50} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet101} & \multicolumn{2}{c}{ResNet152} \\ \cline{2-11}  & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(d_{dist}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(d_{dist}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(d_{dist}\) & \(\mathcal{ACC}\) & \(\mathcal{ECE}\) & \(d_{dist}\) \\ \hline SNE & 94.85 & 3.05 & 0.048 & 94.96 & 3.18 & 0.049 & 76.82 & 11.12 & 0.20 & 77.23 & 11.63 & 0.20 \\ \hline
**DRE (Ours)** & 94.87 & **1.71** & 0.088 & 94.74 & **1.34** & 0.069 & 75.86 & **4.90** & 0.24 & 76.46 & **5.81** & 0.24 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Accuracy, ECE, and prediction disagreement performance with a \(\mathcal{K}=15\%\) density.

Figure 8: Confidence scores of correctly classified samples in baseline models (a)-(d); confidence scores of correctly classified samples in CIFAR100 with ResNet101: (e)-(l)

### Broader Impact

Sparse network training provides a highly promising way to significantly reduce the computational cost for training large-scale deep neural networks without sacrificing their predictive power. Besides energy savings, it also opens the gate for deploying deep neural networks to lightweight computing or edge devices that can further broaden the applications of AI in more diverse and resource constrained settings. The proposed robust ensemble framework provides a general solution to achieve calibrated training of deep learning models. As a result, the trained model is expected to provide more reliable uncertainty predictions, which could be an important step towards using AI in safety-critical domains.

### Limitations and Future Works

As an ensemble model, DRE involves multiple base learners (_i.e.,_ sparse sub-networks). Consequently, it may lead to more computational overhead. This could create issues for real-time application as during the inference time, the input needs to be passed through all base learners to get the final output, which can slow down the prediction speed. A straightforward way to speed up the inference process is to execute all the base learners in parallel, which still incurs additional computational overhead. One interesting future direction is to investigate knowledge distillation and train a single sparse network from the ensemble model. Theoretical evidence [1] shows that knowledge distillation has the potential to largely maintain the ensemble performance while providing a promising way to train a single sparse network with an even higher sparsity level and improved inference speed.

## Appendix F Source Code

For the source code of this paper, please click here.