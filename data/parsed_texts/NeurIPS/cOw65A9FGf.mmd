# Text-Guided Attention is All You Need for

Zero-Shot Robustness in Vision-Language Models

 Lu Yu

1School of Computer Science and Engineering, Tianjin University of Technology

Haiyang Zhang

1School of Computer Science and Engineering, Tianjin University of Technology

Changsheng Xu

2State Key Laboratory of Multimodal Artificial Intelligence Systems,

Institute of Automation, University of Chinese Academy of Sciences

{luyu@email, zshy@stud}.tjut.edu.cn, csxu@nlpr.ia.ac.cn

###### Abstract

Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: _Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)_. This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model's robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. _Our code is available at https://github.com/zhyblue424/TGA-ZSR_.

## 1 Introduction

Large-scale pre-trained vision-language models (VLMs) have showcased remarkable success in artificial intelligence by seamlessly integrating visual and textual data to understand complex multimodal information, such as CLIP [48]. Leveraging vast datasets and powerful architectures such as BERT [10] and its variants [8, 33], these models adeptly capture semantic relationships between images and texts, offering significant advantages across numerous applications. From image classification [14, 67, 55] and semantic segmentation [50] to image captioning [39] and vision question answering [44], pre-trained VLMs revolutionize how machines perceive and interact with multimodal information. Their importance lies in their ability to learn rich representations from varied data streams, enabling zero-shot learning and transfer learning across domains and tasks. Thus ensuring the reliability of large-scale models is crucial. However, these models are vulnerable to adversarial attacks as many other networks as demonstrated by recent studies [38, 59], even slight perturbations to input data can result in misclassification or altered outputs. Such attacks pose a significant challenge, particularly in critical applications like autonomous vehicles [60], medical diagnosis [32], and maritime navigation [29], where the consequences of erroneous decisions can be severe. As these large-scale models become increasingly prevalent in real-world applications, understanding andmitigating the risks posed by adversarial attacks is essential to maintain trust and reliability in AI systems.

Adversarial training [53; 61; 69] has emerged as a crucial technique in enhancing the robustness of deep learning models against adversarial attacks. By augmenting training data with adversarial examples generated through perturbations of input data, models are forced to learn more robust decision boundaries, thereby improving their resilience to adversarial manipulation. Given the rising significance of large-scale VLMs in various applications, understanding their vulnerability to adversarial attacks is essential. While adversarial training presents practical challenges when applied to downstream tasks, especially with large-scale models. Firstly, adversarial training typically involves generating adversarial examples during each training iteration, which increases the computational overhead and may lead to overfitting on the training data. This phenomenon is exacerbated in large-scale models with vast parameter spaces, where fine-tuning becomes more susceptible to overfitting. Moreover, adversarial training may not adequately prepare models for all possible adversarial scenarios, potentially leaving them vulnerable to unknown data distributions encountered in real-world settings. Exploring zero-shot adversarial robustness in these models is particularly pertinent as it sheds light on their ability to generalize and perform reliably in unseen scenarios. Additionally, considering the multimodal nature of VLMs, the exploration of zero-shot adversarial robustness offers insights into the complex interactions between visual and textual modalities, paving the way for more robust and trustworthy multimodal AI systems.

Text-guided Contrastive Adversarial Training (TeCoA) method [38] represents the pioneering effort in investigating the zero-shot adversarial robustness of large-scale VLMs. They aim to bolster CLIP's zero-shot generalization capacity against adversarial inputs. While their primary focus lies on enhancing accuracy in the face of adversarial samples, this improvement comes at the expense of decreased performance on clean data. Subsequent work by PMG-AFT [59] builds upon this by introducing a pre-trained model guided adversarial fine-tuning technique, further enhancing both generalizability and adversarial robustness. However, despite the advancements made by both studies in enhancing CLIP's zero-shot robustness, significant questions regarding the interpretability of adversarial attacks and the efficacy of adversarial training remain unanswered. Specifically, _the mechanisms through which adversarial attacks influence network outputs and the reasons behind the effectiveness of adversarial training strategies remain elusive_. In our paper, we delve into the text-guided attention shift phenomenon to shed light on how adversarial attacks alter model outputs. Leveraging these insights, we propose a simple yet effective strategy, TGA-ZSR, aimed at enhancing the robustness of the CLIP model and preserving its performance on clean examples.

Our main contributions are summarized follows:

* To our knowledge, we are the first to introduce text-guided attention to enhance zero-shot robustness on vision-language models while maintaining performance on clean sample.
* We improve the interpretability of adversarial attacks for zero-shot robustness on vision-language models through a text-guided attention mechanism.
* The experimental results show that TGA-ZSR surpasses previous state-of-the-art methods, establishing a new benchmark in model zero-shot robust accuracy.

## 2 Related Work

**Pre-trained Vision-language Models.** In recent years, advancements in computer vision[12; 17; 34] have primarily relied on training models with image-label pairs to recognize predefined object categories. However, these approaches often overlook the inherent semantic connections between textual descriptions and visual content. Motivated by the remarkable progress witnessed in natural language processing (NLP), exemplified by breakthroughs like Transformer [56], BERT [10], and GPT-3 [3], researchers are increasingly drawn to the prospect of using textual data to enhance the capabilities of DNNs. These methodologies are referred to as VLMs [21; 48; 49; 64] and one prominent approach is to directly learn the semantic similarity between images and corresponding textual descriptions through image-text pairs. By aligning the embeddings of these two modalities, models like CLIP [48], ALIGN [21], BLIP [25], Visual-BERT [47], and ALBEF [26] aim to achieve superior performance across various tasks. CLIP [48] leverages a vast dataset of 400 million image-text pairs sourced from the internet and employs contrastive loss to effectively align the embeddings of both modalities, thereby enhancing the model's capabilities. Experimental results underscore the significant performance gains achieved by incorporating textual information into the model, with zero-shot performance surpassing that of earlier deep neural network architectures. However, despite its impressive zero-shot accuracy, experiments [38; 59] reveal vulnerabilities to adversarial examples, resulting in a notable decline in robustness.

**Adversarial Robustness.** Deep neural networks have been found to be vulnerable to adversarial examples [54; 36; 40; 66], which can fool DNNs to produce false outputs, rendering trained models unreliable. To bolster robustness against such adversarial attacks, various advanced methods have been proposed, including data augmentation [28; 58; 27; 65], adversarial training [69; 53; 61; 68], progressive self-distillation [1], randomization strategy [11; 35], and adversarial purification [41; 24; 62]. While these strategies aim to improve DNNs' adversarial robustness, they often come with increased complexity or limited generalizability. Adversarial training [69; 53; 61; 68] stands out as one of the most widely used and effective approaches, fine-tuning DNNs by generating adversarial examples during training. After the emergence of CLIP [48], many subsequent works [45; 16; 63] have utilized CLIP as a backbone, yet little attention has been given to studying its adversarial robustness. CLIP is shown to be susceptible to adversarial examples [38] as well, posing a significant threat to downstream tasks utilizing CLIP as a backbone. Hence, investigating the adversarial robustness of CLIP is crucial.

**Zero-shot Adversarial Robustness for VLMs.** The visual-language model, trained on both image and text data, serves as a foundational model for various tasks. However, it has shown vulnerability to adversarial examples [38; 59], and training from scratch is time-intensive. TeCoA [38] was the first to explore zero-shot adversarial robustness for VLMs, aiming to enhance CLIP's adversarial robustness by minimizing the cross-entropy loss between image logits and targets. While TeCoA solely utilizes cross-entropy loss, yielding only marginal performance improvements, PMG-AFT [59] extends this approach by minimizing the distance between features of adversarial examples and those of the pre-trained model. FARE [51] primarily focuses on maintaining high clean accuracy while improving model robustness, achieving this by constraining the distance between the original and target model embeddings. Our experiments reveal significant differences in attention maps between original examples and adversarial examples. Leveraging this insight, we enhance model robustness by constraining it with text-guided attention.

## 3 Methodology

### Preliminaries and Problem Setup

Following the previous works [38; 59], we choose CLIP model as the pre-trained VLMs for image classification task. Given an image-text pair \((x,t)\), where \(x\) represents an image and \(t\) represents a textual prompt, CLIP learns to encode both the image and the text into fixed-dimensional embeddings. Let \(f(x)\) denote the embedding of the image \(x\) and \(g(t)\) denote the embedding of the text prompt \(t\), \(y\) is the one-hot vector label. For training or fine-tuning on the downstream tasks, we use the cross-entropy loss, denoted as \(L(x,t,y)\).

\[L(x,t,y)=-\mathbb{E}_{i,j}\bigg{[}y_{ij}log\frac{exp(cos(f(x)_{i},g(t)_{j}))/ \tau)}{\sum_{k}exp(cos(f(x)_{i},g(t)_{k}))/\tau)}\bigg{]}\] (1)

where we set \(y_{ij}=1\) if the image-text pair is positive, otherwise, \(y_{ij}=0\). \(\tau\) is the temperature parameter and \(cos\) indicates calculating the cosine similarity of the two embeddings.

**Adversarial Attacks.** Adversarial attacks are a concerning phenomenon where small, often imperceptible perturbations are intentionally applied to input data with the aim of deceiving a model into producing incorrect outputs. These perturbations are crafted with the goal of causing the model to misclassify or generate erroneous predictions while appearing indistinguishable to human observers. The Projected Gradient Descent (PGD) [36] method is an iterative approach for crafting adversarial examples. It starts with the original input data and then iteratively adjusts the data in the direction that maximizes the model's loss function while ensuring the perturbed data remains within a specified perturbation budget. Mathematically, the PGD attack can be expressed as follows:

\[x_{a+1}=\Pi_{x+S}(x_{a}+\varepsilon\cdot sign(\bigtriangledown_{x_{a}}L(x_{a}, t,y)))\] (2)

Here, \(L\) represents the loss function, \(x\) denotes the original input data, \(\varepsilon\) controls the magnitude of perturbation, and \(\bigtriangledown_{x}L\) represents the gradient of the loss function with respect to the input data.

By adding or subtracting \(\varepsilon\) times the sign of this gradient to the original input data, the PGD attack generates adversarial examples that lead to misclassification or incorrect predictions by the model. \(\Pi_{x+S}\) makes the perturbed data remains within an \(\varepsilon\)-neighborhood of the original input, preventing the generated adversarial examples from straying too far. \(S\) is a set of allowed perturbations that formalizes the manipulative power of the adversary.

**Adversarial Examples Generation and Adversarial Training.** The optimization objective for crafting adversarial examples aims to maximize the loss of model \(f_{\theta}\) with respect to a perturbed input \(x_{a}\) which can be formulated as:

\[x_{a}=\underset{x_{a}}{argmax}\,L(f_{\theta}(x_{a},t,y))\] (3)

Adversarial training is a technique to generate adversarial examples from the original training data and then use these examples to train the model, forcing it to learn to resist adversarial perturbations. To adapt the model to the downstream tasks, we apply adversarial fine-tuning on one target model towards robustness with the following loss:

\[\theta=\underset{\theta}{argmin}\,\mathcal{J}(f_{\theta}(x_{a},t,y))\] (4)

Where \(\mathcal{J}\) represents the total loss function used for training the model.

**Zero-Shot Adversarial Robustness.** In this paper, we investigate the zero-shot adversarial robustness of CLIP model, which refers to the ability of these models to maintain performance and reliability even when encountering unseen adversarial samples during inference, with only adversarial fine-tuning the original CLIP model on one target dataset, such as Tiny-ImageNet.

### Text-Guided Attention based Interpretation of Adversarial Attacks

**Text-Guided Attention.** Attention mechanisms [30, 16, 31] play a crucial role in enhancing vision model performance across various tasks. At its core, attention enables models to focus on relevant parts of the input data while suppressing irrelevant information. Similarly, in VLMs, by incorporating textual guidance, the models can effectively focus on relevant visual features while processing

Figure 1: The four rows depict the original image, its associated attention map, the generated adversarial example, and the attention map of the adversarial example. Labels in black indicate the ground truth, while those in red represent mis-classified labels for the adversarial examples.

language, thus facilitating more accurate and coherent multimodal understanding. Additionally, text-guided attention enhances interpretability by providing insights into the model's decision-making process, fostering trust and understanding in complex multimodal systems. Thus, we investigate the impact of text-guided attention on enhancing and interpreting zero-shot adversarial robustness in VLMs in this paper. We define the text-guided attention as following:

\[A(x)=f_{g}(x)\cdot g(t)^{\mathsf{T}},\quad A\in\mathbb{R}^{P\times 1}\] (5)

Where \(f_{g}(x)\) represents the global image feature before the pooling operation of \(f(x)\), and \(P\) denotes the dimension of the attention embeddings. We reshape \(A\) to \(\mathbb{R}^{\sqrt{P}\times\sqrt{P}}\) to obtain the attention map, which is then resized to \(A\in\mathbb{R}^{H\times W}\). Finally, we apply a normalization operation (\(norm\)) on \(A\) to obtain the final text-guided attention map.

**Interpretation of Adversarial Attacks.** The previous research has predominantly focused on bolstering the zero-shot robustness of Vision-Language Models (VLMs), yet the reasons leading to mis-classifications induced by adversarial attacks remain unclear. This paper aims to shed light on interpreting the impact of adversarial attacks on VLMs. By employing Eq. 5, we compute the text-guided attention for both the original image (_Ori. image_) and its corresponding adversarial counterpart (_Adv. image_), as depicted in Fig. 1. Remarkably, despite the subtle discrepancies imperceptible to the human eye between the adversarial example and the original image, the former is mis-classified (labels in red). However, a significant difference emerges in the respective text-guided attention maps. Specifically, we observe a notable shift in the text-guided attention of the adversarial example, characterized by instances of displacement towards other objects, backgrounds, or even disappearance. For instance, while the original images in the first, second, and fourth columns pay attention to their subjects' heads, in their adversarial counterparts, attention diverges elsewhere. In the third column, the attention shift leads from the correct object to an incorrect one, resulting in mis-classification. In the fifth and seventh columns, the attention in their adversarial counterparts is redirected towards the background.

Figure 2: An overview of our TGA-ZSR framework: We generate adversarial examples and feed them into the target image encoder. To enhance the adversarial robustness of the CLIP model and maintain its generalization, we introduce text-guided attention. This involves refining the framework for adversarial examples through the Attention Refinement module and constraining the model to prevent significant drift via the Attention-based Model Constraint module.

### Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)

The semantic information embedded within text representations are preserved through a frozen text encoder, offering invaluable guidance when adversarial perturbations disrupt relevant visual features, which has not been explored for zero-shot robustness of vision-language models. We introduce the Attention Refinement Module, designed to effectively filter out irrelevant information, thereby mitigating the impact of adversarial attacks seeking to exploit vulnerabilities in the model's decision-making process. Moreover, to maintain model's ability to generalize effectively on clean images, we introduce the Attention-based Model Constraint Module. This module ensures consistent performance on clean data while enhancing the model against adversarial disruptions. Additionally, employing text-guided attention enhances interpretability, offering crucial insights into how the model integrates and processes information across modalities. This interpretability not only instills trust in the model's predictions but also facilitates the detection and mitigation of adversarial attacks. Our approach (i.e. TGA-ZSR) presents a comprehensive framework (as shown in Fig. 2) for enhancing model robustness to adversarial perturbations while concurrently improving interpretability. We will introduce the details as follows.

**Attention Refinement Module.** Based on the insights gained in Section 3.2, we propose an attention refinement module aimed at enhancing the robustness of the model. This module is designed to rectify the text-guided attention of adversarial samples, which often leads to altered predictions. Our approach aligns the adversarial attention map with that of the clean samples, known for their high-accuracy attention distribution. This simple yet effective strategy serves to mitigate the impact of adversarial perturbations on the model's predictions.

We take the generated adversarial sample \(x_{a}\) to the target model \(f_{g}^{tar}(\cdot)\) and the clean sample \(x\) to the original model \(f_{g}^{ori}(\cdot)\) and obtain the adversarial attention map \(A(x_{a}^{i})_{tar}\) and the clean attention map \(A(x^{i})_{ori}\) respectively. The attention refinement loss \(L_{AR}\) is thus defined as:

\[L_{AR}=\frac{1}{N}\cdot\sum_{i=0}^{N}\|A(x_{a}^{i})_{tar}-A(x^{i})_{ori}\|_{2}\] (6)

where \(A(x_{a})_{tar}=f_{g}^{tar}(x_{a})\cdot g(t)^{\mathsf{T}}\) and \(A(x)_{ori}=f_{g}^{ori}(x)\cdot g(t)^{\mathsf{T}}\)1, \(\|\|_{2}\) denotes the \(L_{2}\) distance computation between two attention maps.

Footnote 1: We only compute the attention map for the image corresponding to the text prompt of the ground-truth label.

**Attention-based Model Constraint Module.** The Attention Refinement module serves to enhance the robustness of the models, consequently improving the accuracy of adversarial samples. However, this enhancement comes with a trade-off: it may marginally sacrifice the accuracy on clean samples due to shifts in model parameters. To preserve the generalization capability of pre-trained VLMs, we introduce an Attention-based Model Constraint module. This module aims to mitigate performance drops on clean images, thereby ensuring the overall effectiveness and reliability of the model.

Specifically, we input the clean sample \(x\) into the target model \(f_{g}^{tar}(\cdot)\), adversarially fine-tuned on the Tiny-ImageNet dataset, to acquire the text-guided attention map \(A(x)_{tar}\). Concurrently, the original text-guided attention map outputted from the original CLIP model \(f_{g}^{ori}(\cdot)\) is denoted as \(A(x)_{ori}\). To ensure the preservation of importance parameters for clean images, we enforce an \(L_{2}\) distance constraint between these two attention maps. The attention-based model constraint loss \(L_{AMC}\) is formulated as:

\[L_{AMC}=\frac{1}{N}\cdot\sum_{i=0}^{N}\left\|A(x^{i})_{tar}-A(x^{i})_{ori} \right\|_{2}\] (7)

Thus the final loss function can be represented as:

\[L_{total}=L_{CE}+\alpha\cdot L_{AR}+\beta\cdot L_{AMC}\] (8)

## 4 Experiments

### Experimental Setup

**Datasets.** Our experiments begin with training the pre-trained CLIP model on the Tiny-ImageNet [9]. Then we evaluate the model's zero-shot adversarial robustness across 15 subsequent datasets, followed by previous studies, such as TeCoA [38] and PMG-AFT [59]. These datasets include several commonly used classfication datasets, including CIFAR-10 [23], CIFAR-100 [23], STL-10 [6], ImageNet [9], Caltech-101 [13], and Caltech-256 [15]. Additionally, fine-grained image classification datasets such as StanfordCars [22], Flowers102 [42], Food101 [2], FGVC Aircraft [37], and Oxford-Pets [46] are included. Furthermore, the scene recognition dataset SUN397 [43], the medical image dataset PCAM [57], and the satellite image classification dataset EuroSAT [18] and the texture recognition dataset DTD [5] are incorporated for comprehensive evaluation. We also conduct experiments on four additional datasets (i.e. ImageNet_subset, ImageNet-A, ImageNet-O and ImageNet-R) as shown in Supp. Mat. A.1.

**Implementation Details.** Following the protocol of previous works [59], we fine-tuned the CLIP model on the adversarial samples of Tiny-ImageNet [9] as _'adversarial fine-tuning'_ and subsequently evaluated its performance across 15 datasets and Tiny-ImageNet itself. We employ ViT-B/32 as the backbone in CLIP and utilize the SGD optimizer to minimize loss. During adversarial fine-tuning, we update all parameters of the image encoder with a learning rate of 1e-4, weight decay of 0, momentum of 0.9, and a batch size of 128. We utilize \(l_{\infty}\) norm PGD-2 [36] with 2 iterations to generate adversarial examples, with an attack strength \(\varepsilon\) of 1/255 and the attack step size is 1/255. To evaluate zero-shot adversarial inference, we employ \(l_{\infty}\) norm PGD-100 [36] with 100 iterations, attack step of 1/255 and a batch size of 256 to generate adversarial examples for verifying CLIP's adversarial robustness. Additionally, to assess the model's robustness under different attack strengths, we perform inference using adversarial strengths \(\varepsilon\) of 1/255, 2/255, and 4/255. The hyper-parameters \(\alpha\) and \(\beta\) are set to 0.08 and 0.05 respectively in Eq. 8 in the main experiments. Maintain the same parameters for the CW attack. For the AutoAttack [7] experiments, \(\alpha\) and \(\beta\) are set to 0.08 and 0.009. We conducted the experiment utilizing the RTX 3090, which required a training period ranging from 3 to 4 hours.

### Main Results

To validate the effectiveness of our approach, we conduct comparisons with several state-of-the-art methods such as TeCoA [38], PMG-AFT [59], and FARE [51]. Additionally, we extend the comparison to include CLIP (the original pre-trained CLIP model), FT-Adv. (adversarial fine-tuning using the contrastive loss of the original CLIP) and FT-Clean (fine-tuning on clean examples with the contrastive loss of the original CLIP) for a comprehensive evaluation.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c} \hline \hline
**Methods** & **\begin{tabular}{c} **\#** \\ \end{tabular} & **\begin{tabular}{c} **\#** \\ \end{tabular}** & **
\begin{tabular}{c

**Adversarial Zero-shot Robust Accuracy.** Table 1 shows that the average accuracy of our TGA-ZSR outperforms the original CLIP model by 37.19%. Compared to current stat-of-the-art method, PMG-AFT, the proposed method achieve an average improvement of 9.58%. In general, our method is superior than all the other methods on most datasets except a comparable result on PCAM dataset. In addition, we obtain the best result on Tiny-ImageNet, which is not a strict zero-shot test. It indicates that our method is robust on the adversarial attack on both seen and unseen datasets.

**Zero-shot Clean Accuracy.** Table 2 illustrates the model's accuracy for clean examples using different methods. Our method outperforms PMG-AFT by 9.84% and FT-clean by 2.07% in terms of average accuracy. Similar to Table 1, zero-shot clean accuracy exhibits improvement not only on an individual dataset but across all datasets. However, we observed that our zero-shot clean accuracy is 3.41% lower than that achieved by FARE. It is important to note that FARE prioritizes preserving zero-shot clean accuracy. However, we have significantly enhanced the zero-shot robust accuracy in adversarial scenarios with 23.84% gain compared to FARE in Table 1.

### Experiments on More Attack Types

**Results against AutoAttack.** AutoAttack [7] stands out as a strong attack method for assessing model robustness. We follow TeCoA and PMG-AFT to verify the perturbation bound \(\varepsilon\) of 1/255 in the standard version of AutoAttack. The results are summarized in Table 3. We can see that the original CLIP model experienced a significant performance decline, decreasing to 0.09% on the adversarial example. Our TGA-ZSR also demonstrates a decline but still achieves superior results compared to other methods, validating its effectiveness against stronger attacks.

**Results against CW Attack.** CW attack [4] is an optimization-based approach designed to generate small perturbations to input data, causing the model to make incorrect predictions while keeping the perturbed input visually similar to the original. We further evaluate the robustness of our approach against this challenging attack, using a perturbation bound of \(\varepsilon=1/255\). The results, shown in Table 4, demonstrate that our method significantly outperforms the state-of-the-art method PMG-AFT on both adversarial and clean samples. This substantial margin indicates the adversarial robustness of our proposed method.

### Ablation Study

**Different Types of Attentions.** To validate the important role of text-guided attention in our method, we conducted experiments by replacing it with vision-based attention. We employ Grad-CAM [52], a widely adopted method, for generating attention maps based on vision. Table 5 demonstrates that replacing the text-guided attention with vision-based attention yields results that are still comparable to

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} \\  & & & & & & & & & & & & & & & & & & & & & \\ \hline CLIP [18] & 0.21 & 0.36 & 0.10 & 0.59 & 1.16 & 0.82 & 1.23 & 1.09 & 2.18 & 0.01 & 0.00 & 1.14 & 13.50 & 7.36 & 2.36 & 0.07 & 3.64 \\ PMG-AFT [15] & 44.59 & 44.86 & 24.15 & 74.11 & 19.99 & 37.33 & 39.83 & 20.95 & 13.51 & 12.09 & 1.47 & 19.51 & 69.09 & 44.46 & 10.57 & 48.99 & 31.17 \\ TGA-ZSR(ours) & **63.85** & **60.90** & **34.62** & **84.11** & **22.03** & **33.28** & **58.33** & **32.95** & **21.22** & **13.99** & **4.56** & **20.42** & **70.34** & **59.73** & **20.20** & 48.02 & **40.50** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Zero-shot robust accuracy across 16 datasets with CW attack [4]. The optimal accuracy is highlighted in **bold**.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} & \multirow{2}{*}{**Average**} \\  & & & & & & & & & & & & & & & & & & & & & \\ \hline CLIP [18] & 0.02 & 0.01 & 0.08 & 0.03 & 0.04 & 0.01 & 0.00 & 0.03 & 0.16 & 0.12 & 0.06 & 0.04 & 0.03 & 0.10 & 0.11 & 0.22 & 0.09 \\ FT-clean & 0.08 & 0.03 & 0.01 & 0.91 & 0.09 & 0.04 & 0.06 & 0.03 & 0.48 & 0.02 & 0.03 & 0.12 & 1.38 & 0.66 & 0.03 & 0.02 & 0.25 \\ FT-Adv. & **59.08** & 37.55 & 20.39 & 69.14 & 16.25 & 11.23 & 33.91 & 18.54 & **19.95** & 11.59 & 12.65 & 16.21 & 69.90 & 39.24 & 7.57 & **48.84** & 28.28 \\ TeCoA [3] & 35.03 & 25.18 & 16.09 & 69.08 & 17.41 & 13.05 & 34.41 & 20.80 & 15.37 & 11.04 & 12.32 & 16.32 & 54.54 & 40.15 & 7.15 & 47.12 & 26.55 \\ FABE [51] & 28.59 & 23.37 & 13.58 & 67.00 & 97.23 & 18.38 & 27.15 & **15.48** & 9.15 & 0.25 & 0.87 & 12.67 & 47.43 & 36.68 & 6.77 & 10.23 & 19.78 \\ PMG-AFT [59] & 44.26 & **44.12** & **23.56** & **73.90** & 19.63 & **17.25** & 92.05 & 20.87 & 13.72 & **11.59** & 1.68 & 19.17 & **69.57** & 44.42 & 9.59 & 48.53 & 30.78 \\ TGA-ZSR(ours) & 39.45 & 40.53 & **22.38** & 72.06 & **20.36** & **25.55** & **40.31** & **21.45** & 17.233 & 11.19 & **1.94** & **19.28** & 52.16 & **45.68** & **19.07** & 48.63 & **30.86** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Zero-shot robust accuracy on images attacked with \(\varepsilon\) of 1/255 of AutoAttack [7]. We performed several different methods on Tiny-ImageNet and evaluated on 16 datasets.

[MISSING_PAGE_FAIL:9]

mately 15% compared to state-of-the-art method PMG-AFT. This is due to the additional computation required for the text-guided attention map. The training time for our method is comparable to that of PMG-AFT. The test time remains consistent across all methods.

## 5 Conclusion and Limitations

In this paper, we discovered that adversarial attacks lead shift of text-guided attention. Building on this observation, we introduce a text-guided approach, TGA-ZSR, which incorporates two key components to perform adversarial fine-tuning and constrain the model. This strategy prevents model drift while enhancing model robustness. Extensive experiments validate the performance of TGA-ZSR, which not only improves CLIP's zero-shot adversarial robustness but also maintains zero-shot clean accuracy on clean examples, gaining a favorable balance.

**Limitations.** We use a simple text-guided attention mechanism by multiplying the text embedding and vision embedding which is effective against most attack types. However, for more challenging attacks such as AutoAttack, the improvement remains limited. This indicates that while our approach shows promise, it may require further refinement to enhance robustness under stronger adversarial scenarios.

**Border Impact.** Large-scale pre-trained vision-language models (VLMs) like CLIP [48] integrate visual and textual data, revolutionizing applications such as image classification, semantic segmentation, and vision question answering. While these models excel in zero-shot learning and transfer learning, they are vulnerable to adversarial attacks, posing risks in critical applications like autonomous vehicles and medical diagnosis. Adversarial training improves robustness but has practical challenges, including increased computational overhead and potential overfitting. Exploring zero-shot adversarial robustness is essential to ensure reliability.

**Acknowledgement.** This work was supported by National Science and Technology Major Project under Grant 2021ZD0112200, in part by the National Natural Science Foundation of China under Grants 62202331, U23A20387, 62036012, 62276118.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline  & Robust & Clean & Average \\ \hline CLIP & 4.90 & 64.42 & 34.66 \\ \hline \(L_{CE}\) & 29.45 & 44.97 & 37.21 \\ \(+L_{AR}\) & 31.71 & 49.96 & 40.84 \\ \(+L_{AMC}\) & 41.96 & 56.48 & 49.22 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on each component. After adversarial fine-tuning the model using adversarial examples generated by PGD-2, we verify the robustness of the model using adversarial examples generated by PGD-100.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline
**Methods** & Train memory usage & Train time (per epoch / batch) & Test time (per batch) \\ \hline CLIP [48] & 0Mb & 0s / 0s & 21s \\ TeCoA [38] & 12873Mb & 512s / 0.65s & 21s \\ PMG-AFT[59] & 18449Mb & 828s / 1.06s & 21s \\ TGA-ZSR (ours) & 21227Mb & 885s / 1.13s & 21s \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of memory usage, training time, and test time.

## References

* [1] Alex Andonian, Shixing Chen, and Raffay Hamid. Robust cross-modal representation learning with progressive self-distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16430-16441, 2022.
* [2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13_, pages 446-461. Springer, 2014.
* [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Advances in neural information processing systems_, volume 33, pages 1877-1901, 2020.
* [4] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In _2017 ieee symposium on security and privacy (sp)_, pages 39-57. Ieee, 2017.
* [5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3606-3613, 2014.
* [6] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223, 2011.
* [7] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _International conference on machine learning_, pages 2206-2216. PMLR, 2020.
* [8] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. Revisiting pre-trained models for Chinese natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings_, pages 657-668, 2020.
* [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _North American Chapter of the Association for Computational Linguistics_, 2019.
* [11] Minjing Dong and Chang Xu. Adversarial robustness via random projection filters. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4077-4086, 2023.
* [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [13] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _2004 conference on computer vision and pattern recognition workshop_, pages 178-178. IEEE, 2004.
* [14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. _International Journal of Computer Vision_, 132(2):581-595, 2024.
* [15] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
* [16] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip: Zero-shot enhancement of clip with parameter-free attention. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 746-754, 2023.
* [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [18] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 12(7):2217-2226, 2019.

* [19] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 8340-8349, 2021.
* [20] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15262-15271, 2021.
* [21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR, 2021.
* [22] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [24] Minjong Lee and Dongwoo Kim. Robust evaluation of diffusion-based adversarial purification. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 134-144, 2023.
* [25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International conference on machine learning_, pages 12888-12900. PMLR, 2022.
* [26] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In _Advances in neural information processing systems_, volume 34, pages 9694-9705, 2021.
* [27] Lin Li, Jianing Qiu, and Michael Spratling. Avoid: Improving adversarial robustness through online instance-wise data augmentation. _arXiv preprint arXiv:2306.07197_, 2023.
* [28] Lin Li and Michael W Spratling. Data augmentation alone can improve adversarial training. In _The Eleventh International Conference on Learning Representations_, 2022.
* [29] Shuxin Li, Xu Cheng, Fan Shi, Hanwei Zhang, Hongning Dai, Houxiang Zhang, and Shengyong Chen. A novel robustness-enhancing adversarial defense approach to ai-powered sea state estimation for autonomous marine vessels. _IEEE Transactions on Systems, Man, and Cybernetics: Systems_, 2024.
* [30] Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robust against out-of-distribution data? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3821-3831, 2023.
* [31] Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, and Xiaomeng Li. Exploring visual interpretability for contrastive language-image pre-training. _arXiv preprint arXiv:2209.07046_, 2022.
* [32] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 21152-21164, 2023.
* [33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [35] Yanxiang Ma, Minjing Dong, and Chang Xu. Adversarial robustness through random weight sampling. In _Advances in Neural Information Processing Systems_, volume 36, pages 37657-37669, 2023.
* [36] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.

* [38] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. In _The Eleventh International Conference on Learning Representations_, 2023.
* [39] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. _arXiv preprint arXiv:2111.09734_, 2021.
* [40] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2574-2582, 2016.
* [41] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar. Diffusion models for adversarial purification. In _International Conference on Machine Learning_, pages 16805-16827. PMLR, 2022.
* [42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian conference on computer vision, graphics & image processing_, pages 722-729. IEEE, 2008.
* [43] A Omkar M Parkhi. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* [44] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Clip-guided vision-language pre-training for question answering in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5606-5611, 2023.
* [45] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Clip-guided vision-language pre-training for question answering in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5606-5611, 2023.
* [46] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3498-3505. IEEE, 2012.
* [47] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. _arXiv preprint arXiv:2001.07966_, 2020.
* [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [50] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 18082-18091, 2022.
* [51] Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. In _International Conference on Machine Learning_, 2024.
* [52] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE international conference on computer vision_, pages 618-626, 2017.
* [53] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In _Advances in neural information processing systems_, volume 32, 2019.
* [54] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, 2014.

* [55] Zhe Tao, Lu Yu, Hantao Yao, Shucheng Huang, and Changsheng Xu. Class incremental learning for light-weighted networks. _IEEE Transactions on Circuits and Systems for Video Technology_, 2024.
* [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, volume 30, 2017.
* [57] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11_, pages 210-218. Springer, 2018.
* [58] Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Anima Anandkumar, and Zhangyang Wang. Augmax: Adversarial composition of random augmentations for robust training. In _Advances in neural information processing systems_, volume 34, pages 237-250, 2021.
* [59] Sibo Wang, Jie Zhang, Zheng Yuan, and Shiguang Shan. Pre-trained model guided fine-tuning for zero-shot adversarial robustness. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2024.
* [60] Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, et al. Bev-clip: Multi-modal bev retrieval methodology for complex scene in autonomous driving. _arXiv preprint arXiv:2401.01065_, 2024.
* [61] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In _Advances in neural information processing systems_, volume 33, pages 2958-2969, 2020.
* [62] Zhaoyuan Yang, Zhiwei Xu, Jing Zhang, Richard I. Hartley, and Peter Tu. Adversarial purification with the manifold hypothesis. In _AAAI Conference on Artificial Intelligence_, 2022.
* [63] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context optimization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6757-6767, 2023.
* [64] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In _International Conference on Learning Representations_, 2021.
* [65] Lu Yu, Malvina Nikandrou, Jiali Jin, and Verena Rieser. Quality-agnostic image captioning to safely assist people with vision impairment. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence_, pages 6281-6289, 2023.
* [66] Lu Yu and Verena Rieser. Adversarial textual robustness of visual dialog. In _Findings of 61st Annual Meeting of the Association for Computational Linguistics 2023_, pages 3422-3438. Association for Computational Linguistics, 2023.
* [67] Lu Yu, Zhe Tao, Hantao Yao, Joost Van de Weijer, and Changsheng Xu. Exploiting the semantic knowledge of pre-trained text-encoders for continual learning. _arXiv preprint arXiv:2408.01076_, 2024.
* [68] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International conference on machine learning_, pages 7472-7482. PMLR, 2019.
* [69] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep neural networks via stability training. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4480-4488, 2016.

[MISSING_PAGE_FAIL:15]

### More Ablation Studies

**Trade-off between \(\alpha\) and \(\beta\).** Following the protocol of previous works (TeCoA [38], PMG-AFT [59], FARE [51]), we fine-tuned the CLIP model on adversarial samples from a single dataset (Tiny-ImageNet in our case) for 'adversarial fine-tuning' and subsequently evaluated its performance across 15 datasets, including Tiny-ImageNet itself. Thus we only need to tune hyperparameters on just the training dataset. We randomly selected 80% of the training set for training and the remaining 20% for validation to choose the hyperparameters. The validation set results are shown in Table 13. The final results on the test set were obtained by training on the entire training set using the optimal hyperparameters (\(\alpha\)=0.08, \(\beta\)=0.05) identified from the validation set.

After choosing the optimal hyper-parameter on the validation set of the Tiny-ImageNet dataset, we proceeded to analyze the sensitivity of these hyper-parameters on the overall performance across 16 datasets, including the Tiny-ImageNet and 15 additional datasets. This step was crucial for evaluating the robustness and generalizability of the model under different hyper-parameter settings. Table 14 and Table 15 demonstrate the results of different hyper-parameter of \(\alpha\) and \(\beta\) in Eq. 8.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**Hyper-parameters** & Robust & Clean & Average \\ \hline \(\alpha=0.07,\beta=0.05\) & 64.32 & 75.92 & 70.12 \\ \(\alpha=0.08,\beta=0.04\) & 47.25 & 76.20 & 61.72 \\ \(\alpha=0.08,\beta=0.06\) & 58.28 & 76.08 & 67.18 \\ \(\alpha=0.09,\beta=0.05\) & 46.20 & 76.10 & 61.15 \\ \hline \hline \(\alpha=0.08,\beta=0.05\) & 64.01 & 77.79 & 70.90 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Results on validation set of Tiny-ImageNet dataset.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline
**Methods** & **ImageNet-A** & **ImageNet-O** & **ImageNet-R** & **Average** \\ \hline CLIP [48] & **29.49** & 46.05 & **63.61** & **46.38** \\ FT-Clean & 17.23 & 39.25 & 50.68 & 35.72 \\ FT-Adv. & 6.45 & 37.25 & 35.80 & 26.50 \\ TeCoA [38] & 6.04 & 42.50 & 40.65 & 29.73 \\ FARE [51] & 16.24 & **49.30** & 58.62 & 41.39 \\ PMG-AFT [59] & 6.19 & 42.55 & 40.81 & 29.85 \\ TGA-ZSR (ours) & 13.77 & 47.30 & 52.39 & 37.82 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Zero-shot clean accuracy. We performed different several methods on Tiny-ImageNet and evaluated in the following three additional datasets. The optimal accuracy is highlighted in **bold**, while the second-best accuracy is underlined.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline \hline
**Hyper-parameters** & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Hyper-parameters**} & \multirow{2}{*}{**Average**} \\ \hline \(\alpha\)=0.07, \(\beta\)=0.05 & 62.27 & 57.87 & 33.48 & 82.61 & 30.95 & 31.89 & 54.87 & 32.38 & **21.60** & 13.37 & **4.56** & 27.35 & 68.70 & 56.05 & 18.52 & 46.63 & 40.43 \\ \(\alpha\)=0.09, \(\beta\)=0.05 & 48.33 & 40.62 & 22.51 & 77.25 & 32.36 & 19.93 & 42.27 & 22.57 & 18.25 & 47.12 & 31.29 & 59.82 & 48.87 & 12.28 & 47.11 & 32.31 \\ \(\alpha\)=0.

**Effect of Each Component.** Due to space constraints, a detailed experiment on the effect of each component was not provided in Table 7. Therefore, we present here a comprehensive experiment detailing the effect of each component. To explore the impact of each component on the final model performance, we conducted a series of ablation experiments to evaluate the effectiveness of each component. From the experimental results Table 16 and Table 17, it's clear that the inclusion of our text-guided attention components enhances CLIP's zero-shot robust accuracy and maintains its zero-shot clean accuracy.

**Effect of Distance Metrics on Loss Function.** Except \(l_{2}\), \(cosine\) and \(l_{1}\) are also frequently utilized distance metrics. We compared the performance of our method using these distance metrics. The results from Table 18 and Table 19 demonstrate that \(cosine\) and \(l_{1}\) exhibit similar performance but are inferior to \(l_{2}\), except for the zero-shot clear accuracy on PCAM. \(l_{2}\) outperforms the other two distance measures by enhancing zero-shot adversarial robustness and zero-shot clean accuracy by approximately 12% and 11%, respectively. Thus we choose \(l_{2}\) distance to measure in our loss function.

**Effect of Learning Rate.** Learning rate stands as a significant hyper-parameter in model training. Here we validate the effect of the learning rate for the experiment in Table 20 and Table 21. When the learning rate is set to 0.001, we observe the lowest values for both zero-shot robust accuracy and zero-shot clean accuracy. And, when we reduce the learning rate to 0.00001, we note that CLIP's zero-shot clean accuracy remains relatively stable, demonstrating the highest performance in this experiment. However, it affects CLIP's zero-shot robust accuracy, resulting in a less favorable balance. Selecting a learning rate of 0.0001 achieved a well-balanced improvement, with the average of zero-shot robust accuracy and zero-shot clean accuracy increasing by 18.48% and 6.41%, respectively, compared to the learning rates of 0.001 and 0.00001.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c c} \hline \hline
**Components** & **
\begin{tabular}{c} **\(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}} {\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}}{{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\))))))))}}        \, {\{\{\{\{\{\}{\)\{\{{{\}{\]

\begin{\begin{array{array{}[] *{*{*{*{*{*}{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{{\bm{\bm{\bm{{\bm{{\bm{{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\)\)\) 

[MISSING_PAGE_FAIL:18]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we provide a clear explanation of the motivation behind our approach. At the conclusion of the introduction, we enumerate the contributions of this paper, etc. We provide details in Abstract and Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our approach entails certain limitations, which we acknowledge in Section 5 and commit to addressing through subsequent research. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: Our approach emphasizes applied aspects and does not need theory assumptions and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Section 3, we introduce our methodology, while in Section 4.1, we outline the hyper-parameters employed for the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will provide open access to the code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Section 4.1, we introduce the experimental setup, including the hyperparameters utilized in this paper. Furthermore, in section A, we present highly detailed ablation experiments to enhance comprehension of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conduct multiple experiments to report the error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Section 4.5, we provide comprehensive details on the computational resources used and the duration of the training process. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper adheres to the NeurIPS Code of Ethics in all aspects. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts of the work performed in Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The model and dataset employed in our study are widely recognized and do not exhibit any inherent issues. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The models, methodologies, datasets, and other elements used are appropriately aligned and referenced throughout the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.