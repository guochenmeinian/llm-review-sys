# Lightweight Vision Transformer with Bidirectional Interaction

Qihang Fan \({}^{1,2}\), Huaibo Huang\({}^{1}\), Xiaoqiang Zhou\({}^{1,3}\), Ran He\({}^{1,2}\)

\({}^{1}\)MAIS & CRIPAC, Institute of Automation, Chinese Academy of Sciences, Beijing, China

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China

\({}^{3}\)University of Science and Technology of China, Hefei, China

fanqihang.159@gmail.com, huaibo.huang@cripac.ia.ac.cn,

xq525@mail.ustc.edu.cn, rhe@nlpr.ia.ac.cn

 Ran He is the corresponding author.

###### Abstract

Recent advancements in vision backbones have significantly improved their performance by simultaneously modeling images' local and global contexts. However, the bidirectional interaction between these two contexts has not been well explored and exploited, which is important in the human visual system. This paper proposes a **F**ully **A**daptive **S**elf-**A**ttention (FASA) mechanism for vision transformer to model the local and global information as well as the bidirectional interaction between them in context-aware ways. Specifically, FASA employs self-modulated convolutions to adaptively extract local representation while utilizing self-attention in down-sampled space to extract global representation. Subsequently, it conducts a bidirectional adaptation process between local and global representation to model their interaction. In addition, we introduce a fine-grained downsampling strategy to enhance the down-sampled self-attention mechanism for finer-grained global perception capability. Based on FASA, we develop a family of lightweight vision backbones, **F**ully **A**daptive **T**ransformer (FAT) family. Extensive experiments on multiple vision tasks demonstrate that FAT achieves impressive performance. Notably, FAT accomplishes a **77.6%** accuracy on ImageNet-1K using only **4.5M** parameters and **0.7G** FLOPs, which surpasses the most advanced ConvNets and Transformers with similar model size and computational costs. Moreover, our model exhibits faster speed on modern GPU compared to other models.

## 1 Introduction

Vision Transformers (ViTs) have recently garnered significant attention in the computer vision community due to their exceptional ability for long-range modeling and context-aware characteristics. However, because of the quadratic complexity of self-attention in ViT [13], its computational cost is extremely high. As a result, many studies have emerged to improve ViT's computational efficiency and performance in various ways. For instance, some methods restrict tokens that perform self-attention to a specific region and introduce inductive bias to ViT [34; 12; 66; 55]. Further, some methods aim to transform ViT into lightweight backbones with fewer parameters and computational requirements [38; 40; 4; 30; 37], achieving promising results but still not matching the performance of the most advanced ConvNets [51]. How to design an excellent lightweight Vision Transformer remains a challenge.

In current state-of-the-art Vision Transformers, some either excel in creating local feature extraction modules [34; 12; 66] or employing efficient global information aggregation modules [57; 58], while others incorporate both [42; 41]. For instance, LVT [66] unfolds tokens into separate windows andapplies self-attention within the windows to extract local features, while PVT [57; 58] leverages self-attention with downsampling to extract global features and reduce computational cost. Unlike them, LITv2 [41] relies on window self-attention and spatial reduction attention to capture local and global features, respectively. In terms of the local-global fusion, most methods use simple local-global sequential structures [38; 40; 37; 26; 82], whereas others combine local and global representation with simple linear operations through local-global parallel structures [48; 41; 43]. However, few works have investigated the bidirectional interaction between local and global information. Considering the human visual system where bidirectional local-global interaction plays an important role, these simplistic mixing methods are not fully effective in uncovering the intricate relationship between local and global contexts.

In Fig. 1, we illustrate how humans observe an object and notice its body details. Using the example of a fox, we can observe two types of interaction that occur when humans focus on either the fox's nose or the entire animal. In the first type of interaction, known as Local to Global, our understanding of the local feature transforms into the "Nose of a fox." In the second type of interaction, called Global to Local, the way we comprehend the global feature changes to the "A fox with nose." It can be seen that the bidirectional interaction between local and global features plays an essential role in the human visual system. Based on this fact, we propose that a superior visual model should not only extract good local and global features but also possess adequate modeling capabilities for their interaction.

In this work, our objective is to model the bidirectional interaction between local and global contexts while also improving them separately. To achieve this goal, we introduce three types of Context-Aware Feature Aggregation modules. Specifically, as shown in Fig. 1, we first adaptively aggregate local and global features using context-aware manners to obtain local and global tokens, respectively. Then, we perform point-wise cross-modulation between these two types of tokens to model their

Figure 1: Illustration of the human visual system (top) and our FASA (bottom). The human visual system can perceive both local and global contexts and model the bidirectional interaction between them. Our FASA follows this mechanism and consists of three parts: (a) local adaptive aggregation, (b) global adaptive aggregation, and (c) bidirectional adaptive interaction. Our FASA models local information, global information, and local-global bidirectional interaction in context-aware manners.

Figure 2: Top-1 accuracy v.s. FLOPs on ImageNet-1K of recent SOTA CNN and transformer models. The proposed Fully Adaptive Transformer (FAT) outperforms all the counterparts in all settings.

bidirectional interaction. We streamline all three processes into a simple, concise, and straightforward procedure. Since we use context-aware approaches to adaptively model all local, global, and local-global bidirectional interaction, we name our novel module the Fully Adaptive Self-Attention (FASA). In FASA, we also further utilize a fine-grained downsampling strategy to enhance the self-attention mechanism, which results in its ability to perceive global features with finer granularity. In summary, FASA introduces only a small number of additional parameters and FLOPs, yet it significantly improves the model's performance.

Building upon FASA, we introduce the Fully Adaptive Transformer (FAT) family. The FATs follow the hierarchical design [34; 57] and serve as general-purpose backbones for various computer vision tasks. Through extensive experiments, including image classification, object detection, and semantic segmentation, we validate the performance superiority of the FAT family. Without extra training data or supervision, our FAT-B0 achieves a top-1 accuracy of **77.6%** on ImageNet-1K with only **4.5M** parameters and **0.7G** FLOPs, which is the first model surpasses the most advanced ConvNets with similar model size and computational cost as far as we know. Additionally, as shown in Fig. 2, our FAT-B1, B2, and B3 also achieve state-of-the-art results while maintaining similar model sizes and computational costs.

## 2 Related Work

**Vision Transformer.** Since the earliest version of ViT [13] appeared, numerous works have been proposed that focus on enhancing the self-attention mechanism. Many methods restrict self-attention to a subset of tokens, sacrificing global receptive fields to reduce the computation of ViT [34; 12; 66; 61; 1]. Despite having only local perception capabilities, the context-aware nature of self-attention enables these methods to achieve excellent performance. A famous among them is the Swin-Transformer [34], which divides all tokens into windows and performs self-attention within each window, achieving highly competitive performance. In contrast, some methods downsample the \(K\) and \(V\) in self-attention to preserve the global receptive field while reducing computation by minimizing the number of token pairs involved in the calculation [57; 60; 40; 47; 68; 5]. One such method, PVT [57], leverages large-stride convolution to process \(K\) and \(V\), effectively lowering their spatial resolution. In addition to these methods, numerous efforts have been made to introduce ViT into the design of lightweight vision backbones [26; 40; 37; 15; 38; 4]. For example, MobileViT [38] concatenates convolution and self-attention to obtain a powerful, lightweight backbone. Nevertheless, a performance gap exists between lightweight vision transformers and state-of-the-art lightweight CNNs such as NAS-based EfficientNet [51].

**Local-Global Fusion.** A high-performance vision backbone typically possesses exceptional capabilities for both local and global perception. The capabilities are achieved by either connecting local and global perception modules in a serial manner [58; 31; 10; 26; 40; 55; 1], as demonstrated by DaViT [10], or by simultaneously modeling local and global information within a single module [48; 41; 43], such as the inception transformer [48]. However, current approaches to fusing local and global information are overly simplistic. In serial models, the process of fusing local and global information is not adequately represented. In parallel structures, almost all methods rely on linear modules that depend entirely on trainable parameters to fuse local and global information [41; 48; 43]. These fusing approaches lack the ability to model the interaction between local and global information, which is inconsistent with the human visual system shown in Fig. 1. In contrast, our proposed FASA module models bidirectional interaction between local and global information while separately modeling each one.

**Self-Attention with Downsampling.** Currently, many models utilize self-attention with downsampling, a technique that was earlier used in PVT and so on to improve the computational efficiency [57; 58; 43; 41; 48; 61; 5]. PVT [57] reduces the spatial resolution of \(K\) and \(V\) using non-overlapping large stride convolutions, which decreases the number of token pairs involved in self-attention and lowers the computational cost while maintaining a global receptive field. Similarly, PVTv2 [58] uses large stride average pooling to downsample \(K\) and \(V\). In contrast, inception transformer [48] employs large stride average pooling to downsample all three \(Q\), \(K\), and \(V\), then upsamples the tokens to their original size after self-attention is applied. However, using excessively large strides or non-overlapping downsampling as in these methods may lead to significant information loss. In FASA, we propose a fine-grained downsampling strategy to alleviate this issue.

## 3 Method

### Overall Architecture.

The overall architecture of the Fully Adaptive Transformer (FAT) is illustrated in Fig. 3. To process an input image \(x\in\mathcal{R}^{3\times H\times W}\), we begin by feeding it into the convolutional stem used in [63]. This produces tokens of size \(\frac{H}{4}\times\frac{W}{4}\). Following the hierarchical designs seen in previous works [48; 50; 46; 49], we divide FAT into four stages to obtain hierarchical representation. Then we perform average pooling on the feature map containing the richest semantic information. The obtained one-dimensional vector is subsequently classified using a linear classifier for image classification.

A FAT block comprises three key modules: Conditional Positional Encoding (CPE) [6], Fully Adaptive Self-Attention (FASA), and Convolutional Feed-Forward Network (ConvFFN). The complete FAT block is defined by the following equation (Eq. 1):

\[\begin{split} X&=\mathrm{CPE}(X_{in})+X_{in},\\ Y&=\mathrm{FASA}(\mathrm{LN}(X))+X,\\ Z&=\mathrm{ConvFFN}(\mathrm{LN}(Y))+\mathrm{ ShortCut}(Y).\end{split}\] (1)

Initially, the input tensor \(X_{in}\in\mathcal{R}^{C\times H\times W}\) passes through the CPE to introduce positional information for each token. The subsequent stage employs FASA to extract local and global representation adaptively, while a bidirectional adaptation process enables interaction between these two types of representation. Finally, ConvFFN is applied to enhance local representation further. Two kinds of ConvFFNs are employed in this process. In the in-stage FAT block, ConvFFN's convolutional stride is 1, and \(\mathrm{ShortCut}=\mathrm{Identity}\). At the intersection of the two stages, the ConvFFN's convolutional stride is set to 2, and \(\mathrm{ShortCut}=\mathrm{DownSample}\) is achieved via a Depth-Wise Convolution (DWConv) [23] with the stride of 2 along with a \(1\times 1\) convolution. The second type of ConvFFN accomplishes downsampling using only a small number of parameters, avoiding the necessity of patch merging modules between the two stages, thereby saving the parameters.

### Fully Adaptive Self-Attention

In Fig. 1, we present the visual system of humans, which not only captures local and global information but also models their interaction explicitly. Taking inspiration from this, we aim to develop a similar module that can adaptively model local and global information and their interaction. This leads to the proposal of the **F**ully **A**daptive **S**elf-**A**ttention (FASA) module. Our FASA utilizes context-aware manners to model all three types of information adaptively. It comprises three modules:

Figure 3: Illustration of the FAT. FAT is composed of multiple FAT blocks. A FAT block consists of CPE, FASA and ConvFFN.

global adaptive aggregation, local adaptive aggregation, and bidirectional adaptive interaction. Given the input tokens \(X\in\mathcal{R}^{C\times H\times W}\), each part of the FASA will be elaborated in detail.

#### 3.2.1 Definition of Context-Aware Feature Aggregation

In order to provide readers with a clear comprehension of FASA, we will begin by defining the various methods of context-aware feature aggregation (CAFA). CAFA is a widely used feature aggregation method. Instead of solely relying on shared trainable parameters, CAFA generates token-specific weights based on the target token and its local or global context. These newly generated weights, along with the associated context of the target token, are then used to modulate the target token during feature aggregation, enabling each token to adapt to its related context. Generally, CAFA consists of two processes: aggregation (\(\mathcal{A}\)) and modulation (\(\mathcal{M}\)). In the following, the target token is denoted by \(x_{i}\), and \(\mathcal{F}\) represents a non-linear activation function. Based on the order of \(\mathcal{A}\) and \(\mathcal{M}\), CAFA can be classified into various types. Various forms of self-attention [56; 58; 57; 45] can be expressed as Eq. 2. Aggregation over the contexts \(X\) is performed after the attention scores between query and key are computed. The attention scores are obtained by modulating the query with the keys, and then applying a \(\mathrm{Softmax}\) to the resulting values:

\[y_{i}=\mathcal{A}(\mathcal{F}(\mathcal{M}(x_{i},X)),X),\] (2)

In contrast to the approach outlined in Eq. 2, recent state-of-the-art ConvNets [22; 17; 67] utilize a different CAFA technique. Specifically, they employ DWConv to aggregate features, which are then used to modulate the original features. This process can be succinctly described using Eq. 3.

\[y_{i}=\mathcal{M}(\mathcal{A}(x_{i},X),x_{i}),\] (3)

In our FASA module, global adaptive aggregation improves upon the traditional self-attention method with fine-grained downsampling and can be mathematically represented by Eq. 2. Meanwhile, our local adaptive aggregation, which differs slightly from Eq. 3, can be expressed as Eq. 4:

\[y_{i}=\mathcal{M}(\mathcal{F}(\mathcal{A}(x_{i},X)),\mathcal{A}(x_{i},X)),\] (4)

In terms of the bidirectional adaptive interaction process, it can be formulated by Eq. 5. Compared to the previous CAFA approaches, bidirectional adaptive interaction involves two feature aggregation operators (\(\mathcal{A}_{1}\) and \(\mathcal{A}_{2}\)) that are modulated with each other:

\[\begin{split} y_{i2}&=\mathcal{M}(\mathcal{F}( \mathcal{A}_{1}(x_{i},X)),\mathcal{A}_{2}(x_{i},X)),\\ y_{i1}&=\mathcal{M}(\mathcal{F}(\mathcal{A}_{2}(x_ {i},X)),\mathcal{A}_{1}(x_{i},X)).\end{split}\] (5)

#### 3.2.2 Global Adaptive Aggregation

The inherently context-aware nature and capacity to model long-distance dependencies of self-attention make it highly suitable for adaptively extracting global representation. As such, we utilize self-attention with downsampling for global representation extraction. In contrast to other models that downsample tokens [57; 58; 60; 48] by using large stride convolutions or pooling operations, we adopt a fine-grained downsampling strategy to minimize loss of global information to the greatest extent possible. In particular, our fine-grained downsampling module is composed of several basic units. Each unit utilizes a DWConv with a kernel size of \(5\times 5\) and a stride of 2, followed by a \(1\times 1\) convolution that subtly downsamples both \(K\) and \(V\). After that, \(Q\), \(K\), and \(V\) are processed through the Multi-Head Self-Attention (MHSA) module. Unlike regular MHSA, we omit the last linear layer. The complete procedure for the global adaptive aggregation process is illustrated in Eq. 6 and Eq. 7. Fisrt, we define our fine-grained downsample strategy and the base unit of it in Eq. 6:

\[\begin{split}\mathrm{BaseUnit}(X)\triangleq\mathrm{Conv}_{1\times 1}(\mathrm{ BN}(\mathrm{DWConv}(X))),\\ \mathrm{pool}(X)\triangleq\mathrm{BN}(\mathrm{DWConv}(\mathrm{ BaseUnit}^{(n)}(X)),\end{split}\] (6)

where \((\mathrm{n})\) represents the number of base units that are concatenated and \(\mathrm{pool}\) denotes our fine-grained downsample operator. Then, the MHSA is conducted as Eq. 7:

\[\begin{split} Q,K,V=W_{1}\otimes X,\\ X^{\prime}_{global}=\mathrm{MHSA}(Q,\mathrm{pool}(K),\mathrm{ pool}(V)).\end{split}\] (7)

where the mathematical symbol \(\otimes\) denotes the operation of matrix multiplication, \(W_{1}\) is a learnable matrix.

#### 3.2.3 Local Adaptive Aggregation

In contrast to self-attention, convolution possesses an inductive bias that facilitates extracting high-frequency local information. However, convolutional feature aggregation solely relies on parameter-shared convolutional kernels, resulting in a lack of context-awareness. To address this issue, we utilize a self-modulating convolutional operator that embeds context-awareness into convolution, enabling it to extract local representation adaptively. Specifically, we generate context-aware weights through the \(\mathrm{Sigmoid}\) and combine them with \(\mathrm{DWConv}\) to adaptively aggregate local information. This process is summarized in Eq. 8, where \(\odot\) represents the Hadamard product, and \(Q\) is directly derived from Eq. 7 for saving parameters of linear projection:

\[\begin{split}& Q^{\prime}=\mathrm{DWConv}(Q),\\ & X^{\prime}_{local}=Q^{\prime}\odot\mathrm{Sigmoid}(Q^{\prime}) =Q^{\prime}\odot\frac{1}{1+e^{-Q^{\prime}}}.\end{split}\] (8)

#### 3.2.4 Bidirectional Adaptive Interaction

As illustrated in Fig. 1, when modeling the fox and its nose separately, two types of interactions - "Local to Global" and "Global to Local" - occur between their respective features. Inspired by the human visual system, we design a bidirectional adaptive interaction process that incorporates both types of interactions. To achieve bidirectional interaction, we adopt a method similar to the one described in Sec. 3.2.3 but utilize cross-modulation instead of self-modulation. Specifically, the equation for Local to Global interaction is given by Eq. 9:

\[\begin{split} X_{local}&=X^{\prime}_{local}\odot \mathrm{Sigmoid}(X^{\prime}_{global})=X^{\prime}_{local}\odot\frac{1}{1+e^{-X^ {\prime}_{global}}}\\ &=Q^{\prime}\odot\frac{1}{1+e^{-Q^{\prime}}}\odot\frac{1}{1+e^{- X^{\prime}_{global}}},\end{split}\] (9)

Similar to Eq. 9, Global to Local interaction is achieved by Eq. 10:

\[\begin{split} X_{global}&=X^{\prime}_{global}\odot \mathrm{Sigmoid}(X^{\prime}_{local})=X^{\prime}_{global}\odot\frac{1}{1+e^{-X^ {\prime}_{local}}}\\ &=X^{\prime}_{global}\odot\frac{1}{1+e^{-Q^{\prime}\odot\frac{1} {1+e^{-Q^{\prime}}}}},\end{split}\] (10)

After completing the two interaction processes, the local and global representation contain information about each other. To merge these representation, we use point-wise multiplication and achieve intermingling among channels with a linear projection, as shown in Eq. 11:

\[\begin{split} Y&=W_{2}\otimes(X_{global}\odot X_{ local})\\ &=W_{2}\otimes(X^{\prime}_{global}\odot Q^{\prime}\odot\frac{1}{1+e^{-Q^{ \prime}}}\odot\frac{1}{1+e^{-X^{\prime}_{global}}}\odot\frac{1}{1+e^{-Q^{\prime }\odot\frac{1}{1+e^{-Q^{\prime}}}}})\end{split}\] (11)

It is worth noting that to get a faster and implementable model, we omit the last high-order variable in Eq. 11 and arrive at a concise expression. More details can be found in appendix. Through the combination of local adaptive aggregation, global adaptive aggregation, and bidirectional adaptive interaction, we present the comprehensive FASA module. As depicted in Fig. 3, our model, which draws inspiration from the human visual system, is both straightforward and simple. It adaptively models local, global, and the interaction between the two in context-aware manners. More discussion about bidirectional adaptive interaction can be found in the appendix.

## 4 Experiments

We conducted experiments on a wide range of vision tasks, including image classification on ImageNet-1K [9], object detection and instance segmentation on COCO 2017 [33], and semantic segmentation on ADE20K [80]. In addition to them, we also make ablation studies to validate the importance of each component. More details can be found in the appendix.

[MISSING_PAGE_FAIL:7]

during inference. Following [57], we fine-tune the models by AdamW with the learning rate of \(1\times 10^{-4}\) and batch size of 16. We train them for 80K iterations on the ADE20K training set.

**Results.** The results can be found in Tab. 2. For B0/B1/B2, the FLOPs are measured with input size of \(512\times 512\). While them for B3 are measured with \(512\times 2048\). All our models achieve the best performance in all comparisons. Specifically, our FAT-B1 exceeds EdgeViT-XS for **+1.5** mIoU. Moreover, our FAT-B3 outperforms the recent Shunted-S for **+0.7** mIoU. All the above results demonstrate our model's superiority in dense prediction.

### Object Detection and Instance Segmentation

**Settings.** We conduct object detection and instance segmentation experiments on the COCO 2017 dataset [33]. Following Swin [34], we use the FAT family as the backbones and take RetinaNet [32] and Mask-RCNN [19] as detection and segmentation heads. We pretrain our backbones on ImageNet-1K and fine-tune them on the COCO training set with AdamW [36].

**Results.** Tab. 3 shows the results with RetinaNet and Mask R-CNN. The results demonstrate that the proposed FAT performs best in all comparisons. For the RetinaNet framework, our FAT-B0 outperforms EdgeViT-XXS by **+0.7** AP, while B1/B2/B3 also perform better than their counterparts. As for the Mask R-CNN, our FAT-B3 outperforms the recent Shunted-S by **+0.5** box AP and **+0.6** mask AP. All the above results tell that our Fully Adaptive Transformer outperforms its counterparts by evident margins. More experiments can be found in appendix.

### Comparison in Efficiency

We compared our approach with the state-of-the-art lightweight vision backbones, as shown in Tab 4. Our method achieved the best trade-off between speed and performance. The CPU is Intel i9Core and the GPU is V100. Inference throughput is measured with batch size 64. Inference latency is measured with batch size 1.

### Larger Model

We scale up our FAT to 50M+ and 85M+ to match the general Vision Transformer backbones. For object detection/instance segmentation, all models use the framework of Mask-RCNN [19] with the 1x schedule. The results are shown in Tab. 5. It can be found that FAT has great scalability.

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{Backbone} & \multicolumn{8}{c|}{RetinaNet \(1\times\)} & \multicolumn{8}{c}{Mask R-CNN \(1\times\)} \\ \cline{2-13}  & Params(M) & \(AP\) & \(AP_{50}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) & \(AP_{55}\) \\ \hline DFV-T [15] & - & - & - & - & - & - & - & 25 & 34.8 & 56.9 & 37.0 & 32.6 & 53.7 & 34.5 \\ PVTV-B0 [58] & 13 & 37.2 & 57.2 & 39.5 & 23.1 & 40.4 & 49.7 & 24 & 38.2 & 60.5 & 40.7 & 36.2 & 57.8 & 38.6 \\ QuadTree-B0 [52] & 13 & 38.4 & 58.7 & 41.1 & 22.5 & 41.7 & 51.6 & 24 & 38.8 & 60.7 & 42.1 & 36.5 & 58.0 & 39.1 \\ EdgeViT-XXS [40] & 13 & 38.7 & 59.0 & 41.0 & 22.4 & 42.0 & 51.6 & 24 & 39.9 & 62.0 & 43.1 & 36.9 & 59.0 & 39.4 \\ \hline
**FAR-B0** & 14 & 40.4 & 61.6 & 42.7 & 24.0 & 44.3 & 53.1 & 24 & 40.8 & 63.3 & 44.2 & 37.7 & 60.0 & 40.0 \\ \hline DFV-T [15] & - & - & - & - & - & - & - & 32 & 39.2 & 62.2 & 42.4 & 36.3 & 58.9 & 38.6 \\ EdgeViT-XS [40] & 16 & 40.6 & 61.3 & 43.3 & 25.2 & 43.9 & 54.6 & 27 & 41.4 & 63.7 & 45.0 & 38.3 & 60.9 & 41.3 \\ VL-Tr2 [77] & 17 & 40.8 & 61.3 & 43.6 & 26.7 & 44.9 & 53.6 & 27 & 41.4 & 63.5 & 45.0 & 38.1 & 60.0 & 40.8 \\ MPViT-T [28] & 17 & 41.8 & 62.7 & 44.6 & 27.2 & 45.1 & 54.2 & 28 & 42.2 & 64.2 & 45.8 & 39.0 & 61.4 & 41.8 \\ \hline
**FAR-B1** & 17 & 42.5 & **61.0** & **45.1** & **26.9** & **46.0** & **56.7** & **28 & **43.3** & **55.6** & **47.4** & **39.6** & **61.9** & **42.8** \\ \hline PVTV-T [57] & 23 & 36.7 & 56.9 & 38.9 & 22.6 & 38.8 & 50.0 & 33 & 36.7 & 59.2 & 39.3 & 35.1 & 56.7 & 37.3 \\ ResT-Small [78] & 23 & 40.3 & 61.3 & 42.7 & 25.7 & 43.7 & 51.2 & 33 & 39.6 & 62.9 & 42.3 & 37.2 & 59.8 & 39.7 \\ PVTV-B1 [58] & 24 & 41.2 & 61.9 & 43.9 & 25.4 & 44.5 & 54.3 & 34 & 41.8 & 64.3 & 45.9 & 38.8 & 61.2 & 41.6 \\ DFV-B1 [5] & - & - & - & - & - & - & 58 & 43.4 & 63.2 & 48.2 & 39.0 & 51.8 & 42.0 \\ QuadTree-B-B1 [52] & 24 & 42.6 & 63.6 & 45.3 & 26.8 & 46.1 & 57.2 & 34 & 43.5 & 65.6 & 47.6 & 40.1 & 62.6 & 43.3 \\ EdgeViT-S40 [40] & 23 & 43.4 & 64.9 & 46.5 & 26.9 & 47.5 & 58.1 & 33 & 44.8 & 67.4 & 48.9 & 41.0 & 64.4 & 43.8 \\ MPViT-XS [28] & 20 & 43.8 & 65.0 & 47.1 & 28.1 & 47.6 & 56.5 & 30 & 44.2 & 66.7 & 48.4 & 40.4 & 63.4 & 43.4 \\ \hline
**FAR-B2** & 24 & 48.0 & 65.2 & 47.2 & 27.5 & 47.9 & 58.8 & 33 & 45.2 & 67.9 & 49.0 & 41.3 & 64.6 & 48.0 \\ \hline Swin-T [34] & 38 & 41.5 & 62.1 & 44.2 & 25.1 & 44.9 & 55.5 & 48 & 42.2 & 64.6 & 46.2 & 31.9 & 61.6 & 42.0 \\ DAT-T [61] & 38 & 42.8 & 64.4 & 45.2 & 28.0 & 45.8 & 57.8 & 48 & 44.4 & 67.6 & 48.5 & 40.4 & 64.2 & 43.1 \\ DaViT- Tiny [10] & 39 & 44.0 & - & - & - & - & 48 & 45.0 & - & 41.1 & - & - \\ CMTS [16] & 44 & 44.3 & 65.5 & 47.5 & 27.1 & 48.3 & 59.1 & 45 & 44.6 & 66.8 & 48.9 & 40.7 & 63.9 & 43.4 \\ MPViT-S [28] & 32 & 45.7 & 57.3 & 48.8 & 28.7 & 49.7 & 59.2 & 43 & 46.4 & 68.6 & 51.2 & 42.4 & 65.6 & 45.7 \\ QuadTree-B2 [52] & 35 & 46.2 & 67.2 & 49.5 & 29.0 & 50.1 & 61.8 & 45 & 46.7 & 68.5 & 51.2 & 42.4 & 65.6 & 45.7 \\ CSWu-T [12] & & - & - & - & - & 42 & 46.7 & 68.6 & 51.3 & 42.2 & 65.6 & 45.4 \\ Shunted-S [47] & 32 & 45.4 & 65.9 & 49.2 & 28.7 & 49.3 & 60.0 & 42 & 47.1 & 68.8 & 52.1 & 42.5 & 65.8 & 45.7 \\ \hline
**FAR-B3** & 39

### Ablation Study and Spectral Analysis

In this section, we conduct experiments to understand FAT better. The training settings are the same as in previous experiments. More experiments can be found in the appendix.

**Bidirectional Adaptive Interaction.** Initially, we validate the efficacy of bidirectional adaptive interaction by comparing it with three baselines for fusing local and global information: concatenation, addition and element-wise multiplication. As shown in Tab. 6, it can be observed that bidirectional adaptive interaction outperforms all baselines significantly across various tasks. Specifically, compared to the baseline, which uses the cat+linear to fuse the local and global information, our bidirectional adaptive interaction uses fewer parameters and FLOPs but surpasses the baseline by **1.0%** in Top1-accuracy. Our bidirectional adaptive interaction also performs better than element-wise multiplication.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline  & \multicolumn{3}{c|}{ImageNet-1K} & \multicolumn{2}{c|}{COCO} & ADE20K \\ Model & Params(M) & FLOPs(G) & Top-1(\%) & \(AP^{b}\) & \(AP^{m}\) & mIoU \\ \hline add+linear & 4.5 & 0.72 & 76.2 & 39.0 & 35.8 & 39.6 \\ cat+linear & 4.8 & 0.77 & 76.6 & 39.6 & 36.3 & 40.2 \\ mul+linear & 4.5 & 0.72 & 77.1 & 40.3 & 37.1 & 40.9 \\ interaction & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\ \hline pool down & 4.4 & 0.71 & 77.2 & 40.2 & 36.9 & 40.6 \\ conv w/o overlap & 4.4 & 0.71 & 77.2 & 40.3 & 36.9 & 40.8 \\ conv w/ overlap & 4.5 & 0.71 & 77.3 & 40.5 & 37.3 & 40.9 \\ refined down & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\ \hline w/o conv. pos & 4.4 & 0.70 & 77.4 & 40.5 & 37.3 & 41.2 \\ conv. pos & 4.5 & 0.72 & 77.6 & 40.8 & 37.7 & 41.5 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation of FAT

\begin{table}
\begin{tabular}{c|c c|c c c|c} \hline \hline Model & Params(M) & FLOPs(G) \(\downarrow\) & CPU(ms) \(\downarrow\) & GPU(ms) \(\downarrow\) & Trp(ings/s) \(\uparrow\) & Top1-acc(\%) \\ \hline EdgeViT-XXS [40] & 4.1 & 0.6 & 43.0 & 14.2 & 1926 & 74.4 \\ MobileViT-XS [38] & 2.3 & 1.1 & 100.2 & 15.6 & 1367 & 74.8 \\ tiny-MOAT-0 [65] & 3.4 & 0.8 & 61.1 & 14.7 & 1908 & 75.5 \\ FAT-B0 & 4.5 & 0.7 & 44.3 & 14.4 & 1932 & 77.6 \\ \hline EdgeViT-XS [40] & 6.7 & 1.1 & 62.7 & 15.7 & 1528 & 77.5 \\ Parc-Net-S [76] & 5.0 & 1.7 & 112.1 & 15.8 & 1321 & 78.6 \\ EdgeNext-S [37] & 5.6 & 1.3 & 86.4 & 14.2 & 1243 & 79.4 \\ FAT-B1 & 7.8 & 1.2 & 62.6 & 14.5 & 1452 & 80.1 \\ \hline ParC-ResNet50 [76] & 23.7 & 4.0 & 160.0 & 16.6 & 1039 & 79.6 \\ tiny-MOAT-2 [65] & 9.8 & 2.3 & 122.1 & 15.4 & 1047 & 81.0 \\ EfficientNet-B3 [51] & 12.0 & 1.8 & 124.2 & 25.4 & 624 & 81.6 \\ FAT-B2 & 13.5 & 2.0 & 93.4 & 14.6 & 1064 & 81.9 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison with other methods in efficiency.

\begin{table}
\begin{tabular}{c|c c|c c|c} \hline \hline Model & Params(M) & FLOPs(G) & Top1-acc(\%) & \(AP^{b}\) & \(AP^{m}\) \\ \hline Swin-S [34] & 50 & 8.7 & 83.0 & 44.8 & 40.9 \\ Focal-S [68] & 51 & 9.1 & 83.5 & 47.4 & 42.8 \\ CMT-B [16] & 46 & 9.3 & 84.5 & – & – \\ FAT-B4 & 52 & 9.3 & 84.8 & 49.7 & 44.8 \\ \hline CSwin-B [12] & 78 & 15.0 & 84.2 & – & – \\ MOAT-2 [65] & 73 & 17.2 & 84.7 & – & – \\ CMT-L [16] & 75 & 19.5 & 84.8 & – & – \\ FAT-B5 & 88 & 15.1 & 85.2 & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison with general backbones.

**Fine-Grained Downsampling.** We compared our fine-grained downsampling strategy with three other strategies: non-overlapping large stride pooling, non-overlapping large stride convolution, and overlapping large stride convolution. As shown in Tab. 6, experimental results across various tasks have demonstrated the effectiveness of our fine-grained downsampling strategy. Specifically, our fine-grained downsampling strategy surpasses directly pooling downsampling strategy by **0.4%**

**Positional Encoding.** At the end of Tab. 6, we explore the effect of CPE, and the results in the table show that the flexible CPE also contributes to the performance improvement of the model for **0.2%** in image classification.

**Spectral Analysis.** As shown in Fig. 4, we conduct a spectral analysis on FASA. Compared to add+linear fusion, our bidirectional adaptive interaction can more fully fuse high-frequency and low-frequency information.

## 5 Conclusion

In this paper, we propose a new attention mechanism named Fully Adaptive Self-Attention (FASA), which draws inspiration from the human visual system. FASA is designed to model local and global information adaptively while also accounting for their bidirectional interaction using adaptive strategies. Furthermore, we enhance the self-attention mechanism in FASA by incorporating a fine-grained downsampling strategy to promote better global perception at a more detailed level. Using the flexible FASA as our foundation, we develop a lightweight vision backbone called Fully Adaptive Transformer (FAT), which can be applied across various vision tasks. Our extensive experiments on diverse asks, such as image classification, object detection, instance segmentation, and semantic segmentation, provide compelling evidence of the effectiveness and superiority of FAT. We expect to apply FAT to other vision-related pursuits, including video prediction and vision-language models.

## Acknowledgment

This work is partially funded by National Natural Science Foundation of China (Grant No. 62006228, U21B2045, U20A20223), Youth Innovation Promotion Association CAS (Grant No. 2022132), and Beijing Nova Program (20230484276).

Figure 4: Spectral analysis from 8 output channels of FASA. The larger magnitude has a lighter color. Pixels that are closer to the center have a lower frequency. From top to bottom, the results are from (a) local adaptive aggregation, (b) global adaptive aggregation, (c) add+linear fusion, and (d) bidirectional adaptive interaction.

## References

* [1]M. Arar, A. Shamir, A. H. Bermano, et al. (2022) Learned queries for efficient local attention. In CVPR, Cited by: SS1.
* [2]C. Chen, R. Panda, and Q. Fan (2022) RegionViT: regional-to-Local Attention for Vision Transformers. In ICLR, Cited by: SS1.
* [3]K. Chen, J. Wang, J. Pang, et al. (2019) MMDetection: open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155. Cited by: SS1.
* [4]Y. Chen, X. Dai, D. Chen, et al. (2022) Mobile-former: bridging mobilenet and transformer. In CVPR, Cited by: SS1.
* [5]X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen (2021) Twins: revisiting the design of spatial attention in vision transformers. In NeurIPS, Cited by: SS1.
* [6]X. Chu, Z. Tian, B. Zhang, X. Wang, and C. Shen (2020) Conditional positional encodings for vision transformers. In ICLR, Cited by: SS1.
* [7]M. Contributors. Mmsgmentation, an open source semantic segmentation toolbox, Cited by: SS1.
* [8]E. D. Cubuk, B. Zoph, J. Shlens, et al. (2020) Randaugment: practical automated data augmentation with a reduced search space. In CVPRW, Cited by: SS1.
* [9]J. Deng, W. Dong, R. Socher, et al. (2009) Imagenet: a large-scale hierarchical image database. In CVPR, Cited by: SS1.
* [10]M. Ding, B. Xiao, N. Codella, et al. (2022) Davit: dual attention vision transformers. In ECCV, Cited by: SS1.
* [11]X. Ding, X. Zhang, Y. Zhou, J. Han, G. Ding, and J. Sun (2022) Scaling up your kernels to 31x31: revisiting large kernel design in cnns. Cited by: SS1.
* [12]X. Dong, J. Bao, D. Chen, et al. (2022) Cswin transformer: a general vision transformer backbone with cross-shaped windows. In CVPR, Cited by: SS1.
* [13]A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al. (2021) An image is worth 16x16 words: transformers for image recognition at scale. In ICLR, Cited by: SS1.
* [14]A. El-Nouby, H. Touvron, M. Caron, et al. (2021) SCIT: cross-covariance image transformers. In NeurIPS, Cited by: SS1.
* [15]L. Gao, D. Nie, B. Li, and X. Ren (2022) Doubly-fused vit: fuse information from vision transformer doubly with local representation. In ECCV, Cited by: SS1.
* [16]J. Guo, K. Han, H. Wu, C. Xu, Y. Tang, C. Xu, and Y. Wang (2022) Cmt: convolutional neural networks meet vision transformers. CVPR. Cited by: SS1.
* [17]M. Guo, C. Lu, Z. Liu, M. Cheng, and J. Feng (2022) Conv2former: a simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943. Cited by: SS1.
* [18]K. Han, A. Xiao, E. Wu, et al. (2021) Transformer in transformer. In NeurIPS, Cited by: SS1.
* [19]K. He, G. Gkioxari, P. Dollar, and R. B. Girshick (2017) Mask r-cnn. In ICCV, Cited by: SS1.
* [20]K. He, X. Zhang, S. Ren, and S. Jian (2016) Deep residual learning for image recognition. In CVPR, Cited by: SS1.
* [21]B. Heo, S. Yun, D. Han, et al. (2021) Rethinking spatial dimensions of vision transformers. In ICCV, Cited by: SS1.
* [22]Q. Hou, C. Lu, M. Cheng, and J. Feng (2022) Conv2former: a simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943. Cited by: SS1.
* [23]A. Howard, M. Sandler, G. Chu, et al. (2016) Searching for mobilenetv3. In ICCV, Cited by: SS1.
* [24]G. Huang, Y. Sun, and Z. Liu (2016) Deep networks with stochastic depth. In ECCV, Cited by: SS1.
* [25]H. Huang, X. Zhou, and R. He (2022) Orthogonal transformer: an efficient vision transformer backbone with token orthogonalization. In NeurIPS, Cited by: SS1.
* [26]T. Huang, L. Huang, S. You, F. Wang, C. Qian, and C. Xu (2022) Lightvit: towards light-weight convolution-free vision transformers. arXiv preprint arXiv:2207.05557. Cited by: SS1.
* [27]A. Kirillov, R. Girshick, K. He, and P. Dollar (2019) Panoptic feature pyramid networks. In CVPR, Cited by: SS1.
* [28]Y. Lee, J. Kim, J. Willette, and S. J. Hwang (2022) MPVit: multi-path vision transformer for dense prediction. In CVPR, Cited by: SS1.
* [29]K. Li, Y. Wang, P. Gao, et al. (2022) Uniformer: unified transformer for efficient spatial-temporal representation learning. In ICLR, Cited by: SS1.
* [30]Y. Li, G. Yuan, Y. Wen, et al. (2022) Efficientformer: vision transformers at mobilenet speed. In NeurIPS, Cited by: SS1.
* [31]Y. Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool (2021) Localvit: bringing locality to vision transformers. arXiv preprint arXiv:2104.05707. Cited by: SS1.
* [32]T. Lin, P. Goyal, R. B. Girshick, and K. He and P. Dollar (2017) Focal loss for dense object detection. In ICCV, Cited by: SS1.
* [33]T. Lin, M. Maire, S. Belongie, et al. (2014) Microsoft coco: common objects in context. In ECCV, Cited by: SS1.
* [34]Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo (2021) Swin transformer: hierarchical vision transformer using shifted windows. In ICCV, Cited by: SS1.
* [35]Z. Liu, H. Mao, C. Wu, et al. (2022) A convnet for the 2020s. In CVPR, Cited by: SS1.
* [36]I. Loshchilov and F. Hutter (2019) Decoupled weight decay regularization. In ICLR, Cited by: SS1.
* [37]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [38]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [39]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [40]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [41]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [42]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [43]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [44]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [45]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [46]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [47]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [48]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [49]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [50]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [51]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [52]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [53]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [54]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
* [55]M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. Waqas Zamir, R. Muhammad Anwer, and F. Shahbaz Khan (2022) Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications. In CADL, Cited by: SS1.
** [38] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. In _ICLR_, 2022.
* [39] Juhong Min, Yucheng Zhao, Chong Luo, et al. Peripheral vision transformer. In _NeurIPS_, 2022.
* [40] Junting Pan, Adrian Bulat, Fuwen Tan, et al. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. In _ECCV_, 2022.
* [41] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. In _NeurIPS_, 2022.
* [42] Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, and Jianfei Cai. Less is more: Pay less attention in vision transformers. In _AAAI_, 2022.
* [43] Zhiliang Peng, Wei Huang, Shanzhi Gu, et al. Conformer: Local features coupling global representations for visual recognition. In _ICCV_, 2021.
* [44] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In _CVPR_, 2020.
* [45] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In _NeurIPS_, 2021.
* [46] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Lam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. _NeurIPS_, 2022.
* [47] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang. Shunted self-attention via multi-scale token aggregation. In _CVPR_, 2022.
* [48] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng YAN. Inception transformer. In _NeurIPS_, 2022.
* [49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _ICLR_, 2015.
* [50] Christian Szegedy, Wei Liu, Yangqing Jia, et al. Going deeper with convolutions. In _CVPR_, 2015.
* [51] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.
* [52] Shitao Tang, Jiahui Zhang, Siyu Zhu, et al. Quadtree attention for vision transformers. In _ICLR_, 2022.
* [53] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang. An image patch is a wave: Phase-aware vision mlp. In _CVPR_, 2022.
* [54] Hugo Touvron, Matthieu Cord, Matthijs Douze, et al. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.
* [55] Zhengzhong Tu, Hossein Talebi, Han Zhang, et al. Maxvit: Multi-axis vision transformer. In _ECCV_, 2022.
* [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is all you need. In _NeurIPS_, 2017.
* [57] Wehnai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. _arXiv preprint arXiv:2103.15808_, 2021.
* [58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. _Computational Visual Media_, 8(3):1-10, 2022.
* [59] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer hinging on cross-scale attention. In _ICLR_, 2022.
* [60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. _arXiv preprint arXiv:2103.15808_, 2021.
* [61] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable attention. In _CVPR_, 2022.
* [62] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, 2018.
* [63] Tete Xiao, Mannat Singh, Eric Mintun, et al. Early convolutions help transformers see better. In _NeurIPS_, 2021.
* [64] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In _ICCV_, 2021.
* [65] Chenglin Yang, Siyuan Qiao, Qihang Yu, et al. Moat: Alternating mobile convolution and attention brings strong vision models. In _ICLR_, 2023.
* [66] Chenglin Yang, Yilin Wang, Jianming Zhang, et al. Lite vision transformer with enhanced self-attention. In _CVPR_, 2022.
* [67] Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao. Focal modulation networks. _NeurIPS_, 2022.
* [68] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers, 2021.
* [69] Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng, and Xiu Li. Scalablevit: Rethinking the context-oriented generalization of vision transformer. _ECCV_, 2022.
* [70] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao Mei. Wave-vit: Unifying wavelet and transformers for visual representation learning. In _Proceedings of the European conference on computer vision (ECCV)_, 2022.
* [71] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In _CVPR_, 2022.
* [72] Kun Yuan, Shaopeng Guo, Ziwei Liu, et al. Incorporating convolution designs into visual transformers. In _ICCV_, 2021.

* [73] Li Yuan, Yunpeng Chen, Tao Wang, et al. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _ICCV_, 2021.
* [74] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, et al. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _ICCV_, 2019.
* [75] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, et al. mixup: Beyond empirical risk minimization. In _ICLR_, 2018.
* [76] Haokui Zhang, Wenze Hu, and Xiaoyu Wang. Parc-net: Position aware circular convolution with merits from convnets and transformer. In _ECCV_, 2022.
* [77] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In _ICCV_, 2021.
* [78] Qinglong Zhang and Yu bin Yang. Rest: An efficient transformer for visual recognition. In _NeurIPS_, 2021.
* [79] Zhun Zhong, Liang Zheng, Guoliang Kang, et al. Random erasing data augmentation. In _AAAI_, 2020.
* [80] Bolei Zhou, Hang Zhao, Xavier Puig, et al. Scene parsing through ade20k dataset. In _CVPR_, 2017.
* [81] Daquan Zhou, Zhiding Yu, Enze Xie, et al. Understanding the robustness in vision transformers. In _ICML_, 2022.
* [82] Hao Zhu, Man-Di Luo, Rui Wang, Ai-Hua Zheng, and Ran He. Deep audio-visual learning: A survey. _IJAC_, 18:351, 2021.
* [83] Zhuofan Zong, Kunchang Li, Guanglu Song, et al. Self-slimmed vision transformer. In _ECCV_, 2022.

## Appendix A Discussion About "Interaction" And "Fusion"

Generally speaking, interaction refers to the process of two or more objects or systems affecting each other. In contrast, fusion refers to combining two or more things into a single entity. As shown in Fig. 5, in contrast to all previous feature fusion methods, our proposed bidirectional adaptive interaction stands out in two significant ways:

**1) Bidirectional Property:** Our approach draws inspiration from how visual information flows bidirectionally through the human visual system. Before fusing local and global features, we model a bidirectional process between the two. Specifically, for local features, we utilize global features to generate weights that encompass global information. These weights are applied to the local features, allowing them to have the ability to capture the global context. We execute a similar process for global features. This results in strengthened local and global features, respectively. In contrast to linear fusion, our interaction returns two enhanced vectors representing local and global contexts rather than an inseparable vector mixed with both [41; 48; 43].

**2) Adaptive Capability:** Taking local features as an example, the global weights we use to enhance them are not trainable. They are generated by tokens containing global features, making them context-aware and capable of adapting to the input data. Moreover, when fusing the enhanced local and global features, we do not introduce any additional parameters but calculate their element-wise product (Hadamard product) directly. Compared with linear fusion, our context-aware fusion approach is closer to the attention mechanism [56; 17; 22]. Our ablation experiments have shown that this fusion approach not only saves parameters but also achieves better results. Specifically, for a detailed comparison between the Add+Linear, Cat+Linear, and our model, please refer to the paper's main text.

The implementation of our proposed bidirectional adaptive interaction method is very simple, yet it has achieved remarkable results compared to other methods for fusing local and global features. Through this work, we aim to reveal the critical role that features fusion plays in visual models and hope to inspire more related research.

## Appendix B More experiments

We evaluate our backbones with the framework of UperNet [62] based on MMsegmentation [7]. Following the [34], we adopt AdamW to optimize the model for 160K iterations. As shown in Tab. 7, our model surpasses all its counterparts and achieves the best performance. Specifically, our FAT-B3 surpass the resent Shunted-S by **+0.7** mIoU and **+0.8** MS mIoU.

Figure 5: Comparison between our bidirectional adaptive interaction and traditional fusion method.

## Appendix C Implementation Details

### Architecture Details

The detailed architectures are shown in Tab. 8, where all FLOPs are measured at the resolution of \(224\times 224\) for image classification. For the convolution stem, we adopt four \(3\times 3\) convolutions and one \(1\times 1\) convolution to tokenize the image. Batch normalization and ReLU are used after each \(3\times 3\) convolution, and a layer normalization is used after the final \(1\times 1\) convolution. Inspired by the fact that in the vision backbone, the early stages tend to capture high-frequency local information, and the later stages tend to capture low-frequency global information, we gradually increase the DWConv's kernel sizes used in FASA. Specifically, we set the kernel size to 3 in the first stage and 9 in the last stage. The expansion ratios are set to 4, and the kernel sizes are set to 5 for all ConvFFN layers.

### Image Classification

We follow the same training strategy with Swin-Transformer [34]. All models are trained for 300 epochs from scratch. To train the models, we use the AdamW optimizer with a cosine decay learning rate scheduler and 20 epoch linear warm-up. We set the initial learning rate, weight decay, and batch size to 0.001, 0.05, and 1024, respectively. We adopt the strong data augmentation and regularization used in [34]. Our settings are RandAugment [8] (randm9-mstd0.5-inc1), Mixup [75] (prob=0.8), CutMix [74] (prob=1.0), Random Erasing [79] (prob=0.25), increasing stochastic depth [24] (prob=0.05, 0.1, 0.1, 0.15 for FAT-B0, FAT-B1, FAT-B2 and FAT-B3).

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline Backbone & Params(M) & FLOPs(G) & mIoU(\%) & MS mIoU(\%) \\ \hline Swin-T [34] & 60 & 945 & 44.5 & 45.8 \\ DAT-T [61] & 60 & 957 & 45.5 & 49.0 \\ DaViT-T [10] & 60 & 940 & 46.3 & — \\ UniFormer-S [29] & 52 & 955 & 47.0 & 48.5 \\ CrossFormer-S [59] & 62 & 980 & 47.6 & 48.4 \\ MPViT-S [28] & 52 & 943 & 48.3 & — \\ Ortho-S [25] & 54 & 956 & 48.5 & 49.9 \\ Shunted-S [47] & 52 & 940 & 48.9 & 49.9 \\ FAT-B3 & 59 & 936 & 49.6 & 50.7 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results with the framework of UperNet on ADE20K. The FLOPs are measured with the resolution of \(512\times 2048\).

\begin{table}
\begin{tabular}{c c c|c c c} \hline \hline Output Size & Layer Name & FAT-B0 & FAT-B1 & FAT-B2 & FAT-B3 \\ \hline \multirow{4}{*}{\(\frac{H}{4}\times\frac{W}{4}\)} & & \(3\times 3,16,\) s2 & \(3\times 3,24,\) s2 & \(3\times 3,32,\) s2 & \(3\times 3,32,\) s2 \\  & & \(3\times 3,32,\) s2 & \(3\times 3,48,\) s2 & \(3\times 3,64,\) s2 & \(3\times 3,64,\) s2 \\ \cline{2-5}  & Conv Stem & \(3\times 3,32,\) s1 & \(3\times 3,48,\) s1 & \(3\times 3,64,\) s1 & \(3\times 3,64,\) s1 \\  & & \(3\times 3,32,\) s1 & \(3\times 3,48,\) s1 & \(3\times 3,64,\) s1 & \(3\times 3,64,\) s1 \\  & & \(1\times 1,32,\) s1 & \(1\times 1,48,\) s1 & \(1\times 1,64,\) s1 & \(1\times 1,64,\) s1 \\ \hline \multirow{4}{*}{\(\frac{H}{4}\times\frac{W}{4}\)} & CPE & \(\begin{bmatrix}3\times 3,32\\ kernel\text{, heads}\end{bmatrix}\times 2\end{bmatrix}\times 2\) & \(\begin{bmatrix}3\times 3,48\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}3\times 3,48\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}3\times 3,64\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}3\times 3,64\\ kernel\text{, heads}\end{bmatrix}\times 4\) \\ \cline{2-5}  & ConvFFN & \(\begin{bmatrix}5\times 5,80\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}5\times 5,96\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}5\times 5,128\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}5\times 5,128\\ kernel\text{, heads}\end{bmatrix}\times 2\) \\ \cline{2-5}  & ConvFFN & \(\begin{bmatrix}5\times 5,80\\ 5\times 5,80\end{bmatrix}\) & \(\begin{bmatrix}5\times 5,96\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}5\times 5,128\\ kernel\text{, heads}\end{bmatrix}\times 2\) \\ \hline \multirow{4}{*}{\(\frac{H}{16}\times\frac{W}{16}\)} & CPE & \(\begin{bmatrix}7\times 7,160\\ kernel\text{, heads}\end{bmatrix}\times 6\) & \(\begin{bmatrix}7\times 7,192\\ kernel\text{, heads}\end{bmatrix}\times 6\) & \(\begin{bmatrix}7\times 7,256\\ kernel\text{, heads}\end{bmatrix}\times 6\) & \(\begin{bmatrix}7\times 7,256\\ kernel\text{, heads}\end{bmatrix}\times 6\) \\ \cline{2-5}  & ConvFFN & \(\begin{bmatrix}5\times 5,160\\ 5\times 5,126\end{bmatrix}\) & \(\begin{bmatrix}7\times 5,192\\ 5\times 5,192\end{bmatrix}\times 6\) & \(\begin{bmatrix}7\times 7,256\\ kernel\text{, heads}\end{bmatrix}\times 6\) \\ \hline \multirow{4}{*}{\(\frac{H}{32}\times\frac{W}{32}\)} & CPE & \(\begin{bmatrix}9\times 9,256\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}9\times 9,384\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}9\times 9,512\\ kernel\text{, heads}\end{bmatrix}\times 2\) & \(\begin{bmatrix}9\times 9,512\\ kernel\text{, heads}\end{bmatrix}\times 2\) \\ \cline{2-5}  & ConvFFN & \(\begin{bmatrix}5\times 5,256\end{bmatrix}\) & \(\begin{bmatrix}9\times 9,512\\ 5\times 5,512\end{bmatrix}\times 4\) \\ \hline \(1\times 1\) & Classifier & Fully Connected Layer, 1000 \\ \hline Params & 4.5M & 7.8M & 13.5M & 29M \\ \hline FLOPs & 0.7G & 1.2G & 2.0G & 4.4G \\ \hline \hline \end{tabular}
\end{table}
Table 8: Details about FAT’s architecture.

### Object Detection and Instance Segmentation

We adopt MMDetection [3] to implement RetinaNet [32] and Mask-RCNN [19]. We use the commonly used "\(1\times\)" (12 training epochs) setting for the two strategies. Following [34], during training, images are resized to the shorter side of 800 pixels while the longer side is within 1333 pixels. We adopt the AdamW optimizer with a learning rate of 0.0001, weight decay of 0.05, and batch size of 16 to optimize the model. The learning rate declines with the decay rate of 0.1 at the epoch 8 and 11.

### Semantic Segmentation

We adopt the Semantic FPN [27] and UperNet [62] based on MMSegmentation [7] and apply FAT pretrained on ImageNet-1K as backbone. We use the same setting of PVT [57] to train the Semantic FPN, and we use the "\(1\times\)" schedule (trained for 80k iterations). All the models are trained with the input resolution of \(512\times 512\). When testing the model, we resize the shorter side of the image to 512 pixels. As for UperNet, we follow the default settings in Focal Transformer [68]. We take AdamW with a weight decay of 0.01 as the optimizer and train the models for 160K iterations. The learning rate is set to \(6\times 10^{-5}\) with 1500 iterations warmup.

## Appendix D More Ablation and Analysis Results

Other architecture design choices.We modified FAT-B3 to have the same layout as Swin-T and compared it to four baseline models: Swin-T [34], DAT-T [61], FocalNet-T [67] and Focal-T [68]. As shown in Tab. 9, our model demonstrates significant superiority, with an accuracy 1.7% higher than Swin-T and 0.8% higher than Focal-Tiny.

Contribution Isolation.To better isolate the contribution of our FASA, we compare the different self-attention mechanisms on the same backbone. We choose five kinds of self-attention mechanisms to make the comparison: CSWSA in CSwin-Transformer [12], Max-SA in MaxViT [55], WSA/S-WSA in Swin-Transformer [34], SRA in PVT [57] and LSA/GSA in Twins-SVT [5]. The results are shown in Tab. 10. The networks' layout and the setting of CPE and ConVFFN are the same as FAT-B0. It can be found that our FASA surpasses all its counterparts. Compared to the latest Max-SA, our FASA achieved a gain of 1.9%.

Local Aggregation in FASA.To validate that the model's capability to perceive local context does not solely rely on CPE and ConvFFN, we conducted experiments by removing local adaptive aggregation and bidirectional adaptive interaction from FASA, retaining only global adaptive aggregation. The results presented in Tab. 11 demonstrate a significant decrease in model performance (**1.4%**) when using only global adaptive aggregation. This outcome convincingly supports the soundness of our proposed FASA model.

\begin{table}
\begin{tabular}{c|c c|c} \hline \hline \multirow{2}{*}{Method} & Params & FLOPs & Top1-acc \\  & (M) & (G) & (\%) \\ \hline Max-SA [55] & 4.3 & 0.8 & 75.7 \\ WSA/S-WSA [34] & 4.3 & 0.8 & 76.1 \\ SRA [57] & 4.4 & 0.7 & 76.2 \\ LSA/GSA [5] & 4.3 & 0.7 & 76.6 \\ CSWSA [12] & 4.3 & 0.8 & 76.8 \\ FASA & 4.5 & 0.7 & **77.6** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison among different self-attention mechanisms.

\begin{table}
\begin{tabular}{c|c c|c c|c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multirow{2}{*}{Blocks} & \multirow{2}{*}{Channels} & \multirow{2}{*}{Params(M)} & \multirow{2}{*}{FLOPs(G)} & \multirow{2}{*}{Top-1 acc(\%)} \\ \hline Swin-T [34] & [2, 2, 6, 2] & [96, 192, 384, 768] & 29 & 4.5 & 81.3 \\ DAT-T [61] & [2, 2, 6, 2] & [96, 192, 384, 768] & 29 & 4.6 & 82.0 \\ FocalNet-T [67] & [2, 2, 6, 2] & [96, 192, 384, 768] & 28 & 4.4 & 82.1 \\ Focal-T [68] & [2, 2, 6, 2] & [96, 192, 384, 768] & 29 & 4.9 & 82.2 \\ FAT-B3-ST & [2, 2, 6, 2] & [96, 192, 384, 768] & 29 & 4.7 & **83.0** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison with four baseline models when use the same layout with Swin-T.

[MISSING_PAGE_FAIL:17]

the higher-order variables in bidirectional adaptive interaction, the original expression can be simplified to two SiLU activations followed by the multiplication of two branches. These measures significantly accelerated the inference speed of our model. Here, we list the most recent models in Tab. 17. As can be seen from the table, our model performs the best in terms of both speed and performance.

## Appendix F Limitations and Broader Impacts

While our FAT achieves superior performance with fast processing speed, one limitation of our work is that due to computational resource constraints, we do not apply our lightweight vision backbone to unsupervised learning, video processing, or visual language tasks. Additionally, we do not pretrain our models on large-scale datasets like ImageNet-21K. However, we look forward to exploring more applications of our proposed FAT in the future.

This study is purely academic in nature, and we are unaware of any negative social impact resulting directly from our work. However, we acknowledge that our models' potential malicious use is a concern affecting the entire field. Discussions related to this matter are beyond the scope of our research.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Model & \begin{tabular}{c} Throughput \\ (imgs/s) \\ \end{tabular} & \begin{tabular}{c} Top1-Acc \\ (\%) \\ \end{tabular} & Year & Model & \begin{tabular}{c} Throughput \\ (imgs/s) \\ \end{tabular} & 
\begin{tabular}{c} Top1-Acc \\ (\%) \\ \end{tabular} & Year \\ \hline T2T-ViT-7 [73] & 1762 & 71.7 & ICCV-2021 & Swin-T [34] & 664 & 81.3 & ICCV-2021 \\ QuadTree-B-10 [52] & 885 & 72.0 & ICLR-2022 & PoolFormer-S36 [71] & 601 & 81.4 & CVPR 2022 \\ TNT-Tiny [18] & 545 & 73.9 & NeuroFFs-2021 & CrossFormer-T [59] & 929 & 81.5 & ICLR 2022 \\ EdgeNet-Y-XXX [40] & 1926 & 74.4 & ECCV-2022 & EfficientNet-B3 [51] & 634 & 81.6 & ICML 2019 \\ LVT [66] & 1265 & 74.8 & CVPR 2022 & Rest-Base [75] & 588 & 81.6 & NeuroFFs-2021 \\ MobileViT-X [38] & 1367 & 74.8 & ICLR-2022 & **FAT-B2** & **1064** & **81.9** & **Ours** \\ EdgeNet-XX [37] & 1417 & 75.0 & ECCV-2022 & Coat-L2-S [64] & 540 & 81.9 & ICCV 2021 \\ VAN-B0 [17] & 1662 & 75.4 & Arxiv-2022 & DAT-T [61] & 577 & 82.0 & CVPR 2022 \\ VLT-Tiny [77] & 857 & 76.7 & ICCV-2021 & PVT-B2 [58] & 582 & 82.0 & Arxiv-2022 \\ PoolFormer-S12 [71] & 1722 & 77.2 & CVPR 2022 & PIT-B [21] & 328 & 82.0 & ICCV 2021 \\ XCT-IT2 [14] & 1676 & 77.1 & NeuroFFs-2021 & Focal-T [61] & 610 & 82.1 & NeuroFFs-2022 \\
**FAT-B0** & **1932** & **77.6** & **Ours** & CrossFormer-T [59] & 592 & 82.4 & ICCV 2021 \\ \hline T2T-ViT-12 [73] & 1307 & 76.5 & ICCV-2021 & RegionViT-S [2] & 460 & 82.6 & ICLR 2022 \\ EdgeNet-Y-XX [40] & 1528 & 77.5 & ECCV-2022 & WaveM/P-S [53] & 599 & 82.6 & CVPR 2022 \\ Coat-L2-T [64] & 1045 & 77.5 & ICCV-2021 & CSwin-T [12] & 591 & 82.7 & CVPR 2022 \\ SiT w/o FRD [83] & 1057 & 77.7 & ECCV-2022 & WaveViT-S [70] & 482 & 82.7 & ECCV 2022 \\ RegNet-Y-1-6GF [44] & 1241 & 78.0 & CVPR-2020 & VAN-B2 [71] & 531 & 82.8 & Arxiv 2022 \\ MPMV-ViT-T [28] & 737 & 78.2 & CVPR 2022 & DaViT-T [10] & 616 & 82.8 & ECCV 2022 \\ MobileViT-S [38] & 898 & 78.4 & ICLR-2022 & HoeNet-T [46] & 586 & 82.8 & NeuroFFs-2022 \\ ParC-Net-S [76] & 1321 & 78.6 & ECCV-2022 & FAN-S-VT [31] & 525 & 82.9 & ICML 2022 \\ PVT2-B1 [58] & 1007 & 78.7 & Arxiv-2022 & **FAT-B3-ST** & **641** & **83.0** & **Ours** \\ PerViT [39] & 1402 & 78.8 & NeuFFs-2022 & DeiT-B [54] & 299 & 81.8 & ICML 2021 \\ Coat-L2-Mie (64) & 963 & 79.1 & ICCV-2021 & Focal-T [68] & 301 & 82.2 & NeuroFFs-2021 \\ EfficientNet-B1 [51] & 1395 & 79.2 & ICML-2019 & PoolFormer-M48 [71] & 304 & 82.5 & CVPR 2022 \\ Fan-T-ViT [51] & 1181 & 79.2 & ICML-2022 & QuadTree-B-1 [52] & 299 & 82.7 & ICLR 2022 \\ EdgeNet-Xu-S [37] & 1243 & 79.4 & ECCV-W 2022 & Shunted-S [47] & 456 & 82.9 & CVPR 2022 \\ XCT-T2-14 [14] & 1194 & 79.4 & NeuroFFs-2021 & MPViT-S [28] & 410 & 83.0 & CVPR 2022 \\
**FAT-B1** & **1452** & **80.1** & **Ours** & Swin-S [34] & 390 & 83.0 & ICCV 2021 \\ \hline Rest-S [78] & 918 & 79.6 & NeuroFFs-2021 & ConvConv-Sk [35] & 405 & 83.1 & CVPR 2022 \\ Shunted-T [47] & 957 & 79.8 & CVPR 2022 & PVT-B3 [58] & 398 & 83.2 & Arxiv 2022 \\ DerI-S [54] & 899 & 79.9 & ICML-2021 & RegionViT-B [2] & 256 & 83.2 & ICLR 2022 \\ QuadTree-B-1 [52] & 543 & 80.0 & ICLR-2022 & EfficientFormer-L7 [30] & 368 & 83.3 & NeuroFFs-2022 \\ RegionViT-T [21] & 710 & 80.4 & ICLR 2022 & LLT-T-M [41] & 436 & 83.3 & NeuroFFs-2022 \\ WaveM/P-T [53] & 1052 & 80.6 & CVPR2022 & FocalNet-S [67] & 588 & 83.4 & NeuroFFs-2022 \\ PT-S [21] & 1042 & 80.9 & ICCV-2021 & Former-S [47] & 471 & 83.4 & NeuroFFs-2022 \\ MPViT-X [28] & 597 & 80.9 & CVPR2022 & CrossFormer-B [59] & 368 & 83.4 & ICLR 2022 \\ EdgeViT-X [40] & 1049 & 81.0 & ECCV 2022 & WaveMLP-M [53] & 359 & 83.4 & CVPR 2022 \\ VAN-B1 [17] & 995 & 81.1 & Arxiv-2022 & **FAT-B3** & **474** & **83.6** & **Ours** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Comparisons of speed and performance among different models. All speeds are measured on the V100 32G with the batch size of 64.