# Adaptive SGD with Polyak stepsize and Line-search:

Robust Convergence and Variance Reduction

 Xiaowen Jiang

CISPA

xiaowen.jiang@cispa.de &Sebastian U. Stich

CISPA

stich@cispa.de

CISPA

CISPA

CISPA

stich@cispa.de

CISPA Helmholtz Center for Information Security, Saarbrucken, Germany

###### Abstract

The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, two issues remain unsolved in this line of work. First, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [44]), this approach results in slower convergence rates under interpolation. Second, intuitive line-search methods equipped with variance-reduction (VR) fail to converge (Dubois-Taine et al. [14]). So far, no VR methods successfully accelerate these two stepsizes with a convergence guarantee. In this work, we make two contributions: Firstly, we propose two new robust variants of SPS and SLS, called AdaSPS and AdaSLS, which achieve optimal asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings, except for the case when we have both strong convexity and non-interpolation. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we propose a novel VR method that can use Polyak stepsizes or line-search to achieve acceleration. When it is equipped with AdaSPS or AdaSLS, the resulting algorithms obtain the optimal rate for optimizing convex smooth functions. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.

## 1 Introduction

Stochastic Gradient Descent (SGD) [46] and its variants [7] are among the most preferred algorithms for training modern machine learning models. These methods only compute stochastic gradients in each iteration, which is often more efficient than computing a full batch gradient. However, the performance of SGD is highly sensitive to the choice of the stepsize. Common strategies use a fixed stepsize schedule, such as keeping it constant or decreasing it over time. Unfortunately, the theoretically optimal schedules are disparate across different function classes [8], and usually depend on problem parameters that are often unavailable, such as the Lipschitz constant of the gradient. As a result, a heavy tuning of the stepsize parameter is required, which is typically expensive in practice.

Instead of fixing the stepsize schedule, adaptive SGD methods adjust the stepsize on the fly [15, 26]. These algorithms often require less hyper-parameter tuning and still enjoy competitive performance in practice. Stochastic Polyak Stepsize (SPS) [34, 6, 45] is one of such recent advances. It has received rapid growing interest due to two factors: (i) the only required parameter is the individual optimal function value which is often available in many machine learning applications and (ii) its adaptivityutilizes the local curvature and smoothness information allowing the algorithm to accelerate and converge quickly when training over-parametrized models. Stochastic Line-Search (SLS) [52] is another adaptive stepsize that offers exceptional performance when the interpolation condition holds. In contrast to SPS, the knowledge of the optimal function value is not required for SLS, at the cost of additional function value evaluations per iteration.

An ideal adaptive stepsize should not only require fewer hyper-parameters but should also enjoy _robust_ convergence, in the sense that they can automatically adapt to the optimization setting (interpolation vs. non-interpolation). This will bring great convenience to the users in practice as they no longer need to choose which method to use (or running both of them at double the cost). Indeed, in many real-world scenarios, it can be challenging to ascertain whether a model is effectively interpolating the data or not [11]. For instance, the feature dimension of the rcv1 dataset [10] is twice larger than the number of data points. A logistic regression model with as many parameters as the feature dimension may tend to overfit the data points. But the features are actually sparse and the model is not interpolated. Another example is federated learning [23] where millions of clients jointly train a machine learning model on their mobile devices, which usually cannot support huge-scale models. Due to the fact that each client's data is stored locally, it becomes impractical to check the interpolation condition.

While SPS and SLS are promising adaptive methods, they are not robust since both methods cannot converge to the solution when interpolation does not hold. Orvieto et al. [44] address this issue for SPS by applying an artificially decreasing rule and the resulting algorithm DecSPS is able to converge as quickly as SGD with the optimal stepsize schedule. However, the convergence rates of DecSPS in interpolation regimes are much slower than SPS. For SLS, no solution has been proposed.

If the user is certain that the underlying problem is non-interpolated, then applying variance-reduction (VR) techniques can further accelerate the convergence [22, 40, 13, 27, 49, 33]. While gradient descent with Polayak stepsize and line-search perform well in the deterministic settings, there exists no method that successfully adapt these stepsizes in VR methods. Mairal [36] and Schmidt et al. [49] proposed to use stochastic line-search with VR. However, no theoretical guarantee is shown. Indeed, this is a challenging open question as Dubois-Taine et al. [14] provides a counter-example where classical line-search methods fail in the VR setting. As such, it remains unclear whether we can accelerate SGD with stepsizes from Polyak and line-search family in non-interpolated settings.

### Main contributions

In this work, we provide solutions to the aforementioned two challenges and contribute new theoretical insights on Polyak stepsize and line-search methods. We summarize our main contributions as follows:

* In Section 3, we propose the first robust adaptive methods that simultaneously achieve the best-known asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings except for the case when we have strongly-convexity and non-interpolation. The first method called AdaSPS, a variant of SPS, requires only a lower bound of the optimal function value as input (similar to DecSPS) while AdaSLS, the second method based on SLS, is parameter-free. In the non-interpolated setting, we prove for both algorithms an \(\mathcal{O}(1/\varepsilon^{2})\) convergence rate for convex functions which matches the classical DecSPS and AdaGrad [15] results, whereas SPS and SLS cannot converge in this case. In the interpolated regime, we establish fast \(\mathcal{O}(\log(1/\varepsilon))\) and \(\mathcal{O}(1/\varepsilon)\) rates under strong convexity and convexity conditions respectively, without knowledge of any problem-dependent parameters. In contrast, DecSPS converges at the slower \(\mathcal{O}(1/\varepsilon^{2})\) rate and for AdaGrad, the Lipschitz constant is needed to set its stepsize [56].
* In Section 4, we design a new variance-reduction method that is applicable to both Polyak stepsizes or line-search methods. We prove that to reach an \(\varepsilon\)-accuracy, the total number of gradient evaluations required in expectation is \(\widetilde{\mathcal{O}}(n+1/\varepsilon)\) for convex functions which matches the rate of AdaSVRG [14]. With our newly proposed decreasing probability strategy, the artificially designed multi-stage inner-outer-loop structure is not needed, which makes our methods easier to analyze. Our novel VR-framework is based on proxy function sequences and can recover the standard VR methods [22] as a special case. We believe that this technique can be of independent interest to the optimization community and may motivate more personalized VR techniques in the future.

### Related work

Line-search procedures has been successfully applied to accelerate large-scale machine learning training. Following [52], Galli et al. [16] propose to relax the condition of monotone decrease of objective function for training over-parameterized models. Kunstner et al. [30] extends backtracking line-search to a multidimensional variant which provides better diagonal preconditioners. In recent years, adaptive stepsizes from the AdaGrad family have become widespread and are particularly successful when training deep neural networks. Plenty of contributions have been made to analyze variants of AdaGrad for different classes of functions [15; 51; 43; 55; 56], among which Vaswani et al. [53] first propose to use line-search to set the stepsize for AdaGrad to enhance its practical performance. More recently, variance reduction has successfully been applied to AdaGrad stepsize and faster convergence rates have been established for convex and non-convex functions [14; 25].

Another promising direction is the Polyak stepsize (PS) [45] originally designed as a subgradient method for solving non-smooth convex problems. Hazan and Kakade [20] show that PS indeed gives simultaneously the optimal convergence result for a more general class of convex functions. Nedic and Bertsekas [38] propose several variants of PS as incremental subgradient methods and they also discuss the method of dynamic estimation of the optimal function value when it is not known. Recently, more effort has been put into extending deterministic PS to the stochastic setting [47; 6; 42]. However, theoretical guarantees of the algorithms still remain elusive until the emergence of SPS/SPS\({}_{\max}\)[34]. Subsequently, further improvements and new variants such as DecSPS [44] and SPS with a moving target [17] have been introduced. A more recent line of work interprets stochastic Polyak stepsize as a subsampled Newton Raphson method and interesting algorithms have been designed based on the first-order local expansion [17; 18] as well as the second-order expansion [31]. Wang et al. [54] propose to set the stepsize for SGD with momentum using Polyak stepsize. Abdukhakimov et al. [1] employ more general preconditioning techniques to SPS.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Stepsize**} & \multicolumn{3}{c}{**Interpolation**} & \multicolumn{3}{c}{**Non-interpolation**} \\ \cline{2-7}  & strongly-convex & convex & required input & strongly-convex & convex\({}^{a}\) & required input \\ \hline SPS/SPS\({}_{\max}\)[34] & \(\mathcal{O}(\log(\frac{1}{\varepsilon}))\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & \(f^{*}_{h}\) & \(\varepsilon\geq\Omega(\sigma^{2}_{f,0})\) & \(\varepsilon\geq\Omega(\sigma^{2}_{f,0})\) & \(f^{*}_{i}\) \\ SLS [52] & \(\mathcal{O}(\log(\frac{1}{\varepsilon}))\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & None & \(\varepsilon\geq\Omega(\sigma^{2}_{f,0})\) & \(\varepsilon\geq\Omega(\sigma^{2}_{f,0})\) & None \\ DecSPS [44] & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & \(f^{*}_{h}\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & \(f^{*}_{i}\) \\ \hline AdaSPS (this work) & \(\mathcal{O}(\log(\frac{1}{\varepsilon}))\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & \(f^{*}_{h}\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & \(f^{*}_{i}\) \\ AdaSLS (this work) & \(\mathcal{O}(\log(\frac{1}{\varepsilon}))\) & \(\mathcal{O}(\frac{1}{\varepsilon})\) & None & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & \(\mathcal{O}(\frac{1}{\varepsilon^{3}})\) & None \\ \hline \hline \multicolumn{7}{l}{\({}^{*}\)The assumption of bounded iterates is also required except for SPS and SLS.} \\ \end{tabular}
\end{table}
Table 1: Summary of convergence behaviors of the considered adaptive stepsizes for smooth functions. For SPS/SPS\({}_{\max}\) and SLS in non-interpolation settings, \(\Omega(\cdot)\) indicates the size of the neighborhood that they can converge to. In the other cases, the \(\mathcal{O}(\cdot)\) complexity provides the total number of gradient evaluations required for each algorithm to reach an \(\mathcal{O}(\varepsilon)\) suboptimality. For convex functions, the suboptimality is defined as \(\mathbb{E}[f(\Re_{T})-f^{*}]\) and for strongly convex functions, the suboptimality is defined as \(\mathbb{E}[||\mathbf{x}_{T}-\mathbf{x}^{*}||^{2}]\).

Figure 1: Illustration of the robust convergence of AdaSPS and AdaSLS on synthetic data with quadratic loss. SPS and SLS have superior performance on the two interpolated problems but cannot converge when the interpolation condition does not hold. DecSPS suffers from a slow convergence on both interpolated problems. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)There has been a recent line of work attempting to develop methods that can adapt to both the interpolation setting and the general growth condition beyond strong convexity. Using the iterative halving technique from [5], Cheng and Duchi [11] propose AdaStar-G which gives the desired property if the Lipschitz constant and the diameter of the parameter domain are known. How to remove these requirements is an interesting open question for the future research.

## 2 Problem setup and background

### Notations

In this work, we consider solving the finite-sum smooth convex optimization problem:

\[\min_{\mathbf{x}\in\mathbb{R}^{d}}\left[f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{ n}f_{i}(\mathbf{x})\right]\;.\] (1)

This type of problem appears frequently in the modern machine learning applications [19], where each \(f_{i}(\mathbf{x})\) represents the loss of a model on the \(i\)-th data point parametrized by the parameter \(\mathbf{x}\). Stochastic Gradient Descent (SGD) [46] is one of the most popular methods for solving the problem (1). At each iteration, SGD takes the form:

\[\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}(\mathbf{x}_{t})\;,\] (2)

where \(\eta_{t}\) is the stepsize parameter, \(i_{t}\subseteq[n]\) is a random set of size \(B\) sampled independently at each iteration \(t\) and \(\nabla f_{i_{t}}(\mathbf{x})=\frac{1}{B}\sum_{i\in i_{t}}\nabla f_{i}(\mathbf{ x})\) is the minibatch gradient.

Throughout the paper, we assume that there exists a non-empty set of optimal points \(\mathcal{X}^{\star}\subset\mathbb{R}^{d}\), and we use \(f^{\star}\) to denote the optimal value of \(f\) at a point \(\mathbf{x}^{\star}\in\mathcal{X}^{\star}\). We use \(f^{\star}_{i_{t}}\) to denote the infimum of minibatch function \(f_{i_{t}}(x)\), i.e. \(f^{\star}_{i_{t}}=\inf_{\mathbf{x}\in\mathbb{R}^{d}}\frac{1}{B}\sum_{i\in i_{ t}}f_{i}(\mathbf{x})\). We assume that all the individual functions \(\{f_{i}(\mathbf{x})\}\) are \(L\)-smooth. Finally, we denote the optimal objective difference, first introduced in [34], by \(\sigma^{2}_{f,B}=f^{\star}-\mathbb{E}_{i_{t}}[f^{\star}_{i_{t}}]\). The definitions for the interpolation condition can be defined and studied in various ways [48; 9; 4; 11]. Here, we adopt the notion from [34]. The problem (1) is said to be **interpolated** if \(\sigma^{2}_{f,1}=0\), which implies that \(\sigma^{2}_{f,B}=0\) for all \(B\leq n\) since \(\sigma^{2}_{f,B}\) is non-increasing w.r.t \(B\). Note interpolation implies that the global minimizer of \(f\) is also a minimizer of each individual function \(f_{i}\).

### SGD with stochastic polyak stepsize

Loizou et al. [34] propose to set the stepsize \(\eta_{t}\) as: \(\eta_{t}=2\frac{f_{i_{t}}(\mathbf{x}_{t})-f^{\star}_{i_{t}}}{||\nabla f_{i_{ t}}(\mathbf{x}_{t})||^{2}}\), which is well known as the Stochastic Polyak stepsize (SPS). In addition to SPS, they also propose a bounded variant SPS\({}_{\max}\) which has the form \(\eta_{t}=\min\bigl{\{}2\frac{f_{i_{t}}(\mathbf{x}_{t})-f^{\star}_{i_{t}}}{|| \nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}},\gamma_{b}\bigr{\}}\) where \(\gamma_{b}>0\). Both algorithms require the input of the exact \(f^{\star}_{i_{t}}\) which is often unavailable when the batch size \(B>1\) or when the interpolation condition does not hold. Orveto et al. [44] removes the requirement for \(f^{\star}_{i_{t}}\) and propose to set \(\eta_{t}\) as: \(\eta_{t}=\frac{1}{\sqrt{t+1}}\min\left\{\frac{f_{i_{t}}(\mathbf{x}_{t})-\ell^{ \star}_{i_{t}}}{||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}},\sqrt{t}\eta_{t-1}\right\}\) for \(t\geq 1\) (DecSPS), where \(\eta_{0}>0\) is a constant and \(\ell^{\star}_{i_{t}}\) is an input lower bound such that \(\ell^{\star}_{i_{t}}\leq f^{\star}_{i_{t}}\). In contrast to the exact optimal function value, a lower bound \(\ell^{\star}_{i_{t}}\) is often available in practice, in particular for machine learning problems when the individual loss functions are non-negative. We henceforth denote the estimation error by:

\[\mathrm{err}^{2}_{f,B}:=\mathbb{E}_{i_{t}}[f^{\star}_{i_{t}}-\ell^{\star}_{i_{ t}}]\,.\] (3)

For convex smooth functions, SPS achieves a fast convergence up to a neighborhood of size \(\Omega(\sigma^{2}_{f,B})\) and its variant SPS\({}_{\max}\) converges up to \(\Omega(\sigma^{2}_{f,B}\gamma_{b}/\alpha)\) where \(\alpha=\min\{\frac{1}{L},\gamma_{b}\}\). Note that the size of the neighborhood cannot be further reduced by choosing an appropriate \(\gamma_{b}\). In contrast, DecSPS converges at the rate of \(\mathcal{O}(1/\sqrt{T})\) which matches the standard result for SGD with decreasing stepsize. However, the strictly decreasing \(\Theta(1/\sqrt{t})\) stepsize schedule hurts its performance in interpolated settings. For example, DecSPS has a much slower \(\mathcal{O}(1/\sqrt{T})\) convergence rate compared with the fast \(\mathcal{O}(\exp(-T\mu/L))\) rate of SPS when optimizing strongly-convex objectives. Therefore, both algorithms do not have the _robust_ convergence property (achieving fast convergence guarantees in both interpolated and non-interpolated regimes) and we aim to fill this gap. See Figure 1 for a detailed illustration of the non-robustness of SPS and DecSPS.

Adaptive SGD with polyak stepsize and line-search

In this section, we introduce and analyze two adaptive algorithms to solve problem (1).

### Proposed methods

**AdaSPS.** Our first stepsize is defined as the following:

\[\eta_{t}=\min\left\{\frac{f_{i_{t}}(\mathbf{x}_{t})-\ell_{i_{t}}^{ \star}}{c_{p}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}}\frac{1}{\sqrt{\sum_{s=0 }^{t}f_{i_{s}}(\mathbf{x}_{s})-\ell_{i_{s}}^{\star}}},\eta_{t-1}\right\}\,, \quad\text{ with }\eta_{-1}=+\infty\,,\] (AdaSPS)

where \(\ell_{i_{t}}^{\star}\) is an input parameter that must satisfy \(\ell_{i_{t}}^{\star}\leq f_{i_{t}}^{\star}\) and \(c_{p}>0\) is an input constant to adjust the magnitude of the stepsize (we discuss suggested choices in Section 5).

AdaSPS can be seen as an extension of DecSPS. However, unlike the strict \(\Theta(1/\sqrt{t})\) decreasing rule applied in DecSPS, AdaSPS accumulates the function value difference during the optimization process which enables it to dynamically adapt to the underlying unknown interpolation settings.

**AdaSLS.** We provide another stepsize that can be applied even when a lower bound estimation is unavailable. The method is based on line-search and thus is completely parameter-free, but requires additional function value evaluations in each iteration:

\[\eta_{t}=\min\left\{\frac{\gamma_{t}}{c_{l}\sqrt{\sum_{s=0}^{t} \gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}},\eta_{t-1}\right\}\,, \quad\quad\quad\text{with }\eta_{-1}=+\infty\,,\] (AdaSLS)

where \(c_{l}>0\) is an input constant, and the scale \(\gamma_{t}\) is obtained via stardard Armijo backtracking line-search (see Algorithm 4 for further implementation details in the Appendix D) such that the following conditions are satisfied:

\[f_{i_{t}}(\mathbf{x}_{t}-\gamma_{t}\nabla f_{i_{t}}(\mathbf{x}_{t}))\leq f_{i _{t}}(\mathbf{x}_{t})-\rho\gamma_{t}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2} \quad\text{and}\quad\gamma_{t}\leq\gamma_{\max},\ 0<\rho<1\,\] (4)

for line-search parameters \(\gamma_{\max}\) and \(\rho\). By setting the decreasing factor \(\beta\geq\frac{1}{2}\) defined in Algorithm 4, one can show that \(\gamma_{t}\geq\min(\frac{1-\rho}{L},\gamma_{\max})\). We give a formal proof in Lemma 16 in Appendix A.2.

**Discussion.** Our adaptation mechanism in AdaSPS/AdaSLS is reminiscent of AdaGrad type methods, in particular to AdaGrad-Norm, the scalar version of AdaGrad, that aggregates the gradient norm in the denominator and takes the form \(\eta_{t}=\frac{c_{g}}{\sqrt{\sum_{s=0}^{t}||\nabla f_{i_{t}}(\mathbf{x}_{s})|| ^{2}+b_{0}^{2}}}\) where \(c_{g}>0\) and \(b_{0}^{2}\geq 0\).

The primary distinction between AdaSPS and AdaSLS compared to AdaGrad-Norm is the inclusion of an additional component that captures the curvature information at each step, and not using squared gradient norms in AdaSPS. In contrast to the strict decreasing behavior of AdaGrad-Norm, AdaSPS and AdaSLS can automatically mimic a constant stepsize when navigating a flatter region.

Vaswani et al. [53] suggest using line-search to set the stepsize for AdaGrad-Norm which takes the form \(\eta_{t}=\frac{\gamma_{t}}{\sqrt{\sum_{s=0}^{t}||\nabla f_{i_{t}}(\mathbf{x}_ {s})||^{2}}}\) where \(\gamma_{t}\leq\gamma_{t-1}\) is required for solving non-interpolated convex problems. While this stepsize is similar to AdaSLS, the scaling of the denominator gives a suboptimal convergence rate as we demonstrate in the following section.

### Convergence rates

In this section, we present the convergence results for AdaSPS and AdaSLS. We list the helpful lemmas in Appendix A. The proofs can be found in Appendix B.

**General convex.** We denote \(\mathcal{X}\) to be a convex compact set with diameter \(D\) such that there exists a solution \(\mathbf{x}^{\star}\in\mathcal{X}\) and \(\sup_{\mathbf{x},\mathbf{y}\in\mathcal{X}}||\mathbf{x}-\mathbf{y}||^{2}\leq D ^{2}\). We let \(\Pi_{\mathcal{X}}\) denote the Euclidean projection onto \(\mathcal{X}\). For general convex stochastic optimization, it seems inevitable that adaptive methods require the bounded iterates assumption or an additional projection step to prove convergence due to the lack of knowledge of problem-dependent parameters [12; 15]. Here, we employ the latter solution by running projected stochastic gradient descent (PSGD):

\[\mathbf{x}_{t+1}=\Pi_{\mathcal{X}}(\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}( \mathbf{x}_{t})).\] (5)

**Theorem 1** (General convex).: _Assume that \(f\) is convex, each \(f_{i}\) is L-smooth and \(\mathcal{X}\) is a convex compact feasible set with diameter \(D\), PSGD with AdaSPS or AdaSLS converges as:_

\[\begin{split}(\text{AdaSPS}):\ \mathbb{E}[f(\bar{\mathbf{x}}_{T})-f^{ \star}]&\leq\frac{\tau_{p}^{2}}{T}+\frac{\tau_{p}\sqrt{\sigma_{f,B }^{2}+\mathrm{err}_{f,B}^{2}}}{\sqrt{T}}\,\\ (\text{AdaSLS}):\ \mathbb{E}[f(\bar{\mathbf{x}}_{T})-f^{\star}]& \leq\frac{\tau_{l}^{2}}{T}+\frac{\tau_{l}\sigma_{f,B}}{\sqrt{T}}\,\end{split}\] (6)

_where \(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\), \(\tau_{p}=(2c_{p}LD^{2}+\frac{1}{c_{p}})\) and \(\tau_{l}=\max\Bigl{\{}\frac{L}{(1-\rho)\sqrt{\rho}},\frac{1}{\gamma_{\max} \sqrt{\rho}}\Bigr{\}}c_{l}D^{2}+\frac{1}{c_{1}\sqrt{\rho}}\)._

As a consequence of Theorem 1, if \(\mathrm{err}_{f,B}^{2}=\sigma_{f,B}^{2}=0\), then PSGD with AdaSPS or AdaSLS converges as \(\mathcal{O}(\frac{1}{T})\). Suppose \(\gamma_{\max}\) is sufficiently large, then picking \(c_{p}^{\star}=\frac{1}{\sqrt{2LD^{2}}}\) and \(c_{l}^{\star}=\frac{\sqrt{1-\rho}}{\sqrt{LD^{2}}}\) gives a \(\mathcal{O}(\frac{LD^{2}}{T})\) rate under the interpolation condition, which is slightly worse than \(\frac{L\|\mathbf{x}_{0}-\mathbf{x}^{\star}\|^{2}}{T}\) obtained by SPS and SLS but is better than \(\mathcal{O}(\frac{LD^{2}}{\sqrt{T}})\) obtained by DecSPS. If otherwise \(\sigma_{f,B}^{2}>0\), then AdaSPS, AdaSLS, and DecSPS converge as \(\mathcal{O}(1/\sqrt{T})\) which matches the rate of Vanilla SGD with decreasing stepsize. Finally, AdaGrad-Norm gives a similar rate in both cases while AdaGrad-Norm with line-search [53] shows a suboptimal rate of \(\mathcal{O}(\frac{L^{3}D^{4}}{T}+\frac{D^{2}L^{3/2}\sigma}{\sqrt{T}})\). It is worth noting that SPS, DecSPS and SLS require an additional assumption on individual convexity.

**Theorem 2** (Individual convex+interpolation).: _Assume that \(f\) is convex, each \(f_{i}\) is convex and \(L\)-smooth, and that \(\mathrm{err}_{f,B}^{2}=\sigma_{f,B}^{2}=0\), by setting \(c_{p}=\frac{c_{p}^{\text{rank}}}{\sqrt{f_{0}}(\mathbf{x}_{0})-f_{0}^{\star}}\) and \(c_{l}=\frac{c_{p}^{\text{rank}}}{\rho\sqrt{\gamma_{0}}\|\nabla f_{0}(\mathbf{ x}_{0})\|^{2}}\) with constants \(c_{p}^{\text{scale}}\geq 1\) and \(c_{l}^{\text{scale}}\geq 1\), then for any \(T\geq 1\), SGD (no projection) with AdaSPS or AdaSLS converges as:_

\[\text{(AdaSPS)}\quad\mathbb{E}[f(\bar{\mathbf{x}}_{T})-f^{\star}]\leq\Bigg{(} 4L(c_{p}^{\text{scale}})^{2}\,\mathbb{E}_{i_{0}}\Bigl{[}\frac{||\mathbf{x}_{0} -\mathbf{x}^{\star}||^{2}}{f_{i_{0}}(\mathbf{x}_{0})-f^{\star}}\Bigr{]}\bigg{)} \frac{L||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{T}\,\] (7)

_and_

\[\text{(AdaSLS)}\quad\mathbb{E}[f(\bar{\mathbf{x}}_{T})-f^{\star}]\leq\Bigg{(} \frac{(c_{l}^{\text{scale}})^{2}}{\rho^{3}L\min^{2}\{\frac{1-\rho}{L},\gamma_{ \max}\}}\,\mathbb{E}_{i_{0}}\Bigl{[}\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star} ||^{2}}{\gamma_{0}||\nabla f_{i_{0}}(\mathbf{x}_{0})||^{2}}\Bigr{]}\Bigg{)} \frac{L||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{T}\.\] (8)

_where \(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t}\)._

The result implies that the bounded iterates assumption is not needed if we have both individual convexity and interpolation by picking \(c_{p}\) and \(c_{l}\) to satisfy certain conditions that do not depend on unknown parameters. To our knowledge, no such result exists for stepsizes from the AdaGrad family. It is worth noting that the \(\min\) operator defined in AdaSPS or AdaSLS is not necessary in the proof.

**Remark 3**.: _We note that for non-interpolated problems, AdaSPS only requires the knowledge of \(\ell_{i_{t}}^{\star}\) while the exact \(f_{i_{t}}^{\star}\) is needed under the interpolation condition. We argue that in many standard machine learning problems, simply picking zero will suffice. For instance, \(f_{i_{t}}^{\star}=0\) for over-parameterized logistic regression and after adding a regularizer, \(\ell_{i_{t}}^{\star}=0\)._

**Strongly convex.** We now present two algorithmic behaviors of AdaSPS and AdaSLS for strongly convex functions. In particular, We show that 1) the projection step can be removed as shown in DecSPS, and 2) if the interpolation condition holds, the \(\min\) operator is not needed and the asymptotic linear convergence rate is preserved. The full statement of Lemma 4 can be found in Appendix B.2.

**Lemma 4** (Bounded iterates).: _Let each \(f_{i}\) be \(\mu\)-strongly convex and L-smooth. For any \(t=0,\ldots,T\), the iterates of SGD with AdaSPS or AdaSLS satisfy: \(||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}\leq D_{\max}\), for a constant \(D_{\max}\) specified in the appendix in Equation (B.16)._

**Corollary 5** (Individual strongly convex).: _Assume each \(f_{i}\) is \(\mu\)-strongly convex and \(L\)-smooth, Theorem 1 holds with PSGD and \(D\) replaced by SGD and \(D_{\max}\) defined in Lemma 4._

Although it has not been formally demonstrated that AdaGrad/AdaGrad-Norm can relax the assumption on bounded iterates for strongly convex functions, we believe that with a similar proof technique, this property still holds for AdaGrad/AdaGrad-Norm.

We next show that AdaSPS and AdaSLS achieve linear convergence under the interpolation condition.

**Theorem 6** (Strongly convex + individual convex + interpolation).: _Consider SGD with AdaSPS (AdaSPS) or AdaSLS (AdaSLS) stepsize. Suppose that each \(f_{i}\) is convex and \(L\)-smooth, \(f\) is \(\mu\)-strongly convex and that \(\sigma_{f,B}^{2}=\operatorname{err}_{f,B}^{2}=0\). If we let \(c_{p}=\frac{c_{p}^{\text{scale}}}{\sqrt{f_{i_{0}}(\mathbf{x}_{0})-f_{i_{0}}^{ \star}}}\) and \(c_{l}=\frac{c_{l}^{\text{scale}}}{\rho\sqrt{\gamma_{0}||\nabla f_{i_{0}}(\mathbf{ x}_{0})||^{2}}}\) with constants \(c_{p}^{\text{scale}}\geq 1\) and \(c_{l}^{\text{scale}}\geq 1\), then AdaSPS or AdaSLS converges as:_

\[\text{(AdaSPS)}\quad\mathbb{E}[||\mathbf{x}_{T+1}-\mathbf{x}^{ \star}||^{2}]\leq\mathbb{E}_{i_{0}}\left[\left(1-\frac{(f_{i_{0}}(\mathbf{x}_{ 0})-f^{\star})\mu}{(2c_{p}^{\text{scale}}L||\mathbf{x}_{0}-\mathbf{x}^{\star}|| )^{2}}\right)^{T}\right]||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}\,\] (9)

_and_

\[\text{(AdaSLS)}\quad\mathbb{E}[||\mathbf{x}_{T+1}-\mathbf{x}^{ \star}||^{2}]\leq\mathbb{E}_{i_{0}}\Bigg{[}\Big{(}1-\frac{\mu\rho^{3}\min^{2} \{\frac{1-\rho}{L},\gamma_{\max}\}\gamma_{0}||\nabla f_{i_{0}}(\mathbf{x}_{0} )||^{2}}{(c_{l}^{\text{scale}}||\mathbf{x}_{0}-\mathbf{x}^{\star}||)^{2}}\Big{)} ^{T}\Bigg{]}||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}\.\] (10)

The proof of Theorem 6 is presented in Appendix B.3. We now compare the above results with the other stepsizes. Under the same settings, DecSPS has a slower \(\mathcal{O}(1/\sqrt{T})\) rate due to the usage of \(\Theta(1/\sqrt{t})\) decay stepsize. While AdaGrad-Norm does have a linear acceleration phase when the accumulator grows large, to avoid an \(\mathcal{O}(1/\varepsilon)\) slow down, the parameters of AdaGrad-Norm have to satisfy \(c_{g}<b_{0}/L\), which requires the knowledge of Lipschitz constant [56]. Instead, the conditions on \(c_{p}\) and \(c_{l}\) for AdaSPS and AdaSLS only depend on the function value and gradient norm at \(\mathbf{x}_{0}\) which can be computed at the first iteration. SPS, SLS, and Vannilia-SGD with constant stepsize achieve faster linear convergence rate of order \(\mathcal{O}\big{(}\exp(-\frac{\mu}{L}T)\big{)}\). It is worth noting that Vannila-SGD can further remove the individual convexity assumption.

**Discussion.** In non-interpolation regimes, AdaSPS and AdaSLS only ensure a slower \(\mathcal{O}(1/\sqrt{T})\) convergence rate compared with \(\mathcal{O}(1/T)\) rate achieved by vanilla SGD with \(\Theta(1/t)\) decay stepsize when optimizing strongly-convex functions [7]. To our knowledge, no parameter-free adaptive stepsize exists that achieves such a fast rate under the same assumptions. Therefore, developing an adaptive algorithm that can adapt to both convex and strongly-convex functions would be a significant further contribution.

## 4 AdaSPS and AdaSLS with variance-reduction

Combining variance-reduction (VR) with adaptive Polyak-stepsize and line-search to achieve acceleration is a natural idea that has been explored in the last decade [49; 36]. However, it remains an open challenge as no theoretical guarantee has been proven yet. Indeed, Dubois-Taine et al. [14] provide a counter-example for intuitive line-search methods. In Appendix E we provide counter-examples of the classical SPS and its variants. The reason behind the failure is the biased curvature information provided by \(f_{i_{t}}\) that prevents global convergence. In this section, we introduce a novel framework to address this issue. Since there exists many variance-reduced stochastic gradient estimators, we focus on the classical SVRG estimator in this section, and our framework also applies to other estimators such as SARAH [40].

### Algorithm design: achieving variance-reduction without interpolation

It is known that adaptive methods such as SPS or SLS converge linearly on problems where the interpolation condition holds, i.e. \(f(\mathbf{x})\) with \(\sigma_{f,B}=0\).

For problems that do not satisfy the interpolation condition, our approach is to transition the problem to an equivalent one that satisfies the interpolation condition. One such transformation is to shift each individual function by the gradient of \(f_{i}(\mathbf{x})\) at \(\mathbf{x}^{\star}\), i.e. \(F_{i}(\mathbf{x})=f_{i}(\mathbf{x})-\mathbf{x}^{T}\nabla f_{i}(\mathbf{x}^{ \star})\). In this case \(f(\mathbf{x})\) can be written as \(f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}F_{i}(\mathbf{x})\) due to the fact that \(\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(\mathbf{x}^{\star})=0\). Note that \(\nabla F_{i}(\mathbf{x}^{\star})=\nabla f_{i}(\mathbf{x}^{\star})-\nabla f_{i} (\mathbf{x}^{\star})=0\) which implies that each \(F_{i}(\mathbf{x})\) shares the same minimizer and thus the interpolation condition is satisfied (\(\sigma_{f,1}^{2}=0\)). However, \(\nabla f_{i}(\mathbf{x}^{\star})\) is usually not available at hand. This motivates us to design the following algorithm.

```
0:\(\mathbf{x}_{0}\in\mathbb{R}^{d}\), \(\mu_{F}>0\), \(c_{p}>0\) or \(c_{l}>0\)
1: set \(\mathbf{w}_{0}=\mathbf{x}_{0}\), \(\eta_{-1}=+\infty\)
2:for\(t=0\) to \(T-1\)do
3: uniformly sample \(i_{t}\subseteq[n]\)
4: set \(F_{i_{t}}(\mathbf{x})=f_{i_{t}}(\mathbf{x})+\mathbf{x}^{T}(\nabla f(\mathbf{w }_{t})-\nabla f_{i_{t}}(\mathbf{w}_{t}))+\frac{\mu_{F}}{2}||\mathbf{x}-\mathbf{ x}_{t}||^{2}\)
5:\(\eta_{t}=\min\Bigl{\{}\frac{F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}^{\star}}{c_{p}|| \nabla F_{i_{t}}(\mathbf{x}_{t})||^{2}}\frac{1}{\sqrt{\sum_{s=0}^{t}F_{i_{s}}( \mathbf{x}_{s})-F_{i_{s}}^{\star}}},\eta_{t-1}\Bigr{\}}\) (AdaSVRS)
6:\(\eta_{t}=\min\Bigl{\{}\gamma_{t}\frac{1}{c_{l}\sqrt{\sum_{s=0}^{t}\gamma_{t}|| \nabla F_{i_{t}}(\mathbf{x}_{s})||^{2}}},\eta_{t-1}\Bigr{\}}\) (AdaSVRS)2
7:\(\mathbf{x}_{t+1}=\Pi_{\mathcal{X}}\bigl{(}\mathbf{x}_{t}-\eta_{t}\nabla F_{i_ {t}}(\mathbf{x}_{t})\bigr{)}\)
8:\(\mathbf{w}_{t+1}=\begin{cases}\mathbf{w}_{t}&\text{ with probability }1-p_{t+1}\\ \mathbf{x}_{t}&\text{ with probability }p_{t+1}\end{cases}\)
9:return\(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\) ```

**Algorithm 1** (Loopless) AdaSVRS and AdaSVRLS

### Algorithms and convergence

Inspired by the observation, we attempt to reduce the variance of the functions \(\sigma_{f,B}^{2}\) by constructing a sequence of random functions \(\{F_{i_{t}}(\mathbf{x})\}\) such that \(\sigma_{\frac{1}{n}\sum_{i=1}^{n}F_{i_{t}}(\mathbf{x}),B}^{2}\to 0\) as \(\mathbf{x}_{t}\to\mathbf{x}^{\star}\). However, directly applying SPS or SLS to \(\{F_{i_{t}}(\mathbf{x})\}\) still requires the knowledge of the Lipschitz constant to guarantee convergence. This problem can be solved by using our proposed AdaSPS and AdaSLS. The whole procedure of the final algorithm is summarized in Algorithm 1.

At each iteration of Algorithm 1, we construct a proxy function by adding two quantities to the minibatch function \(f_{i_{t}}(\mathbf{x})\), where \(\frac{\mu_{F}}{2}||\mathbf{x}-\mathbf{x}_{t}||^{2}\) is a proximal term that helps improve the inherent stochasticity due to the partial information obtained from \(f_{i_{t}}(\mathbf{x})\). The additional inner product quantity is used to draw closer the minimizers of \(f_{i_{t}}(\mathbf{x})\) and \(f(\mathbf{x})\). Following [27; 33], the full gradient is computed with a coin flip probability. Note that Algorithm 1 still works with \(\eta_{t}\) replaced with SVRG and AdaSVRG stepsize since \(\nabla F_{i_{t}}(\mathbf{x}_{t})=\nabla f_{i_{t}}(\mathbf{x}_{t})-\nabla f_{ i_{t}}(\mathbf{w}_{t})+\nabla f(\mathbf{w}_{t})\), and thus this framework can be seen as a generalization of the standard VR methods. A similar idea can also be found in the works on federated learning with variance-reduction [32; 24; 37; 50].

**Theorem 7**.: _Assume each \(f_{i}\) is convex and \(L\) smooth and \(\mathcal{X}\) is a convex compact feasible set with diameter \(D\). Let \(p_{t}=\frac{1}{a+1}\) with \(0\leq a<1\). Algorithm 1 converges as:_

\[\text{(AdaSVRPS)}\hskip 28.452756pt\mathbb{E}[f(\bar{\mathbf{x}}_{T})-f ^{\star}]\leq\frac{1+\frac{2L}{(1-a)\mu_{F}}}{T}\Bigl{(}2c_{p}(L+\mu_{F})D^{2} +\frac{1}{c_{p}}\Bigr{)}^{2}\,\] (11) \[\text{(AdaSVRLS)}\hskip 28.452756pt\mathbb{E}[f(\bar{\mathbf{x}}_{T})-f ^{\star}]\leq\frac{1+\frac{2L}{(1-a)\mu_{F}}}{T}\Bigl{(}\max\Bigl{\{}\frac{L+ \mu_{F}}{(1-\rho)\sqrt{\rho}},\frac{1}{\gamma_{\max}\sqrt{\rho}}\Bigr{\}}c_{l}D ^{2}+\frac{1}{c_{l}\sqrt{\rho}}\Bigr{)}^{2}\,\] (12)

_where \(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\)._

Suppose \(\gamma_{\max}\) is sufficiently large, then picking \(\mu_{F}^{\star}=\mathcal{O}(L)\), \(c_{p}^{\star}=\mathcal{O}(\frac{1}{\sqrt{LD^{2}}})\) and \(c_{l}^{\star}=\mathcal{O}(\frac{\sqrt{1-\rho}}{\sqrt{LD^{2}}})\) yields an \(\mathcal{O}(\frac{LD^{2}}{T})\) rate which matches the \(\mathcal{O}(\frac{L||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{T})\) rate of full-batch gradient descent except for a larger term \(D^{2}\) due to the lack of knowledge of the Lipschitz constant.

**Corollary 8**.: _Under the setting of Theorem 7, given an arbitrary accuracy \(\varepsilon\), the total number of gradient evaluations required to have \(\mathbb{E}[f(\bar{\mathbf{x}}_{T})-f^{\star}]\leq\varepsilon\) in expectation is \(\mathcal{O}(\log(1/\varepsilon)n+1/\varepsilon)\)._

The proved efficiency of stochastic gradient calls matches the optimal rates of SARAH [40]/SVRG and AdaSVRG [14] but removes the artificially designed inner and outer loop size. However, note that Algorithm 1 requires an additional assumption on individual convexity. Unfortunately, we believe this assumption is necessary for the VR methods to work for the algorithms in the Polyak and line-search family since SPS/SLS has to assume the same condition for the proof in the interpolation settings.

**Discussion.** The classical SVRG with Armijo line-search (presented as Algorithm 6 in [14]) employs the same gradient estimator as SVRG but chooses its stepsize based on the returning value of line-search on the individual function \(f_{i}\). Similarly, SVRG with classical Polyak stepsize uses the individual curvature information of \(f_{i}\) to set the stepsize for the global variance-reduced gradient. Due to the misleading curvature information provided by the biased function \(f_{i}\), both methods have convergence issues. In constrast, Algorithm 1 reduces the bias by adding a correction term \(\mathbf{x}^{T}(\nabla f(\mathbf{w}_{t})-\nabla f_{i_{t}}(\mathbf{w}_{t}))\) with global information to \(f_{i}\) and then applying line-search or Polyak-stepsize on the variance-reduced functions \(F_{i_{t}}\). This difference essentially guarantees the convergence.

## 5 Numerical evaluation

In this section, we illustrate the main properties of our proposed methods in numerical experiments. A detailed description of the experimental setup can be found in Appendix F. We report the theoretically justified hyperparameter \(c_{p}^{\text{scale}}\) or \(c_{l}^{\text{scale}}\) as defined in Theorem 6 rather than \(c_{p}\) or \(c_{l}\) in the following.

**Synthetic data.** We illustrate the robustness property on a class of synthetic problems. We consider the minimization of a quadratic of the form: \(f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})\) where \(f_{i}(\mathbf{x})=\frac{1}{2}(\mathbf{x}-\mathbf{b}_{i})^{T}A_{i}(\mathbf{x}- \mathbf{b}_{i})\), \(\mathbf{b}_{i}\in\mathbb{R}^{d}\) and \(A_{i}\in\mathbb{R}^{d\times d}\) is a diagonal matrix. We use \(n=50\), \(d=1000\). We can control the convexity of the problem by choosing different matrices \(A_{i}\), and control interpolation by either setting all \(\{\mathbf{b}_{i}\}\) to be identical or different. We generate a strongly convex instance where the eigenvalues of \(\nabla^{2}f(\mathbf{x})\) are between \(1\) and \(10\), and a general convex instance by setting some of the eigenvalues to small values close to zero (while ensuring that each \(\nabla^{2}f_{i}(\mathbf{x})\) is positive semi-definite). The exact procedure to generate these problems is described in Appendix F.

For all methods, we use a batch size \(B=1\). We compare AdaSPS and AdaSLS against DecSPS [44], SPS [34] and SLS [52] to illustrate the robustness property. More comparisons can be found in Appendix F.1. We fix \(c_{p}^{\text{scale}}=c_{l}^{\text{scale}}=1\) for AdaSPS/AdaSLS and use the optimal parameters for DecSPS and SPS. In Figure 1, we observe that SPS does not converge in the non-interpolated settings and DecSPS suffers from a slow \(\mathcal{O}(1/\sqrt{T})\) convergence on the two interpolated problems. AdaSPS and AdaSLS show the desired convergence rates across all cases which matches the theory. When the problems are non-interpolated, AdaSVRPS and AdaSVRLS illustrate faster convergence, which can be seen in Figure 2.

**Binary classification on LIBSVM datasets.** We experiment with binary classification on four diverse datasets from [10]. We consider the standard regularized logistic loss: \(f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}\log(1+\exp(-y_{i}\cdot\mathbf{a}_{i}^{ T}\mathbf{x}))+\frac{1}{2n}||\mathbf{x}||^{2}\) where \((\mathbf{a}_{i},y_{i})\in\mathbb{R}^{d+1}\) are features and labels. We defer the study of variance-reduction methods to Appendix F.2 for clarity of presentation. We benchmark against popular optimization algorithms including Adam [26], SPS [34], DecSPS [44], SLS [52] and AdaGradNorm [15]. We fix \(c_{p}^{\text{scale}}=c_{l}^{\text{scale}}=1\) for AdaSPS/AdaSLS and pick the best learning rate from \(\{10^{i}\}_{i=-4,..,3}\) for SGD, Adam and AdaGrad-Norm. We observe that Adam, SPS and SLS have remarkable performances on duke with \(n=48\) and \(d=7129\), which satisfies interpolation. AdaSPS

Figure 2: Illustration of the accelerated convergence of AdaSVRPS and AdaSVRLS on quadratic loss without interpolation. Both algorithms require less gradient evaluations than AdaSPS and AdaSLS for optimizing non-interpolated problems. However, they are less efficient for solving interpolated problems. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)

and AdaSLS consistently perform reasonably well on the other three larger datasets. It is worth noting that the hyper-parameters \(c_{p}^{\text{scale}}\) and \(c_{l}^{\text{scale}}\) are fixed for all the datasets, which is desired in practice.

**Deep learning task.** We provide a heuristic extension of AdaSPS to over-parameterized non-convex optimization tasks to illustrate its potential. We benchmark the convergence and generalization performance of AdaSPS (DL) 5 for the multi-class classification tasks on CIFAR10 [28] and CIFAR100 [29] datasets using ResNet-34 [21]. More experimental details can be found in Appendix G. We demonstrate the effectiveness of AdaSPS (DL) in Figure 4.

**Discussion.** AdaSPS and AdaSLS consistently demonstrate robust convergence across all tasks and achieve performance on par with, if not better than, the best-tuned algorithms. Consequently, it is reliable and convenient for practical use.

## 6 Conclusion and future work

We proposed new variants of SPS and SLS algorithms and demonstrated their robust and fast convergence in both interpolated and non-interpolated settings. We further accelerate both algorithms for convex optimization with a novel variance reduction technique. Interesting future directions may include: accelerating AdaSPS and AdaSLS with momentum, developing effective robust adaptive methods for training deep neural networks, designing an adaptive algorithm that gives a faster rate \(\mathcal{O}(1/T)\) under strong convexity, extensions to distributed and decentralized settings.

## Acknowledgments

We appreciate the fruitful discussion with Anton Rodomanov.

Figure 4: Comparison of the considered optimizers on multi-class classification tasks with CIFAR10 and CIFAR100 datasets using ResNet34 with softmax loss.

Figure 3: Comparison of AdaSPS/AdaSLS against six other popular optimizers on four LIBSVM datasets, with batch size \(B=1\) for duke, \(B=64\) for rcv1, \(B=64\) for jcnn and \(B=128\) for w8a. AdaSPS and AdaSLS have competitive performance on rcv1, ijcnn, and w8a while SPS, SLS, and Adam converge fast on duke. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)

## References

* Abdukhakimov et al. [2023] Farashed Abdukhakimov, Chulu Xiang, Dmitry Kamzolov, and Martin Takac. Stochastic gradient descent with preconditioned polyak step-size. _arXiv preprint arXiv:2310.02093_, 2023.
* Acar et al. [2021] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. _arXiv preprint arXiv:2111.04263_, 2021.
* Armijo [1966] Larry Armijo. Minimization of functions having lipschitz continuous first partial derivatives. _Pacific Journal of Mathematics_, 16(1):1-3, 1 1966.
* Asi and Duchi [2019] Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity. _SIAM Journal on Optimization_, 29(3):2257-2290, 2019.
* Asi et al. [2021] Hilal Asi, Daniel Levy, and John C Duchi. Adapting to function difficulty and growth conditions in private optimization. _Advances in Neural Information Processing Systems_, 34:19069-19081, 2021.
* Berrada et al. [2020] Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Training neural networks for and by interpolation. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20. JMLR.org, 2020.
* Bottou et al. [2018] Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM Review_, 60(2):223-311, 2018. doi: 10.1137/16M1080173. URL https://doi.org/10.1137/16M1080173.
* Bubeck [2015] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _Found. Trends Mach. Learn._, 8(3-4):231-357, nov 2015. ISSN 1935-8237. doi: 10.1561/2200000050. URL https://doi.org/10.1561/2200000050.
* Chadha et al. [2022] Karan Chadha, Gary Cheng, and John Duchi. Accelerated, optimal and parallel: Some results on model-based stochastic optimization. In _International Conference on Machine Learning_, pages 2811-2827. PMLR, 2022.
* Chang and Lin [2011] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. _ACM Trans. Intell. Syst. Technol._, 2(3), may 2011. ISSN 2157-6904. doi: 10.1145/1961189.1961199. URL https://doi.org/10.1145/1961189.1961199.
* Cheng and Duchi [2022] Gary Cheng and John Duchi. adastar: A method for adapting to interpolation. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022. URL https://openreview.net/forum?id=dMbczvwk6Jw.
* Cutkosky and Boahen [2016] Ashok Cutkosky and Kwabena Boahen. Online convex optimization with unconstrained domains and losses. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS'16, page 748-756, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.
* Defazio et al. [2014] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf.
* Dubois-Taine et al. [2022] Benjamin Dubois-Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Simon Lacoste-Julien. SVRG meets adagrad: painless variance reduction. _Mach. Learn._, 111(12):4359-4409, 2022. doi: 10.1007/s10994-022-06265-x. URL https://doi.org/10.1007/s10994-022-06265-x.
* Duchi et al. [2011] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _J. Mach. Learn. Res._, 12(null):2121-2159, jul 2011. ISSN 1532-4435.
* Galli et al. [2023] Leonardo Galli, Holger Rauhut, and Mark Schmidt. Don't be so monotone: Relaxing stochastic line search in over-parameterized models, 2023.
* Gower et al. [2021] Robert M. Gower, Aaron Defazio, and Michael G. Rabbat. Stochastic polyak stepsize with a moving target. _CoRR_, abs/2106.11851, 2021. URL https://arxiv.org/abs/2106.11851.
* Gower et al. [2022] Robert M Gower, Mathieu Blondel, Nidham Gazagnadou, and Fabian Pedregosa. Cutting some slack for sgd with adaptive polyak stepsizes. _arXiv preprint arXiv:2202.12328_, 2022.

* Hastie et al. [2009] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. _The elements of statistical learning: data mining, inference and prediction_. Springer, 2 edition, 2009. URL http://www-stat.stanford.edu/~tibs/ElemStatLearn/.
* Hazan and Kakade [2019] Elad Hazan and Sham Kakade. Revisiting the polyak step size, 2019. URL https://arxiv.org/abs/1905.00313.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.
* Johnson and Zhang [2013] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf.
* Kairouz et al. [2013] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveria, Hubert Eichner, Salim El Rouayheo, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badh Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. _Found. Trends Mach. Learn._, 14(1-2):1-210, jun 2021. ISSN 1935-8237. doi: 10.1561/2200000083. URL https://doi.org/10.1561/2200000083.
* Karimireddy et al. [2020] Sai Parneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5132-5143. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/karimireddy20a.html.
* Kavis et al. [2022] Ali Kavis, Stratis Skoulakis, Kimon Antonakopoulos, Leello Tadesse Dadi, and Volkan Cevher. Adaptive stochastic variance reduction for non-convex finite-sum minimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 23524-23538. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/94f625ddec313cd432d65f96fcc51c8-Paper-Conference.pdf.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1412.6980.
* Kovalev et al. [2020] Dmitry Kovalev, Samuel Horvath, and Peter Richtarik. Don't jump through hoops and remove those loops: Svrg and katyusha are better without the outer loop. In Aryeh Kontorovich and Gergely Neu, editors, _Proceedings of the 31st International Conference on Algorithmic Learning Theory_, volume 117 of _Proceedings of Machine Learning Research_, pages 451-467. PMLR, 08 Feb-11 Feb 2020. URL https://proceedings.mlr.press/v117/kovalev20a.html.
* Krizhevsky et al. [2020] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).. URL http://www.cs.toronto.edu/~kriz/cifar.html.
* Krizhevsky et al. [2021] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research).. URL http://www.cs.toronto.edu/~kriz/cifar.html.
* Kunstner et al. [2023] Frederik Kunstner, Victor S. Portella, Mark Schmidt, and Nick Harvey. Searching for optimal per-coordinate step-sizes with multidimensional backtracking, 2023.
* Li et al. [2022] Shuang Li, William J Swartworth, Martin Takac, Deanna Needell, and Robert M Gower. Sp2: A second order stochastic polyak method. _arXiv preprint arXiv:2207.08171_, 2022.

* Li et al. [2020] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors, _Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020_. mlsys.org, 2020. URL https://proceedings.mlsys.org/book/316.pdf.
* Li et al. [2021] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 6286-6295. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/l121a.html.
* Loizou et al. [2021] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak stepsize for SGD: an adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji Fukumizu, editors, _The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event_, volume 130 of _Proceedings of Machine Learning Research_, pages 1306-1314. PMLR, 2021. URL http://proceedings.mlr.press/v130/loizou21a.html.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Ska985cxx.
* Mairal [2013] Julien Mairal. Optimization with first-order surrogate functions. In _International Conference on Machine Learning_, pages 783-791. PMLR, 2013.
* Mishchenko et al. [2022] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In _International Conference on Machine Learning_, pages 15750-15769. PMLR, 2022.
* Nedic and Bertsekas [2001] Angelia Nedic and Dimitri Bertsekas. _Convergence Rate of Incremental Subgradient Algorithms_, pages 223-264. Springer US, Boston, MA, 2001. ISBN 978-1-4757-6594-6. doi: 10.1007/978-1-4757-6594-6_11. URL https://doi.org/10.1007/978-1-4757-6594-6_11.
* Nesterov [2014] Yurii Nesterov. _Introductory Lectures on Convex Optimization: A Basic Course_. Springer Publishing Company, Incorporated, 1 edition, 2014. ISBN 1461346916.
* Nguyen et al. [2017] Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 2613-2621. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/ngvsen17b.html.
* Nocedal and Wright [2006] Jorge Nocedal and Stephen J. Wright. _Numerical Optimization_. Springer, New York, NY, USA, 2e edition, 2006.
* Oberman and Prazere [2019] Adam M Oberman and Mariana Prazere. Stochastic gradient descent with polyak's learning rate. _arXiv preprint arXiv:1903.08688_, 2019.
* Orabona and Pal [2015] Francesco Orabona and David Pal. Scale-free algorithms for online linear optimization. In Kamalika Chaudhuri, CLAUDIO GENTILE, and Sandra Zilles, editors, _Algorithmic Learning Theory_, pages 287-301, Cham, 2015. Springer International Publishing. ISBN 978-3-319-24486-0.
* Orvieto et al. [2022] Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 26943-26954. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf.
* Polyak [1987] B. T. Polyak. _Introduction to optimization_. Translations series in mathematics and engineering. Optimization Software, Publications Division, New York, 1987. ISBN 0911575146; 9780911575149.
* 407, 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/aoms/1177729586.
* Rolinek and Martius [2018] Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning. In _Advances in Neural Information Processing Systems 31 (NeurIPS 2018)_, pages 6434-6444. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/7879-14-practical-loss-based-stepsize-adaptation-for-deep-learning.pdf.

* Schmidt and Le Roux [2013] Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong growth condition. _arXiv preprint arXiv:1308.6370_, 2013.
* Schmidt et al. [2017] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162:83-112, 2017.
* Shamir et al. [2014] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In _International conference on machine learning_, pages 1000-1008. PMLR, 2014.
* Streeter and McMahan [2010] Matthew J. Streeter and H. Brendan McMahan. Less regret via online conditioning. _CoRR_, abs/1002.4862, 2010. URL http://arxiv.org/abs/1002.4862.
* Vaswani et al. [2019] Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3727-3740, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/2557911c1bf75c2b643afb4ecbf6ec2-Abstract.html.
* Vaswani et al. [2020] Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien. Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search). _arXiv preprint arXiv:2006.06835_, 2020.
* Wang et al. [2023] Xiaoyu Wang, Mikael Johansson, and Tong Zhang. Generalized polyak step size for first order optimization with momentum. _arXiv preprint arXiv:2305.12939_, 2023.
* Ward et al. [2019] Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex landscapes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 6677-6686. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/ward19a.html.
* Xie et al. [2020] Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient descent. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 1475-1485. PMLR, 26-28 Aug 2020. URL https://proceedings.mlr.press/v108/xie20a.html.

## Appendix A Technical Preliminaries

### Basic Definitions

We use the following definitions throughout the paper.

**Definition 1** (convexity).: _A differentiable function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is convex if \(\forall\ \mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\),_

\[f(\mathbf{y})\geq f(\mathbf{x})+\langle\nabla f(\mathbf{x}),\mathbf{y}- \mathbf{x}\rangle\.\] (A.1)

**Definition 2** (strong convexity).: _A differentiable function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is \(\mu\)-strongly convex if \(\forall\ \mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\),_

\[f(\mathbf{y})\geq f(\mathbf{x})+\langle\nabla f(\mathbf{x}),\mathbf{y}- \mathbf{x}\rangle+\frac{\mu}{2}||\mathbf{x}-\mathbf{y}||^{2}\.\] (A.2)

**Definition 3** (\(L\)-smooth).: _Let function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) be differentiable. \(f\) is smooth if there exists \(L>0\) such that \(\forall\ \mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\),_

\[||\nabla f(\mathbf{x})-\nabla f(\mathbf{y})||\leq L||\mathbf{x}-\mathbf{y}||\.\] (A.3)

### Useful Lemmas

We frequently use the following helpful lemmas for the proof.

**Lemma 9** (Nesterov [39], Lemma 1.2.3).: _Definition 3 implies that there exists a quadratic upper bound on \(f\):_

\[f(\mathbf{y})\leq f(\mathbf{x})+\langle\nabla f(\mathbf{x}),\mathbf{y}- \mathbf{x}\rangle\,|+\frac{L}{2}||\mathbf{y}-\mathbf{x}||^{2}\,\forall\mathbf{x},\mathbf{y}\in \mathbb{R}^{d}\.\] (A.4)

**Lemma 10** (Nesterov [39], Theorem 2.1.5).: _If a convex function \(f\) satisfies (A.4), then it holds that:_

\[f(\mathbf{y})\geq f(\mathbf{x})+\langle\nabla f(\mathbf{x}),\mathbf{y}- \mathbf{x}\rangle\,|+\frac{1}{2L}||\nabla f(\mathbf{y})-\nabla f(\mathbf{x})|| ^{2}\,\forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}.\] (A.5)

**Lemma 11** (Ward et al. [55]).: _For any non-negative sequence \(a_{0},...,a_{T}\), the following holds:_

\[\sqrt{\sum_{t=0}^{T}a_{t}}\leq\sum_{t=0}^{T}\frac{a_{t}}{\sqrt{\sum_{i=0}^{t} a_{i}}}\leq 2\sqrt{\sum_{t=0}^{T}a_{t}}\.\] (A.6)

_If \(a_{0}\geq 1\), then the following holds:_

\[\sum_{t=0}^{T}\frac{a_{t}}{\sum_{i=0}^{t}a_{i}}\leq\log(\sum_{t=0}^{T}a_{t})+1\.\] (A.7)

Proof.: To show equation (A.6), we proceed with the proof by induction. For \(t=0\), (A.6) holds trivially since \(\sqrt{a_{0}}\leq\sqrt{a_{0}}\leq 2\sqrt{a_{0}}\). Assume equation (A.6) holds for \(T-1\). For RHS, we have:

\[\begin{split}\sum_{t=0}^{T-1}\frac{a_{t}}{\sum_{i=0}^{t}a_{i}}+ \frac{a_{T}}{\sqrt{\sum_{i=0}^{T}a_{i}}}&\leq 2\sqrt{\sum_{t=0}^{T-1}a_{t}}+ \frac{a_{T}}{\sqrt{\sum_{i=0}^{T}a_{i}}}\\ &=2\sqrt{\sum_{t=0}^{T}a_{t}-a_{T}}+\frac{a_{T}}{\sqrt{\sum_{t=0} ^{T}a_{t}}}\\ &\leq 2\sqrt{\sum_{t=0}^{T}a_{t}}\.\end{split}\] (A.8)where the last inequality is due to the fact that \(2\sqrt{x-y}+\frac{y}{\sqrt{x}}\leq 2\sqrt{x}\) for any \(x\geq y\geq 0\). For LHS, we have:

\[\sum_{t=0}^{T-1}\frac{a_{t}}{\sum_{i=0}^{t}a_{i}}+\frac{a_{T}}{ \sqrt{\sum_{i=0}^{T}a_{i}}} \geq\sqrt{\sum_{t=0}^{T-1}a_{t}}+\frac{a_{T}}{\sqrt{\sum_{i=0}^{T }a_{i}}}\] (A.9) \[=\sqrt{\sum_{t=0}^{T}a_{t}-a_{T}}+\frac{a_{T}}{\sqrt{\sum_{t=0}^{ T}a_{t}}}\] \[\geq\sqrt{\sum_{t=0}^{T}a_{t}}\.\]

where the last inequality is due to the fact that \(\sqrt{x-y}+\frac{y}{\sqrt{x}}\geq\sqrt{x}\) for any \(x\geq y\geq 0\).

We next show equation (A.7) by induction. For \(t=0\), equation (A.7) trivially holds since \(1\leq\log(a_{0})+1\). Assume (A.7) holds for \(T-1\), we have:

\[\sum_{t=0}^{T}\frac{a_{t}}{\sum_{i=0}^{t}a_{i}} \leq\log(\sum_{t=0}^{T-1}a_{t})+1+\frac{a_{T}}{\sum_{i=0}^{T}a_{i}}\] (A.10) \[\leq\log(\sum_{t=0}^{T}a_{t})+1\.\]

where the last inequality is due to the fact that \(\log(x-y)+\frac{y}{x}\leq\log(x)\) for any \(x\geq y\geq 0\) since \(e^{\frac{y}{x}}\leq\frac{1+\frac{y}{x}}{1-\frac{y}{x^{2}}}\). 

**Lemma 12** (Dubois-Taine et al. [14, Lemma 5]).: _If \(x^{2}\leq a(x+b)\) for \(a\geq 0\) and \(b\geq 0\), then it holds that:_

\[x\leq a+\sqrt{ab}\.\] (A.11)

The following Lemma is an extension of Lemma 5 in [44].

**Lemma 13**.: _Let \(z_{t+1}\leq(1-a\eta_{t})z_{t}+\eta_{t}b\) and \(z_{t}\geq 0\) where \(a>0\), \(b>0\) and \(\eta_{t}>0,\eta_{t+1}\leq\eta_{t},\ \forall t\geq 0\). It holds that:_

\[z_{t}\leq\max\{\frac{b}{a},z_{0},\eta_{0}b\},\ \forall t\geq 0\.\] (A.12)

Proof.: Since \(\eta_{t}\) is non-increasing, \(1-a\eta_{t}\leq 0\) is non-decreasing. For any \(t\geq 0\) such that \(1-a\eta_{t}\leq 0\), we have \(z_{t+1}\leq\eta_{t}b\leq\eta_{0}b\). If \(1-a\eta_{t}\leq 0\) for all \(t\geq 0\), then the proof is done. Otherwise, let us assume there exists a first index \(j\) such that \(1-a\eta_{j}>0\) and we have \(z_{j}\leq\max\{z_{0},\eta_{0}b\}:=\tilde{z}_{0}\). We proceed with the proof starting with the index \(j\) by induction. For \(t=j\), the lemma trivially holds. Let us assume \(z_{t}\leq\max\{\frac{b}{a},\tilde{z}_{0}\}\) for \(t>j\). If \(\frac{b}{a}\geq\tilde{z}_{0}\), then by induction, we have:

\[z_{t+1}\leq(1-a\eta_{t})\frac{b}{a}+\eta_{t}b=\frac{b}{a}\.\] (A.13)

If instead \(\frac{b}{a}\leq\tilde{z}_{0}\), then by induction, we have:

\[z_{t+1}\leq(1-a\eta_{t})\tilde{z}_{0}+\eta_{t}b=\tilde{z}_{0}-\eta_{t}(a\tilde {z}_{0}-b)\leq\tilde{z}_{0}\.\] (A.14)

Combining the above cases concludes the proof. 

The following lemma is commonly used in the works on Polyak stepsize [34, 20].

**Lemma 14**.: _Suppose a function \(f\) is \(L\)-smooth and \(\mu\)-strongly convex, then the following holds:_

\[\frac{1}{2L}\leq\frac{f(\mathbf{x})-f^{\star}}{||\nabla f(\mathbf{x})||^{2}} \leq\frac{1}{2\mu}\.\] (A.15)

The following lemma provides upper and lower bounds for the stepsize of AdaSPS.

**Lemma 15**.: _Suppose each \(f_{i}\) is \(L\)-smooth, then the stepsize of AdaSPS (AdaSPS) satisfies:_

\[\frac{1}{2c_{p}L}\frac{1}{\sqrt{\sum_{s=0}^{t}f_{i_{s}}(\mathbf{x}_{s})-\ell_{ i_{s}}^{\star}}}\leq\eta_{t}\leq\frac{f_{i_{t}}(\mathbf{x}_{t})-\ell_{t_{s}}^{ \star}}{c_{p}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}}\frac{1}{\sqrt{\sum_{s=0 }^{t}f_{i_{s}}(\mathbf{x}_{s})-\ell_{i_{s}}^{\star}}}\.\] (A.16)Proof.: The upper bound follows from the definition of the stepsize (AdaSPS). To prove the lower bound, we note that the stepsize (AdaSPS) is composed of two parts where the first component \(\frac{f_{i_{1}}(\mathbf{x}_{t})-f_{i_{2}}^{\epsilon}}{c_{\mathrm{d}}\|\nabla f_{ i_{1}}(\mathbf{x}_{t})\|^{2}}\geq\frac{1}{2c_{\mathrm{p}}L}\) for all \(0\leq s\leq t\) due to (A.15), and the second component is always decreasing. Finally, recall that \(\eta_{-1}=+\infty\) and thus the proof is completed. 

The following lemma provides upper and lower bounds for the stepsize of AdaSLS. We refer to Appendix D for details of the line-search procedure.

**Lemma 16**.: _Suppose each \(f_{i}\) is \(L\)-smooth, then the stepsize of AdaSLS (AdaSLS) satisfies:_

\[\min\Bigl{\{}\frac{1-\rho}{L},\gamma_{\max}\Bigr{\}}\frac{1}{c_{\mathrm{d}} \sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}}\leq \eta_{t}\leq\frac{\gamma_{t}}{c_{\mathrm{d}}\sqrt{\sum_{s=0}^{t}\gamma_{s}|| \nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}}\.\] (A.17)

Proof.: The upper bound is due to the definition of the stepsize (AdaSLS). We next prove the lower bound. From the smoothness definition, the following holds for all \(\gamma_{t}\):

\[f_{i_{t}}(\mathbf{x}_{t}-\gamma_{t}\nabla f_{i_{t}}(\mathbf{x}_{t}))\stackrel{{ \eqref{eq:AdaSLS}}}{{\leq}}f_{i_{t}}(\mathbf{x}_{t})-\gamma_{t}|| \nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}+\frac{L}{2}\gamma_{t}^{2}||\nabla f_{ i_{t}}(\mathbf{x}_{t})||^{2}\.\] (A.18)

For any \(0<\gamma_{t}\leq\frac{2(1-\rho)}{L}\), we have:

\[f_{i_{t}}(\mathbf{x}_{t}-\gamma_{t}\nabla f_{i_{t}}(\mathbf{x}_{t}))\leq f_{ i_{t}}(\mathbf{x}_{t})-\rho\gamma_{t}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\,\] (A.19)

which satisfies the line-search condition (4). From the procedure of Backtracking line-search (Alg. 4), if \(\gamma_{\max}\leq\frac{1-\rho}{L}\), then \(\gamma_{t}=\gamma_{\max}\) is accepted. Otherwise, since we require the decreasing factor \(\beta\)**to be no smaller than \(\frac{1}{2}\)** in Algorithm 4, we must have \(\gamma_{t}\geq\frac{2(1-\rho)}{2L}\). Therefore, \(\gamma_{t}\) is always lower bounded by \(\min\{\frac{1-\rho}{L},\gamma_{\max}\}\). The second component of AdaSLS is always decreasing, and recall that \(\eta_{-1}=+\infty\). The proof is thus completed. 

## Appendix B Proofs of main results

### Proof of Theorem 1

Proof.: We follow a common proof routine for the general convex optimization [15, 14, 44]. Using the update rule of PSGD (5), we have:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2} =||\Pi_{\mathcal{X}}(\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}( \mathbf{x}_{t}))-\Pi_{\mathcal{X}}(\mathbf{x}^{\star})||^{2}\] (B.1) \[\leq||\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}(\mathbf{x}_{t})- \mathbf{x}^{\star}||^{2}\] \[=||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}-2\eta_{t}\langle \nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle+ \eta_{t}^{2}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\.\]

Dividing by \(2\eta_{t}\) and rearranging gives:

\[\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x }^{\star}\rangle\] (B.2) \[\leq\frac{||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}}{2\eta_{t}}- \frac{||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t}}+\frac{\eta_{t}}{2 }||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\] \[=\frac{||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}}{2\eta_{t}}- \frac{||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t+1}}+\frac{|| \mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t+1}}-\frac{||\mathbf{x}_{t +1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t}}+\frac{\eta_{t}}{2}||\nabla f_{i_{t}}( \mathbf{x}_{t})||^{2}\.\]

Summing from \(t=0\) to \(t=T-1\), we get:

\[\sum_{t=0}^{T-1}\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x }_{t}-\mathbf{x}^{\star}\rangle\] \[\leq\sum_{t=0}^{T-1}\frac{||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{ 2}}{2\eta_{t}}-\frac{||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t+1}}+ \frac{||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t+1}}-\frac{|| \mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}}{2\eta_{t}}+\frac{\eta_{t}}{2}|| \nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\] \[\leq\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{2\eta_{0}}- \frac{||\mathbf{x}_{T}-\mathbf{x}^{\star}||^{2}}{2\eta_{T}}+\frac{||\mathbf{x}_{ T}-\mathbf{x}^{\star}||^{2}}{2\eta_{T}}-\frac{||\mathbf{x}_{T}-\mathbf{x}^{\star}||^{2}}{2 \eta_{T-1}}+\sum_{t=0}^{T-2}(\frac{1}{2\eta_{t+1}}-\frac{1}{2\eta_{t}})D^{2}+ \sum_{t=0}^{T-1}\frac{\eta_{t}}{2}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\] \[\leq\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{2\eta_{0}}- \frac{||\mathbf{x}_{T}-\mathbf{x}^{\star}||^{2}}{2\eta_{T}}+\frac{||\mathbf{x}_{ T}-\mathbf{x}^{\star}||^{2}}{2\eta_{T}}+\frac{D^{2}}{2\eta_{T-1}}+\sum_{t=0}^{T-1} \frac{\eta_{t}}{2}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\]\[=\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{2\eta_{0}}+\frac{D^{2}}{2 \eta_{T-1}}+\sum_{t=0}^{T-1}\frac{\eta_{t}}{2c_{p}\sqrt{\sum_{s=0}^{t}f_{i_{s}}( \mathbf{x}_{s})-\ell_{i_{s}}^{\star}}}\]

\[=\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{2\eta_{0}}+\frac{D^{2}}{2 \eta_{T-1}}+\sum_{t=0}^{T-1}\frac{\eta_{t}}{2}||\nabla f_{i_{t}}(\mathbf{x}_{ t})||^{2}\,\] (B.3)

where in the second inequality, we use the decreasing property of the stepsize \(\eta_{t}\) which guarantees \(\frac{1}{2\eta_{t}}-\frac{1}{2\eta_{t-1}}\geq 0\), and we use the fact that \(||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}\leq D^{2}\) because of the projection step in (5). For clarity, we next separate the proof for AdaSPS and AdaSLS.

**AdaSPS:** We upper bound the last two terms by using Lemma 15 and we obtain:

\[\sum_{t=0}^{T-1}\frac{\eta_{t}}{2}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2} \stackrel{{\eqref{eq:def_def_def_def_def_def_def_def_def_def_def_def _def_def_def_def}}}{{\leq}}\sum_{t=0}^{T-1}\frac{f_{i_{t}}(\mathbf{x}_{t})-\ell _{i_{t}}^{\star}}{2c_{p}\sqrt{\sum_{s=0}^{t}f_{i_{s}}(\mathbf{x}_{s})-\ell_{ i_{s}}^{\star}}}\stackrel{{\eqref{eq:def_def_def_def_def_def _def}}}{{\leq}}\frac{1}{c_{p}}\sqrt{\sum_{s=0}^{T-1}f_{i_{s}}(\mathbf{x}_{s})- \ell_{i_{s}}^{\star}}\,\] (B.4)

and

\[\frac{D^{2}}{2\eta_{T-1}}\stackrel{{\eqref{eq:def_def_def_def_def _def_def_def_def_def_def_def_def_def_def_def_def_def_def}}}{{\leq}}c_{p}LD^{2}\sqrt{ \sum_{s=0}^{T-1}f_{i_{s}}(\mathbf{x}_{s})-\ell_{i_{s}}^{\star}}\.\] (B.5)

Using \(\frac{||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}{2\eta_{0}}\leq\frac{D^{2}}{2 \eta_{T-1}}\) and plugging (B.4) and (B.5) back to (B.3) gives:

\[\sum_{t=0}^{T-1}\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}- \mathbf{x}^{\star}\rangle\leq(2c_{p}LD^{2}+\frac{1}{c_{p}})\sqrt{\sum_{s=0}^{T -1}f_{i_{s}}(\mathbf{x}_{s})-\ell_{i_{s}}^{\star}}\.\] (B.6)

Taking the expectation on both sides, we have:

\[\sum_{t=0}^{T-1}\mathbb{E}[\langle\nabla f(\mathbf{x}_{t}),\mathbf{x}_{t}- \mathbf{x}^{\star}\rangle] \leq(2c_{p}LD^{2}+\frac{1}{c_{p}})\,\mathbb{E}\Big{[}\sqrt{\sum_ {s=0}^{T-1}f_{i_{s}}(\mathbf{x}_{s})-\ell_{i_{s}}^{\star}}\Big{]}\] (B.7) \[=(2c_{p}LD^{2}+\frac{1}{c_{p}})\,\mathbb{E}\Big{[}\sqrt{\sum_{s=0} ^{T-1}f_{i_{s}}(\mathbf{x}_{s})-f_{i_{s}}(\mathbf{x}^{\star})+f_{i_{s}}( \mathbf{x}^{\star})-\ell_{i_{s}}^{\star}}\Big{]}\.\]

Using the convexity assumption of \(f\) and applying Jensen's inequality to the square root function, we get:

\[\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]\leq(2c_{p}LD^{2}+ \frac{1}{c_{p}})\sqrt{\sum_{s=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{s})-f^{\star}] +\sigma_{f,B}^{2}+\mathrm{err}_{f,B}^{2}}\,\] (B.8)

where \(\mathrm{err}_{f,B}^{2}=\mathbb{E}_{i_{s}}[f_{i_{s}}^{\star}-\ell_{i_{s}}^{ \star}]\). Let \(\tau:=2c_{p}LD^{2}+\frac{1}{c_{p}}\). Taking the square gives:

\[(\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}])^{2}\leq\tau^{2} \Big{(}\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]+T(\sigma_{f,B}^ {2}+\mathrm{err}_{f,B}^{2})\Big{)}\.\] (B.9)

We next apply Lemma 12 with \(x=\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f(\mathbf{x}^{\star})]\), \(a=\tau^{2}\) and \(b=T(\sigma_{f,B}^{2}+\mathrm{err}_{f,B}^{2})\):

\[\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]\leq\tau^{2}+\tau\sqrt{ \sigma_{f,B}^{2}+\mathrm{err}_{f,B}^{2}}\sqrt{T}\.\] (B.10)

We conclude by dividing both sides by \(T\) and using Jensen's inequality:

\[\mathbb{E}[f(\tilde{\mathbf{x}}_{T})-f^{\star}]\leq\frac{\sum_{t=0}^{T-1} \mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]}{T}\leq\frac{\tau^{2}}{T}+\frac{\tau \sqrt{\sigma_{f,B}^{2}+\mathrm{err}_{f,B}^{2}}}{\sqrt{T}}\.\] (B.11)

where \(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\).

**AdaSLS:** The proof is almost the same as AdaSPS. We omit procedures with the same proof reasons for simplicity. We first use Lemma 16 to obtain:

\[\sum_{t=0}^{T-1}\frac{\eta_{t}}{2}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2} \stackrel{{\eqref{eq:def_def_def_def_def_def_def_defdef_def_def_def def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_def_ _def_def_def_def_def_def_def_def_def__def_def_def__def_def_ _def_def_def_def_def_def_ _def_def_ _def_ _def_ _def_ _def_ _def_ _ _def_ _def_ _def_ _def_ _ _def_ _def_ _def_ _def_ _ _def_ _def_ _def_ _def_ _ _def_ _def_ _def_ _ _def_ _def_ _def_ _ _def_ _def_ _def_ _ _def_ _def_ _ _def_ _def_ _ _def_ _def_ _def_ _ _def_ _ _def_ _def_ _ _def_ _def_ _ _def_ _ _def_ _def_ _ _def_ _ _def_ _def_ _ _def_ _ _def_ _ _def_ _ _def_ _def_ _def_ _ _def_ _def_ _ _def_ _ _def_ _ _def_ _def_ _ _def_ _def_ _ _def_ _ _def_

[MISSING_PAGE_FAIL:19]

We conclude by applying Lemma 13 with \(z_{t}=||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}\), \(a=\mu\), \(b=(2\sigma_{\max}^{2}+\frac{1}{4c_{p}^{2}\sqrt{f_{i_{0}}(\mathbf{x}_{0})-\ell_{i_{ 0}}^{\star}}})\). Secondly, if instead \(c_{p}\sqrt{\sum_{s=0}^{t}f_{i_{s}}(\mathbf{x}_{t})-\ell_{i_{s}}^{\star}}\geq \frac{1}{2}\), then we have:

\[-2\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-\ell_{i_{t}}^{\star})+\eta_{t}\frac{f_{i_{ t}}(\mathbf{x}_{t})-\ell_{i_{t}}^{\star}}{c_{p}\sqrt{\sum_{s=0}^{t}f_{i_{s}}( \mathbf{x}_{t})-\ell_{i_{s}}^{\star}}}\leq 0\,\] (B.21)

and consequently we can apply Lemma 13 with \(z_{t}=||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}\), \(a=\mu\), \(b=2\sigma_{\max}^{2}\).

**AdaSLS**: Similarly, by plugging the upper bound of \(\eta_{t}\) in Lemma 16, we obtain:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\] \[\stackrel{{\eqref{eq:2017}}}{{\leq}}(1-\eta_{t} \mu)||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}-2\eta_{t}(f_{i_{t}}(\mathbf{x}_ {t})-f_{i_{t}}(\mathbf{x}^{\star}))+\eta_{t}\frac{\gamma_{t}||\nabla f_{i_{t}} (\mathbf{x}_{t})||^{2}}{c_{l}\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}} (\mathbf{x}_{s})||^{2}}}\] \[\leq(1-\eta_{t}\mu)||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}+2 \eta_{t}(f_{i_{t}}(\mathbf{x}^{\star})-f_{i_{t}}^{\star})-2\eta_{t}(f_{i_{t}} (\mathbf{x}_{t})-f_{i_{t}}^{\star})+\eta_{t}\frac{f_{i_{t}}(\mathbf{x}_{t})-f_ {i_{t}}^{\star}}{c_{l}\rho\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}( \mathbf{x}_{s})||^{2}}}\] \[\leq(1-\eta_{t}\mu)||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}+2 \eta_{t}\sigma_{\max}^{2}-2\eta_{t}\underbrace{(f_{i_{t}}(\mathbf{x}_{t})-f_{ i_{t}}^{\star})}_{\geq 0}+\eta_{t}\frac{f_{i_{t}}(\mathbf{x}_{t})-f_{i_{t}}^{ \star}}{c_{l}\rho\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{ s})||^{2}}}\,\] (B.22)

where \(\sigma_{\max}^{2}=\max_{i_{t}}\left\{f_{i_{t}}(\mathbf{x}^{\star})-f_{i_{t}}^{ \star}\right\}\). We can then compare \(c_{l}\rho\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}\) with \(\frac{1}{2}\) and apply Lemma 13 correspondingly. 

### Proofs for Theorem 2 and 6

Proof.: For clarity, we separate the proofs for AdaSPS and AdaSLS.

**AdaSPS:** Plugging in the upper bound of \(\eta_{t}\) in Lemma 15, we have:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\stackrel{{\eqref{eq:2017 }}}{{\leq}}||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}-2\eta_{t}\langle\nabla f_ {i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle+\eta_{t} \frac{f_{i_{t}}(\mathbf{x}_{t})-f^{\star}}{c_{p}\sqrt{\sum_{s=0}^{t}f_{i_{s}}( \mathbf{x}_{s})-f^{\star}}}\.\] (B.23)

Since \(c_{p}\sqrt{f_{i_{0}}(\mathbf{x}_{0})-f^{\star}}\geq 1\), (B.23) can be reduced to:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\leq||\mathbf{x}_{t}-\mathbf{x}^{ \star}||^{2}-2\eta_{t}\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}- \mathbf{x}^{\star}\rangle+\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\.\] (B.24)

By convexity of \(f_{i_{t}}\), we get:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\stackrel{{\eqref{eq:2017 }}}{{\leq}}||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}-\eta_{t}\langle\nabla f_ {i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle\.\] (B.25)

Note that \(\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}^{\star} \rangle\geq 0\) and \(\eta_{t}\) is non-increasing, we thus get:

\[||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\leq||\mathbf{x}_{t}-\mathbf{x}^{ \star}||^{2}-\eta_{T-1}\langle\nabla f_{i_{t}}(\mathbf{x}_{t}),\mathbf{x}_{t}- \mathbf{x}^{\star}\rangle\.\] (B.26)

We first show that \(\eta_{T-1}\) is always lower bounded. From equation (B.25) and using convexity of \(f_{i_{t}}\), we get:

\[\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\leq||\mathbf{x}_{t}-\mathbf{x}^{ \star}||^{2}-||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\.\] (B.27)

Summing from \(t=0\) to \(t=T-1\), we get:

\[\sum_{t=0}^{T-1}\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\leq||\mathbf{x}_{0} -\mathbf{x}^{\star}||^{2}\.\] (B.28)

Using the lower bound of \(\eta_{t}\), we get:

\[\frac{1}{2c_{p}L}\sqrt{\sum_{s=0}^{T-1}f_{i_{s}}(\mathbf{x}_{s})-f^{\star}} \stackrel{{\eqref{eq:2017}}}{{\leq}}\frac{1}{2c_{p}L}\sum_{t=0}^{T-1} \frac{f_{i_{t}}(\mathbf{x}_{t})-f^{\star}}{\sqrt{\sum_{s=0}^{t}f_{i_{s}}(\mathbf{ x}_{s})-f^{\star}}}\stackrel{{\eqref{eq:2017}}}{{\leq}}\sum_{t=0}^{T-1} \eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\,\] (B.29)

This reveals that:

\[\eta_{T-1}\stackrel{{\eqref{eq:2017}}}{{\geq}}\frac{1}{2c_{p}L} \frac{1}{\sqrt{\sum_{s=0}^{T-1}f_{i_{s}}(\mathbf{x}_{s})-f^{\star}}}\geq\frac{ 1}{(2c_{p}L||\mathbf{x}_{0}-\mathbf{x}^{\star}||)^{2}}\.\] (B.30)

[MISSING_PAGE_FAIL:21]

Since \(c_{l}\rho\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}\geq 1\), we obtain the same equation as (B.26). To find a lower bound of \(\eta_{T-1}\), we rearrange (B.25) as:

\[\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\leq||\mathbf{x}_{t}-\mathbf{x}^{ \star}||^{2}-||\mathbf{x}_{t+1}-\mathbf{x}^{\star}||^{2}\,\] (B.41)

the left-hand-side of which can be lower bounded by:

\[\eta_{t}(f_{i_{t}}(\mathbf{x}_{t})-f^{\star})\geq\eta_{t}\rho\gamma_{t}|| \nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\stackrel{{\text{(\ref{eq:1})}} }{{\geq}}\min\{\frac{1-\rho}{L},\gamma_{\max}\}\frac{\rho\gamma_{t}||\nabla f_{ i_{t}}(\mathbf{x}_{t})||^{2}}{c_{l}\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f _{i_{s}}(\mathbf{x}_{s})||^{2}}}\.\] (B.42)

Summing over \(t=0\) to \(t=T-1\) gives:

\[\min\{\frac{1-\rho}{L},\gamma_{\max}\}\frac{\rho}{c_{l}}\sqrt{\sum_{s=0}^{T-1} \gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}\stackrel{{ \text{(\ref{eq:1})}}}{{\leq}}\min\{\frac{1-\rho}{L},\gamma_{\max}\} \frac{\rho}{c_{l}}\sum_{t=0}^{T-1}\frac{\gamma_{t}||\nabla f_{i_{t}}(\mathbf{ x}_{t})||^{2}}{\sqrt{\sum_{s=0}^{t}\gamma_{s}||\nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}}} \leq||\mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}\.\] (B.43)

This implies that:

\[\eta_{T-1}\stackrel{{\text{(\ref{eq:1})}}}{{\geq}}\frac{\min\{ \frac{1-\rho}{L},\gamma_{\max}\}}{c_{l}\sqrt{\sum_{s=0}^{T-1}\gamma_{s}||\nabla f _{i_{s}}(\mathbf{x}_{s})||^{2}}}\stackrel{{\text{(\ref{eq:1})}} }{{\geq}}\frac{\rho\min^{2}\{\frac{1-\rho}{L},\gamma_{\max}\}}{c_{l}^{2}|| \mathbf{x}_{0}-\mathbf{x}^{\star}||^{2}}\.\] (B.44)

After plugging the above into (B.26), the remaining proof follows from the same routine as shown for AdaSPS. 

### Statement and Proof of Lemma 18

The following Lemma provides us with the guarantee that as \(\mathbf{w}_{t},\mathbf{x}_{t}\rightarrow\mathbf{x}^{\star}\), \(\mathbb{E}_{i_{t}}[F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}^{\star}]\to 0\), which implies diminishing variance.

**Lemma 18**.: _Assume each \(f_{i}\) is convex and \(L\)-smooth, for any \(t\geq 0\), the iterates generated by Algorithm 1 satisfy:_

\[\mathbb{E}_{i_{t}}[F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}^{\star}]\leq f(\mathbf{ x}_{t})-f^{\star}+\frac{1}{2\mu_{F}}\,\mathbb{E}_{i_{t}}\big{[}||\nabla f_{i_{t}}( \mathbf{w}_{t})-\nabla f_{i_{t}}(\mathbf{x}^{\star})||^{2}\big{]}\,.\] (B.45)

Proof.: By definition of \(F_{i_{t}}(\mathbf{x})\), we have:

\[F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}^{\star}\] (B.46) \[=F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}(\mathbf{x}^{\star})+F_{i_{t}} (\mathbf{x}^{\star})-F_{i_{t}}^{\star}\] \[=f_{i_{t}}(\mathbf{x}_{t})-f_{i_{t}}(\mathbf{x}^{\star})+( \mathbf{x}_{t}-\mathbf{x}^{\star})^{T}(\nabla f(\mathbf{w}_{t})-\nabla f_{i_{t} }(\mathbf{w}_{t}))-\frac{\mu_{F}}{2}||\mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}+ F_{i_{t}}(\mathbf{x}^{\star})-F_{i_{t}}^{\star}\.\]

By \(\mu_{F}\)-strong convexity of \(F_{i_{t}}(\mathbf{x})\), we obtain:

\[F_{i_{t}}(\mathbf{x}^{\star})-F_{i_{t}}^{\star} \stackrel{{\text{(\ref{eq:1})}}}{{\leq}}\frac{1}{2\mu_{F }}||\nabla F_{i_{t}}(\mathbf{x}^{\star})||^{2}\] (B.47) \[=\frac{1}{2\mu_{F}}||\nabla f_{i_{t}}(\mathbf{x}^{\star})-\nabla f _{i_{t}}(\mathbf{w}_{t})+\nabla f(\mathbf{w}_{t})+\mu_{F}(\mathbf{x}^{\star}- \mathbf{x}_{t})||^{2}\.\]

Plugging (B.47) into (B.46), taking expectation w.r.t the randomness \(i_{t}\) on both sides gives:

\[\mathbb{E}_{i_{t}}[F_{i_{t}}(\mathbf{x}_{t})-F_{i_{t}}^{\star}]\] (B.48) \[\leq f(\mathbf{x}_{t})-f(\mathbf{x}^{\star})-\frac{\mu_{F}}{2}|| \mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}+\mathbb{E}_{i_{t}}[\frac{1}{2\mu_{F}}|| \nabla f_{i_{t}}(\mathbf{x}^{\star})-\nabla f_{i_{t}}(\mathbf{w}_{t})+\nabla f (\mathbf{w}_{t})+\mu_{F}(\mathbf{x}^{\star}-\mathbf{x}_{t})||^{2}]\] \[=f(\mathbf{x}_{t})-f(\mathbf{x}^{\star})-\frac{\mu_{F}}{2}|| \mathbf{x}_{t}-\mathbf{x}^{\star}||^{2}+\frac{\mu_{F}}{2}||\mathbf{x}_{t}- \mathbf{x}^{\star}||^{2}+\frac{1}{2\mu_{F}}\,\mathbb{E}_{i_{t}}[||\nabla f_{i_{t}} (\mathbf{x}^{\star})-\nabla f_{i_{t}}(\mathbf{w}_{t})+\nabla f(\mathbf{w}_{t}) ||^{2}]\] \[\quad+\frac{1}{\mu_{F}}\,\mathbb{E}_{i_{t}}[\langle\nabla f_{i_{t}}( \mathbf{x}^{\star})-\nabla f_{i_{t}}(\mathbf{w}_{t})+\nabla f(\mathbf{w}_{t}),\mu_ {F}(\mathbf{x}^{\star}-\mathbf{x}_{t})\rangle]\] \[=f(\mathbf{x}_{t})-f(\mathbf{x}^{\star})+\frac{1}{2\mu_{F}}\, \mathbb{E}_{i_{t}}[||\nabla f_{i_{t}}(\mathbf{x}^{\star})-\nabla f_{i_{t}}( \mathbf{w}_{t})+\nabla f(\mathbf{w}_{t})||^{2}]\] \[\leq f(\mathbf{x}_{t})-f(\mathbf{x}^{\star})+\frac{1}{2\mu_{F}}\, \mathbb{E}_{i_{t}}[||\nabla f_{i_{t}}(\mathbf{x}^{\star})-\nabla f_{i_{t}}( \mathbf{w}_{t})||^{2}]\.\]

[MISSING_PAGE_FAIL:23]

Hence. we obtain:

\[\left(\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]\right)^{2}\leq(1+ \frac{2L}{(1-a)\mu_{F}})\tau^{2}\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{ \star}].\] (B.58)

It follows that:

\[\sum_{t=0}^{T-1}\mathbb{E}[f(\mathbf{x}_{t})-f^{\star}]\leq(1+\frac{2L}{(1-a) \mu_{F}})\tau^{2}.\] (B.59)

Dividing both sides by \(T\) and applying Jensen's inequality concludes the proof.

### Proof of Corollary 8

Proof.: Algorithm 1 calls the stochastic gradient oracle in expectation \(\mathcal{O}(1+p_{t}n)\) times at iteration \(t\). Therefore, the total number of gradient evaluations is upper bounded by \(\mathcal{O}(\sum_{t=0}^{T-1}p_{t}n+T)\). By our choice of \(p_{t}\), it holds that \(\sum_{t=0}^{T-1}p_{t}\leq\frac{1}{a}\sum_{t=0}^{T-1}\frac{1}{t+2}\leq\frac{1}{ a}(\log(T)+1-1)=\frac{1}{a}\log(T)\). Due to the sublinear convergence rate of Algorithm 1, we conclude that the total number of stochastic gradient calls is \(\mathcal{O}(\log(1/\varepsilon)n+1/\varepsilon)\). 

## Appendix C Pseudo-codes for AdaSPS and AdaSLS

In this section, we provide formal pseudo codes for AdaSPS (AdaSPS) and AdaSLS (AdaSLS).

To implement AdaSPS, a lower bound of optimal function value for each minibatch function is required. For machine learning problems where the individual loss functions are non-negative, we can use zero as an input. Apart from that, we need to provide a constant \(c_{p}\) to adjust the magnitude of the stepsize. Theoretically suggested \(c_{p}\) for robust convergence satisfies \(c_{p}^{\text{scale}}=c_{p}\sqrt{f_{i_{0}}(\mathbf{x}_{0})-\ell_{i_{0}}^{\star} }\geq\frac{1}{2}\). Therefore, a common choice is to set \(c_{p}=\frac{1}{\sqrt{f_{i_{0}}(\mathbf{x}_{0})-\ell_{i_{0}}^{\star}}}\).

```
0:\(\mathbf{x}_{0}\in\mathbb{R}^{d}\), \(T\in\mathbb{N}^{+}\), \(c_{p}>0\)
1: set \(\eta_{-1}=+\infty\)
2: set \(\varepsilon=10^{-10}\)
3:for\(t=0\) to \(T-1\)do
4: uniformly sample \(i_{t}\subseteq[n]\)
5: provide a lower bound \(\ell_{t}^{\star}\leq f_{t}^{\star}\)
6: set \(\eta_{t}=\min\left\{\frac{f_{i_{t}}(\mathbf{x}_{t})-\ell_{i_{t}}^{\star}}{c_ {p}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}}\frac{1}{\sqrt{\sum_{s=0}^{1}f_{ i_{s}}(\mathbf{x}_{s})-\ell_{t_{s}}^{\star}+\varepsilon}},\eta_{t-1}\right\}\)
7:\(\mathbf{x}_{t+1}=\Pi_{\mathcal{X}}(\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}( \mathbf{x}_{t}))\)return\(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\) ```

**Algorithm 2** AdaSPS

To implement AdaSLS (AdaSLS), a line-search sub-solver 4 and an input constant \(c_{l}>0\) are required. Similar to (AdaSPS), we can set \(c_{l}=\frac{1}{\rho\sqrt{\gamma_{0}||\nabla f_{i_{0}}(\mathbf{x}_{0})||^{2}}}\) according to the theory.

```
0:\(\mathbf{x}_{0}\in\mathbb{R}^{d}\), \(T\in\mathbb{N}^{+}\), \(c_{l}>0\)
1: set \(\eta_{-1}=+\infty\)
2: set \(\varepsilon=10^{-10}\)
3:for\(t=0\) to \(T-1\)do
4: uniformly sample \(i_{t}\subseteq[n]\)
5: obtain \(\gamma_{t}\) via backtracking line-search (4)
6: set \(\eta_{t}=\min\left\{\frac{\gamma_{t}}{c_{l}\sqrt{\sum_{s=0}^{t}\gamma_{s}|| \nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}+\varepsilon}},\eta_{t-1}\right\}\)
7:\(\mathbf{x}_{t+1}=\Pi_{\mathcal{X}}(\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}( \mathbf{x}_{t}))\)return\(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\) ```

**Algorithm 3** AdaSLS

To implement AdaSLS (AdaSLS), a line-search sub-solver 4 and an input constant \(c_{l}>0\) are required. Similar to (AdaSPS), we can set \(c_{l}=\frac{1}{\rho\sqrt{\gamma_{0}||\nabla f_{i_{0}}(\mathbf{x}_{0})||^{2}}}\) according to the theory.

```
0:\(\mathbf{x}_{0}\in\mathbb{R}^{d}\), \(T\in\mathbb{N}^{+}\), \(c_{l}>0\)
1: set \(\eta_{-1}=+\infty\)
2: set \(\varepsilon=10^{-10}\)
3:for\(t=0\) to \(T-1\)do
4: uniformly sample \(i_{t}\subseteq[n]\)
5: obtain \(\gamma_{t}\) via backtracking line-search (4)
6: set \(\eta_{t}=\min\left\{\frac{\gamma_{t}}{c_{l}\sqrt{\sum_{s=0}^{t}\gamma_{s}|| \nabla f_{i_{s}}(\mathbf{x}_{s})||^{2}+\varepsilon}},\eta_{t-1}\right\}\)
7:\(\mathbf{x}_{t+1}=\Pi_{\mathcal{X}}(\mathbf{x}_{t}-\eta_{t}\nabla f_{i_{t}}( \mathbf{x}_{t}))\)return\(\bar{\mathbf{x}}_{T}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbf{x}_{t}\) ```

**Algorithm 4** AdaSLS
Line-search procedure

In this section, we introduce the classical Armijo line-search method [3, 41]. Given a function \(f_{i_{t}}(\mathbf{x})\), the Armijo line-search returns a stepsize \(\gamma_{t}\) that satisfies the following condition:

\[f_{i_{t}}(\mathbf{x}_{t}-\gamma_{t}\nabla f_{i_{t}}(\mathbf{x}_{t}))\leq f_{i _{t}}(\mathbf{x}_{t})-\rho\gamma_{t}||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\,\] (D.1)

where \(\rho\in(0,1)\) is an input hyper-parameter. If \(f_{i_{t}}(\mathbf{x})\) is a smooth function, then backtracking line-search 4 is a practical implementation way to ensure that D.1 is satisfied.

```
0:\(\beta\in[\frac{1}{2},1)\), \(\rho\in(0,1)\), \(\gamma_{\max}>0\) (We fix \(\beta=0.8\) and \(\rho=0.5\) for AdaSLS)
1:\(\gamma=\gamma_{\max}\)
2:while\(f_{i_{t}}(\mathbf{x}_{t}-\gamma\nabla f_{i_{t}}(\mathbf{x}_{t}))>f_{i_{t}}( \mathbf{x}_{t})-\rho\gamma||\nabla f_{i_{t}}(\mathbf{x}_{t})||^{2}\)do
3:\(\gamma=\beta\gamma\)return\(\gamma_{t}=\gamma\) ```

**Algorithm 4** Backtracking line-search

To implement Algorithm 4, one needs to provide the decreasing factor \(\beta\), the maximum stepsize \(\gamma_{\max}\), and the condition parameter \(\rho\). Starting from \(\gamma_{\max}\), Algorithm 4 decreases the stepsize iteratively by a constant factor \(\beta\) until the condition D.1 is satisfied. Note that checking the condition requires additional minibatch function value evaluations. Fortunately, note that the output \(\gamma\) cannot be smaller than \(\frac{1-\rho}{L}\) (Lemma 16), and thus the number of extra function value evaluations required is at most \(\mathcal{O}\Big{(}\max\{\log_{1/\beta}^{L\gamma_{\max}/(1-\rho)},1\}\Big{)}\). In practice, Vaswani et al. [52] suggests dynamic initialization of \(\gamma_{\max}\) to reduce the algorithm's running time, that is, setting \(\gamma_{\max_{t}}=\gamma_{t-1}\theta^{1/n}\) where a common choice for \(\theta\) is \(2\). This strategy initializes \(\gamma_{\max}\) by a slightly larger number than the last output and thus is usually more efficient than keeping \(\gamma_{\max}\) constant or always using \(\gamma_{\max_{t}}=\gamma_{t-1}\). In all our experiments, we use the same \(\gamma_{\max}\) at each iteration for AdaSLS to show its theoretical properties.

Goldstein line-search is another line-search method that checks D.1 and an additional curvature condition [41]. We do not study this method in this work and we refer to [52] for more details.

## Appendix E Counter examples of SPS and its variants for SVRG

We provide two simple counterexamples where SVRG with the SPS stepsize and its intuitive variants fail to converge. For simplicity, consider the update rule \(\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\nabla f(\mathbf{x}_{t})\), i.e. \(\mathbf{w}_{t}=\mathbf{x}_{t}\) for all \(t\geq 0\). Consider the function \(f(\mathbf{x})=\frac{f_{i}(\mathbf{x})+f_{j}(\mathbf{x})}{2}\) where \(f_{1}(\mathbf{x})=a_{1}(\mathbf{x}-1)^{2}\) and \(f_{2}(\mathbf{x})=a_{2}(\mathbf{x}+1)^{2}\) with \(a_{1},a_{2}>0\).

**Example 19**.: _Individual curvature is not representative. Consider the standard stochastic Polyak stepsize: \(\eta_{t}=\frac{f_{i}(\mathbf{x}_{t})-f_{i}^{*}}{||\nabla f_{i}(\mathbf{x}_{t}) ||^{2}}\) where \(i\) is randomly chosen from \(\{1,2\}\). We now let \(a_{1}=1\) and \(a_{2}<1\). Note that \(\nabla^{2}f(\mathbf{x})=a_{1}+a_{2}\in(1,2)\) while \(\mathbb{E}_{i}[\eta_{t}]=\frac{1}{8}+\frac{1}{8a_{2}}\rightarrow+\infty\) as \(a_{2}\to 0\), which leads to divergence. The reason behind this is that individual curvature does not match the global curvature._

**Example 20**.: _Minstatching quantity. Consider a variant of stochastic Polyak stepsize: \(\eta_{t}=\frac{f_{i}(\mathbf{x}_{t})-f_{i}^{*}}{||\nabla f_{i}(\mathbf{x}_{t})- \nabla f_{i}(\mathbf{w}_{t})+\nabla f(\mathbf{w}_{t})||^{2}}\) where \(i\) is randomly chosen from \(\{1,2\}\). Let \(a_{1}=a_{2}=1\). We note \(\mathbb{E}_{i}[\eta_{t}\nabla f(\mathbf{x}_{t})]=\frac{x_{t}^{2}+1}{2x_{t}}\neq 0\) and thus no stationary point exists. Similar reasoning can exclude a number of other variants such as: \(\eta_{t}=\frac{f_{i}(\mathbf{x}_{t})-f_{i}(\mathbf{w}_{t})+f(\mathbf{w}_{t})- f_{i}^{*}}{||\nabla f_{i}(\mathbf{x}_{t})-\nabla f_{i}(\mathbf{w}_{t})+ \nabla f(\mathbf{w}_{t})||^{2}}\). Indeed, the numerator is not the proper function value difference of a valid function with the gradient defined in the denominator._

## Appendix F Experimental details and additional experiment results

In this section, we provide a detailed setup of the experiments presented in the main paper.

In practice, we can use a lower bound of \(F_{i_{t}}^{\star}\) for running AdaSVRPS since convergence is still guaranteed thanks to the property of AdaSPS. By default, we use \(\ell_{i_{t}}^{\star}+\min_{\mathbf{x}}\{\mathbf{x}^{T}(\nabla f(\mathbf{w}_{t} )-\nabla f_{i_{t}}(\mathbf{w}_{t}))+\frac{\mu_{F}}{2}||\mathbf{x}-\mathbf{x}_{t }||^{2}\}\) for all the experiments, where \(\ell_{i_{t}}^{\star}\) is a lower bound for \(f_{i_{t}}^{\star}\).

### Synthetic experiment

We consider the minimization of a quadratic of the form: \(f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}f_{i}(\mathbf{x})\) where \(f_{i}(\mathbf{x})=\frac{1}{2}(\mathbf{x}-\mathbf{b}_{i})^{T}A_{i}(\mathbf{x}- \mathbf{b}_{i})\), \(\mathbf{b}_{i}\in\mathbb{R}^{d}\) and \(A_{i}\in\mathbb{R}^{d\times d}\) is a diagonal matrix. We use \(n=50\), \(d=1000\). We control the interpolation by either setting all \(\{b_{i}\}\) to be identical or different. Each component of \(\{b_{i}\}\) is generated according to \(\mathcal{N}(0,10^{2})\). We control the complexity of the problems by choosing different matrices \(A_{i}\). For the strongly-convex case, we first generate a matrix \(A^{N}=\text{clip}(\begin{pmatrix}a_{11}&...&a_{1d}\\...&...&\\ a_{n1}&...&a_{nd}\end{pmatrix},1,10)\) where each \(a_{ij}\sim N(0,15^{2})\) and the clipping operator clips the elements to the interval between \(1\) and \(10\). Then we compute:

\[A=\begin{pmatrix}1&1&...&\frac{n}{\sum_{i=1}^{n}A_{i(d-1)}^{\mathcal{N}}}&\frac {10n}{\sum_{i=1}^{n}A_{id}^{\mathcal{N}}}\\...&...&...&...\\ 1&1&...&\frac{n}{\sum_{i=1}^{n}A_{i(d-1)}^{\mathcal{N}}}&\frac{10n}{\sum_{i=1 }^{n}A_{id}^{\mathcal{N}}}\end{pmatrix}\bigodot A_{N}\,\]

where \(\bigodot\) denotes the Hadamard product. We set the diagonal elements of each \(A_{i}\) using the corresponding row stored in the matrix \(A\). Note that \(\nabla^{2}f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}A_{i}\) has the minimum and the largest eigenvalues being \(1\) and \(10\). For the general convex case, we use the same matrix \(A_{N}\) to generate a sparse matrix \(A_{M}\) such that \(A_{M}=A_{N}\bigodot M\) where \(M\) is a mask matrix with \(M_{ij}\sim B(1,p)\) and \(\begin{pmatrix}1&...&1\end{pmatrix}\cdot M_{:j}\geq 1,\ \forall j\in[1,d]\). We then compute the matrix \(A\) and set each \(A_{i}\) in the same way.

\[A=\begin{pmatrix}\frac{2^{-20}n}{\sum_{i=1}^{n}A_{i1}^{\mathcal{N}}}&\frac{2^ {-1}n}{\sum_{i=1}^{n}A_{i2}^{\mathcal{N}}}&...&\frac{2^{-1}n}{\sum_{i=1}^{n}A_ {i2}^{\mathcal{N}}}&1&...&1&\frac{10n}{\sum_{i=1}^{n}A_{id}^{\mathcal{N}}}\\ \frac{2^{-20}n}{\sum_{i=1}^{n}A_{i1}^{\mathcal{N}}}&\frac{2^{-1}n}{\sum_{i=1 }^{n}A_{i2}^{\mathcal{N}}}&...&\frac{2^{-1}n}{\sum_{i=1}^{n}A_{i2}^{\mathcal{N }}}&1&...&1&\frac{10n}{\sum_{i=1}^{n}A_{id}^{\mathcal{N}}}\end{pmatrix}\bigodot A _{M}\.\]

Through the construction, the smallest eigenvalues of \(\nabla^{2}f(\mathbf{x})\) are clustered around zero, and the largest eigenvalue is \(10\). Additionally, each \(\nabla^{2}f_{i}(\mathbf{x})\) is positive semi-definite.

We set the batch size to be \(1\) and thus we have \(f^{\star}_{it}=0\) with interpolation and \(\ell^{\star}_{it}=0\) without interpolation.

For AdaSPS/AdaSVRPS we fix \(c^{\text{scale}}_{\rho}=1\), and for AdaSVRPS we further use \(\mu_{F}=10\) and \(p_{t}=\frac{1}{0.1t+1}\). We compare against DecSPS [44], SPS [34] and SVRG [22] and tune the stepsize for SVRG by picking the best one from \(\{10^{i}\}_{i=-4,..,3}\).

In addition to these optimizers, we further evaluate the performance of SLS [52], AdaSLS, SGD, SPS\({}_{\max}\)[34] and AdaSVRLS. We fix \(c^{\text{scale}}_{t}=1\), \(\gamma_{\max}=10\), \(\beta=0.8\) and \(\rho=1/2\) for both algorithms and for AdaSVRLS, we further use \(\mu_{F}=10\) and \(p_{t}=\frac{1}{0.1t+1}\). For SGD, we use the best learning rate schedules in different scenarios. Specifically, for both interpolation problems, we keep the stepsize constant and for non-interpolation problems, we apply \(\Theta(1/\sqrt{t})\) and \(\Theta(1/t)\) decay schedules for convex and strongly-convex problems respectively. We further pick the best stepsize from \(\{10^{i}\}_{i=-4,..,3}\). For SPS\({}_{\max}\), we use \(\gamma_{b}=10^{-3}\) and we only showcase its performance in non-interpolated settings. We report the results in Figure F.1. We observe that AdaSLS is comparable if no faster than the best-tuned vanilla SGD. SPS\({}_{\max}\) is reduced to the vanilla SGD with constant stepsize. AdaSVRLS provides similar performance to AdaSVRPS but due to the cost of additional function evaluations, it is less competitive than AdaSVRPS.

Figure F.1: Comparison of the considered optimizers on synthetic data set with quadratic loss. The left block of the label illustrates the variance-reduced methods and the right represents SGD with different stepsizes. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)

[MISSING_PAGE_FAIL:27]

## Appendix G Deep learning task

In this section, we provide a heuristic extension of AdaSPS to over-parameterized non-convex optimization tasks. When training modern deep learning models, Loshchilov and Hutter [35] observe that a cyclic behaviour of the stepsize, i.e., increasing at the beginning and then decreasing up to a constant, can help with fast training and good generalization performance. Since AdaSPS is a non-increasing stepsize, it excludes such a cyclic behaviour. To address this issue, we provide a non-convex version of AdaSPS which incorporates a restart mechanism that allows an increase of the stepsize according to the local curvature. The full algorithm is summarized in Algorithm 5. In practice, we can set \(u=\frac{h}{n}\). Algorithm 5 updates the stepsize and \(c_{p}\) at the beginning of each epoch and uses AdaSPS (AdaSPS) for the rest of the epoch.

Following [34; 52], we benchmark the convergence and generalization performance of AdaSPS (DL) 5 for the multi-class classification tasks on CIFAR10 [28] and CIFAR100 [29] datasets using ResNet-34 [21]. We compare against SPS [34], Adam [26], AdaGrad [15], DecSPS [44] and SGD with momentum. We use the smoothing technique and pick \(c=0.02\) for SPS as suggested in [34]. We use the official implementations of Adam, AdaGrad, and SGD with momentum from https://pytorch.org/docs/stable/optim.html. We choose \(\mathrm{lr}=10^{-3}\), \(\beta_{1}=0.9\) and \(\beta_{2}=0.999\) for Adam. We choose \(\mathrm{lr}=0.01\) for AdaGrad. We choose \(\mathrm{lr}=0.9\) and \(\beta=0.9\) for SGD with momentum. Finally, we pick \(c_{p}^{\mathrm{scale}}=0.02\) for Algorithm 5. In Figure G.1, AdaSPS (DL) shows competitive performance on both datasets. We leave the study of its theoretical properties to future work.

[MISSING_PAGE_EMPTY:29]