# HiGen: Hierarchical Graph Generative Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performance in terms of graph quality on various benchmarks.

## 1 Introduction

Graphs play a fundamental role in representing relationships and are widely applicable in various domains. The task of generating graphs from data holds immense value for diverse applications but also poses significant challenges (Dai et al., 2020). Some of the applications include: the exploration of novel molecular and chemical structures (Lin et al., 2020), document generation (Blei et al., 2003), circuit design (Mirhoseini et al., 2021), the analysis and synthesis of realistic data networks, as well as the synthesis of scene graphs in computer (Manolis Savva et al., 2019; Ramakrishnan et al., 2021).

In all the aforementioned domains, a common observation is the presence of locally heterogeneous edge distributions in the graph representing the system, leading to the formation of clusters or communities and hierarchical structures. These clusters represent groups of nodes characterized by a high density of edges within the group and a comparatively lower density of edges connecting the group with the rest of the graph. In a hierarchical structure that arise from graph clustering, the communities in the lower levels capture the local structures and relationships within the graph. These communities provide insights into the fine-grained interactions among nodes. On the other hand, the higher levels of the hierarchy reflect the broader interactions between communities and characterize global properties of the graph. Therefore, in order to generate realistic graphs, it is essential for graph generation models to learn this multi-scale structure, and be able to capture the cross-level relations. While hierarchical multi-resolution generative models were developed for specific data types such as voice (Oord et al., 2016), image (Reed et al., 2017; Karami et al., 2019) and molecular motifs (Lin et al., 2020), these methods rely on domain-specific priors that are not applicable to general graphs with unordered nature. To the best of our knowledge, there exists no data-driven generative models specifically designed for generic graphs that can effectively incorporate hierarchical structure.

Graph generative models have been extensively studied in the literature. Classical methods based on random graph theory, such as those proposed in (Erdos and Renyi, 1960) and (Barabasi and Albert, 1999),can only capture a limited set of hand-engineered graph statistics. Leskovec et al. (2010) leveraged the Kronecker product of matrices but the resulting generative model is very limited in modeling the underlying graph distributions. With recent advances in graph neural networks, a variety of deep neural network models have been introduced that are based on variational autoencoders (VAE) Kingma and Welling (2013) or generative adversarial networks (GAN) Goodfellow et al. (2020).

Some examples of such models include De Cao and Kipf (2018)Simonovsky and Komodakis (2018)Kipf and Welling (2016)Ma et al. (2018)Liu et al. (2019)Bojchevski et al. (2018)Yang et al. (2019).

The major challenge in VAE based models is that they rely on heuristics to solve a graph matching problem for aligning the VAE's input and sampled output, limiting them to small graphs. On the other hand, GAN-based methods circumvent the need for graph matching by using a permutation invariant discriminator. However, they can still suffer from convergence issues and have difficulty capturing complex dependencies in graph structures for moderate to large graphs Li et al. (2018); Martinkus et al. (2022). To address these limitations, Martinkus et al. (2022) recently proposed using spectral conditioning to enhance the expressivity of GAN models in capturing global graph properties.

On the other hand, autoregressive models approach graph generation as a sequential decision-making process. Following this paradigm, Li et al. (2018) proposed generative model based on GNN but it has high complexity of \(\mathcal{O}(mn^{2})\). In a distinct approach, GraphRNN You et al. (2018) modeled graph generation with a two-stage RNN architecture for generating new nodes and their links, respectively. However, traversing all elements of the adjacency matrix in a predefined order results in \(\mathcal{O}(n^{2})\) time complexity making it non-scalable to large graphs. In contrast, GRAN Liao et al. (2019) employs a graph attention network and generates the adjacency matrix row by row, resulting in a \(\mathcal{O}(n)\) complexity sequential generation process. To improve the scalability of generative models, Dai et al. (2020) proposed an algorithm for sparse graphs that decreases the training complexity to \(\mathcal{O}(\log n)\), but at the expense of increasing the generation time complexity to \(\mathcal{O}((n+m)\log n)\). Despite their improvement in capturing complex statistics of the graphs, autoregressive models highly rely on an appropriate node ordering and do not take into account the community structures of the graphs. Additionally, due to their recursive nature, they are not fully parallelizable.

A new family of diffusion model for graphs has emerged recently. Continuous denoising diffusion was developed by Jo et al. (2022), which adds Gaussian noise to the graph adjacency matrix and node features during the diffusion process. However, since continuous noise destroys the sparsity and structural properties of the graph, discrete denoising diffusion models have been developed as a solution in Haefeli et al. (2022)Vignac et al. (2022). These models progressively edit graphs by adding or removing edges in the diffusion process, and then denoising graph neural networks are trained to reverse the diffusion process. While the denoising diffusion models can offer promising results, their main drawback is the requirement of a long chain of reverse diffusion, which can result in relatively slow sampling.

In his work, we introduce HiGen, a Hierarchical Graph Generative Network to address the limitations of existing generative models by incorporating community structures and cross-level interactions. This approach involves generating graphs in a coarse-to-fine manner, where graph generation at each level is conditioned on a higher level (lower resolution) graph. The generation of communities at lower levels is performed in parallel, followed by the prediction of cross-edges between communities using a separate model. This parallelized approach enables high scalability. To capture hierarchical relations, our model allows each node at a given level to depend not only on its neighbouring nodes but also on its corresponding super-node at the higher level. Furthermore, we address the generation of integer-valued edge weights of the hierarchical structure by modeling the output distribution of edges using a multinomial distribution. We show that multinomial distribution can be factorized successively, enabling the autoregressive generation of each community. This property makes the proposed architecture well-suited for generating graphs with integer-valued edge weights. Furthermore, by breaking down the graph generation process into the generation of multiple small partitions that are conditionally independent of each other, HiGen reduces its sensitivity to a predefined initial ordering of nodes.

## 2 Background

A graph \(\mathcal{G}=(\mathcal{V},\,\mathcal{E})\) is a collection of nodes (vertices) \(\mathcal{V}\) and edges \(\mathcal{E}\) with corresponding sizes \(n=|\mathcal{V}|\) and \(m=|\mathcal{E}|\) and an adjacency matrix \(\mathbf{A}^{\pi}\) for the node ordering \(\pi\). The node set can be partitioned into \(c\) communities (a.k.a. cluster or modules) using a graph partitioning function \(\mathcal{F}:\ \mathcal{V}\rightarrow\ \{1,...,c\}\), where each cluster of nodes forms a sub-graph denoted by \(\mathcal{C}_{i}=(\mathcal{V}(\mathcal{C}_{i}),\ \mathcal{E}(\mathcal{C}_{i}))\) with adjacency matrix \(\mathbf{A}_{i}\). The cross-links between neighboring communities form a _bipartite graph_, denoted by \(\mathcal{B}_{ij}=(\mathcal{V}(\mathcal{C}_{i}),\ \mathcal{V}(\mathcal{C}_{j}),\ \mathcal{E}( \mathcal{B}_{ij}))\) with adjacency matrix \(\mathbf{A}_{ij}\). Each community is aggregated to a super-node and each bipartite corresponds to a super-edge linking neighboring communities, which induces a coarser graph at the higher (a.k.a. parent) level. Herein, the levels are indexed by superscripts. Formally, each community at level \(l\), \(\mathcal{C}_{i}^{l}\), is mapped to a node at the higher level graph, also called its parent node, \(v_{i}^{l-1}:=Pa(\mathcal{C}_{i}^{l})\) and each bipartite at level \(l\) is represented by an edge in the higher level, also called its parent edge, \(e_{i}^{l-1}=Pa(\mathcal{B}_{ij}^{l})=(v_{i}^{l-1},v_{j}^{l-1})\). The weights of the self edges and the weights of the cross-edges in the parent level are determined by the sum of the weights of the edges within their corresponding community and bipartite, respectively. Therefore, the edges in the induced graphs at the higher levels have integer-valued weights: \(w_{ii}^{l-1}=\sum_{e\in\mathcal{E}(\mathcal{C}_{i}^{l})}w_{e}\) and \(w_{ij}^{l-1}=\sum_{e\in\mathcal{E}(\mathcal{B}_{ij}^{l})}w_{e}\), moreover sum of all edge weights remains constant in all levels so \(w_{0}:=\sum_{e\in\mathcal{E}(\mathcal{G}^{l})}w_{e}=|\mathcal{E}|,\ \forall\ l\in[0,...,L]\).

This clustering process continues recursively in a bottom-up approach until a single node graph \(\mathcal{G}^{0}\) is obtained, producing a _hierarchical graph_, defined by the set of graphs in all levels of abstractions, \(\mathcal{H}\mathcal{G}:=\{\mathcal{G}^{0},....,\mathcal{G}^{L-1},\mathcal{G}^ {L}\}\). This forms a dendrogram tree with \(\mathcal{G}^{0}\) being the root and \(\mathcal{G}^{L}\) being the final graph that is generated at the leaf level. An \(\mathcal{H}\mathcal{G}\) is visualized in figure1. The hierarchical tree

Figure 1: (a) A sample hierarchical graph with 2 levels is shown. Communities are shown in different colors and the weight of a node and the weight of an edge in a higher level, represent the sum of the edges in the corresponding community and bipartite, respectively. Node size and edge width indicate their weights. (b) The matrix shows corresponding adjacency of the graph \(\mathcal{G}^{2}\) matrix where each of its sub-graphs corresponds to a block in the adjacency matrix, communities are shown in different colors and bipartites are colored in gray. (c) Decomposition of multinomial distribution as a recursive _stick-breaking_ process where at each iteration, first a fraction of the remaining weights \(\mathrm{w}_{m}\) is allocated to the \(m\)-th row (the \(m\)-th node in the sub-graph) and then this fraction \(\mathrm{v}_{m}\) is distributed among that row of lower triangular adjacency matrix, \(\hat{A}\). (d) Parallel generation of communities. (e) Parallel prediction of bipartites. Shadowed lines are the _augmented edges_ representing candidate edges at each step.

structure enables modeling of both local and long-range interactions among nodes, as well as control over the flow of information between them, across multiple levels of abstraction. This is a key aspect of our proposed generative model.

## 3 Hierarchical Graph Generation

In graph generative networks, the objective is to learn a generative model, \(p(\mathcal{G})\) given a set of training graphs. This work aims to establish a hierarchical multi-resolution framework for generating graphs in a coarse-to-fine fashion. In this framework, we assume that the graphs do not have node attributes, so the generative model only needs to characterize the graph topology. Given a particular node ordering \(\pi\), and a hierarchical graph \(\mathcal{HG}:=\{\mathcal{G}^{0},....,\mathcal{G}^{L-1},\mathcal{G}^{L}\}\), produced by recursively applying a graph partitioning function, \(\mathcal{F}\), we can factorize the generative model using the chain rule of probability as:

\[p(\mathcal{G}=\mathcal{G}^{L},\pi)=p(\{\mathcal{G}^{L},\mathcal{ G}^{L-1},...,\mathcal{G}^{0}\},\pi) =p(\mathcal{G}^{L},\pi\mid\{\mathcal{G}^{L-1},...,\mathcal{G}^{0}\}) \leavevmode\nobreak\...\leavevmode\nobreak\ p(\mathcal{G}^{1},\pi\mid\mathcal{G}^{0}) \leavevmode\nobreak\ p(\mathcal{G}^{0})\] \[=\prod_{l=0}^{L}p(\mathcal{G}^{l},\pi\mid\mathcal{G}^{l-1}) \times p(\mathcal{G}^{0})\] (1)

In other words, the generative process involves specifying the probability of the graph at each level conditioned on its parent level graph in the hierarchy. This process is iterated recursively until the lowest level, or leaf level, is reached. Here, the distribution of the root \(p(\mathcal{G}^{0})=p(\mathbf{w}^{0}=w_{0})\) can be simply estimated using the empirical distribution of the number of edges \(|\mathcal{E}|\) of graphs in the training set.

Based on the partitioned structure within each level of \(\mathcal{HG}\), the conditional generative probability \(p(\mathcal{G}^{l}\mid\mathcal{G}^{l-1})\) can be decomposed into the probability of its communities and bipartite graphs as:

\[p(\mathcal{G}^{l}\mid\mathcal{G}^{l-1}) =p(\{\mathcal{C}^{l}_{i}\leavevmode\nobreak\ \forall i\in\mathcal{V}(\mathcal{G}^{l-1})\}\leavevmode\nobreak\ \leavevmode\nobreak\ \cup\leavevmode\nobreak\ \{\mathcal{B}^{l}_{ij}\leavevmode\nobreak\ \forall(i,j)\in\mathcal{E}(\mathcal{G}^{l-1})\}\mid\mathcal{G}^{l-1})\] \[\approx\prod_{i\leavevmode\nobreak\ \in\leavevmode\nobreak\ \mathcal{V}(\mathcal{G}^{l-1})}p(\mathcal{C}^{l}_{i}\mid\mathcal{G}^{l-1}) \times\prod_{(i,j)\in\leavevmode\nobreak\ \mathcal{E}(\mathcal{G}^{l-1})}p(\mathcal{B}^{l}_{ij}\mid\mathcal{G}^{l-1})\] (2)

The approximation in this decomposition becomes an equivalence when each community \(\mathcal{C}^{l}_{i}\) or bipartite graph \(\mathcal{B}^{l}_{ij}\) is assumed to be independent of all other components in its level conditioned on the parent graph \(\mathcal{G}^{l-1}\). Since the integer-valued weights of the edges in each level can be modeled by a multinomial distribution, we can leverage the properties of multinomial distribution to prove the conditional independence of the components.

**Theorem 3.1**.: _Let the random vector \(\mathbf{w}:=[w_{e}]_{e\leavevmode\nobreak\ \in\leavevmode\nobreak\ \mathcal{E}(\mathcal{G}^{l})}\) denote the set of weights of all edges of \(\mathcal{G}^{l}\) such that their sum is \(w_{0}=\mathbf{1}^{T}\leavevmode\nobreak\ \mathbf{w}\). The joint probability of \(\mathbf{w}\) can be described by a multinomial distribution: \(\mathbf{w}\sim\text{Mu}(\mathbf{w}\mid w_{0},\boldsymbol{\theta}^{l})\). By observing that the sum of edge weights within each community \(\mathcal{C}^{l}_{i}\) and bipartite graph \(\mathcal{B}^{l}_{ij}\) are determined by the weights of their parent edges in the higher level, \(w^{l-1}_{ii}\) and \(w^{l-1}_{ij}\) respectively, we can establish that these components are conditionally independent and each of them follow a multinomial distribution:_

\[p(\mathcal{G}^{l}\mid\mathcal{G}^{l-1})\sim\prod_{i\leavevmode\nobreak\ \in\leavevmode\nobreak\ \mathcal{V}(\mathcal{G}^{l-1})}\text{Mu}([w_{e}]_{e\leavevmode\nobreak\ \in\leavevmode\nobreak\ \mathcal{C}^{l}_{i}}\mid w^{l-1}_{ii},\boldsymbol{\theta}^{l}_{ii})\times\prod_ {(i,j)\in\leavevmode\nobreak\ \mathcal{E}(\mathcal{G}^{l-1})}\text{Mu}([w_{e}]_{e\leavevmode\nobreak\ \in \leavevmode\nobreak\ \mathcal{B}^{l}_{ij}}\mid w^{l-1}_{ij},\boldsymbol{\theta}^{l}_{ij})\] (3)

_where \(\{\boldsymbol{\theta}^{l}_{ij}[e]\in[0,1],\leavevmode\nobreak\ s.t.\leavevmode \nobreak\ \mathbf{1}^{T}\boldsymbol{\theta}^{l}_{ij}=1\mid\forall \leavevmode\nobreak\ (i,j)\in\leavevmode\nobreak\ \mathcal{E}(\mathcal{G}^{l-1})\}\) are the multinomial model parameters._

Proof.: The detailed proof can be found in Appendix A.1 

Therefore, given the parent graph at a higher level, the generation of graph at its subsequent level can be reduced to generation of its partition and bipartite sub-graphs. As illustrated in figure, this decomposition enables parallel generation of the communities in each level which can be followed by predicting all bipartite sub-graphs in that level at one pass. Each of these sub-graphs corresponds to a block in the adjacency matrix, as visualized in figure Ib so the proposed hierarchical model generates adjacency matrix in a blocks-wise fashion and constructs the final graph topology.

### Community Generation

Based on the equation 3, the edge weights within each community can be jointly modeled using a multinomial distribution. Our objective is to model the generative probability of communities in each level as an autoregressive process. To accomplish this, we need to factorize the multinomial distribution accordingly. Toward this goal, we present two different approaches in the following.

**Lemma 3.2**.: _A random counting vector \(\mathbf{w}\in\mathbb{Z}_{+}^{E}\) with a multinomial distribution can be recursively decomposed into a sequence of binomial distributions as follows:_

\[\text{Mu}(\mathbf{w}_{1},\...,\mathbf{w}_{E}\mid w,\ [\theta_{1}, \...,\theta_{E}]) =\prod_{c=1}^{E}\text{Bi}(\mathbf{w}_{e}\mid w-\sum\nolimits_{i<e} \mathbf{w}_{i},\hat{\theta}_{e}),\] (4) \[\text{where: }\hat{\theta}_{e} =\frac{\theta_{e}}{1-\sum_{i<e}\theta_{i}}\]

_This decomposition is known as a stick-breaking process, where \(\hat{\theta}_{e}\) is the fraction of the remaining probabilities we take away every time and allocate to the \(e\)-th component (Linderman et al., 2015)._

This lemma enable us to model the generation of a community as an edge-by-edge autoregressive process, similar to existing algorithms such as GraphRNN (You et al., 2018) or DeepGMG (Li et al., 2018) with \(\mathcal{O}(|\mathcal{V}_{c}|^{2})\) generation steps. However, inspired by GRAN (Liao et al., 2019), a community can be generated more efficiently by generating one node at a time. This requires decomposing the generative probability of edges in a group-wise form, where the candidate edges between the \(t\)-th node and the already generated graph are grouped together. In other words, this model completes the lower triangle adjacency matrix one row at a time conditioned on the already generated sub-graph and the parent-level graph. The following theorem formally derives this decomposition for multinomial distributions.

**Theorem 3.3**.: _For a random counting vector \(\mathbf{w}\in\mathbb{Z}_{+}^{E}\) with a multinomial distribution \(\text{Mu}(\mathbf{w}\mid w,\boldsymbol{\theta})\), let's split it into \(M\) disjoint groups \(\mathbf{w}=[\mathbf{u}_{1},\...,\mathbf{u}_{M}]\) where \(\mathbf{u}_{m}\in\mathbb{Z}_{+}^{E_{m}}\,\ \sum_{m=1}^{M}E_{m}=E\), and also split the probability vector accordingly as \(\boldsymbol{\theta}=[\boldsymbol{\theta}_{1},\...,\boldsymbol{\theta}_{M}]\). Additionally, let's define sum of all variables in the \(m\)-th group by a random count variable \(\mathbf{v}_{m}:=\sum_{e=1}^{E_{m}}\mathbf{u}_{m,e}\). Then, the multinomial distribution can be factorized as a chain of binomial and multinomial distributions:_

\[\text{Mu}(\mathbf{w}=[\mathbf{u}_{1},\...,\mathbf{u}_{M}]\mid w, \boldsymbol{\theta}=[\boldsymbol{\theta}_{1},...,\boldsymbol{\theta}_{M}])= \prod_{m=1}^{M}\text{Bi}(\mathbf{v}_{m}\mid w-\sum_{i<m}\mathbf{v}_{i}, \eta_{\mathbf{v}_{m}})\text{ Mu}(\mathbf{u}_{m}\mid\mathbf{v}_{m},\boldsymbol{ \lambda}_{m}),\]

_where: \(\eta_{\mathbf{v}_{m}}=\frac{\mathbf{1}^{T}\ \boldsymbol{\theta}_{m}}{1-\sum_{i<m} \mathbf{1}^{T}\ \boldsymbol{\theta}_{i}},\ \ \boldsymbol{\lambda}_{m}=\frac{ \boldsymbol{\theta}_{m}}{\mathbf{1}^{T}\ \boldsymbol{\theta}_{m}}\)._ (5)

_Here, the probability of binomial, \(\eta_{\mathbf{v}_{m}}\), is the fraction of the remaining probability mass that is allocated to \(\mathbf{v}_{m}\), i.e. the sum of all weights in the \(m\)-th group. The vector parameter \(\boldsymbol{\lambda}_{m}\) is the normalized multinomial probabilities of all count variables in the \(m\)-th group. Intuitively, this decomposition of multinomial distribution can be viewed as a recursive stick-breaking process where at each step, first a binomial distribution is used to determine how much probability mass to allocate to the current group, and a multinomial distribution is used to distribute that probability mass among the variables in the group. The resulting distribution is equivalent to the original multinomial distribution._

Proof.: Refer to appendix A.2 for the proof. 

Let \(\hat{\mathcal{C}}_{i,t}^{l}\) denote an already generated sub-graph, at the \(t\)-th step, augmented with the set of candidate edges, from the new node, \(v_{t}(\mathcal{C}_{i}^{l})\), to its preceding node denoted by \(\hat{\mathcal{E}}_{t}(\hat{\mathcal{C}}_{i,t}^{l}):=\{(t,j)\mid j<t\}\). We collect the weights of these edges in the random vector \(\mathbf{u}_{t}:=\left|w_{e}\right|_{e\in\hat{\mathcal{E}}_{t}(\hat{\mathcal{C }}_{i,t}^{l})}\) (that is the \(t\)-th row of the lower triangle of adjacency matrix \(\hat{\mathbf{A}}_{i}^{l}\)), where the sum of the candidate edge weights is \(\mathbf{v}_{t}\). Based on theorem 3.3 the probability of \(\mathbf{u}_{t}\) can be characterized by the product of a binomial and a multinomial distribution. This process is illustrated in figure 1c. We further increase theexpressiveness of the generative network by extending this probability to a mixture model with \(K\) mixtures:

\[p(\mathbf{u}_{t}) =\sum_{k=1}^{K}\boldsymbol{\beta}_{k}^{l}\text{Bi}(\mathbf{v}_{t}|w_ {ii}^{l-1}-\sum_{i<t}\mathbf{v}_{i},\eta_{t,k}^{l})\text{Mu}(\mathbf{u}_{t}\mid \mathbf{v}_{t},\boldsymbol{\lambda}_{t,k}^{l})\] (6) \[\boldsymbol{\lambda}_{t,k}^{l} =\operatorname{softmax}\left(\operatorname{MLP}_{\boldsymbol{ \theta}}^{l}\big{(}\left[\operatorname{\Delta}\boldsymbol{h}_{\hat{\mathcal{E} }_{t}(\mathcal{G}_{i,t}^{l})}\mid\mid h_{Pa(\mathcal{G}_{i}^{l})}\right]\big{)} \right)\right)[k,:]\] (7) \[\eta_{t,k}^{l} =\operatorname{sigmoid}\left(\operatorname{MLP}_{\eta}^{l}\big{(} \left[\operatorname{pool}(\boldsymbol{h}_{\hat{\mathcal{C}}_{i,t}^{l}})\mid \mid h_{Pa(\mathcal{G}_{i}^{l})}\right]\big{)}\right)[k]\] \[\boldsymbol{\beta}^{l} =\operatorname{softmax}\left(\operatorname{MLP}_{\beta}^{l}\big{(} \left[\operatorname{pool}(\boldsymbol{h}_{\hat{\mathcal{C}}_{i,t}^{l}})\mid \mid h_{Pa(\mathcal{G}_{i}^{l})}\right]\big{)}\right)\]

Where \(\Delta\boldsymbol{h}_{\hat{\mathcal{E}}_{t}(\mathcal{G}_{i,t}^{l})}\) is a \(|\hat{\mathcal{E}}_{t}(\mathcal{G}_{i,t}^{l})|\times d_{h}\) dimensional matrix, consisting of the set of edge features \(\{\Delta h_{(t,s)}:=h_{t}-h_{s}\mid\forall\;(t,s)\;\in\;\hat{\mathcal{E}}_{t} (\mathcal{G}_{i,t}^{l})\}\), \(\boldsymbol{h}_{\hat{\mathcal{C}}_{i,t}^{l}}\) is a \(t\times d_{h}\) matrix of node features in the augmented community graph. The mixture weights are denoted by \(\boldsymbol{\beta}^{l}\). Here, the node features are learned by GNN models and the graph level representation is obtained by the \(\operatorname{addpool}()\) aggregation function. In order to produce \(K\times|\mathcal{E}_{t}(\mathcal{G}_{i}^{l})|\) dimensional matrix of multinomial probabilities, the \(\operatorname{MLP}_{\boldsymbol{\theta}}^{l}()\) network acts at the edge level, while \(\operatorname{MLP}_{\boldsymbol{\eta}}^{l}()\) and \(\operatorname{MLP}_{\beta}^{l}()\) act at the graph level to produce the binomial probabilities and \(K\) dimensional arrays for \(K\) mixture models, respectively. All of these \(\operatorname{MLP}\) networks are built by two hidden layers with \(\operatorname{ReLU}()\) activation functions.

During the generation process of each community \(\mathcal{C}_{i}^{l}\), the node features of its parent node \(h_{Pa(\mathcal{C}_{i}^{l})}\) are used as the context. This context is concatenated to the node and edge feature matrices using the operation \(\big{[}\boldsymbol{x}\mid\mid y\big{]}\), which concatenates vector \(y\) to each row of matrix \(\boldsymbol{x}\). The purpose of this context is to enrich the node and edge features by capturing long-range interactions and encoding the global structure of the graph, which is important for generating local components.

### Bipartite Generation

Once all the communities in level \(l\) are generated, the edges of all bipartite graphs at that level can be predicted simultaneously. An augmented graph \(\hat{\mathcal{G}}^{l}\) composed of all the communities, \(\{\mathcal{C}_{i}^{l}\;\forall i\in\mathcal{V}(\mathcal{G}^{l-1})\}\), and the candidate edges of all bipartites, \(\{\mathcal{B}_{ij}^{l}\;\;\forall(i,j)\in\mathcal{E}(\mathcal{G}^{l-1})\}\), is used as the input of a GNN to obtain node and edge features. We similarly extend the multinomial distribution of a bipartite, 12, using a mixture model to express its generative probability:

\[p(\mathbf{w}:=\hat{\mathcal{E}}(\mathcal{B}_{ij}^{l}))=\sum_{k=1 }^{K}\boldsymbol{\beta}_{k}^{l}\text{Mu}(\mathbf{w}\mid w_{ij}^{l-1}, \boldsymbol{\theta}_{ij,k}^{l})\] (8) \[\boldsymbol{\theta}_{ij,k}^{l} =\operatorname{softmax}\left(\operatorname{MLP}_{\boldsymbol{ \theta}}^{l}\big{(}\big{[}\Delta\boldsymbol{h}_{\hat{\mathcal{E}}(\mathcal{B}_ {ij}^{l})}\mid\mid\Delta h_{Pa(\mathcal{B}_{ij}^{l})}\big{]}\big{)}\right)[k,:]\] \[\boldsymbol{\beta}^{l} =\operatorname{softmax}\left(\operatorname{MLP}_{\beta}^{l}\big{(} \left[\operatorname{pool}(\Delta\boldsymbol{h}_{\hat{\mathcal{E}}(\mathcal{B}_ {ij}^{l})})\mid\mid\Delta h_{Pa(\mathcal{B}_{ij}^{l})}\right]\big{)}\right)\]

where the random vector \(\mathbf{w}:=\left[w_{e}\right]_{e\in\;\hat{\mathcal{E}}(\mathcal{B}_{ij}^{l})}\) is the set of weights of all candidate edges in bipartite \(\mathcal{B}_{ij}^{l}\) and \(\Delta\boldsymbol{h}_{Pa(\mathcal{B}_{ij}^{l})}\) are the parent edge features of the bipartite graph.

Node Feature Encoding:To encode node features, we extend GraphGPS proposed by [2]GraphGPS combines local message-passing with global attention mechanism and uses positional and structural encoding for nodes and edges to construct a more expressive and a scalable graph transformer (GT) [3]. To apply GraphGPS on augmented graphs, we use distinct initial edge features to distinguish augmented (candidate) edges from real edges. Furthermore, for bipartite generation, the attention scores in the Transformers of the augmented graph \(\hat{\mathcal{G}}^{l}\) are masked to restrict attention only to connected communities. The details of model architecture are provided in appendix Related Work

In order to deal with hierarchical structures in molecular graphs, a generative process was proposed by Jin et al. (2020) which recursively selects motifs, the basic building blocks, from a set and predicts their attachment to the emerging molecule. However, this method requires prior domain-specific knowledge and relies on molecule-specific graph motifs. Additionally, the graphs are only abstracted into two levels, and component generation cannot be performed in parallel. In Kuznetsov and Polykovskiy (2021), a hierarchical normalizing flow model for molecular graphs was introduced, where new molecules are generated from a single node by recursively dividing each node into two. However, the merging and splitting of pairs of nodes in this model is based on the node's neighborhood, and do not consider the diverse community structure of graphs, therefore the hierarchical generation of this model is inherently limited.

## 5 Experiments

In our empirical studies, we compare the proposed hierarchical graph generative network against state-of-the-art autoregressive models: GRAN and GraphRNN models, diffusion models: DiGress (Vignac et al., 2022) and GDSS (O et al., 2022) and a GAN-based model: SPECTRE (Martinkus et al., 2022), on a range of synthetics and real datasets of various sizes.

Datasets:We used 4 different benchmark graph datasets: (1) the synthetic _Stochastic Block Model (SBM)_ dataset consisting of 200 graphs with 2-5 communities each with 20-40 nodes, used in a previous work (Martinkus et al., 2022); (2) the _Protein_ including 918 protein graphs, each has 100 to 500 nodes representing amino acids that are linked if they are closer than 6 Angstroms (Dobson and Doigl, 2003), (3) the _Enzyme_ that has 587 protein graphs of 10-125 nodes, representing protein tertiary structures of the enzymes from the BRENDA database (Schomburg et al., 2004) and (4) the _Ego_ dataset containing 757 3-hop ego networks with 50-300 nodes extracted from the CiteSeer dataset, where nodes represent documents and edges represent citation relationships (Sen et al., 2008).

Graph PartitioningDifferent algorithms approach the problem of graph partitioning (clustering) using various clustering quality functions. Two commonly used families of such metrics are modularity and cut-based metrics (Sitsulin et al., 2020). Although optimizing modularity metric is an NP-hard problem, it is well-studied in the literature and several graph partitioning algorithm based on this metric have been proposed. For example, the Louvain algorithm (Blondel et al., 2008) starts with each node as its community and then repeatedly merges communities based on the highest increase in modularity until no further improvement can be made. This heuristic algorithm is computationally efficient and scalable to large graphs for community detection. Moreover, a spectral relaxation of modularity metrics has been proposed in (Newman, 2006) which results in an analytically solution for graph partitioning. Additionally, an unsupervised GNN-based pooling method inspired by this spectral relaxation was proposed for partitioning graphs with node attributes (Sitsulin et al., 2020). As the modularity metric is based on the graph structure, it is well-suited for our problem. Therefore, we employed the Louvain algorithm to hierarchically cluster the graph datasets in our experiments and then spliced out the intermediate levels to achieve HGs with uniform depth of \(L=2\).

Model ArchitectureIn the experiments, the GNN models consist of \(8\) layers of GraphGPS layers (Rampasek et al., 2022). The input node feature of GNNs is augmented with positional and structural encoding, where the first \(8\) eigenvectors corresponding to the smallest non-zero eigenvalues of the Laplacian and diagonal of the random-walk matrix up to \(8\)-steps are used. Each level has its own GNN and output models. The details of the model architecture are presented in AppendixB andC

We conducted experiments using the proposed hierarchical graph generative network (HiGen) model with two variants for the output distribution of the leaf edges: 1) **HiGen**: the probability of the community edges' weights at the leaf level are modeled by mixture of Bernoulli, using \(\mathrm{sigmoid}()\) activation in equationB since the leaf levels in our experiments have binary edges weights, while higher levels use mixture of multinomials. 2)**HiGen-m**: the model uses a mixture of multinomial distributions (6) to describe the output distribution for all levels. In this case, we observed that modeling the probability parameters of edge weights of the leaf level, denoted as \(\boldsymbol{\lambda}_{t,k}\) in (7), by a multi-hot activation function, defined as \(\sigma(\boldsymbol{z})_{i}:=\mathrm{sigmoid}(z_{i})/\sum_{j=1}^{K}\mathrm{ sigmoid}(z_{j})\) where \(\sigma:\mathbb{R}^{K}\rightarrow(K-1)\)-simplex, provided slightly better performance than the standard \(\mathrm{softmax}()\) function. However, for both HiGen and HiGen-m, the probabilities of the integer-valued edges at the higher levels are still modeled by the standard \(\mathrm{softmax}()\) function2

Footnote 2: As the leaf levels have binary edge weights while the sum of their weights is determined by their parent edge, a possible extension to this work could be using the cardinality potential model Hajimirsadeghi et al. (2015), which is derived to model the distribution over the set of binary random variables, to model the edge weight at the leaf level.

For training, HiGen models used the Adam optimizer Kingma and Ba (2014) with a learning rate of 5e-4 and its default settings of \(\beta_{1}=0.9,\ \beta_{2}=0.999\) and \(\epsilon\)=1e-8.

MetricsTo evaluate the graph generative models, we adopt the approach proposed in Liu et al. (2019). (Liao et al., 2019), which compares the distributions of four different graph statistics between the ground truth and generated graphs: (1) degree distributions, (2) clustering coefficient distributions, (3) the number of occurrences of all orbits with four nodes, and (4) the spectra of the graphs by computing the eigenvalues of the normalized graph Laplacian. The first three metrics capture local graph statistics, while the spectra represents global structure. The maximum mean discrepancy (MMD) score over these statistics are used as the metrics. While Liu et al. (2019) computed MMD scores using the computationally expensive Gaussian earth mover's distance (EMD) kernel, Liao et al. (2019) proposed using the total variation (TV) distance as an alternative measure. TV distance is much faster and still consistent with the Gaussian EMD kernel. Most recently, O'Bray et al. (2021) suggested using other efficient kernels such as an RBF kernel, or a Laplacian kernel, or a linear kernel. Additionally, Thompson et al. (2022) proposed new evaluation metrics for comparing graph sets using a random-GNN approach where GNNs are employed to extract meaningful graph features. However, in this work, we follow the experimental setup and evaluation metrics of Liao et al. (2019), except for the enzyme dataset where we use a Gaussian EMD kernel to be consistent with the results reported in O et al. (2022). GNN-based performance metrics of HiGen model are also reported in appendix D.2

\begin{table}
\begin{tabular}{l||c c c c|c c c c} \hline \hline  & \multicolumn{4}{c}{_Stochastic block model_} & \multicolumn{4}{c}{_Protein_} \\ Model & Deg. \(\downarrow\) & Clus. \(\downarrow\) & Orbit\(\downarrow\) & Spec. \(\downarrow\) & Deg. \(\downarrow\) & Clus. \(\downarrow\) & Orbit\(\downarrow\) & Spec. \(\downarrow\) \\ \hline Training set & 0.0008 & 0.0332 & 0.0255 & 0.0063 & 0.0003 & 0.0068 & 0.0032 & 0.0009 \\ GraphRNN & 0.0055 & 0.0584 & 0.0785 & 0.0065 & 0.0040 & 0.1475 & 0.5851 & 0.0152 \\ GRAN & 0.0113 & 0.0553 & 0.0540 & 0.0054 & 0.0479 & 0.1234 & 0.3458 & 0.0125 \\ SPECTRE & 0.0015 & 0.0521 & 0.0412 & 0.0056 & 0.0056 & 0.0043 & 0.0267 & 0.0052 \\ DGress & **0.0013** & **0.0498** & 0.0433 & - & - & - & - & - \\ HiGen-m & 0.0017 & 0.0503 & 0.0604 & 0.0088 & 0.0041 & 0.109 & 0.0472 & 0.0061 \\ HiGen & 0.0019 & **0.0498** & **0.0352** & **0.0046** & **0.0012** & **0.0435** & **0.0234** & **0.0025** \\ \hline \hline  & \multicolumn{4}{c}{_Enzyme_} & \multicolumn{4}{c}{_Ego_} \\ Model & Deg. \(\downarrow\) & Clus. \(\downarrow\) & Orbit \(\downarrow\) & Deg. \(\downarrow\) & Clus. \(\downarrow\) & Orbit \(\downarrow\) & Spec. \(\downarrow\) \\ \hline Training set & 0.0011 & 0.0025 & 3.7e-4 & 2.2e-4 & 0.010 & 0.012 & 1.4e-3 \\ GraphRNN & 0.017 & 0.062 & 0.046 & 0.024 & 0.34 & 0.14 & 0.089 \\ GRAN & 0.054 & 0.087 & 0.033 & 0.032 & 0.17 & 0.026 & 0.046 \\ GDSS & 0.026 & 0.061 & 0.009 & - & - & - & - \\ HiGen-m & 0.027 & 0.157 & 1.2e-3 & 0.011 & 0.063 & **0.021** & 0.013 \\ HiGen & **0.012** & **0.038** & **7.2e-4** & **1.9e-3** & **0.049** & 0.029 & **0.004** \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of generation metrics on benchmark datasets. The baseline results for SBM and Protein graphs are obtained from Marinkus et al. (2022). (Vignac et al., 2022), and the results for enzyme graphs (except for GRAN, which we implemented) are obtained from Jo et al. (2022), while we implemented them for Ego. “-”: not applicable due to resource issue or not reported in the reference papers.

The performance metrics of the proposed HiGen models are reported in Table 1 with generated graph samples presented in Figure2 The results demonstrate that HiGen effectively captures graph statistics and achieves state-of-the-art on all the benchmarks graphs across various generation metrics. This improvement in both local and global properties of the generated graphs highlights the effectiveness of the hierarchical graph generation approach, which models communities and cross-community interactions separately. The visual comparisons of graph samples generated by the HiGen models, as well as the experimental evaluation of different node ordering and partitioning functions, are presented in Appendix.2

Footnote 2: It is worth noting that all node permutations do not result in distinctive adjacency matrices due to the automorphism property of graphs [11, 12]. Therefore, the number of node permutations provides an upper bound rather than an exact count.

## 6 Discussion

The GRAN model can generate graphs one block of nodes at a time in an autoregressive fashion where the block size is fixed and nodes are assigned to blocks based on an ordering. However, the model's performance deteriorates as the block size increases, since adjacent nodes in an ordering may not be relevant and may belong to different clusters. Additionally, intra-block connections are not modeled separately. In contrast, our proposed method generates blocks of nodes within each community that have strong relationships and then predicts the cross-links between communities using a separate model. As a result, this approach enables the model to capture both local relationships between nodes within a community and global relationships across communities, resulting in improved expressiveness of the graph generative model.

The proposed hierarchical model allows for highly parallelizable training and generation. Specifically, let \(n_{c}\) be the size of the largest graph cluster, then, it only requires \(\mathcal{O}(n_{c}\log n)\) sequential steps to generate a graph of size \(n\).

Node ordering sensitivityThe predefined ordering of dimensions can be crucial for training autoregressive (AR) models [20], and this sensitivity to node orderings is particularly pronounced in autoregressive graph generative model [11, 12]. However, in the proposed approach, the graph generation process is divided into the generation of multiple small partitions, performed sequentially across the levels, rather than generating the entire graph by a single AR model. Therefore, given an ordering for the parent level, the graph generation depends only on the permutation of the nodes within the graph communities rather than the node ordering of the entire graph. In other words, the proposed method is invariant to a large portion of possible node permutations, and therefore the set of distinctive adjacency matrices is much smaller in HiGen. For example, the node ordering \(\pi_{1}=[v_{1},v_{2},v_{3},v_{4}]\) with clusters \(\mathcal{V}_{\mathcal{G}_{1}}=\{v_{1},v_{2}\}\) and \(\mathcal{V}_{\mathcal{G}_{2}}=\{v_{3},v_{4}\}\) has a similar hierarchical graph as \(\pi_{2}=[v_{1},v_{3},v_{2},v_{4}]\), since the node ordering within the communities is preserved at all levels. Formally, let \(\{\mathcal{C}_{i}^{l}\;\;\forall i\in\mathcal{V}_{\mathcal{G}^{l-1}}\}\) be the set of communities at level \(l\) produced by a deterministic partitioning function, where \(n_{i}^{l}=|\mathcal{V}(\mathcal{C}_{i}^{l})|\) denotes the size of each partition. The upper bound on the number of distinct node orderings in an HG generated by the proposed process is then reduced to \(\prod_{i=1}^{L}\prod_{i}n_{i}^{l}!\).

## 7 Conclusion

The proposed HiGen framework generates graphs in a hierarchical and block-wise manner, leveraging the inherent hierarchical structure present in real-world graphs. By decomposing the generation process into separate and parallel generation of communities and bipartite sub-graphs, it combines the benefits of one-shot and AR graph generative models. Experimental results on benchmark datasets demonstrate that HiGen achieves state-of-the-art performance across various generation metrics. The hierarchical and block-wise generation strategy of HiGen enables scaling up graph generative models to large and complex graphs, opening up opportunities to extend it to newer generative paradigms such as diffusion models.

## References

* [1] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. _science_, 286(5439):509-512, 1999.
* [2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. _Journal of machine Learning research_, 3(Jan):993-1022, 2003.
* [3] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. _Journal of statistical mechanics: theory and experiment_, 2008(10):P10008, 2008.
* [4] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zugner, and Stephan Gunnemann. Netgan: Generating graphs via random walks. _arXiv preprint arXiv:1803.00816_, 2018.
* [5] Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Probabilistic modeling of node sequence for graph generation. _arXiv preprint arXiv:2106.06189_, 2021.
* [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. _arXiv preprint arXiv:2009.14794_, 2020.
* [7] Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling for sparse graphs. In _International Conference on Machine Learning_, pp. 2302-2312. PMLR, 2020.
* [8] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. _arXiv preprint arXiv:1805.11973_, 2018.
* [9] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. _Journal of molecular biology_, 330(4):771-783, 2003.
* [10] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. _arXiv preprint arXiv:2012.09699_, 2020.
* [11] Paul Erdos and Alfred Renyi. On the evolution of random graphs. _Publ. Math. Inst. Hung. Acad. Sci_, 5(1):17-60, 1960.
* [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [13] Kilian Konstantin Haefeli, Karolis Martinkus, Nathanael Perraudin, and Roger Wattenhofer. Diffusion models for graphs benefit from discrete state spaces. _arXiv preprint arXiv:2210.01549_, 2022.
* [14] Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, and Greg Mori. Visual recognition by counting instances: A multi-instance cardinality potential kernel. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2596-2605, 2015.
* [15] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In _International conference on machine learning_, pp. 4839-4848. PMLR, 2020.
* [16] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In _International Conference on Machine Learning_, pp. 10362-10383. PMLR, 2022.
* [17] Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel Duckworth. Invertible convolutional flow. _Advances in Neural Information Processing Systems_, 32, 2019.
* [18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [20] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [21] Maksim Kuznetsov and Daniil Polykovskiy. Molgrow: A graph normalizing flow for hierarchical molecular generation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 8226-8234, 2021.
* [22]* Leskovec et al. [2010] Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. Kronecker graphs: An approach to modeling networks. _Journal of Machine Learning Research_, 11(Feb):985-1042, 2010.
* Li et al. [2018] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of graphs. _arXiv preprint arXiv:1803.03324_, 2018.
* Liao et al. [2019] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks. _Advances in neural information processing systems_, 32, 2019.
* Linderman et al. [2015] Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made easy: Stick-breaking with the polya-gamma augmentation. _Advances in Neural Information Processing Systems_, 28, 2015.
* Liu et al. [2019] Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows, 2019.
* Ma et al. [2018] Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via regularizing variational autoencoders. _arXiv preprint arXiv:1809.02630_, 2018.
* Savva et al. [2019] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019.
* Martinkus et al. [2022] Karolis Martinkus, Andreas Loukas, Nathanael Perraudin, and Roger Wattenhofer. Spectre: Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators. In _International Conference on Machine Learning_, pp. 15159-15179. PMLR, 2022.
* Mirhoseini et al. [2021] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A graph placement methodology for fast chip design. _Nature_, 594(7862):207-212, 2021.
* Newman [2006a] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. _Physical review E_, 74(3):036104, 2006a.
* Newman [2006b] Mark EJ Newman. Modularity and community structure in networks. _Proceedings of the national academy of sciences_, 103(23):8577-8582, 2006b.
* O'Bray et al. [2021] Leslie O'Bray, Max Horn, Bastian Rieck, and Karsten Borgwardt. Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions. _arXiv preprint arXiv:2106.01098_, 2021.
* van den Oord et al. [2016] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.
* Ramakrishnan et al. [2021] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Rampasek et al. [2022] Ladislav Rampasek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. _Advances in Neural Information Processing Systems_, 35:14501-14515, 2022.
* Reed et al. [2017] Scott Reed, Aaron Oord, Nal Kalchbrenner, Sergio Gomez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, and Nando Freitas. Parallel multiscale autoregressive density estimation. In _International Conference on Machine Learning_, pp. 2912-2921. PMLR, 2017.
* Schomburg et al. [2004] Ida Schomburg, Antje Chang, Christian Ebeling, Marion Gremse, Christian Heldt, Gregor Huhn, and Dietmar Schomburg. Brenda, the enzyme database: updates and major new developments. _Nucleic acids research_, 32 (suppl_1):D431-D433, 2004.
* Sen et al. [2008] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* Siegrist [2017] Kyle Siegrist. _Probability, Mathematical Statistics, Stochastic Processes_. LibreTexts, 2017. URL https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)* Simonovsky and Komodakis [2018] Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using variational autoencoders. _arXiv preprint arXiv:1802.03480_, 2018.
* Thompson et al. [2022] Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham W Taylor. On evaluation metrics for graph generative models. _arXiv preprint arXiv:2201.09871_, 2022.
* Tsitsulin et al. [2020] Anton Tsitsulin, John Palowitch, Bryan Perozzi, and Emmanuel Muller. Graph clustering with graph neural networks. _arXiv preprint arXiv:2006.16904_, 2020.
* Vignac et al. [2022] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. _arXiv preprint arXiv:2209.14734_, 2022.
* Vinyals et al. [2015] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. _arXiv preprint arXiv:1511.06391_, 2015.
* Yang et al. [2019] Carl Yang, Peijve Zhuang, Wenhan Shi, Alan Luu, and Pan Li. Conditional structure generation through graph variational generative adversarial nets. _Advances in neural information processing systems_, 32, 2019.
* You et al. [2018] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep auto-regressive models. In _ICML_, pp. 5694-5703, 2018.