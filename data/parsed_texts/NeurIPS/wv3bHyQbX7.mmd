# Subject-driven Text-to-Image Generation via Apprenticeship Learning

Wenhu Chen\({}^{\spadesuit}\)

Hexiang Hu\({}^{\spadesuit*}\)

Yandong Li\({}^{\heartsuit}\)

Nataniel Ruiz\({}^{\heartsuit}\)

Xuhui Jia\({}^{\heartsuit}\)

Ming-Wei Chang\({}^{\spadesuit}\)

William W. Cohen\({}^{\spadesuit}\)

\({}^{\spadesuit}\)Google Deepmind

Google Research

{wenhuchen,hexiang,mingweichang,wohen}@google.com

Core Contribution

###### Abstract

Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an "expert model" for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with _in-context_ learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by _apprenticeship learning_, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice SuTI model then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, human evaluation shows that SuTI significantly outperforms other existing models.

Figure 1: We train _a single_SuTI _model_ to generate novel scenes faithfully reflecting given subjects (unseen in training, specified only by 3-5 in-context text\(\rightarrow\)image demonstrations), without any optimization.

Introduction

Recent text-to-image generation models [1] have shown great progress in generating highly realistic, accurate, and diverse images from a given text prompt. These models are pre-trained on web-crawled image-text pairs like LAION [2] with autoregressive backend models [3; 4] or diffusion backend models [5; 1]. Though achieving unprecedented success in generating highly accurate images, these models are not able to customize to a given subject, like a specific dog, shoe, backpack, etc. Therefore, _subject-driven text-to-image generation_, the task of generating highly customized images with respect to a target subject, has attracted significant attention from the community. Subject-driven image generation is related to text-driven image editing but often needs to perform more sophisticated transformations to source images (e.g., rotating the view, zooming in/out, changing the pose of subject, etc.) so existing image editing methods are generally not suitable for this new task.

Current subject-driven text-to-image generation approaches are slow and expensive. While different approaches like DreamBooth [6], Imagic [7], and Textual Inversion [8] have been proposed, they all require fine-tuning specific models for a given subject on one or a few demonstrated examples, which typically takes at least 10-20 minutes2 to specialize the text-to-image model checkpoint for the given subjects. These approaches are time-consuming as they require back-propagating gradients over the entire model for hundreds or even thousands of steps per customization. Moreover, they are space-consuming as they require storing a subject-specific checkpoint per subject. To avoid the excessive cost, Re-Imagen [9] proposed a retrieval-augmented text-to-image framework to train a subject-driven generation model in a weakly-supervised fashion. Since the retrieved neighbor images are not guaranteed to contain the same subjects, the model does not perform as good as DreamBooth [6] for the task of subject-driven image generation.

Footnote 2: Running on A100 according to public colab: https://huggingface.co/sd-dreambooth-library and https://huggingface.co/docs/diffusers/training/text_inversion.

To avoid excessive computation and memory costs, we propose to train a single subject-driven text-to-image generation model that can perform on-the-fly subject customization. Our method is dubbed Subject-driven Text-to-Image generator (SuTI), which is trained with a novel _apprenticeship learning_ algorithm. Unlike standard apprenticeship learning which only focuses on learning from one expert, our apprentice model imitates the behaviors of a massive number of specialized expert models. After such training, SuTI can instantly adapt to unseen subjects and unseen or even compositional descriptions with only 3-5 in-context demonstrations within 30 seconds (on a Cloud TPU v4).

Figure 2 presents a conceptual diagram of the learning and data preparation pipeline. We first group the images in WebLI [10] by their source URL to form tiny image clusters because images from the same URL are likely to contain the same subject. We then performed extensive image-to-image and image-to-text similarity filtering to retain image clusters that contain highly similar content. For each subject image cluster, we fine-tuned an expert model to specialize in the given subject. Then, we use the fine-tuned experts to synthesize new images given unseen creative captions proposed by large language models. However, the tuned expert models are not perfect and prone to errors, therefore, we adopt a quality validation metric to filter out a large portion of degraded outputs. The remaining high-quality images are provided as a training signal to teach the apprentice model SuTI to perform subject-driven image generation with high fidelity. During inference, the trained SuTI can attend to a few in-context demonstrations to synthesize new images on the fly.

We evaluate SuTI on various tasks such as subject re-contextualization, attribute editing, artistic style transfer, and accessorization. We compare SuTI with existing models on DreamBench [6], which contains diverse subjects from wide categories accompanied by some prompt templates. We compute the CLIP-I/CLIP-T and DINO scores of SuTI's generated images on this dataset and compare them with DreamBooth. The results indicate that SuTI can outperform DreamBooth while having 20x faster inference speed and significantly less memory footprint.

Figure 2: Conceptual Diagram of the Learning Pipeline

Further, we manually created 220 diverse and compositional prompts regarding the subjects in DreamBench for human evaluation, which is dubbed the DreamBench-v2 dataset. We then comprehensively compare with other baselines like InstructPix2Pix [11], Null-Text Inversion [12], Imagic [7], Textual Inversion [8], Re-Imagen [9], and DreamBooth [6] on DreamBench-v2. Our human evaluation results indicate that SuTI is 5% higher than DreamBooth and at least 30% better than the other baseline in terms of human evaluation. We conduct detailed fine-grained analysis and found that SuTI's textual alignment is significantly better than DreamBooth while its subject alignment is slightly better than DreamBooth. However, DreamBooth's outputs are still better in the photorealism aspect, especially in terms of fine-grained detail preservation.

We summarize our contributions in the following aspects:

* We introduce the SuTI model, a subject-driven text-to-image generator that performs instant and customized generation for a visual subject with few (image, text) exemplars, _all in context_.
* We employ the _apprenticeship learning_ to train one single apprentice SuTI model to imitate half a million fine-tuned subject-specific experts on a large-scale seed dataset, leading to a generator model that generalizes to unseen subjects and unseen compositional descriptions.
* We perform a comprehensive set of automatic and human evaluations to show the capability of our model on generating highly faithful and creative images on DreamBench and DreamBench-v2.

To facilitate the reproducibility of our model performance, we release the SuTI model API as a Google Cloud Vertex AI model service, under the production name 'Instant tuning'3.

Footnote 3: Generally available at https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-model

## 2 Preliminary

In this section, we introduce the key concepts and notations about subject-driven image-text data, then discuss the basics of text-to-image diffusion models.

**Diffusion Models.** Diffusion models [13] are latent variable models, parameterized by \(\Theta\), in the form of \(p_{\Theta}(\bm{x}_{0}):=\int p_{\Theta}(\bm{x}_{0:T})d\bm{x}_{1:T}\), where \(\bm{x}_{1},\cdots,\bm{x}_{T}\) are "noised" latent versions of the input image \(\bm{x}_{0}\sim q(\bm{x}_{0})\). Note that the dimensionality of both latents and the image is the same throughout the entire process, with \(\bm{x}_{0:T}\in\mathbb{R}^{d}\) and \(d\) equals the product of <height, width, # of channels>. The process that computes the posterior distribution \(q(\bm{x}_{1:T}|\bm{x}_{0})\) is also called the forward (or diffusion) process, and is implemented as a predefined Markov chain that gradually adds Gaussian noise to the data according to a schedule \(\beta_{t}\):

\[q(\bm{x}_{1:T}|\bm{x}_{0})=\prod_{t=1}^{T}q(\bm{x}_{t}|\bm{x}_{t-1});\quad q( \bm{x}_{t}|\bm{x}_{t-1}):=\mathcal{N}(\bm{x}_{t};\sqrt{1-\beta_{t}}\bm{x}_{t- 1},\beta_{t}\bm{I})\] (1)

Diffusion models are trained to learn the image distribution by reversing the diffusion Markov chain. Theoretically, this reduces to learning to denoise \(\bm{x}_{t}\sim q(\bm{x}_{t}|\bm{x}_{0})\) into \(\bm{x}_{0}\), with a time re-weighted square error loss--see [14] for the complete proof:

\[\mathbb{E}_{(\bm{x}_{0},\bm{c})\sim D}\{\mathbb{E}_{\bm{c},t}[w_{t}\cdot|| \hat{\bm{x}}_{\theta}(\bm{x}_{t},\bm{c})-\bm{x}_{0}||_{2}^{2}]\}\] (2)

where \(D\) is the training dataset containing (image, condition) = \((\bm{x}_{0},\bm{c})\) pairs, the condition normally refers to the input text prompt. In practice, \(w_{t}\) can be simplified as 1 according to [14; 15].

**Subject-Driven Text-to-Image Generation.** Existing subject-driven generation models [6; 16; 8] often fine-tune a pre-trained text-to-image diffusion model on a set of provided demonstrations \(\mathbb{C}_{s}\) about a specific subject \(s\). Formally, such demonstration contains a set of text and image pairs \(\mathbb{C}_{s}=\{(\bm{x}_{k},\bm{c}_{k})\}_{k}^{\mathbb{K}_{s}}\), centered around the subject \(s\). Images \(\bm{x}_{k}\) contains images of the same subject \(s\), while \(\bm{c}_{s}\) is a short description of images \(\bm{x}_{k}\). DreamBooth [6] also requires an additional \(\bar{\mathbb{C}}_{s}\), which contains images about different subjects of the same category as \(s\) for prior preservation. To obtain a customized diffusion model \(\hat{\bm{x}}_{\theta_{s}}(\bm{x}_{t},\bm{c})\), we need to optimize the following loss function:

\[\theta_{s}=\operatorname*{arg\,min}_{\theta}\mathbb{E}_{(\bm{x}_{0},\bm{c}) \sim\mathbb{C}_{s}\cup\bar{\mathbb{C}}_{s}}\{\mathbb{E}_{\bm{c},t}[||\hat{\bm{ x}}_{\theta}(\bm{x}_{t},\bm{c})-\bm{x}_{0}||_{2}^{2}]\}\] (3)

The customized diffusion model \(\hat{\bm{x}}_{\theta_{s}}(\bm{x}_{t},\bm{c})\) has shown impressive capabilities to generate highly faithful images of the specified subject \(s\).

## 3 Apprenticeship Learning from Subject-specific Experts

**Notation.** Figure 3 presents the concrete workflow of learning. Our method follows apprenticeship learning [17] with two major component, _i.e_., the expert diffusion models \(\hat{\bm{x}}_{\theta_{s}}(\bm{x}_{t},\bm{c})\) parameterized by \(\theta_{s}\) regarding subject \(s\in\mathbb{S}\) and apprentice diffusion model \(\hat{\bm{x}}_{\Theta}(\bm{x}_{t},\bm{c},\mathbb{C}_{s})\) parameterized by \(\Theta\). The apprentice model takes an additional set of image-text demonstrations \(\mathbb{C}_{s}\) as input. We use \(\mathbb{S}\) to denote the superset of subjects we include in the training set.

**Dataset.** The training set \(\mathcal{D}_{\mathbb{S}}\) contains a collection of \(\{\mathbb{C}_{s},\bm{p}_{s}\}_{s\in\mathbb{S}}\), where each entry contains an image-text cluster \(\mathbb{C}_{s}\) accompanied by an unseen prompt \(\bm{p}_{s}\). The image-text cluster \(\mathbb{C}_{s}\) contains a set of 3-10 image-text pairs. The unseen prompt is an imaginary caption proposed by PaLM [18]. For example, if \(\texttt{c}\) is 'a photo of berry bowl', then \(\bm{p}_{s}\) would be an imaginary caption like 'a photo of berry bowl floating on the river'. We describe the dataset construction process to section 4.

**Learning.** To obtain an expert \(\hat{\bm{x}}_{\theta_{s}}(\bm{x}_{t},\bm{c})\) on a subject \(s\), we fine-tune a pre-trained diffusion model [1] on the image cluster \(\mathbb{C}_{s}\) with the denoising loss as:

\[\theta_{s}=\operatorname*{arg\,min}_{\theta}\mathbb{E}_{(\bm{x}_{s},\bm{c}) \sim\mathbb{C}_{s}}\{\mathbb{E}_{\bm{c},t}[||\hat{\bm{x}}_{\theta}(\bm{x}_{t}, \bm{c})-\bm{x}_{s}||_{2}^{2}]\}\] (4)

where \(\bm{x}_{t}\sim q(\bm{x}_{t}|\bm{x}_{s})\). The training is similar to Eqn. 3 except that we do not have negative examples for prior preservation because finding the negative examples from the same class is expensive.

Once an expert model is trained, we use it to sample images \(\bm{y}_{s}\) for the unseen text description \(\bm{p}_{s}\) to guide the apprentice SuTI model. We gather the outputs from the massive amount of expert models and then use CLIP filtering to construct a dataset \(G\). Similarly, we fine-tune the apprentice model \(\hat{\bm{x}}_{\Theta}(\bm{x}_{t},\bm{p}_{s},\mathbb{C}_{s})\) with the denoising loss on the pseudo target generated by the expert:

\[\Theta=\operatorname*{arg\,min}_{\Theta}\mathbb{E}_{(\bm{y}_{s},\bm{p}_{s}, \mathbb{C}_{s})\sim G}\{\mathbb{E}_{\bm{c},t}[||\hat{\bm{x}}_{\Theta}(\bm{x}_ {t},\bm{p}_{s},\mathbb{C}_{s})-\bm{y}_{s}||_{2}^{2}]\}\] (5)

where \(\bm{x}_{t}\sim q(\bm{x}_{t}|\bm{y}_{s})\), and the training triples \((\bm{y}_{s},\bm{p}_{s},\mathbb{C}_{s})\) are drawn from \(G\).

**Algorithm.** We formally introduce our learning algorithm in the Algorithm 1. To improve the training efficiency, we use distributed training algorithm to accelerate the training process. At each training step, we randomly sample a batch \(\{B_{s_{i}}\}_{i=1}^{\texttt{R}}\) of size \(\texttt{K}\) from the dataset \(\mathcal{D}_{\mathbb{S}}\), with \(B_{s_{i}}=(\mathbb{C}_{s_{i}},\bm{p}_{s_{i}})\). We then fine-tune \(\texttt{K}\) expert models separately w.r.t. Eqn. 4 in parallel, across \(\texttt{K}\) different TPU cores. For every subject \(s\) inside the batch \(B_{s}\), we use the corresponding expert model \(\theta_{s}\) to synthesize the image \(\bm{y}_{s}\) given the unseen prompt \(\bm{p}_{s}\). As not all expert models can generate highly faithful images, we introduce a quality assurance step to validate the synthesized images. Particularly, we measure the quality of an expert's generation by the delta CLIP score [19]\(\Delta(\bm{y}_{s},\mathbb{C}_{s},\bm{p}_{s})\), which is used to decide whether a sample should be included in the dataset \(G\). This ensures the high quality of the text-to-image training signal for SuTI. Specifically, the delta CLIP score is computed as the increment of CLIP score of \(\bm{y}_{s}\) over the demonstrated images \(\bm{x}\in\mathbb{C}_{s}\):

\[\Delta(\bm{y}_{s},\mathbb{C}_{s},\bm{p}_{s})=\text{CLIP}(\bm{y}_{s},\bm{p}_{s} )-\max_{\bm{x}\in\mathbb{C}_{s}}\text{CLIP}(\bm{x},\bm{p}_{s})\] (6)

We then feed \(G\) as a training batch to update the parameter \(\Theta\) of the apprentice model using Eqn. 5. In all our experiments, we set \(\texttt{K}=400\), with each TPU core training an expert model.

**Inference.** To perform subject-driven text-to-image generation, the trained SuTI takes 3-5 image-text pairs as the demonstration to generate new images based on the given text description. No

Figure 3: Overview of the apprentice learning pipeline for SuTI. Left part shows the customization procedure for expert models, and the right parts shows the SuTI model that imitates the behaviors of differently customized experts. Note that this framework can cope with expert models of _arbitrary architecture and model family_.

optimization is needed during inference time. The only overhead of SuTI is the cost of encoding these 3-5 image-text pairs and the attention computation, which is more affordable. Our inference speed is roughly in the same order as the original text-to-image generator [1].

## 4 Mining and Generating Subject-driven Text-to-Image Demonstrations

In this section, we discuss how we created the seed dataset \(\mathcal{D}_{\mathbb{S}}\) by mining images and text over the Internet. We construct the seed dataset using the WebLI [10; 20] dataset. Particularly, we derive our initial image cluster via subsampling the Episodic WebLI data [20], which grouped Web images from the same URL. Then we filter the clusters to ensure high intra-cluster visual similarity, using image matching models. The filtered set of image-text clusters is then denoted \(\{\mathbb{C}_{s}\}_{s\in\mathbb{S}}\).

After obtaining the subject-driven image clusters, we further prompt a large language model [18] to generate a description about the subject, with the goal of creating descriptions of plausible imaginary visual scenes. The generating instances of the descriptions will require skills like _subject re-contextualization_, _attribute editing_, _artistic style transfer_, and _accessorization_. We denote the generated unseen captions as \(p_{s}\). Together with \(\mathbb{C}_{s}\), this forms the final dataset \(\mathcal{D}_{\mathbb{S}}\). More details regarding the grouping and filtering are provided in the Appendix.

The dataset \(\mathcal{D}_{\mathbb{S}}\) contains a total of 2M \((\mathbb{C}_{s},p_{s})\) pairs. Using the aforementioned delta CLIP score filtering (using a high threshold \(\lambda=0.02\)), we remove low-quality synthesized images \(\bm{y}_{s}\) from the expert model, finally obtaining a dataset \(G\) with \(\sim\)500K \((\mathbb{C}_{s},p_{s})\) effective training pairs for the following apprenticeship learning.

## 5 Experiment

In this paper, we only train SuTI on the text \(\rightarrow\) 64x64 diffusion model and retain the original 256x256 and 1024x1024 super-resolution as it is from Imagen [1].

**Expert Models.** The expert model is initialized from the original 2.1B Imagen 64x64 model. We tune each model on a single TPU core (32 GB) for 500 steps using Adafactor optimizer with a learning rate of 1e-5, which only takes 5 minutes to finish. We use classifier-free guidance to sample new images, where the guidance weight is set to \(30\). To avoid excessive memory costs, we use fine-tuned experts to sample pseudo-target images and then write the samples as separate files. SuTI will read these files asynchronously to maximize the training speed. Our expert models have a few distinctions from the DreamBooth [6]: 1) we adopt Adafactor instead of Adam optimizer, 2) we do not include any class word token like '[DOG] dog' in the prompt. 3) we do not include in-class negatives for prior preservation. Though our expert model is weaker than DreamBooth, such design choices significantly reduce time/space costs to enable us to train millions of experts with reasonable resources.

**Apprentice Model.** The apprentice model contains 2.5B parameters, which is 400M parameters larger than the original 2.1B Imagen 64x64 model. The added parameters are coming from the extra attention layers over the demonstrated image-text inputs. We adopt the same architecture as Re-Imagen [9], where the additional image-text pairs are encoded by re-using the UNet DownStack, and the attention layers are added to the down UNet DownStack and UpStack at different resolutions.

We initialize our model from Imagen's checkpoint. For the additional attention layers, we use random initialization. The apprentice training is performed on 128 Cloud TPU v4 chips. We train the model for a total of 150K steps. We use an Adafactor optimizer with a learning rate of 1e-4. We use 3 demonstrations during training, while the model can generalize to leverage any number of demonstrations during inference. We show our ablation studies in the following section.

**Inference.** We normally provide 4 demonstration image-text pairs to SuTI during inference. Increasing the number of demonstrations does not improve the generation quality much. We use a lower classifier-free guidance weight of \(15\) with DDPM [14] sampling strategy.

### Datasets and Metrics

**DreamBench.** In this paper, we use the DreamBench dataset proposed by DreamBooth [6]. The dataset contains 30 subjects like backpacks, stuffed animals, dogs, cats, clocks, etc. These images are downloaded from Unsplash. The original dataset contains 25 prompt templates covering different skills like recontextualization, property modification, accossorization, etc. In total, there are a total of 750 unique prompts generated by the template. We follow the original paper to generate 4 images for each prompt to form the 3000 images for robust evaluation. We follow DreamBooth to adopt DINO, CLIP-I to evaluate the subject fidelity, and CLIP-T to evaluate the text fidelity.

**DreamBench-v2.** To further increase the difficulty and diversity of DreamBench, we annotate 220 prompts for the 30 subjects in DreamBench as DreamBench-v2. We gradually increase the compositional levels of the prompt to increase the difficulty, like 'back view of [dog]' \(\rightarrow\) 'back view of [dog] watching TV' \(\rightarrow\) 'back view of [dog] watching TV about birds'. This enables us to perform a breakdown analysis to understand the model's compositional capabilities.

We use human evaluation to measure the generation quality in DreamBench-v2. Specifically, we aim at measuring the following three aspects: (1) the subject fidelity score \(s_{s}\) measures whether the subject is being preserved, (2) the textual fidelity score \(s_{t}\) measures whether it is aligned with the text description, (3) the photorealism score \(s_{p}\) measures whether the image contains artifacts or blurry subjects. These are all binary scores, which are averaged over the entire dataset. We combine them as an overall score \(s_{o}=s_{s}\wedge s_{t}\wedge s_{p}\), which is the most stringent score.

Figure 4: Comparison with other Image Editing and Image Personalization Models.

### Main Results

**Baselines.** We provide a comprehensive list of baselines to compare with the proposed SuTI model:

* _DreamBooth_[6]: a fine-tuning method with space consumption is \(|M|\times|\mathbb{S}|\), where \(|M|\) is the model size and \(|\mathbb{S}|\) is the number of subjects.
* _Textual Inversion_[8]: a fine-tuning method with space consumption is \(|E|\times|\mathbb{S}|\) with the embedding size of \(|E|\), note that \(|E|\ll|M|\).
* _Null-Text Inversion_[12]: a fine-tuning method with space consumption is \(|T|\times|E|\times|\mathbb{S}|\).
* _Imagic_[7]: a fine-tuning-based method with largest space consumption among all models as it requires training \(|M|\times|\mathbb{S}|\times|\mathbb{P}|\), where \(|\mathbb{P}|\) is the number of a text prompt \(\mathbb{P}=\{\bm{p}_{s}\}\) for the subject set \(\mathbb{S}\).
* _InstructPix2Pix_[11]: a non-tuning method, which can generate and edit a given image really fast within a few seconds. There is no additional space consumption.
* _Re-Imagen_[9]: a non-tuning method, which will take a few images as input and then attend to those retrievals to generate a new image. There is no additional space consumption.

**Experimental Results.** We show our automatic evaluation results on the DreamBench in Table 1. We can observe that SuTI can perform better or on par with DreamBooth on all of the metrics. Specifically, SuTI outperforms DreamBooth on the DINO score by 5%, which indicates that our method is better at preserving the subject's visual appearance. In terms of the CLIP-T score, our method is almost the same as DreamBooth, indicating an equivalent capability in terms of textual alignment. These results indicate that SuTI has achieved promising generalization to a wide variety of visual subjects, without being trained on the exact instances.

We further show our human evaluation results on the DreamBench-V2 in Table 2. It shows the related rankings for the additional storage cost and reported the average inference time measure for inferring on each subject. As can be seen, SuTI is able to outperform DreamBooth by 5% on the overall score mainly due to much higher textual alignment. In contrast, all the other existing baselines are getting much lower human evaluation score (\(<42\%\)).

**Comparisons.** We compare our generation results with other methods in Figure 4. As can be seen, SuTI can generate images highly faithful to the demonstrated subjects. Though SuTI is still

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Methods & Backbone & DINO \(\uparrow\) & CLIP-I \(\uparrow\) & CLIP-T \(\uparrow\) \\ \hline Real Image (Oracle) & - & 0.774 & 0.885 & - \\ \hline DreamBooth [6] & Imagen [1] & 0.696 & 0.812 & **0.306** \\ DreamBooth [6] & SD [21] & 0.668 & 0.803 & 0.305 \\ Textual Inversion [8] & SD [21] & 0.569 & 0.780 & 0.255 \\ Re-Imagen [9] & Imagen [1] & 0.600 & 0.740 & 0.270 \\ \hline Ours: SuTI & Imagen [1] & **0.741** & **0.819** & 0.304 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Automatic Evaluation on the DreamBench.

\begin{table}
\begin{tabular}{l l c|c c c|c|c} \hline \hline Methods & Backbone & Space & Time & Subject \(\uparrow\) & Text \(\uparrow\) & Photorealism \(\uparrow\) & Overall \(\uparrow\) \\ \hline \multicolumn{8}{c}{Models requiring test-time tuning} \\ \hline Textual Inversion [8] & SD [21] & \$ & 30 mins & 0.22 & 0.64 & 0.90 & 0.14 \\ Null-Text Inversion [12] & Imagen [1] & \$\$ & 5 mins & 0.20 & 0.46 & 0.70 & 0.10 \\ Imagic [7] & Imagen [1] & \$\$\$ & 70 mins & 0.78 & 0.34 & 0.68 & 0.28 \\ DreamBooth [6] & SD [21] & \$\$\$ & 6 mins & 0.74 & 0.53 & 0.85 & 0.47 \\ DreamBooth [6] & Imagen [1] & \$\$ & 10 mins & 0.88 & 0.82 & **0.98** & 0.77 \\ InstrCutPix2Pix [11] & SD [21] & - & 10 secs & 0.14 & 0.46 & 0.42 & 0.10 \\ Re-Imagen [9] & Imagen [1] & - & 20 secs & 0.70 & 0.65 & 0.64 & 0.42 \\ \hline Ours: SuTI & Imagen [1] & - & 20 secs & **0.90** & **0.90** & 0.92 & **0.82** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Human Evaluation on the DreamBench-v2. We report an approximated average inference time (averaged over subjects) and the relative rankings of the space cost (more $: more expensive). Methods that do not fine-tune in test-time requires no additional storage (denoted by -).

missing some local textual (words on the bowl gets blurred) or colorization (dog hair color gets darker), the nuance is almost unperceivable for humans. The other baselines like InstructPix2Pix [11], and Null-Text Inversion [12] are not able to perform very sophisticated transformations. Textual Inversion [8] cannot achieve satisfactory results even with 30 minutes of tuning. Re-Imagen [9] though gives reasonable outputs, the subject preservation is much weaker than SuTI. Imagic [7] also generates reasonable outputs, however, its failure rate is still much higher than ours. DreamBooth [6] however generates almost perfect images except for the 'blurry' text on the berry bowl. Through the comparison, we can observe remarkable improvement in the output image quality.

**Skillset.** We provide SuTI's generation to showcase its ability in re-contextualization, novel view synthesis, art rendition, property modification, and accessorization. We demonstrate these different skills in the Appendix Figure 8. In the first row, we show that SuTI is able to synthesize the subjects with different at styles. In the second row, we show that SuTI is able to synthesize the different view angles of the given subject. In the third row, we show that SuTI can modify subjects' facial expressions like'sad','screaming', etc. In the fourth row, we show that SuTI can alter the color of a given toy. In the last two rows, we show that SuTI can add different accessories (hats, clothes, etc) to the given subjects. Further, we found that SuTI can even compose two skills together to perform highly complex image generation. As depicted in Figure 5, we show that SuTI can combine re-contextualization with editing/accessorization/stylization to generate high-quality images.

### Model Analysis and Ablation Study

We further conducted a set of ablation studies to show factors that impact the performance of SuTI.

**Impact of # Demonstrations.** Figure 6 presents the SuTI's in-context generation with respect to an increasing number of subject-specific image examples. Interestingly, we observe a transition in the model's behavior as the number of in-context examples increases. When \(\mathbb{C}_{s}=\varnothing\), SuTI generates images using it prior to the text, similar to traditional text-to-image generation models such as Imagen [1]. When \(|\mathbb{C}_{s}|=1\), SuTI behaves similarly to an image editing model, attempting to edit the generation while preserving the foreground subject, and avoiding sophisticated transformation. When \(|\mathbb{C}_{s}|=5\), SuTI unlocks the capability of rendering novel pose and shape of the demonstrated subject naturally in the targeted scene. In addition, we also observe that a bigger \(|\mathbb{C}_{s}|\) would result in a more robust generation of high text and subject alignment, and better photorealism. We also performed a human evaluation on the SuTI's generation with respect to different numbers of demonstrations and visualizes the results in Figure 6 (right). It shows that as the number of demonstrations increases, the human evaluation score first increases drastically and then gradually converges.

**Quality of the expert dataset matters.** We found that the Delta CLIP score is critical to ensure the quality of synthesized target images. Such a filtering mechanism is highly influential in terms of SuTI's final performance. We evaluated several versions to increase the \(\Delta\) threshold from None \(\rightarrow\) 0.0 \(\rightarrow\)\(\cdots\)\(\rightarrow\) 0.025, we observe that the human evaluation can steadily increase from 0.54 to 0.82.

Figure 5: SuTI not only re-contextualizes subjects but also composes multiple transformation, all in-context.

Without such intensive filtering, the model's overall human score can go to a very low level (54%). With an increasing \(\Delta\), although the size of the dataset \(G\) keeps decreasing from 1.8M to around 500K, the model's generation quality keeps improving until saturation. The empirical study indicates that \(\Delta=0.02\) strikes a good balance between the quality and quantity of the expert-generated dataset \(G\).

**Further fine-tuning SuTI improves generation quality** We note that our model is not exclusive to methods that requires further fine-tuning, such as DreamBooth [6]. Instead, SuTI can be combined with DreamBooth [6] naturally to achieve better quality subject-driven generation (dubbed as Dream-SuTI). Specifically, given \(K\) reference images regarding a subject, we can randomly feed one image as the condition and use another differently sampled image as the target output. Through fine-tuning the SuTI model for 500 steps (without any auxiliary loss), the Dream-SuTI model can generate aligned and faithful results for a given subject. Table 3 shows a comparison of the Dream-SuTI, against SuTI and DreamBooth, suggesting that Dream-SuTI further improves the generation quality. Particularly, it improves the overall score from 0.82 to 0.87, yielding a 5% improvement over SuTI, and 10% improvement over DreamBooth. Since the fine-tuned Dream-SuTI model already trained on all subject images, only one subject image is needed to present during the inference time, which can further reduce the inference cost.

To gain better understanding of the quality, we show an example in Figure 7, where we pick a failure example from SuTI to investigate whether Dream-SuTI improves it. We observe that the DreamBooth does not have strong text alignment, while SuTI's subject lacks fidelity (the generated robot uses legs instead of wheels). With further fine-tuning on subject images, Dream-SuTI is able

Figure 6: (Left) In-context generation by SuTI model, with an increasing # of demonstrations. (Right) Human evaluation score with with respect to the increasing % of demonstrations.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Methods & Inference Time & Subject \(\uparrow\) & Text \(\uparrow\) & Photorealism \(\uparrow\) & Overall \\ \hline DreamBooth & 10 secs & 0.88 & 0.82 & 0.98 & 0.77 \\ \hline SuTI & 20 secs & 0.90 & 0.90 & 0.92 & 0.82 \\ Dream-SuTI & 15 secs & **0.92** & **0.92** & **0.94** & **0.87** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative Human evaluation of the Dream-SuTI model on DreamBench-v2.

Figure 7: Comparison between DreamBooth, SuTI and Dream-SuTI.

to generate images not only faithful to the subject but also to the text description. However, we would like to note that such subject-driven fine-tuned model share the same drawback of a typical Dreambooth model, which can no longer generalize well to a general distribution objects and hence requiring a copy of model parameter per subject.

## 6 Related Work

**Text-Guided Image Editing** With the surge of diffusion-based models, [22; 8] have demonstrated the possibilities to manipulate given image without human intervention. Blended-Diffusion [23] and SDEdit [24] propose to blend the noise with the input image to guide the image synthesize process to maintain the original layout. Text2Live [25] generates an edit layer on top of the original image/video input. Prompt-to-Prompt [26] and Null-Text Inversion [12] aims at manipulating the attention map in the diffusion model to maintain the layout of the image while changing certain subjects. Imagic [7] propose an optimization based to achieve significant progress in manipulating visual details in a given image. InstructPix2Pix [11] propose to distill image editing training pairs synthesized from Prompt-to-Prompt into a single diffusion model to perform instruction-driven image editing. Our method resembles InstructPix2Pix in a sense that we are training the model on expert-generated images. However, our synthesized data is generated generated by fine-tuned experts, which are mostly natural images. In contrast, the images from InstructPix2Pix are synthetic images. In the experiment section, we comprehensively compare with these existing models to show the advantage of our model, especially on more challenging prompts.

**Subject-Driven Text-to-Image Generation** Subject-Driven Image Generation tackles a new challenge, where the model needs to understand the visual subject contained in the demonstrations to synthesize totally new scene. Several GAN-based models [27; 28] pioneered to work on personalizing the image generation model to a particular instance. Later on, DreamBooth [6] and Textual Inversion [8; 29] propose optimization-based approach to adapt image generation to a specific unseen subject. However, these two methods are time and space-consuming, which makes them unrealistic in real-world applications. Another line of work adopt retrieval-augmented architecture for subject-driven generation including KNN-Diffusion [30], Re-Imagen [9], however, these methods are trained with weakly-supervised data leading to much worse faithfulness. In this paper, we aim at developing an apprenticeship learning paradigm to train the image generation model with stronger supervision demonstrated by fine-tuned experts. As a result, SuTI can generate customized images about a specified subject, without requiring any test-time fine-tuning. There are some concurrent and related works [31; 32] focusing on specific visual domains such as human faces and / or animals. To our best knowledge, SuTI is the first subject-driven text-to-image generator that operates fully in-context, _generalizing_ across various visual domains.

## 7 Conclusion

Our method SuTI has shown strong capabilities to generate personalized images instantly without test-time optimization. Our human evaluation indicates that SuTI is already better in the overall score than DreamBooth, however, we do identify a few weakness of our model: (1) SuTI's generations are less diverse than DreamBooth, and our model is less inclined to transform the subjects' poses or views in the new image. (2) SuTI is less faithful to the low-level visual details than DreamBooth, especially for more complex and often manufactured subjects such as 'robots' or 'rc cars' where the subjects contain highly sophisticated visual details that could be arbitrarily different from the examples inside the training dataset. In the future, we plan to investigate how to further improve these two aspects to make SuTI's generation more diverse and detail-preserving.

## Acknowledgement

We thank Boqing Gong, Kaifeng Chen for reviewing an early version of this paper in depth, with valuable comments and suggestions. We thank Neil Houlsby, Xiao Wang and also the PaLI-X team for providing early access to their Episodic WebLI data. We also thank Jason Baldbridge, Andrew Bunner, Nicole Brichtova for discussions and feedback on the project.

## Broader Impact

Subject-driven text-to-image generation has wide downstream applications, like adapting certain given subjects into different contexts. Previously, the process was mostly done manually by experts who are specialized in photo creation software. Such manual modification process is time-consuming. We hope that our model could shed light on how to automate such a process and save huge amount of labors and training. The current model is still highly immature, which can fall into several failure modes as demonstrated in the paper. For example, the model is still prone to certain priors presented in certain subject classes. Some low-level visual details in subjects are not perfectly preserved. However, it could still be used as an intermediate form to help accelerate the creation process. On the flip side, there are risks with such models including misinformation, abuse and bias. See the discussion of broader impacts in [1, 4] for more discussion.

## References

* [1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_, 2022.
* [2] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [3] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [4] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2022.
* [5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [6] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. _CVPR_, 2023.
* [7] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. _CVPR_, 2023.
* [8] Rinon Gal, Yuval Alaulf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.
* [9] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-image generator. _International Conference on Learning Representations_, 2023.
* [10] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.
* [11] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. _arXiv preprint arXiv:2211.09800_, 2022.
* [12] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. _arXiv preprint arXiv:2211.09794_, 2022.

* [13] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [15] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.
* [16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [17] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the twenty-first international conference on Machine learning_, page 1, 2004.
* [18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [19] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event structures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16420-16429, 2022.
* [20] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmoshine, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.
* [21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [22] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2085-2094, 2021.
* [23] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18208-18218, 2022.
* [24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [25] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV_, pages 707-723. Springer, 2022.
* [26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [27] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano. Instance-conditioned gan. _Advances in Neural Information Processing Systems_, 34:27517-27529, 2021.
* [28] Yotam Nitzan, Kfir Aherman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and Daniel Cohen-Or. Mystyle: A personalized generative prior. _ACM Transactions on Graphics (TOG)_, 41(6):1-10, 2022.
* [29] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an encoder for fast personalization of text-to-image models. _arXiv preprint arXiv:2302.12228_, 2023.
* [30] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-diffusion: Image generation via large-scale retrieval. _arXiv preprint arXiv:2204.02849_, 2022.

* [31] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. _arXiv preprint arXiv:2304.02642_, 2023.
* [32] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. _arXiv preprint arXiv:2304.03411_, 2023.
* [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.

Supplementary Material

### Dataset Construction

To validate the effectiveness, we provide an ablation study to show that higher precision is more important than recall in training the apprentice model. Particularly, when the threshold is set to a lower number (_e.g._, 0.01 or 0.015), SuTI becomes less stable.

As our goal is to collect images of the same subject, we create an initial subject cluster by grouping all (image, alt-text) pairs that come from the same URL (\(\sim\)45M clusters), and filter the cluster with less than 3 instances (\(\sim\)77.8% of the clusters). As a result, it leaves us with \(\sim\)10M image clusters. We then apply the pre-trained CLIP ViT-L14 model [33] to filter out 81.1% of clusters that has the average intra-cluster visual similarity between \(0.82\) and \(0.98\) to ensure the quality of clusters.

Though the mined clusters already contain (image, alt-text) information, the alt-text's noise level is too high. Therefore, we apply the state-of-the-art image captioning model [10] to generate descriptive text captions for every image of all image clusters, which forms the data triples of (image, alt-text, caption). However, current image captioning models tend to generate generic descriptions of the visual scene, which often occlude the detailed entity information about the subject. For example, generic captions like 'a pair of red shoes' would greatly decrease the expert model's capability to preserve the subject's visual appearance. To increase the specificity of the visual captions, we propose to merge the alt-text, which normally contains specific meta information like brands, names, etc with the model-generated caption. For example, Given an alt-text of 'duggee talking puppet hey duggee chicco 12m' and a caption of 'a toy on the table', we aim to combine them as a more concrete caption: 'Hey duggee toy on the table'. To achieve this, we prompt the pre-trained large language models [18] to read all (alt-text, caption) pairs inside each image cluster, and output a short descriptive text about the visual subject. These refined captions with the mined images are used as the image-text cluster \(\mathbb{C}_{s}\) w.r.t subject \(s\), which will be used to fine-tune the expert models.

### SuTI Skillset

We demonstrate the complete view of SuTI's skillset in Figure 8, including styled subject generation, multi-view subject rendering, subject expression modification, subject colorization, and subject accessorization.

### Failure Examples

Figure 9 show some failure examples of SuTI. We show several types of failure modes: (1) the model has a strong prior about the subject and hallucinates the visual details based on its prior knowledge. For example, the generation model believes 'teapot' should contain a 'lift handle'. (2) some artifacts from the demonstration images are being transferred to the generated images. For example, the 'bed' from the demonstration is being brought to the generation, (3) the subject's visual appearance is being modified through, mostly influenced by the context, like the 'candle' contains non-existing artifacts when contextualized in the 'toilet'. These three failure modes constitute most of the generation errors. (4) The models are not particularly good at handling compositional prompts like the 'bear plushie' and'sunglasses' example. In the future, we plan to work on how to improve these aspects.

### More Qualitative Examples

We demonstrate more examples from DreamBench-v2 in the following figures:Figure 8: SuT1’s in-context generation that demonstrates its skill set. Results generated from _a single model_. First row: art rendition of the subject. Second row: multi-view synthesis of the subject. Third row: modifying expression for the subject. Fourth row: editing the color of the subject. Fifth row: adding accessories to the subject. Subject (image, text) and editing key words are annotated, with detailed template in the Appendix.

Figure 9: SuTI’s failure examples on DreamBench-v2.

Figure 10: Visualization of SuTI’s generation on the DreamBench-v2 (Part 1).

Figure 11: Visualization of SuTI’s generation on the DreamBench-v2 (Part 2).

Figure 12: Visualization of SuTI’s generation on the DreamBench-v2 (Part 3).

Figure 13: In-context generation by SuTI model, with an increasing # of demonstration (More examples).