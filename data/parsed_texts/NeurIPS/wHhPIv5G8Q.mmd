# Online Corrupted User Detection and Regret Minimization

 Zhiyong Wang

The Chinese University of Hong Kong

zywang21@cse.cuhk.edu.hk

&Jize Xie

Shanghai Jiao Tong University

xjzzjl@sjtu.edu.cn

&Tong Yu

Adobe Research

worktongyu@gmail.com

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

&John C.S. Lui

The Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

Corresponding author.

###### Abstract

In real-world online web systems, multiple users usually arrive sequentially into the system. For applications like click fraud and fake reviews, some users can maliciously perform corrupted (disrupted) behaviors to trick the system. Therefore, it is crucial to design efficient online learning algorithms to robustly learn from potentially corrupted user behaviors and accurately identify the corrupted users in an online manner. Existing works propose bandit algorithms robust to adversarial corruption. However, these algorithms are designed for a single user, and cannot leverage the implicit social relations among multiple users for more efficient learning. Moreover, none of them consider how to detect corrupted users online in the multiple-user scenario. In this paper, we present an important online learning problem named LOCUD to learn and utilize unknown user relations from disrupted behaviors to speed up learning, and identify the corrupted users in an online setting. To robustly learn and utilize the unknown relations among potentially corrupted users, we propose a novel bandit algorithm RCLUB-WCU. To detect the corrupted users, we devise a novel online detection algorithm OCCUD based on RCLUB-WCU's inferred user relations. We prove a regret upper bound for RCLUB-WCU, which asymptotically matches the lower bound with respect to \(T\) up to logarithmic factors, and matches the state-of-the-art results in degenerate cases. We also give a theoretical guarantee for the detection accuracy of OCCUD. With extensive experiments, our methods achieve superior performance over previous bandit algorithms and high corrupted user detection accuracy.

## 1 Introduction

In real-world online recommender systems, data from many users arrive in a streaming fashion [4, 15, 2, 7, 35, 27, 26]. There may exist some corrupted (malicious) users, whose behaviors (_e.g._, click, rating) can be adversarially corrupted (disrupted) over time to fool the system [29, 30, 12, 10, 9]. These corrupted behaviors could disrupt the user preference estimations of the algorithm. As a result, the system would easily be misled and make sub-optimal recommendations [14, 23, 7, 41],which would hurt the user experience. Therefore, it is essential to design efficient online learning algorithms to robustly learn from potentially disrupted behaviors and detect corrupted users in an online manner.

There exist some works on bandits with adversarial corruption [29, 9, 22, 5, 12, 16]. However, they have the following limitations. First, existing algorithms are initially designed for robust online preference learning of a single user. In real-world scenarios with multiple users, they cannot robustly infer and utilize the implicit user relations for more efficient learning. Second, none of them consider how to identify corrupted users online in the multiple-user scenario. Though there also exist some works on corrupted user detection [34, 6, 39, 28, 13], they all focus on detection with _known_ user information in an offline setting, thus can not be applied to do online detection from bandit feedback.

To address these limitations, we propose a novel bandit problem "_Learning and Online Corrupted Users Detection from bandit feedback_" (LOCUD). To model and utilize the relations among users, we assume there is an _unknown_ clustering structure over users, where users with similar preferences lie in the same cluster [8, 19, 21]. The agent can infer the clustering structure to leverage the information of similar users for better recommendations. Among these users, there exists a small fraction of corrupted users. They can occasionally perform corrupted behaviors to fool the agent [12, 29, 30, 9] while mimicking the behaviors of normal users most of the time to make themselves hard to discover. The agent not only needs to learn the _unknown_ user preferences and relations robustly from potentially disrupted feedback, balance the exploration-exploitation trade-off to maximize the cumulative reward, but also needs to detect the corrupted users online from bandit feedback.

The LOCUD problem is very challenging. First, the corrupted behaviors would cause inaccurate user preference estimations, which could lead to erroneous user relation inference and sub-optimal recommendations. Second, it is nontrivial to detect corrupted users online since their behaviors are dynamic over time (sometimes regular while sometimes corrupted), whereas, in the offline setting, corrupted users' information can be fully represented by static embeddings and the existing approaches [18, 32] can typically do binary classifications offline, which are not adaptive over time.

We propose a novel learning framework composed of two algorithms to address these challenges.

**RCLUB-WCU.** To robustly estimate user preferences, learn the unknown relations from potentially corrupted behaviors, and perform high-quality recommendations, we propose a novel bandit algorithm "_Robust CLUstering of Bandits With Corrupted Users_" (RCLUB-WCU), which maintains a dynamic graph over users to represent the learned clustering structure, where users linked by edges are inferred to be in the same cluster. RCLUB-WCU adaptively deletes edges and recommends arms based on aggregated interactive information in clusters. We do the following to ensure robust clustering structure learning. (i) To relieve the estimation inaccuracy caused by disrupted behaviors, we use weighted ridge regressions for robust user preference estimations. Specifically, we use the inverse of the confidence radius to weigh each sample. If the confidence radius associated with user \(i_{t}\) and arm \(a_{t}\) is large at \(t\), the learner is quite uncertain about the estimation of \(i_{t}\)'s preference on \(a_{t}\), indicating the sample at \(t\) is likely to be corrupted. Therefore, we use the inverse of the confidence radius to assign minor importance to the possibly disrupted samples when doing estimations. (ii) We design a robust edge deletion rule to divide the clusters by considering the potential effect of corruptions, which, together with (i), can ensure that after some interactions, users in the same connected component of the graph are in the same underlying cluster with high probability.

**OCCUD.** To detect corrupted users online, based on the learned clustering structure of RCLUB-WCU, we devise a novel algorithm named "_Online Cluster-based Corrupted User Detection_" (OC-CUD). At each round, we compare each user's non-robustly estimated preference vector (by ridge regression) and the robust estimation (by weighted regression) of the user's inferred cluster. If the gap exceeds a carefully-designed threshold, we detect this user as corrupted. The intuitions are as follows. With misleading behaviors, the non-robust preference estimations of corrupted users would be far from ground truths. On the other hand, with the accurate clustering of RCLUB-WCU, the robust estimations of users' inferred clusters should be close to ground truths. Therefore, for corrupted users, their non-robust estimates should be far from the robust estimates of their inferred clusters.

We summarize our contributions as follows.

\(\bullet\) We present a novel online learning problem LOCUD, where the agent needs to (i) robustly learn and leverage the unknown user relations to improve online recommendation qualities under the disruption of corrupted user behaviors; (ii) detect the corrupted users online from bandit feedback.

\(\bullet\) We propose a novel online learning framework composed of two algorithms, RCLUB-WCU and OCCUD, to tackle the challenging LOCUD problem. RCLUB-WCU robustly learns and utilizes the unknown social relations among potentially corrupted users to efficiently minimize regret. Based on RCLUB-WCU's inferred user relations, OCCUD accurately detects corrupted users online.

\(\bullet\) We prove a regret upper bound for RCLUB-WCU, which matches the lower bound asymptotically in \(T\) up to logarithmic factors and matches the state-of-the-art results in several degenerate cases. We also give a theoretical performance guarantee for the online detection algorithm OCCUD.

\(\bullet\) Experiments on both synthetic and real-world data clearly show the advantages of our methods.

## 2 Related Work

Our work is related to bandits with adversarial corruption and bandits leveraging user relations.

The work [29] first studies stochastic bandits with adversarial corruption, where the rewards are corrupted with the sum of corruption magnitudes in all rounds constrained by the _corruption level_\(C\). They propose a robust elimination-based algorithm. The paper [9] proposes an improved algorithm with a tighter regret bound. The paper [22] first studies stochastic linear bandits with adversarial corruptions. To tackle the contextual linear bandit setting where the arm set changes over time, the work [5] proposes a variant of the OFUL [1] that achieves a sub-linear regret. A recent work [12] proposes the CW-OFUL algorithm that achieves a nearly optimal regret bound. All these works focus on designing robust bandit algorithms for a single user; none consider how to robustly learn and leverage the implicit relations among potentially corrupted users for more efficient learning. Moreover, none of them consider how to online detect corrupted users in the multiple-user case.

Some works study how to leverage user relations to accelerate the bandit learning process in the multiple-user case. The work [38] utilizes a _known_ user adjacency graph to share context and payoffs among neighbors. To adaptively learn and utilize _unknown_ user relations, the paper [8] proposes the clustering of bandits (CB) problem where there is an _unknown_ user clustering structure to be learned by the agent. The work [20] uses collaborative effects on items to guide the clustering of users. The paper [19] studies the CB problem in the cascading bandit setting. The work [21] considers the setting where users in the same cluster share both the same preference and the same arrival rate. The paper [25] studies the federated CB problem, considering privacy and communication issues. All these works only consider utilizing the relations among normal users; none of them consider how to robustly learn the user relations from potentially disrupted behaviors, thus would easily be misled by corrupted users. Also, none of them consider how to detect corrupted users from bandit feedback.

To the best of our knowledge, this is the first work to study the problem to (i) learn the unknown user relations and preferences from potentially corrupted feedback, and leverage the learned relations to speed up learning; (ii) adaptively detect the corrupted users online from bandit feedback.

## 3 Problem Setup

This section formulates the problem of "_Learning and Online Corrupted Users Detection from bandit feedback_" (LOCUD) (illustrated in Fig.1). We denote \(\left\|\bm{x}\right\|_{\bm{M}}=\sqrt{\bm{x}^{\top}\bm{M}\bm{x}}\), \(\left[m\right]=\{1,\ldots,m\}\), number of elements in set \(\mathcal{A}\) as \(\left|\mathcal{A}\right|\).

In LOCUD, there are \(u\) users, which we denote by set \(\mathcal{U}=\{1,2,\ldots,u\}\). Some of them are corrupted users, denoted by set \(\tilde{\mathcal{U}}\subseteq\mathcal{U}\). These corrupted users, on the one hand, try to mimic normal users to make themselves hard to detect; on the other hand, they can occasionally perform corrupted behaviors to fool the agent into making sub-optimal decisions. Each user \(i\in\mathcal{U}\), no matter a normal one or corrupted one, is associated with a (possibly mimicked for corrupted users) preference feature vector \(\bm{\theta}_{i}\in\mathbb{R}^{d}\) that is _unknown_ and \(\left\|\bm{\theta}_{i}\right\|_{2}\leq 1\). There is an underlying clustering structure among all the users representing the similarity of their preferences, but it is _unknown_ to the agent and needs to be learned via interactions. Specifically, the set of users \(\mathcal{U}\) can be partitioned into \(m\) (\(m\ll u\)) clusters, \(V_{1},V_{2},\ldots V_{m}\), where \(\cup_{j\in[m]}V_{j}=\mathcal{U}\), and \(V_{j}\cap V_{j^{\prime}}=\emptyset\), for \(j\neq j^{\prime}\). Users in the same cluster have the same preference feature vector, while users in different clusters have different preference vectors. We use \(\bm{\theta}^{j}\) to denote the common preference vector shared by users in the \(j\)-th cluster \(V_{j}\), and use \(j(i)\) to denote the index of cluster user \(i\) belongs to (_i.e.,_\(i\in V_{j(i)}\)). For any two users \(k,i\in\mathcal{U}\), if \(k\in V_{j(i)}\), then \(\bm{\theta}_{k}=\bm{\theta}^{j(i)}=\bm{\theta}_{i}\); otherwise \(\bm{\theta}_{k}\neq\bm{\theta}_{i}\). We assume the arm set \(\mathcal{A}\subseteq\mathbb{R}^{d}\) is finite. Each arm \(a\in\mathcal{A}\) is associated with a feature vector \(\bm{x}_{a}\in\mathbb{R}^{d}\) with \(\left\lVert\bm{x}_{a}\right\rVert_{2}\leq 1\).

The learning process of the agent is as follows. At each round \(t\in[T]\), a user \(i_{t}\in\mathcal{U}\) comes to be served, and the learning agent receives a set of arms \(\mathcal{A}_{t}\subseteq\mathcal{A}\) to choose from. The agent infers the cluster \(V_{t}\) that user \(i_{t}\) belongs to based on the interaction history, and recommends an arm \(a_{t}\in\mathcal{A}_{t}\) according to the aggregated information gathered in the cluster \(V_{t}\). After receiving the recommended arm \(a_{t}\), a normal user \(i_{t}\) will give a random reward with expectation \(\bm{x}_{a_{t}}^{\top}\bm{\theta}_{i_{t}}\) to the agent.

To model the behaviors of corrupted users, following [29; 9; 5; 12], we assume that they can occasionally corrupt the rewards to mislead the agent into recommending sub-optimal arms. Specifically, at each round \(t\), if the current served user is a corrupted user (i.e., \(i_{t}\in\tilde{\mathcal{U}}\)), the user can corrupt the reward by \(c_{t}\). In summary, we model the reward received by the agent at round \(t\) as

\[r_{t}=\bm{x}_{a_{t}}^{\top}\bm{\theta}_{i_{t}}+\eta_{t}+c_{t}\,,\]

where \(c_{t}=0\) if \(i_{t}\) is a normal user, (_i.e._, \(i_{t}\notin\tilde{\mathcal{U}}\)), and \(\eta_{t}\) is 1-sub-Gaussian random noise.

As the number of corrupted users is usually small, and they only corrupt the rewards occasionally with small magnitudes to make themselves hard to detect, we assume the sum of corruption magnitudes in all rounds is upper bounded by the _corruption level_\(C\), _i.e._, \(\sum_{t=1}^{T}\left\lvert c_{t}\right\rvert\leq C\)[29; 9; 5; 12].

We assume the clusters, users, and items satisfy the following assumptions. Note that all these assumptions basically follow the settings from classical works on clustering of bandits [8; 19; 25; 36].

**Assumption 1** (Gap between different clusters).: _The gap between any two preference vectors for different clusters is at least an unknown positive constant \(\gamma\)_

\[\left\lVert\bm{\theta}^{j}-\bm{\theta}^{j^{\prime}}\right\rVert_{2}\geq \gamma>0\,,\forall j,j^{\prime}\in[m]\,,j\neq j^{\prime}\,.\]

**Assumption 2** (Uniform arrival of users).: _At each round \(t\), a user \(i_{t}\) comes uniformly at random from \(\mathcal{U}\) with probability \(1/u\), independent of the past rounds._

**Assumption 3** (Item regularity).: _At each round \(t\), the feature vector \(\bm{x}_{a}\) of each arm \(a\in\mathcal{A}_{t}\) is drawn independently from a fixed unknown distribution \(\rho\) over \(\{\bm{x}\in\mathbb{R}^{d}:\left\lVert\bm{x}\right\rVert_{2}\leq 1\}\), where \(\mathbb{E}_{\bm{x}\sim\rho}[\bm{x}\bm{x}^{\top}]\)'s minimal eigenvalue \(\lambda_{x}>0\). At \(\forall t\), for any fixed unit vector \(\bm{z}\in\mathbb{R}^{d}\), \((\bm{\theta}^{\top}\bm{z})^{2}\) has sub-Gaussian tail with variance no greater than \(\sigma^{2}\)._

Let \(a_{t}^{*}\in\arg\max_{a\in\mathcal{A}_{t}}\bm{x}_{a}^{\top}\bm{\theta}_{i_{t}}\) denote an optimal arm with the highest expected reward at round \(t\). One objective of the learning agent is to minimize the expected cumulative regret

\[R(T)=\mathbb{E}[\sum_{t=1}^{T}(\bm{x}_{a_{t}^{\top}}^{\top}\bm{\theta}_{i_{t} }-\bm{x}_{a_{t}}^{\top}\bm{\theta}_{i_{t}})]\,.\] (1)

Another objective is to detect corrupted users online accurately. Specifically, at round \(t\), the agent will give a set of users \(\tilde{\mathcal{U}}_{t}\) as the detected corrupted users, and we want \(\tilde{\mathcal{U}}_{t}\) to be as close to the ground-truth set of corrupted users \(\tilde{\mathcal{U}}\) as possible.

## 4 Algorithms

This section introduces our algorithms RCLUB-WCU (Algo.1) and OCCUD (Algo.2). RCLUB-WCU robustly learns the unknown user clustering structure and preferences from corrupted feed

Figure 1: Illustration of LOCUD. The _unknown_ user relations are represented by dotted circles, _e.g._, user 3, 7 have similar preferences and thus can be in the same user segment (_i.e._, cluster). Users 6 and 8 are corrupted users with dynamic behaviors over time (_e.g._, for user 8, the behaviors are normal at \(t_{1}\) and \(t_{3}\) (blue), but are adversarially corrupted at \(t_{2}\) and \(t_{4}\) (red)[29; 12]), making them hard to be detected online. The agent needs to learn user relations to utilize information among similar users to speed up learning, and detect corrupted users 6, 8 online from bandit feedback.

back, and leverages the cluster-based information to accelerate learning. Based on the clustering structure learned by RCLUB-WCU, OCCUD can accurately detect corrupted users online.

```
1:Input: Regularization parameter \(\lambda\), confidence radius parameter \(\beta\), threshold parameter \(\alpha\), edge deletion parameter \(\alpha_{1}\), \(f(T)=\sqrt{(1+\ln(1+T))/(1+T)}\).
2:Initialization:\(\bm{M}_{i,0}=\bm{0}_{d\times d},\bm{b}_{i,0}=\bm{0}_{d\times 1},\bm{\hat{M}}_{i,0}=\bm{0}_{d \times d},\bm{\hat{b}}_{i,0}=\bm{0}_{d\times 1},T_{i,0}=0\), \(\forall i\in\mathcal{U}\); A complete graph \(G_{0}=(\mathcal{U},E_{0})\) over \(\mathcal{U}\).
3:forall\(t=1,2,\ldots,T\)do
4: Receive the index of the current served user \(i_{t}\in\mathcal{U}\), get the feasible arm set at this round \(\mathcal{A}_{t}\).
5: Determine the connected components \(V_{t}\) in the current maintained graph \(G_{t-1}=(\mathcal{U},E_{t-1})\) such that \(i_{t}\in V_{t}\).
6: Calculate the robustly estimated statistics for the cluster \(V_{t}\): \(\bm{M}_{V_{t},t-1}=\lambda\bm{I}+\sum_{i\in V_{t}}\bm{M}_{i,t-1},\bm{b}_{V_{t},t-1}=\sum_{i\in V_{t}}\bm{b}_{i,t-1},\hat{\bm{\theta}}_{V_{t},t-1}=\bm{M}_{V_ {t},t-1}^{-1}\bm{b}_{V_{t},t-1}\) ;
7: Select an arm \(a_{t}\) with largest UCB index in Eq.(3) and receive the corresponding reward \(r_{t}\);
8: Update the statistics for robust estimation of user \(i_{t}\): \(\bm{M}_{i_{t},t}=\bm{M}_{i_{t},t-1}+w_{i_{t},t-1}\bm{a}_{a_{t}}\bm{x}_{i_{t}}^ {\top},\bm{b}_{i_{t},t}=\bm{b}_{i_{t},t-1}+w_{i_{t},t-1}r_{t}\bm{x}_{a_{t}}\,,T _{i_{t},t}=T_{i_{t},t-1}+1\,,\) \(\bm{M}_{i_{t},t}^{\prime}=\lambda\bm{I}+\bm{M}_{i_{t},t},\hat{\bm{\theta}}_{i_{ t},t}=\bm{M}_{i_{t},t}^{\prime-1}\bm{b}_{i_{t},t}\,,w_{i_{t},t}=\min\{1, \alpha/\|\bm{x}_{a_{t}}\|_{\bm{M}_{i_{t},t}^{\prime-1}}\}\) ;
9: Keep robust estimation statistics of other users unchanged: \(\bm{M}_{\ell,t}=\bm{M}_{\ell,t-1},\bm{b}_{\ell,t}=\bm{b}_{\ell,t-1},T_{\ell,t} =T_{\ell,t-1}\), \(\hat{\bm{\theta}}_{\ell,t}=\bm{\theta}_{\ell,t-1}\), for all \(\ell\in\mathcal{U},\ell\neq i_{t}\);
10: Delete the edge \((i_{t},\ell)\in E_{t-1}\), if \[\left\|\hat{\bm{\theta}}_{i_{t},t}-\hat{\bm{\theta}}_{\ell,t}\right\|_{2}\geq \alpha_{1}\big{(}f(T_{i_{t},t})+f(T_{\ell,t})+\alpha C\big{)}\,,\] and get an updated graph \(G_{t}=(\mathcal{U},E_{t})\);
11: Use the OCCUD Algorithm (Algo.2) to detect the corrupted users.
12:endfor ```

**Algorithm 1** RCLUB-WCU

### RCLUB-WCU

The corrupted behaviors may cause inaccurate preference estimations, leading to erroneous relation inference and sub-optimal decisions. In this case, how to learn and utilize unknown user relations to accelerate learning becomes non-trivial. Motivated by this, we design RCLUB-WCU as follows.

**Assign the inferred cluster \(V_{t}\) for user \(i_{t}\).** RCLUB-WCU maintains a dynamic undirected graph \(G_{t}=(\mathcal{U},E_{t})\) over users, which is initialized to be a complete graph (Algo.1 Line 2). Users with similar learned preferences will be connected with edges in \(E_{t}\). The connected components in the graph represent the inferred clusters by the algorithm. At round \(t\), user \(i_{t}\) comes to be served with a feasible arm set \(\mathcal{A}_{t}\) for the agent to choose from (Line 4). In Line 5, RCLUB-WCU detects the connected component \(V_{t}\) in the graph containing user \(i_{t}\) to be the current inferred cluster for \(i_{t}\).

**Robust preference estimation of cluster \(V_{t}\).** After determining the cluster \(V_{t}\), RCLUB-WCU estimates the common preferences for users in \(V_{t}\) using the historical feedback of all users in \(V_{t}\) and recommends an arm accordingly. The corrupted behaviors could cause inaccurate preference estimates, which can easily mislead the agent. To address this, inspired by [40, 12], we use weighted ridge regression to make corruption-robust estimations. Specifically, RCLUB-WCU robustly estimates the common preference vector of cluster \(V_{t}\) by solving the following weighted ridge regression

\[\hat{\bm{\theta}}_{V_{t},t-1}=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{ R}^{d}}\sum_{i_{t}\in V_{t}}w_{i_{s},s}(r_{s}-\bm{x}_{a_{s}}^{\top}\bm{ \theta})^{2}+\lambda\left\|\bm{\theta}\right\|_{2}^{2}\,,\] (2)

where \(\lambda>0\) is a regularization coefficient. Its closed-form solution is \(\hat{\bm{\theta}}_{V_{t},t-1}=\bm{M}_{V_{t},t-1}^{-1}\bm{b}_{V_{t},t-1}\), where \(\bm{M}_{V_{t},t-1}=\lambda\bm{I}+\sum_{i_{s}\in V_{t}}w_{i_{s},s}\bm{x}_{a_{s}} \bm{x}_{a_{s}}^{\top}\), \(\bm{b}_{V_{t},t-1}=\sum_{i_{s}\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}r_{a_{s}} \bm{x}_{a_{s}}\).

We set the weight of sample for user \(i_{s}\) in \(V_{t}\) at round \(s\) as \(w_{i_{s},s}=\min\{1,\alpha/\|\bm{x}_{a_{s}}\|_{M_{i_{s},s}^{\prime-1}}\}\), where \(\alpha\) is a coefficient to be determined later. The intuitions of designing these weights are as follows. The term \(\left\|\bm{x}_{a_{s}}\right\|_{M_{i_{s},s}^{\prime-1}}\) is the confidence radius of arm \(a_{s}\) for user \(i_{s}\) at \(s\), reflecting how confident the algorithm is about the estimation of \(i_{s}\)'s preference on \(a_{s}\) at \(s\). If \(\left\|\bm{x}_{a_{s}}\right\|_{M_{i_{s},s}^{\prime-1}}\) is large, it means the agent is uncertain of user \(i_{s}\)'s preference on \(a_{s}\), indicating this sample is probably corrupted.

Therefore, we use the inverse of confidence radius to assign a small weight to this round's sample if it is potentially corrupted. In this way, uncertain information for users in cluster \(V_{\ell}\) is assigned with less importance when estimating the \(V_{t}\)'s preference vector, which could help relieve the estimation inaccuracy caused by corruption. For technical details, please refer to Section 5.1 and Appendix.

**Recommend \(a_{t}\) with estimated preference of cluster \(V_{t}\).** Based on the corruption-robust preference estimation \(\hat{\bm{\theta}}_{V_{t},t-1}\) of cluster \(V_{t}\), in Line 7, the agent recommends an arm using the upper confidence bound (UCB) strategy to balance exploration and exploitation

\[a_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\bm{x}_{a}^{\top}\hat{\bm{ \theta}}_{V_{t},t-1}+\beta\left\lVert\bm{x}_{a}\right\rVert_{\bm{M}_{V_{t},t-1 }^{-1}}\triangleq\hat{R}_{a,t}+C_{a,t}\,,\] (3)

where \(\beta=\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T}{\lambda d} )}+\alpha C\) is the confidence radius parameter, \(\hat{R}_{a,t}\) denotes the estimated reward of arm \(a\) at \(t\), \(C_{a,t}\) denotes the confidence radius of arm \(a\) at \(t\). The design of \(C_{a,t}\) theoretically relies on Lemma 2 that will be given in Section 5.

**Update the robust estimation of user \(i_{t}\).** After receiving \(r_{t}\), the algorithm updates the estimation statistics of user \(i_{t}\), while keeping the statistics of others unchanged (Line 8 and Line 9). Specifically, RCLUB-WCU estimates the preference vector of user \(i_{t}\) by solving a weighted ridge regression

\[\hat{\bm{\theta}}_{i_{t},t}=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{R }^{d}}\sum\limits_{i_{a}=i_{t}}w_{i_{s},s}(r_{s}-\bm{x}_{a_{s}}^{\top}\bm{ \theta})^{2}+\lambda\left\lVert\bm{\theta}\right\rVert_{2}^{2}\] (4)

with closed-form solution \(\hat{\bm{\theta}}_{i_{t},t}=(\lambda\bm{I}+\bm{M}_{i_{t},t})^{-1}\bm{b}_{i_{t},t}\,,\) where \(\bm{M}_{i_{t},t}=\sum\limits_{\genfrac{}{}{0.0pt}{}{x\in[t]}{i_{s}=i_{t}}}w_{ i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top}\), \(\bm{b}_{i_{t},t}=\sum\limits_{\genfrac{}{}{0.0pt}{}{x\in[t]}{i_{s}=i_{t}}}w_{ i_{s},s}r_{a_{s}}\bm{x}_{a_{s}}\,,\) and we design the weights in the same way by the same reasoning.

**Update the dynamic graph.** Finally, with the updated statistics of user \(i_{t}\), RCLUB-WCU checks

```
1: Initialize \(\tilde{\mathcal{U}}_{t}=\emptyset\); input probability parameter \(\delta\).
2: Update the statistics for non-robust estimation of user \(i_{t}\)\(\tilde{\bm{M}}_{i_{t},t}=\tilde{\bm{M}}_{i_{t},t-1}+\bm{x}_{a_{t}}\bm{x}_{a_{t}}^ {\top}\), \(\tilde{\bm{b}}_{i_{t},t}=\tilde{\bm{b}}_{i_{t},t-1}+r_{t}\bm{x}_{a_{t}}\,,\)\(\hat{\bm{\theta}}_{i_{t},t}=(\lambda\bm{I}+\tilde{\bm{M}}_{i_{t},t})^{-1}\tilde{\bm{b}}_{i_{t},t }\,,\)
3: Keep non-robust estimation statistics of other users unchanged\(\tilde{\bm{M}}_{\ell,t}=\tilde{\bm{M}}_{\ell,t-1},\tilde{\bm{b}}_{\ell,t}= \tilde{\bm{b}}_{\ell,t-1},\tilde{\bm{\theta}}_{\ell,t}=\tilde{\bm{\theta}}_{ \ell,t-1},\) for all \(\ell\in\mathcal{U},\ell\neq i_{t}\,.\)
4:for all connected component \(V_{j,t}\in G_{t}\)do
5: Calculate the robust estimation statistics for the cluster \(V_{j,t}\): \(\bm{M}_{V_{j,t},t}=\lambda\bm{I}+\sum_{\ell\in V_{j,t}}\bm{M}_{\ell,t}\,,T_{V_ {j,t}}=\sum_{\ell\in V_{j,t}}T_{\ell,t}\,,\)\(\bm{b}_{V_{j,t},t}=\sum_{\ell\in V_{j,t}}\bm{b}_{V_{j,t},\ell}\,,\)\(\hat{\bm{\theta}}_{V_{j,t},\ell}=\bm{M}_{V_{j,t},\ell}^{-1}\bm{b}_{V_{j,t},\ell}\,;\)
6:for all user \(i\in V_{j,t}\)do
7: Detect user \(i\) to be a corrupted user and add user \(i\) to the set \(\tilde{\mathcal{U}}_{t}\) if the following holds: \[\left\lVert\tilde{\bm{\theta}}_{i,t}-\hat{\bm{\theta}}_{V_{i,t},t}\right\rVert _{2}>\frac{\sqrt{d\log(1+\frac{T_{i,t}}{\lambda d})+2\log(\frac{1}{\delta})}+ \sqrt{\lambda}}{\sqrt{\lambda_{\text{min}}(\tilde{\bm{M}}_{i,t})}+\lambda}+ \frac{\sqrt{d\log(1+\frac{T_{V_{i,t},t}}{\lambda d})+2\log(\frac{1}{\delta})}+ \sqrt{\lambda}+\alpha C}{\sqrt{\lambda_{\text{min}}(\bm{M}_{V_{i,t},t})}}\,,\] (5) where \(\lambda_{\text{min}}(\cdot)\) denotes the minimum eigenvalue of the matrix argument.
8:endfor
9:endfor ```

**Algorithm 2** OCCUD (At round \(t\), used in Line 11 in Algo.1)

whether the inferred \(i_{t}\)'s preference similarities with other users are still true, and updates the graph accordingly. Precisely, if gap between the updated estimation \(\hat{\bm{\theta}}_{i_{t},t}\) of \(i_{t}\) and the estimation \(\hat{\bm{\theta}}_{\ell,t}\) of user \(\ell\) exceeds a threshold in Line 10, RCLUB-WCU will delete the edge \((i_{t},\ell)\) in \(G_{t-1}\) to split them apart. The threshold is carefully designed to handle the estimation uncertainty from both stochastic noises and potential corruptions. The updated graph \(G_{t}=(\mathcal{U},E_{t})\) will be used in the next round.

### Occud

Based on the inferred clustering structure of RCLUB-WCU, we devise a novel online detection algorithm OCCUD (Algo.2). The design ideas and process of OCCUD are as follows.

Besides the robust preference estimations (with weighted regression) of users and clusters kept by RCLUB-WCU, OCCUD also maintains the non-robust estimations for each user by online ridge regression without weights (Line 2 and Line 3). Specifically, at round \(t\), OCCUD updates the non-robust estimation of user \(i_{t}\) by solving the following online ridge regression:

\[\tilde{\bm{\theta}}_{i_{t},t}=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{ R}^{d}}\sum\limits_{{}_{s\in[t]}\atop{}_{s\in i_{t}}}{(r_{s}-\bm{x}_{a_{s}}^{ \top}\bm{\theta})^{2}}+\lambda\left\|\bm{\theta}\right\|_{2}^{2}\,\] (6)

with solution \(\tilde{\bm{\theta}}_{i_{t},t}=(\lambda\bm{I}+\tilde{\bm{M}}_{i_{t},t})^{-1} \tilde{\bm{b}}_{i_{t},t}\), where \(\tilde{\bm{M}}_{i_{t},t}=\sum\limits_{{}_{s\in[t]}\atop{}_{s\in[t]}}{\bm{x}_{a _{s}}\bm{x}_{a_{s}}^{\top}}\tilde{\bm{b}}_{i_{t},t}=\sum\limits_{{}_{s\in[t]} \atop{}_{i_{s}=i_{t}}}{r_{a_{s}}\bm{x}_{a_{s}}}\).

With the robust and non-robust preference estimations, OCCUD does the following to detect corrupted users based on the clustering structure inferred by RCLUB-WCU. First, OCCUD finds the connected components in the graph kept by RCLUB-WCU, which represent the inferred clusters. Then, for each inferred cluster \(V_{j,t}\in G_{t}\): (1) OCCUD computes its robustly estimated preferences vector \(\hat{\bm{\theta}}_{V_{i,t}}\) (Line 5). (2) For each user \(i\) whose inferred cluster is \(V_{j,t}\) (_i.e.,\(i\in V_{j,t}\)_), OCCUD computes the gap between user \(i\)'s non-robustly estimated preference vector \(\tilde{\bm{\theta}}_{i,t}\) and the robust estimation \(\hat{\bm{\theta}}_{V_{i,t},t}\) for user \(i\)'s inferred cluster \(V_{j,t}\). If the gap exceeds a carefully-designed threshold, OCCUD will detect user \(i\) as corrupted and add \(i\) to the detected corrupted user set \(\tilde{\mathcal{U}}_{t}\) (Line 7).

The intuitions of OCCUD are as follows. On the one hand, after some interactions, RCLUB-WCU will infer the user clustering structure accurately. Thus, at round \(t\), the robust estimation \(\hat{\bm{\theta}}_{V_{i,t},t}\) for user \(i\)'s inferred cluster should be pretty close to user \(i\)'s ground-truth preference vector \(\bm{\theta}_{i}\). On the other hand, since the feedback of normal users are always regular, at round \(t\), if user \(i\) is a normal user, the non-robust estimation \(\tilde{\bm{\theta}}_{i,t}\) should also be close to the ground-truth \(\bm{\theta}_{i}\). However, the non-robust estimation of a corrupted user should be quite far from the ground truth due to corruptions. Based on this reasoning, OCCUD compares each user's non-robust estimation and the robust estimation of the user's inferred cluster to detect the corrupted users. For technical details, please refer to Section 5.2 and Appendix. Simple illustrations of our proposed algorithms can be found in Fig.2.

## 5 Theoretical Analysis

In this section, we theoretically analyze the performances of our proposed algorithms, RCLUB-WCU and OCCUD. Due to the page limit, we put the proofs in the Appendix.

### Regret Analysis of RCLUB-WCU

This section gives an upper bound of the expected regret (defined in Eq.(1)) for RCLUB-WCU.

The following lemma provides a sufficient time \(T_{0}(\delta)\), after which RCLUB-WCU can cluster all the users correctly with high probability.

Figure 2: Algorithm illustrations. Users 6 and 8 are corrupted users (orange), and the others are normal (green). (a) illustrates RCLUB-WCU, which starts with a complete user graph, and adaptively deletes edges between users (dashed lines) with dissimilar robustly learned preferences. The corrupted behaviors of users 6 and 8 may cause inaccurate preference estimations, leading to erroneous relation inference. In this case, how to delete edges correctly is non-trivial, and RCLUB-WCU addresses this challenge (detailed in Section 4.1). (b) illustrates OCCUD at some round \(t\), where person icons with triangle hats represent the non-robust user preference estimations. The gap between the non-robust estimation of user 6 and the robust estimation of user 6â€™s inferred cluster (circle \(C_{1}\)) exceeds the threshold \(r_{6}\) at this round (Line 7 in Algo.2), so OCCUD detects user 6 to be corrupted.

**Lemma 1**.: _With probability at least \(1-3\delta\), RCLUB-WCU will cluster all the users correctly after_

\[T_{0}(\delta)\triangleq 16u\log(\frac{u}{\delta})+4u\max\{\frac{288d}{\gamma^{2} \alpha\sqrt{\lambda\lambda_{x}}}\log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda }_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2}\delta}),\frac{72\sqrt{\lambda }}{\alpha\gamma^{2}\lambda_{x}},\frac{72\alpha C^{2}}{\gamma^{2}\sqrt{\lambda \lambda_{x}}}\}\]

_for some \(\delta\in(0,\frac{1}{3})\), where \(\tilde{\lambda}_{x}\triangleq\int_{0}^{\lambda_{x}}(1-e^{-\frac{(\lambda_{x}- x)^{2}}{2\sigma^{2}}})Kdx\), \(|\mathcal{A}_{t}|\leq K,\forall t\in[T]\)._

After \(T_{0}(\delta)\), the following lemma gives a bound of the gap between \(\hat{\bm{\theta}}_{V_{t},t-1}\) and the ground-truth \(\bm{\theta}_{i_{t}}\) in direction of action vector \(\bm{x}_{a}\) for RCLUB-WCU, which supports the design in Eq.(3).

**Lemma 2**.: _With probability at least \(1-4\delta\) for some \(\delta\in(0,\frac{1}{4})\), \(\forall t\geq T_{0}(\delta)\), we have:_

\[\Big{|}\bm{x}_{a}^{\mathrm{T}}(\hat{\bm{\theta}}_{V_{t},t-1}-\bm{\theta}_{i_ {t}})\Big{|}\leq\beta\,\|\bm{x}_{\bm{a}}\|_{\bm{M}_{V_{t},t-1}^{-1}}\triangleq C _{a,t}\,.\]

With Lemma 1 and 2, we prove the following theorem on the regret upper bound of RCLUB-WCU.

**Theorem 3** (**Regret Upper Bound of RCLUB-WCU)**.: _With the assumptions in Section 3, and picking \(\alpha=\frac{\sqrt{d}+\sqrt{\lambda}}{C}\), the expected regret of the RCLUB-WCU algorithm for \(T\) rounds satisfies_

\[R(T)\leq O\big{(}(\frac{C\sqrt{d}}{\gamma^{2}\tilde{\lambda}_{x}}+\frac{1}{ \tilde{\lambda}_{x}^{2}})u\log(T)\big{)}+O\big{(}d\sqrt{mT}\log(T)\big{)}+O \big{(}mCd\log^{1.5}(T)\big{)}\,.\] (7)

**Discussion and Comparison.** The regret bound in Eq.(7) has three terms. The first term is the time needed to get enough information for accurate robust estimations such that RCLUB-WCU could cluster all users correctly afterward with high probability. This term is related to the _corruption level_\(C\), which is inevitable since, if there are more corrupted user feedback, it will be harder for the algorithm to learn the clustering structure correctly. The last two terms correspond to the regret after \(T_{0}\) with the correct clustering. Specifically, the second term is caused by stochastic noises when leveraging the aggregated information within clusters to make recommendations; the third term associated with the _corruption level_\(C\) is the regret caused by the disruption of corrupted behaviors.

When the _corruption level_\(C\) is _unknown_, we can use its estimated upper bound \(\hat{C}\triangleq\sqrt{T}\) to replace \(C\) in the algorithm. In this way, if \(C\leq\hat{C}\), the bound will be replacing \(C\) with \(\hat{C}\) in Eq.(7); when \(C>\sqrt{T}\), \(R(T)=O(T)\), which is already optimal for a large class of bandit algorithms [12].

The following theorem gives a regret lower bound of the LOCUD problem.

**Theorem 4** (Regret lower bound for LOCUD).: _There exists a problem instance for the LOCUD problem such that for any algorithm_

\[R(T)\geq\Omega(d\sqrt{mT}+dC)\,.\]

Its proof and discussions can be found in Appendix D. The upper bound in Theorem 3 asymptotically matches this lower bound in \(T\) up to logarithmic factors, showing our regret bound is nearly optimal.

We then compare our regret upper bound with several degenerated cases. First, when \(C=0\), _i.e._, all users are normal, our setting degenerates to the classic CB problem [8]. In this case the bound in Theorem 3 becomes \(O(1/\tilde{\lambda}_{x}^{2}\cdot u\log(T))+O(d\sqrt{mT}\log(T))\), perfectly matching the state-of-the-art results in CB [8, 19, 21]. Second, when \(m=1\) and \(u=1\), _i.e._, there is only one user, our setting degenerates to linear bandits with adversarial corruptions [22, 12], and the bound in Theorem 3 becomes \(O(d\sqrt{T}\log(T))+O(Cd\log^{1.5}(T))\), it also perfectly matches the nearly optimal result in [12]. The above comparisons also show the tightness of the regret bound of RCLUB-WCU.

### Theoretical Performance Guarantee for OCCUD

The following theorem gives a performance guarantee of the online detection algorithm OCCUD.

**Theorem 5** (**Theoretical Guarantee for OCCUD)**.: _With assumptions in Section 3, at \(\forall t\geq T_{0}(\delta)\), for any detected corrupted user \(i\in\tilde{\mathcal{U}}_{t}\), with probability at least \(1-5\delta\), \(i\) is indeed a corrupted user._

This theorem guarantees that after RCLUB-WCU learns the clustering structure accurately, with high probability, the corrupted users detected by OCCUD are indeed corrupted, showing the high detection accuracy of OCCUD. The proof of Theorem 5 can be found in Appendix D.

Experiments

This section shows experimental results on synthetic and real data to evaluate RCLUB-WCU's recommendation quality and OCCUD's detection accuracy. We compare RCLUB-WCU to LinUCB [1] with a single non-robust estimated vector for all users, LinUCB-Ind with separate non-robust estimated vectors for each user, CW-OFUL [12] with a single robust estimated vector for all users, CW-OFUL-Ind with separate robust estimated vectors for each user, CLUB[8], and SCLUB[21]. More description of these baselines are in Appendix F. To show that the design of OCCUD is non-trivial, we develop a straightforward detection algorithm GCUD, which leverages the same cluster structure as OCCUD but detects corrupted users by selecting users with highest \(\left\|\hat{\bm{\theta}}_{i,t}-\hat{\bm{\theta}}_{V_{i,t}-1}\right\|_{2}\) in each inferred cluster. GCUD selects users according to the underlying percentage of corrupted users, which is unrealistic in practice, but OCCUD still performs better in this unfair condition.

**Remark.** The offline detection methods [39; 6; 18; 32] need to know all the user information in advance to derive the user embedding for classification, so they cannot be directly applied in online detection with bandit feedback thus cannot be directly compared to OCCUD. However, we observe the AUC achieved by OCCUD on Amazon and Yelp (in Tab.1) is similar to recent offline methods [18; 32]. Additionally, OCCUD has rigorous theoretical performance guarantee (Section 5.2).

### Experiments on Synthetic Dataset

We use \(u=1,000\) users and \(m=10\) clusters, where each cluster contains \(100\) users. We randomly select \(100\) users as the corrupted users. The preference and arm (item) vectors are drawn in \(d-1\) (\(d=50\)) dimensions with each entry a standard Gaussian variable and then normalized, added one more dimension with constant 1, and divided by \(\sqrt{2}\)[21]. We fix an arm set with \(|\mathcal{A}|=1000\) items, at each round, 20 items are randomly selected to form a set \(\mathcal{A}_{t}\) to choose from. Following [40; 3], in the first \(k\) rounds, we always flip the reward of corrupted users by setting \(r_{t}=-\bm{x}_{a_{t}}^{\text{T}}\bm{\theta}_{i,t}+\eta_{t}\). And we leave the remaining \(T-k\) rounds intact. Here we set \(T=1,000,000\) and \(k=20,000\).

Fig.3(a) shows the recommendation results. RCLUB-WCU outperforms all baselines and achieves a sub-linear regret. LinUCB and CW-OFUL perform worst as they ignore the preference differences among users. CW-OFUL-Ind outperforms LinUCB-Ind because it considers the corruption, but worse than RCLUB-WCU since it does not consider leveraging user relations to speed up learning.

The detection results are shown in Tab.1. We test the AUC of OCCUD and GCUD in every \(200,000\) rounds. OCCUD's performance improves over time with more interactions, while GCUD's performance is much worse as it detects corrupted users only relying on the robust estimations. OCCUD finally achieves an AUC of 0.855, indicating it can identify most of the corrupted users.

### Experiments on Real-world Datasets

We use three real-world data Movielens [11], Amazon[31], and Yelp [33]. The Movielens data does not have the corrupted users' labels, so following [24], we manually select the corrupted users. On Amazon data, following [39], we label the users with more than 80% helpful votes as normal users, and label users with less than 20% helpful votes as corrupted users. The Yelp data contains users and their comments on restaurants with true labels of the normal users and corrupted users.

We select 1,000 users and 1,000 items for Movielens; 1,400 users and 800 items for Amazon; 2,000 users and 2,000 items for Yelp. The ratios of corrupted users on these data are 10%, 3.5%, and

Figure 3: Recommendation results on the synthetic and real-world datasets

30.9%, respectively. We generate the preference and item vectors following [37; 21]. We first construct the binary feedback matrix through the users' ratings: if the rating is greater than 3, the feedback is 1; otherwise, the feedback is 0. Then we use SVD to decompose the extracted binary feedback matrix \(R_{u\times m}=\bm{\theta}SX^{\mathrm{T}}\), where \(\bm{\theta}=(\bm{\theta}_{i}),i\in[u]\) and \(X=(\bm{x}_{j}),j\in[m]\), and select \(d=50\) dimensions. We have 10 clusters on Movielens and Amazon, and 20 clusters on Yelp. We use the same corruption mechanism as the synthetic data with \(T=1,000,000\) and \(k=20,000\). We conduct more experiments in different environments to show our algorithms' robustness in Appendix.G. The recommendation results are shown in Fig.3(b)-(d). RCLUB-WCU outperforms all baselines. On the Amazon dataset, the percentage of corrupted users is lowest, RCLUB-WCU's advantages over baselines decrease because of the weakened corruption. The corrupted user detection results are provided in Tab.1. OCCUD's performance improves over time and is much better than GCUD. On the Movielens dataset, OCCUD achieves an AUC of 0.85; on the Amazon dataset, OCCUD achieves an AUC of 0.84; and on the Yelp dataset, OCCUD achieves an AUC of 0.628. According to recent works on offline settings [18; 32], our results are relatively high.

## 7 Conclusion

In this paper, we are the first to propose the novel LOCUD problem, where there are many users with _unknown_ preferences and _unknown_ relations, and some corrupted users can occasionally perform disrupted actions to fool the agent. Hence, the agent not only needs to learn the _unknown_ user preferences and relations robustly from potentially disrupted bandit feedback, balance the exploration-exploitation trade-off to minimize regret, but also needs to detect the corrupted users over time. To robustly learn and leverage the _unknown_ user preferences and relations from corrupted behaviors, we propose a novel bandit algorithm RCLUB-WCU. To detect the corrupted users in the online bandit setting, based on the learned user relations of RCLUB-WCU, we propose a novel detection algorithm OCCUD. We prove a regret upper bound for RCLUB-WCU, which matches the lower bound asymptotically in \(T\) up to logarithmic factors and matches the state-of-the-art results in degenerate cases. We also give a theoretical guarantee for the detection accuracy of OCCUD. Extensive experiments show that our proposed algorithms achieve superior performance over previous bandit algorithms and high corrupted user detection accuracy.

## 8 Acknowledgement

The corresponding author Shuai Li is supported by National Key Research and Development Program of China (2022ZD0114804) and National Natural Science Foundation of China (62376154, 62006151, 62076161). The work of John C.S. Lui was supported in part by the RGC's SRFS2122-4S02.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{AlgTime} & \multirow{2}{*}{0.2M} & \multirow{2}{*}{0.4M} & \multirow{2}{*}{0.6M} & \multirow{2}{*}{0.8M} & \multirow{2}{*}{1M} \\ \cline{3-3} \cline{5-8}  & & OCCUD & 0.599 & 0.651 & 0.777 & 0.812 & **0.855** \\ \cline{3-8} \cline{5-8} \multirow{2}{*}{Synthetic} & \multicolumn{2}{c|}{GCUD} & 0.477 & 0.478 & 0.483 & 0.484 & 0.502 \\ \cline{3-8} \cline{5-8}  & OCCUD & 0.65 & 0.750 & 0.785 & 0.83 & **0.85** \\ \cline{2-8} \cline{5-8}  & \multicolumn{2}{c|}{GCUD} & 0.450 & 0.474 & 0.485 & 0.489 & 0.492 \\ \hline \multirow{2}{*}{Amazon} & \multicolumn{2}{c|}{OCUD} & 0.639 & 0.735 & 0.761 & 0.802 & **0.840** \\ \cline{2-8} \cline{5-8}  & \multicolumn{2}{c|}{GCUD} & 0.480 & 0.480 & 0.486 & 0.500 & 0.518 \\ \hline \multirow{2}{*}{Yelp} & \multicolumn{2}{c|}{OCUD} & 0.452 & 0.489 & 0.502 & 0.578 & **0.628** \\ \cline{2-8} \cline{5-8}  & \multicolumn{2}{c|}{GCUD} & 0.473 & 0.481 & 0.496 & 0.500 & 0.510 \\ \hline \end{tabular}
\end{table}
Table 1: Detection results on synthetic and real datasets

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Charu C Aggarwal et al. _Recommender systems_, volume 1. Springer, 2016.
* [3] Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust to adversarial attacks. In _International Conference on Artificial Intelligence and Statistics_, pages 991-999. PMLR, 2021.
* [4] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* [5] Qin Ding, Cho-Jui Hsieh, and James Sharpnack. Robust stochastic linear contextual bandits under adversarial attacks. In _International Conference on Artificial Intelligence and Statistics_, pages 7111-7123. PMLR, 2022.
* [6] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph neural network-based fraud detectors against camouflaged fraudsters. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 315-324, 2020.
* [7] Evrard Garcelon, Baptiste Roziere, Laurent Meunier, Jean Tarbouriech, Olivier Teytaud, Alessandro Lazaric, and Matteo Pirotta. Adversarial attacks on linear contextual bandits. _Advances in Neural Information Processing Systems_, 33:14362-14373, 2020.
* [8] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In _International Conference on Machine Learning_, pages 757-765. PMLR, 2014.
* [9] Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial corruptions. In _Conference on Learning Theory_, pages 1562-1578. PMLR, 2019.
* [10] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John Lui, Wing Shing Wong, et al. Adversarial bandits with corruptions: Regret lower bound and no-regret algorithm. _Advances in Neural Information Processing Systems_, 33:19943-19952, 2020.
* [11] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. _Acm transactions on interactive intelligent systems (tiis)_, 5(4):1-19, 2015.
* [12] Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. In _Advances in Neural Information Processing Systems (2022)_, 2022.
* [13] Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Auc-oriented graph neural network for fraud detection. In _Proceedings of the ACM Web Conference 2022_, pages 1311-1321, 2022.
* [14] Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Jerry Zhu. Adversarial attacks on stochastic bandits. _Advances in neural information processing systems_, 31, 2018.
* [15] Pushmeet Kohli, Mahyar Salek, and Greg Stoddard. A fast bandit algorithm for recommendation to users with heterogenous tastes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 27, pages 1135-1141, 2013.
* [16] Fang Kong, Canzhe Zhao, and Shuai Li. Best-of-three-worlds analysis for linear bandits with follow-the-regularized-leader algorithm. _arXiv preprint arXiv:2303.06825_, 2023.
* [17] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670, 2010.

* [18] Qiutong Li, Yanshen He, Cong Xu, Feng Wu, Jianliang Gao, and Zhao Li. Dual-augment graph neural network for fraud detection. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 4188-4192, 2022.
* [19] Shuai Li and Shengyu Zhang. Online clustering of contextual cascading bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [20] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 539-548, 2016.
* [21] Shuai Li, Wei Chen, and Kwong-Sak Leung. Improved algorithm on online clustering of bandits. _arXiv preprint arXiv:1902.09162_, 2019.
* [22] Yingkai Li, Edmund Y Lou, and Liren Shan. Stochastic linear optimization with adversarial corruption. _arXiv preprint arXiv:1909.02109_, 2019.
* [23] Fang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In _International Conference on Machine Learning_, pages 4042-4050. PMLR, 2019.
* [24] Shenghua Liu, Bryan Hooi, and Christos Faloutsos. Holocene: Topology-and-spike aware fraud detection. In _Proceedings of the 2017 ACM on Conference on Information and Knowledge Management_, pages 1539-1548, 2017.
* [25] Xutong Liu, Haoru Zhao, Tong Yu, Shuai Li, and John CS Lui. Federated online clustering of bandits. In _Uncertainty in Artificial Intelligence_, pages 1221-1231. PMLR, 2022.
* [26] Xutong Liu, Jinhang Zuo, Siwei Wang, Carlee Joe-Wong, John Lui, and Wei Chen. Batch-size independent regret bounds for combinatorial semi-bandits with probabilistically triggered arms or independent arms. _Advances in Neural Information Processing Systems_, 35:14904-14916, 2022.
* [27] Xutong Liu, Jinhang Zuo, Siwei Wang, John CS Lui, Mohammad Hajiesmaili, Adam Wierman, and Wei Chen. Contextual combinatorial bandits with probabilistically triggered arms. In _International Conference on Machine Learning_, pages 22559-22593. PMLR, 2023.
* [28] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: a gnn-based imbalanced learning approach for fraud detection. In _Proceedings of the Web Conference 2021_, pages 3168-3177, 2021.
* [29] Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 114-122, 2018.
* [30] Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In _International Conference on Decision and Game Theory for Security_, pages 186-204. Springer, 2018.
* [31] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In _Proceedings of the 22nd international conference on World Wide Web_, pages 897-908, 2013.
* [32] Zidi Qin, Yang Liu, Qing He, and Xiang Ao. Explainable graph-based fraud detection via neural meta-graph search. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 4414-4418, 2022.
* [33] Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review networks and metadata. In _Proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining_, pages 985-994, 2015.
* [34] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. A semi-supervised graph attentive network for financial fraud detection. In _2019 IEEE International Conference on Data Mining (ICDM)_, pages 598-607. IEEE, 2019.

* [35] Zhiyong Wang, Xutong Liu, Shuai Li, and John CS Lui. Efficient explorative key-term selection strategies for conversational contextual bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10288-10295, 2023.
* [36] Zhiyong Wang, Jize Xie, Xutong Liu, Shuai Li, and John Lui. Online clustering of bandits with misspecified user models. _arXiv preprint arXiv:2310.02717_, 2023.
* [37] Junda Wu, Canzhe Zhao, Tong Yu, Jingyang Li, and Shuai Li. Clustering of conversational bandits for user preference learning and elicitation. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2129-2139, 2021.
* [38] Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collaborative environment. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 529-538, 2016.
* [39] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng. Fraudre: fraud detection dual-resistant to graph inconsistency and imbalance. In _2021 IEEE International Conference on Data Mining (ICDM)_, pages 867-876. IEEE, 2021.
* [40] Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Linear contextual bandits with adversarial corruptions. _arXiv preprint arXiv:2110.12615_, 2021.
* [41] Jinhang Zuo, Zhiyao Zhang, Zhiyong Wang, Shuai Li, Mohammad Hajiesmaili, and Adam Wierman. Adversarial attacks on online learning to rank with click feedback. _arXiv preprint arXiv:2305.17071_, 2023.

## Appendix A Proof of Lemma 1

We first prove the following result:

With probability at least \(1-\delta\) for some \(\delta\in(0,1)\), at any \(t\in[T]\):

\[\left\|\hat{\bm{\theta}}_{i,t}-\bm{\theta}^{j(i)}\right\|_{2}\leq\frac{\beta(T_ {i,t},\frac{\delta}{u})}{\sqrt{\lambda+\lambda_{\text{min}}(\bm{M}_{i,t})}}, \forall i\in\mathcal{U}\,,\] (8)

where \(\beta(T_{i,t},\frac{\delta}{u})\triangleq\sqrt{2\log(\frac{u}{8})+d\log(1+ \frac{T_{i,t}}{\lambda d})}+\sqrt{\lambda}+\alpha C\).

\[\hat{\bm{\theta}}_{i,t}-\bm{\theta}^{j(i)} =(\lambda\bm{I}+\bm{M}_{i,t})^{-1}\bm{b}_{i,t}-\bm{\theta}^{j(i)}\] \[=(\lambda\bm{I}+\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s} \bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w _{i_{s},s}\bm{x}_{a_{s}}r_{s}-\bm{\theta}^{j(i)}\] \[=(\lambda\bm{I}+\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s} \bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}\bigg{(}\sum_{{}_{s\in[t]}\atop{}_{ i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{s}}(\bm{x}_{a_{s}}^{\top}\bm{\theta}_{i_{s}}+ \eta_{s}+c_{s})\bigg{)}-\bm{\theta}^{j(i)}\] \[=(\lambda\bm{I}+\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s} \bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}\bigg{[}(\lambda\bm{I}+\sum_{{}_{s\in [t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})\bm{ \theta}^{j(i)}-\lambda\bm{\theta}^{j(i)}+\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w _{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\] \[\qquad+\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{ s}}c_{s}\bigg{]}-\bm{\theta}^{j(i)}\] \[=-\lambda\bm{M}_{i,t}^{\prime-1}\bm{\theta}^{j(i)}+\bm{M}_{i,t}^{ \prime-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s }+\bm{M}_{i,t}^{\prime-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x} _{a_{s}}c_{s}\,,\]

where we denote \(\bm{M}_{i,t}^{\prime}=\bm{M}_{i,t}+\lambda\bm{I}\), and the above equations hold by definition.

Therefore, we have

\[\left\|\hat{\bm{\theta}}_{i,t}-\bm{\theta}^{j(i)}\right\|_{2}\leq\lambda\left\| \bm{M}_{i,t}^{\prime-1}\bm{\theta}^{j(i)}\right\|_{2}+\left\|\bm{M}_{i,t}^{ \prime-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s }\right\|_{2}+\left\|\bm{M}_{i,t}^{\prime-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}= i}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}\right\|_{2}.\] (9)

We then bound the three terms in Eq.(9) one by one. For the first term:

\[\lambda\left\|\bm{M}_{i,t}^{\prime-1}\bm{\theta}^{j(i)}\right\|_{2}\leq\lambda \left\|\bm{M}_{i,t}^{\prime-\frac{1}{2}}\right\|_{2}^{2}\left\|\bm{\theta}^{j (i)}\right\|_{2}\leq\frac{\sqrt{\lambda}}{\sqrt{\lambda_{\text{min}}(\bm{M}_{i,t}^{\prime})}}\,,\] (10)

where we use the Cauchy-Schwarz inequality, the inequality for the operator norm of matrices, and the fact that \(\lambda_{\text{min}}(\bm{M}_{i,t}^{\prime})\geq\lambda\).

For the second term in Eq.(9), we have

\[\left\|\bm{M}_{i,t}^{\prime-1}\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i }}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\|_{2} \leq\left\|\bm{M}_{i,t}^{\prime-\frac{1}{2}}\sum_{{}_{s\in[t]} \atop{}_{i_{s}=i}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\|_{2}\left\|\bm{M}_ {i,t}^{\prime-\frac{1}{2}}\right\|_{2}\] (11) \[=\frac{\left\|\sum_{{}_{s\in[t]}\atop{}_{i_{s}=i}}w_{i_{s},s}\bm {x}_{a_{s}}\eta_{s}\right\|_{\bm{M}_{i,t}^{\prime-1}}}{\sqrt{\lambda_{\text{min} }(\bm{M}_{i,t}^{\prime})}}\,,\] (12)

where Eq.(11) follows by the Cauchy-Schwarz inequality and the inequality for the operator norm of matrices, and Eq.(12) follows by the Courant-Fischer theorem.

Let \(\tilde{\bm{x}}_{s}\triangleq\sqrt{w_{i_{s},s}}\bm{x}_{a_{s}}\), \(\tilde{\eta}_{s}\triangleq\sqrt{w_{i_{s},s}}\eta_{s}\), then we have: \(\left\|\tilde{\bm{x}}_{s}\right\|_{2}\leq\left\|\sqrt{w_{i_{s},s}}\right\|_{2} \left\|\bm{x}_{a_{s}}\right\|_{2}\leq 1\), \(\tilde{\eta}_{s}\) is still 1-sub-gaussian (since \(\eta_{s}\) is 1-sub-gaussian and \(\sqrt{w_{i_{s},s}}\leq 1\)), \(\bm{M}^{\prime}_{i,t}=\lambda\bm{I}+\sum_{s\in[t]\atop i_{s}=i}\tilde{\bm{x}}_{s }\tilde{\bm{x}}_{s}^{\top}\), and the nominator in Eq.(12) becomes \(\left\|\sum_{s\in[t]\atop i_{s}=i}\tilde{\bm{x}}_{s}\tilde{\eta}_{s}\right\|_ {\bm{M}^{\prime-1}_{i,t}}\). Then, following Theorem 1 in [1] and by union bound, with probability at least \(1-\delta\) for some \(\delta\in(0,1)\), for any \(i\in\mathcal{U}\), we have:

\[\left\|\sum_{s\in[t]\atop i_{s}=i}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s }\right\|_{\bm{M}^{\prime-1}_{i,t}} =\left\|\sum_{s\in[t]\atop i_{s}=i}\tilde{\bm{x}}_{s}\tilde{\eta} _{s}\right\|_{\bm{M}^{\prime-1}_{i,t}}\] \[\leq\sqrt{2\log(\frac{u}{\delta})+\log(\frac{\text{det}(\bm{M}^{ \prime}_{i,t})}{\text{det}(\lambda\bm{I})})}\] \[\leq\sqrt{2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d })}\,,\] (13)

where \(\text{det}(\cdot)\) denotes the determinant of matrix arguement, Eq.(13) is because \(\text{det}(\bm{M}^{\prime}_{i,t})\leq\left(\frac{\frac{\text{trace}(\lambda \bm{I}+\sum_{s\in[t]}w_{i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})}{d}}{d} \right)^{d}\leq\left(\frac{\lambda d+T_{i,t}}{d}\right)^{d}\), and \(\text{det}(\lambda\bm{I})=\lambda^{d}\).

For the third term in Eq.(9), we have

\[\left\|\bm{M}^{\prime-1}_{i,t}\sum_{s\in[t]\atop i_{s}=i}w_{i_{s},s}\bm{x}_{a_{s}}c\right\|_{2} \leq\left\|\bm{M}^{\prime-\frac{1}{2}}_{i,t}\sum_{s\in[t]\atop i _{s}=i}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}\right\|_{\bm{M}^{\prime-\frac{1}{2}}_{i, t}}\] (14) \[=\frac{\left\|\sum_{s\in[t]\atop i_{s}=i}w_{i_{s},s}\bm{x}_{a_{s} }c_{s}\right\|_{\bm{M}^{\prime-1}_{i,t}}}{\sqrt{\lambda_{\text{min}}(\bm{M}^{ \prime}_{i,t})}}\] (15) \[\leq\frac{\sum_{s\in[t]\atop i_{s}=i}|c_{s}|\,w_{i,s}\left\|\bm{x }_{a_{s}}\right\|_{\bm{M}^{\prime-1}_{i,t}}}{\sqrt{\lambda_{\text{min}}(\bm{M}^ {\prime}_{i,t})}}\] \[\leq\frac{\alpha C}{\sqrt{\lambda_{\text{min}}(\bm{M}^{\prime}_ {i,t})}}\] (16)

where Eq.(14) follows by the Cauchy-Schwarz inequality and the inequality for the operator norm of matrices, Eq.(15) follows by the Courant-Fischer theorem, and Eq.(16) is because by definition \(w_{i,s}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1}_{i,s }}}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1}_{i,t}}}\) (since \(\bm{M}^{\prime}_{i,t}\succeq\bm{M}^{\prime}_{i,s}\), \(\bm{M}^{\prime-1}_{i,s}\succeq\bm{M}^{\prime-1}_{i,t}\), \(\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1}_{i,s}}\geq\left\|\bm{x}_{a_{s} }\right\|_{\bm{M}^{\prime-1}_{i,t}}\)), \(\sum_{t=1}^{T}|c_{t}|\leq C\).

Combining the above bounds of these three terms, we can get that Eq.(8) holds.

We then prove the following technical lemma.

**Lemma 6**.: _Under Assumption 3, at any time \(t\), for any fixed unit vector \(\bm{\theta}\in\mathbb{R}^{d}\)_

\[\mathbb{E}_{t}[(\bm{\theta}^{\top}\bm{x}_{a_{t}})^{2}|\left|\mathcal{A}_{t} \right|]\geq\tilde{\lambda}_{x}\triangleq\int_{0}^{\lambda_{x}}(1-e^{-\frac{( \lambda_{x}-\pi)^{2}}{2\sigma^{2}}})^{K}dx\,,\] (17)

_where \(K\) is the upper bound of \(\left|\mathcal{A}_{t}\right|\) for any \(t\)._

Proof.: The proof of this lemma mainly follows the proof of Claim 1 in [8], but with more careful analysis, since their assumption on the arm generation distribution is more stringent than our Assumption 3 by putting more restrictions on the variance upper bound \(\sigma^{2}\) (specifically, they require \(\sigma^{2}\leq\frac{\lambda^{2}}{8\log(4K)}\)).

Denote the feasible arms at round \(t\) by \(\mathcal{A}_{t}=\{\bm{x}_{t,1},\bm{x}_{t,2},\ldots,\bm{x}_{t,|\mathcal{A}_{t}|}\}\). Consider the corresponding i.i.d. random variables \(\theta_{i}=(\bm{\theta}^{\top}\bm{x}_{t,i})^{2}-\mathbb{E}_{t}[(\bm{\theta}^{ \top}\bm{x}_{t,i})^{2}\,||\,\mathcal{A}_{t}||],i=1,2,\ldots,|\mathcal{A}_{t}|\). By Assumption 3, \(\theta_{i}\) s are sub-Gaussian random variables with variance bounded by \(\sigma^{2}\). Therefore, for any \(\alpha>0\) and any \(i\in[|\mathcal{A}_{t}|]\), we have:

\[\mathbb{P}_{t}(\theta_{i}<-\alpha||\mathcal{A}_{t}|)\leq e^{-\frac{\alpha^{2} }{2\sigma^{2}}}\,,\]

where we use \(\mathbb{P}_{t}(\cdot)\) to be the shorthand for the conditional probability \(\mathbb{P}(\cdot|(i_{1},\mathcal{A}_{1},r_{1}),\ldots,(i_{t-1},\mathcal{A}_{t -1},r_{t-1}),i_{t})\).

By Assumption 3, we can also get that \(\mathbb{E}_{t}[(\bm{\theta}^{\top}\bm{x}_{t,i})^{2}\,||\,\mathcal{A}_{t}|= \mathbb{E}_{t}[\bm{\theta}^{\top}\bm{x}_{t,i}\bm{x}_{t,i}^{\top}\bm{\theta} \,||\mathcal{A}_{t}|]\geq\lambda_{\text{min}}(\mathbb{E}_{\bm{x}\sim\rho}[\bm {x}\bm{x}^{\top}])\geq\lambda_{x}\). With these inequalities above, we can get

\[\mathbb{P}_{t}(\min_{i=1,\ldots,|\mathcal{A}_{t}|}(\bm{\theta}^{\top}\bm{x}_{t,i})^{2}\geq\lambda_{x}-\alpha||\mathcal{A}_{t}|)\geq(1-e^{-\frac{\alpha^{2} }{2\sigma^{2}}})^{K}\,.\]

Therefore, we can get

\[\mathbb{E}_{t}[(\bm{\theta}^{\top}\bm{x}_{a_{t}})^{2}\,|\,\mathcal{ A}_{t}|] \geq\mathbb{E}_{t}[\min_{i=1,\ldots,|\mathcal{A}_{t}|}(\bm{\theta }^{\top}\bm{x}_{t,i})^{2}||\,\mathcal{A}_{t}|]\] \[\geq\int_{0}^{\infty}\mathbb{P}_{t}(\min_{i=1,\ldots,|\mathcal{A} _{t}|}(\bm{\theta}^{\top}\bm{x}_{t,i})^{2}\geq x||\mathcal{A}_{t}|)dx\] \[\geq\int_{0}^{\lambda_{x}}(1-e^{-\frac{(\lambda_{x}-x)^{2}}{2 \sigma^{2}}})^{K}dx\triangleq\tilde{\lambda}_{x}\]

Note that \(w_{i,s}=\min\{1,\frac{\alpha}{\left\lVert\bm{x}_{a_{s}}\right\rVert_{\bm{M}_{i, t}^{\prime-1}}}\}\), and we have

\[\frac{\alpha}{\left\lVert\bm{x}_{a_{s}}\right\rVert_{\bm{M}_{i, t}^{\prime-1}}}=\frac{\alpha}{\sqrt{\bm{x}_{a_{s}}^{\top}\bm{M}_{i,t}^{\prime-1} \bm{x}_{a_{s}}}}\geq\frac{\alpha}{\sqrt{\lambda_{\text{min}}(\bm{M}_{i,t}^{ \prime-1})}}=\alpha\sqrt{\lambda_{\text{min}}(\bm{M}_{i,t}^{\prime})}\geq \alpha\sqrt{\lambda}.\]

Since \(\alpha\sqrt{\lambda}<1\) typically holds, we have \(w_{i,s}\geq\alpha\sqrt{\lambda}\).

Then, with the item regularity assumption stated in Assumption 3, the technical Lemma 6, together with Lemma 7 in [19], with probability at least \(1-\delta\), for a particular user \(i\), at any \(t\) such that \(T_{i,t}\geq\frac{16}{\lambda_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2} \delta})\), we have:

\[\lambda_{\text{min}}(\bm{M}_{i,t}^{\prime})\geq 2\alpha\sqrt{\lambda}\tilde{ \lambda}_{x}T_{i,t}+\lambda\,.\] (18)

With this result, together with Eq.(8), we can get that for any \(t\) such that \(T_{i,t}\geq\frac{16}{\lambda_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2} \delta})\), with probability at least \(1-\delta\) for some \(\delta\in(0,1)\), \(\forall i\in\mathcal{U}\), we have:

\[\left\lVert\hat{\bm{\theta}}_{i,t}-\bm{\theta}^{j(i)}\right\rVert _{2} \leq\frac{\beta(T_{i,t},\frac{\delta}{u})}{\sqrt{\lambda_{\text{ min}}(\bm{M}_{i,t}^{\prime})}}\] \[\leq\frac{\beta(T_{i,t},\frac{\delta}{u})}{\sqrt{2\alpha\sqrt{ \lambda}\tilde{\lambda}_{x}T_{i,t}+\lambda}}\] \[\leq\frac{\beta(T_{i,t},\frac{\delta}{u})}{\sqrt{2\alpha\sqrt{ \lambda}\tilde{\lambda}_{x}T_{i,t}}}\] \[=\frac{\sqrt{2\log(\frac{u}{\delta})}+d\log(1+\frac{T_{i,t}}{ \lambda d})}{\sqrt{2\alpha\sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t}}}\,.\] (19)

Then, we want to find a sufficient time \(T_{i,t}\) for a fixed user \(i\) such that

\[\left\lVert\hat{\bm{\theta}}_{i,t}-\bm{\theta}^{j(i)}\right\rVert_{2}<\frac{ \gamma}{4}\,.\] (20)To do this, with Eq.(19), we can get it by letting

\[\frac{\sqrt{\lambda}}{\sqrt{2\alpha\sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t}}}< \frac{\gamma}{12}\,,\] (21)

\[\frac{\alpha C}{\sqrt{2\alpha\sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t}}}<\frac{ \gamma}{12}\,,\] (22)

\[\frac{\sqrt{2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})}}{\sqrt{ 2\alpha\sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t}}}<\frac{\gamma}{12}\,.\] (23)

For Eq.(21), we can get

\[T_{i,t}>\frac{72\sqrt{\lambda}}{\alpha\gamma^{2}\tilde{\lambda}_{x}}\,.\] (24)

For Eq.(22), we can get

\[T_{i,t}>\frac{72\alpha C^{2}}{\gamma^{2}\sqrt{\lambda}\tilde{\lambda}_{x}}\,.\] (25)

For Eq.(23), we have

\[\frac{2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})}{2\alpha \sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t}}<\frac{\gamma^{2}}{144}\,.\] (26)

Then it is sufficient to get Eq.(26) if the following holds

\[\frac{2\log(\frac{u}{\delta})}{2\alpha\sqrt{\lambda}\tilde{\lambda}_{x}T_{i,t }}<\frac{\gamma^{2}}{288}\,,\] (27)

\[\frac{d\log(1+\frac{T_{i,t}}{\lambda d})}{2\alpha\sqrt{\lambda}\tilde{\lambda} _{x}T_{i,t}}<\frac{\gamma^{2}}{288}\,.\] (28)

For Eq.(27), we can get

\[T_{i,t}>\frac{288\log(\frac{u}{\delta})}{\gamma^{2}\alpha\sqrt{\lambda}\tilde {\lambda}_{x}}\] (29)

For Eq.(28), we can get

\[T_{i,t}>\frac{144d}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}}\log(1+ \frac{T_{i,t}}{\lambda d})\,.\] (30)

Following Lemma 9 in [19], we can get the following sufficient condition for Eq.(30):

\[T_{i,t}>\frac{288d}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}}\log( \frac{288}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}})\,.\] (31)

Then, since typically \(\frac{u}{\delta}>\frac{288}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}}\), we can get the following sufficient condition for Eq.(29) and Eq.(31)

\[T_{i,t}>\frac{288d}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}}\log( \frac{u}{\delta})\,.\] (32)

Together with Eq.(24), Eq.(25), and the condition for Eq.(18) we can get the following sufficient condition for Eq.(20) to hold

\[T_{i,t}>\max\{\frac{288d}{\gamma^{2}\alpha\sqrt{\lambda}\tilde{\lambda}_{x}} \log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda}_{x}^{2}}\log(\frac{8d}{\tilde {\lambda}_{x}^{2}\delta}),\frac{72\sqrt{\lambda}}{\alpha\gamma^{2}\tilde{ \lambda}_{x}},\frac{72\alpha C^{2}}{\gamma^{2}\sqrt{\lambda}\tilde{\lambda}_{x }}\}\,.\] (33)

Then, with Assumption 2 on the uniform arrival of users, following Lemma 8 in [19], and by union bound, we can get that with probability at least \(1-\delta\), for all

\[t\geq T_{0}\triangleq 16u\log(\frac{u}{\delta})+4u\max\{\frac{288d}{\gamma^{2} \alpha\sqrt{\lambda}\tilde{\lambda}_{x}}\log(\frac{u}{\delta}),\frac{16}{ \tilde{\lambda}_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2}\delta}),\frac{ 72\sqrt{\lambda}}{\alpha\gamma^{2}\tilde{\lambda}_{x}},\frac{72\alpha C^{2}}{ \gamma^{2}\sqrt{\lambda}\tilde{\lambda}_{x}}\}\,,\] (34)Eq.(32) holds for all \(i\in\mathcal{U}\), and therefore Eq.(20) holds for all \(i\in\mathcal{U}\). With this, we can show that RCLUB-WCU will cluster all the users correctly after \(T_{0}\). First, if RCLUB-WCU deletes the edge \((i,l)\), then user \(i\) and user \(j\) belong to different ground-truth clusters, i.e., \(\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{l}\right\|_{2}>0\). This is because by the deletion rule of the algorithm, the concentration bound, and triangle inequality, \(\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{l}\right\|_{2}=\left\| \boldsymbol{\theta}^{j(i)}-\boldsymbol{\theta}^{j(l)}\right\|_{2}\geq\left\| \boldsymbol{\hat{\theta}}_{i,t}-\hat{\boldsymbol{\theta}}_{l,t}\right\|_{2}- \left\|\boldsymbol{\theta}^{j(l)}-\boldsymbol{\theta}_{l,t}\right\|_{2}- \left\|\boldsymbol{\theta}^{j(i)}-\boldsymbol{\theta}_{i,t}\right\|_{2}>0\). Second, we show that if \(\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{l}\right\|\geq\gamma\), RCLUB-WCU will delete the edge \((i,l)\). This is because if \(\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{l}\right\|\geq\gamma\), then by the triangle inequality, and \(\left\|\hat{\boldsymbol{\theta}}_{i,t}-\boldsymbol{\theta}^{j(i)}\right\|_{ 2}<\frac{\gamma}{4}\), \(\left\|\hat{\boldsymbol{\theta}}_{l,t}-\boldsymbol{\theta}^{j(l)}\right\|_{2} <\frac{\gamma}{4}\), \(\left\|\boldsymbol{\theta}_{i}=\boldsymbol{\theta}^{j(i)}\), \(\boldsymbol{\theta}_{l}=\boldsymbol{\theta}^{j(l)}\), we have \(\left\|\boldsymbol{\hat{\theta}}_{i,t}-\hat{\boldsymbol{\theta}}_{l,t}\right\| _{2}\geq\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{l}\right\|-\left\| \hat{\boldsymbol{\theta}}_{i,t}-\boldsymbol{\theta}^{j(i)}\right\|_{2}- \left\|\hat{\boldsymbol{\theta}}_{l,t}-\boldsymbol{\theta}^{j(l)}\right\|_{2}> \gamma-\frac{\gamma}{4}-\frac{\gamma}{4}=\frac{\gamma}{2}>\frac{\sqrt{\lambda }+\sqrt{2\log(\frac{\gamma}{2})+d\log(1+\frac{T_{1,t}}{\lambda d})}}{\sqrt{ \lambda+2\lambda_{x}T_{t,t}}}+\frac{\sqrt{\lambda}+\sqrt{2\log(\frac{\gamma} {2})+d\log(1+\frac{T_{1,t}}{\lambda d})}}{\sqrt{\lambda+2\lambda_{x}T_{t,t}}}\), which will trigger the deletion condition Line 10 in Algo.1.

## Appendix B Proof of Lemma 2

After \(T_{0}\), if the clustering structure is correct, _i.e._, \(V_{t}=V_{j(i_{t})}\), then we have

\[\hat{\boldsymbol{\theta}}_{V_{t},t-1}-\boldsymbol{\theta}_{i_{t}} =\boldsymbol{M}_{V_{t},t-1}^{-1}\boldsymbol{b}_{V_{t},t-1}- \boldsymbol{\theta}_{i_{t}}\] (35) \[=(\lambda\boldsymbol{I}+\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_ {s},s}\boldsymbol{x}_{a_{s}}\boldsymbol{x}_{a_{s}}^{\top})^{-1}\big{(}\sum_{s \in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\boldsymbol{x}_{a_{s}}(\boldsymbol{x }_{a_{s}}^{\top}\boldsymbol{\theta}_{i_{t}}+\eta_{s}+c_{s})\big{)}-\boldsymbol {\theta}_{i_{t}}\] \[=(\lambda\boldsymbol{I}+\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_ {s},s}\boldsymbol{x}_{a_{s}}\boldsymbol{x}_{a_{s}}^{\top})^{-1}\bigg{(}\sum_{s \in[t-1]\atop i_{s}\in V_{t}}(w_{i_{s},s}\boldsymbol{x}_{a_{s}}\boldsymbol{x }_{a_{s}}^{\top}+\lambda\boldsymbol{I})\boldsymbol{\theta}_{i_{t}}-\lambda \boldsymbol{\theta}_{i_{t}}\] \[\qquad+\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\boldsymbol {x}_{a_{s}}\eta_{s}+\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s} \boldsymbol{x}_{a_{s}}c_{s}\bigg{)}\bigg{)}-\boldsymbol{\theta}_{i_{t}}\] \[=-\lambda\boldsymbol{M}_{V_{t},t-1}^{\prime-1}\boldsymbol{\theta }_{i_{t}}-\boldsymbol{M}_{V_{t},t-1}^{\prime-1}\sum_{s\in[t-1]\atop i_{s}\in V _{t}}w_{i_{s},s}\boldsymbol{x}_{a_{s}}\eta_{s}+\boldsymbol{M}_{V_{t},t-1}^{ \prime-1}\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\boldsymbol{x}_{a_{s }}c_{s}\,,\]

where we denote \(\boldsymbol{M}_{V_{t},t-1}^{\prime}=\boldsymbol{M}_{V_{t},t-1}+\lambda \boldsymbol{I}\), and Eq.(35) is because \(V_{t}=V_{j(i_{t})}\) thus \(\boldsymbol{\theta}_{i_{s}}=\boldsymbol{\theta}_{i_{t}},\forall i_{s}\in V_{t}\).

Therefore, we have

\[\left|\boldsymbol{x}_{a}^{\top}(\hat{\boldsymbol{\theta}}_{V_{t},t -1}-\boldsymbol{\theta}_{i_{t}})\right| \leq\lambda\left|\boldsymbol{x}_{a}^{\top}\boldsymbol{M}_{V_{t},t -1}^{\prime-1}\boldsymbol{\theta}_{i_{t}}\right|+\left|\boldsymbol{x}_{a}^{ \top}\boldsymbol{M}_{V_{t},t-1}^{\prime-1}\sum_{s\in[t-1]\atop i_{s}\in V_{t }}w_{i_{s},s}\boldsymbol{x}_{a_{s}}\eta_{s}\right|+\left|\boldsymbol{x}_{a}^{ \top}\boldsymbol{M}_{V_{t},t-1}^{\prime-1}\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s} \boldsymbol{x}_{a_{s}}c_{s}\right|\] \[\leq\left\|\boldsymbol{x}_{a}\right\|_{\boldsymbol{M}_{V_{t},t-1}^ {\prime-1}}\left(\sqrt{\lambda}+\left\|\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i _{s},s}\boldsymbol{x}_{a_{s}}\eta_{s}\right\|\right.\right.\] \[\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \left.\cdot\left.\cdot\right.\right.\right.\right.\right.\right.\right.\right. \right.\right.\right.\right\|_{M_{V_{t},t-1}^{\prime-1}}\boldsymbol{ \theta}_{i_{t}}\right|\right.\right.\right.\right.\right.\right.\right.\right.\},\] (36)

where Eq.(36) is by Cauchy-Schwarz inequality, matrix operator inequality, and \(\left|\boldsymbol{x}_{a}^{\top}\boldsymbol{M}_{V_{t},t-1}^{\prime-1}\boldsymbol{ \theta}_{i_{t}}\right|\leq\lambda\left\|\boldsymbol{M}_{V_{t},t-1}^{\prime-\frac {1}{2}}\right\|\left\|\boldsymbol{\theta}_{i_{t}}\right\|_{2}=\lambda\frac{1}{ \sqrt{\lambda_{\text{un}}(\boldsymbol{M}_{V_{t},t-1})}}\left\|\boldsymbol{ \theta}_{i_{t}}\right\|_{2}\leq\sqrt{\lambda}\) since \(\lambda_{\text{min}}(\boldsymbol{M}_{V_{t},t-1})\geq\lambda\) and \(\left\|\boldsymbol{\theta}_{i_{t}}\right\|_{2}\leq 1\).

Let \(\tilde{\boldsymbol{x}}_{s}\triangleq\sqrt{w_{i_{s},s}}\boldsymbol{x}_{a_{s}}\), \(\tilde{\eta}_{s}\triangleq\sqrt{w_{i_{s},s}}\eta_{s}\), then we have: \(\left\|\tilde{\boldsymbol{x}}_{s}\right\|_{2}\leq\left\|\sqrt{w_{i_{s},s}} \right\|_{2}\left\|\boldsymbol{x}_{a_{s}}\right\|_{2}\leq 1\), \(\tilde{\eta}_{s}\) is still 1-sub-gaussian (since \(\eta_{s}\) is 1-sub-gaussian and \(\sqrt{w_{i_{s},s}}\leq 1\)), \(\boldsymbol{M}_{i,t}^{\prime}=\lambda\boldsymbol{I}+\sum_{s\in[t] \atop i_{s}=i}\tilde{\boldsymbol{x}}_{s}\tilde{\boldsymbol{x}}_{s}^{\top}\)and \(\left\|\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\| _{\bm{M}^{\prime-1}_{V_{t},t-1}}\) becomes \(\left\|\sum_{s\in[t]\atop i_{s}=i}^{s[t]}\tilde{\bm{x}}_{s}\tilde{\eta}_{s} \right\|_{\bm{M}^{\prime-1}_{V_{t},t-1}}\). Then, following Theorem 1 in [1], with probability at least \(1-\delta\) for some \(\delta\in(0,1)\), we have:

\[\left\|\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\bm{x}_{a_{s} }\eta_{s}\right\|_{\bm{M}^{\prime-1}_{V_{t},t-1}} =\left\|\sum_{s\in[t]\atop i_{s}=i}^{s[t]}\tilde{\bm{x}}_{s}\tilde {\eta}_{s}\right\|_{\bm{M}^{\prime-1}_{V_{t},t-1}}\] \[\leq\sqrt{2\log(\frac{u}{\delta})+\log(\frac{\text{det}(\bm{M}^{ \prime}_{V_{t},t-1})}{\text{det}(\lambda\bm{I})})}\] \[\leq\sqrt{2\log(\frac{u}{\delta})+d\log(1+\frac{T}{\lambda d})}\,,\] (37)

And for \(\left\|\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s} \right\|_{\bm{M}^{\prime-1}_{V_{t},t-1}}\), we have

\[\left\|\sum_{s\in[t-1]\atop i_{s}\in V_{t}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s} \right\|_{\bm{M}^{\prime-1}_{V_{t},t-1}}\leq\sum_{s\in[t-1]\atop i_{s}\in V_{ t}}w_{i_{s},s}\left|c_{s}\right|\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{ \prime-1}_{V_{t},t-1}}\leq\alpha C\,,\] (38)

where we use \(\sum_{t=1}^{T}\left|c_{t}\right|\leq C\), \(w_{i_{s},s}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1 }_{s,t-1}}}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1}_ {V_{t},t-1}}}\).

Plugging Eq.(38) and Eq.(37) into Eq.(36), together with Lemma 1, we can complete the proof of Lemma 2.

## Appendix C Proof of Theorem 3

After \(T_{0}\), we define event

\[\mathcal{E}=\{\text{the algorithm clusters all the users correctly for all }t\geq T_{0}\}\,.\] (39)

Then, with Lemma 1 and picking \(\delta=\frac{1}{T}\), we have

\[\begin{split} R(T)&=\mathbb{P}(\mathcal{E})\mathbb{I }\{\mathcal{E}\}R(T)+\mathbb{P}(\overline{\mathcal{E}})\mathbb{I}\{\overline {\mathcal{E}}\}R(T)\\ &\leq\mathbb{I}\{\mathcal{E}\}R(T)+4\times\frac{1}{T}\times T\\ &=\mathbb{I}\{\mathcal{E}\}R(T)+4\,.\end{split}\] (40)

Then it remains to bound \(\mathbb{I}\{\mathcal{E}\}R(T)\). For the first \(T_{0}\) rounds, we can upper bound the regret in the first \(T_{0}\) rounds by \(T_{0}\). After \(T_{0}\), under event \(\mathcal{E}\) and by Lemma 2, we have that with probability at least \(1-\delta\), for any \(\bm{x}_{a}\):

\[\left|\bm{x}_{a}^{\mathrm{T}}(\hat{\bm{\theta}}_{V_{t},t-1}-\bm{\theta}_{i_{t} })\right|\leq\beta\left\|\bm{x}_{a}\right\|_{\bm{M}^{-1}_{V_{t},t-1}}\triangleq C _{a,t}\,.\] (41)

Therefore, for the instantaneous regret \(R_{t}\) at round \(t\), with \(\mathcal{E}\), with probability at least \(1-\delta\), at \(\forall t\geq T_{0}\):

\[\begin{split} R_{t}&=\bm{x}_{a_{t}^{\mathrm{T}}}^{ \top}\bm{\theta}_{i_{t}}-\bm{x}_{a_{t}}^{\mathrm{T}}\bm{\theta}_{i_{t}}\\ &=\bm{x}_{a_{t}^{\mathrm{T}}}^{\top}(\bm{\theta}_{i_{t}}-\hat{\bm {\theta}}_{V_{t},t-1})+(\bm{x}_{a_{t}^{\mathrm{T}}}^{\top}\hat{\bm{\theta}}_{V_{ t},t-1}+C_{a_{t}^{\mathrm{T}},t})-(\bm{x}_{a_{t}}^{\top}\hat{\bm{\theta}}_{V_{ t},t-1}+C_{a_{t},t})\\ &\quad+\bm{x}_{a_{t}}^{\top}(\hat{\bm{\theta}}_{V_{t},t-1}-\bm{ \theta}_{i_{t}})+C_{a_{t},t}-C_{a_{t}^{\mathrm{T}},t}\\ &\leq 2C_{a_{t},t}\,,\end{split}\] (42)

where the last inequality holds by the UCB arm selection strategy in Eq.(3) and Eq.(41).

Therefore, for \(\mathbb{I}\{\mathcal{E}\}R(T)\):

\[\mathbb{I}\{\mathcal{E}\}R(T) \leq R(T_{0})+\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^ {T}R_{t}]\] \[\leq T_{0}+2\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^ {T}C_{a_{t},t}]\,.\] (43)

Then it remains to bound \(\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}C_{a_{t},t}]\). For \(\sum_{t=T_{0}+1}^{T}C_{a_{t},t}\), we can distinguish it into two cases:

\[\sum_{t=T_{0}+1}^{T}C_{a_{t},t} \leq\beta\sum_{t=1}^{T}\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{t},t-1}^{-1}}\] \[=\beta\sum_{t\in[T]:w_{t_{t},t}=1}\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{t },t-1}^{-1}}+\beta\sum_{t\in[T]:w_{t_{t},t}<1}\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{t },t-1}^{-1}}\,.\] (44)

Then, we prove the following technical lemma.

**Lemma 7**.: \[\sum_{t=T_{0}+1}^{T}\min\{\mathbb{I}\{i_{t}\in V_{j}\}\,\|\bm{x}_{a_{t}}\|_{ \bm{M}_{V_{j},t-1}^{-1}}^{2},1\}\leq 2d\log(1+\frac{T}{\lambda d}),\forall j \in[m]\,.\] (45)

Proof.: \[det(\bm{M}_{V_{j},T}) =det\bigg{(}\bm{M}_{V_{j},T-1}+\mathbb{I}\{i_{T}\in V_{j}\}\bm{x }_{a_{T}}\bm{x}_{a_{T}}^{\top}\bigg{)}\] \[=det(\bm{M}_{V_{j},T-1})det\bigg{(}\bm{I}+\mathbb{I}\{i_{T}\in V _{j}\}\bm{M}_{V_{j},T-1}^{-\frac{1}{2}}\bm{x}_{a_{T}}\bm{x}_{a_{T}}^{\top}\bm {M}_{V_{j},T-1}^{-\frac{1}{2}}\bigg{)}\] \[=det(\bm{M}_{V_{j},T_{0}})\prod_{t=T_{0}+1}^{T}\bigg{(}1+\mathbb{ I}\{i_{t}\in V_{j}\}\,\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{j},t-1}^{2}}^{2}\bigg{)}\] \[\geq det(\lambda\bm{I})\prod_{t=T_{0}+1}^{T}\bigg{(}1+\mathbb{I} \{i_{t}\in V_{j}\}\,\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{j},t-1}^{-1}}^{2}\bigg{)}\,.\] (46)

\(\forall x\in[0,1]\), we have \(x\leq 2\log(1+x)\). Therefore

\[\sum_{t=T_{0}+1}^{T}\min\{\mathbb{I}\{i_{t}\in V_{j}\}\,\|\bm{x}_ {a_{t}}\|_{\bm{M}_{V_{j},t-1}^{-1}}^{2},1\} \leq 2\sum_{t=T_{0}+1}^{T}\log\bigg{(}1+\mathbb{I}\{i_{t}\in V_{j} \}\,\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{j},t-1}^{2}}^{2}\bigg{)}\] \[=2\log\bigg{(}\prod_{t=T_{0}+1}^{T}\big{(}1+\mathbb{I}\{i_{t}\in V _{j}\}\,\|\bm{x}_{a_{t}}\|_{\bm{M}_{V_{j},t-1}^{2}}^{2}\big{)}\bigg{)}\] \[\leq 2[\log(det(\bm{M}_{V_{j},T}))-\log(det(\lambda\bm{I}))]\] \[\leq 2\log\bigg{(}\frac{trace(\lambda\bm{I}+\sum_{t=1}^{T} \mathbb{I}\{i_{t}\in V_{j}\}\bm{x}_{a_{t}}\bm{x}_{a_{t}}^{\top})}{\lambda d} \bigg{)}^{d}\] \[\leq 2d\log(1+\frac{T}{\lambda d})\,.\] (47)Denote the rounds with \(w_{i_{t},t}=1\) as \(\{\tilde{t}_{1},\ldots,\tilde{t}_{l_{1}}\}\), and gram matrix \(\tilde{\bm{G}}_{V_{\tilde{t}_{r}},\tilde{t}_{r}-1}\triangleq\lambda\bm{I}+\sum_{ \genfrac{}{}{0.0pt}{}{s,s\in[\tau]}{s_{\tilde{t}_{r}}}}\bm{x}_{a_{\tilde{t}_{r}}} \bm{x}_{a_{\tilde{t}_{r}}}^{\top}\); denote the rounds with \(w_{i_{t},t}<1\) as \(\{t^{\prime}_{1},\ldots,t^{\prime}_{l_{2}}\}\), gram matrix \(\bm{G}^{\prime}_{V_{t^{\prime}_{r}},t^{\prime}_{r}-1}\triangleq\lambda\bm{I}+ \sum_{\genfrac{}{}{0.0pt}{}{s,s\in[\tau]}{s_{\tilde{t}_{r}}}}w_{i_{\tilde{t}_{ r}},t^{\prime}_{r}}\bm{x}_{a_{\tilde{t}_{r}}}\bm{x}_{a_{\tilde{t}_{r}}}^{ \top}\).

Then we have

\[\sum_{t\in[T]:w_{i_{t},t}=1}\|\bm{x}_{\bm{a}_{t}}\|_{\bm{M}^{-1}_{ V_{t},t-1}} =\sum_{j=1}^{m}\sum_{\tau=1}^{l_{1}}\mathbb{I}\{i_{\tilde{t}_{r}} \in V_{j}\}\left\|\bm{x}_{\bm{a}_{t_{\tau}}}\right\|_{\bm{M}^{-1}_{V_{\tilde{t} _{r}},\tilde{t}_{r}-1}}\leq\sum_{j=1}^{m}\sum_{\tau=1}^{l_{1}}\mathbb{I}\{i_{ \tilde{t}_{r}}\in V_{j}\}\left\|\bm{x}_{\bm{a}_{t_{\tau}}}\right\|_{\tilde{\bm {G}}^{-1}_{V_{\tilde{t}_{r}},\tilde{t}_{r}-1}}\] (48) \[\leq\sum_{j=1}^{m}\sqrt{\sum_{\tau=1}^{l_{1}}\mathbb{I}\{i_{ \tilde{t}_{r}}\in V_{j}\}\sum_{\tau=1}^{l_{1}}\min\{1,\mathbb{I}\{i_{\tilde{t} _{r}}\in V_{j}\}\left\|\bm{x}_{\bm{a}_{t_{\tau}}}\right\|_{\tilde{\bm{G}}^{-1} _{V_{\tilde{t}_{r}},\tilde{t}_{r}-1}}^{2}\}}\] (49) \[\leq\sum_{j=1}^{m}\sqrt{T_{V_{j},T}\times 2d\log(1+\frac{T}{ \lambda d})}\] (50) \[\leq\sqrt{2m\sum_{j=1}^{m}T_{V_{j},T}d\log(1+\frac{T}{\lambda d} )}=\sqrt{2mdT\log(1+\frac{T}{\lambda d})}\,,\] (51)

where Eq.(48) is because \(\tilde{\bm{G}}^{-1}_{V_{\tilde{t}_{r}},\tilde{t}_{r}-1}\succeq\bm{M}^{-1}_{V_ {\tilde{t}_{r}},\tilde{t}_{r}-1}\) in Eq.(49) we use Cauchy-Schwarz inequality, in Eq.(50) we use Lemma 7 and \(\sum_{\tau=1}^{l_{1}}\mathbb{I}\{i_{\tilde{t}_{r}}\in V_{j}\}\leq T_{V_{j},T}\), in Eq.(51) we use Cauchy-Schwarz inequality and \(\sum_{j=1}^{m}T_{V_{j},T}=T\).

For the second part in Eq.(44), Let \(\bm{x}^{\prime}_{a_{\tilde{t}_{r}^{\prime}}}\triangleq\sqrt{w_{i_{\tilde{t}_{ r}^{\prime}},t^{\prime}_{r}}}\bm{x}_{a_{\tilde{t}_{r}^{\prime}}}\), then

\[\sum_{t:w_{i_{t},t}<1}\left\|\bm{x}_{\bm{a}_{t}}\right\|_{\bm{M}^ {-1}_{V_{t},t-1}} =\sum_{t:w_{i_{t},t}<1}\frac{\left\|\bm{x}_{\bm{a}_{t}}\right\|_{ \bm{M}^{-1}_{V_{t},t-1}}^{2}}{\left\|\bm{x}_{\bm{a}_{t}}\right\|_{\bm{M}^{-1}_ {V_{t},t-1}}^{2}}=\sum_{t:w_{i_{t},t}<1}\frac{w_{i_{t},t}\left\|\bm{x}_{\bm{a}_{ t}}\right\|_{\bm{M}^{-1}_{V_{t},t-1}}^{2}}{\alpha}\] (52) \[=\sum_{j=1}^{m}\sum_{\tau=1}^{l_{2}}\mathbb{I}\{i_{\tilde{t}_{r}^ {\prime}}\in V_{j}\}\frac{w_{i_{\tilde{t}_{r}^{\prime}},t^{\prime}_{r}}}{ \alpha}\left\|\bm{x}_{a_{\tilde{t}_{r}^{\prime}}}\right\|_{\bm{M}^{-1}_{V_{ \tilde{t}_{r}^{\prime}},t^{\prime}_{r}-1}}^{2}\] \[\leq\sum_{j=1}^{m}\frac{\sum_{\tau=1}^{l_{2}}\min\{1,\mathbb{I}\{ i_{\tilde{t}_{r}^{\prime}}\in V_{j}\}\left\|\bm{x}^{\prime}_{a_{\tilde{t}_{r}^{ \prime}}}\right\|_{\bm{G}^{\prime-1}_{V_{\tilde{t}_{r}^{\prime}},t^{\prime}_{r} -1}}^{2}\}}{\alpha}\] (53) \[\leq\sum_{j=1}^{m}\frac{2d\log(1+\frac{T}{\lambda d})}{\alpha}= \frac{2md\log(1+\frac{T}{\lambda d})}{\alpha}\] (54)

where in Eq.(52) we use the definition of the weights, in Eq.(53) we use \(\bm{G}^{\prime-1}_{V_{\tilde{t}_{r}^{\prime}},t^{\prime}_{r}-1}\succeq\bm{M}^{-1} _{V_{\tilde{t}_{r}^{\prime}},t^{\prime}_{r}-1}\), and Eq.(54) uses Lemma 7.

Then, with Eq.(54), Eq.(51), Eq.(44), Eq.(40), Eq.(43), \(\delta=\frac{1}{T}\), and \(\beta=\sqrt{\lambda}+\sqrt{2\log(T)+d\log(1+\frac{T}{\lambda d})}+\alpha C\), we can get

\[R(T) \leq 4+T_{0}+\big{(}2\sqrt{\lambda}+\sqrt{2\log(T)+d\log(1+\frac{T }{\lambda d})}+\alpha C\big{)}\times\bigg{(}\sqrt{2mdT\log(1+\frac{T}{\lambda d })}\] \[\qquad+\frac{2md\log(1+\frac{T}{\lambda d})}{\alpha}\bigg{)}\] \[=4+16u\log(uT)+4u\max\{\frac{288d}{\gamma^{2}\alpha\sqrt{\lambda }\tilde{\lambda}_{x}}\log(uT),\frac{16}{\tilde{\lambda}_{x}^{2}}\log(\frac{8dT }{\tilde{\lambda}_{x}^{2}}),\frac{72\sqrt{\lambda}}{\alpha\gamma^{2}\tilde{ \lambda}_{x}},\frac{72\alpha C^{2}}{\gamma^{2}\sqrt{\lambda}\tilde{\lambda}_{x }}\}\] \[\qquad+\big{(}2\sqrt{\lambda}+\sqrt{2\log(T)+d\log(1+\frac{T}{ \lambda d})}+\alpha C\big{)}\times\bigg{(}\sqrt{2mdT\log(1+\frac{T}{\lambda d })}\] \[\qquad+\frac{2md\log(1+\frac{T}{\lambda d})}{\alpha}\bigg{)}\,.\]

Picking \(\alpha=\frac{\sqrt{\lambda}+\sqrt{d}}{C}\), we can get

\[R(T)\leq O\big{(}(\frac{C\sqrt{d}}{\gamma^{2}\tilde{\lambda}_{x}}+\frac{1}{ \tilde{\lambda}_{x}^{2}})u\log(T)\big{)}+O\big{(}d\sqrt{mT}\log(T)\big{)}+O \big{(}mCd\log^{1.5}(T)\big{)}\,.\] (55)

Thus we complete the proof of Theorem 3.

## Appendix D Proof and Discussions of Theorem 4

Table 1 of the work [12] gives a lower bound for linear bandits with adversarial corruption for a single user. The lower bound of \(R(T)\) is given by: \(R(T)\geq\Omega(d\sqrt{T}+dC)\). Therefore, suppose our problem with multiple users and \(m\) underlying clusters where the arrival times are \(T_{i}\) for each cluster, then for any algorithms, even if they know the underlying clustering structure and keep \(m\) independent linear bandit algorithms to leverage the common information of clusters, the best they can get is \(R(T)\geq dC+\sum_{i\in[m]}d\sqrt{T_{i}}\). For a special case where \(T_{i}=\frac{T}{m},\forall i\in[m]\), we can get \(R(T)\geq dC+\sum_{i\in[m]}d\sqrt{\frac{T}{m}}=d\sqrt{mT}+dC\), which gives a lower bound of \(\Omega(d\sqrt{mT}+dC)\) for the LOCUD problem.

Recall that the regret upper bound of RCLUB-WCU shown in Theorem 3 is of \(O\bigg{(}(\frac{C\sqrt{d}}{\gamma^{2}\tilde{\lambda}_{x}}+\frac{1}{\tilde{ \lambda}_{x}^{2}})u\log(T)\bigg{)}+O\big{(}d\sqrt{mT}\log(T)\big{)}+O\big{(}mCd \log^{1.5}(T)\big{)}\), asymptotically matching this lower bound with respect to \(T\) up to logarithmic factors and with respect to \(C\) up to \(O(\sqrt{m})\) factors, showing the tightness of our theoretical results (where \(m\) are typically very small for real applications).

We conjecture that the gap for the \(m\) factor in the \(mC\) term of the lower bound is due to the strong assumption that cluster structures are known to prove our lower bound, and whether there exists a tighter lower bound will be left for future work.

## Appendix E Proof of Theorem 5

We prove the theorem using the proof by contrapositive. Specifically, in Theorem 5, we need to prove that for any \(t\geq T_{0}\), if the detection condition in Line 7 of Algo.2 for user \(i\), then with probability at least \(1-5\delta\), user \(i\) is indeed a corrupted user. By the proof by contrapositive, we can prove Theorem 5 by showing that: for any \(t\geq T_{0}\), if user \(i\) is a normal user, then with probability at least \(1-5\delta\), the detection condition in Line 7 of Algo.2 will not be satisfied for user \(i\).

If the clustering structure is correct at \(t\), then for any normal user \(i\)

\[\tilde{\bm{\theta}}_{i,t}-\hat{\bm{\theta}}_{V_{i,t},t}=\tilde{\bm{\theta}}_{i, t}-\bm{\theta}_{i}+\bm{\theta}_{i}-\hat{\bm{\theta}}_{V_{i,t},t}\,,\] (56)where \(\tilde{\bm{\theta}}_{i,t}\) is the non-robust estimation of the ground-truth \(\theta_{i}\), and \(\tilde{\bm{\theta}}_{V_{i,t},t-1}\) is the robust estimation of the inferred cluster \(V_{i,t}\) for user \(i\) at round \(t\). Since the clustering structure is correct at \(t\), \(\hat{\bm{\theta}}_{V_{i,t},t-1}\) is the robust estimation of user \(i\)'s ground-truth cluster's preference vector \(\bm{\theta}^{j(i)}=\bm{\theta}_{i}\) at round \(t\).

We have

\[\tilde{\bm{\theta}}_{i,t}-\bm{\theta}_{i} =(\lambda\bm{I}+\tilde{\bm{M}}_{i,t})^{-1}\tilde{\bm{b}}_{i,t}- \bm{\theta}_{i}\] \[=(\lambda\bm{I}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm {x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}(\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_ {s}=i}}\bm{x}_{a_{s}}(\bm{x}_{a_{s}}^{\top}\bm{\theta}_{i}+\eta_{s}))-\bm{ \theta}_{i}\] (57) \[=(\lambda\bm{I}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm {x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}\big{(}(\lambda\bm{I}+\sum_{\genfrac{}{ }{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})\bm{\theta}_{i }-\lambda\bm{\theta}_{i}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x} _{a_{s}}\eta_{s})\big{)}-\bm{\theta}_{i}\] \[=-\lambda\tilde{\bm{M}}_{i,t}^{\prime}\bm{\theta}_{i}+\tilde{\bm {M}}_{i,t}^{\prime-1}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x}_{a_{ s}}\eta_{s}\,,\]

where we denote \(\tilde{\bm{M}}_{i,t}^{\prime}\triangleq\lambda\bm{I}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm {x}_{a_{s}}\bm{x}_{a_{s}}^{\top}\), and Eq.(57) is because since user \(i\) is normal, we have \(c_{s}=0,\forall s:i_{s}=i\).

Then, we have

\[\left\|\tilde{\bm{\theta}}_{i,t}-\bm{\theta}_{i}\right\|_{2} \leq\left\|\lambda\tilde{\bm{M}}_{i,t}^{\prime-1}\bm{\theta}_{i} \right\|_{2}+\left\|\tilde{\bm{M}}_{i,t}^{\prime-1}\sum_{\genfrac{}{}{0.0pt}{} {s\in[t]}{i_{s}=i}}\bm{x}_{a_{s}}\eta_{s}\right\|_{2}\] \[\leq\lambda\left\|\tilde{\bm{M}}_{i,t}^{\prime-\frac{1}{2}}\right\| _{2}^{2}\left\|\bm{\theta}_{i}\right\|_{2}+\left\|\tilde{\bm{M}}_{i,t}^{\prime -\frac{1}{2}}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x}_{a_{s}}\eta_{ s}\right\|_{2}\left\|\tilde{\bm{M}}_{i,t}^{\prime-\frac{1}{2}}\right\|_{2}\] (58) \[\leq\frac{\sqrt{\lambda}+\left\|\sum_{\genfrac{}{}{0.0pt}{}{s\in[t ]}{i_{s}=i}}\bm{x}_{a_{s}}\eta_{s}\right\|_{\tilde{\bm{M}}_{i,t}^{\prime-1}}} {\sqrt{\lambda_{\text{min}}(\tilde{\bm{M}}_{i,t}^{\prime})}}\,,\,,\] (59)

where Eq.(58) follows by the Cauchy-Schwarz inequality and the inequality for the operator norm of matrices, and Eq.(59) follows by the Courant-Fischer theorem and the fact that \(\lambda_{\text{min}}(\tilde{\bm{M}}_{i,t}^{\prime})\geq\lambda\).

Following Theorem 1 in [1], for a fixed normal user \(i\), with probability at least \(1-\delta\) for some \(\delta\in(0,1)\) we have:

\[\left\|\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x}_{a_{s} }\eta_{s}\right\|_{\tilde{\bm{M}}_{i,t}^{\prime-1}} \leq\sqrt{2\log(\frac{1}{\delta})+\log(\frac{\text{det}(\tilde{\bm {M}}_{i,t}^{\prime})}{\text{det}(\lambda\bm{I})})}\] \[\leq\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d })}\,,\] (60)

where Eq.(60) is because \(\text{det}(\tilde{\bm{M}}_{i,t}^{\prime})\leq\left(\frac{\text{trace}(\lambda \bm{I}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}=i}}\bm{x}_{a_{s}}\bm{x}_{a_{s} }^{\top})}{d}\right)^{d}\leq\big{(}\frac{\lambda d+T_{i,t}}{d}\big{)}^{d}\), and \(\text{det}(\lambda\bm{I})=\lambda^{d}\).

Plugging this into Eq.(59), we can get

\[\left\|\tilde{\bm{\theta}}_{i,t}-\bm{\theta}_{i}\right\|_{2}\leq\frac{\sqrt{ \lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})}}{ \sqrt{\lambda_{\text{min}}(\tilde{\bm{M}}_{i,t}^{\prime})}}\,.\] (61)Then we need to bound \(\left\|\bm{\theta}_{i}-\hat{\bm{\theta}}_{V_{i,t},t}\right\|_{2}\). With the correct clustering, \(V_{i,t}=V_{j(i)}\), we have

\[\hat{\bm{\theta}}_{V_{i,t},t}-\bm{\theta}_{i} =\bm{M}_{V_{i,t},t}^{-1}\bm{b}_{V_{j,t},t}\] \[=(\bm{\lambda}I+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{ j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}(\sum_{ \genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}r_{s })-\bm{\theta}_{i}\] \[=(\bm{\lambda}I+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{ j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}(\sum_{ \genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}( \bm{x}_{a_{s}}^{\top}\bm{\theta}_{i}+\eta_{s}+c_{s})))-\theta_{i}\] (62) \[=(\bm{\lambda}I+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{ j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})^{-1}\big{(}(\bm{ \lambda}I+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s} \bm{x}_{a_{s}}\bm{x}_{a_{s}}^{\top})\bm{\theta}_{i}-\lambda\bm{\theta}_{i}\] \[\quad+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i _{s},s}\bm{x}_{a_{s}}\eta_{s}+\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V _{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}))\big{)}-\theta_{i}\] \[=-\lambda\bm{M}_{V_{i,t},t}^{-1}\bm{\theta}_{i}+\bm{M}_{V_{i,t},t }^{-1}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x} _{a_{s}}\eta_{s}+\bm{M}_{V_{i,t},t}^{-1}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{ i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}\,.\] (63)

Therefore, we have

\[\left\|\bm{\theta}_{i}-\hat{\bm{\theta}}_{V_{i,t},t}\right\|_{2} \leq\lambda\left\|\bm{M}_{V_{i,t},t}^{-1}\bm{\theta}_{i}\right\|_ {2}+\left\|\bm{M}_{V_{i,t},t}^{-1}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s} \in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\|_{2}+\left\|\bm{M}_{V_{i,t},t}^{-1}\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s} \bm{x}_{a_{s}}c_{s}\right\|_{2}\] \[\leq\lambda\left\|\bm{M}_{V_{i,t},t}^{-\frac{1}{2}}\right\|_{2}^{2 }\left\|\bm{\theta}_{i}\right\|_{2}+\left\|\bm{M}_{V_{i,t},t}^{-\frac{1}{2}} \sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{ s}}\eta_{s}\right\|_{2}\left\|\bm{M}_{V_{i,t},t}^{-\frac{1}{2}}\right\|_{2}\] \[\quad+\left\|\bm{M}_{V_{i,t},t}^{-\frac{1}{2}}\sum_{\genfrac{}{}{ 0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\| \left\|\bm{M}_{V_{i,t},t}^{-\frac{1}{2}}\right\|_{2}\] (64) \[\leq\frac{\sqrt{\lambda}+\left\|\sum_{\genfrac{}{}{0.0pt}{}{s\in[t ]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{x}_{a_{s}}\eta_{s}\right\|_{\bm{M}_{V_{i,t },t}^{-1}}+\left\|\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i _{s},s}\bm{x}_{a_{s}}c_{s}\right\|_{\bm{M}_{V_{i,t},t}^{-1}}}{\sqrt{\lambda_{ \text{min}}(\bm{M}_{V_{i,t},t})}}\] (65)

Let \(\tilde{\bm{x}}_{s}\triangleq\sqrt{w_{i_{s},s}}\bm{x}_{a_{s}}\), \(\tilde{\eta}_{s}\triangleq\sqrt{w_{i_{s},s}}\eta_{s}\), then we have: \(\left\|\tilde{\bm{x}}_{s}\right\|_{2}\leq\left\|\sqrt{w_{i_{s},s}}\right\|_{2 }\left\|\bm{x}_{a_{s}}\right\|_{2}\leq 1\), \(\tilde{\eta}_{s}\) is still 1-sub-gaussian (since \(\eta_{s}\) is 1-sub-gaussian and \(\sqrt{w_{i_{s},s}}\leq 1\)), \(\bm{M}_{V_{i,t},t}=\lambda\bm{I}+\sum_{\genfrac{}{}{0.0pt}{}{s\in(t)}{i_{s}\in(t )}{i_{s}\in V_{j(i)}}}\tilde{\bm{x}}_{s}\tilde{\bm{x}}_{s}^{\top}\), and

\(\left\|\sum_{\genfrac{}{}{0.0pt}{}{s\in[t]}{i_{s}\in V_{j(i)}}}w_{i_{s},s}\bm{For \(\left\|\sum_{\begin{subarray}{c}s\in[t]\\ i_{s}\in V_{j(i)}\end{subarray}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}\right\|_{\bm{M}^ {-1}_{V_{i,t},t}}\), we have

\[\left\|\sum_{\begin{subarray}{c}x\in[t]\\ i_{s}\in V_{j(i)}\end{subarray}}w_{i_{s},s}\bm{x}_{a_{s}}c_{s}\right\|_{\bm{M}^ {-1}_{V_{i,t},t}} \leq\sum_{\begin{subarray}{c}x\in[t]\\ i_{s}\in V_{j(i)}\end{subarray}}\left|c_{s}\right|w_{i_{s},s}\left\|\bm{x}_{a_{s} }\right\|_{\bm{M}^{-1}_{V_{i,t},t}}\] \[\leq\alpha C\,,\] (67)

where Eq.(67) is because \(w_{i_{s},s}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{-1}_{i_{s},s}}}\leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{-1}_{i_{s},t}}} \leq\frac{\alpha}{\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{-1}_{V_{i,t},t}}}\) (since \(\bm{M}_{V_{i,t},t}\succeq\bm{M}^{\prime}_{i_{s},t}\succeq\bm{M}^{\prime}_{i_ {s},s}\), \(\bm{M}^{-1}_{i_{s},s}\succeq\bm{M}^{-1}_{i_{s},t}\succeq\bm{M}^{-1}_{V_{i,t},t}\), \(\left\|\bm{x}_{a_{s}}\right\|_{\bm{M}^{\prime-1}_{i_{s},s}}\geq\left\|\bm{x}_{a _{s}}\right\|_{\bm{M}^{-1}_{i_{s},t}}\geq\left\|\bm{x}_{a_{s}}\right\|_{\bm{M} ^{-1}_{i_{s},t}}\)), and \(\sum_{s\in[t]}\left|c_{s}\right|\leq C\).

Therefore, we have

\[\left\|\bm{\theta}_{i}-\hat{\bm{\theta}}_{V_{i,t},t}\right\|_{2} \leq\frac{\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T_{V_{i,t },t}}{\lambda d})}+\alpha C}{\sqrt{\lambda_{\text{min}}(\bm{M}_{V_{i,t},t})}}\,.\] (68)

With Eq.(68), Eq.(61) and Eq.(56), together with Lemma 1, we have that for a normal user \(i\), for any \(t\geq T_{0}\), with probability at least \(1-5\delta\) for some \(\delta\in(0,\frac{1}{5})\)

\[\left\|\tilde{\bm{\theta}}_{i,t}-\hat{\bm{\theta}}_{V_{i,t},t}\right\| \leq\left\|\tilde{\bm{\theta}}_{i,t}-\bm{\theta}_{i}\right\|_{2}+ \left\|\bm{\theta}_{i}-\hat{\bm{\theta}}_{V_{i,t},t}\right\|_{2}\] \[\leq\frac{\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+ \frac{T_{i,t}}{\lambda d})}}{\sqrt{\lambda_{\text{min}}(\bm{M}^{\prime}_{i,t} )}}+\frac{\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T_{V_{i,t },t}}{\lambda d})}+\alpha C}{\sqrt{\lambda_{\text{min}}(\bm{M}_{V_{i,t},t})}}\,,\] (69)

which is exactly the detection condition in Line 7 of Algo.2.

Therefore, by the proof by contrapositive, we complete the proof of Theorem 5.

## Appendix F Description of Baselines

We compare RCLUB-WCU to the following five baselines for recommendations.

* LinUCB[17]: A state-of-the-art bandit approach for a single user without corruption.
* LinUCB-Ind: Use a separate LinUCB for each user.
* CW-OFUL[12]: A state-of-the-art bandit approach for single user with corruption.
* CW-OFUL-Ind: Use a separate CW-OFUL for each user.
* CLUB[8]: A graph-based clustering of bandits approach for multiple users without corruption.
* SCLUB[21]: A set-based clustering of bandits approach for multiple users without corruption.

## Appendix G More Experiments

### Different Corruption Levels

To see our algorithm's performance under different corruption levels, we conduct the experiments under different corruption levels for RCLUB-WCU, CLUB, and SCLUB on Amazon and Yelp datasets. Recall the corruption mechanism in Section 6.1, we set \(k\) as 1,000; 10,000; 100,000. The results are shown in Fig.4. All the algorithms' performance becomes worse when the corruption level increases. But RCLUB-WCU is much robust than the baselines.

### Different Cluster numbers

Following [19], we test the performances of the cluster-based algorithms (RCLUB-WCU, CLUB, SCLUB) when the underlying cluster number changes. We set \(m\) as 5, 10, 20, and 50. The results are shown in Fig.5. All these algorithms' performances decrease when the cluster numbers increase, matching our theoretical results. The performances of CLUB and SCLUB decrease much faster than RCLUB-WCU, indicating that RCLUB-WCU is more robust when the underlying user cluster number changes.

Figure 4: Cumulative regret in different corruption levels

Figure 5: Cumulative regret with different cluster numbers