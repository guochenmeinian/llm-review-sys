# SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout

 Chiyu Max Jiang  Yijing Bai  Andre Corman Christopher Davis  Xiukun Huang Hong Jeon  Sakhum Kulhrestha John Lambert Shuangyu Li Xuanyu Zhou

Carlos Fuertes  Chang Yuan  Mingxing Tan  Yin Zhou  Dragomir Anguelov

Equal contribution core technical contributors (alphabetically ordered).

Waymo LLC

###### Abstract

Realistic and interactive scene simulation is a key prerequisite for autonomous vehicle (AV) development. In this work, we present SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. It offers a unified framework that addresses two key stages of simulation: scene initialization, which involves generating initial traffic layouts, and scene rollout, which encompasses the closed-loop simulation of agent behaviors. While diffusion models have been proven effective in learning realistic and multimodal agent distributions, several challenges remain, including controllability, maintaining realism in closed-loop simulations, and ensuring inference efficiency. To address these issues, we introduce amortized diffusion for simulation. This novel diffusion denoising paradigm amortizes the computational cost of denoising over future simulation steps, significantly reducing the cost per rollout step (16x less inference steps) while also mitigating closed-loop errors. We further enhance controllability through the introduction of generalized hard constraints, a simple yet effective inference-time constraint mechanism, as well as language-based constrained scene generation via few-shot prompting of a large language model (LLM). Our investigations into model scaling reveal that increased computational resources significantly improve overall simulation realism. We demonstrate the effectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models.

## 1 Introduction

Simulation environments allow efficient and safe evaluation of autonomous driving systems [1; 8; 15; 22; 31; 32; 46; 50; 51; 52; 54]. Simulation involves initialization (determining starting conditions for agents) and rollout (simulating agent behavior over time), typically treated as separate problems [44]. Inspired by diffusion models' success in generative media, such as video generation [2; 10] and video editing (inpainting [21; 24; 28], extension, uncropping etc.), we propose SceneDiffuser, a unified spatiotemporal diffusion model that addresses both initialization and rollout for autonomous driving, trained end-to-end on logged driving scenes. To our knowledge, SceneDiffuser is the first model to jointly enable scene generation, controllable editing, and efficient learned closed-loop rollout (Fig. 1).

One challenge in simulation is evaluating long-tail safety-critical scenarios [1; 8; 22; 32; 46]. While data mining can help, such scenarios are often rare. We address this by learning a generative scene realism prior that allows editing logged scenes or generating diverse scenarios. Our model supports scene perturbation (modifying a scene while retaining similarity) and agent injection (adding agents to create challenging scenarios). We also enable synthetic scene generation on roadgraphs with realisticlayouts. We design a protocol for specifying scenario constraints, enabling scalable generation, and demonstrate how a few-shot prompted LLM can generate constraints from natural language.

Given a scene, realistically simulating agents and AV behavior is challenging [50, 51, 52, 54, 31, 53]. Unlike motion prediction tasks [18, 25, 26, 38, 41, 48] where entire future trajectories are jointly predicted in a single inference, simulator predictions are iteratively fed back into the model, requiring realism at each step. This poses challenges: distributional drift from compounding errors, high computational cost for models like diffusion, and the need to simulate various perception attributes realistically.

We propose Amortized Diffusion for simulation rollout generation, a novel approach for amortizing the cost of the denoising inference over a span of physical steps that effectively addresses the challenges of simulation realism due to closed-loop drift and inference efficiency. Amortized diffusion iteratively carries over prior predictions and refines them over the course of future physical steps (see Sec. 3.2 and Fig. 4). This allows our model to produce stable, consistent, and realistic simulated trajectories, while requiring only a _single_ denoising function evaluation at each physical step while jointly simulating all perception attributes at each step. Experiments show that Amortized Diffusion not only requires 16x less model inferences per step, but is also significantly more realistic.

In summary, SceneDiffuser's main contributions are:

* A unified generative model for scene initialization and rollout, jointly learning distributions for agents, timesteps, and perception features including pose, size and type.
* A novel amortized diffusion method for efficient and realistic rollout generation, significantly improving trajectory consistency and reducing closed-loop error.
* Controllable scene initialization methods, including log perturbation, agent injection, and synthetic generation with a novel hard constraint framework and LLM.
* Investigation of model scaling, showing increased compute effectively improves realism.
* Demonstration of effectiveness on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models.

Figure 1: SceneDiffuser: a generative prior for simulation initialization via log perturbation, agent injection, and synthetic scene generation, and for efficient closed-loop simulation at 10Hz via amortized diffusion. It progressively refines initial trajectories throughout the rollout. Environment sim agents are in green-blue gradient (temporal progression), AV agent in orange-yellow, and synthetic agents in red-purple.

## 2 Related Work

### Data-driven Agent Simulation

A variety of generative models have been explored for scene initialization and simulation, including autoregressive models [8; 22; 46], cVAEs [45], cGANs [1], and Gaussian Mixture Models (GMMs) [8; 47]. For closed-loop rollouts, these models have been extended with GMMs [51], GANs [15], AR models over discrete motion vocabularies [31], cVAE [54], and deterministic policies [50; 52]. Open-loop rollouts have also been explored using cVAE [35].

### Diffusion Models for Agent Simulation

Open-loop SimOpen-loop simulation generates behavior for agents that all lie within one's control, i.e. does not receive any external inputs between steps. Open-loop simulation thus cannot respond to an external planner stack (AV), the evaluation of which is the purpose of simulation. Diffusion models have recently gained traction in multi-agent simulation, particularly in open-loop scenarios (multi-agent trajectory forecasting) [31; 39], using either single-shot or autoregressive (AR) generation. Single-shot approaches employ spatiotemporal transformers in ego-centric [6; 18] or scene-centric frames with motion/velocity deltas [9; 53]. Soft guidance techniques enhance controllability [17; 56]. DJINN [27] uses 2d condition masks for flexible generation.

Closed-loop SimClosed-loop simulation with diffusion remains challenging due to compounding errors and efficiency concerns. Chang _et al._[3] explore route and collision avoidance guidance in closed-loop diffusion, while VBD [14] combines denoising and behavior prediction losses with a query-centric Transformer encoder [42]. VBD found it computationally infeasible to replan at a 1Hz frequency in a receding horizon fashion over the full WOSAC test split due to the high diffusion inference cost, therefore testing in open-loop except over 500 selected scenarios.

Initial Condition GenerationDiffusion-based initial condition generation has also been studied [20]. Pronovost _et al._[32; 33] adapt the LDM framework to rendered scene images, while SLEDGE [5] and DriveSceneGen [44] diffuse initial lane polylines, agent box locations, and AV velocity.

### Diffusion for Temporal World Modeling and Planning

Outside of the autonomous driving domain, diffusion models have proven effective for world simulation through video and for planning. Various diffusion models for 4d data have been proposed, often involving spatiotemporal convolutions and attention mechanisms [11; 12; 43]. In robotics, diffusion-based temporal models leverage Model Predictive Control (MPC) for closed-loop control [4] and have shown state-of-the-art performance for imitation learning [29].

Similar to our Amortized Diffusion approach, TEDi [55] proposes to entangle the physical timestep and diffusion steps for human animation, thereby reducing \(O(T\cdot\mathcal{T})\) complexity for \(\mathcal{T}\) physical timesteps and \(T\) denoising steps to \(O(\mathcal{T})\). However, we are the first work to demonstrate the effectiveness of this approach for reducing closed-loop simulation errors, and the first to extend it to a multi-agent simulation setting.

Figure 2: We formulate various different tasks, including behavior prediction, conditional scenegen and unconditional scenegen as inpainting tasks on the scene tensor. We represent the scene tensor as a normalized tensor \(x\in\mathbb{R}^{A\times\mathcal{T}\times D}\), for the number of agents, timesteps and feature dimensions.

## 3 Method

### Scene Diffusion Setup

We denote the scene tensor as \(\bm{x}\in\mathbb{R}^{A\times\mathcal{T}\times D}\), where \(A\) is the number of agents jointly modeled in the scene, \(\mathcal{T}\) is the total number of modeled physical timesteps, and \(D\) is the dimensionality of all the features that are jointly modeled. We learn to predict the following attributes for each agent: positional coordinates \(x,y,z\), heading \(\gamma\), bounding box dimensions \(l,h,w\), and object type \(k\sim\{\text{AV, car, pedestrian, cyclist}\}\). We model all tasks considered in SceneDiffuser as multi-task inpainting on this scene tensor. Given an inpainting mask \(\bar{\bm{m}}\in\mathbb{R}^{A\times\mathcal{T}\times D}\), the corresponding inpainting context values \(\bar{\bm{x}}:=\bar{\bm{m}}\odot\bm{x}\), a set of global context \(\bm{c}\) (such as roadgraph and traffic signals), and a validity mask for a given agent at a given timestep \(\bar{\bm{v}}\in\mathbb{R}^{A,\mathcal{T}}\) (to account for there being \(<A\) agents in the scene or for occlusion), we train a diffusion model to learn the conditional probability \(p(\bm{x}|\mathcal{C})\), where \(\mathcal{C}:=\{\bar{\bm{m}},\bar{\bm{x}},\bm{c},\bar{\bm{v}}\}\). See Fig. 2 for an illustration of the scene tensor.

Feature NormalizationTo simplify the diffusion model's learning task, we normalize all feature channels before concatenating them along \(D\) to form the scene tensor. We first encode the entire scene in a scene-centric coordinate system, namely the AV's coordinate frame just before the simulation commences. We then scale \(x,y,z\) by fixed constants, \(l,h,w\) by their standard deviation, and one-hot encode \(k\). See Appendix A.6 for more details. This simple yet generalizable process allows us to jointly predict float, boolean, and even categorical attributes by converting into a normalized space of floats. After generating a scene tensor \(\bm{x}\), we apply a reverse process to obtain the generated features.

Diffusion PreliminariesWe adopt the notation and setup for diffusion models from [13]. The forward diffusion process gradually adds Gaussian noise to \(\bm{x}\). The noisy scene tensor at diffusion step \(t\) can be expressed as \(\mathbf{q}(\bm{z}_{t}|\bm{x})=\mathcal{N}(\bm{z}_{t}|\alpha_{t}\bm{x},\sigma_{ t}^{2}\bm{I})\), where \(\alpha_{t}\) and \(\sigma_{t}\) are parameters which control the magnitude and variances of the noise schedule under a variance-preserving model. Therefore \(\bm{z}_{t}=\alpha_{t}\bm{x}+\sigma_{t}\bm{\epsilon}_{t}\), where \(\bm{\epsilon}_{t}\sim\mathcal{N}(0,\bm{I})\). One major departure from the classic diffusion setup in our amortized diffusion regime is that we do not assume a uniform noise level \(t\in\mathbb{R}\) for the entire scene tensor \(\bm{x}\). Instead, we have \(t\in\mathbb{R}^{\mathcal{T}}\) where \(t\) can be relaxed to have a different value per physical timestep in the scene tensor as described in Sec. 3.2. We utilize the commonly used \(\alpha\)-cosine schedule where \(\alpha_{t}=\cos(\pi t/2)\) and \(\sigma_{t}=\sin(\pi t/2)\). At the highest noise level of \(t=1\), the forward diffusion process completely destroys the initial scene tensor \(\bm{x}\) resulting in \(\bm{z}_{t}=\bm{\epsilon}_{t}\sim\mathcal{N}(0,\bm{I})\). Assuming a Markovian transition process, we have the transition distributions \(q(\bm{z}_{t}|\bm{z}_{s})=\mathcal{N}(\bm{z}_{t}|\alpha_{ts}\bm{z}_{s},\sigma_ {ts}^{2}\bm{I})\), where \(\alpha_{ts}=\alpha_{t}/\alpha_{s}\) and \(\sigma_{ts}^{2}=\sigma_{t}^{2}-\alpha_{ts}^{2}\sigma_{s}^{2}\) and \(t>s\). In the denoising process, conditioned on a single datapoint \(\bm{x}\), the denoising process can be written as

\[q(\bm{z}_{s}|\bm{z}_{t},\bm{x})=\mathcal{N}(\bm{z}_{t}|\bm{\mu}_{t\to s}, \sigma_{t\to s}^{2}\bm{I}),\] (1)

where \(\bm{\mu}_{t\to s}=\frac{\alpha_{ts}\sigma_{s}^{2}}{\sigma_{t}^{2}}\bm{z}_{t}+ \frac{\alpha_{s}\sigma_{ts}^{2}}{\sigma_{t}^{2}}\bm{x}\) and \(\sigma_{t\to s}=\frac{\sigma_{ts}^{2}\sigma_{s}^{2}}{\sigma_{t}^{2}}\). In the denoising process, \(\bm{x}\) is approximated using a learned denoiser \(\hat{\bm{x}}\). Following [13] and [37], we adopt the commonly used \(v\)_prediction_, defined as \(\bm{v}_{t}(\bm{\epsilon}_{t},\bm{x})=\alpha_{t}\bm{\epsilon}_{t}-\sigma_{t}\bm {x}\). We trained a model parameterized by \(\bm{\theta}\) to predict \(\bm{v}_{t}\) given \(\bm{z}_{t}\), \(t\) and context \(\mathcal{C}\): \(\hat{\bm{v}}_{t}:=\hat{\bm{v}}_{\theta}(\bm{z}_{t},\mathcal{C})\). The predicted \(\hat{\bm{x}}_{t}\) can be recovered via \(\hat{\bm{x}}_{t}=\alpha_{t}\bm{z}_{t}-\sigma_{t}\hat{\bm{v}}_{t}\). The model is end-to-end trained with a single loss:

\[\mathbb{E}_{(\bm{x},\mathcal{C})\sim\mathcal{D},t\sim(\mathcal{U}(0,1);\hat{t}),\bm{m}\sim\mathcal{M},\epsilon_{t}\sim\mathcal{N}(0,\bm{I})}[||\hat{\bm{v}}_{ \theta}(\bm{z}_{t},t,\mathcal{C})-\bm{v}_{t}(\bm{\epsilon}_{t},\bm{x})||_{2}^{ 2}],\] (2)

Figure 3: SceneDiffuser architecture. Global scene context is encoded into a fixed number of \(N_{c}\) tokens via a Perceiver IO [16] encoder. The noisy scene tokens are fused with local and global context, then used to condition a spatiotemporal transformer-based backbone [49] via Adaptive LayerNorm (AdaLN) [30]. Input/output tensor are in green, context tensors in blue, and ops in _italics_.

\(\mathcal{D}=\{(\bm{x}_{i},\mathcal{C}_{i})|i=1,2,\cdots,|\mathcal{D}|\}\) is the dataset containing paired agents and scene context data, \(t\) is probabilistically either sampled from a uniform distribution, or sampled as a monotonically increasing temporal schedule \(\hat{\bm{t}}\), where \(\hat{\bm{t}}_{\tau}=\max\big{(}0,(\tau-\mathcal{T}_{\text{history}})/\mathcal{T }_{\text{future}}\big{)}\) to facilitate amortized rollout which will be discussed in Sec. 3.2. Each is sampled with 50% probability. \(\mathcal{M}=\{\bar{\bm{m}}_{\text{bp}}\odot\bar{\bm{m}}_{\text{control}},\bar{ \bm{m}}_{\text{sceneg}}\odot\bar{\bm{m}}_{\text{control}}\}\) is the set of inpainting masks for the varied tasks.

**Scene Diffusion Tasks** Different tasks are formulated as inpainting problems (Fig. 2).

_Scene Generation (SceneGen):_ Given the full trajectory of some agents, generate the full trajectory of other agents. We have \(\bar{\bm{m}}_{\text{sceneg}}\in\mathbb{R}^{A,1,1}\) (broadcastable to \(\mathcal{T}\) timesteps and \(D\) features), where \(\bar{\bm{m}}_{\text{sceneg, a}}\sim Pr(X=A_{\text{select}}/A_{\text{valid}})\), where \(A_{\text{select}}\sim\mathcal{U}(0,A_{\text{valid}})\) is the number of agents sampled to be selected as inpainting conditions out of \(A_{\text{valid}}\) valid agents in the scene.

_Behavior Prediction (BP):_ Given past and current data for all agents, predict the future for all agents. We have \(\bar{\bm{m}}_{\text{bp}}\in\mathbb{R}^{1,\mathcal{T},1}\) (broadcastable to \(A\) agents and \(D\) features), where \(\bar{\bm{m}}_{\text{bp},\tau}=\mathcal{I}(\tau<\mathcal{T}_{history})\).

_Conditional SceneGen and Behavior Prediction_: Both scenegen and behavior prediction masks are multiplied by a control mask at training time to enable controllable scenegen and controllable behavior prediction at inference time. We have \(\bar{\bm{m}}_{\text{control}}\in\mathbb{R}^{A,\mathcal{T},D}\), where \(\bar{\bm{m}}_{\text{control},(a,\tau,d)}=I_{a}\cdot I_{\tau}\cdot I_{d},I_{a} \sim Pr(X=A_{\text{control}}/A_{\text{valid}}),I_{\tau}\sim Pr(X=\mathcal{T}_{ \text{control}}/\mathcal{T}),I_{d}\sim Pr(X=p_{d})\) where \(p_{d}\) of the corresponding feature channel. This allows us to condition on certain channels, such as positions \(x,y\) with or without specifying other features such as type and heading.

**Architecture** We present a schematic for the SceneDiffuser architecture in Fig. 3, consisting of two end-to-end trained models: a global context encoder and a transformer denoiser backbone. Validity \(\bar{\bm{v}}\) is used as a transformer attention mask within the transformer denoiser backbone.

**Diffusion Sampler** We use DPM++ [19] with a Heun solver. We utilize 16 denoising steps for our one-shot experiments and for our amortized diffusion warmup process.

### Scene Rollout

Future prediction with no replanning ('One-Shot') is not used in simulation due to its non-reactivity, and forward scene inference, under the standard diffusion paradigm ('Full AR'), is computationally intensive due to the double for-loop over both physical rollout steps and denoising diffusion steps [55]. Moreover, executing only the first step while discarding the remainder leads to inconsistent plans that result in compounding errors. We adopt an amortized autoregressive ('Amortized AR') rollout, aligning the diffusion steps with physical timesteps to amortize diffusion steps over physical time, requiring a single diffusion step at each simulation step while reusing previous plans.

We illustrate the three algorithms in Algorithm 1-3 using the same model trained with a noise mixture \(t\sim\{\mathcal{U}(0,1);\hat{\bm{t}}\) (Eqn. 2). We also illustrate Algorithm 3 in Fig. 4. We denote the total number of timesteps \(\mathcal{T}=H+F\), where \(H,F\) denote the number of past and future steps. We denote \(\bm{x}:=\bm{x}^{[-H:F]}\) to be the temporal slicing operator where \(\bm{x}^{[0]}\) is the final history step.

**Input:** Global context \(\mathbf{c}\) (roadgraph and traffic signals), history states \(\bm{x}^{[-H:0]}\), validity \(\bar{\bm{v}}\).

**Output:** Simulated observations for unobserved futures \(\hat{\bm{x}}^{[1:F]}\).

Figure 4: Amortized diffusion rollout procedure. The warm up step initializes the future predictions for the entire future horizon, which is then perturbed by a monotonic noise schedule \(\hat{t}\). The trajectory is iteratively denoised by one step at each simulation step.

### Controllable Scene Generation

To simulate long-tail scenarios such as rare behavior of other agents, it is important to effectively insert controls into the scene generation process. To do so, we input an inpainting context scene tensor \(\bar{\bm{x}}\), where some pixels are pre-filled. Through pre-filled feature values in \(\bar{\bm{x}}\), we can specify a particular agent of a specified type to be appear at a specific position at a specific timestamp.

Data Augmentation via Log PerturbationThe diffusion framework makes it straightforward to produce additional perturbed examples of existing ground truth (log) scenes. Instead of starting from pure noise \(\bm{z}_{t}\sim\mathcal{N}(0,\bm{I})\) and diffusing backwards from \(t\to 0\), we take our original log scene \(\bm{x}^{\prime}\) and add noise to it such that our initial \(\bm{z}_{t}=\alpha_{t}\bm{x}^{\prime}+\bm{\epsilon}_{t}\) where \(\bm{\epsilon}_{t}\sim\mathcal{N}(0,\sigma_{t}\bm{I})\). Starting the diffusion process at \(t=0\) yields the original data, while \(t=1\) produces purely synthetic data. For \(t\in(0,1)\), higher values increase diversity and decrease resemblance to the log. See Figs. 2 and 2 (Appendix).

Language-based Few-shot Scene GenerationThe diffusion model inpaint constraints can be defined through structured data such as a Protocol Buffer3 ('proto'). Protos can be converted into inpainting values, and we leverage the off-the-shelf generalization capabilities of a publicly accessiblechat app powered by a large language model (LLM)4, to generate new Scene Diffusion constraints prots solely using natural language via few-shot prompt engineering. We show example results generated by the LLM in Fig. 10. Details in the Appendix (A.7).

Footnote 4: The chat app is available at gemini.google.com, powered by Gemini V1.0 Ultra at the time of access.

### Generalized Hard Constraints

Users of simulation often require agents to have specific behaviors while maintaining realistic trajectories. However, diffusion soft constraints [27; 56; 57] require a differentiable cost for the constraint and do not guarantee constraint satisfaction. Diffusion hard constraints [21] are modeled as inpainting values and are limited in their expressivity.

Inspired by dynamic thresholding [36] in the image generation domain, where intermediate images are dynamically clipped to a range at every denoising step, we introduce _generalized hard constraints_ (GHC), where a generalized clipping function is iteratively applied at each denoising step. We modify Eqn. 1 such that at each denoising step \(\boldsymbol{\mu}_{t\to s}=\frac{\alpha_{ts}\sigma_{s}^{2}}{\sigma_{t}^{2}} \boldsymbol{z}+\frac{\alpha_{s}\sigma_{s}^{2}}{\sigma_{t}^{2}}\text{clip}( \boldsymbol{x})\), where \(\text{clip}(\cdot)\) denotes the GHC-specific clipping operator. See more details on constraints in Appendix A.9.

We qualitatively demonstrate the effect of hard constraints for unconditional scene generation in Fig. 8. Applying hard constraints post-diffusion removes overlapping agents but results in unrealistic layouts, while applying the hard constraints after each diffusion step both removes the overlapping agents and takes advantage of the prior to improve the realism of the trajectories. We find that the basis on which the hard constraints operate is important: a good constraint will modify a significant fraction of the scene tensor (for example, shifting an agent's entire trajectory rather than just the overlapping waypoints), or else the model "rejects" the constraint on the next denoising step.

## 4 Experimental Results

**Dataset** We use the Waymo Open Motion Dataset (WOMD)[7] for both our scene generation and agent simulation experiments. WOMD includes tracks of all agents and corresponding vectorized maps in each scenario, and offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system.

### Simulation Rollout

**Benchmark** We evaluate our closed-loop simulation models on the Waymo Open Sim Agent Challenge (WOSAC) [23] metrics (see Appendix A.1), a popular sim agent benchmark used in many recent works [9; 14; 31; 51; 53]. Challenge submissions consist of x/y/z/\(\gamma\) trajectories representing centroid coordinates and heading of the objects' boxes that must be generated in closed-loop and

Figure 8: Applying no-collision constraints prevents collisions (red-purple) in generated scenes (b, c). Iteratively applying constraints with every diffusion step further enhances realism (c vs b).

with factorized AV vs. agent models. WOSAC uses the test data from the Waymo Open Motion Dataset (WOMD)[7]. Up to 128 agents (one of which must represent the AV) must be simulated in each scenario for the 8 second future (comprising 80 steps of simulation), producing 32 rollouts per scenario for evaluation. In a small departure from the official setting, we utilize the logged validity mask as input to our transformer and unify the AV and agents' rollout step for simplicity.

**Evaluation**  In Tab. 2, we show results on WOSAC. We show that Amortized AR (10 Hz) not only requires 16x fewer model inference calls, but is also significantly more realistic than Full AR at a 10Hz replan rate. In Amortized AR, we re-use the plan from the previous step, leading to increased efficiency and consistency. The one-shot inference setting is equivalent to Full AR with no replanning (0.125 Hz) and achieves comparably higher realism, though as it is not executed in closed-loop, it is not reactive to external input in simulation, and thus not a valid WOSAC entry.

In Figs. 5 and 7, we investigate the effects of varied replan rates to simulation realism. While high replan frequency leads to significant degredation in realism under the Full AR rollout paradigm, Amortized AR significantly reduces error accumulation while being \(16\times\) more efficient.

In Tab. 4, we compare against the WOSAC leaderboard with the aforementioned modifications. We achieve top open-loop performance and the best closed-loop performance among diffusion models.

### Scene Generation

**Unconstrained Scene Generation**  We use the unconditional scene generation task as a means to quantitatively measure the distributional realism of our model. We condition the scene using the same logged road graph and traffic signals, as well as the logged agent validity to control for the same number of agents generated per scene. All agent attributes are generated by the model.

Due to a lack of public benchmarks for this task, we adopt a slightly modified version of the WOSAC [23] metrics, where different metrics buckets are aggregated per-scene instead of per-agent, due to the lack of one-to-one correspondence between agents in the generated scene versus the logged scene (see Appendix A.2 for more details). Metrics are aggregated over all agents that are ever valid in the 9 second trajectory.

We show our model's realism metrics in Tab. 1. Even compared to the oracle performance (comparing logged versus logged distributions), our model achieves comparable realism scores in every realism bucket. Introducing hard constraints on collisions can significantly improve the composite metric by preventing collisions, while scaling the model without hard constraints improves most realism metrics as the model learns to generate more realistic trajectories. The realism metrics only apply to trajectories and do not account for generated agent type and size distributions. We compare the generated size distributions versus log distributions in Fig. 10 and find the marginal and joint distributions both closely track the logged distribution. We show more examples of diverse, unconstrained scene generation when conditioning on the same global context in Appendix A.8 Fig. 13.

Constrained Scene Generation and AugmentationThe controllability we possess in the scene generation process as a product of our diffusion model design can be useful for targeted generation and augmentation of scenes. In Fig. 10, we show qualitative results of scenes with constrained agents generated either via manually defined configs or by a few-shot prompted LLM. Extended qualitative results are listed in Appendix A.7.3.

### Model Design Analysis and Ablation Studies

Scaling AnalysisGiven two options of scaling model compute, either by increasing transformer temporal resolution by decreasing temporal patch sizes, or increasing the number of model parameters, we investigate the performance of multiple transformer backbones: {Model Size} \(\times\) {Temporal Patch Size} = {L, M, S} \(\times\) {8, 4, 2, 1}. We vary model size by jointly scaling the number of transformer layers, hidden dimensions, and attention heads (see Sec. A.6 of Appendix for details). We show quantitative results from this model scaling in Fig. 6 and qualitative comparisons in Fig. 11. Increasing both temporal resolution and number of model parameters improves realism of the simulation.

Multi-task CompatibilityWe find that multitask co-training across BP, SceneGen and with random control masks improves performance compared to a single-task, BP only model on the sim agent rollout task, notably reducing collision and offroad rates. We find that jointly learning multiple agent features (\(x,y,z,\gamma\), size, type) achieves on-par performance with a pose-only (\(x,y,z,\gamma\)) model.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**AMER Policy**} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} \\  & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} & \multicolumn{2}{c}{RMSE} \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & (M/S) & (7) & (1) & (7) & (1) & (1) & (1) & (1) & (1) & (1) & (1) & (1) & (1) & (1) \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\ \hline \multirow{2}{*}{_Test_} & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** \\  & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSE** & **RMSEModel Architecture AblationAs shown in Tab. 3, replacing AdaLN-Zero conditioning with cross attention leads to a 7.99% decrease in realism performance, largely due to significantly higher collision rates and offroad rates. Removing the agent-wise spatial attention layer very significantly increases collision rate, as it removes the mechanism for agents to learn a joint distribution.

## 5 Conclusion

We have introduced SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. SceneDiffuser combines scene initialization with scene rollout to provide a diffusion-based approach to closed-loop agent simulation that is efficient (through amortized autoregression) and controllable (through generalized hard constraints). We performed scaling and ablation studies and demonstrated model improvements with computational resources. On WOSAC, we demonstrate competitive results with the leaderboard and state-of-the-art performance among diffusion methods.

LimitationsWhile our amortized diffusion approach is, to our knowledge, the only and best performing closed-loop diffusion-based agent model with competitive performance, we do not exceed current SOTA performance for other autoregressive models. We do not explicitly model validity masks and resort to logged validity in this work. Future work looks to also model the validity mask.

Broader ImpactThis paper aims to improve AV technologies. With our work we aim to make AVs safer by providing more realistic and controllable simulations. The generative scene modeling techniques developed in this work could have broader social implications regarding generative media and content generation, which poses known social benefits as well as risks of misinformation.

## Acknowledgments and Disclosure of Funding

No third-party funding received in direct support of this work. We thank Shimon Whiteson for his detailed for detailed feedback and also the anonymous reviewers. We would like to thank Reza Mahjourian, Rongbing Mu, Paul Mougin, and Nico Montali for offering consultation and feedback on evaluation metrics. We thank Kevin Murphy for his assistance in developing the mathematical notation for the likelihood metrics. We thank Zhaoyi Wei and Carlton Downey for helpful discussions about the project. All the authors are employees of Waymo LLC.

## References

* Bergamini et al. [2021] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero, Blazej Osinski, Hugo Grimmet, and Peter Ondruska. SimNet: Learning reactive self-driving simulations from real-world observations. In _ICRA_, 2021.
* Brooks et al. [2024] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators.
* Chang et al. [2023] Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, and Manmohan Chandraker. Controllable safety-critical closed-loop traffic simulation via guided diffusion, 2023.
* Chi et al. [2023] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In _Proceedings of Robotics: Science and Systems (RSS)_, 2023.
* Chitta et al. [2024] Kashyap Chitta, Daniel Dauner, and Andreas Geiger. Sledge: Synthesizing simulation environments for driving agents with generative models, 2024.
* Choi et al. [2023] Younwoo Choi, Ray Coden Mercurius, Soheil Mohamad Alizadeh Shabestary, and Amir Rasouli. Dice: Diverse diffusion model with scoring for trajectory prediction, 2023.
* Ettinger et al. [2021] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In _ICCV_, 2021.
* Feng et al. [2023] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. Trafficgen: Learning to generate diverse and realistic traffic scenarios. In _ICRA_, 2023.
* Guo et al. [2023] Zhiming Guo, Xing Gao, Jianlan Zhou, Xinyu Cai, and Botian Shi. SceneDM: Scene-level multi-agent trajectory generation with consistent diffusion models, 2023.
* Gupta et al. [2023] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models, 2023.
* Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022.
* Ho et al. [2022] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In _NeurIPS_, volume 35, pages 8633-8646, 2022.
* Hoogeboom et al. [2023] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In _ICML_, pages 13213-13232. PMLR, 2023.
* Huang et al. [2024] Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fernandez Fisac. Versatile scene-consistent traffic scenario generation as optimization with diffusion, 2024.
* Igl et al. [2022] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin, Punit Shah, Kyriacos Shiarlis, Dragomir Anguelov, Mark Palatucci, Brandyn White, and Shimon Whiteson. Symphony: Learning realistic and diverse agents for autonomous driving simulation. In _ICRA_, 2022.
* Jaegle et al. [2021] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. _arXiv preprint arXiv:2107.14795_, 2021.
* Janner et al. [2022] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In _ICML_, 2022.
* Jiang et al. [2023] Chiyu "Max" Jiang, Andre Corman, Cheolho Park, Benjamin Sapp, Yin Zhou, and Dragomir Anguelov. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In _CVPR_, 2023.
* Lu et al. [2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.
* Lu et al. [2024] Jack Lu, Kelvin Wong, Chris Zhang, Simon Suo, and Raquel Urtasun. Scenecontrol: Diffusion for controllable traffic scene generation. In _ICRA_, 2024.

* [21] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11461-11471, June 2022.
* [22] Reza Mahjourian, Rongbing Mu, Valerii Likhosherstov, Paul Mougin, Xiukun Huang, Joao Messias, and Shimon Whiteson. Unigen: Unified modeling of initial agent states and trajectories for generating autonomous driving scenarios. In _ICRA_, 2024.
* [23] Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nick Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Yang, Shimon Whiteson, Brandyn White, and Dragomir Anguelov. The waymo open sim agents challenge. In _Advances in Neural Information Processing Systems Track on Datasets and Benchmarks_, 2023.
* [24] Jiteng Mu, Michael Gharbi, Richard Zhang, Eli Shechtman, Nuno Vasconcelos, Xiaolong Wang, and Taesung Park. Editable image elements for controllable synthesis. _arXiv preprint arXiv:2404.16029_, 2024.
* [25] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. _arXiv preprint arXiv:2207.05844_, 2022.
* [26] Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David J Weiss, Benjamin Sapp, Zhifeng Chen, and Jonathon Shlens. Scene transformer: A unified architecture for predicting future trajectories of multiple agents. In _ICLR_, 2022.
* [27] Matthew Niedoba, Jonathan Lavington, Yunpeng Liu, Vasileios Lioutas, Justice Sefas, Xiaoxuan Liang, Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, and Frank Wood. A diffusion-model of joint interactive navigation. In _NeurIPS_, 2023.
* [28] Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, and Michael Gharbi. Lazy diffusion transformer for interactive image editing, 2024.
* [29] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models. In _ICLR_, 2023.
* [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _ICCV_, pages 4195-4205, October 2023.
* [31] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Learning the language of driving scenarios. In _ICLR_, 2024.
* [32] Ethan Pronovost, Meghana Reddy Ganesina, Noureldin Hendy, Zeyu Wang, Andres Morales, Kai Wang, and Nick Roy. Scenario diffusion: Controllable driving scenario generation with diffusion. In _Advances in Neural Information Processing Systems_, 2023.
* [33] Ethan Pronovost, Kai Wang, and Nick Roy. Generating driving scenes with diffusion. In _ICRA Workshop on Scalable Autonomous Driving_, June 2023.
* [34] Cheng Qian, Di Xiu, and Minghao Tian. A simple yet effective method for simulating realistic multi-agent behaviors. Technical report, 2023.
* [35] Davis Rempe, Jonah Philion, Leonidas J Guibas, Sanja Fidler, and Or Litany. Generating useful accident-prone driving scenarios via a learned traffic prior. In _CVPR_, June 2022.
* [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* [37] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _The Tenth International Conference on Learning Representations, ICLR_. OpenReview.net, 2022.
* [38] Benjamin Sapp, Yuning Chai, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In _Conference on Robot Learning_, pages 86-99. PMLR, 2020.

* [39] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 8579-8590, October 2023.
* [40] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _ICML_, 2018.
* [41] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr-a: 1st place solution for 2022 waymo open dataset challenge-motion prediction. _arXiv preprint arXiv:2209.10033_, 2022.
* [42] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr++: Multi-agent motion prediction with symmetric scene modeling and guided intention querying. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 46(5):3955-3971, 2024.
* [43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In _ICLR_, 2023.
* [44] Shuo Sun, Zekai Gu, Tianchen Sun, Jiawei Sun, Chengran Yuan, Yuhang Han, Dongen Li, and Marcelo H Ang. Drivescenegen: Generating diverse and realistic driving scenarios from scratch. _IEEE Robotics and Automation Letters_, 2024.
* [45] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafficsim: Learning to simulate realistic multi-agent behaviors. In _CVPR_, 2021.
* [46] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In _CVPR_, June 2021.
* [47] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Krahenbuhl. Language conditioned traffic generation. _7th Annual Conference on Robot Learning (CoRL)_, 2023.
* [48] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Corman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction. _arXiv preprint arXiv:2111.14973_, 2021.
* [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, 2017.
* [50] Eugene Vinitsky, Nathan Lichtle, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. In _NeurIPS Datasets and Benchmarks Track_, 2022.
* [51] Yu Wang, Tiebiao Zhao, and Fan Yi. Multiverse transformer: 1st place solution for waymo open sim agents challenge 2023. Technical report, Pegasus, 2023.
* [52] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits: Bi-level imitation for traffic simulation. In _ICRA_, 2023.
* [53] Chen Yang, Aaron Xuxiang Tian, Dong Chen, Tianyu Shi, and Arsalan Heydarian. Wcdt: World-centric diffusion transformer for traffic scene generation, 2024.
* [54] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. In _ICRA_, 2023.
* [55] Zihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka. Tedi: Temporally-entangled diffusion for long-term motion synthesis, 2023.
* [56] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhii Ray. Language-guided traffic simulation via scene-level diffusion. In _CoRL_, 2023.
* [57] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhii Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In _ICRA_, 2023.

Appendix / supplemental material

### WOSAC Metrics

Suppose there are \(N\approx 500k\) scenarios, each of length \(T=80\) steps, each containing \(A\leq 128\) agents (objects). For each scenario, we generate \(K=32\) samples (conditioned on the true initial state), which is a set of trajectories for each object for each time step, where each point in the trajectory is a \(D=4\)-dim vector recording location \((x,y,z)\) and orientation \(\theta\). Let all this generated data be denoted by \(x(1:N\), \(1:A\), \(1:K\), \(1:T\), \(1:D)\). Let the ground truth data be denoted \(x^{*}(1:N\), \(1:A^{\prime}\), \(1:T\), \(1:D)\). Below we discuss how to evaluate the likelihood of the true (test) dataset \(x^{*}\) under the distribution induced by the simulated dataset \(x\).

(Note that we may have \(A^{\prime}>A\), since the ground truth (GT) can contain cars that enter the scene after the initial prefix used by the simulator; this is handled by defining a validity mask, \(v(1:N,1:T,1:A^{\prime})\), which is set to 0 if we want to exclude a GT car from the evaluation, and is set to 1 otherwise.)

Rather than evaluating the realism of the full trajectories in the raw \((x,y,z,\theta)\) state space, WOSAC defines \(M=9\) statistics (scalar quantities of interest) from each trajectory. Let \(F_{j}(x(i,a,:))\) represent the set of statistics/features (of type j) derived from \(x(i,a,1:K,1:T)\) by pooling over \(T,K\). This is used to compute a histogram \(p_{ija}(.)\) for the empirical distribution of \(F_{j}\) for scenario i. Let \(F_{j}(x^{*}(i,a,t))\) be the value of this statistic from the true trajectory \(i\) for vehicle \(a\) at time \(t\). Then we define the negative log likelihood to be

\[NLL(i,a,t,j)=-\log p_{ija}(F_{j}(x^{*}(i,a,t))\] (3)

The j'th metric for scenario i is defined as

\[m(a,i,j) =\exp\Big{(}-[\frac{1}{N(i,a)}]\sum_{t}v(i,a,t)NLL(i,a,t,j)\Big{)}\] \[m(i,j) =\frac{1}{A}\sum_{a}m(a,i,j)\] (4) \[N(i,a) =\sum_{t}v(i,a,t)\text{ is the number of valid points.}\]

Finally an aggregated metric, used to rank entries, is computed as

\[score=\frac{1}{N^{\prime}}\frac{1}{M}\sum_{i=1}^{N^{\prime}}\sum_{j=1}^{M}w_{ j}m(i,j)\] (5)

where \(0\leq w_{j}\leq 1\).

The 9 component metrics are defined as linear speed, linear acceleration magnitude, angular speed, angular acceleration magnitude, distance to nearest object, collisions, time-to-collision (TTC), distance to road edge, and road departures.

### SceneGen Metrics

We instead let \(F_{j}(x(i,:))\) represent the set of statistics/features (of type j) derived from \(x(i,1:A^{\prime},1:K,-H:T)\) by pooling over \(T,A^{\prime},K\). This is used to compute a histogram \(p_{ij}(.)\) for the empirical distribution of \(F_{j}\) for scenario i.

### Additional Evaluation Details

**Simulation step validity** Due to the requirement of validity masks during inference, which is applied as an attention padding mask within the transformer, the model does not generate valid values for invalid steps. As the WOSAC challenge evaluates simulation agents for all steps, regardless of the step's validity, we use linear interpolation / extrapolation to impute values for all invalid steps in our simulations for the final evaluation.

### Additional Dataset Information

WOSAC uses the v1.2.0 release of WOMD, and we treat WOMD as a set \(\mathcal{D}\) of scenarios where each scenario is a history-future pair. This dataset offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. We use WOMD's 9 second 10 Hz sequences (comprising H = 11 observations from 1.1 seconds of history and 80 observations from 8 seconds of future data), which contain object tracks at 10 Hz and map data for the area covered by the sequence. Across the dataset splits, there exists 486,995 scenarios in train, 44,097 in validation, and 44,920 in test.

### Additional Amortized Diffusion Algorithm Details

Warm up:At inference time, the rollout process is preceded by a warm up step. The warm up step is necessary for initializing a buffer of future timesteps before any diffusion iterations take place. The warm up entails a single iteration of a one-shot prediction process described in Algorithm 5a. This process samples pure noise for some future steps and conditions the denoising process on the set of past steps.

Amortized autoregressive rollout:In Fig. 4, we provide a visual illustration of our amortized autoregressive rollout procedure. We operate the rollout procedure using a buffer to track future steps in the trajectory. After the warm up, the future buffer contains \(\mathcal{T}\) predicted steps with an increasing noise level. Note that step \(\tau=1\) has very little noise applied. The future buffer in this state is denoised for a single iteration using past steps to condition the process. After a single iteration, the clean step at \(\tau=1\) is popped off of the buffer, and it is added to the past steps. Before the next iteration, a step \(\tau=\mathcal{T}+1\) is sampled from a pure noise distribution and is appended to the end of the future buffer. The described rollout process can be repeated to generate trajectories of arbitrary length as clean steps are popped off the buffer.

### Additional Implementation Details

Architecture Details : For our base model, our scene encoder architecture uses 256 latent queries. Each scene token is 256-dimensional, with 4 transformer layers and 4 transformer heads, with a transformer model dimension of 256. We train and run inference with all 128 agents.

Scaling Hyperparameters:_Small Model: Scene token dimension 128, 2 Transformer layers, 128 Transformer model dimensions, 2 Transformer Heads.

_Medium Model_: Scene token dimension 256, 4 Transformer layers, 256 Transformer model dimensions, 4 Transformer Heads.

_Large Model_: Scene token dimension 512, 8 Transformer layers, 512 Transformer model dimensions, 8 Transformer Heads.

Optimizer : We use the Adafactor optimizer [40], with EMA (exponential moving average). We decay using Adam, with \(\beta_{1}=0.9\), decay\({}_{adam}=0.9999\), weight decay of 0.01, and clip gradient norms to \(1.0\).

Training details : Train batch size of 1024, and train for 1.2M steps. We select the most competitive model based on validation set performance, for which we perform a final evaluation using the test set. We use an initial learning rate of \(3\times 10^{-4}\). We use 16 diffusion sampling steps. When training, we mix the behavior prediction (BP) task with the scene generation task, with probability 0.5. The randomized control mask is applied to both tasks.

Additional Hyperparameters : To preprocess features, we use scaling constants of \(\frac{1}{80}\) for features \(x,y,z\), and compute mean \(\mu\) and standard deviation \(\sigma\) of features \(l,w,h\).

We preprocess each agent feature \(f\) to produce normalized feature \(f^{\prime}\) via \(f^{\prime}=\frac{f-\mu_{f}}{2*\sigma_{f}}\), where:

\[\mu_{l}=4.5,\quad\mu_{w}=2.0,\quad\mu_{h}=1.75,\quad\mu_{k}=0.5.\] (6)

and

\[\sigma_{l}=2.5,\quad\sigma_{w}=0.8,\quad\sigma_{h}=0.6,\quad\sigma_{k}=0.5.\] (7)We scale by twice the std \(\sigma\) values to allow sufficient dynamic range for high feature values for some channels.

### Prompts used in Language-based few-shot Scene Generation

#### a.7.1 Prompt:

``` Youarewritingproototogenerateandcontrolanagent'sbehaviorforanautonomousvehicle(AV)simulation.Iwillgivethefollow2examplesofinputandthegeneratedprotoMultiAgentMultiConstraint,whichwillconstraintheagent'spositionineitherpast,current,orfuturetimesteptep.5timestep equals1second,soforeexample15stepswouldgqual3seconds.Givenanaturallanguagedescriptionoftheagent'sdesiredbehavior,pleasegeneratethecorrespondingMultiAgentMultiConstraint. VeryImportantlimitations:
1.Onlytime_step_idxvaluesin[0,8]arevalidforPASTtimestep.
2.Onlytime_step_idxvaluesin[0]arevalidforCURRENTtimestep.
3.Onlytime_step_idxvaluesin[0,49]arevalidforFUTUREtimestep.
4.YoumayonlyusetypesPOT_CAR,POT_MOTORCYCLIST,POT_PEDESTRIANtogeneratetheseexamples
5.Notwoagentsshouldoverlapeachotherathesametimestepinthesametimeframe. Thefollowingare2examplesofanaturallanguageinputandtheoutputisacetfilethatcreatesthecorrespondingconstraint. Example1 Input:GenerateconstraintswhereanotorcycleagentcutsinfrontoftheAVcomingfromtherightsideatsometimenthefuture. Output: ``` agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:3.7#SlightlyrightofADV long_distance:0.0 agent_type:POT_MOTORCYCLIST } step_constraints{ relative_step_constraint{

Figure 12: Log-perturbation via noising and denoising the original logs to different noise levels. An increasing level of noise added and then removed results in scenes more and more dissimilar from the original log, yet increasingly diverse. The scenes are realistic regardless of the perturbation noise level.

time_frame: FUTURE  time_step_idx:30  lat_distance:0.0  long_distance:3.0 # ahead of AV  agent_type: POT_MOTORCYCLIST  }  }  agent_name:'motorcycle_0'  }  scene_name:'single_motorcycle_cut_in'  'it'

Example 2  Input: Generateconstraints where a car agent is tailgating the AV by following behind it closely. Output:  'it'

agent_constraints {  step_constraints {  relative_step_constraint {  time_frame: PAST  time_step_idx:0  lat_distance:0.0  long_distance:-15.0  agent_type: POT_CAR  }  }

step_constraints {  relative_step_constraint {  time_frame: CURRENT  time_step_idx:0  lat_distance:0.0  long_distance:-10.0  agent_type: POT_CAR  }  }  step_constraints {  relative_step_constraint {  time_frame: FUTURE  time_step_idx:30  lat_distance:0.0  long_distance:-2.0  agent_type: POT_CAR  }  }  agent_name: 'car_0'  }  scene_name:'single_car_tailgater'  'it'

Please outputjusttheMultiAgentMultiConstraintinthesrsponsedleaveanyexplanationinthemcomments.Pleasedoublechecktheimportantlimitationsdescribedbeforearemet.

Nowtakeadeepbreathandletsthinkstepbysteptowritetheproto giventhefollowinginput:

S-ShapedManeuver:Createamotoryclistagenthatisdoinga S-shapedmaneuveraroundtheAV.

SurroundingTraffic:Createascenewith8agentsthataresurrounding theAVandtravelingalongforafewseconds.Thereare4agentsontheleftlaneofAVinastraightlineand4agentsonthelaneofAVinastraightline,2ofthefrontagentsatmotorcyclistswhilethe

#### a.7.2 Scene Diffusion Constraint Protos Result:

Prompt 2: (Manual) Cut-in Constraint

``` agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:PAST time_step_idx:0 lat_distance:3.0 long_distance:0.0 agent_type:POT_CYCLIST } } step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:9 lat_distance:0.0 long_distance:0.0 agent_type:POT_CYCLIST } } step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:40 lat_distance:-3 long_distance:10 agent_type:POT_CYCLIST } } agent_name:'cut_in_0' } } ```

``` agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:PAST time_step_idx:0 lat_distance:0 long_distance:-20 agent_type:POT_CYCLIST } } step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:40 lat_distance:0 long_distance:-3agent_type: POT_CYCLIST  }  agent_name: 'tailgater_agent_0' } ```

``` agent_constraints{ step_constraints{ # Initial positionslightlytotherightoftheAV( time_step_idx0) relative_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:2.0#Adjustthisvaluesneededforinitial lateraloffset long_distance:0.0 agent_type: POT_CYCLIST } } step_constraints{ #Movethem motorcyclediagonallytotheleftinthefuture( around15steps) relative_step_constraint{ time_frame:PUTRE time_step_idx:15 lat_distance:-3.0#Adjustthisvaluefordesiredleftward movement long_distance:5.0#Adjustthisvaluefordesiredforward movement agent_type: POT_CYCLIST } } step_constraints{ #Movethem motorcyclediagonallybacktotherightinthefuture (around30steps) relative_step_constraint{ time_frame:FUTURE time_step_idx:30 lat_distance:2.0#Adjustthisvaluefordesiredrightward movement long_distance:10.0#Adjustthisvaluefordesiredforward movement agent_type: POT_CYCLIST } agent_name:'motorcycle_0' } ```

``` agent_constraints{ step_constraints{ time_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:-3.7 long_distance:10.0 agent_type: POT_MOTOXCYCLIST } } step_constraints{ relative_step_constraint{time_frame: FUTURE  time_step_idx: 25  lat_distance: -3.7  long_distance: 30.0  agent_type: POT_MOTORCYLIST  }  }  agent_name:'motorcycle_left_0'  } agent_constraints {  step_constraints {  relative_step_constraint {  time_frame: CURRENT  time_step_idx: 0  lat_distance: -3.7  long_distance: 5.0  agent_type: POT_CAR  }  } step_constraints {  relative_step_constraint {  time_frame: FUTURE  time_step_idx: 25  lat_distance: -3.7  long_distance: 25.0  agent_type: POT_CAR  }  }  agent_name: 'car_left_0'  } agent_constraints {  step_constraints {  relative_step_constraint {  time_frame: FUTURE  time_step_idx: 0  lat_distance: -3.7  long_distance: -5.0  agent_type: POT_CAR  }  }  step_constraints {  relative_step_constraint {  time_frame: FUTURE  time_step_idx: 25  lat_distance: -3.7  long_distance: -1.0  agent_type: POT_CAR  }  }  agent_name: 'car_left_1'  } agent_constraints {  step_constraints {  relative_step_constraint {  time_frame: CURRENT  time_step_idx: 0  lat_distance: -3.7  long_distance: -10.0  agent_type: POT_CAR  }  }  step_constraints {  relative_step_constraint {  time_frame: FUTURE  time_step_idx: 25lat_distance:-3.7 long_distance:-6.0 agent_type:POT_CAR } agent_name:'car_left_2' } agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:3.7 long_distance:10.0 agent_type:POT_MOTOSCVLIST } step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:25 lat_distance:3.7 long_distance:30.0 agent_type:POT_MOTOSCVLIST } agent_name:'motorcycle_right_0' } agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:3.7 long_distance:5.0 agent_type:POT_CAR } } step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:25 lat_distance:3.7 long_distance:25.0 agent_type:POT_CAR } agent_name:'car_right_0' } agent_constraints{ step_constraints{ step_constraints{ relative_step_constraint{ lat_distance:3.7 long_distance:25.0 agent_type:POT_CAR } } agent_name:'car_right_0' } agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:CURRENT time_step_idx:0 lat_distance:3.7 long_distance:-5.0 agent_type:POT_CAR } } agent_constraints{ step_constraints{ relative_step_constraint{ time_frame:FUTURE time_step_idx:25 lat_distance:3.7 long_distance:-1.0agent_type: POT_CAR  }  agent_name: 'car_right_1' } agent_constraints {  step_constraints {  relative_step_constraint {  time_frame: CURRENT  time_step_idx: 0  lat_distance: 3.7  long_distance: -10.0  agent_type: POT_CAR  } } step_constraints {  relative_step_constraint {  time_frame: FUTRE  time_step_idx: 25  lat_distance: 3.7  long_distance: -6.0  agent_type: POT_CAR  }  agent_name: 'car_right_2' } ```

#### a.7.3 Controllable Scenegen Results

Qualitative results showing one successful and one failed example of applying the control points to scene generation task with the protos are listed in Appendix A.7.2. For measuring the success and failures of this scene generation task, we randomly selected 25 examples that were generated with each of the 4 control protos and qualitatively determined success on 1) if the new object does not overlap with any existing objects in the scene and 2) if the new object semantically behaves in the way intended by the control points. Otherwise, we considered it a failure. Overall, we measured a success rate of 40/100.

**Cut-in (left: success, right: failure)**

### Scene Generation

We show additional unconditioned scene generation results in Fig. 13.

### Generalized Hard Constraint Definitions

Non-collision Constraintsensure the boxes of generated agents do not overlap. We define the potential field of agent \(a\) to be a rounded square potential \(\phi_{a}(x,y)=\frac{1}{(x-x_{a})^{1}+(y-y_{a})^{4}+\epsilon}\) if \(||(x-x_{a},y-y_{a})||_{2}<1.5\) else \(0\). We define \(\text{clip}_{\text{collision}}(\bm{x})=\arg\min_{\bm{x}}\big{(}\sum_{a\in A} \sum_{i=\pm 0.5,j=\pm 0.5}\sum_{a^{\prime}\in A,a^{\prime}\neq a}\phi_{a^{ \prime}}(x_{a}+iw_{a},y_{a}+jl_{a})\big{)}\) that minimizes the potential of each agent's corners against all other agents. \((x,y)\) is defined in the normalized space.

Range Constraintslimit a certain feature \(\bm{x}_{d}\) within the range of \(d_{min}\) and \(d_{max}\). In the context of Scene Generation for example, this can be used to limit the length of a vehicle to an arbitrary range, e.g. between 7-9 meters. We have \(\text{clip}_{\text{range}}(\bm{x}_{d})=\min(\max(\bm{x}_{d},d_{\text{min}}),d_{ \text{max}})\).

Onroad Constraints ensure that the bounding boxes of specified generated agents stay on road. We define the offroad potential of road graph polyline \(i\) to be \(\phi_{i}(x,y)=(x-x_{i})^{2}+(y-y_{i})^{2}\) if \(W_{i}(x,y)=0\) else \(0\), where \((x_{i},y_{i})\) is the closest point on the road graph with respect to \((x,y)\) and \(W_{i}(x,y)\) is the winding number of position \((x,y)\) to polyline \(i\), such that we only penalize a trajectory for going offroad. We only consider the closest road graph segment and only consider trajectories that are more than \(>20\%\) onroad. We define \(\text{clip}_{\text{onroad}}(\bm{x})=\arg\min_{\bm{x}}\big{(}\min_{i\in RG}\phi_ {i}(x,y)\big{)}\).

Figure 13: Results of unconditioned scene generation for randomly selected road locations. For each example, we show the ground truth log along with 3 generated scenes.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]. Justification: It accurately reflects the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]. Justification: See Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: No theoretical results included. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe in detail the model architecture, training details, and algorithmic pseudocode for our inference procedures. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] and [No]. Justification: We do not plan to release code in the near future. However our methods are clearly described and our dataset is based on the Waymo Open Motion Dataset, which is already publicly accessible. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [No] Justification: It is not affordable to train multiple models for error bars in our experiments, especially the Large models. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due to the high computational cost it is infeasible to run multiple training runs of the large models. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide number of model parameters as well as inference FLOPS count for our family of models in Figure 6. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the Code of Ethics and confirm that we conform to the guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Discussed in Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We are not releasing data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We accessed and utilized public datasets (the Waymo Open Dataset) in compliance with the licenses. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: the paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.