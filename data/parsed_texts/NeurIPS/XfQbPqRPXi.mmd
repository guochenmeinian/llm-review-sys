# Towards Better Evaluation of GNN Expressiveness

with BREC Dataset

Yanbo Wang   Muhan Zhang

Institute for Artificial Intelligence, Peking University

yanxwb202@gmail.com, muhan@pku.edu.cn

###### Abstract

Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limitations, we propose a new expressiveness dataset, **BREC**, which includes 400 pairs of non-isomorphic graphs carefully selected from four primary categories (Basic, Regular, Extension, and CFI). These graphs have higher difficulty (up to 4-WL-indistinguishable), finer granularity (able to compare models between 1-WL and 3-WL), and a larger scale (400 pairs). Further, we synthetically test 23 models with higher-than-1-WL expressiveness on our BREC dataset. Our experiment gives the first thorough comparison of the expressiveness of those state-of-the-art beyond-1-WL GNN models. We expect this dataset to serve as a benchmark for testing the expressiveness of future GNNs. Our dataset and evaluation code are released at: https://github.com/GraphPKU/BREC.

## 1 Introduction

GNNs have been extensively utilized in bioinformatics, recommender systems, social networks, and others, yielding remarkable outcomes [1; 2; 3; 4; 5; 6]. Despite impressive empirical achievements, related investigations have revealed that GNNs exhibit limited abilities to distinguish non-isomorphic graphs, such as regular graphs. In a practical scenario, the inability to recognize structure may cause issues, such as confused representation of a benzene ring (a six-cycle that cannot be recognized). Xu et al. [7], Morris et al. [8] established a connection between the expressiveness of message-passing neural networks (MPNNs) and the WL test for graph isomorphism testing, demonstrating that MPNN's upper bound is 1-WL. Numerous subsequent studies have proposed GNN variants with enhanced expressiveness [9; 10; 11; 12; 13].

Given the multitude of models employing different approaches, such as feature injection, adherence to the WL hierarchy, equivariance maintenance, and subgraph extraction, a unified framework that can theoretically compare the expressive power among various variants is highly desirable. In this regard, Maron et al. [14] propose the concept of \(k\)-order invariant/equivariant graph networks, which unify linear layers while preserving permutation invariance/equivariance. Additionally, Frascaet al. [15] unify recent subgraph GNNs and establish that their expressiveness upper bound is 3-WL. Zhang et al. [16] construct a comprehensive expressiveness hierarchy for subgraph GNNs, providing counterexamples for each pairwise distinction. Nonetheless, the magnitude of the gaps remains unknown. Furthermore, there exist methods that are difficult to categorize within the \(k\)-WL hierarchy. For instance, Papp and Wattenhofer [17] propose four extensions of GNNs, each of which cannot strictly compare with the other. Similarly, Feng et al. [18] propose a GNN that is partially stronger than 3-WL yet fails to distinguish many graphs that are distinguishable by 3-WL. In a different approach, Huang et al. [19] propose evaluating expressiveness by enumerating specific significant substructures, such as 6-cycles. Zhang et al. [20] introduces graph biconnectivity to test expressiveness.

Without a unified theoretical characterization of expressiveness, employing expressiveness datasets for testing proves valuable. Notably, three expressiveness datasets, EXP, CSL, and SR25, have been introduced by Abboud et al. [21], Murphy et al. [22], Balcilar et al. [9] and have found widespread usage in recent studies. However, these datasets exhibit notable limitations. Firstly, they lack sufficient difficulty. The EXP and CSL datasets solely consist of examples where 1-WL fails, and most recent GNN variants have achieved perfect accuracy on these datasets. Secondly, the granularity of these datasets is too coarse, which means that graphs in these datasets are generated using a single method, resulting in a uniform level of discrimination difficulty. Consequently, the performance of GNN variants often falls either at random guessing (completely indistinguishable) or 100% (completely distinguishable), thereby hindering the provision of a nuanced measure of expressiveness. Lastly, these datasets suffer from small sizes, typically comprising only a few substantially different graphs, raising concerns of incomplete measurement.

To overcome the limitations of current expressiveness datasets, we propose a new dataset, BREC, including 400 pairs of non-isomorphic graphs in 4 major categories: Basic graphs, Regular graphs, Extension graphs, and CFI graphs. Compared to previous ones, BREC has a greater difficulty (up to 4-WL-indistinguishable), finer granularity (able to compare models between 1-WL and 3-WL), and larger scale (800 non-isomorphic graphs organized as 400 pairs), addressing the shortcomings.

Due to the increased size and diversity of the dataset, the traditional classification task may not be suitable for training-based evaluation methods which rely on generalization ability. Thus, we propose a novel evaluation procedure based on directly comparing the discrepancies between model outputs to test pure practical expressiveness. Acknowledging the impact of numerical precision owning to tiny differences between graph pairs, we propose reliable paired comparisons building upon a statistical method [23; 24], which offers a precise error bound. Experiments verify that the evaluation procedure aligns well with known theoretical results.

Finally, we comprehensively compared 23 representative beyond-1-WL models on BREC. Our experiments first give a **reliable empirical comparison** of state-of-the-art GNNs' expressiveness. The currently most thorough investigation is a good start for gaining deeper insights into various schemes to enhance GNNs' expressiveness. On BREC, GNN accuracies range from 41.5% to 70.2%, with I\({}^{2}\)-GNN [19] performing the best. The 70.2% highest accuracy also implies that the dataset is **far from saturation**. We expect BREC can serve as a benchmark for testing future GNNs' expressiveness. We also welcome contributions and suggestions to improve BREC. Our dataset and evaluation code are included in https://github.com/GraphPKU/BREC.

## 2 Limitations of Existing Datasets

**Preliminary.** We utilize the notation \(\{\}\) to represent sets and \(\{\{\}\}\) to represent multisets. The cardinality of a (multi)set \(\mathbb{S}\) is denoted as \(|\mathbb{S}|\). The index set is denoted as \([n]=1,\ldots,n\). A graph is denoted as \(\mathcal{G}=(\mathbb{V}(\mathcal{G}),\mathbb{E}(\mathcal{G}))\), where \(\mathbb{V}(\mathcal{G})\) represents the set of _nodes_ or _vertices_ and \(\mathbb{E}(\mathcal{G})\) represents the set of _edges_. Without loss of generality, we assume \(|\mathbb{V}(\mathcal{G})|=n\) and \(\mathbb{V}(\mathcal{G})=[n]\).

The permutation or reindexing of \(\mathcal{G}\) is denoted as \(\mathcal{G}^{\pi}=(\mathbb{V}(\mathcal{G}^{\pi}),\mathbb{E}(\mathcal{G}^{\pi }))\) with the permutation function \(\pi:[n]\rightarrow[n]\), s.t. \((u,v)\in\mathbb{E}(\mathcal{G})\iff(\pi(u),\pi(v))\in\mathbb{E}(\mathcal{G}^ {\pi})\). Node and edge features are excluded from the definitions for simplicity. Additional discussions about features can be found in Appendix B.

**Graph Isomorphism (GI) Problem.** Two graphs \(\mathcal{G}\) and \(\mathcal{H}\) are considered isomorphic (denoted as \(\mathcal{G}\simeq\mathcal{H}\)) if \(\exists\)\(\phi\)(a bijection mapping) \(:\mathbb{V}(\mathcal{G})\rightarrow\mathbb{V}(\mathcal{H})\) s.t. \((u,v)\in\mathbb{E}(\mathcal{G})\) iff. \((\phi(u),\phi(v))\in\mathbb{E}(\mathcal{H})\).

GI is essential in expressiveness. Only if GNN successfully distinguishes two non-isomorphic graphs can they be assigned different labels. Some researchers [25, 26] indicate the equivalence between GI and function approximation, underscoring the importance of GI. However, we currently do not have polynomial-time algorithms for solving the GI problem. A naive solution involves iterating all \(n!\) permutations to test whether such a bijection exists.

**Weisfeiler-Lehman algorithm (WL).** WL is a well-known isomorphism test relying on color refinement [27]. In each iteration, WL assigns a state (or color) to each node by aggregating information from its neighboring nodes' states. This process continues until convergence, resulting in a multiset of node states representing the final graph representation. While WL effectively identifies most non-isomorphic graphs, it may fail in certain simple graphs, leading to the development of extended versions. One such extension is \(k\)-WL, which treats each \(k\)-tuple of nodes as a unit for aggregating information. Another slightly different method [28] is also referred to as \(k\)-WL. To avoid confusion, we follow Morris et al. [8] to call the former \(k\)-WL and the latter \(k\)-FWL. Further information can be found in Appendix C.

Given the significance of GI and WL, several expressiveness datasets have been introduced, with the following three being the most frequently utilized. We selected a pair of graphs from each dataset, which are illustrated in Figure 1. Detailed statistics for these datasets are presented in Table 1.

**EXP Dataset.** This dataset comprises 600 pairs of non-isomorphic graphs where the 1-WL test fails. Graphs are generated pair-wised, and each graph comprises two disconnected components. The first component, the "core component," is designed to be non-isomorphic with the other graph's "core component," each satisfying distinct SAT conditions in the two graphs. The second component, referred to as the "planar component," is identical in both graphs and introduces noise into the dataset. However, it is important to note that there are only **three substantially different** core pairs, which can truly evaluate the expressiveness of the models.

Each graph in EXP is labeled 0/1 based on whether its core component satisfies the SAT condition for a binary classification problem. Although EXP addresses the issue of semantic labeling by introducing SAT problem and enhances the dataset's size and complexity by including planar components, the simplicity of core c generation and the insufficient number of different core pairs result in most recent GNNs achieving nearly 100% accuracy on EXP, making it difficult for detailed comparisons.

**CSL Dataset.** This dataset consists of 150 Circulant Skip Links (CSL) graphs, where the 1-WL test fails. A CSL graph is defined as follows: Let \(r\) and \(m\) be co-prime natural numbers with \(r<m-1\). \(\mathcal{G}(m,r)=(\mathbb{V},\mathbb{E})\) is an undirected 4-regular graph with \(\mathbb{V}=[m]\), where the edges form a cycle and include skip links. Specifically, for the cycle, \((j,j+1)\in\mathbb{E}\) for \(j\in[m-1]\), and \((m,1)\in\mathbb{E}\). For the skip links, the sequence is recursively defined as \(s_{1}=1\), \(s_{i+1}=(s_{i}+r)\) mod \(m+1\), and \((s_{i},s_{i+1})\in\mathbb{E}\) for any \(i\in\mathbb{N}\). In CSL, we consider CSL graphs with \(m=41\) and \(r=2,3,4,5,6,9,11,12,13,16\), resulting in 10 distinct CSL graphs. For each distinct CSL graph, we generate 14 corresponding graphs by randomly reindexing the nodes. As a result, the dataset contains a total of 150 graphs.

In CSL, each of the 10 distinct CSL graphs is treated as a separate class, and the task is to train a 10-way classification model. While the dataset allows for the generation of 4-regular graphs with any number of nodes, the final dataset contains only **ten essentially different** regular graphs with the

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Dataset & \# Graphs & \# Core graphs\({}^{a}\) & \# Nodes & Hardness & Metrics \\ \hline EXP & 1200 & 6 & 33-73 & 1-WL-indistinguishable & 2-way classification \\ CSL & 150 & 10 & 41 & 1-WL-indistinguishable & 10-way classification \\ SR25 & 15 & 15 & 25 & 3-WL-indistinguishable & 15-way classification \\
**BREC** & 800 & **800** & **10-198** & **1-WL to 4-WL-indistinguishable** & Reliable Paired Comparisons \\ \hline \hline \end{tabular} \({}^{a}\) Core graphs represent graphs that actually serve to measure expressiveness.

\end{table}
Table 1: Dataset statistics

Figure 1: Sample graphs in previous datasets

same number of nodes and degree. Due to the nature of regular graphs and their fixed structure, many recent expressive GNN models perform well on this dataset, achieving close to 100% accuracy.

**SR25 Dataset.** It consists of 15 strongly regular graphs (SR) where the 3-WL test fails. Each graph is an SR with 25 nodes and a degree of 12. In these graphs, connected nodes have 5 common neighbors, while non-connected nodes have 6. In practice, SR25 is transformed into a 15-way classification problem for mapping each graph into a different class where the training and test graphs overlap.

Indeed, 3-WL serves as an upper bound for most recent expressive GNNs. Thus most methods only obtain 6.67% (1/15) accuracy. While some models partially surpassing 3-WL easily achieve completely distinguishable (100%) performance [18], since each graph is an SR with the same parameters. This binary outcome can hardly provide a fine-grained expressiveness measure.

**Summary.** These three datasets have limitations regarding difficulty, granularity, and scale. In terms of difficulty, these datasets are all bounded by 3-WL, failing to evaluate models (partly) beyond 3-WL [18; 19]. In terms of granularity, the graphs are generated in one way, and the parameters of the graphs are repetitive, which easily leads to a 0/1 step function of model performance and cannot measure subtle differences between models. In terms of scale, the number of substantially different graphs in the datasets is small, and the test results may be incomplete to reflect expressiveness measurement.

## 3 BREC: A New Dataset for Expressiveness

We propose a new expressiveness dataset, BREC, to address the limitations regarding difficulty, granularity, and scale. It consists of four major categories of graphs: Basic, Regular, Extension, and CFI. Basic graphs include relatively simple 1-WL-indistinguishable graphs. Regular graphs include four types of subcategorized regular graphs. Extension graphs include special graphs that arise when comparing four kinds of GNN extensions [17]. CFI graphs include graphs generated by CFI methods1[28] with high difficulty. Some samples are shown in Fig 2.

Footnote 1: CFI is short for Cai-Furer-Immerman algorithm, which can generate counterexample graphs for any k-WL.

### Dataset Composition

BREC includes 800 non-isomorphic graphs arranged in a pairwise manner to construct 400 pairs, with detailed composition as follows: (For a more detailed generation process, please refer to AppendixK)

**Basic Graphs.** Basic graphs consist of 60 pairs of 10-node graphs. These graphs are collected from an exhaustive search and intentionally designed to be non-regular. Although they are 1-WL-indistinguishable, most can be distinguished by expressive GNN variants. Basic graphs can also be regarded as an augmentation of the EXP dataset, as they both employ non-regular 1-WL-indistinguishable graphs. Nevertheless, Basic graphs offer a greater abundance of instances and more intricate graph patterns. The relatively small size also facilitates visualization and analysis.

**Regular Graphs.** Regular graphs consist of 140 pairs of regular graphs, including 50 pairs of simple regular graphs, 50 pairs of strongly regular graphs, 20 pairs of 4-vertex condition graphs, and 20 pairs

Figure 2: BREC dataset samples

of distance regular graphs. Each pair of graphs shares identical parameters. A regular graph refers to a graph where all nodes possess the same degree. Regular graphs are 1-WL-indistinguishable, and some studies delve into the analysis of GNN expressiveness from this perspective [29; 13]. We denote regular graphs without any special properties as simple regular graphs. When exploring more intricate regular graphs, the concept of strongly regular graphs (where 3-WL fails) is often introduced. Strongly regular graphs further require that the number of neighboring nodes shared by any two nodes depends solely on their connectivity. Notable examples of strongly regular graphs include the \(4\times 4\)-Rook's graph and the Shrikhande graph (Fig 2(c)). Additionally, the \(4\times 4\)-Rook's graph satisfies the 4-vertex condition property, which signifies that the number of connected edges between the common neighbors of any two nodes is solely determined by their connectivity [30]. It is worth mentioning that the diameter of a connected strongly regular graph is always 2 [31]. A more challenging type of graph known as the distance regular graphs [32] is proposed aiming for extending the diameter. Please refer to Appendix A for a more comprehensive exploration of their relationship.

Regular graphs can also as an enriching addition to the CSL and SR25 datasets. By expanding upon the existing subdivisions of regular graphs, this section widens the range of difficulty and raises the upper bound of complexity. Moreover, unlike the previous datasets, regular graphs are not limited to sharing identical parameters for all graphs within each category, greatly enhancing diversity.

**Extension Graphs.** Extension graphs include 100 pairs of graphs inspired by Papp and Wattenhofer [17]. They proposed 4 types of theoretical GNN extensions: \(k\)-WL hierarchy-based, substructure-counting-based, \(k\)-hop-subgraph-based, and marking-based methods. The authors reveal that most of them are not strictly comparable. Leveraging the insights from theoretical analysis and some empirically derived findings, we generated 100 pairs of 1-WL-indistinguishable and 3-WL-distinguishable graphs to improve the granularity. Noting that it was not considered in any of the previous datasets.

**CFI Graphs.** CFI graphs consist of 100 pairs of graphs inspired by Cai et al. [28]. They developed a method to generate graphs distinguishable by \(k\)-WL but not by \((k-1)\)-WL for any \(k\). We utilized this method to create 100 pairs of graphs spanning up to 4-WL-indistinguishable, even surpassing the current research's upper bounds. Specifically, 60 pairs are solely distinguishable by 3-WL, 20 are solely distinguishable by 4-WL, and 20 are even 4-WL-indistinguishable. Similar to the previously mentioned parts, CFI graphs were not considered in the previous datasets. As the most challenging part, it pushes the upper limit of difficulty even higher. Furthermore, the graph sizes in this section are larger than other parts (up to 198 nodes). This aspect intensifies the challenge of the dataset, demanding a model's ability to process graphs with heterogeneous sizes effectively.

### Advantages

**Difficulty.** By utilizing the CFI method, we specifically provide graphs being 4-WL-indistinguishable. Additionally, we include 4-vertex condition graphs and distance regular graphs, which are variants of strongly regular graphs (3-WL-indistinguishable) but pose greater challenges in terms of complexity.

**Granularity.** The different classes of graphs in BREC exhibit varying difficulty levels, each contributing to the dataset in distinct ways. Basic graphs contain fundamental 1-WL-indistinguishable graphs, similar to the EXP dataset, as a starting point for comparison. Regular graphs extend the CSL and SR25 datasets. The major components of regular graphs are simple regular graphs and strongly regular graphs, where 1-WL and 3-WL fail, respectively. Including 4-vertex condition graphs and distance regular graphs further elevates the complexity. Extension graphs bridge the gap between 1-WL and 3-WL, offering a finer-grained comparison for evaluating models beyond 1-WL. CFI graphs span the spectrum of difficulty from 1-WL to 4-WL-indistinguishable. By comprehensive graph composition, BREC explores the boundaries of graph pattern distinguishability.

**Scale.** While previous datasets relied on only tens of different graphs to generate the dataset, BREC utilizes a collection of 800 different graphs. This significant increase in the number of graphs greatly enhances the diversity. The larger graph set in BREC also contributes to a more varied distribution of graph statistics. In contrast, previous datasets such as CSL and SR25 only have the same number of nodes and degrees across all graphs. For detailed statistics of BREC, please refer to Appendix D.

## 4 RPC: A New Evaluation Method

This section introduces a novel training framework and evaluation method for BREC. Unlike previous datasets, BREC departs from the conventional classification setting, where each graph is assigned a label, a classification model is trained, and the accuracy on test graphs serves as the measure of expressiveness. The labeling schemes used in previous datasets like semantic labels based on SAT conditions in EXP, or distinct labels for essentially different graphs in CSL and SR25, do not apply to BREC. There are two primary reasons. First, BREC aims to enrich the diversity of graphs, which precludes using a semantic label tied to SAT conditions, as it would significantly limit the range of possible graphs. Second, assigning a distinct label to each graph in BREC would result in an 800-class classification problem, where performance could be influenced by factors other than expressiveness. Our core idea is to measure models "separating power" directly. Thus BREC is organized in pairs, where each pair is individually tested to determine whether a GNN can distinguish them. By adopting a pairwise evaluation method, BREC provides a more focused measure of models' expressiveness, aligning to assess distinguishing ability.

Nevertheless, how can we say a pair of graphs is successfully distinguished? Previous researchers tend to set a small threshold (like 1E-4) manually. If the embedding distance between them is larger than the threshold, the GNN is considered can distinguish them. However, this method lacks **reliability** due to numerical precision, especially when graphs vary in size. In order to yield dependable outcomes, we propose an evaluation method measuring both **external difference** and **internal fluctuations**. Furthermore, we introduce a training framework for pairwise data, employing the siamese network design [33] and contrastive loss [34; 35]. The pipeline is depicted in Fig 3(a).

### Training Framework

We adhere to the siamese network design [33] to train a model to distinguish each pair of graphs. The central component consists of two identical models that maintain identical parameters. When a pair of graphs is inputted, it produces a corresponding pair of embeddings. Subsequently, the difference between them is assessed using cosine similarity. The loss function is formulated as follows:

\[L(f,\mathcal{G},\mathcal{H})=\text{Max}(0,\frac{f(\mathcal{G})\cdot f( \mathcal{H})}{||f(\mathcal{G})||\;||f(\mathcal{H})||}-\gamma),\] (1)

where the GNN model \(f:\{\mathcal{G}\}\rightarrow\mathbb{R}^{d}\), \(\mathcal{G}\) and \(\mathcal{H}\) are two non-isomorphic graphs, and \(\gamma\) is a margin hyperparameter (set to 0 in our experiments). The loss function aims to promote the cosine similarity value lower than \(\gamma\), thereby encouraging a greater separation between the two graph embeddings.

The training process **yields several benefits** for the models. Firstly, it enables the GNN to achieve its theoretical expressiveness. The theoretical analysis of GNN expressiveness focuses primarily on the network's structure without imposing any constraints on its parameters, which means we are exploring the expressiveness of **a group of functions**. If a model with particular parameters can distinguish a pair of graphs, the model's design and structure possess sufficient expressiveness. However, it is impractical to iterate all possible parameter combinations to test the real upper bound. Hence, training can **realize searching** in the function space, enabling models to achieve better practical expressiveness. Furthermore, training aids components to **possess specific properties**, such as injectivity and universal approximation, which are vital for attaining theoretical expressiveness. These properties require specific parameter configurations, and randomly initialized parameters may not satisfy these requirements. Moreover, through training, model-distinguishable pairs are **more easily discriminated** from model-indistinguishable pairs, which helps reduce the false negative rate

Figure 3: Evaluation Methodcaused by numerical precision. The difference between their embeddings is further magnified in the pairwise contrastive training process if the model distinguishes them. However, the difference remains unaffected mainly and is only influenced by numerical errors for model-indistinguishable pairs. The training framework is illustrated in Fig 3(a).

### Evaluation Method

Recall that our approach involves comparing the outputs on a pair of non-isomorphic graphs. If there exists a notable disparity between them, we consider the GNN to be able to distinguish them. However, determining an appropriate threshold poses a challenge. A large threshold may yield false negatives where the model is expressive enough, but the observed difference falls short of the threshold. Conversely, a small threshold may result in false positives, where the model fails to distinguish the graphs. However, the fluctuating or numerical errors cause the difference to exceed the small threshold.

To address the issue of fluctuating errors, we draw inspiration from Paired Comparisons [23]. It involves comparing two groups of results instead of a single pair. The influence of random errors is mitigated by repeatedly generating results and comparing the two groups of results. Building upon it, we introduce a method called **R**eliable **P**aired **C**omparison (RPC) to verify whether a GNN genuinely produces distinct outputs for a pair of graphs. The pipeline is depicted in Fig 3(b).

RPC consists of two main components: Major procedure and Reliability check. The Major procedure is conducted on a pair of non-isomorphic graphs to measure their dissimilarity. In comparison, the Reliability check is conducted on graph automorphisms to capture internal fluctuations with numerical precision.

**Major procedure.** For two non-isomorphic graphs \(\mathcal{G}\) and \(\mathcal{H}\), we create \(q\) copies of each by randomly reindexing (operate permutation on node indexes, thus generating an isomorphic graph but with different node orders) them. It results in two groups of graphs, where each copy is represented as:

\[\mathcal{G}_{i},\;\mathcal{H}_{i},\;i\in[q].\] (2)

Supposing the GNN \(f:\{\mathcal{G}\}\rightarrow\mathbb{R}^{d}\), we first calculate \(q\) differences utilizing Paired Comparisons.

\[\bm{d}_{i}=f(\mathcal{G}_{i})-f(\mathcal{H}_{i}),\;i\in[q].\] (3)

**Assumption 4.1**.: \(\bm{d}_{i}\) _are independent \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) random vectors._

The above assumption is based on a more basic assumption that \(f(\mathcal{G}_{i}),\;f(\mathcal{H}_{i})\) follow Gaussian distributions, which presumes that random reindexing only introduces Gaussian noise to the result.

The mean difference between two graph embeddings \(\bm{\mu}=\bm{0}\) implies the GNN cannot distinguish them. Therefore, we can obtain the distinguishing result by conducting an \(\alpha\)-level Hotelling's T-square test, comparing the hypotheses \(H_{0}:\bm{\mu}=\bm{0}\) against \(H_{1}:\bm{\mu}\neq\bm{0}\). We calculate the \(T^{2}\)-statistic for \(\bm{\mu}\) as:

\[T^{2}=q(\overline{\bm{d}}-\bm{\mu})^{T}\bm{S}^{-1}(\overline{\bm{d}}-\bm{\mu }),\] (4)

where

\[\overline{\bm{d}}=\frac{1}{q}\sum_{i=1}^{q}\bm{d}_{i},\;\bm{S}=\frac{1}{q-1} \sum_{i=1}^{q}(\bm{d}_{i}-\overline{\bm{d}})(\bm{d}_{i}-\overline{\bm{d}})^{ T}.\] (5)

Hotelling's T-square test proves that \(T^{2}\) is distributed as an \(\frac{(q-1)d}{q-d}F_{d,q-d}\) random variable, whatever the true \(\bm{\mu}\) and \(\bm{\Sigma}\)[36]. The theorem establishes a connection between the unknown parameter \(\bm{\mu}\) and a definite probability distribution \(F_{d,q-d}\), allowing us to confirm the confidence interval of \(\bm{\mu}\) by testing the distribution fit. In order to test the hypothesis \(H_{0}:\bm{\mu}=\bm{0}\), we substitute \(\bm{\mu}=\bm{0}\) into Equation (4) to obtain \(T^{2}_{\text{test}}=q\overline{\bm{d}}^{T}\bm{S}^{-1}\overline{\bm{d}}\). Then, for a specific \(\alpha\), an \(\alpha\)-level test of \(H_{0}:\bm{\mu}=\bm{0}\) versus \(H_{1}:\bm{\mu}\neq\bm{0}\) for a population following \(\mathcal{N}(\bm{\mu},\bm{\Sigma})\) distribution accepts \(H_{0}\) (the GNN cannot distinguish the pair) if:

\[T^{2}_{\text{test}}=q\overline{\bm{d}}^{T}\bm{S}^{-1}\overline{\bm{d}}<\frac{ (q-1)d}{(q-d)}F_{d,q-d}(\alpha),\] (6)where \(F_{d,q-d}(\alpha)\) is the upper (100\(\alpha\))th percentile of the \(F\)-distribution \(F_{d,q-d}\)[37] with \(d\) and \(q-d\) degrees of freedom. Similarly, we reject \(H_{0}\) (the GNN can distinguish the pair) if

\[T_{\text{test}}^{2}=q\overline{d}^{T}\bm{S}^{-1}\overline{\bm{d}}>\frac{(q-1)d }{(q-d)}F_{d,q-d}(\alpha).\] (7)

**Reliability check.** Although the above test is theoretically valid for evaluating the expressiveness of GNNs, in practice, it is susceptible to computational precision limitations. These limitations can manifest in various scenarios, such as comparing numbers close to zero or inverting a matrix close to zero, making it difficult to rely on the test constantly. We incorporate the Reliability check to monitor abnormal results to address this concern. This step effectively bridges the external difference between two graphs and the internal fluctuations within a single graph.

WLOG, we replace \(\mathcal{H}\) by reindexing of \(\mathcal{G}\), i.e., \(\mathcal{G}^{\pi}\). Thus, we can obtain the internal fluctuations within \(\mathcal{G}\) by comparing it with \(\mathcal{G}^{\pi}\), and the external difference between \(\mathcal{G}\) and \(\mathcal{H}\) by comparing \(\mathcal{G}\) and \(\mathcal{H}\). We utilize the same step as Major procedure on \(\mathcal{G}\) and \(\mathcal{G}^{\pi}\), calculating the \(T^{2}\)-statistics as follows:

\[T_{\text{reliability}}^{2}=q\overline{d}^{T}\bm{S}^{-1}\overline{\bm{d}},\] (8)

\[\text{where}\;\;\overline{\bm{d}}=\frac{1}{q}\sum_{i=1}^{q}\bm{d}_{i},\;\bm{d }_{i}=f(\mathcal{G}_{i})-f(\mathcal{G}_{i}^{\pi}),\;i\in[q],\;\bm{S}=\frac{1} {q-1}\sum_{i=1}^{q}(\bm{d}_{i}-\overline{\bm{d}})(\bm{d}_{i}-\overline{\bm{d} })^{T}.\] (9)

Recalling that \(\mathcal{G}\) and \(\mathcal{G}^{\pi}\) are isomorphic, the GNN should not distinguish between them, implying that \(\bm{\mu}=\bm{0}\). Therefore, the test result is considered reliable only if \(T_{\text{reliability}}^{2}<\frac{(q-1)d}{(q-d)}F_{d,q-d}(\alpha)\). Combining the reliability and distinguishability results, we get the complete RPC (Fig 3) as follows:

For each pair of graphs \(\mathcal{G}\) and \(\mathcal{H}\), we first calculate the threshold value, denoted as \(\text{Threshold}=\frac{(q-1)d}{(q-d)}F_{d,q-d}(\alpha)\). Next, we conduct the Major procedure on \(\mathcal{G}\) and \(\mathcal{H}\) for distinguishability and perform the Reliability check on \(\mathcal{G}\) and \(\mathcal{G}^{\pi}\) for Reliability. Only when the \(T^{2}\)-statistic from the Major procedure, denoted as \(T_{\text{test}}^{2}\), and the \(T^{2}\)-statistic from the Reliability check, denoted as \(T_{\text{reliability}}^{2}\), satisfying \(T_{\text{reliability}}^{2}<\text{Threshold}<T_{\text{test}}^{2}\), do we conclude that the GNN can distinguishing \(\mathcal{G}\) and \(\mathcal{H}\).

We further propose **R**eliable **A**d**aptive **P**i**arwise **C**omparison (RAPC), aiming to adaptively adjust the threshold and provide an upper bound for false positive rates. In practice, we use **RPC** due to its less computational time and satisfactory performance. For more about RAPC, please refer to Appendix E.

## 5 Experiment

In this section, we evaluate the expressiveness of 23 representative models using our BREC dataset.

**Model selection.** We evaluate six categories of methods: non-GNN methods, subgraph-based GNNs, \(k\)-WL-hierarchy-based GNNs, substructure-based GNNs, transformer-based GNNs, and random GNNs. Our primary focus will be on the first three categories. We implement four types of non-GNN baselines based on Papp and Wattenhofer [17], Ying et al. [38], including WL test (3-WL and SPD-WL), counting substructures (\(S_{3}\) and \(S_{4}\)), neighborhood up to a certain radius (\(N_{1}\) and \(N_{2}\)), and marking (\(M_{1}\)). We implemented them by adding additional features during the WL test update or using heterogeneous message passing. It is important to note that they are more theoretically significant than practical since they may require exhaustive enumeration or exact isomorphism encoding of various substructures. We additionally included 16 state-of-the-art GNNs, including NGNN [13], DE+NGNN [29], DS/DSS-GNN [10], SUN [15], SSWL_P [16], GNN-AK [39], KP-GNN [18], I\({}^{2}\)-GNN [19], PPGN [40], \(\delta\)-k-LGNN [41], KC-SetGNN [42], GSN [43], DropGNN [44], OSAN [45], and Graphormer [38].

Table 2 presents the primary results. \(N_{2}\) achieves the highest accuracy among non-GNNs, and I\({}^{2}\)-GNN achieves the highest among GNNs. We detail each method's accuracy on different graphs, showing that it matches theoretical results well. Detailed experiment settings are included in Appendix J.

**Non-GNN baselines.** 3-WL successfully distinguishes all Basic graphs, Extension graphs, simple regular graphs and 60 CFI graphs as expected. \(S_{3}\), \(S_{4}\), \(N_{1}\), and \(N_{2}\) demonstrate excellent performance on small-radius graphs such as Basic, Regular, and Extension graphs. However, due to their limited receptive fields, they struggle to distinguish large-radius graphs like CFI graphs. Noting that the 

[MISSING_PAGE_FAIL:9]

**Substructure-based GNNs** For substructure-based GNNs, we select GSN, which incorporate substructure isomorphism counting as features. The best result obtained for GSN-e is reported when setting \(k=4\). For further exploration of policy and size, please refer to Appendix H.

**Random GNNs** Random GNNs are unsuitable for GI problems since even identical graphs can yield different outcomes due to inherent randomness. However, the RPC can quantify fluctuations in the randomization process, thereby enabling the testing of random GNNs. We test DropGNN and OSAN. For more information regarding the crucial factor of random samples, please refer to Appendix I.

**Transformer-based GNNs** For transformer-based GNNs, we select graphormer, which is anticipated to possess a level of expressiveness comparable to SPD-WL. The experimental results verify that.

## 6 Conclusion and Future Work

This paper proposes a new dataset, BREC, for GNN expressiveness comparison. BREC addresses the limitations of previous datasets, including difficulty, granularity, and scale, by incorporating 400 pairs of diverse graphs in four categories. A new evaluation method is proposed for principled expressiveness evaluation. Finally, a thorough comparison of 23 baselines on BREC is conducted.

Apart from the expressiveness comparison based on GI, there are various other metrics for GNN expressiveness evaluation, such as substructure counting, diameter counting, and biconnectivity checking. However, it's worth noting that these tests are often conducted on datasets not specifically designed for expressiveness [19; 46; 39], which can lead to biased results caused by spurious correlations. In other words, certain methods may struggle to identify a particular substructure, but they can capture another property that correlates with substructures, resulting in false high performance. This problem can be alleviated in BREC because of the difficulty. We reveal the data generation process of BREC in Appendix K, hoping that researchers can utilize them in more tasks. We also hope the test of practical expressiveness will aid researchers in exploring its effects on performance in real datasets and other domains.

## References

* [1] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. _Advances in neural information processing systems_, 28, 2015.
* [2] Albert-Laszlo Barabasi, Natali Gulbahce, and Joseph Loscalzo. Network medicine: a network-based approach to human disease. _Nature reviews genetics_, 12(1):56-68, 2011.
* [3] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In _The world wide web conference_, pages 417-426, 2019.
* [4] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In _Proceedings of the 27th ACM international conference on information and knowledge management_, pages 417-426, 2018.
* [5] Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion. _arXiv preprint arXiv:1706.02263_, 2017.
* [6] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. _AI open_, 1:57-81, 2020.
* [7] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, Conference Track Proceedings_. OpenReview.net, 2019. URL https://openreview.net/forum?id=ryQs6iA5Km.
* [8] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 4602-4609, 2019.

* Balcilar et al. [2021] Muhammet Balcilar, Pierre Heroux, Benoit Gauzere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In _International Conference on Machine Learning_, pages 599-608. PMLR, 2021.
* Bevilacqua et al. [2022] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=dFbXQaRx15w.
* Cotta et al. [2021] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. _Advances in Neural Information Processing Systems_, 34:1713-1726, 2021.
* You et al. [2021] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 10737-10745, 2021.
* Zhang and Li [2021] Muhan Zhang and Pan Li. Nested graph neural networks. _Advances in Neural Information Processing Systems_, 34:15734-15747, 2021.
* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Syx72jC9tm.
* Frasca et al. [2022] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. _Advances in Neural Information Processing Systems_, 35:31376-31390, 2022.
* Zhang et al. [2021] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph GNNs via subgraph weisfeiler-lehman tests. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 41019-41077. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/zhang23k.html.
* Papp and Wattenhofer [2022] Pal Andras Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In _International Conference on Machine Learning_, pages 17323-17345. PMLR, 2022.
* Feng et al. [2022] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop message passing graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=nN3aVRQsxGd.
* Huang et al. [2023] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i$~2$-gnns. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=kDSmxOspsXQ.
* Zhang et al. [2023] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In _The Eleventh International Conference on Learning Representations_, 2023.
* Abboud et al. [2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 2112-2118. ijcai.org, 2021. doi: 10.24963/ijcai.2021/291. URL https://doi.org/10.24963/ijcai.2021/291.
* Murphy et al. [2019] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In _International Conference on Machine Learning_, pages 4663-4673. PMLR, 2019.

* Aylmer Fisher [1992] Ronald Aylmer Fisher. _Statistical methods for research workers_. Springer, 1992.
* Johnson and Wichern [2007] Richard A. Johnson and Dean W. Wichern. _Applied multivariate statistical analysis_. Pearson Prentice Hall, Upper Saddle River, N.J, 6th ed edition, 2007. ISBN 978-0-13-187715-3. OCLC: ocm70867129.
* Chen et al. [2019] Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. _Advances in neural information processing systems_, 32, 2019.
* Geerts and Reutter [2022] Floris Geerts and Juan L. Reutter. Expressiveness and approximation properties of graph neural networks. In _The Tenth International Conference on Learning Representations_, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=wIZUeM3TAU.
* Weisfeiler and Leman [1968] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _nti, Series_, 2(9):12-16, 1968.
* Cai et al. [1989] J.-Y. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. In _30th Annual Symposium on Foundations of Computer Science_, pages 612-617, 1989. doi: 10.1109/SFCS.1989.63543.
* Li et al. [2020] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. _Advances in Neural Information Processing Systems_, 33:4465-4478, 2020.
* Brouwer et al. [2023] A. E. Brouwer, F. Ihringer, and W. M. Kantor. Strongly regular graphs satisfying the 4-vertex condition. _Combinatorica_, 43(2):257-276, apr 2023. doi: 10.1007/s00493-023-00005-y. URL https://doi.org/10.1007%2Fs00493-023-00005-y.
* Brouwer et al. [2012] Andries E Brouwer, Willem H Haemers, Andries E Brouwer, and Willem H Haemers. Strongly regular graphs. _Spectra of graphs_, pages 115-149, 2012.
* Brouwer et al. [2012] Andries E Brouwer, Willem H Haemers, Andries E Brouwer, and Willem H Haemers. _Distance-regular graphs_. Springer, 2012.
* Koch et al. [2015] Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot image recognition. In _ICML deep learning workshop_, volume 2. Lille, 2015.
* Hadsell et al. [2006] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)_, volume 2, pages 1735-1742. IEEE, 2006.
* Wang et al. [2018] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5265-5274, 2018.
* Hotelling [1992] Harold Hotelling. The generalization of student's ratio. In _Breakthroughs in statistics: Foundations and basic theory_, pages 54-65. Springer, 1992.
* Fisher [1950] Ronald Aylmer Fisher. Contributions to mathematical statistics. 1950.
* Ying et al. [2021] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* Zhao et al. [2022] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Mspk_WYKoEH.
* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in neural information processing systems_, 32, 2019.

* Morris et al. [2020] Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings. _Advances in Neural Information Processing Systems_, 33:21824-21840, 2020.
* Zhao et al. [2022] Lingxiao Zhao, Neil Shah, and Leman Akoglu. A practical, progressively-expressive gnn. _Advances in Neural Information Processing Systems_, 35:34106-34120, 2022.
* Bouritsas et al. [2022] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(1):657-668, 2022.
* Papp et al. [2021] Pal Andras Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. _Advances in Neural Information Processing Systems_, 34:21997-22009, 2021.
* Qian et al. [2022] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. _Advances in Neural Information Processing Systems_, 35:21030-21045, 2022.
* Chen et al. [2020] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? _Advances in neural information processing systems_, 33:10383-10395, 2020.
* Babai and Kucera [1979] Laszlo Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In _20th Annual Symposium on Foundations of Computer Science (sfcs 1979)_, pages 39-46. IEEE, 1979.
* Sato [2020] Ryoma Sato. A survey on the expressive power of graph neural networks. _arXiv preprint arXiv:2003.04078_, 2020.
* Huang and Villar [2021] Ningyuan Teresa Huang and Soledad Villar. A short tutorial on the weisfeiler-lehman test and its variants. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8533-8537. IEEE, 2021.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We elucidate the constraints inherent in prior datasets pertaining to expressiveness and present a novel dataset along with comprehensive experiments. 2. Did you describe the limitations of your work? [Yes] We refer to alternative metrics for assessing expressiveness and welcome additional experiments on our foundational dataset(shown in Section 6 3. Did you discuss any potential negative societal impacts of your work? [N/A] All data points are synthetic. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [Yes] Please refer to assumption 4.1. Did you include complete proofs of all theoretical results? [Yes] Please refer to proofE.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Please refer to https://github.com/GraphPKU/BREC 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please refer to Appendix J * Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Please refer to Appendix J
* Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please refer to Appendix J
* If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
* If your work uses existing assets, did you cite the creators? [Yes]
* Did you mention the license of the assets? All assets were openly accessible, and the licenses for each asset were retained in the corresponding repositories. For more details, please refer to https://github.com/GraphPKU/BREC.
* Did you include any new assets either in the supplemental material or as a URL? [Yes] Please refer to https://github.com/GraphPKU/BREC
* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [NA] All assets were openly accessible.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA] All data points are synthetic.
* If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] No utilization of crowdsourcing or engagement in research with human subjects took place. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] No utilization of crowdsourcing or engagement in research with human subjects took place. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA] No utilization of crowdsourcing or engagement in research with human subjects took place.