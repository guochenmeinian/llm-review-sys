# Variational Weighting for Kernel Density Ratios

 Sangwoong Yoon

Korea Institute for Advanced Study

swyoon@kias.re.kr

&Frank C. Park

Seoul National University / Saige Research

fcp@snu.ac.kr

&Gunsu Yun

POSTECH

gunsu@postech.ac.kr

&Iljung Kim

Hanyang University

iljung0810@hanyang.ac.kr

&Yung-Kyun Noh

Hanyang University / Korea Institute for Advanced Study

nohyung@hanyang.ac.kr

###### Abstract

Kernel density estimation (KDE) is integral to a range of generative and discriminative tasks in machine learning. Drawing upon tools from the multidimensional calculus of variations, we derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios, leading to improved estimates of prediction posteriors and information-theoretic measures. In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.

## 1 Introduction

One fundamental component for building many applications in machine learning is a correctly estimated density for prediction and estimation tasks, with examples ranging from classification [1; 2], anomaly detection [3], and clustering [4] to the generalization of value functions [5], policy evaluation [6], and estimation of various information-theoretic measures [7; 8; 9]. Nonparametric density estimators, such as the nearest neighbor density estimator or kernel density estimators (KDEs), have been used as substitutes for the probability density component within the equation of the posterior probability, or the density-ratio equation, with theoretical guarantees derived in part from the properties of the density estimators used [10; 11].

Given a specific task which uses the ratio between two densities, \(p_{1}(\mathbf{x})\) and \(p_{2}(\mathbf{x})\) at a point \(\mathbf{x}\in\mathbb{R}^{D}\), we consider the ratio handled by the ratio of their corresponding two KDEs, \(\widehat{p}_{1}(\mathbf{x})\) and \(\widehat{p}_{2}(\mathbf{x})\):

\[\frac{\widehat{p}_{1}(\mathbf{x})}{\widehat{p}_{2}(\mathbf{x})}\xrightarrow[ \mathrm{Estimate}]{}\frac{p_{1}(\mathbf{x})}{p_{2}(\mathbf{x})}.\] (1)

Each estimator is a KDE which counts the effective number of data within a small neighborhood of \(\mathbf{x}\) by averaging the kernels. The biases produced by the KDEs in the nominator and denominator [12, Theorem 6.28] are combined to produce a single bias of the ratio, as demonstrated in Fig. 1. For example, the ratio \(\frac{p_{1}(\mathbf{x})}{p_{2}(\mathbf{x})}\) at \(\mathbf{x}_{0}\) in Fig. 1(a) is clearly expected to be underestimated because of the dual effects in Fig. 1(b): the _under_estimation of the nominator \(p_{1}(\mathbf{x}_{0})\) and the _over_estimation of the denominator \(p_{2}(\mathbf{x}_{0})\). The underestimation is attributed to the concavity of \(p_{1}(\mathbf{x})\) around \(\mathbf{x}_{0}\) which leads to a reduced number of data being generated compared to a uniform density. The underestimation of \(p_{2}(\mathbf{x})\) can be explained similarly. The second derivative--Laplacian--that createsthe concavity or convexity of the underlying density is a dominant factor that causes the bias in this example.

The Laplacian of density has been used to produce equations in various bias reduction methods, such as bias correction [13, 14, 15] and smoothing of the data space [16, 17]. However, the example in Fig. 1 motivates a novel, position-dependent weight function \(\alpha(\mathbf{x})\) to be multiplied with kernels in order to alleviate the bias. For example, to alleviate the overestimation of \(p_{2}(\mathbf{x}_{0})\), we can consider the \(\alpha(\mathbf{x})\) shown in Fig. 1(c) that assigns more weight on the kernels associated with data located to the left of \(\mathbf{x}_{0}\), which is a low-density region. When the weighted kernels are averaged, the overestimation of \(\widehat{p}_{2}(\mathbf{x}_{0})\) can be mitigated or potentially even underestimated. Meanwhile, the bias of \(\widehat{p}_{1}(\mathbf{x}_{0})\) remains unchanged after applying the weights since \(p_{1}(\mathbf{x})\) is symmetric around \(\mathbf{x}_{0}\). This allows the reversed underestimation of \(p_{2}(\mathbf{x}_{0})\) from the initial overestimation to effectively offset or counterbalance the underestimation of \(p_{1}(\mathbf{x}_{0})\) within the ratio.

We derive the \(\alpha(\mathbf{x})\) function that performs this alleviation over the entire data space. The appropriate information for \(\alpha(\mathbf{x})\) comes from the geometry of the underlying densities. The aforementioned principle of bias correction leads to novel, model-based and model-free approaches. Based on the assumption of the underlying densities, we learn the parameters for the densities' first and second derivatives and then variationally adjust \(\alpha(\mathbf{x})\) for the estimator to create the variationally weighted KDE (VWKDE). We note that the model for those densities and their derivatives need not be exact because the goal is not to achieve precise density estimation but rather to accurately capture the well-behaved \(\alpha(\mathbf{x})\) for the KDE ratios.

Applications include classification with posterior information and information-theoretic measure estimates using density-ratio estimation. Calibration of posteriors [18] has been of interest to many researchers, in part, to provide a ratio of correctness of the prediction. Plug-in estimators of information-theoretic measures, such as the Kullback-Leibler (K-L) divergence, can also be advantageous. For K-L divergence estimation, similar previous formulations for the variational approach have included optimizing a functional bound with respect to the function constrained within the reproducing kernel Hilber space (RKHS) [19, 20, 21]. These and other methods that use weighted kernels (e.g., [22, 23, 24]) take advantage of the flexibility offered by universal approximator functions in the form of linear combinations of kernels. These methods, however, do not adequately explain why the weight optimization leads to an improved performance. Based on a derivation of how bias is produced, we provide an explicit modification of weight for standard kernel density estimates, with details of how the estimation is improved.

The remainder of the paper is organized as follows. In Section 2, we introduce the variational formulation for the posterior estimator and explain how to minimize the bias. Section 3 shows how a weight function can be derived using the calculus of variations, which is then extended to general density-ratio and K-L divergence estimation in Section 4. Experimental results are presented in Section 5. Finally, we conclude with discussion in Section 6.

Figure 1: Estimation of the density ratio \(\frac{p_{1}(\mathbf{x}_{0})}{p_{2}(\mathbf{x}_{0})}\) and bias correction using the weight function \(\alpha(\mathbf{x})\). (a) Two density functions, \(p_{1}(\mathbf{x})\) and \(p_{2}(\mathbf{x})\), and the point of interest \(\mathbf{x}_{0}\) for ratio estimation. Two regions delineated by dashed lines are magnified in (b). (b) The concavity and convexity of the density functions around \(\mathbf{x}_{0}\) and their KDEs. Concave density \(p_{1}(\mathbf{x})\) generates less data than the uniform density of \(p_{1}(\mathbf{x}_{0})\) around \(\mathbf{x}_{0}\) resulting in an underestimation. For a similar reason, convex density \(p_{2}(\mathbf{x})\) results in an overestimation. The two biases are combined into an underestimation of the ratio. (c) KDE augmented with a nonsymmetric weight function \(\alpha(\mathbf{x})\) can alleviate this bias by transforming the bias of \(\widehat{p}_{2}(\mathbf{x})\) to an appropriate underestimation from an overestimation.

## 2 Variationally Weighted KDE for Ratio Estimation

KDE \(\widehat{p}(\mathbf{x})=\frac{1}{N}\sum_{j=1}^{N}k_{h}(\mathbf{x},\mathbf{x}_{j})\) is conventionally the average of kernels. The average roughly represents the count of data within a small region around \(\mathbf{x}\), the size of which is determined by a bandwidth parameter \(h\). The amount of convexity and concavity inside the region determines the bias of estimation, as depicted in Fig. 1(a),(b).

### Plug-in estimator of posterior with weight

We consider a weighted KDE as a plug-in component adjusted for reliable ratio estimation using a positive and twice differentiable weight function: \(\alpha(\mathbf{x})\in\mathcal{A}\) with \(\mathcal{A}=\{\alpha:\mathbb{R}^{D}\rightarrow\mathbb{R}^{+}\mid\alpha\in C ^{2}(\mathbb{R}^{D})\}\). For two given sets of i.i.d. samples, \(\mathcal{D}_{1}=\{\mathbf{x}_{i}\}_{i=1}^{N_{1}}\sim p_{1}(\mathbf{x})\) for class \(y=1\) and \(\mathcal{D}_{2}=\{\mathbf{x}_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}}\sim p_{2}( \mathbf{x})\) for class \(y=2\), we use the following weighted KDE formulation:

\[\widehat{p_{1}}(\mathbf{x})=\frac{1}{N_{1}}\sum_{j=1}^{N_{1}}\alpha(\mathbf{ x}_{j})k_{h}(\mathbf{x},\mathbf{x}_{j}),\quad\widehat{p_{2}}(\mathbf{x})=\frac{1} {N_{2}}\sum_{j=N_{1}+1}^{N_{1}+N_{2}}\alpha(\mathbf{x}_{j})k_{h}(\mathbf{x}, \mathbf{x}_{j}).\] (2)

Here, the two estimators use a single \(\alpha(\mathbf{x})\). The kernel function \(k_{h}(\mathbf{x},\mathbf{x}^{\prime})\) is a positive, symmetric, normalized, isotropic, and translation invariant function with bandwidth \(h\). The weight function \(\alpha(\mathbf{x})\) informs the region that should be emphasized, and a constant function \(\alpha(\mathbf{x})=c\) reproduces the ratios from the conventional KDE. We let their plug-in posterior estimator be \(f(\mathbf{x})\), and the function can be calculated using

\[f(\mathbf{x})=\widehat{P}(y=1|\mathbf{x})=\frac{\widehat{p_{1}}(\mathbf{x})}{ \widehat{p_{1}}(\mathbf{x})+\gamma\widehat{p_{2}}(\mathbf{x})}.\] (3)

with a constant \(\gamma\in\mathbb{R}\) determined by the class-priors.

### Bias of the posterior estimator

We are interested in reducing the expectation of the bias square:

\[\mathbb{E}[\mathrm{Bias}(\mathbf{x})^{2}]=\int\Big{(}f(\mathbf{x})-\mathbb{E }_{\mathcal{D}_{1},\mathcal{D}_{2}}[f(\mathbf{x})]\Big{)}^{2}p(\mathbf{x})d \mathbf{x}.\] (4)

The problem of finding the optimal weight function can be reformulated as the following equation in Proposition 1.

**Proposition 1**.: _With small \(h\), the expectation of the bias square in Eq. (4) is minimized by any \(\alpha(\mathbf{x})\) that eliminates the following function_

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x})=(\nabla\log\alpha|_{\mathbf{x}})^{\top} \mathbf{h}(\mathbf{x})+g(\mathbf{x}),\] (5)

_at every point \(\mathbf{x}\). Here, \(\mathbf{h}(\mathbf{x})=\left(\frac{\nabla p_{1}}{p_{1}}-\frac{\nabla p_{2}}{p_ {2}}\right)\) and \(g(\mathbf{x})=\frac{1}{2}\left(\frac{\nabla^{2}p_{1}}{p_{1}}-\frac{\nabla^{2} p_{2}}{p_{2}}\right)\) with gradient and Laplacian operators, \(\nabla\) and \(\nabla^{2}\), respectively. All derivatives are with respect to \(\mathbf{x}\)._

The derivation of Eq. (5) begins with the expectation of the weighted KDE:

\[\mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})] = \mathbb{E}_{\mathbf{x}^{\prime}\sim p_{1}(\mathbf{x})}[\alpha( \mathbf{x}^{\prime})k_{h}(\mathbf{x},\mathbf{x}^{\prime})]\ =\ \int\alpha(\mathbf{x}^{\prime})p_{1}(\mathbf{x}^{\prime})k_{h}(\mathbf{x}, \mathbf{x}^{\prime})\mathrm{d}\mathbf{x}^{\prime}\] (6) \[= \alpha(\mathbf{x})p_{1}(\mathbf{x})+\frac{h^{2}}{2}\nabla^{2}[ \alpha(\mathbf{x})p_{1}(\mathbf{x})]+O(h^{3}).\] (7)

Along with the similar expansion for \(\mathbb{E}_{\mathcal{D}_{2}}[\widehat{p}_{2}(\mathbf{x})]\), the following plug-in posterior can be perturbed with small \(h\):

\[\mathbb{E}_{\mathcal{D}_{1},\mathcal{D}_{2}}\left[f(\mathbf{x})\right] \rightarrow \frac{\mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})]}{ \mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})]+\gamma\mathbb{E}_{ \mathcal{D}_{2}}[\widehat{p}_{2}(\mathbf{x})]}\] \[= f(\mathbf{x})+\frac{h^{2}}{2}\frac{\gamma p_{1}(\mathbf{x})p_{2 }(\mathbf{x})}{(p_{1}(\mathbf{x})+\gamma p_{2}(\mathbf{x}))^{2}}\left(\frac{ \nabla^{2}[\alpha(\mathbf{x})p_{1}(\mathbf{x})]}{\alpha(\mathbf{x})p_{1}( \mathbf{x})}-\frac{\nabla^{2}[\alpha(\mathbf{x})p_{2}(\mathbf{x})]}{\alpha( \mathbf{x})p_{2}(\mathbf{x})}\right)+\mathcal{O}(h^{3})\] \[= f(\mathbf{x})\ +\ \frac{h^{2}}{2}P(y=1|\mathbf{x})P(y=2|\mathbf{x})B_{ \alpha;p_{1},p_{2}}(\mathbf{x})\ +\ \mathcal{O}(h^{3}),\] (9)with the substitution

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x})\equiv\frac{\nabla^{2}[\alpha(\mathbf{x})p_{1}( \mathbf{x})]}{\alpha(\mathbf{x})p_{1}(\mathbf{x})}-\frac{\nabla^{2}[\alpha( \mathbf{x})p_{2}(\mathbf{x})]}{\alpha(\mathbf{x})p_{2}(\mathbf{x})}.\] (10)

The point-wise leading-order bias can be written as

\[\mathrm{Bias}(\mathbf{x})=\frac{h^{2}}{2}P(y=1|\mathbf{x})P(y=2|\mathbf{x})B_{ \alpha;p_{1},p_{2}}(\mathbf{x}).\] (11)

Here, the \(B_{\alpha;p_{1},p_{2}}(\mathbf{x})\) includes the second derivative of \(\alpha(\mathbf{x})p_{1}(\mathbf{x})\) and \(\alpha(\mathbf{x})p_{2}(\mathbf{x})\). Because two classes share the weight function \(\alpha(\mathbf{x})\), Eq. (10) can be simplified into two terms without the second derivative of \(\alpha(\mathbf{x})\) as

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x})=\frac{\nabla^{\top}\alpha\big{|}_{\mathbf{ x}}}{\alpha(\mathbf{x})}\left(\frac{\nabla p_{1}\big{|}_{\mathbf{x}}}{p_{1}( \mathbf{x})}-\frac{\nabla p_{2}\big{|}_{\mathbf{x}}}{p_{2}(\mathbf{x})} \right)+\frac{1}{2}\left(\frac{\nabla^{2}p_{1}\big{|}_{\mathbf{x}}}{p_{1}( \mathbf{x})}-\frac{\nabla^{2}p_{2}\big{|}_{\mathbf{x}}}{p_{2}(\mathbf{x})} \right),\] (12)

which leads to Eq. (5) in the Proposition. The detailed derivation in this section can be found in Appendix A.

### Plug-in estimator of K-L divergence with weight

In order to estimate \(KL(p_{1}||p_{2})=\mathbb{E}_{\mathbf{x}\sim p_{1}}\left[\log\frac{p_{1}( \mathbf{x})}{p_{2}(\mathbf{x})}\right]\), we consider the following plug-in estimator:

\[\widehat{KL}(p_{1}||p_{2})=\frac{1}{N_{1}}\sum_{i=1}^{N_{1}}\log\frac{\widehat {p}_{1}(\mathbf{x}_{i})}{\widehat{p}_{2}(\mathbf{x}_{i})},\] (13)

using \(\mathbf{x}_{i}\) in \(\mathbf{x}_{i}\in\mathcal{D}_{1}\) for Monte Carlo averaging. When we calculate \(\widehat{p}_{1}\) at \(\mathbf{x}_{i}\), we exclude \(\mathbf{x}_{i}\) from the KDE samples. We use \(\widehat{p_{1}}(\mathbf{x}_{i})=\frac{1}{N_{1}-1}\sum_{j=1}^{N_{1}}\alpha( \mathbf{x}_{j})k_{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathbf{I}_{(i\neq j)}\) with the indicator function \(\mathbf{I}_{(\mathcal{I})}\), which is 1 if \(\mathcal{I}\) is true and 0 otherwise.

**Proposition 2**.: _With small \(h\), the expectation of the bias square is minimized by finding any \(\alpha(\mathbf{x})\) that eliminates the same function as \(B_{\alpha;p_{1},p_{2}}(\mathbf{x})\) in Eq. (5) in Proposition 1 at each point \(\mathbf{x}\)._

In the task of estimating the K-L divergence, Proposition 2 claims that we obtain the equivalent \(B_{\alpha;p_{1},p_{2}}(\mathbf{x})\) to Eq. (5) during the derivation of bias. The pointwise bias in the K-L divergence estimator can be written as

\[\mathrm{Bias}(\mathbf{x})=\frac{h^{2}}{2}\frac{p_{1}(\mathbf{x})}{p_{2}( \mathbf{x})}B_{\alpha;p_{1},p_{2}}(\mathbf{x}),\] (14)

with \(B_{\alpha;p_{1},p_{2}}(\mathbf{x})\) equivalent to Eq. (5).

## 3 Variational Formulation and Implementation

Now we consider the \(\alpha(\mathbf{x})\) that minimizes the mean square error for the estimation:

\[\min_{\alpha(\mathbf{x})\in\mathcal{A}}\int\left(\left(\nabla\log\alpha|_{ \mathbf{x}}\right)^{\top}\mathbf{h}(\mathbf{x})+g(\mathbf{x})\right)^{2}r( \mathbf{x})d\mathbf{x},\] (15)

Figure 2: Posterior estimates with KDEs and VEKDEs for two 20-dimensional homoscedastic Gaussian densities. (a) Bias corrected by VWKDE. (b) Bias and variance of posterior estimates depending on the bandwidth for standard KDE and for (c) VWKDE.

with \(\mathbf{h}(\mathbf{x})=\left(\frac{\nabla p_{1}}{p_{1}}-\frac{\nabla p_{2}}{p_{2}}\right)\), \(g(\mathbf{x})=\frac{1}{2}\left(\frac{\nabla^{2}p_{1}}{p_{1}}-\frac{\nabla^{2}p_ {2}}{p_{2}}\right)\). The \(r(\mathbf{x})\) function depends on the problem: \(r(\mathbf{x})=P(y=1|\mathbf{x})^{2}P(y=2|\mathbf{x})^{2}p(\mathbf{x})\) for posterior estimation and \(r(\mathbf{x})=\left(\frac{p_{1}(\mathbf{x})}{p_{2}(\mathbf{x})}\right)^{2}p( \mathbf{x})\) for K-L divergence estimation, with the total density, \(p(\mathbf{x})=(p_{1}(\mathbf{x})+\gamma p_{2}(\mathbf{x}))P(y=1)\). The calculus of variation for optimizing the functional in Eq. (15) provides an equation that the optimal \(\alpha(\mathbf{x})\) should satisfy:

\[\nabla\cdot\left[r((\nabla\log\alpha)^{\!\top}\mathbf{h}+g)\mathbf{h}\right]=0.\] (16)

The detailed derivation of this equation can be found in Appendix B.

### Gaussian density and closed-form solution for \(\alpha(\mathbf{x})\)

A simple analytic solution for this optimal condition can be obtained for two homoscedastic Gaussian density functions. The density functions have two different means, \(\mu_{1}\in\mathbb{R}^{D}\) and \(\mu_{2}\in\mathbb{R}^{D}\), but share a single covariance matrix \(\Sigma\in\mathbb{R}^{D\times D}\):

\[p_{1}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mu_{1},\Sigma),\quad p_{2}( \mathbf{x})=\mathcal{N}(\mathbf{x};\mu_{2},\Sigma).\] (17)

One solution for this homoscedastic setting can be obtained as

\[\alpha(\mathbf{x})=\exp\left(-\frac{1}{2}(\mathbf{x}-\mu^{\prime})^{\top}A( \mathbf{x}-\mu^{\prime})\right),\] (18)

with \(\mu^{\prime}=\frac{\mu_{1}+\mu_{2}}{2}\) and \(A=b\left(I-\frac{\Sigma^{-1}(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{\top}\Sigma^{- 1}}{||\Sigma^{-1}(\mu_{1}-\mu_{2})||^{2}}\right)-\Sigma^{-1}\) using an arbitrary constant \(b\). Due to the choice of \(b\), the solution is not unique. All the solutions produce a zero bias. Its detailed derivation can be found in the Appendix C. The reduction of the bias using Eq. (18) with estimated parameters is shown in Fig. 2.

### Implementation

In this work, we propose a model-free method and a mode-based method. The model-free approach uses the information of \(\widehat{\nabla}\log p_{1}(\mathbf{x})\) and \(\widehat{\nabla}\log p_{2}(\mathbf{x})\) estimated by a score matching neural network [25]. We obtain the second derivative, \(\widehat{\nabla}^{2}\log p\), by the automatic differentiation of the neural network for the scores. We then obtain \(\frac{\widehat{\nabla^{2}p}}{p}\) using \(\frac{\widehat{\nabla^{2}p}}{p}=\widehat{\nabla}^{2}\log p-\widehat{\nabla}^{ \top}\log p\widehat{\nabla}\log p\). With the outputs of the neural networks for \(\widehat{\nabla}\log p\) and \(\frac{\widehat{\nabla^{2}p}}{p}\), we train a new network for the function \(\alpha(\mathbf{x};\theta)\) with neural network parameters \(\theta\).

On the other hand, the model-based approach uses a coarse Gaussian model for class-conditional densities. The Gaussian functions for each class have their estimated parameters \(\widehat{\mu}_{1},\widehat{\mu}_{2}\in\mathbb{R}^{D}\) and \(\widehat{\Sigma}_{1},\widehat{\Sigma}_{2}\in\mathbb{R}^{D\times D}\). We use the score information from these parameters: \(\widehat{\nabla}\log p_{1}(\mathbf{x})=\widehat{\Sigma}_{1}^{-1}(\mathbf{x}- \widehat{\mu_{1}})\) and \(\widehat{\nabla}\log p_{2}(\mathbf{x})=\widehat{\Sigma}_{2}^{-1}(\mathbf{x}- \widehat{\mu_{2}})\). In the model-based approach, we let the log of \(\alpha(\mathbf{x})\) be a function within the RKHS with basis kernels \(\kappa_{\sigma}(\cdot,\cdot)\) with kernel parameter \(\sigma\). We let \(\log\alpha(\mathbf{x};\theta)=\sum_{i=1}^{N_{1}+N_{2}}\theta_{i}\kappa_{\sigma} (\mathbf{x},\mathbf{x}_{i})\) with parameters \(\theta=\{\theta_{1},\ldots,\theta_{N_{1}+N_{2}}\}\) for optimization.

The weight function \(\alpha(\mathbf{x})\) is obtained by optimizing the following objective function with \(N_{1}\) number of data from \(p_{1}(\mathbf{x})\) and \(N_{2}\) number of data from \(p_{2}(\mathbf{x})\):

\[L(\theta)=\sum_{i=1}^{N_{1}+N_{2}}\frac{1}{2}\left(\nabla^{\top}\log\alpha( \mathbf{x}_{i};\theta)\widehat{\mathbf{h}}(\mathbf{x}_{i})\right)^{\!\!2}\!\!+ \nabla^{\top}\log\alpha(\mathbf{x}_{i};\theta)\widehat{\mathbf{h}}(\mathbf{x}_ {i})\widehat{g}(\mathbf{x}_{i}),\] (19)

with the substitutions \(\widehat{\mathbf{h}}(\mathbf{x})=\widehat{\nabla}\log p_{1}(\mathbf{x})- \widehat{\nabla}\log p_{2}(\mathbf{x})\) and \(\widehat{g}(\mathbf{x})=\frac{1}{2}\left(\frac{\widehat{\nabla^{2}p_{1}( \mathbf{x})}}{p_{1}(\mathbf{x})}-\frac{\widehat{\nabla^{2}p_{2}(\mathbf{x})}} {p_{2}(\mathbf{x})}\right)\).

In the model-based method, an addition of \(\ell_{2}-\)regularizer, \(\lambda\sum_{i=1}^{N_{1}+N_{2}}\theta_{i}^{2}\), with a small positive constant \(\lambda\) makes the optimization (19) quadratic. When there are fewer than 3,000 samples, we use all of them as basis points. Otherwise, we randomly sample 3,000 points from \(\{\mathbf{x}_{i}\}_{i=1}^{N_{1}+N_{2}}\).

A brief summary of the implementation process is shown in Algorithms 1 and 2.1``` Input:\(\mathbf{x}\), \(\{\mathbf{x}_{i}\}_{i=1}^{N_{1}}\!\sim\!p_{1}\), \(\{\mathbf{x}_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}}\!\sim\!p_{2}\) Output: Ratio \(\widehat{R}(\mathbf{x})\) (\(=\widehat{p_{1}}/\widehat{p_{2}}(\mathbf{x})\)) Procedure:
1. Estimate \(\widehat{\nabla}\log p_{1}\) using \(\{\mathbf{x}_{i}\}_{i=1}^{N_{1}}\!\sim\!p_{1}\).
2. Estimate \(\widehat{\nabla}\log p_{2}\) using \(\{\mathbf{x}_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}}\!\sim\!p_{2}\).
3. Obtain \(\alpha(\mathbf{x})\) that minimizes Eq. (19)
3. \(\widehat{R}(\mathbf{x})=\frac{\sum_{i=1}^{N_{1}}\alpha(\mathbf{x}_{i})k_{h}( \mathbf{x},\mathbf{x}_{i})}{\sum_{i=N_{1}+1}^{N_{1}+N_{2}}\alpha(\mathbf{x}_{i })k_{h}(\mathbf{x},\mathbf{x}_{i})}\) Return\(\widehat{R}(\mathbf{x})\) ```

**Algorithm 1** Model-free

## 4 Interpretation of \(\alpha(\mathbf{x})\) for Bias Reduction

The process of finding \(\alpha(\mathbf{x})\) that minimizes the square of \(B_{\alpha;p_{1},p_{2}}(\mathbf{x})\) in Eq. (5) can be understood from various perspectives through reformulation.

### Cancellation of the bias

The second term \(\frac{1}{2}\left(\frac{\nabla^{2}p_{1}}{p_{1}}-\frac{\nabla^{2}p_{2}}{p_{2}}\right)\) in Eq. (5) repeatedly appears in the bias of nonparametric processes using discrete labels [26]. In our derivation, the term is achieved with a constant \(\alpha(\mathbf{x})\) or with no weight function. The role of the weight function is to control the first term \(\frac{\nabla^{\top}\alpha}{\alpha}\left(\frac{\nabla p_{1}}{p_{1}}-\frac{ \nabla p_{2}}{p_{2}}\right)\) based on the gradient information in order to let the first term cancel the second.

### Cancellation of flow in a mechanical system

The equation for each class can be compared with the mechanical convection-diffusion equation, \(\frac{\partial u}{\partial t}=-\mathbf{v}^{\top}\nabla u+D^{\prime}\nabla^{2}u\), which is known as the equation for Brownian motion under gravity [27] or the advective diffusion equation of the incompressible fluid [28]. In the equation, \(u\) is the mass of the fluid, \(t\) is the time, \(\mathbf{v}\) is the direction of convection, and \(D^{\prime}\) is the diffusion constant. The amount of mass change is the sum of the convective movement of mass along the direction opposite to \(\nabla u\) and the diffusion from the neighborhood. We reorganize Eq. (5) into the following equation establishing the difference between the convection-diffusion equations of two fluids:

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x}) = \left[\nabla^{\top}\!\left(\log\alpha+\frac{1}{2}\log p_{1}\right) \nabla\log p_{1}+\frac{1}{2}\nabla^{2}\log p_{1}\right]\] (20) \[-\left[\nabla^{\top}\!\left(\log\alpha+\frac{1}{2}\log p_{2}\right) \nabla\log p_{2}+\frac{1}{2}\nabla^{2}\log p_{2}\right].\]

According to the equation, we can consider the two different fluid mass functions, \(u_{1}(\mathbf{x})=\log p_{1}(\mathbf{x})\) and \(u_{2}=\log p_{2}(\mathbf{x})\), and the original convection movement along the directions \(\mathbf{v}_{1}^{\prime}=-\frac{1}{2}\nabla\log p_{1}\) and \(\mathbf{v}_{2}^{\prime}=-\frac{1}{2}\nabla\log p_{2}\). If we make an \(\alpha(\mathbf{x})\) that modifies the convection directions \(\mathbf{v}_{1}^{\prime}\) and \(\mathbf{v}_{2}^{\prime}\) to \(\mathbf{v}_{1}=\mathbf{v}_{1}^{\prime}-\nabla\alpha\) and \(\mathbf{v}_{2}=\mathbf{v}_{2}^{\prime}-\nabla\alpha\), and a mass change in one fluid is compensated by the change of the other, in other words, if \(\frac{\partial u_{1}}{\partial t}=\frac{\partial u_{2}}{\partial t}\), then the \(\alpha(\mathbf{x})\) is the weight function that minimizes the leading term of the bias for ratio estimation.

### Prototype modification in reproducing kernel Hilbert space (RKHS)

A positive definite kernel function has its associated RKHS. A classification using the ratio of KDEs corresponds to a prototype classification in RKHS that determines which of the two classes has a closer mean than the other [29, Section 1.2]. The application of a weight function corresponds to finding a different prototype from the mean [30]. The relationship between the new-found prototype and the KDEs has been previously discussed [31].

## 5 Experiments

### Estimation of log probability density ratio and K-L divergence in 1D

We first demonstrate in Fig. 3 how the use of VWKDE alters log probability density ratio (LPDR) and K-L divergence toward a better estimation. We use two 1-dimensional Gaussians, \(p_{1}(x)\) and \(p_{2}(x)\), with means 0 and 1 and variances \(1.1^{2}\) and \(0.9^{2}\), respectively. We draw 1,000 samples from each density and construct KDEs and model-based VWKDEs for both LPDR and K-L divergence. For LPDR evaluation, we draw a separate 1,000 samples from each density, and the average square of biases and the variances at those points are calculated and presented in Fig. 3(b). The K-L divergence estimation result is shown in Fig. 3(c), where the true K-L divergence can be calculated analytically as \(KL(p_{1}||p_{2})\approx 0.664\), and the estimated values are compared with this true K-L divergence.

KDE-based LPDR estimation exhibits a severe bias, but this is effectively reduced by using VWKDE as an alternative plug-in. Although VWKDE slightly increases the variance of estimation, the reduction of bias is substantial in comparison. Note that since the bias is small over a wide range of \(h\), VWKDE yields a K-L divergence estimate which is relatively insensitive to the choice of \(h\).

### Synthetic distributions

We perform the VWKDE-based K-L divergence estimator along with other state-of-the-art estimators to estimate the K-L divergence \(KL(p_{1}||p_{2})\) between two synthetic Gaussian distributions \(p_{1}\) and \(p_{2}\) having \(\mu_{1}\) and \(\mu_{2}\) as their mean vectors and \(\Sigma_{1}\) and \(\Sigma_{2}\) as their covariance matrices, respectively.

Figure 4: K-L divergence estimation results for synthetic distributions; (NN) Nearest-neighbor estimator; (NNG) NN estimator with metric learning [32]; (KLIEP) Direct importance estimation [21]; (NNGarcia) Risk-based \(f\)-divergence estimator [33]; (NNWang) Bias-reduced NN estimator [34]; (MIN) Mutual Information Neural Estimation [35]; (Ensemble) Weighted ensemble KDE estimator [36]; (vonMises) KDE estimator with von Mises expansion bias correction [37]; (KDE) KDE estimator; (VWKDE-MB, VWKDE-MF) Model-based and model-free approach of the proposed estimator in this paper.

Figure 3: Estimation results of the LPDR \(\log(p_{1}/p_{2})\) and the K-L divergence. (a) Estimation of LPDR at each point. The estimation bias from the true LPDR is reduced by using VWKDE. (b) Squared bias and variance of the estimation with respect to the bandwidth \(h\). Bias has been significantly reduced without increasing variance. (c) Mean and standard deviation of K-L divergence estimates with respect to the bandwidth \(h\).

We use three different settings for \(p_{1}\) and \(p_{2}\): Isotropic **Iso**, Non-isotropic Heteroscedastic (**NH**), and Varying Mean Diff (**VMD**). In **Iso**, \(\Sigma_{1}=\Sigma_{2}=I\) for an identity matrix \(I\). \(\mu_{1}=\mathbf{0}\) for a zero vector \(\mathbf{0}\). The first element of \(\mu_{2}\) is \(\sqrt{2}\), while the other elements are uniformly zero, resulting in \(KL(p_{1}||p_{2})=1\). In **NH**, \(\mu_{1}=\mu_{2}=\mathbf{0}\), and \(\Sigma_{1}=\mathbf{I}\). \(\Sigma_{2}\) is a matrix having a pair of off-diagonal element \((\Sigma_{2})_{1,2}=(\Sigma_{2})_{2,1}=0.1\), and other off-diagonal elements are zero. The diagonal elements have a constant value \(\omega\), which is determined to yield \(KL(p_{1}||p_{2})\approx 1.0\) with \(\omega=0.750^{2}\) (10D) and \(KL(p_{1}||p_{2})\approx 0.5\) with \(\omega=0.863^{2}\) (20D). In **VMD**, the first element of \(\mu_{2}\) has various values between 0 and 2, while \(\mu_{1}=\mathbf{0}\) and the other elements in \(\mu_{2}\) remain zero. \(\Sigma_{1}=\Sigma_{2}=I\). In **Iso** and **NH**, we vary the sample size, and in **VMD**, we use 2,000 samples per distribution. We repeat each experiment 30 times and display the mean and the standard deviation in Figure 4.

In the upper left panel of Figure 4, we observe that almost all algorithms estimate the low-dimensional K-L divergence reliably, but the results deteriorate dramatically as shown in the upper middle and right panels with high-dimensionality. As shown in the lower left and lower middle panels, most of the baseline methods fail to produce the estimates near the true value when data are correlated. The model-based VWKDE-based estimator is the only estimator that recovers the true value in the 20D **NH** case. Figure 5 shows the K-L divergence estimation for non-Gaussian densities. In this example, the model for the score function in model-based VWKDE is different from the data-generating density, in which the estimator still shows very reliable estimates.

### Unsupervised optical surface inspection

We apply the proposed K-L divergence estimation using VWKDE for the inspection of the surface integrity based on the optical images. Most of the previous works have formulated the inspection using the _supervised_ setting [38, 39, 40, 41]; however, often the defect patterns are diverse, and training data do not include all possible defect patterns. In real applications, identification and localization of _unseen_ defect patterns are important. In this example, we apply the model-based VWKDE.

Detection of defective surfaceFollowing the representations of previous works on image classification [42, 43], we extract random small patches from each image \(\mathbf{I}\) and assume that those patches are the independently generated data. We use the probability density \(p_{\mathbf{I}}(\mathbf{x})\), for the patch \(\mathbf{x}\in\mathbb{R}^{D}\) from \(\mathbf{I}\). Given the \(N\) number of normal surface images \(\mathcal{D}=\{\mathbf{I}_{i}\}_{i=1}^{N}\) and a query image \(\mathbf{I}^{*}\), we determine whether \(\mathbf{I}^{*}\) is a defective surface according to the following decision function \(f(\mathbf{I}^{*})\) and a predefined

Figure 5: Estimation of K-L divergence between two non-Gaussian densities, \(p_{1}(\mathbf{x})\) and \(p_{2}(\mathbf{x})\). Each density is the Gaussian mixture of the three Gaussians, as shown in the 2-dimensional density contour in the figure on the left. They are the true densities but are very dissimilar to the single Gaussian model. The figure in the middle shows the estimation with 2-dimensional data, and the figure on the right shows the estimation with 20-dimensional data. With 20-dimensional data, the remaining 18 dimensionalities have the same mean isotropic Gaussians without correlation to the first two dimensionalities.

Figure 6: Detection of an artificially injected defect (MNIST digit ”3”). The first panel shows the image with an injected defect. The remaining three panels show the detection scores of different methods.

threshold:

\[f(\mathbf{I}^{*})=\min_{\mathbf{I}_{i}\in\mathcal{D}}\widehat{KL}(p_{\mathbf{I}^{*} }||p_{\mathbf{I}_{i}}).\] (21)

Defect localizationOnce the defective surface is detected, the spot of the defect can be localized by inspecting the LPDR \(\log(p_{\mathbf{I}^{*}}(\mathbf{x})/p_{\mathbf{I}_{m}}(\mathbf{x}))\) score between the query image \(\mathbf{I}^{*}\) and the \(\mathbf{I}_{m}\) with \(\mathbf{I}_{m}=\arg\min_{\mathbf{I}_{i}\in\mathcal{D}}\widehat{KL}(p_{\mathbf{ I}^{*}}||p_{\mathbf{I}_{i}})\). The location of the patch \(\mathbf{x}\) with the large LPDR score is considered to be the defect location. Note that a similar approach has been used for the witness function in statistical model criticism [44, 45].

For the evaluation of the algorithm, we use a publicly available dataset for surface inspection: DAGM2. The dataset contains six distinct types of normal and defective surfaces. The defective samples are not used in training, but they are used in searching the decision thresholds. We extract 900 patches per image, and each patch is transformed into a four-dimensional feature vector. Then, the detection is performed and compared with many well-known criteria: diverse K-L divergences estimators as well as the maximum mean discrepancy (MMD) [46] and the one-class support vector machines (OSVM) [47]. In addition, the Convolutional Neural Networks (CNNs) training result is presented for comparison with a supervised method.

Footnote 2: Deutsche Arbeitsgemeinschaft für Mustererkennung (The German Association for Pattern Recognition).

In DAGM, the testing data have defect patterns similar to those in the training data. To demonstrate unseen defect patterns, we artificially generate defective images by superimposing a randomly selected 15% of the normal testing images with a small image of the MNIST digit '3' at a random location (see Figure 6). Table 1 presents the area under curve (AUC) of the receiver operating characteristic curve for the detection as well as the mean average precision (mAP) for the localization.

CNNs which use labels for training show good performances only in detecting and localizing DAGM defects. The K-L divergence estimation with VWKDE show the best performance over many unsupervised methods, and it provides significantly better performances both at identifying unseen defects and at localizing them. Figure 6 shows one example of how well the proposed method localizes the position of the unseen defects.

## 6 Conclusion

In this paper, we have shown how a weighted kernel formulation for the plug-in densities could be optimized to mitigate the bias in consideration of the geometry of densities. The underlying mechanism uses the information from the first derivatives to alleviate the bias due to the second derivatives.

In our experiments, a simple choice of Gaussian density model for obtaining the first and second derivatives led to a reliable reduction of bias. This insensitivity to the exactness due to a coarse model is nonintuitive considering the traditional dilemma prevalent in many conventional methods; a coarse and inexact model enjoys a small variance but at the cost of large bias. In our work, the usage of the coarse model had no effect on the flexibility of the plug-in estimator, while the high dimensional bias was tackled precisely.

\begin{table}
\begin{tabular}{l c c} \hline \hline mAUC & DAGM Defect & Unseen Defect \\ \hline
**VWKDE** & **0.785 \(\pm\) 0.002** & **0.967 \(\pm\) 0.003** \\ KDE & 0.734 \(\pm\) 0.005 & 0.926 \(\pm\) 0.003 \\ NN-1 & 0.628 \(\pm\) 0.002 & 0.813 \(\pm\) 0.001 \\ NN-10 & 0.540 \(\pm\) 0.003 & 0.614 \(\pm\) 0.002 \\ NNWang & 0.605 \(\pm\) 0.002 & 0.657 \(\pm\) 0.004 \\ MMD & 0.618 \(\pm\) 0.003 & 0.615 \(\pm\) 0.008 \\ OSVM & 0.579 \(\pm\) 0.001 & 0.538 \(\pm\) 0.000 \\ \hline CNN & 0.901 \(\pm\) 0.011 & 0.809 \(\pm\) 0.029 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c} \hline \hline mAP & DAGM Defect & Unseen Defect \\ \hline
**VWKDE** & **0.369 \(\pm\) 0.005** & **0.903 \(\pm\) 0.007** \\ KDE & 0.294 \(\pm\) 0.004 & 0.849 \(\pm\) 0.006 \\ NN-1 & 0.095 \(\pm\) 0.008 & 0.488 \(\pm\) 0.002 \\ NN-10 & 0.081 \(\pm\) 0.004 & 0.254 \(\pm\) 0.002 \\ NNWang & 0.029 \(\pm\) 0.005 & 0.024 \(\pm\) 0.000 \\ MMD & 0.151 \(\pm\) 0.006 & 0.032 \(\pm\) 0.001 \\ OSVM & 0.249 \(\pm\) 0.012 & 0.444 \(\pm\) 0.009 \\ \hline CNN & 0.699 \(\pm\) 0.037 & 0.564 \(\pm\) 0.060 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performances for defect surface detection (left) and defect localization (right). mAUC and mAP are averaged over six surface types of DAGM. The DAGM dataset is provided with labels, and only CNN used the labels for training. The unseen defect is the artificially injected MNIST digit “3.”Limitations of this study include the computational overhead for score learning using parametric or neural network methods and no benefit for the asymptotic convergence rate because it depends on the convergence rate of KDE. Using a non-flexible parametric model rather than a flexible one provides a consistent benefit to improve the KDE.

## Acknowledgments and Disclosure of Funding

SY was supported by a KIAS Individual Grant (AP095701) via the Center for AI and Natural Sciences at Korea Institute for Advanced Study. SY and FCP were supported in part by IITP-MSIT (2021-0-02068, 2022-0-00480), ATC+ (20008547), SRRC NRF (RS-2023-00208052), and SNU Institute for Engineering Research. GY was partly supported by IITP-MSIT (2019-0-01906), and IK and YKN was supported by NRF/MSIT (No. 2018R1A5A7059549, 2021M3E5D2A01019545), IITP/MSIT (IITP-2021-0-02068, 2020-0-01373, RS-2023-00220628). YKN was supported by Samsung Research Funding & Incubation Center for Future Technology (SRFC-IT1901-13) partly in the derivation of the weight dependency on the bias and its mechanical interpretation.

## References

* [1] Jaewoong Cho, Gyeongjo Hwang, and Changho Suh. A fair classifier using kernel density estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 15088-15099. Curran Associates, Inc., 2020.
* [2] Edward Gan and Peter Bailis. Scalable kernel density classification via threshold-based pruning. In _SIGMOD '17: Proceedings of the 2017 ACM International Conference on Management of Data_, pages 945-959, 2017.
* [3] Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In Petra Perner, editor, _Machine Learning and Data Mining in Pattern Recognition_, pages 61-75, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
* [4] Dirceu Scaldelai, Luiz Carlos Matioli, Solange Regina dos Santos, and Mariana Kleina. MulticlusterKDE: a new algorithm for clustering based on multivariate kernel density estimation. _Journal of Applied Statistics_, 49(1):98-121, 2020.
* [5] Dirk Ormoneit and Saunak Sen. Kernel-based reinforcement learning. _Machine Learning_, 49:161-178, 2002.
* [6] Haanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, and Kee-Eung Kim. Local metric learning for off-policy evaluation in contextual bandits with continuous actions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 3913-3925. Curran Associates, Inc., 2022.
* [7] Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, and Larry Wasserman. Nonparametric estimation of renyi divergence and friends. In Eric P. Xing and Tony Jebara, editors, _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pages 919-927, Bejing, China, 22-24 Jun 2014. PMLR.
* [8] Dongxin Xu and Deniz Erdogmuns. Renyi's entropy, divergence and their nonparametric estimators. _In: Information Theoretic Learning. Information Science and Statistics_, 2010.
* [9] Jose C. Principe, Dongxin Xu, Qun Zhao, and John W. Fisher. Learning from examples with information theoretic criteria. _Journal of VLSI Signal Processing Systems_, 26(1-2):75-113, 2000.
* [10] Kumar Sricharan, Raviv Raich, and Alfred O. Hero III. Estimation of nonlinear functionals of densities with confidence. _IEEE Transactions on Information Theory_, 58(7):4135-4159, 2012.
* [11] Ziv Goldfeld, Kristjan H. Greenewald, Jonathan Niles-Weed, and Yury Polyanskiy. Convergence of smoothed empirical measures with applications to entropy estimation. _IEEE Transactions on Information Theory_, 66(7):4368-4391, 2020.
* [12] Larry Wasserman. _All of nonparametric statistics: a concise course in nonparametric statistical inference_. Springer, 2005.
* 694, 1992.
- 1479, 2002.
* [15] M. C. Jones, David F. Signorini, and Nils Lid Hjort. On multiplicative bias correction in kernel density estimation. _Sankhya: The Indian Journal of Statistics, Series A (1961-2002)_, 61(3):422-430, 1999.
* [16] David Ruppert and Daren B. H. Cline. Bias reduction in kernel density estimation by smoothed empirical transformations. _The Annals of Statistics_, 22(1):185-210, 1994.
* [17] Yung-Kyun Noh, Masashi Sugiyama, Kee-Eung Kim, Frank Park, and Daniel D Lee. Generative local metric learning for kernel regression. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30_, pages 2452-2462. Curran Associates, Inc., 2017.
* [18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1321-1330. PMLR, 06-11 Aug 2017.
* [19] Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and class-probability estimation. In Maria Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 304-313, New York, New York, USA, 20-22 Jun 2016. PMLR.
* [20] XuanLong Nguyen, Martin J Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, _Advances in Neural Information Processing Systems 20_, pages 1089-1096. Curran Associates, Inc., 2008.
* [21] Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von Bunau, and Motoaki Kawanabe. Direct importance estimation for covariate shift adaptation. _Annals of the Institute of Statistical Mathematics_, 60(4):699-746, 2008.
* [22] Vladimir N Vapnik and Sayan Mukherjee. Support vector method for multivariate density estimation. In _Advances in Neural Information Processing Systems_, pages 659-665, 2000.
* [23] Mark Girolami and Chao He. Probability density estimation from optimally condensed data samples. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 25(10):1253-1264, 2003.
* [24] Le Song, Xinhua Zhang, Alex Smola, Arthur Gretton, and Bernhard Scholkopf. Tailoring density estimation via reproducing kernel moment matching. In _Proceedings of the 25th international conference on Machine learning_, pages 992-999. ACM, 2008.
* [25] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, PMLR_, volume 115, pages 574-584, 2020.
* [26] Yung-Kyun Noh, Byoung-Tak. Zhang, and Daniel D. Lee. Generative local metric learning for nearest neighbor classification. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(1):106-118, 2018.
* [27] S. Chandrasekhar. Stochastic problems in physics and astronomy. _Reviews of Modern Physiscs_, 15:1-89, 1943.
* [28] Scott A. Socolofsky and Gerhard H. Jirka. _Environmental fluid mechanics. Part I: Mass transfer and diffusion. Engineering-lectures [online]_. 2004. Karlsruhe : Inst. fur Hydromechanik. 2nd ed. 2002.
* [29] Bernhard. Scholkopf and Alex J. Smola. _Learning with kernels: support vector machines, regularization, optimization, and beyond_. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, USA, December 2002.
* [30] Krikamol Muandet and Bernhard Scholkopf. A unifying view of support measure machines, support vector machines, and parzen window classifiers. _Unpublished_, 2020.
* [31] Thomas Hofmann, Bernhard Scholkopf, and Alexander J. Smola. Kernel methods in machine learning. _The Annals of Statistics_, 36(3):1171-1220, 2008.
* [32] Yung-Kyun Noh, Masashi Sugiyama, Song Liu, Marthinus C. du Plessis, Frank C. Park, and Daniel D. Lee. Bias reduction and metric learning for nearest-neighbor estimation of Kullback-Leibler divergence. _Neural computation_, page 1, 2018.

* [33] Dano Garcia-Garcia, Ulrike von Luxburg, and Raul Santos-Rodriguez. Risk-based generalizations of f-divergences. In _Proceedings of the 28th International Conference on Machine Learning, ICML_, pages 417-424, 2011.
* [34] Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu. Divergence estimation for multidimensional densities via \(k\)-nearest-neighbor distances. _IEEE Transactions on Information Theory_, 55(5):2392-2405, 2009.
* [35] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Devon Hjelm, and Aaron Courville. Mutual information neural estimation. In _International Conference on Machine Learning_, pages 530-539, 2018.
* [36] Kevin Moon, Kumar Sricharan, Kristjan Greenewald, and Alfred Hero. Ensemble estimation of information divergence. _Entropy_, 20(8):560, 2018.
* [37] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, and James M Robins. Influence functions for machine learning: Nonparametric estimators for entropies, divergences and mutual informations. _arXiv preprint arXiv:1411.4342_, 2014.
* [38] Franz Pernkopf and Paul O'Leary. Visual inspection of machined metallic high-precision surfaces. _EURASIP Journal on Advances in Signal Processing_, 2002(7):650750, 2002.
* [39] Fabian Timm and Erhardt Barth. Non-parametric texture defect detection using weibull features. In _Image Processing: Machine Vision Applications IV_, volume 7877, page 78770J. International Society for Optics and Photonics, 2011.
* [40] Daniel Weimer, Bernd Scholz-Reiter, and Moshe Shpitalni. Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection. _CIRP Annals_, 65(1):417-420, 2016.
* [41] Seunghyeon Kim, Wooyoung Kim, Yung-Kyun Noh, and Frank C Park. Transfer learning for automated optical inspection. In _Neural Networks (IJCNN), 2017 International Joint Conference on_, pages 2517-2524. IEEE, 2017.
* [42] Barnabas Poczos, Liang Xiong, Dougal J Sutherland, and Jeff Schneider. Nonparametric kernel estimators for image classification. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 2989-2996. IEEE, 2012.
* [43] GA Kaminka et al. Randomized distribution feature for image classification. In _ECAI 2016: 22nd European Conference on Artificial Intelligence, 29 August-2 September 2016, The Hague, The Netherlands-Including Prestigious Applications of Artificial Intelligence (PAIS 2016)_, volume 285, page 426. IOS Press, 2016.
* [44] James R Lloyd and Zoubin Ghahramani. Statistical model criticism using kernel two sample tests. In _Advances in Neural Information Processing Systems_, pages 829-837, 2015.
* [45] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. In _Advances in Neural Information Processing Systems_, pages 2280-2288, 2016.
* [46] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _Journal of Machine Learning Research_, 13(Mar):723-773, 2012.
* [47] Bernhard Scholkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_, 13(7):1443-1471, 2001.
* [48] John. C. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In _Advances in Large Margin Classifiers_, 2000.
* [49] ABA. Graf, O. Bousquet, G. Ratsch, and B. Scholkopf. Prototype classification: Insights from machine learning. _Neural Computation_, 21(1):272-300, January 2009.
* [50] Adam Kowalczyk. Maximal margin perceptron. _In Smola, Bartlett, Scholkopf, and Schuurmans, editors, Advances in Large Margin Classifiers_, pages 75-113, 2000.

## Appendix A Bias Derivation of the Posterior Estimator

The expectation of the weighted KDE is obtained from the following equation:

\[\mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})] = \mathbb{E}_{\mathbf{x}^{\prime}\sim p_{1}(\mathbf{x})}[\alpha( \mathbf{x}^{\prime})k_{h}(\mathbf{x},\mathbf{x}^{\prime})]\;=\;\int\alpha( \mathbf{x}^{\prime})p_{1}(\mathbf{x}^{\prime})k_{h}(\mathbf{x},\mathbf{x}^{ \prime})\mathrm{d}\mathbf{x}^{\prime}\] (22) \[=\int\alpha(\mathbf{x}^{\prime})p_{1}(\mathbf{x}^{\prime})\frac{ 1}{h^{D}}K\left(\frac{\mathbf{x}^{\prime}-\mathbf{x}}{h}\right)\mathrm{d} \mathbf{x}^{\prime}=\int\alpha(\mathbf{x}+h\mathbf{z})p_{1}(\mathbf{x}+h \mathbf{z})K(\mathbf{z})\mathrm{d}\mathbf{z},\] (23)

with the substitution \(\mathbf{z}=\frac{\mathbf{x}^{\prime}-\mathbf{x}}{h}\), or \(\mathbf{x}^{\prime}=h\mathbf{z}+\mathbf{x}\) to produce \(\mathrm{d}\mathbf{x}^{\prime}=h^{D}\mathbf{dz}\), and \(K\left(\frac{\mathbf{x}^{\prime}-\mathbf{x}}{h}\right)=h^{D}k_{h}(\mathbf{x},\mathbf{x}^{\prime})\) with normalized and isotropic \(K(\mathbf{z})\). We apply Taylor expansion on the term \(\alpha(\mathbf{x}+h\mathbf{z})p_{1}(\mathbf{x}+h\mathbf{z})\) around \(\mathbf{x}\), and with the assumption that \(|h\mathbf{z}|\) is small,

\[\alpha(\mathbf{x}+h\mathbf{z})p_{1}(\mathbf{x}+h\mathbf{z})=\alpha(\mathbf{x} )p_{1}(\mathbf{x})+h\mathbf{z}^{\top}\nabla[\alpha(\mathbf{x})p_{1}(\mathbf{ x})]+\frac{h^{2}}{2}\mathbf{z}^{\top}\nabla\nabla[\alpha(\mathbf{x})p_{1}( \mathbf{x})]\mathbf{z}+O(h^{3}),\] (24)

using the Hessian operator \(\nabla\nabla\). Now the integration yields the expectation with respect to \(K(\mathbf{z})\) that satisfies \(\int K(\mathbf{z})d\mathbf{z}=1\), \(\int\mathbf{z}K(\mathbf{z})d\mathbf{z}=0\), and \(\int\mathbf{z}\mathbf{z}^{\top}K(\mathbf{z})d\mathbf{z}=I\):

\[\mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})]=\alpha(\mathbf{x} )p_{1}(\mathbf{x})+\frac{h^{2}}{2}\nabla^{2}[\alpha(\mathbf{x})p_{1}(\mathbf{ x})]+O(h^{3}),\] (25)

with the Laplacian operator \(\nabla^{2}\).

Along with the expansion for \(\mathbb{E}_{\mathcal{D}_{2}}[\widehat{p}_{2}(\mathbf{x})]\), the following plug-in posterior can be perturbed by \(h\) assuming a small \(h\):

\[\mathbb{E}_{\mathcal{D}_{1},\mathcal{D}_{2}}\left[f(\mathbf{x})\right] \rightarrow \frac{\mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})]}{ \mathbb{E}_{\mathcal{D}_{1}}[\widehat{p}_{1}(\mathbf{x})]+\gamma\mathbb{E}_{ \mathcal{D}_{2}}[\widehat{p}_{2}(\mathbf{x})]}\] (27) \[= f(\mathbf{x})+\frac{h^{2}}{2}\frac{\gamma p_{1}(\mathbf{x})p_{2} (\mathbf{x})}{(p_{1}(\mathbf{x})+\gamma p_{2}(\mathbf{x}))^{2}}\left(\frac{ \nabla^{2}[\alpha(\mathbf{x})p_{1}(\mathbf{x})]}{\alpha(\mathbf{x})p_{1}( \mathbf{x})}-\frac{\nabla^{2}[\alpha(\mathbf{x})p_{2}(\mathbf{x})]}{\alpha( \mathbf{x})p_{2}(\mathbf{x})}\right)+\mathcal{O}(h^{3})\] \[= f(\mathbf{x})\;+\;\frac{h^{2}}{2}P(y=1|\mathbf{x})P(y=2|\mathbf{ x})B_{\alpha;p_{1},p_{2}}(\mathbf{x})\;+\;\mathcal{O}(h^{3}),\]

giving the point-wise leading-order bias with respect to \(h\):

\[\text{Bias}(\mathbf{x})=\frac{h^{2}}{2}P(y=1|\mathbf{x})P(y=2|\mathbf{x})B_{ \alpha;p_{1},p_{2}}(\mathbf{x}).\] (28)

Here, the \(B_{\alpha:p_{1},p_{2}}(\mathbf{x})\) is as follows:

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x})\equiv\frac{\nabla^{2}[\alpha(\mathbf{x})p_ {1}(\mathbf{x})]}{\alpha(\mathbf{x})p_{1}(\mathbf{x})}-\frac{\nabla^{2}[ \alpha(\mathbf{x})p_{2}(\mathbf{x})]}{\alpha(\mathbf{x})p_{2}(\mathbf{x})},\] (29)

which includes the second derivative of \(\alpha(\mathbf{x})p_{1}(\mathbf{x})\) and \(\alpha(\mathbf{x})p_{2}(\mathbf{x})\). Because two classes use the same weight function \(\alpha(\mathbf{x})\), Eq. (29) can be decomposed into two terms without the second derivative of \(\alpha(\mathbf{x})\).

\[B_{\alpha;p_{1},p_{2}}(\mathbf{x})=\frac{\nabla^{\top}\alpha|_{ \mathbf{x}}}{\alpha(\mathbf{x})}\left(\frac{\nabla p_{1}|_{\mathbf{x}}}{p_{1}( \mathbf{x})}-\frac{\nabla p_{2}|_{\mathbf{x}}}{p_{2}(\mathbf{x})}\right)+ \frac{1}{2}\left(\frac{\nabla^{2}p_{1}|_{\mathbf{x}}}{p_{1}(\mathbf{x})}- \frac{\nabla^{2}p_{2}|_{\mathbf{x}}}{p_{2}(\mathbf{x})}\right).\] (30)

## Appendix B Solution of the Calculus of Variation

For the optimization of Eq. (15) with respect to \(\alpha(\mathbf{x})\), we first make a substitution \(\beta=\log\alpha\) and apply a calculus of variation technique for optimal \(\beta(\mathbf{x})\). We express the objective functional with \(\int m(\mathbf{x};\beta,\nabla\beta)\;d\mathbf{x}\) using

\[m(\mathbf{x};\beta,\nabla\beta)=\left(\nabla^{\top}\beta|_{ \mathbf{x}}\mathbf{h}(\mathbf{x})+g(\mathbf{x})\right)^{2}r(\mathbf{x}).\] (31)

With the substitution \(\vec{\beta}^{\prime}=\nabla\beta\) for notational abbreviation, we apply the Euler-Lagrange equation for the \(m(\mathbf{x};\beta,\vec{\beta}^{\prime})\) containing both \(\beta\) and \(\vec{\beta}^{\prime}\):

\[\frac{\partial m(\mathbf{x};\beta,\vec{\beta}^{\prime})}{\partial \beta}-\nabla_{\mathbf{x}}\cdot\nabla_{\vec{\beta}^{\prime}}\;m(\mathbf{x}; \beta,\vec{\beta}^{\prime})=0,\] (32)where the divergence is \(\nabla_{\mathbf{x}}\cdot\nabla_{\vec{\beta}^{\prime}}=\sum_{i=1}^{D}\frac{\partial }{\partial x_{i}}\frac{\partial}{\partial\beta_{i}^{\prime}}\) with the \(i\)-th component of \(\vec{\beta}^{\prime},\beta_{i}^{\prime}\), and the dimensionality is \(D\).

The first term can be calculated as \(\frac{\partial m(\mathbf{x};\beta,\vec{\beta^{\prime}})}{\partial\beta}=0\). The first derivative of the second term is \(\nabla_{\vec{\beta^{\prime}}}\ m(\mathbf{x};\beta,\vec{\beta^{\prime}})=r \left(\vec{\beta}^{\top}\mathbf{h}+g\right)\mathbf{h}\). After we substitute \(\beta(\mathbf{x})\) with \(\log\alpha(\mathbf{x})\) back, we obtain the equation for the optimal \(\alpha(\mathbf{x})\) function:

\[\nabla\cdot\left[r((\nabla\log\alpha)^{\top}\mathbf{h}+g)\mathbf{h}\right]=0.\] (33)

which is Eq. (16).

## Appendix C Analytic Solution for Two Homoscedastic Gaussians

We consider the following two homoscedastic Gaussians

\[p_{1}(\mathbf{x})=\mathcal{N}(\mathbf{x};\mu_{1},\Sigma),\quad p_{2}(\mathbf{ x})=\mathcal{N}(\mathbf{x};\mu_{2},\Sigma),\] (34)

with a common covariance matrix \(\Sigma\).

In order to obtain the zero divergence in Eq. (16), the divergence-free vector field can be obtained using

\[\nabla\log\alpha\equiv\Sigma^{-1}\mathbf{x}+\vec{v}(\mathbf{x})\] (35)

because the inner product of \(\nabla\log\alpha\) with \(\mathbf{h}=\nabla\log p_{1}-\nabla\log p_{2}=\Sigma^{-1}(\mu_{1}-\mu_{2})\) should yield a negative value of \(g(\mathbf{x})\), which is \(-g(\mathbf{x})=-\mathbf{x}^{\top}\Sigma^{-2}(\mu_{2}-\mu_{1})-\frac{1}{2}(\mu _{1}^{\top}\Sigma^{-2}\mu_{1}-\mu_{2}^{\top}\Sigma^{-2}\mu_{2})\). The equation \((\nabla\log\alpha)^{\top}\mathbf{h}=-g(\mathbf{x})\) gives

\[\vec{v}(\mathbf{x})=-\frac{1}{2}\Sigma^{-1}(\mu_{1}+\mu_{2}).\] (36)

Therefore, one possible solution for \(\nabla\log\alpha(\mathbf{x})\) is

\[\nabla\log\alpha(\mathbf{x})=\Sigma^{-1}(\mathbf{x}-\mu),\] (37)

with the mean of the two class-conditional means, \(\mu=\frac{\mu_{1}+\mu_{2}}{2}\). Therefore, one particular solution for \(\alpha(\mathbf{x})\) is

\[\alpha(\mathbf{x})=\exp\left[\frac{1}{2}(\mathbf{x}-\mu)^{\top}\Sigma^{-1}( \mathbf{x}-\mu)\right].\] (38)

This solution is not unique, and any \(\log\alpha\) that has a form of

\[\log\alpha(\mathbf{x})=\frac{1}{2}(\mathbf{x}-\mu)^{\top}\Sigma^{-1}( \mathbf{x}-\mu)+l(\mathbf{x}),\] (39)

with \(l(\mathbf{x})\) satisfying \(\nabla^{\top}l\ \Sigma^{-1}(\mu_{1}-\mu_{2})=0\) is also the solution. One technique for finding such \(l(\mathbf{x})\) is that we pick up any differentiable seed function \(l_{0}(\mathbf{x})\) and consider its derivative \(\nabla l_{0}\) with the \(\vec{a}=\Sigma^{-1}(\mu_{1}-\mu_{2})\) component subtracted: \(\left(I-\frac{\vec{a}\vec{a}^{\top}}{||\vec{a}||^{2}}\right)\nabla l_{0}\). The \(l(\mathbf{x})\) is a function that its derivative satisfies \(\nabla l=\left(I-\frac{\vec{a}\vec{a}^{\top}}{||\vec{a}||^{2}}\right)\nabla l_ {0}\).

For example, if we choose \(l_{0}(\mathbf{x})=\mathbf{x}\), then \(l(\mathbf{x})\) is a function that satisfies \(\nabla l=\left(I-\frac{\vec{a}\vec{a}^{\top}}{||\vec{a}||^{2}}\right)\nabla l _{0}=\mathbf{I}-\frac{\vec{a}^{\top}\mathbf{I}}{||\vec{a}||^{2}}\), and we can get \(l(\mathbf{x})=\left(\mathbf{I}-\frac{\vec{a}^{\top}}{||\vec{a}||^{2}}\right)^{ \top}\mathbf{x}\). If we choose \(l_{0}(\mathbf{x})=\frac{1}{2}||\mathbf{x}||^{2}\), we get \(l(\mathbf{x})=\frac{1}{2}\mathbf{x}^{\top}\left(I-\frac{\vec{a}\vec{a}^{\top} }{||\vec{a}||^{2}}\right)\mathbf{x}\) after similar calculations. Now the choice of

\[l_{0}(\mathbf{x})=-\frac{b}{2}\left(\mathbf{x}-\mu\right)^{2},\] (40)

gives us \(l(\mathbf{x})=-\frac{b}{2}(\mathbf{x}-\mu)^{\top}\left(I-\frac{\vec{a}\vec{a} ^{\top}}{||\vec{a}||^{2}}\right)(\mathbf{x}-\mu)\), which produces our analytic weight function in Eq. (18):

\[\alpha(\mathbf{x})=\exp\left(-\frac{1}{2}(\mathbf{x}-\mu^{\prime})^{\top}A( \mathbf{x}-\mu^{\prime})\right),\] (41)

with \(\mu^{\prime}=\frac{\mu_{1}+\mu_{2}}{2}\) and \(A=b\left(I-\frac{\Sigma^{-1}(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{\top}\Sigma^{-1 }}{||\Sigma^{-1}(\mu_{1}-\mu_{2})||^{2}}\right)-\Sigma^{-1}\), with an arbitrary constant \(b\).

## Appendix D Posterior Prediction for Various Algorithms

Fig. 7 shows the posterior prediction results of various algorithms. For the estimation with support vector machines, [48] is used. For neural network estimation, 2-layer fully connected networks with 100 nodes for each layer were used minimizing the mean square error of the sigmoid output. Both VWKDE-MB and VWKDE-MF show superior results to other methods.

### Kernel density estimation in high dimensions

We also note the difficulty of density estimation with KDE in high dimensional space. Fig. 8 shows a one-dimensional slice of two 20-dimensional Guassians. The maximum density in this slice is on the order of \(10^{-8}\). Meanwhile, the KDE with 5,000 data points per class shows densities on the order of about \(10^{-10}\) with a bandwidth of \(h=0.8\).

The bandwidth should be chosen to be sufficiently large because no pairs are nearby in a high-dimensional space. Although the estimated density with \(h=0.8\) is reasonably smooth, the KDE differs from the true density by several orders of magnitude with 5,000 data points. However, the patterns of relative overestimation and underestimation depicted in Fig. 1 are evident, and the proposed method can be applied even with inexact KDEs.

## Appendix E Fluid Flow Interpretation of Making Bias

The change of concentration \(u(t)\) at time \(t\) due to the convection and diffusion can be written as

\[\frac{\partial u}{\partial t}=-\mathbf{v}^{\top}\nabla u+D^{\prime}\nabla^{2}u,\] (42)

Figure 8: Underlying density functions and their KDE predictions.

Figure 7: Posterior predictions with various algorithms for 20-dimensional Gaussians.

[MISSING_PAGE_EMPTY:16]

performance, the modification improves the classification or information-theoretic measure estimation for the KDE plug-in algorithms, but not necessarily the KDE itself. The improvement is partly supported by the prototype models in RKHS.

## Appendix G Least Square Approach for Binary Classification

Reducing the bias of the posterior equation in Eq. (3) corresponds to the least square of the prediction error. The optimal square error is achieved with the Bayes classifier, which classifies a datum according to the posterior probability. The posterior probability of \(\mathbf{x}_{0}\) being generated from \(p_{1}(\mathbf{x})\) can be written as:

\[P(y=1|\mathbf{x}_{0}) =\frac{p_{1}(\mathbf{x}_{0})p(y=1)}{p_{0}(\mathbf{x}_{0})p(y=0)+p _{1}(\mathbf{x}_{0})p(y=1)}\] (53) \[=\frac{p_{1}(\mathbf{x}_{0})}{\gamma p_{0}(\mathbf{x}_{0})+p_{1} (\mathbf{x}_{0})},\] (54)

where \(\gamma=p(y=0)/p(y=1)\). The least square error with

\[L=\int(f(\mathbf{x})-y)^{2}p(\mathbf{x},y)dyd\mathbf{x},\] (55)

is achieved with the following prediction function

\[f(\mathbf{x})=\mathbb{E}[y=1|\mathbf{x}]=P(y=1|\mathbf{x}).\] (56)

An accurate estimation of posterior is essential for successful classification. We construct a classifier based on the KDE density estimates \(\widehat{p}_{0}(\mathbf{x})\), \(\widehat{p}_{1}(\mathbf{x})\).

\[f(\mathbf{x}) =\frac{\widehat{p}_{1}(\mathbf{x})}{\gamma\widehat{p}_{0}( \mathbf{x})+\widehat{p}_{1}(\mathbf{x})}\] (57) \[=\frac{1}{1+\gamma(\widehat{p}_{0}(\mathbf{x})/\widehat{p}_{1}( \mathbf{x}))},\] (58)

and consider the deviation of \(f(\mathbf{x})\) from the true \(\mathbb{E}[y=1|\mathbf{x}]\).

## Appendix H Details on Optical Surface Inspection Experiments

We use a widely used public surface inspection dataset provided by DAGM3 for experiments. The dataset contains six distinct textile surface types and associated defect types. There are 1,150 images per class, half of which is for training and the remaining is for testing. Approximately 13% of total images are defective, and for each defective image, a masking image which roughly encloses the defective region are provided. The dataset is originally proposed for a supervised setting.

Footnote 3: Deutsche Arbeitsgemeinschaft für Mustererkennung (The German Association for Pattern Recognition). Data access: https://hci.iwr.uni-heidelberg.de/node/3616

We extract 900 patches of size 32\(\times\)32 from each image, using a sliding window with step size 16. In each patch, we apply Gaussian smoothing and Scharr kernel to obtain a gradient distribution which can capture the texture information. To encode the gradient distribution as a feature vector, we compute its mean, standard deviation, skewness, and kurtosis. As a result, a surface image is transformed into a set of 900 four-dimensional vectors, or a 900\(\times\)4 matrix. Feature vectors are standardized and whitened by aggregating all the patches from the same surface type.

VWKDE can be time-consuming as a large number of KL divergences need to be computed. Therefore, we take a two-pass approach when applying VWKDE. Given a query image, we first apply KDE-based KL divergence estimator to obtain rough estimates of KL divergences. Then, we take \(k\) images with the lowest KL divergences and apply VWKDE-based KL divergence estimator to the \(k\) images to finally select the image with the lowest KL divergence. This method enables us to have the best of both worlds, the speed of KDE and the accuracy of VWKDE.

The optimal bandwidth for bias reduction methods such as Ensemble [36], vonMises [37], and VWKDE is usually larger than other methods. We use the bandwidth with maximum leave-one-out log-likelihood of KDE for other methods but in these three methods, we used the heuristic rule of bandwidth selection using the maximum log-likelihood bandwidth for only 25% of randomly selected data.

A convolutional neural network (CNN) which takes a 32\(\times\)32 patch as an input and predicts whether the patch is defective is trained. For training, we label patches with 75% overlap to the defect mask as defective and patcheswithout any overlap to the defect mask as normal. Patches do not belong to either class are discarded. Due to class imbalance, normal patches are undersamples to yield defect to normal ratio of 1:4. CNN is trained for each surface type separately. The structure of our CNN is Conv(20)-Conv(20)-MaxPool-Conv(20)-MaxPool-FC(20)-DropOut-FC(1), where Conv is a 3\(\times\)3 convolution layer, MaxPool is a 2\(\times\) max pooling layer, FC is a fully connected layer, and DropOut is a drop out operation with probability 0.5. We use binary cross entropy loss for objective function and ADAM for optimization.

In unsupervised defect localization, we threshold log probability density ratio (LPDR) estimate to obtain detection results. We threshold LPDR estimates dynamically at 90% of maximum LPDR observed in the image. Then, we use its KL divergence estimate as a confidence score for the detection. For a CNN, we use the output probability for a patch as a detection score, and set a threshold to 0.9, and the maximum probability of defect among the patches in an image is used as a confidence score. Note that, in this experiment, we generate one detection per an image because DAGM dataset is constrained to have as most one defect per an image. However, this condition can be relaxed in future work with other dataset.

Intersection-over-union (IOU) is computed between a detection and a true defect mask. A detection with IOU larger than 0.1 considered as a correct detection. This threshold is lower than a typical threshold in object detection (0.5), because the defect mask is weakly labelled and usually larger than a precise defect region. Using a confidence score assigned for a detection, we compute average precision as in PASCAL VOC challenge, then take average over surface types.