# Rethinking Fine-tuning Through Geometric Perspective

 Krishna Sri Ipsit Mantri

Department of Computer Science

Purdue University

West Lafayette, IN

mantrik@purdue.edu &Moshe Eliasof

Department of Applied Mathematics

University of Cambridge

Cambridge, United Kingdom

me532@cam.ac.uk &Carola-Bibiane Schonlieb

Department of Applied Mathematics

University of Cambridge

Cambridge, United Kingdom

cbs31@cam.ac.uk &Bruno Ribeiro

Department of Computer Science

Purdue University

West Lafayette, IN

ribeirob@purdue.edu

###### Abstract

Fine-tuning pre-trained neural networks has become a cornerstone of transfer learning. However, the practical success of existing methods like low-rank adaptation (LoRA) lacks theoretical explanation. We introduce geometry-guided fine-tuning, a novel paradigm that models the fine-tuning process as the subtle movement of pre-trained weights on a low-dimensional manifold. Our approach formalizes this process through a learnable ordinary differential equation (ODE) - based framework that controls the search space of the weights, bridging existing methods with geometric principles. We empirically evaluate our method in the context of multi-task learning (MTL) fine-tuning of hierarchical vision transformers in computer vision. We propose a parameter-efficient ODE and evaluate it on the PASCAL-Context MTL benchmark. Our approach, dubbed DeLoRA offers competitive performance across multiple dense prediction tasks, reducing trainable parameters by up to 4\(\times\) compared to the best-performing baseline. This work advances both the theoretical understanding and practical application of fine-tuning, promoting efficient learning in resource-constrained environments.

## 1 Introduction

The success of large pre-trained neural networks across various domains, such as GPT-2 [14] and Stable Diffusion [17], has made fine-tuning an essential technique in transfer learning. As these models grow in size and complexity, efficient and effective fine-tuning methods become increasingly crucial. While approaches like Low-Rank Adaptation (LoRA) [15] have demonstrated practical success, they lack a geometric foundation that guides their design philosophy and accounts for their practical effectiveness.

Several works study the relationship between differential equations and neural networks, covering areas such as architecture design [1], stability [13], and activation function design [16, 17]. Taking a different approach, this paper proposes using an ordinary differential equation (ODE) to adapt learned neural network weights through a velocity field.

We introduce a novel fine-tuning paradigm: _geometry-informed fine-tuning of neural networks for multi-task learning (MTL)_. During training, the weights of over-parameterized neural networks often converge to a low-dimensional hyperplane, with an intrinsic dimension much smaller than the nominal parameter count (Li et al., 2018; Aghajanyan et al., 2020). Inspired by this, we hypothesize that the weights of large pre-trained models, such as hierarchical vision transformers (Liu et al., 2021), lie on a low-dimensional manifold within the high-dimensional parameter space. Fine-tuning can then be modeled as the process of moving these weights on this manifold while optimizing performance on downstream tasks.

This geometric perspective on fine-tuning offers several potential advantages:

1. It provides a generalized and geometrically grounded approach to fine-tuning, potentially situating existing low-rank adaptation methods, like LoRA (Hu et al., 2022), within a broader theoretical framework.
2. It allows control of inductive biases through the choice of geometric structures and dynamics in the fine-tuning process.
3. It enables the use of differential geometry tools to analyze the behavior of fine-tuned models and gain insights into the weight spaces of large pre-trained models.
4. It opens new avenues for developing more efficient and interpretable transfer learning techniques, potentially leading to significant reductions in computational resources required for adaptation to new tasks.

Our framework introduces a velocity-field-based dynamics approach to formalize the transportation of weights on their manifold during fine-tuning for MTL. This novel perspective bridges the gap between the empirical success of parameter-efficient fine-tuning methods and theoretical understanding, offering both insights into existing techniques and directions for developing new, principled approaches.

## 2 Geometry-Guided Fine-tuning

In this section, we present a conceptual framework for geometry-guided fine-tuning of neural networks, grounded in principles from differential geometry and dynamical systems. Within the context of MTL, we show in Section 2.2 how an ODE can be used to develop a broader theoretical framework. In Section 2.3, we then demonstrate a theoretical connection between our framework and a widely-used parameter-efficient fine-tuning method, LoRA (Hu et al., 2022).

### Background

We begin by formally introducing Multi-Task Learning (MTL). Let \(\mathcal{T}=\{T_{1},\cdots,T_{K}\}\) represent a set of \(K\) tasks. We consider a large model \(F_{\bm{\theta}}:\mathcal{X}\to\mathcal{Y}\) with pre-trained parameters \(\bm{\theta}\in\mathbb{R}^{n}\), where \(\mathcal{X}\) denotes the input space and \(\mathcal{Y}\) denotes the output space. For each task \(T_{k}\), we have a task-specific dataset \(\mathcal{D}_{k}=\{(x_{i},y_{i}^{k})\}_{i=1}^{N_{k}}\) where \(x_{i}\in\mathcal{X}\) and \(y_{i}^{k}\in\mathcal{Y}_{k}\) are the input-output pairs for task \(k\), with a shared input space across all tasks. The MTL objective is formulated as

\[\min_{\bm{\theta}}\sum_{k=1}^{K}\lambda_{k}\,\mathcal{L}_{k}(\bm{\theta}; \mathcal{D}_{k})\] (1)

where \(\mathcal{L}_{k}\) denotes the task-specific loss function, and \(\lambda_{k}\) are task-specific weighting factors.

### Proposed Method

We hypothesize that the weights of a pre-trained neural network lie on a low-dimensional manifold \(\mathcal{M}\) within the high-dimensional parameter space. This manifold encapsulates the learned features and structure from the pre-training process (Mao et al., 2024). Formally, we posit the existence of a smooth manifold \(\mathcal{M}\subset\mathbb{R}^{n}\) of dimension \(d\ll n\) such that \(\bm{\theta}\in\mathcal{M}\). The question then becomes: **Can we efficiently fine-tune large pre-trained models by guiding the MTL objective along this manifold \(\mathcal{M}\)?**

[MISSING_PAGE_FAIL:3]

attention weight matrices \(\mathbf{W}_{q},\mathbf{W}_{v}\), which are typically present in the Transformer layer (Vaswani et al., 2017). For a weight matrix \(\mathbf{W}\in\mathbb{R}^{d\times k}\), we compute a parameter-efficient velocity field as follows:

\[v(\mathbf{W},t)=\mathrm{softmax}\left(\frac{(\mathbf{W}_{2}\,\mathbf{W})( \mathbf{W}\,\mathbf{W}_{1})^{\top}}{\sqrt{d}}\right)\] (7)

where \(\mathbf{W}_{1}\in\mathbb{R}^{k\times r}\) and \(\mathbf{W}_{2}\in\mathbb{R}^{r\times d}\) are learnable projection matrices, \(r\in\mathbb{N}\) is a hyperparameter that determines the projection dimension, typically chosen such that \(r\ll\min(d,k)\). Given the combination of an ODE perspective and the low-rank parameterization of the learned velocity field, we refer to our method as DeLoRA.

### Results

Setup.We evaluate our proposed geometry-guided fine-tuning approach on the PASCAL-MTL benchmark (Everingham et al., 2010), following the PASCAL-Context split used in MTLoRA (Agiza et al., 2024). This dataset, widely used for multi-task learning (MTL) in dense prediction tasks, includes annotations for semantic segmentation, human part segmentation, surface normal estimation, and saliency detection. The PASCAL-Context dataset consists of 4,998 images in the training split and 5,105 images in the validation split. We apply the same data preprocessing and augmentation techniques as described in MTLoRA (Agiza et al., 2024) to ensure a fair comparison.

Model Architecture.Following MTLoRA (Agiza et al., 2024), we implement our approach using a Swin-Tiny transformer backbone (Liu et al., 2021), pre-trained on the _Imagenet22k_(Deng et al., 2009) dataset. The model follows an MTL architecture with a shared encoder and task-specific decoder for each of the four tasks.

Discussion.Our results are summarized in Table 1, where we compare DeLoRA against MTLoRA (Agiza et al., 2024), LoRA (Hu et al., 2022), single task fine-tuning and MTL full fine-tuning approaches. Our DeLoRA achieves competitive performance across all tasks:

1. **Parameter Efficiency:** Our DeLoRA achieves competitive performance while using only 2.42M trainable parameters. Although LoRA has a similar number of parameters, our method achieves better overall performance through joint training and enables single inference for all tasks.
2. **Multi-Task Balance:** Our approach shows a better balance across tasks, at times even outperforming MTLoRA, while using \(4\times\) fewer trainable parameters. This highlights the benefits of an ODE based approach and the choice of the velocity field.

These preliminary results provide strong evidence for the potential of geometry-guided fine-tuning. They demonstrate that DeLoRA can achieve parameter-efficient adaptation while maintaining or enhancing performance across multiple tasks.

## 4 Conclusions and Discussion

This paper introduces geometry-guided fine-tuning, a paradigm that frames the adaptation of pre-trained neural networks within the context of differential geometry and dynamical systems, introducing an early variant called DeLoRA. Our approach constrains fine-tuning by using the flow of a velocity field on the manifold of neural network parameters, offering several key advantages:

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Method** & **SemSeg** & **Human Parts** & **Saliency** & **Normals** & **Trainable Parameters (M)** & **Single Inference** \\  & (mIOU \(\uparrow\)) & (mIOU \(\uparrow\)) & (mIOU \(\uparrow\)) & (mse \(\downarrow\)) & (M) & **For All Tasks** \\ \hline Single Task & 67.21 & 61.93 & 62.35 & 17.97 & 112.62 & \(\times\) \\ MTL - Full Fine Tuning & 67.56 & 60.24 & 65.21 & 16.64 & 30.06 & \(\checkmark\) \\ LoRA (Hu et al., 2022) & 70.12 & 57.73 & 61.90 & 18.96 & 2.87 & \(\times\) \\ MTLoRA (Agiza et al., 2024) & 67.90 & 59.84 & 65.40 & 16.60 & 8.34 & \(\checkmark\) \\ \hline DeLoRA (Ours) & 69.72 & 58.22 & 58.84 & 19.23 & **2.42** & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with SOTA parameter efficient fine-tuning methods. The table summarizes the number of trainable parameters in each method. The last column indicates whether the model allows simultaneous execution of all tasks.

[MISSING_PAGE_FAIL:5]

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752.
* Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=n2eVKeeFYf9.
* Chen et al. (2018) Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Haber and Ruthotto (2017) Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. _Inverse problems_, 34(1):014004, 2017.
* Ipsit Mantri et al. (2024) Krishna Sri Ipsit Mantri, Xinzhi Wang, Carola-Bibiane Schonlieb, Bruno Ribeiro, Beatrice Bevilacqua, and Moshe Eliasof. Digraf: Diffeomorphic graph-adaptive activation function, 2024. URL https://arxiv.org/abs/2407.02013.
* Chelly et al. (2024) Irit Chelly, Shahaf E. Finder, Shira Irefang, and Oren Freifeld. Trainable highly-expressive activation functions, 2024. URL https://arxiv.org/abs/2407.07564.
* Li et al. (2018) Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. _arXiv preprint arXiv:1804.08838_, 2018.
* Aghajanyan et al. (2020) Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.
* Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021. URL https://arxiv.org/abs/2103.14030.
* Mao et al. (2024) Jialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang, Mark K. Transtrum, James P. Sethna, and Pratik Chaudhari. The training process of many deep networks explores the same low-dimensional manifold. _Proceedings of the National Academy of Sciences_, 121(12), March 2024. ISSN 1091-6490. doi: 10.1073/pnas.2310002121. URL http://dx.doi.org/10.1073/pnas.2310002121.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _International journal of computer vision_, 88:303-338, 2010.
* Agiza et al. (2024) Ahmed Agiza, Marina Neseem, and Sherief Reda. Mtlora: A low-rank adaptation approach for efficient multi-task learning, 2024. URL https://arxiv.org/abs/2403.20320.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Ortiz-Jimenez et al. (2024) Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zakariaei et al. (2024) Niloufar Zakariaei, Siddharth Rout, Eldad Haber, and Moshe Eliasof. Advection augmented convolutional neural networks. _arXiv preprint arXiv:2406.19253_, 2024.
* Zisserman et al. (2018)James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114(13):3521-3526, March 2017. ISSN 1091-6490. doi: 10.1073/pnas.1611835114. URL http://dx.doi.org/10.1073/pnas.1611835114.