# Continuous-Time Functional Diffusion Processes

 Giulio Franzese

EURECOM, France

&Giulio Corallo

EURECOM, France

&Simone Rossi

Stellantis, France

&Markus Heinonen

Aalto University, Finland

&Maurizio Filippone

EURECOM, France

&Pietro Michiardi

EURECOM, France

This work was done while working at EURECOM

###### Abstract

We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models. Code available here.

## 1 Introduction

Diffusion models have recently gained a lot of attention both from academia and industry. The seminal work on denoising diffusion (Sohl-Dickstein et al., 2015) has spurred interest in the understanding of such models from several perspectives, ranging from denoising autoencoders (Vincent, 2011) with multiple noise levels (Ho et al., 2020), variational interpretations (Kingma et al., 2021), annealed (Song and Ermon, 2019) and continuous-time score matching (Song and Ermon, 2020; Song et al., 2021). Several recent extensions of the theory underpinning diffusion models tackle alternatives to Gaussian noise (Bansal et al., 2022; Rissanen et al., 2022), second order dynamics (Dockhorn et al., 2022), and improved training and sampling (Xiao et al., 2022; Kim et al., 2022; Franzese et al., 2022).

Diffusion models have rapidly become the go-to approach for generative modeling, surpassing gans(Dhariwal and Nichol, 2021) for image generation, and have recently been applied to various modalities such as audio (Kong et al., 2021; Liu et al., 2022), video (Ho et al., 2022; He et al., 2022), molecular structures and general 3D shapes (Trippe et al., 2022; Hoogeboom et al., 2022; Luo and Hu, 2021; Zeng et al., 2022). Recently, the generation of diverse and realistic data modalities (images, videos, sound) from open-ended text prompts (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022) has projected practitioners into a whole new paradigm for content creation.

A common trait of diffusion models is the need to understand their design space (Karras et al., 2022), and tailor the inner working parts to the chosen application and data domain. Diffusion models require specialization, ranging from architectural choices of neural networks used to approximate the score (Dhariwal and Nichol, 2021; Karras et al., 2022), to fine-grained details such as an appropriate definition of a noise schedule (Dhariwal and Nichol, 2021; Salimans and Ho, 2022), and mechanisms to deal with resolution and scale (Ho et al., 2021). Clearly, the data domain impacts profoundly such design choices. As a consequence, a growing body of work has focused on the projection of datamodalities into a latent space (Rombach et al., 2022), either by using auxiliary models such as a Vaes(Vahdat et al., 2021), or by using a functional data representation (Dupont et al., 2022). These approaches lead to increased efficiency, because they operate on smaller dimensional spaces, and constitute a step toward broadening the applicability of diffusion models to general data.

The idea of modelling data with continuous functions has several advantages (Dupont et al., 2022): it allows working with data at arbitrary resolutions, it enjoys improved memory-efficiency, and it allows simple architectures to represent a variety of data modalities. However, a theoretically grounded understanding of how diffusion models can operate directly on continuous functions has been elusive so far. Preliminary studies apply established diffusion algorithms on a discretization of functional data by conditioning on point-wise values (Dutordoir et al., 2022; Zhuang et al., 2023). A line of work that is closely related to ours include approaches such as Kerrigan et al. (2022), who consider a Gaussian noise corruption process in Hilbert space and derive a loss function formulated on infinite-dimensional measures to approximate the conditional mean of the reverse process. Within this line of works, Mittal et al. (2022) consider diffusion of Gaussian processes. We are aware of other concurrent works that study diffusion process in Hilbert spaces (Lim et al., 2023; Pidstrigach et al., 2023; Hagemann et al., 2023). However, differently from us, these works do not formally prove that the score matching optimization is a proper evidence lower bound (ELBO), but simply propose it as an heuristic. None of these prior works discuss the limits of discretization, resulting in the failure of identifying which subset of functions can be reconstructed through sampling. Finally, the parametrization we present in our work merges how functions and score are approximated using a single, simple model.

The main goal of our work is to deepen our understanding of diffusion models in function space. We present a new mathematical framework to lift diffusion models from finite-dimensional inputs to function spaces, contributing to a general method for data represented by continuous functions.

In SS 2, we present Functional Diffusion Processs (Fdps), which generalize diffusion processes to infinite-dimensional functional spaces. We define forward (SS 2.1) and backward (SS 2.2) FDPs, and consider _generic_ functional perturbations, including noising and Laplacian blurring. Using an extension of Girsanov theorem, we derive in SS 2.3 an ELBO, which allows defining a parametric model to approximate the score of the functional density of Fdps. Given a Fdp and the associated ELBO, we are one-step closer to the definition of a loss function to learn the parametric score. However, our formulation still resides in an abstract, infinite-dimensional Hilbert space.

Then, for practical reasons, in SS 3, we specify for which subclass of functions we can perfectly reconstruct the original function given only its evaluation in a countable set of points. This is an extension of the sampling theorem, which we use to move from the infinite-dimensional domain of functions to a finite-dimensional domain of discrete mappings.

In SS 4, we discuss various options to implement such discrete mappings. In this work, we explore in particular the usage of implicit neural representations (INs) (Sitzmann et al., 2020) and Transformers Vaswani et al. (2017) to jointly model both the sampled version of infinite-dimensional functions, and the score network, which is central to the training of FDPs, and is required to simulate the backward process. Our training procedure, discussed in SS 5, involves approximate, finite-dimensional Stochastic Differential Equations (SDEs) for the forward and backward processes, as well as for the ELBO.

We complement our theory with a series of experiments to illustrate the viability of FDPs, in SS 6. In our experiments, the score network is a simple multilayer perceptron (MLP), with several orders of magnitude fewer parameters than any existing score-based diffusion model. To the best of our knowledge, we are the first to show that a functional-space diffusion model can generate realistic image data, beyond simple data-sets and toy models.

## 2 Functional Diffusion Processes (FDPs)

We begin by defining diffusion processes in Hilbert Spaces, which we call functional diffusion processes (FDPs). While the study of diffusion processes in Hilbert spaces is not new (Follmer and Wakolbinger, 1986; Millet et al., 1989; Da Prato and Zabczyk, 2014), our objectives depart from prior work, and call for an appropriate treatment of the intricacies of FDPs, when used in the context of generative modeling. In SS 2.1 we introduce a generic class of diffusion processes in Hilbert spaces. The key object is Equation (1), together with its associated path measure \(\mathbb{Q}\) and the time varyingmeasure \(\rho_{t}\), where \(\rho_{0}\) represents the starting (data) measure. In SS 2.2 we derive the reverse FDP with the associated path-reversed measure \(\hat{\mathbb{Q}}\), and in SS 2.3 we use an extension of Girsanov theorem for infinite-dimensional SDEs to compute the ELBO. The ELBO is a training objective involving a generalization of the score function (Song et al., 2021).

### The forward diffusion process

We consider \(H\) to be a real, separable Hilbert space with inner product \(\langle\cdot,\cdot\rangle\), norm \(\left\lVert\cdot\right\rVert_{H}\), and countable orthonormal basis \(\{e^{k}\}_{k=1}^{\infty}\). Let \(L(H)\) be the set of bounded linear operators on \(H\), \(B(H)\) be its Borel \(\sigma-\)algebra, \(B_{b}(H)\) be the set of bounded \(B(H)-\)measurable functions \(H\to\mathbb{R}\), and \(P(H)\) be the set of probability measures on \((H,B(H))\). Consider the following \(H\)-valued SDE:

\[\begin{cases}\mathrm{d}X_{t}=(\mathcal{A}X_{t}+f(X_{t},t))\,\mathrm{d}t+ \mathrm{d}W_{t},\\ X_{0}\sim\rho_{0}\in P(H),\end{cases}\] (1)

where \(t\in[0,T]\), \(W_{t}\) is a \(R-\)Wiener process on \(H\) defined on the quadruplet \((\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathbb{Q})\), and \(\Omega,\mathcal{F}\) are the sample space and canonical filtration, respectively. The domain of \(f\) is \(\bar{D}(f)\in B([0,T]\times H)\), where \(f:D(f)\subset[0,T]\times H\to H\) is a measurable map. The operator \(\mathcal{A}:D(\mathcal{A})\subset H\to H\) is the infinitesimal generator of a \(C_{0}-\) semigroup \(\exp(t\mathcal{A})\) in \(H\)\((t\geq 0)\), and \(\rho_{0}\) is a probability measure in \(H\). We consider \(\Omega\) to be \(C^{1}([0,T])\), that is the space of all continuous mappings \([0,T]\to H\), and \(X_{t}(\omega)=\omega(t),\omega\in\Omega\) to be the _canonical_ process. The requirements on the terms \(\mathcal{A},f\) that ensure existence of solutions to Equation (1) depend on the type of noise -- _trace-class_ (\(\mathrm{Tr}\{R\}<\infty\)) or _cylindrical_ (\(R=I\)) -- used in the FDP (Da Prato & Zabczyk (2014), _Hypothesis 7.1_ or _Hypothesis 7.2_ for trace-class and cylindrical noise, respectively).

The measure associated with Equation (1) is indicated with \(\mathbb{Q}\). The _law_ induced at time \(\tau\in[0,T]\) by the canonical process on the measure \(\mathbb{Q}\) is indicated with \(\rho_{\tau}\in P(H)\), where \(\rho_{\tau}(S)=\mathbb{Q}(\{\omega\in\Omega:X_{\tau}(\omega)\in S\})\), and \(S\) is any element of \(\mathcal{F}\). Notice, that in infinite dimensional spaces there is not an equivalent of the Lebesgue measure to get densities from measures. In our case we consider however, when it exists, the single dimensional density \(\rho_{\tau}^{(d)}(x^{i}|x^{j\neq i})\), defined implicitly through \(\mathrm{d}\rho_{\tau}(x^{i}|x^{j\neq i})=\rho_{\tau}^{(d)}(x^{i}|x^{j\neq i}) \mathrm{d}x^{i}\), being \(\mathrm{d}x^{i}\) the Lebesgue measure. To avoid cluttering the notation, in this work we simply shorten \(\rho_{\tau}^{(d)}(x^{i}|x^{j\neq i})\) with \(\rho_{\tau}^{(d)}(x)\) whenever unambiguous. In Appendix B we provide additional details on the time-varying measure \(\rho_{\epsilon}(\mathrm{d}x)\mathrm{d}t\). Before proceeding, it is useful to notice that Equation (1) can also be expressed as an (infinite) system of stochastic differential equations in terms of \(X_{t}^{k}=\langle X_{t},e^{k}\rangle\) as:

\[\mathrm{d}X_{t}^{k}=b^{k}(X_{t},t)\mathrm{d}t+\mathrm{d}W_{t}^{k},\quad k=1, \ldots,\infty,\] (2)

where we introduced the projection \(b^{k}(X_{t},t)=\langle\mathcal{A}X_{t}+f(X_{t},t),e^{k}\rangle\). Moreover, \(\mathrm{d}W_{t}^{k}=\langle dW_{t},e^{k}\rangle\) with covariance given by \(\mathbb{E}[W_{t}^{k}W_{s}^{j}]=\delta(k-j)r^{k}\min(s,t)\), \(\delta\) in Kroenecker sense, and \(r^{k}\) is the projection on the base of \(R\).

### The reverse diffusion process

We now derive the reverse time dynamics for FDPs of the form defined in Equation (1). We require that the time reversal of the canonical process, \(\hat{X}_{t}=X_{T-t}\), is again a diffusion process, with distribution given by the **path-reversed** measure \(\hat{\mathbb{Q}}(\omega)\), along with the reversed filtration \(\hat{\mathcal{F}}\). Note that the time reversal of an infinite dimensional process is more involved than for the finite dimensional case (Anderson, 1982; Follmer, 1985). There are two major approaches to guarantee the existence of the reverse diffusion process. The first approach (Follmer & Wakolbinger, 1986) is applicable only when \(R=I\) (the case of cylindrical Wiener processes) and it relies on a finite local entropy condition. The second approach, which is valid in the case of trace class noise \(\mathrm{Tr}\{R\}<\infty\), is based on stochastic calculus of variations (Millet et al., 1989). The full technical analysis of the necessary assumptions for the two approaches is involved, and we postpone formal details to Appendix A.

**Theorem 1**.: _Consider Equation (1). If \(R=I\), suppose Assumption 1 in Appendix A.1 holds; else, (\(R\neq I\)) suppose Assumption 5 amd Assumption 6 in Appendix A.2 hold. Then \(\hat{X}_{t}\), corresponding to the path measure \(\hat{\mathbb{Q}}(\omega)\), has the following SDE representation:_\[\begin{cases}\mathrm{d}\hat{X}_{t}=\left(-\mathcal{A}\hat{X}_{t}-f(\hat{X}_{t},T-t)+ RD_{x}\log\rho_{T-t}^{(d)}(\hat{X}_{t})\right)\mathrm{d}t+\mathrm{d}\hat{W}_{t},\\ \hat{X}_{0}\sim\rho_{T},\end{cases}\] (3)

_where \(\hat{W}\) is a \(\hat{\mathbb{Q}}\)\(R-\)Wiener process, and the notation \(D_{x}\log\rho_{t}^{(d)}(x)\) stands for the mapping \(H\to H\) that, when projected, satisfies \(\langle D_{x}\log\rho_{t}^{(d)}(x),e^{k}\rangle=\frac{\partial}{\partial x^{k }}\log\Bigl{(}\rho_{t}^{(d)}(x^{k}\,|\,x^{i\neq k})\Bigr{)}\)._

_By projecting onto the eigenbasis, we have an infinite system of SDEs:_

\[\mathrm{d}\hat{X}_{t}^{k}=\left(-b^{k}(\hat{X}_{t},T-t)+r^{k}\frac{\partial}{ \partial x^{k}}\log\Bigl{(}\rho_{T-t}^{(d)}(\hat{X}_{t})\Bigr{)}\right) \mathrm{d}t+\mathrm{d}\hat{W}_{t}^{k},\quad k=1,\dots,\infty.\] (4)

The methodology proposed in this work requires to operate on proper Wiener processes, with \(\mathrm{Tr}\{R\}<\infty\), which implies, intuitively, that the considered noise has finite variance. We now discuss a Corollary, in which Assumption 5 is replaced by stricter conditions, that we use to check the validity of the practical implementation of FDPs.

**Corollary 1**.: _Suppose Assumption 6 from Appendix A.2 holds. Assume that i) \(\mathrm{Tr}\{R\}=\sum_{i}r^{i}<\infty\), ii) \(b^{i}(x,t)=b^{i}x^{i},\forall i\), i.e. the drift term is linear and only depends on \(x\) through its projection onto the corresponding basis and iii) the drift is bounded, such that \(\exists K>0:-K<b^{i}<0,\forall i\). Then, the reverse process evolves according to Equation (4)._

Theorem 1 stipulates that, given some conditions, the reverse time dynamics for FDPs of the form defined in Equation (1) exist. Our analysis provides theoretical grounding to the observations in concurrent work (Lim et al., 2023) where, empirically, it is observed that the cylindrical class of noise is not suitable. We argue that, when \(R=I\), the difficulty stems from designing the coefficients \(b^{i}\) of the SDEs such that the forward (see requirement (5.3) in Da Prato & Zabczyk (2014)) as well as the backward processes Assumption 1 exist. The work by Bond-Taylor & Willcocks (2023) uses cylindrical (white) noise, but we are not aware of any theoretical justification, since the model architecture is only partially suited for the functional domain.

As an addendum, we note that the advantages of projecting the forward and backward processes on the eigenbasis of the Hilbert space \(H\), as in Equation (2) and Equation (4), become evident when discussing about the implementation of FDPs, specifically when we derive practical expressions for training and the simulation of the backward process, as discussed in SS 5, and in a fully expanded toy example in Appendix D.

### A Girsanov formula for the ELBO

Direct simulation of the backward fdp described by Equation (3) is not possible. Indeed, we have no access to the **true score** of the density \(\rho_{r}^{(d)}\) induced at time \(\tau\in[0,T]\). To solve the problem, we introduce a **parametric score function**\(s_{\boldsymbol{\theta}}:H\times[0,T]\times\mathbb{R}^{m}\to H\). We consider the dynamics:

\[\begin{cases}\mathrm{d}\hat{X}_{t}=\left(-\mathcal{A}\hat{X}_{t}-f(\hat{X}_{t },T-t)+Rs_{\boldsymbol{\theta}}(\hat{X}_{t},T-t)\right)\mathrm{d}t+\mathrm{d} \tilde{W}_{t},\\ \hat{X}_{0}\sim\chi_{T}\in P(H),\end{cases}\] (5)

with path measure \(\hat{\mathbb{P}}^{\chi_{T}}\), and \(\mathrm{d}\tilde{W}_{t}\) being a \(\hat{\mathbb{P}}^{\chi_{T}}\)\(R-\)Wiener process. To emphasize the connection between Equation (3) and Equation (5), we define initial conditions with the subscript \(T\), instead of 0. In principle, we should have \(\chi_{T}=\rho_{T}\), as it will be evident from the ELBO in Equation (8). However, \(\rho_{T}\) has a simple and easy-to-sample-from distribution only for \(T\to\infty\), which is not compatible with a realistic implementation. The analysis of the discrepancy when \(T\) is finite is outside of the scope of this work, and the interested reader can refer to Franzese et al. (2022) for an analysis on standard diffusion models. The final measure of the new process at time \(T\) is indicated by \(\chi_{0}\), i.e. \(\chi_{0}(S)=\hat{\mathbb{P}}^{\chi_{T}}(\{\omega\in\Omega:\hat{X}_{T}(\omega) \in S\})\).

Next, we quantify the discrepancy between \(\chi_{0}\) and the true data measure \(\rho_{0}\) through an ELBO. Thanks to an extension of Girsanov theorem to infinite dimensional SDEs(Da Prato & Zabczyk, 2014), it is possible to relate the path measures (\(\hat{\mathbb{Q}}\) and \(\hat{\mathbb{P}}^{\chi_{T}}\), respectively) of the process \(\hat{X}_{t}\) induced by different drift terms in Equation (3) and different initial conditions.

Starting from the score function \(s_{\bm{\theta}}\), we define:

\[\gamma_{\bm{\theta}}(x,t)=R\left(s_{\bm{\theta}}(x,T-t)-D_{x}\log\rho_{T-t}^{(d)} (x)\right).\] (6)

Under loose regularity assumptions (see Condition 2 in Appendix A.4) \(\tilde{W}_{t}=\hat{W}_{t}-\int\limits_{0}^{t}\gamma_{\bm{\theta}}(X_{s},s)ds\) is a \(\hat{\mathbb{P}}^{\rho_{T}}\)\(R-\)Wiener process (Theorem 10.14 in Da Prato & Zabczyk (2014)), where Girsanov Theorem also tells us that the measure \(\hat{\mathbb{P}}^{\rho_{T}}\) satisfies the Radon-Nikodym derivative:

\[\frac{\mathrm{d}\hat{\mathbb{P}}^{\rho_{T}}}{\mathrm{d}\hat{\mathbb{Q}}}=\exp \left(\int\limits_{0}^{T}\langle\gamma_{\bm{\theta}}(\hat{X}_{t},t),\mathrm{d} \hat{W}_{t}\rangle_{R^{\frac{1}{2}}H}-\frac{1}{2}\int\limits_{0}^{T}\left\| \gamma_{\bm{\theta}}(\hat{X}_{t},t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t \right).\] (7)

By virtue of the disintegration theorem, \(\mathrm{d}\hat{\mathbb{Q}}=\mathrm{d}\hat{\mathbb{Q}}_{0}\mathrm{d}\rho_{T}\) and similarly \(\mathrm{d}\hat{\mathbb{P}}^{\rho_{T}}=\mathrm{d}\hat{\mathbb{P}}_{0}\mathrm{d }\rho_{T}\), being \(\hat{\mathbb{Q}}_{0},\hat{\mathbb{P}}_{0}\) the measures of the processes when considering a particular initial value. Then, \(\hat{\mathbb{P}}^{\chi_{T}}\) satisfies \(\mathrm{d}\hat{\mathbb{P}}^{\chi_{T}}=\mathrm{d}\hat{\mathbb{P}}^{\frac{1}{2} \mathrm{d}\chi_{T}}\), for any measure \(\chi_{T}\in P(H)\). Consequently, the canonical process \(\hat{X}_{t}\) has an SDE representation according to Equation (5), under the new path measure \(\hat{\mathbb{P}}^{\chi_{T}}\). Then (see Appendix A.5 for the derivation) we obtain the elbo:

\[\text{KL}\left[\rho_{0}\parallel\chi_{0}\right]\leq\frac{1}{2}\mathbb{E}_{ \mathbb{Q}}\left[\int\limits_{0}^{T}\left\|\gamma_{\bm{\theta}}(X_{t},t)\right\| _{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right]+\text{KL}\left[\rho_{T}\parallel \chi_{T}\right].\] (8)

Provided that the required assumptions in Theorem 1 are met, the validity of Equation (8) is general. Our goal, however, is to set the stage for a practical implementation of fdps, which calls for design choices that easily enable satisfying the required assumptions for the theory to hold. Then, for the remainder of the paper, we consider the particular case where \(f=0\) in Equation (1). This simplifies the dynamics as follows:

\[\mathrm{d}X_{t} =\mathcal{A}X_{t}\mathrm{d}t+\mathrm{d}W_{t},\quad X_{0}\sim\rho_ {0}\in P(H)\] (9) \[\mathrm{d}\hat{X}_{t} =\left(-\mathcal{A}\hat{X}_{t}+Rs_{\bm{\theta}}(\hat{X}_{t},T-t) \right)\mathrm{d}t+\mathrm{d}\tilde{W}_{t},\quad\hat{X}_{0}\sim\chi_{T}\in P(H)\] (10)

Since the only drift component in Equation (9) is the linear term \(\mathcal{A}\), the projection \(b^{j}\) will be linear as well. Such a design choice, although not necessary from a theoretical point of view, carries several advantages. The design of a drift term satisfying the conditions of Corollary 1 becomes straightforward, where such conditions naturally aligns with the requirements of the existence of the forward process (Chapter 5 of Da Prato & Zabczyk (2014)). Moreover, the forward process conditioned on given initial conditions admits known solutions, which means that simulation of SDE paths is cheap and straightforward, without the need for performing full numerical integration. Finally, it is possible to claim existence of the **true score function** and even provide its analytic expression (full derivation in Appendix A.7) as:

\[D_{x}\log\rho_{t}^{(d)}(x)=-\mathcal{S}(t)^{-1}\left(x-\exp(t\mathcal{A}) \mathbb{E}\left[X_{0}\,|\,X_{t}=x\right]\right),\] (11)

where \(\mathcal{S}(t)=\left(\int\limits_{s=0}^{t}\exp((t-s)\mathcal{A})R\exp((t-s) \mathcal{A}^{\dagger})\mathrm{d}s\right)\). This last aspect is particularly useful when considering the conditional version of Equation (6), through \(\langle D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0}),e^{k}\rangle=\frac{\partial}{ \partial x^{k}}\log\Bigl{(}\rho_{t}^{(d)}(x^{k}\,|\,x^{i\neq k},x_{0})\Bigr{)}\), as:

\[\tilde{\gamma}_{\bm{\theta}}(x,x_{0},t)=R\left(s_{\bm{\theta}}(x,T-t)-D_{x} \log\rho_{T-t}^{(d)}(x\,|\,x_{0})\right),\] (12)

where, similarly to the unconditional case, we have \(D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})=-\mathcal{S}(t)^{-1}\left(x-\exp(t\mathcal{ A})x_{0}\right)\). Then, Equation (12) can be used to rewrite Equation (8):

\[\mathbb{E}_{\mathbb{Q}}\left[\int\limits_{0}^{T}\left\|\gamma_{\bm{\theta}}(X_{ t},t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right]=\mathbb{E}_{\mathbb{Q}} \left[\int\limits_{0}^{T}\left\|\tilde{\gamma}_{\bm{\theta}}(X_{t},X_{0},t) \right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right]+I,\] (13)

where \(I\) is a quantity independent of \(\bm{\theta}\). Knowledge of the conditional true score \(D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})\) and cheap simulation of the forward dynamics, allows for easier numerical optimization than the more general case of \(f\neq 0\).

Sampling theorem for FDPs

The theory of FDPs developed so far is valid for real, separable Hilbert spaces. Our goal now is to specify for which subclass of functions it is possible to perfectly reconstruct the original function given only its evaluation in a countable set of points. We present a generalization of the sampling theorem (Shannon, 1949), which allows us to move from generic Hilbert spaces to a domain which is amenable to a practical implementation of FDPs, and their application to common functional representation of data such as images, data on manifolds, and more. We model these functions as objects belonging to the set of square integrable functions over \(C^{\infty}\)_homogeneous_ manifolds \(M\) (such as \(\mathbb{R}^{N},\mathbb{S}^{N}\), etc...), i.e., the Hilbert space \(H=L_{2}(M)\). Then, exact reconstruction implies that all the relevant information about the considered functions is contained in the set of sampled points.

First, we define functions that are _band-limited_:

**Definition 1**.: _A function \(x\) in \(H=L_{2}(M)\) is a spectral entire function of exponential type \(\nu\) (SE-\(\nu\)) if \(|\Delta^{\frac{k}{2}}x|\leq\nu^{k}|x|,k\in\mathbb{N}\). Informally, the "Fourier Transform" of \(x\) is contained in the interval \([0,\nu]\)(Pesenson, 2000)._

Second, we define grids that cover the manifold with balls, without too much overlap. Those grids will be used to collect the function samples. Their formal definition is as follows:

**Definition 2**.: \(Y(r,\lambda)\) _denotes the set of all sets of points \(Z=\{p_{i}\}\) such that: i) \(\inf_{j\neq i}\text{dist}(p_{j},p_{i})>0\) and ii) balls \(B(p_{i},\lambda)\) form a cover of \(M\) with multiplicity \(<r\)._

Combining the two definitions, we can state the key result of this Section. As long as the sampled function is band-limited, if the samples grid is sufficiently fine, exact reconstruction is possible:

**Theorem 2**.: _For any set \(Z\in Y(r,\lambda)\), any SE-\(\nu\) function \(x\) is uniquely determined by its set of values in \(Z\) (i.e. \(\{x[p_{i}]\}\)) as long as \(\lambda<d\), that is_

\[x=\sum_{p_{i}\in Z}x[p_{i}]m_{p_{i}},\] (14)

_where \(m_{p_{i}}:M\to H\) are known polynomials2, and the notation \(x[p]\) indicates that the function \(x\) is evaluated at point \(p\)._

Footnote 2: Precisely, they are the limits of spline polynomials that form a Riesz basis for the Hilbert space of polyharmonic functions with singularities in \(Z\)(Pesenson, 2000).

A precise definition of the value of the constant \(d\) and its interpretation is outside the goal of this work, and we refer the interested reader to Pesenson (2000) for additional details. For our purposes, it is sufficient to interpret the condition in Theorem 2 as a generalization of the classical Shannon-Nyquist sampling theorem (Shannon, 1949). Under this light, Theorem 2 has practical relevance, because it gives the conditions for which the sampled version of functions contains all the information of the original functions. Indeed, given the set of points \(p_{i}\) on which function \(x\) is evaluated, it is possible to reconstruct exactly \(x[p]\) for arbitrary \(p\).

**The uncertainty principle.** It is not always possible to define Hilbert spaces of square integrable functions that are simultaneously homogeneous and separable, for all the manifolds \(M\) of interest. In other words, it is difficult in practice to satisfy both the requirements for FDPs to exist, and for the sampling theorem to be valid (see an example in Appendix C). Nevertheless, it is possible to quantify the reconstruction error, and realize that practical applications of FDPs are viable. Indeed, given a compactly supported function \(x\), and a set of points \(Z\) with _finite_ cardinality, we can upper-bound the reconstruction error \(\left\|\sum_{p_{i}\in Z}x[p_{i}]m_{p_{i}}-x\right\|_{H}\) with:

\[\underbrace{\left\|\sum_{p_{i}\in Z}\left(x[p_{i}]-x^{\nu}[p_{i}]\right)m_{p_ {i}}\right\|_{H}}_{\epsilon_{1}}+\underbrace{\left\|\sum_{p_{i}\in Z}x^{\nu}[p _{i}]m_{p_{i}}-x^{\nu}\right\|_{H}}_{\epsilon_{2}}+\underbrace{\left\|x^{\nu}- x\right\|_{H}}_{\epsilon_{3}}=\epsilon,\] (15)

where \(x^{\nu}\) is the SE-\(\nu\) bandlimited version of \(x\), obtained by filtering out - in the frequency domain - any component larger than \(\nu\). The error \(\epsilon_{1}\) is due to \(x\neq x^{\nu}\). The term \(\epsilon_{2}\) is the reconstruction error due to finiteness of \(|Z|\): the sampling theorem applies to \(x^{\nu}\), but the corresponding sampling grid has infinite cardinality. Finally, the term \(\epsilon_{3}\) quantifies the energy omitted by filtering out the frequency components of \(x^{\nu}\) larger than \(\nu\). This (loose) upper bound allows us to understand quantitatively the degree to which the sampling theorem does not apply for the cases of interest. Although deriving tighter bounds is possible, this is outside the scope of this work. What suffices is that in many practical cases, when functions are obtained from natural sources, it has been observed that functions are nearly time and bandwidth limited (Slepian, 1983). Consequently, as long as the sampling grid is sufficiently fine, the reconstruction error \(\epsilon\) is negligible.

We now hold all the ingredients to formulate generative functional diffusion models using the Hilbert space formalism and _implement_ them using a finite grid of points, which is what we do next.

## 4 Score Network Architectural Implementations

We are now equipped with the ELBO (Equation (8)) and a score function \(s_{\boldsymbol{\theta}}\) that implements the mapping \(H\times[0,T]\times\mathbb{R}^{m}\to H\). We could then train the score by optimizing the ELBO and produce samples arbitrary close to the true data measure \(\rho_{0}\). However, since the domain of the score function is the infinite-dimensional Hilbert space, such a mapping cannot be implemented in practice. Indeed, having access to samples of functions on finite grid of points is, in general, not sufficient. However, when the conditions for Theorem 2 hold, we can substitute - with no information loss - \(x\in H\) with its collection of samples \(\{x[p_{i}],p_{i}\}\). This allows considering score network architectures that receive as input a collection of points, and not _abstract_ functions. Such architectures should be flexible enough to work with an arbitrary number of input samples at arbitrary grid points, and produce as outputs functions in \(H\).

### Implicit Neural Representation

The first approach we consider in this work is based on the idea of Implicit Neural Representations (INRs) (Sitzmann et al., 2020). These architectures can receive as inputs functions sampled at arbitrary, possibly irregular points, and produce output functions evaluated at any desired point. Unfortunately, the encoding of the inputs is not as straightforward as in the Neural Fourier Operator (NFO) case, and some form of autoencoding is necessary. Note, however, that in traditional score-based diffusion models (Song et al., 2021), the parametric score function can be thought of as a denoising autoencoder. This is a valid interpretation also in our case, as it is evident by observing the term \(\mathbb{E}\left[X_{0}\,|\,X_{t}=x\right]\) of the true score function in Equation (11). Since INRs are powerful denoisers (Kim et al., 2022), combined with their simple design and small number of parameters, in this Section we discuss how to implement the score network of FDPs using INRs.

We define a _valid_ INR as a parametric family (\(\boldsymbol{\psi},t,\boldsymbol{\theta}\)) of functions in \(H\), i.e., mappings \(\mathbb{R}^{m}\times[0,T]\times\mathbb{R}^{m}\to H\). A valid INR is the central building block for the implementation of the parametric score function, and it relies on two sets of parameters: \(\boldsymbol{\theta}\), which are the parameters of the score function that we optimize according to Equation (8), and \(\boldsymbol{\psi}\), which serve the purpose of building a mapping from \(H\) into a finite dimensional space. More formally:

**Definition 3**.: _Given a manifold \(M\), a valid Implicit Neural Representation (INr) is an element of \(H\) defined by a family of parametric mappings \(n(\boldsymbol{\psi},t,\boldsymbol{\theta})\), with \(t\in[0,T],\boldsymbol{\theta},\boldsymbol{\psi}\in\mathbb{R}^{m}\). That is, for \(p\in M\), we have \(n(\boldsymbol{\psi},t,\boldsymbol{\theta})[p]\in\mathbb{R}\). Moreover, we require \(n(\boldsymbol{\psi},t,\boldsymbol{\theta})\in L_{2}(M)\)._

A valid INR as defined in Definition 3 is not sufficiently flexible to implement the parametric score function \(s_{\boldsymbol{\theta}}\), as it cannot accept input elements from the infinite-dimensional Hilbert space \(H\): indeed, the score function is defined as a mapping over \(H\times[0,T]\times\mathbb{R}^{m}\to H\), whereas the valid INR is a mapping defined over \(\mathbb{R}^{m}\times[0,T]\times\mathbb{R}^{m}\to H\). Then, we use the second set of parameters \(\boldsymbol{\psi}\) to condensate all the information of a generic \(x\in H\) into a finite-dimensional vector. When the conditions for Theorem 2 hold, we can substitute -- with no information loss -- \(x\in H\) with its collection of samples \(\{x[p_{i}],p_{i}\}\). Then, we can construct an implicitly defined mapping \(g:H\times[0,T]\times\mathbb{R}^{m}\to\mathbb{R}^{m}\) as:

\[g(\{x[p_{i}],p_{i}\},t,\boldsymbol{\theta})=\arg\min_{\boldsymbol{\psi}}\sum_ {p_{i}}\left(n(\boldsymbol{\psi},t,\boldsymbol{\theta})[p_{i}]-x[p_{i}]\right)^ {2}.\] (16)

In this work, we consider the _modulation_ approach to INRs. The set of parameters \(\boldsymbol{\psi}\) are obtained by minimizing Equation (16) using few steps of gradient descent on the objective \(\sum_{p_{i}}\left(n(\bm{\psi},t,\bm{\theta})[p_{i}]-x[p_{i}]\right)^{2}\), starting from the zero initialization of \(\bm{\psi}\). This approach, also explored by Dupont et al. (2022b), is based on the concept of meta-learning (Finn et al., 2017). In summary, our method constructs mappings \(H\times[0,T]\times\mathbb{R}^{m}\to H\), where the same INR is used first to encode \(x\) into \(\bm{\psi}\), and subsequently to output the value functions for any desired input point \(p\), thus implementing the following score network:

\[s_{\bm{\theta}}(x,t)=-(\mathcal{S}(t))^{-1}\left(x-\exp(t\mathcal{A})n(g(\{x[p _{i}],p_{i}\},t,\bm{\theta}),t,\bm{\theta})\right).\] (17)

### Transformers

As an alternative approach, we consider implementing the score function \(s_{\bm{\theta}}\) using transformer architectures Vaswani et al. (2017), by interpreting them as mappings between Hilbert spaces (Cao, 2021). We briefly summarize here such a perspective, focusing on a single attention layer for simplicity, and adapt the notation used throughout the paper accordingly.

Consider the space \(L_{2}(M)\), with the usual collection of samples \(\{x[p_{i}],p_{i}\}\). As a first step, both the "_features_" \(\{x[p_{i}]\}\) and positions \(\{p_{i}\}\) are embedded into some higher dimensional space and summed together, to obtain a sequence of vectors \(\{y_{i}\}\). Then, three different (learnable) matrices \(\theta^{(Q)},\theta^{(K)},\theta^{(V)}\) are used to construct the linear transformations of the vector sequence \(\{y_{i}\}\) as \(\hat{Y}^{(Q)}=\{\hat{y}_{i}{}^{(Q)}=\theta^{(Q)}y_{i}\},\hat{Y}^{(K)}=\{\hat{ y}_{i}{}^{(K)}=\theta^{(K)}y_{i}\},\hat{Y}^{(V)}=\{\hat{y}_{i}{}^{(V)}= \theta^{(V)}y_{i}\}\). Finally, the three matrices \(\hat{Y}^{(Q,K,V)}\) are multiplied together, according to any variant of the attention mechanism. Indeed, different choices for the order of multiplication and normalization schemes in the products and in the matrices correspond to different attention layers Vaswani et al. (2017). In practical implementations, these operations can be repeated multiple times (multiple attention layers) and can be done in parallel according to multiple projection matrices (multiple heads).

The perspective explored in (Cao, 2021) is that it is possible to interpret the sequences \(\hat{y_{i}}{}^{(Q,K,V)}\) as **learnable** basis functions in some underlying _latent_ Hilbert space, evaluated at the set of coordinates \(\{p_{i}\}\). Furthermore, depending on the type of attention mechanism selected, the operation can be interpreted as a different mapping between Hilbert spaces, such as Fredholm equations of the second-kind or Petrov-Galerkin-type projections (Cao (2021) Eqs. 9 and 14).

While a complete treatment of such an interpretation is outside the scope of this work, what suffices is that it is possible to claim that transformer architectures are a viable candidate for the implementation of the desired mapping \(H\times[0,T]\times\mathbb{R}^{m}\to H\), a possibility that we explore experimentally in this work. It is worth noticing that, compared to the approach based on INRs, resolution invariance is only _learned_, and not guaranteed, and that the number of parameters is generally higher compared to an INR. Nevertheless, learning the parameters of transformer architectures does not require meta-learning, which is a practical pain-point of INRs used in our context. Additional details for the transformer-based implementation of the score network are available in Appendix E.

Finally, for completeness, it is worth mentioning that a related class of architectures, the Neural Operators and NFos(Kovachki et al., 2021; Li et al., 2020), are also valid alternatives. However, such architectures require the input grid to be regularly spaced (Li et al., 2020), and their output function is available only at the same points \(p_{i}\) of the input, which would reduce the flexibility of FDPs.

## 5 Training and sampling of FDPs

Given the parametric score function \(s_{\bm{\theta}}\) from Equation (17), by simulating the reverse FDP, we generate samples whose statistical measure \(\chi_{0}\) is close in KL sense to \(\rho_{0}\). Next, we explain how to numerically compute of the quantities in Equation (13), which is part of the ELBO in Equation (8), and how to generate new samples from the trained FDP (simulation of Equation (10)).

**ELBO Computation.** Equation (8) involves Equation (13), which requires the computation of the Hilbert space norm. The grid of points \(x[p_{i}]\) is interpolated in \(H\) as \(\sum_{i}x[p_{i}]\xi^{i}\). Then, the norm of interest can be computed as:

\[\left\|\sum_{i}x[p_{i}]\xi^{i}\right\|_{R^{\frac{1}{2}}H}^{2}=\langle R^{-\frac {1}{2}}\sum_{i}x[p_{i}]\xi^{i},R^{-\frac{1}{2}}\sum_{i}x[p_{i}]\xi^{i}\rangle_ {H}=\sum_{k=1}^{\infty}(r^{k})^{-1}\left(\left\langle\sum_{i=1}^{N}x[p_{i}] \xi^{i},e^{k}\right\rangle\right)^{2}.\] (18)Depending on the choice of \(\xi^{i},e^{i}\), the sum w.r.t the index \(k\) is either naturally truncated or it needs to be further approximated by selecting a cutoff index value. Finally, training can then be performed by minimizing:

\[\mathbb{E}_{\mathbb{Q}}\left[\int\limits_{0}^{T}\|\tilde{\gamma}_{\bm{\theta}}(X_ {t},t)\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right]\simeq\mathbb{E}_{\sim(20)} \left[\int\limits_{0}^{T}\sum_{k=1}^{\infty}(r^{k})^{-1}\left(\left\langle\sum _{i=1}^{N}\left(\tilde{\gamma}_{\bm{\theta}}(\sum_{i}X_{t}[p_{i}]\xi^{i},t)[p_{ i}]\right)\xi^{i},e^{k}\right\rangle\right)^{2}\mathrm{d}t\right].\] (19)

**Numerical integration.** Simulation of infinite dimensional SDEs is a well studied domain (Debussche, 2011), including finite difference schemes (Gyongy, 1998, 1999; Yoo, 2000), finite element methods and/or Galerkin schemes (Hausenblas, 2003a,b; Shardlow, 1999). In this work, we adopt a finite element approximate scheme, and introduce the _interpolation_ operator, from \(\mathbb{R}^{|Z|}\) to \(H\), i.e. \(\sum_{i}x[p_{i}]\xi^{i}\)(Hausenblas, 2003b). Notice that, in general, the functions \(\xi^{i}\) differ from the basis \(e^{i}\). In addition, the _projection_ operator maps functions from \(H\) into \(\mathbb{R}^{L}\), as \(\langle x,\zeta^{j}\rangle,\zeta^{j}\in H\). Usually, \(L=|Z|\). When \(\zeta^{i}=\xi^{i}\) the scheme is referred to as the Galerkin scheme. We consider instead a point matching scheme (Hausenblas, 2003b), in which \(\zeta^{i}=\delta[p-p_{i}]\) with \(\delta\) in Dirac sense, and consequently \(\langle x,\zeta^{i}\rangle=x[p_{i}]\). Then, the infinite dimensional SDE of the forward process from Equation (9) is approximated by the finite (\(|Z|\)) dimensional SDE:

\[\mathrm{d}X_{t}[p_{k}]=\left(\left\langle\mathcal{A}\sum_{i}X_{t}[p_{i}]\xi^ {i},\zeta^{k}\right\rangle\right)\mathrm{d}t+\mathrm{d}W_{t}[p_{k}],\quad k=1, \ldots,|Z|.\] (20)

Similarly, the reverse process described by Equation (10) corresponds to the following SDE:

\[\mathrm{d}\hat{X}_{t}[p_{k}]=\left(-\left\langle\mathcal{A}\sum _{i}\hat{X}_{t}[p_{i}]\xi^{i},\zeta^{k}\right\rangle+\left\langle Rs_{\bm{ \theta}}(\sum_{i}\hat{X}_{t}\left[p_{i}\right]\xi^{i},T-t),\zeta^{k}\right\rangle \right)\mathrm{d}t+\mathrm{d}\hat{W}_{t}[p_{k}],\] (21) \[k=1,\ldots,|Z|.\]

Equation (21) is a finite dimensional SDE, and consequently we can use any known numerical integrator to simulate its paths. In Appendix D we provide a complete toy example to illustrate our approach in a simple scenario, where we emphasize practical choices.

## 6 Experiments

Despite a rather involved theoretical treatment, the implementation of fdps is simple. We implemented our approach in JaX (Bradbury et al., 2018), and use WanDB(Biewald, 2020) for our experimental protocol. Additional details on implementation, and experimental setup, as well as more experiments are available in Appendix E.

We evaluate our approach on image data, using the CELEBA\(64\times 64\)(Liu et al., 2015) dataset. Our comparative analysis with the state-of-the-art includes generative quality, using the FID score (Heusel et al., 2017), and parameter count for the score network. We also discuss (informally) the complexity of the network architecture, as a measure of the engineering effort in exploring the design space of the score network. We compare against vanilla Score Based Diffusion (SBD) (Song et al., 2021), From Data To Functa (FD2F) (Dupont et al., 2022) which diffuses latent variables obtained from an INR, Infinite Diffusion (\(\infty\)-DIFF) (Bond-Taylor and Willcocks, 2023), which is a recent approach that is only partially suited for the functional domain, as it relies on the combination of Fourier Neural Operators and a classical convolutional U-net backbone. Our FDP method is implemented using either MLP or Transformers. In the first case, we consider a score network implemented as a simple MLP with 15 layers and 256 neurons in each layer. The activation function is a Gabor wavelet activation function (Saragadam et al., 2023). In the latter case, our approach is built upon the UViT backbone as detailed by Bao et al. (2022). The architecture comprises 7 layers, with each layer composed of a self-attention mechanism with 8 attention heads and a feedforward layer.

We present quantitative results in Table 1, showing that our method **FDP(MLP)** achieves an impressively low FID score, given the extremely low parameter count, and the simplicity of the architecture. fd2F obtains a worse (larger) FID score, while having many more parameters, due to the complex parametrization of their score network. As a reference we report the results of SBD, where the price to be pay to achieve an extremely low FID is to have many more parameters and a much more intricate architecture. Finally, the very recent \(\infty\)-DIFF method, has low FID-CLIP score (Kynkaanniemi et al., 2022), but requires a very complex architecture and more than 2 orders of magnitude more parameters than our approach. Showcasing the flexibility of the proposed methodology, we consider a more complex architecture based on Vision Transformers (**FDP(UViT)**). These corresponding results indicate improvements in terms of image quality (FID score=11) and do not require meta-learning steps, but require more parameters (O(20M)) than the 1NR variant. To the best of our knowledge, none of related work in the purely functional domain (Lim et al., 2023; Hagemann et al., 2023; Dutordoir et al., 2022; Kerrigan et al., 2022) provides results going beyond simple data-sets. Finally, we present some qualitative results in Figures 2 and 2 clearly showing that the proposed methodology is capable of producing diverse and detailed images.

## 7 Conclusion, Limitations and Broader Impact

We presented a theoretical framework to define functional diffusion processes for generative modeling. FDPs generalize traditional score-based diffusion models to infinite-dimensional function spaces, and in this context we were the first to provide a full characterization of forward and backward dynamics, together with a formal derivation of an ELBO that allowed the estimation of the parametric score function driving the reverse dynamics.

To use FDPs in practice, we carefully studied for which subset of functions it was possible to operate on a countable set of samples without losing information. We then proceeded to introduce a series of methods to jointly model - using only a simple INR or a Transformer - an approximate functional representation of data on discrete grids, and an approximate score function. Additionally, we detailed practical training procedures of FDPs, and integration schemes to generate new samples.

The implementation of FDPs for generative modeling was simple. We validated the viability of FDPs through a series of experiments on real images, where we show, while only using a simple MLP for learning the score network, extremely promising results in terms of generation quality.

Like other works in the literature, the proposed method can have both positive (e.g., synthesizing new data automatically) and negative (e.g., deep fakes) impacts on society depending on the application.

## 8 Acknowledgments

GF gratefully acknowledges support from Huawei Paris and the European Commission (ADROIT6G Grant agreement ID: 101095363). MF gratefully acknowledges support from the AXA Research Fund and the Agence Nationale de la Recherche (grant ANR-18-CE46-0002 and ANR-19-P3IA-0002).

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Methods** & **FID (\(\downarrow\))** & **FID-CLIP (\(\downarrow\))** & **Params** \\ \hline \hline
**FDP(MLP)** & 35.00 & 12.44 & \(O\)(1 M) \\ \hline
**FDP(UViT)** & 11.00 & 6.55 & \(O\)(20 M) \\ \hline FD2F & 40.40 & - & \(O\)(10 M) \\ \hline SBD & 3.30 & - & \(O\)(100 M) \\ \hline \(\infty\)-DIFF & - & 4.57 & \(O\)(100 M) \\ \hline \end{tabular}
\end{table}
Table 1: Quantitative results, CELEBA data-set. (FID-CLIP (Kynkaanniemi et al., 2022))

## References

* Anderson (1982) Brian D. O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* Bansal et al. (2022) Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. _arXiv preprint arXiv:2208.09392_, 2022.
* Bao et al. (2022) Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. _arXiv preprint arXiv:2209.12152_, 2022.
* Biewald (2020) Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com.
* Bogachev et al. (2009) Vladimir Bogachev, Giuseppe Da Prato, and Michael Rockner. Existence and uniqueness of solutions for fokker-planck equations on hilbert spaces. _J. Evol. Equ._, 10, 07 2009.
* Bogachev et al. (2011) Vladimir Bogachev, Giuseppe Da Prato, and Michael Rockner. Uniqueness for solutions of fokker-planck equations on infinite dimensional spaces. _Communications in Partial Differential Equations_, 36(6):925-939, 2011.
* Bond-Taylor and Willcocks (2023) Sam Bond-Taylor and Chris G Willcocks. \(\infty\)-diff: Infinite resolution diffusion with subsampled mollified states. _arXiv preprint arXiv:2303.18242_, 2023.
* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. Jax: composable transformations of python+ numpy programs. 5:14-24, 2018.
* Cao (2021) Shuhao Cao. Choose a transformer: Fourier or galerkin. _Advances in neural information processing systems_, 34:24924-24940, 2021.
* Da Prato and Zabczyk (2014) Giuseppe Da Prato and Jerzy Zabczyk. _Stochastic equations in infinite dimensions_. Cambridge university press, 2014.
* Debussche (2011) Arnaud Debussche. Weak approximation of stochastic partial differential equations: the nonlinear case. _Mathematics of Computation_, 80(273):89-117, 2011.
* Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 8780-8794. Curran Associates, Inc., 2021.
* Dockhorn et al. (2022) Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. In _International Conference on Learning Representations_, 2022.
* Dupont et al. (2022a) Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5694-5725. PMLR, 17-23 Jul 2022a.
* Dupont et al. (2022b) Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and Arnaud Doucet. COIN++: Neural compression across modalities. _Transactions of Machine Learning Research_, 2022b.
* Dutordoir et al. (2022) Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. _arXiv preprint arXiv:2206.03992_, 2022.
* Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pp. 1126-1135. PMLR, 2017.
* Follmer (1985) Hans Follmer. An entropy approach to the time reversal of diffusion processes. In _Stochastic Differential Systems Filtering and Control_, pp. 156-163. Springer, 1985.
* Follmer et al. (2018)Hans Follmer. Time reversal on wiener space. In _Stochastic processes--mathematics and physics_, pp. 119-129. Springer, 1986.
* Franzese et al. [2022] Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, and Pietro Michiardi. How much is enough? a study on diffusion times in score-based generative models. _arXiv preprint arXiv:2206.05173_, 2022.
* Follmer and Wakolbinger [1986] H. Follmer and A. Wakolbinger. Time reversal of infinite-dimensional diffusions. _Stochastic Processes and their Applications_, 22(1):59-77, 1986. ISSN 0304-4149. doi: https://doi.org/10.1016/0304-4149(86)90114-6. URL https://www.sciencedirect.com/science/article/pii/0304414986901146.
* Gyongy [1998] Istvan Gyongy. Lattice approximations for stochastic quasi-linear parabolic partial differential equations driven by space-time white noise i. _Potential Analysis_, 9(1):1-25, 1998.
* Gyongy [1999] Istvan Gyongy. Lattice approximations for stochastic quasi-linear parabolic partial differential equations driven by space-time white noise ii. _Potential Analysis_, 11(1):1-37, 1999.
* Hagemann et al. [2023] Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang. Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation, 2023.
* Hausenblas [2003a] Erika Hausenblas. Approximation for semilinear stochastic evolution equations. _Potential Analysis_, 18(2):141-186, 2003a.
* Hausenblas [2003b] Erika Hausenblas. Weak approximation for semilinear stochastic evolution equations. In _Stochastic analysis and related topics VIII_, pp. 111-128. Springer, 2003b.
* He et al. [2022] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.
* Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, pp. 6629-6640, 2017.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 6840-6851. Curran Associates, Inc., 2020.
* Ho et al. [2021] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation, 2021. URL https://arxiv.org/abs/2106.15282.
* Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022. URL https://arxiv.org/abs/2210.02303.
* Hoogeboom et al. [2022] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 8867-8887. PMLR, 17-23 Jul 2022.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022.
* Kerrigan et al. [2022] Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions, 2022. URL https://arxiv.org/abs/2212.00886.
* Kim et al. [2022a] Chaewon Kim, Jaeho Lee, and Jinwoo Shin. Zero-shot blind image denoising via implicit neural representations. _arXiv preprint arXiv:2204.02405_, 2022a.
* Krizhevsky et al. [2014]Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 11201-11228. PMLR, 17-23 Jul 2022b.
* Kingma et al. (2021) Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=2LdBqxc1Yv.
* Kong et al. (2021) Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In _International Conference on Learning Representations_, 2021.
* Kovachki et al. (2021) Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021.
* Kynkaanniemi et al. (2022) Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. _arXiv preprint arXiv:2203.06026_, 2022.
* LeCun et al. (2010) Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* Li et al. (2020) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* Lim et al. (2023) Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion models in function space. _arXiv preprint arXiv:2302.07400_, 2023.
* Liu et al. (2022) Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow diffusion mechanism. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(10):11020-11028, Jun. 2022.
* Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Luo and Hu (2021) Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 2837-2845, June 2021.
* Millet et al. (1989) Annie Millet, David Nualart, and Marta Sanz. Time reversal for infinite-dimensional diffusions. _Probability theory and related fields_, 82(3):315-347, 1989.
* Mittal et al. (2022) Sarthak Mittal, Guillaume Lajoie, Stefan Bauer, and Arash Mehrjou. From points to functions: Infinite-dimensional representations in diffusion models. In _ICLR Workshop on Deep Generative Models for Highly Structured Data_, 2022.
* Pesenson (2000) Isaac Pesenson. A sampling theorem on homogeneous manifolds. _Transactions of the American Mathematical Society_, 352(9):4257-4269, 2000.
* Phillips et al. (2022) Angus Phillips, Thomas Seror, Michael Hutchinson, Valentin De Bortoli, Arnaud Doucet, and Emile Mathieu. Spectral diffusion processes. _arXiv preprint arXiv:2209.14125_, 2022.
* Pidstrigach et al. (2023) Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, and Sven Wang. Infinite-dimensional diffusion models for function spaces. _arXiv preprint arXiv: 2302.10130_, 2023.
* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. URL https://arxiv.org/abs/2204.06125.
* Ramesh et al. (2020)* Rissanen et al. [2022] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. _arXiv preprint arXiv:2206.13397_, 2022.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022.
* Salimans and Ho [2022] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations_, 2022.
* Saragadam et al. [2023] Vishwanath Saragadam, Daniel Leleune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, and Richard G Baraniuk. Wire: Wavelet implicit neural representations. _arXiv preprint arXiv:2301.05187_, 2023.
* Shannon [1949] C.E. Shannon. Communication in the presence of noise. _Proceedings of the IRE_, 37(1):10-21, 1949.
* Shardlow [1999] Tony Shardlow. Numerical methods for stochastic parabolic pdes. _Numerical functional analysis and optimization_, 20(1-2):121-145, 1999.
* Sitzmann et al. [2020] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 7462-7473. Curran Associates, Inc., 2020.
* Slepian [1983] David Slepian. Some comments on fourier analysis, uncertainty and modeling. _SIAM review_, 25(3):379-393, 1983.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pp. 2256-2265, Lille, France, 07-09 Jul 2015. PMLR.
* Song and Ermon [2019] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Song and Ermon [2020] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 12438-12448. Curran Associates, Inc., 2020.
* Song et al. [2021] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* Trippe et al. [2022] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem, 2022. URL https://arxiv.org/abs/2206.04119.
* Vahdat et al. [2021] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, 2021.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Vincent [2011] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674, 2011.
* Vahdat et al. [2017]Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In _International Conference on Learning Representations (ICLR)_, 2022.
* Yoo (2000) Hyek Yoo. Semi-discretization of stochastic partial differential equations on r by a finite-difference method. _Mathematics of computation_, 69(230):653-666, 2000.
* Zeng et al. (2022) Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Zhuang et al. (2020) Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. _Advances in neural information processing systems_, 33:18795-18806, 2020.
* Zhuang et al. (2023) Peijve Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, and Miguel Angel Bautista. Diffusion probabilistic fields. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ik91mY-2GN.

## Appendix A Reverse Functional Diffusion Processes

In this Section, we review the mathematical details to obtain the backward FDP discussed in Theorem 1. Depending on the considered class of noise, different approaches are needed. First, we present in Appendix A.1 the conditions to ensure existence of the backward process, which we use if the \(C\) operator is an identity matrix, \(C=I\). Then we move to a different approach in Appendix A.2 for the case \(C\neq I\).

### Follmer Formulation

The work in Follmer (1986) is based on a finite entropy condition, which we report here as Condition 1. One simple way to ensure that the condition is satisfied is to assume:

**Condition 1**.: _For a given \(k\), define \(\mathbb{Q}_{(k)}\) to be the path measure corresponding to the (infinite) system_

\[\begin{cases}\mathrm{d}X_{t}^{i}=b^{i}(X_{t},t)\mathrm{d}t+\mathrm{d}W_{t}^{i },\quad i\neq k\\ \mathrm{d}X_{t}^{i}=\mathrm{d}W_{t}^{k},\quad i=k.\end{cases}\] (22)

_We say that \(\mathbb{Q}\) satisfies the finite local entropy condition if \(\text{\sc KL}\left[\mathbb{Q}\parallel\mathbb{Q}_{(k)}\right]<\infty,\forall k\)._

Define \(\mathcal{F}_{t}^{(i)}=\sigma(X_{0}^{i},X_{s}^{j},0\leq s\leq t,j\neq i)\).

**Assumption 1**.: \[\int_{0}^{T}b^{i}(X_{t},t)^{2}\mathrm{d}t+\sum_{j\neq i}\mathbb{E}[\int_{0}^{T }\left(b^{j}(X_{t},t)-\mathbb{E}\left[b^{j}(X_{t},t)\,|\,\mathcal{F}_{t}^{(i)} \right]\right)^{2}\mathrm{d}t]<\infty,\mathbb{Q}_{(i)}a.s.\] (23)

Notice that if Assumption 1 is true, then Condition 1 holds (Follmer (1986), Thm. 2.23)

**Theorem 3**.: _If \(\text{\sc KL}\left[\mathbb{Q}\parallel\mathbb{Q}_{(k)}\right]<\infty\), then \(\text{\sc KL}\left[\hat{\mathbb{Q}}\parallel\hat{\mathbb{Q}}_{(k)}\right]<\infty\)._

Proof.: The proof can be obtained by adapting the result of Lemma 3.6 of Follmer & Wakolbinger (1986). 

This Theorem states that if the forward FDP path measure \(\mathbb{Q}\) satisfies the finite local entropy condition, then also the reverse FDP path measure \(\hat{\mathbb{Q}}\) satisfies the finite local entropy condition.

**Theorem 4**.: _Let \(\mathbb{Q}\) be a finite entropy measure. Then:_

\[\begin{cases}\mathrm{d}X_{t}^{k}=b^{k}(X_{t},t)\mathrm{d}t+\mathrm{d}W_{t}^{k },\quad\text{under}\quad\mathbb{Q}\\ \mathrm{d}\hat{X}_{t}^{k}=\hat{b}^{k}(\hat{X}_{t},t)\mathrm{d}t+\mathrm{d}\hat {W}_{t}^{k},\quad\text{under}\quad\hat{\mathbb{Q}}\end{cases}\] (24)

_where:_

\[\frac{\partial\log\Bigl{(}\rho_{t}^{(d)}(x^{k}\,|\,x^{j},j\neq k)\Bigr{)}}{ \partial x^{k}}=\hat{b}^{k}(x,T-t)+b^{k}(x,t)\] (25)

Proof.: For the proof, we refer to Theorem 3.14 of Follmer & Wakolbinger (1986). 

### Millet Formulation

Let \(L^{2}(R)=\{x\in H:\sum r^{i}(x^{i})^{2}<\infty\}\). For simplicity, we overload the notation of the letter \(K\), and use it for generic constants, that might be different on a case by case basis.

**Assumption 2**.: \[\forall x\in L^{2}(R),\sup_{t}\{\sum r^{i}(b^{i}(x,t))^{2}\}+ \sum(r^{i})^{2}\leq K(1+\sum r^{i}(x^{i})^{2})\] \[\forall x,y\in L^{2}(R),\sup_{t}\{\sum r^{i}(b^{i}(x,t)-b^{i}(y,t) )^{2}\}\leq K\sum r^{i}(x^{i}-y^{i})^{2}\]This assumption is simply the translation of H1 from milletet et al. (1989) to our notation.

**Assumption 3**.: _There exists an increasing sequence of finite subsets \(J(n),n\in\mathrm{N},\cup_{n}J(n)=\mathrm{N}\) such that \(\forall n\in\mathrm{N},M>0\) there exists a constant \(K(M,n)\) such that the following holds:_

\[\sup_{t}\left(\sup_{i\in J(n)}\left(\left(\sup_{x}|b^{i}(x,t)|:\sup_{j\in J(n) }|x^{j}|\leq M\right)+\sum_{j}r^{j}\right)\right)\leq K(M,n).\]

Again, this assumption is simply the translation of H5 from milletet et al. (1989) to our notation.

**Assumption 4**.: _Either i):_

\[\forall x,y\in L^{2}(R),\sup_{t}\{\sum r^{i}(b^{i}(x,t)-b^{i}(y,t))^{2}\}\leq K \sum(r^{i})^{2}(x^{i}-y^{i})^{2},\]

_or ii): \(\forall i,b^{i}(x)\) is a function of \(x\) for at most \(M\) coordinates and_

\[\forall x,y\in L^{2}(R),\sup_{t}\{\sum(r^{i})^{2}(b^{i}(x,t)-b^{i}(y,t))^{2}\} \leq K\sum(r^{i})^{2}(x^{i}-y^{i})^{2}.\]

This corresponds to satisfying either H3 or jointly H2 and H4 of milletet et al. (1989). For simplicity, we can combine together the different assumptions into

**Assumption 5**.: _Let Assumption 2, Assumption 3, and Assumption 4 hold._

Finally, we state required assumptions about the density:

**Assumption 6**.: _Suppose that the initial condition is \(X_{0}\in L^{2}(R)\)._

* _Assume that the conditional law of_ \(x^{i}\) _given_ \(x^{j},j\neq i\) _has density_ \(\rho_{t}^{(d)}(x^{i}\,|\,x^{j},j\neq i)\) _w.r.t Lebesgue measure on_ \(\mathbb{R}\)_._
* _Assume that_ \(\int_{t_{0}}^{1}\int_{D_{J}}|r^{i}\frac{\mathrm{d}}{\mathrm{d}x^{i}}(\rho_{t}^ {(d)}(x^{i}\,|\,x^{j},j\neq i))|\mathrm{d}x^{i}\rho_{t}(\mathrm{d}x^{j\neq i}) \mathrm{d}t<\infty\)_, for fixed subset_ \(J\subset\mathrm{N},t_{0}>0\) _and_ \(D_{J}=\{(\prod_{j\in J}K_{j})\times(\prod_{j\notin J}\mathbb{R}),K_{j}\text{ compact in }\mathbb{R}\}\cap L^{2}(R)\)_._

We reported in our notation the content of Theorem 4.3 of milletet et al. (1989). This can be used to prove the existence of the backward process.

### Proof of Theorem 1

If \(R=I\), then we assume Assumption 1. Consequently, \(\mathbb{Q}\) is a finite entropy measure. Then Theorem 4 holds, from which the desired result. If, instead \(R\neq I\), then we require Assumption 5,Assumption 6. Application of Thm 4.3 of milletet et al. (1989) allows to prove the validity of Theorem 1 also in this case.

#### a.3.1 Proof of Corollary 1

Assumption 5 is required directly. We need to show that with the considered restrictions Assumption 6 is valid.

Since \(\sum_{i}r^{i}<\infty\), then \(\sum_{i}(r^{i})^{2}=K_{a}<\infty\). Moreover, \((b^{i}(x^{i},t))^{2}<K_{b}^{2}(x^{i})^{2}\). Then, \(\forall x\in L^{2}(R)\), the following holds \(\sup_{t}\{\sum r^{i}(b^{i}(x,t))^{2}\}+\sum(r^{i})^{2}\leq\sum r^{i}K_{b}^{2} (x^{i})^{2}+K_{a}\leq\max(K_{a},K_{b}^{2})\left(1+\sum r^{i}(x^{i})^{2}\right)\). Similarly, \(\forall x,y\in L^{2}(R)\) we have \(\sup_{t}\{\sum r^{i}(b^{i}(x,t)-b^{i}(y,t))^{2}\}\leq\sum r^{i}K_{b}^{2}(x^{i} -y^{i})^{2}\). Thus Assumption 2 is satisfied.

Since \(b^{i}(x,t)\) is bounded and independent on \(t\), Assumption 3 is satisfied, as explicitly discussed in milletet et al. (1989).

Finally, since \(b^{i}(x)\) is a function of \(x\) for \(M=1\) coordinate, and \(\sup_{t}\{\sum(r^{i})^{2}(b^{i}(x,t)-b^{i}(y,t))^{2}\}\leq\sum(r^{i})^{2}K_{b}^ {2}(x^{i}-y^{i})^{2}\), Assumption 4 is satisfied.

Then, combined together Assumption 5 holds.

### Girsanov Regularity

**Condition 2**.: _Assume that \(\gamma_{\boldsymbol{\theta}}(x,t)\) is an \(\hat{\mathcal{F}}\) measurable process and that either:_

\[\mathbb{E}_{\hat{\mathbb{Q}}}\left[\exp\left(\frac{1}{2}\int\limits_{0}^{T} \left\|\gamma_{\boldsymbol{\theta}}(\hat{X}_{t},t)\right\|_{R^{\frac{1}{2}}H}^ {2}\mathrm{d}t\right)\right]=\mathbb{E}_{\hat{\mathbb{Q}}}\left[\exp\left( \frac{1}{2}\int\limits_{0}^{T}\left\|\gamma_{\boldsymbol{\theta}}(X_{t},t) \right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right)\right]<\infty,\] (26)

_or_

\[\exists\delta>0:\mathbb{E}_{\hat{\mathbb{Q}}}\left[\exp\left(\frac{1}{2} \left\|\gamma_{\boldsymbol{\theta}}(\hat{X}_{\delta},\delta)\right\|_{R^{\frac {1}{2}}H}\mathrm{d}t\right)\right]<\infty.\] (27)

This is equivalent to the regularity condition in eq. 10.23 of Da Prato & Zabczyk (2014) or Proposition 10.17 in Da Prato & Zabczyk (2014).

### Proof of KL divergence expression

We leverage Equation (7) to express the Kullback-Leibler divergence as:

\[\text{KL}\left[\hat{\mathbb{Q}}\parallel\hat{\mathbb{P}}^{\chi_{ T}}\right]=\mathbb{E}_{\hat{\mathbb{Q}}}\left[\log\frac{\mathrm{d}\hat{ \mathbb{Q}}_{0}}{\mathrm{d}\hat{\mathbb{P}}_{0}}+\log\frac{\mathrm{d}\rho_{T} }{\mathrm{d}\chi_{T}}\right]=\mathbb{E}_{\hat{\mathbb{Q}}}\left[\log\frac{ \mathrm{d}\hat{\mathbb{Q}}_{0}}{\mathrm{d}\hat{\mathbb{P}}_{0}}\right]+\text{ KL}\left[\rho_{T}\parallel\chi_{T}\right]=\] \[\mathbb{E}_{\hat{\mathbb{Q}}}\left[-\int\limits_{0}^{T}\langle \gamma_{\boldsymbol{\theta}}(\hat{X}_{t},t),\mathrm{d}\hat{W}_{t}\rangle_{R^{ \frac{1}{2}}H}+\frac{1}{2}\int\limits_{0}^{T}\left\|\gamma_{\boldsymbol{\theta }}(\hat{X}_{t},t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\right]+\text{KL} \left[\rho_{T}\parallel\chi_{T}\right]=\] \[\frac{1}{2}\mathbb{E}_{\hat{\mathbb{Q}}}\left[\int\limits_{0}^{T }\left\|\gamma_{\boldsymbol{\theta}}(\hat{X}_{t},t)\right\|_{R^{\frac{1}{2}}H} ^{2}\mathrm{d}t\right]+\text{KL}\left[\rho_{T}\parallel\chi_{T}\right]=\frac{ 1}{2}\mathbb{E}_{\hat{\mathbb{Q}}}\left[\int\limits_{0}^{T}\left\|\gamma_{ \boldsymbol{\theta}}(X_{t},t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t \right]+\text{KL}\left[\rho_{T}\parallel\chi_{T}\right].\]

Moreover, since

\[\text{KL}\left[\hat{\mathbb{Q}}\parallel\hat{\mathbb{P}}^{\chi_{T}}\right]= \mathbb{E}_{\hat{\mathbb{Q}}}\left[\log\frac{\mathrm{d}\hat{\mathbb{Q}}_{T}}{ \mathrm{d}\hat{\mathbb{P}}_{T}^{\chi_{T}}}+\log\frac{\mathrm{d}\rho_{0}}{ \mathrm{d}\chi_{0}}\right]\geq\text{KL}\left[\rho_{0}\parallel\chi_{0}\right],\]

we can combine the two results and obtain Equation (8)

### Conditional score matching

In this subsection we prove the equality in Equation (13):

\[\mathbb{E}_{\hat{\mathbb{Q}}}\left[\int\limits_{0}^{T}\left\| \gamma_{\boldsymbol{\theta}}(X_{t},t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t \right]=\int\limits_{0}^{T}\int_{H}\left\|\gamma_{\boldsymbol{\theta}}(x,t) \right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\mathrm{d}\rho_{t}(x)=\] \[\int\limits_{0}^{T}\int_{H}\left\|D_{x}\log\rho_{T-t}^{(d)}(x)-s_ {\boldsymbol{\theta}}(x,T-t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\mathrm{d }\rho_{t}(x)=\] \[\int\limits_{0}^{T}\int_{H\times H}\left\|D_{x}\log\rho_{t}^{(d)}( x)-D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})+D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})-s_{ \boldsymbol{\theta}}(x,t)\right\|_{R^{\frac{1}{2}}H}^{2}\mathrm{d}t\mathrm{d} \rho_{t}(x,x_{0})=\] \[\int\limits_{0}^{T}\int_{H\times H}\left\|D_{x}\log\rho_{t}^{(d)}( x)-D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})\right\|_{R^{\frac{1}{2}}H}^{2}+\left\|D_{x} \log\rho_{t}^{(d)}(x\,|\,x_{0})-s_{\boldsymbol{\theta}}(x,t)\right\|_{R^{\frac {1}{2}}H}^{2}+\] \[2\left\langle D_{x}\log\rho_{t}^{(d)}(x)-D_{x}\log\rho_{t}^{(d)}( x\,|\,x_{0}),D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})-s_{\boldsymbol{\theta}}(x,t) \right\rangle\mathrm{d}t\mathrm{d}\rho_{t}(x,x_{0}).\]To simplify the equality, we need to notice that:

\[\rho_{t}^{(d)}(x^{i}|x^{j\neq i})\mathrm{d}x^{i}=\mathrm{d}\rho_{t}(x ^{i}|x^{j\neq i})=\int_{x_{0}}\mathrm{d}\rho_{t}(x_{0}|x)\mathrm{d}\rho_{t}(x^{i }|x^{j\neq i})=\int_{x_{0}}\mathrm{d}\rho_{t}(x^{i},x_{0}|x^{j\neq i})=\] \[\int_{x_{0}}\mathrm{d}\rho_{t}(x^{i}|x_{0},x^{j\neq i})\mathrm{d} \rho_{t}(x_{0}|x^{j\neq i})=\mathrm{d}x^{i}\int_{x_{0}}\rho_{t}^{(d)}(x^{i}|x_{ 0},x^{j\neq i})\mathrm{d}\rho_{t}(x_{0}|x^{j\neq i}).\]

Then, computing

\[\int_{x_{0}}\frac{\mathrm{d}}{\mathrm{d}x^{i}}\log\rho^{(d)}(x^{i} |x^{j\neq i},x_{0})\mathrm{d}\rho_{t}(x,x_{0})=\int_{x_{0}}\frac{\frac{\mathrm{ d}}{\mathrm{d}x^{i}}\rho^{(d)}(x^{i}|x^{j\neq i},x_{0})}{\rho^{(d)}(x^{i}|x^{j \neq i},x_{0})}\mathrm{d}\rho_{t}(x,x_{0})=\] \[\int_{x_{0}}\frac{\frac{\mathrm{d}}{\mathrm{d}x^{i}}\rho^{(d)}(x ^{i}|x^{j\neq i},x_{0})}{\rho^{(d)}(x^{i}|x^{j\neq i},x_{0})}\mathrm{d}\rho_{t} (x^{i}|x^{j\neq i},x_{0})\mathrm{d}\rho_{t}(x_{0},x^{j\neq i})=\int_{x_{0}} \frac{\mathrm{d}}{\mathrm{d}x^{i}}\rho^{(d)}(x^{i}|x^{j\neq i},x_{0})\mathrm{ d}x^{i}\mathrm{d}\rho_{t}(x_{0},x^{j\neq i})=\] \[\int_{x_{0}}\frac{\mathrm{d}}{\mathrm{d}x^{i}}\rho^{(d)}(x^{i}|x^ {j\neq i},x_{0})\mathrm{d}x^{i}\mathrm{d}\rho_{t}(x_{0}|x^{j\neq i})\mathrm{d} \rho_{t}(x^{j\neq i})=\frac{\mathrm{d}}{\mathrm{d}x^{i}}\left(\int_{x_{0}}\rho^ {(d)}(x^{i}|x^{j\neq i},x_{0})\mathrm{d}\rho_{t}(x_{0}|x^{j\neq i})\right) \mathrm{d}x^{i}\mathrm{d}\rho_{t}(x^{j\neq i})=\] \[\frac{\mathrm{d}}{\mathrm{d}x^{i}}\rho^{(d)}_{t}(x^{i}|x^{j\neq i })\mathrm{d}x^{i}\mathrm{d}\rho_{t}(x^{j\neq i})=\frac{\mathrm{d}\log\rho^{(d) }_{t}(x^{i}|x^{j\neq i})}{\mathrm{d}x^{i}}\rho^{(d)}_{t}(x^{i}|x^{j\neq i}) \mathrm{d}x^{i}\mathrm{d}\rho_{t}(x^{j\neq i})=\frac{\mathrm{d}\log\rho^{(d)} _{t}(x^{i}|x^{j\neq i})}{\mathrm{d}x^{i}}\mathrm{d}\rho_{t}(x)\]

Consequently:

\[\int_{H\times H}\left\langle D_{x}\log\rho^{(d)}_{t}(x)-D_{x}\log \rho^{(d)}_{t}(x\,|\,x_{0}),s_{\bm{\theta}}(x,t)\right\rangle\mathrm{d}\rho_{ t}(x,x_{0})=0.\]

Combining together and rearranging the terms, we get the desired Equation (13).

### Explicit expression of score function

As mentioned in the text, we consider the case \(f=0\). In this case, there exists a weak solution to Equation (1) as:

\[X_{t}=\exp(t\mathcal{A})X_{0}+\int\limits_{0}^{t}\exp((t-s) \mathcal{A})\mathrm{d}W_{s}.\] (28)

Consequently, the true score function has expression:\[\frac{\mathrm{d}}{\mathrm{d}x^{i}}\log\rho_{t}^{(d)}(x^{i}|x^{j \neq i})=\frac{\frac{\mathrm{d}}{\mathrm{d}x^{i}}\rho_{t}^{(d)}(x^{i}|x^{j\neq i })}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i})}=\frac{\mathrm{d}}{\mathrm{d}x^{i}}\int_{x _{0}}\rho_{t}^{(d)}(x^{i}|x_{0},x^{j\neq i})\mathrm{d}\rho_{t}(x_{0}|x^{j\neq i })}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i})}=\] \[\frac{-\int_{x_{0}}(s^{i})^{-1}\left(x^{i}-\exp\{tbi^{j}x_{0}^{i} \}\right)\rho_{t}^{(d)}(x^{i}|x_{0},x^{j\neq i})\mathrm{d}\rho_{t}(x_{0}|x^{j \neq i})}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i})}=\] \[\frac{-(s^{i})^{-1}\left(x^{i}\rho_{t}^{(d)}(x^{i}|x^{j\neq i})- \int_{x_{0}}\exp\{tbi^{j}x_{0}^{i}\rho_{t}^{(d)}(x^{i}|x_{0},x^{j\neq i}) \mathrm{d}\rho_{t}(x_{0}|x^{j\neq i})\right)}}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i })}=\] \[\frac{-(s^{i})^{-1}\left(x^{i}\rho_{t}^{(d)}(x^{i}|x^{j\neq i})- \int_{x_{0}}\exp\{tbi^{j}x_{0}^{i}\rho_{t}^{(d)}(x^{i}|x_{0},x^{j\neq i}) \mathrm{d}\rho_{t}(x_{0}|x^{j\neq i})\right)}}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i })}=\] \[\frac{-(s^{i})^{-1}\left(x^{i}\rho_{t}^{(d)}(x^{i}|x^{j\neq i})- \int_{x_{0}^{i}}\exp\{tbi^{j}x_{0}^{i}\rho_{t}^{(d)}(x^{i}|x_{0},x^{j\neq i}) \mathrm{d}\rho_{t}(x_{0}|x^{j\neq i})\right)}}{\rho_{t}^{(d)}(x^{i}|x^{j\neq i })}=\] \[\frac{-(s^{i})^{-1}\left(x^{i}\rho_{t}^{(d)}(x^{i}|x^{j\neq i})- \int_{x_{0}^{i}}\exp\{tbi^{j}x_{0}^{i}\rho_{t}^{(d)}(x^{i}|x_{0}^{i},x^{j\neq i })\rho^{(d)}(x_{0}^{i}|x^{j\neq i})\mathrm{d}x_{0}^{i}\right)}}{\rho_{t}^{(d)} (x^{i}|x^{j\neq i})}=\] \[-(s^{i})^{-1}\left(x^{i}\rho_{t}^{(d)}(x^{i}|x^{j\neq i})- \int_{x_{0}^{i}}\exp\{tbi^{j}x_{0}^{i}\rho_{t}^{(d)}(x^{i}|x_{0}^{i},x^{j\neq i })\rho^{(d)}(x_{0}^{i}|x^{j\neq i})\mathrm{d}x_{0}^{i}\right)}=\] \[-(s^{i})^{-1}\left(x^{i}-\int_{x_{0}^{i}}\exp\{tbi^{j}x_{0}^{i} \rho_{t}^{(d)}(x_{0}^{i}|x)\mathrm{d}x_{0}^{i}\right)}\]

where \(s^{i}=r^{i}\frac{\exp(2b^{i}t)-1}{2b^{i}}\). This is exactly the desired Equation (11). Similar calculations allow to prove \(D_{x}\log\rho_{t}^{(d)}(x\,|\,x_{0})=-\mathcal{S}(t)^{-1}\left(x-\exp(t\mathcal{ A})x_{0}\right)\).

## Appendix B Fokker Planck equation

In this Section we discuss the infinite dimensional generalization of the classical Fokker Planck equation. We can associate to Eq. (1) the differential operator:

\[\mathcal{L}_{0}u(x,t)=D_{t}u(x,t)+\underbrace{\frac{1}{2}\operatorname{Tr} \bigl{\{}RD_{x}^{2}u(x,t)\bigr{\}}+\langle\mathcal{A}x+f(x,t),D_{x}u(x,t) \rangle}_{\mathcal{L}u(x,t)},\quad x\in H,t\in[0,T],\] (29)

where \(D_{t}\) is the time derivative, \(D_{x},D_{x}^{2}\) are first and second order Frechet derivatives in space. The domain of the operator \(\mathcal{L}_{0}\) is \(D(\mathcal{L}_{0})\), the linear span of real parts of functions \(u_{\phi,h}=\phi(t)\exp(i(x,h(t))),x\in H,t\in[0,T]\) where \(\phi\in C^{1}([0,T]),\phi(T)=0\), \(h\in C^{1}([0,T];D(\mathcal{A}^{\dagger}))\), where \(\dagger\) indicates adjoint. Provided appropriate conditions are satisfied, see for example Bogachev et al. (2009, 2011), the time varying measure \(\rho_{t}(\mathrm{d}x)\mathrm{d}t\) exists, is unique, and solves the Fokker-Planck equation \(\mathcal{L}_{0}^{\dagger}\rho_{t}=0\).

## Appendix C Uncertainty principle

We here clarify that Hilbert spaces of square integrable functions that are not, in general, simultaneously homogeneous and separable. For example, while \(\mathbb{R}\) is homogeneous, the set of square integrable functions over \(\mathbb{R}\) is not separable, since the basis is the _uncountable_\(\mathrm{set}\cos(2\pi\nu p),\sin(2\pi\nu p),\nu\in\mathbb{R}\). Then, FDP requirements are not met, as we need a countable basis. Moreover, we would need in general an infinite number of samples (grid over the whole \(\mathbb{R}\)) to reconstruct the functions. Conversely, a set like the interval \(I=[0,1]\subset\mathbb{R}\) has _countable_ basis \(\cos(2\pi tp),\sin(2\pi tp),t\in\mathbb{Z}\) (and thus is separable) and, considering \(x\) to be band-limited, a sampling grid with finite cardinality would allow to reconstruct of the function. However, \(I\) is not homogeneous as no isometry group exists. Consequently, Theorem 2 is not applicable. To fix the issue, one could naively think of extending any function defined over \(I\) to the whole \(\mathbb{R}\) by considering \(\bar{x}[p]=x[p],p\in I\) and \(\bar{x}[p]=0,p\notin I\). Obviously, if \(x\in L_{2}(I)\) then \(\bar{x}\in L_{2}(\mathbb{R})\). However, since \(\bar{x}\) has finite support, it cannot be bandlimited, making such an approach not a viable solution. In classical signal processing literature, the problem is usually referred to as the _uncertainty principle_ (Slepian, 1983).

## Appendix D A complete example

We present an example in which we cast Equation (20) for square integrable functions over the interval \(I=[0,1]\), \(L^{2}(I)\). In this case, one natural selection for the basis is the Fourier basis3\(e^{k}=\{\ldots,\exp(-j2\pi 2p),\exp(-j2\pi p),1,\exp(j2\pi p),\exp(j2\pi 2p),\ldots\}\). Assume the operator \(\mathcal{A}\) to be a pseudo-differential operator, such that \(\left\langle\mathcal{A}x,e^{k}\right\rangle=b^{k}x^{k}\). Also, assume that \(b^{k},r^{k}\) are selected such that conditions of Corollary 1 are met, and consequently the backward process exists. Since we are working with samples collected on the grid \(x\left[\nicefrac{{i}}{{N}}\right]\) we first map the samples to the frequency domain, and then build a Fourier-like representation with a finite set of sinusoids. We then define the mapping \(\mathfrak{F}(z^{i})^{k}\stackrel{{\mathrm{def}}}{{=}}\sum_{i=0}^{ N-1}z^{i}\exp\bigl{(}-j2\pi k\tfrac{i}{N}\bigr{)}\) and its inverse \(\mathfrak{I}(z^{i})^{k}\stackrel{{\mathrm{def}}}{{=}}N^{-1}\sum_ {i=0}^{N-1}z^{i}\exp\bigl{(}j2\pi k\tfrac{i}{N}\bigr{)}\). This suggests to consider the following expression for the interpolating functions:

Footnote 3: We stress that although we should consider a real Hilbert space, we select the complex exponential to avoid cluttering the notation. It is possible to select \(\{\cos(2\pi p),\sin(2\pi p),\cos(2\pi 2p),\sin(2\pi 2p),\ldots\}\) as a basis, and redoing the calculations in this Section we can obtain a functionally equivalent scheme as the one with the real basis.

\[\xi^{i}=\frac{1}{N}\sum_{k=0}^{N-1}e^{k}\exp\biggl{(}-j2\pi k\frac{i}{N} \biggr{)}=\frac{1}{N}\sum_{k=0}^{N-1}\exp\biggl{(}j2\pi k(p-\frac{i}{N})\biggr{)}.\]

Those functions are indeed nothing but a frequency truncated version of the sinc function, which is the classical reconstruction function of the sampling theorem on 1-D signals. Moreover \(\left\langle\xi^{i},\zeta^{k}\right\rangle=\delta(i-k)\). We are now ready to show _i)_ the expression of the forward process, _ii)_ the expression of the parametric score function \(s_{\boldsymbol{\theta}}\) and \(\gamma_{\boldsymbol{\theta}}\), _iii)_ the computation of the ELBO and finally _iv)_ the expression for the backward process.

The forward process defined in Equation (20) has expression:

\[\mathrm{d}X_{t}\left[\nicefrac{{k}}{{N}}\right]=\mathfrak{I}\left(b^{l} \mathfrak{F}(X_{t}[\nicefrac{{i}}{{N}}])^{l}\right)^{k}\mathrm{d}t+\mathrm{d}W _{t}\left[\nicefrac{{k}}{{N}}\right],\quad k=1,\ldots,|Z|,\] (30)

where \(\mathrm{d}W_{t}\left[\nicefrac{{k}}{{N}}\right]\simeq\mathfrak{F}(\mathrm{d} W_{t}^{i})^{k}\). Simple calculations show that \(X_{t}\left[\nicefrac{{k}}{{N}}\right]\) is equivalent in distribution to

\[X_{t}\left[\nicefrac{{k}}{{N}}\right]=\mathfrak{I}\left(\exp\bigl{(}b^{l}t \bigr{)}\mathfrak{F}(X_{0}[\nicefrac{{i}}{{N}}])^{l}+\sqrt{s^{l}}\epsilon^{l} \right)^{k},\] (31)

where \(s^{l}=\left\langle\mathcal{S}(t),e^{l}\right\rangle=r^{l}\frac{\exp\left(2b^{l }t\right)-1}{2b^{l}}\) and \(\epsilon^{l}\sim\mathcal{N}(0,1)\), allowing simulation of the forward process in a single step. The parametric score function can be approximated as:

\[s_{\boldsymbol{\theta}}\left(\sum_{i}X_{t}\left[\nicefrac{{i}}{ {N}}\right]\xi^{i},t\right)\left[\nicefrac{{i}}{{N}}\right]=\] (32) \[-\mathfrak{I}\left(\frac{\mathfrak{F}\left(X_{t}\left[\nicefrac{{ i}}{{N}}\right]\right)^{k}-\exp\bigl{(}b^{k}t\bigr{)}\mathfrak{F}\left(n(g(X_{t} \left[\nicefrac{{i}}{{N}}\right]),t,\boldsymbol{\theta})\left[\nicefrac{{i}}{ {N}}\right]\right)}{s^{k}}\right)^{i}.\]

Similarly:

\[\tilde{\gamma}_{\boldsymbol{\theta}}\left(\sum_{i}X_{t}\left[ \nicefrac{{i}}{{N}}\right]\xi^{i},\sum_{i}X_{0}\left[\nicefrac{{i}}{{N}} \right]\xi^{i},t\right)\left[\nicefrac{{i}}{{N}}\right]=\] (33) \[-\mathfrak{I}\left(\frac{\exp\bigl{(}b^{k}t\bigr{)}}{s^{k}}\left( \mathfrak{F}\left(n(g(X_{t}\left[\nicefrac{{i}}{{N}}\right]),t,\boldsymbol{ \theta})\left[\nicefrac{{i}}{{N}}\right]-X_{0}[\nicefrac{{i}}{{N}}]\right)^{k }\right)\right)^{i}.\]Combining Equation (31) and Equation (33) we can fully characterize the training objective defined in Equation (19). Then, it is possible to optimize the value of the parameters \(\bm{\theta}\) with any gradient-based optimizer.

Finally, the backward process approximation is expressed as:

\[\mathrm{d}\hat{X}_{t}\left[\nicefrac{{k}}{{N}}\right]=-\mathfrak{I}\left(b^{l} \mathfrak{F}(\hat{X}_{t}[\nicefrac{{i}}{{N}}])^{l}\right)^{k}+\mathfrak{I} \left(r^{l}\mathfrak{F}\left(s_{\bm{\theta}}(\sum_{i}\hat{X}_{t}\left[ \nicefrac{{i}}{{N}}\right]\xi^{i},T-t)\left[\nicefrac{{i}}{{N}}\right]\right) ^{l}\right)\mathrm{d}t+\mathrm{d}W_{t}\left[\nicefrac{{k}}{{N}}\right]\] (34)

\(k=1,\ldots,|Z|,\)

from which new samples can be generated.

### Proofs

We start by proving Equation (30). Starting from the drift term of Equation (20), we have the following chain of equalities:

\[\left\langle\mathcal{A}\sum_{i=0}^{N-1}X_{t}[\nicefrac{{i}}{{N}} ]\xi^{i},\zeta^{k}\right\rangle=\left\langle\sum_{i=0}^{N-1}X_{t}[\nicefrac{{ i}}{{N}}]\mathcal{A}\frac{1}{N}\sum_{l=0}^{N-1}e^{l}\exp\biggl{(}-j2\pi l \frac{i}{N}\biggr{)},\zeta^{k}\right\rangle=\] \[\left\langle\sum_{i=0}^{N-1}X_{t}[\nicefrac{{i}}{{N}}]\frac{1}{N }\sum_{l=0}^{N-1}b^{l}e^{l}\exp\biggl{(}-j2\pi l\frac{i}{N}\biggr{)},\zeta^{k} \right\rangle=\] \[\sum_{i=0}^{N-1}X_{t}[\nicefrac{{i}}{{N}}]\frac{1}{N}\sum_{l=0}^ {N-1}b^{l}\exp(j2\pi l\nicefrac{{i}}{{N}})\exp(-j2\pi l\nicefrac{{i}}{{N}})=\] \[\sum_{l=0}^{N-1}b^{l}\exp(j2\pi l\nicefrac{{k}}{{N}})\mathfrak{F} (X_{t}[\nicefrac{{i}}{{N}}])^{l}=\] \[\mathfrak{I}\left(b^{l}\mathfrak{F}(X_{t}[\nicefrac{{i}}{{N}}])^ {l}\right)^{i}.\]

The noise term \(\mathrm{d}W_{t}\left[\nicefrac{{k}}{{N}}\right]\) is approximated as:

\[\mathrm{d}W_{t}\left[\nicefrac{{k}}{{N}}\right]=\left\langle\mathrm{d}W_{t}, \zeta^{k}\right\rangle=\left\langle\sum_{i=0}^{\infty}\mathrm{d}W_{t}^{i}e^{i},\zeta^{k}\right\rangle=\sum_{i=0}^{\infty}\mathrm{d}W_{t}^{i}\exp\biggl{(}j2 \pi i\frac{k}{N}\biggr{)}\simeq\mathfrak{F}(\mathrm{d}W_{t}^{i})^{k},\]

where we are truncating the sum. The score term has expression:

\[s_{\bm{\theta}}(\sum_{i}X_{t}\left[\nicefrac{{i}}{{N}}\right] \xi^{i},t)=-(\mathcal{S}(t))^{-1}\left(\sum_{i}X_{t}\left[\nicefrac{{i}}{{N}} \right]\xi^{i}-\exp(t\mathcal{A})n(g(X_{t}\left[\nicefrac{{i}}{{N}}\right]),t,\bm{\theta})\right)=\] \[-\sum_{k}\frac{\zeta_{t}^{k}-\exp\bigl{(}b^{k}t\bigr{)}\left\langle n (g(X_{t}\left[\nicefrac{{i}}{{N}}\right]),t,\bm{\theta}),\exp(-j2\pi kp) \right\rangle}{s^{k}}e^{k}\simeq\] \[-\sum_{k}\frac{C_{t}^{k}-\exp\bigl{(}b^{k}t\bigr{)}\left(N^{-1} \sum_{r}n(g(X_{t}\left[\nicefrac{{i}}{{N}}\right]),t,\bm{\theta})\left[ \frac{r}{N}\right],\exp\bigl{(}-j2\pi k\frac{r}{N}\bigr{)}\right)}{s^{k}}e^{k},\]where the approximation is due to the substitution of explicit scalar product with the discretized version trough \(\mathfrak{F}\). When evaluated on the grid of interest:

\[s_{\bm{\theta}}\left(\sum_{i}X_{t}\left[\nicefrac{{i}}{{N}}\right] \xi^{i},t\right)\left[\nicefrac{{i}}{{N}}\right]=\] \[-\sum_{k}\frac{\left(C_{t}^{k}-\exp\!\left(b^{k}t\right)\left(N^{-1 }\sum_{r}n(g(X_{t}\left[\nicefrac{{i}}{{N}}\right]),t,\bm{\theta}\right)\left[ \frac{r}{N}\right],\exp\!\left(-j2\pi k\frac{r}{N}\right)\right))}{s^{k}}\exp(j2 \pi k\nicefrac{{i}}{{N}})=\] \[-\mathfrak{I}\left(\frac{\mathfrak{F}\left(X_{t}\left[\nicefrac{{ i}}{{N}}\right]\right)-\exp\!\left(b^{k}t\right)\mathfrak{F}\left(n(g(X_{t}\left[ \nicefrac{{i}}{{N}}\right]),t,\bm{\theta}\right)\left[\nicefrac{{i}}{{N}} \right]\right)}{s^{k}}\right).\]

The value of \(\tilde{\gamma}_{\bm{\theta}}\), Equation (33) and the expression of the backward process, Equation (34), are obtained similarly, considering the above results.

## Appendix E Implementation Details and Additional Experiments

In all experiments we use the the complex Fourier basis for the Hilbert spaces, indexed by \(k\). This extends to the 2-dimensional case what we described in Appendix D.1. As stated in the main paper, our practical implementation sets \(f=0\): then, we only need to specify the value for the parameters \(b^{k},r^{k}\). In our implementation we consider an extended class of SDEs that include time-varying multiplying coefficients in front of the drift and diffusion terms, as done for example in the Variance Preserving SDE originally described by Song and Ermon (2020). This can be simply interpreted as the time-rescaled version of autonomous SDEs.

### Architectural details

**INR-based score network.** In our implementation, we use the original INR architecture (Sitzmann et al., 2020). For the specific denoising task we consider in our model, we extend the input of the network architecture to include the corrupted version of the input sample and the diffusion time \(t\), in addition to the spatial coordinates. We emphasize that our architectural design is simple, and does not require self-attention mechanisms (Song and Ermon, 2020). The non-linearity we use in our network is a Gabor wavelet activation function (Saragadam et al., 2023). Furthermore, we found beneficial the inclusion of skip connections.

As stated in the main paper, we consider the _modulation_ approach to INRs. In particular, we implement the meta-learning scheme described by Dupont et al. (2022b); Finn et al. (2017). The outer loop is dedicated to learning the base parameters of the model, while the inner loop focuses on refining the base parameters for each input sample. In the outer loop, the optimization algorithm is AdaBelief (Zhuang et al., 2020), sweeping the learning rate over 1e-4, 1e-5, 1e-6. We found the use of a cosine warm-up schedule to be beneficial for avoiding training instabilities and convergence to sub-optimal solutions. The inner loop is implemented by using three steps of stochastic gradient descent (SGD).

**Transformer-based score network.** In our experiments with the Transformer architecture for score modeling, we employed the UViT backbone Bao et al. (2022). This backbone processes all inputs, be they temporal or noisy image patches, as tokens. Rather than utilizing UViT's default learned positional embeddings, we adapted it to integrate 2D sinusoidal positional encodings. For the noisy input images, patch embeddings transform them into a sequence of tokens. Notably, we chose a patch size of 1 to fully harness the functional properties of our framework. Time embeddings are computed based on the time and then concatenated with the image tokens.

Our chosen transformer architecture comprises 7 layers, with each layer composed of a self-attention mechanism with 8 attention heads and a feedforward layer. Furthermore, we use long skip connections between the shallower and deeper layers, as outlined by Bao et al. (2022).

For optimization during our training, we utilized the AdamWLoshchilov and Hutter (2017) algorithm with a weight decay of 0.03. We employed a cosine warm-up schedule for the learning rate, which ends at a value of 2e-4.

### Additional results

#### e.2.1 A Toy example.

We here present some qualitative examples on a synthetic data-set of functions \(\in L([-1,1])\), and therefore consider the settings described in Appendix D. The _Quadratic_ data is generated as in (Phillips et al., 2022), i.e. \(X_{0}[p]=qp^{2}+\epsilon\), where \(\epsilon\sim\mathcal{N}(0,0.1)\) and \(q\) is a binary random variable that take values \(\{-1,1\}\) with equal probability. Concerning the design of the forward SDE, we select \(b^{k}=\min(\sqrt{k},10)\) and \(r^{k}=k^{-2}\) (thus satisfying Corollary 1). The real data is generated considering a grid of 100 equally spaced points. We can see in Figure 3 some qualitative results. On the left real (red) and generated through FDP (blue) samples show good agreement. Center and right plots depict some example of diffused samples for times 0.2 and 1.0 respectively.

#### e.2.2 Mnist data-set

We evaluate our approach on a simple data-set, using MNIST \(32\times 32\)(LeCun et al., 2010). In this experiment, we compare our method against the baseline score-based diffusion models from Song et al. (2021), which we take from the official code repository https://github.com/yang-song/score_sde. The baseline implements the score network using a U-NET with self-attention and skip connections, as indicated by current best practices, which amounts to \(O(10^{8})\) parameters.

Instead, our method uses a score-network/INR implemented as a simple MLP with 8 layers and 128 neurons in each layer. The activation function is a sinusoidal non-linearity (Sitzmann et al., 2020). Our model counts \(O(10^{5})\) parameters. We consider an SDE with parameters \(r^{k,m}=\frac{176}{k^{2}+m^{2}+2}\), 4 and \(b^{k,m}=\min((k^{2}+m^{2}+0.3)^{-1}+\left(\frac{r^{k,m}}{33}\right)^{\frac{1} {4}},3.6)\). These values have been determined empirically by observing the power spectral density of the data-set, to ensure a well-behaved Signal to Noise ratio evolution throughout the diffusion process for all frequency components.

Figure 3: Left: real (red) and generated samples (blue). Center and Right: Samples diffused for times 0.2 and 1.0 respectively.

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_FAIL:26]

Figure 8: Uncurated celeba samples generated by the INR.

Figure 9: Uncurated celeba samples generated by the Transformer.

## 6 Conclusion

Figure 10: In-painting experiment using INR. Left: real samples, Center: Masked samples, Right: Reconstructed samples.

Figure 11: Colorization experiment using INR. Left: real samples, Center: Gray-scale samples, Right: Reconstructed samples.

Figure 12: De-blurring experiment using INR. Left: real samples, Center: blurred samples, Right: Reconstructed samples.