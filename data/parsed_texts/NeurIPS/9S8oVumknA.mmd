# Intervention Generalization:

A View from Factor Graph Models

 Gecia Bravo-Hermsdorff1  David S. Watson2  Jialin Yu1  Jakob Zeitler3  Ricardo Silva1

Footnote 1: Department of Statistical Science, University College London.

Footnote 2: Department of Informatics, King’s College London.

Footnote 3: Department of Computer Science, University College London.

###### Abstract

One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated _interventional factor model_ (IFM) may not always be informative, but it conveniently abstracts away a need for explicitly modeling unmeasured confounding and feedback mechanisms, leading to directly testable claims. Given an IFM and datasets from a collection of experimental regimes, we derive conditions for identifiability of the expected outcomes of new regimes never observed in these training data. We implement our framework using several efficient algorithms, and apply them on a range of semi-synthetic experiments.

## 1 Introduction

Causal inference is a fundamental problem in many sciences, such as clinical medicine [8, 73, 69] and molecular biology [66, 41, 29]. For example, causal inference can be used to identify the effects of chemical compounds on cell types [73] or determine the underlying mechanisms of disease [52].

One particular challenge in causal inference is _generalization_ -- the ability to extrapolate knowledge gained from past experiments and observational data to previously unseen scenarios. Consider a laboratory that has performed several gene knockouts and recorded subsequent outcomes. Do they have sufficient information to predict how the system will behave under some new combination(s) of knockouts? Conducting all possible experiments in this setting would be prohibitively expensive and time consuming. A supervised learning method could, in principle, map a vector representation of the design to outcome variables of interest. However, past experimental conditions may be too sparsely distributed in the set of all possible assignments, and such a direct supervised mapping would require leaps of faith about how assignment decisions interact with the outcome, even if under the guise of formal assumptions such as linearity.

In this paper, we propose a novel approach to the task of _intervention generalization_, i.e., predicting the effect of unseen treatment regimes. We rely on little more than a postulated factorization of thedistribution describing how intervention variables \(\sigma\) interact with a random process \(X\), and a possible target outcome \(Y\).

Related setups appear in the causal modeling literature on soft interventions [18], including its use in methods such as causal bandits [45] and causal Bayesian optimization [4] (see Section 6). However, these methods rely on directed acyclic graphs (DAGs), which may be hard to justify in many applications. Moreover, in the realistic situation where the target of a real-world intervention is a _set_ of variables [26] and not only the "child" variable within a DAG factor, the parent-child distinction is blurred and previous identification results do not apply. Without claiming that our proposal is appropriate for every application, we suggest the following take on intervention generalization: _model a causal structure as a set of soft constraints, together with their putative (arbitrary but local) modifications by external actions._ Our interventional factor model_ (IFM) leads to provable intervention generalization via a factor graph decomposition which, when informative, can be tested without further assumptions beyond basic relations of conditional independence. IFMs are fully agnostic with regards to cycles or hidden variables, in the spirit of Dawid [23]'s decision-theoretic approach to causal inference, where the key ingredient boils down to statements of conditional independence among random and intervention variables.

Our primary contributions are as follows. (1) We introduce the _interventional factor model_ (IFM), which aims to solve intervention generalization problems using only claims about which interventions interact with which observable random variables. (2) We establish necessary and sufficient conditions for the identifiability of treatment effects within the IFM framework. (3) We adapt existing results from conformal inference to our setting, providing distribution-free predictive intervals for identifiable causal effects with guaranteed finite sample coverage. (4) We implement our model using efficient algorithms, and apply them to a range of semi-synthetic experiments.

## 2 Problem Statement

Data assumptions.Fig. 1**(a)** illustrates a generative process common to many applications and particularly suitable to our framework: a perturbation, here represented by a set of interventional variables \(\sigma\), is applied and a dynamic feedback loop takes place. Often in these applications (e.g, experiments in cell biology [66] and social science [59]), the sampling of this process is highly restrictive, and the time-resolution may boil down to a single _snapshot_, as illustrated by Fig. 1**(b)**. We assume that data is given as samples of a random vector \(X\) collected cross-sectionally at a well-defined time-point (not necessarily at equilibrium) under a well-defined intervention vector \(\sigma\). The importance of establishing a clear sampling time in the context of graphical causal models is discussed by [21]. The process illustrated in Fig. 1**(a)** may suggest no particular Markovian structure at the time the snapshot is taken. However, in practice, it is possible to model the black-box sampling process represented in Fig. 1**(b)** in terms of an _energy function_ representing soft constraints [48]. These could be the result of particular equilibrium processes [47], including deterministic differential equations [40], or empirically-verifiable approximations [59].

_The role of a causal model is to describe how intervention \(\sigma\) locally changes the energy function._ In the context of causal DAG models, sometimes this takes the guise of "soft interventions" and other variations that can be called "interventions on structure" [43, 55] or edge interventions [70]. Changes of structural coefficients in possibly cyclic models have also been considered [37]. The model family we propose takes this to the most abstract level, modeling energy functions via recombinations of

Figure 1: **(a)** A dynamic process where only the final step is (simultaneously) observed. Intervention variables are represented as white squares, random variables as circles, and dashed lines indicate hidden variables. **(b)** A bird’s eye unstructured view of the sampling process, where an intervention vector dictates \(\sigma\) the distribution obtained at an agreed-upon time frame \(T\).

local interventional effects without acyclicity constraints, as such constraints should not be taken for granted [22] and are at best a crude approximation for some problems at hand (e.g., [66]).

Background and notation.The notion of an _intervention variable_ in causal inference encodes an action that modifies the distribution of a system of random variables. This notion is sometimes brought up explicitly in graphical formulations of causal models [60, 72, 23]. To formalize it, let \(\sigma\) denote a vector of intervention variables (also known as _regime indicators_), with each \(\sigma_{i}\) taking values in a finite set \(\{0,1,2,\ldots,\aleph_{i}-1\}\). A fully specified vector of intervention variables characterizes a _regime_ or an _environment_ (we use these terms interchangeably). Graphically, we represent intervention variables using white squares and random variables using circles (Fig. 2). We use subscripts to index individual (random or interventional) variables and superscripts to index regimes. For instance, \(X_{i}^{j}\) denotes the random variable \(X_{i}\) under regime \(\sigma^{j}\), while \(\sigma_{i}^{j}\) denotes the \(i\)th intervention variable of the \(j\)th environment. (Note that the order of the variables is arbitrary.) We occasionally require parenthetical superscripts to index samples, so \(x_{i}^{\jmath(k)}\) denotes the \(k\)th sample of \(X_{i}\) under regime \(\sigma^{j}\).

For causal DAGs, Pearl's \(do\) operator [60] denotes whether a particular random variable \(X_{i}\) is manipulated to have a given value (regardless of the values of its parents). As an example, consider a binary variable \(X_{i}\in\{True,False\}\). We can use a (categorical) intervention variable \(\sigma_{i}\) to index the two "do-interventional" regimes: \(\sigma_{i}=1\) denotes "\(do(X_{i}=True)\)", and \(\sigma_{i}=2\) denotes "\(do(X_{i}=False)\)." We reserve \(\sigma_{i}=0\) to denote the choice of "no manipulation", also known as the "observational" regime. However, one need not think of this \(\sigma_{i}=0\) setting as fundamentally different than the others; indeed, it is convenient to treat all regimes as choices of data generating process.4 Moreover, there is no need for intervention variables to correspond to deterministic settings of random variables; they may in principle describe any well-defined change in distribution, such as stochastic or conditional interventions [23, 17]. As a stochastic example, \(\sigma_{i}=3\) could correspond to randomly choosing \(do(X_{i}=True)\) 30% of the time and \(do(X_{i}=False)\) 70% of the time. As a conditional example, \(\sigma_{i}=4\) could mean "if parents of \(X_{i}\) satisfy a given condition, \(do(X_{i}=True)\), otherwise do nothing". When \(X_{i}\) can take more values, the options for interventional regimes become even more varied. The main point to remember is that \(\sigma_{i}=1,\sigma_{i}=2,\ldots\) should be treated as distinct categorical options, and we reserve \(\sigma_{i}=0\) to denote the "observational" case of "no manipulation".

Footnote 4: In general, there can be infinitely many choices for interventional settings. However, for the identifiability results in the next section, we cover the finite case only, as it removes the need for smoothness assumptions on the effect of intervention levels.

As discussed in the previous section, the probability density/mass function \(p(x;\sigma)\) can be the result of a feedback process that does not naturally fit a DAG representation. Indeed, a growing literature in causal inference carefully considers how DAGs may give rise to equilibrium distributions (e.g., [47, 12, 11]), or marginals of continuous-time processes (e.g., [57]). However, they come with considerable added complexity of assumptions to ensure identifiability. In this work, we abstract away all low-level details about how an equilibrium distribution comes to be, and instead require solely a model for how a distribution \(p(x)\) factorizes as a function of \(\sigma\). These assumptions are naturally formulated as a factor graph model [44] augmented with intervention variables, which we call an _interventional factor model_ (_IFM_).

Figure 2: Examples of distinct causal graphical models, expressing different factorization assumptions. Random variables are represented as circles, intervention variables as white squares, and factors as black squares. **(a)** A directed acyclic graph (DAG) with explicit intervention variables. **(b)** The corresponding interventional factor model (IFM) for **(a)**. **(c)** A Markov random field (MRF) with interventional variables, thus, forming a chain graph.

Problem statement.We are given a space \(\Sigma\) of possible values for an intervention vector \(\sigma\) of dimension \(d\), making \(|\Sigma|\leq\prod_{i=1}^{d}\aleph_{i}\). For a range of training regimes \(\Sigma_{\mathrm{train}}\subset\Sigma\), we are given collection of datasets \(\mathcal{D}^{1},\mathcal{D}^{2},\ldots,\mathcal{D}^{t}\), with dataset \(\mathcal{D}^{j}\) collected under environment/regime \(\sigma^{j}\in\Sigma_{\mathrm{train}}\). The goal is to learn \(p(x;\sigma^{\star})\), and \(\mu(\sigma^{\star}):=\mathbb{E}[Y;\sigma^{\star}]\), for all test regimes \(\sigma^{\star}\in\Sigma_{\mathrm{test}}=\Sigma\backslash\Sigma_{\mathrm{train}}\).

In each training dataset, we measure a sample of post-treatment i.i.d. draws of some \(m\)-dimensional random vector \(X\) (possibly with \(m\neq d\), as there is no reason to always assume a one-to-one mapping between intervention and random variables), and optionally an extra outcome variable \(Y\). The data generating process \(p(x;\sigma^{j})\) is unknown. We assume \(Y\perp\!\!\!\perp\sigma\mid X\) for simplicity5, which holds automatically in cases where \(Y\) is a known deterministic summary of \(X\). We are also given a factorization of \(p(x;\sigma)\),

Footnote 5: Otherwise, we can just define \(Y:=X_{m}\), and use the identification results in Section 3 to check whether \(Y\) can be predicted from \(\sigma\).

\[p(x;\sigma)\propto\prod_{k=1}^{l}f_{k}(x_{S_{k}};\sigma_{F_{k}}),\;\forall \sigma\in\Sigma, \tag{1}\]

where \(S_{k}\subseteq[m]\) and \(F_{k}\subseteq[d]\) define the causal structure. The (positive) functions \(f_{k}(\cdot;\cdot)\) are unknown. The model \(p(y\mid x)\) can also be unknown, depending on the problem.

As illustrated in Figs. 2**(a-b)**, such an intervention factor model (IFM) can also encode (a relaxation of) the structural constraints implied by DAG assumptions. In particular, note the one-to-one correspondence between the DAG factorization in Fig. 2**(a)** and the factors in the IFM in Fig. 2**(b)**: \(f_{1}(x_{1};\sigma_{1}):=p(x_{1};\sigma_{1})\), \(f_{2}(x_{1},x_{2};\sigma_{2}):=p(x_{2}\mid x_{1};\sigma_{2})\), and \(f_{3}(x_{1},x_{2},x_{3};\sigma_{3}):=p(x_{3}\mid x_{1},x_{2};\sigma_{3})\). This explicitly represents that the conditional distribution of \(x_{1}\) given all other variables fully factorizes in \(\sigma\): i.e., \(p(x_{1}\mid x_{2},x_{3};\sigma)\propto f_{1}(x_{1};\sigma_{1})f_{2}(x_{1},x_{2 };\sigma_{2})f_{3}(x_{1},x_{2},x_{3};\sigma_{3})\). Such factorization is not enforced by usual parameterizations of Markov random fields (i.e., graphical models with only undirected edges) or chain graphs (graphical models with acyclic directed edges and undirected edges) [25] such as the example shown in Fig. 2**(c)**.

Scope and limitations.The factorization in Eq. (1) may come from different sources, e.g., from knowledge about physical connections (it is typically the case that one is able to postulate which variables are directly or only indirectly affected by an intervention), or as the result of structure learning methods (e.g., [1]). For structure learning, faithfulness-like assumptions [72] are required, as conditional independencies discovered under configurations \(\Sigma_{\mathrm{train}}\) can only be extrapolated to \(\Sigma_{\mathrm{test}}\) by assuming that independencies observed over particular values of \(\sigma\) can be generalized across all regimes. We do not commit to any particular structure learning technique, and refer to the literature on eliciting and learning graphical structure for a variety of methods [42]. In Appendix A, we provide a general guide on structure elicitation and learning, and a primer on causal modeling and reasoning based solely on abstract conditional independence statements, without a priori commitment to a particular family of graphical models [23].

Even then, the structural knowledge expressed by the factorization in Eq. (1) may be uninformative. Depending on the nature of \(\Sigma_{\mathrm{train}}\) and \(\Sigma_{\mathrm{test}}\), it may be the case that we cannot generalize from training to test environments, and \(p(x;\sigma^{\star})\) is unidentifiable for all \(\sigma^{\star}\in\Sigma_{\mathrm{test}}\). However, all methods for causal inference rely on a trade-off between assumptions and informativeness. For example, unmeasured confounding may imply no independence constraints, but modeling unmeasured confounding is challenging, even more so for equilibrium data without observable dynamics. If we can get away with pure factorization constraints implied by an array of experimental conditions and domain knowledge, we should embrace this opportunity. This is what is done, for instance, in the literature on causal bandits and causal Bayesian optimization [45, 49, 4, 75], which leverage similar assumptions to decide what to do next. However, we are _not_ proposing a method for bandits, Bayesian optimization, or active learning. The task of estimating \(p(x;\sigma^{\star})\) and \(\mu(\sigma^{\star})\) for a novel regime is relevant in itself.

## 3 Interventional Factor Model: Identification

We define our _identification problem_ as follows: given the _population_ distributions \(\mathbb{P}(\Sigma_{\mathrm{train}}):=\{p(x;\sigma^{1}),p(x;\sigma^{2}),\ldots, p(x;\sigma^{t})\}\) for the training regimes \(\sigma^{j}\in\Sigma_{\mathrm{train}}\), and knowledge of the factorization assumptions as given by Eq. (1), can we identify a given \(p(x;\sigma^{\star})\) corresponding to some unobserved test regime \(\sigma^{\star}\in\Sigma_{\mathrm{test}}\)?This identification problem is not analyzed in setups such as causal Bayesian optimization [4, 75], which build on DAGs with interventions targeting single variables. Without this analysis, it is unclear to what extent the learning might be attributed to artifacts due to the choice of prior distribution or regularization, particularly for a sparsely populated \(\Sigma_{\mathrm{train}}\). As an extreme case that is not uncommon in practice, if we have binary intervention variables and never observe more than one \(\sigma_{i}\) set to \(1\) within the same experiment, it is unclear why we should expect a non-linear model to provide information about pairs of assignments \(\sigma_{i}=\sigma_{j}=1\) using an off-the-shelf prior or regularizer. Although eventually a dense enough process of exploration will enrich the database of experiments, this process may be slow and less suitable to situations where the goal is not just to maximize some expected reward but to provide a more extensive picture of the dose-response relationships in a system. While the smoothness of \(p(x;\sigma)\) as a function of \(\sigma\) is a relevant domain-specific information, it complicates identifiability criteria without further assumptions. In what follows, we assume no smoothness conditions, meaning that for some pair \(p(x;\sigma^{a}),p(x;\sigma^{b})\), with \(a\neq b\), these probability density/mass functions are allowed to be arbitrarily different. Such setting is particularly suitable for situations where interventions do take categorical levels, with limited to no magnitude information.

Identification: preliminaries.Before introducing the main result of this section, let us start by considering the toy example displayed in Fig. 3: Fig. 3(**a**) shows the graphical model (how \(X\) could be factorized is not relevant here), and Fig. 3(**c**) shows the assignments for the training regimes \(\Sigma_{\mathrm{train}}\) (all intervention variables are binary). This training set lacks the experimental assignment \(\sigma_{1}=\sigma_{2}=\sigma_{3}=1\).6 However, this regime is implied by the model and \(\Sigma_{\mathrm{train}}\). To see this, consider the factorization \(p(x;\sigma)\propto f_{1}(x;\sigma_{1},\sigma_{2})f_{2}(x;\sigma_{2},\sigma_{3})\), it implies:

Footnote 6: As well as the assignment \(\sigma_{1}=\sigma_{3}=1,\sigma_{2}=0\), which can be recovered by an analogous argument.

where we used \((1,1,0)\) etc. to represent the \(\sigma\) assignments. Because both \((1,1,0)\) and \((0,1,0)\) are in \(\Sigma_{\mathrm{train}}\), the ratio is identifiable up to a multiplicative constant. Moreover, multiplying and dividing the result by \(f_{2}\big{(}x;(1,1)\big{)}\), we get:

from which, given that \((0,1,1)\) is also in \(\Sigma_{\mathrm{train}}\), we can derive \(p\big{(}x;(1,1,1)\big{)}\).

### Message Passing Formulation

The steps in this reasoning can be visualized in Fig. 3(**b**). The leftmost _hypervertex_ represents \((\sigma_{1},\sigma_{2})\) and suggests \(\sigma_{1}\) can be isolated from \(\sigma_{3}\). The first ratio \(p\big{(}x;(1,1,0)\big{)}/p\big{(}x;(0,1,0)\big{)}\) considers three roles: \(\sigma_{1}\) can be isolated (it is set to a "baseline" of \(0\) in the denominator) and \(\sigma_{3}\) is yet to be considered (it is set to the baseline value of \(0\) in the numerator). This ratio sets the stage for the next step where \((\sigma_{2},\sigma_{3})\) is "absorbed" in the construction of the model evaluated at \((1,1,1)\). The entries in \(\Sigma_{\mathrm{train}}\) were chosen so that we see all four combinations of \((\sigma_{1},\sigma_{2})\) and all four combinations of

Figure 3: **(a)** An interventional factor model (IFM) with three (binary) intervention variables. **(b)** The junction tree of the \(\sigma\)-_graph_ associated with the IFM in **(a)**: intervention variables are arranged as a hypergraph, where the hypervertices represent the sets of intervention variables that share a factor and the edge represents the overlap between the two sets of intervention variables. **(c)** A table displaying the assignments for regimes in \(\Sigma_{\mathrm{train}}\). From this \(\Sigma_{\mathrm{train}}\) and the assumptions in **(a)**, it is possible to generalize to the two missing regimes, i.e., \(\sigma_{1}=\sigma_{2}=\sigma_{3}=1\), and \(\sigma_{1}=\sigma_{3}=1\), \(\sigma_{2}=0\).

\((\sigma_{2},\sigma_{3})\), while avoiding the requirement of seeing all eight combinations of \((\sigma_{1},\sigma_{2},\sigma_{3})\). How to generalize this idea is the challenge. Next, we present our first proposed solution.

Definitions.Before we proceed, we need a few definitions. First, recall that we represent the _baseline_ (or "observational") regime as \(\sigma_{1}=\sigma_{2}=\cdots=\sigma_{d}=0\). This describes data captured under a default protocol, e.g., a transcriptomic study in which no genes are knocked down. Let \(\Sigma^{0}_{[\mathcal{Z}]}\) denote the set of all environments \(\sigma^{j}\in\Sigma\) such that \(\sigma^{j}_{i}=0\) if \(i\not\in Z\). For instance, for the example in Fig. 3, a set \(Z:=\{2\}\) implies that \(\Sigma^{0}_{[\mathcal{Z}]}=\big{\{}(0,0,0),(0,1,0)\big{\}}\). Finally, let \(\sigma^{[Z(\star)]}\) be the intervention vector given by \(\sigma_{i}=\sigma^{\star}_{i}\), if \(i\in Z\), and \(0\) otherwise. In what follows, we also use of the concepts of graph _decompositions_, _decomposable graphs_ and _junction trees_, as commonly applied to graphical models [46, 20]. In Appendix B, we review these concepts.7 An IFM \(\mathcal{I}\) with intervention vertices \(\sigma_{1},\ldots,\sigma_{d}\) has an associated \(\sigma\)_-graph_ denoted by \(\mathcal{G}_{\sigma(\mathcal{I})}\), defined as an undirected graph with vertices \(\sigma_{1},\ldots,\sigma_{d}\), and where edge \(\sigma_{i}-\sigma_{j}\) is present if and only if \(\sigma_{i}\) and \(\sigma_{j}\) are simultaneously present in at least one common factor \(f_{k}\) in \(\mathcal{I}\). For example, the \(\sigma\)-graph of the IFM represented in Fig. 3**(a)** is \(\sigma_{1}-\sigma_{2}-\sigma_{3}\). This is a decomposable graph with vertex partition \(A=\{\sigma_{1}\}\), \(B=\{\sigma_{2}\}\) and \(C=\{\sigma_{3}\}\). As this \(\sigma\)-graph is an undirected decomposable graph, it has a junction tree, which we depict in Fig. 3**(b)**.

Footnote 7: **Quick summary**: a junction tree is formed by turning the cliques of an undirected graph into the (hyper)vertices of the tree. Essential to the definition, a junction tree \(\mathcal{T}\) must have a _running intersection property_: given an intersection \(S:=H_{i}\cap H_{j}\) of any two hypervertices \(H_{i},H_{j}\), all hypervertices in the unique path in \(\mathcal{T}\) between \(H_{i}\) and \(H_{j}\) must contain \(S\). This property captures the notion that satisfying local agreements (“equality of properties” of subsets of elements between two adjacent hypervertices in the tree) should imply global agreements. A decomposable graph is simply a graph whose cliques can be arranged as a junction tree.

Identification: message passing formulation.Let \(\mathcal{I}\) be an IFM representing a model for the \(m\)-dimensional random vector \(X\) under the \(d\)-dimensional intervention vector \(\sigma\in\Sigma\). Model \(\mathcal{I}\) has unknown factor parameters but a known factorization \(p(x;\sigma)\propto\prod_{k}f_{k}(x_{S_{k}};\sigma_{F_{k}})\).

**Theorem 3.1**: _Assume that the \(\sigma\)-graph \(\mathcal{G}_{\sigma(\mathcal{I})}\) is decomposable. Given a set \(\Sigma_{\mathrm{train}}=\{\sigma^{1},\ldots,\sigma^{t}\}\subseteq\Sigma\) and known distributions \(p(x;\sigma^{1}),\ldots,p(x;\sigma^{t})\), the following conditions are sufficient to identify any \(p(x;\sigma^{\star})\), \(\sigma^{\star}\in\Sigma\): i) all distributions indexed by \(\sigma^{1},\ldots,\sigma^{t},\sigma^{\star}\) have the same support; and ii) \(\Sigma^{0}_{[F_{k}]}\in\Sigma_{\mathrm{train}}\) for all \(F_{k}\) in the factorization of \(\mathcal{I}\). The algorithm for computing \(p(x;\sigma^{\star})\) works as follows. Construct a junction tree \(\mathcal{T}\) for \(\mathcal{I}\), choose an arbitrary vertex in \(\mathcal{T}\) to be the root, and direct \(\mathcal{T}\) accordingly. If \(V_{k}\) is a hypervertex in \(\mathcal{T}\), let \(D_{k}\) be the union of all intervention variables contained in the descendants of \(V_{k}\) in \(\mathcal{T}\). Let \(B_{k}\) be the intersection of the intervention variables contained in \(V_{k}\) with the intervention variables contained in the parent \(V_{\pi(k)}\) of \(V_{k}\) in \(\mathcal{T}\). We define a _message_ from a non-root vertex \(V_{k}\) to its parent \(V_{\pi(k)}\) as_

\[m^{x}_{k}:=\frac{p(x;\sigma^{[D_{k}(\star)]})}{p(x;\sigma^{[B_{k}(\star)]})}, \tag{2}\]

_with the update equation_

\[p(x;\sigma^{[D_{k}(\star)]})\propto p(x;\sigma^{[F_{k}(\star)]})\prod_{V_{k^{ \prime}}\in ch(k)}m^{x}_{k^{\prime}}, \tag{3}\]

_where the product over \(ch(k)\), the children of \(V_{k}\) in \(\mathcal{T}\), is defined to be 1 if \(ch(k)=\emptyset\)._

(All proofs in Appendix B.) In particular, if no factor contains more than one intervention variable, the corresponding \(\sigma\)-graph will be fully disconnected. This happens, for example, for an IFM derived from a DAG without hidden variables and where each intervention has one child, as in Fig. 2.

### Algebraic Formulation

If a \(\sigma\)-graph is not decomposable, the usual trick of triangulation prior to clique extraction can be used [46, 20], at the cost of creating cliques which are larger than the original factors. Alternatively, and a generalization of Eqs. (2) and (3), we consider transformations of the distributions in \(\mathbb{P}(\Sigma_{\mathrm{train}})\)by products and ratios. We define a _PR-transformation_ of a density set \(\{p(x;\sigma^{1}),\ldots,p(x;\sigma^{t})\}\) as any formula \(\prod_{i=1}^{t}p(x;\sigma^{i})^{qi}\), for a collection \(q_{1},\ldots,q_{t}\) of real numbers.

**Theorem 3.2**: _Let \(\sigma_{F_{k}}^{v}\) denote a particular value of \(\sigma_{F_{k}}\), and let \(\mathbb{D}_{k}\) be the domain of \(\sigma_{F_{k}}\). Given a collection \(\mathbb{P}(\Sigma_{\mathrm{train}}):=\{p(x;\sigma^{1}),\ldots,p(x;\sigma^{t})\}\) and a postulated model factorization \(p(x;\sigma)\propto\prod_{k=1}^{t}f_{k}(x_{S_{k}};\sigma_{F_{k}})\), a sufficient and almost-everywhere necessary condition for a given \(p(x;\sigma^{\star})\) to be identifiable by PR-transformations of \(\mathbb{P}(\Sigma_{\mathrm{train}})\) is that there exists some solution to the system_

\[\forall k\in\{1,2,\ldots,l\},\forall\sigma_{F_{k}}^{v}\in\mathbb{D}_{k},\left( \sum_{i=1:\sigma_{F_{k}}^{v}=\sigma_{F_{k}}^{v}}^{t}q_{i}\right)=\mathbbm{1}( \sigma_{F_{k}}^{\star}=\sigma_{F_{k}}^{v}), \tag{4}\]

_where \(\mathbbm{1}(\cdot)\) is the indicator function returning 1 or 0 if its argument is true or false, respectively._

The solution to the system gives the PR-transformation. An example is shown in Fig. 4. The main idea is that, for a fixed \(x\), any factor \(f_{k}(x_{S_{k}};\sigma_{F_{k}})\) can algebraically be interpreted as an arbitrary symbol indexed by \(\sigma_{F_{k}}\), say "\(f_{k}^{\sigma_{F_{k}}}\)". This means that any density \(p(x;\sigma^{i})\) is (proportional to) a "squarefree" monomial \(m^{i}\) in those symbols (i.e, products with exponents 0 or 1). We need to manipulate a set of monomials (the unnormalized densities in \(\mathbb{P}(\Sigma_{\mathrm{train}})\)) into another monomial \(m^{\star}\) (the unnormalized \(p(x;\sigma^{\star})\)). More generally, the possible set functions \(g(\cdot),h(\cdot)\) such that \(g(m^{\star},\{m^{1},\ldots,m^{t}\})=h(\{m^{1},\ldots,m^{t}\})\), with \(g(\cdot)\) invertible in \(m^{\star}\), suggest that \(g(\cdot)\) and \(h(\cdot)\) must be the product function (monomials are closed under products, but not other analytical manipulations), although we stop short of stating formally the conditions required for PR-transformations to be complete in the space of all possible functions of \(\mathbb{P}(\Sigma_{\mathrm{train}})\). This would be a stronger claim than Theorem 3.2, which shows that Eq. (4) is almost-everywhere complete in the space of PR transformations.

The message passing scheme and the algebraic method provide complementary views, with the former giving a divide-and-conquer perspective that identifies subsystems that can be estimated without requiring changes from the baseline treatment everywhere else. The algebraic method is more general, but suggests no hierarchy of simpler problems. These results show that the factorization over \(X\) is unimportant for identifiability, which may be surprising. Appendix C discusses the consequences of this finding, along with a discussion about requirements on the size of \(\Sigma_{\mathrm{train}}\).

## 4 Learning Algorithms

The identification results give a license to choose any estimation method we want if identification is established -- while we must make the assumption of the model factorizing according to a postulated IFM, we are not required to explicitly use a likelihood function. Theorems 3.1 and 3.2 provide ways of constructing a target distribution \(p(x;\sigma^{\star})\) from products of ratios of densities from \(\mathbb{P}(\Sigma_{\mathrm{train}})\). Thissuggests a plug-in approach for estimation: estimate products of ratios using density ratio estimation methods and multiply results. However, in practice, we found that fitting a likelihood function directly often works better than estimating the product of density ratios, even for intractable likelihoods. We now discuss three strategies for estimating some \(\mathbb{E}[Y;\sigma^{\star}]\) of interest.

Deep energy-based models and direct regression.The most direct learning algorithm is to first maximize the sum of log-likelihoods \(\mathcal{L}(\theta;\mathcal{D}^{1},\ldots,\mathcal{D}^{t}):=\sum_{i=1}^{t} \sum_{j=1}^{n_{i}}\log p_{\theta}(x^{(j)};\sigma^{i})\), where \(n_{i}\) is the number of samples in the \(i\)th regime. The parameterization of the model is indexed by a vector \(\theta\), which defines \(p(\cdot)\) as \(\log p_{\theta}(x;\sigma):=\sum_{k=1}^{t}\phi_{\theta_{k,\sigma_{F_{k}}}}(x_{S _{k}})+\text{constant}\). Here, \(\phi_{\theta}(\cdot)\) is a differentiable black-box function, which in our experiments is a MLP (aka a multilayer perceptron, or feedforward neural network). Parameter vector \(\theta_{k,\sigma_{F_{k}}}\) is the collection of weights and biases of the MLP, a different instance for each factor \(k\)_and_ combination of values in \(\sigma_{F_{k}}\). In principle, making the parameters smooth functions of \(\sigma_{F_{k}}\) is possible, but in the interest of simplifying the presentation, we instead use a look-up table for completely independent parameters as indexed by the possible values of \(\sigma_{F_{k}}\). Parameter set \(\theta\) is the union of all \(\sum_{k=1}^{l}\prod_{j\in F_{k}}|\aleph_{j}|\) MLP parameter sets. As maximizing log-likelihood is generally intractable, in our implementation we apply pseudo-likelihood with discretization of each variable \(X\) in a grid. The level of discretization does not affect the number of parameters, as we take their numerical value as is, renormalizing over the pre-defined grid. Score matching [38], noise contrastive estimation [32] or other variants (e.g., [71]) could be used; our pipeline is agnostic to this choice, with further details in Appendix G. We then estimate \(f_{y}(x):=\mathbb{E}[Y\mid x]\) using an off-the-shelf method, which in our case is another MLP. For a given \(\sigma^{\star}\), we sample from the corresponding \(p(x;\sigma^{\star})\) with Gibbs sampling and average the results of the regression estimate \(\widehat{f}_{y}(x)\) to obtain an estimate \(\widehat{\mu}(\sigma^{\star})\).

Inverse probability weighting (IPW).A more direct method is to reweight each training sample by the target distribution \(p(x;\sigma^{\star})\) to generate \(\widehat{\mu}(\sigma^{i\star}):=\sum_{j=1}^{n_{i}}y^{i(j)}w^{i(j)^{\star}}\), where \(w^{i(j)^{\star}}\) is the density ratio \(p(x^{i(j)};\sigma^{\star})/p(x^{i(j)};\sigma^{i})\), rescaled such that \(\sum_{j=1}^{n_{i}}w^{i(j)^{\star}}=1\). There are several direct methods for density ratio estimation [53] that could be combined using the messages/product-ratios of the previous section, but we found that it was stabler to just take the density ratio of the fitted models using deep energy-based learning, just like in the previous algorithm. Once estimators \(\widehat{\mu}(\sigma^{\star(1)}),\ldots,\widehat{\mu}(\sigma^{\star(t)})\) are obtained, we combine them by the usual inverse variance weighting rule, \(\widehat{\mu}(\sigma^{i\star}):=\sum_{i=1}^{t}r^{i}\mu_{\sigma^{\star(i)}}/ \sum_{i=1}^{t}r^{i}\), where \(r^{i}:=1/\widehat{v}^{i}\), and \(\widehat{v}^{i}:=\sum_{j=1}^{n_{i}}(y^{i(j)})^{2}(w^{i(j)^{\star}})^{2}\). This method requires neither a model for \(f_{y}(x)\) nor Markov chain Monte Carlo. However, it may behave more erratically than the direct method described above, particularly under strong shifts in distribution.

Covariate shift regression.Finally, a third approach for estimating \(\mu(\sigma^{\star})\) is to combine models for \(f_{y}(x)\) with density ratios, learning a customized \(\widehat{f}_{y}(x)\) for each test regime separately. A s this is very slow and did not appear to be advantageous compared to the direct method, we defer a more complete description to Appendix D.

Predictive Coverage.Even when treatment effects are identifiable within the IFM framework, the uncertainty of resulting estimates can vary widely depending on the training data and learning algorithm. Building on recent work in conformal inference [78, 51, 76], we derive the following finite sample coverage guarantee for potential outcomes, which requires no extra assumptions beyond those stated above.

**Theorem 4.1** (Predictive Coverage.): _Assume the identifiability conditions of Theorems 3.1 or 3.2 hold. Fix a target level \(\alpha\in(0,1)\), and let \(\mathcal{I}_{1},\mathcal{I}_{2}\) be a random partition of observed regimes intro training and test sets of size \(n/2\). Fit a model \(\widehat{\mu}\) using data from \(\mathcal{I}_{1}\) and compute conformity scores \(s^{(i)}=|y^{k(i)}-\widehat{\mu}(\sigma^{k})|\) using data from \(\mathcal{I}_{2}\). For some new test environment \(\sigma^{\star}\), compute the normalized likelihood ratio \(w^{(i)}(\sigma^{\star})\propto p(x^{k(i)};\sigma^{\star})/p(x^{k(i)};\sigma^{ k})\), rescaled to sum to \(n/2\). Let \(\widehat{\tau}(\sigma^{\star})\) be the \(q^{\text{th}}\) smallest value in the reweighted empirical distribution \(\sum_{i}w^{(i)}(\sigma^{\star})\cdot\delta(s^{(i)})\), where \(\delta\) denotes the Dirac delta function and \(q=\lceil(n/2+1)(1-\alpha)\rceil\). Then for any new sample \(n+1\), we have:_

\[\mathbb{P}\big{(}Y^{\star(n+1)}\in\widehat{\mu}(\sigma^{\star})\pm\widehat{ \tau}(\sigma^{\star})\big{)}\geq 1-\alpha.\]

_Moreover, if weighted conformity scores have a continuous joint distribution, then the upper bound on this probability is \(1-\alpha+1/(n/2+1)\)._Experiments

We run a number of semi-synthetic experiments to evaluate the performance of the IFM approach on a range of intervention generalization tasks. In this section, we summarize our experimental set-up and main results. The code for reproducing all results and figures is available online8; in Appendix E, we provide a detailed description of the datasets and models; and in Appendix F we present further analysis and results.

Footnote 8: [https://github.com/rbas-ucl/intgen](https://github.com/rbas-ucl/intgen)

Datasets.Our experiments are based on the following two biomolecular datasets: _i)_**Sachs**[66]: a cellular signaling network with \(11\) nodes representing phosphorylated proteins and phospholipids, several of which were perturbed with targeted reagents to stimulate or inhibit expression. There are \(4\) binary intervention variables. \(\Sigma_{\mathrm{train}}\) has \(5\) regimes: a _baseline_, with all \(\sigma_{i}=0\), and \(4\) "single-intervention" regimes, each with a different single \(\sigma_{i}=1\). \(\Sigma_{\mathrm{test}}\) consist of the remaining \(11\) (\(=2^{4}-5\)) unseen regimes. _ii)_**DREAM**[30]: Simulated data based on a known _E. coli_ regulatory sub-network with \(10\) nodes. There are \(10\) binary intervention variables, and (similarly) \(\Sigma_{\mathrm{train}}\) has a baseline regime, with all \(\sigma_{i}=0\), and \(10\) single-intervention regimes, with single choices of \(\sigma_{i}=1\). \(\Sigma_{\mathrm{test}}\) consists of \(45\) (\(10\times 9/2=45\)) unseen regimes defined by all pairs \(\sigma_{i}=\sigma_{j}=1\).

Oracular simulators.The first step to test intervention generalization is to build a set of proper test beds (i.e., simulators that serve as causal effect oracles for any \(\sigma\in\Sigma\)), motivated by expert knowledge about the underlying system dynamics. Neither the original Sachs data nor the DREAM simulator we used provide joint intervention data required in our evaluation. Thus, for each domain, we trained two ground truth simulators: _i)_**Causal-DAG**: a DAG model following the DAG structure and data provided by the original Sachs et al. and DREAM sources (DAGs shown in Fig. 10**(a)**) and Fig. 11**(a)**, respectively). Given the DAG, we fit a model where each conditional distribution is a heteroskedastic Gaussian with mean and variance parameterized by MLPs (with \(10\) hidden units) of the respective parents. _ii)_**Causal-IFMs**: the corresponding IFM is obtain by a direct projection of the postulated DAG factors (as done in, e.g., Fig. 2**(b)**). The likelihood is a neural energy model (Section 4) with MLPs with \(15\) hidden units defining potential functions. After fitting these models, we compute by Monte Carlo simulation their implied ground truths for every choice of \(\sigma^{*}\). The outcomes \(Y\) are then generated under \(100\) different structural equations of the form \(\tanh(\lambda^{\top}X)+\epsilon\), with random independent normal weights \(\lambda\) and \(\epsilon\sim\mathcal{N}(0,v_{y})\). \(\lambda\) and \(v_{y}\) are scaled such that the ground truth variance of \(\lambda^{\mathsf{T}}X\), \(\text{Var}(\lambda^{\mathsf{T}}X)\), is sampled uniformly at random from the interval \([0.6,0.8]\), and set \(v_{y}:=1-\text{Var}(\lambda^{\mathsf{T}}X)\).

Compared models.We implement three variants of our proposed IFM model, corresponding to the three learning algorithms described in Section 4: _i)_**IFM1** uses deep energy-based models and direct regression; _ii)_**IFM2** uses an IPW estimator; and _iii)_**IFM3** relies on covariate shift regression. We compare these models to the following benchmarks: _i)_**Black-box**: We apply an off-the-shelf algorithm (XGBoost [14]) to learn a direct mapping from \(\sigma\) to \(Y\) without using \(X\). This model does not exploit any structural assumptions. _ii)_**DAG**: We estimate the structural equations in an acyclic topological ordering that is consistent with the data generating process. This represents a strong baseline that exploits ground truth knowledge of the underlying causal graph. The likelihood is defined by conditional Gaussian models with mean and variances parameterized as MLPs, matching exactly one of the simulators described below.

Results.We evaluate model performance based on the proportional root mean squared error (pRMSE), defined as the average of the squared difference between the ground truth \(Y\) and estimated \(Y\), with each entry further divided by the ground truth variance of the corresponding \(Y\). Results are visualized in Fig. 5. We additionally run a series of one-sided binomial tests to determine whether models significantly outperform the black box baseline, and compare the Spearman's rank correlation [80] between expected and observed outcomes for all test regimes. Unsurprisingly, DAGs do best when the ground truth is a Causal-DAG, while IFM methods do better when the data generator is a Causal-IFM. Still, some IFMs are robust to both ground truth models, with IFM1 (the deep energy model) doing especially well on the DREAM dataset, and IFM2 (the IPW estimator) excelling on the Sachs data.

In particular, for the DREAM datasets, IFM1 (scaled) errors remain stable, while the DAG has a major loss of performance when a non-DAG ground truth is presented. However, IFM2 underperform on DREAM datasets, which might be because the knockdowns regimes in those datasets induce dramatic shifts in distribution overlap among regimes. (We omit results for IFM3 (covariate shift regression) on these datasets, as the method does not converge in a reasonable time.) Though the black-box method (XGBoost algorithm and no structural assumption) sometimes struggles to extrapolate, displaying a long-tailed distribution of errors, it actually does surprisingly well in some experiments, especially on the DREAM dataset. In fact, the black-box method is essentially indistinguishable from linear regression in this benchmark (results not shown), which is not surprising given the sparsity of the training data. However, the non-additivity of the \(\sigma\) effect on \(Y\) is more prominent in the Sachs datasets, making it difficult to generalize without structural assumptions.

## 6 Related Work

A factor graph interpretation of Pearl's _do_-calculus framework is described in [79], but without addressing the problem of identifiability. More generally, several authors have exploit structural assumptions to predict the effects of unseen treatments. Much of this research falls under the framework of _transportability_[61, 7], where the goal is to identify causal estimands from a combination of observational and experimental data collected under different regimes. If only atomic interventions are considered, the \(do\)-calculus is sound and complete for this task [50]. The \(\sigma\)-calculus introduced by [28, 17] extends these transportability results to soft interventions [18, 19]. However, these methods are less clearly defined when an intervention affects several variables simultaneously, and while DAGs with additive errors have some identifiability [67] and estimation [75] advantages, acyclic error additivity may not be an appropriate assumption in some domains. Under further parametric assumptions, causal effects can be imputed with matrix completion techniques [73, 3] or more generic supervised learning approaches [31], but these methods often require some data to be collected for all regimes in \(\Sigma_{\mathrm{test}}\). Finally, [2] is the closest related work in terms of goals, factorizing the function space of each \(\mathbb{E}[X_{i};\sigma]\) directly as a function of \(\sigma\).

Another strand of related research pertains to online learning settings. For instance, several works have shown that causal information can boost convergence rates in multi-armed bandit problems when dependencies are present between arms [45, 49, 24], even when these structures must themselves be adaptively learned [9]. This suggests a promising direction for future work, where identification strategies based on the IFM framework are used to prioritize the search for optimal treatments. Indeed, combining samples across multiple regimes can be an effective strategy for causal discovery, as illustrated by recent advances in invariant causal prediction [63, 33, 64, 81], and it can also help with domain adaptation and covariate shift [54, 6, 13, 15].

## 7 Conclusion

We introduced the IFM framework for solving intervention generalization tasks. Results from simulations calibrated by real-world data show that our method successfully predicts outcomes for novel treatments, providing practitioners with new methods for conducting synthetic experiments. Future work includes: _i)_ integrating the approach with experimental design, Bayesian optimization and bandit learning; _ii)_ variations that include pre-treatment variables and generalizations across heterogeneous subpopulations, inspired by complementary matrix factorization methods such as [2]; _iii)_ tackling sequential treatments; and _iv)_ diagnostics of cross-regime overlap issues [36, 58].

Figure 5: Experimental results on a range of intervention generalization tasks, see text for details.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers and Mathias Drton for useful discussions. GBH was supported by the ONR grant 62909-19-1-2096, and JY was supported by the EPSRC grant EP/W024330/1. RS was partially supported by both grants. JZ was supported by UKRI grant EP/S021566/1.

## References

* [1]Abbeel, P., Koller, D., and Ng, A. Y. (2006). Learning factor graphs in polynomial time and sample complexity. _Journal of Machine Learning Research_, 7:1743-1788.
* [2]Agarwal, A., Agarwal, A., and Vijaykumar, S. (2023). Synthetic combinations: A causal inference framework for combinatorial interventions. _arXiv_ preprint, 2303.14226.
* [3]Agarwal, A., Shah, D., and Shen, D. (2020). Synthetic A/B testing using synthetic interventions. _arXiv_ preprint, 2006.07691.
* [4]Aglietti, V., Lu, X., Paleyes, A., and Gonzalez, J. (2020). Causal Bayesian optimization. \(23^{rd}\) _International Conference on Artificial Intelligence and Statistics_.
* [5]Akbari, K., Winter, S., and Tomko, M. (2023). Spatial causality: A systematic review on spatial causal inference. _Geographical Analysis_, 55:56-89.
* [6]Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2020). Invariant risk minimization. _arXiv_ preprint, 1907.02893.
* [7]Bareinboim, E. and Pearl, J. (2014). Transportability from multiple environments with limited experiments: Completeness results. In _Advances in Neural Information Processing Systems_, volume 27.
* [8]Bica, I., Alaa, A. M., Lambert, C., and van der Schaar, M. (2021). From real-world patient data to individualized treatment effects using machine learning: Current and future methods to address underlying challenges. _Clinical Pharmacology & Therapeutics_, 109(1):87-100.
* [9]Bilodeau, B., Wang, L., and Roy, D. M. (2022). Adaptively exploiting \(d\)-separators with causal bandits. In _Advances in Neural Information Processing Systems_.
* [10]Bishop, Y. M. M., Fienberg, S. E., and Holland, P. W. (1974). _Discrete Multivariate Analysis: Theory and Practice_. MIT Press.
* [11]Blom, T. and Mooij, J. M. (2023). Causality and independence in perfectly adapted dynamical systems. _Journal of Causal Inference_, 11(1):2885-2915.
* [12]Bongers, S., Forre, P., Peters, J., and Mooij, J. M. (2021). Foundations of structural causal models with cycles and latent variables. _Annals of Statistics_, 49(5):2885-2915.

* [14]Chen, T. and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, page 785-794.
* [15]Chen, Y. and Buhlmann, P. (2021). Domain adaptation under structural causal models. _J. Mach. Learn. Res._, 22(1).
* [16]Constantinou, P. and Dawid, A. P. (2017). Extended conditional independence and applications in causal inference. _The Annals of Statistics_, 45(6):2618-2653.
* [17]Correa, J. and Bareinboim, E. (2020a). A calculus for stochastic interventions:causal effect identification and surrogate experiments. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(06):10093-10100.
* [18]Correa, J. and Bareinboim, E. (2020b). General transportability of soft interventions: Completeness results. In _Advances in Neural Information Processing Systems_, volume 33, pages 10902-10912.
* [19]Correa, J., Lee, S., and Bareinboim, E. (2022). Counterfactual transportability: A formal approach. In _International Conference on Machine Learning_.
* [20]Cowell, R., Dawid, A., Lauritzen, S., and Spiegelhalter, D. (1999). _Probabilistic Networks and Expert Systems_. Springer-Verlag.

* Dash [2005] Dash, D. (2005). Restructuring dynamic causal systems in equilibrium. _Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics_, pages 81-88.
* Dawid [2010] Dawid, A. P. (2010). Beware of the DAG! _Proceedings of Workshop on Causality: Objectives and Assessment at NIPS 2008, PMLR_, 6:59-86.
* Dawid [2021] Dawid, P. (2021). Decision-theoretic foundations for statistical causality. _Journal of Causal Inference_, 9(56):39-77.
* de Kroon et al. [2020] de Kroon, A. A. W. M., Belgrave, D., and Mooij, J. M. (2020). Causal discovery for causal bandits utilizing separating sets. _arXiv:_2009.07916.
* Drton [2009] Drton, M. (2009). Discrete chain graph models. _Bernoulli_, 15:736-753.
* Eaton and Murphy [2007] Eaton, D. and Murphy, K. (2007). Exact Bayesian structure learning from uncertain interventions. _Artificial Intelligence & Statistics_.
* Eberhardt and Scheines [2007] Eberhardt, F. and Scheines, R. (2007). Interventions and causal inference. _Philosophy of Science_, 74:981-989.
* Forre and Mooij [2018] Forre, P. and Mooij, J. M. (2018). Constraint-based causal discovery for non-linear structural causal models with cycles and latent confounders. In _Proceedings of the 34th Annual Conference on Uncertainty in Artificial Intelligence_, pages 269-278.
* Gentzel et al. [2021] Gentzel, A. M., Pruthi, P., and Jensen, D. (2021). How and why to use experimental data to evaluate methods for observational causal inference. In _International Conference on Machine Learning_, pages 3660-3671. PMLR.
* Greenfield et al. [2010] Greenfield, A., Madar, A., Ostrer, H., and Bonneau, R. (2010). Dream4: Combining genetic and dynamic information to identify biological networks and dynamical models. _PloS one_, 5(10):e13397.
* Gulthin et al. [2021] Gulthin, L., Watson, D., Kusner, M., and Silva, R. (2021). Operationalizing complex causes: A pragmatic view of mediation. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 3875-3885. PMLR.
* Gutmann and Hyvarinen [2010] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. _13th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 297-304.
* Heinze-Deml et al. [2018] Heinze-Deml, C., Peters, J., and Meinshausen, N. (2018). Invariant Causal Prediction for Nonlinear Models. _J. Causal Inference_, 6(2).
* Hernan and J [2020] Hernan, M. and J, R. (2020). _Causal Inference: What If_. Chapman Hall/CRC.
* Higbee [2023] Higbee, S. (2023). Policy learning with new treatments. _arXiv:_2210.04703_.
* Hill and Su [2011] Hill, J. and Su, Y. (2011). Assessing lack of common support in causal inference using bayesian nonparametrics: Implications for evaluating the effect of breastfeeding on children's cognitive outcomes. _The Annals of Applied Statistics_, 7:1386-1420.
* Hoover [2001] Hoover, K. (2001). _Causality in Macroeconomics_. Cambridge University Press.
* Hyvarinen [2005] Hyvarinen, A. (2005). Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6:695-709.
* Hyvarinen et al. [2008] Hyvarinen, A., Shimizu, S., and Hoyer, P. O. (2008). Causal modelling combining instantaneous and lagged effects: an identifiable model based on non-Gaussianity. _Proceedings of the 25th international conference on machine learning (ICML 2008)_, pages 424-431.
* Iwasaki and Simon [1994] Iwasaki, Y. and Simon, H. A. (1994). Artificial intelligence. _Causality and model abstraction_, 67:143-194.
* Karlebach and Shamir [2008] Karlebach, G. and Shamir, R. (2008). Modelling and analysis of gene regulatory networks. _Nature Reviews Molecular Cell Biology_, 9(10):770-780.
* Koller and Friedman [2009] Koller, D. and Friedman, N. (2009). _Probabilistic Graphical Models: Principles and Techniques_. MIT Press.
* Korb et al. [2004] Korb, K. B., Hope, L. R., Nicholson, A. E., and Axnick, K. (2004). Varieties of causal intervention. _Pacific Rim International Conference on Artificial Intelligence (PRICAI 2004)_, pages 322-331.

* Kschischang et al. [2001] Kschischang, F., Frey, B., Brendan, J., and Loeliger, H.-A. (2001). Factor graphs and the sum-product algorithm. _IEEE Transactions on Information Theory_, 47:498--519.
* Lattimore et al. [2016] Lattimore, F., Lattimore, T., and Reid, M. D. (2016). Causal bandits: Learning good interventions via causal inference. In _Advances in Neural Information Processing Systems_, pages 1181-1189.
* Lauritzen [1996] Lauritzen, S. (1996). _Graphical Models_. Oxford University Press.
* Lauritzen and Richardson [2002] Lauritzen, S. L. and Richardson, T. S. (2002). Chain graph models and their causal interpretation. _Journal of the Royal Statistical Society Series B_, 64:321-361.
* LeCun et al. [2006] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. J. (2006). A tutorial on energy-based learning. In BakIr, G., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., and Vishwanathan, S., editors, _Predicting Structured Data_. MIT Press.
* Lee and Bareinboim [2018] Lee, S. and Bareinboim, E. (2018). Structural causal bandits: Where to intervene? In _Advances in Neural Information Processing Systems_, pages 2568-2578.
* synthesizing observations and experiments from heterogeneous domains. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(06):10210-10217.
* Lei et al. [2018] Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. (2018). Distribution-Free Predictive Inference for Regression. _Journal of the American Statistical Association_, 113(523):1094-1111.
* Leist et al. [2022] Leist, A. K., Klee, M., Kim, J. H., Rehkopf, D. H., Bordas, S. P., Muniz-Terrera, G., and Wade, S. (2022). Mapping of machine learning approaches for description, prediction, and causal inference in the social and health sciences. _Science Advances_, 8(42):eabb1942.
* Lu et al. [2023] Lu, N., Zhang, T., Fang, T., Teshima, T., and Sugiyama, M. (2023). Rethinking importance weighting for transfer learning. In _Federated and Transfer Learning_, pages 185-231. Springer.
* Magliacane et al. [2018] Magliacane, S., van Ommen, T., Claassen, T., Bongers, S., Versteeg, P., and Mooij, J. M. (2018). Domain adaptation by using causal inference to predict invariant conditional distributions. In _Advances in Neural Information Processing Systems_, volume 31.
* Malinsky [2018] Malinsky, D. (2018). Intervening on structure. _Synthese_, 195:2295--2312.
* McKay [2003] McKay, D. J. C. (2003). _Information Theory, Inference, and Learning Algorithms_. Cambridge University Press.
* Mogensen et al. [2018] Mogensen, S. W., Malinsky, D., and Hansen, N. R. (2018). Causal learning for partially observed stochastic dynamical systems. _Proceedings of the 34th conference on Uncertainty in Artificial Intelligence (UAI 2018)_, pages 350-360.
* Oberst et al. [2020] Oberst, M., Johansson, F., Wei, D., Gao, T., Brat, G., Sontaga, D., and Varshney, K. (2020). Characterization of overlap in observational studies. \(23^{rd}\) _International Conference on Artificial Intelligence and Statistics (AISTATS 2020)_, pages 788-798.
* Ogburn et al. [2020] Ogburn, E. L., Shpitser, I., and Lee, Y. (2020). Causal inference, social networks and chain graphs. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 183:1659--1676.
* Pearl [2009] Pearl, J. (2009). _Causality: Models, Reasoning and Inference, 2nd edition_. Cambridge University Press.
* Pearl and Bareinboim [2011] Pearl, J. and Bareinboim, E. (2011). Transportability of causal and statistical relations: A formal approach. _Proceedings of the AAAI Conference on Artificial Intelligence_, 25(1):247-254.
* Pearl and McKenzie [2018] Pearl, J. and McKenzie, D. (2018). _The Book of Why_. Allen Lane.
* Peters et al. [2016] Peters, J., Buhlmann, P., and Meinshausen, N. (2016). Causal inference by using invariant prediction: identification and confidence intervals. _J. Royal Stat. Soc. Ser. B Methodol._, 78(5):947-1012.
* Pfister et al. [2019] Pfister, N., Buhlmann, P., and Peters, J. (2019). Invariant causal prediction for sequential data. _Journal of the American Statistical Association_, 114(527):1264-1276.
* Richardson [2003] Richardson, T. (2003). Markov properties for acyclic directed mixed graphs. _Scandinavian Journal of Statistics_, 30:145-157.
* Sachs et al. [2005] Sachs, K., Perez, O., Pe'er, D., Lauffenburger, D. A., and Nolan, G. P. (2005). Causal protein-signaling networks derived from multiparameter single-cell data. _Science_, 308(5721):523-529.

* [67] Saengkyongam, S. and Silva, R. (2020). Learning joint nonlinear effects from single-variable interventions in the presence of hidden confounders. \(36^{th}\) _Conference on Uncertainty in Artificial Intelligence (UAI 2020)_.
* [68] Sejdinovic, D., Gretton, A., and Bergsma, W. (2013). A kernel test for three-variable interactions. _Neural Information Processing Systems (NeurIPS)_, 26:1124---1132.
* [69] Shi, C., Sridhar, D., Misra, V., and Blei, D. (2022). On the assumptions of synthetic control methods. In _International Conference on Artificial Intelligence and Statistics_, pages 7163-7175. PMLR.
* [70] Shpitser, I. and TchetgenTchetgen, E. (2016). Causal inference with a graphical hierarchy of interventions. _Annals of Statistics_, 44:2433-2466.
* [71] Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. _Neural Information Processing Systems (NeurIPS)_, 32.
* [72] Spirtes, P., Glymour, C., and Scheines, R. (2000). _Causation, Prediction and Search_. Cambridge University Press.
* [73] Squires, C., Shen, D., Agarwal, A., Shah, D., and Uhler, C. (2022). Causal imputation via synthetic interventions. In _Conference on Causal Learning and Reasoning_, pages 688-711. PMLR.
* [74] Studeny, M. (2051). _Probabilistic conditional independence structures_. Springer.
* [75] Sussex, S., Makarova, A., and Krause, A. (2023). Model-based causal Bayesian optimization. \(11^{th}\) _International Conference on Learning Representations_.
* [76] Tibshirani, R. J., Foygel Barber, R., Candes, E., and Ramdas, A. (2019). Conformal prediction under covariate shift. In _Advances in Neural Information Processing Systems_, volume 32.
* [77] Tigas, P., Annadani, Y., Jesson, A., Scholkopf, B., Gal, Y., and Bauer, S. (2022). Interventions, where and how? Experimental design for causal models at scale. In _Neural Information Processing Systems (NeurIPS 2022)_.
* [78] Vovk, V., Gammerman, A., and Shafer, G. (2005). _Algorithmic Learning in a Random World_. Springer, New York.
* [79] Winn, J. (2012). Causality with gates. _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, PMLR_, 22:1314-1322.
* [80] Zar, J. H. (2014). Spearman rank correlation: overview. _Wiley StatsRef: Statistics Reference Online_.
* [81] Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau, J., Gal, Y., and Precup, D. (2020). Invariant causal prediction for block MDPs. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119, pages 11214-11224. PMLR.