# Doubly Mild Generalization for Offline Reinforcement Learning

 Yixiu Mao\({}^{1}\), Qi Wang\({}^{1}\), Yun Qu\({}^{1}\), Yuhang Jiang\({}^{1}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Department of Automation, Tsinghua University

myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn

###### Abstract

Offline Reinforcement Learning (RL) suffers from the extrapolation error and value overestimation. From a generalization perspective, this issue can be attributed to the over-generalization of value functions or policies towards out-of-distribution (OOD) actions. Significant efforts have been devoted to mitigating such generalization, and recent in-sample learning approaches have further succeeded in entirely eschewing it. Nevertheless, we show that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation. The former refers to selecting actions in a close neighborhood of the dataset to maximize the Q values. Even so, the potential erroneous generalization can still be propagated, accumulated, and exacerbated by bootstrapping. In light of this, the latter concept is introduced to mitigate the generalization propagation without impeding the propagation of RL learning signals. Theoretically, DMG guarantees better performance than the in-sample optimal policy in the oracle generalization scenario. Even under worst-case generalization, DMG can still control value overestimation at a certain level and lower bound the performance. Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning and attains strong online fine-tuning performance.

## 1 Introduction

Reinforcement learning (RL) aims to solve sequential decision-making problems and has garnered significant attention in recent years [53, 67, 74, 63, 12]. However, its practical applications encounter several challenges, such as risky exploration attempts [20] and time-consuming data collection phases [35]. Offline RL emerges as a promising paradigm to alleviate these challenges by learning without interaction with the environment [40, 42]. It eliminates the need for unsafe exploration and facilitates the utilization of pre-existing large-scale datasets [31, 48, 59].

However, offline RL suffers from the out-of-distribution (OOD) issue and extrapolation error [19]. From a generalization perspective, this well-known challenge can be regarded as a consequence of the over-generalization of value functions or policies towards OOD actions [47]. Specifically, the potential value over-estimation at OOD actions caused by intricate generalization is often improperly captured by the max operation [73]. This over-estimation will propagate to values of in-distribution samples through Bellman backups and further spread to values of OOD ones via generalization. In mitigating value overestimation caused by OOD actions, substantial efforts have been dedicated [19, 39, 38, 17] and recent advancements in in-sample learning have successfully formulated the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21] and extracted policies by weightedbehavior cloning [57; 80]. As a result, these algorithms completely eschew generalization and avoid the extrapolation error. Despite simplicity, this way can not take advantage of the generalization ability of neural networks, which could be beneficial for performance improvement. Until now, how to appropriately exploit generalization in offline RL remains a lasting issue.

This work demonstrates that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. For appropriate exploitation of mild generalization, we propose Doubly Mild Generalization (DMG) for offline RL, comprising (i) mild action generalization and (ii) mild generalization propagation. The former concept refers to choosing actions in the vicinity of the dataset to maximize the Q values. However, the mere utilization of mild action generalization still falls short in adequately circumventing potential erroneous generalization, which can be propagated, accumulated, and exacerbated through the process of bootstrapping. To address this, we propose a novel concept, mild generalization propagation, which involves reducing the generalization propagation while preserving the propagation of RL learning signals. Regarding DMG's implementation, this work presents a simple yet effective scheme. Specifically, we blend the mildly generalized max with the in-sample max in the Bellman target, where the former is achieved by actor-critic learning with regularization towards high-value in-sample actions, and the latter is accomplished using in-sample learning techniques such as expectile regression [37].

We conduct a thorough theoretical analysis of our approach DMG in both oracle and worst-case generalization scenarios. Under oracle generalization, DMG guarantees better performance than the in-sample optimal policy in the dataset [38; 37]. Even under worst-case generalization, DMG can still upper bound the overestimation of value functions and guarantee to output a safe policy with a performance lower bound. Empirically1, DMG achieves state-of-the-art performance on standard offline RL benchmarks [16], including Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG can seamlessly transition from offline to online learning and attain superior online fine-tuning performance.

Footnote 1: Our code is available at https://github.com/maoyixiu/DMG.

## 2 Preliminaries

Rl.The environment in RL is mostly characterized as a Markov decision process (MDP), which can be represented as a tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,\gamma,d_{0})\), comprising the state space \(\mathcal{S}\), action space \(\mathcal{A}\), transition dynamics \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\), reward function \(R:\mathcal{S}\times\mathcal{A}\rightarrow[0,R_{\max}]\), discount factor \(\gamma\in[0,1)\), and initial state distribution \(d_{0}\)[70]. The goal of RL is to find a policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) that can maximize the expected discounted return, denoted as \(J(\pi)\):

\[J(\pi)=\mathbb{E}_{s_{0}\sim d_{0},a_{t}\sim\pi(\cdot|s_{t}),s_{t+1}\sim P( \cdot|s_{t},a_{t})}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})\right].\] (1)

For any policy \(\pi\), we define the value function as \(V^{\pi}(s)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})| s_{0}=s\right]\) and the state-action value function (\(Q\)-value function) as \(Q^{\pi}(s,a)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t} )|s_{0}=s,a_{0}=a\right]\).

Offline RL.Distinguished from traditional online RL training, offline RL handles a static dataset of transitions \(\mathcal{D}=\{(s_{i},a_{i},r_{i},s^{\prime}_{i})\}_{i=0}^{n-1}\) and seeks an optimal policy without any additional data collection [40; 42]. We use \(\hat{\beta}(a|s)\) to denote the empirical behavior policy observed in \(\mathcal{D}\), which depicts the conditional distributions in the dataset [19]. Ordinary approximate dynamic programming methods minimize temporal difference error, according to the following loss [70]:

\[L_{TD}(\theta)=\mathbb{E}_{(s,a,s^{\prime})\sim\mathcal{D}}\left[(Q_{\theta}( s,a)-R(s,a)-\gamma\max_{a^{\prime}}Q_{\theta^{\prime}}(s^{\prime},a^{ \prime}))^{2}\right],\] (2)

where \(\pi_{\phi}\) is a parameterized policy, \(Q_{\theta}(s,a)\) is a parameterized \(Q\) function, and \(Q_{\theta^{\prime}}(s,a)\) is a target \(Q\) function whose parameters are updated via Polyak averaging [53].

## 3 Doubly Mild Generalization for Offline RL

This section discusses the strategy to appropriately exploit generalization in offline RL. In Section 3.1, we introduce a formal perspective on how generalization impacts offline RL and discuss the issues of over-generalization and non-generalization. Subsequently, we propose the DMG concept, comprising mild action generalization and mild generalization propagation in Section 3.2. Following this, we conduct a comprehensive analysis of DMG in both oracle generalization (Section 3.3) and worst-case generalization scenarios (Section 3.4). Finally, we present the practical algorithm in Section 3.5.

### Generalization Issues in Offline RL

Offline RL training typically involves a complex interaction between Bellman backup and generalization [47]. Offline RL algorithms vary in backup mechanisms to train the Q function. Here we denote a generic form of Bellman backup as \(\mathcal{T}_{u}\), where \(u\) is a distribution in the action space.

\[\mathcal{T}_{u}Q(s,a):=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)} \left[\max_{a^{\prime}\sim u(\cdot|s^{\prime})}Q(s^{\prime},a^{\prime})\right]\] (3)

During offline training, this backup is exclusively executed on \((s,a)\in\mathcal{D}\), and the values of \((s,a)\notin\mathcal{D}\) are influenced solely via generalization. A crucial aspect is that \((s^{\prime},a^{\prime})\) in the Bellman target can be absent from the dataset \(\mathcal{D}\), depending on the choice of \(u\). As a result, Bellman backup and generalization exhibit an intricate interaction: the backups on \((s,a)\in\mathcal{D}\) impact the values of \((s,a)\notin\mathcal{D}\) via generalization; the values of \((s,a)\notin\mathcal{D}\) participates in the computation of Bellman target, thereby affecting the values of \((s,a)\in\mathcal{D}\).

This interaction poses a key challenge in offline RL, value overestimation. The potential overestimation of values of \((s,a)\notin\mathcal{D}\), induced by intricate generalization, tends to be improperly captured by the max operation, a phenomenon known as maximization bias [73]. This overestimation propagates to values of \((s,a)\in\mathcal{D}\) through backups and further extends to values of \((s,a)\notin\mathcal{D}\) via generalization. This cyclic process consistently amplifies value overestimation, potentially resulting in value divergence. The crux of this detrimental process can be summarized as **over-generalization**.

To address value overestimation, recent advancements in the field have introduced a paradigm known as in-sample learning, which formulates the Bellman target solely with the actions present in the dataset [37; 85; 92; 88; 21]. Its effect is equivalent to choosing \(u\) in \(\mathcal{T}_{u}\) to be exactly \(\hat{\beta}\), i.e., the empirical behavior policy observed in the dataset. Following in-sample value learning, policies are extracted from the learned Q functions using weighted behavior cloning [57; 9; 55]. By entirely eschewing generalization in offline RL training, they effectively avoid the extrapolation error [19], a strategy we term **non-generalization**. However, the ability to generalize is a critical factor contributing to the extensive utilization of neural networks [41]. In this sense, in-sample learning methods seem too conservative without utilizing generalization, particularly when the offline datasets do not cover the optimal actions in large or continuous spaces.

### Doubly Mild Generalization

The following focuses on the appropriate exploitation of generalization in offline RL.

We start by analyzing the generalization effect under the generic backup operator \(\mathcal{T}_{u}\). We consider a straightforward scenario, where \(Q_{\theta}\) is updated to \(Q_{\theta^{\prime}}\) by one gradient step on a single \((s,a)\in\mathcal{D}\) with learning rate \(\alpha\). We characterize the resulting generalization effect on any \((s,\tilde{a})\notin\mathcal{D}^{2}\) as follows.

**Theorem 1** (Informal).: _Under certain continuity conditions, the following equation holds when the learning rate \(\alpha\) is sufficiently small and \(\tilde{a}\) is sufficiently close to \(a\):_

\[Q_{\theta^{\prime}}(s,\tilde{a})=Q_{\theta}(s,\tilde{a})+C_{1}\left(\mathcal{T }_{u}Q_{\theta}(s,\tilde{a})-Q_{\theta}(s,\tilde{a})+C_{2}\|\tilde{a}-a\| \right)+\mathcal{O}\left(\|\theta^{\prime}-\theta\|^{2}\right)\] (4)

_where \(C_{1}\in[0,1]\) and \(C_{2}\) is a bounded constant._

The formal theorem and all proofs are deferred to Appendix B.

Note that Eq. (4) is the update of the parametric Q function (\(Q_{\theta}\to Q_{\theta^{\prime}}\)) at state-action pairs \((s,\tilde{a})\notin\mathcal{D}\), which is exclusively caused by generalization. If \(\tilde{a}\) is within a close neighborhood of \(a\), then \(C_{2}\|\tilde{a}-a\|\) is small. Moreover, as \(C_{1}\in[0,1]\), Eq. (4) approximates an update towards the true objective \(\mathcal{T}_{u}Q_{\theta}(s,\tilde{a})\), as if \(Q_{\theta}(s,\tilde{a})\) is updated by a true gradient step at \((s,\tilde{a})\notin\mathcal{D}\). Therefore,Theorem 1 shows that, under certain continuity conditions, Q functions can generalize well and approximate true updates in a close neighborhood of samples in the dataset. This implies that mild generalizations beyond the dataset can be leveraged to potentially pursue better performance. Inspired by Theorem 1, we define a mildly generalized policy \(\hat{\beta}\) as follows.

**Definition 1** (Mildly generalized policy).: _Policy \(\tilde{\beta}\) is termed a mildly generalized policy if it satisfies_

\[\mathrm{supp}(\hat{\beta}(\cdot|s))\subseteq\mathrm{supp}(\tilde{\beta}( \cdot|s)),\ \ \text{and}\ \ \max_{a_{1}\sim\hat{\beta}(\cdot|s)}\min_{a_{2}\sim\hat{\beta}(\cdot|s)}\|a_{1} -a_{2}\|\leq\epsilon_{a},\] (5)

_where \(\hat{\beta}\) is the empirical behavior policy observed in the offline dataset._

It means that \(\tilde{\beta}\) has a wider support than \(\hat{\beta}\) (the dataset), and for any \(a_{1}\sim\tilde{\beta}(\cdot|s)\), we can find \(a_{2}\sim\hat{\beta}(\cdot|s)\) (in dataset) such that \(\|a_{1}-a_{2}\|\leq\epsilon_{a}\). In other words, the generalization of \(\tilde{\beta}\) beyond the dataset is bounded by \(\epsilon_{a}\) when measured in the action space distance. According to Theorem 1, there is a high chance that \(Q_{\theta}\) can generalize well in this mild generalization area \(\tilde{\beta}(a|s)>0\).

However, even in this mild generalization area, it is inevitable that the learned value function will incur some degree of generalization error. The possible erroneous generalization can still be propagated and exacerbated by value bootstrapping as discussed in Section 3.1. To this end, we introduce an additional level of mild generalization, termed mild generalization propagation, and propose a novel Doubly Mildly Generalization (DMG) operator as follows.

**Definition 2**.: _The Doubly Mild Generalization (DMG) operator is defined as_

\[\mathcal{T}_{\mathrm{DMG}}Q(s,a):=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P( \cdot|s,a)}\left[\lambda\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}Q(s ^{\prime},a^{\prime})+(1-\lambda)\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{ \prime})}Q(s^{\prime},a^{\prime})\right]\] (6)

_where \(\hat{\beta}\) is the empirical behavior policy in the dataset and \(\tilde{\beta}\) is a mildly generalized policy._

Note that in typical offline RL algorithms, extrapolation error and value overestimation caused by erroneous generalization are propagated through bootstrapping, and the discount factor of this process is \(\gamma\). DMG reduces this discount factor to \(\lambda\gamma\), mitigating the amplification of value overestimation. On the other hand, in contrast to in-sample methods, DMG allows mild generalization, utilizing the generalization ability of neural networks to seek better performance, as Theorem 1 suggests that value functions are highly likely to generalize well in the mild generalization area.

To summarize, the generalization of DMG is mild in two aspects: (i) **mild action generalization**: based on the mildly generalized policy \(\tilde{\beta}\), which generalizes beyond \(\hat{\beta}\), DMG selects actions in a close neighborhood of the dataset to maximize the Q values in the first part of the Bellman target; and (ii) **mild generalization propagation**: DMG mitigates the generalization propagation without hindering the propagation of RL learning signals by blending the mildly generalized max with the in-sample max in the Bellman target. This reduces the discount factor through which generalization propagates, mitigating the amplification of value overestimation caused by bootstrapping.

To support the above claims, we provide a comprehensive analysis of DMG in both oracle and worst-case generalization scenarios, with particular emphasis on value estimation and performance.

### Oracle Generalization

This section conducts analyses under the assumption that the learned value functions can achieve oracle generalization in the mild generalization area \(\tilde{\beta}(a|s)>0\), formally defined as follows.

**Assumption 1** (Oracle generalization).: _The generalization of learned Q functions in the mild generalization area \(\tilde{\beta}(a|s)>0\) reflects the true value updates according to \(\mathcal{T}_{\mathrm{DMG}}\)._

The mild generalization area \(\tilde{\beta}(a|s)>0\) may contain some points outside the offline dataset, and \(\mathcal{T}_{\mathrm{DMG}}\) might query Q values of such points. This assumption assumes that the generalization at such points reflects the true value updates according to \(\mathcal{T}_{\mathrm{DMG}}\). The rationale for such an assumption comes from Theorem 1, which characterizes the generalization effect of value functions in the mild generalization area. Now we analyze the dynamic programming properties of the operators \(\mathcal{T}_{\mathrm{DMG}}\) and \(\mathcal{T}_{\mathrm{In}}\), where \(\mathcal{T}_{\mathrm{In}}\) is the in-sample Q learning operator [37; 88; 21] defined as follows.

**Definition 3**.: _The In-sample Q Learning operator [37] is defined as_

\[\mathcal{T}_{\mathrm{In}}Q(s,a):=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot |s,a)}\left[\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}Q(s^{\prime},a^{ \prime})\right]\] (7)

_where \(\hat{\beta}\) is the empirical behavior policy in the dataset._

**Lemma 1**.: \(\mathcal{T}_{\mathrm{In}}\) _is a \(\gamma\)-contraction operator in the in-sample area \(\hat{\beta}(a|s)>0\) under the \(\mathcal{L}_{\infty}\) norm._

Following Lemma 1, we denote the fixed point of \(\mathcal{T}_{\mathrm{in}}\) as \(Q^{*}_{\mathrm{In}}\), and its induced policy as \(\pi^{*}_{\mathrm{In}}\). Here \(Q^{*}_{\mathrm{In}}\) is known as the in-sample optimal value function [37], which is the value function of the in-sample optimal policy \(\pi^{*}_{\mathrm{In}}\). We refer readers to [37; 38; 88] for more discussions on the in-sample optimality.

Now we present the theoretical properties of DMG for comparison.

**Theorem 2** (Contraction).: _Under Assumption 1, \(\mathcal{T}_{\mathrm{DMG}}\) is a \(\gamma\)-contraction operator in the mild generalization area \(\tilde{\beta}(a|s)>0\) under the \(\mathcal{L}_{\infty}\) norm. Therefore, by repeatedly applying \(\mathcal{T}_{\mathrm{DMG}}\), any initial Q function can converge to the unique fixed point \(Q^{*}_{\mathrm{DMG}}\)._

We denote the induced policy of \(Q^{*}_{\mathrm{DMG}}\) as \(\pi^{*}_{\mathrm{DMG}}\), whose performance is guaranteed as follows.

**Theorem 3** (Performance).: _Under Assumption 1, the value functions of \(\pi^{*}_{\mathrm{DMG}}\) and \(\pi^{*}_{\mathrm{In}}\) satisfy:_

\[V^{\pi^{*}_{\mathrm{DMG}}}(s)\geq V^{\pi^{*}_{\mathrm{In}}}(s),\quad\forall s \in\mathcal{D}.\] (8)

Theorem 3 indicates that the policy learned by DMG can achieve better performance than the in-sample optimal policy under the oracle generalization condition.

### Worst-case Generalization

This section turns to the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area \(\tilde{\beta}(a|s)>0\). In other words, this section considers that \(\mathcal{T}_{\mathrm{DMG}}\) is only defined in the in-sample area \(\hat{\beta}(a|s)>0\) and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation \(\hat{\mathcal{T}}_{\mathrm{DMG}}\) to tell the difference.

We make continuity assumptions about the learned Q function and the transition dynamics.

**Assumption 2** (Lipschitz \(Q\)).: _The learned Q function is \(K_{Q}\)-Lipschitz. \(\forall s\sim\mathcal{D}\), \(\forall a_{1},a_{2}\sim\mathcal{A}\), \(|Q(s,a_{1})-Q(s,a_{2})|\leq K_{Q}\|a_{1}-a_{2}\|\)_

**Assumption 3** (Lipschitz \(P\)).: _The transition dynamics \(P\) is \(K_{P}\)-Lipschitz. \(\forall s,s^{\prime}\sim\mathcal{S}\), \(\forall a_{1},a_{2}\sim\mathcal{A}\), \(|P(s^{\prime}|s,a_{1})-P(s^{\prime}|s,a_{2})|\leq K_{P}\|a_{1}-a_{2}\|\)_

For Assumption 2, a continuous learned Q function is particularly necessary for analyzing value function generalization and can be relatively easily satisfied using neural networks or linear models [24]. Assumption 3 is also a common assumption in theoretical studies of RL [13; 87; 61].

Now we consider the iteration starting from arbitrary function \(Q^{0}\): \(\hat{Q}^{k}_{\mathrm{DMG}}=\hat{\mathcal{T}}_{\mathrm{DMG}}\hat{Q}^{k-1}_{ \mathrm{DMG}}\) and \(Q^{k}_{\mathrm{In}}=\mathcal{T}_{\mathrm{In}}Q^{k-1}_{\mathrm{In}}\), \(\forall k\in\mathbb{Z}^{+}\). The possible value of \(\hat{Q}^{k}_{\mathrm{DMG}}\) is bounded by the following results.

**Theorem 4** (Limited overestimation).: _Under Assumption 2, the learned Q function of DMG by iterating \(\hat{\mathcal{T}}_{\mathrm{DMG}}\) satisfies the following inequality_

\[Q^{k}_{\mathrm{In}}(s,a)\leq\hat{Q}^{k}_{\mathrm{DMG}}(s,a)\leq Q^{k}_{ \mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{k}),\;\forall s,a\sim\mathcal{D},\;\forall k\in\mathbb{Z}^{+}.\] (9)

Since in-sample training eliminates the extrapolation error [37; 92], \(Q^{k}_{\mathrm{In}}\) can be considered a relatively accurate estimate [37]. Therefore, Theorem 4 suggests that DMG exhibits limited value overestimation under the worst-case generalization scenario. Moreover, the bound becomes tighter as \(\epsilon_{a}\) decreases (milder action generalization) and \(\lambda\) decreases (milder generalization propagation). This is consistent with our intuitions in Section 3.2.

Finally, we show in Theorem 5 that even under worst-case generalization, DMG guarantees to output a safe policy with a performance lower bound.

**Theorem 5** (Performance lower bound).: _Let \(\hat{\pi}_{\mathrm{DMG}}\) be the learned policy of DMG by iterating \(\mathcal{T}_{\mathrm{DMG}}\), \(\pi^{*}\) be the optimal policy, and \(\epsilon_{\mathcal{D}}\) be the inherent performance gap of the in-sample optimal policy \(\epsilon_{\mathcal{D}}:=J(\pi^{*})-J(\pi^{*}_{\text{in}})\). Under Assumptions 2 and 3, for sufficiently small \(\epsilon_{a}\), we have_

\[J(\hat{\pi}_{\mathrm{DMG}})\geq J(\pi^{*})-\frac{CK_{P}R_{\max}}{1-\gamma} \epsilon_{a}-\epsilon_{\mathcal{D}}.\] (10)

_where \(C\) is a positive constant._

### Practical Algorithm

This section puts DMG into implementation and presents a simple yet effective practical algorithm. The algorithm comprises the following networks: policy \(\pi_{\phi}\), target policy \(\pi_{\phi^{\prime}}\), \(Q\) network \(Q_{\theta}\), target \(Q\) network \(Q_{\theta^{\prime}}\), and \(V\) network \(V_{\psi}\).

Policy learning.Practically, we expect DMG to exhibit a tendency towards mild generalization around good actions in the dataset. To this end, we first consider reshaping the empirical behavior policy \(\hat{\beta}\) to be skewed towards actions with high advantage values \(\hat{\beta}^{*}(a|s)\propto\hat{\beta}(a|s)\exp(A(s,a))\). Then we enforce the proximity between the trained policy and the reshaped behavior policy to constrain the generalization area. We define the generalization set \(\Pi_{G}\) as follows.

\[\Pi_{G}=\{\pi\mid\mathrm{KL}(\hat{\beta}^{*}(\cdot|s)\|\pi(\cdot|s))\leq\epsilon\}\] (11)

Note that forward KL allows \(\pi\) to select actions outside the support of \(\hat{\beta}^{*}\), enabling \(\Pi_{G}\) to generalize beyond the actions in the dataset. With \(\Pi_{G}\) defined, the next step is to compute the maximal \(Q\) within \(\Pi_{G}\). To accomplish this, we adopt Actor-Critic style training [70] for this part.

\[\max_{\phi}\mathbb{E}_{s\sim\mathcal{D},a\sim\pi_{\phi}(\cdot|s)}Q_{\theta}(s, a),\quad s.t.\ \pi_{\phi}\in\Pi_{G}\] (12)

By treating the constraint term as a penalty, we maximize the following objective.

\[\max_{\phi}\mathbb{E}_{s\sim\mathcal{D},a\sim\pi_{\phi}(\cdot|s)}Q_{\theta}(s,a)-\nu\mathbb{E}_{s\sim\mathcal{D}}\mathrm{KL}(\hat{\beta}^{*}(\cdot|s)\|\pi _{\phi}(\cdot|s))\] (13)

Through straightforward derivations, Eq. (13) is equivalent to the following policy training objective.

\[J_{\pi}(\phi)=\mathbb{E}_{s\sim\mathcal{D},a\sim\pi_{\phi}(\cdot|s)}Q_{\theta }(s,a)-\nu\mathbb{E}_{(s,a)\sim\mathcal{D}}\left[\exp(\alpha(Q_{\theta^{ \prime}}(s,a)-V_{\psi}(s)))\log\pi_{\phi}(a|s)\right]\] (14)

where \(\alpha\) is an inverse temperature and \(Q_{\theta^{\prime}}(s,a)-V_{\psi}(s)\) computes the advantage function \(A(s,a)\).

Value learning.Now we turn to the implementation of the \(\mathcal{T}_{\mathrm{DMG}}\) operator for training value functions. By introducing the aforementioned policy, we can substitute \(\max_{a\sim\hat{\beta}}\) in \(\mathcal{T}_{\mathrm{DMG}}\) with \(\mathbb{E}_{a\sim\pi}\). Regarding \(\max_{a\sim\hat{\beta}}\) in \(\mathcal{T}_{\mathrm{DMG}}\), any in-sample learning techniques can be employed to compute the in-sample maximum [37, 88, 85, 21]. In particular, based on IQL [37], we perform expectile regression.

\[L_{V}(\psi)=\mathbb{E}_{(s,a)\sim\mathcal{D}}\left[L_{2}^{\tau}\left(Q_{\theta ^{\prime}}(s,a)-V_{\psi}(s)\right)\right]\] (15)

where \(L_{2}^{\tau}(u)=|\tau-\mathbbm{1}(u<0)|u^{2}\) and \(\tau\in(0,1)\). For \(\tau\approx 1\), \(V_{\psi}\) can capture the in-sample maximal \(Q\)[37]. Finally, we have the following value training loss.

\[L_{Q}(\theta)=\mathbb{E}_{(s,a,s^{\prime})\sim\mathcal{D}}\left[\left(Q_{\theta }(s,a)-R(s,a)-\gamma\lambda\mathbb{E}_{a^{\prime}\sim\pi_{\phi^{\prime}}}Q_{ \theta^{\prime}}(s^{\prime},a^{\prime})-\gamma(1-\lambda)V_{\psi}(s^{\prime}) \right)^{2}\right]\] (16)

Overall algorithm.Integrating all components, we present our practical algorithm in Algorithm 1.

Discussions and Related Work

Summary of offline RL work from a generalization perspective.As analyzed above, DMG is featured in both mild action generalization and mild generalization propagation. Within the actor-critic framework upon which most offline RL algorithms are built, these two aspects correspond to the policy and value training phases, respectively. Action generalization concerns whether the policy training intentionally selects actions beyond the dataset to maximize Q values, while generalization propagation involves whether value training propagates generalization through bootstrapping. Table 1 presents a clear comparison of offline RL works in this generalization view. The table shows one representative method of each category and we elaborate on others as follows.

Concerning policy learning, AWR [57], AWAC [55], CRR [80], \(10\%\) BC [8], IQL [37], and other works such as [78, 9, 66, 21, 88] extract policies through weighted or filtered behavior cloning, thereby lacking intentional action generalization to maximize Q values beyond the dataset. Typical policy-regularized offline RL methods like TD3BC [17], BRAC [84], BEAR [38], SPOT [83], and others such as [79, 61, 72] introduce regularization terms to Q maximization objectives to regularize the trained policy towards the behavior policy and allows mild action generalization. Online RL algorithms like TD3 [18] and SAC [27] have no constraints and maximize Q values in the entire action space, corresponding to full action generalization. Regarding value training, in-sample learning methods including OneStep RL [7], IQL [37], InAC [85], IAC [92], \(\chi\)QL [21], and SQL [88] completely avoid generalization propagation and accumulation via bootstrapping, whereas typical offline and online RL approaches allow full generalization propagation through bootstrapping. In the proposed approach DMG, generalization is mild in both aspects.

Recently, Ma et al. [47] have also drawn attention to generalization in offline RL and the issue of over-generalization. They mitigate over-generalization from a representation perspective, differentiating between the representations of in-sample and OOD state-action pairs. Lyu et al. [44] argue that conventional value penalization like CQL [39] tends to harm the generalization of value functions and hinder performance improvement. They propose mild value penalization to mitigate the detrimental effects of value penalization on generalization.

Connection to heuristic blending approaches.Our approach also relates to the framework of blending heuristics into bootstrapping [10, 81, 71, 28, 82, 22]. In offline RL, HUBL [22] blends Monte-Carlo returns into bootstrapping and acts as a data relabeling step, which reduces the degree of bootstrapping and thereby increases its performance. In contrast, DMG blends the in-sample maximal values into the bootstrapping operator. DMG does not reduce the discount for RL learning but reduces the discount for generalization propagation.

For extended discussions on related work, please refer to Appendix A.

## 5 Experiments

In this section, we conduct several experiments to justify the validity of the proposed method DMG. Experimental details and extended results are provided in Appendices C and D, respectively.

### Main Results on Offline RL Benchmarks

Tasks.We evaluate the proposed approach on Gym-MuJoCo locomotion domains and challenging AntMaze domains in D4RL [16]. The latter involves sparse-reward tasks and necessitates "stitching" fragments of suboptimal trajectories traveling undirectedly to find a path to the goal of the maze.

Baselines.Our offline RL baselines include both typical bootstrapping methods and in-sample learning approaches. For the former, we compare to BCQ [19], BEAR [38], AWAC [55], TD3BC [17],

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & IQL & AWAC & TD3BC & TD3 & DMG (Ours) \\ \hline Action generalization & _none_ & _none_ & _mild_ & _full_ & _mild_ \\ \hline Generalization propagation & _none_ & _full_ & _full_ & _full_ & _mild_ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of offline RL work from the generalization perspective.

and CQL [39]. For the latter, we compare to BC [58], OneStep RL [7], IQL [37], \(\mathcal{X}\)QL [21], and SQL [88]. We also include the sequence-modeling method Decision Transformer (DT) [8].

Comparison with baselines.Aggregated results are displayed in Table 2. On the Gym locomotion tasks, DMG outperforms prior methods on most tasks and achieves the highest total score. On the much more challenging AntMaze tasks, DMG outperforms all the baselines by a large margin, especially in the most difficult large mazes. For detailed learning curves, please refer to Appendix D.3. According to [56], we also report the results of DMG over more random seeds in Appendix D.2.

Runtime.We test the runtime of DMG and other baselines on a GeForce RTX 3090. As shown in Appendix D.1, the runtime of DMG is comparable to that of the fastest offline RL algorithm TD3BC.

### Performance Improvement over In-sample Learning Approaches

DMG can be combined with various in-sample learning approaches. Besides IQL [37], we also apply DMG to two recent state-of-the-art in-sample algorithms, \(\mathcal{X}\)QL [21] and SQL [88]. As shown in Table 3 (and Table 2), DMG consistently and substantially improves upon these in-sample methods, particularly on sub-optimal datasets where generalization plays a crucial role in the pursuit of a better policy. This provides compelling empirical evidence that the performance of in-sample methods is largely confined by eschewing generalization beyond the dataset, while DMG effectively exploits generalization, achieving significantly improved performance across tasks.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline Dataset-v2 & BC & BCQ & BEAR & DT & AWAC & OneStep & TD3BC & CQL & IQL & DMG (Ours) \\ \hline halfcheetah-m & 42.0 & 46.6 & 43.0 & 42.6 & 47.9 & 50.4 & 48.3 & 47.0 & 47.4 & **54.9\(\pm\)0.2** \\ hopper-m & 56.2 & 59.4 & 51.8 & 67.6 & 59.8 & 87.5 & 59.3 & 53.0 & 66.2 & **100.6\(\pm\)1.9** \\ walker2d-m & 71.0 & 71.8 & -0.2 & 74.0 & 83.1 & 84.8 & 83.7 & 73.3 & 78.3 & **92.4\(\pm\)2.7** \\ halfcheetah-m-r & 36.4 & 42.2 & 36.3 & 36.6 & 44.8 & 42.7 & 44.6 & 45.5 & 44.2 & **51.4\(\pm\)0.3** \\ hopper-m-r & 21.8 & 60.9 & 52.2 & 82.7 & 69.8 & 98.5 & 60.9 & 88.7 & 94.7 & **101.9\(\pm\)1.4** \\ walker2d-m-r & 24.9 & 57.0 & 7.0 & 66.6 & 78.1 & 61.7 & 81.8 & 81.8 & 73.8 & **89.7\(\pm\)5.0** \\ halfcheetah-m-e & 59.6 & **95.4** & 46.0 & 86.8 & 64.9 & 75.1 & 90.7 & 75.6 & 86.7 & 91.1\(\pm\)4.2 \\ hopper-m-e & 51.7 & 106.9 & 50.6 & 107.6 & 100.1 & 108.6 & 98.0 & 105.6 & 91.5 & **110.4\(\pm\)3.4** \\ walker2d-m-e & 101.2 & 107.7 & 22.1 & 108.1 & 110.0 & 111.3 & 110.1 & 107.9 & 109.6 & **114.4\(\pm\)0.7** \\ halfcheetah-e & 92.9 & 89.9 & 92.7 & 87.7 & 81.7 & 88.2 & **96.7** & **96.3** & **95.0** & **95.9\(\pm\)0.3** \\ hopper-e & **110.9** & 109.0 & 54.6 & 94.2 & **109.5** & 106.9 & 107.8 & 96.5 & **109.4** & **111.5\(\pm\)2.2** \\ walker2d-e & 107.7 & 106.3 & 106.6 & 108.3 & 110.1 & 110.7 & 110.2 & 108.5 & 109.9 & **114.7\(\pm\)0.4** \\ halfcheetah-r & 2.6 & 2.2 & 2.3 & 2.2 & 6.1 & 2.3 & 11.0 & 17.5 & 13.1 & **28.8\(\pm\)1.3** \\ hopper-r & 4.1 & 7.8 & 3.9 & 5.4 & 9.2 & 5.6 & 8.5 & 7.9 & 7.9 & **20.4\(\pm\)10.4** \\ walker2d-r & 1.2 & 4.9 & **12.8** & 2.2 & 0.2 & 6.9 & 1.6 & 5.1 & 5.4 & 4.8\(\pm\)2.2 \\ locomotion total & 784.2 & 968.0 & 581.7 & 972.6 & 975.3 & 1041.2 & 1013.2 & 1010.2 & 1033.1 & **1182.8** \\ \hline antmaze-u & 66.8 & 78.9 & 73.0 & 54.2 & 80.0 & 54.0 & 73.0 & 82.6 & 89.6 & **92.4\(\pm\)1.8** \\ antmaze-u-d & 56.8 & 55.0 & 61.0 & 41.2 & 52.0 & 57.8 & 47.0 & 10.2 & 65.6 & **75.4\(\pm\)8.1** \\ antmaze-m-p & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 59.0 & 76.4 & **80.2\(\pm\)5.1** \\ antmaze-m-d & 0.0 & 0.0 & 8.0 & 0.0 & 0.2 & 0.6 & 0.2 & 46.6 & 72.8 & **77.2\(\pm\)6.1** \\ antmaze-l-p & 0.0 & 6.7 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 16.4 & 42.0 & **55.4\(\pm\)6.2** \\ antmaze-l-d & 0.0 & 2.2 & 0.0 & 0.0 & 0.0 & 0.2 & 0.0 & 3.2 & 46.0 & **58.8\(\pm\)4.5** \\ \hline antmaze total & 123.6 & 142.8 & 142.0 & 95.4 & 132.2 & 112.6 & 120.2 & 218.0 & 392.4 & **439.4** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Averaged normalized scores on Gym locomotion and Antmaze tasks over five random seeds. m = medium, m-r = medium-replay, m-e = medium-expert, e = expert, r = random; u = umaze, u-d = umaze-diverse, m-p = medium-play, m-d = medium-diverse, l-p= large-play, l-d = large-diverse.

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset-v2 & \(\mathcal{X}\)QL (+DMG) & SQL(+DMG) \\ \hline halfcheetah-m & 47.7 \(\rightarrow\)**55.3** & 48.3 \(\rightarrow\)**54.5** \\ hopper-m & 71.1 \(\rightarrow\)**90.1** & 75.5 \(\rightarrow\)**97.7** \\ walker2d-m & 81.5 \(\rightarrow\)**88.7** & 84.2 \(\rightarrow\)**89.8** \\ halfcheetah-m-r & 44.8 \(\rightarrow\)**51.1** & 44.8 \(\rightarrow\)**51.8** \\ hopper-m-r & 97.3 \(\rightarrow\)**102.5** & **101.7 \(\rightarrow\)**101.8** \\ walker2d-m-r & 75.9 \(\rightarrow\)**90.0** & 77.2 \(\rightarrow\)**95.2** \\ halfcheetah-m-e & 89.8 \(\rightarrow\)**92.5** & **94.0 \(\rightarrow\)**93.5** \\ hopper-m-e & 107.1 \(\rightarrow\)**111.1** & **111.8 \(\rightarrow\)**110.4** \\ walker2d-m-e & 1101.1 \(\rightarrow\)**111.3** & **110.0 \(\rightarrow\)**109.6** \\ \hline total & 725.3 \(\rightarrow\)**792.7** & 747.5 \(\rightarrow\)**804.2** \\ \hline \hline \end{tabular}
\end{table}
Table 3: DMG combined with various in-sample approaches, showing averaged scores over 5 seeds.

### Ablation Study for Performance and Value Estimation

Mixture coefficient \(\lambda\).The mixture coefficient \(\lambda\) controls the extent of generalization propagation. We fix \(\nu=0.1\) and vary \(\lambda\in[0,1]\), presenting the learned Q values and performance on several tasks in Figure 1. As \(\lambda\) increases, DMG enables increased generalization propagation through bootstrapping, and the learned Q values become larger and probably diverge. A moderate \(\lambda\) (mild generalization propagation) is crucial for achieving strong performance across datasets. Under the same degree of action generalization, mild generalization propagation effectively suppresses value overestimation, facilitating more stable policy learning.

Penalty coefficient \(\nu\).The penalty coefficient \(\nu\) regulates the degree of action generalization. We fix \(\lambda=0.25\) and vary \(\nu\). The results are shown in Figure 2. As \(\nu\) decreases, DMG allows broader action generalization beyond the dataset, which results in higher learned values. Regarding performance, a moderate \(\nu\) (mild action generalization) is also crucial for achieving superior performance.

### Online Fine-tuning after Offline RL

Benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning. This is accomplished through a gradual enhancement of both action generalization and generalization propagation. Since IQL [37] has demonstrated superior online fine-tuning performance compared to previous methods [55; 39] in its paper, we follow the experimental setup of IQL and compare to IQL. We also train online RL algorithm TD3 [18] from scratch for comparison. We use the challenging AntMaze domains [16], given DMG's already high offline performance in Gym locomotion domains. Results are presented in Table 4. While online training from scratch fails in the challenging sparse reward AntMaze tasks, DMG initialized with offline pretraining succeeds in learning near-optimal policies, outperforming IQL by a significant margin. Please refer to Appendix C.2 for experimental details, and to Appendix D.4 for learning curves.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset-v2 & TD3 & IQL & DMG (Ours) \\ \hline antmaze-u & 0.0 & \(89.6\to 96.2\) & \(92.4\to\textbf{98.4}\) \\ antmaze-u-d & 0.0 & \(65.6\to 62.2\) & \(75.4\to\textbf{89.2}\) \\ antmaze-m-p & 0.0 & \(76.4\to 89.8\) & \(80.2\to\textbf{96.8}\) \\ antmaze-m-d & 0.0 & \(72.8\to 90.2\) & \(77.2\to\textbf{96.2}\) \\ antmaze-l-p & 0.0 & \(42.0\to 78.6\) & \(55.4\to\textbf{86.8}\) \\ antmaze-l-d & 0.0 & \(46.0\to 73.4\) & \(58.8\to\textbf{89.0}\) \\ \hline antmaze total & 0.0 & \(392.4\to 490.4\) & \(439.4\to\textbf{556.4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Online fine-tuning results on AntMaze tasks, showing normalized scores of offline training and 1M steps online fine-tuning, averaged over 5 seeds.

Figure 1: Performance and Q values of DMG with varying mixture coefficient \(\lambda\) over 5 random seeds. The crosses \(\times\) mean that the value functions diverge in several seeds. As \(\lambda\) increases, DMG enables stronger generalization propagation, resulting in higher and probably divergent learned Q values. Mild generalization propagation plays a crucial role in achieving strong performance.

Figure 2: Performance and Q values of DMG with varying penalty coefficient \(\nu\) over 5 random seeds. As \(\nu\) decreases, DMG allows broader action generalization, leading to larger learned Q values. Mild action generalization is also critical for attaining superior performance.

Conclusion and Limitations

This work scrutinizes offline RL through the lens of generalization and proposes DMG, comprising mild action generalization and mild generalization propagation, to exploit generalization in offline RL appropriately. We theoretically analyze DMG in oracle and worst-case generalization scenarios, and empirically demonstrate its SOTA performance in offline training and online fine-tuning experiments.

While our work contributes valuable insights, it also has limitations. The DMG principle is shown to be effective across most scenarios. However, when the function approximator employed is highly compatible with a specific task setting, the learned value functions may generalize well in the entire action space. In such case, DMG may underperform full generalization methods due to conservatism.

## Acknowledgment

We thank the anonymous reviewers for feedback on an early version of this paper. This work was supported by the National Key R&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61827804.

## References

* Achiam et al. [2017] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017.
* An et al. [2021] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in neural information processing systems_, 34:7436-7447, 2021.
* Bai et al. [2022] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Y4cs1Z3Hnql.
* Beck et al. [2023] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. _arXiv preprint arXiv:2301.08028_, 2023.
* Bose et al. [2024] Avinandan Bose, Simon Shaolei Du, and Maryam Fazel. Offline multi-task transfer rl with representational penalization. _arXiv preprint arXiv:2402.12570_, 2024.
* Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Brandfonbrener et al. [2021] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. _Advances in Neural Information Processing Systems_, 34:4933-4946, 2021.
* Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Chen et al. [2020] Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action imitation learning for batch deep reinforcement learning. _Advances in Neural Information Processing Systems_, 33:18353-18363, 2020.
* Cheng et al. [2021] Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided reinforcement learning. _Advances in Neural Information Processing Systems_, 34:13550-13563, 2021.
* Cheng et al. [2022] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In _International Conference on Machine Learning_, pages 3852-3878. PMLR, 2022.

* [12] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. _Nature_, 602(7897):414-419, 2022.
* [13] Francois Dufour and Tomas Prieto-Rumeau. Finite linear programming approximations of constrained discounted markov decision processes. _SIAM Journal on Control and Optimization_, 51(2):1298-1324, 2013.
* [14] Francois Dufour and Tomas Prieto-Rumeau. Approximation of average cost markov decision processes using empirical distributions and concentration inequalities. _Stochastics An International Journal of Probability and Stochastic Processes_, 87(2):273-307, 2015.
* [15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [16] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [17] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [18] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* [19] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, pages 2052-2062. PMLR, 2019.
* [20] Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_, 16(1):1437-1480, 2015.
* [21] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent RL without entropy. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=SJ0Lde3tRL.
* [22] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline RL by blending heuristics. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=MC10TLboP1.
* [23] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In _International Conference on Machine Learning_, pages 3682-3691. PMLR, 2021.
* [24] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. _Machine Learning_, 110:393-416, 2021.
* [25] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial Intelligence Review_, 55(2):895-943, 2022.
* [26] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022.
* [27] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [28] Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem using emphatic weightings. _Advances in neural information processing systems_, 31, 2018.

* Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_, 32, 2019.
* Jaques et al. [2019] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. _arXiv preprint arXiv:1907.00456_, 2019.
* Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* Kaiser et al. [2019] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. _arXiv preprint arXiv:1903.00374_, 2019.
* Kidambi et al. [2020] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. _Advances in neural information processing systems_, 33:21810-21823, 2020.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kober et al. [2013] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Kostrikov et al. [2021] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In _International Conference on Machine Learning_, pages 5774-5783. PMLR, 2021.
* Kostrikov et al. [2022] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=68n2s9ZJWF8.
* Kumar et al. [2019] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Lange et al. [2012] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. _Reinforcement learning: State-of-the-art_, pages 45-73, 2012.
* LeCun et al. [2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Lowe et al. [2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. _Advances in neural information processing systems_, 30, 2017.
* Lyu et al. [2022] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=VYYf6S67pQc.
* Ma et al. [2021] Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, and Bin Liang. Offline reinforcement learning with value-based episodic memory. _arXiv preprint arXiv:2110.09796_, 2021.

* [46] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. _Advances in Neural Information Processing Systems_, 34:19235-19247, 2021.
* [47] Yi Ma, Hongyao Tang, Dong Li, and Zhaopeng Meng. Reining generalization in offline reinforcement learning via representation distinction. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=mVywRIDN1.
* [48] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The oxford robotcar dataset. _The International Journal of Robotics Research_, 36(1):3-15, 2017.
* [49] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In _International Conference on Machine Learning_, pages 23829-23851. PMLR, 2023.
* [50] Yixiu Mao, Qi Wang, Chen Chen, Yun Qu, and Xiangyang Ji. Offline reinforcement learning with ood state correction and ood action suppression. _arXiv preprint arXiv:2410.19400_, 2024.
* [51] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [52] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=3hGNqp14WS.
* [53] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* [54] Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement learning: A survey. _Foundations and Trends(r) in Machine Learning_, 16(1):1-118, 2023.
* [55] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* [56] Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical design in reinforcement learning. _arXiv preprint arXiv:2304.01315_, 2023.
* [57] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [58] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. _Advances in neural information processing systems_, 1988.
* [59] Yun Qu, Boyuan Wang, Jianzhu Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Linc Liu, Junfeng Yang, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its offline reinforcement learning benchmarks. In _Thirty-seventh Conference on Neural Information Processing Systems Track on Datasets and Benchmarks_, 2023.
* [60] Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhu Shao, Yixiu Mao, Cheems Wang, Chang Liu, and Xiangyang Ji. Choices are more important than efforts: Llm enables efficient multi-agent exploration. _arXiv preprint arXiv:2410.02511_, 2024.
* [61] Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, and Yang Yu. Policy regularization with dataset constraint for offline reinforcement learning. In _International Conference on Machine Learning_, pages 28701-28717. PMLR, 2023.
* [62] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. _Journal of Machine Learning Research_, 21(178):1-51, 2020.

* Schrittwieser et al. [2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Shao et al. [2023] Jianzhur Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=62zm04mv8X.
* Shao et al. [2023] Jianzhur Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 30776-30793. PMLR, 2023.
* Siegel et al. [2020] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rke7geHtwH.
* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Sun et al. [2023] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In _International Conference on Machine Learning_, pages 33177-33194. PMLR, 2023.
* Sutton [1991] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _ACM Sigart Bulletin_, 2(4):160-163, 1991.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Sutton et al. [2016] Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. _Journal of Machine Learning Research_, 17(73):1-29, 2016.
* Tarasov et al. [2024] Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Van Hasselt et al. [2016] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30, 2016.
* Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* Wang et al. [2024] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation from adversarially explicit task distribution generation. _arXiv preprint arXiv:2407.19523_, 2024.
* Wang and Van Hoof [2022] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search. In _International Conference on Machine Learning_, pages 23055-23077. PMLR, 2022.
* Wang et al. [2024] Qi Wang, Yiqin Lv, Zheng Xie, Jincai Huang, et al. A simple yet effective strategy to robustify the meta learning paradigm. _Advances in Neural Information Processing Systems_, 36, 2024.
* Wang et al. [2018] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. _Advances in Neural Information Processing Systems_, 31, 2018.
* Wang et al. [2023] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=AHvFDPi-FA.

* Wang et al. [2020] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. _Advances in Neural Information Processing Systems_, 33:7768-7778, 2020.
* Wilcox et al. [2022] Albert Wilcox, Ashwin Balakrishna, Jules Dedieu, Wyame Benslimane, Daniel Brown, and Ken Goldberg. Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations. _Advances in Neural Information Processing Systems_, 35:2254-2267, 2022.
* Wright et al. [2013] Robert Wright, Steven Loscalzo, Philip Dexter, and Lei Yu. Exploiting multi-step sample trajectories for approximate value iteration. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part I 13_, pages 113-128. Springer, 2013.
* Wu et al. [2022] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=KCXQ5HoM-fy.
* Wu et al. [2019] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
* Xiao et al. [2023] Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White. The in-sample softmax for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=u-RuvyDYqCM.
* Xie et al. [2021] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* Xiong et al. [2022] Huaqing Xiong, Tengyu Xu, Lin Zhao, Yingbin Liang, and Wei Zhang. Deterministic policy gradient: Convergence analysis. In _Uncertainty in Artificial Intelligence_, pages 2159-2169. PMLR, 2022.
* Xu et al. [2023] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline RL with no OOD actions: In-sample learning via implicit value regularization. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ueYYgo2pSSU.
* Yang et al. [2022] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: Robust offline reinforcement learning via conservative smoothing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=Q_2JJGH_KE.
* Yu et al. [2020] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. _Advances in Neural Information Processing Systems_, 33:14129-14142, 2020.
* Yu et al. [2021] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in neural information processing systems_, 34:28954-28967, 2021.
* Zhang et al. [2023] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. In-sample actor critic for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=dfDvOWUB53R.
* Zhou et al. [2021] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In _Conference on Robot Learning_, pages 1719-1735. PMLR, 2021.

Extended Related Work

Model-free offline RL.In offline RL, a fixed dataset is provided and no further interactions are allowed [40, 42]. As a result, conventional off-policy RL algorithms suffer from the extrapolation error due to OOD actions and exhibit poor performance [19]. To address this challenge, various offline RL algorithms have been developed, primarily categorized into model-free and model-based approaches. In model-free solutions, value regularization methods introduce conservatism in value estimation through direct penalization [39, 36, 46, 86, 11, 64, 51], or via value ensembles [3, 89, 2]. Policy constraint approaches enforce proximity between the trained policy and the behavior policy, either explicitly via divergence penalties [84, 38, 30, 17, 83], implicitly by weighted behavior cloning [9, 57, 55, 80, 49], or directly through specific parameterization of the policy [19, 23, 93]. Some recent efforts focus on learning the optimal policy within the dataset's support (known as in-support or in-sample optimal policy) in a theoretically sound manner [49, 51, 83]. These approaches are less influenced by the the dataset's average quality. Another popular branch of algorithms opts for in-sample learning, which formulates the Bellman target without querying the values of any unseen actions [7, 45, 37, 85, 92, 88, 21]. Among these, OneStep RL [7] evaluates the behavior policy via SARSA [70] and performs only one step of constrained policy improvement without off-policy evaluation. IQL [37] modifies the SARSA update, using expectile regression to approximate an upper expectile of the value distribution and enable multi-step dynamic programming. Following IQL, several recent works such as InAC [85], IAC [92], \(\mathcal{X}\)QL [21], and SQL [88] have developed different in-sample learning frameworks, further enhancing the performance of in-sample learning approaches. However, this work shows that the performance of in-sample approaches is confined by eschewing generalization beyond the offline dataset. In contrast, the proposed approach DMG utilizes doubly mild generalization to appropriately exploit generalization and achieves significantly stronger performance across tasks.

Model-based offline RL.Model-based offline RL methods involve training an environmental dynamics model, from which synthetic data is generated to facilitate policy optimization [69, 29, 32]. In the context of offline RL, algorithms such as MOPO [90] and MOReL [33] propose to estimate the uncertainty within the trained model and subsequently impose penalties or constraints on state-action pairs characterized by high uncertainty levels, thus achieving conservatism in the learning process. Some model-based approaches incorporate conservatism in a similar way to those model-free ones. For example, COMBO [91] leverages value penalization, while BREMEN [52] employs behavior regularization. More recently, MOBILE [68] introduces uncertainty quantification via the inconsistency of Bellman estimations within a learned dynamics ensemble. SCAS [50] proposes a generic model-based regularizer that unifies OOD state correction and OOD action suppression in offline RL. However, typical model-based methods often involve heavy computational overhead [29], and their effectiveness hinges on the accuracy of the trained dynamics model [54].

Recently, Bose et al. [5] explores multi-task offline RL from the perspective of representation learning and introduced a notion of neighborhood occupancy density. The neighborhood occupancy density at a given state-action pair in the dataset for a source task is defined as the fraction of points in the dataset within a certain distance from that state-action pair in the representation space. Bose et al. [5] use this concept to bound the representational transfer error in the downstream target task. In contrast, DMG is a wildly compatible idea in offline RL and provides insights into many offline RL methods. DMG balances the need for generalization with the risk of over-generalization in offline RL. Generalization to state-action pairs in the neighborhood of the dataset corresponds to mild action generalization in the DMG framework.

## Appendix B Proofs

In this section, we provide the proofs of all the theories in the paper.

### Proof of Theorem 1

This section presents the formal theorem for the Theorem 1 in the main paper, along with its proof.

We first make several common continuity assumptions for Theorem 1.

[MISSING_PAGE_FAIL:17]

Proof.: We formalize \(Q_{\theta^{\prime}}(s,\tilde{a})\) by Taylor expansion at the parameter \(\theta\):

\[Q_{\theta^{\prime}}(s,\tilde{a})=Q_{\theta}(s,\tilde{a})+\nabla_{\theta}Q_{ \theta}(s,\tilde{a})^{\top}(\theta^{\prime}-\theta)+\mathcal{O}\left(\|\theta^{ \prime}-\theta\|^{2}\right)\] (20)

By plugging Eq. (18) into Eq. (20), we have

\[Q_{\theta^{\prime}}(s,\tilde{a})=Q_{\theta}(s,\tilde{a})+\alpha\nabla_{\theta} Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{\theta}(s,a)\left(\mathcal{T}_{u}Q_{ \theta}(s,a)-Q_{\theta}(s,a)\right)+\mathcal{O}\left(\|\theta^{\prime}-\theta \|^{2}\right)\] (21)

According to Assumption 4 and Lemma 2, it holds that

\[|Q_{\theta}(s,\tilde{a})-Q_{\theta}(s,a)|\leq K_{Q}\|\tilde{a}-a\|\] (22) \[|\mathcal{T}_{u}Q_{\theta}(s,\tilde{a})-\mathcal{T}_{u}Q_{\theta }(s,a)|\leq K_{\mathcal{T}}\|\tilde{a}-a\|\] (23)

where \(K_{\mathcal{T}}:=K_{R}+\gamma K_{P}|\mathcal{S}|Q_{\max}\) is a positive bounded constant.

Therefore,

\[\begin{array}{l}|(\mathcal{T}_{u}Q_{\theta}(s,\tilde{a})-Q_{\theta}(s, \tilde{a}))-(\mathcal{T}_{u}Q_{\theta}(s,a)-Q_{\theta}(s,a))|\\ =|(\mathcal{T}_{u}Q_{\theta}(s,\tilde{a})-\mathcal{T}_{u}Q_{\theta}(s,a))+(Q_{ \theta}(s,a)-Q_{\theta}(s,\tilde{a}))|\\ \leq|(\mathcal{T}_{u}Q_{\theta}(s,\tilde{a})-\mathcal{T}_{u}Q_{\theta}(s,a))|+ |(Q_{\theta}(s,a)-Q_{\theta}(s,\tilde{a}))|\\ \leq K_{\mathcal{T}}\|\tilde{a}-a\|+K_{Q}\|\tilde{a}-a\|\end{array}\]

As a result,

\[\begin{array}{l}\mathcal{T}_{u}Q_{\theta}(s,a)-Q_{\theta}(s,a)\leq\mathcal{T }_{u}Q_{\theta}(s,\tilde{a})-Q_{\theta}(s,\tilde{a})+(K_{Q}+K_{\mathcal{T}}) \|\tilde{a}-a\|\\ \mathcal{T}_{u}Q_{\theta}(s,a)-Q_{\theta}(s,a)\geq\mathcal{T}_{u}Q_{\theta}(s, \tilde{a})-Q_{\theta}(s,\tilde{a})-(K_{Q}+K_{\mathcal{T}})\|\tilde{a}-a\|\end{array}\]

Thus we can let

\[\mathcal{T}_{u}Q_{\theta}(s,a)-Q_{\theta}(s,a)=\mathcal{T}_{u}Q_{\theta}(s, \tilde{a})-Q_{\theta}(s,\tilde{a})+C_{2}\|\tilde{a}-a\|,\] (24)

where \(C_{2}\in[-K_{Q}-K_{\mathcal{T}},K_{Q}+K_{\mathcal{T}}]\) is a bounded constant.

Now we shift our focus to \(\alpha\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{\theta}( s,a)\). Let \(v=\nabla_{\theta}Q_{\theta}(s,\tilde{a})-\nabla_{\theta}Q_{\theta}(s,a)\). According to the smoothness of \(Q_{\theta}\) in Assumption 5, it holds that

\[\|v\|=\|\nabla_{\theta}Q_{\theta}(s,\tilde{a})-\nabla_{\theta}Q_{\theta}(s,a) \|\leq K_{g}\|\tilde{a}-a\|.\] (25)

Therefore,

\[\begin{array}{l}\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{ \theta}Q_{\theta}(s,a)\\ =(\nabla_{\theta}Q_{\theta}(s,a)+v)^{\top}\nabla_{\theta}Q_{\theta}(s,a)\\ =\|\nabla_{\theta}Q_{\theta}(s,a)\|^{2}+v^{\top}\nabla_{\theta}Q_{\theta}(s,a) \\ \geq\|\nabla_{\theta}Q_{\theta}(s,a)\|^{2}-\|v\|\|\nabla_{\theta}Q_{\theta}(s,a )\|\\ \geq\|\nabla_{\theta}Q_{\theta}(s,a)\|^{2}-K_{g}\|\tilde{a}-a\|\|\nabla_{ \theta}Q_{\theta}(s,a)\|\end{array}\]

Therefore, for sufficiently close \(\tilde{a}\) and \(a\) such that \(\|\tilde{a}-a\|\leq\|\nabla_{\theta}Q_{\theta}(s,a)\|/K_{g}\), it holds that \(\alpha\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{\theta}(s,a)\geq 0\).

On the other hand, because \(\|\nabla_{\theta}Q_{\theta}\|\) is bounded by \(g_{\max}\) according to Assumption 6, it holds that

\[\alpha\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{\theta}(s, a)\leq\alpha g_{\max}^{2}\]

By choosing a small learning rate \(\alpha\) such that \(\alpha\leq 1/g_{\max}^{2}\),

\[\alpha\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{\theta}(s, a)\leq 1\]

In such cases (sufficiently close \(\tilde{a}\) and \(a\), and sufficiently small \(\alpha\)), let

\[C_{1}:=\alpha\nabla_{\theta}Q_{\theta}(s,\tilde{a})^{\top}\nabla_{\theta}Q_{ \theta}(s,a)\] (26)

We have \(C_{1}\in[0,1]\).

By plugging Equations (24) and (26) into Equation (21), the following equation holds.

\[Q_{\theta^{\prime}}(s,\tilde{a})=Q_{\theta}(s,\tilde{a})+C_{1}\left(\mathcal{T} _{u}Q_{\theta}(s,\tilde{a})-Q_{\theta}(s,\tilde{a})+C_{2}\|\tilde{a}-a\| \right)+\mathcal{O}\left(\|\theta^{\prime}-\theta\|^{2}\right)\] (27)

where \(C_{1}\in[0,1]\), \(C_{2}\in[-K_{Q}-K_{\mathcal{T}},K_{Q}+K_{\mathcal{T}}]\), and \(K_{\mathcal{T}}=K_{R}+\gamma K_{P}|\mathcal{S}|Q_{\max}\).

This concludes the proof.

### Proofs under Oracle Generalization

We first restate the several definitions in the main paper.

**Definition 4** (Mildly generalized policy, Definition 1).: _Policy \(\tilde{\beta}\) is termed a mildly generalized policy if it satisfies_

\[\mathrm{supp}(\hat{\beta}(\cdot|s))\subseteq\mathrm{supp}(\tilde{\beta}(\cdot |s)),\;\text{ and }\;\max_{a_{1}\sim\hat{\beta}(\cdot|s)}\min_{a_{2}\sim\hat{\beta}(\cdot|s)}\|a _{1}-a_{2}\|\leq\epsilon_{a},\] (28)

_where \(\hat{\beta}\) is the empirical behavior policy in the offline dataset._

**Definition 5** (Definition 2).: _The Doubly Mildly Generalization (DMG) operator is defined as_

\[\mathcal{T}_{\mathrm{DMG}}Q(s,a):=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P( \cdot|s,a)}\left[\lambda\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}Q(s ^{\prime},a^{\prime})+(1-\lambda)\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{ \prime})}Q(s^{\prime},a^{\prime})\right]\] (29)

_where \(\hat{\beta}\) is the empirical behavior policy in the dataset and \(\tilde{\beta}\) is a mildly generalized policy._

**Definition 6** (Definition 3).: _The In-sample Q Learning operator [37] is defined as_

\[\mathcal{T}_{\mathrm{In}}Q(s,a):=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P( \cdot|s,a)}\left[\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}Q(s^{ \prime},a^{\prime})\right]\] (30)

_where \(\hat{\beta}\) is the empirical behavior policy in the dataset._

In this subsection, we assume that the learned value function can make oracle generalization in the mild generalization area \(\tilde{\beta}(a|s)>0\), which is formally defined as follows.

**Assumption 9** (Oracle generalization, Assumption 1).: _The generalization of learned Q functions in the mild generalization area \(\tilde{\beta}(a|s)>0\) reflects the true value updates according to \(\mathcal{T}_{\mathrm{DMG}}\). In other words, \(\mathcal{T}_{\mathrm{DMG}}\) is well defined in the mild generalization area \(\tilde{\beta}(a|s)>0\)._

This assumption can be considered reasonable according to the results presented in Theorem 6 above. In such cases, we can analyze the dynamic programming properties of operators \(\mathcal{T}_{\mathrm{In}}\) and \(\mathcal{T}_{\mathrm{DMG}}\).

Before we start the proofs of Lemma 1 and Theorem 2 in the main paper, we prove a lemma.

**Lemma 3**.: _For any function \(f_{1}\), \(f_{2}\), any variant \(x\in\mathcal{X}\), the following inequality holds:_

\[\left|\max_{x\in\mathcal{X}}f_{1}(x)-\max_{x\in\mathcal{X}}f_{2}(x)\right| \leq\max_{x\in\mathcal{X}}\left|f_{1}(x)-f_{2}(x)\right|.\] (31)

Proof.: Define \(x_{1}:=\operatorname*{argmax}_{x\in\mathcal{X}}f_{1}(x)\) and \(x_{2}:=\operatorname*{argmax}_{x\in\mathcal{X}}f_{2}(x)\).

According to the definition, the following inequality holds:

\[f_{1}(x_{2})-f_{2}(x_{2})\leq f_{1}(x_{1})-f_{2}(x_{2})\leq f_{1}(x_{1})-f_{2} (x_{1})\] (32)

Therefore,

\[\left|\max_{x\in\mathcal{X}}f_{1}(x)-\max_{x\in\mathcal{X}}f_{2}( x)\right|\] \[= \left|f_{1}(x_{1})-f_{2}(x_{2})\right|\] \[\leq \max\left\{\left|f_{1}(x_{2})-f_{2}(x_{2})\right|,\left|f_{1}(x_ {1})-f_{2}(x_{1})\right|\right\}\] \[\leq \max_{x\in\mathcal{X}}\left|f_{1}(x)-f_{2}(x)\right|\]

This concludes the proof of Lemma 3. 

**Lemma 4** (Lemma 1).: \(\mathcal{T}_{\mathrm{In}}\) _is a \(\gamma\)-contraction operator in the in-sample area \(\hat{\beta}(a|s)>0\) under the \(\mathcal{L}_{\infty}\) norm._Proof.: Let \(f_{1}\) and \(f_{2}\) be two arbitrary functions.

For all \((s,a)\)\(s.t.\)\(\hat{\beta}(a|s)>0\), we have

\[|\mathcal{T}_{\mathrm{In}}f_{1}(s,a)-\mathcal{T}_{\mathrm{In}}f_{2 }(s,a)|\] \[= \left|R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime}) \right]-R(s,a)-\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max_{a^{ \prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{\prime})\right]\right|\] \[= \gamma\left|\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max_{a ^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime})-\max_ {a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{\prime}) \right]\right|\] \[\leq \gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max_{a^{ \prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime})-\max_{a ^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{\prime})\right]\right]\] \[\leq \gamma\max_{(s,a):\hat{\beta}(a|s)>0}|f_{1}(s,a)-f_{2}(s,a)|\]

where the second inequality holds by Lemma 3.

Therefore, in the in-sample area \(\tilde{\beta}(a|s)>0\), \(\mathcal{T}_{\mathrm{In}}\) is a \(\gamma\)-contraction operator under the \(\mathcal{L}_{\infty}\) norm. This concludes the proof for \(\mathcal{T}_{\mathrm{In}}\). 

Thus, by repeatedly applying \(\mathcal{T}_{\mathrm{In}}\), any initial Q function can converge to the unique fixed point \(Q_{\mathrm{In}}^{*}\). We denote its induced policy by \(\pi_{\mathrm{In}}^{*}\):

\[Q_{\mathrm{In}}^{*}(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P (\cdot|s,a)}\left[\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}Q_{ \mathrm{In}}^{*}(s^{\prime},a^{\prime})\right],\quad\hat{\beta}(a|s)>0,\] (33) \[\pi_{\mathrm{In}}^{*}(s):=\operatorname*{argmax}_{a\sim\hat{ \beta}(\cdot|s)}Q_{\mathrm{In}}^{*}(s,a).\] (34)

Here, \(Q_{\mathrm{In}}^{*}\) is known as the in-sample optimal value function [38, 37], which is the value function of the in-sample optimal policy \(\pi_{\mathrm{In}}^{*}\). We refer readers to [83, 37, 49, 51] for more discussions on the in-sample or in-support optimality.

Now we start the proof of Theorem 2 in the main paper.

**Theorem 7** (Contraction, Theorem 2).: _Under Assumption 9, \(\mathcal{T}_{\mathrm{DMG}}\) is a \(\gamma\)-contraction operator in the mild generalization area \(\tilde{\beta}(a|s)>0\) under the \(\mathcal{L}_{\infty}\) norm. Therefore, by repeatedly applying \(\mathcal{T}_{\mathrm{DMG}}\), any initial Q function can converge to the unique fixed point \(Q_{\mathrm{DMG}}^{*}\)._

Proof.: By the oracle generalization assumption (Assumption 9), \(\mathcal{T}_{\mathrm{DMG}}\) is well defined in the mild generalization area \(\tilde{\beta}(a|s)>0\).

Let \(f_{1}\) and \(f_{2}\) be two arbitrary functions. For all \((s,a)\)\(s.t.\)\(\tilde{\beta}(a|s)>0\), we have

\[\mathcal{T}_{\mathrm{DMG}}f_{1}(s,a)-\mathcal{T}_{\mathrm{DMG}}f_ {2}(s,a)\] \[= R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \lambda\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^ {\prime})+(1-\lambda)\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1} (s^{\prime},a^{\prime})\right]\] \[-R(s,a)-\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \lambda\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a ^{\prime})+(1-\lambda)\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2} (s^{\prime},a^{\prime})\right]\] \[= \gamma\lambda\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max_ {a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime})- \max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{ \prime})\right]\] \[+\gamma(1-\lambda)\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a ^{\prime})\right]\]Therefore, for all \((s,a)\)\(s.t.\)\(\tilde{\beta}(a|s)>0\),

\[|\mathcal{T}_{\mathrm{DMG}}f_{1}(s,a)-\mathcal{T}_{\mathrm{DMG}}f_{ 2}(s,a)|\] \[\leq \left|\gamma\lambda\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime} )-\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{ \prime})\right]\right|\] \[+\left|\gamma(1-\lambda)\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)} \left[\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime}, a^{\prime})\right]\right|\] \[\leq \gamma\lambda\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \left|\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime}, a^{\prime})\right|\right]\] \[+\gamma(1-\lambda)\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \left|\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime}, a^{\prime})\right|\right]\] \[\leq \gamma\lambda\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max_{a ^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}\left|f_{1}(s^{\prime},a^{\prime} )-f_{2}(s^{\prime},a^{\prime})\right|\right]\] \[+\gamma(1-\lambda)\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}\left|f_{1}(s^{\prime},a^{ \prime})-f_{2}(s^{\prime},a^{\prime})\right|\right]\] \[\leq \gamma\lambda\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\max_{(s,a) :\tilde{\beta}(a|s)>0}\left|f_{1}(s,a)-f_{2}(s,a)\right|\] \[+\gamma(1-\lambda)\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\max_{ (s,a):\tilde{\beta}(a|s)>0}\left|f_{1}(s,a)-f_{2}(s,a)\right|\] \[= \gamma\max_{(s,a):\tilde{\beta}(a|s)>0}\left|f_{1}(s,a)-f_{2}(s,a)\right|\]

where the third inequality holds by Lemma 3.

Therefore, in the mild generalization area \(\tilde{\beta}(a|s)>0\), \(\mathcal{T}_{\mathrm{DMG}}\) is a \(\gamma\)-contraction operator under the \(\mathcal{L}_{\infty}\) norm. This concludes the proof. 

As a result, by repeatedly applying \(\mathcal{T}_{\mathrm{DMG}}\), any initial Q function can converge to the unique fixed point \(Q_{\mathrm{DMG}}^{*}\). We denote the induced policy of \(Q_{\mathrm{DMG}}^{*}\) by \(\pi_{\mathrm{DMG}}^{*}\):

\[Q_{\mathrm{DMG}}^{*}(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime} \sim P(\cdot|s,a)}\left[\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}Q _{\mathrm{DMG}}^{*}(s^{\prime},a^{\prime})\right],\quad\tilde{\beta}(a|s)>0,\] (35) \[\pi_{\mathrm{DMG}}^{*}(s):=\operatorname*{argmax}_{a\sim\tilde{ \beta}(\cdot|s)}Q_{\mathrm{DMG}}^{*}(s,a).\] (36)

Before we start the proof of Theorem 3, we prove two lemmas.

**Lemma 5**.: _Under Assumption 9, for any function \(f\), the following inequality holds:_

\[\mathcal{T}_{\mathrm{DMG}}f(s,a)\geq\mathcal{T}_{\mathrm{In}}f(s,a),\ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\] (37)

Proof.: The oracle generalization assumption (Assumption 9) implies that \(\mathcal{T}_{\mathrm{In}}\) is also well defined in the mild generalization area \(\tilde{\beta}(a|s)>0\). Because \(\mathrm{supp}(\tilde{\beta}(\cdot|s))\subseteq\mathrm{supp}(\tilde{\beta}( \cdot|s))\), \(\mathcal{T}_{\mathrm{In}}\) requires less information than \(\mathcal{T}_{\mathrm{DMG}}\). Therefore, \(\mathcal{T}_{\mathrm{DMG}}\) being well defined in the mild generalization area implies \(\mathcal{T}_{\mathrm{In}}\) also being well defined in that area.

According to the definitions, for all \((s,a)\)\(s.t.\)\(\tilde{\beta}(a|s)>0\),

\[\mathcal{T}_{\mathrm{DMG}}f(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot |s,a)}\left[\lambda\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f(s^{ \prime},a^{\prime})+(1-\lambda)\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{ \prime})}f(s^{\prime},a^{\prime})\right]\] (38)

\[\mathcal{T}_{\mathrm{In}}f(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot |s,a)}\left[\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{ \prime})\right]\] (39)Therefore, for all \((s,a)\ s.t.\ \tilde{\beta}(a|s)>0\), we have

\[\mathcal{T}_{\mathrm{DMG}}f(s,a)-\mathcal{T}_{\mathrm{In}}f(s,a)\] \[= \gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\lambda\max_{a ^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{\prime})-\lambda \max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{\prime})\right]\] \[\geq 0\]

where the last inequality holds because \(\tilde{\beta}\) has a wider support than \(\hat{\beta}\). 

**Lemma 6**.: _Under Assumption 9, for any function \(f_{1},f_{2}\) such that \(f_{1}(s,a)\geq f_{2}(s,a)\), \(\forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0\), the following inequality holds:_

\[\mathcal{T}_{\mathrm{DMG}}f_{1}(s,a)\geq\mathcal{T}_{\mathrm{DMG}}f_{2}(s,a), \ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0\] (40)

Proof.: By Assumption 9, \(\mathcal{T}_{\mathrm{DMG}}\) is well defined in the mild generalization area \(\tilde{\beta}(a|s)>0\).

According to the definition, for all \((s,a)\ s.t.\ \tilde{\beta}(a|s)>0\),

\[\mathcal{T}_{\mathrm{DMG}}f(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P( \cdot|s,a)}\left[\lambda\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f( s^{\prime},a^{\prime})+(1-\lambda)\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{ \prime})}f(s^{\prime},a^{\prime})\right]\] (41)

\(f_{1}\) and \(f_{2}\) satisfy

\[f_{1}(s,a)\geq f_{2}(s,a),\forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\] (42)

Therefore, for all \((s,a)\ s.t.\ \tilde{\beta}(a|s)>0\),

\[\mathcal{T}_{\mathrm{DMG}}f_{1}(s,a)-\mathcal{T}_{\mathrm{DMG}}f _{2}(s,a)\] \[= \gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\lambda\max_ {a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{\prime})- \lambda\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}(s^{\prime},a^{\prime})\right]\] \[+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[(1-\lambda )\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{1}(s^{\prime},a^{ \prime})-(1-\lambda)\max_{a^{\prime}\sim\tilde{\beta}(\cdot|s^{\prime})}f_{2}( s^{\prime},a^{\prime})\right]\] \[\geq 0\]

Now we start the proof of Theorem 3 in the main paper.

**Theorem 8** (Performance, Theorem 3).: _Under Assumption 9, the value functions of \(\pi^{*}_{\mathrm{DMG}}\) and \(\pi^{*}_{\mathrm{In}}\) satisfy:_

\[V^{\pi^{*}_{\mathrm{DMG}}}(s)\geq V^{\pi^{*}_{\mathrm{In}}}(s),\quad\forall s \in\mathcal{D}.\] (43)

Proof.: We first prove the following inequality:

\[(\mathcal{T}_{\mathrm{DMG}})^{k}f(s,a)\geq(\mathcal{T}_{\mathrm{In}})^{k}f(s, a),\ \forall k\in\mathbb{Z}^{+},\ \forall f,\ \ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\] (44)

When \(k=1\), according to Lemma 5, it holds that

\[(\mathcal{T}_{\mathrm{DMG}})^{1}f(s,a)\geq(\mathcal{T}_{\mathrm{In}})^{1}f(s, a),\ \forall f,\ \ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\]

Suppose when \(k=i\), the following inequality holds:

\[(\mathcal{T}_{\mathrm{DMG}})^{i}f(s,a)\geq(\mathcal{T}_{\mathrm{In}})^{i}f(s, a),\ \forall f,\ \ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\]

Then \((\mathcal{T}_{\mathrm{DMG}})^{i}f\) and \((\mathcal{T}_{\mathrm{In}})^{i}f\) are the two functions \(f_{1},f_{2}\) that satisfy the condition in Lemma 6. Therefore, by Lemma 6, it holds that

\[\mathcal{T}_{\mathrm{DMG}}(\mathcal{T}_{\mathrm{DMG}})^{i}f(s,a)\geq\mathcal{T} _{\mathrm{DMG}}(\mathcal{T}_{\mathrm{In}})^{i}f(s,a),\ \forall f,\ \ \forall(s,a)\ s.t.\ \tilde{\beta}(a|s)>0.\] (45)Now considering \((\mathcal{T}_{\mathrm{In}})^{i}f\) as the function \(f\) in Lemma 5. By Lemma 5, it holds that

\[\mathcal{T}_{\mathrm{DMG}}(\mathcal{T}_{\mathrm{In}})^{i}f(s,a)\geq\mathcal{T}_{ \mathrm{In}}(\mathcal{T}_{\mathrm{In}})^{i}f(s,a),\;\forall f,\;\;\forall(s,a) \;s.t.\;\tilde{\beta}(a|s)>0.\] (46)

Combining Equations (45) and (46), we have

\[(\mathcal{T}_{\mathrm{DMG}})^{i+1}f(s,a)\geq(\mathcal{T}_{\mathrm{In}})^{i+1} f(s,a),\;\forall f,\;\;\forall(s,a)\;s.t.\;\tilde{\beta}(a|s)>0.\]

Therefore, for all \(k\in\mathbb{Z}^{+}\), the following inequality holds:

\[(\mathcal{T}_{\mathrm{DMG}})^{k}f(s,a)\geq(\mathcal{T}_{\mathrm{In}})^{k}f(s,a ),\;\forall f,\;\;\forall(s,a)\;s.t.\;\tilde{\beta}(a|s)>0.\] (47)

Lemma 4 states that \(\mathcal{T}_{\mathrm{In}}\) is a \(\gamma\)-contraction operator in the in-sample area \(\hat{\beta}(a|s)>0\). Thus we have

\[Q_{\mathrm{In}}^{*}(s,a)=\lim_{k\to\infty}(\mathcal{T}_{\mathrm{In}})^{k}f(s, a),\;\forall(s,a)\;s.t.\;\hat{\beta}(a|s)>0.\] (48)

Under Assumption 9, Theorem 7 states that \(\mathcal{T}_{\mathrm{DMG}}\) is a \(\gamma\)-contraction operator in the mild generalization area \(\tilde{\beta}(a|s)>0\). Thus we have

\[Q_{\mathrm{DMG}}^{*}(s,a)=\lim_{k\to\infty}(\mathcal{T}_{\mathrm{DMG}})^{k}f(s,a),\;\forall(s,a)\;s.t.\;\tilde{\beta}(a|s)>0.\] (49)

As \(\tilde{\beta}\) has a wider support than \(\hat{\beta}\), \(\mathrm{supp}(\hat{\beta}(\cdot|s))\subseteq\mathrm{supp}(\tilde{\beta}(\cdot |s))\), the following inequality holds by combining Equations (47) to (49):

\[Q_{\mathrm{DMG}}^{*}(s,a)\geq Q_{\mathrm{In}}^{*}(s,a),\;\forall(s,a)\;s.t.\; \hat{\beta}(a|s)>0.\] (50)

Therefore, for any \(s\sim\mathcal{D}\),

\[V^{*}_{\mathrm{DMG}}(s)=V_{\mathrm{DMG}}^{*}(s)=Q_{\mathrm{DMG} }^{*}(s,\pi_{\mathrm{DMG}}^{*}(s))\] \[\geq Q_{\mathrm{DMG}}^{*}(s,\pi_{\mathrm{In}}^{*}(s))\] \[\geq Q_{\mathrm{In}}^{*}(s,\pi_{\mathrm{In}}^{*}(s))=V_{\mathrm{In} }^{*}(s)=V^{\pi_{\mathrm{In}}^{*}}(s)\]

where the first inequality holds because \(\pi_{\mathrm{DMG}}^{*}(s):=\operatorname*{argmax}_{a\sim\tilde{\beta}(\cdot|s) }Q_{\mathrm{DMG}}^{*}(s,a)\) and \(\pi_{\mathrm{In}}^{*}(s)\in\hat{\beta}(\cdot|s)\) (thus \(\pi_{\mathrm{In}}^{*}(s)\in\tilde{\beta}(\cdot|s)\)), and the second inequality holds by Equation (50).

This concludes the proof. 

Theorem 8 indicates that the policy induced by the DMG operator can behave better than the in-sample optimal policy under the oracle generalization condition.

### Proofs under Worst-case Generalization

In this section, we focus on the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area \(\tilde{\beta}(a|s)>0\). In other words, this section considers that \(\mathcal{T}_{\mathrm{DMG}}\) is only defined in the in-sample area \(\hat{\beta}(a|s)>0\) and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation \(\hat{\mathcal{T}}_{\mathrm{DMG}}\) for differentiation.

In this case, we make the following continuity assumptions about the learned \(Q\) function and the transition dynamics \(P\).

**Assumption 10** (Lipschitz \(Q\)).: _The learned Q function is \(K_{Q}\)-Lipschitz. \(\forall s\sim\mathcal{D}\), \(\forall a_{1},a_{2}\sim\mathcal{A}\), \(|Q(s,a_{1})-Q(s,a_{2})|\leq K_{Q}\|a_{1}-a_{2}\|\)_

**Assumption 11** (Lipschitz \(P\)).: _The transition dynamics \(P\) is \(K_{P}\)-Lipschitz. \(\forall s,s^{\prime}\sim\mathcal{S}\), \(\forall a_{1},a_{2}\sim\mathcal{A}\)\(|P(s^{\prime}|s,a_{1})-P(s^{\prime}|s,a_{2})|\leq K_{P}\|a_{1}-a_{2}\|\)_

For Assumption 10, a continuous learned Q function is particularly necessary for the analysis of value function generalization and can be relatively easily satisfied [24], since we often use neural networks or linear models to parameterize the value function. For Assumption 11, continuous transition dynamics is also a standard assumption in the theoretical studies of RL [13; 14; 87; 61]. Several previous works assume the transition to be Lipschitz continuous with respect to (w.r.t.) both state and action [13; 14]. In our paper, we need the Lipschitz continuity to hold only w.r.t. action.

Before we start the proof of Theorem 4, we prove two lemmas.

**Lemma 7**.: _Under Assumption 10, for any function \(f\) and \(s\sim\mathcal{D}\), the following inequality holds:_

\[\max_{a\sim\beta(\cdot\,|s)}f(s,a)-\max_{a\sim\beta(\cdot\,|s)}f(s,a)\leq \epsilon_{a}K_{Q}.\] (51)

Proof.: For any \(s\sim\mathcal{D}\), we define \(\tilde{a}^{*}\), \(\hat{a}^{*}\), \(\hat{a}^{\prime}\) as follows:

\[\tilde{a}^{*} =\operatorname*{argmax}_{a\sim\tilde{\beta}(\cdot\,|s)}f(s,a)\] (52) \[\hat{a}^{*} =\operatorname*{argmax}_{a\sim\tilde{\beta}(\cdot\,|s)}f(s,a)\] (53) \[\hat{a}^{\prime} =\operatorname*{argmin}_{a\sim\tilde{\beta}(\cdot\,|s)}\|\tilde{ a}^{*}-a\|\] (54)

According to the definition of mildly generalized policy \(\tilde{\beta}\) (Definition 4), it holds that \(\|\tilde{a}^{*}-\hat{a}^{\prime}\|\leq\epsilon_{a}\). Further by Assumption 10, it holds that

\[|f(s,\tilde{a}^{*})-f(s,\hat{a}^{\prime})|\leq K_{Q}\|\tilde{a}^{*}-\hat{a}^{ \prime}\|\leq\epsilon_{a}K_{Q},\ \ \forall s\sim\mathcal{D}.\]

Therefore,

\[f(s,\tilde{a}^{*})-f(s,\hat{a}^{*})\leq f(s,\tilde{a}^{*})-f(s,\hat{a}^{\prime })\leq\epsilon_{a}K_{Q},\ \ \forall s\sim\mathcal{D}.\]

**Lemma 8**.: _For any function \(f_{1},f_{2}\) such that \(f_{1}(s,a)\geq f_{2}(s,a)\), \(\forall(s,a)\ s.t.\ \hat{\beta}(a|s)>0\), the following inequality holds:_

\[\mathcal{T}_{\mathrm{In}}f_{1}(s,a)\geq\mathcal{T}_{\mathrm{In}}f_{2}(s,a),\ \ \forall(s,a)\ s.t.\ \hat{\beta}(a|s)>0.\] (55)

Proof.: According to the definitions, for all \((s,a)\ s.t.\ \hat{\beta}(a|s)>0\),

\[\mathcal{T}_{\mathrm{In}}f(s,a)=R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P( \cdot\,|s,a)}\left[\max_{a^{\prime}\sim\tilde{\beta}(\cdot\,|s^{\prime})}f(s^ {\prime},a^{\prime})\right]\] (56)

\(f_{1}\) and \(f_{2}\) satisfy

\[f_{1}(s,a)\geq f_{2}(s,a),\forall(s,a)\ \ s.t.\ \hat{\beta}(a|s)>0.\]

Therefore, for all \((s,a)\ s.t.\ \hat{\beta}(a|s)>0\),

\[\mathcal{T}_{\mathrm{In}}f_{1}(s,a)-\mathcal{T}_{\mathrm{In}}f_{ 2}(s,a)\] \[= \gamma\mathbb{E}_{s^{\prime}\sim P(\cdot\,|s,a)}\left[\max_{a^{ \prime}\sim\tilde{\beta}(\cdot\,|s^{\prime})}f_{1}(s^{\prime},a^{\prime})- \max_{a^{\prime}\sim\tilde{\beta}(\cdot\,|s^{\prime})}f_{2}(s^{\prime},a^{ \prime})\right]\] \[\geq 0\]

Now we start the proof of Theorem 4 in the main paper.

We consider the iteration starting from arbitrary function \(Q^{0}\): \(\hat{Q}^{k}_{\mathrm{DMG}}=\hat{\mathcal{T}}_{\mathrm{DMG}}\hat{Q}^{k-1}_{ \mathrm{DMG}}\) and \(Q^{k}_{\mathrm{In}}=\mathcal{T}_{\mathrm{In}}Q^{k-1}_{\mathrm{In}},\forall k \in\mathbb{Z}^{+}\). The possible value of \(\hat{Q}^{k}_{\mathrm{DMG}}\) is upper bounded by the following results.

**Theorem 9** (Limited over-estimation, Theorem 4).: _Under Assumption 10, the learned Q function of DMG by iterating \(\hat{\mathcal{T}}_{\mathrm{DMG}}\) satisfies the following inequality_

\[Q^{k}_{\mathrm{In}}(s,a)\leq\hat{Q}^{k}_{\mathrm{DMG}}(s,a)\leq Q^{k}_{ \mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{k} ),\ \forall s,a\sim\mathcal{D},\ \forall k\in\mathbb{Z}^{+}.\] (57)Proof.: Under worst-case generalization, \(\hat{\mathcal{T}}_{\mathrm{DMG}}\) is only defined in the area \(\hat{\beta}(a|s)>0\), i.e., the dataset, and may have any generalization error at other \((s,a)\).

For any function \(f\) and any \(s,a\sim\mathcal{D}\),

\[\hat{\mathcal{T}}_{\mathrm{DMG}}f(s,a)-\mathcal{T}_{\mathrm{In}}f (s,a)\] \[= R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[ \lambda\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{ \prime})+(1-\lambda)\max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f(s^{ \prime},a^{\prime})\right]\] \[-R(s,a)-\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max _{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{\prime})\right]\] \[= \gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\lambda\max_ {a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{\prime})-\lambda \max_{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}f(s^{\prime},a^{\prime})\right]\] \[\leq \gamma\lambda\epsilon_{a}K_{Q}\]

where the last inequality holds by Lemma 7.

On the other hand, because \(\hat{\beta}\) has a wider support than \(\hat{\beta}\), we also have

\[\hat{\mathcal{T}}_{\mathrm{DMG}}f(s,a)-\mathcal{T}_{\mathrm{In}}f(s,a)\geq 0\]

Therefore, for any function \(f\), the following inequality holds:

\[\mathcal{T}_{\mathrm{In}}f(s,a)\leq\hat{\mathcal{T}}_{\mathrm{DMG}}f(s,a)\leq \mathcal{T}_{\mathrm{In}}f(s,a)+\gamma\lambda\epsilon_{a}K_{Q},\ \ \forall s,a\sim\mathcal{D}.\] (58)

Let \(f\) in Equation (58) be \(Q^{0}\). We have

\[Q^{1}_{\mathrm{In}}(s,a)\leq\hat{Q}^{1}_{\mathrm{DMG}}(s,a)\leq Q^{1}_{ \mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma),\ \forall s,a\sim \mathcal{D}.\] (59)

This is the same as Equation (57) with \(k=1\). Therefore, Equation (57) holds when \(k=1\).

Suppose when \(k=i\), Equation (57) holds:

\[Q^{i}_{\mathrm{In}}(s,a)\leq\hat{Q}^{i}_{\mathrm{DMG}}(s,a)\leq Q^{i}_{ \mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{i} ),\ \forall s,a\sim\mathcal{D}.\] (60)

Then let \(f\) in Equation (58) be \(\hat{Q}^{i}_{\mathrm{DMG}}\). We have

\[\mathcal{T}_{\mathrm{In}}\hat{Q}^{i}_{\mathrm{DMG}}(s,a)\leq\hat{Q}^{i+1}_{ \mathrm{DMG}}(s,a)=\hat{\mathcal{T}}_{\mathrm{DMG}}\hat{Q}^{i}_{\mathrm{DMG}}( s,a)\leq\mathcal{T}_{\mathrm{In}}\hat{Q}^{i}_{\mathrm{DMG}}(s,a)+\gamma \lambda\epsilon_{a}K_{Q},\ \ \forall s,a\sim\mathcal{D}.\] (61)

On the one hand, according to Lemma 8 and Equation (60), for any \(s,a\sim\mathcal{D}\), we have

\[\mathcal{T}_{\mathrm{In}}\hat{Q}^{i}_{\mathrm{DMG}}(s,a)\] \[\leq \mathcal{T}_{\mathrm{In}}\left(Q^{i}_{\mathrm{In}}(s,a)+\frac{ \lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{i})\right)\] \[= R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max _{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}\left(Q^{i}_{\mathrm{In}}(s^{ \prime},a^{\prime})+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^ {i})\right)\right]\] \[= R(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\max _{a^{\prime}\sim\hat{\beta}(\cdot|s^{\prime})}Q^{i}_{\mathrm{In}}(s^{\prime},a ^{\prime})\right]+\gamma\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1- \gamma^{i})\] \[= \mathcal{T}_{\mathrm{In}}Q^{i}_{\mathrm{In}}(s,a)+\gamma\frac{ \lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{i})\] \[= Q^{i+1}_{\mathrm{In}}(s,a)+\gamma\frac{\lambda\epsilon_{a}K_{Q} \gamma}{1-\gamma}(1-\gamma^{i})\] (62)Combining Equations (61) and (62), for any \(s,a\sim\mathcal{D}\), we have

\[\hat{Q}^{i+1}_{\mathrm{DMG}}(s,a)\] \[\leq Q^{i+1}_{\mathrm{In}}(s,a)+\gamma\frac{\lambda\epsilon_{a}K_{Q} \gamma}{1-\gamma}(1-\gamma^{i})+\gamma\lambda\epsilon_{a}K_{Q}\] \[= Q^{i+1}_{\mathrm{In}}(s,a)+\lambda\epsilon_{a}K_{Q}\gamma\left( \frac{\gamma(1-\gamma^{i})}{1-\gamma}+1\right)\] \[= Q^{i+1}_{\mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma }{1-\gamma}(1-\gamma^{i+1})\]

On the other hand, according to Lemma 8 and Equation (60), for any \(s,a\sim\mathcal{D}\), we have

\[\mathcal{T}_{\mathrm{In}}\hat{Q}^{i}_{\mathrm{DMG}}(s,a)\geq\mathcal{T}_{ \mathrm{In}}Q^{i}_{\mathrm{In}}(s,a)=Q^{i+1}_{\mathrm{In}}(s,a)\] (63)

Combining Equations (61) and (63), for any \(s,a\sim\mathcal{D}\), we have

\[\hat{Q}^{i+1}_{\mathrm{DMG}}(s,a)\geq Q^{i+1}_{\mathrm{In}}(s,a).\] (64)

Hence, Equation (57) still holds when \(k=i+1\):

\[Q^{i+1}_{\mathrm{In}}(s,a)\leq\hat{Q}^{i+1}_{\mathrm{DMG}}(s,a)\leq Q^{i+1}_{ \mathrm{In}}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}(1-\gamma^{i+ 1}),\ \forall s,a\sim\mathcal{D}.\] (65)

Therefore, Equation (57) holds for all \(k\in\mathbb{Z}^{+}\), which concludes the proof. 

Since in-sample training eliminates extrapolation error completely [37, 92], \(Q^{k}_{\mathrm{In}}\) can be considered a relatively accurate estimate. Therefore, Theorem 9 indicates that DMG has limited over-estimation under the worst generalization case. Moreover, the bound gets tighter as \(\epsilon_{a}\) gets smaller (more mild action generalization) and \(\lambda\) gets smaller (more mild generalization propagation). This is consistent with our intuitions in Section 3.2.

Finally, Theorem 5 in the main paper shows that even under worst-case generalization, DMG is guaranteed to output a safe policy with a performance lower bound.

We give a lemma before we start the proof of Theorem 5,

**Lemma 9**.: _Let \(\pi_{1}\) and \(\pi_{2}\) be two deterministic policies. Under Assumption 11, the following inequality holds:_

\[\mathrm{TV}\left(d^{\pi_{1}}||d^{\pi_{2}}\right)\leq CK_{P}\max_{s}\|\pi_{1}(s )-\pi_{2}(s)\|\] (66)

_where \(C\) is a positive constant and \(d^{\pi}(s)\) is the state occupancy induced by \(\pi\)._

\[d^{\pi}(s)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathbb{E}_{\pi}\left[ \mathbb{I}\left[s_{t}=s\right]\right].\] (67)

Proof.: Please refer to Lemma A.5 in [61] and Lemma 1 in [87]. 

**Theorem 10** (Performance lower bound, Theorem 5).: _Let \(\hat{\pi}_{\mathrm{DMG}}\) be the learned policy of DMG by iterating \(\hat{\mathcal{T}}_{\mathrm{DMG}}\), \(\pi^{*}\) be the optimal policy, and \(\epsilon_{\mathcal{D}}\) be the inherent performance gap of the in-sample optimal policy \(\epsilon_{\mathcal{D}}:=J(\pi^{*})-J(\pi_{\mathrm{In}}^{*})\). Under Assumptions 10 and 11, for sufficiently small \(\epsilon_{a}\), we have_

\[J(\hat{\pi}_{\mathrm{DMG}})\geq J(\pi^{*})-\frac{CK_{P}R_{\mathrm{max}}}{1- \gamma}\epsilon_{a}-\epsilon_{\mathcal{D}}.\] (68)

_where \(C\) is a positive constant._

Proof.: Following previous works [38, 33, 37, 49], we define the in-sample optimal policy as \(\pi_{\mathrm{In}}^{*}\):

\[\pi_{\mathrm{In}}^{*}(s)=\operatorname*{argmax}_{a\sim\hat{\beta}(\cdot|s)}Q^{ *}_{\mathrm{In}}(s,a)\] (69)We also use \(\epsilon_{\mathcal{D}}\) to denote the performance gap between the in-sample optimal policy and the globally optimal policy, which is fixed once the dataset is provided.

\[\epsilon_{\mathcal{D}}=J(\pi^{*})-J(\pi_{\mathrm{In}}^{*}).\] (70)

We use \(\hat{Q}_{\mathrm{DMG}}\) to denote the learned Q function of DMG with sufficient iteration steps \(\hat{Q}_{\mathrm{DMG}}^{k}\), \(k\rightarrow\infty\). And \(\hat{\pi}_{\mathrm{DMG}}\) is the output policy of \(\hat{Q}_{\mathrm{DMG}}\):

\[\hat{\pi}_{\mathrm{DMG}}(s)=\operatorname*{argmax}_{a\sim\tilde{\beta}(\cdot|s )}\hat{Q}_{\mathrm{DMG}}(s,a)\] (71)

It holds that

\[|J(\pi^{*})-J(\hat{\pi}_{\mathrm{DMG}})|\] \[= |J(\pi^{*})-J(\pi_{\mathrm{In}}^{*})+J(\pi_{\mathrm{In}}^{*})-J( \hat{\pi}_{\mathrm{DMG}})|\] \[\leq |J(\pi^{*})-J(\pi_{\mathrm{In}}^{*})|+|J(\pi_{\mathrm{In}}^{*})- J(\hat{\pi}_{\mathrm{DMG}})|\] \[= \epsilon_{\mathcal{D}}+|J(\pi_{\mathrm{In}}^{*})-J(\hat{\pi}_{ \mathrm{DMG}})|\] (72)

In the following, we bound the term \(|J(\pi_{\mathrm{In}}^{*})-J(\hat{\pi}_{\mathrm{DMG}})|\).

\[|J(\pi_{\mathrm{In}}^{*})-J(\hat{\pi}_{\mathrm{DMG}})|\] \[= \left|\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\mathrm{FDMG}}}\left[ r(s)\right]-\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{*}\pi_{\mathrm{In}}^{*}}[r(s)]\right|\] \[= \frac{1}{1-\gamma}\left|\sum_{s}\left(d^{\hat{\pi}_{\mathrm{DMG} }}(s)-d^{\pi_{\mathrm{In}}^{*}}(s)\right)r(s)\right|\] \[\leq \frac{1}{1-\gamma}\sum_{s}\left|\left(d^{\hat{\pi}_{\mathrm{DMG} }}(s)-d^{\pi_{\mathrm{In}}^{*}}(s)\right)\right||r(s)|\] \[\leq \frac{R_{\mathrm{max}}}{1-\gamma}\mathrm{TV}\left(d^{\hat{\pi}_{ \mathrm{DMG}}}(s)||d^{\pi_{\mathrm{In}}^{*}}(s)\right)\] \[\leq \frac{R_{\mathrm{max}}}{1-\gamma}CK_{P}\max_{s}\|\hat{\pi}_{ \mathrm{DMG}}(s)-\pi_{\mathrm{In}}^{*}(s)\|\] (73)

where the last inequality holds by Lemma 9.

According to Theorem 9, \(\hat{Q}_{\mathrm{DMG}}\) satisfies the following inequality:

\[Q_{\mathrm{In}}^{*}(s,a)\leq\hat{Q}_{\mathrm{DMG}}(s,a)\leq Q_{\mathrm{In}}^ {*}(s,a)+\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma},\;\forall s,a\sim \mathcal{D}.\] (74)

It means that for any \((s,a)\sim\mathcal{D}\), with sufficiently small \(\epsilon_{a}\), \(\hat{Q}_{\mathrm{DMG}}(s,a)\) sufficiently approximates \(Q_{\mathrm{In}}^{*}(s,a)\). By Definition 4, \(\tilde{\beta}\) is a mildly generalized policy. That is, for any \(s\sim\mathcal{D}\), \(\tilde{\beta}\) satisfies

\[\mathrm{supp}(\hat{\beta}(\cdot|s))\subseteq\mathrm{supp}(\tilde{\beta}(\cdot |s)),\;\;\text{and}\;\;\max_{a_{1}\sim\tilde{\beta}(\cdot|s)}\min_{a_{2}\sim \tilde{\beta}(\cdot|s)}\|a_{1}-a_{2}\|\leq\epsilon_{a},\]

As \(\hat{\pi}_{\mathrm{DMG}}(s)\in\tilde{\beta}(\cdot|s)\), it implies that we can find \(a_{\mathrm{in}}\in\hat{\beta}(\cdot|s)\) (in dataset) such that \(\|\hat{\pi}_{\mathrm{DMG}}(s)-a_{\mathrm{in}}\|\leq\epsilon_{a}\).

Now suppose \(a_{\mathrm{in}}\) is not the maximum point of \(Q_{\mathrm{In}}^{*}(s,\cdot)\) at a certain \(s\). We use \(\pi_{\mathrm{In}}^{*}(s)\) to denote the maximum point of \(Q_{\mathrm{In}}^{*}(s,\cdot)\). Let \(\epsilon_{Q_{\mathrm{In}}^{*}}\) be the gap between \(Q_{\mathrm{In}}^{*}(s,a_{\mathrm{in}})\) and \(Q_{\mathrm{In}}^{*}(s,\pi_{\mathrm{In}}^{*}(s))\):

\[\epsilon_{Q_{\mathrm{In}}^{*}}(s):=Q_{\mathrm{In}}^{*}(s,\pi_{\mathrm{In}}^{*}( s))-Q_{\mathrm{In}}^{*}(s,a_{\mathrm{in}})>0.\] (75)

By Assumption 10 (Lipschitz \(Q\)), we have

\[\hat{Q}_{\mathrm{DMG}}(s,\hat{\pi}_{\mathrm{DMG}}(s))-\hat{Q}_{\mathrm{DMG}}(s, a_{\mathrm{in}})\leq K_{Q}\|\hat{\pi}_{\mathrm{DMG}}(s)-a_{\mathrm{in}}\|\leq K_{Q} \epsilon_{a}.\] (76)Therefore,

\[\hat{Q}_{\mathrm{DMG}}(s,\pi_{\mathrm{In}}^{*}(s))-\hat{Q}_{\mathrm{ DMG}}(s,\hat{\pi}_{\mathrm{DMG}}(s))\] \[\geq \hat{Q}_{\mathrm{DMG}}(s,\pi_{\mathrm{In}}^{*}(s))-\hat{Q}_{ \mathrm{DMG}}(s,a_{\mathrm{in}})-K_{Q}\epsilon_{a}\] \[\geq Q_{\mathrm{In}}^{*}(s,\pi_{\mathrm{In}}^{*}(s))-Q_{\mathrm{In}}^ {*}(s,a_{\mathrm{in}})-\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1-\gamma}-K_{Q} \epsilon_{a}\] \[= \epsilon_{Q_{\mathrm{In}}^{*}}(s)-\frac{\lambda\epsilon_{a}K_{Q} \gamma}{1-\gamma}-K_{Q}\epsilon_{a}\]

where the first inequality holds by Equation (76), the second inequality holds by Equation (74), and the last equality holds by Equation (75).

Hence, for sufficiently small \(\epsilon_{a}\) such that \(\epsilon_{Q_{\mathrm{In}}^{*}}(s)-\frac{\lambda\epsilon_{a}K_{Q}\gamma}{1- \gamma}-K_{Q}\epsilon_{a}>0\), i.e.,

\[\epsilon_{a}<\frac{(1-\gamma)\epsilon_{Q_{\mathrm{In}}^{*}}(s)}{K_{Q}(1- \gamma+\lambda\gamma)},\] (77)

it holds that \(\hat{Q}_{\mathrm{DMG}}(s,\pi_{\mathrm{In}}^{*}(s))-\hat{Q}_{\mathrm{DMG}}(s, \hat{\pi}_{\mathrm{DMG}}(s))>0\). As \(\pi_{\mathrm{In}}^{*}(s)\in\hat{\beta}(\cdot|s)\), it also satisfies \(\pi_{\mathrm{In}}^{*}(s)\in\hat{\beta}(\cdot|s)\). This contradicts the definition of \(\hat{\pi}_{\mathrm{DMG}}(s)\) in Equation (71):

\[\hat{\pi}_{\mathrm{DMG}}(s)=\operatorname*{argmax}_{a\sim\tilde{\beta}(\cdot| s)}\hat{Q}_{\mathrm{DMG}}(s,a)\]

Therefore, \(a_{\mathrm{in}}\) is the maximum point of \(Q_{\mathrm{In}}^{*}(s,\cdot)\). In other words, the maximum point of \(Q_{\mathrm{In}}^{*}(s,\cdot)\) (denoted by \(\pi_{\mathrm{In}}^{*}(s)\)) is the closest neighbor of \(\hat{\pi}_{\mathrm{DMG}}(s)\) in the dataset (\(\hat{\beta}(\cdot|s)>0\)):

\[\pi_{\mathrm{In}}^{*}(s)=\operatorname*{argmin}_{a\sim\tilde{\beta}(\cdot|s)} \|a-\hat{\pi}_{\mathrm{DMG}}(s)\|\]

As \(\hat{\pi}_{\mathrm{DMG}}(s)\in\tilde{\beta}(\cdot|s)\), the following inequality holds by Definition 4:

\[\|\hat{\pi}_{\mathrm{DMG}}(s)-\pi_{\mathrm{In}}^{*}(s)\|\leq\epsilon_{a}.\]

Therefore, we have

\[|J(\pi_{\mathrm{In}}^{*})-J(\hat{\pi}_{\mathrm{DMG}})|\leq\frac{R_{\mathrm{ max}}}{1-\gamma}CK_{P}\epsilon_{a}.\] (78)

By combining Equations (72) and (78), we have

\[J(\hat{\pi}_{\mathrm{DMG}})\geq J(\pi^{*})-\frac{CK_{P}R_{\mathrm{max}}}{1- \gamma}\epsilon_{a}-\epsilon_{\mathcal{D}}.\] (79)

This concludes the proof. 

## Appendix C Experimental Details

### Experimental Details in Offline Experiments

Our evaluation criteria follow those used in most previous works. For the Gym locomotion tasks, we average returns over 10 evaluation trajectories and 5 random seeds, while for the AntMaze tasks, we average over 100 evaluation trajectories and 5 random seeds. Following the suggestions in the benchmark [16], we subtract 1 from the rewards for the AntMaze datasets. And following previous works [17, 37, 83, 88], we normalize the states in Gym locomotion datasets. We choose TD3 [18] as our base algorithm and optimize a deterministic policy. Thus we replace the log likelihood in Eq. (14) with mean squared error in practice, which is equivalent to optimizing a Gaussian policy with fixed variance [17]. The reported results are the normalized scores, which are offered by the D4RL benchmark [16] to measure how the learned policy compared with random and expert policy:

\[\text{D4RL score}=100\times\frac{\text{learned policy return}-\text{ random policy return}}{\text{expert policy return}-\text{random policy return}}\]As we implement our main algorithm based on IQL [37], we use the hyperparameters suggested in their paper for fair comparisons, i.e., \(\tau=0.7\) and \(\alpha=3\) for Gym locomotion tasks and \(\tau=0.9\) and \(\alpha=10\) for AntMaze tasks. For the results of \(\mathcal{X}\)QL+DMG and SQL+DMG, we also adopt the suggested hyperparameters in their papers [21, 88] for fair comparisons. In detail, we choose \(\beta\) in \(\mathcal{X}\)QL [21] as \(5.0\) in medium, medium-replay, and medium-expert datasets, and \(\alpha\) in SQL [88] as \(2.0\) for medium, medium-replay datasets, and \(5.0\) for medium-expert datasets.

DMG has two main hyperparameters: mixture coefficient \(\lambda\) and penalty coefficient \(\nu\). We use \(\lambda=0.25\) for all tasks. We use \(\nu=0.5\) for Antmaze tasks and \(\nu\in\{0.1,10\}\) for Gym locomotion tasks (\(0.1\) for medium, medium-replay, random datasets; \(10\) for expert and medium-expert datasets). All hyperparameters of DMG are included in Table 5.

### Experimental Details in Offline-to-online Experiments

For online fine-tuning experiments, we first run offline RL for \(1\times 10^{6}\) gradient steps. Then we continue training while collecting data actively in the environment and adding the data to the replay buffer. We perform online fine-tuning for \(1\times 10^{6}\) steps with \(1\) update-to-data (UTD) ratio, and collect data with exploration noise \(0.1\) as suggested by TD3 [18]. During offline pre-training, we fix the mixture coefficient \(\lambda=0.25\) and the penalty coefficient \(\nu=0.5\), while in the online phase, we exponentially adjust \(\lambda\) and \(\nu\), as DMG with \(\lambda=1\) and \(\nu=0\) corresponds to standard online RL. In the challenging AntMaze domains characterized by high-dimensional state and action spaces, as well as sparse rewards, the extrapolation error remains significant even during the online phase [83]. Therefore, we decay \(\lambda\) from \(0.25\) to \(0.5\) and \(\nu\) from \(0.5\) to \(0.005\) (\(1\%\) of its initial value), employing a decay rate of \(0.99\) every \(1000\) gradient steps. Additionally, following previous works [83, 72], we set \(\gamma=0.995\) when fine-tuning on antmaze-large datasets, for both DMG and IQL to ensure a fair comparison. All other training details remain consistent between the offline RL phase and the online fine-tuning phase.

## Appendix D Additional Experimental Results

### Computational Cost

We test the runtime of offline RL algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090. The results of DMG and other baselines are shown in Figure 3. It takes 1.7h for DMG to finish the task, which is comparable to the fastest offline RL algorithm TD3BC [17].

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Hyperparameter & Value \\ \hline \multirow{6}{*}{DMG} & Optimizer & Adam [34] \\  & Critic learning rate & \(3\times 10^{-4}\) \\  & Actor learning rate & \(3\times 10^{-4}\) with cosine schedule \\  & Batch size & 256 \\  & Discount factor & 0.99 \\  & Number of iterations & \(10^{6}\) \\  & Target update rate & 0.005 \\  & Number of Critics & 2 \\  & Penalty coefficient \(\nu\) & \{0.1,10\} for Gym-MuJoCo \\  & & \{0.5\} for Antmaze \\  & Mixture coefficient \(\lambda\) & 0.25 \\ \hline \multirow{6}{*}{IQL Specific} & Expectile \(\tau\) & 0.7 for Gym-MuJoCo \\  & & 0.9 for Antmaze \\ \cline{1-1}  & Inverse temperature \(\alpha\) & 3.0 for Gym-MuJoCo \\ \cline{1-1}  & & 10.0 for Antmaze \\ \hline \multirow{2}{*}{Architecture} & Actor & input-256-256-output \\ \cline{1-1}  & Critic & input-256-256-1 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters of DMG.

### Offline Training Results of DMG on More Random Seeds

The experimental results in the main paper show the mean and standard deviation (SD) over five random seeds. According to [56], we conduct experiments to test DMG on additional random seeds, reporting 95% confidence interval (CI) over 10 random seeds. Table 6 shows the comparison between the new results (10seeds/95%CI) and the previously reported results (5seeds/SD in Table 2) on the D4RL offline training tasks. The results show that our method achieves about the same performance as under the previous evaluation criterion.

### Learning Curves of DMG during Offline Training

Learning curves during offline training on Gym-MuJoCo locomotion tasks and Antmaze tasks are presented in Figure 4 and Figure 5, respectively. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset-v2 & DMG (5seeds/SD) & DMG (10seeds/95\%CI) \\ \hline halfcheetah-m & **54.9\(\pm\)0.2** & **54.9\(\pm\)0.3** \\ hopper-m & **100.6\(\pm\)1.9** & 100.5\(\pm\)1.0 \\ walker2d-m & **92.4\(\pm\)2.7** & 92.0\(\pm\)1.2 \\ halfcheetah-m-r & **51.4\(\pm\)0.3** & **51.4\(\pm\)0.4** \\ hopper-m-r & 101.9\(\pm\)1.4 & **102.1\(\pm\)0.6** \\ walker2d-m-r & 89.7\(\pm\)5.0 & **90.3\(\pm\)2.8** \\ halfcheetah-m-e & 91.1\(\pm\)4.2 & **92.9\(\pm\)2.1** \\ hopper-m-e & **110.4\(\pm\)3.4** & 109.0\(\pm\)2.6 \\ walker2d-m-e & **114.4\(\pm\)0.7** & 113.9\(\pm\)1.2 \\ halfcheetah-e & **95.9\(\pm\)0.3** & **95.9\(\pm\)0.2** \\ hopper-e & 111.5\(\pm\)2.2 & **111.8\(\pm\)1.3** \\ walker2d-e & **114.7\(\pm\)0.4** & 114.5\(\pm\)0.3 \\ halfcheetah-r & **28.8\(\pm\)1.3** & 28.7\(\pm\)1.2 \\ hopper-r & 20.4\(\pm\)10.4 & **21.6\(\pm\)6.6** \\ walker2d-r & 4.8\(\pm\)2.2 & **7.7\(\pm\)3.0** \\ \hline locomotion total & 1182.8 & **1187.2** \\ \hline antmaze-u & **92.4\(\pm\)1.8** & 91.8\(\pm\)1.6 \\ antmaze-u-d & **75.4\(\pm\)8.1** & 73.0\(\pm\)5.0 \\ antmaze-m-p & 80.2\(\pm\)5.1 & **80.5\(\pm\)2.1** \\ antmaze-m-d & **77.2\(\pm\)6.1** & 76.7\(\pm\)3.6 \\ antmaze-l-p & 55.4\(\pm\)6.2 & **56.7\(\pm\)3.6** \\ antmaze-l-d & **58.8\(\pm\)4.5** & 57.2\(\pm\)2.7 \\ \hline antmaze total & **439.4** & 435.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of DMG under different evaluation criteria on D4RL offline training tasks.

Figure 3: Runtime of algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090.

### Learning Curves of DMG during Online Fine-tuning

Learning curves during online fine-tuning on Antmaze tasks are presented in Figure 6. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

## Appendix E Broader Impact

Offline reinforcement learning (RL) presents a promising avenue for enhancing and broadening the practical applicability of RL across various domains including robotics, recommendation systems, healthcare, and education, characterized by costly or hazardous data collection processes. However, it is imperative to recognize the potential adverse societal ramifications associated with any offline RL algorithm. One such concern pertains to the possibility that the offline data utilized for training may harbor inherent biases, which could subsequently permeate into the acquired policy. Furthermore, it is essential to contemplate the potential implications of offline RL on employment, given its contribution to automating tasks conventionally executed by human experts, such as factory automation or autonomous driving. Addressing these challenges is essential for fostering the responsible development and deployment of offline RL algorithms, with the aim of maximizing their positive impact while mitigating negative societal consequences.

From an academic perspective, this research scrutinizes offline RL through the lens of generalization, balancing the need for generalization with the risk of over-generalization. The proposed approach DMG potentially offers researchers a new perspective on appropriately exploiting generalization in offline RL. Besides, DMG also holds the promise to be extended to safe RL [1; 26; 20], multi-agent RL [43; 62; 65; 60; 25], and meta RL [15; 76; 77; 75; 4].

Figure 4: Learning curves of DMG on Gym locomotion tasks during offline training. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

Figure 5: Learning curves of DMG on Antmaze tasks during offline training. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

Figure 6: Learning curves of DMG on Antmaze tasks during online fine-tuning. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please refer to Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Please refer to the code in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The results in the paper are accompanied by standard deviations across multiple seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix D.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Appendix E. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use are explicitly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is well documented and anonymized. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.