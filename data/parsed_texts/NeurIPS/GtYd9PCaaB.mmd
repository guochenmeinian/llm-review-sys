# SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution

 Soufiane Belharbi\({}^{1}\)  Mara KM Whitford\({}^{2,3}\)  Phuong Hoang\({}^{2}\)  Shakeeb Murtaza\({}^{1}\)

**Luke McCaffrey\({}^{2,3,4}\)  Eric Granger\({}^{1}\)**

\({}^{1}\) LIVIA, ILLS, Dept. of Systems Engineering, ETS Montreal, Canada

\({}^{2}\) Goodman Cancer Institute, McGill University, Montreal, Canada

\({}^{3}\) Dept. of Biochemistry, McGill University, Montreal, Canada

\({}^{4}\) Gerald Bronfman Dept. of Oncology, McGill University, Montreal, Canada

{soufiane.belharbi, eric.granger}@etsmtl.ca,

{mara.whitford, phuong.hoang2}@mail.mcgill.ca,

shakeeb.murtaza.1@ens.etsmtl.ca,luke.mccaffrey@mcgill.ca

###### Abstract

Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes at the cellular and subcellular levels. Scanning confocal microscopy allows the capture of high-quality images from thick three-dimensional (3D) samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells. Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality. Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR). These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available datasets. In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy. In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers. It allows to evaluate the performance of SISR methods on three different upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured with four resolutions and three markers, that have been translated in the form of 9,937 patches for experiments with SISR methods. Given the new SR-CACO-2 dataset, we also provide benchmarking results for 16 state-of-the-art methods that are representative of the main SISR families. Results show that these methods have limited success in producing high-resolution textures, indicating that SR-CACO-2 represents a challenging problem. The dataset is released under a Creative Commons license (CC BY-NC-SA 4.0), and it can be accessed freely. Our dataset, code and pretrained weights for SISR methods are publicly available: https://github.com/sbelharbi/sr-caco-2.

SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution

## 1 Introduction

Confocal fluorescence microscopy is a standard imaging technique in biological and biomedical research [32, 33, 76]. It can resolve cell and tissue structures at a maximum lateral resolution of about 200nm, providing high-quality images at a moderate cost [79]. Point scanning confocal microscopy and spinning disk confocal microscopy are two advanced imaging techniques used forhigh-resolution fluorescence imaging. Point scanning confocal microscopy uses a single laser beam to sequentially scan each point of the specimen, providing high-resolution images with excellent optical sectioning capability. This method is ideal for the detailed examination of thick three-dimensional (3D) specimens and allows for precise control over imaging parameters. However, it can be relatively slow and may cause photobleaching due to prolonged exposure to the laser [18, 36]. In contrast, spinning disk confocal microscopy uses a disk with multiple pinholes to simultaneously scan multiple points of the specimen. This results in much faster image acquisition, making it suitable for live cell imaging and dynamic processes. Spinning disk confocal microscopy also reduces photobleaching and phototoxicity compared to point scanning. However, its optical sectioning capability is generally lower, and it does not provide the same level of detail for thick 3D specimens. Therefore, an ongoing challenge with confocal microscopy of thick 3D samples is optimizing imaging parameters (laser intensity, exposure duration, scanning speed) to obtain high-quality images while minimizing phototoxicity. This is particularly challenging when capturing large 3D image stacks or time-lapse videos, where samples are repeatedly imaged 10s or 100s of times.

The success of microscopy is however limited due to the photobleaching of fluorescent probes caused by the excitation light of the emitted laser. Additionally, long exposure to this light can also cause phototoxicity of cells which leads to their damage and even death, further limiting the usage of such techniques in live cell imaging [18, 36]. Moreover, high-resolution images require long exposure to light which increases the risk of these issues and impedes the observation of instantaneous intercellular events. Different approaches have been considered to limit these issues, such as pulsed excitation [61], specialized culture media [6, 7], or more sophisticated techniques such as controlled light exposure microscopy [10, 34]. However, these techniques remain cumbersome, expensive, and less general. Another common and practical approach considers imaging with fluorescence microscopy with short exposure time or low excitation light intensity. This can lead to low-quality images which are subsequently improved using image-enhancing techniques [3, 9, 74, 75, 84].

Deep learning models have recently provided significant improvements in diverse image analysis tasks [26] and biosciences [43, 69]. In particular, they have allowed for considerable advances in single-image super-resolution (SISR) [92] where high-resolution (HR) images are restored from low-resolution (LR) photo-realistic images [52, 85, 95, 99]. However, the success of these models relies heavily on the availability of large-scale public datasets to train a deep SISR model. Few works in microscopy imaging aim to leverage SISR [8, 15, 35, 51, 60, 63, 65]. This extends to different microscopy techniques [79] including standard fluorescence microscopy such as confocal [76], Structured Illumination Microscopy (SIM) [27], STimulated Emission Depletion (STED) microscopy [31], Single-Molecule Localization Microscopy (SMLM) such as PhotoActivated Localization Microscopy (PALM) [5], and Stochastic Optical Reconstruction (STORM) [66]. Some of these works enhance image quality [8, 35, 60, 63], while others aim to upscale images by a factor [51, 65]. In contrast with standard SISR evaluations on photo-realistic images, where an LR image is typically a synthetic downsampled version of an HR image, SISR models for microscopy images deal with real LR images. This translates into a difficult task since real LR images are produced through a different unknown process than a deterministic interpolation which is relatively easy to learn. Despite the success of these SISR models, the availability of datasets remains an issue in microscopy, where datasets are private and inaccessible. This data unavailability hinders progress in microscopy imaging research and prevents leveraging the potential of deep SISR models.

Our work addresses the lack of public datasets for SISR in fluorescence microscopy. To the best of our knowledge, no public dataset exists for this modality. SR-CACO-2 is a new confocal fluorescence dataset proposed for bio-imaging with pairs of HR and (real) LR images for SISR. It is based on human epithelial cell line Caco-2 [46] (ATCC HTB-37). The imaged tiles (see Fig. 2) capture epithelial cells isolated from colon tissue with colorectal adenocarcinoma. Three different proteins are marked and imaged, yielding different views of a cell: Survivin (CELLO), E-cadherin or Tubulin (CELL1), and Histone H2B (CELL2). The dataset is composed of 2,200 unique images forming 22 tiles with the HR tiles measuring \(9,300\times 9,300\) pixels. They are captured by laser scanning over a regular raster. Three different LR scales are provided (Fig. 1): /2, /4, and /8 in addition to the HR scale. The SR-CACO-2 dataset is designed for convenient evaluation of machine learning models - patches are cropped to a size \(512\times 512\) from HR tiles, and their corresponding patches from all LR tiles at different scales. These patches are only comprised of regions of interest (ROI) (_i.e._, cells), while irrelevant regions with black backgrounds are discarded. This allows for the collection of \(9,937\)patches, per scale and cell type, that can be directly employed for the training and testing of deep SISR models.

**Our main contributions are summarized as follows.**

**(1)** A large, diverse, and challenging dataset, SR-CACO-2, for confocal fluorescence SISR microscopy is proposed. It captures an epithelial cell line derived from colorectal adenocarcinoma grown as 3D spheroids. SR-CACO-2 contains 2,200 unique images in HR format and three corresponding real LR versions /2, /4, and /8 covering three separate protein markers: Survivin, E-cadherin or Tubulin, and Histone H2B.

**(2)** A reproducible experimental protocol is introduced to perform machine learning experiments. It allows preparing the dataset for experiments with SISR methods.

**(3)** An extensive benchmarking study on 16 representative SISR methods is provided to assess the quality of super-resolved images. These results show that state-of-the-art SISR methods yield smooth images and fail to correctly produce accurate HR textures across all scales. The full SR-CACO-2 dataset (tiles and patches), code, and pretrained weights of methods are made public. The dataset is freely available under a Creative Commons license (CC BY-NC-SA 4.0).

**(4)** Images produced by all SISR methods are analyzed to assess their efficiency in downstream biology tasks for cell object (nucleus) detection and segmentation. Although they provide poor visual quality, several methods achieve promising results on these tasks compared to LR and HR.

## 2 The SR-CACO-2 Dataset

SR-CACO-2 is a dataset suitable for designing and evaluating machine/deep learning methods that can perform SISR in fluorescence microscopy imaging. In particular, image tiles of size \(\sim 9k\times 9k\) are comprised of \(10\times 10\) unique images, and capture the human epithelial cell line Caco-2. These cells are a well-established model for studying mitotic spindle orientation and epithelial cell polarity. As they are cell lines, they can be cultured over long periods without the requirement for repeated re-isolation from tissue. They are also easily modified with lentivectors to overexpress or knock-down proteins of interest, or to introduce fluorescently-tagged proteins for use in live imaging.

The dataset was captured via fixed-cell imaging since it prevents the cells from moving, allowing for accurate capturing of all scales. The captured images allow training of SISR models that can potentially be used for live-imaging videos. Three proteins involved in cell division were used, as cell division is a behavior commonly studied via live imaging. First, **mCherry-Histone H2B** (CELL2, bright): Histone H2B marks chromatin (the DNA inside cells). It is tagged with the red

Figure 1: Illustration of SR-CACO-2 patch content for cells CELLO, CELL1, and CELL2, and for the HR patch and its corresponding three LR patches (/2, /4, /8). The HR patch size is \(512\times 512\), while the LR patch sizes are \(256\times 256\), \(128\times 128\), and \(64\times 64\) for scales /2, /4, and /8, respectively.

fluorescent protein mCherry. **GFP-tubulin or E-cadherin** (CELL1, medium): Tubulin is a marker for microtubules, which are one of the structural components of cells and can be used to see the cell outline/shape. Microtubules also form the mitotic spindle, which is important during mitosis (cell division) for separating DNA into two new daughter cells. Tubulin is tagged with green fluorescent protein (GFP). E-cadherin is a standard marker for epithelial cells. It is found along the cell-cell contact sites and shows epithelial polarity. It is also useful because it stains the membrane of each cell, so this (or GFP-tubulin), in combination with mCherry-Histone H2B shows the cell shape and the nucleus respectively. **Survivin** (CELL0, dim): Survivin marks the midbody, which is a bridge between cells present during the very last step of cell division.

SR-CAC0-2 is comprised of more than 9k real pairs of LR and HR patches, per scale, and cell type. It is a representative dataset in this field since it is captured using a standard process in fluorescence microscopy. In addition, three different upscaling scenarios are provided, X2, X4, and X8, in addition to HR, allowing a better study of the limit of SISR methods. The process of capturing our SR-CAC0-2 dataset is described in Sec.2.1, while the experimental protocol described in Sec.2.3 allows preparing the dataset for the design and evaluation of SISR models.

### Dataset Capture:

Caco-2 cells (ATCC HTB-37), a colorectal adenocarcinoma cell line, were used for all data collection (Fig.2). These cells were used unmodified, or modified via lentiviral infection for stable overexpression of mCherry-Histone H2B or mCherry-Histone H2B and GFP-tubulin, to label chromosomes and microtubules respectively. Cells were seeded at a density of 12,000 cells per well into \(\upmu\)-slide 8 well plates (Ibidi 80826), pre-coated with 12 \(\upmu\)L Geltrex basement membrane extract (Gibco A1413202). Cells were cultured at 37\({}^{\circ}\)C in 5% CO\({}_{2}\) in Dulbecco's Modified Eagle Medium (DMEM) (Wisent 319-005-CL) supplemented with 10% Fetal Bovine Serum (FBS) (Wisent 091-150, lot 091150) and 2% Geltrex. Media was changed every 2-3 days. Cells were cultured for a total of 5 days to allow the formation of single-layered 3D epithelial structures (cysts) with the open lumen.

After 5 days, cells were fixed in 2% paraformaldehyde in phosphate-buffered saline (PBS) for 10 minutes, followed by immunostaining. Cells were blocked and permeabilized using 10% goat serum, 0.5% fish skin gelatin and 0.5% Triton X-100 in PBS for 1 hour. Primary antibodies were incubated

Figure 2: Methodology for capturing the SR-CAC0-2 dataset follows these steps:

**Step 1**: Lentiviral infection. mCherry-Histone H2B and GFP-tubulin lentivirus are added to a monolayer of Caco-2 epithelial cells. Infection of the monolayer with the viruses results in permanent modification of the cells to expression mCherry-Histone H2B (Magenta circles = red fluorescent chromosomes) and GFP-tubulin (Green rectangles = green fluorescent microtubules).

**Step 2**: Growing 3D epithelial cysts. The Caco-2 monolayer is detached to form a single-cell suspension. Cell culture plates are coated in a layer of Geltrex basement membrane extract (BME), onto which the Caco-2 single-cell suspension is added. After 5 days in culture, the single cells grow to form organized multicellular 3D structures, known as cysts or spheroids.

**Step 3**: Immunostaining. Primary antibodies target survivin (a marker of midbodies, a structure present at the end of cell division) and E-cadherin (a cell membrane marker). Secondary antibodies result in the fluorescence of these markers in either green or far red channels.

**Step 4**: Image acquisition. Tile scans are performed with an LSM700 microscope at 4 resolution levels. All 3 channels are captured with each tile scan: Survivin (CELL0), E-cadherin or GFP-tubulin (CELL1), mCherry-Histone H2B (CELL2).

overnight at 4\({}^{\circ}\)C in the blocking/permeabilization buffer. Primary antibodies used were survivin (midbody marker, Cell Signaling 2808T, 1:300 dilution) and E-cadherin (basolateral cell membrane marker, BD Transduction Laboratories 610181, 1:500 dilution). Secondary antibodies were incubated in the blocking/permeabilization buffer for 1 hour at room temperature. Secondary antibodies used were Alexa Fluor 488 AffiniPure Donkey anti-Rabbit IgG (Jackson ImmunoResearch 711-545-152, dilution 1:750) and Alexa Fluor 647 AffiniPure Donkey Anti-Mouse IgG (Jackson ImmunoResearch 715-605-151, dilution 1:200). Following immunostaining, cells were stored in PBS at 4\({}^{\circ}\)C.

Imaging was performed with a Zeiss LSM700 confocal microscope using a 20x/0.8NA objective lens. Each tile was scanned using a \(10\times 10\) grid at 4 different resolutions. The size of each sub-image in the grid changes with the scale: **HR: 1024x1024 pixels**, scan speed 9, averaging 8; **LR (/2): 512x512 pixels**, scan speed 9, averaging 1; **LR (/4): 256x256 pixels**, scan speed 9, averaging 1; **LR (/8): 128x128 pixels**, scan speed 9, averaging 1. The averaging indicates how many slices are captured and averaged, which reduces noise. Initial images were acquired with HR tile scans completed for each well of cells, before capturing images for subsequent resolutions. Positions were saved using automated Zeiss software, to ensure the same region of interest was imaged at each resolution. Later tiles were acquired with all four resolutions imaged for one well, before imaging subsequent wells.

### Dataset Variability:

During the acquisition of SR-CACO-2, we ensured the technical and biological diversity of samples. To automate the image acquisition, we programmed the microscope to capture multiple sets of 100 images. Each set of 100 images was stitched together (\(10\times 10\)) to form an image tile. Therefore, each tile represents 100 unique images (fields of view) and the 22 tiles correspond to 2,200 unique images. Each image is captured at four scales and for three markers. The automatic collection of a tile of the 4 different resolutions takes over \(\sim\)12-16 hours. We note that each tile was captured in a multi-well plate. The 22 wells were collected from 4 independent experiments using 5-6 wells per plate/experiment, ensuring adequate and standard biological diversity.

To further assess the diversity of our dataset, we performed an object-based analysis of cellular structures captured in the image dataset. The 2,200 (\(22\times 10\times 10\)) unique image fields of view at the high resolution contain \(\sim 16,800\) multi-cellular objects. The cells were cultivated in a three-dimensional protein matrix that allows them to form tissue-like cyst structures that resemble the natural organization of cells in tissues and organs, such as the colon, lung, breast, etc. Cells grown in a three-dimensional matrix exhibit spontaneous variations in cyst size and organization (hollow or solid). We recently [87, 86] reported multiple cellular phenotypes in Caco2 cells that resemble tissue geometries found in healthy and disease states. Our dataset contains cysts with size variations over 2 orders of magnitude. Moreover, shape analysis of the cysts demonstrates substantial variation in aspect ratio and circularity. This reflects cysts covering a spectrum of phenotypes, including single-layered, multi-layered, and solid. This contrasts with cells grown as two-dimensional cultures on plastic or glass surfaces, where there is substantially less variation in size and organization when viewing single cells or confluent patches as is often used in other recently published microscopy datasets. Fig.3 shows the analysis of the cells in the SR-CACO-2 dataset. We can observe from these plots that the cells span a large size range with a relatively circular shape.

Our dataset also includes diverse cellular markers with unique properties. Cells were labeled with Hoechst dye that labels chromatin and is frequently used as a nuclear stain in cell biology. We also include mCherry-Histone, a red-fluorescent protein conjugated to histone H2B that is part of chromatin. This serves as an additional marker of cell nuclei and represents a signal that is often employed in live imaging experiments. mCherry-Histone represents a lower signal than Hoechst

Figure 3: Object-based analysis of cellular structures captured of all the 2,200 high-resolution images (\(22\times 10\times 10\)).

because the cells were selected to have a low-to-moderate expression of this protein and are detected by direct fluorescence. The other two markers (E-cadherin and Survivin) represent typical staining obtained with indirect fluorescence methods, using primary and fluorophore-conjugated secondary antibodies. E-cadherin is expressed in all Caco2 cells and is a cell surface protein. Therefore, this has a distinct expression pattern from the blob-like nuclear labels described above. Despite being expressed in all cells, there is heterogeneity at the cellular level, providing extensive natural variation within our dataset. Survivin is transiently expressed in cells during cell division. This represents an additional unique marker pattern that appears as an intense patch connecting two cells. Survivin is only expressed in a subset of cells during mitosis and cytokinesis, processes that often utilize imaging and machine learning for analysis [40].

### Experimental Protocol:

**Patches sampling.** Since the imaged tiles are quite large ( \(9k\times 9k\)), they are not well suited for machine learning models as they usually operate on smaller images. To this end, we pre-processed the tiles to patches as presented in Fig.4. On HR tiles of the brightest cell, _i.e._ CELL2, a sliding window was employed that scans from left to right and from top to bottom of the entire tile. Windows can overlap by \(25\%\) to increase the chance of capturing large patterns and minimize cropping cells. The window size is set to \(576\times 576\) with \(64\) extra margin at each side that will be useful later for registration. This additional margin was discarded, and only windows of size \(512\times 512\) were preserved as final patches.

Many of the sampled windows will land on empty regions, _i.e._, black background. Across all the 22 HR tiles of CELL2, \(74.24\%\) of total _pixels_ are background while only \(25.74\%\) are foreground (cells). Windows which are more black will not be helpful in learning or evaluation since they are easy to reconstruct using SISR models. They would introduce an evaluation bias by giving a high level of performance, which does not reflect the true performance of the models to super-resolve relevant regions, _i.e._, cells. For this reason, we discarded the windows without enough cell content. To detect these windows, we apply a threshold to tile with a value of \(4\) which is deemed sufficient to spot the cells without including noise. Only windows with at least \(20\%\) of cell content were preserved.

The aforementioned process is performed over HR tiles of CELL2 to determine useful HR patches. Once done, the coordinates of these patches are used to crop HR patches from HR tiles of CELL1 and CELL0 at the same location. Then, the coordinates of LR patches from LR tiles are estimated using the coordinates of the corresponding HR patches. This is performed by downsizing the coordinates using the corresponding downscale factor. For example, for the LR patch at scale /2 for CELL2 in a specific location, we divide the coordinates of its corresponding HR patch of the same cell and location by \(2\).

Scanning the same tiles at different resolutions can lead to small but detectable shifts (fractions of a micron) due to the physical movement of the microscope stage from the position of tile 100 back to the imaging start position at tile 1. This is a common issue for any type of microscope with a mechanical stage. To mitigate this issue, we align patches once cropped. Given an HR patch and its corresponding LR patches, we perform image registration of each LR patch to be aligned with its corresponding HR patch, where the HR patch is used as the image reference. A _global_ shift between the two patches is estimated using TV-L1 algorithm for optical flow estimation [101]1. This allows performing a global shift of all the pixels at once while preserving all their neighbouring pixels. Such a shift can lead to artifacts at the patch boundaries. Therefore, we re-crop the patch at the center with size \(512\times 512\) and discard the extra margin of \(64\) at each side. The final HR patches are \(512\times 512\) while LR patches of /2, /4, and /8 are \(256\times 256\), \(128\times 128\), and \(64\times 64\), respectively.

Footnote 1: In practice, we used the function skimage.registration.optical_flow_tv11 from scikit-image library https://scikit-image.org.

**Data split.** First, the 22 tiles are randomly split into 15 for training, 3 for validation, and 4 for testing. This prevents mixing patches of the same tile across different sets. Tiles with identifiers (ID) = ['9', '10', '14', '20'] are assigned to the test set. Tiles with ID ['7', '11', '19'] are used for the validation set. Finally, tiles with ID = ['1', '2', '3', '4', '5', '6', '8', '12', '13', '15', '16', '17', '18', '21', '22'] are used for training. This is summarized in Tab.1 along with the patch distribution for all subsets.

## 3 Single-Image Super-Resolution Methods

In addition to the proposed dataset, we provide an extensive benchmarking of different SISR methods. In particular, 16 representative SISR methods were selected from 4 common families [92] are evaluated. We also compare to a simple baseline which is interpolation via the bicubic method. Our results can serve as baselines for future comparisons. We selected the following state-of-the-art methods as follows. **Pre-upsampling SR**: SRCNN [19], VDSR [42], DRRN [77], and MemNet [78]. **Post-upsampling SR**: NLSN [59], DFCAN [65], SwinIR [55], EDSR (LIIF) [16], ENLCN [95], GRL [52], ACT [99], and Omni-SR [85]. **Iterative up-and-down sampling SR**: DBPN [28], and SRFBN [54]. **Progressive upsampling SR**: ProSR [89], and MS-LapSRN [44]. The post-upsampling family is the most recent and dominant approach [92]. Other families have seen less progress mainly due to their high computational cost.

Given the limited space, we provide the training details, ablations, evaluation metrics, and more results in the supplementary materials. In summary, we follow a standard performance evaluation protocol for SISR. Three standard measures were used to evaluate the performance of SISR methods - peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) [91], and normalized root mean square error (NRMSE) [65]. The evaluation is performed on the full patch, referred to as _full image_, as commonly done in the literature. Moreover, we report refined performance over the cells only, referred to as _ROI only_, to assess the predictive quality without the black background inside the patches. ROIs are determined by thresholding the HR patch using a set of thresholds2. Performance is computed per threshold over the ROI only. The final performance is reported as the average of all per-threshold performances. SISR models are trained on each cell type, and each scale separately.

Footnote 2: Evaluation thresholds are {4, 5, 6, 7, 8, 9, 10}.

**Super-Resolution Task**: Table 2 shows results on ROI only. Given the space limitations, we present only one method per family. The full results are in the supplementary materials. Overall, SISR

\begin{table}
\begin{tabular}{|l|c c|c c c|} \hline  & **Tiles** & & **Patches** & \\ \hline \hline Data subsets & Train & Validation & Test & Total & Train & Validation & Test & Total \\ \hline Number & 15 & 3 & 4 & 22 & 7,349 & 1,117 & 1,471 & 9,937 \\ \hline Image size: & & & & & & \\ HR & \(\sim 9,318\times 9,318\) & & & \(512\times 512\) & \\ LR /2 & \(\sim 4,658\times 4,658\) & & & \(256\times 256\) & \\ LR /4 & \(\sim 2,328\times 2,328\) & & & \(128\times 128\) & \\ LR /8 & \(\sim 1,164\times 1,164\) & & & \(64\times 64\) & \\ \hline File size: & & & & & \\ HR & 260.5 MB & & & & \\ LR /2 & 65.1 MB & & & & \\ LR /4 & 16.3 MB & & & & \\ LR /8 & 4.1 MB & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Split of SR-CACO-2 into the train, validation and test subsets, along with relevant statistics. Numbers are defined per scale (X2, X4, X8, and HR), and per cell type (CELL0, CELL1, and CELL2). Numbers are defined per scale (X2, X4, X8, and HR), and per cell type (CELL0, CELL1, and CELL2).

Figure 4: Pre-processing of tiles to patches. First, the HR tile of cell CELL2 (the brightest) is binarized to allow localizing cells only (_i.e._ ROI). A sliding window of size \(512\times 512\) is used to scan the entire tile with specific overlap. Only windows with enough cell mass are preserved while the rest are discarded. The same coordinates of the preserved patches are used for CELL1 and CELL2 of HR tiles. Coordinates of LR patches X2, X4, and X8 are computed by scaling down the HR patch coordinates according to the corresponding scale.

methods achieved better results than simple interpolation. As commonly known, the higher the scale factor, the more difficult the task. This is also observed in our results across all methods and performance measures. For instance, SRCNN [19] yields a PSNR of \(35.08\), \(33.17\), and \(29.19\) for X2, X4, and X8, respectively. Another emerging pattern in these results is the discrepancy performance across cell types: CELL0, CELL1, and CELL2. Over PSNR and SSIM performance, CELL0, _i.e._ dimmest, is the easiest while CELL2 is the most difficult, _i.e._ the brightest. Over NRMSE, results are mixed. Across all four studied SISR families, we observe that pre-upsampling SR yielded better results overall across all metrics and scales. This is unexpected since post-upsampling SR are state-of-the-art approaches [92]. These results may suggest that _upscaling_ our LR images through deep models may fail to reconstruct the details of our HR images. This may be explained by the nature of the details present in our HR images which paradoxically comes from noise. Confocal microscopes perform multiple scans to produce a set of slices that are aggregated to produce a final 2D image. Due to an inherited noise produced by the detectors on microscopes, each slice is different. However, averaging these slices decreases the noise and maintains the true signal. The work of [83] shows that it is difficult for CNN-based models to reconstruct input images with noise as they tend to filter out that noise even when it is provided as input. Therefore, it is challenging for such models to produce similar images to our HR images. Such results should be considered in designing future SISR methods for this dataset. We provide in Fig.5 typical visual results for scale X2 over CELL2 which illustrate the limited capacity of these methods to produce accurate HR details as they yield blurry images.

In some cases, the methods fail, yielding a decline in performance such as the case of MS-LapSRN [44]. We have analyzed the reason for this decline in performance and found that the model fails to converge properly as the loss does not decrease as expected. This is most clear on CELL0 data leading to a large decline compared to CELL1 and CELL2. We believe that this is caused by the combination of the very low level of brightness for the cell, and the use of residual learning in MS-LapSRN. This model has difficulty producing very small residuals. Instead, it yields large residuals. This can be confirmed when measuring performance over a full image, i.e., by including a dark background where the typical intensity is 0, which leads to a sharp decline in performance. By inspecting some predicted samples, the intensity of these background regions is typically 10. Note that this is not the case for the ProSR model that also uses residual learning. However, ProSR [89] employs a different architecture than MS-LapSRN which, in this case, is much more efficient due to its depth or its local residual layers. During our experiments, we observed that hyper-parameters do not transfer well across cell types or scales of a model. Therefore, we performed a grid search separately for each model, for each cell type, and each scale. This provides each method with a good and fair chance to perform well. Failure cases are mostly related to the method and the data's nature.

**Downstream Biology Tasks - Object Detection/Segmentation.** To evaluate performance from each SISR model on downstream biology tasks, we evaluated each model at X2, X4, and X8 using an object segmentation problem that is widely used and openly accessible software for image analysis

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**SISR Methods**} & \multirow{2}{*}{Scale} & \multicolumn{4}{c|}{FBNR \(\uparrow\)} & \multicolumn{4}{c|}{MNMSE \(\downarrow\)} & \multicolumn{4}{c|}{SSIM \(\uparrow\)} \\ \cline{3-14}  & & CELL1 & CELL2 & Mean & CELL0 & CELL1 & CELL1 & CELL2 & Mean & CELL0 & CELL1 & CELL2 & Mean \\ \hline \hline \multirow{2}{*}{Bicubic} & X2 & \(35.02\) & \(32.15\) & \(30.38\) & \(32.92\) & \(0.1085\) & \(0.0601\) & \(0.0724\) & \(0.0803\) & \(0.7618\) & \(0.7658\) & \(0.6891\) & \(0.7389\) \\  & X4 & \(35.46\) & \(32.03\) & \(31.10\) & \(32.86\) & \(0.0985\) & \(0.0586\) & \(0.0660\) & \(0.0744\) & \(0.8266\) & \(0.8002\) & \(0.7673\) & \(0.7960\) \\  & X8 & \(31.88\) & \(27.50\) & \(26.10\) & \(28.49\) & \(0.1653\) & \(0.1139\) & \(0.1349\) & \(0.1381\) & \(0.6683\) & \(0.6266\) & \(0.6511\) & \(0.6487\) \\ \hline \hline \multicolumn{14}{|l|}{**Pre-upsampling SR**} \\ \cline{2-14}  & X2 & \(37.54\) & \(34.34\) & \(37.32\) & \(34.36\) & \(0.0710\) & \(0.0450\) & \(0.0550\) & \(0.0551\) & \(0.8517\) & \(0.8524\) & \(0.8210\) & \(0.8417\) \\ \hline \multirow{2}{*}{SRCNN [19]_(_execx,2014_)_} & X4 & \(36.14\) & \(32.73\) & \(32.35\) & \(31.01\) & \(0.0817\) & \(0.0528\) & \(0.0572\) & \(0.0639\) & \(0.8522\) & \(0.8216\) & \(0.8079\) & \(0.8272\) \\  & X8 & \(31.05\) & \(28.04\) & \(26.49\) & \(29.19\) & \(0.1255\) & \(0.0697\) & \(0.1220\) & \(0.1151\) & \(0.7711\) & \(0.7085\) & \(0.7092\) & \(\mathbf{0.7296}\) \\ \hline \multicolumn{14}{|l|}{**Post-upsampling SR**} \\ \cline{2-14}  & X2 & \(37.70\) & \(34.11\) & \(33.51\) & \(35.11\) & \(0.0759\) & \(0.0461\) & \(0.0496\) & \(0.0572\) & \(0.8744\) & \(0.8539\) & \(0.8313\) & \(0.8532\) \\ \hline \multirow{2}{*}{Omini-SR [85]_(_exp,2023_)_} & X4 & \(36.44\) & \(32.59\) & \(32.34\) & \(33.79\) & \(0.0849\) & \(0.0536\) & \(0.0563\) & \(0.0649\) & \(0.8592\) & \(0.8203\) & \(0.8111\) & \(0.8302\) \\  & X0 & \(30.75\) & \(27.16\) & \(25.30\) & \(27.84\) & \(0.1713\) & \(0.1098\) & \(0.1352\) & \(0.1387\) & \(0.6715\) & \(0.6419\) & \(0.6591\) & \(0.6575\) \\ \hline \multicolumn{14}{|l|}{**Iterative up-and-down sampling SR**} \\ \cline{2-14}  & X2 & \(36.13\) & \(33.13\) & \(31.51\) & \(36.31\) & \(36.83\) & \(0.0555\) & \(0.0531\) & \(0.0625\) & \(0.0704\) & \(0.8078\) & \(0.8091\) & \(0.7470\) & \(0.7880\) \\ \hline \multirow{2}{*}{SRFBN [54]_(_exp,2019_)_} & X4 & \(36.08\) & \(32.51\) & \(37.19\) & \(0.46\) & \(0.0911\) & \(0.0545\) & \(0.0605\) & \(0.0687\) & \(0.8405\) & \(0.8417\) & \(0.7880\) & \(0.8147\) \\  & X8 & \(32.27\) & \(27.78\) & \(26.47\) & \(28.84\) & \(0.1560\) & \(0.1091\) & \(0.1278\) & \(0.1310\) & \(0.7022\) & \(0.6549\) & \(0.6904\) & \(0.6825\) \\ \hline \multicolumn{14}{|l|}{**Progressive upsampling SR**} \\ \cline{2-14}  & X2 & \(31.88\) & \(32.36\) & \(29.34\) & \(31.86\) & \(0.1130\) & \(0.0535\) & \(0.0791\) & \(0.0819\) & \(0.7652\) & \(0.8164\) & \(0.7695\) & \(0.7837\) \\ \cline{2-14}  & X4 & \(30.80\) & \(30.39\) & \(31.08\) & \(30.96\) & \(0.1192\) & \(0.0615\) & \(0.0626\) & \(0.0811\) & \(0.7885\) & \(0.7837\) & \(0.7806\) & \(0.7843\) \\ \(ImageJ) [71]3. 30 random images from the test set are selected. Images were resized to 512 x 512 pixels without interpolation. Automatic object segmentation was performed using Star-convex Shapes (StarDist) [70; 93; 94] with the following parameters: modelChoice = Versatile (fluorescent nuclei), normalizeInput = TRUE, percentileBottom = 1.1, percentileTop = 100.0, probTresh = 0.6, nmsThresh = 0.4. The resulting output was the number of objects detected. Manual object counting was performed to create the Ground Truth number of objects. Manual comparisons were made to identify over-segmentation and under-segmentation errors for each image. Graphing and statistical analysis were performed using JMP Pro (version 17.0.0)4. Steel's test for non-parametric multiple comparisons using HR as the control group was performed to assess model performance. Our objective was to evaluate model performance relative to HR, not ground truth. Models that perform as well as HR (i.e. not statistically different than HR at p=0.05) are represented in orange. Fig.6 shows the results for CELL2, X2 where the VDSR, DRRN, NLSN, DFCAN, ENLCN, ACT, Omni-SR, DBPN, and ProSR models show promising results. The supplementary materials provide more analysis.

Footnote 3: https://imagej.net/tj

Footnote 4: https://www.jmp.com/en_ca/home.html

## 4 Ethical Considerations and Dataset Accessibility

The SR-CAC0-2 dataset does not require ethics approval. Caco-2 (_C_ancer _coli_-2) cells were isolated from a 72-year-old white male in the 1970s as part of a collection of gastrointestinal cancer cell lines at the Memorial Sloan Kettering Cancer Center [46]. The use of Caco-2 cells does not require ethics approval because the cells were established prior to the US Federal Policy for the

Figure 5: Illustrative visual results for X2, CELL2 across all SISR models. HR patch file sample name: hr_div_1/tile_HighRes1024-10_293_6912_7552_6400_7040_CELL2.tif.

Protection of Human Subjects (1991) and the UK Human Tissue Act (2004). This is the standard for the biological field for the use of cell lines derived from human tissue that are now considered publicly available. The cells were obtained from the American Type Culture Collection (ATCC)5, a nonprofit organization that collects and distributes cell lines and other biological materials that are publicly accessible. Its legal disclaimers allow for the use of Caco-2 cells for laboratory research https://www.atcc.org/products/htb-37.

Footnote 5: https://www.atcc.org

The dataset is made public and can be freely accessed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0) https://creativecommons.org/licenses/by-nc-sa/4.0/. The dataset which includes full tiles, and patches, is stored permanently on Google Drive. The provided code uses an open-source license and it is located at https://github.com/sbelharbi/sr-caco-2 along with the dataset downloading link. The pretrained weights can be found at https://huggingface.co/sbelharbi/sr-caco-2.

## 5 Conclusion

The few existing datasets for SISR microscopy are mostly private which limits research advancements. Our work seeks to address this gap. In particular, we propose a new dataset, SR-CACO-2, for Confocal Fluorescence Microscopy Image Super-Resolution. It contains 3 scales (X2, X4, and X8, in addition to HR) with three protein markers in different lighting conditions and about 9k real pairs of LR/HR patches. The procedure for data capture and the experimental protocol for the evaluation of SISR methods are follow standard practices, in addition of being reproducible. Our benchmarking of state-of-the-art SISR methods indicates that they cannot accurately produce the HR images, making this new dataset extremely challenging. Aside from the evaluation of quality for super-resolved images, we conducted further analysis to assess their performance in downstream biology tasks such as cell segmentation. Several methods achieved promising results compared to LR and HR images over cell detection/segmentation tasks. SR-CACO-2 can contribute to driving progress in the design of SISR models for microscopy. This extends from fixed-imaging to live-imaging. SISR models trained with our dataset can be employed for live-imaging videos captured with low quality under reduced light exposure to preserve cells. This leads to fast imaging which allows for the observation of instantaneous inter-cellular events with less damage to the cells. Accurately upscaling these videos with SISR models may ultimately open the door to performing biological tasks such as cell tracking, counting, and segmentation.

**The following supplementary materials** contain: the related works, a discussion on the limitations of SR-CACO-2 dataset, an empirical comparison between interpolated and microscopic LR images, an overview of SR-CACO-2 dataset (more statistics and data format), a hosting/licensing/intended uses/ethical considerations, SISR baselines with the evaluation protocol, implementation details, and more results.

Figure 6: Analysis of cell detection performance for CELL2, X2 over 30 random test samples (red dots). More results can be found in the supplementary materials.

## Acknowledgments and Disclosure of Funding

This research was supported in part by the Canadian Institutes of Health Research, the Natural Sciences and Engineering Research Council of Canada, and the Digital Research Alliance of Canada.

## References

* [1] Open source initiative. the 3-clause bsd license. https://opensource.org/license/bsd-3-clause/, July 1999.
* [2] N. Ahn, B. Kang, and K.-A. Sohn. Fast, accurate, and lightweight super-resolution with cascading residual network. In _ECCV_, 2018.
* [3] M. Arigovindan, J. Fung, D. Elnatan, V. Mennella, Y.-H. M. Chan, M. Pollard, E. Branlund, J. Sedat, and D. Agard. High-resolution restoration of 3d structures from widefield images with extreme low signal-to-noise-ratio. _Proceedings of the National Academy of Sciences_, 110(43):17344-17349, 2013.
* [4] J. Bandy and N. Vincent. Nutrition label template for dataset documentation. https://www.overleaf.com/latex/templates/nutrition-label-template-for-dataset-documentation/gxzpbfmncyfp. Accessed: 2024-06-10.
* [5] E. Betzig, G. H. Patterson, R. Sougrat, O. Lindwasser, S. Olenych, J. Bonifacino, M. Davidson, J. Lippincott-Schwartz, and H. F. Hess. Imaging intracellular fluorescent proteins at nanometer resolution. _Science_, 313(5793):1642-1645, 2006.
* [6] A. Bogdanov, E. Bogdanova, D. Chudakov, T. Gorodnicheva, S. Lukyanov, and K. Lukyanov. Cell culture medium affects gfp photostability: a solution. _nature methods_, 6(12):859-860, 2009.
* [7] A. Bogdanov, E. Kudryavtseva, and K. Lukyanov. Anti-fading media for live cell gfp imaging. _PloS one_, 7(12):e53004, 2012.
* [8] C. Bouchard, T. Wiesner, A. Deschenes, A. Bilodeau, B. Turcotte, C. Gagne, and F. Lavoie-Cardinal. Resolution enhancement with a task-assisted gan to guide optical nanoscopy image analysis and acquisition. _Nature Machine Intelligence_, 5(8):830-844, 2023.
* [9] J. Boulanger, C. Kervrann, P. Bouthemy, P. Elbau, J.-B. Sibarita, and J. Salamero. Patch-based nonlocal functional for denoising fluorescence microscopy image sequences. _IEEE transactions on medical imaging_, 29(2):442-454, 2009.
* [10] W. Caarls, B. Rieger, A. De Vries, D. Arndt-Jovin, and T. Jovin. Minimizing light exposure with the programmable array microscope. _Journal of microscopy_, 241(1):101-110, 2011.
* [11] J. Cai, H. Zeng, H. Yong, Z. Cao, and L. Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In _ICCV_, pages 3086-3095, 2019.
* [12] J. Cao, Q. Wang, Y. Xian, Y. Li, B. Ni, Z. Pi, K. Zhang, Y. Zhang, R. Timofte, and L. Van Gool. Caiosr: Continuous implicit attention-in-attention network for arbitrary-scale image super-resolution. In _CVPR_, 2023.
* [13] H. Chen, X. He, L. Qing, Y. Wu, C. Ren, R. Sheriff, and C. Zhu. Real-world single image super-resolution: A brief review. _Information Fusion_, 79:124-145, 2022.
* [14] H.-W. Chen, Y.-S. Xu, M.-F. Hong, Y.-M. Tsai, H.-K. Kuo, and C.-Y. Lee. Cascaded local implicit transformer for arbitrary-scale super-resolution. In _CVPR_, 2023.
* [15] R. Chen, X. Tang, Y. Zhao, Z. Shen, M. Zhang, Y. Shen, T. Li, C. Chung, L. Zhang, J. Wang, B. Cui, P. Fei, Y. Guo, S. Du, and S. Yao. Single-frame deep-learning super-resolution microscopy for intracellular dynamics imaging. _Nature Communications_, 14(1):2854, May 2023.

* [16] Y. Chen, S. Liu, and X. Wang. Learning continuous image representation with local implicit image function. In _CVPR_, 2021.
* [17] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang. Second-order attention network for single image super-resolution. In _CVPR_, 2019.
* [18] R. Dixit and R. Cyr. Cell damage and reactive oxygen species production induced by fluorescence microscopy: effect on mitosis and guidelines for non-invasive fluorescence microscopy. _The Plant Journal_, 36(2):280-290, 2003.
* [19] C. Dong, C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In _ECCV_, 2014.
* [20] C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. _TPAMI_, 38(2):295-307, 2015.
* [21] C. Dong, C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In _ECCV_, 2016.
* [22] M. Elad and D. Datsenko. Example-based regularization deployed to super-resolution reconstruction of a single image. _The Computer Journal_, 52(1):15-30, 2009.
* [23] S. Farsiu, M. Robinson, M. Elad, and P. Milanfar. Fast and robust multiframe super resolution. _IEEE Transactions on Image Processing_, 13(10):1327-1344, 2004.
* [24] S. Gao, X. Liu, B. Zeng, S. Xu, Y. Li, X. Luo, J. Liu, X. Zhen, and B. Zhang. Implicit diffusion models for continuous super-resolution. In _CVPR_, 2023.
* [25] T. Gebru, J. Morgenstern, B. Vecchione, J. Vaughan, H. Wallach, H. Iii, and K. Crawford. Datasheets for datasets. _Communications of the ACM_, 64(12):86-92, 2021.
* [26] I. Goodfellow, Y. Bengio, and A. Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.
* [27] M. Gustafsson. Surpassing the lateral resolution limit by a factor of two using structured illumination microscopy. _Journal of microscopy_, 198(2):82-87, 2000.
* [28] M. Haris, G. Shakhnarovich, and N. Ukita. Deep back-projection networks for super-resolution. In _CVPR_, 2018.
* [29] M. Haris, G. Shakhnarovich, and N. Ukita. Recurrent back-projection network for video super-resolution. In _CVPR_, 2019.
* [30] Z. He and Z. Jin. Latent modulated function for computational optimal continuous image representation. In _CVPR_, 2024.
* [31] S. Hell and J. Wichmann. Breaking the diffraction resolution limit by stimulated emission: stimulated-emission-depletion fluorescence microscopy. _Optics letters_, 19(11):780-782, 1994.
* [32] S. Hickey, B. Ung, C. Bader, R. Brooks, J. Lazniewska, I. Johnson, A. Sorvina, J. Logan, C. Martini, C. Moore, L. Karageorgos, M. Sweetman, and D. Brooks. Fluorescence microscopy--an outline of hardware, biological handling, and fluorophore considerations. _Cells_, 11(1):35, 2021.
* [33] C. Hobson and J. Aaron. Combining multiple fluorescence imaging techniques in biology: when one microscope is not enough. _Molecular Biology of the Cell_, 33(6):tp1, 2022.
* [34] R. Hoebe, C. Van Oven, T. Gadella J., P. Dhonukshe, C. Van Noorden, and E. Manders. Controlled light-exposure microscopy reduces photobleaching and phototoxicity in fluorescence live-cell imaging. _Nature biotechnology_, 25(2):249-253, 2007.
* [35] B. Huang, J. Li, B. Yao, Z. Yang, E. Lam, J. Zhang, W. Yan, and J. Qu. Enhancing image resolution of confocal fluorescence microscopy with deep learning. _PhotoniX_, 4(1):2, 2023.

* [36] J. Icha, M. Weber, J. Waters, and C. Norden. Phototoxicity in live fluorescence microscopy, and how to avoid it. _BioEssays_, 39(8):1700003, 2017.
* [37] M. Irani and S. Peleg. Improving resolution by image registration. _CVGIP: Graphical models and image processing_, 53(3):231-239, 1991.
* [38] X. Ji, Y. Cao, Y. Tai, C. Wang, J. Li, and F. Huang. Real-world super-resolution via kernel estimation and noise injection. In _CVPRw_, 2020.
* [39] A. Khorashadizadeh, A. Chaman, V. Debarnot, and I. Dokmanic. FunkNN: Neural interpolation for functional generation. In _ICLR_, 2023.
* [40] M. Khushi, I. Dean, E. Teber, M. Chircop, J. Arthur, and N. Flores-Rodriguez. Automated classification and characterization of the mitotic spindle following knockdown of a mitosis-related protein. _BMC bioinformatics_, 18:149-159, 2017.
* [41] J. Kim, J. Lee, and K. Lee. Deeply-recursive convolutional network for image super-resolution. In _CVPR_, 2016.
* [42] J. Kim, J. K. Lee, and K. Lee. Accurate image super-resolution using very deep convolutional networks. In _CVPR_, 2016.
* [43] S. Kumar, D. Guruparan, P. Aaron, P. Telajan, K. Mahadevan, D. Davagandhi, and O. Yue. Deep learning in computational biology: Advancements, challenges, and future outlook. _CoRR_, abs/2310.03086, 2023.
* [44] W. Lai, J. Huang, N. Ahuja, and M. Yang. Fast and accurate image super-resolution with deep laplacian pyramid networks. _TPAMI_, 41(11):2599-2613, 2019.
* [45] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In _CVPR_, 2017.
* [46] T. Lea. Caco-2 cell line. _The Impact of Food Bioactives on Health: in vitro and ex vivo models_, pages 103-111, 2015.
* [47] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. In _CVPR_, 2017.
* [48] J. Lee and K. H. Jin. Local texture estimator for implicit representation function. In _CVPR_, 2022.
* [49] H. Li, J. Qin, Z. Yang, P. Wei, J. Pan, L. Lin, and Y. Shi. Real-world image super-resolution by exclusionary dual-learning. _IEEE Transactions on Multimedia_, 25:4752-4763, 2022.
* [50] J. Li, F. Fang, K. Mei, and G. Zhang. Multi-scale residual network for image super-resolution. In _ECCV_, 2018.
* [51] J. Li, G. Tong, Y. Pan, and Y. Yu. Spatial and temporal super-resolution for fluorescence microscopy by a recurrent neural network. _Optics Express_, 29(10):15747-15763, 2021.
* [52] Y. Li, Y. Fan, X. Xiang, D. Demandolx, R. Ranjan, R. Timofte, and L. Gool. Efficient and explicit modelling of image hierarchies for image restoration. In _CVPR_, 2023.
* [53] Z. Li, H. Liu, F. Shang, Y. Liu, L. Wan, and W. Feng. Savsr: Arbitrary-scale video super-resolution via a learned scale-adaptive network. In _AAAI_, 2024.
* [54] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon, and W. Wu. Feedback network for image super-resolution. In _CVPR_, 2019.
* [55] J. Liang, J. Cao, G. Sun, K. Zhang, L. V. Gool, and R. Timofte. Swinir: Image restoration using swin transformer. In _ICCVW_, 2021.
* [56] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced deep residual networks for single image super-resolution. In _CVPRw_, 2017.

* [57] H. Liu, Z. Li, F. Shang, Y. Liu, L. Wan, W. Feng, and R. Timofte. Arbitrary-scale super-resolution via deep learning: A comprehensive survey. _Information Fusion_, 102:102015, 2024.
* [58] J. Ma, J. Yu, S. Liu, L. Chen, X. Li, J. Feng, Z. Chen, S. Zeng, X. Liu, and S. Cheng. Pathsrgan: multi-supervised super-resolution for cytopathological images using generative adversarial network. _IEEE Transactions on Medical Imaging_, 39(9):2920-2930, 2020.
* [59] Y. Mei, Y. Fan, and Y. Zhou. Image super-resolution with non-local sparse attention. In _CVPR_, 2021.
* [60] E. Nehme, L. Weiss, T. Michaeli, and Y. Shechtman. Deep-storm: super-resolution single-molecule microscopy by deep learning. _Optica_, 5(4):458-464, 2018.
* [61] T. Nishigaki, C. Wood, K. Shiba, S. Baba, and A. Darszon. Stroboscopic illumination using light-emitting diodes reduces phototoxicity in fluorescence cell imaging. _Biotechniques_, 41(2):191-197, 2006.
* [62] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao, and H. Shen. Single image super-resolution via a holistic attention network. In _ECCV_, 2020.
* [63] W. Ouyang, A. Aristov, M. Lelek, X. Hao, and C. Zimmer. Deep learning massively accelerates super-resolution localization microscopy. _Nature biotechnology_, 36(5):460-468, 2018.
* [64] E. Pantin and J.-L. Starck. Deconvolution of astronomical images using the multiscale maximum entropy method. _Astronomy and Astrophysics Supplement Series_, 118(3):575-585, 1996.
* [65] C. Qiao, D. Li, Y. Guo, C. Liu, T. Jiang, Q. Dai, and D. Li. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. _Nature Methods_, 18(2):194-202, 2021.
* [66] M. Rust, M. Bates, and X. Zhuang. Sub-diffraction-limit imaging by stochastic optical reconstruction microscopy (storm). _Nature methods_, 3(10):793-796, 2006.
* [67] C. Saharia, J. Ho, W. Chan, T. Salimans, D. Fleet, and M. Norouzi. Image super-resolution via iterative refinement. _IEEE transactions on pattern analysis and machine intelligence_, 45(4):4713-4726, 2022.
* [68] J. Salvador. _Example-Based super resolution_. Academic Press, 2016.
* [69] N. Sapoval, A. Aghazadeh, M. Nute, D. Antunes, A. Balaji, R. Baraniuk, C. Barberan, R. Dannenfelser, C. Dun, M. Edrisi, R. Elworth, B. Kille, A. Kyrillidis, L. Nakhleh, C. R. Wolfe, Z. Yan, V. Yao, and T. Treangen. Current progress and open challenges for applying deep learning across the biosciences. _Nature Communications_, 13(1):1728, Apr 2022.
* [70] U. Schmidt, M. Weigert, C. Broaddus, and G. Myers. Cell detection with star-convex polygons. In _MICCAI_, 2018.
* [71] C. Schneider, W. Rasband, and K. Eliceiri. Nih image to imagej: 25 years of image analysis. _Nature methods_, 9(7):671-675, 2012.
* [72] H. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical evaluation of recent full reference image quality assessment algorithms. _IEEE Transactions on Image Processing_, 15(11):3440-3451, 2006.
* [73] W. Shi, J. Caballero, F. Huszar, J. Totz, A. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016.
* [74] J. Sibarita. Deconvolution microscopy. _Advances in Biochemical Engineering/biotechnology_, 95:201-243, 2005.

* [75] E. Soubies, F. Soulez, M. McCann, T. Pham, L. Donati, T. Debarre, D. Sage, and M. Unser. Pocket guide to solve inverse problems with globalbioim. _Inverse Problems_, 35(10):104006, 2019.
* [76] J. Stockert and A. Blazquez-Castro. _Fluorescence microscopy in life sciences_. Bentham Science Publishers, 2017.
* [77] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep recursive residual network. In _CVPR_, 2017.
* [78] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent memory network for image restoration. In _ICCV_, 2017.
* [79] J. Thorley, J. Pike, and J. Rappoport. Super-resolution microscopy: a comparison of commercially available options. In _Fluorescence microscopy_, pages 199-212. 2014.
* [80] R. Timofte, R. Rothe, and L. Van Gool. Seven ways to improve example-based single image super resolution. In _CVPR_, 2016.
* [81] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution using dense skip connections. In _ICCV_, 2017.
* [82] M. Turkan, D. Thoreau, and P. Guillotel. Self-content super-resolution for ultra-hd up-sampling. In _European Conference on Visual Media Production_, 2012.
* [83] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. _International Journal of Computer Vision_, 128(7):1867-1888, 2020.
* [84] P. Verveer, M. Gemkow, and T. Jovin. A comparison of image restoration approaches applied to three-dimensional confocal and wide-field fluorescence microscopy. _Journal of microscopy_, 193(1):50-61, 1999.
* [85] H. Wang, X. Chen, B. Ni, Y. Liu, and J. Liu. Omni aggregation networks for lightweight image super-resolution. In _CVPR_, 2023.
* [86] L.-T. Wang, M.-E. Proulx, A. Kim, V. Lelarge, and L. McCaffrey. A proximity proteomics screen in three-dimensional spheroid cultures identifies novel regulators of lumen formation. _Scientific reports_, 11(1):22807, 2021.
* [87] L.-T. Wang, A. Rajah, C. Brown, and L. McCaffrey. Cd13 orients the apical-basal polarity axis necessary for lumen formation. _Nature Communications_, 12(1):4697, 2021.
* [88] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In _ECCVw_, 2018.
* [89] Y. Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung, O. Sorkine-Hornung, and C. Schroers. A fully progressive approach to single-image super-resolution. In _CVPRW_, 2018.
* [90] Z. Wang and A. C. Bovik. Mean squared error: Love it or leave it? A new look at signal fidelity measures. _IEEE Signal Processing Magazine_, 26(1):98-117, 2009.
* [91] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* [92] Z. Wang, J. Chen, and S. C. H. Hoi. Deep learning for image super-resolution: A survey. _TPAMI_, 43(10):3365-3387, 2021.
* [93] M. Weigert and U. Schmidt. Nuclei instance segmentation and classification in histopathology images with stardist. In _IEEE International Symposium on Biomedical Imaging Challenges (ISBIC)_, 2022.
* [94] M. Weigert, U. Schmidt, R. Haase, K. Sugawara, and G. Myers. Star-convex polyhedra for 3d object detection and segmentation in microscopy. In _WACV_, 2020.

* [95] B. Xia, Y. Hang, Y. Tian, W. Yang, Q. Liao, and J. Zhou. Efficient non-local contrastive attention for image super-resolution. In _AAAI_, pages 2759-2767, 2022.
* [96] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-resolution: A benchmark. In _ECCV_, pages 372-386, 2014.
* [97] X. Yang, W. Xiang, H. Zeng, and L. Zhang. Real-world video super-resolution: A benchmark dataset and a decomposition based learning scheme. In _ICCV_, 2021.
* [98] J.-E. Yao, L.-Y. Tsao, Y.-C. Lo, R. Tseng, C.-C. Chang, and C.-Y. Lee. Local implicit normalizing flow for arbitrary-scale image super-resolution. In _CVPR_, 2023.
* [99] J. Yoo, T. Kim, S. Lee, S. H. Kim, H. Lee, and T. Kim. Enriched cnn-transformer feature aggregation networks for super-resolution. In _WACV_, 2023.
* [100] Z. Yue, J. Wang, and C. Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. _NeurIPS_, 2024.
* [101] C. Zach, T. Pock, and H. Bischof. A duality based approach for realtime tv-\(L^{\mbox{1}}\) optical flow. In F. A. Hamprecht, C. Schnorr, and B. Jahne, editors, _Pattern Recognition, DAGM Symposium_, 2007.
* [102] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In _Curves and Surfaces_, 2012.
* [103] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image super-resolution using very deep residual channel attention networks. In _ECCV_, 2018.
* [104] Y. Zhang, K. Li, K. Li, B. Zhong, and Y. Fu. Residual non-local attention networks for image restoration. _CoRR_, abs/1903.10082, 2019.
* [105] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual dense network for image super-resolution. In _CVPR_, 2018.
* [106] S. Zhou, J. Zhang, W. Zuo, and C. Loy. Cross-scale internal graph neural network for image super-resolution. _NeurIPS_, 2020.

[MISSING_PAGE_EMPTY:17]

of memory since these very deep models perform all their operations in HR image space. Subsequent works aim to find different approaches to bypass this computational cost while improving the quality of the produced SR image.

_Iterative up-and-down sampling super-resolution._ This family leverages back-projection technique [37] suggested in [80] as a way to improve SR methods. In particular, it is used in an iterative way to reinforce the mutual dependency LR-HR in image pairs. For instance, DBPN [28] uses consecutive up and down layers. A residual error is then added to current image prediction which is then feed to the next layer. The final layer concatenates all the previous predictions, then, followed by a CNN layer which yields the final prediction. A parallel work, SRFBN [54], uses a similar strategy with more dense skip connections. Such approach has been also used in video super-resolution [29] but it has not been adopted largely by the community.

Figure 7: SISR families. The grey block indicates a predefined upsampling such as interpolation. Blue, green, and yellow indicate learnable convolution, upsampling, and downsampling modules, respectively.

_Progressive upsampling super-resolution._ This family of methods tackles two common issues in SISR. Exiting works upscale the LR image at once which is difficult. A second issues is that each scale requires training one model. This category aims to build a single model for all scales, while scaling up the LR image gradually. This creates less cumbersome models for deployment, and also allows multi-step upscaling which facilitates more SISR task. For instance, LapSRN [45] and MS-LapSRN [44] adopted Laplacian pyramid SR network. It is based on a cascade of CNN that learns residual sub-bands of high resolution images, which is added to an interpolated version of the image. This is done repeatedly where the scale increases until reaching the final requested scale where the interpolated images has accumulated all the residuals. ProSR [89] follows a similar approach however it uses the same input for interpolation instead of intermediate resulting image. The methods under this family greatly reduce the learning difficulty by decomposing the task into multiple steps that support each others. However, training models with multi-stage scales remains difficult and vulnerable to instability.

_Post-upsampling super-resolution._ These methods perform most of the computations in LR dimensions while placing learnable upsampling layers toward the end for minimal computations at HR dimensions. The aim is to reduce the training computational cost which is a well known bottleneck in SISR [21, 73]. SISR field is dominated by this framework. Several works have been proposed. FSRCNN [21] improved SRCNN [19] by changing its architecture from a pre-sampling method to post-sampling one. This is done by introducing a deconvolution layer at the output layer while learning an end-to-end mapping from LR to HR space. To furthermore reduce computation, they shrink feature maps towards the input then expand them back toward the last layers while using small filter size but more mapping layers. Different methods focused on improving residual learning for deep models as it has been shown to be beneficial for SISR task, including EDSR [56], RCAN [103], RDN [105], CARN [2], and MSRN [50]. In SRDenseNet [81], authors leverage more skip connections in a very deep network. Adversarial generative models have also been introduced in SRGAN [47] and ESRGAN [88]. Non-local aggregation method has been extensively used in image restoration. Under the assumption that similar patches frequently recur in a natural scene image, several SISR methods have been proposed to leverage this idea. IGNN [106] extends the Non-local self-similarity idea across scales using graphs. Attention-based models have been also developed for SISR task to improve features such as RNAN [104], SAN [17], HAN [62], SwinIR [55], NLSN [59], ENLCN [95]. In GRL method [52], authors propose to explicitly model image hierarchies in the global, regional, and local range into a new self-attention module. In ACT method [99], authors improve self-attention by including local features from CNNs and long-range multi-scale dependencies captured by transformers. Omni-SR method leverages both spatial and channel-wise self-attention [85]. Recently, iterative-based approaches such as diffusion methods have attracted the community attention [67, 100]. However, they remain computationally expensive.

A recent line of research tackles the task of arbitrary-scale SR (ASSR) [16, 24, 30, 39, 53, 57, 98] where the aim is to design techniques that can upscale an image to any arbitrary scale, mainly by relying on neural fields [12, 14, 48] to to represent continuous signals that can be sampled at arbitrary rates. For instance, in [16], authors employ local implicit neural representation to represent images in a continuous domain. In particular, they propose a Local Implicit Image Function (LIIF) method. ASSR is achieved by replacing standard fixed-upsampling techniques with a multilayer perceptron (MLP) used to query pixel-value at any coordinate and any scale.

**2. Microscopy SISR**: Compared to photo-realistic field, microscopy domain has seen very limited work. Each existing work is specialized in one single imaging technique. Some of these methods enhance image quality [8, 35, 60, 63], while others aim to upscale images by a factor [51, 65]. In [8], authors propose task-assisted generative adversarial network (TA-GAN) which incorporates an auxiliary task such as segmentation which has shown to be better than unassisted methods. It has been used to convert confocal to STED images. Authors in [35] perform a similar task using GANs. Deep models haven used in [60] for STORM modality while PALM modality has been tackled in [63]. Additionally, recurrent neural networks (RNN) have been used to upscale images for STORM modality [51]. Authors in [65] propose deep Fourier channel attention network for SIM modality. In [58], authors consider a GAN-based approach SISR of cytopathological images. All these works employ private real or synthetic datasets. Only the work of [65] provides a public dataset for SIM modality. Such lack of public data can contribute in slowing down research in microscopy SISR field. Our work is an effort to provide a public dataset for fluorescence microscopy SISR task for model's training and evaluation.

It is worth mentioning that there is an emerging line of research in SISR for natural scene images that deal with real-world SISR [11, 13, 38, 49, 97]. Such methods avoid using simulated low resolution images obtained by interpolation. Instead, real low resolution images are used in order to deal with real-world scenarios.

## Appendix B Sr-Caco-2 Dataset Limitations

First, it would have been interesting to supplement this data set with oversaturation/undersaturation or very bright vs very dim imaging, as live imaging of cells requires low laser power to minimize damage to cells. This therefore often results in dim images, compared to what can be acquired using fixed and immunostained images. Second, Caco2 cells were ideal for cell for use in this study due to the ease and efficiency of culturing them in 3D and their ability to efficiently polarize. However, often the next step in biological studies is to use human patient-derived tissue or mouse tissue. Future studies could involve the imaging of either primary human or primary mouse organoids. Third, live cells are more dynamic, while fixed cells are static. Live cells are actively growing, dividing, and synthesizing building blocks. This results in changes to the markers that we have used, like mCherry-Histone H2B and GFP-tubulin, which can change shape and position within an individual cell over time, and provide important biological information, such as the stage of cell division. This information cannot be followed for an individual cell once fixed and immunostained, which may impact the SISR model performance. Although, live cells look nearly identical to fixed cells, using fixed dead cells for training SISR models remains a practical solution. Ideally, collecting images of live cells as time-lapse videos for training could be better choice as it reflects the test time scenario. However, acquiring such data is extremely challenging for the task of SISR due to cell movements or appearance of new cells due to cell division. This will create a large misalignment between tiles at different scales, rendering the training impractical. Forth, the dataset is limited in term of the types of proteins that have been captured. We are only looking at chromosomes, cell membrane/cell structural proteins, and proteins specific for division. The dataset could perhaps be improved with an increase in the number and type of markers used, such as proteins that appear as puncta in the cytoplasm, proteins localized to the apical membrane (mutually exclusive localization with E-cadherin), basement membrane proteins (extracellular matrix found around the outside of cysts), and signaling molecules.

## Appendix C Low Resolution Images: Interpolation vs Microscope

We conducted an empirical analysis to compare both images: real (microscopic) LR vs. bicubic LR. Here are some relevant observations:

* BICUBICI histogram in Fig.8]. The difference is mainly concentrated over cell regions.
* Compared to real LR, bicubic LR can maintain better cell structure and even full cells from the HR images (see the real and bicubic images in Fig.8).
* Intensity span: depending on the image, the real LR images are sometimes much more expressive in terms of pixel intensity as they span larger intensities compared to bicubic LR (see the pixel-intensity histogram in Fig.8).
* Real LR images are more noisy compared to bicubic LR ones which tend to be very smooth (see the visual result of the Laplace filter, and the histogram of its absolute value [Laplace LRl in Fig.8]. Note that the noise in real LR is the results of a single scan while HR are scanned 9 times to be averaged.

This large contrast between real and bicubic LR only confirms that bicubic LR cannot be used as a replacement to substitute real LR images for training and evaluation, as done in SR methods over natural scene images. Real LR images must be used to effectively simulate a realistic scenario for the model at deployment time. Therefore our collection of real LR images is extremely valuable to designing realistic SR models.

## Appendix D Overview of Dataset

### Dataset statistics

We provide a nutrition label for our SR-CACO-2 dataset (Fig.9). Furthermore, Tab.3 presents more statistics about ROI distribution per HR tile for SR-CACO-2 dataset. Note that cells cover only small part of a tile in general.

### Data Format and Organization

Figure 10 shows the file organization of patches and tiles for SR-CACO-2 dataset. The names of the tiles follows this format:

1. HR tiles: HighRes1024/HighRes1024-<ID>.tif
2. LR tiles (/2): LowRes512/LowRes512-<ID>.tif
3. LR tiles (/4): LowRes256/LowRes256-<ID>.tif
4. LR tiles (/8): LowRes128/LowRes128-<ID>.tif

Figure 8: Interpolated vs Microscopic low resolution image for scale /2 and CELLO. For interpolation, we used bicubic interpolation of HR image to obtain low resolution with down scale factor of 2.

## 6 Conclusion

Figure 9: A data card styled (nutrition labels) for SR-CACO-2 dataset following [4, 25].

\begin{table}
\begin{tabular}{l c c c c} \hline \multirow{2}{*}{Tile name} & \multirow{2}{*}{Size (\(h\times w\))} & \multicolumn{3}{c}{Cell area / background area} \\  & & CELLO & CELLO & CELLO \\ \hline \multicolumn{5}{c}{Train set} \\ \hline HighRes1024-1.tif & 9318x9318 & \(9.01/90.98\) & \(12.63/87.36\) & \(13.31/86.68\) \\ HighRes1024-2.tif & 9318x9318 & \(18.34/81.65\) & \(28.54/71.45\) & \(26.24/73.75\) \\ HighRes1024-3.tif & 9318x9318 & \(9.50/90.49\) & \(13.77/86.22\) & \(14.80/85.19\) \\ HighRes1024-4.tif & 9318x9318 & \(7.32/92.67\) & \(10.47/89.52\) & \(10.69/89.30\) \\ HighRes1024-5.tif & 9318x9318 & \(15.88/84.11\) & \(20.92/79.07\) & \(22.06/77.93\) \\ HighRes1024-6.tif & 9317x9317 & \(14.29/85.70\) & \(17.02/82.97\) & \(14.33/85.66\) \\ HighRes1024-8.tif & 9317x9317 & \(22.89/77.10\) & \(28.19/71.80\) & \(25.12/74.87\) \\ HighRes1024-12.tif & 9318x9318 & \(13.57/86.42\) & \(27.64/72.35\) & \(39.15/60.84\) \\ HighRes1024-13.tif & 9318x9319 & \(14.74/85.25\) & \(19.30/80.69\) & \(27.20/72.79\) \\ HighRes1024-15.tif & 9319x9318 & \(12.70/87.29\) & \(25.61/74.38\) & \(35.14/64.85\) \\ HighRes1024-17.tif & 9318x9319 & \(20.77/79.22\) & \(38.70/61.29\) & \(47.23/52.76\) \\ HighRes1024-18.tif & 9319x9318 & \(12.70/87.29\) & \(25.61/74.38\) & \(35.14/64.85\) \\ HighRes1024-21.tif & 9318x9318 & \(13.57/86.42\) & \(27.64/72.35\) & \(39.15/60.84\) \\ HighRes1024-22.tif & 9317x9318 & \(15.47/84.52\) & \(23.85/76.14\) & \(27.37/72.62\) \\ \hline \multicolumn{5}{c}{Validation set} \\ \hline HighRes1024-7.tif & 9317x9317 & \(19.90/80.09\) & \(25.91/74.08\) & \(21.27/78.72\) \\ HighRes1024-11.tif & 9317x9317 & \(15.77/84.22\) & \(19.53/80.46\) & \(17.84/82.15\) \\ HighRes1024-19.tif & 9317x9318 & \(10.16/89.83\) & \(17.68/82.31\) & \(22.65/77.34\) \\ \hline \multicolumn{5}{c}{**Average set**} \\ \hline \multicolumn{5}{c}{Test set} \\ \hline HighRes1024-9.tif & 9317x9317 & \(11.79/88.20\) & \(14.76/85.23\) & \(11.26/88.73\) \\ HighRes1024-10.tif & 9317x9317 & \(19.85/80.14\) & \(22.67/77.32\) & \(19.37/80.62\) \\ HighRes1024-14.tif & 9317x9318 & \(10.16/89.83\) & \(17.68/82.31\) & \(22.65/77.34\) \\ HighRes1024-20.tif & 9318x9319 & \(14.74/85.25\) & \(19.30/80.69\) & \(27.20/72.79\) \\ \hline \multicolumn{5}{c}{**Average set**} \\ \hline \multicolumn{5}{c}{**Total average**} \\ \hline \multicolumn{5}{c}{} \\ \hline \end{tabular}
\end{table}
Table 3: Statistics of SR-CACO-2 dataset, HR tiles: tiles size, and foreground (cell) / background areas (\(\%\)) of **high-resolution tiles**. The splits of train, validation, and test sets are done randomly. Foreground regions are determined automatically by thresholding where the threshold is set to 4 to capture all relevant parts.

where ID \(\in\) ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']. Note that a tile contains all the three markers, one in each plane: plane 0 for CELL1, plane 1 for CELL1, plane 2 for CELL2. The names of the patches follows this format:

1. HR tiles: HighRes1024/HighRes1024-<ID>_S_h0_h1_w0_v1_CELL<I>.tif
2. LR tiles (/2): LowRes1512/LowRes1512-<ID>_S_h0_h1_w0_v1_CELL<I>.tif
3. LR tiles (/4): LowRes256/LowRes256-<ID>_S_h0_h1_w0_v1_CELL<I>.tif
4. LR tiles (/8): LowRes128/LowRes128-<ID>_S_h0_h1_w0_v1_CELL<I>.tif

where S is the patch ID, a sequential counter of the valid patches in a tile, h0_h1_w0_w1 are the coordinates of the patch, I is the cell \(\in\) [0, 1, 2].

## Appendix E Dataset Publishing and Usage

### Dataset Hosting and Licensing

The dataset and code are available for researchers. The dataset is released under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0) https://creativecommons.org/licenses/by-nc-sa/4.0/. It is freely accessible. The dataset represents 1.8 GB for all the tiles, and 3.4 GB for all the patches. The dataset is stored in Google Drive 6 intended for long time availability. The Google Drive account is accessible by the all the authors of the dataset for future updates, and withdrawal if needed. The Github code web-page is meant as the front-end site for the dataset https://github.com/sbellharbi/sr-caco-2. The ReamMe file contains a brief overview of the dataset, its download link, link to a permanent arXiv7 pdf with the content of this

Figure 10: File structure of SR-CAC0-2 dataset for patches and tiles folders.

paper, link to download the pretrained weights, and installation and usage of the code. The code uses an open-source 3-Clause BSD License [1]. Due to their large size, the pretrained weights of different benchmarked methods are stored in Hugging Face https://huggingface.co/sbelharbi/sr-caco-2 under the same license as the code.

### Intended Uses and Ethical Considerations

The SR-CACO-2 dataset does not require ethics approval. Caco-2 (_C_a_ccer _c_oli-2) cells were isolated from a 72 year old white male in the 1970s as part of a collection of gastrointestinal cancer cell lines at the Memorial Sloan Kettering Cancer Center [46]. The use of Caco-2 cells does not require ethics approval because the cells were established prior to the US Federal Policy for the Protection of Human Subjects (1991) and the UK Human Tissue Act (2004). This is the standard for biological field to use of cell lines from human that are now considered publicly available. The cells were obtained from the American Type Culture Collection (ATCC)8, a nonprofit organization that collects and distributes cell lines and other biological materials that are publicly accessible.

Footnote 8: https://www.atcc.org

The dataset is made available freely available under Creative Commons license CC BY-NC-SA 4.0. No personal information of human subject who provided the cells is available. Based on our dataset, it is impossible to uncover their identity.

The dataset is meant primarily to train models for SISR task for confocal fluorescence microscopy imaging for 3 different scales (X2, X4, and X8). Accurate models trained with our dataset can be efficiently used to perform SISR on live-imaging of low-resolution videos. This allows fast imaging at low-resolution, and therefore, reduce the cell damage caused by long exposure to light, and also allowing the observation of instantaneous inter-cellular events. Additionally, improving live-imaging videos quality has a huge benefit for downstream biological tasks including cell segmentation, cell counting, cell movement tracking, identifying and characterizing cell divisions.

Provided the adequate annotation, this dataset could be used for other biological/cell related applications for computational training such as: **Cell segmentation**: the availability of both nuclei and cell membranes makes cell segmentation possible. However, the dataset does not exhibit multiple different cell types/cell shapes, making it less universal. **Cell count**: Models can be trained to identify cell number, cell shape, perhaps also identifying dividing cells as well, since there is staining for chromosomes as well as Tubulin/Survivin, which both mark certain stages of division.

We are not aware of any way to misuse this dataset. The authors declare that they bear all responsibility in case of any violation of rights during the collection of the data or other work, and will take appropriate action when needed, _e.g._, by removing data with such issues.

## Appendix F SISR Baselines

### Evaluation Metrics

To provide quantitative comparison of different methods, we consider standard super-resolution performance measures [92]. In particular, we use Peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) [91], and normalized root mean square error (NRMSE) [65].

Let us consider \(\bm{Y}\), and \(\bm{\hat{Y}}\) the ground truth, and predicted high resolution images, respectively.

The PSNR is one of the most common metrics for image reconstruction task [92]. It is defined as the log of the ratio between the maximum pixel value and the mean squared error (MSE) between the ground truth and the prediction,

\[\texttt{PSNR}(\bm{Y},\bm{\hat{Y}})=10\cdot\log_{10}\left(\frac{m^{2}}{\frac{1 }{n}\sum_{i=1}^{n}(\bm{Y}_{i}-\bm{\hat{Y}}_{i})^{2}}\right)\,\] (1)

where \(m\) is the maximum pixel value which is \(m=255\) in the case of 8-bit image representation, and \(n=h\times w\) is the total number of pixels in the image with height \(h\), and width \(w\). Note that high PSNR measure indicates better predicted image. A related measure to PSNR is NRMSE which normalizes the MSE measure as follows,\[\texttt{NRMSE}(\bm{Y},\bm{\hat{Y}})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\bm{Y}_{i}- \bm{\hat{Y}}_{i})^{2}}/(\max(\bm{Y})-\min(\bm{Y}))\.\] (2)

Note that low NRMSE indicate better predicted image.

Different from PSNR and NRMSE measures which compare only pixel intensities, SSIM considers a neighboring window around a pixel to capture more local statistics [91]. This local context is modeled via a kernel capturing specific size. In particular, SSIM measure combines luminance, contrast, and structure. Since this measure has more fidelity to signal quality [72, 90], it has been largely adopted in super-resolution community [92].

We denote by \(\bm{y}\), and \(\bm{\hat{y}}\) as local patches at the same location from \(\bm{Y}\), and \(\bm{\hat{Y}}\), respectively. The SSIM measure at this patch is referred to as \(\texttt{ssim}\), and it measures as,

\[\texttt{ssim}(\bm{y},\bm{\hat{y}})=\frac{(2\mu_{\bm{y}}\mu_{\bm{\hat{y}}}+C_{1 })+(2\sigma_{\bm{y}\bm{\hat{y}}}+C_{2})}{(\mu_{\bm{y}}^{2}+\mu_{\bm{\hat{y}}}^{ 2}+C_{1})(\sigma_{\bm{y}}^{2}+\sigma_{\bm{\hat{y}}}^{2}+C_{2})}\,\] (3)

where \(\mu_{(\cdot)}\) is the intensity mean of the corresponding patch, \(\mu_{(\cdot)}\) is its corresponding variance, while \(\mu_{\bm{y}\bm{\hat{y}}}\) is 2 patches covariance. The SSIM between two images \(\bm{y}\), and \(\bm{\hat{y}}\) is the average \(\texttt{ssim}\) at each location,

\[\texttt{SSIM}(\bm{Y},\bm{\hat{Y}})=\frac{1}{n}\sum_{i=1}^{n}\texttt{ssim}(\bm {Y}[i],\bm{\hat{Y}}[i])\,\] (4)

where \(\bm{Y}[i]\) is the patch at location \(i\) of image \(\bm{Y}\). High SSIM measure indicates better predicted image.

The evaluation is performed on the full patch, referred to as _full image_, as commonly done. We furthermore report refined performance over the cells only, referred to as _ROI only_, to better assess the prediction quality without the black background inside the patches. ROIs are determined by thresholding the HR patch using a set of thresholds9. A performance is computed per-threshold over the ROI only. The final reported performance is the average of all per-threshold performances. Models are trained on each cell type, and each scale, separately.

Footnote 9: Evaluation thresholds are [4, 5, 6, 7, 8, 9, 10].

### Implementation Details

Training deep super-resolution methods typically requires defining a set of hyper-parameters. Most relevant ones are the batch size, patch size, and training loss. Before training all methods, we conduct an initial ablation over these hyper-parameters over SRCNN method [19] for CELL2 with scale X2. In term of training loss, we compared three standard losses commonly used in super-resolution task [92], \(\mathcal{L}_{1}\), \(\mathcal{L}_{2}\), and \(\mathcal{L}_{\texttt{ssim}}\). Over a single predicted image \(\bm{\hat{Y}}\), and its corresponding HR ground truth \(\bm{Y}\), these losses are defined as,

\[\mathcal{L}_{1}(\bm{Y},\bm{\hat{Y}}) =\|\bm{Y}-\bm{\hat{Y}}\|_{1}\.\] (5) \[\mathcal{L}_{2}(\bm{Y},\bm{\hat{Y}}) =\|\bm{Y}-\bm{\hat{Y}}\|_{2}\.\] (6) \[\mathcal{L}_{\texttt{ssim}}(\bm{Y},\bm{\hat{Y}}) =\texttt{SSIM}(\bm{Y},\bm{\hat{Y}})\.\] (7)

Stochastic Gradient Descent (SGD) is used for their optimization. Table 4 shows the performance when using different losses. Since the quality of SR over cells is the most important, compared to black background, we opted for the case combining \(\mathcal{L}_{2}\) and \(\mathcal{L}_{\texttt{ssim}}\) which has the best performance. Therefore, all the next reported results are obtained by minimizing this composite loss,

\[\mathcal{L}(\bm{Y},\bm{\hat{Y}})=\mathcal{L}_{2}(\bm{Y},\bm{\hat{Y}})-\lambda \mathcal{L}_{\texttt{ssim}}(\bm{Y},\bm{\hat{Y}})\.\] (8)

Using Eq.8 for training, we explored other hyper-parameters. Results are reported in Fig.11. In all our next experiments, we set patch size to \(96\times 96\), batch size to \(8\), kernel size for SSIM loss to \(19\times 19\), and its \(\lambda=5\). The remaining important hyper-parameter which is the learning rate is tuned with respect to each method, scale, and cell type over 26 values that ranges from \(0.0009\) to \(0.01\). We use 100 epochs for training amounting to a total of \(\sim 92k\) SGD updates on a single NVIDIA A100 GPU. The training time of each method varies from \(1.5\) hours to \(16\) hours depending on the scale. In total, we conducted around \(\sim 4.5k\) experiments with total computation time of \(\sim 5k\) hours. The training time per method is presented in Tab.5.

### More Results

**SISR task.** Tables 6, 7 show more results over all the methods **on ROI only**, _i.e.,_ **cells**, and **on full image**, respectively. Figure 12 shows the PSNR performance in a visual form across all the cells, scales, and methods. Note that performance over full images are relatively way higher compared to when only ROI is considered.

\begin{table}
\begin{tabular}{l l l l l l} \multicolumn{2}{c}{ROI only} & \multicolumn{3}{c}{Full image} \\ \multirow{2}{*}{Train loss / Performance} & \multirow{2}{*}{PSNR \(\uparrow\)} & \multirow{2}{*}{INMSE \(\downarrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multirow{2}{*}{PSNR \(\uparrow\)} & \multirow{2}{*}{NRMSE \(\downarrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} \\ \cline{1-1} \cline{6-6} \cline{7}  & & & & & & \\ \hline Bicubic & \(30.38\) & \(0.0723\) & \(0.6891\) & \(36.33\) & \(0.0373\) & \(0.8858\) \\ \hline \(\mathcal{L}_{2}\) & \(33.13\) & \(0.0520\) & \(0.8140\) & \(37.23\) & \(0.0332\) & \(0.8042\) \\ \(\mathcal{L}_{1}\) & \(33.03\) & \(0.0530\) & \(0.8166\) & \(39.17\) & \(0.0263\) & \(0.9352\) \\ \(\mathcal{L}_{min}\) & \(33.19\) & \(0.0518\) & \(\mathbf{0.8223}\) & \(\mathbf{39.23}\) & \(\mathbf{0.0260}\) & \(\mathbf{0.9356}\) \\ \(\mathcal{L}_{2}+\lambda\mathcal{L}_{asin}\) & \(\mathbf{33.42}\) & \(\mathbf{0.0499}\) & \(0.8210\) & \(37.81\) & \(0.0311\) & \(0.8437\) \\ \(\mathcal{L}_{1}+\lambda\mathcal{L}_{asin}\) & \(33.38\) & \(0.0501\) & \(0.8206\) & \(37.75\) & \(0.0313\) & \(0.8426\) \\ \hline \end{tabular}
\end{table}
Table 4: Ablation of training loss ablation over SR-CACO-2: performance on test set for CELL2 with scale X2 for both cases ROI only and full image; using SRCNN method [19]. Training is performed for 20 epochs.

\begin{table}
\begin{tabular}{|l|c|c|c|} \multicolumn{2}{c}{} & \multicolumn{2}{c|}{Training time (h)} \\ \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{(100 epochs)} \\ \hline Methods / Scale & X2 & X4 & X8 \\ \hline
**Pre-upsampling SR** & & & \\ \hline SRCNN [19] _(eccv,2014)_ & 3.4 & 2.0 & 1.4 \\ VDSR [42] _(cvpr,2016)_ & 3.5 & 2.0 & 1.4 \\ DRRN [77] _(cvpr,2017)_ & 3.6 & 2.0 & 1.3 \\ MemNet [78] _(iccv,2017)_ & 11.6 & 11.6 & 11.8 \\ \hline
**Post-upsampling SR** & & & \\ \hline NLSN [59] _(cvpr,2021)_ & 5.5 & 2.5 & 2.0 \\ DFCAN [65]_(nat. methods,2021)_ & 3.5 & 2.0 & 1.7 \\ SwinIR [55]_(iccvw,2021)_ & 6.4 & 3.3 & 2.9 \\ EDSR (LIIF) [16]_(cvpr,2021)_ & 3.4 & 2.8 & 3.0 \\ ENLCN [95]_(aaai,2022)_ & 4.0 & 2.2 & 2.0 \\ GRL [52]_(cvpr,2023)_ & 16.2 & 12.6 & 12.2 \\ ACT [99]_(cvpr,2023)_ & 5.8 & 4.7 & 4.4 \\ Omni-SR [85]_(cvpr,2023)_ & 10.5 & 9.8 & 9.9 \\ \hline
**Iterative up-and-down sampling SR** & & & \\ \hline DBPN [28]_(cvpr,2018)_ & 3.8 & 2.9 & 3.0 \\ SRFBN [54]_(cvpr,2019)_ & 3.7 & 2.2 & 2.2 \\ \hline
**Progressive upsampling SR** & & & \\ \hline ProSR [89]_(cvprw,2018)_ & 3.7 & 3.5 & 3.3 \\ MS-LapSRN [44]_(tpami,2019)_ & 3.4 & 2.0 & 1.3 \\ \hline \end{tabular}
\end{table}
Table 5: Models training time for 100 epochs.

Figure 11: Ablations over general hyper-parameters: Patch size (a), batch size (b), SSIM kernel size (c), and its \(\lambda\) weight (d). The PSNR performance on test set for CELL2 with scale X2 for ROI only is reported using SRCNN model [19] trained for 20 epochs.

Figure 12: Super-resolution performance of different methods: PSNR performance over SR-CACO-2 test set **on ROI only**, _i.e._**cells,** for the scales: X2, X4, and X8.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{**SISR Methods**} & \multicolumn{1}{c|}{**Scale**} & \multicolumn{1}{c|}{**CS

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{**SISR Methods**} & \multicolumn{1}{c|}{**SESR \(\uparrow\)**} &

[MISSING_PAGE_EMPTY:32]

## Appendix A

Figure 14: Analysis of cell detection performance: CELLO.

## Appendix A

Figure 15: Analysis of cell segmentation performance: CELLO, CELL2, X2.

Figure 16: Cell segmentation example using different methods: CELL2, X2. Red arrow for undersegmented errors; Blue arrow for oversegmented error; and green arrow for boundary error. HR patch file sample name: hr_div_1/tile_HighRes1024-20_396_6912_7552_5632_6272_CELL2.tif. In all cases, the brightness has been enhanced just for visualization. Best visualized in color.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? Please see the supplementary materials. 3. Did you discuss any potential negative societal impacts of your work? Please see the supplementary materials. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? We did not include theoretical results. 2. Did you include complete proofs of all theoretical results? We did not include theoretical results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Please see the code Github link. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Please see the supplementary materials. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Due to the high computational cost of the experiments and the limited computation resources we did not perform several runs with different seeds. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Please see the supplementary materials.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 2.1. We follow ATCC recommendation for citation when using Caco2 cells. 2. Did you mention the license of the assets? See Section 4. The legal disclaimers of ATCC allow the use of Caco-2 cells for laboratory research https://www.atcc.org/products/htb-37. 3. Did you include any new assets either in the supplemental material or as a URL? We saw assets are not needed. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? See Section 4. There is no ethics/consent approval needed for this because it is not considered "patient-derived material". The Caco2 cells were derived from a patient decades ago and are now considered in the public domain. This is the standard for our field to use of cell lines from human that are now considered publicly available. The cells were obtained from the American Type Culture Collection (ATCC), a nonprofit organization that collects and distributes cell lines and other biological materials that are publicly accessible. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? See the supplementary materials. There is no personally identifiable information or offensive content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We did not used crowdsourcing 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? We did not used crowdsourcing 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We did not used crowdsourcing