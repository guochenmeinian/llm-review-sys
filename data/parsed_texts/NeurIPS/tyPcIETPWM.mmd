# Conditional Outcome Equivalence: A Quantile Alternative to CATE

 Josh Givens

University of Bristol

josh.givens@bristol.ac.uk

Henry W J Reeve

University of Bristol

henry.reeve@bristol.ac.uk

Song Liu

University of Bristol

song.liu@bristol.ac.uk

Katarzyna Reluga

University of Bristol

katarzyna.reluga@bristol.ac.uk

###### Abstract

The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes the CQTE especially valuable in cases where the effect of a treatment is not well-modelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to the CATE where it is possible to obtain high-quality estimates which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of the CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to the CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles.

## 1 Introduction

In many real world scenarios such as personalised treatment allocation and individual level policy decisions, understanding the effect of a treatment/intervention at an individual level is invaluable in providing bespoke care. The field which aims to understand a treatment's effect given certain covariates is referred to as heterogeneous treatment effect (HTE) estimation and has seen popularity across many applications [11; 10; 24; 20]. Within HTE, the conditional average treatment effect (CATE) has proved itself to be a popular target of study in this area due to its simplicity and interpretability [2; 13; 28]. A key limitation of the CATE however, is that it fails to paint a full picture of the differences between distributions of the two responses. In addition, it can be sensitive to outliers, with extreme values leading to a biased outcome. As such, the conditional quantile treatment effect (CQTE), an estimand which compares the conditional quantiles of the distributions in the treated and untreated populations, has established itself as a popular alternative [1; 5; 26].

While the CQTE offers more information than the CATE and is more robust to outliers, it lacks some of the CATE's desirable estimation properties. Specifically, CQTE estimation involves estimating the quantile functions for the two marginal outcomes. This harms the estimation procedure in cases where estimation of marginal quantile functions is more challenging than estimation of the CQTE itself. An example of this is when the marginal quantile functions are less smooth as a function of the covariates than the CQTE. This aligns with a recurring idea within HTE estimation that the effect of a treatment may be simpler than the marginal outcomes. In contrast to the CQTE, there are many CATE estimators which aim to learn the CATE directly allowing them to exploit its relative simplicity. These include the X-learner [17], R-learner [23], and Doubly Robust (DR) learner [15]. Before estimating the CATE, these procedures require estimation of intermediary estimands (nuisance parameters) which condition on the covariates such as the average marginal outcomes and the propensity score (the probability of being assigned to treatment group). These nuisance parameters are then used to aid the estimation of the CATE. With the DR learner specifically, it has been shown that it can still achieve optimal convergence rates even when estimation of the nuisance parameters is worse than estimation of the CATE itself. This notion is referred to as **double robustness**, as our estimation is robust to sub-optimal estimation of both of the nuisance parameters.

Some attempts have been made to improve CQTE estimation [34; 33] with a key work being that of Kallus and Oprescu [14]. In this they provide an extension of the double robustness property to the CQTE, creating an estimation procedure that can achieve strong convergence even when nuisance parameters are more difficult to estimate. Unfortunately, one of the nuisance parameters which must be estimated is the reciprocal of the conditional densities over the response. These are highly difficult to estimate and risk the errors blowing up in low density regions which could potentially nullify the desirable estimation rates they achieve even with the dependence on the estimation accuracy of these nuisance parameters being less strong. Furthermore it is still unclear how one can interpret relative smoothness in the CQTE compared to the individual quantiles with their being relatively little discussion of this within the literature. In general there is a distinct lack of illustrative examples; which are present for the CATE. To our knowledge no other works specifically aim to tackle this double robustness phenomenon for the CQTE or other quantile based treatment effect estimands.

We introduce a novel estimand called the "conditional quantile comparator" (CQC). The CQCE gives the outcome for a treated individual in the same quantile as a given outcome for an untreated individual, conditional on covariates. This relates to the conditional Quantile-Quantile (QQ) plot for the treated and untreated outcomes as demonstrated in Figure 1.

Similarly to the CQTE, our new estimand, the CQCE, allows us to compare equivalent quantiles while working exclusively in the response landscape, making it a more interpretable tool. This allows us to construct canonical examples of the CQCE being smoother than various nuisance parameters, the CQTE, and the CATE; adding to this interpretability. In addition, using the pseudo-outcome framework presented in Kennedy [15], we can leverage CATE estimation procedures to estimate the CQC in a doubly robust way, as mentioned

Figure 1: The left plot gives the CQC surface which takes in covariates (\(x\)) and an untreated response (\(y\)) and returns the treated response of the equivalent quantile (\(g^{*}(y|x)\)). The right plot is a QQ-plot of the responses (\(Y\)) in the untreated (\(A=0\)) vs treated (\(A=1\)) population conditional on various covariates (\(X=0,0.5,1\)). These conditional QQ-plots correspond to “slices” of the CQC surface, as shown by the coloured lines in the left plot. The plot is best viewed in colour.

above. Crucially, the CQC can keep the valuable quantile-level information previously offered by the CQTE while building on much of the CATE literature to acquire its desirable robustness properties and interpretability. Our contributions are as follows:

* Introduce a new estimand for HTE analysis: the conditional quantile comparator (CQC).
* Propose an estimation procedure which we prove to be doubly robust.
* Demonstrate better estimation accuracy especially when the CQC is smooth but individual conditional cumulative distribution functions are not.
* Provide insights into real-world datasets on employment intervention and medical treatment.

## 2 Set-up

We now introduce the standard HTE set-up in our notation. Let \(Z\) denote the random triple \((Y,X,A)\) with \(Y\) a random variable (RV) on \(\mathcal{Y}\subseteq\mathbb{R}\), \(X\) a RV on \(\mathcal{X}\subseteq\mathbb{R}^{d}\), and \(A\) a RV on \(\{0,1\}\). We treat \(Z\) as representing an individual and interpret the components as

\[Y:\text{ Outcome/Response}\qquad\quad X:\text{ Observed covariates}\qquad\quad A: \text{ Treatment assignment}\]

**Remark 1**.: _We could view our setting as coming from a potential outcome framework [27]. Under this framework we assume there exists RVs \(Y_{0},Y_{1}\) on \(\mathcal{Y}\) representing the outcome with and without treatment and that \(Y\equiv Y_{A}\). \(Y_{1-A}\) would then be unobserved/unknown for each individual._

We define the _propensity score_\(\pi:\mathcal{X}\to(0,1)\) by

\[\pi(\bm{x})\coloneqq\mathbb{P}(A=1|X=\bm{x}),\]

in other words, \(\pi\) denotes the conditional probability of being assigned to treatment given the covariates. We shall assume that \(\pi\) is continuous and bounded away from \(0\) and \(1\). From a potential outcomes perspective, this means that each individual could potentially be assigned to either treatment. We also define

\[F_{a}(y|\bm{x}) \coloneqq\mathbb{P}(Y\leq y|X=\bm{x},A=a),\] (1) \[F_{a}^{-1}(\alpha|\bm{x}) \coloneqq\inf\{y\in\mathbb{R}|F_{a}(y|\bm{x})\geq\alpha\},\] (2)

and refer to them as the _conditional cumulative distribution function (CCDF)_ and the _quantile function_ respectively. We also refer to \(F_{a}^{-1}\) in (2) as the _generalised inverse_ of \(F_{a}\).

We can now define the CATE and CQTE to be given by \(\tau:\mathcal{X}\to\mathbb{R}\) and \(\tau_{q}:[0,1]\times\mathcal{X}\to\mathbb{R}\) with

\[\tau(\bm{x}) \coloneqq\mathbb{E}[Y|X=\bm{x},A=1]-\mathbb{E}[Y|X=\bm{x},A=0],\] \[\tau_{q}(\alpha|\bm{x}) \coloneqq F_{1}^{-1}(\alpha|\bm{x})-F_{0}^{-1}(\alpha|\bm{x}).\]

We let \(D\coloneqq\{Z_{i}\}_{i=1}^{2n}\equiv\{(Y_{i},X_{i},A_{i})\}_{i=1}^{2n}\) for \(n\in\mathbb{N}\) be IID copies of \(Z\) representing our data sample with \(i\) indexing the individual. We assume an even number of samples for notational convenience. For \(a\in\{0,1\}\), we take \(I_{a}\coloneqq\{i|A_{i}=1\}\), the indices of individuals on treatment \(a\). We can then define \(D_{a}\coloneqq\{Z_{i}\}_{\{i\in I_{a}\}}\) and \(n_{a}\coloneqq|I_{a}|\) as the dataset and sample size of those on treatment \(a\).

For \(n\in\mathbb{N}\), let \([n]\coloneqq\{1,\ldots,n\}\). For a vector \(\bm{w}\in\mathbb{R}^{p}\) let \(w_{j}\) to represent the \(j^{\text{th}}\) component of \(\bm{w}\) and let \(\|\bm{w}\|\) be the Euclidean norm unless otherwise specified. We also take \(\|\bm{w}\|_{1}\) as the \(1\)-norm and \(\|\bm{w}\|_{\infty}\coloneqq\max_{j\in[p]}|w_{j}|\). We keep a summary table of all notation used in Appendix A.1.

### Introducing the quantile comparator

Our aim is to find "equivalent quantiles" between the treated and non-treated distributions conditional on the covariates. Specifically, for each \(y_{0}\in\mathcal{Y}\), \(\bm{x}\in\mathcal{X}\) we aim to find \(y_{1}\) such that

\[F_{1}(y_{1}|\bm{x})=F_{0}(y_{0}|\bm{x}).\]

This now allows us to define our primary estimand of interest, the _conditional quantile comparator_.

**Definition 1** (Conditional quantile comparator (CQC)).: _For our triple \((Y,X,A)\), the conditional quantile comparator is the measurable function \(g^{*}:\mathcal{Y}\times\mathcal{X}\to\mathcal{Y}\) such that, for all \(y\in\mathcal{Y},\ \bm{x}\in\mathcal{X}\),_

\[F_{1}(g^{*}(y|\bm{x})|\bm{x})=F_{0}(y|\bm{x}).\]We then simply define \(y_{1}\) as \(g^{*}(y_{0}|\bm{x})\). The name conditional quantile comparator derives from the fact that it returns the value of \(y_{1}\) in the equivalent quantile of \(Y|X=\bm{x},A=1\) as the quantile of \(Y|X=\bm{x},A=0\) that \(y_{0}\) is in.

**Remark 2**.: _For simplicity and to ensure such a function is well defined, we will assume that \(Y|X=\bm{x},A=a\) is a continuous RV for any given \(\bm{x}\in\mathcal{X},\ a\in\{0,1\}\) with strictly positive density on its support. We will however allow the support of \(Y|X=x,A=a\) to vary in both \(\bm{x}\) and \(a\)._

We now introduce another estimand which will serve as a useful stepping stone in our estimation.

**Definition 2** (CCDF contrasting function).: _The CCDF contrasting function is defined to be \(h^{*}:\mathcal{Y}\times\mathcal{Y}\times\mathcal{X}\to[-1,1]\) given by_

\[h^{*}(y_{0},y_{1}|\bm{x})\coloneqq F_{1}(y_{1}|\bm{x})-F_{0}(y_{0}|\bm{x}).\] (3)

This estimand allows following alternative definitions for \(g^{*}\) which help its interpretation and estimation (detailed later). We take \(h^{*-1}\) representing the inverse of \(h^{*}\) with respect to the \(2^{\text{nd}}\) argument.

\[g^{*}(y_{0}|\bm{x})=h^{*-1}(y_{0},0|\bm{x})=F_{1}^{-1}(F_{0}(y_{0}|\bm{x})| \bm{x}).\] (4)

The second equality still holds if we replace the inverses in the above with generalised inverses. The equality (4) falls straight from the definition of each object and shows how we can use \(h^{*}\) to estimate \(g^{*}\). Moreover, the equality (4) allows us to generalise \(g^{*}\) to discontinuous \(Y\) (or pdfs with non-trivial \(0\) density regions inside the support).

### Exploring the CQC

We specifically focus on the CQC, \(g^{*}\), as we feel it gives insightful information allowing comparison of the two distributions (\(Y|X,A=1\)) and (\(Y|X,A=0\)).

The CQC allows us to compare the two distributions beyond simply a single point estimate such as that given by the CATE. This is especially valuable in cases where the two distributions differ beyond just a shift. For example, the effect of some treatments varies greatly between individuals with the same or similar covariates. An example of this is antidepressants, where some patients respond positively while others may have adverse reactions leading to a worse outcome than no treatment whatsoever. Another example is the use of opioids as painkillers where some patients have an increased tolerance making them less effective [12; 22].

As well as being of interest on its own, the quantile comparator relates closely to other estimands of interest. For example, if we take \(\Delta^{*}(y_{0}|\bm{x})\coloneqq g^{*}(y_{0}|\bm{x})-y_{0}\) then \(\Delta^{*}\) tells us whether the equivalent quantile in the treated distribution is higher or lower. This estimand then serves as a heuristic for whether the treatment is beneficial at that untreated response value.

Furthermore, CQTE can be written as

\[\tau_{q}(\alpha|\bm{x})=\Delta^{*}\left(F_{Y|X,A=0}^{-1}(\alpha|\bm{x}) \Big{|}\bm{x}\right)=g^{*}\left(F_{Y|X,A=0}^{-1}(\alpha|\bm{x})\Big{|}\bm{x} \right)-F_{Y|X,A=0}^{-1}(\alpha|\bm{x}),\] (5)

linking the quantile comparator back to the CQTE. This equivalence highlights the perspective that the CQC can be seen as rephrasing the input of the CQTE in terms of the outcome space.

A key idea within CATE literature is the notion that the CATE itself may be a simpler estimand to study than the marginal treatment outcomes (\(\mathbb{E}[Y|X=\bm{x},A=a]\)) may be individually. One can exploit this feature to improve the CATE's estimation. A similar concept exists with the CQC as we will see in the example below.

**Example 1** (Illustrative Example).: _Suppose that_

\[Y|X=x,A=0\sim\mathrm{N}(\sin(10x),\ 1^{2}),\ \ \ \ \ \ \ \ Y|X=x,A=1\sim\mathrm{N}(2\sin(10x),\ 2^{2}).\]

_Then we have \(g^{*}(y|x)=2y\) which does not depend on \(x\) and does not include the sine term present in the individual CDFs. Interestingly, \(\mathbb{E}[Y|X,A=1]-\mathbb{E}[Y|X,A=0]=\sin(x)\) hence the CATE is still non-constant in this case (the same also holds for the CQTE). Additionally, we have \(\Delta^{*}(y|x)=g^{*}(y|x)-y=y\) suggesting the intervention is beneficial for positive \(y\) and detrimental for negative \(y\). We now show 3D plots of a CCDF, the CQC, and the CQTE in Figure 2._

**Example 2** (General Smoothness Case).: _Suppose we are in the potential outcomes framework so that \(Y_{0},Y_{1}\) exist with \(Y\equiv Y_{A}\). Now also suppose that \(Y_{1}=\phi(Y_{0},X)\) for some transformation \(\phi\) increasing in \(Y_{0}\) for each \(X\). Then \(\phi\) gives the CQC (i.e. \(\phi=g^{*}\)) meaning that smoothness of the CQC can be seen as smoothness of \(\phi\). This gives a generalisation of the CATE case where smoothness is present when \(Y_{1}=Y_{0}+\psi(X)\) with \(\psi\) smooth_

_A specific example could be a treatment which halves all individuals blood pressure. In this case \(\phi^{*}(y,\bm{x})=g^{*}(y|\bm{x})=\frac{1}{2}y\) and so the CQC is smooth but the CQTE and CATE would not be if the individual responses are non-smooth._

## 3 Estimation procedure

We now describe our estimation procedure for the CQC which is motivated by equation (4). At a high level our approach for estimating \(g^{*}(y_{0}|\bm{x})\) will be the following:

* a specified proxy response computed from \((Y,X,A)\) which we will regress against.
* Find the value \(\hat{y}_{1}\) which makes our estimate of \(h^{*}(y_{0},\cdot|\bm{x})\) closest to \(0\).

Section 3.1 and Algorithm 1 give our \(h^{*}\) estimation procedure while Section 3.3 and Algorithm 2 give our \(g^{*}\) estimation procedure.

### Estimating the CCDF contrasting function \(h^{*}\)

We focus on estimating \(h^{*}\) primarily for its two nice properties:

1. Similar to \(g^{*}\), \(h^{*}\) can exhibit smoothness even when the individual CCDFs are not smooth.
2. The estimation of \(h^{*}\) can be re-framed as a CATE problem.

The first property is important as the smoothness of \(h^{*}\) determines the best estimation rate that can be achieved when using non-parametric regression, setting a target for our approach. In particular, smoother functions have better estimation rates. The second property is important as it gives us a method for attaining this target rate. By re-framing the estimation as a CATE problem, we can leverage existing results to build a robust estimator which achieves the target estimation accuracy rate even when the rate of estimating nuisance parameters is sub-optimal. We demonstrate this robustness later using finite sample bounds on the estimation accuracy (Proposition 1 & Theorem 2).

First, we show how the estimation of \(h^{*}\) can be solved using a CATE estimator. Note that

\[h^{*}(y_{0},y_{1}|\bm{x})=\mathbb{E}[\mathds{1}\{Y\leq y_{1}\}|X=\bm{x},A=1]- \mathbb{E}[\mathds{1}\{Y\leq y_{0}\}|X=\bm{x},A=0].\]

Hence, for a given \(y_{0},y_{1}\), if we define the RV \(W_{y_{0},y_{1}}:=\mathds{1}\{Y\leq y_{A}\}\), then estimating \(h^{*}(y_{0},y_{1}|.)\) is equivalent to estimating the CATE with \(W_{y_{0},y_{1}}\) replacing \(Y\) as the response. To perform this estimation, we turn to a recent method developed by Kennedy [15]. They propose to write the CATE as a conditional expectation of a function of \(Z\) called a pseudo-outcome. A robust estimator

Figure 2: Surface plots for CCDF (panel (a)), CQC (panel (b)) and CQTE (panel (c)). We can see that CCDF, and CQTE have high-frequency change in \(x\) while the CQC does not depend on \(x\).

is then obtained by regressing this pseudo-outcome against \(X\). In our setting, the pseudo-outcome with the new response \(W_{y_{0},y_{1}}\), for a sample \((y,\bm{x},a)\) is given by

\[\varphi_{y_{0},y_{1}}(y,\bm{x},a): =\frac{a-\pi(\bm{x})}{\pi(\bm{x})(1-\pi(\bm{x}))}\left\{\mathds{1 }\{y\leq y_{a}\}-F_{a}(y_{a}|\bm{x})\right\}+F_{1}(y_{1}|\bm{x})-F_{0}(y_{0}| \bm{x})\] (6) \[=\frac{a-\pi(\bm{x})}{\pi(\bm{x})(1-\pi(\bm{x}))}\left\{\mathds{1 }\{y\leq y_{a}\}-F_{a}(y_{a}|\bm{x})\right\}+h^{*}(y_{0},y_{1}|\bm{x}).\]

Since \(h^{*}(y_{0},y_{1}|\bm{x})=\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x}]\) (Proposition 5, Appendix C.1), regressing \(\varphi_{y_{0},y_{1}}(Z)\) on \(X\) provides an estimate for \(h^{*}\).

As we do not know the CDFs nor the propensity score, we need to replace them in (6) with estimates. We define \(\hat{\pi},\hat{F}_{0},\hat{F}_{1}\) to be estimates of \(\pi,F_{0},F_{1}\) respectively. We then construct \(\hat{\varphi}_{y_{0},y_{1}}\) in the same way as \(\varphi_{y_{0},y_{1}}\), but using estimated quantities \(\hat{\pi},\ \hat{F}_{a}\) instead. \(\hat{\varphi}_{y_{0},y_{1}}\) can now serve as the pseudo-outcome in our regression. We also use sample splitting to de-correlate the propensity score and CDF estimates from the \(h^{*}\) estimate. This helps make our estimator doubly robust, as we will see in the following theory.

We are now ready to define our Doubly Robust (DR)-learner to estimate \(h^{*}\) in Algorithm 1.

```
0:\(y_{0},y_{1}\in\mathcal{Y}\), Data \(D\), a regressor (e.g. linear smoother)
1: Define \(\mathcal{I}\coloneqq[n]\), \(\mathcal{J}\coloneqq\{n+1,\ldots,2n\}\) and split \(D\) into \(D_{\mathcal{I}}\coloneqq\{Z_{i}\}_{i\in\mathcal{I}},D_{\mathcal{J}}\coloneqq\{Z _{j}\}_{j\in\mathcal{J}}\).
2: Using \(D_{\mathcal{I}}\) to estimate \(\hat{\pi},\hat{F}_{0}(y_{0}|.),\hat{F}_{1}(y_{1}|.)\) by regressing \(\mathds{1}\{A=1\},\mathds{1}\{Y\leq y_{0}\},\mathds{1}\{Y\leq y_{1}\}\) respectively against \(X\).
3: Use these estimates to obtain \(\hat{\varphi}(Z_{j})\) for \(j\in D_{\mathcal{J}}\) using equation (6)
4: Using \(D_{\mathcal{J}}\) to regress \(\hat{\varphi}\) against \(X\) to obtain estimate \(\hat{h}(y_{0},y_{1}|.)\) of \(h^{*}(y_{0},y_{1}|.)\). ```

**Algorithm 1** DR estimation procedure for the CCDF contrasting function \(h^{*}\)

**Remark 3**.: _Cross-fitting can be implemented by repeating the procedure with the roles of \(\mathcal{I}\) and \(\mathcal{J}\) switched and then averaging the two estimates of \(h^{*}\). We could also perform this procedure multiple times with different random splits of the data to improve our estimator's potential stability._

Note that our algorithm is not specific on which form of regression to use allowing for any parametric or non-parametric procedure. Further to this, it can also be easily adapted to use other pseudo-outcome procedures such as the R-learner of Nie and Wager [23] or a standard inverse propensity weighting approach which we describe in Appendix A.3.

### Finite sample bound of \(h^{*}\) estimator

In this section, we prove the estimation accuracy of \(\hat{h}\), which will play important roles in the following notation and assumptions we need for estimating \(g^{*}\). These accuracy statements will be made for an arbitrarily fixed \(\bm{x}\in\mathcal{X}\). For our theoretical and experimental results we use linear smoothers for the final regression. This means our estimate \(\hat{h}\) and oracle estimate \(h^{\prime}\) are of the form

\[\hat{h}(y_{0},y_{1}|\bm{x}) =\sum_{j\in\mathcal{J}}w_{j}\hat{\varphi}_{y_{0},y_{1}}(Z_{j}) h^{\prime}(y_{0},y_{1}|\bm{x}) =\sum_{j\in\mathcal{J}}w_{j}\varphi_{y_{0},y_{1}}(Z_{j})\]

where the weights \(w_{j}\equiv w_{j}(\bm{x},X_{\mathcal{J}})\) are constructed using \(X_{\mathcal{J}}\coloneqq\{X_{j}\}_{j\in\mathcal{J}}\) with \(w_{j}\geq 0\) and \(\|\bm{w}\|_{1}\). Linear smoothers encompass a broad class of estimation techniques used in both low and high-dimensional settings. Examples include k-NN regression [8; 9], kernel ridge regression [29], generalised forests [3], and Mondrian forests [18]. Additionally linear smoothers have been shown to adapt to intrinsic low dimensionality in regression problems in higher dimensions [16], making them an apt estimator for our purposes.

For \(\phi:\mathcal{Z}\to\mathbb{R}\) treated as deterministic and \(p>1\), we also define the norms

\[\|\phi\|_{\bm{w}^{*}}\!\coloneqq\sqrt{\frac{1}{\|\bm{w}^{s}\|_{1}}\sum_{i\in \mathcal{J}}w_{j}^{s}\,\mathbb{E}\left[\phi(Z)^{2}|X=X_{i}\right]}\]

and take \(\|\phi\|_{\bm{w}}\!\coloneqq\|\phi\|_{\bm{w}^{1}}\). Note that this norm is random as the weights depend upon \(X_{\mathcal{J}}\).

We now aim to show that we are able to exploit smoothness in \(h^{*}\) even when the CCDFs and propensity score are less smooth. We introduce the notion of smoothness through Holder functions.

**Definition 3** (Holder functions).: _We say that a function \(f:\mathcal{X}\to\mathbb{R}\) is \((\gamma,C)\)-Holder for \(\gamma\in(0,1],C\geq 1\) if for any \(\bm{x}^{\prime},\bm{x}^{\prime\prime}\in\mathcal{X}\),_

\[|f(\bm{x}^{\prime})-f(\bm{x}^{\prime\prime})|\leq C\|\bm{x}^{\prime}-\bm{x}^{ \prime\prime}\|^{\gamma}.\]

Here, larger \(\gamma\) represents a smoother function which can be estimated at faster rates.

**Assumption 1**.: _For any \(y_{0},y_{1}\in\mathcal{Y},\;\delta>0,a\in\{0,1\}\):_

1. _[label=()]_
2. _There exists_ \(\xi\in(0,1/2]\) _such that_ \(\pi(\bm{x}),\hat{\pi}(\bm{x}^{\prime})\in[\xi,1-\xi]\) _for all_ \(\bm{x}^{\prime}\in\mathcal{X}\)_._
3. _With probability at least_ \(1-\delta\)_,_ \(\|\hat{\varphi}_{y_{0},y_{1}}-\varphi_{y_{0},y_{1}}\|_{\bm{w}^{2}}\!\leq \varepsilon_{\hat{\varphi}}(n,\delta)\)_._
4. _With probability at least_ \(1-\delta\)_,_ \(\|\hat{\pi}-\pi\|_{\bm{w}}\!\leq\!\varepsilon_{\alpha}(n,\delta)\) _and_ \(\|F_{a}(y_{a}|.)-\hat{F}_{a}(y_{a}|.)\|_{\bm{w}}\!\leq\varepsilon_{\beta}(n, \delta)\)_._
5. _For_ \(\gamma\in(0,1],\;C\geq 1\)_,_ \(h^{*}(y_{0},y_{1}|.)\) _is_ \((\gamma,C)\)_-Holder._

Assumption 1(a) exists to ensure for any covariate value, neither treatment assignment has too low a probability. Assumption 1(b) controls the convergence of the estimated pseudo-outcome to the true pseudo-outcome. Assumption 1(c) sets up the smoothness of the propensity score and CCDFs alongside the convergence rates of their estimators as \(\varepsilon_{\alpha},\varepsilon_{\beta}\) respectively. Assumption 1(d) sets up the smoothness of \(h^{*}\) which will control the convergence rate of the oracle estimation procedure. Assumptions 1 (c) and (d) control the accuracy of our estimator in the following result.

**Proposition 1**.: _Suppose that Assumption 1 holds and let \(\hat{h}\) be a linear smoother estimated as in Algorithm 1. Then for any \(y_{1},y_{0}\in\mathcal{Y},\;\delta\in(0,2/e]\) and our \(\bm{x}\in\mathcal{X}\), with probability at least \(1-\delta\),_

\[\left|\hat{h}(y_{0},y_{1}|\bm{x})-h^{*}(y_{0},y_{1}|\bm{x})\right|\leq \varepsilon_{h}(n,\delta).\]

_Here, for each \(\delta\in(0,2/e]\) we have_

\[\varepsilon_{h}(n,\delta) \coloneqq\sqrt{2\log(8/\delta)/n}\,\varepsilon_{\hat{\varphi}}(n, \delta/4)+\varepsilon_{\alpha}(n,\delta/4)\varepsilon_{\beta}(n,\delta/4)+ \varepsilon_{\gamma}(n,\delta/4),\] \[\varepsilon_{\gamma}(n,\delta) \coloneqq|\mathbb{E}[h^{\prime}(y_{0},y_{1}|\bm{x})-h^{*}(y_{0},y _{1}|\bm{x})|X_{\mathcal{J}},D_{I}]\] \[\quad+\sqrt{2\log(2/\delta)/n}\,\|\bm{w}\|\|\varphi-h(y_{0},y_{1} |.)\|_{\bm{w}^{2}}\!+\!2\|\bm{w}\|_{\infty}\!\log(2/\delta)/(3\xi).\]

We have that \(\varepsilon_{\gamma}\) gives an upper bound on the accuracy of the oracle estimation and so acts as a target for our estimation procedure. If \(\varepsilon_{\hat{\varphi}}(n,\delta)\to 0\) then the first term in \(\varepsilon_{h}\) is guaranteed to be \(o(\varepsilon_{\gamma}(n,\delta))\) for fixed \(\delta\). Hence the first and last terms converge at oracle rates with respect to \(n\). As the \(\varepsilon_{\alpha}\) and \(\varepsilon_{\beta}\) terms are multiplied together we can obtain better rates than either of them individually have. This is because both \(\varepsilon_{\alpha}\) and \(\varepsilon_{\beta}\) can converge to \(0\) slower than \(\varepsilon_{\gamma}\) while their product \(\varepsilon_{\alpha}\cdot\varepsilon_{\beta}\) converges quicker. This provides the desired double robustness as our estimation can converge at oracle rates even when convergence for the nuisance parameters is slower. Now that we have this we can convert our estimate of \(h^{*}\), into an estimate of \(g^{*}\).

### Estimating the conditional quantile comparator \(g^{*}\)

In order to obtain an estimate of \(g^{*}(y_{0}|\bm{x})\) at a fixed \(y_{0},\bm{x}\), we need to obtain estimates of \(h^{*}(y_{0},y_{1}|\bm{x})\) at various values of \(y_{1}\). As \(h^{*}\) is monotonic, we would then like to search for a monotonic function which aligns with these estimates. This monotonicity is especially important because it allows us to bound the estimation accuracy of \(\hat{h}\) uniformly over all \(y_{1}\) at a similar rate to our pointwise accuracy. With this, we can easily translate the estimation accuracy in \(\hat{h}\) (obtained in proposition 1) into the accuracy in \(\hat{g}\). Additionally, it simplifies the process of inverting \(\hat{h}\). In general our method in Algorithm 1 will not produce monotonic \(\hat{h}\) (this is in contrast to some other approaches such as an IPW pseudo-outcome, see Appendix A.3), or separately estimating the CCDFs). We can however obtain a monotonic estimate of \(h^{*}\) using isotonic projection.

**Definition 4** (Isotonic Projection).: _We define the isotonic projection of \(\bm{\alpha}^{\prime}\in\mathbb{R}^{p}\) as follows:_

\[P(\bm{\alpha}^{\prime})\coloneqq\operatorname*{argmin}_{\bm{\alpha}\in\mathrm{ Iso}(p)}\|\bm{\alpha}-\bm{\alpha}^{\prime}\|\]

_where \(\mathrm{Iso}(p)\coloneqq\{\bm{\alpha}\in\mathbb{R}^{p}|\alpha_{j}\leq\alpha_{l+ 1}\;\forall l\in[p-1]\}\), the set of all isotonic vectors in \(\mathbb{R}^{p}\)._

**Remark 4**.: _We can use the Pool Adjacent Violators Algorithm (PAVA) [6] which performs isotonic projection and is implemented in the IsotonicRegression class of sci-kit learn in Python [25]._

Hence, for a fixed value of \((y_{0},\bm{x})\) and a set of predictions \(\hat{\alpha}_{l}=\hat{h}(y_{0},y_{1}^{(l)}|\bm{x})\) with \(y^{(l)}\leq y^{(l+1)}\), we can take \(\tilde{\bm{\alpha}}:=P_{\mathrm{Iso}(p)}(\hat{\bm{\alpha}})\) and use these to obtain a new monotonic estimate of \(h^{*}\). Furthermore, by a result in Yang and Barber [32], \(\tilde{\bm{\alpha}}\) will be at least as accurate as \(\hat{\bm{\alpha}}\) in the worst case. We now describe our approach for estimating the CQC using this projection approach in Algorithm 2.

```
0: Data \(D\); test point \((y_{0},\bm{x})\); sorted evaluation points \(\{y^{(l)}\}_{l=1}^{p}\)
1: Apply Algorithm 1 to obtain estimate of \(h^{*}\) given by \(\hat{h}\).
2: Define \(\hat{\alpha}_{l}\coloneqq\hat{h}(y_{0},y^{(l)}|\bm{x})\) for \(l\in[p]\).
3: Isotonically project \(\tilde{\bm{\alpha}}\) using PAVA to obtain \(\tilde{\bm{\alpha}}\) with \(\tilde{\alpha}_{i}\leq\tilde{\alpha}_{i+1}\).
4: Take \(\hat{g}(y_{0}|\bm{x})\coloneqq y^{(l^{*})}\) with \(l^{*}\coloneqq\operatorname*{argmin}_{l\in[p]}|\tilde{\alpha}_{l}|\). ```

**Algorithm 2** DR estimation procedure for the CQC \(g^{*}\)

**Remark 5**.: _For the case where \(\hat{h}\) is a step function, these steps can serve as our evaluation points while for continuous \(\hat{h}\) one could take these candidate \(y_{1}\) points at small evenly spaced intervals. Empirically we also find that \(\hat{h}\) is already close to isotonic and so step 3. of the algorithm is mostly for the theoretical justification of our approach._

While it may seem inefficient to be estimating the CQC via the CCDF contrasting function and then inverting, due to the monotonicity of \(h^{*}\), we actually pay a very small cost in estimation accuracy for having to estimate the CCDF contrasting function over all \(y_{1}\). We make this notion more explicit in the following section.

### Finite sample bound of the CQC estimator

We now provide the accuracy of our estimate \(\hat{g}\) obtained by Algorithm 2 when used in conjunction with linear smoothers. We assume that \(\hat{F}_{a}\) are also fit using linear smoothers of the form

\[\hat{F}_{a}(y|\bm{x}^{\prime},a)=\sum_{i\in\mathcal{I}}w_{F_{a};i}(\bm{x}^{ \prime};X_{\mathcal{I}},A_{\mathcal{I}})\mathds{1}\{Y_{i}\leq y\}\]

with \(w_{j}(\bm{x}^{\prime})\equiv w_{j}(\bm{x}^{\prime},X_{\mathcal{I}},A_{\mathcal{ I}})>0,\|\bm{w}(\bm{x}^{\prime})\|_{1}=1\). We will also require the following assumptions.

**Assumption 2**.: _For our RV \(X\), any \(y\in\mathcal{Y},\ \bm{x}^{\prime}\in\mathcal{X}\), \(\delta<e^{-1}\):_

1. _There exists some_ \(s,\eta>0\) _such that_ \(F_{1}(y^{\prime}|\bm{x})\geq\eta\) _for all_ \(y^{\prime}\in B_{s}(g^{*}(y|\bm{x}))\)_._
2. _W.p. at least_ \(1-\delta\)_,_ \(\max_{j\in\mathcal{J}}w_{j}\leq\varepsilon_{\bm{w}}(n,\delta)\) _and_ \(\max_{i\in\mathcal{I}}w_{F_{a};i}(X)\leq\varepsilon_{\bm{w}}(n,\delta)\)_._

Assumption 2(a) is a mild assumption which allows us to convert the \(\hat{h}\) accuracy into \(\hat{g}\) accuracy while Assumption 2(b) bounds the rates of decay of the weights in our linear smoothers.

**Theorem 2**.: _Let \(\hat{g}\) be estimated as using Algorithm 2 with \(\{Y_{i}\}_{i\in I_{1}}\), sorted and then used as our evaluation points and linear smoothers used for regressions in Algorithm 1. Then provided Assumptions 1 & 2 hold we have that for \(\delta\in(0,e^{-1})\) and sufficiently large \(n\), w.p. at least \(1-\delta\),_

\[|\hat{g}(y|\bm{x})-g^{*}(y|\bm{x})|\leq 2\left(\eta^{-1}\varepsilon_{h}(n, \delta/(2n))+\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/(2n))\right).\]

From this result we see that if our weights decay at rate faster than \(\varepsilon_{h}(n,\delta)\) then this error will be dominated by the \(\varepsilon_{h}\) term. We believe this to hold in most cases and show that it does comfortably when using Nadaraya-Watson (NW) estimation [21, 31] with a box kernel in Appendix C.4. Furthermore, if the dependence on \(\delta\) in both terms is of the form \(\log^{c}(1/\delta)\) for some \(c>0\) then we obtain the same rate of estimation as for \(h^{*}\) up to polylog factors. This means we translate our desirable double robustness \(\hat{h}\) over to \(\hat{g}\). We also obtain finite sample bounds on \(\mathbb{E}[|\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x})|\ \mid A=0,\hat{g}]\) with high probability and present this in Appendix C.3.

## 4 Numerical experiments

We now apply our approach to a series of simulated and real data scenarios in order to demonstrate the utility of our estimand and the effectiveness of our estimation procedure. For these, we use NW estimation as our regression procedure throughout. See appendix A.2 for details on NW estimation.

### Simulated experiment

In this section, we test our method's performance in terms of our estimator's mean absolute error under simulated scenarios.1 In each scenario we test against a separate estimator which estimates the two CCDFs separately and simply takes their difference, an IPW pseudo-outcome estimator detailed in Appendix A.3, the CQTE estimator of Kallus and Oprescu [14], and the oracle DR estimator where \(\hat{\varphi}\) is replaced with \(\varphi\) (i.e. exact \(\pi,F_{a}\) are used). In this experiment, we return back to the set-up of example 1. We now change the frequency of the sine term by taking

Footnote 1: Code implementation can be found at: github.com/joshgivens/ConditionalOutcomeEquivalence

\[Y|X=x,A=0 \sim\mathrm{N}(\sin(\gamma\pi x),\;1^{2}),\hskip 28.452756ptY|X=x,A=1 \sim\mathrm{N}(2\sin(\gamma\pi x),\;2^{2}),\] \[\pi(x) =0.4\sin(\gamma\pi x)+0.5.\]

for \(\gamma\in[0,10]\) so that increasing \(\gamma\) imitates decreasing smoothness of our nuisance parameters. In our experiments half the samples are used to estimate the propensity score and CCDFs and the other half are used to regress against the pseudo-outcome. Our estimate \(\hat{g}\) is then compared against \(g^{*}\) using a hold-out testing set. This process is repeated 500 times with new training data on each run. From this, a Monte-Carlo estimate of \(\mathbb{E}_{\hat{g}}[\mathbb{E}_{X}|\mathbb{E}_{Y|X,A=0}[|\hat{g}(Y|X)-g^{*}( Y|X)|]]\) is produced alongside 95% confidence intervals (CIs). In our first experiment, we let \(2n=1000\) and vary \(\gamma\) in \([0,10]\). In our second experiment, we let \(\gamma=6\) and \(2n\) vary in \([200,5000]\). The results of this are shown in Figure 3.

We can see that the average error decreases as the sample size \(2n\) grows and mildly increases as \(\gamma\) grows. This is expected as increasing \(\gamma\) makes estimating the nuisance parameters more challenging. The result shows that the proposed DR method achieves the best performance compared with the Separate and IPW estimators and is only marginally outperformed by the oracle estimator. Additionally we see that the method of Kallus and Oprescu [14] is much more affected by the increase in \(\gamma\). This is because unlike the CQC, the CQTE has a complexity that depends on the frequency term (\(\gamma\)). We also observe much better performance as the sample size increases. We hypothesise that the plateau in the CQTE approach is due to the difficulty of estimating the reciprocal of the PDF, which causes the estimation to be unstable irrespective of sample size. Further simulated experiments including 10-dimensional \(\bm{x}\) and linear CQC are given in Appendix B.1.

### Real world employment example

To show the performance in the real-world scenarios, we use a dataset on an employment programme which has been studied in various prior works [4; 5; 26]. Within the programme, some participants were given job placements or temporary help jobs while others received no intervention. Participants' earnings were then monitored over the next 8 quarters following their enrollents. We take their net earnings as our response (\(Y\)) and the employment intervention as the treatment (\(A=1\)). We use each participant's age at their entry to the study as our covariate (\(X\)). We fit our quantile comparator function on 2,000 participants. Figure 4 shows our estimate of \(\Delta^{*}(y|\bm{x})=g^{*}(y|\bm{x})-y\) for various values of \((y,\bm{x})\).

We see that participants around age 23 and between ages 32-37 benefit most from this scheme, as indicated by the darker colour on the heat map. Additionally, the lower quantile of the income distribution (wage \(\leq\) $7500) shows the least change, indicating that wage improvements primarily occur

Figure 3: Mean absolute error with 95% CIs for various estimators. The left plot has fixed sample size (\(2n=1000\)) and increasing \(\gamma\). The right plot has \(\gamma=6\) and increasing sample size.

for the higher income group. For participants aged 40, there appears to be little change in outcomes overall. To demonstrate our approach in a medical setting, we apply our method to evaluate the effectiveness of a colon cancer treatment, as detailed in Appendix B.3. For comparative purposes, we also provide an estimate of the CQTE for this data in Appendix B.2.

## 5 Limitations

The theory provided here gives a strong foundation and motivation for framing our problem in this particular manner. There is however a great deal more to be explored in this area from a theoretical perspective. For example, one immediate improvement would be to give a more general case where our weight decay (in Assumption 2 (b) for Theorem 2) is sufficiently fast. In addition more work needs to be done exploring the relationship between the smoothness of \(g^{*}\) and the smoothness \(h^{*}\). Smoothness in \(h^{*}\) appears to be a stronger condition so ideally we would like to make theoretical statements directly on the smoothness of \(g^{*}\). Interestingly our experiments on synthetic data do seem to suggest that it is the complexity of \(g^{*}\) which drives the estimation rate as in these experiments \(h^{*}\) increases in complexity while \(g^{*}\) remains constant. Additionally, changing our experiment in Section 4.1 to have uniform response so that both \(h^{*},g^{*}\) are constant rather than just \(g^{*}\), seems to give no material improvement to the performance of our estimator (see Appendix B.1.3).

Another limitation of our current estimation procedure is that it requires learning an estimand and then inverting it to obtain our final estimator. While this process is relatively simple, it could be streamlined and made more computationally efficient if we could produce a more direct estimator similar to the DR-Learner for CATE [15] or the CQTE estimator in Kallus and Oprescu [14].

Finally, while we have been able to provide more concrete examples of smoothness for the CQC these are still limited to the case of a deterministic treatment effect which we would like to expand upon. This is closely related to a more general limitation with quantile-based estimands, the CQC included, in that they lack meaningful interpretability for the individual. While the CATE can be viewed as the expected difference in an individual's outcome on and off the treatment, no such individual-level interpretation exists for the CQC or indeed any other estimand trying to learn higher level distributional information than the mean. As a result the CATE is still a more naturally interpretable estimand. To facilitate this interpretation however, one still needs to make assumptions about a lack of confounding between the treatment assignment and the potential outcomes, which are only verifiable in certain restrictive scenarios [30].

## 6 Conclusion

In this paper we have introduced a new treatment effect estimand, the conditional quantile comparator and demonstrated its efficacy both in terms of its doubly robust estimation, and its ability to provide valuable data insights. This is a promising direction as it allows quantile-based treatment effect exploration to "keep up" with the CATE in terms of estimation quality offering more flexibility as to which estimand can be used to best describe the data. For these reasons, we see the CQC as an exciting and worthwhile new direction within the HTE framework.

Figure 4: Surface and heat plot of \(\Delta^{*}(y|\bm{x})\) for our employment data with \(X=\)Age, \(Y\)=Income.

### Acknowledgments and Disclosure of Funding

Josh Givens was supported by a PhD studentship from the EPSRC Centre for Doctoral Training in Computational Statistics and Data Science (COMPASS).

## References

* Abadie et al. [2002] Abadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of subsidized training on the quantiles of trainee earnings. _Econometrica_, 70(1):91-117.
* Abadie and Imbens [2002] Abadie, A. and Imbens, G. W. (2002). Simple and bias-corrected matching estimators for average treatment effects. Working Paper 283, National Bureau of Economic Research. Series: Technical working paper series.
* 1178. Publisher: Institute of Mathematical Statistics.
* Autor and Houseman [2010] Autor, D. H. and Houseman, S. N. (2010). Do temporary-help jobs improve labor market outcomes for low-skilled workers? Evidence from "Work First". _American Economic Journal: Applied Economics_, 2(3):96-128.
* Autor et al. [2017] Autor, D. H., Houseman, S. N., and Kerr, S. P. (2017). The effect of work first job placements on the distribution of earnings: An instrumental variable quantile regression approach. _Journal of Labor Economics_, 35(1):149-190.
* Barlow et al. [1972] Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972). _Statistical Inference under Order Restrictions (The Theory and Application of Isotonic Regression)_. Wiley Series in Probability and Statistics. John Wiley & Sons Ltd.
* Boucheron et al. [2013] Boucheron, S., Lugosi, G., and Massart, P. (2013). _Concentration inequalities: a nonasymptotic theory of independence_. Oxford University Press.
* Chen [2019] Chen, G. (2019). Nearest neighbor and kernel survival analysis: Nonasymptotic error bounds and strong consistency rates. In Chaudhuri, K. and Salakhutdinov, R., editors, _Proceedings of the 36th international conference on machine learning_, volume 97 of _Proceedings of machine learning research_, pages 1001-1010. PMLR.
* Chen et al. [2019] Chen, P., Dong, W., Lu, X., Kaymak, U., He, K., and Huang, Z. (2019). Deep representation learning for individualized treatment effect estimation using electronic health records. _Journal of Biomedical Informatics_, 100:103303.
* Collins and Varmus [2015] Collins, F. S. and Varmus, H. (2015). A new initiative on precision medicine. _The New England journal of medicine_, 372(9):793-795. Place: United States.
* Hirano and Porter [2009] Hirano, K. and Porter, J. R. (2009). Asymptotics for statistical treatment rules. _Econometrica_, 77(5):1683-1701. Publisher: [Wiley, The Econometric Society].
* Huynh et al. [2021] Huynh, P., Villaluz, J., Bhandal, H., Alem, N., and Dayal, R. (2021). Long-Term Opioid Therapy: The Burden of Adverse Effects. _Pain Medicine_, 22(9):2128-2130.
* Imbens [2004] Imbens, G. W. (2004). Nonparametric estimation of average treatment effects under exogeneity: a review. _The Review of Economics and Statistics_, 86(1):4-29.
* Kallus and Oprescu [2023] Kallus, N. and Oprescu, M. (2023). Robust and agnostic learning of conditional distributional treatment effects. In Ruiz, F., Dy, J., and van de Meent, J.-W., editors, _Proceedings of the 26th international conference on artificial intelligence and statistics_, volume 206 of _Proceedings of machine learning research_, pages 6037-6060. PMLR.
* 3049. Institute of Mathematical Statistics and Bernoulli Society.
* Kpotufe [2011] Kpotufe, S. (2011). k-NN regression adapts to local intrinsic dimension. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, _Advances in neural information processing systems_, volume 24. Curran Associates, Inc.

* Kunzel et al. [2019] Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Bin Yu (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. _Proceedings of the National Academy of Sciences_, 116(10):4156-4165.
* Lakshminarayanan et al. [2014] Lakshminarayanan, B., Roy, D. M., and Teh, Y. W. (2014). Mondrian forests: Efficient online random forests. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K., editors, _Advances in neural information processing systems_, volume 27. Curran Associates, Inc.
* Laurie et al. [1989] Laurie, J. A., Moertel, C. G., Fleming, T. R., Wieand, H. S., Leigh, J. E., Rubin, J., McCormack, G. W., Gerstner, J. B., Krook, J. E., and Malliard, J. (1989). Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. The North Central Cancer Treatment Group and the Mayo Clinic. _Journal of clinical oncology_, 7(10):1447-1456. Place: United States.
* Lei and Candes [2021] Lei, L. and Candes, E. J. (2021). Conformal inference of counterfactuals and individual treatment effects. _Journal of the Royal Statistical Society, Series B_, 83(5):911-938.
* Nadaraya [1965] Nadaraya, E. (1965). On non-parametric estimates of density functions and regression curves. _Theory of Probability & Its Applications_, 10(1):186-190.
* Nadeau et al. [2021] Nadeau, S. E., Wu, J. K., and Lawhern, R. A. (2021). Opioids and chronic pain: An analytic review of the clinical evidence. _Frontiers in Pain Research_, 2.
* Nie and Wager [2020] Nie, X. and Wager, S. (2020). Quasi-oracle estimation of heterogeneous treatment effects. _Biometrika_, 108(2):299-319.
* Big Data, Machine Learning, and Clinical Medicine. _The New England journal of medicine_, 375(13):1216-1219. Place: United States.
* Pedregosa et al. [2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830.
* Powell [2020] Powell, D. (2020). Quantile Treatment Effects in the Presence of Covariates. _The Review of Economics and Statistics_, 102(5):994-1005.
* Rubin [2005] Rubin, D. B. (2005). Causal inference using potential outcomes. _Journal of the American Statistical Association_, 100(469):322-331.
* Semenova and Chernozhukov [2021] Semenova, V. and Chernozhukov, V. (2021). Debiased machine learning of conditional average treatment effects and other causal functions. _The Econometrics Journal_, 24(2):264-289.
* Singh et al. [2023] Singh, R., Xu, L., and Gretton, A. (2023). Kernel methods for causal functions: dose, heterogeneous and incremental response curves. _Biometrika_, 111(2):497-516. tex.eprint: https://academic.oup.com/biomet/article-pdf/111/2/497/57467664/asad042.pdf.
* VanderWeele [2008] VanderWeele, T. J. (2008). The sign of the bias of unmeasured confounding. _Biometrics. Journal of the International Biometric Society_, 64(3):702-706. Publisher: [Wiley, International Biometric Society].
* Watson [1964] Watson, G. S. (1964). Smooth regression analysis. _Sankhya: The Indian Journal of Statistics, Series A_, pages 359-372.
* 677. Publisher: Institute of Mathematical Statistics and Bernoulli Society.
* Ying Zhang et al. [2020] Ying Zhang, Lei Wang, M. Y. and Shao, J. (2020). Quantile treatment effect estimation with dimension reduction. _Statistical Theory and Related Fields_, 4(2):202-213. Publisher: Taylor & Francis tex.eprint: https://doi.org/10.1080/24754269.2019.1696645.
* Zhou et al. [2022] Zhou, T., Carson, W. E. t., and Carlson, D. (2022). Estimating Potential Outcome Distributions with Collaborating Causal Networks. _Transactions on machine learning research_, 2022. Place: United States.

[MISSING_PAGE_EMPTY:13]

### Nadaraya-Watson estimation

Throughout, we use NW estimation as our standard non-parametric regression technique. For a kernel \(k:\mathcal{X}\times\mathcal{X}\rightarrow[0,\infty)\), IID data sample \(D:=\{(Y_{i},X_{i})\}_{i=1}^{n}\), and \(\bm{x}\in\mathcal{X}\) the NW estimate of \(\mathbb{E}[Y|X=\bm{x}]\) is given by

\[\sum_{i=1}^{n}\frac{k(\bm{x},X_{i})}{\sum_{j=1}^{n}k(\bm{x},X_{j})}Y_{i}.\]

Applying this to our pseudo-outcome regression in Algorithm 1, we get that

\[\hat{h}(y_{0},y_{1}|\bm{x}) \coloneqq\sum_{i\in\mathcal{J}}\frac{k(\bm{x},X_{i})}{\sum_{j=1}^ {n}k(\bm{x},X_{j})}\hat{\varphi}_{y_{0},y_{1}}(Y_{i},X_{i},A_{i})\] (7) \[=\sum_{i\in\mathcal{J}}\frac{k(\bm{x},X_{i})}{\sum_{j\in\mathcal{ J}}k(\bm{x},X_{j})}\bigg{(}\frac{A_{i}-\hat{\pi}(X_{i})}{\hat{\pi}(X_{i})(1- \hat{\pi}(X_{i}))}\] (8) \[\cdot\ \ \Big{\{}\mathds{1}\{Y_{i}\leq y_{A_{i}}\}-\hat{F}_{Y|X,A_{i}}( y_{A_{i}}|X_{i})\Big{\}}\] \[+\ \hat{F}_{a}\left(y_{1}|X_{i}\right)-\hat{F}_{0}\left(y_{0}|X=X_{i }\right)\bigg{)}\]

where for any \(\bm{x}^{\prime}\in\mathcal{X}\), \(a\in\{0,1\}\)

\[\hat{\pi}(\bm{x}^{\prime}) \coloneqq\sum_{i\in\mathcal{I}}\frac{k(\bm{x}^{\prime},X_{i})}{ \sum_{j\in\mathcal{I}}k(\bm{x}^{\prime},X_{j})}\mathds{1}\{A_{i}=1\}\] (9) \[\hat{F}_{a}(y_{a}|\bm{x}^{\prime}) \coloneqq\sum_{i\in\mathcal{I}}\frac{k(\bm{x}^{\prime},X_{i})}{ \sum_{j\in\mathcal{I}}k(\bm{x}^{\prime},X_{j})\mathds{1}\{A_{j}=a\}}\mathds{1} \{Y_{i}\leq y_{a}\}\mathds{1}\{A_{i}=a\}.\] (10)

Note that for our theoretical results we do not specify how our nuisance parameters are estimated and the results only depend on the accuracy of our estimation of the nuisance parameters.

**Remark 6**.: _We can re-write (8) as:_

\[\hat{h}(y_{0},y_{1}|\bm{x}) \coloneqq\sum_{i\in\mathcal{I}_{1}}\frac{k(\bm{x},X_{i})}{\sum_{i= 1}^{n}k(\bm{x},X_{i})}\bigg{(}\frac{1}{\hat{\pi}(X_{i})}\left\{\mathds{1}\{Y_ {i}\leq y_{1}\}-\hat{F}_{1}(y_{1}|X_{i})\right\}\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\hat{F}_{1}(y_{1}|X_{i})-\hat {F}_{0}(y_{0}|X_{i})\bigg{)}\] \[-\ \sum_{i\in\mathcal{I}_{0}}\frac{k(\bm{x},X_{i})}{\sum_{i=1}^{n}k( \bm{x},X_{i})}\bigg{(}\frac{1}{1-\hat{\pi}(X_{i})}\left\{\mathds{1}\{Y_{i}\leq y _{0}\}-\hat{F}_{0}(y_{0}|X_{i})\right\}\] \[+\hat{F}_{0}(y_{0}|X_{i})-\hat{F}_{1}(y_{1}|X_{i})\bigg{)}\]

_Which can be helpful in terms of the practical implementation._

### IPW pseudo-outcome estimator

Another pseudo-outcome one can use for estimating the treatment effect is the based on inverse propensity weighting. Specifically we can take our pseudo-outcome to be

\[\psi_{y_{0},y_{1}}(Y^{\prime},X^{\prime},A^{\prime})\coloneqq\frac{A^{\prime}- \pi(X^{\prime})}{\pi(X^{\prime})(1-\pi(X^{\prime}))}\mathds{1}\{Y^{\prime} \leq y_{A^{\prime}}\}.\] (11)

If we regress against it using NW estimation, and also use NW estimation for our nuisance parameter estimation as in (9) & (10), our estimate \(\hat{h}\) will be increasing in \(y_{1}\) and decreasing in \(y_{0}\) meaning we do not need to perform any isotonic projection.

### Additional experimental details

As the method of Kallus and Oprescu [14] is one for estimating the CQTE, we transform it into an estimator of the CQC (which we denote by \(\hat{g}\)) using the following formula where \(\hat{\tau}_{q}(.|.)\) is our CQTE estimator

\[\hat{g}(y|\bm{x})=\hat{\tau}_{q}\left(F_{Y|X,0}(y|\bm{x})|\bm{x}\right)+y.\]

using the true CCDF, \(F_{Y|X,0}\).

This is the inverse of equation (5) which defines the CQTE in terms of the CQC. Conversely we also tested transforming all our CQC estimators into CQTE estimators (using exactly equation (5) with \(\hat{g}\) replacing \(g^{*}\)) and testing the accuracy on this space and found similar results.

Each experiment took no longer than 1 hour to run on a single 4 core CPU with 8GB of RAM.

The bandwidths of the kernels for the NW estimation of the nuisance parameters were chosen by a limited grid search on additional simulated data by validating against the true value of the nuisance parameters. While this is unrealistic in practice it was done to make estimation of the nuisance parameters as strong as possible. This was to err on the side of caution as strong nuisance parameter estimation would naturally favour the baseline approaches.

Code to implement our approach alongside Jupyter notebooks running our numerical experiments can be found in the supplementary materials. The code to implement the kernels is adapted from https://github.com/wittawatj/kernel-gof/ which is free to use under the MIT Licence.

## Appendix B Additional results

### Simulation results

We run additional simulated experiments in order to further test and explore our approach. Throughout, our overall experimental set-up is the same as in Section 4.1

#### b.1.1 10-dimensional example

To test our problem in higher dimensions we ran experiments where \(X\) was 10-dimensional with \(X\) uniform on \([-1,1]^{10}\). That is each component \(X_{j}\) was independent with \(X_{j}\sim U(-1,1)\). We then took

\[Y|X=\bm{x},A=a \sim N(\sin(\gamma\pi(\bm{\beta}^{\top}\bm{x}),1)\] \[\pi(\bm{x})=0.4\sin(\gamma\pi(\bm{\beta}^{\top}\bm{x}))+0.5\]

where \(\bm{\beta}\) was randomly sampled from \(N(0,(0.2)^{2})\) and \(\gamma\in[0,3]\). This gave the maximum gradient multiplied by \(0.5\operatorname{diam}(\mathcal{X})=\sqrt{d}\) close to \(1\) making the rate of change of the CDFs and propensities similar to our 1-dimensional examples.

In our first experiment we take \(2n=1000\) and \(\gamma\in\{0,0.5,1,1.5,2,2.5,3\}\). The results of this are shown in Figure 4(a) In our second experiment we take \(\gamma=1\) and \(2n\in\{200,500,1000,2000,5000\}\). The results of this are shown in Figure 4(b).

As we can see the DR approach performs the best with it performing close to oracle for low sample size and low frequency. As the frequency increases the DR estimator deviates from the oracle.

#### b.1.2 Varying CQC

In this experiment our set-up is

\[Y|X,A=0 \sim N\left(\frac{\sin(\gamma\pi x)}{0.5x+1.5},1\right)\] \[Y|X,A=1 \sim N\left(\sin(\gamma\pi x)+0.25x+0.75,(0.5x+1.5)^{2}\right)\] \[\pi(x) =0.4\sin(\gamma\pi x)+0.5\]

for \(\gamma\in[0,10]\). This gives \(g^{*}=(y+0.5)(0.5x+1.5)\) which is still simpler than the individual CCDFs but now does depend upon \(x\).

In our first experiment we take \(2n=1000\) and \(\gamma\in\{0,2,4,6,8,10\}\). The results of this are shown in figure 4(a) In our second experiment we take \(\gamma=6\) and \(2n\in\{200,500,1000,2000,5000\}\). The results of this are shown in Figure 4(b).

#### b.1.3 Constant \(h^{*}\)

In our previous examples, while \(g^{*}\) has been simple and or constant, \(h^{*}\) has actually included the high frequency sine term. In this experiment we adjust our original illustrative example to give a constant \(h^{*}\) as well. Specifically we take

\[Y|X=x,A=0 \sim\text{Unif}(\sin(\gamma\pi x),\sin(\gamma\pi x)+1),\] \[Y|X=x,A=1 \sim\text{Unif}(2\sin(\gamma\pi x),2\sin(\gamma\pi x)+2).\]

for \(\gamma\in[0,10]\). We also take the propensity to be \(\pi(x)=0.4\sin(\gamma\pi x)+0.5\). In this case \(h^{*}(y_{0},y_{1}|\bm{x})=\frac{1}{2}y_{1}-y_{0}\) which does not depend on \(\bm{x}\) for any \(y_{0},y_{1}\in\mathcal{Y}\).

Figure 5: Estimation accuracy of 10-dimensional CQC estimation procedures for highly varying CCDFs and constant CQC with respect to \(\bm{x}\).

Figure 6: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and linear CQC with respect to \(x\).

In our first experiment we take \(2n=1000\) and \(\gamma\in\{0,2,4,6,8,10\}\). The results of this are shown in figure 4(a) In our second experiment we take \(\gamma=6\) and \(2n\in\{200,500,1000,2000,5000\}\). The results of this are shown in Figure 4(b).

As we can see we obtain very similar results to original illustrative example suggesting that our method is effectively exploiting simplicity in \(g^{*}\) rather than in \(h^{*}\).

### Employment example: Comparing to the CQTE

To further explore the interpretability of our estimator we provide an estimate of the CQTE for comparison. This can be seen in Figure 9.

In these plots we can see that the interpretation of the CQTE is less immediate than for the case of the CQC. This is because, for each covariate value, the quantile value corresponds to a different untreated response making comparisons between various values of the covariates less direct. From this plot we do still see that higher income quantiles are associated with a greater increase in wages. Interestingly the value of the CQTE plummets 90% at the highest quantile for individuals of age 40. This is likely an estimation error due to the limited data at higher quantiles as and higher ages.

### Colon cancer treatment

To demonstrate the effectiveness of our approach in medical settings, we apply it to a trial on the effect of colon cancer treatment on survival time/time to remission. This dataset was originally introduced in Laurie et al. [19] and can be found in the "survival" package in R and loaded with the line data(colon, package="survival"). In this dataset 929 are randomised to either receive Placebo or Levamisole. They are then followed up for a period of up to 3329 days and the time till

Figure 8: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and constant \(h^{*}\) and CQC.

Figure 7: Constant \(h^{*}\)

either death or recurrence of their cancer. For our analysis we take our response (\(Y\)) as the time to first of death or recurrence. For simplicity, when an individual makes it to the end of a trial without an event and is censored we take that to be the time of their event. Looking at the data censoring times are mostly at around 2,000-3,000 days while over 90% of death or recurrence events that do occur, occur within 1,500 days. Therefore censored individuals will still have better responses than those who had events, as we would want. As our covariate, we took the patients' ages when joining the trial. We show a 3D plot and heat plot of the estimate of \(\Delta(y|\bm{x})=g^{*}(y|\bm{x})-y\) in Figure 10.

Interestingly from Figure 10 we see that the treatment appears to have little effect on patients aged 53-57. For the reaming patients, we can see that there is relatively little effect on the time to event for events of 400 days or less while at around 500 days the difference in time to event jumps to 1000 days. This jump takes the time to event (TTE) to the time when censoring begins. This gives evidence that rather than the treatment delaying the time to event, it increases the proportion of the patients who have no event during the trial essentially fully treating those patients for the duration of the trial. This shows the value of the CQC it was able to reveal meaningful information about the treatment beyond simply its positive effect. On top of this, we are able to identify the censoring within our results without explicitly controlling for it in any way.

## Appendix C Additional theory

### Proof of \(\hat{h}\) accuracy

We first define some additional notation. For an output \(f\) which depends upon \(z=(y,x,a)\) define \(m_{f}(\bm{x})=\mathbb{E}[f(Z)|X=\bm{x}]\) and

\[\hat{m}_{f}(\bm{x})=\sum_{j\in\mathcal{J}}w_{j}f(Z_{j}),\]

with \(w_{j}\) defined as before so that \(\hat{m}_{\hat{\varphi}}(\bm{x})\) is our estimator and \(\hat{m}_{\varphi}(\bm{x})\) is the oracle estimator.

Additionally define

\[\hat{b}(\bm{x})\coloneqq m_{\hat{\varphi}-\varphi}(\bm{x}),\]

in other words the bias in our estimate of the pseudo-outcome for a given \(\bm{x}\). We will also work with \(\hat{m}_{b}(\bm{x})\) where we view \(b\) as being a function of \(z\).

Finally for \(n\in\mathbb{N}\), \(\delta\in(0,1)\), define

\[\varepsilon_{\delta}(n)\coloneqq\begin{cases}\log(2/\delta)/n&\text{if }\delta \in(0,2/e]\\ 1/n&\text{otherwise.}\end{cases}.\]

**Theorem 3** (Stability Result).: _For \(\delta\in(0,1)\) we have that with probability at least \(1-\delta\)_

\[|\hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})|\leq \mathbb{E}[\hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})|X_{ \mathcal{J}},D_{\mathcal{I}}]\] (12) \[+\sqrt{2\varepsilon_{\delta}(n)}\|\bm{w}\|\|\varphi-h(y_{0},y_{1 }|.)\|_{\bm{w}^{2}}+\frac{2\|\bm{w}\|_{\infty}\varepsilon_{\delta}(n)}{3 \xi}=:\mathbb{B}(\varphi).\]

Figure 10: Surface plot and heat plot of \(\Delta(y|\bm{x})\) over \(y,\bm{x}\) for colon cancer trial data with \(X=\)Age, \(Y=\)Time to Event.

_Moreover, with probability at least \(1-\delta\) we have_

\[|\hat{m}_{\hat{\varphi}}(\bm{x})-m_{\varphi}(\bm{x})|\leq\mathbb{B}(\varphi)+| \hat{m}_{\hat{b}}(\bm{x})|+\sqrt{2\varepsilon_{\delta}(n)}\|\bm{w}\|_{2}\|\hat{ \varphi}-\varphi\|_{\bm{w}^{2}}=:\mathbb{B}^{+}(\hat{\varphi}).\] (13)

Proof.: To prove (12) we first observe that since \(\bm{w}\) is \(D_{\mathcal{I}},X_{\mathcal{J}}\)-measurable we have

\[\mathbb{E}[\hat{m}_{\varphi}(\bm{x})^{2}|D_{\mathcal{I}},X_{ \mathcal{J}}]-\mathbb{E}[\hat{m}_{\varphi}(\bm{x})|D_{\mathcal{I}},X_{ \mathcal{J}}]^{2}=\sum_{j\in\mathcal{J}}w_{j}^{2}\mathbb{E}[(\varphi(Z_{j})- \mathbb{E}[\varphi(Z)|X=X_{j}])^{2}|D_{\mathcal{I}},X_{\mathcal{J}}]\]

Note also that we have \(\max_{j\in\mathcal{J}}\lvert w_{j}\{\varphi(Z_{j})-\mathbb{E}[\varphi(Z_{j}) |D_{\mathcal{I}},X_{\mathcal{J}}]\}\rvert\leq\lVert\bm{w}\rVert_{\infty}2 \xi^{-1}\) almost surely. Note also that \(\{\varphi(Z_{j})\}_{j}\in\mathcal{J}\) are conditionally independent given \(D_{\mathcal{I}},X_{\mathcal{J}}\). Hence, the bound (12) follows from two applications Bernstein's inequality Boucheron et al. [7, Theorem 2.10], applied conditionally on \(D_{\mathcal{I}},X_{\mathcal{J}}\).

Next we note that by the triangle inequality we have

\[\leq\mathbb{E}\{\hat{m}_{\hat{\varphi}}(\bm{x})-\hat{m}_{\varphi }(\bm{x})\,|\,D_{\mathcal{I}},X_{\mathcal{J}}\}\rvert+\lvert\mathbb{E}\{\hat{ m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})\,|\,D_{\mathcal{I}},X_{\mathcal{J}}\}\rvert\] \[\leq\left\lvert\sum_{j\in\mathcal{J}}w_{j}\mathbb{E}\left[\hat{ \varphi}(Z_{j})-\varphi(Z_{j})|D_{\mathcal{I}},X_{\mathcal{J}}\right]\right\rvert +\lvert\mathbb{E}\{\hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})\,|\,D_{ \mathcal{I}},X_{\mathcal{J}}\}\rvert\] \[=\lVert\hat{m}_{\hat{b}}\rVert_{\bm{w},1}+\lvert\mathbb{E}\{ \hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})\,|\,D_{\mathcal{I}},X_{\mathcal{ J}}\}\rvert\]

Moreover, since \(\mathbb{E}[\hat{\varphi}(Z_{j})|D_{\mathcal{I}},X_{\mathcal{J}}]\) is the projection of \(\hat{\varphi}(Z_{j})\) onto the subspace of \(D_{\mathcal{I}},X_{\mathcal{J}}\)-measureable functions we have

\[\lVert\bm{w}\rVert_{2}^{2}\,\lVert\hat{m}_{\hat{\varphi}}-m_{ \hat{\varphi}}(\bm{x})\rVert_{\bm{w}^{2}}\] \[=\sum_{j\in\mathcal{J}}w_{j}^{2}\,\mathbb{E}\{(\hat{\varphi}(Z)- \mathbb{E}\{\hat{\varphi}(Z)|X=X_{j}\})^{2}|D_{\mathcal{I}},X_{\mathcal{J}}\}\] \[\leq\sum_{i\in[n]}w_{j}^{2}\,\mathbb{E}\{(\hat{\varphi}(Z_{j})- \mathbb{E}\{\varphi(Z_{j})|D_{\mathcal{I}},X_{\mathcal{J}}\})^{2}|D_{\mathcal{ I}},X_{\mathcal{J}}\}\] \[=\sum_{i\in[n]}w_{j}^{2}\,\mathbb{E}\{(\hat{\varphi}(Z_{j})- \varphi(Z_{j}))+(\varphi(Z_{j})-\mathbb{E}\{\varphi(Z_{j})|D_{\mathcal{I}},X_ {\mathcal{J}}\})\}^{2}|D_{\mathcal{I}},X_{\mathcal{J}})\] \[=\sum_{i\in[n]}w_{j}^{2}\,(\mathbb{E}\{(\hat{\varphi}(Z_{j})- \varphi(Z_{j}))^{2}|D_{\mathcal{I}},X_{\mathcal{J}}\}+\mathbb{E}\{(\varphi(Z _{j})-\mathbb{E}\{\varphi(Z_{j})|D_{\mathcal{I}},X_{\mathcal{J}}\})^{2}|D_{ \mathcal{I}},X_{\mathcal{J}}\})\] \[=\lVert\bm{w}\rVert_{2}^{2}\,\{(||\hat{m}_{\hat{\varphi}}-\hat{m }_{\varphi}\rVert_{\bm{w}^{2}}^{2}+\lVert\hat{m}_{\varphi}-m_{\varphi}\rVert_{ \bm{w}^{2}}^{2})\] \[\leq\lVert\bm{w}\rVert_{2}^{2}\,\{(||\hat{m}_{\hat{\varphi}}-\hat {m}_{\varphi}\rVert_{\bm{w}^{2}}+\lVert\hat{m}_{\varphi}-m_{\varphi}\rVert_{ \bm{w}^{2}})^{2}.\]

Hence, to deduce (13) we apply the first bound with \(\hat{\varphi}\) in place of \(\varphi\) to obtain the following bound with probability at least \(1-\delta\),

\[|\hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})|\leq \lvert\mathbb{E}\{\hat{m}_{\hat{\varphi}}(\bm{x})-m_{\varphi}(\bm{ x})\,|\,D_{\mathcal{I}},X_{\mathcal{J}}\}\rvert+\sqrt{2\varepsilon_{\delta}(n)} \,\lVert\bm{w}\rVert_{2}\,\lVert\hat{m}_{\hat{\varphi}}-m_{\hat{\varphi}}\rVert_ {\bm{w}^{2}}\] \[+2\xi^{-1}\lVert\bm{w}\rVert_{\infty}\,\varepsilon_{\delta}(n)/3\] \[\leq \lVert\hat{m}_{\hat{b}}\rVert_{\bm{w},1}+\lvert\mathbb{E}\{ \hat{m}_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})\,|\,D_{\mathcal{I}},X_{\mathcal{J}}\}\rvert\] \[+\sqrt{2\varepsilon_{n}(\delta)}\lVert\bm{w}\rVert_{2}\,\{(\lVert \hat{m}_{\hat{\varphi}}-\hat{m}_{\varphi}\rVert_{\bm{w}^{2}}+\lVert\hat{m}_{ \varphi}-m_{\varphi}\rVert_{\bm{w}^{2}}\}\] \[+2\xi^{-1}\lVert\bm{w}\rVert_{\infty}\,\varepsilon_{\delta}(n)/3\] \[= \mathbb{B}(\varphi)+\lVert\hat{m}_{\hat{b}}\rVert_{\bm{w},1}+\sqrt{2 \varepsilon_{\delta}(n)}\,\lVert\bm{w}\rVert_{2}\,\lVert\hat{m}_{\hat{\varphi}}- \hat{m}_{\varphi}\rVert_{\bm{w}^{2}}\] \[= \mathbb{B}^{+}(\hat{\varphi}),\]

as required. 

We apply the following result from Kennedy [15].

**Proposition 4** (Proposition 2 from Kennedy [15]).: _Suppose that \(\hat{b}(\bm{x})=\hat{b}_{1}(\bm{x})\hat{b}_{2}(\bm{x})\) then_

\[|\hat{m}_{\hat{b}}(\bm{x})|=\|\bm{w}\|_{1}\|\hat{b}_{1}\|_{\bm{w}}\|\hat{b}_{1}\| _{\bm{w}}.\]

Proof.: Follows from the Cauchy-Schwartz inequality. 

**Proposition 5**.: _Our pseudo-outcome is conditionally unbiased,_

\[\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x}]=h^{*}(y_{0},y_{1}|\bm{x}).\]

Proof.: We have

\[\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x},A=1] =\frac{1}{\pi(\bm{x})}\Big{(}\mathbb{E}[\mathds{1}\{Y\leq y_{1} \}|X=\bm{x},A=1]-F_{1}(y_{1}|\bm{x})\Big{)}+h^{*}(y_{0},y_{1}|\bm{x})\] \[=h^{*}(y_{0},y_{1}|\bm{x}),\]

and similarly \(\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x},A=1]=h^{*}(y_{0},y_{1}|\bm{x})\). The result now follows by the law of total expectation. 

#### c.1.1 Proof of Proposition 1

Proof of Proposition 1.: Fix \(y_{0},y_{1},\bm{x}\) Let \(\hat{m}_{\hat{\varphi}}(\bm{x})\) be the regression of \(X\) against the estimated pseudo-outcome \(\hat{\varphi}_{y_{0},y_{1}}(Z)\) evaluated at \(\bm{x}\) (i.e. \(\hat{h}(y_{0},y_{1}|\bm{x})\)), let \(\hat{m}_{\varphi}\) be the same but with the estimated pseudo-outcome replaced with the exact pseudo-outcome \(\varphi_{y_{0},y_{1}}\). Finally take

\[m_{\varphi}(\bm{x})\coloneqq\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x}]= \mathbb{P}(Y\leq y_{1}|X=\bm{x},A=1)-\mathbb{P}(Y\leq y_{1}|X=\bm{x},A=0).\]

Theorem 3 gives us that with probability. at least \(1-\delta\)

\[|\hat{m}_{\hat{\varphi}}(\bm{x})-m_{\varphi}(\bm{x})| \leq\varepsilon_{\gamma}(n,\delta)+|\hat{m}_{\hat{b}}(\bm{x})|+ \sqrt{2\varepsilon_{\delta}(n)}\|\bm{w}\|_{2}\|\hat{\varphi}-\varphi\|_{\bm{ w}^{2}}\] \[\leq\varepsilon_{\gamma}(n,\delta)+|\hat{m}_{\hat{b}}(\bm{x})|+ \sqrt{2\varepsilon_{\delta}(n)}\|\hat{\varphi}-\varphi\|_{\bm{w}^{2}}\]

with \(\hat{b}((y,\bm{x},a))\coloneqq\mathbb{E}[\hat{\varphi}(Z)-\varphi(Z)|X=\bm{x}]\).

To bound \(\hat{b}\) we have that from Proposition 5,

\[\mathbb{E}[\varphi_{y_{0},y_{1}}(Z)|X=\bm{x}]=\mathbb{P}(Y\leq y_{1}|X=\bm{x},A=1)-\mathbb{P}(Y\leq y_{0}|X=\bm{x},A=0).\]

Additionally, by splitting over the events \(\{A=1\},\{A=0\}\) and noting that \(\mathbb{P}(A=1|X=\bm{x})=\pi(\bm{x})\) we have

\[\mathbb{E}[\hat{\varphi}_{y_{0},y_{1}}(Z)|X=\bm{x}] =\left(\frac{\pi(\bm{x})}{\hat{\pi}(\bm{x})}\right)\Big{(} \mathbb{P}(Y\leq y_{1}|X=\bm{x},A=1)-\hat{\mathbb{P}}(Y\leq y_{1}|X=\bm{x},A=1) \Big{)}\] \[\quad-\ \frac{1-\pi(\bm{x})}{1-\hat{\pi}(\bm{x})}\left(\mathbb{P}(Y \leq y_{0}|X=\bm{x},A=0)-\hat{\mathbb{P}}(Y\leq y_{0}|X=\bm{x},A=0)\right)\] \[\quad+\ \hat{\mathbb{P}}(Y\leq y_{1}|X=\bm{x},A=1)-\hat{\mathbb{P}}(Y \leq y_{0}|X=\bm{x},A=0).\]

Hence

\[\hat{b}(\bm{x})= \left(\frac{\pi(\bm{x})}{\hat{\pi}(\bm{x})}-1\right)\Big{(} \mathbb{P}(Y\leq y_{1}|X=\bm{x},A=1)-\hat{\mathbb{P}}(Y\leq y_{1}|X=\bm{x},A=1 )\Big{)}\] \[\quad-\ \left(\frac{1-\pi(\bm{x})}{1-\hat{\pi}(\bm{x})}-1\right) \Big{(}\mathbb{P}(Y\leq y_{0}|X=\bm{x},A=0)-\hat{\mathbb{P}}(Y\leq y_{0}|X=\bm {x},A=0)\Big{)}\,.\]We have that using Proposition 4

\[|\hat{m}_{\hat{b}}(\bm{x})| \leq\|\bm{w}\|_{1}\Big{\|}\frac{\pi}{\hat{\pi}}-1\Big{\|}_{\bm{w}} \Big{\|}\mathbb{P}(Y\leq y_{1}|X,A=1)-\hat{\mathbb{P}}(Y\leq y_{1}|X,A=1)\Big{\|} _{\bm{w}}\] \[\quad+\ \left\|\frac{1-\pi}{1-\hat{\pi}}-1\right\|_{\bm{w}}\Big{\|} \mathbb{P}(Y\leq y_{0}|X,A=0)-\hat{\mathbb{P}}(Y\leq y_{0}|X,A=0)\Big{\|}_{\bm {w}}\] \[\leq\frac{1}{\xi}\sum_{a=0}^{1}\|\pi-\hat{\pi}\|_{\bm{w}}\Big{\|} \mathbb{P}(Y\leq y_{a}|X,A=a)-\hat{\mathbb{P}}(Y\leq y_{a}|X,A=a)\Big{\|}_{\bm {w}}\,.\]

Now it is simply a matter of bounding all the relevant terms which we do through the following events

\[E_{\rm oracle} \coloneqq\Big{\{}|\hat{m}_{\hat{\varphi}}-m_{\varphi}|\leq|\hat{m }_{\varphi}(\bm{x})-m_{\varphi}(\bm{x})|+|\hat{m}_{\hat{b}}(\bm{x})|+\sqrt{2 \varepsilon_{\delta}(n)}\|\bm{w}\|_{2}\|\hat{\varphi}-\varphi\|_{\bm{w}^{2}} \Big{\}}\] \[E_{\hat{\varphi}} \coloneqq\{\|\hat{\varphi}-\varphi\|_{\bm{w}^{2}}\leq\varepsilon_ {\hat{\varphi}}(n,\delta/4)\}\] \[E_{\gamma} \coloneqq\{\hat{m}_{\varphi}-m_{\varphi}\leq\varepsilon_{\gamma}( n,\delta/4)\}\] \[E_{\alpha} \coloneqq\Big{\{}\|\pi-\hat{\pi}\|_{\bm{w},2}\leq\varepsilon_{ \alpha}(n,\delta/4)\Big{\}}\] \[E_{\beta,0} \coloneqq\Big{\{}\|\hat{\mathbb{P}}(Y\leq y_{0}|X,A=0)-\mathbb{P }(Y\leq y_{0}|X,A=0)\|_{\bm{w},2}\leq g_{\beta}(n,\delta/4)\Big{\}}\] \[E_{\beta,1} \coloneqq\Big{\{}\|\hat{\mathbb{P}}(Y\leq y_{1}|X,A=1)-\mathbb{P }(Y\leq y_{1}|X,A=1)\|_{\bm{w},2}\leq\varepsilon_{\beta}(n,\delta/4)\Big{\}}\]

According to our assumptions and previous results, \(E_{\rm oracle},E_{\hat{\varphi}},E_{\gamma},E_{\alpha}\cap E_{\beta,0}\cap E_{ \beta,1}\) separately occur w.p. at least \(1-\delta/4\). Then by the union bound, the intersection of all these events holds w.p. at least \(1-\delta\) and under these events

\[|\hat{m}_{\hat{\varphi}}-\hat{m}_{\varphi}+\hat{m}_{\varphi}-m_{\varphi}|\leq \varepsilon_{T_{n}}(n,\delta/4)+\frac{2}{\xi}\varepsilon_{\alpha}(n,\delta/4) \varepsilon_{\beta}(n,\delta/4)+\varepsilon_{\gamma}(n,\delta/4)\varepsilon_ {\bm{w}}(n,\delta/4)\]

Where the first term comes from Proposition 3, the second from Proposition 4 and the final term from smoothness of \(g^{*}\). 

### Proof of \(\hat{g}\) accuracy

**Proposition 6** (Maximum step-size bound).: _Suppose that both the outer regression and estimation of CCDFs is fit using linear smoothers so that_

\[\hat{h}(y_{0},y_{1}|\bm{x})=\sum_{j\in\mathcal{J}}w_{j}(\bm{x};X_{\mathcal{J}} )\hat{\varphi}(Z_{j})\qquad\quad\hat{F}_{a}(y|\bm{x}^{\prime})=\sum_{i\in \mathcal{I}}w_{F_{a};i}(\bm{x}^{\prime};X_{\mathcal{J}})\mathds{1}\{Y_{i}\leq y\}\]

_with \(w_{j}(\bm{x}),w_{i}(\bm{x}^{\prime})\) Additionally suppose with probability at least \(1-\delta\)_

\[\max\left\{\max_{j\in\mathcal{J}}w_{j}(\bm{x}),\max_{i\in\mathcal{I}}w_{F_{a} ;i}(X)\right\}\leq\varepsilon_{\bm{w}}(n,\delta).\]

_Then with probability at least \(1-\delta\)_

\[\max_{j\in[n_{1}+1]_{0}}|\hat{h}(y_{0},Y_{1}^{(j)}|\bm{x})-\hat{h}(y_{0},Y_{1 }^{(j+1)}|\bm{x})|\geq\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n)\]

_where \([n_{1}+1]_{0}\coloneqq[n_{1}+1]\cup\{0\}\), \(\hat{h}(y_{0},Y_{1}^{(0)}|\bm{x})=-1\), \(\hat{h}(y_{0},Y_{1}^{(n_{1}+1)}|\bm{x})\), and \(\{Y^{(i)}\}_{i\in I_{1}}\) is the sorted version of \(\{Y_{i}\}_{i\in I_{1}}\)._

Proof.: We immediately have via union bounds that with probability at least \(1-\delta\)

\[\max\left\{\max_{j\in\mathcal{J}}w_{j}(\bm{x}),\max_{j\in\mathcal{J}}w_{F_{a} ;i}(X_{j})\right\}\leq\varepsilon_{\bm{w}}(n,\delta/n).\]

Now we can try to bound the jump size of our function. To do so for a step function \(f:\mathcal{Y}\to\mathbb{R}\) and a step points \(y\in\mathcal{Y}\) we define

\[f(\Delta y)\coloneqq\lim_{\varepsilon\downarrow 0}f(y)-f(y-\varepsilon),\]i.e. the size of the step at \(y\). From the definition of our pseudo-outcome, the jumps occur in different forms at \(\{Y_{j}\}_{\{j\in\mathcal{J},A_{j}=1\}}\), \(\{Y(i)\}_{\{i\in\mathcal{I}|A^{(i)}=1\}}\). Note that we are assuming \(Y\) continuous so these points are all distinct a.s..

For \(Y_{j}\) with \(j\in\mathcal{J}\) and \(A_{j}=1\). We have

\[\hat{h}(y_{0},\Delta Y_{j}|\bm{x}) =w_{\varphi,j}\frac{1}{\hat{\pi}(X_{j})}\mathds{1}\{Y_{j}\leq \Delta Y_{j}\}\] \[\leq\frac{w_{\varphi,j}}{\xi}.\]

For \(i\in\mathcal{I}\) with \(A^{(i)}=1\), we have that

\[|\hat{h}(y_{0},\Delta Y_{i}|\bm{x})| =\left|\sum_{j\in\mathcal{J}}w_{j}\left(-\mathds{1}\{A_{j}=1\} \frac{1}{\hat{\pi}(X_{j})}\hat{F}_{1}(\Delta Y_{i}|X_{j})+\hat{F}_{1}(\Delta Y _{i}|X_{j})\right)\right|\] \[\leq\frac{1}{\xi}\max_{j\in\mathcal{J}}\hat{F}_{1}(\Delta Y_{i}| X_{j})\] \[\leq\frac{1}{\xi}\max_{j\in\mathcal{J}}w_{F_{a};i}(X_{j})\mathds{ 1}\{Y_{i}\leq\Delta Y_{i}\}\] \[=\frac{1}{\xi}\max_{j\in\mathcal{J}}w_{F_{a};i}(X_{j}).\]

Hence the maximum jump size is less than

\[\frac{1}{\xi}\max\left\{\max_{j\in\mathcal{J}}w_{\varphi,j}(\bm{x}),\max_{i\in \mathcal{I},j\in\mathcal{J}}w_{F_{a};i}(X_{j})\right\}.\]

Therefore combining this with our previous bounds gives that with probability at least \(1-\delta\)

\[\max_{i\in[n_{1}+1]_{0}}|\hat{h}(y_{0},Y^{(i)}|\bm{x})-\hat{h}(y_{0},Y^{(i+1)} |\bm{x})|\leq\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n)\]

\(F_{1}\) We also have a result ensuring the step size of the projected function is even smaller.

**Proposition 7**.: _Let \(\bm{\alpha}\in\mathbb{R}^{p}\) and \(\tilde{\bm{\alpha}}\coloneqq P_{\mathrm{Iso}(p)}(\bm{\alpha})\). Then \(|\alpha_{l}-\alpha_{l+1}|\geq|\tilde{\alpha}_{l}-\tilde{\alpha}_{l+1}|\) for all \(l\in[p]\)._

Proof.: We will prove this by first giving an algorithm to compute the projection. Define

\[\psi_{l}(\bm{\alpha})\coloneqq\begin{cases}\bm{\alpha}&\text{if }\alpha_{l}\leq \alpha_{l+1}\\ \left(\alpha_{1},\dots,\frac{\alpha_{l}+\alpha_{l+1}}{2},\frac{\alpha_{l}+ \alpha_{l+1}}{2},\alpha_{m}\right)&\text{if }\alpha_{l}>\alpha_{l}+1.\end{cases}\]

Now define \(\bm{\alpha}^{(0,0)}=\bm{\alpha}\), \(\bm{\alpha}^{(n,i)}=\bm{\psi}_{i}(\alpha^{(n,i-1)})\) for \(i\in[p-1]\) and \(\bm{\alpha}^{(n,0)}=\bm{\alpha}^{(n-1,p-1)}\) for \(n\in\mathbb{N}\). Finally define \(\bm{\alpha}^{(t)}=\bm{\alpha}^{(\lfloor t/p\rfloor,\text{mod}(t,p)}\). Then by Yang and Barber [32] we know that \(\lim_{t\to\infty}\bm{\alpha}^{(t)}=P_{\mathrm{Iso}(p)}(\bm{\alpha})\).

Now if \(|\alpha_{l+1}^{(n,l+1)}-\alpha_{l}^{(n,l+1)}|>|\alpha_{l+1}^{(n,l)}-\alpha_{l} ^{(n,l)}|\). Then \(\alpha_{l+1}^{(n,l+1)}\) has be moved by \(\psi_{l+1}\). Hence \(\alpha_{l+1}^{(n,l+1)}<\alpha_{l+1}^{(n,l)}\). For this to increase the distance then we must have

\[\alpha_{l+1}^{(n,l+1)}<\alpha_{l}^{(n,l+1)}.\]

Therefore as \(\alpha_{l}^{(n+1,l-1)}\geq\alpha_{l}^{n,l+1}\), we have that

\[\alpha_{l+1}^{(n+1,l)}=\alpha_{l}^{(n+1,l)}.\]

By a similar (simpler) argument \(|\alpha_{l}^{(n,l-1)}-\alpha_{l+1}^{(n,l-1)}|>|\alpha_{l}^{(n,l-2)}-\alpha_{l+ 1}^{(n,l-2)}|\) then \(\alpha_{l}^{(n,l)}=\alpha_{l+1}^{(n,l)}\). Hence if any iteration moves adjacent points further apart, a later iteration will make them equal meaning that on convergence they will be equal. Hence we have proved our claim.

We now have the result which justifies our choice to take the isotonic projection of the data which is take from Yang and Barber [32].

**Proposition 8** (Theorem 1 from Yang and Barber [32]).: _Let \(\bm{z}^{*},\,\hat{\bm{z}}\in\mathbb{R}^{p}\) with \(\bm{z}^{*}\in\mathrm{Iso}(p)\). Then for \(\tilde{\bm{z}}\coloneqq P_{\mathrm{Iso}(p)}(\hat{\bm{z}})\),_

\[\max_{l}\lvert z_{l}^{*}-\tilde{z}_{l}\rvert\leq\max_{l}\lvert z_{l}^{*}- \hat{z}_{l}\rvert.\]

We now use this isotonic projection to obtain supremum bounds on the accuracy of our estimator.

**Proposition 9** (Supremum bound on \(\hat{h}\) accuracy).: _Fix \(\bm{x}\in\mathcal{X},y\in y_{0}\) and let \(\hat{h}\) be our original estimate of \(h^{*}\). For \(m\in\mathbb{N}\), take \(\{Y^{(l)}\}_{l=1}^{m}\) to be a potentially random set of points in \(\mathcal{Y}\) in increasing order._

_Now define \(\bm{\alpha}\in\mathbb{R}^{m}\) by \(\alpha_{l}\coloneqq\hat{h}(y_{0},Y^{(l)}\lvert\bm{x})\) and \(\tilde{\bm{\alpha}}\coloneqq P_{\mathrm{Iso}(m)}(\bm{\alpha})\). Finally, define \(\tilde{h}\) to be the piecewise constant right continuous function with \(\tilde{h}(y_{0},Y^{(l)}\lvert\bm{x})\coloneqq\tilde{\alpha}_{l}\)._

_Suppose for any \(y_{1}\in\mathcal{Y}\), \(n\in\mathbb{N}\), and \(\delta,\delta^{\prime}>0\) that_

\[\mathbb{P}\left(\left|\tilde{h}(y_{0},y_{1}\lvert\bm{x})-h^{*}(y_ {0},y_{1}\lvert\bm{x})\right|\geq\varepsilon_{h}(n,\delta)\right) \leq\delta\] \[\mathbb{P}\left(\max_{l\in[m]_{0}}\left|\hat{h}(y_{0},Y^{(l)} \lvert\bm{x})-\hat{h}(y_{0},Y^{(l+1)}\lvert\bm{x})\right|\geq\varepsilon_{step }(n,m,\delta)\right) \leq\delta\]

_where we take \([m]_{0}\coloneqq[m]\cup\{0\}\), \(\hat{h}(y_{0},Y^{(0)}\lvert\bm{x})\coloneqq-1\), and \(\hat{h}(y_{0},Y^{(m+1)}\lvert\bm{x})\coloneqq 1\)._

_Then for any \(\delta,\delta^{\prime}>0,n\in\mathbb{N}\),_

\[\mathbb{P}\left(\sup_{y_{1}\in\mathcal{Y}}\left|\hat{h}(y_{0},y_{1}\lvert\bm{ x})-h^{*}(y_{0},y_{1}\lvert\bm{x})\right|\leq\varepsilon_{h}(n,\delta/m)+ \varepsilon_{step}(n,m,\delta^{\prime})\right)\geq 1-\delta-\delta^{\prime}.\]

Proof.: For each \(l\in[m]\) define the event

\[E_{h,l}\coloneqq\left\{\left|\hat{h}(y_{0},Y_{1}^{(l)}\lvert\bm{x})-h^{*}(y_ {0},Y_{1}^{(l)}\lvert\bm{x})\right|\leq\varepsilon_{h}(n,\delta/m)\right\}\]

and take \(E_{h}\coloneqq\bigcap_{l=1}^{n}\ E_{h,l}\). Additionally define the event

\[E_{step}\coloneqq\left\{\max_{l\in[m]_{0}}\left|\hat{h}(y_{0},Y^{(l)}\lvert \bm{x})-\hat{h}(y_{0},Y^{(l+1)}\lvert\bm{x})\right|<\varepsilon_{step}(n,m, \delta)\right\}\]

Then \(E_{h}\) and \(E_{step}\) hold w.p.s at least \(1-\delta\) and \(1-\delta^{\prime}\) respectively. Also under \(E_{h},E_{step}\), by Propositions 7 & 8, we have that

\[\max_{l\in[m]_{0}}\left|\tilde{h}(y_{0},Y_{1}^{(l)}\lvert\bm{x}) -h^{*}(y_{0},Y_{1}^{(l)}\lvert\bm{x})\right|\leq\varepsilon_{h}(n,\delta/m)\] \[\max_{l\in[m]_{0}}\left|\tilde{h}(y_{0},Y^{(l)}\lvert\bm{x})- \tilde{h}(y_{0},Y^{(l+1)}\lvert\bm{x})\right|<\varepsilon_{step}(n,m,\delta)\]

We then have that for any \(y_{1}\in\mathcal{Y}\) there exists \(i\in[m]\) such that \(y_{1}^{(l-1)}\leq y_{1}\leq y_{1}^{(l)}\).

Hence by monotonicity, we have the following 2 inequalities

\[\hat{h}(y_{0},y_{1}\lvert\bm{x})-h^{*}(y_{0},y_{1}\lvert\bm{x}) \geq\underbrace{\tilde{h}(y_{0},Y_{1}^{(l-1)}\lvert\bm{x})-h^{*}(y_ {0},Y_{1}^{(l)}\lvert\bm{x})}_{\Lambda_{1}}\] \[\hat{h}(y_{0},y_{1}\lvert\bm{x})-h^{*}(y_{0},y_{1}\lvert\bm{x}) \leq\underbrace{\tilde{h}(y_{0},Y_{1}^{(l)}\lvert\bm{x})-h^{*}(y_ {0},Y_{1}^{(l-1)}\lvert\bm{x})}_{\Lambda_{2}}.\]

Now for the first inequality we have

\[\Lambda_{1} =\tilde{h}(y_{0},Y_{1}^{(l-1)}\lvert\bm{x})-\tilde{h}^{*}(y_{0},Y _{1}^{(l)}\lvert\bm{x})+\tilde{h}(y_{0},Y_{1}^{(l)}\lvert\bm{x})-h^{*}(y_{0},Y_ {1}^{(l)}\lvert\bm{x})\] \[\geq-\varepsilon_{h}(n,\delta)-\varepsilon_{step}(n,m,\delta^{ \prime}).\]With the final inequality coming from \(E_{h,l-1}\) and our definition of \(h\). By a similar argument we get from \(E_{h,l}\) that \(\Lambda_{2}\leq\varepsilon_{h}(n,\delta)+\varepsilon_{step}(n,m,\delta^{\prime})\). Again we can use the same approach to get that under \(E_{h,l-1},E_{h,l},E_{step}\)

\[-\varepsilon_{h}(n,\delta)-\varepsilon_{step}(n,m,\delta^{\prime})\leq h^{*}( y_{0},y_{1}|\bm{x})-\tilde{h}(y_{0},y_{1}|\bm{x})\leq\varepsilon_{h}(n,\delta)+ \varepsilon_{step}(n,m,\delta^{\prime}).\]

Hence for our specific \(y_{1}\) under \(E_{h,l},E_{h,l-1},E_{step}\)

\[\left|h^{*}(y_{0},y_{1}|\bm{x})-\tilde{h}(y_{0},y_{1}|\bm{x})\right|\leq \varepsilon_{h}(n,\delta)+\varepsilon_{step}(n,m,\delta^{\prime})\]

Now as this inequality holds for arbitrary \(y_{1}\) under \(E_{h}\cap E_{step}\) (an event which not depend on our choice of \(y_{1}\)) and the intersection of these two events holds w.p. at least \(1-\delta-\delta^{\prime}\) by the union bound we have our result. 

Our final key result before we can piece them all together will be to obtain accuracy in \(g\) from our accuracy in \(h^{*}\)

**Theorem 10** (Single point accuracy bound).: _Fix \(\bm{x},y_{0}\), assume that \(h,\tilde{h}\) are strictly monotonic in \(y_{1}\) and suppose that_

\[\sup_{y_{1}}\lvert h^{*}(y_{0},y_{1}|\bm{x})-\tilde{h}(y_{0},y_{1}|\bm{x}) \rvert\!\!<\varepsilon.\]

_Additionally let \(\alpha\coloneqq F_{0}(y_{0}|\bm{x})\) and assume that \(f_{Y\mid X,A=1}(y_{1}|\bm{x})>\eta\) for all \(y_{1}\in F_{1}^{-1}(B_{\varepsilon}(\alpha)|\bm{x})\) where here we are taking \(F_{1}^{-1}(A|\bm{x})\) to be the pre-image in \(\mathcal{Y}\) of the set \(A\) for fixed \(\bm{x}\). Then_

\[\lvert g^{*}(y_{0}|\bm{x})-\hat{g}(y_{0}|\bm{x})\rvert\leq\frac{2\varepsilon} {\eta},\]

_where \(g^{*}(y_{0}|\bm{x})\) is the unique \(y_{1}\) such that \(h^{*}(y_{0},y_{1}|\bm{x})=0\)._

Proof.: Let \(y_{1}^{*}\coloneqq g^{*}(y_{0}|\bm{x})\) and \(\hat{y}_{1}\coloneqq\hat{g}(y_{0}|\bm{x})\) so that \(F_{1}(y_{1}^{*}|\bm{x})\). From our accuracy assumption on \(\hat{h}\) in the set-up of the Theorem we have

\[\lvert F_{1}(y_{1}^{*}|\bm{x})-F_{1}(\hat{y}_{1}|\bm{x})\rvert =\lvert h^{*}(y_{0},y_{1}^{*}|\bm{x})-h^{*}(y_{0},\hat{y}_{1}|\bm{ x})\rvert\] \[=\lvert h^{*}(y_{0},\hat{y}_{1}|\bm{x})\rvert\] \[\leq\lvert h^{*}(y_{0},\hat{y}_{1}|\bm{x})-\hat{h}(y_{0},\hat{y} _{1}|\bm{x})\rvert\!+\!\lvert\hat{h}(y_{0},\hat{y}_{1}|\bm{x})\rvert\] \[\leq\lvert h^{*}(y_{0},\hat{y}_{1}|\bm{x})-\hat{h}(y_{0},\hat{y} _{1}|\bm{x})\rvert\!+\!\lvert\hat{h}(y_{0},y_{1}^{*}|\bm{x})\rvert\] \[\leq\lvert h^{*}(y_{0},\hat{y}_{1}|\bm{x})-\hat{h}(y_{0},\hat{y} _{1}|\bm{x})\rvert\!+\!\lvert\hat{h}(y_{0},y_{1}^{*}|\bm{x})-h^{*}(y_{0},y_{1} ^{*}|\bm{x})\rvert\] \[\leq 2\varepsilon,\]

where the second and fifth line come from the definition of \(y_{1}^{*}\) and the fourth from the definition of \(\hat{y}_{1}\).

If we define \(\partial_{k}f\) to be the derivative with respect to the \(k^{\text{th}}\) argument of \(f\) then we have

\[\lvert\hat{g}(y_{0}|\bm{x})-g^{*}(y_{0}|\bm{x})\rvert =\lvert F_{1}^{-1}(F_{1}(y_{1}^{*}|\bm{x})|\bm{x})-F_{1}^{-1}(F_{ 1}(\hat{y}_{1}|\bm{x})|\bm{x})\rvert\] \[=\left|\int_{F_{1}(y_{1}^{*}|\bm{x})}^{F_{1}(\hat{y}_{1}|\bm{x})} \partial_{1}F_{1}^{-1}(\beta|\bm{x})\mathrm{d}\beta\right|\] \[=\left|\int_{F_{1}(y_{1}^{*}|\bm{x})}^{F_{1}(\hat{y}_{1}|\bm{x})} \frac{1}{f((F_{1}^{-1}(\beta|\bm{x})|\bm{x})}\mathrm{d}\beta\right|\] \[\leq 2\varepsilon\max_{y_{1}(y_{1}^{*},\hat{y}_{1})}\frac{1}{f(y_{ 1}|\bm{x})}\] \[\leq\frac{2\varepsilon}{\eta}.\]

#### c.2.1 Proof of Theorem 2

We can now finally combine all these results to prove Theorem 2.

Proof of Theorem 2.: From proposition 1 we have that

\[\mathbb{P}\left(\left|\hat{h}(y_{0},y_{1}|\bm{x})-h^{*}(y_{0},y_{1}|\bm{x}) \right|\leq\varepsilon_{h}(n,\delta)\right)\geq 1-\delta\]

with

\[\varepsilon_{h}(n,\delta)\coloneqq\varepsilon_{T_{n}}(n,\delta/4)+ \varepsilon_{\alpha}(n,\delta/4)\varepsilon_{\beta}(n,\delta/4)+\varepsilon_ {\gamma}(n,\delta/4).\]

Now define \(\{Y^{(i)}\}_{i=1}^{n}\) to be the sorted version of \(\{Y_{i}\}_{i=1}^{n}\). Then by Proposition 6 we have

\[\mathbb{P}\left(\max_{j\in[n]_{0}}|\hat{h}(y_{0},Y_{1}^{(i)}|\bm{x})-h^{*}(y_{ 0},Y_{1}^{(i)}|\bm{x})|\geq\varepsilon_{h-step}(n,\delta)\right)\leq\delta.\]

Plugging this into Proposition 9 with \(\varepsilon_{step}(n,m,\delta)\) replaced by \(\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n))\) and \(\varepsilon_{h}(n,\delta)\) replaced with \(\varepsilon_{h}(n,\delta/2)\) gives

\[\mathbb{P}\left(\sup_{y_{1}\in\mathcal{Y}}\left|\tilde{h}(y_{0},y_{1}|\bm{x}) -h^{*}(y_{0},y_{1}|\bm{x})\right|\leq\varepsilon_{h}(n,\delta/(2n))+\xi^{-1} \varepsilon_{\bm{w}}(n,\delta/n))\right)\geq 1-\delta.\]

Now let \(y_{1}^{*}\coloneqq g^{*}(y_{0}|\bm{x})\). Then for any \(y_{1}\in\mathcal{Y}\) if \(|y_{1}^{\prime}-y_{1}^{*}|>s\) this implies that \(|F_{1}(y_{1}^{\prime}|\bm{x})-F_{1}(y_{1}^{*}|\bm{x})|>s\eta\) by our lower bound on the density in \(B_{s}(y_{1}^{*})\). Hence by the contrapositive, if \(|F_{1}(y_{1}^{\prime}|\bm{x})-F_{1}(y_{1}^{*}|\bm{x})|\leq s\eta\) then \(|y_{1}^{\prime}-y_{1}^{*}|<s\).

Now as

\[|F_{1}(y_{1}^{\prime}|\bm{x})-F_{1}(y_{1}^{*}|\bm{x})|\leq s\eta \Leftrightarrow y_{1}^{\prime}\in F_{1}^{-1}(B_{s\eta}(F_{1}(y_{1}^{*})))\] \[\Leftrightarrow y_{1}^{\prime}\in B_{s\eta}F_{1}^{-1}(B_{s\eta}(F_{0}(y_{0})))\]

Hence if \(n\) is sufficiently large so that \(\varepsilon\coloneqq\varepsilon_{h}(n,\delta/(2n))+\xi^{-1}\varepsilon_{\bm{w }}(n,\delta/n))\leq\eta s\) then we satisfy the bounded density condition of Theorem 10. Therefore we can plug our bound into theorem 10 gives

\[\mathbb{P}\left(|\hat{g}(y_{0}|\bm{x})-g^{*}(y_{0}|\bm{x})|\leq 2\left(\eta^{-1} \varepsilon_{h}(n,\delta/(2n))+\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n)) \right)\right)\geq 1-\delta.\]

### Extension to expectation

**Proposition 11**.: _Let \(Y_{0},D\) be RVs on \(\mathcal{Y},\mathcal{Z}^{n}\) respectively (with \(D\) representing data used to fit a model and \(Y_{0}\) representing the point where the model is fit). Now take \(l(Y_{0},D)\) to be a non-negative bounded loss so that \(l(Y_{0},D)<l_{\max}\) a.s.. Suppose that for any \(\delta>0\), for all \(y\in\mathcal{Y}\)_

\[\mathbb{P}(l(y,D)>\varepsilon(n,\delta))<\delta\]

_Then for any \(t,\delta_{0}\in[0,1]\) and \(p,q\in[1,\infty]\) such that \(1/p+1/q=1\)_

\[\mathbb{P}\left(\mathbb{E}[l(Y_{0},D)|D]\leq t^{1/q}\mathbb{E}[l(Y_{0},D)^{p}| D]^{1/p}+\varepsilon(n,\delta_{0})\right)\geq 1-\frac{\delta_{0}}{t}.\]

_In particular if \(l(y,D)<l_{\max}\) a.s. then taking \(q=1,p=\infty\) yields_

\[\mathbb{P}\left(\mathbb{E}[l(Y_{0},D)|D]\leq tl_{\max}+\varepsilon(n,\delta_{0 })\right)\geq 1-\frac{\delta_{0}}{t}.\]

Proof.: First fix \(t,\delta_{0}\in[0,1]\) We will first bound the probability that the number of \(y\) which don't satisfy our bound isn't too large. We do this by defining the event

\[A\coloneqq\left\{l(Y_{0},D)>\varepsilon(n,\delta_{0})\right\},\]

and now aim to bound the probability that \(\mathbb{P}(A|D)>t\) (this probability is just w.r.t \(Y_{0}\) treating \(D\) as fixed.)By Markov's inequality,

\[\mathbb{P}(\mathbb{P}(A|D)>t) \leq\frac{1}{t}\mathbb{E}[\mathbb{P}(A|D)]\] \[=\frac{1}{t}\mathbb{E}[\mathbb{P}(A|Y_{0})]\quad\text{by Fubini's Theorem}\] \[<\frac{1}{t}\mathbb{E}[\delta_{0}]=\frac{\delta_{0}}{t}.\]

Now define \(B\coloneqq\{\mathbb{P}(A|D)\leq t\}\) so that \(\mathbb{P}(B)=1-\frac{\delta_{0}}{t}\). Then under \(B\)

\[\mathbb{E}[l(Y_{0},D)|D] =\mathbb{E}[\mathds{1}_{A}l(Y_{0},D)|D]+\mathbb{E}[l(Y_{0},D)|A^{ c},D]\mathbb{P}(A^{c}|D)\] \[\leq\mathbb{E}[\mathds{1}_{A}l(Y_{0},D)|D]+\varepsilon(n,\delta_ {0})\quad\text{by definitions of }A\] \[\leq\mathbb{E}[\mathds{1}_{A}|D]^{1/q}\mathbb{E}[l(Y_{0},D)^{p}|D ]^{1/p}+\varepsilon(n,\delta_{0})\quad\text{by Holder's inequality}\] \[\leq t^{1/q}\mathbb{E}[l(Y_{0},D)^{p}|D]^{1/p}t+\varepsilon(n, \delta_{0})\quad\text{As we are assuming }B.\]

**Corollary 12**.: _Assume that \(F_{1}(y|\bm{x})>\eta\) for all \(y_{1}\in\mathcal{Y}\). Then, for our fixed \(\bm{x}\in\mathcal{X}\) and \(\delta\in(0,e^{-1})\), w.p. at least,_

\[1-\frac{\delta\operatorname{diam}(\mathcal{Y})}{2\left(\eta^{-1} \varepsilon_{h}(n,\delta/2n)+\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n))\right)},\] \[\mathbb{E}\Big{[}\left|\,\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x})\right| \,\Big{|}A=0,\hat{g}\Big{]}\leq 4\left(\eta^{-1}\varepsilon_{h}(n,\delta/n)+2\xi^{-1 }\varepsilon_{\bm{w}}(n,\delta/n))\right)\] \[\text{where}\quad\varepsilon_{h}(\delta,n)\coloneqq\varepsilon_{T_ {n}}(n,\delta/4)+\varepsilon_{\alpha}(n,\delta/4)\varepsilon_{\beta}(n,\delta/ 4)+\varepsilon_{\gamma}(n,\delta/4).\]

_In particular for \(2\left(\eta^{-1}\varepsilon_{h}(n,\delta/n)+2\xi^{-1}\varepsilon_{\bm{w}}(n, \delta/n))\right)\leq\log(e_{1}(n)/\delta)^{a}/e_{2}(n).\) For \(\delta<1/e\) we have that w.p. at least \(1-\delta\)_

Proof.: Plugging the result of theorem 2 into proposition 11, noting that \(|\hat{g}(y_{0}|\bm{x})-g^{*}(y|\bm{x})|\leq\operatorname{diam}(\mathcal{Y})\) and taking

\[t=\frac{2}{\operatorname{diam}(\mathcal{Y})}\left(\eta^{-1}\varepsilon_{h}(n, \delta/(2n))+\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n))\right)\]

yields that w.p. at least

\[1-\frac{\delta\operatorname{diam}(\mathcal{Y})}{2\left(\eta^{-1} \varepsilon_{h}(n,\delta/n)+\xi^{-1}\varepsilon_{\bm{w}}(n,\delta/n))\right)},\] \[\mathbb{E}\Big{[}\left|\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x})\right|\, \Big{|}A=0,\hat{g}\Big{]}\leq 4\left(\eta^{-1}\varepsilon_{h}(n,\delta/n)+2\xi^{-1 }\varepsilon_{\bm{w}}(n,\delta/n))\right).\]

Following from this if \(2\left(\eta^{-1}\varepsilon_{h}(n,\delta/n)+2\xi^{-1}\varepsilon_{\bm{w}}(n, \delta/n))\right)\leq\log(e_{1}(n)/\delta)^{\alpha}/e_{2}(n)\). Then we have

\[\mathbb{P}\left(\mathbb{E}\Big{[}\left|\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x})\right| \,\Big{|}A=0,\hat{g}\Big{]}\geq 2\log(e_{1}(n)/\delta)^{a}/e_{2}(n)\right)\leq \frac{\delta\operatorname{diam}(\mathcal{Y})e_{2}(n)}{\log(e_{1}(n)/\delta)^{ a}}\]

for \(\delta<1/e\) we have

\[\delta<1/e \Rightarrow\delta<\exp\left\{-(\frac{1}{2})^{1/a}\right\}\] \[\Leftrightarrow\log(1/\delta)^{a}>\frac{1}{2}\] \[\Rightarrow\log(e_{1}(n)/\delta)^{a}>\frac{1}{2}\] \[\Rightarrow 2\delta\log(e_{1}(n)/\delta)^{a}>\delta\] \[\Leftrightarrow 2\delta>\frac{\delta}{\log(e_{1}(n)/\delta)^{a}}.\]Hence

\[\mathbb{P}\left(\mathbb{E}\Big{[}\left|\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x})\right|\ \Big{|}A=0,\hat{g}\right]\geq 2\log(e_{1}(n)/\delta)^{a}/e_{2}(n) \right)\leq 2\delta\operatorname{diam}(\mathcal{Y})e_{2}(n).\]

Finally \(\delta^{\prime}=2\delta\operatorname{diam}(\mathcal{Y})e_{2}(n)\) gives

\[\mathbb{P}\left(\mathbb{E}\Big{[}\left|\hat{g}(Y|\bm{x})-g^{*}(Y|\bm{x}) \right|\ \Big{|}A=0,\hat{g}\right]\geq 2\log(2\operatorname{diam}(\mathcal{Y})e_{ 1}(n)e_{2}(n)/\delta^{\prime})^{a}/e_{2}(n)\right)\leq\delta^{\prime}.\]

### Application and justification of NW estimation with box kernel

We aim to show that the box kernel satisfies some of our conditions. Specifically conditions 2 & 3 from Proposition 1. We first start by bounding the step size.

**Proposition 13** (Effective sample size).: _Suppose that for our \(\bm{x}\in\mathcal{X}\) there exists \(C_{0},r_{0}>0\) such that for any \(r\in(0,r_{0})\)_

\[\mathbb{P}(X\in B_{r}(\bm{x}))\geq C_{0}r^{d}.\]

_Now for \(r\in(0,r_{0})\) take our kernel to be \(k_{r}(\bm{x},\bm{x}^{\prime})\coloneqq\mathds{1}\{\|\bm{x}-\bm{x}^{\prime}\| \leq r\}\). Then w.p. at least \(1-\delta\)_

\[\sum_{j\in\mathcal{J}}\mathds{1}\{X_{j}\in B_{r}(\bm{x})\}\geq \left(nC_{0}r^{d}-\sqrt{2nC_{0}r^{d}\log(1/\delta)}-\log(1/\delta)/3\right)^{ -1}.\]

Proof.: We have \(\mathds{1}\{X_{j}\in B_{r}(\bm{x})\}\leq 1\) and \(\mathbb{E}[\mathds{1}\{X_{j}\in B_{r}(\bm{x})\}^{2}]=\mathbb{E}[\mathds{1}\{ X_{j}\in B_{r}(\bm{x})\}^{2}]=\mathbb{P}(X_{j}\in B_{r}(\bm{x}))=C_{0}r^{d}\). Therefore by one sided Bernstein's inequality with \(\varepsilon=\log(1/\delta)\) we get

\[\mathbb{P}\left(\sum_{j\in\mathcal{J}}\mathds{1}\{X_{j}\in B_{r}(\bm{x})\} \leq nC_{0}r^{d}-\sqrt{2nC_{0}r^{d}\log(1/\delta)}-\frac{1}{3}\log(1/\delta) \right)\leq\delta.\]

This gives our desired result. 

Note that \(\sum_{j\in\mathcal{J}}\mathds{1}\{X_{j}\in B_{r}(\bm{x})\}\) is also the effective sample size of our estimation as it is the number of samples used in the average.

Now that we have the effective sample size result in terms of our kernel radius \(r\), we need to obtain the optimal rate of decay of \(r\) for our estimation.

**Proposition 14** (NW estimation with box kernel).: _let \(\hat{m}_{f}(\bm{x})\) be the NW estimation of \(m_{f}(\bm{x})\coloneqq\mathbb{E}[f(Z)|X=\bm{x}]\) using IID copies \((Z_{i})_{i=1}^{n}\) of \(Z\). and assume \(|f(Z)|\leq M\). For a fixed \(r\in(0,r)\), use kernel \(k_{r}\) as defined above and suppose the same assumptions hold. Suppose that \(m_{f}(\bm{x})\) is \(\alpha\) smooth for \(\alpha\leq 1\) (i.e. \(\alpha\)-Holder continuous.) Then for sufficiently large \(n\) depending on \(C_{0},\alpha\) and \(\delta\leq 2e^{-1}\) w.p. at least \(1-\delta\),_

\[|\hat{m}_{f}(\bm{x})-m_{f}(\bm{x})|\leq\frac{2M+1}{C_{0}}\log(2/\delta)n^{- \frac{1}{2+\delta/\alpha}}.\]

Proof.: With our kernel define \(\mathcal{I}_{r}\coloneqq\{i\in[n]|k_{r}(\bm{x},X_{i})=1\}\) and \(n_{r}=|I_{r}|\). Now define the event

\[E_{n}\coloneqq\{n_{r}\geq C_{0}nr^{d}-\sqrt{2C_{0}r^{d}n\log(2/\delta)}-\log( 2/\delta)/3\}.\]Then by Proposition 13 this event occur w.p. at least \(1-\delta/2\). Now if we define \(\varepsilon_{i}\coloneqq f(Z_{i})-m_{f}(X_{i})\) then we have \(\mathbb{E}[\varepsilon_{i}|X_{i}]=0\). Also, we have

\[|\hat{m}_{f}(\bm{x})-m_{f}(\bm{x})| =\left|\frac{1}{n_{r}}\sum_{i\in\mathcal{I}_{r}}f(Z_{i})\;-\;m_{ f}(\bm{x})\right|\] \[=\left|\frac{1}{n_{r}}\sum_{i\in\mathcal{I}_{r}}m_{f}(X_{i})+ \varepsilon_{i}\;-\;m_{f}(\bm{x})\right|\] \[\leq\frac{1}{n_{r}}\sum_{i\in\mathcal{I}_{r}}|m_{f}(X_{i})-m_{f}( \bm{x})|+\left|\frac{1}{n_{r}}\sum_{i\in\mathcal{I}_{r}}\varepsilon_{i}\right|\] \[\leq r^{\alpha}+\left|\frac{1}{n_{r}}\sum_{i\in\mathcal{I}_{r}} \varepsilon_{i}\right|.\]

With the final equality coming by our smoothness condition and definition of \(\mathcal{I}_{r}\). Now by Hoeffding bounds we have that

\[\mathbb{P}\left(\left|\sum_{i\in\mathcal{I}_{r}}\varepsilon_{i}\right|\geq \sqrt{\frac{2\log(4/\delta)M^{2}}{n_{r}}}\right)\leq\frac{\delta}{2}.\]

Hence by combining this event and \(E_{n}\) then we have that with probability at least \(1-\delta\)

\[|\hat{m}_{f}(\bm{x})-m_{f}(\bm{x})| \leq r^{\alpha}+\sqrt{\frac{2\log(2/\delta)M^{2}}{n_{r}}}\] \[\leq r^{\alpha}+\sqrt{\frac{2\log(2/\delta)M^{2}}{C_{0}nr^{d}- \sqrt{2C_{0}nr^{d}\log(2/\delta)}-\log(2/\delta)/3}}.\]

Now for sufficiently large \(n\)

\[C_{0}nr^{d}-\sqrt{2C_{0}nr^{d}\log(2/\delta)}-\log(2/\delta)/3\geq\frac{C_{0} nr^{d}}{2\log(2/\delta)}.\]

This in turn gives

\[|\hat{m}_{f}(\bm{x})-m_{f}(\bm{x})|\leq r^{\alpha}+\frac{2\log(2/\delta)M}{ \sqrt{C_{0}nr^{d}}}.\]

Then the optimal choice of \(r\) is such that

\[r^{\alpha} =\frac{1}{\sqrt{nr^{d}}}\] \[\Leftrightarrow r^{\alpha+\frac{d}{2}} =n^{-\frac{1}{2}}\] \[\Leftrightarrow r =n^{-\frac{1}{2\alpha+d}}\] \[\Leftrightarrow r^{\alpha} =n^{-\frac{1}{2\alpha+d/\alpha}}=\frac{1}{\sqrt{nr^{d}}}.\]

Hence plugging this in we get that for sufficiently large \(n\) depending on \(C_{0},\alpha\) we have that for \(\delta\leq 2e^{-1}\) w.p. at least \(1-\delta\)

\[|\hat{m}_{f}(\bm{x})-m_{f}(\bm{x})|\leq\frac{2M+1}{C_{0}}\log(2/\delta)n^{- \frac{1}{2\alpha+d/\alpha}}.\]

**Proposition 15** (Final weight decay).: _Suppose that for our \(\bm{x}\in\mathcal{X}\) there exists \(C_{0},r_{0}>0\) such that for any \(r\in(0,r)\)_

\[\mathbb{P}(X\in B_{r}(\bm{x}))\geq C_{0}r^{d}.\]_Suppose that we are performing NW estimation of an \(\alpha\) smooth function at points \(\bm{x}\in\mathcal{X}\) using kernel \(k_{r_{n}}(\bm{x},\bm{x}^{\prime})\coloneqq\mathds{1}\{\|\bm{x}-\bm{x}^{\prime}\| \leq r\}\) with \(r_{n}\) decaying optimally. Now define_

\[w_{j}\coloneqq\frac{k_{r_{n}}(\bm{x},X_{j})}{\sum_{j^{\prime}\in\mathcal{X}}k_ {r_{n}}(\bm{x},X_{j^{\prime}})}\]

_the weight of the \(j^{\text{th}}\) component. Then with probability at least \(1-\delta\)_

\[\max_{j\in\mathcal{J}}w_{j}\leq=\frac{\log(2/\delta)}{C_{0}}n^{\frac{-2}{2+d/ \gamma}}.\]

Proof.: We have

\[\max_{j\in\mathcal{J}}w_{j} \leq\frac{1}{\sum_{j\in\mathcal{J}}k(\bm{x},X_{j})}\] \[=\frac{1}{\sum_{j\in\mathcal{J}}\mathds{1}\{X_{j}\in B_{r}(\bm{x })\}}.\]

Hence by proposition 13 we have w.p. at least \(1-\delta\)

\[\max_{j\in\mathcal{J}}w_{j}\leq\left(C_{0}nr^{d}-\sqrt{2C_{0}nr^{d}\log(1/ \delta)}-\log(1/\delta)/3\right)\right)^{-1}.\]

We know from Proposition 14 that the optimal radius decay gives \(\frac{1}{nr^{d}}=n^{-\frac{2}{2+d/\gamma}}\). Plugging this into our result gives that

\[\max_{j\in\mathcal{J}}w_{j} \leq\left(C_{0}n^{\frac{2}{2+d/\gamma}}-\sqrt{2C_{0}\log(1/ \delta)}n^{\frac{1}{2+d/\gamma}}-\log(1/\delta)/3\right)\right)^{-1}\] \[\leq\frac{\log(1/\delta)}{C_{0}n^{\frac{2}{2+d/\gamma}}}\quad \text{For sufficiently large $n$ depending on $\gamma,C_{0}$.}\]

Note that for a \(\gamma\) smooth function the MSE is \(C^{-1}n^{-\frac{1}{2+d/\gamma}}\). Hence the box kernel comfortably satisfies our weight decay condition 2 in Assumptions 2.

Additionally now if we have an \(\alpha\) smooth function and assume that for any \(\bm{x}^{\prime}\in\mathcal{X}\) there exists \(C_{0}(\bm{x}^{\prime})\) such that for all \(r\in(0,r_{0})\)\(\mathbb{P}(X\in B_{r}(\bm{x}^{\prime})\geq C_{0}r^{d}\). Then we have that w.p. at least \(1-\delta\),

\[w_{F_{\alpha};i}(X,X^{\mathcal{I}})\leq\frac{\log(2/\delta)}{C_{0}(X)}n^{- \frac{2}{2+d/\alpha}}.\]

Thus if we assume that there exists \(C^{\prime\prime}>0\) such that w.p. at least \(1-\delta\), \(\frac{1}{C_{0}(X)}\leq C^{\prime\prime}\sqrt{\log(1/\delta)}\) then we get that w.p. at least \(1-\delta\)

\[w_{F_{\alpha};i}(X,X^{\mathcal{I}})\leq C^{\prime\prime}\log^{3/2}(3/\delta)n^ {-\frac{2}{2+d/\alpha}}.\]

This then gives our condition 2 in Assumptions 2.

We now try to bound \(\frac{\sum_{j\in\mathcal{J}}w_{j}^{2}\sigma(X_{j})}{\mathbb{E}[\sum_{j\in \mathcal{J}}w_{j}^{2}\sigma(X_{j})]}\).

**Corollary 16** (Accuracy under NW estimation with box kernel).: _Suppose that our linear smoother is NW estimation with the box kernel and optimally decaying radius additionally assume that:_

* \(\varepsilon_{\hat{\varphi}}(n,\delta)=e_{\hat{\varphi}}(n)\sqrt{\log(1/\delta)}\) _with_ \(e_{1}=o(1)\)_._
* \(\varepsilon_{\alpha}(n,\delta)=C_{\alpha}\sqrt{\log(1/\delta)}n^{-\frac{1}{2+ d/\alpha}}\)_._
* \(\varepsilon_{\beta}(n,\delta)=C_{\beta}\sqrt{\log(1/\delta)}n^{-\frac{1}{2+d/ \beta}}\)_._

[MISSING_PAGE_FAIL:30]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We introduce a a new estimand, the CQC. We claim this to maintain the distributional information of the CQTE which we show through the definition and also on real world scenarios in Section 4. We claim that our method is able to obtain double robustness which we show in proposition 1 and Theorem 2. We also demonstrate its strong performance empirically on simulated data scenarios in Section 4 and Appendix B.1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a dedicated limitations section where we discuss limitations in our theoretical results regarding both assumptions of sufficiently quickly decaying weights in our linear smoother and requiring smoothness in \(h^{*}\) rather than in \(g^{*}\). We also discuss the limitations in the interpretability of quantile based approaches in general when exploring treatment effect as well as limitations in the interpretability of our assumptions within our theory. Finally we briefly address the methodological limitations of our approach acknowledging this as a potential area for improvement. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The two main results of the paper, Proposition 1 and Theorem 2 are proved in sections C.1 and C.2. The assumptions for these results are given in Assumptions 1 & 2 with the implications of these assumptions discussed. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper gives detailed information about the experimental set-up and explicit algorithms for methods introduced. The specific use of algorithm 1 with NW estimation is also described Appendix A.2. In addition code is provided in the supplementary materials with 'SimulatedExperiments.ipynb' reproducing all of the experiments. We also provide a link to a public github repository containig this in a footnote on page 1.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided in the supplementary materials with each function documented as well as in a linked public github repository. Jupyter Notebooks are provided to reproduce all of the experiments and real world data settings used within the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Most of the experimental details are provided alongside the experimental results in Section 4 with additional details provided in Appendix A.4. In addition code to replicate all of the experiments is provided in the supplmentary materials. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For our experimental results we explain that 500 replications of the entire fitting procedure (including newly generated data making each run entirely independent) was run and that 95% confidence intervals are provided for the mean absolute error of each estimator. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: Information on runtime and compute resources is provided in Appendix A.4. Overall computational resources were very low with all experiments run on a personal laptop and each one taking less than an hour even with a large number of replications. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: No outsourcing was done for this project/paper and all datasets used are publicly available and completely anonymised. Additionally we feel there is essentially no scope for our work to cause negative societal impact. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential positive impact of our work in terms of further empowering HTE analysis. We do not for see any potential negative impacts of our work as it is simply furthering the field of HTE and quantile based treatment effect analysis. To our knowledge, there is no suggestion that these fields could be used maliciously or unfairly beyond the ways that any regression technique or analysis could. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There are no new datasets released in our work. Our models use standard non-parametric regression techniques and adapt these to best estimate our estimand, the CQC meaning there is no real risk of misuse beyond the misuse of any regression technique. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All datasets are publicly available and properly referenced. Proprietary code used is also open source and used in accordance with its licence. The licence for this code can also be found in code itself. The model we use is our own and so no licensing is required. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The only asset is the code which is implemented in our method which is provided in the supplementary materials alongside a licence. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or research with human subjects is used. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.